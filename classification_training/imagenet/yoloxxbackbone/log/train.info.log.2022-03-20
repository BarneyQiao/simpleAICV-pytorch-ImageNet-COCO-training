2022-03-20 08:06:47 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.5915
2022-03-20 08:07:25 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.5781
2022-03-20 08:08:03 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.3263
2022-03-20 08:08:40 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.5231
2022-03-20 08:08:42 - train: epoch 044, train_loss: 1.4874
2022-03-20 08:09:57 - eval: epoch: 044, acc1: 68.562%, acc5: 88.874%, test_loss: 1.2614, per_image_load_time: 1.187ms, per_image_inference_time: 0.733ms
2022-03-20 08:09:58 - until epoch: 044, best_acc1: 68.812%
2022-03-20 08:09:58 - epoch 045 lr: 0.010000000000000002
2022-03-20 08:10:42 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.2380
2022-03-20 08:11:20 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.4777
2022-03-20 08:11:57 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.5896
2022-03-20 08:12:35 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.3682
2022-03-20 08:13:13 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.6703
2022-03-20 08:13:51 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.4993
2022-03-20 08:14:28 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.1659
2022-03-20 08:15:06 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.4809
2022-03-20 08:15:44 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.4712
2022-03-20 08:16:22 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.4308
2022-03-20 08:16:59 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.6353
2022-03-20 08:17:37 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.5083
2022-03-20 08:18:15 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.5419
2022-03-20 08:18:53 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.4587
2022-03-20 08:19:31 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.5344
2022-03-20 08:20:09 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.4216
2022-03-20 08:20:46 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.4606
2022-03-20 08:21:24 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.3385
2022-03-20 08:22:02 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.5512
2022-03-20 08:22:40 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.6406
2022-03-20 08:23:18 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.5669
2022-03-20 08:23:56 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.6126
2022-03-20 08:24:34 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.2774
2022-03-20 08:25:12 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.6158
2022-03-20 08:25:50 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.4819
2022-03-20 08:26:28 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.4596
2022-03-20 08:27:06 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.3652
2022-03-20 08:27:44 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.4420
2022-03-20 08:28:22 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.6277
2022-03-20 08:28:59 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.6917
2022-03-20 08:29:37 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.4641
2022-03-20 08:30:15 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.5843
2022-03-20 08:30:53 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.5500
2022-03-20 08:31:31 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.2523
2022-03-20 08:32:09 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.7289
2022-03-20 08:32:47 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.5009
2022-03-20 08:33:25 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.4492
2022-03-20 08:34:03 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.2731
2022-03-20 08:34:41 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.4514
2022-03-20 08:35:19 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.5977
2022-03-20 08:35:57 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.4017
2022-03-20 08:36:35 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.6056
2022-03-20 08:37:12 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.6583
2022-03-20 08:37:50 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.5873
2022-03-20 08:38:28 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.4451
2022-03-20 08:39:06 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.6295
2022-03-20 08:39:44 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.5265
2022-03-20 08:40:22 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.4008
2022-03-20 08:40:59 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.4253
2022-03-20 08:41:37 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.4149
2022-03-20 08:41:39 - train: epoch 045, train_loss: 1.4833
2022-03-20 08:42:54 - eval: epoch: 045, acc1: 68.168%, acc5: 88.728%, test_loss: 1.2688, per_image_load_time: 1.058ms, per_image_inference_time: 0.706ms
2022-03-20 08:42:56 - until epoch: 045, best_acc1: 68.812%
2022-03-20 08:42:56 - epoch 046 lr: 0.010000000000000002
2022-03-20 08:43:40 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.1801
2022-03-20 08:44:17 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.3186
2022-03-20 08:44:55 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.5149
2022-03-20 08:45:32 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.6295
2022-03-20 08:46:10 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.2293
2022-03-20 08:46:48 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.5835
2022-03-20 08:47:25 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.3975
2022-03-20 08:48:03 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.5326
2022-03-20 08:48:41 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.4954
2022-03-20 08:49:18 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.2934
2022-03-20 08:49:56 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.4527
2022-03-20 08:50:34 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.4579
2022-03-20 08:51:11 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.3898
2022-03-20 08:51:49 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.5966
2022-03-20 08:52:27 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.4890
2022-03-20 08:53:05 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.5016
2022-03-20 08:53:42 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.3462
2022-03-20 08:54:20 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.4779
2022-03-20 08:54:58 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.2828
2022-03-20 08:55:36 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.5311
2022-03-20 08:56:14 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.6154
2022-03-20 08:56:52 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.2838
2022-03-20 08:57:29 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.6845
2022-03-20 08:58:07 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.5422
2022-03-20 08:58:45 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.5818
2022-03-20 08:59:23 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.6083
2022-03-20 09:00:01 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.3802
2022-03-20 09:00:39 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.3935
2022-03-20 09:01:16 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.4219
2022-03-20 09:01:54 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.2871
2022-03-20 09:02:32 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.3510
2022-03-20 09:03:10 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.7124
2022-03-20 09:03:48 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.4997
2022-03-20 09:04:26 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.6018
2022-03-20 09:05:04 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.5013
2022-03-20 09:05:42 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.3721
2022-03-20 09:06:20 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.3714
2022-03-20 09:06:58 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.6093
2022-03-20 09:07:36 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.4679
2022-03-20 09:08:14 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.4300
2022-03-20 09:08:52 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.7102
2022-03-20 09:09:30 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.3791
2022-03-20 09:10:08 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.6017
2022-03-20 09:10:46 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.5046
2022-03-20 09:11:23 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.3580
2022-03-20 09:12:02 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.5490
2022-03-20 09:12:40 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.4071
2022-03-20 09:13:17 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.4398
2022-03-20 09:13:55 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.5205
2022-03-20 09:14:33 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.4939
2022-03-20 09:14:35 - train: epoch 046, train_loss: 1.4824
2022-03-20 09:15:51 - eval: epoch: 046, acc1: 68.412%, acc5: 88.792%, test_loss: 1.2577, per_image_load_time: 0.669ms, per_image_inference_time: 0.717ms
2022-03-20 09:15:52 - until epoch: 046, best_acc1: 68.812%
2022-03-20 09:15:52 - epoch 047 lr: 0.010000000000000002
2022-03-20 09:16:36 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.3550
2022-03-20 09:17:13 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.5388
2022-03-20 09:17:51 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.3540
2022-03-20 09:18:29 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.3108
2022-03-20 09:19:07 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.5736
2022-03-20 09:19:44 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.5088
2022-03-20 09:20:22 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.5452
2022-03-20 09:21:00 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.3908
2022-03-20 09:21:38 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.7642
2022-03-20 09:22:16 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.4817
2022-03-20 09:22:54 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.5326
2022-03-20 09:23:31 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.4647
2022-03-20 09:24:09 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.5106
2022-03-20 09:24:47 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.4853
2022-03-20 09:25:25 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.4275
2022-03-20 09:26:03 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.4253
2022-03-20 09:26:41 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.3482
2022-03-20 09:27:18 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.4504
2022-03-20 09:27:56 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.3650
2022-03-20 09:28:34 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.4646
2022-03-20 09:29:12 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.5984
2022-03-20 09:29:50 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.5031
2022-03-20 09:30:27 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.4084
2022-03-20 09:31:05 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.3531
2022-03-20 09:31:43 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.4946
2022-03-20 09:32:20 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.7670
2022-03-20 09:32:58 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.6130
2022-03-20 09:33:36 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.6136
2022-03-20 09:34:14 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.4923
2022-03-20 09:34:51 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.6142
2022-03-20 09:35:29 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.4544
2022-03-20 09:36:07 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.7794
2022-03-20 09:36:45 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.3823
2022-03-20 09:37:22 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.5292
2022-03-20 09:38:00 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.6160
2022-03-20 09:38:38 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.6130
2022-03-20 09:39:15 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.6180
2022-03-20 09:39:53 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.4944
2022-03-20 09:40:31 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.3993
2022-03-20 09:41:08 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.6498
2022-03-20 09:41:46 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.5419
2022-03-20 09:42:24 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.4476
2022-03-20 09:43:02 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.3458
2022-03-20 09:43:39 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.4662
2022-03-20 09:44:17 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.6476
2022-03-20 09:44:55 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.5310
2022-03-20 09:45:33 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.5513
2022-03-20 09:46:11 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.3679
2022-03-20 09:46:48 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.6055
2022-03-20 09:47:26 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.3849
2022-03-20 09:47:28 - train: epoch 047, train_loss: 1.4816
2022-03-20 09:48:43 - eval: epoch: 047, acc1: 68.548%, acc5: 88.884%, test_loss: 1.2616, per_image_load_time: 1.376ms, per_image_inference_time: 0.724ms
2022-03-20 09:48:44 - until epoch: 047, best_acc1: 68.812%
2022-03-20 09:48:44 - epoch 048 lr: 0.010000000000000002
2022-03-20 09:49:28 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.6213
2022-03-20 09:50:06 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.8617
2022-03-20 09:50:44 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.4488
2022-03-20 09:51:22 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.5022
2022-03-20 09:52:00 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.3303
2022-03-20 09:52:38 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.4577
2022-03-20 09:53:16 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.3837
2022-03-20 09:53:54 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.5046
2022-03-20 09:54:32 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.7033
2022-03-20 09:55:10 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.6039
2022-03-20 09:55:47 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.4366
2022-03-20 09:56:25 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.5527
2022-03-20 09:57:03 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.3014
2022-03-20 09:57:41 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.5375
2022-03-20 09:58:19 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.5429
2022-03-20 09:58:57 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.4760
2022-03-20 09:59:34 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.6183
2022-03-20 10:00:12 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.4997
2022-03-20 10:00:50 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.5612
2022-03-20 10:01:27 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.7443
2022-03-20 10:02:05 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.5132
2022-03-20 10:02:43 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.6937
2022-03-20 10:03:21 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.2885
2022-03-20 10:03:58 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.5479
2022-03-20 10:04:36 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.4973
2022-03-20 10:05:14 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.6958
2022-03-20 10:05:51 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.7269
2022-03-20 10:06:29 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.2940
2022-03-20 10:07:07 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.5618
2022-03-20 10:07:45 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.4190
2022-03-20 10:08:23 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.4237
2022-03-20 10:09:01 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.3335
2022-03-20 10:09:38 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.5740
2022-03-20 10:10:16 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.3342
2022-03-20 10:10:54 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.7085
2022-03-20 10:11:32 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.3928
2022-03-20 10:12:10 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.5832
2022-03-20 10:12:48 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.4807
2022-03-20 10:13:25 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.6383
2022-03-20 10:14:03 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.2640
2022-03-20 10:14:41 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.6738
2022-03-20 10:15:19 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.4178
2022-03-20 10:15:57 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.4969
2022-03-20 10:16:34 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.3880
2022-03-20 10:17:12 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.5218
2022-03-20 10:17:50 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.3876
2022-03-20 10:18:28 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.5228
2022-03-20 10:19:06 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.8177
2022-03-20 10:19:43 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.5630
2022-03-20 10:20:21 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.5577
2022-03-20 10:20:23 - train: epoch 048, train_loss: 1.4811
2022-03-20 10:21:37 - eval: epoch: 048, acc1: 68.486%, acc5: 88.948%, test_loss: 1.2548, per_image_load_time: 1.045ms, per_image_inference_time: 0.734ms
2022-03-20 10:21:39 - until epoch: 048, best_acc1: 68.812%
2022-03-20 10:21:39 - epoch 049 lr: 0.010000000000000002
2022-03-20 10:22:22 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6376
2022-03-20 10:23:00 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.3344
2022-03-20 10:23:38 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.4772
2022-03-20 10:24:15 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.5295
2022-03-20 10:24:53 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.3978
2022-03-20 10:25:31 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.3780
2022-03-20 10:26:09 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.5635
2022-03-20 10:26:46 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.7179
2022-03-20 10:27:24 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.3533
2022-03-20 10:28:02 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.4598
2022-03-20 10:28:39 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.3443
2022-03-20 10:29:17 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.2452
2022-03-20 10:29:55 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.6417
2022-03-20 10:30:32 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.5290
2022-03-20 10:31:10 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.3295
2022-03-20 10:31:48 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.6633
2022-03-20 10:32:25 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.5278
2022-03-20 10:33:03 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.3377
2022-03-20 10:33:41 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.4400
2022-03-20 10:34:18 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.4016
2022-03-20 10:34:56 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.3196
2022-03-20 10:35:34 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.4668
2022-03-20 10:36:12 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.3687
2022-03-20 10:36:49 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.6104
2022-03-20 10:37:27 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.5318
2022-03-20 10:38:05 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.4335
2022-03-20 10:38:45 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.3463
2022-03-20 10:39:23 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.3997
2022-03-20 10:40:01 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.6261
2022-03-20 10:40:38 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.4148
2022-03-20 10:41:16 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.5168
2022-03-20 10:41:54 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.6762
2022-03-20 10:42:32 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.4584
2022-03-20 10:43:09 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.6608
2022-03-20 10:43:47 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.5653
2022-03-20 10:44:25 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.7298
2022-03-20 10:45:03 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.4985
2022-03-20 10:45:40 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.6138
2022-03-20 10:46:18 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.7293
2022-03-20 10:46:56 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.5870
2022-03-20 10:47:34 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.4744
2022-03-20 10:48:12 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.6393
2022-03-20 10:48:49 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.7615
2022-03-20 10:49:27 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.4953
2022-03-20 10:50:05 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.4341
2022-03-20 10:50:43 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.5785
2022-03-20 10:51:21 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.7188
2022-03-20 10:51:59 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.3495
2022-03-20 10:52:37 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.3331
2022-03-20 10:53:14 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.4193
2022-03-20 10:53:16 - train: epoch 049, train_loss: 1.4739
2022-03-20 10:54:31 - eval: epoch: 049, acc1: 68.738%, acc5: 88.756%, test_loss: 1.2596, per_image_load_time: 0.707ms, per_image_inference_time: 0.752ms
2022-03-20 10:54:33 - until epoch: 049, best_acc1: 68.812%
2022-03-20 10:54:33 - epoch 050 lr: 0.010000000000000002
2022-03-20 10:55:17 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.6000
2022-03-20 10:55:54 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.3992
2022-03-20 10:56:32 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.3776
2022-03-20 10:57:09 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.3221
2022-03-20 10:57:47 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.5050
2022-03-20 10:58:25 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.6294
2022-03-20 10:59:02 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.2867
2022-03-20 10:59:39 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.2163
2022-03-20 11:00:17 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.3139
2022-03-20 11:00:55 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.5999
2022-03-20 11:01:32 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.6009
2022-03-20 11:02:10 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.4982
2022-03-20 11:02:47 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.2934
2022-03-20 11:03:25 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.5552
2022-03-20 11:04:03 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.5024
2022-03-20 11:04:40 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.4048
2022-03-20 11:05:18 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.6007
2022-03-20 11:05:55 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.3813
2022-03-20 11:06:33 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.4422
2022-03-20 11:07:10 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.4728
2022-03-20 11:07:48 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.2276
2022-03-20 11:08:25 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.5826
2022-03-20 11:09:03 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.4400
2022-03-20 11:09:41 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.4817
2022-03-20 11:10:18 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.5737
2022-03-20 11:10:56 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.4311
2022-03-20 11:11:34 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.5131
2022-03-20 11:12:11 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.6426
2022-03-20 11:12:49 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.6683
2022-03-20 11:13:27 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.6289
2022-03-20 11:14:05 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.4183
2022-03-20 11:14:42 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.4886
2022-03-20 11:15:20 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.3943
2022-03-20 11:15:58 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.3427
2022-03-20 11:16:35 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.4054
2022-03-20 11:17:13 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.4285
2022-03-20 11:17:51 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.5593
2022-03-20 11:18:28 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.3805
2022-03-20 11:19:06 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.2885
2022-03-20 11:19:44 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.5617
2022-03-20 11:20:21 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.4365
2022-03-20 11:20:59 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.4763
2022-03-20 11:21:37 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.4429
2022-03-20 11:22:14 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.4450
2022-03-20 11:22:52 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.4491
2022-03-20 11:23:30 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.4830
2022-03-20 11:24:07 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.4355
2022-03-20 11:24:45 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.2493
2022-03-20 11:25:23 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.3045
2022-03-20 11:26:00 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.3728
2022-03-20 11:26:02 - train: epoch 050, train_loss: 1.4717
2022-03-20 11:27:18 - eval: epoch: 050, acc1: 68.418%, acc5: 88.772%, test_loss: 1.2608, per_image_load_time: 0.736ms, per_image_inference_time: 0.720ms
2022-03-20 11:27:19 - until epoch: 050, best_acc1: 68.812%
2022-03-20 11:27:19 - epoch 051 lr: 0.010000000000000002
2022-03-20 11:28:03 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.6562
2022-03-20 11:28:41 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.7621
2022-03-20 11:29:18 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.4831
2022-03-20 11:29:56 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.2477
2022-03-20 11:30:34 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.5365
2022-03-20 11:31:12 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.4327
2022-03-20 11:31:49 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.3878
2022-03-20 11:32:27 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.6465
2022-03-20 11:33:05 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.3735
2022-03-20 11:33:42 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.8055
2022-03-20 11:34:20 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.6829
2022-03-20 11:34:58 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.4199
2022-03-20 11:35:36 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.1457
2022-03-20 11:36:14 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.3800
2022-03-20 11:36:52 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.4349
2022-03-20 11:37:30 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.3876
2022-03-20 11:38:08 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.4952
2022-03-20 11:38:46 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.5061
2022-03-20 11:39:23 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.2573
2022-03-20 11:40:01 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.4632
2022-03-20 11:40:39 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.3629
2022-03-20 11:41:17 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.4936
2022-03-20 11:41:55 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.5024
2022-03-20 11:42:33 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.5490
2022-03-20 11:43:11 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.4174
2022-03-20 11:43:49 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.2910
2022-03-20 11:44:27 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.6078
2022-03-20 11:45:05 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.3203
2022-03-20 11:45:43 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.4505
2022-03-20 11:46:21 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.3860
2022-03-20 11:46:59 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.3867
2022-03-20 11:47:36 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.4584
2022-03-20 11:48:14 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.5234
2022-03-20 11:48:52 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.5037
2022-03-20 11:49:30 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.3493
2022-03-20 11:50:08 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.4116
2022-03-20 11:50:46 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.5749
2022-03-20 11:51:24 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.4677
2022-03-20 11:52:02 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.4108
2022-03-20 11:52:40 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.4819
2022-03-20 11:53:18 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.6043
2022-03-20 11:53:56 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.8265
2022-03-20 11:54:34 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.3132
2022-03-20 11:55:12 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.5713
2022-03-20 11:55:50 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.3240
2022-03-20 11:56:28 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.5575
2022-03-20 11:57:06 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.5269
2022-03-20 11:57:44 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.4906
2022-03-20 11:58:21 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.5111
2022-03-20 11:58:59 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.4850
2022-03-20 11:59:01 - train: epoch 051, train_loss: 1.4706
2022-03-20 12:00:16 - eval: epoch: 051, acc1: 68.502%, acc5: 88.814%, test_loss: 1.2564, per_image_load_time: 0.987ms, per_image_inference_time: 0.737ms
2022-03-20 12:00:18 - until epoch: 051, best_acc1: 68.812%
2022-03-20 12:00:18 - epoch 052 lr: 0.010000000000000002
2022-03-20 12:01:01 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.3237
2022-03-20 12:01:39 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.4497
2022-03-20 12:02:17 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.5136
2022-03-20 12:02:55 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.4837
2022-03-20 12:03:33 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.5127
2022-03-20 12:04:10 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.3883
2022-03-20 12:04:48 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.5285
2022-03-20 12:05:26 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.4695
2022-03-20 12:06:04 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.5717
2022-03-20 12:06:41 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.6354
2022-03-20 12:07:19 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.4656
2022-03-20 12:07:57 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.3904
2022-03-20 12:08:34 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.4531
2022-03-20 12:09:12 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.6712
2022-03-20 12:09:50 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.3368
2022-03-20 12:10:28 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.3434
2022-03-20 12:11:06 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.3041
2022-03-20 12:11:43 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.1948
2022-03-20 12:12:21 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.4846
2022-03-20 12:12:59 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.5601
2022-03-20 12:13:37 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.3590
2022-03-20 12:14:15 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.5841
2022-03-20 12:14:53 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.2278
2022-03-20 12:15:31 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.3322
2022-03-20 12:16:08 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.3340
2022-03-20 12:16:46 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.2930
2022-03-20 12:17:24 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.3818
2022-03-20 12:18:02 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.5223
2022-03-20 12:18:40 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.3860
2022-03-20 12:19:18 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.4658
2022-03-20 12:19:56 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.5344
2022-03-20 12:20:34 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.5483
2022-03-20 12:21:12 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.4688
2022-03-20 12:21:49 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.5015
2022-03-20 12:22:27 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.6567
2022-03-20 12:23:05 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.4645
2022-03-20 12:23:43 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.6440
2022-03-20 12:24:21 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.3895
2022-03-20 12:24:59 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.4093
2022-03-20 12:25:37 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.6732
2022-03-20 12:26:15 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.2786
2022-03-20 12:26:53 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.4205
2022-03-20 12:27:31 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.4499
2022-03-20 12:28:09 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.5416
2022-03-20 12:28:47 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.3075
2022-03-20 12:29:25 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.5148
2022-03-20 12:30:03 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.4437
2022-03-20 12:30:41 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.2409
2022-03-20 12:31:19 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.3517
2022-03-20 12:31:57 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.5055
2022-03-20 12:31:59 - train: epoch 052, train_loss: 1.4661
2022-03-20 12:33:14 - eval: epoch: 052, acc1: 68.670%, acc5: 89.050%, test_loss: 1.2507, per_image_load_time: 0.701ms, per_image_inference_time: 0.713ms
2022-03-20 12:33:15 - until epoch: 052, best_acc1: 68.812%
2022-03-20 12:33:15 - epoch 053 lr: 0.010000000000000002
2022-03-20 12:33:59 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.4423
2022-03-20 12:34:36 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.6372
2022-03-20 12:35:14 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.4879
2022-03-20 12:35:51 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.4297
2022-03-20 12:36:29 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.6277
2022-03-20 12:37:07 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.4275
2022-03-20 12:37:45 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.2844
2022-03-20 12:38:23 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.5726
2022-03-20 12:39:00 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.4396
2022-03-20 12:39:38 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.3849
2022-03-20 12:40:16 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.3845
2022-03-20 12:40:54 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.4224
2022-03-20 12:41:32 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.4774
2022-03-20 12:42:09 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.7214
2022-03-20 12:42:47 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.5040
2022-03-20 12:43:25 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.6930
2022-03-20 12:44:03 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.6787
2022-03-20 12:44:41 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.4875
2022-03-20 12:45:18 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.1747
2022-03-20 12:45:56 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.4879
2022-03-20 12:46:34 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.4352
2022-03-20 12:47:11 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.3739
2022-03-20 12:47:49 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.4324
2022-03-20 12:48:27 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.4400
2022-03-20 12:49:04 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.6098
2022-03-20 12:49:42 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.4882
2022-03-20 12:50:20 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.6600
2022-03-20 12:50:57 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.6670
2022-03-20 12:51:35 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.3497
2022-03-20 12:52:13 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.3574
2022-03-20 12:52:50 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.5985
2022-03-20 12:53:28 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.7650
2022-03-20 12:54:06 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.3858
2022-03-20 12:54:43 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.6108
2022-03-20 12:55:21 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.6119
2022-03-20 12:55:59 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.5021
2022-03-20 12:56:36 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.6170
2022-03-20 12:57:14 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.5366
2022-03-20 12:57:52 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.5731
2022-03-20 12:58:30 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.4526
2022-03-20 12:59:07 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.5800
2022-03-20 12:59:45 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.4505
2022-03-20 13:00:23 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.6024
2022-03-20 13:01:00 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.3830
2022-03-20 13:01:38 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.5500
2022-03-20 13:02:16 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.4795
2022-03-20 13:02:54 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.6637
2022-03-20 13:03:32 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.6081
2022-03-20 13:04:09 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.3967
2022-03-20 13:04:47 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.4878
2022-03-20 13:04:49 - train: epoch 053, train_loss: 1.4615
2022-03-20 13:06:04 - eval: epoch: 053, acc1: 68.596%, acc5: 88.970%, test_loss: 1.2571, per_image_load_time: 1.058ms, per_image_inference_time: 0.722ms
2022-03-20 13:06:06 - until epoch: 053, best_acc1: 68.812%
2022-03-20 13:06:06 - epoch 054 lr: 0.010000000000000002
2022-03-20 13:06:50 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.2102
2022-03-20 13:07:27 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.7358
2022-03-20 13:08:05 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.5062
2022-03-20 13:08:42 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.2465
2022-03-20 13:09:20 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.5656
2022-03-20 13:09:58 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.3355
2022-03-20 13:10:35 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.5934
2022-03-20 13:11:13 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.5943
2022-03-20 13:11:51 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.2420
2022-03-20 13:12:29 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.2880
2022-03-20 13:13:06 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.2706
2022-03-20 13:13:44 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.6858
2022-03-20 13:14:22 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.4052
2022-03-20 13:15:00 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.4399
2022-03-20 13:15:37 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.4518
2022-03-20 13:16:15 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.1161
2022-03-20 13:16:53 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.4129
2022-03-20 13:17:31 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.4539
2022-03-20 13:18:09 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.8420
2022-03-20 13:18:47 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.5158
2022-03-20 13:19:24 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.2799
2022-03-20 13:20:02 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.4161
2022-03-20 13:20:40 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.3474
2022-03-20 13:21:18 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.3028
2022-03-20 13:21:56 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.5882
2022-03-20 13:22:33 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.4277
2022-03-20 13:23:11 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.5103
2022-03-20 13:23:49 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.7312
2022-03-20 13:24:27 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.3187
2022-03-20 13:25:05 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.4599
2022-03-20 13:25:43 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.5083
2022-03-20 13:26:21 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.6258
2022-03-20 13:26:59 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.4339
2022-03-20 13:27:37 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.5667
2022-03-20 13:28:15 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.5352
2022-03-20 13:28:53 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.3745
2022-03-20 13:29:31 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.3643
2022-03-20 13:30:09 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.5308
2022-03-20 13:30:47 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.4137
2022-03-20 13:31:25 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.3551
2022-03-20 13:32:03 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.5503
2022-03-20 13:32:41 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.4631
2022-03-20 13:33:19 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.5324
2022-03-20 13:33:57 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.3085
2022-03-20 13:34:34 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.3402
2022-03-20 13:35:12 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.4061
2022-03-20 13:35:50 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.5895
2022-03-20 13:36:28 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.6281
2022-03-20 13:37:05 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.3310
2022-03-20 13:37:43 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.5664
2022-03-20 13:37:45 - train: epoch 054, train_loss: 1.4614
2022-03-20 13:39:01 - eval: epoch: 054, acc1: 68.590%, acc5: 88.814%, test_loss: 1.2582, per_image_load_time: 1.925ms, per_image_inference_time: 0.720ms
2022-03-20 13:39:02 - until epoch: 054, best_acc1: 68.812%
2022-03-20 13:39:02 - epoch 055 lr: 0.010000000000000002
2022-03-20 13:39:46 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.4635
2022-03-20 13:40:23 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.3004
2022-03-20 13:41:01 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.2874
2022-03-20 13:41:39 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.4603
2022-03-20 13:42:17 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.3127
2022-03-20 13:42:54 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.3197
2022-03-20 13:43:32 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.6055
2022-03-20 13:44:10 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.2797
2022-03-20 13:44:48 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.4852
2022-03-20 13:45:25 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.4644
2022-03-20 13:46:03 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.4450
2022-03-20 13:46:41 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.4368
2022-03-20 13:47:19 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.6170
2022-03-20 13:47:57 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.3769
2022-03-20 13:48:34 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.5498
2022-03-20 13:49:12 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.6117
2022-03-20 13:49:50 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.5673
2022-03-20 13:50:28 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.5874
2022-03-20 13:51:06 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.5044
2022-03-20 13:51:43 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.4066
2022-03-20 13:52:21 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.3071
2022-03-20 13:52:59 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.5985
2022-03-20 13:53:37 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.4162
2022-03-20 13:54:15 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.3498
2022-03-20 13:54:53 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.5289
2022-03-20 13:55:31 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.4076
2022-03-20 13:56:08 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.3198
2022-03-20 13:56:46 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.4474
2022-03-20 13:57:24 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.5587
2022-03-20 13:58:02 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.3500
2022-03-20 13:58:40 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.4998
2022-03-20 13:59:18 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.3799
2022-03-20 13:59:56 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.2505
2022-03-20 14:00:34 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.3826
2022-03-20 14:01:12 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.2221
2022-03-20 14:01:49 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.4996
2022-03-20 14:02:27 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.5611
2022-03-20 14:03:05 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.5439
2022-03-20 14:03:43 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.7478
2022-03-20 14:04:22 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.5564
2022-03-20 14:05:00 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.5245
2022-03-20 14:05:38 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.4419
2022-03-20 14:06:16 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.5005
2022-03-20 14:06:54 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.6536
2022-03-20 14:07:32 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.5086
2022-03-20 14:08:10 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.5014
2022-03-20 14:08:48 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.2130
2022-03-20 14:09:26 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.5188
2022-03-20 14:10:04 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.3871
2022-03-20 14:10:41 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.4818
2022-03-20 14:10:43 - train: epoch 055, train_loss: 1.4544
2022-03-20 14:11:59 - eval: epoch: 055, acc1: 68.676%, acc5: 88.970%, test_loss: 1.2479, per_image_load_time: 1.991ms, per_image_inference_time: 0.716ms
2022-03-20 14:12:00 - until epoch: 055, best_acc1: 68.812%
2022-03-20 14:12:00 - epoch 056 lr: 0.010000000000000002
2022-03-20 14:12:44 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.4836
2022-03-20 14:13:22 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.6753
2022-03-20 14:13:59 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.4365
2022-03-20 14:14:37 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.3673
2022-03-20 14:15:14 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.3647
2022-03-20 14:15:52 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.4984
2022-03-20 14:16:30 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.5678
2022-03-20 14:17:07 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.6378
2022-03-20 14:17:45 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.5649
2022-03-20 14:18:22 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.4905
2022-03-20 14:19:00 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.3218
2022-03-20 14:19:38 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.3352
2022-03-20 14:20:16 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.5380
2022-03-20 14:20:53 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.3984
2022-03-20 14:21:31 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.7218
2022-03-20 14:22:09 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.3326
2022-03-20 14:22:46 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.5594
2022-03-20 14:23:24 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.7568
2022-03-20 14:24:01 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.4935
2022-03-20 14:24:39 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.4688
2022-03-20 14:25:17 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.4669
2022-03-20 14:25:54 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.5790
2022-03-20 14:26:32 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.6355
2022-03-20 14:27:09 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.5266
2022-03-20 14:27:47 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.6231
2022-03-20 14:28:24 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.3329
2022-03-20 14:29:02 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.4512
2022-03-20 14:29:39 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.3833
2022-03-20 14:30:17 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.6216
2022-03-20 14:30:54 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.5643
2022-03-20 14:31:32 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.3476
2022-03-20 14:32:10 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.3479
2022-03-20 14:32:47 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.7136
2022-03-20 14:33:25 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.3263
2022-03-20 14:34:03 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.3839
2022-03-20 14:34:41 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.2508
2022-03-20 14:35:19 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.4851
2022-03-20 14:35:57 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.3099
2022-03-20 14:36:35 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.6176
2022-03-20 14:37:13 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.4391
2022-03-20 14:37:51 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.6968
2022-03-20 14:38:29 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.4584
2022-03-20 14:39:07 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.3981
2022-03-20 14:39:45 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.6214
2022-03-20 14:40:23 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.4137
2022-03-20 14:41:01 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.4446
2022-03-20 14:41:39 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.5550
2022-03-20 14:42:17 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.5858
2022-03-20 14:42:54 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.4609
2022-03-20 14:43:32 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.5089
2022-03-20 14:43:34 - train: epoch 056, train_loss: 1.4505
2022-03-20 14:44:50 - eval: epoch: 056, acc1: 69.096%, acc5: 89.090%, test_loss: 1.2432, per_image_load_time: 1.139ms, per_image_inference_time: 0.718ms
2022-03-20 14:44:52 - until epoch: 056, best_acc1: 69.096%
2022-03-20 14:44:52 - epoch 057 lr: 0.010000000000000002
2022-03-20 14:45:35 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.5688
2022-03-20 14:46:12 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.3307
2022-03-20 14:46:49 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.2562
2022-03-20 14:47:27 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.4085
2022-03-20 14:48:04 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.2027
2022-03-20 14:48:41 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.6775
2022-03-20 14:49:19 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.1644
2022-03-20 14:49:56 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.4626
2022-03-20 14:50:34 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.5271
2022-03-20 14:51:11 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.3500
2022-03-20 14:51:49 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.3781
2022-03-20 14:52:26 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.3016
2022-03-20 14:53:03 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.4885
2022-03-20 14:53:41 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.4791
2022-03-20 14:54:18 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.4876
2022-03-20 14:54:56 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.5996
2022-03-20 14:55:33 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.6061
2022-03-20 14:56:11 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.5811
2022-03-20 14:56:48 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.3719
2022-03-20 14:57:26 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.4935
2022-03-20 14:58:03 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.5052
2022-03-20 14:58:41 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.5674
2022-03-20 14:59:19 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.4248
2022-03-20 14:59:56 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.3388
2022-03-20 15:00:34 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.3977
2022-03-20 15:01:11 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.2593
2022-03-20 15:01:49 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.2437
2022-03-20 15:02:27 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.1699
2022-03-20 15:03:04 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.5514
2022-03-20 15:03:42 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.5801
2022-03-20 15:04:19 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.6470
2022-03-20 15:04:57 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.5276
2022-03-20 15:05:35 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.5440
2022-03-20 15:06:12 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.5904
2022-03-20 15:06:50 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.4570
2022-03-20 15:07:27 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.4374
2022-03-20 15:08:05 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.3497
2022-03-20 15:08:43 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.3472
2022-03-20 15:09:20 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.5502
2022-03-20 15:09:58 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.3641
2022-03-20 15:10:36 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.5382
2022-03-20 15:11:13 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.5648
2022-03-20 15:11:51 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.4281
2022-03-20 15:12:29 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.4119
2022-03-20 15:13:06 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.6071
2022-03-20 15:13:44 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.5876
2022-03-20 15:14:21 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.4146
2022-03-20 15:14:59 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.6145
2022-03-20 15:15:36 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.5806
2022-03-20 15:16:14 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.6199
2022-03-20 15:16:16 - train: epoch 057, train_loss: 1.4489
2022-03-20 15:17:31 - eval: epoch: 057, acc1: 68.874%, acc5: 88.998%, test_loss: 1.2467, per_image_load_time: 0.941ms, per_image_inference_time: 0.732ms
2022-03-20 15:17:33 - until epoch: 057, best_acc1: 69.096%
2022-03-20 15:17:33 - epoch 058 lr: 0.010000000000000002
2022-03-20 15:18:16 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.4631
2022-03-20 15:18:53 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.2119
2022-03-20 15:19:30 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.5957
2022-03-20 15:20:08 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.5215
2022-03-20 15:20:45 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.2632
2022-03-20 15:21:22 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.5103
2022-03-20 15:21:59 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.4558
2022-03-20 15:22:37 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.3529
2022-03-20 15:23:14 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.3103
2022-03-20 15:23:52 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.5616
2022-03-20 15:24:29 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.2300
2022-03-20 15:25:06 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.3414
2022-03-20 15:25:44 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.4825
2022-03-20 15:26:21 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.4599
2022-03-20 15:26:59 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.3908
2022-03-20 15:27:36 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.3607
2022-03-20 15:28:14 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.5894
2022-03-20 15:28:51 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.3954
2022-03-20 15:29:29 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.5713
2022-03-20 15:30:07 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.4711
2022-03-20 15:30:44 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.3275
2022-03-20 15:31:22 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.2347
2022-03-20 15:32:00 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.3341
2022-03-20 15:32:38 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.4978
2022-03-20 15:33:16 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.5432
2022-03-20 15:33:54 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.3983
2022-03-20 15:34:32 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.6158
2022-03-20 15:35:10 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.2559
2022-03-20 15:35:48 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.4106
2022-03-20 15:36:26 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.5978
2022-03-20 15:37:04 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.2012
2022-03-20 15:37:42 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.2535
2022-03-20 15:38:20 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.5084
2022-03-20 15:38:58 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.3806
2022-03-20 15:39:36 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.4025
2022-03-20 15:40:14 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.3903
2022-03-20 15:40:52 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.3357
2022-03-20 15:41:30 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.4717
2022-03-20 15:42:08 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.5269
2022-03-20 15:42:46 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.5537
2022-03-20 15:43:24 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.6301
2022-03-20 15:44:02 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.2241
2022-03-20 15:44:40 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.6749
2022-03-20 15:45:18 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.2893
2022-03-20 15:45:56 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.5585
2022-03-20 15:46:34 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.3227
2022-03-20 15:47:12 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.4961
2022-03-20 15:47:50 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.5456
2022-03-20 15:48:28 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.4104
2022-03-20 15:49:06 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.3793
2022-03-20 15:49:08 - train: epoch 058, train_loss: 1.4433
2022-03-20 15:50:23 - eval: epoch: 058, acc1: 69.024%, acc5: 89.036%, test_loss: 1.2378, per_image_load_time: 0.960ms, per_image_inference_time: 0.709ms
2022-03-20 15:50:23 - until epoch: 058, best_acc1: 69.096%
2022-03-20 15:50:23 - epoch 059 lr: 0.010000000000000002
2022-03-20 15:51:07 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.5280
2022-03-20 15:51:45 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.4145
2022-03-20 15:52:22 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.4917
2022-03-20 15:53:00 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.4057
2022-03-20 15:53:38 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.6044
2022-03-20 15:54:16 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.4116
2022-03-20 15:54:53 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.3883
2022-03-20 15:55:31 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.4975
2022-03-20 15:56:09 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.3971
2022-03-20 15:56:46 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.3762
2022-03-20 15:57:24 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.6201
2022-03-20 15:58:02 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.2557
2022-03-20 15:58:39 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.5265
2022-03-20 15:59:17 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.5627
2022-03-20 15:59:54 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.6535
2022-03-20 16:00:32 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.3549
2022-03-20 16:01:10 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.6143
2022-03-20 16:01:48 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.4482
2022-03-20 16:02:25 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.4563
2022-03-20 16:03:03 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.3789
2022-03-20 16:03:41 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.6770
2022-03-20 16:04:19 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.6264
2022-03-20 16:04:56 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.3879
2022-03-20 16:05:34 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.5292
2022-03-20 16:06:12 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.4727
2022-03-20 16:06:50 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.4355
2022-03-20 16:07:27 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.3325
2022-03-20 16:08:05 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.6216
2022-03-20 16:08:43 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.3814
2022-03-20 16:09:21 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.9290
2022-03-20 16:09:59 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.3111
2022-03-20 16:10:37 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.3951
2022-03-20 16:11:15 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.4655
2022-03-20 16:11:53 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.7413
2022-03-20 16:12:30 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.3087
2022-03-20 16:13:08 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.5487
2022-03-20 16:13:46 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.2538
2022-03-20 16:14:24 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.4231
2022-03-20 16:15:02 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.3119
2022-03-20 16:15:40 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.6241
2022-03-20 16:16:18 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.4916
2022-03-20 16:16:56 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.4460
2022-03-20 16:17:33 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.6485
2022-03-20 16:18:12 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.5668
2022-03-20 16:18:50 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.5302
2022-03-20 16:19:28 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.4293
2022-03-20 16:20:06 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.3722
2022-03-20 16:20:44 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.4223
2022-03-20 16:21:22 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.6554
2022-03-20 16:22:00 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.5046
2022-03-20 16:22:02 - train: epoch 059, train_loss: 1.4405
2022-03-20 16:23:17 - eval: epoch: 059, acc1: 68.902%, acc5: 89.138%, test_loss: 1.2406, per_image_load_time: 0.974ms, per_image_inference_time: 0.706ms
2022-03-20 16:23:18 - until epoch: 059, best_acc1: 69.096%
2022-03-20 16:23:18 - epoch 060 lr: 0.010000000000000002
2022-03-20 16:24:02 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.3652
2022-03-20 16:24:39 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.4742
2022-03-20 16:25:17 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.3718
2022-03-20 16:25:54 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.4622
2022-03-20 16:26:32 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.5772
2022-03-20 16:27:09 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.5240
2022-03-20 16:27:47 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.4567
2022-03-20 16:28:25 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.6219
2022-03-20 16:29:02 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.2096
2022-03-20 16:29:40 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.1127
2022-03-20 16:30:18 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.2867
2022-03-20 16:30:56 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.3278
2022-03-20 16:31:34 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.2892
2022-03-20 16:32:11 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.4495
2022-03-20 16:32:49 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.4766
2022-03-20 16:33:27 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.4284
2022-03-20 16:34:05 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.4645
2022-03-20 16:34:43 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.4359
2022-03-20 16:35:21 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.6224
2022-03-20 16:35:59 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.3117
2022-03-20 16:36:37 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.4920
2022-03-20 16:37:15 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.5330
2022-03-20 16:37:53 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.2731
2022-03-20 16:38:31 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.4410
2022-03-20 16:39:09 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.4874
2022-03-20 16:39:47 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.5245
2022-03-20 16:40:25 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.3627
2022-03-20 16:41:03 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.3821
2022-03-20 16:41:41 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.4539
2022-03-20 16:42:20 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.7187
2022-03-20 16:42:58 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.5630
2022-03-20 16:43:36 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.3424
2022-03-20 16:44:14 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.3129
2022-03-20 16:44:53 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.5200
2022-03-20 16:45:31 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.4205
2022-03-20 16:46:09 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.4317
2022-03-20 16:46:47 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.4407
2022-03-20 16:47:26 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.5306
2022-03-20 16:48:04 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.7394
2022-03-20 16:48:42 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.3833
2022-03-20 16:49:20 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.6995
2022-03-20 16:49:58 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.5726
2022-03-20 16:50:36 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.4308
2022-03-20 16:51:15 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.5550
2022-03-20 16:51:53 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.4325
2022-03-20 16:52:31 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.3753
2022-03-20 16:53:09 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.4645
2022-03-20 16:53:47 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.3180
2022-03-20 16:54:25 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.3995
2022-03-20 16:55:03 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.5151
2022-03-20 16:55:05 - train: epoch 060, train_loss: 1.4373
2022-03-20 16:56:20 - eval: epoch: 060, acc1: 68.878%, acc5: 89.124%, test_loss: 1.2408, per_image_load_time: 1.982ms, per_image_inference_time: 0.717ms
2022-03-20 16:56:21 - until epoch: 060, best_acc1: 69.096%
2022-03-20 16:56:21 - epoch 061 lr: 0.0010000000000000002
2022-03-20 16:57:05 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.1970
2022-03-20 16:57:42 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.3481
2022-03-20 16:58:20 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.1302
2022-03-20 16:58:57 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.5183
2022-03-20 16:59:35 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.3081
2022-03-20 17:00:12 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.3564
2022-03-20 17:00:50 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.1494
2022-03-20 17:01:27 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.3309
2022-03-20 17:02:05 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.1741
2022-03-20 17:02:43 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.1283
2022-03-20 17:03:21 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.1473
2022-03-20 17:03:58 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.1291
2022-03-20 17:04:36 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.1199
2022-03-20 17:05:13 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.1618
2022-03-20 17:05:51 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.2650
2022-03-20 17:06:29 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.0795
2022-03-20 17:07:07 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.2614
2022-03-20 17:07:45 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.2580
2022-03-20 17:08:23 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.3177
2022-03-20 17:09:01 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.1292
2022-03-20 17:09:39 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.6803
2022-03-20 17:10:17 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.1165
2022-03-20 17:10:55 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.2467
2022-03-20 17:11:33 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.0271
2022-03-20 17:12:11 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.1932
2022-03-20 17:12:49 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.3808
2022-03-20 17:13:27 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.4551
2022-03-20 17:14:05 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.1965
2022-03-20 17:14:43 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.4007
2022-03-20 17:15:21 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.1038
2022-03-20 17:15:59 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.3926
2022-03-20 17:16:37 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.2669
2022-03-20 17:17:15 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.1800
2022-03-20 17:17:53 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.1140
2022-03-20 17:18:31 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.2644
2022-03-20 17:19:10 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.2477
2022-03-20 17:19:48 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.3856
2022-03-20 17:20:26 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.2917
2022-03-20 17:21:04 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.1598
2022-03-20 17:21:42 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.2142
2022-03-20 17:22:20 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.3952
2022-03-20 17:22:58 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.1328
2022-03-20 17:23:36 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.3131
2022-03-20 17:24:14 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.2218
2022-03-20 17:24:52 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.1594
2022-03-20 17:25:30 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.3618
2022-03-20 17:26:08 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.2081
2022-03-20 17:26:47 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.1311
2022-03-20 17:27:25 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.3401
2022-03-20 17:28:03 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.2197
2022-03-20 17:28:05 - train: epoch 061, train_loss: 1.2441
2022-03-20 17:29:20 - eval: epoch: 061, acc1: 72.690%, acc5: 91.020%, test_loss: 1.0791, per_image_load_time: 1.225ms, per_image_inference_time: 0.717ms
2022-03-20 17:29:21 - until epoch: 061, best_acc1: 72.690%
2022-03-20 17:29:21 - epoch 062 lr: 0.0010000000000000002
2022-03-20 17:30:04 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.2443
2022-03-20 17:30:42 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.2402
2022-03-20 17:31:19 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.2298
2022-03-20 17:31:57 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.0697
2022-03-20 17:32:34 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.1564
2022-03-20 17:33:12 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.1461
2022-03-20 17:33:49 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.4498
2022-03-20 17:34:27 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.2463
2022-03-20 17:35:04 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.1521
2022-03-20 17:35:42 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.4081
2022-03-20 17:36:20 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.0901
2022-03-20 17:36:58 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.4367
2022-03-20 17:37:36 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.2103
2022-03-20 17:38:14 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.1466
2022-03-20 17:38:52 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.2896
2022-03-20 17:39:30 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.3274
2022-03-20 17:40:08 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.3079
2022-03-20 17:40:45 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.1415
2022-03-20 17:41:23 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.2791
2022-03-20 17:42:01 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.2125
2022-03-20 17:42:39 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.2645
2022-03-20 17:43:16 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.1679
2022-03-20 17:43:54 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.1791
2022-03-20 17:44:32 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.1372
2022-03-20 17:45:10 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.2065
2022-03-20 17:45:48 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.0864
2022-03-20 17:46:26 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.1439
2022-03-20 17:47:04 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.3136
2022-03-20 17:47:41 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.2560
2022-03-20 17:48:19 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.3248
2022-03-20 17:48:57 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.2839
2022-03-20 17:49:35 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.0054
2022-03-20 17:50:12 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.3280
2022-03-20 17:50:50 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.3023
2022-03-20 17:51:28 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.1550
2022-03-20 17:52:06 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.2232
2022-03-20 17:52:44 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.1993
2022-03-20 17:53:22 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.1383
2022-03-20 17:54:00 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.1366
2022-03-20 17:54:37 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.0139
2022-03-20 17:55:15 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.2375
2022-03-20 17:55:53 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 0.9899
2022-03-20 17:56:31 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.3114
2022-03-20 17:57:08 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.1711
2022-03-20 17:57:46 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.0668
2022-03-20 17:58:24 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.9231
2022-03-20 17:59:02 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.1503
2022-03-20 17:59:40 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.1269
2022-03-20 18:00:17 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.1070
2022-03-20 18:00:55 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.2330
2022-03-20 18:00:57 - train: epoch 062, train_loss: 1.1945
2022-03-20 18:02:12 - eval: epoch: 062, acc1: 72.984%, acc5: 91.270%, test_loss: 1.0643, per_image_load_time: 1.920ms, per_image_inference_time: 0.718ms
2022-03-20 18:02:14 - until epoch: 062, best_acc1: 72.984%
2022-03-20 18:02:14 - epoch 063 lr: 0.0010000000000000002
2022-03-20 18:02:57 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.1346
2022-03-20 18:03:34 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.1272
2022-03-20 18:04:12 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.2952
2022-03-20 18:04:49 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.1948
2022-03-20 18:05:27 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.0764
2022-03-20 18:06:04 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.2000
2022-03-20 18:06:42 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.1314
2022-03-20 18:07:19 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.2323
2022-03-20 18:07:57 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.3041
2022-03-20 18:08:34 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.2273
2022-03-20 18:09:12 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.0404
2022-03-20 18:09:49 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.0708
2022-03-20 18:10:27 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.2706
2022-03-20 18:11:05 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.4890
2022-03-20 18:11:42 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.1848
2022-03-20 18:12:20 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.0821
2022-03-20 18:12:57 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.0533
2022-03-20 18:13:35 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.2500
2022-03-20 18:14:13 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.3344
2022-03-20 18:14:50 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.1628
2022-03-20 18:15:28 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.1396
2022-03-20 18:16:06 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.4425
2022-03-20 18:16:43 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.2582
2022-03-20 18:17:21 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.3560
2022-03-20 18:17:59 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.2046
2022-03-20 18:18:36 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.0792
2022-03-20 18:19:14 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.3659
2022-03-20 18:19:52 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.2080
2022-03-20 18:20:30 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.1998
2022-03-20 18:21:08 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.1985
2022-03-20 18:21:46 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.5008
2022-03-20 18:22:24 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.3350
2022-03-20 18:23:02 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.1455
2022-03-20 18:23:40 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 0.9573
2022-03-20 18:24:17 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.2932
2022-03-20 18:24:55 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.0782
2022-03-20 18:25:33 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.4140
2022-03-20 18:26:11 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.4214
2022-03-20 18:26:49 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.8732
2022-03-20 18:27:27 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.8731
2022-03-20 18:28:05 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.3326
2022-03-20 18:28:43 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.1594
2022-03-20 18:29:22 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.3256
2022-03-20 18:30:00 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.1988
2022-03-20 18:30:38 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.0733
2022-03-20 18:31:16 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.1034
2022-03-20 18:31:54 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.0220
2022-03-20 18:32:32 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.2267
2022-03-20 18:33:10 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.1657
2022-03-20 18:33:48 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.1654
2022-03-20 18:33:50 - train: epoch 063, train_loss: 1.1769
2022-03-20 18:35:05 - eval: epoch: 063, acc1: 73.196%, acc5: 91.414%, test_loss: 1.0572, per_image_load_time: 1.286ms, per_image_inference_time: 0.718ms
2022-03-20 18:35:07 - until epoch: 063, best_acc1: 73.196%
2022-03-20 18:35:07 - epoch 064 lr: 0.0010000000000000002
2022-03-20 18:35:51 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.0968
2022-03-20 18:36:28 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.1622
2022-03-20 18:37:05 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 0.9733
2022-03-20 18:37:43 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.2066
2022-03-20 18:38:20 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.0472
2022-03-20 18:38:58 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.3362
2022-03-20 18:39:35 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.2541
2022-03-20 18:40:13 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 0.9973
2022-03-20 18:40:50 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.1633
2022-03-20 18:41:27 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.1647
2022-03-20 18:42:05 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.0871
2022-03-20 18:42:43 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.0403
2022-03-20 18:43:20 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.1880
2022-03-20 18:43:58 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.3980
2022-03-20 18:44:35 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.2790
2022-03-20 18:45:13 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.0939
2022-03-20 18:45:50 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.0173
2022-03-20 18:46:28 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.0616
2022-03-20 18:47:06 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.0860
2022-03-20 18:47:43 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.0429
2022-03-20 18:48:21 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.1820
2022-03-20 18:48:58 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.2316
2022-03-20 18:49:36 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.2183
2022-03-20 18:50:14 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.0866
2022-03-20 18:50:51 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 0.9696
2022-03-20 18:51:29 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.0164
2022-03-20 18:52:07 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.1991
2022-03-20 18:52:45 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.0133
2022-03-20 18:53:23 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.3496
2022-03-20 18:54:00 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.1684
2022-03-20 18:54:38 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.0791
2022-03-20 18:55:16 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.1411
2022-03-20 18:55:54 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.1846
2022-03-20 18:56:32 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.2578
2022-03-20 18:57:10 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.0858
2022-03-20 18:57:47 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.1061
2022-03-20 18:58:25 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.9317
2022-03-20 18:59:03 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.2857
2022-03-20 18:59:41 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.0057
2022-03-20 19:00:19 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.0106
2022-03-20 19:00:57 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.2674
2022-03-20 19:01:35 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.0747
2022-03-20 19:02:13 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.4200
2022-03-20 19:02:51 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.1369
2022-03-20 19:03:29 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.1018
2022-03-20 19:04:06 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.3651
2022-03-20 19:04:44 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.5273
2022-03-20 19:05:22 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.1673
2022-03-20 19:06:00 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.3273
2022-03-20 19:06:37 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.0268
2022-03-20 19:06:39 - train: epoch 064, train_loss: 1.1643
2022-03-20 19:07:54 - eval: epoch: 064, acc1: 73.320%, acc5: 91.416%, test_loss: 1.0514, per_image_load_time: 1.964ms, per_image_inference_time: 0.713ms
2022-03-20 19:07:56 - until epoch: 064, best_acc1: 73.320%
2022-03-20 19:07:56 - epoch 065 lr: 0.0010000000000000002
2022-03-20 19:08:39 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.2959
2022-03-20 19:09:16 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.1600
2022-03-20 19:09:54 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.2265
2022-03-20 19:10:31 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.2367
2022-03-20 19:11:09 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.1225
2022-03-20 19:11:47 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.2199
2022-03-20 19:12:24 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.1757
2022-03-20 19:13:02 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.0733
2022-03-20 19:13:40 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.1026
2022-03-20 19:14:17 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.0908
2022-03-20 19:14:55 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.0834
2022-03-20 19:15:33 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.3495
2022-03-20 19:16:11 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.1225
2022-03-20 19:16:48 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 0.9086
2022-03-20 19:17:26 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.1332
2022-03-20 19:18:04 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.1287
2022-03-20 19:18:42 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.0781
2022-03-20 19:19:20 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.0467
2022-03-20 19:19:58 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.9318
2022-03-20 19:20:35 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.1137
2022-03-20 19:21:13 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.0911
2022-03-20 19:21:51 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.3364
2022-03-20 19:22:29 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.0141
2022-03-20 19:23:07 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.1027
2022-03-20 19:23:45 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.1461
2022-03-20 19:24:23 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.3059
2022-03-20 19:25:00 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.2559
2022-03-20 19:25:38 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.0190
2022-03-20 19:26:16 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.1979
2022-03-20 19:26:54 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.0221
2022-03-20 19:27:32 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.1873
2022-03-20 19:28:10 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.2138
2022-03-20 19:28:48 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.0587
2022-03-20 19:29:26 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.0417
2022-03-20 19:30:04 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.3446
2022-03-20 19:30:42 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.1677
2022-03-20 19:31:20 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.0482
2022-03-20 19:31:58 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 0.9445
2022-03-20 19:32:36 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.1479
2022-03-20 19:33:14 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.2288
2022-03-20 19:33:52 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.1639
2022-03-20 19:34:30 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.2368
2022-03-20 19:35:08 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.0717
2022-03-20 19:35:46 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.1461
2022-03-20 19:36:24 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.0854
2022-03-20 19:37:02 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.1784
2022-03-20 19:37:40 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.1800
2022-03-20 19:38:18 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.9252
2022-03-20 19:38:56 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.0339
2022-03-20 19:39:34 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.1754
2022-03-20 19:39:36 - train: epoch 065, train_loss: 1.1543
2022-03-20 19:40:50 - eval: epoch: 065, acc1: 73.430%, acc5: 91.444%, test_loss: 1.0492, per_image_load_time: 1.258ms, per_image_inference_time: 0.706ms
2022-03-20 19:40:52 - until epoch: 065, best_acc1: 73.430%
2022-03-20 19:40:52 - epoch 066 lr: 0.0010000000000000002
2022-03-20 19:41:35 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.0846
2022-03-20 19:42:13 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.3085
2022-03-20 19:42:50 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 0.9001
2022-03-20 19:43:28 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.9090
2022-03-20 19:44:05 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.3059
2022-03-20 19:44:43 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.1457
2022-03-20 19:45:21 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.0704
2022-03-20 19:45:58 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.4037
2022-03-20 19:46:36 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.2343
2022-03-20 19:47:13 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.2268
2022-03-20 19:47:51 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.1584
2022-03-20 19:48:28 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.3183
2022-03-20 19:49:06 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.3683
2022-03-20 19:49:43 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 0.9651
2022-03-20 19:50:21 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.1447
2022-03-20 19:50:58 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.1916
2022-03-20 19:51:36 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.0741
2022-03-20 19:52:13 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.0136
2022-03-20 19:52:51 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.3591
2022-03-20 19:53:28 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.2631
2022-03-20 19:54:06 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.1819
2022-03-20 19:54:44 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.1736
2022-03-20 19:55:21 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.3195
2022-03-20 19:55:59 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.0077
2022-03-20 19:56:37 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.0953
2022-03-20 19:57:14 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.1292
2022-03-20 19:57:52 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.3054
2022-03-20 19:58:30 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.3108
2022-03-20 19:59:07 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.0457
2022-03-20 19:59:45 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.1292
2022-03-20 20:00:23 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.2604
2022-03-20 20:01:00 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.0324
2022-03-20 20:01:38 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.0260
2022-03-20 20:02:16 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.3340
2022-03-20 20:02:54 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.2162
2022-03-20 20:03:32 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.2341
2022-03-20 20:04:09 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.1813
2022-03-20 20:04:47 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.0017
2022-03-20 20:05:25 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.0357
2022-03-20 20:06:04 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.3152
2022-03-20 20:06:42 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 0.9983
2022-03-20 20:07:20 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 0.9318
2022-03-20 20:07:58 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 0.9483
2022-03-20 20:08:36 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.1473
2022-03-20 20:09:14 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.2347
2022-03-20 20:09:52 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.2856
2022-03-20 20:10:30 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.1112
2022-03-20 20:11:08 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.0433
2022-03-20 20:11:47 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.1261
2022-03-20 20:12:25 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.1036
2022-03-20 20:12:27 - train: epoch 066, train_loss: 1.1470
2022-03-20 20:13:41 - eval: epoch: 066, acc1: 73.516%, acc5: 91.546%, test_loss: 1.0446, per_image_load_time: 2.168ms, per_image_inference_time: 0.718ms
2022-03-20 20:13:43 - until epoch: 066, best_acc1: 73.516%
2022-03-20 20:13:43 - epoch 067 lr: 0.0010000000000000002
2022-03-20 20:14:27 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.0692
2022-03-20 20:15:04 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.1258
2022-03-20 20:15:41 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.3484
2022-03-20 20:16:19 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.2047
2022-03-20 20:16:56 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.0158
2022-03-20 20:17:34 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.0451
2022-03-20 20:18:11 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.1719
2022-03-20 20:18:48 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.1897
2022-03-20 20:19:26 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.1988
2022-03-20 20:20:03 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 0.9808
2022-03-20 20:20:41 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.1799
2022-03-20 20:21:19 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.1462
2022-03-20 20:21:56 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.3085
2022-03-20 20:22:34 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.0673
2022-03-20 20:23:12 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.2198
2022-03-20 20:23:49 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.2169
2022-03-20 20:24:27 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.0209
2022-03-20 20:25:04 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.3297
2022-03-20 20:25:42 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.1389
2022-03-20 20:26:20 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.3647
2022-03-20 20:26:57 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.9812
2022-03-20 20:27:35 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.1744
2022-03-20 20:28:13 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.1676
2022-03-20 20:28:50 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.0654
2022-03-20 20:29:28 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.1079
2022-03-20 20:30:06 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.0039
2022-03-20 20:30:44 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.0252
2022-03-20 20:31:22 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.1740
2022-03-20 20:32:00 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.0927
2022-03-20 20:32:38 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.1944
2022-03-20 20:33:16 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.9776
2022-03-20 20:33:54 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.3231
2022-03-20 20:34:31 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.0404
2022-03-20 20:35:09 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.1436
2022-03-20 20:35:47 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.0862
2022-03-20 20:36:25 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.0927
2022-03-20 20:37:03 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.3317
2022-03-20 20:37:41 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.1658
2022-03-20 20:38:19 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.1219
2022-03-20 20:38:56 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.2610
2022-03-20 20:39:34 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.3311
2022-03-20 20:40:12 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.3696
2022-03-20 20:40:50 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.0003
2022-03-20 20:41:28 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.2034
2022-03-20 20:42:06 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 0.9758
2022-03-20 20:42:44 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.8750
2022-03-20 20:43:22 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 0.9867
2022-03-20 20:44:00 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 0.9059
2022-03-20 20:44:38 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.1646
2022-03-20 20:45:16 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.1817
2022-03-20 20:45:18 - train: epoch 067, train_loss: 1.1387
2022-03-20 20:46:34 - eval: epoch: 067, acc1: 73.432%, acc5: 91.538%, test_loss: 1.0442, per_image_load_time: 1.514ms, per_image_inference_time: 0.726ms
2022-03-20 20:46:35 - until epoch: 067, best_acc1: 73.516%
2022-03-20 20:46:35 - epoch 068 lr: 0.0010000000000000002
2022-03-20 20:47:18 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.1296
2022-03-20 20:47:56 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.1752
2022-03-20 20:48:33 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.2235
2022-03-20 20:49:11 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.1238
2022-03-20 20:49:48 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.1040
2022-03-20 20:50:26 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.1694
2022-03-20 20:51:04 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.4311
2022-03-20 20:51:41 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.0456
2022-03-20 20:52:19 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.0665
2022-03-20 20:52:57 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.2037
2022-03-20 20:53:35 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.2780
2022-03-20 20:54:13 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.0148
2022-03-20 20:54:50 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.0114
2022-03-20 20:55:28 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.2188
2022-03-20 20:56:06 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.2684
2022-03-20 20:56:44 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.1817
2022-03-20 20:57:22 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.2206
2022-03-20 20:58:00 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.1170
2022-03-20 20:58:38 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.1248
2022-03-20 20:59:16 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.2888
2022-03-20 20:59:53 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.1319
2022-03-20 21:00:31 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.1902
2022-03-20 21:01:09 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.0964
2022-03-20 21:01:46 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.2475
2022-03-20 21:02:24 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.1450
2022-03-20 21:03:02 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.0983
2022-03-20 21:03:40 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.1688
2022-03-20 21:04:18 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.3477
2022-03-20 21:04:56 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.2698
2022-03-20 21:05:34 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.2715
2022-03-20 21:06:12 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.0078
2022-03-20 21:06:50 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.2331
2022-03-20 21:07:28 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.1386
2022-03-20 21:08:06 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 0.9839
2022-03-20 21:08:45 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.2206
2022-03-20 21:09:23 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.0227
2022-03-20 21:10:01 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.2380
2022-03-20 21:10:39 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.2274
2022-03-20 21:11:17 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.2466
2022-03-20 21:11:55 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.1312
2022-03-20 21:12:33 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 0.9517
2022-03-20 21:13:11 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.3522
2022-03-20 21:13:49 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.1867
2022-03-20 21:14:28 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.1263
2022-03-20 21:15:06 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.1836
2022-03-20 21:15:44 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.2454
2022-03-20 21:16:22 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.3877
2022-03-20 21:17:00 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.2697
2022-03-20 21:17:38 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.2640
2022-03-20 21:18:16 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.0925
2022-03-20 21:18:18 - train: epoch 068, train_loss: 1.1343
2022-03-20 21:19:35 - eval: epoch: 068, acc1: 73.576%, acc5: 91.596%, test_loss: 1.0402, per_image_load_time: 1.894ms, per_image_inference_time: 0.714ms
2022-03-20 21:19:36 - until epoch: 068, best_acc1: 73.576%
2022-03-20 21:19:36 - epoch 069 lr: 0.0010000000000000002
2022-03-20 21:20:20 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.2697
2022-03-20 21:20:57 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.2427
2022-03-20 21:21:34 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.2170
2022-03-20 21:22:12 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.0655
2022-03-20 21:22:49 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.0562
2022-03-20 21:23:27 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.9871
2022-03-20 21:24:04 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.0568
2022-03-20 21:24:42 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.1670
2022-03-20 21:25:19 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.0953
2022-03-20 21:25:57 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.1337
2022-03-20 21:26:34 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.0460
2022-03-20 21:27:12 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.0941
2022-03-20 21:27:49 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.3824
2022-03-20 21:28:27 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.2289
2022-03-20 21:29:05 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.0935
2022-03-20 21:29:43 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.2637
2022-03-20 21:30:20 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.1323
2022-03-20 21:30:58 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 0.9026
2022-03-20 21:31:36 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.1212
2022-03-20 21:32:14 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.9629
2022-03-20 21:32:52 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.1717
2022-03-20 21:33:30 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.1762
2022-03-20 21:34:08 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.1672
2022-03-20 21:34:46 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.1970
2022-03-20 21:35:24 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.1861
2022-03-20 21:36:02 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.2101
2022-03-20 21:36:40 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.3942
2022-03-20 21:37:18 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.2634
2022-03-20 21:37:56 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 0.9718
2022-03-20 21:38:35 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.0214
2022-03-20 21:39:13 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 0.9945
2022-03-20 21:39:51 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.0177
2022-03-20 21:40:29 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.0076
2022-03-20 21:41:07 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 0.9548
2022-03-20 21:41:46 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 0.9998
2022-03-20 21:42:24 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.0164
2022-03-20 21:43:02 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.2514
2022-03-20 21:43:40 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.2348
2022-03-20 21:44:18 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.1267
2022-03-20 21:44:56 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.2387
2022-03-20 21:45:54 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.2441
2022-03-20 21:46:33 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.0226
2022-03-20 21:47:11 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.1333
2022-03-20 21:47:49 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.1220
2022-03-20 21:48:27 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.1590
2022-03-20 21:49:06 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.2938
2022-03-20 21:49:44 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.1342
2022-03-20 21:50:22 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.0802
2022-03-20 21:51:00 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.1094
2022-03-20 21:51:38 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.1716
2022-03-20 21:51:40 - train: epoch 069, train_loss: 1.1272
2022-03-20 21:52:57 - eval: epoch: 069, acc1: 73.660%, acc5: 91.544%, test_loss: 1.0416, per_image_load_time: 2.220ms, per_image_inference_time: 0.733ms
2022-03-20 21:52:58 - until epoch: 069, best_acc1: 73.660%
2022-03-20 21:52:58 - epoch 070 lr: 0.0010000000000000002
2022-03-20 21:53:41 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.1686
2022-03-20 21:54:19 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.2288
2022-03-20 21:54:56 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.1935
2022-03-20 21:55:34 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.1302
2022-03-20 21:56:11 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.2380
2022-03-20 21:56:49 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.1044
2022-03-20 21:57:26 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.1471
2022-03-20 21:58:04 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.0570
2022-03-20 21:58:41 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.1492
2022-03-20 21:59:18 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.0371
2022-03-20 21:59:56 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.4342
2022-03-20 22:00:33 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 0.9936
2022-03-20 22:01:11 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.0938
2022-03-20 22:01:48 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.1171
2022-03-20 22:02:26 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.0229
2022-03-20 22:03:04 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.1577
2022-03-20 22:03:41 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.1973
2022-03-20 22:04:19 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 0.9884
2022-03-20 22:04:56 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.0480
2022-03-20 22:05:34 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.1092
2022-03-20 22:06:12 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.1176
2022-03-20 22:06:49 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.1895
2022-03-20 22:07:27 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.0684
2022-03-20 22:08:05 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.3188
2022-03-20 22:08:43 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.2076
2022-03-20 22:09:21 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.1478
2022-03-20 22:09:59 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.1258
2022-03-20 22:10:37 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.2528
2022-03-20 22:11:15 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.2746
2022-03-20 22:11:53 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.1158
2022-03-20 22:12:31 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.0995
2022-03-20 22:13:08 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.3340
2022-03-20 22:13:46 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.1085
2022-03-20 22:14:24 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.1578
2022-03-20 22:15:02 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.1754
2022-03-20 22:15:40 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.1924
2022-03-20 22:16:18 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.2396
2022-03-20 22:16:56 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.0867
2022-03-20 22:17:34 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.9925
2022-03-20 22:18:12 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.0308
2022-03-20 22:18:49 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 0.9894
2022-03-20 22:19:27 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.1189
2022-03-20 22:20:05 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.1248
2022-03-20 22:20:43 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.1699
2022-03-20 22:21:21 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.0759
2022-03-20 22:21:59 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.1926
2022-03-20 22:22:37 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.2929
2022-03-20 22:23:33 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.2303
2022-03-20 22:24:17 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.2061
2022-03-20 22:24:55 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.1210
2022-03-20 22:24:57 - train: epoch 070, train_loss: 1.1236
2022-03-20 22:26:37 - eval: epoch: 070, acc1: 73.654%, acc5: 91.616%, test_loss: 1.0374, per_image_load_time: 1.381ms, per_image_inference_time: 0.781ms
2022-03-20 22:26:39 - until epoch: 070, best_acc1: 73.660%
2022-03-20 22:26:39 - epoch 071 lr: 0.0010000000000000002
2022-03-20 22:27:22 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 0.9849
2022-03-20 22:28:00 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.0749
2022-03-20 22:28:37 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.0647
2022-03-20 22:29:14 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.1335
2022-03-20 22:30:09 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.1697
2022-03-20 22:30:54 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.2118
2022-03-20 22:31:32 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.2307
2022-03-20 22:32:09 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.1244
2022-03-20 22:32:47 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.1826
2022-03-20 22:33:24 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.2439
2022-03-20 22:34:02 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.2460
2022-03-20 22:34:39 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.0547
2022-03-20 22:35:16 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.0880
2022-03-20 22:35:54 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.0596
2022-03-20 22:36:31 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.9863
2022-03-20 22:37:09 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.0465
2022-03-20 22:37:46 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.0923
2022-03-20 22:38:23 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.0722
2022-03-20 22:39:01 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.1007
2022-03-20 22:39:38 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.2922
2022-03-20 22:40:16 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 0.9557
2022-03-20 22:40:53 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.9717
2022-03-20 22:41:31 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.0724
2022-03-20 22:42:09 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.0160
2022-03-20 22:42:46 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.2349
2022-03-20 22:43:24 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 0.9565
2022-03-20 22:44:02 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.1589
2022-03-20 22:44:40 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.1405
2022-03-20 22:45:18 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.1133
2022-03-20 22:45:55 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.1386
2022-03-20 22:46:33 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.0493
2022-03-20 22:47:11 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 0.9946
2022-03-20 22:47:48 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.0259
2022-03-20 22:48:26 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.0891
2022-03-20 22:49:04 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.1134
2022-03-20 22:49:41 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.2991
2022-03-20 22:50:19 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.0857
2022-03-20 22:50:57 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.1245
2022-03-20 22:51:34 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.2061
2022-03-20 22:52:12 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.2424
2022-03-20 22:52:50 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.0963
2022-03-20 22:53:28 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.1864
2022-03-20 22:54:05 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.1318
2022-03-20 22:54:43 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.1174
2022-03-20 22:55:21 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.1128
2022-03-20 22:56:03 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.2639
2022-03-20 22:57:05 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.0049
2022-03-20 22:58:16 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 0.9934
2022-03-20 22:59:07 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.8884
2022-03-20 22:59:44 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.0403
2022-03-20 22:59:46 - train: epoch 071, train_loss: 1.1177
2022-03-20 23:01:19 - eval: epoch: 071, acc1: 73.614%, acc5: 91.700%, test_loss: 1.0354, per_image_load_time: 1.063ms, per_image_inference_time: 0.762ms
2022-03-20 23:01:20 - until epoch: 071, best_acc1: 73.660%
2022-03-20 23:01:20 - epoch 072 lr: 0.0010000000000000002
2022-03-20 23:02:07 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.1414
2022-03-20 23:02:44 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 0.9408
2022-03-20 23:03:21 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.0315
2022-03-20 23:03:59 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.1710
2022-03-20 23:04:36 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.0050
2022-03-20 23:05:13 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.0441
2022-03-20 23:05:51 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.0668
2022-03-20 23:06:28 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.2816
2022-03-20 23:07:06 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 0.9924
2022-03-20 23:07:58 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.0444
2022-03-20 23:08:35 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.1118
2022-03-20 23:09:13 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 0.9594
2022-03-20 23:10:13 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.1774
2022-03-20 23:10:51 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.0986
2022-03-20 23:11:29 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.1310
2022-03-20 23:12:07 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.1342
2022-03-20 23:12:45 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.0150
2022-03-20 23:13:22 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 0.9440
2022-03-20 23:14:00 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.0339
2022-03-20 23:14:38 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.2584
2022-03-20 23:15:16 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.2122
2022-03-20 23:15:54 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.1468
2022-03-20 23:16:32 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.2321
2022-03-20 23:17:10 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.0761
2022-03-20 23:17:48 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.0081
2022-03-20 23:18:26 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 0.9556
2022-03-20 23:19:03 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.0803
2022-03-20 23:19:41 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.1774
2022-03-20 23:20:19 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.0591
2022-03-20 23:20:57 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.0480
2022-03-20 23:21:35 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.2218
2022-03-20 23:22:12 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.0867
2022-03-20 23:22:50 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.1014
2022-03-20 23:23:28 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.2578
2022-03-20 23:24:06 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.0804
2022-03-20 23:24:45 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.1083
2022-03-20 23:25:23 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.1962
2022-03-20 23:26:01 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.1534
2022-03-20 23:26:39 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.2645
2022-03-20 23:27:17 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.0215
2022-03-20 23:27:55 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.1880
2022-03-20 23:28:33 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.1098
2022-03-20 23:29:11 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.1346
2022-03-20 23:29:49 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 0.9692
2022-03-20 23:30:27 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.2077
2022-03-20 23:31:05 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 0.9831
2022-03-20 23:31:43 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.1684
2022-03-20 23:32:21 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.2498
2022-03-20 23:32:59 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.0799
2022-03-20 23:33:36 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.1354
2022-03-20 23:33:38 - train: epoch 072, train_loss: 1.1158
2022-03-20 23:34:54 - eval: epoch: 072, acc1: 73.614%, acc5: 91.602%, test_loss: 1.0383, per_image_load_time: 1.285ms, per_image_inference_time: 0.717ms
2022-03-20 23:34:55 - until epoch: 072, best_acc1: 73.660%
2022-03-20 23:34:55 - epoch 073 lr: 0.0010000000000000002
2022-03-20 23:35:38 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.2496
2022-03-20 23:36:15 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.2076
2022-03-20 23:36:53 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.2056
2022-03-20 23:37:30 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.8706
2022-03-20 23:38:08 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.0109
2022-03-20 23:38:45 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.0180
2022-03-20 23:39:22 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.1276
2022-03-20 23:40:00 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.0485
2022-03-20 23:40:37 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 0.9754
2022-03-20 23:41:14 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.9763
2022-03-20 23:41:52 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.1119
2022-03-20 23:42:29 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.0631
2022-03-20 23:43:07 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.1359
2022-03-20 23:43:44 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.0691
2022-03-20 23:44:21 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.0518
2022-03-20 23:44:59 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.1085
2022-03-20 23:45:36 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.4150
2022-03-20 23:46:14 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.9905
2022-03-20 23:46:51 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.2215
2022-03-20 23:47:29 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 0.9432
2022-03-20 23:48:06 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.0855
2022-03-20 23:48:43 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.2144
2022-03-20 23:49:21 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.1833
2022-03-20 23:49:58 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.0255
2022-03-20 23:50:36 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.1748
2022-03-20 23:51:13 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.0883
2022-03-20 23:51:51 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.1796
2022-03-20 23:52:28 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.1819
2022-03-20 23:53:06 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.2600
2022-03-20 23:53:43 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.9346
2022-03-20 23:54:20 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.0617
2022-03-20 23:54:58 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.0239
2022-03-20 23:55:35 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.0537
2022-03-20 23:56:13 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.0500
2022-03-20 23:56:50 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.2036
2022-03-20 23:57:28 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.9809
2022-03-20 23:58:05 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.0836
2022-03-20 23:58:43 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.3569
2022-03-20 23:59:20 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.0988
2022-03-20 23:59:58 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.1256
2022-03-21 00:00:36 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 0.9937
2022-03-21 00:01:14 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.1720
2022-03-21 00:01:52 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.1550
2022-03-21 00:02:30 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.1003
2022-03-21 00:03:08 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.0478
2022-03-21 00:03:45 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.3130
2022-03-21 00:04:23 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 0.9314
2022-03-21 00:05:01 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.2389
2022-03-21 00:05:39 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.0286
2022-03-21 00:06:17 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.2079
2022-03-21 00:06:19 - train: epoch 073, train_loss: 1.1120
2022-03-21 00:07:36 - eval: epoch: 073, acc1: 73.626%, acc5: 91.562%, test_loss: 1.0342, per_image_load_time: 2.261ms, per_image_inference_time: 0.742ms
2022-03-21 00:07:37 - until epoch: 073, best_acc1: 73.660%
2022-03-21 00:07:37 - epoch 074 lr: 0.0010000000000000002
2022-03-21 00:08:20 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.0792
2022-03-21 00:08:58 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.1369
2022-03-21 00:09:35 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.1396
2022-03-21 00:10:12 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.2479
2022-03-21 00:10:50 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2124
2022-03-21 00:11:27 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.1220
2022-03-21 00:12:05 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.1472
2022-03-21 00:12:43 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.2503
2022-03-21 00:13:20 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.0790
2022-03-21 00:13:58 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.1882
2022-03-21 00:14:35 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.1857
2022-03-21 00:15:13 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 0.9899
2022-03-21 00:15:51 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.2763
2022-03-21 00:16:29 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 0.9957
2022-03-21 00:17:07 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.1111
2022-03-21 00:17:45 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.9023
2022-03-21 00:18:23 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.2702
2022-03-21 00:19:01 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.4139
2022-03-21 00:19:39 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.2065
2022-03-21 00:20:17 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.8615
2022-03-21 00:20:55 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.0717
2022-03-21 00:21:33 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.4040
2022-03-21 00:22:11 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.1812
2022-03-21 00:22:49 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.1316
2022-03-21 00:23:27 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.0478
2022-03-21 00:24:05 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 0.9332
2022-03-21 00:24:43 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.0617
2022-03-21 00:25:21 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.2770
2022-03-21 00:25:59 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.0497
2022-03-21 00:26:37 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.1186
2022-03-21 00:27:15 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.0185
2022-03-21 00:27:53 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.0542
2022-03-21 00:28:31 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.2854
2022-03-21 00:29:09 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.1687
2022-03-21 00:29:47 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 0.8793
2022-03-21 00:30:25 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.0735
2022-03-21 00:31:03 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.2641
2022-03-21 00:31:41 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.0140
2022-03-21 00:32:19 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.8694
2022-03-21 00:32:57 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.0560
2022-03-21 00:33:35 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.2785
2022-03-21 00:34:13 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.0515
2022-03-21 00:34:51 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.0073
2022-03-21 00:35:29 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 0.9494
2022-03-21 00:36:07 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.9128
2022-03-21 00:36:45 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.1241
2022-03-21 00:37:23 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.1343
2022-03-21 00:38:01 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.1240
2022-03-21 00:38:40 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.3552
2022-03-21 00:39:17 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.0848
2022-03-21 00:39:19 - train: epoch 074, train_loss: 1.1063
2022-03-21 00:40:36 - eval: epoch: 074, acc1: 73.800%, acc5: 91.640%, test_loss: 1.0313, per_image_load_time: 2.264ms, per_image_inference_time: 0.724ms
2022-03-21 00:40:38 - until epoch: 074, best_acc1: 73.800%
2022-03-21 00:40:38 - epoch 075 lr: 0.0010000000000000002
2022-03-21 00:41:21 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.1834
2022-03-21 00:41:59 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.1582
2022-03-21 00:42:36 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.0698
2022-03-21 00:43:13 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.0593
2022-03-21 00:43:50 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 0.9366
2022-03-21 00:44:28 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.0381
2022-03-21 00:45:05 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.2278
2022-03-21 00:45:42 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.2130
2022-03-21 00:46:19 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.3215
2022-03-21 00:46:57 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.9665
2022-03-21 00:47:34 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.2111
2022-03-21 00:48:11 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.0388
2022-03-21 00:48:49 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.0821
2022-03-21 00:49:26 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.1662
2022-03-21 00:50:04 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.1704
2022-03-21 00:50:41 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.8522
2022-03-21 00:51:19 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.0668
2022-03-21 00:51:57 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.0591
2022-03-21 00:52:34 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 0.9943
2022-03-21 00:53:12 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.1832
2022-03-21 00:53:49 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.0754
2022-03-21 00:54:27 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.1774
2022-03-21 00:55:04 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.1555
2022-03-21 00:55:42 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.0673
2022-03-21 00:56:19 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.1280
2022-03-21 00:56:57 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.1332
2022-03-21 00:57:35 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.8518
2022-03-21 00:58:13 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.1060
2022-03-21 00:58:50 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.2323
2022-03-21 00:59:28 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.2893
2022-03-21 01:00:06 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.2209
2022-03-21 01:00:44 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.0188
2022-03-21 01:01:22 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.1536
2022-03-21 01:01:59 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 0.9086
2022-03-21 01:02:37 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.0511
2022-03-21 01:03:15 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.0590
2022-03-21 01:03:53 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.1799
2022-03-21 01:04:31 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.0244
2022-03-21 01:05:09 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.3063
2022-03-21 01:05:47 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.0446
2022-03-21 01:06:25 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.8732
2022-03-21 01:07:03 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.1152
2022-03-21 01:07:41 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.2668
2022-03-21 01:08:18 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 0.9638
2022-03-21 01:08:56 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.1042
2022-03-21 01:09:34 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.9658
2022-03-21 01:10:12 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.2323
2022-03-21 01:10:50 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.1120
2022-03-21 01:11:28 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 0.9084
2022-03-21 01:12:06 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.2020
2022-03-21 01:12:08 - train: epoch 075, train_loss: 1.1059
2022-03-21 01:13:24 - eval: epoch: 075, acc1: 73.896%, acc5: 91.736%, test_loss: 1.0324, per_image_load_time: 1.395ms, per_image_inference_time: 0.739ms
2022-03-21 01:13:25 - until epoch: 075, best_acc1: 73.896%
2022-03-21 01:13:25 - epoch 076 lr: 0.0010000000000000002
2022-03-21 01:14:08 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 0.9783
2022-03-21 01:14:45 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.0695
2022-03-21 01:15:23 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.2469
2022-03-21 01:16:00 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.1522
2022-03-21 01:16:38 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.0956
2022-03-21 01:17:15 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.0639
2022-03-21 01:17:53 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.1489
2022-03-21 01:18:31 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.0425
2022-03-21 01:19:08 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 0.9601
2022-03-21 01:19:46 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 0.8646
2022-03-21 01:20:23 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.0303
2022-03-21 01:21:01 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.1276
2022-03-21 01:21:39 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.1291
2022-03-21 01:22:16 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.0207
2022-03-21 01:22:54 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.0239
2022-03-21 01:23:32 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.0742
2022-03-21 01:24:10 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.1471
2022-03-21 01:24:47 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 0.9336
2022-03-21 01:25:25 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.2203
2022-03-21 01:26:03 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.0581
2022-03-21 01:26:41 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.2373
2022-03-21 01:27:18 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.2057
2022-03-21 01:27:56 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.0622
2022-03-21 01:28:34 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.2841
2022-03-21 01:29:12 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.0307
2022-03-21 01:29:50 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.2476
2022-03-21 01:30:28 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.1091
2022-03-21 01:31:05 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 0.9732
2022-03-21 01:31:44 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.1321
2022-03-21 01:32:22 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 0.9635
2022-03-21 01:33:00 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.1975
2022-03-21 01:33:38 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.1960
2022-03-21 01:34:16 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.0930
2022-03-21 01:34:55 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.1708
2022-03-21 01:35:33 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.0082
2022-03-21 01:36:11 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.0398
2022-03-21 01:36:49 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.9850
2022-03-21 01:37:27 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.9574
2022-03-21 01:38:05 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.0022
2022-03-21 01:38:43 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.0125
2022-03-21 01:39:22 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.0681
2022-03-21 01:40:00 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.2593
2022-03-21 01:40:38 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.1625
2022-03-21 01:41:16 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.0296
2022-03-21 01:41:54 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.0414
2022-03-21 01:42:32 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 0.9731
2022-03-21 01:43:10 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.1313
2022-03-21 01:43:48 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.8957
2022-03-21 01:44:26 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.1312
2022-03-21 01:45:04 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.0813
2022-03-21 01:45:06 - train: epoch 076, train_loss: 1.1019
2022-03-21 01:46:22 - eval: epoch: 076, acc1: 73.798%, acc5: 91.696%, test_loss: 1.0334, per_image_load_time: 1.621ms, per_image_inference_time: 0.713ms
2022-03-21 01:46:24 - until epoch: 076, best_acc1: 73.896%
2022-03-21 01:46:24 - epoch 077 lr: 0.0010000000000000002
2022-03-21 01:47:07 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.1566
2022-03-21 01:47:44 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.2021
2022-03-21 01:48:21 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.9564
2022-03-21 01:48:58 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.2557
2022-03-21 01:49:36 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.0334
2022-03-21 01:50:13 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.8981
2022-03-21 01:50:51 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.1219
2022-03-21 01:51:28 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.2608
2022-03-21 01:52:06 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.2759
2022-03-21 01:52:43 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.9210
2022-03-21 01:53:21 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.1175
2022-03-21 01:53:59 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.1211
2022-03-21 01:54:36 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.8379
2022-03-21 01:55:14 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.2372
2022-03-21 01:55:52 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.0322
2022-03-21 01:56:30 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.1442
2022-03-21 01:57:07 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.3004
2022-03-21 01:57:45 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 0.9453
2022-03-21 01:58:23 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.0949
2022-03-21 01:59:01 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.0175
2022-03-21 01:59:39 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.0623
2022-03-21 02:00:16 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.1801
2022-03-21 02:00:54 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.2037
2022-03-21 02:01:31 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.0729
2022-03-21 02:02:09 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.2724
2022-03-21 02:02:46 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.8248
2022-03-21 02:03:24 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.1706
2022-03-21 02:04:02 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.2034
2022-03-21 02:04:39 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.4135
2022-03-21 02:05:17 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.0016
2022-03-21 02:05:54 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.0861
2022-03-21 02:06:32 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.2557
2022-03-21 02:07:10 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 0.9482
2022-03-21 02:07:47 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.2665
2022-03-21 02:08:25 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.0087
2022-03-21 02:09:02 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.0065
2022-03-21 02:09:40 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.1880
2022-03-21 02:10:18 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.0898
2022-03-21 02:10:55 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.1873
2022-03-21 02:11:33 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.1054
2022-03-21 02:12:11 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.1841
2022-03-21 02:12:48 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.1417
2022-03-21 02:13:26 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.1114
2022-03-21 02:14:04 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.0745
2022-03-21 02:14:41 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.1362
2022-03-21 02:15:19 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.0727
2022-03-21 02:15:57 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 0.8465
2022-03-21 02:16:36 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.1812
2022-03-21 02:17:14 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.2401
2022-03-21 02:17:51 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.2763
2022-03-21 02:17:53 - train: epoch 077, train_loss: 1.0971
2022-03-21 02:19:10 - eval: epoch: 077, acc1: 73.792%, acc5: 91.654%, test_loss: 1.0326, per_image_load_time: 1.364ms, per_image_inference_time: 0.717ms
2022-03-21 02:19:11 - until epoch: 077, best_acc1: 73.896%
2022-03-21 02:19:11 - epoch 078 lr: 0.0010000000000000002
2022-03-21 02:19:55 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.0318
2022-03-21 02:20:32 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.2500
2022-03-21 02:21:09 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.1651
2022-03-21 02:21:46 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.0858
2022-03-21 02:22:24 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.0173
2022-03-21 02:23:01 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.1581
2022-03-21 02:23:38 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.2008
2022-03-21 02:24:15 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.3222
2022-03-21 02:24:53 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.2125
2022-03-21 02:25:30 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.2205
2022-03-21 02:26:08 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 0.9283
2022-03-21 02:26:45 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 0.8622
2022-03-21 02:27:23 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.2215
2022-03-21 02:28:01 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 0.9765
2022-03-21 02:28:38 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.0507
2022-03-21 02:29:16 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.1634
2022-03-21 02:29:54 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.0785
2022-03-21 02:30:32 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.0427
2022-03-21 02:31:10 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 0.9877
2022-03-21 02:31:48 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 0.9592
2022-03-21 02:32:26 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.4143
2022-03-21 02:33:04 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.0497
2022-03-21 02:33:42 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.2494
2022-03-21 02:34:20 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.0951
2022-03-21 02:34:58 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 0.9519
2022-03-21 02:35:36 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.0456
2022-03-21 02:36:14 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.1422
2022-03-21 02:36:52 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.1556
2022-03-21 02:37:30 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.0231
2022-03-21 02:38:08 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 0.9952
2022-03-21 02:38:46 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.1590
2022-03-21 02:39:24 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.0676
2022-03-21 02:40:02 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.1312
2022-03-21 02:40:40 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.1183
2022-03-21 02:41:18 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.1024
2022-03-21 02:41:56 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.0939
2022-03-21 02:42:34 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 0.9802
2022-03-21 02:43:12 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.1258
2022-03-21 02:43:50 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.1840
2022-03-21 02:44:28 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.0695
2022-03-21 02:45:06 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.2275
2022-03-21 02:45:44 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.2279
2022-03-21 02:46:22 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.1837
2022-03-21 02:47:00 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.0704
2022-03-21 02:47:38 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.2515
2022-03-21 02:48:16 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 0.8861
2022-03-21 02:48:54 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.1989
2022-03-21 02:49:32 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.2769
2022-03-21 02:50:10 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.1818
2022-03-21 02:50:48 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.1439
2022-03-21 02:50:50 - train: epoch 078, train_loss: 1.0927
2022-03-21 02:52:05 - eval: epoch: 078, acc1: 73.794%, acc5: 91.670%, test_loss: 1.0315, per_image_load_time: 2.221ms, per_image_inference_time: 0.724ms
2022-03-21 02:52:07 - until epoch: 078, best_acc1: 73.896%
2022-03-21 02:52:07 - epoch 079 lr: 0.0010000000000000002
2022-03-21 02:52:51 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.0090
2022-03-21 02:53:28 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.0063
2022-03-21 02:54:05 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.1725
2022-03-21 02:54:43 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.1357
2022-03-21 02:55:20 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.1135
2022-03-21 02:55:58 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.0612
2022-03-21 02:56:35 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.0180
2022-03-21 02:57:12 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.1520
2022-03-21 02:57:50 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.0806
2022-03-21 02:58:28 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 0.9394
2022-03-21 02:59:06 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.2264
2022-03-21 02:59:43 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.3128
2022-03-21 03:00:21 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.8889
2022-03-21 03:00:58 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.0144
2022-03-21 03:01:36 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 0.9746
2022-03-21 03:02:14 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 0.9713
2022-03-21 03:02:52 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.0320
2022-03-21 03:03:29 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.0003
2022-03-21 03:04:07 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.1533
2022-03-21 03:04:45 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.2654
2022-03-21 03:05:23 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 0.9062
2022-03-21 03:06:00 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.2054
2022-03-21 03:06:38 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.0078
2022-03-21 03:07:16 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.1298
2022-03-21 03:07:54 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.1016
2022-03-21 03:08:32 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 0.9809
2022-03-21 03:09:10 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.8888
2022-03-21 03:09:48 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 0.9805
2022-03-21 03:10:26 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 0.9822
2022-03-21 03:11:04 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.0533
2022-03-21 03:11:42 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.1666
2022-03-21 03:12:20 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.2934
2022-03-21 03:12:58 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.0468
2022-03-21 03:13:36 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 0.9235
2022-03-21 03:14:14 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.2348
2022-03-21 03:14:51 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.0057
2022-03-21 03:15:29 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 0.9114
2022-03-21 03:16:07 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.1313
2022-03-21 03:16:45 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 0.9747
2022-03-21 03:17:23 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 0.9264
2022-03-21 03:18:01 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.2027
2022-03-21 03:18:39 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 0.9660
2022-03-21 03:19:17 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 0.8979
2022-03-21 03:19:55 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.2114
2022-03-21 03:20:33 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.3011
2022-03-21 03:21:10 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.2303
2022-03-21 03:21:48 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.1330
2022-03-21 03:22:26 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.2208
2022-03-21 03:23:04 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.2948
2022-03-21 03:23:42 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 0.9324
2022-03-21 03:23:44 - train: epoch 079, train_loss: 1.0906
2022-03-21 03:25:00 - eval: epoch: 079, acc1: 73.790%, acc5: 91.694%, test_loss: 1.0322, per_image_load_time: 2.208ms, per_image_inference_time: 0.718ms
2022-03-21 03:25:01 - until epoch: 079, best_acc1: 73.896%
2022-03-21 03:25:01 - epoch 080 lr: 0.0010000000000000002
2022-03-21 03:25:45 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 0.9257
2022-03-21 03:26:22 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 0.9671
2022-03-21 03:26:59 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.2008
2022-03-21 03:27:37 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 0.9678
2022-03-21 03:28:14 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.1563
2022-03-21 03:28:51 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.0282
2022-03-21 03:29:29 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.0339
2022-03-21 03:30:06 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.0071
2022-03-21 03:30:43 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.0989
2022-03-21 03:31:21 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.0088
2022-03-21 03:31:58 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.0404
2022-03-21 03:32:36 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.0996
2022-03-21 03:33:13 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 0.9725
2022-03-21 03:33:50 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.0820
2022-03-21 03:34:28 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 0.9603
2022-03-21 03:35:05 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.0933
2022-03-21 03:35:43 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.1053
2022-03-21 03:36:20 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.2351
2022-03-21 03:36:58 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.0289
2022-03-21 03:37:35 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.1095
2022-03-21 03:38:13 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.0678
2022-03-21 03:38:50 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.0783
2022-03-21 03:39:28 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 0.9885
2022-03-21 03:40:05 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.0829
2022-03-21 03:40:43 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.0719
2022-03-21 03:41:21 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.0979
2022-03-21 03:41:59 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.1331
2022-03-21 03:42:37 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.1638
2022-03-21 03:43:14 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 0.9767
2022-03-21 03:43:52 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.0896
2022-03-21 03:44:30 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.2888
2022-03-21 03:45:08 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.8211
2022-03-21 03:45:46 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 0.9885
2022-03-21 03:46:23 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 0.9703
2022-03-21 03:47:01 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.9653
2022-03-21 03:47:39 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.1357
2022-03-21 03:48:17 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.1159
2022-03-21 03:48:55 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.0589
2022-03-21 03:49:33 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.0075
2022-03-21 03:50:11 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.1831
2022-03-21 03:50:48 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.2101
2022-03-21 03:51:26 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.1133
2022-03-21 03:52:04 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.1579
2022-03-21 03:52:42 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.0490
2022-03-21 03:53:20 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.0248
2022-03-21 03:53:57 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.1619
2022-03-21 03:54:35 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.1312
2022-03-21 03:55:13 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.0680
2022-03-21 03:55:51 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.1700
2022-03-21 03:56:29 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 0.8663
2022-03-21 03:56:30 - train: epoch 080, train_loss: 1.0895
2022-03-21 03:57:47 - eval: epoch: 080, acc1: 73.822%, acc5: 91.732%, test_loss: 1.0296, per_image_load_time: 1.569ms, per_image_inference_time: 0.711ms
2022-03-21 03:57:48 - until epoch: 080, best_acc1: 73.896%
2022-03-21 03:57:48 - epoch 081 lr: 0.0010000000000000002
2022-03-21 03:58:31 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 0.9474
2022-03-21 03:59:09 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.1396
2022-03-21 03:59:46 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.0585
2022-03-21 04:00:23 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.2140
2022-03-21 04:01:01 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.1948
2022-03-21 04:01:39 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.0707
2022-03-21 04:02:16 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.0265
2022-03-21 04:02:54 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.2293
2022-03-21 04:03:32 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 0.9818
2022-03-21 04:04:10 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.0495
2022-03-21 04:04:47 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.0707
2022-03-21 04:05:25 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.1425
2022-03-21 04:06:02 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.0157
2022-03-21 04:06:40 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.8048
2022-03-21 04:07:18 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.1602
2022-03-21 04:07:55 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.0760
2022-03-21 04:08:33 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.1646
2022-03-21 04:09:10 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 0.9776
2022-03-21 04:09:48 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.0162
2022-03-21 04:10:26 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.2054
2022-03-21 04:11:03 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.0712
2022-03-21 04:11:41 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.0792
2022-03-21 04:12:18 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.0208
2022-03-21 04:12:56 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.1390
2022-03-21 04:13:34 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.1992
2022-03-21 04:14:11 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.1788
2022-03-21 04:14:49 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.0401
2022-03-21 04:15:27 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 0.9994
2022-03-21 04:16:05 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 0.9612
2022-03-21 04:16:42 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.1494
2022-03-21 04:17:20 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 0.9838
2022-03-21 04:17:58 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.0954
2022-03-21 04:18:36 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.1484
2022-03-21 04:19:13 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.2643
2022-03-21 04:19:51 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.1330
2022-03-21 04:20:29 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 0.9387
2022-03-21 04:21:07 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.2070
2022-03-21 04:21:45 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 0.9630
2022-03-21 04:22:22 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.1083
2022-03-21 04:23:00 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.9083
2022-03-21 04:23:38 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.1803
2022-03-21 04:24:16 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.1118
2022-03-21 04:24:54 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.0448
2022-03-21 04:25:33 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.1399
2022-03-21 04:26:11 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.2044
2022-03-21 04:26:49 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.0530
2022-03-21 04:27:27 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.1649
2022-03-21 04:28:05 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.2083
2022-03-21 04:28:43 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 0.9714
2022-03-21 04:29:20 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.8587
2022-03-21 04:29:22 - train: epoch 081, train_loss: 1.0866
2022-03-21 04:30:38 - eval: epoch: 081, acc1: 73.808%, acc5: 91.824%, test_loss: 1.0301, per_image_load_time: 1.620ms, per_image_inference_time: 0.736ms
2022-03-21 04:30:39 - until epoch: 081, best_acc1: 73.896%
2022-03-21 04:30:39 - epoch 082 lr: 0.0010000000000000002
2022-03-21 04:31:23 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.8507
2022-03-21 04:32:00 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 0.9446
2022-03-21 04:32:38 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.0285
2022-03-21 04:33:15 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.2301
2022-03-21 04:33:53 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.1657
2022-03-21 04:34:31 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 0.9828
2022-03-21 04:35:08 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.2319
2022-03-21 04:35:46 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.1264
2022-03-21 04:36:23 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.1728
2022-03-21 04:37:01 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.1059
2022-03-21 04:37:38 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.2127
2022-03-21 04:38:16 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.1709
2022-03-21 04:38:54 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.1599
2022-03-21 04:39:32 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.0002
2022-03-21 04:40:10 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 0.9898
2022-03-21 04:40:48 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.0277
2022-03-21 04:41:26 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.0448
2022-03-21 04:42:04 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 0.9776
2022-03-21 04:42:41 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.0223
2022-03-21 04:43:19 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 0.9791
2022-03-21 04:43:57 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 0.9983
2022-03-21 04:44:35 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.1507
2022-03-21 04:45:13 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.0565
2022-03-21 04:45:51 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.2633
2022-03-21 04:46:29 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 0.9739
2022-03-21 04:47:06 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.1011
2022-03-21 04:47:44 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.0236
2022-03-21 04:48:22 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.0168
2022-03-21 04:49:00 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.0135
2022-03-21 04:49:38 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.0688
2022-03-21 04:50:16 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 0.9911
2022-03-21 04:50:54 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.2677
2022-03-21 04:51:32 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.0667
2022-03-21 04:52:10 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.0854
2022-03-21 04:52:48 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.9836
2022-03-21 04:53:26 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.1016
2022-03-21 04:54:04 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.0217
2022-03-21 04:54:41 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.3615
2022-03-21 04:55:19 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.1722
2022-03-21 04:55:57 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.1217
2022-03-21 04:56:35 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.0566
2022-03-21 04:57:13 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.1417
2022-03-21 04:57:51 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 0.9299
2022-03-21 04:58:29 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.0572
2022-03-21 04:59:06 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.0728
2022-03-21 04:59:44 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.1998
2022-03-21 05:00:22 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 0.9705
2022-03-21 05:01:00 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.0097
2022-03-21 05:01:38 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.0014
2022-03-21 05:02:15 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.1001
2022-03-21 05:02:17 - train: epoch 082, train_loss: 1.0830
2022-03-21 05:03:33 - eval: epoch: 082, acc1: 73.774%, acc5: 91.706%, test_loss: 1.0306, per_image_load_time: 1.452ms, per_image_inference_time: 0.715ms
2022-03-21 05:03:34 - until epoch: 082, best_acc1: 73.896%
2022-03-21 05:03:34 - epoch 083 lr: 0.0010000000000000002
2022-03-21 05:04:18 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.9748
2022-03-21 05:04:55 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 0.9414
2022-03-21 05:05:32 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.1045
2022-03-21 05:06:10 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.2204
2022-03-21 05:06:47 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.0605
2022-03-21 05:07:25 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.0269
2022-03-21 05:08:02 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 0.9678
2022-03-21 05:08:40 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 0.9856
2022-03-21 05:09:17 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.1303
2022-03-21 05:09:55 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.2034
2022-03-21 05:10:33 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.0791
2022-03-21 05:11:10 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 0.9858
2022-03-21 05:11:48 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 0.9255
2022-03-21 05:12:26 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.1300
2022-03-21 05:13:03 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 0.9987
2022-03-21 05:13:41 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.0991
2022-03-21 05:14:19 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.1758
2022-03-21 05:14:57 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.2562
2022-03-21 05:15:34 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 0.9022
2022-03-21 05:16:12 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.7947
2022-03-21 05:16:50 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 0.9828
2022-03-21 05:17:28 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.0000
2022-03-21 05:18:06 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.1020
2022-03-21 05:18:44 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.0422
2022-03-21 05:19:21 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.0931
2022-03-21 05:19:59 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 0.9571
2022-03-21 05:20:37 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.0735
2022-03-21 05:21:15 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 0.9435
2022-03-21 05:21:53 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.0696
2022-03-21 05:22:31 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.1998
2022-03-21 05:23:09 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 0.8851
2022-03-21 05:23:47 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 0.9857
2022-03-21 05:24:25 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.3050
2022-03-21 05:25:03 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.1575
2022-03-21 05:25:40 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.1562
2022-03-21 05:26:18 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.1603
2022-03-21 05:26:56 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.0752
2022-03-21 05:27:34 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 0.9308
2022-03-21 05:28:12 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.1329
2022-03-21 05:28:50 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.2849
2022-03-21 05:29:28 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.0857
2022-03-21 05:30:06 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.3244
2022-03-21 05:30:43 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.1319
2022-03-21 05:31:22 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 0.9624
2022-03-21 05:32:00 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.1734
2022-03-21 05:32:37 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.3405
2022-03-21 05:33:15 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.2311
2022-03-21 05:33:53 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.2451
2022-03-21 05:34:30 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.3274
2022-03-21 05:35:08 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.1240
2022-03-21 05:35:10 - train: epoch 083, train_loss: 1.0809
2022-03-21 05:36:26 - eval: epoch: 083, acc1: 73.972%, acc5: 91.768%, test_loss: 1.0287, per_image_load_time: 2.215ms, per_image_inference_time: 0.723ms
2022-03-21 05:36:28 - until epoch: 083, best_acc1: 73.972%
2022-03-21 05:36:28 - epoch 084 lr: 0.0010000000000000002
2022-03-21 05:37:11 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.0398
2022-03-21 05:37:48 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.1186
2022-03-21 05:38:25 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.8691
2022-03-21 05:39:03 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.0692
2022-03-21 05:39:40 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.0544
2022-03-21 05:40:17 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.3048
2022-03-21 05:40:55 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.1331
2022-03-21 05:41:32 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.2233
2022-03-21 05:42:09 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.0073
2022-03-21 05:42:47 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.2124
2022-03-21 05:43:24 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.0997
2022-03-21 05:44:01 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.2409
2022-03-21 05:44:39 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.0079
2022-03-21 05:45:16 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.0440
2022-03-21 05:45:54 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.1090
2022-03-21 05:46:31 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 0.9563
2022-03-21 05:47:09 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.1547
2022-03-21 05:47:47 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.0246
2022-03-21 05:48:24 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.1578
2022-03-21 05:49:02 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.1662
2022-03-21 05:49:40 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 0.9957
2022-03-21 05:50:18 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 0.8065
2022-03-21 05:50:56 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.1107
2022-03-21 05:51:33 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 0.9613
2022-03-21 05:52:11 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.1729
2022-03-21 05:52:49 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.2012
2022-03-21 05:53:27 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.1049
2022-03-21 05:54:05 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.1993
2022-03-21 05:54:43 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.1053
2022-03-21 05:55:21 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.0022
2022-03-21 05:55:59 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 0.9653
2022-03-21 05:56:37 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 0.9658
2022-03-21 05:57:15 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.0784
2022-03-21 05:57:53 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 0.9882
2022-03-21 05:58:31 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 0.8878
2022-03-21 05:59:09 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.0698
2022-03-21 05:59:47 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.1179
2022-03-21 06:00:25 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.3092
2022-03-21 06:01:04 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.1551
2022-03-21 06:01:42 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.1795
2022-03-21 06:02:20 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.0724
2022-03-21 06:02:58 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.2229
2022-03-21 06:03:36 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.0221
2022-03-21 06:04:15 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.3206
2022-03-21 06:04:53 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.0341
2022-03-21 06:05:31 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.0084
2022-03-21 06:06:09 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.3928
2022-03-21 06:06:47 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.0014
2022-03-21 06:07:24 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 0.8797
2022-03-21 06:08:02 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.1283
2022-03-21 06:08:04 - train: epoch 084, train_loss: 1.0789
2022-03-21 06:09:19 - eval: epoch: 084, acc1: 73.862%, acc5: 91.712%, test_loss: 1.0291, per_image_load_time: 1.646ms, per_image_inference_time: 0.713ms
2022-03-21 06:09:21 - until epoch: 084, best_acc1: 73.972%
2022-03-21 06:09:21 - epoch 085 lr: 0.0010000000000000002
2022-03-21 06:10:04 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 0.9459
2022-03-21 06:10:41 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 0.9107
2022-03-21 06:11:18 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.1953
2022-03-21 06:11:56 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 0.9281
2022-03-21 06:12:33 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.2625
2022-03-21 06:13:11 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.8695
2022-03-21 06:13:48 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 0.9187
2022-03-21 06:14:26 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.0064
2022-03-21 06:15:03 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.0396
2022-03-21 06:15:41 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.1418
2022-03-21 06:16:18 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.1125
2022-03-21 06:16:56 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 0.9646
2022-03-21 06:17:33 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.0704
2022-03-21 06:18:11 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.1371
2022-03-21 06:18:48 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 0.9535
2022-03-21 06:19:26 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.0727
2022-03-21 06:20:03 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.3855
2022-03-21 06:20:41 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.9637
2022-03-21 06:21:19 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 0.9969
2022-03-21 06:21:56 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 0.9645
2022-03-21 06:22:34 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.8582
2022-03-21 06:23:12 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.1169
2022-03-21 06:23:49 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.9528
2022-03-21 06:24:27 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.1191
2022-03-21 06:25:04 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.2742
2022-03-21 06:25:42 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.1203
2022-03-21 06:26:20 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.0085
2022-03-21 06:26:57 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.1065
2022-03-21 06:27:35 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.2084
2022-03-21 06:28:12 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.2400
2022-03-21 06:28:50 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.0273
2022-03-21 06:29:28 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.0302
2022-03-21 06:30:05 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.1061
2022-03-21 06:30:43 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.2524
2022-03-21 06:31:21 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.1367
2022-03-21 06:31:59 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.1074
2022-03-21 06:32:36 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 0.9369
2022-03-21 06:33:14 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.0168
2022-03-21 06:33:52 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.0335
2022-03-21 06:34:30 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.2996
2022-03-21 06:35:08 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.0737
2022-03-21 06:35:46 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.2308
2022-03-21 06:36:23 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 0.9332
2022-03-21 06:37:01 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.0501
2022-03-21 06:37:39 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.1922
2022-03-21 06:38:17 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.1498
2022-03-21 06:38:55 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.3169
2022-03-21 06:39:33 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 0.8748
2022-03-21 06:40:11 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.0887
2022-03-21 06:40:49 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 0.9926
2022-03-21 06:40:51 - train: epoch 085, train_loss: 1.0790
2022-03-21 06:42:08 - eval: epoch: 085, acc1: 73.826%, acc5: 91.700%, test_loss: 1.0326, per_image_load_time: 2.262ms, per_image_inference_time: 0.720ms
2022-03-21 06:42:09 - until epoch: 085, best_acc1: 73.972%
2022-03-21 06:42:09 - epoch 086 lr: 0.0010000000000000002
2022-03-21 06:42:52 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 0.8913
2022-03-21 06:43:29 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 0.9984
2022-03-21 06:44:07 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.2178
2022-03-21 06:44:44 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.1365
2022-03-21 06:45:21 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 0.9845
2022-03-21 06:45:59 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.1185
2022-03-21 06:46:36 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 0.9713
2022-03-21 06:47:14 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.1437
2022-03-21 06:47:52 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.1850
2022-03-21 06:48:29 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.1228
2022-03-21 06:49:07 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.1662
2022-03-21 06:49:44 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 0.9811
2022-03-21 06:50:22 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.0982
2022-03-21 06:50:59 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 0.9616
2022-03-21 06:51:37 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.0004
2022-03-21 06:52:15 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.0731
2022-03-21 06:52:52 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.0411
2022-03-21 06:53:30 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.1279
2022-03-21 06:54:07 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.2076
2022-03-21 06:54:45 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.1126
2022-03-21 06:55:22 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 0.9466
2022-03-21 06:56:00 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.1597
2022-03-21 06:56:38 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.0385
2022-03-21 06:57:16 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 0.9133
2022-03-21 06:57:54 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 0.9535
2022-03-21 06:58:31 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.1209
2022-03-21 06:59:09 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 0.9643
2022-03-21 06:59:47 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.1500
2022-03-21 07:00:25 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 0.9452
2022-03-21 07:01:03 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.1285
2022-03-21 07:01:41 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.0193
2022-03-21 07:02:19 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.1712
2022-03-21 07:02:57 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.1227
2022-03-21 07:03:34 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.1142
2022-03-21 07:04:12 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.0639
2022-03-21 07:04:50 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.1584
2022-03-21 07:05:28 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.2149
2022-03-21 07:06:06 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.1209
2022-03-21 07:06:44 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.1166
2022-03-21 07:07:22 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.0577
2022-03-21 07:08:00 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.0706
2022-03-21 07:08:38 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.1686
2022-03-21 07:09:16 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.1603
2022-03-21 07:09:54 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 0.9963
2022-03-21 07:10:32 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 0.9147
2022-03-21 07:11:10 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.0648
2022-03-21 07:11:48 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.0088
2022-03-21 07:12:26 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.0959
2022-03-21 07:13:04 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.1329
2022-03-21 07:13:42 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.0863
2022-03-21 07:13:44 - train: epoch 086, train_loss: 1.0750
2022-03-21 07:15:00 - eval: epoch: 086, acc1: 73.878%, acc5: 91.692%, test_loss: 1.0320, per_image_load_time: 1.265ms, per_image_inference_time: 0.712ms
2022-03-21 07:15:01 - until epoch: 086, best_acc1: 73.972%
2022-03-21 07:15:01 - epoch 087 lr: 0.0010000000000000002
2022-03-21 07:15:45 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.1499
2022-03-21 07:16:22 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 0.9493
2022-03-21 07:16:59 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.1223
2022-03-21 07:17:37 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.2170
2022-03-21 07:18:14 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 0.8232
2022-03-21 07:18:52 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.0734
2022-03-21 07:19:29 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 0.9483
2022-03-21 07:20:07 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.1077
2022-03-21 07:20:44 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.1190
2022-03-21 07:21:22 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.0000
2022-03-21 07:21:59 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.1504
2022-03-21 07:22:37 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.1633
2022-03-21 07:23:15 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.1607
2022-03-21 07:23:53 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.0476
2022-03-21 07:24:31 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 0.9756
2022-03-21 07:25:09 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 0.8966
2022-03-21 07:25:47 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.2773
2022-03-21 07:26:25 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.0946
2022-03-21 07:27:03 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.1511
2022-03-21 07:27:41 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.0525
2022-03-21 07:28:20 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.1436
2022-03-21 07:28:58 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.0943
2022-03-21 07:29:36 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.1703
2022-03-21 07:30:14 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.0109
2022-03-21 07:30:52 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.0569
2022-03-21 07:31:30 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.1068
2022-03-21 07:32:08 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 0.9481
2022-03-21 07:32:47 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 0.8572
2022-03-21 07:33:25 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 0.9753
2022-03-21 07:34:03 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.0648
2022-03-21 07:34:41 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.0522
2022-03-21 07:35:19 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 0.9582
2022-03-21 07:35:58 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.0388
2022-03-21 07:36:36 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.0380
2022-03-21 07:37:14 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 0.9868
2022-03-21 07:37:52 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 0.9955
2022-03-21 07:38:30 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 0.9836
2022-03-21 07:39:08 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.0933
2022-03-21 07:39:46 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.0714
2022-03-21 07:40:24 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.1747
2022-03-21 07:41:02 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.1221
2022-03-21 07:41:40 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.1071
2022-03-21 07:42:18 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.1523
2022-03-21 07:42:56 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.1587
2022-03-21 07:43:34 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.2102
2022-03-21 07:44:12 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.1034
2022-03-21 07:44:50 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.1542
2022-03-21 07:45:28 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.0738
2022-03-21 07:46:06 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.0288
2022-03-21 07:46:44 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.0321
2022-03-21 07:46:46 - train: epoch 087, train_loss: 1.0718
2022-03-21 07:48:03 - eval: epoch: 087, acc1: 73.872%, acc5: 91.716%, test_loss: 1.0302, per_image_load_time: 2.277ms, per_image_inference_time: 0.725ms
2022-03-21 07:48:04 - until epoch: 087, best_acc1: 73.972%
2022-03-21 07:48:04 - epoch 088 lr: 0.0010000000000000002
2022-03-21 07:48:48 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.0827
2022-03-21 07:49:25 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 0.9864
2022-03-21 07:50:03 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.1125
2022-03-21 07:50:40 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.0580
2022-03-21 07:51:17 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 0.9656
2022-03-21 07:51:55 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.0821
2022-03-21 07:52:32 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.0446
2022-03-21 07:53:09 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.0374
2022-03-21 07:53:47 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.0912
2022-03-21 07:54:24 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.0788
2022-03-21 07:55:01 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.0605
2022-03-21 07:55:38 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.1402
2022-03-21 07:56:16 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.1961
2022-03-21 07:56:53 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.0079
2022-03-21 07:57:31 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.0486
2022-03-21 07:58:08 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 0.9340
2022-03-21 07:58:45 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.1593
2022-03-21 07:59:23 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.0326
2022-03-21 08:00:00 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.1065
2022-03-21 08:00:38 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 0.9577
2022-03-21 08:01:16 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.0191
2022-03-21 08:01:53 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 0.9963
2022-03-21 08:02:31 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 0.9554
2022-03-21 08:03:09 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.1649
2022-03-21 08:03:47 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.2396
2022-03-21 08:04:24 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.1079
2022-03-21 08:05:02 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.0915
2022-03-21 08:05:40 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.0415
2022-03-21 08:06:18 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.0017
