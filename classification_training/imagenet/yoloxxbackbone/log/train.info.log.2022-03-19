2022-03-19 08:06:25 - network: yoloxxbackbone
2022-03-19 08:06:25 - num_classes: 1000
2022-03-19 08:06:25 - input_image_size: 256
2022-03-19 08:06:25 - scale: 1.1428571428571428
2022-03-19 08:06:25 - trained_model_path: 
2022-03-19 08:06:25 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-19 08:06:25 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f3b74b94f70>
2022-03-19 08:06:25 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f3b5afc2280>
2022-03-19 08:06:25 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f3b5afc22b0>
2022-03-19 08:06:25 - seed: 0
2022-03-19 08:06:25 - batch_size: 256
2022-03-19 08:06:25 - num_workers: 16
2022-03-19 08:06:25 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-19 08:06:25 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-03-19 08:06:25 - epochs: 100
2022-03-19 08:06:25 - print_interval: 100
2022-03-19 08:06:25 - distributed: True
2022-03-19 08:06:25 - sync_bn: False
2022-03-19 08:06:25 - apex: True
2022-03-19 08:06:25 - gpus_type: NVIDIA RTX A5000
2022-03-19 08:06:25 - gpus_num: 2
2022-03-19 08:06:25 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f3b53d94ab0>
2022-03-19 08:06:25 - --------------------parameters--------------------
2022-03-19 08:06:25 - name: conv.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: conv.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: conv.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-03-19 08:06:25 - name: fc.weight, grad: True
2022-03-19 08:06:25 - name: fc.bias, grad: True
2022-03-19 08:06:25 - --------------------buffers--------------------
2022-03-19 08:06:25 - name: conv.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: conv.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: conv.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer1.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.8.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.9.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.10.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer2.1.bottlenecks.11.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.6.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.7.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.8.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.9.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.10.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer3.1.bottlenecks.11.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-03-19 08:06:25 - name: layer4.2.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-03-19 08:06:25 - epoch 001 lr: 0.1
2022-03-19 08:07:08 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9382
2022-03-19 08:07:45 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8584
2022-03-19 08:08:22 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8111
2022-03-19 08:08:59 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8251
2022-03-19 08:09:36 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.7603
2022-03-19 08:10:14 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.6697
2022-03-19 08:10:51 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.6681
2022-03-19 08:11:28 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.6453
2022-03-19 08:12:05 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.5490
2022-03-19 08:12:42 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.5634
2022-03-19 08:13:19 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.4947
2022-03-19 08:13:56 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 6.4289
2022-03-19 08:14:33 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 6.4529
2022-03-19 08:15:11 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 6.3998
2022-03-19 08:15:48 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 6.3038
2022-03-19 08:16:25 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 6.3945
2022-03-19 08:17:02 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 6.1376
2022-03-19 08:17:40 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 6.1564
2022-03-19 08:18:17 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 6.0548
2022-03-19 08:18:55 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.9823
2022-03-19 08:19:32 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.8463
2022-03-19 08:20:09 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.8377
2022-03-19 08:20:47 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.7710
2022-03-19 08:21:24 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.6883
2022-03-19 08:22:02 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.6801
2022-03-19 08:22:39 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.7958
2022-03-19 08:23:16 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.7019
2022-03-19 08:23:53 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.5226
2022-03-19 08:24:31 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 5.3690
2022-03-19 08:25:08 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.4967
2022-03-19 08:25:45 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.4534
2022-03-19 08:26:23 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 5.2477
2022-03-19 08:27:00 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 5.1313
2022-03-19 08:27:37 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 5.1598
2022-03-19 08:28:14 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 5.1369
2022-03-19 08:28:52 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 5.2221
2022-03-19 08:29:29 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 5.1453
2022-03-19 08:30:07 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 5.0248
2022-03-19 08:30:44 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 5.0348
2022-03-19 08:31:21 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 5.0302
2022-03-19 08:31:59 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 5.1197
2022-03-19 08:32:36 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.9240
2022-03-19 08:33:14 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.8777
2022-03-19 08:33:51 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.6820
2022-03-19 08:34:29 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.9375
2022-03-19 08:35:06 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.8680
2022-03-19 08:35:43 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.6887
2022-03-19 08:36:21 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.9682
2022-03-19 08:36:58 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.7658
2022-03-19 08:37:36 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.5200
2022-03-19 08:37:38 - train: epoch 001, train_loss: 5.7465
2022-03-19 08:38:51 - eval: epoch: 001, acc1: 13.436%, acc5: 32.248%, test_loss: 4.4759, per_image_load_time: 1.442ms, per_image_inference_time: 0.737ms
2022-03-19 08:38:52 - until epoch: 001, best_acc1: 13.436%
2022-03-19 08:38:52 - epoch 002 lr: 0.1
2022-03-19 08:39:35 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.7656
2022-03-19 08:40:12 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.4500
2022-03-19 08:40:49 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.7430
2022-03-19 08:41:26 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.7015
2022-03-19 08:42:04 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.4062
2022-03-19 08:42:41 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.3835
2022-03-19 08:43:18 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.6041
2022-03-19 08:43:56 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.2635
2022-03-19 08:44:33 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 4.2421
2022-03-19 08:45:10 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.6615
2022-03-19 08:45:48 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.4291
2022-03-19 08:46:25 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.3651
2022-03-19 08:47:02 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.1839
2022-03-19 08:47:40 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.4679
2022-03-19 08:48:17 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.5123
2022-03-19 08:48:54 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.1970
2022-03-19 08:49:32 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.2477
2022-03-19 08:50:09 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2508
2022-03-19 08:50:47 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.2661
2022-03-19 08:51:24 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.9909
2022-03-19 08:52:02 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.2733
2022-03-19 08:52:39 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.8740
2022-03-19 08:53:16 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.2772
2022-03-19 08:53:54 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.9650
2022-03-19 08:54:31 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 4.0656
2022-03-19 08:55:09 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 4.0324
2022-03-19 08:55:46 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.2020
2022-03-19 08:56:24 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 4.1140
2022-03-19 08:57:01 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.8740
2022-03-19 08:57:39 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.8585
2022-03-19 08:58:16 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.9008
2022-03-19 08:58:53 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.9672
2022-03-19 08:59:31 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.8841
2022-03-19 09:00:08 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 4.1394
2022-03-19 09:00:46 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.8030
2022-03-19 09:01:23 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.9019
2022-03-19 09:02:00 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 4.0040
2022-03-19 09:02:38 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.6602
2022-03-19 09:03:15 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.9368
2022-03-19 09:03:52 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.8037
2022-03-19 09:04:30 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.9942
2022-03-19 09:05:07 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.8188
2022-03-19 09:05:45 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.7615
2022-03-19 09:06:22 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.6644
2022-03-19 09:07:00 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.7273
2022-03-19 09:07:37 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.7554
2022-03-19 09:08:15 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.7653
2022-03-19 09:08:52 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.7145
2022-03-19 09:09:30 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.6367
2022-03-19 09:10:07 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.6485
2022-03-19 09:10:09 - train: epoch 002, train_loss: 4.1213
2022-03-19 09:11:23 - eval: epoch: 002, acc1: 25.396%, acc5: 50.634%, test_loss: 3.5605, per_image_load_time: 2.114ms, per_image_inference_time: 0.719ms
2022-03-19 09:11:25 - until epoch: 002, best_acc1: 25.396%
2022-03-19 09:11:25 - epoch 003 lr: 0.1
2022-03-19 09:12:07 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.7590
2022-03-19 09:12:44 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.6901
2022-03-19 09:13:22 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.7143
2022-03-19 09:13:59 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.6156
2022-03-19 09:14:36 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.7973
2022-03-19 09:15:13 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.5436
2022-03-19 09:15:51 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.8851
2022-03-19 09:16:28 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.7124
2022-03-19 09:17:05 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.6386
2022-03-19 09:17:43 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.6330
2022-03-19 09:18:20 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.4651
2022-03-19 09:18:57 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.5289
2022-03-19 09:19:35 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.6487
2022-03-19 09:20:12 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.4905
2022-03-19 09:20:49 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.7843
2022-03-19 09:21:27 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.5225
2022-03-19 09:22:04 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.3876
2022-03-19 09:22:41 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.3902
2022-03-19 09:23:18 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.7248
2022-03-19 09:23:56 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.8381
2022-03-19 09:24:33 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.6670
2022-03-19 09:25:10 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.8530
2022-03-19 09:25:48 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.4735
2022-03-19 09:26:25 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.4646
2022-03-19 09:27:02 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.5585
2022-03-19 09:27:40 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.4422
2022-03-19 09:28:17 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.7375
2022-03-19 09:28:54 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.2831
2022-03-19 09:29:31 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.3799
2022-03-19 09:30:09 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.6698
2022-03-19 09:30:46 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.5929
2022-03-19 09:31:23 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.4634
2022-03-19 09:32:01 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.4626
2022-03-19 09:32:38 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.6279
2022-03-19 09:33:15 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.3193
2022-03-19 09:33:53 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.3981
2022-03-19 09:34:30 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.3275
2022-03-19 09:35:08 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.4788
2022-03-19 09:35:45 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.5912
2022-03-19 09:36:22 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2462
2022-03-19 09:37:00 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.4238
2022-03-19 09:37:37 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.3280
2022-03-19 09:38:14 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.1027
2022-03-19 09:38:52 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.1114
2022-03-19 09:39:29 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.1689
2022-03-19 09:40:06 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.1703
2022-03-19 09:40:44 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.2502
2022-03-19 09:41:21 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.3588
2022-03-19 09:41:59 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.4388
2022-03-19 09:42:36 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.3642
2022-03-19 09:42:38 - train: epoch 003, train_loss: 3.4807
2022-03-19 09:43:51 - eval: epoch: 003, acc1: 34.428%, acc5: 60.624%, test_loss: 3.0248, per_image_load_time: 1.324ms, per_image_inference_time: 0.712ms
2022-03-19 09:43:53 - until epoch: 003, best_acc1: 34.428%
2022-03-19 09:43:53 - epoch 004 lr: 0.1
2022-03-19 09:44:36 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.3211
2022-03-19 09:45:13 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.1998
2022-03-19 09:45:50 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.1556
2022-03-19 09:46:27 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.1983
2022-03-19 09:47:05 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.1033
2022-03-19 09:47:42 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.4254
2022-03-19 09:48:19 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.3458
2022-03-19 09:48:57 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.0064
2022-03-19 09:49:34 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.9643
2022-03-19 09:50:12 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.2424
2022-03-19 09:50:49 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.2437
2022-03-19 09:51:26 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.0354
2022-03-19 09:52:04 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.1634
2022-03-19 09:52:41 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.2007
2022-03-19 09:53:19 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.2586
2022-03-19 09:53:56 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.0795
2022-03-19 09:54:34 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1921
2022-03-19 09:55:11 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.3592
2022-03-19 09:55:48 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.2349
2022-03-19 09:56:26 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.1416
2022-03-19 09:57:03 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.2149
2022-03-19 09:57:41 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.1211
2022-03-19 09:58:18 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.9525
2022-03-19 09:58:56 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.9514
2022-03-19 09:59:33 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.0202
2022-03-19 10:00:11 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.1896
2022-03-19 10:00:48 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.9782
2022-03-19 10:01:26 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.0802
2022-03-19 10:02:03 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.0636
2022-03-19 10:02:41 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.0967
2022-03-19 10:03:18 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.1670
2022-03-19 10:03:56 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.0882
2022-03-19 10:04:33 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.0492
2022-03-19 10:05:11 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.0881
2022-03-19 10:05:49 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.0003
2022-03-19 10:06:26 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.8719
2022-03-19 10:07:04 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.0351
2022-03-19 10:07:41 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.9214
2022-03-19 10:08:19 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.9773
2022-03-19 10:08:57 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.7550
2022-03-19 10:09:34 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.9494
2022-03-19 10:10:12 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.9534
2022-03-19 10:10:49 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9569
2022-03-19 10:11:27 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.8393
2022-03-19 10:12:04 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.6662
2022-03-19 10:12:42 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.0081
2022-03-19 10:13:20 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.9003
2022-03-19 10:13:57 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.8342
2022-03-19 10:14:35 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.9388
2022-03-19 10:15:12 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.0555
2022-03-19 10:15:14 - train: epoch 004, train_loss: 3.1378
2022-03-19 10:16:28 - eval: epoch: 004, acc1: 38.540%, acc5: 65.044%, test_loss: 2.7769, per_image_load_time: 2.152ms, per_image_inference_time: 0.707ms
2022-03-19 10:16:30 - until epoch: 004, best_acc1: 38.540%
2022-03-19 10:16:30 - epoch 005 lr: 0.1
2022-03-19 10:17:13 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.0537
2022-03-19 10:17:51 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0435
2022-03-19 10:18:29 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0909
2022-03-19 10:19:06 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.0018
2022-03-19 10:19:44 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.8884
2022-03-19 10:20:22 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.9423
2022-03-19 10:21:00 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.9692
2022-03-19 10:21:37 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.2411
2022-03-19 10:22:15 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.9046
2022-03-19 10:22:53 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.9875
2022-03-19 10:23:31 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.0913
2022-03-19 10:24:09 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.9888
2022-03-19 10:24:47 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.9708
2022-03-19 10:25:24 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0982
2022-03-19 10:26:02 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.8068
2022-03-19 10:26:40 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.8286
2022-03-19 10:27:18 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.7676
2022-03-19 10:27:56 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.9127
2022-03-19 10:28:33 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6833
2022-03-19 10:29:11 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9214
2022-03-19 10:29:49 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7736
2022-03-19 10:30:27 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.8895
2022-03-19 10:31:04 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.8170
2022-03-19 10:31:42 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.8967
2022-03-19 10:32:20 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.9205
2022-03-19 10:32:58 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.0934
2022-03-19 10:33:36 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.0217
2022-03-19 10:34:13 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.9439
2022-03-19 10:34:51 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.7769
2022-03-19 10:35:29 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.8871
2022-03-19 10:36:07 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.0497
2022-03-19 10:36:45 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.9494
2022-03-19 10:37:23 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.7857
2022-03-19 10:38:00 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.8167
2022-03-19 10:38:38 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.0397
2022-03-19 10:39:16 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8822
2022-03-19 10:39:54 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.7480
2022-03-19 10:40:32 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.7375
2022-03-19 10:41:10 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.2394
2022-03-19 10:41:47 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.7896
2022-03-19 10:42:25 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.9192
2022-03-19 10:43:03 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.8380
2022-03-19 10:43:41 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.8056
2022-03-19 10:44:18 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.9618
2022-03-19 10:44:56 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.9491
2022-03-19 10:45:34 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.8305
2022-03-19 10:46:11 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.6274
2022-03-19 10:46:49 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.7442
2022-03-19 10:47:27 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8870
2022-03-19 10:48:05 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.7720
2022-03-19 10:48:07 - train: epoch 005, train_loss: 2.9274
2022-03-19 10:49:20 - eval: epoch: 005, acc1: 41.594%, acc5: 68.272%, test_loss: 2.5916, per_image_load_time: 1.525ms, per_image_inference_time: 0.725ms
2022-03-19 10:49:21 - until epoch: 005, best_acc1: 41.594%
2022-03-19 10:49:21 - epoch 006 lr: 0.1
2022-03-19 10:50:04 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.7347
2022-03-19 10:50:41 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.8515
2022-03-19 10:51:18 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.5764
2022-03-19 10:51:55 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.8662
2022-03-19 10:52:33 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.9042
2022-03-19 10:53:10 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.8104
2022-03-19 10:53:47 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.0309
2022-03-19 10:54:24 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8981
2022-03-19 10:55:01 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.7884
2022-03-19 10:55:39 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.7531
2022-03-19 10:56:16 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.6699
2022-03-19 10:56:53 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.7994
2022-03-19 10:57:30 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.1080
2022-03-19 10:58:08 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.8923
2022-03-19 10:58:45 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.9760
2022-03-19 10:59:22 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.6248
2022-03-19 10:59:59 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8695
2022-03-19 11:00:37 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.9346
2022-03-19 11:01:14 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.6186
2022-03-19 11:01:51 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.8737
2022-03-19 11:02:28 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.9056
2022-03-19 11:03:05 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.7617
2022-03-19 11:03:43 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.5710
2022-03-19 11:04:20 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.8434
2022-03-19 11:04:57 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9888
2022-03-19 11:05:35 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.6973
2022-03-19 11:06:12 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.9328
2022-03-19 11:06:49 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.5618
2022-03-19 11:07:26 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.9532
2022-03-19 11:08:04 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6553
2022-03-19 11:08:41 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.6278
2022-03-19 11:09:19 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.6364
2022-03-19 11:09:56 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5786
2022-03-19 11:10:34 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.9159
2022-03-19 11:11:11 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.8216
2022-03-19 11:11:49 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.8514
2022-03-19 11:12:26 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.7446
2022-03-19 11:13:03 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.5830
2022-03-19 11:13:41 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.7014
2022-03-19 11:14:18 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.0100
2022-03-19 11:14:56 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.7429
2022-03-19 11:15:33 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.6628
2022-03-19 11:16:11 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.7187
2022-03-19 11:16:48 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.7747
2022-03-19 11:17:26 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.7141
2022-03-19 11:18:03 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.7238
2022-03-19 11:18:41 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.8506
2022-03-19 11:19:18 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.7819
2022-03-19 11:19:56 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.8312
2022-03-19 11:20:33 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.4216
2022-03-19 11:20:35 - train: epoch 006, train_loss: 2.7897
2022-03-19 11:21:50 - eval: epoch: 006, acc1: 44.564%, acc5: 71.214%, test_loss: 2.4280, per_image_load_time: 1.180ms, per_image_inference_time: 0.731ms
2022-03-19 11:21:51 - until epoch: 006, best_acc1: 44.564%
2022-03-19 11:21:51 - epoch 007 lr: 0.1
2022-03-19 11:22:34 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.5281
2022-03-19 11:23:11 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.9413
2022-03-19 11:23:48 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.0121
2022-03-19 11:24:26 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7178
2022-03-19 11:25:03 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.6182
2022-03-19 11:25:40 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.9019
2022-03-19 11:26:18 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.7613
2022-03-19 11:26:55 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.7853
2022-03-19 11:27:33 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.7628
2022-03-19 11:28:10 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.6679
2022-03-19 11:28:47 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.4935
2022-03-19 11:29:25 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.7997
2022-03-19 11:30:02 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.5434
2022-03-19 11:30:39 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.8197
2022-03-19 11:31:17 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.7995
2022-03-19 11:31:54 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.6740
2022-03-19 11:32:32 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7469
2022-03-19 11:33:09 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.6107
2022-03-19 11:33:47 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7464
2022-03-19 11:34:25 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.5947
2022-03-19 11:35:02 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.8514
2022-03-19 11:35:40 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.4006
2022-03-19 11:36:18 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.7669
2022-03-19 11:36:55 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.7426
2022-03-19 11:37:33 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.6225
2022-03-19 11:38:10 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.6671
2022-03-19 11:38:48 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.6148
2022-03-19 11:39:25 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6520
2022-03-19 11:40:03 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.4839
2022-03-19 11:40:40 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.7588
2022-03-19 11:41:18 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.5776
2022-03-19 11:41:55 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.6448
2022-03-19 11:42:33 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.0355
2022-03-19 11:43:10 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.6497
2022-03-19 11:43:48 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.8434
2022-03-19 11:44:25 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.5435
2022-03-19 11:45:03 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.7297
2022-03-19 11:45:40 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.8328
2022-03-19 11:46:18 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.6280
2022-03-19 11:46:55 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.8077
2022-03-19 11:47:33 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.5608
2022-03-19 11:48:11 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.5372
2022-03-19 11:48:48 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.9484
2022-03-19 11:49:25 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.5643
2022-03-19 11:50:03 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.7909
2022-03-19 11:50:41 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.5681
2022-03-19 11:51:18 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.7861
2022-03-19 11:51:56 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.8341
2022-03-19 11:52:34 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.7125
2022-03-19 11:53:11 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.6616
2022-03-19 11:53:13 - train: epoch 007, train_loss: 2.6904
2022-03-19 11:54:27 - eval: epoch: 007, acc1: 45.064%, acc5: 71.980%, test_loss: 2.3841, per_image_load_time: 2.036ms, per_image_inference_time: 0.703ms
2022-03-19 11:54:29 - until epoch: 007, best_acc1: 45.064%
2022-03-19 11:54:29 - epoch 008 lr: 0.1
2022-03-19 11:55:12 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6772
2022-03-19 11:55:49 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.7794
2022-03-19 11:56:26 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.4554
2022-03-19 11:57:03 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.5134
2022-03-19 11:57:41 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.4531
2022-03-19 11:58:18 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.5852
2022-03-19 11:58:56 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.9942
2022-03-19 11:59:33 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.5526
2022-03-19 12:00:11 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.5562
2022-03-19 12:00:48 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.7429
2022-03-19 12:01:26 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.4371
2022-03-19 12:02:03 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.5131
2022-03-19 12:02:41 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.7146
2022-03-19 12:03:18 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.4981
2022-03-19 12:03:56 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.8249
2022-03-19 12:04:34 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.6832
2022-03-19 12:05:12 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.4224
2022-03-19 12:05:50 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7413
2022-03-19 12:06:27 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.4630
2022-03-19 12:07:05 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.7373
2022-03-19 12:07:43 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.5895
2022-03-19 12:08:21 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.5829
2022-03-19 12:08:58 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.6616
2022-03-19 12:09:36 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.5199
2022-03-19 12:10:14 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.6463
2022-03-19 12:10:52 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.7288
2022-03-19 12:11:30 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.7481
2022-03-19 12:12:08 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.7232
2022-03-19 12:12:45 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.6653
2022-03-19 12:13:23 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.8114
2022-03-19 12:14:01 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.4954
2022-03-19 12:14:39 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.8902
2022-03-19 12:15:17 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.8469
2022-03-19 12:15:55 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.8391
2022-03-19 12:16:33 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.6075
2022-03-19 12:17:10 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7544
2022-03-19 12:17:48 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.5714
2022-03-19 12:18:26 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.4264
2022-03-19 12:19:04 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7666
2022-03-19 12:19:42 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.0301
2022-03-19 12:20:20 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5518
2022-03-19 12:20:58 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5865
2022-03-19 12:21:36 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.2969
2022-03-19 12:22:14 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.6488
2022-03-19 12:22:52 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.7842
2022-03-19 12:23:30 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.8028
2022-03-19 12:24:08 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.4283
2022-03-19 12:24:46 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.5561
2022-03-19 12:25:24 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.5195
2022-03-19 12:26:02 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.5084
2022-03-19 12:26:04 - train: epoch 008, train_loss: 2.6196
2022-03-19 12:27:17 - eval: epoch: 008, acc1: 47.438%, acc5: 73.376%, test_loss: 2.2794, per_image_load_time: 1.891ms, per_image_inference_time: 0.722ms
2022-03-19 12:27:19 - until epoch: 008, best_acc1: 47.438%
2022-03-19 12:27:19 - epoch 009 lr: 0.1
2022-03-19 12:28:02 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3369
2022-03-19 12:28:39 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.3846
2022-03-19 12:29:16 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3182
2022-03-19 12:29:54 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7234
2022-03-19 12:30:31 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.4830
2022-03-19 12:31:09 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.6648
2022-03-19 12:31:46 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.5298
2022-03-19 12:32:24 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.4441
2022-03-19 12:33:01 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.5194
2022-03-19 12:33:38 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.3838
2022-03-19 12:34:16 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.9049
2022-03-19 12:34:54 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.6922
2022-03-19 12:35:31 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.6473
2022-03-19 12:36:09 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.3086
2022-03-19 12:36:47 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.5646
2022-03-19 12:37:25 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.6210
2022-03-19 12:38:02 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.7025
2022-03-19 12:38:40 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.6238
2022-03-19 12:39:18 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.3232
2022-03-19 12:39:56 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.3461
2022-03-19 12:40:33 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.6123
2022-03-19 12:41:11 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.6676
2022-03-19 12:41:49 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.3076
2022-03-19 12:42:27 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.4768
2022-03-19 12:43:05 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.3955
2022-03-19 12:43:42 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.6236
2022-03-19 12:44:20 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.4907
2022-03-19 12:44:58 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.6761
2022-03-19 12:45:36 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.2443
2022-03-19 12:46:13 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.5451
2022-03-19 12:46:51 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.6677
2022-03-19 12:47:29 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.6071
2022-03-19 12:48:07 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5231
2022-03-19 12:48:45 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8252
2022-03-19 12:49:23 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6243
2022-03-19 12:50:00 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.4412
2022-03-19 12:50:38 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.6893
2022-03-19 12:51:16 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.7438
2022-03-19 12:51:54 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.3138
2022-03-19 12:52:31 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.7935
2022-03-19 12:53:09 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.6290
2022-03-19 12:53:47 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.5035
2022-03-19 12:54:24 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.6666
2022-03-19 12:55:02 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.4862
2022-03-19 12:55:40 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.5965
2022-03-19 12:56:17 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.6375
2022-03-19 12:56:55 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.6305
2022-03-19 12:57:33 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.7188
2022-03-19 12:58:10 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.5635
2022-03-19 12:58:48 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.5041
2022-03-19 12:58:50 - train: epoch 009, train_loss: 2.5656
2022-03-19 13:00:04 - eval: epoch: 009, acc1: 47.740%, acc5: 73.946%, test_loss: 2.2687, per_image_load_time: 1.609ms, per_image_inference_time: 0.721ms
2022-03-19 13:00:06 - until epoch: 009, best_acc1: 47.740%
2022-03-19 13:00:06 - epoch 010 lr: 0.1
2022-03-19 13:00:48 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.4481
2022-03-19 13:01:25 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.6094
2022-03-19 13:02:02 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.6001
2022-03-19 13:02:40 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5794
2022-03-19 13:03:17 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.4742
2022-03-19 13:03:54 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.5632
2022-03-19 13:04:32 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.5533
2022-03-19 13:05:09 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.4038
2022-03-19 13:05:46 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.3098
2022-03-19 13:06:23 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.2792
2022-03-19 13:07:01 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5663
2022-03-19 13:07:38 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.3664
2022-03-19 13:08:15 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.3776
2022-03-19 13:08:53 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.5363
2022-03-19 13:09:30 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.1527
2022-03-19 13:10:07 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.6509
2022-03-19 13:10:44 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.7833
2022-03-19 13:11:22 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.6262
2022-03-19 13:11:59 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.5723
2022-03-19 13:12:36 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.6031
2022-03-19 13:13:13 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.4590
2022-03-19 13:13:51 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.7166
2022-03-19 13:14:28 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.7070
2022-03-19 13:15:06 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.6929
2022-03-19 13:15:43 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.4757
2022-03-19 13:16:21 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.5902
2022-03-19 13:16:59 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.3873
2022-03-19 13:17:37 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.4626
2022-03-19 13:18:14 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.6283
2022-03-19 13:18:52 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.3591
2022-03-19 13:19:30 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.7336
2022-03-19 13:20:08 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.5700
2022-03-19 13:20:46 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.6805
2022-03-19 13:21:24 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.6429
2022-03-19 13:22:02 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.7128
2022-03-19 13:22:39 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.8671
2022-03-19 13:23:17 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.4555
2022-03-19 13:23:55 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.4920
2022-03-19 13:24:33 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.2950
2022-03-19 13:25:11 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.4898
2022-03-19 13:25:49 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.3529
2022-03-19 13:26:26 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.6458
2022-03-19 13:27:04 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.4960
2022-03-19 13:27:42 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.4993
2022-03-19 13:28:20 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.4221
2022-03-19 13:28:58 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.5614
2022-03-19 13:29:36 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.4513
2022-03-19 13:30:13 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.4732
2022-03-19 13:30:51 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.2547
2022-03-19 13:31:29 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.2681
2022-03-19 13:31:31 - train: epoch 010, train_loss: 2.5196
2022-03-19 13:32:46 - eval: epoch: 010, acc1: 48.374%, acc5: 74.338%, test_loss: 2.2328, per_image_load_time: 2.195ms, per_image_inference_time: 0.709ms
2022-03-19 13:32:48 - until epoch: 010, best_acc1: 48.374%
2022-03-19 13:32:48 - epoch 011 lr: 0.1
2022-03-19 13:33:30 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.2491
2022-03-19 13:34:08 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.6672
2022-03-19 13:34:46 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.2340
2022-03-19 13:35:24 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.4419
2022-03-19 13:36:01 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.4449
2022-03-19 13:36:39 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.4802
2022-03-19 13:37:17 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4010
2022-03-19 13:37:55 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.4956
2022-03-19 13:38:33 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.6555
2022-03-19 13:39:10 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.4276
2022-03-19 13:39:48 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.5436
2022-03-19 13:40:26 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.9133
2022-03-19 13:41:03 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.5837
2022-03-19 13:41:41 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.5553
2022-03-19 13:42:19 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.3280
2022-03-19 13:42:57 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.4883
2022-03-19 13:43:35 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.4115
2022-03-19 13:44:12 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.3470
2022-03-19 13:44:50 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.3846
2022-03-19 13:45:28 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.5857
2022-03-19 13:46:06 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.4441
2022-03-19 13:46:43 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.3718
2022-03-19 13:47:21 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.6751
2022-03-19 13:47:59 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.3480
2022-03-19 13:48:37 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.7516
2022-03-19 13:49:14 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.4599
2022-03-19 13:49:52 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5145
2022-03-19 13:50:30 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.1298
2022-03-19 13:51:08 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.6111
2022-03-19 13:51:46 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.7298
2022-03-19 13:52:23 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.6323
2022-03-19 13:53:01 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.2414
2022-03-19 13:53:39 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.5535
2022-03-19 13:54:17 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.3833
2022-03-19 13:54:54 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.5063
2022-03-19 13:55:32 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.5884
2022-03-19 13:56:10 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.6106
2022-03-19 13:56:48 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.2110
2022-03-19 13:57:25 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.5077
2022-03-19 13:58:03 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.4385
2022-03-19 13:58:41 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.3149
2022-03-19 13:59:19 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.4339
2022-03-19 13:59:57 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.5560
2022-03-19 14:00:34 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.4486
2022-03-19 14:01:13 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.3513
2022-03-19 14:01:51 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.3048
2022-03-19 14:02:28 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.2445
2022-03-19 14:03:07 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.2435
2022-03-19 14:03:45 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.3267
2022-03-19 14:04:23 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.3984
2022-03-19 14:04:25 - train: epoch 011, train_loss: 2.4823
2022-03-19 14:05:40 - eval: epoch: 011, acc1: 49.954%, acc5: 76.398%, test_loss: 2.1310, per_image_load_time: 0.990ms, per_image_inference_time: 0.707ms
2022-03-19 14:05:41 - until epoch: 011, best_acc1: 49.954%
2022-03-19 14:05:41 - epoch 012 lr: 0.1
2022-03-19 14:06:25 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.3023
2022-03-19 14:07:02 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.3478
2022-03-19 14:07:40 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.4228
2022-03-19 14:08:18 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.5640
2022-03-19 14:08:55 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.6535
2022-03-19 14:09:33 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.1967
2022-03-19 14:10:11 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.4233
2022-03-19 14:10:48 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.4007
2022-03-19 14:11:26 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.5864
2022-03-19 14:12:04 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.3298
2022-03-19 14:12:42 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8363
2022-03-19 14:13:20 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.3038
2022-03-19 14:13:58 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.3287
2022-03-19 14:14:36 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.5818
2022-03-19 14:15:14 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.2184
2022-03-19 14:15:52 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.4447
2022-03-19 14:16:30 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.3148
2022-03-19 14:17:07 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.4053
2022-03-19 14:17:45 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.5510
2022-03-19 14:18:23 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6228
2022-03-19 14:19:01 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.4804
2022-03-19 14:19:39 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6142
2022-03-19 14:20:17 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.5159
2022-03-19 14:20:55 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.5264
2022-03-19 14:21:33 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.2223
2022-03-19 14:22:11 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.3083
2022-03-19 14:22:49 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.3917
2022-03-19 14:23:27 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.3178
2022-03-19 14:24:05 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3310
2022-03-19 14:24:43 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.2739
2022-03-19 14:25:21 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.4772
2022-03-19 14:25:59 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1587
2022-03-19 14:26:37 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3598
2022-03-19 14:27:15 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.5086
2022-03-19 14:27:53 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.6109
2022-03-19 14:28:31 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.4941
2022-03-19 14:29:09 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.4484
2022-03-19 14:29:47 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.3575
2022-03-19 14:30:25 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.3537
2022-03-19 14:31:03 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.2937
2022-03-19 14:31:41 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.4706
2022-03-19 14:32:19 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.3003
2022-03-19 14:32:57 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.5468
2022-03-19 14:33:35 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.2666
2022-03-19 14:34:13 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.4099
2022-03-19 14:34:51 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.8061
2022-03-19 14:35:29 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3742
2022-03-19 14:36:07 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.5817
2022-03-19 14:36:45 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.4913
2022-03-19 14:37:23 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.2313
2022-03-19 14:37:25 - train: epoch 012, train_loss: 2.4533
2022-03-19 14:38:40 - eval: epoch: 012, acc1: 49.986%, acc5: 76.326%, test_loss: 2.1315, per_image_load_time: 1.605ms, per_image_inference_time: 0.709ms
2022-03-19 14:38:41 - until epoch: 012, best_acc1: 49.986%
2022-03-19 14:38:41 - epoch 013 lr: 0.1
2022-03-19 14:39:25 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.1776
2022-03-19 14:40:03 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.4782
2022-03-19 14:40:40 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.3200
2022-03-19 14:41:18 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2875
2022-03-19 14:41:55 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.3203
2022-03-19 14:42:33 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.5230
2022-03-19 14:43:11 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.2602
2022-03-19 14:43:49 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.5191
2022-03-19 14:44:27 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.3724
2022-03-19 14:45:05 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.5604
2022-03-19 14:45:42 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.5293
2022-03-19 14:46:20 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.6452
2022-03-19 14:46:58 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.5009
2022-03-19 14:47:36 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.4205
2022-03-19 14:48:13 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.6121
2022-03-19 14:48:51 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.0912
2022-03-19 14:49:29 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.4359
2022-03-19 14:50:07 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.5454
2022-03-19 14:50:45 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.4549
2022-03-19 14:51:23 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.6470
2022-03-19 14:52:01 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.7114
2022-03-19 14:52:39 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3767
2022-03-19 14:53:17 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.4648
2022-03-19 14:53:54 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.4669
2022-03-19 14:54:32 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.3156
2022-03-19 14:55:10 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.3817
2022-03-19 14:55:48 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.3260
2022-03-19 14:56:26 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4103
2022-03-19 14:57:04 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.4831
2022-03-19 14:57:42 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.3041
2022-03-19 14:58:20 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.2729
2022-03-19 14:58:58 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.3498
2022-03-19 14:59:35 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.2450
2022-03-19 15:00:13 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.3795
2022-03-19 15:00:51 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.2764
2022-03-19 15:01:29 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6474
2022-03-19 15:02:07 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.1800
2022-03-19 15:02:45 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.5394
2022-03-19 15:03:22 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.4599
2022-03-19 15:04:00 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.4188
2022-03-19 15:04:38 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.3161
2022-03-19 15:05:16 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.3712
2022-03-19 15:05:54 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.3830
2022-03-19 15:06:32 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.3637
2022-03-19 15:07:10 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.4885
2022-03-19 15:07:48 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.4034
2022-03-19 15:08:26 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5931
2022-03-19 15:09:04 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.5747
2022-03-19 15:09:42 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.5475
2022-03-19 15:10:20 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.4829
2022-03-19 15:10:22 - train: epoch 013, train_loss: 2.4276
2022-03-19 15:11:37 - eval: epoch: 013, acc1: 50.646%, acc5: 76.764%, test_loss: 2.1014, per_image_load_time: 1.254ms, per_image_inference_time: 0.727ms
2022-03-19 15:11:38 - until epoch: 013, best_acc1: 50.646%
2022-03-19 15:11:38 - epoch 014 lr: 0.1
2022-03-19 15:12:22 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2809
2022-03-19 15:13:00 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5575
2022-03-19 15:13:37 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.0942
2022-03-19 15:14:15 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.2586
2022-03-19 15:14:52 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.3026
2022-03-19 15:15:30 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.4480
2022-03-19 15:16:07 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.4296
2022-03-19 15:16:45 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.3958
2022-03-19 15:17:23 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5431
2022-03-19 15:18:00 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.6339
2022-03-19 15:18:38 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.4156
2022-03-19 15:19:16 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.5086
2022-03-19 15:19:54 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.3787
2022-03-19 15:20:32 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.4073
2022-03-19 15:21:09 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.7088
2022-03-19 15:21:47 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.3853
2022-03-19 15:22:25 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.5533
2022-03-19 15:23:03 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.4870
2022-03-19 15:23:40 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2682
2022-03-19 15:24:18 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3566
2022-03-19 15:24:56 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.6699
2022-03-19 15:25:34 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.4206
2022-03-19 15:26:12 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.3947
2022-03-19 15:26:49 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6525
2022-03-19 15:27:27 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.3801
2022-03-19 15:28:05 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.5169
2022-03-19 15:28:43 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.3143
2022-03-19 15:29:21 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.6102
2022-03-19 15:29:58 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.3067
2022-03-19 15:30:36 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.4811
2022-03-19 15:31:14 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.2327
2022-03-19 15:31:52 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2836
2022-03-19 15:32:30 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.1990
2022-03-19 15:33:08 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.3922
2022-03-19 15:33:45 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.3375
2022-03-19 15:34:23 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.2508
2022-03-19 15:35:01 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.3518
2022-03-19 15:35:39 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.5804
2022-03-19 15:36:16 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.3498
2022-03-19 15:36:54 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.5970
2022-03-19 15:37:32 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.3581
2022-03-19 15:38:10 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.1655
2022-03-19 15:38:47 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.2241
2022-03-19 15:39:25 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.2590
2022-03-19 15:40:03 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.4236
2022-03-19 15:40:41 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.4175
2022-03-19 15:41:19 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.3328
2022-03-19 15:41:57 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.4074
2022-03-19 15:42:34 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1320
2022-03-19 15:43:12 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.3917
2022-03-19 15:43:14 - train: epoch 014, train_loss: 2.4054
2022-03-19 15:44:30 - eval: epoch: 014, acc1: 50.488%, acc5: 76.240%, test_loss: 2.1304, per_image_load_time: 1.796ms, per_image_inference_time: 0.720ms
2022-03-19 15:44:31 - until epoch: 014, best_acc1: 50.646%
2022-03-19 15:44:31 - epoch 015 lr: 0.1
2022-03-19 15:45:14 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.2433
2022-03-19 15:45:52 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.5686
2022-03-19 15:46:30 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6281
2022-03-19 15:47:07 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.3451
2022-03-19 15:47:45 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.3284
2022-03-19 15:48:22 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.6208
2022-03-19 15:49:00 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.3471
2022-03-19 15:49:37 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.1106
2022-03-19 15:50:15 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.2249
2022-03-19 15:50:53 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5219
2022-03-19 15:51:30 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.2756
2022-03-19 15:52:08 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.4137
2022-03-19 15:52:46 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5630
2022-03-19 15:53:24 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2923
2022-03-19 15:54:01 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.1935
2022-03-19 15:54:39 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.3877
2022-03-19 15:55:17 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.5907
2022-03-19 15:55:54 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.1362
2022-03-19 15:56:32 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.2823
2022-03-19 15:57:10 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.2184
2022-03-19 15:57:48 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.3469
2022-03-19 15:58:25 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.5881
2022-03-19 15:59:03 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3760
2022-03-19 15:59:41 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.2726
2022-03-19 16:00:19 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.4933
2022-03-19 16:00:56 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.0835
2022-03-19 16:01:34 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.2790
2022-03-19 16:02:12 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5845
2022-03-19 16:02:50 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.3396
2022-03-19 16:03:28 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.1734
2022-03-19 16:04:05 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.2909
2022-03-19 16:04:43 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3286
2022-03-19 16:05:21 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.1049
2022-03-19 16:05:59 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.4841
2022-03-19 16:06:37 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.5502
2022-03-19 16:07:14 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.4546
2022-03-19 16:07:52 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.3071
2022-03-19 16:08:30 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.3120
2022-03-19 16:09:08 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5240
2022-03-19 16:09:45 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4793
2022-03-19 16:10:23 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.6590
2022-03-19 16:11:01 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.1829
2022-03-19 16:11:39 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.4744
2022-03-19 16:12:17 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.4171
2022-03-19 16:12:54 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.4331
2022-03-19 16:13:32 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.2972
2022-03-19 16:14:10 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.6340
2022-03-19 16:14:48 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.3695
2022-03-19 16:15:26 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.4929
2022-03-19 16:16:04 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.4973
2022-03-19 16:16:06 - train: epoch 015, train_loss: 2.3874
2022-03-19 16:17:21 - eval: epoch: 015, acc1: 51.732%, acc5: 77.612%, test_loss: 2.0378, per_image_load_time: 0.830ms, per_image_inference_time: 0.709ms
2022-03-19 16:17:22 - until epoch: 015, best_acc1: 51.732%
2022-03-19 16:17:22 - epoch 016 lr: 0.1
2022-03-19 16:18:06 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.3614
2022-03-19 16:18:44 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.1309
2022-03-19 16:19:21 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.4340
2022-03-19 16:19:59 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.6981
2022-03-19 16:20:37 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.0841
2022-03-19 16:21:15 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.4739
2022-03-19 16:21:53 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.1020
2022-03-19 16:22:30 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.3096
2022-03-19 16:23:08 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.4659
2022-03-19 16:23:46 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.2186
2022-03-19 16:24:24 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.3118
2022-03-19 16:25:02 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2289
2022-03-19 16:25:40 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.4785
2022-03-19 16:26:18 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.2348
2022-03-19 16:26:55 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.4311
2022-03-19 16:27:33 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.4643
2022-03-19 16:28:11 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.2649
2022-03-19 16:28:49 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.3729
2022-03-19 16:29:27 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.4403
2022-03-19 16:30:05 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.9924
2022-03-19 16:30:43 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.5006
2022-03-19 16:31:20 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.5430
2022-03-19 16:31:58 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6113
2022-03-19 16:32:36 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.4485
2022-03-19 16:33:14 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.1903
2022-03-19 16:33:52 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.4693
2022-03-19 16:34:30 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.2266
2022-03-19 16:35:07 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.2434
2022-03-19 16:35:45 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.4479
2022-03-19 16:36:23 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.6686
2022-03-19 16:37:01 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.4772
2022-03-19 16:37:39 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.5073
2022-03-19 16:38:17 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.5235
2022-03-19 16:38:55 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.2915
2022-03-19 16:39:33 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3024
2022-03-19 16:40:11 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.2430
2022-03-19 16:40:48 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.4807
2022-03-19 16:41:26 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.7434
2022-03-19 16:42:04 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.5391
2022-03-19 16:42:42 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.4237
2022-03-19 16:43:20 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.3345
2022-03-19 16:43:58 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.3768
2022-03-19 16:44:36 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.2506
2022-03-19 16:45:14 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.2256
2022-03-19 16:45:51 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.4275
2022-03-19 16:46:29 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.3187
2022-03-19 16:47:07 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.5764
2022-03-19 16:47:45 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.2316
2022-03-19 16:48:23 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.3687
2022-03-19 16:49:01 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.4545
2022-03-19 16:49:02 - train: epoch 016, train_loss: 2.3701
2022-03-19 16:50:17 - eval: epoch: 016, acc1: 51.656%, acc5: 77.540%, test_loss: 2.0551, per_image_load_time: 0.793ms, per_image_inference_time: 0.715ms
2022-03-19 16:50:19 - until epoch: 016, best_acc1: 51.732%
2022-03-19 16:50:19 - epoch 017 lr: 0.1
2022-03-19 16:51:03 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.2216
2022-03-19 16:51:41 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.5367
2022-03-19 16:52:18 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.5504
2022-03-19 16:52:56 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1454
2022-03-19 16:53:34 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.2475
2022-03-19 16:54:11 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.7403
2022-03-19 16:54:49 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.2818
2022-03-19 16:55:27 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.3503
2022-03-19 16:56:04 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.3896
2022-03-19 16:56:42 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.2536
2022-03-19 16:57:20 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.5561
2022-03-19 16:57:57 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.5320
2022-03-19 16:58:35 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.3665
2022-03-19 16:59:13 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4096
2022-03-19 16:59:50 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.9887
2022-03-19 17:00:28 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.0917
2022-03-19 17:01:06 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.3095
2022-03-19 17:01:43 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.4141
2022-03-19 17:02:21 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2907
2022-03-19 17:02:59 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.5795
2022-03-19 17:03:36 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.3529
2022-03-19 17:04:14 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.2507
2022-03-19 17:04:51 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.3433
2022-03-19 17:05:29 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.3216
2022-03-19 17:06:07 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.5737
2022-03-19 17:06:45 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.2998
2022-03-19 17:07:22 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3563
2022-03-19 17:08:00 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.5606
2022-03-19 17:08:38 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.5376
2022-03-19 17:09:16 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.2167
2022-03-19 17:09:53 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.4507
2022-03-19 17:10:31 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.2889
2022-03-19 17:11:09 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.4626
2022-03-19 17:11:47 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.1897
2022-03-19 17:12:24 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.2940
2022-03-19 17:13:02 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.4516
2022-03-19 17:13:40 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3563
2022-03-19 17:14:18 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.5028
2022-03-19 17:14:55 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.1386
2022-03-19 17:15:33 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.2134
2022-03-19 17:16:11 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.3030
2022-03-19 17:16:49 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.2902
2022-03-19 17:17:27 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3456
2022-03-19 17:18:04 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.3671
2022-03-19 17:18:42 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.5508
2022-03-19 17:19:20 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.3290
2022-03-19 17:19:58 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.4493
2022-03-19 17:20:35 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.3517
2022-03-19 17:21:13 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.1333
2022-03-19 17:21:51 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.2473
2022-03-19 17:21:53 - train: epoch 017, train_loss: 2.3560
2022-03-19 17:23:08 - eval: epoch: 017, acc1: 52.076%, acc5: 77.466%, test_loss: 2.0447, per_image_load_time: 0.722ms, per_image_inference_time: 0.705ms
2022-03-19 17:23:10 - until epoch: 017, best_acc1: 52.076%
2022-03-19 17:23:10 - epoch 018 lr: 0.1
2022-03-19 17:23:53 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.2992
2022-03-19 17:24:31 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.3221
2022-03-19 17:25:08 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5418
2022-03-19 17:25:46 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.5969
2022-03-19 17:26:24 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.3081
2022-03-19 17:27:01 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.4963
2022-03-19 17:27:39 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.1560
2022-03-19 17:28:17 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.2603
2022-03-19 17:28:55 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.3058
2022-03-19 17:29:32 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.1908
2022-03-19 17:30:10 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.5349
2022-03-19 17:30:48 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.3733
2022-03-19 17:31:26 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.5356
2022-03-19 17:32:04 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.4195
2022-03-19 17:32:42 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.4668
2022-03-19 17:33:20 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.3125
2022-03-19 17:33:57 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.2517
2022-03-19 17:34:35 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.1946
2022-03-19 17:35:13 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.3267
2022-03-19 17:35:51 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.5881
2022-03-19 17:36:29 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.5562
2022-03-19 17:37:06 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.2727
2022-03-19 17:37:44 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.4293
2022-03-19 17:38:22 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.1025
2022-03-19 17:39:00 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.0023
2022-03-19 17:39:38 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.1553
2022-03-19 17:40:16 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.4895
2022-03-19 17:40:54 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.0913
2022-03-19 17:41:32 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.3034
2022-03-19 17:42:10 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2676
2022-03-19 17:42:48 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.7328
2022-03-19 17:43:26 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.1942
2022-03-19 17:44:04 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.2445
2022-03-19 17:44:42 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.3290
2022-03-19 17:45:20 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5769
2022-03-19 17:45:58 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.4068
2022-03-19 17:46:36 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.6864
2022-03-19 17:47:14 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.4087
2022-03-19 17:47:52 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.4208
2022-03-19 17:48:29 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.2081
2022-03-19 17:49:07 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.2993
2022-03-19 17:49:45 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.3633
2022-03-19 17:50:23 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1856
2022-03-19 17:51:01 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.4737
2022-03-19 17:51:39 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.3318
2022-03-19 17:52:17 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.2088
2022-03-19 17:52:55 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.5118
2022-03-19 17:53:32 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.4300
2022-03-19 17:54:10 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.2737
2022-03-19 17:54:48 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.3831
2022-03-19 17:54:50 - train: epoch 018, train_loss: 2.3419
2022-03-19 17:56:06 - eval: epoch: 018, acc1: 51.882%, acc5: 77.640%, test_loss: 2.0371, per_image_load_time: 1.599ms, per_image_inference_time: 0.728ms
2022-03-19 17:56:07 - until epoch: 018, best_acc1: 52.076%
2022-03-19 17:56:07 - epoch 019 lr: 0.1
2022-03-19 17:56:51 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.0562
2022-03-19 17:57:29 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.5219
2022-03-19 17:58:07 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.5043
2022-03-19 17:58:45 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.2402
2022-03-19 17:59:22 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3392
2022-03-19 18:00:00 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.2970
2022-03-19 18:00:38 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.0485
2022-03-19 18:01:16 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.5360
2022-03-19 18:01:54 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.3275
2022-03-19 18:02:32 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.5671
2022-03-19 18:03:10 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.1380
2022-03-19 18:03:48 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.3403
2022-03-19 18:04:25 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.4611
2022-03-19 18:05:03 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.1405
2022-03-19 18:05:41 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.7660
2022-03-19 18:06:19 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.3826
2022-03-19 18:06:57 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.4580
2022-03-19 18:07:35 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.2991
2022-03-19 18:08:13 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.4943
2022-03-19 18:08:51 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.2138
2022-03-19 18:09:29 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.4140
2022-03-19 18:10:07 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.3384
2022-03-19 18:10:45 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.3225
2022-03-19 18:11:23 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.3810
2022-03-19 18:12:00 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.2739
2022-03-19 18:12:38 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.2796
2022-03-19 18:13:16 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.5085
2022-03-19 18:13:54 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4263
2022-03-19 18:14:31 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.4251
2022-03-19 18:15:09 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.4986
2022-03-19 18:15:47 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.3346
2022-03-19 18:16:25 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.0004
2022-03-19 18:17:02 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.2366
2022-03-19 18:17:40 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4060
2022-03-19 18:18:18 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.3682
2022-03-19 18:18:55 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.1463
2022-03-19 18:19:33 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.3893
2022-03-19 18:20:11 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.4218
2022-03-19 18:20:49 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.2451
2022-03-19 18:21:27 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2360
2022-03-19 18:22:05 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.3057
2022-03-19 18:22:42 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.3270
2022-03-19 18:23:20 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.2509
2022-03-19 18:23:58 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.5942
2022-03-19 18:24:35 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.6380
2022-03-19 18:25:13 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.2368
2022-03-19 18:25:51 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.1737
2022-03-19 18:26:29 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3698
2022-03-19 18:27:06 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3842
2022-03-19 18:27:44 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.3695
2022-03-19 18:27:46 - train: epoch 019, train_loss: 2.3338
2022-03-19 18:29:02 - eval: epoch: 019, acc1: 52.326%, acc5: 77.948%, test_loss: 2.0177, per_image_load_time: 1.871ms, per_image_inference_time: 0.719ms
2022-03-19 18:29:04 - until epoch: 019, best_acc1: 52.326%
2022-03-19 18:29:04 - epoch 020 lr: 0.1
2022-03-19 18:29:48 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.4924
2022-03-19 18:30:25 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.1377
2022-03-19 18:31:03 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.4388
2022-03-19 18:31:41 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.0690
2022-03-19 18:32:19 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.2203
2022-03-19 18:32:56 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.4500
2022-03-19 18:33:34 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.0739
2022-03-19 18:34:11 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.3979
2022-03-19 18:34:49 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.7295
2022-03-19 18:35:27 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.4359
2022-03-19 18:36:04 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.1670
2022-03-19 18:36:42 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.1244
2022-03-19 18:37:19 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.2228
2022-03-19 18:37:57 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.5035
2022-03-19 18:38:35 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.3497
2022-03-19 18:39:12 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.1894
2022-03-19 18:39:50 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.0254
2022-03-19 18:40:28 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.2073
2022-03-19 18:41:05 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.1219
2022-03-19 18:41:43 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.3108
2022-03-19 18:42:21 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.5180
2022-03-19 18:42:59 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.1541
2022-03-19 18:43:36 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.3067
2022-03-19 18:44:14 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.5035
2022-03-19 18:44:51 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.1755
2022-03-19 18:45:29 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.0855
2022-03-19 18:46:07 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.3191
2022-03-19 18:46:44 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.4714
2022-03-19 18:47:22 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.4274
2022-03-19 18:48:00 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.2157
2022-03-19 18:48:37 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.3490
2022-03-19 18:49:15 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.5275
2022-03-19 18:49:53 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.0959
2022-03-19 18:50:31 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.4126
2022-03-19 18:51:08 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2147
2022-03-19 18:51:46 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.3177
2022-03-19 18:52:24 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.3044
2022-03-19 18:53:01 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.2522
2022-03-19 18:53:39 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.4091
2022-03-19 18:54:17 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.2207
2022-03-19 18:54:55 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.1886
2022-03-19 18:55:32 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.2680
2022-03-19 18:56:10 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.3088
2022-03-19 18:56:48 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.2055
2022-03-19 18:57:26 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.3312
2022-03-19 18:58:04 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.3615
2022-03-19 18:58:42 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.2370
2022-03-19 18:59:20 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.4650
2022-03-19 18:59:58 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.3414
2022-03-19 19:00:36 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.1462
2022-03-19 19:00:38 - train: epoch 020, train_loss: 2.3200
2022-03-19 19:01:52 - eval: epoch: 020, acc1: 53.530%, acc5: 78.654%, test_loss: 1.9747, per_image_load_time: 2.185ms, per_image_inference_time: 0.721ms
2022-03-19 19:01:54 - until epoch: 020, best_acc1: 53.530%
2022-03-19 19:01:54 - epoch 021 lr: 0.1
2022-03-19 19:02:37 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.1754
2022-03-19 19:03:14 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.4286
2022-03-19 19:03:51 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.0419
2022-03-19 19:04:28 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.3643
2022-03-19 19:05:05 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.1123
2022-03-19 19:05:43 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.1730
2022-03-19 19:06:20 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.2331
2022-03-19 19:06:57 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.3703
2022-03-19 19:07:34 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.3712
2022-03-19 19:08:11 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.2048
2022-03-19 19:08:49 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.2587
2022-03-19 19:09:26 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.3053
2022-03-19 19:10:04 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.1866
2022-03-19 19:10:41 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.2768
2022-03-19 19:11:18 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1632
2022-03-19 19:11:56 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.2945
2022-03-19 19:12:33 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.3317
2022-03-19 19:13:11 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.3560
2022-03-19 19:13:48 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.3846
2022-03-19 19:14:26 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.5195
2022-03-19 19:15:03 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1542
2022-03-19 19:15:41 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.2281
2022-03-19 19:16:18 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1018
2022-03-19 19:16:56 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.1381
2022-03-19 19:17:33 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2119
2022-03-19 19:18:11 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.6187
2022-03-19 19:18:49 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.3156
2022-03-19 19:19:26 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.3599
2022-03-19 19:20:04 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.2981
2022-03-19 19:20:41 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.5374
2022-03-19 19:21:19 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.3962
2022-03-19 19:21:57 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.2854
2022-03-19 19:22:34 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.6604
2022-03-19 19:23:12 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.5585
2022-03-19 19:23:49 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.0792
2022-03-19 19:24:27 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.2416
2022-03-19 19:25:05 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2753
2022-03-19 19:25:42 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.2390
2022-03-19 19:26:20 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.2352
2022-03-19 19:26:57 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.5745
2022-03-19 19:27:35 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1904
2022-03-19 19:28:13 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.2274
2022-03-19 19:28:50 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.3023
2022-03-19 19:29:28 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.3384
2022-03-19 19:30:06 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.4861
2022-03-19 19:30:43 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.3051
2022-03-19 19:31:21 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.4232
2022-03-19 19:31:59 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.4331
2022-03-19 19:32:37 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.0773
2022-03-19 19:33:14 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.1580
2022-03-19 19:33:16 - train: epoch 021, train_loss: 2.3127
2022-03-19 19:34:32 - eval: epoch: 021, acc1: 52.960%, acc5: 78.546%, test_loss: 1.9908, per_image_load_time: 1.557ms, per_image_inference_time: 0.706ms
2022-03-19 19:34:34 - until epoch: 021, best_acc1: 53.530%
2022-03-19 19:34:34 - epoch 022 lr: 0.1
2022-03-19 19:35:17 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1254
2022-03-19 19:35:54 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1478
2022-03-19 19:36:31 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.0958
2022-03-19 19:37:09 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.1537
2022-03-19 19:37:46 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.2751
2022-03-19 19:38:24 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.3598
2022-03-19 19:39:01 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.4591
2022-03-19 19:39:38 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.3943
2022-03-19 19:40:16 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4137
2022-03-19 19:40:53 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.3631
2022-03-19 19:41:31 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.3140
2022-03-19 19:42:08 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.9934
2022-03-19 19:42:46 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.3005
2022-03-19 19:43:23 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.4003
2022-03-19 19:44:01 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.1565
2022-03-19 19:44:38 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.1347
2022-03-19 19:45:16 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.0592
2022-03-19 19:45:54 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.4196
2022-03-19 19:46:31 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.0895
2022-03-19 19:47:09 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4055
2022-03-19 19:47:46 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.2897
2022-03-19 19:48:24 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.1301
2022-03-19 19:49:01 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.4543
2022-03-19 19:49:39 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.4031
2022-03-19 19:50:16 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.3401
2022-03-19 19:50:54 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.1152
2022-03-19 19:51:31 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.1208
2022-03-19 19:52:09 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.5732
2022-03-19 19:52:47 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.1185
2022-03-19 19:53:24 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.4202
2022-03-19 19:54:01 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.4505
2022-03-19 19:54:39 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.3103
2022-03-19 19:55:17 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.2870
2022-03-19 19:55:54 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.1402
2022-03-19 19:56:32 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.3970
2022-03-19 19:57:09 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.3149
2022-03-19 19:57:47 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.4562
2022-03-19 19:58:25 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.5404
2022-03-19 19:59:02 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.2297
2022-03-19 19:59:40 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.3930
2022-03-19 20:00:17 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.2066
2022-03-19 20:00:55 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2204
2022-03-19 20:01:32 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4350
2022-03-19 20:02:10 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.4358
2022-03-19 20:02:47 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.1945
2022-03-19 20:03:25 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.5622
2022-03-19 20:04:03 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.2814
2022-03-19 20:04:40 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.1491
2022-03-19 20:05:18 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.1268
2022-03-19 20:05:55 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.1879
2022-03-19 20:05:57 - train: epoch 022, train_loss: 2.3067
2022-03-19 20:07:13 - eval: epoch: 022, acc1: 52.356%, acc5: 77.914%, test_loss: 2.0226, per_image_load_time: 2.196ms, per_image_inference_time: 0.739ms
2022-03-19 20:07:15 - until epoch: 022, best_acc1: 53.530%
2022-03-19 20:07:15 - epoch 023 lr: 0.1
2022-03-19 20:07:58 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.1663
2022-03-19 20:08:35 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.0460
2022-03-19 20:09:13 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.1831
2022-03-19 20:09:50 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.3326
2022-03-19 20:10:27 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.2751
2022-03-19 20:11:04 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.1939
2022-03-19 20:11:42 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.0498
2022-03-19 20:12:19 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.1845
2022-03-19 20:12:56 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.2848
2022-03-19 20:13:34 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.0813
2022-03-19 20:14:12 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.3834
2022-03-19 20:14:50 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.1261
2022-03-19 20:15:27 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.3294
2022-03-19 20:16:05 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3117
2022-03-19 20:16:43 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.1556
2022-03-19 20:17:21 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3242
2022-03-19 20:17:59 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.5133
2022-03-19 20:18:36 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.1828
2022-03-19 20:19:14 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.2790
2022-03-19 20:19:52 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9349
2022-03-19 20:20:30 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.3901
2022-03-19 20:21:07 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.9919
2022-03-19 20:21:45 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.1360
2022-03-19 20:22:23 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.2849
2022-03-19 20:23:00 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.5486
2022-03-19 20:23:38 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.2788
2022-03-19 20:24:16 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.3236
2022-03-19 20:24:53 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.3639
2022-03-19 20:25:31 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.2535
2022-03-19 20:26:09 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.5763
2022-03-19 20:26:46 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.3882
2022-03-19 20:27:24 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.4830
2022-03-19 20:28:02 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.3326
2022-03-19 20:28:39 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.3762
2022-03-19 20:29:17 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.1002
2022-03-19 20:29:55 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.1655
2022-03-19 20:30:32 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.3472
2022-03-19 20:31:10 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.2295
2022-03-19 20:31:48 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.3051
2022-03-19 20:32:25 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.3265
2022-03-19 20:33:03 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.1648
2022-03-19 20:33:41 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.1823
2022-03-19 20:34:18 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.1736
2022-03-19 20:34:56 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1313
2022-03-19 20:35:34 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.1746
2022-03-19 20:36:11 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.4050
2022-03-19 20:36:49 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.1691
2022-03-19 20:37:27 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.2561
2022-03-19 20:38:04 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.2787
2022-03-19 20:38:42 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.3858
2022-03-19 20:38:44 - train: epoch 023, train_loss: 2.2951
2022-03-19 20:40:00 - eval: epoch: 023, acc1: 52.680%, acc5: 78.076%, test_loss: 2.0115, per_image_load_time: 2.233ms, per_image_inference_time: 0.733ms
2022-03-19 20:40:01 - until epoch: 023, best_acc1: 53.530%
2022-03-19 20:40:01 - epoch 024 lr: 0.1
2022-03-19 20:40:44 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.2975
2022-03-19 20:41:21 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.3533
2022-03-19 20:41:59 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.1846
2022-03-19 20:42:36 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.3584
2022-03-19 20:43:14 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.1232
2022-03-19 20:43:51 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1235
2022-03-19 20:44:29 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.1717
2022-03-19 20:45:06 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.2737
2022-03-19 20:45:44 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.2407
2022-03-19 20:46:21 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.2286
2022-03-19 20:46:59 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.0036
2022-03-19 20:47:36 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.1176
2022-03-19 20:48:14 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.6347
2022-03-19 20:48:52 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.1786
2022-03-19 20:49:29 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.5723
2022-03-19 20:50:07 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.3216
2022-03-19 20:50:45 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.2393
2022-03-19 20:51:23 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.4627
2022-03-19 20:52:00 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.0528
2022-03-19 20:52:38 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.2863
2022-03-19 20:53:16 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.3268
2022-03-19 20:53:54 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.1034
2022-03-19 20:54:31 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.2903
2022-03-19 20:55:09 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.1961
2022-03-19 20:55:47 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1336
2022-03-19 20:56:25 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.1597
2022-03-19 20:57:03 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.4068
2022-03-19 20:57:41 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.4201
2022-03-19 20:58:19 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.3351
2022-03-19 20:58:57 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.1385
2022-03-19 20:59:35 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.2147
2022-03-19 21:00:13 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.3539
2022-03-19 21:00:50 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.1248
2022-03-19 21:01:28 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.4081
2022-03-19 21:02:06 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.2004
2022-03-19 21:02:44 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.3398
2022-03-19 21:03:22 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.2950
2022-03-19 21:04:00 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.4134
2022-03-19 21:04:37 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.2892
2022-03-19 21:05:15 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.3589
2022-03-19 21:05:53 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.2585
2022-03-19 21:06:31 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.3312
2022-03-19 21:07:09 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3664
2022-03-19 21:07:46 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.3166
2022-03-19 21:08:24 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.1284
2022-03-19 21:09:02 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4194
2022-03-19 21:09:40 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.2243
2022-03-19 21:10:18 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.2306
2022-03-19 21:10:55 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.2827
2022-03-19 21:11:33 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.4548
2022-03-19 21:11:35 - train: epoch 024, train_loss: 2.2896
2022-03-19 21:12:51 - eval: epoch: 024, acc1: 53.654%, acc5: 79.026%, test_loss: 1.9508, per_image_load_time: 2.216ms, per_image_inference_time: 0.719ms
2022-03-19 21:12:53 - until epoch: 024, best_acc1: 53.654%
2022-03-19 21:12:53 - epoch 025 lr: 0.1
2022-03-19 21:13:35 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.1070
2022-03-19 21:14:12 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.0670
2022-03-19 21:14:49 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.0827
2022-03-19 21:15:26 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.2680
2022-03-19 21:16:03 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.2433
2022-03-19 21:16:40 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.2640
2022-03-19 21:17:17 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.2778
2022-03-19 21:17:54 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.3169
2022-03-19 21:18:31 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.0827
2022-03-19 21:19:08 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.1728
2022-03-19 21:19:46 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.3317
2022-03-19 21:20:23 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.3270
2022-03-19 21:21:00 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.0952
2022-03-19 21:21:37 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4546
2022-03-19 21:22:14 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.3010
2022-03-19 21:22:51 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.0742
2022-03-19 21:23:28 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3052
2022-03-19 21:24:05 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.0765
2022-03-19 21:24:42 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.1940
2022-03-19 21:25:20 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.3577
2022-03-19 21:25:57 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1399
2022-03-19 21:26:34 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.2114
2022-03-19 21:27:11 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.2713
2022-03-19 21:27:48 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.0299
2022-03-19 21:28:25 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.4086
2022-03-19 21:29:02 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.2847
2022-03-19 21:29:39 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.3620
2022-03-19 21:30:16 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2373
2022-03-19 21:30:54 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.3421
2022-03-19 21:31:31 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.3962
2022-03-19 21:32:08 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.2192
2022-03-19 21:32:45 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.3743
2022-03-19 21:33:22 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.2546
2022-03-19 21:33:59 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4488
2022-03-19 21:34:36 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.0041
2022-03-19 21:35:14 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.3978
2022-03-19 21:35:51 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3058
2022-03-19 21:36:28 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.3146
2022-03-19 21:37:05 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.5520
2022-03-19 21:37:42 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4282
2022-03-19 21:38:19 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.3770
2022-03-19 21:38:56 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.3835
2022-03-19 21:39:34 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1023
2022-03-19 21:40:11 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.1505
2022-03-19 21:40:48 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.2856
2022-03-19 21:41:25 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.2088
2022-03-19 21:42:02 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.2503
2022-03-19 21:42:39 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.9281
2022-03-19 21:43:16 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.2309
2022-03-19 21:43:53 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.4617
2022-03-19 21:43:55 - train: epoch 025, train_loss: 2.2821
2022-03-19 21:45:11 - eval: epoch: 025, acc1: 53.320%, acc5: 78.716%, test_loss: 1.9749, per_image_load_time: 2.143ms, per_image_inference_time: 0.722ms
2022-03-19 21:45:12 - until epoch: 025, best_acc1: 53.654%
2022-03-19 21:45:12 - epoch 026 lr: 0.1
2022-03-19 21:45:55 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.0340
2022-03-19 21:46:33 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.9631
2022-03-19 21:47:10 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.3555
2022-03-19 21:47:48 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.1447
2022-03-19 21:48:25 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.3598
2022-03-19 21:49:03 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.2928
2022-03-19 21:49:41 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2299
2022-03-19 21:50:18 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.0591
2022-03-19 21:50:56 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.6087
2022-03-19 21:51:34 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.1158
2022-03-19 21:52:12 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.2155
2022-03-19 21:52:50 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.3372
2022-03-19 21:53:28 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.1329
2022-03-19 21:54:05 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.3975
2022-03-19 21:54:43 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.2454
2022-03-19 21:55:21 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.4398
2022-03-19 21:55:58 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.3137
2022-03-19 21:56:36 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.3720
2022-03-19 21:57:14 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6336
2022-03-19 21:57:52 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.4816
2022-03-19 21:58:30 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.3751
2022-03-19 21:59:07 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.2586
2022-03-19 21:59:45 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.2991
2022-03-19 22:00:23 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.5763
2022-03-19 22:01:01 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.3389
2022-03-19 22:01:38 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.3596
2022-03-19 22:02:16 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.1028
2022-03-19 22:02:54 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.0687
2022-03-19 22:03:32 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.3881
2022-03-19 22:04:09 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.1800
2022-03-19 22:04:47 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.2640
2022-03-19 22:05:25 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.2657
2022-03-19 22:06:03 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.2693
2022-03-19 22:06:41 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.4523
2022-03-19 22:07:19 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.4162
2022-03-19 22:07:56 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2784
2022-03-19 22:08:34 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4469
2022-03-19 22:09:12 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.4179
2022-03-19 22:09:50 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.4072
2022-03-19 22:10:28 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.4430
2022-03-19 22:11:05 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.2975
2022-03-19 22:11:43 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.3195
2022-03-19 22:12:21 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.4257
2022-03-19 22:12:59 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.3941
2022-03-19 22:13:37 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.5385
2022-03-19 22:14:15 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.2445
2022-03-19 22:14:52 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.1369
2022-03-19 22:15:30 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2390
2022-03-19 22:16:08 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.3458
2022-03-19 22:16:46 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5272
2022-03-19 22:16:48 - train: epoch 026, train_loss: 2.2770
2022-03-19 22:18:04 - eval: epoch: 026, acc1: 53.508%, acc5: 79.166%, test_loss: 1.9522, per_image_load_time: 1.418ms, per_image_inference_time: 0.715ms
2022-03-19 22:18:05 - until epoch: 026, best_acc1: 53.654%
2022-03-19 22:18:05 - epoch 027 lr: 0.1
2022-03-19 22:18:48 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.4113
2022-03-19 22:19:26 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.1748
2022-03-19 22:20:03 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.2719
2022-03-19 22:20:41 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.3647
2022-03-19 22:21:19 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3006
2022-03-19 22:21:56 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6319
2022-03-19 22:22:34 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.3749
2022-03-19 22:23:11 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3573
2022-03-19 22:23:49 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.1605
2022-03-19 22:24:26 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.2734
2022-03-19 22:25:04 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.2152
2022-03-19 22:25:42 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.4620
2022-03-19 22:26:20 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.2925
2022-03-19 22:26:57 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.5373
2022-03-19 22:27:35 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.3512
2022-03-19 22:28:13 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3504
2022-03-19 22:28:50 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.2953
2022-03-19 22:29:28 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.2313
2022-03-19 22:30:05 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.4776
2022-03-19 22:30:43 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.3466
2022-03-19 22:31:20 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.5027
2022-03-19 22:31:57 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.5664
2022-03-19 22:32:35 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.6274
2022-03-19 22:33:12 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1983
2022-03-19 22:33:50 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.0970
2022-03-19 22:34:27 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.5514
2022-03-19 22:35:04 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.3116
2022-03-19 22:35:42 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.3542
2022-03-19 22:36:19 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.1806
2022-03-19 22:36:56 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1119
2022-03-19 22:37:34 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.0780
2022-03-19 22:38:11 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.1060
2022-03-19 22:38:49 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.2619
2022-03-19 22:39:27 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.2443
2022-03-19 22:40:04 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.4230
2022-03-19 22:40:42 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.0870
2022-03-19 22:41:19 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.0827
2022-03-19 22:41:57 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.1106
2022-03-19 22:42:35 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.1200
2022-03-19 22:43:12 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.3947
2022-03-19 22:43:50 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.3019
2022-03-19 22:44:28 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.3300
2022-03-19 22:45:06 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.1768
2022-03-19 22:45:44 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.2935
2022-03-19 22:46:21 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 1.9807
2022-03-19 22:46:59 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.2880
2022-03-19 22:47:37 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.3197
2022-03-19 22:48:15 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.4994
2022-03-19 22:48:52 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.3383
2022-03-19 22:49:30 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.9042
2022-03-19 22:49:31 - train: epoch 027, train_loss: 2.2707
2022-03-19 22:50:48 - eval: epoch: 027, acc1: 53.506%, acc5: 78.774%, test_loss: 1.9620, per_image_load_time: 2.237ms, per_image_inference_time: 0.717ms
2022-03-19 22:50:49 - until epoch: 027, best_acc1: 53.654%
2022-03-19 22:50:49 - epoch 028 lr: 0.1
2022-03-19 22:51:33 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.0359
2022-03-19 22:52:10 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2208
2022-03-19 22:52:47 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.2333
2022-03-19 22:53:24 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.2050
2022-03-19 22:54:02 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.1727
2022-03-19 22:54:39 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.2602
2022-03-19 22:55:17 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.5057
2022-03-19 22:55:54 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.9689
2022-03-19 22:56:31 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.0679
2022-03-19 22:57:09 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.3547
2022-03-19 22:57:47 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.3046
2022-03-19 22:58:24 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.1226
2022-03-19 22:59:02 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.2420
2022-03-19 22:59:39 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.6323
2022-03-19 23:00:17 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.1938
2022-03-19 23:00:55 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.2187
2022-03-19 23:01:32 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.0864
2022-03-19 23:02:10 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.0376
2022-03-19 23:02:47 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.1591
2022-03-19 23:03:25 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.4497
2022-03-19 23:04:02 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.3222
2022-03-19 23:04:40 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.4232
2022-03-19 23:05:17 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.3381
2022-03-19 23:05:55 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.4102
2022-03-19 23:06:33 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.3021
2022-03-19 23:07:10 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.0987
2022-03-19 23:07:48 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.2230
2022-03-19 23:08:26 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.2879
2022-03-19 23:09:03 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.2496
2022-03-19 23:09:41 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.2579
2022-03-19 23:10:19 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.3996
2022-03-19 23:10:57 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.2312
2022-03-19 23:11:34 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.2329
2022-03-19 23:12:12 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.1384
2022-03-19 23:12:49 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.2005
2022-03-19 23:13:26 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.1426
2022-03-19 23:14:04 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.2521
2022-03-19 23:14:41 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.9937
2022-03-19 23:15:19 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.2706
2022-03-19 23:15:57 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.3205
2022-03-19 23:16:34 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.3221
2022-03-19 23:17:12 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.3750
2022-03-19 23:17:50 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.3316
2022-03-19 23:18:28 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.3235
2022-03-19 23:19:06 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.1643
2022-03-19 23:19:43 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.3860
2022-03-19 23:20:21 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.2710
2022-03-19 23:20:59 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.1365
2022-03-19 23:21:36 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.0673
2022-03-19 23:22:14 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.1405
2022-03-19 23:22:16 - train: epoch 028, train_loss: 2.2623
2022-03-19 23:23:31 - eval: epoch: 028, acc1: 54.492%, acc5: 79.504%, test_loss: 1.9284, per_image_load_time: 1.114ms, per_image_inference_time: 0.731ms
2022-03-19 23:23:33 - until epoch: 028, best_acc1: 54.492%
2022-03-19 23:23:33 - epoch 029 lr: 0.1
2022-03-19 23:24:15 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.3396
2022-03-19 23:24:52 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.3004
2022-03-19 23:25:30 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.4767
2022-03-19 23:26:07 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.1957
2022-03-19 23:26:44 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3460
2022-03-19 23:27:21 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.5256
2022-03-19 23:27:59 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9226
2022-03-19 23:28:36 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.3709
2022-03-19 23:29:14 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1551
2022-03-19 23:29:51 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.0022
2022-03-19 23:30:28 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.2195
2022-03-19 23:31:06 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.3447
2022-03-19 23:31:43 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.2467
2022-03-19 23:32:21 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.4897
2022-03-19 23:32:58 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.4006
2022-03-19 23:33:36 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.1646
2022-03-19 23:34:13 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.3853
2022-03-19 23:34:50 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.4229
2022-03-19 23:35:28 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.1506
2022-03-19 23:36:05 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.5319
2022-03-19 23:36:43 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.1926
2022-03-19 23:37:20 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.3039
2022-03-19 23:37:58 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.2449
2022-03-19 23:38:36 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.1765
2022-03-19 23:39:13 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2076
2022-03-19 23:39:51 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.3681
2022-03-19 23:40:29 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.1519
2022-03-19 23:41:07 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.2404
2022-03-19 23:41:44 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.2265
2022-03-19 23:42:22 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.3448
2022-03-19 23:43:00 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.3316
2022-03-19 23:43:38 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.2451
2022-03-19 23:44:15 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.2764
2022-03-19 23:44:53 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.0074
2022-03-19 23:45:31 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.3406
2022-03-19 23:46:08 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.2448
2022-03-19 23:46:46 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.1449
2022-03-19 23:47:24 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.0760
2022-03-19 23:48:02 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.0866
2022-03-19 23:48:40 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.1902
2022-03-19 23:49:17 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.1325
2022-03-19 23:49:55 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.0400
2022-03-19 23:50:33 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.2801
2022-03-19 23:51:11 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.2816
2022-03-19 23:51:49 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.3476
2022-03-19 23:52:27 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.3920
2022-03-19 23:53:04 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.0284
2022-03-19 23:53:42 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.3386
2022-03-19 23:54:20 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.6847
2022-03-19 23:54:58 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.9982
2022-03-19 23:55:00 - train: epoch 029, train_loss: 2.2588
2022-03-19 23:56:16 - eval: epoch: 029, acc1: 53.720%, acc5: 78.974%, test_loss: 1.9500, per_image_load_time: 2.229ms, per_image_inference_time: 0.718ms
2022-03-19 23:56:17 - until epoch: 029, best_acc1: 54.492%
2022-03-19 23:56:17 - epoch 030 lr: 0.1
2022-03-19 23:57:00 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.4153
2022-03-19 23:57:37 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.2034
2022-03-19 23:58:14 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.1830
2022-03-19 23:58:52 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.9716
2022-03-19 23:59:29 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.4981
2022-03-20 00:00:06 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0914
2022-03-20 00:00:44 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.1350
2022-03-20 00:01:21 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.2820
2022-03-20 00:01:58 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.3249
2022-03-20 00:02:36 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.0504
2022-03-20 00:03:13 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.0998
2022-03-20 00:03:51 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.2679
2022-03-20 00:04:28 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.0898
2022-03-20 00:05:06 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.0184
2022-03-20 00:05:43 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.2222
2022-03-20 00:06:21 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.3162
2022-03-20 00:06:59 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.4724
2022-03-20 00:07:36 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2820
2022-03-20 00:08:14 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.3117
2022-03-20 00:08:51 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.3249
2022-03-20 00:09:29 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.3268
2022-03-20 00:10:07 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.1887
2022-03-20 00:10:45 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.2652
2022-03-20 00:11:23 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4132
2022-03-20 00:12:00 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2667
2022-03-20 00:12:38 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.2678
2022-03-20 00:13:16 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.2859
2022-03-20 00:13:54 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.2039
2022-03-20 00:14:32 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.2395
2022-03-20 00:15:10 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.4917
2022-03-20 00:15:47 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.2009
2022-03-20 00:16:25 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.0719
2022-03-20 00:17:03 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.4659
2022-03-20 00:17:41 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.2245
2022-03-20 00:18:19 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.2744
2022-03-20 00:18:57 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.2112
2022-03-20 00:19:35 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.1928
2022-03-20 00:20:13 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.3896
2022-03-20 00:20:51 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3424
2022-03-20 00:21:29 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.1251
2022-03-20 00:22:07 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.1118
2022-03-20 00:22:45 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.4050
2022-03-20 00:23:23 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.1651
2022-03-20 00:24:01 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.1897
2022-03-20 00:24:39 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.4227
2022-03-20 00:25:17 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.9255
2022-03-20 00:25:55 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.3444
2022-03-20 00:26:33 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.4037
2022-03-20 00:27:11 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.3904
2022-03-20 00:27:48 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.4672
2022-03-20 00:27:50 - train: epoch 030, train_loss: 2.2581
2022-03-20 00:29:07 - eval: epoch: 030, acc1: 54.018%, acc5: 79.404%, test_loss: 1.9394, per_image_load_time: 2.095ms, per_image_inference_time: 0.729ms
2022-03-20 00:29:08 - until epoch: 030, best_acc1: 54.492%
2022-03-20 00:29:08 - epoch 031 lr: 0.010000000000000002
2022-03-20 00:29:51 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.1160
2022-03-20 00:30:28 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.8369
2022-03-20 00:31:05 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.8918
2022-03-20 00:31:42 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.9637
2022-03-20 00:32:19 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.6413
2022-03-20 00:32:57 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.7932
2022-03-20 00:33:34 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.7204
2022-03-20 00:34:11 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.7236
2022-03-20 00:34:48 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.7247
2022-03-20 00:35:26 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.0519
2022-03-20 00:36:03 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.0130
2022-03-20 00:36:40 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.7410
2022-03-20 00:37:18 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.5732
2022-03-20 00:37:55 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.8350
2022-03-20 00:38:32 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.8434
2022-03-20 00:39:09 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.6461
2022-03-20 00:39:47 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.6661
2022-03-20 00:40:24 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.6834
2022-03-20 00:41:02 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.8933
2022-03-20 00:41:39 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.7666
2022-03-20 00:42:17 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6277
2022-03-20 00:42:54 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.5729
2022-03-20 00:43:31 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.5750
2022-03-20 00:44:09 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.6329
2022-03-20 00:44:46 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.6212
2022-03-20 00:45:24 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.6054
2022-03-20 00:46:01 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.7085
2022-03-20 00:46:39 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.8702
2022-03-20 00:47:16 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.7360
2022-03-20 00:47:54 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.9174
2022-03-20 00:48:31 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6102
2022-03-20 00:49:09 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.9144
2022-03-20 00:49:46 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.7528
2022-03-20 00:50:24 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.8446
2022-03-20 00:51:01 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.8422
2022-03-20 00:51:39 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7524
2022-03-20 00:52:17 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.6024
2022-03-20 00:52:55 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.5104
2022-03-20 00:53:32 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.6772
2022-03-20 00:54:10 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.5642
2022-03-20 00:54:48 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.5874
2022-03-20 00:55:26 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.7622
2022-03-20 00:56:04 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.6816
2022-03-20 00:56:42 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.7567
2022-03-20 00:57:19 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.7338
2022-03-20 00:57:57 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7342
2022-03-20 00:58:35 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.6673
2022-03-20 00:59:13 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.5654
2022-03-20 00:59:51 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.5849
2022-03-20 01:00:28 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.6781
2022-03-20 01:00:30 - train: epoch 031, train_loss: 1.7330
2022-03-20 01:01:47 - eval: epoch: 031, acc1: 66.692%, acc5: 87.652%, test_loss: 1.3499, per_image_load_time: 2.202ms, per_image_inference_time: 0.724ms
2022-03-20 01:01:48 - until epoch: 031, best_acc1: 66.692%
2022-03-20 01:01:48 - epoch 032 lr: 0.010000000000000002
2022-03-20 01:02:31 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.5632
2022-03-20 01:03:09 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.7506
2022-03-20 01:03:46 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.6629
2022-03-20 01:04:23 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.7868
2022-03-20 01:05:01 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.6075
2022-03-20 01:05:38 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.6794
2022-03-20 01:06:16 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6334
2022-03-20 01:06:53 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.5621
2022-03-20 01:07:31 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.5920
2022-03-20 01:08:08 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7010
2022-03-20 01:08:46 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.6824
2022-03-20 01:09:23 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.6802
2022-03-20 01:10:01 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.5567
2022-03-20 01:10:38 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.7737
2022-03-20 01:11:15 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.6563
2022-03-20 01:11:53 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.7767
2022-03-20 01:12:30 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.7062
2022-03-20 01:13:08 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.9349
2022-03-20 01:13:45 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.4872
2022-03-20 01:14:22 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.6710
2022-03-20 01:15:00 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.5085
2022-03-20 01:15:37 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.5425
2022-03-20 01:16:15 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.6742
2022-03-20 01:16:53 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.5490
2022-03-20 01:17:30 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7155
2022-03-20 01:18:08 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.4754
2022-03-20 01:18:45 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.6896
2022-03-20 01:19:23 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.7371
2022-03-20 01:20:01 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.4645
2022-03-20 01:20:38 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.4236
2022-03-20 01:21:16 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.6256
2022-03-20 01:21:54 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.5804
2022-03-20 01:22:32 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.4037
2022-03-20 01:23:10 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.4565
2022-03-20 01:23:48 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.5998
2022-03-20 01:24:26 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.5789
2022-03-20 01:25:03 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.7090
2022-03-20 01:25:41 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4761
2022-03-20 01:26:19 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.6346
2022-03-20 01:26:56 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4917
2022-03-20 01:27:34 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.5721
2022-03-20 01:28:12 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.5308
2022-03-20 01:28:49 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.7970
2022-03-20 01:29:27 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.4925
2022-03-20 01:30:05 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8117
2022-03-20 01:30:43 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.6776
2022-03-20 01:31:20 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.7271
2022-03-20 01:31:58 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.4842
2022-03-20 01:32:36 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.7593
2022-03-20 01:33:13 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6148
2022-03-20 01:33:15 - train: epoch 032, train_loss: 1.6100
2022-03-20 01:34:31 - eval: epoch: 032, acc1: 67.700%, acc5: 88.114%, test_loss: 1.3068, per_image_load_time: 2.212ms, per_image_inference_time: 0.718ms
2022-03-20 01:34:33 - until epoch: 032, best_acc1: 67.700%
2022-03-20 01:34:33 - epoch 033 lr: 0.010000000000000002
2022-03-20 01:35:17 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.4215
2022-03-20 01:35:55 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.6067
2022-03-20 01:36:33 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.4657
2022-03-20 01:37:11 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4804
2022-03-20 01:37:49 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.6104
2022-03-20 01:38:28 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.4701
2022-03-20 01:39:06 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.6220
2022-03-20 01:39:44 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.7418
2022-03-20 01:40:22 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.6800
2022-03-20 01:41:01 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.6661
2022-03-20 01:41:38 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.5301
2022-03-20 01:42:16 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.6065
2022-03-20 01:42:55 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.4243
2022-03-20 01:43:33 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.7078
2022-03-20 01:44:11 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.8332
2022-03-20 01:44:49 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.7750
2022-03-20 01:45:27 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.3684
2022-03-20 01:46:05 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.7596
2022-03-20 01:46:42 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.5615
2022-03-20 01:47:20 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.4528
2022-03-20 01:47:58 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.5350
2022-03-20 01:48:36 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.6154
2022-03-20 01:49:14 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.5267
2022-03-20 01:49:52 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.8072
2022-03-20 01:50:30 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.5871
2022-03-20 01:51:08 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.4499
2022-03-20 01:51:46 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.8116
2022-03-20 01:52:24 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.6169
2022-03-20 01:53:02 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.6501
2022-03-20 01:53:40 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.6487
2022-03-20 01:54:17 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.5361
2022-03-20 01:54:55 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.4841
2022-03-20 01:55:33 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.6295
2022-03-20 01:56:11 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.4973
2022-03-20 01:56:49 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.6447
2022-03-20 01:57:27 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.7988
2022-03-20 01:58:05 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.4818
2022-03-20 01:58:43 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.4711
2022-03-20 01:59:21 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.6699
2022-03-20 01:59:58 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.6048
2022-03-20 02:00:36 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.5677
2022-03-20 02:01:14 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.4616
2022-03-20 02:01:52 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.7250
2022-03-20 02:02:30 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.5687
2022-03-20 02:03:07 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.6976
2022-03-20 02:03:45 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.4415
2022-03-20 02:04:23 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.4385
2022-03-20 02:05:01 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.8772
2022-03-20 02:05:39 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.4204
2022-03-20 02:06:17 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.5235
2022-03-20 02:06:19 - train: epoch 033, train_loss: 1.5633
2022-03-20 02:07:34 - eval: epoch: 033, acc1: 67.862%, acc5: 88.466%, test_loss: 1.2825, per_image_load_time: 0.678ms, per_image_inference_time: 0.707ms
2022-03-20 02:07:36 - until epoch: 033, best_acc1: 67.862%
2022-03-20 02:07:36 - epoch 034 lr: 0.010000000000000002
2022-03-20 02:08:20 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.6250
2022-03-20 02:08:58 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.4767
2022-03-20 02:09:35 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.4934
2022-03-20 02:10:13 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.3129
2022-03-20 02:10:51 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.5416
2022-03-20 02:11:29 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.7763
2022-03-20 02:12:07 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.4642
2022-03-20 02:12:44 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.4435
2022-03-20 02:13:22 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.4841
2022-03-20 02:14:00 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.4271
2022-03-20 02:14:38 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.5414
2022-03-20 02:15:16 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.4913
2022-03-20 02:15:54 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.4890
2022-03-20 02:16:32 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.5398
2022-03-20 02:17:09 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.4081
2022-03-20 02:17:47 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.4030
2022-03-20 02:18:25 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.5495
2022-03-20 02:19:03 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.6676
2022-03-20 02:19:40 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.6780
2022-03-20 02:20:18 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.6933
2022-03-20 02:20:56 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.7820
2022-03-20 02:21:34 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.4161
2022-03-20 02:22:11 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.7296
2022-03-20 02:22:49 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.4724
2022-03-20 02:23:27 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.5100
2022-03-20 02:24:05 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.5696
2022-03-20 02:24:43 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.6197
2022-03-20 02:25:21 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.3571
2022-03-20 02:25:59 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3129
2022-03-20 02:26:37 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.3186
2022-03-20 02:27:15 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.5615
2022-03-20 02:27:53 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.5249
2022-03-20 02:28:31 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.6255
2022-03-20 02:29:09 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.5601
2022-03-20 02:29:47 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4116
2022-03-20 02:30:25 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.3709
2022-03-20 02:31:03 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.3555
2022-03-20 02:31:41 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.4547
2022-03-20 02:32:19 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.6638
2022-03-20 02:32:57 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.4355
2022-03-20 02:33:35 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.6108
2022-03-20 02:34:13 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.5114
2022-03-20 02:34:50 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5501
2022-03-20 02:35:28 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.5643
2022-03-20 02:36:06 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.6388
2022-03-20 02:36:44 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.6355
2022-03-20 02:37:22 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.5704
2022-03-20 02:38:00 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5040
2022-03-20 02:38:37 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.4543
2022-03-20 02:39:15 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.3226
2022-03-20 02:39:17 - train: epoch 034, train_loss: 1.5379
2022-03-20 02:40:32 - eval: epoch: 034, acc1: 68.308%, acc5: 88.690%, test_loss: 1.2678, per_image_load_time: 0.666ms, per_image_inference_time: 0.705ms
2022-03-20 02:40:34 - until epoch: 034, best_acc1: 68.308%
2022-03-20 02:40:34 - epoch 035 lr: 0.010000000000000002
2022-03-20 02:41:18 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.4467
2022-03-20 02:41:55 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.2621
2022-03-20 02:42:32 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.6807
2022-03-20 02:43:10 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.3780
2022-03-20 02:43:47 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.5536
2022-03-20 02:44:25 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.4016
2022-03-20 02:45:02 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.4593
2022-03-20 02:45:40 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.5773
2022-03-20 02:46:18 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.6476
2022-03-20 02:46:56 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.6534
2022-03-20 02:47:33 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.6554
2022-03-20 02:48:11 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.4798
2022-03-20 02:48:49 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.5789
2022-03-20 02:49:27 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.4952
2022-03-20 02:50:04 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.5744
2022-03-20 02:50:42 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.4773
2022-03-20 02:51:20 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.3474
2022-03-20 02:51:58 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.5992
2022-03-20 02:52:36 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.5465
2022-03-20 02:53:14 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.5462
2022-03-20 02:53:52 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.4834
2022-03-20 02:54:30 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.5791
2022-03-20 02:55:07 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.5410
2022-03-20 02:55:45 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.6522
2022-03-20 02:56:23 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.4735
2022-03-20 02:57:01 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.6469
2022-03-20 02:57:38 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.6606
2022-03-20 02:58:16 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.4825
2022-03-20 02:58:54 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.4864
2022-03-20 02:59:32 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.6231
2022-03-20 03:00:10 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.5349
2022-03-20 03:00:48 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.3833
2022-03-20 03:01:26 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.4467
2022-03-20 03:02:04 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.5651
2022-03-20 03:02:41 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.3421
2022-03-20 03:03:19 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.5158
2022-03-20 03:03:57 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.3019
2022-03-20 03:04:35 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5122
2022-03-20 03:05:13 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.6868
2022-03-20 03:05:51 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.2993
2022-03-20 03:06:29 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.7581
2022-03-20 03:07:06 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4229
2022-03-20 03:07:44 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.6431
2022-03-20 03:08:22 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.6126
2022-03-20 03:09:00 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.5960
2022-03-20 03:09:38 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.4676
2022-03-20 03:10:16 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.8261
2022-03-20 03:10:54 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.6634
2022-03-20 03:11:31 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.6753
2022-03-20 03:12:09 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.3387
2022-03-20 03:12:11 - train: epoch 035, train_loss: 1.5195
2022-03-20 03:13:27 - eval: epoch: 035, acc1: 68.466%, acc5: 88.768%, test_loss: 1.2618, per_image_load_time: 1.043ms, per_image_inference_time: 0.713ms
2022-03-20 03:13:29 - until epoch: 035, best_acc1: 68.466%
2022-03-20 03:13:29 - epoch 036 lr: 0.010000000000000002
2022-03-20 03:14:12 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.5986
2022-03-20 03:14:49 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.2714
2022-03-20 03:15:27 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.3783
2022-03-20 03:16:05 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.5582
2022-03-20 03:16:42 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.5385
2022-03-20 03:17:20 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.4803
2022-03-20 03:17:58 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.2820
2022-03-20 03:18:35 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.3496
2022-03-20 03:19:13 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.4376
2022-03-20 03:19:50 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.5123
2022-03-20 03:20:28 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.6197
2022-03-20 03:21:06 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.4830
2022-03-20 03:21:43 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.3836
2022-03-20 03:22:21 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.7445
2022-03-20 03:22:59 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.7354
2022-03-20 03:23:36 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.5683
2022-03-20 03:24:14 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.5465
2022-03-20 03:24:52 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.3733
2022-03-20 03:25:30 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.4503
2022-03-20 03:26:07 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.4924
2022-03-20 03:26:45 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.5055
2022-03-20 03:27:23 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.4137
2022-03-20 03:28:01 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.5397
2022-03-20 03:28:39 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.6988
2022-03-20 03:29:16 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.3975
2022-03-20 03:29:54 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.5518
2022-03-20 03:30:32 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.5272
2022-03-20 03:31:10 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.3690
2022-03-20 03:31:47 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.3596
2022-03-20 03:32:25 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.6356
2022-03-20 03:33:03 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.5146
2022-03-20 03:33:41 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.4609
2022-03-20 03:34:19 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.3724
2022-03-20 03:34:56 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.5039
2022-03-20 03:35:34 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.4674
2022-03-20 03:36:12 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.6614
2022-03-20 03:36:50 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.4474
2022-03-20 03:37:28 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.3898
2022-03-20 03:38:05 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.4477
2022-03-20 03:38:43 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.4690
2022-03-20 03:39:21 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.5124
2022-03-20 03:39:59 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.4967
2022-03-20 03:40:37 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.4436
2022-03-20 03:41:15 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.6191
2022-03-20 03:41:52 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.2538
2022-03-20 03:42:30 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.3362
2022-03-20 03:43:08 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.4535
2022-03-20 03:43:46 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.5457
2022-03-20 03:44:24 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.5613
2022-03-20 03:45:02 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.3835
2022-03-20 03:45:03 - train: epoch 036, train_loss: 1.5081
2022-03-20 03:46:19 - eval: epoch: 036, acc1: 68.572%, acc5: 88.722%, test_loss: 1.2602, per_image_load_time: 1.655ms, per_image_inference_time: 0.726ms
2022-03-20 03:46:21 - until epoch: 036, best_acc1: 68.572%
2022-03-20 03:46:21 - epoch 037 lr: 0.010000000000000002
2022-03-20 03:47:05 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.3087
2022-03-20 03:47:43 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.2586
2022-03-20 03:48:20 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.3405
2022-03-20 03:48:58 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5610
2022-03-20 03:49:35 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.4089
2022-03-20 03:50:13 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.5624
2022-03-20 03:50:51 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.4911
2022-03-20 03:51:29 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5026
2022-03-20 03:52:06 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.7244
2022-03-20 03:52:44 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.4111
2022-03-20 03:53:22 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.4997
2022-03-20 03:53:59 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.4423
2022-03-20 03:54:37 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.4533
2022-03-20 03:55:15 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.5260
2022-03-20 03:55:53 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.4888
2022-03-20 03:56:31 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.2568
2022-03-20 03:57:09 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.7692
2022-03-20 03:57:47 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.5738
2022-03-20 03:58:25 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.6003
2022-03-20 03:59:03 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.4902
2022-03-20 03:59:41 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.5207
2022-03-20 04:00:18 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.4814
2022-03-20 04:00:56 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.3463
2022-03-20 04:01:34 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.4857
2022-03-20 04:02:12 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.4945
2022-03-20 04:02:50 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.7471
2022-03-20 04:03:28 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.5790
2022-03-20 04:04:05 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5267
2022-03-20 04:04:43 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.4940
2022-03-20 04:05:21 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.6673
2022-03-20 04:05:59 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.4660
2022-03-20 04:06:37 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.6142
2022-03-20 04:07:15 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.3425
2022-03-20 04:07:53 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.3857
2022-03-20 04:08:31 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.5813
2022-03-20 04:09:09 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.6441
2022-03-20 04:09:47 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.6376
2022-03-20 04:10:25 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.4927
2022-03-20 04:11:02 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.6986
2022-03-20 04:11:40 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.4501
2022-03-20 04:12:18 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.5608
2022-03-20 04:12:56 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.6882
2022-03-20 04:13:34 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.5288
2022-03-20 04:14:12 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.4106
2022-03-20 04:14:50 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.4224
2022-03-20 04:15:28 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.5020
2022-03-20 04:16:06 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.5410
2022-03-20 04:16:44 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.4530
2022-03-20 04:17:22 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.5906
2022-03-20 04:18:00 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.5222
2022-03-20 04:18:02 - train: epoch 037, train_loss: 1.5021
2022-03-20 04:19:18 - eval: epoch: 037, acc1: 68.662%, acc5: 88.954%, test_loss: 1.2514, per_image_load_time: 2.116ms, per_image_inference_time: 0.720ms
2022-03-20 04:19:19 - until epoch: 037, best_acc1: 68.662%
2022-03-20 04:19:19 - epoch 038 lr: 0.010000000000000002
2022-03-20 04:20:03 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.4941
2022-03-20 04:20:40 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.2839
2022-03-20 04:21:18 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.2276
2022-03-20 04:21:55 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.4397
2022-03-20 04:22:33 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.3688
2022-03-20 04:23:11 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.6475
2022-03-20 04:23:48 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3091
2022-03-20 04:24:26 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.3341
2022-03-20 04:25:04 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.4509
2022-03-20 04:25:42 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.7448
2022-03-20 04:26:19 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.4766
2022-03-20 04:26:57 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.5719
2022-03-20 04:27:35 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.6839
2022-03-20 04:28:13 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.5048
2022-03-20 04:28:50 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.5268
2022-03-20 04:29:28 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.5503
2022-03-20 04:30:06 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.6825
2022-03-20 04:30:44 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.5083
2022-03-20 04:31:21 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.5997
2022-03-20 04:31:59 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.5277
2022-03-20 04:32:37 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.5616
2022-03-20 04:33:15 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.3736
2022-03-20 04:33:53 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.5440
2022-03-20 04:34:30 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.6595
2022-03-20 04:35:08 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.3479
2022-03-20 04:35:46 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.5383
2022-03-20 04:36:24 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.6674
2022-03-20 04:37:01 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.7195
2022-03-20 04:37:39 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.5437
2022-03-20 04:38:17 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.4692
2022-03-20 04:38:55 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.4834
2022-03-20 04:39:33 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2757
2022-03-20 04:40:10 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.3531
2022-03-20 04:40:48 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.4587
2022-03-20 04:41:26 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.5669
2022-03-20 04:42:04 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.5368
2022-03-20 04:42:41 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.2761
2022-03-20 04:43:19 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.5758
2022-03-20 04:43:57 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.4319
2022-03-20 04:44:35 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.5971
2022-03-20 04:45:12 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.4015
2022-03-20 04:45:50 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.3354
2022-03-20 04:46:28 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.6142
2022-03-20 04:47:06 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.4785
2022-03-20 04:47:44 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.6529
2022-03-20 04:48:21 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.6524
2022-03-20 04:48:59 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.4156
2022-03-20 04:49:37 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.5098
2022-03-20 04:50:15 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.5367
2022-03-20 04:50:53 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.2549
2022-03-20 04:50:55 - train: epoch 038, train_loss: 1.4974
2022-03-20 04:52:10 - eval: epoch: 038, acc1: 68.812%, acc5: 88.848%, test_loss: 1.2581, per_image_load_time: 0.888ms, per_image_inference_time: 0.722ms
2022-03-20 04:52:12 - until epoch: 038, best_acc1: 68.812%
2022-03-20 04:52:12 - epoch 039 lr: 0.010000000000000002
2022-03-20 04:52:55 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.4969
2022-03-20 04:53:33 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.7326
2022-03-20 04:54:11 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.3113
2022-03-20 04:54:49 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.5310
2022-03-20 04:55:26 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4365
2022-03-20 04:56:04 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.3555
2022-03-20 04:56:42 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.6851
2022-03-20 04:57:20 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.4783
2022-03-20 04:57:58 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.5782
2022-03-20 04:58:36 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.3344
2022-03-20 04:59:14 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.5414
2022-03-20 04:59:52 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.5884
2022-03-20 05:00:30 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.7174
2022-03-20 05:01:08 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.4950
2022-03-20 05:01:46 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4306
2022-03-20 05:02:24 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.6553
2022-03-20 05:03:02 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.2843
2022-03-20 05:03:39 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.6198
2022-03-20 05:04:18 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.3039
2022-03-20 05:04:56 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.4649
2022-03-20 05:05:34 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.5352
2022-03-20 05:06:12 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.4614
2022-03-20 05:06:50 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.6656
2022-03-20 05:07:28 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.7192
2022-03-20 05:08:06 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.3918
2022-03-20 05:08:44 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.4766
2022-03-20 05:09:22 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.5955
2022-03-20 05:10:00 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.4090
2022-03-20 05:10:38 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.3313
2022-03-20 05:11:16 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.5560
2022-03-20 05:11:54 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.4051
2022-03-20 05:12:32 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.5314
2022-03-20 05:13:10 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.4711
2022-03-20 05:13:48 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5571
2022-03-20 05:14:26 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.6308
2022-03-20 05:15:04 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.6355
2022-03-20 05:15:42 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.4485
2022-03-20 05:16:20 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.2790
2022-03-20 05:16:58 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.5306
2022-03-20 05:17:36 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.4240
2022-03-20 05:18:14 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.5332
2022-03-20 05:18:52 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.4773
2022-03-20 05:19:30 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.4403
2022-03-20 05:20:08 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.2966
2022-03-20 05:20:45 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.3190
2022-03-20 05:21:23 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.6133
2022-03-20 05:22:02 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.4384
2022-03-20 05:22:40 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.4929
2022-03-20 05:23:17 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.3052
2022-03-20 05:23:55 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.4594
2022-03-20 05:23:57 - train: epoch 039, train_loss: 1.4949
2022-03-20 05:25:13 - eval: epoch: 039, acc1: 68.262%, acc5: 88.896%, test_loss: 1.2580, per_image_load_time: 1.109ms, per_image_inference_time: 0.729ms
2022-03-20 05:25:14 - until epoch: 039, best_acc1: 68.812%
2022-03-20 05:25:14 - epoch 040 lr: 0.010000000000000002
2022-03-20 05:25:57 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.7030
2022-03-20 05:26:35 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.6440
2022-03-20 05:27:13 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.5818
2022-03-20 05:27:51 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.5115
2022-03-20 05:28:28 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.3696
2022-03-20 05:29:06 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.6521
2022-03-20 05:29:44 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.4918
2022-03-20 05:30:22 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.6392
2022-03-20 05:30:59 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.3613
2022-03-20 05:31:37 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.2444
2022-03-20 05:32:15 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.4133
2022-03-20 05:32:53 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.4641
2022-03-20 05:33:31 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.3922
2022-03-20 05:34:08 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.4579
2022-03-20 05:34:46 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.7368
2022-03-20 05:35:24 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.4917
2022-03-20 05:36:02 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.4834
2022-03-20 05:36:40 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.3439
2022-03-20 05:37:18 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.5585
2022-03-20 05:37:56 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.4509
2022-03-20 05:38:34 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.3767
2022-03-20 05:39:12 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.3630
2022-03-20 05:39:50 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.4838
2022-03-20 05:40:28 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.5880
2022-03-20 05:41:06 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.6011
2022-03-20 05:41:44 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.4046
2022-03-20 05:42:22 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.6061
2022-03-20 05:43:00 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.4700
2022-03-20 05:43:38 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.6896
2022-03-20 05:44:15 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.6435
2022-03-20 05:44:53 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.3956
2022-03-20 05:45:31 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.6111
2022-03-20 05:46:09 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.5890
2022-03-20 05:46:47 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.5443
2022-03-20 05:47:25 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.5544
2022-03-20 05:48:03 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.5659
2022-03-20 05:48:41 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.5079
2022-03-20 05:49:19 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.3900
2022-03-20 05:49:56 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.5038
2022-03-20 05:50:34 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.5587
2022-03-20 05:51:12 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.6168
2022-03-20 05:51:50 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.3859
2022-03-20 05:52:28 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.5700
2022-03-20 05:53:06 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.4876
2022-03-20 05:53:44 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.2840
2022-03-20 05:54:22 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5300
2022-03-20 05:55:00 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.4428
2022-03-20 05:55:38 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.3305
2022-03-20 05:56:16 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.5405
2022-03-20 05:56:54 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.5519
2022-03-20 05:56:56 - train: epoch 040, train_loss: 1.4913
2022-03-20 05:58:11 - eval: epoch: 040, acc1: 68.472%, acc5: 88.846%, test_loss: 1.2524, per_image_load_time: 0.938ms, per_image_inference_time: 0.709ms
2022-03-20 05:58:12 - until epoch: 040, best_acc1: 68.812%
2022-03-20 05:58:12 - epoch 041 lr: 0.010000000000000002
2022-03-20 05:58:56 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.7261
2022-03-20 05:59:33 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.5722
2022-03-20 06:00:11 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.4106
2022-03-20 06:00:48 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.5368
2022-03-20 06:01:26 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.2489
2022-03-20 06:02:04 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.7035
2022-03-20 06:02:41 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.3863
2022-03-20 06:03:19 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.2031
2022-03-20 06:03:57 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.2239
2022-03-20 06:04:34 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.7049
2022-03-20 06:05:12 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.4087
2022-03-20 06:05:50 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.2746
2022-03-20 06:06:27 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.4863
2022-03-20 06:07:05 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.5458
2022-03-20 06:07:43 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.8323
2022-03-20 06:08:21 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.3045
2022-03-20 06:08:58 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.4830
2022-03-20 06:09:36 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.3945
2022-03-20 06:10:14 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.5700
2022-03-20 06:10:51 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.5004
2022-03-20 06:11:29 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.2566
2022-03-20 06:12:07 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.3466
2022-03-20 06:12:45 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.2774
2022-03-20 06:13:23 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.7040
2022-03-20 06:14:01 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.4405
2022-03-20 06:14:38 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.6387
2022-03-20 06:15:16 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.7787
2022-03-20 06:15:54 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.4989
2022-03-20 06:16:32 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7272
2022-03-20 06:17:09 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.5835
2022-03-20 06:17:47 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.6851
2022-03-20 06:18:25 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.4227
2022-03-20 06:19:03 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.2935
2022-03-20 06:19:41 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.4176
2022-03-20 06:20:19 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.4599
2022-03-20 06:20:56 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.5975
2022-03-20 06:21:34 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.4298
2022-03-20 06:22:12 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4241
2022-03-20 06:22:50 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.5257
2022-03-20 06:23:28 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.3627
2022-03-20 06:24:06 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.5368
2022-03-20 06:24:43 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.4124
2022-03-20 06:25:21 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.5513
2022-03-20 06:25:59 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.4382
2022-03-20 06:26:37 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5245
2022-03-20 06:27:15 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.5890
2022-03-20 06:27:52 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.6970
2022-03-20 06:28:30 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.4634
2022-03-20 06:29:08 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.4945
2022-03-20 06:29:46 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.5889
2022-03-20 06:29:48 - train: epoch 041, train_loss: 1.4936
2022-03-20 06:31:03 - eval: epoch: 041, acc1: 68.320%, acc5: 88.818%, test_loss: 1.2636, per_image_load_time: 0.680ms, per_image_inference_time: 0.728ms
2022-03-20 06:31:04 - until epoch: 041, best_acc1: 68.812%
2022-03-20 06:31:04 - epoch 042 lr: 0.010000000000000002
2022-03-20 06:31:48 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.2537
2022-03-20 06:32:26 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.4521
2022-03-20 06:33:03 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.5257
2022-03-20 06:33:41 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.2605
2022-03-20 06:34:19 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.5289
2022-03-20 06:34:56 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.3180
2022-03-20 06:35:34 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.4982
2022-03-20 06:36:12 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.4006
2022-03-20 06:36:50 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.6946
2022-03-20 06:37:27 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.5722
2022-03-20 06:38:05 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.2858
2022-03-20 06:38:43 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.5198
2022-03-20 06:39:20 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.5464
2022-03-20 06:39:58 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.7586
2022-03-20 06:40:36 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4653
2022-03-20 06:41:14 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.5425
2022-03-20 06:41:52 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.5105
2022-03-20 06:42:30 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.4587
2022-03-20 06:43:07 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.6557
2022-03-20 06:43:45 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.3179
2022-03-20 06:44:23 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.2376
2022-03-20 06:45:01 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.3675
2022-03-20 06:45:39 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.3723
2022-03-20 06:46:17 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.6734
2022-03-20 06:46:55 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.5737
2022-03-20 06:47:33 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.5123
2022-03-20 06:48:11 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.3309
2022-03-20 06:48:49 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.4745
2022-03-20 06:49:27 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.5072
2022-03-20 06:50:05 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.4840
2022-03-20 06:50:42 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.4672
2022-03-20 06:51:20 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.5850
2022-03-20 06:51:58 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.6569
2022-03-20 06:52:36 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.2805
2022-03-20 06:53:15 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.4007
2022-03-20 06:53:53 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.5289
2022-03-20 06:54:31 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.4286
2022-03-20 06:55:08 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.3289
2022-03-20 06:55:46 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.4080
2022-03-20 06:56:24 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.5371
2022-03-20 06:57:02 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.5681
2022-03-20 06:57:40 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.6158
2022-03-20 06:58:18 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.3817
2022-03-20 06:58:56 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.4347
2022-03-20 06:59:34 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.4779
2022-03-20 07:00:12 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.7089
2022-03-20 07:00:50 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.3847
2022-03-20 07:01:28 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.5603
2022-03-20 07:02:06 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.5368
2022-03-20 07:02:44 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.3662
2022-03-20 07:02:46 - train: epoch 042, train_loss: 1.4910
2022-03-20 07:04:02 - eval: epoch: 042, acc1: 68.384%, acc5: 88.702%, test_loss: 1.2637, per_image_load_time: 0.887ms, per_image_inference_time: 0.723ms
2022-03-20 07:04:03 - until epoch: 042, best_acc1: 68.812%
2022-03-20 07:04:03 - epoch 043 lr: 0.010000000000000002
2022-03-20 07:04:47 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.4584
2022-03-20 07:05:25 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.5378
2022-03-20 07:06:03 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.2223
2022-03-20 07:06:40 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.4314
2022-03-20 07:07:18 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.5070
2022-03-20 07:07:56 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.4570
2022-03-20 07:08:34 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.5749
2022-03-20 07:09:12 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.6843
2022-03-20 07:09:50 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.4764
2022-03-20 07:10:27 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.6109
2022-03-20 07:11:05 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.5891
2022-03-20 07:11:43 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.6322
2022-03-20 07:12:21 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.5559
2022-03-20 07:12:59 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.4526
2022-03-20 07:13:37 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.3079
2022-03-20 07:14:15 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.6009
2022-03-20 07:14:53 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.5055
2022-03-20 07:15:31 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.5192
2022-03-20 07:16:08 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.5110
2022-03-20 07:16:46 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.3333
2022-03-20 07:17:24 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.4312
2022-03-20 07:18:02 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.4662
2022-03-20 07:18:40 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.6450
2022-03-20 07:19:18 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.3488
2022-03-20 07:19:56 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.6229
2022-03-20 07:20:34 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.2186
2022-03-20 07:21:12 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.5129
2022-03-20 07:21:50 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.2964
2022-03-20 07:22:27 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.5059
2022-03-20 07:23:05 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.7229
2022-03-20 07:23:43 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.6912
2022-03-20 07:24:21 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.6504
2022-03-20 07:24:59 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.5805
2022-03-20 07:25:37 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.5280
2022-03-20 07:26:15 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.5975
2022-03-20 07:26:54 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.5280
2022-03-20 07:27:32 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.4896
2022-03-20 07:28:09 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.5993
2022-03-20 07:28:47 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.6891
2022-03-20 07:29:26 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.5803
2022-03-20 07:30:04 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.7908
2022-03-20 07:30:41 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.5155
2022-03-20 07:31:19 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.5675
2022-03-20 07:31:58 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.5560
2022-03-20 07:32:36 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.5196
2022-03-20 07:33:14 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.4605
2022-03-20 07:33:52 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.4240
2022-03-20 07:34:30 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.3853
2022-03-20 07:35:08 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.4318
2022-03-20 07:35:45 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.3816
2022-03-20 07:35:47 - train: epoch 043, train_loss: 1.4881
2022-03-20 07:37:03 - eval: epoch: 043, acc1: 68.500%, acc5: 88.920%, test_loss: 1.2576, per_image_load_time: 1.974ms, per_image_inference_time: 0.731ms
2022-03-20 07:37:04 - until epoch: 043, best_acc1: 68.812%
2022-03-20 07:37:04 - epoch 044 lr: 0.010000000000000002
2022-03-20 07:37:48 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.4893
2022-03-20 07:38:26 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.5185
2022-03-20 07:39:03 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.5004
2022-03-20 07:39:41 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.2606
2022-03-20 07:40:19 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.3805
2022-03-20 07:40:56 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.2003
2022-03-20 07:41:34 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.1984
2022-03-20 07:42:12 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.3807
2022-03-20 07:42:49 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.4526
2022-03-20 07:43:27 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.5303
2022-03-20 07:44:05 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.4409
2022-03-20 07:44:42 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.4495
2022-03-20 07:45:20 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.5468
2022-03-20 07:45:58 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.3475
2022-03-20 07:46:36 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.4057
2022-03-20 07:47:14 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.3805
2022-03-20 07:47:52 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.3480
2022-03-20 07:48:30 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.4944
2022-03-20 07:49:07 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.6596
2022-03-20 07:49:45 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.4860
2022-03-20 07:50:23 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.6750
2022-03-20 07:51:01 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.5664
2022-03-20 07:51:39 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.6339
2022-03-20 07:52:17 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.5914
2022-03-20 07:52:55 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.6357
2022-03-20 07:53:33 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.6036
2022-03-20 07:54:10 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.5855
2022-03-20 07:54:48 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.2049
2022-03-20 07:55:26 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.3870
2022-03-20 07:56:04 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.4307
2022-03-20 07:56:42 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.5504
2022-03-20 07:57:20 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.2797
2022-03-20 07:57:57 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.3988
2022-03-20 07:58:35 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.7037
2022-03-20 07:59:13 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.5073
2022-03-20 07:59:51 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.4961
2022-03-20 08:00:29 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.6187
2022-03-20 08:01:06 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.2594
2022-03-20 08:01:44 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.5775
2022-03-20 08:02:22 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.7023
2022-03-20 08:03:00 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.3514
2022-03-20 08:03:38 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.5417
2022-03-20 08:04:15 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.6421
2022-03-20 08:04:53 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.3246
2022-03-20 08:05:31 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.6038
2022-03-20 08:06:09 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.3427
