2022-02-22 05:09:25 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.1529
2022-02-22 05:10:02 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 0.9721
2022-02-22 05:10:41 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.2140
2022-02-22 05:11:18 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 0.9461
2022-02-22 05:11:57 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.1561
2022-02-22 05:12:33 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.1473
2022-02-22 05:13:12 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 0.9716
2022-02-22 05:13:48 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 0.9609
2022-02-22 05:14:25 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.1035
2022-02-22 05:15:03 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.1816
2022-02-22 05:15:39 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.1116
2022-02-22 05:16:18 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 0.9426
2022-02-22 05:16:55 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.9038
2022-02-22 05:17:31 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.1001
2022-02-22 05:17:32 - train: epoch 088, train_loss: 1.0539
2022-02-22 05:18:55 - eval: epoch: 088, acc1: 73.522%, acc5: 91.482%, test_loss: 1.0610, per_image_load_time: 0.993ms, per_image_inference_time: 0.277ms
2022-02-22 05:18:56 - until epoch: 088, best_acc1: 73.522%
2022-02-22 05:18:56 - epoch 089 lr: 0.0010000000000000002
2022-02-22 05:19:38 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.2518
2022-02-22 05:20:16 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 0.8811
2022-02-22 05:20:53 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.1221
2022-02-22 05:21:30 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 0.9809
2022-02-22 05:22:08 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.0949
2022-02-22 05:22:45 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.0428
2022-02-22 05:23:23 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.1977
2022-02-22 05:24:00 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.1692
2022-02-22 05:24:38 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.0744
2022-02-22 05:25:16 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.1827
2022-02-22 05:25:53 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.0378
2022-02-22 05:26:29 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.1324
2022-02-22 05:27:06 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.0260
2022-02-22 05:27:42 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.1832
2022-02-22 05:28:19 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.0584
2022-02-22 05:28:56 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.0137
2022-02-22 05:29:33 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.1416
2022-02-22 05:30:09 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 0.9851
2022-02-22 05:30:45 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 0.9630
2022-02-22 05:31:22 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 0.9749
2022-02-22 05:31:58 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.1468
2022-02-22 05:32:35 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.0464
2022-02-22 05:33:11 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 0.9492
2022-02-22 05:33:48 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.1546
2022-02-22 05:34:24 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.0065
2022-02-22 05:35:01 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.0648
2022-02-22 05:35:37 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 0.8907
2022-02-22 05:36:14 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.0392
2022-02-22 05:36:50 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.1020
2022-02-22 05:37:26 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.1147
2022-02-22 05:38:03 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.1181
2022-02-22 05:38:39 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.1538
2022-02-22 05:39:16 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.1696
2022-02-22 05:39:53 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.0670
2022-02-22 05:40:29 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 0.9491
2022-02-22 05:41:06 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 0.9885
2022-02-22 05:41:42 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.2744
2022-02-22 05:42:19 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 0.9745
2022-02-22 05:42:56 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.1080
2022-02-22 05:43:33 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 0.9135
2022-02-22 05:44:09 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.3600
2022-02-22 05:44:46 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 0.9656
2022-02-22 05:45:22 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.0782
2022-02-22 05:45:58 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 0.9931
2022-02-22 05:46:34 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 0.8795
2022-02-22 05:47:11 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.0431
2022-02-22 05:47:48 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.1256
2022-02-22 05:48:24 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.0622
2022-02-22 05:49:00 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.0966
2022-02-22 05:49:35 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.8520
2022-02-22 05:49:36 - train: epoch 089, train_loss: 1.0532
2022-02-22 05:50:58 - eval: epoch: 089, acc1: 73.204%, acc5: 91.446%, test_loss: 1.0635, per_image_load_time: 0.966ms, per_image_inference_time: 0.257ms
2022-02-22 05:50:59 - until epoch: 089, best_acc1: 73.522%
2022-02-22 05:50:59 - epoch 090 lr: 0.0010000000000000002
2022-02-22 05:51:41 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.0336
2022-02-22 05:52:16 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.0931
2022-02-22 05:52:52 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.0159
2022-02-22 05:53:30 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.0321
2022-02-22 05:54:07 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.1594
2022-02-22 05:54:43 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.1379
2022-02-22 05:55:20 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.0812
2022-02-22 05:55:55 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.0451
2022-02-22 05:56:32 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.0196
2022-02-22 05:57:09 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 0.8880
2022-02-22 05:57:46 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.1392
2022-02-22 05:58:23 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 0.9451
2022-02-22 05:58:59 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.1793
2022-02-22 05:59:35 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 0.8928
2022-02-22 06:00:11 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.1769
2022-02-22 06:00:49 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.0029
2022-02-22 06:01:24 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 0.8636
2022-02-22 06:02:01 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 0.8485
2022-02-22 06:02:37 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 0.9288
2022-02-22 06:03:14 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.1428
2022-02-22 06:03:49 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.1964
2022-02-22 06:04:27 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.1021
2022-02-22 06:05:04 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.2527
2022-02-22 06:05:40 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.0596
2022-02-22 06:06:16 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.0766
2022-02-22 06:06:53 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.0601
2022-02-22 06:07:29 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 0.9579
2022-02-22 06:08:07 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.1138
2022-02-22 06:08:42 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.0130
2022-02-22 06:09:19 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.0110
2022-02-22 06:09:54 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 0.9435
2022-02-22 06:10:30 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 0.9321
2022-02-22 06:11:05 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.1373
2022-02-22 06:11:42 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 0.8752
2022-02-22 06:12:18 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.0634
2022-02-22 06:12:55 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.0086
2022-02-22 06:13:30 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.0559
2022-02-22 06:14:08 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.1306
2022-02-22 06:14:44 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 0.8582
2022-02-22 06:15:21 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.0868
2022-02-22 06:15:58 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.2892
2022-02-22 06:16:36 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.1127
2022-02-22 06:17:11 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.0436
2022-02-22 06:17:47 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 0.9906
2022-02-22 06:18:24 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.0115
2022-02-22 06:19:01 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.0596
2022-02-22 06:19:36 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 0.9948
2022-02-22 06:20:13 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.0940
2022-02-22 06:20:49 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 0.9932
2022-02-22 06:21:25 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.0054
2022-02-22 06:21:26 - train: epoch 090, train_loss: 1.0499
2022-02-22 06:22:42 - eval: epoch: 090, acc1: 73.508%, acc5: 91.548%, test_loss: 1.0604, per_image_load_time: 2.356ms, per_image_inference_time: 0.295ms
2022-02-22 06:22:43 - until epoch: 090, best_acc1: 73.522%
2022-02-22 06:22:43 - epoch 091 lr: 0.00010000000000000003
2022-02-22 06:23:26 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 0.9818
2022-02-22 06:24:02 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 0.9789
2022-02-22 06:24:40 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 0.9510
2022-02-22 06:25:15 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 0.9767
2022-02-22 06:25:52 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.0286
2022-02-22 06:26:28 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.0056
2022-02-22 06:27:05 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 0.9877
2022-02-22 06:27:40 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.8991
2022-02-22 06:28:18 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.0673
2022-02-22 06:28:53 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 0.9457
2022-02-22 06:29:30 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 0.8407
2022-02-22 06:30:07 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.1303
2022-02-22 06:30:43 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 0.9512
2022-02-22 06:31:19 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.0438
2022-02-22 06:31:56 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.0404
2022-02-22 06:32:33 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.0534
2022-02-22 06:33:10 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.1283
2022-02-22 06:33:46 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 0.9728
2022-02-22 06:34:23 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 0.8990
2022-02-22 06:35:00 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.0213
2022-02-22 06:35:36 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 0.8229
2022-02-22 06:36:13 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.1282
2022-02-22 06:36:50 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.1167
2022-02-22 06:37:26 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.0127
2022-02-22 06:38:03 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 0.8558
2022-02-22 06:38:40 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 0.9807
2022-02-22 06:39:16 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 0.9043
2022-02-22 06:39:53 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.0283
2022-02-22 06:40:30 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.1524
2022-02-22 06:41:07 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.0940
2022-02-22 06:41:44 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 0.9786
2022-02-22 06:42:19 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.1008
2022-02-22 06:42:56 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 0.8888
2022-02-22 06:43:32 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 0.8931
2022-02-22 06:44:09 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.0824
2022-02-22 06:44:46 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 0.9332
2022-02-22 06:45:23 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.0237
2022-02-22 06:45:58 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.0344
2022-02-22 06:46:36 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.0031
2022-02-22 06:47:12 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.0343
2022-02-22 06:47:49 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.0328
2022-02-22 06:48:25 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.0476
2022-02-22 06:49:02 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.0882
2022-02-22 06:49:38 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 0.8893
2022-02-22 06:50:15 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 0.8888
2022-02-22 06:50:51 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 0.9817
2022-02-22 06:51:28 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 0.9616
2022-02-22 06:52:04 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 0.9963
2022-02-22 06:52:41 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 0.9504
2022-02-22 06:53:15 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.1918
2022-02-22 06:53:16 - train: epoch 091, train_loss: 1.0175
2022-02-22 06:54:39 - eval: epoch: 091, acc1: 73.844%, acc5: 91.698%, test_loss: 1.0430, per_image_load_time: 0.976ms, per_image_inference_time: 0.268ms
2022-02-22 06:54:40 - until epoch: 091, best_acc1: 73.844%
2022-02-22 06:54:40 - epoch 092 lr: 0.00010000000000000003
2022-02-22 06:55:21 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.0111
2022-02-22 06:55:58 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.0664
2022-02-22 06:56:34 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 0.9910
2022-02-22 06:57:12 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 0.7860
2022-02-22 06:57:48 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.2080
2022-02-22 06:58:24 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.1691
2022-02-22 06:59:00 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.1030
2022-02-22 06:59:37 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.1268
2022-02-22 07:00:13 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 0.9649
2022-02-22 07:00:50 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.2509
2022-02-22 07:01:27 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 0.8285
2022-02-22 07:02:03 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 0.9764
2022-02-22 07:02:40 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 0.9355
2022-02-22 07:03:17 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.0097
2022-02-22 07:03:54 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 0.9637
2022-02-22 07:04:30 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.0301
2022-02-22 07:05:07 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.0669
2022-02-22 07:05:44 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 0.9042
2022-02-22 07:06:20 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 0.9269
2022-02-22 07:06:58 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 0.9365
2022-02-22 07:07:33 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 0.9506
2022-02-22 07:08:10 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 0.9812
2022-02-22 07:08:46 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 0.9882
2022-02-22 07:09:23 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 0.9123
2022-02-22 07:09:59 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 0.9952
2022-02-22 07:10:36 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.1450
2022-02-22 07:11:12 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.0295
2022-02-22 07:11:49 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 0.9223
2022-02-22 07:12:27 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.0227
2022-02-22 07:13:03 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 0.9399
2022-02-22 07:13:39 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 0.9807
2022-02-22 07:14:15 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 0.8672
2022-02-22 07:14:52 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.0102
2022-02-22 07:15:28 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.1663
2022-02-22 07:16:06 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 0.8758
2022-02-22 07:16:42 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.0246
2022-02-22 07:17:18 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 0.9644
2022-02-22 07:17:55 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.1489
2022-02-22 07:18:31 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.0213
2022-02-22 07:19:07 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.1660
2022-02-22 07:19:44 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 0.8402
2022-02-22 07:20:20 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.0254
2022-02-22 07:20:56 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.0346
2022-02-22 07:21:33 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 0.8617
2022-02-22 07:22:10 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 0.9476
2022-02-22 07:22:47 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 0.9243
2022-02-22 07:23:23 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.6907
2022-02-22 07:24:00 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 0.8982
2022-02-22 07:24:37 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 0.9960
2022-02-22 07:25:12 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.1732
2022-02-22 07:25:13 - train: epoch 092, train_loss: 1.0080
2022-02-22 07:26:34 - eval: epoch: 092, acc1: 73.824%, acc5: 91.686%, test_loss: 1.0413, per_image_load_time: 0.872ms, per_image_inference_time: 0.275ms
2022-02-22 07:26:35 - until epoch: 092, best_acc1: 73.844%
2022-02-22 07:26:35 - epoch 093 lr: 0.00010000000000000003
2022-02-22 07:27:17 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 0.9792
2022-02-22 07:27:54 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 0.9080
2022-02-22 07:28:31 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 0.9331
2022-02-22 07:29:07 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.0288
2022-02-22 07:29:44 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.0738
2022-02-22 07:30:21 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 0.9528
2022-02-22 07:30:58 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.0460
2022-02-22 07:31:35 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.9682
2022-02-22 07:32:12 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 0.9548
2022-02-22 07:32:49 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 0.9099
2022-02-22 07:33:26 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 0.9041
2022-02-22 07:34:03 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 0.9252
2022-02-22 07:34:40 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.0383
2022-02-22 07:35:17 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 0.9586
2022-02-22 07:35:53 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.1552
2022-02-22 07:36:30 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.0542
2022-02-22 07:37:07 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 0.9545
2022-02-22 07:37:43 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.0083
2022-02-22 07:38:20 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 0.9069
2022-02-22 07:38:57 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 0.9392
2022-02-22 07:39:34 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.0334
2022-02-22 07:40:10 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.2815
2022-02-22 07:40:47 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.0489
2022-02-22 07:41:23 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 0.9156
2022-02-22 07:42:00 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.0711
2022-02-22 07:42:37 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.1076
2022-02-22 07:43:14 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 0.9418
2022-02-22 07:43:51 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 0.9369
2022-02-22 07:44:27 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.0916
2022-02-22 07:45:04 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 0.8968
2022-02-22 07:45:40 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.0865
2022-02-22 07:46:18 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 0.9061
2022-02-22 07:46:53 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.1378
2022-02-22 07:47:30 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.2374
2022-02-22 07:48:07 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 0.7326
2022-02-22 07:48:42 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.1339
2022-02-22 07:49:20 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 0.9511
2022-02-22 07:49:55 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 0.8411
2022-02-22 07:50:33 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.0817
2022-02-22 07:51:08 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.2461
2022-02-22 07:51:44 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.0578
2022-02-22 07:52:20 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.0262
2022-02-22 07:52:57 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.1376
2022-02-22 07:53:33 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 0.8613
2022-02-22 07:54:09 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 0.9807
2022-02-22 07:54:45 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 0.7944
2022-02-22 07:55:23 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.2682
2022-02-22 07:55:59 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.0138
2022-02-22 07:56:36 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.0491
2022-02-22 07:57:10 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 0.9450
2022-02-22 07:57:11 - train: epoch 093, train_loss: 1.0057
2022-02-22 07:58:32 - eval: epoch: 093, acc1: 73.924%, acc5: 91.714%, test_loss: 1.0400, per_image_load_time: 0.883ms, per_image_inference_time: 0.257ms
2022-02-22 07:58:33 - until epoch: 093, best_acc1: 73.924%
2022-02-22 22:39:51 - epoch 094 lr: 0.00010000000000000003
2022-02-22 22:40:35 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 0.8913
2022-02-22 22:41:11 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 0.9574
2022-02-22 22:41:50 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.1461
2022-02-22 22:42:26 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 0.8856
2022-02-22 22:43:06 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 0.9158
2022-02-22 22:43:43 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 0.9037
2022-02-22 22:44:21 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 0.9437
2022-02-22 22:45:00 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.0264
2022-02-22 22:45:37 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 0.9259
2022-02-22 22:46:15 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.0722
2022-02-22 22:46:52 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.0122
2022-02-22 22:47:30 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 0.8161
2022-02-22 22:48:08 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 0.9459
2022-02-22 22:48:45 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.0805
2022-02-22 22:49:23 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.0981
2022-02-22 22:50:01 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.2379
2022-02-22 22:50:39 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.0141
2022-02-22 22:51:16 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 0.9825
2022-02-22 22:51:55 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.0361
2022-02-22 22:52:33 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.7819
2022-02-22 22:53:10 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 0.9159
2022-02-22 22:53:49 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 0.8192
2022-02-22 22:54:27 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 0.8963
2022-02-22 22:55:05 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.0653
2022-02-22 22:55:43 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 0.9663
2022-02-22 22:56:22 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 0.9284
2022-02-22 22:57:00 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 0.8143
2022-02-22 22:57:39 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.0877
2022-02-22 22:58:17 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 0.7888
2022-02-22 22:58:56 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.0282
2022-02-22 22:59:35 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.2486
2022-02-22 23:00:12 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 0.9406
2022-02-22 23:00:51 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.1491
2022-02-22 23:01:30 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 0.9717
2022-02-22 23:02:09 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.1356
2022-02-22 23:02:46 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 0.9866
2022-02-22 23:03:25 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.1139
2022-02-22 23:04:02 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 0.9281
2022-02-22 23:04:41 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.0512
2022-02-22 23:05:19 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.0920
2022-02-22 23:05:58 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.1975
2022-02-22 23:06:35 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.0469
2022-02-22 23:07:14 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.1947
2022-02-22 23:07:52 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 0.9385
2022-02-22 23:08:31 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 0.9362
2022-02-22 23:09:08 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.1006
2022-02-22 23:09:47 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.2381
2022-02-22 23:10:24 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.0330
2022-02-22 23:11:02 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.0659
2022-02-22 23:11:38 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.0049
2022-02-22 23:11:39 - train: epoch 094, train_loss: 1.0022
2022-02-22 23:13:03 - eval: epoch: 094, acc1: 73.912%, acc5: 91.732%, test_loss: 1.0403, per_image_load_time: 2.924ms, per_image_inference_time: 0.294ms
2022-02-22 23:13:04 - until epoch: 094, best_acc1: 73.924%
2022-02-22 23:13:04 - epoch 095 lr: 0.00010000000000000003
2022-02-22 23:13:47 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.2109
2022-02-22 23:14:24 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.0905
2022-02-22 23:15:03 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.0820
2022-02-22 23:15:41 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.0567
2022-02-22 23:16:19 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 0.9778
2022-02-22 23:16:56 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 0.9245
2022-02-22 23:17:33 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 0.9992
2022-02-22 23:18:13 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.0707
2022-02-22 23:18:50 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.0931
2022-02-22 23:19:28 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.1349
2022-02-22 23:20:05 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 0.7885
2022-02-22 23:20:42 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.0339
2022-02-22 23:21:20 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 0.8731
2022-02-22 23:21:59 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 0.9251
2022-02-22 23:22:36 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 0.7949
2022-02-22 23:23:13 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.7911
2022-02-22 23:23:51 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 0.9768
2022-02-22 23:24:29 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 0.9960
2022-02-22 23:25:07 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.0672
2022-02-22 23:25:45 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.0047
2022-02-22 23:26:22 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 0.9224
2022-02-22 23:27:00 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.8713
2022-02-22 23:27:37 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 0.9273
2022-02-22 23:28:15 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 0.8038
2022-02-22 23:28:52 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 0.8893
2022-02-22 23:29:31 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.0262
2022-02-22 23:30:08 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.0038
2022-02-22 23:30:43 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 0.7865
2022-02-22 23:31:18 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.0067
2022-02-22 23:31:56 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.1001
2022-02-22 23:32:36 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.0413
2022-02-22 23:33:15 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.0009
2022-02-22 23:33:52 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.0579
2022-02-22 23:34:29 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 0.7886
2022-02-22 23:35:08 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 0.9706
2022-02-22 23:35:45 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 0.8067
2022-02-22 23:36:23 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 0.8712
2022-02-22 23:37:01 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.1639
2022-02-22 23:37:38 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 0.9114
2022-02-22 23:38:15 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 0.8902
2022-02-22 23:38:53 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 0.9531
2022-02-22 23:39:30 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 0.7648
2022-02-22 23:40:08 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.1912
2022-02-22 23:40:46 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 0.9238
2022-02-22 23:41:24 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 0.9734
2022-02-22 23:42:01 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.0763
2022-02-22 23:42:38 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 0.8208
2022-02-22 23:43:17 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 0.9575
2022-02-22 23:43:54 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 0.8937
2022-02-22 23:44:30 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 0.8951
2022-02-22 23:44:31 - train: epoch 095, train_loss: 1.0018
2022-02-22 23:45:55 - eval: epoch: 095, acc1: 73.878%, acc5: 91.742%, test_loss: 1.0397, per_image_load_time: 2.926ms, per_image_inference_time: 0.316ms
2022-02-22 23:45:56 - until epoch: 095, best_acc1: 73.924%
2022-02-22 23:45:56 - epoch 096 lr: 0.00010000000000000003
2022-02-22 23:46:39 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.1907
2022-02-22 23:47:16 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 0.9086
2022-02-22 23:47:55 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 0.9371
2022-02-22 23:48:32 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 0.6867
2022-02-22 23:49:10 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.0736
2022-02-22 23:49:47 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.0455
2022-02-22 23:50:24 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.0235
2022-02-22 23:51:04 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 0.8180
2022-02-22 23:51:41 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 0.9971
2022-02-22 23:52:18 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 0.9132
2022-02-22 23:52:57 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.1213
2022-02-22 23:53:35 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.0906
2022-02-22 23:54:13 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 0.9434
2022-02-22 23:54:50 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.0936
2022-02-22 23:55:28 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.1543
2022-02-22 23:56:06 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.7462
2022-02-22 23:56:44 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 0.9554
2022-02-22 23:57:21 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.0364
2022-02-22 23:57:59 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.0875
2022-02-22 23:58:38 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.0547
2022-02-22 23:59:16 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 0.9502
2022-02-22 23:59:53 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.8019
2022-02-23 00:00:31 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 0.9895
2022-02-23 00:01:08 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 0.8750
2022-02-23 00:01:46 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 0.8547
2022-02-23 00:02:25 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.0302
2022-02-23 00:03:02 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.0815
2022-02-23 00:03:39 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 0.9131
2022-02-23 00:04:18 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.1108
2022-02-23 00:04:57 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 0.9637
2022-02-23 00:05:36 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.0150
2022-02-23 00:06:13 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.1621
2022-02-23 00:06:50 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.3186
2022-02-23 00:07:27 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 0.8125
2022-02-23 00:08:06 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 0.8783
2022-02-23 00:08:43 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 0.8061
2022-02-23 00:09:21 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.0302
2022-02-23 00:09:58 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.0183
2022-02-23 00:10:36 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 0.9760
2022-02-23 00:11:14 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.0758
2022-02-23 00:11:53 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.0355
2022-02-23 00:12:31 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 0.9903
2022-02-23 00:13:08 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 0.8115
2022-02-23 00:13:45 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 0.9128
2022-02-23 00:14:24 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.1445
2022-02-23 00:15:01 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 0.9172
2022-02-23 00:15:40 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.1562
2022-02-23 00:16:17 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.1132
2022-02-23 00:16:54 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.0657
2022-02-23 00:17:30 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 0.9071
2022-02-23 00:17:31 - train: epoch 096, train_loss: 1.0005
2022-02-23 00:18:56 - eval: epoch: 096, acc1: 73.876%, acc5: 91.756%, test_loss: 1.0386, per_image_load_time: 3.032ms, per_image_inference_time: 0.277ms
2022-02-23 00:18:57 - until epoch: 096, best_acc1: 73.924%
2022-02-23 00:18:57 - epoch 097 lr: 0.00010000000000000003
2022-02-23 00:19:40 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 0.9123
2022-02-23 00:20:17 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.0089
2022-02-23 00:20:55 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.0039
2022-02-23 00:21:32 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.0135
2022-02-23 00:22:10 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 0.9722
2022-02-23 00:22:48 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.0216
2022-02-23 00:23:26 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.0090
2022-02-23 00:24:03 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 0.8694
2022-02-23 00:24:41 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 0.9970
2022-02-23 00:25:19 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.0176
2022-02-23 00:25:57 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 0.8059
2022-02-23 00:26:35 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.0319
2022-02-23 00:27:13 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.1393
2022-02-23 00:27:51 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.0527
2022-02-23 00:28:28 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 0.7765
2022-02-23 00:29:06 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 0.9922
2022-02-23 00:29:45 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 0.9748
2022-02-23 00:30:23 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.2428
2022-02-23 00:31:00 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 0.9179
2022-02-23 00:31:38 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 0.9059
2022-02-23 00:32:16 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 0.9242
2022-02-23 00:32:53 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 0.9523
2022-02-23 00:33:32 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 0.9260
2022-02-23 00:34:09 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 0.9518
2022-02-23 00:34:47 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 0.9238
2022-02-23 00:35:24 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 0.9822
2022-02-23 00:36:03 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 0.9859
2022-02-23 00:36:41 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 0.9006
2022-02-23 00:37:18 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.0891
2022-02-23 00:37:55 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 0.9439
2022-02-23 00:38:33 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.0742
2022-02-23 00:39:10 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.1104
2022-02-23 00:39:50 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.1403
2022-02-23 00:40:27 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.0889
2022-02-23 00:41:04 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 0.7866
2022-02-23 00:41:42 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 0.9832
2022-02-23 00:42:19 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 0.7976
2022-02-23 00:42:58 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 0.9725
2022-02-23 00:43:35 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 0.9140
2022-02-23 00:44:12 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.0958
2022-02-23 00:44:49 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 0.9721
2022-02-23 00:45:25 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.0679
2022-02-23 00:46:03 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.0384
2022-02-23 00:46:41 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.0464
2022-02-23 00:47:19 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 0.9949
2022-02-23 00:47:57 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 0.9222
2022-02-23 00:48:35 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 0.9403
2022-02-23 00:49:13 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 0.9219
2022-02-23 00:49:51 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.1560
2022-02-23 00:50:27 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 0.8344
2022-02-23 00:50:28 - train: epoch 097, train_loss: 0.9970
2022-02-23 00:51:53 - eval: epoch: 097, acc1: 73.930%, acc5: 91.760%, test_loss: 1.0378, per_image_load_time: 3.017ms, per_image_inference_time: 0.305ms
2022-02-23 00:51:54 - until epoch: 097, best_acc1: 73.930%
2022-02-23 00:51:54 - epoch 098 lr: 0.00010000000000000003
2022-02-23 00:52:37 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 0.9235
2022-02-23 00:53:15 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.0685
2022-02-23 00:53:53 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 0.9234
2022-02-23 00:54:31 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 0.9890
2022-02-23 00:55:10 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.1391
2022-02-23 00:55:47 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.0256
2022-02-23 00:56:25 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.0174
2022-02-23 00:57:02 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.1024
2022-02-23 00:57:41 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.0516
2022-02-23 00:58:19 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 0.8761
2022-02-23 00:58:56 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.0463
2022-02-23 00:59:34 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.0789
2022-02-23 01:00:12 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.0765
2022-02-23 01:00:51 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 0.9498
2022-02-23 01:01:28 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 0.9518
2022-02-23 01:02:07 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 0.8946
2022-02-23 01:02:44 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.0579
2022-02-23 01:03:23 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.0419
2022-02-23 01:04:00 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 0.8904
2022-02-23 01:04:39 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.0493
2022-02-23 01:05:17 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.0281
2022-02-23 01:05:55 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 0.9080
2022-02-23 01:06:31 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.0483
2022-02-23 01:07:09 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 0.8716
2022-02-23 01:07:46 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 0.9687
2022-02-23 01:08:25 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 0.9731
2022-02-23 01:09:03 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 0.9200
2022-02-23 01:09:40 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 0.9469
2022-02-23 01:10:17 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.0094
2022-02-23 01:10:56 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.0339
2022-02-23 01:11:34 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 0.9114
2022-02-23 01:12:12 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 0.8283
2022-02-23 01:12:49 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 0.9399
2022-02-23 01:13:27 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 0.8938
2022-02-23 01:14:06 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 0.8854
2022-02-23 01:14:43 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.1120
2022-02-23 01:15:21 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.0114
2022-02-23 01:15:59 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 0.9928
2022-02-23 01:16:36 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 0.9506
2022-02-23 01:17:14 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.1725
2022-02-23 01:17:50 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 0.9732
2022-02-23 01:18:29 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 0.9454
2022-02-23 01:19:08 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 0.9575
2022-02-23 01:19:45 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.1665
2022-02-23 01:20:24 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.0257
2022-02-23 01:21:02 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 0.9708
2022-02-23 01:21:39 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 0.9099
2022-02-23 01:22:16 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 0.7000
2022-02-23 01:22:55 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.0588
2022-02-23 01:23:31 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 0.9237
2022-02-23 01:23:33 - train: epoch 098, train_loss: 0.9984
2022-02-23 01:24:57 - eval: epoch: 098, acc1: 73.858%, acc5: 91.726%, test_loss: 1.0383, per_image_load_time: 2.515ms, per_image_inference_time: 0.279ms
2022-02-23 01:24:58 - until epoch: 098, best_acc1: 73.930%
2022-02-23 01:24:58 - epoch 099 lr: 0.00010000000000000003
2022-02-23 01:25:41 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 0.9036
2022-02-23 01:26:18 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 0.9717
2022-02-23 01:26:56 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 0.8059
2022-02-23 01:27:34 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.0084
2022-02-23 01:28:11 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 0.9378
2022-02-23 01:28:50 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 0.9238
2022-02-23 01:29:28 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.1905
2022-02-23 01:30:05 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.0311
2022-02-23 01:30:43 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.1503
2022-02-23 01:31:21 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 1.0502
2022-02-23 01:31:59 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.1489
2022-02-23 01:32:36 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 0.8033
2022-02-23 01:33:15 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 1.0510
2022-02-23 01:33:52 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 0.9426
2022-02-23 01:34:30 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 0.9539
2022-02-23 01:35:07 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.1480
2022-02-23 01:35:43 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 0.9513
2022-02-23 01:36:22 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 0.9098
2022-02-23 01:37:00 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 0.9980
2022-02-23 01:37:37 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 0.8558
2022-02-23 01:38:15 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 0.9348
2022-02-23 01:38:52 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 0.9013
2022-02-23 01:39:26 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 0.8482
2022-02-23 01:40:04 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.2386
2022-02-23 01:40:42 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 1.0751
2022-02-23 01:41:22 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.0511
2022-02-23 01:41:59 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 0.9763
2022-02-23 01:42:38 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.0058
2022-02-23 01:43:16 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 0.9420
2022-02-23 01:43:54 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 0.9922
2022-02-23 01:44:32 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 0.9505
2022-02-23 01:45:10 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.1548
2022-02-23 01:45:48 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 0.9860
2022-02-23 01:46:26 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 0.9069
2022-02-23 01:47:03 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.0651
2022-02-23 01:47:42 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.0055
2022-02-23 01:48:20 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 0.7492
2022-02-23 01:48:57 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.0237
2022-02-23 01:49:37 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 0.8997
2022-02-23 01:50:13 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.0412
2022-02-23 01:50:53 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 0.8674
2022-02-23 01:51:32 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.0845
2022-02-23 01:52:09 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.0305
2022-02-23 01:52:47 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.0025
2022-02-23 01:53:24 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.2742
2022-02-23 01:54:04 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.0906
2022-02-23 01:54:41 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.1796
2022-02-23 01:55:20 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 0.9782
2022-02-23 01:55:57 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.0599
2022-02-23 01:56:34 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 0.9858
2022-02-23 01:56:35 - train: epoch 099, train_loss: 0.9965
2022-02-23 01:58:00 - eval: epoch: 099, acc1: 73.922%, acc5: 91.776%, test_loss: 1.0375, per_image_load_time: 2.951ms, per_image_inference_time: 0.302ms
2022-02-23 01:58:00 - until epoch: 099, best_acc1: 73.930%
2022-02-23 01:58:00 - epoch 100 lr: 0.00010000000000000003
2022-02-23 01:58:44 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.0974
2022-02-23 01:59:21 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.0268
2022-02-23 02:00:00 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.0598
2022-02-23 02:00:38 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 0.7729
2022-02-23 02:01:17 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.2607
2022-02-23 02:01:55 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.0211
2022-02-23 02:02:34 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 0.9134
2022-02-23 02:03:11 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 0.8923
2022-02-23 02:03:50 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 0.9877
2022-02-23 02:04:28 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 1.0225
2022-02-23 02:05:06 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 0.8713
2022-02-23 02:05:45 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.1232
2022-02-23 02:06:22 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 0.9903
2022-02-23 02:06:59 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.0779
2022-02-23 02:07:39 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.1049
2022-02-23 02:08:16 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 0.9171
2022-02-23 02:08:55 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.0199
2022-02-23 02:09:34 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 0.8772
2022-02-23 02:10:14 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 1.1930
2022-02-23 02:10:52 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.0667
2022-02-23 02:11:31 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 0.9393
2022-02-23 02:12:10 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.0328
2022-02-23 02:12:49 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 0.9420
2022-02-23 02:13:26 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.0257
2022-02-23 02:14:05 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 0.9152
2022-02-23 02:14:43 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.0741
2022-02-23 02:15:21 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 0.9384
2022-02-23 02:16:00 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 0.9362
2022-02-23 02:16:38 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 0.9574
2022-02-23 02:17:15 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 0.8202
2022-02-23 02:17:54 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 0.7780
2022-02-23 02:18:31 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.2320
2022-02-23 02:19:10 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 0.9541
2022-02-23 02:19:49 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.1748
2022-02-23 02:20:27 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 0.9483
2022-02-23 02:21:04 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 0.9324
2022-02-23 02:21:44 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 0.9392
2022-02-23 02:22:22 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 0.9277
2022-02-23 02:23:01 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.0241
2022-02-23 02:23:39 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 0.9777
2022-02-23 02:24:18 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.0517
2022-02-23 02:24:57 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 0.9445
2022-02-23 02:25:35 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 0.9358
2022-02-23 02:26:13 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.0296
2022-02-23 02:26:51 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 0.8840
2022-02-23 02:27:29 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 0.9853
2022-02-23 02:28:07 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.0656
2022-02-23 02:28:46 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 0.9246
2022-02-23 02:29:23 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 0.9236
2022-02-23 02:30:01 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.0259
2022-02-23 02:30:02 - train: epoch 100, train_loss: 0.9950
2022-02-23 02:31:27 - eval: epoch: 100, acc1: 73.930%, acc5: 91.740%, test_loss: 1.0371, per_image_load_time: 2.819ms, per_image_inference_time: 0.289ms
2022-02-23 02:31:28 - until epoch: 100, best_acc1: 73.930%
2022-02-23 02:31:28 - train done. model: resnet34, train time: 54.668 hours, best_acc1: 73.930%
