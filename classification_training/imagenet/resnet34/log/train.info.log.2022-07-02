2022-07-02 10:22:13 - network: resnet34
2022-07-02 10:22:13 - num_classes: 1000
2022-07-02 10:22:13 - input_image_size: 224
2022-07-02 10:22:13 - scale: 1.1428571428571428
2022-07-02 10:22:13 - trained_model_path: 
2022-07-02 10:22:13 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-02 10:22:13 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-02 10:22:13 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f927ec81820>
2022-07-02 10:22:13 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f927ec81af0>
2022-07-02 10:22:13 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f927ec81b20>
2022-07-02 10:22:13 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f927ec81b80>
2022-07-02 10:22:13 - seed: 0
2022-07-02 10:22:13 - batch_size: 256
2022-07-02 10:22:13 - num_workers: 16
2022-07-02 10:22:13 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-02 10:22:13 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-07-02 10:22:13 - epochs: 100
2022-07-02 10:22:13 - print_interval: 100
2022-07-02 10:22:13 - sync_bn: False
2022-07-02 10:22:13 - apex: True
2022-07-02 10:22:13 - use_ema_model: False
2022-07-02 10:22:13 - ema_model_decay: 0.9999
2022-07-02 10:22:13 - gpus_type: NVIDIA RTX A5000
2022-07-02 10:22:13 - gpus_num: 2
2022-07-02 10:22:13 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f9270daf8f0>
2022-07-02 10:22:14 - --------------------parameters--------------------
2022-07-02 10:22:14 - name: conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-02 10:22:14 - name: fc.weight, grad: True
2022-07-02 10:22:14 - name: fc.bias, grad: True
2022-07-02 10:22:14 - --------------------buffers--------------------
2022-07-02 10:22:14 - name: conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-02 10:22:14 - -----------no weight decay layers--------------
2022-07-02 10:22:14 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-02 10:22:14 - -------------weight decay layers---------------
2022-07-02 10:22:14 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-02 10:22:14 - epoch 001 lr: 0.100000
2022-07-02 10:22:52 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9045
2022-07-02 10:23:25 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7702
2022-07-02 10:23:58 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6651
2022-07-02 10:24:31 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.6129
2022-07-02 10:25:04 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4932
2022-07-02 10:25:36 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.1465
2022-07-02 10:26:10 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1019
2022-07-02 10:26:42 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.0947
2022-07-02 10:27:15 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.9825
2022-07-02 10:27:48 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.9225
2022-07-02 10:28:20 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.7748
2022-07-02 10:28:54 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.6418
2022-07-02 10:29:27 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.5304
2022-07-02 10:30:00 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.5338
2022-07-02 10:30:33 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.4605
2022-07-02 10:31:05 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.5388
2022-07-02 10:31:39 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.3190
2022-07-02 10:32:12 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.3536
2022-07-02 10:32:45 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.2116
2022-07-02 10:33:18 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.0269
2022-07-02 10:33:51 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.1532
2022-07-02 10:34:24 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.0637
2022-07-02 10:34:58 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.8939
2022-07-02 10:35:30 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.8355
2022-07-02 10:36:04 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9272
2022-07-02 10:36:36 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.1568
2022-07-02 10:37:10 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.0514
2022-07-02 10:37:44 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.7328
2022-07-02 10:38:18 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.5915
2022-07-02 10:38:50 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.8405
2022-07-02 10:39:23 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.8581
2022-07-02 10:39:57 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.6832
2022-07-02 10:40:29 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.4411
2022-07-02 10:41:02 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.4069
2022-07-02 10:41:36 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.5672
2022-07-02 10:42:08 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.4579
2022-07-02 10:42:41 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.6516
2022-07-02 10:43:15 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.3622
2022-07-02 10:43:48 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.4141
2022-07-02 10:44:21 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.4300
2022-07-02 10:44:55 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.3602
2022-07-02 10:45:28 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.2395
2022-07-02 10:46:02 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.2195
2022-07-02 10:46:35 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.0303
2022-07-02 10:47:09 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.2540
2022-07-02 10:47:42 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.3862
2022-07-02 10:48:15 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.0103
2022-07-02 10:48:49 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.3271
2022-07-02 10:49:22 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.0135
2022-07-02 10:49:54 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.9495
2022-07-02 10:49:55 - train: epoch 001, train_loss: 5.0770
2022-07-02 10:51:09 - eval: epoch: 001, acc1: 20.704%, acc5: 43.538%, test_loss: 3.9565, per_image_load_time: 0.939ms, per_image_inference_time: 0.286ms
2022-07-02 10:51:09 - until epoch: 001, best_acc1: 20.704%
2022-07-02 10:51:09 - epoch 002 lr: 0.100000
2022-07-02 10:51:48 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.0950
2022-07-02 10:52:20 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.7692
2022-07-02 10:52:52 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.0863
2022-07-02 10:53:25 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.9568
2022-07-02 10:53:57 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.6886
2022-07-02 10:54:30 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.7032
2022-07-02 10:55:02 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.9988
2022-07-02 10:55:36 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.6521
2022-07-02 10:56:10 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.4254
2022-07-02 10:56:41 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.8932
2022-07-02 10:57:15 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.9832
2022-07-02 10:57:48 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.8029
2022-07-02 10:58:22 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.6273
2022-07-02 10:58:54 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.9046
2022-07-02 10:59:27 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.7435
2022-07-02 11:00:00 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.6580
2022-07-02 11:00:33 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.6474
2022-07-02 11:01:06 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.6896
2022-07-02 11:01:40 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.5549
2022-07-02 11:02:13 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.3192
2022-07-02 11:02:46 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.5524
2022-07-02 11:03:19 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.3294
2022-07-02 11:03:51 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.4466
2022-07-02 11:04:25 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.2924
2022-07-02 11:04:59 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.4449
2022-07-02 11:05:31 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.4081
2022-07-02 11:06:04 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.6514
2022-07-02 11:06:37 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.4448
2022-07-02 11:07:11 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.3609
2022-07-02 11:07:44 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.2590
2022-07-02 11:08:18 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.3219
2022-07-02 11:08:50 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.3887
2022-07-02 11:09:23 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.2954
2022-07-02 11:09:56 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.4564
2022-07-02 11:10:29 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.2913
2022-07-02 11:11:02 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.3678
2022-07-02 11:11:36 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.4329
2022-07-02 11:12:09 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.1694
2022-07-02 11:12:42 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.3260
2022-07-02 11:13:14 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.2635
2022-07-02 11:13:48 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.3599
2022-07-02 11:14:22 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.2137
2022-07-02 11:14:54 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.2789
2022-07-02 11:15:28 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.1522
2022-07-02 11:16:00 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.0815
2022-07-02 11:16:34 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.0839
2022-07-02 11:17:06 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.2537
2022-07-02 11:17:40 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.1791
2022-07-02 11:18:13 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.0649
2022-07-02 11:18:45 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.0815
2022-07-02 11:18:46 - train: epoch 002, train_loss: 3.5195
2022-07-02 11:19:59 - eval: epoch: 002, acc1: 33.994%, acc5: 60.390%, test_loss: 3.0472, per_image_load_time: 2.477ms, per_image_inference_time: 0.270ms
2022-07-02 11:20:00 - until epoch: 002, best_acc1: 33.994%
2022-07-02 11:20:00 - epoch 003 lr: 0.100000
2022-07-02 11:20:38 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.2676
2022-07-02 11:21:11 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.1676
2022-07-02 11:21:44 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.1047
2022-07-02 11:22:16 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.1983
2022-07-02 11:22:48 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3493
2022-07-02 11:23:22 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.0590
2022-07-02 11:23:55 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.3790
2022-07-02 11:24:29 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.1871
2022-07-02 11:25:01 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 2.9759
2022-07-02 11:25:34 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.0358
2022-07-02 11:26:08 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 2.9541
2022-07-02 11:26:41 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 2.9491
2022-07-02 11:27:14 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.0566
2022-07-02 11:27:47 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0738
2022-07-02 11:28:21 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.2762
2022-07-02 11:28:54 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.0741
2022-07-02 11:29:27 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0154
2022-07-02 11:30:01 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.8922
2022-07-02 11:30:33 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.0927
2022-07-02 11:31:07 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.2826
2022-07-02 11:31:40 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.2888
2022-07-02 11:32:14 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5220
2022-07-02 11:32:46 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 2.9847
2022-07-02 11:33:19 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 2.8726
2022-07-02 11:33:52 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 2.9478
2022-07-02 11:34:26 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1512
2022-07-02 11:34:59 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.3024
2022-07-02 11:35:33 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.9060
2022-07-02 11:36:05 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.0069
2022-07-02 11:36:38 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1091
2022-07-02 11:37:12 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.0552
2022-07-02 11:37:44 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.0948
2022-07-02 11:38:18 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.0268
2022-07-02 11:38:52 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.1113
2022-07-02 11:39:25 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8466
2022-07-02 11:39:59 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.7877
2022-07-02 11:40:33 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.9606
2022-07-02 11:41:05 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 2.9924
2022-07-02 11:41:38 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.2727
2022-07-02 11:42:11 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.0195
2022-07-02 11:42:46 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 2.9801
2022-07-02 11:43:18 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.8797
2022-07-02 11:43:52 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6122
2022-07-02 11:44:26 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.7816
2022-07-02 11:44:59 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9081
2022-07-02 11:45:32 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.8705
2022-07-02 11:46:05 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.7336
2022-07-02 11:46:39 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.0516
2022-07-02 11:47:12 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0125
2022-07-02 11:47:44 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.8931
2022-07-02 11:47:45 - train: epoch 003, train_loss: 2.9978
2022-07-02 11:48:59 - eval: epoch: 003, acc1: 41.280%, acc5: 67.812%, test_loss: 2.6253, per_image_load_time: 1.612ms, per_image_inference_time: 0.263ms
2022-07-02 11:48:59 - until epoch: 003, best_acc1: 41.280%
2022-07-02 11:48:59 - epoch 004 lr: 0.100000
2022-07-02 11:49:37 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.8351
2022-07-02 11:50:10 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.7191
2022-07-02 11:50:43 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.7860
2022-07-02 11:51:16 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.6917
2022-07-02 11:51:49 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.6850
2022-07-02 11:52:22 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 2.9868
2022-07-02 11:52:54 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.9604
2022-07-02 11:53:27 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.6507
2022-07-02 11:54:00 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.5591
2022-07-02 11:54:33 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.8015
2022-07-02 11:55:07 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0097
2022-07-02 11:55:39 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.5771
2022-07-02 11:56:13 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.5330
2022-07-02 11:56:45 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.8535
2022-07-02 11:57:19 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.9168
2022-07-02 11:57:52 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.6707
2022-07-02 11:58:25 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.8513
2022-07-02 11:58:58 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9349
2022-07-02 11:59:32 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.9741
2022-07-02 12:00:05 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.6737
2022-07-02 12:00:38 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.7291
2022-07-02 12:01:12 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.8003
2022-07-02 12:01:44 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.4660
2022-07-02 12:02:18 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6875
2022-07-02 12:02:50 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.6870
2022-07-02 12:03:25 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.7821
2022-07-02 12:03:57 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6401
2022-07-02 12:04:31 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.7788
2022-07-02 12:05:03 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.5727
2022-07-02 12:05:38 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.6519
2022-07-02 12:06:11 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8663
2022-07-02 12:06:44 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.6738
2022-07-02 12:07:17 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.7882
2022-07-02 12:07:51 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.7479
2022-07-02 12:08:24 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7086
2022-07-02 12:08:58 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.5863
2022-07-02 12:09:31 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.6920
2022-07-02 12:10:05 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.6977
2022-07-02 12:10:37 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7346
2022-07-02 12:11:11 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.4130
2022-07-02 12:11:44 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.6976
2022-07-02 12:12:17 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.6228
2022-07-02 12:12:51 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.5152
2022-07-02 12:13:24 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5692
2022-07-02 12:13:57 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.1585
2022-07-02 12:14:31 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.6921
2022-07-02 12:15:04 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.5433
2022-07-02 12:15:38 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.5708
2022-07-02 12:16:10 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.6834
2022-07-02 12:16:42 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.7321
2022-07-02 12:16:43 - train: epoch 004, train_loss: 2.7502
2022-07-02 12:17:57 - eval: epoch: 004, acc1: 44.608%, acc5: 71.312%, test_loss: 2.4450, per_image_load_time: 2.584ms, per_image_inference_time: 0.278ms
2022-07-02 12:17:57 - until epoch: 004, best_acc1: 44.608%
2022-07-02 12:17:57 - epoch 005 lr: 0.100000
2022-07-02 12:18:35 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.7138
2022-07-02 12:19:08 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7117
2022-07-02 12:19:41 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.7711
2022-07-02 12:20:14 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.6413
2022-07-02 12:20:47 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.5201
2022-07-02 12:21:19 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.6582
2022-07-02 12:21:52 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.7021
2022-07-02 12:22:24 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.8266
2022-07-02 12:22:58 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5540
2022-07-02 12:23:30 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.7333
2022-07-02 12:24:03 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.7226
2022-07-02 12:24:36 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.7606
2022-07-02 12:25:10 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.5964
2022-07-02 12:25:42 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7251
2022-07-02 12:26:16 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.4392
2022-07-02 12:26:49 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.4554
2022-07-02 12:27:22 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.4903
2022-07-02 12:27:55 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6444
2022-07-02 12:28:29 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.5162
2022-07-02 12:29:02 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.6414
2022-07-02 12:29:35 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.4063
2022-07-02 12:30:08 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.3675
2022-07-02 12:30:41 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.2618
2022-07-02 12:31:15 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.5989
2022-07-02 12:31:48 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7897
2022-07-02 12:32:21 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.8216
2022-07-02 12:32:56 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.6680
2022-07-02 12:33:29 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.5954
2022-07-02 12:34:02 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.4757
2022-07-02 12:34:35 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6094
2022-07-02 12:35:09 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.6725
2022-07-02 12:35:41 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.6990
2022-07-02 12:36:15 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4126
2022-07-02 12:36:49 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.5182
2022-07-02 12:37:22 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.4830
2022-07-02 12:37:55 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.5329
2022-07-02 12:38:29 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.4180
2022-07-02 12:39:02 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3609
2022-07-02 12:39:35 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9149
2022-07-02 12:40:09 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.6358
2022-07-02 12:40:42 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5776
2022-07-02 12:41:16 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6779
2022-07-02 12:41:50 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.5146
2022-07-02 12:42:23 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.5463
2022-07-02 12:42:57 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.4886
2022-07-02 12:43:29 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.4959
2022-07-02 12:44:02 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.3354
2022-07-02 12:44:37 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4389
2022-07-02 12:45:09 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.7315
2022-07-02 12:45:41 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.4233
2022-07-02 12:45:42 - train: epoch 005, train_loss: 2.6073
2022-07-02 12:46:55 - eval: epoch: 005, acc1: 46.190%, acc5: 72.936%, test_loss: 2.3424, per_image_load_time: 1.609ms, per_image_inference_time: 0.267ms
2022-07-02 12:46:55 - until epoch: 005, best_acc1: 46.190%
2022-07-02 12:46:55 - epoch 006 lr: 0.100000
2022-07-02 12:47:33 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.4866
2022-07-02 12:48:07 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5715
2022-07-02 12:48:38 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.4007
2022-07-02 12:49:12 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.6290
2022-07-02 12:49:44 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.5556
2022-07-02 12:50:18 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.5492
2022-07-02 12:50:51 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.5566
2022-07-02 12:51:24 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.5811
2022-07-02 12:51:57 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.4008
2022-07-02 12:52:29 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4527
2022-07-02 12:53:02 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4416
2022-07-02 12:53:36 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.6234
2022-07-02 12:54:09 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.6302
2022-07-02 12:54:42 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.5656
2022-07-02 12:55:15 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.6704
2022-07-02 12:55:48 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.3652
2022-07-02 12:56:21 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.6209
2022-07-02 12:56:55 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.6465
2022-07-02 12:57:27 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.4409
2022-07-02 12:58:01 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.6934
2022-07-02 12:58:34 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.5813
2022-07-02 12:59:08 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.4613
2022-07-02 12:59:41 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.4144
2022-07-02 13:00:14 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.5341
2022-07-02 13:00:47 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.7163
2022-07-02 13:01:21 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.4000
2022-07-02 13:01:54 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.6231
2022-07-02 13:02:27 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3854
2022-07-02 13:03:00 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.6516
2022-07-02 13:03:34 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.4774
2022-07-02 13:04:07 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.2622
2022-07-02 13:04:40 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.5485
2022-07-02 13:05:12 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.3740
2022-07-02 13:05:47 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.7492
2022-07-02 13:06:19 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.6187
2022-07-02 13:06:54 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.4950
2022-07-02 13:07:26 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.5345
2022-07-02 13:08:00 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.3931
2022-07-02 13:08:32 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.4387
2022-07-02 13:09:05 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.7138
2022-07-02 13:09:39 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5318
2022-07-02 13:10:12 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.2920
2022-07-02 13:10:45 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.5276
2022-07-02 13:11:19 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.4664
2022-07-02 13:11:51 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.5329
2022-07-02 13:12:24 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.4586
2022-07-02 13:12:58 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.4399
2022-07-02 13:13:31 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.4812
2022-07-02 13:14:04 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.5325
2022-07-02 13:14:36 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.3850
2022-07-02 13:14:37 - train: epoch 006, train_loss: 2.5212
2022-07-02 13:15:50 - eval: epoch: 006, acc1: 45.934%, acc5: 72.244%, test_loss: 2.3782, per_image_load_time: 1.125ms, per_image_inference_time: 0.268ms
2022-07-02 13:15:51 - until epoch: 006, best_acc1: 46.190%
2022-07-02 13:15:51 - epoch 007 lr: 0.100000
2022-07-02 13:16:29 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.4049
2022-07-02 13:17:01 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.6337
2022-07-02 13:17:35 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6967
2022-07-02 13:18:08 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.5736
2022-07-02 13:18:41 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.4568
2022-07-02 13:19:14 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.6324
2022-07-02 13:19:47 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.4834
2022-07-02 13:20:20 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.5773
2022-07-02 13:20:53 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.5311
2022-07-02 13:21:26 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.5532
2022-07-02 13:21:59 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.3330
2022-07-02 13:22:32 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.3739
2022-07-02 13:23:06 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4902
2022-07-02 13:23:38 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.4874
2022-07-02 13:24:11 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.5596
2022-07-02 13:24:44 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.5360
2022-07-02 13:25:18 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.5695
2022-07-02 13:25:50 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.3555
2022-07-02 13:26:25 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.5022
2022-07-02 13:26:57 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.3216
2022-07-02 13:27:31 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.5496
2022-07-02 13:28:04 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.3724
2022-07-02 13:28:38 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.4453
2022-07-02 13:29:10 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.4492
2022-07-02 13:29:44 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.3695
2022-07-02 13:30:17 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.4492
2022-07-02 13:30:50 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.4277
2022-07-02 13:31:23 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.3633
2022-07-02 13:31:57 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.4059
2022-07-02 13:32:30 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.5219
2022-07-02 13:33:03 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.2931
2022-07-02 13:33:36 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.3978
2022-07-02 13:34:09 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.7293
2022-07-02 13:34:43 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.3053
2022-07-02 13:35:16 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.3944
2022-07-02 13:35:49 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.1732
2022-07-02 13:36:21 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.4977
2022-07-02 13:36:55 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.7513
2022-07-02 13:37:29 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.3382
2022-07-02 13:38:02 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.5233
2022-07-02 13:38:35 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.3127
2022-07-02 13:39:09 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.3010
2022-07-02 13:39:42 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.6859
2022-07-02 13:40:16 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.2301
2022-07-02 13:40:49 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.6205
2022-07-02 13:41:21 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.6055
2022-07-02 13:41:55 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.6722
2022-07-02 13:42:28 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.6887
2022-07-02 13:43:02 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.3854
2022-07-02 13:43:33 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.3860
2022-07-02 13:43:34 - train: epoch 007, train_loss: 2.4569
2022-07-02 13:44:48 - eval: epoch: 007, acc1: 49.688%, acc5: 75.756%, test_loss: 2.1643, per_image_load_time: 0.642ms, per_image_inference_time: 0.258ms
2022-07-02 13:44:49 - until epoch: 007, best_acc1: 49.688%
2022-07-02 13:44:49 - epoch 008 lr: 0.100000
2022-07-02 13:45:27 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.4049
2022-07-02 13:46:00 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.5528
2022-07-02 13:46:34 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.1543
2022-07-02 13:47:06 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.1759
2022-07-02 13:47:39 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.3596
2022-07-02 13:48:12 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.4143
2022-07-02 13:48:45 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.6226
2022-07-02 13:49:18 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.2009
2022-07-02 13:49:51 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.4455
2022-07-02 13:50:24 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.4089
2022-07-02 13:50:57 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.1958
2022-07-02 13:51:30 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.2681
2022-07-02 13:52:03 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.4794
2022-07-02 13:52:36 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.3475
2022-07-02 13:53:09 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.5439
2022-07-02 13:53:43 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.5315
2022-07-02 13:54:17 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.4264
2022-07-02 13:54:49 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.5442
2022-07-02 13:55:23 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.2674
2022-07-02 13:55:56 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.4733
2022-07-02 13:56:30 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.3881
2022-07-02 13:57:03 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.2995
2022-07-02 13:57:36 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.5737
2022-07-02 13:58:09 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.3881
2022-07-02 13:58:43 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.4399
2022-07-02 13:59:15 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.4852
2022-07-02 13:59:49 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.4098
2022-07-02 14:00:23 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.4952
2022-07-02 14:00:55 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.3344
2022-07-02 14:01:29 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.4621
2022-07-02 14:02:02 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.3312
2022-07-02 14:02:36 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.6820
2022-07-02 14:03:09 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.6569
2022-07-02 14:03:42 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.5341
2022-07-02 14:04:15 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.2937
2022-07-02 14:04:48 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.5994
2022-07-02 14:05:22 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.3582
2022-07-02 14:05:56 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.2162
2022-07-02 14:06:28 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.5519
2022-07-02 14:07:02 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.6575
2022-07-02 14:07:35 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.3660
2022-07-02 14:08:08 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.3710
2022-07-02 14:08:43 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.2721
2022-07-02 14:09:16 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.1996
2022-07-02 14:09:49 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.5658
2022-07-02 14:10:23 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.6035
2022-07-02 14:10:57 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2975
2022-07-02 14:11:29 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.4479
2022-07-02 14:12:02 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.5166
2022-07-02 14:12:33 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.4867
2022-07-02 14:12:34 - train: epoch 008, train_loss: 2.4114
2022-07-02 14:13:48 - eval: epoch: 008, acc1: 49.748%, acc5: 75.416%, test_loss: 2.1800, per_image_load_time: 0.924ms, per_image_inference_time: 0.266ms
2022-07-02 14:13:48 - until epoch: 008, best_acc1: 49.748%
2022-07-02 14:13:48 - epoch 009 lr: 0.100000
2022-07-02 14:14:26 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.1991
2022-07-02 14:14:59 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.1525
2022-07-02 14:15:32 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.0406
2022-07-02 14:16:06 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.5377
2022-07-02 14:16:38 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.3232
2022-07-02 14:17:11 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.2864
2022-07-02 14:17:45 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.3279
2022-07-02 14:18:18 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.2046
2022-07-02 14:18:51 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.2280
2022-07-02 14:19:24 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.2324
2022-07-02 14:19:58 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.6527
2022-07-02 14:20:30 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.3953
2022-07-02 14:21:03 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.6372
2022-07-02 14:21:37 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.0735
2022-07-02 14:22:10 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.2703
2022-07-02 14:22:44 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.4256
2022-07-02 14:23:16 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.5777
2022-07-02 14:23:50 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.4019
2022-07-02 14:24:22 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.1292
2022-07-02 14:24:56 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.0811
2022-07-02 14:25:29 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.4485
2022-07-02 14:26:03 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.5335
2022-07-02 14:26:36 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.1678
2022-07-02 14:27:09 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.3618
2022-07-02 14:27:42 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.2103
2022-07-02 14:28:16 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.3480
2022-07-02 14:28:48 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.2538
2022-07-02 14:29:22 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.3939
2022-07-02 14:29:55 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.0784
2022-07-02 14:30:28 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.2820
2022-07-02 14:31:02 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.4670
2022-07-02 14:31:35 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.3185
2022-07-02 14:32:09 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.3364
2022-07-02 14:32:42 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.5235
2022-07-02 14:33:15 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.5254
2022-07-02 14:33:49 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.2789
2022-07-02 14:34:22 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.5337
2022-07-02 14:34:54 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.4673
2022-07-02 14:35:28 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.0668
2022-07-02 14:36:01 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.4932
2022-07-02 14:36:35 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.3180
2022-07-02 14:37:08 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.3123
2022-07-02 14:37:41 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.4675
2022-07-02 14:38:15 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.3538
2022-07-02 14:38:48 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.4055
2022-07-02 14:39:21 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.4325
2022-07-02 14:39:55 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.5979
2022-07-02 14:40:28 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.4971
2022-07-02 14:41:01 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.4526
2022-07-02 14:41:33 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.3658
2022-07-02 14:41:34 - train: epoch 009, train_loss: 2.3764
2022-07-02 14:42:49 - eval: epoch: 009, acc1: 50.022%, acc5: 75.612%, test_loss: 2.1768, per_image_load_time: 0.949ms, per_image_inference_time: 0.302ms
2022-07-02 14:42:49 - until epoch: 009, best_acc1: 50.022%
2022-07-02 14:42:49 - epoch 010 lr: 0.100000
2022-07-02 14:43:27 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.3251
2022-07-02 14:44:00 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.5453
2022-07-02 14:44:33 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.3130
2022-07-02 14:45:07 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.4014
2022-07-02 14:45:39 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.2402
2022-07-02 14:46:13 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.4340
2022-07-02 14:46:46 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.3909
2022-07-02 14:47:19 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.2451
2022-07-02 14:47:51 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.2993
2022-07-02 14:48:24 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.1110
2022-07-02 14:48:58 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.3130
2022-07-02 14:49:31 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.1619
2022-07-02 14:50:05 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.1583
2022-07-02 14:50:37 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.4178
2022-07-02 14:51:11 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.0562
2022-07-02 14:51:44 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.4019
2022-07-02 14:52:18 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.3994
2022-07-02 14:52:50 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.4405
2022-07-02 14:53:23 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.4792
2022-07-02 14:53:56 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.3716
2022-07-02 14:54:29 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.1591
2022-07-02 14:55:03 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.5092
2022-07-02 14:55:36 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.5735
2022-07-02 14:56:09 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.5702
2022-07-02 14:56:43 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.3216
2022-07-02 14:57:15 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.4498
2022-07-02 14:57:49 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.1401
2022-07-02 14:58:21 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.3438
2022-07-02 14:58:55 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.5195
2022-07-02 14:59:28 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.3394
2022-07-02 15:00:02 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.6044
2022-07-02 15:00:35 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.3292
2022-07-02 15:01:08 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.5175
2022-07-02 15:01:42 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.5579
2022-07-02 15:02:15 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.6006
2022-07-02 15:02:48 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.5812
2022-07-02 15:03:21 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.1955
2022-07-02 15:03:54 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.4633
2022-07-02 15:04:27 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.0112
2022-07-02 15:05:01 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.2491
2022-07-02 15:05:33 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.2388
2022-07-02 15:06:07 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.4188
2022-07-02 15:06:40 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.4395
2022-07-02 15:07:13 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.3024
2022-07-02 15:07:47 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.1815
2022-07-02 15:08:20 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.3446
2022-07-02 15:08:54 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.2696
2022-07-02 15:09:26 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.2402
2022-07-02 15:10:00 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.3239
2022-07-02 15:10:31 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.1506
2022-07-02 15:10:33 - train: epoch 010, train_loss: 2.3467
2022-07-02 15:11:47 - eval: epoch: 010, acc1: 51.582%, acc5: 77.470%, test_loss: 2.0649, per_image_load_time: 0.717ms, per_image_inference_time: 0.285ms
2022-07-02 15:11:47 - until epoch: 010, best_acc1: 51.582%
2022-07-02 15:11:47 - epoch 011 lr: 0.100000
2022-07-02 15:12:25 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.0920
2022-07-02 15:12:58 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.3587
2022-07-02 15:13:31 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.1510
2022-07-02 15:14:04 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.4484
2022-07-02 15:14:38 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.2665
2022-07-02 15:15:11 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.3024
2022-07-02 15:15:44 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.3235
2022-07-02 15:16:16 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.3543
2022-07-02 15:16:50 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.3979
2022-07-02 15:17:23 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.3027
2022-07-02 15:17:56 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.3721
2022-07-02 15:18:29 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.6541
2022-07-02 15:19:02 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.5011
2022-07-02 15:19:35 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.4197
2022-07-02 15:20:09 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.2978
2022-07-02 15:20:42 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.3591
2022-07-02 15:21:15 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.3020
2022-07-02 15:21:49 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.1935
2022-07-02 15:22:22 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.0799
2022-07-02 15:22:55 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.4917
2022-07-02 15:23:27 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.2633
2022-07-02 15:24:01 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.1883
2022-07-02 15:24:34 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.5329
2022-07-02 15:25:07 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.0980
2022-07-02 15:25:40 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.5245
2022-07-02 15:26:13 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.2934
2022-07-02 15:26:46 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.3189
2022-07-02 15:27:20 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.9585
2022-07-02 15:27:53 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.4043
2022-07-02 15:28:26 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.6783
2022-07-02 15:29:00 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.4618
2022-07-02 15:29:32 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.0489
2022-07-02 15:30:06 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.4119
2022-07-02 15:30:39 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.2667
2022-07-02 15:31:12 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.3558
2022-07-02 15:31:45 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.2806
2022-07-02 15:32:19 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.5038
2022-07-02 15:32:52 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.1496
2022-07-02 15:33:26 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.2979
2022-07-02 15:34:00 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.3247
2022-07-02 15:34:32 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.0401
2022-07-02 15:35:06 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.3087
2022-07-02 15:35:39 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4719
2022-07-02 15:36:12 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.2562
2022-07-02 15:36:46 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.2106
2022-07-02 15:37:18 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.2232
2022-07-02 15:37:52 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.0755
2022-07-02 15:38:26 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.2567
2022-07-02 15:38:59 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.1986
2022-07-02 15:39:32 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.2320
2022-07-02 15:39:33 - train: epoch 011, train_loss: 2.3224
2022-07-02 15:40:46 - eval: epoch: 011, acc1: 52.384%, acc5: 78.040%, test_loss: 2.0320, per_image_load_time: 0.752ms, per_image_inference_time: 0.271ms
2022-07-02 15:40:47 - until epoch: 011, best_acc1: 52.384%
2022-07-02 15:40:47 - epoch 012 lr: 0.100000
2022-07-02 15:41:26 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.1563
2022-07-02 15:41:58 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.2531
2022-07-02 15:42:31 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.2805
2022-07-02 15:43:04 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.3908
2022-07-02 15:43:38 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.5353
2022-07-02 15:44:10 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9865
2022-07-02 15:44:43 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.1462
2022-07-02 15:45:16 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.3711
2022-07-02 15:45:50 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.4394
2022-07-02 15:46:23 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.1050
2022-07-02 15:46:55 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.6009
2022-07-02 15:47:29 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.1103
2022-07-02 15:48:03 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.2851
2022-07-02 15:48:36 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.3716
2022-07-02 15:49:08 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.1565
2022-07-02 15:49:42 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.1535
2022-07-02 15:50:16 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.1732
2022-07-02 15:50:49 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.3243
2022-07-02 15:51:22 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.3131
2022-07-02 15:51:55 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.4715
2022-07-02 15:52:29 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.2765
2022-07-02 15:53:01 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.5119
2022-07-02 15:53:35 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.2961
2022-07-02 15:54:08 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.3211
2022-07-02 15:54:41 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.1195
2022-07-02 15:55:14 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.1180
2022-07-02 15:55:47 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.1563
2022-07-02 15:56:21 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.3687
2022-07-02 15:56:54 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.1576
2022-07-02 15:57:27 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.1922
2022-07-02 15:58:01 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.4485
2022-07-02 15:58:34 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.0775
2022-07-02 15:59:07 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3982
2022-07-02 15:59:40 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.2388
2022-07-02 16:00:13 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.4299
2022-07-02 16:00:47 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.3129
2022-07-02 16:01:21 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.2048
2022-07-02 16:01:53 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.3889
2022-07-02 16:02:26 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.1821
2022-07-02 16:02:59 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.3480
2022-07-02 16:03:33 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.1337
2022-07-02 16:04:07 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.1503
2022-07-02 16:04:41 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.3279
2022-07-02 16:05:13 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.1858
2022-07-02 16:05:47 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.1622
2022-07-02 16:06:20 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.5664
2022-07-02 16:06:53 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.2694
2022-07-02 16:07:27 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.3648
2022-07-02 16:08:01 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.3453
2022-07-02 16:08:32 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.0034
2022-07-02 16:08:33 - train: epoch 012, train_loss: 2.3032
2022-07-02 16:09:47 - eval: epoch: 012, acc1: 53.352%, acc5: 78.640%, test_loss: 1.9910, per_image_load_time: 0.648ms, per_image_inference_time: 0.249ms
2022-07-02 16:09:47 - until epoch: 012, best_acc1: 53.352%
2022-07-02 16:09:47 - epoch 013 lr: 0.100000
2022-07-02 16:10:25 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.9950
2022-07-02 16:10:59 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.1274
2022-07-02 16:11:32 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.2238
2022-07-02 16:12:04 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2688
2022-07-02 16:12:37 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.2872
2022-07-02 16:13:10 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.4235
2022-07-02 16:13:44 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.1783
2022-07-02 16:14:17 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.4396
2022-07-02 16:14:49 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.2317
2022-07-02 16:15:23 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.4074
2022-07-02 16:15:56 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.3404
2022-07-02 16:16:29 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.4210
2022-07-02 16:17:02 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.4530
2022-07-02 16:17:36 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.1575
2022-07-02 16:18:08 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.3340
2022-07-02 16:18:42 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.0818
2022-07-02 16:19:15 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.3173
2022-07-02 16:19:49 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.2922
2022-07-02 16:20:22 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.2974
2022-07-02 16:20:56 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.4762
2022-07-02 16:21:29 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.5903
2022-07-02 16:22:03 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.2668
2022-07-02 16:22:36 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.3488
2022-07-02 16:23:08 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.2309
2022-07-02 16:23:42 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.1723
2022-07-02 16:24:15 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.2476
2022-07-02 16:24:49 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.2019
2022-07-02 16:25:22 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.2828
2022-07-02 16:25:54 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3144
2022-07-02 16:26:28 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.1803
2022-07-02 16:27:01 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.0945
2022-07-02 16:27:34 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.3160
2022-07-02 16:28:08 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.2321
2022-07-02 16:28:42 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.1637
2022-07-02 16:29:14 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.0604
2022-07-02 16:29:48 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6010
2022-07-02 16:30:20 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.0613
2022-07-02 16:30:54 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.3683
2022-07-02 16:31:27 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.3798
2022-07-02 16:32:00 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.4147
2022-07-02 16:32:34 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.2363
2022-07-02 16:33:08 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.2787
2022-07-02 16:33:41 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.2358
2022-07-02 16:34:14 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.2151
2022-07-02 16:34:47 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.2813
2022-07-02 16:35:20 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.3173
2022-07-02 16:35:54 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.4315
2022-07-02 16:36:26 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.4387
2022-07-02 16:37:00 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.3611
2022-07-02 16:37:32 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.4010
2022-07-02 16:37:33 - train: epoch 013, train_loss: 2.2876
2022-07-02 16:38:47 - eval: epoch: 013, acc1: 51.822%, acc5: 77.296%, test_loss: 2.0756, per_image_load_time: 0.657ms, per_image_inference_time: 0.255ms
2022-07-02 16:38:47 - until epoch: 013, best_acc1: 53.352%
2022-07-02 16:38:47 - epoch 014 lr: 0.100000
2022-07-02 16:39:25 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2217
2022-07-02 16:39:58 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.4020
2022-07-02 16:40:31 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.9946
2022-07-02 16:41:04 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.2397
2022-07-02 16:41:37 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.1516
2022-07-02 16:42:10 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.3196
2022-07-02 16:42:43 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.2989
2022-07-02 16:43:16 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.2681
2022-07-02 16:43:50 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.2140
2022-07-02 16:44:23 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.4402
2022-07-02 16:44:56 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.1772
2022-07-02 16:45:29 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.2061
2022-07-02 16:46:02 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.3691
2022-07-02 16:46:35 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.3454
2022-07-02 16:47:09 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.4126
2022-07-02 16:47:42 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.2057
2022-07-02 16:48:15 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.4231
2022-07-02 16:48:49 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.4465
2022-07-02 16:49:22 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.1688
2022-07-02 16:49:55 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.2408
2022-07-02 16:50:28 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.3472
2022-07-02 16:51:01 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.3501
2022-07-02 16:51:34 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.2788
2022-07-02 16:52:07 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.3680
2022-07-02 16:52:40 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.1145
2022-07-02 16:53:14 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.2084
2022-07-02 16:53:47 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.2603
2022-07-02 16:54:20 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.5981
2022-07-02 16:54:54 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.1981
2022-07-02 16:55:27 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.3500
2022-07-02 16:55:59 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.0594
2022-07-02 16:56:33 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2051
2022-07-02 16:57:06 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.1096
2022-07-02 16:57:40 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.2399
2022-07-02 16:58:13 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.2812
2022-07-02 16:58:45 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.1337
2022-07-02 16:59:19 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.2224
2022-07-02 16:59:53 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.4796
2022-07-02 17:00:26 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.2375
2022-07-02 17:00:59 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.4936
2022-07-02 17:01:33 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.1589
2022-07-02 17:02:06 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.0593
2022-07-02 17:02:40 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.1010
2022-07-02 17:03:12 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.1123
2022-07-02 17:03:45 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.1730
2022-07-02 17:04:18 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.1654
2022-07-02 17:04:52 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.1119
2022-07-02 17:05:25 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.1403
2022-07-02 17:05:59 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1339
2022-07-02 17:06:30 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.2825
2022-07-02 17:06:31 - train: epoch 014, train_loss: 2.2719
2022-07-02 17:07:44 - eval: epoch: 014, acc1: 51.990%, acc5: 77.914%, test_loss: 2.0549, per_image_load_time: 1.132ms, per_image_inference_time: 0.266ms
2022-07-02 17:07:45 - until epoch: 014, best_acc1: 53.352%
2022-07-02 17:07:45 - epoch 015 lr: 0.100000
2022-07-02 17:08:23 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.9570
2022-07-02 17:08:57 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.3864
2022-07-02 17:09:30 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.2900
2022-07-02 17:10:02 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.3066
2022-07-02 17:10:35 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.2035
2022-07-02 17:11:08 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.4199
2022-07-02 17:11:41 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.2926
2022-07-02 17:12:13 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.1129
2022-07-02 17:12:47 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.1551
2022-07-02 17:13:20 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.2906
2022-07-02 17:13:52 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.0592
2022-07-02 17:14:26 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.1602
2022-07-02 17:14:59 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.6310
2022-07-02 17:15:32 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.1220
2022-07-02 17:16:05 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.0315
2022-07-02 17:16:39 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.2941
2022-07-02 17:17:10 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.4079
2022-07-02 17:17:44 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2165
2022-07-02 17:18:17 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.0881
2022-07-02 17:18:50 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.2639
2022-07-02 17:19:23 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.0010
2022-07-02 17:19:57 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.4493
2022-07-02 17:20:30 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.0271
2022-07-02 17:21:03 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4072
2022-07-02 17:21:37 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.2835
2022-07-02 17:22:11 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.0007
2022-07-02 17:22:44 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.3615
2022-07-02 17:23:16 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.4420
2022-07-02 17:23:50 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.1142
2022-07-02 17:24:24 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.0812
2022-07-02 17:24:56 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.1590
2022-07-02 17:25:29 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.1773
2022-07-02 17:26:03 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.9778
2022-07-02 17:26:37 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.3451
2022-07-02 17:27:10 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.4208
2022-07-02 17:27:44 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.2757
2022-07-02 17:28:17 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.0674
2022-07-02 17:28:51 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.2531
2022-07-02 17:29:24 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.4819
2022-07-02 17:29:57 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4495
2022-07-02 17:30:31 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.3830
2022-07-02 17:31:04 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.9927
2022-07-02 17:31:37 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.3162
2022-07-02 17:32:10 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.2955
2022-07-02 17:32:44 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.3217
2022-07-02 17:33:17 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.2034
2022-07-02 17:33:50 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.4290
2022-07-02 17:34:22 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.3032
2022-07-02 17:34:56 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.2121
2022-07-02 17:35:28 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.4887
2022-07-02 17:35:29 - train: epoch 015, train_loss: 2.2609
2022-07-02 17:36:43 - eval: epoch: 015, acc1: 49.422%, acc5: 75.080%, test_loss: 2.2144, per_image_load_time: 0.808ms, per_image_inference_time: 0.276ms
2022-07-02 17:36:44 - until epoch: 015, best_acc1: 53.352%
2022-07-02 17:36:44 - epoch 016 lr: 0.100000
2022-07-02 17:37:22 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.3183
2022-07-02 17:37:55 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.0471
2022-07-02 17:38:28 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.1223
2022-07-02 17:39:01 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.4646
2022-07-02 17:39:33 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.0312
2022-07-02 17:40:07 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.4037
2022-07-02 17:40:40 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.0398
2022-07-02 17:41:13 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.2755
2022-07-02 17:41:46 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.2788
2022-07-02 17:42:19 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.0519
2022-07-02 17:42:53 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.1800
2022-07-02 17:43:26 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2080
2022-07-02 17:43:59 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.4844
2022-07-02 17:44:32 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.2299
2022-07-02 17:45:05 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.3334
2022-07-02 17:45:38 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.2699
2022-07-02 17:46:12 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.1675
2022-07-02 17:46:45 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.2504
2022-07-02 17:47:17 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.3491
2022-07-02 17:47:52 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.9593
2022-07-02 17:48:24 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.3686
2022-07-02 17:48:57 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.3217
2022-07-02 17:49:31 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.4811
2022-07-02 17:50:05 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.3936
2022-07-02 17:50:37 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.1314
2022-07-02 17:51:10 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.4327
2022-07-02 17:51:43 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.2165
2022-07-02 17:52:17 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.1096
2022-07-02 17:52:50 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.3988
2022-07-02 17:53:24 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.4800
2022-07-02 17:53:57 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.4429
2022-07-02 17:54:30 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.4344
2022-07-02 17:55:04 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.3192
2022-07-02 17:55:37 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.2004
2022-07-02 17:56:09 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.1971
2022-07-02 17:56:44 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.0945
2022-07-02 17:57:17 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.3902
2022-07-02 17:57:51 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.6332
2022-07-02 17:58:24 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.1946
2022-07-02 17:58:58 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.2799
2022-07-02 17:59:30 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.1786
2022-07-02 18:00:03 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.2956
2022-07-02 18:00:37 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.1108
2022-07-02 18:01:10 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.1298
2022-07-02 18:01:42 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.4590
2022-07-02 18:02:16 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.2641
2022-07-02 18:02:50 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.5188
2022-07-02 18:03:23 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.2105
2022-07-02 18:03:56 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.1634
2022-07-02 18:04:27 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.2921
2022-07-02 18:04:28 - train: epoch 016, train_loss: 2.2503
2022-07-02 18:05:42 - eval: epoch: 016, acc1: 52.632%, acc5: 78.106%, test_loss: 2.0256, per_image_load_time: 1.468ms, per_image_inference_time: 0.256ms
2022-07-02 18:05:42 - until epoch: 016, best_acc1: 53.352%
2022-07-02 22:43:50 - epoch 017 lr: 0.100000
2022-07-02 22:44:28 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.1776
2022-07-02 22:45:01 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.4238
2022-07-02 22:45:36 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.5106
2022-07-02 22:46:07 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.8736
2022-07-02 22:46:41 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.2172
2022-07-02 22:47:13 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.4990
2022-07-02 22:47:46 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0856
2022-07-02 22:48:19 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.5214
2022-07-02 22:48:52 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.2245
2022-07-02 22:49:25 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.3091
2022-07-02 22:49:58 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.4126
2022-07-02 22:50:31 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 1.9986
2022-07-02 22:51:04 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.2247
2022-07-02 22:51:38 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.3151
2022-07-02 22:52:10 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.0610
2022-07-02 22:52:43 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.0261
2022-07-02 22:53:17 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.2013
2022-07-02 22:53:49 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.3290
2022-07-02 22:54:23 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.0973
2022-07-02 22:54:55 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.1847
2022-07-02 22:55:28 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.2483
2022-07-02 22:56:02 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.0790
2022-07-02 22:56:34 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.3426
2022-07-02 22:57:08 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.3255
2022-07-02 22:57:41 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.4644
2022-07-02 22:58:14 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.3069
2022-07-02 22:58:47 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.1414
2022-07-02 22:59:20 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.2610
2022-07-02 22:59:53 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.4340
2022-07-02 23:00:26 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.0602
2022-07-02 23:01:00 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.3832
2022-07-02 23:01:32 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.0084
2022-07-02 23:02:06 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.3443
2022-07-02 23:02:38 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.1702
2022-07-02 23:03:12 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.2852
2022-07-02 23:03:45 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.3816
2022-07-02 23:04:19 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.1229
2022-07-02 23:04:52 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.3366
2022-07-02 23:05:25 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.1120
2022-07-02 23:05:58 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.2049
2022-07-02 23:06:31 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.3280
2022-07-02 23:07:04 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.0293
2022-07-02 23:07:37 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.1929
2022-07-02 23:08:10 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.2244
2022-07-02 23:08:43 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.2265
2022-07-02 23:09:16 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.2910
2022-07-02 23:09:50 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.4170
2022-07-02 23:10:23 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.1984
2022-07-02 23:10:56 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.2952
2022-07-02 23:11:27 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.9509
2022-07-02 23:11:28 - train: epoch 017, train_loss: 2.2403
2022-07-02 23:12:42 - eval: epoch: 017, acc1: 54.940%, acc5: 79.604%, test_loss: 1.9166, per_image_load_time: 2.554ms, per_image_inference_time: 0.281ms
2022-07-02 23:12:42 - until epoch: 017, best_acc1: 54.940%
2022-07-02 23:12:42 - epoch 018 lr: 0.100000
2022-07-02 23:13:20 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.0912
2022-07-02 23:13:53 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.4825
2022-07-02 23:14:26 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.4744
2022-07-02 23:14:59 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.4894
2022-07-02 23:15:32 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.2667
2022-07-02 23:16:05 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.2522
2022-07-02 23:16:38 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.0908
2022-07-02 23:17:11 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.1800
2022-07-02 23:17:44 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4097
2022-07-02 23:18:18 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.0242
2022-07-02 23:18:50 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.3887
2022-07-02 23:19:24 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.2798
2022-07-02 23:19:58 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.3380
2022-07-02 23:20:31 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.3554
2022-07-02 23:21:04 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.4233
2022-07-02 23:21:37 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.1997
2022-07-02 23:22:11 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.1388
2022-07-02 23:22:44 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.2749
2022-07-02 23:23:18 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.3849
2022-07-02 23:23:50 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.4618
2022-07-02 23:24:25 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.3345
2022-07-02 23:24:57 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.4190
2022-07-02 23:25:30 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.2396
2022-07-02 23:26:04 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.9777
2022-07-02 23:26:37 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.9722
2022-07-02 23:27:09 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.1840
2022-07-02 23:27:43 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.3260
2022-07-02 23:28:17 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.2048
2022-07-02 23:28:50 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.0655
2022-07-02 23:29:23 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.1108
2022-07-02 23:29:56 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.4324
2022-07-02 23:30:29 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.1008
2022-07-02 23:31:03 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.9825
2022-07-02 23:31:35 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.2753
2022-07-02 23:32:08 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.3120
2022-07-02 23:32:42 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.1263
2022-07-02 23:33:16 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.4749
2022-07-02 23:33:48 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.2647
2022-07-02 23:34:21 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.4093
2022-07-02 23:34:55 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.2973
2022-07-02 23:35:28 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.4323
2022-07-02 23:36:01 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.0936
2022-07-02 23:36:34 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1895
2022-07-02 23:37:08 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.2870
2022-07-02 23:37:41 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.3421
2022-07-02 23:38:15 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.1465
2022-07-02 23:38:49 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.4461
2022-07-02 23:39:22 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.4195
2022-07-02 23:39:55 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.2010
2022-07-02 23:40:27 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.2724
2022-07-02 23:40:28 - train: epoch 018, train_loss: 2.2319
2022-07-02 23:41:41 - eval: epoch: 018, acc1: 52.552%, acc5: 77.924%, test_loss: 2.0419, per_image_load_time: 2.554ms, per_image_inference_time: 0.279ms
2022-07-02 23:41:42 - until epoch: 018, best_acc1: 54.940%
2022-07-02 23:41:42 - epoch 019 lr: 0.100000
2022-07-02 23:42:21 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.1366
2022-07-02 23:42:53 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.1965
2022-07-02 23:43:25 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.4456
2022-07-02 23:43:58 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.1474
2022-07-02 23:44:30 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.2850
2022-07-02 23:45:04 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.2153
2022-07-02 23:45:37 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.1645
2022-07-02 23:46:10 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.3747
2022-07-02 23:46:43 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.1669
2022-07-02 23:47:16 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.2121
2022-07-02 23:47:49 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.0991
2022-07-02 23:48:21 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.2013
2022-07-02 23:48:56 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.2590
2022-07-02 23:49:28 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.9566
2022-07-02 23:50:01 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.5070
2022-07-02 23:50:34 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9294
2022-07-02 23:51:08 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.4832
2022-07-02 23:51:41 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.0795
2022-07-02 23:52:14 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.4358
2022-07-02 23:52:47 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.1554
2022-07-02 23:53:20 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.1102
2022-07-02 23:53:54 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.2263
2022-07-02 23:54:27 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.1444
2022-07-02 23:55:00 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.4302
2022-07-02 23:55:33 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.2096
2022-07-02 23:56:07 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.1146
2022-07-02 23:56:40 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.1213
2022-07-02 23:57:12 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.0802
2022-07-02 23:57:47 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.2557
2022-07-02 23:58:19 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.1936
2022-07-02 23:58:52 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.1475
2022-07-02 23:59:25 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.9137
2022-07-02 23:59:59 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.2274
2022-07-03 00:00:32 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.2469
2022-07-03 00:01:04 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.3717
2022-07-03 00:01:37 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.1102
2022-07-03 00:02:11 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.4130
2022-07-03 00:02:44 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.5548
2022-07-03 00:03:16 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.0729
2022-07-03 00:03:50 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2390
2022-07-03 00:04:23 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.2233
2022-07-03 00:04:56 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.2529
2022-07-03 00:05:29 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.0278
2022-07-03 00:06:03 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.1332
2022-07-03 00:06:35 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.5777
2022-07-03 00:07:09 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.1492
2022-07-03 00:07:42 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.3012
2022-07-03 00:08:15 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3219
2022-07-03 00:08:48 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.1125
2022-07-03 00:09:20 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.2336
2022-07-03 00:09:21 - train: epoch 019, train_loss: 2.2235
2022-07-03 00:10:34 - eval: epoch: 019, acc1: 50.066%, acc5: 75.792%, test_loss: 2.1811, per_image_load_time: 2.576ms, per_image_inference_time: 0.260ms
2022-07-03 00:10:34 - until epoch: 019, best_acc1: 54.940%
2022-07-03 00:10:34 - epoch 020 lr: 0.100000
2022-07-03 00:11:12 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.1352
2022-07-03 00:11:45 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.1066
2022-07-03 00:12:19 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.1026
2022-07-03 00:12:51 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.8705
2022-07-03 00:13:26 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.1661
2022-07-03 00:13:58 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.1581
2022-07-03 00:14:31 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.9169
2022-07-03 00:15:03 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.2330
2022-07-03 00:15:37 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.3485
2022-07-03 00:16:10 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.2360
2022-07-03 00:16:43 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.0221
2022-07-03 00:17:16 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.2197
2022-07-03 00:17:50 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.3487
2022-07-03 00:18:23 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.2603
2022-07-03 00:18:56 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.1148
2022-07-03 00:19:29 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2444
2022-07-03 00:20:02 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.1424
2022-07-03 00:20:37 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.1596
2022-07-03 00:21:09 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.1742
2022-07-03 00:21:42 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.3757
2022-07-03 00:22:16 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.4445
2022-07-03 00:22:50 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.9843
2022-07-03 00:23:22 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.1921
2022-07-03 00:23:56 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.4236
2022-07-03 00:24:29 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.0723
2022-07-03 00:25:02 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.2103
2022-07-03 00:25:34 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4133
2022-07-03 00:26:09 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.4094
2022-07-03 00:26:42 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.3016
2022-07-03 00:27:15 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.3263
2022-07-03 00:27:48 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.4560
2022-07-03 00:28:22 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2891
2022-07-03 00:28:55 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.9635
2022-07-03 00:29:28 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.1320
2022-07-03 00:30:01 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.1077
2022-07-03 00:30:35 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.1952
2022-07-03 00:31:07 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.0754
2022-07-03 00:31:41 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.3505
2022-07-03 00:32:13 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.6169
2022-07-03 00:32:47 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.1412
2022-07-03 00:33:20 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2364
2022-07-03 00:33:53 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.1248
2022-07-03 00:34:27 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.4017
2022-07-03 00:35:00 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.1011
2022-07-03 00:35:33 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.1731
2022-07-03 00:36:07 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.1867
2022-07-03 00:36:40 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.1266
2022-07-03 00:37:13 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.1926
2022-07-03 00:37:46 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.2960
2022-07-03 00:38:18 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.2309
2022-07-03 00:38:19 - train: epoch 020, train_loss: 2.2171
2022-07-03 00:39:32 - eval: epoch: 020, acc1: 54.004%, acc5: 79.188%, test_loss: 1.9490, per_image_load_time: 1.953ms, per_image_inference_time: 0.300ms
2022-07-03 00:39:33 - until epoch: 020, best_acc1: 54.940%
2022-07-03 00:39:33 - epoch 021 lr: 0.100000
2022-07-03 00:40:10 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.2009
2022-07-03 00:40:44 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.4069
2022-07-03 00:41:17 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.9363
2022-07-03 00:41:50 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.1050
2022-07-03 00:42:22 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.0435
2022-07-03 00:42:55 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.2089
2022-07-03 00:43:29 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.0529
2022-07-03 00:44:01 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.1783
2022-07-03 00:44:33 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.2805
2022-07-03 00:45:06 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.0617
2022-07-03 00:45:40 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.4177
2022-07-03 00:46:12 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.0333
2022-07-03 00:46:45 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.1216
2022-07-03 00:47:19 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 1.9685
2022-07-03 00:47:52 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1197
2022-07-03 00:48:25 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.0803
2022-07-03 00:48:58 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.2240
2022-07-03 00:49:32 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.9594
2022-07-03 00:50:06 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.3652
2022-07-03 00:50:38 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.2131
2022-07-03 00:51:11 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1000
2022-07-03 00:51:45 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.2957
2022-07-03 00:52:18 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1891
2022-07-03 00:52:50 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2549
2022-07-03 00:53:24 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2517
2022-07-03 00:53:57 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.4688
2022-07-03 00:54:31 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.1890
2022-07-03 00:55:04 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.1882
2022-07-03 00:55:38 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.0285
2022-07-03 00:56:11 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.4099
2022-07-03 00:56:45 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.2039
2022-07-03 00:57:18 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.0532
2022-07-03 00:57:52 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.3506
2022-07-03 00:58:24 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.1958
2022-07-03 00:58:58 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.2837
2022-07-03 00:59:31 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.1778
2022-07-03 01:00:04 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2924
2022-07-03 01:00:38 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.3507
2022-07-03 01:01:10 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.0852
2022-07-03 01:01:44 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.4495
2022-07-03 01:02:17 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.0945
2022-07-03 01:02:50 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.2723
2022-07-03 01:03:24 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.2834
2022-07-03 01:03:57 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4529
2022-07-03 01:04:30 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.5204
2022-07-03 01:05:03 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.0613
2022-07-03 01:05:36 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.4887
2022-07-03 01:06:11 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.3178
2022-07-03 01:06:43 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.0593
2022-07-03 01:07:15 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.2591
2022-07-03 01:07:16 - train: epoch 021, train_loss: 2.2078
2022-07-03 01:08:29 - eval: epoch: 021, acc1: 53.164%, acc5: 78.564%, test_loss: 1.9960, per_image_load_time: 2.565ms, per_image_inference_time: 0.265ms
2022-07-03 01:08:30 - until epoch: 021, best_acc1: 54.940%
2022-07-03 01:08:30 - epoch 022 lr: 0.100000
2022-07-03 01:09:07 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.0091
2022-07-03 01:09:40 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1589
2022-07-03 01:10:13 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.9480
2022-07-03 01:10:46 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.1301
2022-07-03 01:11:19 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.1051
2022-07-03 01:11:53 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.1827
2022-07-03 01:12:25 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.3779
2022-07-03 01:12:59 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.2032
2022-07-03 01:13:31 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.3109
2022-07-03 01:14:04 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.0450
2022-07-03 01:14:38 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.3414
2022-07-03 01:15:11 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.8155
2022-07-03 01:15:44 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.1011
2022-07-03 01:16:18 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.1883
2022-07-03 01:16:50 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.3275
2022-07-03 01:17:24 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.1774
2022-07-03 01:17:57 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.3485
2022-07-03 01:18:30 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.6183
2022-07-03 01:19:04 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.2971
2022-07-03 01:19:37 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.2947
2022-07-03 01:20:11 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.1632
2022-07-03 01:20:43 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.7105
2022-07-03 01:21:17 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.2165
2022-07-03 01:21:49 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.2510
2022-07-03 01:22:23 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.2258
2022-07-03 01:22:56 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.2599
2022-07-03 01:23:29 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.0109
2022-07-03 01:24:01 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.2968
2022-07-03 01:24:35 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.2799
2022-07-03 01:25:08 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.1361
2022-07-03 01:25:41 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.4013
2022-07-03 01:26:13 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.1028
2022-07-03 01:26:47 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3581
2022-07-03 01:27:20 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.9988
2022-07-03 01:27:53 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4468
2022-07-03 01:28:26 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.2653
2022-07-03 01:29:00 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.3114
2022-07-03 01:29:33 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.2853
2022-07-03 01:30:06 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.0938
2022-07-03 01:30:39 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.3302
2022-07-03 01:31:13 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.9226
2022-07-03 01:31:46 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2459
2022-07-03 01:32:19 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.2415
2022-07-03 01:32:52 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.1605
2022-07-03 01:33:26 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.2144
2022-07-03 01:33:57 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.3516
2022-07-03 01:34:31 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.1985
2022-07-03 01:35:04 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.1690
2022-07-03 01:35:37 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.2025
2022-07-03 01:36:09 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.0408
2022-07-03 01:36:10 - train: epoch 022, train_loss: 2.2048
2022-07-03 01:37:23 - eval: epoch: 022, acc1: 54.012%, acc5: 79.354%, test_loss: 1.9429, per_image_load_time: 2.580ms, per_image_inference_time: 0.279ms
2022-07-03 01:37:24 - until epoch: 022, best_acc1: 54.940%
2022-07-03 01:37:24 - epoch 023 lr: 0.100000
2022-07-03 01:38:01 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.0512
2022-07-03 01:38:34 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.1954
2022-07-03 01:39:08 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.1685
2022-07-03 01:39:41 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.1467
2022-07-03 01:40:14 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.3701
2022-07-03 01:40:47 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.0189
2022-07-03 01:41:19 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.1081
2022-07-03 01:41:52 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.3334
2022-07-03 01:42:26 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.2343
2022-07-03 01:42:59 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.1164
2022-07-03 01:43:32 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.1121
2022-07-03 01:44:05 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.0361
2022-07-03 01:44:38 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.1743
2022-07-03 01:45:12 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.2742
2022-07-03 01:45:44 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.9572
2022-07-03 01:46:18 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.1843
2022-07-03 01:46:50 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 1.9766
2022-07-03 01:47:24 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.0456
2022-07-03 01:47:57 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.4256
2022-07-03 01:48:30 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9851
2022-07-03 01:49:03 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.2221
2022-07-03 01:49:37 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.9231
2022-07-03 01:50:10 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.1145
2022-07-03 01:50:43 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.2037
2022-07-03 01:51:16 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.3971
2022-07-03 01:51:49 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.2185
2022-07-03 01:52:22 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.2961
2022-07-03 01:52:56 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.2846
2022-07-03 01:53:29 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.2505
2022-07-03 01:54:02 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.3175
2022-07-03 01:54:36 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.3596
2022-07-03 01:55:09 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.2899
2022-07-03 01:55:42 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.0917
2022-07-03 01:56:16 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.2436
2022-07-03 01:56:49 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.0280
2022-07-03 01:57:22 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.1260
2022-07-03 01:57:56 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.0108
2022-07-03 01:58:29 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.1714
2022-07-03 01:59:02 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.1869
2022-07-03 01:59:35 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.0843
2022-07-03 02:00:09 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.1307
2022-07-03 02:00:42 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.8932
2022-07-03 02:01:16 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.0448
2022-07-03 02:01:48 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1339
2022-07-03 02:02:22 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.3519
2022-07-03 02:02:55 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.3379
2022-07-03 02:03:28 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.9557
2022-07-03 02:04:01 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.2815
2022-07-03 02:04:35 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.0251
2022-07-03 02:05:06 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.2657
2022-07-03 02:05:07 - train: epoch 023, train_loss: 2.1967
2022-07-03 02:06:21 - eval: epoch: 023, acc1: 54.678%, acc5: 79.578%, test_loss: 1.9280, per_image_load_time: 2.619ms, per_image_inference_time: 0.271ms
2022-07-03 02:06:21 - until epoch: 023, best_acc1: 54.940%
2022-07-03 02:06:21 - epoch 024 lr: 0.100000
2022-07-03 02:06:58 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.2095
2022-07-03 02:07:33 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.1293
2022-07-03 02:08:05 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.9698
2022-07-03 02:08:38 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.3425
2022-07-03 02:09:10 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.1678
2022-07-03 02:09:44 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.2483
2022-07-03 02:10:17 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.0767
2022-07-03 02:10:50 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.0617
2022-07-03 02:11:23 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.1253
2022-07-03 02:11:57 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.1748
2022-07-03 02:12:29 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.9196
2022-07-03 02:13:02 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.1750
2022-07-03 02:13:36 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.3791
2022-07-03 02:14:08 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.9204
2022-07-03 02:14:42 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.4560
2022-07-03 02:15:14 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.1300
2022-07-03 02:15:48 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.1061
2022-07-03 02:16:21 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.3582
2022-07-03 02:16:53 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.8781
2022-07-03 02:17:27 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.3081
2022-07-03 02:18:00 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.1513
2022-07-03 02:18:33 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.1052
2022-07-03 02:19:06 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.1878
2022-07-03 02:19:39 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.1615
2022-07-03 02:20:13 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1344
2022-07-03 02:20:46 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.3463
2022-07-03 02:21:20 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.2571
2022-07-03 02:21:52 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.3270
2022-07-03 02:22:27 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.2500
2022-07-03 02:22:59 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.1741
2022-07-03 02:23:32 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.1042
2022-07-03 02:24:06 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.3856
2022-07-03 02:24:40 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.1258
2022-07-03 02:25:12 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.0962
2022-07-03 02:25:46 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.2119
2022-07-03 02:26:19 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.1547
2022-07-03 02:26:52 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.2647
2022-07-03 02:27:26 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.2866
2022-07-03 02:27:59 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.9748
2022-07-03 02:28:33 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.1708
2022-07-03 02:29:05 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.0972
2022-07-03 02:29:39 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.0616
2022-07-03 02:30:11 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 1.9093
2022-07-03 02:30:45 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.2039
2022-07-03 02:31:19 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.1723
2022-07-03 02:31:50 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.2254
2022-07-03 02:32:24 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.2702
2022-07-03 02:32:57 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.8612
2022-07-03 02:33:31 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3333
2022-07-03 02:34:02 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.3042
2022-07-03 02:34:03 - train: epoch 024, train_loss: 2.1940
2022-07-03 02:35:16 - eval: epoch: 024, acc1: 53.976%, acc5: 78.996%, test_loss: 1.9626, per_image_load_time: 2.588ms, per_image_inference_time: 0.269ms
2022-07-03 02:35:17 - until epoch: 024, best_acc1: 54.940%
2022-07-03 02:35:17 - epoch 025 lr: 0.100000
2022-07-03 02:35:55 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.1293
2022-07-03 02:36:28 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.9912
2022-07-03 02:37:01 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.0352
2022-07-03 02:37:34 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.2015
2022-07-03 02:38:07 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.0401
2022-07-03 02:38:40 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.3033
2022-07-03 02:39:13 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.3751
2022-07-03 02:39:46 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.0371
2022-07-03 02:40:19 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.9152
2022-07-03 02:40:52 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.0070
2022-07-03 02:41:25 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.0488
2022-07-03 02:41:59 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.2626
2022-07-03 02:42:31 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.2769
2022-07-03 02:43:04 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.1982
2022-07-03 02:43:38 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.0271
2022-07-03 02:44:10 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.0910
2022-07-03 02:44:43 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.2374
2022-07-03 02:45:17 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.9445
2022-07-03 02:45:50 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.0465
2022-07-03 02:46:24 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.1456
2022-07-03 02:46:57 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.0035
2022-07-03 02:47:31 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.9968
2022-07-03 02:48:04 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.0638
2022-07-03 02:48:37 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.8859
2022-07-03 02:49:10 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.2759
2022-07-03 02:49:44 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.2905
2022-07-03 02:50:17 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.2713
2022-07-03 02:50:50 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.0851
2022-07-03 02:51:24 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.1735
2022-07-03 02:51:56 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.1457
2022-07-03 02:52:30 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.4500
2022-07-03 02:53:02 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.0774
2022-07-03 02:53:35 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.1477
2022-07-03 02:54:09 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.2313
2022-07-03 02:54:42 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.1322
2022-07-03 02:55:15 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.2306
2022-07-03 02:55:49 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.2385
2022-07-03 02:56:22 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.4814
2022-07-03 02:56:55 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.3206
2022-07-03 02:57:29 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4082
2022-07-03 02:58:02 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.4926
2022-07-03 02:58:35 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.2865
2022-07-03 02:59:08 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1557
2022-07-03 02:59:42 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.8716
2022-07-03 03:00:16 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.2465
2022-07-03 03:00:49 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 1.9787
2022-07-03 03:01:22 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.2200
2022-07-03 03:01:55 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.0186
2022-07-03 03:02:28 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.3755
2022-07-03 03:03:00 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.4066
2022-07-03 03:03:01 - train: epoch 025, train_loss: 2.1871
2022-07-03 03:04:14 - eval: epoch: 025, acc1: 53.232%, acc5: 78.722%, test_loss: 1.9855, per_image_load_time: 2.433ms, per_image_inference_time: 0.261ms
2022-07-03 03:04:14 - until epoch: 025, best_acc1: 54.940%
2022-07-03 03:04:14 - epoch 026 lr: 0.100000
2022-07-03 03:04:52 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1956
2022-07-03 03:05:26 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.9152
2022-07-03 03:05:59 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.1263
2022-07-03 03:06:30 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.0884
2022-07-03 03:07:04 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.0736
2022-07-03 03:07:37 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.3731
2022-07-03 03:08:10 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2589
2022-07-03 03:08:42 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.1832
2022-07-03 03:09:16 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.2984
2022-07-03 03:09:49 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.1765
2022-07-03 03:10:22 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3190
2022-07-03 03:10:55 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.2803
2022-07-03 03:11:27 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.0140
2022-07-03 03:12:01 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.1235
2022-07-03 03:12:35 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.0700
2022-07-03 03:13:09 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.1646
2022-07-03 03:13:41 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.0925
2022-07-03 03:14:15 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.4576
2022-07-03 03:14:47 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.2124
2022-07-03 03:15:20 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.1640
2022-07-03 03:15:53 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.1051
2022-07-03 03:16:27 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.1262
2022-07-03 03:17:00 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.2840
2022-07-03 03:17:35 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.2689
2022-07-03 03:18:07 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.2833
2022-07-03 03:18:41 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.1762
2022-07-03 03:19:13 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.2902
2022-07-03 03:19:47 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.1216
2022-07-03 03:20:20 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.2194
2022-07-03 03:20:53 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.2169
2022-07-03 03:21:27 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.2483
2022-07-03 03:22:00 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.9091
2022-07-03 03:22:33 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.3125
2022-07-03 03:23:07 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.2364
2022-07-03 03:23:40 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.5239
2022-07-03 03:24:13 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.1481
2022-07-03 03:24:46 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.0404
2022-07-03 03:25:20 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.3860
2022-07-03 03:25:53 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.4590
2022-07-03 03:26:26 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.2695
2022-07-03 03:27:00 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.1980
2022-07-03 03:27:33 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.1503
2022-07-03 03:28:05 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.2114
2022-07-03 03:28:39 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.3932
2022-07-03 03:29:12 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.2591
2022-07-03 03:29:46 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.3090
2022-07-03 03:30:19 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.1500
2022-07-03 03:30:53 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 1.8812
2022-07-03 03:31:26 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.3341
2022-07-03 03:31:57 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.1891
2022-07-03 03:31:58 - train: epoch 026, train_loss: 2.1820
2022-07-03 03:33:11 - eval: epoch: 026, acc1: 54.452%, acc5: 79.394%, test_loss: 1.9379, per_image_load_time: 2.365ms, per_image_inference_time: 0.277ms
2022-07-03 03:33:11 - until epoch: 026, best_acc1: 54.940%
2022-07-03 03:33:11 - epoch 027 lr: 0.100000
2022-07-03 03:33:49 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.1626
2022-07-03 03:34:22 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.1212
2022-07-03 03:34:55 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.1492
2022-07-03 03:35:28 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.3557
2022-07-03 03:36:01 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3737
2022-07-03 03:36:34 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.1704
2022-07-03 03:37:06 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.1084
2022-07-03 03:37:40 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.0653
2022-07-03 03:38:12 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.1750
2022-07-03 03:38:46 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.2119
2022-07-03 03:39:19 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.1192
2022-07-03 03:39:52 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.2582
2022-07-03 03:40:25 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.1515
2022-07-03 03:40:58 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.3077
2022-07-03 03:41:31 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.3697
2022-07-03 03:42:04 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.2819
2022-07-03 03:42:37 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.1889
2022-07-03 03:43:11 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.2766
2022-07-03 03:43:43 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.3669
2022-07-03 03:44:17 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.0684
2022-07-03 03:44:50 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.1931
2022-07-03 03:45:23 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.3790
2022-07-03 03:45:57 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.4595
2022-07-03 03:46:30 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1479
2022-07-03 03:47:03 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.2198
2022-07-03 03:47:36 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.3131
2022-07-03 03:48:09 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.1804
2022-07-03 03:48:42 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.0185
2022-07-03 03:49:16 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.2249
2022-07-03 03:49:49 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1692
2022-07-03 03:50:22 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.2177
2022-07-03 03:50:55 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.1561
2022-07-03 03:51:28 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.3271
2022-07-03 03:52:02 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.1224
2022-07-03 03:52:35 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.3214
2022-07-03 03:53:08 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.3629
2022-07-03 03:53:41 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.2333
2022-07-03 03:54:14 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.9437
2022-07-03 03:54:48 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.2213
2022-07-03 03:55:21 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.1472
2022-07-03 03:55:55 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.0796
2022-07-03 03:56:28 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.2954
2022-07-03 03:57:01 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.2025
2022-07-03 03:57:35 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3469
2022-07-03 03:58:08 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.0051
2022-07-03 03:58:41 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.9113
2022-07-03 03:59:14 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.4626
2022-07-03 03:59:48 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.5229
2022-07-03 04:00:21 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.0862
2022-07-03 04:00:51 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.7824
2022-07-03 04:00:52 - train: epoch 027, train_loss: 2.1809
2022-07-03 04:02:05 - eval: epoch: 027, acc1: 53.634%, acc5: 78.928%, test_loss: 1.9748, per_image_load_time: 2.344ms, per_image_inference_time: 0.263ms
2022-07-03 04:02:06 - until epoch: 027, best_acc1: 54.940%
2022-07-03 04:02:06 - epoch 028 lr: 0.100000
2022-07-03 04:02:43 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.0498
2022-07-03 04:03:17 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.3240
2022-07-03 04:03:49 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.9731
2022-07-03 04:04:22 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.3896
2022-07-03 04:04:55 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.1644
2022-07-03 04:05:29 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.2688
2022-07-03 04:06:02 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.4259
2022-07-03 04:06:34 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.1338
2022-07-03 04:07:07 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.1819
2022-07-03 04:07:40 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.2058
2022-07-03 04:08:13 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.1556
2022-07-03 04:08:47 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.0146
2022-07-03 04:09:19 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.2780
2022-07-03 04:09:52 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.5752
2022-07-03 04:10:25 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.2901
2022-07-03 04:10:59 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.1432
2022-07-03 04:11:32 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.1991
2022-07-03 04:12:05 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.1381
2022-07-03 04:12:37 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.1838
2022-07-03 04:13:11 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.2295
2022-07-03 04:13:43 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.3554
2022-07-03 04:14:16 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.3515
2022-07-03 04:14:48 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.3781
2022-07-03 04:15:22 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.2859
2022-07-03 04:15:56 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.0890
2022-07-03 04:16:29 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.1108
2022-07-03 04:17:02 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.1386
2022-07-03 04:17:35 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.2517
2022-07-03 04:18:09 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.2783
2022-07-03 04:18:42 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.2555
2022-07-03 04:19:14 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.4792
2022-07-03 04:19:47 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.8113
2022-07-03 04:20:20 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.2798
2022-07-03 04:20:54 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.2903
2022-07-03 04:21:26 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.2215
2022-07-03 04:22:00 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.1214
2022-07-03 04:22:33 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.1382
2022-07-03 04:23:05 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.9456
2022-07-03 04:23:38 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.2841
2022-07-03 04:24:12 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.2964
2022-07-03 04:24:45 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.1770
2022-07-03 04:25:17 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.3970
2022-07-03 04:25:50 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.2304
2022-07-03 04:26:24 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.3095
2022-07-03 04:26:57 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.1050
2022-07-03 04:27:29 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.2158
2022-07-03 04:28:03 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.3334
2022-07-03 04:28:36 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.2364
2022-07-03 04:29:09 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.1757
2022-07-03 04:29:40 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.9857
2022-07-03 04:29:41 - train: epoch 028, train_loss: 2.1760
2022-07-03 04:30:55 - eval: epoch: 028, acc1: 53.738%, acc5: 78.658%, test_loss: 1.9849, per_image_load_time: 2.593ms, per_image_inference_time: 0.271ms
2022-07-03 04:30:55 - until epoch: 028, best_acc1: 54.940%
2022-07-03 04:30:55 - epoch 029 lr: 0.100000
2022-07-03 04:31:33 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.0575
2022-07-03 04:32:07 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.0325
2022-07-03 04:32:40 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.2593
2022-07-03 04:33:12 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.3162
2022-07-03 04:33:45 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 1.9485
2022-07-03 04:34:18 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.3504
2022-07-03 04:34:51 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9899
2022-07-03 04:35:24 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.0341
2022-07-03 04:35:57 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1144
2022-07-03 04:36:30 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.2489
2022-07-03 04:37:03 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.9446
2022-07-03 04:37:35 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.1901
2022-07-03 04:38:09 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.2285
2022-07-03 04:38:42 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.3252
2022-07-03 04:39:15 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.2656
2022-07-03 04:39:48 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.2210
2022-07-03 04:40:22 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.0263
2022-07-03 04:40:55 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.2053
2022-07-03 04:41:28 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.9632
2022-07-03 04:42:00 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 1.9318
2022-07-03 04:42:34 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.2071
2022-07-03 04:43:07 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.2035
2022-07-03 04:43:40 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.2179
2022-07-03 04:44:14 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.0210
2022-07-03 04:44:47 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.0973
2022-07-03 04:45:21 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.2285
2022-07-03 04:45:53 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.2540
2022-07-03 04:46:27 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.3081
2022-07-03 04:46:59 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.1132
2022-07-03 04:47:33 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.1389
2022-07-03 04:48:07 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.1582
2022-07-03 04:48:40 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.1416
2022-07-03 04:49:13 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.1564
2022-07-03 04:49:46 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.1193
2022-07-03 04:50:20 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.3440
2022-07-03 04:50:53 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.0269
2022-07-03 04:51:26 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.1530
2022-07-03 04:52:00 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.1605
2022-07-03 04:52:33 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.0127
2022-07-03 04:53:06 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.3395
2022-07-03 04:53:39 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.1222
2022-07-03 04:54:13 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.8594
2022-07-03 04:54:46 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.2030
2022-07-03 04:55:19 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.3845
2022-07-03 04:55:52 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.3183
2022-07-03 04:56:25 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.3099
2022-07-03 04:56:59 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.0455
2022-07-03 04:57:31 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.2414
2022-07-03 04:58:05 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.2727
2022-07-03 04:58:37 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.8510
2022-07-03 04:58:38 - train: epoch 029, train_loss: 2.1718
2022-07-03 04:59:51 - eval: epoch: 029, acc1: 52.342%, acc5: 77.610%, test_loss: 2.0470, per_image_load_time: 2.579ms, per_image_inference_time: 0.295ms
2022-07-03 04:59:52 - until epoch: 029, best_acc1: 54.940%
2022-07-03 04:59:52 - epoch 030 lr: 0.100000
2022-07-03 05:00:29 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.2060
2022-07-03 05:01:02 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.0606
2022-07-03 05:01:35 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.9297
2022-07-03 05:02:08 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.1087
2022-07-03 05:02:41 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.2025
2022-07-03 05:03:14 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0070
2022-07-03 05:03:47 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.0980
2022-07-03 05:04:20 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.0939
2022-07-03 05:04:53 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.2657
2022-07-03 05:05:26 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.1754
2022-07-03 05:05:59 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.0776
2022-07-03 05:06:31 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.0356
2022-07-03 05:07:04 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.8806
2022-07-03 05:07:38 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.0377
2022-07-03 05:08:11 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.1577
2022-07-03 05:08:44 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.0806
2022-07-03 05:09:17 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.2969
2022-07-03 05:09:49 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.0299
2022-07-03 05:10:23 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.0542
2022-07-03 05:10:55 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.1194
2022-07-03 05:11:28 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.3697
2022-07-03 05:12:01 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.0855
2022-07-03 05:12:35 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.3021
2022-07-03 05:13:08 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.2505
2022-07-03 05:13:40 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.3502
2022-07-03 05:14:14 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.2188
2022-07-03 05:14:47 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.1518
2022-07-03 05:15:20 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3458
2022-07-03 05:15:53 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.1294
2022-07-03 05:16:26 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.2780
2022-07-03 05:16:59 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.1153
2022-07-03 05:17:32 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.2503
2022-07-03 05:18:05 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.0228
2022-07-03 05:18:40 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 1.9880
2022-07-03 05:19:12 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.1895
2022-07-03 05:19:46 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.3870
2022-07-03 05:20:18 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.2608
2022-07-03 05:20:52 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.2086
2022-07-03 05:21:24 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.1938
2022-07-03 05:21:59 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.1319
2022-07-03 05:22:31 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.0258
2022-07-03 05:23:04 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.4305
2022-07-03 05:23:37 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.1556
2022-07-03 05:24:11 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.0839
2022-07-03 05:24:43 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.2109
2022-07-03 05:25:17 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.0233
2022-07-03 05:25:51 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 1.9873
2022-07-03 05:26:24 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.9942
2022-07-03 05:26:57 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.3887
2022-07-03 05:27:28 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.2591
2022-07-03 05:27:29 - train: epoch 030, train_loss: 2.1675
2022-07-03 05:28:43 - eval: epoch: 030, acc1: 53.600%, acc5: 78.974%, test_loss: 1.9856, per_image_load_time: 2.439ms, per_image_inference_time: 0.311ms
2022-07-03 05:28:43 - until epoch: 030, best_acc1: 54.940%
2022-07-03 05:28:43 - epoch 031 lr: 0.010000
2022-07-03 05:29:20 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.9130
2022-07-03 05:29:55 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.7810
2022-07-03 05:30:27 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.9579
2022-07-03 05:31:00 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.9295
2022-07-03 05:31:33 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.8190
2022-07-03 05:32:05 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.8369
2022-07-03 05:32:39 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.8518
2022-07-03 05:33:13 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.8794
2022-07-03 05:33:44 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.7884
2022-07-03 05:34:18 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.8196
2022-07-03 05:34:51 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.8867
2022-07-03 05:35:23 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.8392
2022-07-03 05:35:57 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.2841
2022-07-03 05:36:30 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.5857
2022-07-03 05:37:04 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.8396
2022-07-03 05:37:37 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.5751
2022-07-03 05:38:10 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.7266
2022-07-03 05:38:42 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.3947
2022-07-03 05:39:16 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.5292
2022-07-03 05:39:48 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.6600
2022-07-03 05:40:22 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6473
2022-07-03 05:40:55 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.5461
2022-07-03 05:41:29 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.4505
2022-07-03 05:42:02 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.7885
2022-07-03 05:42:36 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.6294
2022-07-03 05:43:08 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.4735
2022-07-03 05:43:42 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.6083
2022-07-03 05:44:15 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.0658
2022-07-03 05:44:48 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.7508
2022-07-03 05:45:21 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.9799
2022-07-03 05:45:54 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.7255
2022-07-03 05:46:27 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.6874
2022-07-03 05:47:01 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.5461
2022-07-03 05:47:34 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.8784
2022-07-03 05:48:09 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.8713
2022-07-03 05:48:41 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7998
2022-07-03 05:49:14 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.5134
2022-07-03 05:49:47 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.7162
2022-07-03 05:50:20 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.6840
2022-07-03 05:50:54 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.5544
2022-07-03 05:51:27 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.6814
2022-07-03 05:52:00 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.7103
2022-07-03 05:52:34 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.5627
2022-07-03 05:53:06 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.6914
2022-07-03 05:53:40 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.7279
2022-07-03 05:54:13 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7798
2022-07-03 05:54:47 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.4764
2022-07-03 05:55:20 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.6895
2022-07-03 05:55:53 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.8283
2022-07-03 05:56:25 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.5702
2022-07-03 05:56:26 - train: epoch 031, train_loss: 1.6954
2022-07-03 05:57:39 - eval: epoch: 031, acc1: 66.896%, acc5: 87.552%, test_loss: 1.3554, per_image_load_time: 2.564ms, per_image_inference_time: 0.292ms
2022-07-03 05:57:39 - until epoch: 031, best_acc1: 66.896%
2022-07-03 05:57:39 - epoch 032 lr: 0.010000
2022-07-03 05:58:18 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.6850
2022-07-03 05:58:51 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.6637
2022-07-03 05:59:24 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.4713
2022-07-03 05:59:56 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.5289
2022-07-03 06:00:30 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.6178
2022-07-03 06:01:03 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.7546
2022-07-03 06:01:35 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6194
2022-07-03 06:02:09 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.7320
2022-07-03 06:02:42 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.7102
2022-07-03 06:03:15 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7864
2022-07-03 06:03:48 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.5780
2022-07-03 06:04:21 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.5620
2022-07-03 06:04:55 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.5738
2022-07-03 06:05:28 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.7092
2022-07-03 06:06:00 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.4461
2022-07-03 06:06:33 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.7657
2022-07-03 06:07:07 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.6941
2022-07-03 06:07:40 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.9502
2022-07-03 06:08:13 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.3953
2022-07-03 06:08:46 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.5781
2022-07-03 06:09:19 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.4190
2022-07-03 06:09:52 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.3679
2022-07-03 06:10:25 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.4609
2022-07-03 06:10:58 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.5635
2022-07-03 06:11:31 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.4305
2022-07-03 06:12:04 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.3793
2022-07-03 06:12:38 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.4807
2022-07-03 06:13:11 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.5316
2022-07-03 06:13:44 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.4375
2022-07-03 06:14:17 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.5283
2022-07-03 06:14:50 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.5366
2022-07-03 06:15:24 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.6642
2022-07-03 06:15:57 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.5911
2022-07-03 06:16:30 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.2586
2022-07-03 06:17:03 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.6102
2022-07-03 06:17:37 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.7163
2022-07-03 06:18:10 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.6469
2022-07-03 06:18:43 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4130
2022-07-03 06:19:16 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.5311
2022-07-03 06:19:50 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4961
2022-07-03 06:20:23 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.5079
2022-07-03 06:20:56 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.4154
2022-07-03 06:21:29 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.9169
2022-07-03 06:22:03 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.5897
2022-07-03 06:22:36 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.7965
2022-07-03 06:23:09 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.4882
2022-07-03 06:23:42 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.5850
2022-07-03 06:24:16 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.5422
2022-07-03 06:24:49 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.5182
2022-07-03 06:25:20 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6159
2022-07-03 06:25:21 - train: epoch 032, train_loss: 1.5683
2022-07-03 06:26:35 - eval: epoch: 032, acc1: 67.618%, acc5: 88.140%, test_loss: 1.3165, per_image_load_time: 2.003ms, per_image_inference_time: 0.266ms
2022-07-03 06:26:35 - until epoch: 032, best_acc1: 67.618%
2022-07-03 06:26:35 - epoch 033 lr: 0.010000
2022-07-03 06:27:13 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.3464
2022-07-03 06:27:46 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.4520
2022-07-03 06:28:19 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.5798
2022-07-03 06:28:52 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.5324
2022-07-03 06:29:25 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7196
2022-07-03 06:29:58 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.6571
2022-07-03 06:30:30 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.6907
2022-07-03 06:31:04 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.5208
2022-07-03 06:31:36 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.4638
2022-07-03 06:32:10 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.5930
2022-07-03 06:32:42 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.2945
2022-07-03 06:33:16 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.8807
2022-07-03 06:33:49 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.5081
2022-07-03 06:34:22 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.7685
2022-07-03 06:34:56 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.6039
2022-07-03 06:35:28 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.7419
2022-07-03 06:36:01 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.4795
2022-07-03 06:36:34 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.3825
2022-07-03 06:37:08 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.5971
2022-07-03 06:37:40 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.6631
2022-07-03 06:38:13 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.3570
2022-07-03 06:38:46 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.4842
2022-07-03 06:39:19 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.4122
2022-07-03 06:39:52 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.4765
2022-07-03 06:40:25 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.5030
2022-07-03 06:40:59 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.2660
2022-07-03 06:41:32 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.6099
2022-07-03 06:42:05 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.4903
2022-07-03 06:42:38 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.4231
2022-07-03 06:43:11 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.5125
2022-07-03 06:43:44 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.3558
2022-07-03 06:44:18 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.5775
2022-07-03 06:44:51 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.4968
2022-07-03 06:45:24 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.2471
2022-07-03 06:45:57 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.6211
2022-07-03 06:46:30 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.6174
2022-07-03 06:47:03 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.4742
2022-07-03 06:47:37 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.4860
2022-07-03 06:48:09 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.6434
2022-07-03 06:48:42 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.6087
2022-07-03 06:49:15 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.3712
2022-07-03 06:49:49 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.3959
2022-07-03 06:50:22 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.8054
2022-07-03 06:50:56 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.6493
2022-07-03 06:51:28 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.6756
2022-07-03 06:52:01 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.5354
2022-07-03 06:52:36 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.5272
2022-07-03 06:53:09 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.7551
2022-07-03 06:53:42 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.3640
2022-07-03 06:54:14 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.5350
2022-07-03 06:54:15 - train: epoch 033, train_loss: 1.5153
2022-07-03 06:55:28 - eval: epoch: 033, acc1: 68.206%, acc5: 88.518%, test_loss: 1.2908, per_image_load_time: 2.545ms, per_image_inference_time: 0.267ms
2022-07-03 06:55:28 - until epoch: 033, best_acc1: 68.206%
2022-07-03 06:55:28 - epoch 034 lr: 0.010000
2022-07-03 06:56:06 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.4952
2022-07-03 06:56:39 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.5145
2022-07-03 06:57:12 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.4070
2022-07-03 06:57:45 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.2560
2022-07-03 06:58:18 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.3776
2022-07-03 06:58:51 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.7078
2022-07-03 06:59:24 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.3839
2022-07-03 06:59:57 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.7516
2022-07-03 07:00:29 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.3130
2022-07-03 07:01:02 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.5804
2022-07-03 07:01:36 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.4257
2022-07-03 07:02:10 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.6619
2022-07-03 07:02:42 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.5127
2022-07-03 07:03:15 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.5835
2022-07-03 07:03:48 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.5688
2022-07-03 07:04:21 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.5981
2022-07-03 07:04:54 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.4296
2022-07-03 07:05:27 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.3799
2022-07-03 07:06:01 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.4717
2022-07-03 07:06:34 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.6166
2022-07-03 07:07:07 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.7381
2022-07-03 07:07:40 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.4769
2022-07-03 07:08:13 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.5304
2022-07-03 07:08:47 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.2930
2022-07-03 07:09:19 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.6248
2022-07-03 07:09:53 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.5712
2022-07-03 07:10:25 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.3264
2022-07-03 07:10:59 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.1843
2022-07-03 07:11:32 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3321
2022-07-03 07:12:05 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.4004
2022-07-03 07:12:39 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.5585
2022-07-03 07:13:12 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.3848
2022-07-03 07:13:45 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.4111
2022-07-03 07:14:18 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.5903
2022-07-03 07:14:52 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.3128
2022-07-03 07:15:25 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.2147
2022-07-03 07:15:58 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.5875
2022-07-03 07:16:31 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.5843
2022-07-03 07:17:04 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.5896
2022-07-03 07:17:38 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.3417
2022-07-03 07:18:12 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.5633
2022-07-03 07:18:45 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.4578
2022-07-03 07:19:18 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5595
2022-07-03 07:19:51 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.3100
2022-07-03 07:20:24 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.4790
2022-07-03 07:20:58 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.5317
2022-07-03 07:21:31 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.5011
2022-07-03 07:22:04 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5927
2022-07-03 07:22:38 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.5693
2022-07-03 07:23:09 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.3454
2022-07-03 07:23:10 - train: epoch 034, train_loss: 1.4822
2022-07-03 07:24:24 - eval: epoch: 034, acc1: 68.492%, acc5: 88.638%, test_loss: 1.2792, per_image_load_time: 2.584ms, per_image_inference_time: 0.274ms
2022-07-03 07:24:24 - until epoch: 034, best_acc1: 68.492%
2022-07-03 07:24:24 - epoch 035 lr: 0.010000
2022-07-03 07:25:02 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.2304
2022-07-03 07:25:35 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.3028
2022-07-03 07:26:08 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.5800
2022-07-03 07:26:41 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.3069
2022-07-03 07:27:13 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.2872
2022-07-03 07:27:47 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.4070
2022-07-03 07:28:18 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.5149
2022-07-03 07:28:52 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.4428
2022-07-03 07:29:25 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.5829
2022-07-03 07:29:59 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.4622
2022-07-03 07:30:32 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.6073
2022-07-03 07:31:05 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.2231
2022-07-03 07:31:39 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.6159
2022-07-03 07:32:12 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.3932
2022-07-03 07:32:44 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.4835
2022-07-03 07:33:19 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.3823
2022-07-03 07:33:51 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.2815
2022-07-03 07:34:25 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.4226
2022-07-03 07:34:58 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.4110
2022-07-03 07:35:31 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.3728
2022-07-03 07:36:04 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.3867
2022-07-03 07:36:37 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.3352
2022-07-03 07:37:11 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.4394
2022-07-03 07:37:44 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.6697
2022-07-03 07:38:18 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.4030
2022-07-03 07:38:51 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.4649
2022-07-03 07:39:24 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.5415
2022-07-03 07:39:58 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.4298
2022-07-03 07:40:30 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.2846
2022-07-03 07:41:04 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.1338
2022-07-03 07:41:36 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.3940
2022-07-03 07:42:10 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.5289
2022-07-03 07:42:43 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.2316
2022-07-03 07:43:16 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.4222
2022-07-03 07:43:50 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.2262
2022-07-03 07:44:24 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.3470
2022-07-03 07:44:56 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.3716
2022-07-03 07:45:30 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5602
2022-07-03 07:46:02 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.4906
2022-07-03 07:46:36 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.2886
2022-07-03 07:47:10 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.4971
2022-07-03 07:47:43 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.3223
2022-07-03 07:48:17 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.5900
2022-07-03 07:48:50 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.3432
2022-07-03 07:49:23 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.4350
2022-07-03 07:49:56 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5769
2022-07-03 07:50:30 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.5647
2022-07-03 07:51:03 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.5674
2022-07-03 07:51:37 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.5693
2022-07-03 07:52:08 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.4720
2022-07-03 07:52:09 - train: epoch 035, train_loss: 1.4585
2022-07-03 07:53:22 - eval: epoch: 035, acc1: 68.854%, acc5: 89.000%, test_loss: 1.2624, per_image_load_time: 2.458ms, per_image_inference_time: 0.265ms
2022-07-03 07:53:22 - until epoch: 035, best_acc1: 68.854%
2022-07-03 07:53:22 - epoch 036 lr: 0.010000
2022-07-03 07:54:00 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.7431
2022-07-03 07:54:34 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.2854
2022-07-03 07:55:06 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.5121
2022-07-03 07:55:38 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.4931
2022-07-03 07:56:12 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.4424
2022-07-03 07:56:44 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.2715
2022-07-03 07:57:17 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.4505
2022-07-03 07:57:50 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.5744
2022-07-03 07:58:23 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.4316
2022-07-03 07:58:56 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.4176
2022-07-03 07:59:29 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.2897
2022-07-03 08:00:02 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.3595
2022-07-03 08:00:35 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.3867
2022-07-03 08:01:09 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.7193
2022-07-03 08:01:42 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.5297
2022-07-03 08:02:15 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.2881
2022-07-03 08:02:49 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.4018
2022-07-03 08:03:22 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.3393
2022-07-03 08:03:55 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.4354
2022-07-03 08:04:28 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.3585
2022-07-03 08:05:02 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.2733
2022-07-03 08:05:35 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.5607
2022-07-03 08:06:07 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.5058
2022-07-03 08:06:41 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.4007
2022-07-03 08:07:14 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.4072
2022-07-03 08:07:47 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.4582
2022-07-03 08:08:20 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.3784
2022-07-03 08:08:54 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.2008
2022-07-03 08:09:27 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.2639
2022-07-03 08:10:01 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.2524
2022-07-03 08:10:34 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.4698
2022-07-03 08:11:07 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.5969
2022-07-03 08:11:40 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.4124
2022-07-03 08:12:13 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.3899
2022-07-03 08:12:46 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.5930
2022-07-03 08:13:20 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.6202
2022-07-03 08:13:54 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.3801
2022-07-03 08:14:27 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.3501
2022-07-03 08:15:00 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.4242
2022-07-03 08:15:34 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.5412
2022-07-03 08:16:07 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.4914
2022-07-03 08:16:40 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.4399
2022-07-03 08:17:13 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.2265
2022-07-03 08:17:48 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.5086
2022-07-03 08:18:20 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.3719
2022-07-03 08:18:54 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.2366
2022-07-03 08:19:27 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.3040
2022-07-03 08:20:01 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.4888
2022-07-03 08:20:34 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.4304
2022-07-03 08:21:06 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.4043
2022-07-03 08:21:07 - train: epoch 036, train_loss: 1.4411
2022-07-03 08:22:19 - eval: epoch: 036, acc1: 69.040%, acc5: 88.922%, test_loss: 1.2498, per_image_load_time: 2.066ms, per_image_inference_time: 0.293ms
2022-07-03 08:22:20 - until epoch: 036, best_acc1: 69.040%
2022-07-03 08:22:20 - epoch 037 lr: 0.010000
2022-07-03 08:22:58 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.2177
2022-07-03 08:23:30 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.5367
2022-07-03 08:24:03 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.3040
2022-07-03 08:24:37 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.2787
2022-07-03 08:25:10 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.4181
2022-07-03 08:25:43 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.4368
2022-07-03 08:26:16 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.4901
2022-07-03 08:26:49 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5358
2022-07-03 08:27:22 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.6109
2022-07-03 08:27:55 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.3928
2022-07-03 08:28:28 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.3998
2022-07-03 08:29:01 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.2745
2022-07-03 08:29:34 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.5381
2022-07-03 08:30:08 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.7800
2022-07-03 08:30:40 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.4399
2022-07-03 08:31:13 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.2829
2022-07-03 08:31:45 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.3229
2022-07-03 08:32:19 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.6665
2022-07-03 08:32:51 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.5291
2022-07-03 08:33:24 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.4584
2022-07-03 08:33:58 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.3500
2022-07-03 08:34:32 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.2490
2022-07-03 08:35:04 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.4572
2022-07-03 08:35:37 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.5118
2022-07-03 08:36:10 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.4181
2022-07-03 08:36:44 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.5714
2022-07-03 08:37:16 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.2874
2022-07-03 08:37:51 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5585
2022-07-03 08:38:24 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.3444
2022-07-03 08:38:56 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.5667
2022-07-03 08:39:30 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.3310
2022-07-03 08:40:03 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.3355
2022-07-03 08:40:37 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.4210
2022-07-03 08:41:10 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.3106
2022-07-03 08:41:43 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.3539
2022-07-03 08:42:16 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.4844
2022-07-03 08:42:50 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.4565
2022-07-03 08:43:23 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.4299
2022-07-03 08:43:56 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.5017
2022-07-03 08:44:30 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.4198
2022-07-03 08:45:03 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.3422
2022-07-03 08:45:36 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.4552
2022-07-03 08:46:09 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.3708
2022-07-03 08:46:42 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.2921
2022-07-03 08:47:15 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.3051
2022-07-03 08:47:48 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.3684
2022-07-03 08:48:22 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.5151
2022-07-03 08:48:54 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.3528
2022-07-03 08:49:28 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.4826
2022-07-03 08:50:00 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.3559
2022-07-03 08:50:01 - train: epoch 037, train_loss: 1.4283
2022-07-03 08:51:14 - eval: epoch: 037, acc1: 69.110%, acc5: 89.050%, test_loss: 1.2476, per_image_load_time: 2.546ms, per_image_inference_time: 0.286ms
2022-07-03 08:51:15 - until epoch: 037, best_acc1: 69.110%
2022-07-03 08:51:15 - epoch 038 lr: 0.010000
2022-07-03 08:51:52 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.4493
2022-07-03 08:52:26 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.3240
2022-07-03 08:52:58 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.3187
2022-07-03 08:53:32 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.2931
2022-07-03 08:54:04 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.3049
2022-07-03 08:54:38 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.7424
2022-07-03 08:55:11 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3358
2022-07-03 08:55:44 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.3147
2022-07-03 08:56:17 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.3147
2022-07-03 08:56:50 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.4022
2022-07-03 08:57:23 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5204
2022-07-03 08:57:56 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.4467
2022-07-03 08:58:30 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.2864
2022-07-03 08:59:03 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.5578
2022-07-03 08:59:36 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.3769
2022-07-03 09:00:09 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.5158
2022-07-03 09:00:42 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.4005
2022-07-03 09:01:15 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.5643
2022-07-03 09:01:49 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.5370
2022-07-03 09:02:22 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.3359
2022-07-03 09:02:56 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.5542
2022-07-03 09:03:28 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.3709
2022-07-03 09:04:02 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.4193
2022-07-03 09:04:36 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.7643
2022-07-03 09:05:09 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.2245
2022-07-03 09:05:42 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.5622
2022-07-03 09:06:16 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.3852
2022-07-03 09:06:49 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.6690
2022-07-03 09:07:23 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.4196
2022-07-03 09:07:55 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.3678
2022-07-03 09:08:29 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.5177
2022-07-03 09:09:02 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.1753
2022-07-03 09:09:35 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.2948
2022-07-03 09:10:08 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.3075
2022-07-03 09:10:41 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.4335
2022-07-03 09:11:15 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.2908
2022-07-03 09:11:48 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.3296
2022-07-03 09:12:20 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.5401
2022-07-03 09:12:55 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.2296
2022-07-03 09:13:28 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.3099
2022-07-03 09:14:02 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.6155
2022-07-03 09:14:34 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.3451
2022-07-03 09:15:09 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.5312
2022-07-03 09:15:41 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.4383
2022-07-03 09:16:15 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.6786
2022-07-03 09:16:47 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.5409
2022-07-03 09:17:22 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.4938
2022-07-03 09:17:54 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.3674
2022-07-03 09:18:28 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.2952
2022-07-03 09:19:00 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.3316
2022-07-03 09:19:00 - train: epoch 038, train_loss: 1.4178
2022-07-03 09:20:14 - eval: epoch: 038, acc1: 68.936%, acc5: 88.924%, test_loss: 1.2533, per_image_load_time: 2.589ms, per_image_inference_time: 0.280ms
2022-07-03 09:20:14 - until epoch: 038, best_acc1: 69.110%
2022-07-03 09:20:14 - epoch 039 lr: 0.010000
2022-07-03 09:20:52 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.4867
2022-07-03 09:21:26 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.4578
2022-07-03 09:21:59 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.2288
2022-07-03 09:22:32 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.4961
2022-07-03 09:23:05 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4253
2022-07-03 09:23:38 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.3727
2022-07-03 09:24:10 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.5253
2022-07-03 09:24:43 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.4009
2022-07-03 09:25:17 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.3014
2022-07-03 09:25:50 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.3521
2022-07-03 09:26:23 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.4836
2022-07-03 09:26:56 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.4792
2022-07-03 09:27:28 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.5422
2022-07-03 09:28:01 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.4820
2022-07-03 09:28:34 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.2433
2022-07-03 09:29:07 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.4937
2022-07-03 09:29:41 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.2870
2022-07-03 09:30:14 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.4163
2022-07-03 09:30:47 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.2139
2022-07-03 09:31:20 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.3876
2022-07-03 09:31:53 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.5998
2022-07-03 09:32:26 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.3848
2022-07-03 09:33:00 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.5904
2022-07-03 09:33:33 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.4392
2022-07-03 09:34:06 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.5066
2022-07-03 09:34:39 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.3483
2022-07-03 09:35:13 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.4811
2022-07-03 09:35:46 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.4959
2022-07-03 09:36:19 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.1303
2022-07-03 09:36:52 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.5548
2022-07-03 09:37:25 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.1689
2022-07-03 09:37:58 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.5559
2022-07-03 09:38:32 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.6694
2022-07-03 09:39:05 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5849
2022-07-03 09:39:38 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.7540
2022-07-03 09:40:11 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.5190
2022-07-03 09:40:45 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.2847
2022-07-03 09:41:18 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.2877
2022-07-03 09:41:50 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.5764
2022-07-03 09:42:23 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.5237
2022-07-03 09:42:57 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.4651
2022-07-03 09:43:30 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.5079
2022-07-03 09:44:05 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.5589
2022-07-03 09:44:38 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.4080
2022-07-03 09:45:12 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.3518
2022-07-03 09:45:44 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.5652
2022-07-03 09:46:17 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.4940
2022-07-03 09:46:50 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.3604
2022-07-03 09:47:23 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.2095
2022-07-03 09:47:55 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.2713
2022-07-03 09:47:56 - train: epoch 039, train_loss: 1.4113
2022-07-03 09:49:09 - eval: epoch: 039, acc1: 68.982%, acc5: 89.146%, test_loss: 1.2478, per_image_load_time: 2.591ms, per_image_inference_time: 0.262ms
2022-07-03 09:49:10 - until epoch: 039, best_acc1: 69.110%
2022-07-03 09:49:10 - epoch 040 lr: 0.010000
2022-07-03 09:49:48 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.6744
2022-07-03 09:50:22 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.4485
2022-07-03 09:50:54 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.5441
2022-07-03 09:51:27 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.4393
2022-07-03 09:52:00 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.1133
2022-07-03 09:52:32 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.2349
2022-07-03 09:53:05 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.3735
2022-07-03 09:53:38 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.3658
2022-07-03 09:54:11 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.2487
2022-07-03 09:54:45 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.0851
2022-07-03 09:55:17 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.3948
2022-07-03 09:55:50 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.3015
2022-07-03 09:56:23 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.2786
2022-07-03 09:56:56 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.4677
2022-07-03 09:57:28 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.4618
2022-07-03 09:58:01 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.5650
2022-07-03 09:58:34 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.3809
2022-07-03 09:59:08 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.1742
2022-07-03 09:59:41 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.5849
2022-07-03 10:00:15 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.4605
2022-07-03 10:00:48 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.2590
2022-07-03 10:01:21 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.3266
2022-07-03 10:01:54 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.4344
2022-07-03 10:02:28 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.3993
2022-07-03 10:03:00 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.5667
2022-07-03 10:03:34 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.3004
2022-07-03 10:04:07 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.5525
2022-07-03 10:04:41 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.4998
2022-07-03 10:05:13 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.5918
2022-07-03 10:05:46 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.3858
2022-07-03 10:06:20 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.5829
2022-07-03 10:06:53 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.4966
2022-07-03 10:07:26 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.3470
2022-07-03 10:08:00 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.3571
2022-07-03 10:08:32 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4608
2022-07-03 10:09:06 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.3033
2022-07-03 10:09:39 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.3833
2022-07-03 10:10:13 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.4142
2022-07-03 10:10:46 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.4294
2022-07-03 10:11:19 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.3636
2022-07-03 10:11:52 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.3558
2022-07-03 10:12:27 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.3206
2022-07-03 10:12:59 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.3812
2022-07-03 10:13:34 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.3366
2022-07-03 10:14:06 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.2067
2022-07-03 10:14:39 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5778
2022-07-03 10:15:14 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.3739
2022-07-03 10:15:46 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.3390
2022-07-03 10:16:20 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.2808
2022-07-03 10:16:51 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.3423
2022-07-03 10:16:52 - train: epoch 040, train_loss: 1.4080
2022-07-03 10:18:05 - eval: epoch: 040, acc1: 69.192%, acc5: 89.068%, test_loss: 1.2508, per_image_load_time: 2.449ms, per_image_inference_time: 0.293ms
2022-07-03 10:18:05 - until epoch: 040, best_acc1: 69.192%
2022-07-03 10:18:05 - epoch 041 lr: 0.010000
2022-07-03 10:18:43 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.5284
2022-07-03 10:19:16 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.5101
2022-07-03 10:19:49 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.3514
2022-07-03 10:20:23 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.5253
2022-07-03 10:20:55 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.1627
2022-07-03 10:21:30 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.4605
2022-07-03 10:22:02 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.4091
2022-07-03 10:22:36 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3333
2022-07-03 10:23:08 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.1702
2022-07-03 10:23:41 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.3576
2022-07-03 10:24:14 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.4049
2022-07-03 10:24:46 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.3160
2022-07-03 10:25:20 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.2681
2022-07-03 10:25:53 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.6078
2022-07-03 10:26:26 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.5123
2022-07-03 10:27:00 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.3693
2022-07-03 10:27:32 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.4814
2022-07-03 10:28:06 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.2900
2022-07-03 10:28:39 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.5108
2022-07-03 10:29:12 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.1994
2022-07-03 10:29:46 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.0083
2022-07-03 10:30:19 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.1075
2022-07-03 10:30:52 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.2384
2022-07-03 10:31:25 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.5082
2022-07-03 10:31:58 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.5946
2022-07-03 10:32:31 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.5487
2022-07-03 10:33:05 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.4321
2022-07-03 10:33:38 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.2626
2022-07-03 10:34:11 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.4915
2022-07-03 10:34:45 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.4566
2022-07-03 10:35:18 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.5171
2022-07-03 10:35:50 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.3502
2022-07-03 10:36:24 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.3962
2022-07-03 10:36:57 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.1433
2022-07-03 10:37:31 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.3187
2022-07-03 10:38:04 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.5709
2022-07-03 10:38:36 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.5153
2022-07-03 10:39:09 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.5241
2022-07-03 10:39:43 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.4557
2022-07-03 10:40:16 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.2329
2022-07-03 10:40:49 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.4628
2022-07-03 10:41:22 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.0222
2022-07-03 10:41:56 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.2692
2022-07-03 10:42:29 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.2781
2022-07-03 10:43:01 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5828
2022-07-03 10:43:36 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.4903
2022-07-03 10:44:08 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.5929
2022-07-03 10:44:42 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.4312
2022-07-03 10:45:15 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.4646
2022-07-03 10:45:47 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.6269
2022-07-03 10:45:48 - train: epoch 041, train_loss: 1.4058
2022-07-03 10:47:02 - eval: epoch: 041, acc1: 68.976%, acc5: 89.082%, test_loss: 1.2521, per_image_load_time: 2.262ms, per_image_inference_time: 0.284ms
2022-07-03 10:47:02 - until epoch: 041, best_acc1: 69.192%
2022-07-03 10:47:02 - epoch 042 lr: 0.010000
2022-07-03 10:47:40 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.0316
2022-07-03 10:48:13 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.4188
2022-07-03 10:48:46 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.3513
2022-07-03 10:49:19 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.1509
2022-07-03 10:49:52 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.4191
2022-07-03 10:50:24 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.2963
2022-07-03 10:50:58 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.3297
2022-07-03 10:51:31 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.3705
2022-07-03 10:52:03 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.4433
2022-07-03 10:52:36 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.4914
2022-07-03 10:53:09 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.2545
2022-07-03 10:53:43 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.2035
2022-07-03 10:54:16 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.3927
2022-07-03 10:54:49 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.3632
2022-07-03 10:55:22 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4000
2022-07-03 10:55:56 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.4549
2022-07-03 10:56:28 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.2605
2022-07-03 10:57:01 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.3780
2022-07-03 10:57:35 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.5347
2022-07-03 10:58:08 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.2584
2022-07-03 10:58:41 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.2828
2022-07-03 10:59:14 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.2068
2022-07-03 10:59:48 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.4102
2022-07-03 11:00:21 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.3969
2022-07-03 11:00:55 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.5823
2022-07-03 11:01:29 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.4828
2022-07-03 11:02:01 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.3536
2022-07-03 11:02:35 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.2733
2022-07-03 11:03:07 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.4037
2022-07-03 11:03:42 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.3324
2022-07-03 11:04:15 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.4775
2022-07-03 11:04:47 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.4973
2022-07-03 11:05:21 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.4099
2022-07-03 11:05:54 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.2719
2022-07-03 11:06:28 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.4872
2022-07-03 11:07:00 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.5701
2022-07-03 11:07:33 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.2419
2022-07-03 11:08:07 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.1994
2022-07-03 11:08:40 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.5420
2022-07-03 11:09:14 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.3507
2022-07-03 11:09:47 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.5404
2022-07-03 11:10:20 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.5223
2022-07-03 11:10:53 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.2139
2022-07-03 11:11:27 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.2659
2022-07-03 11:12:00 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.1540
2022-07-03 11:12:32 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.3937
2022-07-03 11:13:05 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.3493
2022-07-03 11:13:39 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.4962
2022-07-03 11:14:12 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.4846
2022-07-03 11:14:44 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.4660
2022-07-03 11:14:45 - train: epoch 042, train_loss: 1.4025
2022-07-03 11:15:59 - eval: epoch: 042, acc1: 68.760%, acc5: 89.008%, test_loss: 1.2625, per_image_load_time: 2.576ms, per_image_inference_time: 0.284ms
2022-07-03 11:15:59 - until epoch: 042, best_acc1: 69.192%
2022-07-03 11:15:59 - epoch 043 lr: 0.010000
2022-07-03 11:16:37 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.5453
2022-07-03 11:17:10 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.3976
2022-07-03 11:17:43 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.1072
2022-07-03 11:18:16 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.2692
2022-07-03 11:18:49 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.4583
2022-07-03 11:19:22 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.4565
2022-07-03 11:19:55 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.4551
2022-07-03 11:20:28 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.4883
2022-07-03 11:21:01 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.3949
2022-07-03 11:21:34 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.3565
2022-07-03 11:22:07 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.3802
2022-07-03 11:22:41 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.3612
2022-07-03 11:23:13 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.3180
2022-07-03 11:23:47 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.2829
2022-07-03 11:24:21 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.2135
2022-07-03 11:24:54 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.2819
2022-07-03 11:25:27 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.6277
2022-07-03 11:26:01 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.5538
2022-07-03 11:26:34 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.3298
2022-07-03 11:27:07 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.2817
2022-07-03 11:27:40 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.3337
2022-07-03 11:28:13 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.6623
2022-07-03 11:28:47 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.4986
2022-07-03 11:29:19 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.1088
2022-07-03 11:29:53 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.5240
2022-07-03 11:30:26 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.2654
2022-07-03 11:30:59 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.3961
2022-07-03 11:31:33 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.2862
2022-07-03 11:32:06 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.3141
2022-07-03 11:32:38 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.5690
2022-07-03 11:33:12 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.4914
2022-07-03 11:33:46 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.4559
2022-07-03 11:34:18 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.5537
2022-07-03 11:34:51 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.4090
2022-07-03 11:35:25 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.5274
2022-07-03 11:35:59 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.4230
2022-07-03 11:36:32 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.2550
2022-07-03 11:37:05 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.5237
2022-07-03 11:37:38 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.2933
2022-07-03 11:38:11 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.4990
2022-07-03 11:38:45 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.5676
2022-07-03 11:39:18 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.4145
2022-07-03 11:39:52 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.3664
2022-07-03 11:40:25 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.4073
2022-07-03 11:40:59 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.5029
2022-07-03 11:41:31 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.4477
2022-07-03 11:42:05 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.4279
2022-07-03 11:42:38 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.3621
2022-07-03 11:43:12 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.3172
2022-07-03 11:43:43 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.3854
2022-07-03 11:43:44 - train: epoch 043, train_loss: 1.4022
2022-07-03 11:44:57 - eval: epoch: 043, acc1: 68.552%, acc5: 88.930%, test_loss: 1.2690, per_image_load_time: 1.965ms, per_image_inference_time: 0.255ms
2022-07-03 11:44:57 - until epoch: 043, best_acc1: 69.192%
2022-07-03 11:44:57 - epoch 044 lr: 0.010000
2022-07-03 11:45:36 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.3897
2022-07-03 11:46:09 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.3499
2022-07-03 11:46:42 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.3862
2022-07-03 11:47:16 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.1867
2022-07-03 11:47:48 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.4458
2022-07-03 11:48:21 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.2438
2022-07-03 11:48:54 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.4689
2022-07-03 11:49:28 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.4855
2022-07-03 11:50:00 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.3017
2022-07-03 11:50:34 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.5698
2022-07-03 11:51:06 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.3256
2022-07-03 11:51:39 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.3179
2022-07-03 11:52:13 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.5073
2022-07-03 11:52:47 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.2617
2022-07-03 11:53:20 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.4063
2022-07-03 11:53:53 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.4282
2022-07-03 11:54:27 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.3588
2022-07-03 11:55:00 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.2616
2022-07-03 11:55:32 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.4296
2022-07-03 11:56:06 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.2556
2022-07-03 11:56:39 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.4974
2022-07-03 11:57:12 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.4709
2022-07-03 11:57:45 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.5108
2022-07-03 11:58:19 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.5774
2022-07-03 11:58:52 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.4774
2022-07-03 11:59:25 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.4694
2022-07-03 11:59:58 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.4198
2022-07-03 12:00:32 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.3081
2022-07-03 12:01:06 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.2005
2022-07-03 12:01:39 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.2268
2022-07-03 12:02:12 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.4205
2022-07-03 12:02:46 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.4265
2022-07-03 12:03:18 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.4732
2022-07-03 12:03:52 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.5553
2022-07-03 12:04:26 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.3341
2022-07-03 12:04:59 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.5005
2022-07-03 12:05:31 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.2595
2022-07-03 12:06:07 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.3718
2022-07-03 12:06:38 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.3296
2022-07-03 12:07:12 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.4413
2022-07-03 12:07:45 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.3756
2022-07-03 12:08:19 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.6125
2022-07-03 12:08:51 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.4894
2022-07-03 12:09:25 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.4407
2022-07-03 12:09:58 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.4391
2022-07-03 12:10:31 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.4297
2022-07-03 12:11:05 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.4808
2022-07-03 12:11:38 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.4948
2022-07-03 12:12:11 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.3508
2022-07-03 12:12:43 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.4603
2022-07-03 12:12:43 - train: epoch 044, train_loss: 1.4017
2022-07-03 12:13:56 - eval: epoch: 044, acc1: 68.846%, acc5: 88.946%, test_loss: 1.2613, per_image_load_time: 2.585ms, per_image_inference_time: 0.272ms
2022-07-03 12:13:57 - until epoch: 044, best_acc1: 69.192%
2022-07-03 12:13:57 - epoch 045 lr: 0.010000
2022-07-03 12:14:35 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.2519
2022-07-03 12:15:08 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.3316
2022-07-03 12:15:41 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.3767
2022-07-03 12:16:14 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.4125
2022-07-03 12:16:47 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.5505
2022-07-03 12:17:19 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.8074
2022-07-03 12:17:53 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.1519
2022-07-03 12:18:26 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.2999
2022-07-03 12:19:00 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.2566
2022-07-03 12:19:32 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.3880
2022-07-03 12:20:06 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.5071
2022-07-03 12:20:38 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.3669
2022-07-03 12:21:11 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.2841
2022-07-03 12:21:45 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.5315
2022-07-03 12:22:18 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.2438
2022-07-03 12:22:51 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.2512
2022-07-03 12:23:25 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.3934
2022-07-03 12:23:57 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.4638
2022-07-03 12:24:31 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.2766
2022-07-03 12:25:04 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.5245
2022-07-03 12:25:38 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.4078
2022-07-03 12:26:10 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.3168
2022-07-03 12:26:44 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.4273
2022-07-03 12:27:17 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.4183
2022-07-03 12:27:50 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.4731
2022-07-03 12:28:23 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.3248
2022-07-03 12:28:57 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.2606
2022-07-03 12:29:29 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.3788
2022-07-03 12:30:02 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.5157
2022-07-03 12:30:35 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.5505
2022-07-03 12:31:09 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.3249
2022-07-03 12:31:43 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.7217
2022-07-03 12:32:16 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.3723
2022-07-03 12:32:50 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.2107
2022-07-03 12:33:22 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.5094
2022-07-03 12:33:56 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.5523
2022-07-03 12:34:29 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.3787
2022-07-03 12:35:01 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.3596
2022-07-03 12:35:34 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.4524
2022-07-03 12:36:07 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.4835
2022-07-03 12:36:41 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.4103
2022-07-03 12:37:14 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.4044
2022-07-03 12:37:47 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.5645
2022-07-03 12:38:21 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.3531
2022-07-03 12:38:54 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.3253
2022-07-03 12:39:27 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.4259
2022-07-03 12:40:01 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.3409
2022-07-03 12:40:34 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.2294
2022-07-03 12:41:07 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.4443
2022-07-03 12:41:39 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.3538
2022-07-03 12:41:40 - train: epoch 045, train_loss: 1.3993
2022-07-03 12:42:53 - eval: epoch: 045, acc1: 68.038%, acc5: 88.638%, test_loss: 1.2862, per_image_load_time: 2.021ms, per_image_inference_time: 0.288ms
2022-07-03 12:42:53 - until epoch: 045, best_acc1: 69.192%
2022-07-03 12:42:53 - epoch 046 lr: 0.010000
2022-07-03 12:43:31 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.2813
2022-07-03 12:44:05 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.2193
2022-07-03 12:44:38 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.4143
2022-07-03 12:45:11 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.2736
2022-07-03 12:45:44 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.2905
2022-07-03 12:46:17 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.3680
2022-07-03 12:46:49 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.1996
2022-07-03 12:47:23 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.5150
2022-07-03 12:47:56 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.4183
2022-07-03 12:48:29 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.2001
2022-07-03 12:49:02 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.2929
2022-07-03 12:49:36 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.2864
2022-07-03 12:50:08 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.4451
2022-07-03 12:50:41 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.6246
2022-07-03 12:51:15 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.2636
2022-07-03 12:51:48 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.5640
2022-07-03 12:52:21 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.4936
2022-07-03 12:52:54 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.4292
2022-07-03 12:53:27 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.3278
2022-07-03 12:54:00 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.3141
2022-07-03 12:54:34 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.6658
2022-07-03 12:55:07 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.1880
2022-07-03 12:55:40 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.3139
2022-07-03 12:56:13 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.4258
2022-07-03 12:56:47 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.2708
2022-07-03 12:57:19 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.4091
2022-07-03 12:57:53 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.1665
2022-07-03 12:58:26 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.2319
2022-07-03 12:58:59 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.3827
2022-07-03 12:59:33 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.2481
2022-07-03 13:00:05 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.1980
2022-07-03 13:00:39 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.4911
2022-07-03 13:01:12 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.3532
2022-07-03 13:01:45 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.5817
2022-07-03 13:02:19 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.3890
2022-07-03 13:02:51 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.1986
2022-07-03 13:03:25 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.3210
2022-07-03 13:03:58 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.4202
2022-07-03 13:04:32 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.4566
2022-07-03 13:05:05 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.4943
2022-07-03 13:05:38 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.4864
2022-07-03 13:06:11 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.1312
2022-07-03 13:06:45 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.5455
2022-07-03 13:07:18 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.6425
2022-07-03 13:07:51 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.3249
2022-07-03 13:08:24 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.4996
2022-07-03 13:08:58 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.3546
2022-07-03 13:09:31 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.3447
2022-07-03 13:10:04 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.6306
2022-07-03 13:10:36 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.4453
2022-07-03 13:10:37 - train: epoch 046, train_loss: 1.3996
2022-07-03 13:11:50 - eval: epoch: 046, acc1: 68.642%, acc5: 88.816%, test_loss: 1.2818, per_image_load_time: 2.419ms, per_image_inference_time: 0.274ms
2022-07-03 13:11:51 - until epoch: 046, best_acc1: 69.192%
2022-07-03 13:11:51 - epoch 047 lr: 0.010000
2022-07-03 13:12:29 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.4593
2022-07-03 13:13:02 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.3557
2022-07-03 13:13:36 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.2250
2022-07-03 13:14:08 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.2501
2022-07-03 13:14:41 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.2019
2022-07-03 13:15:14 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.4273
2022-07-03 13:15:47 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.2435
2022-07-03 13:16:20 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.2635
2022-07-03 13:16:53 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.5265
2022-07-03 13:17:26 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.4016
2022-07-03 13:17:59 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.4862
2022-07-03 13:18:31 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.3214
2022-07-03 13:19:05 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.3947
2022-07-03 13:19:39 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.3822
2022-07-03 13:20:12 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.4522
2022-07-03 13:20:45 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.3434
2022-07-03 13:21:18 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.3069
2022-07-03 13:21:52 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.4174
2022-07-03 13:22:24 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.2193
2022-07-03 13:22:57 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.3158
2022-07-03 13:23:31 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.4118
2022-07-03 13:24:04 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.5428
2022-07-03 13:24:37 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.4017
2022-07-03 13:25:10 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.2409
2022-07-03 13:25:44 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.4897
2022-07-03 13:26:17 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.5291
2022-07-03 13:26:50 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.3137
2022-07-03 13:27:23 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.3430
2022-07-03 13:27:55 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.3394
2022-07-03 13:28:29 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.4275
2022-07-03 13:29:02 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.3372
2022-07-03 13:29:34 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.5296
2022-07-03 13:30:08 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.2858
2022-07-03 13:30:41 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.2848
2022-07-03 13:31:14 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.4902
2022-07-03 13:31:48 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.3603
2022-07-03 13:32:21 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.3738
2022-07-03 13:32:55 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.4313
2022-07-03 13:33:28 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.4359
2022-07-03 13:34:01 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.4558
2022-07-03 13:34:35 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.3305
2022-07-03 13:35:07 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.4112
2022-07-03 13:35:40 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.2225
2022-07-03 13:36:14 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.3716
2022-07-03 13:36:47 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.5509
2022-07-03 13:37:20 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.3446
2022-07-03 13:37:53 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.6645
2022-07-03 13:38:26 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.3544
2022-07-03 13:38:59 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.3400
2022-07-03 13:39:31 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.4784
2022-07-03 13:39:32 - train: epoch 047, train_loss: 1.4003
2022-07-03 13:40:45 - eval: epoch: 047, acc1: 68.592%, acc5: 88.752%, test_loss: 1.2750, per_image_load_time: 2.514ms, per_image_inference_time: 0.267ms
2022-07-03 13:40:46 - until epoch: 047, best_acc1: 69.192%
2022-07-03 13:40:46 - epoch 048 lr: 0.010000
2022-07-03 13:41:24 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.4821
2022-07-03 13:41:57 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.6789
2022-07-03 13:42:30 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.3975
2022-07-03 13:43:03 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.4095
2022-07-03 13:43:36 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.3784
2022-07-03 13:44:09 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.5118
2022-07-03 13:44:42 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.4606
2022-07-03 13:45:15 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.2930
2022-07-03 13:45:48 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.4344
2022-07-03 13:46:21 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.3147
2022-07-03 13:46:55 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.4026
2022-07-03 13:47:27 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.4468
2022-07-03 13:48:00 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.1749
2022-07-03 13:48:33 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.4922
2022-07-03 13:49:06 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.3586
2022-07-03 13:49:39 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.3640
2022-07-03 13:50:13 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.3247
2022-07-03 13:50:46 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.2266
2022-07-03 13:51:19 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.4846
2022-07-03 13:51:52 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.5294
2022-07-03 13:52:25 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.5329
2022-07-03 13:52:58 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.5711
2022-07-03 13:53:31 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.3095
2022-07-03 13:54:05 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.4688
2022-07-03 13:54:37 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.4896
2022-07-03 13:55:10 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.3832
2022-07-03 13:55:43 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.6089
2022-07-03 13:56:17 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.3580
2022-07-03 13:56:50 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.5306
2022-07-03 13:57:22 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.2833
2022-07-03 13:57:56 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.4261
2022-07-03 13:58:29 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.1162
2022-07-03 13:59:03 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.6754
2022-07-03 13:59:36 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.3754
2022-07-03 14:00:09 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.6578
2022-07-03 14:00:43 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.5512
2022-07-03 14:01:17 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.6430
2022-07-03 14:01:50 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.3704
2022-07-03 14:02:24 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.6368
2022-07-03 14:02:57 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.2932
2022-07-03 14:03:30 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.5088
2022-07-03 14:04:03 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.4566
2022-07-03 14:04:36 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.3397
2022-07-03 14:05:10 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.3778
2022-07-03 14:05:43 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.3272
2022-07-03 14:06:17 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.3894
2022-07-03 14:06:50 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.3515
2022-07-03 14:07:23 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.6239
2022-07-03 14:07:56 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.5539
2022-07-03 14:08:28 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.3599
2022-07-03 14:08:29 - train: epoch 048, train_loss: 1.3977
2022-07-03 14:09:42 - eval: epoch: 048, acc1: 68.358%, acc5: 88.734%, test_loss: 1.2773, per_image_load_time: 2.570ms, per_image_inference_time: 0.267ms
2022-07-03 14:09:42 - until epoch: 048, best_acc1: 69.192%
2022-07-03 14:09:42 - epoch 049 lr: 0.010000
2022-07-03 14:10:21 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.4394
2022-07-03 14:10:54 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.2808
2022-07-03 14:11:27 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.4098
2022-07-03 14:12:00 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.4036
2022-07-03 14:12:33 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.3920
2022-07-03 14:13:05 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.3513
2022-07-03 14:13:39 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.3164
2022-07-03 14:14:11 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.5934
2022-07-03 14:14:45 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.2703
2022-07-03 14:15:17 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.2424
2022-07-03 14:15:51 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.4567
2022-07-03 14:16:23 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.2908
2022-07-03 14:16:56 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.5731
2022-07-03 14:17:29 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.5058
2022-07-03 14:18:03 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.2570
2022-07-03 14:18:36 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.4226
2022-07-03 14:19:09 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.3972
2022-07-03 14:19:43 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.3067
2022-07-03 14:20:15 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.3179
2022-07-03 14:20:49 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.4815
2022-07-03 14:21:21 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.3696
2022-07-03 14:21:55 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.4419
2022-07-03 14:22:27 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.3535
2022-07-03 14:23:01 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.3041
2022-07-03 14:23:34 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.4126
2022-07-03 14:24:07 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.3836
2022-07-03 14:24:40 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.3239
2022-07-03 14:25:13 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.3947
2022-07-03 14:25:46 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.5053
2022-07-03 14:26:20 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.4283
2022-07-03 14:26:53 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.4677
2022-07-03 14:27:26 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.5987
2022-07-03 14:27:59 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.3636
2022-07-03 14:28:32 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.4868
2022-07-03 14:29:06 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.3177
2022-07-03 14:29:39 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.5945
2022-07-03 14:30:12 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.3416
2022-07-03 14:30:45 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.4066
2022-07-03 14:31:19 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.4811
2022-07-03 14:31:52 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.3759
2022-07-03 14:32:25 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.4256
2022-07-03 14:32:58 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.2677
2022-07-03 14:33:32 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.6397
2022-07-03 14:34:05 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.4091
2022-07-03 14:34:38 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.2293
2022-07-03 14:35:12 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.4140
2022-07-03 14:35:45 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.5888
2022-07-03 14:36:18 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.3224
2022-07-03 14:36:52 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.4992
2022-07-03 14:37:23 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.3968
2022-07-03 14:37:24 - train: epoch 049, train_loss: 1.3957
2022-07-03 14:38:38 - eval: epoch: 049, acc1: 68.572%, acc5: 88.708%, test_loss: 1.2798, per_image_load_time: 1.959ms, per_image_inference_time: 0.271ms
2022-07-03 14:38:38 - until epoch: 049, best_acc1: 69.192%
2022-07-03 14:38:38 - epoch 050 lr: 0.010000
2022-07-03 14:39:16 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.4318
2022-07-03 14:39:48 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.4037
2022-07-03 14:40:22 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.4011
2022-07-03 14:40:54 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.3322
2022-07-03 14:41:28 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.3643
2022-07-03 14:42:01 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.6670
2022-07-03 14:42:35 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.5198
2022-07-03 14:43:07 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.1447
2022-07-03 14:43:40 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.3173
2022-07-03 14:44:13 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.4488
2022-07-03 14:44:46 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.4038
2022-07-03 14:45:19 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.4099
2022-07-03 14:45:51 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.3179
2022-07-03 14:46:25 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.3632
2022-07-03 14:46:58 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.3616
2022-07-03 14:47:31 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.4444
2022-07-03 14:48:04 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.2208
2022-07-03 14:48:38 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.5067
2022-07-03 14:49:11 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.4104
2022-07-03 14:49:45 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.4668
2022-07-03 14:50:18 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.2145
2022-07-03 14:50:51 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.4137
2022-07-03 14:51:24 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.3523
2022-07-03 14:51:58 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.6990
2022-07-03 14:52:32 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.3752
2022-07-03 14:53:05 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.3240
2022-07-03 14:53:37 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.3425
2022-07-03 14:54:11 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.4013
2022-07-03 14:54:45 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.6781
2022-07-03 14:55:18 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.5619
2022-07-03 14:55:52 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.5332
2022-07-03 14:56:25 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.6489
2022-07-03 14:56:58 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.2856
2022-07-03 14:57:32 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.4318
2022-07-03 14:58:06 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.3338
2022-07-03 14:58:38 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.2664
2022-07-03 14:59:11 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.6811
2022-07-03 14:59:44 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.2332
2022-07-03 15:00:17 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.1927
2022-07-03 15:00:51 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.5268
2022-07-03 15:01:24 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.4163
2022-07-03 15:01:58 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.4228
2022-07-03 15:02:31 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.2364
2022-07-03 15:03:03 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.4303
2022-07-03 15:03:37 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.4693
2022-07-03 15:04:10 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.4431
2022-07-03 15:04:43 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.4651
2022-07-03 15:05:16 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.4011
2022-07-03 15:05:50 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.2597
2022-07-03 15:06:21 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.5255
2022-07-03 15:06:22 - train: epoch 050, train_loss: 1.3953
2022-07-03 15:07:36 - eval: epoch: 050, acc1: 68.124%, acc5: 88.480%, test_loss: 1.2995, per_image_load_time: 2.387ms, per_image_inference_time: 0.262ms
2022-07-03 15:07:36 - until epoch: 050, best_acc1: 69.192%
2022-07-03 15:07:36 - epoch 051 lr: 0.010000
2022-07-03 15:08:15 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.4367
2022-07-03 15:08:48 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.7484
2022-07-03 15:09:20 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.4359
2022-07-03 15:09:54 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.1700
2022-07-03 15:10:26 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.4092
2022-07-03 15:10:59 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.3365
2022-07-03 15:11:32 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.3351
2022-07-03 15:12:05 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.5956
2022-07-03 15:12:38 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.3105
2022-07-03 15:13:11 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.6520
2022-07-03 15:13:44 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.4888
2022-07-03 15:14:17 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.2375
2022-07-03 15:14:50 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.2171
2022-07-03 15:15:24 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.3881
2022-07-03 15:15:56 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.3613
2022-07-03 15:16:30 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.2826
2022-07-03 15:17:02 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.3831
2022-07-03 15:17:35 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.4541
2022-07-03 15:18:08 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.3938
2022-07-03 15:18:43 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.3783
2022-07-03 15:19:15 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.3151
2022-07-03 15:19:48 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.3612
2022-07-03 15:20:21 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.4867
2022-07-03 15:20:54 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.3418
2022-07-03 15:21:27 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.3407
2022-07-03 15:22:01 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.4228
2022-07-03 15:22:34 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.5274
2022-07-03 15:23:08 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.3211
2022-07-03 15:23:41 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.2376
2022-07-03 15:24:14 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.3894
2022-07-03 15:24:47 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.4073
2022-07-03 15:25:21 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.3223
2022-07-03 15:25:55 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.4370
2022-07-03 15:26:28 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.5425
2022-07-03 15:27:00 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.3595
2022-07-03 15:27:34 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.3966
2022-07-03 15:28:07 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.7393
2022-07-03 15:28:40 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.4572
2022-07-03 15:29:13 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.4952
2022-07-03 15:29:47 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.4976
2022-07-03 15:30:21 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.4940
2022-07-03 15:30:53 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.5119
2022-07-03 15:31:27 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.2166
2022-07-03 15:32:00 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.5729
2022-07-03 15:32:32 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.1752
2022-07-03 15:33:07 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.5681
2022-07-03 15:33:40 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.0996
2022-07-03 15:34:13 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.4544
2022-07-03 15:34:47 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.3886
2022-07-03 15:35:18 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.4513
2022-07-03 15:35:19 - train: epoch 051, train_loss: 1.3935
2022-07-03 15:36:32 - eval: epoch: 051, acc1: 68.646%, acc5: 88.648%, test_loss: 1.2687, per_image_load_time: 2.476ms, per_image_inference_time: 0.290ms
2022-07-03 15:36:33 - until epoch: 051, best_acc1: 69.192%
2022-07-03 15:36:33 - epoch 052 lr: 0.010000
2022-07-03 15:37:11 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.4805
2022-07-03 15:37:44 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.4735
2022-07-03 15:38:17 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.4208
2022-07-03 15:38:49 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.4186
2022-07-03 15:39:23 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.3773
2022-07-03 15:39:55 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.4005
2022-07-03 15:40:28 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.5861
2022-07-03 15:41:01 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.1818
2022-07-03 15:41:34 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.4413
2022-07-03 15:42:07 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.4702
2022-07-03 15:42:40 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.4336
2022-07-03 15:43:13 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.2311
2022-07-03 15:43:45 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.2647
2022-07-03 15:44:19 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.6133
2022-07-03 15:44:52 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.4385
2022-07-03 15:45:25 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.2584
2022-07-03 15:45:58 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.2551
2022-07-03 15:46:31 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.4175
2022-07-03 15:47:04 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.4102
2022-07-03 15:47:38 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.4323
2022-07-03 15:48:10 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.4172
2022-07-03 15:48:44 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.3938
2022-07-03 15:49:17 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.4108
2022-07-03 15:49:50 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.1689
2022-07-03 15:50:23 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.3191
2022-07-03 15:50:57 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.2319
2022-07-03 15:51:29 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.4058
2022-07-03 15:52:02 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.5424
2022-07-03 15:52:36 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.2943
2022-07-03 15:53:09 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.2454
2022-07-03 15:53:42 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.5078
2022-07-03 15:54:16 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.4830
2022-07-03 15:54:50 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.4897
2022-07-03 15:55:23 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.4309
2022-07-03 15:55:57 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.3229
2022-07-03 15:56:30 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.6258
2022-07-03 15:57:03 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.4565
2022-07-03 15:57:36 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.5060
2022-07-03 15:58:10 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.2590
2022-07-03 15:58:43 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.6053
2022-07-03 15:59:16 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.3000
2022-07-03 15:59:50 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.2795
2022-07-03 16:00:23 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.3843
2022-07-03 16:00:57 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.4365
2022-07-03 16:01:29 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.4346
2022-07-03 16:02:03 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.3309
2022-07-03 16:02:35 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.4397
2022-07-03 16:03:10 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.3731
2022-07-03 16:03:43 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.2877
2022-07-03 16:04:14 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.2920
2022-07-03 16:04:15 - train: epoch 052, train_loss: 1.3896
2022-07-03 16:05:29 - eval: epoch: 052, acc1: 68.730%, acc5: 88.676%, test_loss: 1.2780, per_image_load_time: 2.593ms, per_image_inference_time: 0.278ms
2022-07-03 16:05:29 - until epoch: 052, best_acc1: 69.192%
2022-07-03 16:05:29 - epoch 053 lr: 0.010000
2022-07-03 16:06:07 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.3130
2022-07-03 16:06:40 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.2781
2022-07-03 16:07:12 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.3497
2022-07-03 16:07:45 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.5012
2022-07-03 16:08:18 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.2559
2022-07-03 16:08:51 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.2623
2022-07-03 16:09:23 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.3234
2022-07-03 16:09:57 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.2282
2022-07-03 16:10:29 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.3021
2022-07-03 16:11:03 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.4419
2022-07-03 16:11:36 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.3795
2022-07-03 16:12:10 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.1200
2022-07-03 16:12:42 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.4229
2022-07-03 16:13:15 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.3229
2022-07-03 16:13:48 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.3236
2022-07-03 16:14:22 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.5627
2022-07-03 16:14:54 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.5975
2022-07-03 16:15:27 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.5156
2022-07-03 16:16:01 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.1576
2022-07-03 16:16:34 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.3302
2022-07-03 16:17:07 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.6434
2022-07-03 16:17:41 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.2983
2022-07-03 16:18:13 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.2860
2022-07-03 16:18:48 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.1459
2022-07-03 16:19:21 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.5323
2022-07-03 16:19:54 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.6587
2022-07-03 16:20:26 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.4951
2022-07-03 16:20:59 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.4384
2022-07-03 16:21:33 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.2354
2022-07-03 16:22:05 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.3224
2022-07-03 16:22:39 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.5017
2022-07-03 16:23:12 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.6739
2022-07-03 16:23:45 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.3603
2022-07-03 16:24:19 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.2823
2022-07-03 16:24:52 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.5018
2022-07-03 16:25:25 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.5350
2022-07-03 16:25:58 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.5276
2022-07-03 16:26:31 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.4486
2022-07-03 16:27:04 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.7198
2022-07-03 16:27:38 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.3682
2022-07-03 16:28:10 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.4562
2022-07-03 16:28:44 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.4899
2022-07-03 16:29:17 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.6811
2022-07-03 16:29:51 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.5519
2022-07-03 16:30:24 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.4718
2022-07-03 16:30:57 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.3807
2022-07-03 16:31:30 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.4443
2022-07-03 16:32:03 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.5271
2022-07-03 16:32:36 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.3064
2022-07-03 16:33:08 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.3018
2022-07-03 16:33:09 - train: epoch 053, train_loss: 1.3892
2022-07-03 16:34:23 - eval: epoch: 053, acc1: 68.094%, acc5: 88.658%, test_loss: 1.2861, per_image_load_time: 2.479ms, per_image_inference_time: 0.286ms
2022-07-03 16:34:23 - until epoch: 053, best_acc1: 69.192%
2022-07-03 16:34:23 - epoch 054 lr: 0.010000
2022-07-03 16:35:01 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.2461
2022-07-03 16:35:34 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.3633
2022-07-03 16:36:07 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.2788
2022-07-03 16:36:40 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.3595
2022-07-03 16:37:13 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.2592
2022-07-03 16:37:45 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.4444
2022-07-03 16:38:19 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.5730
2022-07-03 16:38:51 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.4537
2022-07-03 16:39:25 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.0602
2022-07-03 16:39:58 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.1885
2022-07-03 16:40:31 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.2269
2022-07-03 16:41:04 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.4515
2022-07-03 16:41:37 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.4010
2022-07-03 16:42:11 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.3746
2022-07-03 16:42:44 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.2378
2022-07-03 16:43:17 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.4255
2022-07-03 16:43:50 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.4822
2022-07-03 16:44:23 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.1938
2022-07-03 16:44:56 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.4981
2022-07-03 16:45:29 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.3286
2022-07-03 16:46:03 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.3184
2022-07-03 16:46:36 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.3390
2022-07-03 16:47:09 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.0992
2022-07-03 16:47:42 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.4458
2022-07-03 16:48:15 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.4992
2022-07-03 16:48:48 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.3116
2022-07-03 16:49:22 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.3815
2022-07-03 16:49:56 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.3924
2022-07-03 16:50:28 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.4043
2022-07-03 16:51:01 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.4990
2022-07-03 16:51:35 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.4822
2022-07-03 16:52:08 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.6504
2022-07-03 16:52:41 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.2933
2022-07-03 16:53:14 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.3540
2022-07-03 16:53:47 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.3132
2022-07-03 16:54:21 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.5210
2022-07-03 16:54:54 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.4388
2022-07-03 16:55:27 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.4008
2022-07-03 16:56:01 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.2982
2022-07-03 16:56:34 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.3126
2022-07-03 16:57:07 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.4679
2022-07-03 16:57:40 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.3714
2022-07-03 16:58:14 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.4817
2022-07-03 16:58:46 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.2506
2022-07-03 16:59:20 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.3212
2022-07-03 16:59:53 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.5342
2022-07-03 17:00:27 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.7484
2022-07-03 17:01:00 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.3733
2022-07-03 17:01:32 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.2504
2022-07-03 17:02:04 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.2273
2022-07-03 17:02:05 - train: epoch 054, train_loss: 1.3867
2022-07-03 17:03:19 - eval: epoch: 054, acc1: 67.732%, acc5: 88.324%, test_loss: 1.3076, per_image_load_time: 2.569ms, per_image_inference_time: 0.263ms
2022-07-03 17:03:19 - until epoch: 054, best_acc1: 69.192%
2022-07-03 17:03:19 - epoch 055 lr: 0.010000
2022-07-03 17:03:58 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.1299
2022-07-03 17:04:30 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.2642
2022-07-03 17:05:04 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.1994
2022-07-03 17:05:37 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.3214
2022-07-03 17:06:08 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.2092
2022-07-03 17:06:42 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.4819
2022-07-03 17:07:15 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.3074
2022-07-03 17:07:47 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.2276
2022-07-03 17:08:21 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.5633
2022-07-03 17:08:54 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.3333
2022-07-03 17:09:26 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.3798
2022-07-03 17:09:59 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.4610
2022-07-03 17:10:32 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.5634
2022-07-03 17:11:05 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.2241
2022-07-03 17:11:38 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.2982
2022-07-03 17:12:11 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.5970
2022-07-03 17:12:44 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.3515
2022-07-03 17:13:17 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.2260
2022-07-03 17:13:51 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.5014
2022-07-03 17:14:24 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.3643
2022-07-03 17:14:57 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.3748
2022-07-03 17:15:30 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.4348
2022-07-03 17:16:04 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.2113
2022-07-03 17:16:37 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.2155
2022-07-03 17:17:11 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.2225
2022-07-03 17:17:43 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.3826
2022-07-03 17:18:17 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.2779
2022-07-03 17:18:50 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.4150
2022-07-03 17:19:23 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.1876
2022-07-03 17:19:56 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.4388
2022-07-03 17:20:30 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.4536
2022-07-03 17:21:02 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.3655
2022-07-03 17:21:36 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.1878
2022-07-03 17:22:09 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.4639
2022-07-03 17:22:42 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.3984
2022-07-03 17:23:15 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.5483
2022-07-03 17:23:48 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.2978
2022-07-03 17:24:22 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.3499
2022-07-03 17:24:54 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.5724
2022-07-03 17:25:27 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.4021
2022-07-03 17:26:01 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.4105
2022-07-03 17:26:35 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.4779
2022-07-03 17:27:09 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.4403
2022-07-03 17:27:41 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.5064
2022-07-03 17:28:15 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.2024
2022-07-03 17:28:48 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.5511
2022-07-03 17:29:22 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.3216
2022-07-03 17:29:55 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.3870
2022-07-03 17:30:29 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.2723
2022-07-03 17:31:01 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.5398
2022-07-03 17:31:01 - train: epoch 055, train_loss: 1.3822
2022-07-03 17:32:15 - eval: epoch: 055, acc1: 68.806%, acc5: 88.956%, test_loss: 1.2622, per_image_load_time: 2.189ms, per_image_inference_time: 0.280ms
2022-07-03 17:32:15 - until epoch: 055, best_acc1: 69.192%
2022-07-03 17:32:15 - epoch 056 lr: 0.010000
2022-07-03 17:32:53 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.3853
2022-07-03 17:33:26 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.5004
2022-07-03 17:33:59 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.3468
2022-07-03 17:34:31 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.4621
2022-07-03 17:35:05 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.2876
2022-07-03 17:35:38 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.5277
2022-07-03 17:36:11 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.3244
2022-07-03 17:36:43 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.4104
2022-07-03 17:37:17 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.5307
2022-07-03 17:37:49 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.2998
2022-07-03 17:38:23 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.2942
2022-07-03 17:38:55 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.3830
2022-07-03 17:39:28 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.6037
2022-07-03 17:40:01 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.2882
2022-07-03 17:40:35 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.5899
2022-07-03 17:41:07 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.3114
2022-07-03 17:41:41 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.2802
2022-07-03 17:42:14 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.5146
2022-07-03 17:42:47 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.5495
2022-07-03 17:43:20 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.3919
2022-07-03 17:43:52 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.5439
2022-07-03 17:44:26 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.5612
2022-07-03 17:44:59 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.4336
2022-07-03 17:45:32 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.2450
2022-07-03 17:46:06 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.7169
2022-07-03 17:46:40 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.3585
2022-07-03 17:47:13 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.1030
2022-07-03 17:47:45 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.3397
2022-07-03 17:48:20 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.3671
2022-07-03 17:48:53 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.4578
2022-07-03 17:49:26 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.3406
2022-07-03 17:49:59 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.2927
2022-07-03 17:50:32 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.4826
2022-07-03 17:51:06 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.3064
2022-07-03 17:51:38 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.3581
2022-07-03 17:52:11 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.2485
2022-07-03 17:52:44 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.3909
2022-07-03 17:53:17 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.4051
2022-07-03 17:53:51 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.5083
2022-07-03 17:54:25 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.2951
2022-07-03 17:54:58 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.4734
2022-07-03 17:55:32 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.4706
2022-07-03 17:56:05 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.3420
2022-07-03 17:56:38 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.5531
2022-07-03 17:57:11 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.4462
2022-07-03 17:57:44 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.4626
2022-07-03 17:58:17 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.4544
2022-07-03 17:58:50 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.5602
2022-07-03 17:59:24 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.3847
2022-07-03 17:59:56 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.6107
2022-07-03 17:59:57 - train: epoch 056, train_loss: 1.3823
2022-07-03 18:01:10 - eval: epoch: 056, acc1: 68.522%, acc5: 88.828%, test_loss: 1.2749, per_image_load_time: 1.655ms, per_image_inference_time: 0.278ms
2022-07-03 18:01:10 - until epoch: 056, best_acc1: 69.192%
2022-07-03 18:01:10 - epoch 057 lr: 0.010000
2022-07-03 18:01:49 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.3882
2022-07-03 18:02:22 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.3710
2022-07-03 18:02:55 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.4663
2022-07-03 18:03:28 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.2738
2022-07-03 18:04:02 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.3010
2022-07-03 18:04:34 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.4755
2022-07-03 18:05:08 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.1865
2022-07-03 18:05:40 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.2844
2022-07-03 18:06:14 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.4406
2022-07-03 18:06:47 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.3370
2022-07-03 18:07:21 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.2869
