2022-02-20 05:08:47 - network: resnet34
2022-02-20 05:08:47 - num_classes: 1000
2022-02-20 05:08:47 - input_image_size: 224
2022-02-20 05:08:47 - scale: 1.1428571428571428
2022-02-20 05:08:47 - trained_model_path: 
2022-02-20 05:08:47 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-20 05:08:47 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f3d80313220>
2022-02-20 05:08:47 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f3d803134f0>
2022-02-20 05:08:47 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f3d80313520>
2022-02-20 05:08:47 - seed: 0
2022-02-20 05:08:47 - batch_size: 256
2022-02-20 05:08:47 - num_workers: 16
2022-02-20 05:08:47 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-20 05:08:47 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-20 05:08:47 - epochs: 100
2022-02-20 05:08:47 - print_interval: 100
2022-02-20 05:08:47 - distributed: True
2022-02-20 05:08:47 - sync_bn: False
2022-02-20 05:08:47 - apex: True
2022-02-20 05:08:47 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-20 05:08:47 - gpus_num: 2
2022-02-20 05:08:47 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f3d5e865330>
2022-02-20 05:08:54 - --------------------parameters--------------------
2022-02-20 05:08:54 - name: conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-20 05:08:54 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-20 05:08:54 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-20 05:08:54 - name: fc.weight, grad: True
2022-02-20 05:08:54 - name: fc.bias, grad: True
2022-02-20 05:08:54 - --------------------buffers--------------------
2022-02-20 05:08:54 - name: conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:08:54 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:08:54 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:08:54 - epoch 001 lr: 0.1
2022-02-20 05:09:39 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8633
2022-02-20 05:10:18 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7285
2022-02-20 05:10:57 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6101
2022-02-20 05:11:35 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5433
2022-02-20 05:12:14 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4254
2022-02-20 05:12:52 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.0475
2022-02-20 05:13:31 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.0683
2022-02-20 05:14:10 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.0666
2022-02-20 05:14:49 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.9236
2022-02-20 05:15:28 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.8110
2022-02-20 05:16:06 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.8119
2022-02-20 05:16:45 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.5760
2022-02-20 05:17:24 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.4900
2022-02-20 05:18:03 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.5224
2022-02-20 05:18:42 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.3844
2022-02-20 05:19:20 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.4605
2022-02-20 05:20:00 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.2982
2022-02-20 05:20:39 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.3171
2022-02-20 05:21:17 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.2530
2022-02-20 05:21:56 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.0926
2022-02-20 05:22:35 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.1206
2022-02-20 05:23:13 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.9369
2022-02-20 05:23:52 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.8081
2022-02-20 05:24:32 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.9030
2022-02-20 05:25:12 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9101
2022-02-20 05:25:50 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.0690
2022-02-20 05:26:29 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.9907
2022-02-20 05:27:08 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.7216
2022-02-20 05:27:46 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.6093
2022-02-20 05:28:25 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.7994
2022-02-20 05:29:04 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.7581
2022-02-20 05:29:43 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.6572
2022-02-20 05:30:23 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.4029
2022-02-20 05:31:01 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.3896
2022-02-20 05:31:41 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.4876
2022-02-20 05:32:19 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.4387
2022-02-20 05:32:58 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.5161
2022-02-20 05:33:37 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.3501
2022-02-20 05:34:16 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.3543
2022-02-20 05:34:54 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.2800
2022-02-20 05:35:33 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.3745
2022-02-20 05:36:12 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.2181
2022-02-20 05:36:52 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.1475
2022-02-20 05:37:30 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 3.9258
2022-02-20 05:38:08 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.0915
2022-02-20 05:38:48 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.3906
2022-02-20 05:39:27 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.0094
2022-02-20 05:40:04 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.3241
2022-02-20 05:40:43 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.0602
2022-02-20 05:41:20 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.9199
2022-02-20 05:41:21 - train: epoch 001, train_loss: 5.0432
2022-02-20 05:42:49 - eval: epoch: 001, acc1: 21.670%, acc5: 45.480%, test_loss: 3.8411, per_image_load_time: 3.081ms, per_image_inference_time: 0.295ms
2022-02-20 05:42:49 - until epoch: 001, best_acc1: 21.670%
2022-02-20 05:42:49 - epoch 002 lr: 0.1
2022-02-20 05:43:34 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.1067
2022-02-20 05:44:12 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.7606
2022-02-20 05:44:50 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 3.9584
2022-02-20 05:45:29 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.9645
2022-02-20 05:46:05 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.7190
2022-02-20 05:46:41 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.7845
2022-02-20 05:47:21 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.9805
2022-02-20 05:48:01 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.5522
2022-02-20 05:48:40 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.4376
2022-02-20 05:49:19 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.8885
2022-02-20 05:49:58 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.9480
2022-02-20 05:50:37 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.7645
2022-02-20 05:51:17 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.6923
2022-02-20 05:51:56 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.7513
2022-02-20 05:52:35 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.7697
2022-02-20 05:53:14 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.6258
2022-02-20 05:53:53 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.6985
2022-02-20 05:54:32 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.6590
2022-02-20 05:55:10 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.5249
2022-02-20 05:55:49 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.2821
2022-02-20 05:56:28 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.5686
2022-02-20 05:57:07 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.2742
2022-02-20 05:57:46 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.5008
2022-02-20 05:58:25 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.4103
2022-02-20 05:59:04 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.3712
2022-02-20 05:59:43 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.3413
2022-02-20 06:00:22 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.5790
2022-02-20 06:01:01 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.4995
2022-02-20 06:01:39 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.4569
2022-02-20 06:02:18 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.3498
2022-02-20 06:02:58 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.3363
2022-02-20 06:03:36 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.3722
2022-02-20 06:04:16 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.3040
2022-02-20 06:04:53 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.5798
2022-02-20 06:05:32 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.3688
2022-02-20 06:06:11 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.3284
2022-02-20 06:06:51 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.4936
2022-02-20 06:07:30 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.1818
2022-02-20 06:08:09 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.3558
2022-02-20 06:08:49 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.2004
2022-02-20 06:09:27 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.4251
2022-02-20 06:10:07 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.2027
2022-02-20 06:10:46 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.2687
2022-02-20 06:11:24 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.0680
2022-02-20 06:12:03 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.0939
2022-02-20 06:12:42 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.1363
2022-02-20 06:13:20 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.3019
2022-02-20 06:13:59 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.3030
2022-02-20 06:14:38 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.1543
2022-02-20 06:15:15 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.1154
2022-02-20 06:15:16 - train: epoch 002, train_loss: 3.5092
2022-02-20 06:16:42 - eval: epoch: 002, acc1: 34.532%, acc5: 61.014%, test_loss: 3.0156, per_image_load_time: 3.073ms, per_image_inference_time: 0.281ms
2022-02-20 06:16:43 - until epoch: 002, best_acc1: 34.532%
2022-02-20 06:16:43 - epoch 003 lr: 0.1
2022-02-20 06:17:28 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.2521
2022-02-20 06:18:07 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.1729
2022-02-20 06:18:45 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.1842
2022-02-20 06:19:24 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.1436
2022-02-20 06:20:03 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3808
2022-02-20 06:20:42 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.0736
2022-02-20 06:21:21 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.3434
2022-02-20 06:21:59 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.1668
2022-02-20 06:22:38 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.0051
2022-02-20 06:23:17 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.1333
2022-02-20 06:23:55 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 2.9525
2022-02-20 06:24:35 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.1094
2022-02-20 06:25:14 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.0581
2022-02-20 06:25:52 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0233
2022-02-20 06:26:30 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.3613
2022-02-20 06:27:10 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 2.9946
2022-02-20 06:27:49 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0411
2022-02-20 06:28:28 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.9243
2022-02-20 06:29:08 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 2.9747
2022-02-20 06:29:45 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.4129
2022-02-20 06:30:24 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.3597
2022-02-20 06:31:03 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5529
2022-02-20 06:31:41 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 2.9791
2022-02-20 06:32:20 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0399
2022-02-20 06:32:59 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.0079
2022-02-20 06:33:37 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.0111
2022-02-20 06:34:16 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4214
2022-02-20 06:34:56 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.9278
2022-02-20 06:35:34 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 2.9640
2022-02-20 06:36:14 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.1519
2022-02-20 06:36:53 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.1595
2022-02-20 06:37:31 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 2.9148
2022-02-20 06:38:10 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.0309
2022-02-20 06:38:48 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.1254
2022-02-20 06:39:28 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8589
2022-02-20 06:40:06 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.8206
2022-02-20 06:40:45 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.9625
2022-02-20 06:41:23 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.0434
2022-02-20 06:42:03 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.1278
2022-02-20 06:42:41 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 2.9214
2022-02-20 06:43:19 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.0547
2022-02-20 06:43:58 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9343
2022-02-20 06:44:37 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.5821
2022-02-20 06:45:16 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.8292
2022-02-20 06:45:55 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.9302
2022-02-20 06:46:34 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.9264
2022-02-20 06:47:12 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.8319
2022-02-20 06:47:51 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 2.9284
2022-02-20 06:48:30 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.0379
2022-02-20 06:49:06 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.0052
2022-02-20 06:49:07 - train: epoch 003, train_loss: 3.0196
2022-02-20 06:50:34 - eval: epoch: 003, acc1: 40.326%, acc5: 67.188%, test_loss: 2.6714, per_image_load_time: 3.087ms, per_image_inference_time: 0.271ms
2022-02-20 06:50:35 - until epoch: 003, best_acc1: 40.326%
2022-02-20 06:50:35 - epoch 004 lr: 0.1
2022-02-20 06:51:20 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.9259
2022-02-20 06:51:58 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.7019
2022-02-20 06:52:36 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.8518
2022-02-20 06:53:16 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.8322
2022-02-20 06:53:54 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.7251
2022-02-20 06:54:32 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 2.9845
2022-02-20 06:55:10 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.0474
2022-02-20 06:55:50 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.6572
2022-02-20 06:56:29 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6112
2022-02-20 06:57:08 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.8298
2022-02-20 06:57:47 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0067
2022-02-20 06:58:25 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.5909
2022-02-20 06:59:04 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.5933
2022-02-20 06:59:42 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.9094
2022-02-20 07:00:22 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.8776
2022-02-20 07:01:00 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.7438
2022-02-20 07:01:38 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9384
2022-02-20 07:02:17 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.9836
2022-02-20 07:02:56 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.8290
2022-02-20 07:03:35 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.8131
2022-02-20 07:04:13 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9360
2022-02-20 07:04:51 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.7636
2022-02-20 07:05:29 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.6912
2022-02-20 07:06:08 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.6236
2022-02-20 07:06:46 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.7404
2022-02-20 07:07:26 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.8277
2022-02-20 07:08:04 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.5569
2022-02-20 07:08:43 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.6923
2022-02-20 07:09:22 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.7068
2022-02-20 07:10:01 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.7214
2022-02-20 07:10:39 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8876
2022-02-20 07:11:18 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.7067
2022-02-20 07:11:56 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9151
2022-02-20 07:12:35 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.8057
2022-02-20 07:13:13 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.7655
2022-02-20 07:13:51 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.6390
2022-02-20 07:14:30 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.7650
2022-02-20 07:15:10 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.5954
2022-02-20 07:15:47 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.6386
2022-02-20 07:16:26 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.4413
2022-02-20 07:17:05 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7408
2022-02-20 07:17:44 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.5144
2022-02-20 07:18:23 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.5381
2022-02-20 07:19:00 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.5534
2022-02-20 07:19:39 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.2222
2022-02-20 07:20:18 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.7867
2022-02-20 07:20:57 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.6168
2022-02-20 07:21:35 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.5541
2022-02-20 07:22:13 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.8043
2022-02-20 07:22:49 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.6836
2022-02-20 07:22:51 - train: epoch 004, train_loss: 2.7919
2022-02-20 07:24:18 - eval: epoch: 004, acc1: 41.374%, acc5: 68.152%, test_loss: 2.6138, per_image_load_time: 3.075ms, per_image_inference_time: 0.279ms
2022-02-20 07:24:19 - until epoch: 004, best_acc1: 41.374%
2022-02-20 07:24:19 - epoch 005 lr: 0.1
2022-02-20 07:25:03 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.8553
2022-02-20 07:25:42 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.7911
2022-02-20 07:26:20 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.7839
2022-02-20 07:26:58 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.7219
2022-02-20 07:27:38 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.5448
2022-02-20 07:28:16 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.6976
2022-02-20 07:28:55 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.6799
2022-02-20 07:29:34 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.0144
2022-02-20 07:30:12 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.5517
2022-02-20 07:30:51 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.7436
2022-02-20 07:31:29 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.7196
2022-02-20 07:32:08 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.6743
2022-02-20 07:32:47 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.6837
2022-02-20 07:33:26 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.7493
2022-02-20 07:34:04 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.5839
2022-02-20 07:34:43 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.5222
2022-02-20 07:35:23 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.5381
2022-02-20 07:36:02 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.6974
2022-02-20 07:36:39 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.4599
2022-02-20 07:37:16 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.7062
2022-02-20 07:37:56 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.4382
2022-02-20 07:38:35 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.5836
2022-02-20 07:39:15 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.4564
2022-02-20 07:39:53 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.5993
2022-02-20 07:40:32 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.7925
2022-02-20 07:41:10 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.8068
2022-02-20 07:41:50 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.8343
2022-02-20 07:42:29 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.6548
2022-02-20 07:43:08 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.5565
2022-02-20 07:43:46 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6329
2022-02-20 07:44:25 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.6384
2022-02-20 07:45:02 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.8074
2022-02-20 07:45:42 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4223
2022-02-20 07:46:21 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.5488
2022-02-20 07:47:00 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.6763
2022-02-20 07:47:38 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.6585
2022-02-20 07:48:17 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.6806
2022-02-20 07:48:56 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.3947
2022-02-20 07:49:35 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.8972
2022-02-20 07:50:14 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.6160
2022-02-20 07:50:53 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.5159
2022-02-20 07:51:30 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.6966
2022-02-20 07:52:11 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.5863
2022-02-20 07:52:50 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.6277
2022-02-20 07:53:30 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.6740
2022-02-20 07:54:07 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.5695
2022-02-20 07:54:47 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.4231
2022-02-20 07:55:22 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.4815
2022-02-20 07:56:00 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.7149
2022-02-20 07:56:38 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.5406
2022-02-20 07:56:39 - train: epoch 005, train_loss: 2.6618
2022-02-20 07:58:06 - eval: epoch: 005, acc1: 43.880%, acc5: 70.748%, test_loss: 2.4628, per_image_load_time: 3.092ms, per_image_inference_time: 0.283ms
2022-02-20 07:58:07 - until epoch: 005, best_acc1: 43.880%
2022-02-20 07:58:07 - epoch 006 lr: 0.1
2022-02-20 07:58:51 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.5409
2022-02-20 07:59:30 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.5913
2022-02-20 08:00:08 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.3630
2022-02-20 08:00:47 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.7692
2022-02-20 08:01:24 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.5419
2022-02-20 08:02:02 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.5280
2022-02-20 08:02:41 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.6615
2022-02-20 08:03:20 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.6766
2022-02-20 08:03:59 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.4840
2022-02-20 08:04:38 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.4589
2022-02-20 08:05:16 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.4084
2022-02-20 08:05:55 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.6661
2022-02-20 08:06:34 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.7119
2022-02-20 08:07:13 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.7139
2022-02-20 08:07:52 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.7213
2022-02-20 08:08:30 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.4145
2022-02-20 08:09:09 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.7939
2022-02-20 08:09:48 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.7822
2022-02-20 08:10:27 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.4919
2022-02-20 08:11:06 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.7454
2022-02-20 08:11:44 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.6653
2022-02-20 08:12:22 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.5321
2022-02-20 08:13:01 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.3935
2022-02-20 08:13:40 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.6352
2022-02-20 08:14:19 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.7984
2022-02-20 08:14:58 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.4405
2022-02-20 08:15:36 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.6742
2022-02-20 08:16:15 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.3630
2022-02-20 08:16:54 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.7180
2022-02-20 08:17:32 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.5962
2022-02-20 08:18:12 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.3418
2022-02-20 08:18:50 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.5454
2022-02-20 08:19:29 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.3954
2022-02-20 08:20:07 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.7493
2022-02-20 08:20:46 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.6793
2022-02-20 08:21:24 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.5443
2022-02-20 08:22:04 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.7141
2022-02-20 08:22:42 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.4989
2022-02-20 08:23:21 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5249
2022-02-20 08:23:59 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.7752
2022-02-20 08:24:39 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.4382
2022-02-20 08:25:17 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.4193
2022-02-20 08:25:56 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6037
2022-02-20 08:26:34 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.5562
2022-02-20 08:27:13 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.4917
2022-02-20 08:27:51 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.4671
2022-02-20 08:28:31 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.5541
2022-02-20 08:29:09 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.6172
2022-02-20 08:29:48 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.6684
2022-02-20 08:30:24 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.4212
2022-02-20 08:30:25 - train: epoch 006, train_loss: 2.5814
2022-02-20 08:31:53 - eval: epoch: 006, acc1: 45.128%, acc5: 71.648%, test_loss: 2.4121, per_image_load_time: 3.113ms, per_image_inference_time: 0.292ms
2022-02-20 08:31:54 - until epoch: 006, best_acc1: 45.128%
2022-02-20 08:31:54 - epoch 007 lr: 0.1
2022-02-20 08:32:38 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.3658
2022-02-20 08:33:17 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.6656
2022-02-20 08:33:56 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.6763
2022-02-20 08:34:35 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7334
2022-02-20 08:35:14 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.4775
2022-02-20 08:35:52 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.7316
2022-02-20 08:36:32 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.5519
2022-02-20 08:37:09 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.5288
2022-02-20 08:37:48 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.6496
2022-02-20 08:38:27 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.5640
2022-02-20 08:39:06 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.3926
2022-02-20 08:39:44 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.4996
2022-02-20 08:40:23 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4943
2022-02-20 08:41:03 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.5854
2022-02-20 08:41:41 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.6308
2022-02-20 08:42:20 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.5175
2022-02-20 08:42:58 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.5544
2022-02-20 08:43:37 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.4681
2022-02-20 08:44:15 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.6003
2022-02-20 08:44:53 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.3903
2022-02-20 08:45:34 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.6983
2022-02-20 08:46:11 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.4071
2022-02-20 08:46:51 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.4713
2022-02-20 08:47:28 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.5759
2022-02-20 08:48:09 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.4220
2022-02-20 08:48:46 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.4980
2022-02-20 08:49:26 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.4055
2022-02-20 08:50:04 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.4342
2022-02-20 08:50:44 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.4390
2022-02-20 08:51:21 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.5298
2022-02-20 08:52:01 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.3618
2022-02-20 08:52:39 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.4908
2022-02-20 08:53:19 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.7475
2022-02-20 08:53:57 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.3165
2022-02-20 08:54:36 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.6381
2022-02-20 08:55:15 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.3348
2022-02-20 08:55:52 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.5773
2022-02-20 08:56:31 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.8303
2022-02-20 08:57:10 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.3775
2022-02-20 08:57:48 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.6316
2022-02-20 08:58:27 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.4680
2022-02-20 08:59:05 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.4818
2022-02-20 08:59:44 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.6548
2022-02-20 09:00:24 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.3561
2022-02-20 09:01:02 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.7259
2022-02-20 09:01:40 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.5724
2022-02-20 09:02:21 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.5685
2022-02-20 09:02:59 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.7525
2022-02-20 09:03:39 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.5337
2022-02-20 09:04:16 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.4205
2022-02-20 09:04:17 - train: epoch 007, train_loss: 2.5250
2022-02-20 09:05:43 - eval: epoch: 007, acc1: 48.716%, acc5: 75.112%, test_loss: 2.2088, per_image_load_time: 3.048ms, per_image_inference_time: 0.274ms
2022-02-20 09:05:44 - until epoch: 007, best_acc1: 48.716%
2022-02-20 09:05:44 - epoch 008 lr: 0.1
2022-02-20 09:06:28 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.4127
2022-02-20 09:07:08 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.5532
2022-02-20 09:07:45 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.2985
2022-02-20 09:08:25 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.3294
2022-02-20 09:09:02 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.3900
2022-02-20 09:09:41 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.3433
2022-02-20 09:10:20 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.7945
2022-02-20 09:10:59 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.3686
2022-02-20 09:11:37 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.4507
2022-02-20 09:12:16 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.4724
2022-02-20 09:12:55 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.2626
2022-02-20 09:13:33 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.3254
2022-02-20 09:14:12 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.4892
2022-02-20 09:14:52 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.3702
2022-02-20 09:15:30 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.4735
2022-02-20 09:16:09 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.4067
2022-02-20 09:16:47 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.3367
2022-02-20 09:17:27 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.6474
2022-02-20 09:18:04 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.2976
2022-02-20 09:18:41 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.4396
2022-02-20 09:19:21 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.4665
2022-02-20 09:20:00 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.3177
2022-02-20 09:20:39 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.5993
2022-02-20 09:21:18 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.4714
2022-02-20 09:21:56 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.5398
2022-02-20 09:22:36 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.6426
2022-02-20 09:23:14 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.5680
2022-02-20 09:23:53 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.5504
2022-02-20 09:24:31 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.5006
2022-02-20 09:25:10 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.6279
2022-02-20 09:25:49 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.4922
2022-02-20 09:26:29 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.6947
2022-02-20 09:27:07 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.6739
2022-02-20 09:27:46 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.7539
2022-02-20 09:28:25 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.4154
2022-02-20 09:29:04 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.6838
2022-02-20 09:29:41 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.4363
2022-02-20 09:30:20 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.4030
2022-02-20 09:30:58 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.5360
2022-02-20 09:31:38 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.8584
2022-02-20 09:32:16 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.4827
2022-02-20 09:32:56 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.4800
2022-02-20 09:33:33 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.3039
2022-02-20 09:34:12 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.4311
2022-02-20 09:34:52 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.6408
2022-02-20 09:35:29 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.5862
2022-02-20 09:36:09 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.2856
2022-02-20 09:36:47 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.4434
2022-02-20 09:37:26 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.5414
2022-02-20 09:38:03 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.4046
2022-02-20 09:38:04 - train: epoch 008, train_loss: 2.4826
2022-02-20 09:39:30 - eval: epoch: 008, acc1: 47.634%, acc5: 74.006%, test_loss: 2.2673, per_image_load_time: 3.093ms, per_image_inference_time: 0.265ms
2022-02-20 09:39:31 - until epoch: 008, best_acc1: 48.716%
2022-02-20 09:39:31 - epoch 009 lr: 0.1
2022-02-20 09:40:15 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.1694
2022-02-20 09:40:53 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.3365
2022-02-20 09:41:31 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.1862
2022-02-20 09:42:11 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7299
2022-02-20 09:42:48 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.5887
2022-02-20 09:43:27 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.4744
2022-02-20 09:44:06 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.3932
2022-02-20 09:44:44 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.3508
2022-02-20 09:45:24 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.3567
2022-02-20 09:46:02 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.2384
2022-02-20 09:46:42 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.7346
2022-02-20 09:47:20 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.5965
2022-02-20 09:47:58 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.7309
2022-02-20 09:48:37 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.2369
2022-02-20 09:49:17 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.2627
2022-02-20 09:49:55 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.4557
2022-02-20 09:50:33 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.6313
2022-02-20 09:51:12 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.3736
2022-02-20 09:51:52 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.1907
2022-02-20 09:52:30 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.1852
2022-02-20 09:53:09 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.5032
2022-02-20 09:53:48 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.5046
2022-02-20 09:54:26 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.1774
2022-02-20 09:55:05 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.4784
2022-02-20 09:55:43 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.3246
2022-02-20 09:56:23 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.4231
2022-02-20 09:57:00 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.4114
2022-02-20 09:57:41 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.4769
2022-02-20 09:58:19 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.1306
2022-02-20 09:58:57 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.4457
2022-02-20 09:59:36 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.5499
2022-02-20 10:00:16 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.4536
2022-02-20 10:00:54 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.4320
2022-02-20 10:01:33 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.6975
2022-02-20 10:02:11 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.4889
2022-02-20 10:02:50 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.4867
2022-02-20 10:03:29 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.6371
2022-02-20 10:04:05 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.5666
2022-02-20 10:04:41 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.1818
2022-02-20 10:05:20 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.6424
2022-02-20 10:05:59 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.3721
2022-02-20 10:06:40 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.3929
2022-02-20 10:07:19 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.6446
2022-02-20 10:07:58 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.4554
2022-02-20 10:08:36 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.3600
2022-02-20 10:09:14 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.5626
2022-02-20 10:09:54 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.5736
2022-02-20 10:10:33 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.6110
2022-02-20 10:11:12 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.5682
2022-02-20 10:11:48 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.4186
2022-02-20 10:11:49 - train: epoch 009, train_loss: 2.4521
2022-02-20 10:13:16 - eval: epoch: 009, acc1: 48.340%, acc5: 74.716%, test_loss: 2.2382, per_image_load_time: 3.101ms, per_image_inference_time: 0.278ms
2022-02-20 10:13:16 - until epoch: 009, best_acc1: 48.716%
2022-02-20 10:13:16 - epoch 010 lr: 0.1
2022-02-20 10:14:02 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.3924
2022-02-20 10:14:41 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.5747
2022-02-20 10:15:19 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.3901
2022-02-20 10:15:58 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.4022
2022-02-20 10:16:36 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.4017
2022-02-20 10:17:16 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.3915
2022-02-20 10:17:54 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.5394
2022-02-20 10:18:33 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.2735
2022-02-20 10:19:11 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.2796
2022-02-20 10:19:50 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.2402
2022-02-20 10:20:29 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.4051
2022-02-20 10:21:10 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.2085
2022-02-20 10:21:48 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.2783
2022-02-20 10:22:27 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.4820
2022-02-20 10:23:05 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.1337
2022-02-20 10:23:44 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.4479
2022-02-20 10:24:23 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.6183
2022-02-20 10:25:03 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.4254
2022-02-20 10:25:41 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.4356
2022-02-20 10:26:20 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.5337
2022-02-20 10:26:59 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.2902
2022-02-20 10:27:38 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.5489
2022-02-20 10:28:17 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.6171
2022-02-20 10:28:57 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.6201
2022-02-20 10:29:35 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.4169
2022-02-20 10:30:13 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.5783
2022-02-20 10:30:52 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.1023
2022-02-20 10:31:31 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.4493
2022-02-20 10:32:11 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.5191
2022-02-20 10:32:50 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.4168
2022-02-20 10:33:28 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.6190
2022-02-20 10:34:07 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.3504
2022-02-20 10:34:46 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.6500
2022-02-20 10:35:25 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.6007
2022-02-20 10:36:04 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.6529
2022-02-20 10:36:42 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.5834
2022-02-20 10:37:22 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.3354
2022-02-20 10:38:00 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.4321
2022-02-20 10:38:39 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.1509
2022-02-20 10:39:19 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.3915
2022-02-20 10:39:57 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.2582
2022-02-20 10:40:35 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.4963
2022-02-20 10:41:14 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.4542
2022-02-20 10:41:53 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.2990
2022-02-20 10:42:32 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.2645
2022-02-20 10:43:12 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.5951
2022-02-20 10:43:50 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.4471
2022-02-20 10:44:29 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.4560
2022-02-20 10:45:09 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.3334
2022-02-20 10:45:46 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.1630
2022-02-20 10:45:47 - train: epoch 010, train_loss: 2.4258
2022-02-20 10:47:13 - eval: epoch: 010, acc1: 49.506%, acc5: 75.394%, test_loss: 2.1865, per_image_load_time: 3.086ms, per_image_inference_time: 0.284ms
2022-02-20 10:47:14 - until epoch: 010, best_acc1: 49.506%
2022-02-20 10:47:14 - epoch 011 lr: 0.1
2022-02-20 10:47:58 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.2000
2022-02-20 10:48:37 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.4825
2022-02-20 10:49:16 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.1220
2022-02-20 10:49:54 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.5984
2022-02-20 10:50:34 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.3052
2022-02-20 10:51:12 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.3965
2022-02-20 10:51:51 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4842
2022-02-20 10:52:30 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.3529
2022-02-20 10:53:08 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.4113
2022-02-20 10:53:47 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.3846
2022-02-20 10:54:26 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.5228
2022-02-20 10:55:04 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.7198
2022-02-20 10:55:43 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.6441
2022-02-20 10:56:22 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.4204
2022-02-20 10:57:01 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.2748
2022-02-20 10:57:39 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.4874
2022-02-20 10:58:18 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.4604
2022-02-20 10:58:56 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.3387
2022-02-20 10:59:36 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.2439
2022-02-20 11:00:12 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.6105
2022-02-20 11:00:50 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.4774
2022-02-20 11:01:30 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.2943
2022-02-20 11:02:08 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.5974
2022-02-20 11:02:48 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.1764
2022-02-20 11:03:27 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.7233
2022-02-20 11:04:05 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.4118
2022-02-20 11:04:45 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.4622
2022-02-20 11:05:22 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.0894
2022-02-20 11:06:02 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.5069
2022-02-20 11:06:40 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.5923
2022-02-20 11:07:18 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.4255
2022-02-20 11:07:57 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.2654
2022-02-20 11:08:36 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.5207
2022-02-20 11:09:14 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.3782
2022-02-20 11:09:53 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.3341
2022-02-20 11:10:32 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.4343
2022-02-20 11:11:11 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.4427
2022-02-20 11:11:49 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.1881
2022-02-20 11:12:28 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.4043
2022-02-20 11:13:05 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.4423
2022-02-20 11:13:44 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.1475
2022-02-20 11:14:23 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.2915
2022-02-20 11:15:02 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4492
2022-02-20 11:15:41 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.3775
2022-02-20 11:16:19 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.2428
2022-02-20 11:16:58 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.2354
2022-02-20 11:17:38 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.1207
2022-02-20 11:18:17 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.2151
2022-02-20 11:18:54 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.1961
2022-02-20 11:19:31 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.3098
2022-02-20 11:19:32 - train: epoch 011, train_loss: 2.4048
2022-02-20 11:20:59 - eval: epoch: 011, acc1: 49.768%, acc5: 75.620%, test_loss: 2.1663, per_image_load_time: 3.089ms, per_image_inference_time: 0.285ms
2022-02-20 11:21:00 - until epoch: 011, best_acc1: 49.768%
2022-02-20 11:21:00 - epoch 012 lr: 0.1
2022-02-20 11:21:44 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.2572
2022-02-20 11:22:23 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.2959
2022-02-20 11:23:01 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.3559
2022-02-20 11:23:40 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.5420
2022-02-20 11:24:19 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.6555
2022-02-20 11:24:58 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.1675
2022-02-20 11:25:36 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.2578
2022-02-20 11:26:15 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.2970
2022-02-20 11:26:54 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.5175
2022-02-20 11:27:33 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.1955
2022-02-20 11:28:12 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.7460
2022-02-20 11:28:51 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.2001
2022-02-20 11:29:29 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.3093
2022-02-20 11:30:08 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.5173
2022-02-20 11:30:47 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.2366
2022-02-20 11:31:26 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.3176
2022-02-20 11:32:04 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.2887
2022-02-20 11:32:43 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.4378
2022-02-20 11:33:21 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.4000
2022-02-20 11:33:59 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.5707
2022-02-20 11:34:39 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.3893
2022-02-20 11:35:16 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6334
2022-02-20 11:35:56 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.5190
2022-02-20 11:36:34 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.4842
2022-02-20 11:37:12 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.2123
2022-02-20 11:37:51 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.2143
2022-02-20 11:38:30 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.3255
2022-02-20 11:39:09 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.4247
2022-02-20 11:39:48 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.2100
2022-02-20 11:40:26 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.2945
2022-02-20 11:41:05 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.5742
2022-02-20 11:41:43 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1187
2022-02-20 11:42:23 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.4926
2022-02-20 11:43:01 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.3787
2022-02-20 11:43:40 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.5996
2022-02-20 11:44:18 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.3608
2022-02-20 11:44:57 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.3390
2022-02-20 11:45:35 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.3868
2022-02-20 11:46:14 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.3309
2022-02-20 11:46:52 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.3171
2022-02-20 11:47:31 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.2985
2022-02-20 11:48:09 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.2224
2022-02-20 11:48:49 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.5063
2022-02-20 11:49:27 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.1902
2022-02-20 11:50:06 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.3867
2022-02-20 11:50:45 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.7730
2022-02-20 11:51:23 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3747
2022-02-20 11:52:02 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.4423
2022-02-20 11:52:42 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.3594
2022-02-20 11:53:19 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.0516
2022-02-20 11:53:20 - train: epoch 012, train_loss: 2.3886
2022-02-20 11:54:46 - eval: epoch: 012, acc1: 50.938%, acc5: 76.766%, test_loss: 2.1007, per_image_load_time: 3.062ms, per_image_inference_time: 0.279ms
2022-02-20 11:54:47 - until epoch: 012, best_acc1: 50.938%
2022-02-20 11:54:47 - epoch 013 lr: 0.1
2022-02-20 11:55:30 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.1639
2022-02-20 11:56:10 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.3040
2022-02-20 11:56:49 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.3780
2022-02-20 11:57:28 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2675
2022-02-20 11:58:06 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.3595
2022-02-20 11:58:44 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.4412
2022-02-20 11:59:23 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.3042
2022-02-20 12:00:02 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.5322
2022-02-20 12:00:41 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.4051
2022-02-20 12:01:20 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.3774
2022-02-20 12:01:59 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.4021
2022-02-20 12:02:37 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.4761
2022-02-20 12:03:17 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.3945
2022-02-20 12:03:57 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.3092
2022-02-20 12:04:36 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.5603
2022-02-20 12:05:14 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1250
2022-02-20 12:05:53 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.3615
2022-02-20 12:06:32 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.3655
2022-02-20 12:07:12 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.3369
2022-02-20 12:07:50 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.6114
2022-02-20 12:08:29 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.6191
2022-02-20 12:09:08 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3242
2022-02-20 12:09:46 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.4121
2022-02-20 12:10:25 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.3369
2022-02-20 12:11:05 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.2701
2022-02-20 12:11:43 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.3049
2022-02-20 12:12:22 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.2640
2022-02-20 12:13:00 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4151
2022-02-20 12:13:36 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.3294
2022-02-20 12:14:14 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.2437
2022-02-20 12:14:54 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.2285
2022-02-20 12:15:33 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.3468
2022-02-20 12:16:12 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.2198
2022-02-20 12:16:52 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.2474
2022-02-20 12:17:30 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.1745
2022-02-20 12:18:09 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6294
2022-02-20 12:18:48 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.0560
2022-02-20 12:19:26 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.3785
2022-02-20 12:20:05 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.5571
2022-02-20 12:20:44 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.3394
2022-02-20 12:21:23 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.3250
2022-02-20 12:22:04 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.2883
2022-02-20 12:22:41 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.2600
2022-02-20 12:23:19 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.3065
2022-02-20 12:23:58 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.2553
2022-02-20 12:24:38 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.3555
2022-02-20 12:25:16 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5726
2022-02-20 12:25:56 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.4750
2022-02-20 12:26:33 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.4490
2022-02-20 12:27:11 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.4945
2022-02-20 12:27:12 - train: epoch 013, train_loss: 2.3725
2022-02-20 12:28:39 - eval: epoch: 013, acc1: 49.614%, acc5: 75.480%, test_loss: 2.1735, per_image_load_time: 3.097ms, per_image_inference_time: 0.271ms
2022-02-20 12:28:40 - until epoch: 013, best_acc1: 50.938%
2022-02-20 12:28:40 - epoch 014 lr: 0.1
2022-02-20 12:29:24 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.3077
2022-02-20 12:30:03 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5071
2022-02-20 12:30:41 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.1103
2022-02-20 12:31:20 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.2846
2022-02-20 12:31:59 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.3578
2022-02-20 12:32:37 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.3142
2022-02-20 12:33:15 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.3391
2022-02-20 12:33:53 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.3929
2022-02-20 12:34:31 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.4017
2022-02-20 12:35:10 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.5726
2022-02-20 12:35:49 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.2575
2022-02-20 12:36:27 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.3282
2022-02-20 12:37:06 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.3668
2022-02-20 12:37:44 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.3290
2022-02-20 12:38:23 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.3781
2022-02-20 12:39:03 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.2487
2022-02-20 12:39:42 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.6608
2022-02-20 12:40:20 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.5495
2022-02-20 12:41:00 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2143
2022-02-20 12:41:36 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3609
2022-02-20 12:42:14 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.4514
2022-02-20 12:42:53 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.5002
2022-02-20 12:43:30 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.2892
2022-02-20 12:44:09 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.5701
2022-02-20 12:44:48 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.2506
2022-02-20 12:45:27 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.3052
2022-02-20 12:46:07 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.2999
2022-02-20 12:46:45 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.6317
2022-02-20 12:47:24 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.2873
2022-02-20 12:48:02 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.4651
2022-02-20 12:48:40 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.1722
2022-02-20 12:49:19 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2718
2022-02-20 12:49:59 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.2214
2022-02-20 12:50:37 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.1764
2022-02-20 12:51:16 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.2718
2022-02-20 12:51:54 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.1814
2022-02-20 12:52:34 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.3209
2022-02-20 12:53:12 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.4847
2022-02-20 12:53:51 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.2551
2022-02-20 12:54:30 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.5264
2022-02-20 12:55:08 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.2279
2022-02-20 12:55:47 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2451
2022-02-20 12:56:27 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.2103
2022-02-20 12:57:04 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.1621
2022-02-20 12:57:43 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.3555
2022-02-20 12:58:21 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.3471
2022-02-20 12:58:59 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.1920
2022-02-20 12:59:39 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.3085
2022-02-20 13:00:18 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1928
2022-02-20 13:00:56 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.4086
2022-02-20 13:00:57 - train: epoch 014, train_loss: 2.3600
2022-02-20 13:02:23 - eval: epoch: 014, acc1: 47.584%, acc5: 73.780%, test_loss: 2.3065, per_image_load_time: 3.069ms, per_image_inference_time: 0.280ms
2022-02-20 13:02:23 - until epoch: 014, best_acc1: 50.938%
2022-02-20 13:02:23 - epoch 015 lr: 0.1
2022-02-20 13:03:08 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.0142
2022-02-20 13:03:48 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.4755
2022-02-20 13:04:26 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6153
2022-02-20 13:05:04 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.2992
2022-02-20 13:05:43 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.2482
2022-02-20 13:06:23 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.5411
2022-02-20 13:07:02 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.3640
2022-02-20 13:07:41 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.1128
2022-02-20 13:08:20 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.2185
2022-02-20 13:08:58 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.4464
2022-02-20 13:09:37 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.1080
2022-02-20 13:10:15 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.2347
2022-02-20 13:10:55 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5791
2022-02-20 13:11:34 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2643
2022-02-20 13:12:13 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.1565
2022-02-20 13:12:51 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.3596
2022-02-20 13:13:30 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.5479
2022-02-20 13:14:08 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2830
2022-02-20 13:14:48 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.2653
2022-02-20 13:15:26 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.3287
2022-02-20 13:16:06 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.2689
2022-02-20 13:16:45 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.5644
2022-02-20 13:17:23 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.1720
2022-02-20 13:18:03 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4190
2022-02-20 13:18:40 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.4047
2022-02-20 13:19:20 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.2135
2022-02-20 13:19:57 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.4161
2022-02-20 13:20:37 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5550
2022-02-20 13:21:15 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.2930
2022-02-20 13:21:55 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.1530
2022-02-20 13:22:33 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.3033
2022-02-20 13:23:11 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3566
2022-02-20 13:23:51 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.0636
2022-02-20 13:24:30 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.3292
2022-02-20 13:25:09 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.4961
2022-02-20 13:25:48 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.3948
2022-02-20 13:26:25 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.3126
2022-02-20 13:27:05 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.3321
2022-02-20 13:27:45 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5677
2022-02-20 13:28:24 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.3740
2022-02-20 13:29:02 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.5263
2022-02-20 13:29:42 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.0700
2022-02-20 13:30:20 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.3123
2022-02-20 13:30:59 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.2044
2022-02-20 13:31:38 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.3851
2022-02-20 13:32:18 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.1982
2022-02-20 13:32:57 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.4307
2022-02-20 13:33:34 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.3018
2022-02-20 13:34:14 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.3676
2022-02-20 13:34:50 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.4560
2022-02-20 13:34:51 - train: epoch 015, train_loss: 2.3519
2022-02-20 13:36:18 - eval: epoch: 015, acc1: 50.024%, acc5: 76.138%, test_loss: 2.1546, per_image_load_time: 3.096ms, per_image_inference_time: 0.284ms
2022-02-20 13:36:19 - until epoch: 015, best_acc1: 50.938%
2022-02-20 13:36:19 - epoch 016 lr: 0.1
2022-02-20 13:37:03 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.2809
2022-02-20 13:37:42 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.1684
2022-02-20 13:38:21 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.2547
2022-02-20 13:38:59 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.5804
2022-02-20 13:39:38 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.1491
2022-02-20 13:40:16 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.4230
2022-02-20 13:40:55 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.1124
2022-02-20 13:41:34 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.4395
2022-02-20 13:42:12 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.3789
2022-02-20 13:42:53 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.2925
2022-02-20 13:43:31 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.2620
2022-02-20 13:44:09 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2597
2022-02-20 13:44:48 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.4974
2022-02-20 13:45:27 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.2553
2022-02-20 13:46:06 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.3552
2022-02-20 13:46:45 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.4684
2022-02-20 13:47:22 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.2233
2022-02-20 13:48:01 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.2103
2022-02-20 13:48:39 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.4978
2022-02-20 13:49:19 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.0450
2022-02-20 13:49:57 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.4343
2022-02-20 13:50:36 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.3535
2022-02-20 13:51:14 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6428
2022-02-20 13:51:53 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.4813
2022-02-20 13:52:31 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.1559
2022-02-20 13:53:09 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.4830
2022-02-20 13:53:46 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.2746
2022-02-20 13:54:24 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.1618
2022-02-20 13:55:01 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.4218
2022-02-20 13:55:38 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.5877
2022-02-20 13:56:14 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.5192
2022-02-20 13:56:52 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.5332
2022-02-20 13:57:29 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.4967
2022-02-20 13:58:06 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.3342
2022-02-20 13:58:44 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.1513
2022-02-20 13:59:21 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.2072
2022-02-20 13:59:58 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.4384
2022-02-20 14:00:34 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.6745
2022-02-20 14:01:12 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.3997
2022-02-20 14:01:49 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.4072
2022-02-20 14:02:26 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.2664
2022-02-20 14:03:04 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.3929
2022-02-20 14:03:42 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.2900
2022-02-20 14:04:20 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.2472
2022-02-20 14:04:57 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.3059
2022-02-20 14:05:35 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.4251
2022-02-20 14:06:12 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.5853
2022-02-20 14:06:50 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.2883
2022-02-20 14:07:26 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.3015
2022-02-20 14:08:02 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.3253
2022-02-20 14:08:03 - train: epoch 016, train_loss: 2.3409
2022-02-20 14:09:26 - eval: epoch: 016, acc1: 51.184%, acc5: 77.210%, test_loss: 2.0902, per_image_load_time: 2.701ms, per_image_inference_time: 0.309ms
2022-02-20 14:09:27 - until epoch: 016, best_acc1: 51.184%
2022-02-20 14:09:27 - epoch 017 lr: 0.1
2022-02-20 14:10:09 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.2170
2022-02-20 14:10:48 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.4358
2022-02-20 14:11:25 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.5845
2022-02-20 14:12:02 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1314
2022-02-20 14:12:39 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.3128
2022-02-20 14:13:16 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.6369
2022-02-20 14:13:53 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.4115
2022-02-20 14:14:31 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.3084
2022-02-20 14:15:08 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.1625
2022-02-20 14:15:45 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.2759
2022-02-20 14:16:22 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.6211
2022-02-20 14:17:00 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.4465
2022-02-20 14:17:37 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.3046
2022-02-20 14:18:14 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.3987
2022-02-20 14:18:51 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.0693
2022-02-20 14:19:28 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.2425
2022-02-20 14:20:06 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.2477
2022-02-20 14:20:44 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.3705
2022-02-20 14:21:18 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2809
2022-02-20 14:21:53 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.5567
2022-02-20 14:22:28 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.3289
2022-02-20 14:23:06 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.2236
2022-02-20 14:23:44 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.2426
2022-02-20 14:24:22 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.2651
2022-02-20 14:24:59 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.4593
2022-02-20 14:25:36 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.3035
2022-02-20 14:26:13 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3569
2022-02-20 14:26:50 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.5324
2022-02-20 14:27:27 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.5556
2022-02-20 14:28:04 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.1780
2022-02-20 14:28:41 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.4858
2022-02-20 14:29:18 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.2707
2022-02-20 14:29:54 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.4645
2022-02-20 14:30:32 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.1522
2022-02-20 14:31:09 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.2688
2022-02-20 14:31:46 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.5981
2022-02-20 14:32:23 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3847
2022-02-20 14:33:01 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.5520
2022-02-20 14:33:37 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.2196
2022-02-20 14:34:13 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.1711
2022-02-20 14:34:51 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.3044
2022-02-20 14:35:29 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.3024
2022-02-20 14:36:05 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3806
2022-02-20 14:36:43 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.2969
2022-02-20 14:37:19 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.4032
2022-02-20 14:37:57 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.4483
2022-02-20 14:38:35 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.5014
2022-02-20 14:39:12 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.4318
2022-02-20 14:39:49 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.1867
2022-02-20 14:40:24 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.1637
2022-02-20 14:40:25 - train: epoch 017, train_loss: 2.3343
2022-02-20 14:41:48 - eval: epoch: 017, acc1: 50.722%, acc5: 76.914%, test_loss: 2.1008, per_image_load_time: 2.936ms, per_image_inference_time: 0.265ms
2022-02-20 14:41:48 - until epoch: 017, best_acc1: 51.184%
2022-02-20 14:41:48 - epoch 018 lr: 0.1
2022-02-20 14:42:31 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.3325
2022-02-20 14:43:08 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.3834
2022-02-20 14:43:46 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5066
2022-02-20 14:44:23 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.3959
2022-02-20 14:45:00 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.2340
2022-02-20 14:45:38 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.4689
2022-02-20 14:46:17 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.1833
2022-02-20 14:46:53 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.3175
2022-02-20 14:47:31 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4051
2022-02-20 14:48:08 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.1663
2022-02-20 14:48:44 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.6086
2022-02-20 14:49:22 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.2609
2022-02-20 14:50:00 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.6054
2022-02-20 14:50:37 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.2733
2022-02-20 14:51:15 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.4347
2022-02-20 14:51:52 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.3055
2022-02-20 14:52:29 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.1902
2022-02-20 14:53:06 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.1284
2022-02-20 14:53:43 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.3440
2022-02-20 14:54:20 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.6396
2022-02-20 14:54:58 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.4994
2022-02-20 14:55:35 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.2587
2022-02-20 14:56:13 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.3732
2022-02-20 14:56:50 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.1421
2022-02-20 14:57:28 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.0943
2022-02-20 14:58:05 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.1981
2022-02-20 14:58:41 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.4878
2022-02-20 14:59:19 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.0477
2022-02-20 14:59:56 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.3711
2022-02-20 15:00:34 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2684
2022-02-20 15:01:11 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.5919
2022-02-20 15:01:48 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.2172
2022-02-20 15:02:24 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.2088
2022-02-20 15:03:02 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.2633
2022-02-20 15:03:40 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.6327
2022-02-20 15:04:17 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.3297
2022-02-20 15:04:54 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.7458
2022-02-20 15:05:32 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.3049
2022-02-20 15:06:08 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.4022
2022-02-20 15:06:46 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.1036
2022-02-20 15:07:22 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.2146
2022-02-20 15:08:00 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.2411
2022-02-20 15:08:37 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1248
2022-02-20 15:09:13 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.4454
2022-02-20 15:09:50 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.3814
2022-02-20 15:10:28 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.1920
2022-02-20 15:11:06 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.6464
2022-02-20 15:11:44 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.5337
2022-02-20 15:12:20 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.3518
2022-02-20 15:12:55 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.3678
2022-02-20 15:12:56 - train: epoch 018, train_loss: 2.3240
2022-02-20 15:14:19 - eval: epoch: 018, acc1: 51.314%, acc5: 77.044%, test_loss: 2.0953, per_image_load_time: 2.953ms, per_image_inference_time: 0.284ms
2022-02-20 15:14:20 - until epoch: 018, best_acc1: 51.314%
2022-02-20 15:14:20 - epoch 019 lr: 0.1
2022-02-20 15:15:03 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.1362
2022-02-20 15:15:39 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.4524
2022-02-20 15:16:16 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.5707
2022-02-20 15:16:54 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.1806
2022-02-20 15:17:31 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.2551
2022-02-20 15:18:09 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.2783
2022-02-20 15:18:46 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.0644
2022-02-20 15:19:23 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.4666
2022-02-20 15:20:00 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.2753
2022-02-20 15:20:37 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.4592
2022-02-20 15:21:15 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.2408
2022-02-20 15:21:52 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.4145
2022-02-20 15:22:29 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.4443
2022-02-20 15:23:06 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.1507
2022-02-20 15:23:42 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.6967
2022-02-20 15:24:21 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.1462
2022-02-20 15:24:58 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.3472
2022-02-20 15:25:35 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.2762
2022-02-20 15:26:12 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.4703
2022-02-20 15:26:50 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.3160
2022-02-20 15:27:28 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.1291
2022-02-20 15:28:05 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.3788
2022-02-20 15:28:43 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.3086
2022-02-20 15:29:20 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.4283
2022-02-20 15:29:57 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.2449
2022-02-20 15:30:33 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.3587
2022-02-20 15:31:10 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.4313
2022-02-20 15:31:47 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.3651
2022-02-20 15:32:24 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.3322
2022-02-20 15:33:01 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.5323
2022-02-20 15:33:38 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.4493
2022-02-20 15:34:14 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.1646
2022-02-20 15:34:52 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.2433
2022-02-20 15:35:29 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.3963
2022-02-20 15:36:06 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.2871
2022-02-20 15:36:43 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.2008
2022-02-20 15:37:20 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.5016
2022-02-20 15:37:56 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.4535
2022-02-20 15:38:34 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.2917
2022-02-20 15:39:12 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2041
2022-02-20 15:39:49 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.3233
2022-02-20 15:40:26 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.2025
2022-02-20 15:41:04 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.2260
2022-02-20 15:41:41 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.5276
2022-02-20 15:42:17 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.5206
2022-02-20 15:42:54 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.3376
2022-02-20 15:43:31 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.2413
2022-02-20 15:44:09 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3532
2022-02-20 15:44:46 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.2741
2022-02-20 15:45:22 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.2315
2022-02-20 15:45:23 - train: epoch 019, train_loss: 2.3219
2022-02-20 15:46:46 - eval: epoch: 019, acc1: 51.538%, acc5: 77.260%, test_loss: 2.0734, per_image_load_time: 2.934ms, per_image_inference_time: 0.314ms
2022-02-20 15:46:47 - until epoch: 019, best_acc1: 51.538%
2022-02-20 15:46:47 - epoch 020 lr: 0.1
2022-02-20 15:47:29 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.5594
2022-02-20 15:48:05 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.0401
2022-02-20 15:48:43 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.4255
2022-02-20 15:49:21 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.1114
2022-02-20 15:49:59 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.2684
2022-02-20 15:50:35 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.5445
2022-02-20 15:51:13 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.0809
2022-02-20 15:51:51 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.3451
2022-02-20 15:52:28 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.5602
2022-02-20 15:53:07 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.3666
2022-02-20 15:53:43 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.2169
2022-02-20 15:54:20 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.0916
2022-02-20 15:54:57 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.1374
2022-02-20 15:55:34 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.5723
2022-02-20 15:56:12 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.4471
2022-02-20 15:56:49 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.1940
2022-02-20 15:57:26 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.9209
2022-02-20 15:58:04 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.2523
2022-02-20 15:58:40 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.2146
2022-02-20 15:59:18 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.2319
2022-02-20 15:59:55 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.4688
2022-02-20 16:00:32 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.1134
2022-02-20 16:01:08 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.2077
2022-02-20 16:01:45 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.5158
2022-02-20 16:02:22 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.1753
2022-02-20 16:02:59 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.1318
2022-02-20 16:03:36 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.3789
2022-02-20 16:04:13 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.3411
2022-02-20 16:04:50 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.4457
2022-02-20 16:05:27 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.2259
2022-02-20 16:06:04 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.3818
2022-02-20 16:06:41 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.5804
2022-02-20 16:07:19 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.0995
2022-02-20 16:07:56 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.4341
2022-02-20 16:08:34 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2127
2022-02-20 16:09:10 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.2876
2022-02-20 16:09:48 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.3735
2022-02-20 16:10:26 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.2491
2022-02-20 16:11:02 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.4573
2022-02-20 16:11:40 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.2312
2022-02-20 16:12:16 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.1613
2022-02-20 16:12:54 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.1928
2022-02-20 16:13:31 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.3529
2022-02-20 16:14:08 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.3669
2022-02-20 16:14:46 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.3492
2022-02-20 16:15:23 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.3666
2022-02-20 16:16:00 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.1804
2022-02-20 16:16:37 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.3805
2022-02-20 16:17:14 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.4104
2022-02-20 16:17:49 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.1138
2022-02-20 16:17:50 - train: epoch 020, train_loss: 2.3105
2022-02-20 16:19:14 - eval: epoch: 020, acc1: 50.650%, acc5: 76.488%, test_loss: 2.1287, per_image_load_time: 2.919ms, per_image_inference_time: 0.304ms
2022-02-20 16:19:14 - until epoch: 020, best_acc1: 51.538%
2022-02-20 16:19:14 - epoch 021 lr: 0.1
2022-02-20 16:19:56 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.2070
2022-02-20 16:20:34 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.3219
2022-02-20 16:21:11 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.0362
2022-02-20 16:21:49 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.3593
2022-02-20 16:22:26 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.2328
2022-02-20 16:23:04 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.1574
2022-02-20 16:23:41 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.1330
2022-02-20 16:24:18 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.4527
2022-02-20 16:24:56 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.3161
2022-02-20 16:25:34 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.1888
2022-02-20 16:26:12 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.2258
2022-02-20 16:26:48 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.2141
2022-02-20 16:27:22 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.2479
2022-02-20 16:27:57 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.3424
2022-02-20 16:28:36 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1827
2022-02-20 16:29:14 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.3144
2022-02-20 16:29:52 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.2372
2022-02-20 16:30:29 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.1011
2022-02-20 16:31:06 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.2636
2022-02-20 16:31:43 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.4658
2022-02-20 16:32:21 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1816
2022-02-20 16:32:58 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.2714
2022-02-20 16:33:35 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1899
2022-02-20 16:34:11 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2046
2022-02-20 16:34:49 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2474
2022-02-20 16:35:26 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.5731
2022-02-20 16:36:04 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.2573
2022-02-20 16:36:42 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.2053
2022-02-20 16:37:18 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.2789
2022-02-20 16:37:54 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.5250
2022-02-20 16:38:32 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.2460
2022-02-20 16:39:09 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.2733
2022-02-20 16:39:45 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.5150
2022-02-20 16:40:23 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.4477
2022-02-20 16:40:59 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.2769
2022-02-20 16:41:36 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.3671
2022-02-20 16:42:13 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2837
2022-02-20 16:42:52 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.1414
2022-02-20 16:43:28 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.2616
2022-02-20 16:44:05 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.5349
2022-02-20 16:44:43 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1541
2022-02-20 16:45:20 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.2811
2022-02-20 16:45:56 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.2794
2022-02-20 16:46:35 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4710
2022-02-20 16:47:10 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.4935
2022-02-20 16:47:49 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.1458
2022-02-20 16:48:25 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.4653
2022-02-20 16:49:03 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.5079
2022-02-20 16:49:40 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.0855
2022-02-20 16:50:16 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.1753
2022-02-20 16:50:17 - train: epoch 021, train_loss: 2.3066
2022-02-20 16:51:39 - eval: epoch: 021, acc1: 51.592%, acc5: 77.312%, test_loss: 2.0811, per_image_load_time: 2.321ms, per_image_inference_time: 0.295ms
2022-02-20 16:51:40 - until epoch: 021, best_acc1: 51.592%
2022-02-20 16:51:40 - epoch 022 lr: 0.1
2022-02-20 16:52:21 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.9988
2022-02-20 16:52:58 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1778
2022-02-20 16:53:38 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.0642
2022-02-20 16:54:13 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.0603
2022-02-20 16:54:51 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3080
2022-02-20 16:55:27 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.4479
2022-02-20 16:56:05 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.3695
2022-02-20 16:56:42 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.3955
2022-02-20 16:57:20 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4872
2022-02-20 16:57:57 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.3743
2022-02-20 16:58:34 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.2747
2022-02-20 16:59:10 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.9486
2022-02-20 16:59:47 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.2836
2022-02-20 17:00:24 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.3576
2022-02-20 17:01:02 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.2733
2022-02-20 17:01:39 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.1189
2022-02-20 17:02:15 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.0141
2022-02-20 17:02:52 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.6058
2022-02-20 17:03:30 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.1190
2022-02-20 17:04:06 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.3523
2022-02-20 17:04:44 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.3994
2022-02-20 17:05:20 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.0615
2022-02-20 17:05:57 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.3621
2022-02-20 17:06:33 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.2491
2022-02-20 17:07:11 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.2204
2022-02-20 17:07:48 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.0981
2022-02-20 17:08:25 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.0863
2022-02-20 17:09:01 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.6192
2022-02-20 17:09:38 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.0660
2022-02-20 17:10:15 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.3231
2022-02-20 17:10:53 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.4945
2022-02-20 17:11:29 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.4112
2022-02-20 17:12:07 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3450
2022-02-20 17:12:44 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.2246
2022-02-20 17:13:21 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4031
2022-02-20 17:13:58 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.2754
2022-02-20 17:14:35 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.3630
2022-02-20 17:15:12 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.3785
2022-02-20 17:15:49 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.2060
2022-02-20 17:16:25 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.3270
2022-02-20 17:17:04 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.2612
2022-02-20 17:17:41 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2440
2022-02-20 17:18:18 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.5174
2022-02-20 17:18:55 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.3162
2022-02-20 17:19:33 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.3217
2022-02-20 17:20:09 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.4775
2022-02-20 17:20:46 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.2996
2022-02-20 17:21:23 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.2237
2022-02-20 17:22:01 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.9947
2022-02-20 17:22:36 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.1762
2022-02-20 17:22:37 - train: epoch 022, train_loss: 2.3040
2022-02-20 17:24:00 - eval: epoch: 022, acc1: 51.514%, acc5: 77.494%, test_loss: 2.0710, per_image_load_time: 2.927ms, per_image_inference_time: 0.294ms
2022-02-20 17:24:01 - until epoch: 022, best_acc1: 51.592%
2022-02-20 17:24:01 - epoch 023 lr: 0.1
2022-02-20 17:24:43 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2100
2022-02-20 17:25:21 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.0018
2022-02-20 17:25:59 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.0922
2022-02-20 17:26:36 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.3543
2022-02-20 17:27:12 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.3003
2022-02-20 17:27:50 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.1803
2022-02-20 17:28:27 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.0911
2022-02-20 17:29:04 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.1998
2022-02-20 17:29:42 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.2701
2022-02-20 17:30:20 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.1351
2022-02-20 17:30:56 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.3696
2022-02-20 17:31:33 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.2096
2022-02-20 17:32:12 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.3496
2022-02-20 17:32:48 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3715
2022-02-20 17:33:26 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.1193
2022-02-20 17:34:03 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.2611
2022-02-20 17:34:40 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.3883
2022-02-20 17:35:18 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.2249
2022-02-20 17:35:54 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.4246
2022-02-20 17:36:32 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.9349
2022-02-20 17:37:09 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.5083
2022-02-20 17:37:46 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.0474
2022-02-20 17:38:24 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.1978
2022-02-20 17:39:01 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.2771
2022-02-20 17:39:38 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.5113
2022-02-20 17:40:14 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.3312
2022-02-20 17:40:51 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.2057
2022-02-20 17:41:27 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.3836
2022-02-20 17:42:05 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.2988
2022-02-20 17:42:42 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.4769
2022-02-20 17:43:20 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.3489
2022-02-20 17:43:57 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.3934
2022-02-20 17:44:33 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.3674
2022-02-20 17:45:11 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.3380
2022-02-20 17:45:47 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.2707
2022-02-20 17:46:25 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.0339
2022-02-20 17:47:02 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.3662
2022-02-20 17:47:39 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.2649
2022-02-20 17:48:16 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.2242
2022-02-20 17:48:53 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.3722
2022-02-20 17:49:31 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.1225
2022-02-20 17:50:08 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.1483
2022-02-20 17:50:46 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.1444
2022-02-20 17:51:21 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1697
2022-02-20 17:51:58 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.1689
2022-02-20 17:52:36 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.2923
2022-02-20 17:53:13 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.1222
2022-02-20 17:53:50 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.1904
2022-02-20 17:54:28 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.1504
2022-02-20 17:55:03 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.3517
2022-02-20 17:55:04 - train: epoch 023, train_loss: 2.2994
2022-02-20 17:56:28 - eval: epoch: 023, acc1: 51.866%, acc5: 77.794%, test_loss: 2.0556, per_image_load_time: 2.940ms, per_image_inference_time: 0.289ms
2022-02-20 17:56:29 - until epoch: 023, best_acc1: 51.866%
2022-02-20 17:56:29 - epoch 024 lr: 0.1
2022-02-20 17:57:11 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.1919
2022-02-20 17:57:49 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.3127
2022-02-20 17:58:25 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.2182
2022-02-20 17:59:03 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.4146
2022-02-20 17:59:39 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.2001
2022-02-20 18:00:17 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.0517
2022-02-20 18:00:54 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.2163
2022-02-20 18:01:32 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.1812
2022-02-20 18:02:08 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.2451
2022-02-20 18:02:45 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.2943
2022-02-20 18:03:22 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.9187
2022-02-20 18:04:00 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.0983
2022-02-20 18:04:37 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.6630
2022-02-20 18:05:14 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.1462
2022-02-20 18:05:51 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.5014
2022-02-20 18:06:28 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.2618
2022-02-20 18:07:05 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.2653
2022-02-20 18:07:42 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.5057
2022-02-20 18:08:18 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.0954
2022-02-20 18:08:57 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.3309
2022-02-20 18:09:32 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.2551
2022-02-20 18:10:10 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0434
2022-02-20 18:10:47 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.2222
2022-02-20 18:11:25 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.2732
2022-02-20 18:12:01 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.2551
2022-02-20 18:12:39 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.2587
2022-02-20 18:13:16 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.5338
2022-02-20 18:13:52 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.3511
2022-02-20 18:14:29 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.3213
2022-02-20 18:15:08 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.2099
2022-02-20 18:15:44 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.2426
2022-02-20 18:16:21 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.4786
2022-02-20 18:16:58 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.9408
2022-02-20 18:17:36 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.3335
2022-02-20 18:18:13 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.1593
2022-02-20 18:18:50 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.3594
2022-02-20 18:19:27 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.2620
2022-02-20 18:20:05 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.5048
2022-02-20 18:20:42 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.2426
2022-02-20 18:21:20 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.3021
2022-02-20 18:21:57 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.2906
2022-02-20 18:22:35 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.3533
2022-02-20 18:23:11 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.2845
2022-02-20 18:23:48 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.3413
2022-02-20 18:24:24 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.1042
2022-02-20 18:25:02 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.3778
2022-02-20 18:25:39 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.2574
2022-02-20 18:26:16 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.1530
2022-02-20 18:26:53 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3187
2022-02-20 18:27:28 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.3722
2022-02-20 18:27:29 - train: epoch 024, train_loss: 2.2949
2022-02-20 18:28:52 - eval: epoch: 024, acc1: 51.248%, acc5: 77.060%, test_loss: 2.0901, per_image_load_time: 2.924ms, per_image_inference_time: 0.291ms
2022-02-20 18:28:53 - until epoch: 024, best_acc1: 51.866%
2022-02-20 18:28:53 - epoch 025 lr: 0.1
2022-02-20 18:29:35 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.1407
2022-02-20 18:30:11 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.0781
2022-02-20 18:30:49 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.1140
2022-02-20 18:31:25 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.2519
2022-02-20 18:32:03 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.1896
2022-02-20 18:32:38 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.2614
2022-02-20 18:33:12 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.4234
2022-02-20 18:33:49 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.2721
2022-02-20 18:34:27 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.2056
2022-02-20 18:35:05 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.2399
2022-02-20 18:35:42 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.2792
2022-02-20 18:36:20 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.3642
2022-02-20 18:36:56 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.2788
2022-02-20 18:37:32 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.3396
2022-02-20 18:38:10 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.2322
2022-02-20 18:38:46 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.0665
2022-02-20 18:39:24 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.2151
2022-02-20 18:40:00 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.1258
2022-02-20 18:40:38 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.1638
2022-02-20 18:41:15 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.1999
2022-02-20 18:41:52 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.0442
2022-02-20 18:42:30 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.0602
2022-02-20 18:43:08 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.3577
2022-02-20 18:43:45 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.1413
2022-02-20 18:44:22 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.3571
2022-02-20 18:44:59 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.3849
2022-02-20 18:45:38 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.3917
2022-02-20 18:46:13 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2620
2022-02-20 18:46:52 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.4619
2022-02-20 18:47:28 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.5607
2022-02-20 18:48:05 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.1637
2022-02-20 18:48:43 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.5507
2022-02-20 18:49:20 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.2959
2022-02-20 18:49:57 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4042
2022-02-20 18:50:34 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.0606
2022-02-20 18:51:12 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.4038
2022-02-20 18:51:48 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3670
2022-02-20 18:52:25 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.3327
2022-02-20 18:53:02 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.4601
2022-02-20 18:53:40 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.5280
2022-02-20 18:54:16 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.4797
2022-02-20 18:54:54 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.3431
2022-02-20 18:55:31 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1716
2022-02-20 18:56:08 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.1292
2022-02-20 18:56:44 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.2698
2022-02-20 18:57:23 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.2913
2022-02-20 18:57:59 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.2317
2022-02-20 18:58:37 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.0471
2022-02-20 18:59:14 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.3189
2022-02-20 18:59:49 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.4723
2022-02-20 18:59:50 - train: epoch 025, train_loss: 2.2912
2022-02-20 19:01:13 - eval: epoch: 025, acc1: 53.068%, acc5: 78.528%, test_loss: 1.9997, per_image_load_time: 2.956ms, per_image_inference_time: 0.279ms
2022-02-20 19:01:14 - until epoch: 025, best_acc1: 53.068%
2022-02-20 19:01:14 - epoch 026 lr: 0.1
2022-02-20 19:01:55 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1228
2022-02-20 19:02:33 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.1002
2022-02-20 19:03:10 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.2385
2022-02-20 19:03:47 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.1746
2022-02-20 19:04:24 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.4025
2022-02-20 19:05:01 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.3169
2022-02-20 19:05:39 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2005
2022-02-20 19:06:16 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.0803
2022-02-20 19:06:53 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.5254
2022-02-20 19:07:30 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.1905
2022-02-20 19:08:07 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.2388
2022-02-20 19:08:45 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.3967
2022-02-20 19:09:21 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2387
2022-02-20 19:09:58 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.4335
2022-02-20 19:10:35 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3157
2022-02-20 19:11:14 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.3560
2022-02-20 19:11:50 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.2242
2022-02-20 19:12:27 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.4029
2022-02-20 19:13:04 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.5319
2022-02-20 19:13:41 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.4121
2022-02-20 19:14:18 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.4531
2022-02-20 19:14:56 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.3151
2022-02-20 19:15:32 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.2886
2022-02-20 19:16:10 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.4871
2022-02-20 19:16:47 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.4028
2022-02-20 19:17:25 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.2924
2022-02-20 19:18:01 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.2946
2022-02-20 19:18:38 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.0973
2022-02-20 19:19:15 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.4104
2022-02-20 19:19:51 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.2648
2022-02-20 19:20:29 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.1986
2022-02-20 19:21:06 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.2537
2022-02-20 19:21:42 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.1121
2022-02-20 19:22:21 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.5167
2022-02-20 19:22:58 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.4011
2022-02-20 19:23:34 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2134
2022-02-20 19:24:11 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4239
2022-02-20 19:24:49 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.4198
2022-02-20 19:25:27 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.4071
2022-02-20 19:26:04 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.5164
2022-02-20 19:26:41 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.4016
2022-02-20 19:27:18 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.3464
2022-02-20 19:27:56 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.3595
2022-02-20 19:28:32 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.5106
2022-02-20 19:29:10 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.6318
2022-02-20 19:29:46 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.2092
2022-02-20 19:30:24 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.3066
2022-02-20 19:31:01 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2606
2022-02-20 19:31:38 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.3427
2022-02-20 19:32:13 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5031
2022-02-20 19:32:14 - train: epoch 026, train_loss: 2.2869
2022-02-20 19:33:37 - eval: epoch: 026, acc1: 50.672%, acc5: 76.862%, test_loss: 2.1163, per_image_load_time: 2.903ms, per_image_inference_time: 0.288ms
2022-02-20 19:33:38 - until epoch: 026, best_acc1: 53.068%
2022-02-20 19:33:38 - epoch 027 lr: 0.1
2022-02-20 19:34:20 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.4883
2022-02-20 19:34:57 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.1944
2022-02-20 19:35:34 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.2596
2022-02-20 19:36:12 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.5064
2022-02-20 19:36:49 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3424
2022-02-20 19:37:26 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.5327
2022-02-20 19:38:02 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.5729
2022-02-20 19:38:40 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3007
2022-02-20 19:39:18 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.1661
2022-02-20 19:39:55 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.3936
2022-02-20 19:40:33 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.1722
2022-02-20 19:41:10 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.5273
2022-02-20 19:41:46 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.4653
2022-02-20 19:42:23 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.4785
2022-02-20 19:43:02 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.3237
2022-02-20 19:43:38 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.2972
2022-02-20 19:44:15 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.2980
2022-02-20 19:44:51 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.2664
2022-02-20 19:45:28 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.5497
2022-02-20 19:46:07 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.2539
2022-02-20 19:46:44 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.4983
2022-02-20 19:47:21 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.4210
2022-02-20 19:47:59 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.6468
2022-02-20 19:48:37 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.2228
2022-02-20 19:49:14 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.1709
2022-02-20 19:49:51 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.4924
2022-02-20 19:50:27 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.2835
2022-02-20 19:51:05 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.5125
2022-02-20 19:51:41 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.2918
2022-02-20 19:52:19 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1345
2022-02-20 19:52:56 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.1225
2022-02-20 19:53:33 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.1947
2022-02-20 19:54:11 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.3605
2022-02-20 19:54:48 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.2604
2022-02-20 19:55:25 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.6328
2022-02-20 19:56:02 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.0969
2022-02-20 19:56:39 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.1870
2022-02-20 19:57:16 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.0876
2022-02-20 19:57:54 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.2289
2022-02-20 19:58:31 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.4176
2022-02-20 19:59:08 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.2717
2022-02-20 19:59:45 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.3352
2022-02-20 20:00:22 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.1081
2022-02-20 20:01:01 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3343
2022-02-20 20:01:36 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.1201
2022-02-20 20:02:14 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.2513
2022-02-20 20:02:51 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.3225
2022-02-20 20:03:28 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.3539
2022-02-20 20:04:05 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.3761
2022-02-20 20:04:41 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.9186
2022-02-20 20:04:42 - train: epoch 027, train_loss: 2.2845
2022-02-20 20:06:05 - eval: epoch: 027, acc1: 52.758%, acc5: 78.204%, test_loss: 2.0140, per_image_load_time: 2.947ms, per_image_inference_time: 0.281ms
2022-02-20 20:06:05 - until epoch: 027, best_acc1: 53.068%
2022-02-20 20:06:05 - epoch 028 lr: 0.1
2022-02-20 20:06:48 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.0584
2022-02-20 20:07:25 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.1512
2022-02-20 20:08:03 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.2094
2022-02-20 20:08:40 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.1104
2022-02-20 20:09:18 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.2378
2022-02-20 20:09:54 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.4022
2022-02-20 20:10:31 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.5526
2022-02-20 20:11:09 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.9339
2022-02-20 20:11:46 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.0596
2022-02-20 20:12:23 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.4290
2022-02-20 20:12:59 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.3273
2022-02-20 20:13:37 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.1011
2022-02-20 20:14:14 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.2203
2022-02-20 20:14:52 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.5258
2022-02-20 20:15:29 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.1982
2022-02-20 20:16:06 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.2906
2022-02-20 20:16:42 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.2497
2022-02-20 20:17:20 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.2174
2022-02-20 20:17:57 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.2010
2022-02-20 20:18:35 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.4793
2022-02-20 20:19:11 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.3565
2022-02-20 20:19:48 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.3070
2022-02-20 20:20:24 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.3962
2022-02-20 20:21:02 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.6499
2022-02-20 20:21:39 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.1480
2022-02-20 20:22:17 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.1643
2022-02-20 20:22:53 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.3875
2022-02-20 20:23:30 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.2209
2022-02-20 20:24:08 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.2911
2022-02-20 20:24:43 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.3214
2022-02-20 20:25:21 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.4119
2022-02-20 20:25:59 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.2884
2022-02-20 20:26:35 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.3159
2022-02-20 20:27:12 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.1554
2022-02-20 20:27:48 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.1508
2022-02-20 20:28:27 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.0874
2022-02-20 20:29:03 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.2715
2022-02-20 20:29:41 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.9687
2022-02-20 20:30:18 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.2081
2022-02-20 20:30:55 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.3702
2022-02-20 20:31:31 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.3758
2022-02-20 20:32:09 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.3854
2022-02-20 20:32:46 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.2254
2022-02-20 20:33:24 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.2499
2022-02-20 20:34:01 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.2946
2022-02-20 20:34:37 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.3000
2022-02-20 20:35:15 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.4072
2022-02-20 20:35:52 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.2165
2022-02-20 20:36:30 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.2756
2022-02-20 20:37:05 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.2697
2022-02-20 20:37:06 - train: epoch 028, train_loss: 2.2799
2022-02-20 20:38:27 - eval: epoch: 028, acc1: 51.978%, acc5: 77.472%, test_loss: 2.0561, per_image_load_time: 2.810ms, per_image_inference_time: 0.292ms
2022-02-20 20:38:27 - until epoch: 028, best_acc1: 53.068%
2022-02-20 20:38:27 - epoch 029 lr: 0.1
2022-02-20 20:39:09 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.3664
2022-02-20 20:39:48 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.3363
2022-02-20 20:40:27 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.3761
2022-02-20 20:41:04 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.1416
2022-02-20 20:41:41 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3278
2022-02-20 20:42:17 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.5411
2022-02-20 20:42:56 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.8872
2022-02-20 20:43:32 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.3831
2022-02-20 20:44:10 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1489
2022-02-20 20:44:47 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.1442
2022-02-20 20:45:23 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.1637
2022-02-20 20:46:01 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.3073
2022-02-20 20:46:39 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.3358
2022-02-20 20:47:16 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.4789
2022-02-20 20:47:53 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.3114
2022-02-20 20:48:30 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.2345
2022-02-20 20:49:06 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.2852
2022-02-20 20:49:44 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.3739
2022-02-20 20:50:21 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.0351
2022-02-20 20:50:57 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.4750
2022-02-20 20:51:34 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.2976
2022-02-20 20:52:11 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.4913
2022-02-20 20:52:49 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.3423
2022-02-20 20:53:27 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.1798
2022-02-20 20:54:04 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2596
2022-02-20 20:54:41 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.3017
2022-02-20 20:55:17 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.1495
2022-02-20 20:55:55 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.1420
2022-02-20 20:56:31 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.2326
2022-02-20 20:57:10 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.2208
2022-02-20 20:57:46 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.2306
2022-02-20 20:58:22 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.2045
2022-02-20 20:58:59 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.1903
2022-02-20 20:59:36 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.1397
2022-02-20 21:00:13 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.4167
2022-02-20 21:00:51 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.2296
2022-02-20 21:01:28 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.2021
2022-02-20 21:02:04 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.2172
2022-02-20 21:02:42 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.0731
2022-02-20 21:03:19 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.1763
2022-02-20 21:03:56 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.2506
2022-02-20 21:04:33 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.1671
2022-02-20 21:05:11 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.4078
2022-02-20 21:05:46 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.1883
2022-02-20 21:06:25 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.3251
2022-02-20 21:07:00 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.5470
2022-02-20 21:07:38 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.0110
2022-02-20 21:08:15 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.3330
2022-02-20 21:08:53 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.6152
2022-02-20 21:09:28 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.0247
2022-02-20 21:09:29 - train: epoch 029, train_loss: 2.2773
2022-02-20 21:10:53 - eval: epoch: 029, acc1: 52.124%, acc5: 77.704%, test_loss: 2.0394, per_image_load_time: 2.973ms, per_image_inference_time: 0.273ms
2022-02-20 21:10:53 - until epoch: 029, best_acc1: 53.068%
2022-02-20 21:10:53 - epoch 030 lr: 0.1
2022-02-20 21:11:36 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.3667
2022-02-20 21:12:14 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.3870
2022-02-20 21:12:50 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.2467
2022-02-20 21:13:26 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.0432
2022-02-20 21:14:04 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.6445
2022-02-20 21:14:42 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.1676
2022-02-20 21:15:20 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.2639
2022-02-20 21:15:57 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.3394
2022-02-20 21:16:34 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.2958
2022-02-20 21:17:10 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.0039
2022-02-20 21:17:49 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.0445
2022-02-20 21:18:26 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.3038
2022-02-20 21:19:03 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.9762
2022-02-20 21:19:40 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.1357
2022-02-20 21:20:17 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.2648
2022-02-20 21:20:55 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.3522
2022-02-20 21:21:32 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.4389
2022-02-20 21:22:09 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.1537
2022-02-20 21:22:47 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.5295
2022-02-20 21:23:23 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.2586
2022-02-20 21:24:00 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.3563
2022-02-20 21:24:38 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.2766
2022-02-20 21:25:15 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.3336
2022-02-20 21:25:51 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4925
2022-02-20 21:26:28 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2741
2022-02-20 21:27:06 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.1968
2022-02-20 21:27:44 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.1891
2022-02-20 21:28:21 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3186
2022-02-20 21:28:59 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.3856
2022-02-20 21:29:36 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.4452
2022-02-20 21:30:13 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.3283
2022-02-20 21:30:50 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.1724
2022-02-20 21:31:28 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.4523
2022-02-20 21:32:04 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.2778
2022-02-20 21:32:43 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.4279
2022-02-20 21:33:19 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.1376
2022-02-20 21:33:56 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.1702
2022-02-20 21:34:32 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.4665
2022-02-20 21:35:10 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3487
2022-02-20 21:35:47 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.1550
2022-02-20 21:36:25 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.1507
2022-02-20 21:37:03 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5857
2022-02-20 21:37:40 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.2109
2022-02-20 21:38:17 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.3033
2022-02-20 21:38:54 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.4946
2022-02-20 21:39:31 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.0209
2022-02-20 21:40:08 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.2899
2022-02-20 21:40:45 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.2799
2022-02-20 21:41:22 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.4648
2022-02-20 21:41:58 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.3635
2022-02-20 21:41:59 - train: epoch 030, train_loss: 2.2793
2022-02-20 21:43:22 - eval: epoch: 030, acc1: 52.502%, acc5: 78.238%, test_loss: 2.0156, per_image_load_time: 2.956ms, per_image_inference_time: 0.307ms
2022-02-20 21:43:23 - until epoch: 030, best_acc1: 53.068%
2022-02-20 21:43:23 - epoch 031 lr: 0.010000000000000002
2022-02-20 21:44:05 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.1741
2022-02-20 21:44:43 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.8651
2022-02-20 21:45:19 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.8558
2022-02-20 21:45:57 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.9925
2022-02-20 21:46:35 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.7774
2022-02-20 21:47:12 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.7391
2022-02-20 21:47:48 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.7643
2022-02-20 21:48:27 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.7021
2022-02-20 21:49:02 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.7455
2022-02-20 21:49:41 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.0018
2022-02-20 21:50:18 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.0197
2022-02-20 21:50:56 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.8511
2022-02-20 21:51:33 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.5859
2022-02-20 21:52:09 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.7821
2022-02-20 21:52:46 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.8419
2022-02-20 21:53:23 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.6107
2022-02-20 21:54:01 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.5997
2022-02-20 21:54:38 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.6748
2022-02-20 21:55:15 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.7776
2022-02-20 21:55:51 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.7509
2022-02-20 21:56:28 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6426
2022-02-20 21:57:06 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.5415
2022-02-20 21:57:43 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.5974
2022-02-20 21:58:20 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.6694
2022-02-20 21:58:56 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.6216
2022-02-20 21:59:34 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.7000
2022-02-20 22:00:10 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.7237
2022-02-20 22:00:48 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.9756
2022-02-20 22:01:26 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.8008
2022-02-20 22:02:02 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.8098
2022-02-20 22:02:39 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6277
2022-02-20 22:03:15 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.8072
2022-02-20 22:03:52 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8731
2022-02-20 22:04:29 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.8460
2022-02-20 22:05:08 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.7916
2022-02-20 22:05:44 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.8209
2022-02-20 22:06:20 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.6558
2022-02-20 22:06:58 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.5473
2022-02-20 22:07:36 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.6151
2022-02-20 22:08:14 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.5135
2022-02-20 22:08:51 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.6510
2022-02-20 22:09:28 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.7210
2022-02-20 22:10:04 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.6475
2022-02-20 22:10:42 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.8002
2022-02-20 22:11:20 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.8338
2022-02-20 22:11:57 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7042
2022-02-20 22:12:35 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.8127
2022-02-20 22:13:12 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.6497
2022-02-20 22:13:48 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.5824
2022-02-20 22:14:25 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.5950
2022-02-20 22:14:26 - train: epoch 031, train_loss: 1.7432
2022-02-20 22:15:49 - eval: epoch: 031, acc1: 66.166%, acc5: 87.356%, test_loss: 1.3704, per_image_load_time: 2.931ms, per_image_inference_time: 0.299ms
2022-02-20 22:15:50 - until epoch: 031, best_acc1: 66.166%
2022-02-20 22:15:50 - epoch 032 lr: 0.010000000000000002
2022-02-20 22:16:32 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.6219
2022-02-20 22:17:09 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.6994
2022-02-20 22:17:47 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.6734
2022-02-20 22:18:23 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.7485
2022-02-20 22:19:02 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.7457
2022-02-20 22:19:38 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.7591
2022-02-20 22:20:15 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6658
2022-02-20 22:20:53 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.6289
2022-02-20 22:21:31 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.5946
2022-02-20 22:22:09 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7669
2022-02-20 22:22:46 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.7011
2022-02-20 22:23:23 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.6650
2022-02-20 22:24:00 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.5366
2022-02-20 22:24:36 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.7523
2022-02-20 22:25:14 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.5839
2022-02-20 22:25:52 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.7309
2022-02-20 22:26:29 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.7465
2022-02-20 22:27:06 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.8786
2022-02-20 22:27:43 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.3590
2022-02-20 22:28:21 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.5950
2022-02-20 22:28:57 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.4958
2022-02-20 22:29:34 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.4434
2022-02-20 22:30:12 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.6556
2022-02-20 22:30:49 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.5628
2022-02-20 22:31:26 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7092
2022-02-20 22:32:04 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.4168
2022-02-20 22:32:42 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.5778
2022-02-20 22:33:18 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.7419
2022-02-20 22:33:56 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.4030
2022-02-20 22:34:33 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.4155
2022-02-20 22:35:11 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.7241
2022-02-20 22:35:48 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.5560
2022-02-20 22:36:25 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.5059
2022-02-20 22:37:00 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.4686
2022-02-20 22:37:38 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.6528
2022-02-20 22:38:15 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.7288
2022-02-20 22:38:52 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.6331
2022-02-20 22:39:30 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4322
2022-02-20 22:40:08 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.6809
2022-02-20 22:40:45 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4574
2022-02-20 22:41:23 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.6234
2022-02-20 22:42:00 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.4981
2022-02-20 22:42:37 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.8623
2022-02-20 22:43:15 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.5248
2022-02-20 22:43:51 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8109
2022-02-20 22:44:25 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.5939
2022-02-20 22:45:00 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.7279
2022-02-20 22:45:39 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.4207
2022-02-20 22:46:17 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.7846
2022-02-20 22:46:53 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6150
2022-02-20 22:46:54 - train: epoch 032, train_loss: 1.6193
2022-02-20 22:48:17 - eval: epoch: 032, acc1: 67.056%, acc5: 87.898%, test_loss: 1.3265, per_image_load_time: 2.962ms, per_image_inference_time: 0.285ms
2022-02-20 22:48:18 - until epoch: 032, best_acc1: 67.056%
2022-02-20 22:48:18 - epoch 033 lr: 0.010000000000000002
2022-02-20 22:49:01 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.3254
2022-02-20 22:49:38 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.6994
2022-02-20 22:50:16 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.4733
2022-02-20 22:50:53 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4902
2022-02-20 22:51:30 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.6269
2022-02-20 22:52:07 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.3544
2022-02-20 22:52:45 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.6318
2022-02-20 22:53:22 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.6491
2022-02-20 22:53:59 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.5957
2022-02-20 22:54:38 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.5451
2022-02-20 22:55:15 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.4856
2022-02-20 22:55:52 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.5849
2022-02-20 22:56:29 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.4715
2022-02-20 22:57:08 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.7675
2022-02-20 22:57:45 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.8245
2022-02-20 22:58:22 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.7273
2022-02-20 22:58:57 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.3938
2022-02-20 22:59:36 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.7659
2022-02-20 23:00:13 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.6640
2022-02-20 23:00:50 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.4076
2022-02-20 23:01:28 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.5719
2022-02-20 23:02:09 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.6040
2022-02-20 23:02:50 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.4090
2022-02-20 23:03:29 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.8153
2022-02-20 23:04:07 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.5658
2022-02-20 23:04:49 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.3446
2022-02-20 23:05:28 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.6565
2022-02-20 23:06:09 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.5798
2022-02-20 23:06:46 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.6411
2022-02-20 23:07:24 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.6174
2022-02-20 23:08:01 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.5986
2022-02-20 23:08:39 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.5591
2022-02-20 23:09:15 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.6604
2022-02-20 23:09:53 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.4969
2022-02-20 23:10:29 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.5991
2022-02-20 23:11:07 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.7663
2022-02-20 23:11:44 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.5253
2022-02-20 23:12:22 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.4703
2022-02-20 23:12:59 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.6242
2022-02-20 23:13:37 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.6253
2022-02-20 23:14:14 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.5581
2022-02-20 23:14:52 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.4727
2022-02-20 23:15:29 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.6884
2022-02-20 23:16:06 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.6627
2022-02-20 23:16:43 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.7857
2022-02-20 23:17:21 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.5059
2022-02-20 23:17:59 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.4075
2022-02-20 23:18:36 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.9326
2022-02-20 23:19:13 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.3723
2022-02-20 23:19:49 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.5059
2022-02-20 23:19:50 - train: epoch 033, train_loss: 1.5683
2022-02-20 23:21:13 - eval: epoch: 033, acc1: 67.572%, acc5: 88.278%, test_loss: 1.3036, per_image_load_time: 2.918ms, per_image_inference_time: 0.300ms
2022-02-20 23:21:14 - until epoch: 033, best_acc1: 67.572%
2022-02-20 23:21:14 - epoch 034 lr: 0.010000000000000002
2022-02-20 23:21:57 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.4936
2022-02-20 23:22:34 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.5358
2022-02-20 23:23:11 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.5275
2022-02-20 23:23:49 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.3203
2022-02-20 23:24:27 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.5836
2022-02-20 23:25:04 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.8397
2022-02-20 23:25:41 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.5777
2022-02-20 23:26:19 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.4902
2022-02-20 23:26:57 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.5483
2022-02-20 23:27:34 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.3391
2022-02-20 23:28:11 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.5155
2022-02-20 23:28:50 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.4991
2022-02-20 23:29:27 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.4570
2022-02-20 23:30:05 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.5855
2022-02-20 23:30:43 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.3502
2022-02-20 23:31:21 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.4140
2022-02-20 23:31:58 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.5149
2022-02-20 23:32:36 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.7245
2022-02-20 23:33:13 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.6023
2022-02-20 23:33:52 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.5294
2022-02-20 23:34:28 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.7311
2022-02-20 23:35:05 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.4578
2022-02-20 23:35:43 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.6019
2022-02-20 23:36:21 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.4325
2022-02-20 23:37:00 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.5195
2022-02-20 23:37:37 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.5711
2022-02-20 23:38:15 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.6424
2022-02-20 23:38:51 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.3692
2022-02-20 23:39:30 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3179
2022-02-20 23:40:08 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.2996
2022-02-20 23:40:45 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.5552
2022-02-20 23:41:23 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.5775
2022-02-20 23:42:00 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.5068
2022-02-20 23:42:38 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.5959
2022-02-20 23:43:16 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4056
2022-02-20 23:43:53 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.3695
2022-02-20 23:44:31 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.4090
2022-02-20 23:45:08 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.4573
2022-02-20 23:45:46 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.5969
2022-02-20 23:46:24 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.3326
2022-02-20 23:47:01 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.4986
2022-02-20 23:47:38 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.5339
2022-02-20 23:48:16 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5082
2022-02-20 23:48:53 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6304
2022-02-20 23:49:30 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.7872
2022-02-20 23:50:08 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.7168
2022-02-20 23:50:44 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.4973
2022-02-20 23:51:22 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5044
2022-02-20 23:51:59 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.5325
2022-02-20 23:52:35 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.4195
2022-02-20 23:52:36 - train: epoch 034, train_loss: 1.5381
2022-02-20 23:54:00 - eval: epoch: 034, acc1: 67.628%, acc5: 88.376%, test_loss: 1.3015, per_image_load_time: 2.934ms, per_image_inference_time: 0.291ms
2022-02-20 23:54:01 - until epoch: 034, best_acc1: 67.628%
2022-02-20 23:54:01 - epoch 035 lr: 0.010000000000000002
2022-02-20 23:54:44 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.4366
2022-02-20 23:55:21 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.2809
2022-02-20 23:55:59 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.6184
2022-02-20 23:56:36 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.3884
2022-02-20 23:57:14 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.5438
2022-02-20 23:57:50 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.4823
2022-02-20 23:58:28 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.4336
2022-02-20 23:59:05 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.5736
2022-02-20 23:59:44 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.6314
2022-02-21 00:00:21 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.6602
2022-02-21 00:00:58 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.7092
2022-02-21 00:01:35 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.4367
2022-02-21 00:02:13 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.6231
2022-02-21 00:02:51 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.4345
2022-02-21 00:03:29 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.5363
2022-02-21 00:04:06 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.5261
2022-02-21 00:04:42 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.3691
2022-02-21 00:05:20 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.5605
2022-02-21 00:05:58 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.5426
2022-02-21 00:06:35 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.4417
2022-02-21 00:07:12 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.4670
2022-02-21 00:07:50 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.6297
2022-02-21 00:08:28 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.5347
2022-02-21 00:09:05 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.5988
2022-02-21 00:09:43 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.4268
2022-02-21 00:10:21 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.6959
2022-02-21 00:10:57 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.6247
2022-02-21 00:11:35 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.4490
2022-02-21 00:12:12 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.4586
2022-02-21 00:12:50 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.6460
2022-02-21 00:13:27 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.4830
2022-02-21 00:14:04 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.3456
2022-02-21 00:14:42 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.4638
2022-02-21 00:15:18 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.5142
2022-02-21 00:15:55 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.2719
2022-02-21 00:16:33 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.5409
2022-02-21 00:17:10 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.3982
2022-02-21 00:17:46 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.5576
2022-02-21 00:18:25 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.5551
2022-02-21 00:19:01 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.3705
2022-02-21 00:19:40 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.6993
2022-02-21 00:20:16 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4244
2022-02-21 00:20:54 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.5770
2022-02-21 00:21:31 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.5817
2022-02-21 00:22:09 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.6524
2022-02-21 00:22:46 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.4591
2022-02-21 00:23:22 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.7454
2022-02-21 00:24:01 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.6135
2022-02-21 00:24:38 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.5810
2022-02-21 00:25:14 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.4294
2022-02-21 00:25:15 - train: epoch 035, train_loss: 1.5170
2022-02-21 00:26:39 - eval: epoch: 035, acc1: 67.700%, acc5: 88.384%, test_loss: 1.2954, per_image_load_time: 2.955ms, per_image_inference_time: 0.275ms
2022-02-21 00:26:40 - until epoch: 035, best_acc1: 67.700%
2022-02-21 00:26:40 - epoch 036 lr: 0.010000000000000002
2022-02-21 00:27:23 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.5903
2022-02-21 00:28:00 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.2130
2022-02-21 00:28:37 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.3282
2022-02-21 00:29:15 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.5318
2022-02-21 00:29:51 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.5110
2022-02-21 00:30:30 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.3930
2022-02-21 00:31:07 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.3953
2022-02-21 00:31:45 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.3049
2022-02-21 00:32:21 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.4251
2022-02-21 00:32:58 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.4026
2022-02-21 00:33:34 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.5978
2022-02-21 00:34:13 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.5002
2022-02-21 00:34:50 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.4998
2022-02-21 00:35:27 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.7189
2022-02-21 00:36:04 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.7635
2022-02-21 00:36:41 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.4847
2022-02-21 00:37:19 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.4900
2022-02-21 00:37:57 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.4058
2022-02-21 00:38:33 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.4930
2022-02-21 00:39:11 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.5145
2022-02-21 00:39:48 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.5300
2022-02-21 00:40:25 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.4461
2022-02-21 00:41:02 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.5799
2022-02-21 00:41:40 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.5692
2022-02-21 00:42:17 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.3536
2022-02-21 00:42:55 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.6584
2022-02-21 00:43:30 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.4708
2022-02-21 00:44:07 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.3920
2022-02-21 00:44:44 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.3114
2022-02-21 00:45:22 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.6402
2022-02-21 00:45:59 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.4191
2022-02-21 00:46:36 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.4676
2022-02-21 00:47:12 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.3884
2022-02-21 00:47:49 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.4592
2022-02-21 00:48:26 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.4606
2022-02-21 00:49:03 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.5888
2022-02-21 00:49:38 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.6132
2022-02-21 00:50:13 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.4041
2022-02-21 00:50:51 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.5127
2022-02-21 00:51:30 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.4784
2022-02-21 00:52:07 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.5102
2022-02-21 00:52:45 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.5097
2022-02-21 00:53:21 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.3946
2022-02-21 00:53:58 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.4917
2022-02-21 00:54:36 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.1750
2022-02-21 00:55:15 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.3884
2022-02-21 00:55:51 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.5594
2022-02-21 00:56:28 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.4676
2022-02-21 00:57:06 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.4963
2022-02-21 00:57:41 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.4837
2022-02-21 00:57:42 - train: epoch 036, train_loss: 1.5024
2022-02-21 00:59:06 - eval: epoch: 036, acc1: 68.316%, acc5: 88.636%, test_loss: 1.2782, per_image_load_time: 2.968ms, per_image_inference_time: 0.299ms
2022-02-21 00:59:07 - until epoch: 036, best_acc1: 68.316%
2022-02-21 00:59:07 - epoch 037 lr: 0.010000000000000002
2022-02-21 00:59:50 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.2270
2022-02-21 01:00:26 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3056
2022-02-21 01:01:04 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.4141
2022-02-21 01:01:41 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5899
2022-02-21 01:02:18 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.4832
2022-02-21 01:02:57 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.5756
2022-02-21 01:03:33 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.5753
2022-02-21 01:04:10 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.4647
2022-02-21 01:04:48 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.7643
2022-02-21 01:05:24 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.3666
2022-02-21 01:06:02 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.5828
2022-02-21 01:06:40 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.5278
2022-02-21 01:07:16 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.5562
2022-02-21 01:07:54 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.5722
2022-02-21 01:08:31 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.3864
2022-02-21 01:09:08 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.2515
2022-02-21 01:09:46 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.7469
2022-02-21 01:10:23 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.6005
2022-02-21 01:11:00 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.5666
2022-02-21 01:11:38 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.6368
2022-02-21 01:12:14 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.5012
2022-02-21 01:12:52 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.4989
2022-02-21 01:13:29 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.3200
2022-02-21 01:14:06 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.4923
2022-02-21 01:14:42 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.4203
2022-02-21 01:15:20 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.6950
2022-02-21 01:15:58 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.5887
2022-02-21 01:16:35 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5043
2022-02-21 01:17:12 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.5542
2022-02-21 01:17:49 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.6112
2022-02-21 01:18:27 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.3848
2022-02-21 01:19:04 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.6451
2022-02-21 01:19:40 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.4256
2022-02-21 01:20:18 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.3130
2022-02-21 01:20:55 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.4889
2022-02-21 01:21:33 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.5507
2022-02-21 01:22:09 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.5672
2022-02-21 01:22:47 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.4760
2022-02-21 01:23:24 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.6158
2022-02-21 01:24:02 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.4527
2022-02-21 01:24:39 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.5771
2022-02-21 01:25:16 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.7145
2022-02-21 01:25:53 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.5080
2022-02-21 01:26:31 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.4561
2022-02-21 01:27:08 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.5212
2022-02-21 01:27:45 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.5895
2022-02-21 01:28:23 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.4938
2022-02-21 01:28:59 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.4877
2022-02-21 01:29:37 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.4771
2022-02-21 01:30:13 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.4558
2022-02-21 01:30:14 - train: epoch 037, train_loss: 1.4947
2022-02-21 01:31:37 - eval: epoch: 037, acc1: 68.392%, acc5: 88.820%, test_loss: 1.2699, per_image_load_time: 2.956ms, per_image_inference_time: 0.290ms
2022-02-21 01:31:38 - until epoch: 037, best_acc1: 68.392%
2022-02-21 01:31:38 - epoch 038 lr: 0.010000000000000002
2022-02-21 01:32:20 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.4653
2022-02-21 01:32:57 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.2987
2022-02-21 01:33:34 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.0841
2022-02-21 01:34:13 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.4571
2022-02-21 01:34:49 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.3637
2022-02-21 01:35:26 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.6136
2022-02-21 01:36:02 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3512
2022-02-21 01:36:40 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.3094
2022-02-21 01:37:18 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.3787
2022-02-21 01:37:55 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.6137
2022-02-21 01:38:33 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.4417
2022-02-21 01:39:10 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.4964
2022-02-21 01:39:48 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.6401
2022-02-21 01:40:24 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.6566
2022-02-21 01:41:02 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.5882
2022-02-21 01:41:39 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.5437
2022-02-21 01:42:16 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.6961
2022-02-21 01:42:53 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.6129
2022-02-21 01:43:30 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.6245
2022-02-21 01:44:07 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.4257
2022-02-21 01:44:45 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.5251
2022-02-21 01:45:22 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.4787
2022-02-21 01:46:00 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.5375
2022-02-21 01:46:37 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.6948
2022-02-21 01:47:14 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.4467
2022-02-21 01:47:52 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.5043
2022-02-21 01:48:29 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.5568
2022-02-21 01:49:07 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.5996
2022-02-21 01:49:45 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.5838
2022-02-21 01:50:21 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.5335
2022-02-21 01:50:57 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.4928
2022-02-21 01:51:35 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2010
2022-02-21 01:52:13 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.2673
2022-02-21 01:52:51 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.4566
2022-02-21 01:53:28 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.5825
2022-02-21 01:54:04 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.5427
2022-02-21 01:54:41 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.2740
2022-02-21 01:55:17 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.5259
2022-02-21 01:55:55 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.5058
2022-02-21 01:56:32 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.4939
2022-02-21 01:57:11 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.4553
2022-02-21 01:57:47 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.3457
2022-02-21 01:58:24 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.5938
2022-02-21 01:59:01 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.5770
2022-02-21 01:59:40 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.6035
2022-02-21 02:00:16 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.6127
2022-02-21 02:00:54 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.3243
2022-02-21 02:01:30 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.5501
2022-02-21 02:02:07 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.4368
2022-02-21 02:02:43 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.3007
2022-02-21 02:02:44 - train: epoch 038, train_loss: 1.4879
2022-02-21 02:04:07 - eval: epoch: 038, acc1: 67.894%, acc5: 88.436%, test_loss: 1.2888, per_image_load_time: 2.965ms, per_image_inference_time: 0.293ms
2022-02-21 02:04:08 - until epoch: 038, best_acc1: 68.392%
2022-02-21 02:04:08 - epoch 039 lr: 0.010000000000000002
2022-02-21 02:04:50 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.4927
2022-02-21 02:05:28 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.6527
2022-02-21 02:06:04 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.3986
2022-02-21 02:06:42 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.5259
2022-02-21 02:07:20 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4961
2022-02-21 02:07:56 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.3863
2022-02-21 02:08:34 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.6193
2022-02-21 02:09:11 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.4128
2022-02-21 02:09:49 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.5342
2022-02-21 02:10:25 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.3111
2022-02-21 02:11:03 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.4747
2022-02-21 02:11:39 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.5123
2022-02-21 02:12:17 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.8104
2022-02-21 02:12:55 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.5623
2022-02-21 02:13:31 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4262
2022-02-21 02:14:09 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.7241
2022-02-21 02:14:46 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.2506
2022-02-21 02:15:23 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.5019
2022-02-21 02:16:00 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.2720
2022-02-21 02:16:38 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.4613
2022-02-21 02:17:15 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.4405
2022-02-21 02:17:52 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.3075
2022-02-21 02:18:29 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.7015
2022-02-21 02:19:06 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.5885
2022-02-21 02:19:44 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.4061
2022-02-21 02:20:21 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.5383
2022-02-21 02:20:58 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.6987
2022-02-21 02:21:36 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.4421
2022-02-21 02:22:13 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.3207
2022-02-21 02:22:50 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.5735
2022-02-21 02:23:29 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.3921
2022-02-21 02:24:05 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.5023
2022-02-21 02:24:43 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.5863
2022-02-21 02:25:20 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5783
2022-02-21 02:25:57 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.7093
2022-02-21 02:26:35 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.6463
2022-02-21 02:27:13 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.4336
2022-02-21 02:27:51 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.3210
2022-02-21 02:28:27 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.6278
2022-02-21 02:29:05 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.4506
2022-02-21 02:29:42 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.6231
2022-02-21 02:30:19 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.5153
2022-02-21 02:30:57 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.5186
2022-02-21 02:31:34 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.2447
2022-02-21 02:32:12 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.3832
2022-02-21 02:32:48 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.5692
2022-02-21 02:33:25 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.5109
2022-02-21 02:34:03 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.4966
2022-02-21 02:34:40 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.3734
2022-02-21 02:35:16 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.3539
2022-02-21 02:35:17 - train: epoch 039, train_loss: 1.4841
2022-02-21 02:36:40 - eval: epoch: 039, acc1: 67.734%, acc5: 88.422%, test_loss: 1.2968, per_image_load_time: 2.899ms, per_image_inference_time: 0.293ms
2022-02-21 02:36:41 - until epoch: 039, best_acc1: 68.392%
2022-02-21 02:36:41 - epoch 040 lr: 0.010000000000000002
2022-02-21 02:37:23 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.6768
2022-02-21 02:38:00 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.6163
2022-02-21 02:38:39 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.6255
2022-02-21 02:39:15 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.4597
2022-02-21 02:39:52 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.3733
2022-02-21 02:40:30 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.5056
2022-02-21 02:41:08 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.4427
2022-02-21 02:41:45 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.6640
2022-02-21 02:42:21 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.4049
2022-02-21 02:42:59 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.2025
2022-02-21 02:43:36 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.4127
2022-02-21 02:44:14 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.4154
2022-02-21 02:44:52 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.3520
2022-02-21 02:45:29 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.4033
2022-02-21 02:46:05 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.6694
2022-02-21 02:46:43 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.5994
2022-02-21 02:47:20 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.5750
2022-02-21 02:47:56 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.3639
2022-02-21 02:48:34 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.4331
2022-02-21 02:49:12 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.5017
2022-02-21 02:49:50 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.3306
2022-02-21 02:50:27 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.3053
2022-02-21 02:51:04 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.4361
2022-02-21 02:51:42 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.5068
2022-02-21 02:52:19 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.6429
2022-02-21 02:52:56 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.2457
2022-02-21 02:53:33 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.5904
2022-02-21 02:54:11 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.4901
2022-02-21 02:54:47 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.8011
2022-02-21 02:55:20 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.6456
2022-02-21 02:55:59 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.4300
2022-02-21 02:56:37 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.6580
2022-02-21 02:57:14 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.4008
2022-02-21 02:57:52 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.5262
2022-02-21 02:58:30 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.4937
2022-02-21 02:59:07 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.4751
2022-02-21 02:59:45 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.4323
2022-02-21 03:00:21 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.3384
2022-02-21 03:00:57 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.5428
2022-02-21 03:01:35 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.5623
2022-02-21 03:02:12 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.6030
2022-02-21 03:02:49 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.4807
2022-02-21 03:03:26 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.5554
2022-02-21 03:04:03 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.4855
2022-02-21 03:04:40 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.3239
2022-02-21 03:05:17 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5136
2022-02-21 03:05:54 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.4823
2022-02-21 03:06:32 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.3462
2022-02-21 03:07:10 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.5857
2022-02-21 03:07:45 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.5602
2022-02-21 03:07:46 - train: epoch 040, train_loss: 1.4820
2022-02-21 03:09:10 - eval: epoch: 040, acc1: 68.226%, acc5: 88.666%, test_loss: 1.2814, per_image_load_time: 2.927ms, per_image_inference_time: 0.291ms
2022-02-21 03:09:10 - until epoch: 040, best_acc1: 68.392%
2022-02-21 03:09:10 - epoch 041 lr: 0.010000000000000002
2022-02-21 03:09:53 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.6292
2022-02-21 03:10:30 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.6336
2022-02-21 03:11:07 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.4451
2022-02-21 03:11:44 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.4481
2022-02-21 03:12:21 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.2906
2022-02-21 03:12:59 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.5190
2022-02-21 03:13:36 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.3703
2022-02-21 03:14:13 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.1959
2022-02-21 03:14:50 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.1776
2022-02-21 03:15:28 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.6053
2022-02-21 03:16:05 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.3156
2022-02-21 03:16:42 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.2391
2022-02-21 03:17:19 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.5232
2022-02-21 03:17:57 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.5796
2022-02-21 03:18:34 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.8184
2022-02-21 03:19:11 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.2720
2022-02-21 03:19:48 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.4695
2022-02-21 03:20:26 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.4356
2022-02-21 03:21:05 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.5803
2022-02-21 03:21:40 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.4171
2022-02-21 03:22:17 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.3331
2022-02-21 03:22:54 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.3425
2022-02-21 03:23:32 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.1902
2022-02-21 03:24:09 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.6216
2022-02-21 03:24:46 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.4340
2022-02-21 03:25:23 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.6219
2022-02-21 03:26:00 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.6725
2022-02-21 03:26:38 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.4135
2022-02-21 03:27:15 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.5480
2022-02-21 03:27:53 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.5648
2022-02-21 03:28:30 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.6966
2022-02-21 03:29:08 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.3635
2022-02-21 03:29:44 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.4315
2022-02-21 03:30:21 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.3566
2022-02-21 03:30:58 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.5944
2022-02-21 03:31:36 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.5544
2022-02-21 03:32:13 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.4159
2022-02-21 03:32:49 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4051
2022-02-21 03:33:26 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.5808
2022-02-21 03:34:02 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.3493
2022-02-21 03:34:40 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.5114
2022-02-21 03:35:17 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.5067
2022-02-21 03:35:54 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.6635
2022-02-21 03:36:32 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.4022
2022-02-21 03:37:09 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.4932
2022-02-21 03:37:46 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.5630
2022-02-21 03:38:24 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.6891
2022-02-21 03:39:02 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.4880
2022-02-21 03:39:38 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.4486
2022-02-21 03:40:14 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.5523
2022-02-21 03:40:15 - train: epoch 041, train_loss: 1.4822
2022-02-21 03:41:39 - eval: epoch: 041, acc1: 68.148%, acc5: 88.498%, test_loss: 1.2852, per_image_load_time: 2.956ms, per_image_inference_time: 0.282ms
2022-02-21 03:41:39 - until epoch: 041, best_acc1: 68.392%
2022-02-21 03:41:39 - epoch 042 lr: 0.010000000000000002
2022-02-21 03:42:21 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.2073
2022-02-21 03:42:59 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.4714
2022-02-21 03:43:37 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.5175
2022-02-21 03:44:13 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.2492
2022-02-21 03:44:50 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.5133
2022-02-21 03:45:29 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.3179
2022-02-21 03:46:06 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.5031
2022-02-21 03:46:42 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.3828
2022-02-21 03:47:20 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7198
2022-02-21 03:47:57 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.5319
2022-02-21 03:48:34 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.2705
2022-02-21 03:49:11 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.4801
2022-02-21 03:49:49 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.5065
2022-02-21 03:50:26 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.7322
2022-02-21 03:51:04 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4131
2022-02-21 03:51:40 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.6365
2022-02-21 03:52:18 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.4420
2022-02-21 03:52:55 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.4011
2022-02-21 03:53:32 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.6092
2022-02-21 03:54:09 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.3503
2022-02-21 03:54:46 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.3070
2022-02-21 03:55:24 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.3168
2022-02-21 03:56:01 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.4220
2022-02-21 03:56:39 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.7764
2022-02-21 03:57:16 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.6517
2022-02-21 03:57:53 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.6141
2022-02-21 03:58:30 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.3282
2022-02-21 03:59:08 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.3934
2022-02-21 03:59:45 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.5299
2022-02-21 04:00:22 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.4818
2022-02-21 04:01:00 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.4628
2022-02-21 04:01:37 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.6154
2022-02-21 04:02:13 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.6335
2022-02-21 04:02:51 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.3086
2022-02-21 04:03:29 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.4689
2022-02-21 04:04:04 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.6394
2022-02-21 04:04:41 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.5350
2022-02-21 04:05:21 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.2988
2022-02-21 04:05:57 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.4077
2022-02-21 04:06:34 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.4949
2022-02-21 04:07:10 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.6331
2022-02-21 04:07:48 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.6086
2022-02-21 04:08:25 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.2914
2022-02-21 04:09:02 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.4407
2022-02-21 04:09:40 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.5474
2022-02-21 04:10:16 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.7492
2022-02-21 04:10:54 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.3719
2022-02-21 04:11:31 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.5725
2022-02-21 04:12:09 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.5016
2022-02-21 04:12:45 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.4488
2022-02-21 04:12:46 - train: epoch 042, train_loss: 1.4828
2022-02-21 04:14:10 - eval: epoch: 042, acc1: 67.492%, acc5: 88.252%, test_loss: 1.3155, per_image_load_time: 2.945ms, per_image_inference_time: 0.289ms
2022-02-21 04:14:10 - until epoch: 042, best_acc1: 68.392%
2022-02-21 04:14:10 - epoch 043 lr: 0.010000000000000002
2022-02-21 04:14:52 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.4806
2022-02-21 04:15:31 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.5091
2022-02-21 04:16:07 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.2867
2022-02-21 04:16:45 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.3788
2022-02-21 04:17:22 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.5037
2022-02-21 04:18:00 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.4249
2022-02-21 04:18:37 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.5898
2022-02-21 04:19:15 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.7042
2022-02-21 04:19:53 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.4636
2022-02-21 04:20:30 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.5357
2022-02-21 04:21:07 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.5826
2022-02-21 04:21:45 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.5598
2022-02-21 04:22:23 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.5419
2022-02-21 04:23:00 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.3253
2022-02-21 04:23:38 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.3225
2022-02-21 04:24:15 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.4767
2022-02-21 04:24:52 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.5438
2022-02-21 04:25:30 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.5594
2022-02-21 04:26:09 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.5280
2022-02-21 04:26:45 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.2882
2022-02-21 04:27:23 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.4178
2022-02-21 04:28:00 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.5216
2022-02-21 04:28:37 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.6106
2022-02-21 04:29:15 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.3463
2022-02-21 04:29:53 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.7453
2022-02-21 04:30:31 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.2332
2022-02-21 04:31:08 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.5647
2022-02-21 04:31:45 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.3165
2022-02-21 04:32:23 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.5605
2022-02-21 04:33:02 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.6657
2022-02-21 04:33:38 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.6880
2022-02-21 04:34:16 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.5300
2022-02-21 04:34:53 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.4410
2022-02-21 04:35:31 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.5539
2022-02-21 04:36:09 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.5770
2022-02-21 04:36:47 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.5349
2022-02-21 04:37:25 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.5073
2022-02-21 04:38:02 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.6178
2022-02-21 04:38:40 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.6209
2022-02-21 04:39:16 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.4824
2022-02-21 04:39:54 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.7682
2022-02-21 04:40:31 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.5217
2022-02-21 04:41:08 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.5063
2022-02-21 04:41:46 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.4266
2022-02-21 04:42:23 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.5773
2022-02-21 04:43:00 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.4421
2022-02-21 04:43:38 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.3490
2022-02-21 04:44:15 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.3962
2022-02-21 04:44:53 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.3538
2022-02-21 04:45:28 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.3623
2022-02-21 04:45:29 - train: epoch 043, train_loss: 1.4812
2022-02-21 04:46:53 - eval: epoch: 043, acc1: 67.406%, acc5: 88.268%, test_loss: 1.3047, per_image_load_time: 2.990ms, per_image_inference_time: 0.261ms
2022-02-21 04:46:54 - until epoch: 043, best_acc1: 68.392%
2022-02-21 04:46:54 - epoch 044 lr: 0.010000000000000002
2022-02-21 04:47:37 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.5745
2022-02-21 04:48:14 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.5776
2022-02-21 04:48:51 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.4694
2022-02-21 04:49:28 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.1869
2022-02-21 04:50:05 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.3980
2022-02-21 04:50:42 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.2838
2022-02-21 04:51:20 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.2349
2022-02-21 04:51:56 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.4000
2022-02-21 04:52:34 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.4822
2022-02-21 04:53:10 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.4398
2022-02-21 04:53:48 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.3515
2022-02-21 04:54:26 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.3558
2022-02-21 04:55:03 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.5956
2022-02-21 04:55:39 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.3403
2022-02-21 04:56:16 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.3950
2022-02-21 04:56:53 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.3945
2022-02-21 04:57:31 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.4096
2022-02-21 04:58:08 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.3507
2022-02-21 04:58:46 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.6450
2022-02-21 04:59:23 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.4345
2022-02-21 05:00:00 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.5987
2022-02-21 05:00:33 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.4376
2022-02-21 05:01:08 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.6014
2022-02-21 05:01:47 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.5687
2022-02-21 05:02:26 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.6153
2022-02-21 05:03:03 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.5324
2022-02-21 05:03:40 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.6996
2022-02-21 05:04:18 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.2401
2022-02-21 05:04:55 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.3683
2022-02-21 05:05:34 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.4591
2022-02-21 05:06:09 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.5047
2022-02-21 05:06:47 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.2326
2022-02-21 05:07:23 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.3668
2022-02-21 05:08:01 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.6939
2022-02-21 05:08:39 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.3056
