2022-02-23 22:40:25 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.3333
2022-02-23 22:41:01 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.5794
2022-02-23 22:41:39 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.6705
2022-02-23 22:42:18 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.7938
2022-02-23 22:42:56 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.8341
2022-02-23 22:43:35 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.6572
2022-02-23 22:44:13 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.6008
2022-02-23 22:44:52 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.7078
2022-02-23 22:45:28 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.8112
2022-02-23 22:46:07 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.7168
2022-02-23 22:46:44 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.6105
2022-02-23 22:47:23 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.6061
2022-02-23 22:47:59 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.8434
2022-02-23 22:48:37 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.7731
2022-02-23 22:49:14 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.7273
2022-02-23 22:49:15 - train: epoch 056, train_loss: 1.6737
2022-02-23 22:50:41 - eval: epoch: 056, acc1: 64.578%, acc5: 86.340%, test_loss: 1.4527, per_image_load_time: 3.119ms, per_image_inference_time: 0.180ms
2022-02-23 22:50:41 - until epoch: 056, best_acc1: 64.904%
2022-02-23 22:50:41 - epoch 057 lr: 0.010000000000000002
2022-02-23 22:51:26 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.7487
2022-02-23 22:52:05 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.5691
2022-02-23 22:52:43 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.4950
2022-02-23 22:53:21 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.7313
2022-02-23 22:53:59 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.5613
2022-02-23 22:54:37 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.6644
2022-02-23 22:55:16 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.4024
2022-02-23 22:55:53 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.5903
2022-02-23 22:56:31 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.4091
2022-02-23 22:57:08 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.5383
2022-02-23 22:57:48 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.7121
2022-02-23 22:58:25 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.6032
2022-02-23 22:59:05 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.7152
2022-02-23 22:59:42 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.5860
2022-02-23 23:00:21 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.8182
2022-02-23 23:00:58 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.9590
2022-02-23 23:01:37 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.6524
2022-02-23 23:02:14 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.8302
2022-02-23 23:02:51 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.7174
2022-02-23 23:03:32 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.5539
2022-02-23 23:04:09 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.6942
2022-02-23 23:04:47 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.7050
2022-02-23 23:05:25 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.5829
2022-02-23 23:06:04 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.5182
2022-02-23 23:06:41 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.9320
2022-02-23 23:07:21 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.7255
2022-02-23 23:07:59 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.3988
2022-02-23 23:08:36 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.5513
2022-02-23 23:09:15 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.6405
2022-02-23 23:09:54 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.7642
2022-02-23 23:10:32 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.8978
2022-02-23 23:11:11 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.8721
2022-02-23 23:11:49 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.7940
2022-02-23 23:12:27 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.6010
2022-02-23 23:13:05 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.9229
2022-02-23 23:13:44 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.5758
2022-02-23 23:14:22 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.7491
2022-02-23 23:15:00 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.6198
2022-02-23 23:15:37 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.7668
2022-02-23 23:16:15 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.5544
2022-02-23 23:16:52 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.5992
2022-02-23 23:17:30 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.7998
2022-02-23 23:18:08 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.5654
2022-02-23 23:18:46 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.6528
2022-02-23 23:19:22 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.6888
2022-02-23 23:20:01 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.4677
2022-02-23 23:20:37 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.5711
2022-02-23 23:21:16 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.7935
2022-02-23 23:21:52 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.7860
2022-02-23 23:22:28 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.6732
2022-02-23 23:22:29 - train: epoch 057, train_loss: 1.6706
2022-02-23 23:23:53 - eval: epoch: 057, acc1: 64.618%, acc5: 86.210%, test_loss: 1.4558, per_image_load_time: 3.073ms, per_image_inference_time: 0.184ms
2022-02-23 23:23:54 - until epoch: 057, best_acc1: 64.904%
2022-02-23 23:23:54 - epoch 058 lr: 0.010000000000000002
2022-02-23 23:24:37 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.8445
2022-02-23 23:25:15 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.8067
2022-02-23 23:25:53 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.5579
2022-02-23 23:26:32 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.5709
2022-02-23 23:27:09 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.5566
2022-02-23 23:27:46 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.9462
2022-02-23 23:28:24 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.7860
2022-02-23 23:29:01 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.8179
2022-02-23 23:29:38 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.6745
2022-02-23 23:30:16 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.5550
2022-02-23 23:30:53 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.5152
2022-02-23 23:31:31 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.5229
2022-02-23 23:32:09 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.9715
2022-02-23 23:32:48 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.9354
2022-02-23 23:33:25 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.6425
2022-02-23 23:34:02 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.6311
2022-02-23 23:34:39 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.8324
2022-02-23 23:35:16 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.7670
2022-02-23 23:35:55 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.7058
2022-02-23 23:36:32 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.9822
2022-02-23 23:37:10 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.5723
2022-02-23 23:37:46 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.4017
2022-02-23 23:38:25 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.6074
2022-02-23 23:39:02 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.6420
2022-02-23 23:39:39 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.6061
2022-02-23 23:40:17 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.5420
2022-02-23 23:40:54 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.8136
2022-02-23 23:41:31 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.6151
2022-02-23 23:42:09 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.4616
2022-02-23 23:42:48 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.8503
2022-02-23 23:43:24 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.6146
2022-02-23 23:44:02 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.3939
2022-02-23 23:44:38 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.6707
2022-02-23 23:45:16 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.7732
2022-02-23 23:45:53 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.6131
2022-02-23 23:46:31 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.6003
2022-02-23 23:47:07 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.7147
2022-02-23 23:47:44 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.7636
2022-02-23 23:48:22 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.6409
2022-02-23 23:48:59 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.5984
2022-02-23 23:49:36 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.8995
2022-02-23 23:50:13 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.6888
2022-02-23 23:50:49 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.7832
2022-02-23 23:51:27 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.5600
2022-02-23 23:52:05 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.5977
2022-02-23 23:52:42 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.7133
2022-02-23 23:53:19 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.5898
2022-02-23 23:53:57 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.8071
2022-02-23 23:54:35 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.5442
2022-02-23 23:55:10 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.3606
2022-02-23 23:55:11 - train: epoch 058, train_loss: 1.6688
2022-02-23 23:56:34 - eval: epoch: 058, acc1: 64.634%, acc5: 86.310%, test_loss: 1.4517, per_image_load_time: 3.015ms, per_image_inference_time: 0.182ms
2022-02-23 23:56:35 - until epoch: 058, best_acc1: 64.904%
2022-02-23 23:56:35 - epoch 059 lr: 0.010000000000000002
2022-02-23 23:57:20 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.6415
2022-02-23 23:57:57 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.6674
2022-02-23 23:58:36 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.5143
2022-02-23 23:59:14 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.6958
2022-02-23 23:59:51 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.7772
2022-02-24 00:00:29 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.4104
2022-02-24 00:01:05 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.4626
2022-02-24 00:01:43 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.6598
2022-02-24 00:02:20 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.7350
2022-02-24 00:02:57 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.7469
2022-02-24 00:03:36 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.0009
2022-02-24 00:04:13 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.5181
2022-02-24 00:04:50 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.9073
2022-02-24 00:05:28 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.6562
2022-02-24 00:06:46 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.5255
2022-02-24 00:08:35 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.4576
2022-02-24 00:09:28 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.7363
2022-02-24 00:10:25 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.5849
2022-02-24 00:11:47 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.3491
2022-02-24 00:12:56 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.4610
2022-02-24 00:15:10 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.8764
2022-02-24 00:18:05 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.5266
2022-02-24 00:18:55 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.5111
2022-02-24 00:19:55 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.7536
2022-02-24 00:22:40 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.7589
2022-02-24 00:25:10 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.5036
2022-02-24 00:29:12 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.6767
2022-02-24 00:31:32 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.7331
2022-02-24 00:32:08 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.4678
2022-02-24 00:32:46 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.0176
2022-02-24 00:33:24 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.6784
2022-02-24 00:34:08 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.6700
2022-02-24 00:35:27 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.8810
2022-02-24 00:36:04 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.0754
2022-02-24 00:36:45 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.5121
2022-02-24 00:37:41 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.7785
2022-02-24 00:38:19 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.5620
2022-02-24 00:40:15 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.6328
2022-02-24 00:41:04 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.5683
2022-02-24 00:42:17 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.0022
2022-02-24 00:42:59 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.6225
2022-02-24 00:44:17 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.6398
2022-02-24 00:45:41 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.8872
2022-02-24 00:46:49 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.7166
2022-02-24 00:48:29 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.9515
2022-02-24 00:50:47 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.7274
2022-02-24 00:52:33 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.7104
2022-02-24 00:55:37 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.4749
2022-02-24 00:56:44 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.8120
2022-02-24 00:59:14 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.9297
2022-02-24 00:59:14 - train: epoch 059, train_loss: 1.6681
2022-02-24 01:00:38 - eval: epoch: 059, acc1: 64.524%, acc5: 86.198%, test_loss: 1.4600, per_image_load_time: 3.069ms, per_image_inference_time: 0.184ms
2022-02-24 01:00:38 - until epoch: 059, best_acc1: 64.904%
2022-02-24 01:00:38 - epoch 060 lr: 0.010000000000000002
2022-02-24 01:01:21 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.7449
2022-02-24 01:02:00 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.5325
2022-02-24 01:02:36 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.6268
2022-02-24 01:03:13 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.7052
2022-02-24 01:03:50 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.8363
2022-02-24 01:04:28 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.7894
2022-02-24 01:05:04 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.7236
2022-02-24 01:05:41 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.7152
2022-02-24 01:06:18 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.5268
2022-02-24 01:06:57 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.5470
2022-02-24 01:07:33 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.6035
2022-02-24 01:08:12 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.6225
2022-02-24 01:08:49 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.5820
2022-02-24 01:09:27 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.6648
2022-02-24 01:10:03 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.7176
2022-02-24 01:10:41 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.4445
2022-02-24 01:11:18 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.6347
2022-02-24 01:11:55 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.8531
2022-02-24 01:12:31 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.8276
2022-02-24 01:13:09 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.5739
2022-02-24 01:13:45 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.7597
2022-02-24 01:14:23 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.6845
2022-02-24 01:15:00 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.5684
2022-02-24 01:15:37 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.9224
2022-02-24 01:16:15 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.6784
2022-02-24 01:16:51 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.8611
2022-02-24 01:17:28 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.7098
2022-02-24 01:18:06 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.7767
2022-02-24 01:18:43 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.6131
2022-02-24 01:19:20 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.7221
2022-02-24 01:19:56 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.8705
2022-02-24 01:20:34 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.8016
2022-02-24 01:21:08 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.6283
2022-02-24 01:21:47 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.7207
2022-02-24 01:22:23 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.7054
2022-02-24 01:23:00 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.8046
2022-02-24 01:23:36 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.6119
2022-02-24 01:24:13 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.7611
2022-02-24 01:24:50 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.7605
2022-02-24 01:25:27 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.8287
2022-02-24 01:26:04 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.8835
2022-02-24 01:26:41 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.8208
2022-02-24 01:27:18 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.6842
2022-02-24 01:27:53 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.8021
2022-02-24 01:28:32 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.6885
2022-02-24 01:29:08 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.5421
2022-02-24 01:29:45 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.7722
2022-02-24 01:30:22 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.5938
2022-02-24 01:30:59 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.7366
2022-02-24 01:31:33 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.8507
2022-02-24 01:31:35 - train: epoch 060, train_loss: 1.6658
2022-02-24 01:32:56 - eval: epoch: 060, acc1: 64.966%, acc5: 86.498%, test_loss: 1.4404, per_image_load_time: 2.744ms, per_image_inference_time: 0.169ms
2022-02-24 01:32:56 - until epoch: 060, best_acc1: 64.966%
2022-02-24 01:32:56 - epoch 061 lr: 0.0010000000000000002
2022-02-24 01:33:38 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.4519
2022-02-24 01:34:16 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.6588
2022-02-24 01:34:51 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.5373
2022-02-24 01:35:28 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.6817
2022-02-24 01:36:05 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.5072
2022-02-24 01:36:42 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.5464
2022-02-24 01:37:20 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.4159
2022-02-24 01:37:55 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.4921
2022-02-24 01:38:32 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.5012
2022-02-24 01:39:10 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.4739
2022-02-24 01:39:47 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.2951
2022-02-24 01:40:24 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.3927
2022-02-24 01:41:01 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.4928
2022-02-24 01:41:36 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.4428
2022-02-24 01:42:12 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.5223
2022-02-24 01:42:50 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.1586
2022-02-24 01:43:26 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.5951
2022-02-24 01:44:03 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.5577
2022-02-24 01:44:39 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.7774
2022-02-24 01:45:16 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.3158
2022-02-24 01:45:52 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.4958
2022-02-24 01:46:30 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.4030
2022-02-24 01:47:07 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.3942
2022-02-24 01:47:44 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.3957
2022-02-24 01:48:20 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.3720
2022-02-24 01:48:57 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.4391
2022-02-24 01:49:34 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.4442
2022-02-24 01:50:11 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.4570
2022-02-24 01:50:48 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.4321
2022-02-24 01:51:25 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.4305
2022-02-24 01:52:02 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.5723
2022-02-24 01:52:38 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.3346
2022-02-24 01:53:15 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.3993
2022-02-24 01:53:52 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.5502
2022-02-24 01:54:28 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.5186
2022-02-24 01:55:05 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.4707
2022-02-24 01:55:42 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.3999
2022-02-24 01:56:19 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.4081
2022-02-24 01:56:55 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.5276
2022-02-24 01:57:32 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.3817
2022-02-24 01:58:09 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.7414
2022-02-24 01:58:45 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.4765
2022-02-24 01:59:22 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.4600
2022-02-24 01:59:58 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.4848
2022-02-24 02:00:35 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.4407
2022-02-24 02:01:13 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.4813
2022-02-24 02:01:48 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.4866
2022-02-24 02:02:26 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.2715
2022-02-24 02:03:02 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.5369
2022-02-24 02:03:38 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.4986
2022-02-24 02:03:39 - train: epoch 061, train_loss: 1.4786
2022-02-24 02:05:02 - eval: epoch: 061, acc1: 68.396%, acc5: 88.450%, test_loss: 1.2835, per_image_load_time: 3.007ms, per_image_inference_time: 0.181ms
2022-02-24 02:05:02 - until epoch: 061, best_acc1: 68.396%
2022-02-24 02:05:02 - epoch 062 lr: 0.0010000000000000002
2022-02-24 02:05:44 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.5534
2022-02-24 02:06:21 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.6318
2022-02-24 02:06:57 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.4049
2022-02-24 02:07:35 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.4070
2022-02-24 02:08:11 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.4385
2022-02-24 02:08:48 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.2845
2022-02-24 02:09:24 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.3170
2022-02-24 02:10:01 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.5735
2022-02-24 02:10:38 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.3788
2022-02-24 02:11:16 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.6926
2022-02-24 02:11:52 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.5762
2022-02-24 02:12:30 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.6868
2022-02-24 02:13:07 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.3480
2022-02-24 02:13:43 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.4095
2022-02-24 02:14:20 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.5782
2022-02-24 02:14:57 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.6882
2022-02-24 02:15:35 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.4718
2022-02-24 02:16:11 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.2894
2022-02-24 02:16:48 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.3366
2022-02-24 02:17:24 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.4957
2022-02-24 02:18:02 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.5102
2022-02-24 02:18:38 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.2849
2022-02-24 02:19:15 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.4921
2022-02-24 02:19:53 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.3419
2022-02-24 02:20:28 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.4817
2022-02-24 02:21:05 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.4358
2022-02-24 02:21:41 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.4091
2022-02-24 02:22:19 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.4068
2022-02-24 02:22:55 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.5228
2022-02-24 02:23:32 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.5210
2022-02-24 02:24:08 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.5050
2022-02-24 02:24:45 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.3834
2022-02-24 02:25:21 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.4351
2022-02-24 02:25:59 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.1773
2022-02-24 02:26:35 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.3595
2022-02-24 02:27:12 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.4006
2022-02-24 02:27:48 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.4480
2022-02-24 02:28:26 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.4505
2022-02-24 02:29:02 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.5066
2022-02-24 02:29:39 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.3885
2022-02-24 02:30:16 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.5067
2022-02-24 02:30:52 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.4126
2022-02-24 02:31:29 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.4816
2022-02-24 02:32:05 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.2736
2022-02-24 02:32:43 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.3648
2022-02-24 02:33:20 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.3486
2022-02-24 02:33:57 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.3790
2022-02-24 02:34:33 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.4538
2022-02-24 02:35:10 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.4423
2022-02-24 02:35:45 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.6050
2022-02-24 02:35:46 - train: epoch 062, train_loss: 1.4323
2022-02-24 02:37:10 - eval: epoch: 062, acc1: 68.686%, acc5: 88.694%, test_loss: 1.2670, per_image_load_time: 3.102ms, per_image_inference_time: 0.172ms
2022-02-24 02:37:10 - until epoch: 062, best_acc1: 68.686%
2022-02-24 02:37:10 - epoch 063 lr: 0.0010000000000000002
2022-02-24 02:37:52 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.3744
2022-02-24 02:38:29 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.4527
2022-02-24 02:39:04 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.4742
2022-02-24 02:39:37 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.4050
2022-02-24 02:40:16 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.4660
2022-02-24 02:40:52 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.5105
2022-02-24 02:41:29 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.5820
2022-02-24 02:42:05 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.3694
2022-02-24 02:42:42 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.5878
2022-02-24 02:43:19 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.5981
2022-02-24 02:43:57 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.3501
2022-02-24 02:44:33 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.5007
2022-02-24 02:45:11 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.1072
2022-02-24 02:45:46 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.4860
2022-02-24 02:46:24 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.4627
2022-02-24 02:47:00 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.3134
2022-02-24 02:47:37 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.3000
2022-02-24 02:48:15 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.4709
2022-02-24 02:48:50 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.4551
2022-02-24 02:49:28 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.1878
2022-02-24 02:50:04 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.4184
2022-02-24 02:50:41 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.6316
2022-02-24 02:51:19 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.6742
2022-02-24 02:51:55 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.5708
2022-02-24 02:52:33 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.3828
2022-02-24 02:53:09 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.4185
2022-02-24 02:53:46 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.7127
2022-02-24 02:54:23 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.5830
2022-02-24 02:55:00 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.4532
2022-02-24 02:55:35 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.4648
2022-02-24 02:56:13 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.5556
2022-02-24 02:56:49 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.3996
2022-02-24 02:57:27 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.3803
2022-02-24 02:58:03 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.3105
2022-02-24 02:58:39 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.5615
2022-02-24 02:59:15 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.3309
2022-02-24 02:59:51 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.3973
2022-02-24 03:00:29 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.5011
2022-02-24 03:01:06 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.4425
2022-02-24 03:01:43 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.3033
2022-02-24 03:02:20 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.4046
2022-02-24 03:02:54 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.3532
2022-02-24 03:03:33 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.5276
2022-02-24 03:04:08 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.3855
2022-02-24 03:04:46 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.5266
2022-02-24 03:05:23 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.4543
2022-02-24 03:06:00 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.3593
2022-02-24 03:06:36 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.2248
2022-02-24 03:07:13 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.4577
2022-02-24 03:07:48 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.4109
2022-02-24 03:07:48 - train: epoch 063, train_loss: 1.4140
2022-02-24 03:09:09 - eval: epoch: 063, acc1: 69.082%, acc5: 88.802%, test_loss: 1.2561, per_image_load_time: 2.857ms, per_image_inference_time: 0.190ms
2022-02-24 03:09:10 - until epoch: 063, best_acc1: 69.082%
2022-02-24 03:09:10 - epoch 064 lr: 0.0010000000000000002
2022-02-24 03:09:52 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.2900
2022-02-24 03:10:29 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.4277
2022-02-24 03:11:06 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.3531
2022-02-24 03:11:44 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.2560
2022-02-24 03:12:20 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.3286
2022-02-24 03:12:57 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.4949
2022-02-24 03:13:34 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.5702
2022-02-24 03:14:12 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.1418
2022-02-24 03:14:48 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.3472
2022-02-24 03:15:25 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.4194
2022-02-24 03:16:02 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.4446
2022-02-24 03:16:38 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.1313
2022-02-24 03:17:15 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.5192
2022-02-24 03:17:52 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.7094
2022-02-24 03:18:29 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.3803
2022-02-24 03:19:05 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.3318
2022-02-24 03:19:43 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.3296
2022-02-24 03:20:19 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.2868
2022-02-24 03:20:56 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.5067
2022-02-24 03:21:32 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.2351
2022-02-24 03:22:10 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.5111
2022-02-24 03:22:45 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.3693
2022-02-24 03:23:23 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.5518
2022-02-24 03:23:58 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.4178
2022-02-24 03:24:35 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.4008
2022-02-24 03:25:12 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.3605
2022-02-24 03:25:50 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.2881
2022-02-24 03:26:26 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.3633
2022-02-24 03:27:02 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.3573
2022-02-24 03:27:40 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.4238
2022-02-24 03:28:17 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.4142
2022-02-24 03:28:53 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.5077
2022-02-24 03:29:30 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.3668
2022-02-24 03:30:07 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.4937
2022-02-24 03:30:44 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.1996
2022-02-24 03:31:21 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.3554
2022-02-24 03:31:58 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.3251
2022-02-24 03:32:34 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.5188
2022-02-24 03:33:11 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.5171
2022-02-24 03:33:48 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.2275
2022-02-24 03:34:25 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.4297
2022-02-24 03:35:01 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.5180
2022-02-24 03:35:38 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.6013
2022-02-24 03:36:16 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.2539
2022-02-24 03:36:53 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.4051
2022-02-24 03:37:29 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.5710
2022-02-24 03:38:05 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.7423
2022-02-24 03:38:42 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.5242
2022-02-24 03:39:18 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.5619
2022-02-24 03:39:54 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.2842
2022-02-24 03:39:55 - train: epoch 064, train_loss: 1.4030
2022-02-24 03:41:17 - eval: epoch: 064, acc1: 69.312%, acc5: 88.962%, test_loss: 1.2485, per_image_load_time: 2.388ms, per_image_inference_time: 0.168ms
2022-02-24 03:41:17 - until epoch: 064, best_acc1: 69.312%
2022-02-24 03:41:17 - epoch 065 lr: 0.0010000000000000002
2022-02-24 03:42:00 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.3191
2022-02-24 03:42:38 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.5340
2022-02-24 03:43:14 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.2978
2022-02-24 03:43:51 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.3062
2022-02-24 03:44:27 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.6535
2022-02-24 03:45:05 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.2961
2022-02-24 03:45:40 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.5513
2022-02-24 03:46:18 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.2828
2022-02-24 03:46:55 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.4079
2022-02-24 03:47:32 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.4503
2022-02-24 03:48:08 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.3404
2022-02-24 03:48:45 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.6641
2022-02-24 03:49:21 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.4850
2022-02-24 03:49:59 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.3742
2022-02-24 03:50:35 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.2210
2022-02-24 03:51:13 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.7183
2022-02-24 03:51:49 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.2907
2022-02-24 03:52:25 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.1630
2022-02-24 03:53:04 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.2679
2022-02-24 03:53:41 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.3903
2022-02-24 03:54:17 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.4447
2022-02-24 03:54:54 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.4028
2022-02-24 03:55:30 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.2875
2022-02-24 03:56:07 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.4726
2022-02-24 03:56:45 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.3056
2022-02-24 03:57:22 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.5327
2022-02-24 03:57:58 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.3079
2022-02-24 03:58:34 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.4268
2022-02-24 03:59:12 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.3363
2022-02-24 03:59:48 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.3724
2022-02-24 04:00:25 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.4919
2022-02-24 04:01:02 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.5464
2022-02-24 04:01:39 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.3800
2022-02-24 04:02:15 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.2664
2022-02-24 04:02:52 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.5597
2022-02-24 04:03:28 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.6054
2022-02-24 04:04:06 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.2685
2022-02-24 04:04:42 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.3812
2022-02-24 04:05:19 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.5524
2022-02-24 04:05:55 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.5927
2022-02-24 04:06:33 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.3882
2022-02-24 04:07:08 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.3589
2022-02-24 04:07:45 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.4029
2022-02-24 04:08:23 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.5785
2022-02-24 04:08:58 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.4275
2022-02-24 04:09:37 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.4218
2022-02-24 04:10:13 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.3587
2022-02-24 04:10:50 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.1276
2022-02-24 04:11:26 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.2662
2022-02-24 04:12:01 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.5410
2022-02-24 04:12:02 - train: epoch 065, train_loss: 1.3936
2022-02-24 04:13:24 - eval: epoch: 065, acc1: 69.380%, acc5: 89.078%, test_loss: 1.2452, per_image_load_time: 2.732ms, per_image_inference_time: 0.188ms
2022-02-24 04:13:24 - until epoch: 065, best_acc1: 69.380%
2022-02-24 04:13:24 - epoch 066 lr: 0.0010000000000000002
2022-02-24 04:14:06 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.0982
2022-02-24 04:14:43 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.6931
2022-02-24 04:15:19 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.3694
2022-02-24 04:15:57 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.2985
2022-02-24 04:16:32 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.4026
2022-02-24 04:17:09 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.2440
2022-02-24 04:17:46 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.4827
2022-02-24 04:18:23 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.4933
2022-02-24 04:18:59 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.3099
2022-02-24 04:19:36 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.2584
2022-02-24 04:20:13 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3859
2022-02-24 04:20:51 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.5939
2022-02-24 04:21:27 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.3686
2022-02-24 04:22:03 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.3391
2022-02-24 04:22:41 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.4078
2022-02-24 04:23:18 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.4901
2022-02-24 04:23:55 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.3370
2022-02-24 04:24:33 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.3973
2022-02-24 04:25:09 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.4463
2022-02-24 04:25:47 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.2517
2022-02-24 04:26:23 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.4720
2022-02-24 04:27:00 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.4333
2022-02-24 04:27:38 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.3369
2022-02-24 04:28:15 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.5778
2022-02-24 04:28:53 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.4303
2022-02-24 04:29:29 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.2217
2022-02-24 04:30:07 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.5711
2022-02-24 04:30:43 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.4219
2022-02-24 04:31:21 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.5686
2022-02-24 04:31:57 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.2713
2022-02-24 04:32:35 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.6172
2022-02-24 04:33:12 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.4513
2022-02-24 04:33:49 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.1550
2022-02-24 04:34:26 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.5107
2022-02-24 04:35:04 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.5085
2022-02-24 04:35:40 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.3434
2022-02-24 04:36:17 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.2724
2022-02-24 04:36:55 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.4302
2022-02-24 04:37:31 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.2710
2022-02-24 04:38:09 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.5000
2022-02-24 04:38:45 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.3722
2022-02-24 04:39:23 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.2714
2022-02-24 04:39:59 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.1890
2022-02-24 04:40:36 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.3192
2022-02-24 04:41:12 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.5478
2022-02-24 04:41:49 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.3870
2022-02-24 04:42:26 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.3272
2022-02-24 04:43:04 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.3884
2022-02-24 04:43:39 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.2826
2022-02-24 04:44:14 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.4504
2022-02-24 04:44:15 - train: epoch 066, train_loss: 1.3831
2022-02-24 04:45:36 - eval: epoch: 066, acc1: 69.416%, acc5: 89.068%, test_loss: 1.2423, per_image_load_time: 1.710ms, per_image_inference_time: 0.185ms
2022-02-24 04:45:37 - until epoch: 066, best_acc1: 69.416%
2022-02-24 04:45:37 - epoch 067 lr: 0.0010000000000000002
2022-02-24 04:46:20 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.2319
2022-02-24 04:46:57 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.4321
2022-02-24 04:47:33 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.3869
2022-02-24 04:48:05 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.3412
2022-02-24 04:48:40 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.4338
2022-02-24 04:49:19 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.2802
2022-02-24 04:49:56 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.4592
2022-02-24 04:50:34 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.2313
2022-02-24 04:51:10 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.6818
2022-02-24 04:51:46 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.3082
2022-02-24 04:52:23 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.3577
2022-02-24 04:52:59 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.3653
2022-02-24 04:53:36 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.3707
2022-02-24 04:54:13 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.4055
2022-02-24 04:54:50 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.2841
2022-02-24 04:55:27 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.2461
2022-02-24 04:56:04 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.3113
2022-02-24 04:56:40 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.5557
2022-02-24 04:57:17 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.3940
2022-02-24 04:57:53 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.5953
2022-02-24 04:58:30 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.1835
2022-02-24 04:59:07 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.3572
2022-02-24 04:59:43 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.2953
2022-02-24 05:00:21 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.2267
2022-02-24 05:00:57 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.3836
2022-02-24 05:01:34 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.4329
2022-02-24 05:02:10 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.3238
2022-02-24 05:02:47 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.6497
2022-02-24 05:03:24 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.3270
2022-02-24 05:04:01 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.3450
2022-02-24 05:04:37 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.2199
2022-02-24 05:05:14 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.2613
2022-02-24 05:05:50 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.2576
2022-02-24 05:06:28 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.4277
2022-02-24 05:07:04 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.4042
2022-02-24 05:07:41 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.4971
2022-02-24 05:08:18 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.4222
2022-02-24 05:08:55 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.6732
2022-02-24 05:09:31 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.4690
2022-02-24 05:10:09 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.3808
2022-02-24 05:10:44 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.4416
2022-02-24 05:11:22 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.4721
2022-02-24 05:11:58 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.3737
2022-02-24 05:12:35 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.2466
2022-02-24 05:13:12 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.4382
2022-02-24 05:13:48 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.2733
2022-02-24 05:14:26 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.5161
2022-02-24 05:15:02 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.1553
2022-02-24 05:15:39 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.4412
2022-02-24 05:16:14 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.4561
2022-02-24 05:16:14 - train: epoch 067, train_loss: 1.3790
2022-02-24 05:17:37 - eval: epoch: 067, acc1: 69.352%, acc5: 89.148%, test_loss: 1.2403, per_image_load_time: 2.698ms, per_image_inference_time: 0.160ms
2022-02-24 05:17:38 - until epoch: 067, best_acc1: 69.416%
2022-02-24 05:17:38 - epoch 068 lr: 0.0010000000000000002
2022-02-24 05:18:20 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.5045
2022-02-24 05:18:58 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.4287
2022-02-24 05:19:34 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.2639
2022-02-24 05:20:11 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.2937
2022-02-24 05:20:47 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.3295
2022-02-24 05:21:25 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.3699
2022-02-24 05:22:01 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.3529
2022-02-24 05:22:39 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.2451
2022-02-24 05:23:15 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.4421
2022-02-24 05:23:52 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.2119
2022-02-24 05:24:28 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.5159
2022-02-24 05:25:05 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.2623
2022-02-24 05:25:42 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.2020
2022-02-24 05:26:20 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.3107
2022-02-24 05:26:57 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.5720
2022-02-24 05:27:33 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.3668
2022-02-24 05:28:10 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.2875
2022-02-24 05:28:46 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.4070
2022-02-24 05:29:24 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.3199
2022-02-24 05:30:00 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.5055
2022-02-24 05:30:37 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.3506
2022-02-24 05:31:14 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.4363
2022-02-24 05:31:50 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.3049
2022-02-24 05:32:28 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.3982
2022-02-24 05:33:04 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.4620
2022-02-24 05:33:42 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.1115
2022-02-24 05:34:17 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.4248
2022-02-24 05:34:54 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.4571
2022-02-24 05:35:31 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.4799
2022-02-24 05:36:08 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.3306
2022-02-24 05:36:45 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.3226
2022-02-24 05:37:21 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.2066
2022-02-24 05:37:58 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.2507
2022-02-24 05:38:35 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.3213
2022-02-24 05:39:12 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.3656
2022-02-24 05:39:50 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.2557
2022-02-24 05:40:27 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.2455
2022-02-24 05:41:03 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.4448
2022-02-24 05:41:40 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.4290
2022-02-24 05:42:16 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.3941
2022-02-24 05:42:54 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.1675
2022-02-24 05:43:31 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.4047
2022-02-24 05:44:08 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.3540
2022-02-24 05:44:45 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.3455
2022-02-24 05:45:21 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.3440
2022-02-24 05:45:58 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.4622
2022-02-24 05:46:35 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.4769
2022-02-24 05:47:12 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.4097
2022-02-24 05:47:49 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.3471
2022-02-24 05:48:23 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.2774
2022-02-24 05:48:24 - train: epoch 068, train_loss: 1.3724
2022-02-24 05:49:46 - eval: epoch: 068, acc1: 69.534%, acc5: 89.144%, test_loss: 1.2337, per_image_load_time: 2.580ms, per_image_inference_time: 0.170ms
2022-02-24 05:49:46 - until epoch: 068, best_acc1: 69.534%
2022-02-24 05:49:46 - epoch 069 lr: 0.0010000000000000002
2022-02-24 05:50:29 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.5051
2022-02-24 05:51:05 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.4397
2022-02-24 05:51:42 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.3794
2022-02-24 05:52:20 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.2983
2022-02-24 05:52:55 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.4199
2022-02-24 05:53:33 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.3754
2022-02-24 05:54:10 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.3119
2022-02-24 05:54:48 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.4769
2022-02-24 05:55:23 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.3179
2022-02-24 05:56:01 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.4148
2022-02-24 05:56:37 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.3702
2022-02-24 05:57:14 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.2642
2022-02-24 05:57:50 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.4086
2022-02-24 05:58:28 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.4147
2022-02-24 05:59:04 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.2704
2022-02-24 05:59:41 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.4114
2022-02-24 06:00:18 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.2734
2022-02-24 06:00:55 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.2475
2022-02-24 06:01:31 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.3509
2022-02-24 06:02:09 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.2822
2022-02-24 06:02:44 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.3121
2022-02-24 06:03:21 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.4359
2022-02-24 06:03:58 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.2490
2022-02-24 06:04:35 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.5065
2022-02-24 06:05:12 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.2353
2022-02-24 06:05:49 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.3336
2022-02-24 06:06:25 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.6263
2022-02-24 06:07:02 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.3028
2022-02-24 06:07:38 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.4054
2022-02-24 06:08:16 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.2503
2022-02-24 06:08:52 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.3692
2022-02-24 06:09:30 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.2512
2022-02-24 06:10:05 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.4046
2022-02-24 06:10:42 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.2764
2022-02-24 06:11:20 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.3708
2022-02-24 06:11:56 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.2937
2022-02-24 06:12:33 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.4320
2022-02-24 06:13:09 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.3758
2022-02-24 06:13:46 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.4862
2022-02-24 06:14:23 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.4298
2022-02-24 06:14:59 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.2786
2022-02-24 06:15:36 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.3451
2022-02-24 06:16:13 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.2587
2022-02-24 06:16:49 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.4669
2022-02-24 06:17:25 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.5059
2022-02-24 06:18:02 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.4047
2022-02-24 06:18:39 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.3245
2022-02-24 06:19:17 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.4766
2022-02-24 06:19:53 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.1054
2022-02-24 06:20:27 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.4127
2022-02-24 06:20:29 - train: epoch 069, train_loss: 1.3684
2022-02-24 06:21:52 - eval: epoch: 069, acc1: 69.474%, acc5: 89.132%, test_loss: 1.2331, per_image_load_time: 1.269ms, per_image_inference_time: 0.158ms
2022-02-24 06:21:53 - until epoch: 069, best_acc1: 69.534%
2022-02-24 06:21:53 - epoch 070 lr: 0.0010000000000000002
2022-02-24 06:22:35 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.4464
2022-02-24 06:23:12 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.2747
2022-02-24 06:23:48 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.5043
2022-02-24 06:24:24 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.1466
2022-02-24 06:25:00 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.5216
2022-02-24 06:25:38 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.3512
2022-02-24 06:26:14 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.4883
2022-02-24 06:26:51 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.3370
2022-02-24 06:27:27 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.3942
2022-02-24 06:28:04 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.4050
2022-02-24 06:28:42 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.5839
2022-02-24 06:29:18 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.2688
2022-02-24 06:29:55 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.1561
2022-02-24 06:30:32 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.2108
2022-02-24 06:31:09 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.3871
2022-02-24 06:31:44 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.4254
2022-02-24 06:32:22 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.4933
2022-02-24 06:32:59 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.3319
2022-02-24 06:33:36 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.4326
2022-02-24 06:34:12 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.3056
2022-02-24 06:34:49 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.3268
2022-02-24 06:35:25 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.5062
2022-02-24 06:36:02 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.5589
2022-02-24 06:36:40 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.2753
2022-02-24 06:37:18 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.1542
2022-02-24 06:37:54 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.2161
2022-02-24 06:38:31 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.4085
2022-02-24 06:39:07 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.3889
2022-02-24 06:39:45 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.4576
2022-02-24 06:40:21 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.2900
2022-02-24 06:40:58 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.2141
2022-02-24 06:41:34 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.4248
2022-02-24 06:42:11 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.4107
2022-02-24 06:42:47 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.3323
2022-02-24 06:43:25 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.3667
2022-02-24 06:44:02 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.4153
2022-02-24 06:44:38 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.4414
2022-02-24 06:45:15 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.4436
2022-02-24 06:45:51 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.1353
2022-02-24 06:46:28 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.3486
2022-02-24 06:47:06 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.3596
2022-02-24 06:47:41 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.4266
2022-02-24 06:48:18 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.5262
2022-02-24 06:48:54 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.4666
2022-02-24 06:49:31 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.3431
2022-02-24 06:50:08 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.6003
2022-02-24 06:50:46 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.2490
2022-02-24 06:51:21 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.2238
2022-02-24 06:51:59 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.3588
2022-02-24 06:52:34 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.4465
2022-02-24 06:52:35 - train: epoch 070, train_loss: 1.3621
2022-02-24 06:53:57 - eval: epoch: 070, acc1: 69.708%, acc5: 89.266%, test_loss: 1.2282, per_image_load_time: 2.773ms, per_image_inference_time: 0.163ms
2022-02-24 06:53:58 - until epoch: 070, best_acc1: 69.708%
2022-02-24 06:53:58 - epoch 071 lr: 0.0010000000000000002
2022-02-24 06:54:40 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.2298
2022-02-24 06:55:18 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.2785
2022-02-24 06:55:53 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.5178
2022-02-24 06:56:27 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.3833
2022-02-24 06:57:02 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.5810
2022-02-24 06:57:40 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.4271
2022-02-24 06:58:19 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.4260
2022-02-24 06:58:55 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.2974
2022-02-24 06:59:32 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.3990
2022-02-24 07:00:08 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.2876
2022-02-24 07:00:45 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.4209
2022-02-24 07:01:22 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.2737
2022-02-24 07:01:59 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.1689
2022-02-24 07:02:35 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.3117
2022-02-24 07:03:11 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.3803
2022-02-24 07:03:49 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.2629
2022-02-24 07:04:24 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.3648
2022-02-24 07:05:02 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.5210
2022-02-24 07:05:38 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.3756
2022-02-24 07:06:15 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.2841
2022-02-24 07:06:51 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.3755
2022-02-24 07:07:28 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.2577
2022-02-24 07:08:05 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.3407
2022-02-24 07:08:42 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.3766
2022-02-24 07:09:18 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.3790
2022-02-24 07:09:56 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.2957
2022-02-24 07:10:32 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.5368
2022-02-24 07:11:09 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.4975
2022-02-24 07:11:45 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.2696
2022-02-24 07:12:23 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.4123
2022-02-24 07:12:59 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.2891
2022-02-24 07:13:35 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.3773
2022-02-24 07:14:12 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.4061
2022-02-24 07:14:48 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.3042
2022-02-24 07:15:25 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.4970
2022-02-24 07:16:02 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.3876
2022-02-24 07:16:39 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.3344
2022-02-24 07:17:16 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.2800
2022-02-24 07:17:52 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.5710
2022-02-24 07:18:30 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.2345
2022-02-24 07:19:06 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.3159
2022-02-24 07:19:43 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.5497
2022-02-24 07:20:19 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.1532
2022-02-24 07:20:56 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.4707
2022-02-24 07:21:33 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.3651
2022-02-24 07:22:10 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.1760
2022-02-24 07:22:47 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.2830
2022-02-24 07:23:23 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.3582
2022-02-24 07:24:00 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.1055
2022-02-24 07:24:35 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.2231
2022-02-24 07:24:35 - train: epoch 071, train_loss: 1.3566
2022-02-24 07:25:59 - eval: epoch: 071, acc1: 69.756%, acc5: 89.220%, test_loss: 1.2253, per_image_load_time: 3.057ms, per_image_inference_time: 0.174ms
2022-02-24 07:25:59 - until epoch: 071, best_acc1: 69.756%
2022-02-24 07:25:59 - epoch 072 lr: 0.0010000000000000002
2022-02-24 07:26:41 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.4194
2022-02-24 07:27:19 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.1944
2022-02-24 07:27:55 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.2259
2022-02-24 07:28:32 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.4989
2022-02-24 07:29:09 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.4092
2022-02-24 07:29:47 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.2874
2022-02-24 07:30:22 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.3401
2022-02-24 07:31:00 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.5189
2022-02-24 07:31:35 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.3678
2022-02-24 07:32:13 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.3042
2022-02-24 07:32:50 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.6160
2022-02-24 07:33:27 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.1706
2022-02-24 07:34:04 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.4425
2022-02-24 07:34:40 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.1822
2022-02-24 07:35:17 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.2958
2022-02-24 07:35:53 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.2923
2022-02-24 07:36:31 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.2767
2022-02-24 07:37:07 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.1758
2022-02-24 07:37:43 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.2294
2022-02-24 07:38:20 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.2858
2022-02-24 07:38:57 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.4948
2022-02-24 07:39:34 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.3553
2022-02-24 07:40:12 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.4145
2022-02-24 07:40:47 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.5058
2022-02-24 07:41:24 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.5381
2022-02-24 07:42:00 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.4805
2022-02-24 07:42:37 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.5906
2022-02-24 07:43:15 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.4939
2022-02-24 07:43:51 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.2900
2022-02-24 07:44:27 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.2963
2022-02-24 07:45:05 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.3437
2022-02-24 07:45:42 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.6178
2022-02-24 07:46:18 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.4446
2022-02-24 07:46:56 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.4363
2022-02-24 07:47:32 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.2047
2022-02-24 07:48:09 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.3325
2022-02-24 07:48:45 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.4208
2022-02-24 07:49:23 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.3523
2022-02-24 07:49:59 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.2758
2022-02-24 07:50:37 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.3887
2022-02-24 07:51:13 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.4922
2022-02-24 07:51:49 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.3556
2022-02-24 07:52:26 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.2723
2022-02-24 07:53:02 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.3705
2022-02-24 07:53:39 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.3175
2022-02-24 07:54:16 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.2249
2022-02-24 07:54:52 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.4206
2022-02-24 07:55:29 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.4125
2022-02-24 07:56:05 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.4386
2022-02-24 07:56:41 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.5186
2022-02-24 07:56:41 - train: epoch 072, train_loss: 1.3542
2022-02-24 07:58:05 - eval: epoch: 072, acc1: 69.832%, acc5: 89.258%, test_loss: 1.2278, per_image_load_time: 3.108ms, per_image_inference_time: 0.175ms
2022-02-24 07:58:05 - until epoch: 072, best_acc1: 69.832%
2022-02-24 07:58:05 - epoch 073 lr: 0.0010000000000000002
2022-02-24 07:58:48 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.5375
2022-02-24 07:59:25 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.3895
2022-02-24 08:00:01 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.2192
2022-02-24 08:00:37 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.2056
2022-02-24 08:01:15 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.1923
2022-02-24 08:01:52 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.3692
2022-02-24 08:02:29 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.3782
2022-02-24 08:03:05 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.2163
2022-02-24 08:03:43 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.1980
2022-02-24 08:04:19 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.2543
2022-02-24 08:04:57 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.4829
2022-02-24 08:05:33 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.5295
2022-02-24 08:06:09 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.5456
2022-02-24 08:06:45 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.3128
2022-02-24 08:07:22 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.2696
2022-02-24 08:08:00 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.2591
2022-02-24 08:08:35 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.3785
2022-02-24 08:09:12 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.3204
2022-02-24 08:09:49 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.3170
2022-02-24 08:10:25 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.2930
2022-02-24 08:11:03 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.3291
2022-02-24 08:11:41 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.3607
2022-02-24 08:12:16 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.3988
2022-02-24 08:12:54 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.5585
2022-02-24 08:13:30 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.2879
2022-02-24 08:14:08 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.1821
2022-02-24 08:14:43 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.4554
2022-02-24 08:15:21 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.3128
2022-02-24 08:15:58 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.4415
2022-02-24 08:16:34 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.0891
2022-02-24 08:17:12 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.3311
2022-02-24 08:17:47 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.3119
2022-02-24 08:18:25 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.3310
2022-02-24 08:19:01 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.3991
2022-02-24 08:19:39 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.3329
2022-02-24 08:20:14 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.3902
2022-02-24 08:20:51 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.4329
2022-02-24 08:21:27 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.5543
2022-02-24 08:22:05 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.4156
2022-02-24 08:22:41 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.4587
2022-02-24 08:23:18 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.3180
2022-02-24 08:23:54 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.5932
2022-02-24 08:24:30 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.3139
2022-02-24 08:25:07 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.4538
2022-02-24 08:25:44 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.2918
2022-02-24 08:26:21 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.4757
2022-02-24 08:26:58 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.1556
2022-02-24 08:27:34 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.4427
2022-02-24 08:28:10 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.3286
2022-02-24 08:28:45 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.2647
2022-02-24 08:28:46 - train: epoch 073, train_loss: 1.3490
2022-02-24 08:30:08 - eval: epoch: 073, acc1: 69.684%, acc5: 89.300%, test_loss: 1.2290, per_image_load_time: 2.379ms, per_image_inference_time: 0.161ms
2022-02-24 08:30:08 - until epoch: 073, best_acc1: 69.832%
2022-02-24 08:30:08 - epoch 074 lr: 0.0010000000000000002
2022-02-24 08:30:51 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.2504
2022-02-24 08:31:28 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.3310
2022-02-24 08:32:04 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.5088
2022-02-24 08:32:41 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.4425
2022-02-24 08:33:17 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2350
2022-02-24 08:33:54 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.3433
2022-02-24 08:34:30 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.4648
2022-02-24 08:35:07 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.4460
2022-02-24 08:35:44 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.2899
2022-02-24 08:36:22 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.4675
2022-02-24 08:36:58 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.2569
2022-02-24 08:37:36 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.4138
2022-02-24 08:38:11 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.4993
2022-02-24 08:38:48 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.3190
2022-02-24 08:39:24 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.5616
2022-02-24 08:40:01 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.1949
2022-02-24 08:40:37 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.3891
2022-02-24 08:41:14 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.5496
2022-02-24 08:41:50 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.3063
2022-02-24 08:42:27 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.9544
2022-02-24 08:43:03 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.4426
2022-02-24 08:43:40 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.4927
2022-02-24 08:44:16 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.5151
2022-02-24 08:44:53 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.3907
2022-02-24 08:45:28 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.3556
2022-02-24 08:46:06 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.3775
2022-02-24 08:46:42 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.2808
2022-02-24 08:47:20 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.5119
2022-02-24 08:47:56 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.3430
2022-02-24 08:48:33 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.4099
2022-02-24 08:49:09 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.5077
2022-02-24 08:49:43 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.2409
2022-02-24 08:50:16 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.4304
2022-02-24 08:50:49 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.3733
2022-02-24 08:51:22 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.2896
2022-02-24 08:51:54 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.4368
2022-02-24 08:52:27 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.3457
2022-02-24 08:53:00 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.1972
2022-02-24 08:53:32 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.4227
2022-02-24 08:54:06 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.3130
2022-02-24 08:54:38 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.3136
2022-02-24 08:55:12 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.4345
2022-02-24 08:55:44 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.4388
2022-02-24 08:56:18 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.1410
2022-02-24 08:56:51 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.1665
2022-02-24 08:57:23 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.5108
2022-02-24 08:57:57 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.2582
2022-02-24 08:58:29 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.3209
2022-02-24 08:59:03 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.6982
2022-02-24 08:59:34 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.5206
2022-02-24 08:59:35 - train: epoch 074, train_loss: 1.3459
2022-02-24 09:00:49 - eval: epoch: 074, acc1: 69.684%, acc5: 89.216%, test_loss: 1.2273, per_image_load_time: 2.575ms, per_image_inference_time: 0.155ms
2022-02-24 09:00:49 - until epoch: 074, best_acc1: 69.832%
2022-02-24 09:00:49 - epoch 075 lr: 0.0010000000000000002
2022-02-24 09:01:28 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.5469
2022-02-24 09:02:01 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.3024
2022-02-24 09:02:33 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.2672
2022-02-24 09:03:06 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.1167
2022-02-24 09:03:40 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.2741
2022-02-24 09:04:12 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.2519
2022-02-24 09:04:45 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.5061
2022-02-24 09:05:19 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.4553
2022-02-24 09:05:51 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.3296
2022-02-24 09:06:23 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.1815
2022-02-24 09:06:57 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.3543
2022-02-24 09:07:30 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.1533
2022-02-24 09:08:02 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.2355
2022-02-24 09:08:36 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.2133
2022-02-24 09:09:08 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.3428
2022-02-24 09:09:41 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.3005
2022-02-24 09:10:14 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.5469
2022-02-24 09:10:46 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.3364
2022-02-24 09:11:19 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.2894
2022-02-24 09:11:53 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.2720
2022-02-24 09:12:25 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.3887
2022-02-24 09:12:59 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.3903
2022-02-24 09:13:31 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.3601
2022-02-24 09:14:04 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.1710
2022-02-24 09:14:37 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.3043
2022-02-24 09:15:10 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.4791
2022-02-24 09:15:44 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.1869
2022-02-24 09:16:15 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.2001
2022-02-24 09:16:48 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.5348
2022-02-24 09:17:20 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.4581
2022-02-24 09:17:54 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.4012
2022-02-24 09:18:29 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.4148
2022-02-24 09:19:02 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.3451
2022-02-24 09:19:37 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.2569
2022-02-24 09:20:10 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.3218
2022-02-24 09:20:44 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.4439
2022-02-24 09:21:16 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.4015
2022-02-24 09:21:49 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.1895
2022-02-24 09:22:22 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.3580
2022-02-24 09:22:55 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.3313
2022-02-24 09:23:29 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.1894
2022-02-24 09:24:09 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.3094
2022-02-24 09:24:54 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.4587
2022-02-24 09:25:37 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.2917
2022-02-24 09:26:19 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.5227
2022-02-24 09:26:59 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.2002
2022-02-24 09:27:35 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.3639
2022-02-24 09:28:13 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.2300
2022-02-24 09:28:50 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.2286
2022-02-24 09:29:25 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.2095
2022-02-24 09:29:26 - train: epoch 075, train_loss: 1.3443
2022-02-24 09:30:55 - eval: epoch: 075, acc1: 69.798%, acc5: 89.422%, test_loss: 1.2210, per_image_load_time: 2.827ms, per_image_inference_time: 0.162ms
2022-02-24 09:30:55 - until epoch: 075, best_acc1: 69.832%
2022-02-24 09:30:55 - epoch 076 lr: 0.0010000000000000002
2022-02-24 09:31:39 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.4219
2022-02-24 09:32:18 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.3350
2022-02-24 09:32:58 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.4011
2022-02-24 09:33:35 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.4757
2022-02-24 09:34:12 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.2122
2022-02-24 09:34:52 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.1564
2022-02-24 09:35:32 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.3349
2022-02-24 09:36:08 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.3345
2022-02-24 09:36:45 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.3267
2022-02-24 09:37:26 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.3322
2022-02-24 09:38:02 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.4912
2022-02-24 09:38:34 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.2351
2022-02-24 09:39:11 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.3660
2022-02-24 09:39:48 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.2853
2022-02-24 09:40:22 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.3755
2022-02-24 09:40:55 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.3532
2022-02-24 09:41:33 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.3137
2022-02-24 09:42:08 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.2887
2022-02-24 09:42:41 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.6299
2022-02-24 09:43:14 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.3663
2022-02-24 09:43:54 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.4145
2022-02-24 09:44:34 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.3568
2022-02-24 09:45:13 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.3358
2022-02-24 09:45:53 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.4031
2022-02-24 09:46:34 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.3529
2022-02-24 09:47:12 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.6078
2022-02-24 09:47:51 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.4073
2022-02-24 09:48:32 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.1832
2022-02-24 09:49:09 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.3031
2022-02-24 09:49:46 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.1675
2022-02-24 09:50:24 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.3827
2022-02-24 09:51:04 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.4299
2022-02-24 09:51:41 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.5357
2022-02-24 09:52:19 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.2914
2022-02-24 09:53:00 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.1935
2022-02-24 09:53:38 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.3450
2022-02-24 09:54:16 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.3664
2022-02-24 09:54:54 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.1939
2022-02-24 09:55:36 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.2911
2022-02-24 09:56:13 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.3036
2022-02-24 09:56:50 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.3264
2022-02-24 09:57:32 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.3240
2022-02-24 09:58:13 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.4692
2022-02-24 09:58:50 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.3433
2022-02-24 09:59:28 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.2832
2022-02-24 10:00:09 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.3433
2022-02-24 10:00:44 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.5122
2022-02-24 10:01:20 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.3122
2022-02-24 10:01:58 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.3745
2022-02-24 10:02:34 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.5620
2022-02-24 10:02:34 - train: epoch 076, train_loss: 1.3378
2022-02-24 10:04:00 - eval: epoch: 076, acc1: 69.818%, acc5: 89.336%, test_loss: 1.2273, per_image_load_time: 3.139ms, per_image_inference_time: 0.190ms
2022-02-24 10:04:00 - until epoch: 076, best_acc1: 69.832%
2022-02-24 10:04:00 - epoch 077 lr: 0.0010000000000000002
2022-02-24 10:04:47 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.5196
2022-02-24 10:05:22 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.4198
2022-02-24 10:06:01 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.2557
2022-02-24 10:06:39 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.3064
2022-02-24 10:07:18 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.0927
2022-02-24 10:07:54 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.2890
2022-02-24 10:08:34 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.2213
2022-02-24 10:09:13 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.5468
2022-02-24 10:09:50 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.3935
2022-02-24 10:10:27 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.2199
2022-02-24 10:11:09 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.3214
2022-02-24 10:11:48 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.5120
2022-02-24 10:12:26 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.1818
2022-02-24 10:13:03 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.2956
2022-02-24 10:13:43 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.2943
2022-02-24 10:14:19 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.3423
2022-02-24 10:14:55 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.4490
2022-02-24 10:15:33 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.3119
2022-02-24 10:16:11 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.0920
2022-02-24 10:16:45 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.2698
2022-02-24 10:17:22 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.3655
2022-02-24 10:18:01 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.1563
2022-02-24 10:18:37 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.4891
2022-02-24 10:19:12 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.4484
2022-02-24 10:19:50 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.3871
2022-02-24 10:20:27 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.1203
2022-02-24 10:21:03 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.3618
2022-02-24 10:21:39 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.3794
2022-02-24 10:22:17 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.5342
2022-02-24 10:22:54 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.3154
2022-02-24 10:23:29 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.2939
2022-02-24 10:24:06 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.4329
2022-02-24 10:24:44 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.2811
2022-02-24 10:25:20 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.4975
2022-02-24 10:25:57 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.2732
2022-02-24 10:26:37 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.1719
2022-02-24 10:27:12 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.3699
2022-02-24 10:27:48 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.3137
2022-02-24 10:28:26 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.4140
2022-02-24 10:29:04 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.4700
2022-02-24 10:29:42 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.4444
2022-02-24 10:30:17 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.4032
2022-02-24 10:30:55 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.2952
2022-02-24 10:31:32 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.3492
2022-02-24 10:32:08 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.3209
2022-02-24 10:32:45 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.1597
2022-02-24 10:33:23 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.2190
2022-02-24 10:33:59 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.3632
2022-02-24 10:34:34 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.3521
2022-02-24 10:35:12 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.3510
2022-02-24 10:35:12 - train: epoch 077, train_loss: 1.3363
2022-02-24 10:36:33 - eval: epoch: 077, acc1: 69.726%, acc5: 89.274%, test_loss: 1.2275, per_image_load_time: 2.967ms, per_image_inference_time: 0.176ms
2022-02-24 10:36:34 - until epoch: 077, best_acc1: 69.832%
2022-02-24 10:36:34 - epoch 078 lr: 0.0010000000000000002
2022-02-24 10:37:19 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.5376
2022-02-24 10:37:56 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.5605
2022-02-24 10:38:30 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.4825
2022-02-24 10:39:08 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.2918
2022-02-24 10:39:45 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.2535
2022-02-24 10:40:21 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.2682
2022-02-24 10:40:56 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.3470
2022-02-24 10:41:35 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.4080
2022-02-24 10:42:11 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.4714
2022-02-24 10:42:48 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.3000
2022-02-24 10:43:22 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.2998
2022-02-24 10:44:02 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.3640
2022-02-24 10:44:38 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.2585
2022-02-24 10:45:14 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.1496
2022-02-24 10:45:49 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.2073
2022-02-24 10:46:30 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.2852
2022-02-24 10:47:05 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.4513
2022-02-24 10:47:41 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.4487
2022-02-24 10:48:18 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.4346
2022-02-24 10:48:57 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.2987
2022-02-24 10:49:32 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.4162
2022-02-24 10:50:09 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.3045
2022-02-24 10:50:49 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.2500
2022-02-24 10:51:26 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.3054
2022-02-24 10:52:02 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.2460
2022-02-24 10:52:39 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.2455
2022-02-24 10:53:18 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.4440
2022-02-24 10:53:54 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.2530
2022-02-24 10:54:31 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.2501
2022-02-24 10:55:09 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.4755
2022-02-24 10:55:46 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.3496
2022-02-24 10:56:22 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.4070
2022-02-24 10:57:00 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.5317
2022-02-24 10:57:38 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.2979
2022-02-24 10:58:15 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.3682
2022-02-24 10:58:50 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.4671
2022-02-24 10:59:29 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.2119
2022-02-24 11:00:05 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.2626
2022-02-24 11:00:40 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.3312
2022-02-24 11:01:15 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.1983
2022-02-24 11:01:55 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.4451
2022-02-24 11:02:31 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.3554
2022-02-24 11:03:06 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.3754
2022-02-24 11:03:45 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.2386
2022-02-24 11:04:21 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.3578
2022-02-24 11:04:58 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.1802
2022-02-24 11:05:33 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.4634
2022-02-24 11:06:12 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.5238
2022-02-24 11:06:48 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.2773
2022-02-24 11:07:22 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.3219
2022-02-24 11:07:23 - train: epoch 078, train_loss: 1.3337
2022-02-24 11:08:48 - eval: epoch: 078, acc1: 69.598%, acc5: 89.312%, test_loss: 1.2277, per_image_load_time: 1.517ms, per_image_inference_time: 0.180ms
2022-02-24 11:08:49 - until epoch: 078, best_acc1: 69.832%
2022-02-24 11:08:49 - epoch 079 lr: 0.0010000000000000002
2022-02-24 11:09:31 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.2655
2022-02-24 11:10:10 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.2727
2022-02-24 11:10:47 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.3123
2022-02-24 11:11:24 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.4771
2022-02-24 11:12:00 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.1834
2022-02-24 11:12:38 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.2258
2022-02-24 11:13:14 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.3397
2022-02-24 11:13:50 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.4257
2022-02-24 11:14:25 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.3798
2022-02-24 11:15:03 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.2591
2022-02-24 11:15:39 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.4339
2022-02-24 11:16:14 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.4986
2022-02-24 11:16:51 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.2064
2022-02-24 11:17:28 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.4598
2022-02-24 11:18:02 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.3700
2022-02-24 11:18:37 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.1297
2022-02-24 11:19:16 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.2924
2022-02-24 11:19:51 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.4709
2022-02-24 11:20:26 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.3863
2022-02-24 11:21:03 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.5979
2022-02-24 11:21:39 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.3345
2022-02-24 11:22:16 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.2859
2022-02-24 11:22:50 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.3468
2022-02-24 11:23:28 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.3919
2022-02-24 11:24:04 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.3714
2022-02-24 11:24:40 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.3860
2022-02-24 11:25:15 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.1372
2022-02-24 11:25:52 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.2122
2022-02-24 11:26:28 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.2577
2022-02-24 11:27:04 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.4674
2022-02-24 11:27:40 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.3590
2022-02-24 11:28:18 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.4779
2022-02-24 11:28:52 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.3068
2022-02-24 11:29:30 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.3411
2022-02-24 11:30:08 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.3090
2022-02-24 11:30:44 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.1850
2022-02-24 11:31:19 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.3065
2022-02-24 11:31:58 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.2871
2022-02-24 11:32:36 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.2399
2022-02-24 11:33:09 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.1403
2022-02-24 11:33:46 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.3821
2022-02-24 11:34:26 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.1309
2022-02-24 11:35:02 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.0943
2022-02-24 11:35:36 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.4539
2022-02-24 11:36:15 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.5343
2022-02-24 11:36:53 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.3538
2022-02-24 11:37:28 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.2788
2022-02-24 11:38:05 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.4489
2022-02-24 11:38:44 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.3414
2022-02-24 11:39:18 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.1429
2022-02-24 11:39:20 - train: epoch 079, train_loss: 1.3305
2022-02-24 11:40:46 - eval: epoch: 079, acc1: 69.904%, acc5: 89.308%, test_loss: 1.2230, per_image_load_time: 3.177ms, per_image_inference_time: 0.178ms
2022-02-24 11:40:47 - until epoch: 079, best_acc1: 69.904%
2022-02-24 11:40:47 - epoch 080 lr: 0.0010000000000000002
2022-02-24 11:41:29 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.1470
2022-02-24 11:42:09 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.5899
2022-02-24 11:42:50 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.2325
2022-02-24 11:43:36 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.3856
2022-02-24 11:44:18 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.2623
2022-02-24 11:44:57 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.3660
2022-02-24 11:45:40 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.3659
2022-02-24 11:46:22 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.2086
2022-02-24 11:47:07 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.3571
2022-02-24 11:47:45 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.2556
2022-02-24 11:48:22 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.3032
2022-02-24 11:49:01 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.3245
2022-02-24 11:49:40 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.1869
2022-02-24 11:50:15 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.2195
2022-02-24 11:50:50 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.2530
2022-02-24 11:51:29 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.1880
2022-02-24 11:52:07 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.4421
2022-02-24 11:52:43 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.3328
2022-02-24 11:53:20 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.3859
2022-02-24 11:53:59 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.2759
2022-02-24 11:54:36 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.5547
2022-02-24 11:55:13 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.3970
2022-02-24 11:55:50 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.2666
2022-02-24 11:56:28 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.2639
2022-02-24 11:57:04 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.2040
2022-02-24 11:57:42 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.2765
2022-02-24 11:58:26 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.3350
2022-02-24 11:59:15 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.3043
2022-02-24 11:59:51 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.4487
2022-02-24 12:00:29 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.2025
2022-02-24 12:01:05 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.2102
2022-02-24 12:01:41 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.2764
2022-02-24 12:02:19 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.4783
2022-02-24 12:02:58 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.2093
2022-02-24 12:03:32 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.3914
2022-02-24 12:04:08 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.3484
2022-02-24 12:04:47 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.3393
2022-02-24 12:05:25 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.3597
2022-02-24 12:06:02 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.3083
2022-02-24 12:06:41 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.2806
2022-02-24 12:07:17 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.6920
2022-02-24 12:07:52 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.2784
2022-02-24 12:08:27 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.6102
2022-02-24 12:09:05 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.3350
2022-02-24 12:09:42 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.1983
2022-02-24 12:10:15 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.3772
2022-02-24 12:10:52 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.2345
2022-02-24 12:11:30 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.3700
2022-02-24 12:12:05 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.4580
2022-02-24 12:12:37 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.2915
2022-02-24 12:12:38 - train: epoch 080, train_loss: 1.3308
2022-02-24 12:14:04 - eval: epoch: 080, acc1: 69.732%, acc5: 89.334%, test_loss: 1.2227, per_image_load_time: 3.076ms, per_image_inference_time: 0.186ms
2022-02-24 12:14:05 - until epoch: 080, best_acc1: 69.904%
2022-02-24 12:14:05 - epoch 081 lr: 0.0010000000000000002
2022-02-24 12:14:51 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.3183
2022-02-24 12:15:29 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.3346
2022-02-24 12:16:05 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.4254
2022-02-24 12:16:40 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.1986
2022-02-24 12:17:18 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.5687
2022-02-24 12:17:56 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.4379
2022-02-24 12:18:34 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.2457
2022-02-24 12:19:10 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.3366
2022-02-24 12:19:49 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.1678
2022-02-24 12:20:28 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.3093
2022-02-24 12:21:03 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.2306
2022-02-24 12:21:40 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.5106
2022-02-24 12:22:21 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.3646
2022-02-24 12:22:57 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.0785
2022-02-24 12:23:33 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.4152
2022-02-24 12:24:11 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.1491
2022-02-24 12:24:52 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.2020
2022-02-24 12:25:28 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.4229
2022-02-24 12:26:02 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.2337
2022-02-24 12:26:42 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.2456
2022-02-24 12:27:18 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.2949
2022-02-24 12:27:54 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.3176
2022-02-24 12:28:33 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.2791
2022-02-24 12:29:12 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.2969
2022-02-24 12:29:51 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.5418
2022-02-24 12:30:32 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.3939
2022-02-24 12:31:11 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.2848
2022-02-24 12:31:50 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.2985
2022-02-24 12:32:30 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.3172
2022-02-24 12:33:08 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.1811
2022-02-24 12:33:44 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.2032
2022-02-24 12:34:19 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.4186
2022-02-24 12:35:00 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.4130
2022-02-24 12:35:39 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.3079
2022-02-24 12:36:18 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.4000
2022-02-24 12:36:57 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.2629
2022-02-24 12:37:36 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.3094
2022-02-24 12:38:13 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.2840
2022-02-24 12:38:52 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.4857
2022-02-24 12:39:31 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.1885
2022-02-24 12:40:09 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.2171
2022-02-24 12:40:50 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.3169
2022-02-24 12:41:29 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.3416
2022-02-24 12:42:09 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.4114
2022-02-24 12:42:48 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.4176
2022-02-24 12:43:27 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.1895
2022-02-24 12:44:06 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.3620
2022-02-24 12:44:44 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.2602
2022-02-24 12:45:23 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.2794
2022-02-24 12:46:01 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.1411
2022-02-24 12:46:02 - train: epoch 081, train_loss: 1.3288
2022-02-24 12:47:31 - eval: epoch: 081, acc1: 69.862%, acc5: 89.336%, test_loss: 1.2237, per_image_load_time: 2.770ms, per_image_inference_time: 0.172ms
2022-02-24 12:47:31 - until epoch: 081, best_acc1: 69.904%
2022-02-24 12:47:31 - epoch 082 lr: 0.0010000000000000002
2022-02-24 12:48:18 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.2696
2022-02-24 12:48:58 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.2355
2022-02-24 12:49:40 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.2582
2022-02-24 12:50:19 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.3069
2022-02-24 12:50:59 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.3920
2022-02-24 12:51:40 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.1617
2022-02-24 12:52:17 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.5389
2022-02-24 12:52:54 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.3665
2022-02-24 12:53:33 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.3099
2022-02-24 12:54:10 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.2851
2022-02-24 12:54:46 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.4393
2022-02-24 12:55:25 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.4161
2022-02-24 12:56:05 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.3459
2022-02-24 12:56:41 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.3520
2022-02-24 12:57:19 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.3510
2022-02-24 12:57:56 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.2128
2022-02-24 12:58:36 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.2808
2022-02-24 12:59:15 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.2135
2022-02-24 12:59:53 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.3862
2022-02-24 13:00:35 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.2692
2022-02-24 13:01:13 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.3357
2022-02-24 13:01:53 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.4265
2022-02-24 13:02:33 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.3186
2022-02-24 13:03:12 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.2799
2022-02-24 13:03:52 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.1816
2022-02-24 13:04:32 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.1289
2022-02-24 13:05:13 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.2540
2022-02-24 13:05:52 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.1691
2022-02-24 13:06:29 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.3252
2022-02-24 13:07:11 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.2515
2022-02-24 13:07:50 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.3429
2022-02-24 13:08:30 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.4724
2022-02-24 13:09:10 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.3633
2022-02-24 13:09:49 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.2196
2022-02-24 13:10:26 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.3155
2022-02-24 13:11:06 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.2938
2022-02-24 13:11:46 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.1462
2022-02-24 13:12:24 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.4455
2022-02-24 13:13:05 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.2886
2022-02-24 13:13:43 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.3078
2022-02-24 13:14:23 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.2742
2022-02-24 13:15:02 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.5064
2022-02-24 13:15:44 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.2251
2022-02-24 13:16:21 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.2930
2022-02-24 13:17:01 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.0521
2022-02-24 13:17:41 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.2719
2022-02-24 13:18:20 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.2512
2022-02-24 13:19:00 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.2759
2022-02-24 13:19:41 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.3363
2022-02-24 13:20:18 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.3162
2022-02-24 13:20:18 - train: epoch 082, train_loss: 1.3247
2022-02-24 13:21:46 - eval: epoch: 082, acc1: 69.996%, acc5: 89.328%, test_loss: 1.2212, per_image_load_time: 3.016ms, per_image_inference_time: 0.194ms
2022-02-24 13:21:47 - until epoch: 082, best_acc1: 69.996%
2022-02-24 13:21:47 - epoch 083 lr: 0.0010000000000000002
2022-02-24 13:22:28 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.2862
2022-02-24 13:23:07 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.2874
2022-02-24 13:23:46 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.3244
2022-02-24 13:24:22 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.3419
2022-02-24 13:25:00 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.3192
2022-02-24 13:25:37 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.2690
2022-02-24 13:26:14 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.2204
2022-02-24 13:26:51 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.3891
2022-02-24 13:27:28 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.4325
2022-02-24 13:28:04 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.4672
2022-02-24 13:28:40 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.3505
2022-02-24 13:29:15 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.3510
2022-02-24 13:29:53 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.4385
2022-02-24 13:30:28 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.3107
2022-02-24 13:31:01 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.3236
2022-02-24 13:31:40 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.2801
2022-02-24 13:32:15 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.3637
2022-02-24 13:32:48 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.5692
2022-02-24 13:33:22 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.2121
2022-02-24 13:33:59 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.0347
2022-02-24 13:34:36 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.1723
2022-02-24 13:35:08 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.2607
2022-02-24 13:35:43 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.3615
2022-02-24 13:36:20 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.2369
2022-02-24 13:36:56 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.2161
2022-02-24 13:37:28 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.4351
2022-02-24 13:38:05 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.3018
2022-02-24 13:38:41 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.2096
2022-02-24 13:39:16 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.3540
2022-02-24 13:39:48 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.3561
2022-02-24 13:40:25 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.2269
2022-02-24 13:41:01 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.3738
2022-02-24 13:41:36 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.4288
2022-02-24 13:42:08 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.2141
2022-02-24 13:42:47 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.3234
2022-02-24 13:43:22 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.2425
2022-02-24 13:43:54 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.2815
2022-02-24 13:44:29 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.1663
2022-02-24 13:45:08 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.4255
2022-02-24 13:45:43 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.4758
2022-02-24 13:46:16 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.2439
2022-02-24 13:46:51 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.6245
2022-02-24 13:47:29 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.2876
2022-02-24 13:48:03 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.2051
2022-02-24 13:48:38 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.2061
2022-02-24 13:49:16 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.3293
2022-02-24 13:49:53 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.7056
2022-02-24 13:50:28 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.4964
2022-02-24 13:51:02 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.2642
2022-02-24 13:51:39 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.2153
2022-02-24 13:51:40 - train: epoch 083, train_loss: 1.3212
2022-02-24 13:53:00 - eval: epoch: 083, acc1: 69.888%, acc5: 89.388%, test_loss: 1.2193, per_image_load_time: 2.812ms, per_image_inference_time: 0.193ms
2022-02-24 13:53:00 - until epoch: 083, best_acc1: 69.996%
2022-02-24 13:53:00 - epoch 084 lr: 0.0010000000000000002
2022-02-24 13:53:43 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.2060
2022-02-24 13:54:22 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.5654
2022-02-24 13:54:55 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.1960
2022-02-24 13:55:29 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.2108
2022-02-24 13:56:09 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.3671
2022-02-24 13:56:44 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.5944
2022-02-24 13:57:18 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.5014
2022-02-24 13:57:54 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.4807
2022-02-24 13:58:33 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.1816
2022-02-24 13:59:07 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.4819
2022-02-24 13:59:41 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.1817
2022-02-24 14:00:17 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.4700
2022-02-24 14:00:57 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.2824
2022-02-24 14:01:31 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.4081
2022-02-24 14:02:04 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.2990
2022-02-24 14:02:44 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.2070
2022-02-24 14:03:19 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.4431
2022-02-24 14:03:53 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.3458
2022-02-24 14:04:28 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.4068
2022-02-24 14:05:08 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.1598
2022-02-24 14:05:42 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.4200
2022-02-24 14:06:17 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.1598
2022-02-24 14:06:54 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.4104
2022-02-24 14:07:32 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.3640
2022-02-24 14:08:08 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.4500
2022-02-24 14:08:42 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.3093
2022-02-24 14:09:20 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.3393
2022-02-24 14:09:57 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.3740
2022-02-24 14:10:32 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.2360
2022-02-24 14:11:07 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.3713
2022-02-24 14:11:45 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.3577
2022-02-24 14:12:22 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.1909
2022-02-24 14:12:55 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.3642
2022-02-24 14:13:31 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.4286
2022-02-24 14:14:11 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.3672
2022-02-24 14:14:45 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.4055
2022-02-24 14:15:20 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.4255
2022-02-24 14:15:58 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.4003
2022-02-24 14:16:36 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.4259
2022-02-24 14:17:09 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.4978
2022-02-24 14:17:47 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.1321
2022-02-24 14:18:24 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.2459
2022-02-24 14:19:01 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.4237
2022-02-24 14:19:33 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.6034
2022-02-24 14:20:11 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.4278
2022-02-24 14:20:50 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.3063
2022-02-24 14:21:23 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.2635
2022-02-24 14:21:59 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.1874
2022-02-24 14:22:36 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.1018
2022-02-24 14:23:12 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.4026
2022-02-24 14:23:13 - train: epoch 084, train_loss: 1.3208
2022-02-24 14:24:34 - eval: epoch: 084, acc1: 69.992%, acc5: 89.420%, test_loss: 1.2171, per_image_load_time: 1.897ms, per_image_inference_time: 0.180ms
2022-02-24 14:24:34 - until epoch: 084, best_acc1: 69.996%
2022-02-24 14:24:34 - epoch 085 lr: 0.0010000000000000002
2022-02-24 14:25:17 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.1442
2022-02-24 14:25:51 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.2455
2022-02-24 14:26:28 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.3822
2022-02-24 14:27:06 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.2451
2022-02-24 14:27:41 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.5026
2022-02-24 14:28:17 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.0911
2022-02-24 14:28:54 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.2980
2022-02-24 14:29:33 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.1668
2022-02-24 14:30:07 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.2110
2022-02-24 14:30:42 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.3725
2022-02-24 14:31:21 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.2136
2022-02-24 14:31:57 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.1591
2022-02-24 14:32:32 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.3703
2022-02-24 14:33:07 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.2281
2022-02-24 14:33:46 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.4230
2022-02-24 14:34:22 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.1972
2022-02-24 14:34:56 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.4126
2022-02-24 14:35:32 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.0177
2022-02-24 14:36:11 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.2583
2022-02-24 14:36:44 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.2448
2022-02-24 14:37:18 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.1488
2022-02-24 14:37:58 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.1985
2022-02-24 14:38:33 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.1627
2022-02-24 14:39:08 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.3223
2022-02-24 14:39:43 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.4930
2022-02-24 14:40:21 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.3488
2022-02-24 14:40:56 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.1468
2022-02-24 14:41:31 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.6035
2022-02-24 14:42:08 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.4565
2022-02-24 14:42:46 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.4196
2022-02-24 14:43:20 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.2915
2022-02-24 14:43:54 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.3156
2022-02-24 14:44:30 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.5245
2022-02-24 14:45:06 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.3251
2022-02-24 14:45:40 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.3962
2022-02-24 14:46:15 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.3356
2022-02-24 14:46:52 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.2849
2022-02-24 14:47:26 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.3819
2022-02-24 14:48:00 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.1970
2022-02-24 14:48:35 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.5226
2022-02-24 14:49:12 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.4625
2022-02-24 14:49:45 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.4563
2022-02-24 14:50:18 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.2456
2022-02-24 14:50:56 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.4286
2022-02-24 14:51:32 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.4057
2022-02-24 14:52:05 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.3721
2022-02-24 14:52:38 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.6616
2022-02-24 14:53:16 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.2674
2022-02-24 14:53:53 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.3767
2022-02-24 14:54:24 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.2623
2022-02-24 14:54:25 - train: epoch 085, train_loss: 1.3193
2022-02-24 14:55:46 - eval: epoch: 085, acc1: 69.750%, acc5: 89.418%, test_loss: 1.2205, per_image_load_time: 2.747ms, per_image_inference_time: 0.158ms
2022-02-24 14:55:46 - until epoch: 085, best_acc1: 69.996%
2022-02-24 14:55:46 - epoch 086 lr: 0.0010000000000000002
2022-02-24 14:56:26 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.1899
2022-02-24 14:57:00 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.3115
2022-02-24 14:57:37 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.3642
2022-02-24 14:58:15 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.3246
2022-02-24 14:58:48 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.3106
2022-02-24 14:59:23 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.3415
2022-02-24 15:00:00 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.1243
2022-02-24 15:00:36 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.3090
2022-02-24 15:01:10 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.1932
2022-02-24 15:01:44 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.5244
2022-02-24 15:02:22 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.4675
2022-02-24 15:02:57 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.4110
2022-02-24 15:03:30 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.3661
2022-02-24 15:04:05 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.3337
2022-02-24 15:04:41 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.2757
2022-02-24 15:05:17 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.2012
2022-02-24 15:05:53 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.3210
2022-02-24 15:06:28 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.2673
2022-02-24 15:07:05 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.3023
2022-02-24 15:07:42 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.3795
2022-02-24 15:08:16 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.4426
2022-02-24 15:08:55 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.3821
2022-02-24 15:09:32 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.1997
2022-02-24 15:10:08 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.3211
2022-02-24 15:10:42 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.2147
2022-02-24 15:11:19 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.4798
2022-02-24 15:11:56 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.1472
2022-02-24 15:12:30 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.3256
2022-02-24 15:13:03 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.0989
2022-02-24 15:13:40 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.2652
2022-02-24 15:14:17 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.3042
2022-02-24 15:14:52 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.3018
2022-02-24 15:15:25 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.1986
2022-02-24 15:16:03 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.4314
2022-02-24 15:16:40 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.3609
2022-02-24 15:17:15 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.2466
2022-02-24 15:17:52 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.3194
2022-02-24 15:18:29 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.3035
2022-02-24 15:19:07 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.2608
2022-02-24 15:19:41 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.3490
2022-02-24 15:20:17 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.2025
2022-02-24 15:20:55 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.2336
2022-02-24 15:21:31 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.4794
2022-02-24 15:22:06 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.2746
2022-02-24 15:22:41 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.3252
2022-02-24 15:23:20 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.1764
2022-02-24 15:23:58 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.1446
2022-02-24 15:24:33 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.3069
2022-02-24 15:25:09 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.2748
2022-02-24 15:25:46 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.2956
2022-02-24 15:25:46 - train: epoch 086, train_loss: 1.3136
2022-02-24 15:27:05 - eval: epoch: 086, acc1: 69.824%, acc5: 89.456%, test_loss: 1.2203, per_image_load_time: 2.902ms, per_image_inference_time: 0.173ms
2022-02-24 15:27:05 - until epoch: 086, best_acc1: 69.996%
2022-02-24 15:27:05 - epoch 087 lr: 0.0010000000000000002
2022-02-24 15:27:49 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.2546
2022-02-24 15:28:27 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.2625
2022-02-24 15:29:03 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.4507
2022-02-24 15:29:39 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.4151
2022-02-24 15:30:16 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.1265
2022-02-24 15:30:54 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.3454
2022-02-24 15:31:29 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.2641
2022-02-24 15:32:06 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.3900
2022-02-24 15:32:43 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.3111
2022-02-24 15:33:21 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.3023
2022-02-24 15:33:56 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.3486
2022-02-24 15:34:33 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.3682
2022-02-24 15:35:11 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.3333
2022-02-24 15:35:49 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.2865
2022-02-24 15:36:23 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.0748
2022-02-24 15:37:01 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.0925
2022-02-24 15:37:39 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.4648
2022-02-24 15:38:16 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.3815
2022-02-24 15:38:51 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.5485
2022-02-24 15:39:29 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.4790
2022-02-24 15:40:07 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.4703
2022-02-24 15:40:44 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.3477
2022-02-24 15:41:19 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.5581
2022-02-24 15:41:56 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.4069
2022-02-24 15:42:34 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.1851
2022-02-24 15:43:11 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.3437
2022-02-24 15:43:46 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.2219
2022-02-24 15:44:24 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.1462
2022-02-24 15:45:02 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.1822
2022-02-24 15:45:37 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.3768
2022-02-24 15:46:11 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.3915
2022-02-24 15:46:50 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.2378
2022-02-24 15:47:26 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.3951
2022-02-24 15:48:01 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.3600
2022-02-24 15:48:34 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.2691
2022-02-24 15:49:12 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.1656
2022-02-24 15:49:49 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.3102
2022-02-24 15:50:23 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.4900
2022-02-24 15:50:58 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.3970
2022-02-24 15:51:37 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.3157
2022-02-24 15:52:13 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.2744
2022-02-24 15:52:49 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.3910
2022-02-24 15:53:25 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.3003
2022-02-24 15:54:04 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.2605
2022-02-24 15:54:40 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.3533
2022-02-24 15:55:14 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.4648
2022-02-24 15:55:50 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.3562
2022-02-24 15:56:30 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.3400
2022-02-24 15:57:05 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.2336
2022-02-24 15:57:40 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.2609
2022-02-24 15:57:40 - train: epoch 087, train_loss: 1.3120
2022-02-24 15:59:07 - eval: epoch: 087, acc1: 69.922%, acc5: 89.408%, test_loss: 1.2206, per_image_load_time: 3.279ms, per_image_inference_time: 0.138ms
2022-02-24 15:59:08 - until epoch: 087, best_acc1: 69.996%
2022-02-24 15:59:08 - epoch 088 lr: 0.0010000000000000002
2022-02-24 15:59:47 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.1688
2022-02-24 16:00:24 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.1755
2022-02-24 16:01:02 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.3438
2022-02-24 16:01:40 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.3030
2022-02-24 16:02:15 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.4679
2022-02-24 16:02:52 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.2884
2022-02-24 16:03:29 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.2672
2022-02-24 16:04:06 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.3521
2022-02-24 16:04:41 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.5487
2022-02-24 16:05:18 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.2752
2022-02-24 16:05:57 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.0901
2022-02-24 16:06:33 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.3987
2022-02-24 16:07:08 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.4423
2022-02-24 16:07:44 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.1394
2022-02-24 16:08:23 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.2625
2022-02-24 16:08:59 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.2122
2022-02-24 16:09:34 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.3281
2022-02-24 16:10:14 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.1533
2022-02-24 16:10:52 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.3952
2022-02-24 16:11:28 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.1147
2022-02-24 16:12:01 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.1314
2022-02-24 16:12:41 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.2625
2022-02-24 16:13:19 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.3546
2022-02-24 16:13:55 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.3580
2022-02-24 16:14:30 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.4723
2022-02-24 16:15:10 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.2276
2022-02-24 16:15:45 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.4202
2022-02-24 16:16:21 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.4897
2022-02-24 16:16:56 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.2221
2022-02-24 16:17:37 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.3381
2022-02-24 16:18:14 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.1560
2022-02-24 16:18:49 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.2285
2022-02-24 16:19:26 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.1703
2022-02-24 16:20:04 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.2595
2022-02-24 16:20:40 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.3779
2022-02-24 16:21:15 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.2137
2022-02-24 16:21:52 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.3881
2022-02-24 16:22:32 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.4698
2022-02-24 16:23:07 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.2863
2022-02-24 16:23:43 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.0184
2022-02-24 16:24:19 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.1402
2022-02-24 16:24:58 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.4169
2022-02-24 16:25:33 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.2034
2022-02-24 16:26:08 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.0598
2022-02-24 16:26:46 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.3600
2022-02-24 16:27:24 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.3391
2022-02-24 16:28:00 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.4116
2022-02-24 16:28:34 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.2366
2022-02-24 16:29:13 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.2911
2022-02-24 16:29:50 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.2976
2022-02-24 16:29:51 - train: epoch 088, train_loss: 1.3096
2022-02-24 16:31:12 - eval: epoch: 088, acc1: 69.824%, acc5: 89.506%, test_loss: 1.2143, per_image_load_time: 2.997ms, per_image_inference_time: 0.184ms
2022-02-24 16:31:12 - until epoch: 088, best_acc1: 69.996%
2022-02-24 16:31:12 - epoch 089 lr: 0.0010000000000000002
2022-02-24 16:31:52 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.2611
2022-02-24 16:32:30 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.0748
2022-02-24 16:33:08 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.3708
2022-02-24 16:33:44 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.1843
2022-02-24 16:34:20 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.2477
2022-02-24 16:35:00 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.3813
2022-02-24 16:35:36 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.3546
2022-02-24 16:36:10 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.5027
2022-02-24 16:36:49 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.2267
2022-02-24 16:37:28 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.4753
2022-02-24 16:38:03 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.2094
2022-02-24 16:38:39 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.3995
2022-02-24 16:39:20 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.3280
2022-02-24 16:39:55 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.5255
2022-02-24 16:40:30 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.3798
2022-02-24 16:41:06 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.1640
2022-02-24 16:41:46 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.5513
2022-02-24 16:42:19 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.2975
2022-02-24 16:42:53 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.2101
2022-02-24 16:43:31 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.3282
2022-02-24 16:44:09 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.2641
2022-02-24 16:44:44 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.2972
2022-02-24 16:45:20 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.2778
2022-02-24 16:45:57 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.3504
2022-02-24 16:46:37 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.3045
2022-02-24 16:47:10 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.1632
2022-02-24 16:47:45 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.1814
2022-02-24 16:48:24 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.3213
2022-02-24 16:49:02 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.5196
2022-02-24 16:49:35 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.3063
2022-02-24 16:50:11 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.3112
2022-02-24 16:50:50 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.4053
2022-02-24 16:51:27 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.5780
2022-02-24 16:52:01 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.2279
2022-02-24 16:52:37 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.0994
2022-02-24 16:53:15 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.2704
2022-02-24 16:53:51 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.3216
2022-02-24 16:54:27 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.3087
2022-02-24 16:55:04 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.4957
2022-02-24 16:55:44 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.3225
2022-02-24 16:56:20 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.3795
2022-02-24 16:56:54 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.5110
2022-02-24 16:57:31 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.2851
2022-02-24 16:58:12 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.2924
2022-02-24 16:58:45 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 0.9745
2022-02-24 16:59:19 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.2328
2022-02-24 16:59:57 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.3448
2022-02-24 17:00:35 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.4017
2022-02-24 17:01:07 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.2206
2022-02-24 17:01:41 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.1278
2022-02-24 17:01:42 - train: epoch 089, train_loss: 1.3069
2022-02-24 17:03:05 - eval: epoch: 089, acc1: 69.988%, acc5: 89.402%, test_loss: 1.2168, per_image_load_time: 3.113ms, per_image_inference_time: 0.172ms
2022-02-24 17:03:06 - until epoch: 089, best_acc1: 69.996%
2022-02-24 17:03:06 - epoch 090 lr: 0.0010000000000000002
2022-02-24 17:03:46 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.4002
2022-02-24 17:04:25 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.2372
2022-02-24 17:05:04 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.2782
2022-02-24 17:05:40 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.1886
2022-02-24 17:06:18 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.3187
2022-02-24 17:06:57 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.6112
2022-02-24 17:07:33 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.4071
2022-02-24 17:08:09 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.2188
2022-02-24 17:08:47 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.3435
2022-02-24 17:09:26 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.2356
2022-02-24 17:10:04 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.3517
2022-02-24 17:10:37 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.0860
2022-02-24 17:11:15 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.3656
2022-02-24 17:11:54 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.0907
2022-02-24 17:12:29 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.5429
2022-02-24 17:13:07 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.3177
2022-02-24 17:13:44 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.1786
2022-02-24 17:14:20 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.1873
2022-02-24 17:14:55 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.1307
2022-02-24 17:15:33 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.4750
2022-02-24 17:16:13 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.3405
2022-02-24 17:16:49 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.3987
2022-02-24 17:17:28 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.3985
2022-02-24 17:18:06 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.3093
2022-02-24 17:18:45 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.2081
2022-02-24 17:19:22 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.3190
2022-02-24 17:20:00 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.2565
2022-02-24 17:20:39 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.3449
2022-02-24 17:21:17 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.2720
2022-02-24 17:21:54 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.1889
2022-02-24 17:22:34 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.3282
2022-02-24 17:23:13 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.3908
2022-02-24 17:23:49 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.4214
2022-02-24 17:24:27 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.3072
2022-02-24 17:25:06 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.3298
2022-02-24 17:25:45 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.2996
2022-02-24 17:26:20 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.4708
2022-02-24 17:27:01 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.3588
2022-02-24 17:27:39 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.0560
2022-02-24 17:28:17 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.1587
2022-02-24 17:28:55 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.5385
2022-02-24 17:29:36 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.4871
2022-02-24 17:30:13 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.2521
2022-02-24 17:30:50 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.5132
2022-02-24 17:31:29 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.2394
2022-02-24 17:32:10 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.2019
2022-02-24 17:32:45 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.2306
2022-02-24 17:33:22 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.3747
2022-02-24 17:34:00 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.1902
2022-02-24 17:34:35 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.2445
2022-02-24 17:34:36 - train: epoch 090, train_loss: 1.3094
2022-02-24 17:35:56 - eval: epoch: 090, acc1: 69.984%, acc5: 89.388%, test_loss: 1.2147, per_image_load_time: 2.254ms, per_image_inference_time: 0.190ms
2022-02-24 17:35:57 - until epoch: 090, best_acc1: 69.996%
2022-02-24 17:35:57 - epoch 091 lr: 0.00010000000000000003
2022-02-24 17:36:39 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.2563
2022-02-24 17:37:13 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.2221
2022-02-24 17:37:50 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.3953
2022-02-24 17:38:28 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.4358
2022-02-24 17:39:02 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.1036
2022-02-24 17:39:36 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.3013
2022-02-24 17:40:14 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.3926
2022-02-24 17:40:53 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.0637
2022-02-24 17:41:27 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.3147
2022-02-24 17:42:03 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.0912
2022-02-24 17:42:41 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.0489
2022-02-24 17:43:18 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.1999
2022-02-24 17:43:51 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.2387
2022-02-24 17:44:29 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.5174
2022-02-24 17:45:06 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.2675
2022-02-24 17:45:40 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.1687
2022-02-24 17:46:15 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.3768
2022-02-24 17:46:53 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.3372
2022-02-24 17:47:28 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.1914
2022-02-24 17:48:03 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.0515
2022-02-24 17:48:38 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.1710
2022-02-24 17:49:16 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.4045
2022-02-24 17:49:50 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.2850
2022-02-24 17:50:23 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.1596
2022-02-24 17:51:02 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.2512
2022-02-24 17:51:39 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.1984
2022-02-24 17:52:14 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.2285
2022-02-24 17:52:47 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.1849
2022-02-24 17:53:25 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.5075
2022-02-24 17:54:03 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.4408
2022-02-24 17:54:37 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.2275
2022-02-24 17:55:14 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.2594
2022-02-24 17:55:51 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.2560
2022-02-24 17:56:27 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.3174
2022-02-24 17:56:59 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.2575
2022-02-24 17:57:37 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.2975
2022-02-24 17:58:15 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.4763
2022-02-24 17:58:49 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.4745
2022-02-24 17:59:23 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.0573
2022-02-24 17:59:59 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.2600
2022-02-24 18:00:34 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.2753
2022-02-24 18:01:07 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.3707
2022-02-24 18:01:43 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.2132
2022-02-24 18:02:18 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.1495
2022-02-24 18:02:53 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.2348
2022-02-24 18:03:25 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.4793
2022-02-24 18:04:02 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.4206
2022-02-24 18:04:38 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.3491
2022-02-24 18:05:12 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.1785
2022-02-24 18:05:43 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.4364
2022-02-24 18:05:44 - train: epoch 091, train_loss: 1.2774
2022-02-24 18:07:07 - eval: epoch: 091, acc1: 70.252%, acc5: 89.638%, test_loss: 1.2003, per_image_load_time: 2.900ms, per_image_inference_time: 0.172ms
2022-02-24 18:07:07 - until epoch: 091, best_acc1: 70.252%
2022-02-24 18:07:07 - epoch 092 lr: 0.00010000000000000003
2022-02-24 18:07:45 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.2051
2022-02-24 18:08:22 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.3678
2022-02-24 18:08:58 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.1916
2022-02-24 18:09:33 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.2541
2022-02-24 18:10:06 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.2498
2022-02-24 18:10:43 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.2233
2022-02-24 18:11:18 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.2646
2022-02-24 18:11:53 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.3541
2022-02-24 18:12:26 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.2760
2022-02-24 18:13:02 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.2890
2022-02-24 18:13:40 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.3114
2022-02-24 18:14:12 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.2309
2022-02-24 18:14:46 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.2870
2022-02-24 18:15:23 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.1452
2022-02-24 18:16:00 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.2073
2022-02-24 18:16:32 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.0356
2022-02-24 18:17:07 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.2198
2022-02-24 18:17:43 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.1493
2022-02-24 18:18:19 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.3016
2022-02-24 18:18:52 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.1793
2022-02-24 18:19:28 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.2221
2022-02-24 18:20:04 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.3351
2022-02-24 18:20:38 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.2288
2022-02-24 18:21:11 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.1663
2022-02-24 18:21:47 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.2534
2022-02-24 18:22:24 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.2341
2022-02-24 18:22:59 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.3975
2022-02-24 18:23:32 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.2482
2022-02-24 18:24:09 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.4813
2022-02-24 18:24:45 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.2230
2022-02-24 18:25:20 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.4468
2022-02-24 18:25:54 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.1644
2022-02-24 18:26:29 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.1396
2022-02-24 18:27:06 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.5241
2022-02-24 18:27:40 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.1932
2022-02-24 18:28:13 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.2050
2022-02-24 18:28:50 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.1256
2022-02-24 18:29:27 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.4841
2022-02-24 18:29:59 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.3561
2022-02-24 18:30:32 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.3032
2022-02-24 18:31:10 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.1311
2022-02-24 18:31:45 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.3242
2022-02-24 18:32:19 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.2142
2022-02-24 18:32:53 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.3094
2022-02-24 18:33:29 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.2497
2022-02-24 18:34:05 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.2759
2022-02-24 18:34:37 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 1.1158
2022-02-24 18:35:13 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.1588
2022-02-24 18:35:49 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.0756
2022-02-24 18:36:23 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.3307
2022-02-24 18:36:23 - train: epoch 092, train_loss: 1.2717
2022-02-24 18:37:42 - eval: epoch: 092, acc1: 70.270%, acc5: 89.632%, test_loss: 1.1981, per_image_load_time: 2.671ms, per_image_inference_time: 0.194ms
2022-02-24 18:37:43 - until epoch: 092, best_acc1: 70.270%
2022-02-24 18:37:43 - epoch 093 lr: 0.00010000000000000003
2022-02-24 18:38:25 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.1030
2022-02-24 18:38:58 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.1459
2022-02-24 18:39:31 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.2279
2022-02-24 18:40:09 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.4528
2022-02-24 18:40:45 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.3630
2022-02-24 18:41:18 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.1690
2022-02-24 18:41:53 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.3536
2022-02-24 18:42:31 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 1.2042
2022-02-24 18:43:05 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.3295
2022-02-24 18:43:39 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.2044
2022-02-24 18:44:15 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.2670
2022-02-24 18:44:53 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.3403
2022-02-24 18:45:27 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.2914
2022-02-24 18:46:02 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.4246
2022-02-24 18:46:39 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.4187
2022-02-24 18:47:16 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.2001
2022-02-24 18:47:49 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.1803
2022-02-24 18:48:23 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.4975
2022-02-24 18:49:01 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.2721
2022-02-24 18:49:38 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.1883
2022-02-24 18:50:12 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.1157
2022-02-24 18:50:46 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.3963
2022-02-24 18:51:24 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.1623
2022-02-24 18:52:00 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.0449
2022-02-24 18:52:35 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.3027
2022-02-24 18:53:09 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.4918
2022-02-24 18:53:47 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.3259
2022-02-24 18:54:24 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.2264
2022-02-24 18:54:57 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.1581
2022-02-24 18:55:32 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.1380
2022-02-24 18:56:09 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.1686
2022-02-24 18:56:44 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.2318
2022-02-24 18:57:17 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.2124
2022-02-24 18:57:54 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.3198
2022-02-24 18:58:30 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.2635
2022-02-24 18:59:04 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.2421
2022-02-24 18:59:38 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.1692
2022-02-24 19:00:14 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.2109
2022-02-24 19:00:49 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.3116
2022-02-24 19:01:23 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.3066
2022-02-24 19:01:56 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.2415
2022-02-24 19:02:34 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.5627
2022-02-24 19:03:09 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.2655
2022-02-24 19:03:43 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.2976
2022-02-24 19:04:16 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.2090
2022-02-24 19:04:54 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.0770
2022-02-24 19:05:29 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.4124
2022-02-24 19:06:04 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.1920
2022-02-24 19:06:37 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.3113
2022-02-24 19:07:12 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.2762
2022-02-24 19:07:13 - train: epoch 093, train_loss: 1.2676
2022-02-24 19:08:30 - eval: epoch: 093, acc1: 70.470%, acc5: 89.728%, test_loss: 1.1960, per_image_load_time: 2.869ms, per_image_inference_time: 0.156ms
2022-02-24 19:08:31 - until epoch: 093, best_acc1: 70.470%
2022-02-24 19:08:31 - epoch 094 lr: 0.00010000000000000003
2022-02-24 19:09:13 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.3499
2022-02-24 19:09:50 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.2176
2022-02-24 19:10:22 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.2400
2022-02-24 19:10:56 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.2723
2022-02-24 19:11:33 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.1273
2022-02-24 19:12:09 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.1307
2022-02-24 19:12:41 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.3606
2022-02-24 19:13:17 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.3632
2022-02-24 19:13:54 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.2244
2022-02-24 19:14:28 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.1406
2022-02-24 19:15:02 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.3493
2022-02-24 19:15:36 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.4123
2022-02-24 19:16:14 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.0808
2022-02-24 19:16:48 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.2059
2022-02-24 19:17:22 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.3970
2022-02-24 19:17:57 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.4812
2022-02-24 19:18:35 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.2462
2022-02-24 19:19:10 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.1679
2022-02-24 19:19:45 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.3266
2022-02-24 19:20:24 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.1010
2022-02-24 19:21:06 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.2875
2022-02-24 19:21:44 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.2559
2022-02-24 19:22:24 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.0921
2022-02-24 19:23:05 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.3379
2022-02-24 19:23:44 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.1348
2022-02-24 19:24:22 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.2671
2022-02-24 19:25:02 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.2466
2022-02-24 19:25:44 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.2879
2022-02-24 19:26:21 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.0939
2022-02-24 19:27:00 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.2712
2022-02-24 19:27:38 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.2310
2022-02-24 19:28:14 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.3013
2022-02-24 19:28:47 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.2783
2022-02-24 19:29:23 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.3076
2022-02-24 19:30:00 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.5808
2022-02-24 19:30:36 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.2216
2022-02-24 19:31:08 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.2771
2022-02-24 19:31:45 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.2608
2022-02-24 19:32:21 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.3665
2022-02-24 19:32:55 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.1932
2022-02-24 19:33:29 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.3236
2022-02-24 19:34:04 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.2618
2022-02-24 19:34:42 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.1754
2022-02-24 19:35:15 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.3993
2022-02-24 19:35:49 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.2647
2022-02-24 19:36:24 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.3443
2022-02-24 19:37:01 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.3705
2022-02-24 19:37:34 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.0734
2022-02-24 19:38:08 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.3107
2022-02-24 19:38:42 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.0489
2022-02-24 19:38:43 - train: epoch 094, train_loss: 1.2678
2022-02-24 19:40:02 - eval: epoch: 094, acc1: 70.444%, acc5: 89.672%, test_loss: 1.1948, per_image_load_time: 2.926ms, per_image_inference_time: 0.163ms
2022-02-24 19:40:02 - until epoch: 094, best_acc1: 70.470%
2022-02-24 19:40:02 - epoch 095 lr: 0.00010000000000000003
2022-02-24 19:40:43 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.5313
2022-02-24 19:41:22 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.3008
2022-02-24 19:41:55 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.1763
2022-02-24 19:42:29 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.4782
2022-02-24 19:43:06 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.2827
2022-02-24 19:43:44 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.1056
2022-02-24 19:44:15 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.5530
2022-02-24 19:44:49 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.3355
2022-02-24 19:45:28 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.4257
2022-02-24 19:46:04 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.2591
2022-02-24 19:46:37 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.1658
2022-02-24 19:47:12 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.2957
2022-02-24 19:47:51 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.0979
2022-02-24 19:48:26 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.3169
2022-02-24 19:48:59 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.2173
2022-02-24 19:49:35 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.9420
2022-02-24 19:50:14 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.2265
2022-02-24 19:50:47 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.3289
2022-02-24 19:51:20 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.3525
2022-02-24 19:51:59 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.3854
2022-02-24 19:52:35 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.2580
2022-02-24 19:53:09 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.0818
2022-02-24 19:53:43 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.1783
2022-02-24 19:54:21 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.2778
2022-02-24 19:54:55 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.2672
2022-02-24 19:55:28 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.2153
2022-02-24 19:56:06 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.2807
2022-02-24 19:56:43 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.3374
2022-02-24 19:57:17 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.3218
2022-02-24 19:57:49 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.3802
2022-02-24 19:58:28 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.2030
2022-02-24 19:59:04 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.2675
2022-02-24 19:59:38 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.2947
2022-02-24 20:00:11 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.1546
2022-02-24 20:00:50 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.3018
2022-02-24 20:01:24 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.0240
2022-02-24 20:01:58 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.2505
2022-02-24 20:02:33 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.4368
2022-02-24 20:03:11 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.1579
2022-02-24 20:03:44 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.2986
2022-02-24 20:04:18 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.3371
2022-02-24 20:04:52 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.3356
2022-02-24 20:05:28 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.2646
2022-02-24 20:06:05 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.6205
2022-02-24 20:06:41 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.0221
2022-02-24 20:07:13 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.4822
2022-02-24 20:07:50 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.3682
2022-02-24 20:08:25 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.2384
2022-02-24 20:08:59 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.1381
2022-02-24 20:09:30 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.2331
2022-02-24 20:09:31 - train: epoch 095, train_loss: 1.2674
2022-02-24 20:10:54 - eval: epoch: 095, acc1: 70.478%, acc5: 89.666%, test_loss: 1.1955, per_image_load_time: 3.093ms, per_image_inference_time: 0.170ms
2022-02-24 20:10:55 - until epoch: 095, best_acc1: 70.478%
2022-02-24 20:10:55 - epoch 096 lr: 0.00010000000000000003
2022-02-24 20:11:34 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.4241
2022-02-24 20:12:09 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.2527
2022-02-24 20:12:45 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.1637
2022-02-24 20:13:20 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 0.9258
2022-02-24 20:13:53 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.2729
2022-02-24 20:14:31 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.2532
2022-02-24 20:15:07 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.2499
2022-02-24 20:15:41 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.1839
2022-02-24 20:16:14 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.1935
2022-02-24 20:16:51 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.2637
2022-02-24 20:17:26 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.2695
2022-02-24 20:18:01 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.2041
2022-02-24 20:18:33 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.4014
2022-02-24 20:19:11 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.2904
2022-02-24 20:19:47 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.2266
2022-02-24 20:20:20 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.0670
2022-02-24 20:20:56 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.1201
2022-02-24 20:21:32 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.3759
2022-02-24 20:22:08 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.4168
2022-02-24 20:22:40 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.1407
2022-02-24 20:23:17 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.1843
2022-02-24 20:23:53 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.0714
2022-02-24 20:24:28 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.2213
2022-02-24 20:25:01 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.2957
2022-02-24 20:25:37 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.2530
2022-02-24 20:26:15 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.2998
2022-02-24 20:26:48 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.3777
2022-02-24 20:27:22 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.2843
2022-02-24 20:27:59 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.4328
2022-02-24 20:28:36 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.3157
2022-02-24 20:29:10 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.0298
2022-02-24 20:29:43 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.2154
2022-02-24 20:30:20 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.5288
2022-02-24 20:30:56 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.2509
2022-02-24 20:31:30 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.0821
2022-02-24 20:32:04 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.1616
2022-02-24 20:32:42 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.3028
2022-02-24 20:33:17 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.1548
2022-02-24 20:33:50 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.1918
2022-02-24 20:34:27 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.2939
2022-02-24 20:35:03 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.2260
2022-02-24 20:35:38 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.1441
2022-02-24 20:36:11 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.0549
2022-02-24 20:36:49 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.0906
2022-02-24 20:37:25 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.3033
2022-02-24 20:37:59 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.1649
2022-02-24 20:38:32 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.1258
2022-02-24 20:39:10 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.1553
2022-02-24 20:39:46 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.4851
2022-02-24 20:40:19 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.2133
2022-02-24 20:40:20 - train: epoch 096, train_loss: 1.2636
2022-02-24 20:41:41 - eval: epoch: 096, acc1: 70.412%, acc5: 89.758%, test_loss: 1.1929, per_image_load_time: 2.539ms, per_image_inference_time: 0.180ms
2022-02-24 20:41:41 - until epoch: 096, best_acc1: 70.478%
2022-02-24 20:41:41 - epoch 097 lr: 0.00010000000000000003
2022-02-24 20:42:22 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.3537
2022-02-24 20:42:55 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.1797
2022-02-24 20:43:33 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.3917
2022-02-24 20:44:09 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.3573
2022-02-24 20:44:44 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.1657
2022-02-24 20:45:16 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.2698
2022-02-24 20:45:55 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.2186
2022-02-24 20:46:31 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.1106
2022-02-24 20:47:05 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.2780
2022-02-24 20:47:39 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.5194
2022-02-24 20:48:17 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.1307
2022-02-24 20:48:52 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.3052
2022-02-24 20:49:26 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.0822
2022-02-24 20:50:01 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.1587
2022-02-24 20:50:37 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.1901
2022-02-24 20:51:13 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.2029
2022-02-24 20:51:46 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 1.2174
2022-02-24 20:52:22 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.3770
2022-02-24 20:52:58 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.1957
2022-02-24 20:53:34 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.2125
2022-02-24 20:54:06 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.1313
2022-02-24 20:54:43 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.1045
2022-02-24 20:55:19 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.2165
2022-02-24 20:55:53 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.1845
2022-02-24 20:56:27 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.2903
2022-02-24 20:57:03 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.3564
2022-02-24 20:57:41 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.2670
2022-02-24 20:58:15 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.1738
2022-02-24 20:58:48 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.3108
2022-02-24 20:59:25 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.4184
2022-02-24 21:00:02 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.2967
2022-02-24 21:00:36 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.3667
2022-02-24 21:01:09 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.5170
2022-02-24 21:01:47 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.3884
2022-02-24 21:02:22 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.1985
2022-02-24 21:02:57 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.2141
2022-02-24 21:03:31 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.0051
2022-02-24 21:04:08 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.2137
2022-02-24 21:04:41 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.2318
2022-02-24 21:05:15 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.3267
2022-02-24 21:05:51 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.3303
2022-02-24 21:06:28 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.2529
2022-02-24 21:07:01 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.1952
2022-02-24 21:07:35 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.2292
2022-02-24 21:08:10 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.2814
2022-02-24 21:08:49 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.1748
2022-02-24 21:09:21 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.2819
2022-02-24 21:09:56 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.1919
2022-02-24 21:10:33 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.3700
2022-02-24 21:11:08 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.1306
2022-02-24 21:11:09 - train: epoch 097, train_loss: 1.2644
2022-02-24 21:12:26 - eval: epoch: 097, acc1: 70.468%, acc5: 89.736%, test_loss: 1.1929, per_image_load_time: 2.131ms, per_image_inference_time: 0.194ms
2022-02-24 21:12:26 - until epoch: 097, best_acc1: 70.478%
2022-02-24 21:12:26 - epoch 098 lr: 0.00010000000000000003
2022-02-24 21:13:10 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.2201
2022-02-24 21:13:44 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.3151
2022-02-24 21:14:17 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.3508
2022-02-24 21:14:54 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.2031
2022-02-24 21:15:33 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.5447
2022-02-24 21:16:05 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.3031
2022-02-24 21:16:39 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.2398
2022-02-24 21:17:16 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.1927
2022-02-24 21:17:53 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.3609
2022-02-24 21:18:27 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.2541
2022-02-24 21:19:01 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.1150
2022-02-24 21:19:39 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.3612
2022-02-24 21:20:15 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.2941
2022-02-24 21:20:49 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.1935
2022-02-24 21:21:25 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.2700
2022-02-24 21:22:02 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.2409
2022-02-24 21:22:38 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.2778
2022-02-24 21:23:15 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.5591
2022-02-24 21:23:55 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.0834
2022-02-24 21:24:30 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.6244
2022-02-24 21:25:05 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.4054
2022-02-24 21:25:41 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.1110
2022-02-24 21:26:19 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.2369
2022-02-24 21:26:53 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.3452
2022-02-24 21:27:28 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.3693
2022-02-24 21:28:05 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 1.2785
2022-02-24 21:28:44 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.2324
2022-02-24 21:29:16 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.2851
2022-02-24 21:29:52 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.3558
2022-02-24 21:30:30 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.3018
2022-02-24 21:31:08 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.0927
2022-02-24 21:31:41 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.1728
2022-02-24 21:32:16 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.1707
2022-02-24 21:32:53 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 1.2682
2022-02-24 21:33:29 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 1.2373
2022-02-24 21:34:02 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.4707
2022-02-24 21:34:40 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.2161
2022-02-24 21:35:15 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 1.2452
2022-02-24 21:35:49 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 1.1414
2022-02-24 21:36:25 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.1769
2022-02-24 21:37:02 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.2770
2022-02-24 21:37:37 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.3795
2022-02-24 21:38:13 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 1.0814
2022-02-24 21:38:48 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.4078
2022-02-24 21:39:24 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.3677
2022-02-24 21:40:00 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.3252
2022-02-24 21:40:34 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.3845
2022-02-24 21:41:09 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 1.1341
2022-02-24 21:41:47 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.3079
2022-02-24 21:42:21 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 1.2887
2022-02-24 21:42:22 - train: epoch 098, train_loss: 1.2608
2022-02-24 21:43:41 - eval: epoch: 098, acc1: 70.426%, acc5: 89.750%, test_loss: 1.1930, per_image_load_time: 2.117ms, per_image_inference_time: 0.164ms
2022-02-24 21:43:42 - until epoch: 098, best_acc1: 70.478%
2022-02-24 21:43:42 - epoch 099 lr: 0.00010000000000000003
2022-02-24 21:44:24 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.3058
2022-02-24 21:44:58 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 1.0749
2022-02-24 21:45:33 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.0951
2022-02-24 21:46:11 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.3792
2022-02-24 21:46:45 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 1.4118
2022-02-24 21:47:19 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.2749
2022-02-24 21:47:55 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.4636
2022-02-24 21:48:31 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.1930
2022-02-24 21:49:06 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.0480
2022-02-24 21:49:40 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 1.1772
2022-02-24 21:50:16 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.2842
2022-02-24 21:50:54 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 1.1318
2022-02-24 21:51:28 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 1.2504
2022-02-24 21:52:03 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.1373
2022-02-24 21:52:40 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.1788
2022-02-24 21:53:17 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.2075
2022-02-24 21:53:50 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 1.2548
2022-02-24 21:54:24 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.2855
2022-02-24 21:55:01 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.1153
2022-02-24 21:55:38 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.3665
2022-02-24 21:56:11 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.2978
2022-02-24 21:56:46 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 1.3061
2022-02-24 21:57:23 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.2011
2022-02-24 21:57:58 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.6249
2022-02-24 21:58:33 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 1.2290
2022-02-24 21:59:09 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.3057
2022-02-24 21:59:44 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.2205
2022-02-24 22:00:20 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.4448
2022-02-24 22:00:54 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.1328
2022-02-24 22:01:30 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.3208
2022-02-24 22:02:06 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.0912
2022-02-24 22:02:41 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.2669
2022-02-24 22:03:15 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.1196
2022-02-24 22:03:51 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.4830
2022-02-24 22:04:27 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.2940
2022-02-24 22:05:03 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.1203
2022-02-24 22:05:37 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 1.1763
2022-02-24 22:06:13 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.2785
2022-02-24 22:06:51 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 1.3064
2022-02-24 22:07:24 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.2774
2022-02-24 22:08:00 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 1.1388
2022-02-24 22:08:36 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.3619
2022-02-24 22:09:12 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.3333
2022-02-24 22:09:45 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.2213
2022-02-24 22:10:22 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.3255
2022-02-24 22:10:58 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.2116
2022-02-24 22:11:33 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.3106
2022-02-24 22:12:07 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.2699
2022-02-24 22:12:42 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.3590
2022-02-24 22:13:18 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.4622
2022-02-24 22:13:19 - train: epoch 099, train_loss: 1.2607
2022-02-24 22:14:39 - eval: epoch: 099, acc1: 70.478%, acc5: 89.776%, test_loss: 1.1926, per_image_load_time: 2.962ms, per_image_inference_time: 0.178ms
2022-02-24 22:14:39 - until epoch: 099, best_acc1: 70.478%
2022-02-24 22:14:39 - epoch 100 lr: 0.00010000000000000003
2022-02-24 22:15:22 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.5049
2022-02-24 22:15:58 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.1887
2022-02-24 22:16:31 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.2805
2022-02-24 22:17:08 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.3079
2022-02-24 22:17:44 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.4492
2022-02-24 22:18:20 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.2863
2022-02-24 22:18:54 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 1.0914
2022-02-24 22:19:31 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 1.2711
2022-02-24 22:20:07 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 1.0940
2022-02-24 22:20:41 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 1.1787
2022-02-24 22:21:14 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 1.1833
2022-02-24 22:21:53 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.4651
2022-02-24 22:22:28 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 1.2834
2022-02-24 22:23:02 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.2366
2022-02-24 22:23:36 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.3798
2022-02-24 22:24:15 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.1499
2022-02-24 22:24:50 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.2669
2022-02-24 22:25:24 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 1.2963
2022-02-24 22:25:58 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 1.1590
2022-02-24 22:26:36 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.3304
2022-02-24 22:27:12 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 1.2141
2022-02-24 22:27:46 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.3223
2022-02-24 22:28:22 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.3355
2022-02-24 22:29:00 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.4125
2022-02-24 22:29:35 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.3857
2022-02-24 22:30:09 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.0945
2022-02-24 22:30:44 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 1.2817
2022-02-24 22:31:22 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 1.1711
2022-02-24 22:31:55 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.2345
2022-02-24 22:32:30 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 1.1225
2022-02-24 22:33:06 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 1.0410
2022-02-24 22:33:43 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.3058
2022-02-24 22:34:16 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.1980
2022-02-24 22:34:52 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.2544
2022-02-24 22:35:28 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 1.1206
2022-02-24 22:36:06 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 1.2475
2022-02-24 22:36:38 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 1.1363
2022-02-24 22:37:12 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 1.2092
2022-02-24 22:37:50 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.5188
2022-02-24 22:38:26 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 1.0318
2022-02-24 22:39:00 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.2929
2022-02-24 22:39:36 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.4567
2022-02-24 22:40:12 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 1.2325
