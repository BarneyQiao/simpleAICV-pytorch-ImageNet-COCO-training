2022-02-22 08:09:50 - network: resnet18
2022-02-22 08:09:50 - num_classes: 1000
2022-02-22 08:09:50 - input_image_size: 224
2022-02-22 08:09:50 - scale: 1.1428571428571428
2022-02-22 08:09:50 - trained_model_path: 
2022-02-22 08:09:50 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-22 08:09:50 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f382626afa0>
2022-02-22 08:09:50 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f380c0fe2b0>
2022-02-22 08:09:50 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f380c0fe2e0>
2022-02-22 08:09:50 - seed: 0
2022-02-22 08:09:50 - batch_size: 256
2022-02-22 08:09:50 - num_workers: 16
2022-02-22 08:09:50 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-22 08:09:50 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-22 08:09:50 - epochs: 100
2022-02-22 08:09:50 - print_interval: 100
2022-02-22 08:09:50 - distributed: True
2022-02-22 08:09:50 - sync_bn: False
2022-02-22 08:09:50 - apex: True
2022-02-22 08:09:50 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-22 08:09:50 - gpus_num: 2
2022-02-22 08:09:50 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f38051842b0>
2022-02-22 08:09:56 - --------------------parameters--------------------
2022-02-22 08:09:56 - name: conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-22 08:09:56 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-22 08:09:56 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-22 08:09:56 - name: fc.weight, grad: True
2022-02-22 08:09:56 - name: fc.bias, grad: True
2022-02-22 08:09:56 - --------------------buffers--------------------
2022-02-22 08:09:56 - name: conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:09:56 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:09:56 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:09:56 - epoch 001 lr: 0.1
2022-02-22 08:10:38 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8503
2022-02-22 08:11:15 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.6108
2022-02-22 08:11:51 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.4902
2022-02-22 08:12:28 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.3076
2022-02-22 08:13:04 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.2058
2022-02-22 08:13:39 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 5.8799
2022-02-22 08:14:17 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 5.8634
2022-02-22 08:14:52 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.8543
2022-02-22 08:15:29 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.7268
2022-02-22 08:16:04 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.6002
2022-02-22 08:16:41 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.5895
2022-02-22 08:17:17 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.3942
2022-02-22 08:17:53 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.3577
2022-02-22 08:18:29 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.2799
2022-02-22 08:19:07 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.2244
2022-02-22 08:19:42 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.3154
2022-02-22 08:20:19 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.0675
2022-02-22 08:20:54 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.1115
2022-02-22 08:21:31 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.0291
2022-02-22 08:22:07 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.7883
2022-02-22 08:22:43 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.8698
2022-02-22 08:23:20 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.8145
2022-02-22 08:23:55 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.6766
2022-02-22 08:24:31 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.7019
2022-02-22 08:25:08 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.7193
2022-02-22 08:25:44 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.7905
2022-02-22 08:26:22 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.7799
2022-02-22 08:26:57 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.5269
2022-02-22 08:27:34 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.3787
2022-02-22 08:28:10 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.5894
2022-02-22 08:28:48 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.5806
2022-02-22 08:29:23 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.4401
2022-02-22 08:30:00 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.1970
2022-02-22 08:30:34 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.2685
2022-02-22 08:31:07 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.2398
2022-02-22 08:31:44 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.2934
2022-02-22 08:32:22 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.4143
2022-02-22 08:32:58 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.0968
2022-02-22 08:33:35 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.2919
2022-02-22 08:34:11 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.2180
2022-02-22 08:34:48 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.2319
2022-02-22 08:35:23 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.0611
2022-02-22 08:36:00 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.0328
2022-02-22 08:36:36 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 3.8075
2022-02-22 08:37:13 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.0337
2022-02-22 08:37:49 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.2014
2022-02-22 08:38:26 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 3.9035
2022-02-22 08:39:02 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.2064
2022-02-22 08:39:39 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 3.8046
2022-02-22 08:40:13 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.7907
2022-02-22 08:40:15 - train: epoch 001, train_loss: 4.8710
2022-02-22 08:41:36 - eval: epoch: 001, acc1: 23.558%, acc5: 48.160%, test_loss: 3.7037, per_image_load_time: 0.961ms, per_image_inference_time: 0.161ms
2022-02-22 08:41:36 - until epoch: 001, best_acc1: 23.558%
2022-02-22 08:41:36 - epoch 002 lr: 0.1
2022-02-22 08:42:19 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 3.9690
2022-02-22 08:42:55 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.6786
2022-02-22 08:43:31 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 3.9729
2022-02-22 08:44:07 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.8690
2022-02-22 08:44:44 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.6847
2022-02-22 08:45:19 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.6448
2022-02-22 08:45:55 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.9234
2022-02-22 08:46:32 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.4820
2022-02-22 08:47:08 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.3556
2022-02-22 08:47:45 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.8518
2022-02-22 08:48:21 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.8847
2022-02-22 08:48:58 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.6526
2022-02-22 08:49:33 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.6196
2022-02-22 08:50:11 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.6723
2022-02-22 08:50:47 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.6611
2022-02-22 08:51:24 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.6489
2022-02-22 08:51:59 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.6330
2022-02-22 08:52:36 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.6159
2022-02-22 08:53:12 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.4699
2022-02-22 08:53:49 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.2518
2022-02-22 08:54:25 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.5934
2022-02-22 08:55:02 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.3090
2022-02-22 08:55:38 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.5221
2022-02-22 08:56:15 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.3402
2022-02-22 08:56:50 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.3695
2022-02-22 08:57:27 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.2459
2022-02-22 08:58:02 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.5440
2022-02-22 08:58:40 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.4884
2022-02-22 08:59:15 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.3502
2022-02-22 08:59:52 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.2287
2022-02-22 09:00:28 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.2809
2022-02-22 09:01:05 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.3806
2022-02-22 09:01:41 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.3888
2022-02-22 09:02:16 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.5011
2022-02-22 09:02:53 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.2917
2022-02-22 09:03:28 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.4059
2022-02-22 09:04:06 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.4315
2022-02-22 09:04:41 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.1795
2022-02-22 09:05:18 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.3384
2022-02-22 09:05:54 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.2194
2022-02-22 09:06:31 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.4450
2022-02-22 09:07:07 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.1957
2022-02-22 09:07:44 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.2363
2022-02-22 09:08:21 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.0720
2022-02-22 09:08:57 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.0616
2022-02-22 09:09:33 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.2462
2022-02-22 09:10:10 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.2778
2022-02-22 09:10:46 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.2257
2022-02-22 09:11:23 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.1847
2022-02-22 09:11:58 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.1241
2022-02-22 09:11:59 - train: epoch 002, train_loss: 3.4888
2022-02-22 09:13:21 - eval: epoch: 002, acc1: 32.242%, acc5: 58.468%, test_loss: 3.1689, per_image_load_time: 1.073ms, per_image_inference_time: 0.172ms
2022-02-22 09:13:22 - until epoch: 002, best_acc1: 32.242%
2022-02-22 09:13:22 - epoch 003 lr: 0.1
2022-02-22 09:14:03 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.2795
2022-02-22 09:14:40 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.2487
2022-02-22 09:15:17 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.1059
2022-02-22 09:15:53 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.1982
2022-02-22 09:16:29 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.4099
2022-02-22 09:17:05 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.0787
2022-02-22 09:17:42 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.3739
2022-02-22 09:18:18 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.1204
2022-02-22 09:18:55 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.0426
2022-02-22 09:19:31 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.1592
2022-02-22 09:20:07 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 2.9838
2022-02-22 09:20:42 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.1678
2022-02-22 09:21:19 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.1039
2022-02-22 09:21:54 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.0998
2022-02-22 09:22:32 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.3835
2022-02-22 09:23:07 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.1231
2022-02-22 09:23:44 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0516
2022-02-22 09:24:20 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.9385
2022-02-22 09:24:58 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.1500
2022-02-22 09:25:33 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.4691
2022-02-22 09:26:10 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.2924
2022-02-22 09:26:45 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5414
2022-02-22 09:27:22 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.0184
2022-02-22 09:27:57 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0947
2022-02-22 09:28:35 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1397
2022-02-22 09:29:10 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1423
2022-02-22 09:29:48 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4159
2022-02-22 09:30:24 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0245
2022-02-22 09:31:00 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.1010
2022-02-22 09:31:35 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.2638
2022-02-22 09:32:13 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.1951
2022-02-22 09:32:48 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.1197
2022-02-22 09:33:23 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1527
2022-02-22 09:33:58 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.2223
2022-02-22 09:34:34 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.9421
2022-02-22 09:35:10 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.9061
2022-02-22 09:35:47 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.0468
2022-02-22 09:36:23 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.1690
2022-02-22 09:37:00 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.2915
2022-02-22 09:37:35 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 2.9791
2022-02-22 09:38:12 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.0420
2022-02-22 09:38:47 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.9330
2022-02-22 09:39:24 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.6575
2022-02-22 09:39:59 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9308
2022-02-22 09:40:37 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.0057
2022-02-22 09:41:11 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0198
2022-02-22 09:41:48 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.9518
2022-02-22 09:42:24 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.1811
2022-02-22 09:43:02 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.1283
2022-02-22 09:43:35 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.0080
2022-02-22 09:43:37 - train: epoch 003, train_loss: 3.0843
2022-02-22 09:44:59 - eval: epoch: 003, acc1: 37.698%, acc5: 64.210%, test_loss: 2.8514, per_image_load_time: 0.883ms, per_image_inference_time: 0.166ms
2022-02-22 09:44:59 - until epoch: 003, best_acc1: 37.698%
2022-02-22 09:44:59 - epoch 004 lr: 0.1
2022-02-22 09:45:41 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.0141
2022-02-22 09:46:17 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.8242
2022-02-22 09:46:55 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9265
2022-02-22 09:47:30 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.9084
2022-02-22 09:48:07 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.6987
2022-02-22 09:48:43 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.1992
2022-02-22 09:49:20 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.0173
2022-02-22 09:49:55 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.7762
2022-02-22 09:50:33 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.6774
2022-02-22 09:51:08 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.9792
2022-02-22 09:51:46 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0458
2022-02-22 09:52:21 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.6533
2022-02-22 09:52:58 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.6903
2022-02-22 09:53:33 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0103
2022-02-22 09:54:11 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.0838
2022-02-22 09:54:46 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.8305
2022-02-22 09:55:23 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.0264
2022-02-22 09:55:58 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.1564
2022-02-22 09:56:36 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.1146
2022-02-22 09:57:12 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.7497
2022-02-22 09:57:49 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9929
2022-02-22 09:58:24 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.8525
2022-02-22 09:59:01 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.7340
2022-02-22 09:59:37 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8095
2022-02-22 10:00:13 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.0562
2022-02-22 10:00:49 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.9848
2022-02-22 10:01:26 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.7209
2022-02-22 10:02:02 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.8538
2022-02-22 10:02:39 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.7629
2022-02-22 10:03:14 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.7585
2022-02-22 10:03:51 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.9386
2022-02-22 10:04:27 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.8052
2022-02-22 10:05:04 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9444
2022-02-22 10:05:39 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.9197
2022-02-22 10:06:16 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.8077
2022-02-22 10:06:51 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.7346
2022-02-22 10:07:28 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.8346
2022-02-22 10:08:05 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.8005
2022-02-22 10:08:42 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.9079
2022-02-22 10:09:18 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.5789
2022-02-22 10:09:54 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.7247
2022-02-22 10:10:31 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.7771
2022-02-22 10:11:07 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.7084
2022-02-22 10:11:44 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.6825
2022-02-22 10:12:20 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.3444
2022-02-22 10:12:56 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.8244
2022-02-22 10:13:32 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.7412
2022-02-22 10:14:09 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.7151
2022-02-22 10:14:44 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.8900
2022-02-22 10:15:20 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.9015
2022-02-22 10:15:20 - train: epoch 004, train_loss: 2.8925
2022-02-22 10:16:41 - eval: epoch: 004, acc1: 40.840%, acc5: 67.640%, test_loss: 2.6573, per_image_load_time: 0.841ms, per_image_inference_time: 0.162ms
2022-02-22 10:16:42 - until epoch: 004, best_acc1: 40.840%
2022-02-22 10:16:42 - epoch 005 lr: 0.1
2022-02-22 10:17:24 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.9382
2022-02-22 10:18:01 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.9258
2022-02-22 10:18:37 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0072
2022-02-22 10:19:14 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.7937
2022-02-22 10:19:49 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.5830
2022-02-22 10:20:25 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.7114
2022-02-22 10:21:01 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.8504
2022-02-22 10:21:39 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.0911
2022-02-22 10:22:14 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.6911
2022-02-22 10:22:50 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.8334
2022-02-22 10:23:26 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.8920
2022-02-22 10:24:03 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.9518
2022-02-22 10:24:39 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.7332
2022-02-22 10:25:16 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.9703
2022-02-22 10:25:52 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.6420
2022-02-22 10:26:30 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.7011
2022-02-22 10:27:06 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.6696
2022-02-22 10:27:43 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.7576
2022-02-22 10:28:20 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6335
2022-02-22 10:28:58 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.8941
2022-02-22 10:29:34 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.5908
2022-02-22 10:30:11 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.6368
2022-02-22 10:30:48 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.6395
2022-02-22 10:31:26 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.7432
2022-02-22 10:32:03 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.8635
2022-02-22 10:32:40 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.8857
2022-02-22 10:33:17 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.8645
2022-02-22 10:33:54 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.8860
2022-02-22 10:34:31 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.6768
2022-02-22 10:35:08 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6948
2022-02-22 10:35:44 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.7691
2022-02-22 10:36:22 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.7802
2022-02-22 10:36:58 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.5149
2022-02-22 10:37:35 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.7207
2022-02-22 10:38:12 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.8505
2022-02-22 10:38:48 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8156
2022-02-22 10:39:21 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.7180
2022-02-22 10:39:58 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.4814
2022-02-22 10:40:36 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9819
2022-02-22 10:41:15 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.7878
2022-02-22 10:41:52 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.7865
2022-02-22 10:42:30 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.7552
2022-02-22 10:43:06 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.6255
2022-02-22 10:43:44 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.7843
2022-02-22 10:44:22 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.8150
2022-02-22 10:44:59 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.6761
2022-02-22 10:45:36 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.5796
2022-02-22 10:46:14 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.5223
2022-02-22 10:46:50 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.7705
2022-02-22 10:47:25 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.6739
2022-02-22 10:47:26 - train: epoch 005, train_loss: 2.7832
2022-02-22 10:48:50 - eval: epoch: 005, acc1: 42.390%, acc5: 69.390%, test_loss: 2.5576, per_image_load_time: 0.986ms, per_image_inference_time: 0.165ms
2022-02-22 10:48:51 - until epoch: 005, best_acc1: 42.390%
2022-02-22 10:48:51 - epoch 006 lr: 0.1
2022-02-22 10:49:33 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.6986
2022-02-22 10:50:10 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.7060
2022-02-22 10:50:48 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.6063
2022-02-22 10:51:26 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.8426
2022-02-22 10:52:03 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.7104
2022-02-22 10:52:41 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.7825
2022-02-22 10:53:18 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.8171
2022-02-22 10:53:56 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.7296
2022-02-22 10:54:32 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.5268
2022-02-22 10:55:09 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.7351
2022-02-22 10:55:47 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.6634
2022-02-22 10:56:24 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.8525
2022-02-22 10:57:01 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.8248
2022-02-22 10:57:38 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.7780
2022-02-22 10:58:16 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.8582
2022-02-22 10:58:53 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.5453
2022-02-22 10:59:31 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8438
2022-02-22 11:00:08 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8203
2022-02-22 11:00:45 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.6574
2022-02-22 11:01:22 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.9356
2022-02-22 11:02:01 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.7978
2022-02-22 11:02:38 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.7225
2022-02-22 11:03:16 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.5809
2022-02-22 11:03:54 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7551
2022-02-22 11:04:30 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.0088
2022-02-22 11:05:08 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.5993
2022-02-22 11:05:45 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.8139
2022-02-22 11:06:23 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.5726
2022-02-22 11:07:00 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.9741
2022-02-22 11:07:37 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6739
2022-02-22 11:08:14 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.4033
2022-02-22 11:08:51 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.6982
2022-02-22 11:09:28 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5165
2022-02-22 11:10:07 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.8632
2022-02-22 11:10:44 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.6918
2022-02-22 11:11:21 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.8012
2022-02-22 11:11:57 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.7292
2022-02-22 11:12:35 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.6342
2022-02-22 11:13:11 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.6291
2022-02-22 11:13:50 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.9723
2022-02-22 11:14:25 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5735
2022-02-22 11:15:01 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.5643
2022-02-22 11:15:39 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6597
2022-02-22 11:16:17 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.6553
2022-02-22 11:16:54 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.6900
2022-02-22 11:17:30 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.7427
2022-02-22 11:18:08 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.7735
2022-02-22 11:18:45 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.7274
2022-02-22 11:19:21 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.7531
2022-02-22 11:19:56 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.5118
2022-02-22 11:19:57 - train: epoch 006, train_loss: 2.7164
2022-02-22 11:21:21 - eval: epoch: 006, acc1: 43.200%, acc5: 69.880%, test_loss: 2.5275, per_image_load_time: 0.913ms, per_image_inference_time: 0.154ms
2022-02-22 11:21:21 - until epoch: 006, best_acc1: 43.200%
2022-02-22 11:21:21 - epoch 007 lr: 0.1
2022-02-22 11:22:04 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.5993
2022-02-22 11:22:40 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.8758
2022-02-22 11:23:18 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9467
2022-02-22 11:23:55 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.8322
2022-02-22 11:24:33 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.6411
2022-02-22 11:25:09 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.7847
2022-02-22 11:25:46 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.6996
2022-02-22 11:26:23 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.6623
2022-02-22 11:27:01 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.6902
2022-02-22 11:27:37 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.7314
2022-02-22 11:28:15 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.4660
2022-02-22 11:28:50 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.5946
2022-02-22 11:29:27 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.5747
2022-02-22 11:30:04 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.7027
2022-02-22 11:30:41 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.7534
2022-02-22 11:31:18 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.6093
2022-02-22 11:31:56 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7417
2022-02-22 11:32:33 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.4801
2022-02-22 11:33:10 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7191
2022-02-22 11:33:47 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.4978
2022-02-22 11:34:24 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.8147
2022-02-22 11:35:01 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.5919
2022-02-22 11:35:38 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.6962
2022-02-22 11:36:15 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.6497
2022-02-22 11:36:52 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.5616
2022-02-22 11:37:30 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.6440
2022-02-22 11:38:07 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.4929
2022-02-22 11:38:46 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6082
2022-02-22 11:39:21 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.5644
2022-02-22 11:39:59 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.7174
2022-02-22 11:40:35 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.6256
2022-02-22 11:41:13 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.4924
2022-02-22 11:41:50 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.8077
2022-02-22 11:42:27 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.5216
2022-02-22 11:43:03 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.7738
2022-02-22 11:43:41 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.5016
2022-02-22 11:44:17 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.6876
2022-02-22 11:44:54 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.9050
2022-02-22 11:45:31 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.6202
2022-02-22 11:46:08 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.8658
2022-02-22 11:46:45 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.6078
2022-02-22 11:47:23 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.6251
2022-02-22 11:48:00 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.7889
2022-02-22 11:48:37 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.5257
2022-02-22 11:49:16 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.9294
2022-02-22 11:49:52 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.6133
2022-02-22 11:50:29 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.7132
2022-02-22 11:51:06 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.8194
2022-02-22 11:51:44 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.7674
2022-02-22 11:52:18 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.6511
2022-02-22 11:52:19 - train: epoch 007, train_loss: 2.6686
2022-02-22 11:53:41 - eval: epoch: 007, acc1: 42.938%, acc5: 69.766%, test_loss: 2.5263, per_image_load_time: 0.947ms, per_image_inference_time: 0.176ms
2022-02-22 11:53:42 - until epoch: 007, best_acc1: 43.200%
2022-02-22 11:53:42 - epoch 008 lr: 0.1
2022-02-22 11:54:24 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.5473
2022-02-22 11:55:01 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.8257
2022-02-22 11:55:38 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.4367
2022-02-22 11:56:16 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.4429
2022-02-22 11:56:52 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.5199
2022-02-22 11:57:30 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.5056
2022-02-22 11:58:06 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.8840
2022-02-22 11:58:44 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.4307
2022-02-22 11:59:20 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.6081
2022-02-22 11:59:58 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.7291
2022-02-22 12:00:34 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.4021
2022-02-22 12:01:12 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.4291
2022-02-22 12:01:49 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.5849
2022-02-22 12:02:27 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.4994
2022-02-22 12:03:03 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.7224
2022-02-22 12:03:44 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.7308
2022-02-22 12:04:22 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.5523
2022-02-22 12:04:59 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7336
2022-02-22 12:05:36 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.4126
2022-02-22 12:06:14 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.6714
2022-02-22 12:06:54 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.5652
2022-02-22 12:07:34 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.6039
2022-02-22 12:08:11 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.6590
2022-02-22 12:08:49 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.5621
2022-02-22 12:09:25 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.5741
2022-02-22 12:10:03 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.7347
2022-02-22 12:10:40 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.6638
2022-02-22 12:11:18 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.6505
2022-02-22 12:11:55 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.6796
2022-02-22 12:12:33 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.7374
2022-02-22 12:13:09 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.6773
2022-02-22 12:13:48 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.8380
2022-02-22 12:14:25 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.8282
2022-02-22 12:15:03 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.9104
2022-02-22 12:15:40 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.6198
2022-02-22 12:16:19 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7886
2022-02-22 12:16:58 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.6173
2022-02-22 12:17:36 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.6068
2022-02-22 12:18:12 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7443
2022-02-22 12:18:51 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.8405
2022-02-22 12:19:29 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5366
2022-02-22 12:20:06 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5579
2022-02-22 12:20:42 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.4226
2022-02-22 12:21:19 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.5265
2022-02-22 12:21:57 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.8645
2022-02-22 12:22:34 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.8335
2022-02-22 12:23:11 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.5962
2022-02-22 12:23:49 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.6554
2022-02-22 12:24:25 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.6873
2022-02-22 12:25:01 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.5673
2022-02-22 12:25:02 - train: epoch 008, train_loss: 2.6308
2022-02-22 12:26:25 - eval: epoch: 008, acc1: 46.596%, acc5: 72.612%, test_loss: 2.3533, per_image_load_time: 1.085ms, per_image_inference_time: 0.178ms
2022-02-22 12:26:26 - until epoch: 008, best_acc1: 46.596%
2022-02-22 12:26:26 - epoch 009 lr: 0.1
2022-02-22 12:27:08 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3404
2022-02-22 12:27:46 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.4234
2022-02-22 12:28:22 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3001
2022-02-22 12:28:58 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7595
2022-02-22 12:29:36 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.5897
2022-02-22 12:30:12 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.6301
2022-02-22 12:30:50 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.5628
2022-02-22 12:31:26 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.4574
2022-02-22 12:32:04 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.4611
2022-02-22 12:32:41 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.4767
2022-02-22 12:33:18 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.9681
2022-02-22 12:33:56 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.6331
2022-02-22 12:34:32 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.7076
2022-02-22 12:35:10 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.3845
2022-02-22 12:35:47 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.5164
2022-02-22 12:36:25 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.6065
2022-02-22 12:37:01 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.8330
2022-02-22 12:37:38 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.6040
2022-02-22 12:38:14 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.3092
2022-02-22 12:38:52 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.3532
2022-02-22 12:39:29 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.7078
2022-02-22 12:40:07 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.7522
2022-02-22 12:40:44 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.4320
2022-02-22 12:41:24 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.5570
2022-02-22 12:42:01 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.5341
2022-02-22 12:42:39 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.6556
2022-02-22 12:43:21 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.6116
2022-02-22 12:44:05 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.6512
2022-02-22 12:44:48 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.3968
2022-02-22 12:45:30 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.5048
2022-02-22 12:46:15 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.7211
2022-02-22 12:46:58 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.6208
2022-02-22 12:47:41 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5443
2022-02-22 12:48:21 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8122
2022-02-22 12:49:03 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6035
2022-02-22 12:49:39 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.5542
2022-02-22 12:50:17 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.8016
2022-02-22 12:50:55 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.8036
2022-02-22 12:51:31 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.3972
2022-02-22 12:52:09 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.7743
2022-02-22 12:52:46 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.5339
2022-02-22 12:53:21 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.5597
2022-02-22 12:54:00 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.6922
2022-02-22 12:54:37 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.5935
2022-02-22 12:55:14 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.4904
2022-02-22 12:55:51 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.6412
2022-02-22 12:56:26 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.7199
2022-02-22 12:57:03 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.8032
2022-02-22 12:57:41 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.6100
2022-02-22 12:58:17 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.5299
2022-02-22 12:58:17 - train: epoch 009, train_loss: 2.6042
2022-02-22 12:59:39 - eval: epoch: 009, acc1: 46.110%, acc5: 72.456%, test_loss: 2.3693, per_image_load_time: 1.895ms, per_image_inference_time: 0.172ms
2022-02-22 12:59:40 - until epoch: 009, best_acc1: 46.596%
2022-02-22 12:59:40 - epoch 010 lr: 0.1
2022-02-22 13:00:23 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.5458
2022-02-22 13:00:59 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.7117
2022-02-22 13:01:37 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.5127
2022-02-22 13:02:14 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.7115
2022-02-22 13:02:51 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.5510
2022-02-22 13:03:28 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.5678
2022-02-22 13:04:05 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.6979
2022-02-22 13:04:42 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.4731
2022-02-22 13:05:19 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.4374
2022-02-22 13:05:57 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.3741
2022-02-22 13:06:33 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5005
2022-02-22 13:07:10 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.4200
2022-02-22 13:07:47 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.4262
2022-02-22 13:08:25 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.6276
2022-02-22 13:09:02 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.3024
2022-02-22 13:09:38 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.7481
2022-02-22 13:10:14 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.6982
2022-02-22 13:10:52 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5796
2022-02-22 13:11:29 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.6299
2022-02-22 13:12:06 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.6416
2022-02-22 13:12:42 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.4923
2022-02-22 13:13:20 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.7772
2022-02-22 13:13:56 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.7793
2022-02-22 13:14:33 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.7170
2022-02-22 13:15:10 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.7162
2022-02-22 13:15:48 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.6974
2022-02-22 13:16:25 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.3669
2022-02-22 13:17:01 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5917
2022-02-22 13:17:39 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.6477
2022-02-22 13:18:15 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.5540
2022-02-22 13:18:53 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.7268
2022-02-22 13:19:29 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.5861
2022-02-22 13:20:07 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.7000
2022-02-22 13:20:43 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.7212
2022-02-22 13:21:19 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.9279
2022-02-22 13:21:56 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.8924
2022-02-22 13:22:32 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.5145
2022-02-22 13:23:08 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.5957
2022-02-22 13:23:44 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.2803
2022-02-22 13:24:20 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.5482
2022-02-22 13:24:56 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.4201
2022-02-22 13:25:32 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.6862
2022-02-22 13:26:09 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.6358
2022-02-22 13:26:44 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.5225
2022-02-22 13:27:22 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.4670
2022-02-22 13:28:00 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6557
2022-02-22 13:28:36 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.5772
2022-02-22 13:29:14 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.5858
2022-02-22 13:29:52 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.4932
2022-02-22 13:30:25 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.3660
2022-02-22 13:30:27 - train: epoch 010, train_loss: 2.5811
2022-02-22 13:31:49 - eval: epoch: 010, acc1: 46.210%, acc5: 72.478%, test_loss: 2.3656, per_image_load_time: 0.835ms, per_image_inference_time: 0.159ms
2022-02-22 13:31:49 - until epoch: 010, best_acc1: 46.596%
2022-02-22 13:31:49 - epoch 011 lr: 0.1
2022-02-22 13:32:31 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.3328
2022-02-22 13:33:07 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.6810
2022-02-22 13:33:44 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.3693
2022-02-22 13:34:22 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.7593
2022-02-22 13:34:58 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.4327
2022-02-22 13:35:35 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.6858
2022-02-22 13:36:12 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4721
2022-02-22 13:36:50 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5110
2022-02-22 13:37:26 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.6153
2022-02-22 13:38:04 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.5242
2022-02-22 13:38:38 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.6639
2022-02-22 13:39:16 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.9237
2022-02-22 13:39:52 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.7722
2022-02-22 13:40:30 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.6425
2022-02-22 13:41:06 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.3677
2022-02-22 13:41:45 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.5355
2022-02-22 13:42:21 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.5998
2022-02-22 13:42:59 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.4514
2022-02-22 13:43:36 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.3740
2022-02-22 13:44:13 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.6332
2022-02-22 13:44:50 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.6311
2022-02-22 13:45:28 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.5630
2022-02-22 13:46:06 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.8696
2022-02-22 13:46:43 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.3796
2022-02-22 13:47:21 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.8186
2022-02-22 13:47:59 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.4692
2022-02-22 13:48:38 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5870
2022-02-22 13:49:15 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.2615
2022-02-22 13:49:54 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.5541
2022-02-22 13:50:30 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.8193
2022-02-22 13:51:08 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.6403
2022-02-22 13:51:47 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.3485
2022-02-22 13:52:24 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.6264
2022-02-22 13:53:03 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.5718
2022-02-22 13:53:41 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.6074
2022-02-22 13:54:20 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.4999
2022-02-22 13:54:57 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.6070
2022-02-22 13:55:35 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.2893
2022-02-22 13:56:13 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.6466
2022-02-22 13:56:52 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.5537
2022-02-22 13:57:30 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.3204
2022-02-22 13:58:08 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.5549
2022-02-22 13:58:47 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.6066
2022-02-22 13:59:26 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.4706
2022-02-22 14:00:04 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.4013
2022-02-22 14:00:44 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.4420
2022-02-22 14:01:21 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.3706
2022-02-22 14:01:59 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.3066
2022-02-22 14:02:37 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.3820
2022-02-22 14:03:14 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.6069
2022-02-22 14:03:15 - train: epoch 011, train_loss: 2.5609
2022-02-22 14:04:41 - eval: epoch: 011, acc1: 43.888%, acc5: 70.250%, test_loss: 2.5055, per_image_load_time: 1.196ms, per_image_inference_time: 0.180ms
2022-02-22 14:04:41 - until epoch: 011, best_acc1: 46.596%
2022-02-22 14:04:41 - epoch 012 lr: 0.1
2022-02-22 14:05:25 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.4152
2022-02-22 14:06:02 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4089
2022-02-22 14:06:41 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.4004
2022-02-22 14:07:19 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.6328
2022-02-22 14:07:57 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.8282
2022-02-22 14:08:34 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.3346
2022-02-22 14:09:14 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.4077
2022-02-22 14:09:51 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.4906
2022-02-22 14:10:32 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.7498
2022-02-22 14:11:12 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.3651
2022-02-22 14:11:50 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.9098
2022-02-22 14:12:30 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.4176
2022-02-22 14:13:08 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.5196
2022-02-22 14:13:47 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.7153
2022-02-22 14:14:25 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.3442
2022-02-22 14:15:01 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.4960
2022-02-22 14:15:40 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.4767
2022-02-22 14:16:17 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.4835
2022-02-22 14:16:56 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.5878
2022-02-22 14:17:35 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6932
2022-02-22 14:18:15 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.5727
2022-02-22 14:18:58 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6147
2022-02-22 14:19:40 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.6537
2022-02-22 14:20:24 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.5819
2022-02-22 14:21:08 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.3165
2022-02-22 14:21:52 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.3762
2022-02-22 14:22:33 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.4485
2022-02-22 14:23:18 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.5838
2022-02-22 14:24:01 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.4263
2022-02-22 14:24:43 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.4802
2022-02-22 14:25:28 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.6834
2022-02-22 14:26:11 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.2625
2022-02-22 14:26:53 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.4265
2022-02-22 14:27:38 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.5499
2022-02-22 14:28:22 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.7415
2022-02-22 14:29:01 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.7098
2022-02-22 14:29:39 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.6491
2022-02-22 14:30:18 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.6719
2022-02-22 14:30:56 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.4599
2022-02-22 14:31:36 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.4737
2022-02-22 14:32:16 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.4039
2022-02-22 14:32:56 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.4629
2022-02-22 14:33:35 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.6753
2022-02-22 14:34:13 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.3607
2022-02-22 14:34:51 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.5517
2022-02-22 14:35:29 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.8476
2022-02-22 14:36:09 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3862
2022-02-22 14:36:48 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.5826
2022-02-22 14:37:25 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5614
2022-02-22 14:38:02 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.1420
2022-02-22 14:38:03 - train: epoch 012, train_loss: 2.5480
2022-02-22 14:39:28 - eval: epoch: 012, acc1: 47.458%, acc5: 73.802%, test_loss: 2.2976, per_image_load_time: 1.097ms, per_image_inference_time: 0.171ms
2022-02-22 14:39:29 - until epoch: 012, best_acc1: 47.458%
2022-02-22 14:39:29 - epoch 013 lr: 0.1
2022-02-22 14:40:11 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.3463
2022-02-22 14:40:51 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.4872
2022-02-22 14:41:30 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.3780
2022-02-22 14:42:09 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.3895
2022-02-22 14:42:47 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.4744
2022-02-22 14:43:26 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.6844
2022-02-22 14:44:04 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.3481
2022-02-22 14:44:43 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.6290
2022-02-22 14:45:22 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.4777
2022-02-22 14:46:03 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.6337
2022-02-22 14:46:41 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.6035
2022-02-22 14:47:19 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.5611
2022-02-22 14:47:58 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.6533
2022-02-22 14:48:37 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.4879
2022-02-22 14:49:15 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.7118
2022-02-22 14:49:54 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1871
2022-02-22 14:50:32 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5026
2022-02-22 14:51:11 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.5108
2022-02-22 14:51:49 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.4839
2022-02-22 14:52:28 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.8411
2022-02-22 14:53:06 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.8260
2022-02-22 14:53:46 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3796
2022-02-22 14:54:24 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.5815
2022-02-22 14:55:02 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.4937
2022-02-22 14:55:42 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.4555
2022-02-22 14:56:20 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.4771
2022-02-22 14:56:56 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.4188
2022-02-22 14:57:34 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.6310
2022-02-22 14:58:13 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.5741
2022-02-22 14:58:53 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.3988
2022-02-22 14:59:30 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.3687
2022-02-22 15:00:11 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.4986
2022-02-22 15:00:49 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.3733
2022-02-22 15:01:28 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.4280
2022-02-22 15:02:06 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.3798
2022-02-22 15:02:45 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.8130
2022-02-22 15:03:24 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.3182
2022-02-22 15:04:03 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.7011
2022-02-22 15:04:40 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.6368
2022-02-22 15:05:20 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.5779
2022-02-22 15:05:58 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.5562
2022-02-22 15:06:37 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.3675
2022-02-22 15:07:14 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.5439
2022-02-22 15:07:51 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.4134
2022-02-22 15:08:30 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.5841
2022-02-22 15:09:07 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.4897
2022-02-22 15:09:46 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5454
2022-02-22 15:10:25 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.7150
2022-02-22 15:11:06 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.6453
2022-02-22 15:11:44 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.7200
2022-02-22 15:11:46 - train: epoch 013, train_loss: 2.5331
2022-02-22 15:13:15 - eval: epoch: 013, acc1: 47.710%, acc5: 73.882%, test_loss: 2.2880, per_image_load_time: 0.935ms, per_image_inference_time: 0.156ms
2022-02-22 15:13:15 - until epoch: 013, best_acc1: 47.710%
2022-02-22 15:13:15 - epoch 014 lr: 0.1
2022-02-22 15:14:01 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.5193
2022-02-22 15:14:42 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5802
2022-02-22 15:15:21 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.2122
2022-02-22 15:16:02 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.4053
2022-02-22 15:16:41 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.4361
2022-02-22 15:17:22 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.5246
2022-02-22 15:17:59 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.4949
2022-02-22 15:18:37 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4928
2022-02-22 15:19:14 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5734
2022-02-22 15:19:53 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.6241
2022-02-22 15:20:34 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.4700
2022-02-22 15:21:11 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.5640
2022-02-22 15:21:50 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4859
2022-02-22 15:22:30 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.5429
2022-02-22 15:23:11 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.6574
2022-02-22 15:23:49 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.5107
2022-02-22 15:24:30 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.8152
2022-02-22 15:25:09 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.6877
2022-02-22 15:25:49 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.3602
2022-02-22 15:26:28 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3892
2022-02-22 15:27:06 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.6659
2022-02-22 15:27:42 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.4786
2022-02-22 15:28:20 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.5132
2022-02-22 15:28:59 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6487
2022-02-22 15:29:36 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.4988
2022-02-22 15:30:16 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.5153
2022-02-22 15:30:54 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.4397
2022-02-22 15:31:33 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.7281
2022-02-22 15:32:11 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.4323
2022-02-22 15:32:50 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.6355
2022-02-22 15:33:30 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.2830
2022-02-22 15:34:11 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.4837
2022-02-22 15:34:53 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.3229
2022-02-22 15:35:32 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.4277
2022-02-22 15:36:16 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.4608
2022-02-22 15:36:59 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.3005
2022-02-22 15:37:40 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.4674
2022-02-22 15:38:24 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.7041
2022-02-22 15:39:05 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.3890
2022-02-22 15:39:48 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.6278
2022-02-22 15:40:31 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.4585
2022-02-22 15:41:15 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.3333
2022-02-22 15:41:58 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.3591
2022-02-22 15:42:42 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.3305
2022-02-22 15:43:24 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.5512
2022-02-22 15:44:07 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.3887
2022-02-22 15:44:48 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.3982
2022-02-22 15:45:27 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.5126
2022-02-22 15:46:05 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.2989
2022-02-22 15:46:41 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.4718
2022-02-22 15:46:42 - train: epoch 014, train_loss: 2.5218
2022-02-22 15:48:05 - eval: epoch: 014, acc1: 45.590%, acc5: 72.346%, test_loss: 2.3949, per_image_load_time: 1.476ms, per_image_inference_time: 0.190ms
2022-02-22 15:48:05 - until epoch: 014, best_acc1: 47.710%
2022-02-22 15:48:05 - epoch 015 lr: 0.1
2022-02-22 15:48:49 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.3391
2022-02-22 15:49:27 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.7348
2022-02-22 15:50:04 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6609
2022-02-22 15:50:41 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.4792
2022-02-22 15:51:19 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.5117
2022-02-22 15:51:56 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.6393
2022-02-22 15:52:35 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.5400
2022-02-22 15:53:12 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.3133
2022-02-22 15:53:51 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.3968
2022-02-22 15:54:28 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5584
2022-02-22 15:55:06 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.3129
2022-02-22 15:55:45 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.5536
2022-02-22 15:56:22 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.7557
2022-02-22 15:57:00 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.3839
2022-02-22 15:57:35 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.2212
2022-02-22 15:58:13 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.5276
2022-02-22 15:58:48 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.6118
2022-02-22 15:59:26 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.4061
2022-02-22 16:00:03 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.4466
2022-02-22 16:00:40 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.4585
2022-02-22 16:01:16 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.3912
2022-02-22 16:01:53 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.6435
2022-02-22 16:02:29 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3621
2022-02-22 16:03:05 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.6340
2022-02-22 16:03:42 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.5952
2022-02-22 16:04:18 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.3851
2022-02-22 16:04:54 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.5398
2022-02-22 16:05:31 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5942
2022-02-22 16:06:07 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.3964
2022-02-22 16:06:44 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.3742
2022-02-22 16:07:19 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.4537
2022-02-22 16:07:56 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.5197
2022-02-22 16:08:32 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.3373
2022-02-22 16:09:08 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.4241
2022-02-22 16:09:45 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.6294
2022-02-22 16:10:21 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.6125
2022-02-22 16:10:58 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.5296
2022-02-22 16:11:33 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.4232
2022-02-22 16:12:11 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.7612
2022-02-22 16:12:46 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.5549
2022-02-22 16:13:23 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.7369
2022-02-22 16:13:59 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.2619
2022-02-22 16:14:36 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.6517
2022-02-22 16:15:11 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.4642
2022-02-22 16:15:47 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.6157
2022-02-22 16:16:24 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.4005
2022-02-22 16:17:01 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.7605
2022-02-22 16:17:37 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.5016
2022-02-22 16:18:13 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.5265
2022-02-22 16:18:46 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.6122
2022-02-22 16:18:47 - train: epoch 015, train_loss: 2.5128
2022-02-22 16:20:09 - eval: epoch: 015, acc1: 48.236%, acc5: 74.234%, test_loss: 2.2640, per_image_load_time: 0.943ms, per_image_inference_time: 0.163ms
2022-02-22 16:20:10 - until epoch: 015, best_acc1: 48.236%
2022-02-22 16:20:10 - epoch 016 lr: 0.1
2022-02-22 16:20:51 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.5187
2022-02-22 16:21:29 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.2891
2022-02-22 16:22:05 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.5276
2022-02-22 16:22:42 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.7717
2022-02-22 16:23:17 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.2419
2022-02-22 16:23:55 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.6218
2022-02-22 16:24:31 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.2641
2022-02-22 16:25:09 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.5565
2022-02-22 16:25:44 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.5111
2022-02-22 16:26:22 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.4239
2022-02-22 16:26:58 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.3671
2022-02-22 16:27:35 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.3280
2022-02-22 16:28:11 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.6352
2022-02-22 16:28:48 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.3118
2022-02-22 16:29:26 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.5458
2022-02-22 16:30:01 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.5442
2022-02-22 16:30:38 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.4411
2022-02-22 16:31:15 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.4303
2022-02-22 16:31:52 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.4843
2022-02-22 16:32:29 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.2011
2022-02-22 16:33:05 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.6445
2022-02-22 16:33:42 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.4097
2022-02-22 16:34:18 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6615
2022-02-22 16:34:55 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.6587
2022-02-22 16:35:31 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.3250
2022-02-22 16:36:08 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.6917
2022-02-22 16:36:44 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.4761
2022-02-22 16:37:21 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.3697
2022-02-22 16:37:57 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.5598
2022-02-22 16:38:34 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.7452
2022-02-22 16:39:09 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.6357
2022-02-22 16:39:47 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.6232
2022-02-22 16:40:23 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.5679
2022-02-22 16:41:00 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.5378
2022-02-22 16:41:36 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3959
2022-02-22 16:42:14 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.3233
2022-02-22 16:42:49 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.6704
2022-02-22 16:43:27 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.8536
2022-02-22 16:44:02 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.5138
2022-02-22 16:44:39 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.5685
2022-02-22 16:45:14 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.3962
2022-02-22 16:45:52 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4474
2022-02-22 16:46:27 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.3902
2022-02-22 16:47:05 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.3997
2022-02-22 16:47:41 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.5282
2022-02-22 16:48:17 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.4279
2022-02-22 16:48:53 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.7778
2022-02-22 16:49:30 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.4574
2022-02-22 16:50:06 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5216
2022-02-22 16:50:41 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5430
2022-02-22 16:50:42 - train: epoch 016, train_loss: 2.5032
2022-02-22 16:52:04 - eval: epoch: 016, acc1: 47.600%, acc5: 74.036%, test_loss: 2.2948, per_image_load_time: 1.404ms, per_image_inference_time: 0.194ms
2022-02-22 16:52:04 - until epoch: 016, best_acc1: 48.236%
2022-02-22 16:52:04 - epoch 017 lr: 0.1
2022-02-22 16:52:46 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.4122
2022-02-22 16:53:24 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.6419
2022-02-22 16:54:01 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.7201
2022-02-22 16:54:39 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.3052
2022-02-22 16:55:16 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4902
2022-02-22 16:55:53 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.7923
2022-02-22 16:56:29 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.5640
2022-02-22 16:57:07 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.4132
2022-02-22 16:57:43 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.4742
2022-02-22 16:58:20 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.3153
2022-02-22 16:58:59 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.7117
2022-02-22 16:59:37 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.7028
2022-02-22 17:00:18 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.5303
2022-02-22 17:00:56 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.6106
2022-02-22 17:01:35 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.1790
2022-02-22 17:02:14 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.3362
2022-02-22 17:02:55 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.4392
2022-02-22 17:03:34 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.6109
2022-02-22 17:04:12 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2920
2022-02-22 17:04:47 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.5909
2022-02-22 17:05:22 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.5152
2022-02-22 17:06:00 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.4336
2022-02-22 17:06:40 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.4324
2022-02-22 17:07:18 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.4659
2022-02-22 17:07:57 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.6619
2022-02-22 17:08:34 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.4485
2022-02-22 17:09:12 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.4725
2022-02-22 17:09:49 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.7060
2022-02-22 17:10:28 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.7027
2022-02-22 17:11:08 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.3933
2022-02-22 17:11:50 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.6525
2022-02-22 17:12:31 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.4486
2022-02-22 17:13:11 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.5652
2022-02-22 17:13:54 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.3123
2022-02-22 17:14:36 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.3946
2022-02-22 17:15:19 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.6954
2022-02-22 17:16:04 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.4539
2022-02-22 17:16:47 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.6750
2022-02-22 17:17:28 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.2803
2022-02-22 17:18:12 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.3218
2022-02-22 17:18:53 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.5343
2022-02-22 17:19:38 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.4938
2022-02-22 17:20:21 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.5470
2022-02-22 17:21:06 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.4079
2022-02-22 17:21:48 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.6637
2022-02-22 17:22:32 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.4869
2022-02-22 17:23:09 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.6039
2022-02-22 17:23:49 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.5598
2022-02-22 17:24:26 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.3710
2022-02-22 17:25:02 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.2652
2022-02-22 17:25:03 - train: epoch 017, train_loss: 2.4953
2022-02-22 17:26:29 - eval: epoch: 017, acc1: 48.098%, acc5: 74.662%, test_loss: 2.2498, per_image_load_time: 0.980ms, per_image_inference_time: 0.163ms
2022-02-22 17:26:29 - until epoch: 017, best_acc1: 48.236%
2022-02-22 17:26:29 - epoch 018 lr: 0.1
2022-02-22 17:27:13 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.4936
2022-02-22 17:27:51 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.6383
2022-02-22 17:28:32 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.6599
2022-02-22 17:29:11 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.6054
2022-02-22 17:29:49 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.4753
2022-02-22 17:30:28 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.5268
2022-02-22 17:31:06 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.2235
2022-02-22 17:31:45 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.4504
2022-02-22 17:32:22 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.5731
2022-02-22 17:33:01 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.3850
2022-02-22 17:33:38 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.7477
2022-02-22 17:34:16 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.6135
2022-02-22 17:34:54 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.8710
2022-02-22 17:35:31 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.4088
2022-02-22 17:36:10 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.6688
2022-02-22 17:36:46 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.5681
2022-02-22 17:37:24 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.3686
2022-02-22 17:38:01 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.2572
2022-02-22 17:38:38 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.4641
2022-02-22 17:39:16 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.8512
2022-02-22 17:39:53 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.6136
2022-02-22 17:40:31 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.5091
2022-02-22 17:41:08 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.5925
2022-02-22 17:41:46 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.3135
2022-02-22 17:42:23 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.2238
2022-02-22 17:43:02 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.3280
2022-02-22 17:43:39 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.6019
2022-02-22 17:44:16 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.2483
2022-02-22 17:44:54 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.4810
2022-02-22 17:45:31 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.3638
2022-02-22 17:46:09 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.7507
2022-02-22 17:46:47 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.3751
2022-02-22 17:47:25 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.4397
2022-02-22 17:48:02 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.4370
2022-02-22 17:48:40 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.7292
2022-02-22 17:49:18 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.5950
2022-02-22 17:49:57 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.9086
2022-02-22 17:50:35 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.4546
2022-02-22 17:51:12 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.5323
2022-02-22 17:51:50 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.3083
2022-02-22 17:52:27 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.3802
2022-02-22 17:53:04 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.3913
2022-02-22 17:53:41 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.2501
2022-02-22 17:54:18 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.6756
2022-02-22 17:54:57 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.4990
2022-02-22 17:55:33 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.3089
2022-02-22 17:56:12 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.7734
2022-02-22 17:56:48 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.6732
2022-02-22 17:57:27 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.5383
2022-02-22 17:58:02 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.5908
2022-02-22 17:58:03 - train: epoch 018, train_loss: 2.4889
2022-02-22 17:59:26 - eval: epoch: 018, acc1: 48.234%, acc5: 73.832%, test_loss: 2.2721, per_image_load_time: 1.639ms, per_image_inference_time: 0.185ms
2022-02-22 17:59:27 - until epoch: 018, best_acc1: 48.236%
2022-02-22 17:59:27 - epoch 019 lr: 0.1
2022-02-22 18:00:11 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.2625
2022-02-22 18:00:47 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.6018
2022-02-22 18:01:25 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.6597
2022-02-22 18:02:03 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.4050
2022-02-22 18:02:41 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3369
2022-02-22 18:03:20 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.3915
2022-02-22 18:03:57 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.1995
2022-02-22 18:04:36 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.7146
2022-02-22 18:05:13 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.4465
2022-02-22 18:05:51 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.6702
2022-02-22 18:06:29 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.3133
2022-02-22 18:07:06 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.5933
2022-02-22 18:07:44 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.6960
2022-02-22 18:08:22 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.3782
2022-02-22 18:09:01 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.8215
2022-02-22 18:09:38 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.4650
2022-02-22 18:10:17 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.5749
2022-02-22 18:10:54 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.3681
2022-02-22 18:11:32 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.5835
2022-02-22 18:12:10 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.5073
2022-02-22 18:12:49 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.3810
2022-02-22 18:13:27 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.4761
2022-02-22 18:14:05 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.5515
2022-02-22 18:14:44 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.6215
2022-02-22 18:15:20 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.3561
2022-02-22 18:16:00 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.4857
2022-02-22 18:16:37 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.6220
2022-02-22 18:17:15 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.6031
2022-02-22 18:17:54 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.4704
2022-02-22 18:18:32 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.7324
2022-02-22 18:19:10 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.5655
2022-02-22 18:19:47 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.2328
2022-02-22 18:20:26 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.3498
2022-02-22 18:21:04 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4836
2022-02-22 18:21:41 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.5844
2022-02-22 18:22:20 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.2611
2022-02-22 18:22:58 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.5659
2022-02-22 18:23:36 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.6392
2022-02-22 18:24:16 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.4444
2022-02-22 18:24:53 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.3470
2022-02-22 18:25:32 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.5641
2022-02-22 18:26:09 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.5255
2022-02-22 18:26:48 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.3896
2022-02-22 18:27:26 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.5691
2022-02-22 18:28:04 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.7816
2022-02-22 18:28:41 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.4129
2022-02-22 18:29:19 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.4579
2022-02-22 18:29:56 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.5464
2022-02-22 18:30:34 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3872
2022-02-22 18:31:11 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.4302
2022-02-22 18:31:12 - train: epoch 019, train_loss: 2.4866
2022-02-22 18:32:36 - eval: epoch: 019, acc1: 47.116%, acc5: 73.206%, test_loss: 2.3244, per_image_load_time: 1.643ms, per_image_inference_time: 0.168ms
2022-02-22 18:32:36 - until epoch: 019, best_acc1: 48.236%
2022-02-22 18:32:36 - epoch 020 lr: 0.1
2022-02-22 18:33:20 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.6579
2022-02-22 18:33:58 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.2357
2022-02-22 18:34:36 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.6135
2022-02-22 18:35:14 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.3156
2022-02-22 18:35:52 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3867
2022-02-22 18:36:31 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.5724
2022-02-22 18:37:08 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.3243
2022-02-22 18:37:47 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.5205
2022-02-22 18:38:24 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.8168
2022-02-22 18:39:02 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.5200
2022-02-22 18:39:39 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.3376
2022-02-22 18:40:17 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.2414
2022-02-22 18:40:54 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.4050
2022-02-22 18:41:33 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.6946
2022-02-22 18:42:11 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.5499
2022-02-22 18:42:49 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.3043
2022-02-22 18:43:26 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.1696
2022-02-22 18:44:04 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.4128
2022-02-22 18:44:41 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.3455
2022-02-22 18:45:19 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.4071
2022-02-22 18:45:57 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.6670
2022-02-22 18:46:34 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.3045
2022-02-22 18:47:12 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.3878
2022-02-22 18:47:50 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.6260
2022-02-22 18:48:27 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.3965
2022-02-22 18:49:06 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.2020
2022-02-22 18:49:42 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4738
2022-02-22 18:50:20 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.5683
2022-02-22 18:50:58 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5426
2022-02-22 18:51:35 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.4069
2022-02-22 18:52:14 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.4878
2022-02-22 18:52:51 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.7031
2022-02-22 18:53:29 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.3849
2022-02-22 18:54:05 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.5115
2022-02-22 18:54:43 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.4231
2022-02-22 18:55:22 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.5383
2022-02-22 18:56:00 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.3914
2022-02-22 18:56:38 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.4558
2022-02-22 18:57:16 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.6462
2022-02-22 18:57:53 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.4114
2022-02-22 18:58:30 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2901
2022-02-22 18:59:09 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.3984
2022-02-22 18:59:46 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.6019
2022-02-22 19:00:24 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.4509
2022-02-22 19:01:02 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.6080
2022-02-22 19:01:39 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.5524
2022-02-22 19:02:17 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.3531
2022-02-22 19:02:56 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.5044
2022-02-22 19:03:34 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.5158
2022-02-22 19:04:10 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.2454
2022-02-22 19:04:11 - train: epoch 020, train_loss: 2.4763
2022-02-22 19:05:34 - eval: epoch: 020, acc1: 47.844%, acc5: 74.244%, test_loss: 2.2877, per_image_load_time: 1.450ms, per_image_inference_time: 0.198ms
2022-02-22 19:05:34 - until epoch: 020, best_acc1: 48.236%
2022-02-22 19:05:34 - epoch 021 lr: 0.1
2022-02-22 19:06:17 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.4255
2022-02-22 19:06:56 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.6253
2022-02-22 19:07:33 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.0590
2022-02-22 19:08:09 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.5235
2022-02-22 19:08:48 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.3545
2022-02-22 19:09:26 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.2956
2022-02-22 19:10:04 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.2849
2022-02-22 19:10:42 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.6746
2022-02-22 19:11:20 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.5412
2022-02-22 19:11:57 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.3820
2022-02-22 19:12:35 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.3880
2022-02-22 19:13:10 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.3961
2022-02-22 19:13:45 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.3820
2022-02-22 19:14:25 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.4306
2022-02-22 19:15:03 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.2406
2022-02-22 19:15:42 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.5725
2022-02-22 19:16:19 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.3584
2022-02-22 19:16:58 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.2946
2022-02-22 19:17:35 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.5667
2022-02-22 19:18:14 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.6467
2022-02-22 19:18:51 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.3187
2022-02-22 19:19:29 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.5476
2022-02-22 19:20:06 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.3121
2022-02-22 19:20:44 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.3618
2022-02-22 19:21:22 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.5083
2022-02-22 19:22:00 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.8003
2022-02-22 19:22:39 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.4416
2022-02-22 19:23:17 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.3897
2022-02-22 19:23:53 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.4954
2022-02-22 19:24:32 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.6308
2022-02-22 19:25:09 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.6115
2022-02-22 19:25:46 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.4745
2022-02-22 19:26:22 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.7423
2022-02-22 19:27:01 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.5314
2022-02-22 19:27:39 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.3237
2022-02-22 19:28:16 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.4899
2022-02-22 19:28:54 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.5217
2022-02-22 19:29:32 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.3962
2022-02-22 19:30:11 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.3240
2022-02-22 19:30:47 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.7178
2022-02-22 19:31:26 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.3214
2022-02-22 19:32:02 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.4402
2022-02-22 19:32:41 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.5300
2022-02-22 19:33:17 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.5771
2022-02-22 19:33:55 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.6720
2022-02-22 19:34:33 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.3875
2022-02-22 19:35:10 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.6449
2022-02-22 19:35:49 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.7089
2022-02-22 19:36:27 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.3411
2022-02-22 19:37:03 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.3081
2022-02-22 19:37:04 - train: epoch 021, train_loss: 2.4715
2022-02-22 19:38:29 - eval: epoch: 021, acc1: 48.740%, acc5: 74.946%, test_loss: 2.2252, per_image_load_time: 1.634ms, per_image_inference_time: 0.192ms
2022-02-22 19:38:29 - until epoch: 021, best_acc1: 48.740%
2022-02-22 19:38:29 - epoch 022 lr: 0.1
2022-02-22 19:39:13 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.2820
2022-02-22 19:39:52 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.3467
2022-02-22 19:40:29 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.1591
2022-02-22 19:41:06 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.2524
2022-02-22 19:41:44 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.4439
2022-02-22 19:42:23 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.5464
2022-02-22 19:43:00 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.5551
2022-02-22 19:43:38 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.5757
2022-02-22 19:44:15 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.6729
2022-02-22 19:44:54 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.4349
2022-02-22 19:45:31 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.5730
2022-02-22 19:46:11 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.2052
2022-02-22 19:46:48 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.3275
2022-02-22 19:47:26 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.5208
2022-02-22 19:48:04 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.4120
2022-02-22 19:48:43 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.2834
2022-02-22 19:49:20 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.2450
2022-02-22 19:49:58 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.7049
2022-02-22 19:50:37 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.3084
2022-02-22 19:51:15 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.5478
2022-02-22 19:51:54 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.4886
2022-02-22 19:52:31 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.2410
2022-02-22 19:53:09 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.5894
2022-02-22 19:53:46 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.3318
2022-02-22 19:54:26 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.3414
2022-02-22 19:55:03 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.2576
2022-02-22 19:55:40 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.2666
2022-02-22 19:56:18 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.7775
2022-02-22 19:56:55 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.2642
2022-02-22 19:57:32 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.5648
2022-02-22 19:58:12 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.7195
2022-02-22 19:58:49 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.5605
2022-02-22 19:59:28 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.4627
2022-02-22 20:00:05 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.4016
2022-02-22 20:00:43 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.5400
2022-02-22 20:01:22 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.4614
2022-02-22 20:02:00 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.5730
2022-02-22 20:02:36 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.6449
2022-02-22 20:03:14 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.4962
2022-02-22 20:03:53 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.5819
2022-02-22 20:04:31 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.3479
2022-02-22 20:05:08 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.3305
2022-02-22 20:05:47 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.6854
2022-02-22 20:06:23 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.5435
2022-02-22 20:07:03 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.4677
2022-02-22 20:07:39 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.5906
2022-02-22 20:08:18 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.5820
2022-02-22 20:08:55 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.3162
2022-02-22 20:09:33 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.2600
2022-02-22 20:10:08 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.2457
2022-02-22 20:10:09 - train: epoch 022, train_loss: 2.4697
2022-02-22 20:11:33 - eval: epoch: 022, acc1: 49.542%, acc5: 75.422%, test_loss: 2.1872, per_image_load_time: 2.656ms, per_image_inference_time: 0.204ms
2022-02-22 20:11:34 - until epoch: 022, best_acc1: 49.542%
2022-02-22 20:11:34 - epoch 023 lr: 0.1
2022-02-22 20:12:17 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2919
2022-02-22 20:12:54 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.0674
2022-02-22 20:13:33 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.3371
2022-02-22 20:14:09 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.5220
2022-02-22 20:14:48 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.4714
2022-02-22 20:15:26 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.4836
2022-02-22 20:16:04 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.3230
2022-02-22 20:16:41 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.2959
2022-02-22 20:17:19 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.4819
2022-02-22 20:17:56 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.3328
2022-02-22 20:18:36 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.5071
2022-02-22 20:19:13 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.2126
2022-02-22 20:19:53 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.3501
2022-02-22 20:20:29 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3749
2022-02-22 20:21:09 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.3300
2022-02-22 20:21:46 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3871
2022-02-22 20:22:26 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.5777
2022-02-22 20:23:03 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.3104
2022-02-22 20:23:41 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.4469
2022-02-22 20:24:20 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.1877
2022-02-22 20:24:58 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.5672
2022-02-22 20:25:37 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.1153
2022-02-22 20:26:15 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.3517
2022-02-22 20:26:54 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.4453
2022-02-22 20:27:32 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.5947
2022-02-22 20:28:11 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.4420
2022-02-22 20:28:49 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.3875
2022-02-22 20:29:28 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.5370
2022-02-22 20:30:08 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.4914
2022-02-22 20:30:45 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.6949
2022-02-22 20:31:24 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.5864
2022-02-22 20:32:01 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.5835
2022-02-22 20:32:40 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.6736
2022-02-22 20:33:20 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.5986
2022-02-22 20:33:58 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.3540
2022-02-22 20:34:35 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.2472
2022-02-22 20:35:12 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.4129
2022-02-22 20:35:50 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.5886
2022-02-22 20:36:29 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.5424
2022-02-22 20:37:08 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.5657
2022-02-22 20:37:46 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.3708
2022-02-22 20:38:23 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.3903
2022-02-22 20:39:02 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.3457
2022-02-22 20:39:39 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.2167
2022-02-22 20:40:18 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.3188
2022-02-22 20:40:56 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.4778
2022-02-22 20:41:32 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.2235
2022-02-22 20:42:10 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.3937
2022-02-22 20:42:50 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.3177
2022-02-22 20:43:26 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.5332
2022-02-22 20:43:27 - train: epoch 023, train_loss: 2.4624
2022-02-22 20:44:54 - eval: epoch: 023, acc1: 48.358%, acc5: 74.366%, test_loss: 2.2495, per_image_load_time: 2.432ms, per_image_inference_time: 0.167ms
2022-02-22 20:44:54 - until epoch: 023, best_acc1: 49.542%
2022-02-22 20:44:54 - epoch 024 lr: 0.1
2022-02-22 20:45:39 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.2946
2022-02-22 20:46:18 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.3410
2022-02-22 20:46:57 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.4299
2022-02-22 20:47:36 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.6132
2022-02-22 20:48:13 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.3255
2022-02-22 20:48:50 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1999
2022-02-22 20:49:28 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.3627
2022-02-22 20:50:05 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.3479
2022-02-22 20:50:43 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.4266
2022-02-22 20:51:22 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.3907
2022-02-22 20:51:59 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.2377
2022-02-22 20:52:36 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.2566
2022-02-22 20:53:14 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.7913
2022-02-22 20:53:54 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.3921
2022-02-22 20:54:30 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.7441
2022-02-22 20:55:08 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.5129
2022-02-22 20:55:45 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.4567
2022-02-22 20:56:24 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.6455
2022-02-22 20:57:01 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.2805
2022-02-22 20:57:39 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.4443
2022-02-22 20:58:18 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.4714
2022-02-22 20:58:55 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.3133
2022-02-22 20:59:33 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.3974
2022-02-22 21:00:11 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.4383
2022-02-22 21:00:50 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.4770
2022-02-22 21:01:28 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.3041
2022-02-22 21:02:07 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.5875
2022-02-22 21:02:44 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.5990
2022-02-22 21:03:23 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.5461
2022-02-22 21:04:00 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.4255
2022-02-22 21:04:39 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.3396
2022-02-22 21:05:18 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.6238
2022-02-22 21:05:55 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.1522
2022-02-22 21:06:34 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.4712
2022-02-22 21:07:12 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.2796
2022-02-22 21:07:51 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.6086
2022-02-22 21:08:30 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.3837
2022-02-22 21:09:09 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.6996
2022-02-22 21:09:46 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.3791
2022-02-22 21:10:24 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.5206
2022-02-22 21:11:01 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.4333
2022-02-22 21:11:40 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.4839
2022-02-22 21:12:18 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.4573
2022-02-22 21:12:54 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.4996
2022-02-22 21:13:33 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.2828
2022-02-22 21:14:11 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.5405
2022-02-22 21:14:49 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.4247
2022-02-22 21:15:27 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.3341
2022-02-22 21:16:06 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.5559
2022-02-22 21:16:41 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.6696
2022-02-22 21:16:42 - train: epoch 024, train_loss: 2.4597
2022-02-22 21:18:05 - eval: epoch: 024, acc1: 47.608%, acc5: 73.882%, test_loss: 2.2833, per_image_load_time: 2.932ms, per_image_inference_time: 0.170ms
2022-02-22 21:18:05 - until epoch: 024, best_acc1: 49.542%
2022-02-22 21:18:05 - epoch 025 lr: 0.1
2022-02-22 21:18:49 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.2374
2022-02-22 21:19:26 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.1832
2022-02-22 21:20:03 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.3551
2022-02-22 21:20:41 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.3540
2022-02-22 21:21:18 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.2784
2022-02-22 21:21:57 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.4609
2022-02-22 21:22:30 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.4536
2022-02-22 21:23:06 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.3698
2022-02-22 21:23:40 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.3722
2022-02-22 21:24:20 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.2279
2022-02-22 21:24:58 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.4587
2022-02-22 21:25:38 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.3767
2022-02-22 21:26:14 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.3949
2022-02-22 21:26:53 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4260
2022-02-22 21:27:30 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.3156
2022-02-22 21:28:08 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.2077
2022-02-22 21:28:46 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3640
2022-02-22 21:29:24 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.3246
2022-02-22 21:30:01 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.2939
2022-02-22 21:30:40 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.5065
2022-02-22 21:31:19 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.3346
2022-02-22 21:31:56 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.2352
2022-02-22 21:32:34 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.5623
2022-02-22 21:33:12 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.2535
2022-02-22 21:33:51 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.5269
2022-02-22 21:34:28 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.5720
2022-02-22 21:35:08 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.5681
2022-02-22 21:35:45 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.3830
2022-02-22 21:36:25 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.6139
2022-02-22 21:37:02 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.6290
2022-02-22 21:37:40 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.4248
2022-02-22 21:38:18 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.6244
2022-02-22 21:38:57 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.4703
2022-02-22 21:39:36 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.5893
2022-02-22 21:40:15 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.2286
2022-02-22 21:40:52 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.4861
2022-02-22 21:41:31 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3813
2022-02-22 21:42:08 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.5312
2022-02-22 21:42:47 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.5627
2022-02-22 21:43:25 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.6129
2022-02-22 21:44:04 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.6981
2022-02-22 21:44:40 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.5469
2022-02-22 21:45:18 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.3052
2022-02-22 21:45:55 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.3214
2022-02-22 21:46:33 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.4508
2022-02-22 21:47:12 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.4996
2022-02-22 21:47:48 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.4851
2022-02-22 21:48:26 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.2169
2022-02-22 21:49:03 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.3719
2022-02-22 21:49:39 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.6496
2022-02-22 21:49:40 - train: epoch 025, train_loss: 2.4572
2022-02-22 21:51:05 - eval: epoch: 025, acc1: 49.104%, acc5: 75.052%, test_loss: 2.2130, per_image_load_time: 3.028ms, per_image_inference_time: 0.183ms
2022-02-22 21:51:05 - until epoch: 025, best_acc1: 49.542%
2022-02-22 21:51:05 - epoch 026 lr: 0.1
2022-02-22 21:51:48 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.3613
2022-02-22 21:52:26 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.3518
2022-02-22 21:53:03 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.3367
2022-02-22 21:53:42 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.3181
2022-02-22 21:54:20 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.4841
2022-02-22 21:54:59 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.5896
2022-02-22 21:55:35 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.3848
2022-02-22 21:56:14 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.3432
2022-02-22 21:56:53 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.6786
2022-02-22 21:57:31 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.3883
2022-02-22 21:58:10 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.2926
2022-02-22 21:58:49 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.5624
2022-02-22 21:59:26 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.3762
2022-02-22 22:00:05 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.4569
2022-02-22 22:00:42 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3322
2022-02-22 22:01:22 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.5491
2022-02-22 22:02:00 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.3833
2022-02-22 22:02:38 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.6533
2022-02-22 22:03:16 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.7416
2022-02-22 22:03:54 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.6752
2022-02-22 22:04:33 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.5032
2022-02-22 22:05:10 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.4704
2022-02-22 22:05:49 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.4120
2022-02-22 22:06:28 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.6144
2022-02-22 22:07:05 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.6472
2022-02-22 22:07:44 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.4690
2022-02-22 22:08:22 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.3450
2022-02-22 22:09:02 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.2465
2022-02-22 22:09:38 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.6003
2022-02-22 22:10:16 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.3158
2022-02-22 22:10:55 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.4827
2022-02-22 22:11:34 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.3424
2022-02-22 22:12:12 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.2865
2022-02-22 22:12:50 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.6116
2022-02-22 22:13:28 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.4900
2022-02-22 22:14:06 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.4316
2022-02-22 22:14:46 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.5073
2022-02-22 22:15:24 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.6401
2022-02-22 22:16:02 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.6077
2022-02-22 22:16:40 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.6522
2022-02-22 22:17:19 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.5222
2022-02-22 22:17:57 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.4931
2022-02-22 22:18:33 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.5005
2022-02-22 22:19:12 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.5581
2022-02-22 22:19:49 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.6618
2022-02-22 22:20:28 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.3956
2022-02-22 22:21:06 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.3724
2022-02-22 22:21:44 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.3136
2022-02-22 22:22:22 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.4737
2022-02-22 22:22:59 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5581
2022-02-22 22:22:59 - train: epoch 026, train_loss: 2.4521
2022-02-22 22:24:25 - eval: epoch: 026, acc1: 48.078%, acc5: 74.534%, test_loss: 2.2520, per_image_load_time: 2.803ms, per_image_inference_time: 0.192ms
2022-02-22 22:24:26 - until epoch: 026, best_acc1: 49.542%
2022-02-23 06:08:27 - epoch 027 lr: 0.1
2022-02-23 06:09:09 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.6630
2022-02-23 06:09:46 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.4216
2022-02-23 06:10:21 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.4571
2022-02-23 06:10:59 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.6694
2022-02-23 06:11:35 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.5869
2022-02-23 06:12:13 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6337
2022-02-23 06:12:48 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.2848
2022-02-23 06:13:25 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3665
2022-02-23 06:14:03 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.2525
2022-02-23 06:14:40 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.7123
2022-02-23 06:15:16 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.5567
2022-02-23 06:15:53 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.6172
2022-02-23 06:16:31 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.5394
2022-02-23 06:17:07 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.6090
2022-02-23 06:17:45 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.5349
2022-02-23 06:18:22 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.6259
2022-02-23 06:18:58 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.3431
2022-02-23 06:19:36 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.4526
2022-02-23 06:20:12 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.5441
2022-02-23 06:20:50 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.3843
2022-02-23 06:21:27 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.5467
2022-02-23 06:22:04 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.6360
2022-02-23 06:22:41 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.5885
2022-02-23 06:23:19 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.3445
2022-02-23 06:23:55 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.3300
2022-02-23 06:24:32 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.5023
2022-02-23 06:25:08 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.5312
2022-02-23 06:25:45 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.4369
2022-02-23 06:26:23 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.3828
2022-02-23 06:27:00 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.3165
2022-02-23 06:27:36 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.3924
2022-02-23 06:28:14 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.3602
2022-02-23 06:28:50 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.4411
2022-02-23 06:29:28 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.4084
2022-02-23 06:30:04 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.6326
2022-02-23 06:30:41 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.3610
2022-02-23 06:31:17 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.3834
2022-02-23 06:31:55 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.1383
2022-02-23 06:32:32 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.4053
2022-02-23 06:33:09 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.7426
2022-02-23 06:33:46 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.5087
2022-02-23 06:34:22 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.4758
2022-02-23 06:35:01 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.6598
2022-02-23 06:35:37 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.5332
2022-02-23 06:36:14 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.1865
2022-02-23 06:36:50 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.3666
2022-02-23 06:37:27 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.5733
2022-02-23 06:38:05 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.7123
2022-02-23 06:38:42 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.1407
2022-02-23 06:39:16 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.0978
2022-02-23 06:39:17 - train: epoch 027, train_loss: 2.4491
2022-02-23 06:40:39 - eval: epoch: 027, acc1: 50.042%, acc5: 76.012%, test_loss: 2.1543, per_image_load_time: 3.000ms, per_image_inference_time: 0.174ms
2022-02-23 06:40:39 - until epoch: 027, best_acc1: 50.042%
2022-02-23 06:40:39 - epoch 028 lr: 0.1
2022-02-23 06:41:21 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.3507
2022-02-23 06:41:58 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2988
2022-02-23 06:42:35 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.3621
2022-02-23 06:43:11 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.5115
2022-02-23 06:43:48 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.2537
2022-02-23 06:44:25 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.5900
2022-02-23 06:45:02 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.6098
2022-02-23 06:45:38 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.0693
2022-02-23 06:46:15 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.2104
2022-02-23 06:46:52 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.5818
2022-02-23 06:47:28 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.3319
2022-02-23 06:48:05 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.3255
2022-02-23 06:48:41 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.4122
2022-02-23 06:49:18 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.6896
2022-02-23 06:49:56 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.3055
2022-02-23 06:50:33 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.5434
2022-02-23 06:51:09 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.5042
2022-02-23 06:51:46 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.2471
2022-02-23 06:52:22 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.4102
2022-02-23 06:52:59 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.5223
2022-02-23 06:53:36 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.5413
2022-02-23 06:54:14 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.5282
2022-02-23 06:54:49 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.9425
2022-02-23 06:55:27 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.4800
2022-02-23 06:56:03 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.4586
2022-02-23 06:56:40 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.5616
2022-02-23 06:57:16 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.4393
2022-02-23 06:57:53 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.5300
2022-02-23 06:58:27 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.4801
2022-02-23 06:59:06 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.4352
2022-02-23 06:59:41 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.5568
2022-02-23 07:00:19 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.4064
2022-02-23 07:00:55 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.4266
2022-02-23 07:01:32 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.5580
2022-02-23 07:02:08 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.5071
2022-02-23 07:02:45 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.3912
2022-02-23 07:03:20 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.3763
2022-02-23 07:03:57 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.2485
2022-02-23 07:04:33 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.4660
2022-02-23 07:05:12 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.4741
2022-02-23 07:05:48 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.5292
2022-02-23 07:06:24 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.5166
2022-02-23 07:07:01 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.3592
2022-02-23 07:07:37 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.4180
2022-02-23 07:08:13 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.4562
2022-02-23 07:08:51 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.2955
2022-02-23 07:09:26 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.8047
2022-02-23 07:10:03 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.6067
2022-02-23 07:10:40 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.5253
2022-02-23 07:11:15 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.2930
2022-02-23 07:11:16 - train: epoch 028, train_loss: 2.4460
2022-02-23 07:12:37 - eval: epoch: 028, acc1: 46.582%, acc5: 72.650%, test_loss: 2.3671, per_image_load_time: 3.016ms, per_image_inference_time: 0.177ms
2022-02-23 07:12:38 - until epoch: 028, best_acc1: 50.042%
2022-02-23 07:12:38 - epoch 029 lr: 0.1
2022-02-23 07:13:19 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.4022
2022-02-23 07:13:57 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4779
2022-02-23 07:14:34 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.4560
2022-02-23 07:15:09 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.4331
2022-02-23 07:15:46 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3547
2022-02-23 07:16:23 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.5676
2022-02-23 07:17:00 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.2231
2022-02-23 07:17:37 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.3607
2022-02-23 07:18:12 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.4147
2022-02-23 07:18:50 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.3516
2022-02-23 07:19:26 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.2273
2022-02-23 07:20:04 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.3936
2022-02-23 07:20:40 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.4207
2022-02-23 07:21:17 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.5850
2022-02-23 07:21:54 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.5624
2022-02-23 07:22:31 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.4017
2022-02-23 07:23:07 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.3335
2022-02-23 07:23:44 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.2961
2022-02-23 07:24:20 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.2912
2022-02-23 07:24:57 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.4251
2022-02-23 07:25:35 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.3355
2022-02-23 07:26:11 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.5475
2022-02-23 07:26:49 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.6793
2022-02-23 07:27:25 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.2364
2022-02-23 07:28:02 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.6433
2022-02-23 07:28:39 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.4614
2022-02-23 07:29:15 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.4227
2022-02-23 07:29:53 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.3367
2022-02-23 07:30:28 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.3691
2022-02-23 07:31:05 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.3840
2022-02-23 07:31:43 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.4037
2022-02-23 07:32:17 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.4181
2022-02-23 07:32:51 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.5601
2022-02-23 07:33:29 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.3830
2022-02-23 07:34:07 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.6473
2022-02-23 07:34:44 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.5333
2022-02-23 07:35:21 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.5806
2022-02-23 07:35:58 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.3889
2022-02-23 07:36:34 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.3497
2022-02-23 07:37:10 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.6846
2022-02-23 07:37:47 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.3370
2022-02-23 07:38:23 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.1966
2022-02-23 07:39:00 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.3644
2022-02-23 07:39:36 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.4711
2022-02-23 07:40:14 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.6155
2022-02-23 07:40:49 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.6077
2022-02-23 07:41:27 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.2197
2022-02-23 07:42:03 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.6682
2022-02-23 07:42:41 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.6137
2022-02-23 07:43:16 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.2666
2022-02-23 07:43:16 - train: epoch 029, train_loss: 2.4410
2022-02-23 07:44:38 - eval: epoch: 029, acc1: 49.918%, acc5: 76.020%, test_loss: 2.1627, per_image_load_time: 2.409ms, per_image_inference_time: 0.196ms
2022-02-23 07:44:38 - until epoch: 029, best_acc1: 50.042%
2022-02-23 07:44:38 - epoch 030 lr: 0.1
2022-02-23 07:45:20 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.3044
2022-02-23 07:45:57 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.4639
2022-02-23 07:46:34 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.3955
2022-02-23 07:47:09 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.3203
2022-02-23 07:47:46 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.5729
2022-02-23 07:48:23 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.6188
2022-02-23 07:49:01 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.5367
2022-02-23 07:50:14 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.3445
2022-02-23 07:50:52 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.3470
2022-02-23 07:51:27 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.2919
2022-02-23 07:52:04 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.3855
2022-02-23 07:52:41 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.2058
2022-02-23 07:53:18 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.2698
2022-02-23 07:53:55 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.3064
2022-02-23 07:54:32 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.4695
2022-02-23 07:55:08 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.6390
2022-02-23 07:55:46 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2944
2022-02-23 07:56:22 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.4114
2022-02-23 07:56:59 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.3001
2022-02-23 07:57:36 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.6257
2022-02-23 07:58:13 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.4758
2022-02-23 07:58:50 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.5505
2022-02-23 07:59:25 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4941
2022-02-23 08:00:03 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.5250
2022-02-23 08:00:40 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.2414
2022-02-23 08:01:17 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.4184
2022-02-23 08:01:53 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3389
2022-02-23 08:02:29 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.4090
2022-02-23 08:03:07 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.5008
2022-02-23 08:03:44 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.6218
2022-02-23 08:04:20 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.3860
2022-02-23 08:04:58 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.5622
2022-02-23 08:05:34 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.2944
2022-02-23 08:06:10 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.5958
2022-02-23 08:06:47 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.2506
2022-02-23 08:07:24 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.3347
2022-02-23 08:08:00 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.4246
2022-02-23 08:08:37 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3484
2022-02-23 08:09:13 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.2764
2022-02-23 08:09:50 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.4238
2022-02-23 08:10:27 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5745
2022-02-23 08:11:05 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.3649
2022-02-23 08:11:41 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.3578
2022-02-23 08:12:17 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.4569
2022-02-23 08:12:54 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.2205
2022-02-23 08:13:31 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.3816
2022-02-23 08:14:08 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.5798
2022-02-23 08:14:45 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.3452
2022-02-23 08:15:20 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.5036
2022-02-23 08:15:21 - train: epoch 030, train_loss: 2.4419
2022-02-23 08:16:42 - eval: epoch: 030, acc1: 49.546%, acc5: 75.410%, test_loss: 2.1946, per_image_load_time: 2.570ms, per_image_inference_time: 0.197ms
2022-02-23 08:16:43 - until epoch: 030, best_acc1: 50.042%
2022-02-23 08:16:43 - epoch 031 lr: 0.010000000000000002
2022-02-23 08:17:25 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.2664
2022-02-23 08:18:02 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.2198
2022-02-23 08:18:38 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.9729
2022-02-23 08:19:15 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.9211
2022-02-23 08:19:51 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.8797
2022-02-23 08:20:28 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.0699
2022-02-23 08:21:06 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.1908
2022-02-23 08:21:42 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.0559
2022-02-23 08:22:19 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.9183
2022-02-23 08:22:56 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.2845
2022-02-23 08:23:32 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.1026
2022-02-23 08:24:09 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.9055
2022-02-23 08:24:46 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.7893
2022-02-23 08:25:24 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.9770
2022-02-23 08:25:59 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.0806
2022-02-23 08:26:36 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.8579
2022-02-23 08:27:13 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.8134
2022-02-23 08:27:49 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.7961
2022-02-23 08:28:27 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.8909
2022-02-23 08:29:03 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.9033
2022-02-23 08:29:39 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6388
2022-02-23 08:30:15 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.5732
2022-02-23 08:30:51 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.7632
2022-02-23 08:31:27 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.8359
2022-02-23 08:32:05 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.8050
2022-02-23 08:32:41 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.9363
2022-02-23 08:33:19 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.9776
2022-02-23 08:33:54 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.2585
2022-02-23 08:34:31 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.9744
2022-02-23 08:35:07 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.2280
2022-02-23 08:35:45 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.7699
2022-02-23 08:36:20 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.9006
2022-02-23 08:36:56 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8376
2022-02-23 08:37:33 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.0349
2022-02-23 08:38:10 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.1295
2022-02-23 08:38:46 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7666
2022-02-23 08:39:23 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.8296
2022-02-23 08:39:59 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.8148
2022-02-23 08:40:36 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.9811
2022-02-23 08:41:13 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.8460
2022-02-23 08:41:49 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.7671
2022-02-23 08:42:25 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.8407
2022-02-23 08:43:02 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.8717
2022-02-23 08:43:40 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.0928
2022-02-23 08:44:16 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.9870
2022-02-23 08:44:53 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.9577
2022-02-23 08:45:29 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.7545
2022-02-23 08:46:06 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.7366
2022-02-23 08:46:43 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.8402
2022-02-23 08:47:17 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.7494
2022-02-23 08:47:19 - train: epoch 031, train_loss: 1.9271
2022-02-23 08:48:41 - eval: epoch: 031, acc1: 62.586%, acc5: 84.984%, test_loss: 1.5486, per_image_load_time: 2.785ms, per_image_inference_time: 0.192ms
2022-02-23 08:48:42 - until epoch: 031, best_acc1: 62.586%
2022-02-23 08:48:42 - epoch 032 lr: 0.010000000000000002
2022-02-23 08:49:24 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.6584
2022-02-23 08:50:00 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.9270
2022-02-23 08:50:38 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.3818
2022-02-23 08:51:14 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.6506
2022-02-23 08:51:51 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.8431
2022-02-23 08:52:27 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.9552
2022-02-23 08:53:06 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6722
2022-02-23 08:53:41 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.9282
2022-02-23 08:54:19 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.9016
2022-02-23 08:54:55 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7692
2022-02-23 08:55:32 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.8217
2022-02-23 08:56:08 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.8765
2022-02-23 08:56:45 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.8588
2022-02-23 08:57:22 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.9760
2022-02-23 08:57:59 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.7996
2022-02-23 08:58:35 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.9476
2022-02-23 08:59:12 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.9221
2022-02-23 08:59:47 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.1045
2022-02-23 09:00:24 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.3824
2022-02-23 09:00:59 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.6069
2022-02-23 09:01:37 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.7837
2022-02-23 09:02:14 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.8950
2022-02-23 09:02:50 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.9472
2022-02-23 09:03:26 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.8988
2022-02-23 09:04:03 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7635
2022-02-23 09:04:40 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.6226
2022-02-23 09:05:16 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.7045
2022-02-23 09:05:55 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.7572
2022-02-23 09:06:32 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.8006
2022-02-23 09:07:11 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.5619
2022-02-23 09:07:46 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.7743
2022-02-23 09:08:25 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.8886
2022-02-23 09:09:02 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.6973
2022-02-23 09:09:40 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.5900
2022-02-23 09:10:17 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.7049
2022-02-23 09:10:55 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.9902
2022-02-23 09:11:32 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.9510
2022-02-23 09:12:10 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.7246
2022-02-23 09:12:46 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.5996
2022-02-23 09:13:24 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.8976
2022-02-23 09:14:03 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.7081
2022-02-23 09:14:38 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.5910
2022-02-23 09:15:16 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.9429
2022-02-23 09:15:54 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.9858
2022-02-23 09:16:30 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.9620
2022-02-23 09:17:08 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.6173
2022-02-23 09:17:46 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.0347
2022-02-23 09:18:23 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.8285
2022-02-23 09:19:00 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.6813
2022-02-23 09:19:36 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.7139
2022-02-23 09:19:37 - train: epoch 032, train_loss: 1.8083
2022-02-23 09:21:01 - eval: epoch: 032, acc1: 63.644%, acc5: 85.588%, test_loss: 1.4986, per_image_load_time: 2.339ms, per_image_inference_time: 0.176ms
2022-02-23 09:21:01 - until epoch: 032, best_acc1: 63.644%
2022-02-23 09:21:01 - epoch 033 lr: 0.010000000000000002
2022-02-23 09:21:43 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.6630
2022-02-23 09:22:21 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.6914
2022-02-23 09:23:00 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.7699
2022-02-23 09:23:38 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.6144
2022-02-23 09:24:15 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7422
2022-02-23 09:24:54 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.7117
2022-02-23 09:25:30 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.0272
2022-02-23 09:26:07 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.1331
2022-02-23 09:26:46 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.8643
2022-02-23 09:27:23 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.8627
2022-02-23 09:28:01 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.3854
2022-02-23 09:28:39 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.0255
2022-02-23 09:29:17 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.6794
2022-02-23 09:29:55 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.8761
2022-02-23 09:30:32 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.8940
2022-02-23 09:31:10 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.8803
2022-02-23 09:31:47 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.8149
2022-02-23 09:32:25 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.8285
2022-02-23 09:33:02 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.8323
2022-02-23 09:33:40 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.7199
2022-02-23 09:34:17 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.8295
2022-02-23 09:34:56 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.9221
2022-02-23 09:35:33 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.6807
2022-02-23 09:36:12 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.9071
2022-02-23 09:36:48 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.8178
2022-02-23 09:37:24 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.5500
2022-02-23 09:38:00 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.8217
2022-02-23 09:38:40 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.8457
2022-02-23 09:39:19 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.9631
2022-02-23 09:39:56 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.7870
2022-02-23 09:40:35 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.7616
2022-02-23 09:41:13 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.8987
2022-02-23 09:41:50 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.7122
2022-02-23 09:42:28 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.5914
2022-02-23 09:43:05 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.8326
2022-02-23 09:43:44 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.9988
2022-02-23 09:44:21 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.7113
2022-02-23 09:45:00 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.7176
2022-02-23 09:45:37 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.8677
2022-02-23 09:46:15 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.9852
2022-02-23 09:46:53 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.7213
2022-02-23 09:47:32 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.6196
2022-02-23 09:48:09 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.9152
2022-02-23 09:48:48 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.7871
2022-02-23 09:49:25 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.0120
2022-02-23 09:50:03 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.7199
2022-02-23 09:50:41 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.6725
2022-02-23 09:51:19 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.8695
2022-02-23 09:51:57 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.5146
2022-02-23 09:52:32 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.6330
2022-02-23 09:52:33 - train: epoch 033, train_loss: 1.7620
2022-02-23 09:53:58 - eval: epoch: 033, acc1: 64.268%, acc5: 85.938%, test_loss: 1.4713, per_image_load_time: 2.572ms, per_image_inference_time: 0.179ms
2022-02-23 09:53:59 - until epoch: 033, best_acc1: 64.268%
2022-02-23 09:53:59 - epoch 034 lr: 0.010000000000000002
2022-02-23 09:54:43 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.7369
2022-02-23 09:55:20 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.6556
2022-02-23 09:55:58 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.5657
2022-02-23 09:56:36 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.5489
2022-02-23 09:57:15 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.8139
2022-02-23 09:57:52 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.9747
2022-02-23 09:58:30 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.6442
2022-02-23 09:59:07 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.8606
2022-02-23 09:59:45 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.6217
2022-02-23 10:00:23 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.6444
2022-02-23 10:01:02 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.8813
2022-02-23 10:01:38 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.8764
2022-02-23 10:02:17 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.7384
2022-02-23 10:02:54 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.7509
2022-02-23 10:03:32 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.7256
2022-02-23 10:04:10 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.8505
2022-02-23 10:04:47 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.6778
2022-02-23 10:05:24 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.7511
2022-02-23 10:06:02 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.8708
2022-02-23 10:06:40 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.7905
2022-02-23 10:07:18 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.8123
2022-02-23 10:07:55 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.6560
2022-02-23 10:08:32 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.7866
2022-02-23 10:09:11 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.7723
2022-02-23 10:09:47 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.7888
2022-02-23 10:10:26 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.8194
2022-02-23 10:11:03 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.8951
2022-02-23 10:11:41 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.6319
2022-02-23 10:12:18 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.5393
2022-02-23 10:12:56 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.6818
2022-02-23 10:13:33 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.9760
2022-02-23 10:14:12 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.7644
2022-02-23 10:14:49 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.7433
2022-02-23 10:15:26 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.9558
2022-02-23 10:16:04 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.6547
2022-02-23 10:16:40 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.5749
2022-02-23 10:17:19 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.6465
2022-02-23 10:17:55 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.7512
2022-02-23 10:18:34 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.9444
2022-02-23 10:19:11 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.7892
2022-02-23 10:19:49 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.8939
2022-02-23 10:20:26 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.7362
2022-02-23 10:21:04 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.7115
2022-02-23 10:21:41 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6236
2022-02-23 10:22:19 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.9082
2022-02-23 10:22:57 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.8429
2022-02-23 10:23:33 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.7727
2022-02-23 10:24:12 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.6944
2022-02-23 10:24:48 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.9143
2022-02-23 10:25:24 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.5108
2022-02-23 10:25:25 - train: epoch 034, train_loss: 1.7348
2022-02-23 10:26:49 - eval: epoch: 034, acc1: 64.634%, acc5: 86.164%, test_loss: 1.4577, per_image_load_time: 2.996ms, per_image_inference_time: 0.187ms
2022-02-23 10:26:49 - until epoch: 034, best_acc1: 64.634%
2022-02-23 10:26:49 - epoch 035 lr: 0.010000000000000002
2022-02-23 10:27:32 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.4816
2022-02-23 10:28:10 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.5182
2022-02-23 10:28:49 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.7284
2022-02-23 10:29:26 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.5462
2022-02-23 10:30:03 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.6222
2022-02-23 10:30:40 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.6592
2022-02-23 10:31:18 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.6868
2022-02-23 10:31:56 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.7058
2022-02-23 10:32:34 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.7501
2022-02-23 10:33:11 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.6305
2022-02-23 10:33:48 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.6420
2022-02-23 10:34:26 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.6359
2022-02-23 10:35:02 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.8198
2022-02-23 10:35:40 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.5896
2022-02-23 10:36:19 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.8473
2022-02-23 10:36:56 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.7066
2022-02-23 10:37:33 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.4351
2022-02-23 10:38:10 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.8369
2022-02-23 10:38:50 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.8317
2022-02-23 10:39:26 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.9463
2022-02-23 10:40:05 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.5653
2022-02-23 10:40:41 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.7972
2022-02-23 10:41:18 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.5591
2022-02-23 10:41:56 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.8420
2022-02-23 10:42:34 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.8066
2022-02-23 10:43:13 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.7024
2022-02-23 10:43:49 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.7483
2022-02-23 10:44:27 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.8626
2022-02-23 10:45:04 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.7602
2022-02-23 10:45:43 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.5713
2022-02-23 10:46:20 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.7270
2022-02-23 10:46:57 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.7290
2022-02-23 10:47:35 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.6797
2022-02-23 10:48:11 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.6161
2022-02-23 10:48:50 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.5155
2022-02-23 10:49:28 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.8271
2022-02-23 10:50:06 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.8000
2022-02-23 10:50:43 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.6718
2022-02-23 10:51:21 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.5384
2022-02-23 10:51:58 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.4566
2022-02-23 10:52:36 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.9339
2022-02-23 10:53:13 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4773
2022-02-23 10:53:51 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.9657
2022-02-23 10:54:29 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.7737
2022-02-23 10:55:06 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.8330
2022-02-23 10:55:43 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.6918
2022-02-23 10:56:21 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.8470
2022-02-23 10:56:59 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.8868
2022-02-23 10:57:36 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.7253
2022-02-23 10:58:11 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.6569
2022-02-23 10:58:12 - train: epoch 035, train_loss: 1.7142
2022-02-23 10:59:38 - eval: epoch: 035, acc1: 64.692%, acc5: 86.256%, test_loss: 1.4531, per_image_load_time: 2.617ms, per_image_inference_time: 0.168ms
2022-02-23 10:59:38 - until epoch: 035, best_acc1: 64.692%
2022-02-23 10:59:38 - epoch 036 lr: 0.010000000000000002
2022-02-23 11:00:22 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.8438
2022-02-23 11:01:00 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.3859
2022-02-23 11:01:38 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.4556
2022-02-23 11:02:16 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.5654
2022-02-23 11:02:53 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.8581
2022-02-23 11:03:32 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.6634
2022-02-23 11:04:09 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.6560
2022-02-23 11:04:47 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.5406
2022-02-23 11:05:23 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.6827
2022-02-23 11:06:01 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.6921
2022-02-23 11:06:38 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.5662
2022-02-23 11:07:17 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.6265
2022-02-23 11:07:54 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.6796
2022-02-23 11:08:32 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8026
2022-02-23 11:09:09 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.6170
2022-02-23 11:09:47 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.7470
2022-02-23 11:10:24 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.6574
2022-02-23 11:11:02 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.7609
2022-02-23 11:11:39 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.6552
2022-02-23 11:12:16 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.8212
2022-02-23 11:12:54 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.5658
2022-02-23 11:13:32 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.7458
2022-02-23 11:14:09 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.7949
2022-02-23 11:14:47 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.8224
2022-02-23 11:15:24 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.4600
2022-02-23 11:16:03 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.8101
2022-02-23 11:16:39 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.5022
2022-02-23 11:17:17 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.6551
2022-02-23 11:17:55 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.5192
2022-02-23 11:18:33 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.7069
2022-02-23 11:19:09 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.7605
2022-02-23 11:19:47 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.7928
2022-02-23 11:20:23 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.6252
2022-02-23 11:21:02 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.6000
2022-02-23 11:21:39 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.0728
2022-02-23 11:22:18 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.8551
2022-02-23 11:22:54 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.7171
2022-02-23 11:23:33 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.6292
2022-02-23 11:24:10 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.7611
2022-02-23 11:24:48 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.8274
2022-02-23 11:25:25 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.6551
2022-02-23 11:26:03 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.8694
2022-02-23 11:26:40 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.6800
2022-02-23 11:27:17 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.7461
2022-02-23 11:27:55 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.6173
2022-02-23 11:28:34 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.5857
2022-02-23 11:29:11 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.5445
2022-02-23 11:29:49 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.5235
2022-02-23 11:30:26 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.6683
2022-02-23 11:31:02 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.5891
2022-02-23 11:31:03 - train: epoch 036, train_loss: 1.7020
2022-02-23 11:32:29 - eval: epoch: 036, acc1: 64.904%, acc5: 86.250%, test_loss: 1.4445, per_image_load_time: 3.051ms, per_image_inference_time: 0.199ms
2022-02-23 11:32:30 - until epoch: 036, best_acc1: 64.904%
2022-02-23 11:32:30 - epoch 037 lr: 0.010000000000000002
2022-02-23 11:33:14 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.5684
2022-02-23 11:33:51 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.5546
2022-02-23 11:34:29 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.6523
2022-02-23 11:35:08 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5460
2022-02-23 11:35:45 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.6838
2022-02-23 11:36:22 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.8813
2022-02-23 11:36:59 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.8396
2022-02-23 11:37:36 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5276
2022-02-23 11:38:14 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.9229
2022-02-23 11:38:52 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.4942
2022-02-23 11:39:30 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.8272
2022-02-23 11:40:07 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.9179
2022-02-23 11:40:44 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.8796
2022-02-23 11:41:21 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.1221
2022-02-23 11:41:57 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.6567
2022-02-23 11:42:33 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.4487
2022-02-23 11:43:13 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.6698
2022-02-23 11:43:51 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.7120
2022-02-23 11:44:29 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.6831
2022-02-23 11:45:06 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.6739
2022-02-23 11:45:44 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.6807
2022-02-23 11:46:22 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.5952
2022-02-23 11:46:59 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.6917
2022-02-23 11:47:37 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.7670
2022-02-23 11:48:15 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.7027
2022-02-23 11:48:52 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.8210
2022-02-23 11:49:30 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.8903
2022-02-23 11:50:08 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.7027
2022-02-23 11:50:45 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.7314
2022-02-23 11:51:22 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.8998
2022-02-23 11:52:01 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.7566
2022-02-23 11:52:39 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.7219
2022-02-23 11:53:17 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.6485
2022-02-23 11:53:53 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.6030
2022-02-23 11:54:31 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.6327
2022-02-23 11:55:08 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.8628
2022-02-23 11:55:45 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.5181
2022-02-23 11:56:23 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.7391
2022-02-23 11:57:00 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.6696
2022-02-23 11:57:38 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.7799
2022-02-23 11:58:15 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.5387
2022-02-23 11:58:54 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.7407
2022-02-23 11:59:30 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.7351
2022-02-23 12:00:08 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.5207
2022-02-23 12:00:45 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.4106
2022-02-23 12:01:23 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.6658
2022-02-23 12:02:01 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.7952
2022-02-23 12:02:38 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.7144
2022-02-23 12:03:15 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.7623
2022-02-23 12:03:51 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.5285
2022-02-23 12:03:53 - train: epoch 037, train_loss: 1.6949
2022-02-23 12:05:16 - eval: epoch: 037, acc1: 64.636%, acc5: 86.216%, test_loss: 1.4536, per_image_load_time: 2.905ms, per_image_inference_time: 0.179ms
2022-02-23 12:05:16 - until epoch: 037, best_acc1: 64.904%
2022-02-23 12:05:16 - epoch 038 lr: 0.010000000000000002
2022-02-23 12:06:00 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.5954
2022-02-23 12:06:38 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.7530
2022-02-23 12:07:15 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.4578
2022-02-23 12:07:54 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.5837
2022-02-23 12:08:31 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.5579
2022-02-23 12:09:09 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.0212
2022-02-23 12:09:46 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.4792
2022-02-23 12:10:25 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.7066
2022-02-23 12:11:01 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.7855
2022-02-23 12:11:39 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.7637
2022-02-23 12:12:16 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5655
2022-02-23 12:12:55 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.7958
2022-02-23 12:13:32 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.6511
2022-02-23 12:14:10 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.7064
2022-02-23 12:14:47 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.6673
2022-02-23 12:15:26 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.6489
2022-02-23 12:16:02 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.9211
2022-02-23 12:16:40 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.6872
2022-02-23 12:17:17 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.6982
2022-02-23 12:17:56 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.7013
2022-02-23 12:18:33 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.6166
2022-02-23 12:19:11 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.5836
2022-02-23 12:19:48 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.9933
2022-02-23 12:20:26 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.8467
2022-02-23 12:21:04 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.5058
2022-02-23 12:21:41 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.6153
2022-02-23 12:22:19 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.5439
2022-02-23 12:22:57 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.8779
2022-02-23 12:23:36 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.7489
2022-02-23 12:24:13 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.6968
2022-02-23 12:24:50 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.7798
2022-02-23 12:25:28 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2915
2022-02-23 12:26:05 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.6397
2022-02-23 12:26:43 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.6314
2022-02-23 12:27:20 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.0372
2022-02-23 12:27:58 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.7384
2022-02-23 12:28:35 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.5928
2022-02-23 12:29:14 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.8310
2022-02-23 12:29:51 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.6605
2022-02-23 12:30:29 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.5782
2022-02-23 12:31:05 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.7823
2022-02-23 12:31:44 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.5562
2022-02-23 12:32:21 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.6955
2022-02-23 12:32:58 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.7051
2022-02-23 12:33:35 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.7759
2022-02-23 12:34:12 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.8054
2022-02-23 12:34:50 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.5992
2022-02-23 12:35:29 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.6239
2022-02-23 12:36:06 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.5529
2022-02-23 12:36:41 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.5807
2022-02-23 12:36:42 - train: epoch 038, train_loss: 1.6897
2022-02-23 12:38:06 - eval: epoch: 038, acc1: 64.810%, acc5: 86.280%, test_loss: 1.4495, per_image_load_time: 2.682ms, per_image_inference_time: 0.187ms
2022-02-23 12:38:07 - until epoch: 038, best_acc1: 64.904%
2022-02-23 12:38:07 - epoch 039 lr: 0.010000000000000002
2022-02-23 12:38:50 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.6438
2022-02-23 12:39:30 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.7539
2022-02-23 12:40:07 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.5434
2022-02-23 12:40:44 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.6950
2022-02-23 12:41:22 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4998
2022-02-23 12:42:00 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.7591
2022-02-23 12:42:38 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.8628
2022-02-23 12:43:16 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.5575
2022-02-23 12:43:53 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.9851
2022-02-23 12:44:31 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.6404
2022-02-23 12:45:08 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.8267
2022-02-23 12:45:47 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.6088
2022-02-23 12:46:24 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.9431
2022-02-23 12:47:02 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.6883
2022-02-23 12:47:39 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.5681
2022-02-23 12:48:16 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.7133
2022-02-23 12:48:55 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.5910
2022-02-23 12:49:32 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.6893
2022-02-23 12:50:09 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.4053
2022-02-23 12:50:47 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.6653
2022-02-23 12:51:25 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.6945
2022-02-23 12:52:01 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.6743
2022-02-23 12:52:39 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.9789
2022-02-23 12:53:17 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.6870
2022-02-23 12:53:55 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.6126
2022-02-23 12:54:32 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.7991
2022-02-23 12:55:09 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.7507
2022-02-23 12:55:47 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.6921
2022-02-23 12:56:26 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.6106
2022-02-23 12:57:03 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.7397
2022-02-23 12:57:40 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.5350
2022-02-23 12:58:17 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.6959
2022-02-23 12:58:54 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.7225
2022-02-23 12:59:32 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5997
2022-02-23 13:00:09 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.9669
2022-02-23 13:00:48 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.8758
2022-02-23 13:01:24 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.5162
2022-02-23 13:02:03 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.5686
2022-02-23 13:02:39 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.7941
2022-02-23 13:03:19 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.7053
2022-02-23 13:03:56 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.7171
2022-02-23 13:04:34 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.6814
2022-02-23 13:05:11 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.4964
2022-02-23 13:05:48 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.6024
2022-02-23 13:06:27 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.5940
2022-02-23 13:07:04 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.7644
2022-02-23 13:07:41 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.7087
2022-02-23 13:08:19 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.6837
2022-02-23 13:08:56 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.5778
2022-02-23 13:09:32 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.6488
2022-02-23 13:09:33 - train: epoch 039, train_loss: 1.6836
2022-02-23 13:10:57 - eval: epoch: 039, acc1: 64.234%, acc5: 86.022%, test_loss: 1.4693, per_image_load_time: 2.827ms, per_image_inference_time: 0.191ms
2022-02-23 13:10:58 - until epoch: 039, best_acc1: 64.904%
2022-02-23 13:10:58 - epoch 040 lr: 0.010000000000000002
2022-02-23 13:11:42 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.8775
2022-02-23 13:12:20 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.7107
2022-02-23 13:12:59 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.8186
2022-02-23 13:13:35 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.5243
2022-02-23 13:14:13 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.6930
2022-02-23 13:14:51 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.6261
2022-02-23 13:15:29 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.7252
2022-02-23 13:16:06 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.6190
2022-02-23 13:16:44 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.5448
2022-02-23 13:17:21 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.5385
2022-02-23 13:18:00 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.7154
2022-02-23 13:18:37 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.5320
2022-02-23 13:19:15 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.6642
2022-02-23 13:19:52 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.6649
2022-02-23 13:20:30 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.7522
2022-02-23 13:21:07 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.8847
2022-02-23 13:21:45 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.6588
2022-02-23 13:22:22 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.6671
2022-02-23 13:23:01 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.5811
2022-02-23 13:23:37 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.7181
2022-02-23 13:24:16 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.6352
2022-02-23 13:24:54 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.5545
2022-02-23 13:25:31 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.6905
2022-02-23 13:26:10 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.7060
2022-02-23 13:26:46 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.9059
2022-02-23 13:27:25 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.8531
2022-02-23 13:28:02 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.7844
2022-02-23 13:28:40 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.7843
2022-02-23 13:29:17 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.9824
2022-02-23 13:29:55 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.5736
2022-02-23 13:30:32 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.7836
2022-02-23 13:31:10 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.7559
2022-02-23 13:31:47 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.6448
2022-02-23 13:32:25 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.6556
2022-02-23 13:33:03 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.7785
2022-02-23 13:33:40 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.7258
2022-02-23 13:34:18 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.6777
2022-02-23 13:34:55 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.6727
2022-02-23 13:35:33 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.6044
2022-02-23 13:36:11 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.7292
2022-02-23 13:36:48 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.5299
2022-02-23 13:37:26 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.6916
2022-02-23 13:38:02 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.6343
2022-02-23 13:38:39 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.6650
2022-02-23 13:39:18 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.6012
2022-02-23 13:39:57 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.7948
2022-02-23 13:40:34 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.7780
2022-02-23 13:41:10 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.5989
2022-02-23 13:41:49 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.4983
2022-02-23 13:42:24 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.6710
2022-02-23 13:42:25 - train: epoch 040, train_loss: 1.6842
2022-02-23 13:43:51 - eval: epoch: 040, acc1: 64.790%, acc5: 86.188%, test_loss: 1.4515, per_image_load_time: 3.018ms, per_image_inference_time: 0.178ms
2022-02-23 13:43:51 - until epoch: 040, best_acc1: 64.904%
2022-02-23 13:43:51 - epoch 041 lr: 0.010000000000000002
2022-02-23 13:44:35 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.7469
2022-02-23 13:45:13 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.9119
2022-02-23 13:45:51 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.8840
2022-02-23 13:46:28 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.7209
2022-02-23 13:47:03 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.5958
2022-02-23 13:47:41 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.7906
2022-02-23 13:48:20 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.5925
2022-02-23 13:49:00 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.5345
2022-02-23 13:49:36 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.4455
2022-02-23 13:50:14 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.7694
2022-02-23 13:50:51 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.7888
2022-02-23 13:51:29 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.5002
2022-02-23 13:52:06 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.5258
2022-02-23 13:52:44 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.6616
2022-02-23 13:53:21 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.8106
2022-02-23 13:53:59 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.6914
2022-02-23 13:54:36 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.6551
2022-02-23 13:55:14 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.6002
2022-02-23 13:55:51 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.8818
2022-02-23 13:56:29 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.5818
2022-02-23 13:57:07 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.5880
2022-02-23 13:57:44 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.5526
2022-02-23 13:58:21 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.5073
2022-02-23 13:58:59 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.8953
2022-02-23 13:59:36 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.7916
2022-02-23 14:00:14 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.9465
2022-02-23 14:00:51 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.7007
2022-02-23 14:01:29 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.5970
2022-02-23 14:02:08 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7037
2022-02-23 14:02:45 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.6018
2022-02-23 14:03:23 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.7300
2022-02-23 14:04:01 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.4963
2022-02-23 14:04:38 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.6217
2022-02-23 14:05:16 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.5261
2022-02-23 14:05:54 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.5181
2022-02-23 14:06:30 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.7144
2022-02-23 14:07:07 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.6897
2022-02-23 14:07:46 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.6766
2022-02-23 14:08:22 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.5953
2022-02-23 14:09:01 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.5769
2022-02-23 14:09:37 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.9263
2022-02-23 14:10:13 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.5695
2022-02-23 14:10:51 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.5467
2022-02-23 14:11:30 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.6606
2022-02-23 14:12:08 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.6713
2022-02-23 14:12:45 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.6663
2022-02-23 14:13:24 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.7861
2022-02-23 14:14:01 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.7826
2022-02-23 14:14:39 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.7454
2022-02-23 14:15:15 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.7072
2022-02-23 14:15:16 - train: epoch 041, train_loss: 1.6824
2022-02-23 14:16:40 - eval: epoch: 041, acc1: 64.382%, acc5: 86.184%, test_loss: 1.4648, per_image_load_time: 3.086ms, per_image_inference_time: 0.190ms
2022-02-23 14:16:40 - until epoch: 041, best_acc1: 64.904%
2022-02-23 14:16:40 - epoch 042 lr: 0.010000000000000002
2022-02-23 14:17:24 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.4198
2022-02-23 14:18:02 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.5954
2022-02-23 14:18:40 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.7856
2022-02-23 14:19:19 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.7863
2022-02-23 14:19:55 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.6513
2022-02-23 14:20:33 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.7627
2022-02-23 14:21:09 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.8097
2022-02-23 14:21:49 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.7115
2022-02-23 14:22:25 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7751
2022-02-23 14:23:03 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.8860
2022-02-23 14:23:40 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.4312
2022-02-23 14:24:18 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.4902
2022-02-23 14:24:58 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.5402
2022-02-23 14:25:35 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.8851
2022-02-23 14:26:13 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.5972
2022-02-23 14:26:50 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.7739
2022-02-23 14:27:28 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.5952
2022-02-23 14:28:06 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.6310
2022-02-23 14:28:45 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.8188
2022-02-23 14:29:21 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.5317
2022-02-23 14:29:59 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.4751
2022-02-23 14:30:36 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.5822
2022-02-23 14:31:15 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.7802
2022-02-23 14:31:53 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.6945
2022-02-23 14:32:30 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.6482
2022-02-23 14:33:09 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.7454
2022-02-23 14:33:45 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.5649
2022-02-23 14:34:24 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.6082
2022-02-23 14:35:01 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.7912
2022-02-23 14:35:39 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.7049
2022-02-23 14:36:17 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.6870
2022-02-23 14:36:53 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.8596
2022-02-23 14:37:30 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.0331
2022-02-23 14:38:07 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.6205
2022-02-23 14:38:45 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.7495
2022-02-23 14:39:22 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.8351
2022-02-23 14:39:59 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.6307
2022-02-23 14:40:39 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.4132
2022-02-23 14:41:14 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.6856
2022-02-23 14:41:52 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.7045
2022-02-23 14:42:29 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.6525
2022-02-23 14:43:07 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.8500
2022-02-23 14:43:43 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.4924
2022-02-23 14:44:20 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.5880
2022-02-23 14:44:58 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.6464
2022-02-23 14:45:35 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.6777
2022-02-23 14:46:12 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.7093
2022-02-23 14:46:49 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.7313
2022-02-23 14:47:27 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.5201
2022-02-23 14:48:02 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.9283
2022-02-23 14:48:03 - train: epoch 042, train_loss: 1.6821
2022-02-23 14:49:27 - eval: epoch: 042, acc1: 64.444%, acc5: 86.050%, test_loss: 1.4685, per_image_load_time: 2.593ms, per_image_inference_time: 0.176ms
2022-02-23 14:49:27 - until epoch: 042, best_acc1: 64.904%
2022-02-23 14:49:27 - epoch 043 lr: 0.010000000000000002
2022-02-23 14:50:11 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.8897
2022-02-23 14:50:49 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.6858
2022-02-23 14:51:26 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.3202
2022-02-23 14:52:03 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.5128
2022-02-23 14:52:41 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.7684
2022-02-23 14:53:18 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.7227
2022-02-23 14:53:56 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.6258
2022-02-23 14:54:33 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.6752
2022-02-23 14:55:10 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.5426
2022-02-23 14:55:48 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.7422
2022-02-23 14:56:26 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.5989
2022-02-23 14:57:04 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.7782
2022-02-23 14:57:40 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.8208
2022-02-23 14:58:17 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.6662
2022-02-23 14:58:56 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.5195
2022-02-23 14:59:33 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.5012
2022-02-23 15:00:10 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.7814
2022-02-23 15:00:48 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.8851
2022-02-23 15:01:26 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.5686
2022-02-23 15:02:02 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.6393
2022-02-23 15:02:40 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.4539
2022-02-23 15:03:17 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.9403
2022-02-23 15:03:54 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.7965
2022-02-23 15:04:32 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.5326
2022-02-23 15:05:09 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.9572
2022-02-23 15:05:47 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.5969
2022-02-23 15:06:22 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.7179
2022-02-23 15:07:00 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.6494
2022-02-23 15:07:38 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.5643
2022-02-23 15:08:15 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.7613
2022-02-23 15:08:52 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.9259
2022-02-23 15:09:30 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.9217
2022-02-23 15:10:07 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.6434
2022-02-23 15:10:43 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.6251
2022-02-23 15:11:20 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.8666
2022-02-23 15:11:58 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.9251
2022-02-23 15:12:35 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.6770
2022-02-23 15:13:12 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.8775
2022-02-23 15:13:51 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.6498
2022-02-23 15:14:28 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.7303
2022-02-23 15:15:05 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.9626
2022-02-23 15:15:42 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.7629
2022-02-23 15:16:20 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.8337
2022-02-23 15:16:58 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.6724
2022-02-23 15:17:35 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.6750
2022-02-23 15:18:13 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.7317
2022-02-23 15:18:50 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.8787
2022-02-23 15:19:27 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.7446
2022-02-23 15:20:04 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.7398
2022-02-23 15:20:39 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.6341
2022-02-23 15:20:41 - train: epoch 043, train_loss: 1.6827
2022-02-23 15:22:06 - eval: epoch: 043, acc1: 64.270%, acc5: 86.214%, test_loss: 1.4658, per_image_load_time: 3.030ms, per_image_inference_time: 0.182ms
2022-02-23 15:22:06 - until epoch: 043, best_acc1: 64.904%
2022-02-23 15:22:06 - epoch 044 lr: 0.010000000000000002
2022-02-23 15:22:49 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.6339
2022-02-23 15:23:26 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.6121
2022-02-23 15:24:04 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.8969
2022-02-23 15:24:42 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.2655
2022-02-23 15:25:19 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.5958
2022-02-23 15:25:58 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.3453
2022-02-23 15:26:34 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.5357
2022-02-23 15:27:12 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.6990
2022-02-23 15:27:48 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.6454
2022-02-23 15:28:26 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.8277
2022-02-23 15:29:03 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.5689
2022-02-23 15:29:41 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.5262
2022-02-23 15:30:17 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.8861
2022-02-23 15:30:54 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.7791
2022-02-23 15:31:31 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.7296
2022-02-23 15:32:09 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.6717
2022-02-23 15:32:46 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.6029
2022-02-23 15:33:24 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.6317
2022-02-23 15:34:00 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.8740
2022-02-23 15:34:39 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.5364
2022-02-23 15:35:15 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.8218
2022-02-23 15:35:54 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.6747
2022-02-23 15:36:31 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.7000
2022-02-23 15:37:09 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.6910
2022-02-23 15:37:46 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.8388
2022-02-23 15:38:24 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.7958
2022-02-23 15:39:01 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.5824
2022-02-23 15:39:39 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.6217
2022-02-23 15:40:16 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.5324
2022-02-23 15:40:54 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.4988
2022-02-23 15:41:32 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.4861
2022-02-23 15:42:07 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.6329
2022-02-23 15:42:46 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.6321
2022-02-23 15:43:22 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.8964
2022-02-23 15:44:01 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.7776
2022-02-23 15:44:38 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.6654
2022-02-23 15:45:15 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.6448
2022-02-23 15:45:53 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.5086
2022-02-23 15:46:30 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.5900
2022-02-23 15:47:08 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.6903
2022-02-23 15:47:44 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.5759
2022-02-23 15:48:21 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.8342
2022-02-23 15:48:58 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.8595
2022-02-23 15:49:36 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.6794
2022-02-23 15:50:13 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.6810
2022-02-23 15:50:51 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.7991
2022-02-23 15:51:28 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.6637
2022-02-23 15:52:04 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.7810
2022-02-23 15:52:42 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.5126
2022-02-23 15:53:17 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.6740
2022-02-23 15:53:18 - train: epoch 044, train_loss: 1.6838
2022-02-23 15:54:41 - eval: epoch: 044, acc1: 64.178%, acc5: 85.974%, test_loss: 1.4733, per_image_load_time: 3.021ms, per_image_inference_time: 0.185ms
2022-02-23 15:54:41 - until epoch: 044, best_acc1: 64.904%
2022-02-23 15:54:41 - epoch 045 lr: 0.010000000000000002
2022-02-23 15:55:26 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.5074
2022-02-23 15:56:04 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.4503
2022-02-23 15:56:41 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.6521
2022-02-23 15:57:19 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.5031
2022-02-23 15:57:55 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.9184
2022-02-23 15:58:32 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.7867
2022-02-23 15:59:11 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.5707
2022-02-23 15:59:48 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.7219
2022-02-23 16:00:27 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.5229
2022-02-23 16:01:03 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.5960
2022-02-23 16:01:40 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.7901
2022-02-23 16:02:17 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.7304
2022-02-23 16:02:55 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.6669
2022-02-23 16:03:31 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.5891
2022-02-23 16:04:09 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.6796
2022-02-23 16:04:45 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.5535
2022-02-23 16:05:23 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.4011
2022-02-23 16:05:59 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.8464
2022-02-23 16:06:37 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.6012
2022-02-23 16:07:14 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.7492
2022-02-23 16:07:52 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.6313
2022-02-23 16:08:28 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.6524
2022-02-23 16:09:06 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.7179
2022-02-23 16:09:42 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.6114
2022-02-23 16:10:21 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.8566
2022-02-23 16:10:58 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.7486
2022-02-23 16:11:35 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.7191
2022-02-23 16:12:13 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.7498
2022-02-23 16:12:50 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.7375
2022-02-23 16:13:27 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.6762
2022-02-23 16:14:05 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.7766
2022-02-23 16:14:42 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.8713
2022-02-23 16:15:20 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.8138
2022-02-23 16:15:56 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.5617
2022-02-23 16:16:35 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.6740
2022-02-23 16:17:11 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.7035
2022-02-23 16:17:49 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.8021
2022-02-23 16:18:25 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.6361
2022-02-23 16:19:04 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.8044
2022-02-23 16:19:40 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.8238
2022-02-23 16:20:18 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.6974
2022-02-23 16:20:56 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.9118
2022-02-23 16:21:32 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.8325
2022-02-23 16:22:11 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.7830
2022-02-23 16:22:47 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.4797
2022-02-23 16:23:25 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.6747
2022-02-23 16:24:03 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.7489
2022-02-23 16:24:40 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.5216
2022-02-23 16:25:17 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.9478
2022-02-23 16:25:53 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.7343
2022-02-23 16:25:54 - train: epoch 045, train_loss: 1.6838
2022-02-23 16:27:18 - eval: epoch: 045, acc1: 64.200%, acc5: 86.106%, test_loss: 1.4719, per_image_load_time: 3.047ms, per_image_inference_time: 0.190ms
2022-02-23 16:27:18 - until epoch: 045, best_acc1: 64.904%
2022-02-23 16:27:18 - epoch 046 lr: 0.010000000000000002
2022-02-23 16:28:01 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.6870
2022-02-23 16:28:39 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.6699
2022-02-23 16:29:17 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.5932
2022-02-23 16:29:56 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.6531
2022-02-23 16:30:31 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.4630
2022-02-23 16:31:09 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.7727
2022-02-23 16:31:46 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.6202
2022-02-23 16:32:24 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.7810
2022-02-23 16:33:00 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.5619
2022-02-23 16:33:38 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.6435
2022-02-23 16:34:15 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.4884
2022-02-23 16:34:53 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.5396
2022-02-23 16:35:30 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.8007
2022-02-23 16:36:09 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.6395
2022-02-23 16:36:46 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.4932
2022-02-23 16:37:25 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.7709
2022-02-23 16:38:01 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.6991
2022-02-23 16:38:38 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.7784
2022-02-23 16:39:15 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.7090
2022-02-23 16:39:54 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.6096
2022-02-23 16:40:30 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.1170
2022-02-23 16:41:08 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.3956
2022-02-23 16:41:44 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.9042
2022-02-23 16:42:22 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.5196
2022-02-23 16:42:59 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.6830
2022-02-23 16:43:39 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.9211
2022-02-23 16:44:16 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.6161
2022-02-23 16:44:53 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.6606
2022-02-23 16:45:31 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.6484
2022-02-23 16:46:09 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.4413
2022-02-23 16:46:47 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.5416
2022-02-23 16:47:25 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.8424
2022-02-23 16:48:02 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.6841
2022-02-23 16:48:39 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.7364
2022-02-23 16:49:16 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.8744
2022-02-23 16:49:53 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.6934
2022-02-23 16:50:31 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.5437
2022-02-23 16:51:08 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.5787
2022-02-23 16:51:45 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.6625
2022-02-23 16:52:21 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.6239
2022-02-23 16:52:57 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.8588
2022-02-23 16:53:35 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.6700
2022-02-23 16:54:12 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.7023
2022-02-23 16:54:49 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.8373
2022-02-23 16:55:27 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.6757
2022-02-23 16:56:04 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.6923
2022-02-23 16:56:42 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.6266
2022-02-23 16:57:19 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.5115
2022-02-23 16:57:56 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.8317
2022-02-23 16:58:32 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.7512
2022-02-23 16:58:33 - train: epoch 046, train_loss: 1.6852
2022-02-23 16:59:57 - eval: epoch: 046, acc1: 64.124%, acc5: 85.938%, test_loss: 1.4749, per_image_load_time: 2.831ms, per_image_inference_time: 0.181ms
2022-02-23 16:59:57 - until epoch: 046, best_acc1: 64.904%
2022-02-23 16:59:57 - epoch 047 lr: 0.010000000000000002
2022-02-23 17:00:42 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.5781
2022-02-23 17:01:21 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.7158
2022-02-23 17:02:00 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.4750
2022-02-23 17:02:37 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.4698
2022-02-23 17:03:15 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.5328
2022-02-23 17:03:51 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.7003
2022-02-23 17:04:27 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.7211
2022-02-23 17:05:04 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.7295
2022-02-23 17:05:42 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.7340
2022-02-23 17:06:18 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.6840
2022-02-23 17:06:56 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.9174
2022-02-23 17:07:33 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.6042
2022-02-23 17:08:11 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.8125
2022-02-23 17:08:49 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.6894
2022-02-23 17:09:26 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.6953
2022-02-23 17:10:04 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.6747
2022-02-23 17:10:41 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.5072
2022-02-23 17:11:17 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.7554
2022-02-23 17:11:55 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.3976
2022-02-23 17:12:32 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.8058
2022-02-23 17:13:09 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.0399
2022-02-23 17:13:47 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.7655
2022-02-23 17:14:23 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.7454
2022-02-23 17:15:01 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.5008
2022-02-23 17:15:38 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.6524
2022-02-23 17:16:17 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.0943
2022-02-23 17:16:53 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.3690
2022-02-23 17:17:31 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.7155
2022-02-23 17:18:08 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.6253
2022-02-23 17:18:45 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.7539
2022-02-23 17:19:23 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.6145
2022-02-23 17:20:00 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.7719
2022-02-23 17:20:38 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.6321
2022-02-23 17:21:15 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.3911
2022-02-23 17:21:51 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.9558
2022-02-23 17:22:30 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.8176
2022-02-23 17:23:07 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.6778
2022-02-23 17:23:44 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.5053
2022-02-23 17:24:22 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.7188
2022-02-23 17:24:59 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.7263
2022-02-23 17:25:36 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.7325
2022-02-23 17:26:14 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.5636
2022-02-23 17:26:52 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.5573
2022-02-23 17:27:30 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.7275
2022-02-23 17:28:08 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.8319
2022-02-23 17:28:44 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.5999
2022-02-23 17:29:22 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.7582
2022-02-23 17:30:00 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.6276
2022-02-23 17:30:37 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.6409
2022-02-23 17:31:14 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.6731
2022-02-23 17:31:14 - train: epoch 047, train_loss: 1.6851
2022-02-23 17:32:40 - eval: epoch: 047, acc1: 64.092%, acc5: 85.928%, test_loss: 1.4790, per_image_load_time: 3.066ms, per_image_inference_time: 0.173ms
2022-02-23 17:32:40 - until epoch: 047, best_acc1: 64.904%
2022-02-23 17:32:40 - epoch 048 lr: 0.010000000000000002
2022-02-23 17:33:23 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.7393
2022-02-23 17:34:02 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.9548
2022-02-23 17:34:39 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.6774
2022-02-23 17:35:15 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.5812
2022-02-23 17:35:53 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.7067
2022-02-23 17:36:30 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.7232
2022-02-23 17:37:08 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.6028
2022-02-23 17:37:46 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.6388
2022-02-23 17:38:23 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.6643
2022-02-23 17:39:01 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.8059
2022-02-23 17:39:38 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.8550
2022-02-23 17:40:16 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.5159
2022-02-23 17:40:53 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.4827
2022-02-23 17:41:31 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.4739
2022-02-23 17:42:08 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.7418
2022-02-23 17:42:45 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.6907
2022-02-23 17:43:23 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.5349
2022-02-23 17:44:01 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.7342
2022-02-23 17:44:38 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.6679
2022-02-23 17:45:15 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.6033
2022-02-23 17:45:53 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.8811
2022-02-23 17:46:31 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.8820
2022-02-23 17:47:07 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.5653
2022-02-23 17:47:44 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.6136
2022-02-23 17:48:21 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.7901
2022-02-23 17:48:59 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.5274
2022-02-23 17:49:36 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.8684
2022-02-23 17:50:14 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.6462
2022-02-23 17:50:50 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.6529
2022-02-23 17:51:28 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.5975
2022-02-23 17:52:05 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.8126
2022-02-23 17:52:43 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.5336
2022-02-23 17:53:19 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.7092
2022-02-23 17:53:56 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.5162
2022-02-23 17:54:34 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.9691
2022-02-23 17:55:12 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.7043
2022-02-23 17:55:49 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.6936
2022-02-23 17:56:27 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.7252
2022-02-23 17:57:04 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.8300
2022-02-23 17:57:43 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.5890
2022-02-23 17:58:20 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.9028
2022-02-23 17:59:01 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.5472
2022-02-23 17:59:41 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.6381
2022-02-23 18:00:22 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.5440
2022-02-23 18:01:03 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.7101
2022-02-23 18:01:41 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.7602
2022-02-23 18:02:20 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.7570
2022-02-23 18:03:00 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.8980
2022-02-23 18:03:37 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.8171
2022-02-23 18:04:15 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.7326
2022-02-23 18:04:16 - train: epoch 048, train_loss: 1.6844
2022-02-23 18:05:43 - eval: epoch: 048, acc1: 64.294%, acc5: 86.102%, test_loss: 1.4727, per_image_load_time: 3.183ms, per_image_inference_time: 0.185ms
2022-02-23 18:05:43 - until epoch: 048, best_acc1: 64.904%
2022-02-23 18:05:43 - epoch 049 lr: 0.010000000000000002
2022-02-23 18:06:29 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.7374
2022-02-23 18:07:07 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.6442
2022-02-23 18:07:45 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.7936
2022-02-23 18:08:25 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.7019
2022-02-23 18:09:02 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.5130
2022-02-23 18:09:41 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.4276
2022-02-23 18:10:20 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.5728
2022-02-23 18:10:58 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.7263
2022-02-23 18:11:35 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.4588
2022-02-23 18:12:16 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.6226
2022-02-23 18:12:53 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.6333
2022-02-23 18:13:41 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.5646
2022-02-23 18:14:19 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.8643
2022-02-23 18:14:59 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.8450
2022-02-23 18:15:40 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.7126
2022-02-23 18:16:23 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.8781
2022-02-23 18:17:04 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.8269
2022-02-23 18:17:49 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.6550
2022-02-23 18:18:27 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.7248
2022-02-23 18:19:09 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.6800
2022-02-23 18:19:49 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.5566
2022-02-23 18:20:33 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.6050
2022-02-23 18:21:14 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.7928
2022-02-23 18:21:57 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.7028
2022-02-23 18:22:37 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.9380
2022-02-23 18:23:20 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.8837
2022-02-23 18:24:06 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.5852
2022-02-23 18:24:49 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.7172
2022-02-23 18:25:34 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.6905
2022-02-23 18:26:18 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.6987
2022-02-23 18:27:03 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.7587
2022-02-23 18:27:46 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.7100
2022-02-23 18:28:28 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.7809
2022-02-23 18:29:12 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.7026
2022-02-23 18:29:51 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.8865
2022-02-23 18:30:34 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.6986
2022-02-23 18:31:14 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.5054
2022-02-23 18:31:56 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.6656
2022-02-23 18:32:36 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.9678
2022-02-23 18:33:21 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.5575
2022-02-23 18:34:00 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.6710
2022-02-23 18:34:39 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.6067
2022-02-23 18:35:18 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.8624
2022-02-23 18:35:55 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.8370
2022-02-23 18:36:34 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.6514
2022-02-23 18:37:16 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.8091
2022-02-23 18:37:58 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.0008
2022-02-23 18:38:45 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.6417
2022-02-23 18:39:22 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.7349
2022-02-23 18:39:58 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.5414
2022-02-23 18:39:59 - train: epoch 049, train_loss: 1.6800
2022-02-23 18:41:27 - eval: epoch: 049, acc1: 64.554%, acc5: 86.178%, test_loss: 1.4676, per_image_load_time: 3.195ms, per_image_inference_time: 0.179ms
2022-02-23 18:41:27 - until epoch: 049, best_acc1: 64.904%
2022-02-23 18:41:27 - epoch 050 lr: 0.010000000000000002
2022-02-23 18:42:12 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.6563
2022-02-23 18:42:50 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.7581
2022-02-23 18:43:31 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.5801
2022-02-23 18:44:09 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.7148
2022-02-23 18:44:49 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.4771
2022-02-23 18:45:31 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.8771
2022-02-23 18:46:09 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.4590
2022-02-23 18:46:48 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.4847
2022-02-23 18:47:27 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.6239
2022-02-23 18:48:07 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.7229
2022-02-23 18:48:47 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.6729
2022-02-23 18:49:26 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.5123
2022-02-23 18:50:09 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.6677
2022-02-23 18:50:48 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.7427
2022-02-23 18:51:40 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.6772
2022-02-23 18:52:42 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.7097
2022-02-23 18:53:40 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.6471
2022-02-23 18:54:39 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.9176
2022-02-23 18:55:56 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.5908
2022-02-23 18:56:58 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.5543
2022-02-23 18:58:04 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.5997
2022-02-23 18:59:03 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.7762
2022-02-23 19:00:17 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.4336
2022-02-23 19:01:12 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.7798
2022-02-23 19:02:11 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.6391
2022-02-23 19:03:01 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.4246
2022-02-23 19:03:56 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.5638
2022-02-23 19:04:54 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.7709
2022-02-23 19:05:48 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.8819
2022-02-23 19:06:49 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.6960
2022-02-23 19:07:41 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.6773
2022-02-23 19:08:27 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.7671
2022-02-23 19:09:24 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.6215
2022-02-23 19:10:19 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.7970
2022-02-23 19:10:58 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.7185
2022-02-23 19:11:40 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.6754
2022-02-23 19:12:18 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.8447
2022-02-23 19:13:00 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.5827
2022-02-23 19:13:43 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.5797
2022-02-23 19:14:41 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.7670
2022-02-23 19:16:00 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.8164
2022-02-23 19:16:58 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.8008
2022-02-23 19:18:10 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.7754
2022-02-23 19:19:17 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.6639
2022-02-23 19:20:24 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.6878
2022-02-23 19:21:26 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.6947
2022-02-23 19:22:34 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.7582
2022-02-23 19:23:42 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.5529
2022-02-23 19:24:44 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.5365
2022-02-23 19:26:10 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.7242
2022-02-23 19:26:11 - train: epoch 050, train_loss: 1.6824
2022-02-23 19:28:14 - eval: epoch: 050, acc1: 64.094%, acc5: 85.982%, test_loss: 1.4779, per_image_load_time: 1.760ms, per_image_inference_time: 0.140ms
2022-02-23 19:28:14 - until epoch: 050, best_acc1: 64.904%
2022-02-23 19:28:14 - epoch 051 lr: 0.010000000000000002
2022-02-23 19:29:08 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.8344
2022-02-23 19:29:53 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.7157
2022-02-23 19:30:45 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.7060
2022-02-23 19:31:43 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.5577
2022-02-23 19:32:40 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.5247
2022-02-23 19:33:34 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.5441
2022-02-23 19:34:34 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.6735
2022-02-23 19:35:33 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.9819
2022-02-23 19:36:54 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.5549
2022-02-23 19:38:00 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.7142
2022-02-23 19:39:15 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.7805
2022-02-23 19:40:11 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.6033
2022-02-23 19:41:11 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.5421
2022-02-23 19:42:22 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.5787
2022-02-23 19:43:03 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.6508
2022-02-23 19:43:39 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.5815
2022-02-23 19:44:16 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.6383
2022-02-23 19:44:55 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.8131
2022-02-23 19:45:32 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.6218
2022-02-23 19:46:09 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.6240
2022-02-23 19:46:48 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.6211
2022-02-23 19:47:24 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.6595
2022-02-23 19:48:02 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.8057
2022-02-23 19:48:39 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.5699
2022-02-23 19:49:17 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.7243
2022-02-23 19:49:56 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.7266
2022-02-23 19:50:33 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.7502
2022-02-23 19:51:11 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.5102
2022-02-23 19:51:49 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.7497
2022-02-23 19:52:26 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.5789
2022-02-23 19:53:03 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.5498
2022-02-23 19:53:41 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.6638
2022-02-23 19:54:19 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.7240
2022-02-23 19:54:57 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.8142
2022-02-23 19:55:33 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.6197
2022-02-23 19:56:10 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.7525
2022-02-23 19:56:47 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.8713
2022-02-23 19:57:25 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.7523
2022-02-23 19:58:01 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.6976
2022-02-23 19:58:39 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.7214
2022-02-23 19:59:15 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.6376
2022-02-23 19:59:52 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.7373
2022-02-23 20:00:29 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.6110
2022-02-23 20:01:07 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.9954
2022-02-23 20:01:43 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.4922
2022-02-23 20:02:21 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.7783
2022-02-23 20:02:57 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.8492
2022-02-23 20:03:35 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.8122
2022-02-23 20:04:11 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.7601
2022-02-23 20:04:47 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.6276
2022-02-23 20:04:48 - train: epoch 051, train_loss: 1.6832
2022-02-23 20:06:11 - eval: epoch: 051, acc1: 64.162%, acc5: 86.078%, test_loss: 1.4668, per_image_load_time: 2.421ms, per_image_inference_time: 0.175ms
2022-02-23 20:06:11 - until epoch: 051, best_acc1: 64.904%
2022-02-23 20:06:11 - epoch 052 lr: 0.010000000000000002
2022-02-23 20:06:54 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.5761
2022-02-23 20:07:32 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.6702
2022-02-23 20:08:09 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.5774
2022-02-23 20:08:45 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.6016
2022-02-23 20:09:24 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.6196
2022-02-23 20:10:00 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.7693
2022-02-23 20:10:38 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.6687
2022-02-23 20:11:13 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.4948
2022-02-23 20:11:50 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.7510
2022-02-23 20:12:28 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.9181
2022-02-23 20:13:07 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.6292
2022-02-23 20:13:45 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.5692
2022-02-23 20:14:23 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.4844
2022-02-23 20:15:00 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.0517
2022-02-23 20:15:38 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.5159
2022-02-23 20:16:15 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.5050
2022-02-23 20:16:52 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.4662
2022-02-23 20:17:29 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.7223
2022-02-23 20:18:08 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.5733
2022-02-23 20:18:48 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.6805
2022-02-23 20:19:24 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.8200
2022-02-23 20:20:03 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.7643
2022-02-23 20:20:41 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.6917
2022-02-23 20:21:19 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.4783
2022-02-23 20:21:57 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.5373
2022-02-23 20:22:35 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.6302
2022-02-23 20:23:13 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.6549
2022-02-23 20:23:51 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.6212
2022-02-23 20:24:31 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.5080
2022-02-23 20:25:09 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.4908
2022-02-23 20:25:47 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.7431
2022-02-23 20:26:26 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.7034
2022-02-23 20:27:03 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.6001
2022-02-23 20:27:41 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.7863
2022-02-23 20:28:19 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.7529
2022-02-23 20:28:57 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.9150
2022-02-23 20:29:34 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.7383
2022-02-23 20:30:12 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.5830
2022-02-23 20:30:49 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.6585
2022-02-23 20:31:27 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.8690
2022-02-23 20:32:05 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.6593
2022-02-23 20:32:42 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.9025
2022-02-23 20:33:20 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.5696
2022-02-23 20:33:56 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.6354
2022-02-23 20:34:35 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.5840
2022-02-23 20:35:11 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.8015
2022-02-23 20:35:49 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.7483
2022-02-23 20:36:26 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.5554
2022-02-23 20:37:03 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.7217
2022-02-23 20:37:39 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.6350
2022-02-23 20:37:40 - train: epoch 052, train_loss: 1.6796
2022-02-23 20:39:04 - eval: epoch: 052, acc1: 63.872%, acc5: 85.752%, test_loss: 1.4976, per_image_load_time: 2.967ms, per_image_inference_time: 0.169ms
2022-02-23 20:39:05 - until epoch: 052, best_acc1: 64.904%
2022-02-23 20:39:05 - epoch 053 lr: 0.010000000000000002
2022-02-23 20:39:48 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.7912
2022-02-23 20:40:26 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.8345
2022-02-23 20:41:03 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.8165
2022-02-23 20:41:40 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.7131
2022-02-23 20:42:18 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.4698
2022-02-23 20:42:56 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.7080
2022-02-23 20:43:32 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.5767
2022-02-23 20:44:11 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.7174
2022-02-23 20:44:49 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.7538
2022-02-23 20:45:27 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.7701
2022-02-23 20:46:05 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.5200
2022-02-23 20:46:43 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.4692
2022-02-23 20:47:19 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.6305
2022-02-23 20:47:58 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.8262
2022-02-23 20:48:35 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.4598
2022-02-23 20:49:13 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.9192
2022-02-23 20:49:51 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.8223
2022-02-23 20:50:27 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.5976
2022-02-23 20:51:06 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.5862
2022-02-23 20:51:43 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.7237
2022-02-23 20:52:20 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.7817
2022-02-23 20:52:58 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.6680
2022-02-23 20:53:37 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.7444
2022-02-23 20:54:13 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.6113
2022-02-23 20:54:53 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.8172
2022-02-23 20:55:29 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.9293
2022-02-23 20:56:07 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.8371
2022-02-23 20:56:45 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.7227
2022-02-23 20:57:23 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.6833
2022-02-23 20:58:00 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.3582
2022-02-23 20:58:37 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.7651
2022-02-23 20:59:16 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.7601
2022-02-23 20:59:54 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.7896
2022-02-23 21:00:31 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.6392
2022-02-23 21:01:08 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.7370
2022-02-23 21:01:45 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.7935
2022-02-23 21:02:23 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.8363
2022-02-23 21:03:02 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.6234
2022-02-23 21:03:38 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.8221
2022-02-23 21:04:16 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.7092
2022-02-23 21:04:53 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.7543
2022-02-23 21:05:32 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.6997
2022-02-23 21:06:09 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.0266
2022-02-23 21:06:47 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.7097
2022-02-23 21:07:24 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.9637
2022-02-23 21:08:02 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.5233
2022-02-23 21:08:38 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.5306
2022-02-23 21:09:17 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.8726
2022-02-23 21:09:53 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.6892
2022-02-23 21:10:30 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.5530
2022-02-23 21:10:31 - train: epoch 053, train_loss: 1.6780
2022-02-23 21:11:55 - eval: epoch: 053, acc1: 64.766%, acc5: 86.358%, test_loss: 1.4520, per_image_load_time: 2.990ms, per_image_inference_time: 0.191ms
2022-02-23 21:11:55 - until epoch: 053, best_acc1: 64.904%
2022-02-23 21:11:55 - epoch 054 lr: 0.010000000000000002
2022-02-23 21:12:39 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.4766
2022-02-23 21:13:16 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.7722
2022-02-23 21:13:53 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.6965
2022-02-23 21:14:31 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.6479
2022-02-23 21:15:09 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.5894
2022-02-23 21:15:47 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.6343
2022-02-23 21:16:23 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.8585
2022-02-23 21:17:01 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.8063
2022-02-23 21:17:40 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.4418
2022-02-23 21:18:16 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.4916
2022-02-23 21:18:55 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.5195
2022-02-23 21:19:31 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.6338
2022-02-23 21:20:09 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.6506
2022-02-23 21:20:46 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.6112
2022-02-23 21:21:24 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.5931
2022-02-23 21:22:02 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.4397
2022-02-23 21:22:39 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.6957
2022-02-23 21:23:17 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.5651
2022-02-23 21:23:54 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.8302
2022-02-23 21:24:31 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.7503
2022-02-23 21:25:09 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.5218
2022-02-23 21:25:45 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.6788
2022-02-23 21:26:23 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.5848
2022-02-23 21:27:02 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.6222
2022-02-23 21:27:38 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.9298
2022-02-23 21:28:15 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.6421
2022-02-23 21:28:54 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.5462
2022-02-23 21:29:30 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.8259
2022-02-23 21:30:08 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.4581
2022-02-23 21:30:44 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.7494
2022-02-23 21:31:22 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.7530
2022-02-23 21:31:59 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.7707
2022-02-23 21:32:37 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.6719
2022-02-23 21:33:14 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.5430
2022-02-23 21:33:51 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.6837
2022-02-23 21:34:29 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.7685
2022-02-23 21:35:07 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.6360
2022-02-23 21:35:45 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.5671
2022-02-23 21:36:22 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.7494
2022-02-23 21:37:00 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.5540
2022-02-23 21:37:37 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.7707
2022-02-23 21:38:17 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.7592
2022-02-23 21:38:53 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.8569
2022-02-23 21:39:32 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.4243
2022-02-23 21:40:09 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.4220
2022-02-23 21:40:47 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.7190
2022-02-23 21:41:24 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.8589
2022-02-23 21:42:01 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.7814
2022-02-23 21:42:38 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.5678
2022-02-23 21:43:16 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.6912
2022-02-23 21:43:17 - train: epoch 054, train_loss: 1.6763
2022-02-23 21:44:43 - eval: epoch: 054, acc1: 64.180%, acc5: 85.978%, test_loss: 1.4720, per_image_load_time: 2.480ms, per_image_inference_time: 0.140ms
2022-02-23 21:44:44 - until epoch: 054, best_acc1: 64.904%
2022-02-23 21:44:44 - epoch 055 lr: 0.010000000000000002
2022-02-23 21:45:28 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.6274
2022-02-23 21:46:05 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.7196
2022-02-23 21:46:43 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.3420
2022-02-23 21:47:23 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.5930
2022-02-23 21:47:58 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.4050
2022-02-23 21:48:35 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.7396
2022-02-23 21:49:12 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.5450
2022-02-23 21:49:49 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.6096
2022-02-23 21:50:26 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.8144
2022-02-23 21:51:04 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.6108
2022-02-23 21:51:41 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.5878
2022-02-23 21:52:18 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.7599
2022-02-23 21:52:56 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.7381
2022-02-23 21:53:33 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.7789
2022-02-23 21:54:11 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.6144
2022-02-23 21:54:48 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.6600
2022-02-23 21:55:26 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.6251
2022-02-23 21:56:04 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.6107
2022-02-23 21:56:40 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.7248
2022-02-23 21:57:17 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.6878
2022-02-23 21:57:54 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.4212
2022-02-23 21:58:35 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.8448
2022-02-23 21:59:13 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.5056
2022-02-23 21:59:53 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.4832
2022-02-23 22:00:31 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.4713
2022-02-23 22:01:06 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.6274
2022-02-23 22:01:44 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.6767
2022-02-23 22:02:21 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.6656
2022-02-23 22:03:00 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.6422
2022-02-23 22:03:37 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.8614
2022-02-23 22:04:13 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.7943
2022-02-23 22:04:51 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.6015
2022-02-23 22:05:28 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.4043
2022-02-23 22:06:05 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.4990
2022-02-23 22:06:42 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.4867
2022-02-23 22:07:20 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.6335
2022-02-23 22:07:57 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.6249
2022-02-23 22:08:35 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.7700
2022-02-23 22:09:11 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.9420
2022-02-23 22:09:50 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.6095
2022-02-23 22:10:27 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.6371
2022-02-23 22:11:08 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.8249
2022-02-23 22:11:47 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.6486
2022-02-23 22:12:23 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.7928
2022-02-23 22:13:02 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.6261
2022-02-23 22:13:39 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.6726
2022-02-23 22:14:17 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.6096
2022-02-23 22:14:54 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.6344
2022-02-23 22:15:33 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.6415
2022-02-23 22:16:08 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.7938
2022-02-23 22:16:09 - train: epoch 055, train_loss: 1.6729
2022-02-23 22:17:34 - eval: epoch: 055, acc1: 64.498%, acc5: 86.094%, test_loss: 1.4587, per_image_load_time: 1.412ms, per_image_inference_time: 0.174ms
2022-02-23 22:17:35 - until epoch: 055, best_acc1: 64.904%
2022-02-23 22:17:35 - epoch 056 lr: 0.010000000000000002
2022-02-23 22:18:18 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.6642
2022-02-23 22:18:57 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.6343
2022-02-23 22:19:34 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.5272
2022-02-23 22:20:11 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.6137
2022-02-23 22:20:49 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.6111
2022-02-23 22:21:26 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.7435
2022-02-23 22:22:03 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.6588
2022-02-23 22:22:37 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.8397
2022-02-23 22:23:15 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.7275
2022-02-23 22:23:52 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.6762
2022-02-23 22:24:31 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.6231
2022-02-23 22:25:08 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.6012
2022-02-23 22:25:46 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.6988
2022-02-23 22:26:22 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.5385
2022-02-23 22:26:59 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.8651
2022-02-23 22:27:37 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.7619
2022-02-23 22:28:18 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.8876
2022-02-23 22:28:55 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.7210
2022-02-23 22:29:32 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.8937
2022-02-23 22:30:10 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.6924
2022-02-23 22:30:47 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.8563
2022-02-23 22:31:25 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.6847
2022-02-23 22:32:07 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.7963
2022-02-23 22:32:46 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.7477
2022-02-23 22:33:22 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.7497
2022-02-23 22:34:01 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.6227
2022-02-23 22:34:38 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.4044
2022-02-23 22:35:16 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.6716
2022-02-23 22:35:54 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.9621
2022-02-23 22:36:33 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.7257
2022-02-23 22:37:12 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.3857
2022-02-23 22:37:53 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.4723
2022-02-23 22:38:30 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.7922
2022-02-23 22:39:10 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.7577
2022-02-23 22:39:46 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.5083
