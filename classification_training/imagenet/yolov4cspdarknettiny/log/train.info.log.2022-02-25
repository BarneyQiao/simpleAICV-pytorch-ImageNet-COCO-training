2022-02-25 08:59:12 - network: yolov4cspdarknettiny
2022-02-25 08:59:12 - num_classes: 1000
2022-02-25 08:59:12 - input_image_size: 256
2022-02-25 08:59:12 - scale: 1.1428571428571428
2022-02-25 08:59:12 - trained_model_path: 
2022-02-25 08:59:12 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-25 08:59:12 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f04b4cf3e20>
2022-02-25 08:59:12 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f049c4f2130>
2022-02-25 08:59:12 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f049c4f2160>
2022-02-25 08:59:12 - seed: 0
2022-02-25 08:59:12 - batch_size: 256
2022-02-25 08:59:12 - num_workers: 16
2022-02-25 08:59:12 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-25 08:59:12 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-25 08:59:12 - epochs: 100
2022-02-25 08:59:12 - print_interval: 100
2022-02-25 08:59:12 - distributed: True
2022-02-25 08:59:12 - sync_bn: False
2022-02-25 08:59:12 - apex: True
2022-02-25 08:59:12 - gpus_type: NVIDIA RTX A5000
2022-02-25 08:59:12 - gpus_num: 2
2022-02-25 08:59:12 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f0497138530>
2022-02-25 08:59:17 - --------------------parameters--------------------
2022-02-25 08:59:17 - name: conv1.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: conv1.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: conv1.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: conv2.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: conv2.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: conv2.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block1.conv1.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv1.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv1.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block1.conv2.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv2.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv2.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block1.conv3.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv3.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv3.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block1.conv4.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv4.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block1.conv4.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block2.conv1.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv1.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv1.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block2.conv2.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv2.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv2.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block2.conv3.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv3.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv3.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block2.conv4.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv4.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block2.conv4.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block3.conv1.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv1.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv1.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block3.conv2.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv2.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv2.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block3.conv3.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv3.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv3.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: block3.conv4.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv4.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: block3.conv4.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: conv3.layer.0.weight, grad: True
2022-02-25 08:59:17 - name: conv3.layer.1.weight, grad: True
2022-02-25 08:59:17 - name: conv3.layer.1.bias, grad: True
2022-02-25 08:59:17 - name: fc.weight, grad: True
2022-02-25 08:59:17 - name: fc.bias, grad: True
2022-02-25 08:59:17 - --------------------buffers--------------------
2022-02-25 08:59:17 - name: conv1.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: conv1.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: conv2.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: conv2.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: conv2.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block1.conv1.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block1.conv1.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block1.conv2.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block1.conv2.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block1.conv3.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block1.conv3.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block1.conv4.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block1.conv4.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block1.conv4.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block2.conv1.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block2.conv1.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block2.conv2.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block2.conv2.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block2.conv3.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block2.conv3.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block2.conv4.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block2.conv4.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block2.conv4.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block3.conv1.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block3.conv1.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block3.conv2.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block3.conv2.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block3.conv3.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block3.conv3.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: block3.conv4.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: block3.conv4.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: block3.conv4.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - name: conv3.layer.1.running_mean, grad: False
2022-02-25 08:59:17 - name: conv3.layer.1.running_var, grad: False
2022-02-25 08:59:17 - name: conv3.layer.1.num_batches_tracked, grad: False
2022-02-25 08:59:17 - epoch 001 lr: 0.1
2022-02-25 08:59:58 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.6600
2022-02-25 09:00:33 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.3226
2022-02-25 09:01:10 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.1937
2022-02-25 09:01:45 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 5.9347
2022-02-25 09:02:22 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 5.8471
2022-02-25 09:02:57 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 5.5526
2022-02-25 09:03:32 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 5.6206
2022-02-25 09:04:09 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.5612
2022-02-25 09:04:43 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.4782
2022-02-25 09:05:20 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.3589
2022-02-25 09:05:55 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.3877
2022-02-25 09:06:31 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.1094
2022-02-25 09:07:06 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.0474
2022-02-25 09:07:42 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.1380
2022-02-25 09:08:18 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 4.9331
2022-02-25 09:08:53 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.0759
2022-02-25 09:09:28 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 4.8405
2022-02-25 09:10:05 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 4.9539
2022-02-25 09:10:39 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 4.7149
2022-02-25 09:11:16 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.6652
2022-02-25 09:11:50 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.6083
2022-02-25 09:12:26 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.7534
2022-02-25 09:13:00 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.5516
2022-02-25 09:13:35 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.5764
2022-02-25 09:14:11 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.7001
2022-02-25 09:14:45 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.6222
2022-02-25 09:15:20 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.7065
2022-02-25 09:15:56 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.4039
2022-02-25 09:16:29 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.2792
2022-02-25 09:17:06 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.4552
2022-02-25 09:17:41 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.5375
2022-02-25 09:18:16 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.3371
2022-02-25 09:18:52 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.1988
2022-02-25 09:19:26 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.2396
2022-02-25 09:20:02 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.2253
2022-02-25 09:20:36 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.2312
2022-02-25 09:21:12 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.4904
2022-02-25 09:21:46 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.1174
2022-02-25 09:22:21 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.2042
2022-02-25 09:22:56 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.1898
2022-02-25 09:23:33 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.2305
2022-02-25 09:24:09 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.0328
2022-02-25 09:24:48 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.0756
2022-02-25 09:25:24 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 3.7924
2022-02-25 09:26:07 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.0358
2022-02-25 09:26:48 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.2609
2022-02-25 09:27:27 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.0719
2022-02-25 09:28:10 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.3021
2022-02-25 09:28:58 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 3.9040
2022-02-25 09:29:47 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.8281
2022-02-25 09:29:48 - train: epoch 001, train_loss: 4.7418
2022-02-25 09:31:30 - eval: epoch: 001, acc1: 22.546%, acc5: 46.220%, test_loss: 3.8346, per_image_load_time: 3.855ms, per_image_inference_time: 0.125ms
2022-02-25 09:31:30 - until epoch: 001, best_acc1: 22.546%
2022-02-25 09:31:30 - epoch 002 lr: 0.1
2022-02-25 09:32:11 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.0257
2022-02-25 09:32:46 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.7904
2022-02-25 09:33:21 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 3.9864
2022-02-25 09:33:55 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.9266
2022-02-25 09:34:31 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.6797
2022-02-25 09:35:05 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.7372
2022-02-25 09:35:42 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.9130
2022-02-25 09:36:16 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.7282
2022-02-25 09:36:51 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.5533
2022-02-25 09:37:26 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.9690
2022-02-25 09:38:01 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.9105
2022-02-25 09:38:36 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.7211
2022-02-25 09:39:12 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.6470
2022-02-25 09:39:47 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.8099
2022-02-25 09:40:23 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.7420
2022-02-25 09:41:00 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.7377
2022-02-25 09:41:34 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.8290
2022-02-25 09:42:11 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.7178
2022-02-25 09:42:45 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.6196
2022-02-25 09:43:22 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.4057
2022-02-25 09:43:56 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.6470
2022-02-25 09:44:33 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.5308
2022-02-25 09:45:08 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.6712
2022-02-25 09:45:43 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.4236
2022-02-25 09:46:19 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.6047
2022-02-25 09:46:54 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.4584
2022-02-25 09:47:30 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.7016
2022-02-25 09:48:05 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.6286
2022-02-25 09:48:41 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6277
2022-02-25 09:49:15 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.4867
2022-02-25 09:49:50 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.5030
2022-02-25 09:50:25 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.5018
2022-02-25 09:51:01 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.4976
2022-02-25 09:51:35 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.7190
2022-02-25 09:52:10 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.4927
2022-02-25 09:52:50 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.5833
2022-02-25 09:53:29 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.5696
2022-02-25 09:54:08 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.3011
2022-02-25 09:54:50 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.4492
2022-02-25 09:55:26 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.3515
2022-02-25 09:56:01 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.5706
2022-02-25 09:56:38 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.4061
2022-02-25 09:57:16 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.4099
2022-02-25 09:57:51 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.2671
2022-02-25 09:58:32 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.2447
2022-02-25 09:59:15 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.3895
2022-02-25 09:59:56 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.4125
2022-02-25 10:00:36 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4444
2022-02-25 10:01:23 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.3642
2022-02-25 10:02:12 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.2775
2022-02-25 10:02:13 - train: epoch 002, train_loss: 3.6087
2022-02-25 10:03:52 - eval: epoch: 002, acc1: 30.936%, acc5: 56.738%, test_loss: 3.2815, per_image_load_time: 3.801ms, per_image_inference_time: 0.109ms
2022-02-25 10:03:53 - until epoch: 002, best_acc1: 30.936%
2022-02-25 10:03:53 - epoch 003 lr: 0.1
2022-02-25 10:04:35 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.4214
2022-02-25 10:05:10 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.3771
2022-02-25 10:05:47 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.2817
2022-02-25 10:06:21 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4007
2022-02-25 10:06:56 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3636
2022-02-25 10:07:33 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.2442
2022-02-25 10:08:08 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.5313
2022-02-25 10:08:43 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.5300
2022-02-25 10:09:19 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.2358
2022-02-25 10:09:55 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.4524
2022-02-25 10:10:29 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.2624
2022-02-25 10:11:05 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.3279
2022-02-25 10:11:40 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.2558
2022-02-25 10:12:14 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.1570
2022-02-25 10:12:49 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5581
2022-02-25 10:13:24 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.1752
2022-02-25 10:13:59 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.1355
2022-02-25 10:14:34 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1105
2022-02-25 10:15:08 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.2405
2022-02-25 10:15:44 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.6167
2022-02-25 10:16:18 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4588
2022-02-25 10:16:55 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.6285
2022-02-25 10:17:30 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.2681
2022-02-25 10:18:06 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.2019
2022-02-25 10:18:41 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.2760
2022-02-25 10:19:17 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2569
2022-02-25 10:19:51 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.4996
2022-02-25 10:20:27 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0313
2022-02-25 10:21:05 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.1567
2022-02-25 10:21:40 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.4174
2022-02-25 10:22:16 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.3876
2022-02-25 10:22:50 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2568
2022-02-25 10:23:27 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.2994
2022-02-25 10:24:01 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.4636
2022-02-25 10:24:37 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.9786
2022-02-25 10:25:12 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.2052
2022-02-25 10:25:46 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.2969
2022-02-25 10:26:23 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.3811
2022-02-25 10:26:56 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.3476
2022-02-25 10:27:33 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2799
2022-02-25 10:28:08 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.1989
2022-02-25 10:28:44 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.1221
2022-02-25 10:29:20 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.8331
2022-02-25 10:29:59 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.0871
2022-02-25 10:30:39 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.1982
2022-02-25 10:31:17 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.1597
2022-02-25 10:32:00 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.0449
2022-02-25 10:32:46 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.3034
2022-02-25 10:33:34 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.2494
2022-02-25 10:34:09 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.1908
2022-02-25 10:34:09 - train: epoch 003, train_loss: 3.2641
2022-02-25 10:36:11 - eval: epoch: 003, acc1: 31.960%, acc5: 57.222%, test_loss: 3.2301, per_image_load_time: 4.698ms, per_image_inference_time: 0.100ms
2022-02-25 10:36:12 - until epoch: 003, best_acc1: 31.960%
2022-02-25 10:36:12 - epoch 004 lr: 0.1
2022-02-25 10:36:52 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.1381
2022-02-25 10:37:28 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.1416
2022-02-25 10:38:03 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.2255
2022-02-25 10:38:39 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.9930
2022-02-25 10:39:14 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.0112
2022-02-25 10:39:50 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.3091
2022-02-25 10:40:26 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.1980
2022-02-25 10:41:03 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.9375
2022-02-25 10:41:37 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.7713
2022-02-25 10:42:13 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.1566
2022-02-25 10:42:47 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.2189
2022-02-25 10:43:23 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.9837
2022-02-25 10:43:57 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.9513
2022-02-25 10:44:33 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.1842
2022-02-25 10:45:09 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.1561
2022-02-25 10:45:43 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.9656
2022-02-25 10:46:20 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.0951
2022-02-25 10:46:53 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.4084
2022-02-25 10:47:28 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.1516
2022-02-25 10:48:01 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.1560
2022-02-25 10:48:35 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.2300
2022-02-25 10:49:11 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.0722
2022-02-25 10:49:45 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.9116
2022-02-25 10:50:19 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8575
2022-02-25 10:50:54 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.0852
2022-02-25 10:51:28 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.2281
2022-02-25 10:52:04 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.9913
2022-02-25 10:52:39 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.0760
2022-02-25 10:53:15 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.0595
2022-02-25 10:53:50 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.0890
2022-02-25 10:54:24 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.1110
2022-02-25 10:54:58 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.0777
2022-02-25 10:55:33 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.1756
2022-02-25 10:56:08 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.0976
2022-02-25 10:56:42 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.0813
2022-02-25 10:57:17 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.8667
2022-02-25 10:57:51 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.1014
2022-02-25 10:58:27 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.9183
2022-02-25 10:59:01 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.9397
2022-02-25 10:59:37 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.7697
2022-02-25 11:00:11 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.0611
2022-02-25 11:00:47 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.0520
2022-02-25 11:01:25 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.8995
2022-02-25 11:02:02 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.8350
2022-02-25 11:02:40 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.5352
2022-02-25 11:03:19 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.0708
2022-02-25 11:04:02 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.9935
2022-02-25 11:04:46 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.9096
2022-02-25 11:05:26 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.1719
2022-02-25 11:06:05 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.2313
2022-02-25 11:06:06 - train: epoch 004, train_loss: 3.0939
2022-02-25 11:07:31 - eval: epoch: 004, acc1: 37.542%, acc5: 63.030%, test_loss: 2.9381, per_image_load_time: 1.477ms, per_image_inference_time: 0.118ms
2022-02-25 11:07:31 - until epoch: 004, best_acc1: 37.542%
2022-02-25 11:07:31 - epoch 005 lr: 0.1
2022-02-25 11:08:12 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.0757
2022-02-25 11:08:48 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0220
2022-02-25 11:09:23 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.2458
2022-02-25 11:09:58 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.0323
2022-02-25 11:10:34 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.8106
2022-02-25 11:11:07 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.9648
2022-02-25 11:11:43 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.1222
2022-02-25 11:12:16 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.2502
2022-02-25 11:12:51 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.9523
2022-02-25 11:13:26 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.0809
2022-02-25 11:14:01 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.1311
2022-02-25 11:14:35 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.0706
2022-02-25 11:15:11 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.8464
2022-02-25 11:15:47 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0327
2022-02-25 11:16:21 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.8313
2022-02-25 11:16:57 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.7745
2022-02-25 11:17:31 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.8808
2022-02-25 11:18:07 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.2009
2022-02-25 11:18:41 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.8115
2022-02-25 11:19:17 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9878
2022-02-25 11:19:52 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7202
2022-02-25 11:20:28 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.0954
2022-02-25 11:21:02 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.7845
2022-02-25 11:21:39 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.9005
2022-02-25 11:22:13 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.0237
2022-02-25 11:22:48 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.2275
2022-02-25 11:23:21 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.1429
2022-02-25 11:23:56 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.0683
2022-02-25 11:24:29 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.7861
2022-02-25 11:25:04 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.8873
2022-02-25 11:25:39 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.0413
2022-02-25 11:26:13 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.1093
2022-02-25 11:26:46 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.7919
2022-02-25 11:27:21 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.8790
2022-02-25 11:27:55 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.0393
2022-02-25 11:28:28 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.9912
2022-02-25 11:29:02 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.9998
2022-02-25 11:29:37 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.8566
2022-02-25 11:30:11 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.3182
2022-02-25 11:30:46 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8217
2022-02-25 11:31:20 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.8981
2022-02-25 11:31:55 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.9748
2022-02-25 11:32:29 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.8084
2022-02-25 11:33:03 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.9993
2022-02-25 11:33:38 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.0716
2022-02-25 11:34:12 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.9176
2022-02-25 11:34:48 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.7954
2022-02-25 11:35:23 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.8164
2022-02-25 11:36:00 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.0466
2022-02-25 11:36:34 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.9283
2022-02-25 11:36:35 - train: epoch 005, train_loss: 2.9897
2022-02-25 11:37:57 - eval: epoch: 005, acc1: 39.646%, acc5: 66.206%, test_loss: 2.7686, per_image_load_time: 3.069ms, per_image_inference_time: 0.134ms
2022-02-25 11:37:57 - until epoch: 005, best_acc1: 39.646%
2022-02-25 11:37:57 - epoch 006 lr: 0.1
2022-02-25 11:38:36 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.8711
2022-02-25 11:39:10 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.9528
2022-02-25 11:39:43 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.8563
2022-02-25 11:40:16 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.0395
2022-02-25 11:40:50 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.9466
2022-02-25 11:41:23 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.0653
2022-02-25 11:41:57 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.0593
2022-02-25 11:42:30 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8563
2022-02-25 11:43:04 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.8856
2022-02-25 11:43:37 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.8940
2022-02-25 11:44:11 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.8184
2022-02-25 11:44:44 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.9613
2022-02-25 11:45:18 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.1713
2022-02-25 11:45:51 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.9939
2022-02-25 11:46:26 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.0473
2022-02-25 11:46:58 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.6458
2022-02-25 11:47:32 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.0314
2022-02-25 11:48:04 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.0416
2022-02-25 11:48:38 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.7923
2022-02-25 11:49:12 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.0454
2022-02-25 11:49:45 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.9368
2022-02-25 11:50:18 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.8379
2022-02-25 11:50:53 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.8333
2022-02-25 11:51:25 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.9500
2022-02-25 11:51:59 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.1433
2022-02-25 11:52:32 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.8955
2022-02-25 11:53:06 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.0354
2022-02-25 11:53:39 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7554
2022-02-25 11:54:13 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.0194
2022-02-25 11:54:47 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.9476
2022-02-25 11:55:20 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.7041
2022-02-25 11:55:54 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.8651
2022-02-25 11:56:28 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.7749
2022-02-25 11:57:02 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.0717
2022-02-25 11:57:36 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.0291
2022-02-25 11:58:10 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.0340
2022-02-25 11:58:43 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.9357
2022-02-25 11:59:17 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.8148
2022-02-25 11:59:51 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.6814
2022-02-25 12:00:24 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.1884
2022-02-25 12:00:58 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.8767
2022-02-25 12:01:31 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.8056
2022-02-25 12:02:04 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.8865
2022-02-25 12:02:39 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.9698
2022-02-25 12:03:12 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.8855
2022-02-25 12:03:48 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.9030
2022-02-25 12:04:24 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.0139
2022-02-25 12:05:06 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.0287
2022-02-25 12:05:41 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.9346
2022-02-25 12:06:17 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.7804
2022-02-25 12:06:18 - train: epoch 006, train_loss: 2.9259
2022-02-25 12:07:35 - eval: epoch: 006, acc1: 40.816%, acc5: 66.990%, test_loss: 2.6994, per_image_load_time: 2.851ms, per_image_inference_time: 0.116ms
2022-02-25 12:07:35 - until epoch: 006, best_acc1: 40.816%
2022-02-25 12:07:35 - epoch 007 lr: 0.1
2022-02-25 12:08:13 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.7117
2022-02-25 12:08:48 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.0883
2022-02-25 12:09:21 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.2701
2022-02-25 12:09:54 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.9232
2022-02-25 12:10:27 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.9182
2022-02-25 12:11:00 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.0809
2022-02-25 12:11:34 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.8924
2022-02-25 12:12:06 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.7805
2022-02-25 12:12:40 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.9195
2022-02-25 12:13:12 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.8158
2022-02-25 12:13:46 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.7231
2022-02-25 12:14:19 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.8403
2022-02-25 12:14:53 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.7206
2022-02-25 12:15:26 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 3.0451
2022-02-25 12:15:59 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8997
2022-02-25 12:16:32 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.9846
2022-02-25 12:17:06 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.9233
2022-02-25 12:17:38 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.6937
2022-02-25 12:18:12 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.9912
2022-02-25 12:18:44 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.6204
2022-02-25 12:19:19 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.9942
2022-02-25 12:19:52 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.7475
2022-02-25 12:20:25 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.9770
2022-02-25 12:20:58 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.9920
2022-02-25 12:21:33 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.7890
2022-02-25 12:22:05 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.7762
2022-02-25 12:22:38 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.7608
2022-02-25 12:23:11 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.8341
2022-02-25 12:23:45 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.8142
2022-02-25 12:24:18 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.0011
2022-02-25 12:24:52 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.7470
2022-02-25 12:25:26 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.8249
2022-02-25 12:25:59 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.1778
2022-02-25 12:26:33 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.8714
2022-02-25 12:27:06 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 3.0286
2022-02-25 12:27:42 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.7220
2022-02-25 12:28:15 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.9360
2022-02-25 12:28:49 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.0885
2022-02-25 12:29:22 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.7818
2022-02-25 12:29:56 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.9826
2022-02-25 12:30:28 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.7569
2022-02-25 12:31:03 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.6057
2022-02-25 12:31:36 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.9856
2022-02-25 12:32:10 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.7121
2022-02-25 12:32:44 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.0999
2022-02-25 12:33:18 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.8251
2022-02-25 12:33:55 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.9437
2022-02-25 12:34:30 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.0568
2022-02-25 12:35:07 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.9170
2022-02-25 12:35:40 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.9176
2022-02-25 12:35:42 - train: epoch 007, train_loss: 2.8792
2022-02-25 12:37:00 - eval: epoch: 007, acc1: 41.282%, acc5: 67.582%, test_loss: 2.6644, per_image_load_time: 1.840ms, per_image_inference_time: 0.115ms
2022-02-25 12:37:00 - until epoch: 007, best_acc1: 41.282%
2022-02-25 12:37:00 - epoch 008 lr: 0.1
2022-02-25 12:37:38 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.8232
2022-02-25 12:38:12 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.0530
2022-02-25 12:38:46 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.5702
2022-02-25 12:39:17 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.6678
2022-02-25 12:39:51 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.7263
2022-02-25 12:40:24 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.7362
2022-02-25 12:40:58 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.1309
2022-02-25 12:41:30 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.7261
2022-02-25 12:42:04 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.8936
2022-02-25 12:42:36 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.8510
2022-02-25 12:43:10 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.5740
2022-02-25 12:43:44 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.6603
2022-02-25 12:44:16 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.7882
2022-02-25 12:44:50 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.7239
2022-02-25 12:45:22 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.9363
2022-02-25 12:45:57 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.8938
2022-02-25 12:46:30 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.7143
2022-02-25 12:47:04 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 3.0511
2022-02-25 12:47:36 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.6504
2022-02-25 12:48:10 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.8927
2022-02-25 12:48:43 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.9277
2022-02-25 12:49:18 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.7444
2022-02-25 12:49:51 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.8287
2022-02-25 12:50:25 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.6975
2022-02-25 12:50:57 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.8788
2022-02-25 12:51:31 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.9866
2022-02-25 12:52:03 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.9389
2022-02-25 12:52:38 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.9705
2022-02-25 12:53:10 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.9734
2022-02-25 12:53:44 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.9500
2022-02-25 12:54:17 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.6779
2022-02-25 12:54:51 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.1987
2022-02-25 12:55:24 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.0401
2022-02-25 12:55:58 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.0037
2022-02-25 12:56:30 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.8619
2022-02-25 12:57:05 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.9135
2022-02-25 12:57:38 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.8167
2022-02-25 12:58:11 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.8296
2022-02-25 12:58:44 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.9676
2022-02-25 12:59:19 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.1978
2022-02-25 12:59:53 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.6589
2022-02-25 13:00:26 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.7937
2022-02-25 13:01:01 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.6447
2022-02-25 13:01:33 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.8201
2022-02-25 13:02:09 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.0399
2022-02-25 13:02:43 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.9727
2022-02-25 13:03:19 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.7711
2022-02-25 13:03:54 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.8542
2022-02-25 13:04:30 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.9112
2022-02-25 13:05:04 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.7990
2022-02-25 13:05:05 - train: epoch 008, train_loss: 2.8462
2022-02-25 13:06:29 - eval: epoch: 008, acc1: 42.030%, acc5: 68.266%, test_loss: 2.6338, per_image_load_time: 2.232ms, per_image_inference_time: 0.120ms
2022-02-25 13:06:29 - until epoch: 008, best_acc1: 42.030%
2022-02-25 13:06:29 - epoch 009 lr: 0.1
2022-02-25 13:07:08 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.5906
2022-02-25 13:07:42 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.6989
2022-02-25 13:08:14 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.5145
2022-02-25 13:08:48 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.1219
2022-02-25 13:09:20 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.7787
2022-02-25 13:09:55 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.8144
2022-02-25 13:10:28 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.7643
2022-02-25 13:11:01 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.6039
2022-02-25 13:11:34 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.6975
2022-02-25 13:12:08 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.6556
2022-02-25 13:12:40 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.0468
2022-02-25 13:13:14 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.8490
2022-02-25 13:13:47 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.9442
2022-02-25 13:14:21 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.6109
2022-02-25 13:14:53 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.7233
2022-02-25 13:15:27 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.8670
2022-02-25 13:16:00 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.8855
2022-02-25 13:16:34 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.7764
2022-02-25 13:17:07 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.7406
2022-02-25 13:17:39 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.6094
2022-02-25 13:18:14 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.9272
2022-02-25 13:18:47 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.9654
2022-02-25 13:19:20 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.5989
2022-02-25 13:19:54 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.7634
2022-02-25 13:20:27 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.6214
2022-02-25 13:21:01 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.9452
2022-02-25 13:21:34 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.6776
2022-02-25 13:22:07 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.9520
2022-02-25 13:22:40 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.5602
2022-02-25 13:23:14 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.8326
2022-02-25 13:23:47 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.9307
2022-02-25 13:24:20 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.9412
2022-02-25 13:24:53 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.9066
2022-02-25 13:25:28 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.9589
2022-02-25 13:26:00 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.8852
2022-02-25 13:26:35 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.6850
2022-02-25 13:27:08 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 3.0079
2022-02-25 13:27:41 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.0473
2022-02-25 13:28:14 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.5400
2022-02-25 13:28:49 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.0085
2022-02-25 13:29:22 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.7685
2022-02-25 13:29:57 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.7647
2022-02-25 13:30:30 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.9213
2022-02-25 13:31:05 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.8155
2022-02-25 13:31:38 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.8455
2022-02-25 13:32:11 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.9503
2022-02-25 13:32:45 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.9348
2022-02-25 13:33:21 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.0431
2022-02-25 13:33:56 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.8622
2022-02-25 13:34:31 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.7753
2022-02-25 13:34:32 - train: epoch 009, train_loss: 2.8218
2022-02-25 13:35:50 - eval: epoch: 009, acc1: 40.480%, acc5: 66.274%, test_loss: 2.7647, per_image_load_time: 2.914ms, per_image_inference_time: 0.121ms
2022-02-25 13:35:50 - until epoch: 009, best_acc1: 42.030%
2022-02-25 13:35:50 - epoch 010 lr: 0.1
2022-02-25 13:36:28 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.7630
2022-02-25 13:37:00 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.9708
2022-02-25 13:37:33 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.8418
2022-02-25 13:38:07 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.8179
2022-02-25 13:38:39 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.6527
2022-02-25 13:39:12 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.9529
2022-02-25 13:39:45 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.7960
2022-02-25 13:40:17 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.6285
2022-02-25 13:40:51 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.5923
2022-02-25 13:41:23 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.6683
2022-02-25 13:41:57 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.8326
2022-02-25 13:42:29 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.7162
2022-02-25 13:43:03 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.6725
2022-02-25 13:43:34 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.7880
2022-02-25 13:44:08 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.5098
2022-02-25 13:44:39 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.8956
2022-02-25 13:45:13 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.9737
2022-02-25 13:45:45 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.6619
2022-02-25 13:46:18 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.8156
2022-02-25 13:46:51 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.7993
2022-02-25 13:47:25 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.6828
2022-02-25 13:47:57 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.9966
2022-02-25 13:48:29 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.9751
2022-02-25 13:49:03 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.9644
2022-02-25 13:49:36 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.8702
2022-02-25 13:50:08 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.9252
2022-02-25 13:50:40 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.6419
2022-02-25 13:51:15 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.8730
2022-02-25 13:51:47 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.7919
2022-02-25 13:52:21 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.7663
2022-02-25 13:52:54 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.8663
2022-02-25 13:53:27 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.8538
2022-02-25 13:54:00 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.9729
2022-02-25 13:54:33 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 3.0773
2022-02-25 13:55:06 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.9555
2022-02-25 13:55:40 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.0887
2022-02-25 13:56:13 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.7403
2022-02-25 13:56:46 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.8634
2022-02-25 13:57:18 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.4658
2022-02-25 13:57:53 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.6757
2022-02-25 13:58:25 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.6985
2022-02-25 13:58:59 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.8941
2022-02-25 13:59:32 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.6389
2022-02-25 14:00:06 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.7670
2022-02-25 14:00:40 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.7642
2022-02-25 14:01:14 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.8706
2022-02-25 14:01:48 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.8062
2022-02-25 14:02:23 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.7621
2022-02-25 14:02:58 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.6927
2022-02-25 14:03:32 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.5431
2022-02-25 14:03:33 - train: epoch 010, train_loss: 2.7993
2022-02-25 14:04:46 - eval: epoch: 010, acc1: 42.788%, acc5: 68.776%, test_loss: 2.5892, per_image_load_time: 1.821ms, per_image_inference_time: 0.141ms
2022-02-25 14:04:46 - until epoch: 010, best_acc1: 42.788%
2022-02-25 14:04:46 - epoch 011 lr: 0.1
2022-02-25 14:05:24 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.6632
2022-02-25 14:05:58 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.9154
2022-02-25 14:06:30 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.6315
2022-02-25 14:07:03 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.7568
2022-02-25 14:07:35 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.6905
2022-02-25 14:08:08 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.7985
2022-02-25 14:08:41 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.8260
2022-02-25 14:09:15 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.7942
2022-02-25 14:09:47 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.9980
2022-02-25 14:10:21 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.6931
2022-02-25 14:10:53 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.7399
2022-02-25 14:11:27 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.0869
2022-02-25 14:12:00 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.9323
2022-02-25 14:12:32 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.7808
2022-02-25 14:13:05 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.5954
2022-02-25 14:13:38 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.8577
2022-02-25 14:14:12 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.8232
2022-02-25 14:14:45 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.6903
2022-02-25 14:15:18 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.6581
2022-02-25 14:15:52 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.8255
2022-02-25 14:16:24 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.7685
2022-02-25 14:16:58 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.7601
2022-02-25 14:17:31 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.9936
2022-02-25 14:18:04 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.6371
2022-02-25 14:18:37 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.9927
2022-02-25 14:19:10 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.7963
2022-02-25 14:19:44 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.7898
2022-02-25 14:20:17 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.4574
2022-02-25 14:20:50 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.7637
2022-02-25 14:21:23 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.9436
2022-02-25 14:21:57 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.8738
2022-02-25 14:22:29 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.5702
2022-02-25 14:23:02 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.8779
2022-02-25 14:23:36 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.8161
2022-02-25 14:24:10 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.7296
2022-02-25 14:24:43 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.8431
2022-02-25 14:25:16 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.9860
2022-02-25 14:25:50 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.4988
2022-02-25 14:26:23 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.8111
2022-02-25 14:26:55 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.6928
2022-02-25 14:27:29 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.5778
2022-02-25 14:28:02 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.6272
2022-02-25 14:28:36 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.9263
2022-02-25 14:29:09 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.7815
2022-02-25 14:29:42 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.6088
2022-02-25 14:30:16 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.7009
2022-02-25 14:30:51 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.5504
2022-02-25 14:31:25 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.5482
2022-02-25 14:32:01 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.5804
2022-02-25 14:32:34 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.6898
2022-02-25 14:32:35 - train: epoch 011, train_loss: 2.7808
2022-02-25 14:33:51 - eval: epoch: 011, acc1: 39.746%, acc5: 66.178%, test_loss: 2.7569, per_image_load_time: 0.869ms, per_image_inference_time: 0.122ms
2022-02-25 14:33:51 - until epoch: 011, best_acc1: 42.788%
2022-02-25 14:33:51 - epoch 012 lr: 0.1
2022-02-25 14:34:29 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.7328
2022-02-25 14:35:01 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.6120
2022-02-25 14:35:34 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.7444
2022-02-25 14:36:06 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.8052
2022-02-25 14:36:40 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.1344
2022-02-25 14:37:12 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.5890
2022-02-25 14:37:45 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.5870
2022-02-25 14:38:16 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.7853
2022-02-25 14:38:49 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.8157
2022-02-25 14:39:22 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.5077
2022-02-25 14:39:56 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.1781
2022-02-25 14:40:28 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.6386
2022-02-25 14:41:01 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.6961
2022-02-25 14:41:32 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.8076
2022-02-25 14:42:06 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.4779
2022-02-25 14:42:38 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.5902
2022-02-25 14:43:11 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.6585
2022-02-25 14:43:45 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.7404
2022-02-25 14:44:16 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.9003
2022-02-25 14:44:50 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.9691
2022-02-25 14:45:22 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.7867
2022-02-25 14:45:54 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.9393
2022-02-25 14:46:27 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.6666
2022-02-25 14:47:00 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.7320
2022-02-25 14:47:33 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.5319
2022-02-25 14:48:05 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.5853
2022-02-25 14:48:38 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.6880
2022-02-25 14:49:10 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.7276
2022-02-25 14:49:44 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.5782
2022-02-25 14:50:15 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.6640
2022-02-25 14:50:49 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.9786
2022-02-25 14:51:21 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.3782
2022-02-25 14:51:55 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.7412
2022-02-25 14:52:26 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.7325
2022-02-25 14:53:00 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.8324
2022-02-25 14:53:32 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.8698
2022-02-25 14:54:05 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.7973
2022-02-25 14:54:38 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.7789
2022-02-25 14:55:12 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.7262
2022-02-25 14:55:44 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.6681
2022-02-25 14:56:17 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.7454
2022-02-25 14:56:50 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.6860
2022-02-25 14:57:25 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.8450
2022-02-25 14:58:00 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.5912
2022-02-25 14:58:35 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.7556
2022-02-25 14:59:10 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.0627
2022-02-25 14:59:43 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.5743
2022-02-25 15:00:18 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.8669
2022-02-25 15:00:52 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.8300
2022-02-25 15:01:27 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.4135
2022-02-25 15:01:28 - train: epoch 012, train_loss: 2.7671
2022-02-25 15:02:45 - eval: epoch: 012, acc1: 43.982%, acc5: 70.244%, test_loss: 2.5158, per_image_load_time: 2.418ms, per_image_inference_time: 0.120ms
2022-02-25 15:02:45 - until epoch: 012, best_acc1: 43.982%
2022-02-25 15:02:45 - epoch 013 lr: 0.1
2022-02-25 15:03:21 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.5621
2022-02-25 15:03:55 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.8757
2022-02-25 15:04:28 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.6617
2022-02-25 15:05:00 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.6601
2022-02-25 15:05:33 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.5612
2022-02-25 15:06:05 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.9425
2022-02-25 15:06:38 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.6250
2022-02-25 15:07:11 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.9095
2022-02-25 15:07:43 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.7076
2022-02-25 15:08:16 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.8389
2022-02-25 15:08:49 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.7761
2022-02-25 15:09:20 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.9767
2022-02-25 15:09:53 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.9060
2022-02-25 15:10:26 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.8085
2022-02-25 15:10:59 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.9132
2022-02-25 15:11:32 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.4464
2022-02-25 15:12:04 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.7509
2022-02-25 15:12:38 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.6954
2022-02-25 15:13:10 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.7163
2022-02-25 15:13:43 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.9928
2022-02-25 15:14:15 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.0779
2022-02-25 15:14:48 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.6464
2022-02-25 15:15:20 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.7377
2022-02-25 15:15:53 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.7468
2022-02-25 15:16:26 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.6802
2022-02-25 15:17:00 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.6398
2022-02-25 15:17:32 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.6772
2022-02-25 15:18:05 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.8392
2022-02-25 15:18:37 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.8111
2022-02-25 15:19:11 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.6637
2022-02-25 15:19:43 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.5208
2022-02-25 15:20:16 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.7835
2022-02-25 15:20:49 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.5772
2022-02-25 15:21:22 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.7123
2022-02-25 15:21:54 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.6048
2022-02-25 15:22:26 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.0485
2022-02-25 15:22:59 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.4060
2022-02-25 15:23:32 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.9393
2022-02-25 15:24:05 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.7739
2022-02-25 15:24:39 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.6994
2022-02-25 15:25:14 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.7453
2022-02-25 15:25:48 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.7697
2022-02-25 15:26:21 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.7647
2022-02-25 15:26:55 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.6704
2022-02-25 15:27:30 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.8016
2022-02-25 15:28:05 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.7078
2022-02-25 15:28:39 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.8930
2022-02-25 15:29:15 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.7230
2022-02-25 15:29:48 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.7640
2022-02-25 15:30:24 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.8340
2022-02-25 15:30:24 - train: epoch 013, train_loss: 2.7543
2022-02-25 15:31:40 - eval: epoch: 013, acc1: 45.188%, acc5: 71.056%, test_loss: 2.4688, per_image_load_time: 2.287ms, per_image_inference_time: 0.136ms
2022-02-25 15:31:40 - until epoch: 013, best_acc1: 45.188%
2022-02-25 15:31:40 - epoch 014 lr: 0.1
2022-02-25 15:32:18 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.6986
2022-02-25 15:32:51 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.8716
2022-02-25 15:33:24 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.4321
2022-02-25 15:33:56 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.5682
2022-02-25 15:34:29 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.6433
2022-02-25 15:35:02 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.6849
2022-02-25 15:35:34 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.6323
2022-02-25 15:36:08 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.7154
2022-02-25 15:36:39 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.8244
2022-02-25 15:37:12 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.8528
2022-02-25 15:37:45 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.7140
2022-02-25 15:38:18 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.8107
2022-02-25 15:38:50 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.7422
2022-02-25 15:39:24 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.7059
2022-02-25 15:39:57 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.8915
2022-02-25 15:40:29 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.6302
2022-02-25 15:41:02 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.9732
2022-02-25 15:41:34 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.9261
2022-02-25 15:42:06 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.5075
2022-02-25 15:42:39 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.6125
2022-02-25 15:43:12 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.8568
2022-02-25 15:43:44 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.7367
2022-02-25 15:44:16 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.7317
2022-02-25 15:44:50 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.8882
2022-02-25 15:45:21 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.7014
2022-02-25 15:45:55 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.7165
2022-02-25 15:46:27 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.6645
2022-02-25 15:47:00 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.9929
2022-02-25 15:47:32 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.5303
2022-02-25 15:48:06 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.7543
2022-02-25 15:48:37 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.6774
2022-02-25 15:49:11 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.6269
2022-02-25 15:49:43 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.5594
2022-02-25 15:50:17 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.6885
2022-02-25 15:50:49 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.6891
2022-02-25 15:51:22 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.6870
2022-02-25 15:51:54 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.7590
2022-02-25 15:52:27 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.9407
2022-02-25 15:52:58 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.6040
2022-02-25 15:53:32 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.9353
2022-02-25 15:54:05 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.7029
2022-02-25 15:54:38 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.5409
2022-02-25 15:55:11 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.6323
2022-02-25 15:55:45 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.6521
2022-02-25 15:56:17 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.7268
2022-02-25 15:56:52 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.6645
2022-02-25 15:57:26 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.7081
2022-02-25 15:58:01 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.7307
2022-02-25 15:58:34 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.5010
2022-02-25 15:59:07 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.8525
2022-02-25 15:59:08 - train: epoch 014, train_loss: 2.7441
2022-02-25 16:00:22 - eval: epoch: 014, acc1: 43.330%, acc5: 69.468%, test_loss: 2.5442, per_image_load_time: 1.141ms, per_image_inference_time: 0.136ms
2022-02-25 16:00:22 - until epoch: 014, best_acc1: 45.188%
2022-02-25 16:00:22 - epoch 015 lr: 0.1
2022-02-25 16:01:00 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.4920
2022-02-25 16:01:35 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.9191
2022-02-25 16:02:07 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.9191
2022-02-25 16:02:40 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.7334
2022-02-25 16:03:13 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.6825
2022-02-25 16:03:45 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.8581
2022-02-25 16:04:17 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.7356
2022-02-25 16:04:51 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.6311
2022-02-25 16:05:23 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.6190
2022-02-25 16:05:56 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.8276
2022-02-25 16:06:29 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.7308
2022-02-25 16:07:03 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.7381
2022-02-25 16:07:35 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.8780
2022-02-25 16:08:08 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.6668
2022-02-25 16:08:41 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.4336
2022-02-25 16:09:15 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.7107
2022-02-25 16:09:47 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.8833
2022-02-25 16:10:20 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.6028
2022-02-25 16:10:52 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.6337
2022-02-25 16:11:26 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.5767
2022-02-25 16:11:59 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.6313
2022-02-25 16:12:32 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.7687
2022-02-25 16:13:04 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.4797
2022-02-25 16:13:37 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.8227
2022-02-25 16:14:09 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.9804
2022-02-25 16:14:42 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.4408
2022-02-25 16:15:15 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.7703
2022-02-25 16:15:49 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.8196
2022-02-25 16:16:20 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.6387
2022-02-25 16:16:54 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.5728
2022-02-25 16:17:26 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.6682
2022-02-25 16:17:59 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.7454
2022-02-25 16:18:31 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.5752
2022-02-25 16:19:05 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.7399
2022-02-25 16:19:37 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.8275
2022-02-25 16:20:10 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.8308
2022-02-25 16:20:44 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.6980
2022-02-25 16:21:17 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.8021
2022-02-25 16:21:49 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.9578
2022-02-25 16:22:23 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.6864
2022-02-25 16:22:56 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.8976
2022-02-25 16:23:29 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.5019
2022-02-25 16:24:02 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.7604
2022-02-25 16:24:35 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.6671
2022-02-25 16:25:08 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.7511
2022-02-25 16:25:41 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.7337
2022-02-25 16:26:14 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.8706
2022-02-25 16:26:48 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.8470
2022-02-25 16:27:21 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.6937
2022-02-25 16:27:54 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.9023
2022-02-25 16:27:55 - train: epoch 015, train_loss: 2.7360
2022-02-25 16:29:10 - eval: epoch: 015, acc1: 43.334%, acc5: 69.250%, test_loss: 2.5674, per_image_load_time: 1.130ms, per_image_inference_time: 0.116ms
2022-02-25 16:29:10 - until epoch: 015, best_acc1: 45.188%
2022-02-25 16:29:10 - epoch 016 lr: 0.1
2022-02-25 16:29:48 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.7022
2022-02-25 16:30:22 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.5286
2022-02-25 16:30:54 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.7002
2022-02-25 16:31:28 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 3.0479
2022-02-25 16:32:01 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.4337
2022-02-25 16:32:35 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.7640
2022-02-25 16:33:06 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.5430
2022-02-25 16:33:40 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.6730
2022-02-25 16:34:11 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.9098
2022-02-25 16:34:44 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.4936
2022-02-25 16:35:18 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.6501
2022-02-25 16:35:50 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.5959
2022-02-25 16:36:23 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.7605
2022-02-25 16:36:55 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.5893
2022-02-25 16:37:29 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.8141
2022-02-25 16:38:01 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.9043
2022-02-25 16:38:34 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.6850
2022-02-25 16:39:08 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.6944
2022-02-25 16:39:40 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.6928
2022-02-25 16:40:14 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.4053
2022-02-25 16:40:46 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.9264
2022-02-25 16:41:19 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.8741
2022-02-25 16:41:52 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.8520
2022-02-25 16:42:25 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.8359
2022-02-25 16:42:58 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.5983
2022-02-25 16:43:32 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.9489
2022-02-25 16:44:04 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.6488
2022-02-25 16:44:38 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.5295
2022-02-25 16:45:10 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.7079
2022-02-25 16:45:44 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.9862
2022-02-25 16:46:16 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.9201
2022-02-25 16:46:50 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.8289
2022-02-25 16:47:22 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.8353
2022-02-25 16:47:56 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.6719
2022-02-25 16:48:28 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.6424
2022-02-25 16:49:02 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.6105
2022-02-25 16:49:34 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.8468
2022-02-25 16:50:07 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.0648
2022-02-25 16:50:39 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.7002
2022-02-25 16:51:13 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.7926
2022-02-25 16:51:45 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.7511
2022-02-25 16:52:19 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.7036
2022-02-25 16:52:52 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.6978
2022-02-25 16:53:25 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.6315
2022-02-25 16:53:59 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.8270
2022-02-25 16:54:31 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.6484
2022-02-25 16:55:05 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.9087
2022-02-25 16:55:38 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.6926
2022-02-25 16:56:12 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.8533
2022-02-25 16:56:43 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.9138
2022-02-25 16:56:44 - train: epoch 016, train_loss: 2.7256
2022-02-25 16:57:59 - eval: epoch: 016, acc1: 44.088%, acc5: 70.080%, test_loss: 2.5416, per_image_load_time: 2.589ms, per_image_inference_time: 0.131ms
2022-02-25 16:57:59 - until epoch: 016, best_acc1: 45.188%
2022-02-25 16:57:59 - epoch 017 lr: 0.1
2022-02-25 16:58:36 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.6085
2022-02-25 16:59:10 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.8679
2022-02-25 16:59:43 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.9244
2022-02-25 17:00:16 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.3697
2022-02-25 17:00:49 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.6478
2022-02-25 17:01:23 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.1689
2022-02-25 17:01:56 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.7145
2022-02-25 17:02:29 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.7064
2022-02-25 17:03:01 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.6465
2022-02-25 17:03:34 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.6614
2022-02-25 17:04:07 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.0693
2022-02-25 17:04:40 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.8334
2022-02-25 17:05:13 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.7929
2022-02-25 17:05:46 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.7917
2022-02-25 17:06:19 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.5397
2022-02-25 17:06:52 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.5647
2022-02-25 17:07:24 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.6077
2022-02-25 17:07:58 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.7991
2022-02-25 17:08:30 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.4207
2022-02-25 17:09:04 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.9103
2022-02-25 17:09:37 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.7475
2022-02-25 17:10:09 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.5958
2022-02-25 17:10:43 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.7919
2022-02-25 17:11:15 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.5593
2022-02-25 17:11:49 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.9038
2022-02-25 17:12:22 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.7355
2022-02-25 17:12:56 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.7170
2022-02-25 17:13:29 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 3.0221
2022-02-25 17:14:03 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.8837
2022-02-25 17:14:36 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.4019
2022-02-25 17:15:11 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.8604
2022-02-25 17:15:44 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.5822
2022-02-25 17:16:19 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.7983
2022-02-25 17:16:52 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.4821
2022-02-25 17:17:27 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.8014
2022-02-25 17:18:00 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.9491
2022-02-25 17:18:34 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.7500
2022-02-25 17:19:07 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.9510
2022-02-25 17:19:41 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.5733
2022-02-25 17:20:14 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.5708
2022-02-25 17:20:49 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.7332
2022-02-25 17:21:23 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.7031
2022-02-25 17:21:57 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.7873
2022-02-25 17:22:31 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.6991
2022-02-25 17:23:06 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.8312
2022-02-25 17:23:42 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.6826
2022-02-25 17:24:16 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.7129
2022-02-25 17:24:50 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.8367
2022-02-25 17:25:29 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.6407
2022-02-25 17:26:03 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.5753
2022-02-25 17:26:04 - train: epoch 017, train_loss: 2.7177
2022-02-25 17:27:20 - eval: epoch: 017, acc1: 44.970%, acc5: 71.064%, test_loss: 2.4714, per_image_load_time: 2.667ms, per_image_inference_time: 0.115ms
2022-02-25 17:27:21 - until epoch: 017, best_acc1: 45.188%
2022-02-25 17:27:21 - epoch 018 lr: 0.1
2022-02-25 17:27:59 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.7015
2022-02-25 17:28:32 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.7293
2022-02-25 17:29:06 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.8492
2022-02-25 17:29:39 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.7412
2022-02-25 17:30:12 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.6902
2022-02-25 17:30:45 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.8575
2022-02-25 17:31:19 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.5203
2022-02-25 17:31:51 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.6272
2022-02-25 17:32:25 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.7747
2022-02-25 17:32:58 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.5894
2022-02-25 17:33:32 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.0169
2022-02-25 17:34:04 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.6095
2022-02-25 17:34:38 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 3.0798
2022-02-25 17:35:11 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.7729
2022-02-25 17:35:45 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.8361
2022-02-25 17:36:18 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.7181
2022-02-25 17:36:52 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.5792
2022-02-25 17:37:24 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.4294
2022-02-25 17:37:59 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.7213
2022-02-25 17:38:32 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.0774
2022-02-25 17:39:07 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.8204
2022-02-25 17:39:39 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.7546
2022-02-25 17:40:12 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.7850
2022-02-25 17:40:46 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.4280
2022-02-25 17:41:20 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.3522
2022-02-25 17:41:53 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.6062
2022-02-25 17:42:27 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.8861
2022-02-25 17:43:00 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.4090
2022-02-25 17:43:33 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.7601
2022-02-25 17:44:06 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.5767
2022-02-25 17:44:40 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.0656
2022-02-25 17:45:12 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.6287
2022-02-25 17:45:47 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.7094
2022-02-25 17:46:19 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.6309
2022-02-25 17:46:52 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.8797
2022-02-25 17:47:26 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.7814
2022-02-25 17:48:01 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.0788
2022-02-25 17:48:34 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.8104
2022-02-25 17:49:06 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.7187
2022-02-25 17:49:40 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.6290
2022-02-25 17:50:13 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.6156
2022-02-25 17:50:47 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.7100
2022-02-25 17:51:20 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.5360
2022-02-25 17:51:54 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.8369
2022-02-25 17:52:26 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.7201
2022-02-25 17:53:01 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.5528
2022-02-25 17:53:33 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.9730
2022-02-25 17:54:07 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.9968
2022-02-25 17:54:40 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.7005
2022-02-25 17:55:13 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.7474
2022-02-25 17:55:13 - train: epoch 018, train_loss: 2.7112
2022-02-25 17:56:28 - eval: epoch: 018, acc1: 45.258%, acc5: 71.174%, test_loss: 2.4628, per_image_load_time: 2.753ms, per_image_inference_time: 0.134ms
2022-02-25 17:56:28 - until epoch: 018, best_acc1: 45.258%
2022-02-25 17:56:28 - epoch 019 lr: 0.1
2022-02-25 17:57:07 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.5161
2022-02-25 17:57:39 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.8380
2022-02-25 17:58:13 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.9081
2022-02-25 17:58:47 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.5067
2022-02-25 17:59:20 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.7106
2022-02-25 17:59:53 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.7338
2022-02-25 18:00:26 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.4436
2022-02-25 18:01:00 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.8851
2022-02-25 18:01:32 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.7192
2022-02-25 18:02:06 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.8908
2022-02-25 18:02:39 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.4983
2022-02-25 18:03:13 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.8079
2022-02-25 18:03:46 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.7647
2022-02-25 18:04:19 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.6371
2022-02-25 18:04:53 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.0286
2022-02-25 18:05:26 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.5988
2022-02-25 18:05:59 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.7971
2022-02-25 18:06:33 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.6952
2022-02-25 18:07:06 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.9994
2022-02-25 18:07:39 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.6706
2022-02-25 18:08:12 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.6788
2022-02-25 18:08:46 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.6410
2022-02-25 18:09:19 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.7303
2022-02-25 18:09:53 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.8318
2022-02-25 18:10:26 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.6760
2022-02-25 18:11:00 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.8007
2022-02-25 18:11:32 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.8095
2022-02-25 18:12:07 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.7642
2022-02-25 18:12:40 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.5719
2022-02-25 18:13:13 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.9897
2022-02-25 18:13:47 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.7996
2022-02-25 18:14:19 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.3119
2022-02-25 18:14:53 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.6947
2022-02-25 18:15:26 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.7847
2022-02-25 18:16:00 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.7794
2022-02-25 18:16:33 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.6681
2022-02-25 18:17:06 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.8701
2022-02-25 18:17:39 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.8901
2022-02-25 18:18:12 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.5831
2022-02-25 18:18:46 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.5646
2022-02-25 18:19:19 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.8077
2022-02-25 18:19:52 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.6925
2022-02-25 18:20:25 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.6769
2022-02-25 18:21:00 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.8409
2022-02-25 18:21:32 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 3.0653
2022-02-25 18:22:06 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.6417
2022-02-25 18:22:39 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.6310
2022-02-25 18:23:14 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.7720
2022-02-25 18:23:47 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.7167
2022-02-25 18:24:20 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.7217
2022-02-25 18:24:20 - train: epoch 019, train_loss: 2.7081
2022-02-25 18:25:36 - eval: epoch: 019, acc1: 45.598%, acc5: 71.202%, test_loss: 2.4399, per_image_load_time: 1.542ms, per_image_inference_time: 0.141ms
2022-02-25 18:25:36 - until epoch: 019, best_acc1: 45.598%
2022-02-25 18:25:36 - epoch 020 lr: 0.1
2022-02-25 18:26:14 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.9020
2022-02-25 18:26:48 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.4691
2022-02-25 18:27:19 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.8295
2022-02-25 18:27:54 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.3806
2022-02-25 18:28:27 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.6445
2022-02-25 18:29:02 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.9081
2022-02-25 18:29:35 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.5706
2022-02-25 18:30:09 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.7681
2022-02-25 18:30:41 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.9518
2022-02-25 18:31:16 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.8007
2022-02-25 18:31:48 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.5564
2022-02-25 18:32:22 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.4729
2022-02-25 18:32:55 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.5901
2022-02-25 18:33:29 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.7519
2022-02-25 18:34:03 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.7121
2022-02-25 18:34:37 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.6306
2022-02-25 18:35:10 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.3624
2022-02-25 18:35:44 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.6798
2022-02-25 18:36:17 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.5575
2022-02-25 18:36:51 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.6607
2022-02-25 18:37:24 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.8455
2022-02-25 18:38:00 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.5172
2022-02-25 18:38:34 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.5910
2022-02-25 18:39:07 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.8875
2022-02-25 18:39:41 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.5623
2022-02-25 18:40:14 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.4595
2022-02-25 18:40:48 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.7956
2022-02-25 18:41:21 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.7188
2022-02-25 18:41:56 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.8149
2022-02-25 18:42:29 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.5209
2022-02-25 18:43:03 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.6811
2022-02-25 18:43:37 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.8183
2022-02-25 18:44:11 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.6713
2022-02-25 18:44:46 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.5898
2022-02-25 18:45:19 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.5720
2022-02-25 18:45:53 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.6880
2022-02-25 18:46:26 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.6326
2022-02-25 18:47:00 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.7450
2022-02-25 18:47:33 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.8247
2022-02-25 18:48:06 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.6069
2022-02-25 18:48:40 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.4993
2022-02-25 18:49:13 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.5739
2022-02-25 18:49:46 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.7460
2022-02-25 18:50:20 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.7342
2022-02-25 18:50:52 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.6675
2022-02-25 18:51:27 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.8214
2022-02-25 18:52:01 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.4900
2022-02-25 18:52:34 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.7213
2022-02-25 18:53:08 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.8504
2022-02-25 18:53:39 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.4802
2022-02-25 18:53:41 - train: epoch 020, train_loss: 2.6973
2022-02-25 18:54:56 - eval: epoch: 020, acc1: 44.772%, acc5: 70.690%, test_loss: 2.5008, per_image_load_time: 2.179ms, per_image_inference_time: 0.131ms
2022-02-25 18:54:56 - until epoch: 020, best_acc1: 45.598%
2022-02-25 18:54:56 - epoch 021 lr: 0.1
2022-02-25 18:55:34 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.5381
2022-02-25 18:56:08 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.7993
2022-02-25 18:56:41 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.3198
2022-02-25 18:57:13 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.6913
2022-02-25 18:57:47 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.5117
2022-02-25 18:58:19 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.5954
2022-02-25 18:58:52 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.4928
2022-02-25 18:59:24 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.9541
2022-02-25 18:59:58 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.7535
2022-02-25 19:00:30 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.4982
2022-02-25 19:01:03 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.6686
2022-02-25 19:01:36 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.6119
2022-02-25 19:02:09 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.6410
2022-02-25 19:02:43 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.6308
2022-02-25 19:03:15 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.5859
2022-02-25 19:03:48 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.7721
2022-02-25 19:04:21 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.8101
2022-02-25 19:04:56 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.5305
2022-02-25 19:05:32 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.8323
2022-02-25 19:06:04 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.8673
2022-02-25 19:06:39 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.5357
2022-02-25 19:07:12 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.7923
2022-02-25 19:07:47 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.4985
2022-02-25 19:08:21 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.6419
2022-02-25 19:08:56 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.7133
2022-02-25 19:09:28 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.9776
2022-02-25 19:10:02 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.5270
2022-02-25 19:10:34 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.6398
2022-02-25 19:11:07 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.5746
2022-02-25 19:11:40 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 3.0166
2022-02-25 19:12:13 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.7576
2022-02-25 19:12:46 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.7034
2022-02-25 19:13:17 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.9666
2022-02-25 19:13:51 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.8949
2022-02-25 19:14:23 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.5549
2022-02-25 19:14:56 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.5774
2022-02-25 19:15:28 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.6259
2022-02-25 19:16:01 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.5817
2022-02-25 19:16:32 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.6360
2022-02-25 19:17:06 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.9015
2022-02-25 19:17:38 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.5857
2022-02-25 19:18:11 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.6389
2022-02-25 19:18:43 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.6970
2022-02-25 19:19:16 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.7623
2022-02-25 19:19:47 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.9330
2022-02-25 19:20:21 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.6416
2022-02-25 19:20:53 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.7869
2022-02-25 19:21:27 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.7178
2022-02-25 19:21:59 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.4659
2022-02-25 19:22:31 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.4979
2022-02-25 19:22:33 - train: epoch 021, train_loss: 2.6947
2022-02-25 19:23:48 - eval: epoch: 021, acc1: 43.782%, acc5: 69.804%, test_loss: 2.5363, per_image_load_time: 2.296ms, per_image_inference_time: 0.129ms
2022-02-25 19:23:48 - until epoch: 021, best_acc1: 45.598%
2022-02-25 19:23:48 - epoch 022 lr: 0.1
2022-02-25 19:24:24 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.4382
2022-02-25 19:24:58 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.6218
2022-02-25 19:25:31 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.3205
2022-02-25 19:26:03 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.5461
2022-02-25 19:26:36 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.7164
2022-02-25 19:27:09 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.7185
2022-02-25 19:27:41 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.8175
2022-02-25 19:28:14 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.7916
2022-02-25 19:28:45 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.9251
2022-02-25 19:29:18 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.6441
2022-02-25 19:29:50 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.7347
2022-02-25 19:30:23 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.3890
2022-02-25 19:30:55 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.6467
2022-02-25 19:31:27 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.7793
2022-02-25 19:32:00 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.7179
2022-02-25 19:32:34 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.6242
2022-02-25 19:33:05 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.4076
2022-02-25 19:33:39 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.8750
2022-02-25 19:34:10 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.4951
2022-02-25 19:34:43 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.6738
2022-02-25 19:35:16 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.5774
2022-02-25 19:35:49 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.5051
2022-02-25 19:36:21 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.8224
2022-02-25 19:36:55 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.6891
2022-02-25 19:37:27 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.6218
2022-02-25 19:38:01 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.4979
2022-02-25 19:38:33 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.4676
2022-02-25 19:39:06 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.0750
2022-02-25 19:39:38 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.5384
2022-02-25 19:40:11 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.7491
2022-02-25 19:40:43 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.8180
2022-02-25 19:41:17 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.7661
2022-02-25 19:41:49 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.7359
2022-02-25 19:42:22 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.6064
2022-02-25 19:42:55 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.8598
2022-02-25 19:43:29 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.7010
2022-02-25 19:44:01 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.7313
2022-02-25 19:44:35 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.8445
2022-02-25 19:45:07 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.6429
2022-02-25 19:45:41 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.7351
2022-02-25 19:46:16 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.5557
2022-02-25 19:46:50 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.7095
2022-02-25 19:47:22 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.8756
2022-02-25 19:47:56 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.6769
2022-02-25 19:48:29 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.5907
2022-02-25 19:49:03 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.8569
2022-02-25 19:49:36 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.8087
2022-02-25 19:50:10 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.5455
2022-02-25 19:50:43 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.5884
2022-02-25 19:51:19 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.4674
2022-02-25 19:51:20 - train: epoch 022, train_loss: 2.6920
2022-02-25 19:52:36 - eval: epoch: 022, acc1: 44.386%, acc5: 70.432%, test_loss: 2.5001, per_image_load_time: 0.915ms, per_image_inference_time: 0.117ms
2022-02-25 19:52:36 - until epoch: 022, best_acc1: 45.598%
2022-02-25 19:52:36 - epoch 023 lr: 0.1
2022-02-25 19:53:15 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.5431
2022-02-25 19:53:49 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.3208
2022-02-25 19:54:22 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.5006
2022-02-25 19:54:54 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.7769
2022-02-25 19:55:27 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.7256
2022-02-25 19:56:00 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.5967
2022-02-25 19:56:34 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.5127
2022-02-25 19:57:07 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.5397
2022-02-25 19:57:40 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.6874
2022-02-25 19:58:13 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.5125
2022-02-25 19:58:47 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.7491
2022-02-25 19:59:20 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.3627
2022-02-25 19:59:55 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.6095
2022-02-25 20:00:27 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.5699
2022-02-25 20:01:01 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.6703
2022-02-25 20:01:35 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.7206
2022-02-25 20:02:07 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.8742
2022-02-25 20:02:41 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.6220
2022-02-25 20:03:12 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.6460
2022-02-25 20:03:46 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.3817
2022-02-25 20:04:19 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.8550
2022-02-25 20:04:53 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.2466
2022-02-25 20:05:26 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.4372
2022-02-25 20:05:59 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.6518
2022-02-25 20:06:32 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.7366
2022-02-25 20:07:05 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.7299
2022-02-25 20:07:38 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.6210
2022-02-25 20:08:11 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.7214
2022-02-25 20:08:44 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.8236
2022-02-25 20:09:16 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.8134
2022-02-25 20:09:49 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.7513
2022-02-25 20:10:23 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.9198
2022-02-25 20:10:55 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.7781
2022-02-25 20:11:29 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.8406
2022-02-25 20:12:01 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.6106
2022-02-25 20:12:34 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.4918
2022-02-25 20:13:07 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.7523
2022-02-25 20:13:41 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.6408
2022-02-25 20:14:14 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.6933
2022-02-25 20:14:47 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.7017
2022-02-25 20:15:22 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.6000
2022-02-25 20:15:55 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.5079
2022-02-25 20:16:29 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.4802
2022-02-25 20:17:02 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.5183
2022-02-25 20:17:36 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.4886
2022-02-25 20:18:08 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.6725
2022-02-25 20:18:42 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.5390
2022-02-25 20:19:14 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.6199
2022-02-25 20:19:48 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.6420
2022-02-25 20:20:21 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.8098
2022-02-25 20:20:22 - train: epoch 023, train_loss: 2.6864
2022-02-25 20:21:36 - eval: epoch: 023, acc1: 43.914%, acc5: 70.178%, test_loss: 2.5199, per_image_load_time: 2.660ms, per_image_inference_time: 0.139ms
2022-02-25 20:21:37 - until epoch: 023, best_acc1: 45.598%
2022-02-25 20:21:37 - epoch 024 lr: 0.1
2022-02-25 20:22:14 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.6605
2022-02-25 20:22:50 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.6933
2022-02-25 20:23:22 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.7142
2022-02-25 20:23:55 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.7408
2022-02-25 20:24:28 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.7372
2022-02-25 20:25:02 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.3812
2022-02-25 20:25:35 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.5702
2022-02-25 20:26:09 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.5706
2022-02-25 20:26:42 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.6747
2022-02-25 20:27:16 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.6262
2022-02-25 20:27:49 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.3776
2022-02-25 20:28:21 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.4857
2022-02-25 20:28:54 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 3.0287
2022-02-25 20:29:27 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.4813
2022-02-25 20:30:00 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.9197
2022-02-25 20:30:33 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.6977
2022-02-25 20:31:06 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.7374
2022-02-25 20:31:40 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.7262
2022-02-25 20:32:12 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.5103
2022-02-25 20:32:45 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.7607
2022-02-25 20:33:18 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.6114
2022-02-25 20:33:51 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.5562
2022-02-25 20:34:25 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.6210
2022-02-25 20:34:57 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.6295
2022-02-25 20:35:30 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.7151
2022-02-25 20:36:03 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.6411
2022-02-25 20:36:36 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.7107
2022-02-25 20:37:09 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.8476
2022-02-25 20:37:41 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.7855
2022-02-25 20:38:15 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.4919
2022-02-25 20:38:48 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.6302
2022-02-25 20:39:21 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.8356
2022-02-25 20:39:55 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.3813
2022-02-25 20:40:27 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.6733
2022-02-25 20:41:00 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.5608
2022-02-25 20:41:32 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.7058
2022-02-25 20:42:05 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.7000
2022-02-25 20:42:39 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.9468
2022-02-25 20:43:12 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.6077
2022-02-25 20:43:45 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.6034
2022-02-25 20:44:18 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.6559
2022-02-25 20:44:51 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.7538
2022-02-25 20:45:24 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.6589
2022-02-25 20:45:58 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.7042
2022-02-25 20:46:32 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.4725
2022-02-25 20:47:05 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.7417
2022-02-25 20:47:40 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.6065
2022-02-25 20:48:13 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.5463
2022-02-25 20:48:48 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.7487
2022-02-25 20:49:20 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.8406
2022-02-25 20:49:22 - train: epoch 024, train_loss: 2.6822
2022-02-25 20:50:37 - eval: epoch: 024, acc1: 43.462%, acc5: 69.670%, test_loss: 2.5452, per_image_load_time: 2.783ms, per_image_inference_time: 0.127ms
2022-02-25 20:50:37 - until epoch: 024, best_acc1: 45.598%
2022-02-25 20:50:37 - epoch 025 lr: 0.1
2022-02-25 20:51:17 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.5848
2022-02-25 20:51:51 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.3846
2022-02-25 20:52:24 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.5031
2022-02-25 20:52:59 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.6975
2022-02-25 20:53:32 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.5560
2022-02-25 20:54:06 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.6667
2022-02-25 20:54:39 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.7054
2022-02-25 20:55:12 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.6624
2022-02-25 20:55:45 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.5022
2022-02-25 20:56:20 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.6032
2022-02-25 20:56:52 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.7609
2022-02-25 20:57:26 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.6132
2022-02-25 20:57:59 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.6065
2022-02-25 20:58:34 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.7096
2022-02-25 20:59:07 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.6344
2022-02-25 20:59:40 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.4444
2022-02-25 21:00:13 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.7313
2022-02-25 21:00:48 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.4651
2022-02-25 21:01:20 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.5900
2022-02-25 21:01:54 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.7095
2022-02-25 21:02:26 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.5234
2022-02-25 21:02:59 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.5344
2022-02-25 21:03:33 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.7117
2022-02-25 21:04:06 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.4677
2022-02-25 21:04:39 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.7272
2022-02-25 21:05:13 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.8633
2022-02-25 21:05:48 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.7600
2022-02-25 21:06:20 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.5432
2022-02-25 21:06:54 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.8340
2022-02-25 21:07:27 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.7862
2022-02-25 21:08:00 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.6200
2022-02-25 21:08:32 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.8297
2022-02-25 21:09:05 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.7439
2022-02-25 21:09:39 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.7692
2022-02-25 21:10:12 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.4228
2022-02-25 21:10:44 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.7749
2022-02-25 21:11:18 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.5798
2022-02-25 21:11:50 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.6166
2022-02-25 21:12:24 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.9509
2022-02-25 21:12:56 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.7228
2022-02-25 21:13:29 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.8660
2022-02-25 21:14:02 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.7873
2022-02-25 21:14:36 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.5215
2022-02-25 21:15:08 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.6338
2022-02-25 21:15:41 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.6797
2022-02-25 21:16:15 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.6232
2022-02-25 21:16:47 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.6444
2022-02-25 21:17:21 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.3990
2022-02-25 21:17:54 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.6472
2022-02-25 21:18:26 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.7817
2022-02-25 21:18:27 - train: epoch 025, train_loss: 2.6793
2022-02-25 21:19:42 - eval: epoch: 025, acc1: 44.668%, acc5: 70.570%, test_loss: 2.5036, per_image_load_time: 2.792ms, per_image_inference_time: 0.137ms
2022-02-25 21:19:43 - until epoch: 025, best_acc1: 45.598%
2022-02-25 21:19:43 - epoch 026 lr: 0.1
2022-02-25 21:20:20 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.5462
2022-02-25 21:20:54 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.5185
2022-02-25 21:21:28 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.6568
2022-02-25 21:22:01 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.6462
2022-02-25 21:22:33 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.7749
2022-02-25 21:23:06 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.7805
2022-02-25 21:23:38 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.5656
2022-02-25 21:24:12 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.4611
2022-02-25 21:24:45 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.8580
2022-02-25 21:25:18 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.5733
2022-02-25 21:25:52 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.5785
2022-02-25 21:26:24 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.7673
2022-02-25 21:26:57 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.5592
2022-02-25 21:27:29 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.7434
2022-02-25 21:28:02 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.6149
2022-02-25 21:28:34 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.7900
2022-02-25 21:29:07 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.6503
2022-02-25 21:29:41 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.8581
2022-02-25 21:30:15 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 3.0489
2022-02-25 21:30:48 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.8307
2022-02-25 21:31:22 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.7361
2022-02-25 21:31:56 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.6044
2022-02-25 21:32:29 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.6941
2022-02-25 21:33:04 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.8466
2022-02-25 21:33:37 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.8911
2022-02-25 21:34:12 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.7349
2022-02-25 21:34:44 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.6984
2022-02-25 21:35:18 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.4845
2022-02-25 21:35:51 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.7108
2022-02-25 21:36:25 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.6242
2022-02-25 21:36:58 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.5617
2022-02-25 21:37:33 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.7126
2022-02-25 21:38:05 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.5806
2022-02-25 21:38:38 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.8171
2022-02-25 21:39:11 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.6832
2022-02-25 21:39:45 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.6003
2022-02-25 21:40:19 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.8537
2022-02-25 21:40:52 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.8965
2022-02-25 21:41:28 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.7812
2022-02-25 21:42:01 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.9052
2022-02-25 21:42:34 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.7038
2022-02-25 21:43:08 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.6829
2022-02-25 21:43:40 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.7771
2022-02-25 21:44:14 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.9491
2022-02-25 21:44:48 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.8992
2022-02-25 21:45:22 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.5517
2022-02-25 21:45:55 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.5739
2022-02-25 21:46:29 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.5116
2022-02-25 21:47:03 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.7466
2022-02-25 21:47:35 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.7255
2022-02-25 21:47:36 - train: epoch 026, train_loss: 2.6759
2022-02-25 21:48:52 - eval: epoch: 026, acc1: 45.022%, acc5: 70.908%, test_loss: 2.4604, per_image_load_time: 1.450ms, per_image_inference_time: 0.147ms
2022-02-25 21:48:52 - until epoch: 026, best_acc1: 45.598%
2022-02-25 21:48:52 - epoch 027 lr: 0.1
2022-02-25 21:49:30 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.7711
2022-02-25 21:50:05 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.5246
2022-02-25 21:50:39 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.7614
2022-02-25 21:51:11 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.7386
2022-02-25 21:51:44 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.7140
2022-02-25 21:52:18 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.8564
2022-02-25 21:52:50 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.8727
2022-02-25 21:53:24 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.7401
2022-02-25 21:53:56 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.6442
2022-02-25 21:54:29 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.7643
2022-02-25 21:55:01 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.5638
2022-02-25 21:55:34 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.9207
2022-02-25 21:56:07 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.7889
2022-02-25 21:56:40 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.9831
2022-02-25 21:57:13 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.6578
2022-02-25 21:57:47 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.6676
2022-02-25 21:58:19 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.6874
2022-02-25 21:58:54 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.6947
2022-02-25 21:59:26 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.8560
2022-02-25 22:00:00 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.6209
2022-02-25 22:00:33 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.9807
2022-02-25 22:01:06 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.8238
2022-02-25 22:01:39 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.9531
2022-02-25 22:02:13 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.5624
2022-02-25 22:02:47 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.7403
2022-02-25 22:03:21 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.8818
2022-02-25 22:03:54 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.6928
2022-02-25 22:04:27 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.7019
2022-02-25 22:05:00 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.6083
2022-02-25 22:05:34 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.6154
2022-02-25 22:06:07 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.4776
2022-02-25 22:06:42 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.5638
2022-02-25 22:07:14 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.7600
2022-02-25 22:07:48 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.6963
2022-02-25 22:08:21 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.9027
2022-02-25 22:08:55 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.5865
2022-02-25 22:09:27 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.6208
2022-02-25 22:10:02 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.4728
2022-02-25 22:10:34 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.5215
2022-02-25 22:11:09 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.7357
2022-02-25 22:11:42 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.5876
2022-02-25 22:12:17 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.7357
2022-02-25 22:12:51 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.6033
2022-02-25 22:13:25 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.7562
2022-02-25 22:14:00 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.6530
2022-02-25 22:14:35 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.7220
2022-02-25 22:15:09 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.6543
2022-02-25 22:15:45 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.8607
2022-02-25 22:16:22 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.7610
2022-02-25 22:16:57 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.3107
2022-02-25 22:16:58 - train: epoch 027, train_loss: 2.6728
2022-02-25 22:18:14 - eval: epoch: 027, acc1: 45.314%, acc5: 71.196%, test_loss: 2.4505, per_image_load_time: 2.812ms, per_image_inference_time: 0.133ms
2022-02-25 22:18:14 - until epoch: 027, best_acc1: 45.598%
2022-02-25 23:13:07 - epoch 028 lr: 0.1
2022-02-25 23:13:46 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.6120
2022-02-25 23:14:20 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.7062
2022-02-25 23:14:52 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.7160
2022-02-25 23:15:27 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.6344
2022-02-25 23:15:59 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.4778
2022-02-25 23:16:33 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.7130
2022-02-25 23:17:06 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 3.0152
2022-02-25 23:17:39 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.5422
2022-02-25 23:18:12 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.6093
2022-02-25 23:18:46 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.4634
2022-02-25 23:19:18 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.6509
2022-02-25 23:19:52 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.6641
2022-02-25 23:20:24 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.7788
2022-02-25 23:20:58 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.0261
2022-02-25 23:21:30 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.5582
2022-02-25 23:22:04 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.8105
2022-02-25 23:22:37 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.7436
2022-02-25 23:23:09 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.6899
2022-02-25 23:23:42 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.6168
2022-02-25 23:24:14 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.6365
2022-02-25 23:24:47 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.8555
2022-02-25 23:25:20 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.9696
2022-02-25 23:25:53 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.8596
2022-02-25 23:26:26 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.9138
2022-02-25 23:26:59 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.7665
2022-02-25 23:27:31 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.7377
2022-02-25 23:28:05 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.6986
2022-02-25 23:28:37 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.6561
2022-02-25 23:29:11 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.6308
2022-02-25 23:29:43 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.6591
2022-02-25 23:30:17 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.9493
2022-02-25 23:30:50 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.5143
2022-02-25 23:31:23 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.6157
2022-02-25 23:31:57 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.7321
2022-02-25 23:32:30 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.6694
2022-02-25 23:33:03 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.7290
2022-02-25 23:33:36 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.9276
2022-02-25 23:34:08 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.1537
2022-02-25 23:34:42 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.8762
2022-02-25 23:35:15 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.8816
2022-02-25 23:35:48 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.7205
2022-02-25 23:36:21 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.8884
2022-02-25 23:36:55 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.4917
2022-02-25 23:37:28 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.6188
2022-02-25 23:38:01 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.7218
2022-02-25 23:38:34 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.8575
2022-02-25 23:39:07 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.9267
2022-02-25 23:39:42 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.7612
2022-02-25 23:40:14 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.5913
2022-02-25 23:40:47 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.4870
2022-02-25 23:40:48 - train: epoch 028, train_loss: 2.6686
2022-02-25 23:42:05 - eval: epoch: 028, acc1: 45.234%, acc5: 71.012%, test_loss: 2.4707, per_image_load_time: 2.863ms, per_image_inference_time: 0.117ms
2022-02-25 23:42:05 - until epoch: 028, best_acc1: 45.598%
2022-02-25 23:42:05 - epoch 029 lr: 0.1
2022-02-25 23:42:43 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.6360
2022-02-25 23:43:17 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.6726
2022-02-25 23:43:50 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.6977
2022-02-25 23:44:24 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.7310
2022-02-25 23:44:57 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.5894
2022-02-25 23:45:30 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.7001
2022-02-25 23:46:02 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.5878
2022-02-25 23:46:36 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.6838
2022-02-25 23:47:09 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.6535
2022-02-25 23:47:42 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.8394
2022-02-25 23:48:16 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.5063
2022-02-25 23:48:49 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.7787
2022-02-25 23:49:23 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.6683
2022-02-25 23:49:55 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.7900
2022-02-25 23:50:29 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.8488
2022-02-25 23:51:01 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.9354
2022-02-25 23:51:35 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.5699
2022-02-25 23:52:08 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.7441
2022-02-25 23:52:42 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.5046
2022-02-25 23:53:15 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.5429
2022-02-25 23:53:49 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.5397
2022-02-25 23:54:22 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.7244
2022-02-25 23:54:55 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.4045
2022-02-25 23:55:27 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.5629
2022-02-25 23:56:01 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.6896
2022-02-25 23:56:33 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.7512
2022-02-25 23:57:08 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.5612
2022-02-25 23:57:40 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.6059
2022-02-25 23:58:14 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.3932
2022-02-25 23:58:47 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.4412
2022-02-25 23:59:21 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.6490
2022-02-25 23:59:54 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.6765
2022-02-26 00:00:28 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.6026
2022-02-26 00:01:00 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.4995
2022-02-26 00:01:35 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 3.0250
2022-02-26 00:02:08 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.5338
2022-02-26 00:02:41 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.5468
2022-02-26 00:03:14 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.4541
2022-02-26 00:03:48 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.5304
2022-02-26 00:04:21 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.8545
2022-02-26 00:04:55 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.6186
2022-02-26 00:05:28 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.6498
2022-02-26 00:06:02 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.5950
2022-02-26 00:06:35 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.8574
2022-02-26 00:07:09 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.8767
2022-02-26 00:07:42 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.7752
2022-02-26 00:08:15 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.4804
2022-02-26 00:08:48 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.7280
2022-02-26 00:09:23 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.9073
2022-02-26 00:09:56 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.4390
2022-02-26 00:09:56 - train: epoch 029, train_loss: 2.6664
2022-02-26 00:11:11 - eval: epoch: 029, acc1: 45.144%, acc5: 70.946%, test_loss: 2.4607, per_image_load_time: 2.758ms, per_image_inference_time: 0.118ms
2022-02-26 00:11:11 - until epoch: 029, best_acc1: 45.598%
2022-02-26 00:11:11 - epoch 030 lr: 0.1
2022-02-26 00:11:49 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.6917
2022-02-26 00:12:23 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.6759
2022-02-26 00:12:56 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.5220
2022-02-26 00:13:30 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.4800
2022-02-26 00:14:02 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.6077
2022-02-26 00:14:35 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.6492
2022-02-26 00:15:09 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.6618
2022-02-26 00:15:41 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.5866
2022-02-26 00:16:15 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.6102
2022-02-26 00:16:47 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.5972
2022-02-26 00:17:21 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.5954
2022-02-26 00:17:54 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.5680
2022-02-26 00:18:27 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.4580
2022-02-26 00:19:01 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.5229
2022-02-26 00:19:34 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.4875
2022-02-26 00:20:08 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.7050
2022-02-26 00:20:41 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.9796
2022-02-26 00:21:13 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.6536
2022-02-26 00:21:48 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.6947
2022-02-26 00:22:21 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.5181
2022-02-26 00:22:55 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.7672
2022-02-26 00:23:27 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.7365
2022-02-26 00:24:02 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.4573
2022-02-26 00:24:34 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.7470
2022-02-26 00:25:08 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.8275
2022-02-26 00:25:40 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.7634
2022-02-26 00:26:14 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.5426
2022-02-26 00:26:47 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.5986
2022-02-26 00:27:21 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.4892
2022-02-26 00:27:54 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.8507
2022-02-26 00:28:27 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.7718
2022-02-26 00:28:59 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.5845
2022-02-26 00:29:34 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.8913
2022-02-26 00:30:06 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.4950
2022-02-26 00:30:41 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.6008
2022-02-26 00:31:14 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.6283
2022-02-26 00:31:48 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.4935
2022-02-26 00:32:20 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.5328
2022-02-26 00:32:54 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.4279
2022-02-26 00:33:28 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.5127
2022-02-26 00:34:02 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.6219
2022-02-26 00:34:35 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.6897
2022-02-26 00:35:09 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.6354
2022-02-26 00:35:41 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.6348
2022-02-26 00:36:16 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.5636
2022-02-26 00:36:49 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.4197
2022-02-26 00:37:24 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.6599
2022-02-26 00:37:56 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.6741
2022-02-26 00:38:31 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.5931
2022-02-26 00:39:04 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.6633
2022-02-26 00:39:05 - train: epoch 030, train_loss: 2.6632
2022-02-26 00:40:19 - eval: epoch: 030, acc1: 44.292%, acc5: 70.554%, test_loss: 2.4967, per_image_load_time: 2.014ms, per_image_inference_time: 0.134ms
2022-02-26 00:40:20 - until epoch: 030, best_acc1: 45.598%
2022-02-26 00:40:20 - epoch 031 lr: 0.010000000000000002
2022-02-26 00:40:58 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.3693
2022-02-26 00:41:33 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.3193
2022-02-26 00:42:05 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.2902
2022-02-26 00:42:38 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.2616
2022-02-26 00:43:12 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.2821
2022-02-26 00:43:45 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.1627
2022-02-26 00:44:19 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.2554
2022-02-26 00:44:52 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.2699
2022-02-26 00:45:25 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.2004
2022-02-26 00:45:59 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.2986
2022-02-26 00:46:32 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.1244
2022-02-26 00:47:05 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.2987
2022-02-26 00:47:39 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.8874
2022-02-26 00:48:12 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.1554
2022-02-26 00:48:46 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.2577
2022-02-26 00:49:20 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.2239
2022-02-26 00:49:53 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.1043
2022-02-26 00:50:27 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.0826
2022-02-26 00:51:00 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.1702
2022-02-26 00:51:33 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.3201
2022-02-26 00:52:07 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.1272
2022-02-26 00:52:40 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.9368
2022-02-26 00:53:14 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.0919
2022-02-26 00:53:47 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.0916
2022-02-26 00:54:21 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.1982
2022-02-26 00:54:54 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.1386
2022-02-26 00:55:28 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.3498
2022-02-26 00:56:01 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.6966
2022-02-26 00:56:35 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.3501
2022-02-26 00:57:07 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.5033
2022-02-26 00:57:41 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.2935
2022-02-26 00:58:14 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.2824
2022-02-26 00:58:48 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.1532
2022-02-26 00:59:21 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.3824
2022-02-26 00:59:55 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.1997
2022-02-26 01:00:27 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.9597
2022-02-26 01:01:01 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.2482
2022-02-26 01:01:34 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.0007
2022-02-26 01:02:08 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.2960
2022-02-26 01:02:41 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.1238
2022-02-26 01:03:15 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.0456
2022-02-26 01:03:49 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.1164
2022-02-26 01:04:25 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.1150
2022-02-26 01:05:00 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.2001
2022-02-26 01:05:33 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.0479
2022-02-26 01:06:07 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.3576
2022-02-26 01:06:40 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.0294
2022-02-26 01:07:14 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.1052
2022-02-26 01:07:48 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.3118
2022-02-26 01:08:20 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.3813
2022-02-26 01:08:21 - train: epoch 031, train_loss: 2.2154
2022-02-26 01:09:36 - eval: epoch: 031, acc1: 58.384%, acc5: 81.262%, test_loss: 1.7983, per_image_load_time: 1.503ms, per_image_inference_time: 0.116ms
2022-02-26 01:09:36 - until epoch: 031, best_acc1: 58.384%
2022-02-26 01:09:36 - epoch 032 lr: 0.010000000000000002
2022-02-26 01:10:15 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.9721
2022-02-26 01:10:48 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.2032
2022-02-26 01:11:21 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.9471
2022-02-26 01:11:54 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.2858
2022-02-26 01:12:27 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.1320
2022-02-26 01:12:59 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.1776
2022-02-26 01:13:33 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.1140
2022-02-26 01:14:05 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.1626
2022-02-26 01:14:38 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.1455
2022-02-26 01:15:11 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.3555
2022-02-26 01:15:44 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.2651
2022-02-26 01:16:18 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.3776
2022-02-26 01:16:50 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.1029
2022-02-26 01:17:24 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.4745
2022-02-26 01:17:57 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.1262
2022-02-26 01:18:31 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.4009
2022-02-26 01:19:03 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.2010
2022-02-26 01:19:36 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.4822
2022-02-26 01:20:09 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.9731
2022-02-26 01:20:44 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.0113
2022-02-26 01:21:16 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.9106
2022-02-26 01:21:49 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.0117
2022-02-26 01:22:23 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.2133
2022-02-26 01:22:56 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.0047
2022-02-26 01:23:29 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.1050
2022-02-26 01:24:02 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.9599
2022-02-26 01:24:35 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.0878
2022-02-26 01:25:09 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.1529
2022-02-26 01:25:42 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 2.1305
2022-02-26 01:26:15 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 2.0502
2022-02-26 01:26:47 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.2593
2022-02-26 01:27:21 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.1808
2022-02-26 01:27:54 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 2.0398
2022-02-26 01:28:28 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.8058
2022-02-26 01:29:00 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.1646
2022-02-26 01:29:34 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.0981
2022-02-26 01:30:06 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.1308
2022-02-26 01:30:40 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 2.0193
2022-02-26 01:31:13 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 2.0035
2022-02-26 01:31:47 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 2.1319
2022-02-26 01:32:18 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.0833
2022-02-26 01:32:52 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.9413
2022-02-26 01:33:24 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.2595
2022-02-26 01:33:58 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 2.0507
2022-02-26 01:34:30 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.2911
2022-02-26 01:35:06 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 2.0940
2022-02-26 01:35:39 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.2939
2022-02-26 01:36:13 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 2.0318
2022-02-26 01:36:46 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.0914
2022-02-26 01:37:19 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.0807
2022-02-26 01:37:19 - train: epoch 032, train_loss: 2.1217
2022-02-26 01:38:35 - eval: epoch: 032, acc1: 58.928%, acc5: 81.808%, test_loss: 1.7599, per_image_load_time: 2.796ms, per_image_inference_time: 0.125ms
2022-02-26 01:38:35 - until epoch: 032, best_acc1: 58.928%
2022-02-26 01:38:35 - epoch 033 lr: 0.010000000000000002
2022-02-26 01:39:15 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 2.0285
2022-02-26 01:39:47 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.3329
2022-02-26 01:40:21 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.9784
2022-02-26 01:40:53 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.9748
2022-02-26 01:41:26 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.2464
2022-02-26 01:41:59 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.8392
2022-02-26 01:42:33 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.4475
2022-02-26 01:43:06 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.2069
2022-02-26 01:43:38 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.3193
2022-02-26 01:44:12 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.2988
2022-02-26 01:44:44 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.8717
2022-02-26 01:45:18 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.1995
2022-02-26 01:45:50 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.8768
2022-02-26 01:46:23 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.1474
2022-02-26 01:46:56 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.2245
2022-02-26 01:47:29 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.2257
2022-02-26 01:48:02 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.9516
2022-02-26 01:48:35 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.0924
2022-02-26 01:49:08 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.9314
2022-02-26 01:49:41 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.8046
2022-02-26 01:50:14 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 2.0428
2022-02-26 01:50:48 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.2470
2022-02-26 01:51:20 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.9391
2022-02-26 01:51:54 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.3421
2022-02-26 01:52:26 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 2.1801
2022-02-26 01:53:00 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.8711
2022-02-26 01:53:33 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.0257
2022-02-26 01:54:07 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.0651
2022-02-26 01:54:39 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.0913
2022-02-26 01:55:13 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.0057
2022-02-26 01:55:46 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.0907
2022-02-26 01:56:20 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.9603
2022-02-26 01:56:52 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 2.1034
2022-02-26 01:57:26 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.8979
2022-02-26 01:57:58 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.1773
2022-02-26 01:58:32 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.3758
2022-02-26 01:59:05 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 2.1025
2022-02-26 01:59:38 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 2.0620
2022-02-26 02:00:11 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.3442
2022-02-26 02:00:45 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.2371
2022-02-26 02:01:18 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.0660
2022-02-26 02:01:52 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.9276
2022-02-26 02:02:24 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.0736
2022-02-26 02:02:57 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.1084
2022-02-26 02:03:31 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.3364
2022-02-26 02:04:04 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.1467
2022-02-26 02:04:37 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.8985
2022-02-26 02:05:11 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.2726
2022-02-26 02:05:45 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.7438
2022-02-26 02:06:16 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.1422
2022-02-26 02:06:17 - train: epoch 033, train_loss: 2.0921
2022-02-26 02:07:32 - eval: epoch: 033, acc1: 59.006%, acc5: 82.042%, test_loss: 1.7489, per_image_load_time: 0.620ms, per_image_inference_time: 0.124ms
2022-02-26 02:07:32 - until epoch: 033, best_acc1: 59.006%
2022-02-26 02:07:32 - epoch 034 lr: 0.010000000000000002
2022-02-26 02:08:10 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 2.0432
2022-02-26 02:08:43 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.9442
2022-02-26 02:09:16 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.9985
2022-02-26 02:09:49 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.8018
2022-02-26 02:10:22 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 2.2329
2022-02-26 02:10:56 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.0948
2022-02-26 02:11:28 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.9605
2022-02-26 02:12:01 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.3607
2022-02-26 02:12:35 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 2.1608
2022-02-26 02:13:08 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 2.1680
2022-02-26 02:13:42 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.0972
2022-02-26 02:14:16 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.1002
2022-02-26 02:14:48 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 2.1031
2022-02-26 02:15:23 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.1284
2022-02-26 02:15:56 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.1129
2022-02-26 02:16:29 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.0431
2022-02-26 02:17:02 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 2.1912
2022-02-26 02:17:36 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.1832
2022-02-26 02:18:10 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.1767
2022-02-26 02:18:43 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.8612
2022-02-26 02:19:16 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.0434
2022-02-26 02:19:50 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.7479
2022-02-26 02:20:23 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.0697
2022-02-26 02:20:56 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.9050
2022-02-26 02:21:30 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.0800
2022-02-26 02:22:02 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.2707
2022-02-26 02:22:36 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.1744
2022-02-26 02:23:09 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.9151
2022-02-26 02:23:42 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.9567
2022-02-26 02:24:16 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.9150
2022-02-26 02:24:48 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.9876
2022-02-26 02:25:22 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.1217
2022-02-26 02:25:56 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.8784
2022-02-26 02:26:29 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.0955
2022-02-26 02:27:03 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.8475
2022-02-26 02:27:36 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.8208
2022-02-26 02:28:10 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 2.1365
2022-02-26 02:28:42 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 2.0028
2022-02-26 02:29:16 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.2569
2022-02-26 02:29:50 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 2.1390
2022-02-26 02:30:22 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.2773
2022-02-26 02:30:56 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.0229
2022-02-26 02:31:28 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.1148
2022-02-26 02:32:03 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.9742
2022-02-26 02:32:36 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.1255
2022-02-26 02:33:10 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.0705
2022-02-26 02:33:43 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.0057
2022-02-26 02:34:16 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.3466
2022-02-26 02:34:51 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.0688
2022-02-26 02:35:23 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.9045
2022-02-26 02:35:24 - train: epoch 034, train_loss: 2.0731
2022-02-26 02:36:40 - eval: epoch: 034, acc1: 59.514%, acc5: 82.208%, test_loss: 1.7298, per_image_load_time: 1.875ms, per_image_inference_time: 0.130ms
2022-02-26 02:36:40 - until epoch: 034, best_acc1: 59.514%
2022-02-26 02:36:40 - epoch 035 lr: 0.010000000000000002
2022-02-26 02:37:19 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.8856
2022-02-26 02:37:51 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.8220
2022-02-26 02:38:25 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.2877
2022-02-26 02:38:57 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.1161
2022-02-26 02:39:30 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 2.1143
2022-02-26 02:40:03 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.1638
2022-02-26 02:40:36 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.9656
2022-02-26 02:41:09 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 2.1460
2022-02-26 02:41:42 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.1712
2022-02-26 02:42:15 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.1429
2022-02-26 02:42:48 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.1079
2022-02-26 02:43:21 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.8267
2022-02-26 02:43:53 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.9908
2022-02-26 02:44:27 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.0157
2022-02-26 02:44:59 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 2.0702
2022-02-26 02:45:34 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 2.0797
2022-02-26 02:46:06 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.9021
2022-02-26 02:46:40 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.2372
2022-02-26 02:47:12 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.9320
2022-02-26 02:47:47 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.2372
2022-02-26 02:48:19 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 2.0827
2022-02-26 02:48:53 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 2.0168
2022-02-26 02:49:25 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 2.1584
2022-02-26 02:49:59 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.0207
2022-02-26 02:50:32 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.1179
2022-02-26 02:51:06 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.1685
2022-02-26 02:51:38 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.9646
2022-02-26 02:52:12 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.1890
2022-02-26 02:52:44 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.9379
2022-02-26 02:53:19 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.7933
2022-02-26 02:53:51 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.3149
2022-02-26 02:54:25 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 2.0384
2022-02-26 02:54:57 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 2.0786
2022-02-26 02:55:31 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.1060
2022-02-26 02:56:03 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.8258
2022-02-26 02:56:38 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 2.0180
2022-02-26 02:57:10 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 2.1044
2022-02-26 02:57:44 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 2.0356
2022-02-26 02:58:16 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.1907
2022-02-26 02:58:50 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.7990
2022-02-26 02:59:23 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.2757
2022-02-26 02:59:57 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.8521
2022-02-26 03:00:29 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.2085
2022-02-26 03:01:03 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.1930
2022-02-26 03:01:36 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.2457
2022-02-26 03:02:10 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.9665
2022-02-26 03:02:42 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.1235
2022-02-26 03:03:17 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.9951
2022-02-26 03:03:50 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.2011
2022-02-26 03:04:22 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.9879
2022-02-26 03:04:22 - train: epoch 035, train_loss: 2.0645
2022-02-26 03:05:37 - eval: epoch: 035, acc1: 59.552%, acc5: 82.446%, test_loss: 1.7260, per_image_load_time: 0.866ms, per_image_inference_time: 0.120ms
2022-02-26 03:05:37 - until epoch: 035, best_acc1: 59.552%
2022-02-26 03:05:37 - epoch 036 lr: 0.010000000000000002
2022-02-26 03:06:16 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.0920
2022-02-26 03:06:49 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.9018
2022-02-26 03:07:22 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.8966
2022-02-26 03:07:56 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 2.0493
2022-02-26 03:08:28 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.9191
2022-02-26 03:09:02 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.9304
2022-02-26 03:09:34 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 2.0552
2022-02-26 03:10:08 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.8749
2022-02-26 03:10:40 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 2.1145
2022-02-26 03:11:14 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.8482
2022-02-26 03:11:46 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.9445
2022-02-26 03:12:20 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 2.0683
2022-02-26 03:12:53 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.0718
2022-02-26 03:13:27 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.0512
2022-02-26 03:14:00 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.1839
2022-02-26 03:14:34 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.2561
2022-02-26 03:15:07 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 2.1369
2022-02-26 03:15:40 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.9210
2022-02-26 03:16:13 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.2272
2022-02-26 03:16:46 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.9858
2022-02-26 03:17:19 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 2.0227
2022-02-26 03:17:52 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.1512
2022-02-26 03:18:25 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 2.1071
2022-02-26 03:18:58 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.3563
2022-02-26 03:19:30 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.9510
2022-02-26 03:20:05 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.2134
2022-02-26 03:20:37 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 2.1190
2022-02-26 03:21:11 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 2.0216
2022-02-26 03:21:44 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.9121
2022-02-26 03:22:18 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.8129
2022-02-26 03:22:50 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.1158
2022-02-26 03:23:24 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.1538
2022-02-26 03:23:57 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.8468
2022-02-26 03:24:31 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.9310
2022-02-26 03:25:03 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.2463
2022-02-26 03:25:36 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.2745
2022-02-26 03:26:09 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 2.0546
2022-02-26 03:26:43 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 2.1036
2022-02-26 03:27:16 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.0061
2022-02-26 03:27:50 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.1725
2022-02-26 03:28:23 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.9070
2022-02-26 03:28:57 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.0856
2022-02-26 03:29:29 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 2.1307
2022-02-26 03:30:03 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.0441
2022-02-26 03:30:36 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.8314
2022-02-26 03:31:10 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 2.0327
2022-02-26 03:31:43 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.8966
2022-02-26 03:32:18 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 2.0965
2022-02-26 03:32:50 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.9581
2022-02-26 03:33:23 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 2.0744
2022-02-26 03:33:24 - train: epoch 036, train_loss: 2.0553
2022-02-26 03:34:39 - eval: epoch: 036, acc1: 59.476%, acc5: 82.342%, test_loss: 1.7241, per_image_load_time: 0.746ms, per_image_inference_time: 0.126ms
2022-02-26 03:34:39 - until epoch: 036, best_acc1: 59.552%
2022-02-26 03:34:39 - epoch 037 lr: 0.010000000000000002
2022-02-26 03:35:18 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.8531
2022-02-26 03:35:51 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.7686
2022-02-26 03:36:25 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.9374
2022-02-26 03:36:58 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.7113
2022-02-26 03:37:31 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.1400
2022-02-26 03:38:03 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.3920
2022-02-26 03:38:37 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.0413
2022-02-26 03:39:10 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 2.0857
2022-02-26 03:39:44 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.2386
2022-02-26 03:40:16 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.9242
2022-02-26 03:40:49 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.2170
2022-02-26 03:41:22 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.1920
2022-02-26 03:41:55 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.0981
2022-02-26 03:42:28 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.4354
2022-02-26 03:43:01 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.8809
2022-02-26 03:43:34 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.9446
2022-02-26 03:44:07 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.0718
2022-02-26 03:44:41 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.1773
2022-02-26 03:45:13 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 2.0431
2022-02-26 03:45:46 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.1916
2022-02-26 03:46:19 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 2.0841
2022-02-26 03:46:53 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.8617
2022-02-26 03:47:25 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 2.0396
2022-02-26 03:47:58 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 2.0097
2022-02-26 03:48:32 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 2.1877
2022-02-26 03:49:05 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.1141
2022-02-26 03:49:39 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.1524
2022-02-26 03:50:11 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 2.1452
2022-02-26 03:50:45 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 2.1464
2022-02-26 03:51:17 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.2267
2022-02-26 03:51:51 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.8883
2022-02-26 03:52:23 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.9671
2022-02-26 03:52:57 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 2.1215
2022-02-26 03:53:30 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.8714
2022-02-26 03:54:04 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 2.0071
2022-02-26 03:54:37 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.9900
2022-02-26 03:55:11 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 2.1256
2022-02-26 03:55:43 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.1383
2022-02-26 03:56:16 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.0931
2022-02-26 03:56:49 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.9806
2022-02-26 03:57:23 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 2.0402
2022-02-26 03:57:55 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.0545
2022-02-26 03:58:29 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.2343
2022-02-26 03:59:02 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.9006
2022-02-26 03:59:35 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.8772
2022-02-26 04:00:08 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 2.2000
2022-02-26 04:00:42 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.9600
2022-02-26 04:01:16 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.8164
2022-02-26 04:01:50 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 2.0773
2022-02-26 04:02:22 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 2.0324
2022-02-26 04:02:23 - train: epoch 037, train_loss: 2.0569
2022-02-26 04:03:39 - eval: epoch: 037, acc1: 59.884%, acc5: 82.514%, test_loss: 1.7201, per_image_load_time: 2.120ms, per_image_inference_time: 0.117ms
2022-02-26 04:03:40 - until epoch: 037, best_acc1: 59.884%
2022-02-26 04:03:40 - epoch 038 lr: 0.010000000000000002
2022-02-26 04:04:18 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.9883
2022-02-26 04:04:51 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.9793
2022-02-26 04:05:23 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.8067
2022-02-26 04:05:57 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.9762
2022-02-26 04:06:30 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 2.0205
2022-02-26 04:07:03 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.2491
2022-02-26 04:07:35 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.8438
2022-02-26 04:08:10 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.9377
2022-02-26 04:08:42 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.0767
2022-02-26 04:09:15 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.2886
2022-02-26 04:09:48 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.9198
2022-02-26 04:10:22 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 2.1418
2022-02-26 04:10:54 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.1611
2022-02-26 04:11:28 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 2.0852
2022-02-26 04:12:00 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.8620
2022-02-26 04:12:34 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.1749
2022-02-26 04:13:07 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.0982
2022-02-26 04:13:41 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.9393
2022-02-26 04:14:13 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.9535
2022-02-26 04:14:47 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 2.0426
2022-02-26 04:15:19 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 2.0879
2022-02-26 04:15:53 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.7770
2022-02-26 04:16:26 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.3133
2022-02-26 04:16:59 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.3823
2022-02-26 04:17:32 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.9327
2022-02-26 04:18:06 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.9048
2022-02-26 04:18:39 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.9554
2022-02-26 04:19:12 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.4505
2022-02-26 04:19:45 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 2.1367
2022-02-26 04:20:19 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.9218
2022-02-26 04:20:51 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.9813
2022-02-26 04:21:25 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.8863
2022-02-26 04:21:58 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.9424
2022-02-26 04:22:32 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.9115
2022-02-26 04:23:04 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.0326
2022-02-26 04:23:38 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.1150
2022-02-26 04:24:11 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 2.0535
2022-02-26 04:24:44 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.2332
2022-02-26 04:25:16 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 2.0285
2022-02-26 04:25:50 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 2.0765
2022-02-26 04:26:23 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.8704
2022-02-26 04:26:56 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.8840
2022-02-26 04:27:29 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.0899
2022-02-26 04:28:03 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 2.2701
2022-02-26 04:28:35 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.0842
2022-02-26 04:29:09 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.0306
2022-02-26 04:29:42 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.9081
2022-02-26 04:30:16 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.8504
2022-02-26 04:30:48 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 2.0500
2022-02-26 04:31:21 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.8897
2022-02-26 04:31:21 - train: epoch 038, train_loss: 2.0549
2022-02-26 04:32:37 - eval: epoch: 038, acc1: 59.574%, acc5: 82.424%, test_loss: 1.7268, per_image_load_time: 1.376ms, per_image_inference_time: 0.133ms
2022-02-26 04:32:37 - until epoch: 038, best_acc1: 59.884%
2022-02-26 04:32:37 - epoch 039 lr: 0.010000000000000002
2022-02-26 04:33:15 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.9918
2022-02-26 04:33:48 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.2459
2022-02-26 04:34:20 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.9658
2022-02-26 04:34:53 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 2.1429
2022-02-26 04:35:26 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.1199
2022-02-26 04:35:59 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 2.0219
2022-02-26 04:36:32 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.2069
2022-02-26 04:37:04 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.9297
2022-02-26 04:37:37 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.3136
2022-02-26 04:38:10 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.8636
2022-02-26 04:38:43 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.2233
2022-02-26 04:39:17 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.1322
2022-02-26 04:39:49 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.0601
2022-02-26 04:40:23 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.2808
2022-02-26 04:40:56 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.8707
2022-02-26 04:41:30 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.9521
2022-02-26 04:42:03 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.9036
2022-02-26 04:42:36 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 2.0517
2022-02-26 04:43:09 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.8156
2022-02-26 04:43:42 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 2.0084
2022-02-26 04:44:16 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.2201
2022-02-26 04:44:49 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 2.1123
2022-02-26 04:45:22 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.2132
2022-02-26 04:45:55 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.1497
2022-02-26 04:46:29 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.9228
2022-02-26 04:47:01 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 2.0039
2022-02-26 04:47:35 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.9497
2022-02-26 04:48:08 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 2.1249
2022-02-26 04:48:41 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.8914
2022-02-26 04:49:15 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.2835
2022-02-26 04:49:48 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.9014
2022-02-26 04:50:21 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.0533
2022-02-26 04:50:54 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 2.1493
2022-02-26 04:51:26 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 2.2493
2022-02-26 04:52:00 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.3507
2022-02-26 04:52:33 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.0614
2022-02-26 04:53:06 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.9338
2022-02-26 04:53:39 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.8608
2022-02-26 04:54:12 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 2.2609
2022-02-26 04:54:46 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.9281
2022-02-26 04:55:19 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.9271
2022-02-26 04:55:52 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 2.2416
2022-02-26 04:56:26 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.9473
2022-02-26 04:56:59 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.9107
2022-02-26 04:57:33 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.8577
2022-02-26 04:58:05 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.1880
2022-02-26 04:58:39 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 2.1958
2022-02-26 04:59:12 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.9778
2022-02-26 04:59:47 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 2.0858
2022-02-26 05:00:18 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 2.1121
2022-02-26 05:00:19 - train: epoch 039, train_loss: 2.0550
2022-02-26 05:01:34 - eval: epoch: 039, acc1: 59.246%, acc5: 82.112%, test_loss: 1.7374, per_image_load_time: 0.994ms, per_image_inference_time: 0.147ms
2022-02-26 05:01:34 - until epoch: 039, best_acc1: 59.884%
2022-02-26 05:01:34 - epoch 040 lr: 0.010000000000000002
2022-02-26 05:02:13 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.3785
2022-02-26 05:02:45 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.1340
2022-02-26 05:03:20 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.2459
2022-02-26 05:03:51 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.9198
2022-02-26 05:04:24 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.8271
2022-02-26 05:04:57 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 2.1456
2022-02-26 05:05:30 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 2.0057
2022-02-26 05:06:03 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.0986
2022-02-26 05:06:36 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 2.0470
2022-02-26 05:07:10 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.9744
2022-02-26 05:07:42 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 2.0063
2022-02-26 05:08:15 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.9922
2022-02-26 05:08:48 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 2.0378
2022-02-26 05:09:22 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 2.0870
2022-02-26 05:09:54 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.9888
2022-02-26 05:10:28 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 2.0643
2022-02-26 05:11:01 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 2.0393
2022-02-26 05:11:35 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.9434
2022-02-26 05:12:07 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.9612
2022-02-26 05:12:40 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 2.0681
2022-02-26 05:13:14 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.9491
2022-02-26 05:13:46 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.9621
2022-02-26 05:14:20 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.0476
2022-02-26 05:14:53 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 2.3005
2022-02-26 05:15:27 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.1519
2022-02-26 05:15:59 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 2.0085
2022-02-26 05:16:33 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.0800
2022-02-26 05:17:05 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 2.1120
2022-02-26 05:17:40 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.2269
2022-02-26 05:18:12 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 2.0094
2022-02-26 05:18:46 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.2435
2022-02-26 05:19:18 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.1633
2022-02-26 05:19:52 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.1731
2022-02-26 05:20:25 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.9774
2022-02-26 05:20:58 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 2.1660
2022-02-26 05:21:32 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 2.0482
2022-02-26 05:22:04 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.9848
2022-02-26 05:22:38 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 2.0012
2022-02-26 05:23:10 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.0124
2022-02-26 05:23:44 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.0369
2022-02-26 05:24:17 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.8275
2022-02-26 05:24:50 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.8353
2022-02-26 05:25:23 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.9548
2022-02-26 05:25:57 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.8508
2022-02-26 05:26:30 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.8572
2022-02-26 05:27:04 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.0440
2022-02-26 05:27:37 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.1237
2022-02-26 05:28:11 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.9810
2022-02-26 05:28:44 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 2.1599
2022-02-26 05:29:17 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 2.3367
2022-02-26 05:29:18 - train: epoch 040, train_loss: 2.0582
2022-02-26 05:30:33 - eval: epoch: 040, acc1: 58.588%, acc5: 81.646%, test_loss: 1.7717, per_image_load_time: 0.722ms, per_image_inference_time: 0.114ms
2022-02-26 05:30:33 - until epoch: 040, best_acc1: 59.884%
2022-02-26 05:30:33 - epoch 041 lr: 0.010000000000000002
2022-02-26 05:31:12 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.2190
2022-02-26 05:31:44 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.1848
2022-02-26 05:32:18 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 2.1294
2022-02-26 05:32:50 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.0558
2022-02-26 05:33:24 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.8479
2022-02-26 05:33:56 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.1474
2022-02-26 05:34:30 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 2.0003
2022-02-26 05:35:03 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 2.1356
2022-02-26 05:35:37 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.8908
2022-02-26 05:36:10 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.2719
2022-02-26 05:36:43 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.8810
2022-02-26 05:37:15 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.8141
2022-02-26 05:37:49 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.8423
2022-02-26 05:38:22 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.0885
2022-02-26 05:38:56 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.3405
2022-02-26 05:39:28 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 2.0813
2022-02-26 05:40:02 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 2.0834
2022-02-26 05:40:33 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.9477
2022-02-26 05:41:07 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.1008
2022-02-26 05:41:38 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 2.0188
2022-02-26 05:42:12 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.8271
2022-02-26 05:42:43 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.8552
2022-02-26 05:43:16 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.8979
2022-02-26 05:43:49 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.4000
2022-02-26 05:44:21 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 2.1454
2022-02-26 05:44:54 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.1779
2022-02-26 05:45:26 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.0344
2022-02-26 05:46:00 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 2.1197
2022-02-26 05:46:32 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 2.2169
2022-02-26 05:47:05 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.0857
2022-02-26 05:47:37 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.0367
2022-02-26 05:48:11 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 2.1204
2022-02-26 05:48:42 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.8691
2022-02-26 05:49:15 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.7475
2022-02-26 05:49:48 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 2.0811
2022-02-26 05:50:21 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.1724
2022-02-26 05:50:53 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.3084
2022-02-26 05:51:26 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 2.0037
2022-02-26 05:51:58 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.9579
2022-02-26 05:52:33 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.9377
2022-02-26 05:53:05 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.2221
2022-02-26 05:53:40 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.9535
2022-02-26 05:54:12 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.9856
2022-02-26 05:54:46 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.9095
2022-02-26 05:55:18 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 2.0191
2022-02-26 05:55:53 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.1433
2022-02-26 05:56:25 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.1039
2022-02-26 05:57:00 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 2.2235
2022-02-26 05:57:32 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 2.0776
2022-02-26 05:58:05 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.2650
2022-02-26 05:58:06 - train: epoch 041, train_loss: 2.0597
2022-02-26 05:59:21 - eval: epoch: 041, acc1: 59.194%, acc5: 82.106%, test_loss: 1.7462, per_image_load_time: 0.737ms, per_image_inference_time: 0.119ms
2022-02-26 05:59:22 - until epoch: 041, best_acc1: 59.884%
2022-02-26 05:59:22 - epoch 042 lr: 0.010000000000000002
2022-02-26 06:00:01 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.8153
2022-02-26 06:00:33 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.8910
2022-02-26 06:01:06 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.0712
2022-02-26 06:01:39 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.7831
2022-02-26 06:02:11 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 2.1674
2022-02-26 06:02:45 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 2.0522
2022-02-26 06:03:17 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.9639
2022-02-26 06:03:51 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 2.1154
2022-02-26 06:04:23 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.2062
2022-02-26 06:04:57 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.3063
2022-02-26 06:05:30 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.8421
2022-02-26 06:06:03 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.8475
2022-02-26 06:06:36 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 2.0419
2022-02-26 06:07:09 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.1110
2022-02-26 06:07:43 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.9518
2022-02-26 06:08:16 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.3413
2022-02-26 06:08:49 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.0974
2022-02-26 06:09:22 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.8623
2022-02-26 06:09:56 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.1949
2022-02-26 06:10:28 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 2.0758
2022-02-26 06:11:02 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.9606
2022-02-26 06:11:35 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.9665
2022-02-26 06:12:09 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 2.0573
2022-02-26 06:12:42 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.1984
2022-02-26 06:13:15 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 2.1038
2022-02-26 06:13:49 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.0857
2022-02-26 06:14:21 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.8528
2022-02-26 06:14:56 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.9309
2022-02-26 06:15:28 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.9910
2022-02-26 06:16:02 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 2.1821
2022-02-26 06:16:34 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.1333
2022-02-26 06:17:07 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.1278
2022-02-26 06:17:40 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.1138
2022-02-26 06:18:14 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.7771
2022-02-26 06:18:47 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.2389
2022-02-26 06:19:20 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.2461
2022-02-26 06:19:52 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 2.1323
2022-02-26 06:20:28 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.8568
2022-02-26 06:21:00 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 2.0209
2022-02-26 06:21:34 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 2.0086
2022-02-26 06:22:06 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.3483
2022-02-26 06:22:40 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.0845
2022-02-26 06:23:13 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.7947
2022-02-26 06:23:46 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.9280
2022-02-26 06:24:19 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.9230
2022-02-26 06:24:53 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.0814
2022-02-26 06:25:26 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 2.0177
2022-02-26 06:26:00 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.0936
2022-02-26 06:26:33 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 2.1433
2022-02-26 06:27:06 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 2.0536
2022-02-26 06:27:06 - train: epoch 042, train_loss: 2.0611
2022-02-26 06:28:21 - eval: epoch: 042, acc1: 59.018%, acc5: 81.976%, test_loss: 1.7492, per_image_load_time: 0.768ms, per_image_inference_time: 0.141ms
2022-02-26 06:28:21 - until epoch: 042, best_acc1: 59.884%
2022-02-26 06:28:21 - epoch 043 lr: 0.010000000000000002
2022-02-26 06:29:01 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.1038
2022-02-26 06:29:32 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.3223
2022-02-26 06:30:06 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.8263
2022-02-26 06:30:38 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.9592
2022-02-26 06:31:11 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 2.0281
2022-02-26 06:31:44 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 2.1679
2022-02-26 06:32:17 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 2.0380
2022-02-26 06:32:49 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.8733
2022-02-26 06:33:23 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.9454
2022-02-26 06:33:55 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.2078
2022-02-26 06:34:29 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.9744
2022-02-26 06:35:01 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 2.0031
2022-02-26 06:35:34 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.1304
2022-02-26 06:36:08 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.8736
2022-02-26 06:36:41 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 2.0160
2022-02-26 06:37:15 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.9612
2022-02-26 06:37:47 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.1592
2022-02-26 06:38:21 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.4940
2022-02-26 06:38:53 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 2.2589
2022-02-26 06:39:27 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.8607
2022-02-26 06:39:59 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 2.0726
2022-02-26 06:40:33 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.2155
2022-02-26 06:41:05 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.0922
2022-02-26 06:41:39 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.9228
2022-02-26 06:42:12 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.3170
2022-02-26 06:42:46 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.9124
2022-02-26 06:43:18 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 2.0540
2022-02-26 06:43:52 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.9603
2022-02-26 06:44:25 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.9622
2022-02-26 06:44:58 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.3882
2022-02-26 06:45:31 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.2841
2022-02-26 06:46:04 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 2.0287
2022-02-26 06:46:37 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.0758
2022-02-26 06:47:11 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.9905
2022-02-26 06:47:43 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.0731
2022-02-26 06:48:17 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 2.2347
2022-02-26 06:48:50 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.7886
2022-02-26 06:49:23 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.2067
2022-02-26 06:49:56 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.1603
2022-02-26 06:50:29 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.1580
2022-02-26 06:51:02 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.3837
2022-02-26 06:51:35 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.0928
2022-02-26 06:52:08 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.9746
2022-02-26 06:52:41 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.1232
2022-02-26 06:53:15 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 2.0900
2022-02-26 06:53:47 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 2.0608
2022-02-26 06:54:21 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 2.2154
2022-02-26 06:54:53 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.9189
2022-02-26 06:55:30 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 2.0282
2022-02-26 06:56:01 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 2.0183
2022-02-26 06:56:03 - train: epoch 043, train_loss: 2.0628
2022-02-26 06:57:19 - eval: epoch: 043, acc1: 58.946%, acc5: 81.974%, test_loss: 1.7464, per_image_load_time: 1.728ms, per_image_inference_time: 0.112ms
2022-02-26 06:57:19 - until epoch: 043, best_acc1: 59.884%
2022-02-26 06:57:19 - epoch 044 lr: 0.010000000000000002
2022-02-26 06:57:59 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 2.0349
2022-02-26 06:58:32 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.9451
2022-02-26 06:59:05 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.8883
2022-02-26 06:59:38 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.8036
2022-02-26 07:00:12 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 2.0476
2022-02-26 07:00:44 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.9980
2022-02-26 07:01:18 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 2.0115
2022-02-26 07:01:50 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 2.0874
2022-02-26 07:02:24 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 2.0582
2022-02-26 07:02:57 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.9061
2022-02-26 07:03:31 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.8000
2022-02-26 07:04:04 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 2.0603
2022-02-26 07:04:37 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.0586
2022-02-26 07:05:11 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.9972
2022-02-26 07:05:44 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 2.1572
2022-02-26 07:06:17 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.9831
2022-02-26 07:06:50 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 2.1131
2022-02-26 07:07:23 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 2.0045
2022-02-26 07:07:56 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.0537
2022-02-26 07:08:30 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 2.0150
2022-02-26 07:09:02 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.2548
2022-02-26 07:09:36 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.0973
2022-02-26 07:10:09 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.0923
2022-02-26 07:10:42 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 2.2099
2022-02-26 07:11:16 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.2145
2022-02-26 07:11:49 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.0060
2022-02-26 07:12:22 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.9280
2022-02-26 07:12:55 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.9516
2022-02-26 07:13:28 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.8443
2022-02-26 07:14:01 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.9614
2022-02-26 07:14:34 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.8763
2022-02-26 07:15:07 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.8335
2022-02-26 07:15:40 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.9873
2022-02-26 07:16:14 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.1851
2022-02-26 07:16:47 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 2.1204
2022-02-26 07:17:19 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.2018
2022-02-26 07:17:52 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 2.1524
2022-02-26 07:18:26 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.7481
2022-02-26 07:18:59 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.0094
2022-02-26 07:19:32 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.9945
2022-02-26 07:20:06 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.8911
2022-02-26 07:20:39 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.0997
2022-02-26 07:21:12 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 2.0357
2022-02-26 07:21:45 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 2.2165
2022-02-26 07:22:19 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.2416
2022-02-26 07:22:52 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 2.1045
2022-02-26 07:23:26 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.1227
2022-02-26 07:23:58 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.2221
2022-02-26 07:24:33 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.9906
2022-02-26 07:25:05 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.1147
2022-02-26 07:25:05 - train: epoch 044, train_loss: 2.0621
2022-02-26 07:26:20 - eval: epoch: 044, acc1: 58.736%, acc5: 81.852%, test_loss: 1.7656, per_image_load_time: 1.177ms, per_image_inference_time: 0.128ms
2022-02-26 07:26:21 - until epoch: 044, best_acc1: 59.884%
2022-02-26 07:26:21 - epoch 045 lr: 0.010000000000000002
2022-02-26 07:26:59 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 2.0936
2022-02-26 07:27:32 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.9524
2022-02-26 07:28:06 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 2.0255
2022-02-26 07:28:38 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 2.1462
2022-02-26 07:29:11 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.2087
2022-02-26 07:29:44 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.3817
2022-02-26 07:30:18 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.8938
2022-02-26 07:30:49 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.9731
2022-02-26 07:31:23 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 2.0502
2022-02-26 07:31:55 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 2.0307
2022-02-26 07:32:29 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.9392
2022-02-26 07:33:01 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 2.1536
2022-02-26 07:33:34 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.9738
2022-02-26 07:34:08 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 2.0104
2022-02-26 07:34:42 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 2.0185
2022-02-26 07:35:14 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.8851
2022-02-26 07:35:48 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.9006
2022-02-26 07:36:21 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 2.1067
2022-02-26 07:36:54 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.2253
2022-02-26 07:37:28 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.3114
2022-02-26 07:38:01 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.1674
2022-02-26 07:38:35 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.0963
2022-02-26 07:39:07 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.8913
2022-02-26 07:39:41 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 2.1307
2022-02-26 07:40:14 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 2.1083
2022-02-26 07:40:47 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.8487
2022-02-26 07:41:20 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.9157
2022-02-26 07:41:54 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 2.0112
2022-02-26 07:42:26 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.1427
2022-02-26 07:43:00 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.2818
2022-02-26 07:43:32 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 2.1522
2022-02-26 07:44:06 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.1363
2022-02-26 07:44:38 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 2.0316
2022-02-26 07:45:12 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 2.1207
2022-02-26 07:45:45 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.1377
2022-02-26 07:46:19 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 2.1509
2022-02-26 07:46:52 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 2.0384
2022-02-26 07:47:25 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 2.0412
2022-02-26 07:47:58 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.0855
2022-02-26 07:48:32 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.2184
2022-02-26 07:49:04 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 2.1192
2022-02-26 07:49:38 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.1871
2022-02-26 07:50:11 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.0362
2022-02-26 07:50:45 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.0536
2022-02-26 07:51:18 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 2.0148
2022-02-26 07:51:51 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.1585
2022-02-26 07:52:25 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 2.0880
2022-02-26 07:52:58 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.9919
2022-02-26 07:53:33 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.1485
2022-02-26 07:54:05 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 2.0223
2022-02-26 07:54:06 - train: epoch 045, train_loss: 2.0643
2022-02-26 07:55:21 - eval: epoch: 045, acc1: 58.856%, acc5: 82.058%, test_loss: 1.7478, per_image_load_time: 0.950ms, per_image_inference_time: 0.138ms
2022-02-26 07:55:21 - until epoch: 045, best_acc1: 59.884%
2022-02-26 07:55:21 - epoch 046 lr: 0.010000000000000002
2022-02-26 07:55:59 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.8604
2022-02-26 07:56:32 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.9998
2022-02-26 07:57:05 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.8258
2022-02-26 07:57:38 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.9107
2022-02-26 07:58:11 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.8705
2022-02-26 07:58:44 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.1706
2022-02-26 07:59:17 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 2.0317
2022-02-26 07:59:51 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.0936
2022-02-26 08:00:23 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 2.1304
2022-02-26 08:00:57 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.8211
2022-02-26 08:01:29 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 2.0184
2022-02-26 08:02:03 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 2.0420
2022-02-26 08:02:36 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 2.2844
2022-02-26 08:03:10 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.9887
2022-02-26 08:03:42 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 2.0277
2022-02-26 08:04:16 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.1128
2022-02-26 08:04:48 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 2.1164
2022-02-26 08:05:22 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.1030
2022-02-26 08:05:55 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.9776
2022-02-26 08:06:28 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.8672
2022-02-26 08:07:01 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.2261
2022-02-26 08:07:34 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.7470
2022-02-26 08:08:06 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.1968
2022-02-26 08:08:39 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 2.0057
2022-02-26 08:09:13 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 2.1017
2022-02-26 08:09:46 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.1440
2022-02-26 08:10:19 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.9165
2022-02-26 08:10:52 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 2.0148
2022-02-26 08:11:26 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.0278
2022-02-26 08:11:59 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.9110
2022-02-26 08:12:32 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.9184
2022-02-26 08:13:05 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 2.1407
2022-02-26 08:13:38 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 2.0454
2022-02-26 08:14:13 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 2.1666
2022-02-26 08:14:45 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.2564
2022-02-26 08:15:19 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.9984
2022-02-26 08:15:52 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.8143
2022-02-26 08:16:26 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 2.1678
2022-02-26 08:16:58 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.9593
2022-02-26 08:17:33 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.8963
2022-02-26 08:18:05 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.1151
2022-02-26 08:18:38 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.8174
2022-02-26 08:19:12 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.3408
2022-02-26 08:19:45 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.3696
2022-02-26 08:20:18 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 2.0882
2022-02-26 08:20:53 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.9847
2022-02-26 08:21:26 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 2.0851
2022-02-26 08:22:00 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.8066
2022-02-26 08:22:33 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.1024
2022-02-26 08:23:06 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.8823
2022-02-26 08:23:07 - train: epoch 046, train_loss: 2.0653
2022-02-26 08:24:22 - eval: epoch: 046, acc1: 59.022%, acc5: 81.940%, test_loss: 1.7487, per_image_load_time: 1.554ms, per_image_inference_time: 0.132ms
2022-02-26 08:24:22 - until epoch: 046, best_acc1: 59.884%
2022-02-26 08:24:22 - epoch 047 lr: 0.010000000000000002
2022-02-26 08:25:01 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 2.2032
2022-02-26 08:25:34 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.3472
2022-02-26 08:26:08 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.8982
2022-02-26 08:26:41 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.8513
2022-02-26 08:27:13 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.9168
2022-02-26 08:27:47 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.0756
2022-02-26 08:28:20 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 2.3036
2022-02-26 08:28:54 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 2.1019
2022-02-26 08:29:26 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.2908
2022-02-26 08:30:00 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.9596
2022-02-26 08:30:32 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.3329
2022-02-26 08:31:06 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.9305
2022-02-26 08:31:40 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 2.1537
2022-02-26 08:32:13 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 2.1461
2022-02-26 08:32:47 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.9666
2022-02-26 08:33:19 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.8987
2022-02-26 08:33:54 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.9886
2022-02-26 08:34:26 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.1485
2022-02-26 08:35:01 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.9441
2022-02-26 08:35:34 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.0828
2022-02-26 08:36:07 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.1773
2022-02-26 08:36:41 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.1647
2022-02-26 08:37:14 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 2.2168
2022-02-26 08:37:47 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.9855
2022-02-26 08:38:21 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.9807
2022-02-26 08:38:53 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.2338
2022-02-26 08:39:28 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.8953
2022-02-26 08:40:00 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.1391
2022-02-26 08:40:35 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.9401
2022-02-26 08:41:06 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.2225
2022-02-26 08:41:40 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.9633
2022-02-26 08:42:13 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.1675
2022-02-26 08:42:46 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.9848
2022-02-26 08:43:19 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.9278
2022-02-26 08:43:53 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.2673
2022-02-26 08:44:26 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 2.1338
2022-02-26 08:45:00 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.0504
2022-02-26 08:45:33 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 2.0095
2022-02-26 08:46:08 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.9930
2022-02-26 08:46:40 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 2.1423
2022-02-26 08:47:14 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.1598
2022-02-26 08:47:47 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.9670
2022-02-26 08:48:22 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.9130
2022-02-26 08:48:55 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 2.0736
2022-02-26 08:49:29 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 2.1221
2022-02-26 08:50:01 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.8825
2022-02-26 08:50:36 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.2815
2022-02-26 08:51:09 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.9980
2022-02-26 08:51:44 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.9812
2022-02-26 08:52:16 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.9989
2022-02-26 08:52:16 - train: epoch 047, train_loss: 2.0636
2022-02-26 08:53:30 - eval: epoch: 047, acc1: 58.720%, acc5: 81.884%, test_loss: 1.7660, per_image_load_time: 2.076ms, per_image_inference_time: 0.153ms
2022-02-26 08:53:30 - until epoch: 047, best_acc1: 59.884%
2022-02-26 08:53:30 - epoch 048 lr: 0.010000000000000002
2022-02-26 08:54:09 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 2.0412
2022-02-26 08:54:42 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.3507
2022-02-26 08:55:15 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 2.0500
2022-02-26 08:55:48 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 2.0867
2022-02-26 08:56:21 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 2.0379
2022-02-26 08:56:54 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 2.1088
2022-02-26 08:57:27 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 2.0389
2022-02-26 08:58:01 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 2.0926
2022-02-26 08:58:33 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.2592
2022-02-26 08:59:07 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.1416
2022-02-26 08:59:40 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.3893
2022-02-26 09:00:14 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.0878
2022-02-26 09:00:46 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.8325
2022-02-26 09:01:20 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 2.0519
2022-02-26 09:01:53 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 2.0133
2022-02-26 09:02:27 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 2.1074
2022-02-26 09:02:59 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.9245
2022-02-26 09:03:34 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.9760
2022-02-26 09:04:06 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.0029
2022-02-26 09:04:40 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.0782
2022-02-26 09:05:13 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 2.1223
2022-02-26 09:05:46 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.2780
2022-02-26 09:06:19 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.8679
2022-02-26 09:06:53 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.1317
2022-02-26 09:07:25 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.1972
2022-02-26 09:07:59 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.0419
2022-02-26 09:08:32 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.0710
2022-02-26 09:09:06 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 2.0035
2022-02-26 09:09:39 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.0914
2022-02-26 09:10:13 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.9204
2022-02-26 09:10:45 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 2.1032
2022-02-26 09:11:19 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 2.1240
2022-02-26 09:11:52 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.2106
2022-02-26 09:12:25 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 2.1128
2022-02-26 09:12:59 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.2929
2022-02-26 09:13:32 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.1549
2022-02-26 09:14:06 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 2.1114
2022-02-26 09:14:39 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.1602
2022-02-26 09:15:13 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.0053
2022-02-26 09:15:46 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 2.0564
2022-02-26 09:16:20 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.2728
2022-02-26 09:16:53 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.9846
2022-02-26 09:17:27 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.8791
2022-02-26 09:17:59 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.9763
2022-02-26 09:18:33 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.2246
2022-02-26 09:19:06 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.9148
2022-02-26 09:19:40 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.2500
2022-02-26 09:20:14 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.2726
2022-02-26 09:20:49 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.0116
2022-02-26 09:21:21 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.8900
2022-02-26 09:21:22 - train: epoch 048, train_loss: 2.0675
2022-02-26 09:22:37 - eval: epoch: 048, acc1: 59.248%, acc5: 82.094%, test_loss: 1.7445, per_image_load_time: 1.387ms, per_image_inference_time: 0.126ms
2022-02-26 09:22:38 - until epoch: 048, best_acc1: 59.884%
2022-02-26 09:22:38 - epoch 049 lr: 0.010000000000000002
2022-02-26 09:23:17 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.2225
2022-02-26 09:23:50 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.9546
2022-02-26 09:24:23 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.0290
2022-02-26 09:24:56 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.0953
2022-02-26 09:25:29 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.2096
2022-02-26 09:26:02 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 2.1849
2022-02-26 09:26:36 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.1398
2022-02-26 09:27:09 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.2210
2022-02-26 09:27:42 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.9262
2022-02-26 09:28:16 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 2.2774
2022-02-26 09:28:49 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 2.1475
2022-02-26 09:29:22 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.8734
2022-02-26 09:29:56 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.2270
2022-02-26 09:30:29 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.2391
2022-02-26 09:31:03 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 2.0426
2022-02-26 09:31:36 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.2394
2022-02-26 09:32:09 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 2.1385
2022-02-26 09:32:43 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 2.0335
2022-02-26 09:33:15 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.9947
2022-02-26 09:33:49 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.9945
2022-02-26 09:34:22 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.9814
2022-02-26 09:34:56 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 2.2010
2022-02-26 09:35:29 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 2.1200
2022-02-26 09:36:03 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.1903
2022-02-26 09:36:36 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 2.0939
2022-02-26 09:37:10 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 2.0634
2022-02-26 09:37:44 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 2.1046
2022-02-26 09:38:17 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.9022
2022-02-26 09:38:50 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 2.1856
2022-02-26 09:39:24 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 2.0241
2022-02-26 09:39:57 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 2.2668
2022-02-26 09:40:30 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.1779
2022-02-26 09:41:04 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 2.0182
2022-02-26 09:41:38 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.1985
2022-02-26 09:42:11 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.9807
2022-02-26 09:42:45 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.1383
2022-02-26 09:43:18 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.9711
2022-02-26 09:43:53 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.0293
2022-02-26 09:44:25 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.2582
2022-02-26 09:44:59 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.9836
2022-02-26 09:45:32 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 2.0323
2022-02-26 09:46:06 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.1691
2022-02-26 09:46:40 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.1320
2022-02-26 09:47:13 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.1345
2022-02-26 09:47:47 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.9063
2022-02-26 09:48:20 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.1885
2022-02-26 09:48:55 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.1634
2022-02-26 09:49:28 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 2.0132
2022-02-26 09:50:02 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 2.1434
2022-02-26 09:50:35 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 2.0549
2022-02-26 09:50:36 - train: epoch 049, train_loss: 2.0623
2022-02-26 09:51:52 - eval: epoch: 049, acc1: 58.794%, acc5: 81.850%, test_loss: 1.7663, per_image_load_time: 2.696ms, per_image_inference_time: 0.120ms
2022-02-26 09:51:52 - until epoch: 049, best_acc1: 59.884%
2022-02-26 09:51:52 - epoch 050 lr: 0.010000000000000002
2022-02-26 09:52:30 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 2.1569
2022-02-26 09:53:04 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 2.0230
2022-02-26 09:53:35 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 2.0720
2022-02-26 09:54:09 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 2.1005
2022-02-26 09:54:41 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 2.1394
2022-02-26 09:55:14 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.1706
2022-02-26 09:55:47 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.8283
2022-02-26 09:56:20 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.8757
2022-02-26 09:56:53 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 2.0610
2022-02-26 09:57:26 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.1396
2022-02-26 09:58:00 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 2.2965
2022-02-26 09:58:32 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.8579
2022-02-26 09:59:06 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.8768
2022-02-26 09:59:38 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 2.1724
2022-02-26 10:00:13 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.9812
2022-02-26 10:00:45 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 2.1276
2022-02-26 10:01:18 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 2.1036
2022-02-26 10:01:50 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.2589
2022-02-26 10:02:24 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.8921
2022-02-26 10:02:57 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.9429
2022-02-26 10:03:30 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.9929
2022-02-26 10:04:03 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.0550
2022-02-26 10:04:37 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.8688
2022-02-26 10:05:09 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.2988
2022-02-26 10:05:43 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 2.2451
2022-02-26 10:06:15 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.8639
2022-02-26 10:06:49 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 2.0972
2022-02-26 10:07:21 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.9960
2022-02-26 10:07:54 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.3124
2022-02-26 10:08:28 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.3205
2022-02-26 10:09:00 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.1081
2022-02-26 10:09:35 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.2934
2022-02-26 10:10:07 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.9850
2022-02-26 10:10:41 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 2.1348
2022-02-26 10:11:14 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.9773
2022-02-26 10:11:47 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.0647
2022-02-26 10:12:20 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.2522
2022-02-26 10:12:53 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.9795
2022-02-26 10:13:27 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.9533
2022-02-26 10:13:59 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.2180
2022-02-26 10:14:33 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 2.0509
2022-02-26 10:15:05 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.0819
2022-02-26 10:15:39 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 2.3609
2022-02-26 10:16:12 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.9641
2022-02-26 10:16:46 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 2.0485
2022-02-26 10:17:19 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.9469
2022-02-26 10:17:52 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.0484
2022-02-26 10:18:26 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 2.0964
2022-02-26 10:18:59 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.9190
2022-02-26 10:19:32 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.9076
2022-02-26 10:19:33 - train: epoch 050, train_loss: 2.0642
2022-02-26 10:20:46 - eval: epoch: 050, acc1: 58.464%, acc5: 81.854%, test_loss: 1.7693, per_image_load_time: 1.066ms, per_image_inference_time: 0.132ms
2022-02-26 10:20:46 - until epoch: 050, best_acc1: 59.884%
2022-02-26 10:20:46 - epoch 051 lr: 0.010000000000000002
2022-02-26 10:21:25 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.2887
2022-02-26 10:21:58 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.3793
2022-02-26 10:22:31 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.9097
2022-02-26 10:23:03 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.9768
2022-02-26 10:23:37 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.0018
2022-02-26 10:24:09 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.8736
2022-02-26 10:24:42 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 2.0649
2022-02-26 10:25:14 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.2194
2022-02-26 10:25:48 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.9415
2022-02-26 10:26:20 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.3075
2022-02-26 10:26:54 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.0698
2022-02-26 10:27:26 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.7854
2022-02-26 10:28:00 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.8408
2022-02-26 10:28:32 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 2.1310
2022-02-26 10:29:05 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 2.0477
2022-02-26 10:29:37 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.8978
2022-02-26 10:30:11 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.0261
2022-02-26 10:30:44 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 2.1498
2022-02-26 10:31:17 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.8833
2022-02-26 10:31:50 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 2.0364
2022-02-26 10:32:24 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 2.0465
2022-02-26 10:32:56 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.9924
2022-02-26 10:33:29 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.1854
2022-02-26 10:34:03 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 2.0238
2022-02-26 10:34:35 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.9763
2022-02-26 10:35:09 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.8809
2022-02-26 10:35:42 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.9565
2022-02-26 10:36:15 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.9336
2022-02-26 10:36:48 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 2.0402
2022-02-26 10:37:20 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.9606
2022-02-26 10:37:54 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.9889
2022-02-26 10:38:27 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 2.0402
2022-02-26 10:39:00 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.2042
2022-02-26 10:39:33 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.9863
2022-02-26 10:40:06 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 2.0815
2022-02-26 10:40:39 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 2.0422
2022-02-26 10:41:12 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 2.1401
2022-02-26 10:41:45 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.9375
2022-02-26 10:42:18 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.0872
2022-02-26 10:42:52 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 2.2862
2022-02-26 10:43:24 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 2.0359
2022-02-26 10:43:58 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.1820
2022-02-26 10:44:31 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 2.0258
2022-02-26 10:45:05 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.9990
2022-02-26 10:45:38 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.7631
2022-02-26 10:46:12 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.0937
2022-02-26 10:46:45 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 2.0303
2022-02-26 10:47:19 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 2.1570
2022-02-26 10:47:52 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.1446
2022-02-26 10:48:25 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.9213
2022-02-26 10:48:26 - train: epoch 051, train_loss: 2.0663
2022-02-26 10:49:43 - eval: epoch: 051, acc1: 58.846%, acc5: 81.768%, test_loss: 1.7646, per_image_load_time: 1.245ms, per_image_inference_time: 0.129ms
2022-02-26 10:49:43 - until epoch: 051, best_acc1: 59.884%
2022-02-26 10:49:43 - epoch 052 lr: 0.010000000000000002
2022-02-26 10:50:22 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 2.1628
2022-02-26 10:50:54 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.0449
2022-02-26 10:51:28 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 2.0643
2022-02-26 10:52:01 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.1419
2022-02-26 10:52:34 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 2.0662
2022-02-26 10:53:07 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 2.1484
2022-02-26 10:53:41 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.9990
2022-02-26 10:54:13 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.9713
2022-02-26 10:54:46 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 2.1419
2022-02-26 10:55:19 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.2004
2022-02-26 10:55:52 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 2.2473
2022-02-26 10:56:26 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 2.0179
2022-02-26 10:56:59 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 2.1026
2022-02-26 10:57:32 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.1915
2022-02-26 10:58:05 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.9269
2022-02-26 10:58:39 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.9696
2022-02-26 10:59:12 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 2.0431
2022-02-26 10:59:46 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 2.2205
2022-02-26 11:00:19 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.8723
2022-02-26 11:00:53 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.3847
2022-02-26 11:01:25 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 2.0644
2022-02-26 11:02:00 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.3003
2022-02-26 11:02:33 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 2.0435
2022-02-26 11:03:07 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.8379
2022-02-26 11:03:39 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.8332
2022-02-26 11:04:12 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.9231
2022-02-26 11:04:46 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 2.0485
2022-02-26 11:05:19 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 2.1749
2022-02-26 11:05:53 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.9641
2022-02-26 11:06:25 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.9352
2022-02-26 11:06:58 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.8775
2022-02-26 11:07:32 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 2.0113
2022-02-26 11:08:05 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 2.0763
2022-02-26 11:08:38 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.1838
2022-02-26 11:09:12 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.1168
2022-02-26 11:09:45 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.1569
2022-02-26 11:10:18 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 2.1365
2022-02-26 11:10:53 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 2.1397
2022-02-26 11:11:25 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.8983
2022-02-26 11:12:00 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.0885
2022-02-26 11:12:32 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.8850
2022-02-26 11:13:07 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 2.1960
2022-02-26 11:13:39 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.9922
2022-02-26 11:14:14 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 2.1136
2022-02-26 11:14:46 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 2.1170
2022-02-26 11:15:20 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.9731
2022-02-26 11:15:53 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.0013
2022-02-26 11:16:28 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.8787
2022-02-26 11:17:01 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 2.1699
2022-02-26 11:17:34 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 2.0923
2022-02-26 11:17:35 - train: epoch 052, train_loss: 2.0640
2022-02-26 11:18:49 - eval: epoch: 052, acc1: 58.932%, acc5: 81.862%, test_loss: 1.7513, per_image_load_time: 2.747ms, per_image_inference_time: 0.125ms
2022-02-26 11:18:49 - until epoch: 052, best_acc1: 59.884%
2022-02-26 11:18:49 - epoch 053 lr: 0.010000000000000002
2022-02-26 11:19:28 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.1007
2022-02-26 11:20:02 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 2.3386
2022-02-26 11:20:34 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 2.1631
2022-02-26 11:21:07 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.2812
2022-02-26 11:21:41 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.8876
2022-02-26 11:22:14 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 2.0555
2022-02-26 11:22:46 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 2.1355
2022-02-26 11:23:19 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 2.0267
2022-02-26 11:23:54 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 2.0089
2022-02-26 11:24:25 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 2.1638
2022-02-26 11:25:00 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 2.0233
2022-02-26 11:25:32 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.9644
2022-02-26 11:26:07 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.1409
2022-02-26 11:26:40 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.2221
2022-02-26 11:27:14 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.8691
2022-02-26 11:27:47 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.1725
2022-02-26 11:28:21 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.1563
2022-02-26 11:28:54 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.0693
2022-02-26 11:29:28 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 2.0498
2022-02-26 11:30:01 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 2.0592
2022-02-26 11:30:35 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.1818
2022-02-26 11:31:08 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.9623
2022-02-26 11:31:42 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.9920
2022-02-26 11:32:15 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.9916
2022-02-26 11:32:48 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.2062
2022-02-26 11:33:20 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.1738
2022-02-26 11:33:53 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 2.2008
2022-02-26 11:34:26 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.2560
2022-02-26 11:34:59 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.8768
2022-02-26 11:35:32 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.7909
2022-02-26 11:36:06 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.4599
2022-02-26 11:36:38 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.2269
2022-02-26 11:37:12 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 2.0365
2022-02-26 11:37:44 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 2.0130
2022-02-26 11:38:18 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 2.0797
2022-02-26 11:38:51 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.9949
2022-02-26 11:39:24 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.1890
2022-02-26 11:39:59 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 2.0308
2022-02-26 11:40:31 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.1511
2022-02-26 11:41:05 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 2.2226
2022-02-26 11:41:38 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 2.0709
2022-02-26 11:42:12 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 2.1183
2022-02-26 11:42:45 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.3930
2022-02-26 11:43:19 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 2.0790
2022-02-26 11:43:52 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.2265
2022-02-26 11:44:27 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 2.1421
2022-02-26 11:45:00 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 2.1879
2022-02-26 11:45:33 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.3446
2022-02-26 11:46:07 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 2.0312
2022-02-26 11:46:40 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 2.0408
2022-02-26 11:46:42 - train: epoch 053, train_loss: 2.0644
2022-02-26 11:47:57 - eval: epoch: 053, acc1: 58.774%, acc5: 81.660%, test_loss: 1.7666, per_image_load_time: 2.771ms, per_image_inference_time: 0.127ms
2022-02-26 11:47:57 - until epoch: 053, best_acc1: 59.884%
2022-02-26 11:47:57 - epoch 054 lr: 0.010000000000000002
2022-02-26 11:48:36 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 2.0581
2022-02-26 11:49:10 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.0604
2022-02-26 11:49:42 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 2.2277
2022-02-26 11:50:15 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 2.0030
2022-02-26 11:50:49 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.9334
2022-02-26 11:51:21 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 2.0262
2022-02-26 11:51:56 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 2.2547
2022-02-26 11:52:28 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.2190
2022-02-26 11:53:01 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.8174
2022-02-26 11:53:35 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.7614
2022-02-26 11:54:09 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.8019
2022-02-26 11:54:41 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.1720
2022-02-26 11:55:15 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.9193
2022-02-26 11:55:48 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.2619
2022-02-26 11:56:21 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 2.1102
2022-02-26 11:56:55 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.9459
2022-02-26 11:57:28 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.9244
2022-02-26 11:58:01 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.8635
2022-02-26 11:58:34 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.2143
2022-02-26 11:59:08 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 2.1652
2022-02-26 11:59:41 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.9119
2022-02-26 12:00:15 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 2.1293
2022-02-26 12:00:49 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.7624
2022-02-26 12:01:22 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 2.2185
2022-02-26 12:01:55 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 2.1907
2022-02-26 12:02:28 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 2.2354
2022-02-26 12:03:01 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 2.0044
2022-02-26 12:03:35 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.0891
2022-02-26 12:04:07 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.8584
2022-02-26 12:04:40 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.1547
2022-02-26 12:05:12 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 2.0505
2022-02-26 12:05:45 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.1943
2022-02-26 12:06:19 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 2.1293
2022-02-26 12:06:52 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.9555
2022-02-26 12:07:26 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.2216
2022-02-26 12:07:58 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 2.1042
2022-02-26 12:08:33 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.9101
2022-02-26 12:09:05 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 2.0076
2022-02-26 12:09:40 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 2.0743
2022-02-26 12:10:13 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.9723
2022-02-26 12:10:47 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 2.0084
2022-02-26 12:11:20 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 2.1956
2022-02-26 12:11:54 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 2.1086
2022-02-26 12:12:27 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.8130
2022-02-26 12:13:02 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 2.0742
2022-02-26 12:13:35 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 2.2398
2022-02-26 12:14:09 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.2871
2022-02-26 12:14:43 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.3006
2022-02-26 12:15:17 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.9050
2022-02-26 12:15:49 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.9659
2022-02-26 12:15:51 - train: epoch 054, train_loss: 2.0651
2022-02-26 12:17:06 - eval: epoch: 054, acc1: 58.594%, acc5: 81.600%, test_loss: 1.7816, per_image_load_time: 2.370ms, per_image_inference_time: 0.145ms
2022-02-26 12:17:06 - until epoch: 054, best_acc1: 59.884%
2022-02-26 12:17:06 - epoch 055 lr: 0.010000000000000002
2022-02-26 12:17:44 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.9075
2022-02-26 12:18:17 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 2.1803
2022-02-26 12:18:51 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.9256
2022-02-26 12:19:25 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.9917
2022-02-26 12:19:58 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 2.0499
2022-02-26 12:20:31 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 2.0567
2022-02-26 12:21:06 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 2.1749
2022-02-26 12:21:39 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.8906
2022-02-26 12:22:12 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 2.2585
2022-02-26 12:22:46 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 2.0835
2022-02-26 12:23:18 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 2.0910
2022-02-26 12:23:52 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 2.1162
2022-02-26 12:24:25 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.3995
2022-02-26 12:25:00 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.8452
2022-02-26 12:25:33 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.9698
2022-02-26 12:26:07 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 2.2189
2022-02-26 12:26:40 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 2.1038
2022-02-26 12:27:14 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.9846
2022-02-26 12:27:46 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 2.0825
2022-02-26 12:28:21 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.9906
2022-02-26 12:28:55 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.7936
2022-02-26 12:29:29 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.1799
2022-02-26 12:30:02 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.9978
2022-02-26 12:30:36 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.9119
2022-02-26 12:31:09 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.9295
2022-02-26 12:31:44 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 2.0880
2022-02-26 12:32:17 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.9497
2022-02-26 12:32:51 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.1716
2022-02-26 12:33:24 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.9432
2022-02-26 12:33:58 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.0646
2022-02-26 12:34:31 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 2.0789
2022-02-26 12:35:05 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 2.0561
2022-02-26 12:35:38 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.7122
2022-02-26 12:36:12 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.9925
2022-02-26 12:36:47 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.9168
2022-02-26 12:37:20 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.1272
2022-02-26 12:37:53 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 2.0051
2022-02-26 12:38:26 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 2.0140
2022-02-26 12:39:01 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.2105
2022-02-26 12:39:33 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.8710
2022-02-26 12:40:08 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.9560
2022-02-26 12:40:41 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.2056
2022-02-26 12:41:14 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.1458
2022-02-26 12:41:49 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.0755
2022-02-26 12:42:22 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 2.0278
2022-02-26 12:42:57 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.9623
2022-02-26 12:43:30 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.8552
2022-02-26 12:44:04 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.2829
2022-02-26 12:44:38 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.9769
2022-02-26 12:45:11 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 2.1239
2022-02-26 12:45:12 - train: epoch 055, train_loss: 2.0623
2022-02-26 12:46:28 - eval: epoch: 055, acc1: 59.246%, acc5: 82.130%, test_loss: 1.7447, per_image_load_time: 2.510ms, per_image_inference_time: 0.138ms
2022-02-26 12:46:28 - until epoch: 055, best_acc1: 59.884%
2022-02-26 12:46:28 - epoch 056 lr: 0.010000000000000002
2022-02-26 12:47:07 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 2.0731
2022-02-26 12:47:40 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.2488
2022-02-26 12:48:14 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.9469
2022-02-26 12:48:47 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 2.1370
2022-02-26 12:49:21 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 2.1191
2022-02-26 12:49:54 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 2.1555
2022-02-26 12:50:28 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 2.0304
2022-02-26 12:51:01 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.3951
2022-02-26 12:51:35 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.0557
2022-02-26 12:52:09 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.7802
2022-02-26 12:52:42 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.8915
2022-02-26 12:53:17 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 2.1741
2022-02-26 12:53:50 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.9879
2022-02-26 12:54:24 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 2.1764
2022-02-26 12:54:58 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.3748
2022-02-26 12:55:32 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.8157
2022-02-26 12:56:06 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.3356
2022-02-26 12:56:40 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.1575
2022-02-26 12:57:14 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 2.2795
2022-02-26 12:57:47 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 2.1569
2022-02-26 12:58:21 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.2077
2022-02-26 12:58:54 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.1174
2022-02-26 12:59:29 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 2.2151
2022-02-26 13:00:03 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 2.0948
2022-02-26 13:00:35 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.3224
2022-02-26 13:01:10 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.9864
2022-02-26 13:01:44 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.9622
2022-02-26 13:02:17 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.9833
2022-02-26 13:02:51 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 2.0888
2022-02-26 13:03:26 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.1884
2022-02-26 13:03:59 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 2.0751
2022-02-26 13:04:33 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 2.0645
2022-02-26 13:05:07 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 2.1743
2022-02-26 13:05:40 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 2.0377
2022-02-26 13:06:14 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.9875
2022-02-26 13:06:47 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.8276
2022-02-26 13:07:22 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 2.1069
2022-02-26 13:07:55 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 2.1488
2022-02-26 13:08:29 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.3692
2022-02-26 13:09:03 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 2.1104
2022-02-26 13:09:38 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.3175
2022-02-26 13:10:10 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 2.1881
2022-02-26 13:10:45 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.9822
2022-02-26 13:11:17 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.0776
2022-02-26 13:11:52 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 2.0980
2022-02-26 13:12:25 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 2.0153
2022-02-26 13:13:00 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.9093
2022-02-26 13:13:33 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.0793
2022-02-26 13:14:08 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.0702
2022-02-26 13:14:39 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 2.0841
2022-02-26 13:14:41 - train: epoch 056, train_loss: 2.0623
2022-02-26 13:15:56 - eval: epoch: 056, acc1: 58.894%, acc5: 81.918%, test_loss: 1.7571, per_image_load_time: 2.724ms, per_image_inference_time: 0.145ms
2022-02-26 13:15:57 - until epoch: 056, best_acc1: 59.884%
2022-02-26 13:15:57 - epoch 057 lr: 0.010000000000000002
2022-02-26 13:16:35 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 2.0753
2022-02-26 13:17:09 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.9874
2022-02-26 13:17:42 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 2.1101
2022-02-26 13:18:16 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 2.0252
2022-02-26 13:18:49 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 2.1413
2022-02-26 13:19:21 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.0233
2022-02-26 13:19:55 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.8059
2022-02-26 13:20:27 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 2.0385
2022-02-26 13:21:00 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.9828
2022-02-26 13:21:34 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 2.0230
2022-02-26 13:22:07 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.8257
2022-02-26 13:22:41 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 2.1187
2022-02-26 13:23:14 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 2.1597
2022-02-26 13:23:48 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 2.1093
2022-02-26 13:24:20 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 2.1635
2022-02-26 13:24:55 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.2181
2022-02-26 13:25:27 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 2.2209
2022-02-26 13:26:01 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 2.0196
2022-02-26 13:26:35 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 2.0416
2022-02-26 13:27:09 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 2.1366
2022-02-26 13:27:41 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.9528
2022-02-26 13:28:15 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.9466
2022-02-26 13:28:49 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 2.2097
2022-02-26 13:29:23 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 2.0634
2022-02-26 13:29:55 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.2037
2022-02-26 13:30:29 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.9476
2022-02-26 13:31:02 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.7151
2022-02-26 13:31:36 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.9504
2022-02-26 13:32:09 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.9969
2022-02-26 13:32:43 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.2317
2022-02-26 13:33:15 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.0800
2022-02-26 13:33:49 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.1637
2022-02-26 13:34:22 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.1553
2022-02-26 13:34:55 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 2.1386
2022-02-26 13:35:29 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.1285
2022-02-26 13:36:03 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 2.2341
2022-02-26 13:36:35 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 2.0409
2022-02-26 13:37:09 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 2.0511
2022-02-26 13:37:42 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.2072
2022-02-26 13:38:14 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 2.0913
2022-02-26 13:38:47 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 2.3603
2022-02-26 13:39:21 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 2.0816
2022-02-26 13:39:55 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.8984
2022-02-26 13:40:28 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.2328
2022-02-26 13:41:02 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.1924
2022-02-26 13:41:35 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 2.2080
2022-02-26 13:42:09 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 2.1748
2022-02-26 13:42:44 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.3222
2022-02-26 13:43:20 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.1019
2022-02-26 13:43:55 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 2.1478
2022-02-26 13:43:57 - train: epoch 057, train_loss: 2.0596
2022-02-26 13:45:13 - eval: epoch: 057, acc1: 58.828%, acc5: 81.464%, test_loss: 1.7781, per_image_load_time: 2.416ms, per_image_inference_time: 0.126ms
2022-02-26 13:45:13 - until epoch: 057, best_acc1: 59.884%
2022-02-26 13:45:13 - epoch 058 lr: 0.010000000000000002
2022-02-26 13:45:52 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 2.0717
2022-02-26 13:46:25 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 2.0736
2022-02-26 13:46:58 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 2.1883
2022-02-26 13:47:30 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.9931
2022-02-26 13:48:04 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.9448
2022-02-26 13:48:38 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.1146
2022-02-26 13:49:11 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.9174
2022-02-26 13:49:44 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.8916
2022-02-26 13:50:17 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.8748
2022-02-26 13:50:50 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 2.1569
2022-02-26 13:51:23 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.9564
2022-02-26 13:51:57 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 2.1832
2022-02-26 13:52:29 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.2417
2022-02-26 13:53:03 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.1101
2022-02-26 13:53:36 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.9720
2022-02-26 13:54:10 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 2.1137
2022-02-26 13:54:42 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.9757
2022-02-26 13:55:15 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.3440
2022-02-26 13:55:48 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.1079
2022-02-26 13:56:22 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.1506
2022-02-26 13:56:54 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 2.2258
2022-02-26 13:57:28 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 2.0213
2022-02-26 13:58:02 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 2.1887
2022-02-26 13:58:34 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 2.3324
2022-02-26 13:59:08 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 2.0241
2022-02-26 13:59:40 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.8069
2022-02-26 14:00:13 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.1802
2022-02-26 14:00:47 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.9624
2022-02-26 14:01:19 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.9189
2022-02-26 14:01:52 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.2243
2022-02-26 14:02:24 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 2.0017
2022-02-26 14:02:58 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.9187
2022-02-26 14:03:31 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 2.0508
2022-02-26 14:04:05 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 2.0329
2022-02-26 14:04:38 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 2.0474
2022-02-26 14:05:10 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.8249
2022-02-26 14:05:44 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 2.0015
2022-02-26 14:06:18 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.9230
2022-02-26 14:06:50 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 2.0964
2022-02-26 14:07:24 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.0994
2022-02-26 14:07:56 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.9709
2022-02-26 14:08:30 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.9374
2022-02-26 14:09:04 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.9339
2022-02-26 14:09:37 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.8909
2022-02-26 14:10:12 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.8086
2022-02-26 14:10:46 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.8282
2022-02-26 14:11:22 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 2.3085
2022-02-26 14:11:58 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 2.1127
2022-02-26 14:12:34 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.7995
2022-02-26 14:13:10 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.8429
2022-02-26 14:13:12 - train: epoch 058, train_loss: 2.0595
2022-02-26 14:14:27 - eval: epoch: 058, acc1: 59.010%, acc5: 81.980%, test_loss: 1.7592, per_image_load_time: 2.406ms, per_image_inference_time: 0.140ms
2022-02-26 14:14:27 - until epoch: 058, best_acc1: 59.884%
2022-02-26 14:14:27 - epoch 059 lr: 0.010000000000000002
2022-02-26 14:15:06 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.0307
2022-02-26 14:15:40 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.9445
2022-02-26 14:16:13 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.9545
2022-02-26 14:16:46 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 2.3683
2022-02-26 14:17:20 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 2.0304
2022-02-26 14:17:52 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 2.1448
2022-02-26 14:18:26 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.9630
2022-02-26 14:18:59 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 2.1358
2022-02-26 14:19:32 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.9447
2022-02-26 14:20:06 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.2239
2022-02-26 14:20:40 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.3669
2022-02-26 14:21:14 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.8506
2022-02-26 14:21:47 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.2835
2022-02-26 14:22:21 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.1644
2022-02-26 14:22:54 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.9972
2022-02-26 14:23:28 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.8698
2022-02-26 14:24:01 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 2.2598
2022-02-26 14:24:35 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.8817
2022-02-26 14:25:07 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 2.1632
2022-02-26 14:25:42 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.8706
2022-02-26 14:26:15 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.0756
2022-02-26 14:26:49 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 2.1705
2022-02-26 14:27:22 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 2.0195
2022-02-26 14:27:55 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.2589
2022-02-26 14:28:28 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 2.0957
2022-02-26 14:29:02 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.8798
2022-02-26 14:29:35 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 2.2932
2022-02-26 14:30:07 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.3145
2022-02-26 14:30:41 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 2.0879
2022-02-26 14:31:14 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.3247
2022-02-26 14:31:47 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.9539
2022-02-26 14:32:20 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 2.2903
2022-02-26 14:32:54 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 2.0629
2022-02-26 14:33:27 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.4104
2022-02-26 14:34:01 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.9058
2022-02-26 14:34:33 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.0557
2022-02-26 14:35:08 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 2.1430
2022-02-26 14:35:39 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 2.0210
2022-02-26 14:36:14 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.7582
2022-02-26 14:36:47 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.1696
2022-02-26 14:37:21 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 2.0755
2022-02-26 14:37:54 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.1442
2022-02-26 14:38:27 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.1224
2022-02-26 14:39:00 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.0589
2022-02-26 14:39:35 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 2.2319
2022-02-26 14:40:09 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 2.0781
2022-02-26 14:40:45 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 2.0726
2022-02-26 14:41:18 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 2.0129
2022-02-26 14:41:54 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 2.2377
2022-02-26 14:42:28 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.4193
2022-02-26 14:42:30 - train: epoch 059, train_loss: 2.0593
2022-02-26 14:43:47 - eval: epoch: 059, acc1: 58.576%, acc5: 81.880%, test_loss: 1.7634, per_image_load_time: 2.826ms, per_image_inference_time: 0.133ms
2022-02-26 14:43:47 - until epoch: 059, best_acc1: 59.884%
2022-02-26 14:43:47 - epoch 060 lr: 0.010000000000000002
2022-02-26 14:44:25 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 2.0920
2022-02-26 14:44:58 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 2.0197
2022-02-26 14:45:31 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 2.0373
2022-02-26 14:46:04 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.2326
2022-02-26 14:46:37 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.4833
2022-02-26 14:47:11 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.2439
2022-02-26 14:47:43 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 2.0371
2022-02-26 14:48:16 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.1283
2022-02-26 14:48:50 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 2.0176
2022-02-26 14:49:23 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.8010
2022-02-26 14:49:56 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.8595
2022-02-26 14:50:30 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.8435
2022-02-26 14:51:03 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 2.1594
2022-02-26 14:51:35 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 2.0621
2022-02-26 14:52:08 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 2.2535
2022-02-26 14:52:41 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.8986
2022-02-26 14:53:13 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 2.1640
2022-02-26 14:53:47 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 2.2786
2022-02-26 14:54:20 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.1390
2022-02-26 14:54:53 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 2.0840
2022-02-26 14:55:26 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.1063
2022-02-26 14:56:00 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.1037
2022-02-26 14:56:32 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.7858
2022-02-26 14:57:05 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 2.0087
2022-02-26 14:57:38 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.0989
2022-02-26 14:58:12 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 2.2079
2022-02-26 14:58:46 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.9412
2022-02-26 14:59:18 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 2.1056
2022-02-26 14:59:52 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.9470
2022-02-26 15:00:24 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.0508
2022-02-26 15:00:57 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.0715
2022-02-26 15:01:31 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.9869
2022-02-26 15:02:04 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.9111
2022-02-26 15:02:37 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 2.2195
2022-02-26 15:03:10 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.0514
2022-02-26 15:03:44 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.1161
2022-02-26 15:04:16 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 2.1625
2022-02-26 15:04:50 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 2.1860
2022-02-26 15:05:22 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 2.2084
2022-02-26 15:05:56 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 2.1142
2022-02-26 15:06:28 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.1730
2022-02-26 15:07:02 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.1487
2022-02-26 15:07:34 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.9925
2022-02-26 15:08:08 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.2882
2022-02-26 15:08:41 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.0590
2022-02-26 15:09:16 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.9182
2022-02-26 15:09:50 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 2.1710
2022-02-26 15:10:25 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.8737
2022-02-26 15:10:58 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.9529
2022-02-26 15:11:32 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 2.1198
2022-02-26 15:11:34 - train: epoch 060, train_loss: 2.0587
2022-02-26 15:12:49 - eval: epoch: 060, acc1: 58.798%, acc5: 81.910%, test_loss: 1.7644, per_image_load_time: 2.638ms, per_image_inference_time: 0.141ms
2022-02-26 15:12:50 - until epoch: 060, best_acc1: 59.884%
2022-02-26 15:12:50 - epoch 061 lr: 0.0010000000000000002
2022-02-26 15:13:27 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.9459
2022-02-26 15:14:02 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.9864
2022-02-26 15:14:34 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.8337
2022-02-26 15:15:07 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.0606
2022-02-26 15:15:39 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.8643
2022-02-26 15:16:13 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.9847
2022-02-26 15:16:45 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.9025
2022-02-26 15:17:19 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.9550
2022-02-26 15:17:51 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.8473
2022-02-26 15:18:25 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.8136
2022-02-26 15:18:58 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.7878
2022-02-26 15:19:30 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.7682
2022-02-26 15:20:03 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.7240
2022-02-26 15:20:37 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.8480
2022-02-26 15:21:10 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.9218
2022-02-26 15:21:44 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.6188
2022-02-26 15:22:15 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.9942
2022-02-26 15:22:48 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 2.0495
2022-02-26 15:23:20 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.9696
2022-02-26 15:23:54 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.9410
2022-02-26 15:24:26 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 2.0460
2022-02-26 15:25:00 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.7474
2022-02-26 15:25:32 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.7518
2022-02-26 15:26:05 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.7649
2022-02-26 15:26:38 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.7748
2022-02-26 15:27:11 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.9320
2022-02-26 15:27:44 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.9213
2022-02-26 15:28:17 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.8012
2022-02-26 15:28:50 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.8457
2022-02-26 15:29:23 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.9237
2022-02-26 15:29:57 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 2.0174
2022-02-26 15:30:29 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.8467
2022-02-26 15:31:02 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.9114
2022-02-26 15:31:35 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.7977
2022-02-26 15:32:08 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.9061
2022-02-26 15:32:41 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.8909
2022-02-26 15:33:13 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.8677
2022-02-26 15:33:47 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.9231
2022-02-26 15:34:19 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.7753
2022-02-26 15:34:53 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.8211
2022-02-26 15:35:25 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.9026
2022-02-26 15:35:58 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.8923
2022-02-26 15:36:31 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.9207
2022-02-26 15:37:04 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.9163
2022-02-26 15:37:38 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.7230
2022-02-26 15:38:11 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.7949
2022-02-26 15:38:47 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.8605
2022-02-26 15:39:21 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.8627
2022-02-26 15:39:58 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.7498
2022-02-26 15:40:31 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 2.0221
2022-02-26 15:40:33 - train: epoch 061, train_loss: 1.8897
2022-02-26 15:41:50 - eval: epoch: 061, acc1: 62.766%, acc5: 84.446%, test_loss: 1.5802, per_image_load_time: 2.813ms, per_image_inference_time: 0.125ms
2022-02-26 15:41:50 - until epoch: 061, best_acc1: 62.766%
2022-02-26 15:41:50 - epoch 062 lr: 0.0010000000000000002
2022-02-26 15:42:28 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.9085
2022-02-26 15:43:01 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 2.0035
2022-02-26 15:43:33 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.9924
2022-02-26 15:44:07 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.6400
2022-02-26 15:44:39 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.9258
2022-02-26 15:45:13 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.6197
2022-02-26 15:45:46 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.7994
2022-02-26 15:46:19 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 2.0070
2022-02-26 15:46:51 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.9849
2022-02-26 15:47:25 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 2.2909
2022-02-26 15:47:57 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.7846
2022-02-26 15:48:31 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 2.0336
2022-02-26 15:49:04 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.9489
2022-02-26 15:49:37 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.8800
2022-02-26 15:50:10 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.8118
2022-02-26 15:50:43 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 2.1625
2022-02-26 15:51:15 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.9502
2022-02-26 15:51:49 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.6540
2022-02-26 15:52:22 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.8505
2022-02-26 15:52:55 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.9207
2022-02-26 15:53:29 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 2.0655
2022-02-26 15:54:00 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.7292
2022-02-26 15:54:34 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.6938
2022-02-26 15:55:07 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.8032
2022-02-26 15:55:41 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.9805
2022-02-26 15:56:13 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.7970
2022-02-26 15:56:47 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.9401
2022-02-26 15:57:19 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.9596
2022-02-26 15:57:53 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.7923
2022-02-26 15:58:27 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 2.1238
2022-02-26 15:59:00 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.7666
2022-02-26 15:59:33 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.7428
2022-02-26 16:00:06 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.8476
2022-02-26 16:00:40 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 2.1288
2022-02-26 16:01:13 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 2.0521
2022-02-26 16:01:46 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.8871
2022-02-26 16:02:19 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.7687
2022-02-26 16:02:52 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.8127
2022-02-26 16:03:25 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 2.0929
2022-02-26 16:03:58 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.6413
2022-02-26 16:04:31 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.9191
2022-02-26 16:05:05 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 2.0040
2022-02-26 16:05:37 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.9490
2022-02-26 16:06:10 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.9277
2022-02-26 16:06:45 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.7446
2022-02-26 16:07:20 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.8208
2022-02-26 16:07:55 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.7639
2022-02-26 16:08:31 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.8047
2022-02-26 16:09:07 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.8384
2022-02-26 16:09:43 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 2.0476
2022-02-26 16:09:45 - train: epoch 062, train_loss: 1.8572
2022-02-26 16:11:00 - eval: epoch: 062, acc1: 62.892%, acc5: 84.658%, test_loss: 1.5671, per_image_load_time: 2.652ms, per_image_inference_time: 0.149ms
2022-02-26 16:11:00 - until epoch: 062, best_acc1: 62.892%
2022-02-26 16:11:00 - epoch 063 lr: 0.0010000000000000002
2022-02-26 16:11:38 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 2.0307
2022-02-26 16:12:12 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.8357
2022-02-26 16:12:45 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 2.0040
2022-02-26 16:13:18 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.6464
2022-02-26 16:13:51 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.7416
2022-02-26 16:14:24 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.8319
2022-02-26 16:14:57 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.8123
2022-02-26 16:15:30 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.8831
2022-02-26 16:16:03 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.9787
2022-02-26 16:16:37 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.9967
2022-02-26 16:17:09 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.9103
2022-02-26 16:17:43 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.9840
2022-02-26 16:18:15 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.5501
2022-02-26 16:18:50 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 2.0678
2022-02-26 16:19:23 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.7757
2022-02-26 16:19:56 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.7503
2022-02-26 16:20:29 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.7335
2022-02-26 16:21:03 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.8605
2022-02-26 16:21:36 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 2.0059
2022-02-26 16:22:11 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.5904
2022-02-26 16:22:44 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.7215
2022-02-26 16:23:20 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 2.0937
2022-02-26 16:23:53 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.9978
2022-02-26 16:24:27 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.9495
2022-02-26 16:25:03 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.8804
2022-02-26 16:25:36 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.7678
2022-02-26 16:26:08 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 2.0085
2022-02-26 16:26:41 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.8698
2022-02-26 16:27:16 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.9191
2022-02-26 16:27:49 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 2.0184
2022-02-26 16:28:23 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 2.0012
2022-02-26 16:28:54 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.9627
2022-02-26 16:29:29 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.6780
2022-02-26 16:30:02 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.8399
2022-02-26 16:30:36 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 2.0745
2022-02-26 16:31:09 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.8647
2022-02-26 16:31:43 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.8081
2022-02-26 16:32:16 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.9573
2022-02-26 16:32:50 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.7745
2022-02-26 16:33:23 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.6703
2022-02-26 16:33:57 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.9313
2022-02-26 16:34:29 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.7670
2022-02-26 16:35:05 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 2.0324
2022-02-26 16:35:37 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.8729
2022-02-26 16:36:13 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.7418
2022-02-26 16:36:45 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.9701
2022-02-26 16:37:20 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.7516
2022-02-26 16:37:54 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.8265
2022-02-26 16:38:29 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.7790
2022-02-26 16:39:04 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.7638
2022-02-26 16:39:06 - train: epoch 063, train_loss: 1.8454
2022-02-26 16:40:22 - eval: epoch: 063, acc1: 63.012%, acc5: 84.782%, test_loss: 1.5613, per_image_load_time: 0.798ms, per_image_inference_time: 0.143ms
2022-02-26 16:40:22 - until epoch: 063, best_acc1: 63.012%
2022-02-26 16:40:22 - epoch 064 lr: 0.0010000000000000002
2022-02-26 16:41:01 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.8200
2022-02-26 16:41:33 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.9509
2022-02-26 16:42:06 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.7117
2022-02-26 16:42:41 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.8049
2022-02-26 16:43:13 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.7715
2022-02-26 16:43:47 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.9327
2022-02-26 16:44:19 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.7088
2022-02-26 16:44:53 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.6571
2022-02-26 16:45:27 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.8623
2022-02-26 16:46:00 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.8567
2022-02-26 16:46:34 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.9149
2022-02-26 16:47:08 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.5503
2022-02-26 16:47:42 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.8041
2022-02-26 16:48:15 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 2.0250
2022-02-26 16:48:48 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 2.0935
2022-02-26 16:49:21 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.8131
2022-02-26 16:49:53 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.6900
2022-02-26 16:50:28 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.5873
2022-02-26 16:51:00 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.8855
2022-02-26 16:51:34 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.8429
2022-02-26 16:52:08 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 2.0111
2022-02-26 16:52:40 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.9603
2022-02-26 16:53:14 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.8133
2022-02-26 16:53:47 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.7982
2022-02-26 16:54:19 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.8848
2022-02-26 16:54:52 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.7233
2022-02-26 16:55:25 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.7259
2022-02-26 16:55:58 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.8304
2022-02-26 16:56:31 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.8301
2022-02-26 16:57:04 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.8378
2022-02-26 16:57:38 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.9132
2022-02-26 16:58:10 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.8872
2022-02-26 16:58:44 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.9084
2022-02-26 16:59:17 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.9595
2022-02-26 16:59:50 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.7564
2022-02-26 17:00:24 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.8133
2022-02-26 17:00:59 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.5399
2022-02-26 17:01:31 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.8063
2022-02-26 17:02:05 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.7624
2022-02-26 17:02:38 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.8072
2022-02-26 17:03:12 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.8007
2022-02-26 17:03:45 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.6185
2022-02-26 17:04:18 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.8506
2022-02-26 17:04:52 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.8075
2022-02-26 17:05:26 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.8420
2022-02-26 17:06:01 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.9154
2022-02-26 17:06:36 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 2.2385
2022-02-26 17:07:10 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.9243
2022-02-26 17:07:46 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.9365
2022-02-26 17:08:20 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.7033
2022-02-26 17:08:22 - train: epoch 064, train_loss: 1.8403
2022-02-26 17:09:38 - eval: epoch: 064, acc1: 63.208%, acc5: 84.768%, test_loss: 1.5537, per_image_load_time: 1.284ms, per_image_inference_time: 0.142ms
2022-02-26 17:09:38 - until epoch: 064, best_acc1: 63.208%
2022-02-26 17:09:38 - epoch 065 lr: 0.0010000000000000002
2022-02-26 17:10:17 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.8285
2022-02-26 17:10:50 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.8437
2022-02-26 17:11:24 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.7018
2022-02-26 17:11:56 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.9800
2022-02-26 17:12:29 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.9621
2022-02-26 17:13:03 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 2.0709
2022-02-26 17:13:37 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.9034
2022-02-26 17:14:11 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.7862
2022-02-26 17:14:44 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.6426
2022-02-26 17:15:18 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.8119
2022-02-26 17:15:52 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.6944
2022-02-26 17:16:25 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 2.0929
2022-02-26 17:16:59 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.9190
2022-02-26 17:17:33 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.9245
2022-02-26 17:18:08 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.7872
2022-02-26 17:18:41 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.9236
2022-02-26 17:19:15 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.8761
2022-02-26 17:19:48 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.6829
2022-02-26 17:20:23 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.6943
2022-02-26 17:20:55 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.6985
2022-02-26 17:21:30 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.9364
2022-02-26 17:22:04 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.8131
2022-02-26 17:22:37 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.8796
2022-02-26 17:23:10 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.7396
2022-02-26 17:23:44 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.8397
2022-02-26 17:24:17 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.9164
2022-02-26 17:24:52 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.8854
2022-02-26 17:25:24 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.8645
2022-02-26 17:25:59 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.9286
2022-02-26 17:26:32 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.7917
2022-02-26 17:27:05 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.7962
2022-02-26 17:27:39 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.8531
2022-02-26 17:28:13 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.8222
2022-02-26 17:28:46 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.8437
2022-02-26 17:29:20 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 2.1250
2022-02-26 17:29:52 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.9043
2022-02-26 17:30:25 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.6282
2022-02-26 17:30:57 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.8019
2022-02-26 17:31:31 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 2.0826
2022-02-26 17:32:04 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.7607
2022-02-26 17:32:38 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.7781
2022-02-26 17:33:11 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.8213
2022-02-26 17:33:45 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.9328
2022-02-26 17:34:18 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.8266
2022-02-26 17:34:50 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.8293
2022-02-26 17:35:25 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 2.0085
2022-02-26 17:36:01 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 2.0481
2022-02-26 17:36:33 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.6479
2022-02-26 17:37:09 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.7861
2022-02-26 17:37:42 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.7567
2022-02-26 17:37:44 - train: epoch 065, train_loss: 1.8345
2022-02-26 17:38:59 - eval: epoch: 065, acc1: 63.296%, acc5: 84.874%, test_loss: 1.5528, per_image_load_time: 1.819ms, per_image_inference_time: 0.141ms
2022-02-26 17:38:59 - until epoch: 065, best_acc1: 63.296%
2022-02-26 17:38:59 - epoch 066 lr: 0.0010000000000000002
2022-02-26 17:39:37 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.5096
2022-02-26 17:40:11 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.8741
2022-02-26 17:40:44 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.6076
2022-02-26 17:41:17 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.6131
2022-02-26 17:41:50 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.8632
2022-02-26 17:42:23 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.6830
2022-02-26 17:42:56 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.8596
2022-02-26 17:43:29 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.9919
2022-02-26 17:44:02 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.7551
2022-02-26 17:44:37 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.7996
2022-02-26 17:45:09 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 2.0343
2022-02-26 17:45:42 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.8915
2022-02-26 17:46:15 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.8917
2022-02-26 17:46:49 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.6854
2022-02-26 17:47:22 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.8256
2022-02-26 17:47:55 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.9599
2022-02-26 17:48:27 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.6704
2022-02-26 17:49:02 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.5849
2022-02-26 17:49:34 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.9529
2022-02-26 17:50:08 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.6529
2022-02-26 17:50:40 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.8744
2022-02-26 17:51:14 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.6483
2022-02-26 17:51:48 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.7324
2022-02-26 17:52:22 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.9886
2022-02-26 17:52:55 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.7392
2022-02-26 17:53:28 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.7847
2022-02-26 17:54:03 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.9574
2022-02-26 17:54:35 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.8450
2022-02-26 17:55:09 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.8902
2022-02-26 17:55:43 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.6952
2022-02-26 17:56:18 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.9127
2022-02-26 17:56:49 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.8536
2022-02-26 17:57:24 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.7357
2022-02-26 17:57:57 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.9483
2022-02-26 17:58:30 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.8995
2022-02-26 17:59:04 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 2.0268
2022-02-26 17:59:36 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.7813
2022-02-26 18:00:11 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.8883
2022-02-26 18:00:44 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.8980
2022-02-26 18:01:18 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.8913
2022-02-26 18:01:50 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.8662
2022-02-26 18:02:24 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.6416
2022-02-26 18:02:58 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.5790
2022-02-26 18:03:30 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.6948
2022-02-26 18:04:04 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 2.1014
2022-02-26 18:04:39 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.9610
2022-02-26 18:05:12 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.7373
2022-02-26 18:05:49 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.7916
2022-02-26 18:06:24 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.7525
2022-02-26 18:06:59 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.7445
2022-02-26 18:07:01 - train: epoch 066, train_loss: 1.8297
2022-02-26 18:08:16 - eval: epoch: 066, acc1: 63.326%, acc5: 84.914%, test_loss: 1.5476, per_image_load_time: 2.777ms, per_image_inference_time: 0.129ms
2022-02-26 18:08:16 - until epoch: 066, best_acc1: 63.326%
2022-02-26 18:08:16 - epoch 067 lr: 0.0010000000000000002
2022-02-26 18:08:55 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.9054
2022-02-26 18:09:29 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.9198
2022-02-26 18:10:02 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.9163
2022-02-26 18:10:35 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.7511
2022-02-26 18:11:09 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.7038
2022-02-26 18:11:42 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.8820
2022-02-26 18:12:16 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.9188
2022-02-26 18:12:50 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 2.0477
2022-02-26 18:13:23 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 2.1419
2022-02-26 18:13:57 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.7739
2022-02-26 18:14:31 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.8813
2022-02-26 18:15:03 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.6333
2022-02-26 18:15:37 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.6536
2022-02-26 18:16:10 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.8889
2022-02-26 18:16:45 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.6517
2022-02-26 18:17:17 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.7419
2022-02-26 18:17:52 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.7862
2022-02-26 18:18:24 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 2.1700
2022-02-26 18:18:58 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.7185
2022-02-26 18:19:31 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.8153
2022-02-26 18:20:04 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.6341
2022-02-26 18:20:37 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.8015
2022-02-26 18:21:12 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.8817
2022-02-26 18:21:45 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.7929
2022-02-26 18:22:18 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 2.0153
2022-02-26 18:22:52 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.7718
2022-02-26 18:23:25 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.6446
2022-02-26 18:23:59 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 2.0044
2022-02-26 18:24:33 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.8654
2022-02-26 18:25:07 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.6622
2022-02-26 18:25:40 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.8199
2022-02-26 18:26:14 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.9256
2022-02-26 18:26:46 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.7152
2022-02-26 18:27:21 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.8998
2022-02-26 18:27:54 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.9434
2022-02-26 18:28:28 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.7070
2022-02-26 18:29:00 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.8998
2022-02-26 18:29:34 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.6860
2022-02-26 18:30:06 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 2.1149
2022-02-26 18:30:40 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.8719
2022-02-26 18:31:14 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.7746
2022-02-26 18:31:47 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.7980
2022-02-26 18:32:21 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.6797
2022-02-26 18:32:55 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.7359
2022-02-26 18:33:31 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.8284
2022-02-26 18:34:05 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.5520
2022-02-26 18:34:42 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.8553
2022-02-26 18:35:17 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.7279
2022-02-26 18:35:54 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.8721
2022-02-26 18:36:29 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 2.0513
2022-02-26 18:36:31 - train: epoch 067, train_loss: 1.8258
2022-02-26 18:37:45 - eval: epoch: 067, acc1: 63.520%, acc5: 84.950%, test_loss: 1.5468, per_image_load_time: 2.632ms, per_image_inference_time: 0.150ms
2022-02-26 18:37:45 - until epoch: 067, best_acc1: 63.520%
2022-02-26 18:37:45 - epoch 068 lr: 0.0010000000000000002
2022-02-26 18:38:23 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.8294
2022-02-26 18:38:57 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.9828
2022-02-26 18:39:32 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.9245
2022-02-26 18:40:05 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.7156
2022-02-26 18:40:39 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.7768
2022-02-26 18:41:12 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.7845
2022-02-26 18:41:46 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.8458
2022-02-26 18:42:21 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.5485
2022-02-26 18:42:54 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.9522
2022-02-26 18:43:28 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.8438
2022-02-26 18:44:01 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 2.0669
2022-02-26 18:44:36 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.5372
2022-02-26 18:45:09 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.8457
2022-02-26 18:45:44 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.7870
2022-02-26 18:46:16 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 2.0936
2022-02-26 18:46:50 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.7060
2022-02-26 18:47:23 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.8393
2022-02-26 18:47:58 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.8749
2022-02-26 18:48:30 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.7347
2022-02-26 18:49:05 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 2.0184
2022-02-26 18:49:39 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.8091
2022-02-26 18:50:13 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.7590
2022-02-26 18:50:46 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.6048
2022-02-26 18:51:22 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.8149
2022-02-26 18:51:55 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.7713
2022-02-26 18:52:29 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.9143
2022-02-26 18:53:02 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.8166
2022-02-26 18:53:36 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.8371
2022-02-26 18:54:10 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.9974
2022-02-26 18:54:44 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.8662
2022-02-26 18:55:17 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.6215
2022-02-26 18:55:52 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.9246
2022-02-26 18:56:25 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.7031
2022-02-26 18:57:00 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.7216
2022-02-26 18:57:32 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 2.0486
2022-02-26 18:58:07 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.9601
2022-02-26 18:58:40 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.8003
2022-02-26 18:59:13 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.9757
2022-02-26 18:59:47 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.8937
2022-02-26 19:00:21 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.7778
2022-02-26 19:00:55 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.6772
2022-02-26 19:01:29 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.6916
2022-02-26 19:02:03 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 2.0069
2022-02-26 19:02:37 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.8791
2022-02-26 19:03:13 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.8963
2022-02-26 19:03:49 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.6484
2022-02-26 19:04:24 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.9279
2022-02-26 19:04:59 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.8479
2022-02-26 19:05:36 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.7592
2022-02-26 19:06:11 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.7122
2022-02-26 19:06:14 - train: epoch 068, train_loss: 1.8221
2022-02-26 19:07:33 - eval: epoch: 068, acc1: 63.344%, acc5: 85.018%, test_loss: 1.5405, per_image_load_time: 2.932ms, per_image_inference_time: 0.127ms
2022-02-26 19:07:33 - until epoch: 068, best_acc1: 63.520%
2022-02-26 19:07:33 - epoch 069 lr: 0.0010000000000000002
2022-02-26 19:08:12 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.9466
2022-02-26 19:08:46 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.8981
2022-02-26 19:09:20 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.8765
2022-02-26 19:09:52 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.6988
2022-02-26 19:10:27 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.9514
2022-02-26 19:10:59 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.6843
2022-02-26 19:11:33 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.8950
2022-02-26 19:12:07 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.8298
2022-02-26 19:12:40 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.7221
2022-02-26 19:13:14 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.8600
2022-02-26 19:13:47 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.7058
2022-02-26 19:14:21 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.6437
2022-02-26 19:14:55 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.8523
2022-02-26 19:15:30 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.8134
2022-02-26 19:16:02 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.7661
2022-02-26 19:16:37 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.9308
2022-02-26 19:17:10 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.8091
2022-02-26 19:17:44 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.6926
2022-02-26 19:18:17 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.7696
2022-02-26 19:18:52 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.6046
2022-02-26 19:19:25 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.7971
2022-02-26 19:19:59 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.8905
2022-02-26 19:20:33 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.6622
2022-02-26 19:21:07 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 2.0020
2022-02-26 19:21:40 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.6620
2022-02-26 19:22:13 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.7480
2022-02-26 19:22:48 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.8733
2022-02-26 19:23:20 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 2.2188
2022-02-26 19:23:54 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.7753
2022-02-26 19:24:27 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.7883
2022-02-26 19:25:01 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.8588
2022-02-26 19:25:34 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.6393
2022-02-26 19:26:07 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.6567
2022-02-26 19:26:40 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.7661
2022-02-26 19:27:15 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.7611
2022-02-26 19:27:47 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.6009
2022-02-26 19:28:21 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.7576
2022-02-26 19:28:55 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.9221
2022-02-26 19:29:28 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.8048
2022-02-26 19:30:02 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.9550
2022-02-26 19:30:35 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.7575
2022-02-26 19:31:09 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.6709
2022-02-26 19:31:42 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.7244
2022-02-26 19:32:15 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.8901
2022-02-26 19:32:50 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.8957
2022-02-26 19:33:25 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.8932
2022-02-26 19:34:00 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.8161
2022-02-26 19:34:35 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.8434
2022-02-26 19:35:13 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.7325
2022-02-26 19:35:48 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.8043
2022-02-26 19:35:50 - train: epoch 069, train_loss: 1.8182
2022-02-26 19:37:08 - eval: epoch: 069, acc1: 63.486%, acc5: 85.012%, test_loss: 1.5431, per_image_load_time: 2.908ms, per_image_inference_time: 0.129ms
2022-02-26 19:37:08 - until epoch: 069, best_acc1: 63.520%
2022-02-26 19:37:08 - epoch 070 lr: 0.0010000000000000002
2022-02-26 19:37:47 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 2.0282
2022-02-26 19:38:21 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.7758
2022-02-26 19:38:55 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.8924
2022-02-26 19:39:27 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.8101
2022-02-26 19:40:01 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.8130
2022-02-26 19:40:35 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.8440
2022-02-26 19:41:08 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.6654
2022-02-26 19:41:43 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.7333
2022-02-26 19:42:16 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.6692
2022-02-26 19:42:50 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.9117
2022-02-26 19:43:23 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 2.0693
2022-02-26 19:43:56 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 2.0153
2022-02-26 19:44:28 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.7592
2022-02-26 19:45:02 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.7004
2022-02-26 19:45:35 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.7912
2022-02-26 19:46:10 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.9199
2022-02-26 19:46:44 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.7802
2022-02-26 19:47:17 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.5805
2022-02-26 19:47:51 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.6761
2022-02-26 19:48:25 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.7924
2022-02-26 19:48:58 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 2.0092
2022-02-26 19:49:32 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.9669
2022-02-26 19:50:05 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.9664
2022-02-26 19:50:40 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.9698
2022-02-26 19:51:12 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.7555
2022-02-26 19:51:46 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.7948
2022-02-26 19:52:18 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.7860
2022-02-26 19:52:52 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.9429
2022-02-26 19:53:24 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.6964
2022-02-26 19:53:58 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.8638
2022-02-26 19:54:31 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.7750
2022-02-26 19:55:05 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.7869
2022-02-26 19:55:37 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.9224
2022-02-26 19:56:12 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.6789
2022-02-26 19:56:45 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.8131
2022-02-26 19:57:19 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.8659
2022-02-26 19:57:50 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.7650
2022-02-26 19:58:25 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.6651
2022-02-26 19:58:57 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.7133
2022-02-26 19:59:32 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.8776
2022-02-26 20:00:05 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.7704
2022-02-26 20:00:38 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.7096
2022-02-26 20:01:12 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.7839
2022-02-26 20:01:46 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.8674
2022-02-26 20:02:19 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.9067
2022-02-26 20:02:54 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.9346
2022-02-26 20:03:29 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.7998
2022-02-26 20:04:04 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.8620
2022-02-26 20:04:40 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.8864
2022-02-26 20:05:16 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.9152
2022-02-26 20:05:18 - train: epoch 070, train_loss: 1.8174
2022-02-26 20:06:33 - eval: epoch: 070, acc1: 63.494%, acc5: 85.044%, test_loss: 1.5387, per_image_load_time: 0.996ms, per_image_inference_time: 0.130ms
2022-02-26 20:06:34 - until epoch: 070, best_acc1: 63.520%
2022-02-26 20:06:34 - epoch 071 lr: 0.0010000000000000002
2022-02-26 20:07:13 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.6412
2022-02-26 20:07:48 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.7494
2022-02-26 20:08:21 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.9789
2022-02-26 20:08:55 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.7345
2022-02-26 20:09:28 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.9646
2022-02-26 20:10:03 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.7653
2022-02-26 20:10:36 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.8574
2022-02-26 20:11:10 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.7707
2022-02-26 20:11:43 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.9540
2022-02-26 20:12:17 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.9022
2022-02-26 20:12:50 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.9674
2022-02-26 20:13:24 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.8716
2022-02-26 20:13:57 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.7739
2022-02-26 20:14:31 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.8706
2022-02-26 20:15:04 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.6956
2022-02-26 20:15:38 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.7809
2022-02-26 20:16:11 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.8402
2022-02-26 20:16:45 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.9629
2022-02-26 20:17:18 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.7486
2022-02-26 20:17:52 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.7951
2022-02-26 20:18:25 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.7651
2022-02-26 20:18:58 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.7525
2022-02-26 20:19:31 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.6996
2022-02-26 20:20:05 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.7385
2022-02-26 20:20:36 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.8865
2022-02-26 20:21:10 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.6340
2022-02-26 20:21:43 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 2.0098
2022-02-26 20:22:17 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.8314
2022-02-26 20:22:49 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.7154
2022-02-26 20:23:24 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 2.0170
2022-02-26 20:23:56 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.5567
2022-02-26 20:24:29 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.7661
2022-02-26 20:25:03 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.7539
2022-02-26 20:25:35 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.6789
2022-02-26 20:26:09 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.9806
2022-02-26 20:26:43 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.7829
2022-02-26 20:27:15 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.8227
2022-02-26 20:27:50 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.8545
2022-02-26 20:28:22 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.7649
2022-02-26 20:28:56 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.7946
2022-02-26 20:29:28 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.8556
2022-02-26 20:30:02 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.9602
2022-02-26 20:30:34 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.6049
2022-02-26 20:31:08 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.9951
2022-02-26 20:31:41 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.8697
2022-02-26 20:32:15 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.9671
2022-02-26 20:32:48 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.8012
2022-02-26 20:33:22 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.8179
2022-02-26 20:33:54 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.5809
2022-02-26 20:34:27 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.9252
2022-02-26 20:34:28 - train: epoch 071, train_loss: 1.8154
2022-02-26 20:35:44 - eval: epoch: 071, acc1: 63.550%, acc5: 85.100%, test_loss: 1.5364, per_image_load_time: 1.440ms, per_image_inference_time: 0.139ms
2022-02-26 20:35:44 - until epoch: 071, best_acc1: 63.550%
2022-02-26 20:35:44 - epoch 072 lr: 0.0010000000000000002
2022-02-26 20:36:22 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 2.0726
2022-02-26 20:36:55 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.7297
2022-02-26 20:37:28 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.6903
2022-02-26 20:38:01 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.8455
2022-02-26 20:38:34 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.6177
2022-02-26 20:39:08 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.6072
2022-02-26 20:39:40 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.7190
2022-02-26 20:40:14 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.9684
2022-02-26 20:40:48 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.7060
2022-02-26 20:41:21 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.5506
2022-02-26 20:41:55 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 2.0245
2022-02-26 20:42:28 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.7068
2022-02-26 20:43:02 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.7745
2022-02-26 20:43:35 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.8596
2022-02-26 20:44:09 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.8987
2022-02-26 20:44:42 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.7916
2022-02-26 20:45:16 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.7652
2022-02-26 20:45:49 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.7676
2022-02-26 20:46:22 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.9043
2022-02-26 20:46:56 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.7813
2022-02-26 20:47:30 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.9509
2022-02-26 20:48:03 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.8422
2022-02-26 20:48:37 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.8721
2022-02-26 20:49:10 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 2.0479
2022-02-26 20:49:43 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.7497
2022-02-26 20:50:17 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.8789
2022-02-26 20:50:50 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 2.0616
2022-02-26 20:51:25 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.8017
2022-02-26 20:51:57 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.5814
2022-02-26 20:52:32 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.6868
2022-02-26 20:53:05 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.8706
2022-02-26 20:53:40 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.6151
2022-02-26 20:54:12 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.9107
2022-02-26 20:54:47 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 2.1744
2022-02-26 20:55:19 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 2.0120
2022-02-26 20:55:54 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.9373
2022-02-26 20:56:26 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.9897
2022-02-26 20:57:00 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.8566
2022-02-26 20:57:33 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.9152
2022-02-26 20:58:07 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.9036
2022-02-26 20:58:40 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 2.0302
2022-02-26 20:59:13 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.6364
2022-02-26 20:59:46 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.7663
2022-02-26 21:00:20 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.7186
2022-02-26 21:00:52 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.8450
2022-02-26 21:01:26 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.8982
2022-02-26 21:01:58 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.9273
2022-02-26 21:02:32 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.9493
2022-02-26 21:03:05 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.9746
2022-02-26 21:03:37 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.7769
2022-02-26 21:03:39 - train: epoch 072, train_loss: 1.8138
2022-02-26 21:04:55 - eval: epoch: 072, acc1: 63.586%, acc5: 85.054%, test_loss: 1.5348, per_image_load_time: 1.502ms, per_image_inference_time: 0.126ms
2022-02-26 21:04:55 - until epoch: 072, best_acc1: 63.586%
2022-02-26 21:04:55 - epoch 073 lr: 0.0010000000000000002
2022-02-26 21:05:33 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 2.0462
2022-02-26 21:06:07 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.9418
2022-02-26 21:06:40 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.7679
2022-02-26 21:07:12 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.5628
2022-02-26 21:07:47 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.8472
2022-02-26 21:08:19 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.6991
2022-02-26 21:08:53 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.8529
2022-02-26 21:09:25 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.6981
2022-02-26 21:09:59 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.6690
2022-02-26 21:10:32 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.7370
2022-02-26 21:11:05 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.8645
2022-02-26 21:11:38 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.9270
2022-02-26 21:12:10 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.8462
2022-02-26 21:12:43 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.7108
2022-02-26 21:13:16 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.6847
2022-02-26 21:13:49 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.7313
2022-02-26 21:14:24 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 2.0153
2022-02-26 21:14:56 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.5800
2022-02-26 21:15:29 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 2.0536
2022-02-26 21:16:01 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.6015
2022-02-26 21:16:36 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.6843
2022-02-26 21:17:08 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.7346
2022-02-26 21:17:42 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.8700
2022-02-26 21:18:15 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.7775
2022-02-26 21:18:49 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.8704
2022-02-26 21:19:21 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.6718
2022-02-26 21:19:55 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.7108
2022-02-26 21:20:27 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.7793
2022-02-26 21:21:01 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.9872
2022-02-26 21:21:33 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.7774
2022-02-26 21:22:08 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.8122
2022-02-26 21:22:40 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.6072
2022-02-26 21:23:13 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.8460
2022-02-26 21:23:46 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.9378
2022-02-26 21:24:19 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.8493
2022-02-26 21:24:51 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.6199
2022-02-26 21:25:24 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.8394
2022-02-26 21:25:57 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.9285
2022-02-26 21:26:31 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.7406
2022-02-26 21:27:04 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.8224
2022-02-26 21:27:36 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.7233
2022-02-26 21:28:09 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 2.0974
2022-02-26 21:28:44 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.8907
2022-02-26 21:29:16 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.9075
2022-02-26 21:29:52 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.8768
2022-02-26 21:30:26 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.8965
2022-02-26 21:31:03 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.7659
2022-02-26 21:31:46 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.6892
2022-02-26 21:32:27 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.7795
2022-02-26 21:33:07 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.9216
2022-02-26 21:33:10 - train: epoch 073, train_loss: 1.8129
2022-02-26 21:34:27 - eval: epoch: 073, acc1: 63.412%, acc5: 85.008%, test_loss: 1.5359, per_image_load_time: 1.277ms, per_image_inference_time: 0.130ms
2022-02-26 21:34:27 - until epoch: 073, best_acc1: 63.586%
2022-02-26 21:34:27 - epoch 074 lr: 0.0010000000000000002
2022-02-26 21:35:06 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.9495
2022-02-26 21:35:40 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 2.0995
2022-02-26 21:36:13 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.8551
2022-02-26 21:36:46 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.9351
2022-02-26 21:37:18 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.7594
2022-02-26 21:37:52 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.8205
2022-02-26 21:38:24 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.8560
2022-02-26 21:38:56 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.8779
2022-02-26 21:39:29 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.7354
2022-02-26 21:40:03 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.9542
2022-02-26 21:40:35 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.6441
2022-02-26 21:41:08 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.7579
2022-02-26 21:41:41 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.8875
2022-02-26 21:42:15 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.7826
2022-02-26 21:42:47 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.8124
2022-02-26 21:43:20 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.8455
2022-02-26 21:43:53 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.9291
2022-02-26 21:44:25 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 2.0819
2022-02-26 21:44:59 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.7507
2022-02-26 21:45:32 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.6305
2022-02-26 21:46:06 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.7683
2022-02-26 21:46:38 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.9839
2022-02-26 21:47:12 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.8895
2022-02-26 21:47:44 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.7946
2022-02-26 21:48:18 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.6216
2022-02-26 21:48:50 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.7832
2022-02-26 21:49:24 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.9055
2022-02-26 21:49:57 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 2.1551
2022-02-26 21:50:29 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.9040
2022-02-26 21:51:02 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.7561
2022-02-26 21:51:36 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.7660
2022-02-26 21:52:09 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.6933
2022-02-26 21:52:42 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 2.1543
2022-02-26 21:53:15 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.7849
2022-02-26 21:53:48 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.8393
2022-02-26 21:54:22 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 2.0283
2022-02-26 21:54:55 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.9233
2022-02-26 21:55:28 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.6579
2022-02-26 21:56:01 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.6824
2022-02-26 21:56:33 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 2.0614
2022-02-26 21:57:06 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.9355
2022-02-26 21:57:39 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.7990
2022-02-26 21:58:12 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.7775
2022-02-26 21:58:48 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.8704
2022-02-26 21:59:21 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.8996
2022-02-26 21:59:55 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.9757
2022-02-26 22:00:28 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.7641
2022-02-26 22:01:03 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.7826
2022-02-26 22:01:40 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 2.0030
2022-02-26 22:02:17 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.7812
2022-02-26 22:02:19 - train: epoch 074, train_loss: 1.8093
2022-02-26 22:03:41 - eval: epoch: 074, acc1: 63.546%, acc5: 85.122%, test_loss: 1.5348, per_image_load_time: 1.026ms, per_image_inference_time: 0.118ms
2022-02-26 22:03:41 - until epoch: 074, best_acc1: 63.586%
2022-02-26 22:03:41 - epoch 075 lr: 0.0010000000000000002
2022-02-26 22:04:20 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 2.0008
2022-02-26 22:04:55 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 2.0193
2022-02-26 22:05:27 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.7879
2022-02-26 22:06:00 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.7240
2022-02-26 22:06:33 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.8797
2022-02-26 22:07:06 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.7432
2022-02-26 22:07:39 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 2.0887
2022-02-26 22:08:13 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.8201
2022-02-26 22:08:48 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.9031
2022-02-26 22:09:20 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.5601
2022-02-26 22:09:55 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.8690
2022-02-26 22:10:27 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.6892
2022-02-26 22:11:00 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.9509
2022-02-26 22:11:33 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.6130
2022-02-26 22:12:07 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.7632
2022-02-26 22:12:39 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.6146
2022-02-26 22:13:12 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 2.0439
2022-02-26 22:13:45 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.7914
2022-02-26 22:14:18 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.7445
2022-02-26 22:14:51 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.7911
2022-02-26 22:15:24 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.7928
2022-02-26 22:15:58 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.8976
2022-02-26 22:16:31 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.6727
2022-02-26 22:17:03 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.8628
2022-02-26 22:17:38 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.8730
2022-02-26 22:18:11 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.9536
2022-02-26 22:18:44 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.4961
2022-02-26 22:19:18 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.8577
2022-02-26 22:19:51 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.9712
2022-02-26 22:20:24 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.9186
2022-02-26 22:20:57 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.8419
2022-02-26 22:21:30 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.8146
2022-02-26 22:22:04 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.8333
2022-02-26 22:22:37 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.7631
2022-02-26 22:23:11 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.7333
2022-02-26 22:23:45 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.8546
2022-02-26 22:24:18 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.8753
2022-02-26 22:24:52 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.7560
2022-02-26 22:25:24 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.7847
2022-02-26 22:25:59 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.6147
2022-02-26 22:26:31 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.6855
2022-02-26 22:27:04 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.9533
2022-02-26 22:27:38 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 2.0464
2022-02-26 22:28:12 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.8453
2022-02-26 22:28:48 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.8698
2022-02-26 22:29:24 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.6508
2022-02-26 22:30:01 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.9149
2022-02-26 22:30:38 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.6397
2022-02-26 22:31:16 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.7498
2022-02-26 22:31:50 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.8104
2022-02-26 22:31:52 - train: epoch 075, train_loss: 1.8097
2022-02-26 22:33:09 - eval: epoch: 075, acc1: 63.718%, acc5: 85.134%, test_loss: 1.5315, per_image_load_time: 0.964ms, per_image_inference_time: 0.127ms
2022-02-26 22:33:09 - until epoch: 075, best_acc1: 63.718%
2022-02-26 22:33:09 - epoch 076 lr: 0.0010000000000000002
2022-02-26 22:33:49 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.6760
2022-02-26 22:34:22 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.7561
2022-02-26 22:34:56 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.8994
2022-02-26 22:35:29 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.8904
2022-02-26 22:36:02 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.8098
2022-02-26 22:36:36 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.8148
2022-02-26 22:37:09 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.7462
2022-02-26 22:37:43 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.7470
2022-02-26 22:38:28 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.8116
2022-02-26 22:39:02 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.8054
2022-02-26 22:39:35 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.6418
2022-02-26 22:40:09 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.7417
2022-02-26 22:40:42 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.7903
2022-02-26 22:41:15 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.8290
2022-02-26 22:41:48 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.7816
2022-02-26 22:42:22 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.8010
2022-02-26 22:42:54 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.7785
