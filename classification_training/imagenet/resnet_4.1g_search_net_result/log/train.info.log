2022-08-23 23:05:51 - network: resnet_4.1g
2022-08-23 23:05:51 - num_classes: 1000
2022-08-23 23:05:51 - input_image_size: 224
2022-08-23 23:05:51 - scale: 1.1428571428571428
2022-08-23 23:05:51 - net_config: {'stem_width': 64, 'depth': 18, 'w_0': 32, 'w_a': 17.77597968937125, 'w_m': 1.9609275565633586}
2022-08-23 23:05:51 - trained_model_path: 
2022-08-23 23:05:51 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-23 23:05:51 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-23 23:05:51 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fd5b4467fd0>
2022-08-23 23:05:51 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fd5b22f72e0>
2022-08-23 23:05:51 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fd5b22f7310>
2022-08-23 23:05:51 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fd5b22f7370>
2022-08-23 23:05:51 - seed: 0
2022-08-23 23:05:51 - batch_size: 256
2022-08-23 23:05:51 - num_workers: 16
2022-08-23 23:05:51 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-08-23 23:05:51 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-08-23 23:05:51 - epochs: 100
2022-08-23 23:05:51 - print_interval: 100
2022-08-23 23:05:51 - accumulation_steps: 1
2022-08-23 23:05:51 - sync_bn: False
2022-08-23 23:05:51 - apex: True
2022-08-23 23:05:51 - use_ema_model: False
2022-08-23 23:05:51 - ema_model_decay: 0.9999
2022-08-23 23:05:51 - gpus_type: NVIDIA RTX A5000
2022-08-23 23:05:51 - gpus_num: 2
2022-08-23 23:05:51 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fd595431eb0>
2022-08-23 23:05:51 - --------------------parameters--------------------
2022-08-23 23:05:51 - name: conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.0.weight, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.weight, grad: True
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.bias, grad: True
2022-08-23 23:05:51 - name: fc.weight, grad: True
2022-08-23 23:05:51 - name: fc.bias, grad: True
2022-08-23 23:05:51 - --------------------buffers--------------------
2022-08-23 23:05:51 - name: conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.running_mean, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.running_var, grad: False
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.num_batches_tracked, grad: False
2022-08-23 23:05:51 - -----------no weight decay layers--------------
2022-08-23 23:05:51 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-23 23:05:51 - -------------weight decay layers---------------
2022-08-23 23:05:51 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.6.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.7.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.8.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: layer4.9.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-08-23 23:05:51 - epoch 001 lr: 0.100000
2022-08-23 23:06:33 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9061
2022-08-23 23:07:08 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.8912
2022-08-23 23:07:44 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8927
2022-08-23 23:08:19 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.8267
2022-08-23 23:08:55 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.8588
2022-08-23 23:09:30 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.7987
2022-08-23 23:10:05 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.7530
2022-08-23 23:10:40 - train: epoch 0001, iter [00800, 05004], lr: 0.099999, loss: 6.7346
2022-08-23 23:11:16 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 6.7127
2022-08-23 23:11:51 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 6.7104
2022-08-23 23:12:25 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 6.5776
2022-08-23 23:13:00 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 6.5270
2022-08-23 23:13:36 - train: epoch 0001, iter [01300, 05004], lr: 0.099998, loss: 6.6378
2022-08-23 23:14:11 - train: epoch 0001, iter [01400, 05004], lr: 0.099998, loss: 6.5536
2022-08-23 23:14:46 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 6.3745
2022-08-23 23:15:21 - train: epoch 0001, iter [01600, 05004], lr: 0.099997, loss: 6.3879
2022-08-23 23:15:57 - train: epoch 0001, iter [01700, 05004], lr: 0.099997, loss: 6.2376
2022-08-23 23:16:32 - train: epoch 0001, iter [01800, 05004], lr: 0.099997, loss: 6.2538
2022-08-23 23:17:07 - train: epoch 0001, iter [01900, 05004], lr: 0.099996, loss: 6.2527
2022-08-23 23:17:42 - train: epoch 0001, iter [02000, 05004], lr: 0.099996, loss: 6.1981
2022-08-23 23:18:18 - train: epoch 0001, iter [02100, 05004], lr: 0.099996, loss: 6.1108
2022-08-23 23:18:53 - train: epoch 0001, iter [02200, 05004], lr: 0.099995, loss: 5.9896
2022-08-23 23:19:28 - train: epoch 0001, iter [02300, 05004], lr: 0.099995, loss: 5.9266
2022-08-23 23:20:03 - train: epoch 0001, iter [02400, 05004], lr: 0.099994, loss: 5.8634
2022-08-23 23:20:38 - train: epoch 0001, iter [02500, 05004], lr: 0.099994, loss: 5.7669
2022-08-23 23:21:13 - train: epoch 0001, iter [02600, 05004], lr: 0.099993, loss: 5.9068
2022-08-23 23:21:48 - train: epoch 0001, iter [02700, 05004], lr: 0.099993, loss: 5.8152
2022-08-23 23:22:23 - train: epoch 0001, iter [02800, 05004], lr: 0.099992, loss: 5.6582
2022-08-23 23:22:58 - train: epoch 0001, iter [02900, 05004], lr: 0.099992, loss: 5.5781
2022-08-23 23:23:34 - train: epoch 0001, iter [03000, 05004], lr: 0.099991, loss: 5.6145
2022-08-23 23:24:10 - train: epoch 0001, iter [03100, 05004], lr: 0.099991, loss: 5.6088
2022-08-23 23:24:43 - train: epoch 0001, iter [03200, 05004], lr: 0.099990, loss: 5.5584
2022-08-23 23:25:19 - train: epoch 0001, iter [03300, 05004], lr: 0.099989, loss: 5.4058
2022-08-23 23:25:55 - train: epoch 0001, iter [03400, 05004], lr: 0.099989, loss: 5.4225
2022-08-23 23:26:30 - train: epoch 0001, iter [03500, 05004], lr: 0.099988, loss: 5.2363
2022-08-23 23:27:05 - train: epoch 0001, iter [03600, 05004], lr: 0.099987, loss: 5.3812
2022-08-23 23:27:40 - train: epoch 0001, iter [03700, 05004], lr: 0.099987, loss: 5.4162
2022-08-23 23:28:15 - train: epoch 0001, iter [03800, 05004], lr: 0.099986, loss: 5.1240
2022-08-23 23:28:49 - train: epoch 0001, iter [03900, 05004], lr: 0.099985, loss: 5.1886
2022-08-23 23:29:24 - train: epoch 0001, iter [04000, 05004], lr: 0.099984, loss: 5.1569
2022-08-23 23:29:59 - train: epoch 0001, iter [04100, 05004], lr: 0.099983, loss: 5.2465
2022-08-23 23:30:35 - train: epoch 0001, iter [04200, 05004], lr: 0.099983, loss: 5.0386
2022-08-23 23:31:10 - train: epoch 0001, iter [04300, 05004], lr: 0.099982, loss: 4.9728
2022-08-23 23:31:44 - train: epoch 0001, iter [04400, 05004], lr: 0.099981, loss: 4.7616
2022-08-23 23:32:19 - train: epoch 0001, iter [04500, 05004], lr: 0.099980, loss: 5.0014
2022-08-23 23:32:55 - train: epoch 0001, iter [04600, 05004], lr: 0.099979, loss: 5.0541
2022-08-23 23:33:30 - train: epoch 0001, iter [04700, 05004], lr: 0.099978, loss: 4.7245
2022-08-23 23:34:05 - train: epoch 0001, iter [04800, 05004], lr: 0.099977, loss: 5.0441
2022-08-23 23:34:40 - train: epoch 0001, iter [04900, 05004], lr: 0.099976, loss: 4.8949
2022-08-23 23:35:14 - train: epoch 0001, iter [05000, 05004], lr: 0.099975, loss: 4.6815
2022-08-23 23:35:16 - train: epoch 001, train_loss: 5.8767
2022-08-23 23:36:32 - eval: epoch: 001, acc1: 12.396%, acc5: 30.206%, test_loss: 4.6599, per_image_load_time: 2.305ms, per_image_inference_time: 0.595ms
2022-08-23 23:36:33 - until epoch: 001, best_acc1: 12.396%
2022-08-23 23:36:33 - epoch 002 lr: 0.099975
2022-08-23 23:37:13 - train: epoch 0002, iter [00100, 05004], lr: 0.099974, loss: 4.7634
2022-08-23 23:37:49 - train: epoch 0002, iter [00200, 05004], lr: 0.099973, loss: 4.5269
2022-08-23 23:38:23 - train: epoch 0002, iter [00300, 05004], lr: 0.099972, loss: 4.8861
2022-08-23 23:38:58 - train: epoch 0002, iter [00400, 05004], lr: 0.099971, loss: 4.7169
2022-08-23 23:39:31 - train: epoch 0002, iter [00500, 05004], lr: 0.099970, loss: 4.4786
2022-08-23 23:40:06 - train: epoch 0002, iter [00600, 05004], lr: 0.099969, loss: 4.4378
2022-08-23 23:40:40 - train: epoch 0002, iter [00700, 05004], lr: 0.099968, loss: 4.7011
2022-08-23 23:41:15 - train: epoch 0002, iter [00800, 05004], lr: 0.099967, loss: 4.3377
2022-08-23 23:41:50 - train: epoch 0002, iter [00900, 05004], lr: 0.099966, loss: 4.0859
2022-08-23 23:42:25 - train: epoch 0002, iter [01000, 05004], lr: 0.099964, loss: 4.5812
2022-08-23 23:42:59 - train: epoch 0002, iter [01100, 05004], lr: 0.099963, loss: 4.4316
2022-08-23 23:43:34 - train: epoch 0002, iter [01200, 05004], lr: 0.099962, loss: 4.3653
2022-08-23 23:44:08 - train: epoch 0002, iter [01300, 05004], lr: 0.099961, loss: 4.2539
2022-08-23 23:44:42 - train: epoch 0002, iter [01400, 05004], lr: 0.099960, loss: 4.4990
2022-08-23 23:45:17 - train: epoch 0002, iter [01500, 05004], lr: 0.099958, loss: 4.3411
2022-08-23 23:45:52 - train: epoch 0002, iter [01600, 05004], lr: 0.099957, loss: 4.3013
2022-08-23 23:46:26 - train: epoch 0002, iter [01700, 05004], lr: 0.099956, loss: 4.2559
2022-08-23 23:47:00 - train: epoch 0002, iter [01800, 05004], lr: 0.099954, loss: 4.2942
2022-08-23 23:47:34 - train: epoch 0002, iter [01900, 05004], lr: 0.099953, loss: 4.0287
2022-08-23 23:48:09 - train: epoch 0002, iter [02000, 05004], lr: 0.099952, loss: 3.8614
2022-08-23 23:48:43 - train: epoch 0002, iter [02100, 05004], lr: 0.099950, loss: 4.1655
2022-08-23 23:49:18 - train: epoch 0002, iter [02200, 05004], lr: 0.099949, loss: 3.8890
2022-08-23 23:49:53 - train: epoch 0002, iter [02300, 05004], lr: 0.099947, loss: 4.1080
2022-08-23 23:50:27 - train: epoch 0002, iter [02400, 05004], lr: 0.099946, loss: 3.9149
2022-08-23 23:51:01 - train: epoch 0002, iter [02500, 05004], lr: 0.099945, loss: 3.9681
2022-08-23 23:51:36 - train: epoch 0002, iter [02600, 05004], lr: 0.099943, loss: 3.8486
2022-08-23 23:52:11 - train: epoch 0002, iter [02700, 05004], lr: 0.099942, loss: 4.1381
2022-08-23 23:52:44 - train: epoch 0002, iter [02800, 05004], lr: 0.099940, loss: 3.9091
2022-08-23 23:53:19 - train: epoch 0002, iter [02900, 05004], lr: 0.099938, loss: 3.7980
2022-08-23 23:53:53 - train: epoch 0002, iter [03000, 05004], lr: 0.099937, loss: 3.7067
2022-08-23 23:54:27 - train: epoch 0002, iter [03100, 05004], lr: 0.099935, loss: 3.8232
2022-08-23 23:55:02 - train: epoch 0002, iter [03200, 05004], lr: 0.099934, loss: 3.7924
2022-08-23 23:55:36 - train: epoch 0002, iter [03300, 05004], lr: 0.099932, loss: 3.8362
2022-08-23 23:56:11 - train: epoch 0002, iter [03400, 05004], lr: 0.099930, loss: 3.8373
2022-08-23 23:56:46 - train: epoch 0002, iter [03500, 05004], lr: 0.099929, loss: 3.7077
2022-08-23 23:57:20 - train: epoch 0002, iter [03600, 05004], lr: 0.099927, loss: 3.7387
2022-08-23 23:57:54 - train: epoch 0002, iter [03700, 05004], lr: 0.099925, loss: 3.7968
2022-08-23 23:58:29 - train: epoch 0002, iter [03800, 05004], lr: 0.099924, loss: 3.6000
2022-08-23 23:59:04 - train: epoch 0002, iter [03900, 05004], lr: 0.099922, loss: 3.7600
2022-08-23 23:59:39 - train: epoch 0002, iter [04000, 05004], lr: 0.099920, loss: 3.5498
2022-08-24 00:00:14 - train: epoch 0002, iter [04100, 05004], lr: 0.099918, loss: 3.7163
2022-08-24 00:00:48 - train: epoch 0002, iter [04200, 05004], lr: 0.099917, loss: 3.5810
2022-08-24 00:01:22 - train: epoch 0002, iter [04300, 05004], lr: 0.099915, loss: 3.6287
2022-08-24 00:01:57 - train: epoch 0002, iter [04400, 05004], lr: 0.099913, loss: 3.4458
2022-08-24 00:02:31 - train: epoch 0002, iter [04500, 05004], lr: 0.099911, loss: 3.4592
2022-08-24 00:03:06 - train: epoch 0002, iter [04600, 05004], lr: 0.099909, loss: 3.5780
2022-08-24 00:03:40 - train: epoch 0002, iter [04700, 05004], lr: 0.099907, loss: 3.6034
2022-08-24 00:04:14 - train: epoch 0002, iter [04800, 05004], lr: 0.099905, loss: 3.5887
2022-08-24 00:04:49 - train: epoch 0002, iter [04900, 05004], lr: 0.099903, loss: 3.5120
2022-08-24 00:05:23 - train: epoch 0002, iter [05000, 05004], lr: 0.099901, loss: 3.4333
2022-08-24 00:05:24 - train: epoch 002, train_loss: 4.0240
2022-08-24 00:06:41 - eval: epoch: 002, acc1: 29.166%, acc5: 55.042%, test_loss: 3.4483, per_image_load_time: 2.159ms, per_image_inference_time: 0.597ms
2022-08-24 00:06:41 - until epoch: 002, best_acc1: 29.166%
2022-08-24 00:06:41 - epoch 003 lr: 0.099901
2022-08-24 00:07:21 - train: epoch 0003, iter [00100, 05004], lr: 0.099899, loss: 3.6458
2022-08-24 00:07:56 - train: epoch 0003, iter [00200, 05004], lr: 0.099897, loss: 3.5810
2022-08-24 00:08:30 - train: epoch 0003, iter [00300, 05004], lr: 0.099895, loss: 3.3554
2022-08-24 00:09:05 - train: epoch 0003, iter [00400, 05004], lr: 0.099893, loss: 3.4065
2022-08-24 00:09:39 - train: epoch 0003, iter [00500, 05004], lr: 0.099891, loss: 3.4339
2022-08-24 00:10:14 - train: epoch 0003, iter [00600, 05004], lr: 0.099889, loss: 3.2862
2022-08-24 00:10:49 - train: epoch 0003, iter [00700, 05004], lr: 0.099887, loss: 3.5958
2022-08-24 00:11:23 - train: epoch 0003, iter [00800, 05004], lr: 0.099885, loss: 3.5403
2022-08-24 00:11:58 - train: epoch 0003, iter [00900, 05004], lr: 0.099883, loss: 3.2495
2022-08-24 00:12:32 - train: epoch 0003, iter [01000, 05004], lr: 0.099881, loss: 3.3604
2022-08-24 00:13:07 - train: epoch 0003, iter [01100, 05004], lr: 0.099878, loss: 3.3058
2022-08-24 00:13:42 - train: epoch 0003, iter [01200, 05004], lr: 0.099876, loss: 3.3077
2022-08-24 00:14:17 - train: epoch 0003, iter [01300, 05004], lr: 0.099874, loss: 3.2884
2022-08-24 00:14:52 - train: epoch 0003, iter [01400, 05004], lr: 0.099872, loss: 3.2349
2022-08-24 00:15:26 - train: epoch 0003, iter [01500, 05004], lr: 0.099870, loss: 3.5558
2022-08-24 00:16:00 - train: epoch 0003, iter [01600, 05004], lr: 0.099867, loss: 3.1699
2022-08-24 00:16:36 - train: epoch 0003, iter [01700, 05004], lr: 0.099865, loss: 3.2098
2022-08-24 00:17:10 - train: epoch 0003, iter [01800, 05004], lr: 0.099863, loss: 3.1790
2022-08-24 00:17:45 - train: epoch 0003, iter [01900, 05004], lr: 0.099860, loss: 3.2733
2022-08-24 00:18:19 - train: epoch 0003, iter [02000, 05004], lr: 0.099858, loss: 3.6329
2022-08-24 00:18:54 - train: epoch 0003, iter [02100, 05004], lr: 0.099856, loss: 3.4388
2022-08-24 00:19:29 - train: epoch 0003, iter [02200, 05004], lr: 0.099853, loss: 3.6575
2022-08-24 00:20:03 - train: epoch 0003, iter [02300, 05004], lr: 0.099851, loss: 3.2478
2022-08-24 00:20:38 - train: epoch 0003, iter [02400, 05004], lr: 0.099848, loss: 3.2132
2022-08-24 00:21:13 - train: epoch 0003, iter [02500, 05004], lr: 0.099846, loss: 3.2502
2022-08-24 00:21:48 - train: epoch 0003, iter [02600, 05004], lr: 0.099843, loss: 3.2595
2022-08-24 00:22:22 - train: epoch 0003, iter [02700, 05004], lr: 0.099841, loss: 3.4575
2022-08-24 00:22:57 - train: epoch 0003, iter [02800, 05004], lr: 0.099838, loss: 3.0401
2022-08-24 00:23:32 - train: epoch 0003, iter [02900, 05004], lr: 0.099836, loss: 3.1438
2022-08-24 00:24:06 - train: epoch 0003, iter [03000, 05004], lr: 0.099833, loss: 3.2633
2022-08-24 00:24:41 - train: epoch 0003, iter [03100, 05004], lr: 0.099831, loss: 3.3635
2022-08-24 00:25:16 - train: epoch 0003, iter [03200, 05004], lr: 0.099828, loss: 3.2489
2022-08-24 00:25:50 - train: epoch 0003, iter [03300, 05004], lr: 0.099826, loss: 3.1587
2022-08-24 00:26:25 - train: epoch 0003, iter [03400, 05004], lr: 0.099823, loss: 3.2881
2022-08-24 00:27:00 - train: epoch 0003, iter [03500, 05004], lr: 0.099820, loss: 2.8947
2022-08-24 00:27:34 - train: epoch 0003, iter [03600, 05004], lr: 0.099818, loss: 3.0666
2022-08-24 00:28:09 - train: epoch 0003, iter [03700, 05004], lr: 0.099815, loss: 3.0423
2022-08-24 00:28:44 - train: epoch 0003, iter [03800, 05004], lr: 0.099812, loss: 3.1509
2022-08-24 00:29:19 - train: epoch 0003, iter [03900, 05004], lr: 0.099810, loss: 3.1796
2022-08-24 00:29:53 - train: epoch 0003, iter [04000, 05004], lr: 0.099807, loss: 3.1137
2022-08-24 00:30:28 - train: epoch 0003, iter [04100, 05004], lr: 0.099804, loss: 3.1360
2022-08-24 00:31:03 - train: epoch 0003, iter [04200, 05004], lr: 0.099801, loss: 2.9997
2022-08-24 00:31:37 - train: epoch 0003, iter [04300, 05004], lr: 0.099798, loss: 2.7913
2022-08-24 00:32:13 - train: epoch 0003, iter [04400, 05004], lr: 0.099796, loss: 2.9873
2022-08-24 00:32:47 - train: epoch 0003, iter [04500, 05004], lr: 0.099793, loss: 3.0623
2022-08-24 00:33:21 - train: epoch 0003, iter [04600, 05004], lr: 0.099790, loss: 3.0133
2022-08-24 00:33:56 - train: epoch 0003, iter [04700, 05004], lr: 0.099787, loss: 2.9683
2022-08-24 00:34:30 - train: epoch 0003, iter [04800, 05004], lr: 0.099784, loss: 3.0730
2022-08-24 00:35:05 - train: epoch 0003, iter [04900, 05004], lr: 0.099781, loss: 3.0319
2022-08-24 00:35:38 - train: epoch 0003, iter [05000, 05004], lr: 0.099778, loss: 3.0685
2022-08-24 00:35:39 - train: epoch 003, train_loss: 3.2224
2022-08-24 00:36:56 - eval: epoch: 003, acc1: 36.856%, acc5: 63.570%, test_loss: 2.9076, per_image_load_time: 2.287ms, per_image_inference_time: 0.597ms
2022-08-24 00:36:56 - until epoch: 003, best_acc1: 36.856%
2022-08-24 00:36:56 - epoch 004 lr: 0.099778
2022-08-24 00:37:37 - train: epoch 0004, iter [00100, 05004], lr: 0.099775, loss: 3.0135
2022-08-24 00:38:12 - train: epoch 0004, iter [00200, 05004], lr: 0.099772, loss: 2.8650
2022-08-24 00:38:46 - train: epoch 0004, iter [00300, 05004], lr: 0.099769, loss: 2.9129
2022-08-24 00:39:21 - train: epoch 0004, iter [00400, 05004], lr: 0.099766, loss: 2.8840
2022-08-24 00:39:55 - train: epoch 0004, iter [00500, 05004], lr: 0.099763, loss: 2.8588
2022-08-24 00:40:30 - train: epoch 0004, iter [00600, 05004], lr: 0.099760, loss: 3.1557
2022-08-24 00:41:04 - train: epoch 0004, iter [00700, 05004], lr: 0.099757, loss: 2.9451
2022-08-24 00:41:39 - train: epoch 0004, iter [00800, 05004], lr: 0.099754, loss: 2.7794
2022-08-24 00:42:14 - train: epoch 0004, iter [00900, 05004], lr: 0.099751, loss: 2.5757
2022-08-24 00:42:48 - train: epoch 0004, iter [01000, 05004], lr: 0.099748, loss: 2.9980
2022-08-24 00:43:23 - train: epoch 0004, iter [01100, 05004], lr: 0.099744, loss: 3.1371
2022-08-24 00:43:58 - train: epoch 0004, iter [01200, 05004], lr: 0.099741, loss: 2.7132
2022-08-24 00:44:33 - train: epoch 0004, iter [01300, 05004], lr: 0.099738, loss: 2.7479
2022-08-24 00:45:07 - train: epoch 0004, iter [01400, 05004], lr: 0.099735, loss: 2.9385
2022-08-24 00:45:42 - train: epoch 0004, iter [01500, 05004], lr: 0.099732, loss: 3.0011
2022-08-24 00:46:16 - train: epoch 0004, iter [01600, 05004], lr: 0.099728, loss: 2.7905
2022-08-24 00:46:51 - train: epoch 0004, iter [01700, 05004], lr: 0.099725, loss: 2.9377
2022-08-24 00:47:26 - train: epoch 0004, iter [01800, 05004], lr: 0.099722, loss: 3.1339
2022-08-24 00:48:01 - train: epoch 0004, iter [01900, 05004], lr: 0.099718, loss: 2.9817
2022-08-24 00:48:36 - train: epoch 0004, iter [02000, 05004], lr: 0.099715, loss: 2.8598
2022-08-24 00:49:11 - train: epoch 0004, iter [02100, 05004], lr: 0.099712, loss: 2.8478
2022-08-24 00:49:46 - train: epoch 0004, iter [02200, 05004], lr: 0.099708, loss: 2.8809
2022-08-24 00:50:20 - train: epoch 0004, iter [02300, 05004], lr: 0.099705, loss: 2.6778
2022-08-24 00:50:55 - train: epoch 0004, iter [02400, 05004], lr: 0.099702, loss: 2.7498
2022-08-24 00:51:30 - train: epoch 0004, iter [02500, 05004], lr: 0.099698, loss: 2.8451
2022-08-24 00:52:05 - train: epoch 0004, iter [02600, 05004], lr: 0.099695, loss: 2.8670
2022-08-24 00:52:39 - train: epoch 0004, iter [02700, 05004], lr: 0.099691, loss: 2.6488
2022-08-24 00:53:14 - train: epoch 0004, iter [02800, 05004], lr: 0.099688, loss: 2.7561
2022-08-24 00:53:48 - train: epoch 0004, iter [02900, 05004], lr: 0.099684, loss: 2.7570
2022-08-24 00:54:24 - train: epoch 0004, iter [03000, 05004], lr: 0.099681, loss: 2.8534
2022-08-24 00:54:59 - train: epoch 0004, iter [03100, 05004], lr: 0.099677, loss: 2.9010
2022-08-24 00:55:33 - train: epoch 0004, iter [03200, 05004], lr: 0.099674, loss: 2.6577
2022-08-24 00:56:08 - train: epoch 0004, iter [03300, 05004], lr: 0.099670, loss: 2.9316
2022-08-24 00:56:42 - train: epoch 0004, iter [03400, 05004], lr: 0.099666, loss: 2.9009
2022-08-24 00:57:17 - train: epoch 0004, iter [03500, 05004], lr: 0.099663, loss: 2.7608
2022-08-24 00:57:51 - train: epoch 0004, iter [03600, 05004], lr: 0.099659, loss: 2.6886
2022-08-24 00:58:26 - train: epoch 0004, iter [03700, 05004], lr: 0.099655, loss: 2.7543
2022-08-24 00:59:01 - train: epoch 0004, iter [03800, 05004], lr: 0.099652, loss: 2.7197
2022-08-24 00:59:36 - train: epoch 0004, iter [03900, 05004], lr: 0.099648, loss: 2.7473
2022-08-24 01:00:11 - train: epoch 0004, iter [04000, 05004], lr: 0.099644, loss: 2.5180
2022-08-24 01:00:46 - train: epoch 0004, iter [04100, 05004], lr: 0.099641, loss: 2.7465
2022-08-24 01:01:20 - train: epoch 0004, iter [04200, 05004], lr: 0.099637, loss: 2.6581
2022-08-24 01:01:55 - train: epoch 0004, iter [04300, 05004], lr: 0.099633, loss: 2.5592
2022-08-24 01:02:30 - train: epoch 0004, iter [04400, 05004], lr: 0.099629, loss: 2.6057
2022-08-24 01:03:04 - train: epoch 0004, iter [04500, 05004], lr: 0.099625, loss: 2.2097
2022-08-24 01:03:39 - train: epoch 0004, iter [04600, 05004], lr: 0.099621, loss: 2.7879
2022-08-24 01:04:14 - train: epoch 0004, iter [04700, 05004], lr: 0.099618, loss: 2.6125
2022-08-24 01:04:49 - train: epoch 0004, iter [04800, 05004], lr: 0.099614, loss: 2.6239
2022-08-24 01:05:23 - train: epoch 0004, iter [04900, 05004], lr: 0.099610, loss: 2.8768
2022-08-24 01:05:57 - train: epoch 0004, iter [05000, 05004], lr: 0.099606, loss: 2.7843
2022-08-24 01:05:58 - train: epoch 004, train_loss: 2.8500
2022-08-24 01:07:14 - eval: epoch: 004, acc1: 43.564%, acc5: 70.248%, test_loss: 2.5389, per_image_load_time: 2.358ms, per_image_inference_time: 0.583ms
2022-08-24 01:07:15 - until epoch: 004, best_acc1: 43.564%
2022-08-24 01:07:15 - epoch 005 lr: 0.099606
2022-08-24 01:07:55 - train: epoch 0005, iter [00100, 05004], lr: 0.099602, loss: 2.8012
2022-08-24 01:08:29 - train: epoch 0005, iter [00200, 05004], lr: 0.099598, loss: 2.7712
2022-08-24 01:09:04 - train: epoch 0005, iter [00300, 05004], lr: 0.099594, loss: 2.8843
2022-08-24 01:09:38 - train: epoch 0005, iter [00400, 05004], lr: 0.099590, loss: 2.7052
2022-08-24 01:10:13 - train: epoch 0005, iter [00500, 05004], lr: 0.099586, loss: 2.4815
2022-08-24 01:10:48 - train: epoch 0005, iter [00600, 05004], lr: 0.099582, loss: 2.6922
2022-08-24 01:11:22 - train: epoch 0005, iter [00700, 05004], lr: 0.099578, loss: 2.8173
2022-08-24 01:11:57 - train: epoch 0005, iter [00800, 05004], lr: 0.099574, loss: 2.8710
2022-08-24 01:12:32 - train: epoch 0005, iter [00900, 05004], lr: 0.099570, loss: 2.7239
2022-08-24 01:13:06 - train: epoch 0005, iter [01000, 05004], lr: 0.099565, loss: 2.7173
2022-08-24 01:13:41 - train: epoch 0005, iter [01100, 05004], lr: 0.099561, loss: 2.7440
2022-08-24 01:14:16 - train: epoch 0005, iter [01200, 05004], lr: 0.099557, loss: 2.6498
2022-08-24 01:14:50 - train: epoch 0005, iter [01300, 05004], lr: 0.099553, loss: 2.6545
2022-08-24 01:15:25 - train: epoch 0005, iter [01400, 05004], lr: 0.099549, loss: 2.7472
2022-08-24 01:15:59 - train: epoch 0005, iter [01500, 05004], lr: 0.099545, loss: 2.5210
2022-08-24 01:16:34 - train: epoch 0005, iter [01600, 05004], lr: 0.099540, loss: 2.5396
2022-08-24 01:17:09 - train: epoch 0005, iter [01700, 05004], lr: 0.099536, loss: 2.6007
2022-08-24 01:17:43 - train: epoch 0005, iter [01800, 05004], lr: 0.099532, loss: 2.7129
2022-08-24 01:18:18 - train: epoch 0005, iter [01900, 05004], lr: 0.099527, loss: 2.5336
2022-08-24 01:18:53 - train: epoch 0005, iter [02000, 05004], lr: 0.099523, loss: 2.7252
2022-08-24 01:19:28 - train: epoch 0005, iter [02100, 05004], lr: 0.099519, loss: 2.5129
2022-08-24 01:20:02 - train: epoch 0005, iter [02200, 05004], lr: 0.099514, loss: 2.4807
2022-08-24 01:20:37 - train: epoch 0005, iter [02300, 05004], lr: 0.099510, loss: 2.3999
2022-08-24 01:21:12 - train: epoch 0005, iter [02400, 05004], lr: 0.099506, loss: 2.5997
2022-08-24 01:21:47 - train: epoch 0005, iter [02500, 05004], lr: 0.099501, loss: 2.6766
2022-08-24 01:22:23 - train: epoch 0005, iter [02600, 05004], lr: 0.099497, loss: 2.8703
2022-08-24 01:22:57 - train: epoch 0005, iter [02700, 05004], lr: 0.099492, loss: 2.8365
2022-08-24 01:23:32 - train: epoch 0005, iter [02800, 05004], lr: 0.099488, loss: 2.6169
2022-08-24 01:24:07 - train: epoch 0005, iter [02900, 05004], lr: 0.099483, loss: 2.5098
2022-08-24 01:24:42 - train: epoch 0005, iter [03000, 05004], lr: 0.099479, loss: 2.5942
2022-08-24 01:25:17 - train: epoch 0005, iter [03100, 05004], lr: 0.099474, loss: 2.6550
2022-08-24 01:25:51 - train: epoch 0005, iter [03200, 05004], lr: 0.099470, loss: 2.7289
2022-08-24 01:26:26 - train: epoch 0005, iter [03300, 05004], lr: 0.099465, loss: 2.4173
2022-08-24 01:27:01 - train: epoch 0005, iter [03400, 05004], lr: 0.099461, loss: 2.5230
2022-08-24 01:27:36 - train: epoch 0005, iter [03500, 05004], lr: 0.099456, loss: 2.4818
2022-08-24 01:28:11 - train: epoch 0005, iter [03600, 05004], lr: 0.099451, loss: 2.6519
2022-08-24 01:28:45 - train: epoch 0005, iter [03700, 05004], lr: 0.099447, loss: 2.5052
2022-08-24 01:29:20 - train: epoch 0005, iter [03800, 05004], lr: 0.099442, loss: 2.4681
2022-08-24 01:29:55 - train: epoch 0005, iter [03900, 05004], lr: 0.099437, loss: 2.9037
2022-08-24 01:30:30 - train: epoch 0005, iter [04000, 05004], lr: 0.099433, loss: 2.5679
2022-08-24 01:31:05 - train: epoch 0005, iter [04100, 05004], lr: 0.099428, loss: 2.6307
2022-08-24 01:31:40 - train: epoch 0005, iter [04200, 05004], lr: 0.099423, loss: 2.6248
2022-08-24 01:32:15 - train: epoch 0005, iter [04300, 05004], lr: 0.099419, loss: 2.4779
2022-08-24 01:32:49 - train: epoch 0005, iter [04400, 05004], lr: 0.099414, loss: 2.5970
2022-08-24 01:33:25 - train: epoch 0005, iter [04500, 05004], lr: 0.099409, loss: 2.6170
2022-08-24 01:33:59 - train: epoch 0005, iter [04600, 05004], lr: 0.099404, loss: 2.5097
2022-08-24 01:34:34 - train: epoch 0005, iter [04700, 05004], lr: 0.099399, loss: 2.3876
2022-08-24 01:35:09 - train: epoch 0005, iter [04800, 05004], lr: 0.099394, loss: 2.4653
2022-08-24 01:35:43 - train: epoch 0005, iter [04900, 05004], lr: 0.099390, loss: 2.6852
2022-08-24 01:36:17 - train: epoch 0005, iter [05000, 05004], lr: 0.099385, loss: 2.5380
2022-08-24 01:36:18 - train: epoch 005, train_loss: 2.6375
2022-08-24 01:37:35 - eval: epoch: 005, acc1: 47.096%, acc5: 73.208%, test_loss: 2.3264, per_image_load_time: 1.910ms, per_image_inference_time: 0.586ms
2022-08-24 01:37:35 - until epoch: 005, best_acc1: 47.096%
2022-08-24 01:37:35 - epoch 006 lr: 0.099384
2022-08-24 01:38:16 - train: epoch 0006, iter [00100, 05004], lr: 0.099379, loss: 2.5106
2022-08-24 01:38:51 - train: epoch 0006, iter [00200, 05004], lr: 0.099375, loss: 2.6792
2022-08-24 01:39:25 - train: epoch 0006, iter [00300, 05004], lr: 0.099370, loss: 2.3644
2022-08-24 01:40:00 - train: epoch 0006, iter [00400, 05004], lr: 0.099365, loss: 2.6424
2022-08-24 01:40:35 - train: epoch 0006, iter [00500, 05004], lr: 0.099360, loss: 2.4658
2022-08-24 01:41:10 - train: epoch 0006, iter [00600, 05004], lr: 0.099355, loss: 2.6374
2022-08-24 01:41:44 - train: epoch 0006, iter [00700, 05004], lr: 0.099350, loss: 2.5773
2022-08-24 01:42:18 - train: epoch 0006, iter [00800, 05004], lr: 0.099345, loss: 2.5980
2022-08-24 01:42:52 - train: epoch 0006, iter [00900, 05004], lr: 0.099339, loss: 2.4493
2022-08-24 01:43:27 - train: epoch 0006, iter [01000, 05004], lr: 0.099334, loss: 2.3838
2022-08-24 01:44:01 - train: epoch 0006, iter [01100, 05004], lr: 0.099329, loss: 2.4828
2022-08-24 01:44:36 - train: epoch 0006, iter [01200, 05004], lr: 0.099324, loss: 2.5531
2022-08-24 01:45:10 - train: epoch 0006, iter [01300, 05004], lr: 0.099319, loss: 2.6569
2022-08-24 01:45:45 - train: epoch 0006, iter [01400, 05004], lr: 0.099314, loss: 2.6162
2022-08-24 01:46:20 - train: epoch 0006, iter [01500, 05004], lr: 0.099309, loss: 2.7395
2022-08-24 01:46:54 - train: epoch 0006, iter [01600, 05004], lr: 0.099303, loss: 2.3986
2022-08-24 01:47:29 - train: epoch 0006, iter [01700, 05004], lr: 0.099298, loss: 2.6627
2022-08-24 01:48:03 - train: epoch 0006, iter [01800, 05004], lr: 0.099293, loss: 2.6160
2022-08-24 01:48:37 - train: epoch 0006, iter [01900, 05004], lr: 0.099288, loss: 2.4559
2022-08-24 01:49:12 - train: epoch 0006, iter [02000, 05004], lr: 0.099282, loss: 2.6273
2022-08-24 01:49:46 - train: epoch 0006, iter [02100, 05004], lr: 0.099277, loss: 2.5049
2022-08-24 01:50:21 - train: epoch 0006, iter [02200, 05004], lr: 0.099272, loss: 2.4496
2022-08-24 01:50:55 - train: epoch 0006, iter [02300, 05004], lr: 0.099266, loss: 2.3307
2022-08-24 01:51:29 - train: epoch 0006, iter [02400, 05004], lr: 0.099261, loss: 2.4817
2022-08-24 01:52:04 - train: epoch 0006, iter [02500, 05004], lr: 0.099256, loss: 2.7680
2022-08-24 01:52:39 - train: epoch 0006, iter [02600, 05004], lr: 0.099250, loss: 2.4212
2022-08-24 01:53:13 - train: epoch 0006, iter [02700, 05004], lr: 0.099245, loss: 2.6768
2022-08-24 01:53:48 - train: epoch 0006, iter [02800, 05004], lr: 0.099239, loss: 2.3320
2022-08-24 01:54:23 - train: epoch 0006, iter [02900, 05004], lr: 0.099234, loss: 2.6108
2022-08-24 01:54:58 - train: epoch 0006, iter [03000, 05004], lr: 0.099228, loss: 2.3944
2022-08-24 01:55:32 - train: epoch 0006, iter [03100, 05004], lr: 0.099223, loss: 2.2565
2022-08-24 01:56:06 - train: epoch 0006, iter [03200, 05004], lr: 0.099217, loss: 2.3644
2022-08-24 01:56:40 - train: epoch 0006, iter [03300, 05004], lr: 0.099212, loss: 2.2603
2022-08-24 01:57:14 - train: epoch 0006, iter [03400, 05004], lr: 0.099206, loss: 2.7032
2022-08-24 01:57:49 - train: epoch 0006, iter [03500, 05004], lr: 0.099201, loss: 2.5713
2022-08-24 01:58:24 - train: epoch 0006, iter [03600, 05004], lr: 0.099195, loss: 2.4766
2022-08-24 01:58:59 - train: epoch 0006, iter [03700, 05004], lr: 0.099189, loss: 2.4823
2022-08-24 01:59:33 - train: epoch 0006, iter [03800, 05004], lr: 0.099184, loss: 2.2673
2022-08-24 02:00:07 - train: epoch 0006, iter [03900, 05004], lr: 0.099178, loss: 2.3781
2022-08-24 02:00:42 - train: epoch 0006, iter [04000, 05004], lr: 0.099172, loss: 2.6383
2022-08-24 02:01:17 - train: epoch 0006, iter [04100, 05004], lr: 0.099167, loss: 2.4775
2022-08-24 02:01:51 - train: epoch 0006, iter [04200, 05004], lr: 0.099161, loss: 2.3245
2022-08-24 02:02:26 - train: epoch 0006, iter [04300, 05004], lr: 0.099155, loss: 2.5432
2022-08-24 02:03:01 - train: epoch 0006, iter [04400, 05004], lr: 0.099150, loss: 2.4451
2022-08-24 02:03:35 - train: epoch 0006, iter [04500, 05004], lr: 0.099144, loss: 2.5133
2022-08-24 02:04:09 - train: epoch 0006, iter [04600, 05004], lr: 0.099138, loss: 2.4253
2022-08-24 02:04:44 - train: epoch 0006, iter [04700, 05004], lr: 0.099132, loss: 2.4291
2022-08-24 02:05:19 - train: epoch 0006, iter [04800, 05004], lr: 0.099126, loss: 2.3919
2022-08-24 02:05:53 - train: epoch 0006, iter [04900, 05004], lr: 0.099120, loss: 2.4178
2022-08-24 02:06:26 - train: epoch 0006, iter [05000, 05004], lr: 0.099115, loss: 2.3797
2022-08-24 02:06:28 - train: epoch 006, train_loss: 2.5038
2022-08-24 02:07:44 - eval: epoch: 006, acc1: 49.284%, acc5: 74.958%, test_loss: 2.2279, per_image_load_time: 2.366ms, per_image_inference_time: 0.608ms
2022-08-24 02:07:45 - until epoch: 006, best_acc1: 49.284%
2022-08-24 02:07:45 - epoch 007 lr: 0.099114
2022-08-24 02:08:26 - train: epoch 0007, iter [00100, 05004], lr: 0.099108, loss: 2.4416
2022-08-24 02:09:01 - train: epoch 0007, iter [00200, 05004], lr: 0.099103, loss: 2.6249
2022-08-24 02:09:36 - train: epoch 0007, iter [00300, 05004], lr: 0.099097, loss: 2.6051
2022-08-24 02:10:11 - train: epoch 0007, iter [00400, 05004], lr: 0.099091, loss: 2.5232
2022-08-24 02:10:46 - train: epoch 0007, iter [00500, 05004], lr: 0.099085, loss: 2.3103
2022-08-24 02:11:20 - train: epoch 0007, iter [00600, 05004], lr: 0.099079, loss: 2.5276
2022-08-24 02:11:55 - train: epoch 0007, iter [00700, 05004], lr: 0.099073, loss: 2.4344
2022-08-24 02:12:30 - train: epoch 0007, iter [00800, 05004], lr: 0.099067, loss: 2.5371
2022-08-24 02:13:04 - train: epoch 0007, iter [00900, 05004], lr: 0.099061, loss: 2.4443
2022-08-24 02:13:39 - train: epoch 0007, iter [01000, 05004], lr: 0.099055, loss: 2.4063
2022-08-24 02:14:14 - train: epoch 0007, iter [01100, 05004], lr: 0.099048, loss: 2.2763
2022-08-24 02:14:49 - train: epoch 0007, iter [01200, 05004], lr: 0.099042, loss: 2.4041
2022-08-24 02:15:24 - train: epoch 0007, iter [01300, 05004], lr: 0.099036, loss: 2.2808
2022-08-24 02:15:59 - train: epoch 0007, iter [01400, 05004], lr: 0.099030, loss: 2.4505
2022-08-24 02:16:34 - train: epoch 0007, iter [01500, 05004], lr: 0.099024, loss: 2.5717
2022-08-24 02:17:08 - train: epoch 0007, iter [01600, 05004], lr: 0.099018, loss: 2.4701
2022-08-24 02:17:43 - train: epoch 0007, iter [01700, 05004], lr: 0.099012, loss: 2.5020
2022-08-24 02:18:18 - train: epoch 0007, iter [01800, 05004], lr: 0.099005, loss: 2.3505
2022-08-24 02:18:53 - train: epoch 0007, iter [01900, 05004], lr: 0.098999, loss: 2.4417
2022-08-24 02:19:28 - train: epoch 0007, iter [02000, 05004], lr: 0.098993, loss: 2.2318
2022-08-24 02:20:02 - train: epoch 0007, iter [02100, 05004], lr: 0.098987, loss: 2.5514
2022-08-24 02:20:37 - train: epoch 0007, iter [02200, 05004], lr: 0.098980, loss: 2.2670
2022-08-24 02:21:13 - train: epoch 0007, iter [02300, 05004], lr: 0.098974, loss: 2.4415
2022-08-24 02:21:47 - train: epoch 0007, iter [02400, 05004], lr: 0.098968, loss: 2.4858
2022-08-24 02:22:22 - train: epoch 0007, iter [02500, 05004], lr: 0.098961, loss: 2.3405
2022-08-24 02:22:56 - train: epoch 0007, iter [02600, 05004], lr: 0.098955, loss: 2.3456
2022-08-24 02:23:31 - train: epoch 0007, iter [02700, 05004], lr: 0.098948, loss: 2.3710
2022-08-24 02:24:06 - train: epoch 0007, iter [02800, 05004], lr: 0.098942, loss: 2.3427
2022-08-24 02:24:40 - train: epoch 0007, iter [02900, 05004], lr: 0.098936, loss: 2.3216
2022-08-24 02:25:15 - train: epoch 0007, iter [03000, 05004], lr: 0.098929, loss: 2.4403
2022-08-24 02:25:50 - train: epoch 0007, iter [03100, 05004], lr: 0.098923, loss: 2.3073
2022-08-24 02:26:24 - train: epoch 0007, iter [03200, 05004], lr: 0.098916, loss: 2.3291
2022-08-24 02:26:59 - train: epoch 0007, iter [03300, 05004], lr: 0.098910, loss: 2.6463
2022-08-24 02:27:34 - train: epoch 0007, iter [03400, 05004], lr: 0.098903, loss: 2.3160
2022-08-24 02:28:09 - train: epoch 0007, iter [03500, 05004], lr: 0.098897, loss: 2.5167
2022-08-24 02:28:43 - train: epoch 0007, iter [03600, 05004], lr: 0.098890, loss: 2.1534
2022-08-24 02:29:18 - train: epoch 0007, iter [03700, 05004], lr: 0.098883, loss: 2.3822
2022-08-24 02:29:52 - train: epoch 0007, iter [03800, 05004], lr: 0.098877, loss: 2.6760
2022-08-24 02:30:27 - train: epoch 0007, iter [03900, 05004], lr: 0.098870, loss: 2.2470
2022-08-24 02:31:02 - train: epoch 0007, iter [04000, 05004], lr: 0.098864, loss: 2.4970
2022-08-24 02:31:36 - train: epoch 0007, iter [04100, 05004], lr: 0.098857, loss: 2.3944
2022-08-24 02:32:11 - train: epoch 0007, iter [04200, 05004], lr: 0.098850, loss: 2.2422
2022-08-24 02:32:46 - train: epoch 0007, iter [04300, 05004], lr: 0.098844, loss: 2.5237
2022-08-24 02:33:21 - train: epoch 0007, iter [04400, 05004], lr: 0.098837, loss: 2.2611
2022-08-24 02:33:55 - train: epoch 0007, iter [04500, 05004], lr: 0.098830, loss: 2.5528
2022-08-24 02:34:30 - train: epoch 0007, iter [04600, 05004], lr: 0.098823, loss: 2.5168
2022-08-24 02:35:05 - train: epoch 0007, iter [04700, 05004], lr: 0.098817, loss: 2.5902
2022-08-24 02:35:41 - train: epoch 0007, iter [04800, 05004], lr: 0.098810, loss: 2.6876
2022-08-24 02:36:15 - train: epoch 0007, iter [04900, 05004], lr: 0.098803, loss: 2.3479
2022-08-24 02:36:49 - train: epoch 0007, iter [05000, 05004], lr: 0.098796, loss: 2.3556
2022-08-24 02:36:50 - train: epoch 007, train_loss: 2.4082
2022-08-24 02:38:06 - eval: epoch: 007, acc1: 51.096%, acc5: 76.738%, test_loss: 2.1115, per_image_load_time: 2.339ms, per_image_inference_time: 0.605ms
2022-08-24 02:38:07 - until epoch: 007, best_acc1: 51.096%
2022-08-24 02:38:07 - epoch 008 lr: 0.098796
2022-08-24 02:38:47 - train: epoch 0008, iter [00100, 05004], lr: 0.098789, loss: 2.3062
2022-08-24 02:39:22 - train: epoch 0008, iter [00200, 05004], lr: 0.098782, loss: 2.4792
2022-08-24 02:39:56 - train: epoch 0008, iter [00300, 05004], lr: 0.098775, loss: 2.2045
2022-08-24 02:40:30 - train: epoch 0008, iter [00400, 05004], lr: 0.098768, loss: 2.2027
2022-08-24 02:41:05 - train: epoch 0008, iter [00500, 05004], lr: 0.098761, loss: 2.2854
2022-08-24 02:41:39 - train: epoch 0008, iter [00600, 05004], lr: 0.098754, loss: 2.3486
2022-08-24 02:42:13 - train: epoch 0008, iter [00700, 05004], lr: 0.098747, loss: 2.6999
2022-08-24 02:42:48 - train: epoch 0008, iter [00800, 05004], lr: 0.098740, loss: 2.1316
2022-08-24 02:43:23 - train: epoch 0008, iter [00900, 05004], lr: 0.098733, loss: 2.3582
2022-08-24 02:43:58 - train: epoch 0008, iter [01000, 05004], lr: 0.098726, loss: 2.2833
2022-08-24 02:44:32 - train: epoch 0008, iter [01100, 05004], lr: 0.098719, loss: 2.1692
2022-08-24 02:45:07 - train: epoch 0008, iter [01200, 05004], lr: 0.098712, loss: 2.2593
2022-08-24 02:45:42 - train: epoch 0008, iter [01300, 05004], lr: 0.098705, loss: 2.3551
2022-08-24 02:46:17 - train: epoch 0008, iter [01400, 05004], lr: 0.098698, loss: 2.2571
2022-08-24 02:46:52 - train: epoch 0008, iter [01500, 05004], lr: 0.098691, loss: 2.3444
2022-08-24 02:47:27 - train: epoch 0008, iter [01600, 05004], lr: 0.098684, loss: 2.4680
2022-08-24 02:48:03 - train: epoch 0008, iter [01700, 05004], lr: 0.098677, loss: 2.2027
2022-08-24 02:48:38 - train: epoch 0008, iter [01800, 05004], lr: 0.098669, loss: 2.5390
2022-08-24 02:49:12 - train: epoch 0008, iter [01900, 05004], lr: 0.098662, loss: 2.1598
2022-08-24 02:49:47 - train: epoch 0008, iter [02000, 05004], lr: 0.098655, loss: 2.3614
2022-08-24 02:50:22 - train: epoch 0008, iter [02100, 05004], lr: 0.098648, loss: 2.2751
2022-08-24 02:50:56 - train: epoch 0008, iter [02200, 05004], lr: 0.098641, loss: 2.2102
2022-08-24 02:51:32 - train: epoch 0008, iter [02300, 05004], lr: 0.098633, loss: 2.4942
2022-08-24 02:52:07 - train: epoch 0008, iter [02400, 05004], lr: 0.098626, loss: 2.2617
2022-08-24 02:52:42 - train: epoch 0008, iter [02500, 05004], lr: 0.098619, loss: 2.3620
2022-08-24 02:53:17 - train: epoch 0008, iter [02600, 05004], lr: 0.098611, loss: 2.4800
2022-08-24 02:53:51 - train: epoch 0008, iter [02700, 05004], lr: 0.098604, loss: 2.5003
2022-08-24 02:54:27 - train: epoch 0008, iter [02800, 05004], lr: 0.098597, loss: 2.4510
2022-08-24 02:55:01 - train: epoch 0008, iter [02900, 05004], lr: 0.098589, loss: 2.2604
2022-08-24 02:55:36 - train: epoch 0008, iter [03000, 05004], lr: 0.098582, loss: 2.4161
2022-08-24 02:56:11 - train: epoch 0008, iter [03100, 05004], lr: 0.098574, loss: 2.3630
2022-08-24 02:56:46 - train: epoch 0008, iter [03200, 05004], lr: 0.098567, loss: 2.5156
2022-08-24 02:57:21 - train: epoch 0008, iter [03300, 05004], lr: 0.098559, loss: 2.5258
2022-08-24 02:57:55 - train: epoch 0008, iter [03400, 05004], lr: 0.098552, loss: 2.5352
2022-08-24 02:58:29 - train: epoch 0008, iter [03500, 05004], lr: 0.098544, loss: 2.3181
2022-08-24 02:59:04 - train: epoch 0008, iter [03600, 05004], lr: 0.098537, loss: 2.4934
2022-08-24 02:59:38 - train: epoch 0008, iter [03700, 05004], lr: 0.098529, loss: 2.2593
2022-08-24 03:00:12 - train: epoch 0008, iter [03800, 05004], lr: 0.098522, loss: 2.2336
2022-08-24 03:00:48 - train: epoch 0008, iter [03900, 05004], lr: 0.098514, loss: 2.4112
2022-08-24 03:01:22 - train: epoch 0008, iter [04000, 05004], lr: 0.098507, loss: 2.6776
2022-08-24 03:01:58 - train: epoch 0008, iter [04100, 05004], lr: 0.098499, loss: 2.2794
2022-08-24 03:02:32 - train: epoch 0008, iter [04200, 05004], lr: 0.098491, loss: 2.2195
2022-08-24 03:03:08 - train: epoch 0008, iter [04300, 05004], lr: 0.098484, loss: 2.0388
2022-08-24 03:03:42 - train: epoch 0008, iter [04400, 05004], lr: 0.098476, loss: 2.2165
2022-08-24 03:04:17 - train: epoch 0008, iter [04500, 05004], lr: 0.098468, loss: 2.5276
2022-08-24 03:04:51 - train: epoch 0008, iter [04600, 05004], lr: 0.098461, loss: 2.4841
2022-08-24 03:05:26 - train: epoch 0008, iter [04700, 05004], lr: 0.098453, loss: 2.2419
2022-08-24 03:06:00 - train: epoch 0008, iter [04800, 05004], lr: 0.098445, loss: 2.3720
2022-08-24 03:06:35 - train: epoch 0008, iter [04900, 05004], lr: 0.098437, loss: 2.4062
2022-08-24 03:07:08 - train: epoch 0008, iter [05000, 05004], lr: 0.098429, loss: 2.3094
2022-08-24 03:07:09 - train: epoch 008, train_loss: 2.3366
2022-08-24 03:08:26 - eval: epoch: 008, acc1: 52.156%, acc5: 77.668%, test_loss: 2.0632, per_image_load_time: 1.962ms, per_image_inference_time: 0.604ms
2022-08-24 03:08:26 - until epoch: 008, best_acc1: 52.156%
2022-08-24 03:08:26 - epoch 009 lr: 0.098429
2022-08-24 03:09:07 - train: epoch 0009, iter [00100, 05004], lr: 0.098421, loss: 2.0224
2022-08-24 03:09:41 - train: epoch 0009, iter [00200, 05004], lr: 0.098414, loss: 2.2012
2022-08-24 03:10:16 - train: epoch 0009, iter [00300, 05004], lr: 0.098406, loss: 2.0493
2022-08-24 03:10:51 - train: epoch 0009, iter [00400, 05004], lr: 0.098398, loss: 2.5046
2022-08-24 03:11:26 - train: epoch 0009, iter [00500, 05004], lr: 0.098390, loss: 2.3743
2022-08-24 03:12:00 - train: epoch 0009, iter [00600, 05004], lr: 0.098382, loss: 2.2914
2022-08-24 03:12:35 - train: epoch 0009, iter [00700, 05004], lr: 0.098374, loss: 2.1933
2022-08-24 03:13:10 - train: epoch 0009, iter [00800, 05004], lr: 0.098366, loss: 2.1729
2022-08-24 03:13:45 - train: epoch 0009, iter [00900, 05004], lr: 0.098358, loss: 2.2057
2022-08-24 03:14:19 - train: epoch 0009, iter [01000, 05004], lr: 0.098350, loss: 2.1863
2022-08-24 03:14:55 - train: epoch 0009, iter [01100, 05004], lr: 0.098342, loss: 2.5530
2022-08-24 03:15:29 - train: epoch 0009, iter [01200, 05004], lr: 0.098334, loss: 2.4052
2022-08-24 03:16:05 - train: epoch 0009, iter [01300, 05004], lr: 0.098326, loss: 2.5010
2022-08-24 03:16:40 - train: epoch 0009, iter [01400, 05004], lr: 0.098318, loss: 2.0110
2022-08-24 03:17:16 - train: epoch 0009, iter [01500, 05004], lr: 0.098310, loss: 2.2081
2022-08-24 03:17:50 - train: epoch 0009, iter [01600, 05004], lr: 0.098302, loss: 2.2790
2022-08-24 03:18:25 - train: epoch 0009, iter [01700, 05004], lr: 0.098294, loss: 2.4149
2022-08-24 03:19:00 - train: epoch 0009, iter [01800, 05004], lr: 0.098286, loss: 2.2529
2022-08-24 03:19:35 - train: epoch 0009, iter [01900, 05004], lr: 0.098277, loss: 2.0340
2022-08-24 03:20:09 - train: epoch 0009, iter [02000, 05004], lr: 0.098269, loss: 1.9480
2022-08-24 03:20:44 - train: epoch 0009, iter [02100, 05004], lr: 0.098261, loss: 2.2213
2022-08-24 03:21:18 - train: epoch 0009, iter [02200, 05004], lr: 0.098253, loss: 2.4133
2022-08-24 03:21:53 - train: epoch 0009, iter [02300, 05004], lr: 0.098245, loss: 2.0776
2022-08-24 03:22:28 - train: epoch 0009, iter [02400, 05004], lr: 0.098236, loss: 2.2178
2022-08-24 03:23:02 - train: epoch 0009, iter [02500, 05004], lr: 0.098228, loss: 2.2544
2022-08-24 03:23:36 - train: epoch 0009, iter [02600, 05004], lr: 0.098220, loss: 2.2718
2022-08-24 03:24:10 - train: epoch 0009, iter [02700, 05004], lr: 0.098211, loss: 2.2199
2022-08-24 03:24:45 - train: epoch 0009, iter [02800, 05004], lr: 0.098203, loss: 2.3867
2022-08-24 03:25:19 - train: epoch 0009, iter [02900, 05004], lr: 0.098195, loss: 2.0220
2022-08-24 03:25:54 - train: epoch 0009, iter [03000, 05004], lr: 0.098186, loss: 2.0939
2022-08-24 03:26:28 - train: epoch 0009, iter [03100, 05004], lr: 0.098178, loss: 2.3071
2022-08-24 03:27:03 - train: epoch 0009, iter [03200, 05004], lr: 0.098170, loss: 2.3401
2022-08-24 03:27:38 - train: epoch 0009, iter [03300, 05004], lr: 0.098161, loss: 2.3238
2022-08-24 03:28:12 - train: epoch 0009, iter [03400, 05004], lr: 0.098153, loss: 2.5104
2022-08-24 03:28:46 - train: epoch 0009, iter [03500, 05004], lr: 0.098144, loss: 2.3251
2022-08-24 03:29:21 - train: epoch 0009, iter [03600, 05004], lr: 0.098136, loss: 2.2486
2022-08-24 03:29:56 - train: epoch 0009, iter [03700, 05004], lr: 0.098127, loss: 2.4530
2022-08-24 03:30:30 - train: epoch 0009, iter [03800, 05004], lr: 0.098119, loss: 2.4968
2022-08-24 03:31:05 - train: epoch 0009, iter [03900, 05004], lr: 0.098110, loss: 2.0452
2022-08-24 03:31:39 - train: epoch 0009, iter [04000, 05004], lr: 0.098102, loss: 2.4922
2022-08-24 03:32:14 - train: epoch 0009, iter [04100, 05004], lr: 0.098093, loss: 2.3256
2022-08-24 03:32:48 - train: epoch 0009, iter [04200, 05004], lr: 0.098084, loss: 2.1454
2022-08-24 03:33:24 - train: epoch 0009, iter [04300, 05004], lr: 0.098076, loss: 2.3967
2022-08-24 03:33:59 - train: epoch 0009, iter [04400, 05004], lr: 0.098067, loss: 2.2424
2022-08-24 03:34:33 - train: epoch 0009, iter [04500, 05004], lr: 0.098059, loss: 2.2337
2022-08-24 03:35:08 - train: epoch 0009, iter [04600, 05004], lr: 0.098050, loss: 2.3499
2022-08-24 03:35:43 - train: epoch 0009, iter [04700, 05004], lr: 0.098041, loss: 2.4337
2022-08-24 03:36:18 - train: epoch 0009, iter [04800, 05004], lr: 0.098033, loss: 2.5682
2022-08-24 03:36:53 - train: epoch 0009, iter [04900, 05004], lr: 0.098024, loss: 2.3643
2022-08-24 03:37:25 - train: epoch 0009, iter [05000, 05004], lr: 0.098015, loss: 2.2126
2022-08-24 03:37:27 - train: epoch 009, train_loss: 2.2842
2022-08-24 03:38:44 - eval: epoch: 009, acc1: 53.270%, acc5: 78.676%, test_loss: 1.9948, per_image_load_time: 2.224ms, per_image_inference_time: 0.604ms
2022-08-24 03:38:44 - until epoch: 009, best_acc1: 53.270%
2022-08-24 03:38:44 - epoch 010 lr: 0.098015
2022-08-24 03:39:25 - train: epoch 0010, iter [00100, 05004], lr: 0.098006, loss: 2.2498
2022-08-24 03:39:59 - train: epoch 0010, iter [00200, 05004], lr: 0.097997, loss: 2.2534
2022-08-24 03:40:33 - train: epoch 0010, iter [00300, 05004], lr: 0.097988, loss: 2.2223
2022-08-24 03:41:07 - train: epoch 0010, iter [00400, 05004], lr: 0.097980, loss: 2.2565
2022-08-24 03:41:42 - train: epoch 0010, iter [00500, 05004], lr: 0.097971, loss: 2.0479
2022-08-24 03:42:17 - train: epoch 0010, iter [00600, 05004], lr: 0.097962, loss: 2.2935
2022-08-24 03:42:51 - train: epoch 0010, iter [00700, 05004], lr: 0.097953, loss: 2.3555
2022-08-24 03:43:26 - train: epoch 0010, iter [00800, 05004], lr: 0.097944, loss: 2.1877
2022-08-24 03:44:01 - train: epoch 0010, iter [00900, 05004], lr: 0.097935, loss: 2.1271
2022-08-24 03:44:36 - train: epoch 0010, iter [01000, 05004], lr: 0.097926, loss: 2.0691
2022-08-24 03:45:10 - train: epoch 0010, iter [01100, 05004], lr: 0.097917, loss: 2.2086
2022-08-24 03:45:45 - train: epoch 0010, iter [01200, 05004], lr: 0.097908, loss: 2.0785
2022-08-24 03:46:20 - train: epoch 0010, iter [01300, 05004], lr: 0.097899, loss: 2.1124
2022-08-24 03:46:54 - train: epoch 0010, iter [01400, 05004], lr: 0.097890, loss: 2.3446
2022-08-24 03:47:30 - train: epoch 0010, iter [01500, 05004], lr: 0.097881, loss: 2.0334
2022-08-24 03:48:04 - train: epoch 0010, iter [01600, 05004], lr: 0.097872, loss: 2.3486
2022-08-24 03:48:39 - train: epoch 0010, iter [01700, 05004], lr: 0.097863, loss: 2.4267
2022-08-24 03:49:14 - train: epoch 0010, iter [01800, 05004], lr: 0.097854, loss: 2.1761
2022-08-24 03:49:48 - train: epoch 0010, iter [01900, 05004], lr: 0.097845, loss: 2.2549
2022-08-24 03:50:23 - train: epoch 0010, iter [02000, 05004], lr: 0.097836, loss: 2.2845
2022-08-24 03:50:57 - train: epoch 0010, iter [02100, 05004], lr: 0.097827, loss: 2.1829
2022-08-24 03:51:32 - train: epoch 0010, iter [02200, 05004], lr: 0.097817, loss: 2.3585
2022-08-24 03:52:07 - train: epoch 0010, iter [02300, 05004], lr: 0.097808, loss: 2.4673
2022-08-24 03:52:42 - train: epoch 0010, iter [02400, 05004], lr: 0.097799, loss: 2.3867
2022-08-24 03:53:17 - train: epoch 0010, iter [02500, 05004], lr: 0.097790, loss: 2.2227
2022-08-24 03:53:52 - train: epoch 0010, iter [02600, 05004], lr: 0.097781, loss: 2.2864
2022-08-24 03:54:27 - train: epoch 0010, iter [02700, 05004], lr: 0.097771, loss: 1.9963
2022-08-24 03:55:01 - train: epoch 0010, iter [02800, 05004], lr: 0.097762, loss: 2.2655
2022-08-24 03:55:36 - train: epoch 0010, iter [02900, 05004], lr: 0.097753, loss: 2.2465
2022-08-24 03:56:10 - train: epoch 0010, iter [03000, 05004], lr: 0.097743, loss: 2.1523
2022-08-24 03:56:44 - train: epoch 0010, iter [03100, 05004], lr: 0.097734, loss: 2.4273
2022-08-24 03:57:19 - train: epoch 0010, iter [03200, 05004], lr: 0.097725, loss: 2.2464
2022-08-24 03:57:54 - train: epoch 0010, iter [03300, 05004], lr: 0.097715, loss: 2.4352
2022-08-24 03:58:29 - train: epoch 0010, iter [03400, 05004], lr: 0.097706, loss: 2.3591
2022-08-24 03:59:04 - train: epoch 0010, iter [03500, 05004], lr: 0.097697, loss: 2.4871
2022-08-24 03:59:39 - train: epoch 0010, iter [03600, 05004], lr: 0.097687, loss: 2.4263
2022-08-24 04:00:14 - train: epoch 0010, iter [03700, 05004], lr: 0.097678, loss: 2.1053
2022-08-24 04:00:49 - train: epoch 0010, iter [03800, 05004], lr: 0.097668, loss: 2.2518
2022-08-24 04:01:23 - train: epoch 0010, iter [03900, 05004], lr: 0.097659, loss: 1.9135
2022-08-24 04:01:57 - train: epoch 0010, iter [04000, 05004], lr: 0.097649, loss: 2.2198
2022-08-24 04:02:32 - train: epoch 0010, iter [04100, 05004], lr: 0.097640, loss: 2.0395
2022-08-24 04:03:07 - train: epoch 0010, iter [04200, 05004], lr: 0.097630, loss: 2.3460
2022-08-24 04:03:42 - train: epoch 0010, iter [04300, 05004], lr: 0.097621, loss: 2.3024
2022-08-24 04:04:17 - train: epoch 0010, iter [04400, 05004], lr: 0.097611, loss: 2.2228
2022-08-24 04:04:52 - train: epoch 0010, iter [04500, 05004], lr: 0.097601, loss: 2.1300
2022-08-24 04:05:27 - train: epoch 0010, iter [04600, 05004], lr: 0.097592, loss: 2.2641
2022-08-24 04:06:01 - train: epoch 0010, iter [04700, 05004], lr: 0.097582, loss: 2.3050
2022-08-24 04:06:36 - train: epoch 0010, iter [04800, 05004], lr: 0.097573, loss: 2.1833
2022-08-24 04:07:11 - train: epoch 0010, iter [04900, 05004], lr: 0.097563, loss: 2.0485
2022-08-24 04:07:44 - train: epoch 0010, iter [05000, 05004], lr: 0.097553, loss: 1.9830
2022-08-24 04:07:45 - train: epoch 010, train_loss: 2.2405
2022-08-24 04:09:01 - eval: epoch: 010, acc1: 52.846%, acc5: 78.198%, test_loss: 2.0228, per_image_load_time: 1.233ms, per_image_inference_time: 0.601ms
2022-08-24 04:09:02 - until epoch: 010, best_acc1: 53.270%
2022-08-24 04:09:02 - epoch 011 lr: 0.097553
2022-08-24 04:09:43 - train: epoch 0011, iter [00100, 05004], lr: 0.097543, loss: 2.1024
2022-08-24 04:10:17 - train: epoch 0011, iter [00200, 05004], lr: 0.097533, loss: 2.3361
2022-08-24 04:10:51 - train: epoch 0011, iter [00300, 05004], lr: 0.097524, loss: 2.0253
2022-08-24 04:11:26 - train: epoch 0011, iter [00400, 05004], lr: 0.097514, loss: 2.2377
2022-08-24 04:12:01 - train: epoch 0011, iter [00500, 05004], lr: 0.097504, loss: 2.1953
2022-08-24 04:12:36 - train: epoch 0011, iter [00600, 05004], lr: 0.097494, loss: 2.0998
2022-08-24 04:13:10 - train: epoch 0011, iter [00700, 05004], lr: 0.097484, loss: 2.3149
2022-08-24 04:13:44 - train: epoch 0011, iter [00800, 05004], lr: 0.097475, loss: 2.2178
2022-08-24 04:14:19 - train: epoch 0011, iter [00900, 05004], lr: 0.097465, loss: 2.4019
2022-08-24 04:14:54 - train: epoch 0011, iter [01000, 05004], lr: 0.097455, loss: 2.1434
2022-08-24 04:15:29 - train: epoch 0011, iter [01100, 05004], lr: 0.097445, loss: 2.3484
2022-08-24 04:16:04 - train: epoch 0011, iter [01200, 05004], lr: 0.097435, loss: 2.5212
2022-08-24 04:16:39 - train: epoch 0011, iter [01300, 05004], lr: 0.097425, loss: 2.4711
2022-08-24 04:17:14 - train: epoch 0011, iter [01400, 05004], lr: 0.097415, loss: 2.3065
2022-08-24 04:17:49 - train: epoch 0011, iter [01500, 05004], lr: 0.097405, loss: 2.0865
2022-08-24 04:18:24 - train: epoch 0011, iter [01600, 05004], lr: 0.097395, loss: 2.2790
2022-08-24 04:18:59 - train: epoch 0011, iter [01700, 05004], lr: 0.097385, loss: 2.3184
2022-08-24 04:19:34 - train: epoch 0011, iter [01800, 05004], lr: 0.097375, loss: 2.0737
2022-08-24 04:20:10 - train: epoch 0011, iter [01900, 05004], lr: 0.097365, loss: 2.1202
2022-08-24 04:20:45 - train: epoch 0011, iter [02000, 05004], lr: 0.097355, loss: 2.3139
2022-08-24 04:21:20 - train: epoch 0011, iter [02100, 05004], lr: 0.097345, loss: 2.2845
2022-08-24 04:21:55 - train: epoch 0011, iter [02200, 05004], lr: 0.097335, loss: 2.2060
2022-08-24 04:22:30 - train: epoch 0011, iter [02300, 05004], lr: 0.097325, loss: 2.4845
2022-08-24 04:23:05 - train: epoch 0011, iter [02400, 05004], lr: 0.097315, loss: 2.0008
2022-08-24 04:23:40 - train: epoch 0011, iter [02500, 05004], lr: 0.097304, loss: 2.4897
2022-08-24 04:24:14 - train: epoch 0011, iter [02600, 05004], lr: 0.097294, loss: 2.1157
2022-08-24 04:24:50 - train: epoch 0011, iter [02700, 05004], lr: 0.097284, loss: 2.2466
2022-08-24 04:25:26 - train: epoch 0011, iter [02800, 05004], lr: 0.097274, loss: 1.8604
2022-08-24 04:26:01 - train: epoch 0011, iter [02900, 05004], lr: 0.097264, loss: 2.3074
2022-08-24 04:26:35 - train: epoch 0011, iter [03000, 05004], lr: 0.097253, loss: 2.3613
2022-08-24 04:27:11 - train: epoch 0011, iter [03100, 05004], lr: 0.097243, loss: 2.2632
2022-08-24 04:27:45 - train: epoch 0011, iter [03200, 05004], lr: 0.097233, loss: 2.0511
2022-08-24 04:28:20 - train: epoch 0011, iter [03300, 05004], lr: 0.097223, loss: 2.2303
2022-08-24 04:28:56 - train: epoch 0011, iter [03400, 05004], lr: 0.097212, loss: 2.1372
2022-08-24 04:29:31 - train: epoch 0011, iter [03500, 05004], lr: 0.097202, loss: 2.1145
2022-08-24 04:30:06 - train: epoch 0011, iter [03600, 05004], lr: 0.097191, loss: 2.1698
2022-08-24 04:30:40 - train: epoch 0011, iter [03700, 05004], lr: 0.097181, loss: 2.3852
2022-08-24 04:31:15 - train: epoch 0011, iter [03800, 05004], lr: 0.097171, loss: 1.9708
2022-08-24 04:31:49 - train: epoch 0011, iter [03900, 05004], lr: 0.097160, loss: 2.1910
2022-08-24 04:32:24 - train: epoch 0011, iter [04000, 05004], lr: 0.097150, loss: 2.0802
2022-08-24 04:32:59 - train: epoch 0011, iter [04100, 05004], lr: 0.097139, loss: 1.9634
2022-08-24 04:33:34 - train: epoch 0011, iter [04200, 05004], lr: 0.097129, loss: 2.1494
2022-08-24 04:34:09 - train: epoch 0011, iter [04300, 05004], lr: 0.097118, loss: 2.2867
2022-08-24 04:34:45 - train: epoch 0011, iter [04400, 05004], lr: 0.097108, loss: 2.0635
2022-08-24 04:35:20 - train: epoch 0011, iter [04500, 05004], lr: 0.097097, loss: 2.0356
2022-08-24 04:35:55 - train: epoch 0011, iter [04600, 05004], lr: 0.097087, loss: 2.1511
2022-08-24 04:36:31 - train: epoch 0011, iter [04700, 05004], lr: 0.097076, loss: 1.8973
2022-08-24 04:37:06 - train: epoch 0011, iter [04800, 05004], lr: 0.097066, loss: 1.9882
2022-08-24 04:37:41 - train: epoch 0011, iter [04900, 05004], lr: 0.097055, loss: 1.9414
2022-08-24 04:38:14 - train: epoch 0011, iter [05000, 05004], lr: 0.097044, loss: 2.0353
2022-08-24 04:38:15 - train: epoch 011, train_loss: 2.2062
2022-08-24 04:39:32 - eval: epoch: 011, acc1: 55.500%, acc5: 80.070%, test_loss: 1.8976, per_image_load_time: 2.386ms, per_image_inference_time: 0.607ms
2022-08-24 04:39:33 - until epoch: 011, best_acc1: 55.500%
2022-08-24 04:39:33 - epoch 012 lr: 0.097044
2022-08-24 04:40:14 - train: epoch 0012, iter [00100, 05004], lr: 0.097033, loss: 2.1099
2022-08-24 04:40:48 - train: epoch 0012, iter [00200, 05004], lr: 0.097023, loss: 2.0703
2022-08-24 04:41:23 - train: epoch 0012, iter [00300, 05004], lr: 0.097012, loss: 2.1269
2022-08-24 04:41:57 - train: epoch 0012, iter [00400, 05004], lr: 0.097001, loss: 2.2626
2022-08-24 04:42:32 - train: epoch 0012, iter [00500, 05004], lr: 0.096991, loss: 2.2992
2022-08-24 04:43:07 - train: epoch 0012, iter [00600, 05004], lr: 0.096980, loss: 1.8395
2022-08-24 04:43:42 - train: epoch 0012, iter [00700, 05004], lr: 0.096969, loss: 2.0750
2022-08-24 04:44:16 - train: epoch 0012, iter [00800, 05004], lr: 0.096958, loss: 2.1872
2022-08-24 04:44:51 - train: epoch 0012, iter [00900, 05004], lr: 0.096948, loss: 2.2283
2022-08-24 04:45:26 - train: epoch 0012, iter [01000, 05004], lr: 0.096937, loss: 2.0555
2022-08-24 04:46:00 - train: epoch 0012, iter [01100, 05004], lr: 0.096926, loss: 2.5075
2022-08-24 04:46:35 - train: epoch 0012, iter [01200, 05004], lr: 0.096915, loss: 1.9167
2022-08-24 04:47:09 - train: epoch 0012, iter [01300, 05004], lr: 0.096904, loss: 2.1857
2022-08-24 04:47:44 - train: epoch 0012, iter [01400, 05004], lr: 0.096893, loss: 2.4103
2022-08-24 04:48:19 - train: epoch 0012, iter [01500, 05004], lr: 0.096882, loss: 1.9867
2022-08-24 04:48:53 - train: epoch 0012, iter [01600, 05004], lr: 0.096872, loss: 2.0798
2022-08-24 04:49:28 - train: epoch 0012, iter [01700, 05004], lr: 0.096861, loss: 2.0127
2022-08-24 04:50:02 - train: epoch 0012, iter [01800, 05004], lr: 0.096850, loss: 2.2317
2022-08-24 04:50:37 - train: epoch 0012, iter [01900, 05004], lr: 0.096839, loss: 2.2080
2022-08-24 04:51:12 - train: epoch 0012, iter [02000, 05004], lr: 0.096828, loss: 2.3395
2022-08-24 04:51:46 - train: epoch 0012, iter [02100, 05004], lr: 0.096817, loss: 2.1251
2022-08-24 04:52:21 - train: epoch 0012, iter [02200, 05004], lr: 0.096806, loss: 2.3215
2022-08-24 04:52:56 - train: epoch 0012, iter [02300, 05004], lr: 0.096795, loss: 2.1936
2022-08-24 04:53:31 - train: epoch 0012, iter [02400, 05004], lr: 0.096784, loss: 2.1415
2022-08-24 04:54:05 - train: epoch 0012, iter [02500, 05004], lr: 0.096772, loss: 2.0148
2022-08-24 04:54:40 - train: epoch 0012, iter [02600, 05004], lr: 0.096761, loss: 1.8741
2022-08-24 04:55:15 - train: epoch 0012, iter [02700, 05004], lr: 0.096750, loss: 2.1537
2022-08-24 04:55:50 - train: epoch 0012, iter [02800, 05004], lr: 0.096739, loss: 2.0860
2022-08-24 04:56:25 - train: epoch 0012, iter [02900, 05004], lr: 0.096728, loss: 2.0629
2022-08-24 04:56:59 - train: epoch 0012, iter [03000, 05004], lr: 0.096717, loss: 2.1473
2022-08-24 04:57:35 - train: epoch 0012, iter [03100, 05004], lr: 0.096706, loss: 2.3584
2022-08-24 04:58:09 - train: epoch 0012, iter [03200, 05004], lr: 0.096694, loss: 1.9305
2022-08-24 04:58:44 - train: epoch 0012, iter [03300, 05004], lr: 0.096683, loss: 2.2686
2022-08-24 04:59:19 - train: epoch 0012, iter [03400, 05004], lr: 0.096672, loss: 2.1610
2022-08-24 04:59:54 - train: epoch 0012, iter [03500, 05004], lr: 0.096661, loss: 2.2654
2022-08-24 05:00:29 - train: epoch 0012, iter [03600, 05004], lr: 0.096649, loss: 2.2001
2022-08-24 05:01:04 - train: epoch 0012, iter [03700, 05004], lr: 0.096638, loss: 2.2028
2022-08-24 05:01:39 - train: epoch 0012, iter [03800, 05004], lr: 0.096627, loss: 2.1529
2022-08-24 05:02:14 - train: epoch 0012, iter [03900, 05004], lr: 0.096615, loss: 2.0460
2022-08-24 05:02:48 - train: epoch 0012, iter [04000, 05004], lr: 0.096604, loss: 2.0748
2022-08-24 05:03:23 - train: epoch 0012, iter [04100, 05004], lr: 0.096593, loss: 2.0084
2022-08-24 05:03:59 - train: epoch 0012, iter [04200, 05004], lr: 0.096581, loss: 2.0522
2022-08-24 05:04:34 - train: epoch 0012, iter [04300, 05004], lr: 0.096570, loss: 2.2150
2022-08-24 05:05:08 - train: epoch 0012, iter [04400, 05004], lr: 0.096558, loss: 2.0715
2022-08-24 05:05:43 - train: epoch 0012, iter [04500, 05004], lr: 0.096547, loss: 2.0306
2022-08-24 05:06:18 - train: epoch 0012, iter [04600, 05004], lr: 0.096535, loss: 2.4265
2022-08-24 05:06:53 - train: epoch 0012, iter [04700, 05004], lr: 0.096524, loss: 2.1627
2022-08-24 05:07:28 - train: epoch 0012, iter [04800, 05004], lr: 0.096512, loss: 2.4149
2022-08-24 05:08:03 - train: epoch 0012, iter [04900, 05004], lr: 0.096501, loss: 2.1550
2022-08-24 05:08:37 - train: epoch 0012, iter [05000, 05004], lr: 0.096489, loss: 1.9536
2022-08-24 05:08:38 - train: epoch 012, train_loss: 2.1763
2022-08-24 05:09:54 - eval: epoch: 012, acc1: 55.660%, acc5: 80.422%, test_loss: 1.8877, per_image_load_time: 1.768ms, per_image_inference_time: 0.608ms
2022-08-24 05:09:54 - until epoch: 012, best_acc1: 55.660%
2022-08-24 05:09:54 - epoch 013 lr: 0.096489
2022-08-24 05:10:35 - train: epoch 0013, iter [00100, 05004], lr: 0.096477, loss: 2.0188
2022-08-24 05:11:10 - train: epoch 0013, iter [00200, 05004], lr: 0.096466, loss: 2.0867
2022-08-24 05:11:44 - train: epoch 0013, iter [00300, 05004], lr: 0.096454, loss: 2.0198
2022-08-24 05:12:18 - train: epoch 0013, iter [00400, 05004], lr: 0.096442, loss: 2.1195
2022-08-24 05:12:52 - train: epoch 0013, iter [00500, 05004], lr: 0.096431, loss: 2.0807
2022-08-24 05:13:26 - train: epoch 0013, iter [00600, 05004], lr: 0.096419, loss: 2.2556
2022-08-24 05:14:01 - train: epoch 0013, iter [00700, 05004], lr: 0.096407, loss: 2.0634
2022-08-24 05:14:36 - train: epoch 0013, iter [00800, 05004], lr: 0.096396, loss: 2.2362
2022-08-24 05:15:11 - train: epoch 0013, iter [00900, 05004], lr: 0.096384, loss: 1.9850
2022-08-24 05:15:46 - train: epoch 0013, iter [01000, 05004], lr: 0.096372, loss: 2.2450
2022-08-24 05:16:20 - train: epoch 0013, iter [01100, 05004], lr: 0.096361, loss: 2.2731
2022-08-24 05:16:56 - train: epoch 0013, iter [01200, 05004], lr: 0.096349, loss: 2.3151
2022-08-24 05:17:31 - train: epoch 0013, iter [01300, 05004], lr: 0.096337, loss: 2.1643
2022-08-24 05:18:06 - train: epoch 0013, iter [01400, 05004], lr: 0.096325, loss: 2.0804
2022-08-24 05:18:42 - train: epoch 0013, iter [01500, 05004], lr: 0.096313, loss: 2.3176
2022-08-24 05:19:16 - train: epoch 0013, iter [01600, 05004], lr: 0.096302, loss: 1.9193
2022-08-24 05:19:52 - train: epoch 0013, iter [01700, 05004], lr: 0.096290, loss: 2.1352
2022-08-24 05:20:27 - train: epoch 0013, iter [01800, 05004], lr: 0.096278, loss: 2.1690
2022-08-24 05:21:02 - train: epoch 0013, iter [01900, 05004], lr: 0.096266, loss: 2.1843
2022-08-24 05:21:37 - train: epoch 0013, iter [02000, 05004], lr: 0.096254, loss: 2.3794
2022-08-24 05:22:12 - train: epoch 0013, iter [02100, 05004], lr: 0.096242, loss: 2.3490
2022-08-24 05:22:47 - train: epoch 0013, iter [02200, 05004], lr: 0.096230, loss: 2.0474
2022-08-24 05:23:22 - train: epoch 0013, iter [02300, 05004], lr: 0.096218, loss: 2.2504
2022-08-24 05:23:57 - train: epoch 0013, iter [02400, 05004], lr: 0.096206, loss: 2.1673
2022-08-24 05:24:31 - train: epoch 0013, iter [02500, 05004], lr: 0.096194, loss: 2.0724
2022-08-24 05:25:06 - train: epoch 0013, iter [02600, 05004], lr: 0.096182, loss: 2.0706
2022-08-24 05:25:41 - train: epoch 0013, iter [02700, 05004], lr: 0.096170, loss: 2.0046
2022-08-24 05:26:16 - train: epoch 0013, iter [02800, 05004], lr: 0.096158, loss: 2.1977
2022-08-24 05:26:51 - train: epoch 0013, iter [02900, 05004], lr: 0.096146, loss: 2.2143
2022-08-24 05:27:25 - train: epoch 0013, iter [03000, 05004], lr: 0.096134, loss: 1.9748
2022-08-24 05:28:00 - train: epoch 0013, iter [03100, 05004], lr: 0.096122, loss: 1.9400
2022-08-24 05:28:35 - train: epoch 0013, iter [03200, 05004], lr: 0.096110, loss: 2.1817
2022-08-24 05:29:10 - train: epoch 0013, iter [03300, 05004], lr: 0.096098, loss: 1.9691
2022-08-24 05:29:45 - train: epoch 0013, iter [03400, 05004], lr: 0.096085, loss: 2.0397
2022-08-24 05:30:20 - train: epoch 0013, iter [03500, 05004], lr: 0.096073, loss: 2.1286
2022-08-24 05:30:54 - train: epoch 0013, iter [03600, 05004], lr: 0.096061, loss: 2.4513
2022-08-24 05:31:29 - train: epoch 0013, iter [03700, 05004], lr: 0.096049, loss: 1.8671
2022-08-24 05:32:05 - train: epoch 0013, iter [03800, 05004], lr: 0.096037, loss: 2.2858
2022-08-24 05:32:39 - train: epoch 0013, iter [03900, 05004], lr: 0.096024, loss: 2.2425
2022-08-24 05:33:14 - train: epoch 0013, iter [04000, 05004], lr: 0.096012, loss: 2.2057
2022-08-24 05:33:49 - train: epoch 0013, iter [04100, 05004], lr: 0.096000, loss: 1.9521
2022-08-24 05:34:25 - train: epoch 0013, iter [04200, 05004], lr: 0.095987, loss: 2.0791
2022-08-24 05:34:59 - train: epoch 0013, iter [04300, 05004], lr: 0.095975, loss: 2.0975
2022-08-24 05:35:35 - train: epoch 0013, iter [04400, 05004], lr: 0.095963, loss: 1.9968
2022-08-24 05:36:10 - train: epoch 0013, iter [04500, 05004], lr: 0.095950, loss: 2.1115
2022-08-24 05:36:45 - train: epoch 0013, iter [04600, 05004], lr: 0.095938, loss: 2.1758
2022-08-24 05:37:21 - train: epoch 0013, iter [04700, 05004], lr: 0.095926, loss: 2.2111
2022-08-24 05:37:55 - train: epoch 0013, iter [04800, 05004], lr: 0.095913, loss: 2.2742
2022-08-24 05:38:30 - train: epoch 0013, iter [04900, 05004], lr: 0.095901, loss: 2.1527
2022-08-24 05:39:05 - train: epoch 0013, iter [05000, 05004], lr: 0.095888, loss: 2.2541
2022-08-24 05:39:06 - train: epoch 013, train_loss: 2.1515
2022-08-24 05:40:23 - eval: epoch: 013, acc1: 54.832%, acc5: 79.710%, test_loss: 1.9419, per_image_load_time: 2.398ms, per_image_inference_time: 0.619ms
2022-08-24 05:40:24 - until epoch: 013, best_acc1: 55.660%
2022-08-24 05:40:24 - epoch 014 lr: 0.095888
2022-08-24 05:41:05 - train: epoch 0014, iter [00100, 05004], lr: 0.095875, loss: 2.1198
2022-08-24 05:41:39 - train: epoch 0014, iter [00200, 05004], lr: 0.095863, loss: 2.1159
2022-08-24 05:42:14 - train: epoch 0014, iter [00300, 05004], lr: 0.095850, loss: 1.8937
2022-08-24 05:42:48 - train: epoch 0014, iter [00400, 05004], lr: 0.095838, loss: 2.0384
2022-08-24 05:43:24 - train: epoch 0014, iter [00500, 05004], lr: 0.095825, loss: 2.0558
2022-08-24 05:43:59 - train: epoch 0014, iter [00600, 05004], lr: 0.095813, loss: 2.1234
2022-08-24 05:44:34 - train: epoch 0014, iter [00700, 05004], lr: 0.095800, loss: 2.1292
2022-08-24 05:45:08 - train: epoch 0014, iter [00800, 05004], lr: 0.095787, loss: 2.1013
2022-08-24 05:45:43 - train: epoch 0014, iter [00900, 05004], lr: 0.095775, loss: 2.2303
2022-08-24 05:46:18 - train: epoch 0014, iter [01000, 05004], lr: 0.095762, loss: 2.2318
2022-08-24 05:46:52 - train: epoch 0014, iter [01100, 05004], lr: 0.095750, loss: 2.1544
2022-08-24 05:47:27 - train: epoch 0014, iter [01200, 05004], lr: 0.095737, loss: 2.1723
2022-08-24 05:48:02 - train: epoch 0014, iter [01300, 05004], lr: 0.095724, loss: 2.2405
2022-08-24 05:48:37 - train: epoch 0014, iter [01400, 05004], lr: 0.095711, loss: 2.1681
2022-08-24 05:49:12 - train: epoch 0014, iter [01500, 05004], lr: 0.095699, loss: 2.3257
2022-08-24 05:49:47 - train: epoch 0014, iter [01600, 05004], lr: 0.095686, loss: 2.1275
2022-08-24 05:50:22 - train: epoch 0014, iter [01700, 05004], lr: 0.095673, loss: 2.3954
2022-08-24 05:50:57 - train: epoch 0014, iter [01800, 05004], lr: 0.095660, loss: 2.3812
2022-08-24 05:51:31 - train: epoch 0014, iter [01900, 05004], lr: 0.095648, loss: 2.0123
2022-08-24 05:52:06 - train: epoch 0014, iter [02000, 05004], lr: 0.095635, loss: 2.0273
2022-08-24 05:52:41 - train: epoch 0014, iter [02100, 05004], lr: 0.095622, loss: 2.1951
2022-08-24 05:53:15 - train: epoch 0014, iter [02200, 05004], lr: 0.095609, loss: 2.1146
2022-08-24 05:53:50 - train: epoch 0014, iter [02300, 05004], lr: 0.095596, loss: 2.1838
2022-08-24 05:54:25 - train: epoch 0014, iter [02400, 05004], lr: 0.095583, loss: 2.2924
2022-08-24 05:55:00 - train: epoch 0014, iter [02500, 05004], lr: 0.095570, loss: 2.0842
2022-08-24 05:55:35 - train: epoch 0014, iter [02600, 05004], lr: 0.095557, loss: 2.1393
2022-08-24 05:56:10 - train: epoch 0014, iter [02700, 05004], lr: 0.095545, loss: 2.0601
2022-08-24 05:56:45 - train: epoch 0014, iter [02800, 05004], lr: 0.095532, loss: 2.4642
2022-08-24 05:57:19 - train: epoch 0014, iter [02900, 05004], lr: 0.095519, loss: 2.0521
2022-08-24 05:57:55 - train: epoch 0014, iter [03000, 05004], lr: 0.095506, loss: 2.1830
2022-08-24 05:58:29 - train: epoch 0014, iter [03100, 05004], lr: 0.095493, loss: 1.9302
2022-08-24 05:59:04 - train: epoch 0014, iter [03200, 05004], lr: 0.095480, loss: 2.1790
2022-08-24 05:59:39 - train: epoch 0014, iter [03300, 05004], lr: 0.095467, loss: 1.9825
2022-08-24 06:00:14 - train: epoch 0014, iter [03400, 05004], lr: 0.095453, loss: 2.2331
2022-08-24 06:00:49 - train: epoch 0014, iter [03500, 05004], lr: 0.095440, loss: 2.0803
2022-08-24 06:01:24 - train: epoch 0014, iter [03600, 05004], lr: 0.095427, loss: 2.0365
2022-08-24 06:01:59 - train: epoch 0014, iter [03700, 05004], lr: 0.095414, loss: 2.0767
2022-08-24 06:02:33 - train: epoch 0014, iter [03800, 05004], lr: 0.095401, loss: 2.2520
2022-08-24 06:03:08 - train: epoch 0014, iter [03900, 05004], lr: 0.095388, loss: 2.0489
2022-08-24 06:03:43 - train: epoch 0014, iter [04000, 05004], lr: 0.095375, loss: 2.2674
2022-08-24 06:04:17 - train: epoch 0014, iter [04100, 05004], lr: 0.095361, loss: 2.0227
2022-08-24 06:04:52 - train: epoch 0014, iter [04200, 05004], lr: 0.095348, loss: 1.9536
2022-08-24 06:05:27 - train: epoch 0014, iter [04300, 05004], lr: 0.095335, loss: 1.9903
2022-08-24 06:06:01 - train: epoch 0014, iter [04400, 05004], lr: 0.095322, loss: 1.8879
2022-08-24 06:06:36 - train: epoch 0014, iter [04500, 05004], lr: 0.095308, loss: 2.2062
2022-08-24 06:07:11 - train: epoch 0014, iter [04600, 05004], lr: 0.095295, loss: 1.9938
2022-08-24 06:07:46 - train: epoch 0014, iter [04700, 05004], lr: 0.095282, loss: 2.0033
2022-08-24 06:08:21 - train: epoch 0014, iter [04800, 05004], lr: 0.095269, loss: 2.0361
2022-08-24 06:08:55 - train: epoch 0014, iter [04900, 05004], lr: 0.095255, loss: 1.8298
2022-08-24 06:09:29 - train: epoch 0014, iter [05000, 05004], lr: 0.095242, loss: 2.0735
2022-08-24 06:09:30 - train: epoch 014, train_loss: 2.1263
2022-08-24 06:10:47 - eval: epoch: 014, acc1: 54.396%, acc5: 79.722%, test_loss: 1.9595, per_image_load_time: 2.433ms, per_image_inference_time: 0.574ms
2022-08-24 06:10:47 - until epoch: 014, best_acc1: 55.660%
2022-08-24 06:10:47 - epoch 015 lr: 0.095241
2022-08-24 06:11:29 - train: epoch 0015, iter [00100, 05004], lr: 0.095228, loss: 1.7986
2022-08-24 06:12:03 - train: epoch 0015, iter [00200, 05004], lr: 0.095215, loss: 2.2621
2022-08-24 06:12:38 - train: epoch 0015, iter [00300, 05004], lr: 0.095201, loss: 2.2447
2022-08-24 06:13:13 - train: epoch 0015, iter [00400, 05004], lr: 0.095188, loss: 2.0809
2022-08-24 06:13:48 - train: epoch 0015, iter [00500, 05004], lr: 0.095174, loss: 2.1256
2022-08-24 06:14:22 - train: epoch 0015, iter [00600, 05004], lr: 0.095161, loss: 2.2685
2022-08-24 06:14:57 - train: epoch 0015, iter [00700, 05004], lr: 0.095147, loss: 2.0071
2022-08-24 06:15:31 - train: epoch 0015, iter [00800, 05004], lr: 0.095134, loss: 1.9191
2022-08-24 06:16:06 - train: epoch 0015, iter [00900, 05004], lr: 0.095120, loss: 1.9535
2022-08-24 06:16:41 - train: epoch 0015, iter [01000, 05004], lr: 0.095107, loss: 2.1836
2022-08-24 06:17:16 - train: epoch 0015, iter [01100, 05004], lr: 0.095093, loss: 1.9379
2022-08-24 06:17:50 - train: epoch 0015, iter [01200, 05004], lr: 0.095080, loss: 2.0915
2022-08-24 06:18:25 - train: epoch 0015, iter [01300, 05004], lr: 0.095066, loss: 2.3075
2022-08-24 06:18:59 - train: epoch 0015, iter [01400, 05004], lr: 0.095052, loss: 1.9874
2022-08-24 06:19:33 - train: epoch 0015, iter [01500, 05004], lr: 0.095039, loss: 1.7830
2022-08-24 06:20:08 - train: epoch 0015, iter [01600, 05004], lr: 0.095025, loss: 2.1064
2022-08-24 06:20:42 - train: epoch 0015, iter [01700, 05004], lr: 0.095012, loss: 2.1965
2022-08-24 06:21:17 - train: epoch 0015, iter [01800, 05004], lr: 0.094998, loss: 1.9991
2022-08-24 06:21:52 - train: epoch 0015, iter [01900, 05004], lr: 0.094984, loss: 1.9650
2022-08-24 06:22:27 - train: epoch 0015, iter [02000, 05004], lr: 0.094970, loss: 2.0836
2022-08-24 06:23:02 - train: epoch 0015, iter [02100, 05004], lr: 0.094957, loss: 2.0520
2022-08-24 06:23:37 - train: epoch 0015, iter [02200, 05004], lr: 0.094943, loss: 2.2485
2022-08-24 06:24:12 - train: epoch 0015, iter [02300, 05004], lr: 0.094929, loss: 1.9368
2022-08-24 06:24:47 - train: epoch 0015, iter [02400, 05004], lr: 0.094915, loss: 2.2390
2022-08-24 06:25:23 - train: epoch 0015, iter [02500, 05004], lr: 0.094902, loss: 2.2169
2022-08-24 06:25:57 - train: epoch 0015, iter [02600, 05004], lr: 0.094888, loss: 1.9415
2022-08-24 06:26:32 - train: epoch 0015, iter [02700, 05004], lr: 0.094874, loss: 2.1858
2022-08-24 06:27:07 - train: epoch 0015, iter [02800, 05004], lr: 0.094860, loss: 2.2377
2022-08-24 06:27:41 - train: epoch 0015, iter [02900, 05004], lr: 0.094846, loss: 2.0197
2022-08-24 06:28:16 - train: epoch 0015, iter [03000, 05004], lr: 0.094832, loss: 1.9483
2022-08-24 06:28:51 - train: epoch 0015, iter [03100, 05004], lr: 0.094818, loss: 2.0792
2022-08-24 06:29:26 - train: epoch 0015, iter [03200, 05004], lr: 0.094805, loss: 2.0812
2022-08-24 06:30:01 - train: epoch 0015, iter [03300, 05004], lr: 0.094791, loss: 1.8136
2022-08-24 06:30:35 - train: epoch 0015, iter [03400, 05004], lr: 0.094777, loss: 2.2261
2022-08-24 06:31:09 - train: epoch 0015, iter [03500, 05004], lr: 0.094763, loss: 2.3028
2022-08-24 06:31:44 - train: epoch 0015, iter [03600, 05004], lr: 0.094749, loss: 2.1446
2022-08-24 06:32:19 - train: epoch 0015, iter [03700, 05004], lr: 0.094735, loss: 1.9650
2022-08-24 06:32:52 - train: epoch 0015, iter [03800, 05004], lr: 0.094721, loss: 2.1350
2022-08-24 06:33:26 - train: epoch 0015, iter [03900, 05004], lr: 0.094707, loss: 2.2573
2022-08-24 06:33:59 - train: epoch 0015, iter [04000, 05004], lr: 0.094693, loss: 2.1823
2022-08-24 06:34:33 - train: epoch 0015, iter [04100, 05004], lr: 0.094678, loss: 2.2730
2022-08-24 06:35:07 - train: epoch 0015, iter [04200, 05004], lr: 0.094664, loss: 1.8994
2022-08-24 06:35:40 - train: epoch 0015, iter [04300, 05004], lr: 0.094650, loss: 2.1734
2022-08-24 06:36:14 - train: epoch 0015, iter [04400, 05004], lr: 0.094636, loss: 2.0589
2022-08-24 06:36:48 - train: epoch 0015, iter [04500, 05004], lr: 0.094622, loss: 2.0957
2022-08-24 06:37:22 - train: epoch 0015, iter [04600, 05004], lr: 0.094608, loss: 2.0522
2022-08-24 06:37:56 - train: epoch 0015, iter [04700, 05004], lr: 0.094594, loss: 2.1603
2022-08-24 06:38:30 - train: epoch 0015, iter [04800, 05004], lr: 0.094579, loss: 2.0032
2022-08-24 06:39:04 - train: epoch 0015, iter [04900, 05004], lr: 0.094565, loss: 2.1509
2022-08-24 06:39:37 - train: epoch 0015, iter [05000, 05004], lr: 0.094551, loss: 2.1520
2022-08-24 06:39:38 - train: epoch 015, train_loss: 2.1073
2022-08-24 06:40:55 - eval: epoch: 015, acc1: 56.876%, acc5: 80.962%, test_loss: 1.8339, per_image_load_time: 0.935ms, per_image_inference_time: 0.595ms
2022-08-24 06:40:56 - until epoch: 015, best_acc1: 56.876%
2022-08-24 06:40:56 - epoch 016 lr: 0.094550
2022-08-24 06:41:36 - train: epoch 0016, iter [00100, 05004], lr: 0.094536, loss: 2.0732
2022-08-24 06:42:10 - train: epoch 0016, iter [00200, 05004], lr: 0.094522, loss: 1.9031
2022-08-24 06:42:44 - train: epoch 0016, iter [00300, 05004], lr: 0.094507, loss: 2.1237
2022-08-24 06:43:18 - train: epoch 0016, iter [00400, 05004], lr: 0.094493, loss: 2.2941
2022-08-24 06:43:53 - train: epoch 0016, iter [00500, 05004], lr: 0.094479, loss: 1.8726
2022-08-24 06:44:27 - train: epoch 0016, iter [00600, 05004], lr: 0.094465, loss: 2.1568
2022-08-24 06:45:01 - train: epoch 0016, iter [00700, 05004], lr: 0.094450, loss: 1.7919
2022-08-24 06:45:35 - train: epoch 0016, iter [00800, 05004], lr: 0.094436, loss: 2.0935
2022-08-24 06:46:10 - train: epoch 0016, iter [00900, 05004], lr: 0.094421, loss: 2.1194
2022-08-24 06:46:44 - train: epoch 0016, iter [01000, 05004], lr: 0.094407, loss: 2.0392
2022-08-24 06:47:19 - train: epoch 0016, iter [01100, 05004], lr: 0.094393, loss: 1.9214
2022-08-24 06:47:53 - train: epoch 0016, iter [01200, 05004], lr: 0.094378, loss: 1.9555
2022-08-24 06:48:26 - train: epoch 0016, iter [01300, 05004], lr: 0.094364, loss: 2.1786
2022-08-24 06:49:00 - train: epoch 0016, iter [01400, 05004], lr: 0.094349, loss: 1.9266
2022-08-24 06:49:34 - train: epoch 0016, iter [01500, 05004], lr: 0.094335, loss: 2.1962
2022-08-24 06:50:08 - train: epoch 0016, iter [01600, 05004], lr: 0.094320, loss: 2.2616
2022-08-24 06:50:42 - train: epoch 0016, iter [01700, 05004], lr: 0.094306, loss: 2.0323
2022-08-24 06:51:15 - train: epoch 0016, iter [01800, 05004], lr: 0.094291, loss: 2.0310
2022-08-24 06:51:49 - train: epoch 0016, iter [01900, 05004], lr: 0.094276, loss: 2.1698
2022-08-24 06:52:23 - train: epoch 0016, iter [02000, 05004], lr: 0.094262, loss: 1.7412
2022-08-24 06:52:57 - train: epoch 0016, iter [02100, 05004], lr: 0.094247, loss: 2.2012
2022-08-24 06:53:31 - train: epoch 0016, iter [02200, 05004], lr: 0.094233, loss: 2.0941
2022-08-24 06:54:04 - train: epoch 0016, iter [02300, 05004], lr: 0.094218, loss: 2.2868
2022-08-24 06:54:38 - train: epoch 0016, iter [02400, 05004], lr: 0.094203, loss: 2.2062
2022-08-24 06:55:12 - train: epoch 0016, iter [02500, 05004], lr: 0.094189, loss: 1.9982
2022-08-24 06:55:47 - train: epoch 0016, iter [02600, 05004], lr: 0.094174, loss: 2.2265
2022-08-24 06:56:21 - train: epoch 0016, iter [02700, 05004], lr: 0.094159, loss: 2.0552
2022-08-24 06:56:54 - train: epoch 0016, iter [02800, 05004], lr: 0.094144, loss: 1.9514
2022-08-24 06:57:28 - train: epoch 0016, iter [02900, 05004], lr: 0.094130, loss: 2.2385
2022-08-24 06:58:02 - train: epoch 0016, iter [03000, 05004], lr: 0.094115, loss: 2.2991
2022-08-24 06:58:36 - train: epoch 0016, iter [03100, 05004], lr: 0.094100, loss: 2.1622
2022-08-24 06:59:09 - train: epoch 0016, iter [03200, 05004], lr: 0.094085, loss: 2.1436
2022-08-24 06:59:44 - train: epoch 0016, iter [03300, 05004], lr: 0.094071, loss: 2.0994
2022-08-24 07:00:18 - train: epoch 0016, iter [03400, 05004], lr: 0.094056, loss: 2.0043
2022-08-24 07:00:52 - train: epoch 0016, iter [03500, 05004], lr: 0.094041, loss: 2.1616
2022-08-24 07:01:26 - train: epoch 0016, iter [03600, 05004], lr: 0.094026, loss: 1.9061
2022-08-24 07:02:00 - train: epoch 0016, iter [03700, 05004], lr: 0.094011, loss: 2.1740
2022-08-24 07:02:35 - train: epoch 0016, iter [03800, 05004], lr: 0.093996, loss: 2.4463
2022-08-24 07:03:09 - train: epoch 0016, iter [03900, 05004], lr: 0.093981, loss: 2.1328
2022-08-24 07:03:43 - train: epoch 0016, iter [04000, 05004], lr: 0.093966, loss: 2.1957
2022-08-24 07:04:17 - train: epoch 0016, iter [04100, 05004], lr: 0.093951, loss: 2.0413
2022-08-24 07:04:51 - train: epoch 0016, iter [04200, 05004], lr: 0.093936, loss: 2.0945
2022-08-24 07:05:25 - train: epoch 0016, iter [04300, 05004], lr: 0.093921, loss: 1.8597
2022-08-24 07:05:59 - train: epoch 0016, iter [04400, 05004], lr: 0.093906, loss: 2.0020
2022-08-24 07:06:33 - train: epoch 0016, iter [04500, 05004], lr: 0.093891, loss: 2.1803
2022-08-24 07:07:07 - train: epoch 0016, iter [04600, 05004], lr: 0.093876, loss: 1.9822
2022-08-24 07:07:42 - train: epoch 0016, iter [04700, 05004], lr: 0.093861, loss: 2.3541
2022-08-24 07:08:17 - train: epoch 0016, iter [04800, 05004], lr: 0.093846, loss: 2.0238
2022-08-24 07:08:50 - train: epoch 0016, iter [04900, 05004], lr: 0.093831, loss: 2.0843
2022-08-24 07:09:23 - train: epoch 0016, iter [05000, 05004], lr: 0.093816, loss: 2.0843
2022-08-24 07:09:24 - train: epoch 016, train_loss: 2.0900
2022-08-24 07:10:40 - eval: epoch: 016, acc1: 56.766%, acc5: 81.486%, test_loss: 1.8257, per_image_load_time: 2.372ms, per_image_inference_time: 0.608ms
2022-08-24 07:10:40 - until epoch: 016, best_acc1: 56.876%
2022-08-24 07:10:40 - epoch 017 lr: 0.093815
2022-08-24 07:11:21 - train: epoch 0017, iter [00100, 05004], lr: 0.093800, loss: 1.9353
2022-08-24 07:11:55 - train: epoch 0017, iter [00200, 05004], lr: 0.093785, loss: 2.0165
2022-08-24 07:12:28 - train: epoch 0017, iter [00300, 05004], lr: 0.093770, loss: 2.2982
2022-08-24 07:13:02 - train: epoch 0017, iter [00400, 05004], lr: 0.093755, loss: 1.8829
2022-08-24 07:13:36 - train: epoch 0017, iter [00500, 05004], lr: 0.093740, loss: 2.1542
2022-08-24 07:14:10 - train: epoch 0017, iter [00600, 05004], lr: 0.093724, loss: 2.4344
2022-08-24 07:14:44 - train: epoch 0017, iter [00700, 05004], lr: 0.093709, loss: 2.0480
2022-08-24 07:15:18 - train: epoch 0017, iter [00800, 05004], lr: 0.093694, loss: 2.0642
2022-08-24 07:15:52 - train: epoch 0017, iter [00900, 05004], lr: 0.093679, loss: 2.0278
2022-08-24 07:16:26 - train: epoch 0017, iter [01000, 05004], lr: 0.093663, loss: 1.9984
2022-08-24 07:17:00 - train: epoch 0017, iter [01100, 05004], lr: 0.093648, loss: 2.3143
2022-08-24 07:17:34 - train: epoch 0017, iter [01200, 05004], lr: 0.093633, loss: 2.2782
2022-08-24 07:18:08 - train: epoch 0017, iter [01300, 05004], lr: 0.093617, loss: 1.9914
2022-08-24 07:18:43 - train: epoch 0017, iter [01400, 05004], lr: 0.093602, loss: 2.2012
2022-08-24 07:19:17 - train: epoch 0017, iter [01500, 05004], lr: 0.093587, loss: 1.6243
2022-08-24 07:19:51 - train: epoch 0017, iter [01600, 05004], lr: 0.093571, loss: 1.8533
2022-08-24 07:20:25 - train: epoch 0017, iter [01700, 05004], lr: 0.093556, loss: 2.0344
2022-08-24 07:20:59 - train: epoch 0017, iter [01800, 05004], lr: 0.093540, loss: 2.0611
2022-08-24 07:21:33 - train: epoch 0017, iter [01900, 05004], lr: 0.093525, loss: 1.9665
2022-08-24 07:22:08 - train: epoch 0017, iter [02000, 05004], lr: 0.093509, loss: 2.2405
2022-08-24 07:22:41 - train: epoch 0017, iter [02100, 05004], lr: 0.093494, loss: 2.1671
2022-08-24 07:23:15 - train: epoch 0017, iter [02200, 05004], lr: 0.093478, loss: 1.9858
2022-08-24 07:23:50 - train: epoch 0017, iter [02300, 05004], lr: 0.093463, loss: 2.0981
2022-08-24 07:24:24 - train: epoch 0017, iter [02400, 05004], lr: 0.093447, loss: 2.0043
2022-08-24 07:24:59 - train: epoch 0017, iter [02500, 05004], lr: 0.093432, loss: 2.2732
2022-08-24 07:25:32 - train: epoch 0017, iter [02600, 05004], lr: 0.093416, loss: 2.0423
2022-08-24 07:26:06 - train: epoch 0017, iter [02700, 05004], lr: 0.093401, loss: 1.8920
2022-08-24 07:26:42 - train: epoch 0017, iter [02800, 05004], lr: 0.093385, loss: 2.3521
2022-08-24 07:27:16 - train: epoch 0017, iter [02900, 05004], lr: 0.093370, loss: 2.3038
2022-08-24 07:27:50 - train: epoch 0017, iter [03000, 05004], lr: 0.093354, loss: 1.9170
2022-08-24 07:28:24 - train: epoch 0017, iter [03100, 05004], lr: 0.093338, loss: 2.2617
2022-08-24 07:28:59 - train: epoch 0017, iter [03200, 05004], lr: 0.093323, loss: 1.9739
2022-08-24 07:29:33 - train: epoch 0017, iter [03300, 05004], lr: 0.093307, loss: 2.1573
2022-08-24 07:30:07 - train: epoch 0017, iter [03400, 05004], lr: 0.093291, loss: 1.8173
2022-08-24 07:30:41 - train: epoch 0017, iter [03500, 05004], lr: 0.093276, loss: 2.1556
2022-08-24 07:31:15 - train: epoch 0017, iter [03600, 05004], lr: 0.093260, loss: 2.4131
2022-08-24 07:31:50 - train: epoch 0017, iter [03700, 05004], lr: 0.093244, loss: 2.0791
2022-08-24 07:32:24 - train: epoch 0017, iter [03800, 05004], lr: 0.093228, loss: 2.2763
2022-08-24 07:32:57 - train: epoch 0017, iter [03900, 05004], lr: 0.093212, loss: 1.8869
2022-08-24 07:33:32 - train: epoch 0017, iter [04000, 05004], lr: 0.093197, loss: 1.9424
2022-08-24 07:34:07 - train: epoch 0017, iter [04100, 05004], lr: 0.093181, loss: 2.1205
2022-08-24 07:34:41 - train: epoch 0017, iter [04200, 05004], lr: 0.093165, loss: 1.9790
2022-08-24 07:35:15 - train: epoch 0017, iter [04300, 05004], lr: 0.093149, loss: 1.9948
2022-08-24 07:35:49 - train: epoch 0017, iter [04400, 05004], lr: 0.093133, loss: 2.0196
2022-08-24 07:36:23 - train: epoch 0017, iter [04500, 05004], lr: 0.093117, loss: 2.2223
2022-08-24 07:36:57 - train: epoch 0017, iter [04600, 05004], lr: 0.093102, loss: 2.0804
2022-08-24 07:37:31 - train: epoch 0017, iter [04700, 05004], lr: 0.093086, loss: 2.1359
2022-08-24 07:38:04 - train: epoch 0017, iter [04800, 05004], lr: 0.093070, loss: 2.2536
2022-08-24 07:38:39 - train: epoch 0017, iter [04900, 05004], lr: 0.093054, loss: 1.9800
2022-08-24 07:39:13 - train: epoch 0017, iter [05000, 05004], lr: 0.093038, loss: 1.8462
2022-08-24 07:39:14 - train: epoch 017, train_loss: 2.0736
2022-08-24 07:40:30 - eval: epoch: 017, acc1: 57.550%, acc5: 81.902%, test_loss: 1.7899, per_image_load_time: 2.209ms, per_image_inference_time: 0.617ms
2022-08-24 07:40:30 - until epoch: 017, best_acc1: 57.550%
2022-08-24 07:40:30 - epoch 018 lr: 0.093037
2022-08-24 07:41:10 - train: epoch 0018, iter [00100, 05004], lr: 0.093021, loss: 2.1110
2022-08-24 07:41:43 - train: epoch 0018, iter [00200, 05004], lr: 0.093005, loss: 2.0166
2022-08-24 07:42:17 - train: epoch 0018, iter [00300, 05004], lr: 0.092989, loss: 2.3182
2022-08-24 07:42:51 - train: epoch 0018, iter [00400, 05004], lr: 0.092973, loss: 2.1944
2022-08-24 07:43:25 - train: epoch 0018, iter [00500, 05004], lr: 0.092957, loss: 1.9951
2022-08-24 07:44:00 - train: epoch 0018, iter [00600, 05004], lr: 0.092941, loss: 2.1764
2022-08-24 07:44:34 - train: epoch 0018, iter [00700, 05004], lr: 0.092925, loss: 1.8575
2022-08-24 07:45:08 - train: epoch 0018, iter [00800, 05004], lr: 0.092909, loss: 2.0824
2022-08-24 07:45:41 - train: epoch 0018, iter [00900, 05004], lr: 0.092893, loss: 2.0978
2022-08-24 07:46:16 - train: epoch 0018, iter [01000, 05004], lr: 0.092876, loss: 1.8847
2022-08-24 07:46:50 - train: epoch 0018, iter [01100, 05004], lr: 0.092860, loss: 2.3260
2022-08-24 07:47:24 - train: epoch 0018, iter [01200, 05004], lr: 0.092844, loss: 2.0191
2022-08-24 07:47:58 - train: epoch 0018, iter [01300, 05004], lr: 0.092828, loss: 2.3788
2022-08-24 07:48:32 - train: epoch 0018, iter [01400, 05004], lr: 0.092812, loss: 2.0875
2022-08-24 07:49:07 - train: epoch 0018, iter [01500, 05004], lr: 0.092796, loss: 2.1641
2022-08-24 07:49:41 - train: epoch 0018, iter [01600, 05004], lr: 0.092779, loss: 2.0318
2022-08-24 07:50:15 - train: epoch 0018, iter [01700, 05004], lr: 0.092763, loss: 2.0745
2022-08-24 07:50:49 - train: epoch 0018, iter [01800, 05004], lr: 0.092747, loss: 1.8150
2022-08-24 07:51:23 - train: epoch 0018, iter [01900, 05004], lr: 0.092730, loss: 2.0312
2022-08-24 07:51:57 - train: epoch 0018, iter [02000, 05004], lr: 0.092714, loss: 2.2995
2022-08-24 07:52:32 - train: epoch 0018, iter [02100, 05004], lr: 0.092698, loss: 2.2660
2022-08-24 07:53:06 - train: epoch 0018, iter [02200, 05004], lr: 0.092681, loss: 2.1175
2022-08-24 07:53:40 - train: epoch 0018, iter [02300, 05004], lr: 0.092665, loss: 2.1184
2022-08-24 07:54:14 - train: epoch 0018, iter [02400, 05004], lr: 0.092649, loss: 1.9354
2022-08-24 07:54:48 - train: epoch 0018, iter [02500, 05004], lr: 0.092632, loss: 1.8489
2022-08-24 07:55:22 - train: epoch 0018, iter [02600, 05004], lr: 0.092616, loss: 1.9419
2022-08-24 07:55:56 - train: epoch 0018, iter [02700, 05004], lr: 0.092600, loss: 2.2062
2022-08-24 07:56:30 - train: epoch 0018, iter [02800, 05004], lr: 0.092583, loss: 1.7426
2022-08-24 07:57:05 - train: epoch 0018, iter [02900, 05004], lr: 0.092567, loss: 2.0309
2022-08-24 07:57:39 - train: epoch 0018, iter [03000, 05004], lr: 0.092550, loss: 2.0606
2022-08-24 07:58:13 - train: epoch 0018, iter [03100, 05004], lr: 0.092534, loss: 2.3927
2022-08-24 07:58:47 - train: epoch 0018, iter [03200, 05004], lr: 0.092517, loss: 1.9594
2022-08-24 07:59:21 - train: epoch 0018, iter [03300, 05004], lr: 0.092501, loss: 2.0034
2022-08-24 07:59:55 - train: epoch 0018, iter [03400, 05004], lr: 0.092484, loss: 2.1631
2022-08-24 08:00:29 - train: epoch 0018, iter [03500, 05004], lr: 0.092467, loss: 2.3343
2022-08-24 08:01:03 - train: epoch 0018, iter [03600, 05004], lr: 0.092451, loss: 2.0704
2022-08-24 08:01:37 - train: epoch 0018, iter [03700, 05004], lr: 0.092434, loss: 2.4216
2022-08-24 08:02:12 - train: epoch 0018, iter [03800, 05004], lr: 0.092418, loss: 2.1578
2022-08-24 08:02:46 - train: epoch 0018, iter [03900, 05004], lr: 0.092401, loss: 2.1258
2022-08-24 08:03:20 - train: epoch 0018, iter [04000, 05004], lr: 0.092384, loss: 1.9683
2022-08-24 08:03:54 - train: epoch 0018, iter [04100, 05004], lr: 0.092368, loss: 2.0722
2022-08-24 08:04:28 - train: epoch 0018, iter [04200, 05004], lr: 0.092351, loss: 2.0523
2022-08-24 08:05:02 - train: epoch 0018, iter [04300, 05004], lr: 0.092334, loss: 1.8790
2022-08-24 08:05:37 - train: epoch 0018, iter [04400, 05004], lr: 0.092318, loss: 2.2381
2022-08-24 08:06:11 - train: epoch 0018, iter [04500, 05004], lr: 0.092301, loss: 2.0302
2022-08-24 08:06:45 - train: epoch 0018, iter [04600, 05004], lr: 0.092284, loss: 1.9154
2022-08-24 08:07:19 - train: epoch 0018, iter [04700, 05004], lr: 0.092267, loss: 2.4292
2022-08-24 08:07:53 - train: epoch 0018, iter [04800, 05004], lr: 0.092251, loss: 2.1314
2022-08-24 08:08:27 - train: epoch 0018, iter [04900, 05004], lr: 0.092234, loss: 2.0704
2022-08-24 08:09:00 - train: epoch 0018, iter [05000, 05004], lr: 0.092217, loss: 2.2256
2022-08-24 08:09:02 - train: epoch 018, train_loss: 2.0584
2022-08-24 08:10:18 - eval: epoch: 018, acc1: 56.646%, acc5: 80.840%, test_loss: 1.8473, per_image_load_time: 2.344ms, per_image_inference_time: 0.616ms
2022-08-24 08:10:18 - until epoch: 018, best_acc1: 57.550%
2022-08-24 08:10:18 - epoch 019 lr: 0.092216
2022-08-24 08:10:59 - train: epoch 0019, iter [00100, 05004], lr: 0.092200, loss: 1.8852
2022-08-24 08:11:33 - train: epoch 0019, iter [00200, 05004], lr: 0.092183, loss: 2.2598
2022-08-24 08:12:07 - train: epoch 0019, iter [00300, 05004], lr: 0.092166, loss: 2.2652
2022-08-24 08:12:41 - train: epoch 0019, iter [00400, 05004], lr: 0.092149, loss: 1.9968
2022-08-24 08:13:15 - train: epoch 0019, iter [00500, 05004], lr: 0.092132, loss: 2.0388
2022-08-24 08:13:49 - train: epoch 0019, iter [00600, 05004], lr: 0.092115, loss: 1.9346
2022-08-24 08:14:23 - train: epoch 0019, iter [00700, 05004], lr: 0.092098, loss: 1.8090
2022-08-24 08:14:57 - train: epoch 0019, iter [00800, 05004], lr: 0.092081, loss: 2.1819
2022-08-24 08:15:32 - train: epoch 0019, iter [00900, 05004], lr: 0.092064, loss: 2.0963
2022-08-24 08:16:06 - train: epoch 0019, iter [01000, 05004], lr: 0.092047, loss: 2.1762
2022-08-24 08:16:40 - train: epoch 0019, iter [01100, 05004], lr: 0.092030, loss: 1.9336
2022-08-24 08:17:15 - train: epoch 0019, iter [01200, 05004], lr: 0.092013, loss: 2.1453
2022-08-24 08:17:48 - train: epoch 0019, iter [01300, 05004], lr: 0.091996, loss: 2.1813
2022-08-24 08:18:22 - train: epoch 0019, iter [01400, 05004], lr: 0.091979, loss: 1.9301
2022-08-24 08:18:57 - train: epoch 0019, iter [01500, 05004], lr: 0.091962, loss: 2.5043
2022-08-24 08:19:31 - train: epoch 0019, iter [01600, 05004], lr: 0.091945, loss: 2.0503
2022-08-24 08:20:05 - train: epoch 0019, iter [01700, 05004], lr: 0.091928, loss: 2.1289
2022-08-24 08:20:39 - train: epoch 0019, iter [01800, 05004], lr: 0.091911, loss: 1.9371
2022-08-24 08:21:13 - train: epoch 0019, iter [01900, 05004], lr: 0.091894, loss: 2.1507
2022-08-24 08:21:48 - train: epoch 0019, iter [02000, 05004], lr: 0.091877, loss: 1.9675
2022-08-24 08:22:22 - train: epoch 0019, iter [02100, 05004], lr: 0.091860, loss: 1.8708
2022-08-24 08:22:56 - train: epoch 0019, iter [02200, 05004], lr: 0.091842, loss: 2.0408
2022-08-24 08:23:31 - train: epoch 0019, iter [02300, 05004], lr: 0.091825, loss: 2.0384
2022-08-24 08:24:05 - train: epoch 0019, iter [02400, 05004], lr: 0.091808, loss: 2.1165
2022-08-24 08:24:39 - train: epoch 0019, iter [02500, 05004], lr: 0.091791, loss: 1.9778
2022-08-24 08:25:13 - train: epoch 0019, iter [02600, 05004], lr: 0.091773, loss: 2.0411
2022-08-24 08:25:47 - train: epoch 0019, iter [02700, 05004], lr: 0.091756, loss: 2.2125
2022-08-24 08:26:21 - train: epoch 0019, iter [02800, 05004], lr: 0.091739, loss: 2.0293
2022-08-24 08:26:55 - train: epoch 0019, iter [02900, 05004], lr: 0.091722, loss: 2.0584
2022-08-24 08:27:30 - train: epoch 0019, iter [03000, 05004], lr: 0.091704, loss: 2.3955
2022-08-24 08:28:04 - train: epoch 0019, iter [03100, 05004], lr: 0.091687, loss: 2.1142
2022-08-24 08:28:38 - train: epoch 0019, iter [03200, 05004], lr: 0.091670, loss: 1.7180
2022-08-24 08:29:12 - train: epoch 0019, iter [03300, 05004], lr: 0.091652, loss: 1.9202
2022-08-24 08:29:46 - train: epoch 0019, iter [03400, 05004], lr: 0.091635, loss: 2.1402
2022-08-24 08:30:20 - train: epoch 0019, iter [03500, 05004], lr: 0.091618, loss: 2.0141
2022-08-24 08:30:54 - train: epoch 0019, iter [03600, 05004], lr: 0.091600, loss: 1.8643
2022-08-24 08:31:28 - train: epoch 0019, iter [03700, 05004], lr: 0.091583, loss: 2.1425
2022-08-24 08:32:02 - train: epoch 0019, iter [03800, 05004], lr: 0.091565, loss: 2.2348
2022-08-24 08:32:37 - train: epoch 0019, iter [03900, 05004], lr: 0.091548, loss: 1.9828
2022-08-24 08:33:12 - train: epoch 0019, iter [04000, 05004], lr: 0.091530, loss: 1.9263
2022-08-24 08:33:46 - train: epoch 0019, iter [04100, 05004], lr: 0.091513, loss: 2.1478
2022-08-24 08:34:20 - train: epoch 0019, iter [04200, 05004], lr: 0.091495, loss: 2.0260
2022-08-24 08:34:55 - train: epoch 0019, iter [04300, 05004], lr: 0.091478, loss: 1.9504
2022-08-24 08:35:29 - train: epoch 0019, iter [04400, 05004], lr: 0.091460, loss: 2.1277
2022-08-24 08:36:03 - train: epoch 0019, iter [04500, 05004], lr: 0.091443, loss: 2.2980
2022-08-24 08:36:37 - train: epoch 0019, iter [04600, 05004], lr: 0.091425, loss: 1.9606
2022-08-24 08:37:12 - train: epoch 0019, iter [04700, 05004], lr: 0.091408, loss: 1.9251
2022-08-24 08:37:46 - train: epoch 0019, iter [04800, 05004], lr: 0.091390, loss: 2.0573
2022-08-24 08:38:20 - train: epoch 0019, iter [04900, 05004], lr: 0.091372, loss: 2.0306
2022-08-24 08:38:53 - train: epoch 0019, iter [05000, 05004], lr: 0.091355, loss: 2.0347
2022-08-24 08:38:55 - train: epoch 019, train_loss: 2.0477
2022-08-24 08:40:10 - eval: epoch: 019, acc1: 56.100%, acc5: 80.406%, test_loss: 1.8783, per_image_load_time: 2.300ms, per_image_inference_time: 0.628ms
2022-08-24 08:40:10 - until epoch: 019, best_acc1: 57.550%
2022-08-24 08:40:10 - epoch 020 lr: 0.091354
2022-08-24 08:40:50 - train: epoch 0020, iter [00100, 05004], lr: 0.091336, loss: 2.1500
2022-08-24 08:41:24 - train: epoch 0020, iter [00200, 05004], lr: 0.091319, loss: 1.8399
2022-08-24 08:41:58 - train: epoch 0020, iter [00300, 05004], lr: 0.091301, loss: 2.0098
2022-08-24 08:42:32 - train: epoch 0020, iter [00400, 05004], lr: 0.091283, loss: 1.8161
2022-08-24 08:43:06 - train: epoch 0020, iter [00500, 05004], lr: 0.091266, loss: 2.0675
2022-08-24 08:43:40 - train: epoch 0020, iter [00600, 05004], lr: 0.091248, loss: 2.2540
2022-08-24 08:44:14 - train: epoch 0020, iter [00700, 05004], lr: 0.091230, loss: 1.7732
2022-08-24 08:44:48 - train: epoch 0020, iter [00800, 05004], lr: 0.091212, loss: 1.9889
2022-08-24 08:45:22 - train: epoch 0020, iter [00900, 05004], lr: 0.091195, loss: 2.3405
2022-08-24 08:45:56 - train: epoch 0020, iter [01000, 05004], lr: 0.091177, loss: 2.1521
2022-08-24 08:46:30 - train: epoch 0020, iter [01100, 05004], lr: 0.091159, loss: 1.7981
2022-08-24 08:47:04 - train: epoch 0020, iter [01200, 05004], lr: 0.091141, loss: 1.7651
2022-08-24 08:47:39 - train: epoch 0020, iter [01300, 05004], lr: 0.091123, loss: 1.9061
2022-08-24 08:48:13 - train: epoch 0020, iter [01400, 05004], lr: 0.091105, loss: 2.1519
2022-08-24 08:48:47 - train: epoch 0020, iter [01500, 05004], lr: 0.091088, loss: 2.1098
2022-08-24 08:49:22 - train: epoch 0020, iter [01600, 05004], lr: 0.091070, loss: 2.0248
2022-08-24 08:49:56 - train: epoch 0020, iter [01700, 05004], lr: 0.091052, loss: 1.7696
2022-08-24 08:50:31 - train: epoch 0020, iter [01800, 05004], lr: 0.091034, loss: 2.0567
2022-08-24 08:51:04 - train: epoch 0020, iter [01900, 05004], lr: 0.091016, loss: 1.9252
2022-08-24 08:51:39 - train: epoch 0020, iter [02000, 05004], lr: 0.090998, loss: 1.9985
2022-08-24 08:52:13 - train: epoch 0020, iter [02100, 05004], lr: 0.090980, loss: 2.1577
2022-08-24 08:52:48 - train: epoch 0020, iter [02200, 05004], lr: 0.090962, loss: 1.8332
2022-08-24 08:53:22 - train: epoch 0020, iter [02300, 05004], lr: 0.090944, loss: 1.9470
2022-08-24 08:53:56 - train: epoch 0020, iter [02400, 05004], lr: 0.090926, loss: 2.1934
2022-08-24 08:54:31 - train: epoch 0020, iter [02500, 05004], lr: 0.090908, loss: 2.0340
2022-08-24 08:55:04 - train: epoch 0020, iter [02600, 05004], lr: 0.090890, loss: 1.8654
2022-08-24 08:55:38 - train: epoch 0020, iter [02700, 05004], lr: 0.090872, loss: 2.1252
2022-08-24 08:56:13 - train: epoch 0020, iter [02800, 05004], lr: 0.090854, loss: 2.0932
2022-08-24 08:56:47 - train: epoch 0020, iter [02900, 05004], lr: 0.090836, loss: 2.1807
2022-08-24 08:57:22 - train: epoch 0020, iter [03000, 05004], lr: 0.090817, loss: 1.9602
2022-08-24 08:57:56 - train: epoch 0020, iter [03100, 05004], lr: 0.090799, loss: 2.1551
2022-08-24 08:58:31 - train: epoch 0020, iter [03200, 05004], lr: 0.090781, loss: 2.1652
2022-08-24 08:59:05 - train: epoch 0020, iter [03300, 05004], lr: 0.090763, loss: 1.8024
2022-08-24 08:59:40 - train: epoch 0020, iter [03400, 05004], lr: 0.090745, loss: 2.0641
2022-08-24 09:00:14 - train: epoch 0020, iter [03500, 05004], lr: 0.090727, loss: 1.8777
2022-08-24 09:00:48 - train: epoch 0020, iter [03600, 05004], lr: 0.090708, loss: 1.9283
2022-08-24 09:01:23 - train: epoch 0020, iter [03700, 05004], lr: 0.090690, loss: 1.9677
2022-08-24 09:01:58 - train: epoch 0020, iter [03800, 05004], lr: 0.090672, loss: 2.0461
2022-08-24 09:02:32 - train: epoch 0020, iter [03900, 05004], lr: 0.090654, loss: 2.1553
2022-08-24 09:03:07 - train: epoch 0020, iter [04000, 05004], lr: 0.090635, loss: 1.8644
2022-08-24 09:03:41 - train: epoch 0020, iter [04100, 05004], lr: 0.090617, loss: 1.9255
2022-08-24 09:04:15 - train: epoch 0020, iter [04200, 05004], lr: 0.090599, loss: 1.9639
2022-08-24 09:04:49 - train: epoch 0020, iter [04300, 05004], lr: 0.090580, loss: 2.0651
2022-08-24 09:05:24 - train: epoch 0020, iter [04400, 05004], lr: 0.090562, loss: 1.9873
2022-08-24 09:05:58 - train: epoch 0020, iter [04500, 05004], lr: 0.090544, loss: 2.1273
2022-08-24 09:06:32 - train: epoch 0020, iter [04600, 05004], lr: 0.090525, loss: 2.1120
2022-08-24 09:07:06 - train: epoch 0020, iter [04700, 05004], lr: 0.090507, loss: 1.8312
2022-08-24 09:07:40 - train: epoch 0020, iter [04800, 05004], lr: 0.090488, loss: 1.9710
2022-08-24 09:08:15 - train: epoch 0020, iter [04900, 05004], lr: 0.090470, loss: 2.0654
2022-08-24 09:08:48 - train: epoch 0020, iter [05000, 05004], lr: 0.090452, loss: 1.9037
2022-08-24 09:08:49 - train: epoch 020, train_loss: 2.0294
2022-08-24 09:10:06 - eval: epoch: 020, acc1: 59.292%, acc5: 83.148%, test_loss: 1.7083, per_image_load_time: 2.263ms, per_image_inference_time: 0.622ms
2022-08-24 09:10:06 - until epoch: 020, best_acc1: 59.292%
2022-08-24 09:10:06 - epoch 021 lr: 0.090451
2022-08-24 09:10:46 - train: epoch 0021, iter [00100, 05004], lr: 0.090432, loss: 1.7645
2022-08-24 09:11:20 - train: epoch 0021, iter [00200, 05004], lr: 0.090414, loss: 1.9967
2022-08-24 09:11:53 - train: epoch 0021, iter [00300, 05004], lr: 0.090395, loss: 1.7067
2022-08-24 09:12:27 - train: epoch 0021, iter [00400, 05004], lr: 0.090377, loss: 2.0736
2022-08-24 09:13:01 - train: epoch 0021, iter [00500, 05004], lr: 0.090358, loss: 1.9630
2022-08-24 09:13:34 - train: epoch 0021, iter [00600, 05004], lr: 0.090340, loss: 1.7900
2022-08-24 09:14:08 - train: epoch 0021, iter [00700, 05004], lr: 0.090321, loss: 1.8318
2022-08-24 09:14:42 - train: epoch 0021, iter [00800, 05004], lr: 0.090303, loss: 2.0704
2022-08-24 09:15:16 - train: epoch 0021, iter [00900, 05004], lr: 0.090284, loss: 2.1201
2022-08-24 09:15:50 - train: epoch 0021, iter [01000, 05004], lr: 0.090266, loss: 1.9228
2022-08-24 09:16:24 - train: epoch 0021, iter [01100, 05004], lr: 0.090247, loss: 1.9424
2022-08-24 09:16:57 - train: epoch 0021, iter [01200, 05004], lr: 0.090228, loss: 1.9780
2022-08-24 09:17:31 - train: epoch 0021, iter [01300, 05004], lr: 0.090210, loss: 1.8873
2022-08-24 09:18:05 - train: epoch 0021, iter [01400, 05004], lr: 0.090191, loss: 1.9257
2022-08-24 09:18:39 - train: epoch 0021, iter [01500, 05004], lr: 0.090172, loss: 1.8098
2022-08-24 09:19:13 - train: epoch 0021, iter [01600, 05004], lr: 0.090154, loss: 2.0985
2022-08-24 09:19:47 - train: epoch 0021, iter [01700, 05004], lr: 0.090135, loss: 2.0347
2022-08-24 09:20:21 - train: epoch 0021, iter [01800, 05004], lr: 0.090116, loss: 1.8690
2022-08-24 09:20:55 - train: epoch 0021, iter [01900, 05004], lr: 0.090097, loss: 2.2255
2022-08-24 09:21:29 - train: epoch 0021, iter [02000, 05004], lr: 0.090079, loss: 2.2476
2022-08-24 09:22:02 - train: epoch 0021, iter [02100, 05004], lr: 0.090060, loss: 1.8865
2022-08-24 09:22:36 - train: epoch 0021, iter [02200, 05004], lr: 0.090041, loss: 1.9098
2022-08-24 09:23:10 - train: epoch 0021, iter [02300, 05004], lr: 0.090022, loss: 1.9113
2022-08-24 09:23:44 - train: epoch 0021, iter [02400, 05004], lr: 0.090003, loss: 1.8979
2022-08-24 09:24:18 - train: epoch 0021, iter [02500, 05004], lr: 0.089985, loss: 2.1041
2022-08-24 09:24:52 - train: epoch 0021, iter [02600, 05004], lr: 0.089966, loss: 2.2511
2022-08-24 09:25:26 - train: epoch 0021, iter [02700, 05004], lr: 0.089947, loss: 1.9534
2022-08-24 09:26:00 - train: epoch 0021, iter [02800, 05004], lr: 0.089928, loss: 1.9480
2022-08-24 09:26:34 - train: epoch 0021, iter [02900, 05004], lr: 0.089909, loss: 1.9255
2022-08-24 09:27:07 - train: epoch 0021, iter [03000, 05004], lr: 0.089890, loss: 2.1619
2022-08-24 09:27:41 - train: epoch 0021, iter [03100, 05004], lr: 0.089871, loss: 2.0296
2022-08-24 09:28:15 - train: epoch 0021, iter [03200, 05004], lr: 0.089852, loss: 1.9728
2022-08-24 09:28:49 - train: epoch 0021, iter [03300, 05004], lr: 0.089833, loss: 2.3035
2022-08-24 09:29:23 - train: epoch 0021, iter [03400, 05004], lr: 0.089814, loss: 2.1034
2022-08-24 09:29:57 - train: epoch 0021, iter [03500, 05004], lr: 0.089795, loss: 1.8782
2022-08-24 09:30:31 - train: epoch 0021, iter [03600, 05004], lr: 0.089776, loss: 1.9375
2022-08-24 09:31:05 - train: epoch 0021, iter [03700, 05004], lr: 0.089757, loss: 2.0290
2022-08-24 09:31:39 - train: epoch 0021, iter [03800, 05004], lr: 0.089738, loss: 1.9658
2022-08-24 09:32:13 - train: epoch 0021, iter [03900, 05004], lr: 0.089719, loss: 1.9360
2022-08-24 09:32:47 - train: epoch 0021, iter [04000, 05004], lr: 0.089700, loss: 2.2583
2022-08-24 09:33:22 - train: epoch 0021, iter [04100, 05004], lr: 0.089681, loss: 2.0557
2022-08-24 09:33:55 - train: epoch 0021, iter [04200, 05004], lr: 0.089662, loss: 1.8855
2022-08-24 09:34:29 - train: epoch 0021, iter [04300, 05004], lr: 0.089643, loss: 1.9132
2022-08-24 09:35:02 - train: epoch 0021, iter [04400, 05004], lr: 0.089624, loss: 2.1673
2022-08-24 09:35:36 - train: epoch 0021, iter [04500, 05004], lr: 0.089605, loss: 2.0831
2022-08-24 09:36:10 - train: epoch 0021, iter [04600, 05004], lr: 0.089585, loss: 1.9426
2022-08-24 09:36:44 - train: epoch 0021, iter [04700, 05004], lr: 0.089566, loss: 2.2526
2022-08-24 09:37:18 - train: epoch 0021, iter [04800, 05004], lr: 0.089547, loss: 2.2032
2022-08-24 09:37:52 - train: epoch 0021, iter [04900, 05004], lr: 0.089528, loss: 1.8064
2022-08-24 09:38:25 - train: epoch 0021, iter [05000, 05004], lr: 0.089509, loss: 1.9657
2022-08-24 09:38:27 - train: epoch 021, train_loss: 2.0196
2022-08-24 09:39:42 - eval: epoch: 021, acc1: 58.840%, acc5: 82.778%, test_loss: 1.7221, per_image_load_time: 2.137ms, per_image_inference_time: 0.614ms
2022-08-24 09:39:43 - until epoch: 021, best_acc1: 59.292%
2022-08-24 09:39:43 - epoch 022 lr: 0.089508
2022-08-24 09:40:23 - train: epoch 0022, iter [00100, 05004], lr: 0.089489, loss: 1.7502
2022-08-24 09:40:57 - train: epoch 0022, iter [00200, 05004], lr: 0.089469, loss: 1.8726
2022-08-24 09:41:32 - train: epoch 0022, iter [00300, 05004], lr: 0.089450, loss: 1.7513
2022-08-24 09:42:05 - train: epoch 0022, iter [00400, 05004], lr: 0.089431, loss: 1.9061
2022-08-24 09:42:40 - train: epoch 0022, iter [00500, 05004], lr: 0.089411, loss: 2.0216
2022-08-24 09:43:14 - train: epoch 0022, iter [00600, 05004], lr: 0.089392, loss: 2.1768
2022-08-24 09:43:48 - train: epoch 0022, iter [00700, 05004], lr: 0.089373, loss: 2.0434
2022-08-24 09:44:21 - train: epoch 0022, iter [00800, 05004], lr: 0.089353, loss: 2.1149
2022-08-24 09:44:55 - train: epoch 0022, iter [00900, 05004], lr: 0.089334, loss: 2.1654
2022-08-24 09:45:29 - train: epoch 0022, iter [01000, 05004], lr: 0.089315, loss: 2.0884
2022-08-24 09:46:04 - train: epoch 0022, iter [01100, 05004], lr: 0.089295, loss: 2.0395
2022-08-24 09:46:38 - train: epoch 0022, iter [01200, 05004], lr: 0.089276, loss: 1.6882
2022-08-24 09:47:12 - train: epoch 0022, iter [01300, 05004], lr: 0.089256, loss: 2.0530
2022-08-24 09:47:46 - train: epoch 0022, iter [01400, 05004], lr: 0.089237, loss: 2.0818
2022-08-24 09:48:20 - train: epoch 0022, iter [01500, 05004], lr: 0.089217, loss: 1.9743
2022-08-24 09:48:54 - train: epoch 0022, iter [01600, 05004], lr: 0.089198, loss: 1.8067
2022-08-24 09:49:27 - train: epoch 0022, iter [01700, 05004], lr: 0.089178, loss: 1.9203
2022-08-24 09:50:01 - train: epoch 0022, iter [01800, 05004], lr: 0.089159, loss: 2.2099
2022-08-24 09:50:35 - train: epoch 0022, iter [01900, 05004], lr: 0.089139, loss: 1.7751
2022-08-24 09:51:09 - train: epoch 0022, iter [02000, 05004], lr: 0.089120, loss: 2.0181
2022-08-24 09:51:43 - train: epoch 0022, iter [02100, 05004], lr: 0.089100, loss: 2.0642
2022-08-24 09:52:16 - train: epoch 0022, iter [02200, 05004], lr: 0.089081, loss: 1.7257
2022-08-24 09:52:50 - train: epoch 0022, iter [02300, 05004], lr: 0.089061, loss: 2.1447
2022-08-24 09:53:24 - train: epoch 0022, iter [02400, 05004], lr: 0.089042, loss: 2.0637
2022-08-24 09:53:58 - train: epoch 0022, iter [02500, 05004], lr: 0.089022, loss: 1.9982
2022-08-24 09:54:32 - train: epoch 0022, iter [02600, 05004], lr: 0.089002, loss: 1.8014
2022-08-24 09:55:06 - train: epoch 0022, iter [02700, 05004], lr: 0.088983, loss: 1.8258
2022-08-24 09:55:40 - train: epoch 0022, iter [02800, 05004], lr: 0.088963, loss: 2.2829
2022-08-24 09:56:14 - train: epoch 0022, iter [02900, 05004], lr: 0.088943, loss: 1.9491
2022-08-24 09:56:47 - train: epoch 0022, iter [03000, 05004], lr: 0.088924, loss: 2.1565
2022-08-24 09:57:21 - train: epoch 0022, iter [03100, 05004], lr: 0.088904, loss: 2.0778
2022-08-24 09:57:55 - train: epoch 0022, iter [03200, 05004], lr: 0.088884, loss: 2.1497
2022-08-24 09:58:29 - train: epoch 0022, iter [03300, 05004], lr: 0.088864, loss: 1.9603
2022-08-24 09:59:02 - train: epoch 0022, iter [03400, 05004], lr: 0.088845, loss: 1.8663
2022-08-24 09:59:36 - train: epoch 0022, iter [03500, 05004], lr: 0.088825, loss: 2.1381
2022-08-24 10:00:10 - train: epoch 0022, iter [03600, 05004], lr: 0.088805, loss: 2.0933
2022-08-24 10:00:43 - train: epoch 0022, iter [03700, 05004], lr: 0.088785, loss: 2.1054
2022-08-24 10:01:17 - train: epoch 0022, iter [03800, 05004], lr: 0.088765, loss: 2.1042
2022-08-24 10:01:51 - train: epoch 0022, iter [03900, 05004], lr: 0.088746, loss: 1.9469
2022-08-24 10:02:26 - train: epoch 0022, iter [04000, 05004], lr: 0.088726, loss: 2.2047
2022-08-24 10:02:59 - train: epoch 0022, iter [04100, 05004], lr: 0.088706, loss: 2.0005
2022-08-24 10:03:33 - train: epoch 0022, iter [04200, 05004], lr: 0.088686, loss: 1.9640
2022-08-24 10:04:07 - train: epoch 0022, iter [04300, 05004], lr: 0.088666, loss: 2.3092
2022-08-24 10:04:41 - train: epoch 0022, iter [04400, 05004], lr: 0.088646, loss: 2.0695
2022-08-24 10:05:15 - train: epoch 0022, iter [04500, 05004], lr: 0.088626, loss: 1.8858
2022-08-24 10:05:49 - train: epoch 0022, iter [04600, 05004], lr: 0.088606, loss: 2.3403
2022-08-24 10:06:23 - train: epoch 0022, iter [04700, 05004], lr: 0.088586, loss: 2.1407
2022-08-24 10:06:57 - train: epoch 0022, iter [04800, 05004], lr: 0.088566, loss: 1.9038
2022-08-24 10:07:31 - train: epoch 0022, iter [04900, 05004], lr: 0.088546, loss: 1.8046
2022-08-24 10:08:04 - train: epoch 0022, iter [05000, 05004], lr: 0.088526, loss: 1.9459
2022-08-24 10:08:05 - train: epoch 022, train_loss: 2.0080
2022-08-24 10:09:21 - eval: epoch: 022, acc1: 58.826%, acc5: 82.944%, test_loss: 1.7247, per_image_load_time: 2.176ms, per_image_inference_time: 0.622ms
2022-08-24 10:09:21 - until epoch: 022, best_acc1: 59.292%
2022-08-24 10:09:21 - epoch 023 lr: 0.088525
2022-08-24 10:10:02 - train: epoch 0023, iter [00100, 05004], lr: 0.088506, loss: 1.8344
2022-08-24 10:10:35 - train: epoch 0023, iter [00200, 05004], lr: 0.088486, loss: 1.6834
2022-08-24 10:11:09 - train: epoch 0023, iter [00300, 05004], lr: 0.088466, loss: 1.8310
2022-08-24 10:11:43 - train: epoch 0023, iter [00400, 05004], lr: 0.088446, loss: 2.0132
2022-08-24 10:12:17 - train: epoch 0023, iter [00500, 05004], lr: 0.088425, loss: 2.0096
2022-08-24 10:12:51 - train: epoch 0023, iter [00600, 05004], lr: 0.088405, loss: 1.8951
2022-08-24 10:13:25 - train: epoch 0023, iter [00700, 05004], lr: 0.088385, loss: 1.7230
2022-08-24 10:13:58 - train: epoch 0023, iter [00800, 05004], lr: 0.088365, loss: 1.8721
2022-08-24 10:14:32 - train: epoch 0023, iter [00900, 05004], lr: 0.088345, loss: 2.0355
2022-08-24 10:15:07 - train: epoch 0023, iter [01000, 05004], lr: 0.088325, loss: 1.9494
2022-08-24 10:15:40 - train: epoch 0023, iter [01100, 05004], lr: 0.088305, loss: 2.1322
2022-08-24 10:16:15 - train: epoch 0023, iter [01200, 05004], lr: 0.088284, loss: 1.7567
2022-08-24 10:16:49 - train: epoch 0023, iter [01300, 05004], lr: 0.088264, loss: 1.9106
2022-08-24 10:17:23 - train: epoch 0023, iter [01400, 05004], lr: 0.088244, loss: 2.0565
2022-08-24 10:17:57 - train: epoch 0023, iter [01500, 05004], lr: 0.088224, loss: 1.8262
2022-08-24 10:18:31 - train: epoch 0023, iter [01600, 05004], lr: 0.088204, loss: 2.1058
2022-08-24 10:19:05 - train: epoch 0023, iter [01700, 05004], lr: 0.088183, loss: 2.1930
2022-08-24 10:19:40 - train: epoch 0023, iter [01800, 05004], lr: 0.088163, loss: 1.9189
2022-08-24 10:20:13 - train: epoch 0023, iter [01900, 05004], lr: 0.088143, loss: 2.0681
2022-08-24 10:20:47 - train: epoch 0023, iter [02000, 05004], lr: 0.088122, loss: 1.6993
2022-08-24 10:21:21 - train: epoch 0023, iter [02100, 05004], lr: 0.088102, loss: 2.0673
2022-08-24 10:21:57 - train: epoch 0023, iter [02200, 05004], lr: 0.088082, loss: 1.7363
2022-08-24 10:22:31 - train: epoch 0023, iter [02300, 05004], lr: 0.088061, loss: 1.8231
2022-08-24 10:23:05 - train: epoch 0023, iter [02400, 05004], lr: 0.088041, loss: 2.0190
2022-08-24 10:23:40 - train: epoch 0023, iter [02500, 05004], lr: 0.088021, loss: 2.0283
2022-08-24 10:24:14 - train: epoch 0023, iter [02600, 05004], lr: 0.088000, loss: 1.9853
2022-08-24 10:24:49 - train: epoch 0023, iter [02700, 05004], lr: 0.087980, loss: 1.9848
2022-08-24 10:25:23 - train: epoch 0023, iter [02800, 05004], lr: 0.087959, loss: 2.0704
2022-08-24 10:25:57 - train: epoch 0023, iter [02900, 05004], lr: 0.087939, loss: 2.1093
2022-08-24 10:26:30 - train: epoch 0023, iter [03000, 05004], lr: 0.087919, loss: 2.2078
2022-08-24 10:27:05 - train: epoch 0023, iter [03100, 05004], lr: 0.087898, loss: 2.1203
2022-08-24 10:27:39 - train: epoch 0023, iter [03200, 05004], lr: 0.087878, loss: 2.1943
2022-08-24 10:28:14 - train: epoch 0023, iter [03300, 05004], lr: 0.087857, loss: 1.9956
2022-08-24 10:28:48 - train: epoch 0023, iter [03400, 05004], lr: 0.087837, loss: 2.1005
2022-08-24 10:29:23 - train: epoch 0023, iter [03500, 05004], lr: 0.087816, loss: 1.8145
2022-08-24 10:29:57 - train: epoch 0023, iter [03600, 05004], lr: 0.087796, loss: 1.7260
2022-08-24 10:30:31 - train: epoch 0023, iter [03700, 05004], lr: 0.087775, loss: 1.9818
2022-08-24 10:31:05 - train: epoch 0023, iter [03800, 05004], lr: 0.087754, loss: 2.0007
2022-08-24 10:31:38 - train: epoch 0023, iter [03900, 05004], lr: 0.087734, loss: 2.0119
2022-08-24 10:32:12 - train: epoch 0023, iter [04000, 05004], lr: 0.087713, loss: 1.9520
2022-08-24 10:32:46 - train: epoch 0023, iter [04100, 05004], lr: 0.087693, loss: 1.8757
2022-08-24 10:33:20 - train: epoch 0023, iter [04200, 05004], lr: 0.087672, loss: 1.9563
2022-08-24 10:33:54 - train: epoch 0023, iter [04300, 05004], lr: 0.087651, loss: 1.7653
2022-08-24 10:34:29 - train: epoch 0023, iter [04400, 05004], lr: 0.087631, loss: 1.8541
2022-08-24 10:35:03 - train: epoch 0023, iter [04500, 05004], lr: 0.087610, loss: 1.8257
2022-08-24 10:35:38 - train: epoch 0023, iter [04600, 05004], lr: 0.087589, loss: 2.1603
2022-08-24 10:36:12 - train: epoch 0023, iter [04700, 05004], lr: 0.087569, loss: 1.8156
2022-08-24 10:36:46 - train: epoch 0023, iter [04800, 05004], lr: 0.087548, loss: 1.9440
2022-08-24 10:37:20 - train: epoch 0023, iter [04900, 05004], lr: 0.087527, loss: 1.9533
2022-08-24 10:37:52 - train: epoch 0023, iter [05000, 05004], lr: 0.087506, loss: 2.0319
2022-08-24 10:37:53 - train: epoch 023, train_loss: 1.9960
2022-08-24 10:39:10 - eval: epoch: 023, acc1: 58.912%, acc5: 82.638%, test_loss: 1.7456, per_image_load_time: 2.133ms, per_image_inference_time: 0.619ms
2022-08-24 10:39:10 - until epoch: 023, best_acc1: 59.292%
2022-08-24 10:39:10 - epoch 024 lr: 0.087505
2022-08-24 10:39:50 - train: epoch 0024, iter [00100, 05004], lr: 0.087485, loss: 1.9215
2022-08-24 10:40:25 - train: epoch 0024, iter [00200, 05004], lr: 0.087464, loss: 1.9512
2022-08-24 10:40:59 - train: epoch 0024, iter [00300, 05004], lr: 0.087443, loss: 1.9358
2022-08-24 10:41:34 - train: epoch 0024, iter [00400, 05004], lr: 0.087422, loss: 2.0000
2022-08-24 10:42:07 - train: epoch 0024, iter [00500, 05004], lr: 0.087402, loss: 1.9021
2022-08-24 10:42:41 - train: epoch 0024, iter [00600, 05004], lr: 0.087381, loss: 1.7753
2022-08-24 10:43:15 - train: epoch 0024, iter [00700, 05004], lr: 0.087360, loss: 1.8839
2022-08-24 10:43:50 - train: epoch 0024, iter [00800, 05004], lr: 0.087339, loss: 1.8851
2022-08-24 10:44:24 - train: epoch 0024, iter [00900, 05004], lr: 0.087318, loss: 1.9771
2022-08-24 10:44:58 - train: epoch 0024, iter [01000, 05004], lr: 0.087297, loss: 1.9056
2022-08-24 10:45:32 - train: epoch 0024, iter [01100, 05004], lr: 0.087276, loss: 1.6755
2022-08-24 10:46:05 - train: epoch 0024, iter [01200, 05004], lr: 0.087255, loss: 1.8579
2022-08-24 10:46:39 - train: epoch 0024, iter [01300, 05004], lr: 0.087234, loss: 2.2950
2022-08-24 10:47:13 - train: epoch 0024, iter [01400, 05004], lr: 0.087213, loss: 1.8650
2022-08-24 10:47:48 - train: epoch 0024, iter [01500, 05004], lr: 0.087193, loss: 2.2022
2022-08-24 10:48:23 - train: epoch 0024, iter [01600, 05004], lr: 0.087172, loss: 2.0625
2022-08-24 10:48:57 - train: epoch 0024, iter [01700, 05004], lr: 0.087151, loss: 1.9939
2022-08-24 10:49:31 - train: epoch 0024, iter [01800, 05004], lr: 0.087130, loss: 2.2032
2022-08-24 10:50:06 - train: epoch 0024, iter [01900, 05004], lr: 0.087108, loss: 1.7314
2022-08-24 10:50:40 - train: epoch 0024, iter [02000, 05004], lr: 0.087087, loss: 1.9875
2022-08-24 10:51:13 - train: epoch 0024, iter [02100, 05004], lr: 0.087066, loss: 1.9833
2022-08-24 10:51:49 - train: epoch 0024, iter [02200, 05004], lr: 0.087045, loss: 1.7699
2022-08-24 10:52:22 - train: epoch 0024, iter [02300, 05004], lr: 0.087024, loss: 2.0388
2022-08-24 10:52:57 - train: epoch 0024, iter [02400, 05004], lr: 0.087003, loss: 2.0738
2022-08-24 10:53:31 - train: epoch 0024, iter [02500, 05004], lr: 0.086982, loss: 2.0052
2022-08-24 10:54:05 - train: epoch 0024, iter [02600, 05004], lr: 0.086961, loss: 1.9674
2022-08-24 10:54:39 - train: epoch 0024, iter [02700, 05004], lr: 0.086940, loss: 2.0536
2022-08-24 10:55:13 - train: epoch 0024, iter [02800, 05004], lr: 0.086919, loss: 2.0632
2022-08-24 10:55:48 - train: epoch 0024, iter [02900, 05004], lr: 0.086897, loss: 2.0446
2022-08-24 10:56:22 - train: epoch 0024, iter [03000, 05004], lr: 0.086876, loss: 1.9596
2022-08-24 10:56:56 - train: epoch 0024, iter [03100, 05004], lr: 0.086855, loss: 1.8665
2022-08-24 10:57:31 - train: epoch 0024, iter [03200, 05004], lr: 0.086834, loss: 2.0902
2022-08-24 10:58:05 - train: epoch 0024, iter [03300, 05004], lr: 0.086813, loss: 1.6426
2022-08-24 10:58:40 - train: epoch 0024, iter [03400, 05004], lr: 0.086791, loss: 2.0252
2022-08-24 10:59:14 - train: epoch 0024, iter [03500, 05004], lr: 0.086770, loss: 1.9177
2022-08-24 10:59:48 - train: epoch 0024, iter [03600, 05004], lr: 0.086749, loss: 2.1058
2022-08-24 11:00:23 - train: epoch 0024, iter [03700, 05004], lr: 0.086727, loss: 1.8939
2022-08-24 11:00:57 - train: epoch 0024, iter [03800, 05004], lr: 0.086706, loss: 2.1087
2022-08-24 11:01:30 - train: epoch 0024, iter [03900, 05004], lr: 0.086685, loss: 1.9632
2022-08-24 11:02:05 - train: epoch 0024, iter [04000, 05004], lr: 0.086663, loss: 2.0177
2022-08-24 11:02:40 - train: epoch 0024, iter [04100, 05004], lr: 0.086642, loss: 1.9357
2022-08-24 11:03:15 - train: epoch 0024, iter [04200, 05004], lr: 0.086621, loss: 1.9933
2022-08-24 11:03:49 - train: epoch 0024, iter [04300, 05004], lr: 0.086599, loss: 1.9030
2022-08-24 11:04:23 - train: epoch 0024, iter [04400, 05004], lr: 0.086578, loss: 2.0754
2022-08-24 11:04:58 - train: epoch 0024, iter [04500, 05004], lr: 0.086557, loss: 1.8742
2022-08-24 11:05:32 - train: epoch 0024, iter [04600, 05004], lr: 0.086535, loss: 2.0802
2022-08-24 11:06:06 - train: epoch 0024, iter [04700, 05004], lr: 0.086514, loss: 1.9397
2022-08-24 11:06:40 - train: epoch 0024, iter [04800, 05004], lr: 0.086492, loss: 1.9020
2022-08-24 11:07:14 - train: epoch 0024, iter [04900, 05004], lr: 0.086471, loss: 2.0262
2022-08-24 11:07:48 - train: epoch 0024, iter [05000, 05004], lr: 0.086449, loss: 1.9873
2022-08-24 11:07:49 - train: epoch 024, train_loss: 1.9855
2022-08-24 11:09:05 - eval: epoch: 024, acc1: 59.284%, acc5: 82.682%, test_loss: 1.7170, per_image_load_time: 1.722ms, per_image_inference_time: 0.603ms
2022-08-24 11:09:05 - until epoch: 024, best_acc1: 59.292%
2022-08-24 11:09:05 - epoch 025 lr: 0.086448
2022-08-24 11:09:45 - train: epoch 0025, iter [00100, 05004], lr: 0.086427, loss: 1.8126
2022-08-24 11:10:20 - train: epoch 0025, iter [00200, 05004], lr: 0.086405, loss: 1.7806
2022-08-24 11:10:54 - train: epoch 0025, iter [00300, 05004], lr: 0.086384, loss: 1.8859
2022-08-24 11:11:28 - train: epoch 0025, iter [00400, 05004], lr: 0.086362, loss: 1.8682
2022-08-24 11:12:02 - train: epoch 0025, iter [00500, 05004], lr: 0.086341, loss: 1.7871
2022-08-24 11:12:36 - train: epoch 0025, iter [00600, 05004], lr: 0.086319, loss: 1.8941
2022-08-24 11:13:10 - train: epoch 0025, iter [00700, 05004], lr: 0.086298, loss: 1.9935
2022-08-24 11:13:44 - train: epoch 0025, iter [00800, 05004], lr: 0.086276, loss: 1.9082
2022-08-24 11:14:18 - train: epoch 0025, iter [00900, 05004], lr: 0.086254, loss: 1.8260
2022-08-24 11:14:53 - train: epoch 0025, iter [01000, 05004], lr: 0.086233, loss: 1.9456
2022-08-24 11:15:27 - train: epoch 0025, iter [01100, 05004], lr: 0.086211, loss: 1.9050
2022-08-24 11:16:02 - train: epoch 0025, iter [01200, 05004], lr: 0.086190, loss: 2.0118
2022-08-24 11:16:36 - train: epoch 0025, iter [01300, 05004], lr: 0.086168, loss: 1.9848
2022-08-24 11:17:10 - train: epoch 0025, iter [01400, 05004], lr: 0.086146, loss: 2.0514
2022-08-24 11:17:44 - train: epoch 0025, iter [01500, 05004], lr: 0.086124, loss: 1.9421
2022-08-24 11:18:18 - train: epoch 0025, iter [01600, 05004], lr: 0.086103, loss: 1.6533
2022-08-24 11:18:53 - train: epoch 0025, iter [01700, 05004], lr: 0.086081, loss: 1.8656
2022-08-24 11:19:26 - train: epoch 0025, iter [01800, 05004], lr: 0.086059, loss: 1.7303
2022-08-24 11:20:01 - train: epoch 0025, iter [01900, 05004], lr: 0.086038, loss: 1.7788
2022-08-24 11:20:36 - train: epoch 0025, iter [02000, 05004], lr: 0.086016, loss: 2.0503
2022-08-24 11:21:11 - train: epoch 0025, iter [02100, 05004], lr: 0.085994, loss: 1.8211
2022-08-24 11:21:46 - train: epoch 0025, iter [02200, 05004], lr: 0.085972, loss: 1.8099
2022-08-24 11:22:20 - train: epoch 0025, iter [02300, 05004], lr: 0.085950, loss: 2.0495
2022-08-24 11:22:55 - train: epoch 0025, iter [02400, 05004], lr: 0.085929, loss: 1.7743
2022-08-24 11:23:29 - train: epoch 0025, iter [02500, 05004], lr: 0.085907, loss: 2.0779
2022-08-24 11:24:03 - train: epoch 0025, iter [02600, 05004], lr: 0.085885, loss: 2.1039
2022-08-24 11:24:37 - train: epoch 0025, iter [02700, 05004], lr: 0.085863, loss: 1.9680
2022-08-24 11:25:12 - train: epoch 0025, iter [02800, 05004], lr: 0.085841, loss: 1.9069
2022-08-24 11:25:46 - train: epoch 0025, iter [02900, 05004], lr: 0.085819, loss: 2.0977
2022-08-24 11:26:21 - train: epoch 0025, iter [03000, 05004], lr: 0.085797, loss: 2.2080
2022-08-24 11:26:55 - train: epoch 0025, iter [03100, 05004], lr: 0.085775, loss: 1.9045
2022-08-24 11:27:30 - train: epoch 0025, iter [03200, 05004], lr: 0.085753, loss: 2.2431
2022-08-24 11:28:04 - train: epoch 0025, iter [03300, 05004], lr: 0.085732, loss: 1.9391
2022-08-24 11:28:39 - train: epoch 0025, iter [03400, 05004], lr: 0.085710, loss: 2.1980
2022-08-24 11:29:13 - train: epoch 0025, iter [03500, 05004], lr: 0.085688, loss: 1.7027
2022-08-24 11:29:47 - train: epoch 0025, iter [03600, 05004], lr: 0.085666, loss: 1.9959
2022-08-24 11:30:21 - train: epoch 0025, iter [03700, 05004], lr: 0.085644, loss: 1.9408
2022-08-24 11:30:56 - train: epoch 0025, iter [03800, 05004], lr: 0.085622, loss: 1.9983
2022-08-24 11:31:30 - train: epoch 0025, iter [03900, 05004], lr: 0.085600, loss: 2.1905
2022-08-24 11:32:04 - train: epoch 0025, iter [04000, 05004], lr: 0.085577, loss: 2.0936
2022-08-24 11:32:38 - train: epoch 0025, iter [04100, 05004], lr: 0.085555, loss: 2.3469
2022-08-24 11:33:12 - train: epoch 0025, iter [04200, 05004], lr: 0.085533, loss: 1.9721
2022-08-24 11:33:46 - train: epoch 0025, iter [04300, 05004], lr: 0.085511, loss: 1.8585
2022-08-24 11:34:20 - train: epoch 0025, iter [04400, 05004], lr: 0.085489, loss: 1.9004
2022-08-24 11:34:54 - train: epoch 0025, iter [04500, 05004], lr: 0.085467, loss: 1.9870
2022-08-24 11:35:28 - train: epoch 0025, iter [04600, 05004], lr: 0.085445, loss: 2.0448
2022-08-24 11:36:02 - train: epoch 0025, iter [04700, 05004], lr: 0.085423, loss: 1.8162
2022-08-24 11:36:36 - train: epoch 0025, iter [04800, 05004], lr: 0.085401, loss: 1.7714
2022-08-24 11:37:10 - train: epoch 0025, iter [04900, 05004], lr: 0.085378, loss: 1.9981
2022-08-24 11:37:43 - train: epoch 0025, iter [05000, 05004], lr: 0.085356, loss: 2.1224
2022-08-24 11:37:45 - train: epoch 025, train_loss: 1.9734
2022-08-24 11:39:00 - eval: epoch: 025, acc1: 59.746%, acc5: 83.494%, test_loss: 1.6796, per_image_load_time: 2.325ms, per_image_inference_time: 0.608ms
2022-08-24 11:39:00 - until epoch: 025, best_acc1: 59.746%
2022-08-24 11:39:00 - epoch 026 lr: 0.085355
2022-08-24 11:39:40 - train: epoch 0026, iter [00100, 05004], lr: 0.085333, loss: 1.8332
2022-08-24 11:40:15 - train: epoch 0026, iter [00200, 05004], lr: 0.085311, loss: 1.7113
2022-08-24 11:40:48 - train: epoch 0026, iter [00300, 05004], lr: 0.085289, loss: 1.8664
2022-08-24 11:41:21 - train: epoch 0026, iter [00400, 05004], lr: 0.085266, loss: 1.8410
2022-08-24 11:41:55 - train: epoch 0026, iter [00500, 05004], lr: 0.085244, loss: 1.9991
2022-08-24 11:42:29 - train: epoch 0026, iter [00600, 05004], lr: 0.085222, loss: 2.1013
2022-08-24 11:43:02 - train: epoch 0026, iter [00700, 05004], lr: 0.085200, loss: 1.8272
2022-08-24 11:43:36 - train: epoch 0026, iter [00800, 05004], lr: 0.085177, loss: 1.8105
2022-08-24 11:44:10 - train: epoch 0026, iter [00900, 05004], lr: 0.085155, loss: 2.2557
2022-08-24 11:44:44 - train: epoch 0026, iter [01000, 05004], lr: 0.085133, loss: 1.8590
2022-08-24 11:45:17 - train: epoch 0026, iter [01100, 05004], lr: 0.085110, loss: 1.9803
2022-08-24 11:45:51 - train: epoch 0026, iter [01200, 05004], lr: 0.085088, loss: 1.9823
2022-08-24 11:46:25 - train: epoch 0026, iter [01300, 05004], lr: 0.085066, loss: 1.8463
2022-08-24 11:46:59 - train: epoch 0026, iter [01400, 05004], lr: 0.085043, loss: 2.0794
2022-08-24 11:47:33 - train: epoch 0026, iter [01500, 05004], lr: 0.085021, loss: 1.9362
2022-08-24 11:48:07 - train: epoch 0026, iter [01600, 05004], lr: 0.084998, loss: 2.1113
2022-08-24 11:48:41 - train: epoch 0026, iter [01700, 05004], lr: 0.084976, loss: 1.9153
2022-08-24 11:49:15 - train: epoch 0026, iter [01800, 05004], lr: 0.084954, loss: 1.9048
2022-08-24 11:49:49 - train: epoch 0026, iter [01900, 05004], lr: 0.084931, loss: 2.2715
2022-08-24 11:50:23 - train: epoch 0026, iter [02000, 05004], lr: 0.084909, loss: 2.0508
2022-08-24 11:50:56 - train: epoch 0026, iter [02100, 05004], lr: 0.084886, loss: 1.9865
2022-08-24 11:51:30 - train: epoch 0026, iter [02200, 05004], lr: 0.084864, loss: 1.9493
2022-08-24 11:52:05 - train: epoch 0026, iter [02300, 05004], lr: 0.084841, loss: 1.9487
2022-08-24 11:52:39 - train: epoch 0026, iter [02400, 05004], lr: 0.084819, loss: 2.2271
2022-08-24 11:53:13 - train: epoch 0026, iter [02500, 05004], lr: 0.084796, loss: 2.0911
2022-08-24 11:53:47 - train: epoch 0026, iter [02600, 05004], lr: 0.084774, loss: 1.9348
2022-08-24 11:54:21 - train: epoch 0026, iter [02700, 05004], lr: 0.084751, loss: 1.9092
2022-08-24 11:54:54 - train: epoch 0026, iter [02800, 05004], lr: 0.084728, loss: 1.8146
2022-08-24 11:55:28 - train: epoch 0026, iter [02900, 05004], lr: 0.084706, loss: 2.0914
2022-08-24 11:56:02 - train: epoch 0026, iter [03000, 05004], lr: 0.084683, loss: 1.9040
2022-08-24 11:56:36 - train: epoch 0026, iter [03100, 05004], lr: 0.084661, loss: 2.0135
2022-08-24 11:57:10 - train: epoch 0026, iter [03200, 05004], lr: 0.084638, loss: 1.9798
2022-08-24 11:57:44 - train: epoch 0026, iter [03300, 05004], lr: 0.084615, loss: 1.9320
2022-08-24 11:58:18 - train: epoch 0026, iter [03400, 05004], lr: 0.084593, loss: 2.2081
2022-08-24 11:58:52 - train: epoch 0026, iter [03500, 05004], lr: 0.084570, loss: 2.1207
2022-08-24 11:59:26 - train: epoch 0026, iter [03600, 05004], lr: 0.084547, loss: 1.7335
2022-08-24 12:00:00 - train: epoch 0026, iter [03700, 05004], lr: 0.084525, loss: 2.0786
2022-08-24 12:00:34 - train: epoch 0026, iter [03800, 05004], lr: 0.084502, loss: 2.1530
2022-08-24 12:01:07 - train: epoch 0026, iter [03900, 05004], lr: 0.084479, loss: 2.0702
2022-08-24 12:01:42 - train: epoch 0026, iter [04000, 05004], lr: 0.084456, loss: 2.1183
2022-08-24 12:02:16 - train: epoch 0026, iter [04100, 05004], lr: 0.084434, loss: 2.0050
2022-08-24 12:02:49 - train: epoch 0026, iter [04200, 05004], lr: 0.084411, loss: 2.0916
2022-08-24 12:03:24 - train: epoch 0026, iter [04300, 05004], lr: 0.084388, loss: 2.0490
2022-08-24 12:03:58 - train: epoch 0026, iter [04400, 05004], lr: 0.084365, loss: 2.1648
2022-08-24 12:04:32 - train: epoch 0026, iter [04500, 05004], lr: 0.084343, loss: 2.1923
2022-08-24 12:05:06 - train: epoch 0026, iter [04600, 05004], lr: 0.084320, loss: 1.9363
2022-08-24 12:05:40 - train: epoch 0026, iter [04700, 05004], lr: 0.084297, loss: 1.8543
2022-08-24 12:06:16 - train: epoch 0026, iter [04800, 05004], lr: 0.084274, loss: 1.8497
2022-08-24 12:06:49 - train: epoch 0026, iter [04900, 05004], lr: 0.084251, loss: 1.9874
2022-08-24 12:07:23 - train: epoch 0026, iter [05000, 05004], lr: 0.084228, loss: 2.1487
2022-08-24 12:07:25 - train: epoch 026, train_loss: 1.9636
2022-08-24 12:08:40 - eval: epoch: 026, acc1: 59.626%, acc5: 83.602%, test_loss: 1.6859, per_image_load_time: 2.327ms, per_image_inference_time: 0.598ms
2022-08-24 12:08:40 - until epoch: 026, best_acc1: 59.746%
2022-08-24 12:08:40 - epoch 027 lr: 0.084227
2022-08-24 12:09:20 - train: epoch 0027, iter [00100, 05004], lr: 0.084204, loss: 2.0386
2022-08-24 12:09:55 - train: epoch 0027, iter [00200, 05004], lr: 0.084182, loss: 1.9220
2022-08-24 12:10:29 - train: epoch 0027, iter [00300, 05004], lr: 0.084159, loss: 2.0057
2022-08-24 12:11:03 - train: epoch 0027, iter [00400, 05004], lr: 0.084136, loss: 2.1365
2022-08-24 12:11:37 - train: epoch 0027, iter [00500, 05004], lr: 0.084113, loss: 2.0055
2022-08-24 12:12:11 - train: epoch 0027, iter [00600, 05004], lr: 0.084090, loss: 2.2348
2022-08-24 12:12:46 - train: epoch 0027, iter [00700, 05004], lr: 0.084067, loss: 2.2244
2022-08-24 12:13:20 - train: epoch 0027, iter [00800, 05004], lr: 0.084044, loss: 1.9836
2022-08-24 12:13:55 - train: epoch 0027, iter [00900, 05004], lr: 0.084021, loss: 1.8131
2022-08-24 12:14:30 - train: epoch 0027, iter [01000, 05004], lr: 0.083998, loss: 1.9332
2022-08-24 12:15:05 - train: epoch 0027, iter [01100, 05004], lr: 0.083975, loss: 1.8213
2022-08-24 12:15:39 - train: epoch 0027, iter [01200, 05004], lr: 0.083952, loss: 2.1375
2022-08-24 12:16:13 - train: epoch 0027, iter [01300, 05004], lr: 0.083929, loss: 1.8391
2022-08-24 12:16:48 - train: epoch 0027, iter [01400, 05004], lr: 0.083906, loss: 2.1266
2022-08-24 12:17:22 - train: epoch 0027, iter [01500, 05004], lr: 0.083883, loss: 1.9900
2022-08-24 12:17:57 - train: epoch 0027, iter [01600, 05004], lr: 0.083860, loss: 1.9968
2022-08-24 12:18:31 - train: epoch 0027, iter [01700, 05004], lr: 0.083836, loss: 2.0361
2022-08-24 12:19:05 - train: epoch 0027, iter [01800, 05004], lr: 0.083813, loss: 1.9921
2022-08-24 12:19:40 - train: epoch 0027, iter [01900, 05004], lr: 0.083790, loss: 2.0395
2022-08-24 12:20:14 - train: epoch 0027, iter [02000, 05004], lr: 0.083767, loss: 1.8553
2022-08-24 12:20:48 - train: epoch 0027, iter [02100, 05004], lr: 0.083744, loss: 2.2375
2022-08-24 12:21:22 - train: epoch 0027, iter [02200, 05004], lr: 0.083721, loss: 2.1167
2022-08-24 12:21:57 - train: epoch 0027, iter [02300, 05004], lr: 0.083697, loss: 2.2199
2022-08-24 12:22:31 - train: epoch 0027, iter [02400, 05004], lr: 0.083674, loss: 1.9894
2022-08-24 12:23:05 - train: epoch 0027, iter [02500, 05004], lr: 0.083651, loss: 1.8126
2022-08-24 12:23:39 - train: epoch 0027, iter [02600, 05004], lr: 0.083628, loss: 2.1619
2022-08-24 12:24:13 - train: epoch 0027, iter [02700, 05004], lr: 0.083605, loss: 1.8727
2022-08-24 12:24:47 - train: epoch 0027, iter [02800, 05004], lr: 0.083581, loss: 2.0804
2022-08-24 12:25:21 - train: epoch 0027, iter [02900, 05004], lr: 0.083558, loss: 2.0095
2022-08-24 12:25:55 - train: epoch 0027, iter [03000, 05004], lr: 0.083535, loss: 1.8219
2022-08-24 12:26:29 - train: epoch 0027, iter [03100, 05004], lr: 0.083512, loss: 1.8699
2022-08-24 12:27:03 - train: epoch 0027, iter [03200, 05004], lr: 0.083488, loss: 1.7995
2022-08-24 12:27:37 - train: epoch 0027, iter [03300, 05004], lr: 0.083465, loss: 1.9805
2022-08-24 12:28:11 - train: epoch 0027, iter [03400, 05004], lr: 0.083442, loss: 1.8801
2022-08-24 12:28:46 - train: epoch 0027, iter [03500, 05004], lr: 0.083418, loss: 2.1303
2022-08-24 12:29:19 - train: epoch 0027, iter [03600, 05004], lr: 0.083395, loss: 1.7477
2022-08-24 12:29:53 - train: epoch 0027, iter [03700, 05004], lr: 0.083372, loss: 1.8517
2022-08-24 12:30:28 - train: epoch 0027, iter [03800, 05004], lr: 0.083348, loss: 1.6453
2022-08-24 12:31:02 - train: epoch 0027, iter [03900, 05004], lr: 0.083325, loss: 1.7526
2022-08-24 12:31:36 - train: epoch 0027, iter [04000, 05004], lr: 0.083301, loss: 1.9754
2022-08-24 12:32:11 - train: epoch 0027, iter [04100, 05004], lr: 0.083278, loss: 1.8947
2022-08-24 12:32:45 - train: epoch 0027, iter [04200, 05004], lr: 0.083254, loss: 1.9857
2022-08-24 12:33:19 - train: epoch 0027, iter [04300, 05004], lr: 0.083231, loss: 1.9403
2022-08-24 12:33:53 - train: epoch 0027, iter [04400, 05004], lr: 0.083208, loss: 1.8829
2022-08-24 12:34:28 - train: epoch 0027, iter [04500, 05004], lr: 0.083184, loss: 1.7606
2022-08-24 12:35:02 - train: epoch 0027, iter [04600, 05004], lr: 0.083161, loss: 1.9352
2022-08-24 12:35:36 - train: epoch 0027, iter [04700, 05004], lr: 0.083137, loss: 2.0055
2022-08-24 12:36:10 - train: epoch 0027, iter [04800, 05004], lr: 0.083114, loss: 2.0676
2022-08-24 12:36:44 - train: epoch 0027, iter [04900, 05004], lr: 0.083090, loss: 2.0642
2022-08-24 12:37:17 - train: epoch 0027, iter [05000, 05004], lr: 0.083067, loss: 1.7272
2022-08-24 12:37:18 - train: epoch 027, train_loss: 1.9513
2022-08-24 12:38:34 - eval: epoch: 027, acc1: 60.008%, acc5: 83.592%, test_loss: 1.6779, per_image_load_time: 2.314ms, per_image_inference_time: 0.630ms
2022-08-24 12:38:35 - until epoch: 027, best_acc1: 60.008%
2022-08-24 12:38:35 - epoch 028 lr: 0.083065
2022-08-24 12:39:15 - train: epoch 0028, iter [00100, 05004], lr: 0.083042, loss: 1.6904
2022-08-24 12:39:49 - train: epoch 0028, iter [00200, 05004], lr: 0.083018, loss: 1.7897
2022-08-24 12:40:23 - train: epoch 0028, iter [00300, 05004], lr: 0.082995, loss: 1.9375
2022-08-24 12:40:57 - train: epoch 0028, iter [00400, 05004], lr: 0.082971, loss: 1.8582
2022-08-24 12:41:31 - train: epoch 0028, iter [00500, 05004], lr: 0.082948, loss: 1.8853
2022-08-24 12:42:05 - train: epoch 0028, iter [00600, 05004], lr: 0.082924, loss: 2.0782
2022-08-24 12:42:39 - train: epoch 0028, iter [00700, 05004], lr: 0.082900, loss: 2.0591
2022-08-24 12:43:12 - train: epoch 0028, iter [00800, 05004], lr: 0.082877, loss: 1.6220
2022-08-24 12:43:47 - train: epoch 0028, iter [00900, 05004], lr: 0.082853, loss: 1.7379
2022-08-24 12:44:20 - train: epoch 0028, iter [01000, 05004], lr: 0.082829, loss: 2.0206
2022-08-24 12:44:54 - train: epoch 0028, iter [01100, 05004], lr: 0.082806, loss: 1.9249
2022-08-24 12:45:28 - train: epoch 0028, iter [01200, 05004], lr: 0.082782, loss: 1.8755
2022-08-24 12:46:03 - train: epoch 0028, iter [01300, 05004], lr: 0.082758, loss: 1.9306
2022-08-24 12:46:37 - train: epoch 0028, iter [01400, 05004], lr: 0.082735, loss: 2.3297
2022-08-24 12:47:11 - train: epoch 0028, iter [01500, 05004], lr: 0.082711, loss: 1.8734
2022-08-24 12:47:45 - train: epoch 0028, iter [01600, 05004], lr: 0.082687, loss: 1.9984
2022-08-24 12:48:19 - train: epoch 0028, iter [01700, 05004], lr: 0.082663, loss: 1.9219
2022-08-24 12:48:54 - train: epoch 0028, iter [01800, 05004], lr: 0.082640, loss: 1.8483
2022-08-24 12:49:29 - train: epoch 0028, iter [01900, 05004], lr: 0.082616, loss: 1.9746
2022-08-24 12:50:03 - train: epoch 0028, iter [02000, 05004], lr: 0.082592, loss: 2.1408
2022-08-24 12:50:37 - train: epoch 0028, iter [02100, 05004], lr: 0.082568, loss: 1.9812
2022-08-24 12:51:11 - train: epoch 0028, iter [02200, 05004], lr: 0.082544, loss: 1.9726
2022-08-24 12:51:46 - train: epoch 0028, iter [02300, 05004], lr: 0.082521, loss: 2.2205
2022-08-24 12:52:20 - train: epoch 0028, iter [02400, 05004], lr: 0.082497, loss: 2.2349
2022-08-24 12:52:54 - train: epoch 0028, iter [02500, 05004], lr: 0.082473, loss: 1.9519
2022-08-24 12:53:28 - train: epoch 0028, iter [02600, 05004], lr: 0.082449, loss: 1.7955
2022-08-24 12:54:02 - train: epoch 0028, iter [02700, 05004], lr: 0.082425, loss: 1.8939
2022-08-24 12:54:36 - train: epoch 0028, iter [02800, 05004], lr: 0.082401, loss: 1.8798
2022-08-24 12:55:11 - train: epoch 0028, iter [02900, 05004], lr: 0.082377, loss: 2.1006
2022-08-24 12:55:45 - train: epoch 0028, iter [03000, 05004], lr: 0.082353, loss: 2.0308
2022-08-24 12:56:20 - train: epoch 0028, iter [03100, 05004], lr: 0.082329, loss: 2.1061
2022-08-24 12:56:53 - train: epoch 0028, iter [03200, 05004], lr: 0.082305, loss: 1.9672
2022-08-24 12:57:27 - train: epoch 0028, iter [03300, 05004], lr: 0.082282, loss: 1.9103
2022-08-24 12:58:00 - train: epoch 0028, iter [03400, 05004], lr: 0.082258, loss: 1.9195
2022-08-24 12:58:34 - train: epoch 0028, iter [03500, 05004], lr: 0.082234, loss: 1.9320
2022-08-24 12:59:07 - train: epoch 0028, iter [03600, 05004], lr: 0.082210, loss: 1.7629
2022-08-24 12:59:40 - train: epoch 0028, iter [03700, 05004], lr: 0.082186, loss: 1.8813
2022-08-24 13:00:14 - train: epoch 0028, iter [03800, 05004], lr: 0.082161, loss: 1.5719
2022-08-24 13:00:48 - train: epoch 0028, iter [03900, 05004], lr: 0.082137, loss: 1.8690
2022-08-24 13:01:21 - train: epoch 0028, iter [04000, 05004], lr: 0.082113, loss: 2.0699
2022-08-24 13:01:55 - train: epoch 0028, iter [04100, 05004], lr: 0.082089, loss: 2.1104
2022-08-24 13:02:28 - train: epoch 0028, iter [04200, 05004], lr: 0.082065, loss: 2.1238
2022-08-24 13:03:02 - train: epoch 0028, iter [04300, 05004], lr: 0.082041, loss: 1.8617
2022-08-24 13:03:35 - train: epoch 0028, iter [04400, 05004], lr: 0.082017, loss: 1.9050
2022-08-24 13:04:09 - train: epoch 0028, iter [04500, 05004], lr: 0.081993, loss: 1.9067
2022-08-24 13:04:43 - train: epoch 0028, iter [04600, 05004], lr: 0.081969, loss: 2.0942
2022-08-24 13:05:17 - train: epoch 0028, iter [04700, 05004], lr: 0.081945, loss: 1.9305
2022-08-24 13:05:51 - train: epoch 0028, iter [04800, 05004], lr: 0.081921, loss: 1.8421
2022-08-24 13:06:24 - train: epoch 0028, iter [04900, 05004], lr: 0.081896, loss: 1.8384
2022-08-24 13:06:57 - train: epoch 0028, iter [05000, 05004], lr: 0.081872, loss: 1.8511
2022-08-24 13:06:58 - train: epoch 028, train_loss: 1.9390
2022-08-24 13:08:14 - eval: epoch: 028, acc1: 60.478%, acc5: 83.524%, test_loss: 1.6648, per_image_load_time: 2.289ms, per_image_inference_time: 0.586ms
2022-08-24 13:08:14 - until epoch: 028, best_acc1: 60.478%
2022-08-24 13:08:14 - epoch 029 lr: 0.081871
2022-08-24 13:08:54 - train: epoch 0029, iter [00100, 05004], lr: 0.081847, loss: 2.0503
2022-08-24 13:09:28 - train: epoch 0029, iter [00200, 05004], lr: 0.081823, loss: 1.8807
2022-08-24 13:10:02 - train: epoch 0029, iter [00300, 05004], lr: 0.081799, loss: 2.0861
2022-08-24 13:10:36 - train: epoch 0029, iter [00400, 05004], lr: 0.081774, loss: 1.9142
2022-08-24 13:11:10 - train: epoch 0029, iter [00500, 05004], lr: 0.081750, loss: 1.9067
2022-08-24 13:11:44 - train: epoch 0029, iter [00600, 05004], lr: 0.081726, loss: 2.1606
2022-08-24 13:12:18 - train: epoch 0029, iter [00700, 05004], lr: 0.081702, loss: 1.5406
2022-08-24 13:12:53 - train: epoch 0029, iter [00800, 05004], lr: 0.081677, loss: 2.1065
2022-08-24 13:13:26 - train: epoch 0029, iter [00900, 05004], lr: 0.081653, loss: 1.8220
2022-08-24 13:14:00 - train: epoch 0029, iter [01000, 05004], lr: 0.081629, loss: 1.7801
2022-08-24 13:14:33 - train: epoch 0029, iter [01100, 05004], lr: 0.081604, loss: 1.8486
2022-08-24 13:15:08 - train: epoch 0029, iter [01200, 05004], lr: 0.081580, loss: 1.9958
2022-08-24 13:15:42 - train: epoch 0029, iter [01300, 05004], lr: 0.081556, loss: 1.8622
2022-08-24 13:16:16 - train: epoch 0029, iter [01400, 05004], lr: 0.081531, loss: 2.0545
2022-08-24 13:16:50 - train: epoch 0029, iter [01500, 05004], lr: 0.081507, loss: 1.9789
2022-08-24 13:17:24 - train: epoch 0029, iter [01600, 05004], lr: 0.081483, loss: 1.8954
2022-08-24 13:17:57 - train: epoch 0029, iter [01700, 05004], lr: 0.081458, loss: 2.0841
2022-08-24 13:18:31 - train: epoch 0029, iter [01800, 05004], lr: 0.081434, loss: 2.0155
2022-08-24 13:19:05 - train: epoch 0029, iter [01900, 05004], lr: 0.081409, loss: 1.7798
2022-08-24 13:19:38 - train: epoch 0029, iter [02000, 05004], lr: 0.081385, loss: 2.1418
2022-08-24 13:20:12 - train: epoch 0029, iter [02100, 05004], lr: 0.081361, loss: 1.8731
2022-08-24 13:20:46 - train: epoch 0029, iter [02200, 05004], lr: 0.081336, loss: 1.9805
2022-08-24 13:21:19 - train: epoch 0029, iter [02300, 05004], lr: 0.081312, loss: 2.0026
2022-08-24 13:21:54 - train: epoch 0029, iter [02400, 05004], lr: 0.081287, loss: 1.8407
2022-08-24 13:22:27 - train: epoch 0029, iter [02500, 05004], lr: 0.081263, loss: 1.7635
2022-08-24 13:23:01 - train: epoch 0029, iter [02600, 05004], lr: 0.081238, loss: 2.0205
2022-08-24 13:23:34 - train: epoch 0029, iter [02700, 05004], lr: 0.081214, loss: 1.8524
2022-08-24 13:24:08 - train: epoch 0029, iter [02800, 05004], lr: 0.081189, loss: 1.8045
2022-08-24 13:24:42 - train: epoch 0029, iter [02900, 05004], lr: 0.081165, loss: 1.9244
2022-08-24 13:25:15 - train: epoch 0029, iter [03000, 05004], lr: 0.081140, loss: 1.8124
2022-08-24 13:25:49 - train: epoch 0029, iter [03100, 05004], lr: 0.081115, loss: 1.8746
2022-08-24 13:26:23 - train: epoch 0029, iter [03200, 05004], lr: 0.081091, loss: 2.1190
2022-08-24 13:26:57 - train: epoch 0029, iter [03300, 05004], lr: 0.081066, loss: 1.9291
2022-08-24 13:27:30 - train: epoch 0029, iter [03400, 05004], lr: 0.081042, loss: 1.7524
2022-08-24 13:28:04 - train: epoch 0029, iter [03500, 05004], lr: 0.081017, loss: 2.0944
2022-08-24 13:28:38 - train: epoch 0029, iter [03600, 05004], lr: 0.080992, loss: 1.9491
2022-08-24 13:29:11 - train: epoch 0029, iter [03700, 05004], lr: 0.080968, loss: 1.8121
2022-08-24 13:29:45 - train: epoch 0029, iter [03800, 05004], lr: 0.080943, loss: 1.9345
2022-08-24 13:30:19 - train: epoch 0029, iter [03900, 05004], lr: 0.080918, loss: 1.7415
2022-08-24 13:30:53 - train: epoch 0029, iter [04000, 05004], lr: 0.080894, loss: 1.8386
2022-08-24 13:31:27 - train: epoch 0029, iter [04100, 05004], lr: 0.080869, loss: 1.7583
2022-08-24 13:32:01 - train: epoch 0029, iter [04200, 05004], lr: 0.080844, loss: 1.8709
2022-08-24 13:32:34 - train: epoch 0029, iter [04300, 05004], lr: 0.080820, loss: 2.0148
2022-08-24 13:33:08 - train: epoch 0029, iter [04400, 05004], lr: 0.080795, loss: 1.8679
2022-08-24 13:33:42 - train: epoch 0029, iter [04500, 05004], lr: 0.080770, loss: 2.0573
2022-08-24 13:34:16 - train: epoch 0029, iter [04600, 05004], lr: 0.080745, loss: 2.0918
2022-08-24 13:34:49 - train: epoch 0029, iter [04700, 05004], lr: 0.080721, loss: 1.7113
2022-08-24 13:35:22 - train: epoch 0029, iter [04800, 05004], lr: 0.080696, loss: 1.9760
2022-08-24 13:35:56 - train: epoch 0029, iter [04900, 05004], lr: 0.080671, loss: 2.2582
2022-08-24 13:36:29 - train: epoch 0029, iter [05000, 05004], lr: 0.080646, loss: 1.7424
2022-08-24 13:36:31 - train: epoch 029, train_loss: 1.9298
2022-08-24 13:37:46 - eval: epoch: 029, acc1: 60.066%, acc5: 83.554%, test_loss: 1.6670, per_image_load_time: 2.327ms, per_image_inference_time: 0.605ms
2022-08-24 13:37:46 - until epoch: 029, best_acc1: 60.478%
2022-08-24 13:37:46 - epoch 030 lr: 0.080645
2022-08-24 13:38:26 - train: epoch 0030, iter [00100, 05004], lr: 0.080621, loss: 2.0588
2022-08-24 13:39:00 - train: epoch 0030, iter [00200, 05004], lr: 0.080596, loss: 1.9017
2022-08-24 13:39:34 - train: epoch 0030, iter [00300, 05004], lr: 0.080571, loss: 1.8127
2022-08-24 13:40:07 - train: epoch 0030, iter [00400, 05004], lr: 0.080546, loss: 1.5948
2022-08-24 13:40:42 - train: epoch 0030, iter [00500, 05004], lr: 0.080521, loss: 2.2010
2022-08-24 13:41:16 - train: epoch 0030, iter [00600, 05004], lr: 0.080496, loss: 1.7696
2022-08-24 13:41:50 - train: epoch 0030, iter [00700, 05004], lr: 0.080471, loss: 1.8468
2022-08-24 13:42:24 - train: epoch 0030, iter [00800, 05004], lr: 0.080447, loss: 2.1459
2022-08-24 13:42:58 - train: epoch 0030, iter [00900, 05004], lr: 0.080422, loss: 1.9501
2022-08-24 13:43:32 - train: epoch 0030, iter [01000, 05004], lr: 0.080397, loss: 1.6005
2022-08-24 13:44:06 - train: epoch 0030, iter [01100, 05004], lr: 0.080372, loss: 1.7562
2022-08-24 13:44:40 - train: epoch 0030, iter [01200, 05004], lr: 0.080347, loss: 1.9915
2022-08-24 13:45:14 - train: epoch 0030, iter [01300, 05004], lr: 0.080322, loss: 1.6276
2022-08-24 13:45:48 - train: epoch 0030, iter [01400, 05004], lr: 0.080297, loss: 1.8887
2022-08-24 13:46:22 - train: epoch 0030, iter [01500, 05004], lr: 0.080272, loss: 1.8982
2022-08-24 13:46:56 - train: epoch 0030, iter [01600, 05004], lr: 0.080247, loss: 1.8779
2022-08-24 13:47:29 - train: epoch 0030, iter [01700, 05004], lr: 0.080222, loss: 2.1131
2022-08-24 13:48:02 - train: epoch 0030, iter [01800, 05004], lr: 0.080197, loss: 1.9321
2022-08-24 13:48:37 - train: epoch 0030, iter [01900, 05004], lr: 0.080172, loss: 2.0432
2022-08-24 13:49:10 - train: epoch 0030, iter [02000, 05004], lr: 0.080147, loss: 1.9798
2022-08-24 13:49:45 - train: epoch 0030, iter [02100, 05004], lr: 0.080122, loss: 2.0212
2022-08-24 13:50:18 - train: epoch 0030, iter [02200, 05004], lr: 0.080097, loss: 1.7784
2022-08-24 13:50:52 - train: epoch 0030, iter [02300, 05004], lr: 0.080072, loss: 1.8840
2022-08-24 13:51:26 - train: epoch 0030, iter [02400, 05004], lr: 0.080047, loss: 2.1395
2022-08-24 13:52:00 - train: epoch 0030, iter [02500, 05004], lr: 0.080022, loss: 1.9545
2022-08-24 13:52:33 - train: epoch 0030, iter [02600, 05004], lr: 0.079996, loss: 1.9659
2022-08-24 13:53:08 - train: epoch 0030, iter [02700, 05004], lr: 0.079971, loss: 1.7834
2022-08-24 13:53:41 - train: epoch 0030, iter [02800, 05004], lr: 0.079946, loss: 1.8994
2022-08-24 13:54:16 - train: epoch 0030, iter [02900, 05004], lr: 0.079921, loss: 2.0445
2022-08-24 13:54:49 - train: epoch 0030, iter [03000, 05004], lr: 0.079896, loss: 2.0030
2022-08-24 13:55:23 - train: epoch 0030, iter [03100, 05004], lr: 0.079871, loss: 1.8545
2022-08-24 13:55:57 - train: epoch 0030, iter [03200, 05004], lr: 0.079846, loss: 1.8409
2022-08-24 13:56:30 - train: epoch 0030, iter [03300, 05004], lr: 0.079820, loss: 2.1406
2022-08-24 13:57:04 - train: epoch 0030, iter [03400, 05004], lr: 0.079795, loss: 1.9353
2022-08-24 13:57:38 - train: epoch 0030, iter [03500, 05004], lr: 0.079770, loss: 2.0113
2022-08-24 13:58:12 - train: epoch 0030, iter [03600, 05004], lr: 0.079745, loss: 1.7611
2022-08-24 13:58:46 - train: epoch 0030, iter [03700, 05004], lr: 0.079719, loss: 2.0163
2022-08-24 13:59:20 - train: epoch 0030, iter [03800, 05004], lr: 0.079694, loss: 2.0688
2022-08-24 13:59:54 - train: epoch 0030, iter [03900, 05004], lr: 0.079669, loss: 1.9874
2022-08-24 14:00:28 - train: epoch 0030, iter [04000, 05004], lr: 0.079644, loss: 1.7978
2022-08-24 14:01:02 - train: epoch 0030, iter [04100, 05004], lr: 0.079618, loss: 1.8616
2022-08-24 14:01:35 - train: epoch 0030, iter [04200, 05004], lr: 0.079593, loss: 2.0313
2022-08-24 14:02:09 - train: epoch 0030, iter [04300, 05004], lr: 0.079568, loss: 1.9577
2022-08-24 14:02:43 - train: epoch 0030, iter [04400, 05004], lr: 0.079542, loss: 2.0127
2022-08-24 14:03:18 - train: epoch 0030, iter [04500, 05004], lr: 0.079517, loss: 2.0529
2022-08-24 14:03:51 - train: epoch 0030, iter [04600, 05004], lr: 0.079492, loss: 1.7375
2022-08-24 14:04:25 - train: epoch 0030, iter [04700, 05004], lr: 0.079466, loss: 1.9141
2022-08-24 14:04:59 - train: epoch 0030, iter [04800, 05004], lr: 0.079441, loss: 1.8752
2022-08-24 14:05:33 - train: epoch 0030, iter [04900, 05004], lr: 0.079416, loss: 2.0576
2022-08-24 14:06:06 - train: epoch 0030, iter [05000, 05004], lr: 0.079390, loss: 2.0297
2022-08-24 14:06:07 - train: epoch 030, train_loss: 1.9237
2022-08-24 14:07:22 - eval: epoch: 030, acc1: 60.050%, acc5: 83.642%, test_loss: 1.6676, per_image_load_time: 2.304ms, per_image_inference_time: 0.572ms
2022-08-24 14:07:22 - until epoch: 030, best_acc1: 60.478%
2022-08-24 14:07:22 - epoch 031 lr: 0.079389
2022-08-24 14:08:02 - train: epoch 0031, iter [00100, 05004], lr: 0.079364, loss: 2.0225
2022-08-24 14:08:37 - train: epoch 0031, iter [00200, 05004], lr: 0.079338, loss: 1.6896
2022-08-24 14:09:11 - train: epoch 0031, iter [00300, 05004], lr: 0.079313, loss: 1.8347
2022-08-24 14:09:44 - train: epoch 0031, iter [00400, 05004], lr: 0.079288, loss: 1.9625
2022-08-24 14:10:18 - train: epoch 0031, iter [00500, 05004], lr: 0.079262, loss: 1.7465
2022-08-24 14:10:52 - train: epoch 0031, iter [00600, 05004], lr: 0.079237, loss: 1.8954
2022-08-24 14:11:26 - train: epoch 0031, iter [00700, 05004], lr: 0.079211, loss: 1.9083
2022-08-24 14:12:00 - train: epoch 0031, iter [00800, 05004], lr: 0.079186, loss: 1.8674
2022-08-24 14:12:33 - train: epoch 0031, iter [00900, 05004], lr: 0.079160, loss: 1.8630
2022-08-24 14:13:07 - train: epoch 0031, iter [01000, 05004], lr: 0.079135, loss: 2.0959
2022-08-24 14:13:41 - train: epoch 0031, iter [01100, 05004], lr: 0.079109, loss: 2.0885
2022-08-24 14:14:15 - train: epoch 0031, iter [01200, 05004], lr: 0.079084, loss: 1.9210
2022-08-24 14:14:48 - train: epoch 0031, iter [01300, 05004], lr: 0.079058, loss: 1.5952
2022-08-24 14:15:23 - train: epoch 0031, iter [01400, 05004], lr: 0.079033, loss: 1.9923
2022-08-24 14:15:57 - train: epoch 0031, iter [01500, 05004], lr: 0.079007, loss: 2.0396
2022-08-24 14:16:30 - train: epoch 0031, iter [01600, 05004], lr: 0.078981, loss: 1.8215
2022-08-24 14:17:04 - train: epoch 0031, iter [01700, 05004], lr: 0.078956, loss: 1.7958
2022-08-24 14:17:38 - train: epoch 0031, iter [01800, 05004], lr: 0.078930, loss: 1.8453
2022-08-24 14:18:11 - train: epoch 0031, iter [01900, 05004], lr: 0.078905, loss: 1.8867
2022-08-24 14:18:45 - train: epoch 0031, iter [02000, 05004], lr: 0.078879, loss: 2.0063
2022-08-24 14:19:19 - train: epoch 0031, iter [02100, 05004], lr: 0.078853, loss: 1.7274
2022-08-24 14:19:53 - train: epoch 0031, iter [02200, 05004], lr: 0.078828, loss: 1.8289
2022-08-24 14:20:27 - train: epoch 0031, iter [02300, 05004], lr: 0.078802, loss: 1.7626
2022-08-24 14:21:00 - train: epoch 0031, iter [02400, 05004], lr: 0.078776, loss: 1.9628
2022-08-24 14:21:35 - train: epoch 0031, iter [02500, 05004], lr: 0.078751, loss: 1.7705
2022-08-24 14:22:08 - train: epoch 0031, iter [02600, 05004], lr: 0.078725, loss: 1.8449
2022-08-24 14:22:42 - train: epoch 0031, iter [02700, 05004], lr: 0.078699, loss: 1.8944
2022-08-24 14:23:16 - train: epoch 0031, iter [02800, 05004], lr: 0.078674, loss: 2.1528
2022-08-24 14:23:50 - train: epoch 0031, iter [02900, 05004], lr: 0.078648, loss: 2.0022
2022-08-24 14:24:24 - train: epoch 0031, iter [03000, 05004], lr: 0.078622, loss: 2.0595
2022-08-24 14:24:57 - train: epoch 0031, iter [03100, 05004], lr: 0.078596, loss: 1.9007
2022-08-24 14:25:31 - train: epoch 0031, iter [03200, 05004], lr: 0.078571, loss: 2.0941
2022-08-24 14:26:04 - train: epoch 0031, iter [03300, 05004], lr: 0.078545, loss: 1.9645
2022-08-24 14:26:39 - train: epoch 0031, iter [03400, 05004], lr: 0.078519, loss: 2.0164
2022-08-24 14:27:13 - train: epoch 0031, iter [03500, 05004], lr: 0.078493, loss: 2.0878
2022-08-24 14:27:47 - train: epoch 0031, iter [03600, 05004], lr: 0.078468, loss: 1.9532
2022-08-24 14:28:20 - train: epoch 0031, iter [03700, 05004], lr: 0.078442, loss: 1.8065
2022-08-24 14:28:54 - train: epoch 0031, iter [03800, 05004], lr: 0.078416, loss: 1.8153
2022-08-24 14:29:28 - train: epoch 0031, iter [03900, 05004], lr: 0.078390, loss: 1.9884
2022-08-24 14:30:02 - train: epoch 0031, iter [04000, 05004], lr: 0.078364, loss: 1.8504
2022-08-24 14:30:36 - train: epoch 0031, iter [04100, 05004], lr: 0.078338, loss: 1.9193
2022-08-24 14:31:10 - train: epoch 0031, iter [04200, 05004], lr: 0.078313, loss: 1.9594
2022-08-24 14:31:44 - train: epoch 0031, iter [04300, 05004], lr: 0.078287, loss: 1.9223
2022-08-24 14:32:18 - train: epoch 0031, iter [04400, 05004], lr: 0.078261, loss: 2.0624
2022-08-24 14:32:52 - train: epoch 0031, iter [04500, 05004], lr: 0.078235, loss: 2.0589
2022-08-24 14:33:26 - train: epoch 0031, iter [04600, 05004], lr: 0.078209, loss: 2.0850
2022-08-24 14:34:00 - train: epoch 0031, iter [04700, 05004], lr: 0.078183, loss: 2.0213
2022-08-24 14:34:34 - train: epoch 0031, iter [04800, 05004], lr: 0.078157, loss: 1.8790
2022-08-24 14:35:08 - train: epoch 0031, iter [04900, 05004], lr: 0.078131, loss: 1.7953
2022-08-24 14:35:40 - train: epoch 0031, iter [05000, 05004], lr: 0.078105, loss: 1.8534
2022-08-24 14:35:42 - train: epoch 031, train_loss: 1.9115
2022-08-24 14:36:57 - eval: epoch: 031, acc1: 60.992%, acc5: 84.500%, test_loss: 1.6217, per_image_load_time: 2.363ms, per_image_inference_time: 0.590ms
2022-08-24 14:36:58 - until epoch: 031, best_acc1: 60.992%
2022-08-24 14:36:58 - epoch 032 lr: 0.078104
2022-08-24 14:37:37 - train: epoch 0032, iter [00100, 05004], lr: 0.078078, loss: 1.7565
2022-08-24 14:38:12 - train: epoch 0032, iter [00200, 05004], lr: 0.078052, loss: 1.9426
2022-08-24 14:38:45 - train: epoch 0032, iter [00300, 05004], lr: 0.078026, loss: 1.8986
2022-08-24 14:39:20 - train: epoch 0032, iter [00400, 05004], lr: 0.078000, loss: 2.0345
2022-08-24 14:39:53 - train: epoch 0032, iter [00500, 05004], lr: 0.077974, loss: 1.9549
2022-08-24 14:40:27 - train: epoch 0032, iter [00600, 05004], lr: 0.077948, loss: 1.9381
2022-08-24 14:41:01 - train: epoch 0032, iter [00700, 05004], lr: 0.077922, loss: 1.8749
2022-08-24 14:41:35 - train: epoch 0032, iter [00800, 05004], lr: 0.077896, loss: 1.8452
2022-08-24 14:42:09 - train: epoch 0032, iter [00900, 05004], lr: 0.077870, loss: 1.8566
2022-08-24 14:42:43 - train: epoch 0032, iter [01000, 05004], lr: 0.077844, loss: 1.9660
2022-08-24 14:43:17 - train: epoch 0032, iter [01100, 05004], lr: 0.077818, loss: 1.9793
2022-08-24 14:43:51 - train: epoch 0032, iter [01200, 05004], lr: 0.077792, loss: 1.9741
2022-08-24 14:44:24 - train: epoch 0032, iter [01300, 05004], lr: 0.077766, loss: 1.7963
2022-08-24 14:44:58 - train: epoch 0032, iter [01400, 05004], lr: 0.077740, loss: 2.2237
2022-08-24 14:45:32 - train: epoch 0032, iter [01500, 05004], lr: 0.077713, loss: 1.9335
2022-08-24 14:46:06 - train: epoch 0032, iter [01600, 05004], lr: 0.077687, loss: 2.0585
2022-08-24 14:46:39 - train: epoch 0032, iter [01700, 05004], lr: 0.077661, loss: 2.0755
2022-08-24 14:47:14 - train: epoch 0032, iter [01800, 05004], lr: 0.077635, loss: 2.1361
2022-08-24 14:47:47 - train: epoch 0032, iter [01900, 05004], lr: 0.077609, loss: 1.7358
2022-08-24 14:48:21 - train: epoch 0032, iter [02000, 05004], lr: 0.077583, loss: 1.9553
2022-08-24 14:48:55 - train: epoch 0032, iter [02100, 05004], lr: 0.077557, loss: 1.7673
2022-08-24 14:49:29 - train: epoch 0032, iter [02200, 05004], lr: 0.077530, loss: 1.7845
2022-08-24 14:50:01 - train: epoch 0032, iter [02300, 05004], lr: 0.077504, loss: 1.9630
2022-08-24 14:50:36 - train: epoch 0032, iter [02400, 05004], lr: 0.077478, loss: 1.8084
2022-08-24 14:51:09 - train: epoch 0032, iter [02500, 05004], lr: 0.077452, loss: 2.0960
2022-08-24 14:51:43 - train: epoch 0032, iter [02600, 05004], lr: 0.077425, loss: 1.6491
2022-08-24 14:52:16 - train: epoch 0032, iter [02700, 05004], lr: 0.077399, loss: 1.8986
2022-08-24 14:52:50 - train: epoch 0032, iter [02800, 05004], lr: 0.077373, loss: 2.0087
2022-08-24 14:53:24 - train: epoch 0032, iter [02900, 05004], lr: 0.077347, loss: 1.7521
2022-08-24 14:53:58 - train: epoch 0032, iter [03000, 05004], lr: 0.077320, loss: 1.6827
2022-08-24 14:54:31 - train: epoch 0032, iter [03100, 05004], lr: 0.077294, loss: 2.0050
2022-08-24 14:55:05 - train: epoch 0032, iter [03200, 05004], lr: 0.077268, loss: 2.0320
2022-08-24 14:55:39 - train: epoch 0032, iter [03300, 05004], lr: 0.077241, loss: 1.7640
2022-08-24 14:56:12 - train: epoch 0032, iter [03400, 05004], lr: 0.077215, loss: 1.6977
2022-08-24 14:56:46 - train: epoch 0032, iter [03500, 05004], lr: 0.077189, loss: 1.9463
2022-08-24 14:57:20 - train: epoch 0032, iter [03600, 05004], lr: 0.077162, loss: 1.9206
2022-08-24 14:57:54 - train: epoch 0032, iter [03700, 05004], lr: 0.077136, loss: 2.0575
2022-08-24 14:58:27 - train: epoch 0032, iter [03800, 05004], lr: 0.077110, loss: 1.7338
2022-08-24 14:59:01 - train: epoch 0032, iter [03900, 05004], lr: 0.077083, loss: 1.9238
2022-08-24 14:59:35 - train: epoch 0032, iter [04000, 05004], lr: 0.077057, loss: 1.8031
2022-08-24 15:00:08 - train: epoch 0032, iter [04100, 05004], lr: 0.077031, loss: 1.7826
2022-08-24 15:00:42 - train: epoch 0032, iter [04200, 05004], lr: 0.077004, loss: 1.7625
2022-08-24 15:01:16 - train: epoch 0032, iter [04300, 05004], lr: 0.076978, loss: 2.1829
2022-08-24 15:01:49 - train: epoch 0032, iter [04400, 05004], lr: 0.076951, loss: 1.8973
2022-08-24 15:02:24 - train: epoch 0032, iter [04500, 05004], lr: 0.076925, loss: 2.0879
2022-08-24 15:02:57 - train: epoch 0032, iter [04600, 05004], lr: 0.076898, loss: 1.8482
2022-08-24 15:03:31 - train: epoch 0032, iter [04700, 05004], lr: 0.076872, loss: 2.0992
2022-08-24 15:04:05 - train: epoch 0032, iter [04800, 05004], lr: 0.076845, loss: 1.7144
2022-08-24 15:04:39 - train: epoch 0032, iter [04900, 05004], lr: 0.076819, loss: 2.0037
2022-08-24 15:05:12 - train: epoch 0032, iter [05000, 05004], lr: 0.076792, loss: 1.9200
2022-08-24 15:05:13 - train: epoch 032, train_loss: 1.9021
2022-08-24 15:06:29 - eval: epoch: 032, acc1: 61.354%, acc5: 84.424%, test_loss: 1.6078, per_image_load_time: 2.358ms, per_image_inference_time: 0.574ms
2022-08-24 15:06:29 - until epoch: 032, best_acc1: 61.354%
2022-08-24 15:06:29 - epoch 033 lr: 0.076791
2022-08-24 15:07:09 - train: epoch 0033, iter [00100, 05004], lr: 0.076765, loss: 1.7475
2022-08-24 15:07:42 - train: epoch 0033, iter [00200, 05004], lr: 0.076738, loss: 1.8958
2022-08-24 15:08:15 - train: epoch 0033, iter [00300, 05004], lr: 0.076712, loss: 1.7286
2022-08-24 15:08:49 - train: epoch 0033, iter [00400, 05004], lr: 0.076685, loss: 1.6501
2022-08-24 15:09:24 - train: epoch 0033, iter [00500, 05004], lr: 0.076659, loss: 1.9666
2022-08-24 15:09:58 - train: epoch 0033, iter [00600, 05004], lr: 0.076632, loss: 1.8267
2022-08-24 15:10:32 - train: epoch 0033, iter [00700, 05004], lr: 0.076606, loss: 1.9002
2022-08-24 15:11:05 - train: epoch 0033, iter [00800, 05004], lr: 0.076579, loss: 2.0077
2022-08-24 15:11:38 - train: epoch 0033, iter [00900, 05004], lr: 0.076552, loss: 2.0349
2022-08-24 15:12:12 - train: epoch 0033, iter [01000, 05004], lr: 0.076526, loss: 1.9109
2022-08-24 15:12:45 - train: epoch 0033, iter [01100, 05004], lr: 0.076499, loss: 1.7573
2022-08-24 15:13:20 - train: epoch 0033, iter [01200, 05004], lr: 0.076473, loss: 1.8384
2022-08-24 15:13:53 - train: epoch 0033, iter [01300, 05004], lr: 0.076446, loss: 1.7992
2022-08-24 15:14:27 - train: epoch 0033, iter [01400, 05004], lr: 0.076419, loss: 2.0186
2022-08-24 15:15:01 - train: epoch 0033, iter [01500, 05004], lr: 0.076393, loss: 2.2558
2022-08-24 15:15:34 - train: epoch 0033, iter [01600, 05004], lr: 0.076366, loss: 1.9421
2022-08-24 15:16:08 - train: epoch 0033, iter [01700, 05004], lr: 0.076339, loss: 1.8128
2022-08-24 15:16:41 - train: epoch 0033, iter [01800, 05004], lr: 0.076313, loss: 2.0859
2022-08-24 15:17:16 - train: epoch 0033, iter [01900, 05004], lr: 0.076286, loss: 2.0054
2022-08-24 15:17:49 - train: epoch 0033, iter [02000, 05004], lr: 0.076259, loss: 1.7809
2022-08-24 15:18:23 - train: epoch 0033, iter [02100, 05004], lr: 0.076232, loss: 1.8729
2022-08-24 15:18:57 - train: epoch 0033, iter [02200, 05004], lr: 0.076206, loss: 1.8661
2022-08-24 15:19:30 - train: epoch 0033, iter [02300, 05004], lr: 0.076179, loss: 1.7148
2022-08-24 15:20:03 - train: epoch 0033, iter [02400, 05004], lr: 0.076152, loss: 2.0318
2022-08-24 15:20:38 - train: epoch 0033, iter [02500, 05004], lr: 0.076125, loss: 1.8954
2022-08-24 15:21:12 - train: epoch 0033, iter [02600, 05004], lr: 0.076099, loss: 1.7713
2022-08-24 15:21:46 - train: epoch 0033, iter [02700, 05004], lr: 0.076072, loss: 2.0682
2022-08-24 15:22:19 - train: epoch 0033, iter [02800, 05004], lr: 0.076045, loss: 2.0012
2022-08-24 15:22:53 - train: epoch 0033, iter [02900, 05004], lr: 0.076018, loss: 2.0269
2022-08-24 15:23:27 - train: epoch 0033, iter [03000, 05004], lr: 0.075992, loss: 1.8542
2022-08-24 15:24:01 - train: epoch 0033, iter [03100, 05004], lr: 0.075965, loss: 2.0691
2022-08-24 15:24:34 - train: epoch 0033, iter [03200, 05004], lr: 0.075938, loss: 1.7635
2022-08-24 15:25:08 - train: epoch 0033, iter [03300, 05004], lr: 0.075911, loss: 1.9902
2022-08-24 15:25:41 - train: epoch 0033, iter [03400, 05004], lr: 0.075884, loss: 1.8989
2022-08-24 15:26:15 - train: epoch 0033, iter [03500, 05004], lr: 0.075857, loss: 1.9657
2022-08-24 15:26:49 - train: epoch 0033, iter [03600, 05004], lr: 0.075830, loss: 2.1149
2022-08-24 15:27:22 - train: epoch 0033, iter [03700, 05004], lr: 0.075804, loss: 1.9034
2022-08-24 15:27:56 - train: epoch 0033, iter [03800, 05004], lr: 0.075777, loss: 1.8871
2022-08-24 15:28:30 - train: epoch 0033, iter [03900, 05004], lr: 0.075750, loss: 1.9041
2022-08-24 15:29:04 - train: epoch 0033, iter [04000, 05004], lr: 0.075723, loss: 1.9623
2022-08-24 15:29:38 - train: epoch 0033, iter [04100, 05004], lr: 0.075696, loss: 1.9808
2022-08-24 15:30:11 - train: epoch 0033, iter [04200, 05004], lr: 0.075669, loss: 1.9205
2022-08-24 15:30:45 - train: epoch 0033, iter [04300, 05004], lr: 0.075642, loss: 2.0902
2022-08-24 15:31:18 - train: epoch 0033, iter [04400, 05004], lr: 0.075615, loss: 1.9271
2022-08-24 15:31:52 - train: epoch 0033, iter [04500, 05004], lr: 0.075588, loss: 2.0627
2022-08-24 15:32:26 - train: epoch 0033, iter [04600, 05004], lr: 0.075561, loss: 1.8530
2022-08-24 15:33:00 - train: epoch 0033, iter [04700, 05004], lr: 0.075534, loss: 1.7727
2022-08-24 15:33:33 - train: epoch 0033, iter [04800, 05004], lr: 0.075507, loss: 2.2038
2022-08-24 15:34:08 - train: epoch 0033, iter [04900, 05004], lr: 0.075480, loss: 1.6345
2022-08-24 15:34:40 - train: epoch 0033, iter [05000, 05004], lr: 0.075453, loss: 1.8329
2022-08-24 15:34:42 - train: epoch 033, train_loss: 1.8892
2022-08-24 15:35:58 - eval: epoch: 033, acc1: 60.966%, acc5: 84.242%, test_loss: 1.6189, per_image_load_time: 2.354ms, per_image_inference_time: 0.599ms
2022-08-24 15:35:58 - until epoch: 033, best_acc1: 61.354%
2022-08-24 15:35:58 - epoch 034 lr: 0.075452
2022-08-24 15:36:38 - train: epoch 0034, iter [00100, 05004], lr: 0.075425, loss: 1.8738
2022-08-24 15:37:12 - train: epoch 0034, iter [00200, 05004], lr: 0.075398, loss: 1.7684
2022-08-24 15:37:45 - train: epoch 0034, iter [00300, 05004], lr: 0.075371, loss: 1.7931
2022-08-24 15:38:19 - train: epoch 0034, iter [00400, 05004], lr: 0.075344, loss: 1.5482
2022-08-24 15:38:53 - train: epoch 0034, iter [00500, 05004], lr: 0.075317, loss: 1.8424
2022-08-24 15:39:27 - train: epoch 0034, iter [00600, 05004], lr: 0.075290, loss: 2.1132
2022-08-24 15:40:01 - train: epoch 0034, iter [00700, 05004], lr: 0.075263, loss: 1.8941
2022-08-24 15:40:35 - train: epoch 0034, iter [00800, 05004], lr: 0.075236, loss: 1.7096
2022-08-24 15:41:08 - train: epoch 0034, iter [00900, 05004], lr: 0.075208, loss: 1.8337
2022-08-24 15:41:42 - train: epoch 0034, iter [01000, 05004], lr: 0.075181, loss: 1.7694
2022-08-24 15:42:16 - train: epoch 0034, iter [01100, 05004], lr: 0.075154, loss: 1.8505
2022-08-24 15:42:49 - train: epoch 0034, iter [01200, 05004], lr: 0.075127, loss: 1.8299
2022-08-24 15:43:23 - train: epoch 0034, iter [01300, 05004], lr: 0.075100, loss: 1.7900
2022-08-24 15:43:57 - train: epoch 0034, iter [01400, 05004], lr: 0.075073, loss: 1.9175
2022-08-24 15:44:30 - train: epoch 0034, iter [01500, 05004], lr: 0.075046, loss: 1.8582
2022-08-24 15:45:05 - train: epoch 0034, iter [01600, 05004], lr: 0.075018, loss: 1.7850
2022-08-24 15:45:38 - train: epoch 0034, iter [01700, 05004], lr: 0.074991, loss: 1.7506
2022-08-24 15:46:12 - train: epoch 0034, iter [01800, 05004], lr: 0.074964, loss: 2.1386
2022-08-24 15:46:46 - train: epoch 0034, iter [01900, 05004], lr: 0.074937, loss: 2.0591
2022-08-24 15:47:20 - train: epoch 0034, iter [02000, 05004], lr: 0.074910, loss: 2.0212
2022-08-24 15:47:54 - train: epoch 0034, iter [02100, 05004], lr: 0.074882, loss: 2.0412
2022-08-24 15:48:28 - train: epoch 0034, iter [02200, 05004], lr: 0.074855, loss: 1.7262
2022-08-24 15:49:02 - train: epoch 0034, iter [02300, 05004], lr: 0.074828, loss: 1.9408
2022-08-24 15:49:35 - train: epoch 0034, iter [02400, 05004], lr: 0.074801, loss: 1.8056
2022-08-24 15:50:09 - train: epoch 0034, iter [02500, 05004], lr: 0.074773, loss: 1.9442
2022-08-24 15:50:43 - train: epoch 0034, iter [02600, 05004], lr: 0.074746, loss: 1.8960
2022-08-24 15:51:17 - train: epoch 0034, iter [02700, 05004], lr: 0.074719, loss: 1.8185
2022-08-24 15:51:51 - train: epoch 0034, iter [02800, 05004], lr: 0.074692, loss: 1.6476
2022-08-24 15:52:24 - train: epoch 0034, iter [02900, 05004], lr: 0.074664, loss: 1.6059
2022-08-24 15:52:58 - train: epoch 0034, iter [03000, 05004], lr: 0.074637, loss: 1.6851
2022-08-24 15:53:32 - train: epoch 0034, iter [03100, 05004], lr: 0.074610, loss: 1.7487
2022-08-24 15:54:06 - train: epoch 0034, iter [03200, 05004], lr: 0.074582, loss: 1.8734
2022-08-24 15:54:40 - train: epoch 0034, iter [03300, 05004], lr: 0.074555, loss: 1.8563
2022-08-24 15:55:14 - train: epoch 0034, iter [03400, 05004], lr: 0.074528, loss: 1.9776
2022-08-24 15:55:48 - train: epoch 0034, iter [03500, 05004], lr: 0.074500, loss: 1.7293
2022-08-24 15:56:22 - train: epoch 0034, iter [03600, 05004], lr: 0.074473, loss: 1.7846
2022-08-24 15:56:56 - train: epoch 0034, iter [03700, 05004], lr: 0.074446, loss: 1.7691
2022-08-24 15:57:30 - train: epoch 0034, iter [03800, 05004], lr: 0.074418, loss: 1.8108
2022-08-24 15:58:03 - train: epoch 0034, iter [03900, 05004], lr: 0.074391, loss: 1.9274
2022-08-24 15:58:37 - train: epoch 0034, iter [04000, 05004], lr: 0.074363, loss: 1.7413
2022-08-24 15:59:11 - train: epoch 0034, iter [04100, 05004], lr: 0.074336, loss: 1.8379
2022-08-24 15:59:45 - train: epoch 0034, iter [04200, 05004], lr: 0.074309, loss: 1.8233
2022-08-24 16:00:18 - train: epoch 0034, iter [04300, 05004], lr: 0.074281, loss: 1.9348
2022-08-24 16:00:52 - train: epoch 0034, iter [04400, 05004], lr: 0.074254, loss: 1.9164
2022-08-24 16:01:26 - train: epoch 0034, iter [04500, 05004], lr: 0.074226, loss: 1.9727
2022-08-24 16:02:00 - train: epoch 0034, iter [04600, 05004], lr: 0.074199, loss: 2.0329
2022-08-24 16:02:34 - train: epoch 0034, iter [04700, 05004], lr: 0.074171, loss: 1.8437
2022-08-24 16:03:07 - train: epoch 0034, iter [04800, 05004], lr: 0.074144, loss: 1.8295
2022-08-24 16:03:41 - train: epoch 0034, iter [04900, 05004], lr: 0.074116, loss: 1.9068
2022-08-24 16:04:14 - train: epoch 0034, iter [05000, 05004], lr: 0.074089, loss: 1.8561
2022-08-24 16:04:16 - train: epoch 034, train_loss: 1.8790
2022-08-24 16:05:31 - eval: epoch: 034, acc1: 61.174%, acc5: 84.628%, test_loss: 1.6116, per_image_load_time: 2.330ms, per_image_inference_time: 0.591ms
2022-08-24 16:05:31 - until epoch: 034, best_acc1: 61.354%
2022-08-24 16:05:31 - epoch 035 lr: 0.074087
2022-08-24 16:06:11 - train: epoch 0035, iter [00100, 05004], lr: 0.074060, loss: 1.6853
2022-08-24 16:06:45 - train: epoch 0035, iter [00200, 05004], lr: 0.074033, loss: 1.7205
2022-08-24 16:07:19 - train: epoch 0035, iter [00300, 05004], lr: 0.074005, loss: 1.9835
2022-08-24 16:07:53 - train: epoch 0035, iter [00400, 05004], lr: 0.073978, loss: 1.6887
2022-08-24 16:08:26 - train: epoch 0035, iter [00500, 05004], lr: 0.073950, loss: 1.9205
2022-08-24 16:09:00 - train: epoch 0035, iter [00600, 05004], lr: 0.073922, loss: 1.8696
2022-08-24 16:09:34 - train: epoch 0035, iter [00700, 05004], lr: 0.073895, loss: 1.8218
2022-08-24 16:10:07 - train: epoch 0035, iter [00800, 05004], lr: 0.073867, loss: 1.8245
2022-08-24 16:10:41 - train: epoch 0035, iter [00900, 05004], lr: 0.073840, loss: 2.0730
2022-08-24 16:11:15 - train: epoch 0035, iter [01000, 05004], lr: 0.073812, loss: 2.0128
2022-08-24 16:11:50 - train: epoch 0035, iter [01100, 05004], lr: 0.073785, loss: 1.9495
2022-08-24 16:12:23 - train: epoch 0035, iter [01200, 05004], lr: 0.073757, loss: 1.9202
2022-08-24 16:12:57 - train: epoch 0035, iter [01300, 05004], lr: 0.073729, loss: 1.9490
2022-08-24 16:13:31 - train: epoch 0035, iter [01400, 05004], lr: 0.073702, loss: 1.8348
2022-08-24 16:14:05 - train: epoch 0035, iter [01500, 05004], lr: 0.073674, loss: 1.9263
2022-08-24 16:14:39 - train: epoch 0035, iter [01600, 05004], lr: 0.073646, loss: 1.8933
2022-08-24 16:15:13 - train: epoch 0035, iter [01700, 05004], lr: 0.073619, loss: 1.7258
2022-08-24 16:15:46 - train: epoch 0035, iter [01800, 05004], lr: 0.073591, loss: 1.9215
2022-08-24 16:16:21 - train: epoch 0035, iter [01900, 05004], lr: 0.073563, loss: 1.8772
2022-08-24 16:16:55 - train: epoch 0035, iter [02000, 05004], lr: 0.073536, loss: 1.9154
2022-08-24 16:17:29 - train: epoch 0035, iter [02100, 05004], lr: 0.073508, loss: 1.8030
2022-08-24 16:18:02 - train: epoch 0035, iter [02200, 05004], lr: 0.073480, loss: 1.9532
2022-08-24 16:18:36 - train: epoch 0035, iter [02300, 05004], lr: 0.073453, loss: 1.7816
2022-08-24 16:19:10 - train: epoch 0035, iter [02400, 05004], lr: 0.073425, loss: 1.9256
2022-08-24 16:19:44 - train: epoch 0035, iter [02500, 05004], lr: 0.073397, loss: 1.7275
2022-08-24 16:20:18 - train: epoch 0035, iter [02600, 05004], lr: 0.073369, loss: 2.0565
2022-08-24 16:20:52 - train: epoch 0035, iter [02700, 05004], lr: 0.073342, loss: 1.9887
2022-08-24 16:21:25 - train: epoch 0035, iter [02800, 05004], lr: 0.073314, loss: 1.7951
2022-08-24 16:21:59 - train: epoch 0035, iter [02900, 05004], lr: 0.073286, loss: 1.8795
2022-08-24 16:22:33 - train: epoch 0035, iter [03000, 05004], lr: 0.073258, loss: 1.9442
2022-08-24 16:23:07 - train: epoch 0035, iter [03100, 05004], lr: 0.073230, loss: 1.8636
2022-08-24 16:23:41 - train: epoch 0035, iter [03200, 05004], lr: 0.073203, loss: 1.8015
2022-08-24 16:24:15 - train: epoch 0035, iter [03300, 05004], lr: 0.073175, loss: 1.7807
2022-08-24 16:24:49 - train: epoch 0035, iter [03400, 05004], lr: 0.073147, loss: 1.8837
2022-08-24 16:25:23 - train: epoch 0035, iter [03500, 05004], lr: 0.073119, loss: 1.6195
2022-08-24 16:25:57 - train: epoch 0035, iter [03600, 05004], lr: 0.073091, loss: 1.9577
2022-08-24 16:26:31 - train: epoch 0035, iter [03700, 05004], lr: 0.073063, loss: 1.7625
2022-08-24 16:27:04 - train: epoch 0035, iter [03800, 05004], lr: 0.073036, loss: 1.8546
2022-08-24 16:27:38 - train: epoch 0035, iter [03900, 05004], lr: 0.073008, loss: 2.0357
2022-08-24 16:28:12 - train: epoch 0035, iter [04000, 05004], lr: 0.072980, loss: 1.6369
2022-08-24 16:28:46 - train: epoch 0035, iter [04100, 05004], lr: 0.072952, loss: 2.1640
2022-08-24 16:29:19 - train: epoch 0035, iter [04200, 05004], lr: 0.072924, loss: 1.7912
2022-08-24 16:29:54 - train: epoch 0035, iter [04300, 05004], lr: 0.072896, loss: 1.8546
2022-08-24 16:30:27 - train: epoch 0035, iter [04400, 05004], lr: 0.072868, loss: 1.9592
2022-08-24 16:31:01 - train: epoch 0035, iter [04500, 05004], lr: 0.072840, loss: 1.9000
2022-08-24 16:31:35 - train: epoch 0035, iter [04600, 05004], lr: 0.072812, loss: 1.7835
2022-08-24 16:32:08 - train: epoch 0035, iter [04700, 05004], lr: 0.072785, loss: 2.1686
2022-08-24 16:32:42 - train: epoch 0035, iter [04800, 05004], lr: 0.072757, loss: 2.0816
2022-08-24 16:33:16 - train: epoch 0035, iter [04900, 05004], lr: 0.072729, loss: 1.9397
2022-08-24 16:33:50 - train: epoch 0035, iter [05000, 05004], lr: 0.072701, loss: 1.7864
2022-08-24 16:33:51 - train: epoch 035, train_loss: 1.8681
2022-08-24 16:35:07 - eval: epoch: 035, acc1: 61.416%, acc5: 84.696%, test_loss: 1.6080, per_image_load_time: 2.336ms, per_image_inference_time: 0.567ms
2022-08-24 16:35:07 - until epoch: 035, best_acc1: 61.416%
2022-08-24 16:35:07 - epoch 036 lr: 0.072699
2022-08-24 16:35:46 - train: epoch 0036, iter [00100, 05004], lr: 0.072672, loss: 1.9710
2022-08-24 16:36:20 - train: epoch 0036, iter [00200, 05004], lr: 0.072644, loss: 1.7048
2022-08-24 16:36:55 - train: epoch 0036, iter [00300, 05004], lr: 0.072616, loss: 1.7216
2022-08-24 16:37:29 - train: epoch 0036, iter [00400, 05004], lr: 0.072588, loss: 1.7918
2022-08-24 16:38:02 - train: epoch 0036, iter [00500, 05004], lr: 0.072560, loss: 1.8787
2022-08-24 16:38:36 - train: epoch 0036, iter [00600, 05004], lr: 0.072532, loss: 1.8002
2022-08-24 16:39:10 - train: epoch 0036, iter [00700, 05004], lr: 0.072504, loss: 1.7040
2022-08-24 16:39:44 - train: epoch 0036, iter [00800, 05004], lr: 0.072475, loss: 1.6543
2022-08-24 16:40:18 - train: epoch 0036, iter [00900, 05004], lr: 0.072447, loss: 1.8290
2022-08-24 16:40:51 - train: epoch 0036, iter [01000, 05004], lr: 0.072419, loss: 1.7955
2022-08-24 16:41:25 - train: epoch 0036, iter [01100, 05004], lr: 0.072391, loss: 1.8359
2022-08-24 16:41:59 - train: epoch 0036, iter [01200, 05004], lr: 0.072363, loss: 1.8490
2022-08-24 16:42:33 - train: epoch 0036, iter [01300, 05004], lr: 0.072335, loss: 1.8353
2022-08-24 16:43:07 - train: epoch 0036, iter [01400, 05004], lr: 0.072307, loss: 2.0418
2022-08-24 16:43:40 - train: epoch 0036, iter [01500, 05004], lr: 0.072279, loss: 1.9623
2022-08-24 16:44:15 - train: epoch 0036, iter [01600, 05004], lr: 0.072251, loss: 1.9957
2022-08-24 16:44:48 - train: epoch 0036, iter [01700, 05004], lr: 0.072223, loss: 1.8786
2022-08-24 16:45:22 - train: epoch 0036, iter [01800, 05004], lr: 0.072195, loss: 1.7440
2022-08-24 16:45:55 - train: epoch 0036, iter [01900, 05004], lr: 0.072167, loss: 1.9710
2022-08-24 16:46:29 - train: epoch 0036, iter [02000, 05004], lr: 0.072138, loss: 1.8310
2022-08-24 16:47:03 - train: epoch 0036, iter [02100, 05004], lr: 0.072110, loss: 1.7646
2022-08-24 16:47:37 - train: epoch 0036, iter [02200, 05004], lr: 0.072082, loss: 1.7498
2022-08-24 16:48:11 - train: epoch 0036, iter [02300, 05004], lr: 0.072054, loss: 1.9185
2022-08-24 16:48:45 - train: epoch 0036, iter [02400, 05004], lr: 0.072026, loss: 2.0014
2022-08-24 16:49:18 - train: epoch 0036, iter [02500, 05004], lr: 0.071998, loss: 1.6563
2022-08-24 16:49:52 - train: epoch 0036, iter [02600, 05004], lr: 0.071969, loss: 1.9853
2022-08-24 16:50:25 - train: epoch 0036, iter [02700, 05004], lr: 0.071941, loss: 1.8196
2022-08-24 16:51:00 - train: epoch 0036, iter [02800, 05004], lr: 0.071913, loss: 1.6304
2022-08-24 16:51:33 - train: epoch 0036, iter [02900, 05004], lr: 0.071885, loss: 1.6313
2022-08-24 16:52:07 - train: epoch 0036, iter [03000, 05004], lr: 0.071856, loss: 2.0100
2022-08-24 16:52:41 - train: epoch 0036, iter [03100, 05004], lr: 0.071828, loss: 1.9313
2022-08-24 16:53:14 - train: epoch 0036, iter [03200, 05004], lr: 0.071800, loss: 1.8421
2022-08-24 16:53:48 - train: epoch 0036, iter [03300, 05004], lr: 0.071772, loss: 1.7544
2022-08-24 16:54:22 - train: epoch 0036, iter [03400, 05004], lr: 0.071743, loss: 1.6419
2022-08-24 16:54:57 - train: epoch 0036, iter [03500, 05004], lr: 0.071715, loss: 1.9100
2022-08-24 16:55:31 - train: epoch 0036, iter [03600, 05004], lr: 0.071687, loss: 2.0283
2022-08-24 16:56:05 - train: epoch 0036, iter [03700, 05004], lr: 0.071659, loss: 1.9422
2022-08-24 16:56:38 - train: epoch 0036, iter [03800, 05004], lr: 0.071630, loss: 1.6970
2022-08-24 16:57:12 - train: epoch 0036, iter [03900, 05004], lr: 0.071602, loss: 1.8008
2022-08-24 16:57:46 - train: epoch 0036, iter [04000, 05004], lr: 0.071574, loss: 1.9261
2022-08-24 16:58:20 - train: epoch 0036, iter [04100, 05004], lr: 0.071545, loss: 1.8903
2022-08-24 16:58:54 - train: epoch 0036, iter [04200, 05004], lr: 0.071517, loss: 1.7254
2022-08-24 16:59:28 - train: epoch 0036, iter [04300, 05004], lr: 0.071489, loss: 1.8506
2022-08-24 17:00:02 - train: epoch 0036, iter [04400, 05004], lr: 0.071460, loss: 1.8198
2022-08-24 17:00:36 - train: epoch 0036, iter [04500, 05004], lr: 0.071432, loss: 1.5480
2022-08-24 17:01:10 - train: epoch 0036, iter [04600, 05004], lr: 0.071404, loss: 1.6922
2022-08-24 17:01:44 - train: epoch 0036, iter [04700, 05004], lr: 0.071375, loss: 1.8107
2022-08-24 17:02:17 - train: epoch 0036, iter [04800, 05004], lr: 0.071347, loss: 1.8349
2022-08-24 17:02:51 - train: epoch 0036, iter [04900, 05004], lr: 0.071318, loss: 1.9295
2022-08-24 17:03:24 - train: epoch 0036, iter [05000, 05004], lr: 0.071290, loss: 1.9068
2022-08-24 17:03:25 - train: epoch 036, train_loss: 1.8554
2022-08-24 17:04:40 - eval: epoch: 036, acc1: 61.018%, acc5: 84.440%, test_loss: 1.6231, per_image_load_time: 2.350ms, per_image_inference_time: 0.591ms
2022-08-24 17:04:41 - until epoch: 036, best_acc1: 61.416%
2022-08-24 17:04:41 - epoch 037 lr: 0.071289
2022-08-24 17:05:20 - train: epoch 0037, iter [00100, 05004], lr: 0.071261, loss: 1.5869
2022-08-24 17:05:55 - train: epoch 0037, iter [00200, 05004], lr: 0.071232, loss: 1.3972
2022-08-24 17:06:28 - train: epoch 0037, iter [00300, 05004], lr: 0.071204, loss: 1.8746
2022-08-24 17:07:02 - train: epoch 0037, iter [00400, 05004], lr: 0.071175, loss: 1.9794
2022-08-24 17:07:36 - train: epoch 0037, iter [00500, 05004], lr: 0.071147, loss: 1.8982
2022-08-24 17:08:10 - train: epoch 0037, iter [00600, 05004], lr: 0.071118, loss: 1.9480
2022-08-24 17:08:43 - train: epoch 0037, iter [00700, 05004], lr: 0.071090, loss: 1.8793
2022-08-24 17:09:17 - train: epoch 0037, iter [00800, 05004], lr: 0.071061, loss: 1.8149
2022-08-24 17:09:51 - train: epoch 0037, iter [00900, 05004], lr: 0.071033, loss: 2.1893
2022-08-24 17:10:25 - train: epoch 0037, iter [01000, 05004], lr: 0.071005, loss: 1.8337
2022-08-24 17:10:59 - train: epoch 0037, iter [01100, 05004], lr: 0.070976, loss: 1.7881
2022-08-24 17:11:33 - train: epoch 0037, iter [01200, 05004], lr: 0.070948, loss: 1.8605
2022-08-24 17:12:06 - train: epoch 0037, iter [01300, 05004], lr: 0.070919, loss: 1.8301
2022-08-24 17:12:41 - train: epoch 0037, iter [01400, 05004], lr: 0.070891, loss: 1.8989
2022-08-24 17:13:15 - train: epoch 0037, iter [01500, 05004], lr: 0.070862, loss: 1.7790
2022-08-24 17:13:48 - train: epoch 0037, iter [01600, 05004], lr: 0.070833, loss: 1.6829
2022-08-24 17:14:21 - train: epoch 0037, iter [01700, 05004], lr: 0.070805, loss: 2.0468
2022-08-24 17:14:55 - train: epoch 0037, iter [01800, 05004], lr: 0.070776, loss: 1.9341
2022-08-24 17:15:29 - train: epoch 0037, iter [01900, 05004], lr: 0.070748, loss: 1.9175
2022-08-24 17:16:02 - train: epoch 0037, iter [02000, 05004], lr: 0.070719, loss: 1.8798
2022-08-24 17:16:36 - train: epoch 0037, iter [02100, 05004], lr: 0.070691, loss: 1.8289
2022-08-24 17:17:09 - train: epoch 0037, iter [02200, 05004], lr: 0.070662, loss: 1.7793
2022-08-24 17:17:43 - train: epoch 0037, iter [02300, 05004], lr: 0.070633, loss: 1.6974
2022-08-24 17:18:17 - train: epoch 0037, iter [02400, 05004], lr: 0.070605, loss: 1.8694
2022-08-24 17:18:50 - train: epoch 0037, iter [02500, 05004], lr: 0.070576, loss: 1.7669
2022-08-24 17:19:24 - train: epoch 0037, iter [02600, 05004], lr: 0.070548, loss: 2.0131
2022-08-24 17:19:58 - train: epoch 0037, iter [02700, 05004], lr: 0.070519, loss: 1.9538
2022-08-24 17:20:31 - train: epoch 0037, iter [02800, 05004], lr: 0.070490, loss: 1.9125
2022-08-24 17:21:05 - train: epoch 0037, iter [02900, 05004], lr: 0.070462, loss: 1.9734
2022-08-24 17:21:38 - train: epoch 0037, iter [03000, 05004], lr: 0.070433, loss: 1.9881
2022-08-24 17:22:12 - train: epoch 0037, iter [03100, 05004], lr: 0.070404, loss: 1.7814
2022-08-24 17:22:46 - train: epoch 0037, iter [03200, 05004], lr: 0.070376, loss: 2.0133
2022-08-24 17:23:19 - train: epoch 0037, iter [03300, 05004], lr: 0.070347, loss: 1.7021
2022-08-24 17:23:53 - train: epoch 0037, iter [03400, 05004], lr: 0.070318, loss: 1.6623
2022-08-24 17:24:27 - train: epoch 0037, iter [03500, 05004], lr: 0.070290, loss: 1.8541
2022-08-24 17:25:01 - train: epoch 0037, iter [03600, 05004], lr: 0.070261, loss: 1.9319
2022-08-24 17:25:35 - train: epoch 0037, iter [03700, 05004], lr: 0.070232, loss: 1.8882
2022-08-24 17:26:09 - train: epoch 0037, iter [03800, 05004], lr: 0.070204, loss: 1.7591
2022-08-24 17:26:42 - train: epoch 0037, iter [03900, 05004], lr: 0.070175, loss: 1.9831
2022-08-24 17:27:16 - train: epoch 0037, iter [04000, 05004], lr: 0.070146, loss: 1.7794
2022-08-24 17:27:49 - train: epoch 0037, iter [04100, 05004], lr: 0.070118, loss: 1.7962
2022-08-24 17:28:24 - train: epoch 0037, iter [04200, 05004], lr: 0.070089, loss: 2.1463
2022-08-24 17:28:58 - train: epoch 0037, iter [04300, 05004], lr: 0.070060, loss: 1.9217
2022-08-24 17:29:31 - train: epoch 0037, iter [04400, 05004], lr: 0.070031, loss: 1.7629
2022-08-24 17:30:04 - train: epoch 0037, iter [04500, 05004], lr: 0.070002, loss: 1.7599
2022-08-24 17:30:38 - train: epoch 0037, iter [04600, 05004], lr: 0.069974, loss: 1.8505
2022-08-24 17:31:12 - train: epoch 0037, iter [04700, 05004], lr: 0.069945, loss: 1.8555
2022-08-24 17:31:46 - train: epoch 0037, iter [04800, 05004], lr: 0.069916, loss: 1.8486
2022-08-24 17:32:20 - train: epoch 0037, iter [04900, 05004], lr: 0.069887, loss: 1.8376
2022-08-24 17:32:52 - train: epoch 0037, iter [05000, 05004], lr: 0.069859, loss: 1.8543
2022-08-24 17:32:54 - train: epoch 037, train_loss: 1.8477
2022-08-24 17:34:09 - eval: epoch: 037, acc1: 61.456%, acc5: 84.620%, test_loss: 1.6061, per_image_load_time: 2.338ms, per_image_inference_time: 0.602ms
2022-08-24 17:34:09 - until epoch: 037, best_acc1: 61.456%
2022-08-24 17:34:09 - epoch 038 lr: 0.069857
2022-08-24 17:34:49 - train: epoch 0038, iter [00100, 05004], lr: 0.069829, loss: 1.8980
2022-08-24 17:35:22 - train: epoch 0038, iter [00200, 05004], lr: 0.069800, loss: 1.5788
2022-08-24 17:35:56 - train: epoch 0038, iter [00300, 05004], lr: 0.069771, loss: 1.5863
2022-08-24 17:36:29 - train: epoch 0038, iter [00400, 05004], lr: 0.069742, loss: 1.8068
2022-08-24 17:37:03 - train: epoch 0038, iter [00500, 05004], lr: 0.069713, loss: 1.6276
2022-08-24 17:37:37 - train: epoch 0038, iter [00600, 05004], lr: 0.069684, loss: 1.9621
2022-08-24 17:38:10 - train: epoch 0038, iter [00700, 05004], lr: 0.069656, loss: 1.6930
2022-08-24 17:38:44 - train: epoch 0038, iter [00800, 05004], lr: 0.069627, loss: 1.6922
2022-08-24 17:39:18 - train: epoch 0038, iter [00900, 05004], lr: 0.069598, loss: 1.7410
2022-08-24 17:39:52 - train: epoch 0038, iter [01000, 05004], lr: 0.069569, loss: 2.1680
2022-08-24 17:40:26 - train: epoch 0038, iter [01100, 05004], lr: 0.069540, loss: 1.7901
2022-08-24 17:40:59 - train: epoch 0038, iter [01200, 05004], lr: 0.069511, loss: 1.9320
2022-08-24 17:41:32 - train: epoch 0038, iter [01300, 05004], lr: 0.069482, loss: 1.9044
2022-08-24 17:42:06 - train: epoch 0038, iter [01400, 05004], lr: 0.069453, loss: 1.7679
2022-08-24 17:42:41 - train: epoch 0038, iter [01500, 05004], lr: 0.069424, loss: 1.8984
2022-08-24 17:43:14 - train: epoch 0038, iter [01600, 05004], lr: 0.069395, loss: 1.9622
2022-08-24 17:43:48 - train: epoch 0038, iter [01700, 05004], lr: 0.069367, loss: 2.1053
2022-08-24 17:44:22 - train: epoch 0038, iter [01800, 05004], lr: 0.069338, loss: 1.9350
2022-08-24 17:44:55 - train: epoch 0038, iter [01900, 05004], lr: 0.069309, loss: 1.8471
2022-08-24 17:45:29 - train: epoch 0038, iter [02000, 05004], lr: 0.069280, loss: 1.8356
2022-08-24 17:46:03 - train: epoch 0038, iter [02100, 05004], lr: 0.069251, loss: 1.8890
2022-08-24 17:46:38 - train: epoch 0038, iter [02200, 05004], lr: 0.069222, loss: 1.7777
2022-08-24 17:47:12 - train: epoch 0038, iter [02300, 05004], lr: 0.069193, loss: 1.9815
2022-08-24 17:47:46 - train: epoch 0038, iter [02400, 05004], lr: 0.069164, loss: 2.1674
2022-08-24 17:48:20 - train: epoch 0038, iter [02500, 05004], lr: 0.069135, loss: 1.8380
2022-08-24 17:48:54 - train: epoch 0038, iter [02600, 05004], lr: 0.069106, loss: 1.8424
2022-08-24 17:49:28 - train: epoch 0038, iter [02700, 05004], lr: 0.069077, loss: 2.0796
2022-08-24 17:50:02 - train: epoch 0038, iter [02800, 05004], lr: 0.069048, loss: 2.0606
2022-08-24 17:50:36 - train: epoch 0038, iter [02900, 05004], lr: 0.069019, loss: 2.0130
2022-08-24 17:51:10 - train: epoch 0038, iter [03000, 05004], lr: 0.068990, loss: 1.8674
2022-08-24 17:51:44 - train: epoch 0038, iter [03100, 05004], lr: 0.068961, loss: 1.8484
2022-08-24 17:52:17 - train: epoch 0038, iter [03200, 05004], lr: 0.068932, loss: 1.6082
2022-08-24 17:52:51 - train: epoch 0038, iter [03300, 05004], lr: 0.068903, loss: 1.6771
2022-08-24 17:53:24 - train: epoch 0038, iter [03400, 05004], lr: 0.068873, loss: 1.7518
2022-08-24 17:53:59 - train: epoch 0038, iter [03500, 05004], lr: 0.068844, loss: 1.8703
2022-08-24 17:54:33 - train: epoch 0038, iter [03600, 05004], lr: 0.068815, loss: 1.9083
2022-08-24 17:55:06 - train: epoch 0038, iter [03700, 05004], lr: 0.068786, loss: 1.5579
2022-08-24 17:55:40 - train: epoch 0038, iter [03800, 05004], lr: 0.068757, loss: 1.9171
2022-08-24 17:56:14 - train: epoch 0038, iter [03900, 05004], lr: 0.068728, loss: 1.8156
2022-08-24 17:56:48 - train: epoch 0038, iter [04000, 05004], lr: 0.068699, loss: 1.8243
2022-08-24 17:57:22 - train: epoch 0038, iter [04100, 05004], lr: 0.068670, loss: 1.8522
2022-08-24 17:57:55 - train: epoch 0038, iter [04200, 05004], lr: 0.068641, loss: 1.6013
2022-08-24 17:58:30 - train: epoch 0038, iter [04300, 05004], lr: 0.068612, loss: 2.0443
2022-08-24 17:59:03 - train: epoch 0038, iter [04400, 05004], lr: 0.068582, loss: 1.8948
2022-08-24 17:59:37 - train: epoch 0038, iter [04500, 05004], lr: 0.068553, loss: 1.9107
2022-08-24 18:00:11 - train: epoch 0038, iter [04600, 05004], lr: 0.068524, loss: 2.0345
2022-08-24 18:00:45 - train: epoch 0038, iter [04700, 05004], lr: 0.068495, loss: 1.6732
2022-08-24 18:01:18 - train: epoch 0038, iter [04800, 05004], lr: 0.068466, loss: 1.8967
2022-08-24 18:01:52 - train: epoch 0038, iter [04900, 05004], lr: 0.068437, loss: 1.7718
2022-08-24 18:02:25 - train: epoch 0038, iter [05000, 05004], lr: 0.068407, loss: 1.5551
2022-08-24 18:02:26 - train: epoch 038, train_loss: 1.8368
2022-08-24 18:03:42 - eval: epoch: 038, acc1: 62.398%, acc5: 85.198%, test_loss: 1.5578, per_image_load_time: 2.308ms, per_image_inference_time: 0.586ms
2022-08-24 18:03:42 - until epoch: 038, best_acc1: 62.398%
2022-08-24 18:03:42 - epoch 039 lr: 0.068406
2022-08-24 18:04:21 - train: epoch 0039, iter [00100, 05004], lr: 0.068377, loss: 1.8419
2022-08-24 18:04:55 - train: epoch 0039, iter [00200, 05004], lr: 0.068348, loss: 1.9759
2022-08-24 18:05:29 - train: epoch 0039, iter [00300, 05004], lr: 0.068319, loss: 1.6370
2022-08-24 18:06:02 - train: epoch 0039, iter [00400, 05004], lr: 0.068289, loss: 1.8397
2022-08-24 18:06:36 - train: epoch 0039, iter [00500, 05004], lr: 0.068260, loss: 1.7893
2022-08-24 18:07:10 - train: epoch 0039, iter [00600, 05004], lr: 0.068231, loss: 1.6918
2022-08-24 18:07:44 - train: epoch 0039, iter [00700, 05004], lr: 0.068202, loss: 1.9605
2022-08-24 18:08:19 - train: epoch 0039, iter [00800, 05004], lr: 0.068173, loss: 1.7007
2022-08-24 18:08:53 - train: epoch 0039, iter [00900, 05004], lr: 0.068143, loss: 1.8620
2022-08-24 18:09:26 - train: epoch 0039, iter [01000, 05004], lr: 0.068114, loss: 1.7340
2022-08-24 18:10:00 - train: epoch 0039, iter [01100, 05004], lr: 0.068085, loss: 1.8176
2022-08-24 18:10:34 - train: epoch 0039, iter [01200, 05004], lr: 0.068055, loss: 1.9408
2022-08-24 18:11:08 - train: epoch 0039, iter [01300, 05004], lr: 0.068026, loss: 2.0734
2022-08-24 18:11:42 - train: epoch 0039, iter [01400, 05004], lr: 0.067997, loss: 1.9552
2022-08-24 18:12:15 - train: epoch 0039, iter [01500, 05004], lr: 0.067968, loss: 1.7145
2022-08-24 18:12:50 - train: epoch 0039, iter [01600, 05004], lr: 0.067938, loss: 1.8904
2022-08-24 18:13:24 - train: epoch 0039, iter [01700, 05004], lr: 0.067909, loss: 1.5894
2022-08-24 18:13:58 - train: epoch 0039, iter [01800, 05004], lr: 0.067880, loss: 1.7656
2022-08-24 18:14:31 - train: epoch 0039, iter [01900, 05004], lr: 0.067850, loss: 1.5133
2022-08-24 18:15:06 - train: epoch 0039, iter [02000, 05004], lr: 0.067821, loss: 1.7047
2022-08-24 18:15:41 - train: epoch 0039, iter [02100, 05004], lr: 0.067792, loss: 1.9429
2022-08-24 18:16:14 - train: epoch 0039, iter [02200, 05004], lr: 0.067762, loss: 1.7692
2022-08-24 18:16:49 - train: epoch 0039, iter [02300, 05004], lr: 0.067733, loss: 2.0515
2022-08-24 18:17:22 - train: epoch 0039, iter [02400, 05004], lr: 0.067704, loss: 1.8612
2022-08-24 18:17:56 - train: epoch 0039, iter [02500, 05004], lr: 0.067674, loss: 1.6468
2022-08-24 18:18:29 - train: epoch 0039, iter [02600, 05004], lr: 0.067645, loss: 1.7939
2022-08-24 18:19:04 - train: epoch 0039, iter [02700, 05004], lr: 0.067616, loss: 1.9992
2022-08-24 18:19:38 - train: epoch 0039, iter [02800, 05004], lr: 0.067586, loss: 1.7470
2022-08-24 18:20:12 - train: epoch 0039, iter [02900, 05004], lr: 0.067557, loss: 1.6837
2022-08-24 18:20:46 - train: epoch 0039, iter [03000, 05004], lr: 0.067527, loss: 1.8582
2022-08-24 18:21:19 - train: epoch 0039, iter [03100, 05004], lr: 0.067498, loss: 1.8532
2022-08-24 18:21:54 - train: epoch 0039, iter [03200, 05004], lr: 0.067469, loss: 1.9193
2022-08-24 18:22:27 - train: epoch 0039, iter [03300, 05004], lr: 0.067439, loss: 1.8912
2022-08-24 18:23:01 - train: epoch 0039, iter [03400, 05004], lr: 0.067410, loss: 1.9180
2022-08-24 18:23:35 - train: epoch 0039, iter [03500, 05004], lr: 0.067380, loss: 2.0421
2022-08-24 18:24:09 - train: epoch 0039, iter [03600, 05004], lr: 0.067351, loss: 1.9756
2022-08-24 18:24:43 - train: epoch 0039, iter [03700, 05004], lr: 0.067321, loss: 1.6736
2022-08-24 18:25:17 - train: epoch 0039, iter [03800, 05004], lr: 0.067292, loss: 1.6648
2022-08-24 18:25:50 - train: epoch 0039, iter [03900, 05004], lr: 0.067263, loss: 1.9214
2022-08-24 18:26:24 - train: epoch 0039, iter [04000, 05004], lr: 0.067233, loss: 1.8057
2022-08-24 18:26:58 - train: epoch 0039, iter [04100, 05004], lr: 0.067204, loss: 1.8917
2022-08-24 18:27:32 - train: epoch 0039, iter [04200, 05004], lr: 0.067174, loss: 1.8790
2022-08-24 18:28:06 - train: epoch 0039, iter [04300, 05004], lr: 0.067145, loss: 1.7693
2022-08-24 18:28:40 - train: epoch 0039, iter [04400, 05004], lr: 0.067115, loss: 1.4540
2022-08-24 18:29:14 - train: epoch 0039, iter [04500, 05004], lr: 0.067086, loss: 1.6913
2022-08-24 18:29:48 - train: epoch 0039, iter [04600, 05004], lr: 0.067056, loss: 1.9504
2022-08-24 18:30:21 - train: epoch 0039, iter [04700, 05004], lr: 0.067027, loss: 1.8618
2022-08-24 18:30:55 - train: epoch 0039, iter [04800, 05004], lr: 0.066997, loss: 1.8848
2022-08-24 18:31:29 - train: epoch 0039, iter [04900, 05004], lr: 0.066968, loss: 1.7245
2022-08-24 18:32:02 - train: epoch 0039, iter [05000, 05004], lr: 0.066938, loss: 1.7647
2022-08-24 18:32:03 - train: epoch 039, train_loss: 1.8255
2022-08-24 18:33:19 - eval: epoch: 039, acc1: 61.742%, acc5: 84.866%, test_loss: 1.5810, per_image_load_time: 2.340ms, per_image_inference_time: 0.600ms
2022-08-24 18:33:19 - until epoch: 039, best_acc1: 62.398%
2022-08-24 18:33:19 - epoch 040 lr: 0.066937
2022-08-24 18:33:59 - train: epoch 0040, iter [00100, 05004], lr: 0.066907, loss: 2.0692
2022-08-24 18:34:33 - train: epoch 0040, iter [00200, 05004], lr: 0.066878, loss: 2.0741
2022-08-24 18:35:08 - train: epoch 0040, iter [00300, 05004], lr: 0.066848, loss: 1.8156
2022-08-24 18:35:41 - train: epoch 0040, iter [00400, 05004], lr: 0.066819, loss: 1.9452
2022-08-24 18:36:15 - train: epoch 0040, iter [00500, 05004], lr: 0.066789, loss: 1.6739
2022-08-24 18:36:50 - train: epoch 0040, iter [00600, 05004], lr: 0.066760, loss: 1.9345
2022-08-24 18:37:23 - train: epoch 0040, iter [00700, 05004], lr: 0.066730, loss: 1.7030
2022-08-24 18:37:57 - train: epoch 0040, iter [00800, 05004], lr: 0.066700, loss: 2.0448
2022-08-24 18:38:31 - train: epoch 0040, iter [00900, 05004], lr: 0.066671, loss: 1.6999
2022-08-24 18:39:05 - train: epoch 0040, iter [01000, 05004], lr: 0.066641, loss: 1.5404
2022-08-24 18:39:39 - train: epoch 0040, iter [01100, 05004], lr: 0.066612, loss: 1.8249
2022-08-24 18:40:13 - train: epoch 0040, iter [01200, 05004], lr: 0.066582, loss: 1.7371
2022-08-24 18:40:46 - train: epoch 0040, iter [01300, 05004], lr: 0.066552, loss: 1.6017
2022-08-24 18:41:20 - train: epoch 0040, iter [01400, 05004], lr: 0.066523, loss: 1.7194
2022-08-24 18:41:54 - train: epoch 0040, iter [01500, 05004], lr: 0.066493, loss: 1.9891
2022-08-24 18:42:27 - train: epoch 0040, iter [01600, 05004], lr: 0.066463, loss: 1.7194
2022-08-24 18:43:01 - train: epoch 0040, iter [01700, 05004], lr: 0.066434, loss: 1.9751
2022-08-24 18:43:35 - train: epoch 0040, iter [01800, 05004], lr: 0.066404, loss: 1.6507
2022-08-24 18:44:08 - train: epoch 0040, iter [01900, 05004], lr: 0.066375, loss: 1.7825
2022-08-24 18:44:43 - train: epoch 0040, iter [02000, 05004], lr: 0.066345, loss: 1.9108
2022-08-24 18:45:16 - train: epoch 0040, iter [02100, 05004], lr: 0.066315, loss: 1.6488
2022-08-24 18:45:50 - train: epoch 0040, iter [02200, 05004], lr: 0.066286, loss: 1.6927
2022-08-24 18:46:24 - train: epoch 0040, iter [02300, 05004], lr: 0.066256, loss: 1.7226
2022-08-24 18:46:57 - train: epoch 0040, iter [02400, 05004], lr: 0.066226, loss: 1.9211
2022-08-24 18:47:31 - train: epoch 0040, iter [02500, 05004], lr: 0.066196, loss: 1.9639
2022-08-24 18:48:06 - train: epoch 0040, iter [02600, 05004], lr: 0.066167, loss: 1.6980
2022-08-24 18:48:39 - train: epoch 0040, iter [02700, 05004], lr: 0.066137, loss: 1.9124
2022-08-24 18:49:13 - train: epoch 0040, iter [02800, 05004], lr: 0.066107, loss: 1.8837
2022-08-24 18:49:46 - train: epoch 0040, iter [02900, 05004], lr: 0.066078, loss: 2.0648
2022-08-24 18:50:20 - train: epoch 0040, iter [03000, 05004], lr: 0.066048, loss: 2.0364
2022-08-24 18:50:54 - train: epoch 0040, iter [03100, 05004], lr: 0.066018, loss: 1.6891
2022-08-24 18:51:27 - train: epoch 0040, iter [03200, 05004], lr: 0.065988, loss: 2.0635
2022-08-24 18:52:01 - train: epoch 0040, iter [03300, 05004], lr: 0.065959, loss: 1.8339
2022-08-24 18:52:35 - train: epoch 0040, iter [03400, 05004], lr: 0.065929, loss: 1.8290
2022-08-24 18:53:09 - train: epoch 0040, iter [03500, 05004], lr: 0.065899, loss: 1.8607
2022-08-24 18:53:43 - train: epoch 0040, iter [03600, 05004], lr: 0.065869, loss: 1.7845
2022-08-24 18:54:17 - train: epoch 0040, iter [03700, 05004], lr: 0.065840, loss: 1.8156
2022-08-24 18:54:51 - train: epoch 0040, iter [03800, 05004], lr: 0.065810, loss: 1.7077
2022-08-24 18:55:25 - train: epoch 0040, iter [03900, 05004], lr: 0.065780, loss: 1.8780
2022-08-24 18:55:59 - train: epoch 0040, iter [04000, 05004], lr: 0.065750, loss: 1.9885
2022-08-24 18:56:33 - train: epoch 0040, iter [04100, 05004], lr: 0.065720, loss: 1.7591
2022-08-24 18:57:07 - train: epoch 0040, iter [04200, 05004], lr: 0.065691, loss: 1.6783
2022-08-24 18:57:41 - train: epoch 0040, iter [04300, 05004], lr: 0.065661, loss: 1.8216
2022-08-24 18:58:14 - train: epoch 0040, iter [04400, 05004], lr: 0.065631, loss: 1.9238
2022-08-24 18:58:49 - train: epoch 0040, iter [04500, 05004], lr: 0.065601, loss: 1.5804
2022-08-24 18:59:22 - train: epoch 0040, iter [04600, 05004], lr: 0.065571, loss: 1.8590
2022-08-24 18:59:57 - train: epoch 0040, iter [04700, 05004], lr: 0.065542, loss: 1.8496
2022-08-24 19:00:31 - train: epoch 0040, iter [04800, 05004], lr: 0.065512, loss: 1.7448
2022-08-24 19:01:05 - train: epoch 0040, iter [04900, 05004], lr: 0.065482, loss: 1.9102
2022-08-24 19:01:38 - train: epoch 0040, iter [05000, 05004], lr: 0.065452, loss: 1.8401
2022-08-24 19:01:39 - train: epoch 040, train_loss: 1.8137
2022-08-24 19:02:55 - eval: epoch: 040, acc1: 62.740%, acc5: 85.644%, test_loss: 1.5384, per_image_load_time: 2.281ms, per_image_inference_time: 0.616ms
2022-08-24 19:02:55 - until epoch: 040, best_acc1: 62.740%
2022-08-24 19:02:55 - epoch 041 lr: 0.065451
2022-08-24 19:03:35 - train: epoch 0041, iter [00100, 05004], lr: 0.065421, loss: 2.0367
2022-08-24 19:04:08 - train: epoch 0041, iter [00200, 05004], lr: 0.065391, loss: 2.0234
2022-08-24 19:04:42 - train: epoch 0041, iter [00300, 05004], lr: 0.065361, loss: 1.7702
2022-08-24 19:05:16 - train: epoch 0041, iter [00400, 05004], lr: 0.065331, loss: 1.7357
2022-08-24 19:05:48 - train: epoch 0041, iter [00500, 05004], lr: 0.065302, loss: 1.5415
2022-08-24 19:06:23 - train: epoch 0041, iter [00600, 05004], lr: 0.065272, loss: 2.0097
2022-08-24 19:06:56 - train: epoch 0041, iter [00700, 05004], lr: 0.065242, loss: 1.7614
2022-08-24 19:07:29 - train: epoch 0041, iter [00800, 05004], lr: 0.065212, loss: 1.5525
2022-08-24 19:08:04 - train: epoch 0041, iter [00900, 05004], lr: 0.065182, loss: 1.6132
2022-08-24 19:08:37 - train: epoch 0041, iter [01000, 05004], lr: 0.065152, loss: 2.0217
2022-08-24 19:09:11 - train: epoch 0041, iter [01100, 05004], lr: 0.065122, loss: 1.7381
2022-08-24 19:09:45 - train: epoch 0041, iter [01200, 05004], lr: 0.065092, loss: 1.5874
2022-08-24 19:10:19 - train: epoch 0041, iter [01300, 05004], lr: 0.065062, loss: 1.7891
2022-08-24 19:10:53 - train: epoch 0041, iter [01400, 05004], lr: 0.065032, loss: 1.9459
2022-08-24 19:11:27 - train: epoch 0041, iter [01500, 05004], lr: 0.065002, loss: 1.9745
2022-08-24 19:12:01 - train: epoch 0041, iter [01600, 05004], lr: 0.064972, loss: 1.6219
2022-08-24 19:12:35 - train: epoch 0041, iter [01700, 05004], lr: 0.064942, loss: 1.9707
2022-08-24 19:13:09 - train: epoch 0041, iter [01800, 05004], lr: 0.064912, loss: 1.8653
2022-08-24 19:13:42 - train: epoch 0041, iter [01900, 05004], lr: 0.064883, loss: 1.9341
2022-08-24 19:14:16 - train: epoch 0041, iter [02000, 05004], lr: 0.064853, loss: 1.8114
2022-08-24 19:14:50 - train: epoch 0041, iter [02100, 05004], lr: 0.064823, loss: 1.6498
2022-08-24 19:15:24 - train: epoch 0041, iter [02200, 05004], lr: 0.064793, loss: 1.7000
2022-08-24 19:15:58 - train: epoch 0041, iter [02300, 05004], lr: 0.064763, loss: 1.5613
2022-08-24 19:16:32 - train: epoch 0041, iter [02400, 05004], lr: 0.064733, loss: 1.9250
2022-08-24 19:17:05 - train: epoch 0041, iter [02500, 05004], lr: 0.064703, loss: 1.6100
2022-08-24 19:17:39 - train: epoch 0041, iter [02600, 05004], lr: 0.064673, loss: 1.9307
2022-08-24 19:18:12 - train: epoch 0041, iter [02700, 05004], lr: 0.064643, loss: 1.9916
2022-08-24 19:18:47 - train: epoch 0041, iter [02800, 05004], lr: 0.064613, loss: 1.7895
2022-08-24 19:19:20 - train: epoch 0041, iter [02900, 05004], lr: 0.064583, loss: 1.8878
2022-08-24 19:19:54 - train: epoch 0041, iter [03000, 05004], lr: 0.064553, loss: 1.8965
2022-08-24 19:20:28 - train: epoch 0041, iter [03100, 05004], lr: 0.064522, loss: 1.8552
2022-08-24 19:21:02 - train: epoch 0041, iter [03200, 05004], lr: 0.064492, loss: 1.7225
2022-08-24 19:21:36 - train: epoch 0041, iter [03300, 05004], lr: 0.064462, loss: 1.7012
2022-08-24 19:22:10 - train: epoch 0041, iter [03400, 05004], lr: 0.064432, loss: 1.6227
2022-08-24 19:22:44 - train: epoch 0041, iter [03500, 05004], lr: 0.064402, loss: 1.8361
2022-08-24 19:23:18 - train: epoch 0041, iter [03600, 05004], lr: 0.064372, loss: 1.9188
2022-08-24 19:23:51 - train: epoch 0041, iter [03700, 05004], lr: 0.064342, loss: 1.8575
2022-08-24 19:24:24 - train: epoch 0041, iter [03800, 05004], lr: 0.064312, loss: 1.7191
2022-08-24 19:24:58 - train: epoch 0041, iter [03900, 05004], lr: 0.064282, loss: 1.8291
2022-08-24 19:25:32 - train: epoch 0041, iter [04000, 05004], lr: 0.064252, loss: 1.5933
2022-08-24 19:26:06 - train: epoch 0041, iter [04100, 05004], lr: 0.064222, loss: 1.7694
2022-08-24 19:26:40 - train: epoch 0041, iter [04200, 05004], lr: 0.064192, loss: 1.7108
2022-08-24 19:27:14 - train: epoch 0041, iter [04300, 05004], lr: 0.064162, loss: 1.8915
2022-08-24 19:27:47 - train: epoch 0041, iter [04400, 05004], lr: 0.064132, loss: 1.7448
2022-08-24 19:28:20 - train: epoch 0041, iter [04500, 05004], lr: 0.064101, loss: 1.9011
2022-08-24 19:28:54 - train: epoch 0041, iter [04600, 05004], lr: 0.064071, loss: 1.7742
2022-08-24 19:29:28 - train: epoch 0041, iter [04700, 05004], lr: 0.064041, loss: 1.9938
2022-08-24 19:30:02 - train: epoch 0041, iter [04800, 05004], lr: 0.064011, loss: 1.7902
2022-08-24 19:30:36 - train: epoch 0041, iter [04900, 05004], lr: 0.063981, loss: 1.9309
2022-08-24 19:31:09 - train: epoch 0041, iter [05000, 05004], lr: 0.063951, loss: 1.8361
2022-08-24 19:31:11 - train: epoch 041, train_loss: 1.8053
2022-08-24 19:32:26 - eval: epoch: 041, acc1: 62.576%, acc5: 85.556%, test_loss: 1.5467, per_image_load_time: 2.392ms, per_image_inference_time: 0.578ms
2022-08-24 19:32:27 - until epoch: 041, best_acc1: 62.740%
2022-08-24 19:32:27 - epoch 042 lr: 0.063949
2022-08-24 19:33:06 - train: epoch 0042, iter [00100, 05004], lr: 0.063919, loss: 1.5456
2022-08-24 19:33:41 - train: epoch 0042, iter [00200, 05004], lr: 0.063889, loss: 1.8456
2022-08-24 19:34:14 - train: epoch 0042, iter [00300, 05004], lr: 0.063859, loss: 1.8627
2022-08-24 19:34:48 - train: epoch 0042, iter [00400, 05004], lr: 0.063829, loss: 1.5324
2022-08-24 19:35:22 - train: epoch 0042, iter [00500, 05004], lr: 0.063799, loss: 1.8330
2022-08-24 19:35:56 - train: epoch 0042, iter [00600, 05004], lr: 0.063769, loss: 1.6380
2022-08-24 19:36:30 - train: epoch 0042, iter [00700, 05004], lr: 0.063738, loss: 1.7232
2022-08-24 19:37:04 - train: epoch 0042, iter [00800, 05004], lr: 0.063708, loss: 1.7752
2022-08-24 19:37:39 - train: epoch 0042, iter [00900, 05004], lr: 0.063678, loss: 1.9782
2022-08-24 19:38:12 - train: epoch 0042, iter [01000, 05004], lr: 0.063648, loss: 1.9074
2022-08-24 19:38:46 - train: epoch 0042, iter [01100, 05004], lr: 0.063618, loss: 1.5473
2022-08-24 19:39:20 - train: epoch 0042, iter [01200, 05004], lr: 0.063587, loss: 1.6795
2022-08-24 19:39:54 - train: epoch 0042, iter [01300, 05004], lr: 0.063557, loss: 1.9187
2022-08-24 19:40:28 - train: epoch 0042, iter [01400, 05004], lr: 0.063527, loss: 2.0849
2022-08-24 19:41:01 - train: epoch 0042, iter [01500, 05004], lr: 0.063497, loss: 1.6295
2022-08-24 19:41:35 - train: epoch 0042, iter [01600, 05004], lr: 0.063467, loss: 1.7876
2022-08-24 19:42:09 - train: epoch 0042, iter [01700, 05004], lr: 0.063436, loss: 1.8204
2022-08-24 19:42:42 - train: epoch 0042, iter [01800, 05004], lr: 0.063406, loss: 1.7354
2022-08-24 19:43:15 - train: epoch 0042, iter [01900, 05004], lr: 0.063376, loss: 1.9872
2022-08-24 19:43:48 - train: epoch 0042, iter [02000, 05004], lr: 0.063346, loss: 1.7094
2022-08-24 19:44:21 - train: epoch 0042, iter [02100, 05004], lr: 0.063315, loss: 1.5638
2022-08-24 19:44:55 - train: epoch 0042, iter [02200, 05004], lr: 0.063285, loss: 1.7164
2022-08-24 19:45:28 - train: epoch 0042, iter [02300, 05004], lr: 0.063255, loss: 1.7115
2022-08-24 19:46:02 - train: epoch 0042, iter [02400, 05004], lr: 0.063225, loss: 2.0673
2022-08-24 19:46:35 - train: epoch 0042, iter [02500, 05004], lr: 0.063194, loss: 1.9712
2022-08-24 19:47:09 - train: epoch 0042, iter [02600, 05004], lr: 0.063164, loss: 1.8596
2022-08-24 19:47:42 - train: epoch 0042, iter [02700, 05004], lr: 0.063134, loss: 1.6549
2022-08-24 19:48:16 - train: epoch 0042, iter [02800, 05004], lr: 0.063103, loss: 1.7238
2022-08-24 19:48:49 - train: epoch 0042, iter [02900, 05004], lr: 0.063073, loss: 1.7880
2022-08-24 19:49:23 - train: epoch 0042, iter [03000, 05004], lr: 0.063043, loss: 1.8951
2022-08-24 19:49:56 - train: epoch 0042, iter [03100, 05004], lr: 0.063012, loss: 1.7982
2022-08-24 19:50:28 - train: epoch 0042, iter [03200, 05004], lr: 0.062982, loss: 1.7995
2022-08-24 19:51:02 - train: epoch 0042, iter [03300, 05004], lr: 0.062952, loss: 1.9945
2022-08-24 19:51:35 - train: epoch 0042, iter [03400, 05004], lr: 0.062922, loss: 1.6639
2022-08-24 19:52:07 - train: epoch 0042, iter [03500, 05004], lr: 0.062891, loss: 1.7061
2022-08-24 19:52:40 - train: epoch 0042, iter [03600, 05004], lr: 0.062861, loss: 1.9731
2022-08-24 19:53:13 - train: epoch 0042, iter [03700, 05004], lr: 0.062831, loss: 1.7834
2022-08-24 19:53:46 - train: epoch 0042, iter [03800, 05004], lr: 0.062800, loss: 1.6623
2022-08-24 19:54:20 - train: epoch 0042, iter [03900, 05004], lr: 0.062770, loss: 1.7806
2022-08-24 19:54:53 - train: epoch 0042, iter [04000, 05004], lr: 0.062740, loss: 1.8067
2022-08-24 19:55:26 - train: epoch 0042, iter [04100, 05004], lr: 0.062709, loss: 1.9789
2022-08-24 19:56:00 - train: epoch 0042, iter [04200, 05004], lr: 0.062679, loss: 1.8674
2022-08-24 19:56:33 - train: epoch 0042, iter [04300, 05004], lr: 0.062648, loss: 1.6940
2022-08-24 19:57:07 - train: epoch 0042, iter [04400, 05004], lr: 0.062618, loss: 1.7816
2022-08-24 19:57:40 - train: epoch 0042, iter [04500, 05004], lr: 0.062588, loss: 1.7668
2022-08-24 19:58:13 - train: epoch 0042, iter [04600, 05004], lr: 0.062557, loss: 2.1613
2022-08-24 19:58:47 - train: epoch 0042, iter [04700, 05004], lr: 0.062527, loss: 1.6830
2022-08-24 19:59:20 - train: epoch 0042, iter [04800, 05004], lr: 0.062497, loss: 1.8393
2022-08-24 19:59:53 - train: epoch 0042, iter [04900, 05004], lr: 0.062466, loss: 1.8661
2022-08-24 20:00:25 - train: epoch 0042, iter [05000, 05004], lr: 0.062436, loss: 1.7246
2022-08-24 20:00:27 - train: epoch 042, train_loss: 1.7938
2022-08-24 20:01:42 - eval: epoch: 042, acc1: 62.178%, acc5: 84.900%, test_loss: 1.5787, per_image_load_time: 2.337ms, per_image_inference_time: 0.574ms
2022-08-24 20:01:42 - until epoch: 042, best_acc1: 62.740%
2022-08-24 20:01:42 - epoch 043 lr: 0.062434
2022-08-24 20:02:21 - train: epoch 0043, iter [00100, 05004], lr: 0.062404, loss: 1.7245
2022-08-24 20:02:55 - train: epoch 0043, iter [00200, 05004], lr: 0.062374, loss: 1.8640
2022-08-24 20:03:29 - train: epoch 0043, iter [00300, 05004], lr: 0.062343, loss: 1.5084
2022-08-24 20:04:02 - train: epoch 0043, iter [00400, 05004], lr: 0.062313, loss: 1.6732
2022-08-24 20:04:35 - train: epoch 0043, iter [00500, 05004], lr: 0.062282, loss: 1.7239
2022-08-24 20:05:10 - train: epoch 0043, iter [00600, 05004], lr: 0.062252, loss: 1.7109
2022-08-24 20:05:43 - train: epoch 0043, iter [00700, 05004], lr: 0.062222, loss: 1.9373
2022-08-24 20:06:17 - train: epoch 0043, iter [00800, 05004], lr: 0.062191, loss: 1.9526
2022-08-24 20:06:50 - train: epoch 0043, iter [00900, 05004], lr: 0.062161, loss: 1.7740
2022-08-24 20:07:24 - train: epoch 0043, iter [01000, 05004], lr: 0.062130, loss: 1.9341
2022-08-24 20:07:57 - train: epoch 0043, iter [01100, 05004], lr: 0.062100, loss: 1.7426
2022-08-24 20:08:31 - train: epoch 0043, iter [01200, 05004], lr: 0.062069, loss: 1.7593
2022-08-24 20:09:04 - train: epoch 0043, iter [01300, 05004], lr: 0.062039, loss: 1.9328
2022-08-24 20:09:37 - train: epoch 0043, iter [01400, 05004], lr: 0.062008, loss: 1.7493
2022-08-24 20:10:09 - train: epoch 0043, iter [01500, 05004], lr: 0.061978, loss: 1.6946
2022-08-24 20:10:43 - train: epoch 0043, iter [01600, 05004], lr: 0.061947, loss: 1.8303
2022-08-24 20:11:17 - train: epoch 0043, iter [01700, 05004], lr: 0.061917, loss: 1.8775
2022-08-24 20:11:50 - train: epoch 0043, iter [01800, 05004], lr: 0.061886, loss: 1.8942
2022-08-24 20:12:23 - train: epoch 0043, iter [01900, 05004], lr: 0.061856, loss: 1.7977
2022-08-24 20:12:57 - train: epoch 0043, iter [02000, 05004], lr: 0.061825, loss: 1.5999
2022-08-24 20:13:30 - train: epoch 0043, iter [02100, 05004], lr: 0.061795, loss: 1.7059
2022-08-24 20:14:03 - train: epoch 0043, iter [02200, 05004], lr: 0.061764, loss: 1.8763
2022-08-24 20:14:37 - train: epoch 0043, iter [02300, 05004], lr: 0.061734, loss: 1.9833
2022-08-24 20:15:10 - train: epoch 0043, iter [02400, 05004], lr: 0.061703, loss: 1.6964
2022-08-24 20:15:43 - train: epoch 0043, iter [02500, 05004], lr: 0.061673, loss: 1.9185
2022-08-24 20:16:17 - train: epoch 0043, iter [02600, 05004], lr: 0.061642, loss: 1.4747
2022-08-24 20:16:51 - train: epoch 0043, iter [02700, 05004], lr: 0.061612, loss: 1.8029
2022-08-24 20:17:24 - train: epoch 0043, iter [02800, 05004], lr: 0.061581, loss: 1.6772
2022-08-24 20:17:58 - train: epoch 0043, iter [02900, 05004], lr: 0.061551, loss: 1.7978
2022-08-24 20:18:30 - train: epoch 0043, iter [03000, 05004], lr: 0.061520, loss: 1.9602
2022-08-24 20:19:04 - train: epoch 0043, iter [03100, 05004], lr: 0.061490, loss: 2.0550
2022-08-24 20:19:37 - train: epoch 0043, iter [03200, 05004], lr: 0.061459, loss: 1.8357
2022-08-24 20:20:11 - train: epoch 0043, iter [03300, 05004], lr: 0.061429, loss: 1.8208
2022-08-24 20:20:44 - train: epoch 0043, iter [03400, 05004], lr: 0.061398, loss: 1.8893
2022-08-24 20:21:17 - train: epoch 0043, iter [03500, 05004], lr: 0.061367, loss: 1.7819
2022-08-24 20:21:51 - train: epoch 0043, iter [03600, 05004], lr: 0.061337, loss: 1.7507
2022-08-24 20:22:23 - train: epoch 0043, iter [03700, 05004], lr: 0.061306, loss: 1.7823
2022-08-24 20:22:56 - train: epoch 0043, iter [03800, 05004], lr: 0.061276, loss: 1.9318
2022-08-24 20:23:30 - train: epoch 0043, iter [03900, 05004], lr: 0.061245, loss: 2.0844
2022-08-24 20:24:03 - train: epoch 0043, iter [04000, 05004], lr: 0.061215, loss: 1.8167
2022-08-24 20:24:37 - train: epoch 0043, iter [04100, 05004], lr: 0.061184, loss: 2.0524
2022-08-24 20:25:10 - train: epoch 0043, iter [04200, 05004], lr: 0.061153, loss: 1.9020
2022-08-24 20:25:44 - train: epoch 0043, iter [04300, 05004], lr: 0.061123, loss: 1.8407
2022-08-24 20:26:17 - train: epoch 0043, iter [04400, 05004], lr: 0.061092, loss: 1.7214
2022-08-24 20:26:50 - train: epoch 0043, iter [04500, 05004], lr: 0.061062, loss: 1.7205
2022-08-24 20:27:23 - train: epoch 0043, iter [04600, 05004], lr: 0.061031, loss: 1.7323
2022-08-24 20:27:56 - train: epoch 0043, iter [04700, 05004], lr: 0.061000, loss: 1.7736
2022-08-24 20:28:29 - train: epoch 0043, iter [04800, 05004], lr: 0.060970, loss: 1.8335
2022-08-24 20:29:02 - train: epoch 0043, iter [04900, 05004], lr: 0.060939, loss: 1.6987
2022-08-24 20:29:35 - train: epoch 0043, iter [05000, 05004], lr: 0.060908, loss: 1.6942
2022-08-24 20:29:36 - train: epoch 043, train_loss: 1.7810
2022-08-24 20:30:51 - eval: epoch: 043, acc1: 62.874%, acc5: 85.534%, test_loss: 1.5532, per_image_load_time: 2.337ms, per_image_inference_time: 0.592ms
2022-08-24 20:30:51 - until epoch: 043, best_acc1: 62.874%
2022-08-24 20:30:51 - epoch 044 lr: 0.060907
2022-08-24 20:31:31 - train: epoch 0044, iter [00100, 05004], lr: 0.060877, loss: 1.8486
2022-08-24 20:32:04 - train: epoch 0044, iter [00200, 05004], lr: 0.060846, loss: 1.8976
2022-08-24 20:32:38 - train: epoch 0044, iter [00300, 05004], lr: 0.060815, loss: 1.8058
2022-08-24 20:33:11 - train: epoch 0044, iter [00400, 05004], lr: 0.060785, loss: 1.5019
2022-08-24 20:33:44 - train: epoch 0044, iter [00500, 05004], lr: 0.060754, loss: 1.5895
2022-08-24 20:34:18 - train: epoch 0044, iter [00600, 05004], lr: 0.060723, loss: 1.4870
2022-08-24 20:34:52 - train: epoch 0044, iter [00700, 05004], lr: 0.060693, loss: 1.5118
2022-08-24 20:35:25 - train: epoch 0044, iter [00800, 05004], lr: 0.060662, loss: 1.7308
2022-08-24 20:35:59 - train: epoch 0044, iter [00900, 05004], lr: 0.060631, loss: 1.7283
2022-08-24 20:36:32 - train: epoch 0044, iter [01000, 05004], lr: 0.060601, loss: 1.7594
2022-08-24 20:37:06 - train: epoch 0044, iter [01100, 05004], lr: 0.060570, loss: 1.7553
2022-08-24 20:37:39 - train: epoch 0044, iter [01200, 05004], lr: 0.060539, loss: 1.7030
2022-08-24 20:38:13 - train: epoch 0044, iter [01300, 05004], lr: 0.060509, loss: 1.7233
2022-08-24 20:38:46 - train: epoch 0044, iter [01400, 05004], lr: 0.060478, loss: 1.6872
2022-08-24 20:39:20 - train: epoch 0044, iter [01500, 05004], lr: 0.060447, loss: 1.6794
2022-08-24 20:39:53 - train: epoch 0044, iter [01600, 05004], lr: 0.060416, loss: 1.7110
2022-08-24 20:40:27 - train: epoch 0044, iter [01700, 05004], lr: 0.060386, loss: 1.7545
2022-08-24 20:41:01 - train: epoch 0044, iter [01800, 05004], lr: 0.060355, loss: 1.6873
2022-08-24 20:41:34 - train: epoch 0044, iter [01900, 05004], lr: 0.060324, loss: 1.9899
2022-08-24 20:42:08 - train: epoch 0044, iter [02000, 05004], lr: 0.060294, loss: 1.6168
2022-08-24 20:42:41 - train: epoch 0044, iter [02100, 05004], lr: 0.060263, loss: 1.9552
2022-08-24 20:43:15 - train: epoch 0044, iter [02200, 05004], lr: 0.060232, loss: 1.7309
2022-08-24 20:43:48 - train: epoch 0044, iter [02300, 05004], lr: 0.060201, loss: 2.0038
2022-08-24 20:44:22 - train: epoch 0044, iter [02400, 05004], lr: 0.060171, loss: 1.6544
2022-08-24 20:44:55 - train: epoch 0044, iter [02500, 05004], lr: 0.060140, loss: 1.8267
2022-08-24 20:45:28 - train: epoch 0044, iter [02600, 05004], lr: 0.060109, loss: 1.7788
2022-08-24 20:46:02 - train: epoch 0044, iter [02700, 05004], lr: 0.060078, loss: 1.8226
2022-08-24 20:46:35 - train: epoch 0044, iter [02800, 05004], lr: 0.060048, loss: 1.5107
2022-08-24 20:47:08 - train: epoch 0044, iter [02900, 05004], lr: 0.060017, loss: 1.6145
2022-08-24 20:47:41 - train: epoch 0044, iter [03000, 05004], lr: 0.059986, loss: 1.7292
2022-08-24 20:48:15 - train: epoch 0044, iter [03100, 05004], lr: 0.059955, loss: 1.7965
2022-08-24 20:48:48 - train: epoch 0044, iter [03200, 05004], lr: 0.059925, loss: 1.5304
2022-08-24 20:49:21 - train: epoch 0044, iter [03300, 05004], lr: 0.059894, loss: 1.6691
2022-08-24 20:49:54 - train: epoch 0044, iter [03400, 05004], lr: 0.059863, loss: 2.0723
2022-08-24 20:50:28 - train: epoch 0044, iter [03500, 05004], lr: 0.059832, loss: 1.6270
2022-08-24 20:51:01 - train: epoch 0044, iter [03600, 05004], lr: 0.059802, loss: 2.0569
2022-08-24 20:51:35 - train: epoch 0044, iter [03700, 05004], lr: 0.059771, loss: 1.8155
2022-08-24 20:52:08 - train: epoch 0044, iter [03800, 05004], lr: 0.059740, loss: 1.4345
2022-08-24 20:52:42 - train: epoch 0044, iter [03900, 05004], lr: 0.059709, loss: 1.7849
2022-08-24 20:53:15 - train: epoch 0044, iter [04000, 05004], lr: 0.059678, loss: 1.8267
2022-08-24 20:53:48 - train: epoch 0044, iter [04100, 05004], lr: 0.059648, loss: 1.6934
2022-08-24 20:54:21 - train: epoch 0044, iter [04200, 05004], lr: 0.059617, loss: 1.8712
2022-08-24 20:54:55 - train: epoch 0044, iter [04300, 05004], lr: 0.059586, loss: 1.9781
2022-08-24 20:55:28 - train: epoch 0044, iter [04400, 05004], lr: 0.059555, loss: 1.5060
2022-08-24 20:56:02 - train: epoch 0044, iter [04500, 05004], lr: 0.059524, loss: 1.8385
2022-08-24 20:56:35 - train: epoch 0044, iter [04600, 05004], lr: 0.059494, loss: 1.8356
2022-08-24 20:57:09 - train: epoch 0044, iter [04700, 05004], lr: 0.059463, loss: 1.9120
2022-08-24 20:57:42 - train: epoch 0044, iter [04800, 05004], lr: 0.059432, loss: 2.0135
2022-08-24 20:58:16 - train: epoch 0044, iter [04900, 05004], lr: 0.059401, loss: 1.5718
2022-08-24 20:58:49 - train: epoch 0044, iter [05000, 05004], lr: 0.059370, loss: 1.9231
2022-08-24 20:58:50 - train: epoch 044, train_loss: 1.7717
2022-08-24 21:00:04 - eval: epoch: 044, acc1: 63.014%, acc5: 85.480%, test_loss: 1.5343, per_image_load_time: 2.296ms, per_image_inference_time: 0.557ms
2022-08-24 21:00:04 - until epoch: 044, best_acc1: 63.014%
2022-08-24 21:00:04 - epoch 045 lr: 0.059369
2022-08-24 21:00:43 - train: epoch 0045, iter [00100, 05004], lr: 0.059338, loss: 1.6838
2022-08-24 21:01:17 - train: epoch 0045, iter [00200, 05004], lr: 0.059307, loss: 1.8106
2022-08-24 21:01:51 - train: epoch 0045, iter [00300, 05004], lr: 0.059277, loss: 1.8006
2022-08-24 21:02:24 - train: epoch 0045, iter [00400, 05004], lr: 0.059246, loss: 1.5316
2022-08-24 21:02:58 - train: epoch 0045, iter [00500, 05004], lr: 0.059215, loss: 1.8338
2022-08-24 21:03:31 - train: epoch 0045, iter [00600, 05004], lr: 0.059184, loss: 1.6830
2022-08-24 21:04:05 - train: epoch 0045, iter [00700, 05004], lr: 0.059153, loss: 1.4794
2022-08-24 21:04:39 - train: epoch 0045, iter [00800, 05004], lr: 0.059122, loss: 1.6836
2022-08-24 21:05:13 - train: epoch 0045, iter [00900, 05004], lr: 0.059091, loss: 1.7478
2022-08-24 21:05:47 - train: epoch 0045, iter [01000, 05004], lr: 0.059061, loss: 1.7731
2022-08-24 21:06:21 - train: epoch 0045, iter [01100, 05004], lr: 0.059030, loss: 1.9092
2022-08-24 21:06:54 - train: epoch 0045, iter [01200, 05004], lr: 0.058999, loss: 1.8110
2022-08-24 21:07:27 - train: epoch 0045, iter [01300, 05004], lr: 0.058968, loss: 1.8839
2022-08-24 21:07:59 - train: epoch 0045, iter [01400, 05004], lr: 0.058937, loss: 1.7847
2022-08-24 21:08:32 - train: epoch 0045, iter [01500, 05004], lr: 0.058906, loss: 1.8315
2022-08-24 21:09:06 - train: epoch 0045, iter [01600, 05004], lr: 0.058875, loss: 1.6869
2022-08-24 21:09:39 - train: epoch 0045, iter [01700, 05004], lr: 0.058844, loss: 1.5262
2022-08-24 21:10:13 - train: epoch 0045, iter [01800, 05004], lr: 0.058813, loss: 1.8035
2022-08-24 21:10:47 - train: epoch 0045, iter [01900, 05004], lr: 0.058783, loss: 1.8530
2022-08-24 21:11:20 - train: epoch 0045, iter [02000, 05004], lr: 0.058752, loss: 1.8320
2022-08-24 21:11:54 - train: epoch 0045, iter [02100, 05004], lr: 0.058721, loss: 1.9228
2022-08-24 21:12:27 - train: epoch 0045, iter [02200, 05004], lr: 0.058690, loss: 1.9037
2022-08-24 21:13:00 - train: epoch 0045, iter [02300, 05004], lr: 0.058659, loss: 1.6735
2022-08-24 21:13:34 - train: epoch 0045, iter [02400, 05004], lr: 0.058628, loss: 1.8901
2022-08-24 21:14:08 - train: epoch 0045, iter [02500, 05004], lr: 0.058597, loss: 1.7014
2022-08-24 21:14:41 - train: epoch 0045, iter [02600, 05004], lr: 0.058566, loss: 1.7768
2022-08-24 21:15:16 - train: epoch 0045, iter [02700, 05004], lr: 0.058535, loss: 1.6654
2022-08-24 21:15:49 - train: epoch 0045, iter [02800, 05004], lr: 0.058504, loss: 1.6951
2022-08-24 21:16:23 - train: epoch 0045, iter [02900, 05004], lr: 0.058473, loss: 1.8815
2022-08-24 21:16:56 - train: epoch 0045, iter [03000, 05004], lr: 0.058442, loss: 1.9626
2022-08-24 21:17:30 - train: epoch 0045, iter [03100, 05004], lr: 0.058411, loss: 1.7416
2022-08-24 21:18:04 - train: epoch 0045, iter [03200, 05004], lr: 0.058381, loss: 1.8335
2022-08-24 21:18:37 - train: epoch 0045, iter [03300, 05004], lr: 0.058350, loss: 1.8454
2022-08-24 21:19:10 - train: epoch 0045, iter [03400, 05004], lr: 0.058319, loss: 1.5400
2022-08-24 21:19:44 - train: epoch 0045, iter [03500, 05004], lr: 0.058288, loss: 1.9436
2022-08-24 21:20:17 - train: epoch 0045, iter [03600, 05004], lr: 0.058257, loss: 1.8871
2022-08-24 21:20:51 - train: epoch 0045, iter [03700, 05004], lr: 0.058226, loss: 1.7795
2022-08-24 21:21:24 - train: epoch 0045, iter [03800, 05004], lr: 0.058195, loss: 1.6868
2022-08-24 21:21:57 - train: epoch 0045, iter [03900, 05004], lr: 0.058164, loss: 1.8519
2022-08-24 21:22:30 - train: epoch 0045, iter [04000, 05004], lr: 0.058133, loss: 1.9315
2022-08-24 21:23:04 - train: epoch 0045, iter [04100, 05004], lr: 0.058102, loss: 1.6207
2022-08-24 21:23:38 - train: epoch 0045, iter [04200, 05004], lr: 0.058071, loss: 1.8823
2022-08-24 21:24:12 - train: epoch 0045, iter [04300, 05004], lr: 0.058040, loss: 1.9585
2022-08-24 21:24:45 - train: epoch 0045, iter [04400, 05004], lr: 0.058009, loss: 1.9117
2022-08-24 21:25:19 - train: epoch 0045, iter [04500, 05004], lr: 0.057978, loss: 1.6782
2022-08-24 21:25:53 - train: epoch 0045, iter [04600, 05004], lr: 0.057947, loss: 1.9800
2022-08-24 21:26:26 - train: epoch 0045, iter [04700, 05004], lr: 0.057916, loss: 1.8112
2022-08-24 21:27:00 - train: epoch 0045, iter [04800, 05004], lr: 0.057885, loss: 1.6917
2022-08-24 21:27:33 - train: epoch 0045, iter [04900, 05004], lr: 0.057854, loss: 1.7381
2022-08-24 21:28:06 - train: epoch 0045, iter [05000, 05004], lr: 0.057823, loss: 1.6937
2022-08-24 21:28:08 - train: epoch 045, train_loss: 1.7570
2022-08-24 21:29:23 - eval: epoch: 045, acc1: 62.508%, acc5: 85.454%, test_loss: 1.5467, per_image_load_time: 2.153ms, per_image_inference_time: 0.617ms
2022-08-24 21:29:24 - until epoch: 045, best_acc1: 63.014%
2022-08-24 21:29:24 - epoch 046 lr: 0.057821
2022-08-24 21:30:03 - train: epoch 0046, iter [00100, 05004], lr: 0.057791, loss: 1.4547
2022-08-24 21:30:37 - train: epoch 0046, iter [00200, 05004], lr: 0.057760, loss: 1.6500
2022-08-24 21:31:10 - train: epoch 0046, iter [00300, 05004], lr: 0.057729, loss: 1.7921
2022-08-24 21:31:44 - train: epoch 0046, iter [00400, 05004], lr: 0.057698, loss: 1.8610
2022-08-24 21:32:17 - train: epoch 0046, iter [00500, 05004], lr: 0.057667, loss: 1.5199
2022-08-24 21:32:50 - train: epoch 0046, iter [00600, 05004], lr: 0.057636, loss: 1.7034
2022-08-24 21:33:25 - train: epoch 0046, iter [00700, 05004], lr: 0.057605, loss: 1.6516
2022-08-24 21:33:58 - train: epoch 0046, iter [00800, 05004], lr: 0.057574, loss: 1.9461
2022-08-24 21:34:32 - train: epoch 0046, iter [00900, 05004], lr: 0.057543, loss: 1.8125
2022-08-24 21:35:05 - train: epoch 0046, iter [01000, 05004], lr: 0.057512, loss: 1.5291
2022-08-24 21:35:39 - train: epoch 0046, iter [01100, 05004], lr: 0.057480, loss: 1.7408
2022-08-24 21:36:12 - train: epoch 0046, iter [01200, 05004], lr: 0.057449, loss: 1.7117
2022-08-24 21:36:45 - train: epoch 0046, iter [01300, 05004], lr: 0.057418, loss: 1.7737
2022-08-24 21:37:19 - train: epoch 0046, iter [01400, 05004], lr: 0.057387, loss: 1.9531
2022-08-24 21:37:53 - train: epoch 0046, iter [01500, 05004], lr: 0.057356, loss: 1.6135
2022-08-24 21:38:26 - train: epoch 0046, iter [01600, 05004], lr: 0.057325, loss: 1.7711
2022-08-24 21:39:00 - train: epoch 0046, iter [01700, 05004], lr: 0.057294, loss: 1.5264
2022-08-24 21:39:33 - train: epoch 0046, iter [01800, 05004], lr: 0.057263, loss: 1.7678
2022-08-24 21:40:06 - train: epoch 0046, iter [01900, 05004], lr: 0.057232, loss: 1.5564
2022-08-24 21:40:40 - train: epoch 0046, iter [02000, 05004], lr: 0.057201, loss: 1.7330
2022-08-24 21:41:14 - train: epoch 0046, iter [02100, 05004], lr: 0.057170, loss: 1.8189
2022-08-24 21:41:48 - train: epoch 0046, iter [02200, 05004], lr: 0.057139, loss: 1.5155
2022-08-24 21:42:22 - train: epoch 0046, iter [02300, 05004], lr: 0.057108, loss: 1.9131
2022-08-24 21:42:55 - train: epoch 0046, iter [02400, 05004], lr: 0.057077, loss: 1.7302
2022-08-24 21:43:29 - train: epoch 0046, iter [02500, 05004], lr: 0.057046, loss: 1.7702
2022-08-24 21:44:02 - train: epoch 0046, iter [02600, 05004], lr: 0.057015, loss: 1.9054
2022-08-24 21:44:37 - train: epoch 0046, iter [02700, 05004], lr: 0.056984, loss: 1.5880
2022-08-24 21:45:10 - train: epoch 0046, iter [02800, 05004], lr: 0.056952, loss: 1.5972
2022-08-24 21:45:43 - train: epoch 0046, iter [02900, 05004], lr: 0.056921, loss: 1.7348
2022-08-24 21:46:18 - train: epoch 0046, iter [03000, 05004], lr: 0.056890, loss: 1.5427
2022-08-24 21:46:52 - train: epoch 0046, iter [03100, 05004], lr: 0.056859, loss: 1.5444
2022-08-24 21:47:25 - train: epoch 0046, iter [03200, 05004], lr: 0.056828, loss: 2.0120
2022-08-24 21:47:59 - train: epoch 0046, iter [03300, 05004], lr: 0.056797, loss: 1.8037
2022-08-24 21:48:32 - train: epoch 0046, iter [03400, 05004], lr: 0.056766, loss: 1.7776
2022-08-24 21:49:06 - train: epoch 0046, iter [03500, 05004], lr: 0.056735, loss: 1.7164
2022-08-24 21:49:40 - train: epoch 0046, iter [03600, 05004], lr: 0.056704, loss: 1.5635
2022-08-24 21:50:13 - train: epoch 0046, iter [03700, 05004], lr: 0.056673, loss: 1.6796
2022-08-24 21:50:47 - train: epoch 0046, iter [03800, 05004], lr: 0.056641, loss: 1.9935
2022-08-24 21:51:21 - train: epoch 0046, iter [03900, 05004], lr: 0.056610, loss: 1.7318
2022-08-24 21:51:55 - train: epoch 0046, iter [04000, 05004], lr: 0.056579, loss: 1.6181
2022-08-24 21:52:30 - train: epoch 0046, iter [04100, 05004], lr: 0.056548, loss: 1.9508
2022-08-24 21:53:03 - train: epoch 0046, iter [04200, 05004], lr: 0.056517, loss: 1.6847
2022-08-24 21:53:37 - train: epoch 0046, iter [04300, 05004], lr: 0.056486, loss: 1.9319
2022-08-24 21:54:11 - train: epoch 0046, iter [04400, 05004], lr: 0.056455, loss: 1.8332
2022-08-24 21:54:44 - train: epoch 0046, iter [04500, 05004], lr: 0.056424, loss: 1.6783
2022-08-24 21:55:18 - train: epoch 0046, iter [04600, 05004], lr: 0.056392, loss: 1.8995
2022-08-24 21:55:52 - train: epoch 0046, iter [04700, 05004], lr: 0.056361, loss: 1.6330
2022-08-24 21:56:26 - train: epoch 0046, iter [04800, 05004], lr: 0.056330, loss: 1.7406
2022-08-24 21:57:00 - train: epoch 0046, iter [04900, 05004], lr: 0.056299, loss: 1.8559
2022-08-24 21:57:33 - train: epoch 0046, iter [05000, 05004], lr: 0.056268, loss: 1.7022
2022-08-24 21:57:34 - train: epoch 046, train_loss: 1.7443
2022-08-24 21:58:50 - eval: epoch: 046, acc1: 63.788%, acc5: 85.952%, test_loss: 1.4931, per_image_load_time: 1.909ms, per_image_inference_time: 0.583ms
2022-08-24 21:58:50 - until epoch: 046, best_acc1: 63.788%
2022-08-24 21:58:50 - epoch 047 lr: 0.056266
2022-08-24 21:59:30 - train: epoch 0047, iter [00100, 05004], lr: 0.056236, loss: 1.5922
2022-08-24 22:00:03 - train: epoch 0047, iter [00200, 05004], lr: 0.056204, loss: 1.8519
2022-08-24 22:00:37 - train: epoch 0047, iter [00300, 05004], lr: 0.056173, loss: 1.7460
2022-08-24 22:01:12 - train: epoch 0047, iter [00400, 05004], lr: 0.056142, loss: 1.6531
2022-08-24 22:01:45 - train: epoch 0047, iter [00500, 05004], lr: 0.056111, loss: 1.8549
2022-08-24 22:02:19 - train: epoch 0047, iter [00600, 05004], lr: 0.056080, loss: 1.7582
2022-08-24 22:02:53 - train: epoch 0047, iter [00700, 05004], lr: 0.056049, loss: 1.8889
2022-08-24 22:03:27 - train: epoch 0047, iter [00800, 05004], lr: 0.056017, loss: 1.5746
2022-08-24 22:04:00 - train: epoch 0047, iter [00900, 05004], lr: 0.055986, loss: 1.9231
2022-08-24 22:04:34 - train: epoch 0047, iter [01000, 05004], lr: 0.055955, loss: 1.8306
2022-08-24 22:05:08 - train: epoch 0047, iter [01100, 05004], lr: 0.055924, loss: 1.8043
2022-08-24 22:05:42 - train: epoch 0047, iter [01200, 05004], lr: 0.055893, loss: 1.7067
2022-08-24 22:06:15 - train: epoch 0047, iter [01300, 05004], lr: 0.055862, loss: 1.6975
2022-08-24 22:06:48 - train: epoch 0047, iter [01400, 05004], lr: 0.055830, loss: 1.6736
2022-08-24 22:07:21 - train: epoch 0047, iter [01500, 05004], lr: 0.055799, loss: 1.5798
2022-08-24 22:07:55 - train: epoch 0047, iter [01600, 05004], lr: 0.055768, loss: 1.6947
2022-08-24 22:08:29 - train: epoch 0047, iter [01700, 05004], lr: 0.055737, loss: 1.4796
2022-08-24 22:09:03 - train: epoch 0047, iter [01800, 05004], lr: 0.055706, loss: 1.6917
2022-08-24 22:09:37 - train: epoch 0047, iter [01900, 05004], lr: 0.055675, loss: 1.6521
2022-08-24 22:10:11 - train: epoch 0047, iter [02000, 05004], lr: 0.055643, loss: 1.6536
2022-08-24 22:10:45 - train: epoch 0047, iter [02100, 05004], lr: 0.055612, loss: 1.8493
2022-08-24 22:11:18 - train: epoch 0047, iter [02200, 05004], lr: 0.055581, loss: 1.9195
2022-08-24 22:11:53 - train: epoch 0047, iter [02300, 05004], lr: 0.055550, loss: 1.7139
2022-08-24 22:12:26 - train: epoch 0047, iter [02400, 05004], lr: 0.055519, loss: 1.5972
2022-08-24 22:13:00 - train: epoch 0047, iter [02500, 05004], lr: 0.055487, loss: 1.7828
2022-08-24 22:13:34 - train: epoch 0047, iter [02600, 05004], lr: 0.055456, loss: 2.0294
2022-08-24 22:14:08 - train: epoch 0047, iter [02700, 05004], lr: 0.055425, loss: 1.6910
2022-08-24 22:14:42 - train: epoch 0047, iter [02800, 05004], lr: 0.055394, loss: 1.9446
2022-08-24 22:15:15 - train: epoch 0047, iter [02900, 05004], lr: 0.055363, loss: 1.6960
2022-08-24 22:15:49 - train: epoch 0047, iter [03000, 05004], lr: 0.055331, loss: 1.8320
2022-08-24 22:16:22 - train: epoch 0047, iter [03100, 05004], lr: 0.055300, loss: 1.6523
2022-08-24 22:16:56 - train: epoch 0047, iter [03200, 05004], lr: 0.055269, loss: 2.0138
2022-08-24 22:17:30 - train: epoch 0047, iter [03300, 05004], lr: 0.055238, loss: 1.6281
2022-08-24 22:18:04 - train: epoch 0047, iter [03400, 05004], lr: 0.055206, loss: 1.7431
2022-08-24 22:18:38 - train: epoch 0047, iter [03500, 05004], lr: 0.055175, loss: 1.9711
2022-08-24 22:19:11 - train: epoch 0047, iter [03600, 05004], lr: 0.055144, loss: 1.7198
2022-08-24 22:19:45 - train: epoch 0047, iter [03700, 05004], lr: 0.055113, loss: 1.9868
2022-08-24 22:20:19 - train: epoch 0047, iter [03800, 05004], lr: 0.055082, loss: 1.7428
2022-08-24 22:20:53 - train: epoch 0047, iter [03900, 05004], lr: 0.055050, loss: 1.5565
2022-08-24 22:21:26 - train: epoch 0047, iter [04000, 05004], lr: 0.055019, loss: 1.7451
2022-08-24 22:22:01 - train: epoch 0047, iter [04100, 05004], lr: 0.054988, loss: 1.8263
2022-08-24 22:22:35 - train: epoch 0047, iter [04200, 05004], lr: 0.054957, loss: 1.8180
2022-08-24 22:23:09 - train: epoch 0047, iter [04300, 05004], lr: 0.054925, loss: 1.6399
2022-08-24 22:23:43 - train: epoch 0047, iter [04400, 05004], lr: 0.054894, loss: 1.7507
2022-08-24 22:24:16 - train: epoch 0047, iter [04500, 05004], lr: 0.054863, loss: 1.8094
2022-08-24 22:24:49 - train: epoch 0047, iter [04600, 05004], lr: 0.054832, loss: 1.6896
2022-08-24 22:25:23 - train: epoch 0047, iter [04700, 05004], lr: 0.054800, loss: 1.8323
2022-08-24 22:25:56 - train: epoch 0047, iter [04800, 05004], lr: 0.054769, loss: 1.6954
2022-08-24 22:26:30 - train: epoch 0047, iter [04900, 05004], lr: 0.054738, loss: 1.8645
2022-08-24 22:27:02 - train: epoch 0047, iter [05000, 05004], lr: 0.054707, loss: 1.6136
2022-08-24 22:27:04 - train: epoch 047, train_loss: 1.7359
2022-08-24 22:28:20 - eval: epoch: 047, acc1: 64.116%, acc5: 86.484%, test_loss: 1.4777, per_image_load_time: 1.824ms, per_image_inference_time: 0.601ms
2022-08-24 22:28:20 - until epoch: 047, best_acc1: 64.116%
2022-08-24 22:28:20 - epoch 048 lr: 0.054705
2022-08-24 22:29:00 - train: epoch 0048, iter [00100, 05004], lr: 0.054674, loss: 1.9622
2022-08-24 22:29:34 - train: epoch 0048, iter [00200, 05004], lr: 0.054643, loss: 2.0893
2022-08-24 22:30:07 - train: epoch 0048, iter [00300, 05004], lr: 0.054612, loss: 1.6928
2022-08-24 22:30:41 - train: epoch 0048, iter [00400, 05004], lr: 0.054580, loss: 1.6410
2022-08-24 22:31:16 - train: epoch 0048, iter [00500, 05004], lr: 0.054549, loss: 1.6130
2022-08-24 22:31:49 - train: epoch 0048, iter [00600, 05004], lr: 0.054518, loss: 1.6758
2022-08-24 22:32:23 - train: epoch 0048, iter [00700, 05004], lr: 0.054487, loss: 1.6240
2022-08-24 22:32:57 - train: epoch 0048, iter [00800, 05004], lr: 0.054455, loss: 1.7107
2022-08-24 22:33:30 - train: epoch 0048, iter [00900, 05004], lr: 0.054424, loss: 1.8606
2022-08-24 22:34:04 - train: epoch 0048, iter [01000, 05004], lr: 0.054393, loss: 1.7929
2022-08-24 22:34:39 - train: epoch 0048, iter [01100, 05004], lr: 0.054362, loss: 1.8419
2022-08-24 22:35:13 - train: epoch 0048, iter [01200, 05004], lr: 0.054330, loss: 1.7548
2022-08-24 22:35:47 - train: epoch 0048, iter [01300, 05004], lr: 0.054299, loss: 1.5800
2022-08-24 22:36:21 - train: epoch 0048, iter [01400, 05004], lr: 0.054268, loss: 1.7413
2022-08-24 22:36:54 - train: epoch 0048, iter [01500, 05004], lr: 0.054236, loss: 1.8462
2022-08-24 22:37:29 - train: epoch 0048, iter [01600, 05004], lr: 0.054205, loss: 1.7007
2022-08-24 22:38:03 - train: epoch 0048, iter [01700, 05004], lr: 0.054174, loss: 1.8815
2022-08-24 22:38:36 - train: epoch 0048, iter [01800, 05004], lr: 0.054143, loss: 1.7378
2022-08-24 22:39:10 - train: epoch 0048, iter [01900, 05004], lr: 0.054111, loss: 1.7921
2022-08-24 22:39:44 - train: epoch 0048, iter [02000, 05004], lr: 0.054080, loss: 1.9555
2022-08-24 22:40:18 - train: epoch 0048, iter [02100, 05004], lr: 0.054049, loss: 1.6933
2022-08-24 22:40:52 - train: epoch 0048, iter [02200, 05004], lr: 0.054017, loss: 1.8828
2022-08-24 22:41:26 - train: epoch 0048, iter [02300, 05004], lr: 0.053986, loss: 1.4665
2022-08-24 22:42:00 - train: epoch 0048, iter [02400, 05004], lr: 0.053955, loss: 1.8360
2022-08-24 22:42:33 - train: epoch 0048, iter [02500, 05004], lr: 0.053924, loss: 1.8170
2022-08-24 22:43:07 - train: epoch 0048, iter [02600, 05004], lr: 0.053892, loss: 2.0720
2022-08-24 22:43:41 - train: epoch 0048, iter [02700, 05004], lr: 0.053861, loss: 1.9309
2022-08-24 22:44:16 - train: epoch 0048, iter [02800, 05004], lr: 0.053830, loss: 1.6365
2022-08-24 22:44:50 - train: epoch 0048, iter [02900, 05004], lr: 0.053798, loss: 1.8443
2022-08-24 22:45:24 - train: epoch 0048, iter [03000, 05004], lr: 0.053767, loss: 1.6866
2022-08-24 22:45:58 - train: epoch 0048, iter [03100, 05004], lr: 0.053736, loss: 1.6995
2022-08-24 22:46:32 - train: epoch 0048, iter [03200, 05004], lr: 0.053704, loss: 1.6404
2022-08-24 22:47:05 - train: epoch 0048, iter [03300, 05004], lr: 0.053673, loss: 1.8364
2022-08-24 22:47:39 - train: epoch 0048, iter [03400, 05004], lr: 0.053642, loss: 1.6916
2022-08-24 22:48:13 - train: epoch 0048, iter [03500, 05004], lr: 0.053611, loss: 2.0301
2022-08-24 22:48:47 - train: epoch 0048, iter [03600, 05004], lr: 0.053579, loss: 1.6815
2022-08-24 22:49:21 - train: epoch 0048, iter [03700, 05004], lr: 0.053548, loss: 1.7967
2022-08-24 22:49:54 - train: epoch 0048, iter [03800, 05004], lr: 0.053517, loss: 1.7013
2022-08-24 22:50:29 - train: epoch 0048, iter [03900, 05004], lr: 0.053485, loss: 1.8200
2022-08-24 22:51:02 - train: epoch 0048, iter [04000, 05004], lr: 0.053454, loss: 1.5133
2022-08-24 22:51:36 - train: epoch 0048, iter [04100, 05004], lr: 0.053423, loss: 1.9857
2022-08-24 22:52:10 - train: epoch 0048, iter [04200, 05004], lr: 0.053391, loss: 1.6314
2022-08-24 22:52:43 - train: epoch 0048, iter [04300, 05004], lr: 0.053360, loss: 1.8146
2022-08-24 22:53:17 - train: epoch 0048, iter [04400, 05004], lr: 0.053329, loss: 1.5733
2022-08-24 22:53:51 - train: epoch 0048, iter [04500, 05004], lr: 0.053297, loss: 1.7288
2022-08-24 22:54:25 - train: epoch 0048, iter [04600, 05004], lr: 0.053266, loss: 1.7604
2022-08-24 22:54:59 - train: epoch 0048, iter [04700, 05004], lr: 0.053235, loss: 1.6885
2022-08-24 22:55:33 - train: epoch 0048, iter [04800, 05004], lr: 0.053203, loss: 2.0339
2022-08-24 22:56:07 - train: epoch 0048, iter [04900, 05004], lr: 0.053172, loss: 1.7916
2022-08-24 22:56:40 - train: epoch 0048, iter [05000, 05004], lr: 0.053141, loss: 1.7873
2022-08-24 22:56:41 - train: epoch 048, train_loss: 1.7264
2022-08-24 22:57:56 - eval: epoch: 048, acc1: 64.590%, acc5: 86.526%, test_loss: 1.4603, per_image_load_time: 1.975ms, per_image_inference_time: 0.590ms
2022-08-24 22:57:56 - until epoch: 048, best_acc1: 64.590%
2022-08-24 22:57:56 - epoch 049 lr: 0.053139
2022-08-24 22:58:37 - train: epoch 0049, iter [00100, 05004], lr: 0.053108, loss: 1.9027
2022-08-24 22:59:10 - train: epoch 0049, iter [00200, 05004], lr: 0.053077, loss: 1.6635
2022-08-24 22:59:44 - train: epoch 0049, iter [00300, 05004], lr: 0.053046, loss: 1.7698
2022-08-24 23:00:17 - train: epoch 0049, iter [00400, 05004], lr: 0.053014, loss: 1.8217
2022-08-24 23:00:51 - train: epoch 0049, iter [00500, 05004], lr: 0.052983, loss: 1.6585
2022-08-24 23:01:25 - train: epoch 0049, iter [00600, 05004], lr: 0.052952, loss: 1.8238
2022-08-24 23:01:58 - train: epoch 0049, iter [00700, 05004], lr: 0.052920, loss: 1.7240
2022-08-24 23:02:32 - train: epoch 0049, iter [00800, 05004], lr: 0.052889, loss: 2.0968
2022-08-24 23:03:06 - train: epoch 0049, iter [00900, 05004], lr: 0.052858, loss: 1.6554
2022-08-24 23:03:39 - train: epoch 0049, iter [01000, 05004], lr: 0.052826, loss: 1.8035
2022-08-24 23:04:13 - train: epoch 0049, iter [01100, 05004], lr: 0.052795, loss: 1.5471
2022-08-24 23:04:47 - train: epoch 0049, iter [01200, 05004], lr: 0.052763, loss: 1.4838
2022-08-24 23:05:20 - train: epoch 0049, iter [01300, 05004], lr: 0.052732, loss: 1.8544
2022-08-24 23:05:54 - train: epoch 0049, iter [01400, 05004], lr: 0.052701, loss: 1.8437
2022-08-24 23:06:28 - train: epoch 0049, iter [01500, 05004], lr: 0.052669, loss: 1.6730
2022-08-24 23:07:02 - train: epoch 0049, iter [01600, 05004], lr: 0.052638, loss: 1.8350
2022-08-24 23:07:36 - train: epoch 0049, iter [01700, 05004], lr: 0.052607, loss: 1.7634
2022-08-24 23:08:10 - train: epoch 0049, iter [01800, 05004], lr: 0.052575, loss: 1.5948
2022-08-24 23:08:44 - train: epoch 0049, iter [01900, 05004], lr: 0.052544, loss: 1.6846
2022-08-24 23:09:16 - train: epoch 0049, iter [02000, 05004], lr: 0.052513, loss: 1.6339
2022-08-24 23:09:50 - train: epoch 0049, iter [02100, 05004], lr: 0.052481, loss: 1.6106
2022-08-24 23:10:23 - train: epoch 0049, iter [02200, 05004], lr: 0.052450, loss: 1.6998
2022-08-24 23:10:57 - train: epoch 0049, iter [02300, 05004], lr: 0.052419, loss: 1.7127
2022-08-24 23:11:31 - train: epoch 0049, iter [02400, 05004], lr: 0.052387, loss: 1.8418
2022-08-24 23:12:05 - train: epoch 0049, iter [02500, 05004], lr: 0.052356, loss: 1.7583
2022-08-24 23:12:40 - train: epoch 0049, iter [02600, 05004], lr: 0.052325, loss: 1.8262
2022-08-24 23:13:13 - train: epoch 0049, iter [02700, 05004], lr: 0.052293, loss: 1.6026
2022-08-24 23:13:47 - train: epoch 0049, iter [02800, 05004], lr: 0.052262, loss: 1.6399
2022-08-24 23:14:22 - train: epoch 0049, iter [02900, 05004], lr: 0.052231, loss: 1.8181
2022-08-24 23:14:56 - train: epoch 0049, iter [03000, 05004], lr: 0.052199, loss: 1.8307
2022-08-24 23:15:29 - train: epoch 0049, iter [03100, 05004], lr: 0.052168, loss: 1.8228
2022-08-24 23:16:03 - train: epoch 0049, iter [03200, 05004], lr: 0.052136, loss: 1.7891
2022-08-24 23:16:36 - train: epoch 0049, iter [03300, 05004], lr: 0.052105, loss: 1.6754
2022-08-24 23:17:11 - train: epoch 0049, iter [03400, 05004], lr: 0.052074, loss: 1.8779
2022-08-24 23:17:44 - train: epoch 0049, iter [03500, 05004], lr: 0.052042, loss: 1.8403
2022-08-24 23:18:18 - train: epoch 0049, iter [03600, 05004], lr: 0.052011, loss: 1.8320
2022-08-24 23:18:51 - train: epoch 0049, iter [03700, 05004], lr: 0.051980, loss: 1.6947
2022-08-24 23:19:24 - train: epoch 0049, iter [03800, 05004], lr: 0.051948, loss: 1.9574
2022-08-24 23:19:58 - train: epoch 0049, iter [03900, 05004], lr: 0.051917, loss: 2.0147
2022-08-24 23:20:33 - train: epoch 0049, iter [04000, 05004], lr: 0.051886, loss: 1.6319
2022-08-24 23:21:06 - train: epoch 0049, iter [04100, 05004], lr: 0.051854, loss: 1.5454
2022-08-24 23:21:40 - train: epoch 0049, iter [04200, 05004], lr: 0.051823, loss: 1.7407
2022-08-24 23:22:14 - train: epoch 0049, iter [04300, 05004], lr: 0.051791, loss: 2.0649
2022-08-24 23:22:47 - train: epoch 0049, iter [04400, 05004], lr: 0.051760, loss: 1.7731
2022-08-24 23:23:21 - train: epoch 0049, iter [04500, 05004], lr: 0.051729, loss: 1.6367
2022-08-24 23:23:55 - train: epoch 0049, iter [04600, 05004], lr: 0.051697, loss: 1.8532
2022-08-24 23:24:28 - train: epoch 0049, iter [04700, 05004], lr: 0.051666, loss: 1.8673
2022-08-24 23:25:01 - train: epoch 0049, iter [04800, 05004], lr: 0.051635, loss: 1.5182
2022-08-24 23:25:36 - train: epoch 0049, iter [04900, 05004], lr: 0.051603, loss: 1.5376
2022-08-24 23:26:08 - train: epoch 0049, iter [05000, 05004], lr: 0.051572, loss: 1.5966
2022-08-24 23:26:10 - train: epoch 049, train_loss: 1.7105
2022-08-24 23:27:24 - eval: epoch: 049, acc1: 65.064%, acc5: 86.822%, test_loss: 1.4406, per_image_load_time: 2.087ms, per_image_inference_time: 0.584ms
2022-08-24 23:27:24 - until epoch: 049, best_acc1: 65.064%
2022-08-24 23:27:24 - epoch 050 lr: 0.051570
2022-08-24 23:28:04 - train: epoch 0050, iter [00100, 05004], lr: 0.051539, loss: 1.8011
2022-08-24 23:28:38 - train: epoch 0050, iter [00200, 05004], lr: 0.051508, loss: 1.7470
2022-08-24 23:29:12 - train: epoch 0050, iter [00300, 05004], lr: 0.051476, loss: 1.7394
2022-08-24 23:29:46 - train: epoch 0050, iter [00400, 05004], lr: 0.051445, loss: 1.5881
2022-08-24 23:30:19 - train: epoch 0050, iter [00500, 05004], lr: 0.051414, loss: 1.6781
2022-08-24 23:30:52 - train: epoch 0050, iter [00600, 05004], lr: 0.051382, loss: 1.7086
2022-08-24 23:31:26 - train: epoch 0050, iter [00700, 05004], lr: 0.051351, loss: 1.4945
2022-08-24 23:32:00 - train: epoch 0050, iter [00800, 05004], lr: 0.051320, loss: 1.5290
2022-08-24 23:32:33 - train: epoch 0050, iter [00900, 05004], lr: 0.051288, loss: 1.5987
2022-08-24 23:33:07 - train: epoch 0050, iter [01000, 05004], lr: 0.051257, loss: 1.8963
2022-08-24 23:33:40 - train: epoch 0050, iter [01100, 05004], lr: 0.051225, loss: 1.9390
2022-08-24 23:34:14 - train: epoch 0050, iter [01200, 05004], lr: 0.051194, loss: 1.6431
2022-08-24 23:34:48 - train: epoch 0050, iter [01300, 05004], lr: 0.051163, loss: 1.5266
2022-08-24 23:35:21 - train: epoch 0050, iter [01400, 05004], lr: 0.051131, loss: 1.7319
2022-08-24 23:35:54 - train: epoch 0050, iter [01500, 05004], lr: 0.051100, loss: 1.7360
2022-08-24 23:36:28 - train: epoch 0050, iter [01600, 05004], lr: 0.051068, loss: 1.6493
2022-08-24 23:37:02 - train: epoch 0050, iter [01700, 05004], lr: 0.051037, loss: 1.8280
2022-08-24 23:37:36 - train: epoch 0050, iter [01800, 05004], lr: 0.051006, loss: 1.7219
2022-08-24 23:38:09 - train: epoch 0050, iter [01900, 05004], lr: 0.050974, loss: 1.6521
2022-08-24 23:38:44 - train: epoch 0050, iter [02000, 05004], lr: 0.050943, loss: 1.7056
2022-08-24 23:39:17 - train: epoch 0050, iter [02100, 05004], lr: 0.050912, loss: 1.3502
2022-08-24 23:39:51 - train: epoch 0050, iter [02200, 05004], lr: 0.050880, loss: 1.7454
2022-08-24 23:40:24 - train: epoch 0050, iter [02300, 05004], lr: 0.050849, loss: 1.6155
2022-08-24 23:40:59 - train: epoch 0050, iter [02400, 05004], lr: 0.050817, loss: 1.6384
2022-08-24 23:41:33 - train: epoch 0050, iter [02500, 05004], lr: 0.050786, loss: 1.7050
2022-08-24 23:42:06 - train: epoch 0050, iter [02600, 05004], lr: 0.050755, loss: 1.6138
2022-08-24 23:42:40 - train: epoch 0050, iter [02700, 05004], lr: 0.050723, loss: 1.7851
2022-08-24 23:43:14 - train: epoch 0050, iter [02800, 05004], lr: 0.050692, loss: 1.8209
2022-08-24 23:43:48 - train: epoch 0050, iter [02900, 05004], lr: 0.050660, loss: 1.9579
2022-08-24 23:44:22 - train: epoch 0050, iter [03000, 05004], lr: 0.050629, loss: 1.9279
2022-08-24 23:44:56 - train: epoch 0050, iter [03100, 05004], lr: 0.050598, loss: 1.6513
2022-08-24 23:45:30 - train: epoch 0050, iter [03200, 05004], lr: 0.050566, loss: 1.6348
2022-08-24 23:46:03 - train: epoch 0050, iter [03300, 05004], lr: 0.050535, loss: 1.6186
2022-08-24 23:46:37 - train: epoch 0050, iter [03400, 05004], lr: 0.050504, loss: 1.4921
2022-08-24 23:47:11 - train: epoch 0050, iter [03500, 05004], lr: 0.050472, loss: 1.6505
2022-08-24 23:47:45 - train: epoch 0050, iter [03600, 05004], lr: 0.050441, loss: 1.7378
2022-08-24 23:48:19 - train: epoch 0050, iter [03700, 05004], lr: 0.050409, loss: 1.7803
2022-08-24 23:48:53 - train: epoch 0050, iter [03800, 05004], lr: 0.050378, loss: 1.5577
2022-08-24 23:49:27 - train: epoch 0050, iter [03900, 05004], lr: 0.050347, loss: 1.5209
2022-08-24 23:50:01 - train: epoch 0050, iter [04000, 05004], lr: 0.050315, loss: 1.7726
2022-08-24 23:50:34 - train: epoch 0050, iter [04100, 05004], lr: 0.050284, loss: 1.6675
2022-08-24 23:51:08 - train: epoch 0050, iter [04200, 05004], lr: 0.050252, loss: 1.8035
2022-08-24 23:51:42 - train: epoch 0050, iter [04300, 05004], lr: 0.050221, loss: 1.6062
2022-08-24 23:52:17 - train: epoch 0050, iter [04400, 05004], lr: 0.050190, loss: 1.6383
2022-08-24 23:52:50 - train: epoch 0050, iter [04500, 05004], lr: 0.050158, loss: 1.7588
2022-08-24 23:53:24 - train: epoch 0050, iter [04600, 05004], lr: 0.050127, loss: 1.7641
2022-08-24 23:53:58 - train: epoch 0050, iter [04700, 05004], lr: 0.050095, loss: 1.7384
2022-08-24 23:54:32 - train: epoch 0050, iter [04800, 05004], lr: 0.050064, loss: 1.4571
2022-08-24 23:55:05 - train: epoch 0050, iter [04900, 05004], lr: 0.050033, loss: 1.5313
2022-08-24 23:55:39 - train: epoch 0050, iter [05000, 05004], lr: 0.050001, loss: 1.6203
2022-08-24 23:55:40 - train: epoch 050, train_loss: 1.6965
2022-08-24 23:56:56 - eval: epoch: 050, acc1: 64.326%, acc5: 86.658%, test_loss: 1.4633, per_image_load_time: 2.256ms, per_image_inference_time: 0.603ms
2022-08-24 23:56:56 - until epoch: 050, best_acc1: 65.064%
2022-08-24 23:56:56 - epoch 051 lr: 0.050000
2022-08-24 23:57:36 - train: epoch 0051, iter [00100, 05004], lr: 0.049969, loss: 1.8322
2022-08-24 23:58:09 - train: epoch 0051, iter [00200, 05004], lr: 0.049937, loss: 2.0632
2022-08-24 23:58:43 - train: epoch 0051, iter [00300, 05004], lr: 0.049906, loss: 1.7537
2022-08-24 23:59:18 - train: epoch 0051, iter [00400, 05004], lr: 0.049874, loss: 1.5095
2022-08-24 23:59:52 - train: epoch 0051, iter [00500, 05004], lr: 0.049843, loss: 1.7953
2022-08-25 00:00:25 - train: epoch 0051, iter [00600, 05004], lr: 0.049812, loss: 1.5971
2022-08-25 00:01:00 - train: epoch 0051, iter [00700, 05004], lr: 0.049780, loss: 1.7389
2022-08-25 00:01:33 - train: epoch 0051, iter [00800, 05004], lr: 0.049749, loss: 1.9792
2022-08-25 00:02:07 - train: epoch 0051, iter [00900, 05004], lr: 0.049717, loss: 1.6299
2022-08-25 00:02:41 - train: epoch 0051, iter [01000, 05004], lr: 0.049686, loss: 1.9936
2022-08-25 00:03:14 - train: epoch 0051, iter [01100, 05004], lr: 0.049655, loss: 1.9058
2022-08-25 00:03:48 - train: epoch 0051, iter [01200, 05004], lr: 0.049623, loss: 1.5632
2022-08-25 00:04:22 - train: epoch 0051, iter [01300, 05004], lr: 0.049592, loss: 1.3598
2022-08-25 00:04:56 - train: epoch 0051, iter [01400, 05004], lr: 0.049561, loss: 1.5339
2022-08-25 00:05:30 - train: epoch 0051, iter [01500, 05004], lr: 0.049529, loss: 1.6959
2022-08-25 00:06:03 - train: epoch 0051, iter [01600, 05004], lr: 0.049498, loss: 1.5321
2022-08-25 00:06:37 - train: epoch 0051, iter [01700, 05004], lr: 0.049466, loss: 1.8054
2022-08-25 00:07:11 - train: epoch 0051, iter [01800, 05004], lr: 0.049435, loss: 1.6686
2022-08-25 00:07:44 - train: epoch 0051, iter [01900, 05004], lr: 0.049404, loss: 1.4209
2022-08-25 00:08:18 - train: epoch 0051, iter [02000, 05004], lr: 0.049372, loss: 1.6966
2022-08-25 00:08:52 - train: epoch 0051, iter [02100, 05004], lr: 0.049341, loss: 1.7305
2022-08-25 00:09:26 - train: epoch 0051, iter [02200, 05004], lr: 0.049309, loss: 1.6879
2022-08-25 00:10:00 - train: epoch 0051, iter [02300, 05004], lr: 0.049278, loss: 1.7762
2022-08-25 00:10:33 - train: epoch 0051, iter [02400, 05004], lr: 0.049247, loss: 1.7661
2022-08-25 00:11:08 - train: epoch 0051, iter [02500, 05004], lr: 0.049215, loss: 1.5555
2022-08-25 00:11:42 - train: epoch 0051, iter [02600, 05004], lr: 0.049184, loss: 1.5547
2022-08-25 00:12:15 - train: epoch 0051, iter [02700, 05004], lr: 0.049152, loss: 1.7854
2022-08-25 00:12:49 - train: epoch 0051, iter [02800, 05004], lr: 0.049121, loss: 1.5859
2022-08-25 00:13:24 - train: epoch 0051, iter [02900, 05004], lr: 0.049090, loss: 1.6461
2022-08-25 00:13:58 - train: epoch 0051, iter [03000, 05004], lr: 0.049058, loss: 1.6286
2022-08-25 00:14:32 - train: epoch 0051, iter [03100, 05004], lr: 0.049027, loss: 1.5614
2022-08-25 00:15:05 - train: epoch 0051, iter [03200, 05004], lr: 0.048996, loss: 1.7277
2022-08-25 00:15:39 - train: epoch 0051, iter [03300, 05004], lr: 0.048964, loss: 1.7346
2022-08-25 00:16:13 - train: epoch 0051, iter [03400, 05004], lr: 0.048933, loss: 1.6924
2022-08-25 00:16:47 - train: epoch 0051, iter [03500, 05004], lr: 0.048901, loss: 1.6478
2022-08-25 00:17:21 - train: epoch 0051, iter [03600, 05004], lr: 0.048870, loss: 1.6816
2022-08-25 00:17:55 - train: epoch 0051, iter [03700, 05004], lr: 0.048839, loss: 1.8975
2022-08-25 00:18:30 - train: epoch 0051, iter [03800, 05004], lr: 0.048807, loss: 1.6984
2022-08-25 00:19:03 - train: epoch 0051, iter [03900, 05004], lr: 0.048776, loss: 1.7164
2022-08-25 00:19:38 - train: epoch 0051, iter [04000, 05004], lr: 0.048744, loss: 1.6371
2022-08-25 00:20:12 - train: epoch 0051, iter [04100, 05004], lr: 0.048713, loss: 1.7557
2022-08-25 00:20:46 - train: epoch 0051, iter [04200, 05004], lr: 0.048682, loss: 1.9382
2022-08-25 00:21:20 - train: epoch 0051, iter [04300, 05004], lr: 0.048650, loss: 1.5273
2022-08-25 00:21:54 - train: epoch 0051, iter [04400, 05004], lr: 0.048619, loss: 1.7201
2022-08-25 00:22:28 - train: epoch 0051, iter [04500, 05004], lr: 0.048588, loss: 1.5026
2022-08-25 00:23:02 - train: epoch 0051, iter [04600, 05004], lr: 0.048556, loss: 1.7926
2022-08-25 00:23:36 - train: epoch 0051, iter [04700, 05004], lr: 0.048525, loss: 1.7630
2022-08-25 00:24:10 - train: epoch 0051, iter [04800, 05004], lr: 0.048493, loss: 1.6742
2022-08-25 00:24:44 - train: epoch 0051, iter [04900, 05004], lr: 0.048462, loss: 1.8761
2022-08-25 00:25:17 - train: epoch 0051, iter [05000, 05004], lr: 0.048431, loss: 1.5502
2022-08-25 00:25:18 - train: epoch 051, train_loss: 1.6856
2022-08-25 00:26:34 - eval: epoch: 051, acc1: 64.900%, acc5: 86.854%, test_loss: 1.4342, per_image_load_time: 2.263ms, per_image_inference_time: 0.591ms
2022-08-25 00:26:34 - until epoch: 051, best_acc1: 65.064%
2022-08-25 00:26:34 - epoch 052 lr: 0.048429
2022-08-25 00:27:13 - train: epoch 0052, iter [00100, 05004], lr: 0.048398, loss: 1.5145
2022-08-25 00:27:47 - train: epoch 0052, iter [00200, 05004], lr: 0.048367, loss: 1.7502
2022-08-25 00:28:21 - train: epoch 0052, iter [00300, 05004], lr: 0.048335, loss: 1.7471
2022-08-25 00:28:53 - train: epoch 0052, iter [00400, 05004], lr: 0.048304, loss: 1.7419
2022-08-25 00:29:28 - train: epoch 0052, iter [00500, 05004], lr: 0.048273, loss: 1.7083
2022-08-25 00:30:01 - train: epoch 0052, iter [00600, 05004], lr: 0.048241, loss: 1.5180
2022-08-25 00:30:35 - train: epoch 0052, iter [00700, 05004], lr: 0.048210, loss: 1.7604
2022-08-25 00:31:08 - train: epoch 0052, iter [00800, 05004], lr: 0.048178, loss: 1.7028
2022-08-25 00:31:43 - train: epoch 0052, iter [00900, 05004], lr: 0.048147, loss: 1.7629
2022-08-25 00:32:17 - train: epoch 0052, iter [01000, 05004], lr: 0.048116, loss: 1.8696
2022-08-25 00:32:50 - train: epoch 0052, iter [01100, 05004], lr: 0.048084, loss: 1.6454
2022-08-25 00:33:24 - train: epoch 0052, iter [01200, 05004], lr: 0.048053, loss: 1.5745
2022-08-25 00:33:58 - train: epoch 0052, iter [01300, 05004], lr: 0.048022, loss: 1.5739
2022-08-25 00:34:31 - train: epoch 0052, iter [01400, 05004], lr: 0.047990, loss: 1.9167
2022-08-25 00:35:05 - train: epoch 0052, iter [01500, 05004], lr: 0.047959, loss: 1.5536
2022-08-25 00:35:38 - train: epoch 0052, iter [01600, 05004], lr: 0.047928, loss: 1.4713
2022-08-25 00:36:12 - train: epoch 0052, iter [01700, 05004], lr: 0.047896, loss: 1.5080
2022-08-25 00:36:45 - train: epoch 0052, iter [01800, 05004], lr: 0.047865, loss: 1.6374
2022-08-25 00:37:19 - train: epoch 0052, iter [01900, 05004], lr: 0.047833, loss: 1.7221
2022-08-25 00:37:54 - train: epoch 0052, iter [02000, 05004], lr: 0.047802, loss: 1.8218
2022-08-25 00:38:27 - train: epoch 0052, iter [02100, 05004], lr: 0.047771, loss: 1.5473
2022-08-25 00:39:00 - train: epoch 0052, iter [02200, 05004], lr: 0.047739, loss: 1.6887
2022-08-25 00:39:33 - train: epoch 0052, iter [02300, 05004], lr: 0.047708, loss: 1.4506
2022-08-25 00:40:07 - train: epoch 0052, iter [02400, 05004], lr: 0.047677, loss: 1.4866
2022-08-25 00:40:41 - train: epoch 0052, iter [02500, 05004], lr: 0.047645, loss: 1.6235
2022-08-25 00:41:15 - train: epoch 0052, iter [02600, 05004], lr: 0.047614, loss: 1.3855
2022-08-25 00:41:48 - train: epoch 0052, iter [02700, 05004], lr: 0.047583, loss: 1.5809
2022-08-25 00:42:23 - train: epoch 0052, iter [02800, 05004], lr: 0.047551, loss: 1.6354
2022-08-25 00:42:57 - train: epoch 0052, iter [02900, 05004], lr: 0.047520, loss: 1.5521
2022-08-25 00:43:31 - train: epoch 0052, iter [03000, 05004], lr: 0.047489, loss: 1.6421
2022-08-25 00:44:04 - train: epoch 0052, iter [03100, 05004], lr: 0.047457, loss: 1.6760
2022-08-25 00:44:39 - train: epoch 0052, iter [03200, 05004], lr: 0.047426, loss: 1.8093
2022-08-25 00:45:12 - train: epoch 0052, iter [03300, 05004], lr: 0.047394, loss: 1.7068
2022-08-25 00:45:47 - train: epoch 0052, iter [03400, 05004], lr: 0.047363, loss: 1.7898
2022-08-25 00:46:20 - train: epoch 0052, iter [03500, 05004], lr: 0.047332, loss: 1.6758
2022-08-25 00:46:54 - train: epoch 0052, iter [03600, 05004], lr: 0.047300, loss: 1.7986
2022-08-25 00:47:28 - train: epoch 0052, iter [03700, 05004], lr: 0.047269, loss: 1.8472
2022-08-25 00:48:02 - train: epoch 0052, iter [03800, 05004], lr: 0.047238, loss: 1.5509
2022-08-25 00:48:36 - train: epoch 0052, iter [03900, 05004], lr: 0.047206, loss: 1.6518
2022-08-25 00:49:09 - train: epoch 0052, iter [04000, 05004], lr: 0.047175, loss: 1.8163
2022-08-25 00:49:43 - train: epoch 0052, iter [04100, 05004], lr: 0.047144, loss: 1.5499
2022-08-25 00:50:18 - train: epoch 0052, iter [04200, 05004], lr: 0.047112, loss: 1.5682
2022-08-25 00:50:52 - train: epoch 0052, iter [04300, 05004], lr: 0.047081, loss: 1.6562
2022-08-25 00:51:26 - train: epoch 0052, iter [04400, 05004], lr: 0.047050, loss: 1.7959
2022-08-25 00:52:00 - train: epoch 0052, iter [04500, 05004], lr: 0.047018, loss: 1.6107
2022-08-25 00:52:34 - train: epoch 0052, iter [04600, 05004], lr: 0.046987, loss: 1.7314
2022-08-25 00:53:08 - train: epoch 0052, iter [04700, 05004], lr: 0.046956, loss: 1.7149
2022-08-25 00:53:42 - train: epoch 0052, iter [04800, 05004], lr: 0.046924, loss: 1.4737
2022-08-25 00:54:16 - train: epoch 0052, iter [04900, 05004], lr: 0.046893, loss: 1.6509
2022-08-25 00:54:49 - train: epoch 0052, iter [05000, 05004], lr: 0.046862, loss: 1.6741
2022-08-25 00:54:50 - train: epoch 052, train_loss: 1.6726
2022-08-25 00:56:06 - eval: epoch: 052, acc1: 64.820%, acc5: 86.992%, test_loss: 1.4396, per_image_load_time: 2.059ms, per_image_inference_time: 0.599ms
2022-08-25 00:56:06 - until epoch: 052, best_acc1: 65.064%
2022-08-25 00:56:06 - epoch 053 lr: 0.046860
2022-08-25 00:56:45 - train: epoch 0053, iter [00100, 05004], lr: 0.046829, loss: 1.5298
2022-08-25 00:57:19 - train: epoch 0053, iter [00200, 05004], lr: 0.046798, loss: 1.8769
2022-08-25 00:57:52 - train: epoch 0053, iter [00300, 05004], lr: 0.046766, loss: 1.7208
2022-08-25 00:58:25 - train: epoch 0053, iter [00400, 05004], lr: 0.046735, loss: 1.7270
2022-08-25 00:58:59 - train: epoch 0053, iter [00500, 05004], lr: 0.046704, loss: 1.8109
2022-08-25 00:59:34 - train: epoch 0053, iter [00600, 05004], lr: 0.046673, loss: 1.6363
2022-08-25 01:00:07 - train: epoch 0053, iter [00700, 05004], lr: 0.046641, loss: 1.5341
2022-08-25 01:00:41 - train: epoch 0053, iter [00800, 05004], lr: 0.046610, loss: 1.6902
2022-08-25 01:01:14 - train: epoch 0053, iter [00900, 05004], lr: 0.046579, loss: 1.5805
2022-08-25 01:01:48 - train: epoch 0053, iter [01000, 05004], lr: 0.046547, loss: 1.6292
2022-08-25 01:02:21 - train: epoch 0053, iter [01100, 05004], lr: 0.046516, loss: 1.6335
2022-08-25 01:02:54 - train: epoch 0053, iter [01200, 05004], lr: 0.046485, loss: 1.5856
2022-08-25 01:03:28 - train: epoch 0053, iter [01300, 05004], lr: 0.046453, loss: 1.6608
2022-08-25 01:04:01 - train: epoch 0053, iter [01400, 05004], lr: 0.046422, loss: 1.9878
2022-08-25 01:04:36 - train: epoch 0053, iter [01500, 05004], lr: 0.046391, loss: 1.5988
2022-08-25 01:05:10 - train: epoch 0053, iter [01600, 05004], lr: 0.046359, loss: 1.9272
2022-08-25 01:05:43 - train: epoch 0053, iter [01700, 05004], lr: 0.046328, loss: 1.8583
2022-08-25 01:06:16 - train: epoch 0053, iter [01800, 05004], lr: 0.046297, loss: 1.8028
2022-08-25 01:06:50 - train: epoch 0053, iter [01900, 05004], lr: 0.046265, loss: 1.4687
2022-08-25 01:07:24 - train: epoch 0053, iter [02000, 05004], lr: 0.046234, loss: 1.6743
2022-08-25 01:07:58 - train: epoch 0053, iter [02100, 05004], lr: 0.046203, loss: 1.7489
2022-08-25 01:08:31 - train: epoch 0053, iter [02200, 05004], lr: 0.046172, loss: 1.5862
2022-08-25 01:09:05 - train: epoch 0053, iter [02300, 05004], lr: 0.046140, loss: 1.6695
2022-08-25 01:09:39 - train: epoch 0053, iter [02400, 05004], lr: 0.046109, loss: 1.7573
2022-08-25 01:10:14 - train: epoch 0053, iter [02500, 05004], lr: 0.046078, loss: 1.8951
2022-08-25 01:10:47 - train: epoch 0053, iter [02600, 05004], lr: 0.046046, loss: 1.7216
2022-08-25 01:11:21 - train: epoch 0053, iter [02700, 05004], lr: 0.046015, loss: 1.8805
2022-08-25 01:11:55 - train: epoch 0053, iter [02800, 05004], lr: 0.045984, loss: 1.7464
2022-08-25 01:12:29 - train: epoch 0053, iter [02900, 05004], lr: 0.045953, loss: 1.4640
2022-08-25 01:13:03 - train: epoch 0053, iter [03000, 05004], lr: 0.045921, loss: 1.4130
2022-08-25 01:13:38 - train: epoch 0053, iter [03100, 05004], lr: 0.045890, loss: 1.8007
2022-08-25 01:14:12 - train: epoch 0053, iter [03200, 05004], lr: 0.045859, loss: 1.9265
2022-08-25 01:14:46 - train: epoch 0053, iter [03300, 05004], lr: 0.045827, loss: 1.6739
2022-08-25 01:15:19 - train: epoch 0053, iter [03400, 05004], lr: 0.045796, loss: 1.7722
2022-08-25 01:15:53 - train: epoch 0053, iter [03500, 05004], lr: 0.045765, loss: 1.7504
2022-08-25 01:16:28 - train: epoch 0053, iter [03600, 05004], lr: 0.045734, loss: 1.7369
2022-08-25 01:17:02 - train: epoch 0053, iter [03700, 05004], lr: 0.045702, loss: 1.8311
2022-08-25 01:17:36 - train: epoch 0053, iter [03800, 05004], lr: 0.045671, loss: 1.5456
2022-08-25 01:18:10 - train: epoch 0053, iter [03900, 05004], lr: 0.045640, loss: 1.8453
2022-08-25 01:18:44 - train: epoch 0053, iter [04000, 05004], lr: 0.045608, loss: 1.6703
2022-08-25 01:19:18 - train: epoch 0053, iter [04100, 05004], lr: 0.045577, loss: 1.6386
2022-08-25 01:19:52 - train: epoch 0053, iter [04200, 05004], lr: 0.045546, loss: 1.6375
2022-08-25 01:20:26 - train: epoch 0053, iter [04300, 05004], lr: 0.045515, loss: 1.9009
2022-08-25 01:21:00 - train: epoch 0053, iter [04400, 05004], lr: 0.045483, loss: 1.7479
2022-08-25 01:21:35 - train: epoch 0053, iter [04500, 05004], lr: 0.045452, loss: 1.7690
2022-08-25 01:22:09 - train: epoch 0053, iter [04600, 05004], lr: 0.045421, loss: 1.6915
2022-08-25 01:22:43 - train: epoch 0053, iter [04700, 05004], lr: 0.045390, loss: 1.8296
2022-08-25 01:23:17 - train: epoch 0053, iter [04800, 05004], lr: 0.045358, loss: 1.8685
2022-08-25 01:23:50 - train: epoch 0053, iter [04900, 05004], lr: 0.045327, loss: 1.5227
2022-08-25 01:24:24 - train: epoch 0053, iter [05000, 05004], lr: 0.045296, loss: 1.5993
2022-08-25 01:24:25 - train: epoch 053, train_loss: 1.6601
2022-08-25 01:25:40 - eval: epoch: 053, acc1: 65.158%, acc5: 87.184%, test_loss: 1.4241, per_image_load_time: 1.145ms, per_image_inference_time: 0.582ms
2022-08-25 01:25:40 - until epoch: 053, best_acc1: 65.158%
2022-08-25 01:25:40 - epoch 054 lr: 0.045294
2022-08-25 01:26:20 - train: epoch 0054, iter [00100, 05004], lr: 0.045263, loss: 1.3752
2022-08-25 01:26:54 - train: epoch 0054, iter [00200, 05004], lr: 0.045232, loss: 1.9561
2022-08-25 01:27:28 - train: epoch 0054, iter [00300, 05004], lr: 0.045201, loss: 1.7008
2022-08-25 01:28:02 - train: epoch 0054, iter [00400, 05004], lr: 0.045170, loss: 1.3974
2022-08-25 01:28:35 - train: epoch 0054, iter [00500, 05004], lr: 0.045138, loss: 1.6280
2022-08-25 01:29:08 - train: epoch 0054, iter [00600, 05004], lr: 0.045107, loss: 1.4809
2022-08-25 01:29:42 - train: epoch 0054, iter [00700, 05004], lr: 0.045076, loss: 1.7387
2022-08-25 01:30:16 - train: epoch 0054, iter [00800, 05004], lr: 0.045045, loss: 1.7231
2022-08-25 01:30:50 - train: epoch 0054, iter [00900, 05004], lr: 0.045013, loss: 1.4549
2022-08-25 01:31:23 - train: epoch 0054, iter [01000, 05004], lr: 0.044982, loss: 1.4088
2022-08-25 01:31:57 - train: epoch 0054, iter [01100, 05004], lr: 0.044951, loss: 1.4814
2022-08-25 01:32:30 - train: epoch 0054, iter [01200, 05004], lr: 0.044920, loss: 1.7321
2022-08-25 01:33:03 - train: epoch 0054, iter [01300, 05004], lr: 0.044888, loss: 1.6367
2022-08-25 01:33:37 - train: epoch 0054, iter [01400, 05004], lr: 0.044857, loss: 1.6960
2022-08-25 01:34:10 - train: epoch 0054, iter [01500, 05004], lr: 0.044826, loss: 1.6231
2022-08-25 01:34:44 - train: epoch 0054, iter [01600, 05004], lr: 0.044795, loss: 1.3019
2022-08-25 01:35:18 - train: epoch 0054, iter [01700, 05004], lr: 0.044764, loss: 1.6867
2022-08-25 01:35:51 - train: epoch 0054, iter [01800, 05004], lr: 0.044732, loss: 1.5664
2022-08-25 01:36:25 - train: epoch 0054, iter [01900, 05004], lr: 0.044701, loss: 2.0873
2022-08-25 01:36:58 - train: epoch 0054, iter [02000, 05004], lr: 0.044670, loss: 1.7984
2022-08-25 01:37:32 - train: epoch 0054, iter [02100, 05004], lr: 0.044639, loss: 1.3741
2022-08-25 01:38:05 - train: epoch 0054, iter [02200, 05004], lr: 0.044608, loss: 1.6137
2022-08-25 01:38:39 - train: epoch 0054, iter [02300, 05004], lr: 0.044576, loss: 1.6550
2022-08-25 01:39:12 - train: epoch 0054, iter [02400, 05004], lr: 0.044545, loss: 1.5706
2022-08-25 01:39:46 - train: epoch 0054, iter [02500, 05004], lr: 0.044514, loss: 1.7129
2022-08-25 01:40:19 - train: epoch 0054, iter [02600, 05004], lr: 0.044483, loss: 1.5952
2022-08-25 01:40:53 - train: epoch 0054, iter [02700, 05004], lr: 0.044452, loss: 1.7500
2022-08-25 01:41:27 - train: epoch 0054, iter [02800, 05004], lr: 0.044420, loss: 1.8386
2022-08-25 01:42:00 - train: epoch 0054, iter [02900, 05004], lr: 0.044389, loss: 1.5148
2022-08-25 01:42:34 - train: epoch 0054, iter [03000, 05004], lr: 0.044358, loss: 1.8376
2022-08-25 01:43:08 - train: epoch 0054, iter [03100, 05004], lr: 0.044327, loss: 1.6911
2022-08-25 01:43:42 - train: epoch 0054, iter [03200, 05004], lr: 0.044296, loss: 1.8334
2022-08-25 01:44:15 - train: epoch 0054, iter [03300, 05004], lr: 0.044264, loss: 1.6288
2022-08-25 01:44:49 - train: epoch 0054, iter [03400, 05004], lr: 0.044233, loss: 1.7505
2022-08-25 01:45:23 - train: epoch 0054, iter [03500, 05004], lr: 0.044202, loss: 1.7741
2022-08-25 01:45:56 - train: epoch 0054, iter [03600, 05004], lr: 0.044171, loss: 1.6015
2022-08-25 01:46:30 - train: epoch 0054, iter [03700, 05004], lr: 0.044140, loss: 1.5830
2022-08-25 01:47:04 - train: epoch 0054, iter [03800, 05004], lr: 0.044108, loss: 1.6260
2022-08-25 01:47:38 - train: epoch 0054, iter [03900, 05004], lr: 0.044077, loss: 1.6469
2022-08-25 01:48:11 - train: epoch 0054, iter [04000, 05004], lr: 0.044046, loss: 1.5673
2022-08-25 01:48:46 - train: epoch 0054, iter [04100, 05004], lr: 0.044015, loss: 1.7060
2022-08-25 01:49:19 - train: epoch 0054, iter [04200, 05004], lr: 0.043984, loss: 1.5732
2022-08-25 01:49:53 - train: epoch 0054, iter [04300, 05004], lr: 0.043953, loss: 1.6648
2022-08-25 01:50:27 - train: epoch 0054, iter [04400, 05004], lr: 0.043921, loss: 1.4670
2022-08-25 01:51:01 - train: epoch 0054, iter [04500, 05004], lr: 0.043890, loss: 1.4173
2022-08-25 01:51:35 - train: epoch 0054, iter [04600, 05004], lr: 0.043859, loss: 1.8120
2022-08-25 01:52:08 - train: epoch 0054, iter [04700, 05004], lr: 0.043828, loss: 1.9183
2022-08-25 01:52:42 - train: epoch 0054, iter [04800, 05004], lr: 0.043797, loss: 1.7516
2022-08-25 01:53:16 - train: epoch 0054, iter [04900, 05004], lr: 0.043766, loss: 1.5534
2022-08-25 01:53:50 - train: epoch 0054, iter [05000, 05004], lr: 0.043735, loss: 1.7285
2022-08-25 01:53:51 - train: epoch 054, train_loss: 1.6479
2022-08-25 01:55:06 - eval: epoch: 054, acc1: 65.300%, acc5: 87.260%, test_loss: 1.4252, per_image_load_time: 2.342ms, per_image_inference_time: 0.595ms
2022-08-25 01:55:07 - until epoch: 054, best_acc1: 65.300%
2022-08-25 01:55:07 - epoch 055 lr: 0.043733
2022-08-25 01:55:47 - train: epoch 0055, iter [00100, 05004], lr: 0.043702, loss: 1.5148
2022-08-25 01:56:20 - train: epoch 0055, iter [00200, 05004], lr: 0.043671, loss: 1.4972
2022-08-25 01:56:54 - train: epoch 0055, iter [00300, 05004], lr: 0.043640, loss: 1.4311
2022-08-25 01:57:27 - train: epoch 0055, iter [00400, 05004], lr: 0.043609, loss: 1.5324
2022-08-25 01:58:01 - train: epoch 0055, iter [00500, 05004], lr: 0.043578, loss: 1.4149
2022-08-25 01:58:35 - train: epoch 0055, iter [00600, 05004], lr: 0.043547, loss: 1.5819
2022-08-25 01:59:08 - train: epoch 0055, iter [00700, 05004], lr: 0.043515, loss: 1.6803
2022-08-25 01:59:42 - train: epoch 0055, iter [00800, 05004], lr: 0.043484, loss: 1.4813
2022-08-25 02:00:16 - train: epoch 0055, iter [00900, 05004], lr: 0.043453, loss: 1.5455
2022-08-25 02:00:49 - train: epoch 0055, iter [01000, 05004], lr: 0.043422, loss: 1.6264
2022-08-25 02:01:23 - train: epoch 0055, iter [01100, 05004], lr: 0.043391, loss: 1.6224
2022-08-25 02:01:57 - train: epoch 0055, iter [01200, 05004], lr: 0.043360, loss: 1.6704
2022-08-25 02:02:31 - train: epoch 0055, iter [01300, 05004], lr: 0.043329, loss: 1.7475
2022-08-25 02:03:04 - train: epoch 0055, iter [01400, 05004], lr: 0.043298, loss: 1.6223
2022-08-25 02:03:38 - train: epoch 0055, iter [01500, 05004], lr: 0.043266, loss: 1.6540
2022-08-25 02:04:11 - train: epoch 0055, iter [01600, 05004], lr: 0.043235, loss: 1.8085
2022-08-25 02:04:45 - train: epoch 0055, iter [01700, 05004], lr: 0.043204, loss: 1.7303
2022-08-25 02:05:19 - train: epoch 0055, iter [01800, 05004], lr: 0.043173, loss: 1.7725
2022-08-25 02:05:53 - train: epoch 0055, iter [01900, 05004], lr: 0.043142, loss: 1.6419
2022-08-25 02:06:28 - train: epoch 0055, iter [02000, 05004], lr: 0.043111, loss: 1.5869
2022-08-25 02:07:01 - train: epoch 0055, iter [02100, 05004], lr: 0.043080, loss: 1.4706
2022-08-25 02:07:35 - train: epoch 0055, iter [02200, 05004], lr: 0.043049, loss: 1.7846
2022-08-25 02:08:08 - train: epoch 0055, iter [02300, 05004], lr: 0.043018, loss: 1.6004
2022-08-25 02:08:43 - train: epoch 0055, iter [02400, 05004], lr: 0.042987, loss: 1.4469
2022-08-25 02:09:17 - train: epoch 0055, iter [02500, 05004], lr: 0.042956, loss: 1.7049
2022-08-25 02:09:51 - train: epoch 0055, iter [02600, 05004], lr: 0.042924, loss: 1.5194
2022-08-25 02:10:24 - train: epoch 0055, iter [02700, 05004], lr: 0.042893, loss: 1.4461
2022-08-25 02:10:58 - train: epoch 0055, iter [02800, 05004], lr: 0.042862, loss: 1.6535
2022-08-25 02:11:32 - train: epoch 0055, iter [02900, 05004], lr: 0.042831, loss: 1.7947
2022-08-25 02:12:06 - train: epoch 0055, iter [03000, 05004], lr: 0.042800, loss: 1.6777
2022-08-25 02:12:39 - train: epoch 0055, iter [03100, 05004], lr: 0.042769, loss: 1.7102
2022-08-25 02:13:13 - train: epoch 0055, iter [03200, 05004], lr: 0.042738, loss: 1.4496
2022-08-25 02:13:47 - train: epoch 0055, iter [03300, 05004], lr: 0.042707, loss: 1.4761
2022-08-25 02:14:21 - train: epoch 0055, iter [03400, 05004], lr: 0.042676, loss: 1.4532
2022-08-25 02:14:55 - train: epoch 0055, iter [03500, 05004], lr: 0.042645, loss: 1.4406
2022-08-25 02:15:28 - train: epoch 0055, iter [03600, 05004], lr: 0.042614, loss: 1.5784
2022-08-25 02:16:01 - train: epoch 0055, iter [03700, 05004], lr: 0.042583, loss: 1.6780
2022-08-25 02:16:36 - train: epoch 0055, iter [03800, 05004], lr: 0.042552, loss: 1.7397
2022-08-25 02:17:10 - train: epoch 0055, iter [03900, 05004], lr: 0.042521, loss: 1.9439
2022-08-25 02:17:43 - train: epoch 0055, iter [04000, 05004], lr: 0.042490, loss: 1.6723
2022-08-25 02:18:17 - train: epoch 0055, iter [04100, 05004], lr: 0.042459, loss: 1.6333
2022-08-25 02:18:50 - train: epoch 0055, iter [04200, 05004], lr: 0.042428, loss: 1.6878
2022-08-25 02:19:25 - train: epoch 0055, iter [04300, 05004], lr: 0.042397, loss: 1.6795
2022-08-25 02:19:58 - train: epoch 0055, iter [04400, 05004], lr: 0.042366, loss: 1.8289
2022-08-25 02:20:32 - train: epoch 0055, iter [04500, 05004], lr: 0.042335, loss: 1.6274
2022-08-25 02:21:06 - train: epoch 0055, iter [04600, 05004], lr: 0.042304, loss: 1.7106
2022-08-25 02:21:39 - train: epoch 0055, iter [04700, 05004], lr: 0.042273, loss: 1.4127
2022-08-25 02:22:13 - train: epoch 0055, iter [04800, 05004], lr: 0.042242, loss: 1.6772
2022-08-25 02:22:47 - train: epoch 0055, iter [04900, 05004], lr: 0.042211, loss: 1.5968
2022-08-25 02:23:20 - train: epoch 0055, iter [05000, 05004], lr: 0.042180, loss: 1.7020
2022-08-25 02:23:21 - train: epoch 055, train_loss: 1.6324
2022-08-25 02:24:36 - eval: epoch: 055, acc1: 66.216%, acc5: 87.910%, test_loss: 1.3652, per_image_load_time: 2.282ms, per_image_inference_time: 0.585ms
2022-08-25 02:24:36 - until epoch: 055, best_acc1: 66.216%
2022-08-25 02:24:36 - epoch 056 lr: 0.042178
2022-08-25 02:25:16 - train: epoch 0056, iter [00100, 05004], lr: 0.042147, loss: 1.6856
2022-08-25 02:25:50 - train: epoch 0056, iter [00200, 05004], lr: 0.042116, loss: 1.8342
2022-08-25 02:26:24 - train: epoch 0056, iter [00300, 05004], lr: 0.042085, loss: 1.4970
2022-08-25 02:26:57 - train: epoch 0056, iter [00400, 05004], lr: 0.042054, loss: 1.5294
2022-08-25 02:27:31 - train: epoch 0056, iter [00500, 05004], lr: 0.042023, loss: 1.5263
2022-08-25 02:28:05 - train: epoch 0056, iter [00600, 05004], lr: 0.041992, loss: 1.5046
2022-08-25 02:28:39 - train: epoch 0056, iter [00700, 05004], lr: 0.041961, loss: 1.6969
2022-08-25 02:29:13 - train: epoch 0056, iter [00800, 05004], lr: 0.041930, loss: 1.7874
2022-08-25 02:29:46 - train: epoch 0056, iter [00900, 05004], lr: 0.041899, loss: 1.6495
2022-08-25 02:30:21 - train: epoch 0056, iter [01000, 05004], lr: 0.041868, loss: 1.6067
2022-08-25 02:30:54 - train: epoch 0056, iter [01100, 05004], lr: 0.041837, loss: 1.5112
2022-08-25 02:31:28 - train: epoch 0056, iter [01200, 05004], lr: 0.041806, loss: 1.5350
2022-08-25 02:32:02 - train: epoch 0056, iter [01300, 05004], lr: 0.041775, loss: 1.7027
2022-08-25 02:32:36 - train: epoch 0056, iter [01400, 05004], lr: 0.041745, loss: 1.5912
2022-08-25 02:33:10 - train: epoch 0056, iter [01500, 05004], lr: 0.041714, loss: 1.8955
2022-08-25 02:33:43 - train: epoch 0056, iter [01600, 05004], lr: 0.041683, loss: 1.3882
2022-08-25 02:34:18 - train: epoch 0056, iter [01700, 05004], lr: 0.041652, loss: 1.6260
2022-08-25 02:34:52 - train: epoch 0056, iter [01800, 05004], lr: 0.041621, loss: 1.8508
2022-08-25 02:35:26 - train: epoch 0056, iter [01900, 05004], lr: 0.041590, loss: 1.5633
2022-08-25 02:36:00 - train: epoch 0056, iter [02000, 05004], lr: 0.041559, loss: 1.5549
2022-08-25 02:36:34 - train: epoch 0056, iter [02100, 05004], lr: 0.041528, loss: 1.6671
2022-08-25 02:37:08 - train: epoch 0056, iter [02200, 05004], lr: 0.041497, loss: 1.7717
2022-08-25 02:37:41 - train: epoch 0056, iter [02300, 05004], lr: 0.041466, loss: 1.7546
2022-08-25 02:38:15 - train: epoch 0056, iter [02400, 05004], lr: 0.041435, loss: 1.6988
2022-08-25 02:38:50 - train: epoch 0056, iter [02500, 05004], lr: 0.041404, loss: 1.8480
2022-08-25 02:39:23 - train: epoch 0056, iter [02600, 05004], lr: 0.041373, loss: 1.5805
2022-08-25 02:39:57 - train: epoch 0056, iter [02700, 05004], lr: 0.041342, loss: 1.6614
2022-08-25 02:40:31 - train: epoch 0056, iter [02800, 05004], lr: 0.041311, loss: 1.5632
2022-08-25 02:41:05 - train: epoch 0056, iter [02900, 05004], lr: 0.041280, loss: 1.7579
2022-08-25 02:41:39 - train: epoch 0056, iter [03000, 05004], lr: 0.041250, loss: 1.8759
2022-08-25 02:42:13 - train: epoch 0056, iter [03100, 05004], lr: 0.041219, loss: 1.4662
2022-08-25 02:42:47 - train: epoch 0056, iter [03200, 05004], lr: 0.041188, loss: 1.5011
2022-08-25 02:43:21 - train: epoch 0056, iter [03300, 05004], lr: 0.041157, loss: 1.9474
2022-08-25 02:43:55 - train: epoch 0056, iter [03400, 05004], lr: 0.041126, loss: 1.5670
2022-08-25 02:44:29 - train: epoch 0056, iter [03500, 05004], lr: 0.041095, loss: 1.6351
2022-08-25 02:45:02 - train: epoch 0056, iter [03600, 05004], lr: 0.041064, loss: 1.3866
2022-08-25 02:45:36 - train: epoch 0056, iter [03700, 05004], lr: 0.041033, loss: 1.5736
2022-08-25 02:46:10 - train: epoch 0056, iter [03800, 05004], lr: 0.041002, loss: 1.4946
2022-08-25 02:46:44 - train: epoch 0056, iter [03900, 05004], lr: 0.040972, loss: 1.8327
2022-08-25 02:47:19 - train: epoch 0056, iter [04000, 05004], lr: 0.040941, loss: 1.7084
2022-08-25 02:47:54 - train: epoch 0056, iter [04100, 05004], lr: 0.040910, loss: 1.9095
2022-08-25 02:48:27 - train: epoch 0056, iter [04200, 05004], lr: 0.040879, loss: 1.5332
2022-08-25 02:49:02 - train: epoch 0056, iter [04300, 05004], lr: 0.040848, loss: 1.6243
2022-08-25 02:49:36 - train: epoch 0056, iter [04400, 05004], lr: 0.040817, loss: 1.7770
2022-08-25 02:50:10 - train: epoch 0056, iter [04500, 05004], lr: 0.040786, loss: 1.6030
2022-08-25 02:50:44 - train: epoch 0056, iter [04600, 05004], lr: 0.040756, loss: 1.6298
2022-08-25 02:51:18 - train: epoch 0056, iter [04700, 05004], lr: 0.040725, loss: 1.7157
2022-08-25 02:51:51 - train: epoch 0056, iter [04800, 05004], lr: 0.040694, loss: 1.7768
2022-08-25 02:52:26 - train: epoch 0056, iter [04900, 05004], lr: 0.040663, loss: 1.6669
2022-08-25 02:52:58 - train: epoch 0056, iter [05000, 05004], lr: 0.040632, loss: 1.8604
2022-08-25 02:53:00 - train: epoch 056, train_loss: 1.6175
2022-08-25 02:54:15 - eval: epoch: 056, acc1: 66.602%, acc5: 88.052%, test_loss: 1.3662, per_image_load_time: 2.301ms, per_image_inference_time: 0.612ms
2022-08-25 02:54:15 - until epoch: 056, best_acc1: 66.602%
2022-08-25 02:54:15 - epoch 057 lr: 0.040631
2022-08-25 02:54:56 - train: epoch 0057, iter [00100, 05004], lr: 0.040600, loss: 1.7193
2022-08-25 02:55:30 - train: epoch 0057, iter [00200, 05004], lr: 0.040569, loss: 1.5658
2022-08-25 02:56:04 - train: epoch 0057, iter [00300, 05004], lr: 0.040538, loss: 1.4060
2022-08-25 02:56:38 - train: epoch 0057, iter [00400, 05004], lr: 0.040508, loss: 1.6306
2022-08-25 02:57:12 - train: epoch 0057, iter [00500, 05004], lr: 0.040477, loss: 1.3662
2022-08-25 02:57:45 - train: epoch 0057, iter [00600, 05004], lr: 0.040446, loss: 1.7561
2022-08-25 02:58:20 - train: epoch 0057, iter [00700, 05004], lr: 0.040415, loss: 1.3563
2022-08-25 02:58:53 - train: epoch 0057, iter [00800, 05004], lr: 0.040384, loss: 1.5624
2022-08-25 02:59:27 - train: epoch 0057, iter [00900, 05004], lr: 0.040354, loss: 1.6967
2022-08-25 03:00:01 - train: epoch 0057, iter [01000, 05004], lr: 0.040323, loss: 1.6054
2022-08-25 03:00:35 - train: epoch 0057, iter [01100, 05004], lr: 0.040292, loss: 1.5755
2022-08-25 03:01:09 - train: epoch 0057, iter [01200, 05004], lr: 0.040261, loss: 1.5342
2022-08-25 03:01:43 - train: epoch 0057, iter [01300, 05004], lr: 0.040230, loss: 1.6820
2022-08-25 03:02:16 - train: epoch 0057, iter [01400, 05004], lr: 0.040200, loss: 1.6313
2022-08-25 03:02:50 - train: epoch 0057, iter [01500, 05004], lr: 0.040169, loss: 1.7948
2022-08-25 03:03:23 - train: epoch 0057, iter [01600, 05004], lr: 0.040138, loss: 1.7367
2022-08-25 03:03:57 - train: epoch 0057, iter [01700, 05004], lr: 0.040107, loss: 1.8327
2022-08-25 03:04:30 - train: epoch 0057, iter [01800, 05004], lr: 0.040077, loss: 1.7285
2022-08-25 03:05:04 - train: epoch 0057, iter [01900, 05004], lr: 0.040046, loss: 1.5289
2022-08-25 03:05:37 - train: epoch 0057, iter [02000, 05004], lr: 0.040015, loss: 1.6424
2022-08-25 03:06:11 - train: epoch 0057, iter [02100, 05004], lr: 0.039984, loss: 1.6641
2022-08-25 03:06:45 - train: epoch 0057, iter [02200, 05004], lr: 0.039953, loss: 1.7289
2022-08-25 03:07:19 - train: epoch 0057, iter [02300, 05004], lr: 0.039923, loss: 1.5645
2022-08-25 03:07:52 - train: epoch 0057, iter [02400, 05004], lr: 0.039892, loss: 1.4952
2022-08-25 03:08:26 - train: epoch 0057, iter [02500, 05004], lr: 0.039861, loss: 1.5833
2022-08-25 03:09:00 - train: epoch 0057, iter [02600, 05004], lr: 0.039831, loss: 1.5399
2022-08-25 03:09:33 - train: epoch 0057, iter [02700, 05004], lr: 0.039800, loss: 1.2977
2022-08-25 03:10:06 - train: epoch 0057, iter [02800, 05004], lr: 0.039769, loss: 1.1749
2022-08-25 03:10:41 - train: epoch 0057, iter [02900, 05004], lr: 0.039738, loss: 1.6796
2022-08-25 03:11:14 - train: epoch 0057, iter [03000, 05004], lr: 0.039708, loss: 1.7209
2022-08-25 03:11:48 - train: epoch 0057, iter [03100, 05004], lr: 0.039677, loss: 1.8583
2022-08-25 03:12:22 - train: epoch 0057, iter [03200, 05004], lr: 0.039646, loss: 1.7357
2022-08-25 03:12:54 - train: epoch 0057, iter [03300, 05004], lr: 0.039615, loss: 1.4650
2022-08-25 03:13:28 - train: epoch 0057, iter [03400, 05004], lr: 0.039585, loss: 1.6943
2022-08-25 03:14:03 - train: epoch 0057, iter [03500, 05004], lr: 0.039554, loss: 1.6698
2022-08-25 03:14:36 - train: epoch 0057, iter [03600, 05004], lr: 0.039523, loss: 1.4590
2022-08-25 03:15:10 - train: epoch 0057, iter [03700, 05004], lr: 0.039493, loss: 1.5452
2022-08-25 03:15:44 - train: epoch 0057, iter [03800, 05004], lr: 0.039462, loss: 1.5332
2022-08-25 03:16:18 - train: epoch 0057, iter [03900, 05004], lr: 0.039431, loss: 1.7775
2022-08-25 03:16:51 - train: epoch 0057, iter [04000, 05004], lr: 0.039401, loss: 1.5799
2022-08-25 03:17:25 - train: epoch 0057, iter [04100, 05004], lr: 0.039370, loss: 1.7521
2022-08-25 03:17:59 - train: epoch 0057, iter [04200, 05004], lr: 0.039339, loss: 1.6933
2022-08-25 03:18:33 - train: epoch 0057, iter [04300, 05004], lr: 0.039309, loss: 1.5656
2022-08-25 03:19:06 - train: epoch 0057, iter [04400, 05004], lr: 0.039278, loss: 1.6041
2022-08-25 03:19:40 - train: epoch 0057, iter [04500, 05004], lr: 0.039247, loss: 1.7974
2022-08-25 03:20:14 - train: epoch 0057, iter [04600, 05004], lr: 0.039217, loss: 1.6523
2022-08-25 03:20:48 - train: epoch 0057, iter [04700, 05004], lr: 0.039186, loss: 1.5214
2022-08-25 03:21:21 - train: epoch 0057, iter [04800, 05004], lr: 0.039155, loss: 1.8749
2022-08-25 03:21:55 - train: epoch 0057, iter [04900, 05004], lr: 0.039125, loss: 1.8437
2022-08-25 03:22:28 - train: epoch 0057, iter [05000, 05004], lr: 0.039094, loss: 1.7641
2022-08-25 03:22:29 - train: epoch 057, train_loss: 1.6043
2022-08-25 03:23:44 - eval: epoch: 057, acc1: 65.992%, acc5: 87.570%, test_loss: 1.4036, per_image_load_time: 2.328ms, per_image_inference_time: 0.587ms
2022-08-25 03:23:44 - until epoch: 057, best_acc1: 66.602%
2022-08-25 03:23:44 - epoch 058 lr: 0.039093
2022-08-25 03:24:23 - train: epoch 0058, iter [00100, 05004], lr: 0.039062, loss: 1.5263
2022-08-25 03:24:58 - train: epoch 0058, iter [00200, 05004], lr: 0.039032, loss: 1.4175
2022-08-25 03:25:32 - train: epoch 0058, iter [00300, 05004], lr: 0.039001, loss: 1.6741
2022-08-25 03:26:06 - train: epoch 0058, iter [00400, 05004], lr: 0.038970, loss: 1.6648
2022-08-25 03:26:39 - train: epoch 0058, iter [00500, 05004], lr: 0.038940, loss: 1.4039
2022-08-25 03:27:13 - train: epoch 0058, iter [00600, 05004], lr: 0.038909, loss: 1.7438
2022-08-25 03:27:46 - train: epoch 0058, iter [00700, 05004], lr: 0.038879, loss: 1.6048
2022-08-25 03:28:20 - train: epoch 0058, iter [00800, 05004], lr: 0.038848, loss: 1.4699
2022-08-25 03:28:54 - train: epoch 0058, iter [00900, 05004], lr: 0.038817, loss: 1.3634
2022-08-25 03:29:28 - train: epoch 0058, iter [01000, 05004], lr: 0.038787, loss: 1.6601
2022-08-25 03:30:02 - train: epoch 0058, iter [01100, 05004], lr: 0.038756, loss: 1.3613
2022-08-25 03:30:36 - train: epoch 0058, iter [01200, 05004], lr: 0.038726, loss: 1.4800
2022-08-25 03:31:09 - train: epoch 0058, iter [01300, 05004], lr: 0.038695, loss: 1.5986
2022-08-25 03:31:43 - train: epoch 0058, iter [01400, 05004], lr: 0.038664, loss: 1.6389
2022-08-25 03:32:17 - train: epoch 0058, iter [01500, 05004], lr: 0.038634, loss: 1.5392
2022-08-25 03:32:51 - train: epoch 0058, iter [01600, 05004], lr: 0.038603, loss: 1.5633
2022-08-25 03:33:24 - train: epoch 0058, iter [01700, 05004], lr: 0.038573, loss: 1.7649
2022-08-25 03:33:58 - train: epoch 0058, iter [01800, 05004], lr: 0.038542, loss: 1.5985
2022-08-25 03:34:32 - train: epoch 0058, iter [01900, 05004], lr: 0.038512, loss: 1.7707
2022-08-25 03:35:06 - train: epoch 0058, iter [02000, 05004], lr: 0.038481, loss: 1.7258
2022-08-25 03:35:40 - train: epoch 0058, iter [02100, 05004], lr: 0.038450, loss: 1.4100
2022-08-25 03:36:14 - train: epoch 0058, iter [02200, 05004], lr: 0.038420, loss: 1.4468
2022-08-25 03:36:48 - train: epoch 0058, iter [02300, 05004], lr: 0.038389, loss: 1.5872
2022-08-25 03:37:22 - train: epoch 0058, iter [02400, 05004], lr: 0.038359, loss: 1.6252
2022-08-25 03:37:55 - train: epoch 0058, iter [02500, 05004], lr: 0.038328, loss: 1.6905
2022-08-25 03:38:29 - train: epoch 0058, iter [02600, 05004], lr: 0.038298, loss: 1.4123
2022-08-25 03:39:02 - train: epoch 0058, iter [02700, 05004], lr: 0.038267, loss: 1.8587
2022-08-25 03:39:37 - train: epoch 0058, iter [02800, 05004], lr: 0.038237, loss: 1.3802
2022-08-25 03:40:11 - train: epoch 0058, iter [02900, 05004], lr: 0.038206, loss: 1.4798
2022-08-25 03:40:44 - train: epoch 0058, iter [03000, 05004], lr: 0.038176, loss: 1.6910
2022-08-25 03:41:18 - train: epoch 0058, iter [03100, 05004], lr: 0.038145, loss: 1.5165
2022-08-25 03:41:51 - train: epoch 0058, iter [03200, 05004], lr: 0.038115, loss: 1.4726
2022-08-25 03:42:25 - train: epoch 0058, iter [03300, 05004], lr: 0.038084, loss: 1.5233
2022-08-25 03:42:59 - train: epoch 0058, iter [03400, 05004], lr: 0.038054, loss: 1.4060
2022-08-25 03:43:33 - train: epoch 0058, iter [03500, 05004], lr: 0.038023, loss: 1.5339
2022-08-25 03:44:06 - train: epoch 0058, iter [03600, 05004], lr: 0.037993, loss: 1.5041
2022-08-25 03:44:40 - train: epoch 0058, iter [03700, 05004], lr: 0.037962, loss: 1.5748
2022-08-25 03:45:12 - train: epoch 0058, iter [03800, 05004], lr: 0.037932, loss: 1.6109
2022-08-25 03:45:46 - train: epoch 0058, iter [03900, 05004], lr: 0.037901, loss: 1.6582
2022-08-25 03:46:20 - train: epoch 0058, iter [04000, 05004], lr: 0.037871, loss: 1.6841
2022-08-25 03:46:54 - train: epoch 0058, iter [04100, 05004], lr: 0.037841, loss: 1.7837
2022-08-25 03:47:28 - train: epoch 0058, iter [04200, 05004], lr: 0.037810, loss: 1.3653
2022-08-25 03:48:01 - train: epoch 0058, iter [04300, 05004], lr: 0.037780, loss: 1.7894
2022-08-25 03:48:35 - train: epoch 0058, iter [04400, 05004], lr: 0.037749, loss: 1.4155
2022-08-25 03:49:09 - train: epoch 0058, iter [04500, 05004], lr: 0.037719, loss: 1.6257
2022-08-25 03:49:43 - train: epoch 0058, iter [04600, 05004], lr: 0.037688, loss: 1.4927
2022-08-25 03:50:16 - train: epoch 0058, iter [04700, 05004], lr: 0.037658, loss: 1.5185
2022-08-25 03:50:50 - train: epoch 0058, iter [04800, 05004], lr: 0.037628, loss: 1.6455
2022-08-25 03:51:23 - train: epoch 0058, iter [04900, 05004], lr: 0.037597, loss: 1.5040
2022-08-25 03:51:57 - train: epoch 0058, iter [05000, 05004], lr: 0.037567, loss: 1.5619
2022-08-25 03:51:58 - train: epoch 058, train_loss: 1.5885
2022-08-25 03:53:14 - eval: epoch: 058, acc1: 66.738%, acc5: 88.032%, test_loss: 1.3641, per_image_load_time: 2.258ms, per_image_inference_time: 0.601ms
2022-08-25 03:53:14 - until epoch: 058, best_acc1: 66.738%
2022-08-25 03:53:14 - epoch 059 lr: 0.037565
2022-08-25 03:53:54 - train: epoch 0059, iter [00100, 05004], lr: 0.037535, loss: 1.7463
2022-08-25 03:54:28 - train: epoch 0059, iter [00200, 05004], lr: 0.037505, loss: 1.4903
2022-08-25 03:55:02 - train: epoch 0059, iter [00300, 05004], lr: 0.037474, loss: 1.6103
2022-08-25 03:55:36 - train: epoch 0059, iter [00400, 05004], lr: 0.037444, loss: 1.6653
2022-08-25 03:56:11 - train: epoch 0059, iter [00500, 05004], lr: 0.037414, loss: 1.8662
2022-08-25 03:56:44 - train: epoch 0059, iter [00600, 05004], lr: 0.037383, loss: 1.5078
2022-08-25 03:57:18 - train: epoch 0059, iter [00700, 05004], lr: 0.037353, loss: 1.5211
2022-08-25 03:57:52 - train: epoch 0059, iter [00800, 05004], lr: 0.037322, loss: 1.5903
2022-08-25 03:58:26 - train: epoch 0059, iter [00900, 05004], lr: 0.037292, loss: 1.5114
2022-08-25 03:58:59 - train: epoch 0059, iter [01000, 05004], lr: 0.037262, loss: 1.5272
2022-08-25 03:59:33 - train: epoch 0059, iter [01100, 05004], lr: 0.037231, loss: 1.8020
2022-08-25 04:00:07 - train: epoch 0059, iter [01200, 05004], lr: 0.037201, loss: 1.3435
2022-08-25 04:00:41 - train: epoch 0059, iter [01300, 05004], lr: 0.037171, loss: 1.7559
2022-08-25 04:01:15 - train: epoch 0059, iter [01400, 05004], lr: 0.037140, loss: 1.7234
2022-08-25 04:01:48 - train: epoch 0059, iter [01500, 05004], lr: 0.037110, loss: 1.6955
2022-08-25 04:02:22 - train: epoch 0059, iter [01600, 05004], lr: 0.037080, loss: 1.4364
2022-08-25 04:02:56 - train: epoch 0059, iter [01700, 05004], lr: 0.037049, loss: 1.5621
2022-08-25 04:03:30 - train: epoch 0059, iter [01800, 05004], lr: 0.037019, loss: 1.5290
2022-08-25 04:04:04 - train: epoch 0059, iter [01900, 05004], lr: 0.036989, loss: 1.6051
2022-08-25 04:04:38 - train: epoch 0059, iter [02000, 05004], lr: 0.036958, loss: 1.4333
2022-08-25 04:05:11 - train: epoch 0059, iter [02100, 05004], lr: 0.036928, loss: 1.7124
2022-08-25 04:05:45 - train: epoch 0059, iter [02200, 05004], lr: 0.036898, loss: 1.8371
2022-08-25 04:06:18 - train: epoch 0059, iter [02300, 05004], lr: 0.036868, loss: 1.5585
2022-08-25 04:06:52 - train: epoch 0059, iter [02400, 05004], lr: 0.036837, loss: 1.7509
2022-08-25 04:07:26 - train: epoch 0059, iter [02500, 05004], lr: 0.036807, loss: 1.5849
2022-08-25 04:08:00 - train: epoch 0059, iter [02600, 05004], lr: 0.036777, loss: 1.5794
2022-08-25 04:08:33 - train: epoch 0059, iter [02700, 05004], lr: 0.036746, loss: 1.5268
2022-08-25 04:09:07 - train: epoch 0059, iter [02800, 05004], lr: 0.036716, loss: 1.7490
2022-08-25 04:09:40 - train: epoch 0059, iter [02900, 05004], lr: 0.036686, loss: 1.5974
2022-08-25 04:10:14 - train: epoch 0059, iter [03000, 05004], lr: 0.036656, loss: 1.9719
2022-08-25 04:10:48 - train: epoch 0059, iter [03100, 05004], lr: 0.036625, loss: 1.4950
2022-08-25 04:11:21 - train: epoch 0059, iter [03200, 05004], lr: 0.036595, loss: 1.5362
2022-08-25 04:11:55 - train: epoch 0059, iter [03300, 05004], lr: 0.036565, loss: 1.7285
2022-08-25 04:12:28 - train: epoch 0059, iter [03400, 05004], lr: 0.036535, loss: 1.8735
2022-08-25 04:13:02 - train: epoch 0059, iter [03500, 05004], lr: 0.036504, loss: 1.4543
2022-08-25 04:13:35 - train: epoch 0059, iter [03600, 05004], lr: 0.036474, loss: 1.5872
2022-08-25 04:14:10 - train: epoch 0059, iter [03700, 05004], lr: 0.036444, loss: 1.5627
2022-08-25 04:14:43 - train: epoch 0059, iter [03800, 05004], lr: 0.036414, loss: 1.5624
2022-08-25 04:15:17 - train: epoch 0059, iter [03900, 05004], lr: 0.036384, loss: 1.5750
2022-08-25 04:15:51 - train: epoch 0059, iter [04000, 05004], lr: 0.036353, loss: 1.8266
2022-08-25 04:16:25 - train: epoch 0059, iter [04100, 05004], lr: 0.036323, loss: 1.6772
2022-08-25 04:16:58 - train: epoch 0059, iter [04200, 05004], lr: 0.036293, loss: 1.5778
2022-08-25 04:17:32 - train: epoch 0059, iter [04300, 05004], lr: 0.036263, loss: 1.7882
2022-08-25 04:18:06 - train: epoch 0059, iter [04400, 05004], lr: 0.036233, loss: 1.7638
2022-08-25 04:18:40 - train: epoch 0059, iter [04500, 05004], lr: 0.036202, loss: 1.7395
2022-08-25 04:19:14 - train: epoch 0059, iter [04600, 05004], lr: 0.036172, loss: 1.6056
2022-08-25 04:19:48 - train: epoch 0059, iter [04700, 05004], lr: 0.036142, loss: 1.6227
2022-08-25 04:20:21 - train: epoch 0059, iter [04800, 05004], lr: 0.036112, loss: 1.5289
2022-08-25 04:20:56 - train: epoch 0059, iter [04900, 05004], lr: 0.036082, loss: 1.8029
2022-08-25 04:21:29 - train: epoch 0059, iter [05000, 05004], lr: 0.036052, loss: 1.7814
2022-08-25 04:21:30 - train: epoch 059, train_loss: 1.5755
2022-08-25 04:22:45 - eval: epoch: 059, acc1: 66.484%, acc5: 87.866%, test_loss: 1.3679, per_image_load_time: 2.322ms, per_image_inference_time: 0.599ms
2022-08-25 04:22:46 - until epoch: 059, best_acc1: 66.738%
2022-08-25 04:22:46 - epoch 060 lr: 0.036050
2022-08-25 04:23:25 - train: epoch 0060, iter [00100, 05004], lr: 0.036020, loss: 1.4004
2022-08-25 04:24:00 - train: epoch 0060, iter [00200, 05004], lr: 0.035990, loss: 1.6250
2022-08-25 04:24:34 - train: epoch 0060, iter [00300, 05004], lr: 0.035960, loss: 1.5279
2022-08-25 04:25:07 - train: epoch 0060, iter [00400, 05004], lr: 0.035930, loss: 1.6206
2022-08-25 04:25:42 - train: epoch 0060, iter [00500, 05004], lr: 0.035900, loss: 1.8193
2022-08-25 04:26:15 - train: epoch 0060, iter [00600, 05004], lr: 0.035870, loss: 1.6283
2022-08-25 04:26:48 - train: epoch 0060, iter [00700, 05004], lr: 0.035840, loss: 1.6047
2022-08-25 04:27:22 - train: epoch 0060, iter [00800, 05004], lr: 0.035809, loss: 1.6611
2022-08-25 04:27:56 - train: epoch 0060, iter [00900, 05004], lr: 0.035779, loss: 1.3243
2022-08-25 04:28:29 - train: epoch 0060, iter [01000, 05004], lr: 0.035749, loss: 1.3014
2022-08-25 04:29:03 - train: epoch 0060, iter [01100, 05004], lr: 0.035719, loss: 1.3363
2022-08-25 04:29:37 - train: epoch 0060, iter [01200, 05004], lr: 0.035689, loss: 1.4596
2022-08-25 04:30:10 - train: epoch 0060, iter [01300, 05004], lr: 0.035659, loss: 1.4452
2022-08-25 04:30:44 - train: epoch 0060, iter [01400, 05004], lr: 0.035629, loss: 1.6157
2022-08-25 04:31:17 - train: epoch 0060, iter [01500, 05004], lr: 0.035599, loss: 1.5900
2022-08-25 04:31:51 - train: epoch 0060, iter [01600, 05004], lr: 0.035569, loss: 1.4695
2022-08-25 04:32:25 - train: epoch 0060, iter [01700, 05004], lr: 0.035539, loss: 1.5206
2022-08-25 04:32:59 - train: epoch 0060, iter [01800, 05004], lr: 0.035509, loss: 1.5198
2022-08-25 04:33:33 - train: epoch 0060, iter [01900, 05004], lr: 0.035479, loss: 1.6786
2022-08-25 04:34:07 - train: epoch 0060, iter [02000, 05004], lr: 0.035449, loss: 1.4250
2022-08-25 04:34:40 - train: epoch 0060, iter [02100, 05004], lr: 0.035419, loss: 1.6073
2022-08-25 04:35:14 - train: epoch 0060, iter [02200, 05004], lr: 0.035389, loss: 1.6513
2022-08-25 04:35:48 - train: epoch 0060, iter [02300, 05004], lr: 0.035359, loss: 1.3461
2022-08-25 04:36:21 - train: epoch 0060, iter [02400, 05004], lr: 0.035329, loss: 1.4685
2022-08-25 04:36:55 - train: epoch 0060, iter [02500, 05004], lr: 0.035299, loss: 1.6211
2022-08-25 04:37:29 - train: epoch 0060, iter [02600, 05004], lr: 0.035269, loss: 1.6039
2022-08-25 04:38:03 - train: epoch 0060, iter [02700, 05004], lr: 0.035239, loss: 1.5862
2022-08-25 04:38:36 - train: epoch 0060, iter [02800, 05004], lr: 0.035209, loss: 1.4453
2022-08-25 04:39:10 - train: epoch 0060, iter [02900, 05004], lr: 0.035179, loss: 1.6464
2022-08-25 04:39:44 - train: epoch 0060, iter [03000, 05004], lr: 0.035149, loss: 1.7981
2022-08-25 04:40:18 - train: epoch 0060, iter [03100, 05004], lr: 0.035119, loss: 1.6297
2022-08-25 04:40:52 - train: epoch 0060, iter [03200, 05004], lr: 0.035089, loss: 1.5534
2022-08-25 04:41:26 - train: epoch 0060, iter [03300, 05004], lr: 0.035059, loss: 1.3961
2022-08-25 04:41:59 - train: epoch 0060, iter [03400, 05004], lr: 0.035029, loss: 1.6255
2022-08-25 04:42:33 - train: epoch 0060, iter [03500, 05004], lr: 0.034999, loss: 1.6378
2022-08-25 04:43:07 - train: epoch 0060, iter [03600, 05004], lr: 0.034969, loss: 1.5491
2022-08-25 04:43:41 - train: epoch 0060, iter [03700, 05004], lr: 0.034939, loss: 1.6010
2022-08-25 04:44:15 - train: epoch 0060, iter [03800, 05004], lr: 0.034909, loss: 1.6411
2022-08-25 04:44:48 - train: epoch 0060, iter [03900, 05004], lr: 0.034879, loss: 1.7714
2022-08-25 04:45:22 - train: epoch 0060, iter [04000, 05004], lr: 0.034849, loss: 1.6105
2022-08-25 04:45:56 - train: epoch 0060, iter [04100, 05004], lr: 0.034819, loss: 1.6794
2022-08-25 04:46:29 - train: epoch 0060, iter [04200, 05004], lr: 0.034789, loss: 1.7028
2022-08-25 04:47:03 - train: epoch 0060, iter [04300, 05004], lr: 0.034759, loss: 1.5419
2022-08-25 04:47:37 - train: epoch 0060, iter [04400, 05004], lr: 0.034730, loss: 1.7256
2022-08-25 04:48:11 - train: epoch 0060, iter [04500, 05004], lr: 0.034700, loss: 1.5529
2022-08-25 04:48:44 - train: epoch 0060, iter [04600, 05004], lr: 0.034670, loss: 1.5421
2022-08-25 04:49:19 - train: epoch 0060, iter [04700, 05004], lr: 0.034640, loss: 1.6357
2022-08-25 04:49:53 - train: epoch 0060, iter [04800, 05004], lr: 0.034610, loss: 1.4218
2022-08-25 04:50:26 - train: epoch 0060, iter [04900, 05004], lr: 0.034580, loss: 1.5487
2022-08-25 04:50:59 - train: epoch 0060, iter [05000, 05004], lr: 0.034550, loss: 1.5078
2022-08-25 04:51:01 - train: epoch 060, train_loss: 1.5624
2022-08-25 04:52:16 - eval: epoch: 060, acc1: 66.844%, acc5: 88.118%, test_loss: 1.3419, per_image_load_time: 1.561ms, per_image_inference_time: 0.612ms
2022-08-25 04:52:16 - until epoch: 060, best_acc1: 66.844%
2022-08-25 04:52:16 - epoch 061 lr: 0.034549
2022-08-25 04:52:56 - train: epoch 0061, iter [00100, 05004], lr: 0.034519, loss: 1.4087
2022-08-25 04:53:30 - train: epoch 0061, iter [00200, 05004], lr: 0.034489, loss: 1.5607
2022-08-25 04:54:04 - train: epoch 0061, iter [00300, 05004], lr: 0.034460, loss: 1.3078
2022-08-25 04:54:38 - train: epoch 0061, iter [00400, 05004], lr: 0.034430, loss: 1.8294
2022-08-25 04:55:12 - train: epoch 0061, iter [00500, 05004], lr: 0.034400, loss: 1.5159
2022-08-25 04:55:47 - train: epoch 0061, iter [00600, 05004], lr: 0.034370, loss: 1.5552
2022-08-25 04:56:20 - train: epoch 0061, iter [00700, 05004], lr: 0.034340, loss: 1.3059
2022-08-25 04:56:54 - train: epoch 0061, iter [00800, 05004], lr: 0.034311, loss: 1.6692
2022-08-25 04:57:29 - train: epoch 0061, iter [00900, 05004], lr: 0.034281, loss: 1.4507
2022-08-25 04:58:03 - train: epoch 0061, iter [01000, 05004], lr: 0.034251, loss: 1.3190
2022-08-25 04:58:38 - train: epoch 0061, iter [01100, 05004], lr: 0.034221, loss: 1.3628
2022-08-25 04:59:12 - train: epoch 0061, iter [01200, 05004], lr: 0.034191, loss: 1.4366
2022-08-25 04:59:47 - train: epoch 0061, iter [01300, 05004], lr: 0.034162, loss: 1.4712
2022-08-25 05:00:22 - train: epoch 0061, iter [01400, 05004], lr: 0.034132, loss: 1.4832
2022-08-25 05:00:56 - train: epoch 0061, iter [01500, 05004], lr: 0.034102, loss: 1.5943
2022-08-25 05:01:30 - train: epoch 0061, iter [01600, 05004], lr: 0.034072, loss: 1.2869
2022-08-25 05:02:04 - train: epoch 0061, iter [01700, 05004], lr: 0.034043, loss: 1.5373
2022-08-25 05:02:38 - train: epoch 0061, iter [01800, 05004], lr: 0.034013, loss: 1.5580
2022-08-25 05:03:12 - train: epoch 0061, iter [01900, 05004], lr: 0.033983, loss: 1.7543
2022-08-25 05:03:47 - train: epoch 0061, iter [02000, 05004], lr: 0.033953, loss: 1.4883
2022-08-25 05:04:22 - train: epoch 0061, iter [02100, 05004], lr: 0.033924, loss: 1.9546
2022-08-25 05:04:56 - train: epoch 0061, iter [02200, 05004], lr: 0.033894, loss: 1.4315
2022-08-25 05:05:30 - train: epoch 0061, iter [02300, 05004], lr: 0.033864, loss: 1.5142
2022-08-25 05:06:04 - train: epoch 0061, iter [02400, 05004], lr: 0.033834, loss: 1.3627
2022-08-25 05:06:40 - train: epoch 0061, iter [02500, 05004], lr: 0.033805, loss: 1.5029
2022-08-25 05:07:14 - train: epoch 0061, iter [02600, 05004], lr: 0.033775, loss: 1.6057
2022-08-25 05:07:48 - train: epoch 0061, iter [02700, 05004], lr: 0.033745, loss: 1.7234
2022-08-25 05:08:22 - train: epoch 0061, iter [02800, 05004], lr: 0.033716, loss: 1.6287
2022-08-25 05:08:57 - train: epoch 0061, iter [02900, 05004], lr: 0.033686, loss: 1.7089
2022-08-25 05:09:32 - train: epoch 0061, iter [03000, 05004], lr: 0.033656, loss: 1.5663
2022-08-25 05:10:06 - train: epoch 0061, iter [03100, 05004], lr: 0.033627, loss: 1.6669
2022-08-25 05:10:40 - train: epoch 0061, iter [03200, 05004], lr: 0.033597, loss: 1.5950
2022-08-25 05:11:15 - train: epoch 0061, iter [03300, 05004], lr: 0.033567, loss: 1.5395
2022-08-25 05:11:49 - train: epoch 0061, iter [03400, 05004], lr: 0.033538, loss: 1.4187
2022-08-25 05:12:24 - train: epoch 0061, iter [03500, 05004], lr: 0.033508, loss: 1.6362
2022-08-25 05:12:59 - train: epoch 0061, iter [03600, 05004], lr: 0.033478, loss: 1.5455
2022-08-25 05:13:33 - train: epoch 0061, iter [03700, 05004], lr: 0.033449, loss: 1.7090
2022-08-25 05:14:08 - train: epoch 0061, iter [03800, 05004], lr: 0.033419, loss: 1.6301
2022-08-25 05:14:42 - train: epoch 0061, iter [03900, 05004], lr: 0.033390, loss: 1.5236
2022-08-25 05:15:16 - train: epoch 0061, iter [04000, 05004], lr: 0.033360, loss: 1.5891
2022-08-25 05:15:51 - train: epoch 0061, iter [04100, 05004], lr: 0.033330, loss: 1.8009
2022-08-25 05:16:26 - train: epoch 0061, iter [04200, 05004], lr: 0.033301, loss: 1.4172
2022-08-25 05:17:00 - train: epoch 0061, iter [04300, 05004], lr: 0.033271, loss: 1.5474
2022-08-25 05:17:35 - train: epoch 0061, iter [04400, 05004], lr: 0.033242, loss: 1.7300
2022-08-25 05:18:10 - train: epoch 0061, iter [04500, 05004], lr: 0.033212, loss: 1.5606
2022-08-25 05:18:43 - train: epoch 0061, iter [04600, 05004], lr: 0.033182, loss: 1.7841
2022-08-25 05:19:18 - train: epoch 0061, iter [04700, 05004], lr: 0.033153, loss: 1.4490
2022-08-25 05:19:53 - train: epoch 0061, iter [04800, 05004], lr: 0.033123, loss: 1.5017
2022-08-25 05:20:27 - train: epoch 0061, iter [04900, 05004], lr: 0.033094, loss: 1.5390
2022-08-25 05:21:01 - train: epoch 0061, iter [05000, 05004], lr: 0.033064, loss: 1.7343
2022-08-25 05:21:02 - train: epoch 061, train_loss: 1.5452
2022-08-25 05:22:18 - eval: epoch: 061, acc1: 67.532%, acc5: 88.518%, test_loss: 1.3210, per_image_load_time: 2.104ms, per_image_inference_time: 0.618ms
2022-08-25 05:22:18 - until epoch: 061, best_acc1: 67.532%
2022-08-25 05:22:18 - epoch 062 lr: 0.033063
2022-08-25 05:22:57 - train: epoch 0062, iter [00100, 05004], lr: 0.033034, loss: 1.5895
2022-08-25 05:23:32 - train: epoch 0062, iter [00200, 05004], lr: 0.033004, loss: 1.6307
2022-08-25 05:24:05 - train: epoch 0062, iter [00300, 05004], lr: 0.032975, loss: 1.5157
2022-08-25 05:24:40 - train: epoch 0062, iter [00400, 05004], lr: 0.032945, loss: 1.4897
2022-08-25 05:25:14 - train: epoch 0062, iter [00500, 05004], lr: 0.032916, loss: 1.3560
2022-08-25 05:25:48 - train: epoch 0062, iter [00600, 05004], lr: 0.032886, loss: 1.4240
2022-08-25 05:26:22 - train: epoch 0062, iter [00700, 05004], lr: 0.032857, loss: 1.6300
2022-08-25 05:26:57 - train: epoch 0062, iter [00800, 05004], lr: 0.032827, loss: 1.5345
2022-08-25 05:27:31 - train: epoch 0062, iter [00900, 05004], lr: 0.032798, loss: 1.4609
2022-08-25 05:28:05 - train: epoch 0062, iter [01000, 05004], lr: 0.032768, loss: 1.6141
2022-08-25 05:28:39 - train: epoch 0062, iter [01100, 05004], lr: 0.032739, loss: 1.4058
2022-08-25 05:29:13 - train: epoch 0062, iter [01200, 05004], lr: 0.032709, loss: 1.8093
2022-08-25 05:29:47 - train: epoch 0062, iter [01300, 05004], lr: 0.032680, loss: 1.5727
2022-08-25 05:30:21 - train: epoch 0062, iter [01400, 05004], lr: 0.032650, loss: 1.4707
2022-08-25 05:30:55 - train: epoch 0062, iter [01500, 05004], lr: 0.032621, loss: 1.6320
2022-08-25 05:31:29 - train: epoch 0062, iter [01600, 05004], lr: 0.032591, loss: 1.4968
2022-08-25 05:32:03 - train: epoch 0062, iter [01700, 05004], lr: 0.032562, loss: 1.5421
2022-08-25 05:32:38 - train: epoch 0062, iter [01800, 05004], lr: 0.032533, loss: 1.3638
2022-08-25 05:33:13 - train: epoch 0062, iter [01900, 05004], lr: 0.032503, loss: 1.5251
2022-08-25 05:33:47 - train: epoch 0062, iter [02000, 05004], lr: 0.032474, loss: 1.4419
2022-08-25 05:34:22 - train: epoch 0062, iter [02100, 05004], lr: 0.032444, loss: 1.6802
2022-08-25 05:34:56 - train: epoch 0062, iter [02200, 05004], lr: 0.032415, loss: 1.3946
2022-08-25 05:35:31 - train: epoch 0062, iter [02300, 05004], lr: 0.032386, loss: 1.5526
2022-08-25 05:36:06 - train: epoch 0062, iter [02400, 05004], lr: 0.032356, loss: 1.5595
2022-08-25 05:36:40 - train: epoch 0062, iter [02500, 05004], lr: 0.032327, loss: 1.6566
2022-08-25 05:37:14 - train: epoch 0062, iter [02600, 05004], lr: 0.032297, loss: 1.3947
2022-08-25 05:37:49 - train: epoch 0062, iter [02700, 05004], lr: 0.032268, loss: 1.5203
2022-08-25 05:38:23 - train: epoch 0062, iter [02800, 05004], lr: 0.032239, loss: 1.6831
2022-08-25 05:38:57 - train: epoch 0062, iter [02900, 05004], lr: 0.032209, loss: 1.5997
2022-08-25 05:39:31 - train: epoch 0062, iter [03000, 05004], lr: 0.032180, loss: 1.6441
2022-08-25 05:40:05 - train: epoch 0062, iter [03100, 05004], lr: 0.032151, loss: 1.6432
2022-08-25 05:40:39 - train: epoch 0062, iter [03200, 05004], lr: 0.032121, loss: 1.4271
2022-08-25 05:41:14 - train: epoch 0062, iter [03300, 05004], lr: 0.032092, loss: 1.5755
2022-08-25 05:41:48 - train: epoch 0062, iter [03400, 05004], lr: 0.032063, loss: 1.5895
2022-08-25 05:42:22 - train: epoch 0062, iter [03500, 05004], lr: 0.032034, loss: 1.6206
2022-08-25 05:42:57 - train: epoch 0062, iter [03600, 05004], lr: 0.032004, loss: 1.5320
2022-08-25 05:43:31 - train: epoch 0062, iter [03700, 05004], lr: 0.031975, loss: 1.5573
2022-08-25 05:44:05 - train: epoch 0062, iter [03800, 05004], lr: 0.031946, loss: 1.5787
2022-08-25 05:44:40 - train: epoch 0062, iter [03900, 05004], lr: 0.031916, loss: 1.5263
2022-08-25 05:45:14 - train: epoch 0062, iter [04000, 05004], lr: 0.031887, loss: 1.2842
2022-08-25 05:45:48 - train: epoch 0062, iter [04100, 05004], lr: 0.031858, loss: 1.5700
2022-08-25 05:46:23 - train: epoch 0062, iter [04200, 05004], lr: 0.031829, loss: 1.3942
2022-08-25 05:46:57 - train: epoch 0062, iter [04300, 05004], lr: 0.031799, loss: 1.5797
2022-08-25 05:47:32 - train: epoch 0062, iter [04400, 05004], lr: 0.031770, loss: 1.4505
2022-08-25 05:48:06 - train: epoch 0062, iter [04500, 05004], lr: 0.031741, loss: 1.4702
2022-08-25 05:48:41 - train: epoch 0062, iter [04600, 05004], lr: 0.031712, loss: 1.2395
2022-08-25 05:49:16 - train: epoch 0062, iter [04700, 05004], lr: 0.031683, loss: 1.5469
2022-08-25 05:49:50 - train: epoch 0062, iter [04800, 05004], lr: 0.031653, loss: 1.3810
2022-08-25 05:50:25 - train: epoch 0062, iter [04900, 05004], lr: 0.031624, loss: 1.5652
2022-08-25 05:50:59 - train: epoch 0062, iter [05000, 05004], lr: 0.031595, loss: 1.5114
2022-08-25 05:51:00 - train: epoch 062, train_loss: 1.5295
2022-08-25 05:52:16 - eval: epoch: 062, acc1: 67.868%, acc5: 88.818%, test_loss: 1.2994, per_image_load_time: 1.750ms, per_image_inference_time: 0.605ms
2022-08-25 05:52:16 - until epoch: 062, best_acc1: 67.868%
2022-08-25 05:52:16 - epoch 063 lr: 0.031593
2022-08-25 05:52:55 - train: epoch 0063, iter [00100, 05004], lr: 0.031565, loss: 1.4128
2022-08-25 05:53:30 - train: epoch 0063, iter [00200, 05004], lr: 0.031535, loss: 1.3467
2022-08-25 05:54:04 - train: epoch 0063, iter [00300, 05004], lr: 0.031506, loss: 1.7029
2022-08-25 05:54:38 - train: epoch 0063, iter [00400, 05004], lr: 0.031477, loss: 1.5672
2022-08-25 05:55:12 - train: epoch 0063, iter [00500, 05004], lr: 0.031448, loss: 1.4199
2022-08-25 05:55:46 - train: epoch 0063, iter [00600, 05004], lr: 0.031419, loss: 1.6143
2022-08-25 05:56:21 - train: epoch 0063, iter [00700, 05004], lr: 0.031390, loss: 1.5142
2022-08-25 05:56:54 - train: epoch 0063, iter [00800, 05004], lr: 0.031361, loss: 1.4571
2022-08-25 05:57:28 - train: epoch 0063, iter [00900, 05004], lr: 0.031331, loss: 1.5964
2022-08-25 05:58:03 - train: epoch 0063, iter [01000, 05004], lr: 0.031302, loss: 1.6036
2022-08-25 05:58:36 - train: epoch 0063, iter [01100, 05004], lr: 0.031273, loss: 1.4136
2022-08-25 05:59:11 - train: epoch 0063, iter [01200, 05004], lr: 0.031244, loss: 1.4809
2022-08-25 05:59:45 - train: epoch 0063, iter [01300, 05004], lr: 0.031215, loss: 1.4731
2022-08-25 06:00:19 - train: epoch 0063, iter [01400, 05004], lr: 0.031186, loss: 1.9007
2022-08-25 06:00:54 - train: epoch 0063, iter [01500, 05004], lr: 0.031157, loss: 1.5906
2022-08-25 06:01:28 - train: epoch 0063, iter [01600, 05004], lr: 0.031128, loss: 1.5097
2022-08-25 06:02:02 - train: epoch 0063, iter [01700, 05004], lr: 0.031099, loss: 1.3707
2022-08-25 06:02:37 - train: epoch 0063, iter [01800, 05004], lr: 0.031070, loss: 1.6270
2022-08-25 06:03:11 - train: epoch 0063, iter [01900, 05004], lr: 0.031041, loss: 1.5968
2022-08-25 06:03:45 - train: epoch 0063, iter [02000, 05004], lr: 0.031012, loss: 1.4044
2022-08-25 06:04:20 - train: epoch 0063, iter [02100, 05004], lr: 0.030982, loss: 1.3559
2022-08-25 06:04:54 - train: epoch 0063, iter [02200, 05004], lr: 0.030953, loss: 1.8779
2022-08-25 06:05:28 - train: epoch 0063, iter [02300, 05004], lr: 0.030924, loss: 1.5335
2022-08-25 06:06:02 - train: epoch 0063, iter [02400, 05004], lr: 0.030895, loss: 1.7091
2022-08-25 06:06:37 - train: epoch 0063, iter [02500, 05004], lr: 0.030866, loss: 1.4775
2022-08-25 06:07:12 - train: epoch 0063, iter [02600, 05004], lr: 0.030837, loss: 1.4799
2022-08-25 06:07:46 - train: epoch 0063, iter [02700, 05004], lr: 0.030808, loss: 1.6626
2022-08-25 06:08:20 - train: epoch 0063, iter [02800, 05004], lr: 0.030779, loss: 1.5319
2022-08-25 06:08:54 - train: epoch 0063, iter [02900, 05004], lr: 0.030750, loss: 1.4240
2022-08-25 06:09:29 - train: epoch 0063, iter [03000, 05004], lr: 0.030721, loss: 1.5335
2022-08-25 06:10:03 - train: epoch 0063, iter [03100, 05004], lr: 0.030693, loss: 1.8571
2022-08-25 06:10:37 - train: epoch 0063, iter [03200, 05004], lr: 0.030664, loss: 1.6057
2022-08-25 06:11:11 - train: epoch 0063, iter [03300, 05004], lr: 0.030635, loss: 1.4519
2022-08-25 06:11:46 - train: epoch 0063, iter [03400, 05004], lr: 0.030606, loss: 1.3465
2022-08-25 06:12:20 - train: epoch 0063, iter [03500, 05004], lr: 0.030577, loss: 1.7088
2022-08-25 06:12:55 - train: epoch 0063, iter [03600, 05004], lr: 0.030548, loss: 1.4620
2022-08-25 06:13:29 - train: epoch 0063, iter [03700, 05004], lr: 0.030519, loss: 1.7380
2022-08-25 06:14:03 - train: epoch 0063, iter [03800, 05004], lr: 0.030490, loss: 1.7226
2022-08-25 06:14:38 - train: epoch 0063, iter [03900, 05004], lr: 0.030461, loss: 1.2436
2022-08-25 06:15:12 - train: epoch 0063, iter [04000, 05004], lr: 0.030432, loss: 1.2799
2022-08-25 06:15:47 - train: epoch 0063, iter [04100, 05004], lr: 0.030403, loss: 1.6581
2022-08-25 06:16:21 - train: epoch 0063, iter [04200, 05004], lr: 0.030374, loss: 1.4556
2022-08-25 06:16:56 - train: epoch 0063, iter [04300, 05004], lr: 0.030346, loss: 1.7521
2022-08-25 06:17:30 - train: epoch 0063, iter [04400, 05004], lr: 0.030317, loss: 1.6651
2022-08-25 06:18:05 - train: epoch 0063, iter [04500, 05004], lr: 0.030288, loss: 1.4024
2022-08-25 06:18:40 - train: epoch 0063, iter [04600, 05004], lr: 0.030259, loss: 1.4247
2022-08-25 06:19:14 - train: epoch 0063, iter [04700, 05004], lr: 0.030230, loss: 1.4795
2022-08-25 06:19:49 - train: epoch 0063, iter [04800, 05004], lr: 0.030201, loss: 1.4989
2022-08-25 06:20:23 - train: epoch 0063, iter [04900, 05004], lr: 0.030173, loss: 1.4968
2022-08-25 06:20:56 - train: epoch 0063, iter [05000, 05004], lr: 0.030144, loss: 1.4357
2022-08-25 06:20:57 - train: epoch 063, train_loss: 1.5148
2022-08-25 06:22:13 - eval: epoch: 063, acc1: 68.408%, acc5: 88.954%, test_loss: 1.2817, per_image_load_time: 1.774ms, per_image_inference_time: 0.613ms
2022-08-25 06:22:13 - until epoch: 063, best_acc1: 68.408%
2022-08-25 06:22:13 - epoch 064 lr: 0.030142
2022-08-25 06:22:53 - train: epoch 0064, iter [00100, 05004], lr: 0.030114, loss: 1.4449
2022-08-25 06:23:27 - train: epoch 0064, iter [00200, 05004], lr: 0.030085, loss: 1.5393
2022-08-25 06:24:02 - train: epoch 0064, iter [00300, 05004], lr: 0.030056, loss: 1.2753
2022-08-25 06:24:36 - train: epoch 0064, iter [00400, 05004], lr: 0.030027, loss: 1.5147
2022-08-25 06:25:10 - train: epoch 0064, iter [00500, 05004], lr: 0.029999, loss: 1.3499
2022-08-25 06:25:43 - train: epoch 0064, iter [00600, 05004], lr: 0.029970, loss: 1.6491
2022-08-25 06:26:17 - train: epoch 0064, iter [00700, 05004], lr: 0.029941, loss: 1.5415
2022-08-25 06:26:52 - train: epoch 0064, iter [00800, 05004], lr: 0.029912, loss: 1.3654
2022-08-25 06:27:26 - train: epoch 0064, iter [00900, 05004], lr: 0.029884, loss: 1.5226
2022-08-25 06:28:01 - train: epoch 0064, iter [01000, 05004], lr: 0.029855, loss: 1.4592
2022-08-25 06:28:35 - train: epoch 0064, iter [01100, 05004], lr: 0.029826, loss: 1.4692
2022-08-25 06:29:09 - train: epoch 0064, iter [01200, 05004], lr: 0.029797, loss: 1.3446
2022-08-25 06:29:44 - train: epoch 0064, iter [01300, 05004], lr: 0.029769, loss: 1.5831
2022-08-25 06:30:17 - train: epoch 0064, iter [01400, 05004], lr: 0.029740, loss: 1.8273
2022-08-25 06:30:51 - train: epoch 0064, iter [01500, 05004], lr: 0.029711, loss: 1.5394
2022-08-25 06:31:25 - train: epoch 0064, iter [01600, 05004], lr: 0.029683, loss: 1.4306
2022-08-25 06:31:59 - train: epoch 0064, iter [01700, 05004], lr: 0.029654, loss: 1.2630
2022-08-25 06:32:32 - train: epoch 0064, iter [01800, 05004], lr: 0.029625, loss: 1.3438
2022-08-25 06:33:07 - train: epoch 0064, iter [01900, 05004], lr: 0.029597, loss: 1.4028
2022-08-25 06:33:41 - train: epoch 0064, iter [02000, 05004], lr: 0.029568, loss: 1.4341
2022-08-25 06:34:14 - train: epoch 0064, iter [02100, 05004], lr: 0.029539, loss: 1.4681
2022-08-25 06:34:49 - train: epoch 0064, iter [02200, 05004], lr: 0.029511, loss: 1.6977
2022-08-25 06:35:23 - train: epoch 0064, iter [02300, 05004], lr: 0.029482, loss: 1.6266
2022-08-25 06:35:58 - train: epoch 0064, iter [02400, 05004], lr: 0.029453, loss: 1.5102
2022-08-25 06:36:32 - train: epoch 0064, iter [02500, 05004], lr: 0.029425, loss: 1.2809
2022-08-25 06:37:06 - train: epoch 0064, iter [02600, 05004], lr: 0.029396, loss: 1.3750
2022-08-25 06:37:41 - train: epoch 0064, iter [02700, 05004], lr: 0.029368, loss: 1.4921
2022-08-25 06:38:15 - train: epoch 0064, iter [02800, 05004], lr: 0.029339, loss: 1.3048
2022-08-25 06:38:50 - train: epoch 0064, iter [02900, 05004], lr: 0.029310, loss: 1.8476
2022-08-25 06:39:24 - train: epoch 0064, iter [03000, 05004], lr: 0.029282, loss: 1.5902
2022-08-25 06:39:59 - train: epoch 0064, iter [03100, 05004], lr: 0.029253, loss: 1.4304
2022-08-25 06:40:33 - train: epoch 0064, iter [03200, 05004], lr: 0.029225, loss: 1.4895
2022-08-25 06:41:07 - train: epoch 0064, iter [03300, 05004], lr: 0.029196, loss: 1.4221
2022-08-25 06:41:42 - train: epoch 0064, iter [03400, 05004], lr: 0.029168, loss: 1.6519
2022-08-25 06:42:17 - train: epoch 0064, iter [03500, 05004], lr: 0.029139, loss: 1.4980
2022-08-25 06:42:51 - train: epoch 0064, iter [03600, 05004], lr: 0.029111, loss: 1.4013
2022-08-25 06:43:25 - train: epoch 0064, iter [03700, 05004], lr: 0.029082, loss: 1.2214
2022-08-25 06:43:59 - train: epoch 0064, iter [03800, 05004], lr: 0.029054, loss: 1.6020
2022-08-25 06:44:34 - train: epoch 0064, iter [03900, 05004], lr: 0.029025, loss: 1.3958
2022-08-25 06:45:08 - train: epoch 0064, iter [04000, 05004], lr: 0.028997, loss: 1.3173
2022-08-25 06:45:43 - train: epoch 0064, iter [04100, 05004], lr: 0.028968, loss: 1.4864
2022-08-25 06:46:17 - train: epoch 0064, iter [04200, 05004], lr: 0.028940, loss: 1.4032
2022-08-25 06:46:52 - train: epoch 0064, iter [04300, 05004], lr: 0.028911, loss: 1.5466
2022-08-25 06:47:26 - train: epoch 0064, iter [04400, 05004], lr: 0.028883, loss: 1.4599
2022-08-25 06:48:01 - train: epoch 0064, iter [04500, 05004], lr: 0.028854, loss: 1.3657
2022-08-25 06:48:35 - train: epoch 0064, iter [04600, 05004], lr: 0.028826, loss: 1.7338
2022-08-25 06:49:10 - train: epoch 0064, iter [04700, 05004], lr: 0.028797, loss: 1.8446
2022-08-25 06:49:44 - train: epoch 0064, iter [04800, 05004], lr: 0.028769, loss: 1.5678
2022-08-25 06:50:18 - train: epoch 0064, iter [04900, 05004], lr: 0.028741, loss: 1.7322
2022-08-25 06:50:52 - train: epoch 0064, iter [05000, 05004], lr: 0.028712, loss: 1.3705
2022-08-25 06:50:53 - train: epoch 064, train_loss: 1.4999
2022-08-25 06:52:09 - eval: epoch: 064, acc1: 68.092%, acc5: 88.856%, test_loss: 1.2926, per_image_load_time: 1.846ms, per_image_inference_time: 0.627ms
2022-08-25 06:52:09 - until epoch: 064, best_acc1: 68.408%
2022-08-25 06:52:09 - epoch 065 lr: 0.028711
2022-08-25 06:52:48 - train: epoch 0065, iter [00100, 05004], lr: 0.028683, loss: 1.5052
2022-08-25 06:53:23 - train: epoch 0065, iter [00200, 05004], lr: 0.028654, loss: 1.4325
2022-08-25 06:53:57 - train: epoch 0065, iter [00300, 05004], lr: 0.028626, loss: 1.4822
2022-08-25 06:54:32 - train: epoch 0065, iter [00400, 05004], lr: 0.028597, loss: 1.6709
2022-08-25 06:55:06 - train: epoch 0065, iter [00500, 05004], lr: 0.028569, loss: 1.3340
2022-08-25 06:55:40 - train: epoch 0065, iter [00600, 05004], lr: 0.028541, loss: 1.5525
2022-08-25 06:56:15 - train: epoch 0065, iter [00700, 05004], lr: 0.028512, loss: 1.3995
2022-08-25 06:56:49 - train: epoch 0065, iter [00800, 05004], lr: 0.028484, loss: 1.4379
2022-08-25 06:57:23 - train: epoch 0065, iter [00900, 05004], lr: 0.028456, loss: 1.4601
2022-08-25 06:57:57 - train: epoch 0065, iter [01000, 05004], lr: 0.028427, loss: 1.4091
2022-08-25 06:58:30 - train: epoch 0065, iter [01100, 05004], lr: 0.028399, loss: 1.4458
2022-08-25 06:59:05 - train: epoch 0065, iter [01200, 05004], lr: 0.028371, loss: 1.6414
2022-08-25 06:59:39 - train: epoch 0065, iter [01300, 05004], lr: 0.028343, loss: 1.4064
2022-08-25 07:00:13 - train: epoch 0065, iter [01400, 05004], lr: 0.028314, loss: 1.3169
2022-08-25 07:00:48 - train: epoch 0065, iter [01500, 05004], lr: 0.028286, loss: 1.4510
2022-08-25 07:01:21 - train: epoch 0065, iter [01600, 05004], lr: 0.028258, loss: 1.5308
2022-08-25 07:01:56 - train: epoch 0065, iter [01700, 05004], lr: 0.028229, loss: 1.4967
2022-08-25 07:02:30 - train: epoch 0065, iter [01800, 05004], lr: 0.028201, loss: 1.4236
2022-08-25 07:03:04 - train: epoch 0065, iter [01900, 05004], lr: 0.028173, loss: 1.3568
2022-08-25 07:03:39 - train: epoch 0065, iter [02000, 05004], lr: 0.028145, loss: 1.4383
2022-08-25 07:04:13 - train: epoch 0065, iter [02100, 05004], lr: 0.028116, loss: 1.4283
2022-08-25 07:04:47 - train: epoch 0065, iter [02200, 05004], lr: 0.028088, loss: 1.5604
2022-08-25 07:05:21 - train: epoch 0065, iter [02300, 05004], lr: 0.028060, loss: 1.3907
2022-08-25 07:05:57 - train: epoch 0065, iter [02400, 05004], lr: 0.028032, loss: 1.5232
2022-08-25 07:06:31 - train: epoch 0065, iter [02500, 05004], lr: 0.028004, loss: 1.4965
2022-08-25 07:07:05 - train: epoch 0065, iter [02600, 05004], lr: 0.027975, loss: 1.6431
2022-08-25 07:07:39 - train: epoch 0065, iter [02700, 05004], lr: 0.027947, loss: 1.5667
2022-08-25 07:08:14 - train: epoch 0065, iter [02800, 05004], lr: 0.027919, loss: 1.4565
2022-08-25 07:08:49 - train: epoch 0065, iter [02900, 05004], lr: 0.027891, loss: 1.5723
2022-08-25 07:09:23 - train: epoch 0065, iter [03000, 05004], lr: 0.027863, loss: 1.4222
2022-08-25 07:09:57 - train: epoch 0065, iter [03100, 05004], lr: 0.027835, loss: 1.5497
2022-08-25 07:10:32 - train: epoch 0065, iter [03200, 05004], lr: 0.027806, loss: 1.6174
2022-08-25 07:11:07 - train: epoch 0065, iter [03300, 05004], lr: 0.027778, loss: 1.5469
2022-08-25 07:11:41 - train: epoch 0065, iter [03400, 05004], lr: 0.027750, loss: 1.3970
2022-08-25 07:12:15 - train: epoch 0065, iter [03500, 05004], lr: 0.027722, loss: 1.7406
2022-08-25 07:12:50 - train: epoch 0065, iter [03600, 05004], lr: 0.027694, loss: 1.6876
2022-08-25 07:13:24 - train: epoch 0065, iter [03700, 05004], lr: 0.027666, loss: 1.4435
2022-08-25 07:13:58 - train: epoch 0065, iter [03800, 05004], lr: 0.027638, loss: 1.3001
2022-08-25 07:14:33 - train: epoch 0065, iter [03900, 05004], lr: 0.027610, loss: 1.5867
2022-08-25 07:15:07 - train: epoch 0065, iter [04000, 05004], lr: 0.027582, loss: 1.5314
2022-08-25 07:15:41 - train: epoch 0065, iter [04100, 05004], lr: 0.027554, loss: 1.3148
2022-08-25 07:16:15 - train: epoch 0065, iter [04200, 05004], lr: 0.027526, loss: 1.4136
2022-08-25 07:16:50 - train: epoch 0065, iter [04300, 05004], lr: 0.027498, loss: 1.4226
2022-08-25 07:17:26 - train: epoch 0065, iter [04400, 05004], lr: 0.027470, loss: 1.5301
2022-08-25 07:18:00 - train: epoch 0065, iter [04500, 05004], lr: 0.027442, loss: 1.4511
2022-08-25 07:18:34 - train: epoch 0065, iter [04600, 05004], lr: 0.027414, loss: 1.5794
2022-08-25 07:19:08 - train: epoch 0065, iter [04700, 05004], lr: 0.027386, loss: 1.5281
2022-08-25 07:19:43 - train: epoch 0065, iter [04800, 05004], lr: 0.027358, loss: 1.3020
2022-08-25 07:20:18 - train: epoch 0065, iter [04900, 05004], lr: 0.027330, loss: 1.4046
2022-08-25 07:20:51 - train: epoch 0065, iter [05000, 05004], lr: 0.027302, loss: 1.5512
2022-08-25 07:20:52 - train: epoch 065, train_loss: 1.4830
2022-08-25 07:22:08 - eval: epoch: 065, acc1: 68.448%, acc5: 89.004%, test_loss: 1.2929, per_image_load_time: 1.603ms, per_image_inference_time: 0.607ms
2022-08-25 07:22:09 - until epoch: 065, best_acc1: 68.448%
2022-08-25 07:22:09 - epoch 066 lr: 0.027300
2022-08-25 07:22:49 - train: epoch 0066, iter [00100, 05004], lr: 0.027273, loss: 1.3021
2022-08-25 07:23:23 - train: epoch 0066, iter [00200, 05004], lr: 0.027245, loss: 1.6027
2022-08-25 07:23:57 - train: epoch 0066, iter [00300, 05004], lr: 0.027217, loss: 1.3997
2022-08-25 07:24:31 - train: epoch 0066, iter [00400, 05004], lr: 0.027189, loss: 1.2563
2022-08-25 07:25:05 - train: epoch 0066, iter [00500, 05004], lr: 0.027161, loss: 1.4958
2022-08-25 07:25:39 - train: epoch 0066, iter [00600, 05004], lr: 0.027133, loss: 1.4323
2022-08-25 07:26:13 - train: epoch 0066, iter [00700, 05004], lr: 0.027105, loss: 1.4811
2022-08-25 07:26:46 - train: epoch 0066, iter [00800, 05004], lr: 0.027077, loss: 1.8312
2022-08-25 07:27:19 - train: epoch 0066, iter [00900, 05004], lr: 0.027049, loss: 1.5867
2022-08-25 07:27:53 - train: epoch 0066, iter [01000, 05004], lr: 0.027021, loss: 1.4757
2022-08-25 07:28:26 - train: epoch 0066, iter [01100, 05004], lr: 0.026993, loss: 1.4682
2022-08-25 07:29:00 - train: epoch 0066, iter [01200, 05004], lr: 0.026965, loss: 1.6177
2022-08-25 07:29:33 - train: epoch 0066, iter [01300, 05004], lr: 0.026938, loss: 1.5360
2022-08-25 07:30:07 - train: epoch 0066, iter [01400, 05004], lr: 0.026910, loss: 1.2067
2022-08-25 07:30:40 - train: epoch 0066, iter [01500, 05004], lr: 0.026882, loss: 1.3400
2022-08-25 07:31:13 - train: epoch 0066, iter [01600, 05004], lr: 0.026854, loss: 1.5015
2022-08-25 07:31:47 - train: epoch 0066, iter [01700, 05004], lr: 0.026826, loss: 1.4077
2022-08-25 07:32:21 - train: epoch 0066, iter [01800, 05004], lr: 0.026798, loss: 1.2861
2022-08-25 07:32:55 - train: epoch 0066, iter [01900, 05004], lr: 0.026771, loss: 1.7483
2022-08-25 07:33:28 - train: epoch 0066, iter [02000, 05004], lr: 0.026743, loss: 1.5728
2022-08-25 07:34:01 - train: epoch 0066, iter [02100, 05004], lr: 0.026715, loss: 1.5096
2022-08-25 07:34:35 - train: epoch 0066, iter [02200, 05004], lr: 0.026687, loss: 1.4516
2022-08-25 07:35:09 - train: epoch 0066, iter [02300, 05004], lr: 0.026660, loss: 1.6348
2022-08-25 07:35:43 - train: epoch 0066, iter [02400, 05004], lr: 0.026632, loss: 1.3185
2022-08-25 07:36:17 - train: epoch 0066, iter [02500, 05004], lr: 0.026604, loss: 1.4306
2022-08-25 07:36:50 - train: epoch 0066, iter [02600, 05004], lr: 0.026576, loss: 1.4081
2022-08-25 07:37:24 - train: epoch 0066, iter [02700, 05004], lr: 0.026549, loss: 1.6741
2022-08-25 07:37:58 - train: epoch 0066, iter [02800, 05004], lr: 0.026521, loss: 1.6092
2022-08-25 07:38:31 - train: epoch 0066, iter [02900, 05004], lr: 0.026493, loss: 1.4981
2022-08-25 07:39:05 - train: epoch 0066, iter [03000, 05004], lr: 0.026465, loss: 1.4002
2022-08-25 07:39:38 - train: epoch 0066, iter [03100, 05004], lr: 0.026438, loss: 1.5040
2022-08-25 07:40:12 - train: epoch 0066, iter [03200, 05004], lr: 0.026410, loss: 1.2842
2022-08-25 07:40:46 - train: epoch 0066, iter [03300, 05004], lr: 0.026382, loss: 1.4276
2022-08-25 07:41:19 - train: epoch 0066, iter [03400, 05004], lr: 0.026355, loss: 1.7924
2022-08-25 07:41:53 - train: epoch 0066, iter [03500, 05004], lr: 0.026327, loss: 1.4394
2022-08-25 07:42:26 - train: epoch 0066, iter [03600, 05004], lr: 0.026299, loss: 1.5739
2022-08-25 07:43:00 - train: epoch 0066, iter [03700, 05004], lr: 0.026272, loss: 1.4696
2022-08-25 07:43:33 - train: epoch 0066, iter [03800, 05004], lr: 0.026244, loss: 1.3966
2022-08-25 07:44:06 - train: epoch 0066, iter [03900, 05004], lr: 0.026217, loss: 1.2136
2022-08-25 07:44:40 - train: epoch 0066, iter [04000, 05004], lr: 0.026189, loss: 1.6110
2022-08-25 07:45:14 - train: epoch 0066, iter [04100, 05004], lr: 0.026161, loss: 1.3232
2022-08-25 07:45:47 - train: epoch 0066, iter [04200, 05004], lr: 0.026134, loss: 1.2872
2022-08-25 07:46:21 - train: epoch 0066, iter [04300, 05004], lr: 0.026106, loss: 1.2413
2022-08-25 07:46:54 - train: epoch 0066, iter [04400, 05004], lr: 0.026079, loss: 1.4312
2022-08-25 07:47:28 - train: epoch 0066, iter [04500, 05004], lr: 0.026051, loss: 1.5799
2022-08-25 07:48:02 - train: epoch 0066, iter [04600, 05004], lr: 0.026024, loss: 1.6937
2022-08-25 07:48:35 - train: epoch 0066, iter [04700, 05004], lr: 0.025996, loss: 1.4285
2022-08-25 07:49:09 - train: epoch 0066, iter [04800, 05004], lr: 0.025968, loss: 1.4044
2022-08-25 07:49:43 - train: epoch 0066, iter [04900, 05004], lr: 0.025941, loss: 1.4535
2022-08-25 07:50:15 - train: epoch 0066, iter [05000, 05004], lr: 0.025913, loss: 1.4008
2022-08-25 07:50:17 - train: epoch 066, train_loss: 1.4670
2022-08-25 07:51:32 - eval: epoch: 066, acc1: 68.614%, acc5: 89.356%, test_loss: 1.2646, per_image_load_time: 2.181ms, per_image_inference_time: 0.590ms
2022-08-25 07:51:32 - until epoch: 066, best_acc1: 68.614%
2022-08-25 07:51:32 - epoch 067 lr: 0.025912
2022-08-25 07:52:12 - train: epoch 0067, iter [00100, 05004], lr: 0.025885, loss: 1.3484
2022-08-25 07:52:46 - train: epoch 0067, iter [00200, 05004], lr: 0.025857, loss: 1.3651
2022-08-25 07:53:20 - train: epoch 0067, iter [00300, 05004], lr: 0.025830, loss: 1.7047
2022-08-25 07:53:54 - train: epoch 0067, iter [00400, 05004], lr: 0.025802, loss: 1.4718
2022-08-25 07:54:27 - train: epoch 0067, iter [00500, 05004], lr: 0.025775, loss: 1.3810
2022-08-25 07:55:01 - train: epoch 0067, iter [00600, 05004], lr: 0.025747, loss: 1.3816
2022-08-25 07:55:35 - train: epoch 0067, iter [00700, 05004], lr: 0.025720, loss: 1.4859
2022-08-25 07:56:08 - train: epoch 0067, iter [00800, 05004], lr: 0.025693, loss: 1.5246
2022-08-25 07:56:42 - train: epoch 0067, iter [00900, 05004], lr: 0.025665, loss: 1.5104
2022-08-25 07:57:15 - train: epoch 0067, iter [01000, 05004], lr: 0.025638, loss: 1.3323
2022-08-25 07:57:49 - train: epoch 0067, iter [01100, 05004], lr: 0.025610, loss: 1.3639
2022-08-25 07:58:22 - train: epoch 0067, iter [01200, 05004], lr: 0.025583, loss: 1.3660
2022-08-25 07:58:56 - train: epoch 0067, iter [01300, 05004], lr: 0.025556, loss: 1.5990
2022-08-25 07:59:30 - train: epoch 0067, iter [01400, 05004], lr: 0.025528, loss: 1.4642
2022-08-25 08:00:04 - train: epoch 0067, iter [01500, 05004], lr: 0.025501, loss: 1.4349
2022-08-25 08:00:37 - train: epoch 0067, iter [01600, 05004], lr: 0.025473, loss: 1.4569
2022-08-25 08:01:11 - train: epoch 0067, iter [01700, 05004], lr: 0.025446, loss: 1.2672
2022-08-25 08:01:45 - train: epoch 0067, iter [01800, 05004], lr: 0.025419, loss: 1.6577
2022-08-25 08:02:18 - train: epoch 0067, iter [01900, 05004], lr: 0.025391, loss: 1.3017
2022-08-25 08:02:51 - train: epoch 0067, iter [02000, 05004], lr: 0.025364, loss: 1.5675
2022-08-25 08:03:25 - train: epoch 0067, iter [02100, 05004], lr: 0.025337, loss: 1.3600
2022-08-25 08:03:59 - train: epoch 0067, iter [02200, 05004], lr: 0.025309, loss: 1.4097
2022-08-25 08:04:33 - train: epoch 0067, iter [02300, 05004], lr: 0.025282, loss: 1.4500
2022-08-25 08:05:06 - train: epoch 0067, iter [02400, 05004], lr: 0.025255, loss: 1.4461
2022-08-25 08:05:40 - train: epoch 0067, iter [02500, 05004], lr: 0.025228, loss: 1.3510
2022-08-25 08:06:14 - train: epoch 0067, iter [02600, 05004], lr: 0.025200, loss: 1.3060
2022-08-25 08:06:47 - train: epoch 0067, iter [02700, 05004], lr: 0.025173, loss: 1.4464
2022-08-25 08:07:21 - train: epoch 0067, iter [02800, 05004], lr: 0.025146, loss: 1.5868
2022-08-25 08:07:55 - train: epoch 0067, iter [02900, 05004], lr: 0.025119, loss: 1.3853
2022-08-25 08:08:29 - train: epoch 0067, iter [03000, 05004], lr: 0.025091, loss: 1.3873
2022-08-25 08:09:03 - train: epoch 0067, iter [03100, 05004], lr: 0.025064, loss: 1.2552
2022-08-25 08:09:36 - train: epoch 0067, iter [03200, 05004], lr: 0.025037, loss: 1.5910
2022-08-25 08:10:10 - train: epoch 0067, iter [03300, 05004], lr: 0.025010, loss: 1.2482
2022-08-25 08:10:44 - train: epoch 0067, iter [03400, 05004], lr: 0.024983, loss: 1.4247
2022-08-25 08:11:18 - train: epoch 0067, iter [03500, 05004], lr: 0.024955, loss: 1.3215
2022-08-25 08:11:52 - train: epoch 0067, iter [03600, 05004], lr: 0.024928, loss: 1.5025
2022-08-25 08:12:26 - train: epoch 0067, iter [03700, 05004], lr: 0.024901, loss: 1.6217
2022-08-25 08:13:00 - train: epoch 0067, iter [03800, 05004], lr: 0.024874, loss: 1.3640
2022-08-25 08:13:33 - train: epoch 0067, iter [03900, 05004], lr: 0.024847, loss: 1.5609
2022-08-25 08:14:07 - train: epoch 0067, iter [04000, 05004], lr: 0.024820, loss: 1.5297
2022-08-25 08:14:41 - train: epoch 0067, iter [04100, 05004], lr: 0.024793, loss: 1.5297
2022-08-25 08:15:15 - train: epoch 0067, iter [04200, 05004], lr: 0.024765, loss: 1.7190
2022-08-25 08:15:49 - train: epoch 0067, iter [04300, 05004], lr: 0.024738, loss: 1.2425
2022-08-25 08:16:23 - train: epoch 0067, iter [04400, 05004], lr: 0.024711, loss: 1.6132
2022-08-25 08:16:58 - train: epoch 0067, iter [04500, 05004], lr: 0.024684, loss: 1.3679
2022-08-25 08:17:32 - train: epoch 0067, iter [04600, 05004], lr: 0.024657, loss: 1.1179
2022-08-25 08:18:06 - train: epoch 0067, iter [04700, 05004], lr: 0.024630, loss: 1.4194
2022-08-25 08:18:41 - train: epoch 0067, iter [04800, 05004], lr: 0.024603, loss: 1.2233
2022-08-25 08:19:15 - train: epoch 0067, iter [04900, 05004], lr: 0.024576, loss: 1.4444
2022-08-25 08:19:48 - train: epoch 0067, iter [05000, 05004], lr: 0.024549, loss: 1.5414
2022-08-25 08:19:50 - train: epoch 067, train_loss: 1.4490
2022-08-25 08:21:05 - eval: epoch: 067, acc1: 68.976%, acc5: 89.278%, test_loss: 1.2538, per_image_load_time: 2.146ms, per_image_inference_time: 0.630ms
2022-08-25 08:21:05 - until epoch: 067, best_acc1: 68.976%
2022-08-25 08:21:05 - epoch 068 lr: 0.024548
2022-08-25 08:21:44 - train: epoch 0068, iter [00100, 05004], lr: 0.024521, loss: 1.4821
2022-08-25 08:22:18 - train: epoch 0068, iter [00200, 05004], lr: 0.024494, loss: 1.4657
2022-08-25 08:22:53 - train: epoch 0068, iter [00300, 05004], lr: 0.024467, loss: 1.5000
2022-08-25 08:23:28 - train: epoch 0068, iter [00400, 05004], lr: 0.024440, loss: 1.4393
2022-08-25 08:24:02 - train: epoch 0068, iter [00500, 05004], lr: 0.024413, loss: 1.3055
2022-08-25 08:24:36 - train: epoch 0068, iter [00600, 05004], lr: 0.024386, loss: 1.4191
2022-08-25 08:25:11 - train: epoch 0068, iter [00700, 05004], lr: 0.024359, loss: 1.6987
2022-08-25 08:25:45 - train: epoch 0068, iter [00800, 05004], lr: 0.024332, loss: 1.4430
2022-08-25 08:26:19 - train: epoch 0068, iter [00900, 05004], lr: 0.024305, loss: 1.3728
2022-08-25 08:26:53 - train: epoch 0068, iter [01000, 05004], lr: 0.024278, loss: 1.3973
2022-08-25 08:27:28 - train: epoch 0068, iter [01100, 05004], lr: 0.024251, loss: 1.6666
2022-08-25 08:28:02 - train: epoch 0068, iter [01200, 05004], lr: 0.024224, loss: 1.4106
2022-08-25 08:28:37 - train: epoch 0068, iter [01300, 05004], lr: 0.024198, loss: 1.3309
2022-08-25 08:29:11 - train: epoch 0068, iter [01400, 05004], lr: 0.024171, loss: 1.5207
2022-08-25 08:29:45 - train: epoch 0068, iter [01500, 05004], lr: 0.024144, loss: 1.5652
2022-08-25 08:30:20 - train: epoch 0068, iter [01600, 05004], lr: 0.024117, loss: 1.4540
2022-08-25 08:30:55 - train: epoch 0068, iter [01700, 05004], lr: 0.024090, loss: 1.5725
2022-08-25 08:31:29 - train: epoch 0068, iter [01800, 05004], lr: 0.024063, loss: 1.5723
2022-08-25 08:32:04 - train: epoch 0068, iter [01900, 05004], lr: 0.024036, loss: 1.4044
2022-08-25 08:32:39 - train: epoch 0068, iter [02000, 05004], lr: 0.024010, loss: 1.3742
2022-08-25 08:33:13 - train: epoch 0068, iter [02100, 05004], lr: 0.023983, loss: 1.4194
2022-08-25 08:33:47 - train: epoch 0068, iter [02200, 05004], lr: 0.023956, loss: 1.5327
2022-08-25 08:34:22 - train: epoch 0068, iter [02300, 05004], lr: 0.023929, loss: 1.3189
2022-08-25 08:34:56 - train: epoch 0068, iter [02400, 05004], lr: 0.023902, loss: 1.5256
2022-08-25 08:35:30 - train: epoch 0068, iter [02500, 05004], lr: 0.023876, loss: 1.4725
2022-08-25 08:36:05 - train: epoch 0068, iter [02600, 05004], lr: 0.023849, loss: 1.3852
2022-08-25 08:36:39 - train: epoch 0068, iter [02700, 05004], lr: 0.023822, loss: 1.4556
2022-08-25 08:37:14 - train: epoch 0068, iter [02800, 05004], lr: 0.023795, loss: 1.6465
2022-08-25 08:37:49 - train: epoch 0068, iter [02900, 05004], lr: 0.023769, loss: 1.4600
2022-08-25 08:38:23 - train: epoch 0068, iter [03000, 05004], lr: 0.023742, loss: 1.6224
2022-08-25 08:38:57 - train: epoch 0068, iter [03100, 05004], lr: 0.023715, loss: 1.2852
2022-08-25 08:39:31 - train: epoch 0068, iter [03200, 05004], lr: 0.023689, loss: 1.5325
2022-08-25 08:40:05 - train: epoch 0068, iter [03300, 05004], lr: 0.023662, loss: 1.4103
2022-08-25 08:40:41 - train: epoch 0068, iter [03400, 05004], lr: 0.023635, loss: 1.3420
2022-08-25 08:41:15 - train: epoch 0068, iter [03500, 05004], lr: 0.023608, loss: 1.5408
2022-08-25 08:41:49 - train: epoch 0068, iter [03600, 05004], lr: 0.023582, loss: 1.3036
2022-08-25 08:42:24 - train: epoch 0068, iter [03700, 05004], lr: 0.023555, loss: 1.5346
2022-08-25 08:42:59 - train: epoch 0068, iter [03800, 05004], lr: 0.023529, loss: 1.5142
2022-08-25 08:43:34 - train: epoch 0068, iter [03900, 05004], lr: 0.023502, loss: 1.5606
2022-08-25 08:44:08 - train: epoch 0068, iter [04000, 05004], lr: 0.023475, loss: 1.5351
2022-08-25 08:44:43 - train: epoch 0068, iter [04100, 05004], lr: 0.023449, loss: 1.2653
2022-08-25 08:45:17 - train: epoch 0068, iter [04200, 05004], lr: 0.023422, loss: 1.6119
2022-08-25 08:45:52 - train: epoch 0068, iter [04300, 05004], lr: 0.023396, loss: 1.5384
2022-08-25 08:46:26 - train: epoch 0068, iter [04400, 05004], lr: 0.023369, loss: 1.3997
2022-08-25 08:47:00 - train: epoch 0068, iter [04500, 05004], lr: 0.023342, loss: 1.5484
2022-08-25 08:47:35 - train: epoch 0068, iter [04600, 05004], lr: 0.023316, loss: 1.5671
2022-08-25 08:48:10 - train: epoch 0068, iter [04700, 05004], lr: 0.023289, loss: 1.7637
2022-08-25 08:48:44 - train: epoch 0068, iter [04800, 05004], lr: 0.023263, loss: 1.6411
2022-08-25 08:49:19 - train: epoch 0068, iter [04900, 05004], lr: 0.023236, loss: 1.6839
2022-08-25 08:49:53 - train: epoch 0068, iter [05000, 05004], lr: 0.023210, loss: 1.3799
2022-08-25 08:49:54 - train: epoch 068, train_loss: 1.4322
2022-08-25 08:51:09 - eval: epoch: 068, acc1: 69.294%, acc5: 89.622%, test_loss: 1.2395, per_image_load_time: 2.316ms, per_image_inference_time: 0.588ms
2022-08-25 08:51:09 - until epoch: 068, best_acc1: 69.294%
2022-08-25 08:51:09 - epoch 069 lr: 0.023208
2022-08-25 08:51:48 - train: epoch 0069, iter [00100, 05004], lr: 0.023182, loss: 1.6734
2022-08-25 08:52:23 - train: epoch 0069, iter [00200, 05004], lr: 0.023156, loss: 1.5535
2022-08-25 08:52:57 - train: epoch 0069, iter [00300, 05004], lr: 0.023129, loss: 1.4898
2022-08-25 08:53:31 - train: epoch 0069, iter [00400, 05004], lr: 0.023103, loss: 1.3038
2022-08-25 08:54:06 - train: epoch 0069, iter [00500, 05004], lr: 0.023076, loss: 1.2814
2022-08-25 08:54:39 - train: epoch 0069, iter [00600, 05004], lr: 0.023050, loss: 1.2481
2022-08-25 08:55:12 - train: epoch 0069, iter [00700, 05004], lr: 0.023023, loss: 1.3768
2022-08-25 08:55:46 - train: epoch 0069, iter [00800, 05004], lr: 0.022997, loss: 1.4838
2022-08-25 08:56:20 - train: epoch 0069, iter [00900, 05004], lr: 0.022971, loss: 1.3655
2022-08-25 08:56:53 - train: epoch 0069, iter [01000, 05004], lr: 0.022944, loss: 1.3538
2022-08-25 08:57:27 - train: epoch 0069, iter [01100, 05004], lr: 0.022918, loss: 1.3741
2022-08-25 08:58:01 - train: epoch 0069, iter [01200, 05004], lr: 0.022891, loss: 1.2926
2022-08-25 08:58:35 - train: epoch 0069, iter [01300, 05004], lr: 0.022865, loss: 1.6248
2022-08-25 08:59:08 - train: epoch 0069, iter [01400, 05004], lr: 0.022839, loss: 1.5089
2022-08-25 08:59:42 - train: epoch 0069, iter [01500, 05004], lr: 0.022812, loss: 1.4824
2022-08-25 09:00:15 - train: epoch 0069, iter [01600, 05004], lr: 0.022786, loss: 1.5061
2022-08-25 09:00:49 - train: epoch 0069, iter [01700, 05004], lr: 0.022760, loss: 1.4041
2022-08-25 09:01:24 - train: epoch 0069, iter [01800, 05004], lr: 0.022733, loss: 1.2501
2022-08-25 09:01:57 - train: epoch 0069, iter [01900, 05004], lr: 0.022707, loss: 1.4252
2022-08-25 09:02:31 - train: epoch 0069, iter [02000, 05004], lr: 0.022681, loss: 1.2554
2022-08-25 09:03:05 - train: epoch 0069, iter [02100, 05004], lr: 0.022654, loss: 1.4727
2022-08-25 09:03:39 - train: epoch 0069, iter [02200, 05004], lr: 0.022628, loss: 1.4804
2022-08-25 09:04:12 - train: epoch 0069, iter [02300, 05004], lr: 0.022602, loss: 1.3631
2022-08-25 09:04:46 - train: epoch 0069, iter [02400, 05004], lr: 0.022576, loss: 1.5279
2022-08-25 09:05:20 - train: epoch 0069, iter [02500, 05004], lr: 0.022549, loss: 1.3869
2022-08-25 09:05:54 - train: epoch 0069, iter [02600, 05004], lr: 0.022523, loss: 1.5593
2022-08-25 09:06:28 - train: epoch 0069, iter [02700, 05004], lr: 0.022497, loss: 1.6320
2022-08-25 09:07:02 - train: epoch 0069, iter [02800, 05004], lr: 0.022471, loss: 1.5312
2022-08-25 09:07:36 - train: epoch 0069, iter [02900, 05004], lr: 0.022445, loss: 1.2773
2022-08-25 09:08:09 - train: epoch 0069, iter [03000, 05004], lr: 0.022418, loss: 1.3526
2022-08-25 09:08:43 - train: epoch 0069, iter [03100, 05004], lr: 0.022392, loss: 1.3739
2022-08-25 09:09:17 - train: epoch 0069, iter [03200, 05004], lr: 0.022366, loss: 1.2617
2022-08-25 09:09:51 - train: epoch 0069, iter [03300, 05004], lr: 0.022340, loss: 1.3609
2022-08-25 09:10:25 - train: epoch 0069, iter [03400, 05004], lr: 0.022314, loss: 1.2754
2022-08-25 09:10:59 - train: epoch 0069, iter [03500, 05004], lr: 0.022288, loss: 1.3276
2022-08-25 09:11:32 - train: epoch 0069, iter [03600, 05004], lr: 0.022261, loss: 1.3087
2022-08-25 09:12:06 - train: epoch 0069, iter [03700, 05004], lr: 0.022235, loss: 1.5021
2022-08-25 09:12:40 - train: epoch 0069, iter [03800, 05004], lr: 0.022209, loss: 1.4339
2022-08-25 09:13:14 - train: epoch 0069, iter [03900, 05004], lr: 0.022183, loss: 1.5364
2022-08-25 09:13:48 - train: epoch 0069, iter [04000, 05004], lr: 0.022157, loss: 1.5446
2022-08-25 09:14:22 - train: epoch 0069, iter [04100, 05004], lr: 0.022131, loss: 1.5827
2022-08-25 09:14:56 - train: epoch 0069, iter [04200, 05004], lr: 0.022105, loss: 1.2261
2022-08-25 09:15:29 - train: epoch 0069, iter [04300, 05004], lr: 0.022079, loss: 1.3734
2022-08-25 09:16:03 - train: epoch 0069, iter [04400, 05004], lr: 0.022053, loss: 1.3938
2022-08-25 09:16:38 - train: epoch 0069, iter [04500, 05004], lr: 0.022027, loss: 1.4301
2022-08-25 09:17:11 - train: epoch 0069, iter [04600, 05004], lr: 0.022001, loss: 1.5876
2022-08-25 09:17:45 - train: epoch 0069, iter [04700, 05004], lr: 0.021975, loss: 1.4956
2022-08-25 09:18:19 - train: epoch 0069, iter [04800, 05004], lr: 0.021949, loss: 1.4026
2022-08-25 09:18:54 - train: epoch 0069, iter [04900, 05004], lr: 0.021923, loss: 1.3555
2022-08-25 09:19:26 - train: epoch 0069, iter [05000, 05004], lr: 0.021897, loss: 1.5225
2022-08-25 09:19:28 - train: epoch 069, train_loss: 1.4141
2022-08-25 09:20:43 - eval: epoch: 069, acc1: 69.096%, acc5: 89.390%, test_loss: 1.2455, per_image_load_time: 1.206ms, per_image_inference_time: 0.561ms
2022-08-25 09:20:43 - until epoch: 069, best_acc1: 69.294%
2022-08-25 09:20:43 - epoch 070 lr: 0.021896
2022-08-25 09:21:22 - train: epoch 0070, iter [00100, 05004], lr: 0.021870, loss: 1.4601
2022-08-25 09:21:57 - train: epoch 0070, iter [00200, 05004], lr: 0.021844, loss: 1.5004
2022-08-25 09:22:31 - train: epoch 0070, iter [00300, 05004], lr: 0.021818, loss: 1.5202
2022-08-25 09:23:05 - train: epoch 0070, iter [00400, 05004], lr: 0.021792, loss: 1.3872
2022-08-25 09:23:39 - train: epoch 0070, iter [00500, 05004], lr: 0.021766, loss: 1.4218
2022-08-25 09:24:12 - train: epoch 0070, iter [00600, 05004], lr: 0.021740, loss: 1.2866
2022-08-25 09:24:46 - train: epoch 0070, iter [00700, 05004], lr: 0.021714, loss: 1.3568
2022-08-25 09:25:20 - train: epoch 0070, iter [00800, 05004], lr: 0.021688, loss: 1.3320
2022-08-25 09:25:54 - train: epoch 0070, iter [00900, 05004], lr: 0.021663, loss: 1.4738
2022-08-25 09:26:28 - train: epoch 0070, iter [01000, 05004], lr: 0.021637, loss: 1.3217
2022-08-25 09:27:02 - train: epoch 0070, iter [01100, 05004], lr: 0.021611, loss: 1.7257
2022-08-25 09:27:36 - train: epoch 0070, iter [01200, 05004], lr: 0.021585, loss: 1.2002
2022-08-25 09:28:10 - train: epoch 0070, iter [01300, 05004], lr: 0.021559, loss: 1.3202
2022-08-25 09:28:44 - train: epoch 0070, iter [01400, 05004], lr: 0.021533, loss: 1.5010
2022-08-25 09:29:17 - train: epoch 0070, iter [01500, 05004], lr: 0.021508, loss: 1.3564
2022-08-25 09:29:52 - train: epoch 0070, iter [01600, 05004], lr: 0.021482, loss: 1.4508
2022-08-25 09:30:27 - train: epoch 0070, iter [01700, 05004], lr: 0.021456, loss: 1.4649
2022-08-25 09:31:01 - train: epoch 0070, iter [01800, 05004], lr: 0.021430, loss: 1.2560
2022-08-25 09:31:35 - train: epoch 0070, iter [01900, 05004], lr: 0.021405, loss: 1.2644
2022-08-25 09:32:09 - train: epoch 0070, iter [02000, 05004], lr: 0.021379, loss: 1.3729
2022-08-25 09:32:44 - train: epoch 0070, iter [02100, 05004], lr: 0.021353, loss: 1.4105
2022-08-25 09:33:18 - train: epoch 0070, iter [02200, 05004], lr: 0.021327, loss: 1.4158
2022-08-25 09:33:52 - train: epoch 0070, iter [02300, 05004], lr: 0.021302, loss: 1.2924
2022-08-25 09:34:26 - train: epoch 0070, iter [02400, 05004], lr: 0.021276, loss: 1.4563
2022-08-25 09:35:00 - train: epoch 0070, iter [02500, 05004], lr: 0.021250, loss: 1.4388
2022-08-25 09:35:34 - train: epoch 0070, iter [02600, 05004], lr: 0.021225, loss: 1.3494
2022-08-25 09:36:08 - train: epoch 0070, iter [02700, 05004], lr: 0.021199, loss: 1.4413
2022-08-25 09:36:43 - train: epoch 0070, iter [02800, 05004], lr: 0.021173, loss: 1.4685
2022-08-25 09:37:17 - train: epoch 0070, iter [02900, 05004], lr: 0.021148, loss: 1.6189
2022-08-25 09:37:51 - train: epoch 0070, iter [03000, 05004], lr: 0.021122, loss: 1.4019
2022-08-25 09:38:25 - train: epoch 0070, iter [03100, 05004], lr: 0.021096, loss: 1.4131
2022-08-25 09:39:00 - train: epoch 0070, iter [03200, 05004], lr: 0.021071, loss: 1.5612
2022-08-25 09:39:34 - train: epoch 0070, iter [03300, 05004], lr: 0.021045, loss: 1.3629
2022-08-25 09:40:09 - train: epoch 0070, iter [03400, 05004], lr: 0.021020, loss: 1.4059
2022-08-25 09:40:43 - train: epoch 0070, iter [03500, 05004], lr: 0.020994, loss: 1.3855
2022-08-25 09:41:17 - train: epoch 0070, iter [03600, 05004], lr: 0.020968, loss: 1.4892
2022-08-25 09:41:52 - train: epoch 0070, iter [03700, 05004], lr: 0.020943, loss: 1.4402
2022-08-25 09:42:25 - train: epoch 0070, iter [03800, 05004], lr: 0.020917, loss: 1.4381
2022-08-25 09:43:00 - train: epoch 0070, iter [03900, 05004], lr: 0.020892, loss: 1.1910
2022-08-25 09:43:34 - train: epoch 0070, iter [04000, 05004], lr: 0.020866, loss: 1.2842
2022-08-25 09:44:08 - train: epoch 0070, iter [04100, 05004], lr: 0.020841, loss: 1.2487
2022-08-25 09:44:43 - train: epoch 0070, iter [04200, 05004], lr: 0.020815, loss: 1.3548
2022-08-25 09:45:17 - train: epoch 0070, iter [04300, 05004], lr: 0.020790, loss: 1.4873
2022-08-25 09:45:51 - train: epoch 0070, iter [04400, 05004], lr: 0.020764, loss: 1.4629
2022-08-25 09:46:25 - train: epoch 0070, iter [04500, 05004], lr: 0.020739, loss: 1.4743
2022-08-25 09:46:59 - train: epoch 0070, iter [04600, 05004], lr: 0.020713, loss: 1.5373
2022-08-25 09:47:34 - train: epoch 0070, iter [04700, 05004], lr: 0.020688, loss: 1.4092
2022-08-25 09:48:08 - train: epoch 0070, iter [04800, 05004], lr: 0.020663, loss: 1.4921
2022-08-25 09:48:44 - train: epoch 0070, iter [04900, 05004], lr: 0.020637, loss: 1.5575
2022-08-25 09:49:16 - train: epoch 0070, iter [05000, 05004], lr: 0.020612, loss: 1.4559
2022-08-25 09:49:18 - train: epoch 070, train_loss: 1.3988
2022-08-25 09:50:34 - eval: epoch: 070, acc1: 69.022%, acc5: 89.352%, test_loss: 1.2489, per_image_load_time: 1.759ms, per_image_inference_time: 0.605ms
2022-08-25 09:50:34 - until epoch: 070, best_acc1: 69.294%
2022-08-25 09:50:34 - epoch 071 lr: 0.020610
2022-08-25 09:51:14 - train: epoch 0071, iter [00100, 05004], lr: 0.020585, loss: 1.2153
2022-08-25 09:51:48 - train: epoch 0071, iter [00200, 05004], lr: 0.020560, loss: 1.3281
2022-08-25 09:52:22 - train: epoch 0071, iter [00300, 05004], lr: 0.020535, loss: 1.3268
2022-08-25 09:52:57 - train: epoch 0071, iter [00400, 05004], lr: 0.020509, loss: 1.4251
2022-08-25 09:53:30 - train: epoch 0071, iter [00500, 05004], lr: 0.020484, loss: 1.2755
2022-08-25 09:54:04 - train: epoch 0071, iter [00600, 05004], lr: 0.020459, loss: 1.4545
2022-08-25 09:54:38 - train: epoch 0071, iter [00700, 05004], lr: 0.020433, loss: 1.5217
2022-08-25 09:55:13 - train: epoch 0071, iter [00800, 05004], lr: 0.020408, loss: 1.3518
2022-08-25 09:55:47 - train: epoch 0071, iter [00900, 05004], lr: 0.020383, loss: 1.5483
2022-08-25 09:56:20 - train: epoch 0071, iter [01000, 05004], lr: 0.020357, loss: 1.3734
2022-08-25 09:56:55 - train: epoch 0071, iter [01100, 05004], lr: 0.020332, loss: 1.5355
2022-08-25 09:57:30 - train: epoch 0071, iter [01200, 05004], lr: 0.020307, loss: 1.3289
2022-08-25 09:58:04 - train: epoch 0071, iter [01300, 05004], lr: 0.020282, loss: 1.4459
2022-08-25 09:58:38 - train: epoch 0071, iter [01400, 05004], lr: 0.020256, loss: 1.2941
2022-08-25 09:59:13 - train: epoch 0071, iter [01500, 05004], lr: 0.020231, loss: 1.1410
2022-08-25 09:59:47 - train: epoch 0071, iter [01600, 05004], lr: 0.020206, loss: 1.2224
2022-08-25 10:00:21 - train: epoch 0071, iter [01700, 05004], lr: 0.020181, loss: 1.4865
2022-08-25 10:00:55 - train: epoch 0071, iter [01800, 05004], lr: 0.020156, loss: 1.3623
2022-08-25 10:01:30 - train: epoch 0071, iter [01900, 05004], lr: 0.020130, loss: 1.3949
2022-08-25 10:02:04 - train: epoch 0071, iter [02000, 05004], lr: 0.020105, loss: 1.4646
2022-08-25 10:02:38 - train: epoch 0071, iter [02100, 05004], lr: 0.020080, loss: 1.1641
2022-08-25 10:03:12 - train: epoch 0071, iter [02200, 05004], lr: 0.020055, loss: 1.2736
2022-08-25 10:03:46 - train: epoch 0071, iter [02300, 05004], lr: 0.020030, loss: 1.3639
2022-08-25 10:04:21 - train: epoch 0071, iter [02400, 05004], lr: 0.020005, loss: 1.3191
2022-08-25 10:04:55 - train: epoch 0071, iter [02500, 05004], lr: 0.019979, loss: 1.4574
2022-08-25 10:05:30 - train: epoch 0071, iter [02600, 05004], lr: 0.019954, loss: 1.2555
2022-08-25 10:06:03 - train: epoch 0071, iter [02700, 05004], lr: 0.019929, loss: 1.4195
2022-08-25 10:06:37 - train: epoch 0071, iter [02800, 05004], lr: 0.019904, loss: 1.4337
2022-08-25 10:07:11 - train: epoch 0071, iter [02900, 05004], lr: 0.019879, loss: 1.3600
2022-08-25 10:07:46 - train: epoch 0071, iter [03000, 05004], lr: 0.019854, loss: 1.4876
2022-08-25 10:08:20 - train: epoch 0071, iter [03100, 05004], lr: 0.019829, loss: 1.2888
2022-08-25 10:08:55 - train: epoch 0071, iter [03200, 05004], lr: 0.019804, loss: 1.2290
2022-08-25 10:09:29 - train: epoch 0071, iter [03300, 05004], lr: 0.019779, loss: 1.2245
2022-08-25 10:10:03 - train: epoch 0071, iter [03400, 05004], lr: 0.019754, loss: 1.3553
2022-08-25 10:10:37 - train: epoch 0071, iter [03500, 05004], lr: 0.019729, loss: 1.5316
2022-08-25 10:11:11 - train: epoch 0071, iter [03600, 05004], lr: 0.019704, loss: 1.5772
2022-08-25 10:11:45 - train: epoch 0071, iter [03700, 05004], lr: 0.019679, loss: 1.3960
2022-08-25 10:12:20 - train: epoch 0071, iter [03800, 05004], lr: 0.019654, loss: 1.3107
2022-08-25 10:12:53 - train: epoch 0071, iter [03900, 05004], lr: 0.019629, loss: 1.4490
2022-08-25 10:13:28 - train: epoch 0071, iter [04000, 05004], lr: 0.019604, loss: 1.6464
2022-08-25 10:14:02 - train: epoch 0071, iter [04100, 05004], lr: 0.019579, loss: 1.3935
2022-08-25 10:14:36 - train: epoch 0071, iter [04200, 05004], lr: 0.019554, loss: 1.5545
2022-08-25 10:15:10 - train: epoch 0071, iter [04300, 05004], lr: 0.019530, loss: 1.4486
2022-08-25 10:15:45 - train: epoch 0071, iter [04400, 05004], lr: 0.019505, loss: 1.4960
2022-08-25 10:16:19 - train: epoch 0071, iter [04500, 05004], lr: 0.019480, loss: 1.3540
2022-08-25 10:16:53 - train: epoch 0071, iter [04600, 05004], lr: 0.019455, loss: 1.4841
2022-08-25 10:17:28 - train: epoch 0071, iter [04700, 05004], lr: 0.019430, loss: 1.3218
2022-08-25 10:18:02 - train: epoch 0071, iter [04800, 05004], lr: 0.019405, loss: 1.2006
2022-08-25 10:18:37 - train: epoch 0071, iter [04900, 05004], lr: 0.019380, loss: 1.1844
2022-08-25 10:19:10 - train: epoch 0071, iter [05000, 05004], lr: 0.019356, loss: 1.1785
2022-08-25 10:19:11 - train: epoch 071, train_loss: 1.3792
2022-08-25 10:20:26 - eval: epoch: 071, acc1: 70.228%, acc5: 89.976%, test_loss: 1.2058, per_image_load_time: 0.726ms, per_image_inference_time: 0.576ms
2022-08-25 10:20:27 - until epoch: 071, best_acc1: 70.228%
2022-08-25 10:20:27 - epoch 072 lr: 0.019354
2022-08-25 10:21:06 - train: epoch 0072, iter [00100, 05004], lr: 0.019330, loss: 1.4266
2022-08-25 10:21:40 - train: epoch 0072, iter [00200, 05004], lr: 0.019305, loss: 1.1816
2022-08-25 10:22:14 - train: epoch 0072, iter [00300, 05004], lr: 0.019280, loss: 1.2856
2022-08-25 10:22:48 - train: epoch 0072, iter [00400, 05004], lr: 0.019256, loss: 1.2167
2022-08-25 10:23:22 - train: epoch 0072, iter [00500, 05004], lr: 0.019231, loss: 1.2665
2022-08-25 10:23:57 - train: epoch 0072, iter [00600, 05004], lr: 0.019206, loss: 1.2656
2022-08-25 10:24:30 - train: epoch 0072, iter [00700, 05004], lr: 0.019181, loss: 1.3692
2022-08-25 10:25:05 - train: epoch 0072, iter [00800, 05004], lr: 0.019157, loss: 1.5622
2022-08-25 10:25:39 - train: epoch 0072, iter [00900, 05004], lr: 0.019132, loss: 1.2352
2022-08-25 10:26:13 - train: epoch 0072, iter [01000, 05004], lr: 0.019107, loss: 1.2948
2022-08-25 10:26:48 - train: epoch 0072, iter [01100, 05004], lr: 0.019083, loss: 1.4566
2022-08-25 10:27:23 - train: epoch 0072, iter [01200, 05004], lr: 0.019058, loss: 1.2799
2022-08-25 10:27:57 - train: epoch 0072, iter [01300, 05004], lr: 0.019033, loss: 1.4220
2022-08-25 10:28:31 - train: epoch 0072, iter [01400, 05004], lr: 0.019009, loss: 1.4033
2022-08-25 10:29:06 - train: epoch 0072, iter [01500, 05004], lr: 0.018984, loss: 1.3766
2022-08-25 10:29:40 - train: epoch 0072, iter [01600, 05004], lr: 0.018959, loss: 1.3954
2022-08-25 10:30:14 - train: epoch 0072, iter [01700, 05004], lr: 0.018935, loss: 1.3002
2022-08-25 10:30:49 - train: epoch 0072, iter [01800, 05004], lr: 0.018910, loss: 1.2128
2022-08-25 10:31:23 - train: epoch 0072, iter [01900, 05004], lr: 0.018886, loss: 1.2215
2022-08-25 10:31:58 - train: epoch 0072, iter [02000, 05004], lr: 0.018861, loss: 1.4785
2022-08-25 10:32:32 - train: epoch 0072, iter [02100, 05004], lr: 0.018836, loss: 1.4435
2022-08-25 10:33:07 - train: epoch 0072, iter [02200, 05004], lr: 0.018812, loss: 1.4664
2022-08-25 10:33:41 - train: epoch 0072, iter [02300, 05004], lr: 0.018787, loss: 1.4980
2022-08-25 10:34:15 - train: epoch 0072, iter [02400, 05004], lr: 0.018763, loss: 1.3594
2022-08-25 10:34:49 - train: epoch 0072, iter [02500, 05004], lr: 0.018738, loss: 1.1639
2022-08-25 10:35:23 - train: epoch 0072, iter [02600, 05004], lr: 0.018714, loss: 1.1635
2022-08-25 10:35:57 - train: epoch 0072, iter [02700, 05004], lr: 0.018689, loss: 1.4184
2022-08-25 10:36:32 - train: epoch 0072, iter [02800, 05004], lr: 0.018665, loss: 1.4945
2022-08-25 10:37:06 - train: epoch 0072, iter [02900, 05004], lr: 0.018640, loss: 1.2601
2022-08-25 10:37:41 - train: epoch 0072, iter [03000, 05004], lr: 0.018616, loss: 1.3464
2022-08-25 10:38:15 - train: epoch 0072, iter [03100, 05004], lr: 0.018592, loss: 1.5597
2022-08-25 10:38:50 - train: epoch 0072, iter [03200, 05004], lr: 0.018567, loss: 1.2934
2022-08-25 10:39:24 - train: epoch 0072, iter [03300, 05004], lr: 0.018543, loss: 1.2873
2022-08-25 10:39:59 - train: epoch 0072, iter [03400, 05004], lr: 0.018518, loss: 1.5178
2022-08-25 10:40:32 - train: epoch 0072, iter [03500, 05004], lr: 0.018494, loss: 1.3894
2022-08-25 10:41:07 - train: epoch 0072, iter [03600, 05004], lr: 0.018470, loss: 1.3846
2022-08-25 10:41:42 - train: epoch 0072, iter [03700, 05004], lr: 0.018445, loss: 1.4296
2022-08-25 10:42:16 - train: epoch 0072, iter [03800, 05004], lr: 0.018421, loss: 1.4190
2022-08-25 10:42:50 - train: epoch 0072, iter [03900, 05004], lr: 0.018397, loss: 1.4148
2022-08-25 10:43:25 - train: epoch 0072, iter [04000, 05004], lr: 0.018372, loss: 1.3613
2022-08-25 10:43:59 - train: epoch 0072, iter [04100, 05004], lr: 0.018348, loss: 1.5234
2022-08-25 10:44:33 - train: epoch 0072, iter [04200, 05004], lr: 0.018324, loss: 1.3446
2022-08-25 10:45:08 - train: epoch 0072, iter [04300, 05004], lr: 0.018299, loss: 1.3382
2022-08-25 10:45:42 - train: epoch 0072, iter [04400, 05004], lr: 0.018275, loss: 1.1916
2022-08-25 10:46:17 - train: epoch 0072, iter [04500, 05004], lr: 0.018251, loss: 1.6171
2022-08-25 10:46:51 - train: epoch 0072, iter [04600, 05004], lr: 0.018227, loss: 1.2513
2022-08-25 10:47:26 - train: epoch 0072, iter [04700, 05004], lr: 0.018202, loss: 1.4453
2022-08-25 10:48:00 - train: epoch 0072, iter [04800, 05004], lr: 0.018178, loss: 1.6278
2022-08-25 10:48:35 - train: epoch 0072, iter [04900, 05004], lr: 0.018154, loss: 1.3732
2022-08-25 10:49:08 - train: epoch 0072, iter [05000, 05004], lr: 0.018130, loss: 1.3253
2022-08-25 10:49:10 - train: epoch 072, train_loss: 1.3609
2022-08-25 10:50:25 - eval: epoch: 072, acc1: 70.648%, acc5: 90.182%, test_loss: 1.1912, per_image_load_time: 0.945ms, per_image_inference_time: 0.580ms
2022-08-25 10:50:26 - until epoch: 072, best_acc1: 70.648%
2022-08-25 10:50:26 - epoch 073 lr: 0.018129
2022-08-25 10:51:05 - train: epoch 0073, iter [00100, 05004], lr: 0.018105, loss: 1.4999
2022-08-25 10:51:39 - train: epoch 0073, iter [00200, 05004], lr: 0.018080, loss: 1.3353
2022-08-25 10:52:13 - train: epoch 0073, iter [00300, 05004], lr: 0.018056, loss: 1.4965
2022-08-25 10:52:48 - train: epoch 0073, iter [00400, 05004], lr: 0.018032, loss: 1.0255
2022-08-25 10:53:22 - train: epoch 0073, iter [00500, 05004], lr: 0.018008, loss: 1.2402
2022-08-25 10:53:56 - train: epoch 0073, iter [00600, 05004], lr: 0.017984, loss: 1.2560
2022-08-25 10:54:30 - train: epoch 0073, iter [00700, 05004], lr: 0.017960, loss: 1.3279
2022-08-25 10:55:05 - train: epoch 0073, iter [00800, 05004], lr: 0.017936, loss: 1.2552
2022-08-25 10:55:39 - train: epoch 0073, iter [00900, 05004], lr: 0.017912, loss: 1.1579
2022-08-25 10:56:13 - train: epoch 0073, iter [01000, 05004], lr: 0.017888, loss: 1.2693
2022-08-25 10:56:47 - train: epoch 0073, iter [01100, 05004], lr: 0.017864, loss: 1.4919
2022-08-25 10:57:21 - train: epoch 0073, iter [01200, 05004], lr: 0.017839, loss: 1.3188
2022-08-25 10:57:55 - train: epoch 0073, iter [01300, 05004], lr: 0.017815, loss: 1.3182
2022-08-25 10:58:29 - train: epoch 0073, iter [01400, 05004], lr: 0.017791, loss: 1.2760
2022-08-25 10:59:03 - train: epoch 0073, iter [01500, 05004], lr: 0.017767, loss: 1.3702
2022-08-25 10:59:37 - train: epoch 0073, iter [01600, 05004], lr: 0.017743, loss: 1.3823
2022-08-25 11:00:11 - train: epoch 0073, iter [01700, 05004], lr: 0.017719, loss: 1.4899
2022-08-25 11:00:46 - train: epoch 0073, iter [01800, 05004], lr: 0.017695, loss: 1.1432
2022-08-25 11:01:20 - train: epoch 0073, iter [01900, 05004], lr: 0.017672, loss: 1.4304
2022-08-25 11:01:54 - train: epoch 0073, iter [02000, 05004], lr: 0.017648, loss: 1.0389
2022-08-25 11:02:28 - train: epoch 0073, iter [02100, 05004], lr: 0.017624, loss: 1.3225
2022-08-25 11:03:02 - train: epoch 0073, iter [02200, 05004], lr: 0.017600, loss: 1.5392
2022-08-25 11:03:36 - train: epoch 0073, iter [02300, 05004], lr: 0.017576, loss: 1.4880
2022-08-25 11:04:11 - train: epoch 0073, iter [02400, 05004], lr: 0.017552, loss: 1.3136
2022-08-25 11:04:45 - train: epoch 0073, iter [02500, 05004], lr: 0.017528, loss: 1.5215
2022-08-25 11:05:19 - train: epoch 0073, iter [02600, 05004], lr: 0.017504, loss: 1.3985
2022-08-25 11:05:54 - train: epoch 0073, iter [02700, 05004], lr: 0.017480, loss: 1.3604
2022-08-25 11:06:29 - train: epoch 0073, iter [02800, 05004], lr: 0.017457, loss: 1.3349
2022-08-25 11:07:03 - train: epoch 0073, iter [02900, 05004], lr: 0.017433, loss: 1.4335
2022-08-25 11:07:37 - train: epoch 0073, iter [03000, 05004], lr: 0.017409, loss: 1.2226
2022-08-25 11:08:11 - train: epoch 0073, iter [03100, 05004], lr: 0.017385, loss: 1.2778
2022-08-25 11:08:46 - train: epoch 0073, iter [03200, 05004], lr: 0.017361, loss: 1.2088
2022-08-25 11:09:20 - train: epoch 0073, iter [03300, 05004], lr: 0.017338, loss: 1.3367
2022-08-25 11:09:55 - train: epoch 0073, iter [03400, 05004], lr: 0.017314, loss: 1.3444
2022-08-25 11:10:29 - train: epoch 0073, iter [03500, 05004], lr: 0.017290, loss: 1.4319
2022-08-25 11:11:04 - train: epoch 0073, iter [03600, 05004], lr: 0.017266, loss: 1.2421
2022-08-25 11:11:38 - train: epoch 0073, iter [03700, 05004], lr: 0.017243, loss: 1.4272
2022-08-25 11:12:12 - train: epoch 0073, iter [03800, 05004], lr: 0.017219, loss: 1.5643
2022-08-25 11:12:46 - train: epoch 0073, iter [03900, 05004], lr: 0.017195, loss: 1.2689
2022-08-25 11:13:20 - train: epoch 0073, iter [04000, 05004], lr: 0.017171, loss: 1.4984
2022-08-25 11:13:54 - train: epoch 0073, iter [04100, 05004], lr: 0.017148, loss: 1.3133
2022-08-25 11:14:29 - train: epoch 0073, iter [04200, 05004], lr: 0.017124, loss: 1.4947
2022-08-25 11:15:03 - train: epoch 0073, iter [04300, 05004], lr: 0.017100, loss: 1.3426
2022-08-25 11:15:38 - train: epoch 0073, iter [04400, 05004], lr: 0.017077, loss: 1.3962
2022-08-25 11:16:12 - train: epoch 0073, iter [04500, 05004], lr: 0.017053, loss: 1.2674
2022-08-25 11:16:46 - train: epoch 0073, iter [04600, 05004], lr: 0.017030, loss: 1.4936
2022-08-25 11:17:21 - train: epoch 0073, iter [04700, 05004], lr: 0.017006, loss: 1.1799
2022-08-25 11:17:55 - train: epoch 0073, iter [04800, 05004], lr: 0.016982, loss: 1.4822
2022-08-25 11:18:29 - train: epoch 0073, iter [04900, 05004], lr: 0.016959, loss: 1.2525
2022-08-25 11:19:03 - train: epoch 0073, iter [05000, 05004], lr: 0.016935, loss: 1.3793
2022-08-25 11:19:04 - train: epoch 073, train_loss: 1.3427
2022-08-25 11:20:21 - eval: epoch: 073, acc1: 71.022%, acc5: 90.432%, test_loss: 1.1699, per_image_load_time: 0.918ms, per_image_inference_time: 0.569ms
2022-08-25 11:20:21 - until epoch: 073, best_acc1: 71.022%
2022-08-25 11:20:21 - epoch 074 lr: 0.016934
2022-08-25 11:21:01 - train: epoch 0074, iter [00100, 05004], lr: 0.016911, loss: 1.1565
2022-08-25 11:21:35 - train: epoch 0074, iter [00200, 05004], lr: 0.016887, loss: 1.3315
2022-08-25 11:22:08 - train: epoch 0074, iter [00300, 05004], lr: 0.016864, loss: 1.4408
2022-08-25 11:22:43 - train: epoch 0074, iter [00400, 05004], lr: 0.016840, loss: 1.4772
2022-08-25 11:23:17 - train: epoch 0074, iter [00500, 05004], lr: 0.016817, loss: 1.2870
2022-08-25 11:23:50 - train: epoch 0074, iter [00600, 05004], lr: 0.016793, loss: 1.3455
2022-08-25 11:24:24 - train: epoch 0074, iter [00700, 05004], lr: 0.016770, loss: 1.2789
2022-08-25 11:24:58 - train: epoch 0074, iter [00800, 05004], lr: 0.016746, loss: 1.5032
2022-08-25 11:25:32 - train: epoch 0074, iter [00900, 05004], lr: 0.016723, loss: 1.2465
2022-08-25 11:26:06 - train: epoch 0074, iter [01000, 05004], lr: 0.016700, loss: 1.4472
2022-08-25 11:26:40 - train: epoch 0074, iter [01100, 05004], lr: 0.016676, loss: 1.4028
2022-08-25 11:27:14 - train: epoch 0074, iter [01200, 05004], lr: 0.016653, loss: 1.2252
2022-08-25 11:27:49 - train: epoch 0074, iter [01300, 05004], lr: 0.016629, loss: 1.4217
2022-08-25 11:28:23 - train: epoch 0074, iter [01400, 05004], lr: 0.016606, loss: 1.2397
2022-08-25 11:28:57 - train: epoch 0074, iter [01500, 05004], lr: 0.016583, loss: 1.3018
2022-08-25 11:29:32 - train: epoch 0074, iter [01600, 05004], lr: 0.016559, loss: 1.1769
2022-08-25 11:30:05 - train: epoch 0074, iter [01700, 05004], lr: 0.016536, loss: 1.4447
2022-08-25 11:30:40 - train: epoch 0074, iter [01800, 05004], lr: 0.016513, loss: 1.6684
2022-08-25 11:31:14 - train: epoch 0074, iter [01900, 05004], lr: 0.016489, loss: 1.4569
2022-08-25 11:31:49 - train: epoch 0074, iter [02000, 05004], lr: 0.016466, loss: 1.0539
2022-08-25 11:32:22 - train: epoch 0074, iter [02100, 05004], lr: 0.016443, loss: 1.3046
2022-08-25 11:32:56 - train: epoch 0074, iter [02200, 05004], lr: 0.016420, loss: 1.5981
2022-08-25 11:33:30 - train: epoch 0074, iter [02300, 05004], lr: 0.016396, loss: 1.4211
2022-08-25 11:34:05 - train: epoch 0074, iter [02400, 05004], lr: 0.016373, loss: 1.3721
2022-08-25 11:34:39 - train: epoch 0074, iter [02500, 05004], lr: 0.016350, loss: 1.1729
2022-08-25 11:35:13 - train: epoch 0074, iter [02600, 05004], lr: 0.016327, loss: 1.1348
2022-08-25 11:35:47 - train: epoch 0074, iter [02700, 05004], lr: 0.016303, loss: 1.2284
2022-08-25 11:36:21 - train: epoch 0074, iter [02800, 05004], lr: 0.016280, loss: 1.6576
2022-08-25 11:36:55 - train: epoch 0074, iter [02900, 05004], lr: 0.016257, loss: 1.2739
2022-08-25 11:37:30 - train: epoch 0074, iter [03000, 05004], lr: 0.016234, loss: 1.4310
2022-08-25 11:38:03 - train: epoch 0074, iter [03100, 05004], lr: 0.016211, loss: 1.3061
2022-08-25 11:38:37 - train: epoch 0074, iter [03200, 05004], lr: 0.016188, loss: 1.1779
2022-08-25 11:39:11 - train: epoch 0074, iter [03300, 05004], lr: 0.016165, loss: 1.3949
2022-08-25 11:39:46 - train: epoch 0074, iter [03400, 05004], lr: 0.016141, loss: 1.4351
2022-08-25 11:40:20 - train: epoch 0074, iter [03500, 05004], lr: 0.016118, loss: 1.1073
2022-08-25 11:40:55 - train: epoch 0074, iter [03600, 05004], lr: 0.016095, loss: 1.4366
2022-08-25 11:41:29 - train: epoch 0074, iter [03700, 05004], lr: 0.016072, loss: 1.3395
2022-08-25 11:42:03 - train: epoch 0074, iter [03800, 05004], lr: 0.016049, loss: 1.2993
2022-08-25 11:42:37 - train: epoch 0074, iter [03900, 05004], lr: 0.016026, loss: 1.1507
2022-08-25 11:43:11 - train: epoch 0074, iter [04000, 05004], lr: 0.016003, loss: 1.2188
2022-08-25 11:43:45 - train: epoch 0074, iter [04100, 05004], lr: 0.015980, loss: 1.4539
2022-08-25 11:44:19 - train: epoch 0074, iter [04200, 05004], lr: 0.015957, loss: 1.3191
2022-08-25 11:44:54 - train: epoch 0074, iter [04300, 05004], lr: 0.015934, loss: 1.2806
2022-08-25 11:45:29 - train: epoch 0074, iter [04400, 05004], lr: 0.015911, loss: 1.1749
2022-08-25 11:46:02 - train: epoch 0074, iter [04500, 05004], lr: 0.015888, loss: 1.1988
2022-08-25 11:46:36 - train: epoch 0074, iter [04600, 05004], lr: 0.015865, loss: 1.3478
2022-08-25 11:47:10 - train: epoch 0074, iter [04700, 05004], lr: 0.015842, loss: 1.3549
2022-08-25 11:47:44 - train: epoch 0074, iter [04800, 05004], lr: 0.015819, loss: 1.3069
2022-08-25 11:48:19 - train: epoch 0074, iter [04900, 05004], lr: 0.015796, loss: 1.6128
2022-08-25 11:48:52 - train: epoch 0074, iter [05000, 05004], lr: 0.015774, loss: 1.2882
2022-08-25 11:48:53 - train: epoch 074, train_loss: 1.3212
2022-08-25 11:50:09 - eval: epoch: 074, acc1: 71.198%, acc5: 90.550%, test_loss: 1.1562, per_image_load_time: 1.278ms, per_image_inference_time: 0.578ms
2022-08-25 11:50:09 - until epoch: 074, best_acc1: 71.198%
2022-08-25 11:50:09 - epoch 075 lr: 0.015772
2022-08-25 11:50:48 - train: epoch 0075, iter [00100, 05004], lr: 0.015750, loss: 1.3640
2022-08-25 11:51:22 - train: epoch 0075, iter [00200, 05004], lr: 0.015727, loss: 1.3206
2022-08-25 11:51:57 - train: epoch 0075, iter [00300, 05004], lr: 0.015704, loss: 1.2086
2022-08-25 11:52:30 - train: epoch 0075, iter [00400, 05004], lr: 0.015681, loss: 1.3507
2022-08-25 11:53:04 - train: epoch 0075, iter [00500, 05004], lr: 0.015658, loss: 1.1148
2022-08-25 11:53:38 - train: epoch 0075, iter [00600, 05004], lr: 0.015636, loss: 1.2384
2022-08-25 11:54:12 - train: epoch 0075, iter [00700, 05004], lr: 0.015613, loss: 1.4541
2022-08-25 11:54:46 - train: epoch 0075, iter [00800, 05004], lr: 0.015590, loss: 1.4706
2022-08-25 11:55:21 - train: epoch 0075, iter [00900, 05004], lr: 0.015567, loss: 1.3840
2022-08-25 11:55:54 - train: epoch 0075, iter [01000, 05004], lr: 0.015544, loss: 1.1532
2022-08-25 11:56:29 - train: epoch 0075, iter [01100, 05004], lr: 0.015522, loss: 1.3692
2022-08-25 11:57:03 - train: epoch 0075, iter [01200, 05004], lr: 0.015499, loss: 1.2192
2022-08-25 11:57:38 - train: epoch 0075, iter [01300, 05004], lr: 0.015476, loss: 1.1900
2022-08-25 11:58:12 - train: epoch 0075, iter [01400, 05004], lr: 0.015454, loss: 1.2445
2022-08-25 11:58:46 - train: epoch 0075, iter [01500, 05004], lr: 0.015431, loss: 1.3838
2022-08-25 11:59:20 - train: epoch 0075, iter [01600, 05004], lr: 0.015408, loss: 0.9946
2022-08-25 11:59:55 - train: epoch 0075, iter [01700, 05004], lr: 0.015386, loss: 1.3102
2022-08-25 12:00:28 - train: epoch 0075, iter [01800, 05004], lr: 0.015363, loss: 1.2583
2022-08-25 12:01:03 - train: epoch 0075, iter [01900, 05004], lr: 0.015340, loss: 1.2159
2022-08-25 12:01:37 - train: epoch 0075, iter [02000, 05004], lr: 0.015318, loss: 1.3660
2022-08-25 12:02:11 - train: epoch 0075, iter [02100, 05004], lr: 0.015295, loss: 1.2008
2022-08-25 12:02:45 - train: epoch 0075, iter [02200, 05004], lr: 0.015273, loss: 1.2980
2022-08-25 12:03:19 - train: epoch 0075, iter [02300, 05004], lr: 0.015250, loss: 1.2215
2022-08-25 12:03:54 - train: epoch 0075, iter [02400, 05004], lr: 0.015227, loss: 1.2531
2022-08-25 12:04:28 - train: epoch 0075, iter [02500, 05004], lr: 0.015205, loss: 1.3983
2022-08-25 12:05:02 - train: epoch 0075, iter [02600, 05004], lr: 0.015182, loss: 1.4283
2022-08-25 12:05:36 - train: epoch 0075, iter [02700, 05004], lr: 0.015160, loss: 1.1117
2022-08-25 12:06:11 - train: epoch 0075, iter [02800, 05004], lr: 0.015137, loss: 1.2093
2022-08-25 12:06:46 - train: epoch 0075, iter [02900, 05004], lr: 0.015115, loss: 1.4638
2022-08-25 12:07:20 - train: epoch 0075, iter [03000, 05004], lr: 0.015092, loss: 1.4816
2022-08-25 12:07:54 - train: epoch 0075, iter [03100, 05004], lr: 0.015070, loss: 1.3583
2022-08-25 12:08:29 - train: epoch 0075, iter [03200, 05004], lr: 0.015047, loss: 1.2721
2022-08-25 12:09:03 - train: epoch 0075, iter [03300, 05004], lr: 0.015025, loss: 1.3207
2022-08-25 12:09:37 - train: epoch 0075, iter [03400, 05004], lr: 0.015002, loss: 1.1343
2022-08-25 12:10:11 - train: epoch 0075, iter [03500, 05004], lr: 0.014980, loss: 1.2737
2022-08-25 12:10:46 - train: epoch 0075, iter [03600, 05004], lr: 0.014958, loss: 1.2948
2022-08-25 12:11:20 - train: epoch 0075, iter [03700, 05004], lr: 0.014935, loss: 1.2733
2022-08-25 12:11:54 - train: epoch 0075, iter [03800, 05004], lr: 0.014913, loss: 1.2472
2022-08-25 12:12:28 - train: epoch 0075, iter [03900, 05004], lr: 0.014891, loss: 1.4426
2022-08-25 12:13:02 - train: epoch 0075, iter [04000, 05004], lr: 0.014868, loss: 1.1827
2022-08-25 12:13:37 - train: epoch 0075, iter [04100, 05004], lr: 0.014846, loss: 1.0200
2022-08-25 12:14:11 - train: epoch 0075, iter [04200, 05004], lr: 0.014824, loss: 1.4455
2022-08-25 12:14:45 - train: epoch 0075, iter [04300, 05004], lr: 0.014801, loss: 1.5515
2022-08-25 12:15:20 - train: epoch 0075, iter [04400, 05004], lr: 0.014779, loss: 1.1400
2022-08-25 12:15:55 - train: epoch 0075, iter [04500, 05004], lr: 0.014757, loss: 1.3327
2022-08-25 12:16:30 - train: epoch 0075, iter [04600, 05004], lr: 0.014734, loss: 1.1823
2022-08-25 12:17:04 - train: epoch 0075, iter [04700, 05004], lr: 0.014712, loss: 1.4477
2022-08-25 12:17:38 - train: epoch 0075, iter [04800, 05004], lr: 0.014690, loss: 1.3183
2022-08-25 12:18:12 - train: epoch 0075, iter [04900, 05004], lr: 0.014668, loss: 1.2229
2022-08-25 12:18:46 - train: epoch 0075, iter [05000, 05004], lr: 0.014646, loss: 1.3607
2022-08-25 12:18:47 - train: epoch 075, train_loss: 1.3035
2022-08-25 12:20:02 - eval: epoch: 075, acc1: 71.322%, acc5: 90.544%, test_loss: 1.1581, per_image_load_time: 1.143ms, per_image_inference_time: 0.558ms
2022-08-25 12:20:03 - until epoch: 075, best_acc1: 71.322%
2022-08-25 12:20:03 - epoch 076 lr: 0.014644
2022-08-25 12:20:42 - train: epoch 0076, iter [00100, 05004], lr: 0.014622, loss: 1.0918
2022-08-25 12:21:16 - train: epoch 0076, iter [00200, 05004], lr: 0.014600, loss: 1.3332
2022-08-25 12:21:49 - train: epoch 0076, iter [00300, 05004], lr: 0.014578, loss: 1.3790
2022-08-25 12:22:23 - train: epoch 0076, iter [00400, 05004], lr: 0.014556, loss: 1.2517
2022-08-25 12:22:57 - train: epoch 0076, iter [00500, 05004], lr: 0.014534, loss: 1.2446
2022-08-25 12:23:31 - train: epoch 0076, iter [00600, 05004], lr: 0.014512, loss: 1.2581
2022-08-25 12:24:05 - train: epoch 0076, iter [00700, 05004], lr: 0.014490, loss: 1.2530
2022-08-25 12:24:40 - train: epoch 0076, iter [00800, 05004], lr: 0.014468, loss: 1.1623
2022-08-25 12:25:14 - train: epoch 0076, iter [00900, 05004], lr: 0.014445, loss: 1.2223
2022-08-25 12:25:48 - train: epoch 0076, iter [01000, 05004], lr: 0.014423, loss: 1.2320
2022-08-25 12:26:22 - train: epoch 0076, iter [01100, 05004], lr: 0.014401, loss: 1.1531
2022-08-25 12:26:56 - train: epoch 0076, iter [01200, 05004], lr: 0.014379, loss: 1.1932
2022-08-25 12:27:31 - train: epoch 0076, iter [01300, 05004], lr: 0.014357, loss: 1.3259
2022-08-25 12:28:05 - train: epoch 0076, iter [01400, 05004], lr: 0.014335, loss: 1.1705
2022-08-25 12:28:39 - train: epoch 0076, iter [01500, 05004], lr: 0.014313, loss: 1.1019
2022-08-25 12:29:13 - train: epoch 0076, iter [01600, 05004], lr: 0.014291, loss: 1.1425
2022-08-25 12:29:47 - train: epoch 0076, iter [01700, 05004], lr: 0.014269, loss: 1.2485
2022-08-25 12:30:21 - train: epoch 0076, iter [01800, 05004], lr: 0.014247, loss: 1.1733
2022-08-25 12:30:56 - train: epoch 0076, iter [01900, 05004], lr: 0.014225, loss: 1.4295
2022-08-25 12:31:30 - train: epoch 0076, iter [02000, 05004], lr: 0.014204, loss: 1.3008
2022-08-25 12:32:04 - train: epoch 0076, iter [02100, 05004], lr: 0.014182, loss: 1.3201
2022-08-25 12:32:39 - train: epoch 0076, iter [02200, 05004], lr: 0.014160, loss: 1.4557
2022-08-25 12:33:13 - train: epoch 0076, iter [02300, 05004], lr: 0.014138, loss: 1.2675
2022-08-25 12:33:46 - train: epoch 0076, iter [02400, 05004], lr: 0.014116, loss: 1.4645
2022-08-25 12:34:20 - train: epoch 0076, iter [02500, 05004], lr: 0.014094, loss: 1.3231
2022-08-25 12:34:54 - train: epoch 0076, iter [02600, 05004], lr: 0.014072, loss: 1.5034
2022-08-25 12:35:29 - train: epoch 0076, iter [02700, 05004], lr: 0.014050, loss: 1.2284
2022-08-25 12:36:03 - train: epoch 0076, iter [02800, 05004], lr: 0.014029, loss: 1.3070
2022-08-25 12:36:37 - train: epoch 0076, iter [02900, 05004], lr: 0.014007, loss: 1.3210
2022-08-25 12:37:11 - train: epoch 0076, iter [03000, 05004], lr: 0.013985, loss: 1.1394
2022-08-25 12:37:45 - train: epoch 0076, iter [03100, 05004], lr: 0.013963, loss: 1.3854
2022-08-25 12:38:19 - train: epoch 0076, iter [03200, 05004], lr: 0.013942, loss: 1.3807
2022-08-25 12:38:54 - train: epoch 0076, iter [03300, 05004], lr: 0.013920, loss: 1.1947
2022-08-25 12:39:28 - train: epoch 0076, iter [03400, 05004], lr: 0.013898, loss: 1.3458
2022-08-25 12:40:02 - train: epoch 0076, iter [03500, 05004], lr: 0.013876, loss: 1.1410
2022-08-25 12:40:37 - train: epoch 0076, iter [03600, 05004], lr: 0.013855, loss: 1.0898
2022-08-25 12:41:12 - train: epoch 0076, iter [03700, 05004], lr: 0.013833, loss: 1.1604
2022-08-25 12:41:46 - train: epoch 0076, iter [03800, 05004], lr: 0.013811, loss: 1.1715
2022-08-25 12:42:20 - train: epoch 0076, iter [03900, 05004], lr: 0.013790, loss: 1.1331
2022-08-25 12:42:54 - train: epoch 0076, iter [04000, 05004], lr: 0.013768, loss: 1.3445
2022-08-25 12:43:28 - train: epoch 0076, iter [04100, 05004], lr: 0.013746, loss: 1.1898
2022-08-25 12:44:02 - train: epoch 0076, iter [04200, 05004], lr: 0.013725, loss: 1.4599
2022-08-25 12:44:36 - train: epoch 0076, iter [04300, 05004], lr: 0.013703, loss: 1.3140
2022-08-25 12:45:11 - train: epoch 0076, iter [04400, 05004], lr: 0.013682, loss: 1.3321
2022-08-25 12:45:45 - train: epoch 0076, iter [04500, 05004], lr: 0.013660, loss: 1.2591
2022-08-25 12:46:19 - train: epoch 0076, iter [04600, 05004], lr: 0.013638, loss: 1.2455
2022-08-25 12:46:54 - train: epoch 0076, iter [04700, 05004], lr: 0.013617, loss: 1.4561
2022-08-25 12:47:28 - train: epoch 0076, iter [04800, 05004], lr: 0.013595, loss: 1.1346
2022-08-25 12:48:02 - train: epoch 0076, iter [04900, 05004], lr: 0.013574, loss: 1.1751
2022-08-25 12:48:36 - train: epoch 0076, iter [05000, 05004], lr: 0.013552, loss: 1.3945
2022-08-25 12:48:37 - train: epoch 076, train_loss: 1.2835
2022-08-25 12:49:53 - eval: epoch: 076, acc1: 71.756%, acc5: 90.682%, test_loss: 1.1454, per_image_load_time: 1.357ms, per_image_inference_time: 0.571ms
2022-08-25 12:49:53 - until epoch: 076, best_acc1: 71.756%
2022-08-25 12:49:53 - epoch 077 lr: 0.013551
2022-08-25 12:50:32 - train: epoch 0077, iter [00100, 05004], lr: 0.013530, loss: 1.4289
2022-08-25 12:51:06 - train: epoch 0077, iter [00200, 05004], lr: 0.013509, loss: 1.3879
2022-08-25 12:51:40 - train: epoch 0077, iter [00300, 05004], lr: 0.013487, loss: 1.0004
2022-08-25 12:52:14 - train: epoch 0077, iter [00400, 05004], lr: 0.013466, loss: 1.4106
2022-08-25 12:52:47 - train: epoch 0077, iter [00500, 05004], lr: 0.013444, loss: 1.0629
2022-08-25 12:53:21 - train: epoch 0077, iter [00600, 05004], lr: 0.013423, loss: 0.9941
2022-08-25 12:53:56 - train: epoch 0077, iter [00700, 05004], lr: 0.013402, loss: 1.3838
2022-08-25 12:54:29 - train: epoch 0077, iter [00800, 05004], lr: 0.013380, loss: 1.4148
2022-08-25 12:55:03 - train: epoch 0077, iter [00900, 05004], lr: 0.013359, loss: 1.4126
2022-08-25 12:55:37 - train: epoch 0077, iter [01000, 05004], lr: 0.013337, loss: 1.0877
2022-08-25 12:56:11 - train: epoch 0077, iter [01100, 05004], lr: 0.013316, loss: 1.2616
2022-08-25 12:56:45 - train: epoch 0077, iter [01200, 05004], lr: 0.013295, loss: 1.3148
2022-08-25 12:57:19 - train: epoch 0077, iter [01300, 05004], lr: 0.013273, loss: 1.0430
2022-08-25 12:57:52 - train: epoch 0077, iter [01400, 05004], lr: 0.013252, loss: 1.3642
2022-08-25 12:58:27 - train: epoch 0077, iter [01500, 05004], lr: 0.013231, loss: 1.1424
2022-08-25 12:59:00 - train: epoch 0077, iter [01600, 05004], lr: 0.013210, loss: 1.2392
2022-08-25 12:59:34 - train: epoch 0077, iter [01700, 05004], lr: 0.013188, loss: 1.5579
2022-08-25 13:00:08 - train: epoch 0077, iter [01800, 05004], lr: 0.013167, loss: 1.0566
2022-08-25 13:00:43 - train: epoch 0077, iter [01900, 05004], lr: 0.013146, loss: 1.2732
2022-08-25 13:01:17 - train: epoch 0077, iter [02000, 05004], lr: 0.013125, loss: 1.1434
2022-08-25 13:01:51 - train: epoch 0077, iter [02100, 05004], lr: 0.013103, loss: 1.2512
2022-08-25 13:02:25 - train: epoch 0077, iter [02200, 05004], lr: 0.013082, loss: 1.3013
2022-08-25 13:03:00 - train: epoch 0077, iter [02300, 05004], lr: 0.013061, loss: 1.4220
2022-08-25 13:03:34 - train: epoch 0077, iter [02400, 05004], lr: 0.013040, loss: 1.2877
2022-08-25 13:04:08 - train: epoch 0077, iter [02500, 05004], lr: 0.013019, loss: 1.4967
2022-08-25 13:04:42 - train: epoch 0077, iter [02600, 05004], lr: 0.012998, loss: 0.9499
2022-08-25 13:05:17 - train: epoch 0077, iter [02700, 05004], lr: 0.012977, loss: 1.2253
2022-08-25 13:05:51 - train: epoch 0077, iter [02800, 05004], lr: 0.012956, loss: 1.4130
2022-08-25 13:06:25 - train: epoch 0077, iter [02900, 05004], lr: 0.012934, loss: 1.5633
2022-08-25 13:06:59 - train: epoch 0077, iter [03000, 05004], lr: 0.012913, loss: 1.1326
2022-08-25 13:07:33 - train: epoch 0077, iter [03100, 05004], lr: 0.012892, loss: 1.2572
2022-08-25 13:08:07 - train: epoch 0077, iter [03200, 05004], lr: 0.012871, loss: 1.4203
2022-08-25 13:08:41 - train: epoch 0077, iter [03300, 05004], lr: 0.012850, loss: 1.1573
2022-08-25 13:09:16 - train: epoch 0077, iter [03400, 05004], lr: 0.012829, loss: 1.5497
2022-08-25 13:09:49 - train: epoch 0077, iter [03500, 05004], lr: 0.012808, loss: 1.1389
2022-08-25 13:10:23 - train: epoch 0077, iter [03600, 05004], lr: 0.012787, loss: 1.2057
2022-08-25 13:10:58 - train: epoch 0077, iter [03700, 05004], lr: 0.012766, loss: 1.2855
2022-08-25 13:11:33 - train: epoch 0077, iter [03800, 05004], lr: 0.012745, loss: 1.3244
2022-08-25 13:12:07 - train: epoch 0077, iter [03900, 05004], lr: 0.012725, loss: 1.2775
2022-08-25 13:12:41 - train: epoch 0077, iter [04000, 05004], lr: 0.012704, loss: 1.2701
2022-08-25 13:13:16 - train: epoch 0077, iter [04100, 05004], lr: 0.012683, loss: 1.3998
2022-08-25 13:13:50 - train: epoch 0077, iter [04200, 05004], lr: 0.012662, loss: 1.4747
2022-08-25 13:14:23 - train: epoch 0077, iter [04300, 05004], lr: 0.012641, loss: 1.1050
2022-08-25 13:14:58 - train: epoch 0077, iter [04400, 05004], lr: 0.012620, loss: 1.2079
2022-08-25 13:15:33 - train: epoch 0077, iter [04500, 05004], lr: 0.012599, loss: 1.3536
2022-08-25 13:16:07 - train: epoch 0077, iter [04600, 05004], lr: 0.012578, loss: 1.1927
2022-08-25 13:16:42 - train: epoch 0077, iter [04700, 05004], lr: 0.012558, loss: 1.0579
2022-08-25 13:17:17 - train: epoch 0077, iter [04800, 05004], lr: 0.012537, loss: 1.2655
2022-08-25 13:17:50 - train: epoch 0077, iter [04900, 05004], lr: 0.012516, loss: 1.4879
2022-08-25 13:18:24 - train: epoch 0077, iter [05000, 05004], lr: 0.012495, loss: 1.4690
2022-08-25 13:18:25 - train: epoch 077, train_loss: 1.2607
2022-08-25 13:19:41 - eval: epoch: 077, acc1: 72.042%, acc5: 91.014%, test_loss: 1.1249, per_image_load_time: 1.182ms, per_image_inference_time: 0.573ms
2022-08-25 13:19:41 - until epoch: 077, best_acc1: 72.042%
2022-08-25 13:19:41 - epoch 078 lr: 0.012494
2022-08-25 13:20:21 - train: epoch 0078, iter [00100, 05004], lr: 0.012474, loss: 1.1294
2022-08-25 13:20:54 - train: epoch 0078, iter [00200, 05004], lr: 0.012453, loss: 1.3733
2022-08-25 13:21:29 - train: epoch 0078, iter [00300, 05004], lr: 0.012432, loss: 1.3091
2022-08-25 13:22:03 - train: epoch 0078, iter [00400, 05004], lr: 0.012412, loss: 1.2622
2022-08-25 13:22:38 - train: epoch 0078, iter [00500, 05004], lr: 0.012391, loss: 1.1025
2022-08-25 13:23:11 - train: epoch 0078, iter [00600, 05004], lr: 0.012370, loss: 1.2122
2022-08-25 13:23:45 - train: epoch 0078, iter [00700, 05004], lr: 0.012349, loss: 1.3569
2022-08-25 13:24:20 - train: epoch 0078, iter [00800, 05004], lr: 0.012329, loss: 1.4319
2022-08-25 13:24:54 - train: epoch 0078, iter [00900, 05004], lr: 0.012308, loss: 1.3634
2022-08-25 13:25:28 - train: epoch 0078, iter [01000, 05004], lr: 0.012288, loss: 1.3354
2022-08-25 13:26:02 - train: epoch 0078, iter [01100, 05004], lr: 0.012267, loss: 1.1151
2022-08-25 13:26:36 - train: epoch 0078, iter [01200, 05004], lr: 0.012246, loss: 1.0593
2022-08-25 13:27:09 - train: epoch 0078, iter [01300, 05004], lr: 0.012226, loss: 1.3140
2022-08-25 13:27:44 - train: epoch 0078, iter [01400, 05004], lr: 0.012205, loss: 1.1580
2022-08-25 13:28:18 - train: epoch 0078, iter [01500, 05004], lr: 0.012185, loss: 1.2714
2022-08-25 13:28:53 - train: epoch 0078, iter [01600, 05004], lr: 0.012164, loss: 1.3408
2022-08-25 13:29:27 - train: epoch 0078, iter [01700, 05004], lr: 0.012144, loss: 1.1918
2022-08-25 13:30:02 - train: epoch 0078, iter [01800, 05004], lr: 0.012123, loss: 1.2223
2022-08-25 13:30:37 - train: epoch 0078, iter [01900, 05004], lr: 0.012103, loss: 1.0590
2022-08-25 13:31:11 - train: epoch 0078, iter [02000, 05004], lr: 0.012082, loss: 1.1467
2022-08-25 13:31:45 - train: epoch 0078, iter [02100, 05004], lr: 0.012062, loss: 1.5209
2022-08-25 13:32:19 - train: epoch 0078, iter [02200, 05004], lr: 0.012041, loss: 1.1922
2022-08-25 13:32:54 - train: epoch 0078, iter [02300, 05004], lr: 0.012021, loss: 1.3785
2022-08-25 13:33:28 - train: epoch 0078, iter [02400, 05004], lr: 0.012001, loss: 1.1933
2022-08-25 13:34:02 - train: epoch 0078, iter [02500, 05004], lr: 0.011980, loss: 1.3128
2022-08-25 13:34:37 - train: epoch 0078, iter [02600, 05004], lr: 0.011960, loss: 1.1463
2022-08-25 13:35:12 - train: epoch 0078, iter [02700, 05004], lr: 0.011939, loss: 1.2089
2022-08-25 13:35:46 - train: epoch 0078, iter [02800, 05004], lr: 0.011919, loss: 1.2278
2022-08-25 13:36:21 - train: epoch 0078, iter [02900, 05004], lr: 0.011899, loss: 1.1823
2022-08-25 13:36:55 - train: epoch 0078, iter [03000, 05004], lr: 0.011878, loss: 1.1356
2022-08-25 13:37:29 - train: epoch 0078, iter [03100, 05004], lr: 0.011858, loss: 1.3149
2022-08-25 13:38:04 - train: epoch 0078, iter [03200, 05004], lr: 0.011838, loss: 1.2395
2022-08-25 13:38:38 - train: epoch 0078, iter [03300, 05004], lr: 0.011817, loss: 1.3180
2022-08-25 13:39:13 - train: epoch 0078, iter [03400, 05004], lr: 0.011797, loss: 1.2415
2022-08-25 13:39:48 - train: epoch 0078, iter [03500, 05004], lr: 0.011777, loss: 1.2881
2022-08-25 13:40:21 - train: epoch 0078, iter [03600, 05004], lr: 0.011757, loss: 1.3119
2022-08-25 13:40:57 - train: epoch 0078, iter [03700, 05004], lr: 0.011737, loss: 1.1590
2022-08-25 13:41:31 - train: epoch 0078, iter [03800, 05004], lr: 0.011716, loss: 1.1554
2022-08-25 13:42:06 - train: epoch 0078, iter [03900, 05004], lr: 0.011696, loss: 1.2657
2022-08-25 13:42:41 - train: epoch 0078, iter [04000, 05004], lr: 0.011676, loss: 1.2151
2022-08-25 13:43:16 - train: epoch 0078, iter [04100, 05004], lr: 0.011656, loss: 1.3553
2022-08-25 13:43:50 - train: epoch 0078, iter [04200, 05004], lr: 0.011636, loss: 1.4181
2022-08-25 13:44:24 - train: epoch 0078, iter [04300, 05004], lr: 0.011616, loss: 1.2214
2022-08-25 13:44:59 - train: epoch 0078, iter [04400, 05004], lr: 0.011595, loss: 1.2535
2022-08-25 13:45:33 - train: epoch 0078, iter [04500, 05004], lr: 0.011575, loss: 1.2769
2022-08-25 13:46:08 - train: epoch 0078, iter [04600, 05004], lr: 0.011555, loss: 1.0717
2022-08-25 13:46:43 - train: epoch 0078, iter [04700, 05004], lr: 0.011535, loss: 1.3536
2022-08-25 13:47:17 - train: epoch 0078, iter [04800, 05004], lr: 0.011515, loss: 1.4100
2022-08-25 13:47:52 - train: epoch 0078, iter [04900, 05004], lr: 0.011495, loss: 1.2706
2022-08-25 13:48:24 - train: epoch 0078, iter [05000, 05004], lr: 0.011475, loss: 1.2452
2022-08-25 13:48:26 - train: epoch 078, train_loss: 1.2376
2022-08-25 13:49:42 - eval: epoch: 078, acc1: 72.142%, acc5: 91.104%, test_loss: 1.1142, per_image_load_time: 1.471ms, per_image_inference_time: 0.552ms
2022-08-25 13:49:42 - until epoch: 078, best_acc1: 72.142%
2022-08-25 13:49:42 - epoch 079 lr: 0.011474
2022-08-25 13:50:21 - train: epoch 0079, iter [00100, 05004], lr: 0.011454, loss: 1.1250
2022-08-25 13:50:55 - train: epoch 0079, iter [00200, 05004], lr: 0.011434, loss: 1.1610
2022-08-25 13:51:30 - train: epoch 0079, iter [00300, 05004], lr: 0.011414, loss: 1.3520
2022-08-25 13:52:03 - train: epoch 0079, iter [00400, 05004], lr: 0.011394, loss: 1.3879
2022-08-25 13:52:37 - train: epoch 0079, iter [00500, 05004], lr: 0.011374, loss: 1.2185
2022-08-25 13:53:12 - train: epoch 0079, iter [00600, 05004], lr: 0.011355, loss: 1.1592
2022-08-25 13:53:46 - train: epoch 0079, iter [00700, 05004], lr: 0.011335, loss: 1.1582
2022-08-25 13:54:20 - train: epoch 0079, iter [00800, 05004], lr: 0.011315, loss: 1.2502
2022-08-25 13:54:54 - train: epoch 0079, iter [00900, 05004], lr: 0.011295, loss: 1.0841
2022-08-25 13:55:29 - train: epoch 0079, iter [01000, 05004], lr: 0.011275, loss: 1.0833
2022-08-25 13:56:03 - train: epoch 0079, iter [01100, 05004], lr: 0.011255, loss: 1.3096
2022-08-25 13:56:37 - train: epoch 0079, iter [01200, 05004], lr: 0.011235, loss: 1.4686
2022-08-25 13:57:11 - train: epoch 0079, iter [01300, 05004], lr: 0.011216, loss: 1.0762
2022-08-25 13:57:46 - train: epoch 0079, iter [01400, 05004], lr: 0.011196, loss: 1.2352
2022-08-25 13:58:20 - train: epoch 0079, iter [01500, 05004], lr: 0.011176, loss: 1.1076
2022-08-25 13:58:54 - train: epoch 0079, iter [01600, 05004], lr: 0.011156, loss: 1.0565
2022-08-25 13:59:28 - train: epoch 0079, iter [01700, 05004], lr: 0.011136, loss: 1.1125
2022-08-25 14:00:02 - train: epoch 0079, iter [01800, 05004], lr: 0.011117, loss: 1.1663
2022-08-25 14:00:37 - train: epoch 0079, iter [01900, 05004], lr: 0.011097, loss: 1.2831
2022-08-25 14:01:11 - train: epoch 0079, iter [02000, 05004], lr: 0.011077, loss: 1.4528
2022-08-25 14:01:45 - train: epoch 0079, iter [02100, 05004], lr: 0.011058, loss: 1.0495
2022-08-25 14:02:19 - train: epoch 0079, iter [02200, 05004], lr: 0.011038, loss: 1.2746
2022-08-25 14:02:54 - train: epoch 0079, iter [02300, 05004], lr: 0.011018, loss: 1.1185
2022-08-25 14:03:29 - train: epoch 0079, iter [02400, 05004], lr: 0.010999, loss: 1.3032
2022-08-25 14:04:03 - train: epoch 0079, iter [02500, 05004], lr: 0.010979, loss: 1.1939
2022-08-25 14:04:36 - train: epoch 0079, iter [02600, 05004], lr: 0.010959, loss: 1.1602
2022-08-25 14:05:11 - train: epoch 0079, iter [02700, 05004], lr: 0.010940, loss: 1.0616
2022-08-25 14:05:45 - train: epoch 0079, iter [02800, 05004], lr: 0.010920, loss: 1.0335
2022-08-25 14:06:19 - train: epoch 0079, iter [02900, 05004], lr: 0.010900, loss: 1.1100
2022-08-25 14:06:54 - train: epoch 0079, iter [03000, 05004], lr: 0.010881, loss: 1.2623
2022-08-25 14:07:28 - train: epoch 0079, iter [03100, 05004], lr: 0.010861, loss: 1.2203
2022-08-25 14:08:03 - train: epoch 0079, iter [03200, 05004], lr: 0.010842, loss: 1.4286
2022-08-25 14:08:37 - train: epoch 0079, iter [03300, 05004], lr: 0.010822, loss: 1.1929
2022-08-25 14:09:11 - train: epoch 0079, iter [03400, 05004], lr: 0.010803, loss: 1.0900
2022-08-25 14:09:46 - train: epoch 0079, iter [03500, 05004], lr: 0.010783, loss: 1.4168
2022-08-25 14:10:20 - train: epoch 0079, iter [03600, 05004], lr: 0.010764, loss: 1.0889
2022-08-25 14:10:55 - train: epoch 0079, iter [03700, 05004], lr: 0.010744, loss: 1.1287
2022-08-25 14:11:29 - train: epoch 0079, iter [03800, 05004], lr: 0.010725, loss: 1.3187
2022-08-25 14:12:03 - train: epoch 0079, iter [03900, 05004], lr: 0.010706, loss: 1.1523
2022-08-25 14:12:37 - train: epoch 0079, iter [04000, 05004], lr: 0.010686, loss: 1.0985
2022-08-25 14:13:12 - train: epoch 0079, iter [04100, 05004], lr: 0.010667, loss: 1.3960
2022-08-25 14:13:47 - train: epoch 0079, iter [04200, 05004], lr: 0.010647, loss: 1.1355
2022-08-25 14:14:21 - train: epoch 0079, iter [04300, 05004], lr: 0.010628, loss: 0.9204
2022-08-25 14:14:55 - train: epoch 0079, iter [04400, 05004], lr: 0.010609, loss: 1.4737
2022-08-25 14:15:29 - train: epoch 0079, iter [04500, 05004], lr: 0.010589, loss: 1.5314
2022-08-25 14:16:03 - train: epoch 0079, iter [04600, 05004], lr: 0.010570, loss: 1.4380
2022-08-25 14:16:38 - train: epoch 0079, iter [04700, 05004], lr: 0.010551, loss: 1.1709
2022-08-25 14:17:12 - train: epoch 0079, iter [04800, 05004], lr: 0.010532, loss: 1.3496
2022-08-25 14:17:46 - train: epoch 0079, iter [04900, 05004], lr: 0.010512, loss: 1.3876
2022-08-25 14:18:20 - train: epoch 0079, iter [05000, 05004], lr: 0.010493, loss: 1.1019
2022-08-25 14:18:21 - train: epoch 079, train_loss: 1.2203
2022-08-25 14:19:37 - eval: epoch: 079, acc1: 72.448%, acc5: 91.314%, test_loss: 1.0997, per_image_load_time: 1.088ms, per_image_inference_time: 0.568ms
2022-08-25 14:19:38 - until epoch: 079, best_acc1: 72.448%
2022-08-25 14:19:38 - epoch 080 lr: 0.010492
2022-08-25 14:20:18 - train: epoch 0080, iter [00100, 05004], lr: 0.010473, loss: 1.0878
2022-08-25 14:20:52 - train: epoch 0080, iter [00200, 05004], lr: 0.010454, loss: 1.1473
2022-08-25 14:21:27 - train: epoch 0080, iter [00300, 05004], lr: 0.010435, loss: 1.2070
2022-08-25 14:22:00 - train: epoch 0080, iter [00400, 05004], lr: 0.010415, loss: 1.0113
2022-08-25 14:22:34 - train: epoch 0080, iter [00500, 05004], lr: 0.010396, loss: 1.3261
2022-08-25 14:23:09 - train: epoch 0080, iter [00600, 05004], lr: 0.010377, loss: 1.1347
2022-08-25 14:23:44 - train: epoch 0080, iter [00700, 05004], lr: 0.010358, loss: 1.1627
2022-08-25 14:24:17 - train: epoch 0080, iter [00800, 05004], lr: 0.010339, loss: 1.0417
2022-08-25 14:24:52 - train: epoch 0080, iter [00900, 05004], lr: 0.010320, loss: 1.2533
2022-08-25 14:25:26 - train: epoch 0080, iter [01000, 05004], lr: 0.010301, loss: 1.1265
2022-08-25 14:26:00 - train: epoch 0080, iter [01100, 05004], lr: 0.010282, loss: 1.1542
2022-08-25 14:26:34 - train: epoch 0080, iter [01200, 05004], lr: 0.010262, loss: 1.1732
2022-08-25 14:27:08 - train: epoch 0080, iter [01300, 05004], lr: 0.010243, loss: 1.0626
2022-08-25 14:27:42 - train: epoch 0080, iter [01400, 05004], lr: 0.010224, loss: 1.1971
2022-08-25 14:28:17 - train: epoch 0080, iter [01500, 05004], lr: 0.010205, loss: 1.1372
2022-08-25 14:28:51 - train: epoch 0080, iter [01600, 05004], lr: 0.010186, loss: 1.1159
2022-08-25 14:29:24 - train: epoch 0080, iter [01700, 05004], lr: 0.010167, loss: 1.2353
2022-08-25 14:29:59 - train: epoch 0080, iter [01800, 05004], lr: 0.010148, loss: 1.2585
2022-08-25 14:30:33 - train: epoch 0080, iter [01900, 05004], lr: 0.010130, loss: 1.1759
2022-08-25 14:31:07 - train: epoch 0080, iter [02000, 05004], lr: 0.010111, loss: 1.2527
2022-08-25 14:31:41 - train: epoch 0080, iter [02100, 05004], lr: 0.010092, loss: 1.2370
2022-08-25 14:32:15 - train: epoch 0080, iter [02200, 05004], lr: 0.010073, loss: 1.2560
2022-08-25 14:32:50 - train: epoch 0080, iter [02300, 05004], lr: 0.010054, loss: 1.1007
2022-08-25 14:33:24 - train: epoch 0080, iter [02400, 05004], lr: 0.010035, loss: 1.1594
2022-08-25 14:33:59 - train: epoch 0080, iter [02500, 05004], lr: 0.010016, loss: 1.1134
2022-08-25 14:34:33 - train: epoch 0080, iter [02600, 05004], lr: 0.009997, loss: 1.1084
2022-08-25 14:35:08 - train: epoch 0080, iter [02700, 05004], lr: 0.009978, loss: 1.2994
2022-08-25 14:35:42 - train: epoch 0080, iter [02800, 05004], lr: 0.009960, loss: 1.3020
2022-08-25 14:36:17 - train: epoch 0080, iter [02900, 05004], lr: 0.009941, loss: 1.1341
2022-08-25 14:36:51 - train: epoch 0080, iter [03000, 05004], lr: 0.009922, loss: 1.2480
2022-08-25 14:37:25 - train: epoch 0080, iter [03100, 05004], lr: 0.009903, loss: 1.4219
2022-08-25 14:38:00 - train: epoch 0080, iter [03200, 05004], lr: 0.009885, loss: 0.8047
2022-08-25 14:38:35 - train: epoch 0080, iter [03300, 05004], lr: 0.009866, loss: 1.1806
2022-08-25 14:39:09 - train: epoch 0080, iter [03400, 05004], lr: 0.009847, loss: 1.0949
2022-08-25 14:39:44 - train: epoch 0080, iter [03500, 05004], lr: 0.009828, loss: 1.0554
2022-08-25 14:40:18 - train: epoch 0080, iter [03600, 05004], lr: 0.009810, loss: 1.2371
2022-08-25 14:40:52 - train: epoch 0080, iter [03700, 05004], lr: 0.009791, loss: 1.1192
2022-08-25 14:41:27 - train: epoch 0080, iter [03800, 05004], lr: 0.009772, loss: 1.3183
2022-08-25 14:42:02 - train: epoch 0080, iter [03900, 05004], lr: 0.009754, loss: 1.1261
2022-08-25 14:42:37 - train: epoch 0080, iter [04000, 05004], lr: 0.009735, loss: 1.3279
2022-08-25 14:43:10 - train: epoch 0080, iter [04100, 05004], lr: 0.009717, loss: 1.1837
2022-08-25 14:43:46 - train: epoch 0080, iter [04200, 05004], lr: 0.009698, loss: 1.1623
2022-08-25 14:44:20 - train: epoch 0080, iter [04300, 05004], lr: 0.009679, loss: 1.3085
2022-08-25 14:44:54 - train: epoch 0080, iter [04400, 05004], lr: 0.009661, loss: 1.2832
2022-08-25 14:45:29 - train: epoch 0080, iter [04500, 05004], lr: 0.009642, loss: 1.2539
2022-08-25 14:46:03 - train: epoch 0080, iter [04600, 05004], lr: 0.009624, loss: 1.2853
2022-08-25 14:46:38 - train: epoch 0080, iter [04700, 05004], lr: 0.009605, loss: 1.2431
2022-08-25 14:47:12 - train: epoch 0080, iter [04800, 05004], lr: 0.009587, loss: 1.1938
2022-08-25 14:47:47 - train: epoch 0080, iter [04900, 05004], lr: 0.009568, loss: 1.2834
2022-08-25 14:48:21 - train: epoch 0080, iter [05000, 05004], lr: 0.009550, loss: 1.0249
2022-08-25 14:48:22 - train: epoch 080, train_loss: 1.1985
2022-08-25 14:49:37 - eval: epoch: 080, acc1: 73.146%, acc5: 91.476%, test_loss: 1.0788, per_image_load_time: 1.010ms, per_image_inference_time: 0.576ms
2022-08-25 14:49:37 - until epoch: 080, best_acc1: 73.146%
2022-08-25 14:49:37 - epoch 081 lr: 0.009549
2022-08-25 14:50:16 - train: epoch 0081, iter [00100, 05004], lr: 0.009531, loss: 1.0109
2022-08-25 14:50:51 - train: epoch 0081, iter [00200, 05004], lr: 0.009512, loss: 1.2357
2022-08-25 14:51:24 - train: epoch 0081, iter [00300, 05004], lr: 0.009494, loss: 1.1111
2022-08-25 14:51:58 - train: epoch 0081, iter [00400, 05004], lr: 0.009475, loss: 1.3011
2022-08-25 14:52:33 - train: epoch 0081, iter [00500, 05004], lr: 0.009457, loss: 1.2552
2022-08-25 14:53:07 - train: epoch 0081, iter [00600, 05004], lr: 0.009439, loss: 1.1571
2022-08-25 14:53:41 - train: epoch 0081, iter [00700, 05004], lr: 0.009420, loss: 1.0471
2022-08-25 14:54:15 - train: epoch 0081, iter [00800, 05004], lr: 0.009402, loss: 1.3394
2022-08-25 14:54:50 - train: epoch 0081, iter [00900, 05004], lr: 0.009384, loss: 1.0411
2022-08-25 14:55:24 - train: epoch 0081, iter [01000, 05004], lr: 0.009365, loss: 1.0968
2022-08-25 14:55:58 - train: epoch 0081, iter [01100, 05004], lr: 0.009347, loss: 1.1483
2022-08-25 14:56:32 - train: epoch 0081, iter [01200, 05004], lr: 0.009329, loss: 1.1426
2022-08-25 14:57:06 - train: epoch 0081, iter [01300, 05004], lr: 0.009311, loss: 1.0104
2022-08-25 14:57:41 - train: epoch 0081, iter [01400, 05004], lr: 0.009292, loss: 0.8426
2022-08-25 14:58:15 - train: epoch 0081, iter [01500, 05004], lr: 0.009274, loss: 1.2749
2022-08-25 14:58:49 - train: epoch 0081, iter [01600, 05004], lr: 0.009256, loss: 1.0271
2022-08-25 14:59:24 - train: epoch 0081, iter [01700, 05004], lr: 0.009238, loss: 1.2838
2022-08-25 14:59:59 - train: epoch 0081, iter [01800, 05004], lr: 0.009220, loss: 1.0513
2022-08-25 15:00:33 - train: epoch 0081, iter [01900, 05004], lr: 0.009201, loss: 1.1090
2022-08-25 15:01:07 - train: epoch 0081, iter [02000, 05004], lr: 0.009183, loss: 1.2537
2022-08-25 15:01:42 - train: epoch 0081, iter [02100, 05004], lr: 0.009165, loss: 1.1978
2022-08-25 15:02:16 - train: epoch 0081, iter [02200, 05004], lr: 0.009147, loss: 1.1357
2022-08-25 15:02:50 - train: epoch 0081, iter [02300, 05004], lr: 0.009129, loss: 1.1287
2022-08-25 15:03:25 - train: epoch 0081, iter [02400, 05004], lr: 0.009111, loss: 1.1643
2022-08-25 15:03:59 - train: epoch 0081, iter [02500, 05004], lr: 0.009093, loss: 1.4202
2022-08-25 15:04:33 - train: epoch 0081, iter [02600, 05004], lr: 0.009075, loss: 1.2735
2022-08-25 15:05:07 - train: epoch 0081, iter [02700, 05004], lr: 0.009057, loss: 1.1343
2022-08-25 15:05:41 - train: epoch 0081, iter [02800, 05004], lr: 0.009039, loss: 1.1109
2022-08-25 15:06:16 - train: epoch 0081, iter [02900, 05004], lr: 0.009021, loss: 1.0479
2022-08-25 15:06:50 - train: epoch 0081, iter [03000, 05004], lr: 0.009003, loss: 1.2001
2022-08-25 15:07:25 - train: epoch 0081, iter [03100, 05004], lr: 0.008985, loss: 1.1046
2022-08-25 15:07:59 - train: epoch 0081, iter [03200, 05004], lr: 0.008967, loss: 1.1746
2022-08-25 15:08:33 - train: epoch 0081, iter [03300, 05004], lr: 0.008949, loss: 1.1531
2022-08-25 15:09:08 - train: epoch 0081, iter [03400, 05004], lr: 0.008931, loss: 1.3120
2022-08-25 15:09:42 - train: epoch 0081, iter [03500, 05004], lr: 0.008913, loss: 1.2364
2022-08-25 15:10:17 - train: epoch 0081, iter [03600, 05004], lr: 0.008895, loss: 1.1106
2022-08-25 15:10:52 - train: epoch 0081, iter [03700, 05004], lr: 0.008877, loss: 1.2956
2022-08-25 15:11:26 - train: epoch 0081, iter [03800, 05004], lr: 0.008860, loss: 1.0329
2022-08-25 15:12:01 - train: epoch 0081, iter [03900, 05004], lr: 0.008842, loss: 1.2998
2022-08-25 15:12:36 - train: epoch 0081, iter [04000, 05004], lr: 0.008824, loss: 0.9669
2022-08-25 15:13:09 - train: epoch 0081, iter [04100, 05004], lr: 0.008806, loss: 1.3024
2022-08-25 15:13:44 - train: epoch 0081, iter [04200, 05004], lr: 0.008788, loss: 1.1807
2022-08-25 15:14:19 - train: epoch 0081, iter [04300, 05004], lr: 0.008771, loss: 1.1635
2022-08-25 15:14:53 - train: epoch 0081, iter [04400, 05004], lr: 0.008753, loss: 1.2254
2022-08-25 15:15:27 - train: epoch 0081, iter [04500, 05004], lr: 0.008735, loss: 1.1914
2022-08-25 15:16:01 - train: epoch 0081, iter [04600, 05004], lr: 0.008717, loss: 1.1089
2022-08-25 15:16:35 - train: epoch 0081, iter [04700, 05004], lr: 0.008700, loss: 1.2135
2022-08-25 15:17:10 - train: epoch 0081, iter [04800, 05004], lr: 0.008682, loss: 1.1523
2022-08-25 15:17:44 - train: epoch 0081, iter [04900, 05004], lr: 0.008664, loss: 1.2152
2022-08-25 15:18:17 - train: epoch 0081, iter [05000, 05004], lr: 0.008647, loss: 1.0018
2022-08-25 15:18:18 - train: epoch 081, train_loss: 1.1752
2022-08-25 15:19:34 - eval: epoch: 081, acc1: 73.246%, acc5: 91.666%, test_loss: 1.0706, per_image_load_time: 1.277ms, per_image_inference_time: 0.575ms
2022-08-25 15:19:35 - until epoch: 081, best_acc1: 73.246%
2022-08-25 15:19:35 - epoch 082 lr: 0.008646
2022-08-25 15:20:14 - train: epoch 0082, iter [00100, 05004], lr: 0.008628, loss: 0.9091
2022-08-25 15:20:48 - train: epoch 0082, iter [00200, 05004], lr: 0.008611, loss: 0.9972
2022-08-25 15:21:22 - train: epoch 0082, iter [00300, 05004], lr: 0.008593, loss: 1.1893
2022-08-25 15:21:56 - train: epoch 0082, iter [00400, 05004], lr: 0.008576, loss: 1.2383
2022-08-25 15:22:30 - train: epoch 0082, iter [00500, 05004], lr: 0.008558, loss: 1.2275
2022-08-25 15:23:05 - train: epoch 0082, iter [00600, 05004], lr: 0.008540, loss: 1.0268
2022-08-25 15:23:39 - train: epoch 0082, iter [00700, 05004], lr: 0.008523, loss: 1.2631
2022-08-25 15:24:13 - train: epoch 0082, iter [00800, 05004], lr: 0.008505, loss: 1.2134
2022-08-25 15:24:47 - train: epoch 0082, iter [00900, 05004], lr: 0.008488, loss: 1.3212
2022-08-25 15:25:21 - train: epoch 0082, iter [01000, 05004], lr: 0.008470, loss: 1.2153
2022-08-25 15:25:55 - train: epoch 0082, iter [01100, 05004], lr: 0.008453, loss: 1.2829
2022-08-25 15:26:29 - train: epoch 0082, iter [01200, 05004], lr: 0.008435, loss: 1.1619
2022-08-25 15:27:03 - train: epoch 0082, iter [01300, 05004], lr: 0.008418, loss: 1.3343
2022-08-25 15:27:37 - train: epoch 0082, iter [01400, 05004], lr: 0.008401, loss: 1.1002
2022-08-25 15:28:12 - train: epoch 0082, iter [01500, 05004], lr: 0.008383, loss: 1.0430
2022-08-25 15:28:46 - train: epoch 0082, iter [01600, 05004], lr: 0.008366, loss: 1.2495
2022-08-25 15:29:21 - train: epoch 0082, iter [01700, 05004], lr: 0.008348, loss: 1.0820
2022-08-25 15:29:55 - train: epoch 0082, iter [01800, 05004], lr: 0.008331, loss: 1.0570
2022-08-25 15:30:29 - train: epoch 0082, iter [01900, 05004], lr: 0.008314, loss: 1.0299
2022-08-25 15:31:04 - train: epoch 0082, iter [02000, 05004], lr: 0.008296, loss: 1.0297
2022-08-25 15:31:37 - train: epoch 0082, iter [02100, 05004], lr: 0.008279, loss: 1.0874
2022-08-25 15:32:12 - train: epoch 0082, iter [02200, 05004], lr: 0.008262, loss: 1.2139
2022-08-25 15:32:46 - train: epoch 0082, iter [02300, 05004], lr: 0.008244, loss: 1.1566
2022-08-25 15:33:21 - train: epoch 0082, iter [02400, 05004], lr: 0.008227, loss: 1.2260
2022-08-25 15:33:55 - train: epoch 0082, iter [02500, 05004], lr: 0.008210, loss: 1.0728
2022-08-25 15:34:30 - train: epoch 0082, iter [02600, 05004], lr: 0.008193, loss: 1.0462
2022-08-25 15:35:04 - train: epoch 0082, iter [02700, 05004], lr: 0.008176, loss: 1.0495
2022-08-25 15:35:39 - train: epoch 0082, iter [02800, 05004], lr: 0.008158, loss: 1.0105
2022-08-25 15:36:13 - train: epoch 0082, iter [02900, 05004], lr: 0.008141, loss: 0.9896
2022-08-25 15:36:48 - train: epoch 0082, iter [03000, 05004], lr: 0.008124, loss: 1.0528
2022-08-25 15:37:23 - train: epoch 0082, iter [03100, 05004], lr: 0.008107, loss: 1.0666
2022-08-25 15:37:56 - train: epoch 0082, iter [03200, 05004], lr: 0.008090, loss: 1.3942
2022-08-25 15:38:31 - train: epoch 0082, iter [03300, 05004], lr: 0.008073, loss: 1.1314
2022-08-25 15:39:06 - train: epoch 0082, iter [03400, 05004], lr: 0.008056, loss: 1.0590
2022-08-25 15:39:40 - train: epoch 0082, iter [03500, 05004], lr: 0.008038, loss: 1.0745
2022-08-25 15:40:15 - train: epoch 0082, iter [03600, 05004], lr: 0.008021, loss: 0.9376
2022-08-25 15:40:49 - train: epoch 0082, iter [03700, 05004], lr: 0.008004, loss: 1.1306
2022-08-25 15:41:23 - train: epoch 0082, iter [03800, 05004], lr: 0.007987, loss: 1.3041
2022-08-25 15:41:58 - train: epoch 0082, iter [03900, 05004], lr: 0.007970, loss: 1.1702
2022-08-25 15:42:33 - train: epoch 0082, iter [04000, 05004], lr: 0.007953, loss: 1.2919
2022-08-25 15:43:07 - train: epoch 0082, iter [04100, 05004], lr: 0.007936, loss: 1.1427
2022-08-25 15:43:41 - train: epoch 0082, iter [04200, 05004], lr: 0.007919, loss: 1.2235
2022-08-25 15:44:16 - train: epoch 0082, iter [04300, 05004], lr: 0.007902, loss: 0.9985
2022-08-25 15:44:51 - train: epoch 0082, iter [04400, 05004], lr: 0.007886, loss: 1.0905
2022-08-25 15:45:25 - train: epoch 0082, iter [04500, 05004], lr: 0.007869, loss: 1.1899
2022-08-25 15:45:59 - train: epoch 0082, iter [04600, 05004], lr: 0.007852, loss: 1.1938
2022-08-25 15:46:35 - train: epoch 0082, iter [04700, 05004], lr: 0.007835, loss: 1.0191
2022-08-25 15:47:08 - train: epoch 0082, iter [04800, 05004], lr: 0.007818, loss: 1.0385
2022-08-25 15:47:43 - train: epoch 0082, iter [04900, 05004], lr: 0.007801, loss: 1.1550
2022-08-25 15:48:16 - train: epoch 0082, iter [05000, 05004], lr: 0.007784, loss: 1.1869
2022-08-25 15:48:18 - train: epoch 082, train_loss: 1.1528
2022-08-25 15:49:33 - eval: epoch: 082, acc1: 73.628%, acc5: 91.684%, test_loss: 1.0616, per_image_load_time: 1.308ms, per_image_inference_time: 0.590ms
2022-08-25 15:49:33 - until epoch: 082, best_acc1: 73.628%
2022-08-25 15:49:33 - epoch 083 lr: 0.007783
2022-08-25 15:50:13 - train: epoch 0083, iter [00100, 05004], lr: 0.007767, loss: 1.0307
2022-08-25 15:50:48 - train: epoch 0083, iter [00200, 05004], lr: 0.007750, loss: 1.0299
2022-08-25 15:51:21 - train: epoch 0083, iter [00300, 05004], lr: 0.007733, loss: 1.1396
2022-08-25 15:51:56 - train: epoch 0083, iter [00400, 05004], lr: 0.007716, loss: 1.2931
2022-08-25 15:52:31 - train: epoch 0083, iter [00500, 05004], lr: 0.007700, loss: 1.1191
2022-08-25 15:53:05 - train: epoch 0083, iter [00600, 05004], lr: 0.007683, loss: 1.0606
2022-08-25 15:53:39 - train: epoch 0083, iter [00700, 05004], lr: 0.007666, loss: 0.9978
2022-08-25 15:54:13 - train: epoch 0083, iter [00800, 05004], lr: 0.007650, loss: 1.1711
2022-08-25 15:54:47 - train: epoch 0083, iter [00900, 05004], lr: 0.007633, loss: 1.1944
2022-08-25 15:55:21 - train: epoch 0083, iter [01000, 05004], lr: 0.007616, loss: 1.2838
2022-08-25 15:55:56 - train: epoch 0083, iter [01100, 05004], lr: 0.007600, loss: 1.1572
2022-08-25 15:56:30 - train: epoch 0083, iter [01200, 05004], lr: 0.007583, loss: 1.0846
2022-08-25 15:57:04 - train: epoch 0083, iter [01300, 05004], lr: 0.007566, loss: 0.9293
2022-08-25 15:57:39 - train: epoch 0083, iter [01400, 05004], lr: 0.007550, loss: 1.1721
2022-08-25 15:58:14 - train: epoch 0083, iter [01500, 05004], lr: 0.007533, loss: 1.0105
2022-08-25 15:58:48 - train: epoch 0083, iter [01600, 05004], lr: 0.007517, loss: 1.1033
2022-08-25 15:59:23 - train: epoch 0083, iter [01700, 05004], lr: 0.007500, loss: 1.2391
2022-08-25 15:59:57 - train: epoch 0083, iter [01800, 05004], lr: 0.007484, loss: 1.3053
2022-08-25 16:00:32 - train: epoch 0083, iter [01900, 05004], lr: 0.007467, loss: 1.0211
2022-08-25 16:01:06 - train: epoch 0083, iter [02000, 05004], lr: 0.007451, loss: 0.8616
2022-08-25 16:01:40 - train: epoch 0083, iter [02100, 05004], lr: 0.007434, loss: 1.0462
2022-08-25 16:02:15 - train: epoch 0083, iter [02200, 05004], lr: 0.007418, loss: 1.0743
2022-08-25 16:02:50 - train: epoch 0083, iter [02300, 05004], lr: 0.007401, loss: 1.2052
2022-08-25 16:03:24 - train: epoch 0083, iter [02400, 05004], lr: 0.007385, loss: 1.0990
2022-08-25 16:03:59 - train: epoch 0083, iter [02500, 05004], lr: 0.007368, loss: 1.2114
2022-08-25 16:04:34 - train: epoch 0083, iter [02600, 05004], lr: 0.007352, loss: 1.0782
2022-08-25 16:05:08 - train: epoch 0083, iter [02700, 05004], lr: 0.007336, loss: 1.1507
2022-08-25 16:05:43 - train: epoch 0083, iter [02800, 05004], lr: 0.007319, loss: 0.9896
2022-08-25 16:06:18 - train: epoch 0083, iter [02900, 05004], lr: 0.007303, loss: 1.0758
2022-08-25 16:06:52 - train: epoch 0083, iter [03000, 05004], lr: 0.007287, loss: 1.2822
2022-08-25 16:07:26 - train: epoch 0083, iter [03100, 05004], lr: 0.007270, loss: 0.9343
2022-08-25 16:08:01 - train: epoch 0083, iter [03200, 05004], lr: 0.007254, loss: 0.9712
2022-08-25 16:08:36 - train: epoch 0083, iter [03300, 05004], lr: 0.007238, loss: 1.3246
2022-08-25 16:09:10 - train: epoch 0083, iter [03400, 05004], lr: 0.007221, loss: 1.2622
2022-08-25 16:09:45 - train: epoch 0083, iter [03500, 05004], lr: 0.007205, loss: 1.2590
2022-08-25 16:10:20 - train: epoch 0083, iter [03600, 05004], lr: 0.007189, loss: 1.1893
2022-08-25 16:10:54 - train: epoch 0083, iter [03700, 05004], lr: 0.007173, loss: 1.1904
2022-08-25 16:11:29 - train: epoch 0083, iter [03800, 05004], lr: 0.007157, loss: 1.0190
2022-08-25 16:12:04 - train: epoch 0083, iter [03900, 05004], lr: 0.007140, loss: 1.0367
2022-08-25 16:12:39 - train: epoch 0083, iter [04000, 05004], lr: 0.007124, loss: 1.3249
2022-08-25 16:13:13 - train: epoch 0083, iter [04100, 05004], lr: 0.007108, loss: 1.1415
2022-08-25 16:13:48 - train: epoch 0083, iter [04200, 05004], lr: 0.007092, loss: 1.2314
2022-08-25 16:14:23 - train: epoch 0083, iter [04300, 05004], lr: 0.007076, loss: 1.1138
2022-08-25 16:14:58 - train: epoch 0083, iter [04400, 05004], lr: 0.007060, loss: 0.9917
2022-08-25 16:15:33 - train: epoch 0083, iter [04500, 05004], lr: 0.007044, loss: 1.1099
2022-08-25 16:16:08 - train: epoch 0083, iter [04600, 05004], lr: 0.007028, loss: 1.1781
2022-08-25 16:16:42 - train: epoch 0083, iter [04700, 05004], lr: 0.007012, loss: 1.2481
2022-08-25 16:17:16 - train: epoch 0083, iter [04800, 05004], lr: 0.006996, loss: 1.1596
2022-08-25 16:17:52 - train: epoch 0083, iter [04900, 05004], lr: 0.006980, loss: 1.4392
2022-08-25 16:18:25 - train: epoch 0083, iter [05000, 05004], lr: 0.006964, loss: 1.2534
2022-08-25 16:18:26 - train: epoch 083, train_loss: 1.1303
2022-08-25 16:19:41 - eval: epoch: 083, acc1: 73.836%, acc5: 91.738%, test_loss: 1.0533, per_image_load_time: 2.107ms, per_image_inference_time: 0.631ms
2022-08-25 16:19:42 - until epoch: 083, best_acc1: 73.836%
2022-08-25 16:19:42 - epoch 084 lr: 0.006963
2022-08-25 16:20:21 - train: epoch 0084, iter [00100, 05004], lr: 0.006947, loss: 1.0981
2022-08-25 16:20:56 - train: epoch 0084, iter [00200, 05004], lr: 0.006931, loss: 1.1464
2022-08-25 16:21:30 - train: epoch 0084, iter [00300, 05004], lr: 0.006915, loss: 0.9197
2022-08-25 16:22:03 - train: epoch 0084, iter [00400, 05004], lr: 0.006899, loss: 1.1347
2022-08-25 16:22:38 - train: epoch 0084, iter [00500, 05004], lr: 0.006883, loss: 1.1423
2022-08-25 16:23:12 - train: epoch 0084, iter [00600, 05004], lr: 0.006867, loss: 1.3146
2022-08-25 16:23:46 - train: epoch 0084, iter [00700, 05004], lr: 0.006851, loss: 1.0930
2022-08-25 16:24:20 - train: epoch 0084, iter [00800, 05004], lr: 0.006836, loss: 1.2094
2022-08-25 16:24:54 - train: epoch 0084, iter [00900, 05004], lr: 0.006820, loss: 1.0841
2022-08-25 16:25:27 - train: epoch 0084, iter [01000, 05004], lr: 0.006804, loss: 1.1791
2022-08-25 16:26:01 - train: epoch 0084, iter [01100, 05004], lr: 0.006788, loss: 1.0948
2022-08-25 16:26:34 - train: epoch 0084, iter [01200, 05004], lr: 0.006772, loss: 1.2853
2022-08-25 16:27:09 - train: epoch 0084, iter [01300, 05004], lr: 0.006757, loss: 1.0858
2022-08-25 16:27:43 - train: epoch 0084, iter [01400, 05004], lr: 0.006741, loss: 1.2252
2022-08-25 16:28:17 - train: epoch 0084, iter [01500, 05004], lr: 0.006725, loss: 1.1221
2022-08-25 16:28:51 - train: epoch 0084, iter [01600, 05004], lr: 0.006709, loss: 1.0150
2022-08-25 16:29:25 - train: epoch 0084, iter [01700, 05004], lr: 0.006694, loss: 1.1901
2022-08-25 16:29:59 - train: epoch 0084, iter [01800, 05004], lr: 0.006678, loss: 1.0964
2022-08-25 16:30:32 - train: epoch 0084, iter [01900, 05004], lr: 0.006662, loss: 1.1127
2022-08-25 16:31:06 - train: epoch 0084, iter [02000, 05004], lr: 0.006647, loss: 1.2215
2022-08-25 16:31:39 - train: epoch 0084, iter [02100, 05004], lr: 0.006631, loss: 1.0357
2022-08-25 16:32:13 - train: epoch 0084, iter [02200, 05004], lr: 0.006615, loss: 0.9106
2022-08-25 16:32:47 - train: epoch 0084, iter [02300, 05004], lr: 0.006600, loss: 1.0341
2022-08-25 16:33:21 - train: epoch 0084, iter [02400, 05004], lr: 0.006584, loss: 1.0404
2022-08-25 16:33:55 - train: epoch 0084, iter [02500, 05004], lr: 0.006569, loss: 1.2669
2022-08-25 16:34:29 - train: epoch 0084, iter [02600, 05004], lr: 0.006553, loss: 1.2459
2022-08-25 16:35:03 - train: epoch 0084, iter [02700, 05004], lr: 0.006538, loss: 1.0892
2022-08-25 16:35:37 - train: epoch 0084, iter [02800, 05004], lr: 0.006522, loss: 1.1849
2022-08-25 16:36:11 - train: epoch 0084, iter [02900, 05004], lr: 0.006507, loss: 1.0468
2022-08-25 16:36:45 - train: epoch 0084, iter [03000, 05004], lr: 0.006491, loss: 1.0478
2022-08-25 16:37:19 - train: epoch 0084, iter [03100, 05004], lr: 0.006476, loss: 1.0447
2022-08-25 16:37:53 - train: epoch 0084, iter [03200, 05004], lr: 0.006460, loss: 0.9709
2022-08-25 16:38:27 - train: epoch 0084, iter [03300, 05004], lr: 0.006445, loss: 1.0546
2022-08-25 16:39:01 - train: epoch 0084, iter [03400, 05004], lr: 0.006429, loss: 1.0385
2022-08-25 16:39:35 - train: epoch 0084, iter [03500, 05004], lr: 0.006414, loss: 0.9400
2022-08-25 16:40:09 - train: epoch 0084, iter [03600, 05004], lr: 0.006399, loss: 1.1831
2022-08-25 16:40:43 - train: epoch 0084, iter [03700, 05004], lr: 0.006383, loss: 1.1984
2022-08-25 16:41:17 - train: epoch 0084, iter [03800, 05004], lr: 0.006368, loss: 1.3455
2022-08-25 16:41:51 - train: epoch 0084, iter [03900, 05004], lr: 0.006353, loss: 1.1998
2022-08-25 16:42:25 - train: epoch 0084, iter [04000, 05004], lr: 0.006337, loss: 1.1936
2022-08-25 16:42:59 - train: epoch 0084, iter [04100, 05004], lr: 0.006322, loss: 1.1295
2022-08-25 16:43:33 - train: epoch 0084, iter [04200, 05004], lr: 0.006307, loss: 1.1456
2022-08-25 16:44:07 - train: epoch 0084, iter [04300, 05004], lr: 0.006292, loss: 1.1500
2022-08-25 16:44:41 - train: epoch 0084, iter [04400, 05004], lr: 0.006276, loss: 1.3914
2022-08-25 16:45:15 - train: epoch 0084, iter [04500, 05004], lr: 0.006261, loss: 1.1531
2022-08-25 16:45:48 - train: epoch 0084, iter [04600, 05004], lr: 0.006246, loss: 1.0954
2022-08-25 16:46:22 - train: epoch 0084, iter [04700, 05004], lr: 0.006231, loss: 1.3312
2022-08-25 16:46:56 - train: epoch 0084, iter [04800, 05004], lr: 0.006216, loss: 1.0396
2022-08-25 16:47:30 - train: epoch 0084, iter [04900, 05004], lr: 0.006200, loss: 0.9839
2022-08-25 16:48:02 - train: epoch 0084, iter [05000, 05004], lr: 0.006185, loss: 1.1280
2022-08-25 16:48:03 - train: epoch 084, train_loss: 1.1073
2022-08-25 16:49:19 - eval: epoch: 084, acc1: 74.094%, acc5: 92.162%, test_loss: 1.0315, per_image_load_time: 1.505ms, per_image_inference_time: 0.609ms
2022-08-25 16:49:19 - until epoch: 084, best_acc1: 74.094%
2022-08-25 16:49:19 - epoch 085 lr: 0.006185
2022-08-25 16:49:59 - train: epoch 0085, iter [00100, 05004], lr: 0.006170, loss: 0.9050
2022-08-25 16:50:33 - train: epoch 0085, iter [00200, 05004], lr: 0.006154, loss: 0.9527
2022-08-25 16:51:07 - train: epoch 0085, iter [00300, 05004], lr: 0.006139, loss: 1.2924
2022-08-25 16:51:41 - train: epoch 0085, iter [00400, 05004], lr: 0.006124, loss: 0.9567
2022-08-25 16:52:15 - train: epoch 0085, iter [00500, 05004], lr: 0.006109, loss: 1.3438
2022-08-25 16:52:50 - train: epoch 0085, iter [00600, 05004], lr: 0.006094, loss: 0.8434
2022-08-25 16:53:23 - train: epoch 0085, iter [00700, 05004], lr: 0.006079, loss: 0.9664
2022-08-25 16:53:58 - train: epoch 0085, iter [00800, 05004], lr: 0.006064, loss: 0.9490
2022-08-25 16:54:31 - train: epoch 0085, iter [00900, 05004], lr: 0.006049, loss: 1.0213
2022-08-25 16:55:06 - train: epoch 0085, iter [01000, 05004], lr: 0.006034, loss: 1.1406
2022-08-25 16:55:40 - train: epoch 0085, iter [01100, 05004], lr: 0.006019, loss: 1.0693
2022-08-25 16:56:14 - train: epoch 0085, iter [01200, 05004], lr: 0.006004, loss: 1.0565
2022-08-25 16:56:48 - train: epoch 0085, iter [01300, 05004], lr: 0.005990, loss: 1.1063
2022-08-25 16:57:22 - train: epoch 0085, iter [01400, 05004], lr: 0.005975, loss: 1.0461
2022-08-25 16:57:57 - train: epoch 0085, iter [01500, 05004], lr: 0.005960, loss: 0.9467
2022-08-25 16:58:31 - train: epoch 0085, iter [01600, 05004], lr: 0.005945, loss: 1.0805
2022-08-25 16:59:06 - train: epoch 0085, iter [01700, 05004], lr: 0.005930, loss: 1.2952
2022-08-25 16:59:40 - train: epoch 0085, iter [01800, 05004], lr: 0.005915, loss: 0.9103
2022-08-25 17:00:15 - train: epoch 0085, iter [01900, 05004], lr: 0.005900, loss: 0.9994
2022-08-25 17:00:49 - train: epoch 0085, iter [02000, 05004], lr: 0.005886, loss: 1.0330
2022-08-25 17:01:23 - train: epoch 0085, iter [02100, 05004], lr: 0.005871, loss: 0.8905
2022-08-25 17:01:58 - train: epoch 0085, iter [02200, 05004], lr: 0.005856, loss: 1.1805
2022-08-25 17:02:32 - train: epoch 0085, iter [02300, 05004], lr: 0.005841, loss: 0.9066
2022-08-25 17:03:06 - train: epoch 0085, iter [02400, 05004], lr: 0.005827, loss: 1.1998
2022-08-25 17:03:40 - train: epoch 0085, iter [02500, 05004], lr: 0.005812, loss: 1.3005
2022-08-25 17:04:14 - train: epoch 0085, iter [02600, 05004], lr: 0.005797, loss: 1.1425
2022-08-25 17:04:48 - train: epoch 0085, iter [02700, 05004], lr: 0.005783, loss: 0.9742
2022-08-25 17:05:23 - train: epoch 0085, iter [02800, 05004], lr: 0.005768, loss: 1.2510
2022-08-25 17:05:57 - train: epoch 0085, iter [02900, 05004], lr: 0.005753, loss: 1.2183
2022-08-25 17:06:31 - train: epoch 0085, iter [03000, 05004], lr: 0.005739, loss: 1.2172
2022-08-25 17:07:05 - train: epoch 0085, iter [03100, 05004], lr: 0.005724, loss: 1.0255
2022-08-25 17:07:39 - train: epoch 0085, iter [03200, 05004], lr: 0.005710, loss: 1.0129
2022-08-25 17:08:13 - train: epoch 0085, iter [03300, 05004], lr: 0.005695, loss: 1.1123
2022-08-25 17:08:47 - train: epoch 0085, iter [03400, 05004], lr: 0.005681, loss: 1.1956
2022-08-25 17:09:22 - train: epoch 0085, iter [03500, 05004], lr: 0.005666, loss: 1.2004
2022-08-25 17:09:56 - train: epoch 0085, iter [03600, 05004], lr: 0.005651, loss: 1.1168
2022-08-25 17:10:31 - train: epoch 0085, iter [03700, 05004], lr: 0.005637, loss: 0.9341
2022-08-25 17:11:05 - train: epoch 0085, iter [03800, 05004], lr: 0.005623, loss: 1.0340
2022-08-25 17:11:39 - train: epoch 0085, iter [03900, 05004], lr: 0.005608, loss: 1.0681
2022-08-25 17:12:13 - train: epoch 0085, iter [04000, 05004], lr: 0.005594, loss: 1.3077
2022-08-25 17:12:47 - train: epoch 0085, iter [04100, 05004], lr: 0.005579, loss: 1.1365
2022-08-25 17:13:21 - train: epoch 0085, iter [04200, 05004], lr: 0.005565, loss: 1.1283
2022-08-25 17:13:55 - train: epoch 0085, iter [04300, 05004], lr: 0.005550, loss: 1.0843
2022-08-25 17:14:29 - train: epoch 0085, iter [04400, 05004], lr: 0.005536, loss: 1.0762
2022-08-25 17:15:03 - train: epoch 0085, iter [04500, 05004], lr: 0.005522, loss: 1.2083
2022-08-25 17:15:37 - train: epoch 0085, iter [04600, 05004], lr: 0.005507, loss: 1.1095
2022-08-25 17:16:11 - train: epoch 0085, iter [04700, 05004], lr: 0.005493, loss: 1.3699
2022-08-25 17:16:45 - train: epoch 0085, iter [04800, 05004], lr: 0.005479, loss: 0.9295
2022-08-25 17:17:19 - train: epoch 0085, iter [04900, 05004], lr: 0.005465, loss: 0.9969
2022-08-25 17:17:52 - train: epoch 0085, iter [05000, 05004], lr: 0.005450, loss: 1.0841
2022-08-25 17:17:54 - train: epoch 085, train_loss: 1.0870
2022-08-25 17:19:09 - eval: epoch: 085, acc1: 74.406%, acc5: 92.244%, test_loss: 1.0263, per_image_load_time: 2.054ms, per_image_inference_time: 0.605ms
2022-08-25 17:19:09 - until epoch: 085, best_acc1: 74.406%
2022-08-25 17:19:09 - epoch 086 lr: 0.005450
2022-08-25 17:19:49 - train: epoch 0086, iter [00100, 05004], lr: 0.005435, loss: 0.8992
2022-08-25 17:20:23 - train: epoch 0086, iter [00200, 05004], lr: 0.005421, loss: 1.0047
2022-08-25 17:20:56 - train: epoch 0086, iter [00300, 05004], lr: 0.005407, loss: 1.2271
2022-08-25 17:21:30 - train: epoch 0086, iter [00400, 05004], lr: 0.005393, loss: 1.0841
2022-08-25 17:22:04 - train: epoch 0086, iter [00500, 05004], lr: 0.005379, loss: 1.0613
2022-08-25 17:22:38 - train: epoch 0086, iter [00600, 05004], lr: 0.005364, loss: 1.0813
2022-08-25 17:23:11 - train: epoch 0086, iter [00700, 05004], lr: 0.005350, loss: 0.9987
2022-08-25 17:23:45 - train: epoch 0086, iter [00800, 05004], lr: 0.005336, loss: 1.2081
2022-08-25 17:24:18 - train: epoch 0086, iter [00900, 05004], lr: 0.005322, loss: 1.1485
2022-08-25 17:24:52 - train: epoch 0086, iter [01000, 05004], lr: 0.005308, loss: 1.0354
2022-08-25 17:25:25 - train: epoch 0086, iter [01100, 05004], lr: 0.005294, loss: 1.1578
2022-08-25 17:25:59 - train: epoch 0086, iter [01200, 05004], lr: 0.005280, loss: 0.9281
2022-08-25 17:26:32 - train: epoch 0086, iter [01300, 05004], lr: 0.005266, loss: 1.1561
2022-08-25 17:27:05 - train: epoch 0086, iter [01400, 05004], lr: 0.005252, loss: 0.9031
2022-08-25 17:27:39 - train: epoch 0086, iter [01500, 05004], lr: 0.005238, loss: 0.9187
2022-08-25 17:28:12 - train: epoch 0086, iter [01600, 05004], lr: 0.005224, loss: 1.1391
2022-08-25 17:28:45 - train: epoch 0086, iter [01700, 05004], lr: 0.005210, loss: 1.0509
2022-08-25 17:29:20 - train: epoch 0086, iter [01800, 05004], lr: 0.005196, loss: 1.0658
2022-08-25 17:29:53 - train: epoch 0086, iter [01900, 05004], lr: 0.005182, loss: 1.1987
2022-08-25 17:30:27 - train: epoch 0086, iter [02000, 05004], lr: 0.005168, loss: 1.0084
2022-08-25 17:31:01 - train: epoch 0086, iter [02100, 05004], lr: 0.005154, loss: 0.9212
2022-08-25 17:31:35 - train: epoch 0086, iter [02200, 05004], lr: 0.005140, loss: 1.1022
2022-08-25 17:32:09 - train: epoch 0086, iter [02300, 05004], lr: 0.005127, loss: 1.0509
2022-08-25 17:32:42 - train: epoch 0086, iter [02400, 05004], lr: 0.005113, loss: 0.8991
2022-08-25 17:33:16 - train: epoch 0086, iter [02500, 05004], lr: 0.005099, loss: 0.9533
2022-08-25 17:33:50 - train: epoch 0086, iter [02600, 05004], lr: 0.005085, loss: 1.1073
2022-08-25 17:34:24 - train: epoch 0086, iter [02700, 05004], lr: 0.005071, loss: 0.9505
2022-08-25 17:34:57 - train: epoch 0086, iter [02800, 05004], lr: 0.005058, loss: 1.2564
2022-08-25 17:35:31 - train: epoch 0086, iter [02900, 05004], lr: 0.005044, loss: 1.0042
2022-08-25 17:36:04 - train: epoch 0086, iter [03000, 05004], lr: 0.005030, loss: 1.1399
2022-08-25 17:36:37 - train: epoch 0086, iter [03100, 05004], lr: 0.005016, loss: 1.0633
2022-08-25 17:37:11 - train: epoch 0086, iter [03200, 05004], lr: 0.005003, loss: 1.0817
2022-08-25 17:37:45 - train: epoch 0086, iter [03300, 05004], lr: 0.004989, loss: 1.0713
2022-08-25 17:38:19 - train: epoch 0086, iter [03400, 05004], lr: 0.004975, loss: 1.1211
2022-08-25 17:38:53 - train: epoch 0086, iter [03500, 05004], lr: 0.004962, loss: 1.0910
2022-08-25 17:39:27 - train: epoch 0086, iter [03600, 05004], lr: 0.004948, loss: 1.1866
2022-08-25 17:40:01 - train: epoch 0086, iter [03700, 05004], lr: 0.004934, loss: 1.2047
2022-08-25 17:40:35 - train: epoch 0086, iter [03800, 05004], lr: 0.004921, loss: 1.0530
2022-08-25 17:41:09 - train: epoch 0086, iter [03900, 05004], lr: 0.004907, loss: 1.0741
2022-08-25 17:41:43 - train: epoch 0086, iter [04000, 05004], lr: 0.004894, loss: 1.1355
2022-08-25 17:42:17 - train: epoch 0086, iter [04100, 05004], lr: 0.004880, loss: 1.0882
2022-08-25 17:42:52 - train: epoch 0086, iter [04200, 05004], lr: 0.004867, loss: 1.1340
2022-08-25 17:43:26 - train: epoch 0086, iter [04300, 05004], lr: 0.004853, loss: 1.1106
2022-08-25 17:44:00 - train: epoch 0086, iter [04400, 05004], lr: 0.004840, loss: 0.9712
2022-08-25 17:44:34 - train: epoch 0086, iter [04500, 05004], lr: 0.004826, loss: 0.9626
2022-08-25 17:45:07 - train: epoch 0086, iter [04600, 05004], lr: 0.004813, loss: 1.0098
2022-08-25 17:45:42 - train: epoch 0086, iter [04700, 05004], lr: 0.004799, loss: 1.1253
2022-08-25 17:46:15 - train: epoch 0086, iter [04800, 05004], lr: 0.004786, loss: 1.0551
2022-08-25 17:46:49 - train: epoch 0086, iter [04900, 05004], lr: 0.004773, loss: 1.1903
2022-08-25 17:47:22 - train: epoch 0086, iter [05000, 05004], lr: 0.004759, loss: 1.0913
2022-08-25 17:47:23 - train: epoch 086, train_loss: 1.0627
2022-08-25 17:48:39 - eval: epoch: 086, acc1: 74.720%, acc5: 92.370%, test_loss: 1.0176, per_image_load_time: 2.018ms, per_image_inference_time: 0.594ms
2022-08-25 17:48:39 - until epoch: 086, best_acc1: 74.720%
2022-08-25 17:48:39 - epoch 087 lr: 0.004759
2022-08-25 17:49:19 - train: epoch 0087, iter [00100, 05004], lr: 0.004745, loss: 1.0539
2022-08-25 17:49:52 - train: epoch 0087, iter [00200, 05004], lr: 0.004732, loss: 0.8707
2022-08-25 17:50:27 - train: epoch 0087, iter [00300, 05004], lr: 0.004719, loss: 1.2782
2022-08-25 17:51:01 - train: epoch 0087, iter [00400, 05004], lr: 0.004705, loss: 1.1962
2022-08-25 17:51:35 - train: epoch 0087, iter [00500, 05004], lr: 0.004692, loss: 0.8420
2022-08-25 17:52:08 - train: epoch 0087, iter [00600, 05004], lr: 0.004679, loss: 1.0830
2022-08-25 17:52:42 - train: epoch 0087, iter [00700, 05004], lr: 0.004666, loss: 0.8362
2022-08-25 17:53:15 - train: epoch 0087, iter [00800, 05004], lr: 0.004652, loss: 1.1081
2022-08-25 17:53:49 - train: epoch 0087, iter [00900, 05004], lr: 0.004639, loss: 1.1483
2022-08-25 17:54:23 - train: epoch 0087, iter [01000, 05004], lr: 0.004626, loss: 1.0073
2022-08-25 17:54:56 - train: epoch 0087, iter [01100, 05004], lr: 0.004613, loss: 1.0833
2022-08-25 17:55:30 - train: epoch 0087, iter [01200, 05004], lr: 0.004600, loss: 1.1147
2022-08-25 17:56:04 - train: epoch 0087, iter [01300, 05004], lr: 0.004586, loss: 1.1738
2022-08-25 17:56:38 - train: epoch 0087, iter [01400, 05004], lr: 0.004573, loss: 0.9835
2022-08-25 17:57:11 - train: epoch 0087, iter [01500, 05004], lr: 0.004560, loss: 0.9252
2022-08-25 17:57:44 - train: epoch 0087, iter [01600, 05004], lr: 0.004547, loss: 0.9567
2022-08-25 17:58:19 - train: epoch 0087, iter [01700, 05004], lr: 0.004534, loss: 1.1063
2022-08-25 17:58:52 - train: epoch 0087, iter [01800, 05004], lr: 0.004521, loss: 1.0949
2022-08-25 17:59:26 - train: epoch 0087, iter [01900, 05004], lr: 0.004508, loss: 1.0735
2022-08-25 17:59:59 - train: epoch 0087, iter [02000, 05004], lr: 0.004495, loss: 0.9685
2022-08-25 18:00:33 - train: epoch 0087, iter [02100, 05004], lr: 0.004482, loss: 1.1562
2022-08-25 18:01:07 - train: epoch 0087, iter [02200, 05004], lr: 0.004469, loss: 1.0040
2022-08-25 18:01:40 - train: epoch 0087, iter [02300, 05004], lr: 0.004456, loss: 1.0964
2022-08-25 18:02:14 - train: epoch 0087, iter [02400, 05004], lr: 0.004443, loss: 1.0736
2022-08-25 18:02:48 - train: epoch 0087, iter [02500, 05004], lr: 0.004430, loss: 1.0245
2022-08-25 18:03:21 - train: epoch 0087, iter [02600, 05004], lr: 0.004417, loss: 1.0507
2022-08-25 18:03:56 - train: epoch 0087, iter [02700, 05004], lr: 0.004404, loss: 0.9605
2022-08-25 18:04:30 - train: epoch 0087, iter [02800, 05004], lr: 0.004391, loss: 0.8278
2022-08-25 18:05:03 - train: epoch 0087, iter [02900, 05004], lr: 0.004379, loss: 0.8498
2022-08-25 18:05:37 - train: epoch 0087, iter [03000, 05004], lr: 0.004366, loss: 1.0914
2022-08-25 18:06:11 - train: epoch 0087, iter [03100, 05004], lr: 0.004353, loss: 0.9968
2022-08-25 18:06:44 - train: epoch 0087, iter [03200, 05004], lr: 0.004340, loss: 0.9520
2022-08-25 18:07:18 - train: epoch 0087, iter [03300, 05004], lr: 0.004327, loss: 1.0521
2022-08-25 18:07:52 - train: epoch 0087, iter [03400, 05004], lr: 0.004315, loss: 1.0940
2022-08-25 18:08:25 - train: epoch 0087, iter [03500, 05004], lr: 0.004302, loss: 0.9356
2022-08-25 18:08:59 - train: epoch 0087, iter [03600, 05004], lr: 0.004289, loss: 0.8953
2022-08-25 18:09:33 - train: epoch 0087, iter [03700, 05004], lr: 0.004276, loss: 0.9479
2022-08-25 18:10:06 - train: epoch 0087, iter [03800, 05004], lr: 0.004264, loss: 1.0911
2022-08-25 18:10:40 - train: epoch 0087, iter [03900, 05004], lr: 0.004251, loss: 0.9465
2022-08-25 18:11:13 - train: epoch 0087, iter [04000, 05004], lr: 0.004238, loss: 1.1116
2022-08-25 18:11:47 - train: epoch 0087, iter [04100, 05004], lr: 0.004226, loss: 1.1425
2022-08-25 18:12:21 - train: epoch 0087, iter [04200, 05004], lr: 0.004213, loss: 1.1127
2022-08-25 18:12:55 - train: epoch 0087, iter [04300, 05004], lr: 0.004200, loss: 1.1471
2022-08-25 18:13:29 - train: epoch 0087, iter [04400, 05004], lr: 0.004188, loss: 1.1153
2022-08-25 18:14:03 - train: epoch 0087, iter [04500, 05004], lr: 0.004175, loss: 1.1091
2022-08-25 18:14:36 - train: epoch 0087, iter [04600, 05004], lr: 0.004163, loss: 1.0703
2022-08-25 18:15:10 - train: epoch 0087, iter [04700, 05004], lr: 0.004150, loss: 1.0593
2022-08-25 18:15:44 - train: epoch 0087, iter [04800, 05004], lr: 0.004138, loss: 0.9778
2022-08-25 18:16:17 - train: epoch 0087, iter [04900, 05004], lr: 0.004125, loss: 0.9494
2022-08-25 18:16:50 - train: epoch 0087, iter [05000, 05004], lr: 0.004113, loss: 1.0157
2022-08-25 18:16:52 - train: epoch 087, train_loss: 1.0403
2022-08-25 18:18:08 - eval: epoch: 087, acc1: 75.082%, acc5: 92.496%, test_loss: 0.9961, per_image_load_time: 2.395ms, per_image_inference_time: 0.579ms
2022-08-25 18:18:08 - until epoch: 087, best_acc1: 75.082%
2022-08-25 18:18:08 - epoch 088 lr: 0.004112
2022-08-25 18:18:48 - train: epoch 0088, iter [00100, 05004], lr: 0.004100, loss: 0.9894
2022-08-25 18:19:22 - train: epoch 0088, iter [00200, 05004], lr: 0.004087, loss: 0.9251
2022-08-25 18:19:56 - train: epoch 0088, iter [00300, 05004], lr: 0.004075, loss: 1.0379
2022-08-25 18:20:30 - train: epoch 0088, iter [00400, 05004], lr: 0.004063, loss: 0.9730
2022-08-25 18:21:04 - train: epoch 0088, iter [00500, 05004], lr: 0.004050, loss: 0.9090
2022-08-25 18:21:38 - train: epoch 0088, iter [00600, 05004], lr: 0.004038, loss: 1.0315
2022-08-25 18:22:12 - train: epoch 0088, iter [00700, 05004], lr: 0.004025, loss: 0.9202
2022-08-25 18:22:46 - train: epoch 0088, iter [00800, 05004], lr: 0.004013, loss: 0.9399
2022-08-25 18:23:19 - train: epoch 0088, iter [00900, 05004], lr: 0.004001, loss: 1.0925
2022-08-25 18:23:53 - train: epoch 0088, iter [01000, 05004], lr: 0.003989, loss: 0.9761
2022-08-25 18:24:27 - train: epoch 0088, iter [01100, 05004], lr: 0.003976, loss: 1.0193
2022-08-25 18:25:00 - train: epoch 0088, iter [01200, 05004], lr: 0.003964, loss: 0.9851
2022-08-25 18:25:34 - train: epoch 0088, iter [01300, 05004], lr: 0.003952, loss: 1.1611
2022-08-25 18:26:07 - train: epoch 0088, iter [01400, 05004], lr: 0.003940, loss: 1.0090
2022-08-25 18:26:41 - train: epoch 0088, iter [01500, 05004], lr: 0.003927, loss: 1.0332
2022-08-25 18:27:15 - train: epoch 0088, iter [01600, 05004], lr: 0.003915, loss: 0.8711
2022-08-25 18:27:48 - train: epoch 0088, iter [01700, 05004], lr: 0.003903, loss: 1.0973
2022-08-25 18:28:21 - train: epoch 0088, iter [01800, 05004], lr: 0.003891, loss: 1.0079
2022-08-25 18:28:55 - train: epoch 0088, iter [01900, 05004], lr: 0.003879, loss: 1.0939
2022-08-25 18:29:28 - train: epoch 0088, iter [02000, 05004], lr: 0.003867, loss: 0.8532
2022-08-25 18:30:02 - train: epoch 0088, iter [02100, 05004], lr: 0.003854, loss: 0.9656
2022-08-25 18:30:35 - train: epoch 0088, iter [02200, 05004], lr: 0.003842, loss: 0.9466
2022-08-25 18:31:09 - train: epoch 0088, iter [02300, 05004], lr: 0.003830, loss: 0.8217
2022-08-25 18:31:43 - train: epoch 0088, iter [02400, 05004], lr: 0.003818, loss: 1.0423
2022-08-25 18:32:17 - train: epoch 0088, iter [02500, 05004], lr: 0.003806, loss: 1.0996
2022-08-25 18:32:51 - train: epoch 0088, iter [02600, 05004], lr: 0.003794, loss: 1.1219
2022-08-25 18:33:25 - train: epoch 0088, iter [02700, 05004], lr: 0.003782, loss: 1.0369
2022-08-25 18:33:59 - train: epoch 0088, iter [02800, 05004], lr: 0.003770, loss: 1.0082
2022-08-25 18:34:32 - train: epoch 0088, iter [02900, 05004], lr: 0.003758, loss: 0.9406
2022-08-25 18:35:06 - train: epoch 0088, iter [03000, 05004], lr: 0.003746, loss: 1.1284
2022-08-25 18:35:40 - train: epoch 0088, iter [03100, 05004], lr: 0.003735, loss: 0.8831
2022-08-25 18:36:13 - train: epoch 0088, iter [03200, 05004], lr: 0.003723, loss: 0.9182
2022-08-25 18:36:47 - train: epoch 0088, iter [03300, 05004], lr: 0.003711, loss: 0.8955
2022-08-25 18:37:21 - train: epoch 0088, iter [03400, 05004], lr: 0.003699, loss: 0.8674
2022-08-25 18:37:54 - train: epoch 0088, iter [03500, 05004], lr: 0.003687, loss: 0.9735
2022-08-25 18:38:28 - train: epoch 0088, iter [03600, 05004], lr: 0.003675, loss: 1.0680
2022-08-25 18:39:02 - train: epoch 0088, iter [03700, 05004], lr: 0.003663, loss: 1.0406
2022-08-25 18:39:36 - train: epoch 0088, iter [03800, 05004], lr: 0.003652, loss: 1.0191
2022-08-25 18:40:09 - train: epoch 0088, iter [03900, 05004], lr: 0.003640, loss: 1.1032
2022-08-25 18:40:43 - train: epoch 0088, iter [04000, 05004], lr: 0.003628, loss: 0.9268
2022-08-25 18:41:17 - train: epoch 0088, iter [04100, 05004], lr: 0.003616, loss: 1.1005
2022-08-25 18:41:51 - train: epoch 0088, iter [04200, 05004], lr: 0.003605, loss: 1.1728
2022-08-25 18:42:25 - train: epoch 0088, iter [04300, 05004], lr: 0.003593, loss: 0.8950
2022-08-25 18:42:58 - train: epoch 0088, iter [04400, 05004], lr: 0.003581, loss: 0.9717
2022-08-25 18:43:31 - train: epoch 0088, iter [04500, 05004], lr: 0.003570, loss: 1.0823
2022-08-25 18:44:05 - train: epoch 0088, iter [04600, 05004], lr: 0.003558, loss: 1.0696
2022-08-25 18:44:39 - train: epoch 0088, iter [04700, 05004], lr: 0.003546, loss: 1.1155
2022-08-25 18:45:13 - train: epoch 0088, iter [04800, 05004], lr: 0.003535, loss: 0.8581
2022-08-25 18:45:48 - train: epoch 0088, iter [04900, 05004], lr: 0.003523, loss: 0.7937
2022-08-25 18:46:21 - train: epoch 0088, iter [05000, 05004], lr: 0.003512, loss: 1.0060
2022-08-25 18:46:22 - train: epoch 088, train_loss: 1.0177
2022-08-25 18:47:37 - eval: epoch: 088, acc1: 75.186%, acc5: 92.676%, test_loss: 0.9880, per_image_load_time: 2.261ms, per_image_inference_time: 0.601ms
2022-08-25 18:47:38 - until epoch: 088, best_acc1: 75.186%
2022-08-25 18:47:38 - epoch 089 lr: 0.003511
2022-08-25 18:48:18 - train: epoch 0089, iter [00100, 05004], lr: 0.003500, loss: 1.1987
2022-08-25 18:48:51 - train: epoch 0089, iter [00200, 05004], lr: 0.003488, loss: 0.7909
2022-08-25 18:49:25 - train: epoch 0089, iter [00300, 05004], lr: 0.003477, loss: 1.1101
2022-08-25 18:49:59 - train: epoch 0089, iter [00400, 05004], lr: 0.003465, loss: 0.9188
2022-08-25 18:50:33 - train: epoch 0089, iter [00500, 05004], lr: 0.003454, loss: 0.9765
2022-08-25 18:51:07 - train: epoch 0089, iter [00600, 05004], lr: 0.003442, loss: 0.9919
2022-08-25 18:51:40 - train: epoch 0089, iter [00700, 05004], lr: 0.003431, loss: 1.0475
2022-08-25 18:52:15 - train: epoch 0089, iter [00800, 05004], lr: 0.003419, loss: 1.1191
2022-08-25 18:52:49 - train: epoch 0089, iter [00900, 05004], lr: 0.003408, loss: 1.0188
2022-08-25 18:53:23 - train: epoch 0089, iter [01000, 05004], lr: 0.003397, loss: 1.1230
2022-08-25 18:53:56 - train: epoch 0089, iter [01100, 05004], lr: 0.003385, loss: 1.0083
2022-08-25 18:54:30 - train: epoch 0089, iter [01200, 05004], lr: 0.003374, loss: 1.0902
2022-08-25 18:55:03 - train: epoch 0089, iter [01300, 05004], lr: 0.003363, loss: 0.9760
2022-08-25 18:55:37 - train: epoch 0089, iter [01400, 05004], lr: 0.003351, loss: 1.0881
2022-08-25 18:56:10 - train: epoch 0089, iter [01500, 05004], lr: 0.003340, loss: 0.9983
2022-08-25 18:56:44 - train: epoch 0089, iter [01600, 05004], lr: 0.003329, loss: 0.9830
2022-08-25 18:57:18 - train: epoch 0089, iter [01700, 05004], lr: 0.003317, loss: 1.0728
2022-08-25 18:57:52 - train: epoch 0089, iter [01800, 05004], lr: 0.003306, loss: 0.9499
2022-08-25 18:58:26 - train: epoch 0089, iter [01900, 05004], lr: 0.003295, loss: 0.9169
2022-08-25 18:59:00 - train: epoch 0089, iter [02000, 05004], lr: 0.003284, loss: 0.9377
2022-08-25 18:59:34 - train: epoch 0089, iter [02100, 05004], lr: 0.003273, loss: 1.1735
2022-08-25 19:00:07 - train: epoch 0089, iter [02200, 05004], lr: 0.003261, loss: 1.1113
2022-08-25 19:00:41 - train: epoch 0089, iter [02300, 05004], lr: 0.003250, loss: 0.9320
2022-08-25 19:01:15 - train: epoch 0089, iter [02400, 05004], lr: 0.003239, loss: 1.0136
2022-08-25 19:01:49 - train: epoch 0089, iter [02500, 05004], lr: 0.003228, loss: 0.9520
2022-08-25 19:02:23 - train: epoch 0089, iter [02600, 05004], lr: 0.003217, loss: 1.0336
2022-08-25 19:02:56 - train: epoch 0089, iter [02700, 05004], lr: 0.003206, loss: 0.9637
2022-08-25 19:03:30 - train: epoch 0089, iter [02800, 05004], lr: 0.003195, loss: 1.0160
2022-08-25 19:04:04 - train: epoch 0089, iter [02900, 05004], lr: 0.003184, loss: 1.0609
2022-08-25 19:04:37 - train: epoch 0089, iter [03000, 05004], lr: 0.003173, loss: 1.0378
2022-08-25 19:05:10 - train: epoch 0089, iter [03100, 05004], lr: 0.003162, loss: 1.1506
2022-08-25 19:05:44 - train: epoch 0089, iter [03200, 05004], lr: 0.003151, loss: 1.1118
2022-08-25 19:06:18 - train: epoch 0089, iter [03300, 05004], lr: 0.003140, loss: 1.1890
2022-08-25 19:06:51 - train: epoch 0089, iter [03400, 05004], lr: 0.003129, loss: 1.0119
2022-08-25 19:07:26 - train: epoch 0089, iter [03500, 05004], lr: 0.003118, loss: 0.8233
2022-08-25 19:07:59 - train: epoch 0089, iter [03600, 05004], lr: 0.003107, loss: 0.8820
2022-08-25 19:08:32 - train: epoch 0089, iter [03700, 05004], lr: 0.003096, loss: 1.1984
2022-08-25 19:09:06 - train: epoch 0089, iter [03800, 05004], lr: 0.003085, loss: 0.9367
2022-08-25 19:09:40 - train: epoch 0089, iter [03900, 05004], lr: 0.003074, loss: 0.9951
2022-08-25 19:10:14 - train: epoch 0089, iter [04000, 05004], lr: 0.003064, loss: 0.8331
2022-08-25 19:10:47 - train: epoch 0089, iter [04100, 05004], lr: 0.003053, loss: 1.2224
2022-08-25 19:11:21 - train: epoch 0089, iter [04200, 05004], lr: 0.003042, loss: 0.9884
2022-08-25 19:11:55 - train: epoch 0089, iter [04300, 05004], lr: 0.003031, loss: 1.0151
2022-08-25 19:12:29 - train: epoch 0089, iter [04400, 05004], lr: 0.003021, loss: 0.9658
2022-08-25 19:13:02 - train: epoch 0089, iter [04500, 05004], lr: 0.003010, loss: 0.8651
2022-08-25 19:13:36 - train: epoch 0089, iter [04600, 05004], lr: 0.002999, loss: 1.0259
2022-08-25 19:14:10 - train: epoch 0089, iter [04700, 05004], lr: 0.002988, loss: 0.9567
2022-08-25 19:14:43 - train: epoch 0089, iter [04800, 05004], lr: 0.002978, loss: 1.0141
2022-08-25 19:15:18 - train: epoch 0089, iter [04900, 05004], lr: 0.002967, loss: 1.0006
2022-08-25 19:15:51 - train: epoch 0089, iter [05000, 05004], lr: 0.002956, loss: 0.7548
2022-08-25 19:15:52 - train: epoch 089, train_loss: 0.9976
2022-08-25 19:17:08 - eval: epoch: 089, acc1: 75.566%, acc5: 92.730%, test_loss: 0.9738, per_image_load_time: 2.374ms, per_image_inference_time: 0.598ms
2022-08-25 19:17:09 - until epoch: 089, best_acc1: 75.566%
2022-08-25 19:17:09 - epoch 090 lr: 0.002956
2022-08-25 19:17:49 - train: epoch 0090, iter [00100, 05004], lr: 0.002945, loss: 0.9782
2022-08-25 19:18:22 - train: epoch 0090, iter [00200, 05004], lr: 0.002935, loss: 1.0632
2022-08-25 19:18:56 - train: epoch 0090, iter [00300, 05004], lr: 0.002924, loss: 0.9392
2022-08-25 19:19:30 - train: epoch 0090, iter [00400, 05004], lr: 0.002914, loss: 0.9308
2022-08-25 19:20:03 - train: epoch 0090, iter [00500, 05004], lr: 0.002903, loss: 0.9738
2022-08-25 19:20:37 - train: epoch 0090, iter [00600, 05004], lr: 0.002892, loss: 1.0434
2022-08-25 19:21:10 - train: epoch 0090, iter [00700, 05004], lr: 0.002882, loss: 1.0140
2022-08-25 19:21:44 - train: epoch 0090, iter [00800, 05004], lr: 0.002871, loss: 0.9434
2022-08-25 19:22:19 - train: epoch 0090, iter [00900, 05004], lr: 0.002861, loss: 0.9227
2022-08-25 19:22:52 - train: epoch 0090, iter [01000, 05004], lr: 0.002851, loss: 0.7954
2022-08-25 19:23:25 - train: epoch 0090, iter [01100, 05004], lr: 0.002840, loss: 1.0338
2022-08-25 19:23:59 - train: epoch 0090, iter [01200, 05004], lr: 0.002830, loss: 0.9379
2022-08-25 19:24:33 - train: epoch 0090, iter [01300, 05004], lr: 0.002819, loss: 1.1370
2022-08-25 19:25:07 - train: epoch 0090, iter [01400, 05004], lr: 0.002809, loss: 0.8617
2022-08-25 19:25:41 - train: epoch 0090, iter [01500, 05004], lr: 0.002799, loss: 1.1712
2022-08-25 19:26:16 - train: epoch 0090, iter [01600, 05004], lr: 0.002788, loss: 0.9144
2022-08-25 19:26:49 - train: epoch 0090, iter [01700, 05004], lr: 0.002778, loss: 0.8219
2022-08-25 19:27:23 - train: epoch 0090, iter [01800, 05004], lr: 0.002768, loss: 0.8912
2022-08-25 19:27:56 - train: epoch 0090, iter [01900, 05004], lr: 0.002757, loss: 0.8376
2022-08-25 19:28:30 - train: epoch 0090, iter [02000, 05004], lr: 0.002747, loss: 1.0909
2022-08-25 19:29:04 - train: epoch 0090, iter [02100, 05004], lr: 0.002737, loss: 1.1149
2022-08-25 19:29:38 - train: epoch 0090, iter [02200, 05004], lr: 0.002727, loss: 0.9849
2022-08-25 19:30:12 - train: epoch 0090, iter [02300, 05004], lr: 0.002716, loss: 1.1288
2022-08-25 19:30:45 - train: epoch 0090, iter [02400, 05004], lr: 0.002706, loss: 0.9273
2022-08-25 19:31:19 - train: epoch 0090, iter [02500, 05004], lr: 0.002696, loss: 1.0740
2022-08-25 19:31:53 - train: epoch 0090, iter [02600, 05004], lr: 0.002686, loss: 1.0632
2022-08-25 19:32:26 - train: epoch 0090, iter [02700, 05004], lr: 0.002676, loss: 0.8793
2022-08-25 19:33:01 - train: epoch 0090, iter [02800, 05004], lr: 0.002666, loss: 0.9883
2022-08-25 19:33:35 - train: epoch 0090, iter [02900, 05004], lr: 0.002655, loss: 0.9358
2022-08-25 19:34:09 - train: epoch 0090, iter [03000, 05004], lr: 0.002645, loss: 0.9422
2022-08-25 19:34:43 - train: epoch 0090, iter [03100, 05004], lr: 0.002635, loss: 0.9202
2022-08-25 19:35:17 - train: epoch 0090, iter [03200, 05004], lr: 0.002625, loss: 0.8764
2022-08-25 19:35:50 - train: epoch 0090, iter [03300, 05004], lr: 0.002615, loss: 1.0820
2022-08-25 19:36:24 - train: epoch 0090, iter [03400, 05004], lr: 0.002605, loss: 0.8585
2022-08-25 19:36:57 - train: epoch 0090, iter [03500, 05004], lr: 0.002595, loss: 0.9023
2022-08-25 19:37:32 - train: epoch 0090, iter [03600, 05004], lr: 0.002585, loss: 0.9076
2022-08-25 19:38:06 - train: epoch 0090, iter [03700, 05004], lr: 0.002575, loss: 0.9401
2022-08-25 19:38:40 - train: epoch 0090, iter [03800, 05004], lr: 0.002565, loss: 1.0251
2022-08-25 19:39:14 - train: epoch 0090, iter [03900, 05004], lr: 0.002555, loss: 0.8314
2022-08-25 19:39:47 - train: epoch 0090, iter [04000, 05004], lr: 0.002546, loss: 0.9762
2022-08-25 19:40:21 - train: epoch 0090, iter [04100, 05004], lr: 0.002536, loss: 1.2002
2022-08-25 19:40:56 - train: epoch 0090, iter [04200, 05004], lr: 0.002526, loss: 1.0393
2022-08-25 19:41:29 - train: epoch 0090, iter [04300, 05004], lr: 0.002516, loss: 1.0550
2022-08-25 19:42:03 - train: epoch 0090, iter [04400, 05004], lr: 0.002506, loss: 0.9092
2022-08-25 19:42:37 - train: epoch 0090, iter [04500, 05004], lr: 0.002496, loss: 0.9533
2022-08-25 19:43:10 - train: epoch 0090, iter [04600, 05004], lr: 0.002487, loss: 0.9659
2022-08-25 19:43:45 - train: epoch 0090, iter [04700, 05004], lr: 0.002477, loss: 0.9176
2022-08-25 19:44:18 - train: epoch 0090, iter [04800, 05004], lr: 0.002467, loss: 1.0050
2022-08-25 19:44:53 - train: epoch 0090, iter [04900, 05004], lr: 0.002457, loss: 0.9138
2022-08-25 19:45:26 - train: epoch 0090, iter [05000, 05004], lr: 0.002448, loss: 0.8685
2022-08-25 19:45:27 - train: epoch 090, train_loss: 0.9766
2022-08-25 19:46:43 - eval: epoch: 090, acc1: 75.664%, acc5: 92.804%, test_loss: 0.9673, per_image_load_time: 2.346ms, per_image_inference_time: 0.595ms
2022-08-25 19:46:43 - until epoch: 090, best_acc1: 75.664%
2022-08-25 19:46:43 - epoch 091 lr: 0.002447
2022-08-25 19:47:22 - train: epoch 0091, iter [00100, 05004], lr: 0.002437, loss: 0.8440
2022-08-25 19:47:57 - train: epoch 0091, iter [00200, 05004], lr: 0.002428, loss: 0.9409
2022-08-25 19:48:31 - train: epoch 0091, iter [00300, 05004], lr: 0.002418, loss: 0.9104
2022-08-25 19:49:05 - train: epoch 0091, iter [00400, 05004], lr: 0.002409, loss: 0.9061
2022-08-25 19:49:39 - train: epoch 0091, iter [00500, 05004], lr: 0.002399, loss: 0.8718
2022-08-25 19:50:13 - train: epoch 0091, iter [00600, 05004], lr: 0.002389, loss: 0.9609
2022-08-25 19:50:47 - train: epoch 0091, iter [00700, 05004], lr: 0.002380, loss: 0.9251
2022-08-25 19:51:21 - train: epoch 0091, iter [00800, 05004], lr: 0.002370, loss: 0.8538
2022-08-25 19:51:55 - train: epoch 0091, iter [00900, 05004], lr: 0.002361, loss: 1.0087
2022-08-25 19:52:29 - train: epoch 0091, iter [01000, 05004], lr: 0.002351, loss: 0.9561
2022-08-25 19:53:03 - train: epoch 0091, iter [01100, 05004], lr: 0.002342, loss: 0.8166
2022-08-25 19:53:36 - train: epoch 0091, iter [01200, 05004], lr: 0.002332, loss: 1.0651
2022-08-25 19:54:11 - train: epoch 0091, iter [01300, 05004], lr: 0.002323, loss: 0.9089
2022-08-25 19:54:45 - train: epoch 0091, iter [01400, 05004], lr: 0.002313, loss: 1.0331
2022-08-25 19:55:19 - train: epoch 0091, iter [01500, 05004], lr: 0.002304, loss: 0.9399
2022-08-25 19:55:53 - train: epoch 0091, iter [01600, 05004], lr: 0.002294, loss: 0.9763
2022-08-25 19:56:27 - train: epoch 0091, iter [01700, 05004], lr: 0.002285, loss: 0.9417
2022-08-25 19:57:02 - train: epoch 0091, iter [01800, 05004], lr: 0.002276, loss: 1.0435
2022-08-25 19:57:35 - train: epoch 0091, iter [01900, 05004], lr: 0.002266, loss: 0.8672
2022-08-25 19:58:09 - train: epoch 0091, iter [02000, 05004], lr: 0.002257, loss: 0.9539
2022-08-25 19:58:43 - train: epoch 0091, iter [02100, 05004], lr: 0.002248, loss: 0.7167
2022-08-25 19:59:17 - train: epoch 0091, iter [02200, 05004], lr: 0.002238, loss: 0.9970
2022-08-25 19:59:51 - train: epoch 0091, iter [02300, 05004], lr: 0.002229, loss: 1.0588
2022-08-25 20:00:24 - train: epoch 0091, iter [02400, 05004], lr: 0.002220, loss: 0.8539
2022-08-25 20:00:58 - train: epoch 0091, iter [02500, 05004], lr: 0.002211, loss: 0.9422
2022-08-25 20:01:32 - train: epoch 0091, iter [02600, 05004], lr: 0.002201, loss: 0.8622
2022-08-25 20:02:06 - train: epoch 0091, iter [02700, 05004], lr: 0.002192, loss: 0.8072
2022-08-25 20:02:39 - train: epoch 0091, iter [02800, 05004], lr: 0.002183, loss: 0.9468
2022-08-25 20:03:14 - train: epoch 0091, iter [02900, 05004], lr: 0.002174, loss: 1.0536
2022-08-25 20:03:47 - train: epoch 0091, iter [03000, 05004], lr: 0.002165, loss: 1.0971
2022-08-25 20:04:21 - train: epoch 0091, iter [03100, 05004], lr: 0.002155, loss: 0.9531
2022-08-25 20:04:55 - train: epoch 0091, iter [03200, 05004], lr: 0.002146, loss: 1.0553
2022-08-25 20:05:29 - train: epoch 0091, iter [03300, 05004], lr: 0.002137, loss: 0.8834
2022-08-25 20:06:02 - train: epoch 0091, iter [03400, 05004], lr: 0.002128, loss: 0.9140
2022-08-25 20:06:36 - train: epoch 0091, iter [03500, 05004], lr: 0.002119, loss: 0.9973
2022-08-25 20:07:10 - train: epoch 0091, iter [03600, 05004], lr: 0.002110, loss: 0.9093
2022-08-25 20:07:45 - train: epoch 0091, iter [03700, 05004], lr: 0.002101, loss: 0.8863
2022-08-25 20:08:18 - train: epoch 0091, iter [03800, 05004], lr: 0.002092, loss: 0.9606
2022-08-25 20:08:52 - train: epoch 0091, iter [03900, 05004], lr: 0.002083, loss: 1.0020
2022-08-25 20:09:26 - train: epoch 0091, iter [04000, 05004], lr: 0.002074, loss: 1.0043
2022-08-25 20:09:59 - train: epoch 0091, iter [04100, 05004], lr: 0.002065, loss: 1.0259
2022-08-25 20:10:34 - train: epoch 0091, iter [04200, 05004], lr: 0.002056, loss: 1.0166
2022-08-25 20:11:07 - train: epoch 0091, iter [04300, 05004], lr: 0.002047, loss: 1.0821
2022-08-25 20:11:41 - train: epoch 0091, iter [04400, 05004], lr: 0.002039, loss: 0.8434
2022-08-25 20:12:14 - train: epoch 0091, iter [04500, 05004], lr: 0.002030, loss: 0.9068
2022-08-25 20:12:48 - train: epoch 0091, iter [04600, 05004], lr: 0.002021, loss: 0.9815
2022-08-25 20:13:22 - train: epoch 0091, iter [04700, 05004], lr: 0.002012, loss: 0.9816
2022-08-25 20:13:57 - train: epoch 0091, iter [04800, 05004], lr: 0.002003, loss: 1.0918
2022-08-25 20:14:30 - train: epoch 0091, iter [04900, 05004], lr: 0.001994, loss: 0.8814
2022-08-25 20:15:03 - train: epoch 0091, iter [05000, 05004], lr: 0.001986, loss: 1.1894
2022-08-25 20:15:05 - train: epoch 091, train_loss: 0.9562
2022-08-25 20:16:20 - eval: epoch: 091, acc1: 75.856%, acc5: 92.958%, test_loss: 0.9637, per_image_load_time: 2.190ms, per_image_inference_time: 0.601ms
2022-08-25 20:16:21 - until epoch: 091, best_acc1: 75.856%
2022-08-25 20:16:21 - epoch 092 lr: 0.001985
2022-08-25 20:17:01 - train: epoch 0092, iter [00100, 05004], lr: 0.001977, loss: 0.8685
2022-08-25 20:17:35 - train: epoch 0092, iter [00200, 05004], lr: 0.001968, loss: 1.0083
2022-08-25 20:18:08 - train: epoch 0092, iter [00300, 05004], lr: 0.001959, loss: 0.9323
2022-08-25 20:18:43 - train: epoch 0092, iter [00400, 05004], lr: 0.001950, loss: 0.7828
2022-08-25 20:19:17 - train: epoch 0092, iter [00500, 05004], lr: 0.001942, loss: 1.0421
2022-08-25 20:19:50 - train: epoch 0092, iter [00600, 05004], lr: 0.001933, loss: 0.9142
2022-08-25 20:20:25 - train: epoch 0092, iter [00700, 05004], lr: 0.001924, loss: 0.9121
2022-08-25 20:20:59 - train: epoch 0092, iter [00800, 05004], lr: 0.001916, loss: 1.0229
2022-08-25 20:21:33 - train: epoch 0092, iter [00900, 05004], lr: 0.001907, loss: 0.9185
2022-08-25 20:22:06 - train: epoch 0092, iter [01000, 05004], lr: 0.001899, loss: 1.1650
2022-08-25 20:22:41 - train: epoch 0092, iter [01100, 05004], lr: 0.001890, loss: 0.8303
2022-08-25 20:23:15 - train: epoch 0092, iter [01200, 05004], lr: 0.001882, loss: 0.8335
2022-08-25 20:23:48 - train: epoch 0092, iter [01300, 05004], lr: 0.001873, loss: 0.8786
2022-08-25 20:24:23 - train: epoch 0092, iter [01400, 05004], lr: 0.001865, loss: 0.9172
2022-08-25 20:24:56 - train: epoch 0092, iter [01500, 05004], lr: 0.001856, loss: 0.8923
2022-08-25 20:25:30 - train: epoch 0092, iter [01600, 05004], lr: 0.001848, loss: 0.9595
2022-08-25 20:26:03 - train: epoch 0092, iter [01700, 05004], lr: 0.001839, loss: 0.9777
2022-08-25 20:26:38 - train: epoch 0092, iter [01800, 05004], lr: 0.001831, loss: 0.8757
2022-08-25 20:27:11 - train: epoch 0092, iter [01900, 05004], lr: 0.001822, loss: 0.8826
2022-08-25 20:27:44 - train: epoch 0092, iter [02000, 05004], lr: 0.001814, loss: 0.8556
2022-08-25 20:28:19 - train: epoch 0092, iter [02100, 05004], lr: 0.001806, loss: 0.9042
2022-08-25 20:28:52 - train: epoch 0092, iter [02200, 05004], lr: 0.001797, loss: 1.0302
2022-08-25 20:29:26 - train: epoch 0092, iter [02300, 05004], lr: 0.001789, loss: 0.9022
2022-08-25 20:30:01 - train: epoch 0092, iter [02400, 05004], lr: 0.001781, loss: 0.8952
2022-08-25 20:30:34 - train: epoch 0092, iter [02500, 05004], lr: 0.001772, loss: 0.8918
2022-08-25 20:31:08 - train: epoch 0092, iter [02600, 05004], lr: 0.001764, loss: 1.0809
2022-08-25 20:31:43 - train: epoch 0092, iter [02700, 05004], lr: 0.001756, loss: 0.8773
2022-08-25 20:32:17 - train: epoch 0092, iter [02800, 05004], lr: 0.001748, loss: 0.8433
2022-08-25 20:32:51 - train: epoch 0092, iter [02900, 05004], lr: 0.001739, loss: 0.9509
2022-08-25 20:33:25 - train: epoch 0092, iter [03000, 05004], lr: 0.001731, loss: 0.8379
2022-08-25 20:33:58 - train: epoch 0092, iter [03100, 05004], lr: 0.001723, loss: 0.9051
2022-08-25 20:34:33 - train: epoch 0092, iter [03200, 05004], lr: 0.001715, loss: 0.7789
2022-08-25 20:35:07 - train: epoch 0092, iter [03300, 05004], lr: 0.001707, loss: 0.9258
2022-08-25 20:35:41 - train: epoch 0092, iter [03400, 05004], lr: 0.001699, loss: 1.1570
2022-08-25 20:36:15 - train: epoch 0092, iter [03500, 05004], lr: 0.001690, loss: 0.8312
2022-08-25 20:36:48 - train: epoch 0092, iter [03600, 05004], lr: 0.001682, loss: 0.9849
2022-08-25 20:37:23 - train: epoch 0092, iter [03700, 05004], lr: 0.001674, loss: 0.7730
2022-08-25 20:37:57 - train: epoch 0092, iter [03800, 05004], lr: 0.001666, loss: 1.0813
2022-08-25 20:38:30 - train: epoch 0092, iter [03900, 05004], lr: 0.001658, loss: 0.9228
2022-08-25 20:39:05 - train: epoch 0092, iter [04000, 05004], lr: 0.001650, loss: 1.1195
2022-08-25 20:39:38 - train: epoch 0092, iter [04100, 05004], lr: 0.001642, loss: 0.8248
2022-08-25 20:40:12 - train: epoch 0092, iter [04200, 05004], lr: 0.001634, loss: 0.9158
2022-08-25 20:40:46 - train: epoch 0092, iter [04300, 05004], lr: 0.001626, loss: 1.0113
2022-08-25 20:41:20 - train: epoch 0092, iter [04400, 05004], lr: 0.001618, loss: 0.7683
2022-08-25 20:41:54 - train: epoch 0092, iter [04500, 05004], lr: 0.001610, loss: 0.8293
2022-08-25 20:42:28 - train: epoch 0092, iter [04600, 05004], lr: 0.001603, loss: 0.8917
2022-08-25 20:43:02 - train: epoch 0092, iter [04700, 05004], lr: 0.001595, loss: 0.6617
2022-08-25 20:43:36 - train: epoch 0092, iter [04800, 05004], lr: 0.001587, loss: 0.8095
2022-08-25 20:44:10 - train: epoch 0092, iter [04900, 05004], lr: 0.001579, loss: 0.8571
2022-08-25 20:44:43 - train: epoch 0092, iter [05000, 05004], lr: 0.001571, loss: 1.0370
2022-08-25 20:44:45 - train: epoch 092, train_loss: 0.9373
2022-08-25 20:46:00 - eval: epoch: 092, acc1: 76.108%, acc5: 92.970%, test_loss: 0.9613, per_image_load_time: 2.337ms, per_image_inference_time: 0.589ms
2022-08-25 20:46:00 - until epoch: 092, best_acc1: 76.108%
2022-08-25 20:46:00 - epoch 093 lr: 0.001571
2022-08-25 20:46:39 - train: epoch 0093, iter [00100, 05004], lr: 0.001563, loss: 0.8079
2022-08-25 20:47:14 - train: epoch 0093, iter [00200, 05004], lr: 0.001555, loss: 0.8144
2022-08-25 20:47:47 - train: epoch 0093, iter [00300, 05004], lr: 0.001548, loss: 0.9497
2022-08-25 20:48:22 - train: epoch 0093, iter [00400, 05004], lr: 0.001540, loss: 0.8966
2022-08-25 20:48:56 - train: epoch 0093, iter [00500, 05004], lr: 0.001532, loss: 0.9996
2022-08-25 20:49:30 - train: epoch 0093, iter [00600, 05004], lr: 0.001524, loss: 0.8592
2022-08-25 20:50:04 - train: epoch 0093, iter [00700, 05004], lr: 0.001517, loss: 0.9809
2022-08-25 20:50:38 - train: epoch 0093, iter [00800, 05004], lr: 0.001509, loss: 0.8246
2022-08-25 20:51:11 - train: epoch 0093, iter [00900, 05004], lr: 0.001501, loss: 0.9141
2022-08-25 20:51:45 - train: epoch 0093, iter [01000, 05004], lr: 0.001494, loss: 0.7862
2022-08-25 20:52:19 - train: epoch 0093, iter [01100, 05004], lr: 0.001486, loss: 0.8150
2022-08-25 20:52:53 - train: epoch 0093, iter [01200, 05004], lr: 0.001479, loss: 0.7994
2022-08-25 20:53:27 - train: epoch 0093, iter [01300, 05004], lr: 0.001471, loss: 0.9818
2022-08-25 20:54:01 - train: epoch 0093, iter [01400, 05004], lr: 0.001463, loss: 0.9251
2022-08-25 20:54:35 - train: epoch 0093, iter [01500, 05004], lr: 0.001456, loss: 1.1867
2022-08-25 20:55:09 - train: epoch 0093, iter [01600, 05004], lr: 0.001448, loss: 1.0278
2022-08-25 20:55:43 - train: epoch 0093, iter [01700, 05004], lr: 0.001441, loss: 0.8422
2022-08-25 20:56:17 - train: epoch 0093, iter [01800, 05004], lr: 0.001433, loss: 0.9423
2022-08-25 20:56:51 - train: epoch 0093, iter [01900, 05004], lr: 0.001426, loss: 0.9082
2022-08-25 20:57:25 - train: epoch 0093, iter [02000, 05004], lr: 0.001419, loss: 0.8384
2022-08-25 20:57:59 - train: epoch 0093, iter [02100, 05004], lr: 0.001411, loss: 0.8365
2022-08-25 20:58:33 - train: epoch 0093, iter [02200, 05004], lr: 0.001404, loss: 1.1384
2022-08-25 20:59:06 - train: epoch 0093, iter [02300, 05004], lr: 0.001396, loss: 0.9006
2022-08-25 20:59:40 - train: epoch 0093, iter [02400, 05004], lr: 0.001389, loss: 0.8040
2022-08-25 21:00:14 - train: epoch 0093, iter [02500, 05004], lr: 0.001382, loss: 0.9016
2022-08-25 21:00:48 - train: epoch 0093, iter [02600, 05004], lr: 0.001374, loss: 1.0542
2022-08-25 21:01:21 - train: epoch 0093, iter [02700, 05004], lr: 0.001367, loss: 0.8272
2022-08-25 21:01:55 - train: epoch 0093, iter [02800, 05004], lr: 0.001360, loss: 0.8850
2022-08-25 21:02:29 - train: epoch 0093, iter [02900, 05004], lr: 0.001352, loss: 0.9008
2022-08-25 21:03:02 - train: epoch 0093, iter [03000, 05004], lr: 0.001345, loss: 0.8980
2022-08-25 21:03:37 - train: epoch 0093, iter [03100, 05004], lr: 0.001338, loss: 1.0504
2022-08-25 21:04:11 - train: epoch 0093, iter [03200, 05004], lr: 0.001331, loss: 0.8582
2022-08-25 21:04:45 - train: epoch 0093, iter [03300, 05004], lr: 0.001324, loss: 0.9937
2022-08-25 21:05:18 - train: epoch 0093, iter [03400, 05004], lr: 0.001316, loss: 1.1246
2022-08-25 21:05:51 - train: epoch 0093, iter [03500, 05004], lr: 0.001309, loss: 0.7468
2022-08-25 21:06:26 - train: epoch 0093, iter [03600, 05004], lr: 0.001302, loss: 0.9922
2022-08-25 21:06:59 - train: epoch 0093, iter [03700, 05004], lr: 0.001295, loss: 0.8715
2022-08-25 21:07:33 - train: epoch 0093, iter [03800, 05004], lr: 0.001288, loss: 0.7395
2022-08-25 21:08:07 - train: epoch 0093, iter [03900, 05004], lr: 0.001281, loss: 0.9745
2022-08-25 21:08:41 - train: epoch 0093, iter [04000, 05004], lr: 0.001274, loss: 1.1351
2022-08-25 21:09:15 - train: epoch 0093, iter [04100, 05004], lr: 0.001267, loss: 1.0355
2022-08-25 21:09:48 - train: epoch 0093, iter [04200, 05004], lr: 0.001260, loss: 0.9619
2022-08-25 21:10:22 - train: epoch 0093, iter [04300, 05004], lr: 0.001253, loss: 1.0889
2022-08-25 21:10:56 - train: epoch 0093, iter [04400, 05004], lr: 0.001246, loss: 0.8686
2022-08-25 21:11:30 - train: epoch 0093, iter [04500, 05004], lr: 0.001239, loss: 0.9027
2022-08-25 21:12:04 - train: epoch 0093, iter [04600, 05004], lr: 0.001232, loss: 0.8145
2022-08-25 21:12:37 - train: epoch 0093, iter [04700, 05004], lr: 0.001225, loss: 1.1339
2022-08-25 21:13:10 - train: epoch 0093, iter [04800, 05004], lr: 0.001218, loss: 0.8882
2022-08-25 21:13:43 - train: epoch 0093, iter [04900, 05004], lr: 0.001211, loss: 1.0841
2022-08-25 21:14:16 - train: epoch 0093, iter [05000, 05004], lr: 0.001204, loss: 0.8261
2022-08-25 21:14:17 - train: epoch 093, train_loss: 0.9184
2022-08-25 21:15:32 - eval: epoch: 093, acc1: 76.370%, acc5: 93.064%, test_loss: 0.9503, per_image_load_time: 2.348ms, per_image_inference_time: 0.598ms
2022-08-25 21:15:32 - until epoch: 093, best_acc1: 76.370%
2022-08-25 21:15:32 - epoch 094 lr: 0.001204
2022-08-25 21:16:12 - train: epoch 0094, iter [00100, 05004], lr: 0.001197, loss: 0.9005
2022-08-25 21:16:46 - train: epoch 0094, iter [00200, 05004], lr: 0.001191, loss: 0.8975
2022-08-25 21:17:20 - train: epoch 0094, iter [00300, 05004], lr: 0.001184, loss: 0.9871
2022-08-25 21:17:54 - train: epoch 0094, iter [00400, 05004], lr: 0.001177, loss: 1.0673
2022-08-25 21:18:27 - train: epoch 0094, iter [00500, 05004], lr: 0.001170, loss: 0.8357
2022-08-25 21:19:01 - train: epoch 0094, iter [00600, 05004], lr: 0.001163, loss: 0.8143
2022-08-25 21:19:35 - train: epoch 0094, iter [00700, 05004], lr: 0.001157, loss: 0.9292
2022-08-25 21:20:09 - train: epoch 0094, iter [00800, 05004], lr: 0.001150, loss: 0.7870
2022-08-25 21:20:43 - train: epoch 0094, iter [00900, 05004], lr: 0.001143, loss: 0.8676
2022-08-25 21:21:17 - train: epoch 0094, iter [01000, 05004], lr: 0.001137, loss: 1.0084
2022-08-25 21:21:51 - train: epoch 0094, iter [01100, 05004], lr: 0.001130, loss: 1.0219
2022-08-25 21:22:24 - train: epoch 0094, iter [01200, 05004], lr: 0.001123, loss: 0.9222
2022-08-25 21:22:59 - train: epoch 0094, iter [01300, 05004], lr: 0.001117, loss: 0.9457
2022-08-25 21:23:33 - train: epoch 0094, iter [01400, 05004], lr: 0.001110, loss: 0.8663
2022-08-25 21:24:06 - train: epoch 0094, iter [01500, 05004], lr: 0.001104, loss: 1.0752
2022-08-25 21:24:40 - train: epoch 0094, iter [01600, 05004], lr: 0.001097, loss: 1.2748
2022-08-25 21:25:14 - train: epoch 0094, iter [01700, 05004], lr: 0.001091, loss: 0.8313
2022-08-25 21:25:48 - train: epoch 0094, iter [01800, 05004], lr: 0.001084, loss: 0.9124
2022-08-25 21:26:22 - train: epoch 0094, iter [01900, 05004], lr: 0.001078, loss: 0.9603
2022-08-25 21:26:56 - train: epoch 0094, iter [02000, 05004], lr: 0.001071, loss: 0.7334
2022-08-25 21:27:30 - train: epoch 0094, iter [02100, 05004], lr: 0.001065, loss: 0.9178
2022-08-25 21:28:04 - train: epoch 0094, iter [02200, 05004], lr: 0.001058, loss: 0.6830
2022-08-25 21:28:39 - train: epoch 0094, iter [02300, 05004], lr: 0.001052, loss: 0.6813
2022-08-25 21:29:13 - train: epoch 0094, iter [02400, 05004], lr: 0.001045, loss: 0.8905
2022-08-25 21:29:46 - train: epoch 0094, iter [02500, 05004], lr: 0.001039, loss: 0.8069
2022-08-25 21:30:20 - train: epoch 0094, iter [02600, 05004], lr: 0.001033, loss: 0.8050
2022-08-25 21:30:55 - train: epoch 0094, iter [02700, 05004], lr: 0.001026, loss: 0.8644
2022-08-25 21:31:29 - train: epoch 0094, iter [02800, 05004], lr: 0.001020, loss: 0.9178
2022-08-25 21:32:03 - train: epoch 0094, iter [02900, 05004], lr: 0.001014, loss: 0.9422
2022-08-25 21:32:38 - train: epoch 0094, iter [03000, 05004], lr: 0.001007, loss: 0.8411
2022-08-25 21:33:11 - train: epoch 0094, iter [03100, 05004], lr: 0.001001, loss: 0.9838
2022-08-25 21:33:45 - train: epoch 0094, iter [03200, 05004], lr: 0.000995, loss: 0.8656
2022-08-25 21:34:20 - train: epoch 0094, iter [03300, 05004], lr: 0.000989, loss: 0.8935
2022-08-25 21:34:54 - train: epoch 0094, iter [03400, 05004], lr: 0.000982, loss: 0.9808
2022-08-25 21:35:28 - train: epoch 0094, iter [03500, 05004], lr: 0.000976, loss: 1.1164
2022-08-25 21:36:01 - train: epoch 0094, iter [03600, 05004], lr: 0.000970, loss: 0.9446
2022-08-25 21:36:35 - train: epoch 0094, iter [03700, 05004], lr: 0.000964, loss: 1.0714
2022-08-25 21:37:09 - train: epoch 0094, iter [03800, 05004], lr: 0.000958, loss: 0.8641
2022-08-25 21:37:43 - train: epoch 0094, iter [03900, 05004], lr: 0.000952, loss: 0.8875
2022-08-25 21:38:17 - train: epoch 0094, iter [04000, 05004], lr: 0.000946, loss: 1.0510
2022-08-25 21:38:51 - train: epoch 0094, iter [04100, 05004], lr: 0.000940, loss: 1.1150
2022-08-25 21:39:25 - train: epoch 0094, iter [04200, 05004], lr: 0.000934, loss: 0.6692
2022-08-25 21:39:59 - train: epoch 0094, iter [04300, 05004], lr: 0.000928, loss: 1.0291
2022-08-25 21:40:34 - train: epoch 0094, iter [04400, 05004], lr: 0.000922, loss: 0.9569
2022-08-25 21:41:07 - train: epoch 0094, iter [04500, 05004], lr: 0.000916, loss: 0.9564
2022-08-25 21:41:42 - train: epoch 0094, iter [04600, 05004], lr: 0.000910, loss: 0.9122
2022-08-25 21:42:16 - train: epoch 0094, iter [04700, 05004], lr: 0.000904, loss: 0.8793
2022-08-25 21:42:50 - train: epoch 0094, iter [04800, 05004], lr: 0.000898, loss: 0.8703
2022-08-25 21:43:25 - train: epoch 0094, iter [04900, 05004], lr: 0.000892, loss: 0.8947
2022-08-25 21:43:58 - train: epoch 0094, iter [05000, 05004], lr: 0.000886, loss: 0.8014
2022-08-25 21:43:59 - train: epoch 094, train_loss: 0.9063
2022-08-25 21:45:15 - eval: epoch: 094, acc1: 76.416%, acc5: 93.140%, test_loss: 0.9447, per_image_load_time: 1.733ms, per_image_inference_time: 0.605ms
2022-08-25 21:45:15 - until epoch: 094, best_acc1: 76.416%
2022-08-25 21:45:15 - epoch 095 lr: 0.000886
2022-08-25 21:45:55 - train: epoch 0095, iter [00100, 05004], lr: 0.000880, loss: 0.7526
2022-08-25 21:46:29 - train: epoch 0095, iter [00200, 05004], lr: 0.000874, loss: 0.9794
2022-08-25 21:47:03 - train: epoch 0095, iter [00300, 05004], lr: 0.000868, loss: 0.8192
2022-08-25 21:47:37 - train: epoch 0095, iter [00400, 05004], lr: 0.000862, loss: 0.9546
2022-08-25 21:48:11 - train: epoch 0095, iter [00500, 05004], lr: 0.000856, loss: 0.9888
2022-08-25 21:48:45 - train: epoch 0095, iter [00600, 05004], lr: 0.000851, loss: 0.9287
2022-08-25 21:49:19 - train: epoch 0095, iter [00700, 05004], lr: 0.000845, loss: 0.9209
2022-08-25 21:49:52 - train: epoch 0095, iter [00800, 05004], lr: 0.000839, loss: 1.0520
2022-08-25 21:50:26 - train: epoch 0095, iter [00900, 05004], lr: 0.000833, loss: 1.0555
2022-08-25 21:51:00 - train: epoch 0095, iter [01000, 05004], lr: 0.000828, loss: 0.9530
2022-08-25 21:51:34 - train: epoch 0095, iter [01100, 05004], lr: 0.000822, loss: 0.8066
2022-08-25 21:52:08 - train: epoch 0095, iter [01200, 05004], lr: 0.000816, loss: 0.7447
2022-08-25 21:52:42 - train: epoch 0095, iter [01300, 05004], lr: 0.000811, loss: 0.8284
2022-08-25 21:53:16 - train: epoch 0095, iter [01400, 05004], lr: 0.000805, loss: 0.8949
2022-08-25 21:53:50 - train: epoch 0095, iter [01500, 05004], lr: 0.000800, loss: 0.8971
2022-08-25 21:54:24 - train: epoch 0095, iter [01600, 05004], lr: 0.000794, loss: 0.6036
2022-08-25 21:54:58 - train: epoch 0095, iter [01700, 05004], lr: 0.000788, loss: 1.0132
2022-08-25 21:55:32 - train: epoch 0095, iter [01800, 05004], lr: 0.000783, loss: 0.9418
2022-08-25 21:56:06 - train: epoch 0095, iter [01900, 05004], lr: 0.000777, loss: 0.7861
2022-08-25 21:56:40 - train: epoch 0095, iter [02000, 05004], lr: 0.000772, loss: 0.9349
2022-08-25 21:57:14 - train: epoch 0095, iter [02100, 05004], lr: 0.000766, loss: 0.8913
2022-08-25 21:57:48 - train: epoch 0095, iter [02200, 05004], lr: 0.000761, loss: 0.6864
2022-08-25 21:58:22 - train: epoch 0095, iter [02300, 05004], lr: 0.000755, loss: 0.8297
2022-08-25 21:58:57 - train: epoch 0095, iter [02400, 05004], lr: 0.000750, loss: 1.0285
2022-08-25 21:59:31 - train: epoch 0095, iter [02500, 05004], lr: 0.000745, loss: 0.8212
2022-08-25 22:00:05 - train: epoch 0095, iter [02600, 05004], lr: 0.000739, loss: 0.8687
2022-08-25 22:00:39 - train: epoch 0095, iter [02700, 05004], lr: 0.000734, loss: 0.9681
2022-08-25 22:01:13 - train: epoch 0095, iter [02800, 05004], lr: 0.000729, loss: 0.8497
2022-08-25 22:01:47 - train: epoch 0095, iter [02900, 05004], lr: 0.000723, loss: 0.7928
2022-08-25 22:02:21 - train: epoch 0095, iter [03000, 05004], lr: 0.000718, loss: 1.0582
2022-08-25 22:02:55 - train: epoch 0095, iter [03100, 05004], lr: 0.000713, loss: 1.1082
2022-08-25 22:03:30 - train: epoch 0095, iter [03200, 05004], lr: 0.000707, loss: 0.8555
2022-08-25 22:04:03 - train: epoch 0095, iter [03300, 05004], lr: 0.000702, loss: 0.9463
2022-08-25 22:04:37 - train: epoch 0095, iter [03400, 05004], lr: 0.000697, loss: 0.8897
2022-08-25 22:05:11 - train: epoch 0095, iter [03500, 05004], lr: 0.000692, loss: 0.8407
2022-08-25 22:05:46 - train: epoch 0095, iter [03600, 05004], lr: 0.000686, loss: 0.8845
2022-08-25 22:06:20 - train: epoch 0095, iter [03700, 05004], lr: 0.000681, loss: 0.8987
2022-08-25 22:06:53 - train: epoch 0095, iter [03800, 05004], lr: 0.000676, loss: 0.6578
2022-08-25 22:07:28 - train: epoch 0095, iter [03900, 05004], lr: 0.000671, loss: 0.9649
2022-08-25 22:08:02 - train: epoch 0095, iter [04000, 05004], lr: 0.000666, loss: 0.7795
2022-08-25 22:08:35 - train: epoch 0095, iter [04100, 05004], lr: 0.000661, loss: 0.9777
2022-08-25 22:09:09 - train: epoch 0095, iter [04200, 05004], lr: 0.000656, loss: 0.7511
2022-08-25 22:09:43 - train: epoch 0095, iter [04300, 05004], lr: 0.000651, loss: 0.9118
2022-08-25 22:10:17 - train: epoch 0095, iter [04400, 05004], lr: 0.000646, loss: 0.8954
2022-08-25 22:10:51 - train: epoch 0095, iter [04500, 05004], lr: 0.000641, loss: 0.7540
2022-08-25 22:11:25 - train: epoch 0095, iter [04600, 05004], lr: 0.000636, loss: 0.8782
2022-08-25 22:11:58 - train: epoch 0095, iter [04700, 05004], lr: 0.000631, loss: 0.8814
2022-08-25 22:12:33 - train: epoch 0095, iter [04800, 05004], lr: 0.000626, loss: 0.9841
2022-08-25 22:13:07 - train: epoch 0095, iter [04900, 05004], lr: 0.000621, loss: 0.8511
2022-08-25 22:13:40 - train: epoch 0095, iter [05000, 05004], lr: 0.000616, loss: 0.7493
2022-08-25 22:13:41 - train: epoch 095, train_loss: 0.8945
2022-08-25 22:14:56 - eval: epoch: 095, acc1: 76.588%, acc5: 93.130%, test_loss: 0.9407, per_image_load_time: 2.345ms, per_image_inference_time: 0.584ms
2022-08-25 22:14:57 - until epoch: 095, best_acc1: 76.588%
2022-08-25 22:14:57 - epoch 096 lr: 0.000616
2022-08-25 22:15:36 - train: epoch 0096, iter [00100, 05004], lr: 0.000611, loss: 0.9046
2022-08-25 22:16:10 - train: epoch 0096, iter [00200, 05004], lr: 0.000606, loss: 0.8888
2022-08-25 22:16:43 - train: epoch 0096, iter [00300, 05004], lr: 0.000601, loss: 0.9286
2022-08-25 22:17:17 - train: epoch 0096, iter [00400, 05004], lr: 0.000596, loss: 0.7246
2022-08-25 22:17:51 - train: epoch 0096, iter [00500, 05004], lr: 0.000591, loss: 0.8290
2022-08-25 22:18:25 - train: epoch 0096, iter [00600, 05004], lr: 0.000586, loss: 1.0455
2022-08-25 22:18:58 - train: epoch 0096, iter [00700, 05004], lr: 0.000582, loss: 0.7102
2022-08-25 22:19:32 - train: epoch 0096, iter [00800, 05004], lr: 0.000577, loss: 0.7790
2022-08-25 22:20:06 - train: epoch 0096, iter [00900, 05004], lr: 0.000572, loss: 0.8545
2022-08-25 22:20:40 - train: epoch 0096, iter [01000, 05004], lr: 0.000567, loss: 0.8249
2022-08-25 22:21:14 - train: epoch 0096, iter [01100, 05004], lr: 0.000563, loss: 0.9000
2022-08-25 22:21:48 - train: epoch 0096, iter [01200, 05004], lr: 0.000558, loss: 0.9049
2022-08-25 22:22:22 - train: epoch 0096, iter [01300, 05004], lr: 0.000553, loss: 0.9166
2022-08-25 22:22:55 - train: epoch 0096, iter [01400, 05004], lr: 0.000549, loss: 0.8124
2022-08-25 22:23:29 - train: epoch 0096, iter [01500, 05004], lr: 0.000544, loss: 0.7621
2022-08-25 22:24:03 - train: epoch 0096, iter [01600, 05004], lr: 0.000540, loss: 0.6141
2022-08-25 22:24:37 - train: epoch 0096, iter [01700, 05004], lr: 0.000535, loss: 0.8501
2022-08-25 22:25:10 - train: epoch 0096, iter [01800, 05004], lr: 0.000530, loss: 0.8967
2022-08-25 22:25:44 - train: epoch 0096, iter [01900, 05004], lr: 0.000526, loss: 1.0126
2022-08-25 22:26:18 - train: epoch 0096, iter [02000, 05004], lr: 0.000521, loss: 0.7747
2022-08-25 22:26:52 - train: epoch 0096, iter [02100, 05004], lr: 0.000517, loss: 0.9115
2022-08-25 22:27:25 - train: epoch 0096, iter [02200, 05004], lr: 0.000512, loss: 0.6859
2022-08-25 22:27:59 - train: epoch 0096, iter [02300, 05004], lr: 0.000508, loss: 0.8237
2022-08-25 22:28:32 - train: epoch 0096, iter [02400, 05004], lr: 0.000503, loss: 0.7386
2022-08-25 22:29:07 - train: epoch 0096, iter [02500, 05004], lr: 0.000499, loss: 0.7470
2022-08-25 22:29:41 - train: epoch 0096, iter [02600, 05004], lr: 0.000494, loss: 0.8212
2022-08-25 22:30:15 - train: epoch 0096, iter [02700, 05004], lr: 0.000490, loss: 0.8628
2022-08-25 22:30:49 - train: epoch 0096, iter [02800, 05004], lr: 0.000486, loss: 0.9854
2022-08-25 22:31:23 - train: epoch 0096, iter [02900, 05004], lr: 0.000481, loss: 0.8420
2022-08-25 22:31:57 - train: epoch 0096, iter [03000, 05004], lr: 0.000477, loss: 0.9372
2022-08-25 22:32:30 - train: epoch 0096, iter [03100, 05004], lr: 0.000473, loss: 0.9001
2022-08-25 22:33:04 - train: epoch 0096, iter [03200, 05004], lr: 0.000468, loss: 0.8851
2022-08-25 22:33:38 - train: epoch 0096, iter [03300, 05004], lr: 0.000464, loss: 1.0564
2022-08-25 22:34:11 - train: epoch 0096, iter [03400, 05004], lr: 0.000460, loss: 0.6858
2022-08-25 22:34:45 - train: epoch 0096, iter [03500, 05004], lr: 0.000456, loss: 0.7635
2022-08-25 22:35:19 - train: epoch 0096, iter [03600, 05004], lr: 0.000451, loss: 0.6947
2022-08-25 22:35:53 - train: epoch 0096, iter [03700, 05004], lr: 0.000447, loss: 0.9062
2022-08-25 22:36:27 - train: epoch 0096, iter [03800, 05004], lr: 0.000443, loss: 0.9067
2022-08-25 22:37:01 - train: epoch 0096, iter [03900, 05004], lr: 0.000439, loss: 0.8241
2022-08-25 22:37:35 - train: epoch 0096, iter [04000, 05004], lr: 0.000435, loss: 0.7224
2022-08-25 22:38:08 - train: epoch 0096, iter [04100, 05004], lr: 0.000431, loss: 0.8581
2022-08-25 22:38:43 - train: epoch 0096, iter [04200, 05004], lr: 0.000427, loss: 0.8928
2022-08-25 22:39:16 - train: epoch 0096, iter [04300, 05004], lr: 0.000422, loss: 0.5924
2022-08-25 22:39:50 - train: epoch 0096, iter [04400, 05004], lr: 0.000418, loss: 0.9006
2022-08-25 22:40:23 - train: epoch 0096, iter [04500, 05004], lr: 0.000414, loss: 0.8441
2022-08-25 22:40:57 - train: epoch 0096, iter [04600, 05004], lr: 0.000410, loss: 0.8387
2022-08-25 22:41:31 - train: epoch 0096, iter [04700, 05004], lr: 0.000406, loss: 0.8629
2022-08-25 22:42:04 - train: epoch 0096, iter [04800, 05004], lr: 0.000402, loss: 0.9563
2022-08-25 22:42:38 - train: epoch 0096, iter [04900, 05004], lr: 0.000398, loss: 1.1074
2022-08-25 22:43:10 - train: epoch 0096, iter [05000, 05004], lr: 0.000394, loss: 0.8404
2022-08-25 22:43:12 - train: epoch 096, train_loss: 0.8806
2022-08-25 22:44:27 - eval: epoch: 096, acc1: 76.614%, acc5: 93.232%, test_loss: 0.9385, per_image_load_time: 2.319ms, per_image_inference_time: 0.604ms
2022-08-25 22:44:28 - until epoch: 096, best_acc1: 76.614%
2022-08-25 22:44:28 - epoch 097 lr: 0.000394
2022-08-25 22:45:07 - train: epoch 0097, iter [00100, 05004], lr: 0.000390, loss: 0.9535
2022-08-25 22:45:41 - train: epoch 0097, iter [00200, 05004], lr: 0.000386, loss: 0.7208
2022-08-25 22:46:15 - train: epoch 0097, iter [00300, 05004], lr: 0.000383, loss: 0.9211
2022-08-25 22:46:49 - train: epoch 0097, iter [00400, 05004], lr: 0.000379, loss: 1.1080
2022-08-25 22:47:23 - train: epoch 0097, iter [00500, 05004], lr: 0.000375, loss: 0.8163
2022-08-25 22:47:57 - train: epoch 0097, iter [00600, 05004], lr: 0.000371, loss: 1.0115
2022-08-25 22:48:31 - train: epoch 0097, iter [00700, 05004], lr: 0.000367, loss: 0.8528
2022-08-25 22:49:04 - train: epoch 0097, iter [00800, 05004], lr: 0.000363, loss: 0.7270
2022-08-25 22:49:39 - train: epoch 0097, iter [00900, 05004], lr: 0.000360, loss: 0.8086
2022-08-25 22:50:12 - train: epoch 0097, iter [01000, 05004], lr: 0.000356, loss: 0.8039
2022-08-25 22:50:45 - train: epoch 0097, iter [01100, 05004], lr: 0.000352, loss: 0.7732
2022-08-25 22:51:20 - train: epoch 0097, iter [01200, 05004], lr: 0.000348, loss: 0.8483
2022-08-25 22:51:54 - train: epoch 0097, iter [01300, 05004], lr: 0.000345, loss: 0.7672
2022-08-25 22:52:27 - train: epoch 0097, iter [01400, 05004], lr: 0.000341, loss: 0.8135
2022-08-25 22:53:02 - train: epoch 0097, iter [01500, 05004], lr: 0.000337, loss: 0.8683
2022-08-25 22:53:35 - train: epoch 0097, iter [01600, 05004], lr: 0.000334, loss: 0.7871
2022-08-25 22:54:09 - train: epoch 0097, iter [01700, 05004], lr: 0.000330, loss: 0.8310
2022-08-25 22:54:43 - train: epoch 0097, iter [01800, 05004], lr: 0.000327, loss: 0.9601
2022-08-25 22:55:17 - train: epoch 0097, iter [01900, 05004], lr: 0.000323, loss: 0.9247
2022-08-25 22:55:51 - train: epoch 0097, iter [02000, 05004], lr: 0.000319, loss: 0.8351
2022-08-25 22:56:25 - train: epoch 0097, iter [02100, 05004], lr: 0.000316, loss: 0.9536
2022-08-25 22:56:58 - train: epoch 0097, iter [02200, 05004], lr: 0.000312, loss: 1.0042
2022-08-25 22:57:32 - train: epoch 0097, iter [02300, 05004], lr: 0.000309, loss: 0.8136
2022-08-25 22:58:06 - train: epoch 0097, iter [02400, 05004], lr: 0.000305, loss: 0.8275
2022-08-25 22:58:40 - train: epoch 0097, iter [02500, 05004], lr: 0.000302, loss: 0.9751
2022-08-25 22:59:13 - train: epoch 0097, iter [02600, 05004], lr: 0.000299, loss: 1.0135
2022-08-25 22:59:47 - train: epoch 0097, iter [02700, 05004], lr: 0.000295, loss: 0.8250
2022-08-25 23:00:21 - train: epoch 0097, iter [02800, 05004], lr: 0.000292, loss: 0.8131
2022-08-25 23:00:55 - train: epoch 0097, iter [02900, 05004], lr: 0.000288, loss: 1.0197
2022-08-25 23:01:28 - train: epoch 0097, iter [03000, 05004], lr: 0.000285, loss: 0.9378
2022-08-25 23:02:02 - train: epoch 0097, iter [03100, 05004], lr: 0.000282, loss: 0.8482
2022-08-25 23:02:35 - train: epoch 0097, iter [03200, 05004], lr: 0.000278, loss: 0.9853
2022-08-25 23:03:09 - train: epoch 0097, iter [03300, 05004], lr: 0.000275, loss: 0.9750
2022-08-25 23:03:43 - train: epoch 0097, iter [03400, 05004], lr: 0.000272, loss: 1.0506
2022-08-25 23:04:17 - train: epoch 0097, iter [03500, 05004], lr: 0.000269, loss: 0.9868
2022-08-25 23:04:51 - train: epoch 0097, iter [03600, 05004], lr: 0.000265, loss: 0.7841
2022-08-25 23:05:25 - train: epoch 0097, iter [03700, 05004], lr: 0.000262, loss: 0.7320
2022-08-25 23:05:59 - train: epoch 0097, iter [03800, 05004], lr: 0.000259, loss: 0.7086
2022-08-25 23:06:33 - train: epoch 0097, iter [03900, 05004], lr: 0.000256, loss: 0.8766
2022-08-25 23:07:06 - train: epoch 0097, iter [04000, 05004], lr: 0.000253, loss: 0.8442
2022-08-25 23:07:40 - train: epoch 0097, iter [04100, 05004], lr: 0.000249, loss: 0.8232
2022-08-25 23:08:14 - train: epoch 0097, iter [04200, 05004], lr: 0.000246, loss: 0.8840
2022-08-25 23:08:48 - train: epoch 0097, iter [04300, 05004], lr: 0.000243, loss: 0.7268
2022-08-25 23:09:22 - train: epoch 0097, iter [04400, 05004], lr: 0.000240, loss: 0.8084
2022-08-25 23:09:56 - train: epoch 0097, iter [04500, 05004], lr: 0.000237, loss: 0.9503
2022-08-25 23:10:31 - train: epoch 0097, iter [04600, 05004], lr: 0.000234, loss: 1.0429
2022-08-25 23:11:05 - train: epoch 0097, iter [04700, 05004], lr: 0.000231, loss: 0.8757
2022-08-25 23:11:39 - train: epoch 0097, iter [04800, 05004], lr: 0.000228, loss: 0.9317
2022-08-25 23:12:13 - train: epoch 0097, iter [04900, 05004], lr: 0.000225, loss: 1.1035
2022-08-25 23:12:46 - train: epoch 0097, iter [05000, 05004], lr: 0.000222, loss: 0.9076
2022-08-25 23:12:47 - train: epoch 097, train_loss: 0.8737
2022-08-25 23:14:03 - eval: epoch: 097, acc1: 76.620%, acc5: 93.214%, test_loss: 0.9353, per_image_load_time: 2.257ms, per_image_inference_time: 0.610ms
2022-08-25 23:14:03 - until epoch: 097, best_acc1: 76.620%
2022-08-25 23:14:03 - epoch 098 lr: 0.000222
2022-08-25 23:14:43 - train: epoch 0098, iter [00100, 05004], lr: 0.000219, loss: 0.9205
2022-08-25 23:15:17 - train: epoch 0098, iter [00200, 05004], lr: 0.000216, loss: 0.9408
2022-08-25 23:15:52 - train: epoch 0098, iter [00300, 05004], lr: 0.000213, loss: 1.1522
2022-08-25 23:16:25 - train: epoch 0098, iter [00400, 05004], lr: 0.000210, loss: 0.7639
2022-08-25 23:16:59 - train: epoch 0098, iter [00500, 05004], lr: 0.000207, loss: 0.7941
2022-08-25 23:17:33 - train: epoch 0098, iter [00600, 05004], lr: 0.000205, loss: 0.9315
2022-08-25 23:18:07 - train: epoch 0098, iter [00700, 05004], lr: 0.000202, loss: 0.8109
2022-08-25 23:18:41 - train: epoch 0098, iter [00800, 05004], lr: 0.000199, loss: 0.9442
2022-08-25 23:19:15 - train: epoch 0098, iter [00900, 05004], lr: 0.000196, loss: 0.8902
2022-08-25 23:19:48 - train: epoch 0098, iter [01000, 05004], lr: 0.000193, loss: 0.7980
2022-08-25 23:20:22 - train: epoch 0098, iter [01100, 05004], lr: 0.000191, loss: 0.7684
2022-08-25 23:20:56 - train: epoch 0098, iter [01200, 05004], lr: 0.000188, loss: 0.7742
2022-08-25 23:21:30 - train: epoch 0098, iter [01300, 05004], lr: 0.000185, loss: 0.9407
2022-08-25 23:22:04 - train: epoch 0098, iter [01400, 05004], lr: 0.000182, loss: 0.9267
2022-08-25 23:22:39 - train: epoch 0098, iter [01500, 05004], lr: 0.000180, loss: 0.8210
2022-08-25 23:23:12 - train: epoch 0098, iter [01600, 05004], lr: 0.000177, loss: 0.8503
2022-08-25 23:23:45 - train: epoch 0098, iter [01700, 05004], lr: 0.000175, loss: 0.8839
2022-08-25 23:24:19 - train: epoch 0098, iter [01800, 05004], lr: 0.000172, loss: 0.7788
2022-08-25 23:24:52 - train: epoch 0098, iter [01900, 05004], lr: 0.000169, loss: 0.7615
2022-08-25 23:25:26 - train: epoch 0098, iter [02000, 05004], lr: 0.000167, loss: 0.9898
2022-08-25 23:26:00 - train: epoch 0098, iter [02100, 05004], lr: 0.000164, loss: 0.7341
2022-08-25 23:26:34 - train: epoch 0098, iter [02200, 05004], lr: 0.000162, loss: 0.7826
2022-08-25 23:27:08 - train: epoch 0098, iter [02300, 05004], lr: 0.000159, loss: 0.8344
2022-08-25 23:27:41 - train: epoch 0098, iter [02400, 05004], lr: 0.000157, loss: 0.7969
2022-08-25 23:28:15 - train: epoch 0098, iter [02500, 05004], lr: 0.000154, loss: 0.9868
2022-08-25 23:28:49 - train: epoch 0098, iter [02600, 05004], lr: 0.000152, loss: 0.7846
2022-08-25 23:29:22 - train: epoch 0098, iter [02700, 05004], lr: 0.000149, loss: 0.9305
2022-08-25 23:29:56 - train: epoch 0098, iter [02800, 05004], lr: 0.000147, loss: 0.8283
2022-08-25 23:30:30 - train: epoch 0098, iter [02900, 05004], lr: 0.000144, loss: 0.7633
2022-08-25 23:31:03 - train: epoch 0098, iter [03000, 05004], lr: 0.000142, loss: 0.8618
2022-08-25 23:31:38 - train: epoch 0098, iter [03100, 05004], lr: 0.000140, loss: 0.7395
2022-08-25 23:32:12 - train: epoch 0098, iter [03200, 05004], lr: 0.000137, loss: 0.7458
2022-08-25 23:32:46 - train: epoch 0098, iter [03300, 05004], lr: 0.000135, loss: 0.7443
2022-08-25 23:33:19 - train: epoch 0098, iter [03400, 05004], lr: 0.000133, loss: 0.9369
2022-08-25 23:33:53 - train: epoch 0098, iter [03500, 05004], lr: 0.000131, loss: 0.8902
2022-08-25 23:34:27 - train: epoch 0098, iter [03600, 05004], lr: 0.000128, loss: 1.0919
2022-08-25 23:35:01 - train: epoch 0098, iter [03700, 05004], lr: 0.000126, loss: 0.9326
2022-08-25 23:35:35 - train: epoch 0098, iter [03800, 05004], lr: 0.000124, loss: 0.8572
2022-08-25 23:36:09 - train: epoch 0098, iter [03900, 05004], lr: 0.000122, loss: 0.8158
2022-08-25 23:36:42 - train: epoch 0098, iter [04000, 05004], lr: 0.000119, loss: 0.9528
2022-08-25 23:37:16 - train: epoch 0098, iter [04100, 05004], lr: 0.000117, loss: 0.9195
2022-08-25 23:37:50 - train: epoch 0098, iter [04200, 05004], lr: 0.000115, loss: 0.8295
2022-08-25 23:38:24 - train: epoch 0098, iter [04300, 05004], lr: 0.000113, loss: 0.6996
2022-08-25 23:38:58 - train: epoch 0098, iter [04400, 05004], lr: 0.000111, loss: 0.9516
2022-08-25 23:39:33 - train: epoch 0098, iter [04500, 05004], lr: 0.000109, loss: 0.9974
2022-08-25 23:40:07 - train: epoch 0098, iter [04600, 05004], lr: 0.000107, loss: 0.9936
2022-08-25 23:40:40 - train: epoch 0098, iter [04700, 05004], lr: 0.000105, loss: 0.8724
2022-08-25 23:41:15 - train: epoch 0098, iter [04800, 05004], lr: 0.000103, loss: 0.6344
2022-08-25 23:41:49 - train: epoch 0098, iter [04900, 05004], lr: 0.000101, loss: 0.9231
2022-08-25 23:42:22 - train: epoch 0098, iter [05000, 05004], lr: 0.000099, loss: 0.9593
2022-08-25 23:42:23 - train: epoch 098, train_loss: 0.8669
2022-08-25 23:43:39 - eval: epoch: 098, acc1: 76.734%, acc5: 93.260%, test_loss: 0.9337, per_image_load_time: 2.318ms, per_image_inference_time: 0.584ms
2022-08-25 23:43:39 - until epoch: 098, best_acc1: 76.734%
2022-08-25 23:43:39 - epoch 099 lr: 0.000099
2022-08-25 23:44:19 - train: epoch 0099, iter [00100, 05004], lr: 0.000097, loss: 0.7519
2022-08-25 23:44:53 - train: epoch 0099, iter [00200, 05004], lr: 0.000095, loss: 0.7844
2022-08-25 23:45:27 - train: epoch 0099, iter [00300, 05004], lr: 0.000093, loss: 0.7681
2022-08-25 23:46:01 - train: epoch 0099, iter [00400, 05004], lr: 0.000091, loss: 0.9345
2022-08-25 23:46:36 - train: epoch 0099, iter [00500, 05004], lr: 0.000089, loss: 1.0055
2022-08-25 23:47:09 - train: epoch 0099, iter [00600, 05004], lr: 0.000087, loss: 0.7965
2022-08-25 23:47:43 - train: epoch 0099, iter [00700, 05004], lr: 0.000085, loss: 0.8079
2022-08-25 23:48:17 - train: epoch 0099, iter [00800, 05004], lr: 0.000084, loss: 0.8724
2022-08-25 23:48:51 - train: epoch 0099, iter [00900, 05004], lr: 0.000082, loss: 0.8455
2022-08-25 23:49:25 - train: epoch 0099, iter [01000, 05004], lr: 0.000080, loss: 0.9401
2022-08-25 23:49:59 - train: epoch 0099, iter [01100, 05004], lr: 0.000078, loss: 0.9049
2022-08-25 23:50:34 - train: epoch 0099, iter [01200, 05004], lr: 0.000076, loss: 0.7848
2022-08-25 23:51:07 - train: epoch 0099, iter [01300, 05004], lr: 0.000075, loss: 0.9587
2022-08-25 23:51:40 - train: epoch 0099, iter [01400, 05004], lr: 0.000073, loss: 0.9017
2022-08-25 23:52:15 - train: epoch 0099, iter [01500, 05004], lr: 0.000071, loss: 0.7017
2022-08-25 23:52:49 - train: epoch 0099, iter [01600, 05004], lr: 0.000070, loss: 1.1769
2022-08-25 23:53:22 - train: epoch 0099, iter [01700, 05004], lr: 0.000068, loss: 0.7314
2022-08-25 23:53:56 - train: epoch 0099, iter [01800, 05004], lr: 0.000066, loss: 0.8127
2022-08-25 23:54:31 - train: epoch 0099, iter [01900, 05004], lr: 0.000065, loss: 0.8924
2022-08-25 23:55:05 - train: epoch 0099, iter [02000, 05004], lr: 0.000063, loss: 0.7610
2022-08-25 23:55:39 - train: epoch 0099, iter [02100, 05004], lr: 0.000062, loss: 0.7680
2022-08-25 23:56:14 - train: epoch 0099, iter [02200, 05004], lr: 0.000060, loss: 0.9537
2022-08-25 23:56:48 - train: epoch 0099, iter [02300, 05004], lr: 0.000059, loss: 0.9067
2022-08-25 23:57:22 - train: epoch 0099, iter [02400, 05004], lr: 0.000057, loss: 0.9769
2022-08-25 23:57:56 - train: epoch 0099, iter [02500, 05004], lr: 0.000056, loss: 0.7882
2022-08-25 23:58:30 - train: epoch 0099, iter [02600, 05004], lr: 0.000054, loss: 0.8229
2022-08-25 23:59:04 - train: epoch 0099, iter [02700, 05004], lr: 0.000053, loss: 0.8048
2022-08-25 23:59:38 - train: epoch 0099, iter [02800, 05004], lr: 0.000051, loss: 1.0425
2022-08-26 00:00:12 - train: epoch 0099, iter [02900, 05004], lr: 0.000050, loss: 0.8781
2022-08-26 00:00:46 - train: epoch 0099, iter [03000, 05004], lr: 0.000048, loss: 0.9925
2022-08-26 00:01:20 - train: epoch 0099, iter [03100, 05004], lr: 0.000047, loss: 0.7326
2022-08-26 00:01:54 - train: epoch 0099, iter [03200, 05004], lr: 0.000046, loss: 0.9704
2022-08-26 00:02:28 - train: epoch 0099, iter [03300, 05004], lr: 0.000044, loss: 0.7683
2022-08-26 00:03:02 - train: epoch 0099, iter [03400, 05004], lr: 0.000043, loss: 0.8610
2022-08-26 00:03:36 - train: epoch 0099, iter [03500, 05004], lr: 0.000042, loss: 0.9025
2022-08-26 00:04:10 - train: epoch 0099, iter [03600, 05004], lr: 0.000040, loss: 0.7333
2022-08-26 00:04:44 - train: epoch 0099, iter [03700, 05004], lr: 0.000039, loss: 0.6786
2022-08-26 00:05:18 - train: epoch 0099, iter [03800, 05004], lr: 0.000038, loss: 1.0618
2022-08-26 00:05:52 - train: epoch 0099, iter [03900, 05004], lr: 0.000037, loss: 0.8825
2022-08-26 00:06:27 - train: epoch 0099, iter [04000, 05004], lr: 0.000036, loss: 0.9830
2022-08-26 00:07:01 - train: epoch 0099, iter [04100, 05004], lr: 0.000034, loss: 0.7942
2022-08-26 00:07:35 - train: epoch 0099, iter [04200, 05004], lr: 0.000033, loss: 0.8190
2022-08-26 00:08:09 - train: epoch 0099, iter [04300, 05004], lr: 0.000032, loss: 0.9208
2022-08-26 00:08:42 - train: epoch 0099, iter [04400, 05004], lr: 0.000031, loss: 0.9595
2022-08-26 00:09:17 - train: epoch 0099, iter [04500, 05004], lr: 0.000030, loss: 0.8934
2022-08-26 00:09:51 - train: epoch 0099, iter [04600, 05004], lr: 0.000029, loss: 0.9947
2022-08-26 00:10:25 - train: epoch 0099, iter [04700, 05004], lr: 0.000028, loss: 0.7483
2022-08-26 00:11:00 - train: epoch 0099, iter [04800, 05004], lr: 0.000027, loss: 1.0399
2022-08-26 00:11:34 - train: epoch 0099, iter [04900, 05004], lr: 0.000026, loss: 0.8772
2022-08-26 00:12:07 - train: epoch 0099, iter [05000, 05004], lr: 0.000025, loss: 0.7683
2022-08-26 00:12:08 - train: epoch 099, train_loss: 0.8648
2022-08-26 00:13:24 - eval: epoch: 099, acc1: 76.772%, acc5: 93.246%, test_loss: 0.9346, per_image_load_time: 2.350ms, per_image_inference_time: 0.597ms
2022-08-26 00:13:24 - until epoch: 099, best_acc1: 76.772%
2022-08-26 00:13:24 - epoch 100 lr: 0.000025
2022-08-26 00:14:04 - train: epoch 0100, iter [00100, 05004], lr: 0.000024, loss: 0.8485
2022-08-26 00:14:38 - train: epoch 0100, iter [00200, 05004], lr: 0.000023, loss: 0.9446
2022-08-26 00:15:12 - train: epoch 0100, iter [00300, 05004], lr: 0.000022, loss: 0.7775
2022-08-26 00:15:46 - train: epoch 0100, iter [00400, 05004], lr: 0.000021, loss: 0.6690
2022-08-26 00:16:20 - train: epoch 0100, iter [00500, 05004], lr: 0.000020, loss: 0.9320
2022-08-26 00:16:54 - train: epoch 0100, iter [00600, 05004], lr: 0.000019, loss: 1.1093
2022-08-26 00:17:27 - train: epoch 0100, iter [00700, 05004], lr: 0.000018, loss: 0.7848
2022-08-26 00:18:01 - train: epoch 0100, iter [00800, 05004], lr: 0.000017, loss: 0.9067
2022-08-26 00:18:35 - train: epoch 0100, iter [00900, 05004], lr: 0.000017, loss: 0.7387
2022-08-26 00:19:09 - train: epoch 0100, iter [01000, 05004], lr: 0.000016, loss: 0.8296
2022-08-26 00:19:42 - train: epoch 0100, iter [01100, 05004], lr: 0.000015, loss: 0.8014
2022-08-26 00:20:16 - train: epoch 0100, iter [01200, 05004], lr: 0.000014, loss: 0.8854
2022-08-26 00:20:50 - train: epoch 0100, iter [01300, 05004], lr: 0.000014, loss: 0.9087
2022-08-26 00:21:24 - train: epoch 0100, iter [01400, 05004], lr: 0.000013, loss: 0.8691
2022-08-26 00:21:59 - train: epoch 0100, iter [01500, 05004], lr: 0.000012, loss: 0.8640
2022-08-26 00:22:32 - train: epoch 0100, iter [01600, 05004], lr: 0.000011, loss: 0.7758
2022-08-26 00:23:06 - train: epoch 0100, iter [01700, 05004], lr: 0.000011, loss: 0.8881
2022-08-26 00:23:40 - train: epoch 0100, iter [01800, 05004], lr: 0.000010, loss: 0.8997
2022-08-26 00:24:15 - train: epoch 0100, iter [01900, 05004], lr: 0.000009, loss: 0.7982
2022-08-26 00:24:48 - train: epoch 0100, iter [02000, 05004], lr: 0.000009, loss: 0.8334
2022-08-26 00:25:22 - train: epoch 0100, iter [02100, 05004], lr: 0.000008, loss: 0.7791
2022-08-26 00:25:56 - train: epoch 0100, iter [02200, 05004], lr: 0.000008, loss: 0.8985
2022-08-26 00:26:30 - train: epoch 0100, iter [02300, 05004], lr: 0.000007, loss: 0.7691
2022-08-26 00:27:03 - train: epoch 0100, iter [02400, 05004], lr: 0.000007, loss: 0.8320
2022-08-26 00:27:38 - train: epoch 0100, iter [02500, 05004], lr: 0.000006, loss: 0.8498
2022-08-26 00:28:12 - train: epoch 0100, iter [02600, 05004], lr: 0.000006, loss: 0.9141
2022-08-26 00:28:46 - train: epoch 0100, iter [02700, 05004], lr: 0.000005, loss: 0.7714
2022-08-26 00:29:20 - train: epoch 0100, iter [02800, 05004], lr: 0.000005, loss: 0.8406
2022-08-26 00:29:54 - train: epoch 0100, iter [02900, 05004], lr: 0.000004, loss: 0.8900
2022-08-26 00:30:28 - train: epoch 0100, iter [03000, 05004], lr: 0.000004, loss: 0.7505
2022-08-26 00:31:02 - train: epoch 0100, iter [03100, 05004], lr: 0.000004, loss: 0.8772
2022-08-26 00:31:36 - train: epoch 0100, iter [03200, 05004], lr: 0.000003, loss: 0.8886
2022-08-26 00:32:10 - train: epoch 0100, iter [03300, 05004], lr: 0.000003, loss: 0.9250
2022-08-26 00:32:44 - train: epoch 0100, iter [03400, 05004], lr: 0.000003, loss: 1.0116
2022-08-26 00:33:18 - train: epoch 0100, iter [03500, 05004], lr: 0.000002, loss: 0.6936
2022-08-26 00:33:53 - train: epoch 0100, iter [03600, 05004], lr: 0.000002, loss: 0.9110
2022-08-26 00:34:27 - train: epoch 0100, iter [03700, 05004], lr: 0.000002, loss: 0.8470
2022-08-26 00:35:01 - train: epoch 0100, iter [03800, 05004], lr: 0.000001, loss: 0.9015
2022-08-26 00:35:35 - train: epoch 0100, iter [03900, 05004], lr: 0.000001, loss: 0.9101
2022-08-26 00:36:09 - train: epoch 0100, iter [04000, 05004], lr: 0.000001, loss: 0.7606
2022-08-26 00:36:44 - train: epoch 0100, iter [04100, 05004], lr: 0.000001, loss: 0.9603
2022-08-26 00:37:17 - train: epoch 0100, iter [04200, 05004], lr: 0.000001, loss: 0.8007
2022-08-26 00:37:51 - train: epoch 0100, iter [04300, 05004], lr: 0.000000, loss: 0.8352
2022-08-26 00:38:26 - train: epoch 0100, iter [04400, 05004], lr: 0.000000, loss: 0.9523
2022-08-26 00:39:00 - train: epoch 0100, iter [04500, 05004], lr: 0.000000, loss: 0.7853
2022-08-26 00:39:34 - train: epoch 0100, iter [04600, 05004], lr: 0.000000, loss: 0.7711
2022-08-26 00:40:07 - train: epoch 0100, iter [04700, 05004], lr: 0.000000, loss: 0.9074
2022-08-26 00:40:41 - train: epoch 0100, iter [04800, 05004], lr: 0.000000, loss: 0.6457
2022-08-26 00:41:15 - train: epoch 0100, iter [04900, 05004], lr: 0.000000, loss: 0.7452
2022-08-26 00:41:48 - train: epoch 0100, iter [05000, 05004], lr: 0.000000, loss: 0.9426
2022-08-26 00:41:49 - train: epoch 100, train_loss: 0.8632
2022-08-26 00:43:05 - eval: epoch: 100, acc1: 76.870%, acc5: 93.256%, test_loss: 0.9344, per_image_load_time: 2.379ms, per_image_inference_time: 0.593ms
2022-08-26 00:43:06 - until epoch: 100, best_acc1: 76.870%
2022-08-26 00:43:06 - train done. model: resnet_4.1g, train time: 49.615 hours, best_acc1: 76.870%
