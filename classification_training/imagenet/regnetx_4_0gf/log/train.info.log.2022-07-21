2022-07-21 23:07:43 - train: epoch 0075, iter [01000, 05004], lr: 0.034242, loss: 0.9149
2022-07-21 23:10:01 - train: epoch 0075, iter [01100, 05004], lr: 0.034192, loss: 1.0891
2022-07-21 23:12:17 - train: epoch 0075, iter [01200, 05004], lr: 0.034143, loss: 0.8066
2022-07-21 23:14:34 - train: epoch 0075, iter [01300, 05004], lr: 0.034093, loss: 0.9736
2022-07-21 23:16:51 - train: epoch 0075, iter [01400, 05004], lr: 0.034043, loss: 1.1258
2022-07-21 23:19:07 - train: epoch 0075, iter [01500, 05004], lr: 0.033994, loss: 1.0618
2022-07-21 23:21:24 - train: epoch 0075, iter [01600, 05004], lr: 0.033944, loss: 0.8643
2022-07-21 23:23:41 - train: epoch 0075, iter [01700, 05004], lr: 0.033894, loss: 1.0349
2022-07-21 23:25:57 - train: epoch 0075, iter [01800, 05004], lr: 0.033845, loss: 1.1770
2022-07-21 23:28:14 - train: epoch 0075, iter [01900, 05004], lr: 0.033795, loss: 1.0467
2022-07-21 23:30:31 - train: epoch 0075, iter [02000, 05004], lr: 0.033746, loss: 0.9922
2022-07-21 23:32:47 - train: epoch 0075, iter [02100, 05004], lr: 0.033696, loss: 1.0909
2022-07-21 23:35:04 - train: epoch 0075, iter [02200, 05004], lr: 0.033647, loss: 1.0973
2022-07-21 23:37:21 - train: epoch 0075, iter [02300, 05004], lr: 0.033597, loss: 1.1210
2022-07-21 23:39:37 - train: epoch 0075, iter [02400, 05004], lr: 0.033548, loss: 1.3104
2022-07-21 23:41:54 - train: epoch 0075, iter [02500, 05004], lr: 0.033499, loss: 1.1517
2022-07-21 23:44:10 - train: epoch 0075, iter [02600, 05004], lr: 0.033449, loss: 1.1257
2022-07-21 23:46:27 - train: epoch 0075, iter [02700, 05004], lr: 0.033400, loss: 1.0033
2022-07-21 23:48:43 - train: epoch 0075, iter [02800, 05004], lr: 0.033351, loss: 0.9701
2022-07-21 23:51:00 - train: epoch 0075, iter [02900, 05004], lr: 0.033301, loss: 1.1287
2022-07-21 23:53:17 - train: epoch 0075, iter [03000, 05004], lr: 0.033252, loss: 1.3265
2022-07-21 23:55:34 - train: epoch 0075, iter [03100, 05004], lr: 0.033203, loss: 1.2024
2022-07-21 23:57:51 - train: epoch 0075, iter [03200, 05004], lr: 0.033154, loss: 1.0830
2022-07-22 00:00:08 - train: epoch 0075, iter [03300, 05004], lr: 0.033105, loss: 1.3060
2022-07-22 00:02:25 - train: epoch 0075, iter [03400, 05004], lr: 0.033056, loss: 0.9801
2022-07-22 00:04:42 - train: epoch 0075, iter [03500, 05004], lr: 0.033006, loss: 1.0510
2022-07-22 00:06:58 - train: epoch 0075, iter [03600, 05004], lr: 0.032957, loss: 1.1464
2022-07-22 00:09:15 - train: epoch 0075, iter [03700, 05004], lr: 0.032908, loss: 1.2010
2022-07-22 00:11:32 - train: epoch 0075, iter [03800, 05004], lr: 0.032859, loss: 1.1326
2022-07-22 00:13:48 - train: epoch 0075, iter [03900, 05004], lr: 0.032810, loss: 1.0529
2022-07-22 00:16:05 - train: epoch 0075, iter [04000, 05004], lr: 0.032761, loss: 1.1244
2022-07-22 00:18:22 - train: epoch 0075, iter [04100, 05004], lr: 0.032713, loss: 1.0434
2022-07-22 00:20:39 - train: epoch 0075, iter [04200, 05004], lr: 0.032664, loss: 1.1932
2022-07-22 00:22:55 - train: epoch 0075, iter [04300, 05004], lr: 0.032615, loss: 1.2730
2022-07-22 00:25:12 - train: epoch 0075, iter [04400, 05004], lr: 0.032566, loss: 1.1509
2022-07-22 00:27:29 - train: epoch 0075, iter [04500, 05004], lr: 0.032517, loss: 1.1989
2022-07-22 00:29:46 - train: epoch 0075, iter [04600, 05004], lr: 0.032469, loss: 0.9690
2022-07-22 00:32:03 - train: epoch 0075, iter [04700, 05004], lr: 0.032420, loss: 1.1241
2022-07-22 00:34:19 - train: epoch 0075, iter [04800, 05004], lr: 0.032371, loss: 1.0906
2022-07-22 00:36:36 - train: epoch 0075, iter [04900, 05004], lr: 0.032322, loss: 1.1407
2022-07-22 00:38:53 - train: epoch 0075, iter [05000, 05004], lr: 0.032274, loss: 1.0639
2022-07-22 00:39:00 - train: epoch 075, train_loss: 1.0889
2022-07-22 00:41:06 - eval: epoch: 075, acc1: 73.600%, acc5: 91.786%, test_loss: 1.0751, per_image_load_time: 1.047ms, per_image_inference_time: 3.905ms
2022-07-22 00:41:07 - until epoch: 075, best_acc1: 73.600%
2022-07-22 00:41:07 - epoch 076 lr: 0.032271
2022-07-22 00:43:31 - train: epoch 0076, iter [00100, 05004], lr: 0.032223, loss: 0.9511
2022-07-22 00:45:48 - train: epoch 0076, iter [00200, 05004], lr: 0.032175, loss: 1.0378
2022-07-22 00:48:05 - train: epoch 0076, iter [00300, 05004], lr: 0.032126, loss: 1.1345
2022-07-22 00:50:21 - train: epoch 0076, iter [00400, 05004], lr: 0.032078, loss: 1.0797
2022-07-22 00:52:38 - train: epoch 0076, iter [00500, 05004], lr: 0.032029, loss: 1.0856
2022-07-22 00:54:55 - train: epoch 0076, iter [00600, 05004], lr: 0.031981, loss: 0.9487
2022-07-22 00:57:12 - train: epoch 0076, iter [00700, 05004], lr: 0.031932, loss: 0.9330
2022-07-22 00:59:28 - train: epoch 0076, iter [00800, 05004], lr: 0.031884, loss: 1.0192
2022-07-22 01:01:45 - train: epoch 0076, iter [00900, 05004], lr: 0.031835, loss: 0.8889
2022-07-22 01:04:02 - train: epoch 0076, iter [01000, 05004], lr: 0.031787, loss: 1.0089
2022-07-22 01:06:19 - train: epoch 0076, iter [01100, 05004], lr: 0.031739, loss: 1.0985
2022-07-22 01:08:35 - train: epoch 0076, iter [01200, 05004], lr: 0.031691, loss: 1.0296
2022-07-22 01:10:52 - train: epoch 0076, iter [01300, 05004], lr: 0.031642, loss: 1.1325
2022-07-22 01:13:09 - train: epoch 0076, iter [01400, 05004], lr: 0.031594, loss: 1.2159
2022-07-22 01:15:26 - train: epoch 0076, iter [01500, 05004], lr: 0.031546, loss: 1.1885
2022-07-22 01:17:43 - train: epoch 0076, iter [01600, 05004], lr: 0.031498, loss: 1.0490
2022-07-22 01:20:00 - train: epoch 0076, iter [01700, 05004], lr: 0.031450, loss: 1.0500
2022-07-22 01:22:17 - train: epoch 0076, iter [01800, 05004], lr: 0.031401, loss: 0.8670
2022-07-22 01:24:34 - train: epoch 0076, iter [01900, 05004], lr: 0.031353, loss: 1.1565
2022-07-22 01:26:51 - train: epoch 0076, iter [02000, 05004], lr: 0.031305, loss: 0.9990
2022-07-22 01:29:08 - train: epoch 0076, iter [02100, 05004], lr: 0.031257, loss: 1.1228
2022-07-22 01:31:25 - train: epoch 0076, iter [02200, 05004], lr: 0.031209, loss: 0.9915
2022-07-22 01:33:42 - train: epoch 0076, iter [02300, 05004], lr: 0.031161, loss: 0.8912
2022-07-22 01:35:58 - train: epoch 0076, iter [02400, 05004], lr: 0.031114, loss: 1.2010
2022-07-22 01:38:15 - train: epoch 0076, iter [02500, 05004], lr: 0.031066, loss: 0.9984
2022-07-22 01:40:32 - train: epoch 0076, iter [02600, 05004], lr: 0.031018, loss: 1.1756
2022-07-22 01:42:49 - train: epoch 0076, iter [02700, 05004], lr: 0.030970, loss: 0.9631
2022-07-22 01:45:06 - train: epoch 0076, iter [02800, 05004], lr: 0.030922, loss: 1.0687
2022-07-22 01:47:23 - train: epoch 0076, iter [02900, 05004], lr: 0.030874, loss: 1.1909
2022-07-22 01:49:40 - train: epoch 0076, iter [03000, 05004], lr: 0.030827, loss: 0.9161
2022-07-22 01:51:57 - train: epoch 0076, iter [03100, 05004], lr: 0.030779, loss: 1.1504
2022-07-22 01:54:14 - train: epoch 0076, iter [03200, 05004], lr: 0.030731, loss: 1.1088
2022-07-22 01:56:31 - train: epoch 0076, iter [03300, 05004], lr: 0.030684, loss: 1.0640
2022-07-22 01:58:48 - train: epoch 0076, iter [03400, 05004], lr: 0.030636, loss: 1.2527
2022-07-22 02:01:06 - train: epoch 0076, iter [03500, 05004], lr: 0.030588, loss: 1.0246
2022-07-22 02:03:23 - train: epoch 0076, iter [03600, 05004], lr: 0.030541, loss: 1.0054
2022-07-22 02:05:40 - train: epoch 0076, iter [03700, 05004], lr: 0.030493, loss: 1.0740
2022-07-22 02:07:57 - train: epoch 0076, iter [03800, 05004], lr: 0.030446, loss: 0.8735
2022-07-22 02:10:14 - train: epoch 0076, iter [03900, 05004], lr: 0.030398, loss: 1.0394
2022-07-22 02:12:31 - train: epoch 0076, iter [04000, 05004], lr: 0.030351, loss: 1.1504
2022-07-22 02:14:48 - train: epoch 0076, iter [04100, 05004], lr: 0.030303, loss: 1.0552
2022-07-22 02:17:05 - train: epoch 0076, iter [04200, 05004], lr: 0.030256, loss: 1.1158
2022-07-22 02:19:22 - train: epoch 0076, iter [04300, 05004], lr: 0.030209, loss: 1.1703
2022-07-22 02:21:39 - train: epoch 0076, iter [04400, 05004], lr: 0.030161, loss: 1.0468
2022-07-22 02:23:56 - train: epoch 0076, iter [04500, 05004], lr: 0.030114, loss: 1.1037
2022-07-22 02:26:13 - train: epoch 0076, iter [04600, 05004], lr: 0.030067, loss: 1.0288
2022-07-22 02:28:30 - train: epoch 0076, iter [04700, 05004], lr: 0.030020, loss: 1.2508
2022-07-22 02:30:47 - train: epoch 0076, iter [04800, 05004], lr: 0.029972, loss: 0.9888
2022-07-22 02:33:04 - train: epoch 0076, iter [04900, 05004], lr: 0.029925, loss: 1.0764
2022-07-22 02:35:20 - train: epoch 0076, iter [05000, 05004], lr: 0.029878, loss: 1.4691
2022-07-22 02:35:27 - train: epoch 076, train_loss: 1.0657
2022-07-22 02:37:40 - eval: epoch: 076, acc1: 74.110%, acc5: 92.072%, test_loss: 1.0490, per_image_load_time: 1.263ms, per_image_inference_time: 3.926ms
2022-07-22 02:37:41 - until epoch: 076, best_acc1: 74.110%
2022-07-22 02:37:41 - epoch 077 lr: 0.029876
2022-07-22 02:40:07 - train: epoch 0077, iter [00100, 05004], lr: 0.029829, loss: 1.2957
2022-07-22 02:42:24 - train: epoch 0077, iter [00200, 05004], lr: 0.029782, loss: 1.0570
2022-07-22 02:44:41 - train: epoch 0077, iter [00300, 05004], lr: 0.029735, loss: 0.8807
2022-07-22 02:46:58 - train: epoch 0077, iter [00400, 05004], lr: 0.029688, loss: 1.0278
2022-07-22 02:49:14 - train: epoch 0077, iter [00500, 05004], lr: 0.029641, loss: 0.8366
2022-07-22 02:51:31 - train: epoch 0077, iter [00600, 05004], lr: 0.029594, loss: 0.9665
2022-07-22 02:53:48 - train: epoch 0077, iter [00700, 05004], lr: 0.029547, loss: 1.2221
2022-07-22 02:56:05 - train: epoch 0077, iter [00800, 05004], lr: 0.029500, loss: 1.0422
2022-07-22 02:58:21 - train: epoch 0077, iter [00900, 05004], lr: 0.029454, loss: 1.0862
2022-07-22 03:00:38 - train: epoch 0077, iter [01000, 05004], lr: 0.029407, loss: 1.0296
2022-07-22 03:02:55 - train: epoch 0077, iter [01100, 05004], lr: 0.029360, loss: 1.0273
2022-07-22 03:05:12 - train: epoch 0077, iter [01200, 05004], lr: 0.029313, loss: 1.1717
2022-07-22 03:07:29 - train: epoch 0077, iter [01300, 05004], lr: 0.029266, loss: 0.8591
2022-07-22 03:09:45 - train: epoch 0077, iter [01400, 05004], lr: 0.029220, loss: 1.0180
2022-07-22 03:12:02 - train: epoch 0077, iter [01500, 05004], lr: 0.029173, loss: 0.7816
2022-07-22 03:14:19 - train: epoch 0077, iter [01600, 05004], lr: 0.029126, loss: 0.8901
2022-07-22 03:16:36 - train: epoch 0077, iter [01700, 05004], lr: 0.029080, loss: 1.0633
2022-07-22 03:18:53 - train: epoch 0077, iter [01800, 05004], lr: 0.029033, loss: 0.8379
2022-07-22 03:21:09 - train: epoch 0077, iter [01900, 05004], lr: 0.028987, loss: 1.0400
2022-07-22 03:23:26 - train: epoch 0077, iter [02000, 05004], lr: 0.028940, loss: 1.1364
2022-07-22 03:25:43 - train: epoch 0077, iter [02100, 05004], lr: 0.028894, loss: 1.2326
2022-07-22 03:28:00 - train: epoch 0077, iter [02200, 05004], lr: 0.028847, loss: 0.9578
2022-07-22 03:30:16 - train: epoch 0077, iter [02300, 05004], lr: 0.028801, loss: 1.1033
2022-07-22 03:32:33 - train: epoch 0077, iter [02400, 05004], lr: 0.028754, loss: 0.9823
2022-07-22 03:34:50 - train: epoch 0077, iter [02500, 05004], lr: 0.028708, loss: 1.1609
2022-07-22 03:37:06 - train: epoch 0077, iter [02600, 05004], lr: 0.028662, loss: 0.7207
2022-07-22 03:39:23 - train: epoch 0077, iter [02700, 05004], lr: 0.028615, loss: 0.9992
2022-07-22 03:41:40 - train: epoch 0077, iter [02800, 05004], lr: 0.028569, loss: 1.0379
2022-07-22 03:43:57 - train: epoch 0077, iter [02900, 05004], lr: 0.028523, loss: 1.1233
2022-07-22 03:46:14 - train: epoch 0077, iter [03000, 05004], lr: 0.028477, loss: 0.9938
2022-07-22 03:48:30 - train: epoch 0077, iter [03100, 05004], lr: 0.028431, loss: 1.0430
2022-07-22 03:50:47 - train: epoch 0077, iter [03200, 05004], lr: 0.028384, loss: 1.1354
2022-07-22 03:53:04 - train: epoch 0077, iter [03300, 05004], lr: 0.028338, loss: 1.1863
2022-07-22 03:55:21 - train: epoch 0077, iter [03400, 05004], lr: 0.028292, loss: 1.1676
2022-07-22 03:57:38 - train: epoch 0077, iter [03500, 05004], lr: 0.028246, loss: 0.9658
2022-07-22 03:59:55 - train: epoch 0077, iter [03600, 05004], lr: 0.028200, loss: 0.9869
2022-07-22 04:02:11 - train: epoch 0077, iter [03700, 05004], lr: 0.028154, loss: 0.9368
2022-07-22 04:04:28 - train: epoch 0077, iter [03800, 05004], lr: 0.028108, loss: 0.9051
2022-07-22 04:06:45 - train: epoch 0077, iter [03900, 05004], lr: 0.028062, loss: 1.0959
2022-07-22 04:09:02 - train: epoch 0077, iter [04000, 05004], lr: 0.028016, loss: 1.1349
2022-07-22 04:11:19 - train: epoch 0077, iter [04100, 05004], lr: 0.027971, loss: 1.0963
2022-07-22 04:13:35 - train: epoch 0077, iter [04200, 05004], lr: 0.027925, loss: 1.1358
2022-07-22 04:15:52 - train: epoch 0077, iter [04300, 05004], lr: 0.027879, loss: 1.1316
2022-07-22 04:18:09 - train: epoch 0077, iter [04400, 05004], lr: 0.027833, loss: 0.9146
2022-07-22 04:20:26 - train: epoch 0077, iter [04500, 05004], lr: 0.027787, loss: 1.2235
2022-07-22 04:22:43 - train: epoch 0077, iter [04600, 05004], lr: 0.027742, loss: 0.9096
2022-07-22 04:25:00 - train: epoch 0077, iter [04700, 05004], lr: 0.027696, loss: 1.1061
2022-07-22 04:27:17 - train: epoch 0077, iter [04800, 05004], lr: 0.027650, loss: 1.0099
2022-07-22 04:29:34 - train: epoch 0077, iter [04900, 05004], lr: 0.027605, loss: 1.0990
2022-07-22 04:31:50 - train: epoch 0077, iter [05000, 05004], lr: 0.027559, loss: 0.9603
2022-07-22 04:31:57 - train: epoch 077, train_loss: 1.0403
2022-07-22 04:34:11 - eval: epoch: 077, acc1: 74.346%, acc5: 92.214%, test_loss: 1.0394, per_image_load_time: 1.287ms, per_image_inference_time: 3.938ms
2022-07-22 04:34:12 - until epoch: 077, best_acc1: 74.346%
2022-07-22 04:34:12 - epoch 078 lr: 0.027557
2022-07-22 04:36:38 - train: epoch 0078, iter [00100, 05004], lr: 0.027512, loss: 1.1255
2022-07-22 04:38:54 - train: epoch 0078, iter [00200, 05004], lr: 0.027466, loss: 1.1946
2022-07-22 04:41:11 - train: epoch 0078, iter [00300, 05004], lr: 0.027421, loss: 1.0330
2022-07-22 04:43:28 - train: epoch 0078, iter [00400, 05004], lr: 0.027376, loss: 1.1280
2022-07-22 04:45:44 - train: epoch 0078, iter [00500, 05004], lr: 0.027330, loss: 1.0145
2022-07-22 04:48:01 - train: epoch 0078, iter [00600, 05004], lr: 0.027285, loss: 0.9999
2022-07-22 04:50:17 - train: epoch 0078, iter [00700, 05004], lr: 0.027239, loss: 0.9619
2022-07-22 04:52:34 - train: epoch 0078, iter [00800, 05004], lr: 0.027194, loss: 1.0369
2022-07-22 04:54:51 - train: epoch 0078, iter [00900, 05004], lr: 0.027149, loss: 0.9164
2022-07-22 04:57:07 - train: epoch 0078, iter [01000, 05004], lr: 0.027103, loss: 0.8670
2022-07-22 04:59:24 - train: epoch 0078, iter [01100, 05004], lr: 0.027058, loss: 1.0866
2022-07-22 05:01:41 - train: epoch 0078, iter [01200, 05004], lr: 0.027013, loss: 0.8502
2022-07-22 05:03:57 - train: epoch 0078, iter [01300, 05004], lr: 0.026968, loss: 0.9182
2022-07-22 05:06:14 - train: epoch 0078, iter [01400, 05004], lr: 0.026923, loss: 0.8299
2022-07-22 05:08:31 - train: epoch 0078, iter [01500, 05004], lr: 0.026878, loss: 0.9457
2022-07-22 05:10:47 - train: epoch 0078, iter [01600, 05004], lr: 0.026833, loss: 1.2192
2022-07-22 05:13:04 - train: epoch 0078, iter [01700, 05004], lr: 0.026788, loss: 0.9086
2022-07-22 05:15:21 - train: epoch 0078, iter [01800, 05004], lr: 0.026743, loss: 1.0977
2022-07-22 05:17:37 - train: epoch 0078, iter [01900, 05004], lr: 0.026698, loss: 1.0264
2022-07-22 05:19:54 - train: epoch 0078, iter [02000, 05004], lr: 0.026653, loss: 1.0504
2022-07-22 05:22:11 - train: epoch 0078, iter [02100, 05004], lr: 0.026608, loss: 1.2288
2022-07-22 05:24:28 - train: epoch 0078, iter [02200, 05004], lr: 0.026563, loss: 0.9949
2022-07-22 05:26:44 - train: epoch 0078, iter [02300, 05004], lr: 0.026518, loss: 0.7566
2022-07-22 05:29:01 - train: epoch 0078, iter [02400, 05004], lr: 0.026473, loss: 0.9654
2022-07-22 05:31:18 - train: epoch 0078, iter [02500, 05004], lr: 0.026429, loss: 1.0389
2022-07-22 05:33:34 - train: epoch 0078, iter [02600, 05004], lr: 0.026384, loss: 1.0040
2022-07-22 05:35:51 - train: epoch 0078, iter [02700, 05004], lr: 0.026339, loss: 0.9323
2022-07-22 05:38:08 - train: epoch 0078, iter [02800, 05004], lr: 0.026294, loss: 1.0700
2022-07-22 05:40:24 - train: epoch 0078, iter [02900, 05004], lr: 0.026250, loss: 0.9464
2022-07-22 05:42:41 - train: epoch 0078, iter [03000, 05004], lr: 0.026205, loss: 0.9218
2022-07-22 05:44:58 - train: epoch 0078, iter [03100, 05004], lr: 0.026161, loss: 1.0480
2022-07-22 05:47:14 - train: epoch 0078, iter [03200, 05004], lr: 0.026116, loss: 1.1579
2022-07-22 05:49:31 - train: epoch 0078, iter [03300, 05004], lr: 0.026071, loss: 1.0115
2022-07-22 05:51:48 - train: epoch 0078, iter [03400, 05004], lr: 0.026027, loss: 0.9992
2022-07-22 05:54:04 - train: epoch 0078, iter [03500, 05004], lr: 0.025983, loss: 1.0732
2022-07-22 05:56:21 - train: epoch 0078, iter [03600, 05004], lr: 0.025938, loss: 1.2160
2022-07-22 05:58:38 - train: epoch 0078, iter [03700, 05004], lr: 0.025894, loss: 0.9883
2022-07-22 06:00:55 - train: epoch 0078, iter [03800, 05004], lr: 0.025849, loss: 1.0254
2022-07-22 06:03:11 - train: epoch 0078, iter [03900, 05004], lr: 0.025805, loss: 1.2307
2022-07-22 06:05:28 - train: epoch 0078, iter [04000, 05004], lr: 0.025761, loss: 1.0261
2022-07-22 06:07:45 - train: epoch 0078, iter [04100, 05004], lr: 0.025716, loss: 1.0106
2022-07-22 06:10:01 - train: epoch 0078, iter [04200, 05004], lr: 0.025672, loss: 1.0313
2022-07-22 06:12:18 - train: epoch 0078, iter [04300, 05004], lr: 0.025628, loss: 1.0456
2022-07-22 06:14:35 - train: epoch 0078, iter [04400, 05004], lr: 0.025584, loss: 0.9576
2022-07-22 06:16:51 - train: epoch 0078, iter [04500, 05004], lr: 0.025540, loss: 1.1547
2022-07-22 06:19:08 - train: epoch 0078, iter [04600, 05004], lr: 0.025496, loss: 1.1106
2022-07-22 06:21:25 - train: epoch 0078, iter [04700, 05004], lr: 0.025452, loss: 0.9476
2022-07-22 06:23:42 - train: epoch 0078, iter [04800, 05004], lr: 0.025408, loss: 1.1835
2022-07-22 06:25:58 - train: epoch 0078, iter [04900, 05004], lr: 0.025364, loss: 1.0090
2022-07-22 06:28:15 - train: epoch 0078, iter [05000, 05004], lr: 0.025320, loss: 1.3140
2022-07-22 06:28:21 - train: epoch 078, train_loss: 1.0159
2022-07-22 06:30:33 - eval: epoch: 078, acc1: 74.532%, acc5: 92.370%, test_loss: 1.0331, per_image_load_time: 1.158ms, per_image_inference_time: 3.920ms
2022-07-22 06:30:33 - until epoch: 078, best_acc1: 74.532%
2022-07-22 06:30:33 - epoch 079 lr: 0.025317
2022-07-22 06:32:58 - train: epoch 0079, iter [00100, 05004], lr: 0.025274, loss: 1.0804
2022-07-22 06:35:14 - train: epoch 0079, iter [00200, 05004], lr: 0.025230, loss: 0.9292
2022-07-22 06:37:31 - train: epoch 0079, iter [00300, 05004], lr: 0.025186, loss: 0.9317
2022-07-22 06:39:48 - train: epoch 0079, iter [00400, 05004], lr: 0.025142, loss: 0.9067
2022-07-22 06:42:04 - train: epoch 0079, iter [00500, 05004], lr: 0.025099, loss: 0.8964
2022-07-22 06:44:21 - train: epoch 0079, iter [00600, 05004], lr: 0.025055, loss: 0.8672
2022-07-22 06:46:38 - train: epoch 0079, iter [00700, 05004], lr: 0.025011, loss: 1.0229
2022-07-22 06:48:54 - train: epoch 0079, iter [00800, 05004], lr: 0.024967, loss: 1.0413
2022-07-22 06:51:11 - train: epoch 0079, iter [00900, 05004], lr: 0.024924, loss: 0.9381
2022-07-22 06:53:27 - train: epoch 0079, iter [01000, 05004], lr: 0.024880, loss: 1.0770
2022-07-22 06:55:44 - train: epoch 0079, iter [01100, 05004], lr: 0.024836, loss: 1.1079
2022-07-22 06:58:01 - train: epoch 0079, iter [01200, 05004], lr: 0.024793, loss: 1.0594
2022-07-22 07:00:17 - train: epoch 0079, iter [01300, 05004], lr: 0.024749, loss: 0.9291
2022-07-22 07:02:34 - train: epoch 0079, iter [01400, 05004], lr: 0.024706, loss: 0.8162
2022-07-22 07:04:51 - train: epoch 0079, iter [01500, 05004], lr: 0.024662, loss: 1.0376
2022-07-22 07:07:07 - train: epoch 0079, iter [01600, 05004], lr: 0.024619, loss: 0.8174
2022-07-22 07:09:24 - train: epoch 0079, iter [01700, 05004], lr: 0.024575, loss: 1.0730
2022-07-22 07:11:40 - train: epoch 0079, iter [01800, 05004], lr: 0.024532, loss: 1.0322
2022-07-22 07:13:57 - train: epoch 0079, iter [01900, 05004], lr: 0.024489, loss: 1.0178
2022-07-22 07:16:13 - train: epoch 0079, iter [02000, 05004], lr: 0.024445, loss: 1.0341
2022-07-22 07:18:30 - train: epoch 0079, iter [02100, 05004], lr: 0.024402, loss: 0.8169
2022-07-22 07:20:47 - train: epoch 0079, iter [02200, 05004], lr: 0.024359, loss: 0.9295
2022-07-22 07:23:03 - train: epoch 0079, iter [02300, 05004], lr: 0.024316, loss: 0.8827
2022-07-22 07:25:20 - train: epoch 0079, iter [02400, 05004], lr: 0.024273, loss: 1.0285
2022-07-22 07:27:36 - train: epoch 0079, iter [02500, 05004], lr: 0.024229, loss: 1.0313
2022-07-22 07:29:53 - train: epoch 0079, iter [02600, 05004], lr: 0.024186, loss: 0.9122
2022-07-22 07:32:10 - train: epoch 0079, iter [02700, 05004], lr: 0.024143, loss: 0.8606
2022-07-22 07:34:26 - train: epoch 0079, iter [02800, 05004], lr: 0.024100, loss: 1.0455
2022-07-22 07:36:43 - train: epoch 0079, iter [02900, 05004], lr: 0.024057, loss: 1.0040
2022-07-22 07:38:59 - train: epoch 0079, iter [03000, 05004], lr: 0.024014, loss: 1.0265
2022-07-22 07:41:16 - train: epoch 0079, iter [03100, 05004], lr: 0.023971, loss: 1.0675
2022-07-22 07:43:32 - train: epoch 0079, iter [03200, 05004], lr: 0.023928, loss: 0.9766
2022-07-22 07:45:49 - train: epoch 0079, iter [03300, 05004], lr: 0.023885, loss: 0.9119
2022-07-22 07:48:06 - train: epoch 0079, iter [03400, 05004], lr: 0.023843, loss: 0.8027
2022-07-22 07:50:22 - train: epoch 0079, iter [03500, 05004], lr: 0.023800, loss: 0.9958
2022-07-22 07:52:39 - train: epoch 0079, iter [03600, 05004], lr: 0.023757, loss: 0.8376
2022-07-22 07:54:55 - train: epoch 0079, iter [03700, 05004], lr: 0.023714, loss: 1.0448
2022-07-22 07:57:12 - train: epoch 0079, iter [03800, 05004], lr: 0.023672, loss: 0.8800
2022-07-22 07:59:29 - train: epoch 0079, iter [03900, 05004], lr: 0.023629, loss: 1.0114
2022-07-22 08:01:45 - train: epoch 0079, iter [04000, 05004], lr: 0.023586, loss: 0.7887
2022-07-22 08:04:02 - train: epoch 0079, iter [04100, 05004], lr: 0.023544, loss: 1.0096
2022-07-22 08:06:18 - train: epoch 0079, iter [04200, 05004], lr: 0.023501, loss: 0.8372
2022-07-22 08:08:35 - train: epoch 0079, iter [04300, 05004], lr: 0.023458, loss: 0.7448
2022-07-22 08:10:52 - train: epoch 0079, iter [04400, 05004], lr: 0.023416, loss: 1.1227
2022-07-22 08:13:08 - train: epoch 0079, iter [04500, 05004], lr: 0.023373, loss: 1.0889
2022-07-22 08:15:25 - train: epoch 0079, iter [04600, 05004], lr: 0.023331, loss: 1.0961
2022-07-22 08:17:41 - train: epoch 0079, iter [04700, 05004], lr: 0.023289, loss: 1.0515
2022-07-22 08:19:58 - train: epoch 0079, iter [04800, 05004], lr: 0.023246, loss: 1.1357
2022-07-22 08:22:15 - train: epoch 0079, iter [04900, 05004], lr: 0.023204, loss: 0.9593
2022-07-22 08:24:31 - train: epoch 0079, iter [05000, 05004], lr: 0.023162, loss: 0.7831
2022-07-22 08:24:38 - train: epoch 079, train_loss: 0.9893
2022-07-22 08:26:49 - eval: epoch: 079, acc1: 74.626%, acc5: 92.432%, test_loss: 1.0316, per_image_load_time: 1.070ms, per_image_inference_time: 3.910ms
2022-07-22 08:26:49 - until epoch: 079, best_acc1: 74.626%
2022-07-22 08:26:49 - epoch 080 lr: 0.023159
2022-07-22 08:29:14 - train: epoch 0080, iter [00100, 05004], lr: 0.023118, loss: 0.7373
2022-07-22 08:31:30 - train: epoch 0080, iter [00200, 05004], lr: 0.023075, loss: 1.1544
2022-07-22 08:33:47 - train: epoch 0080, iter [00300, 05004], lr: 0.023033, loss: 1.0554
2022-07-22 08:36:03 - train: epoch 0080, iter [00400, 05004], lr: 0.022991, loss: 0.7816
2022-07-22 08:38:20 - train: epoch 0080, iter [00500, 05004], lr: 0.022949, loss: 0.6871
2022-07-22 08:40:37 - train: epoch 0080, iter [00600, 05004], lr: 0.022907, loss: 0.9665
2022-07-22 08:42:53 - train: epoch 0080, iter [00700, 05004], lr: 0.022865, loss: 0.9853
2022-07-22 08:45:10 - train: epoch 0080, iter [00800, 05004], lr: 0.022823, loss: 0.7441
2022-07-22 08:47:27 - train: epoch 0080, iter [00900, 05004], lr: 0.022781, loss: 0.9482
2022-07-22 08:49:43 - train: epoch 0080, iter [01000, 05004], lr: 0.022739, loss: 0.9460
2022-07-22 08:52:00 - train: epoch 0080, iter [01100, 05004], lr: 0.022697, loss: 1.0716
2022-07-22 08:54:17 - train: epoch 0080, iter [01200, 05004], lr: 0.022655, loss: 0.7732
2022-07-22 08:56:33 - train: epoch 0080, iter [01300, 05004], lr: 0.022613, loss: 0.9328
2022-07-22 08:58:50 - train: epoch 0080, iter [01400, 05004], lr: 0.022571, loss: 1.0056
2022-07-22 09:01:07 - train: epoch 0080, iter [01500, 05004], lr: 0.022529, loss: 0.8849
2022-07-22 09:03:23 - train: epoch 0080, iter [01600, 05004], lr: 0.022488, loss: 0.9841
2022-07-22 09:05:40 - train: epoch 0080, iter [01700, 05004], lr: 0.022446, loss: 1.0546
2022-07-22 09:07:57 - train: epoch 0080, iter [01800, 05004], lr: 0.022404, loss: 0.9359
2022-07-22 09:10:14 - train: epoch 0080, iter [01900, 05004], lr: 0.022362, loss: 0.9467
2022-07-22 09:12:30 - train: epoch 0080, iter [02000, 05004], lr: 0.022321, loss: 0.9767
2022-07-22 09:14:47 - train: epoch 0080, iter [02100, 05004], lr: 0.022279, loss: 1.0551
2022-07-22 09:17:04 - train: epoch 0080, iter [02200, 05004], lr: 0.022238, loss: 0.9077
2022-07-22 09:19:20 - train: epoch 0080, iter [02300, 05004], lr: 0.022196, loss: 0.9494
2022-07-22 09:21:37 - train: epoch 0080, iter [02400, 05004], lr: 0.022155, loss: 1.1491
2022-07-22 09:23:53 - train: epoch 0080, iter [02500, 05004], lr: 0.022113, loss: 0.8852
2022-07-22 09:26:10 - train: epoch 0080, iter [02600, 05004], lr: 0.022072, loss: 0.8926
2022-07-22 09:28:27 - train: epoch 0080, iter [02700, 05004], lr: 0.022030, loss: 1.0668
2022-07-22 09:30:43 - train: epoch 0080, iter [02800, 05004], lr: 0.021989, loss: 0.9117
2022-07-22 09:33:00 - train: epoch 0080, iter [02900, 05004], lr: 0.021948, loss: 0.9651
2022-07-22 09:35:16 - train: epoch 0080, iter [03000, 05004], lr: 0.021906, loss: 0.8975
2022-07-22 09:37:33 - train: epoch 0080, iter [03100, 05004], lr: 0.021865, loss: 0.9884
2022-07-22 09:39:50 - train: epoch 0080, iter [03200, 05004], lr: 0.021824, loss: 0.8537
2022-07-22 09:42:06 - train: epoch 0080, iter [03300, 05004], lr: 0.021783, loss: 1.1418
2022-07-22 09:44:23 - train: epoch 0080, iter [03400, 05004], lr: 0.021741, loss: 0.8200
2022-07-22 09:46:40 - train: epoch 0080, iter [03500, 05004], lr: 0.021700, loss: 1.0945
2022-07-22 09:48:57 - train: epoch 0080, iter [03600, 05004], lr: 0.021659, loss: 0.8882
2022-07-22 09:51:14 - train: epoch 0080, iter [03700, 05004], lr: 0.021618, loss: 0.7993
2022-07-22 09:53:31 - train: epoch 0080, iter [03800, 05004], lr: 0.021577, loss: 0.9457
2022-07-22 09:55:48 - train: epoch 0080, iter [03900, 05004], lr: 0.021536, loss: 0.9048
2022-07-22 09:58:05 - train: epoch 0080, iter [04000, 05004], lr: 0.021495, loss: 1.0408
2022-07-22 10:00:21 - train: epoch 0080, iter [04100, 05004], lr: 0.021454, loss: 1.1580
2022-07-22 10:02:38 - train: epoch 0080, iter [04200, 05004], lr: 0.021413, loss: 0.9265
2022-07-22 10:04:55 - train: epoch 0080, iter [04300, 05004], lr: 0.021373, loss: 1.1562
2022-07-22 10:07:12 - train: epoch 0080, iter [04400, 05004], lr: 0.021332, loss: 0.9462
2022-07-22 10:09:28 - train: epoch 0080, iter [04500, 05004], lr: 0.021291, loss: 0.8556
2022-07-22 10:11:45 - train: epoch 0080, iter [04600, 05004], lr: 0.021250, loss: 0.9965
2022-07-22 10:14:02 - train: epoch 0080, iter [04700, 05004], lr: 0.021210, loss: 1.0484
2022-07-22 10:16:19 - train: epoch 0080, iter [04800, 05004], lr: 0.021169, loss: 1.0546
2022-07-22 10:18:35 - train: epoch 0080, iter [04900, 05004], lr: 0.021128, loss: 1.1620
2022-07-22 10:20:52 - train: epoch 0080, iter [05000, 05004], lr: 0.021088, loss: 0.9506
2022-07-22 10:20:58 - train: epoch 080, train_loss: 0.9628
2022-07-22 10:23:09 - eval: epoch: 080, acc1: 75.162%, acc5: 92.560%, test_loss: 1.0201, per_image_load_time: 1.010ms, per_image_inference_time: 3.919ms
2022-07-22 10:23:09 - until epoch: 080, best_acc1: 75.162%
2022-07-22 10:23:09 - epoch 081 lr: 0.021086
2022-07-22 10:25:34 - train: epoch 0081, iter [00100, 05004], lr: 0.021045, loss: 0.8137
2022-07-22 10:27:51 - train: epoch 0081, iter [00200, 05004], lr: 0.021005, loss: 0.7912
2022-07-22 10:30:07 - train: epoch 0081, iter [00300, 05004], lr: 0.020964, loss: 0.9159
2022-07-22 10:32:24 - train: epoch 0081, iter [00400, 05004], lr: 0.020924, loss: 0.8248
2022-07-22 10:34:40 - train: epoch 0081, iter [00500, 05004], lr: 0.020883, loss: 0.9098
2022-07-22 10:36:57 - train: epoch 0081, iter [00600, 05004], lr: 0.020843, loss: 0.8604
2022-07-22 10:39:14 - train: epoch 0081, iter [00700, 05004], lr: 0.020803, loss: 0.9050
2022-07-22 10:41:31 - train: epoch 0081, iter [00800, 05004], lr: 0.020762, loss: 1.0014
2022-07-22 10:43:47 - train: epoch 0081, iter [00900, 05004], lr: 0.020722, loss: 0.9851
2022-07-22 10:46:04 - train: epoch 0081, iter [01000, 05004], lr: 0.020682, loss: 0.9180
2022-07-22 10:48:20 - train: epoch 0081, iter [01100, 05004], lr: 0.020642, loss: 0.9435
2022-07-22 10:50:37 - train: epoch 0081, iter [01200, 05004], lr: 0.020601, loss: 0.8778
2022-07-22 10:52:53 - train: epoch 0081, iter [01300, 05004], lr: 0.020561, loss: 0.8971
2022-07-22 10:55:10 - train: epoch 0081, iter [01400, 05004], lr: 0.020521, loss: 0.7813
2022-07-22 10:57:26 - train: epoch 0081, iter [01500, 05004], lr: 0.020481, loss: 1.0021
2022-07-22 10:59:43 - train: epoch 0081, iter [01600, 05004], lr: 0.020441, loss: 0.9029
2022-07-22 11:01:59 - train: epoch 0081, iter [01700, 05004], lr: 0.020401, loss: 1.0174
2022-07-22 11:04:16 - train: epoch 0081, iter [01800, 05004], lr: 0.020361, loss: 0.8376
2022-07-22 11:06:33 - train: epoch 0081, iter [01900, 05004], lr: 0.020321, loss: 0.9561
2022-07-22 11:08:49 - train: epoch 0081, iter [02000, 05004], lr: 0.020281, loss: 1.1213
2022-07-22 11:11:06 - train: epoch 0081, iter [02100, 05004], lr: 0.020241, loss: 0.8099
2022-07-22 11:13:22 - train: epoch 0081, iter [02200, 05004], lr: 0.020201, loss: 1.0160
2022-07-22 11:15:39 - train: epoch 0081, iter [02300, 05004], lr: 0.020162, loss: 0.7125
2022-07-22 11:17:55 - train: epoch 0081, iter [02400, 05004], lr: 0.020122, loss: 0.9837
2022-07-22 11:20:12 - train: epoch 0081, iter [02500, 05004], lr: 0.020082, loss: 1.1314
2022-07-22 11:22:29 - train: epoch 0081, iter [02600, 05004], lr: 0.020042, loss: 1.0439
2022-07-22 11:24:45 - train: epoch 0081, iter [02700, 05004], lr: 0.020003, loss: 0.9563
2022-07-22 11:27:02 - train: epoch 0081, iter [02800, 05004], lr: 0.019963, loss: 0.8733
2022-07-22 11:29:19 - train: epoch 0081, iter [02900, 05004], lr: 0.019923, loss: 0.8341
2022-07-22 11:31:36 - train: epoch 0081, iter [03000, 05004], lr: 0.019884, loss: 0.9140
2022-07-22 11:33:53 - train: epoch 0081, iter [03100, 05004], lr: 0.019844, loss: 0.8393
2022-07-22 11:36:10 - train: epoch 0081, iter [03200, 05004], lr: 0.019805, loss: 0.8727
2022-07-22 11:38:26 - train: epoch 0081, iter [03300, 05004], lr: 0.019765, loss: 0.9222
2022-07-22 11:40:43 - train: epoch 0081, iter [03400, 05004], lr: 0.019726, loss: 0.9132
2022-07-22 11:43:00 - train: epoch 0081, iter [03500, 05004], lr: 0.019687, loss: 1.1458
2022-07-22 11:45:17 - train: epoch 0081, iter [03600, 05004], lr: 0.019647, loss: 0.8082
2022-07-22 11:47:34 - train: epoch 0081, iter [03700, 05004], lr: 0.019608, loss: 0.9438
2022-07-22 11:49:51 - train: epoch 0081, iter [03800, 05004], lr: 0.019569, loss: 0.9315
2022-07-22 11:52:07 - train: epoch 0081, iter [03900, 05004], lr: 0.019529, loss: 1.0435
2022-07-22 11:54:24 - train: epoch 0081, iter [04000, 05004], lr: 0.019490, loss: 0.8343
2022-07-22 11:56:41 - train: epoch 0081, iter [04100, 05004], lr: 0.019451, loss: 1.0124
2022-07-22 11:58:58 - train: epoch 0081, iter [04200, 05004], lr: 0.019412, loss: 1.1102
2022-07-22 12:01:15 - train: epoch 0081, iter [04300, 05004], lr: 0.019373, loss: 0.8543
2022-07-22 12:03:31 - train: epoch 0081, iter [04400, 05004], lr: 0.019334, loss: 1.0357
2022-07-22 12:05:48 - train: epoch 0081, iter [04500, 05004], lr: 0.019295, loss: 1.0689
2022-07-22 12:08:05 - train: epoch 0081, iter [04600, 05004], lr: 0.019256, loss: 0.9868
2022-07-22 12:10:22 - train: epoch 0081, iter [04700, 05004], lr: 0.019217, loss: 0.8880
2022-07-22 12:12:39 - train: epoch 0081, iter [04800, 05004], lr: 0.019178, loss: 0.9786
2022-07-22 12:14:56 - train: epoch 0081, iter [04900, 05004], lr: 0.019139, loss: 1.0864
2022-07-22 12:17:13 - train: epoch 0081, iter [05000, 05004], lr: 0.019100, loss: 0.9275
2022-07-22 12:17:19 - train: epoch 081, train_loss: 0.9357
2022-07-22 12:19:30 - eval: epoch: 081, acc1: 75.412%, acc5: 92.732%, test_loss: 1.0174, per_image_load_time: 1.137ms, per_image_inference_time: 3.909ms
2022-07-22 12:19:30 - until epoch: 081, best_acc1: 75.412%
2022-07-22 12:19:30 - epoch 082 lr: 0.019098
2022-07-22 12:21:56 - train: epoch 0082, iter [00100, 05004], lr: 0.019059, loss: 0.7026
2022-07-22 12:24:13 - train: epoch 0082, iter [00200, 05004], lr: 0.019021, loss: 0.8485
2022-07-22 12:26:29 - train: epoch 0082, iter [00300, 05004], lr: 0.018982, loss: 0.9368
2022-07-22 12:28:46 - train: epoch 0082, iter [00400, 05004], lr: 0.018943, loss: 0.9699
2022-07-22 12:31:03 - train: epoch 0082, iter [00500, 05004], lr: 0.018905, loss: 1.0357
2022-07-22 12:33:20 - train: epoch 0082, iter [00600, 05004], lr: 0.018866, loss: 0.8675
2022-07-22 12:35:36 - train: epoch 0082, iter [00700, 05004], lr: 0.018827, loss: 1.1887
2022-07-22 12:37:53 - train: epoch 0082, iter [00800, 05004], lr: 0.018789, loss: 0.9120
2022-07-22 12:40:10 - train: epoch 0082, iter [00900, 05004], lr: 0.018750, loss: 0.9275
2022-07-22 12:42:27 - train: epoch 0082, iter [01000, 05004], lr: 0.018712, loss: 0.9172
2022-07-22 12:44:43 - train: epoch 0082, iter [01100, 05004], lr: 0.018673, loss: 1.0839
2022-07-22 12:47:00 - train: epoch 0082, iter [01200, 05004], lr: 0.018635, loss: 1.0395
2022-07-22 12:49:17 - train: epoch 0082, iter [01300, 05004], lr: 0.018596, loss: 0.9161
2022-07-22 12:51:34 - train: epoch 0082, iter [01400, 05004], lr: 0.018558, loss: 0.9945
2022-07-22 12:53:50 - train: epoch 0082, iter [01500, 05004], lr: 0.018520, loss: 0.8497
2022-07-22 12:56:07 - train: epoch 0082, iter [01600, 05004], lr: 0.018481, loss: 0.8917
2022-07-22 12:58:24 - train: epoch 0082, iter [01700, 05004], lr: 0.018443, loss: 0.9088
2022-07-22 13:00:41 - train: epoch 0082, iter [01800, 05004], lr: 0.018405, loss: 0.7585
2022-07-22 13:02:58 - train: epoch 0082, iter [01900, 05004], lr: 0.018367, loss: 1.0185
2022-07-22 13:05:15 - train: epoch 0082, iter [02000, 05004], lr: 0.018329, loss: 0.9481
2022-07-22 13:07:32 - train: epoch 0082, iter [02100, 05004], lr: 0.018290, loss: 0.9662
2022-07-22 13:09:48 - train: epoch 0082, iter [02200, 05004], lr: 0.018252, loss: 0.9798
2022-07-22 13:12:05 - train: epoch 0082, iter [02300, 05004], lr: 0.018214, loss: 0.8158
2022-07-22 13:14:22 - train: epoch 0082, iter [02400, 05004], lr: 0.018176, loss: 0.8771
2022-07-22 13:16:39 - train: epoch 0082, iter [02500, 05004], lr: 0.018138, loss: 0.7902
2022-07-22 13:18:55 - train: epoch 0082, iter [02600, 05004], lr: 0.018100, loss: 0.9943
2022-07-22 13:21:12 - train: epoch 0082, iter [02700, 05004], lr: 0.018062, loss: 0.8827
2022-07-22 13:23:28 - train: epoch 0082, iter [02800, 05004], lr: 0.018025, loss: 0.6756
2022-07-22 13:25:45 - train: epoch 0082, iter [02900, 05004], lr: 0.017987, loss: 0.7327
2022-07-22 13:28:02 - train: epoch 0082, iter [03000, 05004], lr: 0.017949, loss: 1.0479
2022-07-22 13:30:19 - train: epoch 0082, iter [03100, 05004], lr: 0.017911, loss: 0.9182
2022-07-22 13:32:36 - train: epoch 0082, iter [03200, 05004], lr: 0.017873, loss: 1.0687
2022-07-22 13:34:53 - train: epoch 0082, iter [03300, 05004], lr: 0.017836, loss: 0.9767
2022-07-22 13:37:10 - train: epoch 0082, iter [03400, 05004], lr: 0.017798, loss: 0.7073
2022-07-22 13:39:27 - train: epoch 0082, iter [03500, 05004], lr: 0.017761, loss: 0.8055
2022-07-22 13:41:44 - train: epoch 0082, iter [03600, 05004], lr: 0.017723, loss: 0.8460
2022-07-22 13:44:01 - train: epoch 0082, iter [03700, 05004], lr: 0.017685, loss: 1.0349
2022-07-22 13:46:18 - train: epoch 0082, iter [03800, 05004], lr: 0.017648, loss: 1.0425
2022-07-22 13:48:35 - train: epoch 0082, iter [03900, 05004], lr: 0.017610, loss: 0.8380
2022-07-22 13:50:52 - train: epoch 0082, iter [04000, 05004], lr: 0.017573, loss: 0.9954
2022-07-22 13:53:09 - train: epoch 0082, iter [04100, 05004], lr: 0.017536, loss: 0.9679
2022-07-22 13:55:26 - train: epoch 0082, iter [04200, 05004], lr: 0.017498, loss: 0.8525
2022-07-22 13:57:43 - train: epoch 0082, iter [04300, 05004], lr: 0.017461, loss: 0.8031
2022-07-22 14:00:00 - train: epoch 0082, iter [04400, 05004], lr: 0.017424, loss: 0.8975
2022-07-22 14:02:17 - train: epoch 0082, iter [04500, 05004], lr: 0.017386, loss: 0.8264
2022-07-22 14:04:34 - train: epoch 0082, iter [04600, 05004], lr: 0.017349, loss: 1.0263
2022-07-22 14:06:51 - train: epoch 0082, iter [04700, 05004], lr: 0.017312, loss: 0.9268
2022-07-22 14:09:08 - train: epoch 0082, iter [04800, 05004], lr: 0.017275, loss: 0.9100
2022-07-22 14:11:25 - train: epoch 0082, iter [04900, 05004], lr: 0.017238, loss: 0.9174
2022-07-22 14:13:42 - train: epoch 0082, iter [05000, 05004], lr: 0.017201, loss: 0.8379
2022-07-22 14:13:49 - train: epoch 082, train_loss: 0.9079
2022-07-22 14:16:05 - eval: epoch: 082, acc1: 75.530%, acc5: 92.808%, test_loss: 1.0102, per_image_load_time: 1.385ms, per_image_inference_time: 3.916ms
2022-07-22 14:16:05 - until epoch: 082, best_acc1: 75.530%
2022-07-22 14:16:05 - epoch 083 lr: 0.017199
2022-07-22 14:18:30 - train: epoch 0083, iter [00100, 05004], lr: 0.017162, loss: 0.6589
2022-07-22 14:20:47 - train: epoch 0083, iter [00200, 05004], lr: 0.017125, loss: 0.8049
2022-07-22 14:23:04 - train: epoch 0083, iter [00300, 05004], lr: 0.017088, loss: 0.8286
2022-07-22 14:25:20 - train: epoch 0083, iter [00400, 05004], lr: 0.017051, loss: 0.9022
2022-07-22 14:27:37 - train: epoch 0083, iter [00500, 05004], lr: 0.017014, loss: 0.8139
2022-07-22 14:29:54 - train: epoch 0083, iter [00600, 05004], lr: 0.016977, loss: 0.7869
2022-07-22 14:32:11 - train: epoch 0083, iter [00700, 05004], lr: 0.016941, loss: 0.9975
2022-07-22 14:34:28 - train: epoch 0083, iter [00800, 05004], lr: 0.016904, loss: 0.9264
2022-07-22 14:36:44 - train: epoch 0083, iter [00900, 05004], lr: 0.016867, loss: 1.1059
2022-07-22 14:39:01 - train: epoch 0083, iter [01000, 05004], lr: 0.016830, loss: 0.9862
2022-07-22 14:41:18 - train: epoch 0083, iter [01100, 05004], lr: 0.016794, loss: 1.1624
2022-07-22 14:43:35 - train: epoch 0083, iter [01200, 05004], lr: 0.016757, loss: 0.8619
2022-07-22 14:45:52 - train: epoch 0083, iter [01300, 05004], lr: 0.016720, loss: 0.8868
2022-07-22 14:48:08 - train: epoch 0083, iter [01400, 05004], lr: 0.016684, loss: 1.0821
2022-07-22 14:50:25 - train: epoch 0083, iter [01500, 05004], lr: 0.016647, loss: 0.7890
2022-07-22 14:52:42 - train: epoch 0083, iter [01600, 05004], lr: 0.016611, loss: 1.0830
2022-07-22 14:54:58 - train: epoch 0083, iter [01700, 05004], lr: 0.016574, loss: 0.8727
2022-07-22 14:57:15 - train: epoch 0083, iter [01800, 05004], lr: 0.016538, loss: 0.9430
2022-07-22 14:59:32 - train: epoch 0083, iter [01900, 05004], lr: 0.016502, loss: 0.8696
2022-07-22 15:01:48 - train: epoch 0083, iter [02000, 05004], lr: 0.016465, loss: 0.6924
2022-07-22 15:04:05 - train: epoch 0083, iter [02100, 05004], lr: 0.016429, loss: 0.8144
2022-07-22 15:06:21 - train: epoch 0083, iter [02200, 05004], lr: 0.016393, loss: 0.8267
2022-07-22 15:08:38 - train: epoch 0083, iter [02300, 05004], lr: 0.016356, loss: 0.9057
2022-07-22 15:10:55 - train: epoch 0083, iter [02400, 05004], lr: 0.016320, loss: 0.8096
2022-07-22 15:13:12 - train: epoch 0083, iter [02500, 05004], lr: 0.016284, loss: 0.8374
2022-07-22 15:15:28 - train: epoch 0083, iter [02600, 05004], lr: 0.016248, loss: 0.8539
2022-07-22 15:17:45 - train: epoch 0083, iter [02700, 05004], lr: 0.016212, loss: 0.8310
2022-07-22 15:20:01 - train: epoch 0083, iter [02800, 05004], lr: 0.016176, loss: 0.8654
2022-07-22 15:22:18 - train: epoch 0083, iter [02900, 05004], lr: 0.016140, loss: 0.9621
2022-07-22 15:24:35 - train: epoch 0083, iter [03000, 05004], lr: 0.016104, loss: 0.9046
2022-07-22 15:26:51 - train: epoch 0083, iter [03100, 05004], lr: 0.016068, loss: 0.9278
2022-07-22 15:29:08 - train: epoch 0083, iter [03200, 05004], lr: 0.016032, loss: 0.7407
2022-07-22 15:31:25 - train: epoch 0083, iter [03300, 05004], lr: 0.015996, loss: 1.0924
2022-07-22 15:33:41 - train: epoch 0083, iter [03400, 05004], lr: 0.015960, loss: 0.8809
2022-07-22 15:35:58 - train: epoch 0083, iter [03500, 05004], lr: 0.015924, loss: 0.9552
2022-07-22 15:38:15 - train: epoch 0083, iter [03600, 05004], lr: 0.015889, loss: 0.8940
2022-07-22 15:40:31 - train: epoch 0083, iter [03700, 05004], lr: 0.015853, loss: 0.7895
2022-07-22 15:42:48 - train: epoch 0083, iter [03800, 05004], lr: 0.015817, loss: 1.0307
2022-07-22 15:45:05 - train: epoch 0083, iter [03900, 05004], lr: 0.015782, loss: 0.9807
2022-07-22 15:47:21 - train: epoch 0083, iter [04000, 05004], lr: 0.015746, loss: 0.9094
2022-07-22 15:49:38 - train: epoch 0083, iter [04100, 05004], lr: 0.015710, loss: 0.8658
2022-07-22 15:51:55 - train: epoch 0083, iter [04200, 05004], lr: 0.015675, loss: 1.0180
2022-07-22 15:54:11 - train: epoch 0083, iter [04300, 05004], lr: 0.015639, loss: 0.8239
2022-07-22 15:56:28 - train: epoch 0083, iter [04400, 05004], lr: 0.015604, loss: 0.9829
2022-07-22 15:58:45 - train: epoch 0083, iter [04500, 05004], lr: 0.015568, loss: 0.9574
2022-07-22 16:01:02 - train: epoch 0083, iter [04600, 05004], lr: 0.015533, loss: 0.7609
2022-07-22 16:03:18 - train: epoch 0083, iter [04700, 05004], lr: 0.015498, loss: 1.0578
2022-07-22 16:05:35 - train: epoch 0083, iter [04800, 05004], lr: 0.015462, loss: 0.7965
2022-07-22 16:07:52 - train: epoch 0083, iter [04900, 05004], lr: 0.015427, loss: 0.9331
2022-07-22 16:10:08 - train: epoch 0083, iter [05000, 05004], lr: 0.015392, loss: 0.8548
2022-07-22 16:10:15 - train: epoch 083, train_loss: 0.8809
2022-07-22 16:12:27 - eval: epoch: 083, acc1: 75.672%, acc5: 92.978%, test_loss: 0.9912, per_image_load_time: 1.160ms, per_image_inference_time: 3.901ms
2022-07-22 16:12:28 - until epoch: 083, best_acc1: 75.672%
2022-07-22 16:12:28 - epoch 084 lr: 0.015390
2022-07-22 16:14:53 - train: epoch 0084, iter [00100, 05004], lr: 0.015355, loss: 0.7739
2022-07-22 16:17:10 - train: epoch 0084, iter [00200, 05004], lr: 0.015320, loss: 0.9019
2022-07-22 16:19:26 - train: epoch 0084, iter [00300, 05004], lr: 0.015285, loss: 0.7129
2022-07-22 16:21:43 - train: epoch 0084, iter [00400, 05004], lr: 0.015250, loss: 0.8414
2022-07-22 16:24:00 - train: epoch 0084, iter [00500, 05004], lr: 0.015215, loss: 0.8892
2022-07-22 16:26:17 - train: epoch 0084, iter [00600, 05004], lr: 0.015180, loss: 0.9929
2022-07-22 16:28:33 - train: epoch 0084, iter [00700, 05004], lr: 0.015145, loss: 0.8141
2022-07-22 16:30:50 - train: epoch 0084, iter [00800, 05004], lr: 0.015110, loss: 1.0587
2022-07-22 16:33:07 - train: epoch 0084, iter [00900, 05004], lr: 0.015075, loss: 0.7543
2022-07-22 16:35:23 - train: epoch 0084, iter [01000, 05004], lr: 0.015040, loss: 0.9458
2022-07-22 16:37:40 - train: epoch 0084, iter [01100, 05004], lr: 0.015005, loss: 0.7799
2022-07-22 16:39:57 - train: epoch 0084, iter [01200, 05004], lr: 0.014970, loss: 1.0991
2022-07-22 16:42:13 - train: epoch 0084, iter [01300, 05004], lr: 0.014936, loss: 0.8486
2022-07-22 16:44:30 - train: epoch 0084, iter [01400, 05004], lr: 0.014901, loss: 0.9863
2022-07-22 16:46:47 - train: epoch 0084, iter [01500, 05004], lr: 0.014866, loss: 0.8253
2022-07-22 16:49:04 - train: epoch 0084, iter [01600, 05004], lr: 0.014832, loss: 0.9583
2022-07-22 16:51:20 - train: epoch 0084, iter [01700, 05004], lr: 0.014797, loss: 0.9697
2022-07-22 16:53:37 - train: epoch 0084, iter [01800, 05004], lr: 0.014762, loss: 0.8852
2022-07-22 16:55:54 - train: epoch 0084, iter [01900, 05004], lr: 0.014728, loss: 1.0287
2022-07-22 16:58:10 - train: epoch 0084, iter [02000, 05004], lr: 0.014693, loss: 0.7330
2022-07-22 17:00:27 - train: epoch 0084, iter [02100, 05004], lr: 0.014659, loss: 1.0264
2022-07-22 17:02:44 - train: epoch 0084, iter [02200, 05004], lr: 0.014624, loss: 0.8150
2022-07-22 17:05:00 - train: epoch 0084, iter [02300, 05004], lr: 0.014590, loss: 0.6905
2022-07-22 17:07:17 - train: epoch 0084, iter [02400, 05004], lr: 0.014556, loss: 0.6705
2022-07-22 17:09:33 - train: epoch 0084, iter [02500, 05004], lr: 0.014521, loss: 1.1018
2022-07-22 17:11:50 - train: epoch 0084, iter [02600, 05004], lr: 0.014487, loss: 0.8908
2022-07-22 17:14:07 - train: epoch 0084, iter [02700, 05004], lr: 0.014453, loss: 0.8875
2022-07-22 17:16:24 - train: epoch 0084, iter [02800, 05004], lr: 0.014419, loss: 0.7896
2022-07-22 17:18:40 - train: epoch 0084, iter [02900, 05004], lr: 0.014385, loss: 0.8898
2022-07-22 17:20:57 - train: epoch 0084, iter [03000, 05004], lr: 0.014350, loss: 0.8865
2022-07-22 17:23:13 - train: epoch 0084, iter [03100, 05004], lr: 0.014316, loss: 0.8852
2022-07-22 17:25:30 - train: epoch 0084, iter [03200, 05004], lr: 0.014282, loss: 0.7012
2022-07-22 17:27:46 - train: epoch 0084, iter [03300, 05004], lr: 0.014248, loss: 0.8306
2022-07-22 17:30:03 - train: epoch 0084, iter [03400, 05004], lr: 0.014214, loss: 0.8804
2022-07-22 17:32:20 - train: epoch 0084, iter [03500, 05004], lr: 0.014180, loss: 0.7769
2022-07-22 17:34:36 - train: epoch 0084, iter [03600, 05004], lr: 0.014146, loss: 0.9796
2022-07-22 17:36:53 - train: epoch 0084, iter [03700, 05004], lr: 0.014113, loss: 1.1066
2022-07-22 17:39:09 - train: epoch 0084, iter [03800, 05004], lr: 0.014079, loss: 0.9767
2022-07-22 17:41:26 - train: epoch 0084, iter [03900, 05004], lr: 0.014045, loss: 0.9473
2022-07-22 17:43:42 - train: epoch 0084, iter [04000, 05004], lr: 0.014011, loss: 0.8310
2022-07-22 17:45:59 - train: epoch 0084, iter [04100, 05004], lr: 0.013977, loss: 0.7732
2022-07-22 17:48:16 - train: epoch 0084, iter [04200, 05004], lr: 0.013944, loss: 0.9498
2022-07-22 17:50:32 - train: epoch 0084, iter [04300, 05004], lr: 0.013910, loss: 0.9666
2022-07-22 17:52:49 - train: epoch 0084, iter [04400, 05004], lr: 0.013877, loss: 0.9649
2022-07-22 17:55:06 - train: epoch 0084, iter [04500, 05004], lr: 0.013843, loss: 0.8157
2022-07-22 17:57:22 - train: epoch 0084, iter [04600, 05004], lr: 0.013809, loss: 0.8876
2022-07-22 17:59:39 - train: epoch 0084, iter [04700, 05004], lr: 0.013776, loss: 0.8234
2022-07-22 18:01:55 - train: epoch 0084, iter [04800, 05004], lr: 0.013742, loss: 0.7878
2022-07-22 18:04:12 - train: epoch 0084, iter [04900, 05004], lr: 0.013709, loss: 0.7962
2022-07-22 18:06:29 - train: epoch 0084, iter [05000, 05004], lr: 0.013676, loss: 0.9022
2022-07-22 18:06:35 - train: epoch 084, train_loss: 0.8563
2022-07-22 18:08:45 - eval: epoch: 084, acc1: 75.534%, acc5: 92.858%, test_loss: 0.9938, per_image_load_time: 0.968ms, per_image_inference_time: 3.921ms
2022-07-22 18:08:45 - until epoch: 084, best_acc1: 75.672%
2022-07-22 18:08:45 - epoch 085 lr: 0.013674
2022-07-22 18:11:10 - train: epoch 0085, iter [00100, 05004], lr: 0.013641, loss: 0.6578
2022-07-22 18:13:26 - train: epoch 0085, iter [00200, 05004], lr: 0.013608, loss: 0.7826
2022-07-22 18:15:43 - train: epoch 0085, iter [00300, 05004], lr: 0.013574, loss: 0.8025
2022-07-22 18:17:59 - train: epoch 0085, iter [00400, 05004], lr: 0.013541, loss: 1.0201
2022-07-22 18:20:16 - train: epoch 0085, iter [00500, 05004], lr: 0.013508, loss: 0.8870
2022-07-22 18:22:32 - train: epoch 0085, iter [00600, 05004], lr: 0.013475, loss: 0.6644
2022-07-22 18:24:49 - train: epoch 0085, iter [00700, 05004], lr: 0.013442, loss: 0.7389
2022-07-22 18:27:06 - train: epoch 0085, iter [00800, 05004], lr: 0.013409, loss: 0.6089
2022-07-22 18:29:22 - train: epoch 0085, iter [00900, 05004], lr: 0.013376, loss: 0.8449
2022-07-22 18:31:39 - train: epoch 0085, iter [01000, 05004], lr: 0.013343, loss: 0.8263
2022-07-22 18:33:55 - train: epoch 0085, iter [01100, 05004], lr: 0.013310, loss: 0.7913
2022-07-22 18:36:12 - train: epoch 0085, iter [01200, 05004], lr: 0.013277, loss: 0.8760
2022-07-22 18:38:28 - train: epoch 0085, iter [01300, 05004], lr: 0.013244, loss: 0.7306
2022-07-22 18:40:45 - train: epoch 0085, iter [01400, 05004], lr: 0.013211, loss: 0.8769
2022-07-22 18:43:02 - train: epoch 0085, iter [01500, 05004], lr: 0.013178, loss: 0.8928
2022-07-22 18:45:18 - train: epoch 0085, iter [01600, 05004], lr: 0.013145, loss: 0.7653
2022-07-22 18:47:35 - train: epoch 0085, iter [01700, 05004], lr: 0.013113, loss: 0.9977
2022-07-22 18:49:52 - train: epoch 0085, iter [01800, 05004], lr: 0.013080, loss: 0.8162
2022-07-22 18:52:09 - train: epoch 0085, iter [01900, 05004], lr: 0.013047, loss: 0.7743
2022-07-22 18:54:25 - train: epoch 0085, iter [02000, 05004], lr: 0.013015, loss: 0.5895
2022-07-22 18:56:42 - train: epoch 0085, iter [02100, 05004], lr: 0.012982, loss: 0.6908
2022-07-22 18:58:59 - train: epoch 0085, iter [02200, 05004], lr: 0.012950, loss: 0.8832
2022-07-22 19:01:15 - train: epoch 0085, iter [02300, 05004], lr: 0.012917, loss: 0.8904
2022-07-22 19:03:32 - train: epoch 0085, iter [02400, 05004], lr: 0.012885, loss: 0.9592
2022-07-22 19:05:49 - train: epoch 0085, iter [02500, 05004], lr: 0.012852, loss: 0.9932
2022-07-22 19:08:05 - train: epoch 0085, iter [02600, 05004], lr: 0.012820, loss: 1.0718
2022-07-22 19:10:22 - train: epoch 0085, iter [02700, 05004], lr: 0.012787, loss: 0.7674
2022-07-22 19:12:39 - train: epoch 0085, iter [02800, 05004], lr: 0.012755, loss: 0.9762
2022-07-22 19:14:56 - train: epoch 0085, iter [02900, 05004], lr: 0.012723, loss: 0.8636
2022-07-22 19:17:13 - train: epoch 0085, iter [03000, 05004], lr: 0.012691, loss: 0.7825
2022-07-22 19:19:30 - train: epoch 0085, iter [03100, 05004], lr: 0.012658, loss: 0.8163
2022-07-22 19:21:47 - train: epoch 0085, iter [03200, 05004], lr: 0.012626, loss: 0.7659
2022-07-22 19:24:04 - train: epoch 0085, iter [03300, 05004], lr: 0.012594, loss: 0.8549
2022-07-22 19:26:21 - train: epoch 0085, iter [03400, 05004], lr: 0.012562, loss: 0.9990
2022-07-22 19:28:38 - train: epoch 0085, iter [03500, 05004], lr: 0.012530, loss: 0.9430
2022-07-22 19:30:55 - train: epoch 0085, iter [03600, 05004], lr: 0.012498, loss: 0.7389
2022-07-22 19:33:13 - train: epoch 0085, iter [03700, 05004], lr: 0.012466, loss: 0.6439
2022-07-22 19:35:29 - train: epoch 0085, iter [03800, 05004], lr: 0.012434, loss: 0.8962
2022-07-22 19:37:46 - train: epoch 0085, iter [03900, 05004], lr: 0.012402, loss: 0.6723
2022-07-22 19:40:03 - train: epoch 0085, iter [04000, 05004], lr: 0.012370, loss: 0.9203
2022-07-22 19:42:21 - train: epoch 0085, iter [04100, 05004], lr: 0.012339, loss: 0.9656
2022-07-22 19:44:38 - train: epoch 0085, iter [04200, 05004], lr: 0.012307, loss: 0.8248
2022-07-22 19:46:55 - train: epoch 0085, iter [04300, 05004], lr: 0.012275, loss: 0.9387
2022-07-22 19:49:12 - train: epoch 0085, iter [04400, 05004], lr: 0.012243, loss: 0.8972
2022-07-22 19:51:29 - train: epoch 0085, iter [04500, 05004], lr: 0.012212, loss: 0.8181
2022-07-22 19:53:46 - train: epoch 0085, iter [04600, 05004], lr: 0.012180, loss: 0.9005
2022-07-22 19:56:03 - train: epoch 0085, iter [04700, 05004], lr: 0.012148, loss: 0.9588
2022-07-22 19:58:19 - train: epoch 0085, iter [04800, 05004], lr: 0.012117, loss: 0.8310
2022-07-22 20:00:36 - train: epoch 0085, iter [04900, 05004], lr: 0.012085, loss: 0.7785
2022-07-22 20:02:53 - train: epoch 0085, iter [05000, 05004], lr: 0.012054, loss: 0.6417
2022-07-22 20:03:00 - train: epoch 085, train_loss: 0.8248
2022-07-22 20:05:13 - eval: epoch: 085, acc1: 76.300%, acc5: 93.114%, test_loss: 0.9746, per_image_load_time: 1.166ms, per_image_inference_time: 3.916ms
2022-07-22 20:05:13 - until epoch: 085, best_acc1: 76.300%
2022-07-22 20:05:13 - epoch 086 lr: 0.012052
2022-07-22 20:07:38 - train: epoch 0086, iter [00100, 05004], lr: 0.012021, loss: 0.6669
2022-07-22 20:09:54 - train: epoch 0086, iter [00200, 05004], lr: 0.011990, loss: 0.7622
2022-07-22 20:12:11 - train: epoch 0086, iter [00300, 05004], lr: 0.011958, loss: 0.8498
2022-07-22 20:14:28 - train: epoch 0086, iter [00400, 05004], lr: 0.011927, loss: 0.8451
2022-07-22 20:16:44 - train: epoch 0086, iter [00500, 05004], lr: 0.011896, loss: 0.7851
2022-07-22 20:19:01 - train: epoch 0086, iter [00600, 05004], lr: 0.011865, loss: 0.8339
2022-07-22 20:21:18 - train: epoch 0086, iter [00700, 05004], lr: 0.011833, loss: 0.6646
2022-07-22 20:23:35 - train: epoch 0086, iter [00800, 05004], lr: 0.011802, loss: 0.8075
2022-07-22 20:25:53 - train: epoch 0086, iter [00900, 05004], lr: 0.011771, loss: 0.6786
2022-07-22 20:28:11 - train: epoch 0086, iter [01000, 05004], lr: 0.011740, loss: 1.0094
2022-07-22 20:30:28 - train: epoch 0086, iter [01100, 05004], lr: 0.011709, loss: 0.8049
2022-07-22 20:32:45 - train: epoch 0086, iter [01200, 05004], lr: 0.011678, loss: 0.7246
2022-07-22 20:35:01 - train: epoch 0086, iter [01300, 05004], lr: 0.011647, loss: 0.8074
2022-07-22 20:37:18 - train: epoch 0086, iter [01400, 05004], lr: 0.011616, loss: 0.7538
2022-07-22 20:39:35 - train: epoch 0086, iter [01500, 05004], lr: 0.011585, loss: 0.6225
2022-07-22 20:41:51 - train: epoch 0086, iter [01600, 05004], lr: 0.011554, loss: 0.7088
2022-07-22 20:44:08 - train: epoch 0086, iter [01700, 05004], lr: 0.011523, loss: 0.9114
2022-07-22 20:46:25 - train: epoch 0086, iter [01800, 05004], lr: 0.011493, loss: 0.7710
2022-07-22 20:48:41 - train: epoch 0086, iter [01900, 05004], lr: 0.011462, loss: 0.8094
2022-07-22 20:50:58 - train: epoch 0086, iter [02000, 05004], lr: 0.011431, loss: 0.7784
2022-07-22 20:53:15 - train: epoch 0086, iter [02100, 05004], lr: 0.011401, loss: 0.6712
2022-07-22 20:55:32 - train: epoch 0086, iter [02200, 05004], lr: 0.011370, loss: 0.6987
2022-07-22 20:57:49 - train: epoch 0086, iter [02300, 05004], lr: 0.011339, loss: 0.7836
2022-07-22 21:00:06 - train: epoch 0086, iter [02400, 05004], lr: 0.011309, loss: 0.7044
2022-07-22 21:02:22 - train: epoch 0086, iter [02500, 05004], lr: 0.011278, loss: 0.8118
2022-07-22 21:04:39 - train: epoch 0086, iter [02600, 05004], lr: 0.011248, loss: 0.7354
2022-07-22 21:06:56 - train: epoch 0086, iter [02700, 05004], lr: 0.011217, loss: 0.8378
2022-07-22 21:09:12 - train: epoch 0086, iter [02800, 05004], lr: 0.011187, loss: 0.7730
2022-07-22 21:11:29 - train: epoch 0086, iter [02900, 05004], lr: 0.011157, loss: 0.8079
2022-07-22 21:13:45 - train: epoch 0086, iter [03000, 05004], lr: 0.011126, loss: 0.6969
2022-07-22 21:16:03 - train: epoch 0086, iter [03100, 05004], lr: 0.011096, loss: 0.6993
2022-07-22 21:18:20 - train: epoch 0086, iter [03200, 05004], lr: 0.011066, loss: 0.8749
2022-07-22 21:20:37 - train: epoch 0086, iter [03300, 05004], lr: 0.011036, loss: 0.7742
2022-07-22 21:22:54 - train: epoch 0086, iter [03400, 05004], lr: 0.011005, loss: 0.8072
2022-07-22 21:25:11 - train: epoch 0086, iter [03500, 05004], lr: 0.010975, loss: 0.8891
2022-07-22 21:27:28 - train: epoch 0086, iter [03600, 05004], lr: 0.010945, loss: 0.7908
2022-07-22 21:29:45 - train: epoch 0086, iter [03700, 05004], lr: 0.010915, loss: 0.7979
2022-07-22 21:32:02 - train: epoch 0086, iter [03800, 05004], lr: 0.010885, loss: 0.8559
2022-07-22 21:34:19 - train: epoch 0086, iter [03900, 05004], lr: 0.010855, loss: 0.8611
2022-07-22 21:36:36 - train: epoch 0086, iter [04000, 05004], lr: 0.010825, loss: 0.7787
2022-07-22 21:38:53 - train: epoch 0086, iter [04100, 05004], lr: 0.010795, loss: 0.7376
2022-07-22 21:41:10 - train: epoch 0086, iter [04200, 05004], lr: 0.010766, loss: 0.7575
2022-07-22 21:43:27 - train: epoch 0086, iter [04300, 05004], lr: 0.010736, loss: 0.6494
2022-07-22 21:45:44 - train: epoch 0086, iter [04400, 05004], lr: 0.010706, loss: 0.9467
2022-07-22 21:48:01 - train: epoch 0086, iter [04500, 05004], lr: 0.010676, loss: 0.7087
2022-07-22 21:50:18 - train: epoch 0086, iter [04600, 05004], lr: 0.010647, loss: 0.5911
2022-07-22 21:52:36 - train: epoch 0086, iter [04700, 05004], lr: 0.010617, loss: 0.7321
2022-07-22 21:54:53 - train: epoch 0086, iter [04800, 05004], lr: 0.010587, loss: 0.8400
2022-07-22 21:57:10 - train: epoch 0086, iter [04900, 05004], lr: 0.010558, loss: 0.8166
2022-07-22 21:59:27 - train: epoch 0086, iter [05000, 05004], lr: 0.010528, loss: 0.8308
2022-07-22 21:59:34 - train: epoch 086, train_loss: 0.7976
2022-07-22 22:01:48 - eval: epoch: 086, acc1: 76.430%, acc5: 93.236%, test_loss: 0.9694, per_image_load_time: 1.172ms, per_image_inference_time: 3.921ms
2022-07-22 22:01:48 - until epoch: 086, best_acc1: 76.430%
2022-07-22 22:01:48 - epoch 087 lr: 0.010527
2022-07-22 22:04:12 - train: epoch 0087, iter [00100, 05004], lr: 0.010498, loss: 0.7932
2022-07-22 22:06:29 - train: epoch 0087, iter [00200, 05004], lr: 0.010468, loss: 0.6348
2022-07-22 22:08:45 - train: epoch 0087, iter [00300, 05004], lr: 0.010439, loss: 0.7366
2022-07-22 22:11:02 - train: epoch 0087, iter [00400, 05004], lr: 0.010409, loss: 0.7012
2022-07-22 22:13:19 - train: epoch 0087, iter [00500, 05004], lr: 0.010380, loss: 0.7284
2022-07-22 22:15:35 - train: epoch 0087, iter [00600, 05004], lr: 0.010351, loss: 0.9566
2022-07-22 22:17:52 - train: epoch 0087, iter [00700, 05004], lr: 0.010321, loss: 0.5869
2022-07-22 22:20:09 - train: epoch 0087, iter [00800, 05004], lr: 0.010292, loss: 0.8129
2022-07-22 22:22:25 - train: epoch 0087, iter [00900, 05004], lr: 0.010263, loss: 0.7604
2022-07-22 22:24:42 - train: epoch 0087, iter [01000, 05004], lr: 0.010234, loss: 0.8525
2022-07-22 22:26:59 - train: epoch 0087, iter [01100, 05004], lr: 0.010205, loss: 0.7765
2022-07-22 22:29:16 - train: epoch 0087, iter [01200, 05004], lr: 0.010176, loss: 0.8958
2022-07-22 22:31:32 - train: epoch 0087, iter [01300, 05004], lr: 0.010147, loss: 0.9509
2022-07-22 22:33:49 - train: epoch 0087, iter [01400, 05004], lr: 0.010118, loss: 0.6374
2022-07-22 22:36:06 - train: epoch 0087, iter [01500, 05004], lr: 0.010089, loss: 0.7399
2022-07-22 22:38:23 - train: epoch 0087, iter [01600, 05004], lr: 0.010060, loss: 0.6818
2022-07-22 22:40:40 - train: epoch 0087, iter [01700, 05004], lr: 0.010031, loss: 0.9335
2022-07-22 22:42:56 - train: epoch 0087, iter [01800, 05004], lr: 0.010002, loss: 1.0215
2022-07-22 22:45:13 - train: epoch 0087, iter [01900, 05004], lr: 0.009973, loss: 0.8757
2022-07-22 22:47:30 - train: epoch 0087, iter [02000, 05004], lr: 0.009945, loss: 0.6358
2022-07-22 22:49:47 - train: epoch 0087, iter [02100, 05004], lr: 0.009916, loss: 0.7780
2022-07-22 22:52:04 - train: epoch 0087, iter [02200, 05004], lr: 0.009887, loss: 0.7333
2022-07-22 22:54:21 - train: epoch 0087, iter [02300, 05004], lr: 0.009859, loss: 0.9729
2022-07-22 22:56:37 - train: epoch 0087, iter [02400, 05004], lr: 0.009830, loss: 0.8917
2022-07-22 22:58:54 - train: epoch 0087, iter [02500, 05004], lr: 0.009801, loss: 0.8517
2022-07-22 23:01:11 - train: epoch 0087, iter [02600, 05004], lr: 0.009773, loss: 0.8234
2022-07-22 23:03:28 - train: epoch 0087, iter [02700, 05004], lr: 0.009744, loss: 0.7589
2022-07-22 23:05:45 - train: epoch 0087, iter [02800, 05004], lr: 0.009716, loss: 0.6456
