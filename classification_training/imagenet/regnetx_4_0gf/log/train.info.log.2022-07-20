2022-07-20 23:06:54 - train: epoch 0062, iter [04300, 05004], lr: 0.069541, loss: 1.4947
2022-07-20 23:09:11 - train: epoch 0062, iter [04400, 05004], lr: 0.069478, loss: 1.2566
2022-07-20 23:11:28 - train: epoch 0062, iter [04500, 05004], lr: 0.069415, loss: 1.4800
2022-07-20 23:13:44 - train: epoch 0062, iter [04600, 05004], lr: 0.069352, loss: 1.1897
2022-07-20 23:16:01 - train: epoch 0062, iter [04700, 05004], lr: 0.069289, loss: 1.3006
2022-07-20 23:18:18 - train: epoch 0062, iter [04800, 05004], lr: 0.069227, loss: 1.4351
2022-07-20 23:20:35 - train: epoch 0062, iter [04900, 05004], lr: 0.069164, loss: 1.3611
2022-07-20 23:22:51 - train: epoch 0062, iter [05000, 05004], lr: 0.069101, loss: 1.5717
2022-07-20 23:22:58 - train: epoch 062, train_loss: 1.3505
2022-07-20 23:25:16 - eval: epoch: 062, acc1: 70.362%, acc5: 90.184%, test_loss: 1.1981, per_image_load_time: 1.306ms, per_image_inference_time: 3.919ms
2022-07-20 23:25:16 - until epoch: 062, best_acc1: 70.362%
2022-07-20 23:25:16 - epoch 063 lr: 0.069098
2022-07-20 23:27:43 - train: epoch 0063, iter [00100, 05004], lr: 0.069035, loss: 1.2788
2022-07-20 23:30:00 - train: epoch 0063, iter [00200, 05004], lr: 0.068973, loss: 1.3082
2022-07-20 23:32:17 - train: epoch 0063, iter [00300, 05004], lr: 0.068910, loss: 1.2856
2022-07-20 23:34:34 - train: epoch 0063, iter [00400, 05004], lr: 0.068847, loss: 1.3202
2022-07-20 23:36:51 - train: epoch 0063, iter [00500, 05004], lr: 0.068784, loss: 1.2591
2022-07-20 23:39:08 - train: epoch 0063, iter [00600, 05004], lr: 0.068721, loss: 1.3320
2022-07-20 23:41:25 - train: epoch 0063, iter [00700, 05004], lr: 0.068659, loss: 1.3099
2022-07-20 23:43:42 - train: epoch 0063, iter [00800, 05004], lr: 0.068596, loss: 1.3140
2022-07-20 23:45:59 - train: epoch 0063, iter [00900, 05004], lr: 0.068533, loss: 1.2409
2022-07-20 23:48:15 - train: epoch 0063, iter [01000, 05004], lr: 0.068470, loss: 1.5573
2022-07-20 23:50:32 - train: epoch 0063, iter [01100, 05004], lr: 0.068408, loss: 1.2662
2022-07-20 23:52:49 - train: epoch 0063, iter [01200, 05004], lr: 0.068345, loss: 1.3365
2022-07-20 23:55:06 - train: epoch 0063, iter [01300, 05004], lr: 0.068282, loss: 1.2030
2022-07-20 23:57:23 - train: epoch 0063, iter [01400, 05004], lr: 0.068220, loss: 1.4745
2022-07-20 23:59:40 - train: epoch 0063, iter [01500, 05004], lr: 0.068157, loss: 1.3587
2022-07-21 00:01:57 - train: epoch 0063, iter [01600, 05004], lr: 0.068094, loss: 1.3110
2022-07-21 00:04:14 - train: epoch 0063, iter [01700, 05004], lr: 0.068032, loss: 1.0011
2022-07-21 00:06:31 - train: epoch 0063, iter [01800, 05004], lr: 0.067969, loss: 1.2950
2022-07-21 00:08:49 - train: epoch 0063, iter [01900, 05004], lr: 0.067907, loss: 1.5594
2022-07-21 00:11:06 - train: epoch 0063, iter [02000, 05004], lr: 0.067844, loss: 1.2337
2022-07-21 00:13:23 - train: epoch 0063, iter [02100, 05004], lr: 0.067781, loss: 1.2933
2022-07-21 00:15:40 - train: epoch 0063, iter [02200, 05004], lr: 0.067719, loss: 1.6180
2022-07-21 00:17:57 - train: epoch 0063, iter [02300, 05004], lr: 0.067656, loss: 1.2849
2022-07-21 00:20:14 - train: epoch 0063, iter [02400, 05004], lr: 0.067594, loss: 1.3685
2022-07-21 00:22:31 - train: epoch 0063, iter [02500, 05004], lr: 0.067531, loss: 1.2897
2022-07-21 00:24:48 - train: epoch 0063, iter [02600, 05004], lr: 0.067469, loss: 1.3735
2022-07-21 00:27:05 - train: epoch 0063, iter [02700, 05004], lr: 0.067406, loss: 1.3825
2022-07-21 00:29:22 - train: epoch 0063, iter [02800, 05004], lr: 0.067344, loss: 1.3174
2022-07-21 00:31:39 - train: epoch 0063, iter [02900, 05004], lr: 0.067281, loss: 1.4941
2022-07-21 00:33:56 - train: epoch 0063, iter [03000, 05004], lr: 0.067219, loss: 1.6103
2022-07-21 00:36:13 - train: epoch 0063, iter [03100, 05004], lr: 0.067157, loss: 1.6054
2022-07-21 00:38:30 - train: epoch 0063, iter [03200, 05004], lr: 0.067094, loss: 1.3725
2022-07-21 00:40:47 - train: epoch 0063, iter [03300, 05004], lr: 0.067032, loss: 1.2337
2022-07-21 00:43:04 - train: epoch 0063, iter [03400, 05004], lr: 0.066969, loss: 1.2558
2022-07-21 00:45:21 - train: epoch 0063, iter [03500, 05004], lr: 0.066907, loss: 1.4555
2022-07-21 00:47:38 - train: epoch 0063, iter [03600, 05004], lr: 0.066845, loss: 1.2544
2022-07-21 00:49:55 - train: epoch 0063, iter [03700, 05004], lr: 0.066782, loss: 1.4340
2022-07-21 00:52:12 - train: epoch 0063, iter [03800, 05004], lr: 0.066720, loss: 1.3457
2022-07-21 00:54:29 - train: epoch 0063, iter [03900, 05004], lr: 0.066658, loss: 1.1608
2022-07-21 00:56:46 - train: epoch 0063, iter [04000, 05004], lr: 0.066595, loss: 1.1678
2022-07-21 00:59:04 - train: epoch 0063, iter [04100, 05004], lr: 0.066533, loss: 1.4725
2022-07-21 01:01:20 - train: epoch 0063, iter [04200, 05004], lr: 0.066471, loss: 1.3061
2022-07-21 01:03:38 - train: epoch 0063, iter [04300, 05004], lr: 0.066409, loss: 1.7425
2022-07-21 01:05:55 - train: epoch 0063, iter [04400, 05004], lr: 0.066346, loss: 1.2806
2022-07-21 01:08:12 - train: epoch 0063, iter [04500, 05004], lr: 0.066284, loss: 1.4208
2022-07-21 01:10:29 - train: epoch 0063, iter [04600, 05004], lr: 0.066222, loss: 1.3318
2022-07-21 01:12:46 - train: epoch 0063, iter [04700, 05004], lr: 0.066160, loss: 1.2414
2022-07-21 01:15:03 - train: epoch 0063, iter [04800, 05004], lr: 0.066097, loss: 1.4272
2022-07-21 01:17:20 - train: epoch 0063, iter [04900, 05004], lr: 0.066035, loss: 1.3443
2022-07-21 01:19:37 - train: epoch 0063, iter [05000, 05004], lr: 0.065973, loss: 1.3822
2022-07-21 01:19:43 - train: epoch 063, train_loss: 1.3326
2022-07-21 01:22:04 - eval: epoch: 063, acc1: 70.672%, acc5: 90.114%, test_loss: 1.1962, per_image_load_time: 1.370ms, per_image_inference_time: 3.934ms
2022-07-21 01:22:04 - until epoch: 063, best_acc1: 70.672%
2022-07-21 01:22:04 - epoch 064 lr: 0.065970
2022-07-21 01:24:30 - train: epoch 0064, iter [00100, 05004], lr: 0.065909, loss: 1.0287
2022-07-21 01:26:47 - train: epoch 0064, iter [00200, 05004], lr: 0.065846, loss: 1.2842
2022-07-21 01:29:04 - train: epoch 0064, iter [00300, 05004], lr: 0.065784, loss: 1.0869
2022-07-21 01:31:21 - train: epoch 0064, iter [00400, 05004], lr: 0.065722, loss: 1.3391
2022-07-21 01:33:38 - train: epoch 0064, iter [00500, 05004], lr: 0.065660, loss: 1.1565
2022-07-21 01:35:55 - train: epoch 0064, iter [00600, 05004], lr: 0.065598, loss: 1.3358
2022-07-21 01:38:12 - train: epoch 0064, iter [00700, 05004], lr: 0.065536, loss: 1.4666
2022-07-21 01:40:29 - train: epoch 0064, iter [00800, 05004], lr: 0.065474, loss: 1.1398
2022-07-21 01:42:46 - train: epoch 0064, iter [00900, 05004], lr: 0.065412, loss: 1.0983
2022-07-21 01:45:03 - train: epoch 0064, iter [01000, 05004], lr: 0.065350, loss: 1.3163
2022-07-21 01:47:20 - train: epoch 0064, iter [01100, 05004], lr: 0.065288, loss: 1.2953
2022-07-21 01:49:37 - train: epoch 0064, iter [01200, 05004], lr: 0.065226, loss: 1.1240
2022-07-21 01:51:54 - train: epoch 0064, iter [01300, 05004], lr: 0.065164, loss: 1.2254
2022-07-21 01:54:11 - train: epoch 0064, iter [01400, 05004], lr: 0.065102, loss: 1.6557
2022-07-21 01:56:28 - train: epoch 0064, iter [01500, 05004], lr: 0.065040, loss: 1.5678
2022-07-21 01:58:45 - train: epoch 0064, iter [01600, 05004], lr: 0.064978, loss: 1.1049
2022-07-21 02:01:02 - train: epoch 0064, iter [01700, 05004], lr: 0.064916, loss: 1.1998
2022-07-21 02:03:20 - train: epoch 0064, iter [01800, 05004], lr: 0.064855, loss: 1.2873
2022-07-21 02:05:37 - train: epoch 0064, iter [01900, 05004], lr: 0.064793, loss: 1.3526
2022-07-21 02:07:54 - train: epoch 0064, iter [02000, 05004], lr: 0.064731, loss: 1.2221
2022-07-21 02:10:11 - train: epoch 0064, iter [02100, 05004], lr: 0.064669, loss: 1.1984
2022-07-21 02:12:28 - train: epoch 0064, iter [02200, 05004], lr: 0.064607, loss: 1.4716
2022-07-21 02:14:45 - train: epoch 0064, iter [02300, 05004], lr: 0.064545, loss: 1.3593
2022-07-21 02:17:02 - train: epoch 0064, iter [02400, 05004], lr: 0.064484, loss: 1.1803
2022-07-21 02:19:19 - train: epoch 0064, iter [02500, 05004], lr: 0.064422, loss: 1.2823
2022-07-21 02:21:36 - train: epoch 0064, iter [02600, 05004], lr: 0.064360, loss: 1.3891
2022-07-21 02:23:53 - train: epoch 0064, iter [02700, 05004], lr: 0.064298, loss: 1.2688
2022-07-21 02:26:10 - train: epoch 0064, iter [02800, 05004], lr: 0.064237, loss: 1.3584
2022-07-21 02:28:28 - train: epoch 0064, iter [02900, 05004], lr: 0.064175, loss: 1.3536
2022-07-21 02:30:45 - train: epoch 0064, iter [03000, 05004], lr: 0.064113, loss: 1.1442
2022-07-21 02:33:02 - train: epoch 0064, iter [03100, 05004], lr: 0.064052, loss: 1.3746
2022-07-21 02:35:19 - train: epoch 0064, iter [03200, 05004], lr: 0.063990, loss: 1.4744
2022-07-21 02:37:36 - train: epoch 0064, iter [03300, 05004], lr: 0.063928, loss: 1.2498
2022-07-21 02:39:53 - train: epoch 0064, iter [03400, 05004], lr: 0.063867, loss: 1.2199
2022-07-21 02:42:10 - train: epoch 0064, iter [03500, 05004], lr: 0.063805, loss: 1.3008
2022-07-21 02:44:27 - train: epoch 0064, iter [03600, 05004], lr: 0.063743, loss: 1.3283
2022-07-21 02:46:44 - train: epoch 0064, iter [03700, 05004], lr: 0.063682, loss: 1.1537
2022-07-21 02:49:01 - train: epoch 0064, iter [03800, 05004], lr: 0.063620, loss: 1.3344
2022-07-21 02:51:18 - train: epoch 0064, iter [03900, 05004], lr: 0.063559, loss: 1.3544
2022-07-21 02:53:35 - train: epoch 0064, iter [04000, 05004], lr: 0.063497, loss: 1.0707
2022-07-21 02:55:52 - train: epoch 0064, iter [04100, 05004], lr: 0.063436, loss: 1.2527
2022-07-21 02:58:09 - train: epoch 0064, iter [04200, 05004], lr: 0.063374, loss: 1.2047
2022-07-21 03:00:27 - train: epoch 0064, iter [04300, 05004], lr: 0.063313, loss: 1.3020
2022-07-21 03:02:44 - train: epoch 0064, iter [04400, 05004], lr: 0.063251, loss: 1.2072
2022-07-21 03:05:01 - train: epoch 0064, iter [04500, 05004], lr: 0.063190, loss: 1.1565
2022-07-21 03:07:18 - train: epoch 0064, iter [04600, 05004], lr: 0.063128, loss: 1.4710
2022-07-21 03:09:35 - train: epoch 0064, iter [04700, 05004], lr: 0.063067, loss: 1.6128
2022-07-21 03:11:52 - train: epoch 0064, iter [04800, 05004], lr: 0.063005, loss: 1.4849
2022-07-21 03:14:09 - train: epoch 0064, iter [04900, 05004], lr: 0.062944, loss: 1.6873
2022-07-21 03:16:26 - train: epoch 0064, iter [05000, 05004], lr: 0.062883, loss: 1.1760
2022-07-21 03:16:33 - train: epoch 064, train_loss: 1.3152
2022-07-21 03:18:57 - eval: epoch: 064, acc1: 70.876%, acc5: 90.474%, test_loss: 1.1773, per_image_load_time: 1.593ms, per_image_inference_time: 3.907ms
2022-07-21 03:18:58 - until epoch: 064, best_acc1: 70.876%
2022-07-21 03:18:58 - epoch 065 lr: 0.062880
2022-07-21 03:21:25 - train: epoch 0065, iter [00100, 05004], lr: 0.062819, loss: 1.3124
2022-07-21 03:23:42 - train: epoch 0065, iter [00200, 05004], lr: 0.062758, loss: 1.4217
2022-07-21 03:25:59 - train: epoch 0065, iter [00300, 05004], lr: 0.062696, loss: 1.2466
2022-07-21 03:28:16 - train: epoch 0065, iter [00400, 05004], lr: 0.062635, loss: 1.3626
2022-07-21 03:30:34 - train: epoch 0065, iter [00500, 05004], lr: 0.062574, loss: 1.3167
2022-07-21 03:32:51 - train: epoch 0065, iter [00600, 05004], lr: 0.062512, loss: 1.2431
2022-07-21 03:35:08 - train: epoch 0065, iter [00700, 05004], lr: 0.062451, loss: 1.2912
2022-07-21 03:37:25 - train: epoch 0065, iter [00800, 05004], lr: 0.062390, loss: 1.3379
2022-07-21 03:39:43 - train: epoch 0065, iter [00900, 05004], lr: 0.062329, loss: 1.2777
2022-07-21 03:42:00 - train: epoch 0065, iter [01000, 05004], lr: 0.062267, loss: 1.4477
2022-07-21 03:44:17 - train: epoch 0065, iter [01100, 05004], lr: 0.062206, loss: 1.3105
2022-07-21 03:46:35 - train: epoch 0065, iter [01200, 05004], lr: 0.062145, loss: 1.3756
2022-07-21 03:48:52 - train: epoch 0065, iter [01300, 05004], lr: 0.062084, loss: 1.4801
2022-07-21 03:51:09 - train: epoch 0065, iter [01400, 05004], lr: 0.062023, loss: 1.2112
2022-07-21 03:53:26 - train: epoch 0065, iter [01500, 05004], lr: 0.061962, loss: 1.1628
2022-07-21 03:55:44 - train: epoch 0065, iter [01600, 05004], lr: 0.061901, loss: 1.3773
2022-07-21 03:58:01 - train: epoch 0065, iter [01700, 05004], lr: 0.061839, loss: 1.3005
2022-07-21 04:00:18 - train: epoch 0065, iter [01800, 05004], lr: 0.061778, loss: 1.2116
2022-07-21 04:02:35 - train: epoch 0065, iter [01900, 05004], lr: 0.061717, loss: 1.1097
2022-07-21 04:04:53 - train: epoch 0065, iter [02000, 05004], lr: 0.061656, loss: 1.3616
2022-07-21 04:07:10 - train: epoch 0065, iter [02100, 05004], lr: 0.061595, loss: 1.2904
2022-07-21 04:09:27 - train: epoch 0065, iter [02200, 05004], lr: 0.061534, loss: 1.3044
2022-07-21 04:11:44 - train: epoch 0065, iter [02300, 05004], lr: 0.061473, loss: 1.2020
2022-07-21 04:14:01 - train: epoch 0065, iter [02400, 05004], lr: 0.061412, loss: 1.1994
2022-07-21 04:16:18 - train: epoch 0065, iter [02500, 05004], lr: 0.061351, loss: 1.3429
2022-07-21 04:18:35 - train: epoch 0065, iter [02600, 05004], lr: 0.061290, loss: 1.4870
2022-07-21 04:20:52 - train: epoch 0065, iter [02700, 05004], lr: 0.061229, loss: 1.2838
2022-07-21 04:23:09 - train: epoch 0065, iter [02800, 05004], lr: 0.061169, loss: 1.4083
2022-07-21 04:25:26 - train: epoch 0065, iter [02900, 05004], lr: 0.061108, loss: 1.3578
2022-07-21 04:27:44 - train: epoch 0065, iter [03000, 05004], lr: 0.061047, loss: 1.5322
2022-07-21 04:30:01 - train: epoch 0065, iter [03100, 05004], lr: 0.060986, loss: 1.2888
2022-07-21 04:32:18 - train: epoch 0065, iter [03200, 05004], lr: 0.060925, loss: 1.3633
2022-07-21 04:34:35 - train: epoch 0065, iter [03300, 05004], lr: 0.060864, loss: 1.2612
2022-07-21 04:36:52 - train: epoch 0065, iter [03400, 05004], lr: 0.060803, loss: 1.1729
2022-07-21 04:39:10 - train: epoch 0065, iter [03500, 05004], lr: 0.060743, loss: 1.6647
2022-07-21 04:41:27 - train: epoch 0065, iter [03600, 05004], lr: 0.060682, loss: 1.3311
2022-07-21 04:43:44 - train: epoch 0065, iter [03700, 05004], lr: 0.060621, loss: 1.2544
2022-07-21 04:46:01 - train: epoch 0065, iter [03800, 05004], lr: 0.060560, loss: 1.1722
2022-07-21 04:48:19 - train: epoch 0065, iter [03900, 05004], lr: 0.060500, loss: 1.5581
2022-07-21 04:50:36 - train: epoch 0065, iter [04000, 05004], lr: 0.060439, loss: 1.3114
2022-07-21 04:52:53 - train: epoch 0065, iter [04100, 05004], lr: 0.060378, loss: 1.3135
2022-07-21 04:55:10 - train: epoch 0065, iter [04200, 05004], lr: 0.060318, loss: 1.4234
2022-07-21 04:57:28 - train: epoch 0065, iter [04300, 05004], lr: 0.060257, loss: 1.3758
2022-07-21 04:59:45 - train: epoch 0065, iter [04400, 05004], lr: 0.060196, loss: 1.3264
2022-07-21 05:02:02 - train: epoch 0065, iter [04500, 05004], lr: 0.060136, loss: 1.4089
2022-07-21 05:04:19 - train: epoch 0065, iter [04600, 05004], lr: 0.060075, loss: 1.4571
2022-07-21 05:06:37 - train: epoch 0065, iter [04700, 05004], lr: 0.060015, loss: 1.3742
2022-07-21 05:08:54 - train: epoch 0065, iter [04800, 05004], lr: 0.059954, loss: 1.0944
2022-07-21 05:11:11 - train: epoch 0065, iter [04900, 05004], lr: 0.059893, loss: 1.4584
2022-07-21 05:13:28 - train: epoch 0065, iter [05000, 05004], lr: 0.059833, loss: 1.4169
2022-07-21 05:13:35 - train: epoch 065, train_loss: 1.2984
2022-07-21 05:15:58 - eval: epoch: 065, acc1: 70.990%, acc5: 90.380%, test_loss: 1.1819, per_image_load_time: 1.510ms, per_image_inference_time: 3.921ms
2022-07-21 05:15:59 - until epoch: 065, best_acc1: 70.990%
2022-07-21 05:15:59 - epoch 066 lr: 0.059830
2022-07-21 05:18:25 - train: epoch 0066, iter [00100, 05004], lr: 0.059770, loss: 1.0701
2022-07-21 05:20:42 - train: epoch 0066, iter [00200, 05004], lr: 0.059709, loss: 1.4439
2022-07-21 05:22:59 - train: epoch 0066, iter [00300, 05004], lr: 0.059649, loss: 1.1899
2022-07-21 05:25:16 - train: epoch 0066, iter [00400, 05004], lr: 0.059589, loss: 0.9365
2022-07-21 05:27:33 - train: epoch 0066, iter [00500, 05004], lr: 0.059528, loss: 1.4254
2022-07-21 05:29:50 - train: epoch 0066, iter [00600, 05004], lr: 0.059468, loss: 1.0343
2022-07-21 05:32:07 - train: epoch 0066, iter [00700, 05004], lr: 0.059407, loss: 1.2545
2022-07-21 05:34:24 - train: epoch 0066, iter [00800, 05004], lr: 0.059347, loss: 1.3157
2022-07-21 05:36:42 - train: epoch 0066, iter [00900, 05004], lr: 0.059286, loss: 1.2393
2022-07-21 05:38:59 - train: epoch 0066, iter [01000, 05004], lr: 0.059226, loss: 1.3984
2022-07-21 05:41:16 - train: epoch 0066, iter [01100, 05004], lr: 0.059166, loss: 1.3066
2022-07-21 05:43:33 - train: epoch 0066, iter [01200, 05004], lr: 0.059105, loss: 1.3228
2022-07-21 05:45:50 - train: epoch 0066, iter [01300, 05004], lr: 0.059045, loss: 1.4194
2022-07-21 05:48:08 - train: epoch 0066, iter [01400, 05004], lr: 0.058985, loss: 1.1830
2022-07-21 05:50:25 - train: epoch 0066, iter [01500, 05004], lr: 0.058925, loss: 1.3808
2022-07-21 05:52:42 - train: epoch 0066, iter [01600, 05004], lr: 0.058864, loss: 1.4029
2022-07-21 05:55:00 - train: epoch 0066, iter [01700, 05004], lr: 0.058804, loss: 1.3014
2022-07-21 05:57:17 - train: epoch 0066, iter [01800, 05004], lr: 0.058744, loss: 1.2345
2022-07-21 05:59:34 - train: epoch 0066, iter [01900, 05004], lr: 0.058684, loss: 1.5806
2022-07-21 06:01:52 - train: epoch 0066, iter [02000, 05004], lr: 0.058624, loss: 1.2794
2022-07-21 06:04:09 - train: epoch 0066, iter [02100, 05004], lr: 0.058563, loss: 1.2881
2022-07-21 06:06:26 - train: epoch 0066, iter [02200, 05004], lr: 0.058503, loss: 1.1824
2022-07-21 06:08:43 - train: epoch 0066, iter [02300, 05004], lr: 0.058443, loss: 1.4756
2022-07-21 06:11:01 - train: epoch 0066, iter [02400, 05004], lr: 0.058383, loss: 1.2820
2022-07-21 06:13:18 - train: epoch 0066, iter [02500, 05004], lr: 0.058323, loss: 1.2092
2022-07-21 06:15:35 - train: epoch 0066, iter [02600, 05004], lr: 0.058263, loss: 1.3022
2022-07-21 06:17:52 - train: epoch 0066, iter [02700, 05004], lr: 0.058203, loss: 1.3672
2022-07-21 06:20:09 - train: epoch 0066, iter [02800, 05004], lr: 0.058143, loss: 1.2465
2022-07-21 06:22:26 - train: epoch 0066, iter [02900, 05004], lr: 0.058083, loss: 1.4298
2022-07-21 06:24:44 - train: epoch 0066, iter [03000, 05004], lr: 0.058023, loss: 1.1765
2022-07-21 06:27:01 - train: epoch 0066, iter [03100, 05004], lr: 0.057963, loss: 1.5344
2022-07-21 06:29:18 - train: epoch 0066, iter [03200, 05004], lr: 0.057903, loss: 1.2408
2022-07-21 06:31:35 - train: epoch 0066, iter [03300, 05004], lr: 0.057843, loss: 1.1691
2022-07-21 06:33:52 - train: epoch 0066, iter [03400, 05004], lr: 0.057783, loss: 1.4243
2022-07-21 06:36:10 - train: epoch 0066, iter [03500, 05004], lr: 0.057723, loss: 1.2871
2022-07-21 06:38:27 - train: epoch 0066, iter [03600, 05004], lr: 0.057663, loss: 1.3304
2022-07-21 06:40:44 - train: epoch 0066, iter [03700, 05004], lr: 0.057603, loss: 1.4036
2022-07-21 06:43:01 - train: epoch 0066, iter [03800, 05004], lr: 0.057544, loss: 1.3796
2022-07-21 06:45:19 - train: epoch 0066, iter [03900, 05004], lr: 0.057484, loss: 1.2483
2022-07-21 06:47:36 - train: epoch 0066, iter [04000, 05004], lr: 0.057424, loss: 1.4165
2022-07-21 06:49:53 - train: epoch 0066, iter [04100, 05004], lr: 0.057364, loss: 1.4356
2022-07-21 06:52:10 - train: epoch 0066, iter [04200, 05004], lr: 0.057304, loss: 1.1531
2022-07-21 06:54:27 - train: epoch 0066, iter [04300, 05004], lr: 0.057245, loss: 1.1611
2022-07-21 06:56:45 - train: epoch 0066, iter [04400, 05004], lr: 0.057185, loss: 1.2179
2022-07-21 06:59:02 - train: epoch 0066, iter [04500, 05004], lr: 0.057125, loss: 1.5112
2022-07-21 07:01:19 - train: epoch 0066, iter [04600, 05004], lr: 0.057066, loss: 1.3899
2022-07-21 07:03:36 - train: epoch 0066, iter [04700, 05004], lr: 0.057006, loss: 1.0773
2022-07-21 07:05:53 - train: epoch 0066, iter [04800, 05004], lr: 0.056946, loss: 1.3986
2022-07-21 07:08:11 - train: epoch 0066, iter [04900, 05004], lr: 0.056887, loss: 1.2012
2022-07-21 07:10:28 - train: epoch 0066, iter [05000, 05004], lr: 0.056827, loss: 1.2237
2022-07-21 07:10:35 - train: epoch 066, train_loss: 1.2795
2022-07-21 07:12:56 - eval: epoch: 066, acc1: 71.406%, acc5: 90.864%, test_loss: 1.1507, per_image_load_time: 1.423ms, per_image_inference_time: 3.902ms
2022-07-21 07:12:56 - until epoch: 066, best_acc1: 71.406%
2022-07-21 07:12:56 - epoch 067 lr: 0.056824
2022-07-21 07:15:22 - train: epoch 0067, iter [00100, 05004], lr: 0.056765, loss: 1.2531
2022-07-21 07:17:39 - train: epoch 0067, iter [00200, 05004], lr: 0.056705, loss: 1.2685
2022-07-21 07:19:56 - train: epoch 0067, iter [00300, 05004], lr: 0.056646, loss: 1.2623
2022-07-21 07:22:13 - train: epoch 0067, iter [00400, 05004], lr: 0.056586, loss: 1.1983
2022-07-21 07:24:30 - train: epoch 0067, iter [00500, 05004], lr: 0.056527, loss: 1.2445
2022-07-21 07:26:47 - train: epoch 0067, iter [00600, 05004], lr: 0.056467, loss: 1.1419
2022-07-21 07:29:04 - train: epoch 0067, iter [00700, 05004], lr: 0.056408, loss: 1.2767
2022-07-21 07:31:21 - train: epoch 0067, iter [00800, 05004], lr: 0.056348, loss: 1.1168
2022-07-21 07:33:38 - train: epoch 0067, iter [00900, 05004], lr: 0.056289, loss: 1.3002
2022-07-21 07:35:55 - train: epoch 0067, iter [01000, 05004], lr: 0.056229, loss: 1.0476
2022-07-21 07:38:12 - train: epoch 0067, iter [01100, 05004], lr: 0.056170, loss: 1.1892
2022-07-21 07:40:29 - train: epoch 0067, iter [01200, 05004], lr: 0.056111, loss: 1.3117
2022-07-21 07:42:46 - train: epoch 0067, iter [01300, 05004], lr: 0.056051, loss: 1.2390
2022-07-21 07:45:03 - train: epoch 0067, iter [01400, 05004], lr: 0.055992, loss: 1.4227
2022-07-21 07:47:20 - train: epoch 0067, iter [01500, 05004], lr: 0.055933, loss: 1.1324
2022-07-21 07:49:37 - train: epoch 0067, iter [01600, 05004], lr: 0.055873, loss: 1.2882
2022-07-21 07:51:54 - train: epoch 0067, iter [01700, 05004], lr: 0.055814, loss: 1.1567
2022-07-21 07:54:11 - train: epoch 0067, iter [01800, 05004], lr: 0.055755, loss: 1.4432
2022-07-21 07:56:28 - train: epoch 0067, iter [01900, 05004], lr: 0.055695, loss: 1.2937
2022-07-21 07:58:45 - train: epoch 0067, iter [02000, 05004], lr: 0.055636, loss: 1.2728
2022-07-21 08:01:02 - train: epoch 0067, iter [02100, 05004], lr: 0.055577, loss: 1.2335
2022-07-21 08:03:19 - train: epoch 0067, iter [02200, 05004], lr: 0.055518, loss: 1.2346
2022-07-21 08:05:36 - train: epoch 0067, iter [02300, 05004], lr: 0.055459, loss: 1.3470
2022-07-21 08:07:53 - train: epoch 0067, iter [02400, 05004], lr: 0.055399, loss: 1.2266
2022-07-21 08:10:11 - train: epoch 0067, iter [02500, 05004], lr: 0.055340, loss: 1.1836
2022-07-21 08:12:28 - train: epoch 0067, iter [02600, 05004], lr: 0.055281, loss: 1.2102
2022-07-21 08:14:45 - train: epoch 0067, iter [02700, 05004], lr: 0.055222, loss: 1.2433
2022-07-21 08:17:02 - train: epoch 0067, iter [02800, 05004], lr: 0.055163, loss: 1.4952
2022-07-21 08:19:19 - train: epoch 0067, iter [02900, 05004], lr: 0.055104, loss: 1.2746
2022-07-21 08:21:36 - train: epoch 0067, iter [03000, 05004], lr: 0.055045, loss: 1.0840
2022-07-21 08:23:53 - train: epoch 0067, iter [03100, 05004], lr: 0.054986, loss: 1.1906
2022-07-21 08:26:11 - train: epoch 0067, iter [03200, 05004], lr: 0.054927, loss: 1.3472
2022-07-21 08:28:28 - train: epoch 0067, iter [03300, 05004], lr: 0.054868, loss: 1.1709
2022-07-21 08:30:45 - train: epoch 0067, iter [03400, 05004], lr: 0.054809, loss: 1.3368
2022-07-21 08:33:02 - train: epoch 0067, iter [03500, 05004], lr: 0.054750, loss: 1.0214
2022-07-21 08:35:19 - train: epoch 0067, iter [03600, 05004], lr: 0.054691, loss: 1.4496
2022-07-21 08:37:36 - train: epoch 0067, iter [03700, 05004], lr: 0.054632, loss: 1.3327
2022-07-21 08:39:53 - train: epoch 0067, iter [03800, 05004], lr: 0.054573, loss: 1.0414
2022-07-21 08:42:10 - train: epoch 0067, iter [03900, 05004], lr: 0.054514, loss: 1.4956
2022-07-21 08:44:27 - train: epoch 0067, iter [04000, 05004], lr: 0.054456, loss: 1.1959
2022-07-21 08:46:45 - train: epoch 0067, iter [04100, 05004], lr: 0.054397, loss: 1.2503
2022-07-21 08:49:02 - train: epoch 0067, iter [04200, 05004], lr: 0.054338, loss: 1.3985
2022-07-21 08:51:19 - train: epoch 0067, iter [04300, 05004], lr: 0.054279, loss: 1.2494
2022-07-21 08:53:36 - train: epoch 0067, iter [04400, 05004], lr: 0.054220, loss: 1.2118
2022-07-21 08:55:53 - train: epoch 0067, iter [04500, 05004], lr: 0.054162, loss: 1.3369
2022-07-21 08:58:10 - train: epoch 0067, iter [04600, 05004], lr: 0.054103, loss: 1.0469
2022-07-21 09:00:28 - train: epoch 0067, iter [04700, 05004], lr: 0.054044, loss: 1.3987
2022-07-21 09:02:45 - train: epoch 0067, iter [04800, 05004], lr: 0.053986, loss: 1.1419
2022-07-21 09:05:02 - train: epoch 0067, iter [04900, 05004], lr: 0.053927, loss: 1.1070
2022-07-21 09:07:19 - train: epoch 0067, iter [05000, 05004], lr: 0.053868, loss: 1.3924
2022-07-21 09:07:25 - train: epoch 067, train_loss: 1.2591
2022-07-21 09:09:41 - eval: epoch: 067, acc1: 71.750%, acc5: 90.842%, test_loss: 1.1463, per_image_load_time: 1.331ms, per_image_inference_time: 3.911ms
2022-07-21 09:09:41 - until epoch: 067, best_acc1: 71.750%
2022-07-21 09:09:41 - epoch 068 lr: 0.053865
2022-07-21 09:12:06 - train: epoch 0068, iter [00100, 05004], lr: 0.053807, loss: 1.2621
2022-07-21 09:14:23 - train: epoch 0068, iter [00200, 05004], lr: 0.053749, loss: 1.3546
2022-07-21 09:16:40 - train: epoch 0068, iter [00300, 05004], lr: 0.053690, loss: 1.1867
2022-07-21 09:18:56 - train: epoch 0068, iter [00400, 05004], lr: 0.053632, loss: 1.1689
2022-07-21 09:21:13 - train: epoch 0068, iter [00500, 05004], lr: 0.053573, loss: 1.0785
2022-07-21 09:23:30 - train: epoch 0068, iter [00600, 05004], lr: 0.053514, loss: 1.2589
2022-07-21 09:25:47 - train: epoch 0068, iter [00700, 05004], lr: 0.053456, loss: 1.1835
2022-07-21 09:28:04 - train: epoch 0068, iter [00800, 05004], lr: 0.053397, loss: 1.0807
2022-07-21 09:30:21 - train: epoch 0068, iter [00900, 05004], lr: 0.053339, loss: 1.2117
2022-07-21 09:32:38 - train: epoch 0068, iter [01000, 05004], lr: 0.053281, loss: 1.3711
2022-07-21 09:34:55 - train: epoch 0068, iter [01100, 05004], lr: 0.053222, loss: 1.3098
2022-07-21 09:37:12 - train: epoch 0068, iter [01200, 05004], lr: 0.053164, loss: 1.2879
2022-07-21 09:39:29 - train: epoch 0068, iter [01300, 05004], lr: 0.053105, loss: 1.1273
2022-07-21 09:41:46 - train: epoch 0068, iter [01400, 05004], lr: 0.053047, loss: 1.0633
2022-07-21 09:44:03 - train: epoch 0068, iter [01500, 05004], lr: 0.052989, loss: 1.4537
2022-07-21 09:46:20 - train: epoch 0068, iter [01600, 05004], lr: 0.052930, loss: 1.2936
2022-07-21 09:48:37 - train: epoch 0068, iter [01700, 05004], lr: 0.052872, loss: 1.2042
2022-07-21 09:50:54 - train: epoch 0068, iter [01800, 05004], lr: 0.052814, loss: 1.2548
2022-07-21 09:53:11 - train: epoch 0068, iter [01900, 05004], lr: 0.052756, loss: 1.1681
2022-07-21 09:55:28 - train: epoch 0068, iter [02000, 05004], lr: 0.052697, loss: 1.3364
2022-07-21 09:57:45 - train: epoch 0068, iter [02100, 05004], lr: 0.052639, loss: 1.2531
2022-07-21 10:00:02 - train: epoch 0068, iter [02200, 05004], lr: 0.052581, loss: 1.1232
2022-07-21 10:02:19 - train: epoch 0068, iter [02300, 05004], lr: 0.052523, loss: 1.1945
2022-07-21 10:04:36 - train: epoch 0068, iter [02400, 05004], lr: 0.052465, loss: 1.2145
2022-07-21 10:06:53 - train: epoch 0068, iter [02500, 05004], lr: 0.052406, loss: 1.2721
2022-07-21 10:09:11 - train: epoch 0068, iter [02600, 05004], lr: 0.052348, loss: 1.0862
2022-07-21 10:11:28 - train: epoch 0068, iter [02700, 05004], lr: 0.052290, loss: 1.0392
2022-07-21 10:13:45 - train: epoch 0068, iter [02800, 05004], lr: 0.052232, loss: 1.3248
2022-07-21 10:16:02 - train: epoch 0068, iter [02900, 05004], lr: 0.052174, loss: 1.2606
2022-07-21 10:18:19 - train: epoch 0068, iter [03000, 05004], lr: 0.052116, loss: 1.2916
2022-07-21 10:20:36 - train: epoch 0068, iter [03100, 05004], lr: 0.052058, loss: 1.2353
2022-07-21 10:22:53 - train: epoch 0068, iter [03200, 05004], lr: 0.052000, loss: 1.1939
2022-07-21 10:25:10 - train: epoch 0068, iter [03300, 05004], lr: 0.051942, loss: 1.2932
2022-07-21 10:27:27 - train: epoch 0068, iter [03400, 05004], lr: 0.051884, loss: 1.1060
2022-07-21 10:29:43 - train: epoch 0068, iter [03500, 05004], lr: 0.051826, loss: 1.2396
2022-07-21 10:32:00 - train: epoch 0068, iter [03600, 05004], lr: 0.051768, loss: 1.2183
2022-07-21 10:34:17 - train: epoch 0068, iter [03700, 05004], lr: 0.051710, loss: 1.2422
2022-07-21 10:36:34 - train: epoch 0068, iter [03800, 05004], lr: 0.051653, loss: 1.3866
2022-07-21 10:38:51 - train: epoch 0068, iter [03900, 05004], lr: 0.051595, loss: 1.3628
2022-07-21 10:41:08 - train: epoch 0068, iter [04000, 05004], lr: 0.051537, loss: 1.2496
2022-07-21 10:43:26 - train: epoch 0068, iter [04100, 05004], lr: 0.051479, loss: 1.3232
2022-07-21 10:45:43 - train: epoch 0068, iter [04200, 05004], lr: 0.051421, loss: 1.2671
2022-07-21 10:47:59 - train: epoch 0068, iter [04300, 05004], lr: 0.051364, loss: 1.1860
2022-07-21 10:50:17 - train: epoch 0068, iter [04400, 05004], lr: 0.051306, loss: 1.2410
2022-07-21 10:52:34 - train: epoch 0068, iter [04500, 05004], lr: 0.051248, loss: 1.3271
2022-07-21 10:54:51 - train: epoch 0068, iter [04600, 05004], lr: 0.051190, loss: 1.2895
2022-07-21 10:57:08 - train: epoch 0068, iter [04700, 05004], lr: 0.051133, loss: 1.4765
2022-07-21 10:59:25 - train: epoch 0068, iter [04800, 05004], lr: 0.051075, loss: 1.3106
2022-07-21 11:01:41 - train: epoch 0068, iter [04900, 05004], lr: 0.051018, loss: 1.3196
2022-07-21 11:03:58 - train: epoch 0068, iter [05000, 05004], lr: 0.050960, loss: 1.1873
2022-07-21 11:04:05 - train: epoch 068, train_loss: 1.2388
2022-07-21 11:06:14 - eval: epoch: 068, acc1: 71.834%, acc5: 90.958%, test_loss: 1.1434, per_image_load_time: 1.096ms, per_image_inference_time: 3.913ms
2022-07-21 11:06:15 - until epoch: 068, best_acc1: 71.834%
2022-07-21 11:06:15 - epoch 069 lr: 0.050957
2022-07-21 11:08:39 - train: epoch 0069, iter [00100, 05004], lr: 0.050900, loss: 1.2781
2022-07-21 11:10:56 - train: epoch 0069, iter [00200, 05004], lr: 0.050843, loss: 1.1388
2022-07-21 11:13:13 - train: epoch 0069, iter [00300, 05004], lr: 0.050785, loss: 1.1617
2022-07-21 11:15:30 - train: epoch 0069, iter [00400, 05004], lr: 0.050727, loss: 1.3421
2022-07-21 11:17:46 - train: epoch 0069, iter [00500, 05004], lr: 0.050670, loss: 1.2777
2022-07-21 11:20:03 - train: epoch 0069, iter [00600, 05004], lr: 0.050612, loss: 1.0931
2022-07-21 11:22:20 - train: epoch 0069, iter [00700, 05004], lr: 0.050555, loss: 1.1984
2022-07-21 11:24:37 - train: epoch 0069, iter [00800, 05004], lr: 0.050498, loss: 1.3162
2022-07-21 11:26:53 - train: epoch 0069, iter [00900, 05004], lr: 0.050440, loss: 1.0272
2022-07-21 11:29:10 - train: epoch 0069, iter [01000, 05004], lr: 0.050383, loss: 1.1469
2022-07-21 11:31:27 - train: epoch 0069, iter [01100, 05004], lr: 0.050325, loss: 1.2942
2022-07-21 11:33:44 - train: epoch 0069, iter [01200, 05004], lr: 0.050268, loss: 1.2707
2022-07-21 11:36:00 - train: epoch 0069, iter [01300, 05004], lr: 0.050211, loss: 1.1849
2022-07-21 11:38:17 - train: epoch 0069, iter [01400, 05004], lr: 0.050153, loss: 1.2448
2022-07-21 11:40:34 - train: epoch 0069, iter [01500, 05004], lr: 0.050096, loss: 1.2772
2022-07-21 11:42:50 - train: epoch 0069, iter [01600, 05004], lr: 0.050039, loss: 1.4233
2022-07-21 11:45:07 - train: epoch 0069, iter [01700, 05004], lr: 0.049982, loss: 1.0983
2022-07-21 11:47:24 - train: epoch 0069, iter [01800, 05004], lr: 0.049924, loss: 1.1943
2022-07-21 11:49:40 - train: epoch 0069, iter [01900, 05004], lr: 0.049867, loss: 1.2078
2022-07-21 11:51:57 - train: epoch 0069, iter [02000, 05004], lr: 0.049810, loss: 1.2171
2022-07-21 11:54:14 - train: epoch 0069, iter [02100, 05004], lr: 0.049753, loss: 1.2835
2022-07-21 11:56:31 - train: epoch 0069, iter [02200, 05004], lr: 0.049696, loss: 1.2761
2022-07-21 11:58:47 - train: epoch 0069, iter [02300, 05004], lr: 0.049639, loss: 1.0438
2022-07-21 12:01:04 - train: epoch 0069, iter [02400, 05004], lr: 0.049582, loss: 1.3984
2022-07-21 12:03:20 - train: epoch 0069, iter [02500, 05004], lr: 0.049525, loss: 0.9552
2022-07-21 12:05:37 - train: epoch 0069, iter [02600, 05004], lr: 0.049468, loss: 1.2103
2022-07-21 12:07:54 - train: epoch 0069, iter [02700, 05004], lr: 0.049411, loss: 1.4579
2022-07-21 12:10:10 - train: epoch 0069, iter [02800, 05004], lr: 0.049354, loss: 1.2587
2022-07-21 12:12:27 - train: epoch 0069, iter [02900, 05004], lr: 0.049297, loss: 1.3114
2022-07-21 12:14:43 - train: epoch 0069, iter [03000, 05004], lr: 0.049240, loss: 1.2120
2022-07-21 12:17:00 - train: epoch 0069, iter [03100, 05004], lr: 0.049183, loss: 1.1489
2022-07-21 12:19:17 - train: epoch 0069, iter [03200, 05004], lr: 0.049126, loss: 1.2751
2022-07-21 12:21:33 - train: epoch 0069, iter [03300, 05004], lr: 0.049069, loss: 1.2880
2022-07-21 12:23:50 - train: epoch 0069, iter [03400, 05004], lr: 0.049012, loss: 1.2259
2022-07-21 12:26:07 - train: epoch 0069, iter [03500, 05004], lr: 0.048955, loss: 1.0268
2022-07-21 12:28:23 - train: epoch 0069, iter [03600, 05004], lr: 0.048898, loss: 1.2194
2022-07-21 12:30:40 - train: epoch 0069, iter [03700, 05004], lr: 0.048842, loss: 1.3344
2022-07-21 12:32:57 - train: epoch 0069, iter [03800, 05004], lr: 0.048785, loss: 1.1375
2022-07-21 12:35:13 - train: epoch 0069, iter [03900, 05004], lr: 0.048728, loss: 1.2814
2022-07-21 12:37:30 - train: epoch 0069, iter [04000, 05004], lr: 0.048671, loss: 1.4008
2022-07-21 12:39:47 - train: epoch 0069, iter [04100, 05004], lr: 0.048615, loss: 1.2628
2022-07-21 12:42:03 - train: epoch 0069, iter [04200, 05004], lr: 0.048558, loss: 1.1643
2022-07-21 12:44:20 - train: epoch 0069, iter [04300, 05004], lr: 0.048501, loss: 1.2660
2022-07-21 12:46:37 - train: epoch 0069, iter [04400, 05004], lr: 0.048445, loss: 1.1922
2022-07-21 12:48:54 - train: epoch 0069, iter [04500, 05004], lr: 0.048388, loss: 1.1402
2022-07-21 12:51:10 - train: epoch 0069, iter [04600, 05004], lr: 0.048331, loss: 1.3450
2022-07-21 12:53:27 - train: epoch 0069, iter [04700, 05004], lr: 0.048275, loss: 1.2469
2022-07-21 12:55:44 - train: epoch 0069, iter [04800, 05004], lr: 0.048218, loss: 1.2296
2022-07-21 12:58:00 - train: epoch 0069, iter [04900, 05004], lr: 0.048162, loss: 1.3647
2022-07-21 13:00:17 - train: epoch 0069, iter [05000, 05004], lr: 0.048105, loss: 1.2676
2022-07-21 13:00:23 - train: epoch 069, train_loss: 1.2204
2022-07-21 13:02:37 - eval: epoch: 069, acc1: 72.318%, acc5: 91.256%, test_loss: 1.1155, per_image_load_time: 1.167ms, per_image_inference_time: 3.912ms
2022-07-21 13:02:38 - until epoch: 069, best_acc1: 72.318%
2022-07-21 13:02:38 - epoch 070 lr: 0.048102
2022-07-21 13:05:02 - train: epoch 0070, iter [00100, 05004], lr: 0.048047, loss: 1.1205
2022-07-21 13:07:18 - train: epoch 0070, iter [00200, 05004], lr: 0.047990, loss: 1.0669
2022-07-21 13:09:35 - train: epoch 0070, iter [00300, 05004], lr: 0.047934, loss: 1.4383
2022-07-21 13:11:51 - train: epoch 0070, iter [00400, 05004], lr: 0.047877, loss: 1.1616
2022-07-21 13:14:08 - train: epoch 0070, iter [00500, 05004], lr: 0.047821, loss: 1.2289
2022-07-21 13:16:25 - train: epoch 0070, iter [00600, 05004], lr: 0.047765, loss: 1.1485
2022-07-21 13:18:41 - train: epoch 0070, iter [00700, 05004], lr: 0.047708, loss: 1.1877
2022-07-21 13:20:58 - train: epoch 0070, iter [00800, 05004], lr: 0.047652, loss: 1.3320
2022-07-21 13:23:15 - train: epoch 0070, iter [00900, 05004], lr: 0.047596, loss: 1.2626
2022-07-21 13:25:31 - train: epoch 0070, iter [01000, 05004], lr: 0.047539, loss: 1.1515
2022-07-21 13:27:48 - train: epoch 0070, iter [01100, 05004], lr: 0.047483, loss: 1.3476
2022-07-21 13:30:04 - train: epoch 0070, iter [01200, 05004], lr: 0.047427, loss: 1.3003
2022-07-21 13:32:21 - train: epoch 0070, iter [01300, 05004], lr: 0.047371, loss: 1.0792
2022-07-21 13:34:38 - train: epoch 0070, iter [01400, 05004], lr: 0.047314, loss: 1.2770
2022-07-21 13:36:54 - train: epoch 0070, iter [01500, 05004], lr: 0.047258, loss: 1.2437
2022-07-21 13:39:11 - train: epoch 0070, iter [01600, 05004], lr: 0.047202, loss: 1.2186
2022-07-21 13:41:28 - train: epoch 0070, iter [01700, 05004], lr: 0.047146, loss: 1.2738
2022-07-21 13:43:44 - train: epoch 0070, iter [01800, 05004], lr: 0.047090, loss: 1.1206
2022-07-21 13:46:01 - train: epoch 0070, iter [01900, 05004], lr: 0.047034, loss: 1.1211
2022-07-21 13:48:18 - train: epoch 0070, iter [02000, 05004], lr: 0.046978, loss: 1.1784
2022-07-21 13:50:34 - train: epoch 0070, iter [02100, 05004], lr: 0.046922, loss: 1.2531
2022-07-21 13:52:51 - train: epoch 0070, iter [02200, 05004], lr: 0.046866, loss: 1.2377
2022-07-21 13:55:08 - train: epoch 0070, iter [02300, 05004], lr: 0.046810, loss: 1.2491
2022-07-21 13:57:24 - train: epoch 0070, iter [02400, 05004], lr: 0.046754, loss: 1.1695
2022-07-21 13:59:41 - train: epoch 0070, iter [02500, 05004], lr: 0.046698, loss: 1.1880
2022-07-21 14:01:58 - train: epoch 0070, iter [02600, 05004], lr: 0.046642, loss: 1.1573
2022-07-21 14:04:14 - train: epoch 0070, iter [02700, 05004], lr: 0.046586, loss: 1.1047
2022-07-21 14:06:31 - train: epoch 0070, iter [02800, 05004], lr: 0.046530, loss: 1.4501
2022-07-21 14:08:48 - train: epoch 0070, iter [02900, 05004], lr: 0.046474, loss: 1.1704
2022-07-21 14:11:04 - train: epoch 0070, iter [03000, 05004], lr: 0.046419, loss: 1.0680
2022-07-21 14:13:21 - train: epoch 0070, iter [03100, 05004], lr: 0.046363, loss: 1.0798
2022-07-21 14:15:37 - train: epoch 0070, iter [03200, 05004], lr: 0.046307, loss: 1.2876
2022-07-21 14:17:54 - train: epoch 0070, iter [03300, 05004], lr: 0.046251, loss: 1.1178
2022-07-21 14:20:11 - train: epoch 0070, iter [03400, 05004], lr: 0.046196, loss: 1.0585
2022-07-21 14:22:27 - train: epoch 0070, iter [03500, 05004], lr: 0.046140, loss: 1.1822
2022-07-21 14:24:44 - train: epoch 0070, iter [03600, 05004], lr: 0.046084, loss: 1.2424
2022-07-21 14:27:01 - train: epoch 0070, iter [03700, 05004], lr: 0.046029, loss: 1.1991
2022-07-21 14:29:18 - train: epoch 0070, iter [03800, 05004], lr: 0.045973, loss: 1.2264
2022-07-21 14:31:34 - train: epoch 0070, iter [03900, 05004], lr: 0.045917, loss: 1.0573
2022-07-21 14:33:51 - train: epoch 0070, iter [04000, 05004], lr: 0.045862, loss: 1.1916
2022-07-21 14:36:08 - train: epoch 0070, iter [04100, 05004], lr: 0.045806, loss: 1.1272
2022-07-21 14:38:25 - train: epoch 0070, iter [04200, 05004], lr: 0.045751, loss: 1.3808
2022-07-21 14:40:41 - train: epoch 0070, iter [04300, 05004], lr: 0.045695, loss: 1.1210
2022-07-21 14:42:58 - train: epoch 0070, iter [04400, 05004], lr: 0.045640, loss: 1.1086
2022-07-21 14:45:15 - train: epoch 0070, iter [04500, 05004], lr: 0.045584, loss: 1.2786
2022-07-21 14:47:32 - train: epoch 0070, iter [04600, 05004], lr: 0.045529, loss: 1.2961
2022-07-21 14:49:48 - train: epoch 0070, iter [04700, 05004], lr: 0.045473, loss: 1.2392
2022-07-21 14:52:05 - train: epoch 0070, iter [04800, 05004], lr: 0.045418, loss: 1.1701
2022-07-21 14:54:22 - train: epoch 0070, iter [04900, 05004], lr: 0.045363, loss: 1.1261
2022-07-21 14:56:39 - train: epoch 0070, iter [05000, 05004], lr: 0.045307, loss: 1.2304
2022-07-21 14:56:45 - train: epoch 070, train_loss: 1.1987
2022-07-21 14:58:52 - eval: epoch: 070, acc1: 72.380%, acc5: 91.230%, test_loss: 1.1240, per_image_load_time: 1.013ms, per_image_inference_time: 3.899ms
2022-07-21 14:58:53 - until epoch: 070, best_acc1: 72.380%
2022-07-21 14:58:53 - epoch 071 lr: 0.045305
2022-07-21 15:01:18 - train: epoch 0071, iter [00100, 05004], lr: 0.045250, loss: 1.1803
2022-07-21 15:03:35 - train: epoch 0071, iter [00200, 05004], lr: 0.045195, loss: 0.9894
2022-07-21 15:05:52 - train: epoch 0071, iter [00300, 05004], lr: 0.045139, loss: 1.2023
2022-07-21 15:08:09 - train: epoch 0071, iter [00400, 05004], lr: 0.045084, loss: 1.3159
2022-07-21 15:10:25 - train: epoch 0071, iter [00500, 05004], lr: 0.045029, loss: 0.9968
2022-07-21 15:12:42 - train: epoch 0071, iter [00600, 05004], lr: 0.044974, loss: 1.1404
2022-07-21 15:14:59 - train: epoch 0071, iter [00700, 05004], lr: 0.044918, loss: 0.9391
2022-07-21 15:17:16 - train: epoch 0071, iter [00800, 05004], lr: 0.044863, loss: 1.0886
2022-07-21 15:19:32 - train: epoch 0071, iter [00900, 05004], lr: 0.044808, loss: 1.2248
2022-07-21 15:21:49 - train: epoch 0071, iter [01000, 05004], lr: 0.044753, loss: 1.2206
2022-07-21 15:24:06 - train: epoch 0071, iter [01100, 05004], lr: 0.044698, loss: 1.2512
2022-07-21 15:26:23 - train: epoch 0071, iter [01200, 05004], lr: 0.044643, loss: 1.4069
2022-07-21 15:28:39 - train: epoch 0071, iter [01300, 05004], lr: 0.044588, loss: 1.2547
2022-07-21 15:30:56 - train: epoch 0071, iter [01400, 05004], lr: 0.044533, loss: 1.1414
2022-07-21 15:33:13 - train: epoch 0071, iter [01500, 05004], lr: 0.044478, loss: 0.9811
2022-07-21 15:35:30 - train: epoch 0071, iter [01600, 05004], lr: 0.044423, loss: 1.1288
2022-07-21 15:37:46 - train: epoch 0071, iter [01700, 05004], lr: 0.044368, loss: 1.2331
2022-07-21 15:40:03 - train: epoch 0071, iter [01800, 05004], lr: 0.044313, loss: 1.3027
2022-07-21 15:42:20 - train: epoch 0071, iter [01900, 05004], lr: 0.044258, loss: 1.0092
2022-07-21 15:44:36 - train: epoch 0071, iter [02000, 05004], lr: 0.044203, loss: 1.2842
2022-07-21 15:46:53 - train: epoch 0071, iter [02100, 05004], lr: 0.044149, loss: 0.9814
2022-07-21 15:49:10 - train: epoch 0071, iter [02200, 05004], lr: 0.044094, loss: 0.9887
2022-07-21 15:51:27 - train: epoch 0071, iter [02300, 05004], lr: 0.044039, loss: 0.9980
2022-07-21 15:53:44 - train: epoch 0071, iter [02400, 05004], lr: 0.043984, loss: 1.1516
2022-07-21 15:56:00 - train: epoch 0071, iter [02500, 05004], lr: 0.043930, loss: 1.1788
2022-07-21 15:58:17 - train: epoch 0071, iter [02600, 05004], lr: 0.043875, loss: 1.0884
2022-07-21 16:00:34 - train: epoch 0071, iter [02700, 05004], lr: 0.043820, loss: 1.3195
2022-07-21 16:02:51 - train: epoch 0071, iter [02800, 05004], lr: 0.043766, loss: 1.3882
2022-07-21 16:05:08 - train: epoch 0071, iter [02900, 05004], lr: 0.043711, loss: 1.0497
2022-07-21 16:07:24 - train: epoch 0071, iter [03000, 05004], lr: 0.043656, loss: 1.2162
2022-07-21 16:09:41 - train: epoch 0071, iter [03100, 05004], lr: 0.043602, loss: 1.1593
2022-07-21 16:11:58 - train: epoch 0071, iter [03200, 05004], lr: 0.043547, loss: 1.4322
2022-07-21 16:14:15 - train: epoch 0071, iter [03300, 05004], lr: 0.043493, loss: 1.2213
2022-07-21 16:16:31 - train: epoch 0071, iter [03400, 05004], lr: 0.043438, loss: 0.9792
2022-07-21 16:18:48 - train: epoch 0071, iter [03500, 05004], lr: 0.043384, loss: 1.3116
2022-07-21 16:21:06 - train: epoch 0071, iter [03600, 05004], lr: 0.043329, loss: 1.2730
2022-07-21 16:23:23 - train: epoch 0071, iter [03700, 05004], lr: 0.043275, loss: 1.2814
2022-07-21 16:25:40 - train: epoch 0071, iter [03800, 05004], lr: 0.043220, loss: 1.2718
2022-07-21 16:27:57 - train: epoch 0071, iter [03900, 05004], lr: 0.043166, loss: 1.2507
2022-07-21 16:30:14 - train: epoch 0071, iter [04000, 05004], lr: 0.043112, loss: 1.3068
2022-07-21 16:32:31 - train: epoch 0071, iter [04100, 05004], lr: 0.043057, loss: 1.2335
2022-07-21 16:34:48 - train: epoch 0071, iter [04200, 05004], lr: 0.043003, loss: 1.1690
2022-07-21 16:37:05 - train: epoch 0071, iter [04300, 05004], lr: 0.042949, loss: 1.0979
2022-07-21 16:39:21 - train: epoch 0071, iter [04400, 05004], lr: 0.042894, loss: 1.2884
2022-07-21 16:41:39 - train: epoch 0071, iter [04500, 05004], lr: 0.042840, loss: 1.2871
2022-07-21 16:43:56 - train: epoch 0071, iter [04600, 05004], lr: 0.042786, loss: 1.3286
2022-07-21 16:46:13 - train: epoch 0071, iter [04700, 05004], lr: 0.042732, loss: 1.2019
2022-07-21 16:48:30 - train: epoch 0071, iter [04800, 05004], lr: 0.042678, loss: 1.0716
2022-07-21 16:50:47 - train: epoch 0071, iter [04900, 05004], lr: 0.042623, loss: 1.0788
2022-07-21 16:53:04 - train: epoch 0071, iter [05000, 05004], lr: 0.042569, loss: 1.2587
2022-07-21 16:53:10 - train: epoch 071, train_loss: 1.1772
2022-07-21 16:55:25 - eval: epoch: 071, acc1: 72.866%, acc5: 91.570%, test_loss: 1.1018, per_image_load_time: 1.230ms, per_image_inference_time: 3.916ms
2022-07-21 16:55:26 - until epoch: 071, best_acc1: 72.866%
2022-07-21 16:55:26 - epoch 072 lr: 0.042567
2022-07-21 16:57:50 - train: epoch 0072, iter [00100, 05004], lr: 0.042513, loss: 1.2046
2022-07-21 17:00:07 - train: epoch 0072, iter [00200, 05004], lr: 0.042459, loss: 1.1011
2022-07-21 17:02:24 - train: epoch 0072, iter [00300, 05004], lr: 0.042405, loss: 1.1402
2022-07-21 17:04:40 - train: epoch 0072, iter [00400, 05004], lr: 0.042351, loss: 1.2233
2022-07-21 17:06:57 - train: epoch 0072, iter [00500, 05004], lr: 0.042297, loss: 1.0981
2022-07-21 17:09:15 - train: epoch 0072, iter [00600, 05004], lr: 0.042243, loss: 1.0991
2022-07-21 17:11:32 - train: epoch 0072, iter [00700, 05004], lr: 0.042189, loss: 1.0063
2022-07-21 17:13:49 - train: epoch 0072, iter [00800, 05004], lr: 0.042135, loss: 1.3089
2022-07-21 17:16:07 - train: epoch 0072, iter [00900, 05004], lr: 0.042081, loss: 1.0987
2022-07-21 17:18:24 - train: epoch 0072, iter [01000, 05004], lr: 0.042027, loss: 1.0533
2022-07-21 17:20:41 - train: epoch 0072, iter [01100, 05004], lr: 0.041974, loss: 1.0807
2022-07-21 17:22:58 - train: epoch 0072, iter [01200, 05004], lr: 0.041920, loss: 1.2029
2022-07-21 17:25:15 - train: epoch 0072, iter [01300, 05004], lr: 0.041866, loss: 1.1056
2022-07-21 17:27:33 - train: epoch 0072, iter [01400, 05004], lr: 0.041812, loss: 1.2268
2022-07-21 17:29:50 - train: epoch 0072, iter [01500, 05004], lr: 0.041758, loss: 1.1530
2022-07-21 17:32:07 - train: epoch 0072, iter [01600, 05004], lr: 0.041705, loss: 1.0896
2022-07-21 17:34:24 - train: epoch 0072, iter [01700, 05004], lr: 0.041651, loss: 1.0575
2022-07-21 17:36:41 - train: epoch 0072, iter [01800, 05004], lr: 0.041597, loss: 1.1169
2022-07-21 17:38:58 - train: epoch 0072, iter [01900, 05004], lr: 0.041544, loss: 1.1810
2022-07-21 17:41:15 - train: epoch 0072, iter [02000, 05004], lr: 0.041490, loss: 1.2730
2022-07-21 17:43:32 - train: epoch 0072, iter [02100, 05004], lr: 0.041437, loss: 1.0860
2022-07-21 17:45:49 - train: epoch 0072, iter [02200, 05004], lr: 0.041383, loss: 1.2364
2022-07-21 17:48:06 - train: epoch 0072, iter [02300, 05004], lr: 0.041330, loss: 1.2276
2022-07-21 17:50:24 - train: epoch 0072, iter [02400, 05004], lr: 0.041276, loss: 1.1379
2022-07-21 17:52:41 - train: epoch 0072, iter [02500, 05004], lr: 0.041223, loss: 1.2015
2022-07-21 17:54:58 - train: epoch 0072, iter [02600, 05004], lr: 0.041169, loss: 1.1597
2022-07-21 17:57:15 - train: epoch 0072, iter [02700, 05004], lr: 0.041116, loss: 1.3786
2022-07-21 17:59:32 - train: epoch 0072, iter [02800, 05004], lr: 0.041062, loss: 1.1254
2022-07-21 18:01:49 - train: epoch 0072, iter [02900, 05004], lr: 0.041009, loss: 1.2042
2022-07-21 18:04:06 - train: epoch 0072, iter [03000, 05004], lr: 0.040956, loss: 1.1398
2022-07-21 18:06:23 - train: epoch 0072, iter [03100, 05004], lr: 0.040902, loss: 1.4914
2022-07-21 18:08:40 - train: epoch 0072, iter [03200, 05004], lr: 0.040849, loss: 1.1960
2022-07-21 18:10:57 - train: epoch 0072, iter [03300, 05004], lr: 0.040796, loss: 1.0715
2022-07-21 18:13:14 - train: epoch 0072, iter [03400, 05004], lr: 0.040742, loss: 1.1749
2022-07-21 18:15:31 - train: epoch 0072, iter [03500, 05004], lr: 0.040689, loss: 1.1534
2022-07-21 18:17:48 - train: epoch 0072, iter [03600, 05004], lr: 0.040636, loss: 1.1052
2022-07-21 18:20:05 - train: epoch 0072, iter [03700, 05004], lr: 0.040583, loss: 1.3361
2022-07-21 18:22:22 - train: epoch 0072, iter [03800, 05004], lr: 0.040530, loss: 1.2558
2022-07-21 18:24:39 - train: epoch 0072, iter [03900, 05004], lr: 0.040477, loss: 1.1055
2022-07-21 18:26:55 - train: epoch 0072, iter [04000, 05004], lr: 0.040423, loss: 1.1781
2022-07-21 18:29:12 - train: epoch 0072, iter [04100, 05004], lr: 0.040370, loss: 1.3229
2022-07-21 18:31:29 - train: epoch 0072, iter [04200, 05004], lr: 0.040317, loss: 0.8957
2022-07-21 18:33:46 - train: epoch 0072, iter [04300, 05004], lr: 0.040264, loss: 1.0801
2022-07-21 18:36:03 - train: epoch 0072, iter [04400, 05004], lr: 0.040211, loss: 1.2109
2022-07-21 18:38:20 - train: epoch 0072, iter [04500, 05004], lr: 0.040158, loss: 1.0994
2022-07-21 18:40:36 - train: epoch 0072, iter [04600, 05004], lr: 0.040105, loss: 1.0944
2022-07-21 18:42:53 - train: epoch 0072, iter [04700, 05004], lr: 0.040053, loss: 1.1438
2022-07-21 18:45:10 - train: epoch 0072, iter [04800, 05004], lr: 0.040000, loss: 1.3251
2022-07-21 18:47:27 - train: epoch 0072, iter [04900, 05004], lr: 0.039947, loss: 1.2707
2022-07-21 18:49:44 - train: epoch 0072, iter [05000, 05004], lr: 0.039894, loss: 1.0773
2022-07-21 18:49:50 - train: epoch 072, train_loss: 1.1584
2022-07-21 18:52:04 - eval: epoch: 072, acc1: 72.828%, acc5: 91.466%, test_loss: 1.1042, per_image_load_time: 1.244ms, per_image_inference_time: 3.924ms
2022-07-21 18:52:05 - until epoch: 072, best_acc1: 72.866%
2022-07-21 18:52:05 - epoch 073 lr: 0.039891
2022-07-21 18:54:29 - train: epoch 0073, iter [00100, 05004], lr: 0.039839, loss: 1.3915
2022-07-21 18:56:46 - train: epoch 0073, iter [00200, 05004], lr: 0.039786, loss: 1.1866
2022-07-21 18:59:02 - train: epoch 0073, iter [00300, 05004], lr: 0.039734, loss: 1.2279
2022-07-21 19:01:19 - train: epoch 0073, iter [00400, 05004], lr: 0.039681, loss: 0.9467
2022-07-21 19:03:36 - train: epoch 0073, iter [00500, 05004], lr: 0.039628, loss: 1.0122
2022-07-21 19:05:52 - train: epoch 0073, iter [00600, 05004], lr: 0.039575, loss: 1.0566
2022-07-21 19:08:09 - train: epoch 0073, iter [00700, 05004], lr: 0.039523, loss: 0.9600
2022-07-21 19:10:26 - train: epoch 0073, iter [00800, 05004], lr: 0.039470, loss: 1.0601
2022-07-21 19:12:42 - train: epoch 0073, iter [00900, 05004], lr: 0.039418, loss: 1.1076
2022-07-21 19:14:59 - train: epoch 0073, iter [01000, 05004], lr: 0.039365, loss: 0.9540
2022-07-21 19:17:15 - train: epoch 0073, iter [01100, 05004], lr: 0.039313, loss: 1.1143
2022-07-21 19:19:32 - train: epoch 0073, iter [01200, 05004], lr: 0.039260, loss: 1.0334
2022-07-21 19:21:49 - train: epoch 0073, iter [01300, 05004], lr: 0.039208, loss: 1.2434
2022-07-21 19:24:05 - train: epoch 0073, iter [01400, 05004], lr: 0.039155, loss: 1.0193
2022-07-21 19:26:22 - train: epoch 0073, iter [01500, 05004], lr: 0.039103, loss: 1.2255
2022-07-21 19:28:39 - train: epoch 0073, iter [01600, 05004], lr: 0.039050, loss: 1.1633
2022-07-21 19:30:55 - train: epoch 0073, iter [01700, 05004], lr: 0.038998, loss: 1.2453
2022-07-21 19:33:12 - train: epoch 0073, iter [01800, 05004], lr: 0.038945, loss: 0.9755
2022-07-21 19:35:29 - train: epoch 0073, iter [01900, 05004], lr: 0.038893, loss: 1.2487
2022-07-21 19:37:46 - train: epoch 0073, iter [02000, 05004], lr: 0.038841, loss: 1.1271
2022-07-21 19:40:03 - train: epoch 0073, iter [02100, 05004], lr: 0.038789, loss: 1.0385
2022-07-21 19:42:20 - train: epoch 0073, iter [02200, 05004], lr: 0.038736, loss: 1.1070
2022-07-21 19:44:37 - train: epoch 0073, iter [02300, 05004], lr: 0.038684, loss: 1.1356
2022-07-21 19:46:54 - train: epoch 0073, iter [02400, 05004], lr: 0.038632, loss: 1.3382
2022-07-21 19:49:11 - train: epoch 0073, iter [02500, 05004], lr: 0.038580, loss: 1.0825
2022-07-21 19:51:28 - train: epoch 0073, iter [02600, 05004], lr: 0.038528, loss: 1.0816
2022-07-21 19:53:45 - train: epoch 0073, iter [02700, 05004], lr: 0.038476, loss: 1.2700
2022-07-21 19:56:02 - train: epoch 0073, iter [02800, 05004], lr: 0.038423, loss: 1.0360
2022-07-21 19:58:19 - train: epoch 0073, iter [02900, 05004], lr: 0.038371, loss: 1.1748
2022-07-21 20:00:35 - train: epoch 0073, iter [03000, 05004], lr: 0.038319, loss: 0.9873
2022-07-21 20:02:52 - train: epoch 0073, iter [03100, 05004], lr: 0.038267, loss: 1.1603
2022-07-21 20:05:09 - train: epoch 0073, iter [03200, 05004], lr: 0.038215, loss: 0.9692
2022-07-21 20:07:26 - train: epoch 0073, iter [03300, 05004], lr: 0.038163, loss: 1.1292
2022-07-21 20:09:43 - train: epoch 0073, iter [03400, 05004], lr: 0.038111, loss: 1.2232
2022-07-21 20:12:00 - train: epoch 0073, iter [03500, 05004], lr: 0.038060, loss: 1.1286
2022-07-21 20:14:17 - train: epoch 0073, iter [03600, 05004], lr: 0.038008, loss: 1.1503
2022-07-21 20:16:34 - train: epoch 0073, iter [03700, 05004], lr: 0.037956, loss: 1.0935
2022-07-21 20:18:51 - train: epoch 0073, iter [03800, 05004], lr: 0.037904, loss: 1.2054
2022-07-21 20:21:08 - train: epoch 0073, iter [03900, 05004], lr: 0.037852, loss: 1.1402
2022-07-21 20:23:25 - train: epoch 0073, iter [04000, 05004], lr: 0.037801, loss: 1.2281
2022-07-21 20:25:42 - train: epoch 0073, iter [04100, 05004], lr: 0.037749, loss: 1.1389
2022-07-21 20:27:59 - train: epoch 0073, iter [04200, 05004], lr: 0.037697, loss: 1.1083
2022-07-21 20:30:16 - train: epoch 0073, iter [04300, 05004], lr: 0.037645, loss: 1.2382
2022-07-21 20:32:33 - train: epoch 0073, iter [04400, 05004], lr: 0.037594, loss: 1.1862
2022-07-21 20:34:50 - train: epoch 0073, iter [04500, 05004], lr: 0.037542, loss: 1.0590
2022-07-21 20:37:07 - train: epoch 0073, iter [04600, 05004], lr: 0.037491, loss: 1.3663
2022-07-21 20:39:24 - train: epoch 0073, iter [04700, 05004], lr: 0.037439, loss: 0.9154
2022-07-21 20:41:41 - train: epoch 0073, iter [04800, 05004], lr: 0.037387, loss: 1.0677
2022-07-21 20:43:58 - train: epoch 0073, iter [04900, 05004], lr: 0.037336, loss: 1.1989
2022-07-21 20:46:15 - train: epoch 0073, iter [05000, 05004], lr: 0.037284, loss: 1.3402
2022-07-21 20:46:22 - train: epoch 073, train_loss: 1.1339
2022-07-21 20:48:23 - eval: epoch: 073, acc1: 72.828%, acc5: 91.476%, test_loss: 1.0930, per_image_load_time: 0.871ms, per_image_inference_time: 3.898ms
2022-07-21 20:48:23 - until epoch: 073, best_acc1: 72.866%
2022-07-21 20:48:23 - epoch 074 lr: 0.037282
2022-07-21 20:50:47 - train: epoch 0074, iter [00100, 05004], lr: 0.037231, loss: 0.9998
2022-07-21 20:53:04 - train: epoch 0074, iter [00200, 05004], lr: 0.037179, loss: 1.0237
2022-07-21 20:55:21 - train: epoch 0074, iter [00300, 05004], lr: 0.037128, loss: 1.0892
2022-07-21 20:57:38 - train: epoch 0074, iter [00400, 05004], lr: 0.037077, loss: 1.2261
2022-07-21 20:59:54 - train: epoch 0074, iter [00500, 05004], lr: 0.037025, loss: 1.0587
2022-07-21 21:02:11 - train: epoch 0074, iter [00600, 05004], lr: 0.036974, loss: 1.1881
2022-07-21 21:04:28 - train: epoch 0074, iter [00700, 05004], lr: 0.036923, loss: 0.9830
2022-07-21 21:06:45 - train: epoch 0074, iter [00800, 05004], lr: 0.036871, loss: 1.0998
2022-07-21 21:09:01 - train: epoch 0074, iter [00900, 05004], lr: 0.036820, loss: 0.9676
2022-07-21 21:11:18 - train: epoch 0074, iter [01000, 05004], lr: 0.036769, loss: 1.2822
2022-07-21 21:13:35 - train: epoch 0074, iter [01100, 05004], lr: 0.036718, loss: 1.0987
2022-07-21 21:15:52 - train: epoch 0074, iter [01200, 05004], lr: 0.036667, loss: 1.0943
2022-07-21 21:18:08 - train: epoch 0074, iter [01300, 05004], lr: 0.036616, loss: 1.0392
2022-07-21 21:20:25 - train: epoch 0074, iter [01400, 05004], lr: 0.036564, loss: 0.8695
2022-07-21 21:22:41 - train: epoch 0074, iter [01500, 05004], lr: 0.036513, loss: 1.1957
2022-07-21 21:24:58 - train: epoch 0074, iter [01600, 05004], lr: 0.036462, loss: 1.2941
2022-07-21 21:27:15 - train: epoch 0074, iter [01700, 05004], lr: 0.036411, loss: 1.1542
2022-07-21 21:29:31 - train: epoch 0074, iter [01800, 05004], lr: 0.036360, loss: 1.2043
2022-07-21 21:31:48 - train: epoch 0074, iter [01900, 05004], lr: 0.036309, loss: 1.2380
2022-07-21 21:34:05 - train: epoch 0074, iter [02000, 05004], lr: 0.036258, loss: 1.1282
2022-07-21 21:36:21 - train: epoch 0074, iter [02100, 05004], lr: 0.036208, loss: 1.2458
2022-07-21 21:38:38 - train: epoch 0074, iter [02200, 05004], lr: 0.036157, loss: 1.3067
2022-07-21 21:40:55 - train: epoch 0074, iter [02300, 05004], lr: 0.036106, loss: 1.2821
2022-07-21 21:43:12 - train: epoch 0074, iter [02400, 05004], lr: 0.036055, loss: 1.2619
2022-07-21 21:45:28 - train: epoch 0074, iter [02500, 05004], lr: 0.036004, loss: 1.0716
2022-07-21 21:47:45 - train: epoch 0074, iter [02600, 05004], lr: 0.035953, loss: 0.8660
2022-07-21 21:50:02 - train: epoch 0074, iter [02700, 05004], lr: 0.035903, loss: 1.0590
2022-07-21 21:52:18 - train: epoch 0074, iter [02800, 05004], lr: 0.035852, loss: 1.2228
2022-07-21 21:54:35 - train: epoch 0074, iter [02900, 05004], lr: 0.035801, loss: 0.9693
2022-07-21 21:56:52 - train: epoch 0074, iter [03000, 05004], lr: 0.035751, loss: 1.1726
2022-07-21 21:59:09 - train: epoch 0074, iter [03100, 05004], lr: 0.035700, loss: 1.1758
2022-07-21 22:01:25 - train: epoch 0074, iter [03200, 05004], lr: 0.035649, loss: 1.0594
2022-07-21 22:03:42 - train: epoch 0074, iter [03300, 05004], lr: 0.035599, loss: 1.2000
2022-07-21 22:05:59 - train: epoch 0074, iter [03400, 05004], lr: 0.035548, loss: 1.2711
2022-07-21 22:08:16 - train: epoch 0074, iter [03500, 05004], lr: 0.035498, loss: 0.9759
2022-07-21 22:10:33 - train: epoch 0074, iter [03600, 05004], lr: 0.035447, loss: 1.1952
2022-07-21 22:12:49 - train: epoch 0074, iter [03700, 05004], lr: 0.035397, loss: 1.0896
2022-07-21 22:15:06 - train: epoch 0074, iter [03800, 05004], lr: 0.035346, loss: 1.0066
2022-07-21 22:17:23 - train: epoch 0074, iter [03900, 05004], lr: 0.035296, loss: 1.0711
2022-07-21 22:19:39 - train: epoch 0074, iter [04000, 05004], lr: 0.035246, loss: 1.2792
2022-07-21 22:21:56 - train: epoch 0074, iter [04100, 05004], lr: 0.035195, loss: 1.2162
2022-07-21 22:24:13 - train: epoch 0074, iter [04200, 05004], lr: 0.035145, loss: 1.0584
2022-07-21 22:26:29 - train: epoch 0074, iter [04300, 05004], lr: 0.035095, loss: 1.1862
2022-07-21 22:28:46 - train: epoch 0074, iter [04400, 05004], lr: 0.035044, loss: 0.9668
2022-07-21 22:31:03 - train: epoch 0074, iter [04500, 05004], lr: 0.034994, loss: 0.9054
2022-07-21 22:33:20 - train: epoch 0074, iter [04600, 05004], lr: 0.034944, loss: 1.2540
2022-07-21 22:35:37 - train: epoch 0074, iter [04700, 05004], lr: 0.034894, loss: 0.9952
2022-07-21 22:37:53 - train: epoch 0074, iter [04800, 05004], lr: 0.034844, loss: 1.1906
2022-07-21 22:40:10 - train: epoch 0074, iter [04900, 05004], lr: 0.034794, loss: 1.2730
2022-07-21 22:42:27 - train: epoch 0074, iter [05000, 05004], lr: 0.034743, loss: 1.0908
2022-07-21 22:42:33 - train: epoch 074, train_loss: 1.1108
2022-07-21 22:44:41 - eval: epoch: 074, acc1: 73.328%, acc5: 91.636%, test_loss: 1.0739, per_image_load_time: 0.992ms, per_image_inference_time: 3.915ms
2022-07-21 22:44:42 - until epoch: 074, best_acc1: 73.328%
2022-07-21 22:44:42 - epoch 075 lr: 0.034741
2022-07-21 22:47:06 - train: epoch 0075, iter [00100, 05004], lr: 0.034691, loss: 1.1744
2022-07-21 22:49:24 - train: epoch 0075, iter [00200, 05004], lr: 0.034641, loss: 1.1693
2022-07-21 22:51:41 - train: epoch 0075, iter [00300, 05004], lr: 0.034591, loss: 1.0328
2022-07-21 22:53:59 - train: epoch 0075, iter [00400, 05004], lr: 0.034541, loss: 0.8958
2022-07-21 22:56:17 - train: epoch 0075, iter [00500, 05004], lr: 0.034491, loss: 1.1042
2022-07-21 22:58:34 - train: epoch 0075, iter [00600, 05004], lr: 0.034441, loss: 1.1302
2022-07-21 23:00:52 - train: epoch 0075, iter [00700, 05004], lr: 0.034392, loss: 1.1847
2022-07-21 23:03:09 - train: epoch 0075, iter [00800, 05004], lr: 0.034342, loss: 1.1004
2022-07-21 23:05:26 - train: epoch 0075, iter [00900, 05004], lr: 0.034292, loss: 1.0822
