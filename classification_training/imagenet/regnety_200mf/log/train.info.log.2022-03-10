2022-03-10 08:27:56 - network: RegNetY_200MF
2022-03-10 08:27:56 - num_classes: 1000
2022-03-10 08:27:56 - input_image_size: 224
2022-03-10 08:27:56 - scale: 1.1428571428571428
2022-03-10 08:27:56 - trained_model_path: 
2022-03-10 08:27:56 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-10 08:27:56 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f73d9369430>
2022-03-10 08:27:56 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f73d9369700>
2022-03-10 08:27:56 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f73d9369730>
2022-03-10 08:27:56 - seed: 0
2022-03-10 08:27:56 - batch_size: 256
2022-03-10 08:27:56 - num_workers: 8
2022-03-10 08:27:56 - optimizer: ('SGD', {'lr': 0.2, 'momentum': 0.9, 'weight_decay': 5e-05})
2022-03-10 08:27:56 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-03-10 08:27:56 - epochs: 100
2022-03-10 08:27:56 - print_interval: 100
2022-03-10 08:27:56 - distributed: True
2022-03-10 08:27:56 - sync_bn: False
2022-03-10 08:27:56 - apex: True
2022-03-10 08:27:56 - gpus_type: NVIDIA GeForce RTX 3090
2022-03-10 08:27:56 - gpus_num: 2
2022-03-10 08:27:56 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f73bc2bf770>
2022-03-10 08:27:56 - --------------------parameters--------------------
2022-03-10 08:27:56 - name: conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer1.0.downsample_layer.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.downsample_layer.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.downsample_layer.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer1.0.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer1.0.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer1.0.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer2.0.downsample_layer.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.downsample_layer.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.downsample_layer.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer2.0.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer2.0.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer2.0.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer3.0.downsample_layer.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.downsample_layer.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.downsample_layer.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.0.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.0.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer3.0.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.1.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.1.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer3.1.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.2.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.2.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer3.2.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.3.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer3.3.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer3.3.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.0.downsample_layer.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.downsample_layer.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.downsample_layer.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.0.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.0.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.0.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.1.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.1.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.1.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.2.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.2.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.2.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.3.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.3.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.3.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.4.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.4.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.4.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.5.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.5.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.5.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv1.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv1.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv1.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv2.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv2.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv2.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv3.layer.0.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv3.layer.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.conv3.layer.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.6.se_block.layers.1.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.se_block.layers.1.bias, grad: True
2022-03-10 08:27:56 - name: layer4.6.se_block.layers.3.weight, grad: True
2022-03-10 08:27:56 - name: layer4.6.se_block.layers.3.bias, grad: True
2022-03-10 08:27:56 - name: fc.weight, grad: True
2022-03-10 08:27:56 - name: fc.bias, grad: True
2022-03-10 08:27:56 - --------------------buffers--------------------
2022-03-10 08:27:56 - name: conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer1.0.downsample_layer.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer1.0.downsample_layer.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer1.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer2.0.downsample_layer.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer2.0.downsample_layer.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer2.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.0.downsample_layer.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.0.downsample_layer.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.0.downsample_layer.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.0.downsample_layer.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv1.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv1.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv1.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv2.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv2.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv2.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv3.layer.1.running_mean, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv3.layer.1.running_var, grad: False
2022-03-10 08:27:56 - name: layer4.6.conv3.layer.1.num_batches_tracked, grad: False
2022-03-10 08:27:56 - epoch 001 lr: 0.04000000000000001
2022-03-10 08:28:38 - train: epoch 0001, iter [00100, 05004], lr: 0.040000, loss: 6.9521
2022-03-10 08:29:17 - train: epoch 0001, iter [00200, 05004], lr: 0.040000, loss: 6.7725
2022-03-10 08:29:56 - train: epoch 0001, iter [00300, 05004], lr: 0.040000, loss: 6.6170
2022-03-10 08:30:34 - train: epoch 0001, iter [00400, 05004], lr: 0.040000, loss: 6.5844
2022-03-10 08:31:12 - train: epoch 0001, iter [00500, 05004], lr: 0.040000, loss: 6.4746
2022-03-10 08:31:50 - train: epoch 0001, iter [00600, 05004], lr: 0.040000, loss: 6.2943
2022-03-10 08:32:28 - train: epoch 0001, iter [00700, 05004], lr: 0.040000, loss: 6.2721
2022-03-10 08:33:07 - train: epoch 0001, iter [00800, 05004], lr: 0.040000, loss: 6.1894
2022-03-10 08:33:45 - train: epoch 0001, iter [00900, 05004], lr: 0.040000, loss: 6.0442
2022-03-10 08:34:21 - train: epoch 0001, iter [01000, 05004], lr: 0.040000, loss: 6.0616
2022-03-10 08:34:59 - train: epoch 0001, iter [01100, 05004], lr: 0.040000, loss: 6.0652
2022-03-10 08:35:36 - train: epoch 0001, iter [01200, 05004], lr: 0.040000, loss: 5.7685
2022-03-10 08:36:13 - train: epoch 0001, iter [01300, 05004], lr: 0.040000, loss: 5.8028
2022-03-10 08:36:50 - train: epoch 0001, iter [01400, 05004], lr: 0.040000, loss: 5.7238
2022-03-10 08:37:28 - train: epoch 0001, iter [01500, 05004], lr: 0.040000, loss: 5.6891
2022-03-10 08:38:05 - train: epoch 0001, iter [01600, 05004], lr: 0.040000, loss: 5.7039
2022-03-10 08:38:42 - train: epoch 0001, iter [01700, 05004], lr: 0.040000, loss: 5.4667
2022-03-10 08:39:19 - train: epoch 0001, iter [01800, 05004], lr: 0.040000, loss: 5.5151
2022-03-10 08:39:56 - train: epoch 0001, iter [01900, 05004], lr: 0.040000, loss: 5.4879
2022-03-10 08:40:32 - train: epoch 0001, iter [02000, 05004], lr: 0.040000, loss: 5.3265
2022-03-10 08:41:09 - train: epoch 0001, iter [02100, 05004], lr: 0.040000, loss: 5.2893
2022-03-10 08:41:46 - train: epoch 0001, iter [02200, 05004], lr: 0.040000, loss: 5.2898
2022-03-10 08:42:23 - train: epoch 0001, iter [02300, 05004], lr: 0.040000, loss: 5.0618
2022-03-10 08:43:00 - train: epoch 0001, iter [02400, 05004], lr: 0.040000, loss: 5.2048
2022-03-10 08:43:38 - train: epoch 0001, iter [02500, 05004], lr: 0.040000, loss: 5.1912
2022-03-10 08:44:15 - train: epoch 0001, iter [02600, 05004], lr: 0.040000, loss: 5.2200
2022-03-10 08:44:52 - train: epoch 0001, iter [02700, 05004], lr: 0.040000, loss: 5.0790
2022-03-10 08:45:28 - train: epoch 0001, iter [02800, 05004], lr: 0.040000, loss: 5.1812
2022-03-10 08:46:06 - train: epoch 0001, iter [02900, 05004], lr: 0.040000, loss: 4.8929
2022-03-10 08:46:44 - train: epoch 0001, iter [03000, 05004], lr: 0.040000, loss: 5.0563
2022-03-10 08:47:21 - train: epoch 0001, iter [03100, 05004], lr: 0.040000, loss: 5.1182
2022-03-10 08:47:58 - train: epoch 0001, iter [03200, 05004], lr: 0.040000, loss: 5.0599
2022-03-10 08:48:36 - train: epoch 0001, iter [03300, 05004], lr: 0.040000, loss: 4.7156
2022-03-10 08:49:12 - train: epoch 0001, iter [03400, 05004], lr: 0.040000, loss: 4.8446
2022-03-10 08:49:49 - train: epoch 0001, iter [03500, 05004], lr: 0.040000, loss: 4.8935
2022-03-10 08:50:26 - train: epoch 0001, iter [03600, 05004], lr: 0.040000, loss: 4.7882
2022-03-10 08:51:03 - train: epoch 0001, iter [03700, 05004], lr: 0.040000, loss: 4.7795
2022-03-10 08:51:40 - train: epoch 0001, iter [03800, 05004], lr: 0.040000, loss: 4.6776
2022-03-10 08:52:17 - train: epoch 0001, iter [03900, 05004], lr: 0.040000, loss: 4.7221
2022-03-10 08:52:54 - train: epoch 0001, iter [04000, 05004], lr: 0.040000, loss: 4.8826
2022-03-10 08:53:32 - train: epoch 0001, iter [04100, 05004], lr: 0.040000, loss: 4.9017
2022-03-10 08:54:08 - train: epoch 0001, iter [04200, 05004], lr: 0.040000, loss: 4.5582
2022-03-10 08:54:45 - train: epoch 0001, iter [04300, 05004], lr: 0.040000, loss: 4.5731
2022-03-10 08:55:22 - train: epoch 0001, iter [04400, 05004], lr: 0.040000, loss: 4.4554
2022-03-10 08:56:00 - train: epoch 0001, iter [04500, 05004], lr: 0.040000, loss: 4.5811
2022-03-10 08:56:37 - train: epoch 0001, iter [04600, 05004], lr: 0.040000, loss: 4.7436
2022-03-10 08:57:14 - train: epoch 0001, iter [04700, 05004], lr: 0.040000, loss: 4.3363
2022-03-10 08:57:52 - train: epoch 0001, iter [04800, 05004], lr: 0.040000, loss: 4.6487
2022-03-10 08:58:29 - train: epoch 0001, iter [04900, 05004], lr: 0.040000, loss: 4.5292
2022-03-10 08:59:06 - train: epoch 0001, iter [05000, 05004], lr: 0.040000, loss: 4.4443
2022-03-10 08:59:07 - train: epoch 001, train_loss: 5.3206
2022-03-10 09:00:26 - eval: epoch: 001, acc1: 16.768%, acc5: 37.834%, test_loss: 4.2504, per_image_load_time: 2.845ms, per_image_inference_time: 0.249ms
2022-03-10 09:00:26 - until epoch: 001, best_acc1: 16.768%
2022-03-10 09:00:26 - epoch 002 lr: 0.08000000000000002
2022-03-10 09:01:09 - train: epoch 0002, iter [00100, 05004], lr: 0.080000, loss: 4.5270
2022-03-10 09:01:47 - train: epoch 0002, iter [00200, 05004], lr: 0.080000, loss: 4.4870
2022-03-10 09:02:25 - train: epoch 0002, iter [00300, 05004], lr: 0.080000, loss: 4.7733
2022-03-10 09:03:03 - train: epoch 0002, iter [00400, 05004], lr: 0.080000, loss: 4.6395
2022-03-10 09:03:41 - train: epoch 0002, iter [00500, 05004], lr: 0.080000, loss: 4.3679
2022-03-10 09:04:19 - train: epoch 0002, iter [00600, 05004], lr: 0.080000, loss: 4.3190
2022-03-10 09:04:58 - train: epoch 0002, iter [00700, 05004], lr: 0.080000, loss: 4.5948
2022-03-10 09:05:37 - train: epoch 0002, iter [00800, 05004], lr: 0.080000, loss: 4.3352
2022-03-10 09:06:16 - train: epoch 0002, iter [00900, 05004], lr: 0.080000, loss: 4.1722
2022-03-10 09:06:53 - train: epoch 0002, iter [01000, 05004], lr: 0.080000, loss: 4.3463
2022-03-10 09:07:32 - train: epoch 0002, iter [01100, 05004], lr: 0.080000, loss: 4.2507
2022-03-10 09:08:09 - train: epoch 0002, iter [01200, 05004], lr: 0.080000, loss: 4.2126
2022-03-10 09:08:46 - train: epoch 0002, iter [01300, 05004], lr: 0.080000, loss: 4.2732
2022-03-10 09:09:23 - train: epoch 0002, iter [01400, 05004], lr: 0.080000, loss: 4.5111
2022-03-10 09:10:01 - train: epoch 0002, iter [01500, 05004], lr: 0.080000, loss: 4.2463
2022-03-10 09:10:37 - train: epoch 0002, iter [01600, 05004], lr: 0.080000, loss: 4.1041
2022-03-10 09:11:15 - train: epoch 0002, iter [01700, 05004], lr: 0.080000, loss: 4.1553
2022-03-10 09:11:52 - train: epoch 0002, iter [01800, 05004], lr: 0.080000, loss: 4.0316
2022-03-10 09:12:29 - train: epoch 0002, iter [01900, 05004], lr: 0.080000, loss: 4.0155
2022-03-10 09:13:07 - train: epoch 0002, iter [02000, 05004], lr: 0.080000, loss: 3.7931
2022-03-10 09:13:44 - train: epoch 0002, iter [02100, 05004], lr: 0.080000, loss: 3.9672
2022-03-10 09:14:23 - train: epoch 0002, iter [02200, 05004], lr: 0.080000, loss: 3.9369
2022-03-10 09:15:02 - train: epoch 0002, iter [02300, 05004], lr: 0.080000, loss: 4.1586
2022-03-10 09:15:38 - train: epoch 0002, iter [02400, 05004], lr: 0.080000, loss: 4.0259
2022-03-10 09:16:16 - train: epoch 0002, iter [02500, 05004], lr: 0.080000, loss: 3.9281
2022-03-10 09:16:54 - train: epoch 0002, iter [02600, 05004], lr: 0.080000, loss: 3.7949
2022-03-10 09:17:32 - train: epoch 0002, iter [02700, 05004], lr: 0.080000, loss: 4.0262
2022-03-10 09:18:09 - train: epoch 0002, iter [02800, 05004], lr: 0.080000, loss: 4.0215
2022-03-10 09:18:47 - train: epoch 0002, iter [02900, 05004], lr: 0.080000, loss: 4.0927
2022-03-10 09:19:24 - train: epoch 0002, iter [03000, 05004], lr: 0.080000, loss: 3.8742
2022-03-10 09:20:02 - train: epoch 0002, iter [03100, 05004], lr: 0.080000, loss: 3.7486
2022-03-10 09:20:40 - train: epoch 0002, iter [03200, 05004], lr: 0.080000, loss: 3.8922
2022-03-10 09:21:18 - train: epoch 0002, iter [03300, 05004], lr: 0.080000, loss: 3.8463
2022-03-10 09:21:55 - train: epoch 0002, iter [03400, 05004], lr: 0.080000, loss: 3.6488
2022-03-10 09:22:32 - train: epoch 0002, iter [03500, 05004], lr: 0.080000, loss: 3.7783
2022-03-10 09:23:10 - train: epoch 0002, iter [03600, 05004], lr: 0.080000, loss: 3.8014
2022-03-10 09:23:48 - train: epoch 0002, iter [03700, 05004], lr: 0.080000, loss: 3.7723
2022-03-10 09:24:25 - train: epoch 0002, iter [03800, 05004], lr: 0.080000, loss: 3.7609
2022-03-10 09:25:03 - train: epoch 0002, iter [03900, 05004], lr: 0.080000, loss: 3.8700
2022-03-10 09:25:41 - train: epoch 0002, iter [04000, 05004], lr: 0.080000, loss: 3.5663
2022-03-10 09:26:19 - train: epoch 0002, iter [04100, 05004], lr: 0.080000, loss: 3.9876
2022-03-10 09:26:57 - train: epoch 0002, iter [04200, 05004], lr: 0.080000, loss: 3.7936
2022-03-10 09:27:36 - train: epoch 0002, iter [04300, 05004], lr: 0.080000, loss: 3.6832
2022-03-10 09:28:14 - train: epoch 0002, iter [04400, 05004], lr: 0.080000, loss: 3.6612
2022-03-10 09:28:52 - train: epoch 0002, iter [04500, 05004], lr: 0.080000, loss: 3.5988
2022-03-10 09:29:30 - train: epoch 0002, iter [04600, 05004], lr: 0.080000, loss: 3.4599
2022-03-10 09:30:10 - train: epoch 0002, iter [04700, 05004], lr: 0.080000, loss: 3.6119
2022-03-10 09:30:51 - train: epoch 0002, iter [04800, 05004], lr: 0.080000, loss: 3.6124
2022-03-10 09:31:31 - train: epoch 0002, iter [04900, 05004], lr: 0.080000, loss: 3.5143
2022-03-10 09:32:11 - train: epoch 0002, iter [05000, 05004], lr: 0.080000, loss: 3.5665
2022-03-10 09:32:12 - train: epoch 002, train_loss: 4.0097
2022-03-10 09:33:37 - eval: epoch: 002, acc1: 25.006%, acc5: 48.914%, test_loss: 3.6745, per_image_load_time: 1.841ms, per_image_inference_time: 0.269ms
2022-03-10 09:33:37 - until epoch: 002, best_acc1: 25.006%
2022-03-10 09:33:37 - epoch 003 lr: 0.12
2022-03-10 09:34:21 - train: epoch 0003, iter [00100, 05004], lr: 0.120000, loss: 3.7158
2022-03-10 09:35:02 - train: epoch 0003, iter [00200, 05004], lr: 0.120000, loss: 3.6242
2022-03-10 09:35:43 - train: epoch 0003, iter [00300, 05004], lr: 0.120000, loss: 3.6124
2022-03-10 09:36:24 - train: epoch 0003, iter [00400, 05004], lr: 0.120000, loss: 3.5686
2022-03-10 09:37:05 - train: epoch 0003, iter [00500, 05004], lr: 0.120000, loss: 3.7812
2022-03-10 09:37:47 - train: epoch 0003, iter [00600, 05004], lr: 0.120000, loss: 3.5125
2022-03-10 09:38:28 - train: epoch 0003, iter [00700, 05004], lr: 0.120000, loss: 3.7831
2022-03-10 09:39:10 - train: epoch 0003, iter [00800, 05004], lr: 0.120000, loss: 3.8578
2022-03-10 09:39:51 - train: epoch 0003, iter [00900, 05004], lr: 0.120000, loss: 3.5910
2022-03-10 09:40:32 - train: epoch 0003, iter [01000, 05004], lr: 0.120000, loss: 3.6914
2022-03-10 09:41:13 - train: epoch 0003, iter [01100, 05004], lr: 0.120000, loss: 3.3981
2022-03-10 09:41:53 - train: epoch 0003, iter [01200, 05004], lr: 0.120000, loss: 3.5451
2022-03-10 09:42:33 - train: epoch 0003, iter [01300, 05004], lr: 0.120000, loss: 3.6219
2022-03-10 09:43:13 - train: epoch 0003, iter [01400, 05004], lr: 0.120000, loss: 3.4242
2022-03-10 09:43:51 - train: epoch 0003, iter [01500, 05004], lr: 0.120000, loss: 3.6826
2022-03-10 09:44:29 - train: epoch 0003, iter [01600, 05004], lr: 0.120000, loss: 3.6087
2022-03-10 09:45:06 - train: epoch 0003, iter [01700, 05004], lr: 0.120000, loss: 3.3521
2022-03-10 09:45:43 - train: epoch 0003, iter [01800, 05004], lr: 0.120000, loss: 3.3808
2022-03-10 09:46:20 - train: epoch 0003, iter [01900, 05004], lr: 0.120000, loss: 3.5610
2022-03-10 09:46:59 - train: epoch 0003, iter [02000, 05004], lr: 0.120000, loss: 3.5207
2022-03-10 09:47:39 - train: epoch 0003, iter [02100, 05004], lr: 0.120000, loss: 3.6967
2022-03-10 09:48:17 - train: epoch 0003, iter [02200, 05004], lr: 0.120000, loss: 3.7723
2022-03-10 09:48:55 - train: epoch 0003, iter [02300, 05004], lr: 0.120000, loss: 3.2908
2022-03-10 09:49:34 - train: epoch 0003, iter [02400, 05004], lr: 0.120000, loss: 3.2515
2022-03-10 09:50:12 - train: epoch 0003, iter [02500, 05004], lr: 0.120000, loss: 3.5548
2022-03-10 09:50:50 - train: epoch 0003, iter [02600, 05004], lr: 0.120000, loss: 3.5357
2022-03-10 09:51:29 - train: epoch 0003, iter [02700, 05004], lr: 0.120000, loss: 3.6983
2022-03-10 09:52:08 - train: epoch 0003, iter [02800, 05004], lr: 0.120000, loss: 3.4161
2022-03-10 09:52:47 - train: epoch 0003, iter [02900, 05004], lr: 0.120000, loss: 3.4105
2022-03-10 09:53:24 - train: epoch 0003, iter [03000, 05004], lr: 0.120000, loss: 3.3241
2022-03-10 09:54:03 - train: epoch 0003, iter [03100, 05004], lr: 0.120000, loss: 3.6501
2022-03-10 09:54:42 - train: epoch 0003, iter [03200, 05004], lr: 0.120000, loss: 3.3925
2022-03-10 09:55:20 - train: epoch 0003, iter [03300, 05004], lr: 0.120000, loss: 3.4539
2022-03-10 09:55:59 - train: epoch 0003, iter [03400, 05004], lr: 0.120000, loss: 3.4714
2022-03-10 09:56:37 - train: epoch 0003, iter [03500, 05004], lr: 0.120000, loss: 3.2694
2022-03-10 09:57:16 - train: epoch 0003, iter [03600, 05004], lr: 0.120000, loss: 3.3186
2022-03-10 09:57:57 - train: epoch 0003, iter [03700, 05004], lr: 0.120000, loss: 3.4921
2022-03-10 09:58:35 - train: epoch 0003, iter [03800, 05004], lr: 0.120000, loss: 3.3866
2022-03-10 09:59:13 - train: epoch 0003, iter [03900, 05004], lr: 0.120000, loss: 3.6276
2022-03-10 09:59:51 - train: epoch 0003, iter [04000, 05004], lr: 0.120000, loss: 3.1653
2022-03-10 10:00:28 - train: epoch 0003, iter [04100, 05004], lr: 0.120000, loss: 3.3142
2022-03-10 10:01:05 - train: epoch 0003, iter [04200, 05004], lr: 0.120000, loss: 3.2896
2022-03-10 10:01:43 - train: epoch 0003, iter [04300, 05004], lr: 0.120000, loss: 3.3229
2022-03-10 10:02:20 - train: epoch 0003, iter [04400, 05004], lr: 0.120000, loss: 3.3440
2022-03-10 10:02:58 - train: epoch 0003, iter [04500, 05004], lr: 0.120000, loss: 3.4263
2022-03-10 10:03:36 - train: epoch 0003, iter [04600, 05004], lr: 0.120000, loss: 3.1309
2022-03-10 10:04:15 - train: epoch 0003, iter [04700, 05004], lr: 0.120000, loss: 3.2547
2022-03-10 10:04:53 - train: epoch 0003, iter [04800, 05004], lr: 0.120000, loss: 3.4057
2022-03-10 10:05:30 - train: epoch 0003, iter [04900, 05004], lr: 0.120000, loss: 3.3238
2022-03-10 10:06:09 - train: epoch 0003, iter [05000, 05004], lr: 0.120000, loss: 3.2527
2022-03-10 10:06:11 - train: epoch 003, train_loss: 3.4907
2022-03-10 10:07:31 - eval: epoch: 003, acc1: 33.292%, acc5: 58.908%, test_loss: 3.1138, per_image_load_time: 2.905ms, per_image_inference_time: 0.249ms
2022-03-10 10:07:31 - until epoch: 003, best_acc1: 33.292%
2022-03-10 10:07:31 - epoch 004 lr: 0.16000000000000003
2022-03-10 10:08:12 - train: epoch 0004, iter [00100, 05004], lr: 0.160000, loss: 3.3379
2022-03-10 10:08:49 - train: epoch 0004, iter [00200, 05004], lr: 0.160000, loss: 3.3380
2022-03-10 10:09:27 - train: epoch 0004, iter [00300, 05004], lr: 0.160000, loss: 3.2301
2022-03-10 10:10:06 - train: epoch 0004, iter [00400, 05004], lr: 0.160000, loss: 3.3861
2022-03-10 10:10:44 - train: epoch 0004, iter [00500, 05004], lr: 0.160000, loss: 3.1985
2022-03-10 10:11:23 - train: epoch 0004, iter [00600, 05004], lr: 0.160000, loss: 3.6017
2022-03-10 10:12:02 - train: epoch 0004, iter [00700, 05004], lr: 0.160000, loss: 3.2616
2022-03-10 10:12:41 - train: epoch 0004, iter [00800, 05004], lr: 0.160000, loss: 3.4114
2022-03-10 10:13:19 - train: epoch 0004, iter [00900, 05004], lr: 0.160000, loss: 3.1149
2022-03-10 10:13:58 - train: epoch 0004, iter [01000, 05004], lr: 0.160000, loss: 3.2963
2022-03-10 10:14:38 - train: epoch 0004, iter [01100, 05004], lr: 0.160000, loss: 3.5550
2022-03-10 10:15:17 - train: epoch 0004, iter [01200, 05004], lr: 0.160000, loss: 3.2038
2022-03-10 10:15:55 - train: epoch 0004, iter [01300, 05004], lr: 0.160000, loss: 3.2762
2022-03-10 10:16:33 - train: epoch 0004, iter [01400, 05004], lr: 0.160000, loss: 3.1347
2022-03-10 10:17:11 - train: epoch 0004, iter [01500, 05004], lr: 0.160000, loss: 3.4867
2022-03-10 10:17:47 - train: epoch 0004, iter [01600, 05004], lr: 0.160000, loss: 3.4278
2022-03-10 10:18:25 - train: epoch 0004, iter [01700, 05004], lr: 0.160000, loss: 3.3571
2022-03-10 10:19:05 - train: epoch 0004, iter [01800, 05004], lr: 0.160000, loss: 3.3240
2022-03-10 10:19:43 - train: epoch 0004, iter [01900, 05004], lr: 0.160000, loss: 3.4994
2022-03-10 10:20:21 - train: epoch 0004, iter [02000, 05004], lr: 0.160000, loss: 3.1761
2022-03-10 10:20:57 - train: epoch 0004, iter [02100, 05004], lr: 0.160000, loss: 3.4148
2022-03-10 10:21:35 - train: epoch 0004, iter [02200, 05004], lr: 0.160000, loss: 3.2817
2022-03-10 10:22:13 - train: epoch 0004, iter [02300, 05004], lr: 0.160000, loss: 2.9615
2022-03-10 10:22:51 - train: epoch 0004, iter [02400, 05004], lr: 0.160000, loss: 3.3977
2022-03-10 10:23:28 - train: epoch 0004, iter [02500, 05004], lr: 0.160000, loss: 3.1376
2022-03-10 10:24:06 - train: epoch 0004, iter [02600, 05004], lr: 0.160000, loss: 3.2787
2022-03-10 10:24:43 - train: epoch 0004, iter [02700, 05004], lr: 0.160000, loss: 3.1442
2022-03-10 10:25:22 - train: epoch 0004, iter [02800, 05004], lr: 0.160000, loss: 3.2042
2022-03-10 10:25:59 - train: epoch 0004, iter [02900, 05004], lr: 0.160000, loss: 3.2904
2022-03-10 10:26:38 - train: epoch 0004, iter [03000, 05004], lr: 0.160000, loss: 3.4659
2022-03-10 10:27:16 - train: epoch 0004, iter [03100, 05004], lr: 0.160000, loss: 3.2254
2022-03-10 10:27:55 - train: epoch 0004, iter [03200, 05004], lr: 0.160000, loss: 3.3054
2022-03-10 10:28:33 - train: epoch 0004, iter [03300, 05004], lr: 0.160000, loss: 3.1848
2022-03-10 10:29:11 - train: epoch 0004, iter [03400, 05004], lr: 0.160000, loss: 3.2259
2022-03-10 10:29:49 - train: epoch 0004, iter [03500, 05004], lr: 0.160000, loss: 3.2278
2022-03-10 10:30:27 - train: epoch 0004, iter [03600, 05004], lr: 0.160000, loss: 3.0306
2022-03-10 10:31:04 - train: epoch 0004, iter [03700, 05004], lr: 0.160000, loss: 3.1480
2022-03-10 10:31:43 - train: epoch 0004, iter [03800, 05004], lr: 0.160000, loss: 2.9956
2022-03-10 10:32:21 - train: epoch 0004, iter [03900, 05004], lr: 0.160000, loss: 3.0046
2022-03-10 10:32:59 - train: epoch 0004, iter [04000, 05004], lr: 0.160000, loss: 2.9313
2022-03-10 10:33:38 - train: epoch 0004, iter [04100, 05004], lr: 0.160000, loss: 3.0915
2022-03-10 10:34:17 - train: epoch 0004, iter [04200, 05004], lr: 0.160000, loss: 3.2254
2022-03-10 10:34:56 - train: epoch 0004, iter [04300, 05004], lr: 0.160000, loss: 3.1437
2022-03-10 10:35:34 - train: epoch 0004, iter [04400, 05004], lr: 0.160000, loss: 3.0556
2022-03-10 10:36:12 - train: epoch 0004, iter [04500, 05004], lr: 0.160000, loss: 2.5988
2022-03-10 10:36:50 - train: epoch 0004, iter [04600, 05004], lr: 0.160000, loss: 3.2733
2022-03-10 10:37:27 - train: epoch 0004, iter [04700, 05004], lr: 0.160000, loss: 3.0444
2022-03-10 10:38:05 - train: epoch 0004, iter [04800, 05004], lr: 0.160000, loss: 3.1646
2022-03-10 10:38:41 - train: epoch 0004, iter [04900, 05004], lr: 0.160000, loss: 3.0395
2022-03-10 10:39:19 - train: epoch 0004, iter [05000, 05004], lr: 0.160000, loss: 3.1463
2022-03-10 10:39:21 - train: epoch 004, train_loss: 3.2481
2022-03-10 10:40:39 - eval: epoch: 004, acc1: 35.852%, acc5: 62.176%, test_loss: 2.9430, per_image_load_time: 2.779ms, per_image_inference_time: 0.241ms
2022-03-10 10:40:39 - until epoch: 004, best_acc1: 35.852%
2022-03-10 10:40:39 - epoch 005 lr: 0.2
2022-03-10 10:41:20 - train: epoch 0005, iter [00100, 05004], lr: 0.200000, loss: 3.1241
2022-03-10 10:42:00 - train: epoch 0005, iter [00200, 05004], lr: 0.200000, loss: 3.1618
2022-03-10 10:42:40 - train: epoch 0005, iter [00300, 05004], lr: 0.200000, loss: 3.3331
2022-03-10 10:43:18 - train: epoch 0005, iter [00400, 05004], lr: 0.200000, loss: 3.3389
2022-03-10 10:43:57 - train: epoch 0005, iter [00500, 05004], lr: 0.200000, loss: 3.2104
2022-03-10 10:44:38 - train: epoch 0005, iter [00600, 05004], lr: 0.200000, loss: 3.2247
2022-03-10 10:45:18 - train: epoch 0005, iter [00700, 05004], lr: 0.200000, loss: 3.1244
2022-03-10 10:45:58 - train: epoch 0005, iter [00800, 05004], lr: 0.200000, loss: 3.3878
2022-03-10 10:46:38 - train: epoch 0005, iter [00900, 05004], lr: 0.200000, loss: 3.2064
2022-03-10 10:47:18 - train: epoch 0005, iter [01000, 05004], lr: 0.200000, loss: 3.1687
2022-03-10 10:47:58 - train: epoch 0005, iter [01100, 05004], lr: 0.200000, loss: 3.0341
2022-03-10 10:48:38 - train: epoch 0005, iter [01200, 05004], lr: 0.200000, loss: 3.3627
2022-03-10 10:49:19 - train: epoch 0005, iter [01300, 05004], lr: 0.200000, loss: 2.8810
2022-03-10 10:50:00 - train: epoch 0005, iter [01400, 05004], lr: 0.200000, loss: 3.0555
2022-03-10 10:50:40 - train: epoch 0005, iter [01500, 05004], lr: 0.200000, loss: 3.0993
2022-03-10 10:51:20 - train: epoch 0005, iter [01600, 05004], lr: 0.200000, loss: 2.8123
2022-03-10 10:52:01 - train: epoch 0005, iter [01700, 05004], lr: 0.200000, loss: 3.0480
2022-03-10 10:52:42 - train: epoch 0005, iter [01800, 05004], lr: 0.200000, loss: 3.1754
2022-03-10 10:53:23 - train: epoch 0005, iter [01900, 05004], lr: 0.200000, loss: 3.1049
2022-03-10 10:54:04 - train: epoch 0005, iter [02000, 05004], lr: 0.200000, loss: 3.1796
2022-03-10 10:54:45 - train: epoch 0005, iter [02100, 05004], lr: 0.200000, loss: 2.9832
2022-03-10 10:55:24 - train: epoch 0005, iter [02200, 05004], lr: 0.200000, loss: 3.0247
2022-03-10 10:56:06 - train: epoch 0005, iter [02300, 05004], lr: 0.200000, loss: 2.9210
2022-03-10 10:56:48 - train: epoch 0005, iter [02400, 05004], lr: 0.200000, loss: 3.0541
2022-03-10 10:57:29 - train: epoch 0005, iter [02500, 05004], lr: 0.200000, loss: 3.1040
2022-03-10 10:58:09 - train: epoch 0005, iter [02600, 05004], lr: 0.200000, loss: 3.1060
2022-03-10 10:58:49 - train: epoch 0005, iter [02700, 05004], lr: 0.200000, loss: 3.3358
2022-03-10 10:59:29 - train: epoch 0005, iter [02800, 05004], lr: 0.200000, loss: 2.9844
2022-03-10 11:00:09 - train: epoch 0005, iter [02900, 05004], lr: 0.200000, loss: 3.0163
2022-03-10 11:00:50 - train: epoch 0005, iter [03000, 05004], lr: 0.200000, loss: 2.9918
2022-03-10 11:01:30 - train: epoch 0005, iter [03100, 05004], lr: 0.200000, loss: 3.3256
2022-03-10 11:02:11 - train: epoch 0005, iter [03200, 05004], lr: 0.200000, loss: 3.1690
2022-03-10 11:02:52 - train: epoch 0005, iter [03300, 05004], lr: 0.200000, loss: 2.9967
2022-03-10 11:03:33 - train: epoch 0005, iter [03400, 05004], lr: 0.200000, loss: 3.0441
2022-03-10 11:04:13 - train: epoch 0005, iter [03500, 05004], lr: 0.200000, loss: 2.9265
2022-03-10 11:04:53 - train: epoch 0005, iter [03600, 05004], lr: 0.200000, loss: 3.0354
2022-03-10 11:05:34 - train: epoch 0005, iter [03700, 05004], lr: 0.200000, loss: 2.9818
2022-03-10 11:06:14 - train: epoch 0005, iter [03800, 05004], lr: 0.200000, loss: 3.1411
2022-03-10 11:06:54 - train: epoch 0005, iter [03900, 05004], lr: 0.200000, loss: 3.1531
2022-03-10 11:07:35 - train: epoch 0005, iter [04000, 05004], lr: 0.200000, loss: 3.0247
2022-03-10 11:08:16 - train: epoch 0005, iter [04100, 05004], lr: 0.200000, loss: 3.1164
2022-03-10 11:08:56 - train: epoch 0005, iter [04200, 05004], lr: 0.200000, loss: 3.0313
2022-03-10 11:09:39 - train: epoch 0005, iter [04300, 05004], lr: 0.200000, loss: 2.9696
2022-03-10 11:10:21 - train: epoch 0005, iter [04400, 05004], lr: 0.200000, loss: 3.3320
2022-03-10 11:11:02 - train: epoch 0005, iter [04500, 05004], lr: 0.200000, loss: 3.1095
2022-03-10 11:11:42 - train: epoch 0005, iter [04600, 05004], lr: 0.200000, loss: 3.0607
2022-03-10 11:12:22 - train: epoch 0005, iter [04700, 05004], lr: 0.200000, loss: 2.7668
2022-03-10 11:13:02 - train: epoch 0005, iter [04800, 05004], lr: 0.200000, loss: 2.9260
2022-03-10 11:13:43 - train: epoch 0005, iter [04900, 05004], lr: 0.200000, loss: 3.1646
2022-03-10 11:14:22 - train: epoch 0005, iter [05000, 05004], lr: 0.200000, loss: 2.8958
2022-03-10 11:14:24 - train: epoch 005, train_loss: 3.1190
2022-03-10 11:15:47 - eval: epoch: 005, acc1: 36.300%, acc5: 63.102%, test_loss: 2.8976, per_image_load_time: 2.430ms, per_image_inference_time: 0.259ms
2022-03-10 11:15:48 - until epoch: 005, best_acc1: 36.300%
2022-03-10 11:15:48 - epoch 006 lr: 0.2
2022-03-10 11:16:31 - train: epoch 0006, iter [00100, 05004], lr: 0.200000, loss: 2.7941
2022-03-10 11:17:11 - train: epoch 0006, iter [00200, 05004], lr: 0.200000, loss: 3.1650
2022-03-10 11:17:51 - train: epoch 0006, iter [00300, 05004], lr: 0.200000, loss: 2.8297
2022-03-10 11:18:32 - train: epoch 0006, iter [00400, 05004], lr: 0.200000, loss: 2.9572
2022-03-10 11:19:12 - train: epoch 0006, iter [00500, 05004], lr: 0.200000, loss: 2.9999
2022-03-10 11:19:51 - train: epoch 0006, iter [00600, 05004], lr: 0.200000, loss: 3.0702
2022-03-10 11:20:31 - train: epoch 0006, iter [00700, 05004], lr: 0.200000, loss: 2.9795
2022-03-10 11:21:11 - train: epoch 0006, iter [00800, 05004], lr: 0.200000, loss: 2.9649
2022-03-10 11:21:50 - train: epoch 0006, iter [00900, 05004], lr: 0.200000, loss: 2.8277
2022-03-10 11:22:29 - train: epoch 0006, iter [01000, 05004], lr: 0.200000, loss: 2.8871
2022-03-10 11:23:08 - train: epoch 0006, iter [01100, 05004], lr: 0.200000, loss: 3.0019
2022-03-10 11:23:49 - train: epoch 0006, iter [01200, 05004], lr: 0.200000, loss: 3.0639
2022-03-10 11:24:29 - train: epoch 0006, iter [01300, 05004], lr: 0.200000, loss: 2.9681
2022-03-10 11:25:10 - train: epoch 0006, iter [01400, 05004], lr: 0.200000, loss: 3.0821
2022-03-10 11:25:51 - train: epoch 0006, iter [01500, 05004], lr: 0.200000, loss: 3.1807
2022-03-10 11:26:33 - train: epoch 0006, iter [01600, 05004], lr: 0.200000, loss: 2.6563
2022-03-10 11:27:15 - train: epoch 0006, iter [01700, 05004], lr: 0.200000, loss: 3.1302
2022-03-10 11:27:56 - train: epoch 0006, iter [01800, 05004], lr: 0.200000, loss: 2.8934
2022-03-10 11:28:37 - train: epoch 0006, iter [01900, 05004], lr: 0.200000, loss: 2.9254
2022-03-10 11:29:18 - train: epoch 0006, iter [02000, 05004], lr: 0.200000, loss: 3.2754
2022-03-10 11:29:59 - train: epoch 0006, iter [02100, 05004], lr: 0.200000, loss: 3.1223
2022-03-10 11:30:40 - train: epoch 0006, iter [02200, 05004], lr: 0.200000, loss: 2.6302
2022-03-10 11:31:21 - train: epoch 0006, iter [02300, 05004], lr: 0.200000, loss: 2.8950
2022-03-10 11:32:02 - train: epoch 0006, iter [02400, 05004], lr: 0.200000, loss: 2.8060
2022-03-10 11:32:42 - train: epoch 0006, iter [02500, 05004], lr: 0.200000, loss: 2.9561
2022-03-10 11:33:22 - train: epoch 0006, iter [02600, 05004], lr: 0.200000, loss: 2.9637
2022-03-10 11:34:02 - train: epoch 0006, iter [02700, 05004], lr: 0.200000, loss: 2.9667
2022-03-10 11:34:43 - train: epoch 0006, iter [02800, 05004], lr: 0.200000, loss: 2.8338
2022-03-10 11:35:25 - train: epoch 0006, iter [02900, 05004], lr: 0.200000, loss: 3.0683
2022-03-10 11:36:07 - train: epoch 0006, iter [03000, 05004], lr: 0.200000, loss: 2.8360
2022-03-10 11:36:48 - train: epoch 0006, iter [03100, 05004], lr: 0.200000, loss: 2.8006
2022-03-10 11:37:29 - train: epoch 0006, iter [03200, 05004], lr: 0.200000, loss: 3.0042
2022-03-10 11:38:09 - train: epoch 0006, iter [03300, 05004], lr: 0.200000, loss: 2.9734
2022-03-10 11:38:50 - train: epoch 0006, iter [03400, 05004], lr: 0.200000, loss: 3.1338
2022-03-10 11:39:31 - train: epoch 0006, iter [03500, 05004], lr: 0.200000, loss: 3.0452
2022-03-10 11:40:10 - train: epoch 0006, iter [03600, 05004], lr: 0.200000, loss: 3.1394
2022-03-10 11:40:50 - train: epoch 0006, iter [03700, 05004], lr: 0.200000, loss: 2.8973
2022-03-10 11:41:31 - train: epoch 0006, iter [03800, 05004], lr: 0.200000, loss: 2.5563
2022-03-10 11:42:12 - train: epoch 0006, iter [03900, 05004], lr: 0.200000, loss: 2.8289
2022-03-10 11:42:52 - train: epoch 0006, iter [04000, 05004], lr: 0.200000, loss: 3.0094
2022-03-10 11:43:34 - train: epoch 0006, iter [04100, 05004], lr: 0.200000, loss: 3.0086
2022-03-10 11:44:16 - train: epoch 0006, iter [04200, 05004], lr: 0.200000, loss: 2.7515
2022-03-10 11:44:56 - train: epoch 0006, iter [04300, 05004], lr: 0.200000, loss: 2.8528
2022-03-10 11:45:33 - train: epoch 0006, iter [04400, 05004], lr: 0.200000, loss: 2.9192
2022-03-10 11:46:13 - train: epoch 0006, iter [04500, 05004], lr: 0.200000, loss: 3.0573
2022-03-10 11:46:50 - train: epoch 0006, iter [04600, 05004], lr: 0.200000, loss: 3.1565
2022-03-10 11:47:27 - train: epoch 0006, iter [04700, 05004], lr: 0.200000, loss: 2.9398
2022-03-10 11:48:04 - train: epoch 0006, iter [04800, 05004], lr: 0.200000, loss: 2.8765
2022-03-10 11:48:42 - train: epoch 0006, iter [04900, 05004], lr: 0.200000, loss: 2.9298
2022-03-10 11:49:19 - train: epoch 0006, iter [05000, 05004], lr: 0.200000, loss: 2.8643
2022-03-10 11:49:20 - train: epoch 006, train_loss: 2.9677
2022-03-10 11:50:40 - eval: epoch: 006, acc1: 40.540%, acc5: 66.714%, test_loss: 2.6882, per_image_load_time: 2.891ms, per_image_inference_time: 0.251ms
2022-03-10 11:50:40 - until epoch: 006, best_acc1: 40.540%
2022-03-10 11:50:40 - epoch 007 lr: 0.19994532573409263
2022-03-10 11:51:25 - train: epoch 0007, iter [00100, 05004], lr: 0.199945, loss: 2.8629
2022-03-10 11:52:04 - train: epoch 0007, iter [00200, 05004], lr: 0.199945, loss: 3.1017
2022-03-10 11:52:44 - train: epoch 0007, iter [00300, 05004], lr: 0.199945, loss: 3.1704
2022-03-10 11:53:24 - train: epoch 0007, iter [00400, 05004], lr: 0.199945, loss: 2.8230
2022-03-10 11:54:03 - train: epoch 0007, iter [00500, 05004], lr: 0.199945, loss: 2.7531
2022-03-10 11:54:41 - train: epoch 0007, iter [00600, 05004], lr: 0.199945, loss: 2.8838
2022-03-10 11:55:20 - train: epoch 0007, iter [00700, 05004], lr: 0.199945, loss: 2.7245
2022-03-10 11:55:59 - train: epoch 0007, iter [00800, 05004], lr: 0.199945, loss: 3.0646
2022-03-10 11:56:36 - train: epoch 0007, iter [00900, 05004], lr: 0.199945, loss: 3.0188
2022-03-10 11:57:14 - train: epoch 0007, iter [01000, 05004], lr: 0.199945, loss: 2.8736
2022-03-10 11:57:53 - train: epoch 0007, iter [01100, 05004], lr: 0.199945, loss: 2.9127
2022-03-10 11:58:33 - train: epoch 0007, iter [01200, 05004], lr: 0.199945, loss: 2.8964
2022-03-10 11:59:11 - train: epoch 0007, iter [01300, 05004], lr: 0.199945, loss: 2.7567
2022-03-10 11:59:49 - train: epoch 0007, iter [01400, 05004], lr: 0.199945, loss: 3.0631
2022-03-10 12:00:27 - train: epoch 0007, iter [01500, 05004], lr: 0.199945, loss: 3.0346
2022-03-10 12:01:06 - train: epoch 0007, iter [01600, 05004], lr: 0.199945, loss: 2.6386
2022-03-10 12:01:45 - train: epoch 0007, iter [01700, 05004], lr: 0.199945, loss: 2.8346
2022-03-10 12:02:23 - train: epoch 0007, iter [01800, 05004], lr: 0.199945, loss: 2.8704
2022-03-10 12:03:02 - train: epoch 0007, iter [01900, 05004], lr: 0.199945, loss: 3.0044
2022-03-10 12:03:42 - train: epoch 0007, iter [02000, 05004], lr: 0.199945, loss: 2.8490
2022-03-10 12:04:22 - train: epoch 0007, iter [02100, 05004], lr: 0.199945, loss: 3.1181
2022-03-10 12:05:01 - train: epoch 0007, iter [02200, 05004], lr: 0.199945, loss: 2.6468
2022-03-10 12:05:40 - train: epoch 0007, iter [02300, 05004], lr: 0.199945, loss: 2.9385
2022-03-10 12:06:19 - train: epoch 0007, iter [02400, 05004], lr: 0.199945, loss: 3.1395
2022-03-10 12:06:57 - train: epoch 0007, iter [02500, 05004], lr: 0.199945, loss: 2.8168
2022-03-10 12:07:36 - train: epoch 0007, iter [02600, 05004], lr: 0.199945, loss: 2.6275
2022-03-10 12:08:14 - train: epoch 0007, iter [02700, 05004], lr: 0.199945, loss: 2.7342
2022-03-10 12:08:52 - train: epoch 0007, iter [02800, 05004], lr: 0.199945, loss: 2.6857
2022-03-10 12:09:31 - train: epoch 0007, iter [02900, 05004], lr: 0.199945, loss: 2.8300
2022-03-10 12:10:08 - train: epoch 0007, iter [03000, 05004], lr: 0.199945, loss: 2.8751
2022-03-10 12:10:47 - train: epoch 0007, iter [03100, 05004], lr: 0.199945, loss: 2.5296
2022-03-10 12:11:25 - train: epoch 0007, iter [03200, 05004], lr: 0.199945, loss: 2.7841
2022-03-10 12:12:03 - train: epoch 0007, iter [03300, 05004], lr: 0.199945, loss: 2.9472
2022-03-10 12:12:42 - train: epoch 0007, iter [03400, 05004], lr: 0.199945, loss: 2.8087
2022-03-10 12:13:20 - train: epoch 0007, iter [03500, 05004], lr: 0.199945, loss: 2.7983
2022-03-10 12:13:58 - train: epoch 0007, iter [03600, 05004], lr: 0.199945, loss: 2.7796
2022-03-10 12:14:36 - train: epoch 0007, iter [03700, 05004], lr: 0.199945, loss: 2.7803
2022-03-10 12:15:14 - train: epoch 0007, iter [03800, 05004], lr: 0.199945, loss: 3.0382
2022-03-10 12:15:53 - train: epoch 0007, iter [03900, 05004], lr: 0.199945, loss: 2.9875
2022-03-10 12:16:33 - train: epoch 0007, iter [04000, 05004], lr: 0.199945, loss: 2.9684
2022-03-10 12:17:12 - train: epoch 0007, iter [04100, 05004], lr: 0.199945, loss: 2.8477
2022-03-10 12:17:51 - train: epoch 0007, iter [04200, 05004], lr: 0.199945, loss: 2.7301
2022-03-10 12:18:30 - train: epoch 0007, iter [04300, 05004], lr: 0.199945, loss: 3.0817
2022-03-10 12:19:08 - train: epoch 0007, iter [04400, 05004], lr: 0.199945, loss: 2.6900
2022-03-10 12:19:47 - train: epoch 0007, iter [04500, 05004], lr: 0.199945, loss: 2.9578
2022-03-10 12:20:25 - train: epoch 0007, iter [04600, 05004], lr: 0.199945, loss: 2.7619
2022-03-10 12:21:03 - train: epoch 0007, iter [04700, 05004], lr: 0.199945, loss: 2.8762
2022-03-10 12:21:42 - train: epoch 0007, iter [04800, 05004], lr: 0.199945, loss: 2.9639
2022-03-10 12:22:21 - train: epoch 0007, iter [04900, 05004], lr: 0.199945, loss: 2.9684
2022-03-10 12:23:00 - train: epoch 0007, iter [05000, 05004], lr: 0.199945, loss: 2.8125
2022-03-10 12:23:01 - train: epoch 007, train_loss: 2.8738
2022-03-10 12:24:24 - eval: epoch: 007, acc1: 40.312%, acc5: 66.932%, test_loss: 2.6920, per_image_load_time: 2.980ms, per_image_inference_time: 0.256ms
2022-03-10 12:24:25 - until epoch: 007, best_acc1: 40.540%
2022-03-10 12:24:25 - epoch 008 lr: 0.19978136272187746
2022-03-10 12:25:07 - train: epoch 0008, iter [00100, 05004], lr: 0.199781, loss: 2.8675
2022-03-10 12:25:44 - train: epoch 0008, iter [00200, 05004], lr: 0.199781, loss: 3.0228
2022-03-10 12:26:22 - train: epoch 0008, iter [00300, 05004], lr: 0.199781, loss: 2.7874
2022-03-10 12:27:01 - train: epoch 0008, iter [00400, 05004], lr: 0.199781, loss: 2.7784
2022-03-10 12:27:40 - train: epoch 0008, iter [00500, 05004], lr: 0.199781, loss: 2.8522
2022-03-10 12:28:18 - train: epoch 0008, iter [00600, 05004], lr: 0.199781, loss: 2.9593
2022-03-10 12:28:57 - train: epoch 0008, iter [00700, 05004], lr: 0.199781, loss: 2.8258
2022-03-10 12:29:35 - train: epoch 0008, iter [00800, 05004], lr: 0.199781, loss: 2.5008
2022-03-10 12:30:14 - train: epoch 0008, iter [00900, 05004], lr: 0.199781, loss: 2.7487
2022-03-10 12:30:52 - train: epoch 0008, iter [01000, 05004], lr: 0.199781, loss: 2.9681
2022-03-10 12:31:31 - train: epoch 0008, iter [01100, 05004], lr: 0.199781, loss: 2.9181
2022-03-10 12:32:10 - train: epoch 0008, iter [01200, 05004], lr: 0.199781, loss: 2.7009
2022-03-10 12:32:50 - train: epoch 0008, iter [01300, 05004], lr: 0.199781, loss: 2.9549
2022-03-10 12:33:28 - train: epoch 0008, iter [01400, 05004], lr: 0.199781, loss: 2.8507
2022-03-10 12:34:06 - train: epoch 0008, iter [01500, 05004], lr: 0.199781, loss: 2.9244
2022-03-10 12:34:44 - train: epoch 0008, iter [01600, 05004], lr: 0.199781, loss: 2.8556
2022-03-10 12:35:23 - train: epoch 0008, iter [01700, 05004], lr: 0.199781, loss: 2.8901
2022-03-10 12:36:00 - train: epoch 0008, iter [01800, 05004], lr: 0.199781, loss: 2.8292
2022-03-10 12:36:39 - train: epoch 0008, iter [01900, 05004], lr: 0.199781, loss: 2.7625
2022-03-10 12:37:18 - train: epoch 0008, iter [02000, 05004], lr: 0.199781, loss: 2.9180
2022-03-10 12:37:59 - train: epoch 0008, iter [02100, 05004], lr: 0.199781, loss: 2.9158
2022-03-10 12:38:38 - train: epoch 0008, iter [02200, 05004], lr: 0.199781, loss: 2.5765
2022-03-10 12:39:17 - train: epoch 0008, iter [02300, 05004], lr: 0.199781, loss: 2.7957
2022-03-10 12:39:55 - train: epoch 0008, iter [02400, 05004], lr: 0.199781, loss: 2.7544
2022-03-10 12:40:34 - train: epoch 0008, iter [02500, 05004], lr: 0.199781, loss: 2.8270
2022-03-10 12:41:13 - train: epoch 0008, iter [02600, 05004], lr: 0.199781, loss: 2.9985
2022-03-10 12:41:51 - train: epoch 0008, iter [02700, 05004], lr: 0.199781, loss: 3.0873
2022-03-10 12:42:28 - train: epoch 0008, iter [02800, 05004], lr: 0.199781, loss: 2.9226
2022-03-10 12:43:06 - train: epoch 0008, iter [02900, 05004], lr: 0.199781, loss: 2.9485
2022-03-10 12:43:45 - train: epoch 0008, iter [03000, 05004], lr: 0.199781, loss: 3.0792
2022-03-10 12:44:22 - train: epoch 0008, iter [03100, 05004], lr: 0.199781, loss: 2.8423
2022-03-10 12:45:01 - train: epoch 0008, iter [03200, 05004], lr: 0.199781, loss: 3.1762
2022-03-10 12:45:39 - train: epoch 0008, iter [03300, 05004], lr: 0.199781, loss: 2.8941
2022-03-10 12:46:18 - train: epoch 0008, iter [03400, 05004], lr: 0.199781, loss: 2.8175
2022-03-10 12:46:58 - train: epoch 0008, iter [03500, 05004], lr: 0.199781, loss: 2.9701
2022-03-10 12:47:38 - train: epoch 0008, iter [03600, 05004], lr: 0.199781, loss: 3.1427
2022-03-10 12:48:17 - train: epoch 0008, iter [03700, 05004], lr: 0.199781, loss: 2.5781
2022-03-10 12:48:57 - train: epoch 0008, iter [03800, 05004], lr: 0.199781, loss: 2.8128
2022-03-10 12:49:37 - train: epoch 0008, iter [03900, 05004], lr: 0.199781, loss: 2.9385
2022-03-10 12:50:17 - train: epoch 0008, iter [04000, 05004], lr: 0.199781, loss: 3.0938
2022-03-10 12:50:55 - train: epoch 0008, iter [04100, 05004], lr: 0.199781, loss: 2.6513
2022-03-10 12:51:35 - train: epoch 0008, iter [04200, 05004], lr: 0.199781, loss: 2.9687
2022-03-10 12:52:13 - train: epoch 0008, iter [04300, 05004], lr: 0.199781, loss: 2.7831
2022-03-10 12:52:49 - train: epoch 0008, iter [04400, 05004], lr: 0.199781, loss: 2.7753
2022-03-10 12:53:25 - train: epoch 0008, iter [04500, 05004], lr: 0.199781, loss: 2.7912
2022-03-10 12:54:03 - train: epoch 0008, iter [04600, 05004], lr: 0.199781, loss: 2.7385
2022-03-10 12:54:41 - train: epoch 0008, iter [04700, 05004], lr: 0.199781, loss: 2.8390
2022-03-10 12:55:19 - train: epoch 0008, iter [04800, 05004], lr: 0.199781, loss: 2.7287
2022-03-10 12:55:58 - train: epoch 0008, iter [04900, 05004], lr: 0.199781, loss: 2.9183
2022-03-10 12:56:36 - train: epoch 0008, iter [05000, 05004], lr: 0.199781, loss: 2.5900
2022-03-10 12:56:38 - train: epoch 008, train_loss: 2.8106
2022-03-10 12:57:59 - eval: epoch: 008, acc1: 41.514%, acc5: 67.940%, test_loss: 2.6221, per_image_load_time: 1.853ms, per_image_inference_time: 0.256ms
2022-03-10 12:57:59 - until epoch: 008, best_acc1: 41.514%
2022-03-10 12:57:59 - epoch 009 lr: 0.19950829025450115
2022-03-10 12:58:40 - train: epoch 0009, iter [00100, 05004], lr: 0.199508, loss: 2.6418
2022-03-10 12:59:18 - train: epoch 0009, iter [00200, 05004], lr: 0.199508, loss: 2.8243
2022-03-10 12:59:57 - train: epoch 0009, iter [00300, 05004], lr: 0.199508, loss: 2.5214
2022-03-10 13:00:36 - train: epoch 0009, iter [00400, 05004], lr: 0.199508, loss: 2.8353
2022-03-10 13:01:14 - train: epoch 0009, iter [00500, 05004], lr: 0.199508, loss: 2.7059
2022-03-10 13:01:52 - train: epoch 0009, iter [00600, 05004], lr: 0.199508, loss: 2.6398
2022-03-10 13:02:31 - train: epoch 0009, iter [00700, 05004], lr: 0.199508, loss: 2.6581
2022-03-10 13:03:09 - train: epoch 0009, iter [00800, 05004], lr: 0.199508, loss: 2.7964
2022-03-10 13:03:48 - train: epoch 0009, iter [00900, 05004], lr: 0.199508, loss: 2.5285
2022-03-10 13:04:26 - train: epoch 0009, iter [01000, 05004], lr: 0.199508, loss: 2.6153
2022-03-10 13:05:05 - train: epoch 0009, iter [01100, 05004], lr: 0.199508, loss: 2.8853
2022-03-10 13:05:43 - train: epoch 0009, iter [01200, 05004], lr: 0.199508, loss: 2.8196
2022-03-10 13:06:21 - train: epoch 0009, iter [01300, 05004], lr: 0.199508, loss: 2.9161
2022-03-10 13:06:59 - train: epoch 0009, iter [01400, 05004], lr: 0.199508, loss: 2.6621
2022-03-10 13:07:38 - train: epoch 0009, iter [01500, 05004], lr: 0.199508, loss: 2.7661
2022-03-10 13:08:16 - train: epoch 0009, iter [01600, 05004], lr: 0.199508, loss: 2.6722
2022-03-10 13:08:54 - train: epoch 0009, iter [01700, 05004], lr: 0.199508, loss: 2.7699
2022-03-10 13:09:32 - train: epoch 0009, iter [01800, 05004], lr: 0.199508, loss: 2.5560
2022-03-10 13:10:10 - train: epoch 0009, iter [01900, 05004], lr: 0.199508, loss: 2.4916
2022-03-10 13:10:47 - train: epoch 0009, iter [02000, 05004], lr: 0.199508, loss: 2.7207
2022-03-10 13:11:25 - train: epoch 0009, iter [02100, 05004], lr: 0.199508, loss: 2.8844
2022-03-10 13:12:03 - train: epoch 0009, iter [02200, 05004], lr: 0.199508, loss: 2.9236
2022-03-10 13:12:40 - train: epoch 0009, iter [02300, 05004], lr: 0.199508, loss: 2.7185
2022-03-10 13:13:18 - train: epoch 0009, iter [02400, 05004], lr: 0.199508, loss: 2.8487
2022-03-10 13:13:56 - train: epoch 0009, iter [02500, 05004], lr: 0.199508, loss: 2.5543
2022-03-10 13:14:36 - train: epoch 0009, iter [02600, 05004], lr: 0.199508, loss: 2.8908
2022-03-10 13:15:14 - train: epoch 0009, iter [02700, 05004], lr: 0.199508, loss: 2.4808
2022-03-10 13:15:50 - train: epoch 0009, iter [02800, 05004], lr: 0.199508, loss: 2.8606
2022-03-10 13:16:28 - train: epoch 0009, iter [02900, 05004], lr: 0.199508, loss: 2.6135
2022-03-10 13:17:07 - train: epoch 0009, iter [03000, 05004], lr: 0.199508, loss: 2.6400
2022-03-10 13:17:44 - train: epoch 0009, iter [03100, 05004], lr: 0.199508, loss: 2.8427
2022-03-10 13:18:22 - train: epoch 0009, iter [03200, 05004], lr: 0.199508, loss: 2.7924
2022-03-10 13:18:58 - train: epoch 0009, iter [03300, 05004], lr: 0.199508, loss: 2.9579
2022-03-10 13:19:36 - train: epoch 0009, iter [03400, 05004], lr: 0.199508, loss: 2.8355
2022-03-10 13:20:14 - train: epoch 0009, iter [03500, 05004], lr: 0.199508, loss: 2.5325
2022-03-10 13:20:52 - train: epoch 0009, iter [03600, 05004], lr: 0.199508, loss: 2.6121
2022-03-10 13:21:30 - train: epoch 0009, iter [03700, 05004], lr: 0.199508, loss: 2.8427
2022-03-10 13:22:08 - train: epoch 0009, iter [03800, 05004], lr: 0.199508, loss: 2.7833
2022-03-10 13:22:46 - train: epoch 0009, iter [03900, 05004], lr: 0.199508, loss: 2.7168
2022-03-10 13:23:25 - train: epoch 0009, iter [04000, 05004], lr: 0.199508, loss: 2.8466
2022-03-10 13:24:02 - train: epoch 0009, iter [04100, 05004], lr: 0.199508, loss: 2.7243
2022-03-10 13:24:40 - train: epoch 0009, iter [04200, 05004], lr: 0.199508, loss: 2.5670
2022-03-10 13:25:19 - train: epoch 0009, iter [04300, 05004], lr: 0.199508, loss: 2.5716
2022-03-10 13:25:57 - train: epoch 0009, iter [04400, 05004], lr: 0.199508, loss: 2.8437
2022-03-10 13:26:35 - train: epoch 0009, iter [04500, 05004], lr: 0.199508, loss: 2.7867
2022-03-10 13:27:12 - train: epoch 0009, iter [04600, 05004], lr: 0.199508, loss: 2.9084
2022-03-10 13:27:50 - train: epoch 0009, iter [04700, 05004], lr: 0.199508, loss: 2.8724
2022-03-10 13:28:28 - train: epoch 0009, iter [04800, 05004], lr: 0.199508, loss: 2.6807
2022-03-10 13:29:06 - train: epoch 0009, iter [04900, 05004], lr: 0.199508, loss: 2.9848
2022-03-10 13:29:43 - train: epoch 0009, iter [05000, 05004], lr: 0.199508, loss: 2.8755
2022-03-10 13:29:45 - train: epoch 009, train_loss: 2.7652
2022-03-10 13:31:04 - eval: epoch: 009, acc1: 42.922%, acc5: 69.624%, test_loss: 2.5399, per_image_load_time: 2.128ms, per_image_inference_time: 0.243ms
2022-03-10 13:31:05 - until epoch: 009, best_acc1: 42.922%
2022-03-10 13:31:05 - epoch 010 lr: 0.19912640693269754
2022-03-10 13:31:46 - train: epoch 0010, iter [00100, 05004], lr: 0.199126, loss: 2.6598
2022-03-10 13:32:24 - train: epoch 0010, iter [00200, 05004], lr: 0.199126, loss: 2.7371
2022-03-10 13:33:02 - train: epoch 0010, iter [00300, 05004], lr: 0.199126, loss: 2.8370
2022-03-10 13:33:40 - train: epoch 0010, iter [00400, 05004], lr: 0.199126, loss: 2.8454
2022-03-10 13:34:18 - train: epoch 0010, iter [00500, 05004], lr: 0.199126, loss: 2.5192
2022-03-10 13:34:57 - train: epoch 0010, iter [00600, 05004], lr: 0.199126, loss: 2.7935
2022-03-10 13:35:35 - train: epoch 0010, iter [00700, 05004], lr: 0.199126, loss: 2.8067
2022-03-10 13:36:14 - train: epoch 0010, iter [00800, 05004], lr: 0.199126, loss: 2.6164
2022-03-10 13:36:52 - train: epoch 0010, iter [00900, 05004], lr: 0.199126, loss: 2.5845
2022-03-10 13:37:30 - train: epoch 0010, iter [01000, 05004], lr: 0.199126, loss: 2.6589
2022-03-10 13:38:08 - train: epoch 0010, iter [01100, 05004], lr: 0.199126, loss: 2.5380
2022-03-10 13:38:46 - train: epoch 0010, iter [01200, 05004], lr: 0.199126, loss: 2.7021
2022-03-10 13:39:25 - train: epoch 0010, iter [01300, 05004], lr: 0.199126, loss: 2.7513
2022-03-10 13:40:02 - train: epoch 0010, iter [01400, 05004], lr: 0.199126, loss: 2.7930
2022-03-10 13:40:41 - train: epoch 0010, iter [01500, 05004], lr: 0.199126, loss: 2.6597
2022-03-10 13:41:19 - train: epoch 0010, iter [01600, 05004], lr: 0.199126, loss: 2.7462
2022-03-10 13:41:57 - train: epoch 0010, iter [01700, 05004], lr: 0.199126, loss: 2.7835
2022-03-10 13:42:35 - train: epoch 0010, iter [01800, 05004], lr: 0.199126, loss: 2.4712
2022-03-10 13:43:13 - train: epoch 0010, iter [01900, 05004], lr: 0.199126, loss: 2.6727
2022-03-10 13:43:51 - train: epoch 0010, iter [02000, 05004], lr: 0.199126, loss: 2.7123
2022-03-10 13:44:29 - train: epoch 0010, iter [02100, 05004], lr: 0.199126, loss: 2.6276
2022-03-10 13:45:07 - train: epoch 0010, iter [02200, 05004], lr: 0.199126, loss: 2.7595
2022-03-10 13:45:45 - train: epoch 0010, iter [02300, 05004], lr: 0.199126, loss: 2.7678
2022-03-10 13:46:22 - train: epoch 0010, iter [02400, 05004], lr: 0.199126, loss: 2.8491
2022-03-10 13:47:00 - train: epoch 0010, iter [02500, 05004], lr: 0.199126, loss: 2.8180
2022-03-10 13:47:38 - train: epoch 0010, iter [02600, 05004], lr: 0.199126, loss: 2.9813
2022-03-10 13:48:15 - train: epoch 0010, iter [02700, 05004], lr: 0.199126, loss: 2.4452
2022-03-10 13:48:53 - train: epoch 0010, iter [02800, 05004], lr: 0.199126, loss: 2.8803
2022-03-10 13:49:31 - train: epoch 0010, iter [02900, 05004], lr: 0.199126, loss: 2.7587
2022-03-10 13:50:08 - train: epoch 0010, iter [03000, 05004], lr: 0.199126, loss: 2.6533
2022-03-10 13:50:46 - train: epoch 0010, iter [03100, 05004], lr: 0.199126, loss: 2.6873
2022-03-10 13:51:23 - train: epoch 0010, iter [03200, 05004], lr: 0.199126, loss: 2.8269
2022-03-10 13:52:01 - train: epoch 0010, iter [03300, 05004], lr: 0.199126, loss: 2.7898
2022-03-10 13:52:38 - train: epoch 0010, iter [03400, 05004], lr: 0.199126, loss: 2.7091
2022-03-10 13:53:15 - train: epoch 0010, iter [03500, 05004], lr: 0.199126, loss: 2.8808
2022-03-10 13:53:53 - train: epoch 0010, iter [03600, 05004], lr: 0.199126, loss: 2.7926
2022-03-10 13:54:31 - train: epoch 0010, iter [03700, 05004], lr: 0.199126, loss: 2.6732
2022-03-10 13:55:09 - train: epoch 0010, iter [03800, 05004], lr: 0.199126, loss: 2.6730
2022-03-10 13:55:49 - train: epoch 0010, iter [03900, 05004], lr: 0.199126, loss: 2.5544
2022-03-10 13:56:27 - train: epoch 0010, iter [04000, 05004], lr: 0.199126, loss: 2.6620
2022-03-10 13:57:05 - train: epoch 0010, iter [04100, 05004], lr: 0.199126, loss: 2.6901
2022-03-10 13:57:44 - train: epoch 0010, iter [04200, 05004], lr: 0.199126, loss: 2.6508
2022-03-10 13:58:21 - train: epoch 0010, iter [04300, 05004], lr: 0.199126, loss: 2.7019
2022-03-10 13:59:00 - train: epoch 0010, iter [04400, 05004], lr: 0.199126, loss: 2.7494
2022-03-10 13:59:39 - train: epoch 0010, iter [04500, 05004], lr: 0.199126, loss: 2.7912
2022-03-10 14:00:17 - train: epoch 0010, iter [04600, 05004], lr: 0.199126, loss: 2.6536
2022-03-10 14:00:55 - train: epoch 0010, iter [04700, 05004], lr: 0.199126, loss: 2.8500
2022-03-10 14:01:33 - train: epoch 0010, iter [04800, 05004], lr: 0.199126, loss: 2.5262
2022-03-10 14:02:12 - train: epoch 0010, iter [04900, 05004], lr: 0.199126, loss: 2.7626
2022-03-10 14:02:51 - train: epoch 0010, iter [05000, 05004], lr: 0.199126, loss: 2.3738
2022-03-10 14:02:53 - train: epoch 010, train_loss: 2.7266
2022-03-10 14:04:13 - eval: epoch: 010, acc1: 42.802%, acc5: 69.338%, test_loss: 2.5479, per_image_load_time: 1.618ms, per_image_inference_time: 0.245ms
2022-03-10 14:04:13 - until epoch: 010, best_acc1: 42.922%
2022-03-10 14:04:13 - epoch 011 lr: 0.19863613034027225
2022-03-10 14:04:55 - train: epoch 0011, iter [00100, 05004], lr: 0.198636, loss: 2.6080
2022-03-10 14:05:33 - train: epoch 0011, iter [00200, 05004], lr: 0.198636, loss: 2.8016
2022-03-10 14:06:11 - train: epoch 0011, iter [00300, 05004], lr: 0.198636, loss: 2.6973
2022-03-10 14:06:49 - train: epoch 0011, iter [00400, 05004], lr: 0.198636, loss: 2.8948
2022-03-10 14:07:27 - train: epoch 0011, iter [00500, 05004], lr: 0.198636, loss: 2.8560
2022-03-10 14:08:06 - train: epoch 0011, iter [00600, 05004], lr: 0.198636, loss: 2.7796
2022-03-10 14:08:46 - train: epoch 0011, iter [00700, 05004], lr: 0.198636, loss: 2.8437
2022-03-10 14:09:24 - train: epoch 0011, iter [00800, 05004], lr: 0.198636, loss: 2.5778
2022-03-10 14:10:03 - train: epoch 0011, iter [00900, 05004], lr: 0.198636, loss: 2.6660
2022-03-10 14:10:41 - train: epoch 0011, iter [01000, 05004], lr: 0.198636, loss: 2.4201
2022-03-10 14:11:19 - train: epoch 0011, iter [01100, 05004], lr: 0.198636, loss: 2.7534
2022-03-10 14:11:57 - train: epoch 0011, iter [01200, 05004], lr: 0.198636, loss: 2.8114
2022-03-10 14:12:36 - train: epoch 0011, iter [01300, 05004], lr: 0.198636, loss: 2.6175
2022-03-10 14:13:14 - train: epoch 0011, iter [01400, 05004], lr: 0.198636, loss: 2.6097
2022-03-10 14:13:52 - train: epoch 0011, iter [01500, 05004], lr: 0.198636, loss: 2.5148
2022-03-10 14:14:30 - train: epoch 0011, iter [01600, 05004], lr: 0.198636, loss: 2.7934
2022-03-10 14:15:08 - train: epoch 0011, iter [01700, 05004], lr: 0.198636, loss: 2.5884
2022-03-10 14:15:46 - train: epoch 0011, iter [01800, 05004], lr: 0.198636, loss: 2.5993
2022-03-10 14:16:26 - train: epoch 0011, iter [01900, 05004], lr: 0.198636, loss: 2.7570
2022-03-10 14:17:05 - train: epoch 0011, iter [02000, 05004], lr: 0.198636, loss: 2.7096
2022-03-10 14:17:43 - train: epoch 0011, iter [02100, 05004], lr: 0.198636, loss: 2.6575
2022-03-10 14:18:21 - train: epoch 0011, iter [02200, 05004], lr: 0.198636, loss: 2.5643
2022-03-10 14:19:00 - train: epoch 0011, iter [02300, 05004], lr: 0.198636, loss: 2.8658
2022-03-10 14:19:38 - train: epoch 0011, iter [02400, 05004], lr: 0.198636, loss: 2.5045
2022-03-10 14:20:17 - train: epoch 0011, iter [02500, 05004], lr: 0.198636, loss: 2.6915
2022-03-10 14:20:55 - train: epoch 0011, iter [02600, 05004], lr: 0.198636, loss: 2.8270
2022-03-10 14:21:33 - train: epoch 0011, iter [02700, 05004], lr: 0.198636, loss: 2.6658
2022-03-10 14:22:10 - train: epoch 0011, iter [02800, 05004], lr: 0.198636, loss: 2.2654
2022-03-10 14:22:48 - train: epoch 0011, iter [02900, 05004], lr: 0.198636, loss: 2.7543
2022-03-10 14:23:25 - train: epoch 0011, iter [03000, 05004], lr: 0.198636, loss: 2.9142
2022-03-10 14:24:01 - train: epoch 0011, iter [03100, 05004], lr: 0.198636, loss: 2.5074
2022-03-10 14:24:37 - train: epoch 0011, iter [03200, 05004], lr: 0.198636, loss: 2.5131
2022-03-10 14:25:15 - train: epoch 0011, iter [03300, 05004], lr: 0.198636, loss: 2.8749
2022-03-10 14:25:53 - train: epoch 0011, iter [03400, 05004], lr: 0.198636, loss: 2.7840
2022-03-10 14:26:29 - train: epoch 0011, iter [03500, 05004], lr: 0.198636, loss: 2.6803
2022-03-10 14:27:06 - train: epoch 0011, iter [03600, 05004], lr: 0.198636, loss: 2.6420
2022-03-10 14:27:43 - train: epoch 0011, iter [03700, 05004], lr: 0.198636, loss: 2.9150
2022-03-10 14:28:20 - train: epoch 0011, iter [03800, 05004], lr: 0.198636, loss: 2.6779
2022-03-10 14:28:58 - train: epoch 0011, iter [03900, 05004], lr: 0.198636, loss: 2.6701
2022-03-10 14:29:35 - train: epoch 0011, iter [04000, 05004], lr: 0.198636, loss: 2.5173
2022-03-10 14:30:13 - train: epoch 0011, iter [04100, 05004], lr: 0.198636, loss: 2.4601
2022-03-10 14:30:51 - train: epoch 0011, iter [04200, 05004], lr: 0.198636, loss: 2.7008
2022-03-10 14:31:29 - train: epoch 0011, iter [04300, 05004], lr: 0.198636, loss: 2.6240
2022-03-10 14:32:06 - train: epoch 0011, iter [04400, 05004], lr: 0.198636, loss: 2.6764
2022-03-10 14:32:43 - train: epoch 0011, iter [04500, 05004], lr: 0.198636, loss: 2.6287
2022-03-10 14:33:21 - train: epoch 0011, iter [04600, 05004], lr: 0.198636, loss: 2.5798
2022-03-10 14:33:59 - train: epoch 0011, iter [04700, 05004], lr: 0.198636, loss: 2.3468
2022-03-10 14:34:36 - train: epoch 0011, iter [04800, 05004], lr: 0.198636, loss: 2.6502
2022-03-10 14:35:14 - train: epoch 0011, iter [04900, 05004], lr: 0.198636, loss: 2.6589
2022-03-10 14:35:52 - train: epoch 0011, iter [05000, 05004], lr: 0.198636, loss: 2.6276
2022-03-10 14:35:54 - train: epoch 011, train_loss: 2.6937
2022-03-10 14:37:12 - eval: epoch: 011, acc1: 41.994%, acc5: 69.042%, test_loss: 2.5704, per_image_load_time: 1.656ms, per_image_inference_time: 0.244ms
2022-03-10 14:37:12 - until epoch: 011, best_acc1: 42.922%
2022-03-10 14:37:12 - epoch 012 lr: 0.19803799658748095
2022-03-10 14:37:52 - train: epoch 0012, iter [00100, 05004], lr: 0.198038, loss: 2.5522
2022-03-10 14:38:29 - train: epoch 0012, iter [00200, 05004], lr: 0.198038, loss: 2.4545
2022-03-10 14:39:06 - train: epoch 0012, iter [00300, 05004], lr: 0.198038, loss: 2.5750
2022-03-10 14:39:45 - train: epoch 0012, iter [00400, 05004], lr: 0.198038, loss: 2.6684
2022-03-10 14:40:22 - train: epoch 0012, iter [00500, 05004], lr: 0.198038, loss: 2.7624
2022-03-10 14:41:00 - train: epoch 0012, iter [00600, 05004], lr: 0.198038, loss: 2.4934
2022-03-10 14:41:39 - train: epoch 0012, iter [00700, 05004], lr: 0.198038, loss: 2.6264
2022-03-10 14:42:16 - train: epoch 0012, iter [00800, 05004], lr: 0.198038, loss: 2.7605
2022-03-10 14:42:55 - train: epoch 0012, iter [00900, 05004], lr: 0.198038, loss: 2.5095
2022-03-10 14:43:33 - train: epoch 0012, iter [01000, 05004], lr: 0.198038, loss: 2.4165
2022-03-10 14:44:10 - train: epoch 0012, iter [01100, 05004], lr: 0.198038, loss: 2.8557
2022-03-10 14:44:47 - train: epoch 0012, iter [01200, 05004], lr: 0.198038, loss: 2.5457
2022-03-10 14:45:25 - train: epoch 0012, iter [01300, 05004], lr: 0.198038, loss: 2.6710
2022-03-10 14:46:03 - train: epoch 0012, iter [01400, 05004], lr: 0.198038, loss: 2.5031
2022-03-10 14:46:41 - train: epoch 0012, iter [01500, 05004], lr: 0.198038, loss: 2.5359
2022-03-10 14:47:20 - train: epoch 0012, iter [01600, 05004], lr: 0.198038, loss: 2.6042
2022-03-10 14:47:57 - train: epoch 0012, iter [01700, 05004], lr: 0.198038, loss: 2.6367
2022-03-10 14:48:36 - train: epoch 0012, iter [01800, 05004], lr: 0.198038, loss: 2.5996
2022-03-10 14:49:13 - train: epoch 0012, iter [01900, 05004], lr: 0.198038, loss: 2.8294
2022-03-10 14:49:51 - train: epoch 0012, iter [02000, 05004], lr: 0.198038, loss: 2.8909
2022-03-10 14:50:29 - train: epoch 0012, iter [02100, 05004], lr: 0.198038, loss: 2.7222
2022-03-10 14:51:07 - train: epoch 0012, iter [02200, 05004], lr: 0.198038, loss: 2.7767
2022-03-10 14:51:45 - train: epoch 0012, iter [02300, 05004], lr: 0.198038, loss: 2.5914
2022-03-10 14:52:23 - train: epoch 0012, iter [02400, 05004], lr: 0.198038, loss: 2.7228
2022-03-10 14:53:01 - train: epoch 0012, iter [02500, 05004], lr: 0.198038, loss: 2.4560
2022-03-10 14:53:38 - train: epoch 0012, iter [02600, 05004], lr: 0.198038, loss: 2.6515
2022-03-10 14:54:16 - train: epoch 0012, iter [02700, 05004], lr: 0.198038, loss: 2.7015
2022-03-10 14:54:55 - train: epoch 0012, iter [02800, 05004], lr: 0.198038, loss: 2.6966
2022-03-10 14:55:31 - train: epoch 0012, iter [02900, 05004], lr: 0.198038, loss: 2.6811
2022-03-10 14:56:09 - train: epoch 0012, iter [03000, 05004], lr: 0.198038, loss: 2.3245
2022-03-10 14:56:47 - train: epoch 0012, iter [03100, 05004], lr: 0.198038, loss: 2.5406
2022-03-10 14:57:26 - train: epoch 0012, iter [03200, 05004], lr: 0.198038, loss: 2.4844
2022-03-10 14:58:05 - train: epoch 0012, iter [03300, 05004], lr: 0.198038, loss: 2.5748
2022-03-10 14:58:43 - train: epoch 0012, iter [03400, 05004], lr: 0.198038, loss: 2.5642
2022-03-10 14:59:22 - train: epoch 0012, iter [03500, 05004], lr: 0.198038, loss: 2.7647
2022-03-10 15:00:01 - train: epoch 0012, iter [03600, 05004], lr: 0.198038, loss: 2.5356
2022-03-10 15:00:40 - train: epoch 0012, iter [03700, 05004], lr: 0.198038, loss: 2.7616
2022-03-10 15:01:18 - train: epoch 0012, iter [03800, 05004], lr: 0.198038, loss: 2.6998
2022-03-10 15:01:56 - train: epoch 0012, iter [03900, 05004], lr: 0.198038, loss: 2.6145
2022-03-10 15:02:36 - train: epoch 0012, iter [04000, 05004], lr: 0.198038, loss: 2.7578
2022-03-10 15:03:15 - train: epoch 0012, iter [04100, 05004], lr: 0.198038, loss: 2.5565
2022-03-10 15:03:54 - train: epoch 0012, iter [04200, 05004], lr: 0.198038, loss: 2.5633
2022-03-10 15:04:31 - train: epoch 0012, iter [04300, 05004], lr: 0.198038, loss: 2.7783
2022-03-10 15:05:10 - train: epoch 0012, iter [04400, 05004], lr: 0.198038, loss: 2.5043
2022-03-10 15:05:49 - train: epoch 0012, iter [04500, 05004], lr: 0.198038, loss: 2.5051
2022-03-10 15:06:27 - train: epoch 0012, iter [04600, 05004], lr: 0.198038, loss: 3.0595
2022-03-10 15:07:07 - train: epoch 0012, iter [04700, 05004], lr: 0.198038, loss: 2.5391
2022-03-10 15:07:45 - train: epoch 0012, iter [04800, 05004], lr: 0.198038, loss: 2.6682
2022-03-10 15:08:24 - train: epoch 0012, iter [04900, 05004], lr: 0.198038, loss: 2.4819
2022-03-10 15:09:04 - train: epoch 0012, iter [05000, 05004], lr: 0.198038, loss: 2.6741
2022-03-10 15:09:06 - train: epoch 012, train_loss: 2.6657
2022-03-10 15:10:24 - eval: epoch: 012, acc1: 45.636%, acc5: 71.828%, test_loss: 2.3929, per_image_load_time: 2.811ms, per_image_inference_time: 0.242ms
2022-03-10 15:10:24 - until epoch: 012, best_acc1: 45.636%
2022-03-10 15:10:24 - epoch 013 lr: 0.1973326597248006
2022-03-10 15:11:04 - train: epoch 0013, iter [00100, 05004], lr: 0.197333, loss: 2.6074
2022-03-10 15:11:41 - train: epoch 0013, iter [00200, 05004], lr: 0.197333, loss: 2.4949
2022-03-10 15:12:19 - train: epoch 0013, iter [00300, 05004], lr: 0.197333, loss: 2.4414
2022-03-10 15:12:55 - train: epoch 0013, iter [00400, 05004], lr: 0.197333, loss: 2.6154
2022-03-10 15:13:32 - train: epoch 0013, iter [00500, 05004], lr: 0.197333, loss: 2.5533
2022-03-10 15:14:09 - train: epoch 0013, iter [00600, 05004], lr: 0.197333, loss: 2.5638
2022-03-10 15:14:47 - train: epoch 0013, iter [00700, 05004], lr: 0.197333, loss: 2.4207
2022-03-10 15:15:24 - train: epoch 0013, iter [00800, 05004], lr: 0.197333, loss: 2.6546
2022-03-10 15:16:01 - train: epoch 0013, iter [00900, 05004], lr: 0.197333, loss: 2.4690
2022-03-10 15:16:40 - train: epoch 0013, iter [01000, 05004], lr: 0.197333, loss: 2.7079
2022-03-10 15:17:17 - train: epoch 0013, iter [01100, 05004], lr: 0.197333, loss: 2.5864
2022-03-10 15:17:55 - train: epoch 0013, iter [01200, 05004], lr: 0.197333, loss: 2.7629
2022-03-10 15:18:34 - train: epoch 0013, iter [01300, 05004], lr: 0.197333, loss: 2.6383
2022-03-10 15:19:12 - train: epoch 0013, iter [01400, 05004], lr: 0.197333, loss: 2.7275
2022-03-10 15:19:50 - train: epoch 0013, iter [01500, 05004], lr: 0.197333, loss: 2.5673
2022-03-10 15:20:27 - train: epoch 0013, iter [01600, 05004], lr: 0.197333, loss: 2.5602
2022-03-10 15:21:05 - train: epoch 0013, iter [01700, 05004], lr: 0.197333, loss: 2.5821
2022-03-10 15:21:42 - train: epoch 0013, iter [01800, 05004], lr: 0.197333, loss: 2.5576
2022-03-10 15:22:21 - train: epoch 0013, iter [01900, 05004], lr: 0.197333, loss: 2.6622
2022-03-10 15:23:01 - train: epoch 0013, iter [02000, 05004], lr: 0.197333, loss: 3.0140
2022-03-10 15:23:41 - train: epoch 0013, iter [02100, 05004], lr: 0.197333, loss: 2.8166
2022-03-10 15:24:18 - train: epoch 0013, iter [02200, 05004], lr: 0.197333, loss: 2.3891
2022-03-10 15:24:55 - train: epoch 0013, iter [02300, 05004], lr: 0.197333, loss: 2.6558
2022-03-10 15:25:32 - train: epoch 0013, iter [02400, 05004], lr: 0.197333, loss: 2.6431
2022-03-10 15:26:09 - train: epoch 0013, iter [02500, 05004], lr: 0.197333, loss: 2.5081
2022-03-10 15:26:47 - train: epoch 0013, iter [02600, 05004], lr: 0.197333, loss: 2.4510
2022-03-10 15:27:25 - train: epoch 0013, iter [02700, 05004], lr: 0.197333, loss: 2.5523
2022-03-10 15:28:05 - train: epoch 0013, iter [02800, 05004], lr: 0.197333, loss: 2.7537
2022-03-10 15:28:45 - train: epoch 0013, iter [02900, 05004], lr: 0.197333, loss: 2.6737
2022-03-10 15:29:24 - train: epoch 0013, iter [03000, 05004], lr: 0.197333, loss: 2.6029
2022-03-10 15:30:02 - train: epoch 0013, iter [03100, 05004], lr: 0.197333, loss: 2.6155
2022-03-10 15:30:40 - train: epoch 0013, iter [03200, 05004], lr: 0.197333, loss: 2.7713
2022-03-10 15:31:19 - train: epoch 0013, iter [03300, 05004], lr: 0.197333, loss: 2.4361
2022-03-10 15:31:57 - train: epoch 0013, iter [03400, 05004], lr: 0.197333, loss: 2.5678
2022-03-10 15:32:36 - train: epoch 0013, iter [03500, 05004], lr: 0.197333, loss: 2.4919
2022-03-10 15:33:14 - train: epoch 0013, iter [03600, 05004], lr: 0.197333, loss: 2.6819
2022-03-10 15:33:53 - train: epoch 0013, iter [03700, 05004], lr: 0.197333, loss: 2.4153
2022-03-10 15:34:32 - train: epoch 0013, iter [03800, 05004], lr: 0.197333, loss: 2.7752
2022-03-10 15:35:11 - train: epoch 0013, iter [03900, 05004], lr: 0.197333, loss: 2.9223
2022-03-10 15:35:50 - train: epoch 0013, iter [04000, 05004], lr: 0.197333, loss: 2.5097
2022-03-10 15:36:29 - train: epoch 0013, iter [04100, 05004], lr: 0.197333, loss: 2.4484
2022-03-10 15:37:08 - train: epoch 0013, iter [04200, 05004], lr: 0.197333, loss: 2.5434
2022-03-10 15:37:46 - train: epoch 0013, iter [04300, 05004], lr: 0.197333, loss: 2.6998
2022-03-10 15:38:25 - train: epoch 0013, iter [04400, 05004], lr: 0.197333, loss: 2.5402
2022-03-10 15:39:04 - train: epoch 0013, iter [04500, 05004], lr: 0.197333, loss: 2.5414
2022-03-10 15:39:43 - train: epoch 0013, iter [04600, 05004], lr: 0.197333, loss: 2.5645
2022-03-10 15:40:22 - train: epoch 0013, iter [04700, 05004], lr: 0.197333, loss: 2.5141
2022-03-10 15:41:02 - train: epoch 0013, iter [04800, 05004], lr: 0.197333, loss: 2.6334
2022-03-10 15:41:41 - train: epoch 0013, iter [04900, 05004], lr: 0.197333, loss: 2.9393
2022-03-10 15:42:20 - train: epoch 0013, iter [05000, 05004], lr: 0.197333, loss: 2.8459
2022-03-10 15:42:22 - train: epoch 013, train_loss: 2.6436
2022-03-10 15:43:40 - eval: epoch: 013, acc1: 45.616%, acc5: 72.286%, test_loss: 2.3777, per_image_load_time: 2.784ms, per_image_inference_time: 0.243ms
2022-03-10 15:43:40 - until epoch: 013, best_acc1: 45.636%
2022-03-10 15:43:40 - epoch 014 lr: 0.19652089102773487
2022-03-10 15:44:21 - train: epoch 0014, iter [00100, 05004], lr: 0.196521, loss: 2.5077
2022-03-10 15:44:57 - train: epoch 0014, iter [00200, 05004], lr: 0.196521, loss: 2.7773
2022-03-10 15:45:35 - train: epoch 0014, iter [00300, 05004], lr: 0.196521, loss: 2.3235
2022-03-10 15:46:12 - train: epoch 0014, iter [00400, 05004], lr: 0.196521, loss: 2.6344
2022-03-10 15:46:51 - train: epoch 0014, iter [00500, 05004], lr: 0.196521, loss: 2.3812
2022-03-10 15:47:30 - train: epoch 0014, iter [00600, 05004], lr: 0.196521, loss: 2.4178
2022-03-10 15:48:08 - train: epoch 0014, iter [00700, 05004], lr: 0.196521, loss: 2.7844
2022-03-10 15:48:48 - train: epoch 0014, iter [00800, 05004], lr: 0.196521, loss: 2.4647
2022-03-10 15:49:25 - train: epoch 0014, iter [00900, 05004], lr: 0.196521, loss: 2.5272
2022-03-10 15:50:01 - train: epoch 0014, iter [01000, 05004], lr: 0.196521, loss: 2.7156
2022-03-10 15:50:40 - train: epoch 0014, iter [01100, 05004], lr: 0.196521, loss: 2.3452
2022-03-10 15:51:18 - train: epoch 0014, iter [01200, 05004], lr: 0.196521, loss: 2.3431
2022-03-10 15:51:56 - train: epoch 0014, iter [01300, 05004], lr: 0.196521, loss: 2.7751
2022-03-10 15:52:36 - train: epoch 0014, iter [01400, 05004], lr: 0.196521, loss: 2.6883
2022-03-10 15:53:15 - train: epoch 0014, iter [01500, 05004], lr: 0.196521, loss: 2.6662
2022-03-10 15:53:53 - train: epoch 0014, iter [01600, 05004], lr: 0.196521, loss: 2.5445
2022-03-10 15:54:32 - train: epoch 0014, iter [01700, 05004], lr: 0.196521, loss: 2.6032
2022-03-10 15:55:09 - train: epoch 0014, iter [01800, 05004], lr: 0.196521, loss: 2.7493
2022-03-10 15:55:47 - train: epoch 0014, iter [01900, 05004], lr: 0.196521, loss: 2.5960
2022-03-10 15:56:26 - train: epoch 0014, iter [02000, 05004], lr: 0.196521, loss: 2.5310
2022-03-10 15:57:04 - train: epoch 0014, iter [02100, 05004], lr: 0.196521, loss: 2.5939
2022-03-10 15:57:43 - train: epoch 0014, iter [02200, 05004], lr: 0.196521, loss: 2.6793
2022-03-10 15:58:22 - train: epoch 0014, iter [02300, 05004], lr: 0.196521, loss: 2.6544
2022-03-10 15:58:59 - train: epoch 0014, iter [02400, 05004], lr: 0.196521, loss: 2.5634
2022-03-10 15:59:37 - train: epoch 0014, iter [02500, 05004], lr: 0.196521, loss: 2.6311
2022-03-10 16:00:14 - train: epoch 0014, iter [02600, 05004], lr: 0.196521, loss: 2.6772
2022-03-10 16:00:52 - train: epoch 0014, iter [02700, 05004], lr: 0.196521, loss: 2.4974
2022-03-10 16:01:28 - train: epoch 0014, iter [02800, 05004], lr: 0.196521, loss: 2.7282
2022-03-10 16:02:06 - train: epoch 0014, iter [02900, 05004], lr: 0.196521, loss: 2.3932
2022-03-10 16:02:45 - train: epoch 0014, iter [03000, 05004], lr: 0.196521, loss: 2.8016
2022-03-10 16:03:23 - train: epoch 0014, iter [03100, 05004], lr: 0.196521, loss: 2.6515
2022-03-10 16:04:01 - train: epoch 0014, iter [03200, 05004], lr: 0.196521, loss: 2.8124
2022-03-10 16:04:39 - train: epoch 0014, iter [03300, 05004], lr: 0.196521, loss: 2.4594
2022-03-10 16:05:18 - train: epoch 0014, iter [03400, 05004], lr: 0.196521, loss: 2.4990
2022-03-10 16:05:57 - train: epoch 0014, iter [03500, 05004], lr: 0.196521, loss: 2.4776
2022-03-10 16:06:35 - train: epoch 0014, iter [03600, 05004], lr: 0.196521, loss: 2.4463
2022-03-10 16:07:13 - train: epoch 0014, iter [03700, 05004], lr: 0.196521, loss: 2.4806
2022-03-10 16:07:51 - train: epoch 0014, iter [03800, 05004], lr: 0.196521, loss: 2.6821
2022-03-10 16:08:29 - train: epoch 0014, iter [03900, 05004], lr: 0.196521, loss: 2.5255
2022-03-10 16:09:08 - train: epoch 0014, iter [04000, 05004], lr: 0.196521, loss: 2.7298
2022-03-10 16:09:45 - train: epoch 0014, iter [04100, 05004], lr: 0.196521, loss: 2.6706
2022-03-10 16:10:24 - train: epoch 0014, iter [04200, 05004], lr: 0.196521, loss: 2.4383
2022-03-10 16:11:02 - train: epoch 0014, iter [04300, 05004], lr: 0.196521, loss: 2.4487
2022-03-10 16:11:40 - train: epoch 0014, iter [04400, 05004], lr: 0.196521, loss: 2.6779
2022-03-10 16:12:20 - train: epoch 0014, iter [04500, 05004], lr: 0.196521, loss: 2.6701
2022-03-10 16:12:58 - train: epoch 0014, iter [04600, 05004], lr: 0.196521, loss: 2.7839
2022-03-10 16:13:36 - train: epoch 0014, iter [04700, 05004], lr: 0.196521, loss: 2.6160
2022-03-10 16:14:15 - train: epoch 0014, iter [04800, 05004], lr: 0.196521, loss: 2.5595
2022-03-10 16:14:53 - train: epoch 0014, iter [04900, 05004], lr: 0.196521, loss: 2.5925
2022-03-10 16:15:30 - train: epoch 0014, iter [05000, 05004], lr: 0.196521, loss: 2.5557
2022-03-10 16:15:32 - train: epoch 014, train_loss: 2.6202
2022-03-10 16:16:50 - eval: epoch: 014, acc1: 46.094%, acc5: 72.120%, test_loss: 2.3801, per_image_load_time: 2.102ms, per_image_inference_time: 0.243ms
2022-03-10 16:16:50 - until epoch: 014, best_acc1: 46.094%
2022-03-10 16:16:50 - epoch 015 lr: 0.19560357815343576
2022-03-10 16:17:30 - train: epoch 0015, iter [00100, 05004], lr: 0.195604, loss: 2.3050
2022-03-10 16:18:08 - train: epoch 0015, iter [00200, 05004], lr: 0.195604, loss: 2.7298
2022-03-10 16:18:47 - train: epoch 0015, iter [00300, 05004], lr: 0.195604, loss: 2.8400
2022-03-10 16:19:26 - train: epoch 0015, iter [00400, 05004], lr: 0.195604, loss: 2.6608
2022-03-10 16:20:06 - train: epoch 0015, iter [00500, 05004], lr: 0.195604, loss: 2.7483
2022-03-10 16:20:44 - train: epoch 0015, iter [00600, 05004], lr: 0.195604, loss: 2.7113
2022-03-10 16:21:24 - train: epoch 0015, iter [00700, 05004], lr: 0.195604, loss: 2.8766
2022-03-10 16:22:04 - train: epoch 0015, iter [00800, 05004], lr: 0.195604, loss: 2.4502
2022-03-10 16:22:44 - train: epoch 0015, iter [00900, 05004], lr: 0.195604, loss: 2.5595
2022-03-10 16:23:23 - train: epoch 0015, iter [01000, 05004], lr: 0.195604, loss: 2.3658
2022-03-10 16:24:01 - train: epoch 0015, iter [01100, 05004], lr: 0.195604, loss: 2.3142
2022-03-10 16:24:40 - train: epoch 0015, iter [01200, 05004], lr: 0.195604, loss: 2.5461
2022-03-10 16:25:19 - train: epoch 0015, iter [01300, 05004], lr: 0.195604, loss: 2.6833
2022-03-10 16:25:58 - train: epoch 0015, iter [01400, 05004], lr: 0.195604, loss: 2.6029
2022-03-10 16:26:37 - train: epoch 0015, iter [01500, 05004], lr: 0.195604, loss: 2.6011
2022-03-10 16:27:15 - train: epoch 0015, iter [01600, 05004], lr: 0.195604, loss: 2.6017
2022-03-10 16:27:55 - train: epoch 0015, iter [01700, 05004], lr: 0.195604, loss: 2.6299
2022-03-10 16:28:34 - train: epoch 0015, iter [01800, 05004], lr: 0.195604, loss: 2.6193
2022-03-10 16:29:13 - train: epoch 0015, iter [01900, 05004], lr: 0.195604, loss: 2.7180
2022-03-10 16:29:50 - train: epoch 0015, iter [02000, 05004], lr: 0.195604, loss: 2.6671
2022-03-10 16:30:30 - train: epoch 0015, iter [02100, 05004], lr: 0.195604, loss: 2.5845
2022-03-10 16:31:09 - train: epoch 0015, iter [02200, 05004], lr: 0.195604, loss: 2.7682
2022-03-10 16:31:49 - train: epoch 0015, iter [02300, 05004], lr: 0.195604, loss: 2.4825
2022-03-10 16:32:28 - train: epoch 0015, iter [02400, 05004], lr: 0.195604, loss: 2.4777
2022-03-10 16:33:08 - train: epoch 0015, iter [02500, 05004], lr: 0.195604, loss: 2.9006
2022-03-10 16:33:48 - train: epoch 0015, iter [02600, 05004], lr: 0.195604, loss: 2.4059
2022-03-10 16:34:28 - train: epoch 0015, iter [02700, 05004], lr: 0.195604, loss: 2.8305
2022-03-10 16:35:08 - train: epoch 0015, iter [02800, 05004], lr: 0.195604, loss: 2.5520
2022-03-10 16:35:47 - train: epoch 0015, iter [02900, 05004], lr: 0.195604, loss: 2.5119
2022-03-10 16:36:24 - train: epoch 0015, iter [03000, 05004], lr: 0.195604, loss: 2.5279
2022-03-10 16:37:01 - train: epoch 0015, iter [03100, 05004], lr: 0.195604, loss: 2.4762
2022-03-10 16:37:39 - train: epoch 0015, iter [03200, 05004], lr: 0.195604, loss: 2.5995
2022-03-10 16:38:16 - train: epoch 0015, iter [03300, 05004], lr: 0.195604, loss: 2.5066
2022-03-10 16:38:53 - train: epoch 0015, iter [03400, 05004], lr: 0.195604, loss: 2.5838
2022-03-10 16:39:30 - train: epoch 0015, iter [03500, 05004], lr: 0.195604, loss: 2.5192
2022-03-10 16:40:09 - train: epoch 0015, iter [03600, 05004], lr: 0.195604, loss: 2.7206
2022-03-10 16:40:48 - train: epoch 0015, iter [03700, 05004], lr: 0.195604, loss: 2.4815
2022-03-10 16:41:28 - train: epoch 0015, iter [03800, 05004], lr: 0.195604, loss: 2.5314
2022-03-10 16:42:09 - train: epoch 0015, iter [03900, 05004], lr: 0.195604, loss: 2.8215
2022-03-10 16:42:47 - train: epoch 0015, iter [04000, 05004], lr: 0.195604, loss: 2.7498
2022-03-10 16:43:26 - train: epoch 0015, iter [04100, 05004], lr: 0.195604, loss: 2.4169
2022-03-10 16:44:07 - train: epoch 0015, iter [04200, 05004], lr: 0.195604, loss: 2.3904
2022-03-10 16:44:49 - train: epoch 0015, iter [04300, 05004], lr: 0.195604, loss: 2.4225
2022-03-10 16:45:30 - train: epoch 0015, iter [04400, 05004], lr: 0.195604, loss: 2.5242
2022-03-10 16:46:09 - train: epoch 0015, iter [04500, 05004], lr: 0.195604, loss: 2.7025
2022-03-10 16:46:49 - train: epoch 0015, iter [04600, 05004], lr: 0.195604, loss: 2.6724
2022-03-10 16:47:29 - train: epoch 0015, iter [04700, 05004], lr: 0.195604, loss: 2.5417
2022-03-10 16:48:10 - train: epoch 0015, iter [04800, 05004], lr: 0.195604, loss: 2.6877
2022-03-10 16:48:55 - train: epoch 0015, iter [04900, 05004], lr: 0.195604, loss: 2.5908
2022-03-10 16:49:39 - train: epoch 0015, iter [05000, 05004], lr: 0.195604, loss: 2.6641
2022-03-10 16:49:41 - train: epoch 015, train_loss: 2.6062
2022-03-10 16:51:12 - eval: epoch: 015, acc1: 45.938%, acc5: 72.028%, test_loss: 2.3746, per_image_load_time: 3.108ms, per_image_inference_time: 0.259ms
2022-03-10 16:51:13 - until epoch: 015, best_acc1: 46.094%
2022-03-10 16:51:13 - epoch 016 lr: 0.19458172417006347
2022-03-10 16:51:55 - train: epoch 0016, iter [00100, 05004], lr: 0.194582, loss: 2.5160
2022-03-10 16:52:36 - train: epoch 0016, iter [00200, 05004], lr: 0.194582, loss: 2.5161
2022-03-10 16:53:18 - train: epoch 0016, iter [00300, 05004], lr: 0.194582, loss: 2.5038
2022-03-10 16:54:01 - train: epoch 0016, iter [00400, 05004], lr: 0.194582, loss: 2.7096
2022-03-10 16:54:45 - train: epoch 0016, iter [00500, 05004], lr: 0.194582, loss: 2.3222
2022-03-10 16:55:29 - train: epoch 0016, iter [00600, 05004], lr: 0.194582, loss: 2.4853
2022-03-10 16:56:12 - train: epoch 0016, iter [00700, 05004], lr: 0.194582, loss: 2.4308
2022-03-10 16:56:55 - train: epoch 0016, iter [00800, 05004], lr: 0.194582, loss: 2.5533
2022-03-10 16:57:40 - train: epoch 0016, iter [00900, 05004], lr: 0.194582, loss: 2.7817
2022-03-10 16:58:21 - train: epoch 0016, iter [01000, 05004], lr: 0.194582, loss: 2.4349
2022-03-10 16:59:02 - train: epoch 0016, iter [01100, 05004], lr: 0.194582, loss: 2.5292
2022-03-10 16:59:42 - train: epoch 0016, iter [01200, 05004], lr: 0.194582, loss: 2.4154
2022-03-10 17:00:23 - train: epoch 0016, iter [01300, 05004], lr: 0.194582, loss: 2.4855
2022-03-10 17:01:04 - train: epoch 0016, iter [01400, 05004], lr: 0.194582, loss: 2.4718
2022-03-10 17:01:47 - train: epoch 0016, iter [01500, 05004], lr: 0.194582, loss: 2.7077
2022-03-10 17:02:29 - train: epoch 0016, iter [01600, 05004], lr: 0.194582, loss: 2.5558
2022-03-10 17:03:10 - train: epoch 0016, iter [01700, 05004], lr: 0.194582, loss: 2.5887
2022-03-10 17:03:53 - train: epoch 0016, iter [01800, 05004], lr: 0.194582, loss: 2.7004
2022-03-10 17:04:35 - train: epoch 0016, iter [01900, 05004], lr: 0.194582, loss: 2.6636
2022-03-10 17:05:19 - train: epoch 0016, iter [02000, 05004], lr: 0.194582, loss: 2.3437
2022-03-10 17:06:05 - train: epoch 0016, iter [02100, 05004], lr: 0.194582, loss: 2.5714
2022-03-10 17:06:50 - train: epoch 0016, iter [02200, 05004], lr: 0.194582, loss: 2.4878
2022-03-10 17:07:34 - train: epoch 0016, iter [02300, 05004], lr: 0.194582, loss: 2.6307
2022-03-10 17:08:18 - train: epoch 0016, iter [02400, 05004], lr: 0.194582, loss: 2.5777
2022-03-10 17:08:58 - train: epoch 0016, iter [02500, 05004], lr: 0.194582, loss: 2.4695
2022-03-10 17:09:37 - train: epoch 0016, iter [02600, 05004], lr: 0.194582, loss: 2.9760
2022-03-10 17:10:16 - train: epoch 0016, iter [02700, 05004], lr: 0.194582, loss: 2.3478
2022-03-10 17:10:53 - train: epoch 0016, iter [02800, 05004], lr: 0.194582, loss: 2.6189
2022-03-10 17:11:31 - train: epoch 0016, iter [02900, 05004], lr: 0.194582, loss: 2.6661
2022-03-10 17:12:11 - train: epoch 0016, iter [03000, 05004], lr: 0.194582, loss: 2.7400
2022-03-10 17:12:50 - train: epoch 0016, iter [03100, 05004], lr: 0.194582, loss: 2.5622
2022-03-10 17:13:30 - train: epoch 0016, iter [03200, 05004], lr: 0.194582, loss: 2.6438
2022-03-10 17:14:11 - train: epoch 0016, iter [03300, 05004], lr: 0.194582, loss: 2.6353
2022-03-10 17:14:50 - train: epoch 0016, iter [03400, 05004], lr: 0.194582, loss: 2.5613
2022-03-10 17:15:29 - train: epoch 0016, iter [03500, 05004], lr: 0.194582, loss: 2.6151
2022-03-10 17:16:09 - train: epoch 0016, iter [03600, 05004], lr: 0.194582, loss: 2.3535
2022-03-10 17:16:50 - train: epoch 0016, iter [03700, 05004], lr: 0.194582, loss: 2.6622
2022-03-10 17:17:30 - train: epoch 0016, iter [03800, 05004], lr: 0.194582, loss: 2.9754
2022-03-10 17:18:09 - train: epoch 0016, iter [03900, 05004], lr: 0.194582, loss: 2.6033
2022-03-10 17:18:50 - train: epoch 0016, iter [04000, 05004], lr: 0.194582, loss: 2.6336
2022-03-10 17:19:32 - train: epoch 0016, iter [04100, 05004], lr: 0.194582, loss: 2.5132
2022-03-10 17:20:16 - train: epoch 0016, iter [04200, 05004], lr: 0.194582, loss: 2.3700
2022-03-10 17:21:00 - train: epoch 0016, iter [04300, 05004], lr: 0.194582, loss: 2.4041
2022-03-10 17:21:45 - train: epoch 0016, iter [04400, 05004], lr: 0.194582, loss: 2.5537
2022-03-10 17:22:29 - train: epoch 0016, iter [04500, 05004], lr: 0.194582, loss: 2.5824
2022-03-10 17:23:15 - train: epoch 0016, iter [04600, 05004], lr: 0.194582, loss: 2.5563
2022-03-10 17:23:59 - train: epoch 0016, iter [04700, 05004], lr: 0.194582, loss: 2.5639
2022-03-10 17:24:45 - train: epoch 0016, iter [04800, 05004], lr: 0.194582, loss: 2.5569
2022-03-10 17:25:25 - train: epoch 0016, iter [04900, 05004], lr: 0.194582, loss: 2.7147
2022-03-10 17:26:06 - train: epoch 0016, iter [05000, 05004], lr: 0.194582, loss: 2.4904
2022-03-10 17:26:08 - train: epoch 016, train_loss: 2.5867
2022-03-10 17:27:32 - eval: epoch: 016, acc1: 45.482%, acc5: 71.956%, test_loss: 2.3987, per_image_load_time: 2.976ms, per_image_inference_time: 0.244ms
2022-03-10 17:27:32 - until epoch: 016, best_acc1: 46.094%
2022-03-10 17:27:32 - epoch 017 lr: 0.19345644645994609
2022-03-10 17:28:13 - train: epoch 0017, iter [00100, 05004], lr: 0.193456, loss: 2.5492
2022-03-10 17:28:54 - train: epoch 0017, iter [00200, 05004], lr: 0.193456, loss: 2.6470
2022-03-10 17:29:33 - train: epoch 0017, iter [00300, 05004], lr: 0.193456, loss: 2.8185
2022-03-10 17:30:12 - train: epoch 0017, iter [00400, 05004], lr: 0.193456, loss: 2.3996
2022-03-10 17:30:52 - train: epoch 0017, iter [00500, 05004], lr: 0.193456, loss: 2.6823
2022-03-10 17:31:32 - train: epoch 0017, iter [00600, 05004], lr: 0.193456, loss: 2.6797
2022-03-10 17:32:13 - train: epoch 0017, iter [00700, 05004], lr: 0.193456, loss: 2.6795
2022-03-10 17:32:54 - train: epoch 0017, iter [00800, 05004], lr: 0.193456, loss: 2.6822
2022-03-10 17:33:35 - train: epoch 0017, iter [00900, 05004], lr: 0.193456, loss: 2.6571
2022-03-10 17:34:16 - train: epoch 0017, iter [01000, 05004], lr: 0.193456, loss: 2.5691
2022-03-10 17:34:57 - train: epoch 0017, iter [01100, 05004], lr: 0.193456, loss: 2.8237
2022-03-10 17:35:40 - train: epoch 0017, iter [01200, 05004], lr: 0.193456, loss: 2.5615
2022-03-10 17:36:21 - train: epoch 0017, iter [01300, 05004], lr: 0.193456, loss: 2.5352
2022-03-10 17:37:03 - train: epoch 0017, iter [01400, 05004], lr: 0.193456, loss: 2.6250
2022-03-10 17:37:43 - train: epoch 0017, iter [01500, 05004], lr: 0.193456, loss: 2.4041
2022-03-10 17:38:25 - train: epoch 0017, iter [01600, 05004], lr: 0.193456, loss: 2.6423
2022-03-10 17:39:05 - train: epoch 0017, iter [01700, 05004], lr: 0.193456, loss: 2.3772
2022-03-10 17:39:44 - train: epoch 0017, iter [01800, 05004], lr: 0.193456, loss: 2.6212
2022-03-10 17:40:22 - train: epoch 0017, iter [01900, 05004], lr: 0.193456, loss: 2.3976
2022-03-10 17:41:00 - train: epoch 0017, iter [02000, 05004], lr: 0.193456, loss: 2.4806
2022-03-10 17:41:41 - train: epoch 0017, iter [02100, 05004], lr: 0.193456, loss: 2.5841
2022-03-10 17:42:18 - train: epoch 0017, iter [02200, 05004], lr: 0.193456, loss: 2.5296
2022-03-10 17:42:55 - train: epoch 0017, iter [02300, 05004], lr: 0.193456, loss: 2.6464
2022-03-10 17:43:32 - train: epoch 0017, iter [02400, 05004], lr: 0.193456, loss: 2.5866
2022-03-10 17:44:10 - train: epoch 0017, iter [02500, 05004], lr: 0.193456, loss: 2.5640
2022-03-10 17:44:49 - train: epoch 0017, iter [02600, 05004], lr: 0.193456, loss: 2.4946
2022-03-10 17:45:27 - train: epoch 0017, iter [02700, 05004], lr: 0.193456, loss: 2.5350
2022-03-10 17:46:07 - train: epoch 0017, iter [02800, 05004], lr: 0.193456, loss: 2.5136
2022-03-10 17:46:48 - train: epoch 0017, iter [02900, 05004], lr: 0.193456, loss: 2.7643
2022-03-10 17:47:29 - train: epoch 0017, iter [03000, 05004], lr: 0.193456, loss: 2.4936
2022-03-10 17:48:10 - train: epoch 0017, iter [03100, 05004], lr: 0.193456, loss: 2.5626
2022-03-10 17:48:51 - train: epoch 0017, iter [03200, 05004], lr: 0.193456, loss: 2.4369
2022-03-10 17:49:29 - train: epoch 0017, iter [03300, 05004], lr: 0.193456, loss: 2.5595
2022-03-10 17:50:08 - train: epoch 0017, iter [03400, 05004], lr: 0.193456, loss: 2.5435
2022-03-10 17:50:50 - train: epoch 0017, iter [03500, 05004], lr: 0.193456, loss: 2.5503
2022-03-10 17:51:32 - train: epoch 0017, iter [03600, 05004], lr: 0.193456, loss: 2.7192
2022-03-10 17:52:15 - train: epoch 0017, iter [03700, 05004], lr: 0.193456, loss: 2.6244
2022-03-10 17:53:00 - train: epoch 0017, iter [03800, 05004], lr: 0.193456, loss: 2.8743
2022-03-10 17:53:40 - train: epoch 0017, iter [03900, 05004], lr: 0.193456, loss: 2.4571
2022-03-10 17:54:22 - train: epoch 0017, iter [04000, 05004], lr: 0.193456, loss: 2.5971
2022-03-10 17:55:01 - train: epoch 0017, iter [04100, 05004], lr: 0.193456, loss: 2.4955
2022-03-10 17:55:40 - train: epoch 0017, iter [04200, 05004], lr: 0.193456, loss: 2.4826
2022-03-10 17:56:18 - train: epoch 0017, iter [04300, 05004], lr: 0.193456, loss: 2.6522
2022-03-10 17:56:58 - train: epoch 0017, iter [04400, 05004], lr: 0.193456, loss: 2.5992
2022-03-10 17:57:39 - train: epoch 0017, iter [04500, 05004], lr: 0.193456, loss: 2.7100
2022-03-10 17:58:18 - train: epoch 0017, iter [04600, 05004], lr: 0.193456, loss: 2.3874
2022-03-10 17:58:58 - train: epoch 0017, iter [04700, 05004], lr: 0.193456, loss: 2.8736
2022-03-10 17:59:38 - train: epoch 0017, iter [04800, 05004], lr: 0.193456, loss: 2.5416
2022-03-10 18:00:18 - train: epoch 0017, iter [04900, 05004], lr: 0.193456, loss: 2.6047
2022-03-10 18:00:58 - train: epoch 0017, iter [05000, 05004], lr: 0.193456, loss: 2.2710
2022-03-10 18:00:59 - train: epoch 017, train_loss: 2.5710
2022-03-10 18:02:24 - eval: epoch: 017, acc1: 46.498%, acc5: 72.878%, test_loss: 2.3315, per_image_load_time: 1.487ms, per_image_inference_time: 0.256ms
2022-03-10 18:02:24 - until epoch: 017, best_acc1: 46.498%
2022-03-10 18:02:24 - epoch 018 lr: 0.19222897549773849
2022-03-10 18:03:09 - train: epoch 0018, iter [00100, 05004], lr: 0.192229, loss: 2.4892
2022-03-10 18:03:49 - train: epoch 0018, iter [00200, 05004], lr: 0.192229, loss: 2.5509
2022-03-10 18:04:29 - train: epoch 0018, iter [00300, 05004], lr: 0.192229, loss: 2.7528
2022-03-10 18:05:09 - train: epoch 0018, iter [00400, 05004], lr: 0.192229, loss: 2.9025
2022-03-10 18:05:48 - train: epoch 0018, iter [00500, 05004], lr: 0.192229, loss: 2.6487
2022-03-10 18:06:29 - train: epoch 0018, iter [00600, 05004], lr: 0.192229, loss: 2.4563
2022-03-10 18:07:08 - train: epoch 0018, iter [00700, 05004], lr: 0.192229, loss: 2.6084
2022-03-10 18:07:47 - train: epoch 0018, iter [00800, 05004], lr: 0.192229, loss: 2.5224
2022-03-10 18:08:27 - train: epoch 0018, iter [00900, 05004], lr: 0.192229, loss: 2.6075
2022-03-10 18:09:08 - train: epoch 0018, iter [01000, 05004], lr: 0.192229, loss: 2.5237
2022-03-10 18:09:47 - train: epoch 0018, iter [01100, 05004], lr: 0.192229, loss: 2.7208
2022-03-10 18:10:26 - train: epoch 0018, iter [01200, 05004], lr: 0.192229, loss: 2.5581
2022-03-10 18:11:05 - train: epoch 0018, iter [01300, 05004], lr: 0.192229, loss: 2.8028
2022-03-10 18:11:45 - train: epoch 0018, iter [01400, 05004], lr: 0.192229, loss: 2.5993
2022-03-10 18:12:25 - train: epoch 0018, iter [01500, 05004], lr: 0.192229, loss: 2.8702
2022-03-10 18:13:04 - train: epoch 0018, iter [01600, 05004], lr: 0.192229, loss: 2.4364
2022-03-10 18:13:43 - train: epoch 0018, iter [01700, 05004], lr: 0.192229, loss: 2.6117
2022-03-10 18:14:25 - train: epoch 0018, iter [01800, 05004], lr: 0.192229, loss: 2.5790
2022-03-10 18:15:05 - train: epoch 0018, iter [01900, 05004], lr: 0.192229, loss: 2.5688
2022-03-10 18:15:45 - train: epoch 0018, iter [02000, 05004], lr: 0.192229, loss: 2.8661
2022-03-10 18:16:28 - train: epoch 0018, iter [02100, 05004], lr: 0.192229, loss: 2.8702
2022-03-10 18:17:10 - train: epoch 0018, iter [02200, 05004], lr: 0.192229, loss: 2.6361
2022-03-10 18:17:51 - train: epoch 0018, iter [02300, 05004], lr: 0.192229, loss: 2.5281
2022-03-10 18:18:34 - train: epoch 0018, iter [02400, 05004], lr: 0.192229, loss: 2.4509
2022-03-10 18:19:17 - train: epoch 0018, iter [02500, 05004], lr: 0.192229, loss: 2.2954
2022-03-10 18:20:00 - train: epoch 0018, iter [02600, 05004], lr: 0.192229, loss: 2.4796
2022-03-10 18:20:43 - train: epoch 0018, iter [02700, 05004], lr: 0.192229, loss: 2.6287
2022-03-10 18:21:27 - train: epoch 0018, iter [02800, 05004], lr: 0.192229, loss: 2.4309
2022-03-10 18:22:09 - train: epoch 0018, iter [02900, 05004], lr: 0.192229, loss: 2.6625
2022-03-10 18:22:52 - train: epoch 0018, iter [03000, 05004], lr: 0.192229, loss: 2.6656
2022-03-10 18:23:34 - train: epoch 0018, iter [03100, 05004], lr: 0.192229, loss: 2.9154
2022-03-10 18:24:17 - train: epoch 0018, iter [03200, 05004], lr: 0.192229, loss: 2.5817
2022-03-10 18:24:57 - train: epoch 0018, iter [03300, 05004], lr: 0.192229, loss: 2.4616
2022-03-10 18:25:39 - train: epoch 0018, iter [03400, 05004], lr: 0.192229, loss: 2.6196
2022-03-10 18:26:18 - train: epoch 0018, iter [03500, 05004], lr: 0.192229, loss: 2.5106
2022-03-10 18:26:57 - train: epoch 0018, iter [03600, 05004], lr: 0.192229, loss: 2.4185
2022-03-10 18:27:36 - train: epoch 0018, iter [03700, 05004], lr: 0.192229, loss: 3.0029
2022-03-10 18:28:15 - train: epoch 0018, iter [03800, 05004], lr: 0.192229, loss: 2.5983
2022-03-10 18:28:54 - train: epoch 0018, iter [03900, 05004], lr: 0.192229, loss: 2.4985
2022-03-10 18:29:33 - train: epoch 0018, iter [04000, 05004], lr: 0.192229, loss: 2.8083
2022-03-10 18:30:11 - train: epoch 0018, iter [04100, 05004], lr: 0.192229, loss: 2.5808
2022-03-10 18:30:51 - train: epoch 0018, iter [04200, 05004], lr: 0.192229, loss: 2.5052
2022-03-10 18:31:30 - train: epoch 0018, iter [04300, 05004], lr: 0.192229, loss: 2.4031
2022-03-10 18:32:10 - train: epoch 0018, iter [04400, 05004], lr: 0.192229, loss: 2.5939
2022-03-10 18:32:50 - train: epoch 0018, iter [04500, 05004], lr: 0.192229, loss: 2.7905
2022-03-10 18:33:30 - train: epoch 0018, iter [04600, 05004], lr: 0.192229, loss: 2.5411
2022-03-10 18:34:10 - train: epoch 0018, iter [04700, 05004], lr: 0.192229, loss: 2.7514
2022-03-10 18:34:49 - train: epoch 0018, iter [04800, 05004], lr: 0.192229, loss: 2.5192
2022-03-10 18:35:30 - train: epoch 0018, iter [04900, 05004], lr: 0.192229, loss: 2.6483
2022-03-10 18:36:10 - train: epoch 0018, iter [05000, 05004], lr: 0.192229, loss: 2.5393
2022-03-10 18:36:11 - train: epoch 018, train_loss: 2.5591
2022-03-10 18:37:36 - eval: epoch: 018, acc1: 45.962%, acc5: 72.364%, test_loss: 2.3690, per_image_load_time: 2.977ms, per_image_inference_time: 0.248ms
2022-03-10 18:37:36 - until epoch: 018, best_acc1: 46.498%
2022-03-10 18:37:36 - epoch 019 lr: 0.19090065350491625
2022-03-10 18:38:19 - train: epoch 0019, iter [00100, 05004], lr: 0.190901, loss: 2.3097
2022-03-10 18:38:59 - train: epoch 0019, iter [00200, 05004], lr: 0.190901, loss: 2.6674
2022-03-10 18:39:39 - train: epoch 0019, iter [00300, 05004], lr: 0.190901, loss: 2.5955
2022-03-10 18:40:20 - train: epoch 0019, iter [00400, 05004], lr: 0.190901, loss: 2.5112
2022-03-10 18:40:59 - train: epoch 0019, iter [00500, 05004], lr: 0.190901, loss: 2.4519
2022-03-10 18:41:40 - train: epoch 0019, iter [00600, 05004], lr: 0.190901, loss: 2.6817
2022-03-10 18:42:19 - train: epoch 0019, iter [00700, 05004], lr: 0.190901, loss: 2.3223
2022-03-10 18:42:59 - train: epoch 0019, iter [00800, 05004], lr: 0.190901, loss: 2.7525
2022-03-10 18:43:39 - train: epoch 0019, iter [00900, 05004], lr: 0.190901, loss: 2.4594
2022-03-10 18:44:20 - train: epoch 0019, iter [01000, 05004], lr: 0.190901, loss: 2.6222
2022-03-10 18:45:02 - train: epoch 0019, iter [01100, 05004], lr: 0.190901, loss: 2.3728
2022-03-10 18:45:45 - train: epoch 0019, iter [01200, 05004], lr: 0.190901, loss: 2.3334
2022-03-10 18:46:25 - train: epoch 0019, iter [01300, 05004], lr: 0.190901, loss: 2.5220
2022-03-10 18:47:06 - train: epoch 0019, iter [01400, 05004], lr: 0.190901, loss: 2.4653
2022-03-10 18:47:48 - train: epoch 0019, iter [01500, 05004], lr: 0.190901, loss: 2.7585
2022-03-10 18:48:28 - train: epoch 0019, iter [01600, 05004], lr: 0.190901, loss: 2.5512
2022-03-10 18:49:09 - train: epoch 0019, iter [01700, 05004], lr: 0.190901, loss: 2.6009
2022-03-10 18:49:51 - train: epoch 0019, iter [01800, 05004], lr: 0.190901, loss: 2.4418
2022-03-10 18:50:32 - train: epoch 0019, iter [01900, 05004], lr: 0.190901, loss: 2.8040
2022-03-10 18:51:13 - train: epoch 0019, iter [02000, 05004], lr: 0.190901, loss: 2.4528
2022-03-10 18:51:54 - train: epoch 0019, iter [02100, 05004], lr: 0.190901, loss: 2.5302
2022-03-10 18:52:35 - train: epoch 0019, iter [02200, 05004], lr: 0.190901, loss: 2.6292
2022-03-10 18:53:16 - train: epoch 0019, iter [02300, 05004], lr: 0.190901, loss: 2.8613
2022-03-10 18:53:57 - train: epoch 0019, iter [02400, 05004], lr: 0.190901, loss: 2.6324
2022-03-10 18:54:38 - train: epoch 0019, iter [02500, 05004], lr: 0.190901, loss: 2.6697
2022-03-10 18:55:20 - train: epoch 0019, iter [02600, 05004], lr: 0.190901, loss: 2.5683
2022-03-10 18:56:01 - train: epoch 0019, iter [02700, 05004], lr: 0.190901, loss: 2.3209
2022-03-10 18:56:43 - train: epoch 0019, iter [02800, 05004], lr: 0.190901, loss: 2.4124
2022-03-10 18:57:24 - train: epoch 0019, iter [02900, 05004], lr: 0.190901, loss: 2.6708
2022-03-10 18:58:06 - train: epoch 0019, iter [03000, 05004], lr: 0.190901, loss: 2.6121
2022-03-10 18:58:47 - train: epoch 0019, iter [03100, 05004], lr: 0.190901, loss: 2.7987
2022-03-10 18:59:27 - train: epoch 0019, iter [03200, 05004], lr: 0.190901, loss: 2.1616
2022-03-10 19:00:08 - train: epoch 0019, iter [03300, 05004], lr: 0.190901, loss: 2.3677
2022-03-10 19:00:49 - train: epoch 0019, iter [03400, 05004], lr: 0.190901, loss: 2.5866
2022-03-10 19:01:30 - train: epoch 0019, iter [03500, 05004], lr: 0.190901, loss: 2.7441
2022-03-10 19:02:11 - train: epoch 0019, iter [03600, 05004], lr: 0.190901, loss: 2.4097
2022-03-10 19:02:53 - train: epoch 0019, iter [03700, 05004], lr: 0.190901, loss: 2.5729
2022-03-10 19:03:35 - train: epoch 0019, iter [03800, 05004], lr: 0.190901, loss: 2.7266
2022-03-10 19:04:16 - train: epoch 0019, iter [03900, 05004], lr: 0.190901, loss: 2.4446
2022-03-10 19:04:56 - train: epoch 0019, iter [04000, 05004], lr: 0.190901, loss: 2.6442
2022-03-10 19:05:37 - train: epoch 0019, iter [04100, 05004], lr: 0.190901, loss: 2.7029
2022-03-10 19:06:16 - train: epoch 0019, iter [04200, 05004], lr: 0.190901, loss: 2.4465
2022-03-10 19:06:56 - train: epoch 0019, iter [04300, 05004], lr: 0.190901, loss: 2.5129
2022-03-10 19:07:37 - train: epoch 0019, iter [04400, 05004], lr: 0.190901, loss: 2.8119
2022-03-10 19:08:17 - train: epoch 0019, iter [04500, 05004], lr: 0.190901, loss: 3.1191
2022-03-10 19:08:57 - train: epoch 0019, iter [04600, 05004], lr: 0.190901, loss: 2.5481
2022-03-10 19:09:38 - train: epoch 0019, iter [04700, 05004], lr: 0.190901, loss: 2.6366
2022-03-10 19:10:18 - train: epoch 0019, iter [04800, 05004], lr: 0.190901, loss: 2.3127
2022-03-10 19:10:57 - train: epoch 0019, iter [04900, 05004], lr: 0.190901, loss: 2.3925
2022-03-10 19:11:36 - train: epoch 0019, iter [05000, 05004], lr: 0.190901, loss: 2.4921
2022-03-10 19:11:38 - train: epoch 019, train_loss: 2.5449
2022-03-10 19:12:58 - eval: epoch: 019, acc1: 47.134%, acc5: 73.166%, test_loss: 2.3165, per_image_load_time: 2.905ms, per_image_inference_time: 0.248ms
2022-03-10 19:12:59 - until epoch: 019, best_acc1: 47.134%
2022-03-10 19:12:59 - epoch 020 lr: 0.18947293298207635
2022-03-10 19:13:39 - train: epoch 0020, iter [00100, 05004], lr: 0.189473, loss: 2.7309
2022-03-10 19:14:16 - train: epoch 0020, iter [00200, 05004], lr: 0.189473, loss: 2.4389
2022-03-10 19:14:54 - train: epoch 0020, iter [00300, 05004], lr: 0.189473, loss: 2.3785
2022-03-10 19:15:31 - train: epoch 0020, iter [00400, 05004], lr: 0.189473, loss: 2.3241
2022-03-10 19:16:09 - train: epoch 0020, iter [00500, 05004], lr: 0.189473, loss: 2.3110
2022-03-10 19:16:48 - train: epoch 0020, iter [00600, 05004], lr: 0.189473, loss: 2.6325
2022-03-10 19:17:27 - train: epoch 0020, iter [00700, 05004], lr: 0.189473, loss: 2.4798
2022-03-10 19:18:06 - train: epoch 0020, iter [00800, 05004], lr: 0.189473, loss: 2.7704
2022-03-10 19:18:46 - train: epoch 0020, iter [00900, 05004], lr: 0.189473, loss: 2.6326
2022-03-10 19:19:26 - train: epoch 0020, iter [01000, 05004], lr: 0.189473, loss: 2.7294
2022-03-10 19:20:07 - train: epoch 0020, iter [01100, 05004], lr: 0.189473, loss: 2.6357
2022-03-10 19:20:48 - train: epoch 0020, iter [01200, 05004], lr: 0.189473, loss: 2.3119
2022-03-10 19:21:27 - train: epoch 0020, iter [01300, 05004], lr: 0.189473, loss: 2.4007
2022-03-10 19:22:08 - train: epoch 0020, iter [01400, 05004], lr: 0.189473, loss: 2.4654
2022-03-10 19:22:48 - train: epoch 0020, iter [01500, 05004], lr: 0.189473, loss: 2.4094
2022-03-10 19:23:28 - train: epoch 0020, iter [01600, 05004], lr: 0.189473, loss: 2.7713
2022-03-10 19:24:09 - train: epoch 0020, iter [01700, 05004], lr: 0.189473, loss: 2.4353
2022-03-10 19:24:47 - train: epoch 0020, iter [01800, 05004], lr: 0.189473, loss: 2.4619
2022-03-10 19:25:26 - train: epoch 0020, iter [01900, 05004], lr: 0.189473, loss: 2.3955
2022-03-10 19:26:06 - train: epoch 0020, iter [02000, 05004], lr: 0.189473, loss: 2.3731
2022-03-10 19:26:45 - train: epoch 0020, iter [02100, 05004], lr: 0.189473, loss: 2.7687
2022-03-10 19:27:24 - train: epoch 0020, iter [02200, 05004], lr: 0.189473, loss: 2.4326
2022-03-10 19:28:04 - train: epoch 0020, iter [02300, 05004], lr: 0.189473, loss: 2.4898
2022-03-10 19:28:44 - train: epoch 0020, iter [02400, 05004], lr: 0.189473, loss: 2.6670
2022-03-10 19:29:24 - train: epoch 0020, iter [02500, 05004], lr: 0.189473, loss: 2.4968
2022-03-10 19:30:04 - train: epoch 0020, iter [02600, 05004], lr: 0.189473, loss: 2.2710
2022-03-10 19:30:44 - train: epoch 0020, iter [02700, 05004], lr: 0.189473, loss: 2.6851
2022-03-10 19:31:23 - train: epoch 0020, iter [02800, 05004], lr: 0.189473, loss: 2.6264
2022-03-10 19:32:05 - train: epoch 0020, iter [02900, 05004], lr: 0.189473, loss: 2.6916
2022-03-10 19:32:44 - train: epoch 0020, iter [03000, 05004], lr: 0.189473, loss: 2.6389
2022-03-10 19:33:23 - train: epoch 0020, iter [03100, 05004], lr: 0.189473, loss: 2.6678
2022-03-10 19:34:02 - train: epoch 0020, iter [03200, 05004], lr: 0.189473, loss: 2.5747
2022-03-10 19:34:41 - train: epoch 0020, iter [03300, 05004], lr: 0.189473, loss: 2.2197
2022-03-10 19:35:21 - train: epoch 0020, iter [03400, 05004], lr: 0.189473, loss: 2.5276
2022-03-10 19:36:01 - train: epoch 0020, iter [03500, 05004], lr: 0.189473, loss: 2.3410
2022-03-10 19:36:40 - train: epoch 0020, iter [03600, 05004], lr: 0.189473, loss: 2.6643
2022-03-10 19:37:19 - train: epoch 0020, iter [03700, 05004], lr: 0.189473, loss: 2.4797
2022-03-10 19:37:58 - train: epoch 0020, iter [03800, 05004], lr: 0.189473, loss: 2.5239
2022-03-10 19:38:37 - train: epoch 0020, iter [03900, 05004], lr: 0.189473, loss: 2.5661
2022-03-10 19:39:16 - train: epoch 0020, iter [04000, 05004], lr: 0.189473, loss: 2.4587
2022-03-10 19:39:55 - train: epoch 0020, iter [04100, 05004], lr: 0.189473, loss: 2.4481
2022-03-10 19:40:35 - train: epoch 0020, iter [04200, 05004], lr: 0.189473, loss: 2.4585
2022-03-10 19:41:14 - train: epoch 0020, iter [04300, 05004], lr: 0.189473, loss: 2.8419
2022-03-10 19:41:55 - train: epoch 0020, iter [04400, 05004], lr: 0.189473, loss: 2.3916
2022-03-10 19:42:33 - train: epoch 0020, iter [04500, 05004], lr: 0.189473, loss: 2.5843
2022-03-10 19:43:12 - train: epoch 0020, iter [04600, 05004], lr: 0.189473, loss: 2.7869
2022-03-10 19:43:52 - train: epoch 0020, iter [04700, 05004], lr: 0.189473, loss: 2.3000
2022-03-10 19:44:31 - train: epoch 0020, iter [04800, 05004], lr: 0.189473, loss: 2.4614
2022-03-10 19:45:11 - train: epoch 0020, iter [04900, 05004], lr: 0.189473, loss: 2.5173
2022-03-10 19:45:50 - train: epoch 0020, iter [05000, 05004], lr: 0.189473, loss: 2.4352
2022-03-10 19:45:52 - train: epoch 020, train_loss: 2.5314
2022-03-10 19:47:17 - eval: epoch: 020, acc1: 46.836%, acc5: 72.556%, test_loss: 2.3505, per_image_load_time: 0.668ms, per_image_inference_time: 0.240ms
2022-03-10 19:47:17 - until epoch: 020, best_acc1: 47.134%
2022-03-10 19:47:17 - epoch 021 lr: 0.1879473751206489
2022-03-10 19:48:00 - train: epoch 0021, iter [00100, 05004], lr: 0.187947, loss: 2.5245
2022-03-10 19:48:41 - train: epoch 0021, iter [00200, 05004], lr: 0.187947, loss: 2.7432
2022-03-10 19:49:20 - train: epoch 0021, iter [00300, 05004], lr: 0.187947, loss: 2.1959
2022-03-10 19:50:00 - train: epoch 0021, iter [00400, 05004], lr: 0.187947, loss: 2.3021
2022-03-10 19:50:41 - train: epoch 0021, iter [00500, 05004], lr: 0.187947, loss: 2.1284
2022-03-10 19:51:20 - train: epoch 0021, iter [00600, 05004], lr: 0.187947, loss: 2.4078
2022-03-10 19:52:00 - train: epoch 0021, iter [00700, 05004], lr: 0.187947, loss: 2.3527
2022-03-10 19:52:41 - train: epoch 0021, iter [00800, 05004], lr: 0.187947, loss: 2.4509
2022-03-10 19:53:22 - train: epoch 0021, iter [00900, 05004], lr: 0.187947, loss: 2.4458
2022-03-10 19:54:02 - train: epoch 0021, iter [01000, 05004], lr: 0.187947, loss: 2.2537
2022-03-10 19:54:43 - train: epoch 0021, iter [01100, 05004], lr: 0.187947, loss: 2.5022
2022-03-10 19:55:24 - train: epoch 0021, iter [01200, 05004], lr: 0.187947, loss: 2.4484
2022-03-10 19:56:04 - train: epoch 0021, iter [01300, 05004], lr: 0.187947, loss: 2.2868
2022-03-10 19:56:44 - train: epoch 0021, iter [01400, 05004], lr: 0.187947, loss: 2.2677
2022-03-10 19:57:24 - train: epoch 0021, iter [01500, 05004], lr: 0.187947, loss: 2.4570
2022-03-10 19:58:04 - train: epoch 0021, iter [01600, 05004], lr: 0.187947, loss: 2.4313
2022-03-10 19:58:44 - train: epoch 0021, iter [01700, 05004], lr: 0.187947, loss: 2.7235
2022-03-10 19:59:24 - train: epoch 0021, iter [01800, 05004], lr: 0.187947, loss: 2.3018
2022-03-10 20:00:03 - train: epoch 0021, iter [01900, 05004], lr: 0.187947, loss: 2.6462
2022-03-10 20:00:43 - train: epoch 0021, iter [02000, 05004], lr: 0.187947, loss: 2.5906
2022-03-10 20:01:22 - train: epoch 0021, iter [02100, 05004], lr: 0.187947, loss: 2.3914
2022-03-10 20:02:02 - train: epoch 0021, iter [02200, 05004], lr: 0.187947, loss: 2.5561
2022-03-10 20:02:42 - train: epoch 0021, iter [02300, 05004], lr: 0.187947, loss: 2.5576
2022-03-10 20:03:23 - train: epoch 0021, iter [02400, 05004], lr: 0.187947, loss: 2.2660
2022-03-10 20:04:02 - train: epoch 0021, iter [02500, 05004], lr: 0.187947, loss: 2.5668
2022-03-10 20:04:41 - train: epoch 0021, iter [02600, 05004], lr: 0.187947, loss: 2.6805
2022-03-10 20:05:22 - train: epoch 0021, iter [02700, 05004], lr: 0.187947, loss: 2.3593
2022-03-10 20:06:01 - train: epoch 0021, iter [02800, 05004], lr: 0.187947, loss: 2.5429
2022-03-10 20:06:41 - train: epoch 0021, iter [02900, 05004], lr: 0.187947, loss: 2.4959
2022-03-10 20:07:21 - train: epoch 0021, iter [03000, 05004], lr: 0.187947, loss: 2.7111
2022-03-10 20:08:02 - train: epoch 0021, iter [03100, 05004], lr: 0.187947, loss: 2.5157
2022-03-10 20:08:43 - train: epoch 0021, iter [03200, 05004], lr: 0.187947, loss: 2.3060
2022-03-10 20:09:22 - train: epoch 0021, iter [03300, 05004], lr: 0.187947, loss: 2.6880
2022-03-10 20:10:01 - train: epoch 0021, iter [03400, 05004], lr: 0.187947, loss: 2.6328
2022-03-10 20:10:41 - train: epoch 0021, iter [03500, 05004], lr: 0.187947, loss: 2.7182
2022-03-10 20:11:20 - train: epoch 0021, iter [03600, 05004], lr: 0.187947, loss: 2.3298
2022-03-10 20:11:59 - train: epoch 0021, iter [03700, 05004], lr: 0.187947, loss: 2.4214
2022-03-10 20:12:42 - train: epoch 0021, iter [03800, 05004], lr: 0.187947, loss: 2.5535
2022-03-10 20:13:24 - train: epoch 0021, iter [03900, 05004], lr: 0.187947, loss: 2.7240
2022-03-10 20:14:05 - train: epoch 0021, iter [04000, 05004], lr: 0.187947, loss: 2.6915
2022-03-10 20:14:45 - train: epoch 0021, iter [04100, 05004], lr: 0.187947, loss: 2.3756
2022-03-10 20:15:24 - train: epoch 0021, iter [04200, 05004], lr: 0.187947, loss: 2.4177
2022-03-10 20:16:04 - train: epoch 0021, iter [04300, 05004], lr: 0.187947, loss: 2.5983
2022-03-10 20:16:43 - train: epoch 0021, iter [04400, 05004], lr: 0.187947, loss: 2.5849
2022-03-10 20:17:22 - train: epoch 0021, iter [04500, 05004], lr: 0.187947, loss: 2.5525
2022-03-10 20:18:02 - train: epoch 0021, iter [04600, 05004], lr: 0.187947, loss: 2.4880
2022-03-10 20:18:41 - train: epoch 0021, iter [04700, 05004], lr: 0.187947, loss: 2.5236
2022-03-10 20:19:21 - train: epoch 0021, iter [04800, 05004], lr: 0.187947, loss: 2.6990
2022-03-10 20:20:00 - train: epoch 0021, iter [04900, 05004], lr: 0.187947, loss: 2.3826
2022-03-10 20:20:39 - train: epoch 0021, iter [05000, 05004], lr: 0.187947, loss: 2.4956
2022-03-10 20:20:41 - train: epoch 021, train_loss: 2.5214
2022-03-10 20:22:04 - eval: epoch: 021, acc1: 47.158%, acc5: 73.588%, test_loss: 2.2906, per_image_load_time: 1.538ms, per_image_inference_time: 0.250ms
2022-03-10 20:22:04 - until epoch: 021, best_acc1: 47.158%
2022-03-10 20:22:04 - epoch 022 lr: 0.1863256480957574
2022-03-10 20:22:47 - train: epoch 0022, iter [00100, 05004], lr: 0.186326, loss: 2.3021
2022-03-10 20:23:28 - train: epoch 0022, iter [00200, 05004], lr: 0.186326, loss: 2.3057
2022-03-10 20:24:09 - train: epoch 0022, iter [00300, 05004], lr: 0.186326, loss: 2.3312
2022-03-10 20:24:48 - train: epoch 0022, iter [00400, 05004], lr: 0.186326, loss: 2.4149
2022-03-10 20:25:29 - train: epoch 0022, iter [00500, 05004], lr: 0.186326, loss: 2.5642
2022-03-10 20:26:09 - train: epoch 0022, iter [00600, 05004], lr: 0.186326, loss: 2.6606
2022-03-10 20:26:49 - train: epoch 0022, iter [00700, 05004], lr: 0.186326, loss: 2.7434
2022-03-10 20:27:28 - train: epoch 0022, iter [00800, 05004], lr: 0.186326, loss: 2.7521
2022-03-10 20:28:08 - train: epoch 0022, iter [00900, 05004], lr: 0.186326, loss: 2.6157
2022-03-10 20:28:49 - train: epoch 0022, iter [01000, 05004], lr: 0.186326, loss: 2.5244
2022-03-10 20:29:28 - train: epoch 0022, iter [01100, 05004], lr: 0.186326, loss: 2.3300
2022-03-10 20:30:07 - train: epoch 0022, iter [01200, 05004], lr: 0.186326, loss: 2.1135
2022-03-10 20:30:47 - train: epoch 0022, iter [01300, 05004], lr: 0.186326, loss: 2.4578
2022-03-10 20:31:27 - train: epoch 0022, iter [01400, 05004], lr: 0.186326, loss: 2.4835
2022-03-10 20:32:07 - train: epoch 0022, iter [01500, 05004], lr: 0.186326, loss: 2.7894
2022-03-10 20:32:48 - train: epoch 0022, iter [01600, 05004], lr: 0.186326, loss: 2.4962
2022-03-10 20:33:28 - train: epoch 0022, iter [01700, 05004], lr: 0.186326, loss: 2.5417
2022-03-10 20:34:08 - train: epoch 0022, iter [01800, 05004], lr: 0.186326, loss: 2.8029
2022-03-10 20:34:48 - train: epoch 0022, iter [01900, 05004], lr: 0.186326, loss: 2.4053
2022-03-10 20:35:30 - train: epoch 0022, iter [02000, 05004], lr: 0.186326, loss: 2.4514
2022-03-10 20:36:09 - train: epoch 0022, iter [02100, 05004], lr: 0.186326, loss: 2.3861
2022-03-10 20:36:48 - train: epoch 0022, iter [02200, 05004], lr: 0.186326, loss: 2.1910
2022-03-10 20:37:26 - train: epoch 0022, iter [02300, 05004], lr: 0.186326, loss: 2.7240
2022-03-10 20:38:05 - train: epoch 0022, iter [02400, 05004], lr: 0.186326, loss: 2.5468
2022-03-10 20:38:45 - train: epoch 0022, iter [02500, 05004], lr: 0.186326, loss: 2.3928
2022-03-10 20:39:24 - train: epoch 0022, iter [02600, 05004], lr: 0.186326, loss: 2.5282
2022-03-10 20:40:03 - train: epoch 0022, iter [02700, 05004], lr: 0.186326, loss: 2.3222
2022-03-10 20:40:43 - train: epoch 0022, iter [02800, 05004], lr: 0.186326, loss: 2.6388
2022-03-10 20:41:22 - train: epoch 0022, iter [02900, 05004], lr: 0.186326, loss: 2.4369
2022-03-10 20:42:01 - train: epoch 0022, iter [03000, 05004], lr: 0.186326, loss: 2.5341
2022-03-10 20:42:41 - train: epoch 0022, iter [03100, 05004], lr: 0.186326, loss: 2.7333
2022-03-10 20:43:21 - train: epoch 0022, iter [03200, 05004], lr: 0.186326, loss: 2.5718
2022-03-10 20:44:00 - train: epoch 0022, iter [03300, 05004], lr: 0.186326, loss: 2.4767
2022-03-10 20:44:39 - train: epoch 0022, iter [03400, 05004], lr: 0.186326, loss: 2.4156
2022-03-10 20:45:18 - train: epoch 0022, iter [03500, 05004], lr: 0.186326, loss: 2.6298
2022-03-10 20:45:57 - train: epoch 0022, iter [03600, 05004], lr: 0.186326, loss: 2.6919
2022-03-10 20:46:36 - train: epoch 0022, iter [03700, 05004], lr: 0.186326, loss: 2.5406
2022-03-10 20:47:15 - train: epoch 0022, iter [03800, 05004], lr: 0.186326, loss: 2.8834
2022-03-10 20:47:55 - train: epoch 0022, iter [03900, 05004], lr: 0.186326, loss: 2.7233
2022-03-10 20:48:34 - train: epoch 0022, iter [04000, 05004], lr: 0.186326, loss: 2.4101
2022-03-10 20:49:13 - train: epoch 0022, iter [04100, 05004], lr: 0.186326, loss: 2.3560
2022-03-10 20:49:53 - train: epoch 0022, iter [04200, 05004], lr: 0.186326, loss: 2.5242
2022-03-10 20:50:33 - train: epoch 0022, iter [04300, 05004], lr: 0.186326, loss: 2.4668
2022-03-10 20:51:12 - train: epoch 0022, iter [04400, 05004], lr: 0.186326, loss: 2.4196
2022-03-10 20:51:51 - train: epoch 0022, iter [04500, 05004], lr: 0.186326, loss: 2.5108
2022-03-10 20:52:30 - train: epoch 0022, iter [04600, 05004], lr: 0.186326, loss: 2.7396
2022-03-10 20:53:10 - train: epoch 0022, iter [04700, 05004], lr: 0.186326, loss: 2.6332
2022-03-10 20:53:49 - train: epoch 0022, iter [04800, 05004], lr: 0.186326, loss: 2.3839
2022-03-10 20:54:28 - train: epoch 0022, iter [04900, 05004], lr: 0.186326, loss: 2.4659
2022-03-10 20:55:09 - train: epoch 0022, iter [05000, 05004], lr: 0.186326, loss: 2.3516
2022-03-10 20:55:10 - train: epoch 022, train_loss: 2.5096
2022-03-10 20:56:35 - eval: epoch: 022, acc1: 46.552%, acc5: 72.720%, test_loss: 2.3407, per_image_load_time: 1.345ms, per_image_inference_time: 0.249ms
2022-03-10 20:56:35 - until epoch: 022, best_acc1: 47.158%
2022-03-10 20:56:35 - epoch 023 lr: 0.18460952524209354
2022-03-10 20:57:20 - train: epoch 0023, iter [00100, 05004], lr: 0.184610, loss: 2.3713
2022-03-10 20:58:00 - train: epoch 0023, iter [00200, 05004], lr: 0.184610, loss: 2.3537
2022-03-10 20:58:40 - train: epoch 0023, iter [00300, 05004], lr: 0.184610, loss: 2.4471
2022-03-10 20:59:21 - train: epoch 0023, iter [00400, 05004], lr: 0.184610, loss: 2.3935
2022-03-10 21:00:01 - train: epoch 0023, iter [00500, 05004], lr: 0.184610, loss: 2.5804
2022-03-10 21:00:42 - train: epoch 0023, iter [00600, 05004], lr: 0.184610, loss: 2.3636
2022-03-10 21:01:24 - train: epoch 0023, iter [00700, 05004], lr: 0.184610, loss: 2.3371
2022-03-10 21:02:05 - train: epoch 0023, iter [00800, 05004], lr: 0.184610, loss: 2.6861
2022-03-10 21:02:46 - train: epoch 0023, iter [00900, 05004], lr: 0.184610, loss: 2.4551
2022-03-10 21:03:28 - train: epoch 0023, iter [01000, 05004], lr: 0.184610, loss: 2.5701
2022-03-10 21:04:09 - train: epoch 0023, iter [01100, 05004], lr: 0.184610, loss: 2.4387
2022-03-10 21:04:50 - train: epoch 0023, iter [01200, 05004], lr: 0.184610, loss: 2.2689
2022-03-10 21:05:31 - train: epoch 0023, iter [01300, 05004], lr: 0.184610, loss: 2.4135
2022-03-10 21:06:13 - train: epoch 0023, iter [01400, 05004], lr: 0.184610, loss: 2.5045
2022-03-10 21:06:54 - train: epoch 0023, iter [01500, 05004], lr: 0.184610, loss: 2.4571
2022-03-10 21:07:36 - train: epoch 0023, iter [01600, 05004], lr: 0.184610, loss: 2.5109
2022-03-10 21:08:17 - train: epoch 0023, iter [01700, 05004], lr: 0.184610, loss: 2.5608
2022-03-10 21:08:58 - train: epoch 0023, iter [01800, 05004], lr: 0.184610, loss: 2.4751
2022-03-10 21:09:39 - train: epoch 0023, iter [01900, 05004], lr: 0.184610, loss: 2.6815
2022-03-10 21:10:20 - train: epoch 0023, iter [02000, 05004], lr: 0.184610, loss: 2.2930
2022-03-10 21:11:00 - train: epoch 0023, iter [02100, 05004], lr: 0.184610, loss: 2.6016
2022-03-10 21:11:41 - train: epoch 0023, iter [02200, 05004], lr: 0.184610, loss: 2.1858
2022-03-10 21:12:22 - train: epoch 0023, iter [02300, 05004], lr: 0.184610, loss: 2.4170
2022-03-10 21:13:03 - train: epoch 0023, iter [02400, 05004], lr: 0.184610, loss: 2.5230
2022-03-10 21:13:44 - train: epoch 0023, iter [02500, 05004], lr: 0.184610, loss: 2.6004
2022-03-10 21:14:26 - train: epoch 0023, iter [02600, 05004], lr: 0.184610, loss: 2.6009
2022-03-10 21:15:06 - train: epoch 0023, iter [02700, 05004], lr: 0.184610, loss: 2.6051
2022-03-10 21:15:45 - train: epoch 0023, iter [02800, 05004], lr: 0.184610, loss: 2.3585
2022-03-10 21:16:25 - train: epoch 0023, iter [02900, 05004], lr: 0.184610, loss: 2.4005
2022-03-10 21:17:05 - train: epoch 0023, iter [03000, 05004], lr: 0.184610, loss: 2.5906
2022-03-10 21:17:45 - train: epoch 0023, iter [03100, 05004], lr: 0.184610, loss: 2.4690
2022-03-10 21:18:24 - train: epoch 0023, iter [03200, 05004], lr: 0.184610, loss: 2.4577
2022-03-10 21:19:04 - train: epoch 0023, iter [03300, 05004], lr: 0.184610, loss: 2.4514
2022-03-10 21:19:44 - train: epoch 0023, iter [03400, 05004], lr: 0.184610, loss: 2.6810
2022-03-10 21:20:23 - train: epoch 0023, iter [03500, 05004], lr: 0.184610, loss: 2.4798
2022-03-10 21:21:03 - train: epoch 0023, iter [03600, 05004], lr: 0.184610, loss: 2.5105
2022-03-10 21:21:44 - train: epoch 0023, iter [03700, 05004], lr: 0.184610, loss: 2.3525
2022-03-10 21:22:25 - train: epoch 0023, iter [03800, 05004], lr: 0.184610, loss: 2.4169
2022-03-10 21:23:05 - train: epoch 0023, iter [03900, 05004], lr: 0.184610, loss: 2.5283
2022-03-10 21:23:46 - train: epoch 0023, iter [04000, 05004], lr: 0.184610, loss: 2.2777
2022-03-10 21:24:27 - train: epoch 0023, iter [04100, 05004], lr: 0.184610, loss: 2.4140
2022-03-10 21:25:09 - train: epoch 0023, iter [04200, 05004], lr: 0.184610, loss: 2.3008
2022-03-10 21:25:50 - train: epoch 0023, iter [04300, 05004], lr: 0.184610, loss: 2.4241
2022-03-10 21:26:30 - train: epoch 0023, iter [04400, 05004], lr: 0.184610, loss: 2.3207
2022-03-10 21:27:11 - train: epoch 0023, iter [04500, 05004], lr: 0.184610, loss: 2.4906
2022-03-10 21:27:53 - train: epoch 0023, iter [04600, 05004], lr: 0.184610, loss: 2.6275
2022-03-10 21:28:34 - train: epoch 0023, iter [04700, 05004], lr: 0.184610, loss: 2.4235
2022-03-10 21:29:15 - train: epoch 0023, iter [04800, 05004], lr: 0.184610, loss: 2.3905
2022-03-10 21:29:55 - train: epoch 0023, iter [04900, 05004], lr: 0.184610, loss: 2.3316
2022-03-10 21:30:36 - train: epoch 0023, iter [05000, 05004], lr: 0.184610, loss: 2.5777
2022-03-10 21:30:37 - train: epoch 023, train_loss: 2.4970
2022-03-10 21:32:01 - eval: epoch: 023, acc1: 47.116%, acc5: 73.540%, test_loss: 2.3092, per_image_load_time: 1.334ms, per_image_inference_time: 0.245ms
2022-03-10 21:32:01 - until epoch: 023, best_acc1: 47.158%
2022-03-10 21:32:01 - epoch 024 lr: 0.18280088311480203
2022-03-10 21:32:44 - train: epoch 0024, iter [00100, 05004], lr: 0.182801, loss: 2.3668
2022-03-10 21:33:25 - train: epoch 0024, iter [00200, 05004], lr: 0.182801, loss: 2.3594
2022-03-10 21:34:06 - train: epoch 0024, iter [00300, 05004], lr: 0.182801, loss: 2.4529
2022-03-10 21:34:48 - train: epoch 0024, iter [00400, 05004], lr: 0.182801, loss: 2.6096
2022-03-10 21:35:28 - train: epoch 0024, iter [00500, 05004], lr: 0.182801, loss: 2.5122
2022-03-10 21:36:08 - train: epoch 0024, iter [00600, 05004], lr: 0.182801, loss: 2.3317
2022-03-10 21:36:48 - train: epoch 0024, iter [00700, 05004], lr: 0.182801, loss: 2.4986
2022-03-10 21:37:29 - train: epoch 0024, iter [00800, 05004], lr: 0.182801, loss: 2.4361
2022-03-10 21:38:09 - train: epoch 0024, iter [00900, 05004], lr: 0.182801, loss: 2.4944
2022-03-10 21:38:49 - train: epoch 0024, iter [01000, 05004], lr: 0.182801, loss: 2.3770
2022-03-10 21:39:29 - train: epoch 0024, iter [01100, 05004], lr: 0.182801, loss: 2.2403
2022-03-10 21:40:09 - train: epoch 0024, iter [01200, 05004], lr: 0.182801, loss: 2.4746
2022-03-10 21:40:50 - train: epoch 0024, iter [01300, 05004], lr: 0.182801, loss: 2.5805
2022-03-10 21:41:31 - train: epoch 0024, iter [01400, 05004], lr: 0.182801, loss: 2.2687
2022-03-10 21:42:11 - train: epoch 0024, iter [01500, 05004], lr: 0.182801, loss: 2.5017
2022-03-10 21:42:52 - train: epoch 0024, iter [01600, 05004], lr: 0.182801, loss: 2.4054
2022-03-10 21:43:34 - train: epoch 0024, iter [01700, 05004], lr: 0.182801, loss: 2.3944
2022-03-10 21:44:15 - train: epoch 0024, iter [01800, 05004], lr: 0.182801, loss: 2.6986
2022-03-10 21:44:57 - train: epoch 0024, iter [01900, 05004], lr: 0.182801, loss: 2.2862
2022-03-10 21:45:38 - train: epoch 0024, iter [02000, 05004], lr: 0.182801, loss: 2.6084
2022-03-10 21:46:20 - train: epoch 0024, iter [02100, 05004], lr: 0.182801, loss: 2.3949
2022-03-10 21:47:01 - train: epoch 0024, iter [02200, 05004], lr: 0.182801, loss: 2.4824
2022-03-10 21:47:42 - train: epoch 0024, iter [02300, 05004], lr: 0.182801, loss: 2.5677
2022-03-10 21:48:23 - train: epoch 0024, iter [02400, 05004], lr: 0.182801, loss: 2.6933
2022-03-10 21:49:04 - train: epoch 0024, iter [02500, 05004], lr: 0.182801, loss: 2.5427
2022-03-10 21:49:45 - train: epoch 0024, iter [02600, 05004], lr: 0.182801, loss: 2.6191
2022-03-10 21:50:26 - train: epoch 0024, iter [02700, 05004], lr: 0.182801, loss: 2.7478
2022-03-10 21:51:07 - train: epoch 0024, iter [02800, 05004], lr: 0.182801, loss: 2.6151
2022-03-10 21:51:48 - train: epoch 0024, iter [02900, 05004], lr: 0.182801, loss: 2.6291
2022-03-10 21:52:30 - train: epoch 0024, iter [03000, 05004], lr: 0.182801, loss: 2.3135
2022-03-10 21:53:11 - train: epoch 0024, iter [03100, 05004], lr: 0.182801, loss: 2.3588
2022-03-10 21:53:53 - train: epoch 0024, iter [03200, 05004], lr: 0.182801, loss: 2.5823
2022-03-10 21:54:33 - train: epoch 0024, iter [03300, 05004], lr: 0.182801, loss: 2.2911
2022-03-10 21:55:14 - train: epoch 0024, iter [03400, 05004], lr: 0.182801, loss: 2.3780
2022-03-10 21:55:54 - train: epoch 0024, iter [03500, 05004], lr: 0.182801, loss: 2.4354
2022-03-10 21:56:35 - train: epoch 0024, iter [03600, 05004], lr: 0.182801, loss: 2.3558
2022-03-10 21:57:17 - train: epoch 0024, iter [03700, 05004], lr: 0.182801, loss: 2.3544
2022-03-10 21:57:58 - train: epoch 0024, iter [03800, 05004], lr: 0.182801, loss: 2.6265
2022-03-10 21:58:38 - train: epoch 0024, iter [03900, 05004], lr: 0.182801, loss: 2.3263
2022-03-10 21:59:20 - train: epoch 0024, iter [04000, 05004], lr: 0.182801, loss: 2.4677
2022-03-10 22:00:00 - train: epoch 0024, iter [04100, 05004], lr: 0.182801, loss: 2.3977
2022-03-10 22:00:41 - train: epoch 0024, iter [04200, 05004], lr: 0.182801, loss: 2.3849
2022-03-10 22:01:22 - train: epoch 0024, iter [04300, 05004], lr: 0.182801, loss: 2.5477
2022-03-10 22:02:02 - train: epoch 0024, iter [04400, 05004], lr: 0.182801, loss: 2.3997
2022-03-10 22:02:43 - train: epoch 0024, iter [04500, 05004], lr: 0.182801, loss: 2.3633
2022-03-10 22:03:24 - train: epoch 0024, iter [04600, 05004], lr: 0.182801, loss: 2.5668
2022-03-10 22:04:04 - train: epoch 0024, iter [04700, 05004], lr: 0.182801, loss: 2.2737
2022-03-10 22:04:46 - train: epoch 0024, iter [04800, 05004], lr: 0.182801, loss: 2.4296
2022-03-10 22:05:27 - train: epoch 0024, iter [04900, 05004], lr: 0.182801, loss: 2.4824
2022-03-10 22:06:07 - train: epoch 0024, iter [05000, 05004], lr: 0.182801, loss: 2.4878
2022-03-10 22:06:08 - train: epoch 024, train_loss: 2.4860
2022-03-10 22:07:33 - eval: epoch: 024, acc1: 48.748%, acc5: 74.930%, test_loss: 2.2226, per_image_load_time: 2.922ms, per_image_inference_time: 0.259ms
2022-03-10 22:07:33 - until epoch: 024, best_acc1: 48.748%
2022-03-10 22:07:33 - epoch 025 lr: 0.18090169943749476
2022-03-10 22:08:16 - train: epoch 0025, iter [00100, 05004], lr: 0.180902, loss: 2.3322
2022-03-10 22:08:57 - train: epoch 0025, iter [00200, 05004], lr: 0.180902, loss: 2.1702
2022-03-10 22:09:37 - train: epoch 0025, iter [00300, 05004], lr: 0.180902, loss: 2.3540
2022-03-10 22:10:17 - train: epoch 0025, iter [00400, 05004], lr: 0.180902, loss: 2.6955
2022-03-10 22:10:58 - train: epoch 0025, iter [00500, 05004], lr: 0.180902, loss: 2.3349
2022-03-10 22:11:37 - train: epoch 0025, iter [00600, 05004], lr: 0.180902, loss: 2.6068
2022-03-10 22:12:18 - train: epoch 0025, iter [00700, 05004], lr: 0.180902, loss: 2.5419
2022-03-10 22:12:59 - train: epoch 0025, iter [00800, 05004], lr: 0.180902, loss: 2.4786
2022-03-10 22:13:40 - train: epoch 0025, iter [00900, 05004], lr: 0.180902, loss: 2.1780
2022-03-10 22:14:21 - train: epoch 0025, iter [01000, 05004], lr: 0.180902, loss: 2.5181
2022-03-10 22:15:02 - train: epoch 0025, iter [01100, 05004], lr: 0.180902, loss: 2.4192
2022-03-10 22:15:42 - train: epoch 0025, iter [01200, 05004], lr: 0.180902, loss: 2.3899
2022-03-10 22:16:23 - train: epoch 0025, iter [01300, 05004], lr: 0.180902, loss: 2.2973
2022-03-10 22:17:04 - train: epoch 0025, iter [01400, 05004], lr: 0.180902, loss: 2.3394
2022-03-10 22:17:47 - train: epoch 0025, iter [01500, 05004], lr: 0.180902, loss: 2.3959
2022-03-10 22:18:27 - train: epoch 0025, iter [01600, 05004], lr: 0.180902, loss: 2.1865
2022-03-10 22:19:08 - train: epoch 0025, iter [01700, 05004], lr: 0.180902, loss: 2.5723
2022-03-10 22:19:49 - train: epoch 0025, iter [01800, 05004], lr: 0.180902, loss: 2.3328
2022-03-10 22:20:29 - train: epoch 0025, iter [01900, 05004], lr: 0.180902, loss: 2.4699
2022-03-10 22:21:13 - train: epoch 0025, iter [02000, 05004], lr: 0.180902, loss: 2.3223
2022-03-10 22:21:55 - train: epoch 0025, iter [02100, 05004], lr: 0.180902, loss: 2.3533
2022-03-10 22:22:38 - train: epoch 0025, iter [02200, 05004], lr: 0.180902, loss: 2.5075
2022-03-10 22:23:19 - train: epoch 0025, iter [02300, 05004], lr: 0.180902, loss: 2.2608
2022-03-10 22:24:01 - train: epoch 0025, iter [02400, 05004], lr: 0.180902, loss: 2.2959
2022-03-10 22:24:43 - train: epoch 0025, iter [02500, 05004], lr: 0.180902, loss: 2.4883
2022-03-10 22:25:25 - train: epoch 0025, iter [02600, 05004], lr: 0.180902, loss: 2.4789
2022-03-10 22:26:06 - train: epoch 0025, iter [02700, 05004], lr: 0.180902, loss: 2.5204
2022-03-10 22:26:48 - train: epoch 0025, iter [02800, 05004], lr: 0.180902, loss: 2.6207
2022-03-10 22:27:30 - train: epoch 0025, iter [02900, 05004], lr: 0.180902, loss: 2.5858
2022-03-10 22:28:12 - train: epoch 0025, iter [03000, 05004], lr: 0.180902, loss: 2.3948
2022-03-10 22:28:53 - train: epoch 0025, iter [03100, 05004], lr: 0.180902, loss: 2.5763
2022-03-10 22:29:35 - train: epoch 0025, iter [03200, 05004], lr: 0.180902, loss: 2.4707
2022-03-10 22:30:18 - train: epoch 0025, iter [03300, 05004], lr: 0.180902, loss: 2.4048
2022-03-10 22:30:59 - train: epoch 0025, iter [03400, 05004], lr: 0.180902, loss: 2.5759
2022-03-10 22:31:41 - train: epoch 0025, iter [03500, 05004], lr: 0.180902, loss: 2.4546
2022-03-10 22:32:21 - train: epoch 0025, iter [03600, 05004], lr: 0.180902, loss: 2.4282
2022-03-10 22:33:03 - train: epoch 0025, iter [03700, 05004], lr: 0.180902, loss: 2.5105
2022-03-10 22:33:45 - train: epoch 0025, iter [03800, 05004], lr: 0.180902, loss: 2.7003
2022-03-10 22:34:26 - train: epoch 0025, iter [03900, 05004], lr: 0.180902, loss: 2.6411
2022-03-10 22:35:07 - train: epoch 0025, iter [04000, 05004], lr: 0.180902, loss: 2.5246
2022-03-10 22:35:49 - train: epoch 0025, iter [04100, 05004], lr: 0.180902, loss: 2.4495
2022-03-10 22:36:31 - train: epoch 0025, iter [04200, 05004], lr: 0.180902, loss: 2.4349
2022-03-10 22:37:12 - train: epoch 0025, iter [04300, 05004], lr: 0.180902, loss: 2.4400
2022-03-10 22:37:54 - train: epoch 0025, iter [04400, 05004], lr: 0.180902, loss: 2.3378
2022-03-10 22:38:34 - train: epoch 0025, iter [04500, 05004], lr: 0.180902, loss: 2.4533
2022-03-10 22:39:15 - train: epoch 0025, iter [04600, 05004], lr: 0.180902, loss: 2.4521
2022-03-10 22:39:57 - train: epoch 0025, iter [04700, 05004], lr: 0.180902, loss: 2.4144
2022-03-10 22:40:39 - train: epoch 0025, iter [04800, 05004], lr: 0.180902, loss: 2.2372
2022-03-10 22:41:20 - train: epoch 0025, iter [04900, 05004], lr: 0.180902, loss: 2.3985
2022-03-10 22:42:01 - train: epoch 0025, iter [05000, 05004], lr: 0.180902, loss: 2.7038
2022-03-10 22:42:02 - train: epoch 025, train_loss: 2.4772
2022-03-10 22:43:30 - eval: epoch: 025, acc1: 47.498%, acc5: 73.502%, test_loss: 2.3006, per_image_load_time: 3.005ms, per_image_inference_time: 0.258ms
2022-03-10 22:43:31 - until epoch: 025, best_acc1: 48.748%
2022-03-10 23:53:25 - epoch 026 lr: 0.17891405093963939
2022-03-10 23:54:12 - train: epoch 0026, iter [00100, 05004], lr: 0.178914, loss: 2.3363
2022-03-10 23:54:53 - train: epoch 0026, iter [00200, 05004], lr: 0.178914, loss: 2.2612
2022-03-10 23:55:32 - train: epoch 0026, iter [00300, 05004], lr: 0.178914, loss: 2.4379
2022-03-10 23:56:11 - train: epoch 0026, iter [00400, 05004], lr: 0.178914, loss: 2.1468
2022-03-10 23:56:51 - train: epoch 0026, iter [00500, 05004], lr: 0.178914, loss: 2.4188
2022-03-10 23:57:32 - train: epoch 0026, iter [00600, 05004], lr: 0.178914, loss: 2.7065
2022-03-10 23:58:14 - train: epoch 0026, iter [00700, 05004], lr: 0.178914, loss: 2.5783
2022-03-10 23:58:56 - train: epoch 0026, iter [00800, 05004], lr: 0.178914, loss: 2.3297
2022-03-10 23:59:39 - train: epoch 0026, iter [00900, 05004], lr: 0.178914, loss: 2.5184
2022-03-11 00:00:20 - train: epoch 0026, iter [01000, 05004], lr: 0.178914, loss: 2.4801
2022-03-11 00:01:00 - train: epoch 0026, iter [01100, 05004], lr: 0.178914, loss: 2.5540
2022-03-11 00:01:42 - train: epoch 0026, iter [01200, 05004], lr: 0.178914, loss: 2.5007
2022-03-11 00:02:21 - train: epoch 0026, iter [01300, 05004], lr: 0.178914, loss: 2.3931
2022-03-11 00:03:01 - train: epoch 0026, iter [01400, 05004], lr: 0.178914, loss: 2.4976
2022-03-11 00:03:41 - train: epoch 0026, iter [01500, 05004], lr: 0.178914, loss: 2.3081
2022-03-11 00:04:21 - train: epoch 0026, iter [01600, 05004], lr: 0.178914, loss: 2.3427
2022-03-11 00:05:00 - train: epoch 0026, iter [01700, 05004], lr: 0.178914, loss: 2.5328
2022-03-11 00:05:40 - train: epoch 0026, iter [01800, 05004], lr: 0.178914, loss: 2.7830
2022-03-11 00:06:20 - train: epoch 0026, iter [01900, 05004], lr: 0.178914, loss: 2.5956
2022-03-11 00:07:01 - train: epoch 0026, iter [02000, 05004], lr: 0.178914, loss: 2.5322
2022-03-11 00:07:43 - train: epoch 0026, iter [02100, 05004], lr: 0.178914, loss: 2.4630
2022-03-11 00:08:25 - train: epoch 0026, iter [02200, 05004], lr: 0.178914, loss: 2.3962
2022-03-11 00:09:07 - train: epoch 0026, iter [02300, 05004], lr: 0.178914, loss: 2.3928
2022-03-11 00:09:48 - train: epoch 0026, iter [02400, 05004], lr: 0.178914, loss: 2.6420
2022-03-11 00:10:30 - train: epoch 0026, iter [02500, 05004], lr: 0.178914, loss: 2.5707
2022-03-11 00:11:10 - train: epoch 0026, iter [02600, 05004], lr: 0.178914, loss: 2.4701
2022-03-11 00:11:50 - train: epoch 0026, iter [02700, 05004], lr: 0.178914, loss: 2.8115
2022-03-11 00:12:30 - train: epoch 0026, iter [02800, 05004], lr: 0.178914, loss: 2.4704
2022-03-11 00:13:12 - train: epoch 0026, iter [02900, 05004], lr: 0.178914, loss: 2.6332
2022-03-11 00:13:53 - train: epoch 0026, iter [03000, 05004], lr: 0.178914, loss: 2.4458
2022-03-11 00:14:35 - train: epoch 0026, iter [03100, 05004], lr: 0.178914, loss: 2.3587
2022-03-11 00:15:16 - train: epoch 0026, iter [03200, 05004], lr: 0.178914, loss: 2.4968
2022-03-11 00:15:57 - train: epoch 0026, iter [03300, 05004], lr: 0.178914, loss: 2.3136
2022-03-11 00:16:39 - train: epoch 0026, iter [03400, 05004], lr: 0.178914, loss: 2.5432
2022-03-11 00:17:20 - train: epoch 0026, iter [03500, 05004], lr: 0.178914, loss: 2.6508
2022-03-11 00:18:01 - train: epoch 0026, iter [03600, 05004], lr: 0.178914, loss: 2.6314
2022-03-11 00:18:43 - train: epoch 0026, iter [03700, 05004], lr: 0.178914, loss: 2.4767
2022-03-11 00:19:23 - train: epoch 0026, iter [03800, 05004], lr: 0.178914, loss: 2.8272
2022-03-11 00:20:06 - train: epoch 0026, iter [03900, 05004], lr: 0.178914, loss: 2.6841
2022-03-11 00:20:47 - train: epoch 0026, iter [04000, 05004], lr: 0.178914, loss: 2.7415
2022-03-11 00:21:27 - train: epoch 0026, iter [04100, 05004], lr: 0.178914, loss: 2.4869
2022-03-11 00:22:08 - train: epoch 0026, iter [04200, 05004], lr: 0.178914, loss: 2.6102
2022-03-11 00:22:50 - train: epoch 0026, iter [04300, 05004], lr: 0.178914, loss: 2.6807
2022-03-11 00:23:31 - train: epoch 0026, iter [04400, 05004], lr: 0.178914, loss: 2.7580
2022-03-11 00:24:12 - train: epoch 0026, iter [04500, 05004], lr: 0.178914, loss: 2.8820
2022-03-11 00:24:52 - train: epoch 0026, iter [04600, 05004], lr: 0.178914, loss: 2.5288
2022-03-11 00:25:32 - train: epoch 0026, iter [04700, 05004], lr: 0.178914, loss: 2.2752
2022-03-11 00:26:12 - train: epoch 0026, iter [04800, 05004], lr: 0.178914, loss: 2.3914
2022-03-11 00:26:51 - train: epoch 0026, iter [04900, 05004], lr: 0.178914, loss: 2.5085
2022-03-11 00:27:31 - train: epoch 0026, iter [05000, 05004], lr: 0.178914, loss: 2.3694
2022-03-11 00:27:32 - train: epoch 026, train_loss: 2.4663
2022-03-11 00:28:57 - eval: epoch: 026, acc1: 48.256%, acc5: 74.312%, test_loss: 2.2469, per_image_load_time: 1.258ms, per_image_inference_time: 0.248ms
2022-03-11 00:28:57 - until epoch: 026, best_acc1: 48.748%
2022-03-11 00:28:57 - epoch 027 lr: 0.17684011108568592
2022-03-11 00:29:39 - train: epoch 0027, iter [00100, 05004], lr: 0.176840, loss: 2.4369
2022-03-11 00:30:18 - train: epoch 0027, iter [00200, 05004], lr: 0.176840, loss: 2.4730
2022-03-11 00:30:58 - train: epoch 0027, iter [00300, 05004], lr: 0.176840, loss: 2.5466
2022-03-11 00:31:38 - train: epoch 0027, iter [00400, 05004], lr: 0.176840, loss: 2.5199
2022-03-11 00:32:18 - train: epoch 0027, iter [00500, 05004], lr: 0.176840, loss: 2.5331
2022-03-11 00:32:57 - train: epoch 0027, iter [00600, 05004], lr: 0.176840, loss: 2.6022
2022-03-11 00:33:39 - train: epoch 0027, iter [00700, 05004], lr: 0.176840, loss: 2.5491
2022-03-11 00:34:19 - train: epoch 0027, iter [00800, 05004], lr: 0.176840, loss: 2.3951
2022-03-11 00:34:57 - train: epoch 0027, iter [00900, 05004], lr: 0.176840, loss: 2.5099
2022-03-11 00:35:38 - train: epoch 0027, iter [01000, 05004], lr: 0.176840, loss: 2.6037
2022-03-11 00:36:18 - train: epoch 0027, iter [01100, 05004], lr: 0.176840, loss: 2.4730
2022-03-11 00:36:58 - train: epoch 0027, iter [01200, 05004], lr: 0.176840, loss: 2.5679
2022-03-11 00:37:37 - train: epoch 0027, iter [01300, 05004], lr: 0.176840, loss: 2.4503
2022-03-11 00:38:18 - train: epoch 0027, iter [01400, 05004], lr: 0.176840, loss: 2.4981
2022-03-11 00:38:57 - train: epoch 0027, iter [01500, 05004], lr: 0.176840, loss: 2.4935
2022-03-11 00:39:37 - train: epoch 0027, iter [01600, 05004], lr: 0.176840, loss: 2.5253
2022-03-11 00:40:18 - train: epoch 0027, iter [01700, 05004], lr: 0.176840, loss: 2.4837
2022-03-11 00:40:59 - train: epoch 0027, iter [01800, 05004], lr: 0.176840, loss: 2.4926
2022-03-11 00:41:39 - train: epoch 0027, iter [01900, 05004], lr: 0.176840, loss: 2.5151
2022-03-11 00:42:20 - train: epoch 0027, iter [02000, 05004], lr: 0.176840, loss: 2.2400
2022-03-11 00:43:01 - train: epoch 0027, iter [02100, 05004], lr: 0.176840, loss: 2.5841
2022-03-11 00:43:42 - train: epoch 0027, iter [02200, 05004], lr: 0.176840, loss: 2.5633
2022-03-11 00:44:22 - train: epoch 0027, iter [02300, 05004], lr: 0.176840, loss: 2.6404
2022-03-11 00:45:03 - train: epoch 0027, iter [02400, 05004], lr: 0.176840, loss: 2.2342
2022-03-11 00:45:44 - train: epoch 0027, iter [02500, 05004], lr: 0.176840, loss: 2.3416
2022-03-11 00:46:24 - train: epoch 0027, iter [02600, 05004], lr: 0.176840, loss: 2.3696
2022-03-11 00:47:05 - train: epoch 0027, iter [02700, 05004], lr: 0.176840, loss: 2.4416
2022-03-11 00:47:45 - train: epoch 0027, iter [02800, 05004], lr: 0.176840, loss: 2.4681
2022-03-11 00:48:26 - train: epoch 0027, iter [02900, 05004], lr: 0.176840, loss: 2.5348
2022-03-11 00:49:10 - train: epoch 0027, iter [03000, 05004], lr: 0.176840, loss: 2.2193
2022-03-11 00:49:54 - train: epoch 0027, iter [03100, 05004], lr: 0.176840, loss: 2.1357
2022-03-11 00:50:36 - train: epoch 0027, iter [03200, 05004], lr: 0.176840, loss: 2.3144
2022-03-11 00:51:18 - train: epoch 0027, iter [03300, 05004], lr: 0.176840, loss: 2.4463
2022-03-11 00:52:01 - train: epoch 0027, iter [03400, 05004], lr: 0.176840, loss: 2.4185
2022-03-11 00:52:43 - train: epoch 0027, iter [03500, 05004], lr: 0.176840, loss: 2.6899
2022-03-11 00:53:24 - train: epoch 0027, iter [03600, 05004], lr: 0.176840, loss: 2.4136
2022-03-11 00:54:04 - train: epoch 0027, iter [03700, 05004], lr: 0.176840, loss: 2.4412
2022-03-11 00:54:44 - train: epoch 0027, iter [03800, 05004], lr: 0.176840, loss: 2.3649
2022-03-11 00:55:25 - train: epoch 0027, iter [03900, 05004], lr: 0.176840, loss: 2.6269
2022-03-11 00:56:05 - train: epoch 0027, iter [04000, 05004], lr: 0.176840, loss: 2.5469
2022-03-11 00:56:47 - train: epoch 0027, iter [04100, 05004], lr: 0.176840, loss: 2.6356
2022-03-11 00:57:27 - train: epoch 0027, iter [04200, 05004], lr: 0.176840, loss: 2.4391
2022-03-11 00:58:10 - train: epoch 0027, iter [04300, 05004], lr: 0.176840, loss: 2.3304
2022-03-11 00:58:53 - train: epoch 0027, iter [04400, 05004], lr: 0.176840, loss: 2.3896
2022-03-11 00:59:35 - train: epoch 0027, iter [04500, 05004], lr: 0.176840, loss: 2.3365
2022-03-11 01:00:18 - train: epoch 0027, iter [04600, 05004], lr: 0.176840, loss: 2.2729
2022-03-11 01:01:00 - train: epoch 0027, iter [04700, 05004], lr: 0.176840, loss: 2.4515
2022-03-11 01:01:42 - train: epoch 0027, iter [04800, 05004], lr: 0.176840, loss: 2.6641
2022-03-11 01:02:26 - train: epoch 0027, iter [04900, 05004], lr: 0.176840, loss: 2.3455
2022-03-11 01:03:09 - train: epoch 0027, iter [05000, 05004], lr: 0.176840, loss: 2.1960
2022-03-11 01:03:10 - train: epoch 027, train_loss: 2.4564
2022-03-11 01:04:40 - eval: epoch: 027, acc1: 49.060%, acc5: 75.044%, test_loss: 2.2059, per_image_load_time: 2.204ms, per_image_inference_time: 0.251ms
2022-03-11 01:04:40 - until epoch: 027, best_acc1: 49.060%
2022-03-11 01:04:40 - epoch 028 lr: 0.1746821476984154
2022-03-11 01:05:26 - train: epoch 0028, iter [00100, 05004], lr: 0.174682, loss: 2.1644
2022-03-11 01:06:08 - train: epoch 0028, iter [00200, 05004], lr: 0.174682, loss: 2.3957
2022-03-11 01:06:50 - train: epoch 0028, iter [00300, 05004], lr: 0.174682, loss: 2.2279
2022-03-11 01:07:29 - train: epoch 0028, iter [00400, 05004], lr: 0.174682, loss: 2.4123
2022-03-11 01:08:10 - train: epoch 0028, iter [00500, 05004], lr: 0.174682, loss: 2.1860
2022-03-11 01:08:52 - train: epoch 0028, iter [00600, 05004], lr: 0.174682, loss: 2.6210
2022-03-11 01:09:33 - train: epoch 0028, iter [00700, 05004], lr: 0.174682, loss: 2.5886
2022-03-11 01:10:14 - train: epoch 0028, iter [00800, 05004], lr: 0.174682, loss: 2.1230
2022-03-11 01:10:56 - train: epoch 0028, iter [00900, 05004], lr: 0.174682, loss: 2.1889
2022-03-11 01:11:38 - train: epoch 0028, iter [01000, 05004], lr: 0.174682, loss: 2.7117
2022-03-11 01:12:20 - train: epoch 0028, iter [01100, 05004], lr: 0.174682, loss: 2.3937
2022-03-11 01:13:00 - train: epoch 0028, iter [01200, 05004], lr: 0.174682, loss: 2.3474
2022-03-11 01:13:43 - train: epoch 0028, iter [01300, 05004], lr: 0.174682, loss: 2.3945
2022-03-11 01:14:26 - train: epoch 0028, iter [01400, 05004], lr: 0.174682, loss: 2.4902
2022-03-11 01:15:11 - train: epoch 0028, iter [01500, 05004], lr: 0.174682, loss: 2.2821
2022-03-11 01:15:56 - train: epoch 0028, iter [01600, 05004], lr: 0.174682, loss: 2.3559
2022-03-11 01:16:40 - train: epoch 0028, iter [01700, 05004], lr: 0.174682, loss: 2.5358
2022-03-11 01:17:24 - train: epoch 0028, iter [01800, 05004], lr: 0.174682, loss: 2.3404
2022-03-11 01:18:08 - train: epoch 0028, iter [01900, 05004], lr: 0.174682, loss: 2.5023
2022-03-11 01:18:52 - train: epoch 0028, iter [02000, 05004], lr: 0.174682, loss: 2.3979
2022-03-11 01:19:36 - train: epoch 0028, iter [02100, 05004], lr: 0.174682, loss: 2.6595
2022-03-11 01:20:20 - train: epoch 0028, iter [02200, 05004], lr: 0.174682, loss: 2.4455
2022-03-11 01:21:04 - train: epoch 0028, iter [02300, 05004], lr: 0.174682, loss: 2.6381
2022-03-11 01:21:48 - train: epoch 0028, iter [02400, 05004], lr: 0.174682, loss: 2.5568
2022-03-11 01:22:30 - train: epoch 0028, iter [02500, 05004], lr: 0.174682, loss: 2.5543
2022-03-11 01:23:11 - train: epoch 0028, iter [02600, 05004], lr: 0.174682, loss: 2.3758
2022-03-11 01:23:51 - train: epoch 0028, iter [02700, 05004], lr: 0.174682, loss: 2.4825
2022-03-11 01:24:32 - train: epoch 0028, iter [02800, 05004], lr: 0.174682, loss: 2.5281
2022-03-11 01:25:13 - train: epoch 0028, iter [02900, 05004], lr: 0.174682, loss: 2.4538
2022-03-11 01:25:54 - train: epoch 0028, iter [03000, 05004], lr: 0.174682, loss: 2.4901
2022-03-11 01:26:34 - train: epoch 0028, iter [03100, 05004], lr: 0.174682, loss: 2.5747
2022-03-11 01:27:14 - train: epoch 0028, iter [03200, 05004], lr: 0.174682, loss: 2.3035
2022-03-11 01:27:56 - train: epoch 0028, iter [03300, 05004], lr: 0.174682, loss: 2.4085
2022-03-11 01:28:41 - train: epoch 0028, iter [03400, 05004], lr: 0.174682, loss: 2.5765
2022-03-11 01:29:25 - train: epoch 0028, iter [03500, 05004], lr: 0.174682, loss: 2.3759
2022-03-11 01:30:11 - train: epoch 0028, iter [03600, 05004], lr: 0.174682, loss: 2.4493
2022-03-11 01:30:55 - train: epoch 0028, iter [03700, 05004], lr: 0.174682, loss: 2.6089
2022-03-11 01:31:40 - train: epoch 0028, iter [03800, 05004], lr: 0.174682, loss: 2.1027
2022-03-11 01:32:24 - train: epoch 0028, iter [03900, 05004], lr: 0.174682, loss: 2.4521
2022-03-11 01:33:09 - train: epoch 0028, iter [04000, 05004], lr: 0.174682, loss: 2.4661
2022-03-11 01:33:52 - train: epoch 0028, iter [04100, 05004], lr: 0.174682, loss: 2.3892
2022-03-11 01:34:35 - train: epoch 0028, iter [04200, 05004], lr: 0.174682, loss: 2.5480
2022-03-11 01:35:19 - train: epoch 0028, iter [04300, 05004], lr: 0.174682, loss: 2.5042
2022-03-11 01:36:02 - train: epoch 0028, iter [04400, 05004], lr: 0.174682, loss: 2.2822
2022-03-11 01:36:45 - train: epoch 0028, iter [04500, 05004], lr: 0.174682, loss: 2.6385
2022-03-11 01:37:29 - train: epoch 0028, iter [04600, 05004], lr: 0.174682, loss: 2.5545
2022-03-11 01:38:13 - train: epoch 0028, iter [04700, 05004], lr: 0.174682, loss: 2.4387
2022-03-11 01:38:57 - train: epoch 0028, iter [04800, 05004], lr: 0.174682, loss: 2.5638
2022-03-11 01:39:41 - train: epoch 0028, iter [04900, 05004], lr: 0.174682, loss: 2.3105
2022-03-11 01:40:26 - train: epoch 0028, iter [05000, 05004], lr: 0.174682, loss: 2.1904
2022-03-11 01:40:28 - train: epoch 028, train_loss: 2.4444
2022-03-11 01:42:02 - eval: epoch: 028, acc1: 48.728%, acc5: 74.612%, test_loss: 2.2287, per_image_load_time: 2.675ms, per_image_inference_time: 0.277ms
2022-03-11 01:42:02 - until epoch: 028, best_acc1: 49.060%
2022-03-11 01:42:02 - epoch 029 lr: 0.1724425204791089
2022-03-11 01:42:51 - train: epoch 0029, iter [00100, 05004], lr: 0.172443, loss: 2.4492
2022-03-11 01:43:36 - train: epoch 0029, iter [00200, 05004], lr: 0.172443, loss: 2.4686
2022-03-11 01:44:21 - train: epoch 0029, iter [00300, 05004], lr: 0.172443, loss: 2.1494
2022-03-11 01:45:07 - train: epoch 0029, iter [00400, 05004], lr: 0.172443, loss: 2.5396
2022-03-11 01:45:53 - train: epoch 0029, iter [00500, 05004], lr: 0.172443, loss: 2.3407
2022-03-11 01:46:37 - train: epoch 0029, iter [00600, 05004], lr: 0.172443, loss: 2.4345
2022-03-11 01:47:23 - train: epoch 0029, iter [00700, 05004], lr: 0.172443, loss: 2.3433
2022-03-11 01:48:09 - train: epoch 0029, iter [00800, 05004], lr: 0.172443, loss: 2.2931
2022-03-11 01:48:53 - train: epoch 0029, iter [00900, 05004], lr: 0.172443, loss: 2.5910
2022-03-11 01:49:37 - train: epoch 0029, iter [01000, 05004], lr: 0.172443, loss: 2.3234
2022-03-11 01:50:23 - train: epoch 0029, iter [01100, 05004], lr: 0.172443, loss: 2.5310
2022-03-11 01:51:08 - train: epoch 0029, iter [01200, 05004], lr: 0.172443, loss: 2.6455
2022-03-11 01:51:52 - train: epoch 0029, iter [01300, 05004], lr: 0.172443, loss: 2.6219
2022-03-11 01:52:38 - train: epoch 0029, iter [01400, 05004], lr: 0.172443, loss: 2.3797
2022-03-11 01:53:22 - train: epoch 0029, iter [01500, 05004], lr: 0.172443, loss: 2.4478
2022-03-11 01:54:02 - train: epoch 0029, iter [01600, 05004], lr: 0.172443, loss: 2.4857
2022-03-11 01:54:42 - train: epoch 0029, iter [01700, 05004], lr: 0.172443, loss: 2.2883
2022-03-11 01:55:21 - train: epoch 0029, iter [01800, 05004], lr: 0.172443, loss: 2.4266
2022-03-11 01:56:02 - train: epoch 0029, iter [01900, 05004], lr: 0.172443, loss: 2.4110
2022-03-11 01:56:41 - train: epoch 0029, iter [02000, 05004], lr: 0.172443, loss: 2.2891
2022-03-11 01:57:23 - train: epoch 0029, iter [02100, 05004], lr: 0.172443, loss: 2.5457
2022-03-11 01:58:02 - train: epoch 0029, iter [02200, 05004], lr: 0.172443, loss: 2.6281
2022-03-11 01:58:44 - train: epoch 0029, iter [02300, 05004], lr: 0.172443, loss: 2.3886
2022-03-11 01:59:25 - train: epoch 0029, iter [02400, 05004], lr: 0.172443, loss: 2.3937
2022-03-11 02:00:06 - train: epoch 0029, iter [02500, 05004], lr: 0.172443, loss: 2.5617
2022-03-11 02:00:48 - train: epoch 0029, iter [02600, 05004], lr: 0.172443, loss: 2.4831
2022-03-11 02:01:30 - train: epoch 0029, iter [02700, 05004], lr: 0.172443, loss: 2.4201
2022-03-11 02:02:11 - train: epoch 0029, iter [02800, 05004], lr: 0.172443, loss: 2.3662
2022-03-11 02:02:53 - train: epoch 0029, iter [02900, 05004], lr: 0.172443, loss: 2.1750
2022-03-11 02:03:35 - train: epoch 0029, iter [03000, 05004], lr: 0.172443, loss: 2.3759
2022-03-11 02:04:14 - train: epoch 0029, iter [03100, 05004], lr: 0.172443, loss: 2.4181
2022-03-11 02:04:56 - train: epoch 0029, iter [03200, 05004], lr: 0.172443, loss: 2.5341
2022-03-11 02:05:36 - train: epoch 0029, iter [03300, 05004], lr: 0.172443, loss: 2.2782
2022-03-11 02:06:15 - train: epoch 0029, iter [03400, 05004], lr: 0.172443, loss: 2.3953
2022-03-11 02:06:54 - train: epoch 0029, iter [03500, 05004], lr: 0.172443, loss: 2.6367
2022-03-11 02:07:34 - train: epoch 0029, iter [03600, 05004], lr: 0.172443, loss: 2.3547
2022-03-11 02:08:14 - train: epoch 0029, iter [03700, 05004], lr: 0.172443, loss: 2.5381
2022-03-11 02:08:54 - train: epoch 0029, iter [03800, 05004], lr: 0.172443, loss: 2.3465
2022-03-11 02:09:34 - train: epoch 0029, iter [03900, 05004], lr: 0.172443, loss: 2.3003
2022-03-11 02:10:14 - train: epoch 0029, iter [04000, 05004], lr: 0.172443, loss: 2.4685
2022-03-11 02:10:53 - train: epoch 0029, iter [04100, 05004], lr: 0.172443, loss: 2.3939
2022-03-11 02:11:33 - train: epoch 0029, iter [04200, 05004], lr: 0.172443, loss: 2.3205
2022-03-11 02:12:13 - train: epoch 0029, iter [04300, 05004], lr: 0.172443, loss: 2.4901
2022-03-11 02:12:53 - train: epoch 0029, iter [04400, 05004], lr: 0.172443, loss: 2.4441
2022-03-11 02:13:33 - train: epoch 0029, iter [04500, 05004], lr: 0.172443, loss: 2.4739
2022-03-11 02:14:13 - train: epoch 0029, iter [04600, 05004], lr: 0.172443, loss: 2.4475
2022-03-11 02:14:53 - train: epoch 0029, iter [04700, 05004], lr: 0.172443, loss: 2.2235
2022-03-11 02:15:34 - train: epoch 0029, iter [04800, 05004], lr: 0.172443, loss: 2.4490
2022-03-11 02:16:15 - train: epoch 0029, iter [04900, 05004], lr: 0.172443, loss: 2.7612
2022-03-11 02:16:55 - train: epoch 0029, iter [05000, 05004], lr: 0.172443, loss: 2.1069
2022-03-11 02:16:57 - train: epoch 029, train_loss: 2.4337
2022-03-11 02:18:21 - eval: epoch: 029, acc1: 49.730%, acc5: 75.588%, test_loss: 2.1698, per_image_load_time: 3.000ms, per_image_inference_time: 0.248ms
2022-03-11 02:18:22 - until epoch: 029, best_acc1: 49.730%
2022-03-11 02:18:22 - epoch 030 lr: 0.17012367842724885
2022-03-11 02:19:05 - train: epoch 0030, iter [00100, 05004], lr: 0.170124, loss: 2.4398
2022-03-11 02:19:44 - train: epoch 0030, iter [00200, 05004], lr: 0.170124, loss: 2.6253
2022-03-11 02:20:24 - train: epoch 0030, iter [00300, 05004], lr: 0.170124, loss: 2.3716
2022-03-11 02:21:04 - train: epoch 0030, iter [00400, 05004], lr: 0.170124, loss: 2.3887
2022-03-11 02:21:44 - train: epoch 0030, iter [00500, 05004], lr: 0.170124, loss: 2.3667
2022-03-11 02:22:25 - train: epoch 0030, iter [00600, 05004], lr: 0.170124, loss: 2.4608
2022-03-11 02:23:05 - train: epoch 0030, iter [00700, 05004], lr: 0.170124, loss: 2.3485
2022-03-11 02:23:45 - train: epoch 0030, iter [00800, 05004], lr: 0.170124, loss: 2.3265
2022-03-11 02:24:25 - train: epoch 0030, iter [00900, 05004], lr: 0.170124, loss: 2.6028
2022-03-11 02:25:06 - train: epoch 0030, iter [01000, 05004], lr: 0.170124, loss: 2.3913
2022-03-11 02:25:46 - train: epoch 0030, iter [01100, 05004], lr: 0.170124, loss: 2.2435
2022-03-11 02:26:27 - train: epoch 0030, iter [01200, 05004], lr: 0.170124, loss: 2.3837
2022-03-11 02:27:08 - train: epoch 0030, iter [01300, 05004], lr: 0.170124, loss: 2.2886
2022-03-11 02:27:48 - train: epoch 0030, iter [01400, 05004], lr: 0.170124, loss: 2.3482
2022-03-11 02:28:28 - train: epoch 0030, iter [01500, 05004], lr: 0.170124, loss: 2.1225
2022-03-11 02:29:09 - train: epoch 0030, iter [01600, 05004], lr: 0.170124, loss: 2.5561
2022-03-11 02:29:49 - train: epoch 0030, iter [01700, 05004], lr: 0.170124, loss: 2.3830
2022-03-11 02:30:29 - train: epoch 0030, iter [01800, 05004], lr: 0.170124, loss: 2.4282
2022-03-11 02:31:09 - train: epoch 0030, iter [01900, 05004], lr: 0.170124, loss: 2.5232
2022-03-11 02:31:48 - train: epoch 0030, iter [02000, 05004], lr: 0.170124, loss: 2.4417
2022-03-11 02:32:29 - train: epoch 0030, iter [02100, 05004], lr: 0.170124, loss: 2.4450
2022-03-11 02:33:09 - train: epoch 0030, iter [02200, 05004], lr: 0.170124, loss: 2.4139
2022-03-11 02:33:48 - train: epoch 0030, iter [02300, 05004], lr: 0.170124, loss: 2.5435
2022-03-11 02:34:26 - train: epoch 0030, iter [02400, 05004], lr: 0.170124, loss: 2.6406
2022-03-11 02:35:06 - train: epoch 0030, iter [02500, 05004], lr: 0.170124, loss: 2.5340
2022-03-11 02:35:45 - train: epoch 0030, iter [02600, 05004], lr: 0.170124, loss: 2.5658
2022-03-11 02:36:25 - train: epoch 0030, iter [02700, 05004], lr: 0.170124, loss: 2.4950
2022-03-11 02:37:05 - train: epoch 0030, iter [02800, 05004], lr: 0.170124, loss: 2.4398
2022-03-11 02:37:45 - train: epoch 0030, iter [02900, 05004], lr: 0.170124, loss: 2.3761
2022-03-11 02:38:24 - train: epoch 0030, iter [03000, 05004], lr: 0.170124, loss: 2.6942
2022-03-11 02:39:05 - train: epoch 0030, iter [03100, 05004], lr: 0.170124, loss: 2.4653
2022-03-11 02:39:45 - train: epoch 0030, iter [03200, 05004], lr: 0.170124, loss: 2.3249
2022-03-11 02:40:26 - train: epoch 0030, iter [03300, 05004], lr: 0.170124, loss: 2.4745
2022-03-11 02:41:06 - train: epoch 0030, iter [03400, 05004], lr: 0.170124, loss: 2.1743
2022-03-11 02:41:46 - train: epoch 0030, iter [03500, 05004], lr: 0.170124, loss: 2.5078
2022-03-11 02:42:28 - train: epoch 0030, iter [03600, 05004], lr: 0.170124, loss: 2.5043
2022-03-11 02:43:07 - train: epoch 0030, iter [03700, 05004], lr: 0.170124, loss: 2.4212
2022-03-11 02:43:48 - train: epoch 0030, iter [03800, 05004], lr: 0.170124, loss: 2.4093
2022-03-11 02:44:28 - train: epoch 0030, iter [03900, 05004], lr: 0.170124, loss: 2.3959
2022-03-11 02:45:08 - train: epoch 0030, iter [04000, 05004], lr: 0.170124, loss: 2.2561
2022-03-11 02:45:49 - train: epoch 0030, iter [04100, 05004], lr: 0.170124, loss: 2.2349
2022-03-11 02:46:29 - train: epoch 0030, iter [04200, 05004], lr: 0.170124, loss: 2.6767
2022-03-11 02:47:10 - train: epoch 0030, iter [04300, 05004], lr: 0.170124, loss: 2.4150
2022-03-11 02:47:51 - train: epoch 0030, iter [04400, 05004], lr: 0.170124, loss: 2.5243
2022-03-11 02:48:31 - train: epoch 0030, iter [04500, 05004], lr: 0.170124, loss: 2.3983
2022-03-11 02:49:12 - train: epoch 0030, iter [04600, 05004], lr: 0.170124, loss: 2.2435
2022-03-11 02:49:52 - train: epoch 0030, iter [04700, 05004], lr: 0.170124, loss: 2.2968
2022-03-11 02:50:32 - train: epoch 0030, iter [04800, 05004], lr: 0.170124, loss: 2.4399
2022-03-11 02:51:13 - train: epoch 0030, iter [04900, 05004], lr: 0.170124, loss: 2.7014
2022-03-11 02:51:54 - train: epoch 0030, iter [05000, 05004], lr: 0.170124, loss: 2.4948
2022-03-11 02:51:56 - train: epoch 030, train_loss: 2.4252
2022-03-11 02:53:17 - eval: epoch: 030, acc1: 48.288%, acc5: 74.804%, test_loss: 2.2266, per_image_load_time: 2.566ms, per_image_inference_time: 0.256ms
2022-03-11 02:53:17 - until epoch: 030, best_acc1: 49.730%
2022-03-11 02:53:17 - epoch 031 lr: 0.16772815716257414
2022-03-11 02:53:57 - train: epoch 0031, iter [00100, 05004], lr: 0.167728, loss: 2.3885
2022-03-11 02:54:35 - train: epoch 0031, iter [00200, 05004], lr: 0.167728, loss: 2.5324
2022-03-11 02:55:13 - train: epoch 0031, iter [00300, 05004], lr: 0.167728, loss: 2.2912
2022-03-11 02:55:52 - train: epoch 0031, iter [00400, 05004], lr: 0.167728, loss: 2.3066
2022-03-11 02:56:32 - train: epoch 0031, iter [00500, 05004], lr: 0.167728, loss: 2.3568
2022-03-11 02:57:11 - train: epoch 0031, iter [00600, 05004], lr: 0.167728, loss: 2.3435
2022-03-11 02:57:50 - train: epoch 0031, iter [00700, 05004], lr: 0.167728, loss: 2.4114
2022-03-11 02:58:29 - train: epoch 0031, iter [00800, 05004], lr: 0.167728, loss: 2.5956
2022-03-11 02:59:07 - train: epoch 0031, iter [00900, 05004], lr: 0.167728, loss: 2.3405
2022-03-11 02:59:46 - train: epoch 0031, iter [01000, 05004], lr: 0.167728, loss: 2.5445
2022-03-11 03:00:25 - train: epoch 0031, iter [01100, 05004], lr: 0.167728, loss: 2.5208
2022-03-11 03:01:03 - train: epoch 0031, iter [01200, 05004], lr: 0.167728, loss: 2.4523
2022-03-11 03:01:43 - train: epoch 0031, iter [01300, 05004], lr: 0.167728, loss: 2.1057
2022-03-11 03:02:22 - train: epoch 0031, iter [01400, 05004], lr: 0.167728, loss: 2.3395
2022-03-11 03:03:01 - train: epoch 0031, iter [01500, 05004], lr: 0.167728, loss: 2.6509
2022-03-11 03:03:41 - train: epoch 0031, iter [01600, 05004], lr: 0.167728, loss: 2.3654
2022-03-11 03:04:21 - train: epoch 0031, iter [01700, 05004], lr: 0.167728, loss: 2.1976
2022-03-11 03:05:00 - train: epoch 0031, iter [01800, 05004], lr: 0.167728, loss: 2.2056
2022-03-11 03:05:40 - train: epoch 0031, iter [01900, 05004], lr: 0.167728, loss: 2.2935
2022-03-11 03:06:18 - train: epoch 0031, iter [02000, 05004], lr: 0.167728, loss: 2.4600
2022-03-11 03:06:57 - train: epoch 0031, iter [02100, 05004], lr: 0.167728, loss: 2.4565
2022-03-11 03:07:36 - train: epoch 0031, iter [02200, 05004], lr: 0.167728, loss: 2.2387
2022-03-11 03:08:15 - train: epoch 0031, iter [02300, 05004], lr: 0.167728, loss: 2.2930
2022-03-11 03:08:53 - train: epoch 0031, iter [02400, 05004], lr: 0.167728, loss: 2.5539
2022-03-11 03:09:32 - train: epoch 0031, iter [02500, 05004], lr: 0.167728, loss: 2.4504
2022-03-11 03:10:10 - train: epoch 0031, iter [02600, 05004], lr: 0.167728, loss: 2.5289
2022-03-11 03:10:49 - train: epoch 0031, iter [02700, 05004], lr: 0.167728, loss: 2.4368
2022-03-11 03:11:30 - train: epoch 0031, iter [02800, 05004], lr: 0.167728, loss: 3.0089
2022-03-11 03:12:09 - train: epoch 0031, iter [02900, 05004], lr: 0.167728, loss: 2.5204
2022-03-11 03:12:50 - train: epoch 0031, iter [03000, 05004], lr: 0.167728, loss: 2.5593
2022-03-11 03:13:28 - train: epoch 0031, iter [03100, 05004], lr: 0.167728, loss: 2.5596
2022-03-11 03:14:08 - train: epoch 0031, iter [03200, 05004], lr: 0.167728, loss: 2.5054
2022-03-11 03:14:48 - train: epoch 0031, iter [03300, 05004], lr: 0.167728, loss: 2.3475
2022-03-11 03:15:27 - train: epoch 0031, iter [03400, 05004], lr: 0.167728, loss: 2.6038
2022-03-11 03:16:08 - train: epoch 0031, iter [03500, 05004], lr: 0.167728, loss: 2.3168
2022-03-11 03:16:47 - train: epoch 0031, iter [03600, 05004], lr: 0.167728, loss: 2.4239
2022-03-11 03:17:27 - train: epoch 0031, iter [03700, 05004], lr: 0.167728, loss: 2.3973
2022-03-11 03:18:06 - train: epoch 0031, iter [03800, 05004], lr: 0.167728, loss: 2.3475
2022-03-11 03:18:45 - train: epoch 0031, iter [03900, 05004], lr: 0.167728, loss: 2.4928
2022-03-11 03:19:26 - train: epoch 0031, iter [04000, 05004], lr: 0.167728, loss: 2.3028
2022-03-11 03:20:05 - train: epoch 0031, iter [04100, 05004], lr: 0.167728, loss: 2.5098
2022-03-11 03:20:44 - train: epoch 0031, iter [04200, 05004], lr: 0.167728, loss: 2.3386
2022-03-11 03:21:25 - train: epoch 0031, iter [04300, 05004], lr: 0.167728, loss: 2.4879
2022-03-11 03:22:05 - train: epoch 0031, iter [04400, 05004], lr: 0.167728, loss: 2.4172
2022-03-11 03:22:44 - train: epoch 0031, iter [04500, 05004], lr: 0.167728, loss: 2.4712
2022-03-11 03:23:23 - train: epoch 0031, iter [04600, 05004], lr: 0.167728, loss: 2.5712
2022-03-11 03:24:03 - train: epoch 0031, iter [04700, 05004], lr: 0.167728, loss: 2.1923
2022-03-11 03:24:44 - train: epoch 0031, iter [04800, 05004], lr: 0.167728, loss: 2.3050
2022-03-11 03:25:22 - train: epoch 0031, iter [04900, 05004], lr: 0.167728, loss: 2.3781
2022-03-11 03:26:02 - train: epoch 0031, iter [05000, 05004], lr: 0.167728, loss: 2.3614
2022-03-11 03:26:04 - train: epoch 031, train_loss: 2.4139
2022-03-11 03:27:24 - eval: epoch: 031, acc1: 49.692%, acc5: 75.566%, test_loss: 2.1696, per_image_load_time: 2.783ms, per_image_inference_time: 0.245ms
2022-03-11 03:27:24 - until epoch: 031, best_acc1: 49.730%
2022-03-11 03:27:24 - epoch 032 lr: 0.16525857615241688
2022-03-11 03:28:05 - train: epoch 0032, iter [00100, 05004], lr: 0.165259, loss: 2.2475
2022-03-11 03:28:43 - train: epoch 0032, iter [00200, 05004], lr: 0.165259, loss: 2.5064
2022-03-11 03:29:22 - train: epoch 0032, iter [00300, 05004], lr: 0.165259, loss: 2.2084
2022-03-11 03:30:02 - train: epoch 0032, iter [00400, 05004], lr: 0.165259, loss: 2.5225
2022-03-11 03:30:42 - train: epoch 0032, iter [00500, 05004], lr: 0.165259, loss: 2.3739
2022-03-11 03:31:22 - train: epoch 0032, iter [00600, 05004], lr: 0.165259, loss: 2.4884
2022-03-11 03:32:02 - train: epoch 0032, iter [00700, 05004], lr: 0.165259, loss: 2.3332
2022-03-11 03:32:43 - train: epoch 0032, iter [00800, 05004], lr: 0.165259, loss: 2.3077
2022-03-11 03:33:23 - train: epoch 0032, iter [00900, 05004], lr: 0.165259, loss: 2.3572
2022-03-11 03:34:03 - train: epoch 0032, iter [01000, 05004], lr: 0.165259, loss: 2.5068
2022-03-11 03:34:43 - train: epoch 0032, iter [01100, 05004], lr: 0.165259, loss: 2.4310
2022-03-11 03:35:22 - train: epoch 0032, iter [01200, 05004], lr: 0.165259, loss: 2.5536
2022-03-11 03:36:00 - train: epoch 0032, iter [01300, 05004], lr: 0.165259, loss: 2.4692
2022-03-11 03:36:37 - train: epoch 0032, iter [01400, 05004], lr: 0.165259, loss: 2.4676
2022-03-11 03:37:17 - train: epoch 0032, iter [01500, 05004], lr: 0.165259, loss: 2.4050
2022-03-11 03:37:57 - train: epoch 0032, iter [01600, 05004], lr: 0.165259, loss: 2.5188
2022-03-11 03:38:35 - train: epoch 0032, iter [01700, 05004], lr: 0.165259, loss: 2.3277
2022-03-11 03:39:12 - train: epoch 0032, iter [01800, 05004], lr: 0.165259, loss: 2.7144
2022-03-11 03:39:50 - train: epoch 0032, iter [01900, 05004], lr: 0.165259, loss: 2.1151
2022-03-11 03:40:27 - train: epoch 0032, iter [02000, 05004], lr: 0.165259, loss: 2.3751
2022-03-11 03:41:04 - train: epoch 0032, iter [02100, 05004], lr: 0.165259, loss: 2.2835
2022-03-11 03:41:43 - train: epoch 0032, iter [02200, 05004], lr: 0.165259, loss: 2.3278
2022-03-11 03:42:21 - train: epoch 0032, iter [02300, 05004], lr: 0.165259, loss: 2.2657
2022-03-11 03:43:01 - train: epoch 0032, iter [02400, 05004], lr: 0.165259, loss: 2.5288
2022-03-11 03:43:40 - train: epoch 0032, iter [02500, 05004], lr: 0.165259, loss: 2.2152
2022-03-11 03:44:17 - train: epoch 0032, iter [02600, 05004], lr: 0.165259, loss: 2.1793
2022-03-11 03:44:55 - train: epoch 0032, iter [02700, 05004], lr: 0.165259, loss: 2.4369
2022-03-11 03:45:33 - train: epoch 0032, iter [02800, 05004], lr: 0.165259, loss: 2.4478
2022-03-11 03:46:10 - train: epoch 0032, iter [02900, 05004], lr: 0.165259, loss: 2.3470
2022-03-11 03:46:48 - train: epoch 0032, iter [03000, 05004], lr: 0.165259, loss: 2.3981
2022-03-11 03:47:25 - train: epoch 0032, iter [03100, 05004], lr: 0.165259, loss: 2.4604
2022-03-11 03:48:01 - train: epoch 0032, iter [03200, 05004], lr: 0.165259, loss: 2.5488
2022-03-11 03:48:40 - train: epoch 0032, iter [03300, 05004], lr: 0.165259, loss: 2.2213
2022-03-11 03:49:17 - train: epoch 0032, iter [03400, 05004], lr: 0.165259, loss: 2.2568
2022-03-11 03:49:54 - train: epoch 0032, iter [03500, 05004], lr: 0.165259, loss: 2.3241
2022-03-11 03:50:33 - train: epoch 0032, iter [03600, 05004], lr: 0.165259, loss: 2.4642
2022-03-11 03:51:13 - train: epoch 0032, iter [03700, 05004], lr: 0.165259, loss: 2.4518
2022-03-11 03:51:52 - train: epoch 0032, iter [03800, 05004], lr: 0.165259, loss: 2.1136
2022-03-11 03:52:31 - train: epoch 0032, iter [03900, 05004], lr: 0.165259, loss: 2.2649
2022-03-11 03:53:11 - train: epoch 0032, iter [04000, 05004], lr: 0.165259, loss: 2.3205
2022-03-11 03:53:50 - train: epoch 0032, iter [04100, 05004], lr: 0.165259, loss: 2.4055
2022-03-11 03:54:28 - train: epoch 0032, iter [04200, 05004], lr: 0.165259, loss: 2.4443
2022-03-11 03:55:07 - train: epoch 0032, iter [04300, 05004], lr: 0.165259, loss: 2.5386
2022-03-11 03:55:46 - train: epoch 0032, iter [04400, 05004], lr: 0.165259, loss: 2.2798
2022-03-11 03:56:26 - train: epoch 0032, iter [04500, 05004], lr: 0.165259, loss: 2.6015
2022-03-11 03:57:05 - train: epoch 0032, iter [04600, 05004], lr: 0.165259, loss: 2.4593
2022-03-11 03:57:43 - train: epoch 0032, iter [04700, 05004], lr: 0.165259, loss: 2.5678
2022-03-11 03:58:22 - train: epoch 0032, iter [04800, 05004], lr: 0.165259, loss: 2.4163
2022-03-11 03:59:00 - train: epoch 0032, iter [04900, 05004], lr: 0.165259, loss: 2.4002
2022-03-11 03:59:38 - train: epoch 0032, iter [05000, 05004], lr: 0.165259, loss: 2.3817
2022-03-11 03:59:40 - train: epoch 032, train_loss: 2.4033
2022-03-11 04:01:01 - eval: epoch: 032, acc1: 49.704%, acc5: 75.636%, test_loss: 2.1691, per_image_load_time: 2.949ms, per_image_inference_time: 0.244ms
2022-03-11 04:01:01 - until epoch: 032, best_acc1: 49.730%
2022-03-11 04:01:01 - epoch 033 lr: 0.16271763584735371
2022-03-11 04:01:42 - train: epoch 0033, iter [00100, 05004], lr: 0.162718, loss: 2.2430
2022-03-11 04:02:20 - train: epoch 0033, iter [00200, 05004], lr: 0.162718, loss: 2.2844
2022-03-11 04:02:59 - train: epoch 0033, iter [00300, 05004], lr: 0.162718, loss: 2.3265
2022-03-11 04:03:37 - train: epoch 0033, iter [00400, 05004], lr: 0.162718, loss: 2.2125
2022-03-11 04:04:16 - train: epoch 0033, iter [00500, 05004], lr: 0.162718, loss: 2.7161
2022-03-11 04:04:55 - train: epoch 0033, iter [00600, 05004], lr: 0.162718, loss: 2.1294
2022-03-11 04:05:33 - train: epoch 0033, iter [00700, 05004], lr: 0.162718, loss: 2.4985
2022-03-11 04:06:13 - train: epoch 0033, iter [00800, 05004], lr: 0.162718, loss: 2.4095
2022-03-11 04:06:53 - train: epoch 0033, iter [00900, 05004], lr: 0.162718, loss: 2.4918
2022-03-11 04:07:33 - train: epoch 0033, iter [01000, 05004], lr: 0.162718, loss: 2.4112
2022-03-11 04:08:13 - train: epoch 0033, iter [01100, 05004], lr: 0.162718, loss: 2.1944
2022-03-11 04:08:53 - train: epoch 0033, iter [01200, 05004], lr: 0.162718, loss: 2.6280
2022-03-11 04:09:32 - train: epoch 0033, iter [01300, 05004], lr: 0.162718, loss: 2.1192
2022-03-11 04:10:12 - train: epoch 0033, iter [01400, 05004], lr: 0.162718, loss: 2.5105
2022-03-11 04:10:52 - train: epoch 0033, iter [01500, 05004], lr: 0.162718, loss: 2.6915
2022-03-11 04:11:32 - train: epoch 0033, iter [01600, 05004], lr: 0.162718, loss: 2.7746
2022-03-11 04:12:10 - train: epoch 0033, iter [01700, 05004], lr: 0.162718, loss: 2.2978
2022-03-11 04:12:51 - train: epoch 0033, iter [01800, 05004], lr: 0.162718, loss: 2.5210
2022-03-11 04:13:31 - train: epoch 0033, iter [01900, 05004], lr: 0.162718, loss: 2.3232
2022-03-11 04:14:10 - train: epoch 0033, iter [02000, 05004], lr: 0.162718, loss: 2.4074
2022-03-11 04:14:48 - train: epoch 0033, iter [02100, 05004], lr: 0.162718, loss: 2.4167
2022-03-11 04:15:27 - train: epoch 0033, iter [02200, 05004], lr: 0.162718, loss: 2.4973
2022-03-11 04:16:07 - train: epoch 0033, iter [02300, 05004], lr: 0.162718, loss: 2.5981
2022-03-11 04:16:47 - train: epoch 0033, iter [02400, 05004], lr: 0.162718, loss: 2.4404
2022-03-11 04:17:26 - train: epoch 0033, iter [02500, 05004], lr: 0.162718, loss: 2.3794
2022-03-11 04:18:05 - train: epoch 0033, iter [02600, 05004], lr: 0.162718, loss: 2.2850
2022-03-11 04:18:44 - train: epoch 0033, iter [02700, 05004], lr: 0.162718, loss: 2.4114
2022-03-11 04:19:22 - train: epoch 0033, iter [02800, 05004], lr: 0.162718, loss: 2.5774
2022-03-11 04:19:59 - train: epoch 0033, iter [02900, 05004], lr: 0.162718, loss: 2.2977
2022-03-11 04:20:39 - train: epoch 0033, iter [03000, 05004], lr: 0.162718, loss: 2.3917
2022-03-11 04:21:18 - train: epoch 0033, iter [03100, 05004], lr: 0.162718, loss: 2.4585
2022-03-11 04:21:58 - train: epoch 0033, iter [03200, 05004], lr: 0.162718, loss: 2.4065
2022-03-11 04:22:38 - train: epoch 0033, iter [03300, 05004], lr: 0.162718, loss: 2.4203
2022-03-11 04:23:17 - train: epoch 0033, iter [03400, 05004], lr: 0.162718, loss: 2.2713
2022-03-11 04:23:56 - train: epoch 0033, iter [03500, 05004], lr: 0.162718, loss: 2.3717
2022-03-11 04:24:35 - train: epoch 0033, iter [03600, 05004], lr: 0.162718, loss: 2.5871
2022-03-11 04:25:13 - train: epoch 0033, iter [03700, 05004], lr: 0.162718, loss: 2.3841
2022-03-11 04:25:53 - train: epoch 0033, iter [03800, 05004], lr: 0.162718, loss: 2.5491
2022-03-11 04:26:31 - train: epoch 0033, iter [03900, 05004], lr: 0.162718, loss: 2.4363
2022-03-11 04:27:11 - train: epoch 0033, iter [04000, 05004], lr: 0.162718, loss: 2.4898
2022-03-11 04:27:49 - train: epoch 0033, iter [04100, 05004], lr: 0.162718, loss: 2.3829
2022-03-11 04:28:28 - train: epoch 0033, iter [04200, 05004], lr: 0.162718, loss: 2.4370
2022-03-11 04:29:07 - train: epoch 0033, iter [04300, 05004], lr: 0.162718, loss: 2.4496
2022-03-11 04:29:45 - train: epoch 0033, iter [04400, 05004], lr: 0.162718, loss: 2.2689
2022-03-11 04:30:23 - train: epoch 0033, iter [04500, 05004], lr: 0.162718, loss: 2.5704
2022-03-11 04:31:01 - train: epoch 0033, iter [04600, 05004], lr: 0.162718, loss: 2.4858
2022-03-11 04:31:40 - train: epoch 0033, iter [04700, 05004], lr: 0.162718, loss: 2.4027
2022-03-11 04:32:17 - train: epoch 0033, iter [04800, 05004], lr: 0.162718, loss: 2.7012
2022-03-11 04:32:56 - train: epoch 0033, iter [04900, 05004], lr: 0.162718, loss: 2.1579
2022-03-11 04:33:35 - train: epoch 0033, iter [05000, 05004], lr: 0.162718, loss: 2.4305
2022-03-11 04:33:37 - train: epoch 033, train_loss: 2.3937
2022-03-11 04:34:57 - eval: epoch: 033, acc1: 48.802%, acc5: 74.972%, test_loss: 2.2132, per_image_load_time: 1.988ms, per_image_inference_time: 0.248ms
2022-03-11 04:34:57 - until epoch: 033, best_acc1: 49.730%
2022-03-11 04:34:57 - epoch 034 lr: 0.16010811472830253
2022-03-11 04:35:39 - train: epoch 0034, iter [00100, 05004], lr: 0.160108, loss: 2.2985
2022-03-11 04:36:16 - train: epoch 0034, iter [00200, 05004], lr: 0.160108, loss: 2.1951
2022-03-11 04:36:53 - train: epoch 0034, iter [00300, 05004], lr: 0.160108, loss: 2.1333
2022-03-11 04:37:30 - train: epoch 0034, iter [00400, 05004], lr: 0.160108, loss: 2.1284
2022-03-11 04:38:09 - train: epoch 0034, iter [00500, 05004], lr: 0.160108, loss: 2.2876
2022-03-11 04:38:48 - train: epoch 0034, iter [00600, 05004], lr: 0.160108, loss: 2.3472
2022-03-11 04:39:25 - train: epoch 0034, iter [00700, 05004], lr: 0.160108, loss: 2.2318
2022-03-11 04:40:03 - train: epoch 0034, iter [00800, 05004], lr: 0.160108, loss: 2.4302
2022-03-11 04:40:41 - train: epoch 0034, iter [00900, 05004], lr: 0.160108, loss: 2.3762
2022-03-11 04:41:18 - train: epoch 0034, iter [01000, 05004], lr: 0.160108, loss: 2.2918
2022-03-11 04:41:55 - train: epoch 0034, iter [01100, 05004], lr: 0.160108, loss: 2.2335
2022-03-11 04:42:34 - train: epoch 0034, iter [01200, 05004], lr: 0.160108, loss: 2.5095
2022-03-11 04:43:13 - train: epoch 0034, iter [01300, 05004], lr: 0.160108, loss: 2.3427
2022-03-11 04:43:51 - train: epoch 0034, iter [01400, 05004], lr: 0.160108, loss: 2.4100
2022-03-11 04:44:29 - train: epoch 0034, iter [01500, 05004], lr: 0.160108, loss: 2.3126
2022-03-11 04:45:08 - train: epoch 0034, iter [01600, 05004], lr: 0.160108, loss: 2.2689
2022-03-11 04:45:46 - train: epoch 0034, iter [01700, 05004], lr: 0.160108, loss: 2.3821
2022-03-11 04:46:26 - train: epoch 0034, iter [01800, 05004], lr: 0.160108, loss: 2.5937
2022-03-11 04:47:04 - train: epoch 0034, iter [01900, 05004], lr: 0.160108, loss: 2.3804
2022-03-11 04:47:43 - train: epoch 0034, iter [02000, 05004], lr: 0.160108, loss: 2.2299
2022-03-11 04:48:23 - train: epoch 0034, iter [02100, 05004], lr: 0.160108, loss: 2.4768
2022-03-11 04:49:01 - train: epoch 0034, iter [02200, 05004], lr: 0.160108, loss: 2.2220
2022-03-11 04:49:39 - train: epoch 0034, iter [02300, 05004], lr: 0.160108, loss: 2.3351
2022-03-11 04:50:17 - train: epoch 0034, iter [02400, 05004], lr: 0.160108, loss: 2.1452
2022-03-11 04:50:56 - train: epoch 0034, iter [02500, 05004], lr: 0.160108, loss: 2.3691
2022-03-11 04:51:34 - train: epoch 0034, iter [02600, 05004], lr: 0.160108, loss: 2.4179
2022-03-11 04:52:13 - train: epoch 0034, iter [02700, 05004], lr: 0.160108, loss: 2.6008
2022-03-11 04:52:51 - train: epoch 0034, iter [02800, 05004], lr: 0.160108, loss: 2.1923
2022-03-11 04:53:30 - train: epoch 0034, iter [02900, 05004], lr: 0.160108, loss: 2.3483
2022-03-11 04:54:09 - train: epoch 0034, iter [03000, 05004], lr: 0.160108, loss: 2.2858
2022-03-11 04:54:47 - train: epoch 0034, iter [03100, 05004], lr: 0.160108, loss: 2.2127
2022-03-11 04:55:25 - train: epoch 0034, iter [03200, 05004], lr: 0.160108, loss: 2.3942
2022-03-11 04:56:04 - train: epoch 0034, iter [03300, 05004], lr: 0.160108, loss: 2.4092
2022-03-11 04:56:42 - train: epoch 0034, iter [03400, 05004], lr: 0.160108, loss: 2.6458
2022-03-11 04:57:21 - train: epoch 0034, iter [03500, 05004], lr: 0.160108, loss: 2.2240
2022-03-11 04:58:00 - train: epoch 0034, iter [03600, 05004], lr: 0.160108, loss: 2.0612
2022-03-11 04:58:38 - train: epoch 0034, iter [03700, 05004], lr: 0.160108, loss: 2.5122
2022-03-11 04:59:18 - train: epoch 0034, iter [03800, 05004], lr: 0.160108, loss: 2.2450
2022-03-11 04:59:57 - train: epoch 0034, iter [03900, 05004], lr: 0.160108, loss: 2.5067
2022-03-11 05:00:37 - train: epoch 0034, iter [04000, 05004], lr: 0.160108, loss: 2.3740
2022-03-11 05:01:15 - train: epoch 0034, iter [04100, 05004], lr: 0.160108, loss: 2.5567
2022-03-11 05:01:54 - train: epoch 0034, iter [04200, 05004], lr: 0.160108, loss: 2.4463
2022-03-11 05:02:32 - train: epoch 0034, iter [04300, 05004], lr: 0.160108, loss: 2.3573
2022-03-11 05:03:12 - train: epoch 0034, iter [04400, 05004], lr: 0.160108, loss: 2.3510
2022-03-11 05:03:51 - train: epoch 0034, iter [04500, 05004], lr: 0.160108, loss: 2.4512
2022-03-11 05:04:31 - train: epoch 0034, iter [04600, 05004], lr: 0.160108, loss: 2.4808
2022-03-11 05:05:10 - train: epoch 0034, iter [04700, 05004], lr: 0.160108, loss: 2.5934
2022-03-11 05:05:50 - train: epoch 0034, iter [04800, 05004], lr: 0.160108, loss: 2.3521
2022-03-11 05:06:30 - train: epoch 0034, iter [04900, 05004], lr: 0.160108, loss: 2.4826
2022-03-11 05:07:09 - train: epoch 0034, iter [05000, 05004], lr: 0.160108, loss: 2.4281
2022-03-11 05:07:10 - train: epoch 034, train_loss: 2.3834
2022-03-11 05:08:29 - eval: epoch: 034, acc1: 49.222%, acc5: 75.366%, test_loss: 2.1862, per_image_load_time: 2.678ms, per_image_inference_time: 0.244ms
2022-03-11 05:08:29 - until epoch: 034, best_acc1: 49.730%
2022-03-11 05:08:29 - epoch 035 lr: 0.15743286626829436
2022-03-11 05:09:10 - train: epoch 0035, iter [00100, 05004], lr: 0.157433, loss: 2.2993
2022-03-11 05:09:50 - train: epoch 0035, iter [00200, 05004], lr: 0.157433, loss: 2.2486
2022-03-11 05:10:30 - train: epoch 0035, iter [00300, 05004], lr: 0.157433, loss: 2.5498
2022-03-11 05:11:10 - train: epoch 0035, iter [00400, 05004], lr: 0.157433, loss: 2.3185
2022-03-11 05:11:49 - train: epoch 0035, iter [00500, 05004], lr: 0.157433, loss: 2.1414
2022-03-11 05:12:29 - train: epoch 0035, iter [00600, 05004], lr: 0.157433, loss: 2.5563
2022-03-11 05:13:08 - train: epoch 0035, iter [00700, 05004], lr: 0.157433, loss: 2.4587
2022-03-11 05:13:49 - train: epoch 0035, iter [00800, 05004], lr: 0.157433, loss: 2.4579
2022-03-11 05:14:29 - train: epoch 0035, iter [00900, 05004], lr: 0.157433, loss: 2.4293
2022-03-11 05:15:10 - train: epoch 0035, iter [01000, 05004], lr: 0.157433, loss: 2.4721
2022-03-11 05:15:50 - train: epoch 0035, iter [01100, 05004], lr: 0.157433, loss: 2.4688
2022-03-11 05:16:30 - train: epoch 0035, iter [01200, 05004], lr: 0.157433, loss: 2.4346
2022-03-11 05:17:11 - train: epoch 0035, iter [01300, 05004], lr: 0.157433, loss: 2.3969
2022-03-11 05:17:52 - train: epoch 0035, iter [01400, 05004], lr: 0.157433, loss: 2.4627
2022-03-11 05:18:31 - train: epoch 0035, iter [01500, 05004], lr: 0.157433, loss: 2.2597
2022-03-11 05:19:11 - train: epoch 0035, iter [01600, 05004], lr: 0.157433, loss: 2.3562
2022-03-11 05:19:50 - train: epoch 0035, iter [01700, 05004], lr: 0.157433, loss: 2.1520
2022-03-11 05:20:31 - train: epoch 0035, iter [01800, 05004], lr: 0.157433, loss: 2.2820
2022-03-11 05:21:10 - train: epoch 0035, iter [01900, 05004], lr: 0.157433, loss: 2.3423
2022-03-11 05:21:52 - train: epoch 0035, iter [02000, 05004], lr: 0.157433, loss: 2.4959
2022-03-11 05:22:31 - train: epoch 0035, iter [02100, 05004], lr: 0.157433, loss: 2.2940
2022-03-11 05:23:10 - train: epoch 0035, iter [02200, 05004], lr: 0.157433, loss: 2.2686
2022-03-11 05:23:49 - train: epoch 0035, iter [02300, 05004], lr: 0.157433, loss: 2.1641
2022-03-11 05:24:28 - train: epoch 0035, iter [02400, 05004], lr: 0.157433, loss: 2.5803
2022-03-11 05:25:08 - train: epoch 0035, iter [02500, 05004], lr: 0.157433, loss: 2.5202
2022-03-11 05:25:48 - train: epoch 0035, iter [02600, 05004], lr: 0.157433, loss: 2.4294
2022-03-11 05:26:28 - train: epoch 0035, iter [02700, 05004], lr: 0.157433, loss: 2.3788
2022-03-11 05:27:07 - train: epoch 0035, iter [02800, 05004], lr: 0.157433, loss: 2.3727
2022-03-11 05:27:47 - train: epoch 0035, iter [02900, 05004], lr: 0.157433, loss: 2.3914
2022-03-11 05:28:27 - train: epoch 0035, iter [03000, 05004], lr: 0.157433, loss: 2.0996
2022-03-11 05:29:06 - train: epoch 0035, iter [03100, 05004], lr: 0.157433, loss: 2.3204
2022-03-11 05:29:45 - train: epoch 0035, iter [03200, 05004], lr: 0.157433, loss: 2.2839
2022-03-11 05:30:26 - train: epoch 0035, iter [03300, 05004], lr: 0.157433, loss: 2.4356
2022-03-11 05:31:06 - train: epoch 0035, iter [03400, 05004], lr: 0.157433, loss: 2.4049
2022-03-11 05:31:45 - train: epoch 0035, iter [03500, 05004], lr: 0.157433, loss: 2.0474
2022-03-11 05:32:25 - train: epoch 0035, iter [03600, 05004], lr: 0.157433, loss: 2.2683
2022-03-11 05:33:04 - train: epoch 0035, iter [03700, 05004], lr: 0.157433, loss: 2.2057
2022-03-11 05:33:43 - train: epoch 0035, iter [03800, 05004], lr: 0.157433, loss: 2.3762
2022-03-11 05:34:22 - train: epoch 0035, iter [03900, 05004], lr: 0.157433, loss: 2.4788
2022-03-11 05:35:02 - train: epoch 0035, iter [04000, 05004], lr: 0.157433, loss: 2.2224
2022-03-11 05:35:42 - train: epoch 0035, iter [04100, 05004], lr: 0.157433, loss: 2.6780
2022-03-11 05:36:21 - train: epoch 0035, iter [04200, 05004], lr: 0.157433, loss: 2.1858
2022-03-11 05:37:02 - train: epoch 0035, iter [04300, 05004], lr: 0.157433, loss: 2.3816
2022-03-11 05:37:40 - train: epoch 0035, iter [04400, 05004], lr: 0.157433, loss: 2.3412
2022-03-11 05:38:20 - train: epoch 0035, iter [04500, 05004], lr: 0.157433, loss: 2.3929
2022-03-11 05:38:59 - train: epoch 0035, iter [04600, 05004], lr: 0.157433, loss: 2.4286
2022-03-11 05:39:38 - train: epoch 0035, iter [04700, 05004], lr: 0.157433, loss: 2.6149
2022-03-11 05:40:17 - train: epoch 0035, iter [04800, 05004], lr: 0.157433, loss: 2.5150
2022-03-11 05:40:57 - train: epoch 0035, iter [04900, 05004], lr: 0.157433, loss: 2.4659
2022-03-11 05:41:36 - train: epoch 0035, iter [05000, 05004], lr: 0.157433, loss: 2.5086
2022-03-11 05:41:38 - train: epoch 035, train_loss: 2.3741
2022-03-11 05:43:00 - eval: epoch: 035, acc1: 50.378%, acc5: 76.194%, test_loss: 2.1292, per_image_load_time: 2.348ms, per_image_inference_time: 0.245ms
2022-03-11 05:43:01 - until epoch: 035, best_acc1: 50.378%
2022-03-11 05:43:01 - epoch 036 lr: 0.15469481581224273
2022-03-11 05:43:45 - train: epoch 0036, iter [00100, 05004], lr: 0.154695, loss: 2.1786
2022-03-11 05:44:25 - train: epoch 0036, iter [00200, 05004], lr: 0.154695, loss: 2.1002
2022-03-11 05:45:06 - train: epoch 0036, iter [00300, 05004], lr: 0.154695, loss: 2.2213
2022-03-11 05:45:46 - train: epoch 0036, iter [00400, 05004], lr: 0.154695, loss: 2.2958
2022-03-11 05:46:26 - train: epoch 0036, iter [00500, 05004], lr: 0.154695, loss: 2.4284
2022-03-11 05:47:06 - train: epoch 0036, iter [00600, 05004], lr: 0.154695, loss: 2.3346
2022-03-11 05:47:45 - train: epoch 0036, iter [00700, 05004], lr: 0.154695, loss: 2.2751
2022-03-11 05:48:25 - train: epoch 0036, iter [00800, 05004], lr: 0.154695, loss: 2.4017
2022-03-11 05:49:05 - train: epoch 0036, iter [00900, 05004], lr: 0.154695, loss: 2.2765
2022-03-11 05:49:44 - train: epoch 0036, iter [01000, 05004], lr: 0.154695, loss: 2.3230
2022-03-11 05:50:25 - train: epoch 0036, iter [01100, 05004], lr: 0.154695, loss: 2.3693
2022-03-11 05:51:05 - train: epoch 0036, iter [01200, 05004], lr: 0.154695, loss: 2.2818
2022-03-11 05:51:45 - train: epoch 0036, iter [01300, 05004], lr: 0.154695, loss: 2.3442
2022-03-11 05:52:25 - train: epoch 0036, iter [01400, 05004], lr: 0.154695, loss: 2.3326
2022-03-11 05:53:05 - train: epoch 0036, iter [01500, 05004], lr: 0.154695, loss: 2.3196
2022-03-11 05:53:45 - train: epoch 0036, iter [01600, 05004], lr: 0.154695, loss: 2.3451
2022-03-11 05:54:26 - train: epoch 0036, iter [01700, 05004], lr: 0.154695, loss: 2.1658
2022-03-11 05:55:07 - train: epoch 0036, iter [01800, 05004], lr: 0.154695, loss: 1.9972
2022-03-11 05:55:47 - train: epoch 0036, iter [01900, 05004], lr: 0.154695, loss: 2.5254
2022-03-11 05:56:27 - train: epoch 0036, iter [02000, 05004], lr: 0.154695, loss: 2.2118
2022-03-11 05:57:08 - train: epoch 0036, iter [02100, 05004], lr: 0.154695, loss: 2.4844
2022-03-11 05:57:48 - train: epoch 0036, iter [02200, 05004], lr: 0.154695, loss: 2.4394
2022-03-11 05:58:28 - train: epoch 0036, iter [02300, 05004], lr: 0.154695, loss: 2.3014
2022-03-11 05:59:07 - train: epoch 0036, iter [02400, 05004], lr: 0.154695, loss: 2.4561
2022-03-11 05:59:48 - train: epoch 0036, iter [02500, 05004], lr: 0.154695, loss: 2.1743
2022-03-11 06:00:29 - train: epoch 0036, iter [02600, 05004], lr: 0.154695, loss: 2.5884
2022-03-11 06:01:08 - train: epoch 0036, iter [02700, 05004], lr: 0.154695, loss: 2.1486
2022-03-11 06:01:48 - train: epoch 0036, iter [02800, 05004], lr: 0.154695, loss: 2.3674
2022-03-11 06:02:28 - train: epoch 0036, iter [02900, 05004], lr: 0.154695, loss: 2.2132
2022-03-11 06:03:08 - train: epoch 0036, iter [03000, 05004], lr: 0.154695, loss: 2.4397
2022-03-11 06:03:49 - train: epoch 0036, iter [03100, 05004], lr: 0.154695, loss: 2.5488
2022-03-11 06:04:29 - train: epoch 0036, iter [03200, 05004], lr: 0.154695, loss: 2.3854
2022-03-11 06:05:09 - train: epoch 0036, iter [03300, 05004], lr: 0.154695, loss: 2.2260
2022-03-11 06:05:49 - train: epoch 0036, iter [03400, 05004], lr: 0.154695, loss: 2.3233
2022-03-11 06:06:29 - train: epoch 0036, iter [03500, 05004], lr: 0.154695, loss: 2.6102
2022-03-11 06:07:09 - train: epoch 0036, iter [03600, 05004], lr: 0.154695, loss: 2.6089
2022-03-11 06:07:49 - train: epoch 0036, iter [03700, 05004], lr: 0.154695, loss: 2.4669
2022-03-11 06:08:28 - train: epoch 0036, iter [03800, 05004], lr: 0.154695, loss: 2.3170
2022-03-11 06:09:08 - train: epoch 0036, iter [03900, 05004], lr: 0.154695, loss: 2.2128
2022-03-11 06:09:49 - train: epoch 0036, iter [04000, 05004], lr: 0.154695, loss: 2.2499
2022-03-11 06:10:29 - train: epoch 0036, iter [04100, 05004], lr: 0.154695, loss: 2.3042
2022-03-11 06:11:10 - train: epoch 0036, iter [04200, 05004], lr: 0.154695, loss: 2.3996
2022-03-11 06:11:50 - train: epoch 0036, iter [04300, 05004], lr: 0.154695, loss: 2.2645
2022-03-11 06:12:31 - train: epoch 0036, iter [04400, 05004], lr: 0.154695, loss: 2.2082
2022-03-11 06:13:11 - train: epoch 0036, iter [04500, 05004], lr: 0.154695, loss: 2.3449
2022-03-11 06:13:51 - train: epoch 0036, iter [04600, 05004], lr: 0.154695, loss: 2.2824
2022-03-11 06:14:31 - train: epoch 0036, iter [04700, 05004], lr: 0.154695, loss: 2.2773
2022-03-11 06:15:12 - train: epoch 0036, iter [04800, 05004], lr: 0.154695, loss: 2.3653
2022-03-11 06:15:51 - train: epoch 0036, iter [04900, 05004], lr: 0.154695, loss: 2.2635
2022-03-11 06:16:30 - train: epoch 0036, iter [05000, 05004], lr: 0.154695, loss: 2.2937
2022-03-11 06:16:32 - train: epoch 036, train_loss: 2.3626
2022-03-11 06:17:57 - eval: epoch: 036, acc1: 50.934%, acc5: 76.658%, test_loss: 2.1054, per_image_load_time: 3.037ms, per_image_inference_time: 0.247ms
2022-03-11 06:17:57 - until epoch: 036, best_acc1: 50.934%
2022-03-11 06:17:57 - epoch 037 lr: 0.15189695737812153
2022-03-11 06:18:41 - train: epoch 0037, iter [00100, 05004], lr: 0.151897, loss: 2.1415
2022-03-11 06:19:22 - train: epoch 0037, iter [00200, 05004], lr: 0.151897, loss: 1.9648
2022-03-11 06:20:05 - train: epoch 0037, iter [00300, 05004], lr: 0.151897, loss: 2.2071
2022-03-11 06:20:47 - train: epoch 0037, iter [00400, 05004], lr: 0.151897, loss: 2.1444
2022-03-11 06:21:28 - train: epoch 0037, iter [00500, 05004], lr: 0.151897, loss: 2.3066
2022-03-11 06:22:08 - train: epoch 0037, iter [00600, 05004], lr: 0.151897, loss: 2.4529
2022-03-11 06:22:49 - train: epoch 0037, iter [00700, 05004], lr: 0.151897, loss: 2.4060
2022-03-11 06:23:30 - train: epoch 0037, iter [00800, 05004], lr: 0.151897, loss: 2.3909
2022-03-11 06:24:11 - train: epoch 0037, iter [00900, 05004], lr: 0.151897, loss: 2.4320
2022-03-11 06:24:52 - train: epoch 0037, iter [01000, 05004], lr: 0.151897, loss: 2.2380
2022-03-11 06:25:33 - train: epoch 0037, iter [01100, 05004], lr: 0.151897, loss: 2.3075
2022-03-11 06:26:13 - train: epoch 0037, iter [01200, 05004], lr: 0.151897, loss: 2.3940
2022-03-11 06:26:55 - train: epoch 0037, iter [01300, 05004], lr: 0.151897, loss: 2.5217
2022-03-11 06:27:36 - train: epoch 0037, iter [01400, 05004], lr: 0.151897, loss: 2.4954
2022-03-11 06:28:16 - train: epoch 0037, iter [01500, 05004], lr: 0.151897, loss: 2.3051
2022-03-11 06:28:57 - train: epoch 0037, iter [01600, 05004], lr: 0.151897, loss: 2.2709
2022-03-11 06:29:38 - train: epoch 0037, iter [01700, 05004], lr: 0.151897, loss: 2.6596
2022-03-11 06:30:19 - train: epoch 0037, iter [01800, 05004], lr: 0.151897, loss: 2.1749
2022-03-11 06:31:00 - train: epoch 0037, iter [01900, 05004], lr: 0.151897, loss: 2.3406
2022-03-11 06:31:40 - train: epoch 0037, iter [02000, 05004], lr: 0.151897, loss: 2.5055
2022-03-11 06:32:21 - train: epoch 0037, iter [02100, 05004], lr: 0.151897, loss: 2.4287
2022-03-11 06:33:01 - train: epoch 0037, iter [02200, 05004], lr: 0.151897, loss: 2.3031
2022-03-11 06:33:41 - train: epoch 0037, iter [02300, 05004], lr: 0.151897, loss: 2.1634
2022-03-11 06:34:19 - train: epoch 0037, iter [02400, 05004], lr: 0.151897, loss: 2.2854
2022-03-11 06:34:57 - train: epoch 0037, iter [02500, 05004], lr: 0.151897, loss: 2.4582
2022-03-11 06:35:37 - train: epoch 0037, iter [02600, 05004], lr: 0.151897, loss: 2.5201
2022-03-11 06:36:17 - train: epoch 0037, iter [02700, 05004], lr: 0.151897, loss: 2.6479
2022-03-11 06:36:55 - train: epoch 0037, iter [02800, 05004], lr: 0.151897, loss: 2.3775
2022-03-11 06:37:33 - train: epoch 0037, iter [02900, 05004], lr: 0.151897, loss: 2.3553
2022-03-11 06:38:12 - train: epoch 0037, iter [03000, 05004], lr: 0.151897, loss: 2.3554
2022-03-11 06:38:51 - train: epoch 0037, iter [03100, 05004], lr: 0.151897, loss: 2.2273
2022-03-11 06:39:30 - train: epoch 0037, iter [03200, 05004], lr: 0.151897, loss: 2.2814
2022-03-11 06:40:09 - train: epoch 0037, iter [03300, 05004], lr: 0.151897, loss: 2.2460
2022-03-11 06:40:48 - train: epoch 0037, iter [03400, 05004], lr: 0.151897, loss: 2.2689
2022-03-11 06:41:27 - train: epoch 0037, iter [03500, 05004], lr: 0.151897, loss: 2.4716
2022-03-11 06:42:05 - train: epoch 0037, iter [03600, 05004], lr: 0.151897, loss: 2.4753
2022-03-11 06:42:43 - train: epoch 0037, iter [03700, 05004], lr: 0.151897, loss: 2.4060
2022-03-11 06:43:22 - train: epoch 0037, iter [03800, 05004], lr: 0.151897, loss: 2.3195
2022-03-11 06:44:01 - train: epoch 0037, iter [03900, 05004], lr: 0.151897, loss: 2.3861
2022-03-11 06:44:39 - train: epoch 0037, iter [04000, 05004], lr: 0.151897, loss: 2.2478
2022-03-11 06:45:19 - train: epoch 0037, iter [04100, 05004], lr: 0.151897, loss: 2.2180
2022-03-11 06:45:59 - train: epoch 0037, iter [04200, 05004], lr: 0.151897, loss: 2.2771
2022-03-11 06:46:38 - train: epoch 0037, iter [04300, 05004], lr: 0.151897, loss: 2.2040
2022-03-11 06:47:17 - train: epoch 0037, iter [04400, 05004], lr: 0.151897, loss: 2.3750
2022-03-11 06:47:55 - train: epoch 0037, iter [04500, 05004], lr: 0.151897, loss: 2.2017
2022-03-11 06:48:34 - train: epoch 0037, iter [04600, 05004], lr: 0.151897, loss: 2.3531
2022-03-11 06:49:13 - train: epoch 0037, iter [04700, 05004], lr: 0.151897, loss: 2.1940
2022-03-11 06:49:50 - train: epoch 0037, iter [04800, 05004], lr: 0.151897, loss: 2.2958
2022-03-11 06:50:29 - train: epoch 0037, iter [04900, 05004], lr: 0.151897, loss: 2.5377
2022-03-11 06:51:08 - train: epoch 0037, iter [05000, 05004], lr: 0.151897, loss: 2.2973
2022-03-11 06:51:10 - train: epoch 037, train_loss: 2.3507
2022-03-11 06:52:31 - eval: epoch: 037, acc1: 49.284%, acc5: 75.782%, test_loss: 2.1731, per_image_load_time: 1.238ms, per_image_inference_time: 0.245ms
2022-03-11 06:52:32 - until epoch: 037, best_acc1: 50.934%
2022-03-11 06:52:32 - epoch 038 lr: 0.14904235038305083
2022-03-11 06:53:14 - train: epoch 0038, iter [00100, 05004], lr: 0.149042, loss: 2.1545
2022-03-11 06:53:53 - train: epoch 0038, iter [00200, 05004], lr: 0.149042, loss: 2.2725
2022-03-11 06:54:32 - train: epoch 0038, iter [00300, 05004], lr: 0.149042, loss: 2.0869
2022-03-11 06:55:12 - train: epoch 0038, iter [00400, 05004], lr: 0.149042, loss: 2.2068
2022-03-11 06:55:52 - train: epoch 0038, iter [00500, 05004], lr: 0.149042, loss: 2.2101
2022-03-11 06:56:30 - train: epoch 0038, iter [00600, 05004], lr: 0.149042, loss: 2.5019
2022-03-11 06:57:09 - train: epoch 0038, iter [00700, 05004], lr: 0.149042, loss: 2.2179
2022-03-11 06:57:48 - train: epoch 0038, iter [00800, 05004], lr: 0.149042, loss: 2.3507
2022-03-11 06:58:28 - train: epoch 0038, iter [00900, 05004], lr: 0.149042, loss: 2.2431
2022-03-11 06:59:07 - train: epoch 0038, iter [01000, 05004], lr: 0.149042, loss: 2.3991
2022-03-11 06:59:45 - train: epoch 0038, iter [01100, 05004], lr: 0.149042, loss: 2.5087
2022-03-11 07:00:24 - train: epoch 0038, iter [01200, 05004], lr: 0.149042, loss: 2.3024
2022-03-11 07:01:04 - train: epoch 0038, iter [01300, 05004], lr: 0.149042, loss: 2.4269
2022-03-11 07:01:43 - train: epoch 0038, iter [01400, 05004], lr: 0.149042, loss: 2.3229
2022-03-11 07:02:22 - train: epoch 0038, iter [01500, 05004], lr: 0.149042, loss: 2.3058
2022-03-11 07:03:01 - train: epoch 0038, iter [01600, 05004], lr: 0.149042, loss: 2.2990
2022-03-11 07:03:39 - train: epoch 0038, iter [01700, 05004], lr: 0.149042, loss: 2.5978
2022-03-11 07:04:17 - train: epoch 0038, iter [01800, 05004], lr: 0.149042, loss: 2.2970
2022-03-11 07:04:57 - train: epoch 0038, iter [01900, 05004], lr: 0.149042, loss: 2.3677
2022-03-11 07:05:36 - train: epoch 0038, iter [02000, 05004], lr: 0.149042, loss: 2.3481
2022-03-11 07:06:15 - train: epoch 0038, iter [02100, 05004], lr: 0.149042, loss: 2.2533
2022-03-11 07:06:55 - train: epoch 0038, iter [02200, 05004], lr: 0.149042, loss: 2.3121
2022-03-11 07:07:33 - train: epoch 0038, iter [02300, 05004], lr: 0.149042, loss: 2.5046
2022-03-11 07:08:12 - train: epoch 0038, iter [02400, 05004], lr: 0.149042, loss: 2.6846
2022-03-11 07:08:52 - train: epoch 0038, iter [02500, 05004], lr: 0.149042, loss: 2.1888
2022-03-11 07:09:31 - train: epoch 0038, iter [02600, 05004], lr: 0.149042, loss: 2.2684
2022-03-11 07:10:11 - train: epoch 0038, iter [02700, 05004], lr: 0.149042, loss: 2.1028
2022-03-11 07:10:50 - train: epoch 0038, iter [02800, 05004], lr: 0.149042, loss: 2.6567
2022-03-11 07:11:29 - train: epoch 0038, iter [02900, 05004], lr: 0.149042, loss: 2.4432
2022-03-11 07:12:06 - train: epoch 0038, iter [03000, 05004], lr: 0.149042, loss: 2.2025
2022-03-11 07:12:45 - train: epoch 0038, iter [03100, 05004], lr: 0.149042, loss: 2.4354
2022-03-11 07:13:24 - train: epoch 0038, iter [03200, 05004], lr: 0.149042, loss: 1.9952
2022-03-11 07:14:02 - train: epoch 0038, iter [03300, 05004], lr: 0.149042, loss: 2.0173
2022-03-11 07:14:41 - train: epoch 0038, iter [03400, 05004], lr: 0.149042, loss: 2.0833
2022-03-11 07:15:20 - train: epoch 0038, iter [03500, 05004], lr: 0.149042, loss: 2.1616
2022-03-11 07:15:58 - train: epoch 0038, iter [03600, 05004], lr: 0.149042, loss: 2.4599
2022-03-11 07:16:36 - train: epoch 0038, iter [03700, 05004], lr: 0.149042, loss: 2.0909
2022-03-11 07:17:15 - train: epoch 0038, iter [03800, 05004], lr: 0.149042, loss: 2.4018
2022-03-11 07:17:53 - train: epoch 0038, iter [03900, 05004], lr: 0.149042, loss: 2.1650
2022-03-11 07:18:32 - train: epoch 0038, iter [04000, 05004], lr: 0.149042, loss: 2.5282
2022-03-11 07:19:12 - train: epoch 0038, iter [04100, 05004], lr: 0.149042, loss: 2.2429
2022-03-11 07:19:52 - train: epoch 0038, iter [04200, 05004], lr: 0.149042, loss: 1.9878
2022-03-11 07:20:32 - train: epoch 0038, iter [04300, 05004], lr: 0.149042, loss: 2.4503
2022-03-11 07:21:11 - train: epoch 0038, iter [04400, 05004], lr: 0.149042, loss: 2.4096
2022-03-11 07:21:50 - train: epoch 0038, iter [04500, 05004], lr: 0.149042, loss: 2.5923
2022-03-11 07:22:28 - train: epoch 0038, iter [04600, 05004], lr: 0.149042, loss: 2.5203
2022-03-11 07:23:06 - train: epoch 0038, iter [04700, 05004], lr: 0.149042, loss: 2.3160
2022-03-11 07:23:45 - train: epoch 0038, iter [04800, 05004], lr: 0.149042, loss: 2.2953
2022-03-11 07:24:23 - train: epoch 0038, iter [04900, 05004], lr: 0.149042, loss: 2.2580
2022-03-11 07:25:02 - train: epoch 0038, iter [05000, 05004], lr: 0.149042, loss: 2.3105
2022-03-11 07:25:04 - train: epoch 038, train_loss: 2.3409
2022-03-11 07:26:26 - eval: epoch: 038, acc1: 51.616%, acc5: 77.062%, test_loss: 2.0790, per_image_load_time: 2.149ms, per_image_inference_time: 0.252ms
2022-03-11 07:26:26 - until epoch: 038, best_acc1: 51.616%
2022-03-11 07:26:26 - epoch 039 lr: 0.1461341162978688
2022-03-11 07:27:09 - train: epoch 0039, iter [00100, 05004], lr: 0.146134, loss: 2.1742
2022-03-11 07:27:48 - train: epoch 0039, iter [00200, 05004], lr: 0.146134, loss: 2.4694
2022-03-11 07:28:28 - train: epoch 0039, iter [00300, 05004], lr: 0.146134, loss: 2.1438
2022-03-11 07:29:07 - train: epoch 0039, iter [00400, 05004], lr: 0.146134, loss: 2.2964
2022-03-11 07:29:48 - train: epoch 0039, iter [00500, 05004], lr: 0.146134, loss: 2.3050
2022-03-11 07:30:28 - train: epoch 0039, iter [00600, 05004], lr: 0.146134, loss: 2.2604
2022-03-11 07:31:08 - train: epoch 0039, iter [00700, 05004], lr: 0.146134, loss: 2.4670
2022-03-11 07:31:48 - train: epoch 0039, iter [00800, 05004], lr: 0.146134, loss: 2.3711
2022-03-11 07:32:27 - train: epoch 0039, iter [00900, 05004], lr: 0.146134, loss: 2.5071
2022-03-11 07:33:07 - train: epoch 0039, iter [01000, 05004], lr: 0.146134, loss: 2.3434
2022-03-11 07:33:47 - train: epoch 0039, iter [01100, 05004], lr: 0.146134, loss: 2.4512
2022-03-11 07:34:27 - train: epoch 0039, iter [01200, 05004], lr: 0.146134, loss: 2.3356
2022-03-11 07:35:06 - train: epoch 0039, iter [01300, 05004], lr: 0.146134, loss: 2.2049
2022-03-11 07:35:47 - train: epoch 0039, iter [01400, 05004], lr: 0.146134, loss: 2.5191
2022-03-11 07:36:27 - train: epoch 0039, iter [01500, 05004], lr: 0.146134, loss: 2.0532
2022-03-11 07:37:07 - train: epoch 0039, iter [01600, 05004], lr: 0.146134, loss: 2.3654
2022-03-11 07:37:47 - train: epoch 0039, iter [01700, 05004], lr: 0.146134, loss: 2.3488
2022-03-11 07:38:27 - train: epoch 0039, iter [01800, 05004], lr: 0.146134, loss: 2.2545
2022-03-11 07:39:08 - train: epoch 0039, iter [01900, 05004], lr: 0.146134, loss: 2.0629
2022-03-11 07:39:46 - train: epoch 0039, iter [02000, 05004], lr: 0.146134, loss: 2.3737
2022-03-11 07:40:26 - train: epoch 0039, iter [02100, 05004], lr: 0.146134, loss: 2.5268
2022-03-11 07:41:06 - train: epoch 0039, iter [02200, 05004], lr: 0.146134, loss: 2.3409
2022-03-11 07:41:45 - train: epoch 0039, iter [02300, 05004], lr: 0.146134, loss: 2.5310
2022-03-11 07:42:25 - train: epoch 0039, iter [02400, 05004], lr: 0.146134, loss: 2.4394
2022-03-11 07:43:05 - train: epoch 0039, iter [02500, 05004], lr: 0.146134, loss: 2.3333
2022-03-11 07:43:45 - train: epoch 0039, iter [02600, 05004], lr: 0.146134, loss: 2.1797
2022-03-11 07:44:26 - train: epoch 0039, iter [02700, 05004], lr: 0.146134, loss: 2.3421
2022-03-11 07:45:06 - train: epoch 0039, iter [02800, 05004], lr: 0.146134, loss: 2.3142
2022-03-11 07:45:46 - train: epoch 0039, iter [02900, 05004], lr: 0.146134, loss: 2.3604
2022-03-11 07:46:25 - train: epoch 0039, iter [03000, 05004], lr: 0.146134, loss: 2.2611
2022-03-11 07:47:04 - train: epoch 0039, iter [03100, 05004], lr: 0.146134, loss: 2.2229
2022-03-11 07:47:43 - train: epoch 0039, iter [03200, 05004], lr: 0.146134, loss: 2.5009
2022-03-11 07:48:23 - train: epoch 0039, iter [03300, 05004], lr: 0.146134, loss: 2.4943
2022-03-11 07:49:03 - train: epoch 0039, iter [03400, 05004], lr: 0.146134, loss: 2.2779
2022-03-11 07:49:42 - train: epoch 0039, iter [03500, 05004], lr: 0.146134, loss: 2.5766
2022-03-11 07:50:22 - train: epoch 0039, iter [03600, 05004], lr: 0.146134, loss: 2.4040
2022-03-11 07:51:01 - train: epoch 0039, iter [03700, 05004], lr: 0.146134, loss: 2.0841
2022-03-11 07:51:41 - train: epoch 0039, iter [03800, 05004], lr: 0.146134, loss: 2.1445
2022-03-11 07:52:22 - train: epoch 0039, iter [03900, 05004], lr: 0.146134, loss: 2.4378
2022-03-11 07:53:02 - train: epoch 0039, iter [04000, 05004], lr: 0.146134, loss: 2.2082
2022-03-11 07:53:41 - train: epoch 0039, iter [04100, 05004], lr: 0.146134, loss: 2.5652
2022-03-11 07:54:20 - train: epoch 0039, iter [04200, 05004], lr: 0.146134, loss: 2.3305
2022-03-11 07:55:00 - train: epoch 0039, iter [04300, 05004], lr: 0.146134, loss: 2.3389
2022-03-11 07:55:38 - train: epoch 0039, iter [04400, 05004], lr: 0.146134, loss: 2.3129
2022-03-11 07:56:18 - train: epoch 0039, iter [04500, 05004], lr: 0.146134, loss: 2.1927
2022-03-11 07:56:58 - train: epoch 0039, iter [04600, 05004], lr: 0.146134, loss: 2.5364
2022-03-11 07:57:36 - train: epoch 0039, iter [04700, 05004], lr: 0.146134, loss: 2.2206
2022-03-11 07:58:16 - train: epoch 0039, iter [04800, 05004], lr: 0.146134, loss: 2.3851
2022-03-11 07:58:55 - train: epoch 0039, iter [04900, 05004], lr: 0.146134, loss: 2.3359
2022-03-11 07:59:34 - train: epoch 0039, iter [05000, 05004], lr: 0.146134, loss: 2.2772
2022-03-11 07:59:35 - train: epoch 039, train_loss: 2.3294
2022-03-11 08:01:01 - eval: epoch: 039, acc1: 51.000%, acc5: 76.888%, test_loss: 2.1039, per_image_load_time: 3.095ms, per_image_inference_time: 0.250ms
2022-03-11 08:01:01 - until epoch: 039, best_acc1: 51.616%
2022-03-11 08:01:01 - epoch 040 lr: 0.1431754352338493
2022-03-11 08:01:45 - train: epoch 0040, iter [00100, 05004], lr: 0.143175, loss: 2.6007
2022-03-11 08:02:25 - train: epoch 0040, iter [00200, 05004], lr: 0.143175, loss: 2.2807
2022-03-11 08:03:07 - train: epoch 0040, iter [00300, 05004], lr: 0.143175, loss: 2.4293
2022-03-11 08:03:46 - train: epoch 0040, iter [00400, 05004], lr: 0.143175, loss: 2.1516
2022-03-11 08:04:27 - train: epoch 0040, iter [00500, 05004], lr: 0.143175, loss: 2.1691
2022-03-11 08:05:07 - train: epoch 0040, iter [00600, 05004], lr: 0.143175, loss: 2.2927
2022-03-11 08:05:48 - train: epoch 0040, iter [00700, 05004], lr: 0.143175, loss: 2.3903
2022-03-11 08:06:27 - train: epoch 0040, iter [00800, 05004], lr: 0.143175, loss: 2.5487
2022-03-11 08:07:08 - train: epoch 0040, iter [00900, 05004], lr: 0.143175, loss: 2.2688
2022-03-11 08:07:49 - train: epoch 0040, iter [01000, 05004], lr: 0.143175, loss: 2.0425
2022-03-11 08:08:28 - train: epoch 0040, iter [01100, 05004], lr: 0.143175, loss: 2.2743
2022-03-11 08:09:08 - train: epoch 0040, iter [01200, 05004], lr: 0.143175, loss: 2.4179
2022-03-11 08:09:49 - train: epoch 0040, iter [01300, 05004], lr: 0.143175, loss: 2.3143
2022-03-11 08:10:28 - train: epoch 0040, iter [01400, 05004], lr: 0.143175, loss: 2.3803
2022-03-11 08:11:07 - train: epoch 0040, iter [01500, 05004], lr: 0.143175, loss: 2.3152
2022-03-11 08:11:47 - train: epoch 0040, iter [01600, 05004], lr: 0.143175, loss: 2.2558
2022-03-11 08:12:26 - train: epoch 0040, iter [01700, 05004], lr: 0.143175, loss: 2.2641
2022-03-11 08:13:05 - train: epoch 0040, iter [01800, 05004], lr: 0.143175, loss: 2.2359
2022-03-11 08:13:43 - train: epoch 0040, iter [01900, 05004], lr: 0.143175, loss: 2.1118
2022-03-11 08:14:22 - train: epoch 0040, iter [02000, 05004], lr: 0.143175, loss: 2.3764
2022-03-11 08:15:02 - train: epoch 0040, iter [02100, 05004], lr: 0.143175, loss: 2.2152
2022-03-11 08:15:40 - train: epoch 0040, iter [02200, 05004], lr: 0.143175, loss: 2.2403
2022-03-11 08:16:19 - train: epoch 0040, iter [02300, 05004], lr: 0.143175, loss: 2.1153
2022-03-11 08:16:59 - train: epoch 0040, iter [02400, 05004], lr: 0.143175, loss: 2.4355
2022-03-11 08:17:37 - train: epoch 0040, iter [02500, 05004], lr: 0.143175, loss: 2.4454
2022-03-11 08:18:16 - train: epoch 0040, iter [02600, 05004], lr: 0.143175, loss: 2.2235
2022-03-11 08:18:55 - train: epoch 0040, iter [02700, 05004], lr: 0.143175, loss: 2.5568
2022-03-11 08:19:34 - train: epoch 0040, iter [02800, 05004], lr: 0.143175, loss: 2.3216
2022-03-11 08:20:12 - train: epoch 0040, iter [02900, 05004], lr: 0.143175, loss: 2.7829
2022-03-11 08:20:51 - train: epoch 0040, iter [03000, 05004], lr: 0.143175, loss: 2.3710
2022-03-11 08:21:31 - train: epoch 0040, iter [03100, 05004], lr: 0.143175, loss: 2.3444
2022-03-11 08:22:09 - train: epoch 0040, iter [03200, 05004], lr: 0.143175, loss: 2.4334
2022-03-11 08:22:49 - train: epoch 0040, iter [03300, 05004], lr: 0.143175, loss: 2.3816
2022-03-11 08:23:28 - train: epoch 0040, iter [03400, 05004], lr: 0.143175, loss: 2.3108
2022-03-11 08:24:08 - train: epoch 0040, iter [03500, 05004], lr: 0.143175, loss: 2.3900
2022-03-11 08:24:48 - train: epoch 0040, iter [03600, 05004], lr: 0.143175, loss: 2.3135
2022-03-11 08:25:27 - train: epoch 0040, iter [03700, 05004], lr: 0.143175, loss: 2.1275
2022-03-11 08:26:07 - train: epoch 0040, iter [03800, 05004], lr: 0.143175, loss: 2.1783
2022-03-11 08:26:47 - train: epoch 0040, iter [03900, 05004], lr: 0.143175, loss: 2.6012
2022-03-11 08:27:29 - train: epoch 0040, iter [04000, 05004], lr: 0.143175, loss: 2.3825
2022-03-11 08:28:09 - train: epoch 0040, iter [04100, 05004], lr: 0.143175, loss: 2.3387
2022-03-11 08:28:48 - train: epoch 0040, iter [04200, 05004], lr: 0.143175, loss: 2.2524
2022-03-11 08:29:27 - train: epoch 0040, iter [04300, 05004], lr: 0.143175, loss: 2.4497
2022-03-11 08:30:07 - train: epoch 0040, iter [04400, 05004], lr: 0.143175, loss: 2.2409
2022-03-11 08:30:49 - train: epoch 0040, iter [04500, 05004], lr: 0.143175, loss: 2.0084
2022-03-11 08:31:27 - train: epoch 0040, iter [04600, 05004], lr: 0.143175, loss: 2.3154
2022-03-11 08:32:04 - train: epoch 0040, iter [04700, 05004], lr: 0.143175, loss: 2.4008
2022-03-11 08:32:42 - train: epoch 0040, iter [04800, 05004], lr: 0.143175, loss: 2.3710
2022-03-11 08:33:19 - train: epoch 0040, iter [04900, 05004], lr: 0.143175, loss: 2.2935
2022-03-11 08:33:56 - train: epoch 0040, iter [05000, 05004], lr: 0.143175, loss: 2.0924
2022-03-11 08:33:58 - train: epoch 040, train_loss: 2.3155
2022-03-11 08:35:19 - eval: epoch: 040, acc1: 50.972%, acc5: 76.754%, test_loss: 2.0977, per_image_load_time: 2.938ms, per_image_inference_time: 0.238ms
2022-03-11 08:35:19 - until epoch: 040, best_acc1: 51.616%
2022-03-11 08:35:19 - epoch 041 lr: 0.14016954246529698
2022-03-11 08:36:00 - train: epoch 0041, iter [00100, 05004], lr: 0.140170, loss: 2.4956
2022-03-11 08:36:39 - train: epoch 0041, iter [00200, 05004], lr: 0.140170, loss: 2.7503
2022-03-11 08:37:17 - train: epoch 0041, iter [00300, 05004], lr: 0.140170, loss: 2.3764
2022-03-11 08:37:55 - train: epoch 0041, iter [00400, 05004], lr: 0.140170, loss: 2.4168
2022-03-11 08:38:33 - train: epoch 0041, iter [00500, 05004], lr: 0.140170, loss: 1.9811
2022-03-11 08:39:11 - train: epoch 0041, iter [00600, 05004], lr: 0.140170, loss: 2.4544
2022-03-11 08:39:49 - train: epoch 0041, iter [00700, 05004], lr: 0.140170, loss: 2.3371
2022-03-11 08:40:26 - train: epoch 0041, iter [00800, 05004], lr: 0.140170, loss: 2.2978
2022-03-11 08:41:04 - train: epoch 0041, iter [00900, 05004], lr: 0.140170, loss: 1.9964
2022-03-11 08:41:43 - train: epoch 0041, iter [01000, 05004], lr: 0.140170, loss: 2.3830
2022-03-11 08:42:24 - train: epoch 0041, iter [01100, 05004], lr: 0.140170, loss: 2.2794
2022-03-11 08:43:04 - train: epoch 0041, iter [01200, 05004], lr: 0.140170, loss: 2.2362
2022-03-11 08:43:45 - train: epoch 0041, iter [01300, 05004], lr: 0.140170, loss: 2.0707
2022-03-11 08:44:26 - train: epoch 0041, iter [01400, 05004], lr: 0.140170, loss: 2.2764
2022-03-11 08:45:07 - train: epoch 0041, iter [01500, 05004], lr: 0.140170, loss: 2.7467
2022-03-11 08:45:48 - train: epoch 0041, iter [01600, 05004], lr: 0.140170, loss: 2.2410
2022-03-11 08:46:29 - train: epoch 0041, iter [01700, 05004], lr: 0.140170, loss: 2.3376
2022-03-11 08:47:07 - train: epoch 0041, iter [01800, 05004], lr: 0.140170, loss: 2.0351
2022-03-11 08:47:45 - train: epoch 0041, iter [01900, 05004], lr: 0.140170, loss: 2.4693
2022-03-11 08:48:22 - train: epoch 0041, iter [02000, 05004], lr: 0.140170, loss: 2.3463
2022-03-11 08:49:00 - train: epoch 0041, iter [02100, 05004], lr: 0.140170, loss: 2.1375
2022-03-11 08:49:39 - train: epoch 0041, iter [02200, 05004], lr: 0.140170, loss: 2.1402
2022-03-11 08:50:16 - train: epoch 0041, iter [02300, 05004], lr: 0.140170, loss: 2.2817
2022-03-11 08:50:53 - train: epoch 0041, iter [02400, 05004], lr: 0.140170, loss: 2.3249
2022-03-11 08:51:32 - train: epoch 0041, iter [02500, 05004], lr: 0.140170, loss: 2.2660
2022-03-11 08:52:09 - train: epoch 0041, iter [02600, 05004], lr: 0.140170, loss: 2.6087
2022-03-11 08:52:48 - train: epoch 0041, iter [02700, 05004], lr: 0.140170, loss: 2.3811
2022-03-11 08:53:25 - train: epoch 0041, iter [02800, 05004], lr: 0.140170, loss: 2.3596
2022-03-11 08:54:03 - train: epoch 0041, iter [02900, 05004], lr: 0.140170, loss: 2.3617
2022-03-11 08:54:42 - train: epoch 0041, iter [03000, 05004], lr: 0.140170, loss: 2.3938
2022-03-11 08:55:20 - train: epoch 0041, iter [03100, 05004], lr: 0.140170, loss: 2.4757
2022-03-11 08:55:59 - train: epoch 0041, iter [03200, 05004], lr: 0.140170, loss: 2.3098
2022-03-11 08:56:39 - train: epoch 0041, iter [03300, 05004], lr: 0.140170, loss: 2.2368
2022-03-11 08:57:18 - train: epoch 0041, iter [03400, 05004], lr: 0.140170, loss: 2.2769
2022-03-11 08:57:56 - train: epoch 0041, iter [03500, 05004], lr: 0.140170, loss: 2.3796
2022-03-11 08:58:33 - train: epoch 0041, iter [03600, 05004], lr: 0.140170, loss: 2.4783
2022-03-11 08:59:11 - train: epoch 0041, iter [03700, 05004], lr: 0.140170, loss: 2.3459
2022-03-11 08:59:48 - train: epoch 0041, iter [03800, 05004], lr: 0.140170, loss: 2.3081
2022-03-11 09:00:27 - train: epoch 0041, iter [03900, 05004], lr: 0.140170, loss: 2.2377
2022-03-11 09:01:05 - train: epoch 0041, iter [04000, 05004], lr: 0.140170, loss: 2.1580
2022-03-11 09:01:43 - train: epoch 0041, iter [04100, 05004], lr: 0.140170, loss: 2.4722
2022-03-11 09:02:20 - train: epoch 0041, iter [04200, 05004], lr: 0.140170, loss: 2.2795
2022-03-11 09:02:58 - train: epoch 0041, iter [04300, 05004], lr: 0.140170, loss: 2.2300
2022-03-11 09:03:36 - train: epoch 0041, iter [04400, 05004], lr: 0.140170, loss: 2.1266
2022-03-11 09:04:13 - train: epoch 0041, iter [04500, 05004], lr: 0.140170, loss: 2.2868
2022-03-11 09:04:51 - train: epoch 0041, iter [04600, 05004], lr: 0.140170, loss: 2.3488
2022-03-11 09:05:28 - train: epoch 0041, iter [04700, 05004], lr: 0.140170, loss: 2.1901
2022-03-11 09:06:05 - train: epoch 0041, iter [04800, 05004], lr: 0.140170, loss: 2.3397
2022-03-11 09:06:42 - train: epoch 0041, iter [04900, 05004], lr: 0.140170, loss: 2.6347
2022-03-11 09:07:19 - train: epoch 0041, iter [05000, 05004], lr: 0.140170, loss: 2.3229
2022-03-11 09:07:21 - train: epoch 041, train_loss: 2.3073
2022-03-11 09:08:41 - eval: epoch: 041, acc1: 50.268%, acc5: 75.772%, test_loss: 2.1524, per_image_load_time: 2.917ms, per_image_inference_time: 0.245ms
2022-03-11 09:08:41 - until epoch: 041, best_acc1: 51.616%
2022-03-11 09:08:41 - epoch 042 lr: 0.1371197248918221
2022-03-11 09:09:23 - train: epoch 0042, iter [00100, 05004], lr: 0.137120, loss: 2.1125
2022-03-11 09:10:02 - train: epoch 0042, iter [00200, 05004], lr: 0.137120, loss: 2.0917
2022-03-11 09:10:41 - train: epoch 0042, iter [00300, 05004], lr: 0.137120, loss: 2.2741
2022-03-11 09:11:18 - train: epoch 0042, iter [00400, 05004], lr: 0.137120, loss: 2.1082
2022-03-11 09:11:56 - train: epoch 0042, iter [00500, 05004], lr: 0.137120, loss: 2.4968
2022-03-11 09:12:33 - train: epoch 0042, iter [00600, 05004], lr: 0.137120, loss: 2.1143
2022-03-11 09:13:10 - train: epoch 0042, iter [00700, 05004], lr: 0.137120, loss: 2.5456
2022-03-11 09:13:50 - train: epoch 0042, iter [00800, 05004], lr: 0.137120, loss: 2.3972
2022-03-11 09:14:27 - train: epoch 0042, iter [00900, 05004], lr: 0.137120, loss: 2.3725
2022-03-11 09:15:06 - train: epoch 0042, iter [01000, 05004], lr: 0.137120, loss: 2.5761
2022-03-11 09:15:44 - train: epoch 0042, iter [01100, 05004], lr: 0.137120, loss: 2.1662
2022-03-11 09:16:22 - train: epoch 0042, iter [01200, 05004], lr: 0.137120, loss: 2.2265
2022-03-11 09:17:01 - train: epoch 0042, iter [01300, 05004], lr: 0.137120, loss: 2.4415
2022-03-11 09:17:39 - train: epoch 0042, iter [01400, 05004], lr: 0.137120, loss: 2.4143
2022-03-11 09:18:18 - train: epoch 0042, iter [01500, 05004], lr: 0.137120, loss: 2.1484
2022-03-11 09:18:58 - train: epoch 0042, iter [01600, 05004], lr: 0.137120, loss: 2.5381
2022-03-11 09:19:37 - train: epoch 0042, iter [01700, 05004], lr: 0.137120, loss: 2.1076
2022-03-11 09:20:16 - train: epoch 0042, iter [01800, 05004], lr: 0.137120, loss: 2.1726
2022-03-11 09:20:54 - train: epoch 0042, iter [01900, 05004], lr: 0.137120, loss: 2.4845
2022-03-11 09:21:33 - train: epoch 0042, iter [02000, 05004], lr: 0.137120, loss: 2.3189
2022-03-11 09:22:11 - train: epoch 0042, iter [02100, 05004], lr: 0.137120, loss: 1.9154
2022-03-11 09:22:50 - train: epoch 0042, iter [02200, 05004], lr: 0.137120, loss: 2.0871
2022-03-11 09:23:27 - train: epoch 0042, iter [02300, 05004], lr: 0.137120, loss: 2.2864
2022-03-11 09:24:05 - train: epoch 0042, iter [02400, 05004], lr: 0.137120, loss: 2.2741
2022-03-11 09:24:42 - train: epoch 0042, iter [02500, 05004], lr: 0.137120, loss: 2.5661
2022-03-11 09:25:19 - train: epoch 0042, iter [02600, 05004], lr: 0.137120, loss: 2.4213
2022-03-11 09:25:55 - train: epoch 0042, iter [02700, 05004], lr: 0.137120, loss: 2.1727
2022-03-11 09:26:33 - train: epoch 0042, iter [02800, 05004], lr: 0.137120, loss: 2.1033
2022-03-11 09:27:12 - train: epoch 0042, iter [02900, 05004], lr: 0.137120, loss: 2.1116
2022-03-11 09:27:51 - train: epoch 0042, iter [03000, 05004], lr: 0.137120, loss: 2.3399
2022-03-11 09:28:28 - train: epoch 0042, iter [03100, 05004], lr: 0.137120, loss: 2.3259
2022-03-11 09:29:07 - train: epoch 0042, iter [03200, 05004], lr: 0.137120, loss: 2.4481
2022-03-11 09:29:46 - train: epoch 0042, iter [03300, 05004], lr: 0.137120, loss: 2.3799
2022-03-11 09:30:24 - train: epoch 0042, iter [03400, 05004], lr: 0.137120, loss: 2.1465
2022-03-11 09:31:02 - train: epoch 0042, iter [03500, 05004], lr: 0.137120, loss: 2.1674
2022-03-11 09:31:41 - train: epoch 0042, iter [03600, 05004], lr: 0.137120, loss: 2.4934
2022-03-11 09:32:18 - train: epoch 0042, iter [03700, 05004], lr: 0.137120, loss: 2.3877
2022-03-11 09:32:58 - train: epoch 0042, iter [03800, 05004], lr: 0.137120, loss: 2.0607
2022-03-11 09:33:37 - train: epoch 0042, iter [03900, 05004], lr: 0.137120, loss: 2.2151
2022-03-11 09:34:15 - train: epoch 0042, iter [04000, 05004], lr: 0.137120, loss: 2.1159
2022-03-11 09:34:54 - train: epoch 0042, iter [04100, 05004], lr: 0.137120, loss: 2.3085
2022-03-11 09:35:33 - train: epoch 0042, iter [04200, 05004], lr: 0.137120, loss: 2.5023
2022-03-11 09:36:12 - train: epoch 0042, iter [04300, 05004], lr: 0.137120, loss: 2.0598
2022-03-11 09:36:49 - train: epoch 0042, iter [04400, 05004], lr: 0.137120, loss: 2.1380
2022-03-11 09:37:28 - train: epoch 0042, iter [04500, 05004], lr: 0.137120, loss: 2.2314
2022-03-11 09:38:08 - train: epoch 0042, iter [04600, 05004], lr: 0.137120, loss: 2.3528
2022-03-11 09:38:46 - train: epoch 0042, iter [04700, 05004], lr: 0.137120, loss: 2.3714
2022-03-11 09:39:24 - train: epoch 0042, iter [04800, 05004], lr: 0.137120, loss: 2.2761
2022-03-11 09:40:04 - train: epoch 0042, iter [04900, 05004], lr: 0.137120, loss: 2.1194
2022-03-11 09:40:42 - train: epoch 0042, iter [05000, 05004], lr: 0.137120, loss: 2.3168
2022-03-11 09:40:43 - train: epoch 042, train_loss: 2.2934
2022-03-11 09:42:04 - eval: epoch: 042, acc1: 52.212%, acc5: 77.562%, test_loss: 2.0384, per_image_load_time: 2.932ms, per_image_inference_time: 0.251ms
2022-03-11 09:42:04 - until epoch: 042, best_acc1: 52.212%
2022-03-11 09:42:04 - epoch 043 lr: 0.13402931744416433
2022-03-11 09:42:45 - train: epoch 0043, iter [00100, 05004], lr: 0.134029, loss: 2.4027
2022-03-11 09:43:25 - train: epoch 0043, iter [00200, 05004], lr: 0.134029, loss: 2.4758
2022-03-11 09:44:07 - train: epoch 0043, iter [00300, 05004], lr: 0.134029, loss: 2.0650
2022-03-11 09:44:45 - train: epoch 0043, iter [00400, 05004], lr: 0.134029, loss: 2.1228
2022-03-11 09:45:24 - train: epoch 0043, iter [00500, 05004], lr: 0.134029, loss: 2.3171
2022-03-11 09:46:02 - train: epoch 0043, iter [00600, 05004], lr: 0.134029, loss: 2.2103
2022-03-11 09:46:41 - train: epoch 0043, iter [00700, 05004], lr: 0.134029, loss: 2.4662
2022-03-11 09:47:20 - train: epoch 0043, iter [00800, 05004], lr: 0.134029, loss: 2.2786
2022-03-11 09:47:58 - train: epoch 0043, iter [00900, 05004], lr: 0.134029, loss: 2.3571
2022-03-11 09:48:37 - train: epoch 0043, iter [01000, 05004], lr: 0.134029, loss: 2.5777
2022-03-11 09:49:14 - train: epoch 0043, iter [01100, 05004], lr: 0.134029, loss: 2.1785
2022-03-11 09:49:54 - train: epoch 0043, iter [01200, 05004], lr: 0.134029, loss: 1.9731
2022-03-11 09:50:34 - train: epoch 0043, iter [01300, 05004], lr: 0.134029, loss: 2.5115
2022-03-11 09:51:16 - train: epoch 0043, iter [01400, 05004], lr: 0.134029, loss: 2.2403
2022-03-11 09:51:56 - train: epoch 0043, iter [01500, 05004], lr: 0.134029, loss: 2.1164
2022-03-11 09:52:35 - train: epoch 0043, iter [01600, 05004], lr: 0.134029, loss: 2.2303
2022-03-11 09:53:13 - train: epoch 0043, iter [01700, 05004], lr: 0.134029, loss: 2.4735
2022-03-11 09:53:51 - train: epoch 0043, iter [01800, 05004], lr: 0.134029, loss: 2.5607
2022-03-11 09:54:28 - train: epoch 0043, iter [01900, 05004], lr: 0.134029, loss: 2.4386
2022-03-11 09:55:06 - train: epoch 0043, iter [02000, 05004], lr: 0.134029, loss: 2.1479
2022-03-11 09:55:44 - train: epoch 0043, iter [02100, 05004], lr: 0.134029, loss: 2.1510
2022-03-11 09:56:22 - train: epoch 0043, iter [02200, 05004], lr: 0.134029, loss: 2.6328
2022-03-11 09:56:59 - train: epoch 0043, iter [02300, 05004], lr: 0.134029, loss: 2.3392
2022-03-11 09:57:37 - train: epoch 0043, iter [02400, 05004], lr: 0.134029, loss: 2.1422
2022-03-11 09:58:17 - train: epoch 0043, iter [02500, 05004], lr: 0.134029, loss: 2.3846
2022-03-11 09:58:55 - train: epoch 0043, iter [02600, 05004], lr: 0.134029, loss: 1.9456
2022-03-11 09:59:35 - train: epoch 0043, iter [02700, 05004], lr: 0.134029, loss: 2.4307
2022-03-11 10:00:15 - train: epoch 0043, iter [02800, 05004], lr: 0.134029, loss: 2.0970
2022-03-11 10:00:53 - train: epoch 0043, iter [02900, 05004], lr: 0.134029, loss: 2.3539
2022-03-11 10:01:32 - train: epoch 0043, iter [03000, 05004], lr: 0.134029, loss: 2.5520
2022-03-11 10:02:11 - train: epoch 0043, iter [03100, 05004], lr: 0.134029, loss: 2.4193
2022-03-11 10:02:49 - train: epoch 0043, iter [03200, 05004], lr: 0.134029, loss: 2.4041
2022-03-11 10:03:26 - train: epoch 0043, iter [03300, 05004], lr: 0.134029, loss: 2.5086
2022-03-11 10:04:04 - train: epoch 0043, iter [03400, 05004], lr: 0.134029, loss: 2.3381
2022-03-11 10:04:42 - train: epoch 0043, iter [03500, 05004], lr: 0.134029, loss: 2.3053
2022-03-11 10:05:23 - train: epoch 0043, iter [03600, 05004], lr: 0.134029, loss: 2.4242
2022-03-11 10:06:02 - train: epoch 0043, iter [03700, 05004], lr: 0.134029, loss: 2.2471
2022-03-11 10:06:42 - train: epoch 0043, iter [03800, 05004], lr: 0.134029, loss: 2.3784
2022-03-11 10:07:21 - train: epoch 0043, iter [03900, 05004], lr: 0.134029, loss: 2.1449
2022-03-11 10:08:00 - train: epoch 0043, iter [04000, 05004], lr: 0.134029, loss: 2.2897
2022-03-11 10:08:39 - train: epoch 0043, iter [04100, 05004], lr: 0.134029, loss: 2.3839
2022-03-11 10:09:21 - train: epoch 0043, iter [04200, 05004], lr: 0.134029, loss: 2.2984
2022-03-11 10:10:02 - train: epoch 0043, iter [04300, 05004], lr: 0.134029, loss: 2.3166
2022-03-11 10:10:42 - train: epoch 0043, iter [04400, 05004], lr: 0.134029, loss: 2.4607
2022-03-11 10:11:22 - train: epoch 0043, iter [04500, 05004], lr: 0.134029, loss: 2.2100
2022-03-11 10:12:03 - train: epoch 0043, iter [04600, 05004], lr: 0.134029, loss: 2.3125
2022-03-11 10:12:43 - train: epoch 0043, iter [04700, 05004], lr: 0.134029, loss: 2.4295
2022-03-11 10:13:23 - train: epoch 0043, iter [04800, 05004], lr: 0.134029, loss: 2.3852
2022-03-11 10:14:04 - train: epoch 0043, iter [04900, 05004], lr: 0.134029, loss: 2.1763
2022-03-11 10:14:44 - train: epoch 0043, iter [05000, 05004], lr: 0.134029, loss: 2.1892
2022-03-11 10:14:46 - train: epoch 043, train_loss: 2.2826
2022-03-11 10:16:10 - eval: epoch: 043, acc1: 51.732%, acc5: 77.454%, test_loss: 2.0601, per_image_load_time: 2.807ms, per_image_inference_time: 0.266ms
2022-03-11 10:16:10 - until epoch: 043, best_acc1: 52.212%
2022-03-11 10:16:10 - epoch 044 lr: 0.13090169943749475
2022-03-11 10:16:54 - train: epoch 0044, iter [00100, 05004], lr: 0.130902, loss: 2.3499
2022-03-11 10:17:35 - train: epoch 0044, iter [00200, 05004], lr: 0.130902, loss: 2.4554
2022-03-11 10:18:15 - train: epoch 0044, iter [00300, 05004], lr: 0.130902, loss: 2.2342
2022-03-11 10:18:57 - train: epoch 0044, iter [00400, 05004], lr: 0.130902, loss: 2.0655
2022-03-11 10:19:37 - train: epoch 0044, iter [00500, 05004], lr: 0.130902, loss: 2.4051
2022-03-11 10:20:17 - train: epoch 0044, iter [00600, 05004], lr: 0.130902, loss: 1.9289
2022-03-11 10:20:58 - train: epoch 0044, iter [00700, 05004], lr: 0.130902, loss: 2.0492
2022-03-11 10:21:38 - train: epoch 0044, iter [00800, 05004], lr: 0.130902, loss: 2.2351
2022-03-11 10:22:18 - train: epoch 0044, iter [00900, 05004], lr: 0.130902, loss: 2.1886
2022-03-11 10:22:58 - train: epoch 0044, iter [01000, 05004], lr: 0.130902, loss: 2.3156
2022-03-11 10:23:39 - train: epoch 0044, iter [01100, 05004], lr: 0.130902, loss: 1.8253
2022-03-11 10:24:20 - train: epoch 0044, iter [01200, 05004], lr: 0.130902, loss: 2.1686
2022-03-11 10:25:00 - train: epoch 0044, iter [01300, 05004], lr: 0.130902, loss: 2.1570
2022-03-11 10:25:41 - train: epoch 0044, iter [01400, 05004], lr: 0.130902, loss: 2.2693
2022-03-11 10:26:22 - train: epoch 0044, iter [01500, 05004], lr: 0.130902, loss: 2.3508
2022-03-11 10:27:01 - train: epoch 0044, iter [01600, 05004], lr: 0.130902, loss: 2.1119
2022-03-11 10:27:42 - train: epoch 0044, iter [01700, 05004], lr: 0.130902, loss: 2.2688
2022-03-11 10:28:24 - train: epoch 0044, iter [01800, 05004], lr: 0.130902, loss: 2.3049
2022-03-11 10:29:04 - train: epoch 0044, iter [01900, 05004], lr: 0.130902, loss: 2.5071
2022-03-11 10:29:43 - train: epoch 0044, iter [02000, 05004], lr: 0.130902, loss: 2.1305
2022-03-11 10:30:23 - train: epoch 0044, iter [02100, 05004], lr: 0.130902, loss: 2.3953
2022-03-11 10:31:03 - train: epoch 0044, iter [02200, 05004], lr: 0.130902, loss: 2.1446
2022-03-11 10:31:42 - train: epoch 0044, iter [02300, 05004], lr: 0.130902, loss: 2.3324
2022-03-11 10:32:20 - train: epoch 0044, iter [02400, 05004], lr: 0.130902, loss: 2.4675
2022-03-11 10:32:59 - train: epoch 0044, iter [02500, 05004], lr: 0.130902, loss: 2.2447
2022-03-11 10:33:38 - train: epoch 0044, iter [02600, 05004], lr: 0.130902, loss: 2.5502
2022-03-11 10:34:16 - train: epoch 0044, iter [02700, 05004], lr: 0.130902, loss: 2.2705
2022-03-11 10:34:55 - train: epoch 0044, iter [02800, 05004], lr: 0.130902, loss: 2.2898
2022-03-11 10:35:35 - train: epoch 0044, iter [02900, 05004], lr: 0.130902, loss: 2.1424
2022-03-11 10:36:16 - train: epoch 0044, iter [03000, 05004], lr: 0.130902, loss: 2.2689
2022-03-11 10:36:55 - train: epoch 0044, iter [03100, 05004], lr: 0.130902, loss: 2.4212
2022-03-11 10:37:35 - train: epoch 0044, iter [03200, 05004], lr: 0.130902, loss: 2.0648
2022-03-11 10:38:15 - train: epoch 0044, iter [03300, 05004], lr: 0.130902, loss: 2.2361
2022-03-11 10:38:56 - train: epoch 0044, iter [03400, 05004], lr: 0.130902, loss: 2.4337
2022-03-11 10:39:36 - train: epoch 0044, iter [03500, 05004], lr: 0.130902, loss: 2.3258
2022-03-11 10:40:16 - train: epoch 0044, iter [03600, 05004], lr: 0.130902, loss: 2.3795
2022-03-11 10:40:57 - train: epoch 0044, iter [03700, 05004], lr: 0.130902, loss: 2.2618
2022-03-11 10:41:37 - train: epoch 0044, iter [03800, 05004], lr: 0.130902, loss: 1.9852
2022-03-11 10:42:15 - train: epoch 0044, iter [03900, 05004], lr: 0.130902, loss: 2.3865
2022-03-11 10:42:53 - train: epoch 0044, iter [04000, 05004], lr: 0.130902, loss: 2.2735
2022-03-11 10:43:30 - train: epoch 0044, iter [04100, 05004], lr: 0.130902, loss: 2.2000
2022-03-11 10:44:08 - train: epoch 0044, iter [04200, 05004], lr: 0.130902, loss: 2.5142
2022-03-11 10:44:46 - train: epoch 0044, iter [04300, 05004], lr: 0.130902, loss: 2.3656
2022-03-11 10:45:24 - train: epoch 0044, iter [04400, 05004], lr: 0.130902, loss: 2.3727
2022-03-11 10:46:03 - train: epoch 0044, iter [04500, 05004], lr: 0.130902, loss: 2.6515
2022-03-11 10:46:42 - train: epoch 0044, iter [04600, 05004], lr: 0.130902, loss: 2.2579
2022-03-11 10:47:21 - train: epoch 0044, iter [04700, 05004], lr: 0.130902, loss: 2.3617
2022-03-11 10:47:59 - train: epoch 0044, iter [04800, 05004], lr: 0.130902, loss: 2.2382
2022-03-11 10:48:38 - train: epoch 0044, iter [04900, 05004], lr: 0.130902, loss: 2.0765
2022-03-11 10:49:17 - train: epoch 0044, iter [05000, 05004], lr: 0.130902, loss: 2.4267
2022-03-11 10:49:19 - train: epoch 044, train_loss: 2.2715
2022-03-11 10:50:43 - eval: epoch: 044, acc1: 52.034%, acc5: 77.294%, test_loss: 2.0597, per_image_load_time: 3.004ms, per_image_inference_time: 0.271ms
2022-03-11 10:50:43 - until epoch: 044, best_acc1: 52.212%
2022-03-11 10:50:43 - epoch 045 lr: 0.12774029087618446
2022-03-11 10:51:27 - train: epoch 0045, iter [00100, 05004], lr: 0.127740, loss: 2.3059
2022-03-11 10:52:07 - train: epoch 0045, iter [00200, 05004], lr: 0.127740, loss: 2.1455
2022-03-11 10:52:48 - train: epoch 0045, iter [00300, 05004], lr: 0.127740, loss: 2.1603
2022-03-11 10:53:29 - train: epoch 0045, iter [00400, 05004], lr: 0.127740, loss: 2.2642
2022-03-11 10:54:10 - train: epoch 0045, iter [00500, 05004], lr: 0.127740, loss: 2.2954
2022-03-11 10:54:50 - train: epoch 0045, iter [00600, 05004], lr: 0.127740, loss: 2.4989
2022-03-11 10:55:31 - train: epoch 0045, iter [00700, 05004], lr: 0.127740, loss: 2.0903
2022-03-11 10:56:12 - train: epoch 0045, iter [00800, 05004], lr: 0.127740, loss: 2.2308
2022-03-11 10:56:52 - train: epoch 0045, iter [00900, 05004], lr: 0.127740, loss: 2.3182
2022-03-11 10:57:36 - train: epoch 0045, iter [01000, 05004], lr: 0.127740, loss: 2.1968
2022-03-11 10:58:20 - train: epoch 0045, iter [01100, 05004], lr: 0.127740, loss: 2.4192
2022-03-11 10:59:02 - train: epoch 0045, iter [01200, 05004], lr: 0.127740, loss: 2.2773
2022-03-11 10:59:45 - train: epoch 0045, iter [01300, 05004], lr: 0.127740, loss: 2.2523
2022-03-11 11:00:28 - train: epoch 0045, iter [01400, 05004], lr: 0.127740, loss: 2.3701
2022-03-11 11:01:11 - train: epoch 0045, iter [01500, 05004], lr: 0.127740, loss: 2.3833
2022-03-11 11:01:53 - train: epoch 0045, iter [01600, 05004], lr: 0.127740, loss: 2.1504
2022-03-11 11:02:36 - train: epoch 0045, iter [01700, 05004], lr: 0.127740, loss: 2.0664
2022-03-11 11:03:18 - train: epoch 0045, iter [01800, 05004], lr: 0.127740, loss: 2.1206
2022-03-11 11:04:01 - train: epoch 0045, iter [01900, 05004], lr: 0.127740, loss: 2.2657
2022-03-11 11:04:43 - train: epoch 0045, iter [02000, 05004], lr: 0.127740, loss: 2.3416
2022-03-11 11:05:21 - train: epoch 0045, iter [02100, 05004], lr: 0.127740, loss: 2.4605
2022-03-11 11:05:58 - train: epoch 0045, iter [02200, 05004], lr: 0.127740, loss: 2.2998
2022-03-11 11:06:37 - train: epoch 0045, iter [02300, 05004], lr: 0.127740, loss: 2.0735
2022-03-11 11:07:16 - train: epoch 0045, iter [02400, 05004], lr: 0.127740, loss: 2.0433
2022-03-11 11:07:54 - train: epoch 0045, iter [02500, 05004], lr: 0.127740, loss: 2.3808
2022-03-11 11:08:34 - train: epoch 0045, iter [02600, 05004], lr: 0.127740, loss: 2.1305
2022-03-11 11:09:15 - train: epoch 0045, iter [02700, 05004], lr: 0.127740, loss: 2.0976
2022-03-11 11:09:58 - train: epoch 0045, iter [02800, 05004], lr: 0.127740, loss: 2.2386
2022-03-11 11:10:39 - train: epoch 0045, iter [02900, 05004], lr: 0.127740, loss: 2.3858
2022-03-11 11:11:22 - train: epoch 0045, iter [03000, 05004], lr: 0.127740, loss: 2.2797
2022-03-11 11:12:03 - train: epoch 0045, iter [03100, 05004], lr: 0.127740, loss: 2.2952
2022-03-11 11:12:45 - train: epoch 0045, iter [03200, 05004], lr: 0.127740, loss: 2.3743
2022-03-11 11:13:27 - train: epoch 0045, iter [03300, 05004], lr: 0.127740, loss: 2.3013
2022-03-11 11:14:09 - train: epoch 0045, iter [03400, 05004], lr: 0.127740, loss: 2.3060
2022-03-11 11:14:51 - train: epoch 0045, iter [03500, 05004], lr: 0.127740, loss: 2.4692
2022-03-11 11:15:33 - train: epoch 0045, iter [03600, 05004], lr: 0.127740, loss: 2.2442
2022-03-11 11:16:14 - train: epoch 0045, iter [03700, 05004], lr: 0.127740, loss: 2.0374
2022-03-11 11:16:54 - train: epoch 0045, iter [03800, 05004], lr: 0.127740, loss: 2.1130
2022-03-11 11:17:34 - train: epoch 0045, iter [03900, 05004], lr: 0.127740, loss: 2.2804
2022-03-11 11:18:12 - train: epoch 0045, iter [04000, 05004], lr: 0.127740, loss: 2.1938
2022-03-11 11:18:54 - train: epoch 0045, iter [04100, 05004], lr: 0.127740, loss: 2.1017
2022-03-11 11:19:36 - train: epoch 0045, iter [04200, 05004], lr: 0.127740, loss: 2.4597
2022-03-11 11:20:18 - train: epoch 0045, iter [04300, 05004], lr: 0.127740, loss: 2.2403
2022-03-11 11:21:00 - train: epoch 0045, iter [04400, 05004], lr: 0.127740, loss: 2.3604
2022-03-11 11:21:42 - train: epoch 0045, iter [04500, 05004], lr: 0.127740, loss: 2.3051
2022-03-11 11:22:25 - train: epoch 0045, iter [04600, 05004], lr: 0.127740, loss: 2.5074
2022-03-11 11:23:08 - train: epoch 0045, iter [04700, 05004], lr: 0.127740, loss: 2.2376
2022-03-11 11:23:52 - train: epoch 0045, iter [04800, 05004], lr: 0.127740, loss: 2.0776
2022-03-11 11:24:34 - train: epoch 0045, iter [04900, 05004], lr: 0.127740, loss: 2.3290
2022-03-11 11:25:18 - train: epoch 0045, iter [05000, 05004], lr: 0.127740, loss: 2.1736
2022-03-11 11:25:19 - train: epoch 045, train_loss: 2.2587
2022-03-11 11:26:44 - eval: epoch: 045, acc1: 50.146%, acc5: 75.970%, test_loss: 2.1421, per_image_load_time: 2.874ms, per_image_inference_time: 0.255ms
2022-03-11 11:26:44 - until epoch: 045, best_acc1: 52.212%
2022-03-11 11:26:44 - epoch 046 lr: 0.12454854871407993
2022-03-11 11:27:29 - train: epoch 0046, iter [00100, 05004], lr: 0.124549, loss: 2.0200
2022-03-11 11:28:12 - train: epoch 0046, iter [00200, 05004], lr: 0.124549, loss: 2.0066
2022-03-11 11:28:54 - train: epoch 0046, iter [00300, 05004], lr: 0.124549, loss: 2.0056
2022-03-11 11:29:36 - train: epoch 0046, iter [00400, 05004], lr: 0.124549, loss: 2.2988
2022-03-11 11:30:18 - train: epoch 0046, iter [00500, 05004], lr: 0.124549, loss: 2.1788
2022-03-11 11:31:00 - train: epoch 0046, iter [00600, 05004], lr: 0.124549, loss: 2.1873
2022-03-11 11:31:43 - train: epoch 0046, iter [00700, 05004], lr: 0.124549, loss: 1.9889
2022-03-11 11:32:26 - train: epoch 0046, iter [00800, 05004], lr: 0.124549, loss: 2.4495
2022-03-11 11:33:09 - train: epoch 0046, iter [00900, 05004], lr: 0.124549, loss: 2.1321
2022-03-11 11:33:52 - train: epoch 0046, iter [01000, 05004], lr: 0.124549, loss: 2.2333
2022-03-11 11:34:34 - train: epoch 0046, iter [01100, 05004], lr: 0.124549, loss: 2.1225
2022-03-11 11:35:17 - train: epoch 0046, iter [01200, 05004], lr: 0.124549, loss: 2.0986
2022-03-11 11:36:01 - train: epoch 0046, iter [01300, 05004], lr: 0.124549, loss: 2.2572
2022-03-11 11:36:45 - train: epoch 0046, iter [01400, 05004], lr: 0.124549, loss: 2.4164
2022-03-11 11:37:28 - train: epoch 0046, iter [01500, 05004], lr: 0.124549, loss: 2.2924
2022-03-11 11:38:11 - train: epoch 0046, iter [01600, 05004], lr: 0.124549, loss: 2.2298
2022-03-11 11:38:55 - train: epoch 0046, iter [01700, 05004], lr: 0.124549, loss: 2.1772
2022-03-11 11:39:39 - train: epoch 0046, iter [01800, 05004], lr: 0.124549, loss: 2.2892
2022-03-11 11:40:22 - train: epoch 0046, iter [01900, 05004], lr: 0.124549, loss: 2.0378
2022-03-11 11:41:04 - train: epoch 0046, iter [02000, 05004], lr: 0.124549, loss: 2.2082
2022-03-11 11:41:46 - train: epoch 0046, iter [02100, 05004], lr: 0.124549, loss: 2.5418
2022-03-11 11:42:30 - train: epoch 0046, iter [02200, 05004], lr: 0.124549, loss: 2.0067
2022-03-11 11:43:12 - train: epoch 0046, iter [02300, 05004], lr: 0.124549, loss: 2.3978
2022-03-11 11:43:55 - train: epoch 0046, iter [02400, 05004], lr: 0.124549, loss: 2.1486
2022-03-11 11:44:38 - train: epoch 0046, iter [02500, 05004], lr: 0.124549, loss: 2.0913
2022-03-11 11:45:20 - train: epoch 0046, iter [02600, 05004], lr: 0.124549, loss: 2.4304
2022-03-11 11:46:03 - train: epoch 0046, iter [02700, 05004], lr: 0.124549, loss: 2.0056
2022-03-11 11:46:44 - train: epoch 0046, iter [02800, 05004], lr: 0.124549, loss: 2.1924
2022-03-11 11:47:24 - train: epoch 0046, iter [02900, 05004], lr: 0.124549, loss: 2.2751
2022-03-11 11:48:05 - train: epoch 0046, iter [03000, 05004], lr: 0.124549, loss: 2.2411
2022-03-11 11:48:45 - train: epoch 0046, iter [03100, 05004], lr: 0.124549, loss: 2.0189
2022-03-11 11:49:27 - train: epoch 0046, iter [03200, 05004], lr: 0.124549, loss: 2.2560
2022-03-11 11:50:09 - train: epoch 0046, iter [03300, 05004], lr: 0.124549, loss: 2.4497
2022-03-11 11:50:51 - train: epoch 0046, iter [03400, 05004], lr: 0.124549, loss: 2.3470
2022-03-11 11:51:29 - train: epoch 0046, iter [03500, 05004], lr: 0.124549, loss: 2.3315
2022-03-11 11:52:08 - train: epoch 0046, iter [03600, 05004], lr: 0.124549, loss: 2.4219
2022-03-11 11:52:46 - train: epoch 0046, iter [03700, 05004], lr: 0.124549, loss: 2.0822
2022-03-11 11:53:25 - train: epoch 0046, iter [03800, 05004], lr: 0.124549, loss: 2.1934
2022-03-11 11:54:04 - train: epoch 0046, iter [03900, 05004], lr: 0.124549, loss: 2.0638
2022-03-11 11:54:44 - train: epoch 0046, iter [04000, 05004], lr: 0.124549, loss: 2.3059
2022-03-11 11:55:27 - train: epoch 0046, iter [04100, 05004], lr: 0.124549, loss: 2.3807
2022-03-11 11:56:10 - train: epoch 0046, iter [04200, 05004], lr: 0.124549, loss: 1.9759
2022-03-11 11:56:51 - train: epoch 0046, iter [04300, 05004], lr: 0.124549, loss: 2.4834
2022-03-11 11:57:32 - train: epoch 0046, iter [04400, 05004], lr: 0.124549, loss: 2.4478
2022-03-11 11:58:14 - train: epoch 0046, iter [04500, 05004], lr: 0.124549, loss: 2.3533
2022-03-11 11:58:57 - train: epoch 0046, iter [04600, 05004], lr: 0.124549, loss: 2.4247
2022-03-11 11:59:39 - train: epoch 0046, iter [04700, 05004], lr: 0.124549, loss: 2.3963
2022-03-11 12:00:21 - train: epoch 0046, iter [04800, 05004], lr: 0.124549, loss: 2.0977
2022-03-11 12:01:02 - train: epoch 0046, iter [04900, 05004], lr: 0.124549, loss: 2.4123
2022-03-11 12:01:43 - train: epoch 0046, iter [05000, 05004], lr: 0.124549, loss: 2.1190
2022-03-11 12:01:45 - train: epoch 046, train_loss: 2.2476
2022-03-11 12:03:14 - eval: epoch: 046, acc1: 51.606%, acc5: 76.886%, test_loss: 2.0896, per_image_load_time: 3.190ms, per_image_inference_time: 0.269ms
2022-03-11 12:03:14 - until epoch: 046, best_acc1: 52.212%
2022-03-11 12:03:14 - epoch 047 lr: 0.1213299630743747
2022-03-11 12:04:00 - train: epoch 0047, iter [00100, 05004], lr: 0.121330, loss: 2.0807
2022-03-11 12:04:41 - train: epoch 0047, iter [00200, 05004], lr: 0.121330, loss: 2.0071
2022-03-11 12:05:25 - train: epoch 0047, iter [00300, 05004], lr: 0.121330, loss: 2.0222
2022-03-11 12:06:07 - train: epoch 0047, iter [00400, 05004], lr: 0.121330, loss: 2.0412
2022-03-11 12:06:50 - train: epoch 0047, iter [00500, 05004], lr: 0.121330, loss: 2.3229
2022-03-11 12:07:32 - train: epoch 0047, iter [00600, 05004], lr: 0.121330, loss: 2.3918
2022-03-11 12:08:14 - train: epoch 0047, iter [00700, 05004], lr: 0.121330, loss: 2.3905
2022-03-11 12:08:57 - train: epoch 0047, iter [00800, 05004], lr: 0.121330, loss: 2.3248
2022-03-11 12:09:38 - train: epoch 0047, iter [00900, 05004], lr: 0.121330, loss: 2.4297
2022-03-11 12:10:20 - train: epoch 0047, iter [01000, 05004], lr: 0.121330, loss: 2.3180
2022-03-11 12:11:02 - train: epoch 0047, iter [01100, 05004], lr: 0.121330, loss: 2.3809
2022-03-11 12:11:43 - train: epoch 0047, iter [01200, 05004], lr: 0.121330, loss: 2.1313
2022-03-11 12:12:25 - train: epoch 0047, iter [01300, 05004], lr: 0.121330, loss: 2.1393
2022-03-11 12:13:09 - train: epoch 0047, iter [01400, 05004], lr: 0.121330, loss: 2.4150
2022-03-11 12:13:52 - train: epoch 0047, iter [01500, 05004], lr: 0.121330, loss: 2.2679
2022-03-11 12:14:34 - train: epoch 0047, iter [01600, 05004], lr: 0.121330, loss: 2.2481
2022-03-11 12:15:16 - train: epoch 0047, iter [01700, 05004], lr: 0.121330, loss: 2.3025
2022-03-11 12:15:56 - train: epoch 0047, iter [01800, 05004], lr: 0.121330, loss: 2.2787
2022-03-11 12:16:40 - train: epoch 0047, iter [01900, 05004], lr: 0.121330, loss: 2.1116
2022-03-11 12:17:23 - train: epoch 0047, iter [02000, 05004], lr: 0.121330, loss: 2.2970
2022-03-11 12:18:06 - train: epoch 0047, iter [02100, 05004], lr: 0.121330, loss: 2.4476
2022-03-11 12:18:48 - train: epoch 0047, iter [02200, 05004], lr: 0.121330, loss: 2.3184
2022-03-11 12:19:29 - train: epoch 0047, iter [02300, 05004], lr: 0.121330, loss: 2.4714
2022-03-11 12:20:11 - train: epoch 0047, iter [02400, 05004], lr: 0.121330, loss: 2.0590
2022-03-11 12:20:53 - train: epoch 0047, iter [02500, 05004], lr: 0.121330, loss: 2.2393
2022-03-11 12:21:37 - train: epoch 0047, iter [02600, 05004], lr: 0.121330, loss: 2.5634
2022-03-11 12:22:20 - train: epoch 0047, iter [02700, 05004], lr: 0.121330, loss: 2.0525
2022-03-11 12:23:03 - train: epoch 0047, iter [02800, 05004], lr: 0.121330, loss: 2.1673
2022-03-11 12:23:45 - train: epoch 0047, iter [02900, 05004], lr: 0.121330, loss: 2.2473
2022-03-11 12:24:27 - train: epoch 0047, iter [03000, 05004], lr: 0.121330, loss: 2.2942
2022-03-11 12:25:09 - train: epoch 0047, iter [03100, 05004], lr: 0.121330, loss: 2.1778
2022-03-11 12:25:52 - train: epoch 0047, iter [03200, 05004], lr: 0.121330, loss: 2.4497
2022-03-11 12:26:32 - train: epoch 0047, iter [03300, 05004], lr: 0.121330, loss: 2.2028
2022-03-11 12:27:12 - train: epoch 0047, iter [03400, 05004], lr: 0.121330, loss: 2.0099
2022-03-11 12:27:50 - train: epoch 0047, iter [03500, 05004], lr: 0.121330, loss: 2.3284
2022-03-11 12:28:28 - train: epoch 0047, iter [03600, 05004], lr: 0.121330, loss: 2.3050
2022-03-11 12:29:06 - train: epoch 0047, iter [03700, 05004], lr: 0.121330, loss: 2.2701
2022-03-11 12:29:44 - train: epoch 0047, iter [03800, 05004], lr: 0.121330, loss: 2.4009
2022-03-11 12:30:23 - train: epoch 0047, iter [03900, 05004], lr: 0.121330, loss: 2.2574
2022-03-11 12:31:01 - train: epoch 0047, iter [04000, 05004], lr: 0.121330, loss: 2.3273
2022-03-11 12:31:39 - train: epoch 0047, iter [04100, 05004], lr: 0.121330, loss: 2.1637
2022-03-11 12:32:16 - train: epoch 0047, iter [04200, 05004], lr: 0.121330, loss: 2.2721
2022-03-11 12:32:54 - train: epoch 0047, iter [04300, 05004], lr: 0.121330, loss: 2.0590
2022-03-11 12:33:32 - train: epoch 0047, iter [04400, 05004], lr: 0.121330, loss: 2.2464
2022-03-11 12:34:11 - train: epoch 0047, iter [04500, 05004], lr: 0.121330, loss: 2.2555
2022-03-11 12:34:49 - train: epoch 0047, iter [04600, 05004], lr: 0.121330, loss: 2.0846
2022-03-11 12:35:27 - train: epoch 0047, iter [04700, 05004], lr: 0.121330, loss: 2.4245
2022-03-11 12:36:04 - train: epoch 0047, iter [04800, 05004], lr: 0.121330, loss: 1.9590
2022-03-11 12:36:42 - train: epoch 0047, iter [04900, 05004], lr: 0.121330, loss: 2.2197
2022-03-11 12:37:20 - train: epoch 0047, iter [05000, 05004], lr: 0.121330, loss: 2.0696
2022-03-11 12:37:21 - train: epoch 047, train_loss: 2.2348
2022-03-11 12:38:42 - eval: epoch: 047, acc1: 52.332%, acc5: 77.564%, test_loss: 2.0418, per_image_load_time: 2.911ms, per_image_inference_time: 0.249ms
2022-03-11 12:38:43 - until epoch: 047, best_acc1: 52.332%
2022-03-11 12:38:43 - epoch 048 lr: 0.11808805343321101
2022-03-11 12:39:26 - train: epoch 0048, iter [00100, 05004], lr: 0.118088, loss: 2.4114
2022-03-11 12:40:05 - train: epoch 0048, iter [00200, 05004], lr: 0.118088, loss: 2.4544
2022-03-11 12:40:43 - train: epoch 0048, iter [00300, 05004], lr: 0.118088, loss: 2.2995
2022-03-11 12:41:22 - train: epoch 0048, iter [00400, 05004], lr: 0.118088, loss: 2.0544
2022-03-11 12:42:01 - train: epoch 0048, iter [00500, 05004], lr: 0.118088, loss: 2.0672
2022-03-11 12:42:40 - train: epoch 0048, iter [00600, 05004], lr: 0.118088, loss: 2.1953
2022-03-11 12:43:18 - train: epoch 0048, iter [00700, 05004], lr: 0.118088, loss: 2.1802
2022-03-11 12:43:58 - train: epoch 0048, iter [00800, 05004], lr: 0.118088, loss: 2.4164
2022-03-11 12:44:36 - train: epoch 0048, iter [00900, 05004], lr: 0.118088, loss: 2.4172
2022-03-11 12:45:15 - train: epoch 0048, iter [01000, 05004], lr: 0.118088, loss: 2.1285
2022-03-11 12:45:52 - train: epoch 0048, iter [01100, 05004], lr: 0.118088, loss: 2.2956
2022-03-11 12:46:31 - train: epoch 0048, iter [01200, 05004], lr: 0.118088, loss: 2.2594
2022-03-11 12:47:09 - train: epoch 0048, iter [01300, 05004], lr: 0.118088, loss: 2.2944
2022-03-11 12:47:47 - train: epoch 0048, iter [01400, 05004], lr: 0.118088, loss: 2.2848
2022-03-11 12:48:25 - train: epoch 0048, iter [01500, 05004], lr: 0.118088, loss: 2.3819
2022-03-11 12:49:04 - train: epoch 0048, iter [01600, 05004], lr: 0.118088, loss: 2.2836
2022-03-11 12:49:42 - train: epoch 0048, iter [01700, 05004], lr: 0.118088, loss: 2.1396
2022-03-11 12:50:19 - train: epoch 0048, iter [01800, 05004], lr: 0.118088, loss: 2.5830
2022-03-11 12:50:58 - train: epoch 0048, iter [01900, 05004], lr: 0.118088, loss: 2.1778
2022-03-11 12:51:35 - train: epoch 0048, iter [02000, 05004], lr: 0.118088, loss: 2.3174
2022-03-11 12:52:13 - train: epoch 0048, iter [02100, 05004], lr: 0.118088, loss: 2.2275
2022-03-11 12:52:51 - train: epoch 0048, iter [02200, 05004], lr: 0.118088, loss: 2.4915
2022-03-11 12:53:28 - train: epoch 0048, iter [02300, 05004], lr: 0.118088, loss: 2.0134
2022-03-11 12:54:06 - train: epoch 0048, iter [02400, 05004], lr: 0.118088, loss: 2.4042
2022-03-11 12:54:44 - train: epoch 0048, iter [02500, 05004], lr: 0.118088, loss: 2.2755
2022-03-11 12:55:26 - train: epoch 0048, iter [02600, 05004], lr: 0.118088, loss: 2.3048
2022-03-11 12:56:09 - train: epoch 0048, iter [02700, 05004], lr: 0.118088, loss: 2.3726
2022-03-11 12:56:51 - train: epoch 0048, iter [02800, 05004], lr: 0.118088, loss: 2.2271
2022-03-11 12:57:34 - train: epoch 0048, iter [02900, 05004], lr: 0.118088, loss: 2.3808
2022-03-11 12:58:17 - train: epoch 0048, iter [03000, 05004], lr: 0.118088, loss: 2.1446
2022-03-11 12:58:59 - train: epoch 0048, iter [03100, 05004], lr: 0.118088, loss: 2.4077
2022-03-11 12:59:42 - train: epoch 0048, iter [03200, 05004], lr: 0.118088, loss: 2.2100
2022-03-11 13:00:25 - train: epoch 0048, iter [03300, 05004], lr: 0.118088, loss: 2.2030
2022-03-11 13:01:08 - train: epoch 0048, iter [03400, 05004], lr: 0.118088, loss: 2.2208
2022-03-11 13:01:51 - train: epoch 0048, iter [03500, 05004], lr: 0.118088, loss: 2.4060
2022-03-11 13:02:33 - train: epoch 0048, iter [03600, 05004], lr: 0.118088, loss: 2.5267
2022-03-11 13:03:16 - train: epoch 0048, iter [03700, 05004], lr: 0.118088, loss: 2.3573
2022-03-11 13:03:59 - train: epoch 0048, iter [03800, 05004], lr: 0.118088, loss: 2.3483
2022-03-11 13:04:43 - train: epoch 0048, iter [03900, 05004], lr: 0.118088, loss: 2.1679
2022-03-11 13:05:25 - train: epoch 0048, iter [04000, 05004], lr: 0.118088, loss: 2.0028
2022-03-11 13:06:08 - train: epoch 0048, iter [04100, 05004], lr: 0.118088, loss: 2.4698
2022-03-11 13:06:50 - train: epoch 0048, iter [04200, 05004], lr: 0.118088, loss: 2.0618
2022-03-11 13:07:34 - train: epoch 0048, iter [04300, 05004], lr: 0.118088, loss: 2.1425
2022-03-11 13:08:17 - train: epoch 0048, iter [04400, 05004], lr: 0.118088, loss: 2.2042
2022-03-11 13:08:59 - train: epoch 0048, iter [04500, 05004], lr: 0.118088, loss: 2.3487
2022-03-11 13:09:42 - train: epoch 0048, iter [04600, 05004], lr: 0.118088, loss: 2.1635
2022-03-11 13:10:26 - train: epoch 0048, iter [04700, 05004], lr: 0.118088, loss: 2.2796
2022-03-11 13:11:09 - train: epoch 0048, iter [04800, 05004], lr: 0.118088, loss: 2.3737
2022-03-11 13:11:52 - train: epoch 0048, iter [04900, 05004], lr: 0.118088, loss: 2.2078
2022-03-11 13:12:34 - train: epoch 0048, iter [05000, 05004], lr: 0.118088, loss: 2.2546
2022-03-11 13:12:35 - train: epoch 048, train_loss: 2.2195
2022-03-11 13:14:09 - eval: epoch: 048, acc1: 51.900%, acc5: 77.210%, test_loss: 2.0681, per_image_load_time: 3.312ms, per_image_inference_time: 0.289ms
2022-03-11 13:14:09 - until epoch: 048, best_acc1: 52.332%
2022-03-11 13:14:09 - epoch 049 lr: 0.1148263647711842
2022-03-11 13:14:57 - train: epoch 0049, iter [00100, 05004], lr: 0.114826, loss: 2.3058
2022-03-11 13:15:40 - train: epoch 0049, iter [00200, 05004], lr: 0.114826, loss: 2.1177
2022-03-11 13:16:22 - train: epoch 0049, iter [00300, 05004], lr: 0.114826, loss: 2.2813
2022-03-11 13:17:05 - train: epoch 0049, iter [00400, 05004], lr: 0.114826, loss: 2.3113
2022-03-11 13:17:48 - train: epoch 0049, iter [00500, 05004], lr: 0.114826, loss: 2.3237
2022-03-11 13:18:31 - train: epoch 0049, iter [00600, 05004], lr: 0.114826, loss: 1.9066
2022-03-11 13:19:14 - train: epoch 0049, iter [00700, 05004], lr: 0.114826, loss: 2.1522
2022-03-11 13:19:58 - train: epoch 0049, iter [00800, 05004], lr: 0.114826, loss: 2.3995
2022-03-11 13:20:42 - train: epoch 0049, iter [00900, 05004], lr: 0.114826, loss: 1.9529
2022-03-11 13:21:26 - train: epoch 0049, iter [01000, 05004], lr: 0.114826, loss: 2.1219
2022-03-11 13:22:10 - train: epoch 0049, iter [01100, 05004], lr: 0.114826, loss: 2.0656
2022-03-11 13:22:55 - train: epoch 0049, iter [01200, 05004], lr: 0.114826, loss: 2.0125
2022-03-11 13:23:39 - train: epoch 0049, iter [01300, 05004], lr: 0.114826, loss: 2.3576
2022-03-11 13:24:23 - train: epoch 0049, iter [01400, 05004], lr: 0.114826, loss: 2.5199
2022-03-11 13:25:07 - train: epoch 0049, iter [01500, 05004], lr: 0.114826, loss: 2.1182
2022-03-11 13:25:51 - train: epoch 0049, iter [01600, 05004], lr: 0.114826, loss: 2.3717
2022-03-11 13:26:34 - train: epoch 0049, iter [01700, 05004], lr: 0.114826, loss: 2.3601
2022-03-11 13:27:17 - train: epoch 0049, iter [01800, 05004], lr: 0.114826, loss: 2.2399
2022-03-11 13:27:56 - train: epoch 0049, iter [01900, 05004], lr: 0.114826, loss: 2.0453
2022-03-11 13:28:35 - train: epoch 0049, iter [02000, 05004], lr: 0.114826, loss: 2.0985
2022-03-11 13:29:14 - train: epoch 0049, iter [02100, 05004], lr: 0.114826, loss: 1.8788
2022-03-11 13:29:52 - train: epoch 0049, iter [02200, 05004], lr: 0.114826, loss: 2.4121
2022-03-11 13:30:30 - train: epoch 0049, iter [02300, 05004], lr: 0.114826, loss: 2.0549
2022-03-11 13:31:10 - train: epoch 0049, iter [02400, 05004], lr: 0.114826, loss: 2.3692
2022-03-11 13:31:54 - train: epoch 0049, iter [02500, 05004], lr: 0.114826, loss: 2.3003
2022-03-11 13:32:38 - train: epoch 0049, iter [02600, 05004], lr: 0.114826, loss: 2.0678
2022-03-11 13:33:20 - train: epoch 0049, iter [02700, 05004], lr: 0.114826, loss: 2.3303
2022-03-11 13:34:03 - train: epoch 0049, iter [02800, 05004], lr: 0.114826, loss: 2.2502
2022-03-11 13:34:47 - train: epoch 0049, iter [02900, 05004], lr: 0.114826, loss: 2.2226
2022-03-11 13:35:30 - train: epoch 0049, iter [03000, 05004], lr: 0.114826, loss: 2.2439
2022-03-11 13:36:14 - train: epoch 0049, iter [03100, 05004], lr: 0.114826, loss: 2.1160
2022-03-11 13:36:56 - train: epoch 0049, iter [03200, 05004], lr: 0.114826, loss: 2.4725
2022-03-11 13:37:39 - train: epoch 0049, iter [03300, 05004], lr: 0.114826, loss: 2.2087
2022-03-11 13:38:23 - train: epoch 0049, iter [03400, 05004], lr: 0.114826, loss: 2.1884
2022-03-11 13:39:06 - train: epoch 0049, iter [03500, 05004], lr: 0.114826, loss: 2.2192
2022-03-11 13:39:48 - train: epoch 0049, iter [03600, 05004], lr: 0.114826, loss: 2.1535
2022-03-11 13:40:32 - train: epoch 0049, iter [03700, 05004], lr: 0.114826, loss: 2.1125
2022-03-11 13:41:15 - train: epoch 0049, iter [03800, 05004], lr: 0.114826, loss: 2.3426
2022-03-11 13:41:58 - train: epoch 0049, iter [03900, 05004], lr: 0.114826, loss: 2.4161
2022-03-11 13:42:41 - train: epoch 0049, iter [04000, 05004], lr: 0.114826, loss: 1.9887
2022-03-11 13:43:24 - train: epoch 0049, iter [04100, 05004], lr: 0.114826, loss: 2.1016
2022-03-11 13:44:07 - train: epoch 0049, iter [04200, 05004], lr: 0.114826, loss: 2.3347
2022-03-11 13:44:50 - train: epoch 0049, iter [04300, 05004], lr: 0.114826, loss: 2.2955
2022-03-11 13:45:34 - train: epoch 0049, iter [04400, 05004], lr: 0.114826, loss: 2.2729
2022-03-11 13:46:17 - train: epoch 0049, iter [04500, 05004], lr: 0.114826, loss: 2.2392
2022-03-11 13:47:00 - train: epoch 0049, iter [04600, 05004], lr: 0.114826, loss: 2.3373
2022-03-11 13:47:44 - train: epoch 0049, iter [04700, 05004], lr: 0.114826, loss: 2.4826
2022-03-11 13:48:28 - train: epoch 0049, iter [04800, 05004], lr: 0.114826, loss: 2.1926
2022-03-11 13:49:11 - train: epoch 0049, iter [04900, 05004], lr: 0.114826, loss: 2.0912
2022-03-11 13:49:55 - train: epoch 0049, iter [05000, 05004], lr: 0.114826, loss: 2.1690
2022-03-11 13:49:56 - train: epoch 049, train_loss: 2.2090
2022-03-11 13:51:26 - eval: epoch: 049, acc1: 53.980%, acc5: 79.112%, test_loss: 1.9542, per_image_load_time: 2.949ms, per_image_inference_time: 0.275ms
2022-03-11 13:51:26 - until epoch: 049, best_acc1: 53.980%
2022-03-11 13:51:26 - epoch 050 lr: 0.11154846369695864
2022-03-11 13:52:12 - train: epoch 0050, iter [00100, 05004], lr: 0.111548, loss: 2.1984
2022-03-11 13:52:54 - train: epoch 0050, iter [00200, 05004], lr: 0.111548, loss: 2.1730
2022-03-11 13:53:37 - train: epoch 0050, iter [00300, 05004], lr: 0.111548, loss: 2.2100
2022-03-11 13:54:18 - train: epoch 0050, iter [00400, 05004], lr: 0.111548, loss: 2.2926
2022-03-11 13:55:00 - train: epoch 0050, iter [00500, 05004], lr: 0.111548, loss: 2.1724
2022-03-11 13:55:43 - train: epoch 0050, iter [00600, 05004], lr: 0.111548, loss: 2.2906
2022-03-11 13:56:27 - train: epoch 0050, iter [00700, 05004], lr: 0.111548, loss: 2.1334
2022-03-11 13:57:11 - train: epoch 0050, iter [00800, 05004], lr: 0.111548, loss: 1.9712
2022-03-11 13:57:54 - train: epoch 0050, iter [00900, 05004], lr: 0.111548, loss: 2.1161
2022-03-11 13:58:38 - train: epoch 0050, iter [01000, 05004], lr: 0.111548, loss: 2.3832
2022-03-11 13:59:20 - train: epoch 0050, iter [01100, 05004], lr: 0.111548, loss: 2.0774
2022-03-11 14:00:04 - train: epoch 0050, iter [01200, 05004], lr: 0.111548, loss: 2.1465
2022-03-11 14:00:48 - train: epoch 0050, iter [01300, 05004], lr: 0.111548, loss: 2.0867
2022-03-11 14:01:31 - train: epoch 0050, iter [01400, 05004], lr: 0.111548, loss: 2.3549
2022-03-11 14:02:15 - train: epoch 0050, iter [01500, 05004], lr: 0.111548, loss: 2.0863
2022-03-11 14:02:59 - train: epoch 0050, iter [01600, 05004], lr: 0.111548, loss: 2.3064
2022-03-11 14:03:43 - train: epoch 0050, iter [01700, 05004], lr: 0.111548, loss: 2.3261
2022-03-11 14:04:27 - train: epoch 0050, iter [01800, 05004], lr: 0.111548, loss: 2.2931
2022-03-11 14:05:10 - train: epoch 0050, iter [01900, 05004], lr: 0.111548, loss: 2.3549
2022-03-11 14:05:54 - train: epoch 0050, iter [02000, 05004], lr: 0.111548, loss: 1.8440
2022-03-11 14:06:37 - train: epoch 0050, iter [02100, 05004], lr: 0.111548, loss: 2.1196
2022-03-11 14:07:20 - train: epoch 0050, iter [02200, 05004], lr: 0.111548, loss: 2.3216
2022-03-11 14:08:04 - train: epoch 0050, iter [02300, 05004], lr: 0.111548, loss: 2.0551
2022-03-11 14:08:48 - train: epoch 0050, iter [02400, 05004], lr: 0.111548, loss: 2.4394
2022-03-11 14:09:32 - train: epoch 0050, iter [02500, 05004], lr: 0.111548, loss: 2.2693
2022-03-11 14:10:16 - train: epoch 0050, iter [02600, 05004], lr: 0.111548, loss: 2.0534
2022-03-11 14:10:58 - train: epoch 0050, iter [02700, 05004], lr: 0.111548, loss: 2.2565
2022-03-11 14:11:41 - train: epoch 0050, iter [02800, 05004], lr: 0.111548, loss: 2.2817
2022-03-11 14:12:24 - train: epoch 0050, iter [02900, 05004], lr: 0.111548, loss: 2.5765
2022-03-11 14:13:08 - train: epoch 0050, iter [03000, 05004], lr: 0.111548, loss: 2.2503
2022-03-11 14:13:52 - train: epoch 0050, iter [03100, 05004], lr: 0.111548, loss: 2.3786
2022-03-11 14:14:35 - train: epoch 0050, iter [03200, 05004], lr: 0.111548, loss: 2.3830
2022-03-11 14:15:18 - train: epoch 0050, iter [03300, 05004], lr: 0.111548, loss: 2.0834
2022-03-11 14:15:59 - train: epoch 0050, iter [03400, 05004], lr: 0.111548, loss: 2.2762
2022-03-11 14:16:42 - train: epoch 0050, iter [03500, 05004], lr: 0.111548, loss: 2.0234
2022-03-11 14:17:26 - train: epoch 0050, iter [03600, 05004], lr: 0.111548, loss: 2.1420
2022-03-11 14:18:10 - train: epoch 0050, iter [03700, 05004], lr: 0.111548, loss: 2.3958
2022-03-11 14:18:53 - train: epoch 0050, iter [03800, 05004], lr: 0.111548, loss: 2.1594
2022-03-11 14:19:35 - train: epoch 0050, iter [03900, 05004], lr: 0.111548, loss: 1.9724
2022-03-11 14:20:19 - train: epoch 0050, iter [04000, 05004], lr: 0.111548, loss: 2.0343
2022-03-11 14:21:02 - train: epoch 0050, iter [04100, 05004], lr: 0.111548, loss: 2.2174
2022-03-11 14:21:46 - train: epoch 0050, iter [04200, 05004], lr: 0.111548, loss: 2.2501
2022-03-11 14:22:31 - train: epoch 0050, iter [04300, 05004], lr: 0.111548, loss: 2.2915
2022-03-11 14:23:13 - train: epoch 0050, iter [04400, 05004], lr: 0.111548, loss: 2.0764
2022-03-11 14:23:58 - train: epoch 0050, iter [04500, 05004], lr: 0.111548, loss: 2.2366
2022-03-11 14:24:42 - train: epoch 0050, iter [04600, 05004], lr: 0.111548, loss: 2.4246
2022-03-11 14:25:27 - train: epoch 0050, iter [04700, 05004], lr: 0.111548, loss: 2.2278
2022-03-11 14:26:10 - train: epoch 0050, iter [04800, 05004], lr: 0.111548, loss: 2.1979
2022-03-11 14:26:54 - train: epoch 0050, iter [04900, 05004], lr: 0.111548, loss: 2.1150
2022-03-11 14:27:38 - train: epoch 0050, iter [05000, 05004], lr: 0.111548, loss: 2.0268
2022-03-11 14:27:40 - train: epoch 050, train_loss: 2.1982
2022-03-11 14:29:11 - eval: epoch: 050, acc1: 54.000%, acc5: 78.884%, test_loss: 1.9630, per_image_load_time: 3.063ms, per_image_inference_time: 0.269ms
2022-03-11 14:29:11 - until epoch: 050, best_acc1: 54.000%
2022-03-11 14:29:11 - epoch 051 lr: 0.10825793454723326
2022-03-11 14:29:57 - train: epoch 0051, iter [00100, 05004], lr: 0.108258, loss: 2.3892
2022-03-11 14:30:40 - train: epoch 0051, iter [00200, 05004], lr: 0.108258, loss: 2.3382
2022-03-11 14:31:22 - train: epoch 0051, iter [00300, 05004], lr: 0.108258, loss: 2.0243
2022-03-11 14:32:03 - train: epoch 0051, iter [00400, 05004], lr: 0.108258, loss: 1.8976
2022-03-11 14:32:46 - train: epoch 0051, iter [00500, 05004], lr: 0.108258, loss: 2.0261
2022-03-11 14:33:29 - train: epoch 0051, iter [00600, 05004], lr: 0.108258, loss: 1.9860
2022-03-11 14:34:11 - train: epoch 0051, iter [00700, 05004], lr: 0.108258, loss: 2.0049
2022-03-11 14:34:54 - train: epoch 0051, iter [00800, 05004], lr: 0.108258, loss: 2.5024
2022-03-11 14:35:36 - train: epoch 0051, iter [00900, 05004], lr: 0.108258, loss: 2.1793
2022-03-11 14:36:17 - train: epoch 0051, iter [01000, 05004], lr: 0.108258, loss: 2.3724
2022-03-11 14:36:59 - train: epoch 0051, iter [01100, 05004], lr: 0.108258, loss: 2.3572
2022-03-11 14:37:43 - train: epoch 0051, iter [01200, 05004], lr: 0.108258, loss: 2.0350
2022-03-11 14:38:26 - train: epoch 0051, iter [01300, 05004], lr: 0.108258, loss: 2.0096
2022-03-11 14:39:07 - train: epoch 0051, iter [01400, 05004], lr: 0.108258, loss: 2.1108
2022-03-11 14:39:49 - train: epoch 0051, iter [01500, 05004], lr: 0.108258, loss: 2.1118
2022-03-11 14:40:30 - train: epoch 0051, iter [01600, 05004], lr: 0.108258, loss: 2.0016
2022-03-11 14:41:12 - train: epoch 0051, iter [01700, 05004], lr: 0.108258, loss: 2.2298
2022-03-11 14:41:54 - train: epoch 0051, iter [01800, 05004], lr: 0.108258, loss: 2.2062
2022-03-11 14:42:37 - train: epoch 0051, iter [01900, 05004], lr: 0.108258, loss: 2.0354
2022-03-11 14:43:20 - train: epoch 0051, iter [02000, 05004], lr: 0.108258, loss: 2.3155
2022-03-11 14:44:01 - train: epoch 0051, iter [02100, 05004], lr: 0.108258, loss: 2.0676
2022-03-11 14:44:43 - train: epoch 0051, iter [02200, 05004], lr: 0.108258, loss: 2.0936
2022-03-11 14:45:26 - train: epoch 0051, iter [02300, 05004], lr: 0.108258, loss: 2.2557
2022-03-11 14:46:09 - train: epoch 0051, iter [02400, 05004], lr: 0.108258, loss: 2.2145
2022-03-11 14:46:53 - train: epoch 0051, iter [02500, 05004], lr: 0.108258, loss: 2.1039
2022-03-11 14:47:37 - train: epoch 0051, iter [02600, 05004], lr: 0.108258, loss: 2.0263
2022-03-11 14:48:20 - train: epoch 0051, iter [02700, 05004], lr: 0.108258, loss: 2.3821
2022-03-11 14:49:03 - train: epoch 0051, iter [02800, 05004], lr: 0.108258, loss: 2.0104
2022-03-11 14:49:46 - train: epoch 0051, iter [02900, 05004], lr: 0.108258, loss: 2.0976
2022-03-11 14:50:29 - train: epoch 0051, iter [03000, 05004], lr: 0.108258, loss: 2.0891
2022-03-11 14:51:12 - train: epoch 0051, iter [03100, 05004], lr: 0.108258, loss: 2.2077
2022-03-11 14:51:56 - train: epoch 0051, iter [03200, 05004], lr: 0.108258, loss: 2.1259
2022-03-11 14:52:39 - train: epoch 0051, iter [03300, 05004], lr: 0.108258, loss: 2.3033
2022-03-11 14:53:22 - train: epoch 0051, iter [03400, 05004], lr: 0.108258, loss: 2.1533
2022-03-11 14:54:05 - train: epoch 0051, iter [03500, 05004], lr: 0.108258, loss: 2.2765
2022-03-11 14:54:48 - train: epoch 0051, iter [03600, 05004], lr: 0.108258, loss: 2.1422
2022-03-11 14:55:31 - train: epoch 0051, iter [03700, 05004], lr: 0.108258, loss: 2.1640
2022-03-11 14:56:14 - train: epoch 0051, iter [03800, 05004], lr: 0.108258, loss: 2.2215
2022-03-11 14:56:56 - train: epoch 0051, iter [03900, 05004], lr: 0.108258, loss: 2.1678
2022-03-11 14:57:39 - train: epoch 0051, iter [04000, 05004], lr: 0.108258, loss: 2.2082
2022-03-11 14:58:22 - train: epoch 0051, iter [04100, 05004], lr: 0.108258, loss: 2.3615
2022-03-11 14:59:04 - train: epoch 0051, iter [04200, 05004], lr: 0.108258, loss: 2.2580
2022-03-11 14:59:47 - train: epoch 0051, iter [04300, 05004], lr: 0.108258, loss: 2.1154
2022-03-11 15:00:29 - train: epoch 0051, iter [04400, 05004], lr: 0.108258, loss: 2.5020
2022-03-11 15:01:10 - train: epoch 0051, iter [04500, 05004], lr: 0.108258, loss: 1.9350
2022-03-11 15:01:53 - train: epoch 0051, iter [04600, 05004], lr: 0.108258, loss: 2.3419
2022-03-11 15:02:35 - train: epoch 0051, iter [04700, 05004], lr: 0.108258, loss: 2.2458
2022-03-11 15:03:18 - train: epoch 0051, iter [04800, 05004], lr: 0.108258, loss: 2.2140
2022-03-11 15:03:59 - train: epoch 0051, iter [04900, 05004], lr: 0.108258, loss: 2.1626
2022-03-11 15:04:41 - train: epoch 0051, iter [05000, 05004], lr: 0.108258, loss: 2.0089
2022-03-11 15:04:42 - train: epoch 051, train_loss: 2.1818
2022-03-11 15:06:14 - eval: epoch: 051, acc1: 53.992%, acc5: 79.156%, test_loss: 1.9447, per_image_load_time: 2.808ms, per_image_inference_time: 0.280ms
2022-03-11 15:06:14 - until epoch: 051, best_acc1: 54.000%
2022-03-11 15:06:14 - epoch 052 lr: 0.10495837546732223
2022-03-11 15:07:03 - train: epoch 0052, iter [00100, 05004], lr: 0.104958, loss: 2.0967
2022-03-11 15:07:48 - train: epoch 0052, iter [00200, 05004], lr: 0.104958, loss: 2.1281
2022-03-11 15:08:31 - train: epoch 0052, iter [00300, 05004], lr: 0.104958, loss: 2.2756
2022-03-11 15:09:14 - train: epoch 0052, iter [00400, 05004], lr: 0.104958, loss: 2.1724
2022-03-11 15:09:58 - train: epoch 0052, iter [00500, 05004], lr: 0.104958, loss: 2.2158
2022-03-11 15:10:41 - train: epoch 0052, iter [00600, 05004], lr: 0.104958, loss: 2.1869
2022-03-11 15:11:23 - train: epoch 0052, iter [00700, 05004], lr: 0.104958, loss: 2.2893
2022-03-11 15:12:05 - train: epoch 0052, iter [00800, 05004], lr: 0.104958, loss: 1.8726
2022-03-11 15:12:48 - train: epoch 0052, iter [00900, 05004], lr: 0.104958, loss: 2.1076
2022-03-11 15:13:30 - train: epoch 0052, iter [01000, 05004], lr: 0.104958, loss: 2.3035
2022-03-11 15:14:12 - train: epoch 0052, iter [01100, 05004], lr: 0.104958, loss: 2.0042
2022-03-11 15:14:55 - train: epoch 0052, iter [01200, 05004], lr: 0.104958, loss: 1.9870
2022-03-11 15:15:37 - train: epoch 0052, iter [01300, 05004], lr: 0.104958, loss: 2.0495
2022-03-11 15:16:19 - train: epoch 0052, iter [01400, 05004], lr: 0.104958, loss: 2.4996
2022-03-11 15:17:02 - train: epoch 0052, iter [01500, 05004], lr: 0.104958, loss: 1.9361
2022-03-11 15:17:45 - train: epoch 0052, iter [01600, 05004], lr: 0.104958, loss: 2.0496
2022-03-11 15:18:26 - train: epoch 0052, iter [01700, 05004], lr: 0.104958, loss: 2.0311
2022-03-11 15:19:07 - train: epoch 0052, iter [01800, 05004], lr: 0.104958, loss: 2.1279
2022-03-11 15:19:50 - train: epoch 0052, iter [01900, 05004], lr: 0.104958, loss: 2.1710
2022-03-11 15:20:33 - train: epoch 0052, iter [02000, 05004], lr: 0.104958, loss: 2.4012
2022-03-11 15:21:15 - train: epoch 0052, iter [02100, 05004], lr: 0.104958, loss: 2.1628
2022-03-11 15:21:57 - train: epoch 0052, iter [02200, 05004], lr: 0.104958, loss: 2.1583
2022-03-11 15:22:39 - train: epoch 0052, iter [02300, 05004], lr: 0.104958, loss: 2.0794
2022-03-11 15:23:20 - train: epoch 0052, iter [02400, 05004], lr: 0.104958, loss: 2.0281
2022-03-11 15:24:02 - train: epoch 0052, iter [02500, 05004], lr: 0.104958, loss: 2.0637
2022-03-11 15:24:45 - train: epoch 0052, iter [02600, 05004], lr: 0.104958, loss: 1.9576
2022-03-11 15:25:26 - train: epoch 0052, iter [02700, 05004], lr: 0.104958, loss: 2.3075
2022-03-11 15:26:07 - train: epoch 0052, iter [02800, 05004], lr: 0.104958, loss: 2.1762
2022-03-11 15:26:49 - train: epoch 0052, iter [02900, 05004], lr: 0.104958, loss: 2.1785
2022-03-11 15:27:32 - train: epoch 0052, iter [03000, 05004], lr: 0.104958, loss: 1.8636
2022-03-11 15:28:14 - train: epoch 0052, iter [03100, 05004], lr: 0.104958, loss: 2.1075
2022-03-11 15:28:56 - train: epoch 0052, iter [03200, 05004], lr: 0.104958, loss: 2.0022
2022-03-11 15:29:39 - train: epoch 0052, iter [03300, 05004], lr: 0.104958, loss: 2.2721
2022-03-11 15:30:19 - train: epoch 0052, iter [03400, 05004], lr: 0.104958, loss: 2.2552
2022-03-11 15:31:04 - train: epoch 0052, iter [03500, 05004], lr: 0.104958, loss: 2.3467
2022-03-11 15:31:49 - train: epoch 0052, iter [03600, 05004], lr: 0.104958, loss: 2.2897
2022-03-11 15:32:33 - train: epoch 0052, iter [03700, 05004], lr: 0.104958, loss: 2.3695
2022-03-11 15:33:17 - train: epoch 0052, iter [03800, 05004], lr: 0.104958, loss: 2.0861
2022-03-11 15:34:00 - train: epoch 0052, iter [03900, 05004], lr: 0.104958, loss: 1.9468
2022-03-11 15:34:45 - train: epoch 0052, iter [04000, 05004], lr: 0.104958, loss: 2.2785
2022-03-11 15:35:28 - train: epoch 0052, iter [04100, 05004], lr: 0.104958, loss: 1.9053
2022-03-11 15:36:11 - train: epoch 0052, iter [04200, 05004], lr: 0.104958, loss: 2.2136
2022-03-11 15:36:53 - train: epoch 0052, iter [04300, 05004], lr: 0.104958, loss: 2.2886
2022-03-11 15:37:35 - train: epoch 0052, iter [04400, 05004], lr: 0.104958, loss: 2.1459
2022-03-11 15:38:18 - train: epoch 0052, iter [04500, 05004], lr: 0.104958, loss: 2.1928
2022-03-11 15:39:02 - train: epoch 0052, iter [04600, 05004], lr: 0.104958, loss: 2.0193
2022-03-11 15:39:44 - train: epoch 0052, iter [04700, 05004], lr: 0.104958, loss: 2.0484
2022-03-11 15:40:27 - train: epoch 0052, iter [04800, 05004], lr: 0.104958, loss: 2.0991
2022-03-11 15:41:09 - train: epoch 0052, iter [04900, 05004], lr: 0.104958, loss: 2.1117
2022-03-11 15:41:52 - train: epoch 0052, iter [05000, 05004], lr: 0.104958, loss: 2.1021
2022-03-11 15:41:53 - train: epoch 052, train_loss: 2.1692
2022-03-11 15:43:28 - eval: epoch: 052, acc1: 53.656%, acc5: 78.546%, test_loss: 1.9697, per_image_load_time: 3.256ms, per_image_inference_time: 0.288ms
2022-03-11 15:43:28 - until epoch: 052, best_acc1: 54.000%
2022-03-11 15:43:28 - epoch 053 lr: 0.10165339447663586
2022-03-11 15:44:16 - train: epoch 0053, iter [00100, 05004], lr: 0.101653, loss: 2.0775
2022-03-11 15:45:01 - train: epoch 0053, iter [00200, 05004], lr: 0.101653, loss: 2.0150
2022-03-11 15:45:45 - train: epoch 0053, iter [00300, 05004], lr: 0.101653, loss: 2.1495
2022-03-11 15:46:28 - train: epoch 0053, iter [00400, 05004], lr: 0.101653, loss: 2.2001
2022-03-11 15:47:12 - train: epoch 0053, iter [00500, 05004], lr: 0.101653, loss: 2.2094
2022-03-11 15:47:54 - train: epoch 0053, iter [00600, 05004], lr: 0.101653, loss: 2.0702
2022-03-11 15:48:36 - train: epoch 0053, iter [00700, 05004], lr: 0.101653, loss: 2.1084
2022-03-11 15:49:20 - train: epoch 0053, iter [00800, 05004], lr: 0.101653, loss: 2.0492
2022-03-11 15:50:03 - train: epoch 0053, iter [00900, 05004], lr: 0.101653, loss: 2.1332
2022-03-11 15:50:47 - train: epoch 0053, iter [01000, 05004], lr: 0.101653, loss: 2.0043
2022-03-11 15:51:30 - train: epoch 0053, iter [01100, 05004], lr: 0.101653, loss: 2.2099
2022-03-11 15:52:13 - train: epoch 0053, iter [01200, 05004], lr: 0.101653, loss: 1.9721
2022-03-11 15:52:56 - train: epoch 0053, iter [01300, 05004], lr: 0.101653, loss: 2.2045
2022-03-11 15:53:40 - train: epoch 0053, iter [01400, 05004], lr: 0.101653, loss: 2.3614
2022-03-11 15:54:24 - train: epoch 0053, iter [01500, 05004], lr: 0.101653, loss: 2.1692
2022-03-11 15:55:06 - train: epoch 0053, iter [01600, 05004], lr: 0.101653, loss: 2.3203
2022-03-11 15:55:50 - train: epoch 0053, iter [01700, 05004], lr: 0.101653, loss: 2.1903
2022-03-11 15:56:34 - train: epoch 0053, iter [01800, 05004], lr: 0.101653, loss: 2.2635
2022-03-11 15:57:18 - train: epoch 0053, iter [01900, 05004], lr: 0.101653, loss: 2.2678
2022-03-11 15:58:02 - train: epoch 0053, iter [02000, 05004], lr: 0.101653, loss: 2.3101
2022-03-11 15:58:46 - train: epoch 0053, iter [02100, 05004], lr: 0.101653, loss: 2.4549
2022-03-11 15:59:30 - train: epoch 0053, iter [02200, 05004], lr: 0.101653, loss: 2.1002
2022-03-11 16:00:12 - train: epoch 0053, iter [02300, 05004], lr: 0.101653, loss: 2.2959
2022-03-11 16:00:54 - train: epoch 0053, iter [02400, 05004], lr: 0.101653, loss: 2.0791
2022-03-11 16:01:36 - train: epoch 0053, iter [02500, 05004], lr: 0.101653, loss: 2.3251
2022-03-11 16:02:18 - train: epoch 0053, iter [02600, 05004], lr: 0.101653, loss: 2.3419
2022-03-11 16:02:59 - train: epoch 0053, iter [02700, 05004], lr: 0.101653, loss: 2.3175
2022-03-11 16:03:40 - train: epoch 0053, iter [02800, 05004], lr: 0.101653, loss: 2.3889
2022-03-11 16:04:22 - train: epoch 0053, iter [02900, 05004], lr: 0.101653, loss: 2.0662
2022-03-11 16:05:03 - train: epoch 0053, iter [03000, 05004], lr: 0.101653, loss: 1.8409
2022-03-11 16:05:45 - train: epoch 0053, iter [03100, 05004], lr: 0.101653, loss: 2.3999
2022-03-11 16:06:27 - train: epoch 0053, iter [03200, 05004], lr: 0.101653, loss: 2.0845
2022-03-11 16:07:10 - train: epoch 0053, iter [03300, 05004], lr: 0.101653, loss: 2.2411
2022-03-11 16:07:50 - train: epoch 0053, iter [03400, 05004], lr: 0.101653, loss: 2.1572
2022-03-11 16:08:32 - train: epoch 0053, iter [03500, 05004], lr: 0.101653, loss: 2.0897
2022-03-11 16:09:15 - train: epoch 0053, iter [03600, 05004], lr: 0.101653, loss: 2.2751
2022-03-11 16:09:55 - train: epoch 0053, iter [03700, 05004], lr: 0.101653, loss: 2.2829
2022-03-11 16:10:38 - train: epoch 0053, iter [03800, 05004], lr: 0.101653, loss: 1.9837
2022-03-11 16:11:20 - train: epoch 0053, iter [03900, 05004], lr: 0.101653, loss: 2.3801
2022-03-11 16:12:02 - train: epoch 0053, iter [04000, 05004], lr: 0.101653, loss: 2.0979
2022-03-11 16:12:44 - train: epoch 0053, iter [04100, 05004], lr: 0.101653, loss: 2.2213
2022-03-11 16:13:26 - train: epoch 0053, iter [04200, 05004], lr: 0.101653, loss: 2.2710
2022-03-11 16:14:07 - train: epoch 0053, iter [04300, 05004], lr: 0.101653, loss: 2.3261
2022-03-11 16:14:50 - train: epoch 0053, iter [04400, 05004], lr: 0.101653, loss: 2.1058
2022-03-11 16:15:31 - train: epoch 0053, iter [04500, 05004], lr: 0.101653, loss: 2.2316
2022-03-11 16:16:12 - train: epoch 0053, iter [04600, 05004], lr: 0.101653, loss: 2.0933
2022-03-11 16:16:53 - train: epoch 0053, iter [04700, 05004], lr: 0.101653, loss: 2.1916
2022-03-11 16:17:34 - train: epoch 0053, iter [04800, 05004], lr: 0.101653, loss: 2.5228
2022-03-11 16:18:16 - train: epoch 0053, iter [04900, 05004], lr: 0.101653, loss: 2.2924
2022-03-11 16:18:57 - train: epoch 0053, iter [05000, 05004], lr: 0.101653, loss: 1.9108
2022-03-11 16:18:59 - train: epoch 053, train_loss: 2.1547
2022-03-11 16:20:31 - eval: epoch: 053, acc1: 55.150%, acc5: 80.154%, test_loss: 1.8808, per_image_load_time: 3.125ms, per_image_inference_time: 0.278ms
2022-03-11 16:20:31 - until epoch: 053, best_acc1: 55.150%
2022-03-11 16:20:31 - epoch 054 lr: 0.09834660552336416
2022-03-11 16:21:20 - train: epoch 0054, iter [00100, 05004], lr: 0.098347, loss: 1.9898
2022-03-11 16:22:03 - train: epoch 0054, iter [00200, 05004], lr: 0.098347, loss: 1.9383
2022-03-11 16:22:45 - train: epoch 0054, iter [00300, 05004], lr: 0.098347, loss: 2.2480
2022-03-11 16:23:26 - train: epoch 0054, iter [00400, 05004], lr: 0.098347, loss: 1.8890
2022-03-11 16:24:08 - train: epoch 0054, iter [00500, 05004], lr: 0.098347, loss: 2.0406
2022-03-11 16:24:50 - train: epoch 0054, iter [00600, 05004], lr: 0.098347, loss: 2.3260
2022-03-11 16:25:31 - train: epoch 0054, iter [00700, 05004], lr: 0.098347, loss: 2.2042
2022-03-11 16:26:12 - train: epoch 0054, iter [00800, 05004], lr: 0.098347, loss: 2.3641
2022-03-11 16:26:53 - train: epoch 0054, iter [00900, 05004], lr: 0.098347, loss: 1.9550
2022-03-11 16:27:35 - train: epoch 0054, iter [01000, 05004], lr: 0.098347, loss: 1.9188
2022-03-11 16:28:17 - train: epoch 0054, iter [01100, 05004], lr: 0.098347, loss: 1.8965
2022-03-11 16:28:59 - train: epoch 0054, iter [01200, 05004], lr: 0.098347, loss: 2.2750
2022-03-11 16:29:41 - train: epoch 0054, iter [01300, 05004], lr: 0.098347, loss: 2.0257
2022-03-11 16:30:22 - train: epoch 0054, iter [01400, 05004], lr: 0.098347, loss: 2.3760
2022-03-11 16:31:03 - train: epoch 0054, iter [01500, 05004], lr: 0.098347, loss: 2.0170
2022-03-11 16:31:45 - train: epoch 0054, iter [01600, 05004], lr: 0.098347, loss: 2.0257
2022-03-11 16:32:26 - train: epoch 0054, iter [01700, 05004], lr: 0.098347, loss: 2.0080
2022-03-11 16:33:07 - train: epoch 0054, iter [01800, 05004], lr: 0.098347, loss: 2.0549
2022-03-11 16:33:48 - train: epoch 0054, iter [01900, 05004], lr: 0.098347, loss: 2.2565
2022-03-11 16:34:28 - train: epoch 0054, iter [02000, 05004], lr: 0.098347, loss: 2.3097
2022-03-11 16:35:10 - train: epoch 0054, iter [02100, 05004], lr: 0.098347, loss: 2.1507
2022-03-11 16:35:51 - train: epoch 0054, iter [02200, 05004], lr: 0.098347, loss: 2.0764
2022-03-11 16:36:32 - train: epoch 0054, iter [02300, 05004], lr: 0.098347, loss: 1.8022
2022-03-11 16:37:14 - train: epoch 0054, iter [02400, 05004], lr: 0.098347, loss: 2.1965
2022-03-11 16:37:57 - train: epoch 0054, iter [02500, 05004], lr: 0.098347, loss: 2.2682
2022-03-11 16:38:37 - train: epoch 0054, iter [02600, 05004], lr: 0.098347, loss: 2.0355
2022-03-11 16:39:19 - train: epoch 0054, iter [02700, 05004], lr: 0.098347, loss: 2.1019
2022-03-11 16:40:00 - train: epoch 0054, iter [02800, 05004], lr: 0.098347, loss: 2.2510
2022-03-11 16:40:41 - train: epoch 0054, iter [02900, 05004], lr: 0.098347, loss: 1.8584
2022-03-11 16:41:23 - train: epoch 0054, iter [03000, 05004], lr: 0.098347, loss: 2.2559
2022-03-11 16:42:06 - train: epoch 0054, iter [03100, 05004], lr: 0.098347, loss: 2.2278
2022-03-11 16:42:47 - train: epoch 0054, iter [03200, 05004], lr: 0.098347, loss: 2.1977
2022-03-11 16:43:28 - train: epoch 0054, iter [03300, 05004], lr: 0.098347, loss: 2.1256
2022-03-11 16:44:07 - train: epoch 0054, iter [03400, 05004], lr: 0.098347, loss: 2.0279
2022-03-11 16:44:50 - train: epoch 0054, iter [03500, 05004], lr: 0.098347, loss: 2.1203
2022-03-11 16:45:32 - train: epoch 0054, iter [03600, 05004], lr: 0.098347, loss: 2.0915
2022-03-11 16:46:15 - train: epoch 0054, iter [03700, 05004], lr: 0.098347, loss: 2.0196
2022-03-11 16:46:55 - train: epoch 0054, iter [03800, 05004], lr: 0.098347, loss: 2.0442
2022-03-11 16:47:36 - train: epoch 0054, iter [03900, 05004], lr: 0.098347, loss: 2.2097
2022-03-11 16:48:17 - train: epoch 0054, iter [04000, 05004], lr: 0.098347, loss: 2.0612
2022-03-11 16:48:58 - train: epoch 0054, iter [04100, 05004], lr: 0.098347, loss: 2.1840
2022-03-11 16:49:40 - train: epoch 0054, iter [04200, 05004], lr: 0.098347, loss: 2.1510
2022-03-11 16:50:21 - train: epoch 0054, iter [04300, 05004], lr: 0.098347, loss: 2.1283
2022-03-11 16:51:01 - train: epoch 0054, iter [04400, 05004], lr: 0.098347, loss: 1.9943
2022-03-11 16:51:42 - train: epoch 0054, iter [04500, 05004], lr: 0.098347, loss: 1.9174
2022-03-11 16:52:22 - train: epoch 0054, iter [04600, 05004], lr: 0.098347, loss: 2.3504
2022-03-11 16:53:04 - train: epoch 0054, iter [04700, 05004], lr: 0.098347, loss: 2.4118
2022-03-11 16:53:45 - train: epoch 0054, iter [04800, 05004], lr: 0.098347, loss: 2.3932
2022-03-11 16:54:27 - train: epoch 0054, iter [04900, 05004], lr: 0.098347, loss: 2.1606
2022-03-11 16:55:08 - train: epoch 0054, iter [05000, 05004], lr: 0.098347, loss: 2.1160
2022-03-11 16:55:09 - train: epoch 054, train_loss: 2.1390
2022-03-11 16:56:36 - eval: epoch: 054, acc1: 55.276%, acc5: 80.106%, test_loss: 1.8901, per_image_load_time: 2.952ms, per_image_inference_time: 0.268ms
2022-03-11 16:56:37 - until epoch: 054, best_acc1: 55.276%
2022-03-11 16:56:37 - epoch 055 lr: 0.09504162453267777
2022-03-11 16:57:21 - train: epoch 0055, iter [00100, 05004], lr: 0.095042, loss: 2.0019
2022-03-11 16:58:01 - train: epoch 0055, iter [00200, 05004], lr: 0.095042, loss: 1.9245
2022-03-11 16:58:43 - train: epoch 0055, iter [00300, 05004], lr: 0.095042, loss: 1.9361
2022-03-11 16:59:24 - train: epoch 0055, iter [00400, 05004], lr: 0.095042, loss: 2.0746
2022-03-11 17:00:06 - train: epoch 0055, iter [00500, 05004], lr: 0.095042, loss: 1.8470
2022-03-11 17:00:47 - train: epoch 0055, iter [00600, 05004], lr: 0.095042, loss: 2.0546
2022-03-11 17:01:29 - train: epoch 0055, iter [00700, 05004], lr: 0.095042, loss: 1.9518
2022-03-11 17:02:09 - train: epoch 0055, iter [00800, 05004], lr: 0.095042, loss: 1.8614
2022-03-11 17:02:50 - train: epoch 0055, iter [00900, 05004], lr: 0.095042, loss: 2.2926
2022-03-11 17:03:31 - train: epoch 0055, iter [01000, 05004], lr: 0.095042, loss: 2.2417
2022-03-11 17:04:12 - train: epoch 0055, iter [01100, 05004], lr: 0.095042, loss: 2.2422
2022-03-11 17:04:52 - train: epoch 0055, iter [01200, 05004], lr: 0.095042, loss: 2.0547
2022-03-11 17:05:34 - train: epoch 0055, iter [01300, 05004], lr: 0.095042, loss: 2.1884
2022-03-11 17:06:15 - train: epoch 0055, iter [01400, 05004], lr: 0.095042, loss: 2.3212
2022-03-11 17:06:56 - train: epoch 0055, iter [01500, 05004], lr: 0.095042, loss: 2.0522
2022-03-11 17:07:39 - train: epoch 0055, iter [01600, 05004], lr: 0.095042, loss: 2.1593
2022-03-11 17:08:19 - train: epoch 0055, iter [01700, 05004], lr: 0.095042, loss: 2.1761
2022-03-11 17:08:59 - train: epoch 0055, iter [01800, 05004], lr: 0.095042, loss: 2.0234
2022-03-11 17:09:41 - train: epoch 0055, iter [01900, 05004], lr: 0.095042, loss: 2.2132
2022-03-11 17:10:22 - train: epoch 0055, iter [02000, 05004], lr: 0.095042, loss: 2.0174
2022-03-11 17:11:02 - train: epoch 0055, iter [02100, 05004], lr: 0.095042, loss: 1.8033
2022-03-11 17:11:43 - train: epoch 0055, iter [02200, 05004], lr: 0.095042, loss: 2.2517
2022-03-11 17:12:25 - train: epoch 0055, iter [02300, 05004], lr: 0.095042, loss: 2.1704
2022-03-11 17:13:06 - train: epoch 0055, iter [02400, 05004], lr: 0.095042, loss: 1.9543
2022-03-11 17:13:47 - train: epoch 0055, iter [02500, 05004], lr: 0.095042, loss: 2.1816
2022-03-11 17:14:28 - train: epoch 0055, iter [02600, 05004], lr: 0.095042, loss: 1.9659
2022-03-11 17:15:09 - train: epoch 0055, iter [02700, 05004], lr: 0.095042, loss: 2.1732
2022-03-11 17:15:50 - train: epoch 0055, iter [02800, 05004], lr: 0.095042, loss: 2.2611
2022-03-11 17:16:33 - train: epoch 0055, iter [02900, 05004], lr: 0.095042, loss: 2.3299
2022-03-11 17:17:14 - train: epoch 0055, iter [03000, 05004], lr: 0.095042, loss: 2.2592
2022-03-11 17:17:56 - train: epoch 0055, iter [03100, 05004], lr: 0.095042, loss: 2.1083
2022-03-11 17:18:38 - train: epoch 0055, iter [03200, 05004], lr: 0.095042, loss: 2.2392
2022-03-11 17:19:20 - train: epoch 0055, iter [03300, 05004], lr: 0.095042, loss: 1.8879
2022-03-11 17:20:02 - train: epoch 0055, iter [03400, 05004], lr: 0.095042, loss: 2.0171
2022-03-11 17:20:43 - train: epoch 0055, iter [03500, 05004], lr: 0.095042, loss: 2.0000
2022-03-11 17:21:23 - train: epoch 0055, iter [03600, 05004], lr: 0.095042, loss: 2.0700
2022-03-11 17:22:04 - train: epoch 0055, iter [03700, 05004], lr: 0.095042, loss: 1.9549
2022-03-11 17:22:45 - train: epoch 0055, iter [03800, 05004], lr: 0.095042, loss: 2.1333
2022-03-11 17:23:26 - train: epoch 0055, iter [03900, 05004], lr: 0.095042, loss: 2.3756
2022-03-11 17:24:08 - train: epoch 0055, iter [04000, 05004], lr: 0.095042, loss: 1.9952
2022-03-11 17:24:49 - train: epoch 0055, iter [04100, 05004], lr: 0.095042, loss: 2.0635
2022-03-11 17:25:32 - train: epoch 0055, iter [04200, 05004], lr: 0.095042, loss: 2.2451
2022-03-11 17:26:14 - train: epoch 0055, iter [04300, 05004], lr: 0.095042, loss: 2.1807
2022-03-11 17:26:56 - train: epoch 0055, iter [04400, 05004], lr: 0.095042, loss: 2.3561
2022-03-11 17:27:38 - train: epoch 0055, iter [04500, 05004], lr: 0.095042, loss: 2.0396
2022-03-11 17:28:19 - train: epoch 0055, iter [04600, 05004], lr: 0.095042, loss: 2.0402
2022-03-11 17:29:00 - train: epoch 0055, iter [04700, 05004], lr: 0.095042, loss: 2.1498
2022-03-11 17:29:41 - train: epoch 0055, iter [04800, 05004], lr: 0.095042, loss: 2.1064
2022-03-11 17:30:23 - train: epoch 0055, iter [04900, 05004], lr: 0.095042, loss: 2.1132
2022-03-11 17:31:04 - train: epoch 0055, iter [05000, 05004], lr: 0.095042, loss: 2.2484
2022-03-11 17:31:05 - train: epoch 055, train_loss: 2.1269
2022-03-11 17:32:36 - eval: epoch: 055, acc1: 55.540%, acc5: 80.370%, test_loss: 1.8657, per_image_load_time: 3.237ms, per_image_inference_time: 0.275ms
2022-03-11 17:32:36 - until epoch: 055, best_acc1: 55.540%
2022-03-11 17:32:36 - epoch 056 lr: 0.09174206545276678
2022-03-11 17:33:24 - train: epoch 0056, iter [00100, 05004], lr: 0.091742, loss: 2.2943
2022-03-11 17:34:08 - train: epoch 0056, iter [00200, 05004], lr: 0.091742, loss: 2.1678
2022-03-11 17:34:53 - train: epoch 0056, iter [00300, 05004], lr: 0.091742, loss: 1.9271
2022-03-11 17:35:40 - train: epoch 0056, iter [00400, 05004], lr: 0.091742, loss: 2.1373
2022-03-11 17:36:25 - train: epoch 0056, iter [00500, 05004], lr: 0.091742, loss: 2.0275
2022-03-11 17:37:10 - train: epoch 0056, iter [00600, 05004], lr: 0.091742, loss: 2.0093
2022-03-11 17:37:55 - train: epoch 0056, iter [00700, 05004], lr: 0.091742, loss: 2.1737
2022-03-11 17:38:39 - train: epoch 0056, iter [00800, 05004], lr: 0.091742, loss: 2.1407
2022-03-11 17:39:23 - train: epoch 0056, iter [00900, 05004], lr: 0.091742, loss: 2.1743
2022-03-11 17:40:05 - train: epoch 0056, iter [01000, 05004], lr: 0.091742, loss: 2.0179
2022-03-11 17:40:49 - train: epoch 0056, iter [01100, 05004], lr: 0.091742, loss: 1.9357
2022-03-11 17:41:33 - train: epoch 0056, iter [01200, 05004], lr: 0.091742, loss: 2.1162
2022-03-11 17:42:16 - train: epoch 0056, iter [01300, 05004], lr: 0.091742, loss: 2.4236
2022-03-11 17:43:00 - train: epoch 0056, iter [01400, 05004], lr: 0.091742, loss: 2.2077
2022-03-11 17:43:44 - train: epoch 0056, iter [01500, 05004], lr: 0.091742, loss: 2.5989
2022-03-11 17:44:28 - train: epoch 0056, iter [01600, 05004], lr: 0.091742, loss: 2.0033
2022-03-11 17:45:11 - train: epoch 0056, iter [01700, 05004], lr: 0.091742, loss: 2.0258
2022-03-11 17:45:55 - train: epoch 0056, iter [01800, 05004], lr: 0.091742, loss: 2.3336
2022-03-11 17:46:38 - train: epoch 0056, iter [01900, 05004], lr: 0.091742, loss: 2.2510
2022-03-11 17:47:21 - train: epoch 0056, iter [02000, 05004], lr: 0.091742, loss: 2.1682
2022-03-11 17:48:03 - train: epoch 0056, iter [02100, 05004], lr: 0.091742, loss: 2.1748
2022-03-11 17:48:46 - train: epoch 0056, iter [02200, 05004], lr: 0.091742, loss: 2.3031
2022-03-11 17:49:28 - train: epoch 0056, iter [02300, 05004], lr: 0.091742, loss: 2.1972
2022-03-11 17:50:11 - train: epoch 0056, iter [02400, 05004], lr: 0.091742, loss: 2.0694
2022-03-11 17:50:55 - train: epoch 0056, iter [02500, 05004], lr: 0.091742, loss: 2.3128
2022-03-11 17:51:38 - train: epoch 0056, iter [02600, 05004], lr: 0.091742, loss: 1.9748
2022-03-11 17:52:20 - train: epoch 0056, iter [02700, 05004], lr: 0.091742, loss: 2.2928
2022-03-11 17:53:03 - train: epoch 0056, iter [02800, 05004], lr: 0.091742, loss: 2.1044
2022-03-11 17:53:46 - train: epoch 0056, iter [02900, 05004], lr: 0.091742, loss: 2.2102
2022-03-11 17:54:29 - train: epoch 0056, iter [03000, 05004], lr: 0.091742, loss: 2.2540
2022-03-11 17:55:13 - train: epoch 0056, iter [03100, 05004], lr: 0.091742, loss: 2.0903
2022-03-11 17:55:56 - train: epoch 0056, iter [03200, 05004], lr: 0.091742, loss: 2.1240
2022-03-11 17:56:38 - train: epoch 0056, iter [03300, 05004], lr: 0.091742, loss: 2.4274
2022-03-11 17:57:20 - train: epoch 0056, iter [03400, 05004], lr: 0.091742, loss: 2.2044
2022-03-11 17:58:00 - train: epoch 0056, iter [03500, 05004], lr: 0.091742, loss: 1.9970
2022-03-11 17:58:39 - train: epoch 0056, iter [03600, 05004], lr: 0.091742, loss: 1.7508
2022-03-11 17:59:22 - train: epoch 0056, iter [03700, 05004], lr: 0.091742, loss: 2.1129
2022-03-11 18:00:05 - train: epoch 0056, iter [03800, 05004], lr: 0.091742, loss: 2.1808
2022-03-11 18:00:48 - train: epoch 0056, iter [03900, 05004], lr: 0.091742, loss: 2.5368
2022-03-11 18:01:30 - train: epoch 0056, iter [04000, 05004], lr: 0.091742, loss: 2.3295
2022-03-11 18:02:11 - train: epoch 0056, iter [04100, 05004], lr: 0.091742, loss: 2.3406
2022-03-11 18:02:52 - train: epoch 0056, iter [04200, 05004], lr: 0.091742, loss: 2.0323
2022-03-11 18:03:35 - train: epoch 0056, iter [04300, 05004], lr: 0.091742, loss: 2.1188
2022-03-11 18:04:16 - train: epoch 0056, iter [04400, 05004], lr: 0.091742, loss: 2.0137
2022-03-11 18:04:57 - train: epoch 0056, iter [04500, 05004], lr: 0.091742, loss: 2.0285
2022-03-11 18:05:37 - train: epoch 0056, iter [04600, 05004], lr: 0.091742, loss: 2.1826
2022-03-11 18:06:19 - train: epoch 0056, iter [04700, 05004], lr: 0.091742, loss: 2.1300
2022-03-11 18:07:00 - train: epoch 0056, iter [04800, 05004], lr: 0.091742, loss: 2.3045
2022-03-11 18:07:42 - train: epoch 0056, iter [04900, 05004], lr: 0.091742, loss: 2.1193
2022-03-11 18:08:23 - train: epoch 0056, iter [05000, 05004], lr: 0.091742, loss: 2.3621
2022-03-11 18:08:25 - train: epoch 056, train_loss: 2.1142
2022-03-11 18:09:57 - eval: epoch: 056, acc1: 53.718%, acc5: 78.732%, test_loss: 1.9689, per_image_load_time: 3.072ms, per_image_inference_time: 0.283ms
2022-03-11 18:09:58 - until epoch: 056, best_acc1: 55.540%
2022-03-11 18:09:58 - epoch 057 lr: 0.0884515363030414
2022-03-11 18:10:45 - train: epoch 0057, iter [00100, 05004], lr: 0.088452, loss: 2.0553
2022-03-11 18:11:28 - train: epoch 0057, iter [00200, 05004], lr: 0.088452, loss: 2.1648
2022-03-11 18:12:12 - train: epoch 0057, iter [00300, 05004], lr: 0.088452, loss: 2.0982
2022-03-11 18:12:55 - train: epoch 0057, iter [00400, 05004], lr: 0.088452, loss: 2.1189
2022-03-11 18:13:39 - train: epoch 0057, iter [00500, 05004], lr: 0.088452, loss: 1.8080
2022-03-11 18:14:23 - train: epoch 0057, iter [00600, 05004], lr: 0.088452, loss: 2.0820
2022-03-11 18:15:06 - train: epoch 0057, iter [00700, 05004], lr: 0.088452, loss: 1.9147
2022-03-11 18:15:50 - train: epoch 0057, iter [00800, 05004], lr: 0.088452, loss: 1.9329
2022-03-11 18:16:34 - train: epoch 0057, iter [00900, 05004], lr: 0.088452, loss: 2.2115
2022-03-11 18:17:17 - train: epoch 0057, iter [01000, 05004], lr: 0.088452, loss: 1.9132
2022-03-11 18:18:01 - train: epoch 0057, iter [01100, 05004], lr: 0.088452, loss: 1.8741
2022-03-11 18:18:44 - train: epoch 0057, iter [01200, 05004], lr: 0.088452, loss: 2.1411
2022-03-11 18:19:26 - train: epoch 0057, iter [01300, 05004], lr: 0.088452, loss: 2.2021
2022-03-11 18:20:11 - train: epoch 0057, iter [01400, 05004], lr: 0.088452, loss: 2.1239
2022-03-11 18:20:54 - train: epoch 0057, iter [01500, 05004], lr: 0.088452, loss: 2.1304
2022-03-11 18:21:39 - train: epoch 0057, iter [01600, 05004], lr: 0.088452, loss: 2.1729
2022-03-11 18:22:20 - train: epoch 0057, iter [01700, 05004], lr: 0.088452, loss: 2.2754
2022-03-11 18:23:04 - train: epoch 0057, iter [01800, 05004], lr: 0.088452, loss: 2.0675
2022-03-11 18:23:49 - train: epoch 0057, iter [01900, 05004], lr: 0.088452, loss: 2.1914
2022-03-11 18:24:34 - train: epoch 0057, iter [02000, 05004], lr: 0.088452, loss: 1.9840
2022-03-11 18:25:19 - train: epoch 0057, iter [02100, 05004], lr: 0.088452, loss: 2.0685
2022-03-11 18:26:01 - train: epoch 0057, iter [02200, 05004], lr: 0.088452, loss: 1.9732
2022-03-11 18:26:45 - train: epoch 0057, iter [02300, 05004], lr: 0.088452, loss: 2.1531
2022-03-11 18:27:28 - train: epoch 0057, iter [02400, 05004], lr: 0.088452, loss: 2.0010
2022-03-11 18:28:10 - train: epoch 0057, iter [02500, 05004], lr: 0.088452, loss: 2.4283
2022-03-11 18:28:53 - train: epoch 0057, iter [02600, 05004], lr: 0.088452, loss: 1.9657
2022-03-11 18:29:36 - train: epoch 0057, iter [02700, 05004], lr: 0.088452, loss: 1.7496
2022-03-11 18:30:18 - train: epoch 0057, iter [02800, 05004], lr: 0.088452, loss: 1.7312
2022-03-11 18:31:01 - train: epoch 0057, iter [02900, 05004], lr: 0.088452, loss: 2.1290
2022-03-11 18:31:44 - train: epoch 0057, iter [03000, 05004], lr: 0.088452, loss: 2.2223
2022-03-11 18:32:26 - train: epoch 0057, iter [03100, 05004], lr: 0.088452, loss: 2.4474
2022-03-11 18:33:09 - train: epoch 0057, iter [03200, 05004], lr: 0.088452, loss: 2.1397
2022-03-11 18:33:50 - train: epoch 0057, iter [03300, 05004], lr: 0.088452, loss: 2.0800
2022-03-11 18:34:32 - train: epoch 0057, iter [03400, 05004], lr: 0.088452, loss: 2.2396
2022-03-11 18:35:15 - train: epoch 0057, iter [03500, 05004], lr: 0.088452, loss: 2.1981
2022-03-11 18:35:57 - train: epoch 0057, iter [03600, 05004], lr: 0.088452, loss: 2.1072
2022-03-11 18:36:38 - train: epoch 0057, iter [03700, 05004], lr: 0.088452, loss: 2.0032
2022-03-11 18:37:20 - train: epoch 0057, iter [03800, 05004], lr: 0.088452, loss: 1.8181
2022-03-11 18:38:02 - train: epoch 0057, iter [03900, 05004], lr: 0.088452, loss: 2.4068
2022-03-11 18:38:45 - train: epoch 0057, iter [04000, 05004], lr: 0.088452, loss: 1.9421
2022-03-11 18:39:26 - train: epoch 0057, iter [04100, 05004], lr: 0.088452, loss: 2.1203
2022-03-11 18:40:07 - train: epoch 0057, iter [04200, 05004], lr: 0.088452, loss: 2.1924
2022-03-11 18:40:49 - train: epoch 0057, iter [04300, 05004], lr: 0.088452, loss: 1.8344
2022-03-11 18:41:32 - train: epoch 0057, iter [04400, 05004], lr: 0.088452, loss: 2.3159
2022-03-11 18:42:12 - train: epoch 0057, iter [04500, 05004], lr: 0.088452, loss: 2.2064
2022-03-11 18:42:54 - train: epoch 0057, iter [04600, 05004], lr: 0.088452, loss: 1.8880
2022-03-11 18:43:35 - train: epoch 0057, iter [04700, 05004], lr: 0.088452, loss: 2.0863
2022-03-11 18:44:17 - train: epoch 0057, iter [04800, 05004], lr: 0.088452, loss: 2.4425
2022-03-11 18:44:59 - train: epoch 0057, iter [04900, 05004], lr: 0.088452, loss: 2.1432
2022-03-11 18:45:42 - train: epoch 0057, iter [05000, 05004], lr: 0.088452, loss: 2.0029
2022-03-11 18:45:44 - train: epoch 057, train_loss: 2.0969
2022-03-11 18:47:11 - eval: epoch: 057, acc1: 55.654%, acc5: 80.248%, test_loss: 1.8693, per_image_load_time: 3.043ms, per_image_inference_time: 0.265ms
2022-03-11 18:47:11 - until epoch: 057, best_acc1: 55.654%
2022-03-11 18:47:11 - epoch 058 lr: 0.0851736352288158
2022-03-11 18:47:58 - train: epoch 0058, iter [00100, 05004], lr: 0.085174, loss: 2.0092
2022-03-11 18:48:40 - train: epoch 0058, iter [00200, 05004], lr: 0.085174, loss: 2.0961
2022-03-11 18:49:23 - train: epoch 0058, iter [00300, 05004], lr: 0.085174, loss: 2.0911
2022-03-11 18:50:04 - train: epoch 0058, iter [00400, 05004], lr: 0.085174, loss: 2.2434
2022-03-11 18:50:46 - train: epoch 0058, iter [00500, 05004], lr: 0.085174, loss: 1.7117
2022-03-11 18:51:28 - train: epoch 0058, iter [00600, 05004], lr: 0.085174, loss: 2.1887
2022-03-11 18:52:11 - train: epoch 0058, iter [00700, 05004], lr: 0.085174, loss: 1.8515
2022-03-11 18:52:53 - train: epoch 0058, iter [00800, 05004], lr: 0.085174, loss: 2.0988
2022-03-11 18:53:35 - train: epoch 0058, iter [00900, 05004], lr: 0.085174, loss: 1.9701
2022-03-11 18:54:19 - train: epoch 0058, iter [01000, 05004], lr: 0.085174, loss: 2.0004
2022-03-11 18:55:02 - train: epoch 0058, iter [01100, 05004], lr: 0.085174, loss: 2.0085
2022-03-11 18:55:43 - train: epoch 0058, iter [01200, 05004], lr: 0.085174, loss: 2.1050
2022-03-11 18:56:25 - train: epoch 0058, iter [01300, 05004], lr: 0.085174, loss: 2.3657
2022-03-11 18:57:08 - train: epoch 0058, iter [01400, 05004], lr: 0.085174, loss: 2.2128
2022-03-11 18:57:49 - train: epoch 0058, iter [01500, 05004], lr: 0.085174, loss: 2.1505
2022-03-11 18:58:31 - train: epoch 0058, iter [01600, 05004], lr: 0.085174, loss: 2.1888
2022-03-11 18:59:13 - train: epoch 0058, iter [01700, 05004], lr: 0.085174, loss: 2.3219
2022-03-11 18:59:56 - train: epoch 0058, iter [01800, 05004], lr: 0.085174, loss: 2.4017
2022-03-11 19:00:37 - train: epoch 0058, iter [01900, 05004], lr: 0.085174, loss: 2.1139
2022-03-11 19:01:19 - train: epoch 0058, iter [02000, 05004], lr: 0.085174, loss: 2.3698
2022-03-11 19:02:02 - train: epoch 0058, iter [02100, 05004], lr: 0.085174, loss: 1.9608
2022-03-11 19:02:44 - train: epoch 0058, iter [02200, 05004], lr: 0.085174, loss: 2.1250
2022-03-11 19:03:26 - train: epoch 0058, iter [02300, 05004], lr: 0.085174, loss: 2.0000
2022-03-11 19:04:06 - train: epoch 0058, iter [02400, 05004], lr: 0.085174, loss: 2.1234
2022-03-11 19:04:48 - train: epoch 0058, iter [02500, 05004], lr: 0.085174, loss: 2.1497
2022-03-11 19:05:30 - train: epoch 0058, iter [02600, 05004], lr: 0.085174, loss: 1.8816
2022-03-11 19:06:11 - train: epoch 0058, iter [02700, 05004], lr: 0.085174, loss: 2.2366
2022-03-11 19:06:53 - train: epoch 0058, iter [02800, 05004], lr: 0.085174, loss: 1.9437
2022-03-11 19:07:35 - train: epoch 0058, iter [02900, 05004], lr: 0.085174, loss: 2.0547
2022-03-11 19:08:14 - train: epoch 0058, iter [03000, 05004], lr: 0.085174, loss: 2.2697
2022-03-11 19:08:55 - train: epoch 0058, iter [03100, 05004], lr: 0.085174, loss: 2.0663
2022-03-11 19:09:37 - train: epoch 0058, iter [03200, 05004], lr: 0.085174, loss: 1.8774
2022-03-11 19:10:18 - train: epoch 0058, iter [03300, 05004], lr: 0.085174, loss: 2.0882
2022-03-11 19:10:57 - train: epoch 0058, iter [03400, 05004], lr: 0.085174, loss: 2.1818
2022-03-11 19:11:39 - train: epoch 0058, iter [03500, 05004], lr: 0.085174, loss: 1.9355
2022-03-11 19:12:21 - train: epoch 0058, iter [03600, 05004], lr: 0.085174, loss: 1.8316
2022-03-11 19:13:03 - train: epoch 0058, iter [03700, 05004], lr: 0.085174, loss: 2.0672
2022-03-11 19:13:45 - train: epoch 0058, iter [03800, 05004], lr: 0.085174, loss: 1.9880
2022-03-11 19:14:29 - train: epoch 0058, iter [03900, 05004], lr: 0.085174, loss: 2.0606
2022-03-11 19:15:10 - train: epoch 0058, iter [04000, 05004], lr: 0.085174, loss: 2.0945
2022-03-11 19:15:53 - train: epoch 0058, iter [04100, 05004], lr: 0.085174, loss: 2.1966
2022-03-11 19:16:34 - train: epoch 0058, iter [04200, 05004], lr: 0.085174, loss: 1.7962
2022-03-11 19:17:16 - train: epoch 0058, iter [04300, 05004], lr: 0.085174, loss: 2.3167
2022-03-11 19:17:56 - train: epoch 0058, iter [04400, 05004], lr: 0.085174, loss: 1.8108
2022-03-11 19:18:37 - train: epoch 0058, iter [04500, 05004], lr: 0.085174, loss: 1.9649
2022-03-11 19:19:19 - train: epoch 0058, iter [04600, 05004], lr: 0.085174, loss: 2.0725
2022-03-11 19:19:58 - train: epoch 0058, iter [04700, 05004], lr: 0.085174, loss: 1.9081
2022-03-11 19:20:40 - train: epoch 0058, iter [04800, 05004], lr: 0.085174, loss: 1.9768
2022-03-11 19:21:20 - train: epoch 0058, iter [04900, 05004], lr: 0.085174, loss: 1.8994
2022-03-11 19:22:02 - train: epoch 0058, iter [05000, 05004], lr: 0.085174, loss: 1.9583
2022-03-11 19:22:03 - train: epoch 058, train_loss: 2.0806
2022-03-11 19:23:33 - eval: epoch: 058, acc1: 56.506%, acc5: 80.730%, test_loss: 1.8307, per_image_load_time: 3.235ms, per_image_inference_time: 0.270ms
2022-03-11 19:23:33 - until epoch: 058, best_acc1: 56.506%
2022-03-11 19:23:33 - epoch 059 lr: 0.08191194656678905
2022-03-11 19:24:20 - train: epoch 0059, iter [00100, 05004], lr: 0.081912, loss: 1.9377
2022-03-11 19:25:05 - train: epoch 0059, iter [00200, 05004], lr: 0.081912, loss: 2.0126
2022-03-11 19:25:48 - train: epoch 0059, iter [00300, 05004], lr: 0.081912, loss: 1.9605
2022-03-11 19:26:33 - train: epoch 0059, iter [00400, 05004], lr: 0.081912, loss: 2.0921
2022-03-11 19:27:17 - train: epoch 0059, iter [00500, 05004], lr: 0.081912, loss: 1.9979
2022-03-11 19:28:00 - train: epoch 0059, iter [00600, 05004], lr: 0.081912, loss: 2.1525
2022-03-11 19:28:43 - train: epoch 0059, iter [00700, 05004], lr: 0.081912, loss: 1.9678
2022-03-11 19:29:26 - train: epoch 0059, iter [00800, 05004], lr: 0.081912, loss: 2.0494
2022-03-11 19:30:09 - train: epoch 0059, iter [00900, 05004], lr: 0.081912, loss: 1.9817
2022-03-11 19:30:53 - train: epoch 0059, iter [01000, 05004], lr: 0.081912, loss: 2.2743
2022-03-11 19:31:37 - train: epoch 0059, iter [01100, 05004], lr: 0.081912, loss: 2.4609
2022-03-11 19:32:20 - train: epoch 0059, iter [01200, 05004], lr: 0.081912, loss: 1.9352
2022-03-11 19:33:04 - train: epoch 0059, iter [01300, 05004], lr: 0.081912, loss: 2.5071
2022-03-11 19:33:49 - train: epoch 0059, iter [01400, 05004], lr: 0.081912, loss: 2.3710
2022-03-11 19:34:33 - train: epoch 0059, iter [01500, 05004], lr: 0.081912, loss: 2.0682
2022-03-11 19:35:14 - train: epoch 0059, iter [01600, 05004], lr: 0.081912, loss: 1.8291
2022-03-11 19:36:00 - train: epoch 0059, iter [01700, 05004], lr: 0.081912, loss: 2.0836
2022-03-11 19:36:44 - train: epoch 0059, iter [01800, 05004], lr: 0.081912, loss: 1.9043
2022-03-11 19:37:29 - train: epoch 0059, iter [01900, 05004], lr: 0.081912, loss: 1.8533
2022-03-11 19:38:14 - train: epoch 0059, iter [02000, 05004], lr: 0.081912, loss: 2.1933
2022-03-11 19:38:59 - train: epoch 0059, iter [02100, 05004], lr: 0.081912, loss: 2.0295
2022-03-11 19:39:42 - train: epoch 0059, iter [02200, 05004], lr: 0.081912, loss: 2.0323
2022-03-11 19:40:26 - train: epoch 0059, iter [02300, 05004], lr: 0.081912, loss: 2.0537
2022-03-11 19:41:10 - train: epoch 0059, iter [02400, 05004], lr: 0.081912, loss: 2.2451
2022-03-11 19:41:54 - train: epoch 0059, iter [02500, 05004], lr: 0.081912, loss: 2.1934
2022-03-11 19:42:38 - train: epoch 0059, iter [02600, 05004], lr: 0.081912, loss: 1.8292
2022-03-11 19:43:22 - train: epoch 0059, iter [02700, 05004], lr: 0.081912, loss: 2.2278
2022-03-11 19:44:05 - train: epoch 0059, iter [02800, 05004], lr: 0.081912, loss: 2.2120
2022-03-11 19:44:47 - train: epoch 0059, iter [02900, 05004], lr: 0.081912, loss: 1.9060
2022-03-11 19:45:30 - train: epoch 0059, iter [03000, 05004], lr: 0.081912, loss: 2.1516
2022-03-11 19:46:13 - train: epoch 0059, iter [03100, 05004], lr: 0.081912, loss: 2.0338
2022-03-11 19:46:57 - train: epoch 0059, iter [03200, 05004], lr: 0.081912, loss: 2.1367
2022-03-11 19:47:40 - train: epoch 0059, iter [03300, 05004], lr: 0.081912, loss: 2.0391
2022-03-11 19:48:23 - train: epoch 0059, iter [03400, 05004], lr: 0.081912, loss: 2.4387
2022-03-11 19:49:05 - train: epoch 0059, iter [03500, 05004], lr: 0.081912, loss: 1.9029
2022-03-11 19:49:48 - train: epoch 0059, iter [03600, 05004], lr: 0.081912, loss: 1.9673
2022-03-11 19:50:31 - train: epoch 0059, iter [03700, 05004], lr: 0.081912, loss: 2.1164
2022-03-11 19:51:14 - train: epoch 0059, iter [03800, 05004], lr: 0.081912, loss: 2.0336
2022-03-11 19:51:58 - train: epoch 0059, iter [03900, 05004], lr: 0.081912, loss: 2.0028
2022-03-11 19:52:40 - train: epoch 0059, iter [04000, 05004], lr: 0.081912, loss: 2.1565
2022-03-11 19:53:23 - train: epoch 0059, iter [04100, 05004], lr: 0.081912, loss: 1.8261
2022-03-11 19:54:08 - train: epoch 0059, iter [04200, 05004], lr: 0.081912, loss: 2.0090
2022-03-11 19:54:51 - train: epoch 0059, iter [04300, 05004], lr: 0.081912, loss: 2.2182
2022-03-11 19:55:34 - train: epoch 0059, iter [04400, 05004], lr: 0.081912, loss: 2.0827
2022-03-11 19:56:16 - train: epoch 0059, iter [04500, 05004], lr: 0.081912, loss: 2.1686
2022-03-11 19:57:00 - train: epoch 0059, iter [04600, 05004], lr: 0.081912, loss: 2.1654
2022-03-11 19:57:44 - train: epoch 0059, iter [04700, 05004], lr: 0.081912, loss: 2.0643
2022-03-11 19:58:28 - train: epoch 0059, iter [04800, 05004], lr: 0.081912, loss: 1.9111
2022-03-11 19:59:11 - train: epoch 0059, iter [04900, 05004], lr: 0.081912, loss: 2.1677
2022-03-11 19:59:54 - train: epoch 0059, iter [05000, 05004], lr: 0.081912, loss: 2.1164
2022-03-11 19:59:56 - train: epoch 059, train_loss: 2.0681
2022-03-11 20:01:34 - eval: epoch: 059, acc1: 55.338%, acc5: 80.266%, test_loss: 1.8685, per_image_load_time: 3.414ms, per_image_inference_time: 0.301ms
2022-03-11 20:01:34 - until epoch: 059, best_acc1: 56.506%
2022-03-11 20:01:34 - epoch 060 lr: 0.07867003692562534
2022-03-11 20:02:23 - train: epoch 0060, iter [00100, 05004], lr: 0.078670, loss: 2.2019
2022-03-11 20:03:09 - train: epoch 0060, iter [00200, 05004], lr: 0.078670, loss: 2.0402
2022-03-11 20:03:53 - train: epoch 0060, iter [00300, 05004], lr: 0.078670, loss: 1.8981
2022-03-11 20:04:37 - train: epoch 0060, iter [00400, 05004], lr: 0.078670, loss: 2.0214
2022-03-11 20:05:21 - train: epoch 0060, iter [00500, 05004], lr: 0.078670, loss: 2.3097
2022-03-11 20:06:05 - train: epoch 0060, iter [00600, 05004], lr: 0.078670, loss: 2.1307
2022-03-11 20:06:50 - train: epoch 0060, iter [00700, 05004], lr: 0.078670, loss: 2.1126
2022-03-11 20:07:34 - train: epoch 0060, iter [00800, 05004], lr: 0.078670, loss: 2.1777
2022-03-11 20:08:18 - train: epoch 0060, iter [00900, 05004], lr: 0.078670, loss: 1.9085
2022-03-11 20:09:03 - train: epoch 0060, iter [01000, 05004], lr: 0.078670, loss: 1.6426
2022-03-11 20:09:46 - train: epoch 0060, iter [01100, 05004], lr: 0.078670, loss: 1.9686
2022-03-11 20:10:32 - train: epoch 0060, iter [01200, 05004], lr: 0.078670, loss: 2.0160
2022-03-11 20:11:16 - train: epoch 0060, iter [01300, 05004], lr: 0.078670, loss: 1.9925
2022-03-11 20:11:59 - train: epoch 0060, iter [01400, 05004], lr: 0.078670, loss: 1.9364
2022-03-11 20:12:41 - train: epoch 0060, iter [01500, 05004], lr: 0.078670, loss: 2.0233
2022-03-11 20:13:22 - train: epoch 0060, iter [01600, 05004], lr: 0.078670, loss: 1.9431
2022-03-11 20:14:04 - train: epoch 0060, iter [01700, 05004], lr: 0.078670, loss: 2.0894
2022-03-11 20:14:47 - train: epoch 0060, iter [01800, 05004], lr: 0.078670, loss: 2.1023
2022-03-11 20:15:29 - train: epoch 0060, iter [01900, 05004], lr: 0.078670, loss: 2.2654
2022-03-11 20:16:10 - train: epoch 0060, iter [02000, 05004], lr: 0.078670, loss: 1.9330
2022-03-11 20:16:52 - train: epoch 0060, iter [02100, 05004], lr: 0.078670, loss: 2.2645
2022-03-11 20:17:33 - train: epoch 0060, iter [02200, 05004], lr: 0.078670, loss: 2.1943
2022-03-11 20:18:15 - train: epoch 0060, iter [02300, 05004], lr: 0.078670, loss: 1.9870
2022-03-11 20:18:57 - train: epoch 0060, iter [02400, 05004], lr: 0.078670, loss: 2.0720
2022-03-11 20:19:39 - train: epoch 0060, iter [02500, 05004], lr: 0.078670, loss: 1.9688
2022-03-11 20:20:20 - train: epoch 0060, iter [02600, 05004], lr: 0.078670, loss: 2.0236
2022-03-11 20:21:01 - train: epoch 0060, iter [02700, 05004], lr: 0.078670, loss: 1.9698
2022-03-11 20:21:42 - train: epoch 0060, iter [02800, 05004], lr: 0.078670, loss: 2.0952
2022-03-11 20:22:24 - train: epoch 0060, iter [02900, 05004], lr: 0.078670, loss: 2.1260
2022-03-11 20:23:07 - train: epoch 0060, iter [03000, 05004], lr: 0.078670, loss: 2.2072
2022-03-11 20:23:49 - train: epoch 0060, iter [03100, 05004], lr: 0.078670, loss: 2.1477
2022-03-11 20:24:30 - train: epoch 0060, iter [03200, 05004], lr: 0.078670, loss: 1.9844
2022-03-11 20:25:15 - train: epoch 0060, iter [03300, 05004], lr: 0.078670, loss: 1.7205
2022-03-11 20:26:00 - train: epoch 0060, iter [03400, 05004], lr: 0.078670, loss: 2.0021
2022-03-11 20:26:45 - train: epoch 0060, iter [03500, 05004], lr: 0.078670, loss: 2.0201
2022-03-11 20:27:29 - train: epoch 0060, iter [03600, 05004], lr: 0.078670, loss: 1.9833
2022-03-11 20:28:14 - train: epoch 0060, iter [03700, 05004], lr: 0.078670, loss: 2.0310
2022-03-11 20:28:59 - train: epoch 0060, iter [03800, 05004], lr: 0.078670, loss: 2.1537
2022-03-11 20:29:42 - train: epoch 0060, iter [03900, 05004], lr: 0.078670, loss: 2.1442
2022-03-11 20:30:27 - train: epoch 0060, iter [04000, 05004], lr: 0.078670, loss: 2.0654
2022-03-11 20:31:09 - train: epoch 0060, iter [04100, 05004], lr: 0.078670, loss: 2.2125
2022-03-11 20:31:53 - train: epoch 0060, iter [04200, 05004], lr: 0.078670, loss: 2.1896
2022-03-11 20:32:38 - train: epoch 0060, iter [04300, 05004], lr: 0.078670, loss: 2.1427
2022-03-11 20:33:21 - train: epoch 0060, iter [04400, 05004], lr: 0.078670, loss: 2.2007
2022-03-11 20:34:04 - train: epoch 0060, iter [04500, 05004], lr: 0.078670, loss: 2.1032
2022-03-11 20:34:48 - train: epoch 0060, iter [04600, 05004], lr: 0.078670, loss: 1.9485
2022-03-11 20:35:32 - train: epoch 0060, iter [04700, 05004], lr: 0.078670, loss: 1.9406
2022-03-11 20:36:15 - train: epoch 0060, iter [04800, 05004], lr: 0.078670, loss: 1.9378
2022-03-11 20:36:59 - train: epoch 0060, iter [04900, 05004], lr: 0.078670, loss: 2.2679
2022-03-11 20:37:42 - train: epoch 0060, iter [05000, 05004], lr: 0.078670, loss: 2.0129
2022-03-11 20:37:44 - train: epoch 060, train_loss: 2.0508
2022-03-11 20:39:19 - eval: epoch: 060, acc1: 56.640%, acc5: 80.892%, test_loss: 1.8172, per_image_load_time: 3.425ms, per_image_inference_time: 0.293ms
2022-03-11 20:39:19 - until epoch: 060, best_acc1: 56.640%
2022-03-11 20:39:19 - epoch 061 lr: 0.0754514512859201
2022-03-11 20:40:06 - train: epoch 0061, iter [00100, 05004], lr: 0.075451, loss: 1.6963
2022-03-11 20:40:50 - train: epoch 0061, iter [00200, 05004], lr: 0.075451, loss: 2.0311
2022-03-11 20:41:32 - train: epoch 0061, iter [00300, 05004], lr: 0.075451, loss: 1.8079
2022-03-11 20:42:17 - train: epoch 0061, iter [00400, 05004], lr: 0.075451, loss: 2.1644
2022-03-11 20:43:01 - train: epoch 0061, iter [00500, 05004], lr: 0.075451, loss: 1.9345
2022-03-11 20:43:45 - train: epoch 0061, iter [00600, 05004], lr: 0.075451, loss: 2.1449
2022-03-11 20:44:28 - train: epoch 0061, iter [00700, 05004], lr: 0.075451, loss: 1.9146
2022-03-11 20:45:12 - train: epoch 0061, iter [00800, 05004], lr: 0.075451, loss: 1.9407
2022-03-11 20:45:54 - train: epoch 0061, iter [00900, 05004], lr: 0.075451, loss: 2.0275
2022-03-11 20:46:38 - train: epoch 0061, iter [01000, 05004], lr: 0.075451, loss: 2.0586
2022-03-11 20:47:22 - train: epoch 0061, iter [01100, 05004], lr: 0.075451, loss: 1.8272
2022-03-11 20:48:05 - train: epoch 0061, iter [01200, 05004], lr: 0.075451, loss: 1.8128
2022-03-11 20:48:48 - train: epoch 0061, iter [01300, 05004], lr: 0.075451, loss: 2.0667
2022-03-11 20:49:33 - train: epoch 0061, iter [01400, 05004], lr: 0.075451, loss: 1.9279
2022-03-11 20:50:18 - train: epoch 0061, iter [01500, 05004], lr: 0.075451, loss: 1.9141
2022-03-11 20:51:03 - train: epoch 0061, iter [01600, 05004], lr: 0.075451, loss: 1.8493
2022-03-11 20:51:49 - train: epoch 0061, iter [01700, 05004], lr: 0.075451, loss: 2.1811
2022-03-11 20:52:34 - train: epoch 0061, iter [01800, 05004], lr: 0.075451, loss: 2.1258
2022-03-11 20:53:18 - train: epoch 0061, iter [01900, 05004], lr: 0.075451, loss: 2.3933
2022-03-11 20:54:02 - train: epoch 0061, iter [02000, 05004], lr: 0.075451, loss: 2.1200
2022-03-11 20:54:46 - train: epoch 0061, iter [02100, 05004], lr: 0.075451, loss: 2.2629
2022-03-11 20:55:30 - train: epoch 0061, iter [02200, 05004], lr: 0.075451, loss: 2.0053
2022-03-11 20:56:14 - train: epoch 0061, iter [02300, 05004], lr: 0.075451, loss: 1.8607
2022-03-11 20:56:58 - train: epoch 0061, iter [02400, 05004], lr: 0.075451, loss: 1.9161
2022-03-11 20:57:43 - train: epoch 0061, iter [02500, 05004], lr: 0.075451, loss: 1.8965
2022-03-11 20:58:26 - train: epoch 0061, iter [02600, 05004], lr: 0.075451, loss: 2.1613
2022-03-11 20:59:09 - train: epoch 0061, iter [02700, 05004], lr: 0.075451, loss: 2.1339
2022-03-11 20:59:52 - train: epoch 0061, iter [02800, 05004], lr: 0.075451, loss: 1.9969
2022-03-11 21:00:35 - train: epoch 0061, iter [02900, 05004], lr: 0.075451, loss: 2.1737
2022-03-11 21:01:18 - train: epoch 0061, iter [03000, 05004], lr: 0.075451, loss: 2.0112
2022-03-11 21:01:59 - train: epoch 0061, iter [03100, 05004], lr: 0.075451, loss: 2.1847
2022-03-11 21:02:40 - train: epoch 0061, iter [03200, 05004], lr: 0.075451, loss: 1.9805
2022-03-11 21:03:22 - train: epoch 0061, iter [03300, 05004], lr: 0.075451, loss: 2.0392
2022-03-11 21:04:03 - train: epoch 0061, iter [03400, 05004], lr: 0.075451, loss: 2.1181
2022-03-11 21:04:45 - train: epoch 0061, iter [03500, 05004], lr: 0.075451, loss: 2.1396
2022-03-11 21:05:25 - train: epoch 0061, iter [03600, 05004], lr: 0.075451, loss: 1.9961
2022-03-11 21:06:06 - train: epoch 0061, iter [03700, 05004], lr: 0.075451, loss: 1.8991
2022-03-11 21:06:47 - train: epoch 0061, iter [03800, 05004], lr: 0.075451, loss: 1.9526
2022-03-11 21:07:28 - train: epoch 0061, iter [03900, 05004], lr: 0.075451, loss: 1.8864
2022-03-11 21:08:11 - train: epoch 0061, iter [04000, 05004], lr: 0.075451, loss: 2.0690
2022-03-11 21:08:54 - train: epoch 0061, iter [04100, 05004], lr: 0.075451, loss: 2.1499
2022-03-11 21:09:37 - train: epoch 0061, iter [04200, 05004], lr: 0.075451, loss: 2.1865
2022-03-11 21:10:19 - train: epoch 0061, iter [04300, 05004], lr: 0.075451, loss: 2.0881
2022-03-11 21:11:01 - train: epoch 0061, iter [04400, 05004], lr: 0.075451, loss: 2.1189
2022-03-11 21:11:43 - train: epoch 0061, iter [04500, 05004], lr: 0.075451, loss: 2.1014
2022-03-11 21:12:24 - train: epoch 0061, iter [04600, 05004], lr: 0.075451, loss: 2.2143
2022-03-11 21:13:06 - train: epoch 0061, iter [04700, 05004], lr: 0.075451, loss: 2.0513
2022-03-11 21:13:47 - train: epoch 0061, iter [04800, 05004], lr: 0.075451, loss: 2.0723
2022-03-11 21:14:31 - train: epoch 0061, iter [04900, 05004], lr: 0.075451, loss: 1.9290
2022-03-11 21:15:12 - train: epoch 0061, iter [05000, 05004], lr: 0.075451, loss: 2.3669
2022-03-11 21:15:14 - train: epoch 061, train_loss: 2.0322
2022-03-11 21:16:46 - eval: epoch: 061, acc1: 57.576%, acc5: 81.276%, test_loss: 1.7931, per_image_load_time: 3.091ms, per_image_inference_time: 0.290ms
2022-03-11 21:16:46 - until epoch: 061, best_acc1: 57.576%
2022-03-11 21:16:46 - epoch 062 lr: 0.07225970912381556
2022-03-11 21:17:32 - train: epoch 0062, iter [00100, 05004], lr: 0.072260, loss: 1.9668
2022-03-11 21:18:13 - train: epoch 0062, iter [00200, 05004], lr: 0.072260, loss: 2.0624
2022-03-11 21:18:56 - train: epoch 0062, iter [00300, 05004], lr: 0.072260, loss: 1.9625
2022-03-11 21:19:38 - train: epoch 0062, iter [00400, 05004], lr: 0.072260, loss: 2.0202
2022-03-11 21:20:20 - train: epoch 0062, iter [00500, 05004], lr: 0.072260, loss: 1.8123
2022-03-11 21:21:02 - train: epoch 0062, iter [00600, 05004], lr: 0.072260, loss: 1.9455
2022-03-11 21:21:43 - train: epoch 0062, iter [00700, 05004], lr: 0.072260, loss: 1.9475
2022-03-11 21:22:25 - train: epoch 0062, iter [00800, 05004], lr: 0.072260, loss: 1.9809
2022-03-11 21:23:06 - train: epoch 0062, iter [00900, 05004], lr: 0.072260, loss: 2.0723
2022-03-11 21:23:49 - train: epoch 0062, iter [01000, 05004], lr: 0.072260, loss: 2.0160
2022-03-11 21:24:31 - train: epoch 0062, iter [01100, 05004], lr: 0.072260, loss: 1.9776
2022-03-11 21:25:14 - train: epoch 0062, iter [01200, 05004], lr: 0.072260, loss: 2.1093
2022-03-11 21:25:55 - train: epoch 0062, iter [01300, 05004], lr: 0.072260, loss: 2.1473
2022-03-11 21:26:36 - train: epoch 0062, iter [01400, 05004], lr: 0.072260, loss: 1.9359
2022-03-11 21:27:17 - train: epoch 0062, iter [01500, 05004], lr: 0.072260, loss: 2.0226
2022-03-11 21:27:59 - train: epoch 0062, iter [01600, 05004], lr: 0.072260, loss: 2.3143
2022-03-11 21:28:40 - train: epoch 0062, iter [01700, 05004], lr: 0.072260, loss: 2.0034
2022-03-11 21:29:21 - train: epoch 0062, iter [01800, 05004], lr: 0.072260, loss: 2.0507
2022-03-11 21:30:01 - train: epoch 0062, iter [01900, 05004], lr: 0.072260, loss: 1.9743
2022-03-11 21:30:42 - train: epoch 0062, iter [02000, 05004], lr: 0.072260, loss: 2.2629
2022-03-11 21:31:23 - train: epoch 0062, iter [02100, 05004], lr: 0.072260, loss: 2.2619
2022-03-11 21:32:04 - train: epoch 0062, iter [02200, 05004], lr: 0.072260, loss: 1.9189
2022-03-11 21:32:46 - train: epoch 0062, iter [02300, 05004], lr: 0.072260, loss: 1.9653
2022-03-11 21:33:27 - train: epoch 0062, iter [02400, 05004], lr: 0.072260, loss: 1.9502
2022-03-11 21:34:08 - train: epoch 0062, iter [02500, 05004], lr: 0.072260, loss: 2.1592
2022-03-11 21:34:49 - train: epoch 0062, iter [02600, 05004], lr: 0.072260, loss: 1.7527
2022-03-11 21:35:30 - train: epoch 0062, iter [02700, 05004], lr: 0.072260, loss: 2.0361
2022-03-11 21:36:12 - train: epoch 0062, iter [02800, 05004], lr: 0.072260, loss: 2.1941
2022-03-11 21:36:55 - train: epoch 0062, iter [02900, 05004], lr: 0.072260, loss: 1.8714
2022-03-11 21:37:37 - train: epoch 0062, iter [03000, 05004], lr: 0.072260, loss: 2.1224
2022-03-11 21:38:19 - train: epoch 0062, iter [03100, 05004], lr: 0.072260, loss: 2.0648
2022-03-11 21:39:03 - train: epoch 0062, iter [03200, 05004], lr: 0.072260, loss: 1.8651
2022-03-11 21:39:47 - train: epoch 0062, iter [03300, 05004], lr: 0.072260, loss: 2.2173
2022-03-11 21:40:31 - train: epoch 0062, iter [03400, 05004], lr: 0.072260, loss: 1.8681
2022-03-11 21:41:14 - train: epoch 0062, iter [03500, 05004], lr: 0.072260, loss: 2.0406
2022-03-11 21:41:54 - train: epoch 0062, iter [03600, 05004], lr: 0.072260, loss: 2.0808
2022-03-11 21:42:35 - train: epoch 0062, iter [03700, 05004], lr: 0.072260, loss: 2.0773
2022-03-11 21:43:16 - train: epoch 0062, iter [03800, 05004], lr: 0.072260, loss: 2.1031
2022-03-11 21:43:57 - train: epoch 0062, iter [03900, 05004], lr: 0.072260, loss: 2.2027
2022-03-11 21:44:38 - train: epoch 0062, iter [04000, 05004], lr: 0.072260, loss: 1.9688
2022-03-11 21:45:19 - train: epoch 0062, iter [04100, 05004], lr: 0.072260, loss: 2.1197
2022-03-11 21:45:59 - train: epoch 0062, iter [04200, 05004], lr: 0.072260, loss: 1.8425
2022-03-11 21:46:40 - train: epoch 0062, iter [04300, 05004], lr: 0.072260, loss: 1.9789
2022-03-11 21:47:20 - train: epoch 0062, iter [04400, 05004], lr: 0.072260, loss: 1.9390
2022-03-11 21:48:01 - train: epoch 0062, iter [04500, 05004], lr: 0.072260, loss: 1.9860
2022-03-11 21:48:42 - train: epoch 0062, iter [04600, 05004], lr: 0.072260, loss: 2.0004
2022-03-11 21:49:23 - train: epoch 0062, iter [04700, 05004], lr: 0.072260, loss: 2.1010
2022-03-11 21:50:04 - train: epoch 0062, iter [04800, 05004], lr: 0.072260, loss: 1.9294
2022-03-11 21:50:45 - train: epoch 0062, iter [04900, 05004], lr: 0.072260, loss: 2.0352
2022-03-11 21:51:25 - train: epoch 0062, iter [05000, 05004], lr: 0.072260, loss: 2.1284
2022-03-11 21:51:27 - train: epoch 062, train_loss: 2.0189
2022-03-11 21:52:56 - eval: epoch: 062, acc1: 58.010%, acc5: 82.076%, test_loss: 1.7457, per_image_load_time: 2.850ms, per_image_inference_time: 0.272ms
2022-03-11 21:52:56 - until epoch: 062, best_acc1: 58.010%
2022-03-11 21:52:56 - epoch 063 lr: 0.06909830056250527
2022-03-11 21:53:42 - train: epoch 0063, iter [00100, 05004], lr: 0.069098, loss: 1.8124
2022-03-11 21:54:26 - train: epoch 0063, iter [00200, 05004], lr: 0.069098, loss: 2.0110
2022-03-11 21:55:11 - train: epoch 0063, iter [00300, 05004], lr: 0.069098, loss: 2.2388
2022-03-11 21:55:54 - train: epoch 0063, iter [00400, 05004], lr: 0.069098, loss: 1.8894
2022-03-11 21:56:38 - train: epoch 0063, iter [00500, 05004], lr: 0.069098, loss: 1.7979
2022-03-11 21:57:21 - train: epoch 0063, iter [00600, 05004], lr: 0.069098, loss: 2.0120
2022-03-11 21:58:04 - train: epoch 0063, iter [00700, 05004], lr: 0.069098, loss: 1.9140
2022-03-11 21:58:48 - train: epoch 0063, iter [00800, 05004], lr: 0.069098, loss: 1.9957
2022-03-11 21:59:31 - train: epoch 0063, iter [00900, 05004], lr: 0.069098, loss: 1.9902
2022-03-11 22:00:15 - train: epoch 0063, iter [01000, 05004], lr: 0.069098, loss: 2.0187
2022-03-11 22:00:59 - train: epoch 0063, iter [01100, 05004], lr: 0.069098, loss: 1.9150
2022-03-11 22:01:40 - train: epoch 0063, iter [01200, 05004], lr: 0.069098, loss: 1.8795
2022-03-11 22:02:25 - train: epoch 0063, iter [01300, 05004], lr: 0.069098, loss: 1.7201
2022-03-11 22:03:13 - train: epoch 0063, iter [01400, 05004], lr: 0.069098, loss: 2.4459
2022-03-11 22:04:01 - train: epoch 0063, iter [01500, 05004], lr: 0.069098, loss: 2.0087
2022-03-11 22:04:47 - train: epoch 0063, iter [01600, 05004], lr: 0.069098, loss: 1.9950
2022-03-11 22:05:34 - train: epoch 0063, iter [01700, 05004], lr: 0.069098, loss: 1.9525
2022-03-11 22:06:19 - train: epoch 0063, iter [01800, 05004], lr: 0.069098, loss: 2.1220
2022-03-11 22:07:03 - train: epoch 0063, iter [01900, 05004], lr: 0.069098, loss: 2.2051
2022-03-11 22:07:48 - train: epoch 0063, iter [02000, 05004], lr: 0.069098, loss: 2.0458
2022-03-11 22:08:31 - train: epoch 0063, iter [02100, 05004], lr: 0.069098, loss: 1.9309
2022-03-11 22:09:17 - train: epoch 0063, iter [02200, 05004], lr: 0.069098, loss: 2.2993
2022-03-11 22:10:01 - train: epoch 0063, iter [02300, 05004], lr: 0.069098, loss: 2.1732
2022-03-11 22:10:45 - train: epoch 0063, iter [02400, 05004], lr: 0.069098, loss: 2.0063
2022-03-11 22:11:29 - train: epoch 0063, iter [02500, 05004], lr: 0.069098, loss: 1.8824
2022-03-11 22:12:11 - train: epoch 0063, iter [02600, 05004], lr: 0.069098, loss: 1.8148
2022-03-11 22:12:53 - train: epoch 0063, iter [02700, 05004], lr: 0.069098, loss: 2.1315
2022-03-11 22:13:35 - train: epoch 0063, iter [02800, 05004], lr: 0.069098, loss: 2.0150
2022-03-11 22:14:16 - train: epoch 0063, iter [02900, 05004], lr: 0.069098, loss: 2.2697
2022-03-11 22:14:57 - train: epoch 0063, iter [03000, 05004], lr: 0.069098, loss: 2.3559
2022-03-11 22:15:38 - train: epoch 0063, iter [03100, 05004], lr: 0.069098, loss: 2.0115
2022-03-11 22:16:19 - train: epoch 0063, iter [03200, 05004], lr: 0.069098, loss: 2.0336
2022-03-11 22:17:01 - train: epoch 0063, iter [03300, 05004], lr: 0.069098, loss: 1.7492
2022-03-11 22:17:42 - train: epoch 0063, iter [03400, 05004], lr: 0.069098, loss: 1.8264
2022-03-11 22:18:23 - train: epoch 0063, iter [03500, 05004], lr: 0.069098, loss: 2.0589
2022-03-11 22:19:06 - train: epoch 0063, iter [03600, 05004], lr: 0.069098, loss: 1.9909
2022-03-11 22:19:47 - train: epoch 0063, iter [03700, 05004], lr: 0.069098, loss: 2.1436
2022-03-11 22:20:29 - train: epoch 0063, iter [03800, 05004], lr: 0.069098, loss: 2.0951
2022-03-11 22:21:12 - train: epoch 0063, iter [03900, 05004], lr: 0.069098, loss: 1.9488
2022-03-11 22:21:54 - train: epoch 0063, iter [04000, 05004], lr: 0.069098, loss: 1.7288
2022-03-11 22:22:36 - train: epoch 0063, iter [04100, 05004], lr: 0.069098, loss: 1.9219
2022-03-11 22:23:19 - train: epoch 0063, iter [04200, 05004], lr: 0.069098, loss: 2.0492
2022-03-11 22:24:01 - train: epoch 0063, iter [04300, 05004], lr: 0.069098, loss: 1.8290
2022-03-11 22:24:43 - train: epoch 0063, iter [04400, 05004], lr: 0.069098, loss: 1.9624
2022-03-11 22:25:25 - train: epoch 0063, iter [04500, 05004], lr: 0.069098, loss: 1.9280
2022-03-11 22:26:05 - train: epoch 0063, iter [04600, 05004], lr: 0.069098, loss: 1.8543
2022-03-11 22:26:44 - train: epoch 0063, iter [04700, 05004], lr: 0.069098, loss: 1.9712
2022-03-11 22:27:27 - train: epoch 0063, iter [04800, 05004], lr: 0.069098, loss: 1.8851
2022-03-11 22:28:11 - train: epoch 0063, iter [04900, 05004], lr: 0.069098, loss: 1.8941
2022-03-11 22:28:54 - train: epoch 0063, iter [05000, 05004], lr: 0.069098, loss: 2.2955
2022-03-11 22:28:55 - train: epoch 063, train_loss: 2.0029
2022-03-11 22:30:26 - eval: epoch: 063, acc1: 57.526%, acc5: 81.272%, test_loss: 1.7898, per_image_load_time: 3.181ms, per_image_inference_time: 0.276ms
2022-03-11 22:30:26 - until epoch: 063, best_acc1: 58.010%
2022-03-11 22:30:26 - epoch 064 lr: 0.0659706825558357
2022-03-11 22:31:13 - train: epoch 0064, iter [00100, 05004], lr: 0.065971, loss: 1.8633
2022-03-11 22:31:57 - train: epoch 0064, iter [00200, 05004], lr: 0.065971, loss: 1.9373
2022-03-11 22:32:41 - train: epoch 0064, iter [00300, 05004], lr: 0.065971, loss: 1.8470
2022-03-11 22:33:26 - train: epoch 0064, iter [00400, 05004], lr: 0.065971, loss: 1.8850
2022-03-11 22:34:12 - train: epoch 0064, iter [00500, 05004], lr: 0.065971, loss: 1.7064
2022-03-11 22:34:57 - train: epoch 0064, iter [00600, 05004], lr: 0.065971, loss: 2.1179
2022-03-11 22:35:41 - train: epoch 0064, iter [00700, 05004], lr: 0.065971, loss: 2.0458
2022-03-11 22:36:25 - train: epoch 0064, iter [00800, 05004], lr: 0.065971, loss: 2.0198
2022-03-11 22:37:08 - train: epoch 0064, iter [00900, 05004], lr: 0.065971, loss: 1.8414
2022-03-11 22:37:51 - train: epoch 0064, iter [01000, 05004], lr: 0.065971, loss: 1.9595
2022-03-11 22:38:36 - train: epoch 0064, iter [01100, 05004], lr: 0.065971, loss: 1.9021
2022-03-11 22:39:18 - train: epoch 0064, iter [01200, 05004], lr: 0.065971, loss: 1.7492
2022-03-11 22:40:04 - train: epoch 0064, iter [01300, 05004], lr: 0.065971, loss: 2.0417
2022-03-11 22:40:48 - train: epoch 0064, iter [01400, 05004], lr: 0.065971, loss: 2.1900
2022-03-11 22:41:32 - train: epoch 0064, iter [01500, 05004], lr: 0.065971, loss: 2.0999
2022-03-11 22:42:14 - train: epoch 0064, iter [01600, 05004], lr: 0.065971, loss: 1.7730
2022-03-11 22:42:57 - train: epoch 0064, iter [01700, 05004], lr: 0.065971, loss: 1.7614
2022-03-11 22:43:40 - train: epoch 0064, iter [01800, 05004], lr: 0.065971, loss: 1.7679
2022-03-11 22:44:21 - train: epoch 0064, iter [01900, 05004], lr: 0.065971, loss: 1.9365
2022-03-11 22:45:04 - train: epoch 0064, iter [02000, 05004], lr: 0.065971, loss: 2.1023
2022-03-11 22:45:47 - train: epoch 0064, iter [02100, 05004], lr: 0.065971, loss: 1.9970
2022-03-11 22:46:29 - train: epoch 0064, iter [02200, 05004], lr: 0.065971, loss: 2.1542
2022-03-11 22:47:11 - train: epoch 0064, iter [02300, 05004], lr: 0.065971, loss: 2.0861
2022-03-11 22:47:53 - train: epoch 0064, iter [02400, 05004], lr: 0.065971, loss: 1.7849
2022-03-11 22:48:37 - train: epoch 0064, iter [02500, 05004], lr: 0.065971, loss: 1.8894
2022-03-11 22:49:22 - train: epoch 0064, iter [02600, 05004], lr: 0.065971, loss: 1.9348
2022-03-11 22:50:06 - train: epoch 0064, iter [02700, 05004], lr: 0.065971, loss: 1.8961
2022-03-11 22:50:50 - train: epoch 0064, iter [02800, 05004], lr: 0.065971, loss: 2.0002
2022-03-11 22:51:34 - train: epoch 0064, iter [02900, 05004], lr: 0.065971, loss: 2.0437
2022-03-11 22:52:21 - train: epoch 0064, iter [03000, 05004], lr: 0.065971, loss: 1.9071
2022-03-11 22:53:06 - train: epoch 0064, iter [03100, 05004], lr: 0.065971, loss: 2.1948
2022-03-11 22:53:52 - train: epoch 0064, iter [03200, 05004], lr: 0.065971, loss: 1.9266
2022-03-11 22:54:36 - train: epoch 0064, iter [03300, 05004], lr: 0.065971, loss: 2.0382
2022-03-11 22:55:21 - train: epoch 0064, iter [03400, 05004], lr: 0.065971, loss: 2.1523
2022-03-11 22:56:04 - train: epoch 0064, iter [03500, 05004], lr: 0.065971, loss: 1.9339
2022-03-11 22:56:48 - train: epoch 0064, iter [03600, 05004], lr: 0.065971, loss: 1.9344
2022-03-11 22:57:31 - train: epoch 0064, iter [03700, 05004], lr: 0.065971, loss: 1.6567
