2022-03-11 22:58:15 - train: epoch 0064, iter [03800, 05004], lr: 0.065971, loss: 2.1381
2022-03-11 22:58:57 - train: epoch 0064, iter [03900, 05004], lr: 0.065971, loss: 1.8937
2022-03-11 22:59:41 - train: epoch 0064, iter [04000, 05004], lr: 0.065971, loss: 1.9451
2022-03-11 23:00:24 - train: epoch 0064, iter [04100, 05004], lr: 0.065971, loss: 1.8627
2022-03-11 23:01:09 - train: epoch 0064, iter [04200, 05004], lr: 0.065971, loss: 1.8464
2022-03-11 23:01:53 - train: epoch 0064, iter [04300, 05004], lr: 0.065971, loss: 2.0881
2022-03-11 23:02:37 - train: epoch 0064, iter [04400, 05004], lr: 0.065971, loss: 1.7752
2022-03-11 23:03:20 - train: epoch 0064, iter [04500, 05004], lr: 0.065971, loss: 1.9102
2022-03-11 23:04:03 - train: epoch 0064, iter [04600, 05004], lr: 0.065971, loss: 2.0842
2022-03-11 23:04:46 - train: epoch 0064, iter [04700, 05004], lr: 0.065971, loss: 2.2450
2022-03-11 23:05:29 - train: epoch 0064, iter [04800, 05004], lr: 0.065971, loss: 2.1252
2022-03-11 23:06:12 - train: epoch 0064, iter [04900, 05004], lr: 0.065971, loss: 2.0233
2022-03-11 23:06:53 - train: epoch 0064, iter [05000, 05004], lr: 0.065971, loss: 1.7239
2022-03-11 23:06:55 - train: epoch 064, train_loss: 1.9862
2022-03-11 23:08:24 - eval: epoch: 064, acc1: 58.786%, acc5: 82.356%, test_loss: 1.7236, per_image_load_time: 3.163ms, per_image_inference_time: 0.269ms
2022-03-11 23:08:24 - until epoch: 064, best_acc1: 58.786%
2022-03-11 23:08:24 - epoch 065 lr: 0.06288027510817791
2022-03-11 23:09:11 - train: epoch 0065, iter [00100, 05004], lr: 0.062880, loss: 1.8671
2022-03-11 23:09:54 - train: epoch 0065, iter [00200, 05004], lr: 0.062880, loss: 1.9789
2022-03-11 23:10:37 - train: epoch 0065, iter [00300, 05004], lr: 0.062880, loss: 1.9985
2022-03-11 23:11:21 - train: epoch 0065, iter [00400, 05004], lr: 0.062880, loss: 1.9332
2022-03-11 23:12:03 - train: epoch 0065, iter [00500, 05004], lr: 0.062880, loss: 1.9582
2022-03-11 23:12:48 - train: epoch 0065, iter [00600, 05004], lr: 0.062880, loss: 2.0571
2022-03-11 23:13:31 - train: epoch 0065, iter [00700, 05004], lr: 0.062880, loss: 2.0630
2022-03-11 23:14:12 - train: epoch 0065, iter [00800, 05004], lr: 0.062880, loss: 1.6500
2022-03-11 23:14:52 - train: epoch 0065, iter [00900, 05004], lr: 0.062880, loss: 1.8625
2022-03-11 23:15:35 - train: epoch 0065, iter [01000, 05004], lr: 0.062880, loss: 1.9819
2022-03-11 23:16:17 - train: epoch 0065, iter [01100, 05004], lr: 0.062880, loss: 1.8478
2022-03-11 23:16:59 - train: epoch 0065, iter [01200, 05004], lr: 0.062880, loss: 2.1655
2022-03-11 23:17:41 - train: epoch 0065, iter [01300, 05004], lr: 0.062880, loss: 2.0275
2022-03-11 23:18:22 - train: epoch 0065, iter [01400, 05004], lr: 0.062880, loss: 1.8493
2022-03-11 23:19:03 - train: epoch 0065, iter [01500, 05004], lr: 0.062880, loss: 2.0048
2022-03-11 23:19:45 - train: epoch 0065, iter [01600, 05004], lr: 0.062880, loss: 2.3611
2022-03-11 23:20:28 - train: epoch 0065, iter [01700, 05004], lr: 0.062880, loss: 1.8030
2022-03-11 23:21:10 - train: epoch 0065, iter [01800, 05004], lr: 0.062880, loss: 2.0235
2022-03-11 23:21:52 - train: epoch 0065, iter [01900, 05004], lr: 0.062880, loss: 1.6787
2022-03-11 23:22:34 - train: epoch 0065, iter [02000, 05004], lr: 0.062880, loss: 1.9037
2022-03-11 23:23:16 - train: epoch 0065, iter [02100, 05004], lr: 0.062880, loss: 2.0178
2022-03-11 23:23:58 - train: epoch 0065, iter [02200, 05004], lr: 0.062880, loss: 1.8674
2022-03-11 23:24:40 - train: epoch 0065, iter [02300, 05004], lr: 0.062880, loss: 1.9957
2022-03-11 23:25:22 - train: epoch 0065, iter [02400, 05004], lr: 0.062880, loss: 2.0590
2022-03-11 23:26:05 - train: epoch 0065, iter [02500, 05004], lr: 0.062880, loss: 1.8741
2022-03-11 23:26:49 - train: epoch 0065, iter [02600, 05004], lr: 0.062880, loss: 2.1673
2022-03-11 23:27:31 - train: epoch 0065, iter [02700, 05004], lr: 0.062880, loss: 1.9733
2022-03-11 23:28:12 - train: epoch 0065, iter [02800, 05004], lr: 0.062880, loss: 2.0633
2022-03-11 23:28:53 - train: epoch 0065, iter [02900, 05004], lr: 0.062880, loss: 1.8473
2022-03-11 23:29:35 - train: epoch 0065, iter [03000, 05004], lr: 0.062880, loss: 1.8701
2022-03-11 23:30:18 - train: epoch 0065, iter [03100, 05004], lr: 0.062880, loss: 1.9810
2022-03-11 23:31:00 - train: epoch 0065, iter [03200, 05004], lr: 0.062880, loss: 2.1414
2022-03-11 23:31:41 - train: epoch 0065, iter [03300, 05004], lr: 0.062880, loss: 1.8924
2022-03-11 23:32:23 - train: epoch 0065, iter [03400, 05004], lr: 0.062880, loss: 2.0487
2022-03-11 23:33:04 - train: epoch 0065, iter [03500, 05004], lr: 0.062880, loss: 2.1421
2022-03-11 23:33:45 - train: epoch 0065, iter [03600, 05004], lr: 0.062880, loss: 1.9937
2022-03-11 23:34:28 - train: epoch 0065, iter [03700, 05004], lr: 0.062880, loss: 1.9696
2022-03-11 23:35:10 - train: epoch 0065, iter [03800, 05004], lr: 0.062880, loss: 1.8504
2022-03-11 23:35:52 - train: epoch 0065, iter [03900, 05004], lr: 0.062880, loss: 2.3775
2022-03-11 23:36:34 - train: epoch 0065, iter [04000, 05004], lr: 0.062880, loss: 1.9529
2022-03-11 23:37:15 - train: epoch 0065, iter [04100, 05004], lr: 0.062880, loss: 1.9207
2022-03-11 23:37:56 - train: epoch 0065, iter [04200, 05004], lr: 0.062880, loss: 1.8648
2022-03-11 23:38:38 - train: epoch 0065, iter [04300, 05004], lr: 0.062880, loss: 2.1293
2022-03-11 23:39:18 - train: epoch 0065, iter [04400, 05004], lr: 0.062880, loss: 2.1916
2022-03-11 23:39:58 - train: epoch 0065, iter [04500, 05004], lr: 0.062880, loss: 1.9527
2022-03-11 23:40:40 - train: epoch 0065, iter [04600, 05004], lr: 0.062880, loss: 1.9499
2022-03-11 23:41:21 - train: epoch 0065, iter [04700, 05004], lr: 0.062880, loss: 1.8804
2022-03-11 23:42:03 - train: epoch 0065, iter [04800, 05004], lr: 0.062880, loss: 1.7614
2022-03-11 23:42:44 - train: epoch 0065, iter [04900, 05004], lr: 0.062880, loss: 1.9147
2022-03-11 23:43:24 - train: epoch 0065, iter [05000, 05004], lr: 0.062880, loss: 2.0806
2022-03-11 23:43:25 - train: epoch 065, train_loss: 1.9681
2022-03-11 23:44:55 - eval: epoch: 065, acc1: 59.634%, acc5: 82.910%, test_loss: 1.6877, per_image_load_time: 3.206ms, per_image_inference_time: 0.266ms
2022-03-11 23:44:55 - until epoch: 065, best_acc1: 59.634%
2022-03-11 23:44:55 - epoch 066 lr: 0.05983045753470308
2022-03-11 23:45:42 - train: epoch 0066, iter [00100, 05004], lr: 0.059830, loss: 1.9040
2022-03-11 23:46:23 - train: epoch 0066, iter [00200, 05004], lr: 0.059830, loss: 2.2666
2022-03-11 23:47:05 - train: epoch 0066, iter [00300, 05004], lr: 0.059830, loss: 1.8848
2022-03-11 23:47:47 - train: epoch 0066, iter [00400, 05004], lr: 0.059830, loss: 1.7783
2022-03-11 23:48:29 - train: epoch 0066, iter [00500, 05004], lr: 0.059830, loss: 1.9620
2022-03-11 23:49:11 - train: epoch 0066, iter [00600, 05004], lr: 0.059830, loss: 1.9345
2022-03-11 23:49:52 - train: epoch 0066, iter [00700, 05004], lr: 0.059830, loss: 2.0563
2022-03-11 23:50:33 - train: epoch 0066, iter [00800, 05004], lr: 0.059830, loss: 2.1883
2022-03-11 23:51:15 - train: epoch 0066, iter [00900, 05004], lr: 0.059830, loss: 1.8244
2022-03-11 23:51:57 - train: epoch 0066, iter [01000, 05004], lr: 0.059830, loss: 1.9937
2022-03-11 23:52:40 - train: epoch 0066, iter [01100, 05004], lr: 0.059830, loss: 2.0402
2022-03-11 23:53:22 - train: epoch 0066, iter [01200, 05004], lr: 0.059830, loss: 1.9860
2022-03-11 23:54:03 - train: epoch 0066, iter [01300, 05004], lr: 0.059830, loss: 1.8913
2022-03-11 23:54:45 - train: epoch 0066, iter [01400, 05004], lr: 0.059830, loss: 1.8753
2022-03-11 23:55:27 - train: epoch 0066, iter [01500, 05004], lr: 0.059830, loss: 1.9604
2022-03-11 23:56:10 - train: epoch 0066, iter [01600, 05004], lr: 0.059830, loss: 2.0391
2022-03-11 23:56:54 - train: epoch 0066, iter [01700, 05004], lr: 0.059830, loss: 1.7800
2022-03-11 23:57:35 - train: epoch 0066, iter [01800, 05004], lr: 0.059830, loss: 1.7512
2022-03-11 23:58:17 - train: epoch 0066, iter [01900, 05004], lr: 0.059830, loss: 2.2049
2022-03-11 23:58:59 - train: epoch 0066, iter [02000, 05004], lr: 0.059830, loss: 1.9791
2022-03-11 23:59:41 - train: epoch 0066, iter [02100, 05004], lr: 0.059830, loss: 2.1409
2022-03-12 00:00:24 - train: epoch 0066, iter [02200, 05004], lr: 0.059830, loss: 2.1075
2022-03-12 00:01:06 - train: epoch 0066, iter [02300, 05004], lr: 0.059830, loss: 2.0443
2022-03-12 00:01:48 - train: epoch 0066, iter [02400, 05004], lr: 0.059830, loss: 1.8303
2022-03-12 00:02:30 - train: epoch 0066, iter [02500, 05004], lr: 0.059830, loss: 1.8802
2022-03-12 00:03:11 - train: epoch 0066, iter [02600, 05004], lr: 0.059830, loss: 1.9396
2022-03-12 00:03:53 - train: epoch 0066, iter [02700, 05004], lr: 0.059830, loss: 2.0126
2022-03-12 00:04:33 - train: epoch 0066, iter [02800, 05004], lr: 0.059830, loss: 1.9938
2022-03-12 00:05:17 - train: epoch 0066, iter [02900, 05004], lr: 0.059830, loss: 2.0513
2022-03-12 00:06:01 - train: epoch 0066, iter [03000, 05004], lr: 0.059830, loss: 1.9052
2022-03-12 00:06:44 - train: epoch 0066, iter [03100, 05004], lr: 0.059830, loss: 2.0443
2022-03-12 00:07:27 - train: epoch 0066, iter [03200, 05004], lr: 0.059830, loss: 1.8679
2022-03-12 00:08:11 - train: epoch 0066, iter [03300, 05004], lr: 0.059830, loss: 1.7712
2022-03-12 00:08:56 - train: epoch 0066, iter [03400, 05004], lr: 0.059830, loss: 2.1203
2022-03-12 00:09:40 - train: epoch 0066, iter [03500, 05004], lr: 0.059830, loss: 2.2141
2022-03-12 00:10:24 - train: epoch 0066, iter [03600, 05004], lr: 0.059830, loss: 1.8951
2022-03-12 00:11:08 - train: epoch 0066, iter [03700, 05004], lr: 0.059830, loss: 1.9376
2022-03-12 00:11:53 - train: epoch 0066, iter [03800, 05004], lr: 0.059830, loss: 1.9990
2022-03-12 00:12:37 - train: epoch 0066, iter [03900, 05004], lr: 0.059830, loss: 1.6581
2022-03-12 00:13:21 - train: epoch 0066, iter [04000, 05004], lr: 0.059830, loss: 1.8762
2022-03-12 00:14:05 - train: epoch 0066, iter [04100, 05004], lr: 0.059830, loss: 2.0938
2022-03-12 00:14:48 - train: epoch 0066, iter [04200, 05004], lr: 0.059830, loss: 1.7726
2022-03-12 00:15:32 - train: epoch 0066, iter [04300, 05004], lr: 0.059830, loss: 1.6793
2022-03-12 00:16:14 - train: epoch 0066, iter [04400, 05004], lr: 0.059830, loss: 1.8938
2022-03-12 00:16:57 - train: epoch 0066, iter [04500, 05004], lr: 0.059830, loss: 2.2233
2022-03-12 00:17:39 - train: epoch 0066, iter [04600, 05004], lr: 0.059830, loss: 2.1595
2022-03-12 00:18:23 - train: epoch 0066, iter [04700, 05004], lr: 0.059830, loss: 1.8323
2022-03-12 00:19:06 - train: epoch 0066, iter [04800, 05004], lr: 0.059830, loss: 2.1365
2022-03-12 00:19:47 - train: epoch 0066, iter [04900, 05004], lr: 0.059830, loss: 2.1001
2022-03-12 00:20:30 - train: epoch 0066, iter [05000, 05004], lr: 0.059830, loss: 1.9377
2022-03-12 00:20:32 - train: epoch 066, train_loss: 1.9536
2022-03-12 00:22:01 - eval: epoch: 066, acc1: 59.036%, acc5: 82.776%, test_loss: 1.7053, per_image_load_time: 3.188ms, per_image_inference_time: 0.259ms
2022-03-12 00:22:01 - until epoch: 066, best_acc1: 59.634%
2022-03-12 00:22:01 - epoch 067 lr: 0.05682456476615072
2022-03-12 00:22:47 - train: epoch 0067, iter [00100, 05004], lr: 0.056825, loss: 1.8540
2022-03-12 00:23:31 - train: epoch 0067, iter [00200, 05004], lr: 0.056825, loss: 2.0821
2022-03-12 00:24:14 - train: epoch 0067, iter [00300, 05004], lr: 0.056825, loss: 2.0452
2022-03-12 00:24:57 - train: epoch 0067, iter [00400, 05004], lr: 0.056825, loss: 1.7774
2022-03-12 00:25:40 - train: epoch 0067, iter [00500, 05004], lr: 0.056825, loss: 1.9745
2022-03-12 00:26:25 - train: epoch 0067, iter [00600, 05004], lr: 0.056825, loss: 1.7449
2022-03-12 00:27:08 - train: epoch 0067, iter [00700, 05004], lr: 0.056825, loss: 2.0269
2022-03-12 00:27:50 - train: epoch 0067, iter [00800, 05004], lr: 0.056825, loss: 1.9065
2022-03-12 00:28:32 - train: epoch 0067, iter [00900, 05004], lr: 0.056825, loss: 2.1349
2022-03-12 00:29:13 - train: epoch 0067, iter [01000, 05004], lr: 0.056825, loss: 1.8228
2022-03-12 00:29:54 - train: epoch 0067, iter [01100, 05004], lr: 0.056825, loss: 1.9359
2022-03-12 00:30:37 - train: epoch 0067, iter [01200, 05004], lr: 0.056825, loss: 1.8935
2022-03-12 00:31:18 - train: epoch 0067, iter [01300, 05004], lr: 0.056825, loss: 1.9605
2022-03-12 00:31:59 - train: epoch 0067, iter [01400, 05004], lr: 0.056825, loss: 2.0296
2022-03-12 00:32:40 - train: epoch 0067, iter [01500, 05004], lr: 0.056825, loss: 1.7896
2022-03-12 00:33:21 - train: epoch 0067, iter [01600, 05004], lr: 0.056825, loss: 2.0071
2022-03-12 00:34:04 - train: epoch 0067, iter [01700, 05004], lr: 0.056825, loss: 1.6678
2022-03-12 00:34:46 - train: epoch 0067, iter [01800, 05004], lr: 0.056825, loss: 1.8082
2022-03-12 00:35:29 - train: epoch 0067, iter [01900, 05004], lr: 0.056825, loss: 2.0208
2022-03-12 00:36:11 - train: epoch 0067, iter [02000, 05004], lr: 0.056825, loss: 1.9149
2022-03-12 00:36:54 - train: epoch 0067, iter [02100, 05004], lr: 0.056825, loss: 1.5806
2022-03-12 00:37:37 - train: epoch 0067, iter [02200, 05004], lr: 0.056825, loss: 1.7478
2022-03-12 00:38:18 - train: epoch 0067, iter [02300, 05004], lr: 0.056825, loss: 2.0485
2022-03-12 00:39:01 - train: epoch 0067, iter [02400, 05004], lr: 0.056825, loss: 1.8358
2022-03-12 00:39:42 - train: epoch 0067, iter [02500, 05004], lr: 0.056825, loss: 1.8471
2022-03-12 00:40:24 - train: epoch 0067, iter [02600, 05004], lr: 0.056825, loss: 1.9797
2022-03-12 00:41:06 - train: epoch 0067, iter [02700, 05004], lr: 0.056825, loss: 2.0380
2022-03-12 00:41:48 - train: epoch 0067, iter [02800, 05004], lr: 0.056825, loss: 2.2172
2022-03-12 00:42:31 - train: epoch 0067, iter [02900, 05004], lr: 0.056825, loss: 1.8752
2022-03-12 00:43:13 - train: epoch 0067, iter [03000, 05004], lr: 0.056825, loss: 1.9425
2022-03-12 00:43:54 - train: epoch 0067, iter [03100, 05004], lr: 0.056825, loss: 1.7056
2022-03-12 00:44:36 - train: epoch 0067, iter [03200, 05004], lr: 0.056825, loss: 1.9959
2022-03-12 00:45:19 - train: epoch 0067, iter [03300, 05004], lr: 0.056825, loss: 1.8475
2022-03-12 00:46:01 - train: epoch 0067, iter [03400, 05004], lr: 0.056825, loss: 1.8407
2022-03-12 00:46:44 - train: epoch 0067, iter [03500, 05004], lr: 0.056825, loss: 2.0496
2022-03-12 00:47:27 - train: epoch 0067, iter [03600, 05004], lr: 0.056825, loss: 1.7940
2022-03-12 00:48:06 - train: epoch 0067, iter [03700, 05004], lr: 0.056825, loss: 1.9677
2022-03-12 00:48:49 - train: epoch 0067, iter [03800, 05004], lr: 0.056825, loss: 2.0840
2022-03-12 00:49:32 - train: epoch 0067, iter [03900, 05004], lr: 0.056825, loss: 2.1235
2022-03-12 00:50:14 - train: epoch 0067, iter [04000, 05004], lr: 0.056825, loss: 1.9418
2022-03-12 00:50:57 - train: epoch 0067, iter [04100, 05004], lr: 0.056825, loss: 1.8300
2022-03-12 00:51:38 - train: epoch 0067, iter [04200, 05004], lr: 0.056825, loss: 1.9162
2022-03-12 00:52:21 - train: epoch 0067, iter [04300, 05004], lr: 0.056825, loss: 1.7837
2022-03-12 00:53:02 - train: epoch 0067, iter [04400, 05004], lr: 0.056825, loss: 1.9052
2022-03-12 00:53:44 - train: epoch 0067, iter [04500, 05004], lr: 0.056825, loss: 1.7350
2022-03-12 00:54:25 - train: epoch 0067, iter [04600, 05004], lr: 0.056825, loss: 1.7501
2022-03-12 00:55:06 - train: epoch 0067, iter [04700, 05004], lr: 0.056825, loss: 2.1151
2022-03-12 00:55:48 - train: epoch 0067, iter [04800, 05004], lr: 0.056825, loss: 1.9997
2022-03-12 00:56:28 - train: epoch 0067, iter [04900, 05004], lr: 0.056825, loss: 1.7812
2022-03-12 00:57:09 - train: epoch 0067, iter [05000, 05004], lr: 0.056825, loss: 1.9646
2022-03-12 00:57:11 - train: epoch 067, train_loss: 1.9352
2022-03-12 00:58:39 - eval: epoch: 067, acc1: 58.988%, acc5: 82.824%, test_loss: 1.6991, per_image_load_time: 1.579ms, per_image_inference_time: 0.271ms
2022-03-12 00:58:39 - until epoch: 067, best_acc1: 59.634%
2022-03-12 00:58:39 - epoch 068 lr: 0.05386588370213124
2022-03-12 00:59:25 - train: epoch 0068, iter [00100, 05004], lr: 0.053866, loss: 1.9374
2022-03-12 01:00:06 - train: epoch 0068, iter [00200, 05004], lr: 0.053866, loss: 1.8151
2022-03-12 01:00:48 - train: epoch 0068, iter [00300, 05004], lr: 0.053866, loss: 1.8846
2022-03-12 01:01:29 - train: epoch 0068, iter [00400, 05004], lr: 0.053866, loss: 1.8518
2022-03-12 01:02:12 - train: epoch 0068, iter [00500, 05004], lr: 0.053866, loss: 1.9162
2022-03-12 01:02:53 - train: epoch 0068, iter [00600, 05004], lr: 0.053866, loss: 2.0430
2022-03-12 01:03:36 - train: epoch 0068, iter [00700, 05004], lr: 0.053866, loss: 1.9733
2022-03-12 01:04:19 - train: epoch 0068, iter [00800, 05004], lr: 0.053866, loss: 1.8242
2022-03-12 01:05:00 - train: epoch 0068, iter [00900, 05004], lr: 0.053866, loss: 1.7471
2022-03-12 01:05:43 - train: epoch 0068, iter [01000, 05004], lr: 0.053866, loss: 1.7525
2022-03-12 01:06:25 - train: epoch 0068, iter [01100, 05004], lr: 0.053866, loss: 2.1915
2022-03-12 01:07:05 - train: epoch 0068, iter [01200, 05004], lr: 0.053866, loss: 1.8049
2022-03-12 01:07:47 - train: epoch 0068, iter [01300, 05004], lr: 0.053866, loss: 1.9010
2022-03-12 01:08:29 - train: epoch 0068, iter [01400, 05004], lr: 0.053866, loss: 1.8380
2022-03-12 01:09:12 - train: epoch 0068, iter [01500, 05004], lr: 0.053866, loss: 2.1797
2022-03-12 01:09:53 - train: epoch 0068, iter [01600, 05004], lr: 0.053866, loss: 1.9565
2022-03-12 01:10:36 - train: epoch 0068, iter [01700, 05004], lr: 0.053866, loss: 1.7859
2022-03-12 01:11:18 - train: epoch 0068, iter [01800, 05004], lr: 0.053866, loss: 1.9644
2022-03-12 01:11:59 - train: epoch 0068, iter [01900, 05004], lr: 0.053866, loss: 1.9052
2022-03-12 01:12:44 - train: epoch 0068, iter [02000, 05004], lr: 0.053866, loss: 2.2547
2022-03-12 01:13:25 - train: epoch 0068, iter [02100, 05004], lr: 0.053866, loss: 1.7543
2022-03-12 01:14:07 - train: epoch 0068, iter [02200, 05004], lr: 0.053866, loss: 1.6621
2022-03-12 01:14:49 - train: epoch 0068, iter [02300, 05004], lr: 0.053866, loss: 1.9950
2022-03-12 01:15:31 - train: epoch 0068, iter [02400, 05004], lr: 0.053866, loss: 1.8896
2022-03-12 01:16:11 - train: epoch 0068, iter [02500, 05004], lr: 0.053866, loss: 1.8927
2022-03-12 01:16:54 - train: epoch 0068, iter [02600, 05004], lr: 0.053866, loss: 1.8327
2022-03-12 01:17:34 - train: epoch 0068, iter [02700, 05004], lr: 0.053866, loss: 1.7277
2022-03-12 01:18:16 - train: epoch 0068, iter [02800, 05004], lr: 0.053866, loss: 2.0511
2022-03-12 01:18:58 - train: epoch 0068, iter [02900, 05004], lr: 0.053866, loss: 1.8838
2022-03-12 01:19:41 - train: epoch 0068, iter [03000, 05004], lr: 0.053866, loss: 2.0538
2022-03-12 01:20:23 - train: epoch 0068, iter [03100, 05004], lr: 0.053866, loss: 1.5931
2022-03-12 01:21:06 - train: epoch 0068, iter [03200, 05004], lr: 0.053866, loss: 1.9266
2022-03-12 01:21:50 - train: epoch 0068, iter [03300, 05004], lr: 0.053866, loss: 2.0055
2022-03-12 01:22:34 - train: epoch 0068, iter [03400, 05004], lr: 0.053866, loss: 1.9458
2022-03-12 01:23:17 - train: epoch 0068, iter [03500, 05004], lr: 0.053866, loss: 1.9210
2022-03-12 01:24:00 - train: epoch 0068, iter [03600, 05004], lr: 0.053866, loss: 2.1327
2022-03-12 01:24:45 - train: epoch 0068, iter [03700, 05004], lr: 0.053866, loss: 1.9261
2022-03-12 01:25:29 - train: epoch 0068, iter [03800, 05004], lr: 0.053866, loss: 1.9160
2022-03-12 01:26:12 - train: epoch 0068, iter [03900, 05004], lr: 0.053866, loss: 1.9883
2022-03-12 01:26:56 - train: epoch 0068, iter [04000, 05004], lr: 0.053866, loss: 1.9404
2022-03-12 01:27:40 - train: epoch 0068, iter [04100, 05004], lr: 0.053866, loss: 1.7838
2022-03-12 01:28:25 - train: epoch 0068, iter [04200, 05004], lr: 0.053866, loss: 1.9395
2022-03-12 01:29:08 - train: epoch 0068, iter [04300, 05004], lr: 0.053866, loss: 1.8999
2022-03-12 01:29:52 - train: epoch 0068, iter [04400, 05004], lr: 0.053866, loss: 1.8336
2022-03-12 01:30:35 - train: epoch 0068, iter [04500, 05004], lr: 0.053866, loss: 1.8605
2022-03-12 01:31:19 - train: epoch 0068, iter [04600, 05004], lr: 0.053866, loss: 2.0131
2022-03-12 01:32:01 - train: epoch 0068, iter [04700, 05004], lr: 0.053866, loss: 2.1480
2022-03-12 01:32:44 - train: epoch 0068, iter [04800, 05004], lr: 0.053866, loss: 1.8881
2022-03-12 01:33:27 - train: epoch 0068, iter [04900, 05004], lr: 0.053866, loss: 1.9263
2022-03-12 01:34:10 - train: epoch 0068, iter [05000, 05004], lr: 0.053866, loss: 1.9205
2022-03-12 01:34:12 - train: epoch 068, train_loss: 1.9153
2022-03-12 01:35:40 - eval: epoch: 068, acc1: 59.888%, acc5: 83.312%, test_loss: 1.6695, per_image_load_time: 3.208ms, per_image_inference_time: 0.275ms
2022-03-12 01:35:40 - until epoch: 068, best_acc1: 59.888%
2022-03-12 01:35:40 - epoch 069 lr: 0.05095764961694922
2022-03-12 01:36:26 - train: epoch 0069, iter [00100, 05004], lr: 0.050958, loss: 2.1872
2022-03-12 01:37:09 - train: epoch 0069, iter [00200, 05004], lr: 0.050958, loss: 2.0655
2022-03-12 01:37:51 - train: epoch 0069, iter [00300, 05004], lr: 0.050958, loss: 1.8205
2022-03-12 01:38:33 - train: epoch 0069, iter [00400, 05004], lr: 0.050958, loss: 1.7976
2022-03-12 01:39:15 - train: epoch 0069, iter [00500, 05004], lr: 0.050958, loss: 1.7615
2022-03-12 01:39:56 - train: epoch 0069, iter [00600, 05004], lr: 0.050958, loss: 1.7652
2022-03-12 01:40:38 - train: epoch 0069, iter [00700, 05004], lr: 0.050958, loss: 1.9431
2022-03-12 01:41:20 - train: epoch 0069, iter [00800, 05004], lr: 0.050958, loss: 2.1505
2022-03-12 01:42:00 - train: epoch 0069, iter [00900, 05004], lr: 0.050958, loss: 1.7556
2022-03-12 01:42:41 - train: epoch 0069, iter [01000, 05004], lr: 0.050958, loss: 2.1575
2022-03-12 01:43:23 - train: epoch 0069, iter [01100, 05004], lr: 0.050958, loss: 1.8149
2022-03-12 01:44:04 - train: epoch 0069, iter [01200, 05004], lr: 0.050958, loss: 1.5536
2022-03-12 01:44:46 - train: epoch 0069, iter [01300, 05004], lr: 0.050958, loss: 1.8176
2022-03-12 01:45:28 - train: epoch 0069, iter [01400, 05004], lr: 0.050958, loss: 1.7931
2022-03-12 01:46:10 - train: epoch 0069, iter [01500, 05004], lr: 0.050958, loss: 1.9437
2022-03-12 01:46:55 - train: epoch 0069, iter [01600, 05004], lr: 0.050958, loss: 2.0772
2022-03-12 01:47:39 - train: epoch 0069, iter [01700, 05004], lr: 0.050958, loss: 1.8409
2022-03-12 01:48:23 - train: epoch 0069, iter [01800, 05004], lr: 0.050958, loss: 1.9095
2022-03-12 01:49:05 - train: epoch 0069, iter [01900, 05004], lr: 0.050958, loss: 2.0835
2022-03-12 01:49:47 - train: epoch 0069, iter [02000, 05004], lr: 0.050958, loss: 1.7075
2022-03-12 01:50:31 - train: epoch 0069, iter [02100, 05004], lr: 0.050958, loss: 1.9349
2022-03-12 01:51:14 - train: epoch 0069, iter [02200, 05004], lr: 0.050958, loss: 1.9644
2022-03-12 01:51:56 - train: epoch 0069, iter [02300, 05004], lr: 0.050958, loss: 1.9305
2022-03-12 01:52:39 - train: epoch 0069, iter [02400, 05004], lr: 0.050958, loss: 1.9476
2022-03-12 01:53:22 - train: epoch 0069, iter [02500, 05004], lr: 0.050958, loss: 1.6993
2022-03-12 01:54:05 - train: epoch 0069, iter [02600, 05004], lr: 0.050958, loss: 1.8596
2022-03-12 01:54:48 - train: epoch 0069, iter [02700, 05004], lr: 0.050958, loss: 2.1749
2022-03-12 01:55:32 - train: epoch 0069, iter [02800, 05004], lr: 0.050958, loss: 1.8602
2022-03-12 01:56:14 - train: epoch 0069, iter [02900, 05004], lr: 0.050958, loss: 1.7565
2022-03-12 01:56:56 - train: epoch 0069, iter [03000, 05004], lr: 0.050958, loss: 1.9710
2022-03-12 01:57:38 - train: epoch 0069, iter [03100, 05004], lr: 0.050958, loss: 2.0622
2022-03-12 01:58:20 - train: epoch 0069, iter [03200, 05004], lr: 0.050958, loss: 1.9989
2022-03-12 01:59:02 - train: epoch 0069, iter [03300, 05004], lr: 0.050958, loss: 1.6798
2022-03-12 01:59:45 - train: epoch 0069, iter [03400, 05004], lr: 0.050958, loss: 1.8540
2022-03-12 02:00:27 - train: epoch 0069, iter [03500, 05004], lr: 0.050958, loss: 1.8130
2022-03-12 02:01:08 - train: epoch 0069, iter [03600, 05004], lr: 0.050958, loss: 1.5192
2022-03-12 02:01:51 - train: epoch 0069, iter [03700, 05004], lr: 0.050958, loss: 2.0975
2022-03-12 02:02:34 - train: epoch 0069, iter [03800, 05004], lr: 0.050958, loss: 1.7992
2022-03-12 02:03:16 - train: epoch 0069, iter [03900, 05004], lr: 0.050958, loss: 1.9767
2022-03-12 02:03:58 - train: epoch 0069, iter [04000, 05004], lr: 0.050958, loss: 1.9308
2022-03-12 02:04:39 - train: epoch 0069, iter [04100, 05004], lr: 0.050958, loss: 1.8076
2022-03-12 02:05:21 - train: epoch 0069, iter [04200, 05004], lr: 0.050958, loss: 1.8719
2022-03-12 02:06:03 - train: epoch 0069, iter [04300, 05004], lr: 0.050958, loss: 1.9482
2022-03-12 02:06:43 - train: epoch 0069, iter [04400, 05004], lr: 0.050958, loss: 1.8790
2022-03-12 02:07:25 - train: epoch 0069, iter [04500, 05004], lr: 0.050958, loss: 1.9511
2022-03-12 02:08:08 - train: epoch 0069, iter [04600, 05004], lr: 0.050958, loss: 2.1269
2022-03-12 02:08:50 - train: epoch 0069, iter [04700, 05004], lr: 0.050958, loss: 1.7800
2022-03-12 02:09:29 - train: epoch 0069, iter [04800, 05004], lr: 0.050958, loss: 1.9151
2022-03-12 02:10:13 - train: epoch 0069, iter [04900, 05004], lr: 0.050958, loss: 1.8118
2022-03-12 02:10:56 - train: epoch 0069, iter [05000, 05004], lr: 0.050958, loss: 1.9680
2022-03-12 02:10:58 - train: epoch 069, train_loss: 1.8949
2022-03-12 02:12:25 - eval: epoch: 069, acc1: 60.168%, acc5: 83.418%, test_loss: 1.6548, per_image_load_time: 3.123ms, per_image_inference_time: 0.264ms
2022-03-12 02:12:26 - until epoch: 069, best_acc1: 60.168%
2022-03-12 02:12:26 - epoch 070 lr: 0.04810304262187851
2022-03-12 02:13:12 - train: epoch 0070, iter [00100, 05004], lr: 0.048103, loss: 1.8636
2022-03-12 02:13:55 - train: epoch 0070, iter [00200, 05004], lr: 0.048103, loss: 1.8735
2022-03-12 02:14:38 - train: epoch 0070, iter [00300, 05004], lr: 0.048103, loss: 1.7585
2022-03-12 02:15:20 - train: epoch 0070, iter [00400, 05004], lr: 0.048103, loss: 1.6170
2022-03-12 02:16:03 - train: epoch 0070, iter [00500, 05004], lr: 0.048103, loss: 1.8641
2022-03-12 02:16:45 - train: epoch 0070, iter [00600, 05004], lr: 0.048103, loss: 1.8081
2022-03-12 02:17:28 - train: epoch 0070, iter [00700, 05004], lr: 0.048103, loss: 1.7507
2022-03-12 02:18:11 - train: epoch 0070, iter [00800, 05004], lr: 0.048103, loss: 1.8962
2022-03-12 02:18:54 - train: epoch 0070, iter [00900, 05004], lr: 0.048103, loss: 1.8431
2022-03-12 02:19:37 - train: epoch 0070, iter [01000, 05004], lr: 0.048103, loss: 1.8296
2022-03-12 02:20:20 - train: epoch 0070, iter [01100, 05004], lr: 0.048103, loss: 2.0988
2022-03-12 02:21:02 - train: epoch 0070, iter [01200, 05004], lr: 0.048103, loss: 1.7421
2022-03-12 02:21:44 - train: epoch 0070, iter [01300, 05004], lr: 0.048103, loss: 1.7499
2022-03-12 02:22:26 - train: epoch 0070, iter [01400, 05004], lr: 0.048103, loss: 1.9161
2022-03-12 02:23:09 - train: epoch 0070, iter [01500, 05004], lr: 0.048103, loss: 1.6789
2022-03-12 02:23:51 - train: epoch 0070, iter [01600, 05004], lr: 0.048103, loss: 1.8975
2022-03-12 02:24:34 - train: epoch 0070, iter [01700, 05004], lr: 0.048103, loss: 1.9342
2022-03-12 02:25:16 - train: epoch 0070, iter [01800, 05004], lr: 0.048103, loss: 1.6443
2022-03-12 02:25:58 - train: epoch 0070, iter [01900, 05004], lr: 0.048103, loss: 1.6510
2022-03-12 02:26:39 - train: epoch 0070, iter [02000, 05004], lr: 0.048103, loss: 1.9541
2022-03-12 02:27:22 - train: epoch 0070, iter [02100, 05004], lr: 0.048103, loss: 2.0247
2022-03-12 02:28:03 - train: epoch 0070, iter [02200, 05004], lr: 0.048103, loss: 1.9024
2022-03-12 02:28:46 - train: epoch 0070, iter [02300, 05004], lr: 0.048103, loss: 1.9112
2022-03-12 02:29:28 - train: epoch 0070, iter [02400, 05004], lr: 0.048103, loss: 1.9796
2022-03-12 02:30:10 - train: epoch 0070, iter [02500, 05004], lr: 0.048103, loss: 1.7117
2022-03-12 02:30:50 - train: epoch 0070, iter [02600, 05004], lr: 0.048103, loss: 1.8631
2022-03-12 02:31:31 - train: epoch 0070, iter [02700, 05004], lr: 0.048103, loss: 1.8541
2022-03-12 02:32:13 - train: epoch 0070, iter [02800, 05004], lr: 0.048103, loss: 1.9161
2022-03-12 02:32:55 - train: epoch 0070, iter [02900, 05004], lr: 0.048103, loss: 1.8954
2022-03-12 02:33:37 - train: epoch 0070, iter [03000, 05004], lr: 0.048103, loss: 1.9205
2022-03-12 02:34:18 - train: epoch 0070, iter [03100, 05004], lr: 0.048103, loss: 1.8914
2022-03-12 02:35:00 - train: epoch 0070, iter [03200, 05004], lr: 0.048103, loss: 2.1143
2022-03-12 02:35:44 - train: epoch 0070, iter [03300, 05004], lr: 0.048103, loss: 1.9323
2022-03-12 02:36:26 - train: epoch 0070, iter [03400, 05004], lr: 0.048103, loss: 1.9357
2022-03-12 02:37:08 - train: epoch 0070, iter [03500, 05004], lr: 0.048103, loss: 1.8865
2022-03-12 02:37:52 - train: epoch 0070, iter [03600, 05004], lr: 0.048103, loss: 1.8627
2022-03-12 02:38:34 - train: epoch 0070, iter [03700, 05004], lr: 0.048103, loss: 1.8551
2022-03-12 02:39:17 - train: epoch 0070, iter [03800, 05004], lr: 0.048103, loss: 1.8993
2022-03-12 02:39:58 - train: epoch 0070, iter [03900, 05004], lr: 0.048103, loss: 1.8417
2022-03-12 02:40:39 - train: epoch 0070, iter [04000, 05004], lr: 0.048103, loss: 1.9152
2022-03-12 02:41:21 - train: epoch 0070, iter [04100, 05004], lr: 0.048103, loss: 1.8920
2022-03-12 02:42:03 - train: epoch 0070, iter [04200, 05004], lr: 0.048103, loss: 1.8400
2022-03-12 02:42:45 - train: epoch 0070, iter [04300, 05004], lr: 0.048103, loss: 1.9231
2022-03-12 02:43:27 - train: epoch 0070, iter [04400, 05004], lr: 0.048103, loss: 1.9311
2022-03-12 02:44:10 - train: epoch 0070, iter [04500, 05004], lr: 0.048103, loss: 1.9977
2022-03-12 02:44:53 - train: epoch 0070, iter [04600, 05004], lr: 0.048103, loss: 2.0165
2022-03-12 02:45:34 - train: epoch 0070, iter [04700, 05004], lr: 0.048103, loss: 2.0226
2022-03-12 02:46:17 - train: epoch 0070, iter [04800, 05004], lr: 0.048103, loss: 1.9501
2022-03-12 02:46:58 - train: epoch 0070, iter [04900, 05004], lr: 0.048103, loss: 1.9127
2022-03-12 02:47:40 - train: epoch 0070, iter [05000, 05004], lr: 0.048103, loss: 1.9164
2022-03-12 02:47:42 - train: epoch 070, train_loss: 1.8775
2022-03-12 02:49:10 - eval: epoch: 070, acc1: 61.452%, acc5: 84.290%, test_loss: 1.5944, per_image_load_time: 2.968ms, per_image_inference_time: 0.262ms
2022-03-12 02:49:10 - until epoch: 070, best_acc1: 61.452%
2022-03-12 02:49:10 - epoch 071 lr: 0.045305184187757336
2022-03-12 02:49:57 - train: epoch 0071, iter [00100, 05004], lr: 0.045305, loss: 1.7820
2022-03-12 02:50:39 - train: epoch 0071, iter [00200, 05004], lr: 0.045305, loss: 1.7286
2022-03-12 02:51:20 - train: epoch 0071, iter [00300, 05004], lr: 0.045305, loss: 1.9886
2022-03-12 02:52:02 - train: epoch 0071, iter [00400, 05004], lr: 0.045305, loss: 2.0038
2022-03-12 02:52:44 - train: epoch 0071, iter [00500, 05004], lr: 0.045305, loss: 1.7877
2022-03-12 02:53:25 - train: epoch 0071, iter [00600, 05004], lr: 0.045305, loss: 1.9503
2022-03-12 02:54:06 - train: epoch 0071, iter [00700, 05004], lr: 0.045305, loss: 2.0073
2022-03-12 02:54:46 - train: epoch 0071, iter [00800, 05004], lr: 0.045305, loss: 1.7936
2022-03-12 02:55:27 - train: epoch 0071, iter [00900, 05004], lr: 0.045305, loss: 1.9297
2022-03-12 02:56:09 - train: epoch 0071, iter [01000, 05004], lr: 0.045305, loss: 1.8979
2022-03-12 02:56:51 - train: epoch 0071, iter [01100, 05004], lr: 0.045305, loss: 1.9924
2022-03-12 02:57:33 - train: epoch 0071, iter [01200, 05004], lr: 0.045305, loss: 1.8734
2022-03-12 02:58:15 - train: epoch 0071, iter [01300, 05004], lr: 0.045305, loss: 1.9535
2022-03-12 02:58:58 - train: epoch 0071, iter [01400, 05004], lr: 0.045305, loss: 1.7683
2022-03-12 02:59:40 - train: epoch 0071, iter [01500, 05004], lr: 0.045305, loss: 1.7730
2022-03-12 03:00:24 - train: epoch 0071, iter [01600, 05004], lr: 0.045305, loss: 1.6793
2022-03-12 03:01:07 - train: epoch 0071, iter [01700, 05004], lr: 0.045305, loss: 1.7694
2022-03-12 03:01:51 - train: epoch 0071, iter [01800, 05004], lr: 0.045305, loss: 1.9359
2022-03-12 03:02:33 - train: epoch 0071, iter [01900, 05004], lr: 0.045305, loss: 1.9645
2022-03-12 03:03:16 - train: epoch 0071, iter [02000, 05004], lr: 0.045305, loss: 2.0633
2022-03-12 03:03:59 - train: epoch 0071, iter [02100, 05004], lr: 0.045305, loss: 1.8153
2022-03-12 03:04:42 - train: epoch 0071, iter [02200, 05004], lr: 0.045305, loss: 1.7271
2022-03-12 03:05:25 - train: epoch 0071, iter [02300, 05004], lr: 0.045305, loss: 1.5963
2022-03-12 03:06:06 - train: epoch 0071, iter [02400, 05004], lr: 0.045305, loss: 1.7019
2022-03-12 03:06:48 - train: epoch 0071, iter [02500, 05004], lr: 0.045305, loss: 2.0001
2022-03-12 03:07:30 - train: epoch 0071, iter [02600, 05004], lr: 0.045305, loss: 1.6946
2022-03-12 03:08:13 - train: epoch 0071, iter [02700, 05004], lr: 0.045305, loss: 1.9053
2022-03-12 03:08:55 - train: epoch 0071, iter [02800, 05004], lr: 0.045305, loss: 1.7924
2022-03-12 03:09:38 - train: epoch 0071, iter [02900, 05004], lr: 0.045305, loss: 1.8330
2022-03-12 03:10:20 - train: epoch 0071, iter [03000, 05004], lr: 0.045305, loss: 1.9486
2022-03-12 03:11:03 - train: epoch 0071, iter [03100, 05004], lr: 0.045305, loss: 1.6831
2022-03-12 03:11:44 - train: epoch 0071, iter [03200, 05004], lr: 0.045305, loss: 1.8560
2022-03-12 03:12:27 - train: epoch 0071, iter [03300, 05004], lr: 0.045305, loss: 1.8029
2022-03-12 03:13:09 - train: epoch 0071, iter [03400, 05004], lr: 0.045305, loss: 1.9039
2022-03-12 03:13:52 - train: epoch 0071, iter [03500, 05004], lr: 0.045305, loss: 1.8762
2022-03-12 03:14:35 - train: epoch 0071, iter [03600, 05004], lr: 0.045305, loss: 2.0886
2022-03-12 03:15:17 - train: epoch 0071, iter [03700, 05004], lr: 0.045305, loss: 1.9395
2022-03-12 03:16:00 - train: epoch 0071, iter [03800, 05004], lr: 0.045305, loss: 1.8709
2022-03-12 03:16:42 - train: epoch 0071, iter [03900, 05004], lr: 0.045305, loss: 1.7859
2022-03-12 03:17:24 - train: epoch 0071, iter [04000, 05004], lr: 0.045305, loss: 1.7115
2022-03-12 03:18:07 - train: epoch 0071, iter [04100, 05004], lr: 0.045305, loss: 1.7117
2022-03-12 03:18:48 - train: epoch 0071, iter [04200, 05004], lr: 0.045305, loss: 2.0822
2022-03-12 03:19:28 - train: epoch 0071, iter [04300, 05004], lr: 0.045305, loss: 1.6577
2022-03-12 03:20:09 - train: epoch 0071, iter [04400, 05004], lr: 0.045305, loss: 1.9646
2022-03-12 03:20:50 - train: epoch 0071, iter [04500, 05004], lr: 0.045305, loss: 1.9452
2022-03-12 03:21:33 - train: epoch 0071, iter [04600, 05004], lr: 0.045305, loss: 1.9943
2022-03-12 03:22:14 - train: epoch 0071, iter [04700, 05004], lr: 0.045305, loss: 1.9269
2022-03-12 03:22:57 - train: epoch 0071, iter [04800, 05004], lr: 0.045305, loss: 1.9881
2022-03-12 03:23:40 - train: epoch 0071, iter [04900, 05004], lr: 0.045305, loss: 1.8479
2022-03-12 03:24:23 - train: epoch 0071, iter [05000, 05004], lr: 0.045305, loss: 1.9286
2022-03-12 03:24:25 - train: epoch 071, train_loss: 1.8565
2022-03-12 03:25:53 - eval: epoch: 071, acc1: 60.900%, acc5: 83.940%, test_loss: 1.6217, per_image_load_time: 3.158ms, per_image_inference_time: 0.265ms
2022-03-12 03:25:53 - until epoch: 071, best_acc1: 61.452%
2022-03-12 03:25:53 - epoch 072 lr: 0.04256713373170565
2022-03-12 03:26:37 - train: epoch 0072, iter [00100, 05004], lr: 0.042567, loss: 1.9273
2022-03-12 03:27:18 - train: epoch 0072, iter [00200, 05004], lr: 0.042567, loss: 1.7563
2022-03-12 03:28:00 - train: epoch 0072, iter [00300, 05004], lr: 0.042567, loss: 1.7286
2022-03-12 03:28:41 - train: epoch 0072, iter [00400, 05004], lr: 0.042567, loss: 1.7358
2022-03-12 03:29:23 - train: epoch 0072, iter [00500, 05004], lr: 0.042567, loss: 1.7584
2022-03-12 03:30:06 - train: epoch 0072, iter [00600, 05004], lr: 0.042567, loss: 1.7517
2022-03-12 03:30:49 - train: epoch 0072, iter [00700, 05004], lr: 0.042567, loss: 1.8706
2022-03-12 03:31:31 - train: epoch 0072, iter [00800, 05004], lr: 0.042567, loss: 2.1342
2022-03-12 03:32:11 - train: epoch 0072, iter [00900, 05004], lr: 0.042567, loss: 1.7538
2022-03-12 03:32:54 - train: epoch 0072, iter [01000, 05004], lr: 0.042567, loss: 1.7804
2022-03-12 03:33:38 - train: epoch 0072, iter [01100, 05004], lr: 0.042567, loss: 1.8027
2022-03-12 03:34:20 - train: epoch 0072, iter [01200, 05004], lr: 0.042567, loss: 1.7097
2022-03-12 03:35:03 - train: epoch 0072, iter [01300, 05004], lr: 0.042567, loss: 2.0913
2022-03-12 03:35:44 - train: epoch 0072, iter [01400, 05004], lr: 0.042567, loss: 1.8461
2022-03-12 03:36:27 - train: epoch 0072, iter [01500, 05004], lr: 0.042567, loss: 1.8857
2022-03-12 03:37:10 - train: epoch 0072, iter [01600, 05004], lr: 0.042567, loss: 1.7820
2022-03-12 03:37:51 - train: epoch 0072, iter [01700, 05004], lr: 0.042567, loss: 1.7011
2022-03-12 03:38:32 - train: epoch 0072, iter [01800, 05004], lr: 0.042567, loss: 1.7937
2022-03-12 03:39:14 - train: epoch 0072, iter [01900, 05004], lr: 0.042567, loss: 1.8967
2022-03-12 03:39:56 - train: epoch 0072, iter [02000, 05004], lr: 0.042567, loss: 1.8938
2022-03-12 03:40:38 - train: epoch 0072, iter [02100, 05004], lr: 0.042567, loss: 2.0689
2022-03-12 03:41:22 - train: epoch 0072, iter [02200, 05004], lr: 0.042567, loss: 1.9692
2022-03-12 03:42:04 - train: epoch 0072, iter [02300, 05004], lr: 0.042567, loss: 1.9293
2022-03-12 03:42:44 - train: epoch 0072, iter [02400, 05004], lr: 0.042567, loss: 1.7464
2022-03-12 03:43:25 - train: epoch 0072, iter [02500, 05004], lr: 0.042567, loss: 1.8435
2022-03-12 03:44:05 - train: epoch 0072, iter [02600, 05004], lr: 0.042567, loss: 1.7222
2022-03-12 03:44:47 - train: epoch 0072, iter [02700, 05004], lr: 0.042567, loss: 1.8442
2022-03-12 03:45:29 - train: epoch 0072, iter [02800, 05004], lr: 0.042567, loss: 1.8558
2022-03-12 03:46:11 - train: epoch 0072, iter [02900, 05004], lr: 0.042567, loss: 1.6904
2022-03-12 03:46:53 - train: epoch 0072, iter [03000, 05004], lr: 0.042567, loss: 1.8989
2022-03-12 03:47:36 - train: epoch 0072, iter [03100, 05004], lr: 0.042567, loss: 1.9077
2022-03-12 03:48:20 - train: epoch 0072, iter [03200, 05004], lr: 0.042567, loss: 1.7594
2022-03-12 03:49:03 - train: epoch 0072, iter [03300, 05004], lr: 0.042567, loss: 1.8222
2022-03-12 03:49:46 - train: epoch 0072, iter [03400, 05004], lr: 0.042567, loss: 2.0765
2022-03-12 03:50:28 - train: epoch 0072, iter [03500, 05004], lr: 0.042567, loss: 1.6574
2022-03-12 03:51:12 - train: epoch 0072, iter [03600, 05004], lr: 0.042567, loss: 1.9714
2022-03-12 03:51:54 - train: epoch 0072, iter [03700, 05004], lr: 0.042567, loss: 2.0594
2022-03-12 03:52:37 - train: epoch 0072, iter [03800, 05004], lr: 0.042567, loss: 1.8361
2022-03-12 03:53:20 - train: epoch 0072, iter [03900, 05004], lr: 0.042567, loss: 1.7713
2022-03-12 03:54:02 - train: epoch 0072, iter [04000, 05004], lr: 0.042567, loss: 1.9094
2022-03-12 03:54:45 - train: epoch 0072, iter [04100, 05004], lr: 0.042567, loss: 1.9781
2022-03-12 03:55:27 - train: epoch 0072, iter [04200, 05004], lr: 0.042567, loss: 1.7029
2022-03-12 03:56:09 - train: epoch 0072, iter [04300, 05004], lr: 0.042567, loss: 1.9626
2022-03-12 03:56:50 - train: epoch 0072, iter [04400, 05004], lr: 0.042567, loss: 1.9405
2022-03-12 03:57:33 - train: epoch 0072, iter [04500, 05004], lr: 0.042567, loss: 1.8418
2022-03-12 03:58:15 - train: epoch 0072, iter [04600, 05004], lr: 0.042567, loss: 1.7341
2022-03-12 03:58:56 - train: epoch 0072, iter [04700, 05004], lr: 0.042567, loss: 1.8191
2022-03-12 03:59:39 - train: epoch 0072, iter [04800, 05004], lr: 0.042567, loss: 1.8546
2022-03-12 04:00:20 - train: epoch 0072, iter [04900, 05004], lr: 0.042567, loss: 1.7253
2022-03-12 04:01:02 - train: epoch 0072, iter [05000, 05004], lr: 0.042567, loss: 1.9646
2022-03-12 04:01:04 - train: epoch 072, train_loss: 1.8374
2022-03-12 04:02:31 - eval: epoch: 072, acc1: 61.006%, acc5: 84.186%, test_loss: 1.6012, per_image_load_time: 3.122ms, per_image_inference_time: 0.268ms
2022-03-12 04:02:31 - until epoch: 072, best_acc1: 61.452%
2022-03-12 04:02:31 - epoch 073 lr: 0.03989188527169749
2022-03-12 04:03:17 - train: epoch 0073, iter [00100, 05004], lr: 0.039892, loss: 2.0189
2022-03-12 04:03:59 - train: epoch 0073, iter [00200, 05004], lr: 0.039892, loss: 1.8344
2022-03-12 04:04:43 - train: epoch 0073, iter [00300, 05004], lr: 0.039892, loss: 1.7266
2022-03-12 04:05:25 - train: epoch 0073, iter [00400, 05004], lr: 0.039892, loss: 1.7627
2022-03-12 04:06:08 - train: epoch 0073, iter [00500, 05004], lr: 0.039892, loss: 1.7980
2022-03-12 04:06:49 - train: epoch 0073, iter [00600, 05004], lr: 0.039892, loss: 1.6668
2022-03-12 04:07:32 - train: epoch 0073, iter [00700, 05004], lr: 0.039892, loss: 1.8653
2022-03-12 04:08:13 - train: epoch 0073, iter [00800, 05004], lr: 0.039892, loss: 1.8511
2022-03-12 04:08:54 - train: epoch 0073, iter [00900, 05004], lr: 0.039892, loss: 1.7792
2022-03-12 04:09:36 - train: epoch 0073, iter [01000, 05004], lr: 0.039892, loss: 1.7945
2022-03-12 04:10:18 - train: epoch 0073, iter [01100, 05004], lr: 0.039892, loss: 1.9524
2022-03-12 04:11:00 - train: epoch 0073, iter [01200, 05004], lr: 0.039892, loss: 1.8647
2022-03-12 04:11:43 - train: epoch 0073, iter [01300, 05004], lr: 0.039892, loss: 1.8949
2022-03-12 04:12:27 - train: epoch 0073, iter [01400, 05004], lr: 0.039892, loss: 1.8295
2022-03-12 04:13:10 - train: epoch 0073, iter [01500, 05004], lr: 0.039892, loss: 1.9007
2022-03-12 04:13:53 - train: epoch 0073, iter [01600, 05004], lr: 0.039892, loss: 1.8437
2022-03-12 04:14:37 - train: epoch 0073, iter [01700, 05004], lr: 0.039892, loss: 1.8610
2022-03-12 04:15:22 - train: epoch 0073, iter [01800, 05004], lr: 0.039892, loss: 1.5355
2022-03-12 04:16:05 - train: epoch 0073, iter [01900, 05004], lr: 0.039892, loss: 2.0533
2022-03-12 04:16:47 - train: epoch 0073, iter [02000, 05004], lr: 0.039892, loss: 1.7819
2022-03-12 04:17:30 - train: epoch 0073, iter [02100, 05004], lr: 0.039892, loss: 1.6681
2022-03-12 04:18:13 - train: epoch 0073, iter [02200, 05004], lr: 0.039892, loss: 2.0377
2022-03-12 04:18:55 - train: epoch 0073, iter [02300, 05004], lr: 0.039892, loss: 1.9199
2022-03-12 04:19:38 - train: epoch 0073, iter [02400, 05004], lr: 0.039892, loss: 1.9339
2022-03-12 04:20:22 - train: epoch 0073, iter [02500, 05004], lr: 0.039892, loss: 1.9188
2022-03-12 04:21:04 - train: epoch 0073, iter [02600, 05004], lr: 0.039892, loss: 1.8864
2022-03-12 04:21:48 - train: epoch 0073, iter [02700, 05004], lr: 0.039892, loss: 1.8041
2022-03-12 04:22:31 - train: epoch 0073, iter [02800, 05004], lr: 0.039892, loss: 1.7001
2022-03-12 04:23:13 - train: epoch 0073, iter [02900, 05004], lr: 0.039892, loss: 1.8991
2022-03-12 04:23:57 - train: epoch 0073, iter [03000, 05004], lr: 0.039892, loss: 1.8636
2022-03-12 04:24:39 - train: epoch 0073, iter [03100, 05004], lr: 0.039892, loss: 1.6754
2022-03-12 04:25:22 - train: epoch 0073, iter [03200, 05004], lr: 0.039892, loss: 1.9094
2022-03-12 04:26:05 - train: epoch 0073, iter [03300, 05004], lr: 0.039892, loss: 1.9444
2022-03-12 04:26:48 - train: epoch 0073, iter [03400, 05004], lr: 0.039892, loss: 1.7481
2022-03-12 04:27:32 - train: epoch 0073, iter [03500, 05004], lr: 0.039892, loss: 1.7597
2022-03-12 04:28:15 - train: epoch 0073, iter [03600, 05004], lr: 0.039892, loss: 1.8102
2022-03-12 04:28:57 - train: epoch 0073, iter [03700, 05004], lr: 0.039892, loss: 1.8918
2022-03-12 04:29:41 - train: epoch 0073, iter [03800, 05004], lr: 0.039892, loss: 2.1116
2022-03-12 04:30:24 - train: epoch 0073, iter [03900, 05004], lr: 0.039892, loss: 1.8547
2022-03-12 04:31:08 - train: epoch 0073, iter [04000, 05004], lr: 0.039892, loss: 1.9219
2022-03-12 04:31:50 - train: epoch 0073, iter [04100, 05004], lr: 0.039892, loss: 1.8563
2022-03-12 04:32:30 - train: epoch 0073, iter [04200, 05004], lr: 0.039892, loss: 1.8356
2022-03-12 04:33:11 - train: epoch 0073, iter [04300, 05004], lr: 0.039892, loss: 1.9440
2022-03-12 04:33:54 - train: epoch 0073, iter [04400, 05004], lr: 0.039892, loss: 1.9641
2022-03-12 04:34:37 - train: epoch 0073, iter [04500, 05004], lr: 0.039892, loss: 1.7686
2022-03-12 04:35:19 - train: epoch 0073, iter [04600, 05004], lr: 0.039892, loss: 1.8903
2022-03-12 04:36:01 - train: epoch 0073, iter [04700, 05004], lr: 0.039892, loss: 1.4380
2022-03-12 04:36:44 - train: epoch 0073, iter [04800, 05004], lr: 0.039892, loss: 1.8669
2022-03-12 04:37:28 - train: epoch 0073, iter [04900, 05004], lr: 0.039892, loss: 1.6364
2022-03-12 04:38:12 - train: epoch 0073, iter [05000, 05004], lr: 0.039892, loss: 1.7903
2022-03-12 04:38:13 - train: epoch 073, train_loss: 1.8193
2022-03-12 04:39:41 - eval: epoch: 073, acc1: 61.396%, acc5: 84.402%, test_loss: 1.5907, per_image_load_time: 3.134ms, per_image_inference_time: 0.272ms
2022-03-12 04:39:41 - until epoch: 073, best_acc1: 61.452%
2022-03-12 04:39:41 - epoch 074 lr: 0.0372823641526463
2022-03-12 04:40:27 - train: epoch 0074, iter [00100, 05004], lr: 0.037282, loss: 1.7507
2022-03-12 04:41:09 - train: epoch 0074, iter [00200, 05004], lr: 0.037282, loss: 2.0518
2022-03-12 04:41:52 - train: epoch 0074, iter [00300, 05004], lr: 0.037282, loss: 1.6902
2022-03-12 04:42:34 - train: epoch 0074, iter [00400, 05004], lr: 0.037282, loss: 2.0832
2022-03-12 04:43:16 - train: epoch 0074, iter [00500, 05004], lr: 0.037282, loss: 1.7347
2022-03-12 04:43:59 - train: epoch 0074, iter [00600, 05004], lr: 0.037282, loss: 1.7742
2022-03-12 04:44:42 - train: epoch 0074, iter [00700, 05004], lr: 0.037282, loss: 1.7162
2022-03-12 04:45:23 - train: epoch 0074, iter [00800, 05004], lr: 0.037282, loss: 1.6935
2022-03-12 04:46:06 - train: epoch 0074, iter [00900, 05004], lr: 0.037282, loss: 1.6658
2022-03-12 04:46:48 - train: epoch 0074, iter [01000, 05004], lr: 0.037282, loss: 2.0223
2022-03-12 04:47:31 - train: epoch 0074, iter [01100, 05004], lr: 0.037282, loss: 1.8670
2022-03-12 04:48:13 - train: epoch 0074, iter [01200, 05004], lr: 0.037282, loss: 1.8029
2022-03-12 04:48:56 - train: epoch 0074, iter [01300, 05004], lr: 0.037282, loss: 1.8472
2022-03-12 04:49:37 - train: epoch 0074, iter [01400, 05004], lr: 0.037282, loss: 1.7121
2022-03-12 04:50:19 - train: epoch 0074, iter [01500, 05004], lr: 0.037282, loss: 1.8977
2022-03-12 04:51:02 - train: epoch 0074, iter [01600, 05004], lr: 0.037282, loss: 1.6687
2022-03-12 04:51:43 - train: epoch 0074, iter [01700, 05004], lr: 0.037282, loss: 1.8877
2022-03-12 04:52:25 - train: epoch 0074, iter [01800, 05004], lr: 0.037282, loss: 2.0174
2022-03-12 04:53:07 - train: epoch 0074, iter [01900, 05004], lr: 0.037282, loss: 1.7896
2022-03-12 04:53:50 - train: epoch 0074, iter [02000, 05004], lr: 0.037282, loss: 1.7397
2022-03-12 04:54:31 - train: epoch 0074, iter [02100, 05004], lr: 0.037282, loss: 1.7954
2022-03-12 04:55:13 - train: epoch 0074, iter [02200, 05004], lr: 0.037282, loss: 1.9499
2022-03-12 04:55:55 - train: epoch 0074, iter [02300, 05004], lr: 0.037282, loss: 1.8174
2022-03-12 04:56:37 - train: epoch 0074, iter [02400, 05004], lr: 0.037282, loss: 1.6564
2022-03-12 04:57:16 - train: epoch 0074, iter [02500, 05004], lr: 0.037282, loss: 1.6817
2022-03-12 04:57:57 - train: epoch 0074, iter [02600, 05004], lr: 0.037282, loss: 1.6495
2022-03-12 04:58:39 - train: epoch 0074, iter [02700, 05004], lr: 0.037282, loss: 1.6762
2022-03-12 04:59:20 - train: epoch 0074, iter [02800, 05004], lr: 0.037282, loss: 2.0230
2022-03-12 05:00:02 - train: epoch 0074, iter [02900, 05004], lr: 0.037282, loss: 1.9184
2022-03-12 05:00:44 - train: epoch 0074, iter [03000, 05004], lr: 0.037282, loss: 1.9270
2022-03-12 05:01:27 - train: epoch 0074, iter [03100, 05004], lr: 0.037282, loss: 1.7787
2022-03-12 05:02:10 - train: epoch 0074, iter [03200, 05004], lr: 0.037282, loss: 1.7691
2022-03-12 05:02:53 - train: epoch 0074, iter [03300, 05004], lr: 0.037282, loss: 1.8568
2022-03-12 05:03:35 - train: epoch 0074, iter [03400, 05004], lr: 0.037282, loss: 1.7939
2022-03-12 05:04:18 - train: epoch 0074, iter [03500, 05004], lr: 0.037282, loss: 1.8431
2022-03-12 05:05:00 - train: epoch 0074, iter [03600, 05004], lr: 0.037282, loss: 1.7052
2022-03-12 05:05:43 - train: epoch 0074, iter [03700, 05004], lr: 0.037282, loss: 1.9855
2022-03-12 05:06:27 - train: epoch 0074, iter [03800, 05004], lr: 0.037282, loss: 1.8207
2022-03-12 05:07:11 - train: epoch 0074, iter [03900, 05004], lr: 0.037282, loss: 1.6082
2022-03-12 05:07:54 - train: epoch 0074, iter [04000, 05004], lr: 0.037282, loss: 2.0934
2022-03-12 05:08:36 - train: epoch 0074, iter [04100, 05004], lr: 0.037282, loss: 1.8868
2022-03-12 05:09:20 - train: epoch 0074, iter [04200, 05004], lr: 0.037282, loss: 1.7884
2022-03-12 05:10:04 - train: epoch 0074, iter [04300, 05004], lr: 0.037282, loss: 1.8433
2022-03-12 05:10:48 - train: epoch 0074, iter [04400, 05004], lr: 0.037282, loss: 1.7504
2022-03-12 05:11:30 - train: epoch 0074, iter [04500, 05004], lr: 0.037282, loss: 1.8015
2022-03-12 05:12:13 - train: epoch 0074, iter [04600, 05004], lr: 0.037282, loss: 1.8683
2022-03-12 05:12:55 - train: epoch 0074, iter [04700, 05004], lr: 0.037282, loss: 1.7782
2022-03-12 05:13:39 - train: epoch 0074, iter [04800, 05004], lr: 0.037282, loss: 1.9431
2022-03-12 05:14:23 - train: epoch 0074, iter [04900, 05004], lr: 0.037282, loss: 2.1902
2022-03-12 05:15:05 - train: epoch 0074, iter [05000, 05004], lr: 0.037282, loss: 1.7503
2022-03-12 05:15:06 - train: epoch 074, train_loss: 1.7970
2022-03-12 05:16:34 - eval: epoch: 074, acc1: 61.720%, acc5: 84.528%, test_loss: 1.5694, per_image_load_time: 3.024ms, per_image_inference_time: 0.265ms
2022-03-12 05:16:34 - until epoch: 074, best_acc1: 61.720%
2022-03-12 05:16:34 - epoch 075 lr: 0.03474142384758313
2022-03-12 05:17:19 - train: epoch 0075, iter [00100, 05004], lr: 0.034741, loss: 2.1334
2022-03-12 05:18:01 - train: epoch 0075, iter [00200, 05004], lr: 0.034741, loss: 1.8833
2022-03-12 05:18:44 - train: epoch 0075, iter [00300, 05004], lr: 0.034741, loss: 1.7974
2022-03-12 05:19:27 - train: epoch 0075, iter [00400, 05004], lr: 0.034741, loss: 1.6942
2022-03-12 05:20:11 - train: epoch 0075, iter [00500, 05004], lr: 0.034741, loss: 1.9179
2022-03-12 05:20:53 - train: epoch 0075, iter [00600, 05004], lr: 0.034741, loss: 1.8742
2022-03-12 05:21:33 - train: epoch 0075, iter [00700, 05004], lr: 0.034741, loss: 1.8517
2022-03-12 05:22:15 - train: epoch 0075, iter [00800, 05004], lr: 0.034741, loss: 1.7301
2022-03-12 05:22:59 - train: epoch 0075, iter [00900, 05004], lr: 0.034741, loss: 1.8957
2022-03-12 05:23:41 - train: epoch 0075, iter [01000, 05004], lr: 0.034741, loss: 1.4984
2022-03-12 05:24:23 - train: epoch 0075, iter [01100, 05004], lr: 0.034741, loss: 1.9149
2022-03-12 05:25:07 - train: epoch 0075, iter [01200, 05004], lr: 0.034741, loss: 1.5457
2022-03-12 05:25:51 - train: epoch 0075, iter [01300, 05004], lr: 0.034741, loss: 1.5261
2022-03-12 05:26:33 - train: epoch 0075, iter [01400, 05004], lr: 0.034741, loss: 1.7026
2022-03-12 05:27:17 - train: epoch 0075, iter [01500, 05004], lr: 0.034741, loss: 1.7795
2022-03-12 05:28:00 - train: epoch 0075, iter [01600, 05004], lr: 0.034741, loss: 1.5139
2022-03-12 05:28:42 - train: epoch 0075, iter [01700, 05004], lr: 0.034741, loss: 1.7168
2022-03-12 05:29:23 - train: epoch 0075, iter [01800, 05004], lr: 0.034741, loss: 1.7867
2022-03-12 05:30:04 - train: epoch 0075, iter [01900, 05004], lr: 0.034741, loss: 1.8166
2022-03-12 05:30:46 - train: epoch 0075, iter [02000, 05004], lr: 0.034741, loss: 1.7442
2022-03-12 05:31:29 - train: epoch 0075, iter [02100, 05004], lr: 0.034741, loss: 1.5448
2022-03-12 05:32:12 - train: epoch 0075, iter [02200, 05004], lr: 0.034741, loss: 1.8533
2022-03-12 05:32:54 - train: epoch 0075, iter [02300, 05004], lr: 0.034741, loss: 1.5338
2022-03-12 05:33:37 - train: epoch 0075, iter [02400, 05004], lr: 0.034741, loss: 1.7801
2022-03-12 05:34:19 - train: epoch 0075, iter [02500, 05004], lr: 0.034741, loss: 1.8687
2022-03-12 05:35:00 - train: epoch 0075, iter [02600, 05004], lr: 0.034741, loss: 1.8387
2022-03-12 05:35:43 - train: epoch 0075, iter [02700, 05004], lr: 0.034741, loss: 1.6892
2022-03-12 05:36:26 - train: epoch 0075, iter [02800, 05004], lr: 0.034741, loss: 1.7011
2022-03-12 05:37:08 - train: epoch 0075, iter [02900, 05004], lr: 0.034741, loss: 1.8544
2022-03-12 05:37:50 - train: epoch 0075, iter [03000, 05004], lr: 0.034741, loss: 1.8787
2022-03-12 05:38:32 - train: epoch 0075, iter [03100, 05004], lr: 0.034741, loss: 1.9670
2022-03-12 05:39:15 - train: epoch 0075, iter [03200, 05004], lr: 0.034741, loss: 1.9337
2022-03-12 05:39:57 - train: epoch 0075, iter [03300, 05004], lr: 0.034741, loss: 1.7031
2022-03-12 05:40:38 - train: epoch 0075, iter [03400, 05004], lr: 0.034741, loss: 1.5822
2022-03-12 05:41:20 - train: epoch 0075, iter [03500, 05004], lr: 0.034741, loss: 1.7486
2022-03-12 05:42:02 - train: epoch 0075, iter [03600, 05004], lr: 0.034741, loss: 2.0811
2022-03-12 05:42:44 - train: epoch 0075, iter [03700, 05004], lr: 0.034741, loss: 1.8775
2022-03-12 05:43:26 - train: epoch 0075, iter [03800, 05004], lr: 0.034741, loss: 1.7762
2022-03-12 05:44:08 - train: epoch 0075, iter [03900, 05004], lr: 0.034741, loss: 1.6857
2022-03-12 05:44:49 - train: epoch 0075, iter [04000, 05004], lr: 0.034741, loss: 1.6983
2022-03-12 05:45:29 - train: epoch 0075, iter [04100, 05004], lr: 0.034741, loss: 1.7755
2022-03-12 05:46:09 - train: epoch 0075, iter [04200, 05004], lr: 0.034741, loss: 2.1153
2022-03-12 05:46:52 - train: epoch 0075, iter [04300, 05004], lr: 0.034741, loss: 1.8064
2022-03-12 05:47:33 - train: epoch 0075, iter [04400, 05004], lr: 0.034741, loss: 1.6635
2022-03-12 05:48:14 - train: epoch 0075, iter [04500, 05004], lr: 0.034741, loss: 1.7723
2022-03-12 05:48:56 - train: epoch 0075, iter [04600, 05004], lr: 0.034741, loss: 1.6169
2022-03-12 05:49:39 - train: epoch 0075, iter [04700, 05004], lr: 0.034741, loss: 1.9307
2022-03-12 05:50:23 - train: epoch 0075, iter [04800, 05004], lr: 0.034741, loss: 1.8443
2022-03-12 05:51:07 - train: epoch 0075, iter [04900, 05004], lr: 0.034741, loss: 1.6546
2022-03-12 05:51:49 - train: epoch 0075, iter [05000, 05004], lr: 0.034741, loss: 1.5777
2022-03-12 05:51:51 - train: epoch 075, train_loss: 1.7756
2022-03-12 05:53:21 - eval: epoch: 075, acc1: 61.808%, acc5: 84.484%, test_loss: 1.5825, per_image_load_time: 3.019ms, per_image_inference_time: 0.272ms
2022-03-12 05:53:21 - until epoch: 075, best_acc1: 61.808%
2022-03-12 05:53:21 - epoch 076 lr: 0.032271842837425915
2022-03-12 05:54:06 - train: epoch 0076, iter [00100, 05004], lr: 0.032272, loss: 1.5015
2022-03-12 05:54:50 - train: epoch 0076, iter [00200, 05004], lr: 0.032272, loss: 1.6335
2022-03-12 05:55:31 - train: epoch 0076, iter [00300, 05004], lr: 0.032272, loss: 1.7902
2022-03-12 05:56:13 - train: epoch 0076, iter [00400, 05004], lr: 0.032272, loss: 1.7726
2022-03-12 05:56:56 - train: epoch 0076, iter [00500, 05004], lr: 0.032272, loss: 1.8805
2022-03-12 05:57:40 - train: epoch 0076, iter [00600, 05004], lr: 0.032272, loss: 1.7809
2022-03-12 05:58:22 - train: epoch 0076, iter [00700, 05004], lr: 0.032272, loss: 1.7898
2022-03-12 05:59:05 - train: epoch 0076, iter [00800, 05004], lr: 0.032272, loss: 1.6910
2022-03-12 05:59:50 - train: epoch 0076, iter [00900, 05004], lr: 0.032272, loss: 1.7164
2022-03-12 06:00:32 - train: epoch 0076, iter [01000, 05004], lr: 0.032272, loss: 1.7786
2022-03-12 06:01:15 - train: epoch 0076, iter [01100, 05004], lr: 0.032272, loss: 1.7298
2022-03-12 06:01:57 - train: epoch 0076, iter [01200, 05004], lr: 0.032272, loss: 1.7095
2022-03-12 06:02:39 - train: epoch 0076, iter [01300, 05004], lr: 0.032272, loss: 1.6880
2022-03-12 06:03:21 - train: epoch 0076, iter [01400, 05004], lr: 0.032272, loss: 1.6279
2022-03-12 06:04:04 - train: epoch 0076, iter [01500, 05004], lr: 0.032272, loss: 1.6527
2022-03-12 06:04:47 - train: epoch 0076, iter [01600, 05004], lr: 0.032272, loss: 1.7176
2022-03-12 06:05:28 - train: epoch 0076, iter [01700, 05004], lr: 0.032272, loss: 1.6764
2022-03-12 06:06:11 - train: epoch 0076, iter [01800, 05004], lr: 0.032272, loss: 1.6178
2022-03-12 06:06:53 - train: epoch 0076, iter [01900, 05004], lr: 0.032272, loss: 1.7532
2022-03-12 06:07:35 - train: epoch 0076, iter [02000, 05004], lr: 0.032272, loss: 1.6332
2022-03-12 06:08:17 - train: epoch 0076, iter [02100, 05004], lr: 0.032272, loss: 1.7455
2022-03-12 06:09:00 - train: epoch 0076, iter [02200, 05004], lr: 0.032272, loss: 1.5728
2022-03-12 06:09:39 - train: epoch 0076, iter [02300, 05004], lr: 0.032272, loss: 1.5601
2022-03-12 06:10:21 - train: epoch 0076, iter [02400, 05004], lr: 0.032272, loss: 1.9018
2022-03-12 06:11:03 - train: epoch 0076, iter [02500, 05004], lr: 0.032272, loss: 1.7760
2022-03-12 06:11:45 - train: epoch 0076, iter [02600, 05004], lr: 0.032272, loss: 1.9435
2022-03-12 06:12:28 - train: epoch 0076, iter [02700, 05004], lr: 0.032272, loss: 1.7547
2022-03-12 06:13:10 - train: epoch 0076, iter [02800, 05004], lr: 0.032272, loss: 1.7781
2022-03-12 06:13:54 - train: epoch 0076, iter [02900, 05004], lr: 0.032272, loss: 1.7561
2022-03-12 06:14:38 - train: epoch 0076, iter [03000, 05004], lr: 0.032272, loss: 1.6277
2022-03-12 06:15:22 - train: epoch 0076, iter [03100, 05004], lr: 0.032272, loss: 1.8920
2022-03-12 06:16:05 - train: epoch 0076, iter [03200, 05004], lr: 0.032272, loss: 1.8511
2022-03-12 06:16:49 - train: epoch 0076, iter [03300, 05004], lr: 0.032272, loss: 1.7076
2022-03-12 06:17:33 - train: epoch 0076, iter [03400, 05004], lr: 0.032272, loss: 1.6661
2022-03-12 06:18:17 - train: epoch 0076, iter [03500, 05004], lr: 0.032272, loss: 1.6144
2022-03-12 06:19:01 - train: epoch 0076, iter [03600, 05004], lr: 0.032272, loss: 1.6573
2022-03-12 06:19:45 - train: epoch 0076, iter [03700, 05004], lr: 0.032272, loss: 1.7242
2022-03-12 06:20:30 - train: epoch 0076, iter [03800, 05004], lr: 0.032272, loss: 1.5380
2022-03-12 06:21:14 - train: epoch 0076, iter [03900, 05004], lr: 0.032272, loss: 1.7202
2022-03-12 06:21:57 - train: epoch 0076, iter [04000, 05004], lr: 0.032272, loss: 1.8248
2022-03-12 06:22:40 - train: epoch 0076, iter [04100, 05004], lr: 0.032272, loss: 1.7308
2022-03-12 06:23:25 - train: epoch 0076, iter [04200, 05004], lr: 0.032272, loss: 1.7247
2022-03-12 06:24:08 - train: epoch 0076, iter [04300, 05004], lr: 0.032272, loss: 1.8619
2022-03-12 06:24:51 - train: epoch 0076, iter [04400, 05004], lr: 0.032272, loss: 1.5858
2022-03-12 06:25:34 - train: epoch 0076, iter [04500, 05004], lr: 0.032272, loss: 1.6845
2022-03-12 06:26:16 - train: epoch 0076, iter [04600, 05004], lr: 0.032272, loss: 1.7536
2022-03-12 06:26:59 - train: epoch 0076, iter [04700, 05004], lr: 0.032272, loss: 1.8484
2022-03-12 06:27:41 - train: epoch 0076, iter [04800, 05004], lr: 0.032272, loss: 1.5450
2022-03-12 06:28:25 - train: epoch 0076, iter [04900, 05004], lr: 0.032272, loss: 1.6387
2022-03-12 06:29:08 - train: epoch 0076, iter [05000, 05004], lr: 0.032272, loss: 2.0128
2022-03-12 06:29:09 - train: epoch 076, train_loss: 1.7565
2022-03-12 06:30:38 - eval: epoch: 076, acc1: 62.188%, acc5: 85.042%, test_loss: 1.5515, per_image_load_time: 2.793ms, per_image_inference_time: 0.267ms
2022-03-12 06:30:38 - until epoch: 076, best_acc1: 62.188%
2022-03-12 06:30:38 - epoch 077 lr: 0.029876321572751142
2022-03-12 06:31:24 - train: epoch 0077, iter [00100, 05004], lr: 0.029876, loss: 2.0575
2022-03-12 06:32:06 - train: epoch 0077, iter [00200, 05004], lr: 0.029876, loss: 1.8347
2022-03-12 06:32:49 - train: epoch 0077, iter [00300, 05004], lr: 0.029876, loss: 1.4490
2022-03-12 06:33:31 - train: epoch 0077, iter [00400, 05004], lr: 0.029876, loss: 1.6322
2022-03-12 06:34:12 - train: epoch 0077, iter [00500, 05004], lr: 0.029876, loss: 1.4744
2022-03-12 06:34:54 - train: epoch 0077, iter [00600, 05004], lr: 0.029876, loss: 1.5700
2022-03-12 06:35:36 - train: epoch 0077, iter [00700, 05004], lr: 0.029876, loss: 1.7201
2022-03-12 06:36:19 - train: epoch 0077, iter [00800, 05004], lr: 0.029876, loss: 1.7177
2022-03-12 06:37:00 - train: epoch 0077, iter [00900, 05004], lr: 0.029876, loss: 1.8579
2022-03-12 06:37:44 - train: epoch 0077, iter [01000, 05004], lr: 0.029876, loss: 1.4593
2022-03-12 06:38:28 - train: epoch 0077, iter [01100, 05004], lr: 0.029876, loss: 1.8594
2022-03-12 06:39:12 - train: epoch 0077, iter [01200, 05004], lr: 0.029876, loss: 1.8223
2022-03-12 06:39:57 - train: epoch 0077, iter [01300, 05004], lr: 0.029876, loss: 1.6585
2022-03-12 06:40:42 - train: epoch 0077, iter [01400, 05004], lr: 0.029876, loss: 1.6919
2022-03-12 06:41:26 - train: epoch 0077, iter [01500, 05004], lr: 0.029876, loss: 1.6166
2022-03-12 06:42:11 - train: epoch 0077, iter [01600, 05004], lr: 0.029876, loss: 1.5349
2022-03-12 06:42:55 - train: epoch 0077, iter [01700, 05004], lr: 0.029876, loss: 1.9008
2022-03-12 06:43:39 - train: epoch 0077, iter [01800, 05004], lr: 0.029876, loss: 1.6670
2022-03-12 06:44:23 - train: epoch 0077, iter [01900, 05004], lr: 0.029876, loss: 1.6791
2022-03-12 06:45:07 - train: epoch 0077, iter [02000, 05004], lr: 0.029876, loss: 1.5619
2022-03-12 06:45:51 - train: epoch 0077, iter [02100, 05004], lr: 0.029876, loss: 1.9115
2022-03-12 06:46:35 - train: epoch 0077, iter [02200, 05004], lr: 0.029876, loss: 1.6078
2022-03-12 06:47:20 - train: epoch 0077, iter [02300, 05004], lr: 0.029876, loss: 1.8923
2022-03-12 06:48:03 - train: epoch 0077, iter [02400, 05004], lr: 0.029876, loss: 1.5305
2022-03-12 06:48:48 - train: epoch 0077, iter [02500, 05004], lr: 0.029876, loss: 1.6699
2022-03-12 06:49:32 - train: epoch 0077, iter [02600, 05004], lr: 0.029876, loss: 1.5747
2022-03-12 06:50:17 - train: epoch 0077, iter [02700, 05004], lr: 0.029876, loss: 1.6707
2022-03-12 06:51:00 - train: epoch 0077, iter [02800, 05004], lr: 0.029876, loss: 1.6971
2022-03-12 06:51:43 - train: epoch 0077, iter [02900, 05004], lr: 0.029876, loss: 1.8940
2022-03-12 06:52:28 - train: epoch 0077, iter [03000, 05004], lr: 0.029876, loss: 1.6818
2022-03-12 06:53:12 - train: epoch 0077, iter [03100, 05004], lr: 0.029876, loss: 1.7183
2022-03-12 06:53:54 - train: epoch 0077, iter [03200, 05004], lr: 0.029876, loss: 1.9977
2022-03-12 06:54:39 - train: epoch 0077, iter [03300, 05004], lr: 0.029876, loss: 1.6331
2022-03-12 06:55:21 - train: epoch 0077, iter [03400, 05004], lr: 0.029876, loss: 1.8293
2022-03-12 06:56:05 - train: epoch 0077, iter [03500, 05004], lr: 0.029876, loss: 1.7775
2022-03-12 06:56:47 - train: epoch 0077, iter [03600, 05004], lr: 0.029876, loss: 1.6221
2022-03-12 06:57:30 - train: epoch 0077, iter [03700, 05004], lr: 0.029876, loss: 1.6263
2022-03-12 06:58:13 - train: epoch 0077, iter [03800, 05004], lr: 0.029876, loss: 1.5720
2022-03-12 06:58:55 - train: epoch 0077, iter [03900, 05004], lr: 0.029876, loss: 1.7227
2022-03-12 06:59:38 - train: epoch 0077, iter [04000, 05004], lr: 0.029876, loss: 1.7090
2022-03-12 07:00:21 - train: epoch 0077, iter [04100, 05004], lr: 0.029876, loss: 1.7185
2022-03-12 07:01:04 - train: epoch 0077, iter [04200, 05004], lr: 0.029876, loss: 1.7315
2022-03-12 07:01:45 - train: epoch 0077, iter [04300, 05004], lr: 0.029876, loss: 1.5917
2022-03-12 07:02:30 - train: epoch 0077, iter [04400, 05004], lr: 0.029876, loss: 1.6249
2022-03-12 07:03:14 - train: epoch 0077, iter [04500, 05004], lr: 0.029876, loss: 1.9847
2022-03-12 07:03:58 - train: epoch 0077, iter [04600, 05004], lr: 0.029876, loss: 1.5963
2022-03-12 07:04:42 - train: epoch 0077, iter [04700, 05004], lr: 0.029876, loss: 1.5724
2022-03-12 07:05:26 - train: epoch 0077, iter [04800, 05004], lr: 0.029876, loss: 1.6927
2022-03-12 07:06:10 - train: epoch 0077, iter [04900, 05004], lr: 0.029876, loss: 1.9955
2022-03-12 07:06:53 - train: epoch 0077, iter [05000, 05004], lr: 0.029876, loss: 1.7642
2022-03-12 07:06:55 - train: epoch 077, train_loss: 1.7333
2022-03-12 07:08:24 - eval: epoch: 077, acc1: 63.116%, acc5: 85.580%, test_loss: 1.5025, per_image_load_time: 2.963ms, per_image_inference_time: 0.266ms
2022-03-12 07:08:25 - until epoch: 077, best_acc1: 63.116%
2022-03-12 07:08:25 - epoch 078 lr: 0.027557479520891104
2022-03-12 07:09:11 - train: epoch 0078, iter [00100, 05004], lr: 0.027557, loss: 1.6829
2022-03-12 07:09:54 - train: epoch 0078, iter [00200, 05004], lr: 0.027557, loss: 1.8127
2022-03-12 07:10:37 - train: epoch 0078, iter [00300, 05004], lr: 0.027557, loss: 1.6917
2022-03-12 07:11:20 - train: epoch 0078, iter [00400, 05004], lr: 0.027557, loss: 1.8676
2022-03-12 07:12:03 - train: epoch 0078, iter [00500, 05004], lr: 0.027557, loss: 1.6005
2022-03-12 07:12:45 - train: epoch 0078, iter [00600, 05004], lr: 0.027557, loss: 1.6526
2022-03-12 07:13:28 - train: epoch 0078, iter [00700, 05004], lr: 0.027557, loss: 1.7143
2022-03-12 07:14:12 - train: epoch 0078, iter [00800, 05004], lr: 0.027557, loss: 1.8302
2022-03-12 07:14:54 - train: epoch 0078, iter [00900, 05004], lr: 0.027557, loss: 1.7808
2022-03-12 07:15:37 - train: epoch 0078, iter [01000, 05004], lr: 0.027557, loss: 1.5334
2022-03-12 07:16:20 - train: epoch 0078, iter [01100, 05004], lr: 0.027557, loss: 1.5597
2022-03-12 07:17:03 - train: epoch 0078, iter [01200, 05004], lr: 0.027557, loss: 1.8325
2022-03-12 07:17:47 - train: epoch 0078, iter [01300, 05004], lr: 0.027557, loss: 1.7275
2022-03-12 07:18:30 - train: epoch 0078, iter [01400, 05004], lr: 0.027557, loss: 1.4983
2022-03-12 07:19:12 - train: epoch 0078, iter [01500, 05004], lr: 0.027557, loss: 1.6900
2022-03-12 07:19:55 - train: epoch 0078, iter [01600, 05004], lr: 0.027557, loss: 1.5673
2022-03-12 07:20:36 - train: epoch 0078, iter [01700, 05004], lr: 0.027557, loss: 1.6819
2022-03-12 07:21:18 - train: epoch 0078, iter [01800, 05004], lr: 0.027557, loss: 1.6842
2022-03-12 07:22:00 - train: epoch 0078, iter [01900, 05004], lr: 0.027557, loss: 1.7396
2022-03-12 07:22:42 - train: epoch 0078, iter [02000, 05004], lr: 0.027557, loss: 1.6640
2022-03-12 07:23:22 - train: epoch 0078, iter [02100, 05004], lr: 0.027557, loss: 2.0084
2022-03-12 07:24:06 - train: epoch 0078, iter [02200, 05004], lr: 0.027557, loss: 1.8034
2022-03-12 07:24:49 - train: epoch 0078, iter [02300, 05004], lr: 0.027557, loss: 1.6658
2022-03-12 07:25:32 - train: epoch 0078, iter [02400, 05004], lr: 0.027557, loss: 1.6923
2022-03-12 07:26:13 - train: epoch 0078, iter [02500, 05004], lr: 0.027557, loss: 1.7188
2022-03-12 07:26:57 - train: epoch 0078, iter [02600, 05004], lr: 0.027557, loss: 1.6478
2022-03-12 07:27:41 - train: epoch 0078, iter [02700, 05004], lr: 0.027557, loss: 1.6386
2022-03-12 07:28:24 - train: epoch 0078, iter [02800, 05004], lr: 0.027557, loss: 1.7831
2022-03-12 07:29:07 - train: epoch 0078, iter [02900, 05004], lr: 0.027557, loss: 1.5625
2022-03-12 07:29:51 - train: epoch 0078, iter [03000, 05004], lr: 0.027557, loss: 1.6605
2022-03-12 07:30:35 - train: epoch 0078, iter [03100, 05004], lr: 0.027557, loss: 1.8317
2022-03-12 07:31:18 - train: epoch 0078, iter [03200, 05004], lr: 0.027557, loss: 1.8479
2022-03-12 07:32:01 - train: epoch 0078, iter [03300, 05004], lr: 0.027557, loss: 1.9187
2022-03-12 07:32:43 - train: epoch 0078, iter [03400, 05004], lr: 0.027557, loss: 1.7732
2022-03-12 07:33:26 - train: epoch 0078, iter [03500, 05004], lr: 0.027557, loss: 1.6355
2022-03-12 07:34:10 - train: epoch 0078, iter [03600, 05004], lr: 0.027557, loss: 1.8163
2022-03-12 07:34:53 - train: epoch 0078, iter [03700, 05004], lr: 0.027557, loss: 1.7057
2022-03-12 07:35:35 - train: epoch 0078, iter [03800, 05004], lr: 0.027557, loss: 1.6312
2022-03-12 07:36:17 - train: epoch 0078, iter [03900, 05004], lr: 0.027557, loss: 1.6608
2022-03-12 07:36:58 - train: epoch 0078, iter [04000, 05004], lr: 0.027557, loss: 1.6766
2022-03-12 07:37:42 - train: epoch 0078, iter [04100, 05004], lr: 0.027557, loss: 1.8271
2022-03-12 07:38:25 - train: epoch 0078, iter [04200, 05004], lr: 0.027557, loss: 1.8755
2022-03-12 07:39:06 - train: epoch 0078, iter [04300, 05004], lr: 0.027557, loss: 1.7761
2022-03-12 07:39:49 - train: epoch 0078, iter [04400, 05004], lr: 0.027557, loss: 1.6439
2022-03-12 07:40:31 - train: epoch 0078, iter [04500, 05004], lr: 0.027557, loss: 1.8095
2022-03-12 07:41:12 - train: epoch 0078, iter [04600, 05004], lr: 0.027557, loss: 1.4463
2022-03-12 07:41:55 - train: epoch 0078, iter [04700, 05004], lr: 0.027557, loss: 1.7010
2022-03-12 07:42:37 - train: epoch 0078, iter [04800, 05004], lr: 0.027557, loss: 2.0437
2022-03-12 07:43:19 - train: epoch 0078, iter [04900, 05004], lr: 0.027557, loss: 1.7040
2022-03-12 07:44:01 - train: epoch 0078, iter [05000, 05004], lr: 0.027557, loss: 1.6774
2022-03-12 07:44:03 - train: epoch 078, train_loss: 1.7105
2022-03-12 07:45:31 - eval: epoch: 078, acc1: 63.342%, acc5: 85.782%, test_loss: 1.4873, per_image_load_time: 3.106ms, per_image_inference_time: 0.272ms
2022-03-12 07:45:31 - until epoch: 078, best_acc1: 63.342%
2022-03-12 07:45:31 - epoch 079 lr: 0.025317852301584644
2022-03-12 07:46:16 - train: epoch 0079, iter [00100, 05004], lr: 0.025318, loss: 1.4552
2022-03-12 07:46:58 - train: epoch 0079, iter [00200, 05004], lr: 0.025318, loss: 1.6557
2022-03-12 07:47:38 - train: epoch 0079, iter [00300, 05004], lr: 0.025318, loss: 1.5488
2022-03-12 07:48:21 - train: epoch 0079, iter [00400, 05004], lr: 0.025318, loss: 1.8162
2022-03-12 07:49:04 - train: epoch 0079, iter [00500, 05004], lr: 0.025318, loss: 1.6388
2022-03-12 07:49:46 - train: epoch 0079, iter [00600, 05004], lr: 0.025318, loss: 1.6855
2022-03-12 07:50:29 - train: epoch 0079, iter [00700, 05004], lr: 0.025318, loss: 1.6062
2022-03-12 07:51:12 - train: epoch 0079, iter [00800, 05004], lr: 0.025318, loss: 1.7658
2022-03-12 07:51:56 - train: epoch 0079, iter [00900, 05004], lr: 0.025318, loss: 1.8588
2022-03-12 07:52:40 - train: epoch 0079, iter [01000, 05004], lr: 0.025318, loss: 1.7481
2022-03-12 07:53:26 - train: epoch 0079, iter [01100, 05004], lr: 0.025318, loss: 1.8601
2022-03-12 07:54:10 - train: epoch 0079, iter [01200, 05004], lr: 0.025318, loss: 1.8720
2022-03-12 07:54:54 - train: epoch 0079, iter [01300, 05004], lr: 0.025318, loss: 1.5091
2022-03-12 07:55:39 - train: epoch 0079, iter [01400, 05004], lr: 0.025318, loss: 1.6746
2022-03-12 07:56:24 - train: epoch 0079, iter [01500, 05004], lr: 0.025318, loss: 1.8959
2022-03-12 07:57:08 - train: epoch 0079, iter [01600, 05004], lr: 0.025318, loss: 1.6209
2022-03-12 07:57:53 - train: epoch 0079, iter [01700, 05004], lr: 0.025318, loss: 1.7130
2022-03-12 07:58:39 - train: epoch 0079, iter [01800, 05004], lr: 0.025318, loss: 1.8757
2022-03-12 07:59:24 - train: epoch 0079, iter [01900, 05004], lr: 0.025318, loss: 1.6421
2022-03-12 08:00:08 - train: epoch 0079, iter [02000, 05004], lr: 0.025318, loss: 1.8398
2022-03-12 08:00:52 - train: epoch 0079, iter [02100, 05004], lr: 0.025318, loss: 1.6806
2022-03-12 08:01:36 - train: epoch 0079, iter [02200, 05004], lr: 0.025318, loss: 1.6862
2022-03-12 08:02:19 - train: epoch 0079, iter [02300, 05004], lr: 0.025318, loss: 1.6855
2022-03-12 08:03:02 - train: epoch 0079, iter [02400, 05004], lr: 0.025318, loss: 1.4621
2022-03-12 08:03:45 - train: epoch 0079, iter [02500, 05004], lr: 0.025318, loss: 1.7485
2022-03-12 08:04:28 - train: epoch 0079, iter [02600, 05004], lr: 0.025318, loss: 1.7237
2022-03-12 08:05:13 - train: epoch 0079, iter [02700, 05004], lr: 0.025318, loss: 1.5109
2022-03-12 08:05:57 - train: epoch 0079, iter [02800, 05004], lr: 0.025318, loss: 1.6694
2022-03-12 08:06:40 - train: epoch 0079, iter [02900, 05004], lr: 0.025318, loss: 1.6476
2022-03-12 08:07:23 - train: epoch 0079, iter [03000, 05004], lr: 0.025318, loss: 1.7319
2022-03-12 08:08:06 - train: epoch 0079, iter [03100, 05004], lr: 0.025318, loss: 1.5136
2022-03-12 08:08:49 - train: epoch 0079, iter [03200, 05004], lr: 0.025318, loss: 1.7545
2022-03-12 08:09:32 - train: epoch 0079, iter [03300, 05004], lr: 0.025318, loss: 1.7050
2022-03-12 08:10:15 - train: epoch 0079, iter [03400, 05004], lr: 0.025318, loss: 1.6033
2022-03-12 08:10:57 - train: epoch 0079, iter [03500, 05004], lr: 0.025318, loss: 1.9146
2022-03-12 08:11:39 - train: epoch 0079, iter [03600, 05004], lr: 0.025318, loss: 1.6018
2022-03-12 08:12:19 - train: epoch 0079, iter [03700, 05004], lr: 0.025318, loss: 1.6312
2022-03-12 08:13:03 - train: epoch 0079, iter [03800, 05004], lr: 0.025318, loss: 1.6491
2022-03-12 08:13:46 - train: epoch 0079, iter [03900, 05004], lr: 0.025318, loss: 1.5745
2022-03-12 08:14:28 - train: epoch 0079, iter [04000, 05004], lr: 0.025318, loss: 1.5216
2022-03-12 08:15:12 - train: epoch 0079, iter [04100, 05004], lr: 0.025318, loss: 1.8553
2022-03-12 08:15:57 - train: epoch 0079, iter [04200, 05004], lr: 0.025318, loss: 1.5230
2022-03-12 08:16:41 - train: epoch 0079, iter [04300, 05004], lr: 0.025318, loss: 1.4246
2022-03-12 08:17:25 - train: epoch 0079, iter [04400, 05004], lr: 0.025318, loss: 1.8245
2022-03-12 08:18:07 - train: epoch 0079, iter [04500, 05004], lr: 0.025318, loss: 1.9852
2022-03-12 08:18:52 - train: epoch 0079, iter [04600, 05004], lr: 0.025318, loss: 1.7879
2022-03-12 08:19:36 - train: epoch 0079, iter [04700, 05004], lr: 0.025318, loss: 1.6491
2022-03-12 08:20:19 - train: epoch 0079, iter [04800, 05004], lr: 0.025318, loss: 1.7520
2022-03-12 08:21:02 - train: epoch 0079, iter [04900, 05004], lr: 0.025318, loss: 1.7292
2022-03-12 08:21:46 - train: epoch 0079, iter [05000, 05004], lr: 0.025318, loss: 1.6520
2022-03-12 08:21:48 - train: epoch 079, train_loss: 1.6915
2022-03-12 08:23:17 - eval: epoch: 079, acc1: 63.864%, acc5: 85.916%, test_loss: 1.4735, per_image_load_time: 3.235ms, per_image_inference_time: 0.269ms
2022-03-12 08:23:17 - until epoch: 079, best_acc1: 63.864%
2022-03-12 08:23:17 - epoch 080 lr: 0.02315988891431412
2022-03-12 08:24:02 - train: epoch 0080, iter [00100, 05004], lr: 0.023160, loss: 1.5414
2022-03-12 08:24:45 - train: epoch 0080, iter [00200, 05004], lr: 0.023160, loss: 1.7412
2022-03-12 08:25:28 - train: epoch 0080, iter [00300, 05004], lr: 0.023160, loss: 1.6796
2022-03-12 08:26:11 - train: epoch 0080, iter [00400, 05004], lr: 0.023160, loss: 1.5322
2022-03-12 08:26:54 - train: epoch 0080, iter [00500, 05004], lr: 0.023160, loss: 1.4294
2022-03-12 08:27:37 - train: epoch 0080, iter [00600, 05004], lr: 0.023160, loss: 1.6553
2022-03-12 08:28:20 - train: epoch 0080, iter [00700, 05004], lr: 0.023160, loss: 1.6143
2022-03-12 08:29:02 - train: epoch 0080, iter [00800, 05004], lr: 0.023160, loss: 1.3779
2022-03-12 08:29:45 - train: epoch 0080, iter [00900, 05004], lr: 0.023160, loss: 1.5510
2022-03-12 08:30:28 - train: epoch 0080, iter [01000, 05004], lr: 0.023160, loss: 1.5641
2022-03-12 08:31:11 - train: epoch 0080, iter [01100, 05004], lr: 0.023160, loss: 1.6706
2022-03-12 08:31:54 - train: epoch 0080, iter [01200, 05004], lr: 0.023160, loss: 1.6000
2022-03-12 08:32:38 - train: epoch 0080, iter [01300, 05004], lr: 0.023160, loss: 1.3414
2022-03-12 08:33:20 - train: epoch 0080, iter [01400, 05004], lr: 0.023160, loss: 1.6202
2022-03-12 08:34:05 - train: epoch 0080, iter [01500, 05004], lr: 0.023160, loss: 1.6560
2022-03-12 08:34:48 - train: epoch 0080, iter [01600, 05004], lr: 0.023160, loss: 1.6039
2022-03-12 08:35:29 - train: epoch 0080, iter [01700, 05004], lr: 0.023160, loss: 1.8072
2022-03-12 08:36:10 - train: epoch 0080, iter [01800, 05004], lr: 0.023160, loss: 1.7267
2022-03-12 08:36:52 - train: epoch 0080, iter [01900, 05004], lr: 0.023160, loss: 1.8382
2022-03-12 08:37:34 - train: epoch 0080, iter [02000, 05004], lr: 0.023160, loss: 1.6673
2022-03-12 08:38:16 - train: epoch 0080, iter [02100, 05004], lr: 0.023160, loss: 1.8731
2022-03-12 08:38:57 - train: epoch 0080, iter [02200, 05004], lr: 0.023160, loss: 1.5999
2022-03-12 08:39:40 - train: epoch 0080, iter [02300, 05004], lr: 0.023160, loss: 1.6699
2022-03-12 08:40:22 - train: epoch 0080, iter [02400, 05004], lr: 0.023160, loss: 1.8822
2022-03-12 08:41:05 - train: epoch 0080, iter [02500, 05004], lr: 0.023160, loss: 1.4235
2022-03-12 08:41:49 - train: epoch 0080, iter [02600, 05004], lr: 0.023160, loss: 1.6649
2022-03-12 08:42:31 - train: epoch 0080, iter [02700, 05004], lr: 0.023160, loss: 1.7132
2022-03-12 08:43:15 - train: epoch 0080, iter [02800, 05004], lr: 0.023160, loss: 1.6606
2022-03-12 08:43:59 - train: epoch 0080, iter [02900, 05004], lr: 0.023160, loss: 1.8685
2022-03-12 08:44:42 - train: epoch 0080, iter [03000, 05004], lr: 0.023160, loss: 1.5833
2022-03-12 08:45:24 - train: epoch 0080, iter [03100, 05004], lr: 0.023160, loss: 1.9059
2022-03-12 08:46:07 - train: epoch 0080, iter [03200, 05004], lr: 0.023160, loss: 1.4033
2022-03-12 08:46:51 - train: epoch 0080, iter [03300, 05004], lr: 0.023160, loss: 1.6357
2022-03-12 08:47:32 - train: epoch 0080, iter [03400, 05004], lr: 0.023160, loss: 1.5538
2022-03-12 08:48:17 - train: epoch 0080, iter [03500, 05004], lr: 0.023160, loss: 1.8302
2022-03-12 08:49:00 - train: epoch 0080, iter [03600, 05004], lr: 0.023160, loss: 1.7067
2022-03-12 08:49:43 - train: epoch 0080, iter [03700, 05004], lr: 0.023160, loss: 1.4157
2022-03-12 08:50:26 - train: epoch 0080, iter [03800, 05004], lr: 0.023160, loss: 1.7964
2022-03-12 08:51:08 - train: epoch 0080, iter [03900, 05004], lr: 0.023160, loss: 1.5493
2022-03-12 08:51:51 - train: epoch 0080, iter [04000, 05004], lr: 0.023160, loss: 1.7042
2022-03-12 08:52:32 - train: epoch 0080, iter [04100, 05004], lr: 0.023160, loss: 1.9200
2022-03-12 08:53:14 - train: epoch 0080, iter [04200, 05004], lr: 0.023160, loss: 1.6936
2022-03-12 08:53:57 - train: epoch 0080, iter [04300, 05004], lr: 0.023160, loss: 1.9791
2022-03-12 08:54:38 - train: epoch 0080, iter [04400, 05004], lr: 0.023160, loss: 1.6986
2022-03-12 08:55:20 - train: epoch 0080, iter [04500, 05004], lr: 0.023160, loss: 1.6051
2022-03-12 08:56:03 - train: epoch 0080, iter [04600, 05004], lr: 0.023160, loss: 1.5978
2022-03-12 08:56:45 - train: epoch 0080, iter [04700, 05004], lr: 0.023160, loss: 1.4448
2022-03-12 08:57:27 - train: epoch 0080, iter [04800, 05004], lr: 0.023160, loss: 1.7208
2022-03-12 08:58:09 - train: epoch 0080, iter [04900, 05004], lr: 0.023160, loss: 2.0215
2022-03-12 08:58:52 - train: epoch 0080, iter [05000, 05004], lr: 0.023160, loss: 1.6802
2022-03-12 08:58:53 - train: epoch 080, train_loss: 1.6674
2022-03-12 09:00:22 - eval: epoch: 080, acc1: 64.384%, acc5: 86.234%, test_loss: 1.4525, per_image_load_time: 3.168ms, per_image_inference_time: 0.269ms
2022-03-12 09:00:22 - until epoch: 080, best_acc1: 64.384%
2022-03-12 09:00:22 - epoch 081 lr: 0.021085949060360654
2022-03-12 09:01:06 - train: epoch 0081, iter [00100, 05004], lr: 0.021086, loss: 1.5673
2022-03-12 09:01:47 - train: epoch 0081, iter [00200, 05004], lr: 0.021086, loss: 1.5231
2022-03-12 09:02:30 - train: epoch 0081, iter [00300, 05004], lr: 0.021086, loss: 1.8240
2022-03-12 09:03:12 - train: epoch 0081, iter [00400, 05004], lr: 0.021086, loss: 1.7729
2022-03-12 09:03:52 - train: epoch 0081, iter [00500, 05004], lr: 0.021086, loss: 1.7971
2022-03-12 09:04:36 - train: epoch 0081, iter [00600, 05004], lr: 0.021086, loss: 1.8107
2022-03-12 09:05:19 - train: epoch 0081, iter [00700, 05004], lr: 0.021086, loss: 1.6251
2022-03-12 09:06:02 - train: epoch 0081, iter [00800, 05004], lr: 0.021086, loss: 1.6769
2022-03-12 09:06:45 - train: epoch 0081, iter [00900, 05004], lr: 0.021086, loss: 1.8331
2022-03-12 09:07:28 - train: epoch 0081, iter [01000, 05004], lr: 0.021086, loss: 1.6996
2022-03-12 09:08:09 - train: epoch 0081, iter [01100, 05004], lr: 0.021086, loss: 1.5525
2022-03-12 09:08:52 - train: epoch 0081, iter [01200, 05004], lr: 0.021086, loss: 1.6229
2022-03-12 09:09:35 - train: epoch 0081, iter [01300, 05004], lr: 0.021086, loss: 1.6497
2022-03-12 09:10:18 - train: epoch 0081, iter [01400, 05004], lr: 0.021086, loss: 1.4654
2022-03-12 09:11:02 - train: epoch 0081, iter [01500, 05004], lr: 0.021086, loss: 1.6971
2022-03-12 09:11:45 - train: epoch 0081, iter [01600, 05004], lr: 0.021086, loss: 1.4745
2022-03-12 09:12:30 - train: epoch 0081, iter [01700, 05004], lr: 0.021086, loss: 1.4676
2022-03-12 09:13:14 - train: epoch 0081, iter [01800, 05004], lr: 0.021086, loss: 1.6410
2022-03-12 09:13:58 - train: epoch 0081, iter [01900, 05004], lr: 0.021086, loss: 1.6463
2022-03-12 09:14:41 - train: epoch 0081, iter [02000, 05004], lr: 0.021086, loss: 1.7021
2022-03-12 09:15:24 - train: epoch 0081, iter [02100, 05004], lr: 0.021086, loss: 1.5626
2022-03-12 09:16:06 - train: epoch 0081, iter [02200, 05004], lr: 0.021086, loss: 1.5812
2022-03-12 09:16:48 - train: epoch 0081, iter [02300, 05004], lr: 0.021086, loss: 1.5458
2022-03-12 09:17:30 - train: epoch 0081, iter [02400, 05004], lr: 0.021086, loss: 1.6525
2022-03-12 09:18:12 - train: epoch 0081, iter [02500, 05004], lr: 0.021086, loss: 1.9237
2022-03-12 09:18:54 - train: epoch 0081, iter [02600, 05004], lr: 0.021086, loss: 1.6223
2022-03-12 09:19:35 - train: epoch 0081, iter [02700, 05004], lr: 0.021086, loss: 1.8370
2022-03-12 09:20:17 - train: epoch 0081, iter [02800, 05004], lr: 0.021086, loss: 1.4868
2022-03-12 09:20:59 - train: epoch 0081, iter [02900, 05004], lr: 0.021086, loss: 1.6350
2022-03-12 09:21:42 - train: epoch 0081, iter [03000, 05004], lr: 0.021086, loss: 1.6082
2022-03-12 09:22:23 - train: epoch 0081, iter [03100, 05004], lr: 0.021086, loss: 1.5874
2022-03-12 09:23:06 - train: epoch 0081, iter [03200, 05004], lr: 0.021086, loss: 1.6795
2022-03-12 09:23:48 - train: epoch 0081, iter [03300, 05004], lr: 0.021086, loss: 1.3952
2022-03-12 09:24:28 - train: epoch 0081, iter [03400, 05004], lr: 0.021086, loss: 1.6371
2022-03-12 09:25:08 - train: epoch 0081, iter [03500, 05004], lr: 0.021086, loss: 1.8535
2022-03-12 09:25:50 - train: epoch 0081, iter [03600, 05004], lr: 0.021086, loss: 1.3469
2022-03-12 09:26:32 - train: epoch 0081, iter [03700, 05004], lr: 0.021086, loss: 1.7105
2022-03-12 09:27:14 - train: epoch 0081, iter [03800, 05004], lr: 0.021086, loss: 1.7044
2022-03-12 09:27:55 - train: epoch 0081, iter [03900, 05004], lr: 0.021086, loss: 1.8417
2022-03-12 09:28:39 - train: epoch 0081, iter [04000, 05004], lr: 0.021086, loss: 1.5155
2022-03-12 09:29:24 - train: epoch 0081, iter [04100, 05004], lr: 0.021086, loss: 1.6045
2022-03-12 09:30:07 - train: epoch 0081, iter [04200, 05004], lr: 0.021086, loss: 1.7173
2022-03-12 09:30:50 - train: epoch 0081, iter [04300, 05004], lr: 0.021086, loss: 1.6903
2022-03-12 09:31:32 - train: epoch 0081, iter [04400, 05004], lr: 0.021086, loss: 1.6918
2022-03-12 09:32:13 - train: epoch 0081, iter [04500, 05004], lr: 0.021086, loss: 1.7894
2022-03-12 09:32:55 - train: epoch 0081, iter [04600, 05004], lr: 0.021086, loss: 1.6753
2022-03-12 09:33:37 - train: epoch 0081, iter [04700, 05004], lr: 0.021086, loss: 1.6850
2022-03-12 09:34:19 - train: epoch 0081, iter [04800, 05004], lr: 0.021086, loss: 1.5938
2022-03-12 09:35:00 - train: epoch 0081, iter [04900, 05004], lr: 0.021086, loss: 1.7521
2022-03-12 09:35:41 - train: epoch 0081, iter [05000, 05004], lr: 0.021086, loss: 1.4900
2022-03-12 09:35:42 - train: epoch 081, train_loss: 1.6411
2022-03-12 09:37:09 - eval: epoch: 081, acc1: 65.096%, acc5: 86.596%, test_loss: 1.4266, per_image_load_time: 2.404ms, per_image_inference_time: 0.263ms
2022-03-12 09:37:09 - until epoch: 081, best_acc1: 65.096%
2022-03-12 09:37:09 - epoch 082 lr: 0.019098300562505267
2022-03-12 09:37:55 - train: epoch 0082, iter [00100, 05004], lr: 0.019098, loss: 1.4771
2022-03-12 09:38:37 - train: epoch 0082, iter [00200, 05004], lr: 0.019098, loss: 1.3541
2022-03-12 09:39:20 - train: epoch 0082, iter [00300, 05004], lr: 0.019098, loss: 1.6786
2022-03-12 09:40:02 - train: epoch 0082, iter [00400, 05004], lr: 0.019098, loss: 1.6011
2022-03-12 09:40:44 - train: epoch 0082, iter [00500, 05004], lr: 0.019098, loss: 1.6241
2022-03-12 09:41:27 - train: epoch 0082, iter [00600, 05004], lr: 0.019098, loss: 1.5277
2022-03-12 09:42:09 - train: epoch 0082, iter [00700, 05004], lr: 0.019098, loss: 1.8297
2022-03-12 09:42:52 - train: epoch 0082, iter [00800, 05004], lr: 0.019098, loss: 1.6361
2022-03-12 09:43:33 - train: epoch 0082, iter [00900, 05004], lr: 0.019098, loss: 1.8012
2022-03-12 09:44:15 - train: epoch 0082, iter [01000, 05004], lr: 0.019098, loss: 1.5100
2022-03-12 09:44:58 - train: epoch 0082, iter [01100, 05004], lr: 0.019098, loss: 1.6946
2022-03-12 09:45:40 - train: epoch 0082, iter [01200, 05004], lr: 0.019098, loss: 1.6092
2022-03-12 09:46:22 - train: epoch 0082, iter [01300, 05004], lr: 0.019098, loss: 1.7409
2022-03-12 09:47:05 - train: epoch 0082, iter [01400, 05004], lr: 0.019098, loss: 1.5475
2022-03-12 09:47:46 - train: epoch 0082, iter [01500, 05004], lr: 0.019098, loss: 1.6396
2022-03-12 09:48:28 - train: epoch 0082, iter [01600, 05004], lr: 0.019098, loss: 1.6666
2022-03-12 09:49:08 - train: epoch 0082, iter [01700, 05004], lr: 0.019098, loss: 1.6627
2022-03-12 09:49:49 - train: epoch 0082, iter [01800, 05004], lr: 0.019098, loss: 1.2618
2022-03-12 09:50:30 - train: epoch 0082, iter [01900, 05004], lr: 0.019098, loss: 1.6160
2022-03-12 09:51:12 - train: epoch 0082, iter [02000, 05004], lr: 0.019098, loss: 1.4921
2022-03-12 09:51:53 - train: epoch 0082, iter [02100, 05004], lr: 0.019098, loss: 1.5976
2022-03-12 09:52:34 - train: epoch 0082, iter [02200, 05004], lr: 0.019098, loss: 1.6853
2022-03-12 09:53:16 - train: epoch 0082, iter [02300, 05004], lr: 0.019098, loss: 1.5558
2022-03-12 09:53:57 - train: epoch 0082, iter [02400, 05004], lr: 0.019098, loss: 1.7255
2022-03-12 09:54:39 - train: epoch 0082, iter [02500, 05004], lr: 0.019098, loss: 1.5347
2022-03-12 09:55:21 - train: epoch 0082, iter [02600, 05004], lr: 0.019098, loss: 1.4968
2022-03-12 09:56:04 - train: epoch 0082, iter [02700, 05004], lr: 0.019098, loss: 1.4128
2022-03-12 09:56:46 - train: epoch 0082, iter [02800, 05004], lr: 0.019098, loss: 1.2993
2022-03-12 09:57:28 - train: epoch 0082, iter [02900, 05004], lr: 0.019098, loss: 1.4162
2022-03-12 09:58:10 - train: epoch 0082, iter [03000, 05004], lr: 0.019098, loss: 1.4569
2022-03-12 09:58:51 - train: epoch 0082, iter [03100, 05004], lr: 0.019098, loss: 1.7717
2022-03-12 09:59:34 - train: epoch 0082, iter [03200, 05004], lr: 0.019098, loss: 1.5271
2022-03-12 10:00:15 - train: epoch 0082, iter [03300, 05004], lr: 0.019098, loss: 1.5810
2022-03-12 10:00:57 - train: epoch 0082, iter [03400, 05004], lr: 0.019098, loss: 1.4778
2022-03-12 10:01:38 - train: epoch 0082, iter [03500, 05004], lr: 0.019098, loss: 1.5283
2022-03-12 10:02:20 - train: epoch 0082, iter [03600, 05004], lr: 0.019098, loss: 1.7497
2022-03-12 10:03:01 - train: epoch 0082, iter [03700, 05004], lr: 0.019098, loss: 1.5962
2022-03-12 10:03:43 - train: epoch 0082, iter [03800, 05004], lr: 0.019098, loss: 1.6004
2022-03-12 10:04:24 - train: epoch 0082, iter [03900, 05004], lr: 0.019098, loss: 1.5872
2022-03-12 10:05:06 - train: epoch 0082, iter [04000, 05004], lr: 0.019098, loss: 1.5796
2022-03-12 10:05:47 - train: epoch 0082, iter [04100, 05004], lr: 0.019098, loss: 1.7521
2022-03-12 10:06:28 - train: epoch 0082, iter [04200, 05004], lr: 0.019098, loss: 1.6386
2022-03-12 10:07:11 - train: epoch 0082, iter [04300, 05004], lr: 0.019098, loss: 1.4459
2022-03-12 10:07:51 - train: epoch 0082, iter [04400, 05004], lr: 0.019098, loss: 1.5333
2022-03-12 10:08:33 - train: epoch 0082, iter [04500, 05004], lr: 0.019098, loss: 1.4550
2022-03-12 10:09:15 - train: epoch 0082, iter [04600, 05004], lr: 0.019098, loss: 1.7987
2022-03-12 10:09:57 - train: epoch 0082, iter [04700, 05004], lr: 0.019098, loss: 1.6046
2022-03-12 10:10:37 - train: epoch 0082, iter [04800, 05004], lr: 0.019098, loss: 1.4718
2022-03-12 10:11:19 - train: epoch 0082, iter [04900, 05004], lr: 0.019098, loss: 1.5054
2022-03-12 10:12:00 - train: epoch 0082, iter [05000, 05004], lr: 0.019098, loss: 1.4838
2022-03-12 10:12:01 - train: epoch 082, train_loss: 1.6176
2022-03-12 10:13:26 - eval: epoch: 082, acc1: 65.824%, acc5: 87.012%, test_loss: 1.3933, per_image_load_time: 1.400ms, per_image_inference_time: 0.257ms
2022-03-12 10:13:26 - until epoch: 082, best_acc1: 65.824%
2022-03-12 10:13:26 - epoch 083 lr: 0.017199116885197995
2022-03-12 10:14:10 - train: epoch 0083, iter [00100, 05004], lr: 0.017199, loss: 1.5319
2022-03-12 10:14:52 - train: epoch 0083, iter [00200, 05004], lr: 0.017199, loss: 1.3796
2022-03-12 10:15:34 - train: epoch 0083, iter [00300, 05004], lr: 0.017199, loss: 1.5497
2022-03-12 10:16:15 - train: epoch 0083, iter [00400, 05004], lr: 0.017199, loss: 1.6087
2022-03-12 10:16:56 - train: epoch 0083, iter [00500, 05004], lr: 0.017199, loss: 1.5483
2022-03-12 10:17:39 - train: epoch 0083, iter [00600, 05004], lr: 0.017199, loss: 1.4500
2022-03-12 10:18:19 - train: epoch 0083, iter [00700, 05004], lr: 0.017199, loss: 1.7177
2022-03-12 10:19:01 - train: epoch 0083, iter [00800, 05004], lr: 0.017199, loss: 1.4257
2022-03-12 10:19:44 - train: epoch 0083, iter [00900, 05004], lr: 0.017199, loss: 1.7368
2022-03-12 10:20:26 - train: epoch 0083, iter [01000, 05004], lr: 0.017199, loss: 1.7353
2022-03-12 10:21:09 - train: epoch 0083, iter [01100, 05004], lr: 0.017199, loss: 1.7076
2022-03-12 10:21:51 - train: epoch 0083, iter [01200, 05004], lr: 0.017199, loss: 1.4935
2022-03-12 10:22:34 - train: epoch 0083, iter [01300, 05004], lr: 0.017199, loss: 1.7227
2022-03-12 10:23:17 - train: epoch 0083, iter [01400, 05004], lr: 0.017199, loss: 1.6661
2022-03-12 10:23:59 - train: epoch 0083, iter [01500, 05004], lr: 0.017199, loss: 1.5990
2022-03-12 10:24:41 - train: epoch 0083, iter [01600, 05004], lr: 0.017199, loss: 1.3845
2022-03-12 10:25:22 - train: epoch 0083, iter [01700, 05004], lr: 0.017199, loss: 1.8183
2022-03-12 10:26:04 - train: epoch 0083, iter [01800, 05004], lr: 0.017199, loss: 1.9359
2022-03-12 10:26:46 - train: epoch 0083, iter [01900, 05004], lr: 0.017199, loss: 1.6450
2022-03-12 10:27:27 - train: epoch 0083, iter [02000, 05004], lr: 0.017199, loss: 1.4484
2022-03-12 10:28:08 - train: epoch 0083, iter [02100, 05004], lr: 0.017199, loss: 1.5495
2022-03-12 10:28:48 - train: epoch 0083, iter [02200, 05004], lr: 0.017199, loss: 1.2586
2022-03-12 10:29:30 - train: epoch 0083, iter [02300, 05004], lr: 0.017199, loss: 1.6922
2022-03-12 10:30:12 - train: epoch 0083, iter [02400, 05004], lr: 0.017199, loss: 1.5707
2022-03-12 10:30:53 - train: epoch 0083, iter [02500, 05004], lr: 0.017199, loss: 1.4790
2022-03-12 10:31:34 - train: epoch 0083, iter [02600, 05004], lr: 0.017199, loss: 1.6011
2022-03-12 10:32:16 - train: epoch 0083, iter [02700, 05004], lr: 0.017199, loss: 1.5362
2022-03-12 10:32:59 - train: epoch 0083, iter [02800, 05004], lr: 0.017199, loss: 1.6270
2022-03-12 10:33:41 - train: epoch 0083, iter [02900, 05004], lr: 0.017199, loss: 1.8231
2022-03-12 10:34:22 - train: epoch 0083, iter [03000, 05004], lr: 0.017199, loss: 1.8040
2022-03-12 10:35:03 - train: epoch 0083, iter [03100, 05004], lr: 0.017199, loss: 1.5643
2022-03-12 10:35:45 - train: epoch 0083, iter [03200, 05004], lr: 0.017199, loss: 1.5522
2022-03-12 10:36:27 - train: epoch 0083, iter [03300, 05004], lr: 0.017199, loss: 1.6842
2022-03-12 10:37:08 - train: epoch 0083, iter [03400, 05004], lr: 0.017199, loss: 1.6009
2022-03-12 10:37:50 - train: epoch 0083, iter [03500, 05004], lr: 0.017199, loss: 1.6223
2022-03-12 10:38:29 - train: epoch 0083, iter [03600, 05004], lr: 0.017199, loss: 1.7594
2022-03-12 10:39:10 - train: epoch 0083, iter [03700, 05004], lr: 0.017199, loss: 1.3950
2022-03-12 10:39:51 - train: epoch 0083, iter [03800, 05004], lr: 0.017199, loss: 1.4779
2022-03-12 10:40:33 - train: epoch 0083, iter [03900, 05004], lr: 0.017199, loss: 1.6226
2022-03-12 10:41:14 - train: epoch 0083, iter [04000, 05004], lr: 0.017199, loss: 1.9136
2022-03-12 10:41:56 - train: epoch 0083, iter [04100, 05004], lr: 0.017199, loss: 1.7077
2022-03-12 10:42:38 - train: epoch 0083, iter [04200, 05004], lr: 0.017199, loss: 1.8327
2022-03-12 10:43:20 - train: epoch 0083, iter [04300, 05004], lr: 0.017199, loss: 1.4486
2022-03-12 10:44:02 - train: epoch 0083, iter [04400, 05004], lr: 0.017199, loss: 1.7002
2022-03-12 10:44:44 - train: epoch 0083, iter [04500, 05004], lr: 0.017199, loss: 1.6304
2022-03-12 10:45:27 - train: epoch 0083, iter [04600, 05004], lr: 0.017199, loss: 1.5606
2022-03-12 10:46:10 - train: epoch 0083, iter [04700, 05004], lr: 0.017199, loss: 1.9464
2022-03-12 10:46:52 - train: epoch 0083, iter [04800, 05004], lr: 0.017199, loss: 1.6835
2022-03-12 10:47:35 - train: epoch 0083, iter [04900, 05004], lr: 0.017199, loss: 1.7423
2022-03-12 10:48:18 - train: epoch 0083, iter [05000, 05004], lr: 0.017199, loss: 1.6240
2022-03-12 10:48:20 - train: epoch 083, train_loss: 1.5937
2022-03-12 10:49:49 - eval: epoch: 083, acc1: 65.832%, acc5: 87.188%, test_loss: 1.3796, per_image_load_time: 1.149ms, per_image_inference_time: 0.261ms
2022-03-12 10:49:49 - until epoch: 083, best_acc1: 65.832%
2022-03-12 10:49:49 - epoch 084 lr: 0.015390474757906447
2022-03-12 10:50:34 - train: epoch 0084, iter [00100, 05004], lr: 0.015390, loss: 1.4768
2022-03-12 10:51:17 - train: epoch 0084, iter [00200, 05004], lr: 0.015390, loss: 1.6489
2022-03-12 10:51:59 - train: epoch 0084, iter [00300, 05004], lr: 0.015390, loss: 1.4930
2022-03-12 10:52:41 - train: epoch 0084, iter [00400, 05004], lr: 0.015390, loss: 1.6204
2022-03-12 10:53:23 - train: epoch 0084, iter [00500, 05004], lr: 0.015390, loss: 1.6679
2022-03-12 10:54:06 - train: epoch 0084, iter [00600, 05004], lr: 0.015390, loss: 1.7856
2022-03-12 10:54:48 - train: epoch 0084, iter [00700, 05004], lr: 0.015390, loss: 1.6735
2022-03-12 10:55:30 - train: epoch 0084, iter [00800, 05004], lr: 0.015390, loss: 1.6618
2022-03-12 10:56:12 - train: epoch 0084, iter [00900, 05004], lr: 0.015390, loss: 1.5666
2022-03-12 10:56:54 - train: epoch 0084, iter [01000, 05004], lr: 0.015390, loss: 1.4989
2022-03-12 10:57:36 - train: epoch 0084, iter [01100, 05004], lr: 0.015390, loss: 1.4941
2022-03-12 10:58:19 - train: epoch 0084, iter [01200, 05004], lr: 0.015390, loss: 1.6700
2022-03-12 10:59:01 - train: epoch 0084, iter [01300, 05004], lr: 0.015390, loss: 1.6940
2022-03-12 10:59:43 - train: epoch 0084, iter [01400, 05004], lr: 0.015390, loss: 1.6380
2022-03-12 11:00:25 - train: epoch 0084, iter [01500, 05004], lr: 0.015390, loss: 1.6216
2022-03-12 11:01:05 - train: epoch 0084, iter [01600, 05004], lr: 0.015390, loss: 1.5915
2022-03-12 11:01:47 - train: epoch 0084, iter [01700, 05004], lr: 0.015390, loss: 1.7396
2022-03-12 11:02:27 - train: epoch 0084, iter [01800, 05004], lr: 0.015390, loss: 1.7402
2022-03-12 11:03:06 - train: epoch 0084, iter [01900, 05004], lr: 0.015390, loss: 1.5589
2022-03-12 11:03:47 - train: epoch 0084, iter [02000, 05004], lr: 0.015390, loss: 1.4236
2022-03-12 11:04:29 - train: epoch 0084, iter [02100, 05004], lr: 0.015390, loss: 1.5058
2022-03-12 11:05:11 - train: epoch 0084, iter [02200, 05004], lr: 0.015390, loss: 1.3335
2022-03-12 11:05:53 - train: epoch 0084, iter [02300, 05004], lr: 0.015390, loss: 1.5586
2022-03-12 11:06:33 - train: epoch 0084, iter [02400, 05004], lr: 0.015390, loss: 1.4102
2022-03-12 11:07:15 - train: epoch 0084, iter [02500, 05004], lr: 0.015390, loss: 1.6108
2022-03-12 11:07:56 - train: epoch 0084, iter [02600, 05004], lr: 0.015390, loss: 1.5218
2022-03-12 11:08:39 - train: epoch 0084, iter [02700, 05004], lr: 0.015390, loss: 1.7162
2022-03-12 11:09:21 - train: epoch 0084, iter [02800, 05004], lr: 0.015390, loss: 1.3404
2022-03-12 11:10:03 - train: epoch 0084, iter [02900, 05004], lr: 0.015390, loss: 1.5963
2022-03-12 11:10:43 - train: epoch 0084, iter [03000, 05004], lr: 0.015390, loss: 1.5387
2022-03-12 11:11:21 - train: epoch 0084, iter [03100, 05004], lr: 0.015390, loss: 1.5473
2022-03-12 11:11:58 - train: epoch 0084, iter [03200, 05004], lr: 0.015390, loss: 1.4375
2022-03-12 11:12:35 - train: epoch 0084, iter [03300, 05004], lr: 0.015390, loss: 1.5586
2022-03-12 11:13:12 - train: epoch 0084, iter [03400, 05004], lr: 0.015390, loss: 1.5798
2022-03-12 11:13:49 - train: epoch 0084, iter [03500, 05004], lr: 0.015390, loss: 1.5033
2022-03-12 11:14:27 - train: epoch 0084, iter [03600, 05004], lr: 0.015390, loss: 1.3985
2022-03-12 11:15:06 - train: epoch 0084, iter [03700, 05004], lr: 0.015390, loss: 1.7887
2022-03-12 11:15:44 - train: epoch 0084, iter [03800, 05004], lr: 0.015390, loss: 1.6812
2022-03-12 11:16:22 - train: epoch 0084, iter [03900, 05004], lr: 0.015390, loss: 1.5890
2022-03-12 11:17:01 - train: epoch 0084, iter [04000, 05004], lr: 0.015390, loss: 1.5796
2022-03-12 11:17:39 - train: epoch 0084, iter [04100, 05004], lr: 0.015390, loss: 1.4872
2022-03-12 11:18:17 - train: epoch 0084, iter [04200, 05004], lr: 0.015390, loss: 1.6261
2022-03-12 11:18:56 - train: epoch 0084, iter [04300, 05004], lr: 0.015390, loss: 1.7119
2022-03-12 11:19:34 - train: epoch 0084, iter [04400, 05004], lr: 0.015390, loss: 1.8006
2022-03-12 11:20:12 - train: epoch 0084, iter [04500, 05004], lr: 0.015390, loss: 1.4900
2022-03-12 11:20:51 - train: epoch 0084, iter [04600, 05004], lr: 0.015390, loss: 1.4379
2022-03-12 11:21:33 - train: epoch 0084, iter [04700, 05004], lr: 0.015390, loss: 1.6999
2022-03-12 11:22:16 - train: epoch 0084, iter [04800, 05004], lr: 0.015390, loss: 1.3935
2022-03-12 11:22:59 - train: epoch 0084, iter [04900, 05004], lr: 0.015390, loss: 1.5423
2022-03-12 11:23:42 - train: epoch 0084, iter [05000, 05004], lr: 0.015390, loss: 1.5387
2022-03-12 11:23:44 - train: epoch 084, train_loss: 1.5684
2022-03-12 11:25:14 - eval: epoch: 084, acc1: 66.408%, acc5: 87.598%, test_loss: 1.3542, per_image_load_time: 3.300ms, per_image_inference_time: 0.258ms
2022-03-12 11:25:14 - until epoch: 084, best_acc1: 66.408%
2022-03-12 11:25:14 - epoch 085 lr: 0.01367435190424261
2022-03-12 11:26:00 - train: epoch 0085, iter [00100, 05004], lr: 0.013674, loss: 1.4608
2022-03-12 11:26:41 - train: epoch 0085, iter [00200, 05004], lr: 0.013674, loss: 1.3081
2022-03-12 11:27:24 - train: epoch 0085, iter [00300, 05004], lr: 0.013674, loss: 1.6838
2022-03-12 11:28:07 - train: epoch 0085, iter [00400, 05004], lr: 0.013674, loss: 1.5805
2022-03-12 11:28:49 - train: epoch 0085, iter [00500, 05004], lr: 0.013674, loss: 1.5690
2022-03-12 11:29:32 - train: epoch 0085, iter [00600, 05004], lr: 0.013674, loss: 1.4183
2022-03-12 11:30:17 - train: epoch 0085, iter [00700, 05004], lr: 0.013674, loss: 1.6537
2022-03-12 11:31:01 - train: epoch 0085, iter [00800, 05004], lr: 0.013674, loss: 1.4008
2022-03-12 11:31:44 - train: epoch 0085, iter [00900, 05004], lr: 0.013674, loss: 1.4432
2022-03-12 11:32:26 - train: epoch 0085, iter [01000, 05004], lr: 0.013674, loss: 1.4419
2022-03-12 11:33:09 - train: epoch 0085, iter [01100, 05004], lr: 0.013674, loss: 1.5658
2022-03-12 11:33:50 - train: epoch 0085, iter [01200, 05004], lr: 0.013674, loss: 1.6168
2022-03-12 11:34:33 - train: epoch 0085, iter [01300, 05004], lr: 0.013674, loss: 1.5678
2022-03-12 11:35:15 - train: epoch 0085, iter [01400, 05004], lr: 0.013674, loss: 1.4844
2022-03-12 11:35:58 - train: epoch 0085, iter [01500, 05004], lr: 0.013674, loss: 1.4699
2022-03-12 11:36:42 - train: epoch 0085, iter [01600, 05004], lr: 0.013674, loss: 1.5487
2022-03-12 11:37:26 - train: epoch 0085, iter [01700, 05004], lr: 0.013674, loss: 1.7396
2022-03-12 11:38:11 - train: epoch 0085, iter [01800, 05004], lr: 0.013674, loss: 1.5738
2022-03-12 11:38:55 - train: epoch 0085, iter [01900, 05004], lr: 0.013674, loss: 1.4282
2022-03-12 11:39:40 - train: epoch 0085, iter [02000, 05004], lr: 0.013674, loss: 1.4118
2022-03-12 11:40:24 - train: epoch 0085, iter [02100, 05004], lr: 0.013674, loss: 1.5001
2022-03-12 11:41:09 - train: epoch 0085, iter [02200, 05004], lr: 0.013674, loss: 1.6339
2022-03-12 11:41:54 - train: epoch 0085, iter [02300, 05004], lr: 0.013674, loss: 1.5540
2022-03-12 11:42:40 - train: epoch 0085, iter [02400, 05004], lr: 0.013674, loss: 1.6758
2022-03-12 11:43:26 - train: epoch 0085, iter [02500, 05004], lr: 0.013674, loss: 1.6261
2022-03-12 11:44:10 - train: epoch 0085, iter [02600, 05004], lr: 0.013674, loss: 1.4187
2022-03-12 11:44:55 - train: epoch 0085, iter [02700, 05004], lr: 0.013674, loss: 1.3621
2022-03-12 11:45:40 - train: epoch 0085, iter [02800, 05004], lr: 0.013674, loss: 1.7573
2022-03-12 11:46:25 - train: epoch 0085, iter [02900, 05004], lr: 0.013674, loss: 1.5447
2022-03-12 11:47:11 - train: epoch 0085, iter [03000, 05004], lr: 0.013674, loss: 1.6662
2022-03-12 11:47:54 - train: epoch 0085, iter [03100, 05004], lr: 0.013674, loss: 1.5669
2022-03-12 11:48:37 - train: epoch 0085, iter [03200, 05004], lr: 0.013674, loss: 1.6489
2022-03-12 11:49:21 - train: epoch 0085, iter [03300, 05004], lr: 0.013674, loss: 1.5905
2022-03-12 11:50:04 - train: epoch 0085, iter [03400, 05004], lr: 0.013674, loss: 1.6852
2022-03-12 11:50:51 - train: epoch 0085, iter [03500, 05004], lr: 0.013674, loss: 1.6104
2022-03-12 11:51:35 - train: epoch 0085, iter [03600, 05004], lr: 0.013674, loss: 1.6006
2022-03-12 11:52:20 - train: epoch 0085, iter [03700, 05004], lr: 0.013674, loss: 1.4304
2022-03-12 11:53:04 - train: epoch 0085, iter [03800, 05004], lr: 0.013674, loss: 1.6818
2022-03-12 11:53:47 - train: epoch 0085, iter [03900, 05004], lr: 0.013674, loss: 1.5700
2022-03-12 11:54:32 - train: epoch 0085, iter [04000, 05004], lr: 0.013674, loss: 1.7184
2022-03-12 11:55:15 - train: epoch 0085, iter [04100, 05004], lr: 0.013674, loss: 1.6452
2022-03-12 11:55:58 - train: epoch 0085, iter [04200, 05004], lr: 0.013674, loss: 1.5188
2022-03-12 11:56:41 - train: epoch 0085, iter [04300, 05004], lr: 0.013674, loss: 1.4756
2022-03-12 11:57:25 - train: epoch 0085, iter [04400, 05004], lr: 0.013674, loss: 1.5817
2022-03-12 11:58:09 - train: epoch 0085, iter [04500, 05004], lr: 0.013674, loss: 1.5156
2022-03-12 11:58:52 - train: epoch 0085, iter [04600, 05004], lr: 0.013674, loss: 1.6443
2022-03-12 11:59:35 - train: epoch 0085, iter [04700, 05004], lr: 0.013674, loss: 1.8219
2022-03-12 12:00:19 - train: epoch 0085, iter [04800, 05004], lr: 0.013674, loss: 1.4304
2022-03-12 12:01:03 - train: epoch 0085, iter [04900, 05004], lr: 0.013674, loss: 1.5518
2022-03-12 12:01:45 - train: epoch 0085, iter [05000, 05004], lr: 0.013674, loss: 1.2576
2022-03-12 12:01:47 - train: epoch 085, train_loss: 1.5430
2022-03-12 12:03:18 - eval: epoch: 085, acc1: 66.510%, acc5: 87.568%, test_loss: 1.3574, per_image_load_time: 3.306ms, per_image_inference_time: 0.272ms
2022-03-12 12:03:18 - until epoch: 085, best_acc1: 66.510%
2022-03-12 12:03:18 - epoch 086 lr: 0.012052624879351104
2022-03-12 12:04:04 - train: epoch 0086, iter [00100, 05004], lr: 0.012053, loss: 1.3582
2022-03-12 12:04:47 - train: epoch 0086, iter [00200, 05004], lr: 0.012053, loss: 1.6592
2022-03-12 12:05:30 - train: epoch 0086, iter [00300, 05004], lr: 0.012053, loss: 1.6299
2022-03-12 12:06:12 - train: epoch 0086, iter [00400, 05004], lr: 0.012053, loss: 1.5415
2022-03-12 12:06:55 - train: epoch 0086, iter [00500, 05004], lr: 0.012053, loss: 1.6669
2022-03-12 12:07:37 - train: epoch 0086, iter [00600, 05004], lr: 0.012053, loss: 1.2943
2022-03-12 12:08:21 - train: epoch 0086, iter [00700, 05004], lr: 0.012053, loss: 1.4411
2022-03-12 12:09:04 - train: epoch 0086, iter [00800, 05004], lr: 0.012053, loss: 1.5852
2022-03-12 12:09:47 - train: epoch 0086, iter [00900, 05004], lr: 0.012053, loss: 1.4247
2022-03-12 12:10:31 - train: epoch 0086, iter [01000, 05004], lr: 0.012053, loss: 1.6486
2022-03-12 12:11:13 - train: epoch 0086, iter [01100, 05004], lr: 0.012053, loss: 1.6465
2022-03-12 12:11:56 - train: epoch 0086, iter [01200, 05004], lr: 0.012053, loss: 1.4614
2022-03-12 12:12:38 - train: epoch 0086, iter [01300, 05004], lr: 0.012053, loss: 1.6224
2022-03-12 12:13:19 - train: epoch 0086, iter [01400, 05004], lr: 0.012053, loss: 1.5598
2022-03-12 12:14:02 - train: epoch 0086, iter [01500, 05004], lr: 0.012053, loss: 1.3999
2022-03-12 12:14:44 - train: epoch 0086, iter [01600, 05004], lr: 0.012053, loss: 1.4406
2022-03-12 12:15:26 - train: epoch 0086, iter [01700, 05004], lr: 0.012053, loss: 1.3891
2022-03-12 12:16:04 - train: epoch 0086, iter [01800, 05004], lr: 0.012053, loss: 1.5480
2022-03-12 12:16:44 - train: epoch 0086, iter [01900, 05004], lr: 0.012053, loss: 1.4887
2022-03-12 12:17:25 - train: epoch 0086, iter [02000, 05004], lr: 0.012053, loss: 1.6015
2022-03-12 12:18:05 - train: epoch 0086, iter [02100, 05004], lr: 0.012053, loss: 1.4594
2022-03-12 12:18:49 - train: epoch 0086, iter [02200, 05004], lr: 0.012053, loss: 1.6047
2022-03-12 12:19:30 - train: epoch 0086, iter [02300, 05004], lr: 0.012053, loss: 1.3924
2022-03-12 12:20:12 - train: epoch 0086, iter [02400, 05004], lr: 0.012053, loss: 1.4863
2022-03-12 12:20:54 - train: epoch 0086, iter [02500, 05004], lr: 0.012053, loss: 1.5321
2022-03-12 12:21:37 - train: epoch 0086, iter [02600, 05004], lr: 0.012053, loss: 1.5541
2022-03-12 12:22:19 - train: epoch 0086, iter [02700, 05004], lr: 0.012053, loss: 1.4249
2022-03-12 12:23:02 - train: epoch 0086, iter [02800, 05004], lr: 0.012053, loss: 1.6097
2022-03-12 12:23:42 - train: epoch 0086, iter [02900, 05004], lr: 0.012053, loss: 1.4055
2022-03-12 12:24:23 - train: epoch 0086, iter [03000, 05004], lr: 0.012053, loss: 1.3125
2022-03-12 12:25:06 - train: epoch 0086, iter [03100, 05004], lr: 0.012053, loss: 1.5447
2022-03-12 12:25:48 - train: epoch 0086, iter [03200, 05004], lr: 0.012053, loss: 1.8545
2022-03-12 12:26:29 - train: epoch 0086, iter [03300, 05004], lr: 0.012053, loss: 1.5864
2022-03-12 12:27:10 - train: epoch 0086, iter [03400, 05004], lr: 0.012053, loss: 1.3811
2022-03-12 12:27:52 - train: epoch 0086, iter [03500, 05004], lr: 0.012053, loss: 1.5602
2022-03-12 12:28:32 - train: epoch 0086, iter [03600, 05004], lr: 0.012053, loss: 1.6420
2022-03-12 12:29:13 - train: epoch 0086, iter [03700, 05004], lr: 0.012053, loss: 1.4348
2022-03-12 12:29:52 - train: epoch 0086, iter [03800, 05004], lr: 0.012053, loss: 1.5993
2022-03-12 12:30:34 - train: epoch 0086, iter [03900, 05004], lr: 0.012053, loss: 1.5198
2022-03-12 12:31:16 - train: epoch 0086, iter [04000, 05004], lr: 0.012053, loss: 1.4965
2022-03-12 12:31:57 - train: epoch 0086, iter [04100, 05004], lr: 0.012053, loss: 1.3639
2022-03-12 12:32:42 - train: epoch 0086, iter [04200, 05004], lr: 0.012053, loss: 1.4056
2022-03-12 12:33:25 - train: epoch 0086, iter [04300, 05004], lr: 0.012053, loss: 1.4642
2022-03-12 12:34:06 - train: epoch 0086, iter [04400, 05004], lr: 0.012053, loss: 1.6554
2022-03-12 12:34:48 - train: epoch 0086, iter [04500, 05004], lr: 0.012053, loss: 1.4726
2022-03-12 12:35:30 - train: epoch 0086, iter [04600, 05004], lr: 0.012053, loss: 1.5008
2022-03-12 12:36:11 - train: epoch 0086, iter [04700, 05004], lr: 0.012053, loss: 1.4289
2022-03-12 12:36:52 - train: epoch 0086, iter [04800, 05004], lr: 0.012053, loss: 1.4708
2022-03-12 12:37:33 - train: epoch 0086, iter [04900, 05004], lr: 0.012053, loss: 1.5516
2022-03-12 12:38:14 - train: epoch 0086, iter [05000, 05004], lr: 0.012053, loss: 1.4286
2022-03-12 12:38:16 - train: epoch 086, train_loss: 1.5178
2022-03-12 12:39:43 - eval: epoch: 086, acc1: 66.914%, acc5: 87.874%, test_loss: 1.3411, per_image_load_time: 2.032ms, per_image_inference_time: 0.256ms
2022-03-12 12:39:43 - until epoch: 086, best_acc1: 66.914%
2022-03-12 12:39:43 - epoch 087 lr: 0.010527067017923653
2022-03-12 12:40:27 - train: epoch 0087, iter [00100, 05004], lr: 0.010527, loss: 1.6457
2022-03-12 12:41:08 - train: epoch 0087, iter [00200, 05004], lr: 0.010527, loss: 1.3821
2022-03-12 12:41:51 - train: epoch 0087, iter [00300, 05004], lr: 0.010527, loss: 1.6419
2022-03-12 12:42:33 - train: epoch 0087, iter [00400, 05004], lr: 0.010527, loss: 1.4630
2022-03-12 12:43:14 - train: epoch 0087, iter [00500, 05004], lr: 0.010527, loss: 1.2311
2022-03-12 12:43:54 - train: epoch 0087, iter [00600, 05004], lr: 0.010527, loss: 1.5624
2022-03-12 12:44:35 - train: epoch 0087, iter [00700, 05004], lr: 0.010527, loss: 1.4532
2022-03-12 12:45:18 - train: epoch 0087, iter [00800, 05004], lr: 0.010527, loss: 1.4238
2022-03-12 12:46:00 - train: epoch 0087, iter [00900, 05004], lr: 0.010527, loss: 1.3892
2022-03-12 12:46:45 - train: epoch 0087, iter [01000, 05004], lr: 0.010527, loss: 1.3586
2022-03-12 12:47:26 - train: epoch 0087, iter [01100, 05004], lr: 0.010527, loss: 1.7315
2022-03-12 12:48:09 - train: epoch 0087, iter [01200, 05004], lr: 0.010527, loss: 1.5102
2022-03-12 12:48:51 - train: epoch 0087, iter [01300, 05004], lr: 0.010527, loss: 1.7058
2022-03-12 12:49:33 - train: epoch 0087, iter [01400, 05004], lr: 0.010527, loss: 1.2643
2022-03-12 12:50:16 - train: epoch 0087, iter [01500, 05004], lr: 0.010527, loss: 1.4401
2022-03-12 12:50:58 - train: epoch 0087, iter [01600, 05004], lr: 0.010527, loss: 1.3682
2022-03-12 12:51:41 - train: epoch 0087, iter [01700, 05004], lr: 0.010527, loss: 1.7390
2022-03-12 12:52:22 - train: epoch 0087, iter [01800, 05004], lr: 0.010527, loss: 1.7500
2022-03-12 12:53:05 - train: epoch 0087, iter [01900, 05004], lr: 0.010527, loss: 1.6000
2022-03-12 12:53:47 - train: epoch 0087, iter [02000, 05004], lr: 0.010527, loss: 1.5479
2022-03-12 12:54:30 - train: epoch 0087, iter [02100, 05004], lr: 0.010527, loss: 1.6216
2022-03-12 12:55:13 - train: epoch 0087, iter [02200, 05004], lr: 0.010527, loss: 1.5748
2022-03-12 12:55:56 - train: epoch 0087, iter [02300, 05004], lr: 0.010527, loss: 1.6010
2022-03-12 12:56:37 - train: epoch 0087, iter [02400, 05004], lr: 0.010527, loss: 1.3259
2022-03-12 12:57:17 - train: epoch 0087, iter [02500, 05004], lr: 0.010527, loss: 1.5429
2022-03-12 12:57:58 - train: epoch 0087, iter [02600, 05004], lr: 0.010527, loss: 1.5687
2022-03-12 12:58:40 - train: epoch 0087, iter [02700, 05004], lr: 0.010527, loss: 1.4756
2022-03-12 12:59:21 - train: epoch 0087, iter [02800, 05004], lr: 0.010527, loss: 1.3680
2022-03-12 13:00:05 - train: epoch 0087, iter [02900, 05004], lr: 0.010527, loss: 1.1897
2022-03-12 13:00:50 - train: epoch 0087, iter [03000, 05004], lr: 0.010527, loss: 1.3423
2022-03-12 13:01:34 - train: epoch 0087, iter [03100, 05004], lr: 0.010527, loss: 1.4398
2022-03-12 13:02:18 - train: epoch 0087, iter [03200, 05004], lr: 0.010527, loss: 1.4062
2022-03-12 13:03:01 - train: epoch 0087, iter [03300, 05004], lr: 0.010527, loss: 1.3287
2022-03-12 13:03:46 - train: epoch 0087, iter [03400, 05004], lr: 0.010527, loss: 1.4648
2022-03-12 13:04:29 - train: epoch 0087, iter [03500, 05004], lr: 0.010527, loss: 1.6372
2022-03-12 13:05:12 - train: epoch 0087, iter [03600, 05004], lr: 0.010527, loss: 1.3133
2022-03-12 13:05:55 - train: epoch 0087, iter [03700, 05004], lr: 0.010527, loss: 1.3470
2022-03-12 13:06:38 - train: epoch 0087, iter [03800, 05004], lr: 0.010527, loss: 1.5415
2022-03-12 13:07:22 - train: epoch 0087, iter [03900, 05004], lr: 0.010527, loss: 1.5443
2022-03-12 13:08:05 - train: epoch 0087, iter [04000, 05004], lr: 0.010527, loss: 1.5511
2022-03-12 13:08:49 - train: epoch 0087, iter [04100, 05004], lr: 0.010527, loss: 1.3848
2022-03-12 13:09:31 - train: epoch 0087, iter [04200, 05004], lr: 0.010527, loss: 1.5785
2022-03-12 13:10:15 - train: epoch 0087, iter [04300, 05004], lr: 0.010527, loss: 1.3898
2022-03-12 13:10:58 - train: epoch 0087, iter [04400, 05004], lr: 0.010527, loss: 1.4522
2022-03-12 13:11:39 - train: epoch 0087, iter [04500, 05004], lr: 0.010527, loss: 1.5997
2022-03-12 13:12:21 - train: epoch 0087, iter [04600, 05004], lr: 0.010527, loss: 1.7235
2022-03-12 13:13:03 - train: epoch 0087, iter [04700, 05004], lr: 0.010527, loss: 1.7313
2022-03-12 13:13:45 - train: epoch 0087, iter [04800, 05004], lr: 0.010527, loss: 1.4113
2022-03-12 13:14:28 - train: epoch 0087, iter [04900, 05004], lr: 0.010527, loss: 1.4922
2022-03-12 13:15:13 - train: epoch 0087, iter [05000, 05004], lr: 0.010527, loss: 1.5565
2022-03-12 13:15:14 - train: epoch 087, train_loss: 1.4909
2022-03-12 13:16:43 - eval: epoch: 087, acc1: 67.572%, acc5: 88.130%, test_loss: 1.3116, per_image_load_time: 2.069ms, per_image_inference_time: 0.261ms
2022-03-12 13:16:43 - until epoch: 087, best_acc1: 67.572%
2022-03-12 13:16:43 - epoch 088 lr: 0.00909934649508375
2022-03-12 13:17:31 - train: epoch 0088, iter [00100, 05004], lr: 0.009099, loss: 1.2972
2022-03-12 13:18:15 - train: epoch 0088, iter [00200, 05004], lr: 0.009099, loss: 1.3538
2022-03-12 13:18:58 - train: epoch 0088, iter [00300, 05004], lr: 0.009099, loss: 1.4996
2022-03-12 13:19:42 - train: epoch 0088, iter [00400, 05004], lr: 0.009099, loss: 1.3039
2022-03-12 13:20:25 - train: epoch 0088, iter [00500, 05004], lr: 0.009099, loss: 1.5170
2022-03-12 13:21:08 - train: epoch 0088, iter [00600, 05004], lr: 0.009099, loss: 1.3950
2022-03-12 13:21:51 - train: epoch 0088, iter [00700, 05004], lr: 0.009099, loss: 1.3832
2022-03-12 13:22:35 - train: epoch 0088, iter [00800, 05004], lr: 0.009099, loss: 1.3189
2022-03-12 13:23:18 - train: epoch 0088, iter [00900, 05004], lr: 0.009099, loss: 1.4486
2022-03-12 13:24:01 - train: epoch 0088, iter [01000, 05004], lr: 0.009099, loss: 1.2294
2022-03-12 13:24:44 - train: epoch 0088, iter [01100, 05004], lr: 0.009099, loss: 1.3635
2022-03-12 13:25:26 - train: epoch 0088, iter [01200, 05004], lr: 0.009099, loss: 1.4837
2022-03-12 13:26:07 - train: epoch 0088, iter [01300, 05004], lr: 0.009099, loss: 1.4362
2022-03-12 13:26:51 - train: epoch 0088, iter [01400, 05004], lr: 0.009099, loss: 1.2671
2022-03-12 13:27:32 - train: epoch 0088, iter [01500, 05004], lr: 0.009099, loss: 1.5500
2022-03-12 13:28:17 - train: epoch 0088, iter [01600, 05004], lr: 0.009099, loss: 1.3865
2022-03-12 13:29:02 - train: epoch 0088, iter [01700, 05004], lr: 0.009099, loss: 1.6209
2022-03-12 13:29:45 - train: epoch 0088, iter [01800, 05004], lr: 0.009099, loss: 1.3694
2022-03-12 13:30:29 - train: epoch 0088, iter [01900, 05004], lr: 0.009099, loss: 1.4816
2022-03-12 13:31:12 - train: epoch 0088, iter [02000, 05004], lr: 0.009099, loss: 1.3685
2022-03-12 13:31:55 - train: epoch 0088, iter [02100, 05004], lr: 0.009099, loss: 1.5050
2022-03-12 13:32:39 - train: epoch 0088, iter [02200, 05004], lr: 0.009099, loss: 1.4561
2022-03-12 13:33:23 - train: epoch 0088, iter [02300, 05004], lr: 0.009099, loss: 1.3640
2022-03-12 13:34:08 - train: epoch 0088, iter [02400, 05004], lr: 0.009099, loss: 1.5247
2022-03-12 13:34:51 - train: epoch 0088, iter [02500, 05004], lr: 0.009099, loss: 1.5279
2022-03-12 13:35:35 - train: epoch 0088, iter [02600, 05004], lr: 0.009099, loss: 1.4246
2022-03-12 13:36:19 - train: epoch 0088, iter [02700, 05004], lr: 0.009099, loss: 1.3680
2022-03-12 13:37:02 - train: epoch 0088, iter [02800, 05004], lr: 0.009099, loss: 1.5588
2022-03-12 13:37:47 - train: epoch 0088, iter [02900, 05004], lr: 0.009099, loss: 1.4228
2022-03-12 13:38:32 - train: epoch 0088, iter [03000, 05004], lr: 0.009099, loss: 1.5463
2022-03-12 13:39:15 - train: epoch 0088, iter [03100, 05004], lr: 0.009099, loss: 1.4609
2022-03-12 13:39:57 - train: epoch 0088, iter [03200, 05004], lr: 0.009099, loss: 1.2829
2022-03-12 13:40:38 - train: epoch 0088, iter [03300, 05004], lr: 0.009099, loss: 1.3625
2022-03-12 13:41:20 - train: epoch 0088, iter [03400, 05004], lr: 0.009099, loss: 1.3910
2022-03-12 13:42:03 - train: epoch 0088, iter [03500, 05004], lr: 0.009099, loss: 1.4941
2022-03-12 13:42:47 - train: epoch 0088, iter [03600, 05004], lr: 0.009099, loss: 1.3656
2022-03-12 13:43:31 - train: epoch 0088, iter [03700, 05004], lr: 0.009099, loss: 1.5759
2022-03-12 13:44:15 - train: epoch 0088, iter [03800, 05004], lr: 0.009099, loss: 1.6032
2022-03-12 13:44:58 - train: epoch 0088, iter [03900, 05004], lr: 0.009099, loss: 1.4487
2022-03-12 13:45:42 - train: epoch 0088, iter [04000, 05004], lr: 0.009099, loss: 1.3139
2022-03-12 13:46:24 - train: epoch 0088, iter [04100, 05004], lr: 0.009099, loss: 1.5114
2022-03-12 13:47:06 - train: epoch 0088, iter [04200, 05004], lr: 0.009099, loss: 1.4570
2022-03-12 13:47:47 - train: epoch 0088, iter [04300, 05004], lr: 0.009099, loss: 1.4018
2022-03-12 13:48:31 - train: epoch 0088, iter [04400, 05004], lr: 0.009099, loss: 1.2061
2022-03-12 13:49:14 - train: epoch 0088, iter [04500, 05004], lr: 0.009099, loss: 1.2613
2022-03-12 13:49:58 - train: epoch 0088, iter [04600, 05004], lr: 0.009099, loss: 1.7033
2022-03-12 13:50:42 - train: epoch 0088, iter [04700, 05004], lr: 0.009099, loss: 1.4848
2022-03-12 13:51:26 - train: epoch 0088, iter [04800, 05004], lr: 0.009099, loss: 1.3527
2022-03-12 13:52:09 - train: epoch 0088, iter [04900, 05004], lr: 0.009099, loss: 1.2188
2022-03-12 13:52:52 - train: epoch 0088, iter [05000, 05004], lr: 0.009099, loss: 1.5279
2022-03-12 13:52:55 - train: epoch 088, train_loss: 1.4674
2022-03-12 13:54:24 - eval: epoch: 088, acc1: 67.872%, acc5: 88.436%, test_loss: 1.2962, per_image_load_time: 3.248ms, per_image_inference_time: 0.261ms
2022-03-12 13:54:24 - until epoch: 088, best_acc1: 67.872%
2022-03-12 13:54:24 - epoch 089 lr: 0.007771024502261525
2022-03-12 13:55:09 - train: epoch 0089, iter [00100, 05004], lr: 0.007771, loss: 1.4948
2022-03-12 13:55:51 - train: epoch 0089, iter [00200, 05004], lr: 0.007771, loss: 1.2606
2022-03-12 13:56:35 - train: epoch 0089, iter [00300, 05004], lr: 0.007771, loss: 1.4970
2022-03-12 13:57:19 - train: epoch 0089, iter [00400, 05004], lr: 0.007771, loss: 1.3493
2022-03-12 13:58:01 - train: epoch 0089, iter [00500, 05004], lr: 0.007771, loss: 1.3873
2022-03-12 13:58:45 - train: epoch 0089, iter [00600, 05004], lr: 0.007771, loss: 1.4556
2022-03-12 13:59:29 - train: epoch 0089, iter [00700, 05004], lr: 0.007771, loss: 1.7180
2022-03-12 14:00:14 - train: epoch 0089, iter [00800, 05004], lr: 0.007771, loss: 1.5802
2022-03-12 14:00:57 - train: epoch 0089, iter [00900, 05004], lr: 0.007771, loss: 1.3516
2022-03-12 14:01:41 - train: epoch 0089, iter [01000, 05004], lr: 0.007771, loss: 1.4268
2022-03-12 14:02:24 - train: epoch 0089, iter [01100, 05004], lr: 0.007771, loss: 1.2379
2022-03-12 14:03:07 - train: epoch 0089, iter [01200, 05004], lr: 0.007771, loss: 1.6248
2022-03-12 14:03:52 - train: epoch 0089, iter [01300, 05004], lr: 0.007771, loss: 1.4191
2022-03-12 14:04:35 - train: epoch 0089, iter [01400, 05004], lr: 0.007771, loss: 1.6327
2022-03-12 14:05:19 - train: epoch 0089, iter [01500, 05004], lr: 0.007771, loss: 1.4746
2022-03-12 14:06:04 - train: epoch 0089, iter [01600, 05004], lr: 0.007771, loss: 1.3656
2022-03-12 14:06:49 - train: epoch 0089, iter [01700, 05004], lr: 0.007771, loss: 1.4844
2022-03-12 14:07:32 - train: epoch 0089, iter [01800, 05004], lr: 0.007771, loss: 1.6031
2022-03-12 14:08:16 - train: epoch 0089, iter [01900, 05004], lr: 0.007771, loss: 1.2408
2022-03-12 14:09:01 - train: epoch 0089, iter [02000, 05004], lr: 0.007771, loss: 1.3531
2022-03-12 14:09:44 - train: epoch 0089, iter [02100, 05004], lr: 0.007771, loss: 1.4188
2022-03-12 14:10:25 - train: epoch 0089, iter [02200, 05004], lr: 0.007771, loss: 1.6899
2022-03-12 14:11:08 - train: epoch 0089, iter [02300, 05004], lr: 0.007771, loss: 1.4392
2022-03-12 14:11:52 - train: epoch 0089, iter [02400, 05004], lr: 0.007771, loss: 1.6460
2022-03-12 14:12:35 - train: epoch 0089, iter [02500, 05004], lr: 0.007771, loss: 1.3713
2022-03-12 14:13:18 - train: epoch 0089, iter [02600, 05004], lr: 0.007771, loss: 1.3271
2022-03-12 14:14:01 - train: epoch 0089, iter [02700, 05004], lr: 0.007771, loss: 1.3700
2022-03-12 14:14:45 - train: epoch 0089, iter [02800, 05004], lr: 0.007771, loss: 1.7070
2022-03-12 14:15:28 - train: epoch 0089, iter [02900, 05004], lr: 0.007771, loss: 1.4881
2022-03-12 14:16:12 - train: epoch 0089, iter [03000, 05004], lr: 0.007771, loss: 1.4993
2022-03-12 14:16:55 - train: epoch 0089, iter [03100, 05004], lr: 0.007771, loss: 1.4163
2022-03-12 14:17:37 - train: epoch 0089, iter [03200, 05004], lr: 0.007771, loss: 1.4059
2022-03-12 14:18:22 - train: epoch 0089, iter [03300, 05004], lr: 0.007771, loss: 1.5034
2022-03-12 14:19:05 - train: epoch 0089, iter [03400, 05004], lr: 0.007771, loss: 1.4342
2022-03-12 14:19:49 - train: epoch 0089, iter [03500, 05004], lr: 0.007771, loss: 1.3967
2022-03-12 14:20:33 - train: epoch 0089, iter [03600, 05004], lr: 0.007771, loss: 1.5481
2022-03-12 14:21:17 - train: epoch 0089, iter [03700, 05004], lr: 0.007771, loss: 1.5450
2022-03-12 14:22:01 - train: epoch 0089, iter [03800, 05004], lr: 0.007771, loss: 1.2897
2022-03-12 14:22:46 - train: epoch 0089, iter [03900, 05004], lr: 0.007771, loss: 1.6449
2022-03-12 14:23:30 - train: epoch 0089, iter [04000, 05004], lr: 0.007771, loss: 1.3314
2022-03-12 14:24:11 - train: epoch 0089, iter [04100, 05004], lr: 0.007771, loss: 1.6084
2022-03-12 14:24:53 - train: epoch 0089, iter [04200, 05004], lr: 0.007771, loss: 1.5326
2022-03-12 14:25:36 - train: epoch 0089, iter [04300, 05004], lr: 0.007771, loss: 1.4563
2022-03-12 14:26:19 - train: epoch 0089, iter [04400, 05004], lr: 0.007771, loss: 1.5503
2022-03-12 14:27:02 - train: epoch 0089, iter [04500, 05004], lr: 0.007771, loss: 1.2568
2022-03-12 14:27:45 - train: epoch 0089, iter [04600, 05004], lr: 0.007771, loss: 1.2948
2022-03-12 14:28:28 - train: epoch 0089, iter [04700, 05004], lr: 0.007771, loss: 1.4362
2022-03-12 14:29:11 - train: epoch 0089, iter [04800, 05004], lr: 0.007771, loss: 1.2723
2022-03-12 14:29:53 - train: epoch 0089, iter [04900, 05004], lr: 0.007771, loss: 1.4687
2022-03-12 14:30:37 - train: epoch 0089, iter [05000, 05004], lr: 0.007771, loss: 1.2298
2022-03-12 14:30:39 - train: epoch 089, train_loss: 1.4431
2022-03-12 14:32:09 - eval: epoch: 089, acc1: 68.106%, acc5: 88.498%, test_loss: 1.2824, per_image_load_time: 3.285ms, per_image_inference_time: 0.259ms
2022-03-12 14:32:09 - until epoch: 089, best_acc1: 68.106%
2022-03-12 14:32:09 - epoch 090 lr: 0.006543553540053926
2022-03-12 14:32:55 - train: epoch 0090, iter [00100, 05004], lr: 0.006544, loss: 1.4884
2022-03-12 14:33:37 - train: epoch 0090, iter [00200, 05004], lr: 0.006544, loss: 1.4026
2022-03-12 14:34:18 - train: epoch 0090, iter [00300, 05004], lr: 0.006544, loss: 1.2460
2022-03-12 14:35:02 - train: epoch 0090, iter [00400, 05004], lr: 0.006544, loss: 1.3531
2022-03-12 14:35:44 - train: epoch 0090, iter [00500, 05004], lr: 0.006544, loss: 1.4249
2022-03-12 14:36:26 - train: epoch 0090, iter [00600, 05004], lr: 0.006544, loss: 1.6402
2022-03-12 14:37:10 - train: epoch 0090, iter [00700, 05004], lr: 0.006544, loss: 1.3720
2022-03-12 14:37:53 - train: epoch 0090, iter [00800, 05004], lr: 0.006544, loss: 1.5083
2022-03-12 14:38:35 - train: epoch 0090, iter [00900, 05004], lr: 0.006544, loss: 1.4291
2022-03-12 14:39:19 - train: epoch 0090, iter [01000, 05004], lr: 0.006544, loss: 1.3705
2022-03-12 14:40:02 - train: epoch 0090, iter [01100, 05004], lr: 0.006544, loss: 1.3518
2022-03-12 14:40:48 - train: epoch 0090, iter [01200, 05004], lr: 0.006544, loss: 1.2327
2022-03-12 14:41:32 - train: epoch 0090, iter [01300, 05004], lr: 0.006544, loss: 1.2389
2022-03-12 14:42:16 - train: epoch 0090, iter [01400, 05004], lr: 0.006544, loss: 1.3265
2022-03-12 14:43:00 - train: epoch 0090, iter [01500, 05004], lr: 0.006544, loss: 1.7154
2022-03-12 14:43:45 - train: epoch 0090, iter [01600, 05004], lr: 0.006544, loss: 1.3854
2022-03-12 14:44:28 - train: epoch 0090, iter [01700, 05004], lr: 0.006544, loss: 1.3972
2022-03-12 14:45:12 - train: epoch 0090, iter [01800, 05004], lr: 0.006544, loss: 1.3258
2022-03-12 14:45:57 - train: epoch 0090, iter [01900, 05004], lr: 0.006544, loss: 1.4180
2022-03-12 14:46:41 - train: epoch 0090, iter [02000, 05004], lr: 0.006544, loss: 1.3849
2022-03-12 14:47:24 - train: epoch 0090, iter [02100, 05004], lr: 0.006544, loss: 1.4415
2022-03-12 14:48:07 - train: epoch 0090, iter [02200, 05004], lr: 0.006544, loss: 1.4537
2022-03-12 14:48:52 - train: epoch 0090, iter [02300, 05004], lr: 0.006544, loss: 1.4138
2022-03-12 14:49:36 - train: epoch 0090, iter [02400, 05004], lr: 0.006544, loss: 1.3049
2022-03-12 14:50:20 - train: epoch 0090, iter [02500, 05004], lr: 0.006544, loss: 1.3162
2022-03-12 14:51:03 - train: epoch 0090, iter [02600, 05004], lr: 0.006544, loss: 1.4965
2022-03-12 14:51:46 - train: epoch 0090, iter [02700, 05004], lr: 0.006544, loss: 1.3140
2022-03-12 14:52:29 - train: epoch 0090, iter [02800, 05004], lr: 0.006544, loss: 1.3926
2022-03-12 14:53:11 - train: epoch 0090, iter [02900, 05004], lr: 0.006544, loss: 1.5234
2022-03-12 14:53:55 - train: epoch 0090, iter [03000, 05004], lr: 0.006544, loss: 1.5298
2022-03-12 14:54:38 - train: epoch 0090, iter [03100, 05004], lr: 0.006544, loss: 1.3553
2022-03-12 14:55:21 - train: epoch 0090, iter [03200, 05004], lr: 0.006544, loss: 1.4770
2022-03-12 14:56:06 - train: epoch 0090, iter [03300, 05004], lr: 0.006544, loss: 1.5230
2022-03-12 14:56:49 - train: epoch 0090, iter [03400, 05004], lr: 0.006544, loss: 1.4048
2022-03-12 14:57:32 - train: epoch 0090, iter [03500, 05004], lr: 0.006544, loss: 1.3433
2022-03-12 14:58:16 - train: epoch 0090, iter [03600, 05004], lr: 0.006544, loss: 1.4274
2022-03-12 14:59:00 - train: epoch 0090, iter [03700, 05004], lr: 0.006544, loss: 1.5192
2022-03-12 14:59:42 - train: epoch 0090, iter [03800, 05004], lr: 0.006544, loss: 1.2775
2022-03-12 15:00:26 - train: epoch 0090, iter [03900, 05004], lr: 0.006544, loss: 1.3797
2022-03-12 15:01:09 - train: epoch 0090, iter [04000, 05004], lr: 0.006544, loss: 1.3806
2022-03-12 15:01:53 - train: epoch 0090, iter [04100, 05004], lr: 0.006544, loss: 1.6003
2022-03-12 15:02:37 - train: epoch 0090, iter [04200, 05004], lr: 0.006544, loss: 1.4688
2022-03-12 15:03:21 - train: epoch 0090, iter [04300, 05004], lr: 0.006544, loss: 1.1733
2022-03-12 15:04:05 - train: epoch 0090, iter [04400, 05004], lr: 0.006544, loss: 1.2586
2022-03-12 15:04:49 - train: epoch 0090, iter [04500, 05004], lr: 0.006544, loss: 1.3469
2022-03-12 15:05:32 - train: epoch 0090, iter [04600, 05004], lr: 0.006544, loss: 1.6269
2022-03-12 15:06:15 - train: epoch 0090, iter [04700, 05004], lr: 0.006544, loss: 1.3108
2022-03-12 15:06:55 - train: epoch 0090, iter [04800, 05004], lr: 0.006544, loss: 1.5633
2022-03-12 15:07:38 - train: epoch 0090, iter [04900, 05004], lr: 0.006544, loss: 1.2956
2022-03-12 15:08:22 - train: epoch 0090, iter [05000, 05004], lr: 0.006544, loss: 1.5276
2022-03-12 15:08:23 - train: epoch 090, train_loss: 1.4174
2022-03-12 15:09:50 - eval: epoch: 090, acc1: 68.470%, acc5: 88.736%, test_loss: 1.2675, per_image_load_time: 2.160ms, per_image_inference_time: 0.268ms
2022-03-12 15:09:50 - until epoch: 090, best_acc1: 68.470%
2022-03-12 15:09:50 - epoch 091 lr: 0.005418275829936536
2022-03-12 15:10:38 - train: epoch 0091, iter [00100, 05004], lr: 0.005418, loss: 1.2569
2022-03-12 15:11:22 - train: epoch 0091, iter [00200, 05004], lr: 0.005418, loss: 1.3906
2022-03-12 15:12:06 - train: epoch 0091, iter [00300, 05004], lr: 0.005418, loss: 1.4305
2022-03-12 15:12:50 - train: epoch 0091, iter [00400, 05004], lr: 0.005418, loss: 1.3801
2022-03-12 15:13:34 - train: epoch 0091, iter [00500, 05004], lr: 0.005418, loss: 1.5220
2022-03-12 15:14:18 - train: epoch 0091, iter [00600, 05004], lr: 0.005418, loss: 1.4208
2022-03-12 15:15:01 - train: epoch 0091, iter [00700, 05004], lr: 0.005418, loss: 1.5832
2022-03-12 15:15:45 - train: epoch 0091, iter [00800, 05004], lr: 0.005418, loss: 1.0621
2022-03-12 15:16:30 - train: epoch 0091, iter [00900, 05004], lr: 0.005418, loss: 1.3394
2022-03-12 15:17:13 - train: epoch 0091, iter [01000, 05004], lr: 0.005418, loss: 1.2292
2022-03-12 15:17:57 - train: epoch 0091, iter [01100, 05004], lr: 0.005418, loss: 1.1183
2022-03-12 15:18:40 - train: epoch 0091, iter [01200, 05004], lr: 0.005418, loss: 1.5088
2022-03-12 15:19:24 - train: epoch 0091, iter [01300, 05004], lr: 0.005418, loss: 1.2934
2022-03-12 15:20:07 - train: epoch 0091, iter [01400, 05004], lr: 0.005418, loss: 1.5631
2022-03-12 15:20:49 - train: epoch 0091, iter [01500, 05004], lr: 0.005418, loss: 1.3804
2022-03-12 15:21:31 - train: epoch 0091, iter [01600, 05004], lr: 0.005418, loss: 1.3704
2022-03-12 15:22:14 - train: epoch 0091, iter [01700, 05004], lr: 0.005418, loss: 1.3266
2022-03-12 15:22:57 - train: epoch 0091, iter [01800, 05004], lr: 0.005418, loss: 1.4454
2022-03-12 15:23:42 - train: epoch 0091, iter [01900, 05004], lr: 0.005418, loss: 1.3746
2022-03-12 15:24:25 - train: epoch 0091, iter [02000, 05004], lr: 0.005418, loss: 1.1988
2022-03-12 15:25:10 - train: epoch 0091, iter [02100, 05004], lr: 0.005418, loss: 1.2455
2022-03-12 15:25:52 - train: epoch 0091, iter [02200, 05004], lr: 0.005418, loss: 1.5570
2022-03-12 15:26:35 - train: epoch 0091, iter [02300, 05004], lr: 0.005418, loss: 1.5703
2022-03-12 15:27:19 - train: epoch 0091, iter [02400, 05004], lr: 0.005418, loss: 1.1801
2022-03-12 15:28:03 - train: epoch 0091, iter [02500, 05004], lr: 0.005418, loss: 1.4790
2022-03-12 15:28:47 - train: epoch 0091, iter [02600, 05004], lr: 0.005418, loss: 1.2710
2022-03-12 15:29:30 - train: epoch 0091, iter [02700, 05004], lr: 0.005418, loss: 1.2496
2022-03-12 15:30:14 - train: epoch 0091, iter [02800, 05004], lr: 0.005418, loss: 1.3869
2022-03-12 15:30:58 - train: epoch 0091, iter [02900, 05004], lr: 0.005418, loss: 1.4633
2022-03-12 15:31:42 - train: epoch 0091, iter [03000, 05004], lr: 0.005418, loss: 1.4114
2022-03-12 15:32:25 - train: epoch 0091, iter [03100, 05004], lr: 0.005418, loss: 1.2973
2022-03-12 15:33:08 - train: epoch 0091, iter [03200, 05004], lr: 0.005418, loss: 1.6536
2022-03-12 15:33:51 - train: epoch 0091, iter [03300, 05004], lr: 0.005418, loss: 1.4860
2022-03-12 15:34:34 - train: epoch 0091, iter [03400, 05004], lr: 0.005418, loss: 1.4812
2022-03-12 15:35:17 - train: epoch 0091, iter [03500, 05004], lr: 0.005418, loss: 1.2692
2022-03-12 15:36:00 - train: epoch 0091, iter [03600, 05004], lr: 0.005418, loss: 1.4254
2022-03-12 15:36:43 - train: epoch 0091, iter [03700, 05004], lr: 0.005418, loss: 1.5200
2022-03-12 15:37:26 - train: epoch 0091, iter [03800, 05004], lr: 0.005418, loss: 1.4425
2022-03-12 15:38:09 - train: epoch 0091, iter [03900, 05004], lr: 0.005418, loss: 1.3825
2022-03-12 15:38:53 - train: epoch 0091, iter [04000, 05004], lr: 0.005418, loss: 1.2094
2022-03-12 15:39:37 - train: epoch 0091, iter [04100, 05004], lr: 0.005418, loss: 1.4398
2022-03-12 15:40:19 - train: epoch 0091, iter [04200, 05004], lr: 0.005418, loss: 1.6450
2022-03-12 15:41:03 - train: epoch 0091, iter [04300, 05004], lr: 0.005418, loss: 1.3510
2022-03-12 15:41:46 - train: epoch 0091, iter [04400, 05004], lr: 0.005418, loss: 1.2803
2022-03-12 15:42:29 - train: epoch 0091, iter [04500, 05004], lr: 0.005418, loss: 1.3183
2022-03-12 15:43:12 - train: epoch 0091, iter [04600, 05004], lr: 0.005418, loss: 1.3675
2022-03-12 15:43:56 - train: epoch 0091, iter [04700, 05004], lr: 0.005418, loss: 1.3606
2022-03-12 15:44:40 - train: epoch 0091, iter [04800, 05004], lr: 0.005418, loss: 1.5667
2022-03-12 15:45:23 - train: epoch 0091, iter [04900, 05004], lr: 0.005418, loss: 1.4942
2022-03-12 15:46:07 - train: epoch 0091, iter [05000, 05004], lr: 0.005418, loss: 1.5142
2022-03-12 15:46:09 - train: epoch 091, train_loss: 1.3911
2022-03-12 15:47:41 - eval: epoch: 091, acc1: 68.796%, acc5: 88.874%, test_loss: 1.2566, per_image_load_time: 3.370ms, per_image_inference_time: 0.258ms
2022-03-12 15:47:41 - until epoch: 091, best_acc1: 68.796%
2022-03-12 15:47:41 - epoch 092 lr: 0.0043964218465642356
2022-03-12 15:48:28 - train: epoch 0092, iter [00100, 05004], lr: 0.004396, loss: 1.3235
2022-03-12 15:49:10 - train: epoch 0092, iter [00200, 05004], lr: 0.004396, loss: 1.3156
2022-03-12 15:49:52 - train: epoch 0092, iter [00300, 05004], lr: 0.004396, loss: 1.5288
2022-03-12 15:50:34 - train: epoch 0092, iter [00400, 05004], lr: 0.004396, loss: 1.3157
2022-03-12 15:51:15 - train: epoch 0092, iter [00500, 05004], lr: 0.004396, loss: 1.4621
2022-03-12 15:51:58 - train: epoch 0092, iter [00600, 05004], lr: 0.004396, loss: 1.3583
2022-03-12 15:52:43 - train: epoch 0092, iter [00700, 05004], lr: 0.004396, loss: 1.1443
2022-03-12 15:53:28 - train: epoch 0092, iter [00800, 05004], lr: 0.004396, loss: 1.4226
2022-03-12 15:54:12 - train: epoch 0092, iter [00900, 05004], lr: 0.004396, loss: 1.4459
2022-03-12 15:54:57 - train: epoch 0092, iter [01000, 05004], lr: 0.004396, loss: 1.3620
2022-03-12 15:55:41 - train: epoch 0092, iter [01100, 05004], lr: 0.004396, loss: 1.3106
2022-03-12 15:56:25 - train: epoch 0092, iter [01200, 05004], lr: 0.004396, loss: 1.3533
2022-03-12 15:57:08 - train: epoch 0092, iter [01300, 05004], lr: 0.004396, loss: 1.2212
2022-03-12 15:57:52 - train: epoch 0092, iter [01400, 05004], lr: 0.004396, loss: 1.2215
2022-03-12 15:58:36 - train: epoch 0092, iter [01500, 05004], lr: 0.004396, loss: 1.2857
2022-03-12 15:59:20 - train: epoch 0092, iter [01600, 05004], lr: 0.004396, loss: 1.4411
2022-03-12 16:00:00 - train: epoch 0092, iter [01700, 05004], lr: 0.004396, loss: 1.2751
2022-03-12 16:00:42 - train: epoch 0092, iter [01800, 05004], lr: 0.004396, loss: 1.2245
2022-03-12 16:01:24 - train: epoch 0092, iter [01900, 05004], lr: 0.004396, loss: 1.2909
2022-03-12 16:02:06 - train: epoch 0092, iter [02000, 05004], lr: 0.004396, loss: 1.1795
2022-03-12 16:02:47 - train: epoch 0092, iter [02100, 05004], lr: 0.004396, loss: 1.5328
2022-03-12 16:03:27 - train: epoch 0092, iter [02200, 05004], lr: 0.004396, loss: 1.4645
2022-03-12 16:04:10 - train: epoch 0092, iter [02300, 05004], lr: 0.004396, loss: 1.3694
2022-03-12 16:04:51 - train: epoch 0092, iter [02400, 05004], lr: 0.004396, loss: 1.2672
2022-03-12 16:05:32 - train: epoch 0092, iter [02500, 05004], lr: 0.004396, loss: 1.5417
2022-03-12 16:06:14 - train: epoch 0092, iter [02600, 05004], lr: 0.004396, loss: 1.3288
2022-03-12 16:06:57 - train: epoch 0092, iter [02700, 05004], lr: 0.004396, loss: 1.4979
2022-03-12 16:07:39 - train: epoch 0092, iter [02800, 05004], lr: 0.004396, loss: 1.4335
2022-03-12 16:08:22 - train: epoch 0092, iter [02900, 05004], lr: 0.004396, loss: 1.6275
2022-03-12 16:09:03 - train: epoch 0092, iter [03000, 05004], lr: 0.004396, loss: 1.4491
2022-03-12 16:09:46 - train: epoch 0092, iter [03100, 05004], lr: 0.004396, loss: 1.5297
2022-03-12 16:10:29 - train: epoch 0092, iter [03200, 05004], lr: 0.004396, loss: 1.2115
2022-03-12 16:11:11 - train: epoch 0092, iter [03300, 05004], lr: 0.004396, loss: 1.1382
2022-03-12 16:11:55 - train: epoch 0092, iter [03400, 05004], lr: 0.004396, loss: 1.4011
2022-03-12 16:12:35 - train: epoch 0092, iter [03500, 05004], lr: 0.004396, loss: 1.3579
2022-03-12 16:13:16 - train: epoch 0092, iter [03600, 05004], lr: 0.004396, loss: 1.3901
2022-03-12 16:13:57 - train: epoch 0092, iter [03700, 05004], lr: 0.004396, loss: 1.4557
2022-03-12 16:14:38 - train: epoch 0092, iter [03800, 05004], lr: 0.004396, loss: 1.5152
2022-03-12 16:15:19 - train: epoch 0092, iter [03900, 05004], lr: 0.004396, loss: 1.4480
2022-03-12 16:16:01 - train: epoch 0092, iter [04000, 05004], lr: 0.004396, loss: 1.4774
2022-03-12 16:16:43 - train: epoch 0092, iter [04100, 05004], lr: 0.004396, loss: 1.1857
2022-03-12 16:17:23 - train: epoch 0092, iter [04200, 05004], lr: 0.004396, loss: 1.4557
2022-03-12 16:18:03 - train: epoch 0092, iter [04300, 05004], lr: 0.004396, loss: 1.3257
2022-03-12 16:18:43 - train: epoch 0092, iter [04400, 05004], lr: 0.004396, loss: 1.2461
2022-03-12 16:19:24 - train: epoch 0092, iter [04500, 05004], lr: 0.004396, loss: 1.2772
2022-03-12 16:20:06 - train: epoch 0092, iter [04600, 05004], lr: 0.004396, loss: 1.5798
2022-03-12 16:20:48 - train: epoch 0092, iter [04700, 05004], lr: 0.004396, loss: 1.3689
2022-03-12 16:21:31 - train: epoch 0092, iter [04800, 05004], lr: 0.004396, loss: 1.3830
2022-03-12 16:22:14 - train: epoch 0092, iter [04900, 05004], lr: 0.004396, loss: 1.1177
2022-03-12 16:22:57 - train: epoch 0092, iter [05000, 05004], lr: 0.004396, loss: 1.2167
2022-03-12 16:22:59 - train: epoch 092, train_loss: 1.3670
2022-03-12 16:24:26 - eval: epoch: 092, acc1: 69.280%, acc5: 89.212%, test_loss: 1.2362, per_image_load_time: 1.885ms, per_image_inference_time: 0.274ms
2022-03-12 16:24:26 - until epoch: 092, best_acc1: 69.280%
2022-03-12 16:24:26 - epoch 093 lr: 0.0034791089722651437
2022-03-12 16:25:12 - train: epoch 0093, iter [00100, 05004], lr: 0.003479, loss: 1.3723
2022-03-12 16:25:56 - train: epoch 0093, iter [00200, 05004], lr: 0.003479, loss: 1.2534
2022-03-12 16:26:38 - train: epoch 0093, iter [00300, 05004], lr: 0.003479, loss: 1.4441
2022-03-12 16:27:20 - train: epoch 0093, iter [00400, 05004], lr: 0.003479, loss: 1.3897
2022-03-12 16:28:03 - train: epoch 0093, iter [00500, 05004], lr: 0.003479, loss: 1.3516
2022-03-12 16:28:44 - train: epoch 0093, iter [00600, 05004], lr: 0.003479, loss: 1.5656
2022-03-12 16:29:27 - train: epoch 0093, iter [00700, 05004], lr: 0.003479, loss: 1.2992
2022-03-12 16:30:10 - train: epoch 0093, iter [00800, 05004], lr: 0.003479, loss: 1.1693
2022-03-12 16:30:52 - train: epoch 0093, iter [00900, 05004], lr: 0.003479, loss: 1.3337
2022-03-12 16:31:32 - train: epoch 0093, iter [01000, 05004], lr: 0.003479, loss: 1.1721
2022-03-12 16:32:13 - train: epoch 0093, iter [01100, 05004], lr: 0.003479, loss: 1.3617
2022-03-12 16:32:54 - train: epoch 0093, iter [01200, 05004], lr: 0.003479, loss: 1.4521
2022-03-12 16:33:35 - train: epoch 0093, iter [01300, 05004], lr: 0.003479, loss: 1.3190
2022-03-12 16:34:17 - train: epoch 0093, iter [01400, 05004], lr: 0.003479, loss: 1.4260
2022-03-12 16:35:00 - train: epoch 0093, iter [01500, 05004], lr: 0.003479, loss: 1.4326
2022-03-12 16:35:42 - train: epoch 0093, iter [01600, 05004], lr: 0.003479, loss: 1.2596
2022-03-12 16:36:25 - train: epoch 0093, iter [01700, 05004], lr: 0.003479, loss: 1.4624
2022-03-12 16:37:07 - train: epoch 0093, iter [01800, 05004], lr: 0.003479, loss: 1.4007
2022-03-12 16:37:50 - train: epoch 0093, iter [01900, 05004], lr: 0.003479, loss: 1.1921
2022-03-12 16:38:32 - train: epoch 0093, iter [02000, 05004], lr: 0.003479, loss: 1.0629
2022-03-12 16:39:15 - train: epoch 0093, iter [02100, 05004], lr: 0.003479, loss: 1.2356
2022-03-12 16:39:56 - train: epoch 0093, iter [02200, 05004], lr: 0.003479, loss: 1.5784
2022-03-12 16:40:40 - train: epoch 0093, iter [02300, 05004], lr: 0.003479, loss: 1.3690
2022-03-12 16:41:23 - train: epoch 0093, iter [02400, 05004], lr: 0.003479, loss: 1.2333
2022-03-12 16:42:07 - train: epoch 0093, iter [02500, 05004], lr: 0.003479, loss: 1.5332
2022-03-12 16:42:49 - train: epoch 0093, iter [02600, 05004], lr: 0.003479, loss: 1.5294
2022-03-12 16:43:30 - train: epoch 0093, iter [02700, 05004], lr: 0.003479, loss: 1.2466
2022-03-12 16:44:14 - train: epoch 0093, iter [02800, 05004], lr: 0.003479, loss: 1.3570
2022-03-12 16:44:56 - train: epoch 0093, iter [02900, 05004], lr: 0.003479, loss: 1.1690
2022-03-12 16:45:37 - train: epoch 0093, iter [03000, 05004], lr: 0.003479, loss: 1.4358
2022-03-12 16:46:17 - train: epoch 0093, iter [03100, 05004], lr: 0.003479, loss: 1.3710
2022-03-12 16:47:00 - train: epoch 0093, iter [03200, 05004], lr: 0.003479, loss: 1.2955
2022-03-12 16:47:41 - train: epoch 0093, iter [03300, 05004], lr: 0.003479, loss: 1.4088
2022-03-12 16:48:23 - train: epoch 0093, iter [03400, 05004], lr: 0.003479, loss: 1.5020
2022-03-12 16:49:07 - train: epoch 0093, iter [03500, 05004], lr: 0.003479, loss: 1.3753
2022-03-12 16:49:51 - train: epoch 0093, iter [03600, 05004], lr: 0.003479, loss: 1.5029
2022-03-12 16:50:33 - train: epoch 0093, iter [03700, 05004], lr: 0.003479, loss: 1.2154
2022-03-12 16:51:15 - train: epoch 0093, iter [03800, 05004], lr: 0.003479, loss: 1.1464
2022-03-12 16:51:58 - train: epoch 0093, iter [03900, 05004], lr: 0.003479, loss: 1.4539
2022-03-12 16:52:40 - train: epoch 0093, iter [04000, 05004], lr: 0.003479, loss: 1.6549
2022-03-12 16:53:23 - train: epoch 0093, iter [04100, 05004], lr: 0.003479, loss: 1.3748
2022-03-12 16:54:05 - train: epoch 0093, iter [04200, 05004], lr: 0.003479, loss: 1.2832
2022-03-12 16:54:46 - train: epoch 0093, iter [04300, 05004], lr: 0.003479, loss: 1.5097
2022-03-12 16:55:28 - train: epoch 0093, iter [04400, 05004], lr: 0.003479, loss: 1.2798
2022-03-12 16:56:10 - train: epoch 0093, iter [04500, 05004], lr: 0.003479, loss: 1.3917
2022-03-12 16:56:50 - train: epoch 0093, iter [04600, 05004], lr: 0.003479, loss: 1.2210
2022-03-12 16:57:31 - train: epoch 0093, iter [04700, 05004], lr: 0.003479, loss: 1.5631
2022-03-12 16:58:12 - train: epoch 0093, iter [04800, 05004], lr: 0.003479, loss: 1.1534
2022-03-12 16:58:53 - train: epoch 0093, iter [04900, 05004], lr: 0.003479, loss: 1.2931
2022-03-12 16:59:36 - train: epoch 0093, iter [05000, 05004], lr: 0.003479, loss: 1.3238
2022-03-12 16:59:38 - train: epoch 093, train_loss: 1.3466
2022-03-12 17:01:04 - eval: epoch: 093, acc1: 69.576%, acc5: 89.390%, test_loss: 1.2223, per_image_load_time: 2.907ms, per_image_inference_time: 0.275ms
2022-03-12 17:01:04 - until epoch: 093, best_acc1: 69.576%
2022-03-12 17:01:04 - epoch 094 lr: 0.002667340275199426
2022-03-12 17:01:49 - train: epoch 0094, iter [00100, 05004], lr: 0.002667, loss: 1.2928
2022-03-12 17:02:31 - train: epoch 0094, iter [00200, 05004], lr: 0.002667, loss: 1.2682
2022-03-12 17:03:14 - train: epoch 0094, iter [00300, 05004], lr: 0.002667, loss: 1.3852
2022-03-12 17:03:57 - train: epoch 0094, iter [00400, 05004], lr: 0.002667, loss: 1.3800
2022-03-12 17:04:41 - train: epoch 0094, iter [00500, 05004], lr: 0.002667, loss: 1.2629
2022-03-12 17:05:23 - train: epoch 0094, iter [00600, 05004], lr: 0.002667, loss: 1.2082
2022-03-12 17:06:05 - train: epoch 0094, iter [00700, 05004], lr: 0.002667, loss: 1.4973
2022-03-12 17:06:49 - train: epoch 0094, iter [00800, 05004], lr: 0.002667, loss: 1.3741
2022-03-12 17:07:31 - train: epoch 0094, iter [00900, 05004], lr: 0.002667, loss: 1.4155
2022-03-12 17:08:13 - train: epoch 0094, iter [01000, 05004], lr: 0.002667, loss: 1.3511
2022-03-12 17:08:56 - train: epoch 0094, iter [01100, 05004], lr: 0.002667, loss: 1.3924
2022-03-12 17:09:38 - train: epoch 0094, iter [01200, 05004], lr: 0.002667, loss: 1.2107
2022-03-12 17:10:20 - train: epoch 0094, iter [01300, 05004], lr: 0.002667, loss: 1.1807
2022-03-12 17:11:03 - train: epoch 0094, iter [01400, 05004], lr: 0.002667, loss: 1.2573
2022-03-12 17:11:44 - train: epoch 0094, iter [01500, 05004], lr: 0.002667, loss: 1.4103
2022-03-12 17:12:27 - train: epoch 0094, iter [01600, 05004], lr: 0.002667, loss: 1.4837
2022-03-12 17:13:08 - train: epoch 0094, iter [01700, 05004], lr: 0.002667, loss: 1.2447
2022-03-12 17:13:49 - train: epoch 0094, iter [01800, 05004], lr: 0.002667, loss: 1.2056
2022-03-12 17:14:29 - train: epoch 0094, iter [01900, 05004], lr: 0.002667, loss: 1.3416
2022-03-12 17:15:09 - train: epoch 0094, iter [02000, 05004], lr: 0.002667, loss: 1.0913
2022-03-12 17:15:50 - train: epoch 0094, iter [02100, 05004], lr: 0.002667, loss: 1.1636
2022-03-12 17:16:31 - train: epoch 0094, iter [02200, 05004], lr: 0.002667, loss: 1.3418
2022-03-12 17:17:13 - train: epoch 0094, iter [02300, 05004], lr: 0.002667, loss: 1.3368
2022-03-12 17:17:56 - train: epoch 0094, iter [02400, 05004], lr: 0.002667, loss: 1.2288
2022-03-12 17:18:38 - train: epoch 0094, iter [02500, 05004], lr: 0.002667, loss: 1.3920
2022-03-12 17:19:20 - train: epoch 0094, iter [02600, 05004], lr: 0.002667, loss: 1.2963
2022-03-12 17:20:01 - train: epoch 0094, iter [02700, 05004], lr: 0.002667, loss: 1.3129
2022-03-12 17:20:43 - train: epoch 0094, iter [02800, 05004], lr: 0.002667, loss: 1.4493
2022-03-12 17:21:25 - train: epoch 0094, iter [02900, 05004], lr: 0.002667, loss: 1.2565
2022-03-12 17:22:05 - train: epoch 0094, iter [03000, 05004], lr: 0.002667, loss: 1.3648
2022-03-12 17:22:47 - train: epoch 0094, iter [03100, 05004], lr: 0.002667, loss: 1.5453
2022-03-12 17:23:27 - train: epoch 0094, iter [03200, 05004], lr: 0.002667, loss: 1.4029
2022-03-12 17:24:09 - train: epoch 0094, iter [03300, 05004], lr: 0.002667, loss: 1.2559
2022-03-12 17:24:50 - train: epoch 0094, iter [03400, 05004], lr: 0.002667, loss: 1.3007
2022-03-12 17:25:32 - train: epoch 0094, iter [03500, 05004], lr: 0.002667, loss: 1.5634
2022-03-12 17:26:13 - train: epoch 0094, iter [03600, 05004], lr: 0.002667, loss: 1.1224
2022-03-12 17:26:54 - train: epoch 0094, iter [03700, 05004], lr: 0.002667, loss: 1.3486
2022-03-12 17:27:35 - train: epoch 0094, iter [03800, 05004], lr: 0.002667, loss: 1.1521
2022-03-12 17:28:15 - train: epoch 0094, iter [03900, 05004], lr: 0.002667, loss: 1.3253
2022-03-12 17:28:55 - train: epoch 0094, iter [04000, 05004], lr: 0.002667, loss: 1.4375
2022-03-12 17:29:36 - train: epoch 0094, iter [04100, 05004], lr: 0.002667, loss: 1.3857
2022-03-12 17:30:16 - train: epoch 0094, iter [04200, 05004], lr: 0.002667, loss: 1.2134
2022-03-12 17:30:57 - train: epoch 0094, iter [04300, 05004], lr: 0.002667, loss: 1.4787
2022-03-12 17:31:38 - train: epoch 0094, iter [04400, 05004], lr: 0.002667, loss: 1.4864
2022-03-12 17:32:18 - train: epoch 0094, iter [04500, 05004], lr: 0.002667, loss: 1.2003
2022-03-12 17:33:00 - train: epoch 0094, iter [04600, 05004], lr: 0.002667, loss: 1.5022
2022-03-12 17:33:41 - train: epoch 0094, iter [04700, 05004], lr: 0.002667, loss: 1.2700
2022-03-12 17:34:23 - train: epoch 0094, iter [04800, 05004], lr: 0.002667, loss: 1.2650
2022-03-12 17:35:05 - train: epoch 0094, iter [04900, 05004], lr: 0.002667, loss: 1.3594
2022-03-12 17:35:45 - train: epoch 0094, iter [05000, 05004], lr: 0.002667, loss: 1.3581
2022-03-12 17:35:47 - train: epoch 094, train_loss: 1.3258
2022-03-12 17:37:15 - eval: epoch: 094, acc1: 69.986%, acc5: 89.462%, test_loss: 1.2095, per_image_load_time: 3.196ms, per_image_inference_time: 0.267ms
2022-03-12 17:37:15 - until epoch: 094, best_acc1: 69.986%
2022-03-12 17:37:15 - epoch 095 lr: 0.0019620034125190644
2022-03-12 17:38:01 - train: epoch 0095, iter [00100, 05004], lr: 0.001962, loss: 1.4848
2022-03-12 17:38:43 - train: epoch 0095, iter [00200, 05004], lr: 0.001962, loss: 1.4126
2022-03-12 17:39:25 - train: epoch 0095, iter [00300, 05004], lr: 0.001962, loss: 1.3220
2022-03-12 17:40:07 - train: epoch 0095, iter [00400, 05004], lr: 0.001962, loss: 1.3293
2022-03-12 17:40:49 - train: epoch 0095, iter [00500, 05004], lr: 0.001962, loss: 1.1762
2022-03-12 17:41:31 - train: epoch 0095, iter [00600, 05004], lr: 0.001962, loss: 1.2372
2022-03-12 17:42:11 - train: epoch 0095, iter [00700, 05004], lr: 0.001962, loss: 1.4542
2022-03-12 17:42:52 - train: epoch 0095, iter [00800, 05004], lr: 0.001962, loss: 1.4483
2022-03-12 17:43:34 - train: epoch 0095, iter [00900, 05004], lr: 0.001962, loss: 1.5015
2022-03-12 17:44:14 - train: epoch 0095, iter [01000, 05004], lr: 0.001962, loss: 1.3882
2022-03-12 17:44:55 - train: epoch 0095, iter [01100, 05004], lr: 0.001962, loss: 1.2380
2022-03-12 17:45:38 - train: epoch 0095, iter [01200, 05004], lr: 0.001962, loss: 1.3926
2022-03-12 17:46:22 - train: epoch 0095, iter [01300, 05004], lr: 0.001962, loss: 1.3133
2022-03-12 17:47:05 - train: epoch 0095, iter [01400, 05004], lr: 0.001962, loss: 1.4664
2022-03-12 17:47:48 - train: epoch 0095, iter [01500, 05004], lr: 0.001962, loss: 1.2940
2022-03-12 17:48:28 - train: epoch 0095, iter [01600, 05004], lr: 0.001962, loss: 1.1986
2022-03-12 17:49:09 - train: epoch 0095, iter [01700, 05004], lr: 0.001962, loss: 1.2837
2022-03-12 17:49:51 - train: epoch 0095, iter [01800, 05004], lr: 0.001962, loss: 1.4355
2022-03-12 17:50:33 - train: epoch 0095, iter [01900, 05004], lr: 0.001962, loss: 1.2221
2022-03-12 17:51:14 - train: epoch 0095, iter [02000, 05004], lr: 0.001962, loss: 1.4102
2022-03-12 17:51:54 - train: epoch 0095, iter [02100, 05004], lr: 0.001962, loss: 1.4130
2022-03-12 17:52:35 - train: epoch 0095, iter [02200, 05004], lr: 0.001962, loss: 1.1842
2022-03-12 17:53:17 - train: epoch 0095, iter [02300, 05004], lr: 0.001962, loss: 1.3276
2022-03-12 17:53:58 - train: epoch 0095, iter [02400, 05004], lr: 0.001962, loss: 1.3218
2022-03-12 17:54:39 - train: epoch 0095, iter [02500, 05004], lr: 0.001962, loss: 1.1842
2022-03-12 17:55:20 - train: epoch 0095, iter [02600, 05004], lr: 0.001962, loss: 1.0527
2022-03-12 17:56:01 - train: epoch 0095, iter [02700, 05004], lr: 0.001962, loss: 1.1918
2022-03-12 17:56:40 - train: epoch 0095, iter [02800, 05004], lr: 0.001962, loss: 1.0856
2022-03-12 17:57:21 - train: epoch 0095, iter [02900, 05004], lr: 0.001962, loss: 1.5394
2022-03-12 17:58:03 - train: epoch 0095, iter [03000, 05004], lr: 0.001962, loss: 1.2594
2022-03-12 17:58:44 - train: epoch 0095, iter [03100, 05004], lr: 0.001962, loss: 1.3618
2022-03-12 17:59:27 - train: epoch 0095, iter [03200, 05004], lr: 0.001962, loss: 1.3405
2022-03-12 18:00:11 - train: epoch 0095, iter [03300, 05004], lr: 0.001962, loss: 1.3971
2022-03-12 18:00:54 - train: epoch 0095, iter [03400, 05004], lr: 0.001962, loss: 1.1224
2022-03-12 18:01:36 - train: epoch 0095, iter [03500, 05004], lr: 0.001962, loss: 1.1566
2022-03-12 18:02:19 - train: epoch 0095, iter [03600, 05004], lr: 0.001962, loss: 1.2047
2022-03-12 18:03:01 - train: epoch 0095, iter [03700, 05004], lr: 0.001962, loss: 1.4032
2022-03-12 18:03:43 - train: epoch 0095, iter [03800, 05004], lr: 0.001962, loss: 1.2007
2022-03-12 18:04:25 - train: epoch 0095, iter [03900, 05004], lr: 0.001962, loss: 1.3120
2022-03-12 18:05:06 - train: epoch 0095, iter [04000, 05004], lr: 0.001962, loss: 1.0786
2022-03-12 18:05:49 - train: epoch 0095, iter [04100, 05004], lr: 0.001962, loss: 1.3536
2022-03-12 18:06:31 - train: epoch 0095, iter [04200, 05004], lr: 0.001962, loss: 1.1669
2022-03-12 18:07:13 - train: epoch 0095, iter [04300, 05004], lr: 0.001962, loss: 1.4732
2022-03-12 18:07:55 - train: epoch 0095, iter [04400, 05004], lr: 0.001962, loss: 1.3723
2022-03-12 18:08:36 - train: epoch 0095, iter [04500, 05004], lr: 0.001962, loss: 1.2786
2022-03-12 18:09:17 - train: epoch 0095, iter [04600, 05004], lr: 0.001962, loss: 1.2640
2022-03-12 18:09:58 - train: epoch 0095, iter [04700, 05004], lr: 0.001962, loss: 1.1826
2022-03-12 18:10:37 - train: epoch 0095, iter [04800, 05004], lr: 0.001962, loss: 1.1103
2022-03-12 18:11:17 - train: epoch 0095, iter [04900, 05004], lr: 0.001962, loss: 1.1995
2022-03-12 18:11:58 - train: epoch 0095, iter [05000, 05004], lr: 0.001962, loss: 1.2715
2022-03-12 18:12:00 - train: epoch 095, train_loss: 1.3090
2022-03-12 18:13:27 - eval: epoch: 095, acc1: 70.122%, acc5: 89.668%, test_loss: 1.2006, per_image_load_time: 2.848ms, per_image_inference_time: 0.270ms
2022-03-12 18:13:27 - until epoch: 095, best_acc1: 70.122%
2022-03-12 18:13:27 - epoch 096 lr: 0.0013638696597277678
2022-03-12 18:14:14 - train: epoch 0096, iter [00100, 05004], lr: 0.001364, loss: 1.2461
2022-03-12 18:14:56 - train: epoch 0096, iter [00200, 05004], lr: 0.001364, loss: 1.1789
2022-03-12 18:15:39 - train: epoch 0096, iter [00300, 05004], lr: 0.001364, loss: 1.1366
2022-03-12 18:16:21 - train: epoch 0096, iter [00400, 05004], lr: 0.001364, loss: 1.2251
2022-03-12 18:17:04 - train: epoch 0096, iter [00500, 05004], lr: 0.001364, loss: 1.3409
2022-03-12 18:17:48 - train: epoch 0096, iter [00600, 05004], lr: 0.001364, loss: 1.3035
2022-03-12 18:18:30 - train: epoch 0096, iter [00700, 05004], lr: 0.001364, loss: 1.3883
2022-03-12 18:19:14 - train: epoch 0096, iter [00800, 05004], lr: 0.001364, loss: 1.1278
2022-03-12 18:19:55 - train: epoch 0096, iter [00900, 05004], lr: 0.001364, loss: 1.3373
2022-03-12 18:20:39 - train: epoch 0096, iter [01000, 05004], lr: 0.001364, loss: 1.3201
2022-03-12 18:21:21 - train: epoch 0096, iter [01100, 05004], lr: 0.001364, loss: 1.3695
2022-03-12 18:22:04 - train: epoch 0096, iter [01200, 05004], lr: 0.001364, loss: 1.1129
2022-03-12 18:22:45 - train: epoch 0096, iter [01300, 05004], lr: 0.001364, loss: 1.3994
2022-03-12 18:23:26 - train: epoch 0096, iter [01400, 05004], lr: 0.001364, loss: 1.2095
2022-03-12 18:24:07 - train: epoch 0096, iter [01500, 05004], lr: 0.001364, loss: 1.2981
2022-03-12 18:24:46 - train: epoch 0096, iter [01600, 05004], lr: 0.001364, loss: 1.0876
2022-03-12 18:25:27 - train: epoch 0096, iter [01700, 05004], lr: 0.001364, loss: 1.2074
2022-03-12 18:26:07 - train: epoch 0096, iter [01800, 05004], lr: 0.001364, loss: 1.4237
2022-03-12 18:26:48 - train: epoch 0096, iter [01900, 05004], lr: 0.001364, loss: 1.3313
2022-03-12 18:27:31 - train: epoch 0096, iter [02000, 05004], lr: 0.001364, loss: 1.2634
2022-03-12 18:28:13 - train: epoch 0096, iter [02100, 05004], lr: 0.001364, loss: 1.1001
2022-03-12 18:28:55 - train: epoch 0096, iter [02200, 05004], lr: 0.001364, loss: 1.0646
2022-03-12 18:29:37 - train: epoch 0096, iter [02300, 05004], lr: 0.001364, loss: 1.3506
2022-03-12 18:30:19 - train: epoch 0096, iter [02400, 05004], lr: 0.001364, loss: 1.4016
2022-03-12 18:31:01 - train: epoch 0096, iter [02500, 05004], lr: 0.001364, loss: 1.2662
2022-03-12 18:31:42 - train: epoch 0096, iter [02600, 05004], lr: 0.001364, loss: 1.3042
2022-03-12 18:32:25 - train: epoch 0096, iter [02700, 05004], lr: 0.001364, loss: 1.3529
2022-03-12 18:33:06 - train: epoch 0096, iter [02800, 05004], lr: 0.001364, loss: 1.2686
2022-03-12 18:33:49 - train: epoch 0096, iter [02900, 05004], lr: 0.001364, loss: 1.3943
2022-03-12 18:34:32 - train: epoch 0096, iter [03000, 05004], lr: 0.001364, loss: 1.3176
2022-03-12 18:35:15 - train: epoch 0096, iter [03100, 05004], lr: 0.001364, loss: 1.4801
2022-03-12 18:35:59 - train: epoch 0096, iter [03200, 05004], lr: 0.001364, loss: 1.3098
2022-03-12 18:36:42 - train: epoch 0096, iter [03300, 05004], lr: 0.001364, loss: 1.3571
2022-03-12 18:37:26 - train: epoch 0096, iter [03400, 05004], lr: 0.001364, loss: 1.4108
2022-03-12 18:38:10 - train: epoch 0096, iter [03500, 05004], lr: 0.001364, loss: 1.2920
2022-03-12 18:38:51 - train: epoch 0096, iter [03600, 05004], lr: 0.001364, loss: 1.1381
2022-03-12 18:39:34 - train: epoch 0096, iter [03700, 05004], lr: 0.001364, loss: 1.3035
2022-03-12 18:40:16 - train: epoch 0096, iter [03800, 05004], lr: 0.001364, loss: 1.1595
2022-03-12 18:40:58 - train: epoch 0096, iter [03900, 05004], lr: 0.001364, loss: 1.0585
2022-03-12 18:41:43 - train: epoch 0096, iter [04000, 05004], lr: 0.001364, loss: 1.2331
2022-03-12 18:42:28 - train: epoch 0096, iter [04100, 05004], lr: 0.001364, loss: 1.2829
2022-03-12 18:43:13 - train: epoch 0096, iter [04200, 05004], lr: 0.001364, loss: 1.1696
2022-03-12 18:43:56 - train: epoch 0096, iter [04300, 05004], lr: 0.001364, loss: 1.1231
2022-03-12 18:44:40 - train: epoch 0096, iter [04400, 05004], lr: 0.001364, loss: 1.1421
2022-03-12 18:45:23 - train: epoch 0096, iter [04500, 05004], lr: 0.001364, loss: 1.5274
2022-03-12 18:46:07 - train: epoch 0096, iter [04600, 05004], lr: 0.001364, loss: 1.3367
2022-03-12 18:46:51 - train: epoch 0096, iter [04700, 05004], lr: 0.001364, loss: 1.3959
2022-03-12 18:47:35 - train: epoch 0096, iter [04800, 05004], lr: 0.001364, loss: 1.3558
2022-03-12 18:48:19 - train: epoch 0096, iter [04900, 05004], lr: 0.001364, loss: 1.4143
2022-03-12 18:49:02 - train: epoch 0096, iter [05000, 05004], lr: 0.001364, loss: 1.3665
2022-03-12 18:49:04 - train: epoch 096, train_loss: 1.2920
2022-03-12 18:50:37 - eval: epoch: 096, acc1: 70.196%, acc5: 89.730%, test_loss: 1.1942, per_image_load_time: 3.272ms, per_image_inference_time: 0.286ms
2022-03-12 18:50:37 - until epoch: 096, best_acc1: 70.196%
2022-03-12 18:50:37 - epoch 097 lr: 0.0008735930673024806
2022-03-12 18:51:25 - train: epoch 0097, iter [00100, 05004], lr: 0.000874, loss: 1.2726
2022-03-12 18:52:09 - train: epoch 0097, iter [00200, 05004], lr: 0.000874, loss: 1.3167
2022-03-12 18:52:51 - train: epoch 0097, iter [00300, 05004], lr: 0.000874, loss: 1.2070
2022-03-12 18:53:33 - train: epoch 0097, iter [00400, 05004], lr: 0.000874, loss: 1.6728
2022-03-12 18:54:15 - train: epoch 0097, iter [00500, 05004], lr: 0.000874, loss: 1.4395
2022-03-12 18:54:58 - train: epoch 0097, iter [00600, 05004], lr: 0.000874, loss: 1.2475
2022-03-12 18:55:41 - train: epoch 0097, iter [00700, 05004], lr: 0.000874, loss: 1.3495
2022-03-12 18:56:24 - train: epoch 0097, iter [00800, 05004], lr: 0.000874, loss: 1.2198
2022-03-12 18:57:09 - train: epoch 0097, iter [00900, 05004], lr: 0.000874, loss: 1.3324
2022-03-12 18:57:53 - train: epoch 0097, iter [01000, 05004], lr: 0.000874, loss: 1.4360
2022-03-12 18:58:36 - train: epoch 0097, iter [01100, 05004], lr: 0.000874, loss: 1.2250
2022-03-12 18:59:20 - train: epoch 0097, iter [01200, 05004], lr: 0.000874, loss: 1.2197
2022-03-12 19:00:05 - train: epoch 0097, iter [01300, 05004], lr: 0.000874, loss: 1.1850
2022-03-12 19:00:49 - train: epoch 0097, iter [01400, 05004], lr: 0.000874, loss: 1.3055
2022-03-12 19:01:33 - train: epoch 0097, iter [01500, 05004], lr: 0.000874, loss: 1.2666
2022-03-12 19:02:16 - train: epoch 0097, iter [01600, 05004], lr: 0.000874, loss: 1.1685
2022-03-12 19:03:01 - train: epoch 0097, iter [01700, 05004], lr: 0.000874, loss: 1.2252
2022-03-12 19:03:44 - train: epoch 0097, iter [01800, 05004], lr: 0.000874, loss: 1.4337
2022-03-12 19:04:26 - train: epoch 0097, iter [01900, 05004], lr: 0.000874, loss: 1.2674
2022-03-12 19:05:10 - train: epoch 0097, iter [02000, 05004], lr: 0.000874, loss: 1.3365
2022-03-12 19:05:53 - train: epoch 0097, iter [02100, 05004], lr: 0.000874, loss: 1.0713
2022-03-12 19:06:37 - train: epoch 0097, iter [02200, 05004], lr: 0.000874, loss: 1.0948
2022-03-12 19:07:17 - train: epoch 0097, iter [02300, 05004], lr: 0.000874, loss: 1.3191
2022-03-12 19:07:58 - train: epoch 0097, iter [02400, 05004], lr: 0.000874, loss: 1.2742
2022-03-12 19:08:38 - train: epoch 0097, iter [02500, 05004], lr: 0.000874, loss: 1.1661
2022-03-12 19:09:20 - train: epoch 0097, iter [02600, 05004], lr: 0.000874, loss: 1.3898
2022-03-12 19:10:03 - train: epoch 0097, iter [02700, 05004], lr: 0.000874, loss: 1.4054
2022-03-12 19:10:45 - train: epoch 0097, iter [02800, 05004], lr: 0.000874, loss: 1.2871
2022-03-12 19:11:28 - train: epoch 0097, iter [02900, 05004], lr: 0.000874, loss: 1.3208
2022-03-12 19:12:11 - train: epoch 0097, iter [03000, 05004], lr: 0.000874, loss: 1.3977
2022-03-12 19:12:53 - train: epoch 0097, iter [03100, 05004], lr: 0.000874, loss: 1.0757
2022-03-12 19:13:36 - train: epoch 0097, iter [03200, 05004], lr: 0.000874, loss: 1.5461
2022-03-12 19:14:19 - train: epoch 0097, iter [03300, 05004], lr: 0.000874, loss: 1.3485
2022-03-12 19:15:01 - train: epoch 0097, iter [03400, 05004], lr: 0.000874, loss: 1.2284
2022-03-12 19:15:43 - train: epoch 0097, iter [03500, 05004], lr: 0.000874, loss: 1.3243
2022-03-12 19:16:27 - train: epoch 0097, iter [03600, 05004], lr: 0.000874, loss: 1.1655
2022-03-12 19:17:10 - train: epoch 0097, iter [03700, 05004], lr: 0.000874, loss: 1.1816
2022-03-12 19:17:52 - train: epoch 0097, iter [03800, 05004], lr: 0.000874, loss: 1.1427
2022-03-12 19:18:35 - train: epoch 0097, iter [03900, 05004], lr: 0.000874, loss: 1.2777
2022-03-12 19:19:17 - train: epoch 0097, iter [04000, 05004], lr: 0.000874, loss: 1.4198
2022-03-12 19:19:59 - train: epoch 0097, iter [04100, 05004], lr: 0.000874, loss: 1.3446
2022-03-12 19:20:41 - train: epoch 0097, iter [04200, 05004], lr: 0.000874, loss: 1.0553
2022-03-12 19:21:22 - train: epoch 0097, iter [04300, 05004], lr: 0.000874, loss: 1.2184
2022-03-12 19:22:04 - train: epoch 0097, iter [04400, 05004], lr: 0.000874, loss: 1.2940
2022-03-12 19:22:46 - train: epoch 0097, iter [04500, 05004], lr: 0.000874, loss: 1.3873
2022-03-12 19:23:29 - train: epoch 0097, iter [04600, 05004], lr: 0.000874, loss: 1.3780
2022-03-12 19:24:13 - train: epoch 0097, iter [04700, 05004], lr: 0.000874, loss: 1.1341
2022-03-12 19:24:57 - train: epoch 0097, iter [04800, 05004], lr: 0.000874, loss: 1.1652
2022-03-12 19:25:41 - train: epoch 0097, iter [04900, 05004], lr: 0.000874, loss: 1.3763
2022-03-12 19:26:24 - train: epoch 0097, iter [05000, 05004], lr: 0.000874, loss: 1.1264
2022-03-12 19:26:26 - train: epoch 097, train_loss: 1.2785
2022-03-12 19:27:58 - eval: epoch: 097, acc1: 70.472%, acc5: 89.860%, test_loss: 1.1860, per_image_load_time: 3.279ms, per_image_inference_time: 0.282ms
2022-03-12 19:27:58 - until epoch: 097, best_acc1: 70.472%
2022-03-12 19:27:58 - epoch 098 lr: 0.0004917097454988584
2022-03-12 19:28:44 - train: epoch 0098, iter [00100, 05004], lr: 0.000492, loss: 1.4682
2022-03-12 19:29:27 - train: epoch 0098, iter [00200, 05004], lr: 0.000492, loss: 1.5652
2022-03-12 19:30:10 - train: epoch 0098, iter [00300, 05004], lr: 0.000492, loss: 1.2941
2022-03-12 19:30:52 - train: epoch 0098, iter [00400, 05004], lr: 0.000492, loss: 1.3202
2022-03-12 19:31:35 - train: epoch 0098, iter [00500, 05004], lr: 0.000492, loss: 1.3701
2022-03-12 19:32:18 - train: epoch 0098, iter [00600, 05004], lr: 0.000492, loss: 1.3278
2022-03-12 19:33:01 - train: epoch 0098, iter [00700, 05004], lr: 0.000492, loss: 1.3173
2022-03-12 19:33:42 - train: epoch 0098, iter [00800, 05004], lr: 0.000492, loss: 1.3250
2022-03-12 19:34:26 - train: epoch 0098, iter [00900, 05004], lr: 0.000492, loss: 1.2243
2022-03-12 19:35:09 - train: epoch 0098, iter [01000, 05004], lr: 0.000492, loss: 1.2040
2022-03-12 19:35:50 - train: epoch 0098, iter [01100, 05004], lr: 0.000492, loss: 1.2400
2022-03-12 19:36:31 - train: epoch 0098, iter [01200, 05004], lr: 0.000492, loss: 1.2845
2022-03-12 19:37:13 - train: epoch 0098, iter [01300, 05004], lr: 0.000492, loss: 1.2806
2022-03-12 19:37:55 - train: epoch 0098, iter [01400, 05004], lr: 0.000492, loss: 1.4305
2022-03-12 19:38:39 - train: epoch 0098, iter [01500, 05004], lr: 0.000492, loss: 1.1814
2022-03-12 19:39:22 - train: epoch 0098, iter [01600, 05004], lr: 0.000492, loss: 1.1933
2022-03-12 19:40:06 - train: epoch 0098, iter [01700, 05004], lr: 0.000492, loss: 1.3500
2022-03-12 19:40:50 - train: epoch 0098, iter [01800, 05004], lr: 0.000492, loss: 1.2997
2022-03-12 19:41:33 - train: epoch 0098, iter [01900, 05004], lr: 0.000492, loss: 1.1929
2022-03-12 19:42:18 - train: epoch 0098, iter [02000, 05004], lr: 0.000492, loss: 1.3915
2022-03-12 19:43:01 - train: epoch 0098, iter [02100, 05004], lr: 0.000492, loss: 1.3469
2022-03-12 19:43:44 - train: epoch 0098, iter [02200, 05004], lr: 0.000492, loss: 1.2730
2022-03-12 19:44:28 - train: epoch 0098, iter [02300, 05004], lr: 0.000492, loss: 1.0899
2022-03-12 19:45:11 - train: epoch 0098, iter [02400, 05004], lr: 0.000492, loss: 1.2939
2022-03-12 19:45:55 - train: epoch 0098, iter [02500, 05004], lr: 0.000492, loss: 1.3282
2022-03-12 19:46:38 - train: epoch 0098, iter [02600, 05004], lr: 0.000492, loss: 1.1040
2022-03-12 19:47:22 - train: epoch 0098, iter [02700, 05004], lr: 0.000492, loss: 1.2963
2022-03-12 19:48:05 - train: epoch 0098, iter [02800, 05004], lr: 0.000492, loss: 1.2187
2022-03-12 19:48:48 - train: epoch 0098, iter [02900, 05004], lr: 0.000492, loss: 1.3235
2022-03-12 19:49:30 - train: epoch 0098, iter [03000, 05004], lr: 0.000492, loss: 1.2472
2022-03-12 19:50:10 - train: epoch 0098, iter [03100, 05004], lr: 0.000492, loss: 1.3222
2022-03-12 19:50:53 - train: epoch 0098, iter [03200, 05004], lr: 0.000492, loss: 1.1329
2022-03-12 19:51:35 - train: epoch 0098, iter [03300, 05004], lr: 0.000492, loss: 1.2986
2022-03-12 19:52:18 - train: epoch 0098, iter [03400, 05004], lr: 0.000492, loss: 1.3546
2022-03-12 19:53:03 - train: epoch 0098, iter [03500, 05004], lr: 0.000492, loss: 1.3919
2022-03-12 19:53:47 - train: epoch 0098, iter [03600, 05004], lr: 0.000492, loss: 1.3588
2022-03-12 19:54:31 - train: epoch 0098, iter [03700, 05004], lr: 0.000492, loss: 1.2464
2022-03-12 19:55:16 - train: epoch 0098, iter [03800, 05004], lr: 0.000492, loss: 1.3055
2022-03-12 19:56:01 - train: epoch 0098, iter [03900, 05004], lr: 0.000492, loss: 1.1402
2022-03-12 19:56:44 - train: epoch 0098, iter [04000, 05004], lr: 0.000492, loss: 1.3647
2022-03-12 19:57:28 - train: epoch 0098, iter [04100, 05004], lr: 0.000492, loss: 1.2797
2022-03-12 19:58:13 - train: epoch 0098, iter [04200, 05004], lr: 0.000492, loss: 1.2062
2022-03-12 19:58:57 - train: epoch 0098, iter [04300, 05004], lr: 0.000492, loss: 1.1979
2022-03-12 19:59:43 - train: epoch 0098, iter [04400, 05004], lr: 0.000492, loss: 1.4292
2022-03-12 20:00:27 - train: epoch 0098, iter [04500, 05004], lr: 0.000492, loss: 1.2152
2022-03-12 20:01:12 - train: epoch 0098, iter [04600, 05004], lr: 0.000492, loss: 1.1591
2022-03-12 20:01:58 - train: epoch 0098, iter [04700, 05004], lr: 0.000492, loss: 1.3631
2022-03-12 20:02:40 - train: epoch 0098, iter [04800, 05004], lr: 0.000492, loss: 1.2234
2022-03-12 20:03:23 - train: epoch 0098, iter [04900, 05004], lr: 0.000492, loss: 1.2494
2022-03-12 20:04:04 - train: epoch 0098, iter [05000, 05004], lr: 0.000492, loss: 1.2847
2022-03-12 20:04:07 - train: epoch 098, train_loss: 1.2697
2022-03-12 20:05:36 - eval: epoch: 098, acc1: 70.500%, acc5: 89.924%, test_loss: 1.1811, per_image_load_time: 3.135ms, per_image_inference_time: 0.284ms
2022-03-12 20:05:36 - until epoch: 098, best_acc1: 70.500%
2022-03-12 20:05:36 - epoch 099 lr: 0.00021863727812254652
2022-03-12 20:06:24 - train: epoch 0099, iter [00100, 05004], lr: 0.000219, loss: 1.1155
2022-03-12 20:07:09 - train: epoch 0099, iter [00200, 05004], lr: 0.000219, loss: 1.2859
2022-03-12 20:07:54 - train: epoch 0099, iter [00300, 05004], lr: 0.000219, loss: 1.2459
2022-03-12 20:08:40 - train: epoch 0099, iter [00400, 05004], lr: 0.000219, loss: 1.1575
2022-03-12 20:09:24 - train: epoch 0099, iter [00500, 05004], lr: 0.000219, loss: 1.2593
2022-03-12 20:10:09 - train: epoch 0099, iter [00600, 05004], lr: 0.000219, loss: 1.2443
2022-03-12 20:10:54 - train: epoch 0099, iter [00700, 05004], lr: 0.000219, loss: 1.2997
2022-03-12 20:11:38 - train: epoch 0099, iter [00800, 05004], lr: 0.000219, loss: 1.0212
2022-03-12 20:12:23 - train: epoch 0099, iter [00900, 05004], lr: 0.000219, loss: 1.1074
2022-03-12 20:13:07 - train: epoch 0099, iter [01000, 05004], lr: 0.000219, loss: 1.2281
2022-03-12 20:13:53 - train: epoch 0099, iter [01100, 05004], lr: 0.000219, loss: 1.4400
2022-03-12 20:14:37 - train: epoch 0099, iter [01200, 05004], lr: 0.000219, loss: 1.2023
2022-03-12 20:15:23 - train: epoch 0099, iter [01300, 05004], lr: 0.000219, loss: 1.4934
2022-03-12 20:16:07 - train: epoch 0099, iter [01400, 05004], lr: 0.000219, loss: 1.3574
2022-03-12 20:16:52 - train: epoch 0099, iter [01500, 05004], lr: 0.000219, loss: 1.2844
2022-03-12 20:17:36 - train: epoch 0099, iter [01600, 05004], lr: 0.000219, loss: 1.1669
2022-03-12 20:18:16 - train: epoch 0099, iter [01700, 05004], lr: 0.000219, loss: 1.2263
2022-03-12 20:19:01 - train: epoch 0099, iter [01800, 05004], lr: 0.000219, loss: 1.3622
2022-03-12 20:19:43 - train: epoch 0099, iter [01900, 05004], lr: 0.000219, loss: 1.3158
2022-03-12 20:20:27 - train: epoch 0099, iter [02000, 05004], lr: 0.000219, loss: 1.2347
2022-03-12 20:21:12 - train: epoch 0099, iter [02100, 05004], lr: 0.000219, loss: 1.2007
2022-03-12 20:21:56 - train: epoch 0099, iter [02200, 05004], lr: 0.000219, loss: 1.1556
2022-03-12 20:22:40 - train: epoch 0099, iter [02300, 05004], lr: 0.000219, loss: 1.1692
2022-03-12 20:23:25 - train: epoch 0099, iter [02400, 05004], lr: 0.000219, loss: 1.4443
2022-03-12 20:24:09 - train: epoch 0099, iter [02500, 05004], lr: 0.000219, loss: 1.2622
2022-03-12 20:24:55 - train: epoch 0099, iter [02600, 05004], lr: 0.000219, loss: 1.2971
2022-03-12 20:25:40 - train: epoch 0099, iter [02700, 05004], lr: 0.000219, loss: 1.2908
2022-03-12 20:26:25 - train: epoch 0099, iter [02800, 05004], lr: 0.000219, loss: 1.2272
2022-03-12 20:27:09 - train: epoch 0099, iter [02900, 05004], lr: 0.000219, loss: 1.1502
2022-03-12 20:27:52 - train: epoch 0099, iter [03000, 05004], lr: 0.000219, loss: 1.2528
2022-03-12 20:28:35 - train: epoch 0099, iter [03100, 05004], lr: 0.000219, loss: 1.2198
2022-03-12 20:29:18 - train: epoch 0099, iter [03200, 05004], lr: 0.000219, loss: 1.2857
2022-03-12 20:30:00 - train: epoch 0099, iter [03300, 05004], lr: 0.000219, loss: 1.2361
2022-03-12 20:30:42 - train: epoch 0099, iter [03400, 05004], lr: 0.000219, loss: 1.3392
2022-03-12 20:31:24 - train: epoch 0099, iter [03500, 05004], lr: 0.000219, loss: 1.2399
2022-03-12 20:32:04 - train: epoch 0099, iter [03600, 05004], lr: 0.000219, loss: 1.1509
2022-03-12 20:32:45 - train: epoch 0099, iter [03700, 05004], lr: 0.000219, loss: 1.1783
2022-03-12 20:33:27 - train: epoch 0099, iter [03800, 05004], lr: 0.000219, loss: 1.3223
2022-03-12 20:34:10 - train: epoch 0099, iter [03900, 05004], lr: 0.000219, loss: 1.3355
2022-03-12 20:34:52 - train: epoch 0099, iter [04000, 05004], lr: 0.000219, loss: 1.1474
2022-03-12 20:35:35 - train: epoch 0099, iter [04100, 05004], lr: 0.000219, loss: 1.2245
2022-03-12 20:36:19 - train: epoch 0099, iter [04200, 05004], lr: 0.000219, loss: 1.4309
2022-03-12 20:37:02 - train: epoch 0099, iter [04300, 05004], lr: 0.000219, loss: 1.2373
2022-03-12 20:37:44 - train: epoch 0099, iter [04400, 05004], lr: 0.000219, loss: 1.0592
2022-03-12 20:38:26 - train: epoch 0099, iter [04500, 05004], lr: 0.000219, loss: 1.3674
2022-03-12 20:39:09 - train: epoch 0099, iter [04600, 05004], lr: 0.000219, loss: 1.3274
2022-03-12 20:39:51 - train: epoch 0099, iter [04700, 05004], lr: 0.000219, loss: 1.2118
2022-03-12 20:40:33 - train: epoch 0099, iter [04800, 05004], lr: 0.000219, loss: 1.1876
2022-03-12 20:41:14 - train: epoch 0099, iter [04900, 05004], lr: 0.000219, loss: 1.3592
2022-03-12 20:41:55 - train: epoch 0099, iter [05000, 05004], lr: 0.000219, loss: 1.3359
2022-03-12 20:41:57 - train: epoch 099, train_loss: 1.2625
2022-03-12 20:43:25 - eval: epoch: 099, acc1: 70.558%, acc5: 89.926%, test_loss: 1.1795, per_image_load_time: 2.911ms, per_image_inference_time: 0.277ms
2022-03-12 20:43:25 - until epoch: 099, best_acc1: 70.558%
2022-03-12 20:43:25 - epoch 100 lr: 5.467426590739511e-05
2022-03-12 20:44:11 - train: epoch 0100, iter [00100, 05004], lr: 0.000055, loss: 1.3186
2022-03-12 20:44:53 - train: epoch 0100, iter [00200, 05004], lr: 0.000055, loss: 1.4201
2022-03-12 20:45:35 - train: epoch 0100, iter [00300, 05004], lr: 0.000055, loss: 1.3219
2022-03-12 20:46:17 - train: epoch 0100, iter [00400, 05004], lr: 0.000055, loss: 1.1572
2022-03-12 20:47:00 - train: epoch 0100, iter [00500, 05004], lr: 0.000055, loss: 1.4403
2022-03-12 20:47:42 - train: epoch 0100, iter [00600, 05004], lr: 0.000055, loss: 1.2463
2022-03-12 20:48:27 - train: epoch 0100, iter [00700, 05004], lr: 0.000055, loss: 1.2347
2022-03-12 20:49:09 - train: epoch 0100, iter [00800, 05004], lr: 0.000055, loss: 1.3007
2022-03-12 20:49:52 - train: epoch 0100, iter [00900, 05004], lr: 0.000055, loss: 1.1046
2022-03-12 20:50:35 - train: epoch 0100, iter [01000, 05004], lr: 0.000055, loss: 1.1565
2022-03-12 20:51:17 - train: epoch 0100, iter [01100, 05004], lr: 0.000055, loss: 1.3058
2022-03-12 20:52:00 - train: epoch 0100, iter [01200, 05004], lr: 0.000055, loss: 1.1992
2022-03-12 20:52:42 - train: epoch 0100, iter [01300, 05004], lr: 0.000055, loss: 1.2512
2022-03-12 20:53:24 - train: epoch 0100, iter [01400, 05004], lr: 0.000055, loss: 1.4774
2022-03-12 20:54:05 - train: epoch 0100, iter [01500, 05004], lr: 0.000055, loss: 1.2992
2022-03-12 20:54:46 - train: epoch 0100, iter [01600, 05004], lr: 0.000055, loss: 1.1974
2022-03-12 20:55:27 - train: epoch 0100, iter [01700, 05004], lr: 0.000055, loss: 1.1349
2022-03-12 20:56:10 - train: epoch 0100, iter [01800, 05004], lr: 0.000055, loss: 1.2836
2022-03-12 20:56:51 - train: epoch 0100, iter [01900, 05004], lr: 0.000055, loss: 1.5700
2022-03-12 20:57:34 - train: epoch 0100, iter [02000, 05004], lr: 0.000055, loss: 1.4111
2022-03-12 20:58:16 - train: epoch 0100, iter [02100, 05004], lr: 0.000055, loss: 1.2289
2022-03-12 20:58:58 - train: epoch 0100, iter [02200, 05004], lr: 0.000055, loss: 1.1720
2022-03-12 20:59:36 - train: epoch 0100, iter [02300, 05004], lr: 0.000055, loss: 1.3953
2022-03-12 21:00:19 - train: epoch 0100, iter [02400, 05004], lr: 0.000055, loss: 1.3087
2022-03-12 21:01:01 - train: epoch 0100, iter [02500, 05004], lr: 0.000055, loss: 1.3208
2022-03-12 21:01:42 - train: epoch 0100, iter [02600, 05004], lr: 0.000055, loss: 1.1399
2022-03-12 21:02:24 - train: epoch 0100, iter [02700, 05004], lr: 0.000055, loss: 1.2822
2022-03-12 21:03:06 - train: epoch 0100, iter [02800, 05004], lr: 0.000055, loss: 1.2338
2022-03-12 21:03:49 - train: epoch 0100, iter [02900, 05004], lr: 0.000055, loss: 1.3290
2022-03-12 21:04:31 - train: epoch 0100, iter [03000, 05004], lr: 0.000055, loss: 1.1350
2022-03-12 21:05:14 - train: epoch 0100, iter [03100, 05004], lr: 0.000055, loss: 1.0328
2022-03-12 21:05:55 - train: epoch 0100, iter [03200, 05004], lr: 0.000055, loss: 1.4614
2022-03-12 21:06:38 - train: epoch 0100, iter [03300, 05004], lr: 0.000055, loss: 1.3823
2022-03-12 21:07:19 - train: epoch 0100, iter [03400, 05004], lr: 0.000055, loss: 1.1817
2022-03-12 21:08:02 - train: epoch 0100, iter [03500, 05004], lr: 0.000055, loss: 1.1292
2022-03-12 21:08:43 - train: epoch 0100, iter [03600, 05004], lr: 0.000055, loss: 1.0554
2022-03-12 21:09:26 - train: epoch 0100, iter [03700, 05004], lr: 0.000055, loss: 1.1881
2022-03-12 21:10:07 - train: epoch 0100, iter [03800, 05004], lr: 0.000055, loss: 1.2629
2022-03-12 21:10:50 - train: epoch 0100, iter [03900, 05004], lr: 0.000055, loss: 1.2409
2022-03-12 21:11:32 - train: epoch 0100, iter [04000, 05004], lr: 0.000055, loss: 1.2871
2022-03-12 21:12:14 - train: epoch 0100, iter [04100, 05004], lr: 0.000055, loss: 1.2223
2022-03-12 21:12:56 - train: epoch 0100, iter [04200, 05004], lr: 0.000055, loss: 1.4264
2022-03-12 21:13:36 - train: epoch 0100, iter [04300, 05004], lr: 0.000055, loss: 1.0682
2022-03-12 21:14:19 - train: epoch 0100, iter [04400, 05004], lr: 0.000055, loss: 1.2618
2022-03-12 21:15:02 - train: epoch 0100, iter [04500, 05004], lr: 0.000055, loss: 1.2898
2022-03-12 21:15:46 - train: epoch 0100, iter [04600, 05004], lr: 0.000055, loss: 1.2922
2022-03-12 21:16:30 - train: epoch 0100, iter [04700, 05004], lr: 0.000055, loss: 1.3846
2022-03-12 21:17:16 - train: epoch 0100, iter [04800, 05004], lr: 0.000055, loss: 1.2107
2022-03-12 21:18:00 - train: epoch 0100, iter [04900, 05004], lr: 0.000055, loss: 1.1853
2022-03-12 21:18:43 - train: epoch 0100, iter [05000, 05004], lr: 0.000055, loss: 1.4958
2022-03-12 21:18:44 - train: epoch 100, train_loss: 1.2584
2022-03-12 21:20:18 - eval: epoch: 100, acc1: 70.602%, acc5: 89.890%, test_loss: 1.1798, per_image_load_time: 3.336ms, per_image_inference_time: 0.282ms
2022-03-12 21:20:18 - until epoch: 100, best_acc1: 70.602%
2022-03-12 21:20:18 - train done. model: RegNetY_200MF, train time: 59.704 hours, best_acc1: 70.602%
