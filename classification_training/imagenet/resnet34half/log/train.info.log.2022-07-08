2022-07-08 21:40:07 - network: resnet34half
2022-07-08 21:40:07 - num_classes: 1000
2022-07-08 21:40:07 - input_image_size: 224
2022-07-08 21:40:07 - scale: 1.1428571428571428
2022-07-08 21:40:07 - trained_model_path: 
2022-07-08 21:40:07 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-08 21:40:07 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-08 21:40:07 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f2a403cf820>
2022-07-08 21:40:07 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f2a403cfaf0>
2022-07-08 21:40:07 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f2a403cfb20>
2022-07-08 21:40:07 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f2a403cfb80>
2022-07-08 21:40:07 - seed: 0
2022-07-08 21:40:07 - batch_size: 256
2022-07-08 21:40:07 - num_workers: 16
2022-07-08 21:40:07 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-08 21:40:07 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-07-08 21:40:07 - epochs: 100
2022-07-08 21:40:07 - print_interval: 100
2022-07-08 21:40:07 - sync_bn: False
2022-07-08 21:40:07 - apex: True
2022-07-08 21:40:07 - use_ema_model: False
2022-07-08 21:40:07 - ema_model_decay: 0.9999
2022-07-08 21:40:07 - gpus_type: NVIDIA RTX A5000
2022-07-08 21:40:07 - gpus_num: 2
2022-07-08 21:40:07 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f2a0d6a3870>
2022-07-08 21:40:07 - --------------------parameters--------------------
2022-07-08 21:40:07 - name: conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-08 21:40:07 - name: fc.weight, grad: True
2022-07-08 21:40:07 - name: fc.bias, grad: True
2022-07-08 21:40:07 - --------------------buffers--------------------
2022-07-08 21:40:07 - name: conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-08 21:40:07 - -----------no weight decay layers--------------
2022-07-08 21:40:07 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-08 21:40:07 - -------------weight decay layers---------------
2022-07-08 21:40:07 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-08 21:40:07 - epoch 001 lr: 0.100000
2022-07-08 21:40:45 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8778
2022-07-08 21:41:18 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7805
2022-07-08 21:41:51 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6491
2022-07-08 21:42:24 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5645
2022-07-08 21:42:58 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4287
2022-07-08 21:43:30 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.2184
2022-07-08 21:44:04 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1278
2022-07-08 21:44:35 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.1040
2022-07-08 21:45:09 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.0264
2022-07-08 21:45:42 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.9067
2022-07-08 21:46:15 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.8446
2022-07-08 21:46:49 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.6296
2022-07-08 21:47:22 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.5916
2022-07-08 21:47:54 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.5591
2022-07-08 21:48:27 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.4767
2022-07-08 21:49:00 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.6011
2022-07-08 21:49:34 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.3318
2022-07-08 21:50:06 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.4068
2022-07-08 21:50:40 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.2507
2022-07-08 21:51:11 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.1106
2022-07-08 21:51:45 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.1749
2022-07-08 21:52:18 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.1424
2022-07-08 21:52:52 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.0093
2022-07-08 21:53:24 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.9623
2022-07-08 21:53:58 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9728
2022-07-08 21:54:30 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.1490
2022-07-08 21:55:04 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.0560
2022-07-08 21:55:37 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.8388
2022-07-08 21:56:10 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.6395
2022-07-08 21:56:43 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.8780
2022-07-08 21:57:17 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.8717
2022-07-08 21:57:49 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.7833
2022-07-08 21:58:23 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.5372
2022-07-08 21:58:56 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.6140
2022-07-08 21:59:29 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.6096
2022-07-08 22:00:01 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.6128
2022-07-08 22:00:34 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.7507
2022-07-08 22:01:07 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.4708
2022-07-08 22:01:41 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.5636
2022-07-08 22:02:14 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.4921
2022-07-08 22:02:47 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.6046
2022-07-08 22:03:20 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.3456
2022-07-08 22:03:52 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.3170
2022-07-08 22:04:26 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.1864
2022-07-08 22:04:57 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.3836
2022-07-08 22:05:31 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5369
2022-07-08 22:06:03 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2714
2022-07-08 22:06:37 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.4300
2022-07-08 22:07:09 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.1516
2022-07-08 22:07:40 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.1342
2022-07-08 22:07:42 - train: epoch 001, train_loss: 5.1442
2022-07-08 22:08:55 - eval: epoch: 001, acc1: 18.238%, acc5: 40.118%, test_loss: 4.1409, per_image_load_time: 2.711ms, per_image_inference_time: 0.159ms
2022-07-08 22:08:55 - until epoch: 001, best_acc1: 18.238%
2022-07-08 22:08:55 - epoch 002 lr: 0.100000
2022-07-08 22:09:34 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.3087
2022-07-08 22:10:07 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.0079
2022-07-08 22:10:39 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.2876
2022-07-08 22:11:12 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2211
2022-07-08 22:11:45 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.9884
2022-07-08 22:12:18 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0192
2022-07-08 22:12:51 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.1617
2022-07-08 22:13:24 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.8017
2022-07-08 22:13:57 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.6407
2022-07-08 22:14:30 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.1544
2022-07-08 22:15:04 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1046
2022-07-08 22:15:36 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.9611
2022-07-08 22:16:10 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.0128
2022-07-08 22:16:43 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.0444
2022-07-08 22:17:16 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.9340
2022-07-08 22:17:50 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8084
2022-07-08 22:18:22 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9864
2022-07-08 22:18:56 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9349
2022-07-08 22:19:29 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8451
2022-07-08 22:20:02 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.5774
2022-07-08 22:20:35 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.8530
2022-07-08 22:21:08 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6275
2022-07-08 22:21:42 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.8253
2022-07-08 22:22:14 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6059
2022-07-08 22:22:49 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.6996
2022-07-08 22:23:21 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.5880
2022-07-08 22:23:54 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.8523
2022-07-08 22:24:27 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.7118
2022-07-08 22:25:01 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.7498
2022-07-08 22:25:33 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5260
2022-07-08 22:26:06 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.5874
2022-07-08 22:26:40 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7194
2022-07-08 22:27:13 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6165
2022-07-08 22:27:46 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8759
2022-07-08 22:28:19 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.5917
2022-07-08 22:28:53 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6669
2022-07-08 22:29:26 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7176
2022-07-08 22:29:59 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.4511
2022-07-08 22:30:33 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6422
2022-07-08 22:31:06 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5111
2022-07-08 22:31:38 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.7095
2022-07-08 22:32:12 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5142
2022-07-08 22:32:45 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5779
2022-07-08 22:33:18 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.3628
2022-07-08 22:33:51 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.5015
2022-07-08 22:34:25 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.4858
2022-07-08 22:34:58 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5441
2022-07-08 22:35:31 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4262
2022-07-08 22:36:04 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4350
2022-07-08 22:36:36 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.3943
2022-07-08 22:36:36 - train: epoch 002, train_loss: 3.7841
2022-07-08 22:37:50 - eval: epoch: 002, acc1: 30.108%, acc5: 55.490%, test_loss: 3.3152, per_image_load_time: 2.693ms, per_image_inference_time: 0.175ms
2022-07-08 22:37:50 - until epoch: 002, best_acc1: 30.108%
2022-07-08 22:37:50 - epoch 003 lr: 0.100000
2022-07-08 22:38:28 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5367
2022-07-08 22:39:01 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.5379
2022-07-08 22:39:34 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.4374
2022-07-08 22:40:06 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4976
2022-07-08 22:40:38 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.5844
2022-07-08 22:41:13 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.2849
2022-07-08 22:41:43 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.6010
2022-07-08 22:42:17 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4872
2022-07-08 22:42:50 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.4398
2022-07-08 22:43:23 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.4410
2022-07-08 22:43:56 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.2383
2022-07-08 22:44:28 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.3040
2022-07-08 22:45:02 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.4454
2022-07-08 22:45:35 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.3330
2022-07-08 22:46:08 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.6264
2022-07-08 22:46:42 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3743
2022-07-08 22:47:14 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2570
2022-07-08 22:47:48 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1829
2022-07-08 22:48:22 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.4201
2022-07-08 22:48:54 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.7664
2022-07-08 22:49:27 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.5492
2022-07-08 22:50:00 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.7602
2022-07-08 22:50:32 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.2911
2022-07-08 22:51:05 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.3248
2022-07-08 22:51:39 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3353
2022-07-08 22:52:12 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.3320
2022-07-08 22:52:46 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.7085
2022-07-08 22:53:19 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.1842
2022-07-08 22:53:53 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.4470
2022-07-08 22:54:24 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.4848
2022-07-08 22:55:00 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.4030
2022-07-08 22:55:32 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.4226
2022-07-08 22:56:06 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.3254
2022-07-08 22:56:38 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3598
2022-07-08 22:57:12 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.1306
2022-07-08 22:57:46 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.1781
2022-07-08 22:58:20 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.2503
2022-07-08 22:58:53 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.3373
2022-07-08 22:59:26 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.4303
2022-07-08 22:59:59 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2931
2022-07-08 23:00:33 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.2343
2022-07-08 23:01:05 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.2361
2022-07-08 23:01:38 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.9129
2022-07-08 23:02:11 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.1220
2022-07-08 23:02:44 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.2560
2022-07-08 23:03:17 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.1556
2022-07-08 23:03:51 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.1304
2022-07-08 23:04:25 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.3564
2022-07-08 23:04:58 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.3254
2022-07-08 23:05:29 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.1924
2022-07-08 23:05:30 - train: epoch 003, train_loss: 3.3362
2022-07-08 23:06:43 - eval: epoch: 003, acc1: 34.384%, acc5: 60.610%, test_loss: 3.0463, per_image_load_time: 2.679ms, per_image_inference_time: 0.152ms
2022-07-08 23:06:43 - until epoch: 003, best_acc1: 34.384%
2022-07-08 23:33:32 - epoch 004 lr: 0.100000
2022-07-08 23:34:11 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.2257
2022-07-08 23:34:43 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.9150
2022-07-08 23:35:17 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.1723
2022-07-08 23:35:49 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.0375
2022-07-08 23:36:22 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.2138
2022-07-08 23:36:55 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.3901
2022-07-08 23:37:29 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.0789
2022-07-08 23:38:02 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.0987
2022-07-08 23:38:34 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.9949
2022-07-08 23:39:07 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.2626
2022-07-08 23:39:40 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.1810
2022-07-08 23:40:13 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.7891
2022-07-08 23:40:46 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.1465
2022-07-08 23:41:19 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.1517
2022-07-08 23:41:52 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.2657
2022-07-08 23:42:25 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.1570
2022-07-08 23:42:58 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1379
2022-07-08 23:43:31 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.0023
2022-07-08 23:44:04 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.2607
2022-07-08 23:44:38 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.0972
2022-07-08 23:45:10 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.3047
2022-07-08 23:45:44 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.1374
2022-07-08 23:46:16 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.8611
2022-07-08 23:46:50 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.9238
2022-07-08 23:47:23 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.0688
2022-07-08 23:47:57 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.1297
2022-07-08 23:48:28 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.0513
2022-07-08 23:49:02 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.2663
2022-07-08 23:49:36 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.1065
2022-07-08 23:50:08 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.2346
2022-07-08 23:50:42 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.8702
2022-07-08 23:51:15 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.9843
2022-07-08 23:51:48 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.1418
2022-07-08 23:52:22 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.1898
2022-07-08 23:52:54 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.1175
2022-07-08 23:53:27 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.9442
2022-07-08 23:54:00 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.9807
2022-07-08 23:54:34 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.9302
2022-07-08 23:55:06 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.8985
2022-07-08 23:55:40 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 3.0177
2022-07-08 23:56:13 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.9226
2022-07-08 23:56:46 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.1736
2022-07-08 23:57:19 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9854
2022-07-08 23:57:51 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.7987
2022-07-08 23:58:24 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.5814
2022-07-08 23:58:58 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.0437
2022-07-08 23:59:31 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.8873
2022-07-09 00:00:04 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.9742
2022-07-09 00:00:37 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.1160
2022-07-09 00:01:09 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.0507
2022-07-09 00:01:10 - train: epoch 004, train_loss: 3.1176
2022-07-09 00:02:23 - eval: epoch: 004, acc1: 35.472%, acc5: 61.974%, test_loss: 2.9903, per_image_load_time: 2.629ms, per_image_inference_time: 0.163ms
2022-07-09 00:02:23 - until epoch: 004, best_acc1: 35.472%
2022-07-09 00:02:23 - epoch 005 lr: 0.100000
2022-07-09 00:03:01 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.0798
2022-07-09 00:03:33 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0915
2022-07-09 00:04:07 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.2381
2022-07-09 00:04:39 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.0072
2022-07-09 00:05:12 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.7426
2022-07-09 00:05:45 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.9754
2022-07-09 00:06:17 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.8749
2022-07-09 00:06:51 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.1250
2022-07-09 00:07:23 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.1103
2022-07-09 00:07:56 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.2293
2022-07-09 00:08:28 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.8998
2022-07-09 00:09:01 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.2136
2022-07-09 00:09:34 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.8111
2022-07-09 00:10:07 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0298
2022-07-09 00:10:40 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.7834
2022-07-09 00:11:13 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.9261
2022-07-09 00:11:47 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.2156
2022-07-09 00:12:19 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.2434
2022-07-09 00:12:53 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.9012
2022-07-09 00:13:26 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9263
2022-07-09 00:13:59 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.8514
2022-07-09 00:14:33 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.1234
2022-07-09 00:15:06 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.9169
2022-07-09 00:15:39 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.9580
2022-07-09 00:16:11 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.0305
2022-07-09 00:16:45 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.0361
2022-07-09 00:17:18 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.0019
2022-07-09 00:17:51 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.0050
2022-07-09 00:18:23 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.8969
2022-07-09 00:18:56 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.7549
2022-07-09 00:19:30 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.1862
2022-07-09 00:20:03 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.8252
2022-07-09 00:20:37 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.8281
2022-07-09 00:21:10 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.9550
2022-07-09 00:21:43 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.7074
2022-07-09 00:22:15 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.0011
2022-07-09 00:22:48 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.8388
2022-07-09 00:23:22 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.0574
2022-07-09 00:23:54 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.1230
2022-07-09 00:24:27 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8591
2022-07-09 00:25:00 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.8952
2022-07-09 00:25:33 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.0733
2022-07-09 00:26:07 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.9448
2022-07-09 00:26:40 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.0090
2022-07-09 00:27:14 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.1831
2022-07-09 00:27:46 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.0374
2022-07-09 00:28:20 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.8825
2022-07-09 00:28:53 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.6965
2022-07-09 00:29:26 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.0286
2022-07-09 00:29:58 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.8759
2022-07-09 00:29:58 - train: epoch 005, train_loss: 2.9896
2022-07-09 00:31:12 - eval: epoch: 005, acc1: 39.402%, acc5: 66.400%, test_loss: 2.7259, per_image_load_time: 2.677ms, per_image_inference_time: 0.169ms
2022-07-09 00:31:12 - until epoch: 005, best_acc1: 39.402%
2022-07-09 00:31:12 - epoch 006 lr: 0.100000
2022-07-09 00:31:51 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.8552
2022-07-09 00:32:23 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.9431
2022-07-09 00:32:57 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.7066
2022-07-09 00:33:30 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.9979
2022-07-09 00:34:03 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.9142
2022-07-09 00:34:36 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.9806
2022-07-09 00:35:09 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.0281
2022-07-09 00:35:43 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8534
2022-07-09 00:36:15 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.7503
2022-07-09 00:36:48 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.7112
2022-07-09 00:37:21 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.8286
2022-07-09 00:37:54 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.1644
2022-07-09 00:38:28 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.9430
2022-07-09 00:39:01 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.1218
2022-07-09 00:39:34 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.2087
2022-07-09 00:40:06 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.8158
2022-07-09 00:40:40 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.0799
2022-07-09 00:41:13 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8737
2022-07-09 00:41:46 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.9848
2022-07-09 00:42:19 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.2131
2022-07-09 00:42:52 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.9075
2022-07-09 00:43:26 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.8261
2022-07-09 00:43:58 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.8668
2022-07-09 00:44:32 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7251
2022-07-09 00:45:04 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9403
2022-07-09 00:45:38 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.7475
2022-07-09 00:46:11 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.9657
2022-07-09 00:46:44 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.5133
2022-07-09 00:47:17 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.9373
2022-07-09 00:47:50 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.9518
2022-07-09 00:48:23 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.6794
2022-07-09 00:48:56 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 3.0083
2022-07-09 00:49:29 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.8053
2022-07-09 00:50:03 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.0359
2022-07-09 00:50:36 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.1445
2022-07-09 00:51:09 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.8799
2022-07-09 00:51:42 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.8818
2022-07-09 00:52:15 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.7559
2022-07-09 00:52:49 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.7858
2022-07-09 00:53:23 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.9980
2022-07-09 00:53:55 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.8276
2022-07-09 00:54:28 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.5755
2022-07-09 00:55:02 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.7989
2022-07-09 00:55:35 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.0330
2022-07-09 00:56:08 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.9218
2022-07-09 00:56:41 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.8582
2022-07-09 00:57:14 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.9808
2022-07-09 00:57:48 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.9569
2022-07-09 00:58:20 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.0180
2022-07-09 00:58:52 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.6477
2022-07-09 00:58:53 - train: epoch 006, train_loss: 2.9099
2022-07-09 01:00:05 - eval: epoch: 006, acc1: 39.772%, acc5: 66.032%, test_loss: 2.7320, per_image_load_time: 2.533ms, per_image_inference_time: 0.159ms
2022-07-09 01:00:05 - until epoch: 006, best_acc1: 39.772%
2022-07-09 01:00:05 - epoch 007 lr: 0.100000
2022-07-09 01:00:44 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.7369
2022-07-09 01:01:15 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.0481
2022-07-09 01:01:50 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9501
2022-07-09 01:02:22 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7255
2022-07-09 01:02:55 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.7388
2022-07-09 01:03:29 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.9245
2022-07-09 01:04:02 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 3.0024
2022-07-09 01:04:34 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.8971
2022-07-09 01:05:08 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.9045
2022-07-09 01:05:40 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.9054
2022-07-09 01:06:15 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.9039
2022-07-09 01:06:47 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.7625
2022-07-09 01:07:21 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4648
2022-07-09 01:07:53 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.9457
2022-07-09 01:08:26 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8423
2022-07-09 01:08:59 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.8373
2022-07-09 01:09:33 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.8232
2022-07-09 01:10:05 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.8064
2022-07-09 01:10:40 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.8506
2022-07-09 01:11:12 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.7045
2022-07-09 01:11:45 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.9510
2022-07-09 01:12:17 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.7421
2022-07-09 01:12:51 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.9823
2022-07-09 01:13:23 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 3.0600
2022-07-09 01:13:57 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 3.0066
2022-07-09 01:14:29 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.8007
2022-07-09 01:15:03 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.6167
2022-07-09 01:15:35 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.5990
2022-07-09 01:16:08 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.7357
2022-07-09 01:16:41 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.9763
2022-07-09 01:17:14 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.8237
2022-07-09 01:17:47 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.6949
2022-07-09 01:18:20 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.0335
2022-07-09 01:18:53 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 3.0188
2022-07-09 01:19:26 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.7763
2022-07-09 01:19:59 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.6267
2022-07-09 01:20:33 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.7796
2022-07-09 01:21:05 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.0312
2022-07-09 01:21:39 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.8047
2022-07-09 01:22:11 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 3.0384
2022-07-09 01:22:45 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.7834
2022-07-09 01:23:18 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.5937
2022-07-09 01:23:50 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.9701
2022-07-09 01:24:24 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.7740
2022-07-09 01:24:57 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.9570
2022-07-09 01:25:30 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 3.0651
2022-07-09 01:26:04 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.8016
2022-07-09 01:26:37 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.8461
2022-07-09 01:27:10 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.6534
2022-07-09 01:27:42 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.9722
2022-07-09 01:27:43 - train: epoch 007, train_loss: 2.8514
2022-07-09 01:28:55 - eval: epoch: 007, acc1: 40.646%, acc5: 67.118%, test_loss: 2.6795, per_image_load_time: 1.616ms, per_image_inference_time: 0.168ms
2022-07-09 01:28:56 - until epoch: 007, best_acc1: 40.646%
2022-07-09 01:28:56 - epoch 008 lr: 0.100000
2022-07-09 01:29:34 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6831
2022-07-09 01:30:07 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.8942
2022-07-09 01:30:38 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.7132
2022-07-09 01:31:11 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.6735
2022-07-09 01:31:43 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.5339
2022-07-09 01:32:16 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.7757
2022-07-09 01:32:48 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.2902
2022-07-09 01:33:22 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.7036
2022-07-09 01:33:55 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.8316
2022-07-09 01:34:28 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.8717
2022-07-09 01:35:01 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.7029
2022-07-09 01:35:34 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.6658
2022-07-09 01:36:07 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 3.0374
2022-07-09 01:36:41 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.8128
2022-07-09 01:37:13 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.8414
2022-07-09 01:37:47 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.8749
2022-07-09 01:38:19 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.9331
2022-07-09 01:38:53 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.8991
2022-07-09 01:39:25 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.8487
2022-07-09 01:39:59 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.8547
2022-07-09 01:40:32 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.7009
2022-07-09 01:41:05 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.4943
2022-07-09 01:41:38 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.7308
2022-07-09 01:42:11 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.7632
2022-07-09 01:42:44 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.6892
2022-07-09 01:43:17 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.7833
2022-07-09 01:43:50 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.0891
2022-07-09 01:44:23 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.8528
2022-07-09 01:44:56 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.9341
2022-07-09 01:45:29 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.9932
2022-07-09 01:46:02 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 3.0016
2022-07-09 01:46:35 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.8782
2022-07-09 01:47:08 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.0695
2022-07-09 01:47:41 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.8109
2022-07-09 01:48:15 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.9256
2022-07-09 01:48:47 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.8146
2022-07-09 01:49:21 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.7246
2022-07-09 01:49:54 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.8462
2022-07-09 01:50:28 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.9484
2022-07-09 01:51:00 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.2338
2022-07-09 01:51:34 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5936
2022-07-09 01:52:07 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.7676
2022-07-09 01:52:41 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.6314
2022-07-09 01:53:13 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.8569
2022-07-09 01:53:47 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.8623
2022-07-09 01:54:20 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.7259
2022-07-09 01:54:53 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.8389
2022-07-09 01:55:26 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.8531
2022-07-09 01:55:59 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.6951
2022-07-09 01:56:30 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.6361
2022-07-09 01:56:31 - train: epoch 008, train_loss: 2.8084
2022-07-09 01:57:44 - eval: epoch: 008, acc1: 43.266%, acc5: 69.986%, test_loss: 2.5180, per_image_load_time: 2.216ms, per_image_inference_time: 0.152ms
2022-07-09 01:57:44 - until epoch: 008, best_acc1: 43.266%
2022-07-09 01:57:44 - epoch 009 lr: 0.100000
2022-07-09 01:58:22 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.4071
2022-07-09 01:58:56 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.8326
2022-07-09 01:59:27 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.5172
2022-07-09 02:00:01 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.0634
2022-07-09 02:00:34 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.6496
2022-07-09 02:01:07 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.5791
2022-07-09 02:01:40 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.7592
2022-07-09 02:02:13 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.6239
2022-07-09 02:02:46 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.4724
2022-07-09 02:03:18 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.6147
2022-07-09 02:03:52 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.0632
2022-07-09 02:04:24 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.6982
2022-07-09 02:04:57 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.0111
2022-07-09 02:05:30 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.5276
2022-07-09 02:06:03 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.6303
2022-07-09 02:06:36 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.8523
2022-07-09 02:07:10 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.8134
2022-07-09 02:07:43 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.6868
2022-07-09 02:08:16 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.5740
2022-07-09 02:08:49 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.6691
2022-07-09 02:09:22 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.8646
2022-07-09 02:09:56 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.8584
2022-07-09 02:10:28 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.7470
2022-07-09 02:11:01 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.7761
2022-07-09 02:11:34 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.6407
2022-07-09 02:12:07 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.7924
2022-07-09 02:12:40 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.5607
2022-07-09 02:13:14 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 3.0648
2022-07-09 02:13:46 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.5821
2022-07-09 02:14:19 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.7172
2022-07-09 02:14:52 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.9386
2022-07-09 02:15:25 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.8264
2022-07-09 02:15:59 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 3.0065
2022-07-09 02:16:32 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.9863
2022-07-09 02:17:04 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6985
2022-07-09 02:17:37 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.6019
2022-07-09 02:18:11 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.9908
2022-07-09 02:18:43 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.7653
2022-07-09 02:19:17 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.5315
2022-07-09 02:19:50 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.9265
2022-07-09 02:20:22 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.6302
2022-07-09 02:20:56 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.6273
2022-07-09 02:21:29 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.7146
2022-07-09 02:22:03 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.8710
2022-07-09 02:22:36 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.8275
2022-07-09 02:23:09 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.9630
2022-07-09 02:23:42 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.9851
2022-07-09 02:24:15 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.8101
2022-07-09 02:24:49 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.8990
2022-07-09 02:25:20 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.7509
2022-07-09 02:25:21 - train: epoch 009, train_loss: 2.7724
2022-07-09 02:26:34 - eval: epoch: 009, acc1: 42.200%, acc5: 68.964%, test_loss: 2.5868, per_image_load_time: 2.654ms, per_image_inference_time: 0.155ms
2022-07-09 02:26:34 - until epoch: 009, best_acc1: 43.266%
2022-07-09 02:26:34 - epoch 010 lr: 0.100000
2022-07-09 02:27:12 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.6371
2022-07-09 02:27:45 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.8686
2022-07-09 02:28:17 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.7433
2022-07-09 02:28:51 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.9000
2022-07-09 02:29:23 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.7199
2022-07-09 02:29:56 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.9076
2022-07-09 02:30:29 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.7895
2022-07-09 02:31:02 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 3.0987
2022-07-09 02:31:35 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.7796
2022-07-09 02:32:08 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.8159
2022-07-09 02:32:42 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5991
2022-07-09 02:33:15 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.6353
2022-07-09 02:33:48 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.5813
2022-07-09 02:34:21 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.9120
2022-07-09 02:34:55 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.5910
2022-07-09 02:35:27 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.7675
2022-07-09 02:36:02 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.9545
2022-07-09 02:36:34 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5643
2022-07-09 02:37:08 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.6752
2022-07-09 02:37:41 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.7808
2022-07-09 02:38:14 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.5929
2022-07-09 02:38:47 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.9499
2022-07-09 02:39:20 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.8112
2022-07-09 02:39:54 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.7481
2022-07-09 02:40:27 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.9023
2022-07-09 02:41:00 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.9746
2022-07-09 02:41:33 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.4979
2022-07-09 02:42:06 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.7844
2022-07-09 02:42:41 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.9278
2022-07-09 02:43:13 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.6399
2022-07-09 02:43:46 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.7737
2022-07-09 02:44:20 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.6882
2022-07-09 02:44:53 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.6438
2022-07-09 02:45:26 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.7711
2022-07-09 02:45:58 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.9987
2022-07-09 02:46:32 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.9812
2022-07-09 02:47:05 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.5858
2022-07-09 02:47:39 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.6952
2022-07-09 02:48:11 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.4331
2022-07-09 02:48:45 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.6595
2022-07-09 02:49:18 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.6690
2022-07-09 02:49:51 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.6439
2022-07-09 02:50:24 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.7566
2022-07-09 02:50:58 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.6005
2022-07-09 02:51:32 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.4940
2022-07-09 02:52:05 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.7407
2022-07-09 02:52:38 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.6953
2022-07-09 02:53:11 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.7066
2022-07-09 02:53:44 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.6216
2022-07-09 02:54:16 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.5339
2022-07-09 02:54:17 - train: epoch 010, train_loss: 2.7445
2022-07-09 02:55:31 - eval: epoch: 010, acc1: 42.154%, acc5: 68.824%, test_loss: 2.5787, per_image_load_time: 2.722ms, per_image_inference_time: 0.148ms
2022-07-09 02:55:31 - until epoch: 010, best_acc1: 43.266%
2022-07-09 02:55:31 - epoch 011 lr: 0.100000
2022-07-09 02:56:08 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.6575
2022-07-09 02:56:42 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.7059
2022-07-09 02:57:14 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.7354
2022-07-09 02:57:48 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.8758
2022-07-09 02:58:20 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.7812
2022-07-09 02:58:53 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.6801
2022-07-09 02:59:27 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.5201
2022-07-09 02:59:59 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.7532
2022-07-09 03:00:32 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.7150
2022-07-09 03:01:05 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.7342
2022-07-09 03:01:38 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.9039
2022-07-09 03:02:11 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.8481
2022-07-09 03:02:45 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.8411
2022-07-09 03:03:18 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.7145
2022-07-09 03:03:51 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.6614
2022-07-09 03:04:24 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.6636
2022-07-09 03:04:57 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.9438
2022-07-09 03:05:30 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.4760
2022-07-09 03:06:03 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.4073
2022-07-09 03:06:37 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.7833
2022-07-09 03:07:10 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.5484
2022-07-09 03:07:43 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.8906
2022-07-09 03:08:16 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.7757
2022-07-09 03:08:50 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.6642
2022-07-09 03:09:23 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.6618
2022-07-09 03:09:57 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.8329
2022-07-09 03:10:29 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5693
2022-07-09 03:11:02 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.4536
2022-07-09 03:11:36 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.6609
2022-07-09 03:12:10 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.8821
2022-07-09 03:12:42 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.6742
2022-07-09 03:13:16 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.6546
2022-07-09 03:13:48 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.9550
2022-07-09 03:14:21 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.7518
2022-07-09 03:14:55 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.5526
2022-07-09 03:15:28 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.6372
2022-07-09 03:16:02 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.9052
2022-07-09 03:16:34 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.5409
2022-07-09 03:17:08 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.6149
2022-07-09 03:17:41 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.8601
2022-07-09 03:18:14 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.6385
2022-07-09 03:18:47 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.8390
2022-07-09 03:19:20 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.6553
2022-07-09 03:19:54 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.7393
2022-07-09 03:20:27 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.5356
2022-07-09 03:21:00 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.6593
2022-07-09 03:21:34 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.3366
2022-07-09 03:22:06 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.6258
2022-07-09 03:22:40 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.4850
2022-07-09 03:23:12 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.5243
2022-07-09 03:23:12 - train: epoch 011, train_loss: 2.7210
2022-07-09 03:24:26 - eval: epoch: 011, acc1: 44.506%, acc5: 71.172%, test_loss: 2.4406, per_image_load_time: 2.707ms, per_image_inference_time: 0.149ms
2022-07-09 03:24:26 - until epoch: 011, best_acc1: 44.506%
2022-07-09 03:24:26 - epoch 012 lr: 0.100000
2022-07-09 03:25:04 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.4467
2022-07-09 03:25:37 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4377
2022-07-09 03:26:11 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.3610
2022-07-09 03:26:44 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.7757
2022-07-09 03:27:16 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.6840
2022-07-09 03:27:49 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.5760
2022-07-09 03:28:22 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.5869
2022-07-09 03:28:55 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.7988
2022-07-09 03:29:29 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.7536
2022-07-09 03:30:02 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.4370
2022-07-09 03:30:35 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8972
2022-07-09 03:31:08 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.6325
2022-07-09 03:31:41 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.6185
2022-07-09 03:32:14 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.8501
2022-07-09 03:32:47 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.5489
2022-07-09 03:33:20 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.7434
2022-07-09 03:33:54 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.5014
2022-07-09 03:34:26 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.8094
2022-07-09 03:35:00 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.7659
2022-07-09 03:35:32 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.0116
2022-07-09 03:36:06 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.7221
2022-07-09 03:36:38 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.7587
2022-07-09 03:37:12 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.6708
2022-07-09 03:37:46 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.7263
2022-07-09 03:38:19 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.6190
2022-07-09 03:38:52 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.6241
2022-07-09 03:39:25 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.7207
2022-07-09 03:39:59 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.6056
2022-07-09 03:40:32 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.6646
2022-07-09 03:41:05 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.5027
2022-07-09 03:41:39 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.6841
2022-07-09 03:42:10 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.7057
2022-07-09 03:42:44 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.5839
2022-07-09 03:43:17 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.6936
2022-07-09 03:43:51 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.8198
2022-07-09 03:44:24 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.6164
2022-07-09 03:44:58 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.6843
2022-07-09 03:45:30 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.8682
2022-07-09 03:46:04 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.5215
2022-07-09 03:46:37 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.7518
2022-07-09 03:47:10 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.6209
2022-07-09 03:47:44 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.7359
2022-07-09 03:48:16 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.9243
2022-07-09 03:48:49 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.6886
2022-07-09 03:49:23 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.7149
2022-07-09 03:49:56 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.9236
2022-07-09 03:50:30 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.6293
2022-07-09 03:51:04 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.9528
2022-07-09 03:51:37 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5674
2022-07-09 03:52:08 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.4450
2022-07-09 03:52:09 - train: epoch 012, train_loss: 2.7011
2022-07-09 03:53:21 - eval: epoch: 012, acc1: 43.678%, acc5: 70.304%, test_loss: 2.4966, per_image_load_time: 2.133ms, per_image_inference_time: 0.172ms
2022-07-09 03:53:21 - until epoch: 012, best_acc1: 44.506%
2022-07-09 03:53:21 - epoch 013 lr: 0.100000
2022-07-09 03:53:59 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.5351
2022-07-09 03:54:33 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.7180
2022-07-09 03:55:06 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.6222
2022-07-09 03:55:39 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.6256
2022-07-09 03:56:12 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.4591
2022-07-09 03:56:44 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.6677
2022-07-09 03:57:18 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.5192
2022-07-09 03:57:50 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.7311
2022-07-09 03:58:24 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.6047
2022-07-09 03:58:56 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.6655
2022-07-09 03:59:29 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.6900
2022-07-09 04:00:03 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.8860
2022-07-09 04:00:36 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.4998
2022-07-09 04:01:09 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.6890
2022-07-09 04:01:43 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.9067
2022-07-09 04:02:16 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.6488
2022-07-09 04:02:50 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5819
2022-07-09 04:03:22 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.5776
2022-07-09 04:03:56 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.8459
2022-07-09 04:04:28 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.8181
2022-07-09 04:05:02 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.9802
2022-07-09 04:05:35 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.6133
2022-07-09 04:06:08 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.8872
2022-07-09 04:06:42 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.7626
2022-07-09 04:07:15 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.5835
2022-07-09 04:07:48 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.5083
2022-07-09 04:08:22 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.7709
2022-07-09 04:08:54 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.8249
2022-07-09 04:09:28 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.5944
2022-07-09 04:10:01 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.5360
2022-07-09 04:10:34 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.6103
2022-07-09 04:11:07 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.6340
2022-07-09 04:11:40 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.6153
2022-07-09 04:12:14 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.8848
2022-07-09 04:12:46 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.5711
2022-07-09 04:13:19 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.7226
2022-07-09 04:13:53 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.4288
2022-07-09 04:14:25 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.8093
2022-07-09 04:14:59 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.9372
2022-07-09 04:15:32 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.8404
2022-07-09 04:16:06 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.5523
2022-07-09 04:16:38 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.6352
2022-07-09 04:17:12 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.7474
2022-07-09 04:17:45 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.6306
2022-07-09 04:18:19 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.7202
2022-07-09 04:18:52 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.6762
2022-07-09 04:19:24 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5784
2022-07-09 04:19:58 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.7164
2022-07-09 04:20:32 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.9503
2022-07-09 04:21:03 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.7048
2022-07-09 04:21:03 - train: epoch 013, train_loss: 2.6852
2022-07-09 04:22:17 - eval: epoch: 013, acc1: 45.210%, acc5: 71.848%, test_loss: 2.4101, per_image_load_time: 2.551ms, per_image_inference_time: 0.162ms
2022-07-09 04:22:17 - until epoch: 013, best_acc1: 45.210%
2022-07-09 04:22:17 - epoch 014 lr: 0.100000
2022-07-09 04:22:55 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.6175
2022-07-09 04:23:27 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.7486
2022-07-09 04:24:01 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.4777
2022-07-09 04:24:33 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.5286
2022-07-09 04:25:07 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.3589
2022-07-09 04:25:39 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.5968
2022-07-09 04:26:13 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.5255
2022-07-09 04:26:45 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4103
2022-07-09 04:27:18 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.6549
2022-07-09 04:27:52 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.5422
2022-07-09 04:28:25 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.3749
2022-07-09 04:28:58 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.7721
2022-07-09 04:29:32 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.7077
2022-07-09 04:30:04 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.7771
2022-07-09 04:30:37 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.8874
2022-07-09 04:31:10 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.6332
2022-07-09 04:31:44 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.7272
2022-07-09 04:32:18 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 3.0595
2022-07-09 04:32:50 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.4440
2022-07-09 04:33:24 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.5950
2022-07-09 04:33:57 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.5961
2022-07-09 04:34:30 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.6944
2022-07-09 04:35:03 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.5038
2022-07-09 04:35:36 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.7891
2022-07-09 04:36:09 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.5482
2022-07-09 04:36:43 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.6278
2022-07-09 04:37:17 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.5579
2022-07-09 04:37:50 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.9179
2022-07-09 04:38:23 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.7418
2022-07-09 04:38:56 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.7994
2022-07-09 04:39:29 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.8704
2022-07-09 04:40:02 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.6503
2022-07-09 04:40:35 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.5636
2022-07-09 04:41:08 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.5839
2022-07-09 04:41:42 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.7527
2022-07-09 04:42:16 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.6801
2022-07-09 04:42:49 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.4742
2022-07-09 04:43:22 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.8132
2022-07-09 04:43:55 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.7497
2022-07-09 04:44:29 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.6452
2022-07-09 04:45:02 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.8300
2022-07-09 04:45:36 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.6997
2022-07-09 04:46:09 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.6385
2022-07-09 04:46:43 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.6904
2022-07-09 04:47:16 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.6197
2022-07-09 04:47:49 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.4947
2022-07-09 04:48:22 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.8039
2022-07-09 04:48:56 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.7927
2022-07-09 04:49:29 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.5554
2022-07-09 04:50:00 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.5432
2022-07-09 04:50:01 - train: epoch 014, train_loss: 2.6701
2022-07-09 04:51:13 - eval: epoch: 014, acc1: 45.184%, acc5: 71.810%, test_loss: 2.4201, per_image_load_time: 2.438ms, per_image_inference_time: 0.177ms
2022-07-09 04:51:13 - until epoch: 014, best_acc1: 45.210%
2022-07-09 04:51:13 - epoch 015 lr: 0.100000
2022-07-09 04:51:52 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.6596
2022-07-09 04:52:25 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.8473
2022-07-09 04:52:57 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.8968
2022-07-09 04:53:30 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.9663
2022-07-09 04:54:04 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.5651
2022-07-09 04:54:36 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.7717
2022-07-09 04:55:09 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.6144
2022-07-09 04:55:41 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.5963
2022-07-09 04:56:15 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.4413
2022-07-09 04:56:46 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.4109
2022-07-09 04:57:20 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.5102
2022-07-09 04:57:53 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.5773
2022-07-09 04:58:26 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.8139
2022-07-09 04:58:59 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.7787
2022-07-09 04:59:33 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.5028
2022-07-09 05:00:05 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.6271
2022-07-09 05:00:38 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.7981
2022-07-09 05:01:10 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.5203
2022-07-09 05:01:44 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.4912
2022-07-09 05:02:17 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.5683
2022-07-09 05:02:50 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.4927
2022-07-09 05:03:23 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.6453
2022-07-09 05:03:56 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.5905
2022-07-09 05:04:30 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.6197
2022-07-09 05:05:03 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.8225
2022-07-09 05:05:37 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.7233
2022-07-09 05:06:09 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.8019
2022-07-09 05:06:43 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.6265
2022-07-09 05:07:16 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.7753
2022-07-09 05:07:48 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.6758
2022-07-09 05:08:21 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.6351
2022-07-09 05:08:54 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.4245
2022-07-09 05:09:28 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.6211
2022-07-09 05:10:00 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.7145
2022-07-09 05:10:33 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.9490
2022-07-09 05:11:07 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.6850
2022-07-09 05:11:39 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.6065
2022-07-09 05:12:13 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.6127
2022-07-09 05:12:46 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5956
2022-07-09 05:13:19 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.6443
2022-07-09 05:13:53 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.6355
2022-07-09 05:14:25 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.5738
2022-07-09 05:14:59 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.6369
2022-07-09 05:15:31 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.3815
2022-07-09 05:16:05 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.5998
2022-07-09 05:16:38 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.7428
2022-07-09 05:17:12 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.5090
2022-07-09 05:17:45 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.5919
2022-07-09 05:18:18 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.5145
2022-07-09 05:18:49 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.6394
2022-07-09 05:18:50 - train: epoch 015, train_loss: 2.6582
2022-07-09 05:20:02 - eval: epoch: 015, acc1: 42.524%, acc5: 68.622%, test_loss: 2.5938, per_image_load_time: 2.627ms, per_image_inference_time: 0.171ms
2022-07-09 05:20:02 - until epoch: 015, best_acc1: 45.210%
2022-07-09 05:20:02 - epoch 016 lr: 0.100000
2022-07-09 05:20:40 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.6042
2022-07-09 05:21:14 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.4802
2022-07-09 05:21:47 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.7950
2022-07-09 05:22:19 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.7775
2022-07-09 05:22:53 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.4493
2022-07-09 05:23:27 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.7370
2022-07-09 05:23:59 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.4582
2022-07-09 05:24:34 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.5322
2022-07-09 05:25:05 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.7073
2022-07-09 05:25:39 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.7141
2022-07-09 05:26:12 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.6025
2022-07-09 05:26:45 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.4816
2022-07-09 05:27:18 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.7364
2022-07-09 05:27:51 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.5274
2022-07-09 05:28:24 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.7312
2022-07-09 05:28:57 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.5881
2022-07-09 05:29:30 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.6688
2022-07-09 05:30:04 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.8698
2022-07-09 05:30:36 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.4830
2022-07-09 05:31:09 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.3218
2022-07-09 05:31:43 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.6854
2022-07-09 05:32:16 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.7523
2022-07-09 05:32:49 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.7190
2022-07-09 05:33:23 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.7064
2022-07-09 05:33:55 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.5127
2022-07-09 05:34:29 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.8310
2022-07-09 05:35:03 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.7447
2022-07-09 05:35:37 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.3914
2022-07-09 05:36:09 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.7572
2022-07-09 05:36:43 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.9894
2022-07-09 05:37:15 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.7224
2022-07-09 05:37:49 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.5374
2022-07-09 05:38:22 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.7984
2022-07-09 05:38:56 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.3874
2022-07-09 05:39:29 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.5914
2022-07-09 05:40:02 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.5699
2022-07-09 05:40:35 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.7432
2022-07-09 05:41:09 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.7566
2022-07-09 05:41:41 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.6871
2022-07-09 05:42:15 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.7838
2022-07-09 05:42:47 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.7518
2022-07-09 05:43:21 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4293
2022-07-09 05:43:54 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.7253
2022-07-09 05:44:28 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.6539
2022-07-09 05:45:01 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.6505
2022-07-09 05:45:33 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.4418
2022-07-09 05:46:07 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.9929
2022-07-09 05:46:40 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.5983
2022-07-09 05:47:14 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5784
2022-07-09 05:47:45 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.7272
2022-07-09 05:47:46 - train: epoch 016, train_loss: 2.6461
2022-07-09 05:49:00 - eval: epoch: 016, acc1: 43.464%, acc5: 70.046%, test_loss: 2.5160, per_image_load_time: 2.542ms, per_image_inference_time: 0.157ms
2022-07-09 05:49:00 - until epoch: 016, best_acc1: 45.210%
2022-07-09 05:49:00 - epoch 017 lr: 0.100000
2022-07-09 05:49:37 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.5994
2022-07-09 05:50:10 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.8148
2022-07-09 05:50:43 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.8078
2022-07-09 05:51:16 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.3828
2022-07-09 05:51:49 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4758
2022-07-09 05:52:22 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.0815
2022-07-09 05:52:54 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.7117
2022-07-09 05:53:28 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.8445
2022-07-09 05:54:00 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.5796
2022-07-09 05:54:33 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.5027
2022-07-09 05:55:07 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.7244
2022-07-09 05:55:40 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.6640
2022-07-09 05:56:13 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.5856
2022-07-09 05:56:46 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.6251
2022-07-09 05:57:20 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.5125
2022-07-09 05:57:52 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.5683
2022-07-09 05:58:26 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.8642
2022-07-09 05:58:58 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.6929
2022-07-09 05:59:32 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.5063
2022-07-09 06:00:05 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.5801
2022-07-09 06:00:37 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.5255
2022-07-09 06:01:11 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.5040
2022-07-09 06:01:44 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.8069
2022-07-09 06:02:17 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.7744
2022-07-09 06:02:50 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.7871
2022-07-09 06:03:24 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.4833
2022-07-09 06:03:56 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.4829
2022-07-09 06:04:29 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.7162
2022-07-09 06:05:03 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.8452
2022-07-09 06:05:36 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.5369
2022-07-09 06:06:09 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.7875
2022-07-09 06:06:43 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.4684
2022-07-09 06:07:15 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.5681
2022-07-09 06:07:49 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.4937
2022-07-09 06:08:22 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.7383
2022-07-09 06:08:55 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.8530
2022-07-09 06:09:29 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.5776
2022-07-09 06:10:01 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.6771
2022-07-09 06:10:35 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.4587
2022-07-09 06:11:09 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.7368
2022-07-09 06:11:41 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.8995
2022-07-09 06:12:15 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.7040
2022-07-09 06:12:48 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.5085
2022-07-09 06:13:21 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.6406
2022-07-09 06:13:54 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.7831
2022-07-09 06:14:28 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.4944
2022-07-09 06:15:01 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.6501
2022-07-09 06:15:34 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.6946
2022-07-09 06:16:07 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.6048
2022-07-09 06:16:38 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.5926
2022-07-09 06:16:39 - train: epoch 017, train_loss: 2.6366
2022-07-09 06:17:52 - eval: epoch: 017, acc1: 46.440%, acc5: 73.096%, test_loss: 2.3463, per_image_load_time: 2.426ms, per_image_inference_time: 0.154ms
2022-07-09 06:17:52 - until epoch: 017, best_acc1: 46.440%
2022-07-09 06:17:52 - epoch 018 lr: 0.100000
2022-07-09 06:18:31 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.5130
2022-07-09 06:19:03 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.7038
2022-07-09 06:19:35 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.9065
2022-07-09 06:20:09 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.7229
2022-07-09 06:20:41 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.5377
2022-07-09 06:21:13 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.6268
2022-07-09 06:21:47 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.5180
2022-07-09 06:22:19 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.5886
2022-07-09 06:22:52 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.7117
2022-07-09 06:23:26 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.5129
2022-07-09 06:23:58 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.7832
2022-07-09 06:24:31 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.6752
2022-07-09 06:25:05 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.6843
2022-07-09 06:25:38 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.6221
2022-07-09 06:26:12 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.7614
2022-07-09 06:26:44 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.6693
2022-07-09 06:27:18 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.7844
2022-07-09 06:27:50 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.6359
2022-07-09 06:28:24 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.7108
2022-07-09 06:28:57 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.0068
2022-07-09 06:29:31 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.8034
2022-07-09 06:30:03 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.8743
2022-07-09 06:30:36 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.6609
2022-07-09 06:31:09 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.4492
2022-07-09 06:31:43 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.3760
2022-07-09 06:32:15 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.6027
2022-07-09 06:32:49 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.6919
2022-07-09 06:33:22 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.4540
2022-07-09 06:33:55 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.6879
2022-07-09 06:34:28 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.5857
2022-07-09 06:35:01 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.8691
2022-07-09 06:35:34 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.4936
2022-07-09 06:36:07 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.6384
2022-07-09 06:36:41 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.6397
2022-07-09 06:37:13 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.7318
2022-07-09 06:37:46 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.6216
2022-07-09 06:38:19 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.9829
2022-07-09 06:38:53 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.7286
2022-07-09 06:39:26 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.5975
2022-07-09 06:39:59 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.5946
2022-07-09 06:40:32 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.6340
2022-07-09 06:41:05 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.6365
2022-07-09 06:41:38 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.3929
2022-07-09 06:42:11 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.5823
2022-07-09 06:42:45 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.5947
2022-07-09 06:43:18 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.7709
2022-07-09 06:43:51 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.6781
2022-07-09 06:44:24 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.6882
2022-07-09 06:44:57 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.5567
2022-07-09 06:45:28 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.5311
2022-07-09 06:45:29 - train: epoch 018, train_loss: 2.6267
2022-07-09 06:46:42 - eval: epoch: 018, acc1: 45.772%, acc5: 72.446%, test_loss: 2.3671, per_image_load_time: 2.107ms, per_image_inference_time: 0.145ms
2022-07-09 06:46:42 - until epoch: 018, best_acc1: 46.440%
2022-07-09 06:46:42 - epoch 019 lr: 0.100000
2022-07-09 06:47:21 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.4173
2022-07-09 06:47:53 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.8414
2022-07-09 06:48:26 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.7749
2022-07-09 06:48:58 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.4711
2022-07-09 06:49:32 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3369
2022-07-09 06:50:05 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.7902
2022-07-09 06:50:38 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.4616
2022-07-09 06:51:11 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.8195
2022-07-09 06:51:44 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.6033
2022-07-09 06:52:16 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.5394
2022-07-09 06:52:49 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.3938
2022-07-09 06:53:23 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.4829
2022-07-09 06:53:56 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.4951
2022-07-09 06:54:29 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.4675
2022-07-09 06:55:02 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.9481
2022-07-09 06:55:36 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.3714
2022-07-09 06:56:09 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.6617
2022-07-09 06:56:42 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.5210
2022-07-09 06:57:14 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.9677
2022-07-09 06:57:48 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.4593
2022-07-09 06:58:21 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.6385
2022-07-09 06:58:54 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.6835
2022-07-09 06:59:27 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.7131
2022-07-09 07:00:01 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.7720
2022-07-09 07:00:34 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.6780
2022-07-09 07:01:07 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.7896
2022-07-09 07:01:40 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.6303
2022-07-09 07:02:14 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.5770
2022-07-09 07:02:47 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.8103
2022-07-09 07:03:20 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.8216
2022-07-09 07:03:53 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.5548
2022-07-09 07:04:26 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.5697
2022-07-09 07:05:00 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.4571
2022-07-09 07:05:33 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.5095
2022-07-09 07:06:06 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.6506
2022-07-09 07:06:40 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.3068
2022-07-09 07:07:14 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.6940
2022-07-09 07:07:46 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 3.0045
2022-07-09 07:08:20 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.5460
2022-07-09 07:08:53 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.4862
2022-07-09 07:09:25 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.7267
2022-07-09 07:10:00 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.4591
2022-07-09 07:10:33 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.5238
2022-07-09 07:11:06 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.7810
2022-07-09 07:11:38 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.9129
2022-07-09 07:12:12 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.5020
2022-07-09 07:12:45 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.5809
2022-07-09 07:13:19 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.5111
2022-07-09 07:13:52 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.5360
2022-07-09 07:14:24 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.5493
2022-07-09 07:14:24 - train: epoch 019, train_loss: 2.6188
2022-07-09 07:15:37 - eval: epoch: 019, acc1: 46.450%, acc5: 72.638%, test_loss: 2.3512, per_image_load_time: 1.224ms, per_image_inference_time: 0.182ms
2022-07-09 07:15:37 - until epoch: 019, best_acc1: 46.450%
2022-07-09 07:15:37 - epoch 020 lr: 0.100000
2022-07-09 07:16:16 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.7706
2022-07-09 07:16:49 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.4625
2022-07-09 07:17:21 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.5606
2022-07-09 07:17:54 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.4341
2022-07-09 07:18:27 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3892
2022-07-09 07:18:59 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.8212
2022-07-09 07:19:32 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.2036
2022-07-09 07:20:05 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.5688
2022-07-09 07:20:38 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.5546
2022-07-09 07:21:11 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.6191
2022-07-09 07:21:44 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.5207
2022-07-09 07:22:17 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.7300
2022-07-09 07:22:50 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.5893
2022-07-09 07:23:23 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.8387
2022-07-09 07:23:56 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.6447
2022-07-09 07:24:29 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.7332
2022-07-09 07:25:02 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.3904
2022-07-09 07:25:35 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.5332
2022-07-09 07:26:08 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.4817
2022-07-09 07:26:41 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.7277
2022-07-09 07:27:14 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.7899
2022-07-09 07:27:47 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.4673
2022-07-09 07:28:20 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.6320
2022-07-09 07:28:53 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.8165
2022-07-09 07:29:26 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.5381
2022-07-09 07:29:58 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.5703
2022-07-09 07:30:32 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.7621
2022-07-09 07:31:05 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.7928
2022-07-09 07:31:38 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5623
2022-07-09 07:32:11 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.7083
2022-07-09 07:32:43 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.8084
2022-07-09 07:33:17 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.9764
2022-07-09 07:33:50 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.3341
2022-07-09 07:34:22 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.6445
2022-07-09 07:34:56 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.4605
2022-07-09 07:35:29 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.6410
2022-07-09 07:36:02 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.6522
2022-07-09 07:36:35 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.5681
2022-07-09 07:37:08 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.7149
2022-07-09 07:37:41 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.5176
2022-07-09 07:38:14 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.5226
2022-07-09 07:38:47 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.5645
2022-07-09 07:39:20 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.7142
2022-07-09 07:39:53 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.4919
2022-07-09 07:40:27 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.5827
2022-07-09 07:40:59 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.6920
2022-07-09 07:41:33 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.4475
2022-07-09 07:42:06 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.4599
2022-07-09 07:42:40 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.5638
2022-07-09 07:43:11 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.4529
2022-07-09 07:43:12 - train: epoch 020, train_loss: 2.6069
2022-07-09 07:44:25 - eval: epoch: 020, acc1: 46.930%, acc5: 73.142%, test_loss: 2.3367, per_image_load_time: 2.631ms, per_image_inference_time: 0.185ms
2022-07-09 07:44:26 - until epoch: 020, best_acc1: 46.930%
2022-07-09 07:44:26 - epoch 021 lr: 0.100000
2022-07-09 07:45:02 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.5437
2022-07-09 07:45:36 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.7334
2022-07-09 07:46:07 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.3143
2022-07-09 07:46:41 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.4229
2022-07-09 07:47:13 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.3556
2022-07-09 07:47:47 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.4599
2022-07-09 07:48:18 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.3806
2022-07-09 07:48:51 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.6704
2022-07-09 07:49:24 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.6989
2022-07-09 07:49:56 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.4233
2022-07-09 07:50:30 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.6791
2022-07-09 07:51:02 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.6292
2022-07-09 07:51:36 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.3057
2022-07-09 07:52:09 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.4642
2022-07-09 07:52:42 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.4534
2022-07-09 07:53:15 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.2414
2022-07-09 07:53:48 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.8199
2022-07-09 07:54:21 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.3502
2022-07-09 07:54:54 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.8285
2022-07-09 07:55:27 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.7054
2022-07-09 07:55:59 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.3945
2022-07-09 07:56:34 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.7330
2022-07-09 07:57:06 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.4480
2022-07-09 07:57:39 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2730
2022-07-09 07:58:12 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.5771
2022-07-09 07:58:46 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.6949
2022-07-09 07:59:19 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.5361
2022-07-09 07:59:53 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.7594
2022-07-09 08:00:25 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.4799
2022-07-09 08:00:59 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.7065
2022-07-09 08:01:32 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.5996
2022-07-09 08:02:06 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.5300
2022-07-09 08:02:38 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.8831
2022-07-09 08:03:11 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.9061
2022-07-09 08:03:44 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.7807
2022-07-09 08:04:18 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.6813
2022-07-09 08:04:51 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.7208
2022-07-09 08:05:25 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.7267
2022-07-09 08:05:58 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.6336
2022-07-09 08:06:32 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.7984
2022-07-09 08:07:05 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.5591
2022-07-09 08:07:38 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.4605
2022-07-09 08:08:11 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.7728
2022-07-09 08:08:45 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4628
2022-07-09 08:09:17 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.5762
2022-07-09 08:09:51 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.5389
2022-07-09 08:10:23 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.6128
2022-07-09 08:10:56 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.6007
2022-07-09 08:11:30 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.3411
2022-07-09 08:12:01 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.5329
2022-07-09 08:12:02 - train: epoch 021, train_loss: 2.6031
2022-07-09 08:13:15 - eval: epoch: 021, acc1: 45.238%, acc5: 71.676%, test_loss: 2.4117, per_image_load_time: 2.426ms, per_image_inference_time: 0.174ms
2022-07-09 08:13:15 - until epoch: 021, best_acc1: 46.930%
2022-07-09 08:13:15 - epoch 022 lr: 0.100000
2022-07-09 08:13:53 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1258
2022-07-09 08:14:26 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.4570
2022-07-09 08:14:59 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.4706
2022-07-09 08:15:31 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.4559
2022-07-09 08:16:04 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.5930
2022-07-09 08:16:37 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.4726
2022-07-09 08:17:10 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.8304
2022-07-09 08:17:43 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.7507
2022-07-09 08:18:16 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.6321
2022-07-09 08:18:48 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.5961
2022-07-09 08:19:21 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.7349
2022-07-09 08:19:55 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.3071
2022-07-09 08:20:27 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.5534
2022-07-09 08:21:01 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.5205
2022-07-09 08:21:33 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.6960
2022-07-09 08:22:06 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.5818
2022-07-09 08:22:39 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.6588
2022-07-09 08:23:12 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.7501
2022-07-09 08:23:45 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.4606
2022-07-09 08:24:18 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4233
2022-07-09 08:24:50 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.5099
2022-07-09 08:25:24 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.3091
2022-07-09 08:25:56 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.7277
2022-07-09 08:26:30 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.7522
2022-07-09 08:27:02 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.4509
2022-07-09 08:27:36 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.5083
2022-07-09 08:28:09 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.3592
2022-07-09 08:28:41 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.7537
2022-07-09 08:29:14 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.5078
2022-07-09 08:29:47 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.8501
2022-07-09 08:30:20 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.7551
2022-07-09 08:30:54 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.6870
2022-07-09 08:31:28 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.4421
2022-07-09 08:32:00 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.5491
2022-07-09 08:32:34 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.7047
2022-07-09 08:33:07 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.7807
2022-07-09 08:33:41 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.6688
2022-07-09 08:34:12 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.7608
2022-07-09 08:34:46 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.5138
2022-07-09 08:35:20 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.6065
2022-07-09 08:35:53 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.5085
2022-07-09 08:36:26 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.6937
2022-07-09 08:36:59 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.5269
2022-07-09 08:37:33 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.4932
2022-07-09 08:38:05 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.5256
2022-07-09 08:38:38 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.7393
2022-07-09 08:39:11 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.6411
2022-07-09 08:39:45 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.5412
2022-07-09 08:40:16 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.5050
2022-07-09 08:40:48 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.4762
2022-07-09 08:40:49 - train: epoch 022, train_loss: 2.5973
2022-07-09 08:42:02 - eval: epoch: 022, acc1: 42.398%, acc5: 68.838%, test_loss: 2.5879, per_image_load_time: 2.579ms, per_image_inference_time: 0.158ms
2022-07-09 08:42:02 - until epoch: 022, best_acc1: 46.930%
2022-07-09 08:42:02 - epoch 023 lr: 0.100000
2022-07-09 08:42:40 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.4735
2022-07-09 08:43:13 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.5823
2022-07-09 08:43:45 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.6009
2022-07-09 08:44:18 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.4466
2022-07-09 08:44:51 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.6532
2022-07-09 08:45:24 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.5733
2022-07-09 08:45:57 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.4129
2022-07-09 08:46:30 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.4575
2022-07-09 08:47:03 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.6199
2022-07-09 08:47:35 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.6562
2022-07-09 08:48:09 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.5789
2022-07-09 08:48:42 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.4621
2022-07-09 08:49:14 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.7340
2022-07-09 08:49:48 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.5419
2022-07-09 08:50:20 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.3269
2022-07-09 08:50:54 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.4249
2022-07-09 08:51:27 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.5885
2022-07-09 08:52:01 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.6084
2022-07-09 08:52:33 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.5941
2022-07-09 08:53:07 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.3296
2022-07-09 08:53:39 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.5884
2022-07-09 08:54:13 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.2122
2022-07-09 08:54:45 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.4831
2022-07-09 08:55:19 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.5429
2022-07-09 08:55:52 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.6841
2022-07-09 08:56:25 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.7815
2022-07-09 08:56:58 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.6388
2022-07-09 08:57:32 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.5260
2022-07-09 08:58:05 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.6649
2022-07-09 08:58:38 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.7706
2022-07-09 08:59:11 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.6144
2022-07-09 08:59:44 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.8978
2022-07-09 09:00:17 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.7792
2022-07-09 09:00:51 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.6862
2022-07-09 09:01:24 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.3803
2022-07-09 09:01:57 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.6814
2022-07-09 09:02:30 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.5113
2022-07-09 09:03:03 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.4895
2022-07-09 09:03:36 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.5757
2022-07-09 09:04:09 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.3426
2022-07-09 09:04:43 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.4591
2022-07-09 09:05:16 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.4290
2022-07-09 09:05:49 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.4077
2022-07-09 09:06:23 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.5515
2022-07-09 09:06:55 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.4651
2022-07-09 09:07:29 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.6809
2022-07-09 09:08:02 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.3392
2022-07-09 09:08:35 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.5684
2022-07-09 09:09:08 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.3886
2022-07-09 09:09:40 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.7099
2022-07-09 09:09:41 - train: epoch 023, train_loss: 2.5902
2022-07-09 09:10:54 - eval: epoch: 023, acc1: 46.878%, acc5: 73.312%, test_loss: 2.3252, per_image_load_time: 2.728ms, per_image_inference_time: 0.139ms
2022-07-09 09:10:54 - until epoch: 023, best_acc1: 46.930%
2022-07-09 09:10:54 - epoch 024 lr: 0.100000
2022-07-09 09:11:33 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.5576
2022-07-09 09:12:07 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.4476
2022-07-09 09:12:39 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.5335
2022-07-09 09:13:12 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.7020
2022-07-09 09:13:44 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.7675
2022-07-09 09:14:18 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.5032
2022-07-09 09:14:51 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.6352
2022-07-09 09:15:24 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.7946
2022-07-09 09:15:57 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.5375
2022-07-09 09:16:30 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.5760
2022-07-09 09:17:02 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.4614
2022-07-09 09:17:35 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.5428
2022-07-09 09:18:09 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.8788
2022-07-09 09:18:41 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.3319
2022-07-09 09:19:15 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.8493
2022-07-09 09:19:48 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.6686
2022-07-09 09:20:20 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.4911
2022-07-09 09:20:54 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.4945
2022-07-09 09:21:27 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.1520
2022-07-09 09:22:00 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.8227
2022-07-09 09:22:33 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.5096
2022-07-09 09:23:07 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.4387
2022-07-09 09:23:39 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.5799
2022-07-09 09:24:13 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.6222
2022-07-09 09:24:46 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.7001
2022-07-09 09:25:19 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.8141
2022-07-09 09:25:53 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.5668
2022-07-09 09:26:25 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.6817
2022-07-09 09:26:59 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.5684
2022-07-09 09:27:32 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.4384
2022-07-09 09:28:05 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.4404
2022-07-09 09:28:39 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.8371
2022-07-09 09:29:13 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.2363
2022-07-09 09:29:44 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.5546
2022-07-09 09:30:18 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.4260
2022-07-09 09:30:51 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.5258
2022-07-09 09:31:26 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.4126
2022-07-09 09:31:59 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.7628
2022-07-09 09:32:31 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.3893
2022-07-09 09:33:05 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.4291
2022-07-09 09:33:39 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.4522
2022-07-09 09:34:11 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.3648
2022-07-09 09:34:45 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3332
2022-07-09 09:35:18 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.5320
2022-07-09 09:35:52 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.2992
2022-07-09 09:36:25 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4673
2022-07-09 09:36:59 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.6016
2022-07-09 09:37:31 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.5317
2022-07-09 09:38:06 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.9182
2022-07-09 09:38:37 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.5271
2022-07-09 09:38:37 - train: epoch 024, train_loss: 2.5864
2022-07-09 09:39:51 - eval: epoch: 024, acc1: 48.012%, acc5: 74.334%, test_loss: 2.2643, per_image_load_time: 2.322ms, per_image_inference_time: 0.162ms
2022-07-09 09:39:51 - until epoch: 024, best_acc1: 48.012%
2022-07-09 09:39:51 - epoch 025 lr: 0.100000
2022-07-09 09:40:29 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.4847
2022-07-09 09:41:02 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.3167
2022-07-09 09:41:33 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.6289
2022-07-09 09:42:07 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.9404
2022-07-09 09:42:40 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.4448
2022-07-09 09:43:14 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.6177
2022-07-09 09:43:45 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.6300
2022-07-09 09:44:20 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.6788
2022-07-09 09:44:53 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.2465
2022-07-09 09:45:26 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.5355
2022-07-09 09:45:59 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.6198
2022-07-09 09:46:33 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.6305
2022-07-09 09:47:06 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.4241
2022-07-09 09:47:40 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.5739
2022-07-09 09:48:13 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.4415
2022-07-09 09:48:46 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.3761
2022-07-09 09:49:19 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.4963
2022-07-09 09:49:52 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.4447
2022-07-09 09:50:26 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.2612
2022-07-09 09:50:58 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.5437
2022-07-09 09:51:32 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.2160
2022-07-09 09:52:06 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.5853
2022-07-09 09:52:39 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.6540
2022-07-09 09:53:12 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.3445
2022-07-09 09:53:45 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.6176
2022-07-09 09:54:18 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.6742
2022-07-09 09:54:53 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.7433
2022-07-09 09:55:25 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.6532
2022-07-09 09:56:00 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.5459
2022-07-09 09:56:32 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.6150
2022-07-09 09:57:06 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.7419
2022-07-09 09:57:39 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.7062
2022-07-09 09:58:13 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.5356
2022-07-09 09:58:45 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.6292
2022-07-09 09:59:19 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.4353
2022-07-09 09:59:51 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.4994
2022-07-09 10:00:25 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.5091
2022-07-09 10:00:57 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.6350
2022-07-09 10:01:30 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.8688
2022-07-09 10:02:04 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4860
2022-07-09 10:02:36 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.8943
2022-07-09 10:03:11 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.6039
2022-07-09 10:03:43 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.3716
2022-07-09 10:04:16 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.2129
2022-07-09 10:04:50 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.7331
2022-07-09 10:05:23 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.4051
2022-07-09 10:05:57 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.6876
2022-07-09 10:06:29 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.3002
2022-07-09 10:07:03 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.4372
2022-07-09 10:07:34 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.7332
2022-07-09 10:07:34 - train: epoch 025, train_loss: 2.5813
2022-07-09 10:08:48 - eval: epoch: 025, acc1: 45.826%, acc5: 72.492%, test_loss: 2.3812, per_image_load_time: 0.991ms, per_image_inference_time: 0.140ms
2022-07-09 10:08:48 - until epoch: 025, best_acc1: 48.012%
2022-07-09 10:08:48 - epoch 026 lr: 0.100000
2022-07-09 10:09:26 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.4502
2022-07-09 10:09:59 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.3395
2022-07-09 10:10:31 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.7582
2022-07-09 10:11:04 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.5818
2022-07-09 10:11:37 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.5789
2022-07-09 10:12:10 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.9416
2022-07-09 10:12:42 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.5931
2022-07-09 10:13:15 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.3467
2022-07-09 10:13:49 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.5797
2022-07-09 10:14:21 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.7332
2022-07-09 10:14:55 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.7319
2022-07-09 10:15:29 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.5375
2022-07-09 10:16:01 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.4515
2022-07-09 10:16:34 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.6343
2022-07-09 10:17:08 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.5119
2022-07-09 10:17:41 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.8721
2022-07-09 10:18:13 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.6501
2022-07-09 10:18:47 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.7067
2022-07-09 10:19:20 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6589
2022-07-09 10:19:54 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.5117
2022-07-09 10:20:26 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.5667
2022-07-09 10:20:59 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.6350
2022-07-09 10:21:32 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.5438
2022-07-09 10:22:07 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.6355
2022-07-09 10:22:39 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.7261
2022-07-09 10:23:13 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.6563
2022-07-09 10:23:45 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.5007
2022-07-09 10:24:18 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.5069
2022-07-09 10:24:51 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.5957
2022-07-09 10:25:24 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.5999
2022-07-09 10:25:58 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.3872
2022-07-09 10:26:32 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.5596
2022-07-09 10:27:03 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.5237
2022-07-09 10:27:38 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.8731
2022-07-09 10:28:11 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.7041
2022-07-09 10:28:43 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.4956
2022-07-09 10:29:17 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4632
2022-07-09 10:29:50 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.7351
2022-07-09 10:30:23 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.7134
2022-07-09 10:30:56 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.6910
2022-07-09 10:31:29 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.5843
2022-07-09 10:32:03 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.7392
2022-07-09 10:32:35 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.6666
2022-07-09 10:33:09 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.7578
2022-07-09 10:33:42 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.7708
2022-07-09 10:34:15 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.6675
2022-07-09 10:34:48 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.5041
2022-07-09 10:35:22 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.5173
2022-07-09 10:35:56 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.8304
2022-07-09 10:36:27 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.6166
2022-07-09 10:36:28 - train: epoch 026, train_loss: 2.5740
2022-07-09 10:37:41 - eval: epoch: 026, acc1: 45.398%, acc5: 71.948%, test_loss: 2.4017, per_image_load_time: 2.635ms, per_image_inference_time: 0.158ms
2022-07-09 10:37:41 - until epoch: 026, best_acc1: 48.012%
2022-07-09 10:37:41 - epoch 027 lr: 0.100000
2022-07-09 10:38:19 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.7435
2022-07-09 10:38:52 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.5024
2022-07-09 10:39:25 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.6914
2022-07-09 10:39:58 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.6944
2022-07-09 10:40:30 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.4961
2022-07-09 10:41:03 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6246
2022-07-09 10:41:37 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.6794
2022-07-09 10:42:09 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.5549
2022-07-09 10:42:43 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.5392
2022-07-09 10:43:15 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.6303
2022-07-09 10:43:49 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.4476
2022-07-09 10:44:21 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.6890
2022-07-09 10:44:55 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.5645
2022-07-09 10:45:29 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.6585
2022-07-09 10:46:02 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.7800
2022-07-09 10:46:34 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.5936
2022-07-09 10:47:08 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.4376
2022-07-09 10:47:42 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.3607
2022-07-09 10:48:15 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.7482
2022-07-09 10:48:48 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.5397
2022-07-09 10:49:21 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.7730
2022-07-09 10:49:54 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.5881
2022-07-09 10:50:26 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.8137
2022-07-09 10:51:01 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.5443
2022-07-09 10:51:34 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.5683
2022-07-09 10:52:07 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.6264
2022-07-09 10:52:40 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.5374
2022-07-09 10:53:13 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.5819
2022-07-09 10:53:46 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.8878
2022-07-09 10:54:20 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.7272
2022-07-09 10:54:53 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.2755
2022-07-09 10:55:26 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.5573
2022-07-09 10:55:59 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.5680
2022-07-09 10:56:32 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.5776
2022-07-09 10:57:05 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.7079
2022-07-09 10:57:39 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.6344
2022-07-09 10:58:13 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.6549
2022-07-09 10:58:45 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.5203
2022-07-09 10:59:19 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.5990
2022-07-09 10:59:52 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.5094
2022-07-09 11:00:26 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.6181
2022-07-09 11:00:59 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.4414
2022-07-09 11:01:32 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.5759
2022-07-09 11:02:06 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.5804
2022-07-09 11:02:37 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.3249
2022-07-09 11:03:12 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.5390
2022-07-09 11:03:44 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.6141
2022-07-09 11:04:17 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.7403
2022-07-09 11:04:51 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.3971
2022-07-09 11:05:22 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.2235
2022-07-09 11:05:23 - train: epoch 027, train_loss: 2.5722
2022-07-09 11:06:35 - eval: epoch: 027, acc1: 48.406%, acc5: 74.622%, test_loss: 2.2465, per_image_load_time: 2.561ms, per_image_inference_time: 0.176ms
2022-07-09 11:06:35 - until epoch: 027, best_acc1: 48.406%
2022-07-09 11:06:35 - epoch 028 lr: 0.100000
2022-07-09 11:07:13 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.4009
2022-07-09 11:07:45 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.4189
2022-07-09 11:08:19 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.6081
2022-07-09 11:08:51 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.5124
2022-07-09 11:09:23 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.5404
2022-07-09 11:09:58 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.6911
2022-07-09 11:10:30 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.7180
2022-07-09 11:11:03 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.4054
2022-07-09 11:11:36 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.2852
2022-07-09 11:12:10 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.6023
2022-07-09 11:12:42 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.5128
2022-07-09 11:13:16 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.5312
2022-07-09 11:13:48 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.6218
2022-07-09 11:14:23 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.8577
2022-07-09 11:14:55 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.5031
2022-07-09 11:15:29 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.6588
2022-07-09 11:16:01 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.6870
2022-07-09 11:16:34 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.7513
2022-07-09 11:17:07 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.6451
2022-07-09 11:17:40 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.5998
2022-07-09 11:18:14 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.7949
2022-07-09 11:18:47 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.4528
2022-07-09 11:19:20 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.8213
2022-07-09 11:19:53 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.7555
2022-07-09 11:20:26 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.7110
2022-07-09 11:20:59 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.4481
2022-07-09 11:21:33 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.5443
2022-07-09 11:22:05 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.4042
2022-07-09 11:22:39 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.3896
2022-07-09 11:23:12 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.5330
2022-07-09 11:23:45 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.7447
2022-07-09 11:24:18 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.5261
2022-07-09 11:24:51 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.5033
2022-07-09 11:25:25 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.6931
2022-07-09 11:25:58 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.4005
2022-07-09 11:26:32 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.5166
2022-07-09 11:27:05 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.8401
2022-07-09 11:27:38 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.3919
2022-07-09 11:28:10 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.8543
2022-07-09 11:28:44 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.6579
2022-07-09 11:29:16 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.5925
2022-07-09 11:29:51 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.6198
2022-07-09 11:30:23 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.3153
2022-07-09 11:30:57 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.5887
2022-07-09 11:31:30 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.5567
2022-07-09 11:32:04 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.4730
2022-07-09 11:32:37 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.6107
2022-07-09 11:33:10 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.5731
2022-07-09 11:33:43 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.6086
2022-07-09 11:34:15 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.2725
2022-07-09 11:34:16 - train: epoch 028, train_loss: 2.5671
2022-07-09 11:35:30 - eval: epoch: 028, acc1: 47.004%, acc5: 73.310%, test_loss: 2.3194, per_image_load_time: 2.668ms, per_image_inference_time: 0.171ms
2022-07-09 11:35:30 - until epoch: 028, best_acc1: 48.406%
2022-07-09 11:35:30 - epoch 029 lr: 0.100000
2022-07-09 11:36:07 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.5673
2022-07-09 11:36:41 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.5160
2022-07-09 11:37:14 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.5747
2022-07-09 11:37:45 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.4729
2022-07-09 11:38:18 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.5017
2022-07-09 11:38:51 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.6179
2022-07-09 11:39:24 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.2070
2022-07-09 11:39:57 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.5666
2022-07-09 11:40:30 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.5803
2022-07-09 11:41:03 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.6053
2022-07-09 11:41:36 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.2505
2022-07-09 11:42:09 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.5331
2022-07-09 11:42:42 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.5393
2022-07-09 11:43:16 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.8259
2022-07-09 11:43:49 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.5377
2022-07-09 11:44:23 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.6870
2022-07-09 11:44:55 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.4945
2022-07-09 11:45:28 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.7649
2022-07-09 11:46:02 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.4045
2022-07-09 11:46:34 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.5068
2022-07-09 11:47:09 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.4924
2022-07-09 11:47:42 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.5464
2022-07-09 11:48:15 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.4122
2022-07-09 11:48:49 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.4182
2022-07-09 11:49:22 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.6129
2022-07-09 11:49:55 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.6404
2022-07-09 11:50:27 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.6296
2022-07-09 11:51:00 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.5623
2022-07-09 11:51:33 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.4178
2022-07-09 11:52:07 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.3816
2022-07-09 11:52:40 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.5012
2022-07-09 11:53:13 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.7236
2022-07-09 11:53:47 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.4046
2022-07-09 11:54:20 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.4777
2022-07-09 11:54:53 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.7580
2022-07-09 11:55:27 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.3279
2022-07-09 11:56:00 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.4963
2022-07-09 11:56:33 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.6902
2022-07-09 11:57:06 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.1831
2022-07-09 11:57:40 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.7428
2022-07-09 11:58:14 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.4924
2022-07-09 11:58:47 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.5286
2022-07-09 11:59:20 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.5272
2022-07-09 11:59:53 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.6013
2022-07-09 12:00:27 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.7627
2022-07-09 12:01:00 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.5137
2022-07-09 12:01:33 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.4346
2022-07-09 12:02:05 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.7345
2022-07-09 12:02:40 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.6630
2022-07-09 12:03:11 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.2573
2022-07-09 12:03:12 - train: epoch 029, train_loss: 2.5664
2022-07-09 12:04:25 - eval: epoch: 029, acc1: 47.704%, acc5: 73.698%, test_loss: 2.2850, per_image_load_time: 2.604ms, per_image_inference_time: 0.154ms
2022-07-09 12:04:25 - until epoch: 029, best_acc1: 48.406%
2022-07-09 12:04:25 - epoch 030 lr: 0.100000
2022-07-09 12:05:04 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.5379
2022-07-09 12:05:35 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.7304
2022-07-09 12:06:09 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.4195
2022-07-09 12:06:41 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.2650
2022-07-09 12:07:15 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.5933
2022-07-09 12:07:46 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.5820
2022-07-09 12:08:19 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.3916
2022-07-09 12:08:51 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.6946
2022-07-09 12:09:24 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.5810
2022-07-09 12:09:57 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.4197
2022-07-09 12:10:29 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.5819
2022-07-09 12:11:02 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.4276
2022-07-09 12:11:35 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.3093
2022-07-09 12:12:09 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.4694
2022-07-09 12:12:41 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.6719
2022-07-09 12:13:15 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.5132
2022-07-09 12:13:48 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.6213
2022-07-09 12:14:21 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2858
2022-07-09 12:14:55 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.5863
2022-07-09 12:15:27 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.3846
2022-07-09 12:16:01 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.8255
2022-07-09 12:16:33 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.7188
2022-07-09 12:17:07 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.5632
2022-07-09 12:17:39 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.6954
2022-07-09 12:18:12 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.6803
2022-07-09 12:18:46 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.5063
2022-07-09 12:19:19 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.3890
2022-07-09 12:19:53 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.4761
2022-07-09 12:20:26 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.5331
2022-07-09 12:20:59 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.4500
2022-07-09 12:21:32 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.5733
2022-07-09 12:22:06 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.2252
2022-07-09 12:22:39 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.6359
2022-07-09 12:23:13 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.5416
2022-07-09 12:23:46 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.4830
2022-07-09 12:24:20 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.4991
2022-07-09 12:24:52 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.4996
2022-07-09 12:25:26 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.5143
2022-07-09 12:25:59 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.4643
2022-07-09 12:26:33 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.3673
2022-07-09 12:27:06 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.3684
2022-07-09 12:27:39 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 3.0530
2022-07-09 12:28:11 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.5457
2022-07-09 12:28:45 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.6048
2022-07-09 12:29:18 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.7418
2022-07-09 12:29:51 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.4097
2022-07-09 12:30:25 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.4290
2022-07-09 12:30:58 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.6873
2022-07-09 12:31:31 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.7355
2022-07-09 12:32:02 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.7649
2022-07-09 12:32:03 - train: epoch 030, train_loss: 2.5634
2022-07-09 12:33:16 - eval: epoch: 030, acc1: 46.382%, acc5: 72.956%, test_loss: 2.3482, per_image_load_time: 2.510ms, per_image_inference_time: 0.159ms
2022-07-09 12:33:17 - until epoch: 030, best_acc1: 48.406%
2022-07-09 12:33:17 - epoch 031 lr: 0.010000
2022-07-09 12:33:55 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.3053
2022-07-09 12:34:28 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.2306
2022-07-09 12:35:00 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.2555
2022-07-09 12:35:32 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.1460
2022-07-09 12:36:06 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.3355
2022-07-09 12:36:39 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.0678
2022-07-09 12:37:11 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.1713
2022-07-09 12:37:44 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.1206
2022-07-09 12:38:17 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.9974
2022-07-09 12:38:50 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.6131
2022-07-09 12:39:23 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.2778
2022-07-09 12:39:56 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.0710
2022-07-09 12:40:29 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.7547
2022-07-09 12:41:02 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.9746
2022-07-09 12:41:35 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.2322
2022-07-09 12:42:09 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.0879
2022-07-09 12:42:42 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.9057
2022-07-09 12:43:15 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.8948
2022-07-09 12:43:48 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.1267
2022-07-09 12:44:21 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.0327
2022-07-09 12:44:55 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.9816
2022-07-09 12:45:28 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.0172
2022-07-09 12:46:01 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.9779
2022-07-09 12:46:34 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.1977
2022-07-09 12:47:07 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.9555
2022-07-09 12:47:41 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.1895
2022-07-09 12:48:14 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.1397
2022-07-09 12:48:48 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.3994
2022-07-09 12:49:20 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.1986
2022-07-09 12:49:54 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.3858
2022-07-09 12:50:26 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.0007
2022-07-09 12:51:00 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.1457
2022-07-09 12:51:33 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.0125
2022-07-09 12:52:06 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.1143
2022-07-09 12:52:41 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.1962
2022-07-09 12:53:12 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.0354
2022-07-09 12:53:45 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.8463
2022-07-09 12:54:19 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.9929
2022-07-09 12:54:53 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.1683
2022-07-09 12:55:26 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.7734
2022-07-09 12:55:59 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.9464
2022-07-09 12:56:33 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.2886
2022-07-09 12:57:06 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.0697
2022-07-09 12:57:39 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.1232
2022-07-09 12:58:12 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.0203
2022-07-09 12:58:45 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.0849
2022-07-09 12:59:18 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.7970
2022-07-09 12:59:52 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.0514
2022-07-09 13:00:24 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.0474
2022-07-09 13:00:56 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.8516
2022-07-09 13:00:57 - train: epoch 031, train_loss: 2.0940
2022-07-09 13:02:11 - eval: epoch: 031, acc1: 59.756%, acc5: 82.734%, test_loss: 1.6925, per_image_load_time: 2.542ms, per_image_inference_time: 0.165ms
2022-07-09 13:02:11 - until epoch: 031, best_acc1: 59.756%
2022-07-09 13:02:11 - epoch 032 lr: 0.010000
2022-07-09 13:02:49 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.9053
2022-07-09 13:03:22 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.9419
2022-07-09 13:03:54 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.8795
2022-07-09 13:04:27 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.9705
2022-07-09 13:05:00 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.8647
2022-07-09 13:05:33 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.0180
2022-07-09 13:06:06 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.9375
2022-07-09 13:06:38 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.0740
2022-07-09 13:07:11 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.0180
2022-07-09 13:07:45 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.1784
2022-07-09 13:08:18 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.9600
2022-07-09 13:08:50 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.0226
2022-07-09 13:09:25 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.8410
2022-07-09 13:09:56 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.1832
2022-07-09 13:10:29 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.0012
2022-07-09 13:11:03 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.0017
2022-07-09 13:11:35 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.0599
2022-07-09 13:12:08 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.2713
2022-07-09 13:12:40 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.7821
2022-07-09 13:13:14 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.8992
2022-07-09 13:13:46 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.7590
2022-07-09 13:14:20 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.9043
2022-07-09 13:14:53 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.0334
2022-07-09 13:15:27 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.1471
2022-07-09 13:15:59 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7697
2022-07-09 13:16:32 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.8373
2022-07-09 13:17:05 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.0585
2022-07-09 13:17:38 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.9569
2022-07-09 13:18:12 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.7458
2022-07-09 13:18:45 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 2.0256
2022-07-09 13:19:17 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.0131
2022-07-09 13:19:50 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.9463
2022-07-09 13:20:23 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.9162
2022-07-09 13:20:56 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.6896
2022-07-09 13:21:30 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.9546
2022-07-09 13:22:02 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.1789
2022-07-09 13:22:37 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.0143
2022-07-09 13:23:09 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.8362
2022-07-09 13:23:43 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.6450
2022-07-09 13:24:15 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.9182
2022-07-09 13:24:49 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.0446
2022-07-09 13:25:21 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.7561
2022-07-09 13:25:55 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.1286
2022-07-09 13:26:28 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 2.0377
2022-07-09 13:27:01 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.1785
2022-07-09 13:27:34 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.9092
2022-07-09 13:28:07 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.0092
2022-07-09 13:28:41 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.9111
2022-07-09 13:29:13 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.8606
2022-07-09 13:29:45 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.0490
2022-07-09 13:29:46 - train: epoch 032, train_loss: 1.9793
2022-07-09 13:31:00 - eval: epoch: 032, acc1: 60.482%, acc5: 83.484%, test_loss: 1.6500, per_image_load_time: 2.611ms, per_image_inference_time: 0.171ms
2022-07-09 13:31:00 - until epoch: 032, best_acc1: 60.482%
2022-07-09 13:31:00 - epoch 033 lr: 0.010000
2022-07-09 13:31:38 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.7378
2022-07-09 13:32:10 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.1891
2022-07-09 13:32:43 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.8314
2022-07-09 13:33:15 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.8279
2022-07-09 13:33:48 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.2034
2022-07-09 13:34:22 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.8196
2022-07-09 13:34:54 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.3749
2022-07-09 13:35:28 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.3006
2022-07-09 13:36:01 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.0167
2022-07-09 13:36:35 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.2144
2022-07-09 13:37:06 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.9085
2022-07-09 13:37:40 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.2169
2022-07-09 13:38:13 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.9663
2022-07-09 13:38:47 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.8808
2022-07-09 13:39:20 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.1175
2022-07-09 13:39:53 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.1006
2022-07-09 13:40:26 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.7288
2022-07-09 13:40:59 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.0877
2022-07-09 13:41:33 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.9642
2022-07-09 13:42:06 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.8574
2022-07-09 13:42:38 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.9651
2022-07-09 13:43:12 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.0907
2022-07-09 13:43:45 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.9748
2022-07-09 13:44:18 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.1976
2022-07-09 13:44:52 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.9835
2022-07-09 13:45:25 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.7911
2022-07-09 13:45:58 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.8896
2022-07-09 13:46:31 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.9888
2022-07-09 13:47:05 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.8782
2022-07-09 13:47:38 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.9879
2022-07-09 13:48:11 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.0584
2022-07-09 13:48:45 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.9420
2022-07-09 13:49:18 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.8434
2022-07-09 13:49:50 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.8763
2022-07-09 13:50:24 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.9480
2022-07-09 13:50:57 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.1876
2022-07-09 13:51:30 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.8202
2022-07-09 13:52:04 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.7817
2022-07-09 13:52:37 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.1922
2022-07-09 13:53:11 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.9662
2022-07-09 13:53:44 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.8420
2022-07-09 13:54:18 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.7580
2022-07-09 13:54:50 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.0012
2022-07-09 13:55:23 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.9752
2022-07-09 13:55:57 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.1029
2022-07-09 13:56:30 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.9813
2022-07-09 13:57:03 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.8804
2022-07-09 13:57:37 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.1529
2022-07-09 13:58:10 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.6871
2022-07-09 13:58:42 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.9994
2022-07-09 13:58:42 - train: epoch 033, train_loss: 1.9328
2022-07-09 13:59:55 - eval: epoch: 033, acc1: 61.054%, acc5: 83.756%, test_loss: 1.6240, per_image_load_time: 1.993ms, per_image_inference_time: 0.171ms
2022-07-09 13:59:55 - until epoch: 033, best_acc1: 61.054%
2022-07-09 13:59:55 - epoch 034 lr: 0.010000
2022-07-09 14:00:33 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.9530
2022-07-09 14:01:05 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.8472
2022-07-09 14:01:37 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.8255
2022-07-09 14:02:10 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.6696
2022-07-09 14:02:43 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.9185
2022-07-09 14:03:16 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.9033
2022-07-09 14:03:48 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.6467
2022-07-09 14:04:21 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.9480
2022-07-09 14:04:54 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.7596
2022-07-09 14:05:27 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.8543
2022-07-09 14:06:00 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.8293
2022-07-09 14:06:34 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.9953
2022-07-09 14:07:06 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.8872
2022-07-09 14:07:39 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.8230
2022-07-09 14:08:13 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.9020
2022-07-09 14:08:45 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.8230
2022-07-09 14:09:18 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.8666
2022-07-09 14:09:51 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.0183
2022-07-09 14:10:24 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.8938
2022-07-09 14:10:57 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.8961
2022-07-09 14:11:30 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.9081
2022-07-09 14:12:03 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.7258
2022-07-09 14:12:37 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.0190
2022-07-09 14:13:10 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.8001
2022-07-09 14:13:44 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.8301
2022-07-09 14:14:16 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.1224
2022-07-09 14:14:50 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.9575
2022-07-09 14:15:23 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.6907
2022-07-09 14:15:57 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.6625
2022-07-09 14:16:29 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.8039
2022-07-09 14:17:02 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.9686
2022-07-09 14:17:36 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.9928
2022-07-09 14:18:10 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.8582
2022-07-09 14:18:42 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.0404
2022-07-09 14:19:16 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.6666
2022-07-09 14:19:48 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.6736
2022-07-09 14:20:21 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.9243
2022-07-09 14:20:54 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 2.0495
2022-07-09 14:21:27 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.9435
2022-07-09 14:22:01 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.7050
2022-07-09 14:22:34 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.9292
2022-07-09 14:23:07 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.0104
2022-07-09 14:23:40 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.9242
2022-07-09 14:24:13 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.7627
2022-07-09 14:24:46 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.9516
2022-07-09 14:25:19 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.8528
2022-07-09 14:25:52 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.1423
2022-07-09 14:26:25 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.0798
2022-07-09 14:26:58 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.0082
2022-07-09 14:27:30 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.7466
2022-07-09 14:27:31 - train: epoch 034, train_loss: 1.9080
2022-07-09 14:28:44 - eval: epoch: 034, acc1: 61.438%, acc5: 84.024%, test_loss: 1.6071, per_image_load_time: 2.658ms, per_image_inference_time: 0.166ms
2022-07-09 14:28:44 - until epoch: 034, best_acc1: 61.438%
2022-07-09 14:28:44 - epoch 035 lr: 0.010000
2022-07-09 14:29:22 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.7631
2022-07-09 14:29:55 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.7725
2022-07-09 14:30:28 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.9605
2022-07-09 14:31:00 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.8771
2022-07-09 14:31:34 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.8858
2022-07-09 14:32:05 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.0245
2022-07-09 14:32:40 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.0477
2022-07-09 14:33:12 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.8662
2022-07-09 14:33:46 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.8484
2022-07-09 14:34:18 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.6516
2022-07-09 14:34:53 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.9748
2022-07-09 14:35:25 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.7823
2022-07-09 14:35:58 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.9449
2022-07-09 14:36:31 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.1424
2022-07-09 14:37:04 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.8830
2022-07-09 14:37:38 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.8968
2022-07-09 14:38:10 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.8815
2022-07-09 14:38:44 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.9609
2022-07-09 14:39:17 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.9261
2022-07-09 14:39:50 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.1528
2022-07-09 14:40:24 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.8898
2022-07-09 14:40:57 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.9050
2022-07-09 14:41:29 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.7614
2022-07-09 14:42:03 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.9380
2022-07-09 14:42:36 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.6867
2022-07-09 14:43:09 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.9009
2022-07-09 14:43:42 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.8501
2022-07-09 14:44:16 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.9876
2022-07-09 14:44:48 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.8776
2022-07-09 14:45:22 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.7946
2022-07-09 14:45:54 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.0199
2022-07-09 14:46:27 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.7870
2022-07-09 14:47:00 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.8842
2022-07-09 14:47:33 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.0217
2022-07-09 14:48:06 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.4740
2022-07-09 14:48:39 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.8834
2022-07-09 14:49:13 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.8218
2022-07-09 14:49:46 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.7744
2022-07-09 14:50:19 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.9385
2022-07-09 14:50:53 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.5983
2022-07-09 14:51:25 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.0362
2022-07-09 14:51:58 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.5413
2022-07-09 14:52:31 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.9671
2022-07-09 14:53:05 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.0973
2022-07-09 14:53:38 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.9157
2022-07-09 14:54:11 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.8535
2022-07-09 14:54:43 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.0418
2022-07-09 14:55:17 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.9726
2022-07-09 14:55:51 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.9231
2022-07-09 14:56:22 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.8788
2022-07-09 14:56:23 - train: epoch 035, train_loss: 1.8893
2022-07-09 14:57:36 - eval: epoch: 035, acc1: 61.666%, acc5: 84.310%, test_loss: 1.5954, per_image_load_time: 2.667ms, per_image_inference_time: 0.152ms
2022-07-09 14:57:36 - until epoch: 035, best_acc1: 61.666%
2022-07-09 14:57:36 - epoch 036 lr: 0.010000
2022-07-09 14:58:14 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.0450
2022-07-09 14:58:46 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.6682
2022-07-09 14:59:20 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.6949
2022-07-09 14:59:51 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.8013
2022-07-09 15:00:24 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.9854
2022-07-09 15:00:57 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.8884
2022-07-09 15:01:30 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.7773
2022-07-09 15:02:03 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.8864
2022-07-09 15:02:36 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.8323
2022-07-09 15:03:08 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.7209
2022-07-09 15:03:41 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.7854
2022-07-09 15:04:14 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.9573
2022-07-09 15:04:47 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.0068
2022-07-09 15:05:20 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8402
2022-07-09 15:05:54 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.1463
2022-07-09 15:06:27 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.0804
2022-07-09 15:06:59 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.8218
2022-07-09 15:07:33 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.7617
2022-07-09 15:08:06 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.9921
2022-07-09 15:08:39 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.7800
2022-07-09 15:09:13 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.9611
2022-07-09 15:09:46 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.0326
2022-07-09 15:10:19 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.8124
2022-07-09 15:10:51 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.9947
2022-07-09 15:11:24 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.7681
2022-07-09 15:11:58 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.9942
2022-07-09 15:12:30 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.7715
2022-07-09 15:13:05 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.6452
2022-07-09 15:13:38 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.8009
2022-07-09 15:14:11 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.9241
2022-07-09 15:14:43 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.9956
2022-07-09 15:15:17 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.0383
2022-07-09 15:15:50 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.8333
2022-07-09 15:16:23 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.5803
2022-07-09 15:16:56 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.1932
2022-07-09 15:17:30 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.9618
2022-07-09 15:18:03 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.8522
2022-07-09 15:18:37 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.8922
2022-07-09 15:19:09 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.8582
2022-07-09 15:19:43 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.9166
2022-07-09 15:20:16 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.8757
2022-07-09 15:20:49 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.8061
2022-07-09 15:21:23 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.8072
2022-07-09 15:21:56 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.9891
2022-07-09 15:22:29 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.7982
2022-07-09 15:23:03 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.6346
2022-07-09 15:23:36 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.7707
2022-07-09 15:24:09 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.7093
2022-07-09 15:24:42 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.7145
2022-07-09 15:25:14 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.9134
2022-07-09 15:25:15 - train: epoch 036, train_loss: 1.8786
2022-07-09 15:26:27 - eval: epoch: 036, acc1: 61.412%, acc5: 83.932%, test_loss: 1.6108, per_image_load_time: 2.372ms, per_image_inference_time: 0.170ms
2022-07-09 15:26:27 - until epoch: 036, best_acc1: 61.666%
2022-07-09 15:26:27 - epoch 037 lr: 0.010000
2022-07-09 15:27:05 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.8936
2022-07-09 15:27:38 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.7621
2022-07-09 15:28:11 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.9365
2022-07-09 15:28:44 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.8327
2022-07-09 15:29:17 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.0049
2022-07-09 15:29:50 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.1092
2022-07-09 15:30:22 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.9059
2022-07-09 15:30:55 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.8293
2022-07-09 15:31:29 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.0799
2022-07-09 15:32:01 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.7571
2022-07-09 15:32:35 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.8943
2022-07-09 15:33:07 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.8888
2022-07-09 15:33:40 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.7751
2022-07-09 15:34:15 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.0855
2022-07-09 15:34:46 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.8341
2022-07-09 15:35:20 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.5350
2022-07-09 15:35:54 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.9327
2022-07-09 15:36:27 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.9737
2022-07-09 15:36:59 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.9197
2022-07-09 15:37:33 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.0236
2022-07-09 15:38:06 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.9360
2022-07-09 15:38:40 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.7958
2022-07-09 15:39:12 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.7787
2022-07-09 15:39:45 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.9040
2022-07-09 15:40:18 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.7175
2022-07-09 15:40:51 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.1204
2022-07-09 15:41:25 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.9958
2022-07-09 15:41:58 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.7834
2022-07-09 15:42:31 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.8572
2022-07-09 15:43:04 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.0431
2022-07-09 15:43:38 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.6763
2022-07-09 15:44:10 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.9611
2022-07-09 15:44:44 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.8072
2022-07-09 15:45:18 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.7674
2022-07-09 15:45:51 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.8831
2022-07-09 15:46:25 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.9157
2022-07-09 15:46:56 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.7598
2022-07-09 15:47:31 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.9392
2022-07-09 15:48:04 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.9379
2022-07-09 15:48:37 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.9022
2022-07-09 15:49:10 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.9658
2022-07-09 15:49:44 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.9117
2022-07-09 15:50:17 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.7991
2022-07-09 15:50:50 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.7247
2022-07-09 15:51:23 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.8318
2022-07-09 15:51:57 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.9631
2022-07-09 15:52:29 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.8306
2022-07-09 15:53:03 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.7587
2022-07-09 15:53:37 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.7514
2022-07-09 15:54:08 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.6254
2022-07-09 15:54:09 - train: epoch 037, train_loss: 1.8674
2022-07-09 15:55:22 - eval: epoch: 037, acc1: 62.214%, acc5: 84.322%, test_loss: 1.5798, per_image_load_time: 2.078ms, per_image_inference_time: 0.181ms
2022-07-09 15:55:22 - until epoch: 037, best_acc1: 62.214%
2022-07-09 15:55:22 - epoch 038 lr: 0.010000
2022-07-09 15:56:00 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.6825
2022-07-09 15:56:33 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.9369
2022-07-09 15:57:06 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.6282
2022-07-09 15:57:37 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.8754
2022-07-09 15:58:10 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.8039
2022-07-09 15:58:42 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.9406
2022-07-09 15:59:15 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.8086
2022-07-09 15:59:48 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.8605
2022-07-09 16:00:21 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.0710
2022-07-09 16:00:54 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.1304
2022-07-09 16:01:27 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.8075
2022-07-09 16:02:00 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.9794
2022-07-09 16:02:34 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.9913
2022-07-09 16:03:06 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.8615
2022-07-09 16:03:39 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.7670
2022-07-09 16:04:11 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.8498
2022-07-09 16:04:46 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.0266
2022-07-09 16:05:18 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.8520
2022-07-09 16:05:52 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.9369
2022-07-09 16:06:24 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.8264
2022-07-09 16:06:58 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.7486
2022-07-09 16:07:31 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.6681
2022-07-09 16:08:05 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.0289
2022-07-09 16:08:37 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.0683
2022-07-09 16:09:11 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.5896
2022-07-09 16:09:44 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.9000
2022-07-09 16:10:18 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 2.0113
2022-07-09 16:10:50 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.0745
2022-07-09 16:11:25 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.8836
2022-07-09 16:11:57 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.8119
2022-07-09 16:12:31 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.9584
2022-07-09 16:13:03 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.5565
2022-07-09 16:13:37 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.8711
2022-07-09 16:14:10 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.7579
2022-07-09 16:14:44 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.1622
2022-07-09 16:15:16 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.9720
2022-07-09 16:15:51 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.6811
2022-07-09 16:16:23 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.8400
2022-07-09 16:16:57 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.8562
2022-07-09 16:17:29 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.8376
2022-07-09 16:18:03 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.8088
2022-07-09 16:18:36 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.8208
2022-07-09 16:19:11 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.9760
2022-07-09 16:19:43 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.9906
2022-07-09 16:20:16 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.9929
2022-07-09 16:20:49 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.8545
2022-07-09 16:21:23 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.9028
2022-07-09 16:21:56 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.8964
2022-07-09 16:22:28 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.7785
2022-07-09 16:23:00 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.7284
2022-07-09 16:23:01 - train: epoch 038, train_loss: 1.8651
2022-07-09 16:24:14 - eval: epoch: 038, acc1: 61.526%, acc5: 84.086%, test_loss: 1.6029, per_image_load_time: 2.667ms, per_image_inference_time: 0.146ms
2022-07-09 16:24:14 - until epoch: 038, best_acc1: 62.214%
2022-07-09 16:24:14 - epoch 039 lr: 0.010000
2022-07-09 16:24:51 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.8347
2022-07-09 16:25:25 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.9543
2022-07-09 16:25:57 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.9548
2022-07-09 16:26:29 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.7736
2022-07-09 16:27:02 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.0173
2022-07-09 16:27:35 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.8214
2022-07-09 16:28:08 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.9105
2022-07-09 16:28:40 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.8380
2022-07-09 16:29:14 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.8387
2022-07-09 16:29:47 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.8879
2022-07-09 16:30:20 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.8793
2022-07-09 16:30:53 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.7792
2022-07-09 16:31:26 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.9141
2022-07-09 16:31:58 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.0175
2022-07-09 16:32:32 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.8669
2022-07-09 16:33:05 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.8545
2022-07-09 16:33:38 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.7361
2022-07-09 16:34:11 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.7317
2022-07-09 16:34:44 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.5612
2022-07-09 16:35:16 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.9059
2022-07-09 16:35:51 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.1906
2022-07-09 16:36:22 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.6765
2022-07-09 16:36:56 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.0058
2022-07-09 16:37:28 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.9009
2022-07-09 16:38:02 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 2.0030
2022-07-09 16:38:35 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.9308
2022-07-09 16:39:08 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.9921
2022-07-09 16:39:42 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.9202
2022-07-09 16:40:15 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.6478
2022-07-09 16:40:47 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.0609
2022-07-09 16:41:20 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.7433
2022-07-09 16:41:54 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.0462
2022-07-09 16:42:27 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.9467
2022-07-09 16:43:00 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.8923
2022-07-09 16:43:32 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.0410
2022-07-09 16:44:06 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.9123
2022-07-09 16:44:40 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.7372
2022-07-09 16:45:12 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.6755
2022-07-09 16:45:46 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.7629
2022-07-09 16:46:19 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.9369
2022-07-09 16:46:52 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.9750
2022-07-09 16:47:25 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.8348
2022-07-09 16:47:59 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.7083
2022-07-09 16:48:32 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.7741
2022-07-09 16:49:06 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.7676
2022-07-09 16:49:39 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.9645
2022-07-09 16:50:12 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.7054
2022-07-09 16:50:45 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.7280
2022-07-09 16:51:18 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.6807
2022-07-09 16:51:49 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.8049
2022-07-09 16:51:50 - train: epoch 039, train_loss: 1.8587
2022-07-09 16:53:03 - eval: epoch: 039, acc1: 61.726%, acc5: 84.258%, test_loss: 1.5851, per_image_load_time: 2.385ms, per_image_inference_time: 0.177ms
2022-07-09 16:53:03 - until epoch: 039, best_acc1: 62.214%
2022-07-09 16:53:03 - epoch 040 lr: 0.010000
2022-07-09 16:53:41 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.0563
2022-07-09 16:54:15 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.7709
2022-07-09 16:54:48 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.9815
2022-07-09 16:55:20 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.8540
2022-07-09 16:55:52 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.8459
2022-07-09 16:56:25 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.6286
2022-07-09 16:56:59 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.8709
2022-07-09 16:57:31 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.9437
2022-07-09 16:58:05 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.7200
2022-07-09 16:58:37 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.7315
2022-07-09 16:59:11 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.8553
2022-07-09 16:59:43 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.9135
2022-07-09 17:00:16 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.7733
2022-07-09 17:00:49 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.7335
2022-07-09 17:01:22 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.9869
2022-07-09 17:01:55 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.7623
2022-07-09 17:02:28 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.9428
2022-07-09 17:03:01 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.7444
2022-07-09 17:03:34 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.7891
2022-07-09 17:04:06 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.9509
2022-07-09 17:04:40 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.7687
2022-07-09 17:05:13 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.6792
2022-07-09 17:05:46 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.7344
2022-07-09 17:06:20 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.9260
2022-07-09 17:06:52 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.0335
2022-07-09 17:07:26 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.9078
2022-07-09 17:07:58 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.9208
2022-07-09 17:08:32 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.8110
2022-07-09 17:09:05 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.0792
2022-07-09 17:09:39 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.9174
2022-07-09 17:10:11 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.0859
2022-07-09 17:10:45 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.1099
2022-07-09 17:11:18 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.0747
2022-07-09 17:11:51 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.8415
2022-07-09 17:12:24 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 2.0528
2022-07-09 17:12:58 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.9177
2022-07-09 17:13:31 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 2.0223
2022-07-09 17:14:05 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.6592
2022-07-09 17:14:36 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.8526
2022-07-09 17:15:10 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.9842
2022-07-09 17:15:43 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.8283
2022-07-09 17:16:17 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.8969
2022-07-09 17:16:50 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.7844
2022-07-09 17:17:22 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.8153
2022-07-09 17:17:55 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.6803
2022-07-09 17:18:28 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.0820
2022-07-09 17:19:02 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.9617
2022-07-09 17:19:35 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.7297
2022-07-09 17:20:09 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.7991
2022-07-09 17:20:40 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.9616
2022-07-09 17:20:41 - train: epoch 040, train_loss: 1.8588
2022-07-09 17:21:54 - eval: epoch: 040, acc1: 61.454%, acc5: 84.138%, test_loss: 1.5995, per_image_load_time: 2.676ms, per_image_inference_time: 0.151ms
2022-07-09 17:21:54 - until epoch: 040, best_acc1: 62.214%
2022-07-09 17:21:54 - epoch 041 lr: 0.010000
2022-07-09 17:22:33 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.0456
2022-07-09 17:23:05 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.0530
2022-07-09 17:23:39 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.8744
2022-07-09 17:24:10 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.0355
2022-07-09 17:24:43 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.7639
2022-07-09 17:25:16 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.7841
2022-07-09 17:25:49 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.9583
2022-07-09 17:26:22 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.8509
2022-07-09 17:26:55 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.5711
2022-07-09 17:27:28 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.0941
2022-07-09 17:28:02 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.7386
2022-07-09 17:28:34 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.7597
2022-07-09 17:29:07 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.7811
2022-07-09 17:29:40 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.8869
2022-07-09 17:30:13 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.1925
2022-07-09 17:30:46 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.8288
2022-07-09 17:31:20 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.9406
2022-07-09 17:31:52 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.7162
2022-07-09 17:32:26 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.0222
2022-07-09 17:32:59 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.7303
2022-07-09 17:33:32 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.7771
2022-07-09 17:34:05 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.5150
2022-07-09 17:34:38 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.6648
2022-07-09 17:35:11 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.0052
2022-07-09 17:35:44 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.8780
2022-07-09 17:36:18 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.1501
2022-07-09 17:36:50 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.1066
2022-07-09 17:37:24 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.9000
2022-07-09 17:37:58 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.9048
2022-07-09 17:38:30 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.9427
2022-07-09 17:39:04 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.9803
2022-07-09 17:39:37 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.7462
2022-07-09 17:40:10 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.7760
2022-07-09 17:40:43 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.6279
2022-07-09 17:41:17 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.7948
2022-07-09 17:41:49 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.1652
2022-07-09 17:42:22 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.8871
2022-07-09 17:42:55 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 2.1023
2022-07-09 17:43:28 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.9438
2022-07-09 17:44:02 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.8251
2022-07-09 17:44:34 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.8051
2022-07-09 17:45:06 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.6099
2022-07-09 17:45:40 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.8579
2022-07-09 17:46:13 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.7248
2022-07-09 17:46:47 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.8741
2022-07-09 17:47:19 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.1659
2022-07-09 17:47:53 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.9970
2022-07-09 17:48:26 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.8210
2022-07-09 17:48:59 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.9578
2022-07-09 17:49:30 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.9132
2022-07-09 17:49:31 - train: epoch 041, train_loss: 1.8574
2022-07-09 17:50:44 - eval: epoch: 041, acc1: 61.708%, acc5: 84.326%, test_loss: 1.5942, per_image_load_time: 2.164ms, per_image_inference_time: 0.153ms
2022-07-09 17:50:44 - until epoch: 041, best_acc1: 62.214%
2022-07-09 17:50:44 - epoch 042 lr: 0.010000
2022-07-09 17:51:22 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.5608
2022-07-09 17:51:55 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.6892
2022-07-09 17:52:28 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.7327
2022-07-09 17:52:59 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.6391
2022-07-09 17:53:33 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.8638
2022-07-09 17:54:06 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.6496
2022-07-09 17:54:38 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.9143
2022-07-09 17:55:11 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.8627
2022-07-09 17:55:44 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.8897
2022-07-09 17:56:17 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.0650
2022-07-09 17:56:51 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 2.0047
2022-07-09 17:57:23 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.4894
2022-07-09 17:57:57 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.9016
2022-07-09 17:58:29 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.1152
2022-07-09 17:59:03 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.6897
2022-07-09 17:59:35 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.0259
2022-07-09 18:00:08 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.8947
2022-07-09 18:00:41 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.7323
2022-07-09 18:01:14 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.0412
2022-07-09 18:01:47 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.7343
2022-07-09 18:02:20 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.8077
2022-07-09 18:02:54 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.7124
2022-07-09 18:03:27 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.8776
2022-07-09 18:04:01 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.0713
2022-07-09 18:04:34 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.8791
2022-07-09 18:05:07 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.8002
2022-07-09 18:05:40 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.5629
2022-07-09 18:06:13 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.7065
2022-07-09 18:06:46 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.7175
2022-07-09 18:07:19 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.7563
2022-07-09 18:07:53 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.1740
2022-07-09 18:08:26 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.0658
2022-07-09 18:09:00 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.0062
2022-07-09 18:09:33 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.5549
2022-07-09 18:10:06 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.8694
2022-07-09 18:10:38 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.1076
2022-07-09 18:11:11 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.8164
2022-07-09 18:11:45 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.6261
2022-07-09 18:12:18 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.7617
2022-07-09 18:12:50 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.9279
2022-07-09 18:13:25 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.9426
2022-07-09 18:13:57 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.9914
2022-07-09 18:14:31 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.7156
2022-07-09 18:15:04 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.8575
2022-07-09 18:15:38 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.6100
2022-07-09 18:16:11 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.9819
2022-07-09 18:16:43 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.7801
2022-07-09 18:17:17 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.8129
2022-07-09 18:17:50 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.8647
2022-07-09 18:18:22 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.9198
2022-07-09 18:18:23 - train: epoch 042, train_loss: 1.8572
2022-07-09 18:19:37 - eval: epoch: 042, acc1: 61.688%, acc5: 84.462%, test_loss: 1.5871, per_image_load_time: 2.725ms, per_image_inference_time: 0.152ms
2022-07-09 18:19:37 - until epoch: 042, best_acc1: 62.214%
2022-07-09 18:19:37 - epoch 043 lr: 0.010000
2022-07-09 18:20:15 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.0723
2022-07-09 18:20:47 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.9498
2022-07-09 18:21:20 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.7617
2022-07-09 18:21:53 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.6922
2022-07-09 18:22:26 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.7105
2022-07-09 18:22:59 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.8344
2022-07-09 18:23:32 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.8598
2022-07-09 18:24:05 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.9450
2022-07-09 18:24:37 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.9471
2022-07-09 18:25:11 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.8796
2022-07-09 18:25:44 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.7967
2022-07-09 18:26:18 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.7026
2022-07-09 18:26:51 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.7972
2022-07-09 18:27:23 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.7493
2022-07-09 18:27:58 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.7658
2022-07-09 18:28:30 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.6784
2022-07-09 18:29:04 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.0523
2022-07-09 18:29:37 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.9845
2022-07-09 18:30:11 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.8010
2022-07-09 18:30:43 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.5698
2022-07-09 18:31:17 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.7410
2022-07-09 18:31:50 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.1025
2022-07-09 18:32:23 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.8967
2022-07-09 18:32:56 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.7786
2022-07-09 18:33:29 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.1073
2022-07-09 18:34:03 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.8752
2022-07-09 18:34:34 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.8909
2022-07-09 18:35:07 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.7879
2022-07-09 18:35:40 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.7788
2022-07-09 18:36:13 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.0313
2022-07-09 18:36:46 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.0026
2022-07-09 18:37:19 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.7923
2022-07-09 18:37:52 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.8515
2022-07-09 18:38:25 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.9043
2022-07-09 18:38:59 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.0229
2022-07-09 18:39:31 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.8967
2022-07-09 18:40:05 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.7309
2022-07-09 18:40:38 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.8258
2022-07-09 18:41:12 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.0675
2022-07-09 18:41:45 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.0371
2022-07-09 18:42:19 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.1177
2022-07-09 18:42:51 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.8215
2022-07-09 18:43:25 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.9924
2022-07-09 18:43:58 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.8866
2022-07-09 18:44:32 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.8126
2022-07-09 18:45:05 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.7502
2022-07-09 18:45:38 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.9352
2022-07-09 18:46:10 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.6932
2022-07-09 18:46:44 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.8972
2022-07-09 18:47:15 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.8948
2022-07-09 18:47:16 - train: epoch 043, train_loss: 1.8573
2022-07-09 18:48:29 - eval: epoch: 043, acc1: 60.784%, acc5: 83.504%, test_loss: 1.6378, per_image_load_time: 1.939ms, per_image_inference_time: 0.180ms
2022-07-09 18:48:29 - until epoch: 043, best_acc1: 62.214%
2022-07-09 18:48:29 - epoch 044 lr: 0.010000
2022-07-09 18:49:08 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.8194
2022-07-09 18:49:40 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.0535
2022-07-09 18:50:13 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.6945
2022-07-09 18:50:45 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.5551
2022-07-09 18:51:19 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.7980
2022-07-09 18:51:50 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.7372
2022-07-09 18:52:23 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.9489
2022-07-09 18:52:57 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.8612
2022-07-09 18:53:29 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.8415
2022-07-09 18:54:02 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.7607
2022-07-09 18:54:35 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.5483
2022-07-09 18:55:08 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.6454
2022-07-09 18:55:41 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.0407
2022-07-09 18:56:14 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.7794
2022-07-09 18:56:47 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.8761
2022-07-09 18:57:20 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.6591
2022-07-09 18:57:52 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.7795
2022-07-09 18:58:26 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.6528
2022-07-09 18:58:58 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.9944
2022-07-09 18:59:32 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.8132
2022-07-09 19:00:04 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.2848
2022-07-09 19:00:38 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.7758
2022-07-09 19:01:10 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.9037
2022-07-09 19:01:43 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.8489
2022-07-09 19:02:16 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.7869
2022-07-09 19:02:49 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.9200
2022-07-09 19:03:22 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.9299
2022-07-09 19:03:55 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.6942
2022-07-09 19:04:29 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.8923
2022-07-09 19:05:02 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.4681
2022-07-09 19:05:35 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.8790
2022-07-09 19:06:08 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.6404
2022-07-09 19:06:41 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.9350
2022-07-09 19:07:14 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.9521
2022-07-09 19:07:48 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.8810
2022-07-09 19:08:21 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.0039
2022-07-09 19:08:53 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.9304
2022-07-09 19:09:27 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.6490
2022-07-09 19:10:00 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.1566
2022-07-09 19:10:33 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.8576
2022-07-09 19:11:06 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.5980
2022-07-09 19:11:39 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.0371
2022-07-09 19:12:13 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.8791
2022-07-09 19:12:46 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.8457
2022-07-09 19:13:20 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.1193
2022-07-09 19:13:52 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.7193
2022-07-09 19:14:26 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.8737
2022-07-09 19:14:58 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.7449
2022-07-09 19:15:32 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.9162
2022-07-09 19:16:03 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.8104
2022-07-09 19:16:04 - train: epoch 044, train_loss: 1.8588
2022-07-09 19:17:17 - eval: epoch: 044, acc1: 61.184%, acc5: 83.970%, test_loss: 1.6131, per_image_load_time: 2.476ms, per_image_inference_time: 0.164ms
2022-07-09 19:17:18 - until epoch: 044, best_acc1: 62.214%
2022-07-09 19:17:18 - epoch 045 lr: 0.010000
2022-07-09 19:17:56 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.8037
2022-07-09 19:18:28 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.9520
2022-07-09 19:19:01 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.8420
2022-07-09 19:19:33 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.7927
2022-07-09 19:20:08 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.0306
2022-07-09 19:20:39 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.8991
2022-07-09 19:21:13 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.7822
2022-07-09 19:21:46 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.7553
2022-07-09 19:22:19 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.6175
2022-07-09 19:22:53 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.9042
2022-07-09 19:23:26 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.8687
2022-07-09 19:23:58 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.8950
2022-07-09 19:24:32 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.9364
2022-07-09 19:25:05 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.8651
2022-07-09 19:25:38 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.8314
2022-07-09 19:26:11 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.7285
2022-07-09 19:26:44 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.6946
2022-07-09 19:27:17 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.8940
2022-07-09 19:27:50 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.9311
2022-07-09 19:28:24 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.7785
2022-07-09 19:28:56 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.9642
2022-07-09 19:29:29 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.9538
2022-07-09 19:30:03 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.8578
2022-07-09 19:30:37 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.8519
2022-07-09 19:31:08 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.8638
2022-07-09 19:31:43 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.8218
2022-07-09 19:32:15 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.6737
2022-07-09 19:32:49 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.8511
2022-07-09 19:33:21 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.9765
2022-07-09 19:33:55 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.0648
2022-07-09 19:34:28 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.7634
2022-07-09 19:35:02 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.1308
2022-07-09 19:35:35 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.8536
2022-07-09 19:36:08 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.8211
2022-07-09 19:36:41 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.8827
2022-07-09 19:37:14 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.8692
2022-07-09 19:37:47 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.9111
2022-07-09 19:38:20 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.9096
2022-07-09 19:38:54 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.9552
2022-07-09 19:39:27 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.8539
2022-07-09 19:40:00 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.7891
2022-07-09 19:40:33 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.9286
2022-07-09 19:41:07 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.8273
2022-07-09 19:41:40 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.0549
2022-07-09 19:42:14 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.7208
2022-07-09 19:42:46 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.0888
2022-07-09 19:43:20 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.8682
2022-07-09 19:43:52 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.7669
2022-07-09 19:44:27 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.8352
2022-07-09 19:44:58 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.7187
2022-07-09 19:44:59 - train: epoch 045, train_loss: 1.8598
2022-07-09 19:46:12 - eval: epoch: 045, acc1: 61.284%, acc5: 84.108%, test_loss: 1.6094, per_image_load_time: 2.696ms, per_image_inference_time: 0.159ms
2022-07-09 19:46:12 - until epoch: 045, best_acc1: 62.214%
2022-07-09 19:46:12 - epoch 046 lr: 0.010000
2022-07-09 19:46:51 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.5512
2022-07-09 19:47:22 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.8037
2022-07-09 19:47:55 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.7621
2022-07-09 19:48:28 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.7592
2022-07-09 19:49:00 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.7819
2022-07-09 19:49:32 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.1628
2022-07-09 19:50:05 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.9436
2022-07-09 19:50:38 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.1299
2022-07-09 19:51:11 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.9795
2022-07-09 19:51:44 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.6796
2022-07-09 19:52:18 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.8812
2022-07-09 19:52:51 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.8207
2022-07-09 19:53:23 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.8221
2022-07-09 19:53:57 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.8179
2022-07-09 19:54:29 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.7175
2022-07-09 19:55:03 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.0230
2022-07-09 19:55:36 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.8682
2022-07-09 19:56:09 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.0532
2022-07-09 19:56:43 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.7241
2022-07-09 19:57:14 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.6257
2022-07-09 19:57:48 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.0539
2022-07-09 19:58:21 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.6075
2022-07-09 19:58:54 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.9471
2022-07-09 19:59:27 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.7927
2022-07-09 20:00:00 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.7513
2022-07-09 20:00:33 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.1907
2022-07-09 20:01:08 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.7179
2022-07-09 20:01:40 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.5880
2022-07-09 20:02:14 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.9019
2022-07-09 20:02:46 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.9019
2022-07-09 20:03:20 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.7650
2022-07-09 20:03:52 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.8546
2022-07-09 20:04:25 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.8711
2022-07-09 20:04:58 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.8661
2022-07-09 20:05:31 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.1517
2022-07-09 20:06:05 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.8988
2022-07-09 20:06:39 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.7284
2022-07-09 20:07:12 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.8070
2022-07-09 20:07:45 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.8123
2022-07-09 20:08:18 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.7135
2022-07-09 20:08:51 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.1332
2022-07-09 20:09:24 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.8270
2022-07-09 20:09:58 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.0057
2022-07-09 20:10:31 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.2081
2022-07-09 20:11:04 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.6630
2022-07-09 20:11:37 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.9476
2022-07-09 20:12:10 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.7864
2022-07-09 20:12:43 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.5554
2022-07-09 20:13:17 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.8659
2022-07-09 20:13:48 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.8457
2022-07-09 20:13:49 - train: epoch 046, train_loss: 1.8598
2022-07-09 20:15:02 - eval: epoch: 046, acc1: 61.492%, acc5: 84.094%, test_loss: 1.6060, per_image_load_time: 2.655ms, per_image_inference_time: 0.161ms
2022-07-09 20:15:02 - until epoch: 046, best_acc1: 62.214%
2022-07-09 20:15:02 - epoch 047 lr: 0.010000
2022-07-09 20:15:39 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.9228
2022-07-09 20:16:13 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.8929
2022-07-09 20:16:46 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.6881
2022-07-09 20:17:19 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.6004
2022-07-09 20:17:52 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.7697
2022-07-09 20:18:25 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.8465
2022-07-09 20:18:57 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.8544
2022-07-09 20:19:31 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 2.1036
2022-07-09 20:20:03 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.9645
2022-07-09 20:20:36 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.9188
2022-07-09 20:21:09 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.1856
2022-07-09 20:21:42 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.8780
2022-07-09 20:22:16 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.6655
2022-07-09 20:22:49 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.8957
2022-07-09 20:23:22 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.7347
2022-07-09 20:23:54 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.7621
2022-07-09 20:24:27 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.8199
2022-07-09 20:25:00 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.9661
2022-07-09 20:25:34 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.6474
2022-07-09 20:26:07 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.9888
2022-07-09 20:26:40 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.0489
2022-07-09 20:27:13 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.9097
2022-07-09 20:27:46 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.9585
2022-07-09 20:28:19 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.8160
2022-07-09 20:28:53 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.8590
2022-07-09 20:29:26 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.0619
2022-07-09 20:29:59 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.6267
2022-07-09 20:30:32 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.6714
2022-07-09 20:31:06 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.6438
2022-07-09 20:31:39 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.9349
2022-07-09 20:32:13 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.7446
2022-07-09 20:32:45 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.9078
2022-07-09 20:33:19 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.9389
2022-07-09 20:33:52 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.6653
2022-07-09 20:34:25 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.1810
2022-07-09 20:34:59 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.8795
2022-07-09 20:35:32 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.8617
2022-07-09 20:36:05 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.7327
2022-07-09 20:36:40 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.7942
2022-07-09 20:37:12 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.9207
2022-07-09 20:37:45 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.0856
2022-07-09 20:38:19 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.8251
2022-07-09 20:38:52 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.7532
2022-07-09 20:39:25 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.7773
2022-07-09 20:39:58 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.9688
2022-07-09 20:40:31 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.9292
2022-07-09 20:41:05 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.2456
2022-07-09 20:41:38 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.7430
2022-07-09 20:42:10 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.9625
2022-07-09 20:42:43 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.7636
2022-07-09 20:42:44 - train: epoch 047, train_loss: 1.8579
2022-07-09 20:43:57 - eval: epoch: 047, acc1: 60.584%, acc5: 83.474%, test_loss: 1.6435, per_image_load_time: 2.698ms, per_image_inference_time: 0.159ms
2022-07-09 20:43:57 - until epoch: 047, best_acc1: 62.214%
2022-07-09 20:43:57 - epoch 048 lr: 0.010000
2022-07-09 20:44:36 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.9221
2022-07-09 20:45:09 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.0084
2022-07-09 20:45:41 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.9492
2022-07-09 20:46:13 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.7735
2022-07-09 20:46:46 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.7357
2022-07-09 20:47:18 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.9294
2022-07-09 20:47:51 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.9148
2022-07-09 20:48:24 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.9492
2022-07-09 20:48:57 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.7466
2022-07-09 20:49:30 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.0156
2022-07-09 20:50:04 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.9708
2022-07-09 20:50:36 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.9983
2022-07-09 20:51:10 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.6239
2022-07-09 20:51:43 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.8574
2022-07-09 20:52:16 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.9261
2022-07-09 20:52:48 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.8357
2022-07-09 20:53:22 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.8130
2022-07-09 20:53:55 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.8027
2022-07-09 20:54:28 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.9870
2022-07-09 20:55:02 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.0188
2022-07-09 20:55:35 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.7639
2022-07-09 20:56:08 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.2373
2022-07-09 20:56:41 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.6699
2022-07-09 20:57:15 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.0874
2022-07-09 20:57:47 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.8966
2022-07-09 20:58:21 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.9269
2022-07-09 20:58:53 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.9153
2022-07-09 20:59:26 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.7781
2022-07-09 21:00:00 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.8374
2022-07-09 21:00:33 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.8262
2022-07-09 21:01:06 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.7442
2022-07-09 21:01:40 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.7173
2022-07-09 21:02:12 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.9298
2022-07-09 21:02:45 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.8686
2022-07-09 21:03:19 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.0450
2022-07-09 21:03:52 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.9821
2022-07-09 21:04:25 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.8787
2022-07-09 21:04:59 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.0159
2022-07-09 21:05:32 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.9582
2022-07-09 21:06:05 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.8161
2022-07-09 21:06:38 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.8785
2022-07-09 21:07:11 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.7201
2022-07-09 21:07:44 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.7759
2022-07-09 21:08:17 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.9686
2022-07-09 21:08:51 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.8483
2022-07-09 21:09:24 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.7294
2022-07-09 21:09:58 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.9574
2022-07-09 21:10:30 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.1757
2022-07-09 21:11:03 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.9547
2022-07-09 21:11:36 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.7388
2022-07-09 21:11:36 - train: epoch 048, train_loss: 1.8582
2022-07-09 21:12:49 - eval: epoch: 048, acc1: 60.692%, acc5: 83.576%, test_loss: 1.6407, per_image_load_time: 2.283ms, per_image_inference_time: 0.164ms
2022-07-09 21:12:49 - until epoch: 048, best_acc1: 62.214%
2022-07-09 21:12:49 - epoch 049 lr: 0.010000
2022-07-09 21:13:28 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.0894
2022-07-09 21:14:00 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.6883
2022-07-09 21:14:33 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.8657
2022-07-09 21:15:06 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.1083
2022-07-09 21:15:38 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.0843
2022-07-09 21:16:11 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.8869
2022-07-09 21:16:44 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.8074
2022-07-09 21:17:17 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.1138
2022-07-09 21:17:50 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.5663
2022-07-09 21:18:23 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.7045
2022-07-09 21:18:57 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.8325
2022-07-09 21:19:29 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.6472
2022-07-09 21:20:03 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.0361
2022-07-09 21:20:36 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.9789
2022-07-09 21:21:09 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.9792
2022-07-09 21:21:41 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.9559
2022-07-09 21:22:15 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.9365
2022-07-09 21:22:49 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.7683
2022-07-09 21:23:21 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.8396
2022-07-09 21:23:55 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.7438
2022-07-09 21:24:27 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.7079
2022-07-09 21:25:01 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.8685
2022-07-09 21:25:33 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.8440
2022-07-09 21:26:07 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.9758
2022-07-09 21:26:40 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 2.0791
2022-07-09 21:27:13 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.9928
2022-07-09 21:27:46 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.8518
2022-07-09 21:28:20 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.8120
2022-07-09 21:28:52 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.7270
2022-07-09 21:29:27 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.9959
2022-07-09 21:29:59 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.9452
2022-07-09 21:30:32 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.0441
2022-07-09 21:31:05 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.7379
2022-07-09 21:31:38 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.8214
2022-07-09 21:32:11 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.0464
2022-07-09 21:32:44 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.8624
2022-07-09 21:33:18 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.8468
2022-07-09 21:33:51 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.0114
2022-07-09 21:34:24 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.0360
2022-07-09 21:34:58 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.7168
2022-07-09 21:35:30 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.9112
2022-07-09 21:36:04 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.8892
2022-07-09 21:36:37 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.0524
2022-07-09 21:37:10 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.9179
2022-07-09 21:37:43 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.7727
2022-07-09 21:38:15 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.0927
2022-07-09 21:38:49 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.1366
2022-07-09 21:39:22 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.8007
2022-07-09 21:39:55 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.9375
2022-07-09 21:40:27 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.7988
2022-07-09 21:40:28 - train: epoch 049, train_loss: 1.8565
2022-07-09 21:41:41 - eval: epoch: 049, acc1: 61.488%, acc5: 83.952%, test_loss: 1.6057, per_image_load_time: 2.587ms, per_image_inference_time: 0.156ms
2022-07-09 21:41:41 - until epoch: 049, best_acc1: 62.214%
2022-07-09 21:41:41 - epoch 050 lr: 0.010000
2022-07-09 21:42:19 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.9805
2022-07-09 21:42:52 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.7786
2022-07-09 21:43:25 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.8437
2022-07-09 21:43:57 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.7983
2022-07-09 21:44:30 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.9120
2022-07-09 21:45:03 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.9574
2022-07-09 21:45:35 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.6921
2022-07-09 21:46:09 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.6268
2022-07-09 21:46:41 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.9362
2022-07-09 21:47:14 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.8849
2022-07-09 21:47:47 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.8093
2022-07-09 21:48:20 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 2.0271
2022-07-09 21:48:53 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.7380
2022-07-09 21:49:27 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.8271
2022-07-09 21:50:00 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.7370
2022-07-09 21:50:33 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.6996
2022-07-09 21:51:06 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.8482
2022-07-09 21:51:40 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.1383
2022-07-09 21:52:12 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.9277
2022-07-09 21:52:46 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.7156
2022-07-09 21:53:18 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.8901
2022-07-09 21:53:52 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.9814
2022-07-09 21:54:25 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.7327
2022-07-09 21:54:58 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.1261
2022-07-09 21:55:31 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.7256
2022-07-09 21:56:04 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.7075
2022-07-09 21:56:37 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.8223
2022-07-09 21:57:11 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.0333
2022-07-09 21:57:45 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.0161
2022-07-09 21:58:17 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.9853
2022-07-09 21:58:51 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.1178
2022-07-09 21:59:24 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.0750
2022-07-09 21:59:58 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.6062
2022-07-09 22:00:31 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.8167
2022-07-09 22:01:04 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.8505
2022-07-09 22:01:37 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.9178
2022-07-09 22:02:10 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.0429
2022-07-09 22:02:44 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.6390
2022-07-09 22:03:17 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.7819
2022-07-09 22:03:50 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.8984
2022-07-09 22:04:24 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.9091
2022-07-09 22:04:57 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.9906
2022-07-09 22:05:30 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.8491
2022-07-09 22:06:03 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.8719
2022-07-09 22:06:36 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.7586
2022-07-09 22:07:10 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.6897
2022-07-09 22:07:43 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.8972
2022-07-09 22:08:16 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.8242
2022-07-09 22:08:49 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.6416
2022-07-09 22:09:21 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.8062
2022-07-09 22:09:22 - train: epoch 050, train_loss: 1.8574
2022-07-09 22:10:34 - eval: epoch: 050, acc1: 60.872%, acc5: 83.670%, test_loss: 1.6255, per_image_load_time: 1.500ms, per_image_inference_time: 0.156ms
2022-07-09 22:10:34 - until epoch: 050, best_acc1: 62.214%
2022-07-09 22:10:34 - epoch 051 lr: 0.010000
2022-07-09 22:11:12 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.9885
2022-07-09 22:11:45 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.1941
2022-07-09 22:12:18 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.8982
2022-07-09 22:12:51 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.6557
2022-07-09 22:13:24 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.9502
2022-07-09 22:13:56 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 2.0682
2022-07-09 22:14:29 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.9738
2022-07-09 22:15:03 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.0993
2022-07-09 22:15:36 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.8422
2022-07-09 22:16:09 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.1925
2022-07-09 22:16:41 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.9777
2022-07-09 22:17:14 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.7072
2022-07-09 22:17:48 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.7116
2022-07-09 22:18:21 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.7194
2022-07-09 22:18:54 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.8908
2022-07-09 22:19:26 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.5485
2022-07-09 22:20:00 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.9509
2022-07-09 22:20:32 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.8366
2022-07-09 22:21:06 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.7954
2022-07-09 22:21:39 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.7680
2022-07-09 22:22:12 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.8369
2022-07-09 22:22:46 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.8154
2022-07-09 22:23:18 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.2258
2022-07-09 22:23:52 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.8238
2022-07-09 22:24:25 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 2.0452
2022-07-09 22:24:58 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.8864
2022-07-09 22:25:31 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.8376
2022-07-09 22:26:05 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.5992
2022-07-09 22:26:38 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.8785
2022-07-09 22:27:11 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.8529
2022-07-09 22:27:44 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.8288
2022-07-09 22:28:17 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.7364
2022-07-09 22:28:51 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.9973
2022-07-09 22:29:24 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.8360
2022-07-09 22:29:57 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.7969
2022-07-09 22:30:30 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.8704
2022-07-09 22:31:04 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.9650
2022-07-09 22:31:37 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.8638
2022-07-09 22:32:10 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.8759
2022-07-09 22:32:43 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.8191
2022-07-09 22:33:17 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.9327
2022-07-09 22:33:50 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.9116
2022-07-09 22:34:23 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.8796
2022-07-09 22:34:56 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 2.0373
2022-07-09 22:35:29 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.7873
2022-07-09 22:36:02 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.8308
2022-07-09 22:36:36 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.8483
2022-07-09 22:37:09 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.9580
2022-07-09 22:37:43 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.9007
2022-07-09 22:38:14 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.7801
2022-07-09 22:38:15 - train: epoch 051, train_loss: 1.8567
2022-07-09 22:39:27 - eval: epoch: 051, acc1: 60.674%, acc5: 83.516%, test_loss: 1.6450, per_image_load_time: 2.645ms, per_image_inference_time: 0.162ms
2022-07-09 22:39:27 - until epoch: 051, best_acc1: 62.214%
2022-07-09 22:39:27 - epoch 052 lr: 0.010000
2022-07-09 22:40:06 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.8076
2022-07-09 22:40:39 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.8981
2022-07-09 22:41:10 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.7595
2022-07-09 22:41:44 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.9153
2022-07-09 22:42:17 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.8024
2022-07-09 22:42:49 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.8696
2022-07-09 22:43:21 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.9678
2022-07-09 22:43:53 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.8035
2022-07-09 22:44:26 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.7910
2022-07-09 22:44:59 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.8147
2022-07-09 22:45:32 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.8123
2022-07-09 22:46:05 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.8604
2022-07-09 22:46:37 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.6355
2022-07-09 22:47:12 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.8941
2022-07-09 22:47:44 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.8119
2022-07-09 22:48:18 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.7232
2022-07-09 22:48:50 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.7679
2022-07-09 22:49:24 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.7353
2022-07-09 22:49:56 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.6611
2022-07-09 22:50:29 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.0619
2022-07-09 22:51:02 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.6972
2022-07-09 22:51:35 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.0364
2022-07-09 22:52:09 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.8296
2022-07-09 22:52:43 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.7192
2022-07-09 22:53:15 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.7109
2022-07-09 22:53:49 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.7408
2022-07-09 22:54:21 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.8665
2022-07-09 22:54:55 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.9708
2022-07-09 22:55:27 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.6480
2022-07-09 22:56:01 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.7320
2022-07-09 22:56:35 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.8772
2022-07-09 22:57:07 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.8793
2022-07-09 22:57:41 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.8894
2022-07-09 22:58:14 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.9428
2022-07-09 22:58:47 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.8782
2022-07-09 22:59:20 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.0100
2022-07-09 22:59:54 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.8731
2022-07-09 23:00:27 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.8576
2022-07-09 23:01:00 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.5444
2022-07-09 23:01:33 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.9208
2022-07-09 23:02:07 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.6887
2022-07-09 23:02:40 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.8837
2022-07-09 23:03:13 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.8879
2022-07-09 23:03:46 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.8216
2022-07-09 23:04:19 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.7865
2022-07-09 23:04:52 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.9379
2022-07-09 23:05:25 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.1152
2022-07-09 23:05:59 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.7109
2022-07-09 23:06:32 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.8140
2022-07-09 23:07:04 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.9124
2022-07-09 23:07:04 - train: epoch 052, train_loss: 1.8530
2022-07-09 23:08:18 - eval: epoch: 052, acc1: 61.528%, acc5: 84.064%, test_loss: 1.6025, per_image_load_time: 2.643ms, per_image_inference_time: 0.165ms
2022-07-09 23:08:18 - until epoch: 052, best_acc1: 62.214%
2022-07-09 23:08:18 - epoch 053 lr: 0.010000
2022-07-09 23:08:56 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.9262
2022-07-09 23:09:29 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 2.0671
2022-07-09 23:10:01 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 2.0843
2022-07-09 23:10:34 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.9086
2022-07-09 23:11:07 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.8836
2022-07-09 23:11:40 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.7851
2022-07-09 23:12:12 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.6777
2022-07-09 23:12:46 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.8838
2022-07-09 23:13:18 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.6714
2022-07-09 23:13:51 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.7642
2022-07-09 23:14:24 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 2.0360
2022-07-09 23:14:58 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.6219
2022-07-09 23:15:30 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.8149
2022-07-09 23:16:04 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.9611
2022-07-09 23:16:36 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.8252
2022-07-09 23:17:09 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.0107
2022-07-09 23:17:43 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.9721
2022-07-09 23:18:16 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.1527
2022-07-09 23:18:50 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.8596
2022-07-09 23:19:22 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.8105
2022-07-09 23:19:56 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.8731
2022-07-09 23:20:30 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.8794
2022-07-09 23:21:02 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.9218
2022-07-09 23:21:36 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.6728
2022-07-09 23:22:10 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.0606
2022-07-09 23:22:42 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.2675
2022-07-09 23:23:15 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.8957
2022-07-09 23:23:48 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.0090
2022-07-09 23:24:22 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.7969
2022-07-09 23:24:54 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.8810
2022-07-09 23:25:28 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.0035
2022-07-09 23:26:01 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.9825
2022-07-09 23:26:35 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.7667
2022-07-09 23:27:07 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.6960
2022-07-09 23:27:40 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.8458
2022-07-09 23:28:14 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.9406
2022-07-09 23:28:46 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.8936
2022-07-09 23:29:20 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.8822
2022-07-09 23:29:54 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.1226
2022-07-09 23:30:27 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.8895
2022-07-09 23:31:00 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.9794
2022-07-09 23:31:34 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.9383
2022-07-09 23:32:07 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.0256
