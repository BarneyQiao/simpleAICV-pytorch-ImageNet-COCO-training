2022-02-21 05:11:27 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.8975
2022-02-21 05:12:01 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.5751
2022-02-21 05:12:34 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.9873
2022-02-21 05:13:06 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.8434
2022-02-21 05:13:40 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.9667
2022-02-21 05:14:13 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.9729
2022-02-21 05:14:45 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.8179
2022-02-21 05:15:19 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.9706
2022-02-21 05:15:52 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.1139
2022-02-21 05:16:24 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.2129
2022-02-21 05:16:58 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.0248
2022-02-21 05:17:31 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.8550
2022-02-21 05:18:05 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.8934
2022-02-21 05:18:38 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.7696
2022-02-21 05:19:12 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.8314
2022-02-21 05:19:44 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.8941
2022-02-21 05:20:17 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.8442
2022-02-21 05:20:50 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.9788
2022-02-21 05:21:24 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.7186
2022-02-21 05:21:57 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.7020
2022-02-21 05:22:29 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.0224
2022-02-21 05:23:03 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.8278
2022-02-21 05:23:35 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.8865
2022-02-21 05:24:09 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.9688
2022-02-21 05:24:41 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.8495
2022-02-21 05:25:15 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.8769
2022-02-21 05:25:48 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.8886
2022-02-21 05:26:21 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.9687
2022-02-21 05:26:54 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.6312
2022-02-21 05:27:28 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.6586
2022-02-21 05:27:59 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.7197
2022-02-21 05:28:01 - train: epoch 050, train_loss: 1.9146
2022-02-21 05:29:16 - eval: epoch: 050, acc1: 60.654%, acc5: 83.592%, test_loss: 1.6392, per_image_load_time: 1.612ms, per_image_inference_time: 0.170ms
2022-02-21 05:29:16 - until epoch: 050, best_acc1: 61.478%
2022-02-21 05:29:16 - epoch 051 lr: 0.010000000000000002
2022-02-21 05:29:55 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.1471
2022-02-21 05:30:29 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.2464
2022-02-21 05:31:02 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 2.0425
2022-02-21 05:31:35 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.6130
2022-02-21 05:32:09 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.0051
2022-02-21 05:32:42 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.7981
2022-02-21 05:33:15 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.8744
2022-02-21 05:33:48 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.1292
2022-02-21 05:34:21 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.8803
2022-02-21 05:34:54 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.1399
2022-02-21 05:35:28 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.9625
2022-02-21 05:36:02 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.9208
2022-02-21 05:36:35 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.6657
2022-02-21 05:37:07 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.7991
2022-02-21 05:37:40 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.8689
2022-02-21 05:38:14 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.8371
2022-02-21 05:38:47 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.9988
2022-02-21 05:39:20 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.8291
2022-02-21 05:39:53 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.6225
2022-02-21 05:40:26 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.9402
2022-02-21 05:40:59 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.8789
2022-02-21 05:41:32 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.9087
2022-02-21 05:42:05 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.1507
2022-02-21 05:42:39 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 2.0457
2022-02-21 05:43:12 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.9584
2022-02-21 05:43:46 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.7609
2022-02-21 05:44:19 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.9874
2022-02-21 05:44:52 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.8132
2022-02-21 05:45:25 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.8790
2022-02-21 05:45:58 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.8198
2022-02-21 05:46:32 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.8231
2022-02-21 05:47:05 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.9147
2022-02-21 05:47:38 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.0681
2022-02-21 05:48:10 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.9473
2022-02-21 05:48:44 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.8726
2022-02-21 05:49:18 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.8147
2022-02-21 05:49:50 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.9690
2022-02-21 05:50:23 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.9644
2022-02-21 05:50:56 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.0612
2022-02-21 05:51:30 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.9037
2022-02-21 05:52:02 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 2.0914
2022-02-21 05:52:35 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.0878
2022-02-21 05:53:08 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.7671
2022-02-21 05:53:42 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.9048
2022-02-21 05:54:14 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.7765
2022-02-21 05:54:49 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.0128
2022-02-21 05:55:21 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 2.0464
2022-02-21 05:55:54 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.8213
2022-02-21 05:56:26 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.0157
2022-02-21 05:56:59 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.8656
2022-02-21 05:57:01 - train: epoch 051, train_loss: 1.9145
2022-02-21 05:58:16 - eval: epoch: 051, acc1: 60.772%, acc5: 83.420%, test_loss: 1.6456, per_image_load_time: 2.622ms, per_image_inference_time: 0.189ms
2022-02-21 05:58:16 - until epoch: 051, best_acc1: 61.478%
2022-02-21 05:58:16 - epoch 052 lr: 0.010000000000000002
2022-02-21 05:58:54 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.7875
2022-02-21 05:59:28 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.9076
2022-02-21 06:00:01 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.9780
2022-02-21 06:00:34 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.9396
2022-02-21 06:01:08 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.9577
2022-02-21 06:01:41 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.8223
2022-02-21 06:02:13 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.8574
2022-02-21 06:02:46 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.8965
2022-02-21 06:03:20 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.9465
2022-02-21 06:03:52 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.1039
2022-02-21 06:04:26 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.8563
2022-02-21 06:04:59 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.7264
2022-02-21 06:05:32 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.8294
2022-02-21 06:06:06 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.2277
2022-02-21 06:06:39 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.7696
2022-02-21 06:07:12 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.7259
2022-02-21 06:07:46 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.6643
2022-02-21 06:08:18 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.7461
2022-02-21 06:08:52 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.8709
2022-02-21 06:09:25 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.0880
2022-02-21 06:09:59 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.7382
2022-02-21 06:10:31 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.9579
2022-02-21 06:11:05 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.7794
2022-02-21 06:11:38 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.7738
2022-02-21 06:12:11 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.6804
2022-02-21 06:12:44 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.6849
2022-02-21 06:13:16 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.8009
2022-02-21 06:13:49 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.9558
2022-02-21 06:14:22 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.7286
2022-02-21 06:14:55 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 2.0058
2022-02-21 06:15:29 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.9845
2022-02-21 06:16:01 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.9396
2022-02-21 06:16:35 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.9383
2022-02-21 06:17:07 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.0633
2022-02-21 06:17:41 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.0028
2022-02-21 06:18:13 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.9390
2022-02-21 06:18:46 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 2.1868
2022-02-21 06:19:19 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.8801
2022-02-21 06:19:52 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.9565
2022-02-21 06:20:25 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.9774
2022-02-21 06:20:59 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.6727
2022-02-21 06:21:32 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.8454
2022-02-21 06:22:05 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.8989
2022-02-21 06:22:39 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.9097
2022-02-21 06:23:12 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.7285
2022-02-21 06:23:46 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.9660
2022-02-21 06:24:17 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.9759
2022-02-21 06:24:50 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.7445
2022-02-21 06:25:24 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.7577
2022-02-21 06:25:56 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.8628
2022-02-21 06:25:59 - train: epoch 052, train_loss: 1.9129
2022-02-21 06:27:14 - eval: epoch: 052, acc1: 60.536%, acc5: 83.570%, test_loss: 1.6447, per_image_load_time: 2.201ms, per_image_inference_time: 0.185ms
2022-02-21 06:27:14 - until epoch: 052, best_acc1: 61.478%
2022-02-21 06:27:14 - epoch 053 lr: 0.010000000000000002
2022-02-21 06:27:52 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.9089
2022-02-21 06:28:26 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 2.0619
2022-02-21 06:28:59 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.9221
2022-02-21 06:29:33 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.0331
2022-02-21 06:30:05 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 2.1098
2022-02-21 06:30:38 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 2.0116
2022-02-21 06:31:11 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.8398
2022-02-21 06:31:44 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.9821
2022-02-21 06:32:18 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.7626
2022-02-21 06:32:51 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.7807
2022-02-21 06:33:24 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.8729
2022-02-21 06:33:57 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.8669
2022-02-21 06:34:31 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.9822
2022-02-21 06:35:03 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.1777
2022-02-21 06:35:37 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.9011
2022-02-21 06:36:09 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.2155
2022-02-21 06:36:43 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.1045
2022-02-21 06:37:16 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.9565
2022-02-21 06:37:49 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.7849
2022-02-21 06:38:22 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.9141
2022-02-21 06:38:55 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.9400
2022-02-21 06:39:29 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.8455
2022-02-21 06:40:02 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.9408
2022-02-21 06:40:36 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.8257
2022-02-21 06:41:08 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.2322
2022-02-21 06:41:40 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.9745
2022-02-21 06:42:14 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 2.0710
2022-02-21 06:42:47 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.0183
2022-02-21 06:43:21 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.6944
2022-02-21 06:43:54 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.7017
2022-02-21 06:44:27 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.0623
2022-02-21 06:44:59 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.2037
2022-02-21 06:45:32 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.8082
2022-02-21 06:46:05 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.9977
2022-02-21 06:46:38 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.9449
2022-02-21 06:47:10 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.9502
2022-02-21 06:47:45 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.1125
2022-02-21 06:48:17 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.8416
2022-02-21 06:48:51 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.0341
2022-02-21 06:49:23 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.9166
2022-02-21 06:49:57 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.9691
2022-02-21 06:50:29 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.9479
2022-02-21 06:51:03 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.2193
2022-02-21 06:51:36 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.8349
2022-02-21 06:52:09 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.1165
2022-02-21 06:52:42 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.8433
2022-02-21 06:53:16 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 2.1824
2022-02-21 06:53:49 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.1381
2022-02-21 06:54:22 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.7685
2022-02-21 06:54:53 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.8425
2022-02-21 06:54:56 - train: epoch 053, train_loss: 1.9098
2022-02-21 06:56:11 - eval: epoch: 053, acc1: 61.268%, acc5: 83.794%, test_loss: 1.6183, per_image_load_time: 1.034ms, per_image_inference_time: 0.178ms
2022-02-21 06:56:11 - until epoch: 053, best_acc1: 61.478%
2022-02-21 06:56:11 - epoch 054 lr: 0.010000000000000002
2022-02-21 06:56:49 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.7742
2022-02-21 06:57:22 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.2564
2022-02-21 06:57:55 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 2.0089
2022-02-21 06:58:27 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.7304
2022-02-21 06:59:02 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.9202
2022-02-21 06:59:34 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.7748
2022-02-21 07:00:08 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.9963
2022-02-21 07:00:41 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.1148
2022-02-21 07:01:14 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.6764
2022-02-21 07:01:48 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.7421
2022-02-21 07:02:21 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.6927
2022-02-21 07:02:54 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.0279
2022-02-21 07:03:27 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.8314
2022-02-21 07:04:00 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.9371
2022-02-21 07:04:32 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.9144
2022-02-21 07:05:06 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.7047
2022-02-21 07:05:39 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.9578
2022-02-21 07:06:12 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.8558
2022-02-21 07:06:45 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.2616
2022-02-21 07:07:19 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.9343
2022-02-21 07:07:52 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.6765
2022-02-21 07:08:25 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.9401
2022-02-21 07:08:58 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.8708
2022-02-21 07:09:31 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.9009
2022-02-21 07:10:04 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 2.0388
2022-02-21 07:10:36 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.8570
2022-02-21 07:11:10 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 2.0690
2022-02-21 07:11:42 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.2335
2022-02-21 07:12:16 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.7284
2022-02-21 07:12:49 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.0523
2022-02-21 07:13:21 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.8823
2022-02-21 07:13:55 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.1247
2022-02-21 07:14:27 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.8432
2022-02-21 07:15:00 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 2.0160
2022-02-21 07:15:33 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.0510
2022-02-21 07:16:06 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.8258
2022-02-21 07:16:40 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.7618
2022-02-21 07:17:13 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.8682
2022-02-21 07:17:47 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.8745
2022-02-21 07:18:19 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.6804
2022-02-21 07:18:52 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 2.0136
2022-02-21 07:19:25 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.9762
2022-02-21 07:19:58 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.8820
2022-02-21 07:20:31 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.7551
2022-02-21 07:21:04 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.7824
2022-02-21 07:21:38 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.8815
2022-02-21 07:22:11 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.1118
2022-02-21 07:22:44 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.0310
2022-02-21 07:23:17 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.7803
2022-02-21 07:23:49 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 2.0027
2022-02-21 07:23:51 - train: epoch 054, train_loss: 1.9132
2022-02-21 07:25:06 - eval: epoch: 054, acc1: 60.420%, acc5: 83.226%, test_loss: 1.6514, per_image_load_time: 1.768ms, per_image_inference_time: 0.171ms
2022-02-21 07:25:06 - until epoch: 054, best_acc1: 61.478%
2022-02-21 07:25:06 - epoch 055 lr: 0.010000000000000002
2022-02-21 07:25:45 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.7903
2022-02-21 07:26:17 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.9039
2022-02-21 07:26:50 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.7048
2022-02-21 07:27:24 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.8692
2022-02-21 07:27:57 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.7466
2022-02-21 07:28:30 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.8686
2022-02-21 07:29:03 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 2.0760
2022-02-21 07:29:37 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.7096
2022-02-21 07:30:10 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.9757
2022-02-21 07:30:43 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.8691
2022-02-21 07:31:16 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.8967
2022-02-21 07:31:48 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.8911
2022-02-21 07:32:21 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.0533
2022-02-21 07:32:55 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.8444
2022-02-21 07:33:27 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.9446
2022-02-21 07:34:02 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 2.0062
2022-02-21 07:34:33 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.9737
2022-02-21 07:35:07 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 2.0814
2022-02-21 07:35:39 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.9249
2022-02-21 07:36:12 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.8549
2022-02-21 07:36:46 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.5142
2022-02-21 07:37:18 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.0220
2022-02-21 07:37:52 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.8571
2022-02-21 07:38:25 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.7055
2022-02-21 07:38:58 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.9173
2022-02-21 07:39:31 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.8290
2022-02-21 07:40:04 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.7282
2022-02-21 07:40:37 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.8974
2022-02-21 07:41:10 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 2.0708
2022-02-21 07:41:44 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.8692
2022-02-21 07:42:16 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.9601
2022-02-21 07:42:50 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.8252
2022-02-21 07:43:22 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.8149
2022-02-21 07:43:56 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.7634
2022-02-21 07:44:28 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.6800
2022-02-21 07:45:02 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.9411
2022-02-21 07:45:36 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.8980
2022-02-21 07:46:09 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.9585
2022-02-21 07:46:42 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.2768
2022-02-21 07:47:15 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.8604
2022-02-21 07:47:48 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.8959
2022-02-21 07:48:20 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.9318
2022-02-21 07:48:54 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.9740
2022-02-21 07:49:26 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.0604
2022-02-21 07:50:00 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.9732
2022-02-21 07:50:33 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.9458
2022-02-21 07:51:07 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.7121
2022-02-21 07:51:41 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.0233
2022-02-21 07:52:14 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.7990
2022-02-21 07:52:46 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.8945
2022-02-21 07:52:48 - train: epoch 055, train_loss: 1.9082
2022-02-21 07:54:04 - eval: epoch: 055, acc1: 60.874%, acc5: 83.822%, test_loss: 1.6219, per_image_load_time: 1.202ms, per_image_inference_time: 0.167ms
2022-02-21 07:54:04 - until epoch: 055, best_acc1: 61.478%
2022-02-21 07:54:04 - epoch 056 lr: 0.010000000000000002
2022-02-21 07:54:41 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.9873
2022-02-21 07:55:15 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.1085
2022-02-21 07:55:48 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.9811
2022-02-21 07:56:20 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.9770
2022-02-21 07:56:55 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.7387
2022-02-21 07:57:27 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.9164
2022-02-21 07:58:01 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.9461
2022-02-21 07:58:34 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.0608
2022-02-21 07:59:07 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.0026
2022-02-21 07:59:40 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.9167
2022-02-21 08:00:14 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.7602
2022-02-21 08:00:47 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.8168
2022-02-21 08:01:20 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.8911
2022-02-21 08:01:53 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.8239
2022-02-21 08:02:27 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.1886
2022-02-21 08:02:59 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.8520
2022-02-21 08:03:32 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.0690
2022-02-21 08:04:04 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.1324
2022-02-21 08:04:38 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.9876
2022-02-21 08:05:10 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.9004
2022-02-21 08:05:44 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.0236
2022-02-21 08:06:17 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.0908
2022-02-21 08:06:49 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.9883
2022-02-21 08:07:23 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.8773
2022-02-21 08:07:57 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.9766
2022-02-21 08:08:30 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.7466
2022-02-21 08:09:03 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.9379
2022-02-21 08:09:36 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.8297
2022-02-21 08:10:10 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.9781
2022-02-21 08:10:44 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.1356
2022-02-21 08:11:15 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.8592
2022-02-21 08:11:49 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.7525
2022-02-21 08:12:23 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 2.1095
2022-02-21 08:12:55 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.8018
2022-02-21 08:13:28 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.8910
2022-02-21 08:14:02 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.6355
2022-02-21 08:14:36 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.7474
2022-02-21 08:15:08 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.7469
2022-02-21 08:15:42 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.2097
2022-02-21 08:16:14 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.8794
2022-02-21 08:16:48 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.0747
2022-02-21 08:17:20 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.8425
2022-02-21 08:17:54 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.7819
2022-02-21 08:18:27 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.0112
2022-02-21 08:19:00 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.9416
2022-02-21 08:19:34 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.9122
2022-02-21 08:20:06 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.9559
2022-02-21 08:20:40 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.1221
2022-02-21 08:21:13 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.8879
2022-02-21 08:21:46 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.9439
2022-02-21 08:21:48 - train: epoch 056, train_loss: 1.9045
2022-02-21 08:23:02 - eval: epoch: 056, acc1: 60.306%, acc5: 83.402%, test_loss: 1.6585, per_image_load_time: 1.862ms, per_image_inference_time: 0.172ms
2022-02-21 08:23:02 - until epoch: 056, best_acc1: 61.478%
2022-02-21 08:23:02 - epoch 057 lr: 0.010000000000000002
2022-02-21 08:23:41 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.9457
2022-02-21 08:24:15 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.7314
2022-02-21 08:24:48 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.7360
2022-02-21 08:25:21 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.7901
2022-02-21 08:25:54 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.6170
2022-02-21 08:26:28 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.0422
2022-02-21 08:27:00 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.7202
2022-02-21 08:27:34 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.8750
2022-02-21 08:28:07 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.8713
2022-02-21 08:28:40 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.7787
2022-02-21 08:29:13 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.8865
2022-02-21 08:29:47 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.7687
2022-02-21 08:30:20 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.9732
2022-02-21 08:30:54 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 2.0959
2022-02-21 08:31:26 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 2.0721
2022-02-21 08:32:00 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.0495
2022-02-21 08:32:34 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 2.1223
2022-02-21 08:33:06 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.9645
2022-02-21 08:33:39 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.8213
2022-02-21 08:34:12 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.8938
2022-02-21 08:34:46 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.9542
2022-02-21 08:35:18 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 2.0559
2022-02-21 08:35:52 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.9351
2022-02-21 08:36:25 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.7966
2022-02-21 08:36:59 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.9962
2022-02-21 08:37:32 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.7205
2022-02-21 08:38:06 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.6713
2022-02-21 08:38:40 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.5123
2022-02-21 08:39:12 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 2.0972
2022-02-21 08:39:45 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.9909
2022-02-21 08:40:19 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.2015
2022-02-21 08:40:51 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.1265
2022-02-21 08:41:24 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.8962
2022-02-21 08:41:57 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.9493
2022-02-21 08:42:31 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.9758
2022-02-21 08:43:04 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.8588
2022-02-21 08:43:38 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.8592
2022-02-21 08:44:10 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.8964
2022-02-21 08:44:44 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.9952
2022-02-21 08:45:16 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.8475
2022-02-21 08:45:49 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.9530
2022-02-21 08:46:23 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.9693
2022-02-21 08:46:56 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.7087
2022-02-21 08:47:29 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.8543
2022-02-21 08:48:02 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.1209
2022-02-21 08:48:36 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 2.0445
2022-02-21 08:49:10 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.7018
2022-02-21 08:49:42 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.2228
2022-02-21 08:50:16 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.1071
2022-02-21 08:50:47 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 2.0133
2022-02-21 08:50:49 - train: epoch 057, train_loss: 1.9057
2022-02-21 08:52:04 - eval: epoch: 057, acc1: 60.702%, acc5: 83.362%, test_loss: 1.6410, per_image_load_time: 2.572ms, per_image_inference_time: 0.171ms
2022-02-21 08:52:04 - until epoch: 057, best_acc1: 61.478%
2022-02-21 08:52:04 - epoch 058 lr: 0.010000000000000002
2022-02-21 08:52:43 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.9803
2022-02-21 08:53:16 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.7038
2022-02-21 08:53:49 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.9499
2022-02-21 08:54:22 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.9049
2022-02-21 08:54:55 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.7216
2022-02-21 08:55:28 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.1067
2022-02-21 08:56:02 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.7862
2022-02-21 08:56:34 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.7187
2022-02-21 08:57:07 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.6594
2022-02-21 08:57:40 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 2.0440
2022-02-21 08:58:14 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.6980
2022-02-21 08:58:47 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.8498
2022-02-21 08:59:20 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.1041
2022-02-21 08:59:53 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.9943
2022-02-21 09:00:27 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.8723
2022-02-21 09:00:59 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.7735
2022-02-21 09:01:33 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.0138
2022-02-21 09:02:05 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.9268
2022-02-21 09:02:38 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.0901
2022-02-21 09:03:12 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.0953
2022-02-21 09:03:44 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.7466
2022-02-21 09:04:18 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.6564
2022-02-21 09:04:51 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.7921
2022-02-21 09:05:25 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.9386
2022-02-21 09:05:58 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.9470
2022-02-21 09:06:31 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.6968
2022-02-21 09:07:04 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.0169
2022-02-21 09:07:37 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.5662
2022-02-21 09:08:10 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.7011
2022-02-21 09:08:44 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.1445
2022-02-21 09:09:16 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.7702
2022-02-21 09:09:49 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.6615
2022-02-21 09:10:21 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.9308
2022-02-21 09:10:55 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.7664
2022-02-21 09:11:27 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.8500
2022-02-21 09:12:00 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.8152
2022-02-21 09:12:33 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.8071
2022-02-21 09:13:06 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.0046
2022-02-21 09:13:39 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.9835
2022-02-21 09:14:12 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.0316
2022-02-21 09:14:46 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.9112
2022-02-21 09:15:18 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.6377
2022-02-21 09:15:52 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 2.1277
2022-02-21 09:16:24 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.6810
2022-02-21 09:16:58 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 2.0004
2022-02-21 09:17:30 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.7511
2022-02-21 09:18:04 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 2.0557
2022-02-21 09:18:36 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 2.1540
2022-02-21 09:19:10 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.7938
2022-02-21 09:19:42 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.8168
2022-02-21 09:19:44 - train: epoch 058, train_loss: 1.9019
2022-02-21 09:20:59 - eval: epoch: 058, acc1: 60.844%, acc5: 83.848%, test_loss: 1.6280, per_image_load_time: 1.733ms, per_image_inference_time: 0.177ms
2022-02-21 09:20:59 - until epoch: 058, best_acc1: 61.478%
2022-02-21 09:20:59 - epoch 059 lr: 0.010000000000000002
2022-02-21 09:21:37 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.0177
2022-02-21 09:22:10 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.8229
2022-02-21 09:22:44 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.9153
2022-02-21 09:23:17 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 2.0521
2022-02-21 09:23:51 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 2.1246
2022-02-21 09:24:23 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.8473
2022-02-21 09:24:57 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.8072
2022-02-21 09:25:29 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.9858
2022-02-21 09:26:03 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.8425
2022-02-21 09:26:37 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.9102
2022-02-21 09:27:11 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.1189
2022-02-21 09:27:43 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.5259
2022-02-21 09:28:17 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.9804
2022-02-21 09:28:50 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.1223
2022-02-21 09:29:24 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.9152
2022-02-21 09:29:57 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.7854
2022-02-21 09:30:30 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 2.0372
2022-02-21 09:31:02 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.8244
2022-02-21 09:31:35 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.9555
2022-02-21 09:32:09 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.8449
2022-02-21 09:32:43 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.1229
2022-02-21 09:33:15 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.9848
2022-02-21 09:33:49 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.8169
2022-02-21 09:34:22 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.9789
2022-02-21 09:34:55 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.9130
2022-02-21 09:35:29 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.8113
2022-02-21 09:36:01 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.9735
2022-02-21 09:36:35 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.0259
2022-02-21 09:37:08 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.8668
2022-02-21 09:37:41 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.4121
2022-02-21 09:38:14 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.7763
2022-02-21 09:38:46 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.8992
2022-02-21 09:39:20 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.9728
2022-02-21 09:39:54 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.3227
2022-02-21 09:40:27 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.6807
2022-02-21 09:40:59 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.8772
2022-02-21 09:41:32 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.7130
2022-02-21 09:42:05 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.9244
2022-02-21 09:42:38 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.7566
2022-02-21 09:43:11 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.2353
2022-02-21 09:43:45 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.8939
2022-02-21 09:44:18 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.0388
2022-02-21 09:44:51 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.0952
2022-02-21 09:45:24 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.0820
2022-02-21 09:45:58 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 2.0136
2022-02-21 09:46:30 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.9314
2022-02-21 09:47:04 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.8243
2022-02-21 09:47:37 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.7884
2022-02-21 09:48:11 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 2.0337
2022-02-21 09:48:43 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.0081
2022-02-21 09:48:45 - train: epoch 059, train_loss: 1.9022
2022-02-21 09:50:00 - eval: epoch: 059, acc1: 60.696%, acc5: 83.502%, test_loss: 1.6427, per_image_load_time: 2.695ms, per_image_inference_time: 0.175ms
2022-02-21 09:50:00 - until epoch: 059, best_acc1: 61.478%
2022-02-21 09:50:00 - epoch 060 lr: 0.010000000000000002
2022-02-21 09:50:39 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.7513
2022-02-21 09:51:12 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.9330
2022-02-21 09:51:45 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.9111
2022-02-21 09:52:17 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.9774
2022-02-21 09:52:51 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.0304
2022-02-21 09:53:22 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.1248
2022-02-21 09:53:56 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.9702
2022-02-21 09:54:29 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.1284
2022-02-21 09:55:03 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.6231
2022-02-21 09:55:36 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.5109
2022-02-21 09:56:08 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.5836
2022-02-21 09:56:43 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.8057
2022-02-21 09:57:15 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.7846
2022-02-21 09:57:48 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.8802
2022-02-21 09:58:21 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.9600
2022-02-21 09:58:54 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.9718
2022-02-21 09:59:28 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.9937
2022-02-21 10:00:01 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.9298
2022-02-21 10:00:34 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.1275
2022-02-21 10:01:07 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.7619
2022-02-21 10:01:41 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.0743
2022-02-21 10:02:14 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.0486
2022-02-21 10:02:48 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.7011
2022-02-21 10:03:20 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.8161
2022-02-21 10:03:53 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.9369
2022-02-21 10:04:26 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.9976
2022-02-21 10:04:59 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.7561
2022-02-21 10:05:32 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.8832
2022-02-21 10:06:05 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.9817
2022-02-21 10:06:38 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.1325
2022-02-21 10:07:12 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.0299
2022-02-21 10:07:44 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.8540
2022-02-21 10:08:17 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.7624
2022-02-21 10:08:49 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.9731
2022-02-21 10:09:22 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.9349
2022-02-21 10:09:55 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.0141
2022-02-21 10:10:29 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.9399
2022-02-21 10:11:02 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.8844
2022-02-21 10:11:35 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 2.2041
2022-02-21 10:12:08 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.8801
2022-02-21 10:12:42 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.0722
2022-02-21 10:13:14 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.0080
2022-02-21 10:13:48 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.9498
2022-02-21 10:14:21 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.0356
2022-02-21 10:14:54 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.8460
2022-02-21 10:15:27 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.8074
2022-02-21 10:16:00 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.9088
2022-02-21 10:16:34 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.7274
2022-02-21 10:17:07 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.9027
2022-02-21 10:17:39 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.9116
2022-02-21 10:17:41 - train: epoch 060, train_loss: 1.9002
2022-02-21 10:18:57 - eval: epoch: 060, acc1: 60.290%, acc5: 83.584%, test_loss: 1.6465, per_image_load_time: 1.966ms, per_image_inference_time: 0.174ms
2022-02-21 10:18:57 - until epoch: 060, best_acc1: 61.478%
2022-02-21 10:18:57 - epoch 061 lr: 0.0010000000000000002
2022-02-21 10:19:35 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.6937
2022-02-21 10:20:09 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.8513
2022-02-21 10:20:41 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.5316
2022-02-21 10:21:14 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.0623
2022-02-21 10:21:48 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.7468
2022-02-21 10:22:20 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.8877
2022-02-21 10:22:54 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.5154
2022-02-21 10:23:26 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.7821
2022-02-21 10:24:00 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.6702
2022-02-21 10:24:33 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.5464
2022-02-21 10:25:05 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.5855
2022-02-21 10:25:40 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.6669
2022-02-21 10:26:12 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.6371
2022-02-21 10:26:45 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.6599
2022-02-21 10:27:19 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.7560
2022-02-21 10:27:51 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.4705
2022-02-21 10:28:25 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.7272
2022-02-21 10:28:57 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.6785
2022-02-21 10:29:31 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.8611
2022-02-21 10:30:03 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.6038
2022-02-21 10:30:36 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 2.0814
2022-02-21 10:31:09 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.6365
2022-02-21 10:31:41 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.5893
2022-02-21 10:32:14 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.4513
2022-02-21 10:32:47 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.5721
2022-02-21 10:33:21 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.7892
2022-02-21 10:33:53 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.7012
2022-02-21 10:34:26 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.7416
2022-02-21 10:34:59 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.8083
2022-02-21 10:35:33 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.6728
2022-02-21 10:36:06 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.8524
2022-02-21 10:36:39 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.7535
2022-02-21 10:37:12 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.7014
2022-02-21 10:37:45 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.5475
2022-02-21 10:38:19 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.7642
2022-02-21 10:38:52 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.8402
2022-02-21 10:39:25 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.8102
2022-02-21 10:39:58 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.6882
2022-02-21 10:40:31 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.6203
2022-02-21 10:41:04 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.7271
2022-02-21 10:41:38 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.8886
2022-02-21 10:42:11 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.6043
2022-02-21 10:42:45 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.7899
2022-02-21 10:43:17 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.7596
2022-02-21 10:43:51 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.6722
2022-02-21 10:44:24 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.7527
2022-02-21 10:44:57 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.5956
2022-02-21 10:45:30 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.7166
2022-02-21 10:46:04 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.7355
2022-02-21 10:46:36 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.7730
2022-02-21 10:46:38 - train: epoch 061, train_loss: 1.7063
2022-02-21 10:47:54 - eval: epoch: 061, acc1: 65.302%, acc5: 86.448%, test_loss: 1.4240, per_image_load_time: 2.358ms, per_image_inference_time: 0.154ms
2022-02-21 10:47:54 - until epoch: 061, best_acc1: 65.302%
2022-02-21 10:47:54 - epoch 062 lr: 0.0010000000000000002
2022-02-21 10:48:32 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.7419
2022-02-21 10:49:05 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.8751
2022-02-21 10:49:38 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.6463
2022-02-21 10:50:11 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.4843
2022-02-21 10:50:44 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.5800
2022-02-21 10:51:17 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.5177
2022-02-21 10:51:49 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.9177
2022-02-21 10:52:23 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.6663
2022-02-21 10:52:56 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.6544
2022-02-21 10:53:28 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.7840
2022-02-21 10:54:01 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.6329
2022-02-21 10:54:34 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 2.0324
2022-02-21 10:55:07 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.7963
2022-02-21 10:55:41 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.6282
2022-02-21 10:56:14 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.6650
2022-02-21 10:56:47 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.8289
2022-02-21 10:57:20 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.7459
2022-02-21 10:57:52 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.4979
2022-02-21 10:58:27 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.6485
2022-02-21 10:58:58 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.5901
2022-02-21 10:59:31 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.7330
2022-02-21 11:00:03 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.5168
2022-02-21 11:00:36 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.6158
2022-02-21 11:01:08 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.6552
2022-02-21 11:01:41 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.7589
2022-02-21 11:02:13 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.4265
2022-02-21 11:02:46 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.6986
2022-02-21 11:03:19 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.7764
2022-02-21 11:03:51 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.7402
2022-02-21 11:04:24 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.8902
2022-02-21 11:04:57 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.8370
2022-02-21 11:05:29 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.4691
2022-02-21 11:06:01 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.6991
2022-02-21 11:06:34 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.7448
2022-02-21 11:07:06 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.6853
2022-02-21 11:07:39 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.6890
2022-02-21 11:08:11 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.6454
2022-02-21 11:08:44 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.6572
2022-02-21 11:09:16 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.6531
2022-02-21 11:09:49 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.3863
2022-02-21 11:10:21 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.7785
2022-02-21 11:10:54 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.4213
2022-02-21 11:11:25 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.7232
2022-02-21 11:11:57 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.7049
2022-02-21 11:12:30 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.5469
2022-02-21 11:13:01 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.4998
2022-02-21 11:13:35 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.6721
2022-02-21 11:14:08 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.6607
2022-02-21 11:14:39 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.6085
2022-02-21 11:15:12 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.7068
2022-02-21 11:15:14 - train: epoch 062, train_loss: 1.6618
2022-02-21 11:16:27 - eval: epoch: 062, acc1: 65.818%, acc5: 86.732%, test_loss: 1.4052, per_image_load_time: 2.281ms, per_image_inference_time: 0.188ms
2022-02-21 11:16:28 - until epoch: 062, best_acc1: 65.818%
2022-02-21 11:16:28 - epoch 063 lr: 0.0010000000000000002
2022-02-21 11:17:05 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.6937
2022-02-21 11:17:37 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.4827
2022-02-21 11:18:10 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.7572
2022-02-21 11:18:42 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.5850
2022-02-21 11:19:14 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.5443
2022-02-21 11:19:47 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.7725
2022-02-21 11:20:20 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.6059
2022-02-21 11:20:51 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.8420
2022-02-21 11:21:25 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.6881
2022-02-21 11:21:56 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.7513
2022-02-21 11:22:29 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.5555
2022-02-21 11:23:01 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.7153
2022-02-21 11:23:34 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.4871
2022-02-21 11:24:06 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.9445
2022-02-21 11:24:39 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.7015
2022-02-21 11:25:11 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.5365
2022-02-21 11:25:43 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.5040
2022-02-21 11:26:16 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.7555
2022-02-21 11:26:48 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.8650
2022-02-21 11:27:21 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.6194
2022-02-21 11:27:52 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.5435
2022-02-21 11:28:26 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.9781
2022-02-21 11:28:57 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.9196
2022-02-21 11:29:30 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.8509
2022-02-21 11:30:03 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.6954
2022-02-21 11:30:36 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.4709
2022-02-21 11:31:08 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.8690
2022-02-21 11:31:43 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.7872
2022-02-21 11:32:15 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.6887
2022-02-21 11:32:49 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.7751
2022-02-21 11:33:22 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 2.0414
2022-02-21 11:33:56 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.7159
2022-02-21 11:34:28 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.5769
2022-02-21 11:35:01 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.4318
2022-02-21 11:35:35 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.7655
2022-02-21 11:36:07 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.5521
2022-02-21 11:36:41 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.7370
2022-02-21 11:37:14 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.7987
2022-02-21 11:37:48 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.3744
2022-02-21 11:38:21 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.3446
2022-02-21 11:38:54 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.7212
2022-02-21 11:39:28 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.5806
2022-02-21 11:40:00 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.6892
2022-02-21 11:40:33 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.7131
2022-02-21 11:41:06 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.5286
2022-02-21 11:41:40 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.6524
2022-02-21 11:42:13 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.5431
2022-02-21 11:42:47 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.6065
2022-02-21 11:43:20 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.5182
2022-02-21 11:43:52 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.6108
2022-02-21 11:43:54 - train: epoch 063, train_loss: 1.6447
2022-02-21 11:45:09 - eval: epoch: 063, acc1: 66.098%, acc5: 86.806%, test_loss: 1.4002, per_image_load_time: 1.715ms, per_image_inference_time: 0.146ms
2022-02-21 11:45:09 - until epoch: 063, best_acc1: 66.098%
2022-02-21 11:45:09 - epoch 064 lr: 0.0010000000000000002
2022-02-21 11:45:47 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.5179
2022-02-21 11:46:21 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.6484
2022-02-21 11:46:54 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.4295
2022-02-21 11:47:27 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.7494
2022-02-21 11:48:00 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.4611
2022-02-21 11:48:34 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.7787
2022-02-21 11:49:07 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.7423
2022-02-21 11:49:40 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.4526
2022-02-21 11:50:13 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.7001
2022-02-21 11:50:47 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.6249
2022-02-21 11:51:20 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.5841
2022-02-21 11:51:53 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.4516
2022-02-21 11:52:27 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.6880
2022-02-21 11:53:00 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.9374
2022-02-21 11:53:34 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.6680
2022-02-21 11:54:06 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.5664
2022-02-21 11:54:39 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.4751
2022-02-21 11:55:11 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.5027
2022-02-21 11:55:45 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.5951
2022-02-21 11:56:18 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.5171
2022-02-21 11:56:50 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.6626
2022-02-21 11:57:23 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.6735
2022-02-21 11:57:57 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.7987
2022-02-21 11:58:30 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.6379
2022-02-21 11:59:03 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.4996
2022-02-21 11:59:36 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.5446
2022-02-21 12:00:10 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.6430
2022-02-21 12:00:43 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.3932
2022-02-21 12:01:16 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.8288
2022-02-21 12:01:49 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.6917
2022-02-21 12:02:22 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.6631
2022-02-21 12:02:55 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.5721
2022-02-21 12:03:29 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.5058
2022-02-21 12:04:02 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.7630
2022-02-21 12:04:34 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.5315
2022-02-21 12:05:08 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.6172
2022-02-21 12:05:41 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.2131
2022-02-21 12:06:15 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.8876
2022-02-21 12:06:47 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.5368
2022-02-21 12:07:21 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.4377
2022-02-21 12:07:53 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.6764
2022-02-21 12:08:25 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.4359
2022-02-21 12:08:58 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.8723
2022-02-21 12:09:30 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.5616
2022-02-21 12:10:03 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.5679
2022-02-21 12:10:37 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.8732
2022-02-21 12:11:09 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 2.0090
2022-02-21 12:11:43 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.6230
2022-02-21 12:12:16 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.8422
2022-02-21 12:12:49 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.4682
2022-02-21 12:12:51 - train: epoch 064, train_loss: 1.6337
2022-02-21 12:14:06 - eval: epoch: 064, acc1: 66.026%, acc5: 86.878%, test_loss: 1.3920, per_image_load_time: 0.969ms, per_image_inference_time: 0.157ms
2022-02-21 12:14:06 - until epoch: 064, best_acc1: 66.098%
2022-02-21 12:14:06 - epoch 065 lr: 0.0010000000000000002
2022-02-21 12:14:43 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.6832
2022-02-21 12:15:16 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.5147
2022-02-21 12:15:49 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.6074
2022-02-21 12:16:22 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.6432
2022-02-21 12:16:55 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.6574
2022-02-21 12:17:29 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.7437
2022-02-21 12:18:02 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.6070
2022-02-21 12:18:35 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.6048
2022-02-21 12:19:09 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.5244
2022-02-21 12:19:41 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.4965
2022-02-21 12:20:14 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.5250
2022-02-21 12:20:48 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.9260
2022-02-21 12:21:21 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.6105
2022-02-21 12:21:54 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.4363
2022-02-21 12:22:28 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.5802
2022-02-21 12:23:00 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.6504
2022-02-21 12:23:33 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.5349
2022-02-21 12:24:06 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.4198
2022-02-21 12:24:41 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.4755
2022-02-21 12:25:12 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.6352
2022-02-21 12:25:46 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.5497
2022-02-21 12:26:19 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.7605
2022-02-21 12:26:52 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.5202
2022-02-21 12:27:25 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.6330
2022-02-21 12:27:58 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.6620
2022-02-21 12:28:31 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.8679
2022-02-21 12:29:05 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.6549
2022-02-21 12:29:38 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.4937
2022-02-21 12:30:11 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.6571
2022-02-21 12:30:44 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.4678
2022-02-21 12:31:16 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.6314
2022-02-21 12:31:50 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.6572
2022-02-21 12:32:23 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.5649
2022-02-21 12:32:56 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.4736
2022-02-21 12:33:29 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.8399
2022-02-21 12:34:03 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.7624
2022-02-21 12:34:35 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.4766
2022-02-21 12:35:08 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.4704
2022-02-21 12:35:41 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.7702
2022-02-21 12:36:14 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.6569
2022-02-21 12:36:46 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.5225
2022-02-21 12:37:20 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.6502
2022-02-21 12:37:53 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.5203
2022-02-21 12:38:26 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.5753
2022-02-21 12:38:59 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.6738
2022-02-21 12:39:32 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.6763
2022-02-21 12:40:05 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.6946
2022-02-21 12:40:39 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.4446
2022-02-21 12:41:12 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.5343
2022-02-21 12:41:45 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.6153
2022-02-21 12:41:46 - train: epoch 065, train_loss: 1.6249
2022-02-21 12:43:01 - eval: epoch: 065, acc1: 66.266%, acc5: 86.982%, test_loss: 1.3881, per_image_load_time: 1.593ms, per_image_inference_time: 0.189ms
2022-02-21 12:43:01 - until epoch: 065, best_acc1: 66.266%
2022-02-21 12:43:01 - epoch 066 lr: 0.0010000000000000002
2022-02-21 12:43:39 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.5797
2022-02-21 12:44:12 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.7863
2022-02-21 12:44:46 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.3943
2022-02-21 12:45:19 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.4234
2022-02-21 12:45:52 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.7139
2022-02-21 12:46:25 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.5460
2022-02-21 12:46:58 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.6004
2022-02-21 12:47:31 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.9717
2022-02-21 12:48:04 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.7631
2022-02-21 12:48:38 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.6765
2022-02-21 12:49:11 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.6366
2022-02-21 12:49:44 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.7985
2022-02-21 12:50:17 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.7485
2022-02-21 12:50:50 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.5294
2022-02-21 12:51:22 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.6435
2022-02-21 12:51:55 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.6789
2022-02-21 12:52:29 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.5628
2022-02-21 12:53:02 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.4148
2022-02-21 12:53:35 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.8074
2022-02-21 12:54:07 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.6273
2022-02-21 12:54:41 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.6727
2022-02-21 12:55:15 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.5465
2022-02-21 12:55:48 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.7383
2022-02-21 12:56:22 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.5081
2022-02-21 12:56:55 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.6055
2022-02-21 12:57:28 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.5844
2022-02-21 12:58:01 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.7818
2022-02-21 12:58:34 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.7051
2022-02-21 12:59:07 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.6066
2022-02-21 12:59:40 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.5496
2022-02-21 13:00:14 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.6815
2022-02-21 13:00:47 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.5344
2022-02-21 13:01:21 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.5148
2022-02-21 13:01:53 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.7941
2022-02-21 13:02:27 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.7335
2022-02-21 13:02:59 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.6880
2022-02-21 13:03:33 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.5514
2022-02-21 13:04:06 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.5440
2022-02-21 13:04:39 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.4438
2022-02-21 13:05:11 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.7788
2022-02-21 13:05:44 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.4386
2022-02-21 13:06:17 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.5627
2022-02-21 13:06:50 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.3905
2022-02-21 13:07:24 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.5900
2022-02-21 13:07:56 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.6473
2022-02-21 13:08:30 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.7546
2022-02-21 13:09:03 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.5782
2022-02-21 13:09:37 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.4669
2022-02-21 13:10:09 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.6540
2022-02-21 13:10:41 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.5284
2022-02-21 13:10:43 - train: epoch 066, train_loss: 1.6168
2022-02-21 13:11:58 - eval: epoch: 066, acc1: 66.324%, acc5: 87.152%, test_loss: 1.3817, per_image_load_time: 1.443ms, per_image_inference_time: 0.185ms
2022-02-21 13:11:58 - until epoch: 066, best_acc1: 66.324%
2022-02-21 13:11:58 - epoch 067 lr: 0.0010000000000000002
2022-02-21 13:12:37 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.5348
2022-02-21 13:13:10 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.5674
2022-02-21 13:13:43 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.8623
2022-02-21 13:14:16 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.5759
2022-02-21 13:14:49 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.5376
2022-02-21 13:15:22 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.5016
2022-02-21 13:15:55 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.6825
2022-02-21 13:16:27 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.6338
2022-02-21 13:17:00 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.8128
2022-02-21 13:17:33 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.4857
2022-02-21 13:18:06 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.5661
2022-02-21 13:18:39 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.5662
2022-02-21 13:19:13 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.8283
2022-02-21 13:19:46 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.6122
2022-02-21 13:20:19 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.5775
2022-02-21 13:20:53 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.7228
2022-02-21 13:21:25 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.5983
2022-02-21 13:21:58 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.8526
2022-02-21 13:22:31 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.6264
2022-02-21 13:23:04 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.8301
2022-02-21 13:23:37 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.4278
2022-02-21 13:24:11 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.5534
2022-02-21 13:24:44 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.5735
2022-02-21 13:25:18 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.5421
2022-02-21 13:25:50 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.5325
2022-02-21 13:26:23 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.5515
2022-02-21 13:26:55 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.4736
2022-02-21 13:27:29 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.7034
2022-02-21 13:28:02 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.5369
2022-02-21 13:28:36 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.5267
2022-02-21 13:29:08 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.4395
2022-02-21 13:29:42 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.8311
2022-02-21 13:30:15 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.4039
2022-02-21 13:30:48 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.7367
2022-02-21 13:31:21 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.5990
2022-02-21 13:31:55 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.5891
2022-02-21 13:32:27 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.7661
2022-02-21 13:33:00 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.6119
2022-02-21 13:33:32 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.7289
2022-02-21 13:34:06 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.8030
2022-02-21 13:34:38 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.7404
2022-02-21 13:35:12 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.8099
2022-02-21 13:35:45 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.3751
2022-02-21 13:36:18 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.7572
2022-02-21 13:36:50 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.4376
2022-02-21 13:37:24 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.3447
2022-02-21 13:37:57 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.5100
2022-02-21 13:38:31 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.3639
2022-02-21 13:39:04 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.6386
2022-02-21 13:39:36 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.6507
2022-02-21 13:39:38 - train: epoch 067, train_loss: 1.6111
2022-02-21 13:40:53 - eval: epoch: 067, acc1: 66.304%, acc5: 87.134%, test_loss: 1.3780, per_image_load_time: 0.895ms, per_image_inference_time: 0.176ms
2022-02-21 13:40:53 - until epoch: 067, best_acc1: 66.324%
2022-02-21 13:40:53 - epoch 068 lr: 0.0010000000000000002
2022-02-21 13:41:31 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.6617
2022-02-21 13:42:05 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.6542
2022-02-21 13:42:38 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.8287
2022-02-21 13:43:11 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.6253
2022-02-21 13:43:44 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.4893
2022-02-21 13:44:17 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.6150
2022-02-21 13:44:50 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.9084
2022-02-21 13:45:24 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.6313
2022-02-21 13:45:57 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.5082
2022-02-21 13:46:29 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.6767
2022-02-21 13:47:02 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.8697
2022-02-21 13:47:35 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.4547
2022-02-21 13:48:09 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.5343
2022-02-21 13:48:42 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.6392
2022-02-21 13:49:16 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.7272
2022-02-21 13:49:48 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.6127
2022-02-21 13:50:22 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.7858
2022-02-21 13:50:54 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.5623
2022-02-21 13:51:27 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.7247
2022-02-21 13:51:59 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.7464
2022-02-21 13:52:33 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.5642
2022-02-21 13:53:06 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.5677
2022-02-21 13:53:40 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.4998
2022-02-21 13:54:12 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.7057
2022-02-21 13:54:45 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.7309
2022-02-21 13:55:18 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.5628
2022-02-21 13:55:51 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.6739
2022-02-21 13:56:25 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.7517
2022-02-21 13:56:57 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.7463
2022-02-21 13:57:30 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.7756
2022-02-21 13:58:02 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.3976
2022-02-21 13:58:36 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.8010
2022-02-21 13:59:09 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.6518
2022-02-21 13:59:42 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.4825
2022-02-21 14:00:15 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.6582
2022-02-21 14:00:48 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.4737
2022-02-21 14:01:20 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.6603
2022-02-21 14:01:53 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.7799
2022-02-21 14:02:26 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.6437
2022-02-21 14:02:59 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.5890
2022-02-21 14:03:33 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.4030
2022-02-21 14:04:07 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.7326
2022-02-21 14:04:39 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.8220
2022-02-21 14:05:12 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.5268
2022-02-21 14:05:45 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.6670
2022-02-21 14:06:18 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.7199
2022-02-21 14:06:50 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.8742
2022-02-21 14:07:24 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.7027
2022-02-21 14:07:57 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.7013
2022-02-21 14:08:29 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.5627
2022-02-21 14:08:31 - train: epoch 068, train_loss: 1.6063
2022-02-21 14:09:46 - eval: epoch: 068, acc1: 66.388%, acc5: 87.198%, test_loss: 1.3735, per_image_load_time: 1.406ms, per_image_inference_time: 0.169ms
2022-02-21 14:09:46 - until epoch: 068, best_acc1: 66.388%
2022-02-21 14:09:46 - epoch 069 lr: 0.0010000000000000002
2022-02-21 14:10:24 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.8671
2022-02-21 14:10:58 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.8664
2022-02-21 14:11:30 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.6915
2022-02-21 14:12:03 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.5133
2022-02-21 14:12:37 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.5104
2022-02-21 14:13:10 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.4742
2022-02-21 14:13:43 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.5797
2022-02-21 14:14:15 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.6194
2022-02-21 14:14:49 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.5469
2022-02-21 14:15:22 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.5818
2022-02-21 14:15:56 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.5499
2022-02-21 14:16:28 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.5099
2022-02-21 14:17:02 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.9418
2022-02-21 14:17:34 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.7451
2022-02-21 14:18:07 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.6178
2022-02-21 14:18:41 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.7150
2022-02-21 14:19:15 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.5623
2022-02-21 14:19:48 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.4397
2022-02-21 14:20:21 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.5297
2022-02-21 14:20:54 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.3999
2022-02-21 14:21:27 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.5814
2022-02-21 14:21:59 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.6396
2022-02-21 14:22:32 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.6466
2022-02-21 14:23:05 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.7002
2022-02-21 14:23:38 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.5042
2022-02-21 14:24:10 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.6735
2022-02-21 14:24:44 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.8531
2022-02-21 14:25:17 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.7158
2022-02-21 14:25:49 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.4664
2022-02-21 14:26:23 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.6040
2022-02-21 14:26:56 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.5101
2022-02-21 14:27:31 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.4936
2022-02-21 14:28:02 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.4864
2022-02-21 14:28:36 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.4779
2022-02-21 14:29:08 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.4583
2022-02-21 14:29:42 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.4141
2022-02-21 14:30:15 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.6702
2022-02-21 14:30:48 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.6670
2022-02-21 14:31:21 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.5625
2022-02-21 14:31:54 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.7695
2022-02-21 14:32:28 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.7055
2022-02-21 14:33:00 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.4094
2022-02-21 14:33:34 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.6315
2022-02-21 14:34:07 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.6045
2022-02-21 14:34:40 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.6184
2022-02-21 14:35:12 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.7459
2022-02-21 14:35:46 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.5059
2022-02-21 14:36:19 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.6326
2022-02-21 14:36:53 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.4932
2022-02-21 14:37:25 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.6687
2022-02-21 14:37:27 - train: epoch 069, train_loss: 1.6007
2022-02-21 14:38:43 - eval: epoch: 069, acc1: 66.504%, acc5: 87.116%, test_loss: 1.3740, per_image_load_time: 1.856ms, per_image_inference_time: 0.161ms
2022-02-21 14:38:43 - until epoch: 069, best_acc1: 66.504%
2022-02-21 14:38:43 - epoch 070 lr: 0.0010000000000000002
2022-02-21 14:39:21 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.7104
2022-02-21 14:39:54 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.6547
2022-02-21 14:40:28 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.7272
2022-02-21 14:41:00 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.6487
2022-02-21 14:41:34 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.6901
2022-02-21 14:42:07 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.5865
2022-02-21 14:42:40 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.6033
2022-02-21 14:43:14 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.4848
2022-02-21 14:43:47 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.7492
2022-02-21 14:44:20 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.5457
2022-02-21 14:44:53 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 2.0250
2022-02-21 14:45:27 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.4910
2022-02-21 14:45:58 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.5820
2022-02-21 14:46:31 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.5597
2022-02-21 14:47:04 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.5847
2022-02-21 14:47:37 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.6895
2022-02-21 14:48:11 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.6615
2022-02-21 14:48:44 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.6017
2022-02-21 14:49:16 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.5241
2022-02-21 14:49:50 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.5641
2022-02-21 14:50:23 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.7198
2022-02-21 14:50:56 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.6398
2022-02-21 14:51:29 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.6162
2022-02-21 14:52:02 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.6907
2022-02-21 14:52:35 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.6067
2022-02-21 14:53:09 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.5379
2022-02-21 14:53:42 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.5555
2022-02-21 14:54:15 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.7241
2022-02-21 14:54:47 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.6681
2022-02-21 14:55:21 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.6296
2022-02-21 14:55:54 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.6334
2022-02-21 14:56:28 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.7746
2022-02-21 14:57:00 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.6329
2022-02-21 14:57:34 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.6831
2022-02-21 14:58:07 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.7149
2022-02-21 14:58:41 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.6546
2022-02-21 14:59:13 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.4933
2022-02-21 14:59:48 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.5518
2022-02-21 15:00:20 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.4112
2022-02-21 15:00:53 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.4753
2022-02-21 15:01:26 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.4930
2022-02-21 15:02:00 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.6681
2022-02-21 15:02:32 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.5788
2022-02-21 15:03:07 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.5757
2022-02-21 15:03:39 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.6339
2022-02-21 15:04:12 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.7395
2022-02-21 15:04:45 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.6690
2022-02-21 15:05:19 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.6872
2022-02-21 15:05:55 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.7519
2022-02-21 15:06:28 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.5730
2022-02-21 15:06:30 - train: epoch 070, train_loss: 1.5974
2022-02-21 15:07:45 - eval: epoch: 070, acc1: 66.540%, acc5: 87.232%, test_loss: 1.3708, per_image_load_time: 2.673ms, per_image_inference_time: 0.158ms
2022-02-21 15:07:45 - until epoch: 070, best_acc1: 66.540%
2022-02-21 15:07:45 - epoch 071 lr: 0.0010000000000000002
2022-02-21 15:08:23 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.4986
2022-02-21 15:08:57 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.4631
2022-02-21 15:09:30 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.4813
2022-02-21 15:10:03 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.6469
2022-02-21 15:10:34 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.6240
2022-02-21 15:11:07 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.7090
2022-02-21 15:11:39 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.7311
2022-02-21 15:12:12 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.5622
2022-02-21 15:12:44 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.6579
2022-02-21 15:13:17 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.6400
2022-02-21 15:13:50 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.8358
2022-02-21 15:14:22 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.5203
2022-02-21 15:14:55 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.6131
2022-02-21 15:15:27 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.6763
2022-02-21 15:16:01 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.3977
2022-02-21 15:16:34 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.4588
2022-02-21 15:17:06 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.5132
2022-02-21 15:17:39 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.5167
2022-02-21 15:18:12 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.4843
2022-02-21 15:18:43 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.6694
2022-02-21 15:19:17 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.4423
2022-02-21 15:19:49 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.4579
2022-02-21 15:20:22 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.5512
2022-02-21 15:20:54 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.5698
2022-02-21 15:21:27 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.7560
2022-02-21 15:22:00 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.4435
2022-02-21 15:22:33 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.6028
2022-02-21 15:23:05 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.7259
2022-02-21 15:24:21 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.6145
2022-02-21 15:25:57 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.6717
2022-02-21 15:27:38 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.5127
2022-02-21 15:30:21 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.3891
2022-02-21 15:33:56 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.4712
2022-02-21 15:35:08 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.5162
2022-02-21 15:35:40 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.6830
2022-02-21 15:39:05 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.7639
2022-02-21 15:39:57 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.5217
2022-02-21 15:40:31 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.5273
2022-02-21 15:41:03 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.6078
2022-02-21 15:41:36 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.7782
2022-02-21 15:42:08 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.6140
2022-02-21 15:42:41 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.7526
2022-02-21 15:43:13 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.5033
2022-02-21 15:43:46 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.5867
2022-02-21 15:44:19 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.5331
2022-02-21 15:44:51 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.6863
2022-02-21 15:45:24 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.5614
2022-02-21 15:45:57 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.4640
2022-02-21 15:46:29 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.3760
2022-02-21 15:47:01 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.4690
2022-02-21 15:47:03 - train: epoch 071, train_loss: 1.5922
2022-02-21 15:48:16 - eval: epoch: 071, acc1: 66.696%, acc5: 87.228%, test_loss: 1.3693, per_image_load_time: 2.550ms, per_image_inference_time: 0.172ms
2022-02-21 15:48:16 - until epoch: 071, best_acc1: 66.696%
2022-02-21 15:48:16 - epoch 072 lr: 0.0010000000000000002
2022-02-21 15:48:54 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.7758
2022-02-21 15:49:28 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.4562
2022-02-21 15:50:28 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.4133
2022-02-21 15:52:52 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.6494
2022-02-21 15:54:08 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.4917
2022-02-21 15:54:40 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.4351
2022-02-21 15:55:30 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.5459
2022-02-21 15:56:26 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.7693
2022-02-21 15:58:25 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.4422
2022-02-21 16:00:38 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.5309
2022-02-21 16:04:04 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.7692
2022-02-21 16:08:09 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.4736
2022-02-21 16:09:02 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.5562
2022-02-21 16:09:35 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.6597
2022-02-21 16:10:07 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.4683
2022-02-21 16:10:40 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.6265
2022-02-21 16:11:12 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.4452
2022-02-21 16:11:45 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.4966
2022-02-21 16:12:17 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.5010
2022-02-21 16:12:51 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.6136
2022-02-21 16:13:23 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.7011
2022-02-21 16:13:57 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.7058
2022-02-21 16:14:28 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.7124
2022-02-21 16:15:01 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.5415
2022-02-21 16:15:33 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.5030
2022-02-21 16:16:06 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.4678
2022-02-21 16:16:38 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.6025
2022-02-21 16:17:11 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.6928
2022-02-21 16:17:44 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.5121
2022-02-21 16:18:16 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.5158
2022-02-21 16:18:48 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.6845
2022-02-21 16:19:21 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.5349
2022-02-21 16:19:53 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.5629
2022-02-21 16:20:26 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.7036
2022-02-21 16:24:54 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.5490
2022-02-21 16:32:37 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.5315
2022-02-21 16:38:25 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.7112
2022-02-21 16:38:58 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.5766
2022-02-21 16:39:30 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.7702
2022-02-21 16:40:03 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.5149
2022-02-21 16:40:35 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.6899
2022-02-21 16:41:09 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.5963
2022-02-21 16:41:41 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.5806
2022-02-21 16:42:14 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.4624
2022-02-21 16:42:47 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.7051
2022-02-21 16:43:20 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.4896
2022-02-21 16:43:51 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.5872
2022-02-21 16:44:25 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.7635
2022-02-21 16:44:57 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.6022
2022-02-21 16:45:29 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.4853
2022-02-21 16:45:31 - train: epoch 072, train_loss: 1.5918
2022-02-21 16:46:46 - eval: epoch: 072, acc1: 66.506%, acc5: 87.334%, test_loss: 1.3657, per_image_load_time: 2.401ms, per_image_inference_time: 0.189ms
2022-02-21 16:46:46 - until epoch: 072, best_acc1: 66.696%
2022-02-21 16:46:46 - epoch 073 lr: 0.0010000000000000002
2022-02-21 16:47:25 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.8446
2022-02-21 16:47:58 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.6863
2022-02-21 16:48:29 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.7006
2022-02-21 16:49:04 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.2720
2022-02-21 16:49:35 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.4412
2022-02-21 16:50:09 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.5328
2022-02-21 16:50:42 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.6325
2022-02-21 16:51:15 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.4852
2022-02-21 16:51:48 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.3414
2022-02-21 16:52:21 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.4964
2022-02-21 16:52:53 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.6478
2022-02-21 16:53:26 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.5523
2022-02-21 16:53:58 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.6452
2022-02-21 16:54:32 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.5977
2022-02-21 16:55:04 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.6338
2022-02-21 16:55:37 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.6638
2022-02-21 16:56:10 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.8279
2022-02-21 16:56:43 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.4572
2022-02-21 16:57:16 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.7369
2022-02-21 16:57:49 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.3713
2022-02-21 16:58:22 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.7018
2022-02-21 16:58:56 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.7639
2022-02-21 16:59:28 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.6451
2022-02-21 17:00:01 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.5291
2022-02-21 17:00:33 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.7950
2022-02-21 17:01:07 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.5555
2022-02-21 17:01:39 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.7172
2022-02-21 17:02:12 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.5951
2022-02-21 17:02:45 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.7468
2022-02-21 17:03:19 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.4561
2022-02-21 17:03:51 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.4584
2022-02-21 17:04:25 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.4040
2022-02-21 17:04:57 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.5812
2022-02-21 17:05:31 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.5092
2022-02-21 17:06:03 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.6580
2022-02-21 17:06:37 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.4341
2022-02-21 17:07:09 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.6142
2022-02-21 17:07:43 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.8641
2022-02-21 17:08:15 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.5674
2022-02-21 17:08:48 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.7199
2022-02-21 17:09:21 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.5640
2022-02-21 17:09:54 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.8640
2022-02-21 17:10:27 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.6268
2022-02-21 17:11:00 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.7294
2022-02-21 17:11:33 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.4406
2022-02-21 17:12:06 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.8113
2022-02-21 17:12:38 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.3171
2022-02-21 17:13:12 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.7006
2022-02-21 17:13:45 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.5008
2022-02-21 17:14:17 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.6413
2022-02-21 17:14:19 - train: epoch 073, train_loss: 1.5871
2022-02-21 17:15:34 - eval: epoch: 073, acc1: 66.462%, acc5: 87.340%, test_loss: 1.3670, per_image_load_time: 2.705ms, per_image_inference_time: 0.160ms
2022-02-21 17:15:34 - until epoch: 073, best_acc1: 66.696%
2022-02-21 17:15:34 - epoch 074 lr: 0.0010000000000000002
2022-02-21 17:16:12 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.4921
2022-02-21 17:16:46 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.6535
2022-02-21 17:17:19 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.6382
2022-02-21 17:17:51 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.8827
2022-02-21 17:18:24 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.5233
2022-02-21 17:18:57 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.6677
2022-02-21 17:19:29 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.6028
2022-02-21 17:20:02 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.7489
2022-02-21 17:20:35 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.5486
2022-02-21 17:21:09 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.7007
2022-02-21 17:21:41 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.7236
2022-02-21 17:22:13 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.4431
2022-02-21 17:22:47 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.6655
2022-02-21 17:23:21 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.4731
2022-02-21 17:23:54 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.5533
2022-02-21 17:24:26 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.2879
2022-02-21 17:25:00 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.7715
2022-02-21 17:25:32 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 2.0411
2022-02-21 17:26:06 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.6741
2022-02-21 17:26:39 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.3526
2022-02-21 17:27:11 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.6349
2022-02-21 17:27:44 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.8165
2022-02-21 17:28:16 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.7046
2022-02-21 17:28:50 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.6101
2022-02-21 17:29:23 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.3860
2022-02-21 17:29:55 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.2576
2022-02-21 17:30:28 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.5518
2022-02-21 17:31:01 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.8585
2022-02-21 17:31:34 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.5653
2022-02-21 17:32:07 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.6819
2022-02-21 17:32:40 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.5620
2022-02-21 17:33:12 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.5209
2022-02-21 17:33:45 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.7585
2022-02-21 17:34:17 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.6180
2022-02-21 17:34:50 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.4788
2022-02-21 17:35:24 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.7140
2022-02-21 17:35:56 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.6556
2022-02-21 17:36:30 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.5391
2022-02-21 17:37:02 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.3646
2022-02-21 17:37:35 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.4427
2022-02-21 17:38:07 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.7221
2022-02-21 17:38:41 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.5901
2022-02-21 17:39:14 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.4843
2022-02-21 17:39:47 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.3788
2022-02-21 17:40:19 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.4131
2022-02-21 17:40:52 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.7373
2022-02-21 17:41:25 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.6306
2022-02-21 17:41:58 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.5921
2022-02-21 17:42:31 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.7905
2022-02-21 17:43:03 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.5930
2022-02-21 17:43:05 - train: epoch 074, train_loss: 1.5844
2022-02-21 17:44:19 - eval: epoch: 074, acc1: 66.664%, acc5: 87.376%, test_loss: 1.3591, per_image_load_time: 2.369ms, per_image_inference_time: 0.166ms
2022-02-21 17:44:19 - until epoch: 074, best_acc1: 66.696%
2022-02-21 17:44:19 - epoch 075 lr: 0.0010000000000000002
2022-02-21 17:44:56 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.7255
2022-02-21 17:45:30 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.6041
2022-02-21 17:46:02 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.6233
2022-02-21 17:46:34 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.6121
2022-02-21 17:47:06 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.4248
2022-02-21 17:47:39 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.5031
2022-02-21 17:48:12 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.7574
2022-02-21 17:48:44 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.6300
2022-02-21 17:49:17 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.6968
2022-02-21 17:49:50 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.3693
2022-02-21 17:50:23 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.5993
2022-02-21 17:50:55 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.4875
2022-02-21 17:51:29 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.3924
2022-02-21 17:52:01 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.4968
2022-02-21 17:52:34 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.7074
2022-02-21 17:53:07 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.2484
2022-02-21 17:53:39 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.5373
2022-02-21 17:54:12 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.5639
2022-02-21 17:54:46 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.4333
2022-02-21 17:55:18 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.5292
2022-02-21 17:55:51 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.5653
2022-02-21 17:56:25 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.6887
2022-02-21 17:56:57 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.5142
2022-02-21 17:57:31 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.5878
2022-02-21 17:58:02 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.6108
2022-02-21 17:58:36 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.7173
2022-02-21 17:59:08 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.3045
2022-02-21 17:59:42 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.5822
2022-02-21 18:00:14 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.7141
2022-02-21 18:00:48 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.7621
2022-02-21 18:01:20 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.7729
2022-02-21 18:01:53 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.5238
2022-02-21 18:02:26 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.6399
2022-02-21 18:02:59 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.3659
2022-02-21 18:03:32 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.5150
2022-02-21 18:04:05 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.5652
2022-02-21 18:04:38 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.6328
2022-02-21 18:05:11 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.5299
2022-02-21 18:05:43 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.8120
2022-02-21 18:06:16 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.3959
2022-02-21 18:06:49 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.2638
2022-02-21 18:07:23 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.6899
2022-02-21 18:07:55 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.7249
2022-02-21 18:08:28 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.3616
2022-02-21 18:09:01 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.5390
2022-02-21 18:09:35 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.3424
2022-02-21 18:10:07 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.7708
2022-02-21 18:10:41 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.5685
2022-02-21 18:11:13 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.3799
2022-02-21 18:11:46 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.6081
2022-02-21 18:11:47 - train: epoch 075, train_loss: 1.5811
2022-02-21 18:13:03 - eval: epoch: 075, acc1: 66.634%, acc5: 87.298%, test_loss: 1.3594, per_image_load_time: 2.359ms, per_image_inference_time: 0.166ms
2022-02-21 18:13:03 - until epoch: 075, best_acc1: 66.696%
2022-02-21 18:13:03 - epoch 076 lr: 0.0010000000000000002
2022-02-21 18:13:41 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.3287
2022-02-21 18:14:14 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.5604
2022-02-21 18:14:47 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.6967
2022-02-21 18:15:19 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.6257
2022-02-21 18:15:53 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.6001
2022-02-21 18:16:26 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.4889
2022-02-21 18:16:58 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.5733
2022-02-21 18:17:32 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.5131
2022-02-21 18:18:04 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.4593
2022-02-21 18:18:37 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.4621
2022-02-21 18:19:11 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.4008
2022-02-21 18:19:43 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.5731
2022-02-21 18:20:16 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.6315
2022-02-21 18:20:50 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.5382
2022-02-21 18:21:22 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.4627
2022-02-21 18:21:56 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.5144
2022-02-21 18:22:28 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.6136
2022-02-21 18:23:02 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.3694
2022-02-21 18:23:35 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.6314
2022-02-21 18:24:08 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.5743
2022-02-21 18:24:40 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.6672
2022-02-21 18:25:13 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.7064
2022-02-21 18:25:46 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.4844
2022-02-21 18:26:19 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.7998
2022-02-21 18:26:52 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.5488
2022-02-21 18:27:26 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.8512
2022-02-21 18:27:58 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.6932
2022-02-21 18:28:31 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.5321
2022-02-21 18:29:04 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.5527
2022-02-21 18:29:38 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.3876
2022-02-21 18:30:10 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.7609
2022-02-21 18:30:44 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.6024
2022-02-21 18:31:16 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.5237
2022-02-21 18:31:50 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.5578
2022-02-21 18:32:22 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.4465
2022-02-21 18:32:56 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.4721
2022-02-21 18:33:29 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.4647
2022-02-21 18:34:02 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.3667
2022-02-21 18:34:34 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.6105
2022-02-21 18:35:07 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.5969
2022-02-21 18:35:40 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.5653
2022-02-21 18:36:13 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.7593
2022-02-21 18:36:46 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.6683
2022-02-21 18:37:19 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.4782
2022-02-21 18:37:52 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.6017
2022-02-21 18:38:24 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.5390
2022-02-21 18:38:58 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.6612
2022-02-21 18:39:30 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.4050
2022-02-21 18:40:05 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.5737
2022-02-21 18:40:36 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.5802
2022-02-21 18:40:39 - train: epoch 076, train_loss: 1.5796
2022-02-21 18:41:53 - eval: epoch: 076, acc1: 66.478%, acc5: 87.240%, test_loss: 1.3689, per_image_load_time: 2.656ms, per_image_inference_time: 0.174ms
2022-02-21 18:41:53 - until epoch: 076, best_acc1: 66.696%
2022-02-21 18:41:53 - epoch 077 lr: 0.0010000000000000002
2022-02-21 18:42:31 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.6863
2022-02-21 18:43:04 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.7235
2022-02-21 18:43:38 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.3001
2022-02-21 18:44:10 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.6360
2022-02-21 18:44:44 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.4420
2022-02-21 18:45:16 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.3241
2022-02-21 18:45:49 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.5483
2022-02-21 18:46:22 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.7217
2022-02-21 18:46:55 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.8245
2022-02-21 18:47:28 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.3823
2022-02-21 18:48:02 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.5502
2022-02-21 18:48:34 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.7924
2022-02-21 18:49:07 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.2885
2022-02-21 18:49:40 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.5899
2022-02-21 18:50:13 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.4246
2022-02-21 18:50:46 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.5241
2022-02-21 18:51:19 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.9038
2022-02-21 18:51:52 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.4297
2022-02-21 18:52:25 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.5358
2022-02-21 18:52:58 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.4881
2022-02-21 18:53:32 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.5913
2022-02-21 18:54:04 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.5582
2022-02-21 18:54:38 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.6771
2022-02-21 18:55:11 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.5015
2022-02-21 18:55:45 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.7811
2022-02-21 18:56:17 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.3251
2022-02-21 18:56:50 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.5900
2022-02-21 18:57:24 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.6194
2022-02-21 18:57:57 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.8643
2022-02-21 18:58:30 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.4204
2022-02-21 18:59:04 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.5205
2022-02-21 18:59:36 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.6829
2022-02-21 19:00:09 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.4539
2022-02-21 19:00:43 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.8507
2022-02-21 19:01:15 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.3691
2022-02-21 19:01:49 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.6476
2022-02-21 19:02:22 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.5367
2022-02-21 19:02:56 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.6182
2022-02-21 19:03:29 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.6590
2022-02-21 19:04:02 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.6237
2022-02-21 19:04:35 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.6581
2022-02-21 19:05:08 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.7427
2022-02-21 19:05:42 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.4495
2022-02-21 19:06:15 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.4847
2022-02-21 19:06:48 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.7271
2022-02-21 19:07:21 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.5796
2022-02-21 19:07:54 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.4296
2022-02-21 19:08:28 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.6997
2022-02-21 19:09:00 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.8098
2022-02-21 19:09:33 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.7241
2022-02-21 19:09:35 - train: epoch 077, train_loss: 1.5754
2022-02-21 19:10:50 - eval: epoch: 077, acc1: 66.792%, acc5: 87.404%, test_loss: 1.3565, per_image_load_time: 1.771ms, per_image_inference_time: 0.177ms
2022-02-21 19:10:50 - until epoch: 077, best_acc1: 66.792%
2022-02-21 19:10:50 - epoch 078 lr: 0.0010000000000000002
2022-02-21 19:11:28 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.5428
2022-02-21 19:12:02 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.7324
2022-02-21 19:12:35 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.6302
2022-02-21 19:13:09 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.4983
2022-02-21 19:13:40 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.5444
2022-02-21 19:14:14 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.4845
2022-02-21 19:14:47 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.7890
2022-02-21 19:15:21 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.8358
2022-02-21 19:15:54 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.6830
2022-02-21 19:16:28 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.6488
2022-02-21 19:17:02 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.3143
2022-02-21 19:17:34 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.3246
2022-02-21 19:18:07 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.6010
2022-02-21 19:18:40 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.3585
2022-02-21 19:19:13 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.4104
2022-02-21 19:19:47 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.7122
2022-02-21 19:20:20 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.5537
2022-02-21 19:20:54 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.5078
2022-02-21 19:21:27 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.4740
2022-02-21 19:22:00 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.4886
2022-02-21 19:22:33 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.8171
2022-02-21 19:23:07 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.5241
2022-02-21 19:23:40 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.7581
2022-02-21 19:24:13 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.6100
2022-02-21 19:24:46 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.4077
2022-02-21 19:25:18 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.5923
2022-02-21 19:25:52 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.5560
2022-02-21 19:26:25 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.5714
2022-02-21 19:26:58 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.5412
2022-02-21 19:27:31 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.5341
2022-02-21 19:28:06 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.7022
2022-02-21 19:28:38 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.6159
2022-02-21 19:29:11 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.6744
2022-02-21 19:29:45 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.5426
2022-02-21 19:30:18 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.5228
2022-02-21 19:30:52 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.6412
2022-02-21 19:31:25 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.5083
2022-02-21 19:31:59 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.6253
2022-02-21 19:32:31 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.5231
2022-02-21 19:33:05 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.5993
2022-02-21 19:33:38 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.6240
2022-02-21 19:34:12 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.7622
2022-02-21 19:34:44 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.5469
2022-02-21 19:35:19 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.5697
2022-02-21 19:35:52 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.7005
2022-02-21 19:36:27 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.3539
2022-02-21 19:36:59 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.6998
2022-02-21 19:37:32 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.8547
2022-02-21 19:38:07 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.6238
2022-02-21 19:38:38 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.5843
2022-02-21 19:38:41 - train: epoch 078, train_loss: 1.5704
2022-02-21 19:39:57 - eval: epoch: 078, acc1: 66.696%, acc5: 87.336%, test_loss: 1.3589, per_image_load_time: 2.695ms, per_image_inference_time: 0.179ms
2022-02-21 19:39:57 - until epoch: 078, best_acc1: 66.792%
2022-02-21 19:39:57 - epoch 079 lr: 0.0010000000000000002
2022-02-21 19:40:35 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.5112
2022-02-21 19:41:09 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.5086
2022-02-21 19:41:43 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.6428
2022-02-21 19:42:16 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.6824
2022-02-21 19:42:49 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.5826
2022-02-21 19:43:22 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.5200
2022-02-21 19:43:56 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.4886
2022-02-21 19:44:29 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.7245
2022-02-21 19:45:02 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.4926
2022-02-21 19:45:35 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.4791
2022-02-21 19:46:08 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.7630
2022-02-21 19:46:41 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.8798
2022-02-21 19:47:14 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.3663
2022-02-21 19:47:48 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.5346
2022-02-21 19:48:21 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.5774
2022-02-21 19:48:54 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.3498
2022-02-21 19:49:28 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.5117
2022-02-21 19:50:02 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.5200
2022-02-21 19:50:36 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.5854
2022-02-21 19:51:09 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.7495
2022-02-21 19:51:42 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.4687
2022-02-21 19:52:16 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.6166
2022-02-21 19:52:50 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.4911
2022-02-21 19:53:22 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.6360
2022-02-21 19:53:55 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.4594
2022-02-21 19:54:29 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.4770
2022-02-21 19:55:02 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.3084
2022-02-21 19:55:35 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.4151
2022-02-21 19:56:09 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.4934
2022-02-21 19:56:41 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.5292
2022-02-21 19:57:15 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.4671
2022-02-21 19:57:48 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.9208
2022-02-21 19:58:22 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.4981
2022-02-21 19:58:54 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.3950
2022-02-21 19:59:28 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.8122
2022-02-21 20:00:01 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.5057
2022-02-21 20:00:33 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.4714
2022-02-21 20:01:08 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.5914
2022-02-21 20:01:41 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.4149
2022-02-21 20:02:15 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.4521
2022-02-21 20:02:47 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.6651
2022-02-21 20:03:22 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.3990
2022-02-21 20:03:54 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.3300
2022-02-21 20:04:28 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.7121
2022-02-21 20:05:00 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.8314
2022-02-21 20:05:34 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.7589
2022-02-21 20:06:06 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.7379
2022-02-21 20:06:39 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.6495
2022-02-21 20:07:14 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.7252
2022-02-21 20:07:45 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.3644
2022-02-21 20:07:48 - train: epoch 079, train_loss: 1.5702
2022-02-21 20:09:04 - eval: epoch: 079, acc1: 66.840%, acc5: 87.384%, test_loss: 1.3563, per_image_load_time: 2.696ms, per_image_inference_time: 0.186ms
2022-02-21 20:09:04 - until epoch: 079, best_acc1: 66.840%
2022-02-21 20:09:04 - epoch 080 lr: 0.0010000000000000002
2022-02-21 20:09:43 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.3781
2022-02-21 20:10:17 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.4471
2022-02-21 20:10:49 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.6545
2022-02-21 20:11:23 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.3834
2022-02-21 20:11:56 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.6422
2022-02-21 20:12:29 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.5264
2022-02-21 20:13:02 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.4486
2022-02-21 20:13:35 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.4220
2022-02-21 20:14:08 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.6949
2022-02-21 20:14:42 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.4590
2022-02-21 20:15:15 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.5283
2022-02-21 20:15:48 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.5417
2022-02-21 20:16:21 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.4547
2022-02-21 20:16:54 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.4540
2022-02-21 20:17:28 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.5231
2022-02-21 20:18:01 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.4463
2022-02-21 20:18:34 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.5728
2022-02-21 20:19:07 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.6557
2022-02-21 20:19:40 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.6045
2022-02-21 20:20:14 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.6402
2022-02-21 20:20:47 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.6328
2022-02-21 20:21:20 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.5727
2022-02-21 20:21:52 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.4598
2022-02-21 20:22:27 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.5648
2022-02-21 20:22:59 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.4562
2022-02-21 20:23:33 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.4700
2022-02-21 20:24:06 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.6465
2022-02-21 20:24:40 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.7161
2022-02-21 20:25:12 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.5583
2022-02-21 20:25:46 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.5488
2022-02-21 20:26:19 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.8419
2022-02-21 20:26:52 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.2317
2022-02-21 20:27:25 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.5174
2022-02-21 20:27:59 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.3616
2022-02-21 20:28:31 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.3907
2022-02-21 20:29:05 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.5113
2022-02-21 20:29:38 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.4986
2022-02-21 20:30:12 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.6183
2022-02-21 20:30:45 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.3788
2022-02-21 20:31:19 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.6784
2022-02-21 20:31:52 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.7379
2022-02-21 20:32:26 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.4505
2022-02-21 20:32:59 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.7443
2022-02-21 20:33:32 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.6189
2022-02-21 20:34:05 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.4929
2022-02-21 20:34:39 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.6608
2022-02-21 20:35:12 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.6567
2022-02-21 20:35:45 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.5802
2022-02-21 20:36:19 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.6958
2022-02-21 20:36:52 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.3524
2022-02-21 20:36:54 - train: epoch 080, train_loss: 1.5694
2022-02-21 20:38:09 - eval: epoch: 080, acc1: 66.760%, acc5: 87.442%, test_loss: 1.3539, per_image_load_time: 2.578ms, per_image_inference_time: 0.165ms
2022-02-21 20:38:10 - until epoch: 080, best_acc1: 66.840%
2022-02-21 20:38:10 - epoch 081 lr: 0.0010000000000000002
2022-02-21 20:38:48 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.4751
2022-02-21 20:39:22 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.5701
2022-02-21 20:39:55 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.5191
2022-02-21 20:40:29 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.7142
2022-02-21 20:41:02 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.6958
2022-02-21 20:41:35 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.5601
2022-02-21 20:42:09 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.5554
2022-02-21 20:42:42 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.8258
2022-02-21 20:43:15 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.5505
2022-02-21 20:43:49 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.5500
2022-02-21 20:44:22 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.4688
2022-02-21 20:44:55 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.6062
2022-02-21 20:45:29 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.3652
2022-02-21 20:46:02 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.2378
2022-02-21 20:46:36 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.5673
2022-02-21 20:47:09 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.5219
2022-02-21 20:47:43 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.6676
2022-02-21 20:48:17 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.5209
2022-02-21 20:48:50 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.4673
2022-02-21 20:49:23 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.6429
2022-02-21 20:49:57 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.5391
2022-02-21 20:50:30 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.5226
2022-02-21 20:51:03 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.4984
2022-02-21 20:51:36 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.4803
2022-02-21 20:52:10 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.7183
2022-02-21 20:52:44 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.7548
2022-02-21 20:53:18 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.5148
2022-02-21 20:53:50 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.4308
2022-02-21 20:54:24 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.4624
2022-02-21 20:54:58 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.5508
2022-02-21 20:55:31 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.4401
2022-02-21 20:56:05 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.6168
2022-02-21 20:56:39 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.5737
2022-02-21 20:57:12 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.7329
2022-02-21 20:57:45 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.5923
2022-02-21 20:58:20 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.3382
2022-02-21 20:58:53 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.7586
2022-02-21 20:59:26 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.3935
2022-02-21 21:00:00 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.7181
2022-02-21 21:00:34 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.3550
2022-02-21 21:01:07 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.5876
2022-02-21 21:01:41 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.6729
2022-02-21 21:02:14 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.6327
2022-02-21 21:02:48 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.6829
2022-02-21 21:03:22 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.7801
2022-02-21 21:03:55 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.5238
2022-02-21 21:04:29 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.5487
2022-02-21 21:05:03 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.6490
2022-02-21 21:05:37 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.5160
2022-02-21 21:06:10 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.2906
2022-02-21 21:06:12 - train: epoch 081, train_loss: 1.5674
2022-02-21 21:07:27 - eval: epoch: 081, acc1: 66.848%, acc5: 87.470%, test_loss: 1.3529, per_image_load_time: 2.726ms, per_image_inference_time: 0.166ms
2022-02-21 21:07:27 - until epoch: 081, best_acc1: 66.848%
2022-02-21 21:07:27 - epoch 082 lr: 0.0010000000000000002
2022-02-21 21:08:06 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.2758
2022-02-21 21:08:40 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.5514
2022-02-21 21:09:14 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.6578
2022-02-21 21:09:46 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.6663
2022-02-21 21:10:19 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.7462
2022-02-21 21:10:53 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.5504
2022-02-21 21:11:26 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.7117
2022-02-21 21:12:00 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.4944
2022-02-21 21:12:33 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.7217
2022-02-21 21:13:07 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.6191
2022-02-21 21:13:40 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.6849
2022-02-21 21:14:13 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.6582
2022-02-21 21:14:46 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.6884
2022-02-21 21:15:19 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.3848
2022-02-21 21:15:53 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.4899
2022-02-21 21:16:25 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.4802
2022-02-21 21:16:59 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.5462
2022-02-21 21:17:32 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.3953
2022-02-21 21:18:06 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.5310
2022-02-21 21:18:40 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.4158
2022-02-21 21:19:14 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.5795
2022-02-21 21:19:46 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.6187
2022-02-21 21:20:20 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.4689
2022-02-21 21:20:53 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.6298
2022-02-21 21:21:26 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.6115
2022-02-21 21:21:59 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.5514
2022-02-21 21:22:34 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.4242
2022-02-21 21:23:06 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.4184
2022-02-21 21:23:39 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.3924
2022-02-21 21:24:13 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.5051
2022-02-21 21:24:46 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.5240
2022-02-21 21:25:18 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.8358
2022-02-21 21:25:52 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.5112
2022-02-21 21:26:25 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.5097
2022-02-21 21:26:58 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.4214
2022-02-21 21:27:30 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.5502
2022-02-21 21:28:03 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.4348
2022-02-21 21:28:36 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.7821
2022-02-21 21:29:09 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.5654
2022-02-21 21:29:42 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.6185
2022-02-21 21:30:15 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.5677
2022-02-21 21:30:49 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.7601
2022-02-21 21:31:21 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.4785
2022-02-21 21:31:55 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.4215
2022-02-21 21:32:28 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.6082
2022-02-21 21:33:01 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.7782
2022-02-21 21:33:34 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.4112
2022-02-21 21:34:08 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.5460
2022-02-21 21:34:40 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.5256
2022-02-21 21:35:13 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.5369
2022-02-21 21:35:15 - train: epoch 082, train_loss: 1.5646
2022-02-21 21:36:30 - eval: epoch: 082, acc1: 66.982%, acc5: 87.414%, test_loss: 1.3498, per_image_load_time: 2.731ms, per_image_inference_time: 0.156ms
2022-02-21 21:36:30 - until epoch: 082, best_acc1: 66.982%
2022-02-21 21:36:30 - epoch 083 lr: 0.0010000000000000002
2022-02-21 21:37:09 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.4650
2022-02-21 21:37:43 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.4904
2022-02-21 21:38:15 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.5837
2022-02-21 21:38:49 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.6695
2022-02-21 21:39:21 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.5134
2022-02-21 21:39:54 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.4856
2022-02-21 21:40:28 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.5012
2022-02-21 21:41:00 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.5086
2022-02-21 21:41:34 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.6756
2022-02-21 21:42:07 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.7329
2022-02-21 21:42:41 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.6059
2022-02-21 21:43:13 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.5403
2022-02-21 21:43:46 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.4517
2022-02-21 21:44:18 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.5496
2022-02-21 21:44:53 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.5049
2022-02-21 21:45:26 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.5814
2022-02-21 21:46:00 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.7355
2022-02-21 21:46:32 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.7164
2022-02-21 21:47:05 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.4535
2022-02-21 21:47:39 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.2516
2022-02-21 21:48:12 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.4170
2022-02-21 21:48:45 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.5126
2022-02-21 21:49:19 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.7606
2022-02-21 21:49:52 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.4833
2022-02-21 21:50:25 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.5462
2022-02-21 21:50:59 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.4157
2022-02-21 21:51:32 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.5273
2022-02-21 21:52:05 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.4855
2022-02-21 21:52:38 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.6272
2022-02-21 21:53:11 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.6593
2022-02-21 21:53:45 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.4160
2022-02-21 21:54:19 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.4020
2022-02-21 21:54:51 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.7158
2022-02-21 21:55:24 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.6597
2022-02-21 21:55:59 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.6123
2022-02-21 21:56:32 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.5842
2022-02-21 21:57:06 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.5959
2022-02-21 21:57:39 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.3992
2022-02-21 21:58:13 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.6095
2022-02-21 21:58:46 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.7799
2022-02-21 21:59:19 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.5539
2022-02-21 21:59:53 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.9495
2022-02-21 22:00:25 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.6096
2022-02-21 22:00:59 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.4716
2022-02-21 22:01:32 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.6438
2022-02-21 22:02:06 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.7138
2022-02-21 22:02:38 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.7281
2022-02-21 22:03:12 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.6964
2022-02-21 22:03:46 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.7733
2022-02-21 22:04:18 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.5846
2022-02-21 22:04:20 - train: epoch 083, train_loss: 1.5642
2022-02-21 22:05:36 - eval: epoch: 083, acc1: 66.906%, acc5: 87.324%, test_loss: 1.3549, per_image_load_time: 2.736ms, per_image_inference_time: 0.161ms
2022-02-21 22:05:36 - until epoch: 083, best_acc1: 66.982%
2022-02-21 22:05:36 - epoch 084 lr: 0.0010000000000000002
2022-02-21 22:06:15 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.5781
2022-02-21 22:06:47 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.6395
2022-02-21 22:07:22 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.4142
2022-02-21 22:07:54 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.5713
2022-02-21 22:08:28 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.6969
2022-02-21 22:09:02 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.7725
2022-02-21 22:09:35 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.6421
2022-02-21 22:10:08 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.7354
2022-02-21 22:10:42 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.5411
2022-02-21 22:11:15 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.6625
2022-02-21 22:11:49 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.5400
2022-02-21 22:12:22 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.8448
2022-02-21 22:12:56 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.5444
2022-02-21 22:13:29 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.5675
2022-02-21 22:14:02 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.6999
2022-02-21 22:14:35 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.5184
2022-02-21 22:15:10 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.6405
2022-02-21 22:15:42 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.5462
2022-02-21 22:16:15 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.5346
2022-02-21 22:16:49 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.5028
2022-02-21 22:17:22 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.5462
2022-02-21 22:17:55 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.2906
2022-02-21 22:18:28 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.5418
2022-02-21 22:19:01 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.4140
2022-02-21 22:19:34 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.6778
2022-02-21 22:20:07 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.7044
2022-02-21 22:20:41 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.5250
2022-02-21 22:21:14 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.6821
2022-02-21 22:21:48 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.5336
2022-02-21 22:22:21 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.6510
2022-02-21 22:22:54 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.4728
2022-02-21 22:23:27 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.5194
2022-02-21 22:24:01 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.6261
2022-02-21 22:24:34 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.4387
2022-02-21 22:25:08 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.4672
2022-02-21 22:25:40 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.6436
2022-02-21 22:26:14 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.6671
2022-02-21 22:26:47 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.8403
2022-02-21 22:27:21 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.6119
2022-02-21 22:27:54 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.5961
2022-02-21 22:28:28 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.6090
2022-02-21 22:29:02 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.6899
2022-02-21 22:29:35 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.6040
2022-02-21 22:30:09 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.7380
2022-02-21 22:30:42 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.4137
2022-02-21 22:31:16 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.4793
2022-02-21 22:31:49 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.7462
2022-02-21 22:32:23 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.4370
2022-02-21 22:32:56 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.3666
2022-02-21 22:33:27 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.5665
2022-02-21 22:33:30 - train: epoch 084, train_loss: 1.5608
2022-02-21 22:34:46 - eval: epoch: 084, acc1: 66.900%, acc5: 87.416%, test_loss: 1.3502, per_image_load_time: 2.460ms, per_image_inference_time: 0.160ms
2022-02-21 22:34:46 - until epoch: 084, best_acc1: 66.982%
2022-02-21 22:34:46 - epoch 085 lr: 0.0010000000000000002
2022-02-21 22:35:25 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.4156
2022-02-21 22:35:58 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.3868
2022-02-21 22:36:32 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.7623
2022-02-21 22:37:06 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.4630
2022-02-21 22:37:39 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.6621
2022-02-21 22:38:12 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.3424
2022-02-21 22:38:46 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.3227
2022-02-21 22:39:20 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.4370
2022-02-21 22:39:52 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.4038
2022-02-21 22:40:26 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.7354
2022-02-21 22:40:59 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.5813
2022-02-21 22:41:33 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.4134
2022-02-21 22:42:06 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.6600
2022-02-21 22:42:40 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.5776
2022-02-21 22:43:13 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.3329
2022-02-21 22:43:48 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.5455
2022-02-21 22:44:21 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.8169
2022-02-21 22:44:54 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.3752
2022-02-21 22:45:28 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.4175
2022-02-21 22:46:01 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.5868
2022-02-21 22:46:35 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.2252
2022-02-21 22:47:09 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.6410
2022-02-21 22:47:42 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.3579
2022-02-21 22:48:15 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.6796
2022-02-21 22:48:48 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.7849
2022-02-21 22:49:21 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.5878
2022-02-21 22:49:55 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.4277
2022-02-21 22:50:28 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.7268
2022-02-21 22:51:01 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.7091
2022-02-21 22:51:35 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.7199
2022-02-21 22:52:08 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.6203
2022-02-21 22:52:42 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.5533
2022-02-21 22:53:14 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.5469
2022-02-21 22:53:48 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.6340
2022-02-21 22:54:21 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.7140
2022-02-21 22:54:54 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.5815
2022-02-21 22:55:28 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.4166
2022-02-21 22:56:01 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.5819
2022-02-21 22:56:35 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.5306
2022-02-21 22:57:08 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.8989
2022-02-21 22:57:41 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.6018
2022-02-21 22:58:14 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.5915
2022-02-21 22:58:48 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.4312
2022-02-21 22:59:22 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.4306
2022-02-21 22:59:55 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.7180
2022-02-21 23:00:29 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.6111
2022-02-21 23:01:01 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.8215
2022-02-21 23:01:36 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.3849
2022-02-21 23:02:10 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.5859
2022-02-21 23:02:42 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.4753
2022-02-21 23:02:44 - train: epoch 085, train_loss: 1.5620
2022-02-21 23:03:59 - eval: epoch: 085, acc1: 66.826%, acc5: 87.430%, test_loss: 1.3514, per_image_load_time: 2.662ms, per_image_inference_time: 0.183ms
2022-02-21 23:03:59 - until epoch: 085, best_acc1: 66.982%
2022-02-21 23:03:59 - epoch 086 lr: 0.0010000000000000002
2022-02-21 23:04:37 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.3951
2022-02-21 23:05:11 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.4383
2022-02-21 23:05:44 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.7518
2022-02-21 23:06:18 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.6413
2022-02-21 23:06:52 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.5155
2022-02-21 23:07:24 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.4621
2022-02-21 23:07:57 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.4586
2022-02-21 23:08:29 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.8521
2022-02-21 23:09:02 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.6594
2022-02-21 23:09:34 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.6407
2022-02-21 23:10:07 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.6435
2022-02-21 23:10:40 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.4268
2022-02-21 23:11:12 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.5594
2022-02-21 23:11:46 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.4849
2022-02-21 23:12:18 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.4509
2022-02-21 23:12:53 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.5316
2022-02-21 23:13:25 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.4343
2022-02-21 23:14:00 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.6601
2022-02-21 23:14:33 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.8120
2022-02-21 23:15:07 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.5703
2022-02-21 23:15:40 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.4534
2022-02-21 23:16:14 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.5876
2022-02-21 23:16:46 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.5449
2022-02-21 23:17:21 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.3666
2022-02-21 23:17:54 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.3968
2022-02-21 23:18:27 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.5692
2022-02-21 23:19:00 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.4553
2022-02-21 23:19:34 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.6454
2022-02-21 23:20:07 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.3998
2022-02-21 23:20:40 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.6526
2022-02-21 23:21:13 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.5467
2022-02-21 23:21:47 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.6814
2022-02-21 23:22:21 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.6888
2022-02-21 23:22:53 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.6722
2022-02-21 23:23:28 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.6310
2022-02-21 23:24:01 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.5249
2022-02-21 23:24:34 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.6450
2022-02-21 23:25:08 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.5644
2022-02-21 23:25:42 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.5834
2022-02-21 23:26:14 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.5448
2022-02-21 23:26:48 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.5399
2022-02-21 23:27:21 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.6434
2022-02-21 23:27:54 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.5654
2022-02-21 23:28:28 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.4404
2022-02-21 23:29:00 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.3276
2022-02-21 23:29:35 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.4930
2022-02-21 23:30:08 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.5638
2022-02-21 23:30:42 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.5516
2022-02-21 23:31:16 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.6159
2022-02-21 23:31:48 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.4565
2022-02-21 23:31:50 - train: epoch 086, train_loss: 1.5586
2022-02-21 23:33:06 - eval: epoch: 086, acc1: 66.906%, acc5: 87.364%, test_loss: 1.3518, per_image_load_time: 1.844ms, per_image_inference_time: 0.210ms
2022-02-21 23:33:06 - until epoch: 086, best_acc1: 66.982%
2022-02-21 23:33:06 - epoch 087 lr: 0.0010000000000000002
2022-02-21 23:33:44 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.6370
2022-02-21 23:34:18 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.3910
2022-02-21 23:34:51 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.6658
2022-02-21 23:35:24 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.7497
2022-02-21 23:35:56 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.3254
2022-02-21 23:36:30 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.6218
2022-02-21 23:37:03 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.3882
2022-02-21 23:37:36 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.5759
2022-02-21 23:38:10 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.6227
2022-02-21 23:38:43 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.5319
2022-02-21 23:39:17 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.6785
2022-02-21 23:39:50 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.6233
2022-02-21 23:40:24 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.6904
2022-02-21 23:40:56 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.5872
2022-02-21 23:41:31 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.4876
2022-02-21 23:42:03 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.4252
2022-02-21 23:42:37 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.7201
2022-02-21 23:43:10 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.5577
2022-02-21 23:43:43 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.6001
2022-02-21 23:44:15 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.5984
2022-02-21 23:44:49 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.6860
2022-02-21 23:45:22 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.5909
2022-02-21 23:45:57 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.6183
2022-02-21 23:46:29 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.4802
2022-02-21 23:47:03 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.5466
2022-02-21 23:47:35 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.5943
2022-02-21 23:48:09 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.5641
2022-02-21 23:48:41 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.4374
2022-02-21 23:49:15 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.3971
2022-02-21 23:49:48 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.5430
2022-02-21 23:50:21 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.5626
2022-02-21 23:50:55 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.4590
2022-02-21 23:51:28 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.5404
2022-02-21 23:52:01 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.6161
2022-02-21 23:52:35 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.4435
2022-02-21 23:53:08 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.4203
2022-02-21 23:53:41 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.5586
2022-02-21 23:54:15 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.5836
2022-02-21 23:54:48 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.5713
2022-02-21 23:55:21 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.6028
2022-02-21 23:55:53 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.6313
2022-02-21 23:56:27 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.5895
2022-02-21 23:57:01 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.6262
2022-02-21 23:57:34 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.6416
2022-02-21 23:58:07 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.6515
2022-02-21 23:58:40 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.5217
2022-02-21 23:59:13 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.6142
2022-02-21 23:59:46 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.4037
2022-02-22 00:00:19 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.5294
2022-02-22 00:00:52 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.5804
2022-02-22 00:00:54 - train: epoch 087, train_loss: 1.5577
2022-02-22 00:02:09 - eval: epoch: 087, acc1: 66.804%, acc5: 87.376%, test_loss: 1.3557, per_image_load_time: 2.452ms, per_image_inference_time: 0.140ms
2022-02-22 00:02:09 - until epoch: 087, best_acc1: 66.982%
2022-02-22 00:02:09 - epoch 088 lr: 0.0010000000000000002
2022-02-22 00:02:48 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.4777
2022-02-22 00:03:22 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.5159
2022-02-22 00:03:54 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.5570
2022-02-22 00:04:28 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.6786
2022-02-22 00:05:00 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.4780
2022-02-22 00:05:34 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.5225
2022-02-22 00:06:08 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.5289
2022-02-22 00:06:40 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.5757
2022-02-22 00:07:13 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.6730
2022-02-22 00:07:45 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.5347
2022-02-22 00:08:19 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.5609
2022-02-22 00:08:53 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.4716
2022-02-22 00:09:26 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.6829
2022-02-22 00:10:00 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.4162
2022-02-22 00:10:32 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.6685
2022-02-22 00:11:06 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.4416
2022-02-22 00:11:39 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.6806
2022-02-22 00:12:13 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.5308
2022-02-22 00:12:46 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.6617
2022-02-22 00:13:19 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.4335
2022-02-22 00:13:52 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.5230
2022-02-22 00:14:26 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.5090
2022-02-22 00:14:59 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.4599
2022-02-22 00:15:33 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.7311
2022-02-22 00:16:05 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.5974
2022-02-22 00:16:39 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.5826
2022-02-22 00:17:12 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.5371
2022-02-22 00:17:45 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.5412
2022-02-22 00:18:18 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.5190
2022-02-22 00:18:53 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.7128
2022-02-22 00:19:25 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.4656
2022-02-22 00:19:59 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.4294
2022-02-22 00:20:31 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.3580
2022-02-22 00:21:05 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.3332
2022-02-22 00:21:39 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.4939
2022-02-22 00:22:13 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.5266
2022-02-22 00:22:45 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.6503
2022-02-22 00:23:19 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.5676
2022-02-22 00:23:52 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.7466
2022-02-22 00:24:26 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.4311
2022-02-22 00:24:59 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.6169
2022-02-22 00:25:33 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.6339
2022-02-22 00:26:05 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.5272
2022-02-22 00:26:39 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.4090
2022-02-22 00:27:12 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.5508
2022-02-22 00:27:45 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.6946
2022-02-22 00:28:19 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.5941
2022-02-22 00:28:52 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.4931
2022-02-22 00:29:24 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.2564
2022-02-22 00:29:57 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.5036
2022-02-22 00:29:59 - train: epoch 088, train_loss: 1.5566
2022-02-22 00:31:14 - eval: epoch: 088, acc1: 66.942%, acc5: 87.474%, test_loss: 1.3495, per_image_load_time: 2.409ms, per_image_inference_time: 0.171ms
2022-02-22 00:31:15 - until epoch: 088, best_acc1: 66.982%
2022-02-22 00:31:15 - epoch 089 lr: 0.0010000000000000002
2022-02-22 00:31:53 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.8430
2022-02-22 00:32:27 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.3378
2022-02-22 00:33:00 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.6584
2022-02-22 00:33:34 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.4873
2022-02-22 00:34:07 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.5755
2022-02-22 00:34:39 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.5098
2022-02-22 00:35:13 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.5674
2022-02-22 00:35:46 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.8600
2022-02-22 00:36:20 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.5938
2022-02-22 00:36:53 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.6322
2022-02-22 00:37:26 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.5022
2022-02-22 00:37:59 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.6370
2022-02-22 00:38:32 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.5808
2022-02-22 00:39:05 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.6328
2022-02-22 00:39:38 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.5559
2022-02-22 00:40:12 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.4881
2022-02-22 00:40:45 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.6260
2022-02-22 00:41:19 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.4848
2022-02-22 00:41:51 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.4835
2022-02-22 00:42:26 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.5037
2022-02-22 00:42:58 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.6179
2022-02-22 00:43:32 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.5296
2022-02-22 00:44:05 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.4197
2022-02-22 00:44:39 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.6836
2022-02-22 00:45:12 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.5676
2022-02-22 00:45:45 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.5417
2022-02-22 00:46:18 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.3841
2022-02-22 00:46:51 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.6415
2022-02-22 00:47:24 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.5138
2022-02-22 00:47:57 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.5531
2022-02-22 00:48:31 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.6134
2022-02-22 00:49:04 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.6655
2022-02-22 00:49:37 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.8600
2022-02-22 00:50:10 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.6230
2022-02-22 00:50:43 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.3327
2022-02-22 00:51:18 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.4582
2022-02-22 00:51:50 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.8198
2022-02-22 00:52:24 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.4696
2022-02-22 00:52:57 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.6583
2022-02-22 00:53:31 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.3788
2022-02-22 00:54:04 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.8807
2022-02-22 00:54:37 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.4624
2022-02-22 00:55:10 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.5225
2022-02-22 00:55:45 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.4821
2022-02-22 00:56:17 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.3708
2022-02-22 00:56:51 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.5786
2022-02-22 00:57:24 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.5088
2022-02-22 00:57:58 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.6034
2022-02-22 00:58:31 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.5711
2022-02-22 00:59:04 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.3098
2022-02-22 00:59:07 - train: epoch 089, train_loss: 1.5547
2022-02-22 01:00:22 - eval: epoch: 089, acc1: 66.918%, acc5: 87.542%, test_loss: 1.3465, per_image_load_time: 1.065ms, per_image_inference_time: 0.146ms
2022-02-22 01:00:22 - until epoch: 089, best_acc1: 66.982%
2022-02-22 01:00:22 - epoch 090 lr: 0.0010000000000000002
2022-02-22 01:01:01 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.5106
2022-02-22 01:01:34 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.5354
2022-02-22 01:02:08 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.4790
2022-02-22 01:02:40 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.4096
2022-02-22 01:03:14 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.5496
2022-02-22 01:03:46 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.7661
2022-02-22 01:04:20 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.5859
2022-02-22 01:04:53 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.5104
2022-02-22 01:05:27 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.4389
2022-02-22 01:06:00 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.3427
2022-02-22 01:06:33 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.6438
2022-02-22 01:07:06 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.5782
2022-02-22 01:07:39 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.7127
2022-02-22 01:08:12 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.3189
2022-02-22 01:08:45 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.7309
2022-02-22 01:09:18 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.4542
2022-02-22 01:09:52 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.3581
2022-02-22 01:10:24 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.5006
2022-02-22 01:10:57 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.4228
2022-02-22 01:11:31 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.6240
2022-02-22 01:12:05 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.7536
2022-02-22 01:12:38 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.5135
2022-02-22 01:13:11 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.7138
2022-02-22 01:13:44 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.5012
2022-02-22 01:14:18 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.5103
2022-02-22 01:14:51 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.5542
2022-02-22 01:15:24 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.4458
2022-02-22 01:15:58 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.5641
2022-02-22 01:16:31 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.4362
2022-02-22 01:17:05 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.5567
2022-02-22 01:17:37 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.4514
2022-02-22 01:18:11 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.4498
2022-02-22 01:18:43 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.7159
2022-02-22 01:19:17 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.3257
2022-02-22 01:19:49 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.5683
2022-02-22 01:20:24 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.4866
2022-02-22 01:20:56 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.5822
2022-02-22 01:21:29 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.5559
2022-02-22 01:22:02 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.3456
2022-02-22 01:22:36 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.5602
2022-02-22 01:23:09 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.8871
2022-02-22 01:23:43 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.8099
2022-02-22 01:24:15 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.5656
2022-02-22 01:24:49 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.6614
2022-02-22 01:25:22 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.3990
2022-02-22 01:25:56 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.5520
2022-02-22 01:26:28 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.4240
2022-02-22 01:27:01 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.6781
2022-02-22 01:27:35 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.5403
2022-02-22 01:28:08 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.4382
2022-02-22 01:28:10 - train: epoch 090, train_loss: 1.5535
2022-02-22 01:29:26 - eval: epoch: 090, acc1: 66.940%, acc5: 87.440%, test_loss: 1.3464, per_image_load_time: 1.417ms, per_image_inference_time: 0.181ms
2022-02-22 01:29:26 - until epoch: 090, best_acc1: 66.982%
2022-02-22 01:29:26 - epoch 091 lr: 0.00010000000000000003
2022-02-22 01:30:05 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.4662
2022-02-22 01:30:38 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.5009
2022-02-22 01:31:12 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.4882
2022-02-22 01:31:45 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.5093
2022-02-22 01:32:18 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.4170
2022-02-22 01:32:51 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.5876
2022-02-22 01:33:25 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.5287
2022-02-22 01:33:57 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.3136
2022-02-22 01:34:31 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.5331
2022-02-22 01:35:04 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.4087
2022-02-22 01:35:37 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.2869
2022-02-22 01:36:11 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.6483
2022-02-22 01:36:45 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.4448
2022-02-22 01:37:18 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.5376
2022-02-22 01:37:51 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.5766
2022-02-22 01:38:23 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.5436
2022-02-22 01:38:57 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.6238
2022-02-22 01:39:30 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.6219
2022-02-22 01:40:04 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.4586
2022-02-22 01:40:37 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.4726
2022-02-22 01:41:10 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.3208
2022-02-22 01:41:43 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.5921
2022-02-22 01:42:16 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.6100
2022-02-22 01:42:50 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.4404
2022-02-22 01:43:24 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.3935
2022-02-22 01:43:56 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.5186
2022-02-22 01:44:31 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.3722
2022-02-22 01:45:03 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.5002
2022-02-22 01:45:36 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.6013
2022-02-22 01:46:09 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.5946
2022-02-22 01:46:43 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.5219
2022-02-22 01:47:15 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.5948
2022-02-22 01:47:49 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.4447
2022-02-22 01:48:21 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.4041
2022-02-22 01:48:56 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.6015
2022-02-22 01:49:28 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.4889
2022-02-22 01:50:03 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.5853
2022-02-22 01:50:35 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.6095
2022-02-22 01:51:09 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.4553
2022-02-22 01:51:42 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.4890
2022-02-22 01:52:16 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.5400
2022-02-22 01:52:48 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.5406
2022-02-22 01:53:22 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.6443
2022-02-22 01:53:54 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.3702
2022-02-22 01:54:28 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.3545
2022-02-22 01:55:01 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.4503
2022-02-22 01:55:34 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.6098
2022-02-22 01:56:06 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.4690
2022-02-22 01:56:40 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.5059
2022-02-22 01:57:12 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.7047
2022-02-22 01:57:14 - train: epoch 091, train_loss: 1.5231
2022-02-22 01:58:30 - eval: epoch: 091, acc1: 67.470%, acc5: 87.802%, test_loss: 1.3238, per_image_load_time: 1.438ms, per_image_inference_time: 0.174ms
2022-02-22 01:58:30 - until epoch: 091, best_acc1: 67.470%
2022-02-22 01:58:30 - epoch 092 lr: 0.00010000000000000003
2022-02-22 01:59:08 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.5438
2022-02-22 01:59:42 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.5389
2022-02-22 02:00:15 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.5379
2022-02-22 02:00:48 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.3450
2022-02-22 02:01:22 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.6703
2022-02-22 02:01:55 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.5503
2022-02-22 02:02:28 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.5435
2022-02-22 02:03:02 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.6445
2022-02-22 02:03:34 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.4523
2022-02-22 02:04:09 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.6691
2022-02-22 02:04:41 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.4191
2022-02-22 02:05:15 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.4338
2022-02-22 02:05:47 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.4471
2022-02-22 02:06:21 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.4111
2022-02-22 02:06:54 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.5120
2022-02-22 02:07:28 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.5193
2022-02-22 02:08:02 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.5202
2022-02-22 02:08:34 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.4065
2022-02-22 02:09:08 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.4368
2022-02-22 02:09:42 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.3728
2022-02-22 02:10:14 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.4927
2022-02-22 02:10:48 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.6527
2022-02-22 02:11:21 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.6012
2022-02-22 02:11:55 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.3843
2022-02-22 02:12:28 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.4547
2022-02-22 02:13:02 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.6048
2022-02-22 02:13:35 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.4935
2022-02-22 02:14:09 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.4557
2022-02-22 02:14:41 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.5841
2022-02-22 02:15:15 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.4542
2022-02-22 02:15:47 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.4278
2022-02-22 02:16:20 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.3063
2022-02-22 02:16:54 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.5812
2022-02-22 02:17:26 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.6462
2022-02-22 02:18:00 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.4603
2022-02-22 02:18:33 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.4493
2022-02-22 02:19:06 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.4420
2022-02-22 02:19:39 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.6992
2022-02-22 02:20:12 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.6528
2022-02-22 02:20:46 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.6687
2022-02-22 02:21:19 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.2420
2022-02-22 02:21:52 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.4784
2022-02-22 02:22:24 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.5760
2022-02-22 02:22:58 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.3791
2022-02-22 02:23:31 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.4655
2022-02-22 02:24:04 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.4199
2022-02-22 02:24:37 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 1.2273
2022-02-22 02:25:11 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.3721
2022-02-22 02:25:43 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.4334
2022-02-22 02:26:16 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.6204
2022-02-22 02:26:18 - train: epoch 092, train_loss: 1.5138
2022-02-22 02:27:33 - eval: epoch: 092, acc1: 67.438%, acc5: 87.842%, test_loss: 1.3230, per_image_load_time: 1.626ms, per_image_inference_time: 0.151ms
2022-02-22 02:27:33 - until epoch: 092, best_acc1: 67.470%
2022-02-22 02:27:33 - epoch 093 lr: 0.00010000000000000003
2022-02-22 02:28:12 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.4041
2022-02-22 02:28:44 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.3360
2022-02-22 02:29:18 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.5800
2022-02-22 02:29:51 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.4829
2022-02-22 02:30:24 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.7098
2022-02-22 02:30:56 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.4307
2022-02-22 02:31:30 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.5450
2022-02-22 02:32:03 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 1.3907
2022-02-22 02:32:36 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.4343
2022-02-22 02:33:10 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.3666
2022-02-22 02:33:43 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.4155
2022-02-22 02:34:16 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.5022
2022-02-22 02:34:49 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.4739
2022-02-22 02:35:23 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.4538
2022-02-22 02:35:55 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.6401
2022-02-22 02:36:29 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.5008
2022-02-22 02:37:02 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.4375
2022-02-22 02:37:35 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.5346
2022-02-22 02:38:08 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.4109
2022-02-22 02:38:41 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.4118
2022-02-22 02:39:14 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.3925
2022-02-22 02:39:48 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.6547
2022-02-22 02:40:21 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.5872
2022-02-22 02:40:54 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.2896
2022-02-22 02:41:27 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.5424
2022-02-22 02:42:02 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.7166
2022-02-22 02:42:34 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.4505
2022-02-22 02:43:08 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.4972
2022-02-22 02:43:41 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.5471
2022-02-22 02:44:14 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.3347
2022-02-22 02:44:47 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.6040
2022-02-22 02:45:20 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.4053
2022-02-22 02:45:54 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.5577
2022-02-22 02:46:27 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.7563
2022-02-22 02:47:01 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.2283
2022-02-22 02:47:33 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.6810
2022-02-22 02:48:08 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.4122
2022-02-22 02:48:40 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.3871
2022-02-22 02:49:14 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.5216
2022-02-22 02:49:47 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.7498
2022-02-22 02:50:21 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.5317
2022-02-22 02:50:54 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.6447
2022-02-22 02:51:28 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.7429
2022-02-22 02:52:00 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.4727
2022-02-22 02:52:34 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.4723
2022-02-22 02:53:06 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.2933
2022-02-22 02:53:40 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.8706
2022-02-22 02:54:13 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.4513
2022-02-22 02:54:48 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.4889
2022-02-22 02:55:19 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.4550
2022-02-22 02:55:21 - train: epoch 093, train_loss: 1.5110
2022-02-22 02:56:37 - eval: epoch: 093, acc1: 67.580%, acc5: 87.832%, test_loss: 1.3215, per_image_load_time: 1.597ms, per_image_inference_time: 0.149ms
2022-02-22 02:56:37 - until epoch: 093, best_acc1: 67.580%
2022-02-22 02:56:37 - epoch 094 lr: 0.00010000000000000003
2022-02-22 02:57:16 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.5872
2022-02-22 02:57:48 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.4398
2022-02-22 02:58:22 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.7137
2022-02-22 02:58:55 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.6880
2022-02-22 02:59:28 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.4671
2022-02-22 03:00:01 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.3576
2022-02-22 03:00:35 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.6333
2022-02-22 03:01:08 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.4929
2022-02-22 03:01:41 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.4757
2022-02-22 03:02:15 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.6512
2022-02-22 03:02:48 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.7321
2022-02-22 03:03:20 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.4831
2022-02-22 03:03:54 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.4109
2022-02-22 03:04:27 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.4962
2022-02-22 03:05:00 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.5272
2022-02-22 03:05:33 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.9163
2022-02-22 03:06:07 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.4289
2022-02-22 03:06:40 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.3897
2022-02-22 03:07:14 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.4763
2022-02-22 03:07:47 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.3090
2022-02-22 03:08:20 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.5615
2022-02-22 03:08:54 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.3237
2022-02-22 03:09:28 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.3442
2022-02-22 03:10:02 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.4820
2022-02-22 03:10:34 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.5036
2022-02-22 03:11:08 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.2910
2022-02-22 03:11:43 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.4678
2022-02-22 03:12:15 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.5439
2022-02-22 03:12:49 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.5120
2022-02-22 03:13:23 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.4125
2022-02-22 03:13:54 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.6184
2022-02-22 03:14:28 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.4850
2022-02-22 03:15:02 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.4232
2022-02-22 03:15:36 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.5138
2022-02-22 03:16:09 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.7232
2022-02-22 03:16:43 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.5393
2022-02-22 03:17:15 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.7036
2022-02-22 03:17:49 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.4242
2022-02-22 03:18:22 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.3836
2022-02-22 03:18:55 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.5955
2022-02-22 03:19:29 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.7243
2022-02-22 03:20:03 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.3478
2022-02-22 03:20:34 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.6256
2022-02-22 03:21:08 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.5839
2022-02-22 03:21:42 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.5796
2022-02-22 03:22:16 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.6491
2022-02-22 03:22:49 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.4084
2022-02-22 03:23:23 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.4637
2022-02-22 03:23:57 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.4662
2022-02-22 03:24:28 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.4007
2022-02-22 03:24:30 - train: epoch 094, train_loss: 1.5105
2022-02-22 03:25:46 - eval: epoch: 094, acc1: 67.514%, acc5: 87.818%, test_loss: 1.3195, per_image_load_time: 2.469ms, per_image_inference_time: 0.183ms
2022-02-22 03:25:46 - until epoch: 094, best_acc1: 67.580%
2022-02-22 03:25:46 - epoch 095 lr: 0.00010000000000000003
2022-02-22 03:26:26 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.4097
2022-02-22 03:26:59 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.6964
2022-02-22 03:27:32 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.4712
2022-02-22 03:28:05 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.5607
2022-02-22 03:28:39 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.6395
2022-02-22 03:29:11 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.4232
2022-02-22 03:29:45 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.7075
2022-02-22 03:30:19 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.5990
2022-02-22 03:30:52 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.7495
2022-02-22 03:31:25 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.4996
2022-02-22 03:31:58 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.3560
2022-02-22 03:32:31 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.4499
2022-02-22 03:33:04 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.4811
2022-02-22 03:33:38 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.5627
2022-02-22 03:34:11 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.4568
2022-02-22 03:34:45 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 1.2162
2022-02-22 03:35:17 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.4083
2022-02-22 03:35:51 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.5197
2022-02-22 03:36:24 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.4844
2022-02-22 03:36:58 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.6242
2022-02-22 03:37:31 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.5876
2022-02-22 03:38:05 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.1983
2022-02-22 03:38:38 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.5770
2022-02-22 03:39:12 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.5648
2022-02-22 03:39:44 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.3758
2022-02-22 03:40:18 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.5050
2022-02-22 03:40:51 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.4971
2022-02-22 03:41:24 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.4188
2022-02-22 03:41:58 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.4287
2022-02-22 03:42:30 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.6838
2022-02-22 03:43:05 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.6903
2022-02-22 03:43:37 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.5485
2022-02-22 03:44:11 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.5940
2022-02-22 03:44:44 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.3575
2022-02-22 03:45:18 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.4317
2022-02-22 03:45:51 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.3761
2022-02-22 03:46:24 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.4391
2022-02-22 03:46:58 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.3727
2022-02-22 03:47:30 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.6114
2022-02-22 03:48:04 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.2948
2022-02-22 03:48:37 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.4599
2022-02-22 03:49:10 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.4522
2022-02-22 03:49:44 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.5461
2022-02-22 03:50:17 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.5663
2022-02-22 03:50:51 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.4206
2022-02-22 03:51:23 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.5176
2022-02-22 03:51:57 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.3506
2022-02-22 03:52:30 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.5030
2022-02-22 03:53:04 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.3348
2022-02-22 03:53:35 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.2989
2022-02-22 03:53:37 - train: epoch 095, train_loss: 1.5100
2022-02-22 03:54:52 - eval: epoch: 095, acc1: 67.528%, acc5: 87.830%, test_loss: 1.3206, per_image_load_time: 1.811ms, per_image_inference_time: 0.186ms
2022-02-22 03:54:52 - until epoch: 095, best_acc1: 67.580%
2022-02-22 03:54:52 - epoch 096 lr: 0.00010000000000000003
2022-02-22 03:55:31 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.5446
2022-02-22 03:56:04 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.5318
2022-02-22 03:56:37 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.5088
2022-02-22 03:57:11 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.4234
2022-02-22 03:57:43 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.3903
2022-02-22 03:58:17 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.5971
2022-02-22 03:58:50 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.4856
2022-02-22 03:59:23 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.4839
2022-02-22 03:59:57 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.5146
2022-02-22 04:00:31 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.4727
2022-02-22 04:01:03 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.6148
2022-02-22 04:01:37 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.5266
2022-02-22 04:02:09 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.5603
2022-02-22 04:02:43 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.4141
2022-02-22 04:03:16 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.3829
2022-02-22 04:03:49 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.1428
2022-02-22 04:04:22 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.5104
2022-02-22 04:04:55 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.6076
2022-02-22 04:05:28 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.6188
2022-02-22 04:06:02 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.4052
2022-02-22 04:06:36 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.4408
2022-02-22 04:07:08 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.3248
2022-02-22 04:07:42 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.3592
2022-02-22 04:08:14 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.4697
2022-02-22 04:08:48 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.3036
2022-02-22 04:09:21 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.4030
2022-02-22 04:09:53 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.6277
2022-02-22 04:10:28 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.6291
2022-02-22 04:11:00 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.4355
2022-02-22 04:11:34 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.5867
2022-02-22 04:12:08 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.5652
2022-02-22 04:12:41 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.5710
2022-02-22 04:13:14 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.6578
2022-02-22 04:13:48 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.3168
2022-02-22 04:14:21 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.3730
2022-02-22 04:14:55 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.3856
2022-02-22 04:15:27 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.5749
2022-02-22 04:16:01 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.3883
2022-02-22 04:16:33 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.4100
2022-02-22 04:17:07 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.3620
2022-02-22 04:17:40 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.4461
2022-02-22 04:18:13 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.5237
2022-02-22 04:18:46 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.2146
2022-02-22 04:19:19 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.4667
2022-02-22 04:19:53 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.4396
2022-02-22 04:20:27 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.4326
2022-02-22 04:20:59 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.5379
2022-02-22 04:21:32 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.5463
2022-02-22 04:22:07 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.6986
2022-02-22 04:22:38 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.5085
2022-02-22 04:22:41 - train: epoch 096, train_loss: 1.5071
2022-02-22 04:23:57 - eval: epoch: 096, acc1: 67.590%, acc5: 87.862%, test_loss: 1.3185, per_image_load_time: 2.755ms, per_image_inference_time: 0.139ms
2022-02-22 04:23:57 - until epoch: 096, best_acc1: 67.590%
2022-02-22 04:23:57 - epoch 097 lr: 0.00010000000000000003
2022-02-22 04:24:35 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.5460
2022-02-22 04:25:09 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.3083
2022-02-22 04:25:41 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.6947
2022-02-22 04:26:15 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.8096
2022-02-22 04:26:48 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.5392
2022-02-22 04:27:22 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.6334
2022-02-22 04:27:55 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.5049
2022-02-22 04:28:27 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.3401
2022-02-22 04:29:02 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.4046
2022-02-22 04:29:34 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.4417
2022-02-22 04:30:07 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.3534
2022-02-22 04:30:41 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.4661
2022-02-22 04:31:13 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.3501
2022-02-22 04:31:46 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.4098
2022-02-22 04:32:19 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.4540
2022-02-22 04:32:53 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.4501
2022-02-22 04:33:26 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 1.3598
2022-02-22 04:33:59 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.6094
2022-02-22 04:34:32 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.4669
2022-02-22 04:35:06 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.5102
2022-02-22 04:35:39 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.5160
2022-02-22 04:36:12 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.6075
2022-02-22 04:36:45 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.4053
2022-02-22 04:37:18 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.5488
2022-02-22 04:37:52 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.4715
2022-02-22 04:38:24 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.6800
2022-02-22 04:38:58 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.3027
2022-02-22 04:39:31 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.3941
2022-02-22 04:40:04 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.6932
2022-02-22 04:40:37 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.5373
2022-02-22 04:41:10 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.3862
2022-02-22 04:41:44 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.5700
2022-02-22 04:42:17 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.6333
2022-02-22 04:42:51 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.7360
2022-02-22 04:43:24 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.6035
2022-02-22 04:43:58 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.5006
2022-02-22 04:44:30 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.1833
2022-02-22 04:45:03 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.3395
2022-02-22 04:45:35 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.4765
2022-02-22 04:46:09 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.6374
2022-02-22 04:46:42 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.5521
2022-02-22 04:47:15 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.4041
2022-02-22 04:47:48 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.3606
2022-02-22 04:48:22 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.4526
2022-02-22 04:48:53 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.4837
2022-02-22 04:49:27 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.5769
2022-02-22 04:50:01 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.5160
2022-02-22 04:50:35 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.4306
2022-02-22 04:51:08 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.6715
2022-02-22 04:51:40 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.5020
2022-02-22 04:51:43 - train: epoch 097, train_loss: 1.5068
2022-02-22 04:52:58 - eval: epoch: 097, acc1: 67.576%, acc5: 87.926%, test_loss: 1.3164, per_image_load_time: 1.485ms, per_image_inference_time: 0.163ms
2022-02-22 04:52:58 - until epoch: 097, best_acc1: 67.590%
2022-02-22 04:52:58 - epoch 098 lr: 0.00010000000000000003
2022-02-22 04:53:37 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.5830
2022-02-22 04:54:10 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.5694
2022-02-22 04:54:43 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.7513
2022-02-22 04:55:17 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.3937
2022-02-22 04:55:49 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.5101
2022-02-22 04:56:23 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.5748
2022-02-22 04:56:57 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.5610
2022-02-22 04:57:29 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.5165
2022-02-22 04:58:03 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.5023
2022-02-22 04:58:36 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.3679
2022-02-22 04:59:09 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.3019
2022-02-22 04:59:43 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.3870
2022-02-22 05:00:16 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.6532
2022-02-22 05:00:49 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.5043
2022-02-22 05:01:22 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.3742
2022-02-22 05:01:56 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.3826
2022-02-22 05:02:29 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.6273
2022-02-22 05:03:03 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.5537
2022-02-22 05:03:35 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.3757
2022-02-22 05:04:09 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.6368
2022-02-22 05:04:42 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.4142
2022-02-22 05:05:16 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.4751
2022-02-22 05:05:48 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.4648
2022-02-22 05:06:22 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.5456
2022-02-22 05:06:55 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.6782
2022-02-22 05:07:29 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 1.4632
2022-02-22 05:08:02 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.5964
2022-02-22 05:08:35 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.4377
2022-02-22 05:09:09 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.4337
2022-02-22 05:09:42 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.4883
2022-02-22 05:10:15 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.4627
2022-02-22 05:10:49 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.2537
2022-02-22 05:11:22 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.3209
