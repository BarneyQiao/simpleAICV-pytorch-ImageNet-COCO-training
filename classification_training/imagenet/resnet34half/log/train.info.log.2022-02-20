2022-02-20 05:11:25 - network: resnet34half
2022-02-20 05:11:25 - num_classes: 1000
2022-02-20 05:11:25 - input_image_size: 224
2022-02-20 05:11:25 - scale: 1.1428571428571428
2022-02-20 05:11:25 - trained_model_path: 
2022-02-20 05:11:25 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-20 05:11:25 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f74e48b5220>
2022-02-20 05:11:25 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f74e48b54f0>
2022-02-20 05:11:25 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f74e48b5520>
2022-02-20 05:11:25 - seed: 0
2022-02-20 05:11:25 - batch_size: 256
2022-02-20 05:11:25 - num_workers: 16
2022-02-20 05:11:25 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-20 05:11:25 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-20 05:11:25 - epochs: 100
2022-02-20 05:11:25 - print_interval: 100
2022-02-20 05:11:25 - distributed: True
2022-02-20 05:11:25 - sync_bn: False
2022-02-20 05:11:25 - apex: True
2022-02-20 05:11:25 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-20 05:11:25 - gpus_num: 2
2022-02-20 05:11:25 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f74c5793230>
2022-02-20 05:11:30 - --------------------parameters--------------------
2022-02-20 05:11:30 - name: conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-20 05:11:30 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-20 05:11:30 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-20 05:11:30 - name: fc.weight, grad: True
2022-02-20 05:11:30 - name: fc.bias, grad: True
2022-02-20 05:11:30 - --------------------buffers--------------------
2022-02-20 05:11:30 - name: conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-20 05:11:30 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-20 05:11:30 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-20 05:11:30 - epoch 001 lr: 0.1
2022-02-20 05:12:08 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8950
2022-02-20 05:12:41 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7800
2022-02-20 05:13:16 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6610
2022-02-20 05:13:49 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5662
2022-02-20 05:14:23 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4577
2022-02-20 05:14:56 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.1484
2022-02-20 05:15:29 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.0760
2022-02-20 05:16:04 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.1299
2022-02-20 05:16:37 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.0438
2022-02-20 05:17:11 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.9016
2022-02-20 05:17:44 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.8238
2022-02-20 05:18:18 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.6601
2022-02-20 05:18:50 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.5800
2022-02-20 05:19:24 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.5827
2022-02-20 05:19:58 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.5144
2022-02-20 05:20:31 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.6360
2022-02-20 05:21:04 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.3411
2022-02-20 05:21:38 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.4657
2022-02-20 05:22:11 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.2418
2022-02-20 05:22:45 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.1098
2022-02-20 05:23:19 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.1241
2022-02-20 05:23:52 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.0870
2022-02-20 05:24:26 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.9622
2022-02-20 05:24:59 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.9956
2022-02-20 05:25:33 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9778
2022-02-20 05:26:06 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.0599
2022-02-20 05:26:40 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.1252
2022-02-20 05:27:13 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.9118
2022-02-20 05:27:47 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.6635
2022-02-20 05:28:20 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.8030
2022-02-20 05:28:54 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.9623
2022-02-20 05:29:27 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.7607
2022-02-20 05:30:01 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.4714
2022-02-20 05:30:34 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.5840
2022-02-20 05:31:08 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.5423
2022-02-20 05:31:41 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.6562
2022-02-20 05:32:15 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.7296
2022-02-20 05:32:48 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.4314
2022-02-20 05:33:22 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.5636
2022-02-20 05:33:56 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.5208
2022-02-20 05:34:29 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.5901
2022-02-20 05:35:04 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.3431
2022-02-20 05:35:37 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.3528
2022-02-20 05:36:11 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.0311
2022-02-20 05:36:46 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.3830
2022-02-20 05:37:20 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.5555
2022-02-20 05:37:53 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.1010
2022-02-20 05:38:28 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.4978
2022-02-20 05:39:02 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.1846
2022-02-20 05:39:35 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.1273
2022-02-20 05:39:36 - train: epoch 001, train_loss: 5.1582
2022-02-20 05:40:51 - eval: epoch: 001, acc1: 17.430%, acc5: 39.164%, test_loss: 4.1966, per_image_load_time: 2.717ms, per_image_inference_time: 0.169ms
2022-02-20 05:40:52 - until epoch: 001, best_acc1: 17.430%
2022-02-20 05:40:52 - epoch 002 lr: 0.1
2022-02-20 05:41:30 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.2209
2022-02-20 05:42:04 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.0230
2022-02-20 05:42:37 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.3187
2022-02-20 05:43:12 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2140
2022-02-20 05:43:45 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.9979
2022-02-20 05:44:17 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.9142
2022-02-20 05:44:52 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.2601
2022-02-20 05:45:25 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.7897
2022-02-20 05:46:00 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.5443
2022-02-20 05:46:33 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.1148
2022-02-20 05:47:07 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.1537
2022-02-20 05:47:41 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.0356
2022-02-20 05:48:15 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.9266
2022-02-20 05:48:48 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1234
2022-02-20 05:49:21 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.9355
2022-02-20 05:49:55 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8224
2022-02-20 05:50:29 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9448
2022-02-20 05:51:03 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.9106
2022-02-20 05:51:36 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8913
2022-02-20 05:52:11 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.5912
2022-02-20 05:52:44 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.7829
2022-02-20 05:53:18 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6575
2022-02-20 05:53:51 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.8012
2022-02-20 05:54:24 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6111
2022-02-20 05:54:59 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.6326
2022-02-20 05:55:32 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.5419
2022-02-20 05:56:06 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.7923
2022-02-20 05:56:39 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.7397
2022-02-20 05:57:13 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.7671
2022-02-20 05:57:46 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5557
2022-02-20 05:58:20 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6667
2022-02-20 05:58:53 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.7136
2022-02-20 05:59:27 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6966
2022-02-20 06:00:00 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.7766
2022-02-20 06:00:34 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6098
2022-02-20 06:01:07 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.5841
2022-02-20 06:01:41 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7049
2022-02-20 06:02:14 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.4686
2022-02-20 06:02:48 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6301
2022-02-20 06:03:21 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5703
2022-02-20 06:03:56 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.6521
2022-02-20 06:04:30 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5871
2022-02-20 06:05:02 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.5609
2022-02-20 06:05:36 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.4013
2022-02-20 06:06:10 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.3779
2022-02-20 06:06:44 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.5661
2022-02-20 06:07:17 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.5683
2022-02-20 06:07:52 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.4801
2022-02-20 06:08:26 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.4240
2022-02-20 06:09:00 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.3779
2022-02-20 06:09:01 - train: epoch 002, train_loss: 3.7716
2022-02-20 06:10:16 - eval: epoch: 002, acc1: 29.108%, acc5: 54.742%, test_loss: 3.3434, per_image_load_time: 2.629ms, per_image_inference_time: 0.174ms
2022-02-20 06:10:16 - until epoch: 002, best_acc1: 29.108%
2022-02-20 06:10:16 - epoch 003 lr: 0.1
2022-02-20 06:10:55 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5151
2022-02-20 06:11:29 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4906
2022-02-20 06:12:02 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.3414
2022-02-20 06:12:36 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3920
2022-02-20 06:13:09 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.5604
2022-02-20 06:13:44 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.3556
2022-02-20 06:14:16 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.6590
2022-02-20 06:14:50 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.3877
2022-02-20 06:15:24 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.3218
2022-02-20 06:15:58 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.3851
2022-02-20 06:16:32 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.2127
2022-02-20 06:17:06 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.4433
2022-02-20 06:17:38 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.4460
2022-02-20 06:18:13 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.3254
2022-02-20 06:18:46 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5656
2022-02-20 06:19:20 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3651
2022-02-20 06:19:53 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.2211
2022-02-20 06:20:27 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.2202
2022-02-20 06:21:01 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.4436
2022-02-20 06:21:35 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.6474
2022-02-20 06:22:10 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.5633
2022-02-20 06:22:43 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.8701
2022-02-20 06:23:16 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.3593
2022-02-20 06:23:50 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.3951
2022-02-20 06:24:24 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3862
2022-02-20 06:24:57 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.3084
2022-02-20 06:25:31 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.6358
2022-02-20 06:26:04 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.1554
2022-02-20 06:26:37 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.4099
2022-02-20 06:27:12 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.4512
2022-02-20 06:27:45 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.5396
2022-02-20 06:28:19 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.3246
2022-02-20 06:28:52 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.3140
2022-02-20 06:29:26 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.4459
2022-02-20 06:30:00 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.1573
2022-02-20 06:30:35 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.3178
2022-02-20 06:31:08 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.4326
2022-02-20 06:31:42 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.3041
2022-02-20 06:32:16 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.5469
2022-02-20 06:32:50 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.2955
2022-02-20 06:33:24 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.3125
2022-02-20 06:33:57 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.2626
2022-02-20 06:34:32 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.9168
2022-02-20 06:35:05 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.1086
2022-02-20 06:35:40 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.2778
2022-02-20 06:36:14 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.2504
2022-02-20 06:36:48 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.1670
2022-02-20 06:37:23 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.3072
2022-02-20 06:37:57 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.2366
2022-02-20 06:38:30 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.2111
2022-02-20 06:38:30 - train: epoch 003, train_loss: 3.3397
2022-02-20 06:39:45 - eval: epoch: 003, acc1: 30.668%, acc5: 55.986%, test_loss: 3.2908, per_image_load_time: 1.097ms, per_image_inference_time: 0.162ms
2022-02-20 06:39:45 - until epoch: 003, best_acc1: 30.668%
2022-02-20 06:39:45 - epoch 004 lr: 0.1
2022-02-20 06:40:24 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.2732
2022-02-20 06:40:58 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.1184
2022-02-20 06:41:31 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.1573
2022-02-20 06:42:06 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.1364
2022-02-20 06:42:39 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.1158
2022-02-20 06:43:12 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.4574
2022-02-20 06:43:46 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.2783
2022-02-20 06:44:19 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.9819
2022-02-20 06:44:54 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.9053
2022-02-20 06:45:28 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.1603
2022-02-20 06:46:01 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.3216
2022-02-20 06:46:36 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.0109
2022-02-20 06:47:09 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.9118
2022-02-20 06:47:42 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.1702
2022-02-20 06:48:15 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.3561
2022-02-20 06:48:49 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.1000
2022-02-20 06:49:22 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1696
2022-02-20 06:49:55 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.4293
2022-02-20 06:50:29 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.2942
2022-02-20 06:51:05 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.0921
2022-02-20 06:51:37 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.3357
2022-02-20 06:52:11 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.1365
2022-02-20 06:52:44 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.9530
2022-02-20 06:53:18 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.9442
2022-02-20 06:53:52 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.1043
2022-02-20 06:54:25 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.2781
2022-02-20 06:55:00 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.9100
2022-02-20 06:55:34 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.0561
2022-02-20 06:56:07 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.0132
2022-02-20 06:56:41 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.0800
2022-02-20 06:57:14 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.1310
2022-02-20 06:57:47 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.1440
2022-02-20 06:58:21 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.2141
2022-02-20 06:58:55 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.1793
2022-02-20 06:59:29 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.1284
2022-02-20 07:00:03 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.9393
2022-02-20 07:00:36 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.1126
2022-02-20 07:01:10 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.0334
2022-02-20 07:01:44 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.0648
2022-02-20 07:02:18 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.8536
2022-02-20 07:02:51 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.0825
2022-02-20 07:03:25 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.8682
2022-02-20 07:03:59 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.9465
2022-02-20 07:04:33 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.8949
2022-02-20 07:05:06 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.5619
2022-02-20 07:05:40 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.0787
2022-02-20 07:06:15 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.9463
2022-02-20 07:06:50 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.9961
2022-02-20 07:07:23 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.9176
2022-02-20 07:07:57 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.1245
2022-02-20 07:07:57 - train: epoch 004, train_loss: 3.1326
2022-02-20 07:09:12 - eval: epoch: 004, acc1: 35.330%, acc5: 62.242%, test_loss: 2.9661, per_image_load_time: 2.675ms, per_image_inference_time: 0.176ms
2022-02-20 07:09:12 - until epoch: 004, best_acc1: 35.330%
2022-02-20 07:09:12 - epoch 005 lr: 0.1
2022-02-20 07:09:51 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.0966
2022-02-20 07:10:25 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0548
2022-02-20 07:10:59 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.1412
2022-02-20 07:11:31 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.0822
2022-02-20 07:12:05 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.9280
2022-02-20 07:12:38 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.9602
2022-02-20 07:13:13 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.0409
2022-02-20 07:13:46 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.2862
2022-02-20 07:14:20 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.0570
2022-02-20 07:14:52 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.1569
2022-02-20 07:15:26 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.1057
2022-02-20 07:16:00 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.1160
2022-02-20 07:16:34 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.9801
2022-02-20 07:17:07 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0275
2022-02-20 07:17:41 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.9268
2022-02-20 07:18:14 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.7594
2022-02-20 07:18:48 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.9771
2022-02-20 07:19:21 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.0597
2022-02-20 07:19:55 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.8659
2022-02-20 07:20:28 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.0336
2022-02-20 07:21:02 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7906
2022-02-20 07:21:35 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.9365
2022-02-20 07:22:10 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.9038
2022-02-20 07:22:43 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.0221
2022-02-20 07:23:17 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.0834
2022-02-20 07:23:51 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.1766
2022-02-20 07:24:24 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.1565
2022-02-20 07:24:59 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.9585
2022-02-20 07:25:32 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.8806
2022-02-20 07:26:06 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.9540
2022-02-20 07:26:39 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.0565
2022-02-20 07:27:13 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.0123
2022-02-20 07:27:47 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.8507
2022-02-20 07:28:21 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.0116
2022-02-20 07:28:54 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.9716
2022-02-20 07:29:28 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.0257
2022-02-20 07:30:02 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.9330
2022-02-20 07:30:36 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.7848
2022-02-20 07:31:09 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.1799
2022-02-20 07:31:43 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8996
2022-02-20 07:32:17 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.9222
2022-02-20 07:32:50 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.0094
2022-02-20 07:33:25 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.7338
2022-02-20 07:33:59 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.9685
2022-02-20 07:34:33 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.1023
2022-02-20 07:35:07 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.9033
2022-02-20 07:35:40 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.7552
2022-02-20 07:36:14 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.8349
2022-02-20 07:36:49 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.1342
2022-02-20 07:37:21 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.7834
2022-02-20 07:37:23 - train: epoch 005, train_loss: 3.0145
2022-02-20 07:38:38 - eval: epoch: 005, acc1: 37.798%, acc5: 64.716%, test_loss: 2.8117, per_image_load_time: 1.273ms, per_image_inference_time: 0.181ms
2022-02-20 07:38:38 - until epoch: 005, best_acc1: 37.798%
2022-02-20 07:38:38 - epoch 006 lr: 0.1
2022-02-20 07:39:17 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.9495
2022-02-20 07:39:50 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.9465
2022-02-20 07:40:24 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.8326
2022-02-20 07:40:59 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.0575
2022-02-20 07:41:31 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.9580
2022-02-20 07:42:06 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.9705
2022-02-20 07:42:39 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.0418
2022-02-20 07:43:12 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.0229
2022-02-20 07:43:47 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.8090
2022-02-20 07:44:21 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.8524
2022-02-20 07:44:53 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.8650
2022-02-20 07:45:28 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.0692
2022-02-20 07:46:00 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.0850
2022-02-20 07:46:35 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.0680
2022-02-20 07:47:09 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.0418
2022-02-20 07:47:42 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.6926
2022-02-20 07:48:16 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.0404
2022-02-20 07:48:49 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.0859
2022-02-20 07:49:23 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.9644
2022-02-20 07:49:57 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.2417
2022-02-20 07:50:31 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.0310
2022-02-20 07:51:05 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.7907
2022-02-20 07:51:38 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.8058
2022-02-20 07:52:13 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.9724
2022-02-20 07:52:46 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.2987
2022-02-20 07:53:19 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.7646
2022-02-20 07:53:53 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.9747
2022-02-20 07:54:27 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7748
2022-02-20 07:55:00 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.0916
2022-02-20 07:55:35 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 3.0039
2022-02-20 07:56:08 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.6533
2022-02-20 07:56:41 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.8127
2022-02-20 07:57:15 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.8339
2022-02-20 07:57:49 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.0225
2022-02-20 07:58:23 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.9666
2022-02-20 07:58:56 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.9192
2022-02-20 07:59:29 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.9716
2022-02-20 08:00:04 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.7564
2022-02-20 08:00:37 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.8090
2022-02-20 08:01:11 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.2009
2022-02-20 08:01:45 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.9243
2022-02-20 08:02:18 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.7164
2022-02-20 08:02:52 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.9078
2022-02-20 08:03:25 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.9589
2022-02-20 08:03:58 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.9158
2022-02-20 08:04:32 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.8837
2022-02-20 08:05:05 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.9424
2022-02-20 08:05:40 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.8493
2022-02-20 08:06:14 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.9537
2022-02-20 08:06:47 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.8672
2022-02-20 08:06:48 - train: epoch 006, train_loss: 2.9393
2022-02-20 08:08:02 - eval: epoch: 006, acc1: 39.482%, acc5: 66.572%, test_loss: 2.7198, per_image_load_time: 1.991ms, per_image_inference_time: 0.167ms
2022-02-20 08:08:02 - until epoch: 006, best_acc1: 39.482%
2022-02-20 08:08:02 - epoch 007 lr: 0.1
2022-02-20 08:08:42 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.6892
2022-02-20 08:09:15 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.1208
2022-02-20 08:09:49 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.1766
2022-02-20 08:10:23 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.8531
2022-02-20 08:10:57 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.7530
2022-02-20 08:11:31 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.9565
2022-02-20 08:12:06 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.8645
2022-02-20 08:12:38 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.8970
2022-02-20 08:13:12 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.9458
2022-02-20 08:13:45 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.8944
2022-02-20 08:14:20 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.7207
2022-02-20 08:14:52 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.8693
2022-02-20 08:15:27 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.8041
2022-02-20 08:15:59 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.8716
2022-02-20 08:16:33 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.9850
2022-02-20 08:17:07 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.7837
2022-02-20 08:17:41 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.9959
2022-02-20 08:18:15 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.7825
2022-02-20 08:18:48 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.9096
2022-02-20 08:19:23 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.7317
2022-02-20 08:19:55 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.9702
2022-02-20 08:20:29 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.7358
2022-02-20 08:21:03 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.9285
2022-02-20 08:21:35 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.9826
2022-02-20 08:22:10 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.7993
2022-02-20 08:22:42 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.8374
2022-02-20 08:23:17 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.6700
2022-02-20 08:23:50 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.8474
2022-02-20 08:24:24 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.8787
2022-02-20 08:24:57 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.9407
2022-02-20 08:25:31 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.7913
2022-02-20 08:26:04 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.7613
2022-02-20 08:26:38 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.1942
2022-02-20 08:27:12 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.8061
2022-02-20 08:27:45 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.9565
2022-02-20 08:28:19 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.6951
2022-02-20 08:28:52 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.9460
2022-02-20 08:29:25 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.1580
2022-02-20 08:29:59 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.7533
2022-02-20 08:30:33 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.9861
2022-02-20 08:31:06 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.8366
2022-02-20 08:31:41 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.8368
2022-02-20 08:32:13 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.1129
2022-02-20 08:32:47 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.7744
2022-02-20 08:33:20 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.0136
2022-02-20 08:33:55 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.8571
2022-02-20 08:34:29 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.9453
2022-02-20 08:35:04 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.0911
2022-02-20 08:35:38 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.9052
2022-02-20 08:36:12 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.8575
2022-02-20 08:36:12 - train: epoch 007, train_loss: 2.8849
2022-02-20 08:37:28 - eval: epoch: 007, acc1: 39.038%, acc5: 64.946%, test_loss: 2.7820, per_image_load_time: 1.111ms, per_image_inference_time: 0.147ms
2022-02-20 08:37:28 - until epoch: 007, best_acc1: 39.482%
2022-02-20 08:37:28 - epoch 008 lr: 0.1
2022-02-20 08:38:06 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.7548
2022-02-20 08:38:41 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.0127
2022-02-20 08:39:13 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.6267
2022-02-20 08:39:48 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.6603
2022-02-20 08:40:20 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.6944
2022-02-20 08:40:53 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.8631
2022-02-20 08:41:28 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.2017
2022-02-20 08:42:01 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.6844
2022-02-20 08:42:36 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.7770
2022-02-20 08:43:08 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.8661
2022-02-20 08:43:43 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.7174
2022-02-20 08:44:16 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.5503
2022-02-20 08:44:49 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.9271
2022-02-20 08:45:24 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.6754
2022-02-20 08:45:56 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.8545
2022-02-20 08:46:31 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.9238
2022-02-20 08:47:04 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.5945
2022-02-20 08:47:39 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.9918
2022-02-20 08:48:11 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.6124
2022-02-20 08:48:45 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.9770
2022-02-20 08:49:19 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.8621
2022-02-20 08:49:53 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.8142
2022-02-20 08:50:27 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.8633
2022-02-20 08:51:01 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.8548
2022-02-20 08:51:34 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.9551
2022-02-20 08:52:07 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.9102
2022-02-20 08:52:41 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.8766
2022-02-20 08:53:16 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.8966
2022-02-20 08:53:49 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.8665
2022-02-20 08:54:23 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.9864
2022-02-20 08:54:56 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.7659
2022-02-20 08:55:31 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.0228
2022-02-20 08:56:04 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.0920
2022-02-20 08:56:38 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.0147
2022-02-20 08:57:11 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.7929
2022-02-20 08:57:46 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 3.0028
2022-02-20 08:58:19 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.8279
2022-02-20 08:58:53 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.7951
2022-02-20 08:59:26 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.9272
2022-02-20 09:00:01 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.1839
2022-02-20 09:00:35 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.6921
2022-02-20 09:01:09 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.8828
2022-02-20 09:01:42 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.7255
2022-02-20 09:02:17 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.8502
2022-02-20 09:02:50 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.1032
2022-02-20 09:03:24 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.0705
2022-02-20 09:03:58 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.6694
2022-02-20 09:04:33 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.8909
2022-02-20 09:05:06 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.8479
2022-02-20 09:05:40 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.8509
2022-02-20 09:05:40 - train: epoch 008, train_loss: 2.8428
2022-02-20 09:06:56 - eval: epoch: 008, acc1: 39.330%, acc5: 66.300%, test_loss: 2.7309, per_image_load_time: 2.518ms, per_image_inference_time: 0.167ms
2022-02-20 09:06:56 - until epoch: 008, best_acc1: 39.482%
2022-02-20 09:06:56 - epoch 009 lr: 0.1
2022-02-20 09:07:34 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.5897
2022-02-20 09:08:09 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.7106
2022-02-20 09:08:42 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.4837
2022-02-20 09:09:16 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.0781
2022-02-20 09:09:49 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.8506
2022-02-20 09:10:24 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.8501
2022-02-20 09:10:56 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.8138
2022-02-20 09:11:31 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.6846
2022-02-20 09:12:05 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.5998
2022-02-20 09:12:38 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.6132
2022-02-20 09:13:11 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.1824
2022-02-20 09:13:45 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.9824
2022-02-20 09:14:19 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.9205
2022-02-20 09:14:53 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.5771
2022-02-20 09:15:26 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.6256
2022-02-20 09:16:00 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.8411
2022-02-20 09:16:33 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.8792
2022-02-20 09:17:08 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.6673
2022-02-20 09:17:41 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.6002
2022-02-20 09:18:15 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.5975
2022-02-20 09:18:49 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.8701
2022-02-20 09:19:21 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.9746
2022-02-20 09:19:56 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.6252
2022-02-20 09:20:31 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.7401
2022-02-20 09:21:04 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.6982
2022-02-20 09:21:37 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.9619
2022-02-20 09:22:11 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.7195
2022-02-20 09:22:45 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.8058
2022-02-20 09:23:18 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.4723
2022-02-20 09:23:52 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.7588
2022-02-20 09:24:25 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.9237
2022-02-20 09:24:59 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.8325
2022-02-20 09:25:33 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.8147
2022-02-20 09:26:07 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.0634
2022-02-20 09:26:40 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.9535
2022-02-20 09:27:14 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.7394
2022-02-20 09:27:48 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 3.0205
2022-02-20 09:28:21 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.0225
2022-02-20 09:28:55 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.4662
2022-02-20 09:29:29 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.0262
2022-02-20 09:30:01 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.8709
2022-02-20 09:30:36 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.7066
2022-02-20 09:31:09 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.8242
2022-02-20 09:31:44 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.9208
2022-02-20 09:32:16 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.7829
2022-02-20 09:32:50 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.8075
2022-02-20 09:33:25 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.8781
2022-02-20 09:33:59 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.0778
2022-02-20 09:34:33 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.8942
2022-02-20 09:35:06 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.6612
2022-02-20 09:35:07 - train: epoch 009, train_loss: 2.8140
2022-02-20 09:36:22 - eval: epoch: 009, acc1: 41.982%, acc5: 68.664%, test_loss: 2.5766, per_image_load_time: 2.737ms, per_image_inference_time: 0.168ms
2022-02-20 09:36:22 - until epoch: 009, best_acc1: 41.982%
2022-02-20 09:36:22 - epoch 010 lr: 0.1
2022-02-20 09:37:01 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.8435
2022-02-20 09:37:35 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 3.0071
2022-02-20 09:38:09 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.8349
2022-02-20 09:38:43 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.8758
2022-02-20 09:39:16 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.6998
2022-02-20 09:39:50 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.8431
2022-02-20 09:40:22 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.8704
2022-02-20 09:40:56 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.6696
2022-02-20 09:41:30 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.5636
2022-02-20 09:42:03 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.6489
2022-02-20 09:42:37 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.7703
2022-02-20 09:43:10 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.6559
2022-02-20 09:43:45 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.5184
2022-02-20 09:44:18 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.7363
2022-02-20 09:44:53 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.4684
2022-02-20 09:45:25 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.8205
2022-02-20 09:45:59 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.9821
2022-02-20 09:46:33 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.8011
2022-02-20 09:47:07 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.7451
2022-02-20 09:47:41 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.8414
2022-02-20 09:48:14 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.7461
2022-02-20 09:48:48 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.8859
2022-02-20 09:49:22 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.9739
2022-02-20 09:49:55 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.8974
2022-02-20 09:50:30 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.8596
2022-02-20 09:51:03 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.9237
2022-02-20 09:51:37 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.5654
2022-02-20 09:52:11 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.8201
2022-02-20 09:52:43 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.8707
2022-02-20 09:53:18 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.7343
2022-02-20 09:53:51 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.9669
2022-02-20 09:54:25 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.8284
2022-02-20 09:54:59 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.9088
2022-02-20 09:55:32 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.9896
2022-02-20 09:56:06 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 3.0160
2022-02-20 09:56:39 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.0676
2022-02-20 09:57:13 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.6909
2022-02-20 09:57:47 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.7981
2022-02-20 09:58:20 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.5406
2022-02-20 09:58:55 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.7176
2022-02-20 09:59:28 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.6251
2022-02-20 10:00:03 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.7651
2022-02-20 10:00:36 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.7414
2022-02-20 10:01:10 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.7351
2022-02-20 10:01:43 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.5871
2022-02-20 10:02:18 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.8880
2022-02-20 10:02:51 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.7295
2022-02-20 10:03:26 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.6904
2022-02-20 10:04:00 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.5823
2022-02-20 10:04:33 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.5305
2022-02-20 10:04:33 - train: epoch 010, train_loss: 2.7880
2022-02-20 10:05:49 - eval: epoch: 010, acc1: 43.182%, acc5: 70.072%, test_loss: 2.5120, per_image_load_time: 2.730ms, per_image_inference_time: 0.177ms
2022-02-20 10:05:49 - until epoch: 010, best_acc1: 43.182%
2022-02-20 10:05:49 - epoch 011 lr: 0.1
2022-02-20 10:06:27 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.5442
2022-02-20 10:07:01 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.8361
2022-02-20 10:07:35 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.5945
2022-02-20 10:08:08 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.9232
2022-02-20 10:08:42 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.6113
2022-02-20 10:09:16 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.7857
2022-02-20 10:09:49 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.7862
2022-02-20 10:10:23 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.7715
2022-02-20 10:10:56 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.9478
2022-02-20 10:11:30 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.8344
2022-02-20 10:12:03 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.8132
2022-02-20 10:12:37 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.9981
2022-02-20 10:13:11 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.8522
2022-02-20 10:13:44 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.7075
2022-02-20 10:14:18 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.5627
2022-02-20 10:14:52 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.7830
2022-02-20 10:15:26 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.8259
2022-02-20 10:16:00 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.5497
2022-02-20 10:16:34 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.5915
2022-02-20 10:17:07 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.9005
2022-02-20 10:17:41 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.7916
2022-02-20 10:18:15 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.6922
2022-02-20 10:18:48 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.9833
2022-02-20 10:19:23 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.6039
2022-02-20 10:19:56 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.9564
2022-02-20 10:20:31 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.7228
2022-02-20 10:21:04 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.6618
2022-02-20 10:21:39 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.4279
2022-02-20 10:22:12 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.7141
2022-02-20 10:22:46 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.9713
2022-02-20 10:23:20 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.7978
2022-02-20 10:23:52 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.4721
2022-02-20 10:24:26 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.8423
2022-02-20 10:25:01 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.7530
2022-02-20 10:25:34 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.8396
2022-02-20 10:26:08 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.8177
2022-02-20 10:26:41 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.8096
2022-02-20 10:27:15 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.4401
2022-02-20 10:27:48 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.8213
2022-02-20 10:28:22 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.8341
2022-02-20 10:28:55 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.5864
2022-02-20 10:29:29 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.7120
2022-02-20 10:30:03 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.7338
2022-02-20 10:30:37 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.7134
2022-02-20 10:31:11 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.5733
2022-02-20 10:31:44 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.6681
2022-02-20 10:32:18 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.5021
2022-02-20 10:32:52 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.5141
2022-02-20 10:33:26 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.6720
2022-02-20 10:33:59 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.6498
2022-02-20 10:34:00 - train: epoch 011, train_loss: 2.7673
2022-02-20 10:35:14 - eval: epoch: 011, acc1: 40.478%, acc5: 67.286%, test_loss: 2.6588, per_image_load_time: 1.588ms, per_image_inference_time: 0.193ms
2022-02-20 10:35:14 - until epoch: 011, best_acc1: 43.182%
2022-02-20 10:35:14 - epoch 012 lr: 0.1
2022-02-20 10:35:53 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.5939
2022-02-20 10:36:27 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.6878
2022-02-20 10:37:00 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.7282
2022-02-20 10:37:35 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.8409
2022-02-20 10:38:08 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.9746
2022-02-20 10:38:41 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.5568
2022-02-20 10:39:15 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.7148
2022-02-20 10:39:48 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.7222
2022-02-20 10:40:22 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.7512
2022-02-20 10:40:56 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.5758
2022-02-20 10:41:29 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.1226
2022-02-20 10:42:04 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.4956
2022-02-20 10:42:36 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.7157
2022-02-20 10:43:10 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.9590
2022-02-20 10:43:43 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.4990
2022-02-20 10:44:16 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.7225
2022-02-20 10:44:51 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.7338
2022-02-20 10:45:24 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.7054
2022-02-20 10:45:59 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.8396
2022-02-20 10:46:32 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.8954
2022-02-20 10:47:05 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.7128
2022-02-20 10:47:38 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.9579
2022-02-20 10:48:13 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.7552
2022-02-20 10:48:46 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.7785
2022-02-20 10:49:20 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.4613
2022-02-20 10:49:52 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.6229
2022-02-20 10:50:26 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.6580
2022-02-20 10:51:00 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.7442
2022-02-20 10:51:34 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.5521
2022-02-20 10:52:08 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.7663
2022-02-20 10:52:42 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.8526
2022-02-20 10:53:14 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.4451
2022-02-20 10:53:48 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.6351
2022-02-20 10:54:21 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.6862
2022-02-20 10:54:55 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.9224
2022-02-20 10:55:29 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 3.0492
2022-02-20 10:56:03 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.7225
2022-02-20 10:56:37 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.7923
2022-02-20 10:57:10 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.7411
2022-02-20 10:57:44 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.7052
2022-02-20 10:58:18 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.6775
2022-02-20 10:58:52 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.6069
2022-02-20 10:59:26 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.8837
2022-02-20 11:00:00 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.5070
2022-02-20 11:00:33 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.7429
2022-02-20 11:01:08 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.0247
2022-02-20 11:01:41 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.6864
2022-02-20 11:02:16 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.7499
2022-02-20 11:02:50 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.7890
2022-02-20 11:03:22 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.4142
2022-02-20 11:03:23 - train: epoch 012, train_loss: 2.7524
2022-02-20 11:04:38 - eval: epoch: 012, acc1: 43.256%, acc5: 70.166%, test_loss: 2.5072, per_image_load_time: 1.411ms, per_image_inference_time: 0.168ms
2022-02-20 11:04:39 - until epoch: 012, best_acc1: 43.256%
2022-02-20 11:04:39 - epoch 013 lr: 0.1
2022-02-20 11:05:18 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.4944
2022-02-20 11:05:52 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.6223
2022-02-20 11:06:25 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.6110
2022-02-20 11:06:58 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.6267
2022-02-20 11:07:32 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.7041
2022-02-20 11:08:05 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.8020
2022-02-20 11:08:39 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.5997
2022-02-20 11:09:13 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.8457
2022-02-20 11:09:47 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.5958
2022-02-20 11:10:21 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.7839
2022-02-20 11:10:56 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.7531
2022-02-20 11:11:29 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.9697
2022-02-20 11:12:02 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.8018
2022-02-20 11:12:36 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.7782
2022-02-20 11:13:10 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.9292
2022-02-20 11:13:44 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.5005
2022-02-20 11:14:17 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.6751
2022-02-20 11:14:51 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.7036
2022-02-20 11:15:25 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.8259
2022-02-20 11:15:58 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.0266
2022-02-20 11:16:32 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.0160
2022-02-20 11:17:05 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.6051
2022-02-20 11:17:38 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.8345
2022-02-20 11:18:12 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.7739
2022-02-20 11:18:46 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.6353
2022-02-20 11:19:19 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.5787
2022-02-20 11:19:53 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.6379
2022-02-20 11:20:27 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.8252
2022-02-20 11:21:00 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.7097
2022-02-20 11:21:34 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.6685
2022-02-20 11:22:07 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.5549
2022-02-20 11:22:41 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.6951
2022-02-20 11:23:15 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.6073
2022-02-20 11:23:49 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.7354
2022-02-20 11:24:21 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.6444
2022-02-20 11:24:55 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.8608
2022-02-20 11:25:29 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.4069
2022-02-20 11:26:03 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.7193
2022-02-20 11:26:37 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.7441
2022-02-20 11:27:11 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.6817
2022-02-20 11:27:44 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.6337
2022-02-20 11:28:19 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.7016
2022-02-20 11:28:51 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.6914
2022-02-20 11:29:26 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.5826
2022-02-20 11:29:59 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.7024
2022-02-20 11:30:34 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.7508
2022-02-20 11:31:07 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.8203
2022-02-20 11:31:41 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.8896
2022-02-20 11:32:15 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.8588
2022-02-20 11:32:49 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.9051
2022-02-20 11:32:49 - train: epoch 013, train_loss: 2.7377
2022-02-20 11:34:04 - eval: epoch: 013, acc1: 43.100%, acc5: 69.756%, test_loss: 2.5280, per_image_load_time: 2.455ms, per_image_inference_time: 0.166ms
2022-02-20 11:34:04 - until epoch: 013, best_acc1: 43.256%
2022-02-20 11:34:04 - epoch 014 lr: 0.1
2022-02-20 11:34:43 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.7557
2022-02-20 11:35:16 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.8818
2022-02-20 11:35:50 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.4229
2022-02-20 11:36:24 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.5780
2022-02-20 11:36:58 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.6817
2022-02-20 11:37:31 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.7042
2022-02-20 11:38:06 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.7079
2022-02-20 11:38:39 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.8153
2022-02-20 11:39:13 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.8498
2022-02-20 11:39:45 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.7930
2022-02-20 11:40:19 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.6331
2022-02-20 11:40:53 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.7639
2022-02-20 11:41:27 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.7393
2022-02-20 11:42:00 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.7975
2022-02-20 11:42:33 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.7473
2022-02-20 11:43:08 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.7574
2022-02-20 11:43:41 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.8301
2022-02-20 11:44:15 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.8929
2022-02-20 11:44:49 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.5627
2022-02-20 11:45:23 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.6642
2022-02-20 11:45:56 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.9448
2022-02-20 11:46:29 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.7380
2022-02-20 11:47:03 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.7645
2022-02-20 11:47:36 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.8484
2022-02-20 11:48:10 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.6193
2022-02-20 11:48:44 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.7538
2022-02-20 11:49:17 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.5981
2022-02-20 11:49:51 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.9573
2022-02-20 11:50:24 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.5242
2022-02-20 11:50:58 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.8558
2022-02-20 11:51:32 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.5551
2022-02-20 11:52:05 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.6971
2022-02-20 11:52:38 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.6239
2022-02-20 11:53:12 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.5789
2022-02-20 11:53:45 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.6951
2022-02-20 11:54:19 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.5592
2022-02-20 11:54:52 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.6008
2022-02-20 11:55:25 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.7934
2022-02-20 11:55:59 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.5175
2022-02-20 11:56:33 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.9459
2022-02-20 11:57:06 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.7610
2022-02-20 11:57:41 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.5173
2022-02-20 11:58:14 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.6160
2022-02-20 11:58:48 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.5272
2022-02-20 11:59:21 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.7893
2022-02-20 11:59:54 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.5112
2022-02-20 12:00:28 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.6732
2022-02-20 12:01:04 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.5982
2022-02-20 12:01:38 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.5005
2022-02-20 12:02:11 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.7371
2022-02-20 12:02:12 - train: epoch 014, train_loss: 2.7272
2022-02-20 12:03:26 - eval: epoch: 014, acc1: 41.756%, acc5: 68.590%, test_loss: 2.6089, per_image_load_time: 2.605ms, per_image_inference_time: 0.177ms
2022-02-20 12:03:27 - until epoch: 014, best_acc1: 43.256%
2022-02-20 12:03:27 - epoch 015 lr: 0.1
2022-02-20 12:04:06 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.5452
2022-02-20 12:04:40 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.8560
2022-02-20 12:05:13 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.8766
2022-02-20 12:05:47 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.7357
2022-02-20 12:06:20 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.5865
2022-02-20 12:06:54 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.8250
2022-02-20 12:07:27 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.7183
2022-02-20 12:08:02 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.5264
2022-02-20 12:08:35 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.4787
2022-02-20 12:09:08 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.7474
2022-02-20 12:09:41 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.5845
2022-02-20 12:10:15 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.6102
2022-02-20 12:10:48 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 3.0077
2022-02-20 12:11:22 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.6724
2022-02-20 12:11:56 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.4807
2022-02-20 12:12:29 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.7439
2022-02-20 12:13:03 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.9455
2022-02-20 12:13:37 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.5872
2022-02-20 12:14:10 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.6216
2022-02-20 12:14:43 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.6604
2022-02-20 12:15:18 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.5607
2022-02-20 12:15:50 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.8684
2022-02-20 12:16:25 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.5235
2022-02-20 12:16:59 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.7872
2022-02-20 12:17:32 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.8211
2022-02-20 12:18:06 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.5500
2022-02-20 12:18:39 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.6902
2022-02-20 12:19:13 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.8951
2022-02-20 12:19:47 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.6852
2022-02-20 12:20:20 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.4944
2022-02-20 12:20:54 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.5999
2022-02-20 12:21:28 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.7403
2022-02-20 12:22:01 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.5229
2022-02-20 12:22:35 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.7697
2022-02-20 12:23:09 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.9390
2022-02-20 12:23:42 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.6619
2022-02-20 12:24:16 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.5356
2022-02-20 12:24:49 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.6557
2022-02-20 12:25:24 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.9147
2022-02-20 12:25:57 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.7161
2022-02-20 12:26:31 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.8959
2022-02-20 12:27:05 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.3453
2022-02-20 12:27:39 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.8153
2022-02-20 12:28:13 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.6994
2022-02-20 12:28:46 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.7552
2022-02-20 12:29:20 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.6348
2022-02-20 12:29:53 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.9267
2022-02-20 12:30:28 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.7435
2022-02-20 12:31:01 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.6982
2022-02-20 12:31:35 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.8165
2022-02-20 12:31:36 - train: epoch 015, train_loss: 2.7184
2022-02-20 12:32:51 - eval: epoch: 015, acc1: 43.618%, acc5: 70.454%, test_loss: 2.5005, per_image_load_time: 1.302ms, per_image_inference_time: 0.172ms
2022-02-20 12:32:51 - until epoch: 015, best_acc1: 43.618%
2022-02-20 12:32:51 - epoch 016 lr: 0.1
2022-02-20 12:33:30 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.6869
2022-02-20 12:34:04 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.5625
2022-02-20 12:34:37 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.7508
2022-02-20 12:35:12 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.9739
2022-02-20 12:35:45 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.4634
2022-02-20 12:36:19 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.7494
2022-02-20 12:36:52 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.4775
2022-02-20 12:37:26 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.7233
2022-02-20 12:38:00 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.8085
2022-02-20 12:38:34 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.5570
2022-02-20 12:39:07 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.6289
2022-02-20 12:39:41 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.5913
2022-02-20 12:40:15 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.8400
2022-02-20 12:40:48 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.5692
2022-02-20 12:41:23 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.7929
2022-02-20 12:41:56 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.6957
2022-02-20 12:42:31 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.6808
2022-02-20 12:43:04 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.6546
2022-02-20 12:43:38 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.7583
2022-02-20 12:44:11 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.4818
2022-02-20 12:44:45 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.7922
2022-02-20 12:45:19 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.7539
2022-02-20 12:45:52 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.8836
2022-02-20 12:46:26 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.8284
2022-02-20 12:46:59 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.5977
2022-02-20 12:47:32 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.9378
2022-02-20 12:48:06 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.6502
2022-02-20 12:48:40 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.5407
2022-02-20 12:49:13 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.8123
2022-02-20 12:49:47 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.9540
2022-02-20 12:50:21 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.8133
2022-02-20 12:50:54 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.8006
2022-02-20 12:51:27 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.7478
2022-02-20 12:52:02 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.6910
2022-02-20 12:52:35 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.5702
2022-02-20 12:53:09 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.6019
2022-02-20 12:53:43 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.8763
2022-02-20 12:54:17 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.0761
2022-02-20 12:54:51 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.8113
2022-02-20 12:55:24 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.7825
2022-02-20 12:55:58 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.6197
2022-02-20 12:56:32 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.7218
2022-02-20 12:57:07 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.4423
2022-02-20 12:57:41 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.6001
2022-02-20 12:58:15 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.7649
2022-02-20 12:58:49 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.6063
2022-02-20 12:59:23 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.9254
2022-02-20 12:59:57 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.6248
2022-02-20 13:00:31 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.7728
2022-02-20 13:01:04 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.7693
2022-02-20 13:01:05 - train: epoch 016, train_loss: 2.7092
2022-02-20 13:02:20 - eval: epoch: 016, acc1: 44.114%, acc5: 70.872%, test_loss: 2.4672, per_image_load_time: 2.138ms, per_image_inference_time: 0.159ms
2022-02-20 13:02:20 - until epoch: 016, best_acc1: 44.114%
2022-02-20 13:02:20 - epoch 017 lr: 0.1
2022-02-20 13:02:58 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.5943
2022-02-20 13:03:32 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.9716
2022-02-20 13:04:06 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.9478
2022-02-20 13:04:40 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.4972
2022-02-20 13:05:15 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.5979
2022-02-20 13:05:48 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.9817
2022-02-20 13:06:21 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.7282
2022-02-20 13:06:54 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.6283
2022-02-20 13:07:29 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.7681
2022-02-20 13:08:01 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.4510
2022-02-20 13:08:36 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.9539
2022-02-20 13:09:10 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.8635
2022-02-20 13:09:43 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.7830
2022-02-20 13:10:16 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.7915
2022-02-20 13:10:51 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.3335
2022-02-20 13:11:25 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.5529
2022-02-20 13:11:59 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.5395
2022-02-20 13:12:32 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.7978
2022-02-20 13:13:07 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.5561
2022-02-20 13:13:39 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.8409
2022-02-20 13:14:14 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.6939
2022-02-20 13:14:47 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.6996
2022-02-20 13:15:21 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.7334
2022-02-20 13:15:54 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.5571
2022-02-20 13:16:27 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.9043
2022-02-20 13:17:02 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.6525
2022-02-20 13:17:35 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.6638
2022-02-20 13:18:09 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.8525
2022-02-20 13:18:42 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.8491
2022-02-20 13:19:16 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.5546
2022-02-20 13:19:49 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.9159
2022-02-20 13:20:23 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.5267
2022-02-20 13:20:57 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.6757
2022-02-20 13:21:30 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.5423
2022-02-20 13:22:04 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.8234
2022-02-20 13:22:38 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.8608
2022-02-20 13:23:11 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.6710
2022-02-20 13:23:45 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.9513
2022-02-20 13:24:19 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.5666
2022-02-20 13:24:52 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.5430
2022-02-20 13:25:26 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.8100
2022-02-20 13:25:59 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.6530
2022-02-20 13:26:33 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.6407
2022-02-20 13:27:07 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.7726
2022-02-20 13:27:41 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.7968
2022-02-20 13:28:14 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.8349
2022-02-20 13:28:49 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.7409
2022-02-20 13:29:22 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.7592
2022-02-20 13:29:57 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.5548
2022-02-20 13:30:29 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.6082
2022-02-20 13:30:30 - train: epoch 017, train_loss: 2.7017
2022-02-20 13:31:45 - eval: epoch: 017, acc1: 43.198%, acc5: 70.146%, test_loss: 2.5091, per_image_load_time: 2.427ms, per_image_inference_time: 0.190ms
2022-02-20 13:31:45 - until epoch: 017, best_acc1: 44.114%
2022-02-20 13:31:45 - epoch 018 lr: 0.1
2022-02-20 13:32:24 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.7111
2022-02-20 13:32:58 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.8149
2022-02-20 13:33:32 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.9013
2022-02-20 13:34:05 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.8406
2022-02-20 13:34:39 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.6425
2022-02-20 13:35:13 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.8003
2022-02-20 13:35:46 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.4133
2022-02-20 13:36:20 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.7255
2022-02-20 13:36:52 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.7994
2022-02-20 13:37:26 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.5926
2022-02-20 13:37:59 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.9383
2022-02-20 13:38:34 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.7203
2022-02-20 13:39:07 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.8993
2022-02-20 13:39:41 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.6277
2022-02-20 13:40:15 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.9623
2022-02-20 13:40:48 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.6879
2022-02-20 13:41:22 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.5476
2022-02-20 13:41:56 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.3851
2022-02-20 13:42:30 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.6045
2022-02-20 13:43:04 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.9674
2022-02-20 13:43:37 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.9662
2022-02-20 13:44:11 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.6611
2022-02-20 13:44:43 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.7947
2022-02-20 13:45:17 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.4655
2022-02-20 13:45:50 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.4307
2022-02-20 13:46:24 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.5231
2022-02-20 13:46:58 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.8471
2022-02-20 13:47:32 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.4422
2022-02-20 13:48:05 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.7181
2022-02-20 13:48:39 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.5584
2022-02-20 13:49:12 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.1570
2022-02-20 13:49:46 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.5915
2022-02-20 13:50:19 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.5717
2022-02-20 13:50:54 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.5190
2022-02-20 13:51:27 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.9205
2022-02-20 13:52:01 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.7018
2022-02-20 13:52:34 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.1284
2022-02-20 13:53:08 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.7200
2022-02-20 13:53:42 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.6833
2022-02-20 13:54:15 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.5968
2022-02-20 13:54:49 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.6075
2022-02-20 13:55:23 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.6209
2022-02-20 13:55:57 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.4264
2022-02-20 13:56:30 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.8777
2022-02-20 13:57:04 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.7739
2022-02-20 13:57:38 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.5004
2022-02-20 13:58:12 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.8961
2022-02-20 13:58:46 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.8690
2022-02-20 13:59:19 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.7651
2022-02-20 13:59:53 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.8471
2022-02-20 13:59:54 - train: epoch 018, train_loss: 2.6938
2022-02-20 14:01:09 - eval: epoch: 018, acc1: 44.072%, acc5: 70.764%, test_loss: 2.4635, per_image_load_time: 2.719ms, per_image_inference_time: 0.190ms
2022-02-20 14:01:09 - until epoch: 018, best_acc1: 44.114%
2022-02-20 14:01:09 - epoch 019 lr: 0.1
2022-02-20 14:01:49 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.4976
2022-02-20 14:02:23 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.9301
2022-02-20 14:02:56 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.8608
2022-02-20 14:03:29 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.5231
2022-02-20 14:04:03 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.5401
2022-02-20 14:04:36 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.6458
2022-02-20 14:05:11 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.4004
2022-02-20 14:05:43 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.9149
2022-02-20 14:06:17 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.5669
2022-02-20 14:06:51 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.8891
2022-02-20 14:07:25 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.4993
2022-02-20 14:07:57 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.7052
2022-02-20 14:08:31 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.8181
2022-02-20 14:09:05 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.5442
2022-02-20 14:09:37 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.0682
2022-02-20 14:10:12 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.6379
2022-02-20 14:10:45 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.8239
2022-02-20 14:11:19 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.7487
2022-02-20 14:11:52 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.8590
2022-02-20 14:12:26 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.6913
2022-02-20 14:12:59 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.6600
2022-02-20 14:13:33 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.7171
2022-02-20 14:14:06 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.6733
2022-02-20 14:14:40 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.7898
2022-02-20 14:15:13 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.6276
2022-02-20 14:15:47 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.7558
2022-02-20 14:16:20 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.8344
2022-02-20 14:16:55 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.7422
2022-02-20 14:17:29 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.6365
2022-02-20 14:18:02 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.9746
2022-02-20 14:18:36 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.7166
2022-02-20 14:19:09 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.4307
2022-02-20 14:19:43 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.6628
2022-02-20 14:20:17 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.7595
2022-02-20 14:20:50 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.8299
2022-02-20 14:21:24 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.6271
2022-02-20 14:21:59 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.7409
2022-02-20 14:22:32 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.7745
2022-02-20 14:23:07 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.6255
2022-02-20 14:23:40 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.4511
2022-02-20 14:24:14 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.7722
2022-02-20 14:24:47 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.7057
2022-02-20 14:25:22 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.7135
2022-02-20 14:25:55 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.8570
2022-02-20 14:26:29 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.9572
2022-02-20 14:27:02 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.5269
2022-02-20 14:27:37 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.7438
2022-02-20 14:28:12 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.7256
2022-02-20 14:28:44 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.5716
2022-02-20 14:29:18 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.6140
2022-02-20 14:29:19 - train: epoch 019, train_loss: 2.6911
2022-02-20 14:30:34 - eval: epoch: 019, acc1: 41.608%, acc5: 68.482%, test_loss: 2.6058, per_image_load_time: 2.277ms, per_image_inference_time: 0.166ms
2022-02-20 14:30:34 - until epoch: 019, best_acc1: 44.114%
2022-02-20 14:30:34 - epoch 020 lr: 0.1
2022-02-20 14:31:13 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.8351
2022-02-20 14:31:47 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.4337
2022-02-20 14:32:21 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.8163
2022-02-20 14:32:54 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.4857
2022-02-20 14:33:28 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.6525
2022-02-20 14:34:02 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.9171
2022-02-20 14:34:36 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.6245
2022-02-20 14:35:09 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.6992
2022-02-20 14:35:43 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.0153
2022-02-20 14:36:17 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.6643
2022-02-20 14:36:51 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.5520
2022-02-20 14:37:24 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.4350
2022-02-20 14:37:59 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.6842
2022-02-20 14:38:33 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.8834
2022-02-20 14:39:07 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.7912
2022-02-20 14:39:40 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.6020
2022-02-20 14:40:14 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.4163
2022-02-20 14:40:49 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.6583
2022-02-20 14:41:22 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.5092
2022-02-20 14:41:56 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.6034
2022-02-20 14:42:30 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.8286
2022-02-20 14:43:04 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.5894
2022-02-20 14:43:37 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.6000
2022-02-20 14:44:12 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.9197
2022-02-20 14:44:46 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.5267
2022-02-20 14:45:20 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.4257
2022-02-20 14:45:53 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.7319
2022-02-20 14:46:28 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.7707
2022-02-20 14:47:01 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.8278
2022-02-20 14:47:35 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.5712
2022-02-20 14:48:08 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.7144
2022-02-20 14:48:42 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.9120
2022-02-20 14:49:15 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.5045
2022-02-20 14:49:50 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.6346
2022-02-20 14:50:23 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.5872
2022-02-20 14:50:58 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.6345
2022-02-20 14:51:31 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.5606
2022-02-20 14:52:05 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.7182
2022-02-20 14:52:38 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.6946
2022-02-20 14:53:13 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.5660
2022-02-20 14:53:46 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.4687
2022-02-20 14:54:20 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.5114
2022-02-20 14:54:53 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.7319
2022-02-20 14:55:28 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.5943
2022-02-20 14:56:01 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.7533
2022-02-20 14:56:36 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.7232
2022-02-20 14:57:09 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.4105
2022-02-20 14:57:44 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.6641
2022-02-20 14:58:18 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.7081
2022-02-20 14:58:51 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.4780
2022-02-20 14:58:52 - train: epoch 020, train_loss: 2.6796
2022-02-20 15:00:07 - eval: epoch: 020, acc1: 43.110%, acc5: 70.004%, test_loss: 2.5224, per_image_load_time: 1.794ms, per_image_inference_time: 0.185ms
2022-02-20 15:00:07 - until epoch: 020, best_acc1: 44.114%
2022-02-20 15:00:07 - epoch 021 lr: 0.1
2022-02-20 15:00:46 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.5297
2022-02-20 15:01:20 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.7969
2022-02-20 15:01:53 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.3452
2022-02-20 15:02:27 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.7867
2022-02-20 15:03:00 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.4679
2022-02-20 15:03:35 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.4756
2022-02-20 15:04:08 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.5668
2022-02-20 15:04:42 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.8746
2022-02-20 15:05:14 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.7611
2022-02-20 15:05:48 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.5448
2022-02-20 15:06:22 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.6201
2022-02-20 15:06:55 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.5266
2022-02-20 15:07:29 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.6067
2022-02-20 15:08:03 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.6525
2022-02-20 15:08:36 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.4995
2022-02-20 15:09:10 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.6846
2022-02-20 15:09:43 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.6493
2022-02-20 15:10:18 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.5654
2022-02-20 15:10:51 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.7007
2022-02-20 15:11:25 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.9415
2022-02-20 15:11:58 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.5981
2022-02-20 15:12:32 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.7917
2022-02-20 15:13:05 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.6051
2022-02-20 15:13:40 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.5773
2022-02-20 15:14:14 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.5528
2022-02-20 15:14:48 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.9189
2022-02-20 15:15:21 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.5282
2022-02-20 15:15:55 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.7288
2022-02-20 15:16:28 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.6262
2022-02-20 15:17:02 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.9316
2022-02-20 15:17:36 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.8299
2022-02-20 15:18:09 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.7305
2022-02-20 15:18:42 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.9916
2022-02-20 15:19:16 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.8309
2022-02-20 15:19:49 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.5669
2022-02-20 15:20:23 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.7539
2022-02-20 15:20:56 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.6955
2022-02-20 15:21:31 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.6163
2022-02-20 15:22:04 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.5378
2022-02-20 15:22:38 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.8776
2022-02-20 15:23:12 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.5454
2022-02-20 15:23:45 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.6926
2022-02-20 15:24:18 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.7045
2022-02-20 15:24:52 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.7170
2022-02-20 15:25:25 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.8004
2022-02-20 15:26:01 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.5460
2022-02-20 15:26:34 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.9073
2022-02-20 15:27:09 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.7394
2022-02-20 15:27:45 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.4811
2022-02-20 15:28:17 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.6341
2022-02-20 15:28:19 - train: epoch 021, train_loss: 2.6774
2022-02-20 15:29:34 - eval: epoch: 021, acc1: 44.116%, acc5: 71.076%, test_loss: 2.4571, per_image_load_time: 2.721ms, per_image_inference_time: 0.179ms
2022-02-20 15:29:34 - until epoch: 021, best_acc1: 44.116%
2022-02-20 15:29:34 - epoch 022 lr: 0.1
2022-02-20 15:30:12 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.4275
2022-02-20 15:30:46 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.5308
2022-02-20 15:31:19 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.3726
2022-02-20 15:31:52 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.5599
2022-02-20 15:32:25 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.6702
2022-02-20 15:32:58 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.7594
2022-02-20 15:33:31 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.8414
2022-02-20 15:34:05 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.7458
2022-02-20 15:34:38 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.7546
2022-02-20 15:35:13 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.6871
2022-02-20 15:35:45 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.7180
2022-02-20 15:36:20 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.3620
2022-02-20 15:36:53 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.6296
2022-02-20 15:37:25 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.8705
2022-02-20 15:38:00 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.6781
2022-02-20 15:38:33 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.4613
2022-02-20 15:39:07 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.3676
2022-02-20 15:39:39 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 3.0292
2022-02-20 15:40:13 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.5135
2022-02-20 15:40:46 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.7205
2022-02-20 15:41:19 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.6215
2022-02-20 15:41:54 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.4540
2022-02-20 15:42:25 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.7597
2022-02-20 15:43:00 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.5438
2022-02-20 15:43:33 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.5555
2022-02-20 15:44:06 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.3332
2022-02-20 15:44:40 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.4107
2022-02-20 15:45:14 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.9642
2022-02-20 15:45:47 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.4829
2022-02-20 15:46:22 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.6560
2022-02-20 15:46:54 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.8722
2022-02-20 15:47:28 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.6891
2022-02-20 15:48:01 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.7837
2022-02-20 15:48:34 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.5403
2022-02-20 15:49:08 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.8152
2022-02-20 15:49:41 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.7758
2022-02-20 15:50:15 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.8192
2022-02-20 15:50:48 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.7694
2022-02-20 15:51:22 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.5121
2022-02-20 15:51:54 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.7380
2022-02-20 15:52:29 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.5234
2022-02-20 15:53:02 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.5459
2022-02-20 15:53:36 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.8414
2022-02-20 15:54:08 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.6929
2022-02-20 15:54:43 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.7249
2022-02-20 15:55:15 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.9084
2022-02-20 15:55:48 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.7583
2022-02-20 15:56:23 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.4709
2022-02-20 15:56:56 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.5150
2022-02-20 15:57:30 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.5332
2022-02-20 15:57:31 - train: epoch 022, train_loss: 2.6735
2022-02-20 15:58:47 - eval: epoch: 022, acc1: 44.366%, acc5: 71.124%, test_loss: 2.4540, per_image_load_time: 2.246ms, per_image_inference_time: 0.167ms
2022-02-20 15:58:47 - until epoch: 022, best_acc1: 44.366%
2022-02-20 15:58:47 - epoch 023 lr: 0.1
2022-02-20 15:59:26 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.4958
2022-02-20 15:59:59 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.2931
2022-02-20 16:00:33 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.5021
2022-02-20 16:01:06 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.7434
2022-02-20 16:01:40 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.6360
2022-02-20 16:02:13 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.7021
2022-02-20 16:02:47 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.5691
2022-02-20 16:03:21 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.6127
2022-02-20 16:03:54 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.7755
2022-02-20 16:04:28 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.4551
2022-02-20 16:05:02 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.7879
2022-02-20 16:05:35 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.5738
2022-02-20 16:06:08 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.6677
2022-02-20 16:06:42 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.7762
2022-02-20 16:07:15 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.6282
2022-02-20 16:07:49 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.6697
2022-02-20 16:08:22 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.7459
2022-02-20 16:08:56 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.5869
2022-02-20 16:09:30 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.7950
2022-02-20 16:10:03 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.3657
2022-02-20 16:10:38 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.8406
2022-02-20 16:11:11 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.3792
2022-02-20 16:11:44 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.5578
2022-02-20 16:12:17 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.7483
2022-02-20 16:12:51 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.7641
2022-02-20 16:13:24 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.7309
2022-02-20 16:13:58 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.6639
2022-02-20 16:14:31 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.7517
2022-02-20 16:15:05 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.7883
2022-02-20 16:15:38 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.8410
2022-02-20 16:16:12 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.7478
2022-02-20 16:16:44 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.6867
2022-02-20 16:17:18 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.6531
2022-02-20 16:17:51 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.9124
2022-02-20 16:18:25 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.5867
2022-02-20 16:18:58 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.5565
2022-02-20 16:19:32 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.6375
2022-02-20 16:20:05 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.6789
2022-02-20 16:20:38 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.6242
2022-02-20 16:21:11 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.7198
2022-02-20 16:21:45 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.5660
2022-02-20 16:22:18 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.6109
2022-02-20 16:22:52 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.5576
2022-02-20 16:23:25 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.5063
2022-02-20 16:23:59 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.5262
2022-02-20 16:24:32 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.7123
2022-02-20 16:25:05 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.3851
2022-02-20 16:25:38 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.5583
2022-02-20 16:26:13 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.6208
2022-02-20 16:26:46 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.7380
2022-02-20 16:26:47 - train: epoch 023, train_loss: 2.6687
2022-02-20 16:28:02 - eval: epoch: 023, acc1: 44.638%, acc5: 71.502%, test_loss: 2.4286, per_image_load_time: 2.608ms, per_image_inference_time: 0.192ms
2022-02-20 16:28:02 - until epoch: 023, best_acc1: 44.638%
2022-02-20 16:28:02 - epoch 024 lr: 0.1
2022-02-20 16:28:41 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.5991
2022-02-20 16:29:15 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.5869
2022-02-20 16:29:48 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.5823
2022-02-20 16:30:22 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.7409
2022-02-20 16:30:56 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.5627
2022-02-20 16:31:28 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.3529
2022-02-20 16:32:03 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.6617
2022-02-20 16:32:37 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.5646
2022-02-20 16:33:10 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.6503
2022-02-20 16:33:43 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.5273
2022-02-20 16:34:16 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.4380
2022-02-20 16:34:50 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.4654
2022-02-20 16:35:23 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 3.0139
2022-02-20 16:35:57 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.6247
2022-02-20 16:36:30 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.8981
2022-02-20 16:37:04 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.7162
2022-02-20 16:37:36 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.6883
2022-02-20 16:38:10 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.7466
2022-02-20 16:38:43 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.5879
2022-02-20 16:39:17 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.6822
2022-02-20 16:39:50 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.6572
2022-02-20 16:40:23 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.4600
2022-02-20 16:40:56 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.6504
2022-02-20 16:41:30 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.6581
2022-02-20 16:42:03 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.5321
2022-02-20 16:42:36 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.5084
2022-02-20 16:43:11 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.6606
2022-02-20 16:43:43 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.7544
2022-02-20 16:44:18 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.7689
2022-02-20 16:44:50 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.5476
2022-02-20 16:45:24 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.5669
2022-02-20 16:45:57 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.8785
2022-02-20 16:46:31 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.3156
2022-02-20 16:47:04 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.5814
2022-02-20 16:47:38 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.4904
2022-02-20 16:48:11 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.6532
2022-02-20 16:48:45 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.6322
2022-02-20 16:49:18 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.9000
2022-02-20 16:49:52 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.5642
2022-02-20 16:50:24 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.7662
2022-02-20 16:50:57 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.6380
2022-02-20 16:51:31 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.6299
2022-02-20 16:52:04 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.6764
2022-02-20 16:52:38 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.7558
2022-02-20 16:53:11 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.5496
2022-02-20 16:53:44 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.7118
2022-02-20 16:54:18 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.6779
2022-02-20 16:54:52 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.4720
2022-02-20 16:55:26 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.5932
2022-02-20 16:55:57 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.8424
2022-02-20 16:55:59 - train: epoch 024, train_loss: 2.6656
2022-02-20 16:57:13 - eval: epoch: 024, acc1: 45.202%, acc5: 72.032%, test_loss: 2.3997, per_image_load_time: 2.714ms, per_image_inference_time: 0.158ms
2022-02-20 16:57:13 - until epoch: 024, best_acc1: 45.202%
2022-02-20 16:57:13 - epoch 025 lr: 0.1
2022-02-20 16:57:51 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.5138
2022-02-20 16:58:25 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.4875
2022-02-20 16:58:58 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.3883
2022-02-20 16:59:32 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.7451
2022-02-20 17:00:04 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.5683
2022-02-20 17:00:37 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.6634
2022-02-20 17:01:10 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.6551
2022-02-20 17:01:44 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.5934
2022-02-20 17:02:15 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.4986
2022-02-20 17:02:49 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.4543
2022-02-20 17:03:21 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.7461
2022-02-20 17:03:56 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.5745
2022-02-20 17:04:28 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.6325
2022-02-20 17:05:02 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.7582
2022-02-20 17:05:35 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.5188
2022-02-20 17:06:08 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.4148
2022-02-20 17:06:41 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.6069
2022-02-20 17:07:15 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.5822
2022-02-20 17:07:47 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.5582
2022-02-20 17:08:20 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.6242
2022-02-20 17:08:54 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.5684
2022-02-20 17:09:27 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.5174
2022-02-20 17:10:00 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.6878
2022-02-20 17:10:34 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.4134
2022-02-20 17:11:06 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.8270
2022-02-20 17:11:40 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.6563
2022-02-20 17:12:12 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.7580
2022-02-20 17:12:46 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.6293
2022-02-20 17:13:19 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.7873
2022-02-20 17:13:52 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.9290
2022-02-20 17:14:25 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.6295
2022-02-20 17:14:58 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.7676
2022-02-20 17:15:31 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.6855
2022-02-20 17:16:05 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.7591
2022-02-20 17:16:37 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.4659
2022-02-20 17:17:11 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.7015
2022-02-20 17:17:43 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.6033
2022-02-20 17:18:17 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.7744
2022-02-20 17:18:50 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.7646
2022-02-20 17:19:24 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.7836
2022-02-20 17:19:55 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.8856
2022-02-20 17:20:28 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.6534
2022-02-20 17:21:02 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.5882
2022-02-20 17:21:35 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.5906
2022-02-20 17:22:08 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.6753
2022-02-20 17:22:41 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.6495
2022-02-20 17:23:15 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.6268
2022-02-20 17:23:47 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.5348
2022-02-20 17:24:21 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.5787
2022-02-20 17:24:53 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.8161
2022-02-20 17:24:55 - train: epoch 025, train_loss: 2.6637
2022-02-20 17:26:09 - eval: epoch: 025, acc1: 46.446%, acc5: 73.216%, test_loss: 2.3296, per_image_load_time: 2.648ms, per_image_inference_time: 0.179ms
2022-02-20 17:26:09 - until epoch: 025, best_acc1: 46.446%
2022-02-20 17:26:09 - epoch 026 lr: 0.1
2022-02-20 17:26:48 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.5596
2022-02-20 17:27:21 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.4966
2022-02-20 17:27:54 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.7278
2022-02-20 17:28:26 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.5813
2022-02-20 17:28:59 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.7701
2022-02-20 17:29:32 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.7330
2022-02-20 17:30:06 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.5564
2022-02-20 17:30:38 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.5869
2022-02-20 17:31:11 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.9209
2022-02-20 17:31:45 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.6936
2022-02-20 17:32:18 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.6106
2022-02-20 17:32:51 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.6968
2022-02-20 17:33:24 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.5303
2022-02-20 17:33:58 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.6957
2022-02-20 17:34:31 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.5357
2022-02-20 17:35:05 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.8218
2022-02-20 17:35:37 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.6131
2022-02-20 17:36:12 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.8357
2022-02-20 17:36:44 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.9778
2022-02-20 17:37:18 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.9122
2022-02-20 17:37:51 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.7892
2022-02-20 17:38:25 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.6864
2022-02-20 17:38:58 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.5711
2022-02-20 17:39:33 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.8648
2022-02-20 17:40:05 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.9542
2022-02-20 17:40:39 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.6972
2022-02-20 17:41:11 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.6299
2022-02-20 17:41:45 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.4994
2022-02-20 17:42:17 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.8367
2022-02-20 17:42:50 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.6038
2022-02-20 17:43:22 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.6130
2022-02-20 17:43:57 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.6467
2022-02-20 17:44:29 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.4243
2022-02-20 17:45:02 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.7925
2022-02-20 17:45:35 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.6027
2022-02-20 17:46:09 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.6144
2022-02-20 17:46:41 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.8649
2022-02-20 17:47:14 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.7925
2022-02-20 17:47:47 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.8163
2022-02-20 17:48:20 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.8056
2022-02-20 17:48:53 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.5931
2022-02-20 17:49:26 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.8240
2022-02-20 17:50:00 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.5981
2022-02-20 17:50:32 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.8066
2022-02-20 17:51:07 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.9239
2022-02-20 17:51:39 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.5577
2022-02-20 17:52:12 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.4947
2022-02-20 17:52:46 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.6579
2022-02-20 17:53:20 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.7027
2022-02-20 17:53:53 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.8739
2022-02-20 17:53:54 - train: epoch 026, train_loss: 2.6575
2022-02-20 17:55:09 - eval: epoch: 026, acc1: 44.514%, acc5: 71.204%, test_loss: 2.4504, per_image_load_time: 2.064ms, per_image_inference_time: 0.163ms
2022-02-20 17:55:09 - until epoch: 026, best_acc1: 46.446%
2022-02-20 17:55:09 - epoch 027 lr: 0.1
2022-02-20 17:55:49 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.7652
2022-02-20 17:56:23 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.4811
2022-02-20 17:56:54 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.7187
2022-02-20 17:57:29 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.8433
2022-02-20 17:58:01 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.6334
2022-02-20 17:58:36 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.9448
2022-02-20 17:59:09 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.8366
2022-02-20 17:59:43 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.6767
2022-02-20 18:00:16 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.5323
2022-02-20 18:00:51 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.7605
2022-02-20 18:01:25 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.4992
2022-02-20 18:01:58 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.9023
2022-02-20 18:02:32 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.6889
2022-02-20 18:03:06 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.9307
2022-02-20 18:03:40 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.6176
2022-02-20 18:04:13 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.6439
2022-02-20 18:04:46 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.6426
2022-02-20 18:05:19 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.7357
2022-02-20 18:05:53 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.7360
2022-02-20 18:06:26 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.5679
2022-02-20 18:07:00 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.8202
2022-02-20 18:07:34 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.8964
2022-02-20 18:08:08 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.9283
2022-02-20 18:08:41 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.6149
2022-02-20 18:09:14 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.6094
2022-02-20 18:09:48 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.9720
2022-02-20 18:10:21 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.7212
2022-02-20 18:10:55 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.7885
2022-02-20 18:11:28 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.7300
2022-02-20 18:12:03 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.4464
2022-02-20 18:12:36 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.4448
2022-02-20 18:13:10 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.5019
2022-02-20 18:13:43 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.7034
2022-02-20 18:14:16 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.5652
2022-02-20 18:14:49 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.8908
2022-02-20 18:15:24 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.4560
2022-02-20 18:15:56 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.6357
2022-02-20 18:16:31 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.4157
2022-02-20 18:17:04 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.4874
2022-02-20 18:17:38 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.7696
2022-02-20 18:18:11 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.6006
2022-02-20 18:18:45 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.7923
2022-02-20 18:19:18 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.5624
2022-02-20 18:19:53 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.7672
2022-02-20 18:20:25 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.5590
2022-02-20 18:20:58 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.6157
2022-02-20 18:21:31 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.7423
2022-02-20 18:22:05 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.8219
2022-02-20 18:22:38 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.7146
2022-02-20 18:23:11 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.2695
2022-02-20 18:23:12 - train: epoch 027, train_loss: 2.6546
2022-02-20 18:24:27 - eval: epoch: 027, acc1: 44.798%, acc5: 71.284%, test_loss: 2.4341, per_image_load_time: 1.617ms, per_image_inference_time: 0.160ms
2022-02-20 18:24:27 - until epoch: 027, best_acc1: 46.446%
2022-02-20 18:24:27 - epoch 028 lr: 0.1
2022-02-20 18:25:05 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.5229
2022-02-20 18:25:39 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.6267
2022-02-20 18:26:12 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.5267
2022-02-20 18:26:44 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.5134
2022-02-20 18:27:18 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.5726
2022-02-20 18:27:51 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.6991
2022-02-20 18:28:24 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.9733
2022-02-20 18:28:57 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.4369
2022-02-20 18:29:30 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.4090
2022-02-20 18:30:03 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.7616
2022-02-20 18:30:36 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.5882
2022-02-20 18:31:09 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.4971
2022-02-20 18:31:43 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.6706
2022-02-20 18:32:15 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.0595
2022-02-20 18:32:49 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.6904
2022-02-20 18:33:22 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.7234
2022-02-20 18:33:55 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.6935
2022-02-20 18:34:29 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.5401
2022-02-20 18:35:02 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.6007
2022-02-20 18:35:36 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.8312
2022-02-20 18:36:09 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.7077
2022-02-20 18:36:42 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.7121
2022-02-20 18:37:14 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.8312
2022-02-20 18:37:47 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.8774
2022-02-20 18:38:21 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.5951
2022-02-20 18:38:54 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.4064
2022-02-20 18:39:28 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.7415
2022-02-20 18:40:00 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.6099
2022-02-20 18:40:35 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.6093
2022-02-20 18:41:07 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.7914
2022-02-20 18:41:40 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.8748
2022-02-20 18:42:13 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.5839
2022-02-20 18:42:46 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.7587
2022-02-20 18:43:18 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.6370
2022-02-20 18:43:52 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.5106
2022-02-20 18:44:25 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.6195
2022-02-20 18:44:59 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.6266
2022-02-20 18:45:32 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.3747
2022-02-20 18:46:05 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.6338
2022-02-20 18:46:38 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.7954
2022-02-20 18:47:11 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.8145
2022-02-20 18:47:43 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.7286
2022-02-20 18:48:16 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.5710
2022-02-20 18:48:48 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.5864
2022-02-20 18:49:23 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.7358
2022-02-20 18:49:55 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.6697
2022-02-20 18:50:29 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.6567
2022-02-20 18:51:02 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.5075
2022-02-20 18:51:37 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.5509
2022-02-20 18:52:08 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.5845
2022-02-20 18:52:10 - train: epoch 028, train_loss: 2.6488
2022-02-20 18:53:24 - eval: epoch: 028, acc1: 45.048%, acc5: 71.838%, test_loss: 2.4089, per_image_load_time: 1.062ms, per_image_inference_time: 0.184ms
2022-02-20 18:53:24 - until epoch: 028, best_acc1: 46.446%
2022-02-20 18:53:24 - epoch 029 lr: 0.1
2022-02-20 18:54:03 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.7701
2022-02-20 18:54:35 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.6761
2022-02-20 18:55:09 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.8143
2022-02-20 18:55:41 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.7681
2022-02-20 18:56:15 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.6197
2022-02-20 18:56:47 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.8876
2022-02-20 18:57:21 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.1053
2022-02-20 18:57:54 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.7871
2022-02-20 18:58:27 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.6251
2022-02-20 18:58:59 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.5211
2022-02-20 18:59:33 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.6853
2022-02-20 19:00:07 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.7887
2022-02-20 19:00:39 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.6755
2022-02-20 19:01:14 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.8462
2022-02-20 19:01:46 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.7461
2022-02-20 19:02:20 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.6003
2022-02-20 19:02:53 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.6853
2022-02-20 19:03:27 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.8029
2022-02-20 19:04:00 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.4424
2022-02-20 19:04:33 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.8152
2022-02-20 19:05:05 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.4936
2022-02-20 19:05:38 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.7800
2022-02-20 19:06:11 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.6134
2022-02-20 19:06:45 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.5475
2022-02-20 19:07:17 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.5841
2022-02-20 19:07:51 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.6116
2022-02-20 19:08:23 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.5657
2022-02-20 19:08:56 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.5210
2022-02-20 19:09:29 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.6101
2022-02-20 19:10:02 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.6338
2022-02-20 19:10:35 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.6379
2022-02-20 19:11:08 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.6467
2022-02-20 19:11:41 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.4801
2022-02-20 19:12:15 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.6102
2022-02-20 19:12:48 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.8224
2022-02-20 19:13:21 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.5921
2022-02-20 19:13:54 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.6065
2022-02-20 19:14:27 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.6695
2022-02-20 19:15:01 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.4330
2022-02-20 19:15:34 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.5931
2022-02-20 19:16:08 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.6131
2022-02-20 19:16:40 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.5228
2022-02-20 19:17:13 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.7377
2022-02-20 19:17:47 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.6473
2022-02-20 19:18:20 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.7863
2022-02-20 19:18:54 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.8009
2022-02-20 19:19:27 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.4061
2022-02-20 19:20:00 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.7416
2022-02-20 19:20:34 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 3.0407
2022-02-20 19:21:07 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.2849
2022-02-20 19:21:08 - train: epoch 029, train_loss: 2.6487
2022-02-20 19:22:23 - eval: epoch: 029, acc1: 45.974%, acc5: 72.916%, test_loss: 2.3568, per_image_load_time: 2.601ms, per_image_inference_time: 0.176ms
2022-02-20 19:22:23 - until epoch: 029, best_acc1: 46.446%
2022-02-20 19:22:23 - epoch 030 lr: 0.1
2022-02-20 19:23:02 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.8077
2022-02-20 19:23:35 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.8096
2022-02-20 19:24:08 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.5874
2022-02-20 19:24:41 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.5096
2022-02-20 19:25:15 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 3.0131
2022-02-20 19:25:48 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.5073
2022-02-20 19:26:22 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.5894
2022-02-20 19:26:54 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.6705
2022-02-20 19:27:28 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.6632
2022-02-20 19:28:02 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.3353
2022-02-20 19:28:35 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.5095
2022-02-20 19:29:08 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.6647
2022-02-20 19:29:41 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.3465
2022-02-20 19:30:15 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.5563
2022-02-20 19:30:48 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.6736
2022-02-20 19:31:21 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.6604
2022-02-20 19:31:55 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.8704
2022-02-20 19:32:29 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.6222
2022-02-20 19:33:02 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.7365
2022-02-20 19:33:34 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.6130
2022-02-20 19:34:08 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.7480
2022-02-20 19:34:40 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.5861
2022-02-20 19:35:15 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.7854
2022-02-20 19:35:47 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.8512
2022-02-20 19:36:21 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.6236
2022-02-20 19:36:54 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.6684
2022-02-20 19:37:27 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.5623
2022-02-20 19:38:01 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.6164
2022-02-20 19:38:35 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.6177
2022-02-20 19:39:07 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.9384
2022-02-20 19:39:41 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.7090
2022-02-20 19:40:14 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.4768
2022-02-20 19:40:48 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.8874
2022-02-20 19:41:21 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.6107
2022-02-20 19:41:56 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.7477
2022-02-20 19:42:28 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.4794
2022-02-20 19:43:03 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.5973
2022-02-20 19:43:35 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.8671
2022-02-20 19:44:09 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.6560
2022-02-20 19:44:42 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.4287
2022-02-20 19:45:15 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.4969
2022-02-20 19:45:49 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.8487
2022-02-20 19:46:23 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.5077
2022-02-20 19:46:56 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.7339
2022-02-20 19:47:29 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.8835
2022-02-20 19:48:03 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.3896
2022-02-20 19:48:36 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.6889
2022-02-20 19:49:10 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.7225
2022-02-20 19:49:44 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.7402
2022-02-20 19:50:16 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.7534
2022-02-20 19:50:18 - train: epoch 030, train_loss: 2.6480
2022-02-20 19:51:32 - eval: epoch: 030, acc1: 45.398%, acc5: 72.204%, test_loss: 2.3899, per_image_load_time: 1.619ms, per_image_inference_time: 0.163ms
2022-02-20 19:51:32 - until epoch: 030, best_acc1: 46.446%
2022-02-20 19:51:32 - epoch 031 lr: 0.010000000000000002
2022-02-20 19:52:10 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.5105
2022-02-20 19:52:45 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.2398
2022-02-20 19:53:17 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.1750
2022-02-20 19:53:50 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.2755
2022-02-20 19:54:23 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.1255
2022-02-20 19:54:57 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.2200
2022-02-20 19:55:30 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.0834
2022-02-20 19:56:03 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.0581
2022-02-20 19:56:36 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.1043
2022-02-20 19:57:09 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.4309
2022-02-20 19:57:43 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.4464
2022-02-20 19:58:16 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.1818
2022-02-20 19:58:50 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.9564
2022-02-20 19:59:22 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.1559
2022-02-20 19:59:57 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.2783
2022-02-20 20:00:29 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.9750
2022-02-20 20:01:02 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.9882
2022-02-20 20:01:34 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.0278
2022-02-20 20:02:08 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.1693
2022-02-20 20:02:40 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.2114
2022-02-20 20:03:13 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.9372
2022-02-20 20:03:46 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.8989
2022-02-20 20:04:20 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.8861
2022-02-20 20:04:52 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.0196
2022-02-20 20:05:26 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.9429
2022-02-20 20:05:59 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.1185
2022-02-20 20:06:33 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.1519
2022-02-20 20:07:05 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.3800
2022-02-20 20:07:39 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.1097
2022-02-20 20:08:11 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.2402
2022-02-20 20:08:45 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.9137
2022-02-20 20:09:18 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.1389
2022-02-20 20:09:50 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.1318
2022-02-20 20:10:24 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.1868
2022-02-20 20:10:57 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.1964
2022-02-20 20:11:31 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.0780
2022-02-20 20:12:04 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.0351
2022-02-20 20:12:37 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.9129
2022-02-20 20:13:10 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.1549
2022-02-20 20:13:43 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.9449
2022-02-20 20:14:17 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.1060
2022-02-20 20:14:51 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.1492
2022-02-20 20:15:24 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.9474
2022-02-20 20:15:57 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.1840
2022-02-20 20:16:31 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.1818
2022-02-20 20:17:04 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.1400
2022-02-20 20:17:37 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.1031
2022-02-20 20:18:12 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.9837
2022-02-20 20:18:45 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.8241
2022-02-20 20:19:18 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.9667
2022-02-20 20:19:19 - train: epoch 031, train_loss: 2.1293
2022-02-20 20:20:34 - eval: epoch: 031, acc1: 59.164%, acc5: 82.480%, test_loss: 1.7110, per_image_load_time: 1.167ms, per_image_inference_time: 0.181ms
2022-02-20 20:20:34 - until epoch: 031, best_acc1: 59.164%
2022-02-20 20:20:34 - epoch 032 lr: 0.010000000000000002
2022-02-20 20:21:13 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 2.0374
2022-02-20 20:21:47 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.1370
2022-02-20 20:22:21 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 2.0724
2022-02-20 20:22:54 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.1442
2022-02-20 20:23:27 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.1489
2022-02-20 20:24:00 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.1893
2022-02-20 20:24:34 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.0315
2022-02-20 20:25:07 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.1198
2022-02-20 20:25:41 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.9953
2022-02-20 20:26:14 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.1296
2022-02-20 20:26:48 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.0926
2022-02-20 20:27:22 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.1443
2022-02-20 20:27:55 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.9603
2022-02-20 20:28:29 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.1596
2022-02-20 20:29:02 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.1489
2022-02-20 20:29:35 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.1549
2022-02-20 20:30:08 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.2334
2022-02-20 20:30:42 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.3919
2022-02-20 20:31:15 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.8093
2022-02-20 20:31:49 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.0400
2022-02-20 20:32:22 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.9285
2022-02-20 20:32:56 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.9629
2022-02-20 20:33:29 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.0546
2022-02-20 20:34:03 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.9513
2022-02-20 20:34:36 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.2849
2022-02-20 20:35:11 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.8344
2022-02-20 20:35:43 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.1410
2022-02-20 20:36:17 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.0616
2022-02-20 20:36:50 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.8971
2022-02-20 20:37:24 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.8641
2022-02-20 20:37:57 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.0928
2022-02-20 20:38:30 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.9985
2022-02-20 20:39:04 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.8724
2022-02-20 20:39:36 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.8174
2022-02-20 20:40:10 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.0489
2022-02-20 20:40:43 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.2030
2022-02-20 20:41:16 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.0582
2022-02-20 20:41:49 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.8815
2022-02-20 20:42:21 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 2.0797
2022-02-20 20:42:55 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.9687
2022-02-20 20:43:28 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.0796
2022-02-20 20:44:02 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.9166
2022-02-20 20:44:34 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.1376
2022-02-20 20:45:09 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.9184
2022-02-20 20:45:41 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.1798
2022-02-20 20:46:15 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.9780
2022-02-20 20:46:47 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.1140
2022-02-20 20:47:21 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.9208
2022-02-20 20:47:54 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.2257
2022-02-20 20:48:26 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.1116
2022-02-20 20:48:28 - train: epoch 032, train_loss: 2.0160
2022-02-20 20:49:43 - eval: epoch: 032, acc1: 60.020%, acc5: 83.070%, test_loss: 1.6716, per_image_load_time: 1.149ms, per_image_inference_time: 0.157ms
2022-02-20 20:49:43 - until epoch: 032, best_acc1: 60.020%
2022-02-20 20:49:43 - epoch 033 lr: 0.010000000000000002
2022-02-20 20:50:21 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.8420
2022-02-20 20:50:55 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.0557
2022-02-20 20:51:28 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.8825
2022-02-20 20:52:01 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.8198
2022-02-20 20:52:33 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.0705
2022-02-20 20:53:06 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.8720
2022-02-20 20:53:39 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.1775
2022-02-20 20:54:12 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.3220
2022-02-20 20:54:45 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.9316
2022-02-20 20:55:18 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.0965
2022-02-20 20:55:52 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.8564
2022-02-20 20:56:25 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.0247
2022-02-20 20:56:58 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.8804
2022-02-20 20:57:32 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.0963
2022-02-20 20:58:05 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.2418
2022-02-20 20:58:38 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.0831
2022-02-20 20:59:11 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.8216
2022-02-20 20:59:44 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.2166
2022-02-20 21:00:17 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.9545
2022-02-20 21:00:50 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.9037
2022-02-20 21:01:24 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.9586
2022-02-20 21:01:57 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.0725
2022-02-20 21:02:30 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.8754
2022-02-20 21:03:04 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.1746
2022-02-20 21:03:36 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.9764
2022-02-20 21:04:08 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.8661
2022-02-20 21:04:42 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.1548
2022-02-20 21:05:15 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.0340
2022-02-20 21:05:48 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.0539
2022-02-20 21:06:21 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.0205
2022-02-20 21:06:54 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.9351
2022-02-20 21:07:28 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.8790
2022-02-20 21:08:01 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.9243
2022-02-20 21:08:33 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.8419
2022-02-20 21:09:06 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.0249
2022-02-20 21:09:39 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.2084
2022-02-20 21:10:12 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.9242
2022-02-20 21:10:46 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.8289
2022-02-20 21:11:18 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.1401
2022-02-20 21:11:51 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.0337
2022-02-20 21:12:24 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.9080
2022-02-20 21:12:58 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.9003
2022-02-20 21:13:30 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.0538
2022-02-20 21:14:04 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.0070
2022-02-20 21:14:36 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.2546
2022-02-20 21:15:10 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.8546
2022-02-20 21:15:43 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.8078
2022-02-20 21:16:16 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.4084
2022-02-20 21:16:49 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.8493
2022-02-20 21:17:21 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.8237
2022-02-20 21:17:23 - train: epoch 033, train_loss: 1.9708
2022-02-20 21:18:37 - eval: epoch: 033, acc1: 60.556%, acc5: 83.462%, test_loss: 1.6481, per_image_load_time: 1.549ms, per_image_inference_time: 0.185ms
2022-02-20 21:18:37 - until epoch: 033, best_acc1: 60.556%
2022-02-20 21:18:37 - epoch 034 lr: 0.010000000000000002
2022-02-20 21:19:15 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.8864
2022-02-20 21:19:48 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.8305
2022-02-20 21:20:22 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.8847
2022-02-20 21:20:54 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.7882
2022-02-20 21:21:28 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.9115
2022-02-20 21:22:00 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.1028
2022-02-20 21:22:34 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.9734
2022-02-20 21:23:07 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.9054
2022-02-20 21:23:40 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.9069
2022-02-20 21:24:14 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.8415
2022-02-20 21:24:47 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.0136
2022-02-20 21:25:20 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.9984
2022-02-20 21:25:54 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.9214
2022-02-20 21:26:26 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.9988
2022-02-20 21:26:59 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.8466
2022-02-20 21:27:33 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.8703
2022-02-20 21:28:05 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.9148
2022-02-20 21:28:39 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.1904
2022-02-20 21:29:12 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.0974
2022-02-20 21:29:45 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 2.0226
2022-02-20 21:30:19 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.1093
2022-02-20 21:30:52 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.8385
2022-02-20 21:31:25 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.9885
2022-02-20 21:31:59 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.8013
2022-02-20 21:32:31 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.9859
2022-02-20 21:33:05 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.9922
2022-02-20 21:33:37 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.9736
2022-02-20 21:34:11 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.7987
2022-02-20 21:34:44 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.6814
2022-02-20 21:35:17 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.7489
2022-02-20 21:35:50 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.8914
2022-02-20 21:36:24 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.8988
2022-02-20 21:36:57 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.9342
2022-02-20 21:37:31 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.0249
2022-02-20 21:38:03 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.7668
2022-02-20 21:38:36 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.8282
2022-02-20 21:39:09 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.7713
2022-02-20 21:39:42 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.9747
2022-02-20 21:40:17 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.0718
2022-02-20 21:40:50 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.7277
2022-02-20 21:41:23 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.9270
2022-02-20 21:41:57 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.8132
2022-02-20 21:42:29 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.8972
2022-02-20 21:43:02 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.9790
2022-02-20 21:43:34 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.0595
2022-02-20 21:44:08 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.9370
2022-02-20 21:44:41 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.9258
2022-02-20 21:45:15 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.9390
2022-02-20 21:45:48 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.9190
2022-02-20 21:46:20 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.8366
2022-02-20 21:46:22 - train: epoch 034, train_loss: 1.9463
2022-02-20 21:47:36 - eval: epoch: 034, acc1: 60.746%, acc5: 83.686%, test_loss: 1.6305, per_image_load_time: 1.585ms, per_image_inference_time: 0.183ms
2022-02-20 21:47:36 - until epoch: 034, best_acc1: 60.746%
2022-02-20 21:47:36 - epoch 035 lr: 0.010000000000000002
2022-02-20 21:48:15 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.7273
2022-02-20 21:48:49 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.6792
2022-02-20 21:49:21 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.0700
2022-02-20 21:49:54 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.9153
2022-02-20 21:50:28 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.8561
2022-02-20 21:51:00 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.8636
2022-02-20 21:51:33 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.9355
2022-02-20 21:52:06 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.9162
2022-02-20 21:52:40 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.1073
2022-02-20 21:53:13 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.0048
2022-02-20 21:53:47 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.0643
2022-02-20 21:54:20 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.8383
2022-02-20 21:54:54 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 2.0677
2022-02-20 21:55:27 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.9572
2022-02-20 21:56:00 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.9960
2022-02-20 21:56:33 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.9489
2022-02-20 21:57:07 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.7624
2022-02-20 21:57:40 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.9044
2022-02-20 21:58:14 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 2.0078
2022-02-20 21:58:46 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.0351
2022-02-20 21:59:20 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.8034
2022-02-20 21:59:54 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.9463
2022-02-20 22:00:27 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.8775
2022-02-20 22:00:59 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.0243
2022-02-20 22:01:33 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.9125
2022-02-20 22:02:06 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.1403
2022-02-20 22:02:40 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.9933
2022-02-20 22:03:13 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.9236
2022-02-20 22:03:46 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.8846
2022-02-20 22:04:19 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 2.0277
2022-02-20 22:04:52 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.9208
2022-02-20 22:05:27 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.7466
2022-02-20 22:05:59 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.9345
2022-02-20 22:06:32 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.9671
2022-02-20 22:07:05 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.6305
2022-02-20 22:07:39 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.9606
2022-02-20 22:08:12 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.6792
2022-02-20 22:08:46 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.7866
2022-02-20 22:09:19 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.0657
2022-02-20 22:09:53 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.7193
2022-02-20 22:10:25 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.2654
2022-02-20 22:10:59 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.8229
2022-02-20 22:11:32 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.9575
2022-02-20 22:12:05 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.9927
2022-02-20 22:12:38 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.0916
2022-02-20 22:13:11 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.8568
2022-02-20 22:13:44 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.3637
2022-02-20 22:14:18 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.1762
2022-02-20 22:14:51 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.9960
2022-02-20 22:15:24 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.7848
2022-02-20 22:15:25 - train: epoch 035, train_loss: 1.9320
2022-02-20 22:16:40 - eval: epoch: 035, acc1: 60.890%, acc5: 83.814%, test_loss: 1.6292, per_image_load_time: 1.688ms, per_image_inference_time: 0.160ms
2022-02-20 22:16:40 - until epoch: 035, best_acc1: 60.890%
2022-02-20 22:16:40 - epoch 036 lr: 0.010000000000000002
2022-02-20 22:17:18 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.0125
2022-02-20 22:17:51 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.6784
2022-02-20 22:18:24 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.7822
2022-02-20 22:18:58 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.9621
2022-02-20 22:19:31 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.9180
2022-02-20 22:20:05 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.8798
2022-02-20 22:20:37 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.6900
2022-02-20 22:21:11 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.6928
2022-02-20 22:21:44 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.7759
2022-02-20 22:22:18 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.9424
2022-02-20 22:22:51 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 2.0769
2022-02-20 22:23:23 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.9337
2022-02-20 22:23:57 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.0169
2022-02-20 22:24:31 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.0783
2022-02-20 22:25:03 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.1262
2022-02-20 22:25:37 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.0253
2022-02-20 22:26:10 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.8968
2022-02-20 22:26:43 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.8099
2022-02-20 22:27:16 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.9212
2022-02-20 22:27:50 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.8105
2022-02-20 22:28:23 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.9788
2022-02-20 22:28:55 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.8441
2022-02-20 22:29:29 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.9998
2022-02-20 22:30:02 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.0178
2022-02-20 22:30:36 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.7158
2022-02-20 22:31:10 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.0260
2022-02-20 22:31:43 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.8281
2022-02-20 22:32:16 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.7814
2022-02-20 22:32:49 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.7911
2022-02-20 22:33:23 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 2.0657
2022-02-20 22:33:55 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.9380
2022-02-20 22:34:29 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.8309
2022-02-20 22:35:02 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.8262
2022-02-20 22:35:35 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.7284
2022-02-20 22:36:08 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.0290
2022-02-20 22:36:41 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.0208
2022-02-20 22:37:14 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.8840
2022-02-20 22:37:48 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.8750
2022-02-20 22:38:21 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.8389
2022-02-20 22:38:54 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.9634
2022-02-20 22:39:27 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.9491
2022-02-20 22:40:00 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.8221
2022-02-20 22:40:33 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.9569
2022-02-20 22:41:07 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.9725
2022-02-20 22:41:40 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.7042
2022-02-20 22:42:14 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.7904
2022-02-20 22:42:47 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.8865
2022-02-20 22:43:21 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.8432
2022-02-20 22:43:53 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.9117
2022-02-20 22:44:25 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.9370
2022-02-20 22:44:26 - train: epoch 036, train_loss: 1.9210
2022-02-20 22:45:41 - eval: epoch: 036, acc1: 60.382%, acc5: 83.522%, test_loss: 1.6498, per_image_load_time: 2.010ms, per_image_inference_time: 0.160ms
2022-02-20 22:45:41 - until epoch: 036, best_acc1: 60.890%
2022-02-20 22:45:41 - epoch 037 lr: 0.010000000000000002
2022-02-20 22:46:19 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.7203
2022-02-20 22:46:53 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.7008
2022-02-20 22:47:27 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.7477
2022-02-20 22:48:00 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.9525
2022-02-20 22:48:33 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.9027
2022-02-20 22:49:06 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.9664
2022-02-20 22:49:39 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.9953
2022-02-20 22:50:13 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.9033
2022-02-20 22:50:46 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.2106
2022-02-20 22:51:19 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.7997
2022-02-20 22:51:53 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.9674
2022-02-20 22:52:26 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.8285
2022-02-20 22:52:59 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.0565
2022-02-20 22:53:33 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.0613
2022-02-20 22:54:05 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.7623
2022-02-20 22:54:39 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.6315
2022-02-20 22:55:12 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.1152
2022-02-20 22:55:46 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.1128
2022-02-20 22:56:19 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.9387
2022-02-20 22:56:52 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.9872
2022-02-20 22:57:25 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.9884
2022-02-20 22:57:58 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.8340
2022-02-20 22:58:31 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.7648
2022-02-20 22:59:04 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.9589
2022-02-20 22:59:37 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.8054
2022-02-20 23:00:10 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.0620
2022-02-20 23:00:44 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.0838
2022-02-20 23:01:17 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.8019
2022-02-20 23:01:49 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.9685
2022-02-20 23:02:22 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.0410
2022-02-20 23:02:55 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.8360
2022-02-20 23:03:28 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.9527
2022-02-20 23:04:02 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.7578
2022-02-20 23:04:35 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.7182
2022-02-20 23:05:09 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.9268
2022-02-20 23:05:43 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.9735
2022-02-20 23:06:16 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 2.0921
2022-02-20 23:06:50 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.8388
2022-02-20 23:07:23 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.1253
2022-02-20 23:07:55 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.8596
2022-02-20 23:08:28 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.9515
2022-02-20 23:09:02 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.1527
2022-02-20 23:09:34 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.9187
2022-02-20 23:10:09 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.8683
2022-02-20 23:10:41 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.8387
2022-02-20 23:11:15 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.9504
2022-02-20 23:11:47 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.9390
2022-02-20 23:12:22 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.8529
2022-02-20 23:12:55 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.9860
2022-02-20 23:13:27 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.8726
2022-02-20 23:13:29 - train: epoch 037, train_loss: 1.9167
2022-02-20 23:14:43 - eval: epoch: 037, acc1: 61.478%, acc5: 84.084%, test_loss: 1.6041, per_image_load_time: 2.236ms, per_image_inference_time: 0.181ms
2022-02-20 23:14:43 - until epoch: 037, best_acc1: 61.478%
2022-02-20 23:14:43 - epoch 038 lr: 0.010000000000000002
2022-02-20 23:15:21 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.9195
2022-02-20 23:15:55 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.7573
2022-02-20 23:16:27 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.6505
2022-02-20 23:17:01 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.7724
2022-02-20 23:17:33 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.7433
2022-02-20 23:18:06 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.0552
2022-02-20 23:18:39 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.7296
2022-02-20 23:19:13 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.8239
2022-02-20 23:19:45 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.8000
2022-02-20 23:20:19 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.0520
2022-02-20 23:20:52 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.9252
2022-02-20 23:21:26 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.9169
2022-02-20 23:21:58 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.0856
2022-02-20 23:22:33 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.8747
2022-02-20 23:23:05 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 2.0561
2022-02-20 23:23:39 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.0629
2022-02-20 23:24:12 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.0454
2022-02-20 23:24:45 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.0336
2022-02-20 23:25:19 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.0165
2022-02-20 23:25:52 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.8355
2022-02-20 23:26:25 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.9883
2022-02-20 23:26:58 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.7991
2022-02-20 23:27:31 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.0553
2022-02-20 23:28:04 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.1605
2022-02-20 23:28:38 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.8530
2022-02-20 23:29:11 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.9060
2022-02-20 23:29:45 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.9531
2022-02-20 23:30:18 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.0909
2022-02-20 23:30:53 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.9586
2022-02-20 23:31:25 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.7925
2022-02-20 23:31:57 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.9843
2022-02-20 23:32:30 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.6456
2022-02-20 23:33:04 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.6485
2022-02-20 23:33:37 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.8355
2022-02-20 23:34:11 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.9485
2022-02-20 23:34:44 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.0084
2022-02-20 23:35:18 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.6511
2022-02-20 23:35:50 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.0414
2022-02-20 23:36:24 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.9689
2022-02-20 23:36:57 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.9098
2022-02-20 23:37:30 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.8693
2022-02-20 23:38:03 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.7196
2022-02-20 23:38:37 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.9841
2022-02-20 23:39:11 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.9557
2022-02-20 23:39:43 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.0367
2022-02-20 23:40:18 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.0402
2022-02-20 23:40:51 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.8045
2022-02-20 23:41:24 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.8887
2022-02-20 23:41:57 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.9078
2022-02-20 23:42:29 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.7362
2022-02-20 23:42:31 - train: epoch 038, train_loss: 1.9128
2022-02-20 23:43:46 - eval: epoch: 038, acc1: 60.834%, acc5: 83.554%, test_loss: 1.6338, per_image_load_time: 1.506ms, per_image_inference_time: 0.184ms
2022-02-20 23:43:46 - until epoch: 038, best_acc1: 61.478%
2022-02-20 23:43:46 - epoch 039 lr: 0.010000000000000002
2022-02-20 23:44:24 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.9625
2022-02-20 23:44:57 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.0285
2022-02-20 23:45:30 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.7527
2022-02-20 23:46:03 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.9104
2022-02-20 23:46:36 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.9471
2022-02-20 23:47:10 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.8277
2022-02-20 23:47:42 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.1024
2022-02-20 23:48:15 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.9362
2022-02-20 23:48:48 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.9281
2022-02-20 23:49:21 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.7821
2022-02-20 23:49:55 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.0334
2022-02-20 23:50:27 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.0170
2022-02-20 23:51:01 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.2295
2022-02-20 23:51:34 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.9166
2022-02-20 23:52:06 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.8789
2022-02-20 23:52:40 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 2.0836
2022-02-20 23:53:12 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.6003
2022-02-20 23:53:45 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.9506
2022-02-20 23:54:20 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.6779
2022-02-20 23:54:52 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.8317
2022-02-20 23:55:26 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.9197
2022-02-20 23:55:58 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.7944
2022-02-20 23:56:31 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.1888
2022-02-20 23:57:04 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.1436
2022-02-20 23:57:36 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.8073
2022-02-20 23:58:09 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.9730
2022-02-20 23:58:41 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.1458
2022-02-20 23:59:16 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.8430
2022-02-20 23:59:49 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.6495
2022-02-21 00:00:22 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.9191
2022-02-21 00:00:55 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.9073
2022-02-21 00:01:28 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.0081
2022-02-21 00:02:01 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 2.0210
2022-02-21 00:02:34 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 2.0199
2022-02-21 00:03:07 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.2148
2022-02-21 00:03:41 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.0360
2022-02-21 00:04:13 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.8480
2022-02-21 00:04:45 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.7659
2022-02-21 00:05:18 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 2.0633
2022-02-21 00:05:51 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.9439
2022-02-21 00:06:24 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.9914
2022-02-21 00:06:57 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.9629
2022-02-21 00:07:30 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.9159
2022-02-21 00:08:02 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.5860
2022-02-21 00:08:39 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.8573
2022-02-21 00:09:12 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.0027
2022-02-21 00:09:45 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.9164
2022-02-21 00:10:18 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.9548
2022-02-21 00:10:50 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.8231
2022-02-21 00:11:22 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.7549
2022-02-21 00:11:24 - train: epoch 039, train_loss: 1.9119
2022-02-21 00:12:40 - eval: epoch: 039, acc1: 60.560%, acc5: 83.468%, test_loss: 1.6394, per_image_load_time: 1.120ms, per_image_inference_time: 0.164ms
2022-02-21 00:12:40 - until epoch: 039, best_acc1: 61.478%
2022-02-21 00:12:40 - epoch 040 lr: 0.010000000000000002
2022-02-21 00:13:17 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.1533
2022-02-21 00:13:51 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.1267
2022-02-21 00:14:25 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.0860
2022-02-21 00:14:56 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.8759
2022-02-21 00:15:31 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.8149
2022-02-21 00:16:02 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.9990
2022-02-21 00:16:37 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.9869
2022-02-21 00:17:11 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.1307
2022-02-21 00:17:42 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.8129
2022-02-21 00:18:15 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.6387
2022-02-21 00:18:47 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.8962
2022-02-21 00:19:20 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.7070
2022-02-21 00:19:55 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.8015
2022-02-21 00:20:29 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.7719
2022-02-21 00:21:01 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.1057
2022-02-21 00:21:33 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 2.0232
2022-02-21 00:22:05 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 2.0637
2022-02-21 00:22:39 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.6784
2022-02-21 00:23:11 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.8974
2022-02-21 00:23:44 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.9099
2022-02-21 00:24:17 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.7635
2022-02-21 00:24:50 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.6541
2022-02-21 00:25:22 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.8186
2022-02-21 00:25:57 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.8758
2022-02-21 00:26:29 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.9780
2022-02-21 00:27:03 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.7663
2022-02-21 00:27:34 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.0317
2022-02-21 00:28:08 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.9656
2022-02-21 00:28:41 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.2155
2022-02-21 00:29:13 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 2.0437
2022-02-21 00:29:47 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.8762
2022-02-21 00:30:20 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.1177
2022-02-21 00:30:54 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.9479
2022-02-21 00:31:25 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.8930
2022-02-21 00:31:58 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.9455
2022-02-21 00:32:31 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.9503
2022-02-21 00:33:04 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.9581
2022-02-21 00:33:36 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.8314
2022-02-21 00:34:09 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.0708
2022-02-21 00:34:43 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.0350
2022-02-21 00:35:16 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.9854
2022-02-21 00:35:50 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.9023
2022-02-21 00:36:22 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.8691
2022-02-21 00:36:55 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.9752
2022-02-21 00:37:27 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.6927
2022-02-21 00:38:00 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.9471
2022-02-21 00:38:34 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.0140
2022-02-21 00:39:06 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.7564
2022-02-21 00:39:39 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 2.0450
2022-02-21 00:40:11 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.8532
2022-02-21 00:40:13 - train: epoch 040, train_loss: 1.9114
2022-02-21 00:41:27 - eval: epoch: 040, acc1: 60.752%, acc5: 83.766%, test_loss: 1.6280, per_image_load_time: 1.125ms, per_image_inference_time: 0.157ms
2022-02-21 00:41:27 - until epoch: 040, best_acc1: 61.478%
2022-02-21 00:41:27 - epoch 041 lr: 0.010000000000000002
2022-02-21 00:42:06 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.0476
2022-02-21 00:42:39 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.1324
2022-02-21 00:43:13 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.9345
2022-02-21 00:43:45 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.8689
2022-02-21 00:44:17 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.6452
2022-02-21 00:44:50 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.0810
2022-02-21 00:45:22 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.8188
2022-02-21 00:45:55 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.6755
2022-02-21 00:46:27 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.5813
2022-02-21 00:46:59 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.9888
2022-02-21 00:47:32 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.7826
2022-02-21 00:48:04 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.7196
2022-02-21 00:48:36 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.8724
2022-02-21 00:49:10 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.9445
2022-02-21 00:49:41 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.1748
2022-02-21 00:50:13 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.7880
2022-02-21 00:50:46 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.9380
2022-02-21 00:51:19 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.9664
2022-02-21 00:51:52 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.9412
2022-02-21 00:52:26 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.8531
2022-02-21 00:53:00 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.7320
2022-02-21 00:53:32 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.7528
2022-02-21 00:54:05 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.6709
2022-02-21 00:54:37 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.0800
2022-02-21 00:55:11 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.8282
2022-02-21 00:55:42 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.1000
2022-02-21 00:56:15 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.1662
2022-02-21 00:56:48 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.9635
2022-02-21 00:57:21 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.9325
2022-02-21 00:57:54 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.1319
2022-02-21 00:58:26 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.1157
2022-02-21 00:58:58 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.7327
2022-02-21 00:59:31 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.7653
2022-02-21 01:00:03 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.7940
2022-02-21 01:00:37 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 2.0708
2022-02-21 01:01:08 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.0629
2022-02-21 01:01:41 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.9366
2022-02-21 01:02:13 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.8200
2022-02-21 01:02:46 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.9825
2022-02-21 01:03:18 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.9267
2022-02-21 01:03:51 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.9697
2022-02-21 01:04:23 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.8709
2022-02-21 01:04:57 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 2.0048
2022-02-21 01:05:29 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.8160
2022-02-21 01:06:02 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.8603
2022-02-21 01:06:33 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.0259
2022-02-21 01:07:07 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.1327
2022-02-21 01:07:40 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.8869
2022-02-21 01:08:13 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.9114
2022-02-21 01:08:44 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.0504
2022-02-21 01:08:46 - train: epoch 041, train_loss: 1.9146
2022-02-21 01:10:01 - eval: epoch: 041, acc1: 60.738%, acc5: 83.634%, test_loss: 1.6268, per_image_load_time: 0.819ms, per_image_inference_time: 0.152ms
2022-02-21 01:10:01 - until epoch: 041, best_acc1: 61.478%
2022-02-21 01:10:01 - epoch 042 lr: 0.010000000000000002
2022-02-21 01:10:39 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.6002
2022-02-21 01:11:10 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.8181
2022-02-21 01:11:43 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.9466
2022-02-21 01:12:17 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.6341
2022-02-21 01:12:48 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 2.0514
2022-02-21 01:13:21 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.8555
2022-02-21 01:13:52 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.0167
2022-02-21 01:14:25 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.7432
2022-02-21 01:14:57 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.0608
2022-02-21 01:15:30 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.9547
2022-02-21 01:16:03 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.6895
2022-02-21 01:16:36 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.7432
2022-02-21 01:17:08 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.9764
2022-02-21 01:17:40 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.1860
2022-02-21 01:18:13 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.8255
2022-02-21 01:18:45 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.0502
2022-02-21 01:19:17 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.8704
2022-02-21 01:19:49 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.9720
2022-02-21 01:20:22 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.0709
2022-02-21 01:20:54 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.7525
2022-02-21 01:21:26 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.6755
2022-02-21 01:21:59 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.6857
2022-02-21 01:22:33 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.8295
2022-02-21 01:23:05 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.2669
2022-02-21 01:23:38 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 2.0660
2022-02-21 01:24:09 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.0341
2022-02-21 01:24:42 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.7192
2022-02-21 01:25:14 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.8324
2022-02-21 01:25:46 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.9341
2022-02-21 01:26:20 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.9756
2022-02-21 01:26:52 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.9819
2022-02-21 01:27:24 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.0730
2022-02-21 01:27:57 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.1322
2022-02-21 01:28:29 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.7728
2022-02-21 01:29:02 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.8997
2022-02-21 01:29:33 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.9620
2022-02-21 01:30:06 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.8920
2022-02-21 01:30:40 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.6549
2022-02-21 01:31:11 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.8685
2022-02-21 01:31:44 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.9622
2022-02-21 01:32:16 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.0980
2022-02-21 01:32:49 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.0292
2022-02-21 01:33:20 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.5986
2022-02-21 01:33:54 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.8581
2022-02-21 01:34:26 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.9086
2022-02-21 01:34:58 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.1727
2022-02-21 01:35:30 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.7852
2022-02-21 01:36:03 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.0257
2022-02-21 01:36:34 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.9862
2022-02-21 01:37:05 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.8428
2022-02-21 01:37:07 - train: epoch 042, train_loss: 1.9145
2022-02-21 01:38:21 - eval: epoch: 042, acc1: 60.492%, acc5: 83.584%, test_loss: 1.6387, per_image_load_time: 1.245ms, per_image_inference_time: 0.162ms
2022-02-21 01:38:21 - until epoch: 042, best_acc1: 61.478%
2022-02-21 01:38:21 - epoch 043 lr: 0.010000000000000002
2022-02-21 01:38:59 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.8629
2022-02-21 01:39:32 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.9388
2022-02-21 01:40:05 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.6806
2022-02-21 01:40:36 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.9131
2022-02-21 01:41:09 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.9643
2022-02-21 01:41:41 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.8793
2022-02-21 01:42:14 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.9206
2022-02-21 01:42:47 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 2.0511
2022-02-21 01:43:19 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.9203
2022-02-21 01:43:52 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.0606
2022-02-21 01:44:26 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.9780
2022-02-21 01:44:58 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.9194
2022-02-21 01:45:31 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.0210
2022-02-21 01:46:04 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.8449
2022-02-21 01:46:35 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.7259
2022-02-21 01:47:08 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.8964
2022-02-21 01:47:41 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.9649
2022-02-21 01:48:13 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.0437
2022-02-21 01:48:46 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.9449
2022-02-21 01:49:19 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.7203
2022-02-21 01:49:51 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.8933
2022-02-21 01:50:24 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.0106
2022-02-21 01:50:55 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.1391
2022-02-21 01:51:28 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.8743
2022-02-21 01:52:02 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.1911
2022-02-21 01:52:34 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.6690
2022-02-21 01:53:07 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.9766
2022-02-21 01:53:40 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.7975
2022-02-21 01:54:12 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.9320
2022-02-21 01:54:46 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.1673
2022-02-21 01:55:18 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.1310
2022-02-21 01:55:51 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 2.0045
2022-02-21 01:56:22 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.0521
2022-02-21 01:56:55 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.9852
2022-02-21 01:57:27 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.9464
2022-02-21 01:58:00 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.9962
2022-02-21 01:58:32 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.8885
2022-02-21 01:59:05 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.0340
2022-02-21 01:59:37 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.1869
2022-02-21 02:00:11 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.9546
2022-02-21 02:00:43 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.2582
2022-02-21 02:01:16 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.0078
2022-02-21 02:01:48 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.9846
2022-02-21 02:02:21 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.9459
2022-02-21 02:02:53 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.8895
2022-02-21 02:03:26 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.8720
2022-02-21 02:03:58 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.9037
2022-02-21 02:04:31 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.8519
2022-02-21 02:05:03 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.8198
2022-02-21 02:05:35 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.7857
2022-02-21 02:05:37 - train: epoch 043, train_loss: 1.9147
2022-02-21 02:06:51 - eval: epoch: 043, acc1: 60.792%, acc5: 83.712%, test_loss: 1.6277, per_image_load_time: 2.629ms, per_image_inference_time: 0.160ms
2022-02-21 02:06:52 - until epoch: 043, best_acc1: 61.478%
2022-02-21 02:06:52 - epoch 044 lr: 0.010000000000000002
2022-02-21 02:07:29 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 2.0225
2022-02-21 02:08:02 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.0772
2022-02-21 02:08:35 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.9256
2022-02-21 02:09:08 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.5787
2022-02-21 02:09:40 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.8192
2022-02-21 02:10:13 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.6528
2022-02-21 02:10:46 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.7743
2022-02-21 02:11:18 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.8619
2022-02-21 02:11:50 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.9695
2022-02-21 02:12:23 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.9572
2022-02-21 02:12:56 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.7937
2022-02-21 02:13:30 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.8703
2022-02-21 02:14:02 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.9397
2022-02-21 02:14:35 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.8296
2022-02-21 02:15:07 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.8379
2022-02-21 02:15:40 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.8203
2022-02-21 02:16:13 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.8461
2022-02-21 02:16:46 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.8803
2022-02-21 02:17:19 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.0348
2022-02-21 02:17:53 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.8585
2022-02-21 02:18:26 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.9708
2022-02-21 02:19:00 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.8711
2022-02-21 02:19:34 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.0772
2022-02-21 02:20:07 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.9264
2022-02-21 02:20:40 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.0168
2022-02-21 02:21:13 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.9860
2022-02-21 02:21:45 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 2.0752
2022-02-21 02:22:18 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.6782
2022-02-21 02:22:52 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.8011
2022-02-21 02:23:24 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.8967
2022-02-21 02:23:57 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 2.0126
2022-02-21 02:24:28 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.6848
2022-02-21 02:25:01 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.8134
2022-02-21 02:25:33 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.0736
2022-02-21 02:26:06 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.8083
2022-02-21 02:26:38 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.9874
2022-02-21 02:27:13 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.9847
2022-02-21 02:28:31 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.5919
2022-02-21 02:29:02 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.9779
2022-02-21 02:29:34 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.0580
2022-02-21 02:30:08 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.8389
2022-02-21 02:30:39 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.0086
2022-02-21 02:31:12 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 2.0911
2022-02-21 02:31:44 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.6715
2022-02-21 02:32:17 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.0415
2022-02-21 02:32:48 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.7725
2022-02-21 02:33:22 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.0023
2022-02-21 02:33:53 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.1491
2022-02-21 02:34:27 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.7418
2022-02-21 02:34:56 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.0159
2022-02-21 02:34:58 - train: epoch 044, train_loss: 1.9163
2022-02-21 02:36:12 - eval: epoch: 044, acc1: 60.962%, acc5: 83.778%, test_loss: 1.6316, per_image_load_time: 1.265ms, per_image_inference_time: 0.151ms
2022-02-21 02:36:13 - until epoch: 044, best_acc1: 61.478%
2022-02-21 02:36:13 - epoch 045 lr: 0.010000000000000002
2022-02-21 02:36:50 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.8130
2022-02-21 02:37:23 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.9417
2022-02-21 02:37:56 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.9360
2022-02-21 02:38:28 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.7105
2022-02-21 02:39:00 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.1496
2022-02-21 02:39:32 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.9754
2022-02-21 02:40:05 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.5958
2022-02-21 02:40:37 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.8161
2022-02-21 02:41:10 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 2.0026
2022-02-21 02:41:43 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.7365
2022-02-21 02:42:16 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.1061
2022-02-21 02:42:50 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 2.0127
2022-02-21 02:43:22 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.9019
2022-02-21 02:43:55 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 2.0230
2022-02-21 02:44:28 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.9396
2022-02-21 02:45:02 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.7896
2022-02-21 02:45:34 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.7577
2022-02-21 02:46:07 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.7578
2022-02-21 02:46:39 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.1161
2022-02-21 02:47:12 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.1304
2022-02-21 02:47:43 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.1186
2022-02-21 02:48:16 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.0409
2022-02-21 02:48:49 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.8178
2022-02-21 02:49:22 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 2.0107
2022-02-21 02:49:55 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.8666
2022-02-21 02:50:27 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.8255
2022-02-21 02:51:00 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.7276
2022-02-21 02:51:32 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.8137
2022-02-21 02:52:04 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.0514
2022-02-21 02:52:36 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.0900
2022-02-21 02:53:09 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.7816
2022-02-21 02:53:41 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.9533
2022-02-21 02:54:14 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.8393
2022-02-21 02:54:45 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.7726
2022-02-21 02:55:18 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.1232
2022-02-21 02:55:50 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 2.0331
2022-02-21 02:56:23 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.8645
2022-02-21 02:56:54 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.7320
2022-02-21 02:57:27 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.0453
2022-02-21 02:57:59 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.0463
2022-02-21 02:58:33 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.8424
2022-02-21 02:59:05 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.0251
2022-02-21 02:59:38 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.0263
2022-02-21 03:00:10 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.1423
2022-02-21 03:00:42 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.6331
2022-02-21 03:02:33 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.0612
2022-02-21 03:03:07 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.9321
2022-02-21 03:03:42 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.8530
2022-02-21 03:04:15 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.8446
2022-02-21 03:04:47 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.8518
2022-02-21 03:04:50 - train: epoch 045, train_loss: 1.9153
2022-02-21 03:06:04 - eval: epoch: 045, acc1: 60.562%, acc5: 83.688%, test_loss: 1.6309, per_image_load_time: 1.099ms, per_image_inference_time: 0.169ms
2022-02-21 03:06:04 - until epoch: 045, best_acc1: 61.478%
2022-02-21 03:06:04 - epoch 046 lr: 0.010000000000000002
2022-02-21 03:06:42 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.6500
2022-02-21 03:07:14 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.7229
2022-02-21 03:07:48 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.9072
2022-02-21 03:08:20 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 2.0553
2022-02-21 03:08:52 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.6918
2022-02-21 03:09:24 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.1237
2022-02-21 03:09:56 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.8016
2022-02-21 03:10:29 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.0963
2022-02-21 03:11:02 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.9701
2022-02-21 03:11:34 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.7796
2022-02-21 03:12:05 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.8891
2022-02-21 03:12:38 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.8531
2022-02-21 03:13:10 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.8718
2022-02-21 03:13:44 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.1384
2022-02-21 03:14:15 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.9045
2022-02-21 03:14:48 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.8942
2022-02-21 03:15:20 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.8308
2022-02-21 03:15:53 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.9021
2022-02-21 03:16:26 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.6897
2022-02-21 03:16:57 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.8251
2022-02-21 03:17:30 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.1213
2022-02-21 03:18:03 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.6589
2022-02-21 03:18:33 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.1636
2022-02-21 03:19:07 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.8918
2022-02-21 03:19:38 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.8997
2022-02-21 03:20:11 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.0633
2022-02-21 03:20:43 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.7958
2022-02-21 03:21:14 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.6871
2022-02-21 03:21:47 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.8859
2022-02-21 03:22:19 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.7222
2022-02-21 03:22:52 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.7488
2022-02-21 03:23:23 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 2.1295
2022-02-21 03:23:56 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.9958
2022-02-21 03:24:28 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.8596
2022-02-21 03:25:01 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.9751
2022-02-21 03:25:34 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.8869
2022-02-21 03:26:07 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.7727
2022-02-21 03:26:40 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.9965
2022-02-21 03:27:14 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.8539
2022-02-21 03:27:47 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.7978
2022-02-21 03:28:20 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.0818
2022-02-21 03:28:53 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.8612
2022-02-21 03:29:25 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.9928
2022-02-21 03:29:56 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.9746
2022-02-21 03:30:29 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.8182
2022-02-21 03:31:01 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.9689
2022-02-21 03:31:33 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.7874
2022-02-21 03:32:06 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.8941
2022-02-21 03:32:37 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.9173
2022-02-21 03:33:10 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 2.0115
2022-02-21 03:33:12 - train: epoch 046, train_loss: 1.9156
2022-02-21 03:34:27 - eval: epoch: 046, acc1: 60.222%, acc5: 83.174%, test_loss: 1.6543, per_image_load_time: 0.951ms, per_image_inference_time: 0.164ms
2022-02-21 03:34:27 - until epoch: 046, best_acc1: 61.478%
2022-02-21 03:34:27 - epoch 047 lr: 0.010000000000000002
2022-02-21 03:35:04 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.8698
2022-02-21 03:35:37 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.1463
2022-02-21 03:36:08 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.8164
2022-02-21 03:36:40 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.7823
2022-02-21 03:37:13 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.9446
2022-02-21 03:37:45 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.9806
2022-02-21 03:38:17 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 2.0302
2022-02-21 03:38:50 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.7337
2022-02-21 03:39:23 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.0041
2022-02-21 03:39:54 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.9528
2022-02-21 03:40:27 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.9581
2022-02-21 03:40:59 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.8255
2022-02-21 03:41:31 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.9369
2022-02-21 03:42:04 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.8526
2022-02-21 03:42:37 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.8269
2022-02-21 03:43:07 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.8116
2022-02-21 03:43:39 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.7615
2022-02-21 03:44:12 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.8426
2022-02-21 03:44:45 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.8408
2022-02-21 03:45:16 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.8609
2022-02-21 03:45:49 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.0902
2022-02-21 03:46:21 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.0221
2022-02-21 03:46:53 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.8269
2022-02-21 03:47:25 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.7634
2022-02-21 03:47:57 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.9280
2022-02-21 03:48:28 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.1872
2022-02-21 03:48:59 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.8871
2022-02-21 03:49:32 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.0933
2022-02-21 03:50:04 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.8582
2022-02-21 03:50:36 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.9829
2022-02-21 03:51:08 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.8113
2022-02-21 03:51:39 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.2048
2022-02-21 03:52:12 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.7665
2022-02-21 03:52:43 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.8929
2022-02-21 03:53:15 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.0196
2022-02-21 03:53:47 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 2.0592
2022-02-21 03:54:20 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.1034
2022-02-21 03:54:51 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.8591
2022-02-21 03:55:24 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.8051
2022-02-21 03:55:56 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.8810
2022-02-21 03:56:28 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.9767
2022-02-21 03:57:00 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.9782
2022-02-21 03:57:31 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.9366
2022-02-21 03:58:03 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.8894
2022-02-21 03:58:35 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 2.0016
2022-02-21 03:59:07 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.9765
2022-02-21 03:59:39 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.9628
2022-02-21 04:00:11 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.8510
2022-02-21 04:00:45 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 2.1174
2022-02-21 04:01:16 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.8821
2022-02-21 04:01:18 - train: epoch 047, train_loss: 1.9158
2022-02-21 04:02:32 - eval: epoch: 047, acc1: 60.788%, acc5: 83.576%, test_loss: 1.6299, per_image_load_time: 2.378ms, per_image_inference_time: 0.175ms
2022-02-21 04:02:32 - until epoch: 047, best_acc1: 61.478%
2022-02-21 04:02:32 - epoch 048 lr: 0.010000000000000002
2022-02-21 04:03:11 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 2.0491
2022-02-21 04:03:45 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.3359
2022-02-21 04:04:17 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.9482
2022-02-21 04:04:51 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.8631
2022-02-21 04:05:24 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.7936
2022-02-21 04:05:58 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.8842
2022-02-21 04:06:30 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.8644
2022-02-21 04:07:04 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.9929
2022-02-21 04:07:37 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.9607
2022-02-21 04:08:10 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.9136
2022-02-21 04:08:43 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.9896
2022-02-21 04:09:14 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.0075
2022-02-21 04:09:46 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.6612
2022-02-21 04:10:20 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.8974
2022-02-21 04:10:52 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.9630
2022-02-21 04:11:25 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.7714
2022-02-21 04:11:58 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 2.1324
2022-02-21 04:12:32 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.9513
2022-02-21 04:13:05 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.0348
2022-02-21 04:13:39 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.0368
2022-02-21 04:14:11 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.8920
2022-02-21 04:14:43 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.1723
2022-02-21 04:15:16 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.7433
2022-02-21 04:15:49 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.0403
2022-02-21 04:16:23 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.0242
2022-02-21 04:16:55 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.1476
2022-02-21 04:17:28 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.0606
2022-02-21 04:18:01 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.9315
2022-02-21 04:18:35 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.0573
2022-02-21 04:19:08 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.8785
2022-02-21 04:19:41 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.9736
2022-02-21 04:20:13 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.6259
2022-02-21 04:20:47 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.0706
2022-02-21 04:21:19 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.8378
2022-02-21 04:21:53 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.0668
2022-02-21 04:22:26 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.9865
2022-02-21 04:22:59 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.9559
2022-02-21 04:23:32 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.9619
2022-02-21 04:24:06 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.0042
2022-02-21 04:24:38 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.6583
2022-02-21 04:25:12 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.1857
2022-02-21 04:25:44 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.8849
2022-02-21 04:26:17 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.9213
2022-02-21 04:26:51 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.7255
2022-02-21 04:27:25 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.9210
2022-02-21 04:27:58 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.9058
2022-02-21 04:28:31 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.0896
2022-02-21 04:29:04 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.1336
2022-02-21 04:29:38 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.9508
2022-02-21 04:30:10 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.9469
2022-02-21 04:30:12 - train: epoch 048, train_loss: 1.9196
2022-02-21 04:31:27 - eval: epoch: 048, acc1: 60.608%, acc5: 83.504%, test_loss: 1.6311, per_image_load_time: 1.482ms, per_image_inference_time: 0.178ms
2022-02-21 04:31:27 - until epoch: 048, best_acc1: 61.478%
2022-02-21 04:31:27 - epoch 049 lr: 0.010000000000000002
2022-02-21 04:32:05 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.1585
2022-02-21 04:32:39 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.7965
2022-02-21 04:33:11 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.0219
2022-02-21 04:33:45 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.9954
2022-02-21 04:34:17 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.8650
2022-02-21 04:34:51 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.9869
2022-02-21 04:35:24 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.0006
2022-02-21 04:35:57 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.2596
2022-02-21 04:36:29 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.7966
2022-02-21 04:37:02 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.9480
2022-02-21 04:37:35 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.8278
2022-02-21 04:38:08 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.6280
2022-02-21 04:38:41 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.0134
2022-02-21 04:39:15 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.0037
2022-02-21 04:39:48 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.7908
2022-02-21 04:40:22 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.1416
2022-02-21 04:40:54 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.9732
2022-02-21 04:41:28 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.8055
2022-02-21 04:42:00 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.8890
2022-02-21 04:42:34 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.8985
2022-02-21 04:43:06 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.8358
2022-02-21 04:43:40 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.8666
2022-02-21 04:44:12 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.8647
2022-02-21 04:44:44 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.0333
2022-02-21 04:45:17 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.9412
2022-02-21 04:45:50 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.9361
2022-02-21 04:46:24 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.7707
2022-02-21 04:46:57 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.8430
2022-02-21 04:47:30 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.9582
2022-02-21 04:48:04 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.9479
2022-02-21 04:48:36 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.9847
2022-02-21 04:49:10 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.0738
2022-02-21 04:49:43 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.9255
2022-02-21 04:50:17 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.0415
2022-02-21 04:50:49 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.0008
2022-02-21 04:51:22 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.0358
2022-02-21 04:51:54 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.9454
2022-02-21 04:52:29 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.0495
2022-02-21 04:53:01 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.1074
2022-02-21 04:53:35 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.8525
2022-02-21 04:54:07 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.8378
2022-02-21 04:54:39 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.9097
2022-02-21 04:55:13 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.1767
2022-02-21 04:55:46 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.8970
2022-02-21 04:56:18 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.8125
2022-02-21 04:56:51 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.9829
2022-02-21 04:57:23 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.0848
2022-02-21 04:57:57 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.7119
2022-02-21 04:58:30 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.8156
2022-02-21 04:59:02 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.7785
2022-02-21 04:59:04 - train: epoch 049, train_loss: 1.9137
2022-02-21 05:00:19 - eval: epoch: 049, acc1: 60.400%, acc5: 83.426%, test_loss: 1.6501, per_image_load_time: 2.572ms, per_image_inference_time: 0.190ms
2022-02-21 05:00:19 - until epoch: 049, best_acc1: 61.478%
2022-02-21 05:00:19 - epoch 050 lr: 0.010000000000000002
2022-02-21 05:00:57 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 2.1121
2022-02-21 05:01:31 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.8817
2022-02-21 05:02:03 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.9521
2022-02-21 05:02:37 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.7714
2022-02-21 05:03:10 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.9287
2022-02-21 05:03:44 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.0404
2022-02-21 05:04:17 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.7271
2022-02-21 05:04:50 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.6816
2022-02-21 05:05:22 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.6572
2022-02-21 05:05:55 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.0385
2022-02-21 05:06:29 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.9149
2022-02-21 05:07:02 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 2.0351
2022-02-21 05:07:36 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.6900
2022-02-21 05:08:09 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.8349
2022-02-21 05:08:42 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.8757
2022-02-21 05:09:15 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.8916
2022-02-21 05:09:48 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.9754
2022-02-21 05:10:21 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.9075
2022-02-21 05:10:54 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.8319
