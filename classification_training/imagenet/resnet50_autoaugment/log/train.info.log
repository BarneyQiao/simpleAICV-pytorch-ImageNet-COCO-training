2022-03-04 23:11:58 - train: epoch 0173, iter [01800, 05004], lr: 0.005002, loss: 1.7275
2022-03-04 23:12:31 - train: epoch 0173, iter [01900, 05004], lr: 0.005002, loss: 1.6670
2022-03-04 23:13:04 - train: epoch 0173, iter [02000, 05004], lr: 0.005002, loss: 1.9090
2022-03-04 23:13:37 - train: epoch 0173, iter [02100, 05004], lr: 0.005002, loss: 1.5238
2022-03-04 23:14:09 - train: epoch 0173, iter [02200, 05004], lr: 0.005002, loss: 1.5401
2022-03-04 23:14:42 - train: epoch 0173, iter [02300, 05004], lr: 0.005002, loss: 1.5919
2022-03-04 23:15:15 - train: epoch 0173, iter [02400, 05004], lr: 0.005002, loss: 1.6146
2022-03-04 23:15:47 - train: epoch 0173, iter [02500, 05004], lr: 0.005002, loss: 1.5334
2022-03-04 23:16:20 - train: epoch 0173, iter [02600, 05004], lr: 0.005002, loss: 1.6790
2022-03-04 23:16:53 - train: epoch 0173, iter [02700, 05004], lr: 0.005002, loss: 1.4559
2022-03-04 23:17:27 - train: epoch 0173, iter [02800, 05004], lr: 0.005002, loss: 1.8013
2022-03-04 23:17:59 - train: epoch 0173, iter [02900, 05004], lr: 0.005002, loss: 1.5512
2022-03-04 23:18:31 - train: epoch 0173, iter [03000, 05004], lr: 0.005002, loss: 1.4578
2022-03-04 23:19:03 - train: epoch 0173, iter [03100, 05004], lr: 0.005002, loss: 1.5619
2022-03-04 23:19:36 - train: epoch 0173, iter [03200, 05004], lr: 0.005002, loss: 2.0210
2022-03-04 23:20:09 - train: epoch 0173, iter [03300, 05004], lr: 0.005002, loss: 1.6028
2022-03-04 23:20:42 - train: epoch 0173, iter [03400, 05004], lr: 0.005002, loss: 1.4929
2022-03-04 23:21:15 - train: epoch 0173, iter [03500, 05004], lr: 0.005002, loss: 1.8459
2022-03-04 23:21:47 - train: epoch 0173, iter [03600, 05004], lr: 0.005002, loss: 1.6274
2022-03-04 23:22:21 - train: epoch 0173, iter [03700, 05004], lr: 0.005002, loss: 1.6855
2022-03-04 23:22:54 - train: epoch 0173, iter [03800, 05004], lr: 0.005002, loss: 1.6711
2022-03-04 23:23:27 - train: epoch 0173, iter [03900, 05004], lr: 0.005002, loss: 1.3552
2022-03-04 23:24:00 - train: epoch 0173, iter [04000, 05004], lr: 0.005002, loss: 1.8469
2022-03-04 23:24:33 - train: epoch 0173, iter [04100, 05004], lr: 0.005002, loss: 1.6713
2022-03-04 23:25:06 - train: epoch 0173, iter [04200, 05004], lr: 0.005002, loss: 1.5733
2022-03-04 23:25:39 - train: epoch 0173, iter [04300, 05004], lr: 0.005002, loss: 1.4987
2022-03-04 23:26:12 - train: epoch 0173, iter [04400, 05004], lr: 0.005002, loss: 1.6124
2022-03-04 23:26:44 - train: epoch 0173, iter [04500, 05004], lr: 0.005002, loss: 1.4427
2022-03-04 23:27:17 - train: epoch 0173, iter [04600, 05004], lr: 0.005002, loss: 1.4058
2022-03-04 23:27:51 - train: epoch 0173, iter [04700, 05004], lr: 0.005002, loss: 1.6496
2022-03-04 23:28:23 - train: epoch 0173, iter [04800, 05004], lr: 0.005002, loss: 1.8032
2022-03-04 23:28:55 - train: epoch 0173, iter [04900, 05004], lr: 0.005002, loss: 1.6124
2022-03-04 23:29:27 - train: epoch 0173, iter [05000, 05004], lr: 0.005002, loss: 1.6455
2022-03-04 23:29:28 - train: epoch 173, train_loss: 1.6567
2022-03-04 23:30:41 - eval: epoch: 173, acc1: 73.942%, acc5: 91.870%, test_loss: 1.0381, per_image_load_time: 0.765ms, per_image_inference_time: 0.440ms
2022-03-04 23:30:41 - until epoch: 173, best_acc1: 73.970%
2022-03-04 23:30:41 - epoch 174 lr: 0.00465628195747273
2022-03-04 23:31:20 - train: epoch 0174, iter [00100, 05004], lr: 0.004656, loss: 1.4296
2022-03-04 23:31:53 - train: epoch 0174, iter [00200, 05004], lr: 0.004656, loss: 1.8222
2022-03-04 23:32:26 - train: epoch 0174, iter [00300, 05004], lr: 0.004656, loss: 1.7967
2022-03-04 23:32:59 - train: epoch 0174, iter [00400, 05004], lr: 0.004656, loss: 1.6651
2022-03-04 23:33:32 - train: epoch 0174, iter [00500, 05004], lr: 0.004656, loss: 1.6089
2022-03-04 23:34:06 - train: epoch 0174, iter [00600, 05004], lr: 0.004656, loss: 1.5767
2022-03-04 23:34:38 - train: epoch 0174, iter [00700, 05004], lr: 0.004656, loss: 1.4725
2022-03-04 23:35:11 - train: epoch 0174, iter [00800, 05004], lr: 0.004656, loss: 1.6864
2022-03-04 23:35:44 - train: epoch 0174, iter [00900, 05004], lr: 0.004656, loss: 1.7513
2022-03-04 23:36:17 - train: epoch 0174, iter [01000, 05004], lr: 0.004656, loss: 1.6816
2022-03-04 23:36:51 - train: epoch 0174, iter [01100, 05004], lr: 0.004656, loss: 1.5301
2022-03-04 23:37:24 - train: epoch 0174, iter [01200, 05004], lr: 0.004656, loss: 1.7264
2022-03-04 23:37:57 - train: epoch 0174, iter [01300, 05004], lr: 0.004656, loss: 1.4889
2022-03-04 23:38:31 - train: epoch 0174, iter [01400, 05004], lr: 0.004656, loss: 1.7907
2022-03-04 23:39:03 - train: epoch 0174, iter [01500, 05004], lr: 0.004656, loss: 1.5651
2022-03-04 23:39:37 - train: epoch 0174, iter [01600, 05004], lr: 0.004656, loss: 1.6088
2022-03-04 23:40:10 - train: epoch 0174, iter [01700, 05004], lr: 0.004656, loss: 1.8514
2022-03-04 23:40:43 - train: epoch 0174, iter [01800, 05004], lr: 0.004656, loss: 1.8144
2022-03-04 23:41:17 - train: epoch 0174, iter [01900, 05004], lr: 0.004656, loss: 1.7179
2022-03-04 23:41:50 - train: epoch 0174, iter [02000, 05004], lr: 0.004656, loss: 1.7394
2022-03-04 23:42:24 - train: epoch 0174, iter [02100, 05004], lr: 0.004656, loss: 1.5772
2022-03-04 23:42:56 - train: epoch 0174, iter [02200, 05004], lr: 0.004656, loss: 1.6562
2022-03-04 23:43:29 - train: epoch 0174, iter [02300, 05004], lr: 0.004656, loss: 1.8240
2022-03-04 23:44:01 - train: epoch 0174, iter [02400, 05004], lr: 0.004656, loss: 1.5584
2022-03-04 23:44:35 - train: epoch 0174, iter [02500, 05004], lr: 0.004656, loss: 1.6677
2022-03-04 23:45:08 - train: epoch 0174, iter [02600, 05004], lr: 0.004656, loss: 1.3799
2022-03-04 23:45:42 - train: epoch 0174, iter [02700, 05004], lr: 0.004656, loss: 1.8319
2022-03-04 23:46:14 - train: epoch 0174, iter [02800, 05004], lr: 0.004656, loss: 1.7693
2022-03-04 23:46:48 - train: epoch 0174, iter [02900, 05004], lr: 0.004656, loss: 1.5164
2022-03-04 23:47:21 - train: epoch 0174, iter [03000, 05004], lr: 0.004656, loss: 1.5388
2022-03-04 23:47:55 - train: epoch 0174, iter [03100, 05004], lr: 0.004656, loss: 1.7396
2022-03-04 23:48:28 - train: epoch 0174, iter [03200, 05004], lr: 0.004656, loss: 1.8166
2022-03-04 23:49:01 - train: epoch 0174, iter [03300, 05004], lr: 0.004656, loss: 1.6423
2022-03-04 23:49:33 - train: epoch 0174, iter [03400, 05004], lr: 0.004656, loss: 1.5118
2022-03-04 23:50:06 - train: epoch 0174, iter [03500, 05004], lr: 0.004656, loss: 1.6440
2022-03-04 23:50:39 - train: epoch 0174, iter [03600, 05004], lr: 0.004656, loss: 1.6011
2022-03-04 23:51:12 - train: epoch 0174, iter [03700, 05004], lr: 0.004656, loss: 1.7540
2022-03-04 23:51:45 - train: epoch 0174, iter [03800, 05004], lr: 0.004656, loss: 1.7168
2022-03-04 23:52:18 - train: epoch 0174, iter [03900, 05004], lr: 0.004656, loss: 1.8954
2022-03-04 23:52:51 - train: epoch 0174, iter [04000, 05004], lr: 0.004656, loss: 1.7443
2022-03-04 23:53:25 - train: epoch 0174, iter [04100, 05004], lr: 0.004656, loss: 1.5234
2022-03-04 23:53:59 - train: epoch 0174, iter [04200, 05004], lr: 0.004656, loss: 1.6243
2022-03-04 23:54:33 - train: epoch 0174, iter [04300, 05004], lr: 0.004656, loss: 1.3195
2022-03-04 23:55:07 - train: epoch 0174, iter [04400, 05004], lr: 0.004656, loss: 1.6776
2022-03-04 23:55:41 - train: epoch 0174, iter [04500, 05004], lr: 0.004656, loss: 1.7195
2022-03-04 23:56:14 - train: epoch 0174, iter [04600, 05004], lr: 0.004656, loss: 1.3935
2022-03-04 23:56:47 - train: epoch 0174, iter [04700, 05004], lr: 0.004656, loss: 1.8902
2022-03-04 23:57:21 - train: epoch 0174, iter [04800, 05004], lr: 0.004656, loss: 1.9970
2022-03-04 23:57:55 - train: epoch 0174, iter [04900, 05004], lr: 0.004656, loss: 1.5718
2022-03-04 23:58:27 - train: epoch 0174, iter [05000, 05004], lr: 0.004656, loss: 1.5923
2022-03-04 23:58:28 - train: epoch 174, train_loss: 1.6421
2022-03-04 23:59:42 - eval: epoch: 174, acc1: 74.230%, acc5: 92.044%, test_loss: 1.0208, per_image_load_time: 0.762ms, per_image_inference_time: 0.487ms
2022-03-04 23:59:43 - until epoch: 174, best_acc1: 74.230%
2022-03-04 23:59:43 - epoch 175 lr: 0.004322727117869951
2022-03-05 00:00:21 - train: epoch 0175, iter [00100, 05004], lr: 0.004323, loss: 1.5546
2022-03-05 00:00:55 - train: epoch 0175, iter [00200, 05004], lr: 0.004323, loss: 1.7599
2022-03-05 00:01:30 - train: epoch 0175, iter [00300, 05004], lr: 0.004323, loss: 1.5832
2022-03-05 00:02:03 - train: epoch 0175, iter [00400, 05004], lr: 0.004323, loss: 1.7671
2022-03-05 00:02:36 - train: epoch 0175, iter [00500, 05004], lr: 0.004323, loss: 1.6289
2022-03-05 00:03:10 - train: epoch 0175, iter [00600, 05004], lr: 0.004323, loss: 1.6891
2022-03-05 00:03:44 - train: epoch 0175, iter [00700, 05004], lr: 0.004323, loss: 1.6305
2022-03-05 00:04:17 - train: epoch 0175, iter [00800, 05004], lr: 0.004323, loss: 1.6008
2022-03-05 00:04:52 - train: epoch 0175, iter [00900, 05004], lr: 0.004323, loss: 1.6194
2022-03-05 00:05:24 - train: epoch 0175, iter [01000, 05004], lr: 0.004323, loss: 1.7604
2022-03-05 00:05:58 - train: epoch 0175, iter [01100, 05004], lr: 0.004323, loss: 1.4963
2022-03-05 00:06:32 - train: epoch 0175, iter [01200, 05004], lr: 0.004323, loss: 1.4733
2022-03-05 00:07:05 - train: epoch 0175, iter [01300, 05004], lr: 0.004323, loss: 1.7684
2022-03-05 00:07:38 - train: epoch 0175, iter [01400, 05004], lr: 0.004323, loss: 1.7242
2022-03-05 00:08:12 - train: epoch 0175, iter [01500, 05004], lr: 0.004323, loss: 1.9155
2022-03-05 00:08:46 - train: epoch 0175, iter [01600, 05004], lr: 0.004323, loss: 1.7534
2022-03-05 00:09:19 - train: epoch 0175, iter [01700, 05004], lr: 0.004323, loss: 1.5465
2022-03-05 00:09:53 - train: epoch 0175, iter [01800, 05004], lr: 0.004323, loss: 1.6446
2022-03-05 00:10:27 - train: epoch 0175, iter [01900, 05004], lr: 0.004323, loss: 1.4191
2022-03-05 00:11:01 - train: epoch 0175, iter [02000, 05004], lr: 0.004323, loss: 1.5189
2022-03-05 00:11:34 - train: epoch 0175, iter [02100, 05004], lr: 0.004323, loss: 1.7198
2022-03-05 00:12:08 - train: epoch 0175, iter [02200, 05004], lr: 0.004323, loss: 1.4195
2022-03-05 00:12:40 - train: epoch 0175, iter [02300, 05004], lr: 0.004323, loss: 1.4922
2022-03-05 00:13:15 - train: epoch 0175, iter [02400, 05004], lr: 0.004323, loss: 1.4493
2022-03-05 00:13:48 - train: epoch 0175, iter [02500, 05004], lr: 0.004323, loss: 1.5905
2022-03-05 00:14:22 - train: epoch 0175, iter [02600, 05004], lr: 0.004323, loss: 1.8803
2022-03-05 00:14:55 - train: epoch 0175, iter [02700, 05004], lr: 0.004323, loss: 1.4107
2022-03-05 00:15:29 - train: epoch 0175, iter [02800, 05004], lr: 0.004323, loss: 1.5443
2022-03-05 00:16:02 - train: epoch 0175, iter [02900, 05004], lr: 0.004323, loss: 1.6213
2022-03-05 00:16:36 - train: epoch 0175, iter [03000, 05004], lr: 0.004323, loss: 1.6776
2022-03-05 00:17:09 - train: epoch 0175, iter [03100, 05004], lr: 0.004323, loss: 1.3092
2022-03-05 00:17:43 - train: epoch 0175, iter [03200, 05004], lr: 0.004323, loss: 1.7105
2022-03-05 00:18:17 - train: epoch 0175, iter [03300, 05004], lr: 0.004323, loss: 1.8167
2022-03-05 00:18:51 - train: epoch 0175, iter [03400, 05004], lr: 0.004323, loss: 1.8709
2022-03-05 00:19:24 - train: epoch 0175, iter [03500, 05004], lr: 0.004323, loss: 1.3922
2022-03-05 00:19:58 - train: epoch 0175, iter [03600, 05004], lr: 0.004323, loss: 1.7381
2022-03-05 00:20:31 - train: epoch 0175, iter [03700, 05004], lr: 0.004323, loss: 1.7680
2022-03-05 00:21:05 - train: epoch 0175, iter [03800, 05004], lr: 0.004323, loss: 1.9027
2022-03-05 00:21:39 - train: epoch 0175, iter [03900, 05004], lr: 0.004323, loss: 1.7179
2022-03-05 00:22:13 - train: epoch 0175, iter [04000, 05004], lr: 0.004323, loss: 1.4816
2022-03-05 00:22:46 - train: epoch 0175, iter [04100, 05004], lr: 0.004323, loss: 1.5306
2022-03-05 00:23:19 - train: epoch 0175, iter [04200, 05004], lr: 0.004323, loss: 1.3003
2022-03-05 00:23:52 - train: epoch 0175, iter [04300, 05004], lr: 0.004323, loss: 1.6806
2022-03-05 00:24:27 - train: epoch 0175, iter [04400, 05004], lr: 0.004323, loss: 1.5055
2022-03-05 00:25:00 - train: epoch 0175, iter [04500, 05004], lr: 0.004323, loss: 1.5814
2022-03-05 00:25:34 - train: epoch 0175, iter [04600, 05004], lr: 0.004323, loss: 1.6749
2022-03-05 00:26:07 - train: epoch 0175, iter [04700, 05004], lr: 0.004323, loss: 1.5027
2022-03-05 00:26:40 - train: epoch 0175, iter [04800, 05004], lr: 0.004323, loss: 1.6804
2022-03-05 00:27:14 - train: epoch 0175, iter [04900, 05004], lr: 0.004323, loss: 1.4867
2022-03-05 00:27:46 - train: epoch 0175, iter [05000, 05004], lr: 0.004323, loss: 1.4847
2022-03-05 00:27:47 - train: epoch 175, train_loss: 1.6264
2022-03-05 00:29:01 - eval: epoch: 175, acc1: 74.284%, acc5: 92.194%, test_loss: 1.0193, per_image_load_time: 0.777ms, per_image_inference_time: 0.478ms
2022-03-05 00:29:02 - until epoch: 175, best_acc1: 74.284%
2022-03-05 00:29:02 - epoch 176 lr: 0.0040010278170587886
2022-03-05 00:29:39 - train: epoch 0176, iter [00100, 05004], lr: 0.004001, loss: 1.8681
2022-03-05 00:30:14 - train: epoch 0176, iter [00200, 05004], lr: 0.004001, loss: 1.4954
2022-03-05 00:30:48 - train: epoch 0176, iter [00300, 05004], lr: 0.004001, loss: 1.7168
2022-03-05 00:31:22 - train: epoch 0176, iter [00400, 05004], lr: 0.004001, loss: 1.5735
2022-03-05 00:31:56 - train: epoch 0176, iter [00500, 05004], lr: 0.004001, loss: 1.5324
2022-03-05 00:32:29 - train: epoch 0176, iter [00600, 05004], lr: 0.004001, loss: 1.6899
2022-03-05 00:33:03 - train: epoch 0176, iter [00700, 05004], lr: 0.004001, loss: 1.6683
2022-03-05 00:33:36 - train: epoch 0176, iter [00800, 05004], lr: 0.004001, loss: 1.7677
2022-03-05 00:34:11 - train: epoch 0176, iter [00900, 05004], lr: 0.004001, loss: 1.6700
2022-03-05 00:34:44 - train: epoch 0176, iter [01000, 05004], lr: 0.004001, loss: 1.5988
2022-03-05 00:35:17 - train: epoch 0176, iter [01100, 05004], lr: 0.004001, loss: 1.3503
2022-03-05 00:35:51 - train: epoch 0176, iter [01200, 05004], lr: 0.004001, loss: 1.6434
2022-03-05 00:36:24 - train: epoch 0176, iter [01300, 05004], lr: 0.004001, loss: 1.4768
2022-03-05 00:36:58 - train: epoch 0176, iter [01400, 05004], lr: 0.004001, loss: 1.3656
2022-03-05 00:37:32 - train: epoch 0176, iter [01500, 05004], lr: 0.004001, loss: 1.5163
2022-03-05 00:38:06 - train: epoch 0176, iter [01600, 05004], lr: 0.004001, loss: 1.5438
2022-03-05 00:38:39 - train: epoch 0176, iter [01700, 05004], lr: 0.004001, loss: 1.5561
2022-03-05 00:39:13 - train: epoch 0176, iter [01800, 05004], lr: 0.004001, loss: 1.6617
2022-03-05 00:39:46 - train: epoch 0176, iter [01900, 05004], lr: 0.004001, loss: 1.5139
2022-03-05 00:40:21 - train: epoch 0176, iter [02000, 05004], lr: 0.004001, loss: 1.7461
2022-03-05 00:40:54 - train: epoch 0176, iter [02100, 05004], lr: 0.004001, loss: 1.6403
2022-03-05 00:41:29 - train: epoch 0176, iter [02200, 05004], lr: 0.004001, loss: 1.7609
2022-03-05 00:42:03 - train: epoch 0176, iter [02300, 05004], lr: 0.004001, loss: 1.6947
2022-03-05 00:42:36 - train: epoch 0176, iter [02400, 05004], lr: 0.004001, loss: 1.4408
2022-03-05 00:43:10 - train: epoch 0176, iter [02500, 05004], lr: 0.004001, loss: 1.3359
2022-03-05 00:43:44 - train: epoch 0176, iter [02600, 05004], lr: 0.004001, loss: 1.6916
2022-03-05 00:44:18 - train: epoch 0176, iter [02700, 05004], lr: 0.004001, loss: 1.6031
2022-03-05 00:44:52 - train: epoch 0176, iter [02800, 05004], lr: 0.004001, loss: 1.8006
2022-03-05 00:45:27 - train: epoch 0176, iter [02900, 05004], lr: 0.004001, loss: 1.9548
2022-03-05 00:45:59 - train: epoch 0176, iter [03000, 05004], lr: 0.004001, loss: 1.8699
2022-03-05 00:46:33 - train: epoch 0176, iter [03100, 05004], lr: 0.004001, loss: 1.4867
2022-03-05 00:47:07 - train: epoch 0176, iter [03200, 05004], lr: 0.004001, loss: 1.5534
2022-03-05 00:47:41 - train: epoch 0176, iter [03300, 05004], lr: 0.004001, loss: 1.6446
2022-03-05 00:48:14 - train: epoch 0176, iter [03400, 05004], lr: 0.004001, loss: 1.7670
2022-03-05 00:48:48 - train: epoch 0176, iter [03500, 05004], lr: 0.004001, loss: 1.7333
2022-03-05 00:49:21 - train: epoch 0176, iter [03600, 05004], lr: 0.004001, loss: 1.7980
2022-03-05 00:49:55 - train: epoch 0176, iter [03700, 05004], lr: 0.004001, loss: 1.4924
2022-03-05 00:50:28 - train: epoch 0176, iter [03800, 05004], lr: 0.004001, loss: 1.7066
2022-03-05 00:51:02 - train: epoch 0176, iter [03900, 05004], lr: 0.004001, loss: 1.4442
2022-03-05 00:51:36 - train: epoch 0176, iter [04000, 05004], lr: 0.004001, loss: 1.6944
2022-03-05 00:52:10 - train: epoch 0176, iter [04100, 05004], lr: 0.004001, loss: 1.3113
2022-03-05 00:52:43 - train: epoch 0176, iter [04200, 05004], lr: 0.004001, loss: 1.6000
2022-03-05 00:53:17 - train: epoch 0176, iter [04300, 05004], lr: 0.004001, loss: 1.6070
2022-03-05 00:53:50 - train: epoch 0176, iter [04400, 05004], lr: 0.004001, loss: 1.4551
2022-03-05 00:54:24 - train: epoch 0176, iter [04500, 05004], lr: 0.004001, loss: 1.5617
2022-03-05 00:54:58 - train: epoch 0176, iter [04600, 05004], lr: 0.004001, loss: 1.4036
2022-03-05 00:55:32 - train: epoch 0176, iter [04700, 05004], lr: 0.004001, loss: 1.5873
2022-03-05 00:56:06 - train: epoch 0176, iter [04800, 05004], lr: 0.004001, loss: 2.0286
2022-03-05 00:56:39 - train: epoch 0176, iter [04900, 05004], lr: 0.004001, loss: 1.6276
2022-03-05 00:57:11 - train: epoch 0176, iter [05000, 05004], lr: 0.004001, loss: 1.6634
2022-03-05 00:57:12 - train: epoch 176, train_loss: 1.6126
2022-03-05 00:58:27 - eval: epoch: 176, acc1: 74.696%, acc5: 92.184%, test_loss: 1.0055, per_image_load_time: 1.837ms, per_image_inference_time: 0.519ms
2022-03-05 00:58:28 - until epoch: 176, best_acc1: 74.696%
2022-03-05 00:58:28 - epoch 177 lr: 0.003691267552111183
2022-03-05 00:59:07 - train: epoch 0177, iter [00100, 05004], lr: 0.003691, loss: 1.5452
2022-03-05 00:59:40 - train: epoch 0177, iter [00200, 05004], lr: 0.003691, loss: 1.5550
2022-03-05 01:00:14 - train: epoch 0177, iter [00300, 05004], lr: 0.003691, loss: 1.6925
2022-03-05 01:00:48 - train: epoch 0177, iter [00400, 05004], lr: 0.003691, loss: 1.5239
2022-03-05 01:01:22 - train: epoch 0177, iter [00500, 05004], lr: 0.003691, loss: 1.7562
2022-03-05 01:01:56 - train: epoch 0177, iter [00600, 05004], lr: 0.003691, loss: 1.5989
2022-03-05 01:02:29 - train: epoch 0177, iter [00700, 05004], lr: 0.003691, loss: 1.5326
2022-03-05 01:03:03 - train: epoch 0177, iter [00800, 05004], lr: 0.003691, loss: 1.4691
2022-03-05 01:03:37 - train: epoch 0177, iter [00900, 05004], lr: 0.003691, loss: 1.4876
2022-03-05 01:04:10 - train: epoch 0177, iter [01000, 05004], lr: 0.003691, loss: 1.5240
2022-03-05 01:04:44 - train: epoch 0177, iter [01100, 05004], lr: 0.003691, loss: 1.6726
2022-03-05 01:05:18 - train: epoch 0177, iter [01200, 05004], lr: 0.003691, loss: 1.6058
2022-03-05 01:05:52 - train: epoch 0177, iter [01300, 05004], lr: 0.003691, loss: 1.5772
2022-03-05 01:06:24 - train: epoch 0177, iter [01400, 05004], lr: 0.003691, loss: 1.7456
2022-03-05 01:06:59 - train: epoch 0177, iter [01500, 05004], lr: 0.003691, loss: 1.7113
2022-03-05 01:07:33 - train: epoch 0177, iter [01600, 05004], lr: 0.003691, loss: 1.4384
2022-03-05 01:08:06 - train: epoch 0177, iter [01700, 05004], lr: 0.003691, loss: 1.6486
2022-03-05 01:08:40 - train: epoch 0177, iter [01800, 05004], lr: 0.003691, loss: 1.6314
2022-03-05 01:09:14 - train: epoch 0177, iter [01900, 05004], lr: 0.003691, loss: 1.5527
2022-03-05 01:09:47 - train: epoch 0177, iter [02000, 05004], lr: 0.003691, loss: 1.2817
2022-03-05 01:10:21 - train: epoch 0177, iter [02100, 05004], lr: 0.003691, loss: 1.8577
2022-03-05 01:10:55 - train: epoch 0177, iter [02200, 05004], lr: 0.003691, loss: 1.7861
2022-03-05 01:11:29 - train: epoch 0177, iter [02300, 05004], lr: 0.003691, loss: 1.5348
2022-03-05 01:12:04 - train: epoch 0177, iter [02400, 05004], lr: 0.003691, loss: 1.4480
2022-03-05 01:12:37 - train: epoch 0177, iter [02500, 05004], lr: 0.003691, loss: 1.7195
2022-03-05 01:13:12 - train: epoch 0177, iter [02600, 05004], lr: 0.003691, loss: 1.4274
2022-03-05 01:13:45 - train: epoch 0177, iter [02700, 05004], lr: 0.003691, loss: 1.4843
2022-03-05 01:14:19 - train: epoch 0177, iter [02800, 05004], lr: 0.003691, loss: 1.5003
2022-03-05 01:14:53 - train: epoch 0177, iter [02900, 05004], lr: 0.003691, loss: 1.8245
2022-03-05 01:15:26 - train: epoch 0177, iter [03000, 05004], lr: 0.003691, loss: 1.6600
2022-03-05 01:16:00 - train: epoch 0177, iter [03100, 05004], lr: 0.003691, loss: 1.3086
2022-03-05 01:16:33 - train: epoch 0177, iter [03200, 05004], lr: 0.003691, loss: 1.4948
2022-03-05 01:17:08 - train: epoch 0177, iter [03300, 05004], lr: 0.003691, loss: 1.6176
2022-03-05 01:17:42 - train: epoch 0177, iter [03400, 05004], lr: 0.003691, loss: 1.8326
2022-03-05 01:18:16 - train: epoch 0177, iter [03500, 05004], lr: 0.003691, loss: 1.6603
2022-03-05 01:18:49 - train: epoch 0177, iter [03600, 05004], lr: 0.003691, loss: 1.8070
2022-03-05 01:19:23 - train: epoch 0177, iter [03700, 05004], lr: 0.003691, loss: 1.4325
2022-03-05 01:19:56 - train: epoch 0177, iter [03800, 05004], lr: 0.003691, loss: 1.5725
2022-03-05 01:20:31 - train: epoch 0177, iter [03900, 05004], lr: 0.003691, loss: 1.5293
2022-03-05 01:21:05 - train: epoch 0177, iter [04000, 05004], lr: 0.003691, loss: 1.7289
2022-03-05 01:21:39 - train: epoch 0177, iter [04100, 05004], lr: 0.003691, loss: 1.7265
2022-03-05 01:22:13 - train: epoch 0177, iter [04200, 05004], lr: 0.003691, loss: 1.7699
2022-03-05 01:22:46 - train: epoch 0177, iter [04300, 05004], lr: 0.003691, loss: 1.5875
2022-03-05 01:23:19 - train: epoch 0177, iter [04400, 05004], lr: 0.003691, loss: 1.7971
2022-03-05 01:23:53 - train: epoch 0177, iter [04500, 05004], lr: 0.003691, loss: 1.5050
2022-03-05 01:24:27 - train: epoch 0177, iter [04600, 05004], lr: 0.003691, loss: 1.7020
2022-03-05 01:25:01 - train: epoch 0177, iter [04700, 05004], lr: 0.003691, loss: 1.2992
2022-03-05 01:25:35 - train: epoch 0177, iter [04800, 05004], lr: 0.003691, loss: 1.7127
2022-03-05 01:26:09 - train: epoch 0177, iter [04900, 05004], lr: 0.003691, loss: 1.5640
2022-03-05 01:26:41 - train: epoch 0177, iter [05000, 05004], lr: 0.003691, loss: 1.8937
2022-03-05 01:26:42 - train: epoch 177, train_loss: 1.5926
2022-03-05 01:27:56 - eval: epoch: 177, acc1: 74.716%, acc5: 92.406%, test_loss: 0.9964, per_image_load_time: 1.423ms, per_image_inference_time: 0.489ms
2022-03-05 01:27:57 - until epoch: 177, best_acc1: 74.716%
2022-03-05 01:27:57 - epoch 178 lr: 0.003393526721321616
2022-03-05 01:28:36 - train: epoch 0178, iter [00100, 05004], lr: 0.003394, loss: 1.5386
2022-03-05 01:29:10 - train: epoch 0178, iter [00200, 05004], lr: 0.003394, loss: 1.6742
2022-03-05 01:29:43 - train: epoch 0178, iter [00300, 05004], lr: 0.003394, loss: 1.5572
2022-03-05 01:30:17 - train: epoch 0178, iter [00400, 05004], lr: 0.003394, loss: 1.5637
2022-03-05 01:30:51 - train: epoch 0178, iter [00500, 05004], lr: 0.003394, loss: 1.6481
2022-03-05 01:31:24 - train: epoch 0178, iter [00600, 05004], lr: 0.003394, loss: 1.4718
2022-03-05 01:31:58 - train: epoch 0178, iter [00700, 05004], lr: 0.003394, loss: 1.5140
2022-03-05 01:32:32 - train: epoch 0178, iter [00800, 05004], lr: 0.003394, loss: 1.8762
2022-03-05 01:33:04 - train: epoch 0178, iter [00900, 05004], lr: 0.003394, loss: 1.7515
2022-03-05 01:33:38 - train: epoch 0178, iter [01000, 05004], lr: 0.003394, loss: 1.5299
2022-03-05 01:34:13 - train: epoch 0178, iter [01100, 05004], lr: 0.003394, loss: 1.5397
2022-03-05 01:34:47 - train: epoch 0178, iter [01200, 05004], lr: 0.003394, loss: 1.6613
2022-03-05 01:35:19 - train: epoch 0178, iter [01300, 05004], lr: 0.003394, loss: 1.8518
2022-03-05 01:35:54 - train: epoch 0178, iter [01400, 05004], lr: 0.003394, loss: 1.4644
2022-03-05 01:36:27 - train: epoch 0178, iter [01500, 05004], lr: 0.003394, loss: 1.7864
2022-03-05 01:37:00 - train: epoch 0178, iter [01600, 05004], lr: 0.003394, loss: 1.6024
2022-03-05 01:37:35 - train: epoch 0178, iter [01700, 05004], lr: 0.003394, loss: 1.6061
2022-03-05 01:38:08 - train: epoch 0178, iter [01800, 05004], lr: 0.003394, loss: 1.7101
2022-03-05 01:38:43 - train: epoch 0178, iter [01900, 05004], lr: 0.003394, loss: 1.3842
2022-03-05 01:39:16 - train: epoch 0178, iter [02000, 05004], lr: 0.003394, loss: 1.5315
2022-03-05 01:39:50 - train: epoch 0178, iter [02100, 05004], lr: 0.003394, loss: 1.4031
2022-03-05 01:40:24 - train: epoch 0178, iter [02200, 05004], lr: 0.003394, loss: 1.4853
2022-03-05 01:40:58 - train: epoch 0178, iter [02300, 05004], lr: 0.003394, loss: 1.5942
2022-03-05 01:41:31 - train: epoch 0178, iter [02400, 05004], lr: 0.003394, loss: 1.6868
2022-03-05 01:42:06 - train: epoch 0178, iter [02500, 05004], lr: 0.003394, loss: 1.5653
2022-03-05 01:42:39 - train: epoch 0178, iter [02600, 05004], lr: 0.003394, loss: 1.4359
2022-03-05 01:43:13 - train: epoch 0178, iter [02700, 05004], lr: 0.003394, loss: 1.7316
2022-03-05 01:43:46 - train: epoch 0178, iter [02800, 05004], lr: 0.003394, loss: 1.4179
2022-03-05 01:44:20 - train: epoch 0178, iter [02900, 05004], lr: 0.003394, loss: 1.3981
2022-03-05 01:44:55 - train: epoch 0178, iter [03000, 05004], lr: 0.003394, loss: 1.7562
2022-03-05 01:45:28 - train: epoch 0178, iter [03100, 05004], lr: 0.003394, loss: 1.4547
2022-03-05 01:46:02 - train: epoch 0178, iter [03200, 05004], lr: 0.003394, loss: 1.5675
2022-03-05 01:46:35 - train: epoch 0178, iter [03300, 05004], lr: 0.003394, loss: 1.9013
2022-03-05 01:47:09 - train: epoch 0178, iter [03400, 05004], lr: 0.003394, loss: 1.5435
2022-03-05 01:47:43 - train: epoch 0178, iter [03500, 05004], lr: 0.003394, loss: 1.7214
2022-03-05 01:48:17 - train: epoch 0178, iter [03600, 05004], lr: 0.003394, loss: 1.5833
2022-03-05 01:48:51 - train: epoch 0178, iter [03700, 05004], lr: 0.003394, loss: 1.5909
2022-03-05 01:49:25 - train: epoch 0178, iter [03800, 05004], lr: 0.003394, loss: 1.8074
2022-03-05 01:49:58 - train: epoch 0178, iter [03900, 05004], lr: 0.003394, loss: 1.4665
2022-03-05 01:50:32 - train: epoch 0178, iter [04000, 05004], lr: 0.003394, loss: 1.7001
2022-03-05 01:51:06 - train: epoch 0178, iter [04100, 05004], lr: 0.003394, loss: 1.7649
2022-03-05 01:51:40 - train: epoch 0178, iter [04200, 05004], lr: 0.003394, loss: 1.6123
2022-03-05 01:52:13 - train: epoch 0178, iter [04300, 05004], lr: 0.003394, loss: 1.5706
2022-03-05 01:52:47 - train: epoch 0178, iter [04400, 05004], lr: 0.003394, loss: 1.5277
2022-03-05 01:53:20 - train: epoch 0178, iter [04500, 05004], lr: 0.003394, loss: 1.5240
2022-03-05 01:53:55 - train: epoch 0178, iter [04600, 05004], lr: 0.003394, loss: 1.5665
2022-03-05 01:54:28 - train: epoch 0178, iter [04700, 05004], lr: 0.003394, loss: 1.5036
2022-03-05 01:55:02 - train: epoch 0178, iter [04800, 05004], lr: 0.003394, loss: 1.8326
2022-03-05 01:55:36 - train: epoch 0178, iter [04900, 05004], lr: 0.003394, loss: 1.6125
2022-03-05 01:56:08 - train: epoch 0178, iter [05000, 05004], lr: 0.003394, loss: 1.6274
2022-03-05 01:56:10 - train: epoch 178, train_loss: 1.5813
2022-03-05 01:57:24 - eval: epoch: 178, acc1: 74.878%, acc5: 92.466%, test_loss: 0.9915, per_image_load_time: 2.364ms, per_image_inference_time: 0.484ms
2022-03-05 01:57:25 - until epoch: 178, best_acc1: 74.878%
2022-03-05 01:57:25 - epoch 179 lr: 0.0031078826033397846
2022-03-05 01:58:03 - train: epoch 0179, iter [00100, 05004], lr: 0.003108, loss: 1.4766
2022-03-05 01:58:38 - train: epoch 0179, iter [00200, 05004], lr: 0.003108, loss: 1.6493
2022-03-05 01:59:11 - train: epoch 0179, iter [00300, 05004], lr: 0.003108, loss: 1.4440
2022-03-05 01:59:44 - train: epoch 0179, iter [00400, 05004], lr: 0.003108, loss: 1.6051
2022-03-05 02:00:18 - train: epoch 0179, iter [00500, 05004], lr: 0.003108, loss: 1.7822
2022-03-05 02:00:52 - train: epoch 0179, iter [00600, 05004], lr: 0.003108, loss: 1.4848
2022-03-05 02:01:26 - train: epoch 0179, iter [00700, 05004], lr: 0.003108, loss: 1.4034
2022-03-05 02:01:59 - train: epoch 0179, iter [00800, 05004], lr: 0.003108, loss: 1.6305
2022-03-05 02:02:33 - train: epoch 0179, iter [00900, 05004], lr: 0.003108, loss: 1.4762
2022-03-05 02:03:06 - train: epoch 0179, iter [01000, 05004], lr: 0.003108, loss: 1.4979
2022-03-05 02:03:40 - train: epoch 0179, iter [01100, 05004], lr: 0.003108, loss: 1.6502
2022-03-05 02:04:14 - train: epoch 0179, iter [01200, 05004], lr: 0.003108, loss: 1.5427
2022-03-05 02:04:48 - train: epoch 0179, iter [01300, 05004], lr: 0.003108, loss: 1.5320
2022-03-05 02:05:22 - train: epoch 0179, iter [01400, 05004], lr: 0.003108, loss: 1.5015
2022-03-05 02:05:55 - train: epoch 0179, iter [01500, 05004], lr: 0.003108, loss: 1.4974
2022-03-05 02:06:29 - train: epoch 0179, iter [01600, 05004], lr: 0.003108, loss: 1.3677
2022-03-05 02:07:01 - train: epoch 0179, iter [01700, 05004], lr: 0.003108, loss: 1.4232
2022-03-05 02:07:36 - train: epoch 0179, iter [01800, 05004], lr: 0.003108, loss: 1.6264
2022-03-05 02:08:09 - train: epoch 0179, iter [01900, 05004], lr: 0.003108, loss: 1.4801
2022-03-05 02:08:44 - train: epoch 0179, iter [02000, 05004], lr: 0.003108, loss: 1.5790
2022-03-05 02:09:17 - train: epoch 0179, iter [02100, 05004], lr: 0.003108, loss: 1.6174
2022-03-05 02:09:50 - train: epoch 0179, iter [02200, 05004], lr: 0.003108, loss: 1.6220
2022-03-05 02:10:24 - train: epoch 0179, iter [02300, 05004], lr: 0.003108, loss: 1.5860
2022-03-05 02:10:58 - train: epoch 0179, iter [02400, 05004], lr: 0.003108, loss: 1.5050
2022-03-05 02:11:31 - train: epoch 0179, iter [02500, 05004], lr: 0.003108, loss: 1.9696
2022-03-05 02:12:05 - train: epoch 0179, iter [02600, 05004], lr: 0.003108, loss: 1.5472
2022-03-05 02:12:39 - train: epoch 0179, iter [02700, 05004], lr: 0.003108, loss: 1.7379
2022-03-05 02:13:13 - train: epoch 0179, iter [02800, 05004], lr: 0.003108, loss: 1.8107
2022-03-05 02:13:46 - train: epoch 0179, iter [02900, 05004], lr: 0.003108, loss: 1.5362
2022-03-05 02:14:21 - train: epoch 0179, iter [03000, 05004], lr: 0.003108, loss: 1.4586
2022-03-05 02:14:54 - train: epoch 0179, iter [03100, 05004], lr: 0.003108, loss: 1.1173
2022-03-05 02:15:28 - train: epoch 0179, iter [03200, 05004], lr: 0.003108, loss: 1.6739
2022-03-05 02:16:02 - train: epoch 0179, iter [03300, 05004], lr: 0.003108, loss: 1.5946
2022-03-05 02:16:36 - train: epoch 0179, iter [03400, 05004], lr: 0.003108, loss: 1.3275
2022-03-05 02:17:09 - train: epoch 0179, iter [03500, 05004], lr: 0.003108, loss: 1.6606
2022-03-05 02:17:43 - train: epoch 0179, iter [03600, 05004], lr: 0.003108, loss: 2.0651
2022-03-05 02:18:17 - train: epoch 0179, iter [03700, 05004], lr: 0.003108, loss: 1.6368
2022-03-05 02:18:52 - train: epoch 0179, iter [03800, 05004], lr: 0.003108, loss: 1.3909
2022-03-05 02:19:26 - train: epoch 0179, iter [03900, 05004], lr: 0.003108, loss: 1.6046
2022-03-05 02:20:00 - train: epoch 0179, iter [04000, 05004], lr: 0.003108, loss: 1.5211
2022-03-05 02:20:33 - train: epoch 0179, iter [04100, 05004], lr: 0.003108, loss: 1.6527
2022-03-05 02:21:07 - train: epoch 0179, iter [04200, 05004], lr: 0.003108, loss: 1.5490
2022-03-05 02:21:41 - train: epoch 0179, iter [04300, 05004], lr: 0.003108, loss: 1.8051
2022-03-05 02:22:16 - train: epoch 0179, iter [04400, 05004], lr: 0.003108, loss: 1.9765
2022-03-05 02:22:49 - train: epoch 0179, iter [04500, 05004], lr: 0.003108, loss: 1.5770
2022-03-05 02:23:24 - train: epoch 0179, iter [04600, 05004], lr: 0.003108, loss: 1.7785
2022-03-05 02:23:57 - train: epoch 0179, iter [04700, 05004], lr: 0.003108, loss: 1.5616
2022-03-05 02:24:31 - train: epoch 0179, iter [04800, 05004], lr: 0.003108, loss: 1.4852
2022-03-05 02:25:04 - train: epoch 0179, iter [04900, 05004], lr: 0.003108, loss: 1.7220
2022-03-05 02:25:37 - train: epoch 0179, iter [05000, 05004], lr: 0.003108, loss: 1.7274
2022-03-05 02:25:38 - train: epoch 179, train_loss: 1.5608
2022-03-05 02:26:52 - eval: epoch: 179, acc1: 75.506%, acc5: 92.678%, test_loss: 0.9752, per_image_load_time: 1.108ms, per_image_inference_time: 0.503ms
2022-03-05 02:26:53 - until epoch: 179, best_acc1: 75.506%
2022-03-05 02:26:53 - epoch 180 lr: 0.0028344093371128424
2022-03-05 02:27:32 - train: epoch 0180, iter [00100, 05004], lr: 0.002834, loss: 1.4503
2022-03-05 02:28:07 - train: epoch 0180, iter [00200, 05004], lr: 0.002834, loss: 1.4765
2022-03-05 02:28:40 - train: epoch 0180, iter [00300, 05004], lr: 0.002834, loss: 1.4288
2022-03-05 02:29:15 - train: epoch 0180, iter [00400, 05004], lr: 0.002834, loss: 1.4256
2022-03-05 02:29:48 - train: epoch 0180, iter [00500, 05004], lr: 0.002834, loss: 1.5438
2022-03-05 02:30:21 - train: epoch 0180, iter [00600, 05004], lr: 0.002834, loss: 1.5755
2022-03-05 02:30:55 - train: epoch 0180, iter [00700, 05004], lr: 0.002834, loss: 1.4988
2022-03-05 02:31:29 - train: epoch 0180, iter [00800, 05004], lr: 0.002834, loss: 1.4552
2022-03-05 02:32:03 - train: epoch 0180, iter [00900, 05004], lr: 0.002834, loss: 1.6745
2022-03-05 02:32:36 - train: epoch 0180, iter [01000, 05004], lr: 0.002834, loss: 1.6584
2022-03-05 02:33:10 - train: epoch 0180, iter [01100, 05004], lr: 0.002834, loss: 1.5174
2022-03-05 02:33:44 - train: epoch 0180, iter [01200, 05004], lr: 0.002834, loss: 1.4106
2022-03-05 02:34:18 - train: epoch 0180, iter [01300, 05004], lr: 0.002834, loss: 1.6055
2022-03-05 02:34:52 - train: epoch 0180, iter [01400, 05004], lr: 0.002834, loss: 1.6776
2022-03-05 02:35:26 - train: epoch 0180, iter [01500, 05004], lr: 0.002834, loss: 1.5208
2022-03-05 02:35:59 - train: epoch 0180, iter [01600, 05004], lr: 0.002834, loss: 1.3974
2022-03-05 02:36:32 - train: epoch 0180, iter [01700, 05004], lr: 0.002834, loss: 1.5098
2022-03-05 02:37:05 - train: epoch 0180, iter [01800, 05004], lr: 0.002834, loss: 1.7483
2022-03-05 02:37:40 - train: epoch 0180, iter [01900, 05004], lr: 0.002834, loss: 1.6780
2022-03-05 02:38:14 - train: epoch 0180, iter [02000, 05004], lr: 0.002834, loss: 1.5306
2022-03-05 02:38:47 - train: epoch 0180, iter [02100, 05004], lr: 0.002834, loss: 1.4763
2022-03-05 02:39:21 - train: epoch 0180, iter [02200, 05004], lr: 0.002834, loss: 1.8828
2022-03-05 02:39:54 - train: epoch 0180, iter [02300, 05004], lr: 0.002834, loss: 1.7896
2022-03-05 02:40:28 - train: epoch 0180, iter [02400, 05004], lr: 0.002834, loss: 1.6237
2022-03-05 02:41:02 - train: epoch 0180, iter [02500, 05004], lr: 0.002834, loss: 1.6607
2022-03-05 02:41:35 - train: epoch 0180, iter [02600, 05004], lr: 0.002834, loss: 1.2941
2022-03-05 02:42:09 - train: epoch 0180, iter [02700, 05004], lr: 0.002834, loss: 1.5090
2022-03-05 02:42:42 - train: epoch 0180, iter [02800, 05004], lr: 0.002834, loss: 1.5868
2022-03-05 02:43:17 - train: epoch 0180, iter [02900, 05004], lr: 0.002834, loss: 1.3122
2022-03-05 02:43:50 - train: epoch 0180, iter [03000, 05004], lr: 0.002834, loss: 1.3687
2022-03-05 02:44:23 - train: epoch 0180, iter [03100, 05004], lr: 0.002834, loss: 1.5475
2022-03-05 02:44:57 - train: epoch 0180, iter [03200, 05004], lr: 0.002834, loss: 1.5397
2022-03-05 02:45:30 - train: epoch 0180, iter [03300, 05004], lr: 0.002834, loss: 1.5734
2022-03-05 02:46:05 - train: epoch 0180, iter [03400, 05004], lr: 0.002834, loss: 1.6181
2022-03-05 02:46:38 - train: epoch 0180, iter [03500, 05004], lr: 0.002834, loss: 1.3986
2022-03-05 02:47:11 - train: epoch 0180, iter [03600, 05004], lr: 0.002834, loss: 1.4871
2022-03-05 02:47:45 - train: epoch 0180, iter [03700, 05004], lr: 0.002834, loss: 1.5100
2022-03-05 02:48:18 - train: epoch 0180, iter [03800, 05004], lr: 0.002834, loss: 1.6798
2022-03-05 02:48:52 - train: epoch 0180, iter [03900, 05004], lr: 0.002834, loss: 1.6537
2022-03-05 02:49:26 - train: epoch 0180, iter [04000, 05004], lr: 0.002834, loss: 1.2977
2022-03-05 02:50:00 - train: epoch 0180, iter [04100, 05004], lr: 0.002834, loss: 1.5179
2022-03-05 02:50:32 - train: epoch 0180, iter [04200, 05004], lr: 0.002834, loss: 1.5310
2022-03-05 02:51:06 - train: epoch 0180, iter [04300, 05004], lr: 0.002834, loss: 1.7554
2022-03-05 02:51:39 - train: epoch 0180, iter [04400, 05004], lr: 0.002834, loss: 1.4862
2022-03-05 02:52:13 - train: epoch 0180, iter [04500, 05004], lr: 0.002834, loss: 1.6387
2022-03-05 02:52:46 - train: epoch 0180, iter [04600, 05004], lr: 0.002834, loss: 1.7548
2022-03-05 02:53:19 - train: epoch 0180, iter [04700, 05004], lr: 0.002834, loss: 1.5303
2022-03-05 02:53:53 - train: epoch 0180, iter [04800, 05004], lr: 0.002834, loss: 1.5420
2022-03-05 02:54:26 - train: epoch 0180, iter [04900, 05004], lr: 0.002834, loss: 1.5113
2022-03-05 02:54:59 - train: epoch 0180, iter [05000, 05004], lr: 0.002834, loss: 1.5532
2022-03-05 02:55:00 - train: epoch 180, train_loss: 1.5473
2022-03-05 02:56:14 - eval: epoch: 180, acc1: 75.462%, acc5: 92.678%, test_loss: 0.9735, per_image_load_time: 1.410ms, per_image_inference_time: 0.506ms
2022-03-05 02:56:15 - until epoch: 180, best_acc1: 75.506%
2022-03-05 02:56:15 - epoch 181 lr: 0.002573177902642726
2022-03-05 02:56:55 - train: epoch 0181, iter [00100, 05004], lr: 0.002573, loss: 1.5535
2022-03-05 02:57:28 - train: epoch 0181, iter [00200, 05004], lr: 0.002573, loss: 1.7617
2022-03-05 02:58:02 - train: epoch 0181, iter [00300, 05004], lr: 0.002573, loss: 1.4470
2022-03-05 02:58:36 - train: epoch 0181, iter [00400, 05004], lr: 0.002573, loss: 1.2926
2022-03-05 02:59:09 - train: epoch 0181, iter [00500, 05004], lr: 0.002573, loss: 1.6005
2022-03-05 02:59:43 - train: epoch 0181, iter [00600, 05004], lr: 0.002573, loss: 1.4240
2022-03-05 03:00:17 - train: epoch 0181, iter [00700, 05004], lr: 0.002573, loss: 1.2987
2022-03-05 03:00:50 - train: epoch 0181, iter [00800, 05004], lr: 0.002573, loss: 1.4902
2022-03-05 03:01:25 - train: epoch 0181, iter [00900, 05004], lr: 0.002573, loss: 1.2159
2022-03-05 03:01:58 - train: epoch 0181, iter [01000, 05004], lr: 0.002573, loss: 1.4344
2022-03-05 03:02:33 - train: epoch 0181, iter [01100, 05004], lr: 0.002573, loss: 1.5287
2022-03-05 03:03:06 - train: epoch 0181, iter [01200, 05004], lr: 0.002573, loss: 1.4846
2022-03-05 03:03:40 - train: epoch 0181, iter [01300, 05004], lr: 0.002573, loss: 1.7281
2022-03-05 03:04:14 - train: epoch 0181, iter [01400, 05004], lr: 0.002573, loss: 1.6107
2022-03-05 03:04:48 - train: epoch 0181, iter [01500, 05004], lr: 0.002573, loss: 1.5885
2022-03-05 03:05:21 - train: epoch 0181, iter [01600, 05004], lr: 0.002573, loss: 1.5647
2022-03-05 03:05:55 - train: epoch 0181, iter [01700, 05004], lr: 0.002573, loss: 1.6769
2022-03-05 03:06:30 - train: epoch 0181, iter [01800, 05004], lr: 0.002573, loss: 1.5818
2022-03-05 03:07:04 - train: epoch 0181, iter [01900, 05004], lr: 0.002573, loss: 1.3183
2022-03-05 03:07:37 - train: epoch 0181, iter [02000, 05004], lr: 0.002573, loss: 1.5601
2022-03-05 03:08:11 - train: epoch 0181, iter [02100, 05004], lr: 0.002573, loss: 1.5943
2022-03-05 03:08:45 - train: epoch 0181, iter [02200, 05004], lr: 0.002573, loss: 1.7392
2022-03-05 03:09:19 - train: epoch 0181, iter [02300, 05004], lr: 0.002573, loss: 1.6091
2022-03-05 03:09:53 - train: epoch 0181, iter [02400, 05004], lr: 0.002573, loss: 1.4575
2022-03-05 03:10:27 - train: epoch 0181, iter [02500, 05004], lr: 0.002573, loss: 1.3467
2022-03-05 03:11:01 - train: epoch 0181, iter [02600, 05004], lr: 0.002573, loss: 1.4771
2022-03-05 03:11:34 - train: epoch 0181, iter [02700, 05004], lr: 0.002573, loss: 1.5913
2022-03-05 03:12:09 - train: epoch 0181, iter [02800, 05004], lr: 0.002573, loss: 1.3717
2022-03-05 03:12:42 - train: epoch 0181, iter [02900, 05004], lr: 0.002573, loss: 1.6047
2022-03-05 03:13:16 - train: epoch 0181, iter [03000, 05004], lr: 0.002573, loss: 1.6338
2022-03-05 03:13:50 - train: epoch 0181, iter [03100, 05004], lr: 0.002573, loss: 1.5635
2022-03-05 03:14:23 - train: epoch 0181, iter [03200, 05004], lr: 0.002573, loss: 1.6789
2022-03-05 03:14:57 - train: epoch 0181, iter [03300, 05004], lr: 0.002573, loss: 1.6508
2022-03-05 03:15:31 - train: epoch 0181, iter [03400, 05004], lr: 0.002573, loss: 1.8899
2022-03-05 03:16:05 - train: epoch 0181, iter [03500, 05004], lr: 0.002573, loss: 1.6601
2022-03-05 03:16:38 - train: epoch 0181, iter [03600, 05004], lr: 0.002573, loss: 1.6827
2022-03-05 03:17:12 - train: epoch 0181, iter [03700, 05004], lr: 0.002573, loss: 1.9046
2022-03-05 03:17:46 - train: epoch 0181, iter [03800, 05004], lr: 0.002573, loss: 1.5163
2022-03-05 03:18:19 - train: epoch 0181, iter [03900, 05004], lr: 0.002573, loss: 1.6293
2022-03-05 03:18:53 - train: epoch 0181, iter [04000, 05004], lr: 0.002573, loss: 1.4108
2022-03-05 03:19:27 - train: epoch 0181, iter [04100, 05004], lr: 0.002573, loss: 1.2648
2022-03-05 03:20:01 - train: epoch 0181, iter [04200, 05004], lr: 0.002573, loss: 1.3415
2022-03-05 03:20:36 - train: epoch 0181, iter [04300, 05004], lr: 0.002573, loss: 1.5003
2022-03-05 03:21:09 - train: epoch 0181, iter [04400, 05004], lr: 0.002573, loss: 1.2761
2022-03-05 03:21:42 - train: epoch 0181, iter [04500, 05004], lr: 0.002573, loss: 1.7929
2022-03-05 03:22:16 - train: epoch 0181, iter [04600, 05004], lr: 0.002573, loss: 1.6033
2022-03-05 03:22:50 - train: epoch 0181, iter [04700, 05004], lr: 0.002573, loss: 1.5881
2022-03-05 03:23:24 - train: epoch 0181, iter [04800, 05004], lr: 0.002573, loss: 1.5304
2022-03-05 03:23:58 - train: epoch 0181, iter [04900, 05004], lr: 0.002573, loss: 1.4354
2022-03-05 03:24:30 - train: epoch 0181, iter [05000, 05004], lr: 0.002573, loss: 1.3591
2022-03-05 03:24:31 - train: epoch 181, train_loss: 1.5341
2022-03-05 03:25:45 - eval: epoch: 181, acc1: 75.786%, acc5: 92.924%, test_loss: 0.9561, per_image_load_time: 1.012ms, per_image_inference_time: 0.495ms
2022-03-05 03:25:46 - until epoch: 181, best_acc1: 75.786%
2022-03-05 03:25:46 - epoch 182 lr: 0.002324256102563188
2022-03-05 03:26:25 - train: epoch 0182, iter [00100, 05004], lr: 0.002324, loss: 1.4293
2022-03-05 03:26:58 - train: epoch 0182, iter [00200, 05004], lr: 0.002324, loss: 1.2350
2022-03-05 03:27:33 - train: epoch 0182, iter [00300, 05004], lr: 0.002324, loss: 1.7283
2022-03-05 03:28:06 - train: epoch 0182, iter [00400, 05004], lr: 0.002324, loss: 1.6055
2022-03-05 03:28:40 - train: epoch 0182, iter [00500, 05004], lr: 0.002324, loss: 1.3826
2022-03-05 03:29:13 - train: epoch 0182, iter [00600, 05004], lr: 0.002324, loss: 1.3021
2022-03-05 03:29:47 - train: epoch 0182, iter [00700, 05004], lr: 0.002324, loss: 1.5990
2022-03-05 03:30:21 - train: epoch 0182, iter [00800, 05004], lr: 0.002324, loss: 1.5562
2022-03-05 03:30:55 - train: epoch 0182, iter [00900, 05004], lr: 0.002324, loss: 1.6508
2022-03-05 03:31:28 - train: epoch 0182, iter [01000, 05004], lr: 0.002324, loss: 1.7812
2022-03-05 03:32:02 - train: epoch 0182, iter [01100, 05004], lr: 0.002324, loss: 1.4488
2022-03-05 03:32:36 - train: epoch 0182, iter [01200, 05004], lr: 0.002324, loss: 1.6887
2022-03-05 03:33:10 - train: epoch 0182, iter [01300, 05004], lr: 0.002324, loss: 1.4293
2022-03-05 03:33:44 - train: epoch 0182, iter [01400, 05004], lr: 0.002324, loss: 1.6618
2022-03-05 03:34:18 - train: epoch 0182, iter [01500, 05004], lr: 0.002324, loss: 1.5778
2022-03-05 03:34:52 - train: epoch 0182, iter [01600, 05004], lr: 0.002324, loss: 1.7161
2022-03-05 03:35:25 - train: epoch 0182, iter [01700, 05004], lr: 0.002324, loss: 1.6465
2022-03-05 03:35:58 - train: epoch 0182, iter [01800, 05004], lr: 0.002324, loss: 1.3604
2022-03-05 03:36:33 - train: epoch 0182, iter [01900, 05004], lr: 0.002324, loss: 1.4892
2022-03-05 03:37:06 - train: epoch 0182, iter [02000, 05004], lr: 0.002324, loss: 1.4512
2022-03-05 03:37:39 - train: epoch 0182, iter [02100, 05004], lr: 0.002324, loss: 1.6264
2022-03-05 03:38:14 - train: epoch 0182, iter [02200, 05004], lr: 0.002324, loss: 1.3800
2022-03-05 03:38:47 - train: epoch 0182, iter [02300, 05004], lr: 0.002324, loss: 1.4089
2022-03-05 03:39:21 - train: epoch 0182, iter [02400, 05004], lr: 0.002324, loss: 1.5242
2022-03-05 03:39:55 - train: epoch 0182, iter [02500, 05004], lr: 0.002324, loss: 1.3738
2022-03-05 03:40:28 - train: epoch 0182, iter [02600, 05004], lr: 0.002324, loss: 1.3268
2022-03-05 03:41:01 - train: epoch 0182, iter [02700, 05004], lr: 0.002324, loss: 1.1755
2022-03-05 03:41:36 - train: epoch 0182, iter [02800, 05004], lr: 0.002324, loss: 1.5014
2022-03-05 03:42:09 - train: epoch 0182, iter [02900, 05004], lr: 0.002324, loss: 1.3558
2022-03-05 03:42:43 - train: epoch 0182, iter [03000, 05004], lr: 0.002324, loss: 1.4598
2022-03-05 03:43:16 - train: epoch 0182, iter [03100, 05004], lr: 0.002324, loss: 1.6193
2022-03-05 03:43:50 - train: epoch 0182, iter [03200, 05004], lr: 0.002324, loss: 1.4007
2022-03-05 03:44:23 - train: epoch 0182, iter [03300, 05004], lr: 0.002324, loss: 1.3893
2022-03-05 03:44:57 - train: epoch 0182, iter [03400, 05004], lr: 0.002324, loss: 1.2613
2022-03-05 03:45:31 - train: epoch 0182, iter [03500, 05004], lr: 0.002324, loss: 1.3529
2022-03-05 03:46:06 - train: epoch 0182, iter [03600, 05004], lr: 0.002324, loss: 1.4510
2022-03-05 03:46:39 - train: epoch 0182, iter [03700, 05004], lr: 0.002324, loss: 1.5555
2022-03-05 03:47:13 - train: epoch 0182, iter [03800, 05004], lr: 0.002324, loss: 1.6229
2022-03-05 03:47:46 - train: epoch 0182, iter [03900, 05004], lr: 0.002324, loss: 1.6798
2022-03-05 03:48:20 - train: epoch 0182, iter [04000, 05004], lr: 0.002324, loss: 1.5377
2022-03-05 03:48:55 - train: epoch 0182, iter [04100, 05004], lr: 0.002324, loss: 1.6906
2022-03-05 03:49:29 - train: epoch 0182, iter [04200, 05004], lr: 0.002324, loss: 1.5686
2022-03-05 03:50:02 - train: epoch 0182, iter [04300, 05004], lr: 0.002324, loss: 1.7471
2022-03-05 03:50:37 - train: epoch 0182, iter [04400, 05004], lr: 0.002324, loss: 1.5499
2022-03-05 03:51:10 - train: epoch 0182, iter [04500, 05004], lr: 0.002324, loss: 1.5765
2022-03-05 03:51:44 - train: epoch 0182, iter [04600, 05004], lr: 0.002324, loss: 1.6181
2022-03-05 03:52:18 - train: epoch 0182, iter [04700, 05004], lr: 0.002324, loss: 1.6047
2022-03-05 03:52:51 - train: epoch 0182, iter [04800, 05004], lr: 0.002324, loss: 1.3703
2022-03-05 03:53:25 - train: epoch 0182, iter [04900, 05004], lr: 0.002324, loss: 1.3491
2022-03-05 03:53:57 - train: epoch 0182, iter [05000, 05004], lr: 0.002324, loss: 1.3682
2022-03-05 03:53:58 - train: epoch 182, train_loss: 1.5191
2022-03-05 03:55:11 - eval: epoch: 182, acc1: 75.630%, acc5: 92.678%, test_loss: 0.9663, per_image_load_time: 1.167ms, per_image_inference_time: 0.486ms
2022-03-05 03:55:12 - until epoch: 182, best_acc1: 75.786%
2022-03-05 03:55:12 - epoch 183 lr: 0.002087708544541689
2022-03-05 03:55:51 - train: epoch 0183, iter [00100, 05004], lr: 0.002088, loss: 1.6611
2022-03-05 03:56:25 - train: epoch 0183, iter [00200, 05004], lr: 0.002088, loss: 1.4050
2022-03-05 03:56:59 - train: epoch 0183, iter [00300, 05004], lr: 0.002088, loss: 1.2233
2022-03-05 03:57:33 - train: epoch 0183, iter [00400, 05004], lr: 0.002088, loss: 1.3963
2022-03-05 03:58:07 - train: epoch 0183, iter [00500, 05004], lr: 0.002088, loss: 1.5687
2022-03-05 03:58:40 - train: epoch 0183, iter [00600, 05004], lr: 0.002088, loss: 1.3635
2022-03-05 03:59:14 - train: epoch 0183, iter [00700, 05004], lr: 0.002088, loss: 1.7864
2022-03-05 03:59:49 - train: epoch 0183, iter [00800, 05004], lr: 0.002088, loss: 1.6748
2022-03-05 04:00:22 - train: epoch 0183, iter [00900, 05004], lr: 0.002088, loss: 1.5089
2022-03-05 04:00:56 - train: epoch 0183, iter [01000, 05004], lr: 0.002088, loss: 1.4838
2022-03-05 04:01:30 - train: epoch 0183, iter [01100, 05004], lr: 0.002088, loss: 1.8143
2022-03-05 04:02:04 - train: epoch 0183, iter [01200, 05004], lr: 0.002088, loss: 1.5246
2022-03-05 04:02:38 - train: epoch 0183, iter [01300, 05004], lr: 0.002088, loss: 1.7647
2022-03-05 04:03:11 - train: epoch 0183, iter [01400, 05004], lr: 0.002088, loss: 1.4790
2022-03-05 04:03:46 - train: epoch 0183, iter [01500, 05004], lr: 0.002088, loss: 1.4957
2022-03-05 04:04:19 - train: epoch 0183, iter [01600, 05004], lr: 0.002088, loss: 1.5553
2022-03-05 04:04:53 - train: epoch 0183, iter [01700, 05004], lr: 0.002088, loss: 1.4920
2022-03-05 04:05:26 - train: epoch 0183, iter [01800, 05004], lr: 0.002088, loss: 1.3384
2022-03-05 04:06:00 - train: epoch 0183, iter [01900, 05004], lr: 0.002088, loss: 1.4357
2022-03-05 04:06:34 - train: epoch 0183, iter [02000, 05004], lr: 0.002088, loss: 1.4298
2022-03-05 04:07:08 - train: epoch 0183, iter [02100, 05004], lr: 0.002088, loss: 1.3921
2022-03-05 04:07:41 - train: epoch 0183, iter [02200, 05004], lr: 0.002088, loss: 1.6671
2022-03-05 04:08:15 - train: epoch 0183, iter [02300, 05004], lr: 0.002088, loss: 1.3955
2022-03-05 04:08:49 - train: epoch 0183, iter [02400, 05004], lr: 0.002088, loss: 1.7051
2022-03-05 04:09:24 - train: epoch 0183, iter [02500, 05004], lr: 0.002088, loss: 1.2918
2022-03-05 04:09:57 - train: epoch 0183, iter [02600, 05004], lr: 0.002088, loss: 1.4937
2022-03-05 04:10:32 - train: epoch 0183, iter [02700, 05004], lr: 0.002088, loss: 1.4917
2022-03-05 04:11:05 - train: epoch 0183, iter [02800, 05004], lr: 0.002088, loss: 1.4625
2022-03-05 04:11:39 - train: epoch 0183, iter [02900, 05004], lr: 0.002088, loss: 1.7642
2022-03-05 04:12:13 - train: epoch 0183, iter [03000, 05004], lr: 0.002088, loss: 1.5996
2022-03-05 04:12:46 - train: epoch 0183, iter [03100, 05004], lr: 0.002088, loss: 1.2622
2022-03-05 04:13:21 - train: epoch 0183, iter [03200, 05004], lr: 0.002088, loss: 1.4820
2022-03-05 04:13:55 - train: epoch 0183, iter [03300, 05004], lr: 0.002088, loss: 1.5016
2022-03-05 04:14:29 - train: epoch 0183, iter [03400, 05004], lr: 0.002088, loss: 1.3083
2022-03-05 04:15:01 - train: epoch 0183, iter [03500, 05004], lr: 0.002088, loss: 1.2914
2022-03-05 04:15:35 - train: epoch 0183, iter [03600, 05004], lr: 0.002088, loss: 1.5229
2022-03-05 04:16:10 - train: epoch 0183, iter [03700, 05004], lr: 0.002088, loss: 1.5325
2022-03-05 04:16:43 - train: epoch 0183, iter [03800, 05004], lr: 0.002088, loss: 1.3941
2022-03-05 04:17:17 - train: epoch 0183, iter [03900, 05004], lr: 0.002088, loss: 1.8276
2022-03-05 04:17:51 - train: epoch 0183, iter [04000, 05004], lr: 0.002088, loss: 1.4922
2022-03-05 04:18:24 - train: epoch 0183, iter [04100, 05004], lr: 0.002088, loss: 1.3012
2022-03-05 04:18:58 - train: epoch 0183, iter [04200, 05004], lr: 0.002088, loss: 1.4235
2022-03-05 04:19:32 - train: epoch 0183, iter [04300, 05004], lr: 0.002088, loss: 1.5949
2022-03-05 04:20:05 - train: epoch 0183, iter [04400, 05004], lr: 0.002088, loss: 1.4410
2022-03-05 04:20:39 - train: epoch 0183, iter [04500, 05004], lr: 0.002088, loss: 1.3653
2022-03-05 04:21:13 - train: epoch 0183, iter [04600, 05004], lr: 0.002088, loss: 1.5894
2022-03-05 04:21:46 - train: epoch 0183, iter [04700, 05004], lr: 0.002088, loss: 1.5313
2022-03-05 04:22:20 - train: epoch 0183, iter [04800, 05004], lr: 0.002088, loss: 1.2922
2022-03-05 04:22:54 - train: epoch 0183, iter [04900, 05004], lr: 0.002088, loss: 1.3484
2022-03-05 04:23:26 - train: epoch 0183, iter [05000, 05004], lr: 0.002088, loss: 1.6368
2022-03-05 04:23:27 - train: epoch 183, train_loss: 1.4999
2022-03-05 04:24:42 - eval: epoch: 183, acc1: 75.724%, acc5: 92.866%, test_loss: 0.9595, per_image_load_time: 1.318ms, per_image_inference_time: 0.500ms
2022-03-05 04:24:42 - until epoch: 183, best_acc1: 75.786%
2022-03-05 04:24:42 - epoch 184 lr: 0.0018635966245104663
2022-03-05 04:25:21 - train: epoch 0184, iter [00100, 05004], lr: 0.001864, loss: 1.2410
2022-03-05 04:25:55 - train: epoch 0184, iter [00200, 05004], lr: 0.001864, loss: 1.2370
2022-03-05 04:26:30 - train: epoch 0184, iter [00300, 05004], lr: 0.001864, loss: 1.4102
2022-03-05 04:27:03 - train: epoch 0184, iter [00400, 05004], lr: 0.001864, loss: 1.3761
2022-03-05 04:27:37 - train: epoch 0184, iter [00500, 05004], lr: 0.001864, loss: 1.3389
2022-03-05 04:28:10 - train: epoch 0184, iter [00600, 05004], lr: 0.001864, loss: 1.3392
2022-03-05 04:28:44 - train: epoch 0184, iter [00700, 05004], lr: 0.001864, loss: 1.4159
2022-03-05 04:29:18 - train: epoch 0184, iter [00800, 05004], lr: 0.001864, loss: 1.5385
2022-03-05 04:29:52 - train: epoch 0184, iter [00900, 05004], lr: 0.001864, loss: 1.2586
2022-03-05 04:30:26 - train: epoch 0184, iter [01000, 05004], lr: 0.001864, loss: 1.2333
2022-03-05 04:31:00 - train: epoch 0184, iter [01100, 05004], lr: 0.001864, loss: 1.5603
2022-03-05 04:31:33 - train: epoch 0184, iter [01200, 05004], lr: 0.001864, loss: 1.3879
2022-03-05 04:32:07 - train: epoch 0184, iter [01300, 05004], lr: 0.001864, loss: 1.4190
2022-03-05 04:32:40 - train: epoch 0184, iter [01400, 05004], lr: 0.001864, loss: 1.2498
2022-03-05 04:33:14 - train: epoch 0184, iter [01500, 05004], lr: 0.001864, loss: 1.4354
2022-03-05 04:33:48 - train: epoch 0184, iter [01600, 05004], lr: 0.001864, loss: 1.5037
2022-03-05 04:34:23 - train: epoch 0184, iter [01700, 05004], lr: 0.001864, loss: 1.5144
2022-03-05 04:34:56 - train: epoch 0184, iter [01800, 05004], lr: 0.001864, loss: 1.6482
2022-03-05 04:35:29 - train: epoch 0184, iter [01900, 05004], lr: 0.001864, loss: 1.3110
2022-03-05 04:36:04 - train: epoch 0184, iter [02000, 05004], lr: 0.001864, loss: 1.6107
2022-03-05 04:36:37 - train: epoch 0184, iter [02100, 05004], lr: 0.001864, loss: 1.4523
2022-03-05 04:37:11 - train: epoch 0184, iter [02200, 05004], lr: 0.001864, loss: 1.5237
2022-03-05 04:37:45 - train: epoch 0184, iter [02300, 05004], lr: 0.001864, loss: 1.8207
2022-03-05 04:38:18 - train: epoch 0184, iter [02400, 05004], lr: 0.001864, loss: 1.5965
2022-03-05 04:38:52 - train: epoch 0184, iter [02500, 05004], lr: 0.001864, loss: 1.7815
2022-03-05 04:39:26 - train: epoch 0184, iter [02600, 05004], lr: 0.001864, loss: 1.4705
2022-03-05 04:40:00 - train: epoch 0184, iter [02700, 05004], lr: 0.001864, loss: 1.3903
2022-03-05 04:40:34 - train: epoch 0184, iter [02800, 05004], lr: 0.001864, loss: 1.3869
2022-03-05 04:41:08 - train: epoch 0184, iter [02900, 05004], lr: 0.001864, loss: 1.4430
2022-03-05 04:41:41 - train: epoch 0184, iter [03000, 05004], lr: 0.001864, loss: 1.4544
2022-03-05 04:42:15 - train: epoch 0184, iter [03100, 05004], lr: 0.001864, loss: 1.3545
2022-03-05 04:42:49 - train: epoch 0184, iter [03200, 05004], lr: 0.001864, loss: 1.4661
2022-03-05 04:43:23 - train: epoch 0184, iter [03300, 05004], lr: 0.001864, loss: 1.4257
2022-03-05 04:43:57 - train: epoch 0184, iter [03400, 05004], lr: 0.001864, loss: 1.5189
2022-03-05 04:44:31 - train: epoch 0184, iter [03500, 05004], lr: 0.001864, loss: 1.3542
2022-03-05 04:45:04 - train: epoch 0184, iter [03600, 05004], lr: 0.001864, loss: 1.3665
2022-03-05 04:45:37 - train: epoch 0184, iter [03700, 05004], lr: 0.001864, loss: 1.4236
2022-03-05 04:46:12 - train: epoch 0184, iter [03800, 05004], lr: 0.001864, loss: 1.4262
2022-03-05 04:46:45 - train: epoch 0184, iter [03900, 05004], lr: 0.001864, loss: 1.7255
2022-03-05 04:47:19 - train: epoch 0184, iter [04000, 05004], lr: 0.001864, loss: 1.6150
2022-03-05 04:47:53 - train: epoch 0184, iter [04100, 05004], lr: 0.001864, loss: 1.4778
2022-03-05 04:48:27 - train: epoch 0184, iter [04200, 05004], lr: 0.001864, loss: 1.2635
2022-03-05 04:49:00 - train: epoch 0184, iter [04300, 05004], lr: 0.001864, loss: 1.5333
2022-03-05 04:49:34 - train: epoch 0184, iter [04400, 05004], lr: 0.001864, loss: 1.4534
2022-03-05 04:50:08 - train: epoch 0184, iter [04500, 05004], lr: 0.001864, loss: 1.5737
2022-03-05 04:50:43 - train: epoch 0184, iter [04600, 05004], lr: 0.001864, loss: 1.2470
2022-03-05 04:51:16 - train: epoch 0184, iter [04700, 05004], lr: 0.001864, loss: 1.5568
2022-03-05 04:51:49 - train: epoch 0184, iter [04800, 05004], lr: 0.001864, loss: 1.3154
2022-03-05 04:52:23 - train: epoch 0184, iter [04900, 05004], lr: 0.001864, loss: 1.5372
2022-03-05 04:52:56 - train: epoch 0184, iter [05000, 05004], lr: 0.001864, loss: 1.4303
2022-03-05 04:52:57 - train: epoch 184, train_loss: 1.4831
2022-03-05 04:54:11 - eval: epoch: 184, acc1: 76.000%, acc5: 93.036%, test_loss: 0.9460, per_image_load_time: 0.864ms, per_image_inference_time: 0.502ms
2022-03-05 04:54:12 - until epoch: 184, best_acc1: 76.000%
2022-03-05 04:54:12 - epoch 185 lr: 0.0016519785107311892
2022-03-05 04:54:51 - train: epoch 0185, iter [00100, 05004], lr: 0.001652, loss: 1.5156
2022-03-05 04:55:25 - train: epoch 0185, iter [00200, 05004], lr: 0.001652, loss: 1.5855
2022-03-05 04:55:58 - train: epoch 0185, iter [00300, 05004], lr: 0.001652, loss: 1.3411
2022-03-05 04:56:31 - train: epoch 0185, iter [00400, 05004], lr: 0.001652, loss: 1.3472
2022-03-05 04:57:05 - train: epoch 0185, iter [00500, 05004], lr: 0.001652, loss: 1.6462
2022-03-05 04:57:39 - train: epoch 0185, iter [00600, 05004], lr: 0.001652, loss: 1.6201
2022-03-05 04:58:13 - train: epoch 0185, iter [00700, 05004], lr: 0.001652, loss: 1.5802
2022-03-05 04:58:46 - train: epoch 0185, iter [00800, 05004], lr: 0.001652, loss: 1.4323
2022-03-05 04:59:19 - train: epoch 0185, iter [00900, 05004], lr: 0.001652, loss: 1.3212
2022-03-05 04:59:53 - train: epoch 0185, iter [01000, 05004], lr: 0.001652, loss: 1.5781
2022-03-05 05:00:26 - train: epoch 0185, iter [01100, 05004], lr: 0.001652, loss: 1.4931
2022-03-05 05:00:59 - train: epoch 0185, iter [01200, 05004], lr: 0.001652, loss: 1.5199
2022-03-05 05:01:34 - train: epoch 0185, iter [01300, 05004], lr: 0.001652, loss: 1.3947
2022-03-05 05:02:07 - train: epoch 0185, iter [01400, 05004], lr: 0.001652, loss: 1.3768
2022-03-05 05:02:41 - train: epoch 0185, iter [01500, 05004], lr: 0.001652, loss: 1.3063
2022-03-05 05:03:14 - train: epoch 0185, iter [01600, 05004], lr: 0.001652, loss: 1.4139
2022-03-05 05:03:47 - train: epoch 0185, iter [01700, 05004], lr: 0.001652, loss: 1.4079
2022-03-05 05:04:21 - train: epoch 0185, iter [01800, 05004], lr: 0.001652, loss: 1.4238
2022-03-05 05:04:55 - train: epoch 0185, iter [01900, 05004], lr: 0.001652, loss: 1.5656
2022-03-05 05:05:29 - train: epoch 0185, iter [02000, 05004], lr: 0.001652, loss: 1.2173
2022-03-05 05:06:01 - train: epoch 0185, iter [02100, 05004], lr: 0.001652, loss: 1.3875
2022-03-05 05:06:35 - train: epoch 0185, iter [02200, 05004], lr: 0.001652, loss: 1.4223
2022-03-05 05:07:08 - train: epoch 0185, iter [02300, 05004], lr: 0.001652, loss: 1.2883
2022-03-05 05:07:42 - train: epoch 0185, iter [02400, 05004], lr: 0.001652, loss: 1.5045
2022-03-05 05:08:15 - train: epoch 0185, iter [02500, 05004], lr: 0.001652, loss: 1.3614
2022-03-05 05:08:48 - train: epoch 0185, iter [02600, 05004], lr: 0.001652, loss: 1.3662
2022-03-05 05:09:22 - train: epoch 0185, iter [02700, 05004], lr: 0.001652, loss: 1.7239
2022-03-05 05:09:56 - train: epoch 0185, iter [02800, 05004], lr: 0.001652, loss: 1.6253
2022-03-05 05:10:29 - train: epoch 0185, iter [02900, 05004], lr: 0.001652, loss: 1.3897
2022-03-05 05:11:02 - train: epoch 0185, iter [03000, 05004], lr: 0.001652, loss: 1.2334
2022-03-05 05:11:36 - train: epoch 0185, iter [03100, 05004], lr: 0.001652, loss: 1.4978
2022-03-05 05:12:09 - train: epoch 0185, iter [03200, 05004], lr: 0.001652, loss: 1.4288
2022-03-05 05:12:43 - train: epoch 0185, iter [03300, 05004], lr: 0.001652, loss: 1.5312
2022-03-05 05:13:16 - train: epoch 0185, iter [03400, 05004], lr: 0.001652, loss: 1.4061
2022-03-05 05:13:50 - train: epoch 0185, iter [03500, 05004], lr: 0.001652, loss: 1.3993
2022-03-05 05:14:24 - train: epoch 0185, iter [03600, 05004], lr: 0.001652, loss: 1.4820
2022-03-05 05:14:58 - train: epoch 0185, iter [03700, 05004], lr: 0.001652, loss: 1.5442
2022-03-05 05:15:31 - train: epoch 0185, iter [03800, 05004], lr: 0.001652, loss: 1.4906
2022-03-05 05:16:04 - train: epoch 0185, iter [03900, 05004], lr: 0.001652, loss: 1.4396
2022-03-05 05:16:38 - train: epoch 0185, iter [04000, 05004], lr: 0.001652, loss: 1.5465
2022-03-05 05:17:11 - train: epoch 0185, iter [04100, 05004], lr: 0.001652, loss: 1.4698
2022-03-05 05:17:45 - train: epoch 0185, iter [04200, 05004], lr: 0.001652, loss: 1.3826
2022-03-05 05:18:19 - train: epoch 0185, iter [04300, 05004], lr: 0.001652, loss: 1.4554
2022-03-05 05:18:53 - train: epoch 0185, iter [04400, 05004], lr: 0.001652, loss: 1.4729
2022-03-05 05:19:26 - train: epoch 0185, iter [04500, 05004], lr: 0.001652, loss: 1.4359
2022-03-05 05:20:00 - train: epoch 0185, iter [04600, 05004], lr: 0.001652, loss: 1.4001
2022-03-05 05:20:34 - train: epoch 0185, iter [04700, 05004], lr: 0.001652, loss: 1.3858
2022-03-05 05:21:08 - train: epoch 0185, iter [04800, 05004], lr: 0.001652, loss: 1.6146
2022-03-05 05:21:42 - train: epoch 0185, iter [04900, 05004], lr: 0.001652, loss: 1.4961
2022-03-05 05:22:14 - train: epoch 0185, iter [05000, 05004], lr: 0.001652, loss: 1.5015
2022-03-05 05:22:15 - train: epoch 185, train_loss: 1.4690
2022-03-05 05:23:30 - eval: epoch: 185, acc1: 76.584%, acc5: 93.090%, test_loss: 0.9306, per_image_load_time: 0.880ms, per_image_inference_time: 0.445ms
2022-03-05 05:23:30 - until epoch: 185, best_acc1: 76.584%
2022-03-05 05:23:30 - epoch 186 lr: 0.0014529091286973996
2022-03-05 05:24:09 - train: epoch 0186, iter [00100, 05004], lr: 0.001453, loss: 1.5379
2022-03-05 05:24:43 - train: epoch 0186, iter [00200, 05004], lr: 0.001453, loss: 1.5682
2022-03-05 05:25:17 - train: epoch 0186, iter [00300, 05004], lr: 0.001453, loss: 1.5362
2022-03-05 05:25:50 - train: epoch 0186, iter [00400, 05004], lr: 0.001453, loss: 1.3876
2022-03-05 05:26:24 - train: epoch 0186, iter [00500, 05004], lr: 0.001453, loss: 1.5423
2022-03-05 05:26:57 - train: epoch 0186, iter [00600, 05004], lr: 0.001453, loss: 1.4482
2022-03-05 05:27:31 - train: epoch 0186, iter [00700, 05004], lr: 0.001453, loss: 1.5343
2022-03-05 05:28:05 - train: epoch 0186, iter [00800, 05004], lr: 0.001453, loss: 1.3127
2022-03-05 05:28:38 - train: epoch 0186, iter [00900, 05004], lr: 0.001453, loss: 1.5150
2022-03-05 05:29:11 - train: epoch 0186, iter [01000, 05004], lr: 0.001453, loss: 1.4262
2022-03-05 05:29:45 - train: epoch 0186, iter [01100, 05004], lr: 0.001453, loss: 1.6605
2022-03-05 05:30:19 - train: epoch 0186, iter [01200, 05004], lr: 0.001453, loss: 1.5332
2022-03-05 05:30:53 - train: epoch 0186, iter [01300, 05004], lr: 0.001453, loss: 1.2439
2022-03-05 05:31:26 - train: epoch 0186, iter [01400, 05004], lr: 0.001453, loss: 1.7453
2022-03-05 05:32:00 - train: epoch 0186, iter [01500, 05004], lr: 0.001453, loss: 1.4640
2022-03-05 05:32:33 - train: epoch 0186, iter [01600, 05004], lr: 0.001453, loss: 1.4202
2022-03-05 05:33:07 - train: epoch 0186, iter [01700, 05004], lr: 0.001453, loss: 1.4330
2022-03-05 05:33:41 - train: epoch 0186, iter [01800, 05004], lr: 0.001453, loss: 1.4651
2022-03-05 05:34:15 - train: epoch 0186, iter [01900, 05004], lr: 0.001453, loss: 1.5856
2022-03-05 05:34:49 - train: epoch 0186, iter [02000, 05004], lr: 0.001453, loss: 1.3945
2022-03-05 05:35:23 - train: epoch 0186, iter [02100, 05004], lr: 0.001453, loss: 1.7650
2022-03-05 05:35:56 - train: epoch 0186, iter [02200, 05004], lr: 0.001453, loss: 1.4166
2022-03-05 05:36:30 - train: epoch 0186, iter [02300, 05004], lr: 0.001453, loss: 1.3959
2022-03-05 05:37:04 - train: epoch 0186, iter [02400, 05004], lr: 0.001453, loss: 1.4700
2022-03-05 05:37:38 - train: epoch 0186, iter [02500, 05004], lr: 0.001453, loss: 1.5202
2022-03-05 05:38:12 - train: epoch 0186, iter [02600, 05004], lr: 0.001453, loss: 1.3599
2022-03-05 05:38:45 - train: epoch 0186, iter [02700, 05004], lr: 0.001453, loss: 1.5103
2022-03-05 05:39:19 - train: epoch 0186, iter [02800, 05004], lr: 0.001453, loss: 1.4273
2022-03-05 05:39:52 - train: epoch 0186, iter [02900, 05004], lr: 0.001453, loss: 1.4595
2022-03-05 05:40:26 - train: epoch 0186, iter [03000, 05004], lr: 0.001453, loss: 1.5657
2022-03-05 05:40:59 - train: epoch 0186, iter [03100, 05004], lr: 0.001453, loss: 1.3864
2022-03-05 05:41:33 - train: epoch 0186, iter [03200, 05004], lr: 0.001453, loss: 1.2057
2022-03-05 05:42:08 - train: epoch 0186, iter [03300, 05004], lr: 0.001453, loss: 1.7195
2022-03-05 05:42:41 - train: epoch 0186, iter [03400, 05004], lr: 0.001453, loss: 1.3873
2022-03-05 05:43:14 - train: epoch 0186, iter [03500, 05004], lr: 0.001453, loss: 1.5679
2022-03-05 05:43:48 - train: epoch 0186, iter [03600, 05004], lr: 0.001453, loss: 1.4762
2022-03-05 05:44:22 - train: epoch 0186, iter [03700, 05004], lr: 0.001453, loss: 1.5019
2022-03-05 05:44:55 - train: epoch 0186, iter [03800, 05004], lr: 0.001453, loss: 1.4319
2022-03-05 05:45:29 - train: epoch 0186, iter [03900, 05004], lr: 0.001453, loss: 1.7027
2022-03-05 05:46:02 - train: epoch 0186, iter [04000, 05004], lr: 0.001453, loss: 1.1728
2022-03-05 05:46:36 - train: epoch 0186, iter [04100, 05004], lr: 0.001453, loss: 1.3703
2022-03-05 05:47:10 - train: epoch 0186, iter [04200, 05004], lr: 0.001453, loss: 1.3491
2022-03-05 05:47:43 - train: epoch 0186, iter [04300, 05004], lr: 0.001453, loss: 1.5988
2022-03-05 05:48:17 - train: epoch 0186, iter [04400, 05004], lr: 0.001453, loss: 1.4225
2022-03-05 05:48:50 - train: epoch 0186, iter [04500, 05004], lr: 0.001453, loss: 1.5386
2022-03-05 05:49:23 - train: epoch 0186, iter [04600, 05004], lr: 0.001453, loss: 1.4962
2022-03-05 05:49:57 - train: epoch 0186, iter [04700, 05004], lr: 0.001453, loss: 1.4626
2022-03-05 05:50:31 - train: epoch 0186, iter [04800, 05004], lr: 0.001453, loss: 1.3815
2022-03-05 05:51:05 - train: epoch 0186, iter [04900, 05004], lr: 0.001453, loss: 1.1322
2022-03-05 05:51:37 - train: epoch 0186, iter [05000, 05004], lr: 0.001453, loss: 1.7384
2022-03-05 05:51:38 - train: epoch 186, train_loss: 1.4533
2022-03-05 05:52:52 - eval: epoch: 186, acc1: 76.790%, acc5: 93.198%, test_loss: 0.9206, per_image_load_time: 0.557ms, per_image_inference_time: 0.475ms
2022-03-05 05:52:53 - until epoch: 186, best_acc1: 76.790%
2022-03-05 05:52:53 - epoch 187 lr: 0.0012664401468786115
2022-03-05 05:53:32 - train: epoch 0187, iter [00100, 05004], lr: 0.001266, loss: 1.8340
2022-03-05 05:54:06 - train: epoch 0187, iter [00200, 05004], lr: 0.001266, loss: 1.4630
2022-03-05 05:54:40 - train: epoch 0187, iter [00300, 05004], lr: 0.001266, loss: 1.5514
2022-03-05 05:55:14 - train: epoch 0187, iter [00400, 05004], lr: 0.001266, loss: 1.2228
2022-03-05 05:55:48 - train: epoch 0187, iter [00500, 05004], lr: 0.001266, loss: 1.4750
2022-03-05 05:56:20 - train: epoch 0187, iter [00600, 05004], lr: 0.001266, loss: 1.7620
2022-03-05 05:56:55 - train: epoch 0187, iter [00700, 05004], lr: 0.001266, loss: 1.7329
2022-03-05 05:57:29 - train: epoch 0187, iter [00800, 05004], lr: 0.001266, loss: 1.7984
2022-03-05 05:58:02 - train: epoch 0187, iter [00900, 05004], lr: 0.001266, loss: 1.5421
2022-03-05 05:58:36 - train: epoch 0187, iter [01000, 05004], lr: 0.001266, loss: 1.2390
2022-03-05 05:59:10 - train: epoch 0187, iter [01100, 05004], lr: 0.001266, loss: 1.3230
2022-03-05 05:59:43 - train: epoch 0187, iter [01200, 05004], lr: 0.001266, loss: 1.3074
2022-03-05 06:00:17 - train: epoch 0187, iter [01300, 05004], lr: 0.001266, loss: 1.4928
2022-03-05 06:00:51 - train: epoch 0187, iter [01400, 05004], lr: 0.001266, loss: 1.2540
2022-03-05 06:01:25 - train: epoch 0187, iter [01500, 05004], lr: 0.001266, loss: 1.6002
2022-03-05 06:01:59 - train: epoch 0187, iter [01600, 05004], lr: 0.001266, loss: 1.3865
2022-03-05 06:02:32 - train: epoch 0187, iter [01700, 05004], lr: 0.001266, loss: 1.3549
2022-03-05 06:03:05 - train: epoch 0187, iter [01800, 05004], lr: 0.001266, loss: 1.6000
2022-03-05 06:03:39 - train: epoch 0187, iter [01900, 05004], lr: 0.001266, loss: 1.4642
2022-03-05 06:04:14 - train: epoch 0187, iter [02000, 05004], lr: 0.001266, loss: 1.6055
2022-03-05 06:04:46 - train: epoch 0187, iter [02100, 05004], lr: 0.001266, loss: 1.7126
2022-03-05 06:05:21 - train: epoch 0187, iter [02200, 05004], lr: 0.001266, loss: 1.4651
2022-03-05 06:05:54 - train: epoch 0187, iter [02300, 05004], lr: 0.001266, loss: 1.4564
2022-03-05 06:06:27 - train: epoch 0187, iter [02400, 05004], lr: 0.001266, loss: 1.5031
2022-03-05 06:07:01 - train: epoch 0187, iter [02500, 05004], lr: 0.001266, loss: 1.4303
2022-03-05 06:07:34 - train: epoch 0187, iter [02600, 05004], lr: 0.001266, loss: 1.2964
2022-03-05 06:08:09 - train: epoch 0187, iter [02700, 05004], lr: 0.001266, loss: 1.3976
2022-03-05 06:08:42 - train: epoch 0187, iter [02800, 05004], lr: 0.001266, loss: 1.5978
2022-03-05 06:09:16 - train: epoch 0187, iter [02900, 05004], lr: 0.001266, loss: 1.2072
2022-03-05 06:09:50 - train: epoch 0187, iter [03000, 05004], lr: 0.001266, loss: 1.5968
2022-03-05 06:10:22 - train: epoch 0187, iter [03100, 05004], lr: 0.001266, loss: 1.3269
2022-03-05 06:10:57 - train: epoch 0187, iter [03200, 05004], lr: 0.001266, loss: 1.5100
2022-03-05 06:11:31 - train: epoch 0187, iter [03300, 05004], lr: 0.001266, loss: 1.4065
2022-03-05 06:12:05 - train: epoch 0187, iter [03400, 05004], lr: 0.001266, loss: 1.4887
2022-03-05 06:12:39 - train: epoch 0187, iter [03500, 05004], lr: 0.001266, loss: 1.4432
2022-03-05 06:13:12 - train: epoch 0187, iter [03600, 05004], lr: 0.001266, loss: 1.4778
2022-03-05 06:13:46 - train: epoch 0187, iter [03700, 05004], lr: 0.001266, loss: 1.3123
2022-03-05 06:14:19 - train: epoch 0187, iter [03800, 05004], lr: 0.001266, loss: 1.6043
2022-03-05 06:14:53 - train: epoch 0187, iter [03900, 05004], lr: 0.001266, loss: 1.4140
2022-03-05 06:15:26 - train: epoch 0187, iter [04000, 05004], lr: 0.001266, loss: 1.7214
2022-03-05 06:16:00 - train: epoch 0187, iter [04100, 05004], lr: 0.001266, loss: 1.7781
2022-03-05 06:16:34 - train: epoch 0187, iter [04200, 05004], lr: 0.001266, loss: 1.6508
2022-03-05 06:17:07 - train: epoch 0187, iter [04300, 05004], lr: 0.001266, loss: 1.3942
2022-03-05 06:17:41 - train: epoch 0187, iter [04400, 05004], lr: 0.001266, loss: 1.4280
2022-03-05 06:18:15 - train: epoch 0187, iter [04500, 05004], lr: 0.001266, loss: 1.3057
2022-03-05 06:18:49 - train: epoch 0187, iter [04600, 05004], lr: 0.001266, loss: 1.3056
2022-03-05 06:19:22 - train: epoch 0187, iter [04700, 05004], lr: 0.001266, loss: 1.3584
2022-03-05 06:19:56 - train: epoch 0187, iter [04800, 05004], lr: 0.001266, loss: 1.2177
2022-03-05 06:20:29 - train: epoch 0187, iter [04900, 05004], lr: 0.001266, loss: 1.2857
2022-03-05 06:21:02 - train: epoch 0187, iter [05000, 05004], lr: 0.001266, loss: 1.1177
2022-03-05 06:21:03 - train: epoch 187, train_loss: 1.4370
2022-03-05 06:22:17 - eval: epoch: 187, acc1: 76.672%, acc5: 93.298%, test_loss: 0.9191, per_image_load_time: 0.555ms, per_image_inference_time: 0.461ms
2022-03-05 06:22:18 - until epoch: 187, best_acc1: 76.790%
2022-03-05 06:22:18 - epoch 188 lr: 0.0010926199633097156
2022-03-05 06:22:56 - train: epoch 0188, iter [00100, 05004], lr: 0.001093, loss: 1.4067
2022-03-05 06:23:30 - train: epoch 0188, iter [00200, 05004], lr: 0.001093, loss: 1.4601
2022-03-05 06:24:04 - train: epoch 0188, iter [00300, 05004], lr: 0.001093, loss: 1.3123
2022-03-05 06:24:37 - train: epoch 0188, iter [00400, 05004], lr: 0.001093, loss: 1.6730
2022-03-05 06:25:11 - train: epoch 0188, iter [00500, 05004], lr: 0.001093, loss: 1.1582
2022-03-05 06:25:44 - train: epoch 0188, iter [00600, 05004], lr: 0.001093, loss: 1.3935
2022-03-05 06:26:18 - train: epoch 0188, iter [00700, 05004], lr: 0.001093, loss: 1.4264
2022-03-05 06:26:52 - train: epoch 0188, iter [00800, 05004], lr: 0.001093, loss: 1.4073
2022-03-05 06:27:25 - train: epoch 0188, iter [00900, 05004], lr: 0.001093, loss: 1.3718
2022-03-05 06:28:00 - train: epoch 0188, iter [01000, 05004], lr: 0.001093, loss: 1.6292
2022-03-05 06:28:33 - train: epoch 0188, iter [01100, 05004], lr: 0.001093, loss: 1.2682
2022-03-05 06:29:07 - train: epoch 0188, iter [01200, 05004], lr: 0.001093, loss: 1.4941
2022-03-05 06:29:40 - train: epoch 0188, iter [01300, 05004], lr: 0.001093, loss: 1.6084
2022-03-05 06:30:14 - train: epoch 0188, iter [01400, 05004], lr: 0.001093, loss: 1.6106
2022-03-05 06:30:48 - train: epoch 0188, iter [01500, 05004], lr: 0.001093, loss: 1.4544
2022-03-05 06:31:22 - train: epoch 0188, iter [01600, 05004], lr: 0.001093, loss: 1.3759
2022-03-05 06:31:56 - train: epoch 0188, iter [01700, 05004], lr: 0.001093, loss: 1.3666
2022-03-05 06:32:29 - train: epoch 0188, iter [01800, 05004], lr: 0.001093, loss: 1.2731
2022-03-05 06:33:03 - train: epoch 0188, iter [01900, 05004], lr: 0.001093, loss: 1.3280
2022-03-05 06:33:36 - train: epoch 0188, iter [02000, 05004], lr: 0.001093, loss: 1.1705
2022-03-05 06:34:10 - train: epoch 0188, iter [02100, 05004], lr: 0.001093, loss: 1.2086
2022-03-05 06:34:43 - train: epoch 0188, iter [02200, 05004], lr: 0.001093, loss: 1.3356
2022-03-05 06:35:18 - train: epoch 0188, iter [02300, 05004], lr: 0.001093, loss: 1.6426
2022-03-05 06:35:52 - train: epoch 0188, iter [02400, 05004], lr: 0.001093, loss: 1.2671
2022-03-05 06:36:25 - train: epoch 0188, iter [02500, 05004], lr: 0.001093, loss: 1.6598
2022-03-05 06:36:59 - train: epoch 0188, iter [02600, 05004], lr: 0.001093, loss: 1.5168
2022-03-05 06:37:32 - train: epoch 0188, iter [02700, 05004], lr: 0.001093, loss: 1.6142
2022-03-05 06:38:06 - train: epoch 0188, iter [02800, 05004], lr: 0.001093, loss: 1.5153
2022-03-05 06:38:40 - train: epoch 0188, iter [02900, 05004], lr: 0.001093, loss: 1.6077
2022-03-05 06:39:13 - train: epoch 0188, iter [03000, 05004], lr: 0.001093, loss: 1.2916
2022-03-05 06:39:47 - train: epoch 0188, iter [03100, 05004], lr: 0.001093, loss: 1.4758
2022-03-05 06:40:21 - train: epoch 0188, iter [03200, 05004], lr: 0.001093, loss: 1.4761
2022-03-05 06:40:54 - train: epoch 0188, iter [03300, 05004], lr: 0.001093, loss: 1.2935
2022-03-05 06:41:28 - train: epoch 0188, iter [03400, 05004], lr: 0.001093, loss: 1.4518
2022-03-05 06:42:02 - train: epoch 0188, iter [03500, 05004], lr: 0.001093, loss: 1.2415
2022-03-05 06:42:35 - train: epoch 0188, iter [03600, 05004], lr: 0.001093, loss: 1.2911
2022-03-05 06:43:08 - train: epoch 0188, iter [03700, 05004], lr: 0.001093, loss: 1.3670
2022-03-05 06:43:43 - train: epoch 0188, iter [03800, 05004], lr: 0.001093, loss: 1.6294
2022-03-05 06:44:16 - train: epoch 0188, iter [03900, 05004], lr: 0.001093, loss: 1.3608
2022-03-05 06:44:50 - train: epoch 0188, iter [04000, 05004], lr: 0.001093, loss: 1.3612
2022-03-05 06:45:23 - train: epoch 0188, iter [04100, 05004], lr: 0.001093, loss: 1.5542
2022-03-05 06:45:57 - train: epoch 0188, iter [04200, 05004], lr: 0.001093, loss: 1.2561
2022-03-05 06:46:31 - train: epoch 0188, iter [04300, 05004], lr: 0.001093, loss: 1.4266
2022-03-05 06:47:04 - train: epoch 0188, iter [04400, 05004], lr: 0.001093, loss: 1.5636
2022-03-05 06:47:38 - train: epoch 0188, iter [04500, 05004], lr: 0.001093, loss: 1.5570
2022-03-05 06:48:12 - train: epoch 0188, iter [04600, 05004], lr: 0.001093, loss: 1.3688
2022-03-05 06:48:45 - train: epoch 0188, iter [04700, 05004], lr: 0.001093, loss: 1.2655
2022-03-05 06:49:19 - train: epoch 0188, iter [04800, 05004], lr: 0.001093, loss: 1.5620
2022-03-05 06:49:52 - train: epoch 0188, iter [04900, 05004], lr: 0.001093, loss: 1.3511
2022-03-05 06:50:24 - train: epoch 0188, iter [05000, 05004], lr: 0.001093, loss: 1.5808
2022-03-05 06:50:25 - train: epoch 188, train_loss: 1.4201
2022-03-05 06:51:40 - eval: epoch: 188, acc1: 76.902%, acc5: 93.300%, test_loss: 0.9145, per_image_load_time: 0.531ms, per_image_inference_time: 0.449ms
2022-03-05 06:51:41 - until epoch: 188, best_acc1: 76.902%
2022-03-05 06:51:41 - epoch 189 lr: 0.0009314936930293283
2022-03-05 06:52:20 - train: epoch 0189, iter [00100, 05004], lr: 0.000931, loss: 1.3976
2022-03-05 06:52:54 - train: epoch 0189, iter [00200, 05004], lr: 0.000931, loss: 1.4128
2022-03-05 06:53:28 - train: epoch 0189, iter [00300, 05004], lr: 0.000931, loss: 1.3788
2022-03-05 06:54:01 - train: epoch 0189, iter [00400, 05004], lr: 0.000931, loss: 1.4814
2022-03-05 06:54:35 - train: epoch 0189, iter [00500, 05004], lr: 0.000931, loss: 1.1382
2022-03-05 06:55:08 - train: epoch 0189, iter [00600, 05004], lr: 0.000931, loss: 1.6736
2022-03-05 06:55:42 - train: epoch 0189, iter [00700, 05004], lr: 0.000931, loss: 1.6024
2022-03-05 06:56:16 - train: epoch 0189, iter [00800, 05004], lr: 0.000931, loss: 1.4935
2022-03-05 06:56:50 - train: epoch 0189, iter [00900, 05004], lr: 0.000931, loss: 1.3109
2022-03-05 06:57:23 - train: epoch 0189, iter [01000, 05004], lr: 0.000931, loss: 1.1894
2022-03-05 06:57:56 - train: epoch 0189, iter [01100, 05004], lr: 0.000931, loss: 1.2579
2022-03-05 06:58:31 - train: epoch 0189, iter [01200, 05004], lr: 0.000931, loss: 1.5442
2022-03-05 06:59:04 - train: epoch 0189, iter [01300, 05004], lr: 0.000931, loss: 1.2337
2022-03-05 06:59:38 - train: epoch 0189, iter [01400, 05004], lr: 0.000931, loss: 1.2253
2022-03-05 07:00:11 - train: epoch 0189, iter [01500, 05004], lr: 0.000931, loss: 1.2863
2022-03-05 07:00:44 - train: epoch 0189, iter [01600, 05004], lr: 0.000931, loss: 1.3169
2022-03-05 07:01:19 - train: epoch 0189, iter [01700, 05004], lr: 0.000931, loss: 1.6372
2022-03-05 07:01:51 - train: epoch 0189, iter [01800, 05004], lr: 0.000931, loss: 1.6595
2022-03-05 07:02:25 - train: epoch 0189, iter [01900, 05004], lr: 0.000931, loss: 1.5746
2022-03-05 07:02:58 - train: epoch 0189, iter [02000, 05004], lr: 0.000931, loss: 1.2510
2022-03-05 07:03:32 - train: epoch 0189, iter [02100, 05004], lr: 0.000931, loss: 1.5973
2022-03-05 07:04:05 - train: epoch 0189, iter [02200, 05004], lr: 0.000931, loss: 1.4942
2022-03-05 07:04:39 - train: epoch 0189, iter [02300, 05004], lr: 0.000931, loss: 1.5556
2022-03-05 07:05:12 - train: epoch 0189, iter [02400, 05004], lr: 0.000931, loss: 1.4159
2022-03-05 07:05:46 - train: epoch 0189, iter [02500, 05004], lr: 0.000931, loss: 1.3106
2022-03-05 07:06:20 - train: epoch 0189, iter [02600, 05004], lr: 0.000931, loss: 1.5653
2022-03-05 07:06:54 - train: epoch 0189, iter [02700, 05004], lr: 0.000931, loss: 1.2115
2022-03-05 07:07:27 - train: epoch 0189, iter [02800, 05004], lr: 0.000931, loss: 1.3328
2022-03-05 07:08:01 - train: epoch 0189, iter [02900, 05004], lr: 0.000931, loss: 1.4231
2022-03-05 07:08:34 - train: epoch 0189, iter [03000, 05004], lr: 0.000931, loss: 1.3775
2022-03-05 07:09:09 - train: epoch 0189, iter [03100, 05004], lr: 0.000931, loss: 1.4518
2022-03-05 07:09:43 - train: epoch 0189, iter [03200, 05004], lr: 0.000931, loss: 1.5465
2022-03-05 07:10:16 - train: epoch 0189, iter [03300, 05004], lr: 0.000931, loss: 1.4275
2022-03-05 07:10:50 - train: epoch 0189, iter [03400, 05004], lr: 0.000931, loss: 1.3893
2022-03-05 07:11:24 - train: epoch 0189, iter [03500, 05004], lr: 0.000931, loss: 1.3975
2022-03-05 07:11:58 - train: epoch 0189, iter [03600, 05004], lr: 0.000931, loss: 1.3989
2022-03-05 07:12:32 - train: epoch 0189, iter [03700, 05004], lr: 0.000931, loss: 1.1749
2022-03-05 07:13:06 - train: epoch 0189, iter [03800, 05004], lr: 0.000931, loss: 1.3253
2022-03-05 07:13:39 - train: epoch 0189, iter [03900, 05004], lr: 0.000931, loss: 1.4789
2022-03-05 07:14:13 - train: epoch 0189, iter [04000, 05004], lr: 0.000931, loss: 1.4215
2022-03-05 07:14:46 - train: epoch 0189, iter [04100, 05004], lr: 0.000931, loss: 1.2595
2022-03-05 07:15:20 - train: epoch 0189, iter [04200, 05004], lr: 0.000931, loss: 1.1902
2022-03-05 07:15:54 - train: epoch 0189, iter [04300, 05004], lr: 0.000931, loss: 1.4119
2022-03-05 07:16:27 - train: epoch 0189, iter [04400, 05004], lr: 0.000931, loss: 1.4752
2022-03-05 07:17:01 - train: epoch 0189, iter [04500, 05004], lr: 0.000931, loss: 1.4076
2022-03-05 07:17:34 - train: epoch 0189, iter [04600, 05004], lr: 0.000931, loss: 1.2854
2022-03-05 07:18:09 - train: epoch 0189, iter [04700, 05004], lr: 0.000931, loss: 1.4623
2022-03-05 07:18:42 - train: epoch 0189, iter [04800, 05004], lr: 0.000931, loss: 1.6368
2022-03-05 07:19:16 - train: epoch 0189, iter [04900, 05004], lr: 0.000931, loss: 1.5843
2022-03-05 07:19:49 - train: epoch 0189, iter [05000, 05004], lr: 0.000931, loss: 1.4789
2022-03-05 07:19:50 - train: epoch 189, train_loss: 1.4130
2022-03-05 07:21:03 - eval: epoch: 189, acc1: 76.888%, acc5: 93.404%, test_loss: 0.9090, per_image_load_time: 0.649ms, per_image_inference_time: 0.455ms
2022-03-05 07:21:04 - until epoch: 189, best_acc1: 76.902%
2022-03-05 07:21:04 - epoch 190 lr: 0.000783103156370113
2022-03-05 07:21:43 - train: epoch 0190, iter [00100, 05004], lr: 0.000783, loss: 1.5512
2022-03-05 07:22:17 - train: epoch 0190, iter [00200, 05004], lr: 0.000783, loss: 1.2259
2022-03-05 07:22:51 - train: epoch 0190, iter [00300, 05004], lr: 0.000783, loss: 1.2854
2022-03-05 07:23:24 - train: epoch 0190, iter [00400, 05004], lr: 0.000783, loss: 1.5513
2022-03-05 07:23:58 - train: epoch 0190, iter [00500, 05004], lr: 0.000783, loss: 1.4812
2022-03-05 07:24:32 - train: epoch 0190, iter [00600, 05004], lr: 0.000783, loss: 1.3715
2022-03-05 07:25:06 - train: epoch 0190, iter [00700, 05004], lr: 0.000783, loss: 1.4852
2022-03-05 07:25:40 - train: epoch 0190, iter [00800, 05004], lr: 0.000783, loss: 1.1081
2022-03-05 07:26:14 - train: epoch 0190, iter [00900, 05004], lr: 0.000783, loss: 1.2471
2022-03-05 07:26:48 - train: epoch 0190, iter [01000, 05004], lr: 0.000783, loss: 1.3628
2022-03-05 07:27:20 - train: epoch 0190, iter [01100, 05004], lr: 0.000783, loss: 1.6334
2022-03-05 07:27:55 - train: epoch 0190, iter [01200, 05004], lr: 0.000783, loss: 1.0833
2022-03-05 07:28:28 - train: epoch 0190, iter [01300, 05004], lr: 0.000783, loss: 1.6409
2022-03-05 07:29:03 - train: epoch 0190, iter [01400, 05004], lr: 0.000783, loss: 1.5337
2022-03-05 07:29:36 - train: epoch 0190, iter [01500, 05004], lr: 0.000783, loss: 1.3572
2022-03-05 07:30:11 - train: epoch 0190, iter [01600, 05004], lr: 0.000783, loss: 1.6581
2022-03-05 07:30:44 - train: epoch 0190, iter [01700, 05004], lr: 0.000783, loss: 1.3156
2022-03-05 07:31:18 - train: epoch 0190, iter [01800, 05004], lr: 0.000783, loss: 1.3992
2022-03-05 07:31:51 - train: epoch 0190, iter [01900, 05004], lr: 0.000783, loss: 1.5541
2022-03-05 07:32:25 - train: epoch 0190, iter [02000, 05004], lr: 0.000783, loss: 1.5448
2022-03-05 07:32:59 - train: epoch 0190, iter [02100, 05004], lr: 0.000783, loss: 1.3830
2022-03-05 07:33:33 - train: epoch 0190, iter [02200, 05004], lr: 0.000783, loss: 1.6665
2022-03-05 07:34:06 - train: epoch 0190, iter [02300, 05004], lr: 0.000783, loss: 1.4283
2022-03-05 07:34:40 - train: epoch 0190, iter [02400, 05004], lr: 0.000783, loss: 1.2576
2022-03-05 07:35:13 - train: epoch 0190, iter [02500, 05004], lr: 0.000783, loss: 1.4295
2022-03-05 07:35:48 - train: epoch 0190, iter [02600, 05004], lr: 0.000783, loss: 1.2716
2022-03-05 07:36:21 - train: epoch 0190, iter [02700, 05004], lr: 0.000783, loss: 1.3402
2022-03-05 07:36:56 - train: epoch 0190, iter [02800, 05004], lr: 0.000783, loss: 1.3969
2022-03-05 07:37:28 - train: epoch 0190, iter [02900, 05004], lr: 0.000783, loss: 1.3327
2022-03-05 07:38:02 - train: epoch 0190, iter [03000, 05004], lr: 0.000783, loss: 1.3949
2022-03-05 07:38:36 - train: epoch 0190, iter [03100, 05004], lr: 0.000783, loss: 1.2171
2022-03-05 07:39:10 - train: epoch 0190, iter [03200, 05004], lr: 0.000783, loss: 1.5028
2022-03-05 07:39:44 - train: epoch 0190, iter [03300, 05004], lr: 0.000783, loss: 1.5045
2022-03-05 07:40:18 - train: epoch 0190, iter [03400, 05004], lr: 0.000783, loss: 1.4017
2022-03-05 07:40:50 - train: epoch 0190, iter [03500, 05004], lr: 0.000783, loss: 1.3295
2022-03-05 07:41:25 - train: epoch 0190, iter [03600, 05004], lr: 0.000783, loss: 1.5337
2022-03-05 07:41:58 - train: epoch 0190, iter [03700, 05004], lr: 0.000783, loss: 1.6790
2022-03-05 07:42:32 - train: epoch 0190, iter [03800, 05004], lr: 0.000783, loss: 1.4067
2022-03-05 07:43:06 - train: epoch 0190, iter [03900, 05004], lr: 0.000783, loss: 1.2082
2022-03-05 07:43:40 - train: epoch 0190, iter [04000, 05004], lr: 0.000783, loss: 1.5435
2022-03-05 07:44:14 - train: epoch 0190, iter [04100, 05004], lr: 0.000783, loss: 1.4723
2022-03-05 07:44:47 - train: epoch 0190, iter [04200, 05004], lr: 0.000783, loss: 1.1337
2022-03-05 07:45:21 - train: epoch 0190, iter [04300, 05004], lr: 0.000783, loss: 1.3665
2022-03-05 07:45:56 - train: epoch 0190, iter [04400, 05004], lr: 0.000783, loss: 1.4800
2022-03-05 07:46:29 - train: epoch 0190, iter [04500, 05004], lr: 0.000783, loss: 1.3025
2022-03-05 07:47:03 - train: epoch 0190, iter [04600, 05004], lr: 0.000783, loss: 1.4735
2022-03-05 07:47:36 - train: epoch 0190, iter [04700, 05004], lr: 0.000783, loss: 1.4862
2022-03-05 07:48:09 - train: epoch 0190, iter [04800, 05004], lr: 0.000783, loss: 1.6217
2022-03-05 07:48:44 - train: epoch 0190, iter [04900, 05004], lr: 0.000783, loss: 1.5658
2022-03-05 07:49:16 - train: epoch 0190, iter [05000, 05004], lr: 0.000783, loss: 1.3208
2022-03-05 07:49:17 - train: epoch 190, train_loss: 1.4037
2022-03-05 07:50:31 - eval: epoch: 190, acc1: 77.180%, acc5: 93.446%, test_loss: 0.9011, per_image_load_time: 0.724ms, per_image_inference_time: 0.469ms
2022-03-05 07:50:32 - until epoch: 190, best_acc1: 77.180%
2022-03-05 07:50:32 - epoch 191 lr: 0.0006474868681043578
2022-03-05 07:51:11 - train: epoch 0191, iter [00100, 05004], lr: 0.000647, loss: 1.4114
2022-03-05 07:51:45 - train: epoch 0191, iter [00200, 05004], lr: 0.000647, loss: 1.5509
2022-03-05 07:52:19 - train: epoch 0191, iter [00300, 05004], lr: 0.000647, loss: 1.5868
2022-03-05 07:52:52 - train: epoch 0191, iter [00400, 05004], lr: 0.000647, loss: 1.3850
2022-03-05 07:53:26 - train: epoch 0191, iter [00500, 05004], lr: 0.000647, loss: 1.2664
2022-03-05 07:54:00 - train: epoch 0191, iter [00600, 05004], lr: 0.000647, loss: 1.3162
2022-03-05 07:54:33 - train: epoch 0191, iter [00700, 05004], lr: 0.000647, loss: 1.3887
2022-03-05 07:55:07 - train: epoch 0191, iter [00800, 05004], lr: 0.000647, loss: 1.6093
2022-03-05 07:55:40 - train: epoch 0191, iter [00900, 05004], lr: 0.000647, loss: 1.5716
2022-03-05 07:56:15 - train: epoch 0191, iter [01000, 05004], lr: 0.000647, loss: 1.2998
2022-03-05 07:56:49 - train: epoch 0191, iter [01100, 05004], lr: 0.000647, loss: 1.5710
2022-03-05 07:57:23 - train: epoch 0191, iter [01200, 05004], lr: 0.000647, loss: 1.6701
2022-03-05 07:57:56 - train: epoch 0191, iter [01300, 05004], lr: 0.000647, loss: 1.2782
2022-03-05 07:58:30 - train: epoch 0191, iter [01400, 05004], lr: 0.000647, loss: 1.2207
2022-03-05 07:59:03 - train: epoch 0191, iter [01500, 05004], lr: 0.000647, loss: 1.4535
2022-03-05 07:59:37 - train: epoch 0191, iter [01600, 05004], lr: 0.000647, loss: 1.4187
2022-03-05 08:00:10 - train: epoch 0191, iter [01700, 05004], lr: 0.000647, loss: 1.3330
2022-03-05 08:00:44 - train: epoch 0191, iter [01800, 05004], lr: 0.000647, loss: 1.5009
2022-03-05 08:01:17 - train: epoch 0191, iter [01900, 05004], lr: 0.000647, loss: 1.3795
2022-03-05 08:01:51 - train: epoch 0191, iter [02000, 05004], lr: 0.000647, loss: 1.3874
2022-03-05 08:02:24 - train: epoch 0191, iter [02100, 05004], lr: 0.000647, loss: 1.2664
2022-03-05 08:02:58 - train: epoch 0191, iter [02200, 05004], lr: 0.000647, loss: 1.2811
2022-03-05 08:03:32 - train: epoch 0191, iter [02300, 05004], lr: 0.000647, loss: 1.1740
2022-03-05 08:04:05 - train: epoch 0191, iter [02400, 05004], lr: 0.000647, loss: 1.4715
2022-03-05 08:04:39 - train: epoch 0191, iter [02500, 05004], lr: 0.000647, loss: 1.2700
2022-03-05 08:05:13 - train: epoch 0191, iter [02600, 05004], lr: 0.000647, loss: 1.5535
2022-03-05 08:05:47 - train: epoch 0191, iter [02700, 05004], lr: 0.000647, loss: 1.3130
2022-03-05 08:06:21 - train: epoch 0191, iter [02800, 05004], lr: 0.000647, loss: 1.5608
2022-03-05 08:06:54 - train: epoch 0191, iter [02900, 05004], lr: 0.000647, loss: 1.5178
2022-03-05 08:07:28 - train: epoch 0191, iter [03000, 05004], lr: 0.000647, loss: 1.3026
2022-03-05 08:08:01 - train: epoch 0191, iter [03100, 05004], lr: 0.000647, loss: 1.3012
2022-03-05 08:08:35 - train: epoch 0191, iter [03200, 05004], lr: 0.000647, loss: 1.4162
2022-03-05 08:09:09 - train: epoch 0191, iter [03300, 05004], lr: 0.000647, loss: 1.4158
2022-03-05 08:09:43 - train: epoch 0191, iter [03400, 05004], lr: 0.000647, loss: 1.2148
2022-03-05 08:10:16 - train: epoch 0191, iter [03500, 05004], lr: 0.000647, loss: 1.3581
2022-03-05 08:10:51 - train: epoch 0191, iter [03600, 05004], lr: 0.000647, loss: 1.2178
2022-03-05 08:11:23 - train: epoch 0191, iter [03700, 05004], lr: 0.000647, loss: 1.5466
2022-03-05 08:11:57 - train: epoch 0191, iter [03800, 05004], lr: 0.000647, loss: 1.5125
2022-03-05 08:12:31 - train: epoch 0191, iter [03900, 05004], lr: 0.000647, loss: 1.3571
2022-03-05 08:13:05 - train: epoch 0191, iter [04000, 05004], lr: 0.000647, loss: 1.3779
2022-03-05 08:13:39 - train: epoch 0191, iter [04100, 05004], lr: 0.000647, loss: 1.4410
2022-03-05 08:14:13 - train: epoch 0191, iter [04200, 05004], lr: 0.000647, loss: 1.2386
2022-03-05 08:14:45 - train: epoch 0191, iter [04300, 05004], lr: 0.000647, loss: 1.2504
2022-03-05 08:15:20 - train: epoch 0191, iter [04400, 05004], lr: 0.000647, loss: 1.3807
2022-03-05 08:15:53 - train: epoch 0191, iter [04500, 05004], lr: 0.000647, loss: 1.5584
2022-03-05 08:16:27 - train: epoch 0191, iter [04600, 05004], lr: 0.000647, loss: 1.3459
2022-03-05 08:17:00 - train: epoch 0191, iter [04700, 05004], lr: 0.000647, loss: 1.1583
2022-03-05 08:17:35 - train: epoch 0191, iter [04800, 05004], lr: 0.000647, loss: 1.4648
2022-03-05 08:18:07 - train: epoch 0191, iter [04900, 05004], lr: 0.000647, loss: 1.2891
2022-03-05 08:18:40 - train: epoch 0191, iter [05000, 05004], lr: 0.000647, loss: 1.2554
2022-03-05 08:18:41 - train: epoch 191, train_loss: 1.3878
2022-03-05 08:19:56 - eval: epoch: 191, acc1: 77.276%, acc5: 93.470%, test_loss: 0.8981, per_image_load_time: 0.743ms, per_image_inference_time: 0.464ms
2022-03-05 08:19:57 - until epoch: 191, best_acc1: 77.276%
2022-03-05 08:19:57 - epoch 192 lr: 0.000524680027447444
2022-03-05 08:20:35 - train: epoch 0192, iter [00100, 05004], lr: 0.000525, loss: 1.5726
2022-03-05 08:21:10 - train: epoch 0192, iter [00200, 05004], lr: 0.000525, loss: 1.5041
2022-03-05 08:21:43 - train: epoch 0192, iter [00300, 05004], lr: 0.000525, loss: 1.2183
2022-03-05 08:22:17 - train: epoch 0192, iter [00400, 05004], lr: 0.000525, loss: 1.4673
2022-03-05 08:22:51 - train: epoch 0192, iter [00500, 05004], lr: 0.000525, loss: 1.4729
2022-03-05 08:23:25 - train: epoch 0192, iter [00600, 05004], lr: 0.000525, loss: 1.3090
2022-03-05 08:23:58 - train: epoch 0192, iter [00700, 05004], lr: 0.000525, loss: 1.5058
2022-03-05 08:24:33 - train: epoch 0192, iter [00800, 05004], lr: 0.000525, loss: 1.4963
2022-03-05 08:25:05 - train: epoch 0192, iter [00900, 05004], lr: 0.000525, loss: 1.4004
2022-03-05 08:25:40 - train: epoch 0192, iter [01000, 05004], lr: 0.000525, loss: 1.4412
2022-03-05 08:26:14 - train: epoch 0192, iter [01100, 05004], lr: 0.000525, loss: 1.3264
2022-03-05 08:26:49 - train: epoch 0192, iter [01200, 05004], lr: 0.000525, loss: 1.1871
2022-03-05 08:27:22 - train: epoch 0192, iter [01300, 05004], lr: 0.000525, loss: 1.4405
2022-03-05 08:27:56 - train: epoch 0192, iter [01400, 05004], lr: 0.000525, loss: 1.4333
2022-03-05 08:28:31 - train: epoch 0192, iter [01500, 05004], lr: 0.000525, loss: 1.3833
2022-03-05 08:29:05 - train: epoch 0192, iter [01600, 05004], lr: 0.000525, loss: 1.2699
2022-03-05 08:29:39 - train: epoch 0192, iter [01700, 05004], lr: 0.000525, loss: 1.5055
2022-03-05 08:30:13 - train: epoch 0192, iter [01800, 05004], lr: 0.000525, loss: 1.2543
2022-03-05 08:30:48 - train: epoch 0192, iter [01900, 05004], lr: 0.000525, loss: 1.4486
2022-03-05 08:31:22 - train: epoch 0192, iter [02000, 05004], lr: 0.000525, loss: 1.3709
2022-03-05 08:31:55 - train: epoch 0192, iter [02100, 05004], lr: 0.000525, loss: 1.4023
2022-03-05 08:32:29 - train: epoch 0192, iter [02200, 05004], lr: 0.000525, loss: 1.2307
2022-03-05 08:33:02 - train: epoch 0192, iter [02300, 05004], lr: 0.000525, loss: 1.0632
2022-03-05 08:33:36 - train: epoch 0192, iter [02400, 05004], lr: 0.000525, loss: 1.1857
2022-03-05 08:34:10 - train: epoch 0192, iter [02500, 05004], lr: 0.000525, loss: 1.3379
2022-03-05 08:34:43 - train: epoch 0192, iter [02600, 05004], lr: 0.000525, loss: 1.2983
2022-03-05 08:35:17 - train: epoch 0192, iter [02700, 05004], lr: 0.000525, loss: 1.5539
2022-03-05 08:35:50 - train: epoch 0192, iter [02800, 05004], lr: 0.000525, loss: 1.4999
2022-03-05 08:36:25 - train: epoch 0192, iter [02900, 05004], lr: 0.000525, loss: 1.3732
2022-03-05 08:37:01 - train: epoch 0192, iter [03000, 05004], lr: 0.000525, loss: 1.5740
2022-03-05 08:37:35 - train: epoch 0192, iter [03100, 05004], lr: 0.000525, loss: 1.3951
2022-03-05 08:38:08 - train: epoch 0192, iter [03200, 05004], lr: 0.000525, loss: 1.3857
2022-03-05 08:38:46 - train: epoch 0192, iter [03300, 05004], lr: 0.000525, loss: 1.3582
2022-03-05 08:39:20 - train: epoch 0192, iter [03400, 05004], lr: 0.000525, loss: 1.4432
2022-03-05 08:39:54 - train: epoch 0192, iter [03500, 05004], lr: 0.000525, loss: 1.2836
2022-03-05 08:40:29 - train: epoch 0192, iter [03600, 05004], lr: 0.000525, loss: 1.4890
2022-03-05 08:41:04 - train: epoch 0192, iter [03700, 05004], lr: 0.000525, loss: 1.1720
2022-03-05 08:41:40 - train: epoch 0192, iter [03800, 05004], lr: 0.000525, loss: 1.1986
2022-03-05 08:42:14 - train: epoch 0192, iter [03900, 05004], lr: 0.000525, loss: 1.3066
2022-03-05 08:42:47 - train: epoch 0192, iter [04000, 05004], lr: 0.000525, loss: 1.5435
2022-03-05 08:43:21 - train: epoch 0192, iter [04100, 05004], lr: 0.000525, loss: 1.3429
2022-03-05 08:43:55 - train: epoch 0192, iter [04200, 05004], lr: 0.000525, loss: 1.3020
2022-03-05 08:44:28 - train: epoch 0192, iter [04300, 05004], lr: 0.000525, loss: 1.5091
2022-03-05 08:45:01 - train: epoch 0192, iter [04400, 05004], lr: 0.000525, loss: 1.3476
2022-03-05 08:45:35 - train: epoch 0192, iter [04500, 05004], lr: 0.000525, loss: 1.4666
2022-03-05 08:46:07 - train: epoch 0192, iter [04600, 05004], lr: 0.000525, loss: 1.2221
2022-03-05 08:46:41 - train: epoch 0192, iter [04700, 05004], lr: 0.000525, loss: 1.3789
2022-03-05 08:47:14 - train: epoch 0192, iter [04800, 05004], lr: 0.000525, loss: 1.2535
2022-03-05 08:47:48 - train: epoch 0192, iter [04900, 05004], lr: 0.000525, loss: 1.1946
2022-03-05 08:48:20 - train: epoch 0192, iter [05000, 05004], lr: 0.000525, loss: 1.2395
2022-03-05 08:48:21 - train: epoch 192, train_loss: 1.3780
2022-03-05 08:49:34 - eval: epoch: 192, acc1: 77.306%, acc5: 93.562%, test_loss: 0.8952, per_image_load_time: 0.562ms, per_image_inference_time: 0.445ms
2022-03-05 08:49:34 - until epoch: 192, best_acc1: 77.306%
2022-03-05 08:49:34 - epoch 193 lr: 0.00041471450892189844
2022-03-05 08:50:13 - train: epoch 0193, iter [00100, 05004], lr: 0.000415, loss: 1.3383
2022-03-05 08:50:47 - train: epoch 0193, iter [00200, 05004], lr: 0.000415, loss: 1.2831
2022-03-05 08:51:21 - train: epoch 0193, iter [00300, 05004], lr: 0.000415, loss: 1.1894
2022-03-05 08:51:54 - train: epoch 0193, iter [00400, 05004], lr: 0.000415, loss: 1.0809
2022-03-05 08:52:27 - train: epoch 0193, iter [00500, 05004], lr: 0.000415, loss: 1.4203
2022-03-05 08:53:01 - train: epoch 0193, iter [00600, 05004], lr: 0.000415, loss: 1.3254
2022-03-05 08:53:34 - train: epoch 0193, iter [00700, 05004], lr: 0.000415, loss: 1.5228
2022-03-05 08:54:07 - train: epoch 0193, iter [00800, 05004], lr: 0.000415, loss: 1.4216
2022-03-05 08:54:41 - train: epoch 0193, iter [00900, 05004], lr: 0.000415, loss: 1.4436
2022-03-05 08:55:14 - train: epoch 0193, iter [01000, 05004], lr: 0.000415, loss: 1.4577
2022-03-05 08:55:48 - train: epoch 0193, iter [01100, 05004], lr: 0.000415, loss: 1.3153
2022-03-05 08:56:20 - train: epoch 0193, iter [01200, 05004], lr: 0.000415, loss: 1.5232
2022-03-05 08:56:54 - train: epoch 0193, iter [01300, 05004], lr: 0.000415, loss: 1.3564
2022-03-05 08:57:27 - train: epoch 0193, iter [01400, 05004], lr: 0.000415, loss: 1.2430
2022-03-05 08:57:59 - train: epoch 0193, iter [01500, 05004], lr: 0.000415, loss: 1.4660
2022-03-05 08:58:33 - train: epoch 0193, iter [01600, 05004], lr: 0.000415, loss: 1.2247
2022-03-05 08:59:06 - train: epoch 0193, iter [01700, 05004], lr: 0.000415, loss: 1.3623
2022-03-05 08:59:40 - train: epoch 0193, iter [01800, 05004], lr: 0.000415, loss: 1.2219
2022-03-05 09:00:13 - train: epoch 0193, iter [01900, 05004], lr: 0.000415, loss: 1.5628
2022-03-05 09:00:47 - train: epoch 0193, iter [02000, 05004], lr: 0.000415, loss: 1.4167
2022-03-05 09:01:20 - train: epoch 0193, iter [02100, 05004], lr: 0.000415, loss: 1.3137
2022-03-05 09:01:54 - train: epoch 0193, iter [02200, 05004], lr: 0.000415, loss: 1.4833
2022-03-05 09:02:26 - train: epoch 0193, iter [02300, 05004], lr: 0.000415, loss: 1.3771
2022-03-05 09:03:00 - train: epoch 0193, iter [02400, 05004], lr: 0.000415, loss: 1.2513
2022-03-05 09:03:35 - train: epoch 0193, iter [02500, 05004], lr: 0.000415, loss: 1.3854
2022-03-05 09:04:08 - train: epoch 0193, iter [02600, 05004], lr: 0.000415, loss: 1.3915
2022-03-05 09:04:41 - train: epoch 0193, iter [02700, 05004], lr: 0.000415, loss: 1.3085
2022-03-05 09:05:15 - train: epoch 0193, iter [02800, 05004], lr: 0.000415, loss: 1.4359
2022-03-05 09:05:49 - train: epoch 0193, iter [02900, 05004], lr: 0.000415, loss: 1.2603
2022-03-05 09:06:22 - train: epoch 0193, iter [03000, 05004], lr: 0.000415, loss: 1.2625
2022-03-05 09:06:56 - train: epoch 0193, iter [03100, 05004], lr: 0.000415, loss: 1.5023
2022-03-05 09:07:30 - train: epoch 0193, iter [03200, 05004], lr: 0.000415, loss: 1.6296
2022-03-05 09:08:03 - train: epoch 0193, iter [03300, 05004], lr: 0.000415, loss: 1.5694
2022-03-05 09:08:38 - train: epoch 0193, iter [03400, 05004], lr: 0.000415, loss: 1.3139
2022-03-05 09:09:14 - train: epoch 0193, iter [03500, 05004], lr: 0.000415, loss: 1.1353
2022-03-05 09:09:47 - train: epoch 0193, iter [03600, 05004], lr: 0.000415, loss: 1.7233
2022-03-05 09:10:21 - train: epoch 0193, iter [03700, 05004], lr: 0.000415, loss: 1.2732
2022-03-05 09:10:54 - train: epoch 0193, iter [03800, 05004], lr: 0.000415, loss: 1.3023
2022-03-05 09:11:30 - train: epoch 0193, iter [03900, 05004], lr: 0.000415, loss: 1.3062
2022-03-05 09:12:02 - train: epoch 0193, iter [04000, 05004], lr: 0.000415, loss: 1.2080
2022-03-05 09:12:37 - train: epoch 0193, iter [04100, 05004], lr: 0.000415, loss: 1.5735
2022-03-05 09:13:12 - train: epoch 0193, iter [04200, 05004], lr: 0.000415, loss: 1.5088
2022-03-05 09:13:46 - train: epoch 0193, iter [04300, 05004], lr: 0.000415, loss: 1.3264
2022-03-05 09:14:22 - train: epoch 0193, iter [04400, 05004], lr: 0.000415, loss: 1.1836
2022-03-05 09:14:56 - train: epoch 0193, iter [04500, 05004], lr: 0.000415, loss: 1.1755
2022-03-05 09:15:29 - train: epoch 0193, iter [04600, 05004], lr: 0.000415, loss: 1.4437
2022-03-05 09:16:02 - train: epoch 0193, iter [04700, 05004], lr: 0.000415, loss: 1.3266
2022-03-05 09:16:36 - train: epoch 0193, iter [04800, 05004], lr: 0.000415, loss: 1.5745
2022-03-05 09:17:13 - train: epoch 0193, iter [04900, 05004], lr: 0.000415, loss: 1.1921
2022-03-05 09:17:48 - train: epoch 0193, iter [05000, 05004], lr: 0.000415, loss: 1.2111
2022-03-05 09:17:50 - train: epoch 193, train_loss: 1.3687
2022-03-05 09:19:03 - eval: epoch: 193, acc1: 77.478%, acc5: 93.666%, test_loss: 0.8927, per_image_load_time: 0.642ms, per_image_inference_time: 0.470ms
2022-03-05 09:19:04 - until epoch: 193, best_acc1: 77.478%
2022-03-05 09:19:04 - epoch 194 lr: 0.00031761885408435053
2022-03-05 09:19:43 - train: epoch 0194, iter [00100, 05004], lr: 0.000318, loss: 1.4114
2022-03-05 09:20:16 - train: epoch 0194, iter [00200, 05004], lr: 0.000318, loss: 1.2609
2022-03-05 09:20:49 - train: epoch 0194, iter [00300, 05004], lr: 0.000318, loss: 1.2419
2022-03-05 09:21:22 - train: epoch 0194, iter [00400, 05004], lr: 0.000318, loss: 1.5455
2022-03-05 09:21:57 - train: epoch 0194, iter [00500, 05004], lr: 0.000318, loss: 1.5997
2022-03-05 09:22:30 - train: epoch 0194, iter [00600, 05004], lr: 0.000318, loss: 1.2655
2022-03-05 09:23:03 - train: epoch 0194, iter [00700, 05004], lr: 0.000318, loss: 1.3882
2022-03-05 09:23:35 - train: epoch 0194, iter [00800, 05004], lr: 0.000318, loss: 1.3994
2022-03-05 09:24:09 - train: epoch 0194, iter [00900, 05004], lr: 0.000318, loss: 1.3969
2022-03-05 09:24:41 - train: epoch 0194, iter [01000, 05004], lr: 0.000318, loss: 1.1205
2022-03-05 09:25:15 - train: epoch 0194, iter [01100, 05004], lr: 0.000318, loss: 1.3682
2022-03-05 09:25:48 - train: epoch 0194, iter [01200, 05004], lr: 0.000318, loss: 1.4082
2022-03-05 09:26:21 - train: epoch 0194, iter [01300, 05004], lr: 0.000318, loss: 1.3199
2022-03-05 09:26:55 - train: epoch 0194, iter [01400, 05004], lr: 0.000318, loss: 1.3701
2022-03-05 09:27:27 - train: epoch 0194, iter [01500, 05004], lr: 0.000318, loss: 1.4361
2022-03-05 09:28:01 - train: epoch 0194, iter [01600, 05004], lr: 0.000318, loss: 1.5620
2022-03-05 09:28:34 - train: epoch 0194, iter [01700, 05004], lr: 0.000318, loss: 1.2663
2022-03-05 09:29:08 - train: epoch 0194, iter [01800, 05004], lr: 0.000318, loss: 1.2178
2022-03-05 09:29:40 - train: epoch 0194, iter [01900, 05004], lr: 0.000318, loss: 1.4218
2022-03-05 09:30:14 - train: epoch 0194, iter [02000, 05004], lr: 0.000318, loss: 1.2108
2022-03-05 09:30:46 - train: epoch 0194, iter [02100, 05004], lr: 0.000318, loss: 1.5218
2022-03-05 09:31:20 - train: epoch 0194, iter [02200, 05004], lr: 0.000318, loss: 1.2912
2022-03-05 09:31:52 - train: epoch 0194, iter [02300, 05004], lr: 0.000318, loss: 1.2687
2022-03-05 09:32:25 - train: epoch 0194, iter [02400, 05004], lr: 0.000318, loss: 1.3609
2022-03-05 09:32:58 - train: epoch 0194, iter [02500, 05004], lr: 0.000318, loss: 1.3804
2022-03-05 09:33:32 - train: epoch 0194, iter [02600, 05004], lr: 0.000318, loss: 1.5044
2022-03-05 09:34:04 - train: epoch 0194, iter [02700, 05004], lr: 0.000318, loss: 1.3442
2022-03-05 09:34:38 - train: epoch 0194, iter [02800, 05004], lr: 0.000318, loss: 1.5872
2022-03-05 09:35:12 - train: epoch 0194, iter [02900, 05004], lr: 0.000318, loss: 1.5127
2022-03-05 09:35:46 - train: epoch 0194, iter [03000, 05004], lr: 0.000318, loss: 1.3608
2022-03-05 09:36:18 - train: epoch 0194, iter [03100, 05004], lr: 0.000318, loss: 1.6310
2022-03-05 09:36:52 - train: epoch 0194, iter [03200, 05004], lr: 0.000318, loss: 1.3142
2022-03-05 09:37:25 - train: epoch 0194, iter [03300, 05004], lr: 0.000318, loss: 1.3697
2022-03-05 09:37:59 - train: epoch 0194, iter [03400, 05004], lr: 0.000318, loss: 1.2925
2022-03-05 09:38:31 - train: epoch 0194, iter [03500, 05004], lr: 0.000318, loss: 1.3089
2022-03-05 09:39:05 - train: epoch 0194, iter [03600, 05004], lr: 0.000318, loss: 1.1045
2022-03-05 09:39:38 - train: epoch 0194, iter [03700, 05004], lr: 0.000318, loss: 1.3935
2022-03-05 09:40:11 - train: epoch 0194, iter [03800, 05004], lr: 0.000318, loss: 1.3688
2022-03-05 09:40:44 - train: epoch 0194, iter [03900, 05004], lr: 0.000318, loss: 1.0922
2022-03-05 09:41:17 - train: epoch 0194, iter [04000, 05004], lr: 0.000318, loss: 1.4326
2022-03-05 09:41:50 - train: epoch 0194, iter [04100, 05004], lr: 0.000318, loss: 1.1766
2022-03-05 09:42:24 - train: epoch 0194, iter [04200, 05004], lr: 0.000318, loss: 1.0394
2022-03-05 09:42:56 - train: epoch 0194, iter [04300, 05004], lr: 0.000318, loss: 1.1298
2022-03-05 09:43:30 - train: epoch 0194, iter [04400, 05004], lr: 0.000318, loss: 1.4652
2022-03-05 09:44:02 - train: epoch 0194, iter [04500, 05004], lr: 0.000318, loss: 1.3336
2022-03-05 09:44:34 - train: epoch 0194, iter [04600, 05004], lr: 0.000318, loss: 1.4626
2022-03-05 09:45:08 - train: epoch 0194, iter [04700, 05004], lr: 0.000318, loss: 1.2440
2022-03-05 09:45:40 - train: epoch 0194, iter [04800, 05004], lr: 0.000318, loss: 1.2945
2022-03-05 09:46:13 - train: epoch 0194, iter [04900, 05004], lr: 0.000318, loss: 1.6244
2022-03-05 09:46:45 - train: epoch 0194, iter [05000, 05004], lr: 0.000318, loss: 1.2021
2022-03-05 09:46:46 - train: epoch 194, train_loss: 1.3628
2022-03-05 09:47:57 - eval: epoch: 194, acc1: 77.580%, acc5: 93.658%, test_loss: 0.8891, per_image_load_time: 1.873ms, per_image_inference_time: 0.437ms
2022-03-05 09:47:58 - until epoch: 194, best_acc1: 77.580%
2022-03-05 09:47:58 - epoch 195 lr: 0.00023341826411756863
2022-03-05 09:48:36 - train: epoch 0195, iter [00100, 05004], lr: 0.000233, loss: 1.2926
2022-03-05 09:49:09 - train: epoch 0195, iter [00200, 05004], lr: 0.000233, loss: 1.2748
2022-03-05 09:49:41 - train: epoch 0195, iter [00300, 05004], lr: 0.000233, loss: 1.4346
2022-03-05 09:50:15 - train: epoch 0195, iter [00400, 05004], lr: 0.000233, loss: 1.6830
2022-03-05 09:50:48 - train: epoch 0195, iter [00500, 05004], lr: 0.000233, loss: 1.6508
2022-03-05 09:51:21 - train: epoch 0195, iter [00600, 05004], lr: 0.000233, loss: 1.3169
2022-03-05 09:51:53 - train: epoch 0195, iter [00700, 05004], lr: 0.000233, loss: 1.2425
2022-03-05 09:52:27 - train: epoch 0195, iter [00800, 05004], lr: 0.000233, loss: 1.2429
2022-03-05 09:52:58 - train: epoch 0195, iter [00900, 05004], lr: 0.000233, loss: 1.4738
2022-03-05 09:53:33 - train: epoch 0195, iter [01000, 05004], lr: 0.000233, loss: 1.3979
2022-03-05 09:54:05 - train: epoch 0195, iter [01100, 05004], lr: 0.000233, loss: 1.2876
2022-03-05 09:54:39 - train: epoch 0195, iter [01200, 05004], lr: 0.000233, loss: 1.4664
2022-03-05 09:55:11 - train: epoch 0195, iter [01300, 05004], lr: 0.000233, loss: 1.3375
2022-03-05 09:55:44 - train: epoch 0195, iter [01400, 05004], lr: 0.000233, loss: 1.2574
2022-03-05 09:56:16 - train: epoch 0195, iter [01500, 05004], lr: 0.000233, loss: 1.3671
2022-03-05 09:56:49 - train: epoch 0195, iter [01600, 05004], lr: 0.000233, loss: 1.4514
2022-03-05 09:57:21 - train: epoch 0195, iter [01700, 05004], lr: 0.000233, loss: 1.2668
2022-03-05 09:57:55 - train: epoch 0195, iter [01800, 05004], lr: 0.000233, loss: 1.2780
2022-03-05 09:58:28 - train: epoch 0195, iter [01900, 05004], lr: 0.000233, loss: 1.1882
2022-03-05 09:59:01 - train: epoch 0195, iter [02000, 05004], lr: 0.000233, loss: 1.1480
2022-03-05 09:59:34 - train: epoch 0195, iter [02100, 05004], lr: 0.000233, loss: 1.3630
2022-03-05 10:00:07 - train: epoch 0195, iter [02200, 05004], lr: 0.000233, loss: 1.3322
2022-03-05 10:00:40 - train: epoch 0195, iter [02300, 05004], lr: 0.000233, loss: 1.2071
2022-03-05 10:01:13 - train: epoch 0195, iter [02400, 05004], lr: 0.000233, loss: 1.3104
2022-03-05 10:01:46 - train: epoch 0195, iter [02500, 05004], lr: 0.000233, loss: 1.5757
2022-03-05 10:02:19 - train: epoch 0195, iter [02600, 05004], lr: 0.000233, loss: 1.3249
2022-03-05 10:02:51 - train: epoch 0195, iter [02700, 05004], lr: 0.000233, loss: 1.4922
2022-03-05 10:03:25 - train: epoch 0195, iter [02800, 05004], lr: 0.000233, loss: 1.1675
2022-03-05 10:03:57 - train: epoch 0195, iter [02900, 05004], lr: 0.000233, loss: 1.3851
2022-03-05 10:04:29 - train: epoch 0195, iter [03000, 05004], lr: 0.000233, loss: 1.5556
2022-03-05 10:05:02 - train: epoch 0195, iter [03100, 05004], lr: 0.000233, loss: 1.4792
2022-03-05 10:05:35 - train: epoch 0195, iter [03200, 05004], lr: 0.000233, loss: 1.2792
2022-03-05 10:06:07 - train: epoch 0195, iter [03300, 05004], lr: 0.000233, loss: 1.3032
2022-03-05 10:06:41 - train: epoch 0195, iter [03400, 05004], lr: 0.000233, loss: 1.3110
2022-03-05 10:07:15 - train: epoch 0195, iter [03500, 05004], lr: 0.000233, loss: 1.3110
2022-03-05 10:07:46 - train: epoch 0195, iter [03600, 05004], lr: 0.000233, loss: 1.2552
2022-03-05 10:08:19 - train: epoch 0195, iter [03700, 05004], lr: 0.000233, loss: 1.4461
2022-03-05 10:08:52 - train: epoch 0195, iter [03800, 05004], lr: 0.000233, loss: 1.3236
2022-03-05 10:09:26 - train: epoch 0195, iter [03900, 05004], lr: 0.000233, loss: 1.2427
2022-03-05 10:09:58 - train: epoch 0195, iter [04000, 05004], lr: 0.000233, loss: 1.0594
2022-03-05 10:10:32 - train: epoch 0195, iter [04100, 05004], lr: 0.000233, loss: 1.1655
2022-03-05 10:11:04 - train: epoch 0195, iter [04200, 05004], lr: 0.000233, loss: 1.3144
2022-03-05 10:11:37 - train: epoch 0195, iter [04300, 05004], lr: 0.000233, loss: 1.5525
2022-03-05 10:12:09 - train: epoch 0195, iter [04400, 05004], lr: 0.000233, loss: 1.1777
2022-03-05 10:12:42 - train: epoch 0195, iter [04500, 05004], lr: 0.000233, loss: 1.3435
2022-03-05 10:13:15 - train: epoch 0195, iter [04600, 05004], lr: 0.000233, loss: 1.2533
2022-03-05 10:13:48 - train: epoch 0195, iter [04700, 05004], lr: 0.000233, loss: 1.5485
2022-03-05 10:14:20 - train: epoch 0195, iter [04800, 05004], lr: 0.000233, loss: 1.2690
2022-03-05 10:14:54 - train: epoch 0195, iter [04900, 05004], lr: 0.000233, loss: 1.2389
2022-03-05 10:15:25 - train: epoch 0195, iter [05000, 05004], lr: 0.000233, loss: 1.1430
2022-03-05 10:15:26 - train: epoch 195, train_loss: 1.3574
2022-03-05 10:16:38 - eval: epoch: 195, acc1: 77.490%, acc5: 93.668%, test_loss: 0.8863, per_image_load_time: 2.326ms, per_image_inference_time: 0.427ms
2022-03-05 10:16:39 - until epoch: 195, best_acc1: 77.580%
2022-03-05 10:16:39 - epoch 196 lr: 0.00016213459328950355
2022-03-05 10:17:16 - train: epoch 0196, iter [00100, 05004], lr: 0.000162, loss: 1.4141
2022-03-05 10:17:51 - train: epoch 0196, iter [00200, 05004], lr: 0.000162, loss: 1.4515
2022-03-05 10:18:23 - train: epoch 0196, iter [00300, 05004], lr: 0.000162, loss: 1.4320
2022-03-05 10:18:57 - train: epoch 0196, iter [00400, 05004], lr: 0.000162, loss: 1.2467
2022-03-05 10:19:28 - train: epoch 0196, iter [00500, 05004], lr: 0.000162, loss: 1.2401
2022-03-05 10:20:01 - train: epoch 0196, iter [00600, 05004], lr: 0.000162, loss: 1.3960
2022-03-05 10:20:33 - train: epoch 0196, iter [00700, 05004], lr: 0.000162, loss: 1.2766
2022-03-05 10:21:06 - train: epoch 0196, iter [00800, 05004], lr: 0.000162, loss: 1.4841
2022-03-05 10:21:39 - train: epoch 0196, iter [00900, 05004], lr: 0.000162, loss: 1.0131
2022-03-05 10:22:12 - train: epoch 0196, iter [01000, 05004], lr: 0.000162, loss: 1.5714
2022-03-05 10:22:45 - train: epoch 0196, iter [01100, 05004], lr: 0.000162, loss: 1.2876
2022-03-05 10:23:18 - train: epoch 0196, iter [01200, 05004], lr: 0.000162, loss: 1.5357
2022-03-05 10:23:50 - train: epoch 0196, iter [01300, 05004], lr: 0.000162, loss: 1.4662
2022-03-05 10:24:22 - train: epoch 0196, iter [01400, 05004], lr: 0.000162, loss: 0.9882
2022-03-05 10:24:57 - train: epoch 0196, iter [01500, 05004], lr: 0.000162, loss: 1.2761
2022-03-05 10:25:30 - train: epoch 0196, iter [01600, 05004], lr: 0.000162, loss: 1.1390
2022-03-05 10:26:03 - train: epoch 0196, iter [01700, 05004], lr: 0.000162, loss: 1.2528
2022-03-05 10:26:37 - train: epoch 0196, iter [01800, 05004], lr: 0.000162, loss: 1.3387
2022-03-05 10:27:10 - train: epoch 0196, iter [01900, 05004], lr: 0.000162, loss: 1.4988
2022-03-05 10:27:42 - train: epoch 0196, iter [02000, 05004], lr: 0.000162, loss: 1.5803
2022-03-05 10:28:15 - train: epoch 0196, iter [02100, 05004], lr: 0.000162, loss: 1.3528
2022-03-05 10:28:48 - train: epoch 0196, iter [02200, 05004], lr: 0.000162, loss: 1.2235
2022-03-05 10:29:22 - train: epoch 0196, iter [02300, 05004], lr: 0.000162, loss: 1.3355
2022-03-05 10:29:55 - train: epoch 0196, iter [02400, 05004], lr: 0.000162, loss: 1.1736
2022-03-05 10:30:29 - train: epoch 0196, iter [02500, 05004], lr: 0.000162, loss: 1.3484
2022-03-05 10:31:03 - train: epoch 0196, iter [02600, 05004], lr: 0.000162, loss: 1.1442
2022-03-05 10:31:34 - train: epoch 0196, iter [02700, 05004], lr: 0.000162, loss: 1.3798
2022-03-05 10:32:07 - train: epoch 0196, iter [02800, 05004], lr: 0.000162, loss: 1.2409
2022-03-05 10:32:41 - train: epoch 0196, iter [02900, 05004], lr: 0.000162, loss: 1.4874
2022-03-05 10:33:14 - train: epoch 0196, iter [03000, 05004], lr: 0.000162, loss: 1.4657
2022-03-05 10:33:47 - train: epoch 0196, iter [03100, 05004], lr: 0.000162, loss: 1.3168
2022-03-05 10:34:21 - train: epoch 0196, iter [03200, 05004], lr: 0.000162, loss: 0.9865
2022-03-05 10:34:54 - train: epoch 0196, iter [03300, 05004], lr: 0.000162, loss: 1.3512
2022-03-05 10:35:27 - train: epoch 0196, iter [03400, 05004], lr: 0.000162, loss: 1.6093
2022-03-05 10:36:00 - train: epoch 0196, iter [03500, 05004], lr: 0.000162, loss: 1.2238
2022-03-05 10:36:33 - train: epoch 0196, iter [03600, 05004], lr: 0.000162, loss: 1.2865
2022-03-05 10:37:06 - train: epoch 0196, iter [03700, 05004], lr: 0.000162, loss: 1.3791
2022-03-05 10:37:40 - train: epoch 0196, iter [03800, 05004], lr: 0.000162, loss: 1.3773
2022-03-05 10:38:13 - train: epoch 0196, iter [03900, 05004], lr: 0.000162, loss: 1.4218
2022-03-05 10:38:47 - train: epoch 0196, iter [04000, 05004], lr: 0.000162, loss: 1.4229
2022-03-05 10:39:19 - train: epoch 0196, iter [04100, 05004], lr: 0.000162, loss: 1.3443
2022-03-05 10:39:51 - train: epoch 0196, iter [04200, 05004], lr: 0.000162, loss: 1.3728
2022-03-05 10:40:24 - train: epoch 0196, iter [04300, 05004], lr: 0.000162, loss: 1.1451
2022-03-05 10:40:57 - train: epoch 0196, iter [04400, 05004], lr: 0.000162, loss: 1.5648
2022-03-05 10:41:30 - train: epoch 0196, iter [04500, 05004], lr: 0.000162, loss: 1.2320
2022-03-05 10:42:04 - train: epoch 0196, iter [04600, 05004], lr: 0.000162, loss: 1.3767
2022-03-05 10:42:37 - train: epoch 0196, iter [04700, 05004], lr: 0.000162, loss: 1.4435
2022-03-05 10:43:11 - train: epoch 0196, iter [04800, 05004], lr: 0.000162, loss: 1.5515
2022-03-05 10:43:43 - train: epoch 0196, iter [04900, 05004], lr: 0.000162, loss: 1.3966
2022-03-05 10:44:15 - train: epoch 0196, iter [05000, 05004], lr: 0.000162, loss: 1.3315
2022-03-05 10:44:16 - train: epoch 196, train_loss: 1.3471
2022-03-05 10:45:29 - eval: epoch: 196, acc1: 77.512%, acc5: 93.686%, test_loss: 0.8848, per_image_load_time: 1.286ms, per_image_inference_time: 0.465ms
2022-03-05 10:45:30 - until epoch: 196, best_acc1: 77.580%
2022-03-05 10:45:30 - epoch 197 lr: 0.00010378634328099269
2022-03-05 10:46:09 - train: epoch 0197, iter [00100, 05004], lr: 0.000104, loss: 1.4492
2022-03-05 10:46:43 - train: epoch 0197, iter [00200, 05004], lr: 0.000104, loss: 1.3946
2022-03-05 10:47:16 - train: epoch 0197, iter [00300, 05004], lr: 0.000104, loss: 1.3121
2022-03-05 10:47:49 - train: epoch 0197, iter [00400, 05004], lr: 0.000104, loss: 1.2874
2022-03-05 10:48:22 - train: epoch 0197, iter [00500, 05004], lr: 0.000104, loss: 1.1607
2022-03-05 10:48:55 - train: epoch 0197, iter [00600, 05004], lr: 0.000104, loss: 1.5028
2022-03-05 10:49:28 - train: epoch 0197, iter [00700, 05004], lr: 0.000104, loss: 1.3422
2022-03-05 10:50:02 - train: epoch 0197, iter [00800, 05004], lr: 0.000104, loss: 1.4099
2022-03-05 10:50:35 - train: epoch 0197, iter [00900, 05004], lr: 0.000104, loss: 1.4430
2022-03-05 10:51:09 - train: epoch 0197, iter [01000, 05004], lr: 0.000104, loss: 1.3733
2022-03-05 10:51:40 - train: epoch 0197, iter [01100, 05004], lr: 0.000104, loss: 1.3738
2022-03-05 10:52:14 - train: epoch 0197, iter [01200, 05004], lr: 0.000104, loss: 1.5172
2022-03-05 10:52:47 - train: epoch 0197, iter [01300, 05004], lr: 0.000104, loss: 1.2851
2022-03-05 10:53:20 - train: epoch 0197, iter [01400, 05004], lr: 0.000104, loss: 1.2777
2022-03-05 10:53:54 - train: epoch 0197, iter [01500, 05004], lr: 0.000104, loss: 1.1973
2022-03-05 10:54:27 - train: epoch 0197, iter [01600, 05004], lr: 0.000104, loss: 1.4296
2022-03-05 10:55:00 - train: epoch 0197, iter [01700, 05004], lr: 0.000104, loss: 1.6060
2022-03-05 10:55:33 - train: epoch 0197, iter [01800, 05004], lr: 0.000104, loss: 1.5614
2022-03-05 10:56:06 - train: epoch 0197, iter [01900, 05004], lr: 0.000104, loss: 1.3146
2022-03-05 10:56:40 - train: epoch 0197, iter [02000, 05004], lr: 0.000104, loss: 1.1748
2022-03-05 10:57:12 - train: epoch 0197, iter [02100, 05004], lr: 0.000104, loss: 1.4655
2022-03-05 10:57:46 - train: epoch 0197, iter [02200, 05004], lr: 0.000104, loss: 1.2441
2022-03-05 10:58:19 - train: epoch 0197, iter [02300, 05004], lr: 0.000104, loss: 1.5115
2022-03-05 10:58:52 - train: epoch 0197, iter [02400, 05004], lr: 0.000104, loss: 1.2799
2022-03-05 10:59:25 - train: epoch 0197, iter [02500, 05004], lr: 0.000104, loss: 1.6108
2022-03-05 10:59:58 - train: epoch 0197, iter [02600, 05004], lr: 0.000104, loss: 1.5688
2022-03-05 11:00:31 - train: epoch 0197, iter [02700, 05004], lr: 0.000104, loss: 1.2045
2022-03-05 11:01:05 - train: epoch 0197, iter [02800, 05004], lr: 0.000104, loss: 1.3220
2022-03-05 11:01:38 - train: epoch 0197, iter [02900, 05004], lr: 0.000104, loss: 1.2334
2022-03-05 11:02:11 - train: epoch 0197, iter [03000, 05004], lr: 0.000104, loss: 1.1235
2022-03-05 11:02:44 - train: epoch 0197, iter [03100, 05004], lr: 0.000104, loss: 1.3272
2022-03-05 11:03:17 - train: epoch 0197, iter [03200, 05004], lr: 0.000104, loss: 1.3768
2022-03-05 11:03:50 - train: epoch 0197, iter [03300, 05004], lr: 0.000104, loss: 1.2020
2022-03-05 11:04:23 - train: epoch 0197, iter [03400, 05004], lr: 0.000104, loss: 1.3331
2022-03-05 11:04:57 - train: epoch 0197, iter [03500, 05004], lr: 0.000104, loss: 1.3325
2022-03-05 11:05:30 - train: epoch 0197, iter [03600, 05004], lr: 0.000104, loss: 1.1199
2022-03-05 11:06:03 - train: epoch 0197, iter [03700, 05004], lr: 0.000104, loss: 1.2058
2022-03-05 11:06:36 - train: epoch 0197, iter [03800, 05004], lr: 0.000104, loss: 1.4381
2022-03-05 11:07:09 - train: epoch 0197, iter [03900, 05004], lr: 0.000104, loss: 1.3411
2022-03-05 11:07:43 - train: epoch 0197, iter [04000, 05004], lr: 0.000104, loss: 1.2230
2022-03-05 11:08:16 - train: epoch 0197, iter [04100, 05004], lr: 0.000104, loss: 1.1089
2022-03-05 11:08:50 - train: epoch 0197, iter [04200, 05004], lr: 0.000104, loss: 1.4548
2022-03-05 11:09:22 - train: epoch 0197, iter [04300, 05004], lr: 0.000104, loss: 1.4961
2022-03-05 11:09:56 - train: epoch 0197, iter [04400, 05004], lr: 0.000104, loss: 1.6371
2022-03-05 11:10:29 - train: epoch 0197, iter [04500, 05004], lr: 0.000104, loss: 1.1598
2022-03-05 11:11:02 - train: epoch 0197, iter [04600, 05004], lr: 0.000104, loss: 1.3201
2022-03-05 11:11:35 - train: epoch 0197, iter [04700, 05004], lr: 0.000104, loss: 1.3542
2022-03-05 11:12:10 - train: epoch 0197, iter [04800, 05004], lr: 0.000104, loss: 1.2121
2022-03-05 11:12:43 - train: epoch 0197, iter [04900, 05004], lr: 0.000104, loss: 1.2438
2022-03-05 11:13:15 - train: epoch 0197, iter [05000, 05004], lr: 0.000104, loss: 1.3555
2022-03-05 11:13:16 - train: epoch 197, train_loss: 1.3509
2022-03-05 11:14:28 - eval: epoch: 197, acc1: 77.672%, acc5: 93.726%, test_loss: 0.8807, per_image_load_time: 1.595ms, per_image_inference_time: 0.452ms
2022-03-05 11:14:28 - until epoch: 197, best_acc1: 77.672%
2022-03-05 11:14:28 - epoch 198 lr: 5.8388658383667914e-05
2022-03-05 11:15:07 - train: epoch 0198, iter [00100, 05004], lr: 0.000058, loss: 1.1966
2022-03-05 11:15:41 - train: epoch 0198, iter [00200, 05004], lr: 0.000058, loss: 1.2962
2022-03-05 11:16:14 - train: epoch 0198, iter [00300, 05004], lr: 0.000058, loss: 1.2789
2022-03-05 11:16:47 - train: epoch 0198, iter [00400, 05004], lr: 0.000058, loss: 1.3465
2022-03-05 11:17:21 - train: epoch 0198, iter [00500, 05004], lr: 0.000058, loss: 1.4386
2022-03-05 11:17:54 - train: epoch 0198, iter [00600, 05004], lr: 0.000058, loss: 1.4947
2022-03-05 11:18:26 - train: epoch 0198, iter [00700, 05004], lr: 0.000058, loss: 1.3285
2022-03-05 11:19:00 - train: epoch 0198, iter [00800, 05004], lr: 0.000058, loss: 1.2769
2022-03-05 11:19:33 - train: epoch 0198, iter [00900, 05004], lr: 0.000058, loss: 1.3915
2022-03-05 11:20:07 - train: epoch 0198, iter [01000, 05004], lr: 0.000058, loss: 1.4679
2022-03-05 11:20:40 - train: epoch 0198, iter [01100, 05004], lr: 0.000058, loss: 1.3340
2022-03-05 11:21:13 - train: epoch 0198, iter [01200, 05004], lr: 0.000058, loss: 1.4913
2022-03-05 11:21:46 - train: epoch 0198, iter [01300, 05004], lr: 0.000058, loss: 1.2797
2022-03-05 11:22:19 - train: epoch 0198, iter [01400, 05004], lr: 0.000058, loss: 1.5949
2022-03-05 11:22:52 - train: epoch 0198, iter [01500, 05004], lr: 0.000058, loss: 1.2842
2022-03-05 11:23:26 - train: epoch 0198, iter [01600, 05004], lr: 0.000058, loss: 1.3247
2022-03-05 11:23:59 - train: epoch 0198, iter [01700, 05004], lr: 0.000058, loss: 1.2047
2022-03-05 11:24:32 - train: epoch 0198, iter [01800, 05004], lr: 0.000058, loss: 1.4975
2022-03-05 11:25:06 - train: epoch 0198, iter [01900, 05004], lr: 0.000058, loss: 1.4453
2022-03-05 11:25:39 - train: epoch 0198, iter [02000, 05004], lr: 0.000058, loss: 1.4043
2022-03-05 11:26:11 - train: epoch 0198, iter [02100, 05004], lr: 0.000058, loss: 1.3831
2022-03-05 11:26:45 - train: epoch 0198, iter [02200, 05004], lr: 0.000058, loss: 1.4171
2022-03-05 11:27:19 - train: epoch 0198, iter [02300, 05004], lr: 0.000058, loss: 1.3054
2022-03-05 11:27:52 - train: epoch 0198, iter [02400, 05004], lr: 0.000058, loss: 1.1277
2022-03-05 11:28:25 - train: epoch 0198, iter [02500, 05004], lr: 0.000058, loss: 1.5277
2022-03-05 11:28:58 - train: epoch 0198, iter [02600, 05004], lr: 0.000058, loss: 1.2926
2022-03-05 11:29:31 - train: epoch 0198, iter [02700, 05004], lr: 0.000058, loss: 1.3869
2022-03-05 11:30:04 - train: epoch 0198, iter [02800, 05004], lr: 0.000058, loss: 1.4130
2022-03-05 11:30:38 - train: epoch 0198, iter [02900, 05004], lr: 0.000058, loss: 1.7947
2022-03-05 11:31:11 - train: epoch 0198, iter [03000, 05004], lr: 0.000058, loss: 1.4141
2022-03-05 11:31:45 - train: epoch 0198, iter [03100, 05004], lr: 0.000058, loss: 1.5648
2022-03-05 11:32:18 - train: epoch 0198, iter [03200, 05004], lr: 0.000058, loss: 1.5246
2022-03-05 11:32:51 - train: epoch 0198, iter [03300, 05004], lr: 0.000058, loss: 1.5050
2022-03-05 11:33:24 - train: epoch 0198, iter [03400, 05004], lr: 0.000058, loss: 1.1870
2022-03-05 11:33:57 - train: epoch 0198, iter [03500, 05004], lr: 0.000058, loss: 1.4349
2022-03-05 11:34:31 - train: epoch 0198, iter [03600, 05004], lr: 0.000058, loss: 1.2257
2022-03-05 11:35:04 - train: epoch 0198, iter [03700, 05004], lr: 0.000058, loss: 1.5448
2022-03-05 11:35:37 - train: epoch 0198, iter [03800, 05004], lr: 0.000058, loss: 1.3714
2022-03-05 11:36:10 - train: epoch 0198, iter [03900, 05004], lr: 0.000058, loss: 1.4142
2022-03-05 11:36:43 - train: epoch 0198, iter [04000, 05004], lr: 0.000058, loss: 1.0693
2022-03-05 11:37:16 - train: epoch 0198, iter [04100, 05004], lr: 0.000058, loss: 1.2549
2022-03-05 11:37:50 - train: epoch 0198, iter [04200, 05004], lr: 0.000058, loss: 1.3672
2022-03-05 11:38:23 - train: epoch 0198, iter [04300, 05004], lr: 0.000058, loss: 1.3758
2022-03-05 11:38:56 - train: epoch 0198, iter [04400, 05004], lr: 0.000058, loss: 1.6028
2022-03-05 11:39:29 - train: epoch 0198, iter [04500, 05004], lr: 0.000058, loss: 1.4728
2022-03-05 11:40:01 - train: epoch 0198, iter [04600, 05004], lr: 0.000058, loss: 1.2733
2022-03-05 11:40:35 - train: epoch 0198, iter [04700, 05004], lr: 0.000058, loss: 1.1719
2022-03-05 11:41:07 - train: epoch 0198, iter [04800, 05004], lr: 0.000058, loss: 1.2432
2022-03-05 11:41:41 - train: epoch 0198, iter [04900, 05004], lr: 0.000058, loss: 1.0057
2022-03-05 11:42:13 - train: epoch 0198, iter [05000, 05004], lr: 0.000058, loss: 1.3430
2022-03-05 11:42:14 - train: epoch 198, train_loss: 1.3439
2022-03-05 11:43:27 - eval: epoch: 198, acc1: 77.650%, acc5: 93.730%, test_loss: 0.8814, per_image_load_time: 1.840ms, per_image_inference_time: 0.424ms
2022-03-05 11:43:28 - until epoch: 198, best_acc1: 77.672%
2022-03-05 11:43:28 - epoch 199 lr: 2.595332156925534e-05
2022-03-05 11:44:07 - train: epoch 0199, iter [00100, 05004], lr: 0.000026, loss: 1.3177
2022-03-05 11:44:40 - train: epoch 0199, iter [00200, 05004], lr: 0.000026, loss: 1.3387
2022-03-05 11:45:13 - train: epoch 0199, iter [00300, 05004], lr: 0.000026, loss: 1.4623
2022-03-05 11:45:46 - train: epoch 0199, iter [00400, 05004], lr: 0.000026, loss: 1.4284
2022-03-05 11:46:20 - train: epoch 0199, iter [00500, 05004], lr: 0.000026, loss: 1.1237
2022-03-05 11:46:53 - train: epoch 0199, iter [00600, 05004], lr: 0.000026, loss: 1.3495
2022-03-05 11:47:26 - train: epoch 0199, iter [00700, 05004], lr: 0.000026, loss: 1.2726
2022-03-05 11:47:59 - train: epoch 0199, iter [00800, 05004], lr: 0.000026, loss: 1.5579
2022-03-05 11:48:32 - train: epoch 0199, iter [00900, 05004], lr: 0.000026, loss: 1.3254
2022-03-05 11:49:05 - train: epoch 0199, iter [01000, 05004], lr: 0.000026, loss: 1.2581
2022-03-05 11:49:39 - train: epoch 0199, iter [01100, 05004], lr: 0.000026, loss: 1.5764
2022-03-05 11:50:11 - train: epoch 0199, iter [01200, 05004], lr: 0.000026, loss: 1.3502
2022-03-05 11:50:44 - train: epoch 0199, iter [01300, 05004], lr: 0.000026, loss: 1.4989
2022-03-05 11:51:17 - train: epoch 0199, iter [01400, 05004], lr: 0.000026, loss: 1.3341
2022-03-05 11:51:50 - train: epoch 0199, iter [01500, 05004], lr: 0.000026, loss: 1.3556
2022-03-05 11:52:24 - train: epoch 0199, iter [01600, 05004], lr: 0.000026, loss: 1.6310
2022-03-05 11:52:56 - train: epoch 0199, iter [01700, 05004], lr: 0.000026, loss: 1.5859
2022-03-05 11:53:31 - train: epoch 0199, iter [01800, 05004], lr: 0.000026, loss: 1.5227
2022-03-05 11:54:04 - train: epoch 0199, iter [01900, 05004], lr: 0.000026, loss: 1.3947
2022-03-05 11:54:36 - train: epoch 0199, iter [02000, 05004], lr: 0.000026, loss: 1.2415
2022-03-05 11:55:10 - train: epoch 0199, iter [02100, 05004], lr: 0.000026, loss: 1.2185
2022-03-05 11:55:43 - train: epoch 0199, iter [02200, 05004], lr: 0.000026, loss: 1.3724
2022-03-05 11:56:15 - train: epoch 0199, iter [02300, 05004], lr: 0.000026, loss: 1.1817
2022-03-05 11:56:49 - train: epoch 0199, iter [02400, 05004], lr: 0.000026, loss: 1.5592
2022-03-05 11:57:22 - train: epoch 0199, iter [02500, 05004], lr: 0.000026, loss: 1.2768
2022-03-05 11:57:55 - train: epoch 0199, iter [02600, 05004], lr: 0.000026, loss: 1.6589
2022-03-05 11:58:29 - train: epoch 0199, iter [02700, 05004], lr: 0.000026, loss: 1.3093
2022-03-05 11:59:02 - train: epoch 0199, iter [02800, 05004], lr: 0.000026, loss: 1.1793
2022-03-05 11:59:35 - train: epoch 0199, iter [02900, 05004], lr: 0.000026, loss: 1.4539
2022-03-05 12:00:09 - train: epoch 0199, iter [03000, 05004], lr: 0.000026, loss: 1.1997
2022-03-05 12:00:42 - train: epoch 0199, iter [03100, 05004], lr: 0.000026, loss: 1.3568
2022-03-05 12:01:15 - train: epoch 0199, iter [03200, 05004], lr: 0.000026, loss: 1.1363
2022-03-05 12:01:48 - train: epoch 0199, iter [03300, 05004], lr: 0.000026, loss: 1.5037
2022-03-05 12:02:22 - train: epoch 0199, iter [03400, 05004], lr: 0.000026, loss: 1.5602
2022-03-05 12:02:55 - train: epoch 0199, iter [03500, 05004], lr: 0.000026, loss: 1.3947
2022-03-05 12:03:27 - train: epoch 0199, iter [03600, 05004], lr: 0.000026, loss: 1.2978
2022-03-05 12:04:01 - train: epoch 0199, iter [03700, 05004], lr: 0.000026, loss: 1.3024
2022-03-05 12:04:34 - train: epoch 0199, iter [03800, 05004], lr: 0.000026, loss: 1.4394
2022-03-05 12:05:08 - train: epoch 0199, iter [03900, 05004], lr: 0.000026, loss: 1.3613
2022-03-05 12:05:41 - train: epoch 0199, iter [04000, 05004], lr: 0.000026, loss: 1.3637
2022-03-05 12:06:14 - train: epoch 0199, iter [04100, 05004], lr: 0.000026, loss: 1.7206
2022-03-05 12:06:47 - train: epoch 0199, iter [04200, 05004], lr: 0.000026, loss: 1.2107
2022-03-05 12:07:20 - train: epoch 0199, iter [04300, 05004], lr: 0.000026, loss: 0.9112
2022-03-05 12:07:53 - train: epoch 0199, iter [04400, 05004], lr: 0.000026, loss: 1.4310
2022-03-05 12:08:26 - train: epoch 0199, iter [04500, 05004], lr: 0.000026, loss: 1.2446
2022-03-05 12:08:59 - train: epoch 0199, iter [04600, 05004], lr: 0.000026, loss: 1.1616
2022-03-05 12:09:33 - train: epoch 0199, iter [04700, 05004], lr: 0.000026, loss: 1.2113
2022-03-05 12:10:06 - train: epoch 0199, iter [04800, 05004], lr: 0.000026, loss: 1.4254
2022-03-05 12:10:39 - train: epoch 0199, iter [04900, 05004], lr: 0.000026, loss: 1.3657
2022-03-05 12:11:11 - train: epoch 0199, iter [05000, 05004], lr: 0.000026, loss: 1.5284
2022-03-05 12:11:12 - train: epoch 199, train_loss: 1.3416
2022-03-05 12:12:25 - eval: epoch: 199, acc1: 77.664%, acc5: 93.734%, test_loss: 0.8800, per_image_load_time: 1.855ms, per_image_inference_time: 0.459ms
2022-03-05 12:12:26 - until epoch: 199, best_acc1: 77.672%
2022-03-05 12:12:26 - epoch 200 lr: 6.488751431266149e-06
2022-03-05 12:13:05 - train: epoch 0200, iter [00100, 05004], lr: 0.000006, loss: 1.3386
2022-03-05 12:13:39 - train: epoch 0200, iter [00200, 05004], lr: 0.000006, loss: 1.3806
2022-03-05 12:14:11 - train: epoch 0200, iter [00300, 05004], lr: 0.000006, loss: 1.4040
2022-03-05 12:14:45 - train: epoch 0200, iter [00400, 05004], lr: 0.000006, loss: 1.2663
2022-03-05 12:15:18 - train: epoch 0200, iter [00500, 05004], lr: 0.000006, loss: 1.1536
2022-03-05 12:15:51 - train: epoch 0200, iter [00600, 05004], lr: 0.000006, loss: 1.5492
2022-03-05 12:16:24 - train: epoch 0200, iter [00700, 05004], lr: 0.000006, loss: 1.5574
2022-03-05 12:16:58 - train: epoch 0200, iter [00800, 05004], lr: 0.000006, loss: 1.2835
2022-03-05 12:17:31 - train: epoch 0200, iter [00900, 05004], lr: 0.000006, loss: 1.3619
2022-03-05 12:18:05 - train: epoch 0200, iter [01000, 05004], lr: 0.000006, loss: 1.5996
2022-03-05 12:18:36 - train: epoch 0200, iter [01100, 05004], lr: 0.000006, loss: 1.2048
2022-03-05 12:19:10 - train: epoch 0200, iter [01200, 05004], lr: 0.000006, loss: 1.2631
2022-03-05 12:19:42 - train: epoch 0200, iter [01300, 05004], lr: 0.000006, loss: 1.0698
2022-03-05 12:20:16 - train: epoch 0200, iter [01400, 05004], lr: 0.000006, loss: 1.2964
2022-03-05 12:20:49 - train: epoch 0200, iter [01500, 05004], lr: 0.000006, loss: 1.1461
2022-03-05 12:21:22 - train: epoch 0200, iter [01600, 05004], lr: 0.000006, loss: 1.5663
2022-03-05 12:21:55 - train: epoch 0200, iter [01700, 05004], lr: 0.000006, loss: 1.3394
2022-03-05 12:22:28 - train: epoch 0200, iter [01800, 05004], lr: 0.000006, loss: 1.5110
2022-03-05 12:23:01 - train: epoch 0200, iter [01900, 05004], lr: 0.000006, loss: 1.2336
2022-03-05 12:23:34 - train: epoch 0200, iter [02000, 05004], lr: 0.000006, loss: 1.4536
2022-03-05 12:24:07 - train: epoch 0200, iter [02100, 05004], lr: 0.000006, loss: 1.4029
2022-03-05 12:24:40 - train: epoch 0200, iter [02200, 05004], lr: 0.000006, loss: 1.5332
2022-03-05 12:25:14 - train: epoch 0200, iter [02300, 05004], lr: 0.000006, loss: 1.3918
2022-03-05 12:25:46 - train: epoch 0200, iter [02400, 05004], lr: 0.000006, loss: 1.5939
2022-03-05 12:26:19 - train: epoch 0200, iter [02500, 05004], lr: 0.000006, loss: 1.3252
2022-03-05 12:26:53 - train: epoch 0200, iter [02600, 05004], lr: 0.000006, loss: 1.3699
2022-03-05 12:27:26 - train: epoch 0200, iter [02700, 05004], lr: 0.000006, loss: 1.3288
2022-03-05 12:27:59 - train: epoch 0200, iter [02800, 05004], lr: 0.000006, loss: 1.3540
2022-03-05 12:28:33 - train: epoch 0200, iter [02900, 05004], lr: 0.000006, loss: 1.2328
2022-03-05 12:29:05 - train: epoch 0200, iter [03000, 05004], lr: 0.000006, loss: 1.3917
2022-03-05 12:29:38 - train: epoch 0200, iter [03100, 05004], lr: 0.000006, loss: 1.2131
2022-03-05 12:30:10 - train: epoch 0200, iter [03200, 05004], lr: 0.000006, loss: 1.4112
2022-03-05 12:30:44 - train: epoch 0200, iter [03300, 05004], lr: 0.000006, loss: 1.2537
2022-03-05 12:31:17 - train: epoch 0200, iter [03400, 05004], lr: 0.000006, loss: 1.2325
2022-03-05 12:31:51 - train: epoch 0200, iter [03500, 05004], lr: 0.000006, loss: 1.0832
2022-03-05 12:32:24 - train: epoch 0200, iter [03600, 05004], lr: 0.000006, loss: 1.3221
2022-03-05 12:32:57 - train: epoch 0200, iter [03700, 05004], lr: 0.000006, loss: 1.3302
2022-03-05 12:33:29 - train: epoch 0200, iter [03800, 05004], lr: 0.000006, loss: 1.4834
2022-03-05 12:34:02 - train: epoch 0200, iter [03900, 05004], lr: 0.000006, loss: 1.1986
2022-03-05 12:34:35 - train: epoch 0200, iter [04000, 05004], lr: 0.000006, loss: 1.4603
2022-03-05 12:35:09 - train: epoch 0200, iter [04100, 05004], lr: 0.000006, loss: 1.0874
2022-03-05 12:35:42 - train: epoch 0200, iter [04200, 05004], lr: 0.000006, loss: 1.2903
2022-03-05 12:36:15 - train: epoch 0200, iter [04300, 05004], lr: 0.000006, loss: 1.1835
2022-03-05 12:36:48 - train: epoch 0200, iter [04400, 05004], lr: 0.000006, loss: 1.2686
2022-03-05 12:37:20 - train: epoch 0200, iter [04500, 05004], lr: 0.000006, loss: 1.3207
2022-03-05 12:37:54 - train: epoch 0200, iter [04600, 05004], lr: 0.000006, loss: 1.4697
2022-03-05 12:38:27 - train: epoch 0200, iter [04700, 05004], lr: 0.000006, loss: 1.4198
2022-03-05 12:39:00 - train: epoch 0200, iter [04800, 05004], lr: 0.000006, loss: 1.4079
2022-03-05 12:39:34 - train: epoch 0200, iter [04900, 05004], lr: 0.000006, loss: 1.4582
2022-03-05 12:40:06 - train: epoch 0200, iter [05000, 05004], lr: 0.000006, loss: 1.2504
2022-03-05 12:40:07 - train: epoch 200, train_loss: 1.3384
2022-03-05 12:41:19 - eval: epoch: 200, acc1: 77.682%, acc5: 93.702%, test_loss: 0.8805, per_image_load_time: 2.111ms, per_image_inference_time: 0.424ms
2022-03-05 12:41:20 - until epoch: 200, best_acc1: 77.682%
2022-03-05 12:41:20 - train done. model: resnet50, train time: 96.833 hours, best_acc1: 77.682%
