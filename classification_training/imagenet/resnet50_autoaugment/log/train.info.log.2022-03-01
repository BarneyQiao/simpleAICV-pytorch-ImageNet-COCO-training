2022-03-01 08:19:36 - network: resnet50
2022-03-01 08:19:36 - num_classes: 1000
2022-03-01 08:19:36 - input_image_size: 224
2022-03-01 08:19:36 - scale: 1.1428571428571428
2022-03-01 08:19:36 - trained_model_path: 
2022-03-01 08:19:36 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-03-01 08:19:36 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fcc78055940>
2022-03-01 08:19:36 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fcc78055c10>
2022-03-01 08:19:36 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fcc78055c40>
2022-03-01 08:19:36 - seed: 0
2022-03-01 08:19:36 - batch_size: 256
2022-03-01 08:19:36 - num_workers: 16
2022-03-01 08:19:36 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-03-01 08:19:36 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-03-01 08:19:36 - epochs: 200
2022-03-01 08:19:36 - print_interval: 100
2022-03-01 08:19:36 - distributed: True
2022-03-01 08:19:36 - sync_bn: False
2022-03-01 08:19:36 - apex: True
2022-03-01 08:19:36 - gpus_type: NVIDIA GeForce RTX 3090
2022-03-01 08:19:36 - gpus_num: 2
2022-03-01 08:19:36 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fcc557e6c70>
2022-03-01 08:19:40 - --------------------parameters--------------------
2022-03-01 08:19:40 - name: conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-03-01 08:19:40 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-03-01 08:19:40 - name: fc.weight, grad: True
2022-03-01 08:19:40 - name: fc.bias, grad: True
2022-03-01 08:19:40 - --------------------buffers--------------------
2022-03-01 08:19:40 - name: conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-03-01 08:19:40 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-03-01 08:19:41 - epoch 001 lr: 0.020000000000000004
2022-03-01 08:20:20 - train: epoch 0001, iter [00100, 05004], lr: 0.020000, loss: 7.0950
2022-03-01 08:20:53 - train: epoch 0001, iter [00200, 05004], lr: 0.020000, loss: 6.9339
2022-03-01 08:21:26 - train: epoch 0001, iter [00300, 05004], lr: 0.020000, loss: 6.8931
2022-03-01 08:22:00 - train: epoch 0001, iter [00400, 05004], lr: 0.020000, loss: 6.9188
2022-03-01 08:22:33 - train: epoch 0001, iter [00500, 05004], lr: 0.020000, loss: 6.8930
2022-03-01 08:23:07 - train: epoch 0001, iter [00600, 05004], lr: 0.020000, loss: 6.7946
2022-03-01 08:23:40 - train: epoch 0001, iter [00700, 05004], lr: 0.020000, loss: 6.8444
2022-03-01 08:24:14 - train: epoch 0001, iter [00800, 05004], lr: 0.020000, loss: 6.7327
2022-03-01 08:24:48 - train: epoch 0001, iter [00900, 05004], lr: 0.020000, loss: 6.6658
2022-03-01 08:25:22 - train: epoch 0001, iter [01000, 05004], lr: 0.020000, loss: 6.6443
2022-03-01 08:25:56 - train: epoch 0001, iter [01100, 05004], lr: 0.020000, loss: 6.5496
2022-03-01 08:26:30 - train: epoch 0001, iter [01200, 05004], lr: 0.020000, loss: 6.4672
2022-03-01 08:27:04 - train: epoch 0001, iter [01300, 05004], lr: 0.020000, loss: 6.5173
2022-03-01 08:27:38 - train: epoch 0001, iter [01400, 05004], lr: 0.020000, loss: 6.4109
2022-03-01 08:28:11 - train: epoch 0001, iter [01500, 05004], lr: 0.020000, loss: 6.4601
2022-03-01 08:28:44 - train: epoch 0001, iter [01600, 05004], lr: 0.020000, loss: 6.4669
2022-03-01 08:29:16 - train: epoch 0001, iter [01700, 05004], lr: 0.020000, loss: 6.2731
2022-03-01 08:29:51 - train: epoch 0001, iter [01800, 05004], lr: 0.020000, loss: 6.3091
2022-03-01 08:30:25 - train: epoch 0001, iter [01900, 05004], lr: 0.020000, loss: 6.2098
2022-03-01 08:30:59 - train: epoch 0001, iter [02000, 05004], lr: 0.020000, loss: 6.1548
2022-03-01 08:31:33 - train: epoch 0001, iter [02100, 05004], lr: 0.020000, loss: 6.2563
2022-03-01 08:32:07 - train: epoch 0001, iter [02200, 05004], lr: 0.020000, loss: 6.0577
2022-03-01 08:32:41 - train: epoch 0001, iter [02300, 05004], lr: 0.020000, loss: 6.0936
2022-03-01 08:33:15 - train: epoch 0001, iter [02400, 05004], lr: 0.020000, loss: 6.1098
2022-03-01 08:33:49 - train: epoch 0001, iter [02500, 05004], lr: 0.020000, loss: 5.9659
2022-03-01 08:34:23 - train: epoch 0001, iter [02600, 05004], lr: 0.020000, loss: 6.0003
2022-03-01 08:34:56 - train: epoch 0001, iter [02700, 05004], lr: 0.020000, loss: 6.1527
2022-03-01 08:35:29 - train: epoch 0001, iter [02800, 05004], lr: 0.020000, loss: 5.9517
2022-03-01 08:36:03 - train: epoch 0001, iter [02900, 05004], lr: 0.020000, loss: 5.9113
2022-03-01 08:36:37 - train: epoch 0001, iter [03000, 05004], lr: 0.020000, loss: 5.9362
2022-03-01 08:37:12 - train: epoch 0001, iter [03100, 05004], lr: 0.020000, loss: 6.0017
2022-03-01 08:37:46 - train: epoch 0001, iter [03200, 05004], lr: 0.020000, loss: 6.0478
2022-03-01 08:38:20 - train: epoch 0001, iter [03300, 05004], lr: 0.020000, loss: 5.8366
2022-03-01 08:38:53 - train: epoch 0001, iter [03400, 05004], lr: 0.020000, loss: 5.7318
2022-03-01 08:39:27 - train: epoch 0001, iter [03500, 05004], lr: 0.020000, loss: 5.6933
2022-03-01 08:40:02 - train: epoch 0001, iter [03600, 05004], lr: 0.020000, loss: 5.8224
2022-03-01 08:40:35 - train: epoch 0001, iter [03700, 05004], lr: 0.020000, loss: 5.8342
2022-03-01 08:41:09 - train: epoch 0001, iter [03800, 05004], lr: 0.020000, loss: 5.6723
2022-03-01 08:41:42 - train: epoch 0001, iter [03900, 05004], lr: 0.020000, loss: 5.6269
2022-03-01 08:42:15 - train: epoch 0001, iter [04000, 05004], lr: 0.020000, loss: 5.7436
2022-03-01 08:42:48 - train: epoch 0001, iter [04100, 05004], lr: 0.020000, loss: 5.7003
2022-03-01 08:43:23 - train: epoch 0001, iter [04200, 05004], lr: 0.020000, loss: 5.5249
2022-03-01 08:43:56 - train: epoch 0001, iter [04300, 05004], lr: 0.020000, loss: 5.5689
2022-03-01 08:44:30 - train: epoch 0001, iter [04400, 05004], lr: 0.020000, loss: 5.3321
2022-03-01 08:45:04 - train: epoch 0001, iter [04500, 05004], lr: 0.020000, loss: 5.5531
2022-03-01 08:45:38 - train: epoch 0001, iter [04600, 05004], lr: 0.020000, loss: 5.7597
2022-03-01 08:46:12 - train: epoch 0001, iter [04700, 05004], lr: 0.020000, loss: 5.3870
2022-03-01 08:46:45 - train: epoch 0001, iter [04800, 05004], lr: 0.020000, loss: 5.5570
2022-03-01 08:47:20 - train: epoch 0001, iter [04900, 05004], lr: 0.020000, loss: 5.4101
2022-03-01 08:47:52 - train: epoch 0001, iter [05000, 05004], lr: 0.020000, loss: 5.3806
2022-03-01 08:47:53 - train: epoch 001, train_loss: 6.1214
2022-03-01 08:49:06 - eval: epoch: 001, acc1: 9.244%, acc5: 24.364%, test_loss: 5.2085, per_image_load_time: 2.308ms, per_image_inference_time: 0.505ms
2022-03-01 08:49:07 - until epoch: 001, best_acc1: 9.244%
2022-03-01 08:49:07 - epoch 002 lr: 0.04000000000000001
2022-03-01 08:49:45 - train: epoch 0002, iter [00100, 05004], lr: 0.040000, loss: 5.7204
2022-03-01 08:50:20 - train: epoch 0002, iter [00200, 05004], lr: 0.040000, loss: 5.6414
2022-03-01 08:50:53 - train: epoch 0002, iter [00300, 05004], lr: 0.040000, loss: 5.6874
2022-03-01 08:51:27 - train: epoch 0002, iter [00400, 05004], lr: 0.040000, loss: 5.5699
2022-03-01 08:52:01 - train: epoch 0002, iter [00500, 05004], lr: 0.040000, loss: 5.3042
2022-03-01 08:52:35 - train: epoch 0002, iter [00600, 05004], lr: 0.040000, loss: 5.3866
2022-03-01 08:53:09 - train: epoch 0002, iter [00700, 05004], lr: 0.040000, loss: 5.4356
2022-03-01 08:53:44 - train: epoch 0002, iter [00800, 05004], lr: 0.040000, loss: 5.1564
2022-03-01 08:54:17 - train: epoch 0002, iter [00900, 05004], lr: 0.040000, loss: 5.0915
2022-03-01 08:54:50 - train: epoch 0002, iter [01000, 05004], lr: 0.040000, loss: 5.4483
2022-03-01 08:55:24 - train: epoch 0002, iter [01100, 05004], lr: 0.040000, loss: 5.1665
2022-03-01 08:55:58 - train: epoch 0002, iter [01200, 05004], lr: 0.040000, loss: 5.1122
2022-03-01 08:56:32 - train: epoch 0002, iter [01300, 05004], lr: 0.040000, loss: 5.1145
2022-03-01 08:57:06 - train: epoch 0002, iter [01400, 05004], lr: 0.040000, loss: 5.0862
2022-03-01 08:57:40 - train: epoch 0002, iter [01500, 05004], lr: 0.040000, loss: 5.0733
2022-03-01 08:58:15 - train: epoch 0002, iter [01600, 05004], lr: 0.040000, loss: 4.9694
2022-03-01 08:58:48 - train: epoch 0002, iter [01700, 05004], lr: 0.040000, loss: 5.3095
2022-03-01 08:59:22 - train: epoch 0002, iter [01800, 05004], lr: 0.040000, loss: 4.9869
2022-03-01 08:59:56 - train: epoch 0002, iter [01900, 05004], lr: 0.040000, loss: 5.0788
2022-03-01 09:00:31 - train: epoch 0002, iter [02000, 05004], lr: 0.040000, loss: 5.0648
2022-03-01 09:01:03 - train: epoch 0002, iter [02100, 05004], lr: 0.040000, loss: 5.0188
2022-03-01 09:01:37 - train: epoch 0002, iter [02200, 05004], lr: 0.040000, loss: 4.7521
2022-03-01 09:02:11 - train: epoch 0002, iter [02300, 05004], lr: 0.040000, loss: 4.9819
2022-03-01 09:02:45 - train: epoch 0002, iter [02400, 05004], lr: 0.040000, loss: 4.7912
2022-03-01 09:03:18 - train: epoch 0002, iter [02500, 05004], lr: 0.040000, loss: 4.7185
2022-03-01 09:03:53 - train: epoch 0002, iter [02600, 05004], lr: 0.040000, loss: 4.8399
2022-03-01 09:04:26 - train: epoch 0002, iter [02700, 05004], lr: 0.040000, loss: 4.8181
2022-03-01 09:05:00 - train: epoch 0002, iter [02800, 05004], lr: 0.040000, loss: 4.8183
2022-03-01 09:05:34 - train: epoch 0002, iter [02900, 05004], lr: 0.040000, loss: 4.8351
2022-03-01 09:06:08 - train: epoch 0002, iter [03000, 05004], lr: 0.040000, loss: 4.7283
2022-03-01 09:06:42 - train: epoch 0002, iter [03100, 05004], lr: 0.040000, loss: 4.6257
2022-03-01 09:07:16 - train: epoch 0002, iter [03200, 05004], lr: 0.040000, loss: 4.6851
2022-03-01 09:07:49 - train: epoch 0002, iter [03300, 05004], lr: 0.040000, loss: 4.7100
2022-03-01 09:08:23 - train: epoch 0002, iter [03400, 05004], lr: 0.040000, loss: 4.7366
2022-03-01 09:08:57 - train: epoch 0002, iter [03500, 05004], lr: 0.040000, loss: 4.6456
2022-03-01 09:09:31 - train: epoch 0002, iter [03600, 05004], lr: 0.040000, loss: 4.5685
2022-03-01 09:10:05 - train: epoch 0002, iter [03700, 05004], lr: 0.040000, loss: 4.7563
2022-03-01 09:10:39 - train: epoch 0002, iter [03800, 05004], lr: 0.040000, loss: 4.4715
2022-03-01 09:11:13 - train: epoch 0002, iter [03900, 05004], lr: 0.040000, loss: 4.6800
2022-03-01 09:11:48 - train: epoch 0002, iter [04000, 05004], lr: 0.040000, loss: 4.3820
2022-03-01 09:12:21 - train: epoch 0002, iter [04100, 05004], lr: 0.040000, loss: 4.6743
2022-03-01 09:12:56 - train: epoch 0002, iter [04200, 05004], lr: 0.040000, loss: 4.3948
2022-03-01 09:13:29 - train: epoch 0002, iter [04300, 05004], lr: 0.040000, loss: 4.4094
2022-03-01 09:14:02 - train: epoch 0002, iter [04400, 05004], lr: 0.040000, loss: 4.3132
2022-03-01 09:14:35 - train: epoch 0002, iter [04500, 05004], lr: 0.040000, loss: 4.3676
2022-03-01 09:15:10 - train: epoch 0002, iter [04600, 05004], lr: 0.040000, loss: 4.3717
2022-03-01 09:15:43 - train: epoch 0002, iter [04700, 05004], lr: 0.040000, loss: 4.3381
2022-03-01 09:16:17 - train: epoch 0002, iter [04800, 05004], lr: 0.040000, loss: 4.2268
2022-03-01 09:16:51 - train: epoch 0002, iter [04900, 05004], lr: 0.040000, loss: 4.3678
2022-03-01 09:17:24 - train: epoch 0002, iter [05000, 05004], lr: 0.040000, loss: 4.5077
2022-03-01 09:17:25 - train: epoch 002, train_loss: 4.8949
2022-03-01 09:18:39 - eval: epoch: 002, acc1: 22.402%, acc5: 46.464%, test_loss: 3.7638, per_image_load_time: 2.364ms, per_image_inference_time: 0.531ms
2022-03-01 09:18:40 - until epoch: 002, best_acc1: 22.402%
2022-03-01 09:18:40 - epoch 003 lr: 0.06
2022-03-01 09:19:18 - train: epoch 0003, iter [00100, 05004], lr: 0.060000, loss: 4.6527
2022-03-01 09:19:53 - train: epoch 0003, iter [00200, 05004], lr: 0.060000, loss: 4.4575
2022-03-01 09:20:26 - train: epoch 0003, iter [00300, 05004], lr: 0.060000, loss: 4.4414
2022-03-01 09:20:59 - train: epoch 0003, iter [00400, 05004], lr: 0.060000, loss: 4.4651
2022-03-01 09:21:33 - train: epoch 0003, iter [00500, 05004], lr: 0.060000, loss: 4.6732
2022-03-01 09:22:06 - train: epoch 0003, iter [00600, 05004], lr: 0.060000, loss: 4.2641
2022-03-01 09:22:41 - train: epoch 0003, iter [00700, 05004], lr: 0.060000, loss: 4.6947
2022-03-01 09:23:15 - train: epoch 0003, iter [00800, 05004], lr: 0.060000, loss: 4.4756
2022-03-01 09:23:49 - train: epoch 0003, iter [00900, 05004], lr: 0.060000, loss: 4.2143
2022-03-01 09:24:22 - train: epoch 0003, iter [01000, 05004], lr: 0.060000, loss: 4.3173
2022-03-01 09:24:57 - train: epoch 0003, iter [01100, 05004], lr: 0.060000, loss: 4.3202
2022-03-01 09:25:31 - train: epoch 0003, iter [01200, 05004], lr: 0.060000, loss: 4.2140
2022-03-01 09:26:05 - train: epoch 0003, iter [01300, 05004], lr: 0.060000, loss: 4.2662
2022-03-01 09:26:38 - train: epoch 0003, iter [01400, 05004], lr: 0.060000, loss: 4.0898
2022-03-01 09:27:11 - train: epoch 0003, iter [01500, 05004], lr: 0.060000, loss: 4.6137
2022-03-01 09:27:45 - train: epoch 0003, iter [01600, 05004], lr: 0.060000, loss: 4.1719
2022-03-01 09:28:18 - train: epoch 0003, iter [01700, 05004], lr: 0.060000, loss: 4.1354
2022-03-01 09:28:52 - train: epoch 0003, iter [01800, 05004], lr: 0.060000, loss: 4.1572
2022-03-01 09:29:26 - train: epoch 0003, iter [01900, 05004], lr: 0.060000, loss: 4.2227
2022-03-01 09:30:00 - train: epoch 0003, iter [02000, 05004], lr: 0.060000, loss: 4.4767
2022-03-01 09:30:34 - train: epoch 0003, iter [02100, 05004], lr: 0.060000, loss: 4.2533
2022-03-01 09:31:08 - train: epoch 0003, iter [02200, 05004], lr: 0.060000, loss: 4.4823
2022-03-01 09:31:43 - train: epoch 0003, iter [02300, 05004], lr: 0.060000, loss: 4.1690
2022-03-01 09:32:17 - train: epoch 0003, iter [02400, 05004], lr: 0.060000, loss: 4.1440
2022-03-01 09:32:51 - train: epoch 0003, iter [02500, 05004], lr: 0.060000, loss: 4.2051
2022-03-01 09:33:24 - train: epoch 0003, iter [02600, 05004], lr: 0.060000, loss: 4.2162
2022-03-01 09:33:57 - train: epoch 0003, iter [02700, 05004], lr: 0.060000, loss: 4.3627
2022-03-01 09:34:31 - train: epoch 0003, iter [02800, 05004], lr: 0.060000, loss: 4.0215
2022-03-01 09:35:06 - train: epoch 0003, iter [02900, 05004], lr: 0.060000, loss: 3.9954
2022-03-01 09:35:39 - train: epoch 0003, iter [03000, 05004], lr: 0.060000, loss: 4.3062
2022-03-01 09:36:14 - train: epoch 0003, iter [03100, 05004], lr: 0.060000, loss: 4.1644
2022-03-01 09:36:48 - train: epoch 0003, iter [03200, 05004], lr: 0.060000, loss: 4.2353
2022-03-01 09:37:22 - train: epoch 0003, iter [03300, 05004], lr: 0.060000, loss: 4.0039
2022-03-01 09:37:56 - train: epoch 0003, iter [03400, 05004], lr: 0.060000, loss: 4.1007
2022-03-01 09:38:30 - train: epoch 0003, iter [03500, 05004], lr: 0.060000, loss: 3.8933
2022-03-01 09:39:05 - train: epoch 0003, iter [03600, 05004], lr: 0.060000, loss: 3.8755
2022-03-01 09:39:38 - train: epoch 0003, iter [03700, 05004], lr: 0.060000, loss: 3.9635
2022-03-01 09:40:11 - train: epoch 0003, iter [03800, 05004], lr: 0.060000, loss: 4.1923
2022-03-01 09:40:45 - train: epoch 0003, iter [03900, 05004], lr: 0.060000, loss: 4.1216
2022-03-01 09:41:19 - train: epoch 0003, iter [04000, 05004], lr: 0.060000, loss: 3.9326
2022-03-01 09:41:53 - train: epoch 0003, iter [04100, 05004], lr: 0.060000, loss: 3.8929
2022-03-01 09:42:27 - train: epoch 0003, iter [04200, 05004], lr: 0.060000, loss: 3.9313
2022-03-01 09:43:01 - train: epoch 0003, iter [04300, 05004], lr: 0.060000, loss: 3.6293
2022-03-01 09:43:35 - train: epoch 0003, iter [04400, 05004], lr: 0.060000, loss: 3.7404
2022-03-01 09:44:08 - train: epoch 0003, iter [04500, 05004], lr: 0.060000, loss: 3.8918
2022-03-01 09:44:43 - train: epoch 0003, iter [04600, 05004], lr: 0.060000, loss: 3.7203
2022-03-01 09:45:17 - train: epoch 0003, iter [04700, 05004], lr: 0.060000, loss: 3.6219
2022-03-01 09:45:51 - train: epoch 0003, iter [04800, 05004], lr: 0.060000, loss: 4.0890
2022-03-01 09:46:24 - train: epoch 0003, iter [04900, 05004], lr: 0.060000, loss: 3.9628
2022-03-01 09:46:56 - train: epoch 0003, iter [05000, 05004], lr: 0.060000, loss: 3.8189
2022-03-01 09:46:57 - train: epoch 003, train_loss: 4.1395
2022-03-01 09:48:12 - eval: epoch: 003, acc1: 33.154%, acc5: 59.724%, test_loss: 3.1048, per_image_load_time: 1.637ms, per_image_inference_time: 0.529ms
2022-03-01 09:48:13 - until epoch: 003, best_acc1: 33.154%
2022-03-01 09:48:13 - epoch 004 lr: 0.08000000000000002
2022-03-01 09:48:51 - train: epoch 0004, iter [00100, 05004], lr: 0.080000, loss: 4.0100
2022-03-01 09:49:25 - train: epoch 0004, iter [00200, 05004], lr: 0.080000, loss: 3.9752
2022-03-01 09:49:59 - train: epoch 0004, iter [00300, 05004], lr: 0.080000, loss: 3.9344
2022-03-01 09:50:33 - train: epoch 0004, iter [00400, 05004], lr: 0.080000, loss: 3.8886
2022-03-01 09:51:07 - train: epoch 0004, iter [00500, 05004], lr: 0.080000, loss: 3.6976
2022-03-01 09:51:41 - train: epoch 0004, iter [00600, 05004], lr: 0.080000, loss: 4.2566
2022-03-01 09:52:14 - train: epoch 0004, iter [00700, 05004], lr: 0.080000, loss: 4.0113
2022-03-01 09:52:47 - train: epoch 0004, iter [00800, 05004], lr: 0.080000, loss: 3.6948
2022-03-01 09:53:21 - train: epoch 0004, iter [00900, 05004], lr: 0.080000, loss: 3.7037
2022-03-01 09:53:54 - train: epoch 0004, iter [01000, 05004], lr: 0.080000, loss: 3.8470
2022-03-01 09:54:29 - train: epoch 0004, iter [01100, 05004], lr: 0.080000, loss: 3.8714
2022-03-01 09:55:04 - train: epoch 0004, iter [01200, 05004], lr: 0.080000, loss: 3.6451
2022-03-01 09:55:37 - train: epoch 0004, iter [01300, 05004], lr: 0.080000, loss: 3.5463
2022-03-01 09:56:11 - train: epoch 0004, iter [01400, 05004], lr: 0.080000, loss: 3.8658
2022-03-01 09:56:45 - train: epoch 0004, iter [01500, 05004], lr: 0.080000, loss: 3.7651
2022-03-01 09:57:20 - train: epoch 0004, iter [01600, 05004], lr: 0.080000, loss: 3.7449
2022-03-01 09:57:53 - train: epoch 0004, iter [01700, 05004], lr: 0.080000, loss: 3.7670
2022-03-01 09:58:28 - train: epoch 0004, iter [01800, 05004], lr: 0.080000, loss: 4.1250
2022-03-01 09:59:01 - train: epoch 0004, iter [01900, 05004], lr: 0.080000, loss: 3.9896
2022-03-01 09:59:36 - train: epoch 0004, iter [02000, 05004], lr: 0.080000, loss: 3.7806
2022-03-01 10:00:09 - train: epoch 0004, iter [02100, 05004], lr: 0.080000, loss: 3.8623
2022-03-01 10:00:43 - train: epoch 0004, iter [02200, 05004], lr: 0.080000, loss: 3.6564
2022-03-01 10:01:17 - train: epoch 0004, iter [02300, 05004], lr: 0.080000, loss: 3.7092
2022-03-01 10:01:51 - train: epoch 0004, iter [02400, 05004], lr: 0.080000, loss: 3.4449
2022-03-01 10:02:25 - train: epoch 0004, iter [02500, 05004], lr: 0.080000, loss: 3.6837
2022-03-01 10:02:59 - train: epoch 0004, iter [02600, 05004], lr: 0.080000, loss: 3.9587
2022-03-01 10:03:33 - train: epoch 0004, iter [02700, 05004], lr: 0.080000, loss: 3.5553
2022-03-01 10:04:07 - train: epoch 0004, iter [02800, 05004], lr: 0.080000, loss: 3.5209
2022-03-01 10:04:41 - train: epoch 0004, iter [02900, 05004], lr: 0.080000, loss: 3.5767
2022-03-01 10:05:15 - train: epoch 0004, iter [03000, 05004], lr: 0.080000, loss: 3.7146
2022-03-01 10:05:48 - train: epoch 0004, iter [03100, 05004], lr: 0.080000, loss: 3.7576
2022-03-01 10:06:21 - train: epoch 0004, iter [03200, 05004], lr: 0.080000, loss: 3.5377
2022-03-01 10:06:55 - train: epoch 0004, iter [03300, 05004], lr: 0.080000, loss: 3.8775
2022-03-01 10:07:30 - train: epoch 0004, iter [03400, 05004], lr: 0.080000, loss: 3.7184
2022-03-01 10:08:02 - train: epoch 0004, iter [03500, 05004], lr: 0.080000, loss: 3.6122
2022-03-01 10:08:37 - train: epoch 0004, iter [03600, 05004], lr: 0.080000, loss: 3.5299
2022-03-01 10:09:11 - train: epoch 0004, iter [03700, 05004], lr: 0.080000, loss: 3.6166
2022-03-01 10:09:44 - train: epoch 0004, iter [03800, 05004], lr: 0.080000, loss: 3.5430
2022-03-01 10:10:19 - train: epoch 0004, iter [03900, 05004], lr: 0.080000, loss: 3.6888
2022-03-01 10:10:53 - train: epoch 0004, iter [04000, 05004], lr: 0.080000, loss: 3.3578
2022-03-01 10:11:26 - train: epoch 0004, iter [04100, 05004], lr: 0.080000, loss: 3.5783
2022-03-01 10:12:00 - train: epoch 0004, iter [04200, 05004], lr: 0.080000, loss: 3.3045
2022-03-01 10:12:33 - train: epoch 0004, iter [04300, 05004], lr: 0.080000, loss: 3.3779
2022-03-01 10:13:06 - train: epoch 0004, iter [04400, 05004], lr: 0.080000, loss: 3.5055
2022-03-01 10:13:40 - train: epoch 0004, iter [04500, 05004], lr: 0.080000, loss: 3.2514
2022-03-01 10:14:14 - train: epoch 0004, iter [04600, 05004], lr: 0.080000, loss: 3.4905
2022-03-01 10:14:48 - train: epoch 0004, iter [04700, 05004], lr: 0.080000, loss: 3.4982
2022-03-01 10:15:22 - train: epoch 0004, iter [04800, 05004], lr: 0.080000, loss: 3.3139
2022-03-01 10:15:56 - train: epoch 0004, iter [04900, 05004], lr: 0.080000, loss: 3.6261
2022-03-01 10:16:28 - train: epoch 0004, iter [05000, 05004], lr: 0.080000, loss: 3.6673
2022-03-01 10:16:29 - train: epoch 004, train_loss: 3.7415
2022-03-01 10:17:43 - eval: epoch: 004, acc1: 32.678%, acc5: 58.746%, test_loss: 3.1491, per_image_load_time: 2.285ms, per_image_inference_time: 0.524ms
2022-03-01 10:17:44 - until epoch: 004, best_acc1: 33.154%
2022-03-01 10:17:44 - epoch 005 lr: 0.1
2022-03-01 10:18:22 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.9004
2022-03-01 10:18:55 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.8582
2022-03-01 10:19:28 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.8045
2022-03-01 10:20:02 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.5084
2022-03-01 10:20:35 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 3.5130
2022-03-01 10:21:10 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.6607
2022-03-01 10:21:43 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.5031
2022-03-01 10:22:18 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.8629
2022-03-01 10:22:51 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.5086
2022-03-01 10:23:25 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.8068
2022-03-01 10:23:59 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.7910
2022-03-01 10:24:34 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.6983
2022-03-01 10:25:07 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.5570
2022-03-01 10:25:40 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.7779
2022-03-01 10:26:14 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.4561
2022-03-01 10:26:48 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.2649
2022-03-01 10:27:21 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.4129
2022-03-01 10:27:56 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.4425
2022-03-01 10:28:29 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 3.5267
2022-03-01 10:29:04 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.5668
2022-03-01 10:29:38 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 3.3566
2022-03-01 10:30:12 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.5584
2022-03-01 10:30:47 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.2354
2022-03-01 10:31:21 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.6974
2022-03-01 10:31:54 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.6124
2022-03-01 10:32:27 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.7243
2022-03-01 10:33:01 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.5221
2022-03-01 10:33:35 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.5576
2022-03-01 10:34:09 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 3.5180
2022-03-01 10:34:43 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.5720
2022-03-01 10:35:17 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.7118
2022-03-01 10:35:51 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.7169
2022-03-01 10:36:25 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 3.4702
2022-03-01 10:37:00 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.4134
2022-03-01 10:37:34 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.5990
2022-03-01 10:38:07 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.5912
2022-03-01 10:38:40 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.5856
2022-03-01 10:39:14 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.3965
2022-03-01 10:39:48 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.6908
2022-03-01 10:40:21 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.2938
2022-03-01 10:40:56 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.3518
2022-03-01 10:41:29 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.4737
2022-03-01 10:42:04 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.3294
2022-03-01 10:42:38 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.4738
2022-03-01 10:43:13 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.4546
2022-03-01 10:43:46 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.5224
2022-03-01 10:44:20 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 3.3723
2022-03-01 10:44:53 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 3.2565
2022-03-01 10:45:26 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.6378
2022-03-01 10:45:59 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.3023
2022-03-01 10:46:00 - train: epoch 005, train_loss: 3.5477
2022-03-01 10:47:14 - eval: epoch: 005, acc1: 36.016%, acc5: 63.390%, test_loss: 2.8942, per_image_load_time: 1.224ms, per_image_inference_time: 0.501ms
2022-03-01 10:47:15 - until epoch: 005, best_acc1: 36.016%
2022-03-01 10:47:15 - epoch 006 lr: 0.1
2022-03-01 10:47:53 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.4856
2022-03-01 10:48:27 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.3911
2022-03-01 10:49:01 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.2932
2022-03-01 10:49:35 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.4963
2022-03-01 10:50:09 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.2917
2022-03-01 10:50:43 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.5009
2022-03-01 10:51:16 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.4543
2022-03-01 10:51:49 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.2764
2022-03-01 10:52:24 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.3891
2022-03-01 10:52:57 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.2862
2022-03-01 10:53:31 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.2989
2022-03-01 10:54:05 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.2577
2022-03-01 10:54:39 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.6646
2022-03-01 10:55:14 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.5251
2022-03-01 10:55:48 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.5934
2022-03-01 10:56:22 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 3.3935
2022-03-01 10:56:56 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.5783
2022-03-01 10:57:30 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.7604
2022-03-01 10:58:03 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 3.3153
2022-03-01 10:58:38 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.6337
2022-03-01 10:59:12 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.2326
2022-03-01 10:59:46 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 3.2235
2022-03-01 11:00:20 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 3.4015
2022-03-01 11:00:54 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.4781
2022-03-01 11:01:28 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.6401
2022-03-01 11:02:02 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 3.2325
2022-03-01 11:02:36 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.3650
2022-03-01 11:03:10 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.1434
2022-03-01 11:03:44 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.3795
2022-03-01 11:04:17 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 3.4490
2022-03-01 11:04:50 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 3.1288
2022-03-01 11:05:24 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 3.4492
2022-03-01 11:05:58 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 3.0883
2022-03-01 11:06:32 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.3172
2022-03-01 11:07:06 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.4044
2022-03-01 11:07:40 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.1535
2022-03-01 11:08:14 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.2088
2022-03-01 11:08:49 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 3.2057
2022-03-01 11:09:23 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 3.3567
2022-03-01 11:09:57 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.5717
2022-03-01 11:10:30 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 3.3922
2022-03-01 11:11:03 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.2858
2022-03-01 11:11:37 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 3.1559
2022-03-01 11:12:12 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.3871
2022-03-01 11:12:45 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.2045
2022-03-01 11:13:19 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.2888
2022-03-01 11:13:53 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.4151
2022-03-01 11:14:27 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.4095
2022-03-01 11:15:01 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.3039
2022-03-01 11:15:34 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 3.0432
2022-03-01 11:15:35 - train: epoch 006, train_loss: 3.3642
2022-03-01 11:16:49 - eval: epoch: 006, acc1: 40.770%, acc5: 68.076%, test_loss: 2.6573, per_image_load_time: 1.985ms, per_image_inference_time: 0.522ms
2022-03-01 11:16:50 - until epoch: 006, best_acc1: 40.770%
2022-03-01 11:16:50 - epoch 007 lr: 0.09999351124856874
2022-03-01 11:17:26 - train: epoch 0007, iter [00100, 05004], lr: 0.099994, loss: 3.1096
2022-03-01 11:18:00 - train: epoch 0007, iter [00200, 05004], lr: 0.099994, loss: 3.5207
2022-03-01 11:18:34 - train: epoch 0007, iter [00300, 05004], lr: 0.099994, loss: 3.5799
2022-03-01 11:19:08 - train: epoch 0007, iter [00400, 05004], lr: 0.099994, loss: 3.3251
2022-03-01 11:19:42 - train: epoch 0007, iter [00500, 05004], lr: 0.099994, loss: 3.2393
2022-03-01 11:20:16 - train: epoch 0007, iter [00600, 05004], lr: 0.099994, loss: 3.4069
2022-03-01 11:20:49 - train: epoch 0007, iter [00700, 05004], lr: 0.099994, loss: 3.4840
2022-03-01 11:21:23 - train: epoch 0007, iter [00800, 05004], lr: 0.099994, loss: 3.3221
2022-03-01 11:21:57 - train: epoch 0007, iter [00900, 05004], lr: 0.099994, loss: 3.2612
2022-03-01 11:22:32 - train: epoch 0007, iter [01000, 05004], lr: 0.099994, loss: 3.3413
2022-03-01 11:23:06 - train: epoch 0007, iter [01100, 05004], lr: 0.099994, loss: 3.1256
2022-03-01 11:23:39 - train: epoch 0007, iter [01200, 05004], lr: 0.099994, loss: 3.2363
2022-03-01 11:24:13 - train: epoch 0007, iter [01300, 05004], lr: 0.099994, loss: 3.0273
2022-03-01 11:24:47 - train: epoch 0007, iter [01400, 05004], lr: 0.099994, loss: 3.3789
2022-03-01 11:25:21 - train: epoch 0007, iter [01500, 05004], lr: 0.099994, loss: 3.3865
2022-03-01 11:25:55 - train: epoch 0007, iter [01600, 05004], lr: 0.099994, loss: 3.3133
2022-03-01 11:26:30 - train: epoch 0007, iter [01700, 05004], lr: 0.099994, loss: 3.2823
2022-03-01 11:27:04 - train: epoch 0007, iter [01800, 05004], lr: 0.099994, loss: 3.0344
2022-03-01 11:27:39 - train: epoch 0007, iter [01900, 05004], lr: 0.099994, loss: 3.2887
2022-03-01 11:28:12 - train: epoch 0007, iter [02000, 05004], lr: 0.099994, loss: 3.0045
2022-03-01 11:28:47 - train: epoch 0007, iter [02100, 05004], lr: 0.099994, loss: 3.3574
2022-03-01 11:29:20 - train: epoch 0007, iter [02200, 05004], lr: 0.099994, loss: 3.2505
2022-03-01 11:29:54 - train: epoch 0007, iter [02300, 05004], lr: 0.099994, loss: 3.2864
2022-03-01 11:30:27 - train: epoch 0007, iter [02400, 05004], lr: 0.099994, loss: 3.1911
2022-03-01 11:31:02 - train: epoch 0007, iter [02500, 05004], lr: 0.099994, loss: 3.2301
2022-03-01 11:31:35 - train: epoch 0007, iter [02600, 05004], lr: 0.099994, loss: 3.0769
2022-03-01 11:32:10 - train: epoch 0007, iter [02700, 05004], lr: 0.099994, loss: 3.0747
2022-03-01 11:32:44 - train: epoch 0007, iter [02800, 05004], lr: 0.099994, loss: 3.1476
2022-03-01 11:33:18 - train: epoch 0007, iter [02900, 05004], lr: 0.099994, loss: 3.0737
2022-03-01 11:33:52 - train: epoch 0007, iter [03000, 05004], lr: 0.099994, loss: 3.2269
2022-03-01 11:34:27 - train: epoch 0007, iter [03100, 05004], lr: 0.099994, loss: 3.2669
2022-03-01 11:35:01 - train: epoch 0007, iter [03200, 05004], lr: 0.099994, loss: 3.0122
2022-03-01 11:35:35 - train: epoch 0007, iter [03300, 05004], lr: 0.099994, loss: 3.3608
2022-03-01 11:36:09 - train: epoch 0007, iter [03400, 05004], lr: 0.099994, loss: 3.2413
2022-03-01 11:36:42 - train: epoch 0007, iter [03500, 05004], lr: 0.099994, loss: 3.4154
2022-03-01 11:37:15 - train: epoch 0007, iter [03600, 05004], lr: 0.099994, loss: 2.9962
2022-03-01 11:37:49 - train: epoch 0007, iter [03700, 05004], lr: 0.099994, loss: 3.1753
2022-03-01 11:38:24 - train: epoch 0007, iter [03800, 05004], lr: 0.099994, loss: 3.2831
2022-03-01 11:38:57 - train: epoch 0007, iter [03900, 05004], lr: 0.099994, loss: 2.9919
2022-03-01 11:39:32 - train: epoch 0007, iter [04000, 05004], lr: 0.099994, loss: 3.2515
2022-03-01 11:40:05 - train: epoch 0007, iter [04100, 05004], lr: 0.099994, loss: 3.1900
2022-03-01 11:40:40 - train: epoch 0007, iter [04200, 05004], lr: 0.099994, loss: 2.9909
2022-03-01 11:41:14 - train: epoch 0007, iter [04300, 05004], lr: 0.099994, loss: 3.2515
2022-03-01 11:41:49 - train: epoch 0007, iter [04400, 05004], lr: 0.099994, loss: 3.0674
2022-03-01 11:42:22 - train: epoch 0007, iter [04500, 05004], lr: 0.099994, loss: 3.4080
2022-03-01 11:42:56 - train: epoch 0007, iter [04600, 05004], lr: 0.099994, loss: 3.2337
2022-03-01 11:43:30 - train: epoch 0007, iter [04700, 05004], lr: 0.099994, loss: 3.3389
2022-03-01 11:44:04 - train: epoch 0007, iter [04800, 05004], lr: 0.099994, loss: 3.4256
2022-03-01 11:44:38 - train: epoch 0007, iter [04900, 05004], lr: 0.099994, loss: 3.2819
2022-03-01 11:45:11 - train: epoch 0007, iter [05000, 05004], lr: 0.099994, loss: 3.0999
2022-03-01 11:45:12 - train: epoch 007, train_loss: 3.2515
2022-03-01 11:46:25 - eval: epoch: 007, acc1: 40.322%, acc5: 67.578%, test_loss: 2.6569, per_image_load_time: 1.649ms, per_image_inference_time: 0.544ms
2022-03-01 11:46:26 - until epoch: 007, best_acc1: 40.770%
2022-03-01 11:46:26 - epoch 008 lr: 0.09997404667843075
2022-03-01 11:47:04 - train: epoch 0008, iter [00100, 05004], lr: 0.099974, loss: 3.0598
2022-03-01 11:47:38 - train: epoch 0008, iter [00200, 05004], lr: 0.099974, loss: 3.4474
2022-03-01 11:48:12 - train: epoch 0008, iter [00300, 05004], lr: 0.099974, loss: 3.1616
2022-03-01 11:48:45 - train: epoch 0008, iter [00400, 05004], lr: 0.099974, loss: 3.1021
2022-03-01 11:49:18 - train: epoch 0008, iter [00500, 05004], lr: 0.099974, loss: 3.1441
2022-03-01 11:49:51 - train: epoch 0008, iter [00600, 05004], lr: 0.099974, loss: 2.9215
2022-03-01 11:50:25 - train: epoch 0008, iter [00700, 05004], lr: 0.099974, loss: 3.6574
2022-03-01 11:51:00 - train: epoch 0008, iter [00800, 05004], lr: 0.099974, loss: 3.2664
2022-03-01 11:51:33 - train: epoch 0008, iter [00900, 05004], lr: 0.099974, loss: 3.1523
2022-03-01 11:52:07 - train: epoch 0008, iter [01000, 05004], lr: 0.099974, loss: 3.4096
2022-03-01 11:52:41 - train: epoch 0008, iter [01100, 05004], lr: 0.099974, loss: 3.0696
2022-03-01 11:53:15 - train: epoch 0008, iter [01200, 05004], lr: 0.099974, loss: 3.0921
2022-03-01 11:53:49 - train: epoch 0008, iter [01300, 05004], lr: 0.099974, loss: 3.3064
2022-03-01 11:54:23 - train: epoch 0008, iter [01400, 05004], lr: 0.099974, loss: 3.2398
2022-03-01 11:54:58 - train: epoch 0008, iter [01500, 05004], lr: 0.099974, loss: 3.2876
2022-03-01 11:55:31 - train: epoch 0008, iter [01600, 05004], lr: 0.099974, loss: 3.1168
2022-03-01 11:56:05 - train: epoch 0008, iter [01700, 05004], lr: 0.099974, loss: 3.1943
2022-03-01 11:56:38 - train: epoch 0008, iter [01800, 05004], lr: 0.099974, loss: 3.4904
2022-03-01 11:57:12 - train: epoch 0008, iter [01900, 05004], lr: 0.099974, loss: 3.0758
2022-03-01 11:57:46 - train: epoch 0008, iter [02000, 05004], lr: 0.099974, loss: 3.1921
2022-03-01 11:58:21 - train: epoch 0008, iter [02100, 05004], lr: 0.099974, loss: 3.1141
2022-03-01 11:58:55 - train: epoch 0008, iter [02200, 05004], lr: 0.099974, loss: 3.1157
2022-03-01 11:59:29 - train: epoch 0008, iter [02300, 05004], lr: 0.099974, loss: 3.1717
2022-03-01 12:00:03 - train: epoch 0008, iter [02400, 05004], lr: 0.099974, loss: 3.1382
2022-03-01 12:00:37 - train: epoch 0008, iter [02500, 05004], lr: 0.099974, loss: 3.2394
2022-03-01 12:01:12 - train: epoch 0008, iter [02600, 05004], lr: 0.099974, loss: 3.3258
2022-03-01 12:01:45 - train: epoch 0008, iter [02700, 05004], lr: 0.099974, loss: 3.2823
2022-03-01 12:02:20 - train: epoch 0008, iter [02800, 05004], lr: 0.099974, loss: 3.1694
2022-03-01 12:02:51 - train: epoch 0008, iter [02900, 05004], lr: 0.099974, loss: 3.1833
2022-03-01 12:03:26 - train: epoch 0008, iter [03000, 05004], lr: 0.099974, loss: 3.3523
2022-03-01 12:04:00 - train: epoch 0008, iter [03100, 05004], lr: 0.099974, loss: 3.0263
2022-03-01 12:04:34 - train: epoch 0008, iter [03200, 05004], lr: 0.099974, loss: 3.4023
2022-03-01 12:05:08 - train: epoch 0008, iter [03300, 05004], lr: 0.099974, loss: 3.4474
2022-03-01 12:05:42 - train: epoch 0008, iter [03400, 05004], lr: 0.099974, loss: 3.4232
2022-03-01 12:06:16 - train: epoch 0008, iter [03500, 05004], lr: 0.099974, loss: 3.0727
2022-03-01 12:06:51 - train: epoch 0008, iter [03600, 05004], lr: 0.099974, loss: 3.3491
2022-03-01 12:07:25 - train: epoch 0008, iter [03700, 05004], lr: 0.099974, loss: 3.1860
2022-03-01 12:07:59 - train: epoch 0008, iter [03800, 05004], lr: 0.099974, loss: 2.9365
2022-03-01 12:08:32 - train: epoch 0008, iter [03900, 05004], lr: 0.099974, loss: 3.2986
2022-03-01 12:09:05 - train: epoch 0008, iter [04000, 05004], lr: 0.099974, loss: 3.4322
2022-03-01 12:09:38 - train: epoch 0008, iter [04100, 05004], lr: 0.099974, loss: 3.2010
2022-03-01 12:10:13 - train: epoch 0008, iter [04200, 05004], lr: 0.099974, loss: 3.1231
2022-03-01 12:10:47 - train: epoch 0008, iter [04300, 05004], lr: 0.099974, loss: 2.8954
2022-03-01 12:11:20 - train: epoch 0008, iter [04400, 05004], lr: 0.099974, loss: 3.0163
2022-03-01 12:11:54 - train: epoch 0008, iter [04500, 05004], lr: 0.099974, loss: 3.3916
2022-03-01 12:12:29 - train: epoch 0008, iter [04600, 05004], lr: 0.099974, loss: 3.2195
2022-03-01 12:13:02 - train: epoch 0008, iter [04700, 05004], lr: 0.099974, loss: 3.0895
2022-03-01 12:13:36 - train: epoch 0008, iter [04800, 05004], lr: 0.099974, loss: 3.1913
2022-03-01 12:14:10 - train: epoch 0008, iter [04900, 05004], lr: 0.099974, loss: 3.1867
2022-03-01 12:14:43 - train: epoch 0008, iter [05000, 05004], lr: 0.099974, loss: 2.9828
2022-03-01 12:14:44 - train: epoch 008, train_loss: 3.1716
2022-03-01 12:15:57 - eval: epoch: 008, acc1: 43.534%, acc5: 70.284%, test_loss: 2.5025, per_image_load_time: 2.321ms, per_image_inference_time: 0.490ms
2022-03-01 12:15:58 - until epoch: 008, best_acc1: 43.534%
2022-03-01 12:15:58 - epoch 009 lr: 0.09994161134161633
2022-03-01 12:16:37 - train: epoch 0009, iter [00100, 05004], lr: 0.099942, loss: 3.0144
2022-03-01 12:17:10 - train: epoch 0009, iter [00200, 05004], lr: 0.099942, loss: 2.8413
2022-03-01 12:17:43 - train: epoch 0009, iter [00300, 05004], lr: 0.099942, loss: 2.9695
2022-03-01 12:18:18 - train: epoch 0009, iter [00400, 05004], lr: 0.099942, loss: 3.2171
2022-03-01 12:18:52 - train: epoch 0009, iter [00500, 05004], lr: 0.099942, loss: 3.1703
2022-03-01 12:19:26 - train: epoch 0009, iter [00600, 05004], lr: 0.099942, loss: 2.9722
2022-03-01 12:20:00 - train: epoch 0009, iter [00700, 05004], lr: 0.099942, loss: 3.1495
2022-03-01 12:20:33 - train: epoch 0009, iter [00800, 05004], lr: 0.099942, loss: 3.0112
2022-03-01 12:21:09 - train: epoch 0009, iter [00900, 05004], lr: 0.099942, loss: 2.9397
2022-03-01 12:21:41 - train: epoch 0009, iter [01000, 05004], lr: 0.099942, loss: 2.9520
2022-03-01 12:22:14 - train: epoch 0009, iter [01100, 05004], lr: 0.099942, loss: 3.3653
2022-03-01 12:22:49 - train: epoch 0009, iter [01200, 05004], lr: 0.099942, loss: 3.3095
2022-03-01 12:23:22 - train: epoch 0009, iter [01300, 05004], lr: 0.099942, loss: 3.3018
2022-03-01 12:23:57 - train: epoch 0009, iter [01400, 05004], lr: 0.099942, loss: 2.9610
2022-03-01 12:24:31 - train: epoch 0009, iter [01500, 05004], lr: 0.099942, loss: 2.9850
2022-03-01 12:25:06 - train: epoch 0009, iter [01600, 05004], lr: 0.099942, loss: 3.1619
2022-03-01 12:25:40 - train: epoch 0009, iter [01700, 05004], lr: 0.099942, loss: 3.2464
2022-03-01 12:26:14 - train: epoch 0009, iter [01800, 05004], lr: 0.099942, loss: 3.1842
2022-03-01 12:26:48 - train: epoch 0009, iter [01900, 05004], lr: 0.099942, loss: 2.9360
2022-03-01 12:27:22 - train: epoch 0009, iter [02000, 05004], lr: 0.099942, loss: 2.8840
2022-03-01 12:27:57 - train: epoch 0009, iter [02100, 05004], lr: 0.099942, loss: 3.2690
2022-03-01 12:28:30 - train: epoch 0009, iter [02200, 05004], lr: 0.099942, loss: 3.3308
2022-03-01 12:29:04 - train: epoch 0009, iter [02300, 05004], lr: 0.099942, loss: 2.9437
2022-03-01 12:29:39 - train: epoch 0009, iter [02400, 05004], lr: 0.099942, loss: 3.2367
2022-03-01 12:30:12 - train: epoch 0009, iter [02500, 05004], lr: 0.099942, loss: 2.9968
2022-03-01 12:30:47 - train: epoch 0009, iter [02600, 05004], lr: 0.099942, loss: 3.1578
2022-03-01 12:31:20 - train: epoch 0009, iter [02700, 05004], lr: 0.099942, loss: 3.0493
2022-03-01 12:31:55 - train: epoch 0009, iter [02800, 05004], lr: 0.099942, loss: 3.1588
2022-03-01 12:32:29 - train: epoch 0009, iter [02900, 05004], lr: 0.099942, loss: 3.0376
2022-03-01 12:33:03 - train: epoch 0009, iter [03000, 05004], lr: 0.099942, loss: 2.9906
2022-03-01 12:33:37 - train: epoch 0009, iter [03100, 05004], lr: 0.099942, loss: 3.1627
2022-03-01 12:34:11 - train: epoch 0009, iter [03200, 05004], lr: 0.099942, loss: 3.0288
2022-03-01 12:34:44 - train: epoch 0009, iter [03300, 05004], lr: 0.099942, loss: 3.1751
2022-03-01 12:35:18 - train: epoch 0009, iter [03400, 05004], lr: 0.099942, loss: 3.3782
2022-03-01 12:35:52 - train: epoch 0009, iter [03500, 05004], lr: 0.099942, loss: 3.1128
2022-03-01 12:36:26 - train: epoch 0009, iter [03600, 05004], lr: 0.099942, loss: 3.0055
2022-03-01 12:37:00 - train: epoch 0009, iter [03700, 05004], lr: 0.099942, loss: 3.3436
2022-03-01 12:37:34 - train: epoch 0009, iter [03800, 05004], lr: 0.099942, loss: 3.3257
2022-03-01 12:38:08 - train: epoch 0009, iter [03900, 05004], lr: 0.099942, loss: 2.7769
2022-03-01 12:38:42 - train: epoch 0009, iter [04000, 05004], lr: 0.099942, loss: 3.3438
2022-03-01 12:39:16 - train: epoch 0009, iter [04100, 05004], lr: 0.099942, loss: 3.1633
2022-03-01 12:39:50 - train: epoch 0009, iter [04200, 05004], lr: 0.099942, loss: 3.1014
2022-03-01 12:40:24 - train: epoch 0009, iter [04300, 05004], lr: 0.099942, loss: 3.1922
2022-03-01 12:40:59 - train: epoch 0009, iter [04400, 05004], lr: 0.099942, loss: 3.0754
2022-03-01 12:41:31 - train: epoch 0009, iter [04500, 05004], lr: 0.099942, loss: 3.0982
2022-03-01 12:42:05 - train: epoch 0009, iter [04600, 05004], lr: 0.099942, loss: 3.0757
2022-03-01 12:42:38 - train: epoch 0009, iter [04700, 05004], lr: 0.099942, loss: 3.1087
2022-03-01 12:43:13 - train: epoch 0009, iter [04800, 05004], lr: 0.099942, loss: 3.2120
2022-03-01 12:43:48 - train: epoch 0009, iter [04900, 05004], lr: 0.099942, loss: 3.2318
2022-03-01 12:44:20 - train: epoch 0009, iter [05000, 05004], lr: 0.099942, loss: 3.1032
2022-03-01 12:44:21 - train: epoch 009, train_loss: 3.1153
2022-03-01 12:45:34 - eval: epoch: 009, acc1: 45.806%, acc5: 72.532%, test_loss: 2.3716, per_image_load_time: 1.792ms, per_image_inference_time: 0.528ms
2022-03-01 12:45:35 - until epoch: 009, best_acc1: 45.806%
2022-03-01 12:45:35 - epoch 010 lr: 0.09989621365671902
2022-03-01 12:46:14 - train: epoch 0010, iter [00100, 05004], lr: 0.099896, loss: 3.0662
2022-03-01 12:46:48 - train: epoch 0010, iter [00200, 05004], lr: 0.099896, loss: 3.1172
2022-03-01 12:47:22 - train: epoch 0010, iter [00300, 05004], lr: 0.099896, loss: 2.9099
2022-03-01 12:47:55 - train: epoch 0010, iter [00400, 05004], lr: 0.099896, loss: 3.3074
2022-03-01 12:48:28 - train: epoch 0010, iter [00500, 05004], lr: 0.099896, loss: 3.0498
2022-03-01 12:49:03 - train: epoch 0010, iter [00600, 05004], lr: 0.099896, loss: 2.9416
2022-03-01 12:49:37 - train: epoch 0010, iter [00700, 05004], lr: 0.099896, loss: 3.0254
2022-03-01 12:50:10 - train: epoch 0010, iter [00800, 05004], lr: 0.099896, loss: 2.9438
2022-03-01 12:50:44 - train: epoch 0010, iter [00900, 05004], lr: 0.099896, loss: 2.7080
2022-03-01 12:51:18 - train: epoch 0010, iter [01000, 05004], lr: 0.099896, loss: 2.7468
2022-03-01 12:51:53 - train: epoch 0010, iter [01100, 05004], lr: 0.099896, loss: 3.0504
2022-03-01 12:52:27 - train: epoch 0010, iter [01200, 05004], lr: 0.099896, loss: 2.8011
2022-03-01 12:53:02 - train: epoch 0010, iter [01300, 05004], lr: 0.099896, loss: 2.9046
2022-03-01 12:53:35 - train: epoch 0010, iter [01400, 05004], lr: 0.099896, loss: 3.1020
2022-03-01 12:54:08 - train: epoch 0010, iter [01500, 05004], lr: 0.099896, loss: 2.9398
2022-03-01 12:54:41 - train: epoch 0010, iter [01600, 05004], lr: 0.099896, loss: 3.0424
2022-03-01 12:55:16 - train: epoch 0010, iter [01700, 05004], lr: 0.099896, loss: 3.1263
2022-03-01 12:55:49 - train: epoch 0010, iter [01800, 05004], lr: 0.099896, loss: 2.9533
2022-03-01 12:56:24 - train: epoch 0010, iter [01900, 05004], lr: 0.099896, loss: 3.2114
2022-03-01 12:56:58 - train: epoch 0010, iter [02000, 05004], lr: 0.099896, loss: 2.9302
2022-03-01 12:57:32 - train: epoch 0010, iter [02100, 05004], lr: 0.099896, loss: 2.7977
2022-03-01 12:58:06 - train: epoch 0010, iter [02200, 05004], lr: 0.099896, loss: 3.2731
2022-03-01 12:58:41 - train: epoch 0010, iter [02300, 05004], lr: 0.099896, loss: 3.2528
2022-03-01 12:59:15 - train: epoch 0010, iter [02400, 05004], lr: 0.099896, loss: 3.0990
2022-03-01 12:59:49 - train: epoch 0010, iter [02500, 05004], lr: 0.099896, loss: 3.1050
2022-03-01 13:00:23 - train: epoch 0010, iter [02600, 05004], lr: 0.099896, loss: 3.1541
2022-03-01 13:00:56 - train: epoch 0010, iter [02700, 05004], lr: 0.099896, loss: 2.7526
2022-03-01 13:01:30 - train: epoch 0010, iter [02800, 05004], lr: 0.099896, loss: 2.9509
2022-03-01 13:02:04 - train: epoch 0010, iter [02900, 05004], lr: 0.099896, loss: 3.1615
2022-03-01 13:02:38 - train: epoch 0010, iter [03000, 05004], lr: 0.099896, loss: 2.9180
2022-03-01 13:03:12 - train: epoch 0010, iter [03100, 05004], lr: 0.099896, loss: 3.1626
2022-03-01 13:03:46 - train: epoch 0010, iter [03200, 05004], lr: 0.099896, loss: 3.0239
2022-03-01 13:04:20 - train: epoch 0010, iter [03300, 05004], lr: 0.099896, loss: 3.3877
2022-03-01 13:04:55 - train: epoch 0010, iter [03400, 05004], lr: 0.099896, loss: 3.2605
2022-03-01 13:05:28 - train: epoch 0010, iter [03500, 05004], lr: 0.099896, loss: 3.3225
2022-03-01 13:06:02 - train: epoch 0010, iter [03600, 05004], lr: 0.099896, loss: 3.2457
2022-03-01 13:06:37 - train: epoch 0010, iter [03700, 05004], lr: 0.099896, loss: 2.9142
2022-03-01 13:07:10 - train: epoch 0010, iter [03800, 05004], lr: 0.099896, loss: 3.1726
2022-03-01 13:07:43 - train: epoch 0010, iter [03900, 05004], lr: 0.099896, loss: 2.8719
2022-03-01 13:08:17 - train: epoch 0010, iter [04000, 05004], lr: 0.099896, loss: 3.1341
2022-03-01 13:08:51 - train: epoch 0010, iter [04100, 05004], lr: 0.099896, loss: 3.0882
2022-03-01 13:09:25 - train: epoch 0010, iter [04200, 05004], lr: 0.099896, loss: 3.3621
2022-03-01 13:09:59 - train: epoch 0010, iter [04300, 05004], lr: 0.099896, loss: 3.0968
2022-03-01 13:10:33 - train: epoch 0010, iter [04400, 05004], lr: 0.099896, loss: 3.0476
2022-03-01 13:11:07 - train: epoch 0010, iter [04500, 05004], lr: 0.099896, loss: 2.8541
2022-03-01 13:11:41 - train: epoch 0010, iter [04600, 05004], lr: 0.099896, loss: 3.0575
2022-03-01 13:12:15 - train: epoch 0010, iter [04700, 05004], lr: 0.099896, loss: 3.0471
2022-03-01 13:12:50 - train: epoch 0010, iter [04800, 05004], lr: 0.099896, loss: 3.0627
2022-03-01 13:13:23 - train: epoch 0010, iter [04900, 05004], lr: 0.099896, loss: 2.9126
2022-03-01 13:13:55 - train: epoch 0010, iter [05000, 05004], lr: 0.099896, loss: 2.8341
2022-03-01 13:13:56 - train: epoch 010, train_loss: 3.0644
2022-03-01 13:15:10 - eval: epoch: 010, acc1: 46.578%, acc5: 73.178%, test_loss: 2.3389, per_image_load_time: 2.348ms, per_image_inference_time: 0.533ms
2022-03-01 13:15:11 - until epoch: 010, best_acc1: 46.578%
2022-03-01 13:15:11 - epoch 011 lr: 0.09983786540671051
2022-03-01 13:15:50 - train: epoch 0011, iter [00100, 05004], lr: 0.099838, loss: 2.8610
2022-03-01 13:16:24 - train: epoch 0011, iter [00200, 05004], lr: 0.099838, loss: 3.3332
2022-03-01 13:16:58 - train: epoch 0011, iter [00300, 05004], lr: 0.099838, loss: 2.8044
2022-03-01 13:17:32 - train: epoch 0011, iter [00400, 05004], lr: 0.099838, loss: 3.1019
2022-03-01 13:18:06 - train: epoch 0011, iter [00500, 05004], lr: 0.099838, loss: 2.9380
2022-03-01 13:18:41 - train: epoch 0011, iter [00600, 05004], lr: 0.099838, loss: 3.0473
2022-03-01 13:19:15 - train: epoch 0011, iter [00700, 05004], lr: 0.099838, loss: 3.0573
2022-03-01 13:19:49 - train: epoch 0011, iter [00800, 05004], lr: 0.099838, loss: 3.0634
2022-03-01 13:20:21 - train: epoch 0011, iter [00900, 05004], lr: 0.099838, loss: 3.1505
2022-03-01 13:20:56 - train: epoch 0011, iter [01000, 05004], lr: 0.099838, loss: 2.9740
2022-03-01 13:21:30 - train: epoch 0011, iter [01100, 05004], lr: 0.099838, loss: 3.1261
2022-03-01 13:22:04 - train: epoch 0011, iter [01200, 05004], lr: 0.099838, loss: 3.4568
2022-03-01 13:22:38 - train: epoch 0011, iter [01300, 05004], lr: 0.099838, loss: 3.1934
2022-03-01 13:23:13 - train: epoch 0011, iter [01400, 05004], lr: 0.099838, loss: 2.8726
2022-03-01 13:23:48 - train: epoch 0011, iter [01500, 05004], lr: 0.099838, loss: 2.7110
2022-03-01 13:24:21 - train: epoch 0011, iter [01600, 05004], lr: 0.099838, loss: 2.9979
2022-03-01 13:24:56 - train: epoch 0011, iter [01700, 05004], lr: 0.099838, loss: 3.0992
2022-03-01 13:25:30 - train: epoch 0011, iter [01800, 05004], lr: 0.099838, loss: 3.0389
2022-03-01 13:26:05 - train: epoch 0011, iter [01900, 05004], lr: 0.099838, loss: 2.9224
2022-03-01 13:26:37 - train: epoch 0011, iter [02000, 05004], lr: 0.099838, loss: 3.1175
2022-03-01 13:27:11 - train: epoch 0011, iter [02100, 05004], lr: 0.099838, loss: 3.0745
2022-03-01 13:27:46 - train: epoch 0011, iter [02200, 05004], lr: 0.099838, loss: 2.8658
2022-03-01 13:28:20 - train: epoch 0011, iter [02300, 05004], lr: 0.099838, loss: 3.2003
2022-03-01 13:28:54 - train: epoch 0011, iter [02400, 05004], lr: 0.099838, loss: 2.7572
2022-03-01 13:29:29 - train: epoch 0011, iter [02500, 05004], lr: 0.099838, loss: 3.2574
2022-03-01 13:30:02 - train: epoch 0011, iter [02600, 05004], lr: 0.099838, loss: 3.1127
2022-03-01 13:30:37 - train: epoch 0011, iter [02700, 05004], lr: 0.099838, loss: 2.9898
2022-03-01 13:31:11 - train: epoch 0011, iter [02800, 05004], lr: 0.099838, loss: 2.7252
2022-03-01 13:31:46 - train: epoch 0011, iter [02900, 05004], lr: 0.099838, loss: 2.9834
2022-03-01 13:32:20 - train: epoch 0011, iter [03000, 05004], lr: 0.099838, loss: 3.1746
2022-03-01 13:32:53 - train: epoch 0011, iter [03100, 05004], lr: 0.099838, loss: 2.9442
2022-03-01 13:33:25 - train: epoch 0011, iter [03200, 05004], lr: 0.099838, loss: 2.8904
2022-03-01 13:34:00 - train: epoch 0011, iter [03300, 05004], lr: 0.099838, loss: 3.1434
2022-03-01 13:34:35 - train: epoch 0011, iter [03400, 05004], lr: 0.099838, loss: 3.1744
2022-03-01 13:35:09 - train: epoch 0011, iter [03500, 05004], lr: 0.099838, loss: 2.8401
2022-03-01 13:35:42 - train: epoch 0011, iter [03600, 05004], lr: 0.099838, loss: 3.0360
2022-03-01 13:36:17 - train: epoch 0011, iter [03700, 05004], lr: 0.099838, loss: 3.2522
2022-03-01 13:36:51 - train: epoch 0011, iter [03800, 05004], lr: 0.099838, loss: 2.6948
2022-03-01 13:37:25 - train: epoch 0011, iter [03900, 05004], lr: 0.099838, loss: 3.2280
2022-03-01 13:38:00 - train: epoch 0011, iter [04000, 05004], lr: 0.099838, loss: 2.8058
2022-03-01 13:38:34 - train: epoch 0011, iter [04100, 05004], lr: 0.099838, loss: 2.9722
2022-03-01 13:39:07 - train: epoch 0011, iter [04200, 05004], lr: 0.099838, loss: 3.1124
2022-03-01 13:39:41 - train: epoch 0011, iter [04300, 05004], lr: 0.099838, loss: 3.1289
2022-03-01 13:40:15 - train: epoch 0011, iter [04400, 05004], lr: 0.099838, loss: 3.1628
2022-03-01 13:40:49 - train: epoch 0011, iter [04500, 05004], lr: 0.099838, loss: 2.9312
2022-03-01 13:41:22 - train: epoch 0011, iter [04600, 05004], lr: 0.099838, loss: 2.8040
2022-03-01 13:41:57 - train: epoch 0011, iter [04700, 05004], lr: 0.099838, loss: 2.9303
2022-03-01 13:42:31 - train: epoch 0011, iter [04800, 05004], lr: 0.099838, loss: 2.8535
2022-03-01 13:43:05 - train: epoch 0011, iter [04900, 05004], lr: 0.099838, loss: 2.8019
2022-03-01 13:43:39 - train: epoch 0011, iter [05000, 05004], lr: 0.099838, loss: 2.9137
2022-03-01 13:43:40 - train: epoch 011, train_loss: 3.0245
2022-03-01 13:44:54 - eval: epoch: 011, acc1: 47.870%, acc5: 74.378%, test_loss: 2.2582, per_image_load_time: 2.191ms, per_image_inference_time: 0.509ms
2022-03-01 13:44:54 - until epoch: 011, best_acc1: 47.870%
2022-03-01 13:44:54 - epoch 012 lr: 0.09976658173588243
2022-03-01 13:45:32 - train: epoch 0012, iter [00100, 05004], lr: 0.099767, loss: 2.8731
2022-03-01 13:46:06 - train: epoch 0012, iter [00200, 05004], lr: 0.099767, loss: 2.8604
2022-03-01 13:46:39 - train: epoch 0012, iter [00300, 05004], lr: 0.099767, loss: 3.0176
2022-03-01 13:47:13 - train: epoch 0012, iter [00400, 05004], lr: 0.099767, loss: 3.2416
2022-03-01 13:47:46 - train: epoch 0012, iter [00500, 05004], lr: 0.099767, loss: 3.2624
2022-03-01 13:48:21 - train: epoch 0012, iter [00600, 05004], lr: 0.099767, loss: 2.9146
2022-03-01 13:48:55 - train: epoch 0012, iter [00700, 05004], lr: 0.099767, loss: 3.0062
2022-03-01 13:49:29 - train: epoch 0012, iter [00800, 05004], lr: 0.099767, loss: 3.0443
2022-03-01 13:50:04 - train: epoch 0012, iter [00900, 05004], lr: 0.099767, loss: 3.2040
2022-03-01 13:50:38 - train: epoch 0012, iter [01000, 05004], lr: 0.099767, loss: 2.7640
2022-03-01 13:51:13 - train: epoch 0012, iter [01100, 05004], lr: 0.099767, loss: 3.2762
2022-03-01 13:51:47 - train: epoch 0012, iter [01200, 05004], lr: 0.099767, loss: 2.7142
2022-03-01 13:52:19 - train: epoch 0012, iter [01300, 05004], lr: 0.099767, loss: 2.9033
2022-03-01 13:52:53 - train: epoch 0012, iter [01400, 05004], lr: 0.099767, loss: 3.0545
2022-03-01 13:53:27 - train: epoch 0012, iter [01500, 05004], lr: 0.099767, loss: 2.6622
2022-03-01 13:54:01 - train: epoch 0012, iter [01600, 05004], lr: 0.099767, loss: 2.9282
2022-03-01 13:54:35 - train: epoch 0012, iter [01700, 05004], lr: 0.099767, loss: 2.7718
2022-03-01 13:55:09 - train: epoch 0012, iter [01800, 05004], lr: 0.099767, loss: 3.0467
2022-03-01 13:55:43 - train: epoch 0012, iter [01900, 05004], lr: 0.099767, loss: 3.2462
2022-03-01 13:56:17 - train: epoch 0012, iter [02000, 05004], lr: 0.099767, loss: 3.0036
2022-03-01 13:56:51 - train: epoch 0012, iter [02100, 05004], lr: 0.099767, loss: 2.9196
2022-03-01 13:57:25 - train: epoch 0012, iter [02200, 05004], lr: 0.099767, loss: 3.2914
2022-03-01 13:58:00 - train: epoch 0012, iter [02300, 05004], lr: 0.099767, loss: 2.8769
2022-03-01 13:58:33 - train: epoch 0012, iter [02400, 05004], lr: 0.099767, loss: 3.0283
2022-03-01 13:59:06 - train: epoch 0012, iter [02500, 05004], lr: 0.099767, loss: 2.5875
2022-03-01 13:59:40 - train: epoch 0012, iter [02600, 05004], lr: 0.099767, loss: 2.9873
2022-03-01 14:00:14 - train: epoch 0012, iter [02700, 05004], lr: 0.099767, loss: 2.8761
2022-03-01 14:00:48 - train: epoch 0012, iter [02800, 05004], lr: 0.099767, loss: 2.9425
2022-03-01 14:01:22 - train: epoch 0012, iter [02900, 05004], lr: 0.099767, loss: 2.6751
2022-03-01 14:01:56 - train: epoch 0012, iter [03000, 05004], lr: 0.099767, loss: 2.8670
2022-03-01 14:02:31 - train: epoch 0012, iter [03100, 05004], lr: 0.099767, loss: 3.2139
2022-03-01 14:03:04 - train: epoch 0012, iter [03200, 05004], lr: 0.099767, loss: 2.7292
2022-03-01 14:03:39 - train: epoch 0012, iter [03300, 05004], lr: 0.099767, loss: 2.9596
2022-03-01 14:04:13 - train: epoch 0012, iter [03400, 05004], lr: 0.099767, loss: 3.0599
2022-03-01 14:04:47 - train: epoch 0012, iter [03500, 05004], lr: 0.099767, loss: 3.1016
2022-03-01 14:05:20 - train: epoch 0012, iter [03600, 05004], lr: 0.099767, loss: 3.1377
2022-03-01 14:05:54 - train: epoch 0012, iter [03700, 05004], lr: 0.099767, loss: 2.9753
2022-03-01 14:06:27 - train: epoch 0012, iter [03800, 05004], lr: 0.099767, loss: 3.0889
2022-03-01 14:07:01 - train: epoch 0012, iter [03900, 05004], lr: 0.099767, loss: 2.9155
2022-03-01 14:07:36 - train: epoch 0012, iter [04000, 05004], lr: 0.099767, loss: 3.0301
2022-03-01 14:08:11 - train: epoch 0012, iter [04100, 05004], lr: 0.099767, loss: 2.7795
2022-03-01 14:08:44 - train: epoch 0012, iter [04200, 05004], lr: 0.099767, loss: 2.9938
2022-03-01 14:09:19 - train: epoch 0012, iter [04300, 05004], lr: 0.099767, loss: 3.0647
2022-03-01 14:09:53 - train: epoch 0012, iter [04400, 05004], lr: 0.099767, loss: 2.8005
2022-03-01 14:10:26 - train: epoch 0012, iter [04500, 05004], lr: 0.099767, loss: 2.7405
2022-03-01 14:11:00 - train: epoch 0012, iter [04600, 05004], lr: 0.099767, loss: 3.2635
2022-03-01 14:11:34 - train: epoch 0012, iter [04700, 05004], lr: 0.099767, loss: 2.8045
2022-03-01 14:12:07 - train: epoch 0012, iter [04800, 05004], lr: 0.099767, loss: 3.0258
2022-03-01 14:12:41 - train: epoch 0012, iter [04900, 05004], lr: 0.099767, loss: 2.9273
2022-03-01 14:13:14 - train: epoch 0012, iter [05000, 05004], lr: 0.099767, loss: 2.8448
2022-03-01 14:13:15 - train: epoch 012, train_loss: 2.9976
2022-03-01 14:14:29 - eval: epoch: 012, acc1: 45.914%, acc5: 72.182%, test_loss: 2.3845, per_image_load_time: 2.268ms, per_image_inference_time: 0.539ms
2022-03-01 14:14:30 - until epoch: 012, best_acc1: 47.870%
2022-03-01 14:14:30 - epoch 013 lr: 0.09968238114591566
2022-03-01 14:15:08 - train: epoch 0013, iter [00100, 05004], lr: 0.099682, loss: 2.8466
2022-03-01 14:15:43 - train: epoch 0013, iter [00200, 05004], lr: 0.099682, loss: 2.8245
2022-03-01 14:16:17 - train: epoch 0013, iter [00300, 05004], lr: 0.099682, loss: 2.7998
2022-03-01 14:16:50 - train: epoch 0013, iter [00400, 05004], lr: 0.099682, loss: 2.9021
2022-03-01 14:17:25 - train: epoch 0013, iter [00500, 05004], lr: 0.099682, loss: 2.8114
2022-03-01 14:17:58 - train: epoch 0013, iter [00600, 05004], lr: 0.099682, loss: 2.9662
2022-03-01 14:18:31 - train: epoch 0013, iter [00700, 05004], lr: 0.099682, loss: 2.7383
2022-03-01 14:19:05 - train: epoch 0013, iter [00800, 05004], lr: 0.099682, loss: 3.1462
2022-03-01 14:19:39 - train: epoch 0013, iter [00900, 05004], lr: 0.099682, loss: 2.7820
2022-03-01 14:20:14 - train: epoch 0013, iter [01000, 05004], lr: 0.099682, loss: 3.0744
2022-03-01 14:20:47 - train: epoch 0013, iter [01100, 05004], lr: 0.099682, loss: 2.8378
2022-03-01 14:21:21 - train: epoch 0013, iter [01200, 05004], lr: 0.099682, loss: 2.9913
2022-03-01 14:21:56 - train: epoch 0013, iter [01300, 05004], lr: 0.099682, loss: 3.0695
2022-03-01 14:22:30 - train: epoch 0013, iter [01400, 05004], lr: 0.099682, loss: 2.9851
2022-03-01 14:23:04 - train: epoch 0013, iter [01500, 05004], lr: 0.099682, loss: 2.9922
2022-03-01 14:23:38 - train: epoch 0013, iter [01600, 05004], lr: 0.099682, loss: 2.6637
2022-03-01 14:24:13 - train: epoch 0013, iter [01700, 05004], lr: 0.099682, loss: 2.9140
2022-03-01 14:24:45 - train: epoch 0013, iter [01800, 05004], lr: 0.099682, loss: 2.7898
2022-03-01 14:25:19 - train: epoch 0013, iter [01900, 05004], lr: 0.099682, loss: 3.0485
2022-03-01 14:25:52 - train: epoch 0013, iter [02000, 05004], lr: 0.099682, loss: 3.2765
2022-03-01 14:26:26 - train: epoch 0013, iter [02100, 05004], lr: 0.099682, loss: 3.2088
2022-03-01 14:27:01 - train: epoch 0013, iter [02200, 05004], lr: 0.099682, loss: 2.8121
2022-03-01 14:27:35 - train: epoch 0013, iter [02300, 05004], lr: 0.099682, loss: 2.9928
2022-03-01 14:28:09 - train: epoch 0013, iter [02400, 05004], lr: 0.099682, loss: 2.9353
2022-03-01 14:28:43 - train: epoch 0013, iter [02500, 05004], lr: 0.099682, loss: 2.8919
2022-03-01 14:29:17 - train: epoch 0013, iter [02600, 05004], lr: 0.099682, loss: 2.9623
2022-03-01 14:29:52 - train: epoch 0013, iter [02700, 05004], lr: 0.099682, loss: 2.9794
2022-03-01 14:30:25 - train: epoch 0013, iter [02800, 05004], lr: 0.099682, loss: 2.9181
2022-03-01 14:30:59 - train: epoch 0013, iter [02900, 05004], lr: 0.099682, loss: 3.1671
2022-03-01 14:31:31 - train: epoch 0013, iter [03000, 05004], lr: 0.099682, loss: 3.0472
2022-03-01 14:32:05 - train: epoch 0013, iter [03100, 05004], lr: 0.099682, loss: 2.9195
2022-03-01 14:32:39 - train: epoch 0013, iter [03200, 05004], lr: 0.099682, loss: 2.9457
2022-03-01 14:33:14 - train: epoch 0013, iter [03300, 05004], lr: 0.099682, loss: 2.8312
2022-03-01 14:33:48 - train: epoch 0013, iter [03400, 05004], lr: 0.099682, loss: 2.9716
2022-03-01 14:34:22 - train: epoch 0013, iter [03500, 05004], lr: 0.099682, loss: 2.9817
2022-03-01 14:34:56 - train: epoch 0013, iter [03600, 05004], lr: 0.099682, loss: 3.3071
2022-03-01 14:35:31 - train: epoch 0013, iter [03700, 05004], lr: 0.099682, loss: 2.8366
2022-03-01 14:36:05 - train: epoch 0013, iter [03800, 05004], lr: 0.099682, loss: 2.9730
2022-03-01 14:36:38 - train: epoch 0013, iter [03900, 05004], lr: 0.099682, loss: 2.9392
2022-03-01 14:37:12 - train: epoch 0013, iter [04000, 05004], lr: 0.099682, loss: 2.9072
2022-03-01 14:37:45 - train: epoch 0013, iter [04100, 05004], lr: 0.099682, loss: 3.0421
2022-03-01 14:38:20 - train: epoch 0013, iter [04200, 05004], lr: 0.099682, loss: 2.8692
2022-03-01 14:38:52 - train: epoch 0013, iter [04300, 05004], lr: 0.099682, loss: 2.7829
2022-03-01 14:39:26 - train: epoch 0013, iter [04400, 05004], lr: 0.099682, loss: 2.7296
2022-03-01 14:40:01 - train: epoch 0013, iter [04500, 05004], lr: 0.099682, loss: 2.8187
2022-03-01 14:40:36 - train: epoch 0013, iter [04600, 05004], lr: 0.099682, loss: 2.9437
2022-03-01 14:41:10 - train: epoch 0013, iter [04700, 05004], lr: 0.099682, loss: 3.0061
2022-03-01 14:41:44 - train: epoch 0013, iter [04800, 05004], lr: 0.099682, loss: 3.1341
2022-03-01 14:42:17 - train: epoch 0013, iter [04900, 05004], lr: 0.099682, loss: 2.9242
2022-03-01 14:42:51 - train: epoch 0013, iter [05000, 05004], lr: 0.099682, loss: 3.0609
2022-03-01 14:42:52 - train: epoch 013, train_loss: 2.9696
2022-03-01 14:44:04 - eval: epoch: 013, acc1: 49.962%, acc5: 76.298%, test_loss: 2.1558, per_image_load_time: 2.002ms, per_image_inference_time: 0.529ms
2022-03-01 14:44:05 - until epoch: 013, best_acc1: 49.962%
2022-03-01 14:44:05 - epoch 014 lr: 0.0995852854910781
2022-03-01 14:44:43 - train: epoch 0014, iter [00100, 05004], lr: 0.099585, loss: 2.9945
2022-03-01 14:45:17 - train: epoch 0014, iter [00200, 05004], lr: 0.099585, loss: 3.1406
2022-03-01 14:45:51 - train: epoch 0014, iter [00300, 05004], lr: 0.099585, loss: 2.6605
2022-03-01 14:46:24 - train: epoch 0014, iter [00400, 05004], lr: 0.099585, loss: 2.8392
2022-03-01 14:46:59 - train: epoch 0014, iter [00500, 05004], lr: 0.099585, loss: 3.2045
2022-03-01 14:47:33 - train: epoch 0014, iter [00600, 05004], lr: 0.099585, loss: 3.0494
2022-03-01 14:48:07 - train: epoch 0014, iter [00700, 05004], lr: 0.099585, loss: 2.8006
2022-03-01 14:48:41 - train: epoch 0014, iter [00800, 05004], lr: 0.099585, loss: 3.1823
2022-03-01 14:49:15 - train: epoch 0014, iter [00900, 05004], lr: 0.099585, loss: 2.7923
2022-03-01 14:49:49 - train: epoch 0014, iter [01000, 05004], lr: 0.099585, loss: 3.2156
2022-03-01 14:50:22 - train: epoch 0014, iter [01100, 05004], lr: 0.099585, loss: 3.1018
2022-03-01 14:50:55 - train: epoch 0014, iter [01200, 05004], lr: 0.099585, loss: 3.0826
2022-03-01 14:51:30 - train: epoch 0014, iter [01300, 05004], lr: 0.099585, loss: 3.0295
2022-03-01 14:52:05 - train: epoch 0014, iter [01400, 05004], lr: 0.099585, loss: 2.8948
2022-03-01 14:52:39 - train: epoch 0014, iter [01500, 05004], lr: 0.099585, loss: 2.9200
2022-03-01 14:53:13 - train: epoch 0014, iter [01600, 05004], lr: 0.099585, loss: 2.9175
2022-03-01 14:53:47 - train: epoch 0014, iter [01700, 05004], lr: 0.099585, loss: 3.1592
2022-03-01 14:54:22 - train: epoch 0014, iter [01800, 05004], lr: 0.099585, loss: 3.0818
2022-03-01 14:54:56 - train: epoch 0014, iter [01900, 05004], lr: 0.099585, loss: 2.7542
2022-03-01 14:55:30 - train: epoch 0014, iter [02000, 05004], lr: 0.099585, loss: 2.7712
2022-03-01 14:56:05 - train: epoch 0014, iter [02100, 05004], lr: 0.099585, loss: 3.1232
2022-03-01 14:56:38 - train: epoch 0014, iter [02200, 05004], lr: 0.099585, loss: 3.0650
2022-03-01 14:57:12 - train: epoch 0014, iter [02300, 05004], lr: 0.099585, loss: 3.1766
2022-03-01 14:57:46 - train: epoch 0014, iter [02400, 05004], lr: 0.099585, loss: 3.1132
2022-03-01 14:58:20 - train: epoch 0014, iter [02500, 05004], lr: 0.099585, loss: 2.9967
2022-03-01 14:58:54 - train: epoch 0014, iter [02600, 05004], lr: 0.099585, loss: 2.7721
2022-03-01 14:59:29 - train: epoch 0014, iter [02700, 05004], lr: 0.099585, loss: 2.9630
2022-03-01 15:00:02 - train: epoch 0014, iter [02800, 05004], lr: 0.099585, loss: 3.1281
2022-03-01 15:00:37 - train: epoch 0014, iter [02900, 05004], lr: 0.099585, loss: 2.8677
2022-03-01 15:01:11 - train: epoch 0014, iter [03000, 05004], lr: 0.099585, loss: 3.1119
2022-03-01 15:01:45 - train: epoch 0014, iter [03100, 05004], lr: 0.099585, loss: 2.9789
2022-03-01 15:02:19 - train: epoch 0014, iter [03200, 05004], lr: 0.099585, loss: 2.8441
2022-03-01 15:02:53 - train: epoch 0014, iter [03300, 05004], lr: 0.099585, loss: 2.8902
2022-03-01 15:03:27 - train: epoch 0014, iter [03400, 05004], lr: 0.099585, loss: 2.7912
2022-03-01 15:04:00 - train: epoch 0014, iter [03500, 05004], lr: 0.099585, loss: 2.9111
2022-03-01 15:04:34 - train: epoch 0014, iter [03600, 05004], lr: 0.099585, loss: 2.8607
2022-03-01 15:05:08 - train: epoch 0014, iter [03700, 05004], lr: 0.099585, loss: 2.8216
2022-03-01 15:05:42 - train: epoch 0014, iter [03800, 05004], lr: 0.099585, loss: 2.9372
2022-03-01 15:06:16 - train: epoch 0014, iter [03900, 05004], lr: 0.099585, loss: 3.0156
2022-03-01 15:06:51 - train: epoch 0014, iter [04000, 05004], lr: 0.099585, loss: 2.9912
2022-03-01 15:07:24 - train: epoch 0014, iter [04100, 05004], lr: 0.099585, loss: 2.9343
2022-03-01 15:07:59 - train: epoch 0014, iter [04200, 05004], lr: 0.099585, loss: 2.7576
2022-03-01 15:08:33 - train: epoch 0014, iter [04300, 05004], lr: 0.099585, loss: 2.8351
2022-03-01 15:09:07 - train: epoch 0014, iter [04400, 05004], lr: 0.099585, loss: 2.8331
2022-03-01 15:09:41 - train: epoch 0014, iter [04500, 05004], lr: 0.099585, loss: 2.8096
2022-03-01 15:10:14 - train: epoch 0014, iter [04600, 05004], lr: 0.099585, loss: 2.7112
2022-03-01 15:10:48 - train: epoch 0014, iter [04700, 05004], lr: 0.099585, loss: 2.9177
2022-03-01 15:11:22 - train: epoch 0014, iter [04800, 05004], lr: 0.099585, loss: 2.8811
2022-03-01 15:11:57 - train: epoch 0014, iter [04900, 05004], lr: 0.099585, loss: 2.8791
2022-03-01 15:12:29 - train: epoch 0014, iter [05000, 05004], lr: 0.099585, loss: 3.0659
2022-03-01 15:12:30 - train: epoch 014, train_loss: 2.9519
2022-03-01 15:13:44 - eval: epoch: 014, acc1: 47.202%, acc5: 74.250%, test_loss: 2.2937, per_image_load_time: 1.157ms, per_image_inference_time: 0.527ms
2022-03-01 15:13:45 - until epoch: 014, best_acc1: 49.962%
2022-03-01 15:13:45 - epoch 015 lr: 0.09947531997255256
2022-03-01 15:14:23 - train: epoch 0015, iter [00100, 05004], lr: 0.099475, loss: 2.7842
2022-03-01 15:14:57 - train: epoch 0015, iter [00200, 05004], lr: 0.099475, loss: 3.0999
2022-03-01 15:15:32 - train: epoch 0015, iter [00300, 05004], lr: 0.099475, loss: 3.0349
2022-03-01 15:16:05 - train: epoch 0015, iter [00400, 05004], lr: 0.099475, loss: 3.0694
2022-03-01 15:16:37 - train: epoch 0015, iter [00500, 05004], lr: 0.099475, loss: 2.8078
2022-03-01 15:17:11 - train: epoch 0015, iter [00600, 05004], lr: 0.099475, loss: 3.0079
2022-03-01 15:17:45 - train: epoch 0015, iter [00700, 05004], lr: 0.099475, loss: 2.8880
2022-03-01 15:18:19 - train: epoch 0015, iter [00800, 05004], lr: 0.099475, loss: 2.6344
2022-03-01 15:18:53 - train: epoch 0015, iter [00900, 05004], lr: 0.099475, loss: 2.7796
2022-03-01 15:19:27 - train: epoch 0015, iter [01000, 05004], lr: 0.099475, loss: 3.0115
2022-03-01 15:20:01 - train: epoch 0015, iter [01100, 05004], lr: 0.099475, loss: 2.9300
2022-03-01 15:20:34 - train: epoch 0015, iter [01200, 05004], lr: 0.099475, loss: 2.9050
2022-03-01 15:21:08 - train: epoch 0015, iter [01300, 05004], lr: 0.099475, loss: 3.2054
2022-03-01 15:21:43 - train: epoch 0015, iter [01400, 05004], lr: 0.099475, loss: 2.9152
2022-03-01 15:22:17 - train: epoch 0015, iter [01500, 05004], lr: 0.099475, loss: 2.6776
2022-03-01 15:22:50 - train: epoch 0015, iter [01600, 05004], lr: 0.099475, loss: 3.0853
2022-03-01 15:23:22 - train: epoch 0015, iter [01700, 05004], lr: 0.099475, loss: 3.1281
2022-03-01 15:23:56 - train: epoch 0015, iter [01800, 05004], lr: 0.099475, loss: 2.6235
2022-03-01 15:24:30 - train: epoch 0015, iter [01900, 05004], lr: 0.099475, loss: 2.7013
2022-03-01 15:25:04 - train: epoch 0015, iter [02000, 05004], lr: 0.099475, loss: 2.7720
2022-03-01 15:25:38 - train: epoch 0015, iter [02100, 05004], lr: 0.099475, loss: 2.8243
2022-03-01 15:26:12 - train: epoch 0015, iter [02200, 05004], lr: 0.099475, loss: 3.2208
2022-03-01 15:26:46 - train: epoch 0015, iter [02300, 05004], lr: 0.099475, loss: 2.6566
2022-03-01 15:27:20 - train: epoch 0015, iter [02400, 05004], lr: 0.099475, loss: 2.8621
2022-03-01 15:27:53 - train: epoch 0015, iter [02500, 05004], lr: 0.099475, loss: 2.8288
2022-03-01 15:28:28 - train: epoch 0015, iter [02600, 05004], lr: 0.099475, loss: 2.8034
2022-03-01 15:29:01 - train: epoch 0015, iter [02700, 05004], lr: 0.099475, loss: 3.1394
2022-03-01 15:29:34 - train: epoch 0015, iter [02800, 05004], lr: 0.099475, loss: 3.0876
2022-03-01 15:30:08 - train: epoch 0015, iter [02900, 05004], lr: 0.099475, loss: 2.7916
2022-03-01 15:30:42 - train: epoch 0015, iter [03000, 05004], lr: 0.099475, loss: 2.7596
2022-03-01 15:31:16 - train: epoch 0015, iter [03100, 05004], lr: 0.099475, loss: 2.9873
2022-03-01 15:31:50 - train: epoch 0015, iter [03200, 05004], lr: 0.099475, loss: 3.0238
2022-03-01 15:32:24 - train: epoch 0015, iter [03300, 05004], lr: 0.099475, loss: 2.7615
2022-03-01 15:32:58 - train: epoch 0015, iter [03400, 05004], lr: 0.099475, loss: 2.9281
2022-03-01 15:33:33 - train: epoch 0015, iter [03500, 05004], lr: 0.099475, loss: 3.1648
2022-03-01 15:34:06 - train: epoch 0015, iter [03600, 05004], lr: 0.099475, loss: 2.9172
2022-03-01 15:34:40 - train: epoch 0015, iter [03700, 05004], lr: 0.099475, loss: 2.8708
2022-03-01 15:35:15 - train: epoch 0015, iter [03800, 05004], lr: 0.099475, loss: 2.9882
2022-03-01 15:35:47 - train: epoch 0015, iter [03900, 05004], lr: 0.099475, loss: 3.0867
2022-03-01 15:36:20 - train: epoch 0015, iter [04000, 05004], lr: 0.099475, loss: 2.8512
2022-03-01 15:36:55 - train: epoch 0015, iter [04100, 05004], lr: 0.099475, loss: 3.0001
2022-03-01 15:37:29 - train: epoch 0015, iter [04200, 05004], lr: 0.099475, loss: 2.6598
2022-03-01 15:38:02 - train: epoch 0015, iter [04300, 05004], lr: 0.099475, loss: 2.8314
2022-03-01 15:38:37 - train: epoch 0015, iter [04400, 05004], lr: 0.099475, loss: 2.8543
2022-03-01 15:39:10 - train: epoch 0015, iter [04500, 05004], lr: 0.099475, loss: 3.0211
2022-03-01 15:39:44 - train: epoch 0015, iter [04600, 05004], lr: 0.099475, loss: 2.6881
2022-03-01 15:40:18 - train: epoch 0015, iter [04700, 05004], lr: 0.099475, loss: 2.9025
2022-03-01 15:40:52 - train: epoch 0015, iter [04800, 05004], lr: 0.099475, loss: 2.9493
2022-03-01 15:41:26 - train: epoch 0015, iter [04900, 05004], lr: 0.099475, loss: 3.0357
2022-03-01 15:41:58 - train: epoch 0015, iter [05000, 05004], lr: 0.099475, loss: 2.9653
2022-03-01 15:41:59 - train: epoch 015, train_loss: 2.9351
2022-03-01 15:43:12 - eval: epoch: 015, acc1: 49.970%, acc5: 76.424%, test_loss: 2.1511, per_image_load_time: 2.325ms, per_image_inference_time: 0.505ms
2022-03-01 15:43:13 - until epoch: 015, best_acc1: 49.970%
2022-03-01 15:43:13 - epoch 016 lr: 0.09935251313189564
2022-03-01 15:43:51 - train: epoch 0016, iter [00100, 05004], lr: 0.099353, loss: 2.7589
2022-03-01 15:44:26 - train: epoch 0016, iter [00200, 05004], lr: 0.099353, loss: 2.8200
2022-03-01 15:44:59 - train: epoch 0016, iter [00300, 05004], lr: 0.099353, loss: 2.8103
2022-03-01 15:45:34 - train: epoch 0016, iter [00400, 05004], lr: 0.099353, loss: 3.2942
2022-03-01 15:46:08 - train: epoch 0016, iter [00500, 05004], lr: 0.099353, loss: 2.5677
2022-03-01 15:46:42 - train: epoch 0016, iter [00600, 05004], lr: 0.099353, loss: 3.1913
2022-03-01 15:47:16 - train: epoch 0016, iter [00700, 05004], lr: 0.099353, loss: 2.7041
2022-03-01 15:47:50 - train: epoch 0016, iter [00800, 05004], lr: 0.099353, loss: 2.9554
2022-03-01 15:48:23 - train: epoch 0016, iter [00900, 05004], lr: 0.099353, loss: 2.8721
2022-03-01 15:48:56 - train: epoch 0016, iter [01000, 05004], lr: 0.099353, loss: 3.0421
2022-03-01 15:49:29 - train: epoch 0016, iter [01100, 05004], lr: 0.099353, loss: 2.9210
2022-03-01 15:50:03 - train: epoch 0016, iter [01200, 05004], lr: 0.099353, loss: 2.8526
2022-03-01 15:50:37 - train: epoch 0016, iter [01300, 05004], lr: 0.099353, loss: 2.9866
2022-03-01 15:51:12 - train: epoch 0016, iter [01400, 05004], lr: 0.099353, loss: 2.9836
2022-03-01 15:51:45 - train: epoch 0016, iter [01500, 05004], lr: 0.099353, loss: 3.0449
2022-03-01 15:52:20 - train: epoch 0016, iter [01600, 05004], lr: 0.099353, loss: 3.0465
2022-03-01 15:52:52 - train: epoch 0016, iter [01700, 05004], lr: 0.099353, loss: 2.8356
2022-03-01 15:53:27 - train: epoch 0016, iter [01800, 05004], lr: 0.099353, loss: 3.0594
2022-03-01 15:54:01 - train: epoch 0016, iter [01900, 05004], lr: 0.099353, loss: 3.1124
2022-03-01 15:54:35 - train: epoch 0016, iter [02000, 05004], lr: 0.099353, loss: 2.7654
2022-03-01 15:55:07 - train: epoch 0016, iter [02100, 05004], lr: 0.099353, loss: 2.9950
2022-03-01 15:55:41 - train: epoch 0016, iter [02200, 05004], lr: 0.099353, loss: 2.9722
2022-03-01 15:56:15 - train: epoch 0016, iter [02300, 05004], lr: 0.099353, loss: 3.2326
2022-03-01 15:56:48 - train: epoch 0016, iter [02400, 05004], lr: 0.099353, loss: 2.9338
2022-03-01 15:57:23 - train: epoch 0016, iter [02500, 05004], lr: 0.099353, loss: 2.8041
2022-03-01 15:57:57 - train: epoch 0016, iter [02600, 05004], lr: 0.099353, loss: 2.9128
2022-03-01 15:58:31 - train: epoch 0016, iter [02700, 05004], lr: 0.099353, loss: 2.9150
2022-03-01 15:59:05 - train: epoch 0016, iter [02800, 05004], lr: 0.099353, loss: 2.7845
2022-03-01 15:59:39 - train: epoch 0016, iter [02900, 05004], lr: 0.099353, loss: 2.8976
2022-03-01 16:00:14 - train: epoch 0016, iter [03000, 05004], lr: 0.099353, loss: 3.1088
2022-03-01 16:00:48 - train: epoch 0016, iter [03100, 05004], lr: 0.099353, loss: 3.1295
2022-03-01 16:01:21 - train: epoch 0016, iter [03200, 05004], lr: 0.099353, loss: 3.1059
2022-03-01 16:01:54 - train: epoch 0016, iter [03300, 05004], lr: 0.099353, loss: 3.1037
2022-03-01 16:02:28 - train: epoch 0016, iter [03400, 05004], lr: 0.099353, loss: 2.8400
2022-03-01 16:03:01 - train: epoch 0016, iter [03500, 05004], lr: 0.099353, loss: 2.7656
2022-03-01 16:03:36 - train: epoch 0016, iter [03600, 05004], lr: 0.099353, loss: 2.6981
2022-03-01 16:04:11 - train: epoch 0016, iter [03700, 05004], lr: 0.099353, loss: 3.0810
2022-03-01 16:04:45 - train: epoch 0016, iter [03800, 05004], lr: 0.099353, loss: 3.2754
2022-03-01 16:05:18 - train: epoch 0016, iter [03900, 05004], lr: 0.099353, loss: 2.9858
2022-03-01 16:05:53 - train: epoch 0016, iter [04000, 05004], lr: 0.099353, loss: 2.9447
2022-03-01 16:06:27 - train: epoch 0016, iter [04100, 05004], lr: 0.099353, loss: 2.7904
2022-03-01 16:07:01 - train: epoch 0016, iter [04200, 05004], lr: 0.099353, loss: 2.9202
2022-03-01 16:07:35 - train: epoch 0016, iter [04300, 05004], lr: 0.099353, loss: 2.7355
2022-03-01 16:08:08 - train: epoch 0016, iter [04400, 05004], lr: 0.099353, loss: 2.8252
2022-03-01 16:08:41 - train: epoch 0016, iter [04500, 05004], lr: 0.099353, loss: 2.9709
2022-03-01 16:09:15 - train: epoch 0016, iter [04600, 05004], lr: 0.099353, loss: 3.0030
2022-03-01 16:09:49 - train: epoch 0016, iter [04700, 05004], lr: 0.099353, loss: 3.1995
2022-03-01 16:10:23 - train: epoch 0016, iter [04800, 05004], lr: 0.099353, loss: 2.7485
2022-03-01 16:10:58 - train: epoch 0016, iter [04900, 05004], lr: 0.099353, loss: 2.7508
2022-03-01 16:11:29 - train: epoch 0016, iter [05000, 05004], lr: 0.099353, loss: 3.1245
2022-03-01 16:11:30 - train: epoch 016, train_loss: 2.9181
2022-03-01 16:12:45 - eval: epoch: 016, acc1: 46.434%, acc5: 72.866%, test_loss: 2.3576, per_image_load_time: 2.376ms, per_image_inference_time: 0.512ms
2022-03-01 16:12:45 - until epoch: 016, best_acc1: 49.970%
2022-03-01 16:12:45 - epoch 017 lr: 0.09921689684362989
2022-03-01 16:13:23 - train: epoch 0017, iter [00100, 05004], lr: 0.099217, loss: 2.8915
2022-03-01 16:13:58 - train: epoch 0017, iter [00200, 05004], lr: 0.099217, loss: 2.8855
2022-03-01 16:14:32 - train: epoch 0017, iter [00300, 05004], lr: 0.099217, loss: 3.1671
2022-03-01 16:15:03 - train: epoch 0017, iter [00400, 05004], lr: 0.099217, loss: 2.6341
2022-03-01 16:15:37 - train: epoch 0017, iter [00500, 05004], lr: 0.099217, loss: 3.0006
2022-03-01 16:16:11 - train: epoch 0017, iter [00600, 05004], lr: 0.099217, loss: 3.3205
2022-03-01 16:16:45 - train: epoch 0017, iter [00700, 05004], lr: 0.099217, loss: 3.2649
2022-03-01 16:17:19 - train: epoch 0017, iter [00800, 05004], lr: 0.099217, loss: 2.7807
2022-03-01 16:17:53 - train: epoch 0017, iter [00900, 05004], lr: 0.099217, loss: 2.9934
2022-03-01 16:18:27 - train: epoch 0017, iter [01000, 05004], lr: 0.099217, loss: 2.8005
2022-03-01 16:19:01 - train: epoch 0017, iter [01100, 05004], lr: 0.099217, loss: 3.0260
2022-03-01 16:19:35 - train: epoch 0017, iter [01200, 05004], lr: 0.099217, loss: 3.1897
2022-03-01 16:20:09 - train: epoch 0017, iter [01300, 05004], lr: 0.099217, loss: 2.7967
2022-03-01 16:20:43 - train: epoch 0017, iter [01400, 05004], lr: 0.099217, loss: 2.8875
2022-03-01 16:21:16 - train: epoch 0017, iter [01500, 05004], lr: 0.099217, loss: 2.5519
2022-03-01 16:21:49 - train: epoch 0017, iter [01600, 05004], lr: 0.099217, loss: 2.7559
2022-03-01 16:22:24 - train: epoch 0017, iter [01700, 05004], lr: 0.099217, loss: 2.8389
2022-03-01 16:22:58 - train: epoch 0017, iter [01800, 05004], lr: 0.099217, loss: 2.8925
2022-03-01 16:23:32 - train: epoch 0017, iter [01900, 05004], lr: 0.099217, loss: 2.6680
2022-03-01 16:24:06 - train: epoch 0017, iter [02000, 05004], lr: 0.099217, loss: 2.9312
2022-03-01 16:24:40 - train: epoch 0017, iter [02100, 05004], lr: 0.099217, loss: 2.8653
2022-03-01 16:25:14 - train: epoch 0017, iter [02200, 05004], lr: 0.099217, loss: 2.8871
2022-03-01 16:25:48 - train: epoch 0017, iter [02300, 05004], lr: 0.099217, loss: 2.9286
2022-03-01 16:26:23 - train: epoch 0017, iter [02400, 05004], lr: 0.099217, loss: 2.8369
2022-03-01 16:26:56 - train: epoch 0017, iter [02500, 05004], lr: 0.099217, loss: 3.2280
2022-03-01 16:27:30 - train: epoch 0017, iter [02600, 05004], lr: 0.099217, loss: 2.8292
2022-03-01 16:28:03 - train: epoch 0017, iter [02700, 05004], lr: 0.099217, loss: 2.9957
2022-03-01 16:28:36 - train: epoch 0017, iter [02800, 05004], lr: 0.099217, loss: 3.0097
2022-03-01 16:29:10 - train: epoch 0017, iter [02900, 05004], lr: 0.099217, loss: 2.9411
2022-03-01 16:29:45 - train: epoch 0017, iter [03000, 05004], lr: 0.099217, loss: 2.7507
2022-03-01 16:30:18 - train: epoch 0017, iter [03100, 05004], lr: 0.099217, loss: 3.1199
2022-03-01 16:30:52 - train: epoch 0017, iter [03200, 05004], lr: 0.099217, loss: 2.6214
2022-03-01 16:31:27 - train: epoch 0017, iter [03300, 05004], lr: 0.099217, loss: 2.8968
2022-03-01 16:32:01 - train: epoch 0017, iter [03400, 05004], lr: 0.099217, loss: 2.7749
2022-03-01 16:32:35 - train: epoch 0017, iter [03500, 05004], lr: 0.099217, loss: 2.9471
2022-03-01 16:33:10 - train: epoch 0017, iter [03600, 05004], lr: 0.099217, loss: 3.1570
2022-03-01 16:33:44 - train: epoch 0017, iter [03700, 05004], lr: 0.099217, loss: 2.8891
2022-03-01 16:34:16 - train: epoch 0017, iter [03800, 05004], lr: 0.099217, loss: 3.0435
2022-03-01 16:34:50 - train: epoch 0017, iter [03900, 05004], lr: 0.099217, loss: 2.8476
2022-03-01 16:35:24 - train: epoch 0017, iter [04000, 05004], lr: 0.099217, loss: 2.7695
2022-03-01 16:35:58 - train: epoch 0017, iter [04100, 05004], lr: 0.099217, loss: 3.0929
2022-03-01 16:36:33 - train: epoch 0017, iter [04200, 05004], lr: 0.099217, loss: 2.8968
2022-03-01 16:37:06 - train: epoch 0017, iter [04300, 05004], lr: 0.099217, loss: 2.9637
2022-03-01 16:37:40 - train: epoch 0017, iter [04400, 05004], lr: 0.099217, loss: 2.9227
2022-03-01 16:38:14 - train: epoch 0017, iter [04500, 05004], lr: 0.099217, loss: 2.8923
2022-03-01 16:38:48 - train: epoch 0017, iter [04600, 05004], lr: 0.099217, loss: 2.8534
2022-03-01 16:39:22 - train: epoch 0017, iter [04700, 05004], lr: 0.099217, loss: 2.9392
2022-03-01 16:39:56 - train: epoch 0017, iter [04800, 05004], lr: 0.099217, loss: 2.8287
2022-03-01 16:40:29 - train: epoch 0017, iter [04900, 05004], lr: 0.099217, loss: 2.8350
2022-03-01 16:41:01 - train: epoch 0017, iter [05000, 05004], lr: 0.099217, loss: 2.8162
2022-03-01 16:41:02 - train: epoch 017, train_loss: 2.9042
2022-03-01 16:42:15 - eval: epoch: 017, acc1: 50.764%, acc5: 77.254%, test_loss: 2.0995, per_image_load_time: 2.242ms, per_image_inference_time: 0.532ms
2022-03-01 16:42:16 - until epoch: 017, best_acc1: 50.764%
2022-03-01 16:42:16 - epoch 018 lr: 0.09906850630697067
2022-03-01 16:42:54 - train: epoch 0018, iter [00100, 05004], lr: 0.099069, loss: 2.8703
2022-03-01 16:43:28 - train: epoch 0018, iter [00200, 05004], lr: 0.099069, loss: 2.9524
2022-03-01 16:44:03 - train: epoch 0018, iter [00300, 05004], lr: 0.099069, loss: 2.9773
2022-03-01 16:44:36 - train: epoch 0018, iter [00400, 05004], lr: 0.099069, loss: 2.9801
2022-03-01 16:45:10 - train: epoch 0018, iter [00500, 05004], lr: 0.099069, loss: 2.8179
2022-03-01 16:45:44 - train: epoch 0018, iter [00600, 05004], lr: 0.099069, loss: 3.0282
2022-03-01 16:46:19 - train: epoch 0018, iter [00700, 05004], lr: 0.099069, loss: 2.7965
2022-03-01 16:46:52 - train: epoch 0018, iter [00800, 05004], lr: 0.099069, loss: 2.8102
2022-03-01 16:47:25 - train: epoch 0018, iter [00900, 05004], lr: 0.099069, loss: 3.1795
2022-03-01 16:47:59 - train: epoch 0018, iter [01000, 05004], lr: 0.099069, loss: 2.7542
2022-03-01 16:48:32 - train: epoch 0018, iter [01100, 05004], lr: 0.099069, loss: 3.1744
2022-03-01 16:49:05 - train: epoch 0018, iter [01200, 05004], lr: 0.099069, loss: 2.7906
2022-03-01 16:49:40 - train: epoch 0018, iter [01300, 05004], lr: 0.099069, loss: 3.1221
2022-03-01 16:50:14 - train: epoch 0018, iter [01400, 05004], lr: 0.099069, loss: 2.9790
2022-03-01 16:50:49 - train: epoch 0018, iter [01500, 05004], lr: 0.099069, loss: 3.1514
2022-03-01 16:51:22 - train: epoch 0018, iter [01600, 05004], lr: 0.099069, loss: 2.8365
2022-03-01 16:51:57 - train: epoch 0018, iter [01700, 05004], lr: 0.099069, loss: 2.9219
2022-03-01 16:52:31 - train: epoch 0018, iter [01800, 05004], lr: 0.099069, loss: 2.6418
2022-03-01 16:53:05 - train: epoch 0018, iter [01900, 05004], lr: 0.099069, loss: 3.0629
2022-03-01 16:53:38 - train: epoch 0018, iter [02000, 05004], lr: 0.099069, loss: 3.1240
2022-03-01 16:54:12 - train: epoch 0018, iter [02100, 05004], lr: 0.099069, loss: 3.0442
2022-03-01 16:54:46 - train: epoch 0018, iter [02200, 05004], lr: 0.099069, loss: 2.9330
2022-03-01 16:55:20 - train: epoch 0018, iter [02300, 05004], lr: 0.099069, loss: 2.9787
2022-03-01 16:55:54 - train: epoch 0018, iter [02400, 05004], lr: 0.099069, loss: 2.8326
2022-03-01 16:56:28 - train: epoch 0018, iter [02500, 05004], lr: 0.099069, loss: 2.6660
2022-03-01 16:57:01 - train: epoch 0018, iter [02600, 05004], lr: 0.099069, loss: 2.7850
2022-03-01 16:57:36 - train: epoch 0018, iter [02700, 05004], lr: 0.099069, loss: 2.8980
2022-03-01 16:58:10 - train: epoch 0018, iter [02800, 05004], lr: 0.099069, loss: 2.6487
2022-03-01 16:58:43 - train: epoch 0018, iter [02900, 05004], lr: 0.099069, loss: 2.8844
2022-03-01 16:59:18 - train: epoch 0018, iter [03000, 05004], lr: 0.099069, loss: 2.9561
2022-03-01 16:59:51 - train: epoch 0018, iter [03100, 05004], lr: 0.099069, loss: 3.1061
2022-03-01 17:00:25 - train: epoch 0018, iter [03200, 05004], lr: 0.099069, loss: 2.8582
2022-03-01 17:00:58 - train: epoch 0018, iter [03300, 05004], lr: 0.099069, loss: 2.8308
2022-03-01 17:01:33 - train: epoch 0018, iter [03400, 05004], lr: 0.099069, loss: 2.8780
2022-03-01 17:02:08 - train: epoch 0018, iter [03500, 05004], lr: 0.099069, loss: 3.0616
2022-03-01 17:02:41 - train: epoch 0018, iter [03600, 05004], lr: 0.099069, loss: 2.9434
2022-03-01 17:03:16 - train: epoch 0018, iter [03700, 05004], lr: 0.099069, loss: 3.1217
2022-03-01 17:03:49 - train: epoch 0018, iter [03800, 05004], lr: 0.099069, loss: 3.1384
2022-03-01 17:04:23 - train: epoch 0018, iter [03900, 05004], lr: 0.099069, loss: 3.0243
2022-03-01 17:04:57 - train: epoch 0018, iter [04000, 05004], lr: 0.099069, loss: 2.7729
2022-03-01 17:05:32 - train: epoch 0018, iter [04100, 05004], lr: 0.099069, loss: 2.7889
2022-03-01 17:06:05 - train: epoch 0018, iter [04200, 05004], lr: 0.099069, loss: 2.8367
2022-03-01 17:06:38 - train: epoch 0018, iter [04300, 05004], lr: 0.099069, loss: 2.5046
2022-03-01 17:07:11 - train: epoch 0018, iter [04400, 05004], lr: 0.099069, loss: 3.0803
2022-03-01 17:07:45 - train: epoch 0018, iter [04500, 05004], lr: 0.099069, loss: 2.9125
2022-03-01 17:08:19 - train: epoch 0018, iter [04600, 05004], lr: 0.099069, loss: 2.6763
2022-03-01 17:08:54 - train: epoch 0018, iter [04700, 05004], lr: 0.099069, loss: 3.1205
2022-03-01 17:09:27 - train: epoch 0018, iter [04800, 05004], lr: 0.099069, loss: 2.9776
2022-03-01 17:10:02 - train: epoch 0018, iter [04900, 05004], lr: 0.099069, loss: 2.8323
2022-03-01 17:10:35 - train: epoch 0018, iter [05000, 05004], lr: 0.099069, loss: 3.1213
2022-03-01 17:10:36 - train: epoch 018, train_loss: 2.8904
2022-03-01 17:11:49 - eval: epoch: 018, acc1: 50.672%, acc5: 76.996%, test_loss: 2.1160, per_image_load_time: 2.084ms, per_image_inference_time: 0.531ms
2022-03-01 17:11:50 - until epoch: 018, best_acc1: 50.764%
2022-03-01 17:11:50 - epoch 019 lr: 0.09890738003669029
2022-03-01 17:12:29 - train: epoch 0019, iter [00100, 05004], lr: 0.098907, loss: 2.6681
2022-03-01 17:13:01 - train: epoch 0019, iter [00200, 05004], lr: 0.098907, loss: 3.1447
2022-03-01 17:13:36 - train: epoch 0019, iter [00300, 05004], lr: 0.098907, loss: 3.2193
2022-03-01 17:14:09 - train: epoch 0019, iter [00400, 05004], lr: 0.098907, loss: 2.7888
2022-03-01 17:14:43 - train: epoch 0019, iter [00500, 05004], lr: 0.098907, loss: 2.7192
2022-03-01 17:15:17 - train: epoch 0019, iter [00600, 05004], lr: 0.098907, loss: 2.8409
2022-03-01 17:15:52 - train: epoch 0019, iter [00700, 05004], lr: 0.098907, loss: 2.5980
2022-03-01 17:16:26 - train: epoch 0019, iter [00800, 05004], lr: 0.098907, loss: 3.0946
2022-03-01 17:17:00 - train: epoch 0019, iter [00900, 05004], lr: 0.098907, loss: 2.8419
2022-03-01 17:17:34 - train: epoch 0019, iter [01000, 05004], lr: 0.098907, loss: 3.0525
2022-03-01 17:18:08 - train: epoch 0019, iter [01100, 05004], lr: 0.098907, loss: 2.8736
2022-03-01 17:18:43 - train: epoch 0019, iter [01200, 05004], lr: 0.098907, loss: 3.0305
2022-03-01 17:19:16 - train: epoch 0019, iter [01300, 05004], lr: 0.098907, loss: 3.1940
2022-03-01 17:19:49 - train: epoch 0019, iter [01400, 05004], lr: 0.098907, loss: 2.9275
2022-03-01 17:20:23 - train: epoch 0019, iter [01500, 05004], lr: 0.098907, loss: 3.1123
2022-03-01 17:20:57 - train: epoch 0019, iter [01600, 05004], lr: 0.098907, loss: 2.7193
2022-03-01 17:21:31 - train: epoch 0019, iter [01700, 05004], lr: 0.098907, loss: 3.0436
2022-03-01 17:22:05 - train: epoch 0019, iter [01800, 05004], lr: 0.098907, loss: 3.0074
2022-03-01 17:22:39 - train: epoch 0019, iter [01900, 05004], lr: 0.098907, loss: 3.1390
2022-03-01 17:23:14 - train: epoch 0019, iter [02000, 05004], lr: 0.098907, loss: 3.0391
2022-03-01 17:23:48 - train: epoch 0019, iter [02100, 05004], lr: 0.098907, loss: 2.8394
2022-03-01 17:24:22 - train: epoch 0019, iter [02200, 05004], lr: 0.098907, loss: 2.7519
2022-03-01 17:24:57 - train: epoch 0019, iter [02300, 05004], lr: 0.098907, loss: 2.9352
2022-03-01 17:25:31 - train: epoch 0019, iter [02400, 05004], lr: 0.098907, loss: 2.9581
2022-03-01 17:26:04 - train: epoch 0019, iter [02500, 05004], lr: 0.098907, loss: 2.9017
2022-03-01 17:26:37 - train: epoch 0019, iter [02600, 05004], lr: 0.098907, loss: 2.8366
2022-03-01 17:27:12 - train: epoch 0019, iter [02700, 05004], lr: 0.098907, loss: 2.8994
2022-03-01 17:27:46 - train: epoch 0019, iter [02800, 05004], lr: 0.098907, loss: 2.7626
2022-03-01 17:28:20 - train: epoch 0019, iter [02900, 05004], lr: 0.098907, loss: 2.9629
2022-03-01 17:28:55 - train: epoch 0019, iter [03000, 05004], lr: 0.098907, loss: 3.1406
2022-03-01 17:29:28 - train: epoch 0019, iter [03100, 05004], lr: 0.098907, loss: 2.7022
2022-03-01 17:30:02 - train: epoch 0019, iter [03200, 05004], lr: 0.098907, loss: 2.6093
2022-03-01 17:30:37 - train: epoch 0019, iter [03300, 05004], lr: 0.098907, loss: 2.8200
2022-03-01 17:31:11 - train: epoch 0019, iter [03400, 05004], lr: 0.098907, loss: 2.9723
2022-03-01 17:31:45 - train: epoch 0019, iter [03500, 05004], lr: 0.098907, loss: 2.9317
2022-03-01 17:32:18 - train: epoch 0019, iter [03600, 05004], lr: 0.098907, loss: 2.9124
2022-03-01 17:32:51 - train: epoch 0019, iter [03700, 05004], lr: 0.098907, loss: 2.9495
2022-03-01 17:33:25 - train: epoch 0019, iter [03800, 05004], lr: 0.098907, loss: 3.1035
2022-03-01 17:33:59 - train: epoch 0019, iter [03900, 05004], lr: 0.098907, loss: 2.7268
2022-03-01 17:34:33 - train: epoch 0019, iter [04000, 05004], lr: 0.098907, loss: 2.6965
2022-03-01 17:35:06 - train: epoch 0019, iter [04100, 05004], lr: 0.098907, loss: 2.9975
2022-03-01 17:35:41 - train: epoch 0019, iter [04200, 05004], lr: 0.098907, loss: 2.7965
2022-03-01 17:36:15 - train: epoch 0019, iter [04300, 05004], lr: 0.098907, loss: 2.7186
2022-03-01 17:36:49 - train: epoch 0019, iter [04400, 05004], lr: 0.098907, loss: 2.9079
2022-03-01 17:37:23 - train: epoch 0019, iter [04500, 05004], lr: 0.098907, loss: 3.2726
2022-03-01 17:37:57 - train: epoch 0019, iter [04600, 05004], lr: 0.098907, loss: 2.7758
2022-03-01 17:38:31 - train: epoch 0019, iter [04700, 05004], lr: 0.098907, loss: 2.8636
2022-03-01 17:39:04 - train: epoch 0019, iter [04800, 05004], lr: 0.098907, loss: 2.9694
2022-03-01 17:39:36 - train: epoch 0019, iter [04900, 05004], lr: 0.098907, loss: 2.8211
2022-03-01 17:40:10 - train: epoch 0019, iter [05000, 05004], lr: 0.098907, loss: 2.9064
2022-03-01 17:40:11 - train: epoch 019, train_loss: 2.8834
2022-03-01 17:41:25 - eval: epoch: 019, acc1: 47.392%, acc5: 73.620%, test_loss: 2.3079, per_image_load_time: 2.369ms, per_image_inference_time: 0.507ms
2022-03-01 17:41:25 - until epoch: 019, best_acc1: 50.764%
2022-03-01 17:41:25 - epoch 020 lr: 0.0987335598531214
2022-03-01 17:42:04 - train: epoch 0020, iter [00100, 05004], lr: 0.098734, loss: 2.8415
2022-03-01 17:42:38 - train: epoch 0020, iter [00200, 05004], lr: 0.098734, loss: 2.7239
2022-03-01 17:43:12 - train: epoch 0020, iter [00300, 05004], lr: 0.098734, loss: 3.0816
2022-03-01 17:43:46 - train: epoch 0020, iter [00400, 05004], lr: 0.098734, loss: 2.5657
2022-03-01 17:44:20 - train: epoch 0020, iter [00500, 05004], lr: 0.098734, loss: 2.8271
2022-03-01 17:44:55 - train: epoch 0020, iter [00600, 05004], lr: 0.098734, loss: 3.0047
2022-03-01 17:45:28 - train: epoch 0020, iter [00700, 05004], lr: 0.098734, loss: 2.6884
2022-03-01 17:46:01 - train: epoch 0020, iter [00800, 05004], lr: 0.098734, loss: 2.8252
2022-03-01 17:46:35 - train: epoch 0020, iter [00900, 05004], lr: 0.098734, loss: 2.9975
2022-03-01 17:47:09 - train: epoch 0020, iter [01000, 05004], lr: 0.098734, loss: 2.8974
2022-03-01 17:47:43 - train: epoch 0020, iter [01100, 05004], lr: 0.098734, loss: 2.6198
2022-03-01 17:48:18 - train: epoch 0020, iter [01200, 05004], lr: 0.098734, loss: 2.7432
2022-03-01 17:48:51 - train: epoch 0020, iter [01300, 05004], lr: 0.098734, loss: 2.8441
2022-03-01 17:49:26 - train: epoch 0020, iter [01400, 05004], lr: 0.098734, loss: 2.8747
2022-03-01 17:50:00 - train: epoch 0020, iter [01500, 05004], lr: 0.098734, loss: 2.8292
2022-03-01 17:50:34 - train: epoch 0020, iter [01600, 05004], lr: 0.098734, loss: 2.7590
2022-03-01 17:51:09 - train: epoch 0020, iter [01700, 05004], lr: 0.098734, loss: 2.4993
2022-03-01 17:51:42 - train: epoch 0020, iter [01800, 05004], lr: 0.098734, loss: 2.7739
2022-03-01 17:52:16 - train: epoch 0020, iter [01900, 05004], lr: 0.098734, loss: 2.6229
2022-03-01 17:52:49 - train: epoch 0020, iter [02000, 05004], lr: 0.098734, loss: 2.9394
2022-03-01 17:53:23 - train: epoch 0020, iter [02100, 05004], lr: 0.098734, loss: 2.9412
2022-03-01 17:53:57 - train: epoch 0020, iter [02200, 05004], lr: 0.098734, loss: 2.6454
2022-03-01 17:54:31 - train: epoch 0020, iter [02300, 05004], lr: 0.098734, loss: 2.7535
2022-03-01 17:55:06 - train: epoch 0020, iter [02400, 05004], lr: 0.098734, loss: 3.1317
2022-03-01 17:55:40 - train: epoch 0020, iter [02500, 05004], lr: 0.098734, loss: 2.9620
2022-03-01 17:56:15 - train: epoch 0020, iter [02600, 05004], lr: 0.098734, loss: 2.6594
2022-03-01 17:56:49 - train: epoch 0020, iter [02700, 05004], lr: 0.098734, loss: 3.0630
2022-03-01 17:57:23 - train: epoch 0020, iter [02800, 05004], lr: 0.098734, loss: 2.9691
2022-03-01 17:57:57 - train: epoch 0020, iter [02900, 05004], lr: 0.098734, loss: 3.0209
2022-03-01 17:58:30 - train: epoch 0020, iter [03000, 05004], lr: 0.098734, loss: 2.5950
2022-03-01 17:59:03 - train: epoch 0020, iter [03100, 05004], lr: 0.098734, loss: 2.7666
2022-03-01 17:59:38 - train: epoch 0020, iter [03200, 05004], lr: 0.098734, loss: 3.1234
2022-03-01 18:00:13 - train: epoch 0020, iter [03300, 05004], lr: 0.098734, loss: 2.8093
2022-03-01 18:00:48 - train: epoch 0020, iter [03400, 05004], lr: 0.098734, loss: 2.8803
2022-03-01 18:01:22 - train: epoch 0020, iter [03500, 05004], lr: 0.098734, loss: 2.8570
2022-03-01 18:01:58 - train: epoch 0020, iter [03600, 05004], lr: 0.098734, loss: 2.8315
2022-03-01 18:02:32 - train: epoch 0020, iter [03700, 05004], lr: 0.098734, loss: 2.8324
2022-03-01 18:03:08 - train: epoch 0020, iter [03800, 05004], lr: 0.098734, loss: 2.7579
2022-03-01 18:03:43 - train: epoch 0020, iter [03900, 05004], lr: 0.098734, loss: 3.0741
2022-03-01 18:04:17 - train: epoch 0020, iter [04000, 05004], lr: 0.098734, loss: 2.7636
2022-03-01 18:04:52 - train: epoch 0020, iter [04100, 05004], lr: 0.098734, loss: 2.7120
2022-03-01 18:05:26 - train: epoch 0020, iter [04200, 05004], lr: 0.098734, loss: 2.8682
2022-03-01 18:06:02 - train: epoch 0020, iter [04300, 05004], lr: 0.098734, loss: 2.9438
2022-03-01 18:06:36 - train: epoch 0020, iter [04400, 05004], lr: 0.098734, loss: 2.8949
2022-03-01 18:07:11 - train: epoch 0020, iter [04500, 05004], lr: 0.098734, loss: 2.8640
2022-03-01 18:07:46 - train: epoch 0020, iter [04600, 05004], lr: 0.098734, loss: 3.0787
2022-03-01 18:08:20 - train: epoch 0020, iter [04700, 05004], lr: 0.098734, loss: 2.6738
2022-03-01 18:08:55 - train: epoch 0020, iter [04800, 05004], lr: 0.098734, loss: 2.9107
2022-03-01 18:09:30 - train: epoch 0020, iter [04900, 05004], lr: 0.098734, loss: 2.8622
2022-03-01 18:10:04 - train: epoch 0020, iter [05000, 05004], lr: 0.098734, loss: 2.7284
2022-03-01 18:10:05 - train: epoch 020, train_loss: 2.8670
2022-03-01 18:11:20 - eval: epoch: 020, acc1: 48.336%, acc5: 74.398%, test_loss: 2.2698, per_image_load_time: 2.430ms, per_image_inference_time: 0.497ms
2022-03-01 18:11:20 - until epoch: 020, best_acc1: 50.764%
2022-03-01 18:11:20 - epoch 021 lr: 0.0985470908713026
2022-03-01 18:11:58 - train: epoch 0021, iter [00100, 05004], lr: 0.098547, loss: 2.9449
2022-03-01 18:12:33 - train: epoch 0021, iter [00200, 05004], lr: 0.098547, loss: 3.0438
2022-03-01 18:13:08 - train: epoch 0021, iter [00300, 05004], lr: 0.098547, loss: 2.5954
2022-03-01 18:13:42 - train: epoch 0021, iter [00400, 05004], lr: 0.098547, loss: 2.9884
2022-03-01 18:14:16 - train: epoch 0021, iter [00500, 05004], lr: 0.098547, loss: 2.7436
2022-03-01 18:14:51 - train: epoch 0021, iter [00600, 05004], lr: 0.098547, loss: 2.6826
2022-03-01 18:15:26 - train: epoch 0021, iter [00700, 05004], lr: 0.098547, loss: 2.7358
2022-03-01 18:16:00 - train: epoch 0021, iter [00800, 05004], lr: 0.098547, loss: 2.9350
2022-03-01 18:16:34 - train: epoch 0021, iter [00900, 05004], lr: 0.098547, loss: 2.9519
2022-03-01 18:17:09 - train: epoch 0021, iter [01000, 05004], lr: 0.098547, loss: 2.7392
2022-03-01 18:17:44 - train: epoch 0021, iter [01100, 05004], lr: 0.098547, loss: 2.8415
2022-03-01 18:18:18 - train: epoch 0021, iter [01200, 05004], lr: 0.098547, loss: 2.7745
2022-03-01 18:18:52 - train: epoch 0021, iter [01300, 05004], lr: 0.098547, loss: 2.7761
2022-03-01 18:19:27 - train: epoch 0021, iter [01400, 05004], lr: 0.098547, loss: 2.6011
2022-03-01 18:20:02 - train: epoch 0021, iter [01500, 05004], lr: 0.098547, loss: 2.8040
2022-03-01 18:20:36 - train: epoch 0021, iter [01600, 05004], lr: 0.098547, loss: 2.8399
2022-03-01 18:21:11 - train: epoch 0021, iter [01700, 05004], lr: 0.098547, loss: 2.9254
2022-03-01 18:21:45 - train: epoch 0021, iter [01800, 05004], lr: 0.098547, loss: 2.6570
2022-03-01 18:22:20 - train: epoch 0021, iter [01900, 05004], lr: 0.098547, loss: 2.8592
2022-03-01 18:22:55 - train: epoch 0021, iter [02000, 05004], lr: 0.098547, loss: 3.2375
2022-03-01 18:23:29 - train: epoch 0021, iter [02100, 05004], lr: 0.098547, loss: 2.8256
2022-03-01 18:24:03 - train: epoch 0021, iter [02200, 05004], lr: 0.098547, loss: 2.9606
2022-03-01 18:24:38 - train: epoch 0021, iter [02300, 05004], lr: 0.098547, loss: 2.9034
2022-03-01 18:25:12 - train: epoch 0021, iter [02400, 05004], lr: 0.098547, loss: 2.5971
2022-03-01 18:25:46 - train: epoch 0021, iter [02500, 05004], lr: 0.098547, loss: 2.9483
2022-03-01 18:26:21 - train: epoch 0021, iter [02600, 05004], lr: 0.098547, loss: 3.0855
2022-03-01 18:26:56 - train: epoch 0021, iter [02700, 05004], lr: 0.098547, loss: 2.7037
2022-03-01 18:27:30 - train: epoch 0021, iter [02800, 05004], lr: 0.098547, loss: 2.7395
2022-03-01 18:28:05 - train: epoch 0021, iter [02900, 05004], lr: 0.098547, loss: 2.8103
2022-03-01 18:28:39 - train: epoch 0021, iter [03000, 05004], lr: 0.098547, loss: 3.2579
2022-03-01 18:29:14 - train: epoch 0021, iter [03100, 05004], lr: 0.098547, loss: 2.8607
2022-03-01 18:29:48 - train: epoch 0021, iter [03200, 05004], lr: 0.098547, loss: 2.9366
2022-03-01 18:30:22 - train: epoch 0021, iter [03300, 05004], lr: 0.098547, loss: 3.1267
2022-03-01 18:30:57 - train: epoch 0021, iter [03400, 05004], lr: 0.098547, loss: 3.0012
2022-03-01 18:31:31 - train: epoch 0021, iter [03500, 05004], lr: 0.098547, loss: 2.7207
2022-03-01 18:32:06 - train: epoch 0021, iter [03600, 05004], lr: 0.098547, loss: 2.7504
2022-03-01 18:32:40 - train: epoch 0021, iter [03700, 05004], lr: 0.098547, loss: 2.9841
2022-03-01 18:33:15 - train: epoch 0021, iter [03800, 05004], lr: 0.098547, loss: 2.7063
2022-03-01 18:33:51 - train: epoch 0021, iter [03900, 05004], lr: 0.098547, loss: 2.8850
2022-03-01 18:34:24 - train: epoch 0021, iter [04000, 05004], lr: 0.098547, loss: 3.0335
2022-03-01 18:35:00 - train: epoch 0021, iter [04100, 05004], lr: 0.098547, loss: 2.7408
2022-03-01 18:35:33 - train: epoch 0021, iter [04200, 05004], lr: 0.098547, loss: 2.8223
2022-03-01 18:36:07 - train: epoch 0021, iter [04300, 05004], lr: 0.098547, loss: 2.7451
2022-03-01 18:36:39 - train: epoch 0021, iter [04400, 05004], lr: 0.098547, loss: 2.9200
2022-03-01 18:37:14 - train: epoch 0021, iter [04500, 05004], lr: 0.098547, loss: 2.9296
2022-03-01 18:37:49 - train: epoch 0021, iter [04600, 05004], lr: 0.098547, loss: 2.5939
2022-03-01 18:38:24 - train: epoch 0021, iter [04700, 05004], lr: 0.098547, loss: 3.0523
2022-03-01 18:38:58 - train: epoch 0021, iter [04800, 05004], lr: 0.098547, loss: 2.9909
2022-03-01 18:39:33 - train: epoch 0021, iter [04900, 05004], lr: 0.098547, loss: 2.5799
2022-03-01 18:40:06 - train: epoch 0021, iter [05000, 05004], lr: 0.098547, loss: 2.6369
2022-03-01 18:40:07 - train: epoch 021, train_loss: 2.8619
2022-03-01 18:41:21 - eval: epoch: 021, acc1: 51.214%, acc5: 77.278%, test_loss: 2.0896, per_image_load_time: 2.227ms, per_image_inference_time: 0.565ms
2022-03-01 18:41:22 - until epoch: 021, best_acc1: 51.214%
2022-03-01 18:41:22 - epoch 022 lr: 0.09834802148926883
2022-03-01 18:42:01 - train: epoch 0022, iter [00100, 05004], lr: 0.098348, loss: 2.6184
2022-03-01 18:42:35 - train: epoch 0022, iter [00200, 05004], lr: 0.098348, loss: 2.7779
2022-03-01 18:43:10 - train: epoch 0022, iter [00300, 05004], lr: 0.098348, loss: 2.5819
2022-03-01 18:43:44 - train: epoch 0022, iter [00400, 05004], lr: 0.098348, loss: 2.6959
2022-03-01 18:44:18 - train: epoch 0022, iter [00500, 05004], lr: 0.098348, loss: 2.8581
2022-03-01 18:44:52 - train: epoch 0022, iter [00600, 05004], lr: 0.098348, loss: 3.0208
2022-03-01 18:45:27 - train: epoch 0022, iter [00700, 05004], lr: 0.098348, loss: 2.8825
2022-03-01 18:46:02 - train: epoch 0022, iter [00800, 05004], lr: 0.098348, loss: 2.9834
2022-03-01 18:46:36 - train: epoch 0022, iter [00900, 05004], lr: 0.098348, loss: 3.0519
2022-03-01 18:47:11 - train: epoch 0022, iter [01000, 05004], lr: 0.098348, loss: 2.9717
2022-03-01 18:47:45 - train: epoch 0022, iter [01100, 05004], lr: 0.098348, loss: 3.0072
2022-03-01 18:48:20 - train: epoch 0022, iter [01200, 05004], lr: 0.098348, loss: 2.5746
2022-03-01 18:48:55 - train: epoch 0022, iter [01300, 05004], lr: 0.098348, loss: 2.8472
2022-03-01 18:49:29 - train: epoch 0022, iter [01400, 05004], lr: 0.098348, loss: 2.9232
2022-03-01 18:50:03 - train: epoch 0022, iter [01500, 05004], lr: 0.098348, loss: 2.8230
2022-03-01 18:50:37 - train: epoch 0022, iter [01600, 05004], lr: 0.098348, loss: 2.7360
2022-03-01 18:51:13 - train: epoch 0022, iter [01700, 05004], lr: 0.098348, loss: 2.9971
2022-03-01 18:51:48 - train: epoch 0022, iter [01800, 05004], lr: 0.098348, loss: 3.2095
2022-03-01 18:52:22 - train: epoch 0022, iter [01900, 05004], lr: 0.098348, loss: 2.6900
2022-03-01 18:52:57 - train: epoch 0022, iter [02000, 05004], lr: 0.098348, loss: 2.7991
2022-03-01 18:53:31 - train: epoch 0022, iter [02100, 05004], lr: 0.098348, loss: 2.9809
2022-03-01 18:54:07 - train: epoch 0022, iter [02200, 05004], lr: 0.098348, loss: 2.6757
2022-03-01 18:54:42 - train: epoch 0022, iter [02300, 05004], lr: 0.098348, loss: 2.8466
2022-03-01 18:55:17 - train: epoch 0022, iter [02400, 05004], lr: 0.098348, loss: 2.7182
2022-03-01 18:55:51 - train: epoch 0022, iter [02500, 05004], lr: 0.098348, loss: 2.8255
2022-03-01 18:56:26 - train: epoch 0022, iter [02600, 05004], lr: 0.098348, loss: 2.7107
2022-03-01 18:57:00 - train: epoch 0022, iter [02700, 05004], lr: 0.098348, loss: 2.6307
2022-03-01 18:57:35 - train: epoch 0022, iter [02800, 05004], lr: 0.098348, loss: 2.9063
2022-03-01 18:58:09 - train: epoch 0022, iter [02900, 05004], lr: 0.098348, loss: 2.6143
2022-03-01 18:58:44 - train: epoch 0022, iter [03000, 05004], lr: 0.098348, loss: 2.5947
2022-03-01 18:59:19 - train: epoch 0022, iter [03100, 05004], lr: 0.098348, loss: 2.9866
2022-03-01 18:59:53 - train: epoch 0022, iter [03200, 05004], lr: 0.098348, loss: 2.9807
2022-03-01 19:00:28 - train: epoch 0022, iter [03300, 05004], lr: 0.098348, loss: 2.8340
2022-03-01 19:01:02 - train: epoch 0022, iter [03400, 05004], lr: 0.098348, loss: 2.6669
2022-03-01 19:01:36 - train: epoch 0022, iter [03500, 05004], lr: 0.098348, loss: 2.9390
2022-03-01 19:02:11 - train: epoch 0022, iter [03600, 05004], lr: 0.098348, loss: 2.8958
2022-03-01 19:02:46 - train: epoch 0022, iter [03700, 05004], lr: 0.098348, loss: 2.9386
2022-03-01 19:03:21 - train: epoch 0022, iter [03800, 05004], lr: 0.098348, loss: 2.9201
2022-03-01 19:03:55 - train: epoch 0022, iter [03900, 05004], lr: 0.098348, loss: 2.7517
2022-03-01 19:04:30 - train: epoch 0022, iter [04000, 05004], lr: 0.098348, loss: 2.9371
2022-03-01 19:05:04 - train: epoch 0022, iter [04100, 05004], lr: 0.098348, loss: 2.8766
2022-03-01 19:05:39 - train: epoch 0022, iter [04200, 05004], lr: 0.098348, loss: 2.8413
2022-03-01 19:06:14 - train: epoch 0022, iter [04300, 05004], lr: 0.098348, loss: 3.1400
2022-03-01 19:06:48 - train: epoch 0022, iter [04400, 05004], lr: 0.098348, loss: 2.9391
2022-03-01 19:07:22 - train: epoch 0022, iter [04500, 05004], lr: 0.098348, loss: 2.7191
2022-03-01 19:07:56 - train: epoch 0022, iter [04600, 05004], lr: 0.098348, loss: 3.0131
2022-03-01 19:08:31 - train: epoch 0022, iter [04700, 05004], lr: 0.098348, loss: 3.1386
2022-03-01 19:09:06 - train: epoch 0022, iter [04800, 05004], lr: 0.098348, loss: 2.8099
2022-03-01 19:09:41 - train: epoch 0022, iter [04900, 05004], lr: 0.098348, loss: 2.7240
2022-03-01 19:10:14 - train: epoch 0022, iter [05000, 05004], lr: 0.098348, loss: 2.7073
2022-03-01 19:10:15 - train: epoch 022, train_loss: 2.8544
2022-03-01 19:11:30 - eval: epoch: 022, acc1: 50.190%, acc5: 76.502%, test_loss: 2.1370, per_image_load_time: 2.409ms, per_image_inference_time: 0.489ms
2022-03-01 19:11:31 - until epoch: 022, best_acc1: 51.214%
2022-03-01 19:11:31 - epoch 023 lr: 0.09813640337548954
2022-03-01 19:12:10 - train: epoch 0023, iter [00100, 05004], lr: 0.098136, loss: 2.9077
2022-03-01 19:12:44 - train: epoch 0023, iter [00200, 05004], lr: 0.098136, loss: 2.6313
2022-03-01 19:13:19 - train: epoch 0023, iter [00300, 05004], lr: 0.098136, loss: 2.6812
2022-03-01 19:13:53 - train: epoch 0023, iter [00400, 05004], lr: 0.098136, loss: 2.9650
2022-03-01 19:14:28 - train: epoch 0023, iter [00500, 05004], lr: 0.098136, loss: 3.0768
2022-03-01 19:15:02 - train: epoch 0023, iter [00600, 05004], lr: 0.098136, loss: 2.7644
2022-03-01 19:15:37 - train: epoch 0023, iter [00700, 05004], lr: 0.098136, loss: 2.6571
2022-03-01 19:16:11 - train: epoch 0023, iter [00800, 05004], lr: 0.098136, loss: 2.7964
2022-03-01 19:16:46 - train: epoch 0023, iter [00900, 05004], lr: 0.098136, loss: 2.9894
2022-03-01 19:17:20 - train: epoch 0023, iter [01000, 05004], lr: 0.098136, loss: 2.8061
2022-03-01 19:17:55 - train: epoch 0023, iter [01100, 05004], lr: 0.098136, loss: 3.0570
2022-03-01 19:18:30 - train: epoch 0023, iter [01200, 05004], lr: 0.098136, loss: 2.5324
2022-03-01 19:19:04 - train: epoch 0023, iter [01300, 05004], lr: 0.098136, loss: 2.8393
2022-03-01 19:19:40 - train: epoch 0023, iter [01400, 05004], lr: 0.098136, loss: 2.8590
2022-03-01 19:20:14 - train: epoch 0023, iter [01500, 05004], lr: 0.098136, loss: 2.8200
2022-03-01 19:20:48 - train: epoch 0023, iter [01600, 05004], lr: 0.098136, loss: 2.8658
2022-03-01 19:21:22 - train: epoch 0023, iter [01700, 05004], lr: 0.098136, loss: 3.0478
2022-03-01 19:21:57 - train: epoch 0023, iter [01800, 05004], lr: 0.098136, loss: 2.7394
2022-03-01 19:22:31 - train: epoch 0023, iter [01900, 05004], lr: 0.098136, loss: 2.9813
2022-03-01 19:23:06 - train: epoch 0023, iter [02000, 05004], lr: 0.098136, loss: 2.5642
2022-03-01 19:23:41 - train: epoch 0023, iter [02100, 05004], lr: 0.098136, loss: 2.9523
2022-03-01 19:24:16 - train: epoch 0023, iter [02200, 05004], lr: 0.098136, loss: 2.6248
2022-03-01 19:24:50 - train: epoch 0023, iter [02300, 05004], lr: 0.098136, loss: 2.6768
2022-03-01 19:25:24 - train: epoch 0023, iter [02400, 05004], lr: 0.098136, loss: 2.6778
2022-03-01 19:25:59 - train: epoch 0023, iter [02500, 05004], lr: 0.098136, loss: 2.9305
2022-03-01 19:26:33 - train: epoch 0023, iter [02600, 05004], lr: 0.098136, loss: 3.0479
2022-03-01 19:27:08 - train: epoch 0023, iter [02700, 05004], lr: 0.098136, loss: 2.8411
2022-03-01 19:27:43 - train: epoch 0023, iter [02800, 05004], lr: 0.098136, loss: 2.8326
2022-03-01 19:28:16 - train: epoch 0023, iter [02900, 05004], lr: 0.098136, loss: 2.8142
2022-03-01 19:28:50 - train: epoch 0023, iter [03000, 05004], lr: 0.098136, loss: 3.0858
2022-03-01 19:29:25 - train: epoch 0023, iter [03100, 05004], lr: 0.098136, loss: 2.8213
2022-03-01 19:30:00 - train: epoch 0023, iter [03200, 05004], lr: 0.098136, loss: 2.9819
2022-03-01 19:30:34 - train: epoch 0023, iter [03300, 05004], lr: 0.098136, loss: 2.9491
2022-03-01 19:31:10 - train: epoch 0023, iter [03400, 05004], lr: 0.098136, loss: 2.9925
2022-03-01 19:31:44 - train: epoch 0023, iter [03500, 05004], lr: 0.098136, loss: 2.5299
2022-03-01 19:32:18 - train: epoch 0023, iter [03600, 05004], lr: 0.098136, loss: 2.6594
2022-03-01 19:32:52 - train: epoch 0023, iter [03700, 05004], lr: 0.098136, loss: 2.8699
2022-03-01 19:33:27 - train: epoch 0023, iter [03800, 05004], lr: 0.098136, loss: 2.9418
2022-03-01 19:34:02 - train: epoch 0023, iter [03900, 05004], lr: 0.098136, loss: 2.7579
2022-03-01 19:34:37 - train: epoch 0023, iter [04000, 05004], lr: 0.098136, loss: 2.7901
2022-03-01 19:35:11 - train: epoch 0023, iter [04100, 05004], lr: 0.098136, loss: 2.6265
2022-03-01 19:35:46 - train: epoch 0023, iter [04200, 05004], lr: 0.098136, loss: 2.7465
2022-03-01 19:36:20 - train: epoch 0023, iter [04300, 05004], lr: 0.098136, loss: 2.7749
2022-03-01 19:36:55 - train: epoch 0023, iter [04400, 05004], lr: 0.098136, loss: 2.6273
2022-03-01 19:37:29 - train: epoch 0023, iter [04500, 05004], lr: 0.098136, loss: 2.6483
2022-03-01 19:38:04 - train: epoch 0023, iter [04600, 05004], lr: 0.098136, loss: 2.7215
2022-03-01 19:38:38 - train: epoch 0023, iter [04700, 05004], lr: 0.098136, loss: 2.7169
2022-03-01 19:39:12 - train: epoch 0023, iter [04800, 05004], lr: 0.098136, loss: 2.8356
2022-03-01 19:39:47 - train: epoch 0023, iter [04900, 05004], lr: 0.098136, loss: 2.9182
2022-03-01 19:40:20 - train: epoch 0023, iter [05000, 05004], lr: 0.098136, loss: 2.9741
2022-03-01 19:40:21 - train: epoch 023, train_loss: 2.8471
2022-03-01 19:41:36 - eval: epoch: 023, acc1: 51.830%, acc5: 77.714%, test_loss: 2.0636, per_image_load_time: 2.347ms, per_image_inference_time: 0.545ms
2022-03-01 19:41:37 - until epoch: 023, best_acc1: 51.830%
2022-03-01 19:41:37 - epoch 024 lr: 0.09791229145545832
2022-03-01 19:42:16 - train: epoch 0024, iter [00100, 05004], lr: 0.097912, loss: 2.7302
2022-03-01 19:42:51 - train: epoch 0024, iter [00200, 05004], lr: 0.097912, loss: 2.7839
2022-03-01 19:43:25 - train: epoch 0024, iter [00300, 05004], lr: 0.097912, loss: 2.9762
2022-03-01 19:43:59 - train: epoch 0024, iter [00400, 05004], lr: 0.097912, loss: 2.9954
2022-03-01 19:44:35 - train: epoch 0024, iter [00500, 05004], lr: 0.097912, loss: 2.8641
2022-03-01 19:45:09 - train: epoch 0024, iter [00600, 05004], lr: 0.097912, loss: 2.5387
2022-03-01 19:45:44 - train: epoch 0024, iter [00700, 05004], lr: 0.097912, loss: 2.7091
2022-03-01 19:46:18 - train: epoch 0024, iter [00800, 05004], lr: 0.097912, loss: 2.8160
2022-03-01 19:46:53 - train: epoch 0024, iter [00900, 05004], lr: 0.097912, loss: 2.7044
2022-03-01 19:47:27 - train: epoch 0024, iter [01000, 05004], lr: 0.097912, loss: 2.9301
2022-03-01 19:48:02 - train: epoch 0024, iter [01100, 05004], lr: 0.097912, loss: 2.7327
2022-03-01 19:48:36 - train: epoch 0024, iter [01200, 05004], lr: 0.097912, loss: 2.7811
2022-03-01 19:49:12 - train: epoch 0024, iter [01300, 05004], lr: 0.097912, loss: 3.1673
2022-03-01 19:49:46 - train: epoch 0024, iter [01400, 05004], lr: 0.097912, loss: 2.6645
2022-03-01 19:50:21 - train: epoch 0024, iter [01500, 05004], lr: 0.097912, loss: 2.9323
2022-03-01 19:50:55 - train: epoch 0024, iter [01600, 05004], lr: 0.097912, loss: 2.8656
2022-03-01 19:51:30 - train: epoch 0024, iter [01700, 05004], lr: 0.097912, loss: 2.7612
2022-03-01 19:52:05 - train: epoch 0024, iter [01800, 05004], lr: 0.097912, loss: 3.0481
2022-03-01 19:52:39 - train: epoch 0024, iter [01900, 05004], lr: 0.097912, loss: 2.6992
2022-03-01 19:53:14 - train: epoch 0024, iter [02000, 05004], lr: 0.097912, loss: 2.7861
2022-03-01 19:53:49 - train: epoch 0024, iter [02100, 05004], lr: 0.097912, loss: 2.9307
2022-03-01 19:54:24 - train: epoch 0024, iter [02200, 05004], lr: 0.097912, loss: 2.7337
2022-03-01 19:54:58 - train: epoch 0024, iter [02300, 05004], lr: 0.097912, loss: 3.0440
2022-03-01 19:55:33 - train: epoch 0024, iter [02400, 05004], lr: 0.097912, loss: 2.8028
2022-03-01 19:56:07 - train: epoch 0024, iter [02500, 05004], lr: 0.097912, loss: 2.9196
2022-03-01 19:56:42 - train: epoch 0024, iter [02600, 05004], lr: 0.097912, loss: 2.7515
2022-03-01 19:57:16 - train: epoch 0024, iter [02700, 05004], lr: 0.097912, loss: 2.8399
2022-03-01 19:57:51 - train: epoch 0024, iter [02800, 05004], lr: 0.097912, loss: 2.9719
2022-03-01 19:58:25 - train: epoch 0024, iter [02900, 05004], lr: 0.097912, loss: 2.6540
2022-03-01 19:58:59 - train: epoch 0024, iter [03000, 05004], lr: 0.097912, loss: 2.7916
2022-03-01 19:59:33 - train: epoch 0024, iter [03100, 05004], lr: 0.097912, loss: 2.7519
2022-03-01 20:00:08 - train: epoch 0024, iter [03200, 05004], lr: 0.097912, loss: 2.8197
2022-03-01 20:00:42 - train: epoch 0024, iter [03300, 05004], lr: 0.097912, loss: 2.6102
2022-03-01 20:01:17 - train: epoch 0024, iter [03400, 05004], lr: 0.097912, loss: 2.7810
2022-03-01 20:01:51 - train: epoch 0024, iter [03500, 05004], lr: 0.097912, loss: 2.7429
2022-03-01 20:02:26 - train: epoch 0024, iter [03600, 05004], lr: 0.097912, loss: 2.7930
2022-03-01 20:03:00 - train: epoch 0024, iter [03700, 05004], lr: 0.097912, loss: 2.8765
2022-03-01 20:03:35 - train: epoch 0024, iter [03800, 05004], lr: 0.097912, loss: 3.0491
2022-03-01 20:04:09 - train: epoch 0024, iter [03900, 05004], lr: 0.097912, loss: 2.7148
2022-03-01 20:04:43 - train: epoch 0024, iter [04000, 05004], lr: 0.097912, loss: 2.7715
2022-03-01 20:05:18 - train: epoch 0024, iter [04100, 05004], lr: 0.097912, loss: 2.8389
2022-03-01 20:05:53 - train: epoch 0024, iter [04200, 05004], lr: 0.097912, loss: 2.9847
2022-03-01 20:06:26 - train: epoch 0024, iter [04300, 05004], lr: 0.097912, loss: 2.8544
2022-03-01 20:07:02 - train: epoch 0024, iter [04400, 05004], lr: 0.097912, loss: 3.0037
2022-03-01 20:07:36 - train: epoch 0024, iter [04500, 05004], lr: 0.097912, loss: 2.5666
2022-03-01 20:08:11 - train: epoch 0024, iter [04600, 05004], lr: 0.097912, loss: 2.8211
2022-03-01 20:08:45 - train: epoch 0024, iter [04700, 05004], lr: 0.097912, loss: 2.6961
2022-03-01 20:09:20 - train: epoch 0024, iter [04800, 05004], lr: 0.097912, loss: 2.8049
2022-03-01 20:09:55 - train: epoch 0024, iter [04900, 05004], lr: 0.097912, loss: 2.8440
2022-03-01 20:10:27 - train: epoch 0024, iter [05000, 05004], lr: 0.097912, loss: 2.9652
2022-03-01 20:10:28 - train: epoch 024, train_loss: 2.8404
2022-03-01 20:11:44 - eval: epoch: 024, acc1: 52.548%, acc5: 78.588%, test_loss: 2.0172, per_image_load_time: 1.481ms, per_image_inference_time: 0.525ms
2022-03-01 20:11:45 - until epoch: 024, best_acc1: 52.548%
2022-03-01 20:11:45 - epoch 025 lr: 0.09767574389743683
2022-03-01 20:12:23 - train: epoch 0025, iter [00100, 05004], lr: 0.097676, loss: 2.6553
2022-03-01 20:12:58 - train: epoch 0025, iter [00200, 05004], lr: 0.097676, loss: 2.5656
2022-03-01 20:13:32 - train: epoch 0025, iter [00300, 05004], lr: 0.097676, loss: 2.6604
2022-03-01 20:14:06 - train: epoch 0025, iter [00400, 05004], lr: 0.097676, loss: 2.7351
2022-03-01 20:14:41 - train: epoch 0025, iter [00500, 05004], lr: 0.097676, loss: 2.5071
2022-03-01 20:15:16 - train: epoch 0025, iter [00600, 05004], lr: 0.097676, loss: 2.7574
2022-03-01 20:15:51 - train: epoch 0025, iter [00700, 05004], lr: 0.097676, loss: 2.9494
2022-03-01 20:16:24 - train: epoch 0025, iter [00800, 05004], lr: 0.097676, loss: 2.6899
2022-03-01 20:17:00 - train: epoch 0025, iter [00900, 05004], lr: 0.097676, loss: 2.7576
2022-03-01 20:17:35 - train: epoch 0025, iter [01000, 05004], lr: 0.097676, loss: 2.7270
2022-03-01 20:18:08 - train: epoch 0025, iter [01100, 05004], lr: 0.097676, loss: 2.8290
2022-03-01 20:18:44 - train: epoch 0025, iter [01200, 05004], lr: 0.097676, loss: 2.8618
2022-03-01 20:19:18 - train: epoch 0025, iter [01300, 05004], lr: 0.097676, loss: 2.8711
2022-03-01 20:19:53 - train: epoch 0025, iter [01400, 05004], lr: 0.097676, loss: 2.8349
2022-03-01 20:20:27 - train: epoch 0025, iter [01500, 05004], lr: 0.097676, loss: 2.8530
2022-03-01 20:21:02 - train: epoch 0025, iter [01600, 05004], lr: 0.097676, loss: 2.6003
2022-03-01 20:21:37 - train: epoch 0025, iter [01700, 05004], lr: 0.097676, loss: 2.8170
2022-03-01 20:22:11 - train: epoch 0025, iter [01800, 05004], lr: 0.097676, loss: 2.8302
2022-03-01 20:22:45 - train: epoch 0025, iter [01900, 05004], lr: 0.097676, loss: 2.7611
2022-03-01 20:23:20 - train: epoch 0025, iter [02000, 05004], lr: 0.097676, loss: 2.8199
2022-03-01 20:23:55 - train: epoch 0025, iter [02100, 05004], lr: 0.097676, loss: 2.7508
2022-03-01 20:24:29 - train: epoch 0025, iter [02200, 05004], lr: 0.097676, loss: 2.6330
2022-03-01 20:25:05 - train: epoch 0025, iter [02300, 05004], lr: 0.097676, loss: 2.8548
2022-03-01 20:25:40 - train: epoch 0025, iter [02400, 05004], lr: 0.097676, loss: 2.5956
2022-03-01 20:26:14 - train: epoch 0025, iter [02500, 05004], lr: 0.097676, loss: 2.9564
2022-03-01 20:26:48 - train: epoch 0025, iter [02600, 05004], lr: 0.097676, loss: 3.0079
2022-03-01 20:27:23 - train: epoch 0025, iter [02700, 05004], lr: 0.097676, loss: 2.7841
2022-03-01 20:27:58 - train: epoch 0025, iter [02800, 05004], lr: 0.097676, loss: 2.9272
2022-03-01 20:28:33 - train: epoch 0025, iter [02900, 05004], lr: 0.097676, loss: 2.9038
2022-03-01 20:29:07 - train: epoch 0025, iter [03000, 05004], lr: 0.097676, loss: 2.9295
2022-03-01 20:29:42 - train: epoch 0025, iter [03100, 05004], lr: 0.097676, loss: 2.9092
2022-03-01 20:30:16 - train: epoch 0025, iter [03200, 05004], lr: 0.097676, loss: 2.9419
2022-03-01 20:30:51 - train: epoch 0025, iter [03300, 05004], lr: 0.097676, loss: 2.7920
2022-03-01 20:31:26 - train: epoch 0025, iter [03400, 05004], lr: 0.097676, loss: 3.0890
2022-03-01 20:31:59 - train: epoch 0025, iter [03500, 05004], lr: 0.097676, loss: 2.5294
2022-03-01 20:32:34 - train: epoch 0025, iter [03600, 05004], lr: 0.097676, loss: 2.8942
2022-03-01 20:33:09 - train: epoch 0025, iter [03700, 05004], lr: 0.097676, loss: 2.8869
2022-03-01 20:33:43 - train: epoch 0025, iter [03800, 05004], lr: 0.097676, loss: 2.9145
2022-03-01 20:34:17 - train: epoch 0025, iter [03900, 05004], lr: 0.097676, loss: 3.0709
2022-03-01 20:34:52 - train: epoch 0025, iter [04000, 05004], lr: 0.097676, loss: 2.9497
2022-03-01 20:35:27 - train: epoch 0025, iter [04100, 05004], lr: 0.097676, loss: 2.9254
2022-03-01 20:36:00 - train: epoch 0025, iter [04200, 05004], lr: 0.097676, loss: 2.9380
2022-03-01 20:36:34 - train: epoch 0025, iter [04300, 05004], lr: 0.097676, loss: 2.6937
2022-03-01 20:37:11 - train: epoch 0025, iter [04400, 05004], lr: 0.097676, loss: 2.7579
2022-03-01 20:37:44 - train: epoch 0025, iter [04500, 05004], lr: 0.097676, loss: 2.7385
2022-03-01 20:38:19 - train: epoch 0025, iter [04600, 05004], lr: 0.097676, loss: 2.7810
2022-03-01 20:38:54 - train: epoch 0025, iter [04700, 05004], lr: 0.097676, loss: 2.8171
2022-03-01 20:39:28 - train: epoch 0025, iter [04800, 05004], lr: 0.097676, loss: 2.5810
2022-03-01 20:40:03 - train: epoch 0025, iter [04900, 05004], lr: 0.097676, loss: 2.8825
2022-03-01 20:40:36 - train: epoch 0025, iter [05000, 05004], lr: 0.097676, loss: 2.9335
2022-03-01 20:40:37 - train: epoch 025, train_loss: 2.8312
2022-03-01 20:41:51 - eval: epoch: 025, acc1: 50.724%, acc5: 76.460%, test_loss: 2.1371, per_image_load_time: 1.637ms, per_image_inference_time: 0.546ms
2022-03-01 20:41:52 - until epoch: 025, best_acc1: 52.548%
2022-03-01 20:41:52 - epoch 026 lr: 0.09742682209735727
2022-03-01 20:42:31 - train: epoch 0026, iter [00100, 05004], lr: 0.097427, loss: 2.7125
2022-03-01 20:43:05 - train: epoch 0026, iter [00200, 05004], lr: 0.097427, loss: 2.5963
2022-03-01 20:43:40 - train: epoch 0026, iter [00300, 05004], lr: 0.097427, loss: 2.8390
2022-03-01 20:44:14 - train: epoch 0026, iter [00400, 05004], lr: 0.097427, loss: 2.6885
2022-03-01 20:44:47 - train: epoch 0026, iter [00500, 05004], lr: 0.097427, loss: 2.9967
2022-03-01 20:45:22 - train: epoch 0026, iter [00600, 05004], lr: 0.097427, loss: 2.9343
2022-03-01 20:45:57 - train: epoch 0026, iter [00700, 05004], lr: 0.097427, loss: 2.6924
2022-03-01 20:46:31 - train: epoch 0026, iter [00800, 05004], lr: 0.097427, loss: 2.7062
2022-03-01 20:47:05 - train: epoch 0026, iter [00900, 05004], lr: 0.097427, loss: 3.1459
2022-03-01 20:47:40 - train: epoch 0026, iter [01000, 05004], lr: 0.097427, loss: 2.7128
2022-03-01 20:48:15 - train: epoch 0026, iter [01100, 05004], lr: 0.097427, loss: 2.7945
2022-03-01 20:48:49 - train: epoch 0026, iter [01200, 05004], lr: 0.097427, loss: 2.9912
2022-03-01 20:49:24 - train: epoch 0026, iter [01300, 05004], lr: 0.097427, loss: 2.8521
2022-03-01 20:49:59 - train: epoch 0026, iter [01400, 05004], lr: 0.097427, loss: 2.9894
2022-03-01 20:50:34 - train: epoch 0026, iter [01500, 05004], lr: 0.097427, loss: 2.7698
2022-03-01 20:51:08 - train: epoch 0026, iter [01600, 05004], lr: 0.097427, loss: 2.9422
2022-03-01 20:51:42 - train: epoch 0026, iter [01700, 05004], lr: 0.097427, loss: 2.6809
2022-03-01 20:52:17 - train: epoch 0026, iter [01800, 05004], lr: 0.097427, loss: 2.8727
2022-03-01 20:52:52 - train: epoch 0026, iter [01900, 05004], lr: 0.097427, loss: 2.8549
2022-03-01 20:53:26 - train: epoch 0026, iter [02000, 05004], lr: 0.097427, loss: 2.9385
2022-03-01 20:54:00 - train: epoch 0026, iter [02100, 05004], lr: 0.097427, loss: 3.0007
2022-03-01 20:54:35 - train: epoch 0026, iter [02200, 05004], lr: 0.097427, loss: 2.8210
2022-03-01 20:55:10 - train: epoch 0026, iter [02300, 05004], lr: 0.097427, loss: 2.8114
2022-03-01 20:55:45 - train: epoch 0026, iter [02400, 05004], lr: 0.097427, loss: 3.0555
2022-03-01 20:56:18 - train: epoch 0026, iter [02500, 05004], lr: 0.097427, loss: 3.0542
2022-03-01 20:56:53 - train: epoch 0026, iter [02600, 05004], lr: 0.097427, loss: 3.0285
2022-03-01 20:57:28 - train: epoch 0026, iter [02700, 05004], lr: 0.097427, loss: 2.6391
2022-03-01 20:58:02 - train: epoch 0026, iter [02800, 05004], lr: 0.097427, loss: 2.8081
2022-03-01 20:58:37 - train: epoch 0026, iter [02900, 05004], lr: 0.097427, loss: 2.8417
2022-03-01 20:59:11 - train: epoch 0026, iter [03000, 05004], lr: 0.097427, loss: 2.8308
2022-03-01 20:59:46 - train: epoch 0026, iter [03100, 05004], lr: 0.097427, loss: 2.9847
2022-03-01 21:00:21 - train: epoch 0026, iter [03200, 05004], lr: 0.097427, loss: 2.7309
2022-03-01 21:00:55 - train: epoch 0026, iter [03300, 05004], lr: 0.097427, loss: 2.7382
2022-03-01 21:01:29 - train: epoch 0026, iter [03400, 05004], lr: 0.097427, loss: 2.9672
2022-03-01 21:02:03 - train: epoch 0026, iter [03500, 05004], lr: 0.097427, loss: 2.7470
2022-03-01 21:02:37 - train: epoch 0026, iter [03600, 05004], lr: 0.097427, loss: 2.6644
2022-03-01 21:03:12 - train: epoch 0026, iter [03700, 05004], lr: 0.097427, loss: 2.9142
2022-03-01 21:03:47 - train: epoch 0026, iter [03800, 05004], lr: 0.097427, loss: 2.9144
2022-03-01 21:04:21 - train: epoch 0026, iter [03900, 05004], lr: 0.097427, loss: 3.0294
2022-03-01 21:04:56 - train: epoch 0026, iter [04000, 05004], lr: 0.097427, loss: 2.8999
2022-03-01 21:05:30 - train: epoch 0026, iter [04100, 05004], lr: 0.097427, loss: 2.9546
2022-03-01 21:06:05 - train: epoch 0026, iter [04200, 05004], lr: 0.097427, loss: 3.0181
2022-03-01 21:06:39 - train: epoch 0026, iter [04300, 05004], lr: 0.097427, loss: 2.6645
2022-03-01 21:07:15 - train: epoch 0026, iter [04400, 05004], lr: 0.097427, loss: 3.0458
2022-03-01 21:07:49 - train: epoch 0026, iter [04500, 05004], lr: 0.097427, loss: 3.1502
2022-03-01 21:08:24 - train: epoch 0026, iter [04600, 05004], lr: 0.097427, loss: 2.8086
2022-03-01 21:08:58 - train: epoch 0026, iter [04700, 05004], lr: 0.097427, loss: 2.7155
2022-03-01 21:09:32 - train: epoch 0026, iter [04800, 05004], lr: 0.097427, loss: 2.7565
2022-03-01 21:10:07 - train: epoch 0026, iter [04900, 05004], lr: 0.097427, loss: 2.8928
2022-03-01 21:10:41 - train: epoch 0026, iter [05000, 05004], lr: 0.097427, loss: 2.8758
2022-03-01 21:10:42 - train: epoch 026, train_loss: 2.8246
2022-03-01 21:11:56 - eval: epoch: 026, acc1: 50.942%, acc5: 77.184%, test_loss: 2.1010, per_image_load_time: 2.003ms, per_image_inference_time: 0.554ms
2022-03-01 21:11:57 - until epoch: 026, best_acc1: 52.548%
2022-03-01 21:11:57 - epoch 027 lr: 0.09716559066288716
2022-03-01 21:12:36 - train: epoch 0027, iter [00100, 05004], lr: 0.097166, loss: 3.0659
2022-03-01 21:13:10 - train: epoch 0027, iter [00200, 05004], lr: 0.097166, loss: 2.6348
2022-03-01 21:13:44 - train: epoch 0027, iter [00300, 05004], lr: 0.097166, loss: 3.0462
2022-03-01 21:14:19 - train: epoch 0027, iter [00400, 05004], lr: 0.097166, loss: 2.9475
2022-03-01 21:14:53 - train: epoch 0027, iter [00500, 05004], lr: 0.097166, loss: 2.8235
2022-03-01 21:15:27 - train: epoch 0027, iter [00600, 05004], lr: 0.097166, loss: 3.0163
2022-03-01 21:16:01 - train: epoch 0027, iter [00700, 05004], lr: 0.097166, loss: 2.8067
2022-03-01 21:16:37 - train: epoch 0027, iter [00800, 05004], lr: 0.097166, loss: 2.8954
2022-03-01 21:17:11 - train: epoch 0027, iter [00900, 05004], lr: 0.097166, loss: 2.8118
2022-03-01 21:17:45 - train: epoch 0027, iter [01000, 05004], lr: 0.097166, loss: 3.0716
2022-03-01 21:18:20 - train: epoch 0027, iter [01100, 05004], lr: 0.097166, loss: 2.7869
2022-03-01 21:18:54 - train: epoch 0027, iter [01200, 05004], lr: 0.097166, loss: 3.0901
2022-03-01 21:19:28 - train: epoch 0027, iter [01300, 05004], lr: 0.097166, loss: 3.0856
2022-03-01 21:20:03 - train: epoch 0027, iter [01400, 05004], lr: 0.097166, loss: 2.8504
2022-03-01 21:20:37 - train: epoch 0027, iter [01500, 05004], lr: 0.097166, loss: 2.7621
2022-03-01 21:21:12 - train: epoch 0027, iter [01600, 05004], lr: 0.097166, loss: 2.8093
2022-03-01 21:21:46 - train: epoch 0027, iter [01700, 05004], lr: 0.097166, loss: 2.8996
2022-03-01 21:22:20 - train: epoch 0027, iter [01800, 05004], lr: 0.097166, loss: 2.6585
2022-03-01 21:22:55 - train: epoch 0027, iter [01900, 05004], lr: 0.097166, loss: 3.0102
2022-03-01 21:23:29 - train: epoch 0027, iter [02000, 05004], lr: 0.097166, loss: 2.7389
2022-03-01 21:24:03 - train: epoch 0027, iter [02100, 05004], lr: 0.097166, loss: 2.9633
2022-03-01 21:24:37 - train: epoch 0027, iter [02200, 05004], lr: 0.097166, loss: 2.9886
2022-03-01 21:25:12 - train: epoch 0027, iter [02300, 05004], lr: 0.097166, loss: 3.0784
2022-03-01 21:25:47 - train: epoch 0027, iter [02400, 05004], lr: 0.097166, loss: 2.7278
2022-03-01 21:26:21 - train: epoch 0027, iter [02500, 05004], lr: 0.097166, loss: 2.6185
2022-03-01 21:26:55 - train: epoch 0027, iter [02600, 05004], lr: 0.097166, loss: 2.7751
2022-03-01 21:27:29 - train: epoch 0027, iter [02700, 05004], lr: 0.097166, loss: 2.6607
2022-03-01 21:28:03 - train: epoch 0027, iter [02800, 05004], lr: 0.097166, loss: 2.9450
2022-03-01 21:28:38 - train: epoch 0027, iter [02900, 05004], lr: 0.097166, loss: 2.7359
2022-03-01 21:29:12 - train: epoch 0027, iter [03000, 05004], lr: 0.097166, loss: 2.5657
2022-03-01 21:29:46 - train: epoch 0027, iter [03100, 05004], lr: 0.097166, loss: 2.9120
2022-03-01 21:30:21 - train: epoch 0027, iter [03200, 05004], lr: 0.097166, loss: 2.7375
2022-03-01 21:30:55 - train: epoch 0027, iter [03300, 05004], lr: 0.097166, loss: 2.6584
2022-03-01 21:31:30 - train: epoch 0027, iter [03400, 05004], lr: 0.097166, loss: 2.7331
2022-03-01 21:32:04 - train: epoch 0027, iter [03500, 05004], lr: 0.097166, loss: 3.0381
2022-03-01 21:32:39 - train: epoch 0027, iter [03600, 05004], lr: 0.097166, loss: 2.6838
2022-03-01 21:33:13 - train: epoch 0027, iter [03700, 05004], lr: 0.097166, loss: 2.6955
2022-03-01 21:33:48 - train: epoch 0027, iter [03800, 05004], lr: 0.097166, loss: 2.6244
2022-03-01 21:34:22 - train: epoch 0027, iter [03900, 05004], lr: 0.097166, loss: 2.5698
2022-03-01 21:34:57 - train: epoch 0027, iter [04000, 05004], lr: 0.097166, loss: 2.9261
2022-03-01 21:35:31 - train: epoch 0027, iter [04100, 05004], lr: 0.097166, loss: 2.6887
2022-03-01 21:36:05 - train: epoch 0027, iter [04200, 05004], lr: 0.097166, loss: 2.9467
2022-03-01 21:36:40 - train: epoch 0027, iter [04300, 05004], lr: 0.097166, loss: 2.7970
2022-03-01 21:37:15 - train: epoch 0027, iter [04400, 05004], lr: 0.097166, loss: 2.7159
2022-03-01 21:37:49 - train: epoch 0027, iter [04500, 05004], lr: 0.097166, loss: 2.7889
2022-03-01 21:38:24 - train: epoch 0027, iter [04600, 05004], lr: 0.097166, loss: 2.7926
2022-03-01 21:38:59 - train: epoch 0027, iter [04700, 05004], lr: 0.097166, loss: 2.7830
2022-03-01 21:39:32 - train: epoch 0027, iter [04800, 05004], lr: 0.097166, loss: 2.9506
2022-03-01 21:40:07 - train: epoch 0027, iter [04900, 05004], lr: 0.097166, loss: 2.8731
2022-03-01 21:40:40 - train: epoch 0027, iter [05000, 05004], lr: 0.097166, loss: 2.4336
2022-03-01 21:40:41 - train: epoch 027, train_loss: 2.8189
2022-03-01 21:41:56 - eval: epoch: 027, acc1: 50.366%, acc5: 76.424%, test_loss: 2.1407, per_image_load_time: 1.946ms, per_image_inference_time: 0.538ms
2022-03-01 21:41:56 - until epoch: 027, best_acc1: 52.548%
2022-03-01 21:41:56 - epoch 028 lr: 0.09689211739666023
2022-03-01 21:42:35 - train: epoch 0028, iter [00100, 05004], lr: 0.096892, loss: 2.6291
2022-03-01 21:43:10 - train: epoch 0028, iter [00200, 05004], lr: 0.096892, loss: 2.8713
2022-03-01 21:43:44 - train: epoch 0028, iter [00300, 05004], lr: 0.096892, loss: 2.7823
2022-03-01 21:44:19 - train: epoch 0028, iter [00400, 05004], lr: 0.096892, loss: 2.7013
2022-03-01 21:44:54 - train: epoch 0028, iter [00500, 05004], lr: 0.096892, loss: 2.6739
2022-03-01 21:45:28 - train: epoch 0028, iter [00600, 05004], lr: 0.096892, loss: 2.9094
2022-03-01 21:46:02 - train: epoch 0028, iter [00700, 05004], lr: 0.096892, loss: 2.9472
2022-03-01 21:46:36 - train: epoch 0028, iter [00800, 05004], lr: 0.096892, loss: 2.6699
2022-03-01 21:47:11 - train: epoch 0028, iter [00900, 05004], lr: 0.096892, loss: 2.6767
2022-03-01 21:47:45 - train: epoch 0028, iter [01000, 05004], lr: 0.096892, loss: 2.9752
2022-03-01 21:48:20 - train: epoch 0028, iter [01100, 05004], lr: 0.096892, loss: 2.7437
2022-03-01 21:48:53 - train: epoch 0028, iter [01200, 05004], lr: 0.096892, loss: 2.5763
2022-03-01 21:49:28 - train: epoch 0028, iter [01300, 05004], lr: 0.096892, loss: 2.8613
2022-03-01 21:50:02 - train: epoch 0028, iter [01400, 05004], lr: 0.096892, loss: 3.1383
2022-03-01 21:50:36 - train: epoch 0028, iter [01500, 05004], lr: 0.096892, loss: 2.7736
2022-03-01 21:51:11 - train: epoch 0028, iter [01600, 05004], lr: 0.096892, loss: 2.7497
2022-03-01 21:51:45 - train: epoch 0028, iter [01700, 05004], lr: 0.096892, loss: 2.8774
2022-03-01 21:52:20 - train: epoch 0028, iter [01800, 05004], lr: 0.096892, loss: 2.8043
2022-03-01 21:52:54 - train: epoch 0028, iter [01900, 05004], lr: 0.096892, loss: 2.7272
2022-03-01 21:53:29 - train: epoch 0028, iter [02000, 05004], lr: 0.096892, loss: 2.9865
2022-03-01 21:54:02 - train: epoch 0028, iter [02100, 05004], lr: 0.096892, loss: 2.7428
2022-03-01 21:54:37 - train: epoch 0028, iter [02200, 05004], lr: 0.096892, loss: 2.8562
2022-03-01 21:55:12 - train: epoch 0028, iter [02300, 05004], lr: 0.096892, loss: 2.8593
2022-03-01 21:55:46 - train: epoch 0028, iter [02400, 05004], lr: 0.096892, loss: 3.2478
2022-03-01 21:56:21 - train: epoch 0028, iter [02500, 05004], lr: 0.096892, loss: 2.7953
2022-03-01 21:56:56 - train: epoch 0028, iter [02600, 05004], lr: 0.096892, loss: 2.5963
2022-03-01 21:57:30 - train: epoch 0028, iter [02700, 05004], lr: 0.096892, loss: 2.6005
2022-03-01 21:58:04 - train: epoch 0028, iter [02800, 05004], lr: 0.096892, loss: 2.7941
2022-03-01 21:58:38 - train: epoch 0028, iter [02900, 05004], lr: 0.096892, loss: 2.6669
2022-03-01 21:59:13 - train: epoch 0028, iter [03000, 05004], lr: 0.096892, loss: 2.7640
2022-03-01 21:59:47 - train: epoch 0028, iter [03100, 05004], lr: 0.096892, loss: 3.1172
2022-03-01 22:00:21 - train: epoch 0028, iter [03200, 05004], lr: 0.096892, loss: 2.8622
2022-03-01 22:00:55 - train: epoch 0028, iter [03300, 05004], lr: 0.096892, loss: 2.8090
2022-03-01 22:01:29 - train: epoch 0028, iter [03400, 05004], lr: 0.096892, loss: 2.8793
2022-03-01 22:02:03 - train: epoch 0028, iter [03500, 05004], lr: 0.096892, loss: 2.9156
2022-03-01 22:02:37 - train: epoch 0028, iter [03600, 05004], lr: 0.096892, loss: 2.7423
2022-03-01 22:03:12 - train: epoch 0028, iter [03700, 05004], lr: 0.096892, loss: 2.7275
2022-03-01 22:03:46 - train: epoch 0028, iter [03800, 05004], lr: 0.096892, loss: 2.5425
2022-03-01 22:04:21 - train: epoch 0028, iter [03900, 05004], lr: 0.096892, loss: 2.7747
2022-03-01 22:04:55 - train: epoch 0028, iter [04000, 05004], lr: 0.096892, loss: 2.8849
2022-03-01 22:05:29 - train: epoch 0028, iter [04100, 05004], lr: 0.096892, loss: 2.8257
2022-03-01 22:06:04 - train: epoch 0028, iter [04200, 05004], lr: 0.096892, loss: 2.9944
2022-03-01 22:06:38 - train: epoch 0028, iter [04300, 05004], lr: 0.096892, loss: 2.7643
2022-03-01 22:07:12 - train: epoch 0028, iter [04400, 05004], lr: 0.096892, loss: 2.7393
2022-03-01 22:07:48 - train: epoch 0028, iter [04500, 05004], lr: 0.096892, loss: 2.6677
2022-03-01 22:08:22 - train: epoch 0028, iter [04600, 05004], lr: 0.096892, loss: 2.7858
2022-03-01 22:08:57 - train: epoch 0028, iter [04700, 05004], lr: 0.096892, loss: 2.7680
2022-03-01 22:09:30 - train: epoch 0028, iter [04800, 05004], lr: 0.096892, loss: 2.8043
2022-03-01 22:10:05 - train: epoch 0028, iter [04900, 05004], lr: 0.096892, loss: 2.6206
2022-03-01 22:10:38 - train: epoch 0028, iter [05000, 05004], lr: 0.096892, loss: 2.6396
2022-03-01 22:10:39 - train: epoch 028, train_loss: 2.8129
2022-03-01 22:11:54 - eval: epoch: 028, acc1: 51.392%, acc5: 77.128%, test_loss: 2.1074, per_image_load_time: 1.448ms, per_image_inference_time: 0.525ms
2022-03-01 22:11:54 - until epoch: 028, best_acc1: 52.548%
2022-03-01 22:11:54 - epoch 029 lr: 0.0966064732786784
2022-03-01 22:12:33 - train: epoch 0029, iter [00100, 05004], lr: 0.096606, loss: 2.7530
2022-03-01 22:13:08 - train: epoch 0029, iter [00200, 05004], lr: 0.096606, loss: 2.8758
2022-03-01 22:13:42 - train: epoch 0029, iter [00300, 05004], lr: 0.096606, loss: 2.7415
2022-03-01 22:14:16 - train: epoch 0029, iter [00400, 05004], lr: 0.096606, loss: 2.8212
2022-03-01 22:14:50 - train: epoch 0029, iter [00500, 05004], lr: 0.096606, loss: 2.8979
2022-03-01 22:15:24 - train: epoch 0029, iter [00600, 05004], lr: 0.096606, loss: 3.1061
2022-03-01 22:15:59 - train: epoch 0029, iter [00700, 05004], lr: 0.096606, loss: 2.3122
2022-03-01 22:16:34 - train: epoch 0029, iter [00800, 05004], lr: 0.096606, loss: 2.9207
2022-03-01 22:17:08 - train: epoch 0029, iter [00900, 05004], lr: 0.096606, loss: 2.8661
2022-03-01 22:17:43 - train: epoch 0029, iter [01000, 05004], lr: 0.096606, loss: 2.8058
2022-03-01 22:18:17 - train: epoch 0029, iter [01100, 05004], lr: 0.096606, loss: 2.5951
2022-03-01 22:18:51 - train: epoch 0029, iter [01200, 05004], lr: 0.096606, loss: 2.9057
2022-03-01 22:19:26 - train: epoch 0029, iter [01300, 05004], lr: 0.096606, loss: 2.7580
2022-03-01 22:20:01 - train: epoch 0029, iter [01400, 05004], lr: 0.096606, loss: 3.0207
2022-03-01 22:20:35 - train: epoch 0029, iter [01500, 05004], lr: 0.096606, loss: 2.8159
2022-03-01 22:21:10 - train: epoch 0029, iter [01600, 05004], lr: 0.096606, loss: 2.8090
2022-03-01 22:21:44 - train: epoch 0029, iter [01700, 05004], lr: 0.096606, loss: 2.7010
2022-03-01 22:22:19 - train: epoch 0029, iter [01800, 05004], lr: 0.096606, loss: 2.7609
2022-03-01 22:22:53 - train: epoch 0029, iter [01900, 05004], lr: 0.096606, loss: 2.5015
2022-03-01 22:23:28 - train: epoch 0029, iter [02000, 05004], lr: 0.096606, loss: 2.9604
2022-03-01 22:24:02 - train: epoch 0029, iter [02100, 05004], lr: 0.096606, loss: 2.6424
2022-03-01 22:24:37 - train: epoch 0029, iter [02200, 05004], lr: 0.096606, loss: 2.8098
2022-03-01 22:25:12 - train: epoch 0029, iter [02300, 05004], lr: 0.096606, loss: 2.9421
2022-03-01 22:25:46 - train: epoch 0029, iter [02400, 05004], lr: 0.096606, loss: 2.8688
2022-03-01 22:26:21 - train: epoch 0029, iter [02500, 05004], lr: 0.096606, loss: 2.9472
2022-03-01 22:26:56 - train: epoch 0029, iter [02600, 05004], lr: 0.096606, loss: 2.8789
2022-03-01 22:27:31 - train: epoch 0029, iter [02700, 05004], lr: 0.096606, loss: 2.6444
2022-03-01 22:28:04 - train: epoch 0029, iter [02800, 05004], lr: 0.096606, loss: 2.9766
2022-03-01 22:28:39 - train: epoch 0029, iter [02900, 05004], lr: 0.096606, loss: 2.8304
2022-03-01 22:29:13 - train: epoch 0029, iter [03000, 05004], lr: 0.096606, loss: 2.7814
2022-03-01 22:29:47 - train: epoch 0029, iter [03100, 05004], lr: 0.096606, loss: 2.7800
2022-03-01 22:30:22 - train: epoch 0029, iter [03200, 05004], lr: 0.096606, loss: 2.9687
2022-03-01 22:30:56 - train: epoch 0029, iter [03300, 05004], lr: 0.096606, loss: 2.8089
2022-03-01 22:31:31 - train: epoch 0029, iter [03400, 05004], lr: 0.096606, loss: 2.6511
2022-03-01 22:32:05 - train: epoch 0029, iter [03500, 05004], lr: 0.096606, loss: 2.9668
2022-03-01 22:32:39 - train: epoch 0029, iter [03600, 05004], lr: 0.096606, loss: 2.8377
2022-03-01 22:33:13 - train: epoch 0029, iter [03700, 05004], lr: 0.096606, loss: 2.9711
2022-03-01 22:33:47 - train: epoch 0029, iter [03800, 05004], lr: 0.096606, loss: 2.9086
2022-03-01 22:34:23 - train: epoch 0029, iter [03900, 05004], lr: 0.096606, loss: 2.7169
2022-03-01 22:34:57 - train: epoch 0029, iter [04000, 05004], lr: 0.096606, loss: 2.8333
2022-03-01 22:35:31 - train: epoch 0029, iter [04100, 05004], lr: 0.096606, loss: 2.5848
2022-03-01 22:36:05 - train: epoch 0029, iter [04200, 05004], lr: 0.096606, loss: 2.7631
2022-03-01 22:36:39 - train: epoch 0029, iter [04300, 05004], lr: 0.096606, loss: 2.8289
2022-03-01 22:37:14 - train: epoch 0029, iter [04400, 05004], lr: 0.096606, loss: 2.7807
2022-03-01 22:37:48 - train: epoch 0029, iter [04500, 05004], lr: 0.096606, loss: 2.7258
2022-03-01 22:38:23 - train: epoch 0029, iter [04600, 05004], lr: 0.096606, loss: 3.0831
2022-03-01 22:38:58 - train: epoch 0029, iter [04700, 05004], lr: 0.096606, loss: 2.4194
2022-03-01 22:39:31 - train: epoch 0029, iter [04800, 05004], lr: 0.096606, loss: 2.8943
2022-03-01 22:40:05 - train: epoch 0029, iter [04900, 05004], lr: 0.096606, loss: 2.9774
2022-03-01 22:40:39 - train: epoch 0029, iter [05000, 05004], lr: 0.096606, loss: 2.5388
2022-03-01 22:40:40 - train: epoch 029, train_loss: 2.8058
2022-03-01 22:41:55 - eval: epoch: 029, acc1: 52.260%, acc5: 77.914%, test_loss: 2.0500, per_image_load_time: 1.516ms, per_image_inference_time: 0.527ms
2022-03-01 22:41:55 - until epoch: 029, best_acc1: 52.548%
2022-03-02 02:11:13 - epoch 030 lr: 0.09630873244788883
2022-03-02 02:11:53 - train: epoch 0030, iter [00100, 05004], lr: 0.096309, loss: 2.7176
2022-03-02 02:12:27 - train: epoch 0030, iter [00200, 05004], lr: 0.096309, loss: 2.8249
2022-03-02 02:13:02 - train: epoch 0030, iter [00300, 05004], lr: 0.096309, loss: 2.7402
2022-03-02 02:13:35 - train: epoch 0030, iter [00400, 05004], lr: 0.096309, loss: 2.6670
2022-03-02 02:14:09 - train: epoch 0030, iter [00500, 05004], lr: 0.096309, loss: 2.7073
2022-03-02 02:14:44 - train: epoch 0030, iter [00600, 05004], lr: 0.096309, loss: 2.7966
2022-03-02 02:15:18 - train: epoch 0030, iter [00700, 05004], lr: 0.096309, loss: 2.8678
2022-03-02 02:15:52 - train: epoch 0030, iter [00800, 05004], lr: 0.096309, loss: 3.0294
2022-03-02 02:16:26 - train: epoch 0030, iter [00900, 05004], lr: 0.096309, loss: 2.6286
2022-03-02 02:17:02 - train: epoch 0030, iter [01000, 05004], lr: 0.096309, loss: 2.9487
2022-03-02 02:17:36 - train: epoch 0030, iter [01100, 05004], lr: 0.096309, loss: 2.7465
2022-03-02 02:18:09 - train: epoch 0030, iter [01200, 05004], lr: 0.096309, loss: 2.7811
2022-03-02 02:18:44 - train: epoch 0030, iter [01300, 05004], lr: 0.096309, loss: 2.9810
2022-03-02 02:19:19 - train: epoch 0030, iter [01400, 05004], lr: 0.096309, loss: 2.9140
2022-03-02 02:19:54 - train: epoch 0030, iter [01500, 05004], lr: 0.096309, loss: 2.7805
2022-03-02 02:20:28 - train: epoch 0030, iter [01600, 05004], lr: 0.096309, loss: 2.7317
2022-03-02 02:21:02 - train: epoch 0030, iter [01700, 05004], lr: 0.096309, loss: 2.8965
2022-03-02 02:21:37 - train: epoch 0030, iter [01800, 05004], lr: 0.096309, loss: 2.4758
2022-03-02 02:22:12 - train: epoch 0030, iter [01900, 05004], lr: 0.096309, loss: 2.6721
2022-03-02 02:22:46 - train: epoch 0030, iter [02000, 05004], lr: 0.096309, loss: 2.4733
2022-03-02 02:23:22 - train: epoch 0030, iter [02100, 05004], lr: 0.096309, loss: 2.9056
2022-03-02 02:23:56 - train: epoch 0030, iter [02200, 05004], lr: 0.096309, loss: 3.0465
2022-03-02 02:24:30 - train: epoch 0030, iter [02300, 05004], lr: 0.096309, loss: 2.8592
2022-03-02 02:25:04 - train: epoch 0030, iter [02400, 05004], lr: 0.096309, loss: 2.8731
2022-03-02 02:25:38 - train: epoch 0030, iter [02500, 05004], lr: 0.096309, loss: 3.0157
2022-03-02 02:26:14 - train: epoch 0030, iter [02600, 05004], lr: 0.096309, loss: 2.5292
2022-03-02 02:26:47 - train: epoch 0030, iter [02700, 05004], lr: 0.096309, loss: 2.9534
2022-03-02 02:27:21 - train: epoch 0030, iter [02800, 05004], lr: 0.096309, loss: 2.7344
2022-03-02 02:27:56 - train: epoch 0030, iter [02900, 05004], lr: 0.096309, loss: 2.6976
2022-03-02 02:28:30 - train: epoch 0030, iter [03000, 05004], lr: 0.096309, loss: 2.7403
2022-03-02 02:29:04 - train: epoch 0030, iter [03100, 05004], lr: 0.096309, loss: 2.7780
2022-03-02 02:29:40 - train: epoch 0030, iter [03200, 05004], lr: 0.096309, loss: 2.6310
2022-03-02 02:30:14 - train: epoch 0030, iter [03300, 05004], lr: 0.096309, loss: 3.0163
2022-03-02 02:30:48 - train: epoch 0030, iter [03400, 05004], lr: 0.096309, loss: 2.6956
2022-03-02 02:31:23 - train: epoch 0030, iter [03500, 05004], lr: 0.096309, loss: 2.9067
2022-03-02 02:31:57 - train: epoch 0030, iter [03600, 05004], lr: 0.096309, loss: 2.8820
2022-03-02 02:32:32 - train: epoch 0030, iter [03700, 05004], lr: 0.096309, loss: 2.6037
2022-03-02 02:33:05 - train: epoch 0030, iter [03800, 05004], lr: 0.096309, loss: 2.8100
2022-03-02 02:33:40 - train: epoch 0030, iter [03900, 05004], lr: 0.096309, loss: 2.7254
2022-03-02 02:34:15 - train: epoch 0030, iter [04000, 05004], lr: 0.096309, loss: 2.5258
2022-03-02 02:34:49 - train: epoch 0030, iter [04100, 05004], lr: 0.096309, loss: 2.8198
2022-03-02 02:35:24 - train: epoch 0030, iter [04200, 05004], lr: 0.096309, loss: 3.1584
2022-03-02 02:35:58 - train: epoch 0030, iter [04300, 05004], lr: 0.096309, loss: 3.0658
2022-03-02 02:36:32 - train: epoch 0030, iter [04400, 05004], lr: 0.096309, loss: 2.6928
2022-03-02 02:37:07 - train: epoch 0030, iter [04500, 05004], lr: 0.096309, loss: 2.8911
2022-03-02 02:37:41 - train: epoch 0030, iter [04600, 05004], lr: 0.096309, loss: 2.5551
2022-03-02 02:38:16 - train: epoch 0030, iter [04700, 05004], lr: 0.096309, loss: 2.7444
2022-03-02 02:38:50 - train: epoch 0030, iter [04800, 05004], lr: 0.096309, loss: 2.9569
2022-03-02 02:39:25 - train: epoch 0030, iter [04900, 05004], lr: 0.096309, loss: 2.6872
2022-03-02 02:39:57 - train: epoch 0030, iter [05000, 05004], lr: 0.096309, loss: 3.1259
2022-03-02 02:39:58 - train: epoch 030, train_loss: 2.8031
2022-03-02 02:41:12 - eval: epoch: 030, acc1: 51.036%, acc5: 77.472%, test_loss: 2.0902, per_image_load_time: 1.201ms, per_image_inference_time: 0.554ms
2022-03-02 02:41:13 - until epoch: 030, best_acc1: 52.548%
2022-03-02 02:41:13 - epoch 031 lr: 0.09599897218294122
2022-03-02 02:41:52 - train: epoch 0031, iter [00100, 05004], lr: 0.095999, loss: 2.8384
2022-03-02 02:42:27 - train: epoch 0031, iter [00200, 05004], lr: 0.095999, loss: 2.8481
2022-03-02 02:43:02 - train: epoch 0031, iter [00300, 05004], lr: 0.095999, loss: 2.4741
2022-03-02 02:43:35 - train: epoch 0031, iter [00400, 05004], lr: 0.095999, loss: 2.7127
2022-03-02 02:44:10 - train: epoch 0031, iter [00500, 05004], lr: 0.095999, loss: 2.7539
2022-03-02 02:44:45 - train: epoch 0031, iter [00600, 05004], lr: 0.095999, loss: 2.9122
2022-03-02 02:45:19 - train: epoch 0031, iter [00700, 05004], lr: 0.095999, loss: 2.7086
2022-03-02 02:45:53 - train: epoch 0031, iter [00800, 05004], lr: 0.095999, loss: 2.9200
2022-03-02 02:46:28 - train: epoch 0031, iter [00900, 05004], lr: 0.095999, loss: 2.5983
2022-03-02 02:47:02 - train: epoch 0031, iter [01000, 05004], lr: 0.095999, loss: 3.0099
2022-03-02 02:47:36 - train: epoch 0031, iter [01100, 05004], lr: 0.095999, loss: 2.9501
2022-03-02 02:48:11 - train: epoch 0031, iter [01200, 05004], lr: 0.095999, loss: 2.9247
2022-03-02 02:48:46 - train: epoch 0031, iter [01300, 05004], lr: 0.095999, loss: 2.4576
2022-03-02 02:49:20 - train: epoch 0031, iter [01400, 05004], lr: 0.095999, loss: 2.5969
2022-03-02 02:49:54 - train: epoch 0031, iter [01500, 05004], lr: 0.095999, loss: 2.9654
2022-03-02 02:50:28 - train: epoch 0031, iter [01600, 05004], lr: 0.095999, loss: 2.9116
2022-03-02 02:51:02 - train: epoch 0031, iter [01700, 05004], lr: 0.095999, loss: 2.7113
2022-03-02 02:51:37 - train: epoch 0031, iter [01800, 05004], lr: 0.095999, loss: 2.5752
2022-03-02 02:52:10 - train: epoch 0031, iter [01900, 05004], lr: 0.095999, loss: 2.7927
2022-03-02 02:52:45 - train: epoch 0031, iter [02000, 05004], lr: 0.095999, loss: 2.6301
2022-03-02 02:53:18 - train: epoch 0031, iter [02100, 05004], lr: 0.095999, loss: 2.8850
2022-03-02 02:53:53 - train: epoch 0031, iter [02200, 05004], lr: 0.095999, loss: 2.7537
2022-03-02 02:54:27 - train: epoch 0031, iter [02300, 05004], lr: 0.095999, loss: 2.6962
2022-03-02 02:55:01 - train: epoch 0031, iter [02400, 05004], lr: 0.095999, loss: 2.6721
2022-03-02 02:55:36 - train: epoch 0031, iter [02500, 05004], lr: 0.095999, loss: 2.5035
2022-03-02 02:56:09 - train: epoch 0031, iter [02600, 05004], lr: 0.095999, loss: 2.8890
2022-03-02 02:56:44 - train: epoch 0031, iter [02700, 05004], lr: 0.095999, loss: 2.8920
2022-03-02 02:57:18 - train: epoch 0031, iter [02800, 05004], lr: 0.095999, loss: 3.1024
2022-03-02 02:57:53 - train: epoch 0031, iter [02900, 05004], lr: 0.095999, loss: 2.8331
2022-03-02 02:58:26 - train: epoch 0031, iter [03000, 05004], lr: 0.095999, loss: 2.8659
2022-03-02 02:59:00 - train: epoch 0031, iter [03100, 05004], lr: 0.095999, loss: 2.9161
2022-03-02 02:59:35 - train: epoch 0031, iter [03200, 05004], lr: 0.095999, loss: 3.0024
2022-03-02 03:00:09 - train: epoch 0031, iter [03300, 05004], lr: 0.095999, loss: 2.8465
2022-03-02 03:00:44 - train: epoch 0031, iter [03400, 05004], lr: 0.095999, loss: 2.8352
2022-03-02 03:01:18 - train: epoch 0031, iter [03500, 05004], lr: 0.095999, loss: 2.7712
2022-03-02 03:01:53 - train: epoch 0031, iter [03600, 05004], lr: 0.095999, loss: 2.6984
2022-03-02 03:02:27 - train: epoch 0031, iter [03700, 05004], lr: 0.095999, loss: 2.7258
2022-03-02 03:03:01 - train: epoch 0031, iter [03800, 05004], lr: 0.095999, loss: 2.9297
2022-03-02 03:03:35 - train: epoch 0031, iter [03900, 05004], lr: 0.095999, loss: 2.9104
2022-03-02 03:04:10 - train: epoch 0031, iter [04000, 05004], lr: 0.095999, loss: 2.5949
2022-03-02 03:04:44 - train: epoch 0031, iter [04100, 05004], lr: 0.095999, loss: 2.6318
2022-03-02 03:05:17 - train: epoch 0031, iter [04200, 05004], lr: 0.095999, loss: 2.9154
2022-03-02 03:05:51 - train: epoch 0031, iter [04300, 05004], lr: 0.095999, loss: 2.6602
2022-03-02 03:06:26 - train: epoch 0031, iter [04400, 05004], lr: 0.095999, loss: 2.7465
2022-03-02 03:07:00 - train: epoch 0031, iter [04500, 05004], lr: 0.095999, loss: 2.9279
2022-03-02 03:07:34 - train: epoch 0031, iter [04600, 05004], lr: 0.095999, loss: 2.8842
2022-03-02 03:08:09 - train: epoch 0031, iter [04700, 05004], lr: 0.095999, loss: 2.6379
2022-03-02 03:08:43 - train: epoch 0031, iter [04800, 05004], lr: 0.095999, loss: 2.6736
2022-03-02 03:09:18 - train: epoch 0031, iter [04900, 05004], lr: 0.095999, loss: 2.9147
2022-03-02 03:09:51 - train: epoch 0031, iter [05000, 05004], lr: 0.095999, loss: 2.7374
2022-03-02 03:09:52 - train: epoch 031, train_loss: 2.7927
2022-03-02 03:11:07 - eval: epoch: 031, acc1: 53.734%, acc5: 79.386%, test_loss: 1.9721, per_image_load_time: 0.467ms, per_image_inference_time: 0.511ms
2022-03-02 03:11:08 - until epoch: 031, best_acc1: 53.734%
2022-03-02 03:11:08 - epoch 032 lr: 0.09567727288213004
2022-03-02 03:11:48 - train: epoch 0032, iter [00100, 05004], lr: 0.095677, loss: 2.7080
2022-03-02 03:12:22 - train: epoch 0032, iter [00200, 05004], lr: 0.095677, loss: 2.5858
2022-03-02 03:12:56 - train: epoch 0032, iter [00300, 05004], lr: 0.095677, loss: 2.7810
2022-03-02 03:13:31 - train: epoch 0032, iter [00400, 05004], lr: 0.095677, loss: 2.6846
2022-03-02 03:14:05 - train: epoch 0032, iter [00500, 05004], lr: 0.095677, loss: 3.0348
2022-03-02 03:14:39 - train: epoch 0032, iter [00600, 05004], lr: 0.095677, loss: 2.8307
2022-03-02 03:15:13 - train: epoch 0032, iter [00700, 05004], lr: 0.095677, loss: 2.7248
2022-03-02 03:15:48 - train: epoch 0032, iter [00800, 05004], lr: 0.095677, loss: 3.0120
2022-03-02 03:16:23 - train: epoch 0032, iter [00900, 05004], lr: 0.095677, loss: 2.8777
2022-03-02 03:16:57 - train: epoch 0032, iter [01000, 05004], lr: 0.095677, loss: 2.9024
2022-03-02 03:17:31 - train: epoch 0032, iter [01100, 05004], lr: 0.095677, loss: 3.0546
2022-03-02 03:18:06 - train: epoch 0032, iter [01200, 05004], lr: 0.095677, loss: 2.8453
2022-03-02 03:18:40 - train: epoch 0032, iter [01300, 05004], lr: 0.095677, loss: 2.5420
2022-03-02 03:19:14 - train: epoch 0032, iter [01400, 05004], lr: 0.095677, loss: 2.9567
2022-03-02 03:19:49 - train: epoch 0032, iter [01500, 05004], lr: 0.095677, loss: 2.7875
2022-03-02 03:20:24 - train: epoch 0032, iter [01600, 05004], lr: 0.095677, loss: 2.9192
2022-03-02 03:20:57 - train: epoch 0032, iter [01700, 05004], lr: 0.095677, loss: 2.7572
2022-03-02 03:21:31 - train: epoch 0032, iter [01800, 05004], lr: 0.095677, loss: 3.0903
2022-03-02 03:22:06 - train: epoch 0032, iter [01900, 05004], lr: 0.095677, loss: 2.6998
2022-03-02 03:22:41 - train: epoch 0032, iter [02000, 05004], lr: 0.095677, loss: 2.7585
2022-03-02 03:23:15 - train: epoch 0032, iter [02100, 05004], lr: 0.095677, loss: 2.7770
2022-03-02 03:23:49 - train: epoch 0032, iter [02200, 05004], lr: 0.095677, loss: 2.6920
2022-03-02 03:24:24 - train: epoch 0032, iter [02300, 05004], lr: 0.095677, loss: 2.7197
2022-03-02 03:24:58 - train: epoch 0032, iter [02400, 05004], lr: 0.095677, loss: 2.6948
2022-03-02 03:25:32 - train: epoch 0032, iter [02500, 05004], lr: 0.095677, loss: 2.7981
2022-03-02 03:26:08 - train: epoch 0032, iter [02600, 05004], lr: 0.095677, loss: 2.7899
2022-03-02 03:26:41 - train: epoch 0032, iter [02700, 05004], lr: 0.095677, loss: 2.7775
2022-03-02 03:27:16 - train: epoch 0032, iter [02800, 05004], lr: 0.095677, loss: 2.6358
2022-03-02 03:27:50 - train: epoch 0032, iter [02900, 05004], lr: 0.095677, loss: 2.8299
2022-03-02 03:28:24 - train: epoch 0032, iter [03000, 05004], lr: 0.095677, loss: 2.4752
2022-03-02 03:28:58 - train: epoch 0032, iter [03100, 05004], lr: 0.095677, loss: 2.7276
2022-03-02 03:29:32 - train: epoch 0032, iter [03200, 05004], lr: 0.095677, loss: 2.9226
2022-03-02 03:30:06 - train: epoch 0032, iter [03300, 05004], lr: 0.095677, loss: 2.4596
2022-03-02 03:30:41 - train: epoch 0032, iter [03400, 05004], lr: 0.095677, loss: 2.5704
2022-03-02 03:31:14 - train: epoch 0032, iter [03500, 05004], lr: 0.095677, loss: 2.9486
2022-03-02 03:31:49 - train: epoch 0032, iter [03600, 05004], lr: 0.095677, loss: 2.9231
2022-03-02 03:32:24 - train: epoch 0032, iter [03700, 05004], lr: 0.095677, loss: 2.7356
2022-03-02 03:32:58 - train: epoch 0032, iter [03800, 05004], lr: 0.095677, loss: 2.4568
2022-03-02 03:33:32 - train: epoch 0032, iter [03900, 05004], lr: 0.095677, loss: 2.7524
2022-03-02 03:34:07 - train: epoch 0032, iter [04000, 05004], lr: 0.095677, loss: 2.8683
2022-03-02 03:34:41 - train: epoch 0032, iter [04100, 05004], lr: 0.095677, loss: 2.7014
2022-03-02 03:35:15 - train: epoch 0032, iter [04200, 05004], lr: 0.095677, loss: 2.5408
2022-03-02 03:35:49 - train: epoch 0032, iter [04300, 05004], lr: 0.095677, loss: 2.7641
2022-03-02 03:36:23 - train: epoch 0032, iter [04400, 05004], lr: 0.095677, loss: 2.6719
2022-03-02 03:36:58 - train: epoch 0032, iter [04500, 05004], lr: 0.095677, loss: 2.7812
2022-03-02 03:37:32 - train: epoch 0032, iter [04600, 05004], lr: 0.095677, loss: 2.7599
2022-03-02 03:38:07 - train: epoch 0032, iter [04700, 05004], lr: 0.095677, loss: 2.7952
2022-03-02 03:38:40 - train: epoch 0032, iter [04800, 05004], lr: 0.095677, loss: 2.5548
2022-03-02 03:39:15 - train: epoch 0032, iter [04900, 05004], lr: 0.095677, loss: 2.7319
2022-03-02 03:39:48 - train: epoch 0032, iter [05000, 05004], lr: 0.095677, loss: 2.8891
2022-03-02 03:39:49 - train: epoch 032, train_loss: 2.7919
2022-03-02 03:41:04 - eval: epoch: 032, acc1: 52.980%, acc5: 78.780%, test_loss: 2.0103, per_image_load_time: 0.532ms, per_image_inference_time: 0.534ms
2022-03-02 03:41:05 - until epoch: 032, best_acc1: 53.734%
2022-03-02 03:41:05 - epoch 033 lr: 0.09534371804252728
2022-03-02 03:41:45 - train: epoch 0033, iter [00100, 05004], lr: 0.095344, loss: 2.6223
2022-03-02 03:42:18 - train: epoch 0033, iter [00200, 05004], lr: 0.095344, loss: 2.8331
2022-03-02 03:42:53 - train: epoch 0033, iter [00300, 05004], lr: 0.095344, loss: 2.7797
2022-03-02 03:43:27 - train: epoch 0033, iter [00400, 05004], lr: 0.095344, loss: 2.6653
2022-03-02 03:44:02 - train: epoch 0033, iter [00500, 05004], lr: 0.095344, loss: 2.9852
2022-03-02 03:44:36 - train: epoch 0033, iter [00600, 05004], lr: 0.095344, loss: 2.5576
2022-03-02 03:45:11 - train: epoch 0033, iter [00700, 05004], lr: 0.095344, loss: 2.9099
2022-03-02 03:45:45 - train: epoch 0033, iter [00800, 05004], lr: 0.095344, loss: 3.0174
2022-03-02 03:46:19 - train: epoch 0033, iter [00900, 05004], lr: 0.095344, loss: 2.8349
2022-03-02 03:46:54 - train: epoch 0033, iter [01000, 05004], lr: 0.095344, loss: 2.9344
2022-03-02 03:47:28 - train: epoch 0033, iter [01100, 05004], lr: 0.095344, loss: 2.5246
2022-03-02 03:48:03 - train: epoch 0033, iter [01200, 05004], lr: 0.095344, loss: 2.9000
2022-03-02 03:48:37 - train: epoch 0033, iter [01300, 05004], lr: 0.095344, loss: 2.8015
2022-03-02 03:49:11 - train: epoch 0033, iter [01400, 05004], lr: 0.095344, loss: 2.9205
2022-03-02 03:49:45 - train: epoch 0033, iter [01500, 05004], lr: 0.095344, loss: 2.8301
2022-03-02 03:50:20 - train: epoch 0033, iter [01600, 05004], lr: 0.095344, loss: 2.9533
2022-03-02 03:50:54 - train: epoch 0033, iter [01700, 05004], lr: 0.095344, loss: 2.9280
2022-03-02 03:51:29 - train: epoch 0033, iter [01800, 05004], lr: 0.095344, loss: 2.8303
2022-03-02 03:52:03 - train: epoch 0033, iter [01900, 05004], lr: 0.095344, loss: 2.8990
2022-03-02 03:52:38 - train: epoch 0033, iter [02000, 05004], lr: 0.095344, loss: 2.7566
2022-03-02 03:53:11 - train: epoch 0033, iter [02100, 05004], lr: 0.095344, loss: 2.9075
2022-03-02 03:53:46 - train: epoch 0033, iter [02200, 05004], lr: 0.095344, loss: 2.8174
2022-03-02 03:54:20 - train: epoch 0033, iter [02300, 05004], lr: 0.095344, loss: 2.8517
2022-03-02 03:54:55 - train: epoch 0033, iter [02400, 05004], lr: 0.095344, loss: 2.7899
2022-03-02 03:55:29 - train: epoch 0033, iter [02500, 05004], lr: 0.095344, loss: 2.8053
2022-03-02 03:56:04 - train: epoch 0033, iter [02600, 05004], lr: 0.095344, loss: 2.7316
2022-03-02 03:56:38 - train: epoch 0033, iter [02700, 05004], lr: 0.095344, loss: 2.9703
2022-03-02 03:57:12 - train: epoch 0033, iter [02800, 05004], lr: 0.095344, loss: 2.8760
2022-03-02 03:57:47 - train: epoch 0033, iter [02900, 05004], lr: 0.095344, loss: 2.6995
2022-03-02 03:58:21 - train: epoch 0033, iter [03000, 05004], lr: 0.095344, loss: 2.7224
2022-03-02 03:58:55 - train: epoch 0033, iter [03100, 05004], lr: 0.095344, loss: 3.1176
2022-03-02 03:59:29 - train: epoch 0033, iter [03200, 05004], lr: 0.095344, loss: 2.6640
2022-03-02 04:00:04 - train: epoch 0033, iter [03300, 05004], lr: 0.095344, loss: 2.5689
2022-03-02 04:00:37 - train: epoch 0033, iter [03400, 05004], lr: 0.095344, loss: 2.6906
2022-03-02 04:01:11 - train: epoch 0033, iter [03500, 05004], lr: 0.095344, loss: 2.9094
2022-03-02 04:01:46 - train: epoch 0033, iter [03600, 05004], lr: 0.095344, loss: 2.9309
2022-03-02 04:02:20 - train: epoch 0033, iter [03700, 05004], lr: 0.095344, loss: 2.8052
2022-03-02 04:02:55 - train: epoch 0033, iter [03800, 05004], lr: 0.095344, loss: 2.7238
2022-03-02 04:03:29 - train: epoch 0033, iter [03900, 05004], lr: 0.095344, loss: 3.1211
2022-03-02 04:04:04 - train: epoch 0033, iter [04000, 05004], lr: 0.095344, loss: 2.8395
2022-03-02 04:04:38 - train: epoch 0033, iter [04100, 05004], lr: 0.095344, loss: 2.9064
2022-03-02 04:05:12 - train: epoch 0033, iter [04200, 05004], lr: 0.095344, loss: 2.6965
2022-03-02 04:05:47 - train: epoch 0033, iter [04300, 05004], lr: 0.095344, loss: 2.9217
2022-03-02 04:06:21 - train: epoch 0033, iter [04400, 05004], lr: 0.095344, loss: 2.8993
2022-03-02 04:06:54 - train: epoch 0033, iter [04500, 05004], lr: 0.095344, loss: 2.9969
2022-03-02 04:07:30 - train: epoch 0033, iter [04600, 05004], lr: 0.095344, loss: 2.7939
2022-03-02 04:08:04 - train: epoch 0033, iter [04700, 05004], lr: 0.095344, loss: 2.5679
2022-03-02 04:08:39 - train: epoch 0033, iter [04800, 05004], lr: 0.095344, loss: 3.0353
2022-03-02 04:09:12 - train: epoch 0033, iter [04900, 05004], lr: 0.095344, loss: 2.4480
2022-03-02 04:09:46 - train: epoch 0033, iter [05000, 05004], lr: 0.095344, loss: 3.0036
2022-03-02 04:09:47 - train: epoch 033, train_loss: 2.7839
2022-03-02 04:11:01 - eval: epoch: 033, acc1: 53.126%, acc5: 78.912%, test_loss: 1.9946, per_image_load_time: 0.553ms, per_image_inference_time: 0.507ms
2022-03-02 04:11:02 - until epoch: 033, best_acc1: 53.734%
2022-03-02 04:11:02 - epoch 034 lr: 0.09499839423831061
2022-03-02 04:11:42 - train: epoch 0034, iter [00100, 05004], lr: 0.094998, loss: 2.5965
2022-03-02 04:12:16 - train: epoch 0034, iter [00200, 05004], lr: 0.094998, loss: 2.6790
2022-03-02 04:12:50 - train: epoch 0034, iter [00300, 05004], lr: 0.094998, loss: 2.4160
2022-03-02 04:13:24 - train: epoch 0034, iter [00400, 05004], lr: 0.094998, loss: 2.4833
2022-03-02 04:13:59 - train: epoch 0034, iter [00500, 05004], lr: 0.094998, loss: 2.8106
2022-03-02 04:14:32 - train: epoch 0034, iter [00600, 05004], lr: 0.094998, loss: 2.7901
2022-03-02 04:15:07 - train: epoch 0034, iter [00700, 05004], lr: 0.094998, loss: 2.5349
2022-03-02 04:15:41 - train: epoch 0034, iter [00800, 05004], lr: 0.094998, loss: 3.1168
2022-03-02 04:16:16 - train: epoch 0034, iter [00900, 05004], lr: 0.094998, loss: 2.8161
2022-03-02 04:16:51 - train: epoch 0034, iter [01000, 05004], lr: 0.094998, loss: 2.8418
2022-03-02 04:17:25 - train: epoch 0034, iter [01100, 05004], lr: 0.094998, loss: 2.9403
2022-03-02 04:18:00 - train: epoch 0034, iter [01200, 05004], lr: 0.094998, loss: 2.8754
2022-03-02 04:18:34 - train: epoch 0034, iter [01300, 05004], lr: 0.094998, loss: 2.7555
2022-03-02 04:19:08 - train: epoch 0034, iter [01400, 05004], lr: 0.094998, loss: 2.8625
2022-03-02 04:19:43 - train: epoch 0034, iter [01500, 05004], lr: 0.094998, loss: 2.7950
2022-03-02 04:20:17 - train: epoch 0034, iter [01600, 05004], lr: 0.094998, loss: 2.8241
2022-03-02 04:20:51 - train: epoch 0034, iter [01700, 05004], lr: 0.094998, loss: 2.8200
2022-03-02 04:21:25 - train: epoch 0034, iter [01800, 05004], lr: 0.094998, loss: 2.8711
2022-03-02 04:22:01 - train: epoch 0034, iter [01900, 05004], lr: 0.094998, loss: 2.7136
2022-03-02 04:22:35 - train: epoch 0034, iter [02000, 05004], lr: 0.094998, loss: 2.6076
2022-03-02 04:23:09 - train: epoch 0034, iter [02100, 05004], lr: 0.094998, loss: 2.7728
2022-03-02 04:23:44 - train: epoch 0034, iter [02200, 05004], lr: 0.094998, loss: 2.5681
2022-03-02 04:24:19 - train: epoch 0034, iter [02300, 05004], lr: 0.094998, loss: 2.8161
2022-03-02 04:24:53 - train: epoch 0034, iter [02400, 05004], lr: 0.094998, loss: 2.7796
2022-03-02 04:25:28 - train: epoch 0034, iter [02500, 05004], lr: 0.094998, loss: 2.8685
2022-03-02 04:26:02 - train: epoch 0034, iter [02600, 05004], lr: 0.094998, loss: 3.0236
2022-03-02 04:26:36 - train: epoch 0034, iter [02700, 05004], lr: 0.094998, loss: 2.8426
2022-03-02 04:27:10 - train: epoch 0034, iter [02800, 05004], lr: 0.094998, loss: 2.6157
2022-03-02 04:27:45 - train: epoch 0034, iter [02900, 05004], lr: 0.094998, loss: 2.6442
2022-03-02 04:28:19 - train: epoch 0034, iter [03000, 05004], lr: 0.094998, loss: 2.7902
2022-03-02 04:28:53 - train: epoch 0034, iter [03100, 05004], lr: 0.094998, loss: 2.9089
2022-03-02 04:29:28 - train: epoch 0034, iter [03200, 05004], lr: 0.094998, loss: 2.7359
2022-03-02 04:30:03 - train: epoch 0034, iter [03300, 05004], lr: 0.094998, loss: 2.8235
2022-03-02 04:30:37 - train: epoch 0034, iter [03400, 05004], lr: 0.094998, loss: 2.9097
2022-03-02 04:31:12 - train: epoch 0034, iter [03500, 05004], lr: 0.094998, loss: 2.8527
2022-03-02 04:31:46 - train: epoch 0034, iter [03600, 05004], lr: 0.094998, loss: 2.5388
2022-03-02 04:32:21 - train: epoch 0034, iter [03700, 05004], lr: 0.094998, loss: 2.6468
2022-03-02 04:32:54 - train: epoch 0034, iter [03800, 05004], lr: 0.094998, loss: 2.5751
2022-03-02 04:33:30 - train: epoch 0034, iter [03900, 05004], lr: 0.094998, loss: 2.9091
2022-03-02 04:34:03 - train: epoch 0034, iter [04000, 05004], lr: 0.094998, loss: 2.8223
2022-03-02 04:34:38 - train: epoch 0034, iter [04100, 05004], lr: 0.094998, loss: 2.8139
2022-03-02 04:35:13 - train: epoch 0034, iter [04200, 05004], lr: 0.094998, loss: 2.8864
2022-03-02 04:35:47 - train: epoch 0034, iter [04300, 05004], lr: 0.094998, loss: 2.7190
2022-03-02 04:36:22 - train: epoch 0034, iter [04400, 05004], lr: 0.094998, loss: 2.7686
2022-03-02 04:36:56 - train: epoch 0034, iter [04500, 05004], lr: 0.094998, loss: 2.8211
2022-03-02 04:37:30 - train: epoch 0034, iter [04600, 05004], lr: 0.094998, loss: 2.9551
2022-03-02 04:38:04 - train: epoch 0034, iter [04700, 05004], lr: 0.094998, loss: 2.9494
2022-03-02 04:38:39 - train: epoch 0034, iter [04800, 05004], lr: 0.094998, loss: 2.8568
2022-03-02 04:39:14 - train: epoch 0034, iter [04900, 05004], lr: 0.094998, loss: 2.8963
2022-03-02 04:39:46 - train: epoch 0034, iter [05000, 05004], lr: 0.094998, loss: 3.0596
2022-03-02 04:39:47 - train: epoch 034, train_loss: 2.7813
2022-03-02 04:41:01 - eval: epoch: 034, acc1: 52.782%, acc5: 78.776%, test_loss: 2.0152, per_image_load_time: 0.664ms, per_image_inference_time: 0.507ms
2022-03-02 04:41:02 - until epoch: 034, best_acc1: 53.734%
2022-03-02 04:41:02 - epoch 035 lr: 0.09464139109829321
2022-03-02 04:41:42 - train: epoch 0035, iter [00100, 05004], lr: 0.094641, loss: 2.6123
2022-03-02 04:42:16 - train: epoch 0035, iter [00200, 05004], lr: 0.094641, loss: 2.3080
2022-03-02 04:42:50 - train: epoch 0035, iter [00300, 05004], lr: 0.094641, loss: 2.7390
2022-03-02 04:43:24 - train: epoch 0035, iter [00400, 05004], lr: 0.094641, loss: 2.7235
2022-03-02 04:43:59 - train: epoch 0035, iter [00500, 05004], lr: 0.094641, loss: 2.5907
2022-03-02 04:44:33 - train: epoch 0035, iter [00600, 05004], lr: 0.094641, loss: 2.8359
2022-03-02 04:45:08 - train: epoch 0035, iter [00700, 05004], lr: 0.094641, loss: 2.6749
2022-03-02 04:45:42 - train: epoch 0035, iter [00800, 05004], lr: 0.094641, loss: 2.8261
2022-03-02 04:46:16 - train: epoch 0035, iter [00900, 05004], lr: 0.094641, loss: 2.9324
2022-03-02 04:46:51 - train: epoch 0035, iter [01000, 05004], lr: 0.094641, loss: 2.6453
2022-03-02 04:47:25 - train: epoch 0035, iter [01100, 05004], lr: 0.094641, loss: 3.1361
2022-03-02 04:48:00 - train: epoch 0035, iter [01200, 05004], lr: 0.094641, loss: 2.9493
2022-03-02 04:48:33 - train: epoch 0035, iter [01300, 05004], lr: 0.094641, loss: 2.8797
2022-03-02 04:49:07 - train: epoch 0035, iter [01400, 05004], lr: 0.094641, loss: 2.6770
2022-03-02 04:49:42 - train: epoch 0035, iter [01500, 05004], lr: 0.094641, loss: 2.7466
2022-03-02 04:50:17 - train: epoch 0035, iter [01600, 05004], lr: 0.094641, loss: 2.7303
2022-03-02 04:50:50 - train: epoch 0035, iter [01700, 05004], lr: 0.094641, loss: 2.5984
2022-03-02 04:51:26 - train: epoch 0035, iter [01800, 05004], lr: 0.094641, loss: 2.7878
2022-03-02 04:52:00 - train: epoch 0035, iter [01900, 05004], lr: 0.094641, loss: 2.8238
2022-03-02 04:52:34 - train: epoch 0035, iter [02000, 05004], lr: 0.094641, loss: 2.7599
2022-03-02 04:53:08 - train: epoch 0035, iter [02100, 05004], lr: 0.094641, loss: 2.6515
2022-03-02 04:53:43 - train: epoch 0035, iter [02200, 05004], lr: 0.094641, loss: 2.8989
2022-03-02 04:54:17 - train: epoch 0035, iter [02300, 05004], lr: 0.094641, loss: 2.6601
2022-03-02 04:54:51 - train: epoch 0035, iter [02400, 05004], lr: 0.094641, loss: 3.1255
2022-03-02 04:55:25 - train: epoch 0035, iter [02500, 05004], lr: 0.094641, loss: 2.6791
2022-03-02 04:55:59 - train: epoch 0035, iter [02600, 05004], lr: 0.094641, loss: 2.9467
2022-03-02 04:56:33 - train: epoch 0035, iter [02700, 05004], lr: 0.094641, loss: 2.6143
2022-03-02 04:57:08 - train: epoch 0035, iter [02800, 05004], lr: 0.094641, loss: 2.8320
2022-03-02 04:57:42 - train: epoch 0035, iter [02900, 05004], lr: 0.094641, loss: 2.8085
2022-03-02 04:58:16 - train: epoch 0035, iter [03000, 05004], lr: 0.094641, loss: 2.6695
2022-03-02 04:58:50 - train: epoch 0035, iter [03100, 05004], lr: 0.094641, loss: 2.6480
2022-03-02 04:59:24 - train: epoch 0035, iter [03200, 05004], lr: 0.094641, loss: 2.6740
2022-03-02 04:59:59 - train: epoch 0035, iter [03300, 05004], lr: 0.094641, loss: 2.7182
2022-03-02 05:00:33 - train: epoch 0035, iter [03400, 05004], lr: 0.094641, loss: 2.7455
2022-03-02 05:01:06 - train: epoch 0035, iter [03500, 05004], lr: 0.094641, loss: 2.6734
2022-03-02 05:01:41 - train: epoch 0035, iter [03600, 05004], lr: 0.094641, loss: 3.0139
2022-03-02 05:02:15 - train: epoch 0035, iter [03700, 05004], lr: 0.094641, loss: 2.7336
2022-03-02 05:02:49 - train: epoch 0035, iter [03800, 05004], lr: 0.094641, loss: 2.4357
2022-03-02 05:03:24 - train: epoch 0035, iter [03900, 05004], lr: 0.094641, loss: 2.7767
2022-03-02 05:03:59 - train: epoch 0035, iter [04000, 05004], lr: 0.094641, loss: 2.6562
2022-03-02 05:04:32 - train: epoch 0035, iter [04100, 05004], lr: 0.094641, loss: 2.9732
2022-03-02 05:05:07 - train: epoch 0035, iter [04200, 05004], lr: 0.094641, loss: 2.4722
2022-03-02 05:05:41 - train: epoch 0035, iter [04300, 05004], lr: 0.094641, loss: 2.6992
2022-03-02 05:06:15 - train: epoch 0035, iter [04400, 05004], lr: 0.094641, loss: 2.7567
2022-03-02 05:06:49 - train: epoch 0035, iter [04500, 05004], lr: 0.094641, loss: 2.7762
2022-03-02 05:07:23 - train: epoch 0035, iter [04600, 05004], lr: 0.094641, loss: 2.8760
2022-03-02 05:07:58 - train: epoch 0035, iter [04700, 05004], lr: 0.094641, loss: 3.1177
2022-03-02 05:08:33 - train: epoch 0035, iter [04800, 05004], lr: 0.094641, loss: 3.0267
2022-03-02 05:09:06 - train: epoch 0035, iter [04900, 05004], lr: 0.094641, loss: 2.8801
2022-03-02 05:09:39 - train: epoch 0035, iter [05000, 05004], lr: 0.094641, loss: 2.5745
2022-03-02 05:09:40 - train: epoch 035, train_loss: 2.7755
2022-03-02 05:10:55 - eval: epoch: 035, acc1: 52.076%, acc5: 77.894%, test_loss: 2.0596, per_image_load_time: 0.513ms, per_image_inference_time: 0.533ms
2022-03-02 05:10:55 - until epoch: 035, best_acc1: 53.734%
2022-03-02 05:10:55 - epoch 036 lr: 0.0942728012826605
2022-03-02 05:11:35 - train: epoch 0036, iter [00100, 05004], lr: 0.094273, loss: 2.9324
2022-03-02 05:12:09 - train: epoch 0036, iter [00200, 05004], lr: 0.094273, loss: 2.5367
2022-03-02 05:12:43 - train: epoch 0036, iter [00300, 05004], lr: 0.094273, loss: 2.6544
2022-03-02 05:13:17 - train: epoch 0036, iter [00400, 05004], lr: 0.094273, loss: 2.6840
2022-03-02 05:13:52 - train: epoch 0036, iter [00500, 05004], lr: 0.094273, loss: 2.7073
2022-03-02 05:14:26 - train: epoch 0036, iter [00600, 05004], lr: 0.094273, loss: 2.8384
2022-03-02 05:15:00 - train: epoch 0036, iter [00700, 05004], lr: 0.094273, loss: 2.7942
2022-03-02 05:15:34 - train: epoch 0036, iter [00800, 05004], lr: 0.094273, loss: 2.4886
2022-03-02 05:16:08 - train: epoch 0036, iter [00900, 05004], lr: 0.094273, loss: 2.7518
2022-03-02 05:16:43 - train: epoch 0036, iter [01000, 05004], lr: 0.094273, loss: 2.8989
2022-03-02 05:17:17 - train: epoch 0036, iter [01100, 05004], lr: 0.094273, loss: 2.7142
2022-03-02 05:17:52 - train: epoch 0036, iter [01200, 05004], lr: 0.094273, loss: 2.8056
2022-03-02 05:18:26 - train: epoch 0036, iter [01300, 05004], lr: 0.094273, loss: 2.8315
2022-03-02 05:19:01 - train: epoch 0036, iter [01400, 05004], lr: 0.094273, loss: 2.8415
2022-03-02 05:19:35 - train: epoch 0036, iter [01500, 05004], lr: 0.094273, loss: 2.9814
2022-03-02 05:20:09 - train: epoch 0036, iter [01600, 05004], lr: 0.094273, loss: 2.6310
2022-03-02 05:20:44 - train: epoch 0036, iter [01700, 05004], lr: 0.094273, loss: 2.8450
2022-03-02 05:21:18 - train: epoch 0036, iter [01800, 05004], lr: 0.094273, loss: 2.8048
2022-03-02 05:21:52 - train: epoch 0036, iter [01900, 05004], lr: 0.094273, loss: 2.7373
2022-03-02 05:22:26 - train: epoch 0036, iter [02000, 05004], lr: 0.094273, loss: 2.7841
2022-03-02 05:23:01 - train: epoch 0036, iter [02100, 05004], lr: 0.094273, loss: 2.6154
2022-03-02 05:23:35 - train: epoch 0036, iter [02200, 05004], lr: 0.094273, loss: 3.0023
2022-03-02 05:24:10 - train: epoch 0036, iter [02300, 05004], lr: 0.094273, loss: 2.7159
2022-03-02 05:24:44 - train: epoch 0036, iter [02400, 05004], lr: 0.094273, loss: 2.8431
2022-03-02 05:25:19 - train: epoch 0036, iter [02500, 05004], lr: 0.094273, loss: 2.7327
2022-03-02 05:25:53 - train: epoch 0036, iter [02600, 05004], lr: 0.094273, loss: 3.0650
2022-03-02 05:26:27 - train: epoch 0036, iter [02700, 05004], lr: 0.094273, loss: 2.5254
2022-03-02 05:27:01 - train: epoch 0036, iter [02800, 05004], lr: 0.094273, loss: 2.6888
2022-03-02 05:27:36 - train: epoch 0036, iter [02900, 05004], lr: 0.094273, loss: 2.4234
2022-03-02 05:28:10 - train: epoch 0036, iter [03000, 05004], lr: 0.094273, loss: 2.8003
2022-03-02 05:28:44 - train: epoch 0036, iter [03100, 05004], lr: 0.094273, loss: 2.7643
2022-03-02 05:29:18 - train: epoch 0036, iter [03200, 05004], lr: 0.094273, loss: 2.8093
2022-03-02 05:29:52 - train: epoch 0036, iter [03300, 05004], lr: 0.094273, loss: 2.6018
2022-03-02 05:30:26 - train: epoch 0036, iter [03400, 05004], lr: 0.094273, loss: 2.9312
2022-03-02 05:31:01 - train: epoch 0036, iter [03500, 05004], lr: 0.094273, loss: 3.0064
2022-03-02 05:31:34 - train: epoch 0036, iter [03600, 05004], lr: 0.094273, loss: 2.6951
2022-03-02 05:32:09 - train: epoch 0036, iter [03700, 05004], lr: 0.094273, loss: 2.6712
2022-03-02 05:32:43 - train: epoch 0036, iter [03800, 05004], lr: 0.094273, loss: 2.6189
2022-03-02 05:33:17 - train: epoch 0036, iter [03900, 05004], lr: 0.094273, loss: 2.8529
2022-03-02 05:33:52 - train: epoch 0036, iter [04000, 05004], lr: 0.094273, loss: 2.8268
2022-03-02 05:34:26 - train: epoch 0036, iter [04100, 05004], lr: 0.094273, loss: 2.6992
2022-03-02 05:35:00 - train: epoch 0036, iter [04200, 05004], lr: 0.094273, loss: 2.7120
2022-03-02 05:35:35 - train: epoch 0036, iter [04300, 05004], lr: 0.094273, loss: 2.8527
2022-03-02 05:36:10 - train: epoch 0036, iter [04400, 05004], lr: 0.094273, loss: 2.8592
2022-03-02 05:36:44 - train: epoch 0036, iter [04500, 05004], lr: 0.094273, loss: 2.7549
2022-03-02 05:37:18 - train: epoch 0036, iter [04600, 05004], lr: 0.094273, loss: 2.7325
2022-03-02 05:37:52 - train: epoch 0036, iter [04700, 05004], lr: 0.094273, loss: 2.8849
2022-03-02 05:38:27 - train: epoch 0036, iter [04800, 05004], lr: 0.094273, loss: 2.8080
2022-03-02 05:39:01 - train: epoch 0036, iter [04900, 05004], lr: 0.094273, loss: 2.7158
2022-03-02 05:39:35 - train: epoch 0036, iter [05000, 05004], lr: 0.094273, loss: 2.7581
2022-03-02 05:39:36 - train: epoch 036, train_loss: 2.7691
2022-03-02 05:40:50 - eval: epoch: 036, acc1: 53.480%, acc5: 78.884%, test_loss: 1.9768, per_image_load_time: 0.945ms, per_image_inference_time: 0.517ms
2022-03-02 05:40:51 - until epoch: 036, best_acc1: 53.734%
2022-03-02 05:40:51 - epoch 037 lr: 0.09389272045892023
2022-03-02 05:41:30 - train: epoch 0037, iter [00100, 05004], lr: 0.093893, loss: 2.7451
2022-03-02 05:42:05 - train: epoch 0037, iter [00200, 05004], lr: 0.093893, loss: 2.5322
2022-03-02 05:42:39 - train: epoch 0037, iter [00300, 05004], lr: 0.093893, loss: 2.6539
2022-03-02 05:43:13 - train: epoch 0037, iter [00400, 05004], lr: 0.093893, loss: 2.6362
2022-03-02 05:43:47 - train: epoch 0037, iter [00500, 05004], lr: 0.093893, loss: 2.8443
2022-03-02 05:44:21 - train: epoch 0037, iter [00600, 05004], lr: 0.093893, loss: 2.8840
2022-03-02 05:44:55 - train: epoch 0037, iter [00700, 05004], lr: 0.093893, loss: 2.7206
2022-03-02 05:45:30 - train: epoch 0037, iter [00800, 05004], lr: 0.093893, loss: 2.8259
2022-03-02 05:46:04 - train: epoch 0037, iter [00900, 05004], lr: 0.093893, loss: 2.7855
2022-03-02 05:46:38 - train: epoch 0037, iter [01000, 05004], lr: 0.093893, loss: 2.5597
2022-03-02 05:47:14 - train: epoch 0037, iter [01100, 05004], lr: 0.093893, loss: 2.7478
2022-03-02 05:47:47 - train: epoch 0037, iter [01200, 05004], lr: 0.093893, loss: 2.5720
2022-03-02 05:48:21 - train: epoch 0037, iter [01300, 05004], lr: 0.093893, loss: 2.8554
2022-03-02 05:48:56 - train: epoch 0037, iter [01400, 05004], lr: 0.093893, loss: 3.0166
2022-03-02 05:49:29 - train: epoch 0037, iter [01500, 05004], lr: 0.093893, loss: 2.8102
2022-03-02 05:50:04 - train: epoch 0037, iter [01600, 05004], lr: 0.093893, loss: 2.5246
2022-03-02 05:50:38 - train: epoch 0037, iter [01700, 05004], lr: 0.093893, loss: 2.7530
2022-03-02 05:51:13 - train: epoch 0037, iter [01800, 05004], lr: 0.093893, loss: 2.5471
2022-03-02 05:51:46 - train: epoch 0037, iter [01900, 05004], lr: 0.093893, loss: 2.6429
2022-03-02 05:52:21 - train: epoch 0037, iter [02000, 05004], lr: 0.093893, loss: 2.8597
2022-03-02 05:52:55 - train: epoch 0037, iter [02100, 05004], lr: 0.093893, loss: 2.9338
2022-03-02 05:53:30 - train: epoch 0037, iter [02200, 05004], lr: 0.093893, loss: 2.5241
2022-03-02 05:54:03 - train: epoch 0037, iter [02300, 05004], lr: 0.093893, loss: 2.7183
2022-03-02 05:54:36 - train: epoch 0037, iter [02400, 05004], lr: 0.093893, loss: 2.8168
2022-03-02 05:55:11 - train: epoch 0037, iter [02500, 05004], lr: 0.093893, loss: 2.7470
2022-03-02 05:55:45 - train: epoch 0037, iter [02600, 05004], lr: 0.093893, loss: 3.0575
2022-03-02 05:56:19 - train: epoch 0037, iter [02700, 05004], lr: 0.093893, loss: 2.8749
2022-03-02 05:56:53 - train: epoch 0037, iter [02800, 05004], lr: 0.093893, loss: 2.9200
2022-03-02 05:57:28 - train: epoch 0037, iter [02900, 05004], lr: 0.093893, loss: 2.6658
2022-03-02 05:58:02 - train: epoch 0037, iter [03000, 05004], lr: 0.093893, loss: 2.8512
2022-03-02 05:58:36 - train: epoch 0037, iter [03100, 05004], lr: 0.093893, loss: 2.8328
2022-03-02 05:59:11 - train: epoch 0037, iter [03200, 05004], lr: 0.093893, loss: 2.9616
2022-03-02 05:59:44 - train: epoch 0037, iter [03300, 05004], lr: 0.093893, loss: 2.6003
2022-03-02 06:00:18 - train: epoch 0037, iter [03400, 05004], lr: 0.093893, loss: 2.8229
2022-03-02 06:00:53 - train: epoch 0037, iter [03500, 05004], lr: 0.093893, loss: 2.7354
2022-03-02 06:01:27 - train: epoch 0037, iter [03600, 05004], lr: 0.093893, loss: 2.8769
2022-03-02 06:02:01 - train: epoch 0037, iter [03700, 05004], lr: 0.093893, loss: 2.9179
2022-03-02 06:02:35 - train: epoch 0037, iter [03800, 05004], lr: 0.093893, loss: 2.7257
2022-03-02 06:03:09 - train: epoch 0037, iter [03900, 05004], lr: 0.093893, loss: 2.9243
2022-03-02 06:03:43 - train: epoch 0037, iter [04000, 05004], lr: 0.093893, loss: 2.7969
2022-03-02 06:04:18 - train: epoch 0037, iter [04100, 05004], lr: 0.093893, loss: 2.7521
2022-03-02 06:04:52 - train: epoch 0037, iter [04200, 05004], lr: 0.093893, loss: 3.0072
2022-03-02 06:05:27 - train: epoch 0037, iter [04300, 05004], lr: 0.093893, loss: 2.7695
2022-03-02 06:06:01 - train: epoch 0037, iter [04400, 05004], lr: 0.093893, loss: 2.6025
2022-03-02 06:06:35 - train: epoch 0037, iter [04500, 05004], lr: 0.093893, loss: 2.7015
2022-03-02 06:07:10 - train: epoch 0037, iter [04600, 05004], lr: 0.093893, loss: 2.7986
2022-03-02 06:07:44 - train: epoch 0037, iter [04700, 05004], lr: 0.093893, loss: 2.8608
2022-03-02 06:08:18 - train: epoch 0037, iter [04800, 05004], lr: 0.093893, loss: 2.7284
2022-03-02 06:08:52 - train: epoch 0037, iter [04900, 05004], lr: 0.093893, loss: 2.9257
2022-03-02 06:09:25 - train: epoch 0037, iter [05000, 05004], lr: 0.093893, loss: 2.6369
2022-03-02 06:09:26 - train: epoch 037, train_loss: 2.7672
2022-03-02 06:10:41 - eval: epoch: 037, acc1: 49.356%, acc5: 75.884%, test_loss: 2.1775, per_image_load_time: 0.545ms, per_image_inference_time: 0.518ms
2022-03-02 06:10:42 - until epoch: 037, best_acc1: 53.734%
2022-03-02 06:10:42 - epoch 038 lr: 0.09350124727707197
2022-03-02 06:11:21 - train: epoch 0038, iter [00100, 05004], lr: 0.093501, loss: 2.6579
2022-03-02 06:11:55 - train: epoch 0038, iter [00200, 05004], lr: 0.093501, loss: 2.5284
2022-03-02 06:12:28 - train: epoch 0038, iter [00300, 05004], lr: 0.093501, loss: 2.9359
2022-03-02 06:13:03 - train: epoch 0038, iter [00400, 05004], lr: 0.093501, loss: 2.6689
2022-03-02 06:13:37 - train: epoch 0038, iter [00500, 05004], lr: 0.093501, loss: 2.5252
2022-03-02 06:14:12 - train: epoch 0038, iter [00600, 05004], lr: 0.093501, loss: 2.8493
2022-03-02 06:14:46 - train: epoch 0038, iter [00700, 05004], lr: 0.093501, loss: 2.6702
2022-03-02 06:15:20 - train: epoch 0038, iter [00800, 05004], lr: 0.093501, loss: 2.5731
2022-03-02 06:15:55 - train: epoch 0038, iter [00900, 05004], lr: 0.093501, loss: 2.9256
2022-03-02 06:16:29 - train: epoch 0038, iter [01000, 05004], lr: 0.093501, loss: 2.9339
2022-03-02 06:17:03 - train: epoch 0038, iter [01100, 05004], lr: 0.093501, loss: 2.8116
2022-03-02 06:17:38 - train: epoch 0038, iter [01200, 05004], lr: 0.093501, loss: 2.7869
2022-03-02 06:18:12 - train: epoch 0038, iter [01300, 05004], lr: 0.093501, loss: 2.8235
2022-03-02 06:18:47 - train: epoch 0038, iter [01400, 05004], lr: 0.093501, loss: 2.6699
2022-03-02 06:19:20 - train: epoch 0038, iter [01500, 05004], lr: 0.093501, loss: 2.6017
2022-03-02 06:19:55 - train: epoch 0038, iter [01600, 05004], lr: 0.093501, loss: 2.7814
2022-03-02 06:20:29 - train: epoch 0038, iter [01700, 05004], lr: 0.093501, loss: 3.0190
2022-03-02 06:21:03 - train: epoch 0038, iter [01800, 05004], lr: 0.093501, loss: 2.9096
2022-03-02 06:21:38 - train: epoch 0038, iter [01900, 05004], lr: 0.093501, loss: 2.5927
2022-03-02 06:22:12 - train: epoch 0038, iter [02000, 05004], lr: 0.093501, loss: 2.8712
2022-03-02 06:22:47 - train: epoch 0038, iter [02100, 05004], lr: 0.093501, loss: 2.7615
2022-03-02 06:23:21 - train: epoch 0038, iter [02200, 05004], lr: 0.093501, loss: 2.6392
2022-03-02 06:23:55 - train: epoch 0038, iter [02300, 05004], lr: 0.093501, loss: 2.9501
2022-03-02 06:24:31 - train: epoch 0038, iter [02400, 05004], lr: 0.093501, loss: 3.0415
2022-03-02 06:25:05 - train: epoch 0038, iter [02500, 05004], lr: 0.093501, loss: 2.8099
2022-03-02 06:25:39 - train: epoch 0038, iter [02600, 05004], lr: 0.093501, loss: 2.8061
2022-03-02 06:26:13 - train: epoch 0038, iter [02700, 05004], lr: 0.093501, loss: 2.8295
2022-03-02 06:26:48 - train: epoch 0038, iter [02800, 05004], lr: 0.093501, loss: 2.9388
2022-03-02 06:27:23 - train: epoch 0038, iter [02900, 05004], lr: 0.093501, loss: 2.7531
2022-03-02 06:27:57 - train: epoch 0038, iter [03000, 05004], lr: 0.093501, loss: 2.7128
2022-03-02 06:28:32 - train: epoch 0038, iter [03100, 05004], lr: 0.093501, loss: 2.7846
2022-03-02 06:29:06 - train: epoch 0038, iter [03200, 05004], lr: 0.093501, loss: 2.6727
2022-03-02 06:29:41 - train: epoch 0038, iter [03300, 05004], lr: 0.093501, loss: 2.6130
2022-03-02 06:30:15 - train: epoch 0038, iter [03400, 05004], lr: 0.093501, loss: 2.5638
2022-03-02 06:30:50 - train: epoch 0038, iter [03500, 05004], lr: 0.093501, loss: 2.9353
2022-03-02 06:31:25 - train: epoch 0038, iter [03600, 05004], lr: 0.093501, loss: 2.9810
2022-03-02 06:31:59 - train: epoch 0038, iter [03700, 05004], lr: 0.093501, loss: 2.7081
2022-03-02 06:32:33 - train: epoch 0038, iter [03800, 05004], lr: 0.093501, loss: 2.8813
2022-03-02 06:33:07 - train: epoch 0038, iter [03900, 05004], lr: 0.093501, loss: 2.5882
2022-03-02 06:33:42 - train: epoch 0038, iter [04000, 05004], lr: 0.093501, loss: 2.7328
2022-03-02 06:34:17 - train: epoch 0038, iter [04100, 05004], lr: 0.093501, loss: 2.7346
2022-03-02 06:34:51 - train: epoch 0038, iter [04200, 05004], lr: 0.093501, loss: 2.6247
2022-03-02 06:35:24 - train: epoch 0038, iter [04300, 05004], lr: 0.093501, loss: 2.9458
2022-03-02 06:35:59 - train: epoch 0038, iter [04400, 05004], lr: 0.093501, loss: 2.7383
2022-03-02 06:36:34 - train: epoch 0038, iter [04500, 05004], lr: 0.093501, loss: 2.9227
2022-03-02 06:37:08 - train: epoch 0038, iter [04600, 05004], lr: 0.093501, loss: 2.7678
2022-03-02 06:37:43 - train: epoch 0038, iter [04700, 05004], lr: 0.093501, loss: 2.6429
2022-03-02 06:38:18 - train: epoch 0038, iter [04800, 05004], lr: 0.093501, loss: 2.6949
2022-03-02 06:38:52 - train: epoch 0038, iter [04900, 05004], lr: 0.093501, loss: 2.7915
2022-03-02 06:39:25 - train: epoch 0038, iter [05000, 05004], lr: 0.093501, loss: 2.5597
2022-03-02 06:39:26 - train: epoch 038, train_loss: 2.7626
2022-03-02 06:40:42 - eval: epoch: 038, acc1: 52.812%, acc5: 78.542%, test_loss: 2.0114, per_image_load_time: 0.475ms, per_image_inference_time: 0.489ms
2022-03-02 06:40:42 - until epoch: 038, best_acc1: 53.734%
2022-03-02 06:40:42 - epoch 039 lr: 0.09309848334400246
2022-03-02 06:41:23 - train: epoch 0039, iter [00100, 05004], lr: 0.093098, loss: 2.7624
2022-03-02 06:41:57 - train: epoch 0039, iter [00200, 05004], lr: 0.093098, loss: 2.8328
2022-03-02 06:42:31 - train: epoch 0039, iter [00300, 05004], lr: 0.093098, loss: 2.4754
2022-03-02 06:43:05 - train: epoch 0039, iter [00400, 05004], lr: 0.093098, loss: 2.6157
2022-03-02 06:43:40 - train: epoch 0039, iter [00500, 05004], lr: 0.093098, loss: 2.5951
2022-03-02 06:44:14 - train: epoch 0039, iter [00600, 05004], lr: 0.093098, loss: 2.7784
2022-03-02 06:44:49 - train: epoch 0039, iter [00700, 05004], lr: 0.093098, loss: 3.0542
2022-03-02 06:45:23 - train: epoch 0039, iter [00800, 05004], lr: 0.093098, loss: 2.6644
2022-03-02 06:45:58 - train: epoch 0039, iter [00900, 05004], lr: 0.093098, loss: 2.7146
2022-03-02 06:46:32 - train: epoch 0039, iter [01000, 05004], lr: 0.093098, loss: 2.9267
2022-03-02 06:47:07 - train: epoch 0039, iter [01100, 05004], lr: 0.093098, loss: 2.8419
2022-03-02 06:47:41 - train: epoch 0039, iter [01200, 05004], lr: 0.093098, loss: 2.9036
2022-03-02 06:48:15 - train: epoch 0039, iter [01300, 05004], lr: 0.093098, loss: 2.9367
2022-03-02 06:48:49 - train: epoch 0039, iter [01400, 05004], lr: 0.093098, loss: 2.8872
2022-03-02 06:49:24 - train: epoch 0039, iter [01500, 05004], lr: 0.093098, loss: 2.6909
2022-03-02 06:49:58 - train: epoch 0039, iter [01600, 05004], lr: 0.093098, loss: 2.7852
2022-03-02 06:50:32 - train: epoch 0039, iter [01700, 05004], lr: 0.093098, loss: 2.7205
2022-03-02 06:51:07 - train: epoch 0039, iter [01800, 05004], lr: 0.093098, loss: 2.9083
2022-03-02 06:51:42 - train: epoch 0039, iter [01900, 05004], lr: 0.093098, loss: 2.4391
2022-03-02 06:52:16 - train: epoch 0039, iter [02000, 05004], lr: 0.093098, loss: 2.8339
2022-03-02 06:52:51 - train: epoch 0039, iter [02100, 05004], lr: 0.093098, loss: 3.0475
2022-03-02 06:53:25 - train: epoch 0039, iter [02200, 05004], lr: 0.093098, loss: 2.7064
2022-03-02 06:54:00 - train: epoch 0039, iter [02300, 05004], lr: 0.093098, loss: 3.1848
2022-03-02 06:54:35 - train: epoch 0039, iter [02400, 05004], lr: 0.093098, loss: 2.7194
2022-03-02 06:55:09 - train: epoch 0039, iter [02500, 05004], lr: 0.093098, loss: 2.6518
2022-03-02 06:55:44 - train: epoch 0039, iter [02600, 05004], lr: 0.093098, loss: 2.6881
2022-03-02 06:56:18 - train: epoch 0039, iter [02700, 05004], lr: 0.093098, loss: 2.7427
2022-03-02 06:56:52 - train: epoch 0039, iter [02800, 05004], lr: 0.093098, loss: 2.7725
2022-03-02 06:57:27 - train: epoch 0039, iter [02900, 05004], lr: 0.093098, loss: 2.5073
2022-03-02 06:58:01 - train: epoch 0039, iter [03000, 05004], lr: 0.093098, loss: 2.9653
2022-03-02 06:58:36 - train: epoch 0039, iter [03100, 05004], lr: 0.093098, loss: 2.7510
2022-03-02 06:59:10 - train: epoch 0039, iter [03200, 05004], lr: 0.093098, loss: 2.7742
2022-03-02 06:59:43 - train: epoch 0039, iter [03300, 05004], lr: 0.093098, loss: 2.8551
2022-03-02 07:00:17 - train: epoch 0039, iter [03400, 05004], lr: 0.093098, loss: 2.8357
2022-03-02 07:00:52 - train: epoch 0039, iter [03500, 05004], lr: 0.093098, loss: 3.0351
2022-03-02 07:01:27 - train: epoch 0039, iter [03600, 05004], lr: 0.093098, loss: 2.6763
2022-03-02 07:02:02 - train: epoch 0039, iter [03700, 05004], lr: 0.093098, loss: 2.5861
2022-03-02 07:02:36 - train: epoch 0039, iter [03800, 05004], lr: 0.093098, loss: 2.8558
2022-03-02 07:03:10 - train: epoch 0039, iter [03900, 05004], lr: 0.093098, loss: 2.9513
2022-03-02 07:03:46 - train: epoch 0039, iter [04000, 05004], lr: 0.093098, loss: 2.6794
2022-03-02 07:04:20 - train: epoch 0039, iter [04100, 05004], lr: 0.093098, loss: 2.6310
2022-03-02 07:04:54 - train: epoch 0039, iter [04200, 05004], lr: 0.093098, loss: 2.7626
2022-03-02 07:05:29 - train: epoch 0039, iter [04300, 05004], lr: 0.093098, loss: 2.5908
2022-03-02 07:06:03 - train: epoch 0039, iter [04400, 05004], lr: 0.093098, loss: 2.8794
2022-03-02 07:06:38 - train: epoch 0039, iter [04500, 05004], lr: 0.093098, loss: 2.6645
2022-03-02 07:07:12 - train: epoch 0039, iter [04600, 05004], lr: 0.093098, loss: 2.8536
2022-03-02 07:07:45 - train: epoch 0039, iter [04700, 05004], lr: 0.093098, loss: 2.6852
2022-03-02 07:08:20 - train: epoch 0039, iter [04800, 05004], lr: 0.093098, loss: 2.7587
2022-03-02 07:08:56 - train: epoch 0039, iter [04900, 05004], lr: 0.093098, loss: 2.8505
2022-03-02 07:09:28 - train: epoch 0039, iter [05000, 05004], lr: 0.093098, loss: 2.6651
2022-03-02 07:09:30 - train: epoch 039, train_loss: 2.7610
2022-03-02 07:10:44 - eval: epoch: 039, acc1: 51.188%, acc5: 77.500%, test_loss: 2.0770, per_image_load_time: 0.511ms, per_image_inference_time: 0.533ms
2022-03-02 07:10:45 - until epoch: 039, best_acc1: 53.734%
2022-03-02 07:10:45 - epoch 040 lr: 0.09268453319711362
2022-03-02 07:11:24 - train: epoch 0040, iter [00100, 05004], lr: 0.092685, loss: 3.0806
2022-03-02 07:11:58 - train: epoch 0040, iter [00200, 05004], lr: 0.092685, loss: 2.8014
2022-03-02 07:12:33 - train: epoch 0040, iter [00300, 05004], lr: 0.092685, loss: 2.6732
2022-03-02 07:13:07 - train: epoch 0040, iter [00400, 05004], lr: 0.092685, loss: 2.9022
2022-03-02 07:13:41 - train: epoch 0040, iter [00500, 05004], lr: 0.092685, loss: 2.5700
2022-03-02 07:14:15 - train: epoch 0040, iter [00600, 05004], lr: 0.092685, loss: 2.8099
2022-03-02 07:14:50 - train: epoch 0040, iter [00700, 05004], lr: 0.092685, loss: 2.7395
2022-03-02 07:15:23 - train: epoch 0040, iter [00800, 05004], lr: 0.092685, loss: 2.7526
2022-03-02 07:15:58 - train: epoch 0040, iter [00900, 05004], lr: 0.092685, loss: 2.5756
2022-03-02 07:16:32 - train: epoch 0040, iter [01000, 05004], lr: 0.092685, loss: 2.6265
2022-03-02 07:17:07 - train: epoch 0040, iter [01100, 05004], lr: 0.092685, loss: 2.4763
2022-03-02 07:17:41 - train: epoch 0040, iter [01200, 05004], lr: 0.092685, loss: 2.7414
2022-03-02 07:18:15 - train: epoch 0040, iter [01300, 05004], lr: 0.092685, loss: 2.8633
2022-03-02 07:18:49 - train: epoch 0040, iter [01400, 05004], lr: 0.092685, loss: 2.6606
2022-03-02 07:19:22 - train: epoch 0040, iter [01500, 05004], lr: 0.092685, loss: 2.8778
2022-03-02 07:19:56 - train: epoch 0040, iter [01600, 05004], lr: 0.092685, loss: 2.6721
2022-03-02 07:20:31 - train: epoch 0040, iter [01700, 05004], lr: 0.092685, loss: 2.8599
2022-03-02 07:21:06 - train: epoch 0040, iter [01800, 05004], lr: 0.092685, loss: 2.4240
2022-03-02 07:21:40 - train: epoch 0040, iter [01900, 05004], lr: 0.092685, loss: 2.7416
2022-03-02 07:22:14 - train: epoch 0040, iter [02000, 05004], lr: 0.092685, loss: 2.8739
2022-03-02 07:22:49 - train: epoch 0040, iter [02100, 05004], lr: 0.092685, loss: 2.6446
2022-03-02 07:23:24 - train: epoch 0040, iter [02200, 05004], lr: 0.092685, loss: 2.5523
2022-03-02 07:23:57 - train: epoch 0040, iter [02300, 05004], lr: 0.092685, loss: 2.6689
2022-03-02 07:24:31 - train: epoch 0040, iter [02400, 05004], lr: 0.092685, loss: 2.5948
2022-03-02 07:25:06 - train: epoch 0040, iter [02500, 05004], lr: 0.092685, loss: 2.6086
2022-03-02 07:25:40 - train: epoch 0040, iter [02600, 05004], lr: 0.092685, loss: 2.8790
2022-03-02 07:26:14 - train: epoch 0040, iter [02700, 05004], lr: 0.092685, loss: 2.8651
2022-03-02 07:26:48 - train: epoch 0040, iter [02800, 05004], lr: 0.092685, loss: 2.8933
2022-03-02 07:27:23 - train: epoch 0040, iter [02900, 05004], lr: 0.092685, loss: 2.9732
2022-03-02 07:27:57 - train: epoch 0040, iter [03000, 05004], lr: 0.092685, loss: 2.8942
2022-03-02 07:28:31 - train: epoch 0040, iter [03100, 05004], lr: 0.092685, loss: 2.8826
2022-03-02 07:29:05 - train: epoch 0040, iter [03200, 05004], lr: 0.092685, loss: 3.0379
2022-03-02 07:29:39 - train: epoch 0040, iter [03300, 05004], lr: 0.092685, loss: 2.9388
2022-03-02 07:30:14 - train: epoch 0040, iter [03400, 05004], lr: 0.092685, loss: 2.7509
2022-03-02 07:30:48 - train: epoch 0040, iter [03500, 05004], lr: 0.092685, loss: 2.8047
2022-03-02 07:31:23 - train: epoch 0040, iter [03600, 05004], lr: 0.092685, loss: 2.9107
2022-03-02 07:31:57 - train: epoch 0040, iter [03700, 05004], lr: 0.092685, loss: 2.7459
2022-03-02 07:32:32 - train: epoch 0040, iter [03800, 05004], lr: 0.092685, loss: 2.8528
2022-03-02 07:33:05 - train: epoch 0040, iter [03900, 05004], lr: 0.092685, loss: 2.8441
2022-03-02 07:33:40 - train: epoch 0040, iter [04000, 05004], lr: 0.092685, loss: 2.9340
2022-03-02 07:34:15 - train: epoch 0040, iter [04100, 05004], lr: 0.092685, loss: 2.6824
2022-03-02 07:34:50 - train: epoch 0040, iter [04200, 05004], lr: 0.092685, loss: 2.7264
2022-03-02 07:35:25 - train: epoch 0040, iter [04300, 05004], lr: 0.092685, loss: 2.7636
2022-03-02 07:35:59 - train: epoch 0040, iter [04400, 05004], lr: 0.092685, loss: 2.7700
2022-03-02 07:36:33 - train: epoch 0040, iter [04500, 05004], lr: 0.092685, loss: 2.5142
2022-03-02 07:37:08 - train: epoch 0040, iter [04600, 05004], lr: 0.092685, loss: 2.7658
2022-03-02 07:37:42 - train: epoch 0040, iter [04700, 05004], lr: 0.092685, loss: 2.9608
2022-03-02 07:38:17 - train: epoch 0040, iter [04800, 05004], lr: 0.092685, loss: 2.5294
2022-03-02 07:38:51 - train: epoch 0040, iter [04900, 05004], lr: 0.092685, loss: 2.7859
2022-03-02 07:39:24 - train: epoch 0040, iter [05000, 05004], lr: 0.092685, loss: 2.7272
2022-03-02 07:39:25 - train: epoch 040, train_loss: 2.7558
2022-03-02 07:40:40 - eval: epoch: 040, acc1: 53.340%, acc5: 79.038%, test_loss: 1.9952, per_image_load_time: 0.500ms, per_image_inference_time: 0.506ms
2022-03-02 07:40:41 - until epoch: 040, best_acc1: 53.734%
2022-03-02 07:40:41 - epoch 041 lr: 0.09225950427718975
2022-03-02 07:41:21 - train: epoch 0041, iter [00100, 05004], lr: 0.092260, loss: 3.0717
2022-03-02 07:41:55 - train: epoch 0041, iter [00200, 05004], lr: 0.092260, loss: 3.2050
2022-03-02 07:42:30 - train: epoch 0041, iter [00300, 05004], lr: 0.092260, loss: 2.8796
2022-03-02 07:43:04 - train: epoch 0041, iter [00400, 05004], lr: 0.092260, loss: 2.7626
2022-03-02 07:43:38 - train: epoch 0041, iter [00500, 05004], lr: 0.092260, loss: 2.4924
2022-03-02 07:44:12 - train: epoch 0041, iter [00600, 05004], lr: 0.092260, loss: 2.9050
2022-03-02 07:44:47 - train: epoch 0041, iter [00700, 05004], lr: 0.092260, loss: 2.4871
2022-03-02 07:45:21 - train: epoch 0041, iter [00800, 05004], lr: 0.092260, loss: 2.5885
2022-03-02 07:45:56 - train: epoch 0041, iter [00900, 05004], lr: 0.092260, loss: 2.4572
2022-03-02 07:46:30 - train: epoch 0041, iter [01000, 05004], lr: 0.092260, loss: 2.8831
2022-03-02 07:47:04 - train: epoch 0041, iter [01100, 05004], lr: 0.092260, loss: 2.7555
2022-03-02 07:47:40 - train: epoch 0041, iter [01200, 05004], lr: 0.092260, loss: 2.7965
2022-03-02 07:48:13 - train: epoch 0041, iter [01300, 05004], lr: 0.092260, loss: 2.5806
2022-03-02 07:48:48 - train: epoch 0041, iter [01400, 05004], lr: 0.092260, loss: 2.9670
2022-03-02 07:49:22 - train: epoch 0041, iter [01500, 05004], lr: 0.092260, loss: 2.9481
2022-03-02 07:49:56 - train: epoch 0041, iter [01600, 05004], lr: 0.092260, loss: 2.6028
2022-03-02 07:50:30 - train: epoch 0041, iter [01700, 05004], lr: 0.092260, loss: 2.9316
2022-03-02 07:51:04 - train: epoch 0041, iter [01800, 05004], lr: 0.092260, loss: 2.7187
2022-03-02 07:51:38 - train: epoch 0041, iter [01900, 05004], lr: 0.092260, loss: 2.8917
2022-03-02 07:52:13 - train: epoch 0041, iter [02000, 05004], lr: 0.092260, loss: 2.8404
2022-03-02 07:52:48 - train: epoch 0041, iter [02100, 05004], lr: 0.092260, loss: 2.7611
2022-03-02 07:53:22 - train: epoch 0041, iter [02200, 05004], lr: 0.092260, loss: 2.4957
2022-03-02 07:53:56 - train: epoch 0041, iter [02300, 05004], lr: 0.092260, loss: 2.6527
2022-03-02 07:54:30 - train: epoch 0041, iter [02400, 05004], lr: 0.092260, loss: 2.7490
2022-03-02 07:55:05 - train: epoch 0041, iter [02500, 05004], lr: 0.092260, loss: 2.8132
2022-03-02 07:55:39 - train: epoch 0041, iter [02600, 05004], lr: 0.092260, loss: 3.0241
2022-03-02 07:56:13 - train: epoch 0041, iter [02700, 05004], lr: 0.092260, loss: 3.0634
2022-03-02 07:56:48 - train: epoch 0041, iter [02800, 05004], lr: 0.092260, loss: 2.8823
2022-03-02 07:57:22 - train: epoch 0041, iter [02900, 05004], lr: 0.092260, loss: 2.9836
2022-03-02 07:57:57 - train: epoch 0041, iter [03000, 05004], lr: 0.092260, loss: 2.7917
2022-03-02 07:58:31 - train: epoch 0041, iter [03100, 05004], lr: 0.092260, loss: 2.9501
2022-03-02 07:59:05 - train: epoch 0041, iter [03200, 05004], lr: 0.092260, loss: 2.6386
2022-03-02 07:59:40 - train: epoch 0041, iter [03300, 05004], lr: 0.092260, loss: 2.7523
2022-03-02 08:00:14 - train: epoch 0041, iter [03400, 05004], lr: 0.092260, loss: 2.6532
2022-03-02 08:00:49 - train: epoch 0041, iter [03500, 05004], lr: 0.092260, loss: 2.7881
2022-03-02 08:01:23 - train: epoch 0041, iter [03600, 05004], lr: 0.092260, loss: 2.8041
2022-03-02 08:01:57 - train: epoch 0041, iter [03700, 05004], lr: 0.092260, loss: 2.9284
2022-03-02 08:02:32 - train: epoch 0041, iter [03800, 05004], lr: 0.092260, loss: 2.8220
2022-03-02 08:03:06 - train: epoch 0041, iter [03900, 05004], lr: 0.092260, loss: 2.5837
2022-03-02 08:03:40 - train: epoch 0041, iter [04000, 05004], lr: 0.092260, loss: 2.8054
2022-03-02 08:04:14 - train: epoch 0041, iter [04100, 05004], lr: 0.092260, loss: 2.9248
2022-03-02 08:04:48 - train: epoch 0041, iter [04200, 05004], lr: 0.092260, loss: 2.4493
2022-03-02 08:05:23 - train: epoch 0041, iter [04300, 05004], lr: 0.092260, loss: 2.6495
2022-03-02 08:05:57 - train: epoch 0041, iter [04400, 05004], lr: 0.092260, loss: 2.7484
2022-03-02 08:06:32 - train: epoch 0041, iter [04500, 05004], lr: 0.092260, loss: 2.9091
2022-03-02 08:07:06 - train: epoch 0041, iter [04600, 05004], lr: 0.092260, loss: 2.9540
2022-03-02 08:07:41 - train: epoch 0041, iter [04700, 05004], lr: 0.092260, loss: 2.7590
2022-03-02 08:08:14 - train: epoch 0041, iter [04800, 05004], lr: 0.092260, loss: 2.7327
2022-03-02 08:08:49 - train: epoch 0041, iter [04900, 05004], lr: 0.092260, loss: 2.8060
2022-03-02 08:09:21 - train: epoch 0041, iter [05000, 05004], lr: 0.092260, loss: 2.6365
2022-03-02 08:09:23 - train: epoch 041, train_loss: 2.7528
2022-03-02 08:10:37 - eval: epoch: 041, acc1: 53.948%, acc5: 79.420%, test_loss: 1.9513, per_image_load_time: 0.579ms, per_image_inference_time: 0.509ms
2022-03-02 08:10:38 - until epoch: 041, best_acc1: 53.948%
2022-03-02 08:10:38 - epoch 042 lr: 0.09182350690051133
2022-03-02 08:11:18 - train: epoch 0042, iter [00100, 05004], lr: 0.091824, loss: 2.5257
2022-03-02 08:11:52 - train: epoch 0042, iter [00200, 05004], lr: 0.091824, loss: 2.6536
2022-03-02 08:12:27 - train: epoch 0042, iter [00300, 05004], lr: 0.091824, loss: 3.0603
2022-03-02 08:13:01 - train: epoch 0042, iter [00400, 05004], lr: 0.091824, loss: 2.6651
2022-03-02 08:13:35 - train: epoch 0042, iter [00500, 05004], lr: 0.091824, loss: 2.7195
2022-03-02 08:14:10 - train: epoch 0042, iter [00600, 05004], lr: 0.091824, loss: 2.8682
2022-03-02 08:14:44 - train: epoch 0042, iter [00700, 05004], lr: 0.091824, loss: 2.8620
2022-03-02 08:15:19 - train: epoch 0042, iter [00800, 05004], lr: 0.091824, loss: 2.6654
2022-03-02 08:15:53 - train: epoch 0042, iter [00900, 05004], lr: 0.091824, loss: 3.2164
2022-03-02 08:16:27 - train: epoch 0042, iter [01000, 05004], lr: 0.091824, loss: 2.9372
2022-03-02 08:17:02 - train: epoch 0042, iter [01100, 05004], lr: 0.091824, loss: 2.6203
2022-03-02 08:17:35 - train: epoch 0042, iter [01200, 05004], lr: 0.091824, loss: 2.4114
2022-03-02 08:18:09 - train: epoch 0042, iter [01300, 05004], lr: 0.091824, loss: 2.9032
2022-03-02 08:18:45 - train: epoch 0042, iter [01400, 05004], lr: 0.091824, loss: 2.8041
2022-03-02 08:19:19 - train: epoch 0042, iter [01500, 05004], lr: 0.091824, loss: 2.7601
2022-03-02 08:19:53 - train: epoch 0042, iter [01600, 05004], lr: 0.091824, loss: 2.8353
2022-03-02 08:20:28 - train: epoch 0042, iter [01700, 05004], lr: 0.091824, loss: 2.8211
2022-03-02 08:21:02 - train: epoch 0042, iter [01800, 05004], lr: 0.091824, loss: 2.6157
2022-03-02 08:21:36 - train: epoch 0042, iter [01900, 05004], lr: 0.091824, loss: 2.9730
2022-03-02 08:22:10 - train: epoch 0042, iter [02000, 05004], lr: 0.091824, loss: 2.5561
2022-03-02 08:22:45 - train: epoch 0042, iter [02100, 05004], lr: 0.091824, loss: 2.5495
2022-03-02 08:23:20 - train: epoch 0042, iter [02200, 05004], lr: 0.091824, loss: 2.7653
2022-03-02 08:23:54 - train: epoch 0042, iter [02300, 05004], lr: 0.091824, loss: 2.8777
2022-03-02 08:24:29 - train: epoch 0042, iter [02400, 05004], lr: 0.091824, loss: 2.9686
2022-03-02 08:25:03 - train: epoch 0042, iter [02500, 05004], lr: 0.091824, loss: 2.6916
2022-03-02 08:25:38 - train: epoch 0042, iter [02600, 05004], lr: 0.091824, loss: 2.9691
2022-03-02 08:26:12 - train: epoch 0042, iter [02700, 05004], lr: 0.091824, loss: 2.6651
2022-03-02 08:26:47 - train: epoch 0042, iter [02800, 05004], lr: 0.091824, loss: 2.6609
2022-03-02 08:27:21 - train: epoch 0042, iter [02900, 05004], lr: 0.091824, loss: 2.7157
2022-03-02 08:27:55 - train: epoch 0042, iter [03000, 05004], lr: 0.091824, loss: 2.8661
2022-03-02 08:28:29 - train: epoch 0042, iter [03100, 05004], lr: 0.091824, loss: 2.8706
2022-03-02 08:29:03 - train: epoch 0042, iter [03200, 05004], lr: 0.091824, loss: 2.6361
2022-03-02 08:29:37 - train: epoch 0042, iter [03300, 05004], lr: 0.091824, loss: 2.8021
2022-03-02 08:30:12 - train: epoch 0042, iter [03400, 05004], lr: 0.091824, loss: 2.5237
2022-03-02 08:30:46 - train: epoch 0042, iter [03500, 05004], lr: 0.091824, loss: 2.5994
2022-03-02 08:31:20 - train: epoch 0042, iter [03600, 05004], lr: 0.091824, loss: 3.0629
2022-03-02 08:31:55 - train: epoch 0042, iter [03700, 05004], lr: 0.091824, loss: 2.8820
2022-03-02 08:32:29 - train: epoch 0042, iter [03800, 05004], lr: 0.091824, loss: 2.6588
2022-03-02 08:33:03 - train: epoch 0042, iter [03900, 05004], lr: 0.091824, loss: 2.8236
2022-03-02 08:33:38 - train: epoch 0042, iter [04000, 05004], lr: 0.091824, loss: 2.6635
2022-03-02 08:34:12 - train: epoch 0042, iter [04100, 05004], lr: 0.091824, loss: 2.7375
2022-03-02 08:34:47 - train: epoch 0042, iter [04200, 05004], lr: 0.091824, loss: 2.7098
2022-03-02 08:35:21 - train: epoch 0042, iter [04300, 05004], lr: 0.091824, loss: 2.6765
2022-03-02 08:35:55 - train: epoch 0042, iter [04400, 05004], lr: 0.091824, loss: 2.6402
2022-03-02 08:36:29 - train: epoch 0042, iter [04500, 05004], lr: 0.091824, loss: 2.6170
2022-03-02 08:37:04 - train: epoch 0042, iter [04600, 05004], lr: 0.091824, loss: 2.8501
2022-03-02 08:37:38 - train: epoch 0042, iter [04700, 05004], lr: 0.091824, loss: 3.0262
2022-03-02 08:38:12 - train: epoch 0042, iter [04800, 05004], lr: 0.091824, loss: 2.7051
2022-03-02 08:38:47 - train: epoch 0042, iter [04900, 05004], lr: 0.091824, loss: 2.5693
2022-03-02 08:39:20 - train: epoch 0042, iter [05000, 05004], lr: 0.091824, loss: 2.6328
2022-03-02 08:39:21 - train: epoch 042, train_loss: 2.7447
2022-03-02 08:40:36 - eval: epoch: 042, acc1: 52.080%, acc5: 77.838%, test_loss: 2.0555, per_image_load_time: 0.509ms, per_image_inference_time: 0.515ms
2022-03-02 08:40:37 - until epoch: 042, best_acc1: 53.948%
2022-03-02 08:40:37 - epoch 043 lr: 0.09137665423022251
2022-03-02 08:41:16 - train: epoch 0043, iter [00100, 05004], lr: 0.091377, loss: 2.6634
2022-03-02 08:41:50 - train: epoch 0043, iter [00200, 05004], lr: 0.091377, loss: 2.9436
2022-03-02 08:42:25 - train: epoch 0043, iter [00300, 05004], lr: 0.091377, loss: 2.5037
2022-03-02 08:42:59 - train: epoch 0043, iter [00400, 05004], lr: 0.091377, loss: 2.6536
2022-03-02 08:43:33 - train: epoch 0043, iter [00500, 05004], lr: 0.091377, loss: 2.7227
2022-03-02 08:44:07 - train: epoch 0043, iter [00600, 05004], lr: 0.091377, loss: 2.6734
2022-03-02 08:44:42 - train: epoch 0043, iter [00700, 05004], lr: 0.091377, loss: 2.7409
2022-03-02 08:45:16 - train: epoch 0043, iter [00800, 05004], lr: 0.091377, loss: 2.5320
2022-03-02 08:45:49 - train: epoch 0043, iter [00900, 05004], lr: 0.091377, loss: 2.5661
2022-03-02 08:46:24 - train: epoch 0043, iter [01000, 05004], lr: 0.091377, loss: 2.7849
2022-03-02 08:46:58 - train: epoch 0043, iter [01100, 05004], lr: 0.091377, loss: 2.6412
2022-03-02 08:47:33 - train: epoch 0043, iter [01200, 05004], lr: 0.091377, loss: 2.6153
2022-03-02 08:48:07 - train: epoch 0043, iter [01300, 05004], lr: 0.091377, loss: 2.5991
2022-03-02 08:48:41 - train: epoch 0043, iter [01400, 05004], lr: 0.091377, loss: 2.5756
2022-03-02 08:49:15 - train: epoch 0043, iter [01500, 05004], lr: 0.091377, loss: 2.9158
2022-03-02 08:49:49 - train: epoch 0043, iter [01600, 05004], lr: 0.091377, loss: 2.7526
2022-03-02 08:50:24 - train: epoch 0043, iter [01700, 05004], lr: 0.091377, loss: 2.7552
2022-03-02 08:50:59 - train: epoch 0043, iter [01800, 05004], lr: 0.091377, loss: 2.9951
2022-03-02 08:51:33 - train: epoch 0043, iter [01900, 05004], lr: 0.091377, loss: 2.7210
2022-03-02 08:52:08 - train: epoch 0043, iter [02000, 05004], lr: 0.091377, loss: 2.7923
2022-03-02 08:52:42 - train: epoch 0043, iter [02100, 05004], lr: 0.091377, loss: 2.8113
2022-03-02 08:53:16 - train: epoch 0043, iter [02200, 05004], lr: 0.091377, loss: 2.9219
2022-03-02 08:53:50 - train: epoch 0043, iter [02300, 05004], lr: 0.091377, loss: 2.7721
2022-03-02 08:54:25 - train: epoch 0043, iter [02400, 05004], lr: 0.091377, loss: 2.8697
2022-03-02 08:54:59 - train: epoch 0043, iter [02500, 05004], lr: 0.091377, loss: 2.9937
2022-03-02 08:55:34 - train: epoch 0043, iter [02600, 05004], lr: 0.091377, loss: 2.6058
2022-03-02 08:56:08 - train: epoch 0043, iter [02700, 05004], lr: 0.091377, loss: 2.6502
2022-03-02 08:56:42 - train: epoch 0043, iter [02800, 05004], lr: 0.091377, loss: 2.5007
2022-03-02 08:57:17 - train: epoch 0043, iter [02900, 05004], lr: 0.091377, loss: 2.8022
2022-03-02 08:57:51 - train: epoch 0043, iter [03000, 05004], lr: 0.091377, loss: 2.9929
2022-03-02 08:58:25 - train: epoch 0043, iter [03100, 05004], lr: 0.091377, loss: 2.7964
2022-03-02 08:58:59 - train: epoch 0043, iter [03200, 05004], lr: 0.091377, loss: 2.7510
2022-03-02 08:59:34 - train: epoch 0043, iter [03300, 05004], lr: 0.091377, loss: 2.7908
2022-03-02 09:00:08 - train: epoch 0043, iter [03400, 05004], lr: 0.091377, loss: 2.5716
2022-03-02 09:00:43 - train: epoch 0043, iter [03500, 05004], lr: 0.091377, loss: 2.6514
2022-03-02 09:01:17 - train: epoch 0043, iter [03600, 05004], lr: 0.091377, loss: 2.5673
2022-03-02 09:01:51 - train: epoch 0043, iter [03700, 05004], lr: 0.091377, loss: 2.3432
2022-03-02 09:02:25 - train: epoch 0043, iter [03800, 05004], lr: 0.091377, loss: 2.9613
2022-03-02 09:02:59 - train: epoch 0043, iter [03900, 05004], lr: 0.091377, loss: 2.8406
2022-03-02 09:03:33 - train: epoch 0043, iter [04000, 05004], lr: 0.091377, loss: 2.7082
2022-03-02 09:04:07 - train: epoch 0043, iter [04100, 05004], lr: 0.091377, loss: 3.0084
2022-03-02 09:04:42 - train: epoch 0043, iter [04200, 05004], lr: 0.091377, loss: 2.8976
2022-03-02 09:05:16 - train: epoch 0043, iter [04300, 05004], lr: 0.091377, loss: 2.7234
2022-03-02 09:05:51 - train: epoch 0043, iter [04400, 05004], lr: 0.091377, loss: 2.7641
2022-03-02 09:06:25 - train: epoch 0043, iter [04500, 05004], lr: 0.091377, loss: 2.5698
2022-03-02 09:06:59 - train: epoch 0043, iter [04600, 05004], lr: 0.091377, loss: 2.7752
2022-03-02 09:07:33 - train: epoch 0043, iter [04700, 05004], lr: 0.091377, loss: 2.6437
2022-03-02 09:08:08 - train: epoch 0043, iter [04800, 05004], lr: 0.091377, loss: 2.5974
2022-03-02 09:08:42 - train: epoch 0043, iter [04900, 05004], lr: 0.091377, loss: 2.5660
2022-03-02 09:09:15 - train: epoch 0043, iter [05000, 05004], lr: 0.091377, loss: 2.9184
2022-03-02 09:09:16 - train: epoch 043, train_loss: 2.7446
2022-03-02 09:10:31 - eval: epoch: 043, acc1: 52.224%, acc5: 77.918%, test_loss: 2.0405, per_image_load_time: 0.950ms, per_image_inference_time: 0.523ms
2022-03-02 09:10:31 - until epoch: 043, best_acc1: 53.948%
2022-03-02 09:10:31 - epoch 044 lr: 0.09091906224695935
2022-03-02 09:11:12 - train: epoch 0044, iter [00100, 05004], lr: 0.090919, loss: 2.6462
2022-03-02 09:11:46 - train: epoch 0044, iter [00200, 05004], lr: 0.090919, loss: 3.0457
2022-03-02 09:12:20 - train: epoch 0044, iter [00300, 05004], lr: 0.090919, loss: 2.7842
2022-03-02 09:12:55 - train: epoch 0044, iter [00400, 05004], lr: 0.090919, loss: 2.5258
2022-03-02 09:13:28 - train: epoch 0044, iter [00500, 05004], lr: 0.090919, loss: 2.9003
2022-03-02 09:14:02 - train: epoch 0044, iter [00600, 05004], lr: 0.090919, loss: 2.6217
2022-03-02 09:14:36 - train: epoch 0044, iter [00700, 05004], lr: 0.090919, loss: 2.5357
2022-03-02 09:15:11 - train: epoch 0044, iter [00800, 05004], lr: 0.090919, loss: 2.8690
2022-03-02 09:15:44 - train: epoch 0044, iter [00900, 05004], lr: 0.090919, loss: 2.5910
2022-03-02 09:16:19 - train: epoch 0044, iter [01000, 05004], lr: 0.090919, loss: 2.6226
2022-03-02 09:16:53 - train: epoch 0044, iter [01100, 05004], lr: 0.090919, loss: 2.5393
2022-03-02 09:17:28 - train: epoch 0044, iter [01200, 05004], lr: 0.090919, loss: 2.6516
2022-03-02 09:18:01 - train: epoch 0044, iter [01300, 05004], lr: 0.090919, loss: 2.9435
2022-03-02 09:18:37 - train: epoch 0044, iter [01400, 05004], lr: 0.090919, loss: 2.5442
2022-03-02 09:19:10 - train: epoch 0044, iter [01500, 05004], lr: 0.090919, loss: 2.8300
2022-03-02 09:19:45 - train: epoch 0044, iter [01600, 05004], lr: 0.090919, loss: 2.8214
2022-03-02 09:20:19 - train: epoch 0044, iter [01700, 05004], lr: 0.090919, loss: 2.7544
2022-03-02 09:20:53 - train: epoch 0044, iter [01800, 05004], lr: 0.090919, loss: 2.4171
2022-03-02 09:21:27 - train: epoch 0044, iter [01900, 05004], lr: 0.090919, loss: 2.7383
2022-03-02 09:22:02 - train: epoch 0044, iter [02000, 05004], lr: 0.090919, loss: 2.6461
2022-03-02 09:22:36 - train: epoch 0044, iter [02100, 05004], lr: 0.090919, loss: 2.8894
2022-03-02 09:23:10 - train: epoch 0044, iter [02200, 05004], lr: 0.090919, loss: 2.5709
2022-03-02 09:23:45 - train: epoch 0044, iter [02300, 05004], lr: 0.090919, loss: 2.8447
2022-03-02 09:24:19 - train: epoch 0044, iter [02400, 05004], lr: 0.090919, loss: 2.8978
2022-03-02 09:24:53 - train: epoch 0044, iter [02500, 05004], lr: 0.090919, loss: 2.6251
2022-03-02 09:25:27 - train: epoch 0044, iter [02600, 05004], lr: 0.090919, loss: 2.6542
2022-03-02 09:26:01 - train: epoch 0044, iter [02700, 05004], lr: 0.090919, loss: 2.8130
2022-03-02 09:26:35 - train: epoch 0044, iter [02800, 05004], lr: 0.090919, loss: 2.5024
2022-03-02 09:27:09 - train: epoch 0044, iter [02900, 05004], lr: 0.090919, loss: 2.6000
2022-03-02 09:27:44 - train: epoch 0044, iter [03000, 05004], lr: 0.090919, loss: 2.5778
2022-03-02 09:28:17 - train: epoch 0044, iter [03100, 05004], lr: 0.090919, loss: 2.7103
2022-03-02 09:28:52 - train: epoch 0044, iter [03200, 05004], lr: 0.090919, loss: 2.5876
2022-03-02 09:29:25 - train: epoch 0044, iter [03300, 05004], lr: 0.090919, loss: 2.5999
2022-03-02 09:30:00 - train: epoch 0044, iter [03400, 05004], lr: 0.090919, loss: 2.8242
2022-03-02 09:30:35 - train: epoch 0044, iter [03500, 05004], lr: 0.090919, loss: 2.6398
2022-03-02 09:31:09 - train: epoch 0044, iter [03600, 05004], lr: 0.090919, loss: 2.7702
2022-03-02 09:31:42 - train: epoch 0044, iter [03700, 05004], lr: 0.090919, loss: 2.7458
2022-03-02 09:32:17 - train: epoch 0044, iter [03800, 05004], lr: 0.090919, loss: 2.3956
2022-03-02 09:32:51 - train: epoch 0044, iter [03900, 05004], lr: 0.090919, loss: 2.6918
2022-03-02 09:33:26 - train: epoch 0044, iter [04000, 05004], lr: 0.090919, loss: 2.7169
2022-03-02 09:34:00 - train: epoch 0044, iter [04100, 05004], lr: 0.090919, loss: 2.5524
2022-03-02 09:34:34 - train: epoch 0044, iter [04200, 05004], lr: 0.090919, loss: 2.9452
2022-03-02 09:35:08 - train: epoch 0044, iter [04300, 05004], lr: 0.090919, loss: 2.5916
2022-03-02 09:35:43 - train: epoch 0044, iter [04400, 05004], lr: 0.090919, loss: 2.8455
2022-03-02 09:36:17 - train: epoch 0044, iter [04500, 05004], lr: 0.090919, loss: 3.0733
2022-03-02 09:36:51 - train: epoch 0044, iter [04600, 05004], lr: 0.090919, loss: 2.7098
2022-03-02 09:37:26 - train: epoch 0044, iter [04700, 05004], lr: 0.090919, loss: 2.7439
2022-03-02 09:38:00 - train: epoch 0044, iter [04800, 05004], lr: 0.090919, loss: 2.6253
2022-03-02 09:38:35 - train: epoch 0044, iter [04900, 05004], lr: 0.090919, loss: 2.6145
2022-03-02 09:39:07 - train: epoch 0044, iter [05000, 05004], lr: 0.090919, loss: 2.6523
2022-03-02 09:39:08 - train: epoch 044, train_loss: 2.7402
2022-03-02 09:40:23 - eval: epoch: 044, acc1: 52.170%, acc5: 78.008%, test_loss: 2.0558, per_image_load_time: 0.528ms, per_image_inference_time: 0.537ms
2022-03-02 09:40:23 - until epoch: 044, best_acc1: 53.948%
2022-03-02 09:40:23 - epoch 045 lr: 0.09045084971874738
2022-03-02 09:41:04 - train: epoch 0045, iter [00100, 05004], lr: 0.090451, loss: 2.8133
2022-03-02 09:41:37 - train: epoch 0045, iter [00200, 05004], lr: 0.090451, loss: 2.5926
2022-03-02 09:42:12 - train: epoch 0045, iter [00300, 05004], lr: 0.090451, loss: 2.5324
2022-03-02 09:42:46 - train: epoch 0045, iter [00400, 05004], lr: 0.090451, loss: 2.8768
2022-03-02 09:43:20 - train: epoch 0045, iter [00500, 05004], lr: 0.090451, loss: 2.9235
2022-03-02 09:43:54 - train: epoch 0045, iter [00600, 05004], lr: 0.090451, loss: 2.7288
2022-03-02 09:44:29 - train: epoch 0045, iter [00700, 05004], lr: 0.090451, loss: 2.5679
2022-03-02 09:45:03 - train: epoch 0045, iter [00800, 05004], lr: 0.090451, loss: 2.7326
2022-03-02 09:45:37 - train: epoch 0045, iter [00900, 05004], lr: 0.090451, loss: 2.6121
2022-03-02 09:46:11 - train: epoch 0045, iter [01000, 05004], lr: 0.090451, loss: 2.6646
2022-03-02 09:46:46 - train: epoch 0045, iter [01100, 05004], lr: 0.090451, loss: 2.8695
2022-03-02 09:47:20 - train: epoch 0045, iter [01200, 05004], lr: 0.090451, loss: 2.7324
2022-03-02 09:47:54 - train: epoch 0045, iter [01300, 05004], lr: 0.090451, loss: 2.7935
2022-03-02 09:48:29 - train: epoch 0045, iter [01400, 05004], lr: 0.090451, loss: 2.7552
2022-03-02 09:49:03 - train: epoch 0045, iter [01500, 05004], lr: 0.090451, loss: 2.5468
2022-03-02 09:49:37 - train: epoch 0045, iter [01600, 05004], lr: 0.090451, loss: 2.5512
2022-03-02 09:50:11 - train: epoch 0045, iter [01700, 05004], lr: 0.090451, loss: 2.6014
2022-03-02 09:50:46 - train: epoch 0045, iter [01800, 05004], lr: 0.090451, loss: 2.8851
2022-03-02 09:51:20 - train: epoch 0045, iter [01900, 05004], lr: 0.090451, loss: 2.5726
2022-03-02 09:51:54 - train: epoch 0045, iter [02000, 05004], lr: 0.090451, loss: 2.7293
2022-03-02 09:52:29 - train: epoch 0045, iter [02100, 05004], lr: 0.090451, loss: 3.1067
2022-03-02 09:53:03 - train: epoch 0045, iter [02200, 05004], lr: 0.090451, loss: 2.6258
2022-03-02 09:53:38 - train: epoch 0045, iter [02300, 05004], lr: 0.090451, loss: 2.7782
2022-03-02 09:54:12 - train: epoch 0045, iter [02400, 05004], lr: 0.090451, loss: 2.8916
2022-03-02 09:54:46 - train: epoch 0045, iter [02500, 05004], lr: 0.090451, loss: 2.7391
2022-03-02 09:55:21 - train: epoch 0045, iter [02600, 05004], lr: 0.090451, loss: 2.7597
2022-03-02 09:55:56 - train: epoch 0045, iter [02700, 05004], lr: 0.090451, loss: 2.4723
2022-03-02 09:56:30 - train: epoch 0045, iter [02800, 05004], lr: 0.090451, loss: 2.7938
2022-03-02 09:57:04 - train: epoch 0045, iter [02900, 05004], lr: 0.090451, loss: 2.9820
2022-03-02 09:57:39 - train: epoch 0045, iter [03000, 05004], lr: 0.090451, loss: 2.7351
2022-03-02 09:58:14 - train: epoch 0045, iter [03100, 05004], lr: 0.090451, loss: 2.6680
2022-03-02 09:58:48 - train: epoch 0045, iter [03200, 05004], lr: 0.090451, loss: 3.0956
2022-03-02 09:59:23 - train: epoch 0045, iter [03300, 05004], lr: 0.090451, loss: 2.5737
2022-03-02 09:59:57 - train: epoch 0045, iter [03400, 05004], lr: 0.090451, loss: 2.7436
2022-03-02 10:00:32 - train: epoch 0045, iter [03500, 05004], lr: 0.090451, loss: 2.6532
2022-03-02 10:01:06 - train: epoch 0045, iter [03600, 05004], lr: 0.090451, loss: 2.6764
2022-03-02 10:01:40 - train: epoch 0045, iter [03700, 05004], lr: 0.090451, loss: 2.7393
2022-03-02 10:02:15 - train: epoch 0045, iter [03800, 05004], lr: 0.090451, loss: 2.6805
2022-03-02 10:02:49 - train: epoch 0045, iter [03900, 05004], lr: 0.090451, loss: 2.9204
2022-03-02 10:03:24 - train: epoch 0045, iter [04000, 05004], lr: 0.090451, loss: 2.6718
2022-03-02 10:03:58 - train: epoch 0045, iter [04100, 05004], lr: 0.090451, loss: 2.8439
2022-03-02 10:04:32 - train: epoch 0045, iter [04200, 05004], lr: 0.090451, loss: 3.0739
2022-03-02 10:05:06 - train: epoch 0045, iter [04300, 05004], lr: 0.090451, loss: 2.8634
2022-03-02 10:05:41 - train: epoch 0045, iter [04400, 05004], lr: 0.090451, loss: 2.8163
2022-03-02 10:06:16 - train: epoch 0045, iter [04500, 05004], lr: 0.090451, loss: 2.6249
2022-03-02 10:06:50 - train: epoch 0045, iter [04600, 05004], lr: 0.090451, loss: 2.7355
2022-03-02 10:07:25 - train: epoch 0045, iter [04700, 05004], lr: 0.090451, loss: 2.7865
2022-03-02 10:07:58 - train: epoch 0045, iter [04800, 05004], lr: 0.090451, loss: 2.6780
2022-03-02 10:08:33 - train: epoch 0045, iter [04900, 05004], lr: 0.090451, loss: 3.0927
2022-03-02 10:09:07 - train: epoch 0045, iter [05000, 05004], lr: 0.090451, loss: 2.7777
2022-03-02 10:09:08 - train: epoch 045, train_loss: 2.7349
2022-03-02 10:10:23 - eval: epoch: 045, acc1: 51.730%, acc5: 77.676%, test_loss: 2.0683, per_image_load_time: 0.600ms, per_image_inference_time: 0.504ms
2022-03-02 10:10:24 - until epoch: 045, best_acc1: 53.948%
2022-03-02 10:10:24 - epoch 046 lr: 0.08997213817017508
2022-03-02 10:11:03 - train: epoch 0046, iter [00100, 05004], lr: 0.089972, loss: 2.7115
2022-03-02 10:11:37 - train: epoch 0046, iter [00200, 05004], lr: 0.089972, loss: 2.4184
2022-03-02 10:12:11 - train: epoch 0046, iter [00300, 05004], lr: 0.089972, loss: 2.7294
2022-03-02 10:12:46 - train: epoch 0046, iter [00400, 05004], lr: 0.089972, loss: 2.7315
2022-03-02 10:13:19 - train: epoch 0046, iter [00500, 05004], lr: 0.089972, loss: 2.5908
2022-03-02 10:13:54 - train: epoch 0046, iter [00600, 05004], lr: 0.089972, loss: 2.6282
2022-03-02 10:14:28 - train: epoch 0046, iter [00700, 05004], lr: 0.089972, loss: 2.4718
2022-03-02 10:15:02 - train: epoch 0046, iter [00800, 05004], lr: 0.089972, loss: 2.8998
2022-03-02 10:15:36 - train: epoch 0046, iter [00900, 05004], lr: 0.089972, loss: 2.6253
2022-03-02 10:16:10 - train: epoch 0046, iter [01000, 05004], lr: 0.089972, loss: 2.6156
2022-03-02 10:16:45 - train: epoch 0046, iter [01100, 05004], lr: 0.089972, loss: 2.4936
2022-03-02 10:17:18 - train: epoch 0046, iter [01200, 05004], lr: 0.089972, loss: 2.6603
2022-03-02 10:17:53 - train: epoch 0046, iter [01300, 05004], lr: 0.089972, loss: 2.7670
2022-03-02 10:18:27 - train: epoch 0046, iter [01400, 05004], lr: 0.089972, loss: 2.6894
2022-03-02 10:19:01 - train: epoch 0046, iter [01500, 05004], lr: 0.089972, loss: 2.6513
2022-03-02 10:19:35 - train: epoch 0046, iter [01600, 05004], lr: 0.089972, loss: 2.8110
2022-03-02 10:20:10 - train: epoch 0046, iter [01700, 05004], lr: 0.089972, loss: 2.6654
2022-03-02 10:20:44 - train: epoch 0046, iter [01800, 05004], lr: 0.089972, loss: 2.6254
2022-03-02 10:21:18 - train: epoch 0046, iter [01900, 05004], lr: 0.089972, loss: 2.7047
2022-03-02 10:21:52 - train: epoch 0046, iter [02000, 05004], lr: 0.089972, loss: 2.7595
2022-03-02 10:22:26 - train: epoch 0046, iter [02100, 05004], lr: 0.089972, loss: 2.8304
2022-03-02 10:23:01 - train: epoch 0046, iter [02200, 05004], lr: 0.089972, loss: 2.6932
2022-03-02 10:23:35 - train: epoch 0046, iter [02300, 05004], lr: 0.089972, loss: 2.6771
2022-03-02 10:24:10 - train: epoch 0046, iter [02400, 05004], lr: 0.089972, loss: 2.5982
2022-03-02 10:24:44 - train: epoch 0046, iter [02500, 05004], lr: 0.089972, loss: 2.6491
2022-03-02 10:25:18 - train: epoch 0046, iter [02600, 05004], lr: 0.089972, loss: 2.9364
2022-03-02 10:25:53 - train: epoch 0046, iter [02700, 05004], lr: 0.089972, loss: 2.6387
2022-03-02 10:26:27 - train: epoch 0046, iter [02800, 05004], lr: 0.089972, loss: 2.4676
2022-03-02 10:27:02 - train: epoch 0046, iter [02900, 05004], lr: 0.089972, loss: 2.6585
2022-03-02 10:27:36 - train: epoch 0046, iter [03000, 05004], lr: 0.089972, loss: 2.5059
2022-03-02 10:28:10 - train: epoch 0046, iter [03100, 05004], lr: 0.089972, loss: 2.5548
2022-03-02 10:28:44 - train: epoch 0046, iter [03200, 05004], lr: 0.089972, loss: 2.8084
2022-03-02 10:29:19 - train: epoch 0046, iter [03300, 05004], lr: 0.089972, loss: 2.6587
2022-03-02 10:29:53 - train: epoch 0046, iter [03400, 05004], lr: 0.089972, loss: 2.6422
2022-03-02 10:30:28 - train: epoch 0046, iter [03500, 05004], lr: 0.089972, loss: 2.9209
2022-03-02 10:31:01 - train: epoch 0046, iter [03600, 05004], lr: 0.089972, loss: 2.6820
2022-03-02 10:31:35 - train: epoch 0046, iter [03700, 05004], lr: 0.089972, loss: 2.6742
2022-03-02 10:32:10 - train: epoch 0046, iter [03800, 05004], lr: 0.089972, loss: 2.6869
2022-03-02 10:32:45 - train: epoch 0046, iter [03900, 05004], lr: 0.089972, loss: 2.6038
2022-03-02 10:33:19 - train: epoch 0046, iter [04000, 05004], lr: 0.089972, loss: 2.6036
2022-03-02 10:33:53 - train: epoch 0046, iter [04100, 05004], lr: 0.089972, loss: 2.6917
2022-03-02 10:34:27 - train: epoch 0046, iter [04200, 05004], lr: 0.089972, loss: 2.4832
2022-03-02 10:35:02 - train: epoch 0046, iter [04300, 05004], lr: 0.089972, loss: 2.9351
2022-03-02 10:35:36 - train: epoch 0046, iter [04400, 05004], lr: 0.089972, loss: 2.8628
2022-03-02 10:36:10 - train: epoch 0046, iter [04500, 05004], lr: 0.089972, loss: 2.4936
2022-03-02 10:36:45 - train: epoch 0046, iter [04600, 05004], lr: 0.089972, loss: 2.8566
2022-03-02 10:37:19 - train: epoch 0046, iter [04700, 05004], lr: 0.089972, loss: 2.4735
2022-03-02 10:37:54 - train: epoch 0046, iter [04800, 05004], lr: 0.089972, loss: 2.7373
2022-03-02 10:38:27 - train: epoch 0046, iter [04900, 05004], lr: 0.089972, loss: 2.7782
2022-03-02 10:39:01 - train: epoch 0046, iter [05000, 05004], lr: 0.089972, loss: 2.7741
2022-03-02 10:39:02 - train: epoch 046, train_loss: 2.7289
2022-03-02 10:40:17 - eval: epoch: 046, acc1: 49.618%, acc5: 75.232%, test_loss: 2.2201, per_image_load_time: 0.727ms, per_image_inference_time: 0.510ms
2022-03-02 10:40:17 - until epoch: 046, best_acc1: 53.948%
2022-03-02 10:40:17 - epoch 047 lr: 0.08948305185085226
2022-03-02 10:40:57 - train: epoch 0047, iter [00100, 05004], lr: 0.089483, loss: 2.6784
2022-03-02 10:41:31 - train: epoch 0047, iter [00200, 05004], lr: 0.089483, loss: 2.7477
2022-03-02 10:42:06 - train: epoch 0047, iter [00300, 05004], lr: 0.089483, loss: 2.4576
2022-03-02 10:42:40 - train: epoch 0047, iter [00400, 05004], lr: 0.089483, loss: 2.4962
2022-03-02 10:43:15 - train: epoch 0047, iter [00500, 05004], lr: 0.089483, loss: 2.5951
2022-03-02 10:43:48 - train: epoch 0047, iter [00600, 05004], lr: 0.089483, loss: 2.7772
2022-03-02 10:44:23 - train: epoch 0047, iter [00700, 05004], lr: 0.089483, loss: 2.7656
2022-03-02 10:44:57 - train: epoch 0047, iter [00800, 05004], lr: 0.089483, loss: 2.7022
2022-03-02 10:45:31 - train: epoch 0047, iter [00900, 05004], lr: 0.089483, loss: 3.0271
2022-03-02 10:46:06 - train: epoch 0047, iter [01000, 05004], lr: 0.089483, loss: 2.8265
2022-03-02 10:46:40 - train: epoch 0047, iter [01100, 05004], lr: 0.089483, loss: 2.8353
2022-03-02 10:47:14 - train: epoch 0047, iter [01200, 05004], lr: 0.089483, loss: 2.4993
2022-03-02 10:47:48 - train: epoch 0047, iter [01300, 05004], lr: 0.089483, loss: 2.8034
2022-03-02 10:48:23 - train: epoch 0047, iter [01400, 05004], lr: 0.089483, loss: 2.6966
2022-03-02 10:48:58 - train: epoch 0047, iter [01500, 05004], lr: 0.089483, loss: 2.7505
2022-03-02 10:49:31 - train: epoch 0047, iter [01600, 05004], lr: 0.089483, loss: 2.5893
2022-03-02 10:50:05 - train: epoch 0047, iter [01700, 05004], lr: 0.089483, loss: 2.6996
2022-03-02 10:50:40 - train: epoch 0047, iter [01800, 05004], lr: 0.089483, loss: 2.5124
2022-03-02 10:51:14 - train: epoch 0047, iter [01900, 05004], lr: 0.089483, loss: 2.6774
2022-03-02 10:51:48 - train: epoch 0047, iter [02000, 05004], lr: 0.089483, loss: 2.6663
2022-03-02 10:52:22 - train: epoch 0047, iter [02100, 05004], lr: 0.089483, loss: 3.0264
2022-03-02 10:52:56 - train: epoch 0047, iter [02200, 05004], lr: 0.089483, loss: 2.8766
2022-03-02 10:53:30 - train: epoch 0047, iter [02300, 05004], lr: 0.089483, loss: 2.5789
2022-03-02 10:54:05 - train: epoch 0047, iter [02400, 05004], lr: 0.089483, loss: 2.7405
2022-03-02 10:54:39 - train: epoch 0047, iter [02500, 05004], lr: 0.089483, loss: 2.8057
2022-03-02 10:55:13 - train: epoch 0047, iter [02600, 05004], lr: 0.089483, loss: 2.9908
2022-03-02 10:55:48 - train: epoch 0047, iter [02700, 05004], lr: 0.089483, loss: 2.6152
2022-03-02 10:56:22 - train: epoch 0047, iter [02800, 05004], lr: 0.089483, loss: 2.8052
2022-03-02 10:56:56 - train: epoch 0047, iter [02900, 05004], lr: 0.089483, loss: 2.4799
2022-03-02 10:57:31 - train: epoch 0047, iter [03000, 05004], lr: 0.089483, loss: 2.5866
2022-03-02 10:58:05 - train: epoch 0047, iter [03100, 05004], lr: 0.089483, loss: 2.9084
2022-03-02 10:58:39 - train: epoch 0047, iter [03200, 05004], lr: 0.089483, loss: 2.9225
2022-03-02 10:59:13 - train: epoch 0047, iter [03300, 05004], lr: 0.089483, loss: 2.5836
2022-03-02 10:59:48 - train: epoch 0047, iter [03400, 05004], lr: 0.089483, loss: 2.5878
2022-03-02 11:00:22 - train: epoch 0047, iter [03500, 05004], lr: 0.089483, loss: 2.8430
2022-03-02 11:00:57 - train: epoch 0047, iter [03600, 05004], lr: 0.089483, loss: 2.7251
2022-03-02 11:01:31 - train: epoch 0047, iter [03700, 05004], lr: 0.089483, loss: 2.8217
2022-03-02 11:02:05 - train: epoch 0047, iter [03800, 05004], lr: 0.089483, loss: 2.8774
2022-03-02 11:02:40 - train: epoch 0047, iter [03900, 05004], lr: 0.089483, loss: 2.6545
2022-03-02 11:03:13 - train: epoch 0047, iter [04000, 05004], lr: 0.089483, loss: 2.9269
2022-03-02 11:03:48 - train: epoch 0047, iter [04100, 05004], lr: 0.089483, loss: 2.7533
2022-03-02 11:04:23 - train: epoch 0047, iter [04200, 05004], lr: 0.089483, loss: 2.5822
2022-03-02 11:04:58 - train: epoch 0047, iter [04300, 05004], lr: 0.089483, loss: 2.3858
2022-03-02 11:05:32 - train: epoch 0047, iter [04400, 05004], lr: 0.089483, loss: 2.6105
2022-03-02 11:06:07 - train: epoch 0047, iter [04500, 05004], lr: 0.089483, loss: 2.6219
2022-03-02 11:06:40 - train: epoch 0047, iter [04600, 05004], lr: 0.089483, loss: 2.7733
2022-03-02 11:07:14 - train: epoch 0047, iter [04700, 05004], lr: 0.089483, loss: 2.8135
2022-03-02 11:07:48 - train: epoch 0047, iter [04800, 05004], lr: 0.089483, loss: 2.3980
2022-03-02 11:08:23 - train: epoch 0047, iter [04900, 05004], lr: 0.089483, loss: 2.8318
2022-03-02 11:08:56 - train: epoch 0047, iter [05000, 05004], lr: 0.089483, loss: 2.5966
2022-03-02 11:08:57 - train: epoch 047, train_loss: 2.7266
2022-03-02 11:10:11 - eval: epoch: 047, acc1: 52.490%, acc5: 78.386%, test_loss: 2.0323, per_image_load_time: 0.646ms, per_image_inference_time: 0.486ms
2022-03-02 11:10:12 - until epoch: 047, best_acc1: 53.948%
2022-03-02 11:10:12 - epoch 048 lr: 0.08898371770316112
2022-03-02 11:10:51 - train: epoch 0048, iter [00100, 05004], lr: 0.088984, loss: 2.6808
2022-03-02 11:11:26 - train: epoch 0048, iter [00200, 05004], lr: 0.088984, loss: 2.8629
2022-03-02 11:12:00 - train: epoch 0048, iter [00300, 05004], lr: 0.088984, loss: 2.7859
2022-03-02 11:12:33 - train: epoch 0048, iter [00400, 05004], lr: 0.088984, loss: 2.5231
2022-03-02 11:13:07 - train: epoch 0048, iter [00500, 05004], lr: 0.088984, loss: 2.7837
2022-03-02 11:13:41 - train: epoch 0048, iter [00600, 05004], lr: 0.088984, loss: 2.6254
2022-03-02 11:14:15 - train: epoch 0048, iter [00700, 05004], lr: 0.088984, loss: 2.6496
2022-03-02 11:14:49 - train: epoch 0048, iter [00800, 05004], lr: 0.088984, loss: 2.9271
2022-03-02 11:15:22 - train: epoch 0048, iter [00900, 05004], lr: 0.088984, loss: 3.0670
2022-03-02 11:15:57 - train: epoch 0048, iter [01000, 05004], lr: 0.088984, loss: 2.8402
2022-03-02 11:16:31 - train: epoch 0048, iter [01100, 05004], lr: 0.088984, loss: 2.8967
2022-03-02 11:17:04 - train: epoch 0048, iter [01200, 05004], lr: 0.088984, loss: 2.7362
2022-03-02 11:17:38 - train: epoch 0048, iter [01300, 05004], lr: 0.088984, loss: 2.5091
2022-03-02 11:18:11 - train: epoch 0048, iter [01400, 05004], lr: 0.088984, loss: 2.7199
2022-03-02 11:18:45 - train: epoch 0048, iter [01500, 05004], lr: 0.088984, loss: 2.7515
2022-03-02 11:19:19 - train: epoch 0048, iter [01600, 05004], lr: 0.088984, loss: 2.7454
2022-03-02 11:19:53 - train: epoch 0048, iter [01700, 05004], lr: 0.088984, loss: 2.5279
2022-03-02 11:20:27 - train: epoch 0048, iter [01800, 05004], lr: 0.088984, loss: 2.4763
2022-03-02 11:21:00 - train: epoch 0048, iter [01900, 05004], lr: 0.088984, loss: 2.6633
2022-03-02 11:21:33 - train: epoch 0048, iter [02000, 05004], lr: 0.088984, loss: 2.8243
2022-03-02 11:22:07 - train: epoch 0048, iter [02100, 05004], lr: 0.088984, loss: 2.7982
2022-03-02 11:22:41 - train: epoch 0048, iter [02200, 05004], lr: 0.088984, loss: 2.8697
2022-03-02 11:23:14 - train: epoch 0048, iter [02300, 05004], lr: 0.088984, loss: 2.7662
2022-03-02 11:23:48 - train: epoch 0048, iter [02400, 05004], lr: 0.088984, loss: 3.0320
2022-03-02 11:24:22 - train: epoch 0048, iter [02500, 05004], lr: 0.088984, loss: 2.6934
2022-03-02 11:24:55 - train: epoch 0048, iter [02600, 05004], lr: 0.088984, loss: 2.7376
2022-03-02 11:25:29 - train: epoch 0048, iter [02700, 05004], lr: 0.088984, loss: 3.0234
2022-03-02 11:26:03 - train: epoch 0048, iter [02800, 05004], lr: 0.088984, loss: 2.9196
2022-03-02 11:26:36 - train: epoch 0048, iter [02900, 05004], lr: 0.088984, loss: 2.9324
2022-03-02 11:27:10 - train: epoch 0048, iter [03000, 05004], lr: 0.088984, loss: 2.6600
2022-03-02 11:27:44 - train: epoch 0048, iter [03100, 05004], lr: 0.088984, loss: 2.7633
2022-03-02 11:28:18 - train: epoch 0048, iter [03200, 05004], lr: 0.088984, loss: 2.7210
2022-03-02 11:28:51 - train: epoch 0048, iter [03300, 05004], lr: 0.088984, loss: 2.6729
2022-03-02 11:29:25 - train: epoch 0048, iter [03400, 05004], lr: 0.088984, loss: 2.7189
2022-03-02 11:29:59 - train: epoch 0048, iter [03500, 05004], lr: 0.088984, loss: 2.9715
2022-03-02 11:30:33 - train: epoch 0048, iter [03600, 05004], lr: 0.088984, loss: 2.9650
2022-03-02 11:31:07 - train: epoch 0048, iter [03700, 05004], lr: 0.088984, loss: 2.6914
2022-03-02 11:31:41 - train: epoch 0048, iter [03800, 05004], lr: 0.088984, loss: 2.7351
2022-03-02 11:32:15 - train: epoch 0048, iter [03900, 05004], lr: 0.088984, loss: 2.8198
2022-03-02 11:32:50 - train: epoch 0048, iter [04000, 05004], lr: 0.088984, loss: 2.6264
2022-03-02 11:33:24 - train: epoch 0048, iter [04100, 05004], lr: 0.088984, loss: 2.8173
2022-03-02 11:33:58 - train: epoch 0048, iter [04200, 05004], lr: 0.088984, loss: 2.6943
2022-03-02 11:34:32 - train: epoch 0048, iter [04300, 05004], lr: 0.088984, loss: 2.4640
2022-03-02 11:35:06 - train: epoch 0048, iter [04400, 05004], lr: 0.088984, loss: 2.6835
2022-03-02 11:35:40 - train: epoch 0048, iter [04500, 05004], lr: 0.088984, loss: 2.8314
2022-03-02 11:36:14 - train: epoch 0048, iter [04600, 05004], lr: 0.088984, loss: 2.6346
2022-03-02 11:36:48 - train: epoch 0048, iter [04700, 05004], lr: 0.088984, loss: 2.7256
2022-03-02 11:37:22 - train: epoch 0048, iter [04800, 05004], lr: 0.088984, loss: 2.7314
2022-03-02 11:37:56 - train: epoch 0048, iter [04900, 05004], lr: 0.088984, loss: 2.9574
2022-03-02 11:38:28 - train: epoch 0048, iter [05000, 05004], lr: 0.088984, loss: 2.5591
2022-03-02 11:38:30 - train: epoch 048, train_loss: 2.7193
2022-03-02 11:39:44 - eval: epoch: 048, acc1: 52.564%, acc5: 78.046%, test_loss: 2.0252, per_image_load_time: 0.759ms, per_image_inference_time: 0.490ms
2022-03-02 11:39:45 - until epoch: 048, best_acc1: 53.948%
2022-03-02 11:39:45 - epoch 049 lr: 0.0884742653293083
2022-03-02 11:40:25 - train: epoch 0049, iter [00100, 05004], lr: 0.088474, loss: 2.7588
2022-03-02 11:40:59 - train: epoch 0049, iter [00200, 05004], lr: 0.088474, loss: 2.9643
2022-03-02 11:41:32 - train: epoch 0049, iter [00300, 05004], lr: 0.088474, loss: 2.8935
2022-03-02 11:42:07 - train: epoch 0049, iter [00400, 05004], lr: 0.088474, loss: 2.6229
2022-03-02 11:42:40 - train: epoch 0049, iter [00500, 05004], lr: 0.088474, loss: 2.6617
2022-03-02 11:43:14 - train: epoch 0049, iter [00600, 05004], lr: 0.088474, loss: 2.6081
2022-03-02 11:43:48 - train: epoch 0049, iter [00700, 05004], lr: 0.088474, loss: 2.8236
2022-03-02 11:44:22 - train: epoch 0049, iter [00800, 05004], lr: 0.088474, loss: 3.0535
2022-03-02 11:44:56 - train: epoch 0049, iter [00900, 05004], lr: 0.088474, loss: 2.5152
2022-03-02 11:45:30 - train: epoch 0049, iter [01000, 05004], lr: 0.088474, loss: 2.5782
2022-03-02 11:46:04 - train: epoch 0049, iter [01100, 05004], lr: 0.088474, loss: 2.9318
2022-03-02 11:46:38 - train: epoch 0049, iter [01200, 05004], lr: 0.088474, loss: 2.7961
2022-03-02 11:47:12 - train: epoch 0049, iter [01300, 05004], lr: 0.088474, loss: 2.9711
2022-03-02 11:47:45 - train: epoch 0049, iter [01400, 05004], lr: 0.088474, loss: 2.7773
2022-03-02 11:48:20 - train: epoch 0049, iter [01500, 05004], lr: 0.088474, loss: 2.7168
2022-03-02 11:48:54 - train: epoch 0049, iter [01600, 05004], lr: 0.088474, loss: 2.7570
2022-03-02 11:49:28 - train: epoch 0049, iter [01700, 05004], lr: 0.088474, loss: 2.6599
2022-03-02 11:50:03 - train: epoch 0049, iter [01800, 05004], lr: 0.088474, loss: 2.5982
2022-03-02 11:50:37 - train: epoch 0049, iter [01900, 05004], lr: 0.088474, loss: 2.6917
2022-03-02 11:51:11 - train: epoch 0049, iter [02000, 05004], lr: 0.088474, loss: 2.6089
2022-03-02 11:51:45 - train: epoch 0049, iter [02100, 05004], lr: 0.088474, loss: 2.6705
2022-03-02 11:52:19 - train: epoch 0049, iter [02200, 05004], lr: 0.088474, loss: 2.7751
2022-03-02 11:52:53 - train: epoch 0049, iter [02300, 05004], lr: 0.088474, loss: 2.5914
2022-03-02 11:53:27 - train: epoch 0049, iter [02400, 05004], lr: 0.088474, loss: 2.8227
2022-03-02 11:54:02 - train: epoch 0049, iter [02500, 05004], lr: 0.088474, loss: 2.8130
2022-03-02 11:54:36 - train: epoch 0049, iter [02600, 05004], lr: 0.088474, loss: 2.8968
2022-03-02 11:55:11 - train: epoch 0049, iter [02700, 05004], lr: 0.088474, loss: 2.5392
2022-03-02 11:55:45 - train: epoch 0049, iter [02800, 05004], lr: 0.088474, loss: 2.9078
2022-03-02 11:56:19 - train: epoch 0049, iter [02900, 05004], lr: 0.088474, loss: 2.6299
2022-03-02 11:56:53 - train: epoch 0049, iter [03000, 05004], lr: 0.088474, loss: 2.7412
2022-03-02 11:57:27 - train: epoch 0049, iter [03100, 05004], lr: 0.088474, loss: 2.8299
2022-03-02 11:58:01 - train: epoch 0049, iter [03200, 05004], lr: 0.088474, loss: 2.9050
2022-03-02 11:58:36 - train: epoch 0049, iter [03300, 05004], lr: 0.088474, loss: 2.5120
2022-03-02 11:59:09 - train: epoch 0049, iter [03400, 05004], lr: 0.088474, loss: 2.7130
2022-03-02 11:59:43 - train: epoch 0049, iter [03500, 05004], lr: 0.088474, loss: 2.8011
2022-03-02 12:00:18 - train: epoch 0049, iter [03600, 05004], lr: 0.088474, loss: 2.9810
2022-03-02 12:00:52 - train: epoch 0049, iter [03700, 05004], lr: 0.088474, loss: 2.8151
2022-03-02 12:01:26 - train: epoch 0049, iter [03800, 05004], lr: 0.088474, loss: 2.7817
2022-03-02 12:02:00 - train: epoch 0049, iter [03900, 05004], lr: 0.088474, loss: 3.0352
2022-03-02 12:02:34 - train: epoch 0049, iter [04000, 05004], lr: 0.088474, loss: 2.6289
2022-03-02 12:03:08 - train: epoch 0049, iter [04100, 05004], lr: 0.088474, loss: 2.4555
2022-03-02 12:03:42 - train: epoch 0049, iter [04200, 05004], lr: 0.088474, loss: 2.7464
2022-03-02 12:04:17 - train: epoch 0049, iter [04300, 05004], lr: 0.088474, loss: 2.8694
2022-03-02 12:04:51 - train: epoch 0049, iter [04400, 05004], lr: 0.088474, loss: 2.8929
2022-03-02 12:05:24 - train: epoch 0049, iter [04500, 05004], lr: 0.088474, loss: 2.7412
2022-03-02 12:05:59 - train: epoch 0049, iter [04600, 05004], lr: 0.088474, loss: 2.7558
2022-03-02 12:06:33 - train: epoch 0049, iter [04700, 05004], lr: 0.088474, loss: 2.6956
2022-03-02 12:07:07 - train: epoch 0049, iter [04800, 05004], lr: 0.088474, loss: 2.8209
2022-03-02 12:07:41 - train: epoch 0049, iter [04900, 05004], lr: 0.088474, loss: 2.6257
2022-03-02 12:08:13 - train: epoch 0049, iter [05000, 05004], lr: 0.088474, loss: 2.6614
2022-03-02 12:08:14 - train: epoch 049, train_loss: 2.7149
2022-03-02 12:09:29 - eval: epoch: 049, acc1: 54.502%, acc5: 79.932%, test_loss: 1.9095, per_image_load_time: 0.544ms, per_image_inference_time: 0.486ms
2022-03-02 12:09:29 - until epoch: 049, best_acc1: 54.502%
2022-03-02 12:09:29 - epoch 050 lr: 0.08795482695768658
2022-03-02 12:10:09 - train: epoch 0050, iter [00100, 05004], lr: 0.087955, loss: 2.6603
2022-03-02 12:10:43 - train: epoch 0050, iter [00200, 05004], lr: 0.087955, loss: 2.8204
2022-03-02 12:11:17 - train: epoch 0050, iter [00300, 05004], lr: 0.087955, loss: 2.7080
2022-03-02 12:11:51 - train: epoch 0050, iter [00400, 05004], lr: 0.087955, loss: 2.6978
2022-03-02 12:12:24 - train: epoch 0050, iter [00500, 05004], lr: 0.087955, loss: 2.6649
2022-03-02 12:12:59 - train: epoch 0050, iter [00600, 05004], lr: 0.087955, loss: 2.6958
2022-03-02 12:13:32 - train: epoch 0050, iter [00700, 05004], lr: 0.087955, loss: 2.6410
2022-03-02 12:14:06 - train: epoch 0050, iter [00800, 05004], lr: 0.087955, loss: 2.5282
2022-03-02 12:14:41 - train: epoch 0050, iter [00900, 05004], lr: 0.087955, loss: 3.0538
2022-03-02 12:15:14 - train: epoch 0050, iter [01000, 05004], lr: 0.087955, loss: 2.8252
2022-03-02 12:15:48 - train: epoch 0050, iter [01100, 05004], lr: 0.087955, loss: 2.6616
2022-03-02 12:16:23 - train: epoch 0050, iter [01200, 05004], lr: 0.087955, loss: 2.8105
2022-03-02 12:16:57 - train: epoch 0050, iter [01300, 05004], lr: 0.087955, loss: 2.5887
2022-03-02 12:17:31 - train: epoch 0050, iter [01400, 05004], lr: 0.087955, loss: 2.8152
2022-03-02 12:18:05 - train: epoch 0050, iter [01500, 05004], lr: 0.087955, loss: 2.7430
2022-03-02 12:18:39 - train: epoch 0050, iter [01600, 05004], lr: 0.087955, loss: 2.6703
2022-03-02 12:19:13 - train: epoch 0050, iter [01700, 05004], lr: 0.087955, loss: 2.4439
2022-03-02 12:19:47 - train: epoch 0050, iter [01800, 05004], lr: 0.087955, loss: 2.8899
2022-03-02 12:20:21 - train: epoch 0050, iter [01900, 05004], lr: 0.087955, loss: 2.6395
2022-03-02 12:20:56 - train: epoch 0050, iter [02000, 05004], lr: 0.087955, loss: 2.5219
2022-03-02 12:21:29 - train: epoch 0050, iter [02100, 05004], lr: 0.087955, loss: 2.6936
2022-03-02 12:22:04 - train: epoch 0050, iter [02200, 05004], lr: 0.087955, loss: 2.5608
2022-03-02 12:22:38 - train: epoch 0050, iter [02300, 05004], lr: 0.087955, loss: 2.6135
2022-03-02 12:23:12 - train: epoch 0050, iter [02400, 05004], lr: 0.087955, loss: 2.5520
2022-03-02 12:23:46 - train: epoch 0050, iter [02500, 05004], lr: 0.087955, loss: 2.9273
2022-03-02 12:24:20 - train: epoch 0050, iter [02600, 05004], lr: 0.087955, loss: 2.3918
2022-03-02 12:24:54 - train: epoch 0050, iter [02700, 05004], lr: 0.087955, loss: 2.5615
2022-03-02 12:25:29 - train: epoch 0050, iter [02800, 05004], lr: 0.087955, loss: 2.7151
2022-03-02 12:26:03 - train: epoch 0050, iter [02900, 05004], lr: 0.087955, loss: 2.8554
2022-03-02 12:26:37 - train: epoch 0050, iter [03000, 05004], lr: 0.087955, loss: 2.8538
2022-03-02 12:27:11 - train: epoch 0050, iter [03100, 05004], lr: 0.087955, loss: 2.7868
2022-03-02 12:27:45 - train: epoch 0050, iter [03200, 05004], lr: 0.087955, loss: 2.6458
2022-03-02 12:28:20 - train: epoch 0050, iter [03300, 05004], lr: 0.087955, loss: 2.3911
2022-03-02 12:28:53 - train: epoch 0050, iter [03400, 05004], lr: 0.087955, loss: 2.6076
2022-03-02 12:29:28 - train: epoch 0050, iter [03500, 05004], lr: 0.087955, loss: 2.5613
2022-03-02 12:30:02 - train: epoch 0050, iter [03600, 05004], lr: 0.087955, loss: 2.8330
2022-03-02 12:30:36 - train: epoch 0050, iter [03700, 05004], lr: 0.087955, loss: 3.1343
2022-03-02 12:31:10 - train: epoch 0050, iter [03800, 05004], lr: 0.087955, loss: 2.6563
2022-03-02 12:31:44 - train: epoch 0050, iter [03900, 05004], lr: 0.087955, loss: 2.5647
2022-03-02 12:32:19 - train: epoch 0050, iter [04000, 05004], lr: 0.087955, loss: 2.8658
2022-03-02 12:32:54 - train: epoch 0050, iter [04100, 05004], lr: 0.087955, loss: 2.7690
2022-03-02 12:33:28 - train: epoch 0050, iter [04200, 05004], lr: 0.087955, loss: 2.7054
2022-03-02 12:34:03 - train: epoch 0050, iter [04300, 05004], lr: 0.087955, loss: 2.7738
2022-03-02 12:34:37 - train: epoch 0050, iter [04400, 05004], lr: 0.087955, loss: 2.7109
2022-03-02 12:35:12 - train: epoch 0050, iter [04500, 05004], lr: 0.087955, loss: 2.6071
2022-03-02 12:35:46 - train: epoch 0050, iter [04600, 05004], lr: 0.087955, loss: 2.8873
2022-03-02 12:36:21 - train: epoch 0050, iter [04700, 05004], lr: 0.087955, loss: 2.8561
2022-03-02 12:36:56 - train: epoch 0050, iter [04800, 05004], lr: 0.087955, loss: 2.6929
2022-03-02 12:37:30 - train: epoch 0050, iter [04900, 05004], lr: 0.087955, loss: 2.6000
2022-03-02 12:38:03 - train: epoch 0050, iter [05000, 05004], lr: 0.087955, loss: 2.6172
2022-03-02 12:38:04 - train: epoch 050, train_loss: 2.7127
2022-03-02 12:39:19 - eval: epoch: 050, acc1: 52.906%, acc5: 78.078%, test_loss: 2.0273, per_image_load_time: 0.909ms, per_image_inference_time: 0.524ms
2022-03-02 12:39:20 - until epoch: 050, best_acc1: 54.502%
2022-03-02 12:39:20 - epoch 051 lr: 0.08742553740855506
2022-03-02 12:39:59 - train: epoch 0051, iter [00100, 05004], lr: 0.087426, loss: 2.7585
2022-03-02 12:40:34 - train: epoch 0051, iter [00200, 05004], lr: 0.087426, loss: 2.8046
2022-03-02 12:41:07 - train: epoch 0051, iter [00300, 05004], lr: 0.087426, loss: 2.7017
2022-03-02 12:41:41 - train: epoch 0051, iter [00400, 05004], lr: 0.087426, loss: 2.3264
2022-03-02 12:42:16 - train: epoch 0051, iter [00500, 05004], lr: 0.087426, loss: 2.7842
2022-03-02 12:42:50 - train: epoch 0051, iter [00600, 05004], lr: 0.087426, loss: 2.5619
2022-03-02 12:43:24 - train: epoch 0051, iter [00700, 05004], lr: 0.087426, loss: 2.7146
2022-03-02 12:43:58 - train: epoch 0051, iter [00800, 05004], lr: 0.087426, loss: 3.0306
2022-03-02 12:44:32 - train: epoch 0051, iter [00900, 05004], lr: 0.087426, loss: 2.3243
2022-03-02 12:45:06 - train: epoch 0051, iter [01000, 05004], lr: 0.087426, loss: 2.9337
2022-03-02 12:45:40 - train: epoch 0051, iter [01100, 05004], lr: 0.087426, loss: 2.7928
2022-03-02 12:46:15 - train: epoch 0051, iter [01200, 05004], lr: 0.087426, loss: 2.6782
2022-03-02 12:46:49 - train: epoch 0051, iter [01300, 05004], lr: 0.087426, loss: 2.6450
2022-03-02 12:47:24 - train: epoch 0051, iter [01400, 05004], lr: 0.087426, loss: 2.4026
2022-03-02 12:47:58 - train: epoch 0051, iter [01500, 05004], lr: 0.087426, loss: 2.6853
2022-03-02 12:48:33 - train: epoch 0051, iter [01600, 05004], lr: 0.087426, loss: 2.4450
2022-03-02 12:49:07 - train: epoch 0051, iter [01700, 05004], lr: 0.087426, loss: 2.7791
2022-03-02 12:49:42 - train: epoch 0051, iter [01800, 05004], lr: 0.087426, loss: 2.9528
2022-03-02 12:50:15 - train: epoch 0051, iter [01900, 05004], lr: 0.087426, loss: 2.8287
2022-03-02 12:50:50 - train: epoch 0051, iter [02000, 05004], lr: 0.087426, loss: 2.6335
2022-03-02 12:51:25 - train: epoch 0051, iter [02100, 05004], lr: 0.087426, loss: 2.6081
2022-03-02 12:51:59 - train: epoch 0051, iter [02200, 05004], lr: 0.087426, loss: 2.6946
2022-03-02 12:52:33 - train: epoch 0051, iter [02300, 05004], lr: 0.087426, loss: 2.8356
2022-03-02 12:53:07 - train: epoch 0051, iter [02400, 05004], lr: 0.087426, loss: 2.6552
2022-03-02 12:53:43 - train: epoch 0051, iter [02500, 05004], lr: 0.087426, loss: 2.7784
2022-03-02 12:54:17 - train: epoch 0051, iter [02600, 05004], lr: 0.087426, loss: 2.6357
2022-03-02 12:54:52 - train: epoch 0051, iter [02700, 05004], lr: 0.087426, loss: 2.6268
2022-03-02 12:55:26 - train: epoch 0051, iter [02800, 05004], lr: 0.087426, loss: 2.4879
2022-03-02 12:56:01 - train: epoch 0051, iter [02900, 05004], lr: 0.087426, loss: 2.7414
2022-03-02 12:56:34 - train: epoch 0051, iter [03000, 05004], lr: 0.087426, loss: 2.4752
2022-03-02 12:57:08 - train: epoch 0051, iter [03100, 05004], lr: 0.087426, loss: 2.7689
2022-03-02 12:57:43 - train: epoch 0051, iter [03200, 05004], lr: 0.087426, loss: 2.6868
2022-03-02 12:58:17 - train: epoch 0051, iter [03300, 05004], lr: 0.087426, loss: 2.7916
2022-03-02 12:58:52 - train: epoch 0051, iter [03400, 05004], lr: 0.087426, loss: 2.7322
2022-03-02 12:59:26 - train: epoch 0051, iter [03500, 05004], lr: 0.087426, loss: 2.7302
2022-03-02 13:00:00 - train: epoch 0051, iter [03600, 05004], lr: 0.087426, loss: 2.5601
2022-03-02 13:00:35 - train: epoch 0051, iter [03700, 05004], lr: 0.087426, loss: 2.7086
2022-03-02 13:01:09 - train: epoch 0051, iter [03800, 05004], lr: 0.087426, loss: 2.8351
2022-03-02 13:01:43 - train: epoch 0051, iter [03900, 05004], lr: 0.087426, loss: 2.5118
2022-03-02 13:02:17 - train: epoch 0051, iter [04000, 05004], lr: 0.087426, loss: 2.7851
2022-03-02 13:02:51 - train: epoch 0051, iter [04100, 05004], lr: 0.087426, loss: 2.7726
2022-03-02 13:03:26 - train: epoch 0051, iter [04200, 05004], lr: 0.087426, loss: 2.6583
2022-03-02 13:04:00 - train: epoch 0051, iter [04300, 05004], lr: 0.087426, loss: 2.4404
2022-03-02 13:04:34 - train: epoch 0051, iter [04400, 05004], lr: 0.087426, loss: 2.7197
2022-03-02 13:05:09 - train: epoch 0051, iter [04500, 05004], lr: 0.087426, loss: 2.8965
2022-03-02 13:05:44 - train: epoch 0051, iter [04600, 05004], lr: 0.087426, loss: 2.8814
2022-03-02 13:06:18 - train: epoch 0051, iter [04700, 05004], lr: 0.087426, loss: 2.7406
2022-03-02 13:06:53 - train: epoch 0051, iter [04800, 05004], lr: 0.087426, loss: 2.8198
2022-03-02 13:07:28 - train: epoch 0051, iter [04900, 05004], lr: 0.087426, loss: 2.7989
2022-03-02 13:08:00 - train: epoch 0051, iter [05000, 05004], lr: 0.087426, loss: 2.7191
2022-03-02 13:08:01 - train: epoch 051, train_loss: 2.7106
2022-03-02 13:09:16 - eval: epoch: 051, acc1: 53.210%, acc5: 79.026%, test_loss: 1.9899, per_image_load_time: 0.606ms, per_image_inference_time: 0.526ms
2022-03-02 13:09:17 - until epoch: 051, best_acc1: 54.502%
2022-03-02 13:09:17 - epoch 052 lr: 0.08688653405904652
2022-03-02 13:09:57 - train: epoch 0052, iter [00100, 05004], lr: 0.086887, loss: 2.7282
2022-03-02 13:10:32 - train: epoch 0052, iter [00200, 05004], lr: 0.086887, loss: 2.7400
2022-03-02 13:11:04 - train: epoch 0052, iter [00300, 05004], lr: 0.086887, loss: 2.7183
2022-03-02 13:11:38 - train: epoch 0052, iter [00400, 05004], lr: 0.086887, loss: 2.6638
2022-03-02 13:12:12 - train: epoch 0052, iter [00500, 05004], lr: 0.086887, loss: 2.5965
2022-03-02 13:12:47 - train: epoch 0052, iter [00600, 05004], lr: 0.086887, loss: 2.7410
2022-03-02 13:13:21 - train: epoch 0052, iter [00700, 05004], lr: 0.086887, loss: 2.6764
2022-03-02 13:13:56 - train: epoch 0052, iter [00800, 05004], lr: 0.086887, loss: 2.4482
2022-03-02 13:14:30 - train: epoch 0052, iter [00900, 05004], lr: 0.086887, loss: 2.6124
2022-03-02 13:15:04 - train: epoch 0052, iter [01000, 05004], lr: 0.086887, loss: 2.9850
2022-03-02 13:15:38 - train: epoch 0052, iter [01100, 05004], lr: 0.086887, loss: 2.8136
2022-03-02 13:16:13 - train: epoch 0052, iter [01200, 05004], lr: 0.086887, loss: 2.5028
2022-03-02 13:16:47 - train: epoch 0052, iter [01300, 05004], lr: 0.086887, loss: 2.6227
2022-03-02 13:17:21 - train: epoch 0052, iter [01400, 05004], lr: 0.086887, loss: 3.0033
2022-03-02 13:17:55 - train: epoch 0052, iter [01500, 05004], lr: 0.086887, loss: 2.6502
2022-03-02 13:18:30 - train: epoch 0052, iter [01600, 05004], lr: 0.086887, loss: 2.6334
2022-03-02 13:19:03 - train: epoch 0052, iter [01700, 05004], lr: 0.086887, loss: 2.4918
2022-03-02 13:19:39 - train: epoch 0052, iter [01800, 05004], lr: 0.086887, loss: 2.6104
2022-03-02 13:20:12 - train: epoch 0052, iter [01900, 05004], lr: 0.086887, loss: 2.4637
2022-03-02 13:20:47 - train: epoch 0052, iter [02000, 05004], lr: 0.086887, loss: 2.6274
2022-03-02 13:21:21 - train: epoch 0052, iter [02100, 05004], lr: 0.086887, loss: 2.3303
2022-03-02 13:21:55 - train: epoch 0052, iter [02200, 05004], lr: 0.086887, loss: 2.7615
2022-03-02 13:22:30 - train: epoch 0052, iter [02300, 05004], lr: 0.086887, loss: 2.6576
2022-03-02 13:23:04 - train: epoch 0052, iter [02400, 05004], lr: 0.086887, loss: 2.6191
2022-03-02 13:23:39 - train: epoch 0052, iter [02500, 05004], lr: 0.086887, loss: 2.7843
2022-03-02 13:24:13 - train: epoch 0052, iter [02600, 05004], lr: 0.086887, loss: 2.6837
2022-03-02 13:24:46 - train: epoch 0052, iter [02700, 05004], lr: 0.086887, loss: 2.6064
2022-03-02 13:25:21 - train: epoch 0052, iter [02800, 05004], lr: 0.086887, loss: 2.6587
2022-03-02 13:25:55 - train: epoch 0052, iter [02900, 05004], lr: 0.086887, loss: 2.4497
2022-03-02 13:26:29 - train: epoch 0052, iter [03000, 05004], lr: 0.086887, loss: 2.6177
2022-03-02 13:27:03 - train: epoch 0052, iter [03100, 05004], lr: 0.086887, loss: 2.9106
2022-03-02 13:27:37 - train: epoch 0052, iter [03200, 05004], lr: 0.086887, loss: 2.8058
2022-03-02 13:28:12 - train: epoch 0052, iter [03300, 05004], lr: 0.086887, loss: 2.8516
2022-03-02 13:28:46 - train: epoch 0052, iter [03400, 05004], lr: 0.086887, loss: 2.7813
2022-03-02 13:29:20 - train: epoch 0052, iter [03500, 05004], lr: 0.086887, loss: 2.6897
2022-03-02 13:29:55 - train: epoch 0052, iter [03600, 05004], lr: 0.086887, loss: 2.6044
2022-03-02 13:30:29 - train: epoch 0052, iter [03700, 05004], lr: 0.086887, loss: 2.8874
2022-03-02 13:31:04 - train: epoch 0052, iter [03800, 05004], lr: 0.086887, loss: 2.7326
2022-03-02 13:31:38 - train: epoch 0052, iter [03900, 05004], lr: 0.086887, loss: 2.3918
2022-03-02 13:32:12 - train: epoch 0052, iter [04000, 05004], lr: 0.086887, loss: 2.7573
2022-03-02 13:32:47 - train: epoch 0052, iter [04100, 05004], lr: 0.086887, loss: 2.5407
2022-03-02 13:33:21 - train: epoch 0052, iter [04200, 05004], lr: 0.086887, loss: 2.6809
2022-03-02 13:33:55 - train: epoch 0052, iter [04300, 05004], lr: 0.086887, loss: 2.7914
2022-03-02 13:34:29 - train: epoch 0052, iter [04400, 05004], lr: 0.086887, loss: 2.7682
2022-03-02 13:35:04 - train: epoch 0052, iter [04500, 05004], lr: 0.086887, loss: 3.0023
2022-03-02 13:35:38 - train: epoch 0052, iter [04600, 05004], lr: 0.086887, loss: 2.8198
2022-03-02 13:36:12 - train: epoch 0052, iter [04700, 05004], lr: 0.086887, loss: 2.6362
2022-03-02 13:36:46 - train: epoch 0052, iter [04800, 05004], lr: 0.086887, loss: 2.5589
2022-03-02 13:37:21 - train: epoch 0052, iter [04900, 05004], lr: 0.086887, loss: 2.7922
2022-03-02 13:37:53 - train: epoch 0052, iter [05000, 05004], lr: 0.086887, loss: 2.6710
2022-03-02 13:37:55 - train: epoch 052, train_loss: 2.7012
2022-03-02 13:39:09 - eval: epoch: 052, acc1: 53.710%, acc5: 78.790%, test_loss: 1.9847, per_image_load_time: 0.697ms, per_image_inference_time: 0.544ms
2022-03-02 13:39:10 - until epoch: 052, best_acc1: 54.502%
2022-03-02 13:39:10 - epoch 053 lr: 0.08633795680751116
2022-03-02 13:39:49 - train: epoch 0053, iter [00100, 05004], lr: 0.086338, loss: 2.4968
2022-03-02 13:40:24 - train: epoch 0053, iter [00200, 05004], lr: 0.086338, loss: 2.7396
2022-03-02 13:40:58 - train: epoch 0053, iter [00300, 05004], lr: 0.086338, loss: 2.6902
2022-03-02 13:41:32 - train: epoch 0053, iter [00400, 05004], lr: 0.086338, loss: 2.6119
2022-03-02 13:42:07 - train: epoch 0053, iter [00500, 05004], lr: 0.086338, loss: 2.7396
2022-03-02 13:42:40 - train: epoch 0053, iter [00600, 05004], lr: 0.086338, loss: 2.5260
2022-03-02 13:43:15 - train: epoch 0053, iter [00700, 05004], lr: 0.086338, loss: 2.7280
2022-03-02 13:43:49 - train: epoch 0053, iter [00800, 05004], lr: 0.086338, loss: 2.7903
2022-03-02 13:44:24 - train: epoch 0053, iter [00900, 05004], lr: 0.086338, loss: 2.7081
2022-03-02 13:44:58 - train: epoch 0053, iter [01000, 05004], lr: 0.086338, loss: 2.8204
2022-03-02 13:45:31 - train: epoch 0053, iter [01100, 05004], lr: 0.086338, loss: 2.7407
2022-03-02 13:46:06 - train: epoch 0053, iter [01200, 05004], lr: 0.086338, loss: 2.4871
2022-03-02 13:46:40 - train: epoch 0053, iter [01300, 05004], lr: 0.086338, loss: 2.7926
2022-03-02 13:47:15 - train: epoch 0053, iter [01400, 05004], lr: 0.086338, loss: 2.7637
2022-03-02 13:47:48 - train: epoch 0053, iter [01500, 05004], lr: 0.086338, loss: 2.6629
2022-03-02 13:48:23 - train: epoch 0053, iter [01600, 05004], lr: 0.086338, loss: 2.8440
2022-03-02 13:48:57 - train: epoch 0053, iter [01700, 05004], lr: 0.086338, loss: 2.5112
2022-03-02 13:49:31 - train: epoch 0053, iter [01800, 05004], lr: 0.086338, loss: 2.6034
2022-03-02 13:50:06 - train: epoch 0053, iter [01900, 05004], lr: 0.086338, loss: 2.8351
2022-03-02 13:50:40 - train: epoch 0053, iter [02000, 05004], lr: 0.086338, loss: 2.7337
2022-03-02 13:51:14 - train: epoch 0053, iter [02100, 05004], lr: 0.086338, loss: 2.7140
2022-03-02 13:51:48 - train: epoch 0053, iter [02200, 05004], lr: 0.086338, loss: 2.6554
2022-03-02 13:52:22 - train: epoch 0053, iter [02300, 05004], lr: 0.086338, loss: 2.7639
2022-03-02 13:52:57 - train: epoch 0053, iter [02400, 05004], lr: 0.086338, loss: 2.8134
2022-03-02 13:53:32 - train: epoch 0053, iter [02500, 05004], lr: 0.086338, loss: 2.9983
2022-03-02 13:54:06 - train: epoch 0053, iter [02600, 05004], lr: 0.086338, loss: 2.7100
2022-03-02 13:54:40 - train: epoch 0053, iter [02700, 05004], lr: 0.086338, loss: 2.8039
2022-03-02 13:55:14 - train: epoch 0053, iter [02800, 05004], lr: 0.086338, loss: 2.9134
2022-03-02 13:55:48 - train: epoch 0053, iter [02900, 05004], lr: 0.086338, loss: 2.5567
2022-03-02 13:56:22 - train: epoch 0053, iter [03000, 05004], lr: 0.086338, loss: 2.6553
2022-03-02 13:56:57 - train: epoch 0053, iter [03100, 05004], lr: 0.086338, loss: 2.9405
2022-03-02 13:57:32 - train: epoch 0053, iter [03200, 05004], lr: 0.086338, loss: 2.7185
2022-03-02 13:58:06 - train: epoch 0053, iter [03300, 05004], lr: 0.086338, loss: 2.6576
2022-03-02 13:58:40 - train: epoch 0053, iter [03400, 05004], lr: 0.086338, loss: 2.8598
2022-03-02 13:59:14 - train: epoch 0053, iter [03500, 05004], lr: 0.086338, loss: 2.5387
2022-03-02 13:59:49 - train: epoch 0053, iter [03600, 05004], lr: 0.086338, loss: 3.0020
2022-03-02 14:00:23 - train: epoch 0053, iter [03700, 05004], lr: 0.086338, loss: 2.7521
2022-03-02 14:00:57 - train: epoch 0053, iter [03800, 05004], lr: 0.086338, loss: 2.5875
2022-03-02 14:01:31 - train: epoch 0053, iter [03900, 05004], lr: 0.086338, loss: 2.9599
2022-03-02 14:02:05 - train: epoch 0053, iter [04000, 05004], lr: 0.086338, loss: 2.8252
2022-03-02 14:02:40 - train: epoch 0053, iter [04100, 05004], lr: 0.086338, loss: 2.8118
2022-03-02 14:03:14 - train: epoch 0053, iter [04200, 05004], lr: 0.086338, loss: 2.6936
2022-03-02 14:03:48 - train: epoch 0053, iter [04300, 05004], lr: 0.086338, loss: 2.9047
2022-03-02 14:04:23 - train: epoch 0053, iter [04400, 05004], lr: 0.086338, loss: 2.6024
2022-03-02 14:04:57 - train: epoch 0053, iter [04500, 05004], lr: 0.086338, loss: 3.0155
2022-03-02 14:05:32 - train: epoch 0053, iter [04600, 05004], lr: 0.086338, loss: 2.6453
2022-03-02 14:06:06 - train: epoch 0053, iter [04700, 05004], lr: 0.086338, loss: 2.8054
2022-03-02 14:06:40 - train: epoch 0053, iter [04800, 05004], lr: 0.086338, loss: 2.7410
2022-03-02 14:07:14 - train: epoch 0053, iter [04900, 05004], lr: 0.086338, loss: 2.6738
2022-03-02 14:07:47 - train: epoch 0053, iter [05000, 05004], lr: 0.086338, loss: 2.6765
2022-03-02 14:07:48 - train: epoch 053, train_loss: 2.6980
2022-03-02 14:09:03 - eval: epoch: 053, acc1: 54.576%, acc5: 79.750%, test_loss: 1.9231, per_image_load_time: 0.734ms, per_image_inference_time: 0.514ms
2022-03-02 14:09:04 - until epoch: 053, best_acc1: 54.576%
2022-03-02 14:09:04 - epoch 054 lr: 0.08577994803720607
2022-03-02 14:09:43 - train: epoch 0054, iter [00100, 05004], lr: 0.085780, loss: 2.6370
2022-03-02 14:10:18 - train: epoch 0054, iter [00200, 05004], lr: 0.085780, loss: 2.7644
2022-03-02 14:10:51 - train: epoch 0054, iter [00300, 05004], lr: 0.085780, loss: 2.6403
2022-03-02 14:11:25 - train: epoch 0054, iter [00400, 05004], lr: 0.085780, loss: 2.4947
2022-03-02 14:12:01 - train: epoch 0054, iter [00500, 05004], lr: 0.085780, loss: 2.4310
2022-03-02 14:12:35 - train: epoch 0054, iter [00600, 05004], lr: 0.085780, loss: 2.7406
2022-03-02 14:13:10 - train: epoch 0054, iter [00700, 05004], lr: 0.085780, loss: 2.6230
2022-03-02 14:13:43 - train: epoch 0054, iter [00800, 05004], lr: 0.085780, loss: 2.9147
2022-03-02 14:14:18 - train: epoch 0054, iter [00900, 05004], lr: 0.085780, loss: 2.3823
2022-03-02 14:14:52 - train: epoch 0054, iter [01000, 05004], lr: 0.085780, loss: 2.4596
2022-03-02 14:15:26 - train: epoch 0054, iter [01100, 05004], lr: 0.085780, loss: 2.7070
2022-03-02 14:16:01 - train: epoch 0054, iter [01200, 05004], lr: 0.085780, loss: 2.7652
2022-03-02 14:16:35 - train: epoch 0054, iter [01300, 05004], lr: 0.085780, loss: 2.7400
2022-03-02 14:17:09 - train: epoch 0054, iter [01400, 05004], lr: 0.085780, loss: 2.9708
2022-03-02 14:17:44 - train: epoch 0054, iter [01500, 05004], lr: 0.085780, loss: 2.7684
2022-03-02 14:18:17 - train: epoch 0054, iter [01600, 05004], lr: 0.085780, loss: 2.7564
2022-03-02 14:18:52 - train: epoch 0054, iter [01700, 05004], lr: 0.085780, loss: 2.7697
2022-03-02 14:19:27 - train: epoch 0054, iter [01800, 05004], lr: 0.085780, loss: 2.7138
2022-03-02 14:20:01 - train: epoch 0054, iter [01900, 05004], lr: 0.085780, loss: 2.8798
2022-03-02 14:20:34 - train: epoch 0054, iter [02000, 05004], lr: 0.085780, loss: 3.0021
2022-03-02 14:21:09 - train: epoch 0054, iter [02100, 05004], lr: 0.085780, loss: 2.6479
2022-03-02 14:21:43 - train: epoch 0054, iter [02200, 05004], lr: 0.085780, loss: 2.7760
2022-03-02 14:22:17 - train: epoch 0054, iter [02300, 05004], lr: 0.085780, loss: 2.5450
2022-03-02 14:22:52 - train: epoch 0054, iter [02400, 05004], lr: 0.085780, loss: 2.6898
2022-03-02 14:23:26 - train: epoch 0054, iter [02500, 05004], lr: 0.085780, loss: 2.7034
2022-03-02 14:23:59 - train: epoch 0054, iter [02600, 05004], lr: 0.085780, loss: 2.7649
2022-03-02 14:24:35 - train: epoch 0054, iter [02700, 05004], lr: 0.085780, loss: 2.8036
2022-03-02 14:25:08 - train: epoch 0054, iter [02800, 05004], lr: 0.085780, loss: 2.7215
2022-03-02 14:25:42 - train: epoch 0054, iter [02900, 05004], lr: 0.085780, loss: 2.6844
2022-03-02 14:26:16 - train: epoch 0054, iter [03000, 05004], lr: 0.085780, loss: 2.7110
2022-03-02 14:26:50 - train: epoch 0054, iter [03100, 05004], lr: 0.085780, loss: 2.7521
2022-03-02 14:27:24 - train: epoch 0054, iter [03200, 05004], lr: 0.085780, loss: 2.8772
2022-03-02 14:27:59 - train: epoch 0054, iter [03300, 05004], lr: 0.085780, loss: 2.5700
2022-03-02 14:28:32 - train: epoch 0054, iter [03400, 05004], lr: 0.085780, loss: 2.5090
2022-03-02 14:29:07 - train: epoch 0054, iter [03500, 05004], lr: 0.085780, loss: 2.7851
2022-03-02 14:29:41 - train: epoch 0054, iter [03600, 05004], lr: 0.085780, loss: 2.7994
2022-03-02 14:30:15 - train: epoch 0054, iter [03700, 05004], lr: 0.085780, loss: 2.4886
2022-03-02 14:30:49 - train: epoch 0054, iter [03800, 05004], lr: 0.085780, loss: 2.7389
2022-03-02 14:31:24 - train: epoch 0054, iter [03900, 05004], lr: 0.085780, loss: 2.8000
2022-03-02 14:31:58 - train: epoch 0054, iter [04000, 05004], lr: 0.085780, loss: 2.7321
2022-03-02 14:32:32 - train: epoch 0054, iter [04100, 05004], lr: 0.085780, loss: 2.7120
2022-03-02 14:33:07 - train: epoch 0054, iter [04200, 05004], lr: 0.085780, loss: 2.8657
2022-03-02 14:33:41 - train: epoch 0054, iter [04300, 05004], lr: 0.085780, loss: 2.8967
2022-03-02 14:34:16 - train: epoch 0054, iter [04400, 05004], lr: 0.085780, loss: 2.5664
2022-03-02 14:34:50 - train: epoch 0054, iter [04500, 05004], lr: 0.085780, loss: 2.5815
2022-03-02 14:35:25 - train: epoch 0054, iter [04600, 05004], lr: 0.085780, loss: 2.9640
2022-03-02 14:35:59 - train: epoch 0054, iter [04700, 05004], lr: 0.085780, loss: 2.9579
2022-03-02 14:36:34 - train: epoch 0054, iter [04800, 05004], lr: 0.085780, loss: 2.6856
2022-03-02 14:37:08 - train: epoch 0054, iter [04900, 05004], lr: 0.085780, loss: 2.6459
2022-03-02 14:37:41 - train: epoch 0054, iter [05000, 05004], lr: 0.085780, loss: 2.7499
2022-03-02 14:37:42 - train: epoch 054, train_loss: 2.6958
2022-03-02 14:38:57 - eval: epoch: 054, acc1: 54.908%, acc5: 80.122%, test_loss: 1.8993, per_image_load_time: 0.875ms, per_image_inference_time: 0.527ms
2022-03-02 14:38:58 - until epoch: 054, best_acc1: 54.908%
2022-03-02 14:38:58 - epoch 055 lr: 0.08521265257933948
2022-03-02 14:39:38 - train: epoch 0055, iter [00100, 05004], lr: 0.085213, loss: 2.5565
2022-03-02 14:40:12 - train: epoch 0055, iter [00200, 05004], lr: 0.085213, loss: 2.5753
2022-03-02 14:40:46 - train: epoch 0055, iter [00300, 05004], lr: 0.085213, loss: 2.4540
2022-03-02 14:41:20 - train: epoch 0055, iter [00400, 05004], lr: 0.085213, loss: 2.4870
2022-03-02 14:41:54 - train: epoch 0055, iter [00500, 05004], lr: 0.085213, loss: 2.4833
2022-03-02 14:42:28 - train: epoch 0055, iter [00600, 05004], lr: 0.085213, loss: 2.6338
2022-03-02 14:43:02 - train: epoch 0055, iter [00700, 05004], lr: 0.085213, loss: 2.7404
2022-03-02 14:43:36 - train: epoch 0055, iter [00800, 05004], lr: 0.085213, loss: 2.5116
2022-03-02 14:44:11 - train: epoch 0055, iter [00900, 05004], lr: 0.085213, loss: 3.0979
2022-03-02 14:44:46 - train: epoch 0055, iter [01000, 05004], lr: 0.085213, loss: 2.7920
2022-03-02 14:45:19 - train: epoch 0055, iter [01100, 05004], lr: 0.085213, loss: 2.5696
2022-03-02 14:45:53 - train: epoch 0055, iter [01200, 05004], lr: 0.085213, loss: 2.7324
2022-03-02 14:46:28 - train: epoch 0055, iter [01300, 05004], lr: 0.085213, loss: 2.5837
2022-03-02 14:47:02 - train: epoch 0055, iter [01400, 05004], lr: 0.085213, loss: 2.9888
2022-03-02 14:47:37 - train: epoch 0055, iter [01500, 05004], lr: 0.085213, loss: 2.5549
2022-03-02 14:48:11 - train: epoch 0055, iter [01600, 05004], lr: 0.085213, loss: 2.9233
2022-03-02 14:48:45 - train: epoch 0055, iter [01700, 05004], lr: 0.085213, loss: 2.6523
2022-03-02 14:49:19 - train: epoch 0055, iter [01800, 05004], lr: 0.085213, loss: 2.5266
2022-03-02 14:49:54 - train: epoch 0055, iter [01900, 05004], lr: 0.085213, loss: 2.9144
2022-03-02 14:50:28 - train: epoch 0055, iter [02000, 05004], lr: 0.085213, loss: 2.6528
2022-03-02 14:51:02 - train: epoch 0055, iter [02100, 05004], lr: 0.085213, loss: 2.4557
2022-03-02 14:51:37 - train: epoch 0055, iter [02200, 05004], lr: 0.085213, loss: 2.8421
2022-03-02 14:52:11 - train: epoch 0055, iter [02300, 05004], lr: 0.085213, loss: 2.8664
2022-03-02 14:52:46 - train: epoch 0055, iter [02400, 05004], lr: 0.085213, loss: 2.4398
2022-03-02 14:53:20 - train: epoch 0055, iter [02500, 05004], lr: 0.085213, loss: 2.6828
2022-03-02 14:53:54 - train: epoch 0055, iter [02600, 05004], lr: 0.085213, loss: 2.5991
2022-03-02 14:54:28 - train: epoch 0055, iter [02700, 05004], lr: 0.085213, loss: 2.6016
2022-03-02 14:55:02 - train: epoch 0055, iter [02800, 05004], lr: 0.085213, loss: 2.7922
2022-03-02 14:55:37 - train: epoch 0055, iter [02900, 05004], lr: 0.085213, loss: 2.7143
2022-03-02 14:56:10 - train: epoch 0055, iter [03000, 05004], lr: 0.085213, loss: 2.7619
2022-03-02 14:56:44 - train: epoch 0055, iter [03100, 05004], lr: 0.085213, loss: 2.7664
2022-03-02 14:57:19 - train: epoch 0055, iter [03200, 05004], lr: 0.085213, loss: 2.6118
2022-03-02 14:57:53 - train: epoch 0055, iter [03300, 05004], lr: 0.085213, loss: 2.6586
2022-03-02 14:58:28 - train: epoch 0055, iter [03400, 05004], lr: 0.085213, loss: 2.5360
2022-03-02 14:59:02 - train: epoch 0055, iter [03500, 05004], lr: 0.085213, loss: 2.4881
2022-03-02 14:59:37 - train: epoch 0055, iter [03600, 05004], lr: 0.085213, loss: 2.8444
2022-03-02 15:00:11 - train: epoch 0055, iter [03700, 05004], lr: 0.085213, loss: 2.6002
2022-03-02 15:00:45 - train: epoch 0055, iter [03800, 05004], lr: 0.085213, loss: 2.7285
2022-03-02 15:01:19 - train: epoch 0055, iter [03900, 05004], lr: 0.085213, loss: 3.1334
2022-03-02 15:01:53 - train: epoch 0055, iter [04000, 05004], lr: 0.085213, loss: 2.7152
2022-03-02 15:02:27 - train: epoch 0055, iter [04100, 05004], lr: 0.085213, loss: 2.6721
2022-03-02 15:03:02 - train: epoch 0055, iter [04200, 05004], lr: 0.085213, loss: 2.9143
2022-03-02 15:03:36 - train: epoch 0055, iter [04300, 05004], lr: 0.085213, loss: 2.8416
2022-03-02 15:04:10 - train: epoch 0055, iter [04400, 05004], lr: 0.085213, loss: 2.5703
2022-03-02 15:04:46 - train: epoch 0055, iter [04500, 05004], lr: 0.085213, loss: 2.8271
2022-03-02 15:05:20 - train: epoch 0055, iter [04600, 05004], lr: 0.085213, loss: 2.7930
2022-03-02 15:05:55 - train: epoch 0055, iter [04700, 05004], lr: 0.085213, loss: 2.5940
2022-03-02 15:06:28 - train: epoch 0055, iter [04800, 05004], lr: 0.085213, loss: 2.7037
2022-03-02 15:07:04 - train: epoch 0055, iter [04900, 05004], lr: 0.085213, loss: 2.7744
2022-03-02 15:07:36 - train: epoch 0055, iter [05000, 05004], lr: 0.085213, loss: 2.7972
2022-03-02 15:07:38 - train: epoch 055, train_loss: 2.6928
2022-03-02 15:08:52 - eval: epoch: 055, acc1: 55.374%, acc5: 80.458%, test_loss: 1.8896, per_image_load_time: 0.620ms, per_image_inference_time: 0.511ms
2022-03-02 15:08:53 - until epoch: 055, best_acc1: 55.374%
2022-03-02 15:08:53 - epoch 056 lr: 0.08463621767547998
2022-03-02 15:09:33 - train: epoch 0056, iter [00100, 05004], lr: 0.084636, loss: 2.6484
2022-03-02 15:10:07 - train: epoch 0056, iter [00200, 05004], lr: 0.084636, loss: 2.7250
2022-03-02 15:10:42 - train: epoch 0056, iter [00300, 05004], lr: 0.084636, loss: 2.4052
2022-03-02 15:11:16 - train: epoch 0056, iter [00400, 05004], lr: 0.084636, loss: 2.6777
2022-03-02 15:11:51 - train: epoch 0056, iter [00500, 05004], lr: 0.084636, loss: 2.7224
2022-03-02 15:12:25 - train: epoch 0056, iter [00600, 05004], lr: 0.084636, loss: 2.9148
2022-03-02 15:13:00 - train: epoch 0056, iter [00700, 05004], lr: 0.084636, loss: 2.8608
2022-03-02 15:13:33 - train: epoch 0056, iter [00800, 05004], lr: 0.084636, loss: 2.7941
2022-03-02 15:14:07 - train: epoch 0056, iter [00900, 05004], lr: 0.084636, loss: 2.7365
2022-03-02 15:14:41 - train: epoch 0056, iter [01000, 05004], lr: 0.084636, loss: 2.6304
2022-03-02 15:15:15 - train: epoch 0056, iter [01100, 05004], lr: 0.084636, loss: 2.4560
2022-03-02 15:15:49 - train: epoch 0056, iter [01200, 05004], lr: 0.084636, loss: 2.4359
2022-03-02 15:16:22 - train: epoch 0056, iter [01300, 05004], lr: 0.084636, loss: 2.7905
2022-03-02 15:16:56 - train: epoch 0056, iter [01400, 05004], lr: 0.084636, loss: 2.9238
2022-03-02 15:17:30 - train: epoch 0056, iter [01500, 05004], lr: 0.084636, loss: 3.2417
2022-03-02 15:18:04 - train: epoch 0056, iter [01600, 05004], lr: 0.084636, loss: 2.7041
2022-03-02 15:18:37 - train: epoch 0056, iter [01700, 05004], lr: 0.084636, loss: 2.9758
2022-03-02 15:19:11 - train: epoch 0056, iter [01800, 05004], lr: 0.084636, loss: 2.8272
2022-03-02 15:19:45 - train: epoch 0056, iter [01900, 05004], lr: 0.084636, loss: 2.7353
2022-03-02 15:20:19 - train: epoch 0056, iter [02000, 05004], lr: 0.084636, loss: 2.7397
2022-03-02 15:20:53 - train: epoch 0056, iter [02100, 05004], lr: 0.084636, loss: 2.5816
2022-03-02 15:21:27 - train: epoch 0056, iter [02200, 05004], lr: 0.084636, loss: 2.9782
2022-03-02 15:22:00 - train: epoch 0056, iter [02300, 05004], lr: 0.084636, loss: 2.7704
2022-03-02 15:22:34 - train: epoch 0056, iter [02400, 05004], lr: 0.084636, loss: 2.4399
2022-03-02 15:23:07 - train: epoch 0056, iter [02500, 05004], lr: 0.084636, loss: 3.0212
2022-03-02 15:23:42 - train: epoch 0056, iter [02600, 05004], lr: 0.084636, loss: 2.5942
2022-03-02 15:24:15 - train: epoch 0056, iter [02700, 05004], lr: 0.084636, loss: 2.7400
2022-03-02 15:24:49 - train: epoch 0056, iter [02800, 05004], lr: 0.084636, loss: 2.6163
2022-03-02 15:25:23 - train: epoch 0056, iter [02900, 05004], lr: 0.084636, loss: 2.6428
2022-03-02 15:25:57 - train: epoch 0056, iter [03000, 05004], lr: 0.084636, loss: 2.8338
2022-03-02 15:26:31 - train: epoch 0056, iter [03100, 05004], lr: 0.084636, loss: 2.7205
2022-03-02 15:27:05 - train: epoch 0056, iter [03200, 05004], lr: 0.084636, loss: 2.6905
2022-03-02 15:27:39 - train: epoch 0056, iter [03300, 05004], lr: 0.084636, loss: 2.7529
2022-03-02 15:28:12 - train: epoch 0056, iter [03400, 05004], lr: 0.084636, loss: 2.7699
2022-03-02 15:28:46 - train: epoch 0056, iter [03500, 05004], lr: 0.084636, loss: 2.5639
2022-03-02 15:29:20 - train: epoch 0056, iter [03600, 05004], lr: 0.084636, loss: 2.5117
2022-03-02 15:29:54 - train: epoch 0056, iter [03700, 05004], lr: 0.084636, loss: 2.7071
2022-03-02 15:30:28 - train: epoch 0056, iter [03800, 05004], lr: 0.084636, loss: 2.7536
2022-03-02 15:31:02 - train: epoch 0056, iter [03900, 05004], lr: 0.084636, loss: 2.8167
2022-03-02 15:31:37 - train: epoch 0056, iter [04000, 05004], lr: 0.084636, loss: 2.8630
2022-03-02 15:32:11 - train: epoch 0056, iter [04100, 05004], lr: 0.084636, loss: 2.6841
2022-03-02 15:32:45 - train: epoch 0056, iter [04200, 05004], lr: 0.084636, loss: 2.6331
2022-03-02 15:33:20 - train: epoch 0056, iter [04300, 05004], lr: 0.084636, loss: 2.6538
2022-03-02 15:33:53 - train: epoch 0056, iter [04400, 05004], lr: 0.084636, loss: 2.6501
2022-03-02 15:34:27 - train: epoch 0056, iter [04500, 05004], lr: 0.084636, loss: 2.5410
2022-03-02 15:35:01 - train: epoch 0056, iter [04600, 05004], lr: 0.084636, loss: 2.6189
2022-03-02 15:35:35 - train: epoch 0056, iter [04700, 05004], lr: 0.084636, loss: 2.5293
2022-03-02 15:36:09 - train: epoch 0056, iter [04800, 05004], lr: 0.084636, loss: 2.7425
2022-03-02 15:36:43 - train: epoch 0056, iter [04900, 05004], lr: 0.084636, loss: 2.7685
2022-03-02 15:37:16 - train: epoch 0056, iter [05000, 05004], lr: 0.084636, loss: 2.7345
2022-03-02 15:37:17 - train: epoch 056, train_loss: 2.6878
2022-03-02 15:38:31 - eval: epoch: 056, acc1: 54.126%, acc5: 79.634%, test_loss: 1.9378, per_image_load_time: 0.621ms, per_image_inference_time: 0.487ms
2022-03-02 15:38:32 - until epoch: 056, best_acc1: 55.374%
2022-03-02 15:38:32 - epoch 057 lr: 0.08405079293933987
2022-03-02 15:39:11 - train: epoch 0057, iter [00100, 05004], lr: 0.084051, loss: 2.6361
2022-03-02 15:39:45 - train: epoch 0057, iter [00200, 05004], lr: 0.084051, loss: 2.8736
2022-03-02 15:40:19 - train: epoch 0057, iter [00300, 05004], lr: 0.084051, loss: 2.8279
2022-03-02 15:40:53 - train: epoch 0057, iter [00400, 05004], lr: 0.084051, loss: 2.6458
2022-03-02 15:41:27 - train: epoch 0057, iter [00500, 05004], lr: 0.084051, loss: 2.3982
2022-03-02 15:42:01 - train: epoch 0057, iter [00600, 05004], lr: 0.084051, loss: 2.7148
2022-03-02 15:42:36 - train: epoch 0057, iter [00700, 05004], lr: 0.084051, loss: 2.5836
2022-03-02 15:43:09 - train: epoch 0057, iter [00800, 05004], lr: 0.084051, loss: 2.5580
2022-03-02 15:43:44 - train: epoch 0057, iter [00900, 05004], lr: 0.084051, loss: 2.6108
2022-03-02 15:44:17 - train: epoch 0057, iter [01000, 05004], lr: 0.084051, loss: 2.4069
2022-03-02 15:44:52 - train: epoch 0057, iter [01100, 05004], lr: 0.084051, loss: 2.4896
2022-03-02 15:45:27 - train: epoch 0057, iter [01200, 05004], lr: 0.084051, loss: 2.7809
2022-03-02 15:46:00 - train: epoch 0057, iter [01300, 05004], lr: 0.084051, loss: 2.6995
2022-03-02 15:46:34 - train: epoch 0057, iter [01400, 05004], lr: 0.084051, loss: 2.7376
2022-03-02 15:47:08 - train: epoch 0057, iter [01500, 05004], lr: 0.084051, loss: 2.6943
2022-03-02 15:47:42 - train: epoch 0057, iter [01600, 05004], lr: 0.084051, loss: 2.9682
2022-03-02 15:48:17 - train: epoch 0057, iter [01700, 05004], lr: 0.084051, loss: 2.5854
2022-03-02 15:48:51 - train: epoch 0057, iter [01800, 05004], lr: 0.084051, loss: 2.8100
2022-03-02 15:49:24 - train: epoch 0057, iter [01900, 05004], lr: 0.084051, loss: 2.5378
2022-03-02 15:49:58 - train: epoch 0057, iter [02000, 05004], lr: 0.084051, loss: 2.6884
2022-03-02 15:50:32 - train: epoch 0057, iter [02100, 05004], lr: 0.084051, loss: 2.8721
2022-03-02 15:51:06 - train: epoch 0057, iter [02200, 05004], lr: 0.084051, loss: 3.0638
2022-03-02 15:51:41 - train: epoch 0057, iter [02300, 05004], lr: 0.084051, loss: 2.7234
2022-03-02 15:52:14 - train: epoch 0057, iter [02400, 05004], lr: 0.084051, loss: 2.7047
2022-03-02 15:52:48 - train: epoch 0057, iter [02500, 05004], lr: 0.084051, loss: 2.7675
2022-03-02 15:53:22 - train: epoch 0057, iter [02600, 05004], lr: 0.084051, loss: 2.6024
2022-03-02 15:53:56 - train: epoch 0057, iter [02700, 05004], lr: 0.084051, loss: 2.3977
2022-03-02 15:54:29 - train: epoch 0057, iter [02800, 05004], lr: 0.084051, loss: 2.0653
2022-03-02 15:55:03 - train: epoch 0057, iter [02900, 05004], lr: 0.084051, loss: 2.7076
2022-03-02 15:55:37 - train: epoch 0057, iter [03000, 05004], lr: 0.084051, loss: 2.6708
2022-03-02 15:56:12 - train: epoch 0057, iter [03100, 05004], lr: 0.084051, loss: 2.6965
2022-03-02 15:56:46 - train: epoch 0057, iter [03200, 05004], lr: 0.084051, loss: 2.6967
2022-03-02 15:57:20 - train: epoch 0057, iter [03300, 05004], lr: 0.084051, loss: 2.5682
2022-03-02 15:57:54 - train: epoch 0057, iter [03400, 05004], lr: 0.084051, loss: 2.8831
2022-03-02 15:58:29 - train: epoch 0057, iter [03500, 05004], lr: 0.084051, loss: 2.7242
2022-03-02 15:59:03 - train: epoch 0057, iter [03600, 05004], lr: 0.084051, loss: 2.6470
2022-03-02 15:59:37 - train: epoch 0057, iter [03700, 05004], lr: 0.084051, loss: 2.5185
2022-03-02 16:00:11 - train: epoch 0057, iter [03800, 05004], lr: 0.084051, loss: 2.6077
2022-03-02 16:00:44 - train: epoch 0057, iter [03900, 05004], lr: 0.084051, loss: 3.0390
2022-03-02 16:01:18 - train: epoch 0057, iter [04000, 05004], lr: 0.084051, loss: 2.6866
2022-03-02 16:01:52 - train: epoch 0057, iter [04100, 05004], lr: 0.084051, loss: 2.7384
2022-03-02 16:02:26 - train: epoch 0057, iter [04200, 05004], lr: 0.084051, loss: 2.5998
2022-03-02 16:03:00 - train: epoch 0057, iter [04300, 05004], lr: 0.084051, loss: 2.5139
2022-03-02 16:03:35 - train: epoch 0057, iter [04400, 05004], lr: 0.084051, loss: 2.9659
2022-03-02 16:04:08 - train: epoch 0057, iter [04500, 05004], lr: 0.084051, loss: 2.8307
2022-03-02 16:04:43 - train: epoch 0057, iter [04600, 05004], lr: 0.084051, loss: 2.6733
2022-03-02 16:05:16 - train: epoch 0057, iter [04700, 05004], lr: 0.084051, loss: 2.4698
2022-03-02 16:05:51 - train: epoch 0057, iter [04800, 05004], lr: 0.084051, loss: 2.7259
2022-03-02 16:06:24 - train: epoch 0057, iter [04900, 05004], lr: 0.084051, loss: 2.7596
2022-03-02 16:06:58 - train: epoch 0057, iter [05000, 05004], lr: 0.084051, loss: 2.6015
2022-03-02 16:06:59 - train: epoch 057, train_loss: 2.6804
2022-03-02 16:08:13 - eval: epoch: 057, acc1: 52.582%, acc5: 78.260%, test_loss: 2.0344, per_image_load_time: 0.753ms, per_image_inference_time: 0.483ms
2022-03-02 16:08:13 - until epoch: 057, best_acc1: 55.374%
2022-03-02 16:08:13 - epoch 058 lr: 0.08345653031794292
2022-03-02 16:08:53 - train: epoch 0058, iter [00100, 05004], lr: 0.083457, loss: 2.7840
2022-03-02 16:09:26 - train: epoch 0058, iter [00200, 05004], lr: 0.083457, loss: 2.6081
2022-03-02 16:10:01 - train: epoch 0058, iter [00300, 05004], lr: 0.083457, loss: 2.8779
2022-03-02 16:10:35 - train: epoch 0058, iter [00400, 05004], lr: 0.083457, loss: 2.4328
2022-03-02 16:11:09 - train: epoch 0058, iter [00500, 05004], lr: 0.083457, loss: 2.8015
2022-03-02 16:11:42 - train: epoch 0058, iter [00600, 05004], lr: 0.083457, loss: 2.6341
2022-03-02 16:12:17 - train: epoch 0058, iter [00700, 05004], lr: 0.083457, loss: 2.5076
2022-03-02 16:12:50 - train: epoch 0058, iter [00800, 05004], lr: 0.083457, loss: 2.5229
2022-03-02 16:13:24 - train: epoch 0058, iter [00900, 05004], lr: 0.083457, loss: 2.4774
2022-03-02 16:13:58 - train: epoch 0058, iter [01000, 05004], lr: 0.083457, loss: 2.7220
2022-03-02 16:14:31 - train: epoch 0058, iter [01100, 05004], lr: 0.083457, loss: 2.4798
2022-03-02 16:15:06 - train: epoch 0058, iter [01200, 05004], lr: 0.083457, loss: 2.7693
2022-03-02 16:15:40 - train: epoch 0058, iter [01300, 05004], lr: 0.083457, loss: 2.8817
2022-03-02 16:16:13 - train: epoch 0058, iter [01400, 05004], lr: 0.083457, loss: 2.9062
2022-03-02 16:16:48 - train: epoch 0058, iter [01500, 05004], lr: 0.083457, loss: 2.5891
2022-03-02 16:17:22 - train: epoch 0058, iter [01600, 05004], lr: 0.083457, loss: 2.5771
2022-03-02 16:17:56 - train: epoch 0058, iter [01700, 05004], lr: 0.083457, loss: 3.0022
2022-03-02 16:18:30 - train: epoch 0058, iter [01800, 05004], lr: 0.083457, loss: 2.7635
2022-03-02 16:19:04 - train: epoch 0058, iter [01900, 05004], lr: 0.083457, loss: 2.8649
2022-03-02 16:19:39 - train: epoch 0058, iter [02000, 05004], lr: 0.083457, loss: 2.9327
2022-03-02 16:20:12 - train: epoch 0058, iter [02100, 05004], lr: 0.083457, loss: 2.7141
2022-03-02 16:20:46 - train: epoch 0058, iter [02200, 05004], lr: 0.083457, loss: 2.7479
2022-03-02 16:21:21 - train: epoch 0058, iter [02300, 05004], lr: 0.083457, loss: 2.7265
2022-03-02 16:21:55 - train: epoch 0058, iter [02400, 05004], lr: 0.083457, loss: 2.7830
2022-03-02 16:22:30 - train: epoch 0058, iter [02500, 05004], lr: 0.083457, loss: 2.6551
2022-03-02 16:23:02 - train: epoch 0058, iter [02600, 05004], lr: 0.083457, loss: 2.6460
2022-03-02 16:23:38 - train: epoch 0058, iter [02700, 05004], lr: 0.083457, loss: 2.8113
2022-03-02 16:24:12 - train: epoch 0058, iter [02800, 05004], lr: 0.083457, loss: 2.7602
2022-03-02 16:24:46 - train: epoch 0058, iter [02900, 05004], lr: 0.083457, loss: 2.7126
2022-03-02 16:25:19 - train: epoch 0058, iter [03000, 05004], lr: 0.083457, loss: 2.7617
2022-03-02 16:25:54 - train: epoch 0058, iter [03100, 05004], lr: 0.083457, loss: 2.6923
2022-03-02 16:26:28 - train: epoch 0058, iter [03200, 05004], lr: 0.083457, loss: 2.6494
2022-03-02 16:27:01 - train: epoch 0058, iter [03300, 05004], lr: 0.083457, loss: 2.3917
2022-03-02 16:27:36 - train: epoch 0058, iter [03400, 05004], lr: 0.083457, loss: 2.5429
2022-03-02 16:28:09 - train: epoch 0058, iter [03500, 05004], lr: 0.083457, loss: 2.6578
2022-03-02 16:28:44 - train: epoch 0058, iter [03600, 05004], lr: 0.083457, loss: 2.4052
2022-03-02 16:29:18 - train: epoch 0058, iter [03700, 05004], lr: 0.083457, loss: 2.4845
2022-03-02 16:29:51 - train: epoch 0058, iter [03800, 05004], lr: 0.083457, loss: 2.8476
2022-03-02 16:30:26 - train: epoch 0058, iter [03900, 05004], lr: 0.083457, loss: 2.4940
2022-03-02 16:30:59 - train: epoch 0058, iter [04000, 05004], lr: 0.083457, loss: 2.7829
2022-03-02 16:31:34 - train: epoch 0058, iter [04100, 05004], lr: 0.083457, loss: 2.8917
2022-03-02 16:32:07 - train: epoch 0058, iter [04200, 05004], lr: 0.083457, loss: 2.4192
2022-03-02 16:32:42 - train: epoch 0058, iter [04300, 05004], lr: 0.083457, loss: 2.5700
2022-03-02 16:33:16 - train: epoch 0058, iter [04400, 05004], lr: 0.083457, loss: 2.6363
2022-03-02 16:33:49 - train: epoch 0058, iter [04500, 05004], lr: 0.083457, loss: 2.7010
2022-03-02 16:34:24 - train: epoch 0058, iter [04600, 05004], lr: 0.083457, loss: 2.5594
2022-03-02 16:34:58 - train: epoch 0058, iter [04700, 05004], lr: 0.083457, loss: 2.6944
2022-03-02 16:35:31 - train: epoch 0058, iter [04800, 05004], lr: 0.083457, loss: 2.8095
2022-03-02 16:36:06 - train: epoch 0058, iter [04900, 05004], lr: 0.083457, loss: 2.5080
2022-03-02 16:36:38 - train: epoch 0058, iter [05000, 05004], lr: 0.083457, loss: 2.5724
2022-03-02 16:36:39 - train: epoch 058, train_loss: 2.6770
2022-03-02 16:37:53 - eval: epoch: 058, acc1: 53.608%, acc5: 79.468%, test_loss: 1.9573, per_image_load_time: 1.523ms, per_image_inference_time: 0.501ms
2022-03-02 16:37:53 - until epoch: 058, best_acc1: 55.374%
2022-03-02 16:37:53 - epoch 059 lr: 0.08285358405218655
2022-03-02 16:38:33 - train: epoch 0059, iter [00100, 05004], lr: 0.082854, loss: 2.7061
2022-03-02 16:39:07 - train: epoch 0059, iter [00200, 05004], lr: 0.082854, loss: 2.3640
2022-03-02 16:39:41 - train: epoch 0059, iter [00300, 05004], lr: 0.082854, loss: 2.5301
2022-03-02 16:40:15 - train: epoch 0059, iter [00400, 05004], lr: 0.082854, loss: 2.6216
2022-03-02 16:40:49 - train: epoch 0059, iter [00500, 05004], lr: 0.082854, loss: 2.7873
2022-03-02 16:41:23 - train: epoch 0059, iter [00600, 05004], lr: 0.082854, loss: 2.5459
2022-03-02 16:41:57 - train: epoch 0059, iter [00700, 05004], lr: 0.082854, loss: 2.4952
2022-03-02 16:42:31 - train: epoch 0059, iter [00800, 05004], lr: 0.082854, loss: 2.7938
2022-03-02 16:43:05 - train: epoch 0059, iter [00900, 05004], lr: 0.082854, loss: 2.5641
2022-03-02 16:43:39 - train: epoch 0059, iter [01000, 05004], lr: 0.082854, loss: 2.9877
2022-03-02 16:44:13 - train: epoch 0059, iter [01100, 05004], lr: 0.082854, loss: 2.7520
2022-03-02 16:44:47 - train: epoch 0059, iter [01200, 05004], lr: 0.082854, loss: 2.4679
2022-03-02 16:45:20 - train: epoch 0059, iter [01300, 05004], lr: 0.082854, loss: 2.7251
2022-03-02 16:45:55 - train: epoch 0059, iter [01400, 05004], lr: 0.082854, loss: 2.7560
2022-03-02 16:46:28 - train: epoch 0059, iter [01500, 05004], lr: 0.082854, loss: 2.6572
2022-03-02 16:47:02 - train: epoch 0059, iter [01600, 05004], lr: 0.082854, loss: 2.4897
2022-03-02 16:47:36 - train: epoch 0059, iter [01700, 05004], lr: 0.082854, loss: 2.7613
2022-03-02 16:48:11 - train: epoch 0059, iter [01800, 05004], lr: 0.082854, loss: 2.6382
2022-03-02 16:48:43 - train: epoch 0059, iter [01900, 05004], lr: 0.082854, loss: 2.4724
2022-03-02 16:49:18 - train: epoch 0059, iter [02000, 05004], lr: 0.082854, loss: 2.5949
2022-03-02 16:49:52 - train: epoch 0059, iter [02100, 05004], lr: 0.082854, loss: 2.8860
2022-03-02 16:50:26 - train: epoch 0059, iter [02200, 05004], lr: 0.082854, loss: 2.9399
2022-03-02 16:51:00 - train: epoch 0059, iter [02300, 05004], lr: 0.082854, loss: 2.8488
2022-03-02 16:51:34 - train: epoch 0059, iter [02400, 05004], lr: 0.082854, loss: 2.9978
2022-03-02 16:52:08 - train: epoch 0059, iter [02500, 05004], lr: 0.082854, loss: 2.9405
2022-03-02 16:52:42 - train: epoch 0059, iter [02600, 05004], lr: 0.082854, loss: 2.8480
2022-03-02 16:53:16 - train: epoch 0059, iter [02700, 05004], lr: 0.082854, loss: 2.3887
2022-03-02 16:53:50 - train: epoch 0059, iter [02800, 05004], lr: 0.082854, loss: 2.7706
2022-03-02 16:54:24 - train: epoch 0059, iter [02900, 05004], lr: 0.082854, loss: 2.4735
2022-03-02 16:54:58 - train: epoch 0059, iter [03000, 05004], lr: 0.082854, loss: 2.8079
2022-03-02 16:55:32 - train: epoch 0059, iter [03100, 05004], lr: 0.082854, loss: 2.7261
2022-03-02 16:56:05 - train: epoch 0059, iter [03200, 05004], lr: 0.082854, loss: 2.5233
2022-03-02 16:56:39 - train: epoch 0059, iter [03300, 05004], lr: 0.082854, loss: 2.7587
2022-03-02 16:57:14 - train: epoch 0059, iter [03400, 05004], lr: 0.082854, loss: 3.1458
2022-03-02 16:57:47 - train: epoch 0059, iter [03500, 05004], lr: 0.082854, loss: 2.6579
2022-03-02 16:58:21 - train: epoch 0059, iter [03600, 05004], lr: 0.082854, loss: 2.7440
2022-03-02 16:58:55 - train: epoch 0059, iter [03700, 05004], lr: 0.082854, loss: 2.7191
2022-03-02 16:59:30 - train: epoch 0059, iter [03800, 05004], lr: 0.082854, loss: 2.7569
2022-03-02 17:00:04 - train: epoch 0059, iter [03900, 05004], lr: 0.082854, loss: 2.7048
2022-03-02 17:00:38 - train: epoch 0059, iter [04000, 05004], lr: 0.082854, loss: 2.9560
2022-03-02 17:01:12 - train: epoch 0059, iter [04100, 05004], lr: 0.082854, loss: 2.4335
2022-03-02 17:01:46 - train: epoch 0059, iter [04200, 05004], lr: 0.082854, loss: 2.5258
2022-03-02 17:02:21 - train: epoch 0059, iter [04300, 05004], lr: 0.082854, loss: 2.7279
2022-03-02 17:02:55 - train: epoch 0059, iter [04400, 05004], lr: 0.082854, loss: 2.5029
2022-03-02 17:03:29 - train: epoch 0059, iter [04500, 05004], lr: 0.082854, loss: 2.7857
2022-03-02 17:04:03 - train: epoch 0059, iter [04600, 05004], lr: 0.082854, loss: 2.9364
2022-03-02 17:04:38 - train: epoch 0059, iter [04700, 05004], lr: 0.082854, loss: 2.5926
2022-03-02 17:05:12 - train: epoch 0059, iter [04800, 05004], lr: 0.082854, loss: 2.3373
2022-03-02 17:05:46 - train: epoch 0059, iter [04900, 05004], lr: 0.082854, loss: 2.5287
2022-03-02 17:06:18 - train: epoch 0059, iter [05000, 05004], lr: 0.082854, loss: 3.0030
2022-03-02 17:06:20 - train: epoch 059, train_loss: 2.6734
2022-03-02 17:07:34 - eval: epoch: 059, acc1: 54.456%, acc5: 79.676%, test_loss: 1.9379, per_image_load_time: 0.575ms, per_image_inference_time: 0.486ms
2022-03-02 17:07:35 - until epoch: 059, best_acc1: 55.374%
2022-03-02 17:07:35 - epoch 060 lr: 0.08224211063680853
2022-03-02 17:08:14 - train: epoch 0060, iter [00100, 05004], lr: 0.082242, loss: 2.6435
2022-03-02 17:08:48 - train: epoch 0060, iter [00200, 05004], lr: 0.082242, loss: 2.7907
2022-03-02 17:09:22 - train: epoch 0060, iter [00300, 05004], lr: 0.082242, loss: 2.6275
2022-03-02 17:09:56 - train: epoch 0060, iter [00400, 05004], lr: 0.082242, loss: 2.6715
2022-03-02 17:10:30 - train: epoch 0060, iter [00500, 05004], lr: 0.082242, loss: 2.9405
2022-03-02 17:11:04 - train: epoch 0060, iter [00600, 05004], lr: 0.082242, loss: 2.5956
2022-03-02 17:11:37 - train: epoch 0060, iter [00700, 05004], lr: 0.082242, loss: 2.4976
2022-03-02 17:12:12 - train: epoch 0060, iter [00800, 05004], lr: 0.082242, loss: 2.6576
2022-03-02 17:12:46 - train: epoch 0060, iter [00900, 05004], lr: 0.082242, loss: 2.5467
2022-03-02 17:13:20 - train: epoch 0060, iter [01000, 05004], lr: 0.082242, loss: 2.1891
2022-03-02 17:13:55 - train: epoch 0060, iter [01100, 05004], lr: 0.082242, loss: 2.3662
2022-03-02 17:14:29 - train: epoch 0060, iter [01200, 05004], lr: 0.082242, loss: 2.6289
2022-03-02 17:15:02 - train: epoch 0060, iter [01300, 05004], lr: 0.082242, loss: 2.5081
2022-03-02 17:15:36 - train: epoch 0060, iter [01400, 05004], lr: 0.082242, loss: 2.6433
2022-03-02 17:16:10 - train: epoch 0060, iter [01500, 05004], lr: 0.082242, loss: 2.7813
2022-03-02 17:16:44 - train: epoch 0060, iter [01600, 05004], lr: 0.082242, loss: 2.9983
2022-03-02 17:17:19 - train: epoch 0060, iter [01700, 05004], lr: 0.082242, loss: 2.8993
2022-03-02 17:17:53 - train: epoch 0060, iter [01800, 05004], lr: 0.082242, loss: 2.8889
2022-03-02 17:18:28 - train: epoch 0060, iter [01900, 05004], lr: 0.082242, loss: 2.8454
2022-03-02 17:19:02 - train: epoch 0060, iter [02000, 05004], lr: 0.082242, loss: 2.7045
2022-03-02 17:19:37 - train: epoch 0060, iter [02100, 05004], lr: 0.082242, loss: 2.8768
2022-03-02 17:20:11 - train: epoch 0060, iter [02200, 05004], lr: 0.082242, loss: 3.1261
2022-03-02 17:20:45 - train: epoch 0060, iter [02300, 05004], lr: 0.082242, loss: 2.6262
2022-03-02 17:21:19 - train: epoch 0060, iter [02400, 05004], lr: 0.082242, loss: 2.5189
2022-03-02 17:21:54 - train: epoch 0060, iter [02500, 05004], lr: 0.082242, loss: 2.7379
2022-03-02 17:22:27 - train: epoch 0060, iter [02600, 05004], lr: 0.082242, loss: 2.7547
2022-03-02 17:23:02 - train: epoch 0060, iter [02700, 05004], lr: 0.082242, loss: 2.6235
2022-03-02 17:23:35 - train: epoch 0060, iter [02800, 05004], lr: 0.082242, loss: 2.8125
2022-03-02 17:24:10 - train: epoch 0060, iter [02900, 05004], lr: 0.082242, loss: 2.8117
2022-03-02 17:24:44 - train: epoch 0060, iter [03000, 05004], lr: 0.082242, loss: 2.7780
2022-03-02 17:25:17 - train: epoch 0060, iter [03100, 05004], lr: 0.082242, loss: 2.6251
2022-03-02 17:25:51 - train: epoch 0060, iter [03200, 05004], lr: 0.082242, loss: 2.4762
2022-03-02 17:26:25 - train: epoch 0060, iter [03300, 05004], lr: 0.082242, loss: 2.3987
2022-03-02 17:26:59 - train: epoch 0060, iter [03400, 05004], lr: 0.082242, loss: 2.6914
2022-03-02 17:27:33 - train: epoch 0060, iter [03500, 05004], lr: 0.082242, loss: 2.8007
2022-03-02 17:28:07 - train: epoch 0060, iter [03600, 05004], lr: 0.082242, loss: 2.6412
2022-03-02 17:28:41 - train: epoch 0060, iter [03700, 05004], lr: 0.082242, loss: 2.6386
2022-03-02 17:29:15 - train: epoch 0060, iter [03800, 05004], lr: 0.082242, loss: 2.8222
2022-03-02 17:29:49 - train: epoch 0060, iter [03900, 05004], lr: 0.082242, loss: 2.4654
2022-03-02 17:30:23 - train: epoch 0060, iter [04000, 05004], lr: 0.082242, loss: 2.7637
2022-03-02 17:30:57 - train: epoch 0060, iter [04100, 05004], lr: 0.082242, loss: 2.6225
2022-03-02 17:31:31 - train: epoch 0060, iter [04200, 05004], lr: 0.082242, loss: 2.7225
2022-03-02 17:32:05 - train: epoch 0060, iter [04300, 05004], lr: 0.082242, loss: 2.3424
2022-03-02 17:32:39 - train: epoch 0060, iter [04400, 05004], lr: 0.082242, loss: 2.6425
2022-03-02 17:33:14 - train: epoch 0060, iter [04500, 05004], lr: 0.082242, loss: 2.6372
2022-03-02 17:33:47 - train: epoch 0060, iter [04600, 05004], lr: 0.082242, loss: 2.6601
2022-03-02 17:34:21 - train: epoch 0060, iter [04700, 05004], lr: 0.082242, loss: 2.8840
2022-03-02 17:34:55 - train: epoch 0060, iter [04800, 05004], lr: 0.082242, loss: 2.5177
2022-03-02 17:35:29 - train: epoch 0060, iter [04900, 05004], lr: 0.082242, loss: 2.5745
2022-03-02 17:36:02 - train: epoch 0060, iter [05000, 05004], lr: 0.082242, loss: 2.6563
2022-03-02 17:36:03 - train: epoch 060, train_loss: 2.6700
2022-03-02 17:37:17 - eval: epoch: 060, acc1: 53.168%, acc5: 79.084%, test_loss: 1.9799, per_image_load_time: 1.476ms, per_image_inference_time: 0.518ms
2022-03-02 17:37:18 - until epoch: 060, best_acc1: 55.374%
2022-03-02 17:37:18 - epoch 061 lr: 0.08162226877976886
2022-03-02 17:37:56 - train: epoch 0061, iter [00100, 05004], lr: 0.081622, loss: 2.5831
2022-03-02 17:38:31 - train: epoch 0061, iter [00200, 05004], lr: 0.081622, loss: 2.7754
2022-03-02 17:39:05 - train: epoch 0061, iter [00300, 05004], lr: 0.081622, loss: 2.5297
2022-03-02 17:39:39 - train: epoch 0061, iter [00400, 05004], lr: 0.081622, loss: 2.7393
2022-03-02 17:40:13 - train: epoch 0061, iter [00500, 05004], lr: 0.081622, loss: 2.5862
2022-03-02 17:40:48 - train: epoch 0061, iter [00600, 05004], lr: 0.081622, loss: 2.6422
2022-03-02 17:41:21 - train: epoch 0061, iter [00700, 05004], lr: 0.081622, loss: 2.8398
2022-03-02 17:41:56 - train: epoch 0061, iter [00800, 05004], lr: 0.081622, loss: 2.5775
2022-03-02 17:42:30 - train: epoch 0061, iter [00900, 05004], lr: 0.081622, loss: 2.7288
2022-03-02 17:43:04 - train: epoch 0061, iter [01000, 05004], lr: 0.081622, loss: 2.5575
2022-03-02 17:43:38 - train: epoch 0061, iter [01100, 05004], lr: 0.081622, loss: 2.7208
2022-03-02 17:44:12 - train: epoch 0061, iter [01200, 05004], lr: 0.081622, loss: 2.6175
2022-03-02 17:44:46 - train: epoch 0061, iter [01300, 05004], lr: 0.081622, loss: 2.6089
2022-03-02 17:45:20 - train: epoch 0061, iter [01400, 05004], lr: 0.081622, loss: 2.6272
2022-03-02 17:45:54 - train: epoch 0061, iter [01500, 05004], lr: 0.081622, loss: 2.7167
2022-03-02 17:46:28 - train: epoch 0061, iter [01600, 05004], lr: 0.081622, loss: 2.2094
2022-03-02 17:47:02 - train: epoch 0061, iter [01700, 05004], lr: 0.081622, loss: 3.0135
2022-03-02 17:47:36 - train: epoch 0061, iter [01800, 05004], lr: 0.081622, loss: 2.7003
2022-03-02 17:48:10 - train: epoch 0061, iter [01900, 05004], lr: 0.081622, loss: 2.7833
2022-03-02 17:48:44 - train: epoch 0061, iter [02000, 05004], lr: 0.081622, loss: 2.6417
2022-03-02 17:49:18 - train: epoch 0061, iter [02100, 05004], lr: 0.081622, loss: 2.7643
2022-03-02 17:49:53 - train: epoch 0061, iter [02200, 05004], lr: 0.081622, loss: 2.6378
2022-03-02 17:50:27 - train: epoch 0061, iter [02300, 05004], lr: 0.081622, loss: 2.6772
2022-03-02 17:51:00 - train: epoch 0061, iter [02400, 05004], lr: 0.081622, loss: 2.5974
2022-03-02 17:51:36 - train: epoch 0061, iter [02500, 05004], lr: 0.081622, loss: 2.6205
2022-03-02 17:52:09 - train: epoch 0061, iter [02600, 05004], lr: 0.081622, loss: 2.5493
2022-03-02 17:52:44 - train: epoch 0061, iter [02700, 05004], lr: 0.081622, loss: 2.9274
2022-03-02 17:53:17 - train: epoch 0061, iter [02800, 05004], lr: 0.081622, loss: 2.3585
2022-03-02 17:53:52 - train: epoch 0061, iter [02900, 05004], lr: 0.081622, loss: 2.6783
2022-03-02 17:54:27 - train: epoch 0061, iter [03000, 05004], lr: 0.081622, loss: 2.8684
2022-03-02 17:55:00 - train: epoch 0061, iter [03100, 05004], lr: 0.081622, loss: 3.0275
2022-03-02 17:55:35 - train: epoch 0061, iter [03200, 05004], lr: 0.081622, loss: 2.8385
2022-03-02 17:56:09 - train: epoch 0061, iter [03300, 05004], lr: 0.081622, loss: 2.7830
2022-03-02 17:56:43 - train: epoch 0061, iter [03400, 05004], lr: 0.081622, loss: 2.5128
2022-03-02 17:57:17 - train: epoch 0061, iter [03500, 05004], lr: 0.081622, loss: 3.0509
2022-03-02 17:57:52 - train: epoch 0061, iter [03600, 05004], lr: 0.081622, loss: 2.5426
2022-03-02 17:58:27 - train: epoch 0061, iter [03700, 05004], lr: 0.081622, loss: 2.7962
2022-03-02 17:59:00 - train: epoch 0061, iter [03800, 05004], lr: 0.081622, loss: 2.8136
2022-03-02 17:59:35 - train: epoch 0061, iter [03900, 05004], lr: 0.081622, loss: 2.6421
2022-03-02 18:00:08 - train: epoch 0061, iter [04000, 05004], lr: 0.081622, loss: 2.7633
2022-03-02 18:00:43 - train: epoch 0061, iter [04100, 05004], lr: 0.081622, loss: 2.8478
2022-03-02 18:01:17 - train: epoch 0061, iter [04200, 05004], lr: 0.081622, loss: 2.7515
2022-03-02 18:01:51 - train: epoch 0061, iter [04300, 05004], lr: 0.081622, loss: 2.5522
2022-03-02 18:02:25 - train: epoch 0061, iter [04400, 05004], lr: 0.081622, loss: 2.7349
2022-03-02 18:03:00 - train: epoch 0061, iter [04500, 05004], lr: 0.081622, loss: 2.6045
2022-03-02 18:03:34 - train: epoch 0061, iter [04600, 05004], lr: 0.081622, loss: 2.5519
2022-03-02 18:04:08 - train: epoch 0061, iter [04700, 05004], lr: 0.081622, loss: 2.6790
2022-03-02 18:04:41 - train: epoch 0061, iter [04800, 05004], lr: 0.081622, loss: 2.7782
2022-03-02 18:05:15 - train: epoch 0061, iter [04900, 05004], lr: 0.081622, loss: 2.7013
2022-03-02 18:05:48 - train: epoch 0061, iter [05000, 05004], lr: 0.081622, loss: 2.5930
2022-03-02 18:05:49 - train: epoch 061, train_loss: 2.6648
2022-03-02 18:07:04 - eval: epoch: 061, acc1: 53.502%, acc5: 79.142%, test_loss: 1.9773, per_image_load_time: 0.516ms, per_image_inference_time: 0.487ms
2022-03-02 18:07:04 - until epoch: 061, best_acc1: 55.374%
2022-03-02 18:07:04 - epoch 062 lr: 0.08099421936105702
2022-03-02 18:07:44 - train: epoch 0062, iter [00100, 05004], lr: 0.080994, loss: 2.4230
2022-03-02 18:08:18 - train: epoch 0062, iter [00200, 05004], lr: 0.080994, loss: 2.8620
2022-03-02 18:08:52 - train: epoch 0062, iter [00300, 05004], lr: 0.080994, loss: 2.6815
2022-03-02 18:09:26 - train: epoch 0062, iter [00400, 05004], lr: 0.080994, loss: 2.5798
2022-03-02 18:10:00 - train: epoch 0062, iter [00500, 05004], lr: 0.080994, loss: 2.6163
2022-03-02 18:10:34 - train: epoch 0062, iter [00600, 05004], lr: 0.080994, loss: 2.7261
2022-03-02 18:11:08 - train: epoch 0062, iter [00700, 05004], lr: 0.080994, loss: 2.6301
2022-03-02 18:11:43 - train: epoch 0062, iter [00800, 05004], lr: 0.080994, loss: 2.5896
2022-03-02 18:12:16 - train: epoch 0062, iter [00900, 05004], lr: 0.080994, loss: 2.8731
2022-03-02 18:12:50 - train: epoch 0062, iter [01000, 05004], lr: 0.080994, loss: 2.5934
2022-03-02 18:13:24 - train: epoch 0062, iter [01100, 05004], lr: 0.080994, loss: 2.5033
2022-03-02 18:13:59 - train: epoch 0062, iter [01200, 05004], lr: 0.080994, loss: 2.7856
2022-03-02 18:14:33 - train: epoch 0062, iter [01300, 05004], lr: 0.080994, loss: 2.7451
2022-03-02 18:15:07 - train: epoch 0062, iter [01400, 05004], lr: 0.080994, loss: 2.5274
2022-03-02 18:15:42 - train: epoch 0062, iter [01500, 05004], lr: 0.080994, loss: 2.6190
2022-03-02 18:16:15 - train: epoch 0062, iter [01600, 05004], lr: 0.080994, loss: 2.6776
2022-03-02 18:16:49 - train: epoch 0062, iter [01700, 05004], lr: 0.080994, loss: 2.4606
2022-03-02 18:17:23 - train: epoch 0062, iter [01800, 05004], lr: 0.080994, loss: 2.7025
2022-03-02 18:17:57 - train: epoch 0062, iter [01900, 05004], lr: 0.080994, loss: 2.5759
2022-03-02 18:18:32 - train: epoch 0062, iter [02000, 05004], lr: 0.080994, loss: 2.7879
2022-03-02 18:19:07 - train: epoch 0062, iter [02100, 05004], lr: 0.080994, loss: 2.6336
2022-03-02 18:19:40 - train: epoch 0062, iter [02200, 05004], lr: 0.080994, loss: 2.6045
2022-03-02 18:20:14 - train: epoch 0062, iter [02300, 05004], lr: 0.080994, loss: 2.7623
2022-03-02 18:20:48 - train: epoch 0062, iter [02400, 05004], lr: 0.080994, loss: 2.5940
2022-03-02 18:21:23 - train: epoch 0062, iter [02500, 05004], lr: 0.080994, loss: 2.8775
2022-03-02 18:21:56 - train: epoch 0062, iter [02600, 05004], lr: 0.080994, loss: 2.6337
2022-03-02 18:22:31 - train: epoch 0062, iter [02700, 05004], lr: 0.080994, loss: 2.4967
2022-03-02 18:23:05 - train: epoch 0062, iter [02800, 05004], lr: 0.080994, loss: 2.9376
2022-03-02 18:23:39 - train: epoch 0062, iter [02900, 05004], lr: 0.080994, loss: 2.8104
2022-03-02 18:24:13 - train: epoch 0062, iter [03000, 05004], lr: 0.080994, loss: 2.6663
2022-03-02 18:24:47 - train: epoch 0062, iter [03100, 05004], lr: 0.080994, loss: 2.6953
2022-03-02 18:25:21 - train: epoch 0062, iter [03200, 05004], lr: 0.080994, loss: 2.4957
2022-03-02 18:25:55 - train: epoch 0062, iter [03300, 05004], lr: 0.080994, loss: 2.8498
2022-03-02 18:26:30 - train: epoch 0062, iter [03400, 05004], lr: 0.080994, loss: 2.9191
2022-03-02 18:27:03 - train: epoch 0062, iter [03500, 05004], lr: 0.080994, loss: 2.7956
2022-03-02 18:27:38 - train: epoch 0062, iter [03600, 05004], lr: 0.080994, loss: 2.5869
2022-03-02 18:28:12 - train: epoch 0062, iter [03700, 05004], lr: 0.080994, loss: 2.8741
2022-03-02 18:28:46 - train: epoch 0062, iter [03800, 05004], lr: 0.080994, loss: 2.8792
2022-03-02 18:29:20 - train: epoch 0062, iter [03900, 05004], lr: 0.080994, loss: 2.8496
2022-03-02 18:29:54 - train: epoch 0062, iter [04000, 05004], lr: 0.080994, loss: 2.7187
2022-03-02 18:30:28 - train: epoch 0062, iter [04100, 05004], lr: 0.080994, loss: 2.8456
2022-03-02 18:31:03 - train: epoch 0062, iter [04200, 05004], lr: 0.080994, loss: 2.5387
2022-03-02 18:31:36 - train: epoch 0062, iter [04300, 05004], lr: 0.080994, loss: 2.5841
2022-03-02 18:32:11 - train: epoch 0062, iter [04400, 05004], lr: 0.080994, loss: 2.6434
2022-03-02 18:32:45 - train: epoch 0062, iter [04500, 05004], lr: 0.080994, loss: 2.4620
2022-03-02 18:33:19 - train: epoch 0062, iter [04600, 05004], lr: 0.080994, loss: 2.7786
2022-03-02 18:33:56 - train: epoch 0062, iter [04700, 05004], lr: 0.080994, loss: 2.5517
2022-03-02 18:34:30 - train: epoch 0062, iter [04800, 05004], lr: 0.080994, loss: 2.4655
2022-03-02 18:35:03 - train: epoch 0062, iter [04900, 05004], lr: 0.080994, loss: 2.7211
2022-03-02 18:35:37 - train: epoch 0062, iter [05000, 05004], lr: 0.080994, loss: 2.8845
2022-03-02 18:35:38 - train: epoch 062, train_loss: 2.6590
2022-03-02 18:36:52 - eval: epoch: 062, acc1: 54.998%, acc5: 80.152%, test_loss: 1.8980, per_image_load_time: 1.624ms, per_image_inference_time: 0.491ms
2022-03-02 18:36:53 - until epoch: 062, best_acc1: 55.374%
2022-03-02 18:36:53 - epoch 063 lr: 0.08035812539093556
2022-03-02 18:37:32 - train: epoch 0063, iter [00100, 05004], lr: 0.080358, loss: 2.4716
2022-03-02 18:38:06 - train: epoch 0063, iter [00200, 05004], lr: 0.080358, loss: 2.5812
2022-03-02 18:38:41 - train: epoch 0063, iter [00300, 05004], lr: 0.080358, loss: 2.8411
2022-03-02 18:39:15 - train: epoch 0063, iter [00400, 05004], lr: 0.080358, loss: 2.5694
2022-03-02 18:39:50 - train: epoch 0063, iter [00500, 05004], lr: 0.080358, loss: 2.4609
2022-03-02 18:40:23 - train: epoch 0063, iter [00600, 05004], lr: 0.080358, loss: 2.6358
2022-03-02 18:40:58 - train: epoch 0063, iter [00700, 05004], lr: 0.080358, loss: 2.8379
2022-03-02 18:41:31 - train: epoch 0063, iter [00800, 05004], lr: 0.080358, loss: 2.5657
2022-03-02 18:42:07 - train: epoch 0063, iter [00900, 05004], lr: 0.080358, loss: 2.5497
2022-03-02 18:42:41 - train: epoch 0063, iter [01000, 05004], lr: 0.080358, loss: 2.7972
2022-03-02 18:43:15 - train: epoch 0063, iter [01100, 05004], lr: 0.080358, loss: 2.7483
2022-03-02 18:43:49 - train: epoch 0063, iter [01200, 05004], lr: 0.080358, loss: 2.6011
2022-03-02 18:44:25 - train: epoch 0063, iter [01300, 05004], lr: 0.080358, loss: 2.3268
2022-03-02 18:44:58 - train: epoch 0063, iter [01400, 05004], lr: 0.080358, loss: 2.9913
2022-03-02 18:45:33 - train: epoch 0063, iter [01500, 05004], lr: 0.080358, loss: 2.5928
2022-03-02 18:46:06 - train: epoch 0063, iter [01600, 05004], lr: 0.080358, loss: 3.0696
2022-03-02 18:46:40 - train: epoch 0063, iter [01700, 05004], lr: 0.080358, loss: 2.5781
2022-03-02 18:47:15 - train: epoch 0063, iter [01800, 05004], lr: 0.080358, loss: 2.8168
2022-03-02 18:47:49 - train: epoch 0063, iter [01900, 05004], lr: 0.080358, loss: 2.7634
2022-03-02 18:48:23 - train: epoch 0063, iter [02000, 05004], lr: 0.080358, loss: 2.6925
2022-03-02 18:48:57 - train: epoch 0063, iter [02100, 05004], lr: 0.080358, loss: 2.6816
2022-03-02 18:49:31 - train: epoch 0063, iter [02200, 05004], lr: 0.080358, loss: 2.7186
2022-03-02 18:50:05 - train: epoch 0063, iter [02300, 05004], lr: 0.080358, loss: 2.9871
2022-03-02 18:50:39 - train: epoch 0063, iter [02400, 05004], lr: 0.080358, loss: 2.7952
2022-03-02 18:51:14 - train: epoch 0063, iter [02500, 05004], lr: 0.080358, loss: 2.5143
2022-03-02 18:51:48 - train: epoch 0063, iter [02600, 05004], lr: 0.080358, loss: 2.5105
2022-03-02 18:52:22 - train: epoch 0063, iter [02700, 05004], lr: 0.080358, loss: 2.6946
2022-03-02 18:52:56 - train: epoch 0063, iter [02800, 05004], lr: 0.080358, loss: 2.7861
2022-03-02 18:53:30 - train: epoch 0063, iter [02900, 05004], lr: 0.080358, loss: 2.5642
2022-03-02 18:54:03 - train: epoch 0063, iter [03000, 05004], lr: 0.080358, loss: 2.8423
2022-03-02 18:54:38 - train: epoch 0063, iter [03100, 05004], lr: 0.080358, loss: 2.7875
2022-03-02 18:55:12 - train: epoch 0063, iter [03200, 05004], lr: 0.080358, loss: 2.7717
2022-03-02 18:55:46 - train: epoch 0063, iter [03300, 05004], lr: 0.080358, loss: 2.7468
2022-03-02 18:56:20 - train: epoch 0063, iter [03400, 05004], lr: 0.080358, loss: 2.5967
2022-03-02 18:56:54 - train: epoch 0063, iter [03500, 05004], lr: 0.080358, loss: 2.7892
2022-03-02 18:57:27 - train: epoch 0063, iter [03600, 05004], lr: 0.080358, loss: 2.5969
2022-03-02 18:58:02 - train: epoch 0063, iter [03700, 05004], lr: 0.080358, loss: 2.5134
2022-03-02 18:58:36 - train: epoch 0063, iter [03800, 05004], lr: 0.080358, loss: 2.5810
2022-03-02 18:59:10 - train: epoch 0063, iter [03900, 05004], lr: 0.080358, loss: 2.5950
2022-03-02 18:59:44 - train: epoch 0063, iter [04000, 05004], lr: 0.080358, loss: 2.4110
2022-03-02 19:00:18 - train: epoch 0063, iter [04100, 05004], lr: 0.080358, loss: 2.7122
2022-03-02 19:00:53 - train: epoch 0063, iter [04200, 05004], lr: 0.080358, loss: 2.7494
2022-03-02 19:01:27 - train: epoch 0063, iter [04300, 05004], lr: 0.080358, loss: 2.6946
2022-03-02 19:02:00 - train: epoch 0063, iter [04400, 05004], lr: 0.080358, loss: 2.4453
2022-03-02 19:02:34 - train: epoch 0063, iter [04500, 05004], lr: 0.080358, loss: 2.6252
2022-03-02 19:03:09 - train: epoch 0063, iter [04600, 05004], lr: 0.080358, loss: 2.5904
2022-03-02 19:03:43 - train: epoch 0063, iter [04700, 05004], lr: 0.080358, loss: 2.7218
2022-03-02 19:04:18 - train: epoch 0063, iter [04800, 05004], lr: 0.080358, loss: 2.6098
2022-03-02 19:04:52 - train: epoch 0063, iter [04900, 05004], lr: 0.080358, loss: 2.9577
2022-03-02 19:05:24 - train: epoch 0063, iter [05000, 05004], lr: 0.080358, loss: 2.5661
2022-03-02 19:05:25 - train: epoch 063, train_loss: 2.6541
2022-03-02 19:06:40 - eval: epoch: 063, acc1: 53.844%, acc5: 79.432%, test_loss: 1.9542, per_image_load_time: 0.878ms, per_image_inference_time: 0.487ms
2022-03-02 19:06:41 - until epoch: 063, best_acc1: 55.374%
2022-03-02 19:06:41 - epoch 064 lr: 0.07971415196763088
2022-03-02 19:07:20 - train: epoch 0064, iter [00100, 05004], lr: 0.079714, loss: 2.5262
2022-03-02 19:07:54 - train: epoch 0064, iter [00200, 05004], lr: 0.079714, loss: 2.7392
2022-03-02 19:08:28 - train: epoch 0064, iter [00300, 05004], lr: 0.079714, loss: 2.6000
2022-03-02 19:09:02 - train: epoch 0064, iter [00400, 05004], lr: 0.079714, loss: 2.8853
2022-03-02 19:09:36 - train: epoch 0064, iter [00500, 05004], lr: 0.079714, loss: 2.6111
2022-03-02 19:10:09 - train: epoch 0064, iter [00600, 05004], lr: 0.079714, loss: 2.3934
2022-03-02 19:10:43 - train: epoch 0064, iter [00700, 05004], lr: 0.079714, loss: 2.8369
2022-03-02 19:11:18 - train: epoch 0064, iter [00800, 05004], lr: 0.079714, loss: 2.4858
2022-03-02 19:11:52 - train: epoch 0064, iter [00900, 05004], lr: 0.079714, loss: 2.3424
2022-03-02 19:12:26 - train: epoch 0064, iter [01000, 05004], lr: 0.079714, loss: 2.6712
2022-03-02 19:13:00 - train: epoch 0064, iter [01100, 05004], lr: 0.079714, loss: 2.5544
2022-03-02 19:13:34 - train: epoch 0064, iter [01200, 05004], lr: 0.079714, loss: 2.5529
2022-03-02 19:14:09 - train: epoch 0064, iter [01300, 05004], lr: 0.079714, loss: 2.4554
2022-03-02 19:14:43 - train: epoch 0064, iter [01400, 05004], lr: 0.079714, loss: 2.7155
2022-03-02 19:15:17 - train: epoch 0064, iter [01500, 05004], lr: 0.079714, loss: 2.7591
2022-03-02 19:15:51 - train: epoch 0064, iter [01600, 05004], lr: 0.079714, loss: 2.4745
2022-03-02 19:16:27 - train: epoch 0064, iter [01700, 05004], lr: 0.079714, loss: 2.5851
2022-03-02 19:17:00 - train: epoch 0064, iter [01800, 05004], lr: 0.079714, loss: 2.5509
2022-03-02 19:17:35 - train: epoch 0064, iter [01900, 05004], lr: 0.079714, loss: 2.9199
2022-03-02 19:18:08 - train: epoch 0064, iter [02000, 05004], lr: 0.079714, loss: 2.5083
2022-03-02 19:18:43 - train: epoch 0064, iter [02100, 05004], lr: 0.079714, loss: 2.7710
2022-03-02 19:19:16 - train: epoch 0064, iter [02200, 05004], lr: 0.079714, loss: 2.9753
2022-03-02 19:19:50 - train: epoch 0064, iter [02300, 05004], lr: 0.079714, loss: 2.7219
2022-03-02 19:20:25 - train: epoch 0064, iter [02400, 05004], lr: 0.079714, loss: 2.5775
2022-03-02 19:20:59 - train: epoch 0064, iter [02500, 05004], lr: 0.079714, loss: 2.4057
2022-03-02 19:21:33 - train: epoch 0064, iter [02600, 05004], lr: 0.079714, loss: 2.3948
2022-03-02 19:22:07 - train: epoch 0064, iter [02700, 05004], lr: 0.079714, loss: 2.6321
2022-03-02 19:22:41 - train: epoch 0064, iter [02800, 05004], lr: 0.079714, loss: 2.6680
2022-03-02 19:23:15 - train: epoch 0064, iter [02900, 05004], lr: 0.079714, loss: 2.6274
2022-03-02 19:23:50 - train: epoch 0064, iter [03000, 05004], lr: 0.079714, loss: 2.5485
2022-03-02 19:24:23 - train: epoch 0064, iter [03100, 05004], lr: 0.079714, loss: 2.7749
2022-03-02 19:24:58 - train: epoch 0064, iter [03200, 05004], lr: 0.079714, loss: 2.9799
2022-03-02 19:25:32 - train: epoch 0064, iter [03300, 05004], lr: 0.079714, loss: 2.5804
2022-03-02 19:26:06 - train: epoch 0064, iter [03400, 05004], lr: 0.079714, loss: 2.9384
2022-03-02 19:26:40 - train: epoch 0064, iter [03500, 05004], lr: 0.079714, loss: 2.6025
2022-03-02 19:27:16 - train: epoch 0064, iter [03600, 05004], lr: 0.079714, loss: 2.9110
2022-03-02 19:27:49 - train: epoch 0064, iter [03700, 05004], lr: 0.079714, loss: 2.4535
2022-03-02 19:28:22 - train: epoch 0064, iter [03800, 05004], lr: 0.079714, loss: 2.7156
2022-03-02 19:28:57 - train: epoch 0064, iter [03900, 05004], lr: 0.079714, loss: 2.6193
2022-03-02 19:29:32 - train: epoch 0064, iter [04000, 05004], lr: 0.079714, loss: 2.6176
2022-03-02 19:30:06 - train: epoch 0064, iter [04100, 05004], lr: 0.079714, loss: 2.6909
2022-03-02 19:30:40 - train: epoch 0064, iter [04200, 05004], lr: 0.079714, loss: 2.6135
2022-03-02 19:31:14 - train: epoch 0064, iter [04300, 05004], lr: 0.079714, loss: 2.7079
2022-03-02 19:31:48 - train: epoch 0064, iter [04400, 05004], lr: 0.079714, loss: 2.7213
2022-03-02 19:32:23 - train: epoch 0064, iter [04500, 05004], lr: 0.079714, loss: 2.6986
2022-03-02 19:32:57 - train: epoch 0064, iter [04600, 05004], lr: 0.079714, loss: 2.7138
2022-03-02 19:33:32 - train: epoch 0064, iter [04700, 05004], lr: 0.079714, loss: 2.9330
2022-03-02 19:34:07 - train: epoch 0064, iter [04800, 05004], lr: 0.079714, loss: 2.6810
2022-03-02 19:34:41 - train: epoch 0064, iter [04900, 05004], lr: 0.079714, loss: 2.7689
2022-03-02 19:35:14 - train: epoch 0064, iter [05000, 05004], lr: 0.079714, loss: 2.4851
2022-03-02 19:35:15 - train: epoch 064, train_loss: 2.6503
2022-03-02 19:36:29 - eval: epoch: 064, acc1: 56.038%, acc5: 81.040%, test_loss: 1.8575, per_image_load_time: 1.236ms, per_image_inference_time: 0.501ms
2022-03-02 19:36:30 - until epoch: 064, best_acc1: 56.038%
2022-03-02 19:36:30 - epoch 065 lr: 0.07906246623448183
2022-03-02 19:37:09 - train: epoch 0065, iter [00100, 05004], lr: 0.079062, loss: 2.7469
2022-03-02 19:37:43 - train: epoch 0065, iter [00200, 05004], lr: 0.079062, loss: 2.6480
2022-03-02 19:38:16 - train: epoch 0065, iter [00300, 05004], lr: 0.079062, loss: 2.6660
2022-03-02 19:38:50 - train: epoch 0065, iter [00400, 05004], lr: 0.079062, loss: 2.6109
2022-03-02 19:39:25 - train: epoch 0065, iter [00500, 05004], lr: 0.079062, loss: 2.6594
2022-03-02 19:39:59 - train: epoch 0065, iter [00600, 05004], lr: 0.079062, loss: 2.5094
2022-03-02 19:40:33 - train: epoch 0065, iter [00700, 05004], lr: 0.079062, loss: 2.9399
2022-03-02 19:41:07 - train: epoch 0065, iter [00800, 05004], lr: 0.079062, loss: 2.5820
2022-03-02 19:41:41 - train: epoch 0065, iter [00900, 05004], lr: 0.079062, loss: 2.6612
2022-03-02 19:42:15 - train: epoch 0065, iter [01000, 05004], lr: 0.079062, loss: 2.6629
2022-03-02 19:42:49 - train: epoch 0065, iter [01100, 05004], lr: 0.079062, loss: 2.7398
2022-03-02 19:43:22 - train: epoch 0065, iter [01200, 05004], lr: 0.079062, loss: 3.0572
2022-03-02 19:43:55 - train: epoch 0065, iter [01300, 05004], lr: 0.079062, loss: 2.5814
2022-03-02 19:44:29 - train: epoch 0065, iter [01400, 05004], lr: 0.079062, loss: 2.7245
2022-03-02 19:45:03 - train: epoch 0065, iter [01500, 05004], lr: 0.079062, loss: 2.6122
2022-03-02 19:45:37 - train: epoch 0065, iter [01600, 05004], lr: 0.079062, loss: 2.9082
2022-03-02 19:46:10 - train: epoch 0065, iter [01700, 05004], lr: 0.079062, loss: 2.4042
2022-03-02 19:46:44 - train: epoch 0065, iter [01800, 05004], lr: 0.079062, loss: 2.5608
2022-03-02 19:47:16 - train: epoch 0065, iter [01900, 05004], lr: 0.079062, loss: 2.6598
2022-03-02 19:47:50 - train: epoch 0065, iter [02000, 05004], lr: 0.079062, loss: 2.6090
2022-03-02 19:48:25 - train: epoch 0065, iter [02100, 05004], lr: 0.079062, loss: 2.7710
2022-03-02 19:49:00 - train: epoch 0065, iter [02200, 05004], lr: 0.079062, loss: 2.5164
2022-03-02 19:49:40 - train: epoch 0065, iter [02300, 05004], lr: 0.079062, loss: 2.5119
2022-03-02 19:50:21 - train: epoch 0065, iter [02400, 05004], lr: 0.079062, loss: 2.6708
2022-03-02 19:50:58 - train: epoch 0065, iter [02500, 05004], lr: 0.079062, loss: 2.7481
2022-03-02 19:51:38 - train: epoch 0065, iter [02600, 05004], lr: 0.079062, loss: 2.8316
2022-03-02 19:52:14 - train: epoch 0065, iter [02700, 05004], lr: 0.079062, loss: 2.6889
2022-03-02 19:52:47 - train: epoch 0065, iter [02800, 05004], lr: 0.079062, loss: 2.7987
2022-03-02 19:53:20 - train: epoch 0065, iter [02900, 05004], lr: 0.079062, loss: 2.4922
2022-03-02 19:53:54 - train: epoch 0065, iter [03000, 05004], lr: 0.079062, loss: 2.5355
2022-03-02 19:54:28 - train: epoch 0065, iter [03100, 05004], lr: 0.079062, loss: 2.6727
2022-03-02 19:55:01 - train: epoch 0065, iter [03200, 05004], lr: 0.079062, loss: 2.6604
2022-03-02 19:55:34 - train: epoch 0065, iter [03300, 05004], lr: 0.079062, loss: 2.6968
2022-03-02 19:56:08 - train: epoch 0065, iter [03400, 05004], lr: 0.079062, loss: 2.5839
2022-03-02 19:56:41 - train: epoch 0065, iter [03500, 05004], lr: 0.079062, loss: 2.7232
2022-03-02 19:57:14 - train: epoch 0065, iter [03600, 05004], lr: 0.079062, loss: 2.6450
2022-03-02 19:57:47 - train: epoch 0065, iter [03700, 05004], lr: 0.079062, loss: 2.5140
2022-03-02 19:58:20 - train: epoch 0065, iter [03800, 05004], lr: 0.079062, loss: 2.4765
2022-03-02 19:58:54 - train: epoch 0065, iter [03900, 05004], lr: 0.079062, loss: 3.0113
2022-03-02 19:59:33 - train: epoch 0065, iter [04000, 05004], lr: 0.079062, loss: 2.7457
2022-03-02 20:00:13 - train: epoch 0065, iter [04100, 05004], lr: 0.079062, loss: 2.5251
2022-03-02 20:00:52 - train: epoch 0065, iter [04200, 05004], lr: 0.079062, loss: 2.5922
2022-03-02 20:01:38 - train: epoch 0065, iter [04300, 05004], lr: 0.079062, loss: 2.5820
2022-03-02 20:02:17 - train: epoch 0065, iter [04400, 05004], lr: 0.079062, loss: 2.7835
2022-03-02 20:02:53 - train: epoch 0065, iter [04500, 05004], lr: 0.079062, loss: 2.7515
2022-03-02 20:03:40 - train: epoch 0065, iter [04600, 05004], lr: 0.079062, loss: 2.7801
2022-03-02 20:04:13 - train: epoch 0065, iter [04700, 05004], lr: 0.079062, loss: 2.4906
2022-03-02 20:04:46 - train: epoch 0065, iter [04800, 05004], lr: 0.079062, loss: 2.3653
2022-03-02 20:05:20 - train: epoch 0065, iter [04900, 05004], lr: 0.079062, loss: 2.7509
2022-03-02 20:05:52 - train: epoch 0065, iter [05000, 05004], lr: 0.079062, loss: 2.7532
2022-03-02 20:05:53 - train: epoch 065, train_loss: 2.6470
2022-03-02 20:07:06 - eval: epoch: 065, acc1: 55.742%, acc5: 80.874%, test_loss: 1.8789, per_image_load_time: 0.540ms, per_image_inference_time: 0.454ms
2022-03-02 20:07:07 - until epoch: 065, best_acc1: 56.038%
2022-03-02 20:07:07 - epoch 066 lr: 0.0784032373365578
2022-03-02 20:07:45 - train: epoch 0066, iter [00100, 05004], lr: 0.078403, loss: 2.6199
2022-03-02 20:08:18 - train: epoch 0066, iter [00200, 05004], lr: 0.078403, loss: 3.0214
2022-03-02 20:08:52 - train: epoch 0066, iter [00300, 05004], lr: 0.078403, loss: 2.5176
2022-03-02 20:09:25 - train: epoch 0066, iter [00400, 05004], lr: 0.078403, loss: 2.3597
2022-03-02 20:09:58 - train: epoch 0066, iter [00500, 05004], lr: 0.078403, loss: 2.5739
2022-03-02 20:10:32 - train: epoch 0066, iter [00600, 05004], lr: 0.078403, loss: 2.5388
2022-03-02 20:11:05 - train: epoch 0066, iter [00700, 05004], lr: 0.078403, loss: 2.7197
2022-03-02 20:11:38 - train: epoch 0066, iter [00800, 05004], lr: 0.078403, loss: 2.7906
2022-03-02 20:12:12 - train: epoch 0066, iter [00900, 05004], lr: 0.078403, loss: 2.5450
2022-03-02 20:12:46 - train: epoch 0066, iter [01000, 05004], lr: 0.078403, loss: 2.7964
2022-03-02 20:13:19 - train: epoch 0066, iter [01100, 05004], lr: 0.078403, loss: 2.5322
2022-03-02 20:13:52 - train: epoch 0066, iter [01200, 05004], lr: 0.078403, loss: 2.7092
2022-03-02 20:14:26 - train: epoch 0066, iter [01300, 05004], lr: 0.078403, loss: 2.5985
2022-03-02 20:15:00 - train: epoch 0066, iter [01400, 05004], lr: 0.078403, loss: 2.4274
2022-03-02 20:15:33 - train: epoch 0066, iter [01500, 05004], lr: 0.078403, loss: 2.8113
2022-03-02 20:16:06 - train: epoch 0066, iter [01600, 05004], lr: 0.078403, loss: 2.5800
2022-03-02 20:16:40 - train: epoch 0066, iter [01700, 05004], lr: 0.078403, loss: 2.8255
2022-03-02 20:17:13 - train: epoch 0066, iter [01800, 05004], lr: 0.078403, loss: 2.4837
2022-03-02 20:17:47 - train: epoch 0066, iter [01900, 05004], lr: 0.078403, loss: 2.7713
2022-03-02 20:18:20 - train: epoch 0066, iter [02000, 05004], lr: 0.078403, loss: 2.3871
2022-03-02 20:18:54 - train: epoch 0066, iter [02100, 05004], lr: 0.078403, loss: 2.8771
2022-03-02 20:19:27 - train: epoch 0066, iter [02200, 05004], lr: 0.078403, loss: 2.6635
2022-03-02 20:20:01 - train: epoch 0066, iter [02300, 05004], lr: 0.078403, loss: 2.8752
2022-03-02 20:20:34 - train: epoch 0066, iter [02400, 05004], lr: 0.078403, loss: 2.6083
2022-03-02 20:21:07 - train: epoch 0066, iter [02500, 05004], lr: 0.078403, loss: 2.8183
2022-03-02 20:21:40 - train: epoch 0066, iter [02600, 05004], lr: 0.078403, loss: 2.5618
2022-03-02 20:22:14 - train: epoch 0066, iter [02700, 05004], lr: 0.078403, loss: 2.6308
2022-03-02 20:22:47 - train: epoch 0066, iter [02800, 05004], lr: 0.078403, loss: 2.8019
2022-03-02 20:23:21 - train: epoch 0066, iter [02900, 05004], lr: 0.078403, loss: 2.6033
2022-03-02 20:23:54 - train: epoch 0066, iter [03000, 05004], lr: 0.078403, loss: 2.6603
2022-03-02 20:24:27 - train: epoch 0066, iter [03100, 05004], lr: 0.078403, loss: 2.9711
2022-03-02 20:25:00 - train: epoch 0066, iter [03200, 05004], lr: 0.078403, loss: 2.8189
2022-03-02 20:25:34 - train: epoch 0066, iter [03300, 05004], lr: 0.078403, loss: 2.5090
2022-03-02 20:26:07 - train: epoch 0066, iter [03400, 05004], lr: 0.078403, loss: 2.9927
2022-03-02 20:26:41 - train: epoch 0066, iter [03500, 05004], lr: 0.078403, loss: 2.7730
2022-03-02 20:27:14 - train: epoch 0066, iter [03600, 05004], lr: 0.078403, loss: 2.5148
2022-03-02 20:27:47 - train: epoch 0066, iter [03700, 05004], lr: 0.078403, loss: 2.5587
2022-03-02 20:28:20 - train: epoch 0066, iter [03800, 05004], lr: 0.078403, loss: 2.9950
2022-03-02 20:28:53 - train: epoch 0066, iter [03900, 05004], lr: 0.078403, loss: 2.6524
2022-03-02 20:29:25 - train: epoch 0066, iter [04000, 05004], lr: 0.078403, loss: 2.6071
2022-03-02 20:29:59 - train: epoch 0066, iter [04100, 05004], lr: 0.078403, loss: 2.6186
2022-03-02 20:30:32 - train: epoch 0066, iter [04200, 05004], lr: 0.078403, loss: 2.5179
2022-03-02 20:31:06 - train: epoch 0066, iter [04300, 05004], lr: 0.078403, loss: 2.4442
2022-03-02 20:31:42 - train: epoch 0066, iter [04400, 05004], lr: 0.078403, loss: 2.6151
2022-03-02 20:32:19 - train: epoch 0066, iter [04500, 05004], lr: 0.078403, loss: 2.8826
2022-03-02 20:33:06 - train: epoch 0066, iter [04600, 05004], lr: 0.078403, loss: 2.7403
2022-03-02 20:33:45 - train: epoch 0066, iter [04700, 05004], lr: 0.078403, loss: 2.5910
2022-03-02 20:34:32 - train: epoch 0066, iter [04800, 05004], lr: 0.078403, loss: 2.5562
2022-03-02 20:35:09 - train: epoch 0066, iter [04900, 05004], lr: 0.078403, loss: 2.5238
2022-03-02 20:35:41 - train: epoch 0066, iter [05000, 05004], lr: 0.078403, loss: 2.6384
2022-03-02 20:35:42 - train: epoch 066, train_loss: 2.6417
2022-03-02 20:36:54 - eval: epoch: 066, acc1: 55.746%, acc5: 80.946%, test_loss: 1.8665, per_image_load_time: 0.960ms, per_image_inference_time: 0.435ms
2022-03-02 20:36:54 - until epoch: 066, best_acc1: 56.038%
2022-03-02 20:36:54 - epoch 067 lr: 0.07773663637675694
2022-03-02 20:37:32 - train: epoch 0067, iter [00100, 05004], lr: 0.077737, loss: 2.5135
2022-03-02 20:38:05 - train: epoch 0067, iter [00200, 05004], lr: 0.077737, loss: 2.7730
2022-03-02 20:38:37 - train: epoch 0067, iter [00300, 05004], lr: 0.077737, loss: 2.4647
2022-03-02 20:39:10 - train: epoch 0067, iter [00400, 05004], lr: 0.077737, loss: 2.8208
2022-03-02 20:39:42 - train: epoch 0067, iter [00500, 05004], lr: 0.077737, loss: 2.5000
2022-03-02 20:40:16 - train: epoch 0067, iter [00600, 05004], lr: 0.077737, loss: 2.5962
2022-03-02 20:40:49 - train: epoch 0067, iter [00700, 05004], lr: 0.077737, loss: 2.4362
2022-03-02 20:41:22 - train: epoch 0067, iter [00800, 05004], lr: 0.077737, loss: 2.7488
2022-03-02 20:41:54 - train: epoch 0067, iter [00900, 05004], lr: 0.077737, loss: 2.8071
2022-03-02 20:42:26 - train: epoch 0067, iter [01000, 05004], lr: 0.077737, loss: 2.4415
2022-03-02 20:42:59 - train: epoch 0067, iter [01100, 05004], lr: 0.077737, loss: 2.7061
2022-03-02 20:43:32 - train: epoch 0067, iter [01200, 05004], lr: 0.077737, loss: 2.5143
2022-03-02 20:44:05 - train: epoch 0067, iter [01300, 05004], lr: 0.077737, loss: 2.8243
2022-03-02 20:44:37 - train: epoch 0067, iter [01400, 05004], lr: 0.077737, loss: 2.7638
2022-03-02 20:45:10 - train: epoch 0067, iter [01500, 05004], lr: 0.077737, loss: 2.5054
2022-03-02 20:45:43 - train: epoch 0067, iter [01600, 05004], lr: 0.077737, loss: 2.8469
2022-03-02 20:46:16 - train: epoch 0067, iter [01700, 05004], lr: 0.077737, loss: 2.6380
2022-03-02 20:46:48 - train: epoch 0067, iter [01800, 05004], lr: 0.077737, loss: 3.0144
2022-03-02 20:47:21 - train: epoch 0067, iter [01900, 05004], lr: 0.077737, loss: 2.7245
2022-03-02 20:47:54 - train: epoch 0067, iter [02000, 05004], lr: 0.077737, loss: 2.7661
2022-03-02 20:48:26 - train: epoch 0067, iter [02100, 05004], lr: 0.077737, loss: 2.5569
2022-03-02 20:48:58 - train: epoch 0067, iter [02200, 05004], lr: 0.077737, loss: 2.4662
2022-03-02 20:49:31 - train: epoch 0067, iter [02300, 05004], lr: 0.077737, loss: 2.4702
2022-03-02 20:50:05 - train: epoch 0067, iter [02400, 05004], lr: 0.077737, loss: 2.5797
2022-03-02 20:50:37 - train: epoch 0067, iter [02500, 05004], lr: 0.077737, loss: 2.7050
2022-03-02 20:51:11 - train: epoch 0067, iter [02600, 05004], lr: 0.077737, loss: 2.6477
2022-03-02 20:51:42 - train: epoch 0067, iter [02700, 05004], lr: 0.077737, loss: 2.6123
2022-03-02 20:52:16 - train: epoch 0067, iter [02800, 05004], lr: 0.077737, loss: 2.6399
2022-03-02 20:52:50 - train: epoch 0067, iter [02900, 05004], lr: 0.077737, loss: 2.6936
2022-03-02 20:53:25 - train: epoch 0067, iter [03000, 05004], lr: 0.077737, loss: 2.3115
2022-03-02 20:54:01 - train: epoch 0067, iter [03100, 05004], lr: 0.077737, loss: 2.8065
2022-03-02 20:54:43 - train: epoch 0067, iter [03200, 05004], lr: 0.077737, loss: 2.6462
2022-03-02 20:55:18 - train: epoch 0067, iter [03300, 05004], lr: 0.077737, loss: 2.7548
2022-03-02 20:55:52 - train: epoch 0067, iter [03400, 05004], lr: 0.077737, loss: 2.6287
2022-03-02 20:56:26 - train: epoch 0067, iter [03500, 05004], lr: 0.077737, loss: 2.6430
2022-03-02 20:57:03 - train: epoch 0067, iter [03600, 05004], lr: 0.077737, loss: 2.6783
2022-03-02 20:57:36 - train: epoch 0067, iter [03700, 05004], lr: 0.077737, loss: 2.6035
2022-03-02 20:58:11 - train: epoch 0067, iter [03800, 05004], lr: 0.077737, loss: 2.6051
2022-03-02 20:58:51 - train: epoch 0067, iter [03900, 05004], lr: 0.077737, loss: 2.6484
2022-03-02 20:59:26 - train: epoch 0067, iter [04000, 05004], lr: 0.077737, loss: 2.4896
2022-03-02 21:00:00 - train: epoch 0067, iter [04100, 05004], lr: 0.077737, loss: 2.6465
2022-03-02 21:00:33 - train: epoch 0067, iter [04200, 05004], lr: 0.077737, loss: 2.4454
2022-03-02 21:01:06 - train: epoch 0067, iter [04300, 05004], lr: 0.077737, loss: 2.6239
2022-03-02 21:01:39 - train: epoch 0067, iter [04400, 05004], lr: 0.077737, loss: 2.5456
2022-03-02 21:02:12 - train: epoch 0067, iter [04500, 05004], lr: 0.077737, loss: 2.5893
2022-03-02 21:02:47 - train: epoch 0067, iter [04600, 05004], lr: 0.077737, loss: 2.6209
2022-03-02 21:03:25 - train: epoch 0067, iter [04700, 05004], lr: 0.077737, loss: 2.9241
2022-03-02 21:03:58 - train: epoch 0067, iter [04800, 05004], lr: 0.077737, loss: 2.6006
2022-03-02 21:04:34 - train: epoch 0067, iter [04900, 05004], lr: 0.077737, loss: 2.7394
2022-03-02 21:05:06 - train: epoch 0067, iter [05000, 05004], lr: 0.077737, loss: 2.6446
2022-03-02 21:05:07 - train: epoch 067, train_loss: 2.6379
2022-03-02 21:06:19 - eval: epoch: 067, acc1: 54.332%, acc5: 79.786%, test_loss: 1.9353, per_image_load_time: 1.476ms, per_image_inference_time: 0.438ms
2022-03-02 21:06:19 - until epoch: 067, best_acc1: 56.038%
2022-03-02 21:06:19 - epoch 068 lr: 0.07706283637139658
2022-03-02 21:06:57 - train: epoch 0068, iter [00100, 05004], lr: 0.077063, loss: 2.4947
2022-03-02 21:07:30 - train: epoch 0068, iter [00200, 05004], lr: 0.077063, loss: 2.6181
2022-03-02 21:08:03 - train: epoch 0068, iter [00300, 05004], lr: 0.077063, loss: 2.5614
2022-03-02 21:08:36 - train: epoch 0068, iter [00400, 05004], lr: 0.077063, loss: 2.6362
2022-03-02 21:09:07 - train: epoch 0068, iter [00500, 05004], lr: 0.077063, loss: 2.7690
2022-03-02 21:09:40 - train: epoch 0068, iter [00600, 05004], lr: 0.077063, loss: 2.8218
2022-03-02 21:10:12 - train: epoch 0068, iter [00700, 05004], lr: 0.077063, loss: 2.5490
2022-03-02 21:10:45 - train: epoch 0068, iter [00800, 05004], lr: 0.077063, loss: 2.4778
2022-03-02 21:11:17 - train: epoch 0068, iter [00900, 05004], lr: 0.077063, loss: 2.6252
2022-03-02 21:11:50 - train: epoch 0068, iter [01000, 05004], lr: 0.077063, loss: 2.7861
2022-03-02 21:12:23 - train: epoch 0068, iter [01100, 05004], lr: 0.077063, loss: 2.7888
2022-03-02 21:12:55 - train: epoch 0068, iter [01200, 05004], lr: 0.077063, loss: 2.7820
2022-03-02 21:13:28 - train: epoch 0068, iter [01300, 05004], lr: 0.077063, loss: 2.6179
2022-03-02 21:14:01 - train: epoch 0068, iter [01400, 05004], lr: 0.077063, loss: 2.6578
2022-03-02 21:14:34 - train: epoch 0068, iter [01500, 05004], lr: 0.077063, loss: 2.7470
2022-03-02 21:15:06 - train: epoch 0068, iter [01600, 05004], lr: 0.077063, loss: 2.8898
2022-03-02 21:15:38 - train: epoch 0068, iter [01700, 05004], lr: 0.077063, loss: 2.5915
2022-03-02 21:16:11 - train: epoch 0068, iter [01800, 05004], lr: 0.077063, loss: 2.4658
2022-03-02 21:16:44 - train: epoch 0068, iter [01900, 05004], lr: 0.077063, loss: 2.6774
2022-03-02 21:17:16 - train: epoch 0068, iter [02000, 05004], lr: 0.077063, loss: 2.6883
2022-03-02 21:17:49 - train: epoch 0068, iter [02100, 05004], lr: 0.077063, loss: 2.5399
2022-03-02 21:18:23 - train: epoch 0068, iter [02200, 05004], lr: 0.077063, loss: 2.5961
2022-03-02 21:18:57 - train: epoch 0068, iter [02300, 05004], lr: 0.077063, loss: 2.4836
2022-03-02 21:19:31 - train: epoch 0068, iter [02400, 05004], lr: 0.077063, loss: 2.7215
2022-03-02 21:20:07 - train: epoch 0068, iter [02500, 05004], lr: 0.077063, loss: 2.3503
2022-03-02 21:20:40 - train: epoch 0068, iter [02600, 05004], lr: 0.077063, loss: 2.6372
2022-03-02 21:21:12 - train: epoch 0068, iter [02700, 05004], lr: 0.077063, loss: 2.4559
2022-03-02 21:21:48 - train: epoch 0068, iter [02800, 05004], lr: 0.077063, loss: 2.7910
2022-03-02 21:22:28 - train: epoch 0068, iter [02900, 05004], lr: 0.077063, loss: 2.8579
2022-03-02 21:23:01 - train: epoch 0068, iter [03000, 05004], lr: 0.077063, loss: 2.7754
2022-03-02 21:23:41 - train: epoch 0068, iter [03100, 05004], lr: 0.077063, loss: 2.4705
2022-03-02 21:24:20 - train: epoch 0068, iter [03200, 05004], lr: 0.077063, loss: 2.5998
2022-03-02 21:24:58 - train: epoch 0068, iter [03300, 05004], lr: 0.077063, loss: 2.7803
2022-03-02 21:25:32 - train: epoch 0068, iter [03400, 05004], lr: 0.077063, loss: 2.4786
2022-03-02 21:26:07 - train: epoch 0068, iter [03500, 05004], lr: 0.077063, loss: 2.8656
2022-03-02 21:26:40 - train: epoch 0068, iter [03600, 05004], lr: 0.077063, loss: 2.7080
2022-03-02 21:27:17 - train: epoch 0068, iter [03700, 05004], lr: 0.077063, loss: 2.7223
2022-03-02 21:27:49 - train: epoch 0068, iter [03800, 05004], lr: 0.077063, loss: 2.6058
2022-03-02 21:28:22 - train: epoch 0068, iter [03900, 05004], lr: 0.077063, loss: 2.7428
2022-03-02 21:28:59 - train: epoch 0068, iter [04000, 05004], lr: 0.077063, loss: 2.8111
2022-03-02 21:29:37 - train: epoch 0068, iter [04100, 05004], lr: 0.077063, loss: 2.6295
2022-03-02 21:30:14 - train: epoch 0068, iter [04200, 05004], lr: 0.077063, loss: 2.3677
2022-03-02 21:30:47 - train: epoch 0068, iter [04300, 05004], lr: 0.077063, loss: 2.7006
2022-03-02 21:31:21 - train: epoch 0068, iter [04400, 05004], lr: 0.077063, loss: 2.8240
2022-03-02 21:31:54 - train: epoch 0068, iter [04500, 05004], lr: 0.077063, loss: 2.3652
2022-03-02 21:32:26 - train: epoch 0068, iter [04600, 05004], lr: 0.077063, loss: 2.6958
2022-03-02 21:33:02 - train: epoch 0068, iter [04700, 05004], lr: 0.077063, loss: 2.6638
2022-03-02 21:33:35 - train: epoch 0068, iter [04800, 05004], lr: 0.077063, loss: 2.8732
2022-03-02 21:34:11 - train: epoch 0068, iter [04900, 05004], lr: 0.077063, loss: 2.6984
2022-03-02 21:34:48 - train: epoch 0068, iter [05000, 05004], lr: 0.077063, loss: 2.5364
2022-03-02 21:34:49 - train: epoch 068, train_loss: 2.6338
2022-03-02 21:36:01 - eval: epoch: 068, acc1: 54.626%, acc5: 79.990%, test_loss: 1.9131, per_image_load_time: 1.746ms, per_image_inference_time: 0.441ms
2022-03-02 21:36:01 - until epoch: 068, best_acc1: 56.038%
2022-03-02 21:36:01 - epoch 069 lr: 0.07638201220530665
2022-03-02 21:36:39 - train: epoch 0069, iter [00100, 05004], lr: 0.076382, loss: 2.7975
2022-03-02 21:37:12 - train: epoch 0069, iter [00200, 05004], lr: 0.076382, loss: 2.7227
2022-03-02 21:37:45 - train: epoch 0069, iter [00300, 05004], lr: 0.076382, loss: 2.6068
2022-03-02 21:38:16 - train: epoch 0069, iter [00400, 05004], lr: 0.076382, loss: 2.5142
2022-03-02 21:38:50 - train: epoch 0069, iter [00500, 05004], lr: 0.076382, loss: 2.8182
2022-03-02 21:39:22 - train: epoch 0069, iter [00600, 05004], lr: 0.076382, loss: 2.2687
2022-03-02 21:39:55 - train: epoch 0069, iter [00700, 05004], lr: 0.076382, loss: 2.9290
2022-03-02 21:40:28 - train: epoch 0069, iter [00800, 05004], lr: 0.076382, loss: 2.6145
2022-03-02 21:41:01 - train: epoch 0069, iter [00900, 05004], lr: 0.076382, loss: 2.5553
2022-03-02 21:41:34 - train: epoch 0069, iter [01000, 05004], lr: 0.076382, loss: 2.6332
2022-03-02 21:42:06 - train: epoch 0069, iter [01100, 05004], lr: 0.076382, loss: 2.4890
2022-03-02 21:42:39 - train: epoch 0069, iter [01200, 05004], lr: 0.076382, loss: 2.6178
2022-03-02 21:43:11 - train: epoch 0069, iter [01300, 05004], lr: 0.076382, loss: 2.4538
2022-03-02 21:43:44 - train: epoch 0069, iter [01400, 05004], lr: 0.076382, loss: 2.2565
2022-03-02 21:44:17 - train: epoch 0069, iter [01500, 05004], lr: 0.076382, loss: 2.7987
2022-03-02 21:44:50 - train: epoch 0069, iter [01600, 05004], lr: 0.076382, loss: 2.5796
2022-03-02 21:45:23 - train: epoch 0069, iter [01700, 05004], lr: 0.076382, loss: 2.5335
2022-03-02 21:45:55 - train: epoch 0069, iter [01800, 05004], lr: 0.076382, loss: 2.4770
2022-03-02 21:46:29 - train: epoch 0069, iter [01900, 05004], lr: 0.076382, loss: 2.6405
2022-03-02 21:47:00 - train: epoch 0069, iter [02000, 05004], lr: 0.076382, loss: 2.2828
2022-03-02 21:47:33 - train: epoch 0069, iter [02100, 05004], lr: 0.076382, loss: 2.6163
2022-03-02 21:48:06 - train: epoch 0069, iter [02200, 05004], lr: 0.076382, loss: 2.7482
2022-03-02 21:48:39 - train: epoch 0069, iter [02300, 05004], lr: 0.076382, loss: 2.4266
2022-03-02 21:49:11 - train: epoch 0069, iter [02400, 05004], lr: 0.076382, loss: 2.7034
2022-03-02 21:49:44 - train: epoch 0069, iter [02500, 05004], lr: 0.076382, loss: 2.4628
2022-03-02 21:50:18 - train: epoch 0069, iter [02600, 05004], lr: 0.076382, loss: 2.6413
2022-03-02 21:50:52 - train: epoch 0069, iter [02700, 05004], lr: 0.076382, loss: 2.8105
2022-03-02 21:51:24 - train: epoch 0069, iter [02800, 05004], lr: 0.076382, loss: 2.6917
2022-03-02 21:51:58 - train: epoch 0069, iter [02900, 05004], lr: 0.076382, loss: 2.4594
2022-03-02 21:52:33 - train: epoch 0069, iter [03000, 05004], lr: 0.076382, loss: 2.6257
2022-03-02 21:53:07 - train: epoch 0069, iter [03100, 05004], lr: 0.076382, loss: 2.7545
2022-03-02 21:53:50 - train: epoch 0069, iter [03200, 05004], lr: 0.076382, loss: 2.6175
2022-03-02 21:54:27 - train: epoch 0069, iter [03300, 05004], lr: 0.076382, loss: 2.7695
2022-03-02 21:55:01 - train: epoch 0069, iter [03400, 05004], lr: 0.076382, loss: 2.7108
2022-03-02 21:55:34 - train: epoch 0069, iter [03500, 05004], lr: 0.076382, loss: 2.5285
2022-03-02 21:56:14 - train: epoch 0069, iter [03600, 05004], lr: 0.076382, loss: 2.2405
2022-03-02 21:56:48 - train: epoch 0069, iter [03700, 05004], lr: 0.076382, loss: 2.7207
2022-03-02 21:57:22 - train: epoch 0069, iter [03800, 05004], lr: 0.076382, loss: 2.5397
2022-03-02 21:57:57 - train: epoch 0069, iter [03900, 05004], lr: 0.076382, loss: 2.8732
2022-03-02 21:58:30 - train: epoch 0069, iter [04000, 05004], lr: 0.076382, loss: 2.5313
2022-03-02 21:59:03 - train: epoch 0069, iter [04100, 05004], lr: 0.076382, loss: 2.7094
2022-03-02 21:59:35 - train: epoch 0069, iter [04200, 05004], lr: 0.076382, loss: 2.5947
2022-03-02 22:00:10 - train: epoch 0069, iter [04300, 05004], lr: 0.076382, loss: 2.8324
2022-03-02 22:00:46 - train: epoch 0069, iter [04400, 05004], lr: 0.076382, loss: 2.6462
2022-03-02 22:01:24 - train: epoch 0069, iter [04500, 05004], lr: 0.076382, loss: 2.6672
2022-03-02 22:02:01 - train: epoch 0069, iter [04600, 05004], lr: 0.076382, loss: 2.9283
2022-03-02 22:02:38 - train: epoch 0069, iter [04700, 05004], lr: 0.076382, loss: 2.9104
2022-03-02 22:03:21 - train: epoch 0069, iter [04800, 05004], lr: 0.076382, loss: 2.7316
2022-03-02 22:03:57 - train: epoch 0069, iter [04900, 05004], lr: 0.076382, loss: 2.6834
2022-03-02 22:04:33 - train: epoch 0069, iter [05000, 05004], lr: 0.076382, loss: 2.7108
2022-03-02 22:04:34 - train: epoch 069, train_loss: 2.6260
2022-03-02 22:05:45 - eval: epoch: 069, acc1: 55.172%, acc5: 80.378%, test_loss: 1.8947, per_image_load_time: 1.662ms, per_image_inference_time: 0.414ms
2022-03-02 22:05:46 - until epoch: 069, best_acc1: 56.038%
2022-03-02 22:05:46 - epoch 070 lr: 0.07569434058643844
2022-03-02 22:06:23 - train: epoch 0070, iter [00100, 05004], lr: 0.075694, loss: 2.5421
2022-03-02 22:06:56 - train: epoch 0070, iter [00200, 05004], lr: 0.075694, loss: 2.4991
2022-03-02 22:07:29 - train: epoch 0070, iter [00300, 05004], lr: 0.075694, loss: 2.6813
2022-03-02 22:08:02 - train: epoch 0070, iter [00400, 05004], lr: 0.075694, loss: 2.7413
2022-03-02 22:08:34 - train: epoch 0070, iter [00500, 05004], lr: 0.075694, loss: 2.4547
2022-03-02 22:09:08 - train: epoch 0070, iter [00600, 05004], lr: 0.075694, loss: 2.5129
2022-03-02 22:09:40 - train: epoch 0070, iter [00700, 05004], lr: 0.075694, loss: 2.3027
2022-03-02 22:10:13 - train: epoch 0070, iter [00800, 05004], lr: 0.075694, loss: 2.4709
2022-03-02 22:10:45 - train: epoch 0070, iter [00900, 05004], lr: 0.075694, loss: 2.7416
2022-03-02 22:11:18 - train: epoch 0070, iter [01000, 05004], lr: 0.075694, loss: 2.6388
2022-03-02 22:11:50 - train: epoch 0070, iter [01100, 05004], lr: 0.075694, loss: 2.6250
2022-03-02 22:12:24 - train: epoch 0070, iter [01200, 05004], lr: 0.075694, loss: 2.5006
2022-03-02 22:12:56 - train: epoch 0070, iter [01300, 05004], lr: 0.075694, loss: 2.5333
2022-03-02 22:13:30 - train: epoch 0070, iter [01400, 05004], lr: 0.075694, loss: 2.6718
2022-03-02 22:14:02 - train: epoch 0070, iter [01500, 05004], lr: 0.075694, loss: 2.7828
2022-03-02 22:14:35 - train: epoch 0070, iter [01600, 05004], lr: 0.075694, loss: 2.6152
2022-03-02 22:15:10 - train: epoch 0070, iter [01700, 05004], lr: 0.075694, loss: 2.5879
2022-03-02 22:15:42 - train: epoch 0070, iter [01800, 05004], lr: 0.075694, loss: 2.5379
2022-03-02 22:16:15 - train: epoch 0070, iter [01900, 05004], lr: 0.075694, loss: 2.6670
2022-03-02 22:16:49 - train: epoch 0070, iter [02000, 05004], lr: 0.075694, loss: 2.8156
2022-03-02 22:17:22 - train: epoch 0070, iter [02100, 05004], lr: 0.075694, loss: 2.9897
2022-03-02 22:17:55 - train: epoch 0070, iter [02200, 05004], lr: 0.075694, loss: 2.5575
2022-03-02 22:18:30 - train: epoch 0070, iter [02300, 05004], lr: 0.075694, loss: 2.5350
2022-03-02 22:19:02 - train: epoch 0070, iter [02400, 05004], lr: 0.075694, loss: 2.6765
2022-03-02 22:19:35 - train: epoch 0070, iter [02500, 05004], lr: 0.075694, loss: 2.5037
2022-03-02 22:20:07 - train: epoch 0070, iter [02600, 05004], lr: 0.075694, loss: 2.4947
2022-03-02 22:20:43 - train: epoch 0070, iter [02700, 05004], lr: 0.075694, loss: 2.7746
2022-03-02 22:21:16 - train: epoch 0070, iter [02800, 05004], lr: 0.075694, loss: 2.7758
2022-03-02 22:21:50 - train: epoch 0070, iter [02900, 05004], lr: 0.075694, loss: 2.7423
2022-03-02 22:22:22 - train: epoch 0070, iter [03000, 05004], lr: 0.075694, loss: 2.5555
2022-03-02 22:22:55 - train: epoch 0070, iter [03100, 05004], lr: 0.075694, loss: 2.5778
2022-03-02 22:23:29 - train: epoch 0070, iter [03200, 05004], lr: 0.075694, loss: 2.7474
2022-03-02 22:24:03 - train: epoch 0070, iter [03300, 05004], lr: 0.075694, loss: 2.7774
2022-03-02 22:24:39 - train: epoch 0070, iter [03400, 05004], lr: 0.075694, loss: 2.4639
2022-03-02 22:25:13 - train: epoch 0070, iter [03500, 05004], lr: 0.075694, loss: 2.6490
2022-03-02 22:25:49 - train: epoch 0070, iter [03600, 05004], lr: 0.075694, loss: 2.5147
2022-03-02 22:26:29 - train: epoch 0070, iter [03700, 05004], lr: 0.075694, loss: 2.6304
2022-03-02 22:27:04 - train: epoch 0070, iter [03800, 05004], lr: 0.075694, loss: 2.7434
2022-03-02 22:27:43 - train: epoch 0070, iter [03900, 05004], lr: 0.075694, loss: 2.6036
2022-03-02 22:28:17 - train: epoch 0070, iter [04000, 05004], lr: 0.075694, loss: 2.7568
2022-03-02 22:28:51 - train: epoch 0070, iter [04100, 05004], lr: 0.075694, loss: 2.7034
2022-03-02 22:29:28 - train: epoch 0070, iter [04200, 05004], lr: 0.075694, loss: 2.7657
2022-03-02 22:30:07 - train: epoch 0070, iter [04300, 05004], lr: 0.075694, loss: 2.7685
2022-03-02 22:30:40 - train: epoch 0070, iter [04400, 05004], lr: 0.075694, loss: 2.7650
2022-03-02 22:31:13 - train: epoch 0070, iter [04500, 05004], lr: 0.075694, loss: 2.6450
2022-03-02 22:31:46 - train: epoch 0070, iter [04600, 05004], lr: 0.075694, loss: 2.9373
2022-03-02 22:32:18 - train: epoch 0070, iter [04700, 05004], lr: 0.075694, loss: 2.7777
2022-03-02 22:32:51 - train: epoch 0070, iter [04800, 05004], lr: 0.075694, loss: 2.6796
2022-03-02 22:33:25 - train: epoch 0070, iter [04900, 05004], lr: 0.075694, loss: 2.6982
2022-03-02 22:33:57 - train: epoch 0070, iter [05000, 05004], lr: 0.075694, loss: 2.4567
2022-03-02 22:33:58 - train: epoch 070, train_loss: 2.6232
2022-03-02 22:35:09 - eval: epoch: 070, acc1: 56.468%, acc5: 81.412%, test_loss: 1.8315, per_image_load_time: 1.130ms, per_image_inference_time: 0.420ms
2022-03-02 22:35:10 - until epoch: 070, best_acc1: 56.468%
2022-03-02 22:35:10 - epoch 071 lr: 0.07500000000000001
2022-03-02 22:35:48 - train: epoch 0071, iter [00100, 05004], lr: 0.075000, loss: 2.5156
2022-03-02 22:36:21 - train: epoch 0071, iter [00200, 05004], lr: 0.075000, loss: 2.5921
2022-03-02 22:36:53 - train: epoch 0071, iter [00300, 05004], lr: 0.075000, loss: 2.6250
2022-03-02 22:37:26 - train: epoch 0071, iter [00400, 05004], lr: 0.075000, loss: 2.7628
2022-03-02 22:37:58 - train: epoch 0071, iter [00500, 05004], lr: 0.075000, loss: 2.6940
2022-03-02 22:38:30 - train: epoch 0071, iter [00600, 05004], lr: 0.075000, loss: 2.5593
2022-03-02 22:39:04 - train: epoch 0071, iter [00700, 05004], lr: 0.075000, loss: 2.5924
2022-03-02 22:39:36 - train: epoch 0071, iter [00800, 05004], lr: 0.075000, loss: 2.3670
2022-03-02 22:40:09 - train: epoch 0071, iter [00900, 05004], lr: 0.075000, loss: 2.7749
2022-03-02 22:40:41 - train: epoch 0071, iter [01000, 05004], lr: 0.075000, loss: 2.4996
2022-03-02 22:41:14 - train: epoch 0071, iter [01100, 05004], lr: 0.075000, loss: 2.6206
2022-03-02 22:41:47 - train: epoch 0071, iter [01200, 05004], lr: 0.075000, loss: 2.6143
2022-03-02 22:42:20 - train: epoch 0071, iter [01300, 05004], lr: 0.075000, loss: 2.6908
2022-03-02 22:42:52 - train: epoch 0071, iter [01400, 05004], lr: 0.075000, loss: 2.6632
2022-03-02 22:43:25 - train: epoch 0071, iter [01500, 05004], lr: 0.075000, loss: 2.4174
2022-03-02 22:43:57 - train: epoch 0071, iter [01600, 05004], lr: 0.075000, loss: 2.4800
2022-03-02 22:44:31 - train: epoch 0071, iter [01700, 05004], lr: 0.075000, loss: 2.4953
2022-03-02 22:45:03 - train: epoch 0071, iter [01800, 05004], lr: 0.075000, loss: 2.7566
2022-03-02 22:45:36 - train: epoch 0071, iter [01900, 05004], lr: 0.075000, loss: 2.3968
2022-03-02 22:46:08 - train: epoch 0071, iter [02000, 05004], lr: 0.075000, loss: 2.7312
2022-03-02 22:46:41 - train: epoch 0071, iter [02100, 05004], lr: 0.075000, loss: 2.2949
2022-03-02 22:47:13 - train: epoch 0071, iter [02200, 05004], lr: 0.075000, loss: 2.4096
2022-03-02 22:47:46 - train: epoch 0071, iter [02300, 05004], lr: 0.075000, loss: 2.5976
2022-03-02 22:48:20 - train: epoch 0071, iter [02400, 05004], lr: 0.075000, loss: 2.5152
2022-03-02 22:48:52 - train: epoch 0071, iter [02500, 05004], lr: 0.075000, loss: 2.9907
2022-03-02 22:49:26 - train: epoch 0071, iter [02600, 05004], lr: 0.075000, loss: 2.3992
2022-03-02 22:49:58 - train: epoch 0071, iter [02700, 05004], lr: 0.075000, loss: 2.7142
2022-03-02 22:50:31 - train: epoch 0071, iter [02800, 05004], lr: 0.075000, loss: 2.7681
2022-03-02 22:51:03 - train: epoch 0071, iter [02900, 05004], lr: 0.075000, loss: 2.5211
2022-03-02 22:51:42 - train: epoch 0071, iter [03000, 05004], lr: 0.075000, loss: 2.9117
2022-03-02 22:52:16 - train: epoch 0071, iter [03100, 05004], lr: 0.075000, loss: 2.4168
2022-03-02 22:52:49 - train: epoch 0071, iter [03200, 05004], lr: 0.075000, loss: 2.4034
2022-03-02 22:53:21 - train: epoch 0071, iter [03300, 05004], lr: 0.075000, loss: 3.0104
2022-03-02 22:53:54 - train: epoch 0071, iter [03400, 05004], lr: 0.075000, loss: 2.2760
2022-03-02 22:54:26 - train: epoch 0071, iter [03500, 05004], lr: 0.075000, loss: 2.7082
2022-03-02 22:54:59 - train: epoch 0071, iter [03600, 05004], lr: 0.075000, loss: 2.7493
2022-03-02 22:55:31 - train: epoch 0071, iter [03700, 05004], lr: 0.075000, loss: 2.7828
2022-03-02 22:56:05 - train: epoch 0071, iter [03800, 05004], lr: 0.075000, loss: 2.7059
2022-03-02 22:56:38 - train: epoch 0071, iter [03900, 05004], lr: 0.075000, loss: 2.6275
2022-03-02 22:57:11 - train: epoch 0071, iter [04000, 05004], lr: 0.075000, loss: 2.4579
2022-03-02 22:57:43 - train: epoch 0071, iter [04100, 05004], lr: 0.075000, loss: 2.4954
2022-03-02 22:58:16 - train: epoch 0071, iter [04200, 05004], lr: 0.075000, loss: 2.8572
2022-03-02 22:58:48 - train: epoch 0071, iter [04300, 05004], lr: 0.075000, loss: 2.3618
2022-03-02 22:59:36 - train: epoch 0071, iter [04400, 05004], lr: 0.075000, loss: 2.8628
2022-03-02 23:00:12 - train: epoch 0071, iter [04500, 05004], lr: 0.075000, loss: 2.7375
2022-03-02 23:00:45 - train: epoch 0071, iter [04600, 05004], lr: 0.075000, loss: 2.5153
2022-03-02 23:01:21 - train: epoch 0071, iter [04700, 05004], lr: 0.075000, loss: 2.4914
2022-03-02 23:02:07 - train: epoch 0071, iter [04800, 05004], lr: 0.075000, loss: 2.6781
2022-03-02 23:02:40 - train: epoch 0071, iter [04900, 05004], lr: 0.075000, loss: 2.7370
2022-03-02 23:03:13 - train: epoch 0071, iter [05000, 05004], lr: 0.075000, loss: 2.4696
2022-03-02 23:03:14 - train: epoch 071, train_loss: 2.6166
2022-03-02 23:04:26 - eval: epoch: 071, acc1: 56.322%, acc5: 81.314%, test_loss: 1.8340, per_image_load_time: 1.435ms, per_image_inference_time: 0.439ms
2022-03-02 23:04:26 - until epoch: 071, best_acc1: 56.468%
2022-03-02 23:04:26 - epoch 072 lr: 0.0742991706621303
2022-03-02 23:05:04 - train: epoch 0072, iter [00100, 05004], lr: 0.074299, loss: 2.7674
2022-03-02 23:05:37 - train: epoch 0072, iter [00200, 05004], lr: 0.074299, loss: 2.5489
2022-03-02 23:06:09 - train: epoch 0072, iter [00300, 05004], lr: 0.074299, loss: 2.4635
2022-03-02 23:06:41 - train: epoch 0072, iter [00400, 05004], lr: 0.074299, loss: 2.6732
2022-03-02 23:07:14 - train: epoch 0072, iter [00500, 05004], lr: 0.074299, loss: 2.7838
2022-03-02 23:07:48 - train: epoch 0072, iter [00600, 05004], lr: 0.074299, loss: 2.5539
2022-03-02 23:08:20 - train: epoch 0072, iter [00700, 05004], lr: 0.074299, loss: 2.5154
2022-03-02 23:08:53 - train: epoch 0072, iter [00800, 05004], lr: 0.074299, loss: 2.6863
2022-03-02 23:09:25 - train: epoch 0072, iter [00900, 05004], lr: 0.074299, loss: 2.5300
2022-03-02 23:09:59 - train: epoch 0072, iter [01000, 05004], lr: 0.074299, loss: 2.5200
