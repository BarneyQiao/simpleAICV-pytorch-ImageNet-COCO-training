2022-03-02 23:10:31 - train: epoch 0072, iter [01100, 05004], lr: 0.074299, loss: 2.6960
2022-03-02 23:11:03 - train: epoch 0072, iter [01200, 05004], lr: 0.074299, loss: 2.6937
2022-03-02 23:11:36 - train: epoch 0072, iter [01300, 05004], lr: 0.074299, loss: 2.6869
2022-03-02 23:12:09 - train: epoch 0072, iter [01400, 05004], lr: 0.074299, loss: 2.4731
2022-03-02 23:12:43 - train: epoch 0072, iter [01500, 05004], lr: 0.074299, loss: 2.7077
2022-03-02 23:13:15 - train: epoch 0072, iter [01600, 05004], lr: 0.074299, loss: 2.3739
2022-03-02 23:13:49 - train: epoch 0072, iter [01700, 05004], lr: 0.074299, loss: 2.7537
2022-03-02 23:14:22 - train: epoch 0072, iter [01800, 05004], lr: 0.074299, loss: 2.7690
2022-03-02 23:14:55 - train: epoch 0072, iter [01900, 05004], lr: 0.074299, loss: 2.6735
2022-03-02 23:15:27 - train: epoch 0072, iter [02000, 05004], lr: 0.074299, loss: 3.0618
2022-03-02 23:16:01 - train: epoch 0072, iter [02100, 05004], lr: 0.074299, loss: 2.7894
2022-03-02 23:16:34 - train: epoch 0072, iter [02200, 05004], lr: 0.074299, loss: 2.9381
2022-03-02 23:17:07 - train: epoch 0072, iter [02300, 05004], lr: 0.074299, loss: 2.7329
2022-03-02 23:17:39 - train: epoch 0072, iter [02400, 05004], lr: 0.074299, loss: 2.9119
2022-03-02 23:18:15 - train: epoch 0072, iter [02500, 05004], lr: 0.074299, loss: 2.4126
2022-03-02 23:18:48 - train: epoch 0072, iter [02600, 05004], lr: 0.074299, loss: 2.5067
2022-03-02 23:19:22 - train: epoch 0072, iter [02700, 05004], lr: 0.074299, loss: 2.7163
2022-03-02 23:19:55 - train: epoch 0072, iter [02800, 05004], lr: 0.074299, loss: 2.8294
2022-03-02 23:20:28 - train: epoch 0072, iter [02900, 05004], lr: 0.074299, loss: 2.5359
2022-03-02 23:21:02 - train: epoch 0072, iter [03000, 05004], lr: 0.074299, loss: 2.6251
2022-03-02 23:21:35 - train: epoch 0072, iter [03100, 05004], lr: 0.074299, loss: 2.7672
2022-03-02 23:22:08 - train: epoch 0072, iter [03200, 05004], lr: 0.074299, loss: 2.7182
2022-03-02 23:22:41 - train: epoch 0072, iter [03300, 05004], lr: 0.074299, loss: 2.5614
2022-03-02 23:23:14 - train: epoch 0072, iter [03400, 05004], lr: 0.074299, loss: 3.0093
2022-03-02 23:23:47 - train: epoch 0072, iter [03500, 05004], lr: 0.074299, loss: 2.7167
2022-03-02 23:24:19 - train: epoch 0072, iter [03600, 05004], lr: 0.074299, loss: 2.5704
2022-03-02 23:24:53 - train: epoch 0072, iter [03700, 05004], lr: 0.074299, loss: 2.8074
2022-03-02 23:25:26 - train: epoch 0072, iter [03800, 05004], lr: 0.074299, loss: 2.3575
2022-03-02 23:25:59 - train: epoch 0072, iter [03900, 05004], lr: 0.074299, loss: 2.7820
2022-03-02 23:26:31 - train: epoch 0072, iter [04000, 05004], lr: 0.074299, loss: 2.6477
2022-03-02 23:27:05 - train: epoch 0072, iter [04100, 05004], lr: 0.074299, loss: 2.9192
2022-03-02 23:27:37 - train: epoch 0072, iter [04200, 05004], lr: 0.074299, loss: 2.3697
2022-03-02 23:28:11 - train: epoch 0072, iter [04300, 05004], lr: 0.074299, loss: 2.4650
2022-03-02 23:28:43 - train: epoch 0072, iter [04400, 05004], lr: 0.074299, loss: 2.5997
2022-03-02 23:29:17 - train: epoch 0072, iter [04500, 05004], lr: 0.074299, loss: 2.7114
2022-03-02 23:29:50 - train: epoch 0072, iter [04600, 05004], lr: 0.074299, loss: 2.6174
2022-03-02 23:30:24 - train: epoch 0072, iter [04700, 05004], lr: 0.074299, loss: 2.7190
2022-03-02 23:30:56 - train: epoch 0072, iter [04800, 05004], lr: 0.074299, loss: 2.6202
2022-03-02 23:31:36 - train: epoch 0072, iter [04900, 05004], lr: 0.074299, loss: 2.7047
2022-03-02 23:32:07 - train: epoch 0072, iter [05000, 05004], lr: 0.074299, loss: 2.6343
2022-03-02 23:32:08 - train: epoch 072, train_loss: 2.6134
2022-03-02 23:33:20 - eval: epoch: 072, acc1: 55.574%, acc5: 80.682%, test_loss: 1.8655, per_image_load_time: 1.218ms, per_image_inference_time: 0.455ms
2022-03-02 23:33:21 - until epoch: 072, best_acc1: 56.468%
2022-03-02 23:33:21 - epoch 073 lr: 0.07359203447312411
2022-03-02 23:33:59 - train: epoch 0073, iter [00100, 05004], lr: 0.073592, loss: 2.9701
2022-03-02 23:34:33 - train: epoch 0073, iter [00200, 05004], lr: 0.073592, loss: 2.8246
2022-03-02 23:35:06 - train: epoch 0073, iter [00300, 05004], lr: 0.073592, loss: 2.4824
2022-03-02 23:35:39 - train: epoch 0073, iter [00400, 05004], lr: 0.073592, loss: 2.5429
2022-03-02 23:36:12 - train: epoch 0073, iter [00500, 05004], lr: 0.073592, loss: 2.5528
2022-03-02 23:36:45 - train: epoch 0073, iter [00600, 05004], lr: 0.073592, loss: 2.4723
2022-03-02 23:37:18 - train: epoch 0073, iter [00700, 05004], lr: 0.073592, loss: 2.6850
2022-03-02 23:37:51 - train: epoch 0073, iter [00800, 05004], lr: 0.073592, loss: 2.4143
2022-03-02 23:38:24 - train: epoch 0073, iter [00900, 05004], lr: 0.073592, loss: 2.4234
2022-03-02 23:38:58 - train: epoch 0073, iter [01000, 05004], lr: 0.073592, loss: 2.6379
2022-03-02 23:39:31 - train: epoch 0073, iter [01100, 05004], lr: 0.073592, loss: 2.6700
2022-03-02 23:40:04 - train: epoch 0073, iter [01200, 05004], lr: 0.073592, loss: 2.8277
2022-03-02 23:40:37 - train: epoch 0073, iter [01300, 05004], lr: 0.073592, loss: 2.4622
2022-03-02 23:41:10 - train: epoch 0073, iter [01400, 05004], lr: 0.073592, loss: 2.7577
2022-03-02 23:41:42 - train: epoch 0073, iter [01500, 05004], lr: 0.073592, loss: 2.6036
2022-03-02 23:42:16 - train: epoch 0073, iter [01600, 05004], lr: 0.073592, loss: 2.5574
2022-03-02 23:42:48 - train: epoch 0073, iter [01700, 05004], lr: 0.073592, loss: 2.7598
2022-03-02 23:43:22 - train: epoch 0073, iter [01800, 05004], lr: 0.073592, loss: 2.2956
2022-03-02 23:43:55 - train: epoch 0073, iter [01900, 05004], lr: 0.073592, loss: 2.6116
2022-03-02 23:44:28 - train: epoch 0073, iter [02000, 05004], lr: 0.073592, loss: 2.3870
2022-03-02 23:45:01 - train: epoch 0073, iter [02100, 05004], lr: 0.073592, loss: 2.7398
2022-03-02 23:45:34 - train: epoch 0073, iter [02200, 05004], lr: 0.073592, loss: 2.5744
2022-03-02 23:46:07 - train: epoch 0073, iter [02300, 05004], lr: 0.073592, loss: 2.6943
2022-03-02 23:46:41 - train: epoch 0073, iter [02400, 05004], lr: 0.073592, loss: 2.6995
2022-03-02 23:47:13 - train: epoch 0073, iter [02500, 05004], lr: 0.073592, loss: 2.6195
2022-03-02 23:47:47 - train: epoch 0073, iter [02600, 05004], lr: 0.073592, loss: 2.5847
2022-03-02 23:48:19 - train: epoch 0073, iter [02700, 05004], lr: 0.073592, loss: 2.5381
2022-03-02 23:48:53 - train: epoch 0073, iter [02800, 05004], lr: 0.073592, loss: 2.5041
2022-03-02 23:49:26 - train: epoch 0073, iter [02900, 05004], lr: 0.073592, loss: 2.6005
2022-03-02 23:49:59 - train: epoch 0073, iter [03000, 05004], lr: 0.073592, loss: 2.6293
2022-03-02 23:50:32 - train: epoch 0073, iter [03100, 05004], lr: 0.073592, loss: 2.5049
2022-03-02 23:51:05 - train: epoch 0073, iter [03200, 05004], lr: 0.073592, loss: 2.4777
2022-03-02 23:51:37 - train: epoch 0073, iter [03300, 05004], lr: 0.073592, loss: 2.7240
2022-03-02 23:52:11 - train: epoch 0073, iter [03400, 05004], lr: 0.073592, loss: 2.6763
2022-03-02 23:52:44 - train: epoch 0073, iter [03500, 05004], lr: 0.073592, loss: 2.4772
2022-03-02 23:53:17 - train: epoch 0073, iter [03600, 05004], lr: 0.073592, loss: 2.4452
2022-03-02 23:53:50 - train: epoch 0073, iter [03700, 05004], lr: 0.073592, loss: 2.7334
2022-03-02 23:54:24 - train: epoch 0073, iter [03800, 05004], lr: 0.073592, loss: 2.6891
2022-03-02 23:54:57 - train: epoch 0073, iter [03900, 05004], lr: 0.073592, loss: 2.4951
2022-03-02 23:55:30 - train: epoch 0073, iter [04000, 05004], lr: 0.073592, loss: 2.7286
2022-03-02 23:56:02 - train: epoch 0073, iter [04100, 05004], lr: 0.073592, loss: 2.7030
2022-03-02 23:56:35 - train: epoch 0073, iter [04200, 05004], lr: 0.073592, loss: 2.7891
2022-03-02 23:57:08 - train: epoch 0073, iter [04300, 05004], lr: 0.073592, loss: 2.5541
2022-03-02 23:57:41 - train: epoch 0073, iter [04400, 05004], lr: 0.073592, loss: 2.7546
2022-03-02 23:58:14 - train: epoch 0073, iter [04500, 05004], lr: 0.073592, loss: 2.7456
2022-03-02 23:58:48 - train: epoch 0073, iter [04600, 05004], lr: 0.073592, loss: 2.6439
2022-03-02 23:59:21 - train: epoch 0073, iter [04700, 05004], lr: 0.073592, loss: 2.5736
2022-03-02 23:59:54 - train: epoch 0073, iter [04800, 05004], lr: 0.073592, loss: 2.5639
2022-03-03 00:00:27 - train: epoch 0073, iter [04900, 05004], lr: 0.073592, loss: 2.5392
2022-03-03 00:00:59 - train: epoch 0073, iter [05000, 05004], lr: 0.073592, loss: 2.7538
2022-03-03 00:01:00 - train: epoch 073, train_loss: 2.6093
2022-03-03 00:02:12 - eval: epoch: 073, acc1: 55.284%, acc5: 80.606%, test_loss: 1.8740, per_image_load_time: 1.301ms, per_image_inference_time: 0.487ms
2022-03-03 00:02:12 - until epoch: 073, best_acc1: 56.468%
2022-03-03 00:02:12 - epoch 074 lr: 0.07287877497021977
2022-03-03 00:02:51 - train: epoch 0074, iter [00100, 05004], lr: 0.072879, loss: 2.5265
2022-03-03 00:03:25 - train: epoch 0074, iter [00200, 05004], lr: 0.072879, loss: 2.6088
2022-03-03 00:03:58 - train: epoch 0074, iter [00300, 05004], lr: 0.072879, loss: 2.5505
2022-03-03 00:04:31 - train: epoch 0074, iter [00400, 05004], lr: 0.072879, loss: 2.5532
2022-03-03 00:05:04 - train: epoch 0074, iter [00500, 05004], lr: 0.072879, loss: 2.8891
2022-03-03 00:05:37 - train: epoch 0074, iter [00600, 05004], lr: 0.072879, loss: 2.5130
2022-03-03 00:06:10 - train: epoch 0074, iter [00700, 05004], lr: 0.072879, loss: 2.3814
2022-03-03 00:06:43 - train: epoch 0074, iter [00800, 05004], lr: 0.072879, loss: 2.6944
2022-03-03 00:07:17 - train: epoch 0074, iter [00900, 05004], lr: 0.072879, loss: 2.4127
2022-03-03 00:07:50 - train: epoch 0074, iter [01000, 05004], lr: 0.072879, loss: 2.7941
2022-03-03 00:08:23 - train: epoch 0074, iter [01100, 05004], lr: 0.072879, loss: 2.4937
2022-03-03 00:08:57 - train: epoch 0074, iter [01200, 05004], lr: 0.072879, loss: 2.5572
2022-03-03 00:09:29 - train: epoch 0074, iter [01300, 05004], lr: 0.072879, loss: 2.4913
2022-03-03 00:10:03 - train: epoch 0074, iter [01400, 05004], lr: 0.072879, loss: 2.5606
2022-03-03 00:10:35 - train: epoch 0074, iter [01500, 05004], lr: 0.072879, loss: 2.9405
2022-03-03 00:11:09 - train: epoch 0074, iter [01600, 05004], lr: 0.072879, loss: 2.4524
2022-03-03 00:11:42 - train: epoch 0074, iter [01700, 05004], lr: 0.072879, loss: 2.6678
2022-03-03 00:12:14 - train: epoch 0074, iter [01800, 05004], lr: 0.072879, loss: 2.6844
2022-03-03 00:12:47 - train: epoch 0074, iter [01900, 05004], lr: 0.072879, loss: 2.4874
2022-03-03 00:13:21 - train: epoch 0074, iter [02000, 05004], lr: 0.072879, loss: 2.4461
2022-03-03 00:13:53 - train: epoch 0074, iter [02100, 05004], lr: 0.072879, loss: 2.6688
2022-03-03 00:14:26 - train: epoch 0074, iter [02200, 05004], lr: 0.072879, loss: 2.7950
2022-03-03 00:14:59 - train: epoch 0074, iter [02300, 05004], lr: 0.072879, loss: 2.6344
2022-03-03 00:15:32 - train: epoch 0074, iter [02400, 05004], lr: 0.072879, loss: 2.7143
2022-03-03 00:16:05 - train: epoch 0074, iter [02500, 05004], lr: 0.072879, loss: 2.2714
2022-03-03 00:16:39 - train: epoch 0074, iter [02600, 05004], lr: 0.072879, loss: 2.3934
2022-03-03 00:17:11 - train: epoch 0074, iter [02700, 05004], lr: 0.072879, loss: 2.4077
2022-03-03 00:17:45 - train: epoch 0074, iter [02800, 05004], lr: 0.072879, loss: 2.8646
2022-03-03 00:18:17 - train: epoch 0074, iter [02900, 05004], lr: 0.072879, loss: 2.8256
2022-03-03 00:18:50 - train: epoch 0074, iter [03000, 05004], lr: 0.072879, loss: 2.6481
2022-03-03 00:19:24 - train: epoch 0074, iter [03100, 05004], lr: 0.072879, loss: 2.6615
2022-03-03 00:19:57 - train: epoch 0074, iter [03200, 05004], lr: 0.072879, loss: 2.3713
2022-03-03 00:20:30 - train: epoch 0074, iter [03300, 05004], lr: 0.072879, loss: 2.6230
2022-03-03 00:21:03 - train: epoch 0074, iter [03400, 05004], lr: 0.072879, loss: 2.6030
2022-03-03 00:21:36 - train: epoch 0074, iter [03500, 05004], lr: 0.072879, loss: 2.6125
2022-03-03 00:22:08 - train: epoch 0074, iter [03600, 05004], lr: 0.072879, loss: 2.7412
2022-03-03 00:22:41 - train: epoch 0074, iter [03700, 05004], lr: 0.072879, loss: 2.6871
2022-03-03 00:23:14 - train: epoch 0074, iter [03800, 05004], lr: 0.072879, loss: 2.4054
2022-03-03 00:23:47 - train: epoch 0074, iter [03900, 05004], lr: 0.072879, loss: 2.3952
2022-03-03 00:24:21 - train: epoch 0074, iter [04000, 05004], lr: 0.072879, loss: 2.9215
2022-03-03 00:24:53 - train: epoch 0074, iter [04100, 05004], lr: 0.072879, loss: 2.6874
2022-03-03 00:25:26 - train: epoch 0074, iter [04200, 05004], lr: 0.072879, loss: 2.7864
2022-03-03 00:25:59 - train: epoch 0074, iter [04300, 05004], lr: 0.072879, loss: 2.7290
2022-03-03 00:26:33 - train: epoch 0074, iter [04400, 05004], lr: 0.072879, loss: 2.3857
2022-03-03 00:27:06 - train: epoch 0074, iter [04500, 05004], lr: 0.072879, loss: 2.4546
2022-03-03 00:27:39 - train: epoch 0074, iter [04600, 05004], lr: 0.072879, loss: 2.7376
2022-03-03 00:28:12 - train: epoch 0074, iter [04700, 05004], lr: 0.072879, loss: 2.7797
2022-03-03 00:28:45 - train: epoch 0074, iter [04800, 05004], lr: 0.072879, loss: 2.4923
2022-03-03 00:29:18 - train: epoch 0074, iter [04900, 05004], lr: 0.072879, loss: 2.5852
2022-03-03 00:29:50 - train: epoch 0074, iter [05000, 05004], lr: 0.072879, loss: 2.5367
2022-03-03 00:29:51 - train: epoch 074, train_loss: 2.6020
2022-03-03 00:31:03 - eval: epoch: 074, acc1: 55.910%, acc5: 80.696%, test_loss: 1.8651, per_image_load_time: 1.502ms, per_image_inference_time: 0.466ms
2022-03-03 00:31:04 - until epoch: 074, best_acc1: 56.468%
2022-03-03 00:31:04 - epoch 075 lr: 0.07215957727996207
2022-03-03 00:31:43 - train: epoch 0075, iter [00100, 05004], lr: 0.072160, loss: 2.5106
2022-03-03 00:32:15 - train: epoch 0075, iter [00200, 05004], lr: 0.072160, loss: 2.5588
2022-03-03 00:32:49 - train: epoch 0075, iter [00300, 05004], lr: 0.072160, loss: 2.5548
2022-03-03 00:33:22 - train: epoch 0075, iter [00400, 05004], lr: 0.072160, loss: 2.6422
2022-03-03 00:33:55 - train: epoch 0075, iter [00500, 05004], lr: 0.072160, loss: 2.6901
2022-03-03 00:34:27 - train: epoch 0075, iter [00600, 05004], lr: 0.072160, loss: 2.6138
2022-03-03 00:35:01 - train: epoch 0075, iter [00700, 05004], lr: 0.072160, loss: 2.7921
2022-03-03 00:35:34 - train: epoch 0075, iter [00800, 05004], lr: 0.072160, loss: 2.4701
2022-03-03 00:36:07 - train: epoch 0075, iter [00900, 05004], lr: 0.072160, loss: 2.5315
2022-03-03 00:36:40 - train: epoch 0075, iter [01000, 05004], lr: 0.072160, loss: 2.2189
2022-03-03 00:37:14 - train: epoch 0075, iter [01100, 05004], lr: 0.072160, loss: 3.0544
2022-03-03 00:37:47 - train: epoch 0075, iter [01200, 05004], lr: 0.072160, loss: 2.3805
2022-03-03 00:38:19 - train: epoch 0075, iter [01300, 05004], lr: 0.072160, loss: 2.4556
2022-03-03 00:38:52 - train: epoch 0075, iter [01400, 05004], lr: 0.072160, loss: 2.4101
2022-03-03 00:39:26 - train: epoch 0075, iter [01500, 05004], lr: 0.072160, loss: 2.7591
2022-03-03 00:39:58 - train: epoch 0075, iter [01600, 05004], lr: 0.072160, loss: 2.2633
2022-03-03 00:40:31 - train: epoch 0075, iter [01700, 05004], lr: 0.072160, loss: 2.5859
2022-03-03 00:41:03 - train: epoch 0075, iter [01800, 05004], lr: 0.072160, loss: 2.6848
2022-03-03 00:41:35 - train: epoch 0075, iter [01900, 05004], lr: 0.072160, loss: 2.5738
2022-03-03 00:42:07 - train: epoch 0075, iter [02000, 05004], lr: 0.072160, loss: 2.7927
2022-03-03 00:42:40 - train: epoch 0075, iter [02100, 05004], lr: 0.072160, loss: 2.4663
2022-03-03 00:43:12 - train: epoch 0075, iter [02200, 05004], lr: 0.072160, loss: 2.5415
2022-03-03 00:43:44 - train: epoch 0075, iter [02300, 05004], lr: 0.072160, loss: 2.3163
2022-03-03 00:44:15 - train: epoch 0075, iter [02400, 05004], lr: 0.072160, loss: 2.5770
2022-03-03 00:44:47 - train: epoch 0075, iter [02500, 05004], lr: 0.072160, loss: 2.7141
2022-03-03 00:45:18 - train: epoch 0075, iter [02600, 05004], lr: 0.072160, loss: 2.8836
2022-03-03 00:45:51 - train: epoch 0075, iter [02700, 05004], lr: 0.072160, loss: 2.3263
2022-03-03 00:46:23 - train: epoch 0075, iter [02800, 05004], lr: 0.072160, loss: 2.6861
2022-03-03 00:46:55 - train: epoch 0075, iter [02900, 05004], lr: 0.072160, loss: 2.7998
2022-03-03 00:47:26 - train: epoch 0075, iter [03000, 05004], lr: 0.072160, loss: 2.7572
2022-03-03 00:47:59 - train: epoch 0075, iter [03100, 05004], lr: 0.072160, loss: 2.8071
2022-03-03 00:48:31 - train: epoch 0075, iter [03200, 05004], lr: 0.072160, loss: 2.6775
2022-03-03 00:49:03 - train: epoch 0075, iter [03300, 05004], lr: 0.072160, loss: 2.3915
2022-03-03 00:49:34 - train: epoch 0075, iter [03400, 05004], lr: 0.072160, loss: 2.6757
2022-03-03 00:50:07 - train: epoch 0075, iter [03500, 05004], lr: 0.072160, loss: 2.6119
2022-03-03 00:50:38 - train: epoch 0075, iter [03600, 05004], lr: 0.072160, loss: 2.7844
2022-03-03 00:51:10 - train: epoch 0075, iter [03700, 05004], lr: 0.072160, loss: 2.6079
2022-03-03 00:51:42 - train: epoch 0075, iter [03800, 05004], lr: 0.072160, loss: 2.3370
2022-03-03 00:52:14 - train: epoch 0075, iter [03900, 05004], lr: 0.072160, loss: 2.6101
2022-03-03 00:52:46 - train: epoch 0075, iter [04000, 05004], lr: 0.072160, loss: 2.5807
2022-03-03 00:53:17 - train: epoch 0075, iter [04100, 05004], lr: 0.072160, loss: 2.3841
2022-03-03 00:53:49 - train: epoch 0075, iter [04200, 05004], lr: 0.072160, loss: 2.5597
2022-03-03 00:54:21 - train: epoch 0075, iter [04300, 05004], lr: 0.072160, loss: 2.7062
2022-03-03 00:54:53 - train: epoch 0075, iter [04400, 05004], lr: 0.072160, loss: 2.5562
2022-03-03 00:55:25 - train: epoch 0075, iter [04500, 05004], lr: 0.072160, loss: 2.3921
2022-03-03 00:55:56 - train: epoch 0075, iter [04600, 05004], lr: 0.072160, loss: 2.8342
2022-03-03 00:56:28 - train: epoch 0075, iter [04700, 05004], lr: 0.072160, loss: 2.6834
2022-03-03 00:57:00 - train: epoch 0075, iter [04800, 05004], lr: 0.072160, loss: 2.3975
2022-03-03 00:57:33 - train: epoch 0075, iter [04900, 05004], lr: 0.072160, loss: 2.5385
2022-03-03 00:58:04 - train: epoch 0075, iter [05000, 05004], lr: 0.072160, loss: 2.3652
2022-03-03 00:58:05 - train: epoch 075, train_loss: 2.6005
2022-03-03 00:59:15 - eval: epoch: 075, acc1: 57.294%, acc5: 81.944%, test_loss: 1.7981, per_image_load_time: 0.807ms, per_image_inference_time: 0.349ms
2022-03-03 00:59:15 - until epoch: 075, best_acc1: 57.294%
2022-03-03 00:59:15 - epoch 076 lr: 0.0714346280701527
2022-03-03 00:59:52 - train: epoch 0076, iter [00100, 05004], lr: 0.071435, loss: 2.5233
2022-03-03 01:00:23 - train: epoch 0076, iter [00200, 05004], lr: 0.071435, loss: 2.5788
2022-03-03 01:00:55 - train: epoch 0076, iter [00300, 05004], lr: 0.071435, loss: 2.7405
2022-03-03 01:01:26 - train: epoch 0076, iter [00400, 05004], lr: 0.071435, loss: 2.5891
2022-03-03 01:01:58 - train: epoch 0076, iter [00500, 05004], lr: 0.071435, loss: 2.5117
2022-03-03 01:02:30 - train: epoch 0076, iter [00600, 05004], lr: 0.071435, loss: 2.6063
2022-03-03 01:03:02 - train: epoch 0076, iter [00700, 05004], lr: 0.071435, loss: 2.4420
2022-03-03 01:03:34 - train: epoch 0076, iter [00800, 05004], lr: 0.071435, loss: 2.6388
2022-03-03 01:04:07 - train: epoch 0076, iter [00900, 05004], lr: 0.071435, loss: 2.6969
2022-03-03 01:04:39 - train: epoch 0076, iter [01000, 05004], lr: 0.071435, loss: 2.6860
2022-03-03 01:05:10 - train: epoch 0076, iter [01100, 05004], lr: 0.071435, loss: 2.4852
2022-03-03 01:05:42 - train: epoch 0076, iter [01200, 05004], lr: 0.071435, loss: 2.4728
2022-03-03 01:06:14 - train: epoch 0076, iter [01300, 05004], lr: 0.071435, loss: 2.5046
2022-03-03 01:06:46 - train: epoch 0076, iter [01400, 05004], lr: 0.071435, loss: 2.5100
2022-03-03 01:07:18 - train: epoch 0076, iter [01500, 05004], lr: 0.071435, loss: 2.7432
2022-03-03 01:07:50 - train: epoch 0076, iter [01600, 05004], lr: 0.071435, loss: 2.4574
2022-03-03 01:08:21 - train: epoch 0076, iter [01700, 05004], lr: 0.071435, loss: 2.6707
2022-03-03 01:08:54 - train: epoch 0076, iter [01800, 05004], lr: 0.071435, loss: 2.4681
2022-03-03 01:09:26 - train: epoch 0076, iter [01900, 05004], lr: 0.071435, loss: 2.5292
2022-03-03 01:09:58 - train: epoch 0076, iter [02000, 05004], lr: 0.071435, loss: 2.6259
2022-03-03 01:10:29 - train: epoch 0076, iter [02100, 05004], lr: 0.071435, loss: 2.6207
2022-03-03 01:11:01 - train: epoch 0076, iter [02200, 05004], lr: 0.071435, loss: 2.5280
2022-03-03 01:11:33 - train: epoch 0076, iter [02300, 05004], lr: 0.071435, loss: 2.6643
2022-03-03 01:12:04 - train: epoch 0076, iter [02400, 05004], lr: 0.071435, loss: 2.5584
2022-03-03 01:12:37 - train: epoch 0076, iter [02500, 05004], lr: 0.071435, loss: 2.5974
2022-03-03 01:13:08 - train: epoch 0076, iter [02600, 05004], lr: 0.071435, loss: 2.8996
2022-03-03 01:13:40 - train: epoch 0076, iter [02700, 05004], lr: 0.071435, loss: 2.6019
2022-03-03 01:14:13 - train: epoch 0076, iter [02800, 05004], lr: 0.071435, loss: 2.4420
2022-03-03 01:14:45 - train: epoch 0076, iter [02900, 05004], lr: 0.071435, loss: 2.5970
2022-03-03 01:15:17 - train: epoch 0076, iter [03000, 05004], lr: 0.071435, loss: 2.5126
2022-03-03 01:15:48 - train: epoch 0076, iter [03100, 05004], lr: 0.071435, loss: 2.7696
2022-03-03 01:16:20 - train: epoch 0076, iter [03200, 05004], lr: 0.071435, loss: 2.4182
2022-03-03 01:16:53 - train: epoch 0076, iter [03300, 05004], lr: 0.071435, loss: 2.7771
2022-03-03 01:17:24 - train: epoch 0076, iter [03400, 05004], lr: 0.071435, loss: 2.6578
2022-03-03 01:17:57 - train: epoch 0076, iter [03500, 05004], lr: 0.071435, loss: 2.5768
2022-03-03 01:18:28 - train: epoch 0076, iter [03600, 05004], lr: 0.071435, loss: 2.4144
2022-03-03 01:19:00 - train: epoch 0076, iter [03700, 05004], lr: 0.071435, loss: 2.3755
2022-03-03 01:19:32 - train: epoch 0076, iter [03800, 05004], lr: 0.071435, loss: 2.4011
2022-03-03 01:20:03 - train: epoch 0076, iter [03900, 05004], lr: 0.071435, loss: 2.4282
2022-03-03 01:20:35 - train: epoch 0076, iter [04000, 05004], lr: 0.071435, loss: 2.6718
2022-03-03 01:21:07 - train: epoch 0076, iter [04100, 05004], lr: 0.071435, loss: 2.8691
2022-03-03 01:21:39 - train: epoch 0076, iter [04200, 05004], lr: 0.071435, loss: 2.5313
2022-03-03 01:22:11 - train: epoch 0076, iter [04300, 05004], lr: 0.071435, loss: 2.5480
2022-03-03 01:22:43 - train: epoch 0076, iter [04400, 05004], lr: 0.071435, loss: 2.7987
2022-03-03 01:23:16 - train: epoch 0076, iter [04500, 05004], lr: 0.071435, loss: 2.6048
2022-03-03 01:23:46 - train: epoch 0076, iter [04600, 05004], lr: 0.071435, loss: 2.6965
2022-03-03 01:24:19 - train: epoch 0076, iter [04700, 05004], lr: 0.071435, loss: 2.8304
2022-03-03 01:24:50 - train: epoch 0076, iter [04800, 05004], lr: 0.071435, loss: 2.4888
2022-03-03 01:25:22 - train: epoch 0076, iter [04900, 05004], lr: 0.071435, loss: 2.5940
2022-03-03 01:25:53 - train: epoch 0076, iter [05000, 05004], lr: 0.071435, loss: 2.4714
2022-03-03 01:25:54 - train: epoch 076, train_loss: 2.5916
2022-03-03 01:27:05 - eval: epoch: 076, acc1: 54.844%, acc5: 79.654%, test_loss: 1.9337, per_image_load_time: 1.016ms, per_image_inference_time: 0.380ms
2022-03-03 01:27:06 - until epoch: 076, best_acc1: 57.294%
2022-03-03 01:27:06 - epoch 077 lr: 0.0707041155014006
2022-03-03 01:27:43 - train: epoch 0077, iter [00100, 05004], lr: 0.070704, loss: 2.7890
2022-03-03 01:28:16 - train: epoch 0077, iter [00200, 05004], lr: 0.070704, loss: 2.3766
2022-03-03 01:28:48 - train: epoch 0077, iter [00300, 05004], lr: 0.070704, loss: 2.6085
2022-03-03 01:29:20 - train: epoch 0077, iter [00400, 05004], lr: 0.070704, loss: 2.7010
2022-03-03 01:29:52 - train: epoch 0077, iter [00500, 05004], lr: 0.070704, loss: 2.3656
2022-03-03 01:30:23 - train: epoch 0077, iter [00600, 05004], lr: 0.070704, loss: 2.2880
2022-03-03 01:30:55 - train: epoch 0077, iter [00700, 05004], lr: 0.070704, loss: 2.4942
2022-03-03 01:31:27 - train: epoch 0077, iter [00800, 05004], lr: 0.070704, loss: 2.4805
2022-03-03 01:31:59 - train: epoch 0077, iter [00900, 05004], lr: 0.070704, loss: 2.7408
2022-03-03 01:32:31 - train: epoch 0077, iter [01000, 05004], lr: 0.070704, loss: 2.4979
2022-03-03 01:33:03 - train: epoch 0077, iter [01100, 05004], lr: 0.070704, loss: 2.6296
2022-03-03 01:33:35 - train: epoch 0077, iter [01200, 05004], lr: 0.070704, loss: 2.9402
2022-03-03 01:34:08 - train: epoch 0077, iter [01300, 05004], lr: 0.070704, loss: 2.4319
2022-03-03 01:34:39 - train: epoch 0077, iter [01400, 05004], lr: 0.070704, loss: 2.4745
2022-03-03 01:35:12 - train: epoch 0077, iter [01500, 05004], lr: 0.070704, loss: 2.4972
2022-03-03 01:35:43 - train: epoch 0077, iter [01600, 05004], lr: 0.070704, loss: 2.6113
2022-03-03 01:36:15 - train: epoch 0077, iter [01700, 05004], lr: 0.070704, loss: 2.9888
2022-03-03 01:36:47 - train: epoch 0077, iter [01800, 05004], lr: 0.070704, loss: 2.5321
2022-03-03 01:37:20 - train: epoch 0077, iter [01900, 05004], lr: 0.070704, loss: 2.4224
2022-03-03 01:37:52 - train: epoch 0077, iter [02000, 05004], lr: 0.070704, loss: 2.3299
2022-03-03 01:38:24 - train: epoch 0077, iter [02100, 05004], lr: 0.070704, loss: 2.8325
2022-03-03 01:38:56 - train: epoch 0077, iter [02200, 05004], lr: 0.070704, loss: 2.5543
2022-03-03 01:39:28 - train: epoch 0077, iter [02300, 05004], lr: 0.070704, loss: 2.7091
2022-03-03 01:40:00 - train: epoch 0077, iter [02400, 05004], lr: 0.070704, loss: 2.4619
2022-03-03 01:40:32 - train: epoch 0077, iter [02500, 05004], lr: 0.070704, loss: 2.7376
2022-03-03 01:41:05 - train: epoch 0077, iter [02600, 05004], lr: 0.070704, loss: 2.3665
2022-03-03 01:41:37 - train: epoch 0077, iter [02700, 05004], lr: 0.070704, loss: 2.6633
2022-03-03 01:42:09 - train: epoch 0077, iter [02800, 05004], lr: 0.070704, loss: 2.8039
2022-03-03 01:42:41 - train: epoch 0077, iter [02900, 05004], lr: 0.070704, loss: 2.7363
2022-03-03 01:43:13 - train: epoch 0077, iter [03000, 05004], lr: 0.070704, loss: 2.4181
2022-03-03 01:43:45 - train: epoch 0077, iter [03100, 05004], lr: 0.070704, loss: 2.4977
2022-03-03 01:44:18 - train: epoch 0077, iter [03200, 05004], lr: 0.070704, loss: 2.8383
2022-03-03 01:44:50 - train: epoch 0077, iter [03300, 05004], lr: 0.070704, loss: 2.4211
2022-03-03 01:45:22 - train: epoch 0077, iter [03400, 05004], lr: 0.070704, loss: 2.7670
2022-03-03 01:45:54 - train: epoch 0077, iter [03500, 05004], lr: 0.070704, loss: 2.4862
2022-03-03 01:46:27 - train: epoch 0077, iter [03600, 05004], lr: 0.070704, loss: 2.7563
2022-03-03 01:46:58 - train: epoch 0077, iter [03700, 05004], lr: 0.070704, loss: 2.7566
2022-03-03 01:47:31 - train: epoch 0077, iter [03800, 05004], lr: 0.070704, loss: 2.9267
2022-03-03 01:48:03 - train: epoch 0077, iter [03900, 05004], lr: 0.070704, loss: 2.6322
2022-03-03 01:48:35 - train: epoch 0077, iter [04000, 05004], lr: 0.070704, loss: 2.7181
2022-03-03 01:49:06 - train: epoch 0077, iter [04100, 05004], lr: 0.070704, loss: 2.8172
2022-03-03 01:49:39 - train: epoch 0077, iter [04200, 05004], lr: 0.070704, loss: 2.6616
2022-03-03 01:50:11 - train: epoch 0077, iter [04300, 05004], lr: 0.070704, loss: 2.5259
2022-03-03 01:50:44 - train: epoch 0077, iter [04400, 05004], lr: 0.070704, loss: 2.5012
2022-03-03 01:51:16 - train: epoch 0077, iter [04500, 05004], lr: 0.070704, loss: 2.9006
2022-03-03 01:51:48 - train: epoch 0077, iter [04600, 05004], lr: 0.070704, loss: 2.6827
2022-03-03 01:52:19 - train: epoch 0077, iter [04700, 05004], lr: 0.070704, loss: 2.7079
2022-03-03 01:52:51 - train: epoch 0077, iter [04800, 05004], lr: 0.070704, loss: 2.4061
2022-03-03 01:53:23 - train: epoch 0077, iter [04900, 05004], lr: 0.070704, loss: 2.7366
2022-03-03 01:53:54 - train: epoch 0077, iter [05000, 05004], lr: 0.070704, loss: 2.4752
2022-03-03 01:53:55 - train: epoch 077, train_loss: 2.5799
2022-03-03 01:55:05 - eval: epoch: 077, acc1: 54.390%, acc5: 79.578%, test_loss: 1.9446, per_image_load_time: 0.532ms, per_image_inference_time: 0.360ms
2022-03-03 01:55:06 - until epoch: 077, best_acc1: 57.294%
2022-03-03 01:55:06 - epoch 078 lr: 0.06996822917828477
2022-03-03 01:55:43 - train: epoch 0078, iter [00100, 05004], lr: 0.069968, loss: 2.5447
2022-03-03 01:56:15 - train: epoch 0078, iter [00200, 05004], lr: 0.069968, loss: 2.8829
2022-03-03 01:56:47 - train: epoch 0078, iter [00300, 05004], lr: 0.069968, loss: 2.5766
2022-03-03 01:57:20 - train: epoch 0078, iter [00400, 05004], lr: 0.069968, loss: 2.5385
2022-03-03 01:57:52 - train: epoch 0078, iter [00500, 05004], lr: 0.069968, loss: 2.5468
2022-03-03 01:58:23 - train: epoch 0078, iter [00600, 05004], lr: 0.069968, loss: 2.3182
2022-03-03 01:58:54 - train: epoch 0078, iter [00700, 05004], lr: 0.069968, loss: 2.7911
2022-03-03 01:59:27 - train: epoch 0078, iter [00800, 05004], lr: 0.069968, loss: 2.5383
2022-03-03 01:59:59 - train: epoch 0078, iter [00900, 05004], lr: 0.069968, loss: 2.4562
2022-03-03 02:00:31 - train: epoch 0078, iter [01000, 05004], lr: 0.069968, loss: 2.6022
2022-03-03 02:01:02 - train: epoch 0078, iter [01100, 05004], lr: 0.069968, loss: 2.5742
2022-03-03 02:01:35 - train: epoch 0078, iter [01200, 05004], lr: 0.069968, loss: 2.6700
2022-03-03 02:02:07 - train: epoch 0078, iter [01300, 05004], lr: 0.069968, loss: 2.3750
2022-03-03 02:02:39 - train: epoch 0078, iter [01400, 05004], lr: 0.069968, loss: 2.2139
2022-03-03 02:03:10 - train: epoch 0078, iter [01500, 05004], lr: 0.069968, loss: 2.5382
2022-03-03 02:03:42 - train: epoch 0078, iter [01600, 05004], lr: 0.069968, loss: 2.5961
2022-03-03 02:04:15 - train: epoch 0078, iter [01700, 05004], lr: 0.069968, loss: 2.5982
2022-03-03 02:04:47 - train: epoch 0078, iter [01800, 05004], lr: 0.069968, loss: 2.7020
2022-03-03 02:05:19 - train: epoch 0078, iter [01900, 05004], lr: 0.069968, loss: 2.4638
2022-03-03 02:05:51 - train: epoch 0078, iter [02000, 05004], lr: 0.069968, loss: 2.6208
2022-03-03 02:06:23 - train: epoch 0078, iter [02100, 05004], lr: 0.069968, loss: 2.8827
2022-03-03 02:06:55 - train: epoch 0078, iter [02200, 05004], lr: 0.069968, loss: 2.7228
2022-03-03 02:07:27 - train: epoch 0078, iter [02300, 05004], lr: 0.069968, loss: 2.4807
2022-03-03 02:07:59 - train: epoch 0078, iter [02400, 05004], lr: 0.069968, loss: 2.6813
2022-03-03 02:08:31 - train: epoch 0078, iter [02500, 05004], lr: 0.069968, loss: 2.4348
2022-03-03 02:09:04 - train: epoch 0078, iter [02600, 05004], lr: 0.069968, loss: 2.6141
2022-03-03 02:09:35 - train: epoch 0078, iter [02700, 05004], lr: 0.069968, loss: 2.6833
2022-03-03 02:10:07 - train: epoch 0078, iter [02800, 05004], lr: 0.069968, loss: 2.5730
2022-03-03 02:10:40 - train: epoch 0078, iter [02900, 05004], lr: 0.069968, loss: 2.6358
2022-03-03 02:11:12 - train: epoch 0078, iter [03000, 05004], lr: 0.069968, loss: 2.8624
2022-03-03 02:11:44 - train: epoch 0078, iter [03100, 05004], lr: 0.069968, loss: 2.8753
2022-03-03 02:12:16 - train: epoch 0078, iter [03200, 05004], lr: 0.069968, loss: 2.8798
2022-03-03 02:12:47 - train: epoch 0078, iter [03300, 05004], lr: 0.069968, loss: 2.8905
2022-03-03 02:13:18 - train: epoch 0078, iter [03400, 05004], lr: 0.069968, loss: 2.5529
2022-03-03 02:13:51 - train: epoch 0078, iter [03500, 05004], lr: 0.069968, loss: 2.5009
2022-03-03 02:14:23 - train: epoch 0078, iter [03600, 05004], lr: 0.069968, loss: 2.3926
2022-03-03 02:14:55 - train: epoch 0078, iter [03700, 05004], lr: 0.069968, loss: 2.4821
2022-03-03 02:15:27 - train: epoch 0078, iter [03800, 05004], lr: 0.069968, loss: 2.5678
2022-03-03 02:15:59 - train: epoch 0078, iter [03900, 05004], lr: 0.069968, loss: 2.5142
2022-03-03 02:16:32 - train: epoch 0078, iter [04000, 05004], lr: 0.069968, loss: 2.6985
2022-03-03 02:17:05 - train: epoch 0078, iter [04100, 05004], lr: 0.069968, loss: 2.7698
2022-03-03 02:17:37 - train: epoch 0078, iter [04200, 05004], lr: 0.069968, loss: 2.7642
2022-03-03 02:18:10 - train: epoch 0078, iter [04300, 05004], lr: 0.069968, loss: 2.6363
2022-03-03 02:18:41 - train: epoch 0078, iter [04400, 05004], lr: 0.069968, loss: 2.4605
2022-03-03 02:19:13 - train: epoch 0078, iter [04500, 05004], lr: 0.069968, loss: 2.6588
2022-03-03 02:19:45 - train: epoch 0078, iter [04600, 05004], lr: 0.069968, loss: 2.2896
2022-03-03 02:20:18 - train: epoch 0078, iter [04700, 05004], lr: 0.069968, loss: 2.6748
2022-03-03 02:20:50 - train: epoch 0078, iter [04800, 05004], lr: 0.069968, loss: 2.7729
2022-03-03 02:21:24 - train: epoch 0078, iter [04900, 05004], lr: 0.069968, loss: 2.8258
2022-03-03 02:21:55 - train: epoch 0078, iter [05000, 05004], lr: 0.069968, loss: 2.7569
2022-03-03 02:21:57 - train: epoch 078, train_loss: 2.5815
2022-03-03 02:23:10 - eval: epoch: 078, acc1: 54.444%, acc5: 79.642%, test_loss: 1.9312, per_image_load_time: 0.632ms, per_image_inference_time: 0.450ms
2022-03-03 02:23:10 - until epoch: 078, best_acc1: 57.294%
2022-03-03 02:23:10 - epoch 079 lr: 0.06922716010014256
2022-03-03 02:23:48 - train: epoch 0079, iter [00100, 05004], lr: 0.069227, loss: 2.4035
2022-03-03 02:24:21 - train: epoch 0079, iter [00200, 05004], lr: 0.069227, loss: 2.6183
2022-03-03 02:24:54 - train: epoch 0079, iter [00300, 05004], lr: 0.069227, loss: 2.6011
2022-03-03 02:25:27 - train: epoch 0079, iter [00400, 05004], lr: 0.069227, loss: 2.7049
2022-03-03 02:25:59 - train: epoch 0079, iter [00500, 05004], lr: 0.069227, loss: 2.5640
2022-03-03 02:26:33 - train: epoch 0079, iter [00600, 05004], lr: 0.069227, loss: 2.4636
2022-03-03 02:27:04 - train: epoch 0079, iter [00700, 05004], lr: 0.069227, loss: 2.5593
2022-03-03 02:27:37 - train: epoch 0079, iter [00800, 05004], lr: 0.069227, loss: 2.5494
2022-03-03 02:28:09 - train: epoch 0079, iter [00900, 05004], lr: 0.069227, loss: 2.6285
2022-03-03 02:28:42 - train: epoch 0079, iter [01000, 05004], lr: 0.069227, loss: 2.7415
2022-03-03 02:29:14 - train: epoch 0079, iter [01100, 05004], lr: 0.069227, loss: 2.7671
2022-03-03 02:29:47 - train: epoch 0079, iter [01200, 05004], lr: 0.069227, loss: 2.7613
2022-03-03 02:30:19 - train: epoch 0079, iter [01300, 05004], lr: 0.069227, loss: 2.3470
2022-03-03 02:30:52 - train: epoch 0079, iter [01400, 05004], lr: 0.069227, loss: 2.5634
2022-03-03 02:31:25 - train: epoch 0079, iter [01500, 05004], lr: 0.069227, loss: 2.9294
2022-03-03 02:31:58 - train: epoch 0079, iter [01600, 05004], lr: 0.069227, loss: 2.4725
2022-03-03 02:32:31 - train: epoch 0079, iter [01700, 05004], lr: 0.069227, loss: 2.6569
2022-03-03 02:33:03 - train: epoch 0079, iter [01800, 05004], lr: 0.069227, loss: 2.5408
2022-03-03 02:33:35 - train: epoch 0079, iter [01900, 05004], lr: 0.069227, loss: 2.5797
2022-03-03 02:34:09 - train: epoch 0079, iter [02000, 05004], lr: 0.069227, loss: 2.8348
2022-03-03 02:34:41 - train: epoch 0079, iter [02100, 05004], lr: 0.069227, loss: 2.4935
2022-03-03 02:35:13 - train: epoch 0079, iter [02200, 05004], lr: 0.069227, loss: 2.5062
2022-03-03 02:35:46 - train: epoch 0079, iter [02300, 05004], lr: 0.069227, loss: 2.6885
2022-03-03 02:36:19 - train: epoch 0079, iter [02400, 05004], lr: 0.069227, loss: 2.4067
2022-03-03 02:36:51 - train: epoch 0079, iter [02500, 05004], lr: 0.069227, loss: 2.6879
2022-03-03 02:37:23 - train: epoch 0079, iter [02600, 05004], lr: 0.069227, loss: 2.4542
2022-03-03 02:37:56 - train: epoch 0079, iter [02700, 05004], lr: 0.069227, loss: 2.1515
2022-03-03 02:38:29 - train: epoch 0079, iter [02800, 05004], lr: 0.069227, loss: 2.5276
2022-03-03 02:39:01 - train: epoch 0079, iter [02900, 05004], lr: 0.069227, loss: 2.7745
2022-03-03 02:39:34 - train: epoch 0079, iter [03000, 05004], lr: 0.069227, loss: 2.7031
2022-03-03 02:40:06 - train: epoch 0079, iter [03100, 05004], lr: 0.069227, loss: 2.6896
2022-03-03 02:40:38 - train: epoch 0079, iter [03200, 05004], lr: 0.069227, loss: 2.7991
2022-03-03 02:41:10 - train: epoch 0079, iter [03300, 05004], lr: 0.069227, loss: 2.4768
2022-03-03 02:41:42 - train: epoch 0079, iter [03400, 05004], lr: 0.069227, loss: 2.5865
2022-03-03 02:42:15 - train: epoch 0079, iter [03500, 05004], lr: 0.069227, loss: 2.5874
2022-03-03 02:42:48 - train: epoch 0079, iter [03600, 05004], lr: 0.069227, loss: 2.4458
2022-03-03 02:43:21 - train: epoch 0079, iter [03700, 05004], lr: 0.069227, loss: 2.3133
2022-03-03 02:43:54 - train: epoch 0079, iter [03800, 05004], lr: 0.069227, loss: 2.5835
2022-03-03 02:44:26 - train: epoch 0079, iter [03900, 05004], lr: 0.069227, loss: 2.6298
2022-03-03 02:44:59 - train: epoch 0079, iter [04000, 05004], lr: 0.069227, loss: 2.5516
2022-03-03 02:45:30 - train: epoch 0079, iter [04100, 05004], lr: 0.069227, loss: 2.8952
2022-03-03 02:46:04 - train: epoch 0079, iter [04200, 05004], lr: 0.069227, loss: 2.5593
2022-03-03 02:46:35 - train: epoch 0079, iter [04300, 05004], lr: 0.069227, loss: 2.6317
2022-03-03 02:47:07 - train: epoch 0079, iter [04400, 05004], lr: 0.069227, loss: 2.6764
2022-03-03 02:47:40 - train: epoch 0079, iter [04500, 05004], lr: 0.069227, loss: 2.6534
2022-03-03 02:48:13 - train: epoch 0079, iter [04600, 05004], lr: 0.069227, loss: 2.7765
2022-03-03 02:48:46 - train: epoch 0079, iter [04700, 05004], lr: 0.069227, loss: 2.4215
2022-03-03 02:49:19 - train: epoch 0079, iter [04800, 05004], lr: 0.069227, loss: 2.7194
2022-03-03 02:49:51 - train: epoch 0079, iter [04900, 05004], lr: 0.069227, loss: 2.7972
2022-03-03 02:50:23 - train: epoch 0079, iter [05000, 05004], lr: 0.069227, loss: 2.5536
2022-03-03 02:50:24 - train: epoch 079, train_loss: 2.5739
2022-03-03 02:51:35 - eval: epoch: 079, acc1: 56.354%, acc5: 81.584%, test_loss: 1.8221, per_image_load_time: 0.599ms, per_image_inference_time: 0.394ms
2022-03-03 02:51:36 - until epoch: 079, best_acc1: 57.294%
2022-03-03 02:51:36 - epoch 080 lr: 0.06848110061149555
2022-03-03 02:52:13 - train: epoch 0080, iter [00100, 05004], lr: 0.068481, loss: 2.6267
2022-03-03 02:52:46 - train: epoch 0080, iter [00200, 05004], lr: 0.068481, loss: 2.5686
2022-03-03 02:53:18 - train: epoch 0080, iter [00300, 05004], lr: 0.068481, loss: 2.7484
2022-03-03 02:53:51 - train: epoch 0080, iter [00400, 05004], lr: 0.068481, loss: 2.3310
2022-03-03 02:54:23 - train: epoch 0080, iter [00500, 05004], lr: 0.068481, loss: 2.3936
2022-03-03 02:54:56 - train: epoch 0080, iter [00600, 05004], lr: 0.068481, loss: 2.3640
2022-03-03 02:55:28 - train: epoch 0080, iter [00700, 05004], lr: 0.068481, loss: 2.4252
2022-03-03 02:56:01 - train: epoch 0080, iter [00800, 05004], lr: 0.068481, loss: 2.3029
2022-03-03 02:56:34 - train: epoch 0080, iter [00900, 05004], lr: 0.068481, loss: 2.5171
2022-03-03 02:57:07 - train: epoch 0080, iter [01000, 05004], lr: 0.068481, loss: 2.4808
2022-03-03 02:57:38 - train: epoch 0080, iter [01100, 05004], lr: 0.068481, loss: 2.4306
2022-03-03 02:58:11 - train: epoch 0080, iter [01200, 05004], lr: 0.068481, loss: 2.5917
2022-03-03 02:58:44 - train: epoch 0080, iter [01300, 05004], lr: 0.068481, loss: 2.3810
2022-03-03 02:59:16 - train: epoch 0080, iter [01400, 05004], lr: 0.068481, loss: 2.5129
2022-03-03 02:59:49 - train: epoch 0080, iter [01500, 05004], lr: 0.068481, loss: 2.5682
2022-03-03 03:00:22 - train: epoch 0080, iter [01600, 05004], lr: 0.068481, loss: 2.6191
2022-03-03 03:00:54 - train: epoch 0080, iter [01700, 05004], lr: 0.068481, loss: 2.4760
2022-03-03 03:01:27 - train: epoch 0080, iter [01800, 05004], lr: 0.068481, loss: 2.5691
2022-03-03 03:01:59 - train: epoch 0080, iter [01900, 05004], lr: 0.068481, loss: 2.8135
2022-03-03 03:02:31 - train: epoch 0080, iter [02000, 05004], lr: 0.068481, loss: 2.4247
2022-03-03 03:03:04 - train: epoch 0080, iter [02100, 05004], lr: 0.068481, loss: 2.6873
2022-03-03 03:03:36 - train: epoch 0080, iter [02200, 05004], lr: 0.068481, loss: 2.7465
2022-03-03 03:04:08 - train: epoch 0080, iter [02300, 05004], lr: 0.068481, loss: 2.5805
2022-03-03 03:04:41 - train: epoch 0080, iter [02400, 05004], lr: 0.068481, loss: 2.9431
2022-03-03 03:05:14 - train: epoch 0080, iter [02500, 05004], lr: 0.068481, loss: 2.3287
2022-03-03 03:05:47 - train: epoch 0080, iter [02600, 05004], lr: 0.068481, loss: 2.5688
2022-03-03 03:06:19 - train: epoch 0080, iter [02700, 05004], lr: 0.068481, loss: 2.4195
2022-03-03 03:06:52 - train: epoch 0080, iter [02800, 05004], lr: 0.068481, loss: 2.6549
2022-03-03 03:07:25 - train: epoch 0080, iter [02900, 05004], lr: 0.068481, loss: 2.8285
2022-03-03 03:07:57 - train: epoch 0080, iter [03000, 05004], lr: 0.068481, loss: 2.7106
2022-03-03 03:08:29 - train: epoch 0080, iter [03100, 05004], lr: 0.068481, loss: 2.5161
2022-03-03 03:09:01 - train: epoch 0080, iter [03200, 05004], lr: 0.068481, loss: 2.2307
2022-03-03 03:09:34 - train: epoch 0080, iter [03300, 05004], lr: 0.068481, loss: 2.4617
2022-03-03 03:10:07 - train: epoch 0080, iter [03400, 05004], lr: 0.068481, loss: 2.3826
2022-03-03 03:10:39 - train: epoch 0080, iter [03500, 05004], lr: 0.068481, loss: 2.5415
2022-03-03 03:11:12 - train: epoch 0080, iter [03600, 05004], lr: 0.068481, loss: 2.7349
2022-03-03 03:11:44 - train: epoch 0080, iter [03700, 05004], lr: 0.068481, loss: 2.3914
2022-03-03 03:12:17 - train: epoch 0080, iter [03800, 05004], lr: 0.068481, loss: 2.7833
2022-03-03 03:12:50 - train: epoch 0080, iter [03900, 05004], lr: 0.068481, loss: 2.4942
2022-03-03 03:13:21 - train: epoch 0080, iter [04000, 05004], lr: 0.068481, loss: 2.7924
2022-03-03 03:13:55 - train: epoch 0080, iter [04100, 05004], lr: 0.068481, loss: 2.7963
2022-03-03 03:14:26 - train: epoch 0080, iter [04200, 05004], lr: 0.068481, loss: 2.4706
2022-03-03 03:14:59 - train: epoch 0080, iter [04300, 05004], lr: 0.068481, loss: 2.9015
2022-03-03 03:15:30 - train: epoch 0080, iter [04400, 05004], lr: 0.068481, loss: 2.7133
2022-03-03 03:16:03 - train: epoch 0080, iter [04500, 05004], lr: 0.068481, loss: 2.3833
2022-03-03 03:16:35 - train: epoch 0080, iter [04600, 05004], lr: 0.068481, loss: 2.4923
2022-03-03 03:17:08 - train: epoch 0080, iter [04700, 05004], lr: 0.068481, loss: 2.8169
2022-03-03 03:17:41 - train: epoch 0080, iter [04800, 05004], lr: 0.068481, loss: 2.8104
2022-03-03 03:18:13 - train: epoch 0080, iter [04900, 05004], lr: 0.068481, loss: 2.8127
2022-03-03 03:18:44 - train: epoch 0080, iter [05000, 05004], lr: 0.068481, loss: 2.2547
2022-03-03 03:18:45 - train: epoch 080, train_loss: 2.5657
2022-03-03 03:19:57 - eval: epoch: 080, acc1: 56.992%, acc5: 81.280%, test_loss: 1.8100, per_image_load_time: 0.514ms, per_image_inference_time: 0.396ms
2022-03-03 03:19:57 - until epoch: 080, best_acc1: 57.294%
2022-03-03 03:19:57 - epoch 081 lr: 0.06773024435212678
2022-03-03 03:20:35 - train: epoch 0081, iter [00100, 05004], lr: 0.067730, loss: 2.4175
2022-03-03 03:21:08 - train: epoch 0081, iter [00200, 05004], lr: 0.067730, loss: 2.4885
2022-03-03 03:21:41 - train: epoch 0081, iter [00300, 05004], lr: 0.067730, loss: 2.7153
2022-03-03 03:22:14 - train: epoch 0081, iter [00400, 05004], lr: 0.067730, loss: 2.8207
2022-03-03 03:22:45 - train: epoch 0081, iter [00500, 05004], lr: 0.067730, loss: 2.8230
2022-03-03 03:23:18 - train: epoch 0081, iter [00600, 05004], lr: 0.067730, loss: 2.4101
2022-03-03 03:23:51 - train: epoch 0081, iter [00700, 05004], lr: 0.067730, loss: 2.4168
2022-03-03 03:24:23 - train: epoch 0081, iter [00800, 05004], lr: 0.067730, loss: 2.5535
2022-03-03 03:24:56 - train: epoch 0081, iter [00900, 05004], lr: 0.067730, loss: 2.7317
2022-03-03 03:25:29 - train: epoch 0081, iter [01000, 05004], lr: 0.067730, loss: 2.5057
2022-03-03 03:26:01 - train: epoch 0081, iter [01100, 05004], lr: 0.067730, loss: 2.5297
2022-03-03 03:26:34 - train: epoch 0081, iter [01200, 05004], lr: 0.067730, loss: 2.4752
2022-03-03 03:27:06 - train: epoch 0081, iter [01300, 05004], lr: 0.067730, loss: 2.2632
2022-03-03 03:27:38 - train: epoch 0081, iter [01400, 05004], lr: 0.067730, loss: 2.2973
2022-03-03 03:28:11 - train: epoch 0081, iter [01500, 05004], lr: 0.067730, loss: 2.5110
2022-03-03 03:28:44 - train: epoch 0081, iter [01600, 05004], lr: 0.067730, loss: 2.5889
2022-03-03 03:29:17 - train: epoch 0081, iter [01700, 05004], lr: 0.067730, loss: 2.3531
2022-03-03 03:29:49 - train: epoch 0081, iter [01800, 05004], lr: 0.067730, loss: 2.4904
2022-03-03 03:30:22 - train: epoch 0081, iter [01900, 05004], lr: 0.067730, loss: 2.5440
2022-03-03 03:30:55 - train: epoch 0081, iter [02000, 05004], lr: 0.067730, loss: 2.6134
2022-03-03 03:31:27 - train: epoch 0081, iter [02100, 05004], lr: 0.067730, loss: 2.6869
2022-03-03 03:31:58 - train: epoch 0081, iter [02200, 05004], lr: 0.067730, loss: 2.2677
2022-03-03 03:32:31 - train: epoch 0081, iter [02300, 05004], lr: 0.067730, loss: 2.2852
2022-03-03 03:33:03 - train: epoch 0081, iter [02400, 05004], lr: 0.067730, loss: 2.4382
2022-03-03 03:33:36 - train: epoch 0081, iter [02500, 05004], lr: 0.067730, loss: 2.8148
2022-03-03 03:34:09 - train: epoch 0081, iter [02600, 05004], lr: 0.067730, loss: 2.7680
2022-03-03 03:34:42 - train: epoch 0081, iter [02700, 05004], lr: 0.067730, loss: 2.6387
2022-03-03 03:35:14 - train: epoch 0081, iter [02800, 05004], lr: 0.067730, loss: 2.2599
2022-03-03 03:35:47 - train: epoch 0081, iter [02900, 05004], lr: 0.067730, loss: 2.4541
2022-03-03 03:36:19 - train: epoch 0081, iter [03000, 05004], lr: 0.067730, loss: 2.6287
2022-03-03 03:36:51 - train: epoch 0081, iter [03100, 05004], lr: 0.067730, loss: 2.7179
2022-03-03 03:37:23 - train: epoch 0081, iter [03200, 05004], lr: 0.067730, loss: 2.3656
2022-03-03 03:37:56 - train: epoch 0081, iter [03300, 05004], lr: 0.067730, loss: 2.4953
2022-03-03 03:38:28 - train: epoch 0081, iter [03400, 05004], lr: 0.067730, loss: 2.7350
2022-03-03 03:39:01 - train: epoch 0081, iter [03500, 05004], lr: 0.067730, loss: 2.5493
2022-03-03 03:39:34 - train: epoch 0081, iter [03600, 05004], lr: 0.067730, loss: 2.3363
2022-03-03 03:40:06 - train: epoch 0081, iter [03700, 05004], lr: 0.067730, loss: 2.8731
2022-03-03 03:40:38 - train: epoch 0081, iter [03800, 05004], lr: 0.067730, loss: 2.6016
2022-03-03 03:41:11 - train: epoch 0081, iter [03900, 05004], lr: 0.067730, loss: 2.6545
2022-03-03 03:41:43 - train: epoch 0081, iter [04000, 05004], lr: 0.067730, loss: 2.5266
2022-03-03 03:42:16 - train: epoch 0081, iter [04100, 05004], lr: 0.067730, loss: 2.7630
2022-03-03 03:42:49 - train: epoch 0081, iter [04200, 05004], lr: 0.067730, loss: 2.6179
2022-03-03 03:43:21 - train: epoch 0081, iter [04300, 05004], lr: 0.067730, loss: 2.5382
2022-03-03 03:43:53 - train: epoch 0081, iter [04400, 05004], lr: 0.067730, loss: 2.5273
2022-03-03 03:44:25 - train: epoch 0081, iter [04500, 05004], lr: 0.067730, loss: 2.5780
2022-03-03 03:44:57 - train: epoch 0081, iter [04600, 05004], lr: 0.067730, loss: 2.4343
2022-03-03 03:45:30 - train: epoch 0081, iter [04700, 05004], lr: 0.067730, loss: 2.5887
2022-03-03 03:46:02 - train: epoch 0081, iter [04800, 05004], lr: 0.067730, loss: 2.5556
2022-03-03 03:46:35 - train: epoch 0081, iter [04900, 05004], lr: 0.067730, loss: 2.1953
2022-03-03 03:47:06 - train: epoch 0081, iter [05000, 05004], lr: 0.067730, loss: 2.6223
2022-03-03 03:47:07 - train: epoch 081, train_loss: 2.5632
2022-03-03 03:48:19 - eval: epoch: 081, acc1: 58.538%, acc5: 82.798%, test_loss: 1.7389, per_image_load_time: 0.495ms, per_image_inference_time: 0.416ms
2022-03-03 03:48:20 - until epoch: 081, best_acc1: 58.538%
2022-03-03 03:48:20 - epoch 082 lr: 0.06697478620682136
2022-03-03 03:48:57 - train: epoch 0082, iter [00100, 05004], lr: 0.066975, loss: 2.3523
2022-03-03 03:49:29 - train: epoch 0082, iter [00200, 05004], lr: 0.066975, loss: 2.5797
2022-03-03 03:50:02 - train: epoch 0082, iter [00300, 05004], lr: 0.066975, loss: 2.5673
2022-03-03 03:50:34 - train: epoch 0082, iter [00400, 05004], lr: 0.066975, loss: 2.5777
2022-03-03 03:51:07 - train: epoch 0082, iter [00500, 05004], lr: 0.066975, loss: 2.8186
2022-03-03 03:51:40 - train: epoch 0082, iter [00600, 05004], lr: 0.066975, loss: 2.6244
2022-03-03 03:52:12 - train: epoch 0082, iter [00700, 05004], lr: 0.066975, loss: 2.7677
2022-03-03 03:52:45 - train: epoch 0082, iter [00800, 05004], lr: 0.066975, loss: 2.6213
2022-03-03 03:53:17 - train: epoch 0082, iter [00900, 05004], lr: 0.066975, loss: 2.7920
2022-03-03 03:53:49 - train: epoch 0082, iter [01000, 05004], lr: 0.066975, loss: 2.5708
2022-03-03 03:54:22 - train: epoch 0082, iter [01100, 05004], lr: 0.066975, loss: 2.5858
2022-03-03 03:54:54 - train: epoch 0082, iter [01200, 05004], lr: 0.066975, loss: 2.6888
2022-03-03 03:55:26 - train: epoch 0082, iter [01300, 05004], lr: 0.066975, loss: 2.7382
2022-03-03 03:55:59 - train: epoch 0082, iter [01400, 05004], lr: 0.066975, loss: 2.3718
2022-03-03 03:56:31 - train: epoch 0082, iter [01500, 05004], lr: 0.066975, loss: 2.8023
2022-03-03 03:57:05 - train: epoch 0082, iter [01600, 05004], lr: 0.066975, loss: 2.4289
2022-03-03 03:57:37 - train: epoch 0082, iter [01700, 05004], lr: 0.066975, loss: 2.5765
2022-03-03 03:58:09 - train: epoch 0082, iter [01800, 05004], lr: 0.066975, loss: 2.3851
2022-03-03 03:58:42 - train: epoch 0082, iter [01900, 05004], lr: 0.066975, loss: 2.5109
2022-03-03 03:59:15 - train: epoch 0082, iter [02000, 05004], lr: 0.066975, loss: 2.4734
2022-03-03 03:59:47 - train: epoch 0082, iter [02100, 05004], lr: 0.066975, loss: 2.7391
2022-03-03 04:00:19 - train: epoch 0082, iter [02200, 05004], lr: 0.066975, loss: 2.3524
2022-03-03 04:00:52 - train: epoch 0082, iter [02300, 05004], lr: 0.066975, loss: 2.6690
2022-03-03 04:01:25 - train: epoch 0082, iter [02400, 05004], lr: 0.066975, loss: 2.8025
2022-03-03 04:01:57 - train: epoch 0082, iter [02500, 05004], lr: 0.066975, loss: 2.3158
2022-03-03 04:02:29 - train: epoch 0082, iter [02600, 05004], lr: 0.066975, loss: 2.2132
2022-03-03 04:03:02 - train: epoch 0082, iter [02700, 05004], lr: 0.066975, loss: 2.4798
2022-03-03 04:03:35 - train: epoch 0082, iter [02800, 05004], lr: 0.066975, loss: 2.5162
2022-03-03 04:04:07 - train: epoch 0082, iter [02900, 05004], lr: 0.066975, loss: 2.2086
2022-03-03 04:04:40 - train: epoch 0082, iter [03000, 05004], lr: 0.066975, loss: 2.5257
2022-03-03 04:05:12 - train: epoch 0082, iter [03100, 05004], lr: 0.066975, loss: 2.4179
2022-03-03 04:05:45 - train: epoch 0082, iter [03200, 05004], lr: 0.066975, loss: 2.6197
2022-03-03 04:06:16 - train: epoch 0082, iter [03300, 05004], lr: 0.066975, loss: 2.4812
2022-03-03 04:06:48 - train: epoch 0082, iter [03400, 05004], lr: 0.066975, loss: 2.6735
2022-03-03 04:07:21 - train: epoch 0082, iter [03500, 05004], lr: 0.066975, loss: 2.5199
2022-03-03 04:07:53 - train: epoch 0082, iter [03600, 05004], lr: 0.066975, loss: 2.6810
2022-03-03 04:08:26 - train: epoch 0082, iter [03700, 05004], lr: 0.066975, loss: 2.3163
2022-03-03 04:08:58 - train: epoch 0082, iter [03800, 05004], lr: 0.066975, loss: 2.7972
2022-03-03 04:09:31 - train: epoch 0082, iter [03900, 05004], lr: 0.066975, loss: 2.4663
2022-03-03 04:10:03 - train: epoch 0082, iter [04000, 05004], lr: 0.066975, loss: 2.5219
2022-03-03 04:10:35 - train: epoch 0082, iter [04100, 05004], lr: 0.066975, loss: 2.4759
2022-03-03 04:11:07 - train: epoch 0082, iter [04200, 05004], lr: 0.066975, loss: 2.6255
2022-03-03 04:11:40 - train: epoch 0082, iter [04300, 05004], lr: 0.066975, loss: 2.6677
2022-03-03 04:12:12 - train: epoch 0082, iter [04400, 05004], lr: 0.066975, loss: 2.4530
2022-03-03 04:12:45 - train: epoch 0082, iter [04500, 05004], lr: 0.066975, loss: 2.3951
2022-03-03 04:13:17 - train: epoch 0082, iter [04600, 05004], lr: 0.066975, loss: 2.5451
2022-03-03 04:13:50 - train: epoch 0082, iter [04700, 05004], lr: 0.066975, loss: 2.4005
2022-03-03 04:14:22 - train: epoch 0082, iter [04800, 05004], lr: 0.066975, loss: 2.6137
2022-03-03 04:14:54 - train: epoch 0082, iter [04900, 05004], lr: 0.066975, loss: 2.4291
2022-03-03 04:15:25 - train: epoch 0082, iter [05000, 05004], lr: 0.066975, loss: 2.7768
2022-03-03 04:15:26 - train: epoch 082, train_loss: 2.5555
2022-03-03 04:16:38 - eval: epoch: 082, acc1: 57.780%, acc5: 82.060%, test_loss: 1.7710, per_image_load_time: 0.681ms, per_image_inference_time: 0.428ms
2022-03-03 04:16:38 - until epoch: 082, best_acc1: 58.538%
2022-03-03 04:16:38 - epoch 083 lr: 0.06621492225478413
2022-03-03 04:17:17 - train: epoch 0083, iter [00100, 05004], lr: 0.066215, loss: 2.2052
2022-03-03 04:17:50 - train: epoch 0083, iter [00200, 05004], lr: 0.066215, loss: 2.4687
2022-03-03 04:18:22 - train: epoch 0083, iter [00300, 05004], lr: 0.066215, loss: 2.6164
2022-03-03 04:18:55 - train: epoch 0083, iter [00400, 05004], lr: 0.066215, loss: 2.7039
2022-03-03 04:19:26 - train: epoch 0083, iter [00500, 05004], lr: 0.066215, loss: 2.6926
2022-03-03 04:19:59 - train: epoch 0083, iter [00600, 05004], lr: 0.066215, loss: 2.6899
2022-03-03 04:20:31 - train: epoch 0083, iter [00700, 05004], lr: 0.066215, loss: 2.3378
2022-03-03 04:21:05 - train: epoch 0083, iter [00800, 05004], lr: 0.066215, loss: 2.4581
2022-03-03 04:21:37 - train: epoch 0083, iter [00900, 05004], lr: 0.066215, loss: 2.7155
2022-03-03 04:22:09 - train: epoch 0083, iter [01000, 05004], lr: 0.066215, loss: 2.5899
2022-03-03 04:22:42 - train: epoch 0083, iter [01100, 05004], lr: 0.066215, loss: 2.4933
2022-03-03 04:23:14 - train: epoch 0083, iter [01200, 05004], lr: 0.066215, loss: 2.5505
2022-03-03 04:23:47 - train: epoch 0083, iter [01300, 05004], lr: 0.066215, loss: 2.7607
2022-03-03 04:24:19 - train: epoch 0083, iter [01400, 05004], lr: 0.066215, loss: 2.4211
2022-03-03 04:24:51 - train: epoch 0083, iter [01500, 05004], lr: 0.066215, loss: 2.4548
2022-03-03 04:25:24 - train: epoch 0083, iter [01600, 05004], lr: 0.066215, loss: 2.5178
2022-03-03 04:25:57 - train: epoch 0083, iter [01700, 05004], lr: 0.066215, loss: 2.7403
2022-03-03 04:26:29 - train: epoch 0083, iter [01800, 05004], lr: 0.066215, loss: 2.7287
2022-03-03 04:27:02 - train: epoch 0083, iter [01900, 05004], lr: 0.066215, loss: 2.5230
2022-03-03 04:27:35 - train: epoch 0083, iter [02000, 05004], lr: 0.066215, loss: 2.4430
2022-03-03 04:28:08 - train: epoch 0083, iter [02100, 05004], lr: 0.066215, loss: 2.6396
2022-03-03 04:28:39 - train: epoch 0083, iter [02200, 05004], lr: 0.066215, loss: 2.7466
2022-03-03 04:29:12 - train: epoch 0083, iter [02300, 05004], lr: 0.066215, loss: 2.6019
2022-03-03 04:29:44 - train: epoch 0083, iter [02400, 05004], lr: 0.066215, loss: 2.3354
2022-03-03 04:30:17 - train: epoch 0083, iter [02500, 05004], lr: 0.066215, loss: 2.4992
2022-03-03 04:30:50 - train: epoch 0083, iter [02600, 05004], lr: 0.066215, loss: 2.7086
2022-03-03 04:31:22 - train: epoch 0083, iter [02700, 05004], lr: 0.066215, loss: 2.4554
2022-03-03 04:31:55 - train: epoch 0083, iter [02800, 05004], lr: 0.066215, loss: 2.7063
2022-03-03 04:32:27 - train: epoch 0083, iter [02900, 05004], lr: 0.066215, loss: 2.7406
2022-03-03 04:33:00 - train: epoch 0083, iter [03000, 05004], lr: 0.066215, loss: 2.9567
2022-03-03 04:33:32 - train: epoch 0083, iter [03100, 05004], lr: 0.066215, loss: 2.5764
2022-03-03 04:34:05 - train: epoch 0083, iter [03200, 05004], lr: 0.066215, loss: 2.5525
2022-03-03 04:34:37 - train: epoch 0083, iter [03300, 05004], lr: 0.066215, loss: 2.7945
2022-03-03 04:35:10 - train: epoch 0083, iter [03400, 05004], lr: 0.066215, loss: 2.6555
2022-03-03 04:35:41 - train: epoch 0083, iter [03500, 05004], lr: 0.066215, loss: 2.3835
2022-03-03 04:36:15 - train: epoch 0083, iter [03600, 05004], lr: 0.066215, loss: 2.4897
2022-03-03 04:36:47 - train: epoch 0083, iter [03700, 05004], lr: 0.066215, loss: 2.2947
2022-03-03 04:37:20 - train: epoch 0083, iter [03800, 05004], lr: 0.066215, loss: 2.5493
2022-03-03 04:37:53 - train: epoch 0083, iter [03900, 05004], lr: 0.066215, loss: 2.6159
2022-03-03 04:38:25 - train: epoch 0083, iter [04000, 05004], lr: 0.066215, loss: 2.8664
2022-03-03 04:38:58 - train: epoch 0083, iter [04100, 05004], lr: 0.066215, loss: 2.4923
2022-03-03 04:39:31 - train: epoch 0083, iter [04200, 05004], lr: 0.066215, loss: 3.0239
2022-03-03 04:40:03 - train: epoch 0083, iter [04300, 05004], lr: 0.066215, loss: 2.4267
2022-03-03 04:40:35 - train: epoch 0083, iter [04400, 05004], lr: 0.066215, loss: 2.3809
2022-03-03 04:41:08 - train: epoch 0083, iter [04500, 05004], lr: 0.066215, loss: 2.4857
2022-03-03 04:41:40 - train: epoch 0083, iter [04600, 05004], lr: 0.066215, loss: 2.5038
2022-03-03 04:42:12 - train: epoch 0083, iter [04700, 05004], lr: 0.066215, loss: 3.1099
2022-03-03 04:42:45 - train: epoch 0083, iter [04800, 05004], lr: 0.066215, loss: 2.5573
2022-03-03 04:43:18 - train: epoch 0083, iter [04900, 05004], lr: 0.066215, loss: 2.5055
2022-03-03 04:43:49 - train: epoch 0083, iter [05000, 05004], lr: 0.066215, loss: 2.4107
2022-03-03 04:43:50 - train: epoch 083, train_loss: 2.5533
2022-03-03 04:45:02 - eval: epoch: 083, acc1: 55.712%, acc5: 80.424%, test_loss: 1.8877, per_image_load_time: 0.570ms, per_image_inference_time: 0.431ms
2022-03-03 04:45:02 - until epoch: 083, best_acc1: 58.538%
2022-03-03 04:45:02 - epoch 084 lr: 0.06545084971874737
2022-03-03 04:45:41 - train: epoch 0084, iter [00100, 05004], lr: 0.065451, loss: 2.4349
2022-03-03 04:46:13 - train: epoch 0084, iter [00200, 05004], lr: 0.065451, loss: 2.6303
2022-03-03 04:46:45 - train: epoch 0084, iter [00300, 05004], lr: 0.065451, loss: 2.6562
2022-03-03 04:47:17 - train: epoch 0084, iter [00400, 05004], lr: 0.065451, loss: 2.4957
2022-03-03 04:47:51 - train: epoch 0084, iter [00500, 05004], lr: 0.065451, loss: 2.8584
2022-03-03 04:48:22 - train: epoch 0084, iter [00600, 05004], lr: 0.065451, loss: 2.6192
2022-03-03 04:48:55 - train: epoch 0084, iter [00700, 05004], lr: 0.065451, loss: 2.6843
2022-03-03 04:49:28 - train: epoch 0084, iter [00800, 05004], lr: 0.065451, loss: 2.3961
2022-03-03 04:50:01 - train: epoch 0084, iter [00900, 05004], lr: 0.065451, loss: 2.4124
2022-03-03 04:50:33 - train: epoch 0084, iter [01000, 05004], lr: 0.065451, loss: 2.3565
2022-03-03 04:51:06 - train: epoch 0084, iter [01100, 05004], lr: 0.065451, loss: 2.1446
2022-03-03 04:51:38 - train: epoch 0084, iter [01200, 05004], lr: 0.065451, loss: 2.7327
2022-03-03 04:52:11 - train: epoch 0084, iter [01300, 05004], lr: 0.065451, loss: 2.4935
2022-03-03 04:52:43 - train: epoch 0084, iter [01400, 05004], lr: 0.065451, loss: 2.5167
2022-03-03 04:53:16 - train: epoch 0084, iter [01500, 05004], lr: 0.065451, loss: 2.4327
2022-03-03 04:53:48 - train: epoch 0084, iter [01600, 05004], lr: 0.065451, loss: 2.3107
2022-03-03 04:54:21 - train: epoch 0084, iter [01700, 05004], lr: 0.065451, loss: 2.6337
2022-03-03 04:54:53 - train: epoch 0084, iter [01800, 05004], lr: 0.065451, loss: 2.5623
2022-03-03 04:55:26 - train: epoch 0084, iter [01900, 05004], lr: 0.065451, loss: 2.5168
2022-03-03 04:55:58 - train: epoch 0084, iter [02000, 05004], lr: 0.065451, loss: 2.6449
2022-03-03 04:56:32 - train: epoch 0084, iter [02100, 05004], lr: 0.065451, loss: 2.8422
2022-03-03 04:57:04 - train: epoch 0084, iter [02200, 05004], lr: 0.065451, loss: 2.4414
2022-03-03 04:57:37 - train: epoch 0084, iter [02300, 05004], lr: 0.065451, loss: 2.5449
2022-03-03 04:58:08 - train: epoch 0084, iter [02400, 05004], lr: 0.065451, loss: 2.4777
2022-03-03 04:58:42 - train: epoch 0084, iter [02500, 05004], lr: 0.065451, loss: 2.7924
2022-03-03 04:59:14 - train: epoch 0084, iter [02600, 05004], lr: 0.065451, loss: 2.2436
2022-03-03 04:59:47 - train: epoch 0084, iter [02700, 05004], lr: 0.065451, loss: 2.6724
2022-03-03 05:00:19 - train: epoch 0084, iter [02800, 05004], lr: 0.065451, loss: 2.4233
2022-03-03 05:00:52 - train: epoch 0084, iter [02900, 05004], lr: 0.065451, loss: 2.7600
2022-03-03 05:01:25 - train: epoch 0084, iter [03000, 05004], lr: 0.065451, loss: 2.5203
2022-03-03 05:01:58 - train: epoch 0084, iter [03100, 05004], lr: 0.065451, loss: 2.4332
2022-03-03 05:02:30 - train: epoch 0084, iter [03200, 05004], lr: 0.065451, loss: 2.5828
2022-03-03 05:03:02 - train: epoch 0084, iter [03300, 05004], lr: 0.065451, loss: 2.6165
2022-03-03 05:03:34 - train: epoch 0084, iter [03400, 05004], lr: 0.065451, loss: 2.5062
2022-03-03 05:04:06 - train: epoch 0084, iter [03500, 05004], lr: 0.065451, loss: 2.5596
2022-03-03 05:04:39 - train: epoch 0084, iter [03600, 05004], lr: 0.065451, loss: 2.5059
2022-03-03 05:05:11 - train: epoch 0084, iter [03700, 05004], lr: 0.065451, loss: 2.6652
2022-03-03 05:05:44 - train: epoch 0084, iter [03800, 05004], lr: 0.065451, loss: 2.6270
2022-03-03 05:06:17 - train: epoch 0084, iter [03900, 05004], lr: 0.065451, loss: 2.5362
2022-03-03 05:06:50 - train: epoch 0084, iter [04000, 05004], lr: 0.065451, loss: 2.5184
2022-03-03 05:07:23 - train: epoch 0084, iter [04100, 05004], lr: 0.065451, loss: 2.4616
2022-03-03 05:07:54 - train: epoch 0084, iter [04200, 05004], lr: 0.065451, loss: 2.3875
2022-03-03 05:08:27 - train: epoch 0084, iter [04300, 05004], lr: 0.065451, loss: 2.3784
2022-03-03 05:08:59 - train: epoch 0084, iter [04400, 05004], lr: 0.065451, loss: 2.6852
2022-03-03 05:09:31 - train: epoch 0084, iter [04500, 05004], lr: 0.065451, loss: 2.6021
2022-03-03 05:10:04 - train: epoch 0084, iter [04600, 05004], lr: 0.065451, loss: 2.5152
2022-03-03 05:10:37 - train: epoch 0084, iter [04700, 05004], lr: 0.065451, loss: 2.7121
2022-03-03 05:11:09 - train: epoch 0084, iter [04800, 05004], lr: 0.065451, loss: 2.4908
2022-03-03 05:11:42 - train: epoch 0084, iter [04900, 05004], lr: 0.065451, loss: 2.5474
2022-03-03 05:12:13 - train: epoch 0084, iter [05000, 05004], lr: 0.065451, loss: 2.4809
2022-03-03 05:12:14 - train: epoch 084, train_loss: 2.5493
2022-03-03 05:13:25 - eval: epoch: 084, acc1: 56.302%, acc5: 81.012%, test_loss: 1.8448, per_image_load_time: 0.494ms, per_image_inference_time: 0.375ms
2022-03-03 05:13:26 - until epoch: 084, best_acc1: 58.538%
2022-03-03 05:13:26 - epoch 085 lr: 0.06468276691378154
2022-03-03 05:14:04 - train: epoch 0085, iter [00100, 05004], lr: 0.064683, loss: 2.4376
2022-03-03 05:14:37 - train: epoch 0085, iter [00200, 05004], lr: 0.064683, loss: 2.5261
2022-03-03 05:15:09 - train: epoch 0085, iter [00300, 05004], lr: 0.064683, loss: 2.6912
2022-03-03 05:15:42 - train: epoch 0085, iter [00400, 05004], lr: 0.064683, loss: 2.3128
2022-03-03 05:16:15 - train: epoch 0085, iter [00500, 05004], lr: 0.064683, loss: 2.5752
2022-03-03 05:16:46 - train: epoch 0085, iter [00600, 05004], lr: 0.064683, loss: 2.3653
2022-03-03 05:17:18 - train: epoch 0085, iter [00700, 05004], lr: 0.064683, loss: 2.4913
2022-03-03 05:17:51 - train: epoch 0085, iter [00800, 05004], lr: 0.064683, loss: 2.5289
2022-03-03 05:18:23 - train: epoch 0085, iter [00900, 05004], lr: 0.064683, loss: 2.4575
2022-03-03 05:18:56 - train: epoch 0085, iter [01000, 05004], lr: 0.064683, loss: 2.7919
2022-03-03 05:19:28 - train: epoch 0085, iter [01100, 05004], lr: 0.064683, loss: 2.5534
2022-03-03 05:20:01 - train: epoch 0085, iter [01200, 05004], lr: 0.064683, loss: 2.4707
2022-03-03 05:20:34 - train: epoch 0085, iter [01300, 05004], lr: 0.064683, loss: 2.5532
2022-03-03 05:21:06 - train: epoch 0085, iter [01400, 05004], lr: 0.064683, loss: 2.6056
2022-03-03 05:21:39 - train: epoch 0085, iter [01500, 05004], lr: 0.064683, loss: 2.5332
2022-03-03 05:22:10 - train: epoch 0085, iter [01600, 05004], lr: 0.064683, loss: 2.6437
2022-03-03 05:22:43 - train: epoch 0085, iter [01700, 05004], lr: 0.064683, loss: 2.9420
2022-03-03 05:23:15 - train: epoch 0085, iter [01800, 05004], lr: 0.064683, loss: 2.5580
2022-03-03 05:23:49 - train: epoch 0085, iter [01900, 05004], lr: 0.064683, loss: 2.2812
2022-03-03 05:24:21 - train: epoch 0085, iter [02000, 05004], lr: 0.064683, loss: 2.3654
2022-03-03 05:24:54 - train: epoch 0085, iter [02100, 05004], lr: 0.064683, loss: 2.3556
2022-03-03 05:25:26 - train: epoch 0085, iter [02200, 05004], lr: 0.064683, loss: 2.5547
2022-03-03 05:25:58 - train: epoch 0085, iter [02300, 05004], lr: 0.064683, loss: 2.4132
2022-03-03 05:26:30 - train: epoch 0085, iter [02400, 05004], lr: 0.064683, loss: 2.8693
2022-03-03 05:27:02 - train: epoch 0085, iter [02500, 05004], lr: 0.064683, loss: 2.6676
2022-03-03 05:27:34 - train: epoch 0085, iter [02600, 05004], lr: 0.064683, loss: 2.6959
2022-03-03 05:28:07 - train: epoch 0085, iter [02700, 05004], lr: 0.064683, loss: 2.1288
2022-03-03 05:28:40 - train: epoch 0085, iter [02800, 05004], lr: 0.064683, loss: 2.5732
2022-03-03 05:29:13 - train: epoch 0085, iter [02900, 05004], lr: 0.064683, loss: 2.5399
2022-03-03 05:29:45 - train: epoch 0085, iter [03000, 05004], lr: 0.064683, loss: 2.6489
2022-03-03 05:30:17 - train: epoch 0085, iter [03100, 05004], lr: 0.064683, loss: 2.5844
2022-03-03 05:30:49 - train: epoch 0085, iter [03200, 05004], lr: 0.064683, loss: 2.4174
2022-03-03 05:31:22 - train: epoch 0085, iter [03300, 05004], lr: 0.064683, loss: 2.7446
2022-03-03 05:31:55 - train: epoch 0085, iter [03400, 05004], lr: 0.064683, loss: 2.6992
2022-03-03 05:32:27 - train: epoch 0085, iter [03500, 05004], lr: 0.064683, loss: 2.9896
2022-03-03 05:32:59 - train: epoch 0085, iter [03600, 05004], lr: 0.064683, loss: 2.4888
2022-03-03 05:33:32 - train: epoch 0085, iter [03700, 05004], lr: 0.064683, loss: 2.3096
2022-03-03 05:34:04 - train: epoch 0085, iter [03800, 05004], lr: 0.064683, loss: 2.4213
2022-03-03 05:34:38 - train: epoch 0085, iter [03900, 05004], lr: 0.064683, loss: 2.7762
2022-03-03 05:35:09 - train: epoch 0085, iter [04000, 05004], lr: 0.064683, loss: 2.6364
2022-03-03 05:35:42 - train: epoch 0085, iter [04100, 05004], lr: 0.064683, loss: 2.5478
2022-03-03 05:36:15 - train: epoch 0085, iter [04200, 05004], lr: 0.064683, loss: 2.5692
2022-03-03 05:36:47 - train: epoch 0085, iter [04300, 05004], lr: 0.064683, loss: 2.5837
2022-03-03 05:37:19 - train: epoch 0085, iter [04400, 05004], lr: 0.064683, loss: 2.5344
2022-03-03 05:37:52 - train: epoch 0085, iter [04500, 05004], lr: 0.064683, loss: 2.7556
2022-03-03 05:38:25 - train: epoch 0085, iter [04600, 05004], lr: 0.064683, loss: 2.6800
2022-03-03 05:38:56 - train: epoch 0085, iter [04700, 05004], lr: 0.064683, loss: 2.8002
2022-03-03 05:39:29 - train: epoch 0085, iter [04800, 05004], lr: 0.064683, loss: 2.7029
2022-03-03 05:40:02 - train: epoch 0085, iter [04900, 05004], lr: 0.064683, loss: 2.5517
2022-03-03 05:40:33 - train: epoch 0085, iter [05000, 05004], lr: 0.064683, loss: 2.3088
2022-03-03 05:40:34 - train: epoch 085, train_loss: 2.5412
2022-03-03 05:41:46 - eval: epoch: 085, acc1: 57.420%, acc5: 81.800%, test_loss: 1.8033, per_image_load_time: 0.729ms, per_image_inference_time: 0.396ms
2022-03-03 05:41:46 - until epoch: 085, best_acc1: 58.538%
2022-03-03 05:41:46 - epoch 086 lr: 0.06391087319582264
2022-03-03 05:42:25 - train: epoch 0086, iter [00100, 05004], lr: 0.063911, loss: 2.4830
2022-03-03 05:42:57 - train: epoch 0086, iter [00200, 05004], lr: 0.063911, loss: 2.5657
2022-03-03 05:43:29 - train: epoch 0086, iter [00300, 05004], lr: 0.063911, loss: 2.8065
2022-03-03 05:44:01 - train: epoch 0086, iter [00400, 05004], lr: 0.063911, loss: 2.4960
2022-03-03 05:44:34 - train: epoch 0086, iter [00500, 05004], lr: 0.063911, loss: 2.7096
2022-03-03 05:45:07 - train: epoch 0086, iter [00600, 05004], lr: 0.063911, loss: 2.4562
2022-03-03 05:45:39 - train: epoch 0086, iter [00700, 05004], lr: 0.063911, loss: 2.4410
2022-03-03 05:46:12 - train: epoch 0086, iter [00800, 05004], lr: 0.063911, loss: 2.4301
2022-03-03 05:46:44 - train: epoch 0086, iter [00900, 05004], lr: 0.063911, loss: 2.6115
2022-03-03 05:47:16 - train: epoch 0086, iter [01000, 05004], lr: 0.063911, loss: 2.5048
2022-03-03 05:47:49 - train: epoch 0086, iter [01100, 05004], lr: 0.063911, loss: 2.5127
2022-03-03 05:48:21 - train: epoch 0086, iter [01200, 05004], lr: 0.063911, loss: 2.5127
2022-03-03 05:48:53 - train: epoch 0086, iter [01300, 05004], lr: 0.063911, loss: 2.7431
2022-03-03 05:49:25 - train: epoch 0086, iter [01400, 05004], lr: 0.063911, loss: 2.5634
2022-03-03 05:49:58 - train: epoch 0086, iter [01500, 05004], lr: 0.063911, loss: 2.5120
2022-03-03 05:50:30 - train: epoch 0086, iter [01600, 05004], lr: 0.063911, loss: 2.4752
2022-03-03 05:51:03 - train: epoch 0086, iter [01700, 05004], lr: 0.063911, loss: 2.4442
2022-03-03 05:51:35 - train: epoch 0086, iter [01800, 05004], lr: 0.063911, loss: 2.6275
2022-03-03 05:52:08 - train: epoch 0086, iter [01900, 05004], lr: 0.063911, loss: 2.4208
2022-03-03 05:52:39 - train: epoch 0086, iter [02000, 05004], lr: 0.063911, loss: 2.2712
2022-03-03 05:53:13 - train: epoch 0086, iter [02100, 05004], lr: 0.063911, loss: 2.3860
2022-03-03 05:53:45 - train: epoch 0086, iter [02200, 05004], lr: 0.063911, loss: 2.6149
2022-03-03 05:54:18 - train: epoch 0086, iter [02300, 05004], lr: 0.063911, loss: 2.4249
2022-03-03 05:54:50 - train: epoch 0086, iter [02400, 05004], lr: 0.063911, loss: 2.2246
2022-03-03 05:55:23 - train: epoch 0086, iter [02500, 05004], lr: 0.063911, loss: 2.5190
2022-03-03 05:55:55 - train: epoch 0086, iter [02600, 05004], lr: 0.063911, loss: 2.7144
2022-03-03 05:56:28 - train: epoch 0086, iter [02700, 05004], lr: 0.063911, loss: 2.7336
2022-03-03 05:56:59 - train: epoch 0086, iter [02800, 05004], lr: 0.063911, loss: 2.5166
2022-03-03 05:57:32 - train: epoch 0086, iter [02900, 05004], lr: 0.063911, loss: 2.2699
2022-03-03 05:58:04 - train: epoch 0086, iter [03000, 05004], lr: 0.063911, loss: 2.1544
2022-03-03 05:58:37 - train: epoch 0086, iter [03100, 05004], lr: 0.063911, loss: 2.4763
2022-03-03 05:59:10 - train: epoch 0086, iter [03200, 05004], lr: 0.063911, loss: 2.9193
2022-03-03 05:59:42 - train: epoch 0086, iter [03300, 05004], lr: 0.063911, loss: 2.3312
2022-03-03 06:00:15 - train: epoch 0086, iter [03400, 05004], lr: 0.063911, loss: 2.7401
2022-03-03 06:00:48 - train: epoch 0086, iter [03500, 05004], lr: 0.063911, loss: 2.6115
2022-03-03 06:01:20 - train: epoch 0086, iter [03600, 05004], lr: 0.063911, loss: 2.7395
2022-03-03 06:01:51 - train: epoch 0086, iter [03700, 05004], lr: 0.063911, loss: 2.4716
2022-03-03 06:02:23 - train: epoch 0086, iter [03800, 05004], lr: 0.063911, loss: 2.3090
2022-03-03 06:02:56 - train: epoch 0086, iter [03900, 05004], lr: 0.063911, loss: 2.4990
2022-03-03 06:03:29 - train: epoch 0086, iter [04000, 05004], lr: 0.063911, loss: 2.7809
2022-03-03 06:04:01 - train: epoch 0086, iter [04100, 05004], lr: 0.063911, loss: 2.4820
2022-03-03 06:04:34 - train: epoch 0086, iter [04200, 05004], lr: 0.063911, loss: 2.5276
2022-03-03 06:05:06 - train: epoch 0086, iter [04300, 05004], lr: 0.063911, loss: 2.4825
2022-03-03 06:05:39 - train: epoch 0086, iter [04400, 05004], lr: 0.063911, loss: 2.4283
2022-03-03 06:06:11 - train: epoch 0086, iter [04500, 05004], lr: 0.063911, loss: 2.4880
2022-03-03 06:06:44 - train: epoch 0086, iter [04600, 05004], lr: 0.063911, loss: 2.5481
2022-03-03 06:07:16 - train: epoch 0086, iter [04700, 05004], lr: 0.063911, loss: 2.3297
2022-03-03 06:07:48 - train: epoch 0086, iter [04800, 05004], lr: 0.063911, loss: 2.4979
2022-03-03 06:08:20 - train: epoch 0086, iter [04900, 05004], lr: 0.063911, loss: 2.5427
2022-03-03 06:08:51 - train: epoch 0086, iter [05000, 05004], lr: 0.063911, loss: 2.5699
2022-03-03 06:08:53 - train: epoch 086, train_loss: 2.5377
2022-03-03 06:10:04 - eval: epoch: 086, acc1: 58.058%, acc5: 82.350%, test_loss: 1.7641, per_image_load_time: 0.631ms, per_image_inference_time: 0.421ms
2022-03-03 06:10:05 - until epoch: 086, best_acc1: 58.538%
2022-03-03 06:10:05 - epoch 087 lr: 0.06313536890992935
2022-03-03 06:10:43 - train: epoch 0087, iter [00100, 05004], lr: 0.063135, loss: 2.4531
2022-03-03 06:11:16 - train: epoch 0087, iter [00200, 05004], lr: 0.063135, loss: 2.3898
2022-03-03 06:11:48 - train: epoch 0087, iter [00300, 05004], lr: 0.063135, loss: 2.3567
2022-03-03 06:12:21 - train: epoch 0087, iter [00400, 05004], lr: 0.063135, loss: 2.4256
2022-03-03 06:12:53 - train: epoch 0087, iter [00500, 05004], lr: 0.063135, loss: 2.2749
2022-03-03 06:13:26 - train: epoch 0087, iter [00600, 05004], lr: 0.063135, loss: 2.6694
2022-03-03 06:13:58 - train: epoch 0087, iter [00700, 05004], lr: 0.063135, loss: 2.3124
2022-03-03 06:14:31 - train: epoch 0087, iter [00800, 05004], lr: 0.063135, loss: 2.5592
2022-03-03 06:15:03 - train: epoch 0087, iter [00900, 05004], lr: 0.063135, loss: 2.6823
2022-03-03 06:15:36 - train: epoch 0087, iter [01000, 05004], lr: 0.063135, loss: 2.5527
2022-03-03 06:16:09 - train: epoch 0087, iter [01100, 05004], lr: 0.063135, loss: 2.8779
2022-03-03 06:16:41 - train: epoch 0087, iter [01200, 05004], lr: 0.063135, loss: 2.3127
2022-03-03 06:17:13 - train: epoch 0087, iter [01300, 05004], lr: 0.063135, loss: 2.7264
2022-03-03 06:17:46 - train: epoch 0087, iter [01400, 05004], lr: 0.063135, loss: 2.1441
2022-03-03 06:18:19 - train: epoch 0087, iter [01500, 05004], lr: 0.063135, loss: 2.3033
2022-03-03 06:18:52 - train: epoch 0087, iter [01600, 05004], lr: 0.063135, loss: 2.3696
2022-03-03 06:19:24 - train: epoch 0087, iter [01700, 05004], lr: 0.063135, loss: 2.8617
2022-03-03 06:19:56 - train: epoch 0087, iter [01800, 05004], lr: 0.063135, loss: 2.5376
2022-03-03 06:20:29 - train: epoch 0087, iter [01900, 05004], lr: 0.063135, loss: 2.6900
2022-03-03 06:21:01 - train: epoch 0087, iter [02000, 05004], lr: 0.063135, loss: 2.5324
2022-03-03 06:21:34 - train: epoch 0087, iter [02100, 05004], lr: 0.063135, loss: 2.5642
2022-03-03 06:22:07 - train: epoch 0087, iter [02200, 05004], lr: 0.063135, loss: 2.4726
2022-03-03 06:22:39 - train: epoch 0087, iter [02300, 05004], lr: 0.063135, loss: 2.5251
2022-03-03 06:23:12 - train: epoch 0087, iter [02400, 05004], lr: 0.063135, loss: 2.3759
2022-03-03 06:23:44 - train: epoch 0087, iter [02500, 05004], lr: 0.063135, loss: 2.8726
2022-03-03 06:24:17 - train: epoch 0087, iter [02600, 05004], lr: 0.063135, loss: 2.6676
2022-03-03 06:24:50 - train: epoch 0087, iter [02700, 05004], lr: 0.063135, loss: 2.5256
2022-03-03 06:25:22 - train: epoch 0087, iter [02800, 05004], lr: 0.063135, loss: 2.4170
2022-03-03 06:25:54 - train: epoch 0087, iter [02900, 05004], lr: 0.063135, loss: 2.7033
2022-03-03 06:26:27 - train: epoch 0087, iter [03000, 05004], lr: 0.063135, loss: 2.6198
2022-03-03 06:27:00 - train: epoch 0087, iter [03100, 05004], lr: 0.063135, loss: 2.4161
2022-03-03 06:27:32 - train: epoch 0087, iter [03200, 05004], lr: 0.063135, loss: 2.6015
2022-03-03 06:28:05 - train: epoch 0087, iter [03300, 05004], lr: 0.063135, loss: 2.7365
2022-03-03 06:28:37 - train: epoch 0087, iter [03400, 05004], lr: 0.063135, loss: 2.4891
2022-03-03 06:29:10 - train: epoch 0087, iter [03500, 05004], lr: 0.063135, loss: 2.3484
2022-03-03 06:29:42 - train: epoch 0087, iter [03600, 05004], lr: 0.063135, loss: 2.6374
2022-03-03 06:30:15 - train: epoch 0087, iter [03700, 05004], lr: 0.063135, loss: 2.7188
2022-03-03 06:30:48 - train: epoch 0087, iter [03800, 05004], lr: 0.063135, loss: 2.8442
2022-03-03 06:31:20 - train: epoch 0087, iter [03900, 05004], lr: 0.063135, loss: 2.6809
2022-03-03 06:31:52 - train: epoch 0087, iter [04000, 05004], lr: 0.063135, loss: 2.5213
2022-03-03 06:32:24 - train: epoch 0087, iter [04100, 05004], lr: 0.063135, loss: 2.7923
2022-03-03 06:32:56 - train: epoch 0087, iter [04200, 05004], lr: 0.063135, loss: 2.6628
2022-03-03 06:33:29 - train: epoch 0087, iter [04300, 05004], lr: 0.063135, loss: 2.3714
2022-03-03 06:34:02 - train: epoch 0087, iter [04400, 05004], lr: 0.063135, loss: 2.4364
2022-03-03 06:34:35 - train: epoch 0087, iter [04500, 05004], lr: 0.063135, loss: 2.2840
2022-03-03 06:35:07 - train: epoch 0087, iter [04600, 05004], lr: 0.063135, loss: 2.4243
2022-03-03 06:35:40 - train: epoch 0087, iter [04700, 05004], lr: 0.063135, loss: 2.3947
2022-03-03 06:36:12 - train: epoch 0087, iter [04800, 05004], lr: 0.063135, loss: 2.4231
2022-03-03 06:36:45 - train: epoch 0087, iter [04900, 05004], lr: 0.063135, loss: 2.4552
2022-03-03 06:37:15 - train: epoch 0087, iter [05000, 05004], lr: 0.063135, loss: 2.6252
2022-03-03 06:37:17 - train: epoch 087, train_loss: 2.5287
2022-03-03 06:38:28 - eval: epoch: 087, acc1: 57.366%, acc5: 81.530%, test_loss: 1.8027, per_image_load_time: 0.619ms, per_image_inference_time: 0.393ms
2022-03-03 06:38:29 - until epoch: 087, best_acc1: 58.538%
2022-03-03 06:38:29 - epoch 088 lr: 0.06235645533828348
2022-03-03 06:39:07 - train: epoch 0088, iter [00100, 05004], lr: 0.062356, loss: 2.2324
2022-03-03 06:39:41 - train: epoch 0088, iter [00200, 05004], lr: 0.062356, loss: 2.6540
2022-03-03 06:40:13 - train: epoch 0088, iter [00300, 05004], lr: 0.062356, loss: 2.8354
2022-03-03 06:40:46 - train: epoch 0088, iter [00400, 05004], lr: 0.062356, loss: 2.4536
2022-03-03 06:41:18 - train: epoch 0088, iter [00500, 05004], lr: 0.062356, loss: 2.6056
2022-03-03 06:41:50 - train: epoch 0088, iter [00600, 05004], lr: 0.062356, loss: 2.4078
2022-03-03 06:42:23 - train: epoch 0088, iter [00700, 05004], lr: 0.062356, loss: 2.5287
2022-03-03 06:42:55 - train: epoch 0088, iter [00800, 05004], lr: 0.062356, loss: 2.7358
2022-03-03 06:43:27 - train: epoch 0088, iter [00900, 05004], lr: 0.062356, loss: 2.7365
2022-03-03 06:44:00 - train: epoch 0088, iter [01000, 05004], lr: 0.062356, loss: 2.3682
2022-03-03 06:44:33 - train: epoch 0088, iter [01100, 05004], lr: 0.062356, loss: 2.3670
2022-03-03 06:45:06 - train: epoch 0088, iter [01200, 05004], lr: 0.062356, loss: 2.5363
2022-03-03 06:45:38 - train: epoch 0088, iter [01300, 05004], lr: 0.062356, loss: 2.7815
2022-03-03 06:46:11 - train: epoch 0088, iter [01400, 05004], lr: 0.062356, loss: 2.3696
2022-03-03 06:46:43 - train: epoch 0088, iter [01500, 05004], lr: 0.062356, loss: 2.5992
2022-03-03 06:47:16 - train: epoch 0088, iter [01600, 05004], lr: 0.062356, loss: 2.5980
2022-03-03 06:47:48 - train: epoch 0088, iter [01700, 05004], lr: 0.062356, loss: 2.5077
2022-03-03 06:48:22 - train: epoch 0088, iter [01800, 05004], lr: 0.062356, loss: 2.2793
2022-03-03 06:48:54 - train: epoch 0088, iter [01900, 05004], lr: 0.062356, loss: 2.6239
2022-03-03 06:49:26 - train: epoch 0088, iter [02000, 05004], lr: 0.062356, loss: 2.3941
2022-03-03 06:49:59 - train: epoch 0088, iter [02100, 05004], lr: 0.062356, loss: 2.7029
2022-03-03 06:50:31 - train: epoch 0088, iter [02200, 05004], lr: 0.062356, loss: 2.8174
2022-03-03 06:51:03 - train: epoch 0088, iter [02300, 05004], lr: 0.062356, loss: 2.4585
2022-03-03 06:51:36 - train: epoch 0088, iter [02400, 05004], lr: 0.062356, loss: 2.5606
2022-03-03 06:52:09 - train: epoch 0088, iter [02500, 05004], lr: 0.062356, loss: 2.6834
2022-03-03 06:52:42 - train: epoch 0088, iter [02600, 05004], lr: 0.062356, loss: 2.3946
2022-03-03 06:53:15 - train: epoch 0088, iter [02700, 05004], lr: 0.062356, loss: 2.6017
2022-03-03 06:53:47 - train: epoch 0088, iter [02800, 05004], lr: 0.062356, loss: 2.4713
2022-03-03 06:54:20 - train: epoch 0088, iter [02900, 05004], lr: 0.062356, loss: 2.4302
2022-03-03 06:54:52 - train: epoch 0088, iter [03000, 05004], lr: 0.062356, loss: 2.5843
2022-03-03 06:55:24 - train: epoch 0088, iter [03100, 05004], lr: 0.062356, loss: 2.3245
2022-03-03 06:55:56 - train: epoch 0088, iter [03200, 05004], lr: 0.062356, loss: 2.5491
2022-03-03 06:56:29 - train: epoch 0088, iter [03300, 05004], lr: 0.062356, loss: 2.3999
2022-03-03 06:57:01 - train: epoch 0088, iter [03400, 05004], lr: 0.062356, loss: 2.3956
2022-03-03 06:57:34 - train: epoch 0088, iter [03500, 05004], lr: 0.062356, loss: 2.7376
2022-03-03 06:58:06 - train: epoch 0088, iter [03600, 05004], lr: 0.062356, loss: 2.5186
2022-03-03 06:58:39 - train: epoch 0088, iter [03700, 05004], lr: 0.062356, loss: 2.5878
2022-03-03 06:59:12 - train: epoch 0088, iter [03800, 05004], lr: 0.062356, loss: 2.7017
2022-03-03 06:59:44 - train: epoch 0088, iter [03900, 05004], lr: 0.062356, loss: 2.5423
2022-03-03 07:00:17 - train: epoch 0088, iter [04000, 05004], lr: 0.062356, loss: 2.5393
2022-03-03 07:00:49 - train: epoch 0088, iter [04100, 05004], lr: 0.062356, loss: 2.5459
2022-03-03 07:01:21 - train: epoch 0088, iter [04200, 05004], lr: 0.062356, loss: 2.5872
2022-03-03 07:01:54 - train: epoch 0088, iter [04300, 05004], lr: 0.062356, loss: 2.5246
2022-03-03 07:02:26 - train: epoch 0088, iter [04400, 05004], lr: 0.062356, loss: 2.3290
2022-03-03 07:02:59 - train: epoch 0088, iter [04500, 05004], lr: 0.062356, loss: 2.5476
2022-03-03 07:03:31 - train: epoch 0088, iter [04600, 05004], lr: 0.062356, loss: 2.7775
2022-03-03 07:04:03 - train: epoch 0088, iter [04700, 05004], lr: 0.062356, loss: 2.8382
2022-03-03 07:04:37 - train: epoch 0088, iter [04800, 05004], lr: 0.062356, loss: 2.6523
2022-03-03 07:05:09 - train: epoch 0088, iter [04900, 05004], lr: 0.062356, loss: 2.5121
2022-03-03 07:05:41 - train: epoch 0088, iter [05000, 05004], lr: 0.062356, loss: 2.4587
2022-03-03 07:05:42 - train: epoch 088, train_loss: 2.5234
2022-03-03 07:06:54 - eval: epoch: 088, acc1: 57.034%, acc5: 81.710%, test_loss: 1.8157, per_image_load_time: 0.578ms, per_image_inference_time: 0.367ms
2022-03-03 07:06:54 - until epoch: 088, best_acc1: 58.538%
2022-03-03 07:06:54 - epoch 089 lr: 0.06157433464794716
2022-03-03 07:07:33 - train: epoch 0089, iter [00100, 05004], lr: 0.061574, loss: 2.8045
2022-03-03 07:08:05 - train: epoch 0089, iter [00200, 05004], lr: 0.061574, loss: 2.2187
2022-03-03 07:08:37 - train: epoch 0089, iter [00300, 05004], lr: 0.061574, loss: 2.5745
2022-03-03 07:09:10 - train: epoch 0089, iter [00400, 05004], lr: 0.061574, loss: 2.4268
2022-03-03 07:09:42 - train: epoch 0089, iter [00500, 05004], lr: 0.061574, loss: 2.7610
2022-03-03 07:10:15 - train: epoch 0089, iter [00600, 05004], lr: 0.061574, loss: 2.4257
2022-03-03 07:10:48 - train: epoch 0089, iter [00700, 05004], lr: 0.061574, loss: 2.4080
2022-03-03 07:11:20 - train: epoch 0089, iter [00800, 05004], lr: 0.061574, loss: 2.5502
2022-03-03 07:11:53 - train: epoch 0089, iter [00900, 05004], lr: 0.061574, loss: 2.6786
2022-03-03 07:12:25 - train: epoch 0089, iter [01000, 05004], lr: 0.061574, loss: 2.4516
2022-03-03 07:12:57 - train: epoch 0089, iter [01100, 05004], lr: 0.061574, loss: 2.6179
2022-03-03 07:13:29 - train: epoch 0089, iter [01200, 05004], lr: 0.061574, loss: 2.5486
2022-03-03 07:14:02 - train: epoch 0089, iter [01300, 05004], lr: 0.061574, loss: 2.4220
2022-03-03 07:14:35 - train: epoch 0089, iter [01400, 05004], lr: 0.061574, loss: 2.7586
2022-03-03 07:15:07 - train: epoch 0089, iter [01500, 05004], lr: 0.061574, loss: 2.5135
2022-03-03 07:15:40 - train: epoch 0089, iter [01600, 05004], lr: 0.061574, loss: 2.2545
2022-03-03 07:16:13 - train: epoch 0089, iter [01700, 05004], lr: 0.061574, loss: 2.3750
2022-03-03 07:16:45 - train: epoch 0089, iter [01800, 05004], lr: 0.061574, loss: 2.4463
2022-03-03 07:17:17 - train: epoch 0089, iter [01900, 05004], lr: 0.061574, loss: 2.4975
2022-03-03 07:17:49 - train: epoch 0089, iter [02000, 05004], lr: 0.061574, loss: 2.4420
2022-03-03 07:18:22 - train: epoch 0089, iter [02100, 05004], lr: 0.061574, loss: 2.6013
2022-03-03 07:18:54 - train: epoch 0089, iter [02200, 05004], lr: 0.061574, loss: 2.6988
2022-03-03 07:19:28 - train: epoch 0089, iter [02300, 05004], lr: 0.061574, loss: 2.3123
2022-03-03 07:20:00 - train: epoch 0089, iter [02400, 05004], lr: 0.061574, loss: 2.5696
2022-03-03 07:20:32 - train: epoch 0089, iter [02500, 05004], lr: 0.061574, loss: 2.7882
2022-03-03 07:21:05 - train: epoch 0089, iter [02600, 05004], lr: 0.061574, loss: 2.3348
2022-03-03 07:21:37 - train: epoch 0089, iter [02700, 05004], lr: 0.061574, loss: 2.6054
2022-03-03 07:22:09 - train: epoch 0089, iter [02800, 05004], lr: 0.061574, loss: 2.6813
2022-03-03 07:22:43 - train: epoch 0089, iter [02900, 05004], lr: 0.061574, loss: 2.6778
2022-03-03 07:23:15 - train: epoch 0089, iter [03000, 05004], lr: 0.061574, loss: 2.4648
2022-03-03 07:23:47 - train: epoch 0089, iter [03100, 05004], lr: 0.061574, loss: 2.5576
2022-03-03 07:24:19 - train: epoch 0089, iter [03200, 05004], lr: 0.061574, loss: 2.7384
2022-03-03 07:24:52 - train: epoch 0089, iter [03300, 05004], lr: 0.061574, loss: 2.6608
2022-03-03 07:25:24 - train: epoch 0089, iter [03400, 05004], lr: 0.061574, loss: 2.4409
2022-03-03 07:25:57 - train: epoch 0089, iter [03500, 05004], lr: 0.061574, loss: 2.1048
2022-03-03 07:26:28 - train: epoch 0089, iter [03600, 05004], lr: 0.061574, loss: 2.7179
2022-03-03 07:27:01 - train: epoch 0089, iter [03700, 05004], lr: 0.061574, loss: 2.4563
2022-03-03 07:27:33 - train: epoch 0089, iter [03800, 05004], lr: 0.061574, loss: 2.2873
2022-03-03 07:28:07 - train: epoch 0089, iter [03900, 05004], lr: 0.061574, loss: 2.5150
2022-03-03 07:28:39 - train: epoch 0089, iter [04000, 05004], lr: 0.061574, loss: 2.6302
2022-03-03 07:29:12 - train: epoch 0089, iter [04100, 05004], lr: 0.061574, loss: 2.7530
2022-03-03 07:29:44 - train: epoch 0089, iter [04200, 05004], lr: 0.061574, loss: 2.4529
2022-03-03 07:30:16 - train: epoch 0089, iter [04300, 05004], lr: 0.061574, loss: 2.6458
2022-03-03 07:30:48 - train: epoch 0089, iter [04400, 05004], lr: 0.061574, loss: 2.6557
2022-03-03 07:31:21 - train: epoch 0089, iter [04500, 05004], lr: 0.061574, loss: 2.2757
2022-03-03 07:31:53 - train: epoch 0089, iter [04600, 05004], lr: 0.061574, loss: 2.5268
2022-03-03 07:32:26 - train: epoch 0089, iter [04700, 05004], lr: 0.061574, loss: 2.4478
2022-03-03 07:32:59 - train: epoch 0089, iter [04800, 05004], lr: 0.061574, loss: 2.5869
2022-03-03 07:33:32 - train: epoch 0089, iter [04900, 05004], lr: 0.061574, loss: 2.4698
2022-03-03 07:34:03 - train: epoch 0089, iter [05000, 05004], lr: 0.061574, loss: 2.2944
2022-03-03 07:34:04 - train: epoch 089, train_loss: 2.5152
2022-03-03 07:35:16 - eval: epoch: 089, acc1: 58.656%, acc5: 82.910%, test_loss: 1.7219, per_image_load_time: 0.519ms, per_image_inference_time: 0.370ms
2022-03-03 07:35:16 - until epoch: 089, best_acc1: 58.656%
2022-03-03 07:35:16 - epoch 090 lr: 0.06078920983839031
2022-03-03 07:35:54 - train: epoch 0090, iter [00100, 05004], lr: 0.060789, loss: 2.5659
2022-03-03 07:36:26 - train: epoch 0090, iter [00200, 05004], lr: 0.060789, loss: 2.5625
2022-03-03 07:36:59 - train: epoch 0090, iter [00300, 05004], lr: 0.060789, loss: 2.3583
2022-03-03 07:37:32 - train: epoch 0090, iter [00400, 05004], lr: 0.060789, loss: 2.5224
2022-03-03 07:38:04 - train: epoch 0090, iter [00500, 05004], lr: 0.060789, loss: 2.5104
2022-03-03 07:38:37 - train: epoch 0090, iter [00600, 05004], lr: 0.060789, loss: 2.7324
2022-03-03 07:39:10 - train: epoch 0090, iter [00700, 05004], lr: 0.060789, loss: 2.4818
2022-03-03 07:39:41 - train: epoch 0090, iter [00800, 05004], lr: 0.060789, loss: 2.5197
2022-03-03 07:40:15 - train: epoch 0090, iter [00900, 05004], lr: 0.060789, loss: 2.4170
2022-03-03 07:40:46 - train: epoch 0090, iter [01000, 05004], lr: 0.060789, loss: 2.4009
2022-03-03 07:41:19 - train: epoch 0090, iter [01100, 05004], lr: 0.060789, loss: 2.4443
2022-03-03 07:41:51 - train: epoch 0090, iter [01200, 05004], lr: 0.060789, loss: 2.3220
2022-03-03 07:42:24 - train: epoch 0090, iter [01300, 05004], lr: 0.060789, loss: 2.4464
2022-03-03 07:42:56 - train: epoch 0090, iter [01400, 05004], lr: 0.060789, loss: 2.2542
2022-03-03 07:43:28 - train: epoch 0090, iter [01500, 05004], lr: 0.060789, loss: 2.4387
2022-03-03 07:44:01 - train: epoch 0090, iter [01600, 05004], lr: 0.060789, loss: 2.5195
2022-03-03 07:44:33 - train: epoch 0090, iter [01700, 05004], lr: 0.060789, loss: 2.2298
2022-03-03 07:45:06 - train: epoch 0090, iter [01800, 05004], lr: 0.060789, loss: 2.4391
2022-03-03 07:45:39 - train: epoch 0090, iter [01900, 05004], lr: 0.060789, loss: 2.2972
2022-03-03 07:46:11 - train: epoch 0090, iter [02000, 05004], lr: 0.060789, loss: 2.5206
2022-03-03 07:46:43 - train: epoch 0090, iter [02100, 05004], lr: 0.060789, loss: 2.6732
2022-03-03 07:47:16 - train: epoch 0090, iter [02200, 05004], lr: 0.060789, loss: 2.4185
2022-03-03 07:47:49 - train: epoch 0090, iter [02300, 05004], lr: 0.060789, loss: 2.7773
2022-03-03 07:48:21 - train: epoch 0090, iter [02400, 05004], lr: 0.060789, loss: 2.4081
2022-03-03 07:48:54 - train: epoch 0090, iter [02500, 05004], lr: 0.060789, loss: 2.5667
2022-03-03 07:49:26 - train: epoch 0090, iter [02600, 05004], lr: 0.060789, loss: 2.5222
2022-03-03 07:49:59 - train: epoch 0090, iter [02700, 05004], lr: 0.060789, loss: 2.5004
2022-03-03 07:50:32 - train: epoch 0090, iter [02800, 05004], lr: 0.060789, loss: 2.5811
2022-03-03 07:51:03 - train: epoch 0090, iter [02900, 05004], lr: 0.060789, loss: 2.5944
2022-03-03 07:51:36 - train: epoch 0090, iter [03000, 05004], lr: 0.060789, loss: 2.6601
2022-03-03 07:52:09 - train: epoch 0090, iter [03100, 05004], lr: 0.060789, loss: 2.3308
2022-03-03 07:52:41 - train: epoch 0090, iter [03200, 05004], lr: 0.060789, loss: 2.6637
2022-03-03 07:53:13 - train: epoch 0090, iter [03300, 05004], lr: 0.060789, loss: 2.7139
2022-03-03 07:53:45 - train: epoch 0090, iter [03400, 05004], lr: 0.060789, loss: 2.6324
2022-03-03 07:54:18 - train: epoch 0090, iter [03500, 05004], lr: 0.060789, loss: 2.2858
2022-03-03 07:54:51 - train: epoch 0090, iter [03600, 05004], lr: 0.060789, loss: 2.1186
2022-03-03 07:55:23 - train: epoch 0090, iter [03700, 05004], lr: 0.060789, loss: 2.6734
2022-03-03 07:55:55 - train: epoch 0090, iter [03800, 05004], lr: 0.060789, loss: 2.8784
2022-03-03 07:56:27 - train: epoch 0090, iter [03900, 05004], lr: 0.060789, loss: 2.2505
2022-03-03 07:57:00 - train: epoch 0090, iter [04000, 05004], lr: 0.060789, loss: 2.4527
2022-03-03 07:57:31 - train: epoch 0090, iter [04100, 05004], lr: 0.060789, loss: 2.5960
2022-03-03 07:58:05 - train: epoch 0090, iter [04200, 05004], lr: 0.060789, loss: 2.5699
2022-03-03 07:58:37 - train: epoch 0090, iter [04300, 05004], lr: 0.060789, loss: 2.3903
2022-03-03 07:59:09 - train: epoch 0090, iter [04400, 05004], lr: 0.060789, loss: 2.5434
2022-03-03 07:59:42 - train: epoch 0090, iter [04500, 05004], lr: 0.060789, loss: 2.5418
2022-03-03 08:00:15 - train: epoch 0090, iter [04600, 05004], lr: 0.060789, loss: 2.5755
2022-03-03 08:00:46 - train: epoch 0090, iter [04700, 05004], lr: 0.060789, loss: 2.4905
2022-03-03 08:01:19 - train: epoch 0090, iter [04800, 05004], lr: 0.060789, loss: 2.7221
2022-03-03 08:01:51 - train: epoch 0090, iter [04900, 05004], lr: 0.060789, loss: 2.3598
2022-03-03 08:02:23 - train: epoch 0090, iter [05000, 05004], lr: 0.060789, loss: 2.3194
2022-03-03 08:02:24 - train: epoch 090, train_loss: 2.5086
2022-03-03 08:03:35 - eval: epoch: 090, acc1: 59.496%, acc5: 83.538%, test_loss: 1.6821, per_image_load_time: 0.626ms, per_image_inference_time: 0.408ms
2022-03-03 08:03:36 - until epoch: 090, best_acc1: 59.496%
2022-03-03 08:03:36 - epoch 091 lr: 0.060001284688802226
2022-03-03 08:04:14 - train: epoch 0091, iter [00100, 05004], lr: 0.060001, loss: 2.5116
2022-03-03 08:04:46 - train: epoch 0091, iter [00200, 05004], lr: 0.060001, loss: 2.3362
2022-03-03 08:05:19 - train: epoch 0091, iter [00300, 05004], lr: 0.060001, loss: 2.5899
2022-03-03 08:05:51 - train: epoch 0091, iter [00400, 05004], lr: 0.060001, loss: 2.5697
2022-03-03 08:06:23 - train: epoch 0091, iter [00500, 05004], lr: 0.060001, loss: 2.6035
2022-03-03 08:06:55 - train: epoch 0091, iter [00600, 05004], lr: 0.060001, loss: 2.5741
2022-03-03 08:07:28 - train: epoch 0091, iter [00700, 05004], lr: 0.060001, loss: 2.4687
2022-03-03 08:08:01 - train: epoch 0091, iter [00800, 05004], lr: 0.060001, loss: 2.1919
2022-03-03 08:08:33 - train: epoch 0091, iter [00900, 05004], lr: 0.060001, loss: 2.6330
2022-03-03 08:09:06 - train: epoch 0091, iter [01000, 05004], lr: 0.060001, loss: 2.6178
2022-03-03 08:09:39 - train: epoch 0091, iter [01100, 05004], lr: 0.060001, loss: 2.0706
2022-03-03 08:10:11 - train: epoch 0091, iter [01200, 05004], lr: 0.060001, loss: 2.4415
2022-03-03 08:10:43 - train: epoch 0091, iter [01300, 05004], lr: 0.060001, loss: 2.4145
2022-03-03 08:11:14 - train: epoch 0091, iter [01400, 05004], lr: 0.060001, loss: 2.4226
2022-03-03 08:11:47 - train: epoch 0091, iter [01500, 05004], lr: 0.060001, loss: 2.5291
2022-03-03 08:12:20 - train: epoch 0091, iter [01600, 05004], lr: 0.060001, loss: 2.4789
2022-03-03 08:12:52 - train: epoch 0091, iter [01700, 05004], lr: 0.060001, loss: 2.3529
2022-03-03 08:13:25 - train: epoch 0091, iter [01800, 05004], lr: 0.060001, loss: 2.4270
2022-03-03 08:13:57 - train: epoch 0091, iter [01900, 05004], lr: 0.060001, loss: 2.4824
2022-03-03 08:14:30 - train: epoch 0091, iter [02000, 05004], lr: 0.060001, loss: 2.2405
2022-03-03 08:15:02 - train: epoch 0091, iter [02100, 05004], lr: 0.060001, loss: 2.6104
2022-03-03 08:15:35 - train: epoch 0091, iter [02200, 05004], lr: 0.060001, loss: 2.6860
2022-03-03 08:16:07 - train: epoch 0091, iter [02300, 05004], lr: 0.060001, loss: 2.5429
2022-03-03 08:16:39 - train: epoch 0091, iter [02400, 05004], lr: 0.060001, loss: 2.4338
2022-03-03 08:17:12 - train: epoch 0091, iter [02500, 05004], lr: 0.060001, loss: 2.2618
2022-03-03 08:17:44 - train: epoch 0091, iter [02600, 05004], lr: 0.060001, loss: 2.5263
2022-03-03 08:18:17 - train: epoch 0091, iter [02700, 05004], lr: 0.060001, loss: 2.4335
2022-03-03 08:18:49 - train: epoch 0091, iter [02800, 05004], lr: 0.060001, loss: 2.3845
2022-03-03 08:19:21 - train: epoch 0091, iter [02900, 05004], lr: 0.060001, loss: 2.5495
2022-03-03 08:19:53 - train: epoch 0091, iter [03000, 05004], lr: 0.060001, loss: 2.5324
2022-03-03 08:20:26 - train: epoch 0091, iter [03100, 05004], lr: 0.060001, loss: 2.6255
2022-03-03 08:20:58 - train: epoch 0091, iter [03200, 05004], lr: 0.060001, loss: 2.5031
2022-03-03 08:21:31 - train: epoch 0091, iter [03300, 05004], lr: 0.060001, loss: 2.5455
2022-03-03 08:22:03 - train: epoch 0091, iter [03400, 05004], lr: 0.060001, loss: 2.5813
2022-03-03 08:22:36 - train: epoch 0091, iter [03500, 05004], lr: 0.060001, loss: 2.5099
2022-03-03 08:23:08 - train: epoch 0091, iter [03600, 05004], lr: 0.060001, loss: 2.5240
2022-03-03 08:23:41 - train: epoch 0091, iter [03700, 05004], lr: 0.060001, loss: 2.6048
2022-03-03 08:24:12 - train: epoch 0091, iter [03800, 05004], lr: 0.060001, loss: 2.6338
2022-03-03 08:24:46 - train: epoch 0091, iter [03900, 05004], lr: 0.060001, loss: 2.4831
2022-03-03 08:25:18 - train: epoch 0091, iter [04000, 05004], lr: 0.060001, loss: 2.4461
2022-03-03 08:25:50 - train: epoch 0091, iter [04100, 05004], lr: 0.060001, loss: 2.3055
2022-03-03 08:26:22 - train: epoch 0091, iter [04200, 05004], lr: 0.060001, loss: 2.6084
2022-03-03 08:26:55 - train: epoch 0091, iter [04300, 05004], lr: 0.060001, loss: 2.5437
2022-03-03 08:27:27 - train: epoch 0091, iter [04400, 05004], lr: 0.060001, loss: 2.1860
2022-03-03 08:27:59 - train: epoch 0091, iter [04500, 05004], lr: 0.060001, loss: 2.4778
2022-03-03 08:28:31 - train: epoch 0091, iter [04600, 05004], lr: 0.060001, loss: 2.2193
2022-03-03 08:29:04 - train: epoch 0091, iter [04700, 05004], lr: 0.060001, loss: 2.6241
2022-03-03 08:29:36 - train: epoch 0091, iter [04800, 05004], lr: 0.060001, loss: 2.7211
2022-03-03 08:30:09 - train: epoch 0091, iter [04900, 05004], lr: 0.060001, loss: 2.5031
2022-03-03 08:30:40 - train: epoch 0091, iter [05000, 05004], lr: 0.060001, loss: 2.6954
2022-03-03 08:30:41 - train: epoch 091, train_loss: 2.5051
2022-03-03 08:31:53 - eval: epoch: 091, acc1: 57.338%, acc5: 81.738%, test_loss: 1.7887, per_image_load_time: 0.785ms, per_image_inference_time: 0.387ms
2022-03-03 08:31:53 - until epoch: 091, best_acc1: 59.496%
2022-03-03 08:31:53 - epoch 092 lr: 0.05921076370520058
2022-03-03 08:32:31 - train: epoch 0092, iter [00100, 05004], lr: 0.059211, loss: 2.4339
2022-03-03 08:33:03 - train: epoch 0092, iter [00200, 05004], lr: 0.059211, loss: 2.6380
2022-03-03 08:33:36 - train: epoch 0092, iter [00300, 05004], lr: 0.059211, loss: 2.6089
2022-03-03 08:34:08 - train: epoch 0092, iter [00400, 05004], lr: 0.059211, loss: 2.4656
2022-03-03 08:34:41 - train: epoch 0092, iter [00500, 05004], lr: 0.059211, loss: 2.4905
2022-03-03 08:35:13 - train: epoch 0092, iter [00600, 05004], lr: 0.059211, loss: 2.4471
2022-03-03 08:35:45 - train: epoch 0092, iter [00700, 05004], lr: 0.059211, loss: 2.5118
2022-03-03 08:36:19 - train: epoch 0092, iter [00800, 05004], lr: 0.059211, loss: 2.4985
2022-03-03 08:36:51 - train: epoch 0092, iter [00900, 05004], lr: 0.059211, loss: 2.3886
2022-03-03 08:37:23 - train: epoch 0092, iter [01000, 05004], lr: 0.059211, loss: 2.3785
2022-03-03 08:37:56 - train: epoch 0092, iter [01100, 05004], lr: 0.059211, loss: 2.4211
2022-03-03 08:38:28 - train: epoch 0092, iter [01200, 05004], lr: 0.059211, loss: 2.5860
2022-03-03 08:39:01 - train: epoch 0092, iter [01300, 05004], lr: 0.059211, loss: 2.5629
2022-03-03 08:39:33 - train: epoch 0092, iter [01400, 05004], lr: 0.059211, loss: 2.2962
2022-03-03 08:40:05 - train: epoch 0092, iter [01500, 05004], lr: 0.059211, loss: 2.3918
2022-03-03 08:40:38 - train: epoch 0092, iter [01600, 05004], lr: 0.059211, loss: 2.3865
2022-03-03 08:41:10 - train: epoch 0092, iter [01700, 05004], lr: 0.059211, loss: 2.4072
2022-03-03 08:41:42 - train: epoch 0092, iter [01800, 05004], lr: 0.059211, loss: 2.4941
2022-03-03 08:42:15 - train: epoch 0092, iter [01900, 05004], lr: 0.059211, loss: 2.3681
2022-03-03 08:42:47 - train: epoch 0092, iter [02000, 05004], lr: 0.059211, loss: 2.5729
2022-03-03 08:43:19 - train: epoch 0092, iter [02100, 05004], lr: 0.059211, loss: 2.4116
2022-03-03 08:43:53 - train: epoch 0092, iter [02200, 05004], lr: 0.059211, loss: 2.5810
2022-03-03 08:44:25 - train: epoch 0092, iter [02300, 05004], lr: 0.059211, loss: 2.5668
2022-03-03 08:44:58 - train: epoch 0092, iter [02400, 05004], lr: 0.059211, loss: 2.4270
2022-03-03 08:45:29 - train: epoch 0092, iter [02500, 05004], lr: 0.059211, loss: 2.5565
2022-03-03 08:46:02 - train: epoch 0092, iter [02600, 05004], lr: 0.059211, loss: 2.6059
2022-03-03 08:46:34 - train: epoch 0092, iter [02700, 05004], lr: 0.059211, loss: 2.6125
2022-03-03 08:47:06 - train: epoch 0092, iter [02800, 05004], lr: 0.059211, loss: 2.3660
2022-03-03 08:47:39 - train: epoch 0092, iter [02900, 05004], lr: 0.059211, loss: 2.8011
2022-03-03 08:48:11 - train: epoch 0092, iter [03000, 05004], lr: 0.059211, loss: 2.2780
2022-03-03 08:48:44 - train: epoch 0092, iter [03100, 05004], lr: 0.059211, loss: 2.5602
2022-03-03 08:49:17 - train: epoch 0092, iter [03200, 05004], lr: 0.059211, loss: 2.3008
2022-03-03 08:49:49 - train: epoch 0092, iter [03300, 05004], lr: 0.059211, loss: 2.4607
2022-03-03 08:50:22 - train: epoch 0092, iter [03400, 05004], lr: 0.059211, loss: 2.5784
2022-03-03 08:50:53 - train: epoch 0092, iter [03500, 05004], lr: 0.059211, loss: 2.3772
2022-03-03 08:51:25 - train: epoch 0092, iter [03600, 05004], lr: 0.059211, loss: 2.6427
2022-03-03 08:51:56 - train: epoch 0092, iter [03700, 05004], lr: 0.059211, loss: 2.3943
2022-03-03 08:52:30 - train: epoch 0092, iter [03800, 05004], lr: 0.059211, loss: 2.6647
2022-03-03 08:53:02 - train: epoch 0092, iter [03900, 05004], lr: 0.059211, loss: 2.6653
2022-03-03 08:53:34 - train: epoch 0092, iter [04000, 05004], lr: 0.059211, loss: 2.6748
2022-03-03 08:54:07 - train: epoch 0092, iter [04100, 05004], lr: 0.059211, loss: 2.4365
2022-03-03 08:54:40 - train: epoch 0092, iter [04200, 05004], lr: 0.059211, loss: 2.6818
2022-03-03 08:55:12 - train: epoch 0092, iter [04300, 05004], lr: 0.059211, loss: 2.4290
2022-03-03 08:55:44 - train: epoch 0092, iter [04400, 05004], lr: 0.059211, loss: 2.4655
2022-03-03 08:56:16 - train: epoch 0092, iter [04500, 05004], lr: 0.059211, loss: 2.5283
2022-03-03 08:56:49 - train: epoch 0092, iter [04600, 05004], lr: 0.059211, loss: 2.5117
2022-03-03 08:57:21 - train: epoch 0092, iter [04700, 05004], lr: 0.059211, loss: 2.4737
2022-03-03 08:57:53 - train: epoch 0092, iter [04800, 05004], lr: 0.059211, loss: 2.3784
2022-03-03 08:58:26 - train: epoch 0092, iter [04900, 05004], lr: 0.059211, loss: 2.3186
2022-03-03 08:58:57 - train: epoch 0092, iter [05000, 05004], lr: 0.059211, loss: 2.4176
2022-03-03 08:58:58 - train: epoch 092, train_loss: 2.4988
2022-03-03 09:00:10 - eval: epoch: 092, acc1: 57.886%, acc5: 82.208%, test_loss: 1.7592, per_image_load_time: 0.660ms, per_image_inference_time: 0.390ms
2022-03-03 09:00:10 - until epoch: 092, best_acc1: 59.496%
2022-03-03 09:00:10 - epoch 093 lr: 0.05841785206735192
2022-03-03 09:00:48 - train: epoch 0093, iter [00100, 05004], lr: 0.058418, loss: 2.2499
2022-03-03 09:01:21 - train: epoch 0093, iter [00200, 05004], lr: 0.058418, loss: 2.0547
2022-03-03 09:01:53 - train: epoch 0093, iter [00300, 05004], lr: 0.058418, loss: 2.4838
2022-03-03 09:02:27 - train: epoch 0093, iter [00400, 05004], lr: 0.058418, loss: 2.7878
2022-03-03 09:02:58 - train: epoch 0093, iter [00500, 05004], lr: 0.058418, loss: 2.6484
2022-03-03 09:03:31 - train: epoch 0093, iter [00600, 05004], lr: 0.058418, loss: 2.5286
2022-03-03 09:04:02 - train: epoch 0093, iter [00700, 05004], lr: 0.058418, loss: 2.5288
2022-03-03 09:04:35 - train: epoch 0093, iter [00800, 05004], lr: 0.058418, loss: 2.3955
2022-03-03 09:05:08 - train: epoch 0093, iter [00900, 05004], lr: 0.058418, loss: 2.3065
2022-03-03 09:05:40 - train: epoch 0093, iter [01000, 05004], lr: 0.058418, loss: 2.3416
2022-03-03 09:06:13 - train: epoch 0093, iter [01100, 05004], lr: 0.058418, loss: 2.4902
2022-03-03 09:06:46 - train: epoch 0093, iter [01200, 05004], lr: 0.058418, loss: 2.4224
2022-03-03 09:07:19 - train: epoch 0093, iter [01300, 05004], lr: 0.058418, loss: 2.4824
2022-03-03 09:07:51 - train: epoch 0093, iter [01400, 05004], lr: 0.058418, loss: 2.5885
2022-03-03 09:08:24 - train: epoch 0093, iter [01500, 05004], lr: 0.058418, loss: 2.4376
2022-03-03 09:08:56 - train: epoch 0093, iter [01600, 05004], lr: 0.058418, loss: 2.5399
2022-03-03 09:09:29 - train: epoch 0093, iter [01700, 05004], lr: 0.058418, loss: 2.4921
2022-03-03 09:10:01 - train: epoch 0093, iter [01800, 05004], lr: 0.058418, loss: 2.4879
2022-03-03 09:10:33 - train: epoch 0093, iter [01900, 05004], lr: 0.058418, loss: 2.2526
2022-03-03 09:11:05 - train: epoch 0093, iter [02000, 05004], lr: 0.058418, loss: 2.4483
2022-03-03 09:11:39 - train: epoch 0093, iter [02100, 05004], lr: 0.058418, loss: 2.3628
2022-03-03 09:12:11 - train: epoch 0093, iter [02200, 05004], lr: 0.058418, loss: 2.4711
2022-03-03 09:12:44 - train: epoch 0093, iter [02300, 05004], lr: 0.058418, loss: 2.5584
2022-03-03 09:13:16 - train: epoch 0093, iter [02400, 05004], lr: 0.058418, loss: 2.4014
2022-03-03 09:13:49 - train: epoch 0093, iter [02500, 05004], lr: 0.058418, loss: 2.3994
2022-03-03 09:14:22 - train: epoch 0093, iter [02600, 05004], lr: 0.058418, loss: 2.9942
2022-03-03 09:14:55 - train: epoch 0093, iter [02700, 05004], lr: 0.058418, loss: 2.2613
2022-03-03 09:15:26 - train: epoch 0093, iter [02800, 05004], lr: 0.058418, loss: 2.4721
2022-03-03 09:15:59 - train: epoch 0093, iter [02900, 05004], lr: 0.058418, loss: 2.3245
2022-03-03 09:16:31 - train: epoch 0093, iter [03000, 05004], lr: 0.058418, loss: 2.4756
2022-03-03 09:17:04 - train: epoch 0093, iter [03100, 05004], lr: 0.058418, loss: 2.5448
2022-03-03 09:17:36 - train: epoch 0093, iter [03200, 05004], lr: 0.058418, loss: 2.4638
2022-03-03 09:18:09 - train: epoch 0093, iter [03300, 05004], lr: 0.058418, loss: 2.7373
2022-03-03 09:18:42 - train: epoch 0093, iter [03400, 05004], lr: 0.058418, loss: 2.9496
2022-03-03 09:19:15 - train: epoch 0093, iter [03500, 05004], lr: 0.058418, loss: 2.2936
2022-03-03 09:19:47 - train: epoch 0093, iter [03600, 05004], lr: 0.058418, loss: 2.8341
2022-03-03 09:20:20 - train: epoch 0093, iter [03700, 05004], lr: 0.058418, loss: 2.4163
2022-03-03 09:20:52 - train: epoch 0093, iter [03800, 05004], lr: 0.058418, loss: 2.4873
2022-03-03 09:21:24 - train: epoch 0093, iter [03900, 05004], lr: 0.058418, loss: 2.4510
2022-03-03 09:21:55 - train: epoch 0093, iter [04000, 05004], lr: 0.058418, loss: 2.9539
2022-03-03 09:22:29 - train: epoch 0093, iter [04100, 05004], lr: 0.058418, loss: 2.6237
2022-03-03 09:23:01 - train: epoch 0093, iter [04200, 05004], lr: 0.058418, loss: 2.5862
2022-03-03 09:23:34 - train: epoch 0093, iter [04300, 05004], lr: 0.058418, loss: 2.5556
2022-03-03 09:24:06 - train: epoch 0093, iter [04400, 05004], lr: 0.058418, loss: 2.2900
2022-03-03 09:24:39 - train: epoch 0093, iter [04500, 05004], lr: 0.058418, loss: 2.3869
2022-03-03 09:25:12 - train: epoch 0093, iter [04600, 05004], lr: 0.058418, loss: 2.5033
2022-03-03 09:25:44 - train: epoch 0093, iter [04700, 05004], lr: 0.058418, loss: 2.7611
2022-03-03 09:26:16 - train: epoch 0093, iter [04800, 05004], lr: 0.058418, loss: 2.3035
2022-03-03 09:26:48 - train: epoch 0093, iter [04900, 05004], lr: 0.058418, loss: 2.6056
2022-03-03 09:27:19 - train: epoch 0093, iter [05000, 05004], lr: 0.058418, loss: 2.7315
2022-03-03 09:27:21 - train: epoch 093, train_loss: 2.4943
2022-03-03 09:28:32 - eval: epoch: 093, acc1: 57.740%, acc5: 81.988%, test_loss: 1.7757, per_image_load_time: 0.566ms, per_image_inference_time: 0.405ms
2022-03-03 09:28:33 - until epoch: 093, best_acc1: 59.496%
2022-03-03 09:28:33 - epoch 094 lr: 0.05762275557551727
2022-03-03 09:29:11 - train: epoch 0094, iter [00100, 05004], lr: 0.057623, loss: 2.6154
2022-03-03 09:29:44 - train: epoch 0094, iter [00200, 05004], lr: 0.057623, loss: 2.5126
2022-03-03 09:30:16 - train: epoch 0094, iter [00300, 05004], lr: 0.057623, loss: 2.8311
2022-03-03 09:30:48 - train: epoch 0094, iter [00400, 05004], lr: 0.057623, loss: 2.6078
2022-03-03 09:31:21 - train: epoch 0094, iter [00500, 05004], lr: 0.057623, loss: 2.2370
2022-03-03 09:31:53 - train: epoch 0094, iter [00600, 05004], lr: 0.057623, loss: 2.2459
2022-03-03 09:32:26 - train: epoch 0094, iter [00700, 05004], lr: 0.057623, loss: 2.4507
2022-03-03 09:32:59 - train: epoch 0094, iter [00800, 05004], lr: 0.057623, loss: 2.6084
2022-03-03 09:33:31 - train: epoch 0094, iter [00900, 05004], lr: 0.057623, loss: 2.5077
2022-03-03 09:34:04 - train: epoch 0094, iter [01000, 05004], lr: 0.057623, loss: 2.4275
2022-03-03 09:34:36 - train: epoch 0094, iter [01100, 05004], lr: 0.057623, loss: 2.8624
2022-03-03 09:35:08 - train: epoch 0094, iter [01200, 05004], lr: 0.057623, loss: 2.4776
2022-03-03 09:35:41 - train: epoch 0094, iter [01300, 05004], lr: 0.057623, loss: 2.5729
2022-03-03 09:36:13 - train: epoch 0094, iter [01400, 05004], lr: 0.057623, loss: 2.3759
2022-03-03 09:36:45 - train: epoch 0094, iter [01500, 05004], lr: 0.057623, loss: 2.3297
2022-03-03 09:37:19 - train: epoch 0094, iter [01600, 05004], lr: 0.057623, loss: 2.6304
2022-03-03 09:37:51 - train: epoch 0094, iter [01700, 05004], lr: 0.057623, loss: 2.4387
2022-03-03 09:38:23 - train: epoch 0094, iter [01800, 05004], lr: 0.057623, loss: 2.2024
2022-03-03 09:38:56 - train: epoch 0094, iter [01900, 05004], lr: 0.057623, loss: 2.5798
2022-03-03 09:39:28 - train: epoch 0094, iter [02000, 05004], lr: 0.057623, loss: 2.3118
2022-03-03 09:40:00 - train: epoch 0094, iter [02100, 05004], lr: 0.057623, loss: 2.1991
2022-03-03 09:40:32 - train: epoch 0094, iter [02200, 05004], lr: 0.057623, loss: 2.2727
2022-03-03 09:41:05 - train: epoch 0094, iter [02300, 05004], lr: 0.057623, loss: 2.3028
2022-03-03 09:41:38 - train: epoch 0094, iter [02400, 05004], lr: 0.057623, loss: 2.2524
2022-03-03 09:42:10 - train: epoch 0094, iter [02500, 05004], lr: 0.057623, loss: 2.4381
2022-03-03 09:42:43 - train: epoch 0094, iter [02600, 05004], lr: 0.057623, loss: 2.3657
2022-03-03 09:43:15 - train: epoch 0094, iter [02700, 05004], lr: 0.057623, loss: 2.4113
2022-03-03 09:43:48 - train: epoch 0094, iter [02800, 05004], lr: 0.057623, loss: 2.5498
2022-03-03 09:44:20 - train: epoch 0094, iter [02900, 05004], lr: 0.057623, loss: 2.3282
2022-03-03 09:44:52 - train: epoch 0094, iter [03000, 05004], lr: 0.057623, loss: 2.6358
2022-03-03 09:45:25 - train: epoch 0094, iter [03100, 05004], lr: 0.057623, loss: 2.3601
2022-03-03 09:45:57 - train: epoch 0094, iter [03200, 05004], lr: 0.057623, loss: 2.2999
2022-03-03 09:46:29 - train: epoch 0094, iter [03300, 05004], lr: 0.057623, loss: 2.5015
2022-03-03 09:47:03 - train: epoch 0094, iter [03400, 05004], lr: 0.057623, loss: 2.5122
2022-03-03 09:47:35 - train: epoch 0094, iter [03500, 05004], lr: 0.057623, loss: 2.7822
2022-03-03 09:48:08 - train: epoch 0094, iter [03600, 05004], lr: 0.057623, loss: 2.3628
2022-03-03 09:48:40 - train: epoch 0094, iter [03700, 05004], lr: 0.057623, loss: 2.4805
2022-03-03 09:49:13 - train: epoch 0094, iter [03800, 05004], lr: 0.057623, loss: 2.1131
2022-03-03 09:49:45 - train: epoch 0094, iter [03900, 05004], lr: 0.057623, loss: 2.4052
2022-03-03 09:50:17 - train: epoch 0094, iter [04000, 05004], lr: 0.057623, loss: 2.5480
2022-03-03 09:50:49 - train: epoch 0094, iter [04100, 05004], lr: 0.057623, loss: 2.4415
2022-03-03 09:51:21 - train: epoch 0094, iter [04200, 05004], lr: 0.057623, loss: 2.6584
2022-03-03 09:51:54 - train: epoch 0094, iter [04300, 05004], lr: 0.057623, loss: 2.6070
2022-03-03 09:52:26 - train: epoch 0094, iter [04400, 05004], lr: 0.057623, loss: 2.7483
2022-03-03 09:52:58 - train: epoch 0094, iter [04500, 05004], lr: 0.057623, loss: 2.5390
2022-03-03 09:53:31 - train: epoch 0094, iter [04600, 05004], lr: 0.057623, loss: 2.4555
2022-03-03 09:54:03 - train: epoch 0094, iter [04700, 05004], lr: 0.057623, loss: 2.6737
2022-03-03 09:54:36 - train: epoch 0094, iter [04800, 05004], lr: 0.057623, loss: 2.2910
2022-03-03 09:55:08 - train: epoch 0094, iter [04900, 05004], lr: 0.057623, loss: 2.5234
2022-03-03 09:55:39 - train: epoch 0094, iter [05000, 05004], lr: 0.057623, loss: 2.4893
2022-03-03 09:55:40 - train: epoch 094, train_loss: 2.4852
2022-03-03 09:56:51 - eval: epoch: 094, acc1: 59.442%, acc5: 83.392%, test_loss: 1.6807, per_image_load_time: 0.537ms, per_image_inference_time: 0.377ms
2022-03-03 09:56:52 - until epoch: 094, best_acc1: 59.496%
2022-03-03 09:56:52 - epoch 095 lr: 0.05682568059703659
2022-03-03 09:57:31 - train: epoch 0095, iter [00100, 05004], lr: 0.056826, loss: 2.5905
2022-03-03 09:58:03 - train: epoch 0095, iter [00200, 05004], lr: 0.056826, loss: 2.4724
2022-03-03 09:58:35 - train: epoch 0095, iter [00300, 05004], lr: 0.056826, loss: 2.4897
2022-03-03 09:59:07 - train: epoch 0095, iter [00400, 05004], lr: 0.056826, loss: 2.4653
2022-03-03 09:59:40 - train: epoch 0095, iter [00500, 05004], lr: 0.056826, loss: 2.5232
2022-03-03 10:00:13 - train: epoch 0095, iter [00600, 05004], lr: 0.056826, loss: 2.5244
2022-03-03 10:00:46 - train: epoch 0095, iter [00700, 05004], lr: 0.056826, loss: 2.5394
2022-03-03 10:01:17 - train: epoch 0095, iter [00800, 05004], lr: 0.056826, loss: 2.6967
2022-03-03 10:01:50 - train: epoch 0095, iter [00900, 05004], lr: 0.056826, loss: 2.6931
2022-03-03 10:02:22 - train: epoch 0095, iter [01000, 05004], lr: 0.056826, loss: 2.6621
2022-03-03 10:02:54 - train: epoch 0095, iter [01100, 05004], lr: 0.056826, loss: 2.4532
2022-03-03 10:03:27 - train: epoch 0095, iter [01200, 05004], lr: 0.056826, loss: 2.5189
2022-03-03 10:03:59 - train: epoch 0095, iter [01300, 05004], lr: 0.056826, loss: 2.5667
2022-03-03 10:04:32 - train: epoch 0095, iter [01400, 05004], lr: 0.056826, loss: 2.2028
2022-03-03 10:05:04 - train: epoch 0095, iter [01500, 05004], lr: 0.056826, loss: 2.5102
2022-03-03 10:05:37 - train: epoch 0095, iter [01600, 05004], lr: 0.056826, loss: 2.3746
2022-03-03 10:06:08 - train: epoch 0095, iter [01700, 05004], lr: 0.056826, loss: 2.3229
2022-03-03 10:06:41 - train: epoch 0095, iter [01800, 05004], lr: 0.056826, loss: 2.5353
2022-03-03 10:07:13 - train: epoch 0095, iter [01900, 05004], lr: 0.056826, loss: 2.3733
2022-03-03 10:07:46 - train: epoch 0095, iter [02000, 05004], lr: 0.056826, loss: 2.4580
2022-03-03 10:08:18 - train: epoch 0095, iter [02100, 05004], lr: 0.056826, loss: 2.4073
2022-03-03 10:08:51 - train: epoch 0095, iter [02200, 05004], lr: 0.056826, loss: 2.6335
2022-03-03 10:09:24 - train: epoch 0095, iter [02300, 05004], lr: 0.056826, loss: 2.4635
2022-03-03 10:09:57 - train: epoch 0095, iter [02400, 05004], lr: 0.056826, loss: 2.4733
2022-03-03 10:10:29 - train: epoch 0095, iter [02500, 05004], lr: 0.056826, loss: 2.4350
2022-03-03 10:11:01 - train: epoch 0095, iter [02600, 05004], lr: 0.056826, loss: 2.4391
2022-03-03 10:11:33 - train: epoch 0095, iter [02700, 05004], lr: 0.056826, loss: 2.3363
2022-03-03 10:12:06 - train: epoch 0095, iter [02800, 05004], lr: 0.056826, loss: 2.3623
2022-03-03 10:12:39 - train: epoch 0095, iter [02900, 05004], lr: 0.056826, loss: 2.8177
2022-03-03 10:13:12 - train: epoch 0095, iter [03000, 05004], lr: 0.056826, loss: 2.9887
2022-03-03 10:13:44 - train: epoch 0095, iter [03100, 05004], lr: 0.056826, loss: 2.7347
2022-03-03 10:14:17 - train: epoch 0095, iter [03200, 05004], lr: 0.056826, loss: 2.3727
2022-03-03 10:14:48 - train: epoch 0095, iter [03300, 05004], lr: 0.056826, loss: 2.6473
2022-03-03 10:15:21 - train: epoch 0095, iter [03400, 05004], lr: 0.056826, loss: 2.2130
2022-03-03 10:15:53 - train: epoch 0095, iter [03500, 05004], lr: 0.056826, loss: 2.4127
2022-03-03 10:16:25 - train: epoch 0095, iter [03600, 05004], lr: 0.056826, loss: 2.2394
2022-03-03 10:16:57 - train: epoch 0095, iter [03700, 05004], lr: 0.056826, loss: 2.6568
2022-03-03 10:17:30 - train: epoch 0095, iter [03800, 05004], lr: 0.056826, loss: 2.4785
2022-03-03 10:18:03 - train: epoch 0095, iter [03900, 05004], lr: 0.056826, loss: 2.8172
2022-03-03 10:18:36 - train: epoch 0095, iter [04000, 05004], lr: 0.056826, loss: 2.4055
2022-03-03 10:19:08 - train: epoch 0095, iter [04100, 05004], lr: 0.056826, loss: 2.3257
2022-03-03 10:19:41 - train: epoch 0095, iter [04200, 05004], lr: 0.056826, loss: 2.3291
2022-03-03 10:20:12 - train: epoch 0095, iter [04300, 05004], lr: 0.056826, loss: 2.4985
2022-03-03 10:20:45 - train: epoch 0095, iter [04400, 05004], lr: 0.056826, loss: 2.7831
2022-03-03 10:21:17 - train: epoch 0095, iter [04500, 05004], lr: 0.056826, loss: 2.4573
2022-03-03 10:21:50 - train: epoch 0095, iter [04600, 05004], lr: 0.056826, loss: 2.4964
2022-03-03 10:22:22 - train: epoch 0095, iter [04700, 05004], lr: 0.056826, loss: 2.4729
2022-03-03 10:22:55 - train: epoch 0095, iter [04800, 05004], lr: 0.056826, loss: 2.4104
2022-03-03 10:23:28 - train: epoch 0095, iter [04900, 05004], lr: 0.056826, loss: 2.4350
2022-03-03 10:23:59 - train: epoch 0095, iter [05000, 05004], lr: 0.056826, loss: 2.2588
2022-03-03 10:24:00 - train: epoch 095, train_loss: 2.4805
2022-03-03 10:25:12 - eval: epoch: 095, acc1: 57.586%, acc5: 81.772%, test_loss: 1.7833, per_image_load_time: 0.537ms, per_image_inference_time: 0.397ms
2022-03-03 10:25:12 - until epoch: 095, best_acc1: 59.496%
2022-03-03 10:25:12 - epoch 096 lr: 0.05602683401276615
2022-03-03 10:25:50 - train: epoch 0096, iter [00100, 05004], lr: 0.056027, loss: 2.4036
2022-03-03 10:26:22 - train: epoch 0096, iter [00200, 05004], lr: 0.056027, loss: 2.3927
2022-03-03 10:26:55 - train: epoch 0096, iter [00300, 05004], lr: 0.056027, loss: 2.4020
2022-03-03 10:27:27 - train: epoch 0096, iter [00400, 05004], lr: 0.056027, loss: 2.3482
2022-03-03 10:28:00 - train: epoch 0096, iter [00500, 05004], lr: 0.056027, loss: 2.3213
2022-03-03 10:28:32 - train: epoch 0096, iter [00600, 05004], lr: 0.056027, loss: 2.3438
2022-03-03 10:29:04 - train: epoch 0096, iter [00700, 05004], lr: 0.056027, loss: 2.4054
2022-03-03 10:29:37 - train: epoch 0096, iter [00800, 05004], lr: 0.056027, loss: 2.2992
2022-03-03 10:30:10 - train: epoch 0096, iter [00900, 05004], lr: 0.056027, loss: 2.4493
2022-03-03 10:30:43 - train: epoch 0096, iter [01000, 05004], lr: 0.056027, loss: 2.3458
2022-03-03 10:31:15 - train: epoch 0096, iter [01100, 05004], lr: 0.056027, loss: 2.3655
2022-03-03 10:31:48 - train: epoch 0096, iter [01200, 05004], lr: 0.056027, loss: 2.3223
2022-03-03 10:32:19 - train: epoch 0096, iter [01300, 05004], lr: 0.056027, loss: 2.6028
2022-03-03 10:32:52 - train: epoch 0096, iter [01400, 05004], lr: 0.056027, loss: 2.4266
2022-03-03 10:33:23 - train: epoch 0096, iter [01500, 05004], lr: 0.056027, loss: 2.5340
2022-03-03 10:33:57 - train: epoch 0096, iter [01600, 05004], lr: 0.056027, loss: 2.1217
2022-03-03 10:34:29 - train: epoch 0096, iter [01700, 05004], lr: 0.056027, loss: 2.1478
2022-03-03 10:35:02 - train: epoch 0096, iter [01800, 05004], lr: 0.056027, loss: 2.5760
2022-03-03 10:35:34 - train: epoch 0096, iter [01900, 05004], lr: 0.056027, loss: 2.4635
2022-03-03 10:36:07 - train: epoch 0096, iter [02000, 05004], lr: 0.056027, loss: 2.5547
2022-03-03 10:36:39 - train: epoch 0096, iter [02100, 05004], lr: 0.056027, loss: 2.5927
2022-03-03 10:37:12 - train: epoch 0096, iter [02200, 05004], lr: 0.056027, loss: 2.0787
2022-03-03 10:37:45 - train: epoch 0096, iter [02300, 05004], lr: 0.056027, loss: 2.5075
2022-03-03 10:38:17 - train: epoch 0096, iter [02400, 05004], lr: 0.056027, loss: 2.3178
2022-03-03 10:38:49 - train: epoch 0096, iter [02500, 05004], lr: 0.056027, loss: 2.3660
2022-03-03 10:39:22 - train: epoch 0096, iter [02600, 05004], lr: 0.056027, loss: 2.4702
2022-03-03 10:39:55 - train: epoch 0096, iter [02700, 05004], lr: 0.056027, loss: 2.4708
2022-03-03 10:40:27 - train: epoch 0096, iter [02800, 05004], lr: 0.056027, loss: 2.4617
2022-03-03 10:40:59 - train: epoch 0096, iter [02900, 05004], lr: 0.056027, loss: 2.5108
2022-03-03 10:41:32 - train: epoch 0096, iter [03000, 05004], lr: 0.056027, loss: 2.5178
2022-03-03 10:42:04 - train: epoch 0096, iter [03100, 05004], lr: 0.056027, loss: 2.5879
2022-03-03 10:42:37 - train: epoch 0096, iter [03200, 05004], lr: 0.056027, loss: 2.6784
2022-03-03 10:43:10 - train: epoch 0096, iter [03300, 05004], lr: 0.056027, loss: 2.7818
2022-03-03 10:43:42 - train: epoch 0096, iter [03400, 05004], lr: 0.056027, loss: 2.5615
2022-03-03 10:44:14 - train: epoch 0096, iter [03500, 05004], lr: 0.056027, loss: 2.4495
2022-03-03 10:44:47 - train: epoch 0096, iter [03600, 05004], lr: 0.056027, loss: 2.3416
2022-03-03 10:45:20 - train: epoch 0096, iter [03700, 05004], lr: 0.056027, loss: 2.4241
2022-03-03 10:45:52 - train: epoch 0096, iter [03800, 05004], lr: 0.056027, loss: 2.1631
2022-03-03 10:46:24 - train: epoch 0096, iter [03900, 05004], lr: 0.056027, loss: 2.2219
2022-03-03 10:46:57 - train: epoch 0096, iter [04000, 05004], lr: 0.056027, loss: 2.5673
2022-03-03 10:47:29 - train: epoch 0096, iter [04100, 05004], lr: 0.056027, loss: 2.5720
2022-03-03 10:48:02 - train: epoch 0096, iter [04200, 05004], lr: 0.056027, loss: 2.4255
2022-03-03 10:48:34 - train: epoch 0096, iter [04300, 05004], lr: 0.056027, loss: 2.1567
2022-03-03 10:49:06 - train: epoch 0096, iter [04400, 05004], lr: 0.056027, loss: 2.3243
2022-03-03 10:49:40 - train: epoch 0096, iter [04500, 05004], lr: 0.056027, loss: 2.5948
2022-03-03 10:50:11 - train: epoch 0096, iter [04600, 05004], lr: 0.056027, loss: 2.4469
2022-03-03 10:50:44 - train: epoch 0096, iter [04700, 05004], lr: 0.056027, loss: 2.4966
2022-03-03 10:51:16 - train: epoch 0096, iter [04800, 05004], lr: 0.056027, loss: 2.5490
2022-03-03 10:51:49 - train: epoch 0096, iter [04900, 05004], lr: 0.056027, loss: 2.6918
2022-03-03 10:52:20 - train: epoch 0096, iter [05000, 05004], lr: 0.056027, loss: 2.2811
2022-03-03 10:52:21 - train: epoch 096, train_loss: 2.4722
2022-03-03 10:53:33 - eval: epoch: 096, acc1: 58.400%, acc5: 82.872%, test_loss: 1.7268, per_image_load_time: 0.766ms, per_image_inference_time: 0.429ms
2022-03-03 10:53:33 - until epoch: 096, best_acc1: 59.496%
2022-03-03 10:53:33 - epoch 097 lr: 0.05522642316338268
2022-03-03 10:54:12 - train: epoch 0097, iter [00100, 05004], lr: 0.055226, loss: 2.4218
2022-03-03 10:54:45 - train: epoch 0097, iter [00200, 05004], lr: 0.055226, loss: 2.3925
2022-03-03 10:55:16 - train: epoch 0097, iter [00300, 05004], lr: 0.055226, loss: 2.6480
2022-03-03 10:55:49 - train: epoch 0097, iter [00400, 05004], lr: 0.055226, loss: 2.5919
2022-03-03 10:56:21 - train: epoch 0097, iter [00500, 05004], lr: 0.055226, loss: 2.5277
2022-03-03 10:56:53 - train: epoch 0097, iter [00600, 05004], lr: 0.055226, loss: 2.3799
2022-03-03 10:57:26 - train: epoch 0097, iter [00700, 05004], lr: 0.055226, loss: 2.2626
2022-03-03 10:57:59 - train: epoch 0097, iter [00800, 05004], lr: 0.055226, loss: 2.3983
2022-03-03 10:58:31 - train: epoch 0097, iter [00900, 05004], lr: 0.055226, loss: 2.4676
2022-03-03 10:59:04 - train: epoch 0097, iter [01000, 05004], lr: 0.055226, loss: 2.6787
2022-03-03 10:59:36 - train: epoch 0097, iter [01100, 05004], lr: 0.055226, loss: 2.3637
2022-03-03 11:00:08 - train: epoch 0097, iter [01200, 05004], lr: 0.055226, loss: 2.6105
2022-03-03 11:00:41 - train: epoch 0097, iter [01300, 05004], lr: 0.055226, loss: 2.5144
2022-03-03 11:01:14 - train: epoch 0097, iter [01400, 05004], lr: 0.055226, loss: 2.3450
2022-03-03 11:01:46 - train: epoch 0097, iter [01500, 05004], lr: 0.055226, loss: 2.5837
2022-03-03 11:02:18 - train: epoch 0097, iter [01600, 05004], lr: 0.055226, loss: 2.5376
2022-03-03 11:02:51 - train: epoch 0097, iter [01700, 05004], lr: 0.055226, loss: 2.4514
2022-03-03 11:03:24 - train: epoch 0097, iter [01800, 05004], lr: 0.055226, loss: 2.6740
2022-03-03 11:03:55 - train: epoch 0097, iter [01900, 05004], lr: 0.055226, loss: 2.5984
2022-03-03 11:04:28 - train: epoch 0097, iter [02000, 05004], lr: 0.055226, loss: 2.5655
2022-03-03 11:05:00 - train: epoch 0097, iter [02100, 05004], lr: 0.055226, loss: 2.2363
2022-03-03 11:05:33 - train: epoch 0097, iter [02200, 05004], lr: 0.055226, loss: 2.2993
2022-03-03 11:06:05 - train: epoch 0097, iter [02300, 05004], lr: 0.055226, loss: 2.4392
2022-03-03 11:06:38 - train: epoch 0097, iter [02400, 05004], lr: 0.055226, loss: 2.3171
2022-03-03 11:07:10 - train: epoch 0097, iter [02500, 05004], lr: 0.055226, loss: 2.2302
2022-03-03 11:07:42 - train: epoch 0097, iter [02600, 05004], lr: 0.055226, loss: 2.6418
2022-03-03 11:08:15 - train: epoch 0097, iter [02700, 05004], lr: 0.055226, loss: 2.4034
2022-03-03 11:08:47 - train: epoch 0097, iter [02800, 05004], lr: 0.055226, loss: 2.5975
2022-03-03 11:09:20 - train: epoch 0097, iter [02900, 05004], lr: 0.055226, loss: 2.4012
2022-03-03 11:09:52 - train: epoch 0097, iter [03000, 05004], lr: 0.055226, loss: 2.5996
2022-03-03 11:10:25 - train: epoch 0097, iter [03100, 05004], lr: 0.055226, loss: 2.4087
2022-03-03 11:10:58 - train: epoch 0097, iter [03200, 05004], lr: 0.055226, loss: 2.6101
2022-03-03 11:11:31 - train: epoch 0097, iter [03300, 05004], lr: 0.055226, loss: 2.6098
2022-03-03 11:12:03 - train: epoch 0097, iter [03400, 05004], lr: 0.055226, loss: 2.5045
2022-03-03 11:12:35 - train: epoch 0097, iter [03500, 05004], lr: 0.055226, loss: 2.3000
2022-03-03 11:13:07 - train: epoch 0097, iter [03600, 05004], lr: 0.055226, loss: 2.3828
2022-03-03 11:13:39 - train: epoch 0097, iter [03700, 05004], lr: 0.055226, loss: 2.2178
2022-03-03 11:14:12 - train: epoch 0097, iter [03800, 05004], lr: 0.055226, loss: 2.5612
2022-03-03 11:14:44 - train: epoch 0097, iter [03900, 05004], lr: 0.055226, loss: 2.5699
2022-03-03 11:15:16 - train: epoch 0097, iter [04000, 05004], lr: 0.055226, loss: 2.3836
2022-03-03 11:15:49 - train: epoch 0097, iter [04100, 05004], lr: 0.055226, loss: 2.3481
2022-03-03 11:16:21 - train: epoch 0097, iter [04200, 05004], lr: 0.055226, loss: 2.2772
2022-03-03 11:16:54 - train: epoch 0097, iter [04300, 05004], lr: 0.055226, loss: 2.2276
2022-03-03 11:17:26 - train: epoch 0097, iter [04400, 05004], lr: 0.055226, loss: 2.4650
2022-03-03 11:17:59 - train: epoch 0097, iter [04500, 05004], lr: 0.055226, loss: 2.6970
2022-03-03 11:18:31 - train: epoch 0097, iter [04600, 05004], lr: 0.055226, loss: 2.5383
2022-03-03 11:19:04 - train: epoch 0097, iter [04700, 05004], lr: 0.055226, loss: 2.4235
2022-03-03 11:19:36 - train: epoch 0097, iter [04800, 05004], lr: 0.055226, loss: 2.5922
2022-03-03 11:20:09 - train: epoch 0097, iter [04900, 05004], lr: 0.055226, loss: 2.6067
2022-03-03 11:20:40 - train: epoch 0097, iter [05000, 05004], lr: 0.055226, loss: 2.4158
2022-03-03 11:20:41 - train: epoch 097, train_loss: 2.4663
2022-03-03 11:21:52 - eval: epoch: 097, acc1: 57.204%, acc5: 81.678%, test_loss: 1.7942, per_image_load_time: 0.553ms, per_image_inference_time: 0.364ms
2022-03-03 11:21:53 - until epoch: 097, best_acc1: 59.496%
2022-03-03 11:21:53 - epoch 098 lr: 0.054424655795567926
2022-03-03 11:22:31 - train: epoch 0098, iter [00100, 05004], lr: 0.054425, loss: 2.6269
2022-03-03 11:23:03 - train: epoch 0098, iter [00200, 05004], lr: 0.054425, loss: 2.6238
2022-03-03 11:23:37 - train: epoch 0098, iter [00300, 05004], lr: 0.054425, loss: 2.6045
2022-03-03 11:24:09 - train: epoch 0098, iter [00400, 05004], lr: 0.054425, loss: 2.4121
2022-03-03 11:24:42 - train: epoch 0098, iter [00500, 05004], lr: 0.054425, loss: 2.1400
2022-03-03 11:25:14 - train: epoch 0098, iter [00600, 05004], lr: 0.054425, loss: 2.4324
2022-03-03 11:25:47 - train: epoch 0098, iter [00700, 05004], lr: 0.054425, loss: 2.6251
2022-03-03 11:26:18 - train: epoch 0098, iter [00800, 05004], lr: 0.054425, loss: 2.7761
2022-03-03 11:26:50 - train: epoch 0098, iter [00900, 05004], lr: 0.054425, loss: 2.3568
2022-03-03 11:27:22 - train: epoch 0098, iter [01000, 05004], lr: 0.054425, loss: 2.3450
2022-03-03 11:27:56 - train: epoch 0098, iter [01100, 05004], lr: 0.054425, loss: 2.3648
2022-03-03 11:28:28 - train: epoch 0098, iter [01200, 05004], lr: 0.054425, loss: 2.3932
2022-03-03 11:29:00 - train: epoch 0098, iter [01300, 05004], lr: 0.054425, loss: 2.3664
2022-03-03 11:29:33 - train: epoch 0098, iter [01400, 05004], lr: 0.054425, loss: 2.5334
2022-03-03 11:30:06 - train: epoch 0098, iter [01500, 05004], lr: 0.054425, loss: 2.3525
2022-03-03 11:30:38 - train: epoch 0098, iter [01600, 05004], lr: 0.054425, loss: 2.7335
2022-03-03 11:31:10 - train: epoch 0098, iter [01700, 05004], lr: 0.054425, loss: 2.5392
2022-03-03 11:31:42 - train: epoch 0098, iter [01800, 05004], lr: 0.054425, loss: 2.4133
2022-03-03 11:32:14 - train: epoch 0098, iter [01900, 05004], lr: 0.054425, loss: 2.3431
2022-03-03 11:32:47 - train: epoch 0098, iter [02000, 05004], lr: 0.054425, loss: 2.3926
2022-03-03 11:33:20 - train: epoch 0098, iter [02100, 05004], lr: 0.054425, loss: 2.5400
2022-03-03 11:33:52 - train: epoch 0098, iter [02200, 05004], lr: 0.054425, loss: 2.3401
2022-03-03 11:34:25 - train: epoch 0098, iter [02300, 05004], lr: 0.054425, loss: 2.4153
2022-03-03 11:34:57 - train: epoch 0098, iter [02400, 05004], lr: 0.054425, loss: 2.5710
2022-03-03 11:35:29 - train: epoch 0098, iter [02500, 05004], lr: 0.054425, loss: 2.5319
2022-03-03 11:36:02 - train: epoch 0098, iter [02600, 05004], lr: 0.054425, loss: 2.3693
2022-03-03 11:36:34 - train: epoch 0098, iter [02700, 05004], lr: 0.054425, loss: 2.5666
2022-03-03 11:37:05 - train: epoch 0098, iter [02800, 05004], lr: 0.054425, loss: 2.5068
2022-03-03 11:37:39 - train: epoch 0098, iter [02900, 05004], lr: 0.054425, loss: 2.4427
2022-03-03 11:38:11 - train: epoch 0098, iter [03000, 05004], lr: 0.054425, loss: 2.3685
2022-03-03 11:38:44 - train: epoch 0098, iter [03100, 05004], lr: 0.054425, loss: 2.4113
2022-03-03 11:39:17 - train: epoch 0098, iter [03200, 05004], lr: 0.054425, loss: 2.2730
2022-03-03 11:39:49 - train: epoch 0098, iter [03300, 05004], lr: 0.054425, loss: 2.3535
2022-03-03 11:40:21 - train: epoch 0098, iter [03400, 05004], lr: 0.054425, loss: 2.7135
2022-03-03 11:40:54 - train: epoch 0098, iter [03500, 05004], lr: 0.054425, loss: 2.6103
2022-03-03 11:41:26 - train: epoch 0098, iter [03600, 05004], lr: 0.054425, loss: 2.7889
2022-03-03 11:41:58 - train: epoch 0098, iter [03700, 05004], lr: 0.054425, loss: 2.6522
2022-03-03 11:42:31 - train: epoch 0098, iter [03800, 05004], lr: 0.054425, loss: 2.6834
2022-03-03 11:43:03 - train: epoch 0098, iter [03900, 05004], lr: 0.054425, loss: 2.4381
2022-03-03 11:43:36 - train: epoch 0098, iter [04000, 05004], lr: 0.054425, loss: 2.2304
2022-03-03 11:44:08 - train: epoch 0098, iter [04100, 05004], lr: 0.054425, loss: 2.6115
2022-03-03 11:44:41 - train: epoch 0098, iter [04200, 05004], lr: 0.054425, loss: 2.6233
2022-03-03 11:45:13 - train: epoch 0098, iter [04300, 05004], lr: 0.054425, loss: 2.2655
2022-03-03 11:45:46 - train: epoch 0098, iter [04400, 05004], lr: 0.054425, loss: 2.7697
2022-03-03 11:46:18 - train: epoch 0098, iter [04500, 05004], lr: 0.054425, loss: 2.4897
2022-03-03 11:46:50 - train: epoch 0098, iter [04600, 05004], lr: 0.054425, loss: 2.5203
2022-03-03 11:47:23 - train: epoch 0098, iter [04700, 05004], lr: 0.054425, loss: 2.2608
2022-03-03 11:47:56 - train: epoch 0098, iter [04800, 05004], lr: 0.054425, loss: 2.4924
2022-03-03 11:48:28 - train: epoch 0098, iter [04900, 05004], lr: 0.054425, loss: 2.4567
2022-03-03 11:48:59 - train: epoch 0098, iter [05000, 05004], lr: 0.054425, loss: 2.5124
2022-03-03 11:49:00 - train: epoch 098, train_loss: 2.4603
2022-03-03 11:50:12 - eval: epoch: 098, acc1: 59.208%, acc5: 83.192%, test_loss: 1.7058, per_image_load_time: 1.769ms, per_image_inference_time: 0.427ms
2022-03-03 11:50:12 - until epoch: 098, best_acc1: 59.496%
2022-03-03 11:50:12 - epoch 099 lr: 0.05362174000808813
2022-03-03 11:50:50 - train: epoch 0099, iter [00100, 05004], lr: 0.053622, loss: 2.5598
2022-03-03 11:51:23 - train: epoch 0099, iter [00200, 05004], lr: 0.053622, loss: 2.3509
2022-03-03 11:51:55 - train: epoch 0099, iter [00300, 05004], lr: 0.053622, loss: 2.3903
2022-03-03 11:52:28 - train: epoch 0099, iter [00400, 05004], lr: 0.053622, loss: 2.3567
2022-03-03 11:52:59 - train: epoch 0099, iter [00500, 05004], lr: 0.053622, loss: 2.3146
2022-03-03 11:53:32 - train: epoch 0099, iter [00600, 05004], lr: 0.053622, loss: 2.6100
2022-03-03 11:54:05 - train: epoch 0099, iter [00700, 05004], lr: 0.053622, loss: 2.6275
2022-03-03 11:54:38 - train: epoch 0099, iter [00800, 05004], lr: 0.053622, loss: 2.1782
2022-03-03 11:55:09 - train: epoch 0099, iter [00900, 05004], lr: 0.053622, loss: 2.3623
2022-03-03 11:55:42 - train: epoch 0099, iter [01000, 05004], lr: 0.053622, loss: 2.2380
2022-03-03 11:56:15 - train: epoch 0099, iter [01100, 05004], lr: 0.053622, loss: 2.7034
2022-03-03 11:56:48 - train: epoch 0099, iter [01200, 05004], lr: 0.053622, loss: 2.2603
2022-03-03 11:57:19 - train: epoch 0099, iter [01300, 05004], lr: 0.053622, loss: 2.7077
2022-03-03 11:57:52 - train: epoch 0099, iter [01400, 05004], lr: 0.053622, loss: 2.2515
2022-03-03 11:58:25 - train: epoch 0099, iter [01500, 05004], lr: 0.053622, loss: 2.5395
2022-03-03 11:58:57 - train: epoch 0099, iter [01600, 05004], lr: 0.053622, loss: 2.5064
2022-03-03 11:59:30 - train: epoch 0099, iter [01700, 05004], lr: 0.053622, loss: 2.5753
2022-03-03 12:00:02 - train: epoch 0099, iter [01800, 05004], lr: 0.053622, loss: 2.6321
2022-03-03 12:00:34 - train: epoch 0099, iter [01900, 05004], lr: 0.053622, loss: 2.3616
2022-03-03 12:01:08 - train: epoch 0099, iter [02000, 05004], lr: 0.053622, loss: 2.5118
2022-03-03 12:01:39 - train: epoch 0099, iter [02100, 05004], lr: 0.053622, loss: 2.2946
2022-03-03 12:02:12 - train: epoch 0099, iter [02200, 05004], lr: 0.053622, loss: 2.3610
2022-03-03 12:02:44 - train: epoch 0099, iter [02300, 05004], lr: 0.053622, loss: 2.3349
2022-03-03 12:03:17 - train: epoch 0099, iter [02400, 05004], lr: 0.053622, loss: 2.6994
2022-03-03 12:03:50 - train: epoch 0099, iter [02500, 05004], lr: 0.053622, loss: 2.2800
2022-03-03 12:04:23 - train: epoch 0099, iter [02600, 05004], lr: 0.053622, loss: 2.5456
2022-03-03 12:04:55 - train: epoch 0099, iter [02700, 05004], lr: 0.053622, loss: 2.5192
2022-03-03 12:05:28 - train: epoch 0099, iter [02800, 05004], lr: 0.053622, loss: 2.3187
2022-03-03 12:06:00 - train: epoch 0099, iter [02900, 05004], lr: 0.053622, loss: 2.5102
2022-03-03 12:06:32 - train: epoch 0099, iter [03000, 05004], lr: 0.053622, loss: 2.5299
2022-03-03 12:07:04 - train: epoch 0099, iter [03100, 05004], lr: 0.053622, loss: 2.4139
2022-03-03 12:07:37 - train: epoch 0099, iter [03200, 05004], lr: 0.053622, loss: 2.5011
2022-03-03 12:08:09 - train: epoch 0099, iter [03300, 05004], lr: 0.053622, loss: 2.6277
2022-03-03 12:08:41 - train: epoch 0099, iter [03400, 05004], lr: 0.053622, loss: 2.3965
2022-03-03 12:09:14 - train: epoch 0099, iter [03500, 05004], lr: 0.053622, loss: 2.4995
2022-03-03 12:09:46 - train: epoch 0099, iter [03600, 05004], lr: 0.053622, loss: 2.3184
2022-03-03 12:10:19 - train: epoch 0099, iter [03700, 05004], lr: 0.053622, loss: 2.2557
2022-03-03 12:10:50 - train: epoch 0099, iter [03800, 05004], lr: 0.053622, loss: 2.3753
2022-03-03 12:11:23 - train: epoch 0099, iter [03900, 05004], lr: 0.053622, loss: 2.4899
2022-03-03 12:11:56 - train: epoch 0099, iter [04000, 05004], lr: 0.053622, loss: 2.3600
2022-03-03 12:12:28 - train: epoch 0099, iter [04100, 05004], lr: 0.053622, loss: 2.2077
2022-03-03 12:13:00 - train: epoch 0099, iter [04200, 05004], lr: 0.053622, loss: 2.7080
2022-03-03 12:13:33 - train: epoch 0099, iter [04300, 05004], lr: 0.053622, loss: 2.4631
2022-03-03 12:14:05 - train: epoch 0099, iter [04400, 05004], lr: 0.053622, loss: 2.4635
2022-03-03 12:14:38 - train: epoch 0099, iter [04500, 05004], lr: 0.053622, loss: 2.5630
2022-03-03 12:15:10 - train: epoch 0099, iter [04600, 05004], lr: 0.053622, loss: 2.5210
2022-03-03 12:15:43 - train: epoch 0099, iter [04700, 05004], lr: 0.053622, loss: 2.4648
2022-03-03 12:16:15 - train: epoch 0099, iter [04800, 05004], lr: 0.053622, loss: 2.5649
2022-03-03 12:16:47 - train: epoch 0099, iter [04900, 05004], lr: 0.053622, loss: 2.5263
2022-03-03 12:17:19 - train: epoch 0099, iter [05000, 05004], lr: 0.053622, loss: 2.4789
2022-03-03 12:17:20 - train: epoch 099, train_loss: 2.4549
2022-03-03 12:18:32 - eval: epoch: 099, acc1: 60.026%, acc5: 83.976%, test_loss: 1.6604, per_image_load_time: 0.556ms, per_image_inference_time: 0.397ms
2022-03-03 12:18:33 - until epoch: 099, best_acc1: 60.026%
2022-03-03 12:18:33 - epoch 100 lr: 0.05281788419778188
2022-03-03 12:19:10 - train: epoch 0100, iter [00100, 05004], lr: 0.052818, loss: 2.4377
2022-03-03 12:19:43 - train: epoch 0100, iter [00200, 05004], lr: 0.052818, loss: 2.4842
2022-03-03 12:20:15 - train: epoch 0100, iter [00300, 05004], lr: 0.052818, loss: 2.3896
2022-03-03 12:20:48 - train: epoch 0100, iter [00400, 05004], lr: 0.052818, loss: 2.3049
2022-03-03 12:21:20 - train: epoch 0100, iter [00500, 05004], lr: 0.052818, loss: 2.4468
2022-03-03 12:21:53 - train: epoch 0100, iter [00600, 05004], lr: 0.052818, loss: 2.5433
2022-03-03 12:22:25 - train: epoch 0100, iter [00700, 05004], lr: 0.052818, loss: 2.1661
2022-03-03 12:22:59 - train: epoch 0100, iter [00800, 05004], lr: 0.052818, loss: 2.3965
2022-03-03 12:23:31 - train: epoch 0100, iter [00900, 05004], lr: 0.052818, loss: 2.4154
2022-03-03 12:24:04 - train: epoch 0100, iter [01000, 05004], lr: 0.052818, loss: 2.5528
2022-03-03 12:24:36 - train: epoch 0100, iter [01100, 05004], lr: 0.052818, loss: 2.1521
2022-03-03 12:25:09 - train: epoch 0100, iter [01200, 05004], lr: 0.052818, loss: 2.4840
2022-03-03 12:25:41 - train: epoch 0100, iter [01300, 05004], lr: 0.052818, loss: 2.3980
2022-03-03 12:26:15 - train: epoch 0100, iter [01400, 05004], lr: 0.052818, loss: 2.6242
2022-03-03 12:26:47 - train: epoch 0100, iter [01500, 05004], lr: 0.052818, loss: 2.5507
2022-03-03 12:27:20 - train: epoch 0100, iter [01600, 05004], lr: 0.052818, loss: 2.4083
2022-03-03 12:27:51 - train: epoch 0100, iter [01700, 05004], lr: 0.052818, loss: 2.2075
2022-03-03 12:28:24 - train: epoch 0100, iter [01800, 05004], lr: 0.052818, loss: 2.3404
2022-03-03 12:28:57 - train: epoch 0100, iter [01900, 05004], lr: 0.052818, loss: 2.4278
2022-03-03 12:29:30 - train: epoch 0100, iter [02000, 05004], lr: 0.052818, loss: 2.7121
2022-03-03 12:30:02 - train: epoch 0100, iter [02100, 05004], lr: 0.052818, loss: 2.3234
2022-03-03 12:30:35 - train: epoch 0100, iter [02200, 05004], lr: 0.052818, loss: 2.5188
2022-03-03 12:31:07 - train: epoch 0100, iter [02300, 05004], lr: 0.052818, loss: 2.6005
2022-03-03 12:31:40 - train: epoch 0100, iter [02400, 05004], lr: 0.052818, loss: 2.6839
2022-03-03 12:32:13 - train: epoch 0100, iter [02500, 05004], lr: 0.052818, loss: 2.7515
2022-03-03 12:32:45 - train: epoch 0100, iter [02600, 05004], lr: 0.052818, loss: 2.2385
2022-03-03 12:33:17 - train: epoch 0100, iter [02700, 05004], lr: 0.052818, loss: 2.3720
2022-03-03 12:33:50 - train: epoch 0100, iter [02800, 05004], lr: 0.052818, loss: 2.7518
2022-03-03 12:34:22 - train: epoch 0100, iter [02900, 05004], lr: 0.052818, loss: 2.6758
2022-03-03 12:34:56 - train: epoch 0100, iter [03000, 05004], lr: 0.052818, loss: 2.5661
2022-03-03 12:35:29 - train: epoch 0100, iter [03100, 05004], lr: 0.052818, loss: 2.4908
2022-03-03 12:36:01 - train: epoch 0100, iter [03200, 05004], lr: 0.052818, loss: 2.7201
2022-03-03 12:36:33 - train: epoch 0100, iter [03300, 05004], lr: 0.052818, loss: 2.4888
2022-03-03 12:37:06 - train: epoch 0100, iter [03400, 05004], lr: 0.052818, loss: 2.4210
2022-03-03 12:37:37 - train: epoch 0100, iter [03500, 05004], lr: 0.052818, loss: 2.4387
2022-03-03 12:38:10 - train: epoch 0100, iter [03600, 05004], lr: 0.052818, loss: 2.5800
2022-03-03 12:38:43 - train: epoch 0100, iter [03700, 05004], lr: 0.052818, loss: 2.3225
2022-03-03 12:39:16 - train: epoch 0100, iter [03800, 05004], lr: 0.052818, loss: 2.4355
2022-03-03 12:39:48 - train: epoch 0100, iter [03900, 05004], lr: 0.052818, loss: 2.4841
2022-03-03 12:40:22 - train: epoch 0100, iter [04000, 05004], lr: 0.052818, loss: 2.2874
2022-03-03 12:40:54 - train: epoch 0100, iter [04100, 05004], lr: 0.052818, loss: 2.3948
2022-03-03 12:41:26 - train: epoch 0100, iter [04200, 05004], lr: 0.052818, loss: 2.5630
2022-03-03 12:41:58 - train: epoch 0100, iter [04300, 05004], lr: 0.052818, loss: 2.5522
2022-03-03 12:42:31 - train: epoch 0100, iter [04400, 05004], lr: 0.052818, loss: 2.6761
2022-03-03 12:43:03 - train: epoch 0100, iter [04500, 05004], lr: 0.052818, loss: 2.0721
2022-03-03 12:43:36 - train: epoch 0100, iter [04600, 05004], lr: 0.052818, loss: 2.4694
2022-03-03 12:44:08 - train: epoch 0100, iter [04700, 05004], lr: 0.052818, loss: 2.5458
2022-03-03 12:44:41 - train: epoch 0100, iter [04800, 05004], lr: 0.052818, loss: 2.2587
2022-03-03 12:45:14 - train: epoch 0100, iter [04900, 05004], lr: 0.052818, loss: 2.3808
2022-03-03 12:45:45 - train: epoch 0100, iter [05000, 05004], lr: 0.052818, loss: 2.5897
2022-03-03 12:45:46 - train: epoch 100, train_loss: 2.4413
2022-03-03 12:46:57 - eval: epoch: 100, acc1: 58.598%, acc5: 82.608%, test_loss: 1.7309, per_image_load_time: 0.560ms, per_image_inference_time: 0.360ms
2022-03-03 12:46:57 - until epoch: 100, best_acc1: 60.026%
2022-03-03 12:46:57 - epoch 101 lr: 0.05201329700547076
2022-03-03 12:47:35 - train: epoch 0101, iter [00100, 05004], lr: 0.052013, loss: 2.2546
2022-03-03 12:48:08 - train: epoch 0101, iter [00200, 05004], lr: 0.052013, loss: 2.5756
2022-03-03 12:48:40 - train: epoch 0101, iter [00300, 05004], lr: 0.052013, loss: 2.1148
2022-03-03 12:49:13 - train: epoch 0101, iter [00400, 05004], lr: 0.052013, loss: 2.4053
2022-03-03 12:49:45 - train: epoch 0101, iter [00500, 05004], lr: 0.052013, loss: 2.2620
2022-03-03 12:50:18 - train: epoch 0101, iter [00600, 05004], lr: 0.052013, loss: 2.3463
2022-03-03 12:50:50 - train: epoch 0101, iter [00700, 05004], lr: 0.052013, loss: 2.7864
2022-03-03 12:51:23 - train: epoch 0101, iter [00800, 05004], lr: 0.052013, loss: 2.2901
2022-03-03 12:51:55 - train: epoch 0101, iter [00900, 05004], lr: 0.052013, loss: 2.4991
2022-03-03 12:52:29 - train: epoch 0101, iter [01000, 05004], lr: 0.052013, loss: 2.6047
2022-03-03 12:53:01 - train: epoch 0101, iter [01100, 05004], lr: 0.052013, loss: 2.6451
2022-03-03 12:53:33 - train: epoch 0101, iter [01200, 05004], lr: 0.052013, loss: 2.3117
2022-03-03 12:54:05 - train: epoch 0101, iter [01300, 05004], lr: 0.052013, loss: 2.6935
2022-03-03 12:54:38 - train: epoch 0101, iter [01400, 05004], lr: 0.052013, loss: 2.6676
2022-03-03 12:55:11 - train: epoch 0101, iter [01500, 05004], lr: 0.052013, loss: 2.5698
2022-03-03 12:55:44 - train: epoch 0101, iter [01600, 05004], lr: 0.052013, loss: 2.3365
2022-03-03 12:56:16 - train: epoch 0101, iter [01700, 05004], lr: 0.052013, loss: 2.4969
2022-03-03 12:56:48 - train: epoch 0101, iter [01800, 05004], lr: 0.052013, loss: 2.5606
2022-03-03 12:57:20 - train: epoch 0101, iter [01900, 05004], lr: 0.052013, loss: 2.5175
2022-03-03 12:57:53 - train: epoch 0101, iter [02000, 05004], lr: 0.052013, loss: 2.2779
2022-03-03 12:58:26 - train: epoch 0101, iter [02100, 05004], lr: 0.052013, loss: 2.4113
2022-03-03 12:58:59 - train: epoch 0101, iter [02200, 05004], lr: 0.052013, loss: 2.4165
2022-03-03 12:59:31 - train: epoch 0101, iter [02300, 05004], lr: 0.052013, loss: 2.2829
2022-03-03 13:00:03 - train: epoch 0101, iter [02400, 05004], lr: 0.052013, loss: 2.5225
2022-03-03 13:00:35 - train: epoch 0101, iter [02500, 05004], lr: 0.052013, loss: 2.8443
2022-03-03 13:01:08 - train: epoch 0101, iter [02600, 05004], lr: 0.052013, loss: 2.5009
2022-03-03 13:01:40 - train: epoch 0101, iter [02700, 05004], lr: 0.052013, loss: 2.5467
2022-03-03 13:02:13 - train: epoch 0101, iter [02800, 05004], lr: 0.052013, loss: 2.4656
2022-03-03 13:02:45 - train: epoch 0101, iter [02900, 05004], lr: 0.052013, loss: 2.3157
2022-03-03 13:03:19 - train: epoch 0101, iter [03000, 05004], lr: 0.052013, loss: 2.3607
2022-03-03 13:03:51 - train: epoch 0101, iter [03100, 05004], lr: 0.052013, loss: 2.4839
2022-03-03 13:04:24 - train: epoch 0101, iter [03200, 05004], lr: 0.052013, loss: 2.5263
2022-03-03 13:04:56 - train: epoch 0101, iter [03300, 05004], lr: 0.052013, loss: 2.4666
2022-03-03 13:05:28 - train: epoch 0101, iter [03400, 05004], lr: 0.052013, loss: 2.3772
2022-03-03 13:06:00 - train: epoch 0101, iter [03500, 05004], lr: 0.052013, loss: 2.3232
2022-03-03 13:06:33 - train: epoch 0101, iter [03600, 05004], lr: 0.052013, loss: 2.5761
2022-03-03 13:07:05 - train: epoch 0101, iter [03700, 05004], lr: 0.052013, loss: 2.6444
2022-03-03 13:07:37 - train: epoch 0101, iter [03800, 05004], lr: 0.052013, loss: 2.4072
2022-03-03 13:08:11 - train: epoch 0101, iter [03900, 05004], lr: 0.052013, loss: 2.4803
2022-03-03 13:08:43 - train: epoch 0101, iter [04000, 05004], lr: 0.052013, loss: 2.3741
2022-03-03 13:09:16 - train: epoch 0101, iter [04100, 05004], lr: 0.052013, loss: 2.0534
2022-03-03 13:09:48 - train: epoch 0101, iter [04200, 05004], lr: 0.052013, loss: 2.4639
2022-03-03 13:10:21 - train: epoch 0101, iter [04300, 05004], lr: 0.052013, loss: 2.3676
2022-03-03 13:10:54 - train: epoch 0101, iter [04400, 05004], lr: 0.052013, loss: 2.3513
2022-03-03 13:11:26 - train: epoch 0101, iter [04500, 05004], lr: 0.052013, loss: 2.5205
2022-03-03 13:11:58 - train: epoch 0101, iter [04600, 05004], lr: 0.052013, loss: 2.5893
2022-03-03 13:12:31 - train: epoch 0101, iter [04700, 05004], lr: 0.052013, loss: 2.6697
2022-03-03 13:13:02 - train: epoch 0101, iter [04800, 05004], lr: 0.052013, loss: 2.3925
2022-03-03 13:13:35 - train: epoch 0101, iter [04900, 05004], lr: 0.052013, loss: 2.4842
2022-03-03 13:14:06 - train: epoch 0101, iter [05000, 05004], lr: 0.052013, loss: 2.5986
2022-03-03 13:14:07 - train: epoch 101, train_loss: 2.4377
2022-03-03 13:15:19 - eval: epoch: 101, acc1: 59.306%, acc5: 83.270%, test_loss: 1.6950, per_image_load_time: 0.534ms, per_image_inference_time: 0.388ms
2022-03-03 13:15:20 - until epoch: 101, best_acc1: 60.026%
2022-03-03 13:15:20 - epoch 102 lr: 0.05120818726180662
2022-03-03 13:15:58 - train: epoch 0102, iter [00100, 05004], lr: 0.051208, loss: 2.7839
2022-03-03 13:16:31 - train: epoch 0102, iter [00200, 05004], lr: 0.051208, loss: 2.4316
2022-03-03 13:17:03 - train: epoch 0102, iter [00300, 05004], lr: 0.051208, loss: 2.5086
2022-03-03 13:17:35 - train: epoch 0102, iter [00400, 05004], lr: 0.051208, loss: 2.2175
2022-03-03 13:18:07 - train: epoch 0102, iter [00500, 05004], lr: 0.051208, loss: 2.3761
2022-03-03 13:18:40 - train: epoch 0102, iter [00600, 05004], lr: 0.051208, loss: 2.2996
2022-03-03 13:19:12 - train: epoch 0102, iter [00700, 05004], lr: 0.051208, loss: 2.4044
2022-03-03 13:19:44 - train: epoch 0102, iter [00800, 05004], lr: 0.051208, loss: 2.1328
2022-03-03 13:20:17 - train: epoch 0102, iter [00900, 05004], lr: 0.051208, loss: 2.3104
2022-03-03 13:20:50 - train: epoch 0102, iter [01000, 05004], lr: 0.051208, loss: 2.5074
2022-03-03 13:21:23 - train: epoch 0102, iter [01100, 05004], lr: 0.051208, loss: 2.2318
2022-03-03 13:21:55 - train: epoch 0102, iter [01200, 05004], lr: 0.051208, loss: 2.7052
2022-03-03 13:22:27 - train: epoch 0102, iter [01300, 05004], lr: 0.051208, loss: 2.3200
2022-03-03 13:22:59 - train: epoch 0102, iter [01400, 05004], lr: 0.051208, loss: 2.2766
2022-03-03 13:23:32 - train: epoch 0102, iter [01500, 05004], lr: 0.051208, loss: 2.4165
2022-03-03 13:24:04 - train: epoch 0102, iter [01600, 05004], lr: 0.051208, loss: 2.5050
2022-03-03 13:24:37 - train: epoch 0102, iter [01700, 05004], lr: 0.051208, loss: 2.5659
2022-03-03 13:25:10 - train: epoch 0102, iter [01800, 05004], lr: 0.051208, loss: 2.2998
2022-03-03 13:25:43 - train: epoch 0102, iter [01900, 05004], lr: 0.051208, loss: 2.4805
2022-03-03 13:26:15 - train: epoch 0102, iter [02000, 05004], lr: 0.051208, loss: 2.5394
2022-03-03 13:26:47 - train: epoch 0102, iter [02100, 05004], lr: 0.051208, loss: 2.3409
2022-03-03 13:27:19 - train: epoch 0102, iter [02200, 05004], lr: 0.051208, loss: 2.1413
2022-03-03 13:27:52 - train: epoch 0102, iter [02300, 05004], lr: 0.051208, loss: 2.4425
2022-03-03 13:28:24 - train: epoch 0102, iter [02400, 05004], lr: 0.051208, loss: 2.6182
2022-03-03 13:28:57 - train: epoch 0102, iter [02500, 05004], lr: 0.051208, loss: 2.4285
2022-03-03 13:29:28 - train: epoch 0102, iter [02600, 05004], lr: 0.051208, loss: 2.6057
2022-03-03 13:30:02 - train: epoch 0102, iter [02700, 05004], lr: 0.051208, loss: 2.6330
2022-03-03 13:30:34 - train: epoch 0102, iter [02800, 05004], lr: 0.051208, loss: 2.4466
2022-03-03 13:31:06 - train: epoch 0102, iter [02900, 05004], lr: 0.051208, loss: 2.3040
2022-03-03 13:31:39 - train: epoch 0102, iter [03000, 05004], lr: 0.051208, loss: 2.4701
2022-03-03 13:32:11 - train: epoch 0102, iter [03100, 05004], lr: 0.051208, loss: 2.3949
2022-03-03 13:32:44 - train: epoch 0102, iter [03200, 05004], lr: 0.051208, loss: 2.2772
2022-03-03 13:33:16 - train: epoch 0102, iter [03300, 05004], lr: 0.051208, loss: 2.3202
2022-03-03 13:33:49 - train: epoch 0102, iter [03400, 05004], lr: 0.051208, loss: 2.6006
2022-03-03 13:34:21 - train: epoch 0102, iter [03500, 05004], lr: 0.051208, loss: 2.5743
2022-03-03 13:34:55 - train: epoch 0102, iter [03600, 05004], lr: 0.051208, loss: 2.5415
2022-03-03 13:35:25 - train: epoch 0102, iter [03700, 05004], lr: 0.051208, loss: 2.4950
2022-03-03 13:35:57 - train: epoch 0102, iter [03800, 05004], lr: 0.051208, loss: 2.5368
2022-03-03 13:36:31 - train: epoch 0102, iter [03900, 05004], lr: 0.051208, loss: 2.2475
2022-03-03 13:37:03 - train: epoch 0102, iter [04000, 05004], lr: 0.051208, loss: 2.2490
2022-03-03 13:37:35 - train: epoch 0102, iter [04100, 05004], lr: 0.051208, loss: 2.1315
2022-03-03 13:38:07 - train: epoch 0102, iter [04200, 05004], lr: 0.051208, loss: 2.7400
2022-03-03 13:38:40 - train: epoch 0102, iter [04300, 05004], lr: 0.051208, loss: 2.3839
2022-03-03 13:39:12 - train: epoch 0102, iter [04400, 05004], lr: 0.051208, loss: 2.5424
2022-03-03 13:39:45 - train: epoch 0102, iter [04500, 05004], lr: 0.051208, loss: 2.4406
2022-03-03 13:40:17 - train: epoch 0102, iter [04600, 05004], lr: 0.051208, loss: 2.5377
2022-03-03 13:40:50 - train: epoch 0102, iter [04700, 05004], lr: 0.051208, loss: 2.5335
2022-03-03 13:41:22 - train: epoch 0102, iter [04800, 05004], lr: 0.051208, loss: 2.5023
2022-03-03 13:41:55 - train: epoch 0102, iter [04900, 05004], lr: 0.051208, loss: 2.3211
2022-03-03 13:42:25 - train: epoch 0102, iter [05000, 05004], lr: 0.051208, loss: 2.2557
2022-03-03 13:42:27 - train: epoch 102, train_loss: 2.4279
2022-03-03 13:43:38 - eval: epoch: 102, acc1: 60.072%, acc5: 83.872%, test_loss: 1.6547, per_image_load_time: 0.471ms, per_image_inference_time: 0.389ms
2022-03-03 13:43:39 - until epoch: 102, best_acc1: 60.072%
2022-03-03 13:43:39 - epoch 103 lr: 0.0504027639330695
2022-03-03 13:44:17 - train: epoch 0103, iter [00100, 05004], lr: 0.050403, loss: 2.1537
2022-03-03 13:44:50 - train: epoch 0103, iter [00200, 05004], lr: 0.050403, loss: 2.3356
2022-03-03 13:45:21 - train: epoch 0103, iter [00300, 05004], lr: 0.050403, loss: 2.2531
2022-03-03 13:45:54 - train: epoch 0103, iter [00400, 05004], lr: 0.050403, loss: 2.4964
2022-03-03 13:46:26 - train: epoch 0103, iter [00500, 05004], lr: 0.050403, loss: 2.5887
2022-03-03 13:46:59 - train: epoch 0103, iter [00600, 05004], lr: 0.050403, loss: 2.6963
2022-03-03 13:47:31 - train: epoch 0103, iter [00700, 05004], lr: 0.050403, loss: 2.6070
2022-03-03 13:48:04 - train: epoch 0103, iter [00800, 05004], lr: 0.050403, loss: 2.4299
2022-03-03 13:48:37 - train: epoch 0103, iter [00900, 05004], lr: 0.050403, loss: 2.1874
2022-03-03 13:49:09 - train: epoch 0103, iter [01000, 05004], lr: 0.050403, loss: 2.2148
2022-03-03 13:49:41 - train: epoch 0103, iter [01100, 05004], lr: 0.050403, loss: 2.5045
2022-03-03 13:50:13 - train: epoch 0103, iter [01200, 05004], lr: 0.050403, loss: 2.7014
2022-03-03 13:50:46 - train: epoch 0103, iter [01300, 05004], lr: 0.050403, loss: 2.5791
2022-03-03 13:51:19 - train: epoch 0103, iter [01400, 05004], lr: 0.050403, loss: 2.5972
2022-03-03 13:51:51 - train: epoch 0103, iter [01500, 05004], lr: 0.050403, loss: 2.5571
2022-03-03 13:52:23 - train: epoch 0103, iter [01600, 05004], lr: 0.050403, loss: 2.0749
2022-03-03 13:52:55 - train: epoch 0103, iter [01700, 05004], lr: 0.050403, loss: 2.3097
2022-03-03 13:53:28 - train: epoch 0103, iter [01800, 05004], lr: 0.050403, loss: 2.2461
2022-03-03 13:54:01 - train: epoch 0103, iter [01900, 05004], lr: 0.050403, loss: 2.5752
2022-03-03 13:54:33 - train: epoch 0103, iter [02000, 05004], lr: 0.050403, loss: 2.0300
2022-03-03 13:55:05 - train: epoch 0103, iter [02100, 05004], lr: 0.050403, loss: 2.3466
2022-03-03 13:55:37 - train: epoch 0103, iter [02200, 05004], lr: 0.050403, loss: 2.3290
2022-03-03 13:56:09 - train: epoch 0103, iter [02300, 05004], lr: 0.050403, loss: 2.4601
2022-03-03 13:56:42 - train: epoch 0103, iter [02400, 05004], lr: 0.050403, loss: 2.4441
2022-03-03 13:57:15 - train: epoch 0103, iter [02500, 05004], lr: 0.050403, loss: 2.2852
2022-03-03 13:57:47 - train: epoch 0103, iter [02600, 05004], lr: 0.050403, loss: 2.5913
2022-03-03 13:58:20 - train: epoch 0103, iter [02700, 05004], lr: 0.050403, loss: 2.1685
2022-03-03 13:58:51 - train: epoch 0103, iter [02800, 05004], lr: 0.050403, loss: 2.1147
2022-03-03 13:59:24 - train: epoch 0103, iter [02900, 05004], lr: 0.050403, loss: 2.1980
2022-03-03 13:59:56 - train: epoch 0103, iter [03000, 05004], lr: 0.050403, loss: 2.5063
2022-03-03 14:00:29 - train: epoch 0103, iter [03100, 05004], lr: 0.050403, loss: 2.5861
2022-03-03 14:01:01 - train: epoch 0103, iter [03200, 05004], lr: 0.050403, loss: 2.2342
2022-03-03 14:01:34 - train: epoch 0103, iter [03300, 05004], lr: 0.050403, loss: 2.3927
2022-03-03 14:02:06 - train: epoch 0103, iter [03400, 05004], lr: 0.050403, loss: 2.5112
2022-03-03 14:02:39 - train: epoch 0103, iter [03500, 05004], lr: 0.050403, loss: 2.4604
2022-03-03 14:03:11 - train: epoch 0103, iter [03600, 05004], lr: 0.050403, loss: 2.4599
2022-03-03 14:03:44 - train: epoch 0103, iter [03700, 05004], lr: 0.050403, loss: 2.2480
2022-03-03 14:04:16 - train: epoch 0103, iter [03800, 05004], lr: 0.050403, loss: 2.2433
2022-03-03 14:04:48 - train: epoch 0103, iter [03900, 05004], lr: 0.050403, loss: 2.2300
2022-03-03 14:05:21 - train: epoch 0103, iter [04000, 05004], lr: 0.050403, loss: 2.5207
2022-03-03 14:05:54 - train: epoch 0103, iter [04100, 05004], lr: 0.050403, loss: 2.1844
2022-03-03 14:06:26 - train: epoch 0103, iter [04200, 05004], lr: 0.050403, loss: 2.3617
2022-03-03 14:06:58 - train: epoch 0103, iter [04300, 05004], lr: 0.050403, loss: 2.5169
2022-03-03 14:07:32 - train: epoch 0103, iter [04400, 05004], lr: 0.050403, loss: 2.5831
2022-03-03 14:08:03 - train: epoch 0103, iter [04500, 05004], lr: 0.050403, loss: 2.4609
2022-03-03 14:08:35 - train: epoch 0103, iter [04600, 05004], lr: 0.050403, loss: 2.4155
2022-03-03 14:09:07 - train: epoch 0103, iter [04700, 05004], lr: 0.050403, loss: 2.4859
2022-03-03 14:09:40 - train: epoch 0103, iter [04800, 05004], lr: 0.050403, loss: 2.4774
2022-03-03 14:10:12 - train: epoch 0103, iter [04900, 05004], lr: 0.050403, loss: 2.0390
2022-03-03 14:10:44 - train: epoch 0103, iter [05000, 05004], lr: 0.050403, loss: 2.5101
2022-03-03 14:10:45 - train: epoch 103, train_loss: 2.4229
2022-03-03 14:11:57 - eval: epoch: 103, acc1: 61.050%, acc5: 84.360%, test_loss: 1.6116, per_image_load_time: 1.665ms, per_image_inference_time: 0.442ms
2022-03-03 14:11:58 - until epoch: 103, best_acc1: 61.050%
2022-03-03 14:11:58 - epoch 104 lr: 0.04959723606693051
2022-03-03 14:12:36 - train: epoch 0104, iter [00100, 05004], lr: 0.049597, loss: 2.3475
2022-03-03 14:13:08 - train: epoch 0104, iter [00200, 05004], lr: 0.049597, loss: 2.3378
2022-03-03 14:13:40 - train: epoch 0104, iter [00300, 05004], lr: 0.049597, loss: 2.0924
2022-03-03 14:14:13 - train: epoch 0104, iter [00400, 05004], lr: 0.049597, loss: 2.5787
2022-03-03 14:14:44 - train: epoch 0104, iter [00500, 05004], lr: 0.049597, loss: 2.3831
2022-03-03 14:15:17 - train: epoch 0104, iter [00600, 05004], lr: 0.049597, loss: 2.6220
2022-03-03 14:15:50 - train: epoch 0104, iter [00700, 05004], lr: 0.049597, loss: 2.6460
2022-03-03 14:16:23 - train: epoch 0104, iter [00800, 05004], lr: 0.049597, loss: 2.4534
2022-03-03 14:16:56 - train: epoch 0104, iter [00900, 05004], lr: 0.049597, loss: 2.4318
2022-03-03 14:17:28 - train: epoch 0104, iter [01000, 05004], lr: 0.049597, loss: 2.1977
2022-03-03 14:18:01 - train: epoch 0104, iter [01100, 05004], lr: 0.049597, loss: 2.4051
2022-03-03 14:18:33 - train: epoch 0104, iter [01200, 05004], lr: 0.049597, loss: 2.1286
2022-03-03 14:19:05 - train: epoch 0104, iter [01300, 05004], lr: 0.049597, loss: 2.3623
2022-03-03 14:19:38 - train: epoch 0104, iter [01400, 05004], lr: 0.049597, loss: 2.4805
2022-03-03 14:20:11 - train: epoch 0104, iter [01500, 05004], lr: 0.049597, loss: 2.6004
2022-03-03 14:20:44 - train: epoch 0104, iter [01600, 05004], lr: 0.049597, loss: 2.2092
2022-03-03 14:21:16 - train: epoch 0104, iter [01700, 05004], lr: 0.049597, loss: 2.2045
2022-03-03 14:21:50 - train: epoch 0104, iter [01800, 05004], lr: 0.049597, loss: 2.5342
2022-03-03 14:22:22 - train: epoch 0104, iter [01900, 05004], lr: 0.049597, loss: 2.4988
2022-03-03 14:22:54 - train: epoch 0104, iter [02000, 05004], lr: 0.049597, loss: 2.4906
2022-03-03 14:23:26 - train: epoch 0104, iter [02100, 05004], lr: 0.049597, loss: 2.2627
2022-03-03 14:23:57 - train: epoch 0104, iter [02200, 05004], lr: 0.049597, loss: 2.3449
2022-03-03 14:24:30 - train: epoch 0104, iter [02300, 05004], lr: 0.049597, loss: 2.6111
2022-03-03 14:25:02 - train: epoch 0104, iter [02400, 05004], lr: 0.049597, loss: 2.2683
2022-03-03 14:25:35 - train: epoch 0104, iter [02500, 05004], lr: 0.049597, loss: 2.6974
2022-03-03 14:26:06 - train: epoch 0104, iter [02600, 05004], lr: 0.049597, loss: 2.2391
2022-03-03 14:26:39 - train: epoch 0104, iter [02700, 05004], lr: 0.049597, loss: 2.2828
2022-03-03 14:27:11 - train: epoch 0104, iter [02800, 05004], lr: 0.049597, loss: 2.5039
2022-03-03 14:27:43 - train: epoch 0104, iter [02900, 05004], lr: 0.049597, loss: 2.3101
2022-03-03 14:28:17 - train: epoch 0104, iter [03000, 05004], lr: 0.049597, loss: 2.4414
2022-03-03 14:28:49 - train: epoch 0104, iter [03100, 05004], lr: 0.049597, loss: 2.5617
2022-03-03 14:29:22 - train: epoch 0104, iter [03200, 05004], lr: 0.049597, loss: 2.5185
2022-03-03 14:29:54 - train: epoch 0104, iter [03300, 05004], lr: 0.049597, loss: 2.2770
2022-03-03 14:30:28 - train: epoch 0104, iter [03400, 05004], lr: 0.049597, loss: 2.4045
2022-03-03 14:31:01 - train: epoch 0104, iter [03500, 05004], lr: 0.049597, loss: 2.4580
2022-03-03 14:31:34 - train: epoch 0104, iter [03600, 05004], lr: 0.049597, loss: 2.3856
2022-03-03 14:32:08 - train: epoch 0104, iter [03700, 05004], lr: 0.049597, loss: 2.3182
2022-03-03 14:32:42 - train: epoch 0104, iter [03800, 05004], lr: 0.049597, loss: 2.1853
2022-03-03 14:33:16 - train: epoch 0104, iter [03900, 05004], lr: 0.049597, loss: 2.1862
2022-03-03 14:33:50 - train: epoch 0104, iter [04000, 05004], lr: 0.049597, loss: 2.3614
2022-03-03 14:34:25 - train: epoch 0104, iter [04100, 05004], lr: 0.049597, loss: 2.2869
2022-03-03 14:34:58 - train: epoch 0104, iter [04200, 05004], lr: 0.049597, loss: 2.3751
2022-03-03 14:35:32 - train: epoch 0104, iter [04300, 05004], lr: 0.049597, loss: 2.4661
2022-03-03 14:36:05 - train: epoch 0104, iter [04400, 05004], lr: 0.049597, loss: 2.4200
2022-03-03 14:36:39 - train: epoch 0104, iter [04500, 05004], lr: 0.049597, loss: 2.6204
2022-03-03 14:37:14 - train: epoch 0104, iter [04600, 05004], lr: 0.049597, loss: 2.2537
2022-03-03 14:37:47 - train: epoch 0104, iter [04700, 05004], lr: 0.049597, loss: 2.3764
2022-03-03 14:38:21 - train: epoch 0104, iter [04800, 05004], lr: 0.049597, loss: 2.4289
2022-03-03 14:38:56 - train: epoch 0104, iter [04900, 05004], lr: 0.049597, loss: 2.4540
2022-03-03 14:39:28 - train: epoch 0104, iter [05000, 05004], lr: 0.049597, loss: 2.6037
2022-03-03 14:39:29 - train: epoch 104, train_loss: 2.4162
2022-03-03 14:40:43 - eval: epoch: 104, acc1: 60.676%, acc5: 84.308%, test_loss: 1.6301, per_image_load_time: 0.831ms, per_image_inference_time: 0.454ms
2022-03-03 14:40:43 - until epoch: 104, best_acc1: 61.050%
2022-03-03 14:40:43 - epoch 105 lr: 0.0487918127381934
2022-03-03 14:41:22 - train: epoch 0105, iter [00100, 05004], lr: 0.048792, loss: 2.7241
2022-03-03 14:41:55 - train: epoch 0105, iter [00200, 05004], lr: 0.048792, loss: 2.6911
2022-03-03 14:42:29 - train: epoch 0105, iter [00300, 05004], lr: 0.048792, loss: 2.5821
2022-03-03 14:43:02 - train: epoch 0105, iter [00400, 05004], lr: 0.048792, loss: 2.6511
2022-03-03 14:43:35 - train: epoch 0105, iter [00500, 05004], lr: 0.048792, loss: 2.5481
2022-03-03 14:44:08 - train: epoch 0105, iter [00600, 05004], lr: 0.048792, loss: 2.4536
2022-03-03 14:44:42 - train: epoch 0105, iter [00700, 05004], lr: 0.048792, loss: 2.4527
2022-03-03 14:45:15 - train: epoch 0105, iter [00800, 05004], lr: 0.048792, loss: 2.4948
2022-03-03 14:45:47 - train: epoch 0105, iter [00900, 05004], lr: 0.048792, loss: 2.3693
2022-03-03 14:46:20 - train: epoch 0105, iter [01000, 05004], lr: 0.048792, loss: 2.4016
2022-03-03 14:46:53 - train: epoch 0105, iter [01100, 05004], lr: 0.048792, loss: 2.4767
2022-03-03 14:47:27 - train: epoch 0105, iter [01200, 05004], lr: 0.048792, loss: 2.3853
2022-03-03 14:48:00 - train: epoch 0105, iter [01300, 05004], lr: 0.048792, loss: 2.0549
2022-03-03 14:48:33 - train: epoch 0105, iter [01400, 05004], lr: 0.048792, loss: 2.5393
2022-03-03 14:49:06 - train: epoch 0105, iter [01500, 05004], lr: 0.048792, loss: 2.3114
2022-03-03 14:49:39 - train: epoch 0105, iter [01600, 05004], lr: 0.048792, loss: 2.2339
2022-03-03 14:50:12 - train: epoch 0105, iter [01700, 05004], lr: 0.048792, loss: 2.3565
2022-03-03 14:50:45 - train: epoch 0105, iter [01800, 05004], lr: 0.048792, loss: 2.4247
2022-03-03 14:51:18 - train: epoch 0105, iter [01900, 05004], lr: 0.048792, loss: 2.4132
2022-03-03 14:51:51 - train: epoch 0105, iter [02000, 05004], lr: 0.048792, loss: 2.4593
2022-03-03 14:52:25 - train: epoch 0105, iter [02100, 05004], lr: 0.048792, loss: 2.4375
2022-03-03 14:52:57 - train: epoch 0105, iter [02200, 05004], lr: 0.048792, loss: 2.6542
2022-03-03 14:53:30 - train: epoch 0105, iter [02300, 05004], lr: 0.048792, loss: 2.3393
2022-03-03 14:54:03 - train: epoch 0105, iter [02400, 05004], lr: 0.048792, loss: 2.4542
2022-03-03 14:54:36 - train: epoch 0105, iter [02500, 05004], lr: 0.048792, loss: 2.2523
2022-03-03 14:55:09 - train: epoch 0105, iter [02600, 05004], lr: 0.048792, loss: 2.3089
2022-03-03 14:55:43 - train: epoch 0105, iter [02700, 05004], lr: 0.048792, loss: 2.7305
2022-03-03 14:56:15 - train: epoch 0105, iter [02800, 05004], lr: 0.048792, loss: 2.4901
2022-03-03 14:56:48 - train: epoch 0105, iter [02900, 05004], lr: 0.048792, loss: 2.3405
2022-03-03 14:57:21 - train: epoch 0105, iter [03000, 05004], lr: 0.048792, loss: 2.6121
2022-03-03 14:57:55 - train: epoch 0105, iter [03100, 05004], lr: 0.048792, loss: 2.6161
2022-03-03 14:58:28 - train: epoch 0105, iter [03200, 05004], lr: 0.048792, loss: 2.4913
2022-03-03 14:59:02 - train: epoch 0105, iter [03300, 05004], lr: 0.048792, loss: 2.2077
2022-03-03 14:59:35 - train: epoch 0105, iter [03400, 05004], lr: 0.048792, loss: 2.4680
2022-03-03 15:00:08 - train: epoch 0105, iter [03500, 05004], lr: 0.048792, loss: 2.4061
2022-03-03 15:00:40 - train: epoch 0105, iter [03600, 05004], lr: 0.048792, loss: 2.6064
2022-03-03 15:01:13 - train: epoch 0105, iter [03700, 05004], lr: 0.048792, loss: 2.5274
2022-03-03 15:01:46 - train: epoch 0105, iter [03800, 05004], lr: 0.048792, loss: 2.4613
2022-03-03 15:02:20 - train: epoch 0105, iter [03900, 05004], lr: 0.048792, loss: 2.4741
2022-03-03 15:02:54 - train: epoch 0105, iter [04000, 05004], lr: 0.048792, loss: 2.6137
2022-03-03 15:03:27 - train: epoch 0105, iter [04100, 05004], lr: 0.048792, loss: 2.2236
2022-03-03 15:03:59 - train: epoch 0105, iter [04200, 05004], lr: 0.048792, loss: 2.4421
2022-03-03 15:04:32 - train: epoch 0105, iter [04300, 05004], lr: 0.048792, loss: 2.4756
2022-03-03 15:05:05 - train: epoch 0105, iter [04400, 05004], lr: 0.048792, loss: 2.2900
2022-03-03 15:05:39 - train: epoch 0105, iter [04500, 05004], lr: 0.048792, loss: 2.6900
2022-03-03 15:06:12 - train: epoch 0105, iter [04600, 05004], lr: 0.048792, loss: 2.6549
2022-03-03 15:06:45 - train: epoch 0105, iter [04700, 05004], lr: 0.048792, loss: 2.2941
2022-03-03 15:07:18 - train: epoch 0105, iter [04800, 05004], lr: 0.048792, loss: 2.3050
2022-03-03 15:07:52 - train: epoch 0105, iter [04900, 05004], lr: 0.048792, loss: 2.2786
2022-03-03 15:08:23 - train: epoch 0105, iter [05000, 05004], lr: 0.048792, loss: 2.2112
2022-03-03 15:08:24 - train: epoch 105, train_loss: 2.4109
2022-03-03 15:09:36 - eval: epoch: 105, acc1: 60.322%, acc5: 83.896%, test_loss: 1.6442, per_image_load_time: 0.502ms, per_image_inference_time: 0.470ms
2022-03-03 15:09:37 - until epoch: 105, best_acc1: 61.050%
2022-03-03 15:09:37 - epoch 106 lr: 0.04798670299452926
2022-03-03 15:10:16 - train: epoch 0106, iter [00100, 05004], lr: 0.047987, loss: 2.2601
2022-03-03 15:10:48 - train: epoch 0106, iter [00200, 05004], lr: 0.047987, loss: 2.1334
2022-03-03 15:11:22 - train: epoch 0106, iter [00300, 05004], lr: 0.047987, loss: 2.1324
2022-03-03 15:11:54 - train: epoch 0106, iter [00400, 05004], lr: 0.047987, loss: 2.3382
2022-03-03 15:12:28 - train: epoch 0106, iter [00500, 05004], lr: 0.047987, loss: 2.3719
2022-03-03 15:13:01 - train: epoch 0106, iter [00600, 05004], lr: 0.047987, loss: 2.3520
2022-03-03 15:13:35 - train: epoch 0106, iter [00700, 05004], lr: 0.047987, loss: 2.3280
2022-03-03 15:14:07 - train: epoch 0106, iter [00800, 05004], lr: 0.047987, loss: 2.2824
2022-03-03 15:14:40 - train: epoch 0106, iter [00900, 05004], lr: 0.047987, loss: 2.3489
2022-03-03 15:15:13 - train: epoch 0106, iter [01000, 05004], lr: 0.047987, loss: 2.4749
2022-03-03 15:15:46 - train: epoch 0106, iter [01100, 05004], lr: 0.047987, loss: 2.1317
2022-03-03 15:16:19 - train: epoch 0106, iter [01200, 05004], lr: 0.047987, loss: 2.6826
2022-03-03 15:16:52 - train: epoch 0106, iter [01300, 05004], lr: 0.047987, loss: 2.2266
2022-03-03 15:17:26 - train: epoch 0106, iter [01400, 05004], lr: 0.047987, loss: 2.2733
2022-03-03 15:17:58 - train: epoch 0106, iter [01500, 05004], lr: 0.047987, loss: 2.3038
2022-03-03 15:18:31 - train: epoch 0106, iter [01600, 05004], lr: 0.047987, loss: 2.4128
2022-03-03 15:19:04 - train: epoch 0106, iter [01700, 05004], lr: 0.047987, loss: 2.2960
2022-03-03 15:19:37 - train: epoch 0106, iter [01800, 05004], lr: 0.047987, loss: 2.3151
2022-03-03 15:20:10 - train: epoch 0106, iter [01900, 05004], lr: 0.047987, loss: 2.1522
2022-03-03 15:20:43 - train: epoch 0106, iter [02000, 05004], lr: 0.047987, loss: 2.2080
2022-03-03 15:21:17 - train: epoch 0106, iter [02100, 05004], lr: 0.047987, loss: 2.3776
2022-03-03 15:21:49 - train: epoch 0106, iter [02200, 05004], lr: 0.047987, loss: 2.4490
2022-03-03 15:22:23 - train: epoch 0106, iter [02300, 05004], lr: 0.047987, loss: 2.4282
2022-03-03 15:22:55 - train: epoch 0106, iter [02400, 05004], lr: 0.047987, loss: 2.3691
2022-03-03 15:23:28 - train: epoch 0106, iter [02500, 05004], lr: 0.047987, loss: 2.4687
2022-03-03 15:24:01 - train: epoch 0106, iter [02600, 05004], lr: 0.047987, loss: 2.3539
2022-03-03 15:24:34 - train: epoch 0106, iter [02700, 05004], lr: 0.047987, loss: 2.7165
2022-03-03 15:25:07 - train: epoch 0106, iter [02800, 05004], lr: 0.047987, loss: 2.2431
2022-03-03 15:25:39 - train: epoch 0106, iter [02900, 05004], lr: 0.047987, loss: 2.3872
2022-03-03 15:26:13 - train: epoch 0106, iter [03000, 05004], lr: 0.047987, loss: 2.6412
2022-03-03 15:26:45 - train: epoch 0106, iter [03100, 05004], lr: 0.047987, loss: 2.4211
2022-03-03 15:27:19 - train: epoch 0106, iter [03200, 05004], lr: 0.047987, loss: 2.2671
2022-03-03 15:27:51 - train: epoch 0106, iter [03300, 05004], lr: 0.047987, loss: 2.3312
2022-03-03 15:28:25 - train: epoch 0106, iter [03400, 05004], lr: 0.047987, loss: 2.3219
2022-03-03 15:28:57 - train: epoch 0106, iter [03500, 05004], lr: 0.047987, loss: 2.1477
2022-03-03 15:29:31 - train: epoch 0106, iter [03600, 05004], lr: 0.047987, loss: 2.4236
2022-03-03 15:30:03 - train: epoch 0106, iter [03700, 05004], lr: 0.047987, loss: 2.4295
2022-03-03 15:30:36 - train: epoch 0106, iter [03800, 05004], lr: 0.047987, loss: 2.6377
2022-03-03 15:31:10 - train: epoch 0106, iter [03900, 05004], lr: 0.047987, loss: 2.6118
2022-03-03 15:31:43 - train: epoch 0106, iter [04000, 05004], lr: 0.047987, loss: 2.3315
2022-03-03 15:32:15 - train: epoch 0106, iter [04100, 05004], lr: 0.047987, loss: 2.6834
2022-03-03 15:32:48 - train: epoch 0106, iter [04200, 05004], lr: 0.047987, loss: 2.2496
2022-03-03 15:33:22 - train: epoch 0106, iter [04300, 05004], lr: 0.047987, loss: 2.2807
2022-03-03 15:33:54 - train: epoch 0106, iter [04400, 05004], lr: 0.047987, loss: 2.3954
2022-03-03 15:34:28 - train: epoch 0106, iter [04500, 05004], lr: 0.047987, loss: 2.5459
2022-03-03 15:35:01 - train: epoch 0106, iter [04600, 05004], lr: 0.047987, loss: 2.2590
2022-03-03 15:35:35 - train: epoch 0106, iter [04700, 05004], lr: 0.047987, loss: 2.2192
2022-03-03 15:36:07 - train: epoch 0106, iter [04800, 05004], lr: 0.047987, loss: 2.4609
2022-03-03 15:36:41 - train: epoch 0106, iter [04900, 05004], lr: 0.047987, loss: 2.5188
2022-03-03 15:37:12 - train: epoch 0106, iter [05000, 05004], lr: 0.047987, loss: 2.6181
2022-03-03 15:37:13 - train: epoch 106, train_loss: 2.4010
2022-03-03 15:38:28 - eval: epoch: 106, acc1: 59.456%, acc5: 83.286%, test_loss: 1.6873, per_image_load_time: 0.842ms, per_image_inference_time: 0.481ms
2022-03-03 15:38:28 - until epoch: 106, best_acc1: 61.050%
2022-03-03 15:38:28 - epoch 107 lr: 0.04718211580221812
2022-03-03 15:39:08 - train: epoch 0107, iter [00100, 05004], lr: 0.047182, loss: 2.1825
2022-03-03 15:39:42 - train: epoch 0107, iter [00200, 05004], lr: 0.047182, loss: 2.4228
2022-03-03 15:40:16 - train: epoch 0107, iter [00300, 05004], lr: 0.047182, loss: 2.3155
2022-03-03 15:40:50 - train: epoch 0107, iter [00400, 05004], lr: 0.047182, loss: 2.2138
2022-03-03 15:41:24 - train: epoch 0107, iter [00500, 05004], lr: 0.047182, loss: 2.6379
2022-03-03 15:41:58 - train: epoch 0107, iter [00600, 05004], lr: 0.047182, loss: 2.2992
2022-03-03 15:42:32 - train: epoch 0107, iter [00700, 05004], lr: 0.047182, loss: 2.5001
2022-03-03 15:43:05 - train: epoch 0107, iter [00800, 05004], lr: 0.047182, loss: 2.1263
2022-03-03 15:43:39 - train: epoch 0107, iter [00900, 05004], lr: 0.047182, loss: 2.5780
2022-03-03 15:44:13 - train: epoch 0107, iter [01000, 05004], lr: 0.047182, loss: 2.3448
2022-03-03 15:44:47 - train: epoch 0107, iter [01100, 05004], lr: 0.047182, loss: 2.6343
2022-03-03 15:45:20 - train: epoch 0107, iter [01200, 05004], lr: 0.047182, loss: 2.2872
2022-03-03 15:45:53 - train: epoch 0107, iter [01300, 05004], lr: 0.047182, loss: 2.3424
2022-03-03 15:46:26 - train: epoch 0107, iter [01400, 05004], lr: 0.047182, loss: 2.2804
2022-03-03 15:46:59 - train: epoch 0107, iter [01500, 05004], lr: 0.047182, loss: 2.5609
2022-03-03 15:47:32 - train: epoch 0107, iter [01600, 05004], lr: 0.047182, loss: 2.3370
2022-03-03 15:48:06 - train: epoch 0107, iter [01700, 05004], lr: 0.047182, loss: 2.3364
2022-03-03 15:48:39 - train: epoch 0107, iter [01800, 05004], lr: 0.047182, loss: 2.5383
2022-03-03 15:49:12 - train: epoch 0107, iter [01900, 05004], lr: 0.047182, loss: 2.5256
2022-03-03 15:49:45 - train: epoch 0107, iter [02000, 05004], lr: 0.047182, loss: 2.1726
2022-03-03 15:50:18 - train: epoch 0107, iter [02100, 05004], lr: 0.047182, loss: 2.2440
2022-03-03 15:50:52 - train: epoch 0107, iter [02200, 05004], lr: 0.047182, loss: 2.3343
2022-03-03 15:51:25 - train: epoch 0107, iter [02300, 05004], lr: 0.047182, loss: 2.6061
2022-03-03 15:51:57 - train: epoch 0107, iter [02400, 05004], lr: 0.047182, loss: 2.4299
2022-03-03 15:52:30 - train: epoch 0107, iter [02500, 05004], lr: 0.047182, loss: 2.1136
2022-03-03 15:53:04 - train: epoch 0107, iter [02600, 05004], lr: 0.047182, loss: 2.4307
2022-03-03 15:53:37 - train: epoch 0107, iter [02700, 05004], lr: 0.047182, loss: 2.2849
2022-03-03 15:54:10 - train: epoch 0107, iter [02800, 05004], lr: 0.047182, loss: 2.1613
2022-03-03 15:54:43 - train: epoch 0107, iter [02900, 05004], lr: 0.047182, loss: 2.3080
2022-03-03 15:55:15 - train: epoch 0107, iter [03000, 05004], lr: 0.047182, loss: 2.3784
2022-03-03 15:55:48 - train: epoch 0107, iter [03100, 05004], lr: 0.047182, loss: 2.5565
2022-03-03 15:56:20 - train: epoch 0107, iter [03200, 05004], lr: 0.047182, loss: 2.2972
2022-03-03 15:56:53 - train: epoch 0107, iter [03300, 05004], lr: 0.047182, loss: 2.5023
2022-03-03 15:57:25 - train: epoch 0107, iter [03400, 05004], lr: 0.047182, loss: 2.5525
2022-03-03 15:57:58 - train: epoch 0107, iter [03500, 05004], lr: 0.047182, loss: 2.2860
2022-03-03 15:58:30 - train: epoch 0107, iter [03600, 05004], lr: 0.047182, loss: 2.2812
2022-03-03 15:59:03 - train: epoch 0107, iter [03700, 05004], lr: 0.047182, loss: 2.0542
2022-03-03 15:59:36 - train: epoch 0107, iter [03800, 05004], lr: 0.047182, loss: 2.3147
2022-03-03 16:00:09 - train: epoch 0107, iter [03900, 05004], lr: 0.047182, loss: 2.4034
2022-03-03 16:00:41 - train: epoch 0107, iter [04000, 05004], lr: 0.047182, loss: 2.4586
2022-03-03 16:01:13 - train: epoch 0107, iter [04100, 05004], lr: 0.047182, loss: 2.3538
2022-03-03 16:01:47 - train: epoch 0107, iter [04200, 05004], lr: 0.047182, loss: 2.6019
2022-03-03 16:02:19 - train: epoch 0107, iter [04300, 05004], lr: 0.047182, loss: 2.6847
2022-03-03 16:02:52 - train: epoch 0107, iter [04400, 05004], lr: 0.047182, loss: 2.4521
2022-03-03 16:03:25 - train: epoch 0107, iter [04500, 05004], lr: 0.047182, loss: 2.3540
2022-03-03 16:03:58 - train: epoch 0107, iter [04600, 05004], lr: 0.047182, loss: 2.5245
2022-03-03 16:04:30 - train: epoch 0107, iter [04700, 05004], lr: 0.047182, loss: 2.2197
2022-03-03 16:05:02 - train: epoch 0107, iter [04800, 05004], lr: 0.047182, loss: 2.3748
2022-03-03 16:05:35 - train: epoch 0107, iter [04900, 05004], lr: 0.047182, loss: 2.5266
2022-03-03 16:06:06 - train: epoch 0107, iter [05000, 05004], lr: 0.047182, loss: 2.2020
2022-03-03 16:06:07 - train: epoch 107, train_loss: 2.3945
2022-03-03 16:07:18 - eval: epoch: 107, acc1: 59.890%, acc5: 83.524%, test_loss: 1.6691, per_image_load_time: 0.500ms, per_image_inference_time: 0.386ms
2022-03-03 16:07:19 - until epoch: 107, best_acc1: 61.050%
2022-03-03 16:07:19 - epoch 108 lr: 0.04637825999191189
2022-03-03 16:07:56 - train: epoch 0108, iter [00100, 05004], lr: 0.046378, loss: 2.3118
2022-03-03 16:08:28 - train: epoch 0108, iter [00200, 05004], lr: 0.046378, loss: 2.2238
2022-03-03 16:09:00 - train: epoch 0108, iter [00300, 05004], lr: 0.046378, loss: 2.6585
2022-03-03 16:09:33 - train: epoch 0108, iter [00400, 05004], lr: 0.046378, loss: 2.2947
2022-03-03 16:10:04 - train: epoch 0108, iter [00500, 05004], lr: 0.046378, loss: 2.4065
2022-03-03 16:10:37 - train: epoch 0108, iter [00600, 05004], lr: 0.046378, loss: 2.3839
2022-03-03 16:11:09 - train: epoch 0108, iter [00700, 05004], lr: 0.046378, loss: 2.2018
2022-03-03 16:11:43 - train: epoch 0108, iter [00800, 05004], lr: 0.046378, loss: 2.5289
2022-03-03 16:12:15 - train: epoch 0108, iter [00900, 05004], lr: 0.046378, loss: 2.4036
2022-03-03 16:12:47 - train: epoch 0108, iter [01000, 05004], lr: 0.046378, loss: 2.1636
2022-03-03 16:13:18 - train: epoch 0108, iter [01100, 05004], lr: 0.046378, loss: 2.2676
2022-03-03 16:13:52 - train: epoch 0108, iter [01200, 05004], lr: 0.046378, loss: 2.3522
2022-03-03 16:14:24 - train: epoch 0108, iter [01300, 05004], lr: 0.046378, loss: 2.2837
2022-03-03 16:14:56 - train: epoch 0108, iter [01400, 05004], lr: 0.046378, loss: 2.2982
2022-03-03 16:15:28 - train: epoch 0108, iter [01500, 05004], lr: 0.046378, loss: 2.4376
2022-03-03 16:16:01 - train: epoch 0108, iter [01600, 05004], lr: 0.046378, loss: 2.4008
2022-03-03 16:16:34 - train: epoch 0108, iter [01700, 05004], lr: 0.046378, loss: 2.1631
2022-03-03 16:17:06 - train: epoch 0108, iter [01800, 05004], lr: 0.046378, loss: 2.3673
2022-03-03 16:17:38 - train: epoch 0108, iter [01900, 05004], lr: 0.046378, loss: 2.5919
2022-03-03 16:18:10 - train: epoch 0108, iter [02000, 05004], lr: 0.046378, loss: 2.4111
2022-03-03 16:18:42 - train: epoch 0108, iter [02100, 05004], lr: 0.046378, loss: 2.3633
2022-03-03 16:19:15 - train: epoch 0108, iter [02200, 05004], lr: 0.046378, loss: 1.9972
2022-03-03 16:19:47 - train: epoch 0108, iter [02300, 05004], lr: 0.046378, loss: 2.4094
2022-03-03 16:20:20 - train: epoch 0108, iter [02400, 05004], lr: 0.046378, loss: 2.4373
2022-03-03 16:20:51 - train: epoch 0108, iter [02500, 05004], lr: 0.046378, loss: 2.5254
2022-03-03 16:21:23 - train: epoch 0108, iter [02600, 05004], lr: 0.046378, loss: 2.6708
2022-03-03 16:21:55 - train: epoch 0108, iter [02700, 05004], lr: 0.046378, loss: 2.4078
2022-03-03 16:22:28 - train: epoch 0108, iter [02800, 05004], lr: 0.046378, loss: 2.4800
2022-03-03 16:23:01 - train: epoch 0108, iter [02900, 05004], lr: 0.046378, loss: 2.3356
2022-03-03 16:23:33 - train: epoch 0108, iter [03000, 05004], lr: 0.046378, loss: 2.4112
2022-03-03 16:24:05 - train: epoch 0108, iter [03100, 05004], lr: 0.046378, loss: 2.4440
2022-03-03 16:24:37 - train: epoch 0108, iter [03200, 05004], lr: 0.046378, loss: 2.4880
2022-03-03 16:25:10 - train: epoch 0108, iter [03300, 05004], lr: 0.046378, loss: 2.4927
2022-03-03 16:25:42 - train: epoch 0108, iter [03400, 05004], lr: 0.046378, loss: 2.5698
2022-03-03 16:26:15 - train: epoch 0108, iter [03500, 05004], lr: 0.046378, loss: 2.4553
2022-03-03 16:26:47 - train: epoch 0108, iter [03600, 05004], lr: 0.046378, loss: 2.3854
2022-03-03 16:27:20 - train: epoch 0108, iter [03700, 05004], lr: 0.046378, loss: 2.3611
2022-03-03 16:27:52 - train: epoch 0108, iter [03800, 05004], lr: 0.046378, loss: 2.2183
2022-03-03 16:28:24 - train: epoch 0108, iter [03900, 05004], lr: 0.046378, loss: 2.3909
2022-03-03 16:28:56 - train: epoch 0108, iter [04000, 05004], lr: 0.046378, loss: 2.4845
2022-03-03 16:29:29 - train: epoch 0108, iter [04100, 05004], lr: 0.046378, loss: 2.4120
2022-03-03 16:30:00 - train: epoch 0108, iter [04200, 05004], lr: 0.046378, loss: 2.2630
2022-03-03 16:30:33 - train: epoch 0108, iter [04300, 05004], lr: 0.046378, loss: 2.2319
2022-03-03 16:31:06 - train: epoch 0108, iter [04400, 05004], lr: 0.046378, loss: 2.2265
2022-03-03 16:31:38 - train: epoch 0108, iter [04500, 05004], lr: 0.046378, loss: 2.4400
2022-03-03 16:32:11 - train: epoch 0108, iter [04600, 05004], lr: 0.046378, loss: 2.5895
2022-03-03 16:32:43 - train: epoch 0108, iter [04700, 05004], lr: 0.046378, loss: 2.4092
2022-03-03 16:33:15 - train: epoch 0108, iter [04800, 05004], lr: 0.046378, loss: 2.3398
2022-03-03 16:33:47 - train: epoch 0108, iter [04900, 05004], lr: 0.046378, loss: 2.3294
2022-03-03 16:34:19 - train: epoch 0108, iter [05000, 05004], lr: 0.046378, loss: 2.2343
2022-03-03 16:34:20 - train: epoch 108, train_loss: 2.3859
2022-03-03 16:35:31 - eval: epoch: 108, acc1: 60.252%, acc5: 83.770%, test_loss: 1.6608, per_image_load_time: 0.612ms, per_image_inference_time: 0.428ms
2022-03-03 16:35:32 - until epoch: 108, best_acc1: 61.050%
2022-03-03 16:35:32 - epoch 109 lr: 0.045575344204432086
2022-03-03 16:36:10 - train: epoch 0109, iter [00100, 05004], lr: 0.045575, loss: 2.2826
2022-03-03 16:36:43 - train: epoch 0109, iter [00200, 05004], lr: 0.045575, loss: 2.3330
2022-03-03 16:37:14 - train: epoch 0109, iter [00300, 05004], lr: 0.045575, loss: 2.3819
2022-03-03 16:37:46 - train: epoch 0109, iter [00400, 05004], lr: 0.045575, loss: 2.5202
2022-03-03 16:38:19 - train: epoch 0109, iter [00500, 05004], lr: 0.045575, loss: 2.3198
2022-03-03 16:38:51 - train: epoch 0109, iter [00600, 05004], lr: 0.045575, loss: 2.3772
2022-03-03 16:39:24 - train: epoch 0109, iter [00700, 05004], lr: 0.045575, loss: 2.4632
2022-03-03 16:39:56 - train: epoch 0109, iter [00800, 05004], lr: 0.045575, loss: 2.2152
2022-03-03 16:40:29 - train: epoch 0109, iter [00900, 05004], lr: 0.045575, loss: 2.3065
2022-03-03 16:41:01 - train: epoch 0109, iter [01000, 05004], lr: 0.045575, loss: 2.4554
2022-03-03 16:41:33 - train: epoch 0109, iter [01100, 05004], lr: 0.045575, loss: 2.0489
2022-03-03 16:42:05 - train: epoch 0109, iter [01200, 05004], lr: 0.045575, loss: 2.4383
2022-03-03 16:42:38 - train: epoch 0109, iter [01300, 05004], lr: 0.045575, loss: 2.1973
2022-03-03 16:43:11 - train: epoch 0109, iter [01400, 05004], lr: 0.045575, loss: 2.4375
2022-03-03 16:43:43 - train: epoch 0109, iter [01500, 05004], lr: 0.045575, loss: 2.4439
2022-03-03 16:44:16 - train: epoch 0109, iter [01600, 05004], lr: 0.045575, loss: 2.0376
2022-03-03 16:44:48 - train: epoch 0109, iter [01700, 05004], lr: 0.045575, loss: 2.4284
2022-03-03 16:45:20 - train: epoch 0109, iter [01800, 05004], lr: 0.045575, loss: 2.3226
2022-03-03 16:45:52 - train: epoch 0109, iter [01900, 05004], lr: 0.045575, loss: 2.3264
2022-03-03 16:46:25 - train: epoch 0109, iter [02000, 05004], lr: 0.045575, loss: 2.4693
2022-03-03 16:46:57 - train: epoch 0109, iter [02100, 05004], lr: 0.045575, loss: 2.1466
2022-03-03 16:47:30 - train: epoch 0109, iter [02200, 05004], lr: 0.045575, loss: 2.6731
2022-03-03 16:48:03 - train: epoch 0109, iter [02300, 05004], lr: 0.045575, loss: 2.4732
2022-03-03 16:48:35 - train: epoch 0109, iter [02400, 05004], lr: 0.045575, loss: 2.2872
2022-03-03 16:49:08 - train: epoch 0109, iter [02500, 05004], lr: 0.045575, loss: 2.4725
2022-03-03 16:49:40 - train: epoch 0109, iter [02600, 05004], lr: 0.045575, loss: 2.5655
2022-03-03 16:50:12 - train: epoch 0109, iter [02700, 05004], lr: 0.045575, loss: 2.3027
2022-03-03 16:50:45 - train: epoch 0109, iter [02800, 05004], lr: 0.045575, loss: 2.4746
2022-03-03 16:51:17 - train: epoch 0109, iter [02900, 05004], lr: 0.045575, loss: 2.2270
2022-03-03 16:51:50 - train: epoch 0109, iter [03000, 05004], lr: 0.045575, loss: 2.6623
2022-03-03 16:52:23 - train: epoch 0109, iter [03100, 05004], lr: 0.045575, loss: 2.5213
2022-03-03 16:52:56 - train: epoch 0109, iter [03200, 05004], lr: 0.045575, loss: 2.2095
2022-03-03 16:53:28 - train: epoch 0109, iter [03300, 05004], lr: 0.045575, loss: 2.3071
2022-03-03 16:53:59 - train: epoch 0109, iter [03400, 05004], lr: 0.045575, loss: 2.3790
2022-03-03 16:54:32 - train: epoch 0109, iter [03500, 05004], lr: 0.045575, loss: 2.3599
2022-03-03 16:55:04 - train: epoch 0109, iter [03600, 05004], lr: 0.045575, loss: 2.2532
2022-03-03 16:55:37 - train: epoch 0109, iter [03700, 05004], lr: 0.045575, loss: 2.0505
2022-03-03 16:56:09 - train: epoch 0109, iter [03800, 05004], lr: 0.045575, loss: 2.4677
2022-03-03 16:56:42 - train: epoch 0109, iter [03900, 05004], lr: 0.045575, loss: 2.6478
2022-03-03 16:57:15 - train: epoch 0109, iter [04000, 05004], lr: 0.045575, loss: 2.4681
2022-03-03 16:57:47 - train: epoch 0109, iter [04100, 05004], lr: 0.045575, loss: 2.3486
2022-03-03 16:58:19 - train: epoch 0109, iter [04200, 05004], lr: 0.045575, loss: 2.2400
2022-03-03 16:58:51 - train: epoch 0109, iter [04300, 05004], lr: 0.045575, loss: 2.4422
2022-03-03 16:59:24 - train: epoch 0109, iter [04400, 05004], lr: 0.045575, loss: 2.4230
2022-03-03 16:59:56 - train: epoch 0109, iter [04500, 05004], lr: 0.045575, loss: 2.3996
2022-03-03 17:00:29 - train: epoch 0109, iter [04600, 05004], lr: 0.045575, loss: 2.3805
2022-03-03 17:01:01 - train: epoch 0109, iter [04700, 05004], lr: 0.045575, loss: 2.3915
2022-03-03 17:01:34 - train: epoch 0109, iter [04800, 05004], lr: 0.045575, loss: 2.5511
2022-03-03 17:02:06 - train: epoch 0109, iter [04900, 05004], lr: 0.045575, loss: 2.5036
2022-03-03 17:02:37 - train: epoch 0109, iter [05000, 05004], lr: 0.045575, loss: 2.1913
2022-03-03 17:02:38 - train: epoch 109, train_loss: 2.3810
2022-03-03 17:03:49 - eval: epoch: 109, acc1: 60.844%, acc5: 84.134%, test_loss: 1.6373, per_image_load_time: 1.502ms, per_image_inference_time: 0.417ms
2022-03-03 17:03:50 - until epoch: 109, best_acc1: 61.050%
2022-03-03 17:03:50 - epoch 110 lr: 0.04477357683661734
2022-03-03 17:04:27 - train: epoch 0110, iter [00100, 05004], lr: 0.044774, loss: 2.2698
2022-03-03 17:05:00 - train: epoch 0110, iter [00200, 05004], lr: 0.044774, loss: 2.4944
2022-03-03 17:05:33 - train: epoch 0110, iter [00300, 05004], lr: 0.044774, loss: 2.4800
2022-03-03 17:06:06 - train: epoch 0110, iter [00400, 05004], lr: 0.044774, loss: 2.6221
2022-03-03 17:06:38 - train: epoch 0110, iter [00500, 05004], lr: 0.044774, loss: 2.2436
2022-03-03 17:07:11 - train: epoch 0110, iter [00600, 05004], lr: 0.044774, loss: 2.4056
2022-03-03 17:07:43 - train: epoch 0110, iter [00700, 05004], lr: 0.044774, loss: 2.6368
2022-03-03 17:08:15 - train: epoch 0110, iter [00800, 05004], lr: 0.044774, loss: 2.4819
2022-03-03 17:08:48 - train: epoch 0110, iter [00900, 05004], lr: 0.044774, loss: 2.3931
2022-03-03 17:09:22 - train: epoch 0110, iter [01000, 05004], lr: 0.044774, loss: 2.3036
2022-03-03 17:09:54 - train: epoch 0110, iter [01100, 05004], lr: 0.044774, loss: 2.5358
2022-03-03 17:10:26 - train: epoch 0110, iter [01200, 05004], lr: 0.044774, loss: 2.3557
2022-03-03 17:10:58 - train: epoch 0110, iter [01300, 05004], lr: 0.044774, loss: 2.3932
2022-03-03 17:11:31 - train: epoch 0110, iter [01400, 05004], lr: 0.044774, loss: 2.2398
2022-03-03 17:12:03 - train: epoch 0110, iter [01500, 05004], lr: 0.044774, loss: 2.4909
2022-03-03 17:12:36 - train: epoch 0110, iter [01600, 05004], lr: 0.044774, loss: 2.5630
2022-03-03 17:13:08 - train: epoch 0110, iter [01700, 05004], lr: 0.044774, loss: 2.4439
2022-03-03 17:13:41 - train: epoch 0110, iter [01800, 05004], lr: 0.044774, loss: 2.5473
2022-03-03 17:14:13 - train: epoch 0110, iter [01900, 05004], lr: 0.044774, loss: 2.2914
2022-03-03 17:14:45 - train: epoch 0110, iter [02000, 05004], lr: 0.044774, loss: 2.2733
2022-03-03 17:15:17 - train: epoch 0110, iter [02100, 05004], lr: 0.044774, loss: 2.3799
2022-03-03 17:15:50 - train: epoch 0110, iter [02200, 05004], lr: 0.044774, loss: 2.2878
2022-03-03 17:16:22 - train: epoch 0110, iter [02300, 05004], lr: 0.044774, loss: 2.3891
2022-03-03 17:16:55 - train: epoch 0110, iter [02400, 05004], lr: 0.044774, loss: 2.1680
2022-03-03 17:17:28 - train: epoch 0110, iter [02500, 05004], lr: 0.044774, loss: 2.2596
2022-03-03 17:18:00 - train: epoch 0110, iter [02600, 05004], lr: 0.044774, loss: 2.6865
2022-03-03 17:18:33 - train: epoch 0110, iter [02700, 05004], lr: 0.044774, loss: 2.5922
2022-03-03 17:19:05 - train: epoch 0110, iter [02800, 05004], lr: 0.044774, loss: 2.5659
2022-03-03 17:19:38 - train: epoch 0110, iter [02900, 05004], lr: 0.044774, loss: 2.2980
2022-03-03 17:20:10 - train: epoch 0110, iter [03000, 05004], lr: 0.044774, loss: 1.9134
2022-03-03 17:20:43 - train: epoch 0110, iter [03100, 05004], lr: 0.044774, loss: 2.4313
2022-03-03 17:21:15 - train: epoch 0110, iter [03200, 05004], lr: 0.044774, loss: 2.5090
2022-03-03 17:21:48 - train: epoch 0110, iter [03300, 05004], lr: 0.044774, loss: 2.3720
2022-03-03 17:22:21 - train: epoch 0110, iter [03400, 05004], lr: 0.044774, loss: 1.9856
2022-03-03 17:22:52 - train: epoch 0110, iter [03500, 05004], lr: 0.044774, loss: 2.5854
2022-03-03 17:23:25 - train: epoch 0110, iter [03600, 05004], lr: 0.044774, loss: 2.1808
2022-03-03 17:23:58 - train: epoch 0110, iter [03700, 05004], lr: 0.044774, loss: 2.3831
2022-03-03 17:24:29 - train: epoch 0110, iter [03800, 05004], lr: 0.044774, loss: 2.4916
2022-03-03 17:25:02 - train: epoch 0110, iter [03900, 05004], lr: 0.044774, loss: 2.1748
2022-03-03 17:25:34 - train: epoch 0110, iter [04000, 05004], lr: 0.044774, loss: 2.2918
2022-03-03 17:26:08 - train: epoch 0110, iter [04100, 05004], lr: 0.044774, loss: 2.6999
2022-03-03 17:26:40 - train: epoch 0110, iter [04200, 05004], lr: 0.044774, loss: 2.4864
2022-03-03 17:27:13 - train: epoch 0110, iter [04300, 05004], lr: 0.044774, loss: 2.2959
2022-03-03 17:27:44 - train: epoch 0110, iter [04400, 05004], lr: 0.044774, loss: 2.5984
2022-03-03 17:28:16 - train: epoch 0110, iter [04500, 05004], lr: 0.044774, loss: 2.2936
2022-03-03 17:28:49 - train: epoch 0110, iter [04600, 05004], lr: 0.044774, loss: 2.1469
2022-03-03 17:29:21 - train: epoch 0110, iter [04700, 05004], lr: 0.044774, loss: 2.6010
2022-03-03 17:29:53 - train: epoch 0110, iter [04800, 05004], lr: 0.044774, loss: 2.4444
2022-03-03 17:30:26 - train: epoch 0110, iter [04900, 05004], lr: 0.044774, loss: 2.2980
2022-03-03 17:30:58 - train: epoch 0110, iter [05000, 05004], lr: 0.044774, loss: 2.3685
2022-03-03 17:30:59 - train: epoch 110, train_loss: 2.3704
2022-03-03 17:32:09 - eval: epoch: 110, acc1: 61.678%, acc5: 85.012%, test_loss: 1.5770, per_image_load_time: 1.647ms, per_image_inference_time: 0.412ms
2022-03-03 17:32:10 - until epoch: 110, best_acc1: 61.678%
2022-03-03 17:32:10 - epoch 111 lr: 0.04397316598723386
2022-03-03 17:32:47 - train: epoch 0111, iter [00100, 05004], lr: 0.043973, loss: 2.5809
2022-03-03 17:33:21 - train: epoch 0111, iter [00200, 05004], lr: 0.043973, loss: 2.3657
2022-03-03 17:33:52 - train: epoch 0111, iter [00300, 05004], lr: 0.043973, loss: 2.1083
2022-03-03 17:34:25 - train: epoch 0111, iter [00400, 05004], lr: 0.043973, loss: 2.5916
2022-03-03 17:34:58 - train: epoch 0111, iter [00500, 05004], lr: 0.043973, loss: 2.3604
2022-03-03 17:35:31 - train: epoch 0111, iter [00600, 05004], lr: 0.043973, loss: 2.5056
2022-03-03 17:36:03 - train: epoch 0111, iter [00700, 05004], lr: 0.043973, loss: 2.4142
2022-03-03 17:36:35 - train: epoch 0111, iter [00800, 05004], lr: 0.043973, loss: 2.1055
2022-03-03 17:37:07 - train: epoch 0111, iter [00900, 05004], lr: 0.043973, loss: 2.2375
2022-03-03 17:37:40 - train: epoch 0111, iter [01000, 05004], lr: 0.043973, loss: 2.0759
2022-03-03 17:38:12 - train: epoch 0111, iter [01100, 05004], lr: 0.043973, loss: 2.6453
2022-03-03 17:38:46 - train: epoch 0111, iter [01200, 05004], lr: 0.043973, loss: 2.3493
2022-03-03 17:39:17 - train: epoch 0111, iter [01300, 05004], lr: 0.043973, loss: 2.3312
2022-03-03 17:39:50 - train: epoch 0111, iter [01400, 05004], lr: 0.043973, loss: 2.3150
2022-03-03 17:40:23 - train: epoch 0111, iter [01500, 05004], lr: 0.043973, loss: 2.4709
2022-03-03 17:40:55 - train: epoch 0111, iter [01600, 05004], lr: 0.043973, loss: 2.3947
2022-03-03 17:41:27 - train: epoch 0111, iter [01700, 05004], lr: 0.043973, loss: 2.2714
2022-03-03 17:42:00 - train: epoch 0111, iter [01800, 05004], lr: 0.043973, loss: 2.3158
2022-03-03 17:42:32 - train: epoch 0111, iter [01900, 05004], lr: 0.043973, loss: 2.5152
2022-03-03 17:43:05 - train: epoch 0111, iter [02000, 05004], lr: 0.043973, loss: 2.4080
2022-03-03 17:43:38 - train: epoch 0111, iter [02100, 05004], lr: 0.043973, loss: 2.6717
2022-03-03 17:44:09 - train: epoch 0111, iter [02200, 05004], lr: 0.043973, loss: 2.4093
2022-03-03 17:44:42 - train: epoch 0111, iter [02300, 05004], lr: 0.043973, loss: 2.3591
2022-03-03 17:45:15 - train: epoch 0111, iter [02400, 05004], lr: 0.043973, loss: 2.2839
2022-03-03 17:45:48 - train: epoch 0111, iter [02500, 05004], lr: 0.043973, loss: 2.1998
2022-03-03 17:46:19 - train: epoch 0111, iter [02600, 05004], lr: 0.043973, loss: 2.4411
2022-03-03 17:46:53 - train: epoch 0111, iter [02700, 05004], lr: 0.043973, loss: 2.3853
2022-03-03 17:47:25 - train: epoch 0111, iter [02800, 05004], lr: 0.043973, loss: 2.2282
2022-03-03 17:47:58 - train: epoch 0111, iter [02900, 05004], lr: 0.043973, loss: 2.4628
2022-03-03 17:48:30 - train: epoch 0111, iter [03000, 05004], lr: 0.043973, loss: 2.2019
2022-03-03 17:49:03 - train: epoch 0111, iter [03100, 05004], lr: 0.043973, loss: 2.3874
2022-03-03 17:49:34 - train: epoch 0111, iter [03200, 05004], lr: 0.043973, loss: 2.5811
2022-03-03 17:50:07 - train: epoch 0111, iter [03300, 05004], lr: 0.043973, loss: 2.5929
2022-03-03 17:50:40 - train: epoch 0111, iter [03400, 05004], lr: 0.043973, loss: 2.4193
2022-03-03 17:51:12 - train: epoch 0111, iter [03500, 05004], lr: 0.043973, loss: 2.6714
2022-03-03 17:51:45 - train: epoch 0111, iter [03600, 05004], lr: 0.043973, loss: 2.1683
2022-03-03 17:52:17 - train: epoch 0111, iter [03700, 05004], lr: 0.043973, loss: 2.3199
2022-03-03 17:52:50 - train: epoch 0111, iter [03800, 05004], lr: 0.043973, loss: 2.6877
2022-03-03 17:53:22 - train: epoch 0111, iter [03900, 05004], lr: 0.043973, loss: 2.1005
2022-03-03 17:53:54 - train: epoch 0111, iter [04000, 05004], lr: 0.043973, loss: 2.5826
2022-03-03 17:54:26 - train: epoch 0111, iter [04100, 05004], lr: 0.043973, loss: 2.5486
2022-03-03 17:54:59 - train: epoch 0111, iter [04200, 05004], lr: 0.043973, loss: 2.3969
2022-03-03 17:55:32 - train: epoch 0111, iter [04300, 05004], lr: 0.043973, loss: 2.5645
2022-03-03 17:56:05 - train: epoch 0111, iter [04400, 05004], lr: 0.043973, loss: 2.4899
2022-03-03 17:56:38 - train: epoch 0111, iter [04500, 05004], lr: 0.043973, loss: 2.6230
2022-03-03 17:57:09 - train: epoch 0111, iter [04600, 05004], lr: 0.043973, loss: 2.2974
2022-03-03 17:57:42 - train: epoch 0111, iter [04700, 05004], lr: 0.043973, loss: 2.5469
2022-03-03 17:58:13 - train: epoch 0111, iter [04800, 05004], lr: 0.043973, loss: 2.2077
2022-03-03 17:58:46 - train: epoch 0111, iter [04900, 05004], lr: 0.043973, loss: 2.3888
2022-03-03 17:59:17 - train: epoch 0111, iter [05000, 05004], lr: 0.043973, loss: 2.6063
2022-03-03 17:59:19 - train: epoch 111, train_loss: 2.3648
2022-03-03 18:00:30 - eval: epoch: 111, acc1: 61.200%, acc5: 84.580%, test_loss: 1.6098, per_image_load_time: 1.570ms, per_image_inference_time: 0.433ms
2022-03-03 18:00:31 - until epoch: 111, best_acc1: 61.678%
2022-03-03 18:00:31 - epoch 112 lr: 0.04317431940296343
2022-03-03 18:01:08 - train: epoch 0112, iter [00100, 05004], lr: 0.043174, loss: 2.2877
2022-03-03 18:01:40 - train: epoch 0112, iter [00200, 05004], lr: 0.043174, loss: 2.3962
2022-03-03 18:02:12 - train: epoch 0112, iter [00300, 05004], lr: 0.043174, loss: 2.2814
2022-03-03 18:02:44 - train: epoch 0112, iter [00400, 05004], lr: 0.043174, loss: 2.1470
2022-03-03 18:03:18 - train: epoch 0112, iter [00500, 05004], lr: 0.043174, loss: 2.6439
2022-03-03 18:03:50 - train: epoch 0112, iter [00600, 05004], lr: 0.043174, loss: 2.2842
2022-03-03 18:04:23 - train: epoch 0112, iter [00700, 05004], lr: 0.043174, loss: 2.5689
2022-03-03 18:04:56 - train: epoch 0112, iter [00800, 05004], lr: 0.043174, loss: 2.2572
2022-03-03 18:05:27 - train: epoch 0112, iter [00900, 05004], lr: 0.043174, loss: 2.0792
2022-03-03 18:06:00 - train: epoch 0112, iter [01000, 05004], lr: 0.043174, loss: 2.1935
2022-03-03 18:06:33 - train: epoch 0112, iter [01100, 05004], lr: 0.043174, loss: 2.4147
2022-03-03 18:07:04 - train: epoch 0112, iter [01200, 05004], lr: 0.043174, loss: 2.5677
2022-03-03 18:07:37 - train: epoch 0112, iter [01300, 05004], lr: 0.043174, loss: 2.4880
2022-03-03 18:08:09 - train: epoch 0112, iter [01400, 05004], lr: 0.043174, loss: 2.5257
2022-03-03 18:08:41 - train: epoch 0112, iter [01500, 05004], lr: 0.043174, loss: 2.3790
2022-03-03 18:09:15 - train: epoch 0112, iter [01600, 05004], lr: 0.043174, loss: 2.4858
2022-03-03 18:09:48 - train: epoch 0112, iter [01700, 05004], lr: 0.043174, loss: 2.2137
2022-03-03 18:10:20 - train: epoch 0112, iter [01800, 05004], lr: 0.043174, loss: 2.3199
2022-03-03 18:10:52 - train: epoch 0112, iter [01900, 05004], lr: 0.043174, loss: 2.1921
2022-03-03 18:11:25 - train: epoch 0112, iter [02000, 05004], lr: 0.043174, loss: 2.2372
2022-03-03 18:11:57 - train: epoch 0112, iter [02100, 05004], lr: 0.043174, loss: 2.5066
2022-03-03 18:12:29 - train: epoch 0112, iter [02200, 05004], lr: 0.043174, loss: 2.2829
2022-03-03 18:13:02 - train: epoch 0112, iter [02300, 05004], lr: 0.043174, loss: 2.4052
2022-03-03 18:13:34 - train: epoch 0112, iter [02400, 05004], lr: 0.043174, loss: 2.1820
2022-03-03 18:14:06 - train: epoch 0112, iter [02500, 05004], lr: 0.043174, loss: 2.4822
2022-03-03 18:14:39 - train: epoch 0112, iter [02600, 05004], lr: 0.043174, loss: 2.1729
2022-03-03 18:15:11 - train: epoch 0112, iter [02700, 05004], lr: 0.043174, loss: 2.2965
2022-03-03 18:15:44 - train: epoch 0112, iter [02800, 05004], lr: 0.043174, loss: 2.3115
2022-03-03 18:16:17 - train: epoch 0112, iter [02900, 05004], lr: 0.043174, loss: 2.4738
2022-03-03 18:16:49 - train: epoch 0112, iter [03000, 05004], lr: 0.043174, loss: 2.4094
2022-03-03 18:17:21 - train: epoch 0112, iter [03100, 05004], lr: 0.043174, loss: 2.0805
2022-03-03 18:17:53 - train: epoch 0112, iter [03200, 05004], lr: 0.043174, loss: 2.4311
2022-03-03 18:18:26 - train: epoch 0112, iter [03300, 05004], lr: 0.043174, loss: 2.1834
2022-03-03 18:18:58 - train: epoch 0112, iter [03400, 05004], lr: 0.043174, loss: 2.4206
2022-03-03 18:19:31 - train: epoch 0112, iter [03500, 05004], lr: 0.043174, loss: 2.4737
2022-03-03 18:20:03 - train: epoch 0112, iter [03600, 05004], lr: 0.043174, loss: 2.2846
2022-03-03 18:20:36 - train: epoch 0112, iter [03700, 05004], lr: 0.043174, loss: 2.4627
2022-03-03 18:21:09 - train: epoch 0112, iter [03800, 05004], lr: 0.043174, loss: 2.4323
2022-03-03 18:21:41 - train: epoch 0112, iter [03900, 05004], lr: 0.043174, loss: 2.2954
2022-03-03 18:22:13 - train: epoch 0112, iter [04000, 05004], lr: 0.043174, loss: 2.4180
2022-03-03 18:22:45 - train: epoch 0112, iter [04100, 05004], lr: 0.043174, loss: 2.1853
2022-03-03 18:23:18 - train: epoch 0112, iter [04200, 05004], lr: 0.043174, loss: 2.2392
2022-03-03 18:23:50 - train: epoch 0112, iter [04300, 05004], lr: 0.043174, loss: 2.3841
2022-03-03 18:24:23 - train: epoch 0112, iter [04400, 05004], lr: 0.043174, loss: 2.5898
2022-03-03 18:24:55 - train: epoch 0112, iter [04500, 05004], lr: 0.043174, loss: 2.2791
2022-03-03 18:25:28 - train: epoch 0112, iter [04600, 05004], lr: 0.043174, loss: 2.4854
2022-03-03 18:26:00 - train: epoch 0112, iter [04700, 05004], lr: 0.043174, loss: 2.4249
2022-03-03 18:26:32 - train: epoch 0112, iter [04800, 05004], lr: 0.043174, loss: 2.2585
2022-03-03 18:27:04 - train: epoch 0112, iter [04900, 05004], lr: 0.043174, loss: 2.0807
2022-03-03 18:27:36 - train: epoch 0112, iter [05000, 05004], lr: 0.043174, loss: 2.1611
2022-03-03 18:27:37 - train: epoch 112, train_loss: 2.3536
2022-03-03 18:28:48 - eval: epoch: 112, acc1: 62.240%, acc5: 85.278%, test_loss: 1.5536, per_image_load_time: 1.094ms, per_image_inference_time: 0.428ms
2022-03-03 18:28:48 - until epoch: 112, best_acc1: 62.240%
2022-03-03 18:28:48 - epoch 113 lr: 0.042377244424482735
2022-03-03 18:29:26 - train: epoch 0113, iter [00100, 05004], lr: 0.042377, loss: 2.1610
2022-03-03 18:29:58 - train: epoch 0113, iter [00200, 05004], lr: 0.042377, loss: 2.0769
2022-03-03 18:30:31 - train: epoch 0113, iter [00300, 05004], lr: 0.042377, loss: 2.4765
2022-03-03 18:31:04 - train: epoch 0113, iter [00400, 05004], lr: 0.042377, loss: 2.1771
2022-03-03 18:31:36 - train: epoch 0113, iter [00500, 05004], lr: 0.042377, loss: 2.1931
2022-03-03 18:32:09 - train: epoch 0113, iter [00600, 05004], lr: 0.042377, loss: 2.3695
2022-03-03 18:32:40 - train: epoch 0113, iter [00700, 05004], lr: 0.042377, loss: 2.3824
2022-03-03 18:33:14 - train: epoch 0113, iter [00800, 05004], lr: 0.042377, loss: 2.3748
2022-03-03 18:33:45 - train: epoch 0113, iter [00900, 05004], lr: 0.042377, loss: 2.5114
2022-03-03 18:34:19 - train: epoch 0113, iter [01000, 05004], lr: 0.042377, loss: 2.6429
2022-03-03 18:34:51 - train: epoch 0113, iter [01100, 05004], lr: 0.042377, loss: 2.4399
2022-03-03 18:35:23 - train: epoch 0113, iter [01200, 05004], lr: 0.042377, loss: 2.3122
2022-03-03 18:35:56 - train: epoch 0113, iter [01300, 05004], lr: 0.042377, loss: 2.3698
2022-03-03 18:36:28 - train: epoch 0113, iter [01400, 05004], lr: 0.042377, loss: 2.1320
2022-03-03 18:37:00 - train: epoch 0113, iter [01500, 05004], lr: 0.042377, loss: 2.5232
2022-03-03 18:37:32 - train: epoch 0113, iter [01600, 05004], lr: 0.042377, loss: 2.3312
2022-03-03 18:38:05 - train: epoch 0113, iter [01700, 05004], lr: 0.042377, loss: 2.4848
2022-03-03 18:38:38 - train: epoch 0113, iter [01800, 05004], lr: 0.042377, loss: 2.3522
2022-03-03 18:39:10 - train: epoch 0113, iter [01900, 05004], lr: 0.042377, loss: 2.2926
2022-03-03 18:39:43 - train: epoch 0113, iter [02000, 05004], lr: 0.042377, loss: 2.3932
2022-03-03 18:40:15 - train: epoch 0113, iter [02100, 05004], lr: 0.042377, loss: 2.3424
2022-03-03 18:40:48 - train: epoch 0113, iter [02200, 05004], lr: 0.042377, loss: 2.0720
2022-03-03 18:41:20 - train: epoch 0113, iter [02300, 05004], lr: 0.042377, loss: 2.3670
2022-03-03 18:41:52 - train: epoch 0113, iter [02400, 05004], lr: 0.042377, loss: 2.3168
2022-03-03 18:42:25 - train: epoch 0113, iter [02500, 05004], lr: 0.042377, loss: 2.6624
2022-03-03 18:42:57 - train: epoch 0113, iter [02600, 05004], lr: 0.042377, loss: 2.2226
2022-03-03 18:43:30 - train: epoch 0113, iter [02700, 05004], lr: 0.042377, loss: 2.5466
2022-03-03 18:44:03 - train: epoch 0113, iter [02800, 05004], lr: 0.042377, loss: 2.5277
2022-03-03 18:44:35 - train: epoch 0113, iter [02900, 05004], lr: 0.042377, loss: 2.2833
2022-03-03 18:45:07 - train: epoch 0113, iter [03000, 05004], lr: 0.042377, loss: 2.1512
2022-03-03 18:45:40 - train: epoch 0113, iter [03100, 05004], lr: 0.042377, loss: 2.1944
2022-03-03 18:46:11 - train: epoch 0113, iter [03200, 05004], lr: 0.042377, loss: 2.2465
2022-03-03 18:46:44 - train: epoch 0113, iter [03300, 05004], lr: 0.042377, loss: 2.2463
2022-03-03 18:47:16 - train: epoch 0113, iter [03400, 05004], lr: 0.042377, loss: 2.1014
2022-03-03 18:47:48 - train: epoch 0113, iter [03500, 05004], lr: 0.042377, loss: 2.3385
2022-03-03 18:48:21 - train: epoch 0113, iter [03600, 05004], lr: 0.042377, loss: 2.5182
2022-03-03 18:48:53 - train: epoch 0113, iter [03700, 05004], lr: 0.042377, loss: 2.0994
2022-03-03 18:49:26 - train: epoch 0113, iter [03800, 05004], lr: 0.042377, loss: 2.3056
2022-03-03 18:49:59 - train: epoch 0113, iter [03900, 05004], lr: 0.042377, loss: 2.4120
2022-03-03 18:50:32 - train: epoch 0113, iter [04000, 05004], lr: 0.042377, loss: 2.1829
2022-03-03 18:51:04 - train: epoch 0113, iter [04100, 05004], lr: 0.042377, loss: 2.5512
2022-03-03 18:51:36 - train: epoch 0113, iter [04200, 05004], lr: 0.042377, loss: 2.1587
2022-03-03 18:52:07 - train: epoch 0113, iter [04300, 05004], lr: 0.042377, loss: 2.5441
2022-03-03 18:52:40 - train: epoch 0113, iter [04400, 05004], lr: 0.042377, loss: 2.3233
2022-03-03 18:53:12 - train: epoch 0113, iter [04500, 05004], lr: 0.042377, loss: 2.3585
2022-03-03 18:53:44 - train: epoch 0113, iter [04600, 05004], lr: 0.042377, loss: 2.5153
2022-03-03 18:54:17 - train: epoch 0113, iter [04700, 05004], lr: 0.042377, loss: 2.2205
2022-03-03 18:54:50 - train: epoch 0113, iter [04800, 05004], lr: 0.042377, loss: 2.2746
2022-03-03 18:55:22 - train: epoch 0113, iter [04900, 05004], lr: 0.042377, loss: 2.1902
2022-03-03 18:55:53 - train: epoch 0113, iter [05000, 05004], lr: 0.042377, loss: 2.4429
2022-03-03 18:55:55 - train: epoch 113, train_loss: 2.3497
2022-03-03 18:57:05 - eval: epoch: 113, acc1: 60.482%, acc5: 84.084%, test_loss: 1.6352, per_image_load_time: 1.726ms, per_image_inference_time: 0.368ms
2022-03-03 18:57:06 - until epoch: 113, best_acc1: 62.240%
2022-03-03 18:57:06 - epoch 114 lr: 0.04158214793264808
2022-03-03 18:57:43 - train: epoch 0114, iter [00100, 05004], lr: 0.041582, loss: 2.6319
2022-03-03 18:58:16 - train: epoch 0114, iter [00200, 05004], lr: 0.041582, loss: 2.0409
2022-03-03 18:58:48 - train: epoch 0114, iter [00300, 05004], lr: 0.041582, loss: 2.1833
2022-03-03 18:59:22 - train: epoch 0114, iter [00400, 05004], lr: 0.041582, loss: 2.5342
2022-03-03 18:59:54 - train: epoch 0114, iter [00500, 05004], lr: 0.041582, loss: 2.4790
2022-03-03 19:00:28 - train: epoch 0114, iter [00600, 05004], lr: 0.041582, loss: 2.4223
2022-03-03 19:00:59 - train: epoch 0114, iter [00700, 05004], lr: 0.041582, loss: 2.4503
2022-03-03 19:01:31 - train: epoch 0114, iter [00800, 05004], lr: 0.041582, loss: 2.0207
2022-03-03 19:02:03 - train: epoch 0114, iter [00900, 05004], lr: 0.041582, loss: 2.1497
2022-03-03 19:02:35 - train: epoch 0114, iter [01000, 05004], lr: 0.041582, loss: 2.2111
2022-03-03 19:03:08 - train: epoch 0114, iter [01100, 05004], lr: 0.041582, loss: 2.2588
2022-03-03 19:03:41 - train: epoch 0114, iter [01200, 05004], lr: 0.041582, loss: 2.4854
2022-03-03 19:04:13 - train: epoch 0114, iter [01300, 05004], lr: 0.041582, loss: 2.1708
2022-03-03 19:04:46 - train: epoch 0114, iter [01400, 05004], lr: 0.041582, loss: 2.5615
2022-03-03 19:05:18 - train: epoch 0114, iter [01500, 05004], lr: 0.041582, loss: 2.5778
2022-03-03 19:05:51 - train: epoch 0114, iter [01600, 05004], lr: 0.041582, loss: 2.1622
2022-03-03 19:06:22 - train: epoch 0114, iter [01700, 05004], lr: 0.041582, loss: 2.3547
2022-03-03 19:06:55 - train: epoch 0114, iter [01800, 05004], lr: 0.041582, loss: 2.3417
2022-03-03 19:07:27 - train: epoch 0114, iter [01900, 05004], lr: 0.041582, loss: 2.3111
2022-03-03 19:08:00 - train: epoch 0114, iter [02000, 05004], lr: 0.041582, loss: 2.2775
2022-03-03 19:08:32 - train: epoch 0114, iter [02100, 05004], lr: 0.041582, loss: 2.3440
2022-03-03 19:09:06 - train: epoch 0114, iter [02200, 05004], lr: 0.041582, loss: 2.3975
2022-03-03 19:09:38 - train: epoch 0114, iter [02300, 05004], lr: 0.041582, loss: 2.4560
2022-03-03 19:10:11 - train: epoch 0114, iter [02400, 05004], lr: 0.041582, loss: 2.0815
2022-03-03 19:10:43 - train: epoch 0114, iter [02500, 05004], lr: 0.041582, loss: 2.3503
2022-03-03 19:11:15 - train: epoch 0114, iter [02600, 05004], lr: 0.041582, loss: 2.2755
2022-03-03 19:11:47 - train: epoch 0114, iter [02700, 05004], lr: 0.041582, loss: 2.4834
2022-03-03 19:12:19 - train: epoch 0114, iter [02800, 05004], lr: 0.041582, loss: 2.3133
2022-03-03 19:12:53 - train: epoch 0114, iter [02900, 05004], lr: 0.041582, loss: 2.1706
2022-03-03 19:13:24 - train: epoch 0114, iter [03000, 05004], lr: 0.041582, loss: 2.2163
2022-03-03 19:13:57 - train: epoch 0114, iter [03100, 05004], lr: 0.041582, loss: 2.4339
2022-03-03 19:14:30 - train: epoch 0114, iter [03200, 05004], lr: 0.041582, loss: 2.3733
2022-03-03 19:15:03 - train: epoch 0114, iter [03300, 05004], lr: 0.041582, loss: 2.4225
2022-03-03 19:15:35 - train: epoch 0114, iter [03400, 05004], lr: 0.041582, loss: 2.3792
2022-03-03 19:16:08 - train: epoch 0114, iter [03500, 05004], lr: 0.041582, loss: 2.1430
2022-03-03 19:16:39 - train: epoch 0114, iter [03600, 05004], lr: 0.041582, loss: 2.1683
2022-03-03 19:17:11 - train: epoch 0114, iter [03700, 05004], lr: 0.041582, loss: 2.0974
2022-03-03 19:17:44 - train: epoch 0114, iter [03800, 05004], lr: 0.041582, loss: 2.1252
2022-03-03 19:18:17 - train: epoch 0114, iter [03900, 05004], lr: 0.041582, loss: 2.4194
2022-03-03 19:18:50 - train: epoch 0114, iter [04000, 05004], lr: 0.041582, loss: 2.3996
2022-03-03 19:19:22 - train: epoch 0114, iter [04100, 05004], lr: 0.041582, loss: 2.1451
2022-03-03 19:19:55 - train: epoch 0114, iter [04200, 05004], lr: 0.041582, loss: 2.4337
2022-03-03 19:20:26 - train: epoch 0114, iter [04300, 05004], lr: 0.041582, loss: 2.2919
2022-03-03 19:20:59 - train: epoch 0114, iter [04400, 05004], lr: 0.041582, loss: 2.5229
2022-03-03 19:21:31 - train: epoch 0114, iter [04500, 05004], lr: 0.041582, loss: 2.5802
2022-03-03 19:22:02 - train: epoch 0114, iter [04600, 05004], lr: 0.041582, loss: 2.5261
2022-03-03 19:22:35 - train: epoch 0114, iter [04700, 05004], lr: 0.041582, loss: 2.1832
2022-03-03 19:23:08 - train: epoch 0114, iter [04800, 05004], lr: 0.041582, loss: 2.3973
2022-03-03 19:23:40 - train: epoch 0114, iter [04900, 05004], lr: 0.041582, loss: 2.2305
2022-03-03 19:24:11 - train: epoch 0114, iter [05000, 05004], lr: 0.041582, loss: 2.5343
2022-03-03 19:24:13 - train: epoch 114, train_loss: 2.3387
2022-03-03 19:25:24 - eval: epoch: 114, acc1: 61.086%, acc5: 84.692%, test_loss: 1.6118, per_image_load_time: 0.615ms, per_image_inference_time: 0.421ms
2022-03-03 19:25:24 - until epoch: 114, best_acc1: 62.240%
2022-03-03 19:25:24 - epoch 115 lr: 0.04078923629479943
2022-03-03 19:26:01 - train: epoch 0115, iter [00100, 05004], lr: 0.040789, loss: 2.1425
2022-03-03 19:26:34 - train: epoch 0115, iter [00200, 05004], lr: 0.040789, loss: 2.2750
2022-03-03 19:27:06 - train: epoch 0115, iter [00300, 05004], lr: 0.040789, loss: 2.2955
2022-03-03 19:27:39 - train: epoch 0115, iter [00400, 05004], lr: 0.040789, loss: 2.1110
2022-03-03 19:28:12 - train: epoch 0115, iter [00500, 05004], lr: 0.040789, loss: 2.1697
2022-03-03 19:28:44 - train: epoch 0115, iter [00600, 05004], lr: 0.040789, loss: 2.2692
2022-03-03 19:29:16 - train: epoch 0115, iter [00700, 05004], lr: 0.040789, loss: 2.4271
2022-03-03 19:29:49 - train: epoch 0115, iter [00800, 05004], lr: 0.040789, loss: 2.4348
2022-03-03 19:30:21 - train: epoch 0115, iter [00900, 05004], lr: 0.040789, loss: 2.3479
2022-03-03 19:30:54 - train: epoch 0115, iter [01000, 05004], lr: 0.040789, loss: 2.0604
2022-03-03 19:31:26 - train: epoch 0115, iter [01100, 05004], lr: 0.040789, loss: 2.3856
2022-03-03 19:31:58 - train: epoch 0115, iter [01200, 05004], lr: 0.040789, loss: 2.2287
2022-03-03 19:32:30 - train: epoch 0115, iter [01300, 05004], lr: 0.040789, loss: 2.3298
2022-03-03 19:33:03 - train: epoch 0115, iter [01400, 05004], lr: 0.040789, loss: 2.1289
2022-03-03 19:33:35 - train: epoch 0115, iter [01500, 05004], lr: 0.040789, loss: 2.5427
2022-03-03 19:34:08 - train: epoch 0115, iter [01600, 05004], lr: 0.040789, loss: 2.1837
2022-03-03 19:34:41 - train: epoch 0115, iter [01700, 05004], lr: 0.040789, loss: 2.4482
2022-03-03 19:35:13 - train: epoch 0115, iter [01800, 05004], lr: 0.040789, loss: 2.7019
2022-03-03 19:35:45 - train: epoch 0115, iter [01900, 05004], lr: 0.040789, loss: 2.3057
2022-03-03 19:36:18 - train: epoch 0115, iter [02000, 05004], lr: 0.040789, loss: 2.4656
2022-03-03 19:36:49 - train: epoch 0115, iter [02100, 05004], lr: 0.040789, loss: 2.3483
2022-03-03 19:37:22 - train: epoch 0115, iter [02200, 05004], lr: 0.040789, loss: 2.6467
2022-03-03 19:37:54 - train: epoch 0115, iter [02300, 05004], lr: 0.040789, loss: 2.5349
2022-03-03 19:38:27 - train: epoch 0115, iter [02400, 05004], lr: 0.040789, loss: 2.2278
2022-03-03 19:38:59 - train: epoch 0115, iter [02500, 05004], lr: 0.040789, loss: 2.0601
2022-03-03 19:39:32 - train: epoch 0115, iter [02600, 05004], lr: 0.040789, loss: 2.2996
2022-03-03 19:40:05 - train: epoch 0115, iter [02700, 05004], lr: 0.040789, loss: 2.1243
2022-03-03 19:40:36 - train: epoch 0115, iter [02800, 05004], lr: 0.040789, loss: 2.1805
2022-03-03 19:41:09 - train: epoch 0115, iter [02900, 05004], lr: 0.040789, loss: 2.2579
2022-03-03 19:41:41 - train: epoch 0115, iter [03000, 05004], lr: 0.040789, loss: 2.1131
2022-03-03 19:42:13 - train: epoch 0115, iter [03100, 05004], lr: 0.040789, loss: 2.4055
2022-03-03 19:42:46 - train: epoch 0115, iter [03200, 05004], lr: 0.040789, loss: 2.0317
2022-03-03 19:43:18 - train: epoch 0115, iter [03300, 05004], lr: 0.040789, loss: 2.4537
2022-03-03 19:43:50 - train: epoch 0115, iter [03400, 05004], lr: 0.040789, loss: 2.4042
2022-03-03 19:44:23 - train: epoch 0115, iter [03500, 05004], lr: 0.040789, loss: 2.1638
2022-03-03 19:44:56 - train: epoch 0115, iter [03600, 05004], lr: 0.040789, loss: 2.6286
2022-03-03 19:45:28 - train: epoch 0115, iter [03700, 05004], lr: 0.040789, loss: 2.4748
2022-03-03 19:45:59 - train: epoch 0115, iter [03800, 05004], lr: 0.040789, loss: 2.6760
2022-03-03 19:46:32 - train: epoch 0115, iter [03900, 05004], lr: 0.040789, loss: 2.1978
2022-03-03 19:47:04 - train: epoch 0115, iter [04000, 05004], lr: 0.040789, loss: 2.3081
2022-03-03 19:47:36 - train: epoch 0115, iter [04100, 05004], lr: 0.040789, loss: 2.2248
2022-03-03 19:48:08 - train: epoch 0115, iter [04200, 05004], lr: 0.040789, loss: 2.3997
2022-03-03 19:48:42 - train: epoch 0115, iter [04300, 05004], lr: 0.040789, loss: 2.3965
2022-03-03 19:49:14 - train: epoch 0115, iter [04400, 05004], lr: 0.040789, loss: 2.4578
2022-03-03 19:49:46 - train: epoch 0115, iter [04500, 05004], lr: 0.040789, loss: 2.0792
2022-03-03 19:50:18 - train: epoch 0115, iter [04600, 05004], lr: 0.040789, loss: 2.2025
2022-03-03 19:50:50 - train: epoch 0115, iter [04700, 05004], lr: 0.040789, loss: 2.2095
2022-03-03 19:51:23 - train: epoch 0115, iter [04800, 05004], lr: 0.040789, loss: 2.3411
2022-03-03 19:51:55 - train: epoch 0115, iter [04900, 05004], lr: 0.040789, loss: 2.1424
2022-03-03 19:52:26 - train: epoch 0115, iter [05000, 05004], lr: 0.040789, loss: 2.4496
2022-03-03 19:52:27 - train: epoch 115, train_loss: 2.3291
2022-03-03 19:53:38 - eval: epoch: 115, acc1: 62.074%, acc5: 85.132%, test_loss: 1.5575, per_image_load_time: 1.096ms, per_image_inference_time: 0.420ms
2022-03-03 19:53:39 - until epoch: 115, best_acc1: 62.240%
2022-03-03 19:53:39 - epoch 116 lr: 0.03999871531119779
2022-03-03 19:54:17 - train: epoch 0116, iter [00100, 05004], lr: 0.039999, loss: 2.3669
2022-03-03 19:54:49 - train: epoch 0116, iter [00200, 05004], lr: 0.039999, loss: 2.1406
2022-03-03 19:55:21 - train: epoch 0116, iter [00300, 05004], lr: 0.039999, loss: 2.0388
2022-03-03 19:55:53 - train: epoch 0116, iter [00400, 05004], lr: 0.039999, loss: 2.3819
2022-03-03 19:56:26 - train: epoch 0116, iter [00500, 05004], lr: 0.039999, loss: 2.4368
2022-03-03 19:56:58 - train: epoch 0116, iter [00600, 05004], lr: 0.039999, loss: 2.1144
2022-03-03 19:57:31 - train: epoch 0116, iter [00700, 05004], lr: 0.039999, loss: 2.1674
2022-03-03 19:58:03 - train: epoch 0116, iter [00800, 05004], lr: 0.039999, loss: 2.3346
2022-03-03 19:58:36 - train: epoch 0116, iter [00900, 05004], lr: 0.039999, loss: 2.4909
2022-03-03 19:59:08 - train: epoch 0116, iter [01000, 05004], lr: 0.039999, loss: 2.5385
2022-03-03 19:59:40 - train: epoch 0116, iter [01100, 05004], lr: 0.039999, loss: 2.3383
2022-03-03 20:00:13 - train: epoch 0116, iter [01200, 05004], lr: 0.039999, loss: 2.2583
2022-03-03 20:00:45 - train: epoch 0116, iter [01300, 05004], lr: 0.039999, loss: 2.4057
2022-03-03 20:01:17 - train: epoch 0116, iter [01400, 05004], lr: 0.039999, loss: 2.3068
2022-03-03 20:01:49 - train: epoch 0116, iter [01500, 05004], lr: 0.039999, loss: 2.3237
2022-03-03 20:02:22 - train: epoch 0116, iter [01600, 05004], lr: 0.039999, loss: 2.2720
2022-03-03 20:02:54 - train: epoch 0116, iter [01700, 05004], lr: 0.039999, loss: 2.1370
2022-03-03 20:03:27 - train: epoch 0116, iter [01800, 05004], lr: 0.039999, loss: 2.5224
2022-03-03 20:03:59 - train: epoch 0116, iter [01900, 05004], lr: 0.039999, loss: 2.3287
2022-03-03 20:04:31 - train: epoch 0116, iter [02000, 05004], lr: 0.039999, loss: 2.3370
2022-03-03 20:05:04 - train: epoch 0116, iter [02100, 05004], lr: 0.039999, loss: 2.3286
2022-03-03 20:05:35 - train: epoch 0116, iter [02200, 05004], lr: 0.039999, loss: 2.1721
2022-03-03 20:06:08 - train: epoch 0116, iter [02300, 05004], lr: 0.039999, loss: 2.3908
2022-03-03 20:06:41 - train: epoch 0116, iter [02400, 05004], lr: 0.039999, loss: 2.2752
2022-03-03 20:07:12 - train: epoch 0116, iter [02500, 05004], lr: 0.039999, loss: 2.0706
2022-03-03 20:07:45 - train: epoch 0116, iter [02600, 05004], lr: 0.039999, loss: 2.2348
2022-03-03 20:08:17 - train: epoch 0116, iter [02700, 05004], lr: 0.039999, loss: 2.3185
2022-03-03 20:08:49 - train: epoch 0116, iter [02800, 05004], lr: 0.039999, loss: 2.5695
2022-03-03 20:09:22 - train: epoch 0116, iter [02900, 05004], lr: 0.039999, loss: 2.4964
2022-03-03 20:09:55 - train: epoch 0116, iter [03000, 05004], lr: 0.039999, loss: 2.4331
2022-03-03 20:10:26 - train: epoch 0116, iter [03100, 05004], lr: 0.039999, loss: 2.5103
2022-03-03 20:10:58 - train: epoch 0116, iter [03200, 05004], lr: 0.039999, loss: 2.4186
2022-03-03 20:11:32 - train: epoch 0116, iter [03300, 05004], lr: 0.039999, loss: 2.3721
2022-03-03 20:12:02 - train: epoch 0116, iter [03400, 05004], lr: 0.039999, loss: 2.3953
2022-03-03 20:12:35 - train: epoch 0116, iter [03500, 05004], lr: 0.039999, loss: 2.3612
2022-03-03 20:13:08 - train: epoch 0116, iter [03600, 05004], lr: 0.039999, loss: 2.3004
2022-03-03 20:13:40 - train: epoch 0116, iter [03700, 05004], lr: 0.039999, loss: 2.3131
2022-03-03 20:14:13 - train: epoch 0116, iter [03800, 05004], lr: 0.039999, loss: 2.0861
2022-03-03 20:14:45 - train: epoch 0116, iter [03900, 05004], lr: 0.039999, loss: 2.1756
2022-03-03 20:15:18 - train: epoch 0116, iter [04000, 05004], lr: 0.039999, loss: 2.4034
2022-03-03 20:15:50 - train: epoch 0116, iter [04100, 05004], lr: 0.039999, loss: 2.4222
2022-03-03 20:16:22 - train: epoch 0116, iter [04200, 05004], lr: 0.039999, loss: 2.3301
2022-03-03 20:16:54 - train: epoch 0116, iter [04300, 05004], lr: 0.039999, loss: 2.3169
2022-03-03 20:17:27 - train: epoch 0116, iter [04400, 05004], lr: 0.039999, loss: 2.3403
2022-03-03 20:17:59 - train: epoch 0116, iter [04500, 05004], lr: 0.039999, loss: 2.2307
2022-03-03 20:18:31 - train: epoch 0116, iter [04600, 05004], lr: 0.039999, loss: 2.6380
2022-03-03 20:19:04 - train: epoch 0116, iter [04700, 05004], lr: 0.039999, loss: 2.3538
2022-03-03 20:19:37 - train: epoch 0116, iter [04800, 05004], lr: 0.039999, loss: 2.0136
2022-03-03 20:20:09 - train: epoch 0116, iter [04900, 05004], lr: 0.039999, loss: 2.2921
2022-03-03 20:20:40 - train: epoch 0116, iter [05000, 05004], lr: 0.039999, loss: 2.4578
2022-03-03 20:20:41 - train: epoch 116, train_loss: 2.3269
2022-03-03 20:21:51 - eval: epoch: 116, acc1: 62.320%, acc5: 85.252%, test_loss: 1.5501, per_image_load_time: 0.471ms, per_image_inference_time: 0.376ms
2022-03-03 20:21:52 - until epoch: 116, best_acc1: 62.320%
2022-03-03 20:21:52 - epoch 117 lr: 0.0392107901616097
2022-03-03 20:22:29 - train: epoch 0117, iter [00100, 05004], lr: 0.039211, loss: 2.4079
2022-03-03 20:23:02 - train: epoch 0117, iter [00200, 05004], lr: 0.039211, loss: 2.0650
2022-03-03 20:23:34 - train: epoch 0117, iter [00300, 05004], lr: 0.039211, loss: 2.3449
2022-03-03 20:24:08 - train: epoch 0117, iter [00400, 05004], lr: 0.039211, loss: 1.9474
2022-03-03 20:24:40 - train: epoch 0117, iter [00500, 05004], lr: 0.039211, loss: 2.3988
2022-03-03 20:25:13 - train: epoch 0117, iter [00600, 05004], lr: 0.039211, loss: 2.4189
2022-03-03 20:25:44 - train: epoch 0117, iter [00700, 05004], lr: 0.039211, loss: 2.3786
2022-03-03 20:26:17 - train: epoch 0117, iter [00800, 05004], lr: 0.039211, loss: 2.0807
2022-03-03 20:26:49 - train: epoch 0117, iter [00900, 05004], lr: 0.039211, loss: 2.5249
2022-03-03 20:27:21 - train: epoch 0117, iter [01000, 05004], lr: 0.039211, loss: 2.1859
2022-03-03 20:27:54 - train: epoch 0117, iter [01100, 05004], lr: 0.039211, loss: 2.1930
2022-03-03 20:28:26 - train: epoch 0117, iter [01200, 05004], lr: 0.039211, loss: 2.3984
2022-03-03 20:28:59 - train: epoch 0117, iter [01300, 05004], lr: 0.039211, loss: 2.2052
2022-03-03 20:29:31 - train: epoch 0117, iter [01400, 05004], lr: 0.039211, loss: 2.4419
2022-03-03 20:30:03 - train: epoch 0117, iter [01500, 05004], lr: 0.039211, loss: 2.1803
2022-03-03 20:30:35 - train: epoch 0117, iter [01600, 05004], lr: 0.039211, loss: 2.0955
2022-03-03 20:31:08 - train: epoch 0117, iter [01700, 05004], lr: 0.039211, loss: 2.2258
2022-03-03 20:31:40 - train: epoch 0117, iter [01800, 05004], lr: 0.039211, loss: 2.4026
2022-03-03 20:32:13 - train: epoch 0117, iter [01900, 05004], lr: 0.039211, loss: 1.9897
2022-03-03 20:32:44 - train: epoch 0117, iter [02000, 05004], lr: 0.039211, loss: 2.6125
2022-03-03 20:33:17 - train: epoch 0117, iter [02100, 05004], lr: 0.039211, loss: 2.3826
2022-03-03 20:33:49 - train: epoch 0117, iter [02200, 05004], lr: 0.039211, loss: 2.2873
2022-03-03 20:34:22 - train: epoch 0117, iter [02300, 05004], lr: 0.039211, loss: 2.1772
2022-03-03 20:34:54 - train: epoch 0117, iter [02400, 05004], lr: 0.039211, loss: 2.0287
2022-03-03 20:35:27 - train: epoch 0117, iter [02500, 05004], lr: 0.039211, loss: 2.2601
2022-03-03 20:35:59 - train: epoch 0117, iter [02600, 05004], lr: 0.039211, loss: 2.1692
2022-03-03 20:36:32 - train: epoch 0117, iter [02700, 05004], lr: 0.039211, loss: 2.2344
2022-03-03 20:37:04 - train: epoch 0117, iter [02800, 05004], lr: 0.039211, loss: 2.1423
2022-03-03 20:37:36 - train: epoch 0117, iter [02900, 05004], lr: 0.039211, loss: 2.2797
2022-03-03 20:38:09 - train: epoch 0117, iter [03000, 05004], lr: 0.039211, loss: 2.3414
2022-03-03 20:38:41 - train: epoch 0117, iter [03100, 05004], lr: 0.039211, loss: 2.1407
2022-03-03 20:39:13 - train: epoch 0117, iter [03200, 05004], lr: 0.039211, loss: 2.3964
2022-03-03 20:39:45 - train: epoch 0117, iter [03300, 05004], lr: 0.039211, loss: 2.1568
2022-03-03 20:40:18 - train: epoch 0117, iter [03400, 05004], lr: 0.039211, loss: 2.4051
2022-03-03 20:40:50 - train: epoch 0117, iter [03500, 05004], lr: 0.039211, loss: 2.3725
2022-03-03 20:41:22 - train: epoch 0117, iter [03600, 05004], lr: 0.039211, loss: 2.6697
2022-03-03 20:41:55 - train: epoch 0117, iter [03700, 05004], lr: 0.039211, loss: 2.3594
2022-03-03 20:42:28 - train: epoch 0117, iter [03800, 05004], lr: 0.039211, loss: 2.1580
2022-03-03 20:43:00 - train: epoch 0117, iter [03900, 05004], lr: 0.039211, loss: 2.2976
2022-03-03 20:43:33 - train: epoch 0117, iter [04000, 05004], lr: 0.039211, loss: 2.4082
2022-03-03 20:44:05 - train: epoch 0117, iter [04100, 05004], lr: 0.039211, loss: 2.3779
2022-03-03 20:44:36 - train: epoch 0117, iter [04200, 05004], lr: 0.039211, loss: 2.4618
2022-03-03 20:45:09 - train: epoch 0117, iter [04300, 05004], lr: 0.039211, loss: 2.1312
2022-03-03 20:45:42 - train: epoch 0117, iter [04400, 05004], lr: 0.039211, loss: 2.1342
2022-03-03 20:46:13 - train: epoch 0117, iter [04500, 05004], lr: 0.039211, loss: 2.3800
2022-03-03 20:46:46 - train: epoch 0117, iter [04600, 05004], lr: 0.039211, loss: 2.1507
2022-03-03 20:47:18 - train: epoch 0117, iter [04700, 05004], lr: 0.039211, loss: 2.5390
2022-03-03 20:47:51 - train: epoch 0117, iter [04800, 05004], lr: 0.039211, loss: 2.3259
2022-03-03 20:48:23 - train: epoch 0117, iter [04900, 05004], lr: 0.039211, loss: 2.4419
2022-03-03 20:48:54 - train: epoch 0117, iter [05000, 05004], lr: 0.039211, loss: 2.2814
2022-03-03 20:48:56 - train: epoch 117, train_loss: 2.3142
2022-03-03 20:50:06 - eval: epoch: 117, acc1: 63.124%, acc5: 85.846%, test_loss: 1.5174, per_image_load_time: 0.532ms, per_image_inference_time: 0.403ms
2022-03-03 20:50:07 - until epoch: 117, best_acc1: 63.124%
2022-03-03 20:50:07 - epoch 118 lr: 0.03842566535205286
2022-03-03 20:50:45 - train: epoch 0118, iter [00100, 05004], lr: 0.038426, loss: 2.2393
2022-03-03 20:51:18 - train: epoch 0118, iter [00200, 05004], lr: 0.038426, loss: 2.3277
2022-03-03 20:51:50 - train: epoch 0118, iter [00300, 05004], lr: 0.038426, loss: 2.3678
2022-03-03 20:52:23 - train: epoch 0118, iter [00400, 05004], lr: 0.038426, loss: 2.2431
2022-03-03 20:52:55 - train: epoch 0118, iter [00500, 05004], lr: 0.038426, loss: 2.3282
2022-03-03 20:53:28 - train: epoch 0118, iter [00600, 05004], lr: 0.038426, loss: 2.3104
2022-03-03 20:53:58 - train: epoch 0118, iter [00700, 05004], lr: 0.038426, loss: 2.2688
2022-03-03 20:54:31 - train: epoch 0118, iter [00800, 05004], lr: 0.038426, loss: 2.2157
2022-03-03 20:55:04 - train: epoch 0118, iter [00900, 05004], lr: 0.038426, loss: 2.2142
2022-03-03 20:55:36 - train: epoch 0118, iter [01000, 05004], lr: 0.038426, loss: 2.3452
2022-03-03 20:56:08 - train: epoch 0118, iter [01100, 05004], lr: 0.038426, loss: 2.6331
2022-03-03 20:56:40 - train: epoch 0118, iter [01200, 05004], lr: 0.038426, loss: 2.4663
2022-03-03 20:57:13 - train: epoch 0118, iter [01300, 05004], lr: 0.038426, loss: 2.2298
2022-03-03 20:57:46 - train: epoch 0118, iter [01400, 05004], lr: 0.038426, loss: 2.3199
2022-03-03 20:58:18 - train: epoch 0118, iter [01500, 05004], lr: 0.038426, loss: 2.4792
2022-03-03 20:58:50 - train: epoch 0118, iter [01600, 05004], lr: 0.038426, loss: 2.2135
2022-03-03 20:59:23 - train: epoch 0118, iter [01700, 05004], lr: 0.038426, loss: 2.1376
2022-03-03 20:59:55 - train: epoch 0118, iter [01800, 05004], lr: 0.038426, loss: 2.2780
2022-03-03 21:00:27 - train: epoch 0118, iter [01900, 05004], lr: 0.038426, loss: 2.6057
2022-03-03 21:01:00 - train: epoch 0118, iter [02000, 05004], lr: 0.038426, loss: 2.1191
2022-03-03 21:01:32 - train: epoch 0118, iter [02100, 05004], lr: 0.038426, loss: 2.6061
2022-03-03 21:02:05 - train: epoch 0118, iter [02200, 05004], lr: 0.038426, loss: 2.4780
2022-03-03 21:02:37 - train: epoch 0118, iter [02300, 05004], lr: 0.038426, loss: 2.3037
2022-03-03 21:03:09 - train: epoch 0118, iter [02400, 05004], lr: 0.038426, loss: 2.2307
2022-03-03 21:03:42 - train: epoch 0118, iter [02500, 05004], lr: 0.038426, loss: 2.3309
2022-03-03 21:04:14 - train: epoch 0118, iter [02600, 05004], lr: 0.038426, loss: 2.2307
2022-03-03 21:04:45 - train: epoch 0118, iter [02700, 05004], lr: 0.038426, loss: 2.2879
2022-03-03 21:05:17 - train: epoch 0118, iter [02800, 05004], lr: 0.038426, loss: 2.4658
2022-03-03 21:05:50 - train: epoch 0118, iter [02900, 05004], lr: 0.038426, loss: 2.1622
2022-03-03 21:06:23 - train: epoch 0118, iter [03000, 05004], lr: 0.038426, loss: 2.1310
2022-03-03 21:06:55 - train: epoch 0118, iter [03100, 05004], lr: 0.038426, loss: 2.5410
2022-03-03 21:07:28 - train: epoch 0118, iter [03200, 05004], lr: 0.038426, loss: 2.2593
2022-03-03 21:08:01 - train: epoch 0118, iter [03300, 05004], lr: 0.038426, loss: 2.2909
2022-03-03 21:08:33 - train: epoch 0118, iter [03400, 05004], lr: 0.038426, loss: 2.2006
2022-03-03 21:09:05 - train: epoch 0118, iter [03500, 05004], lr: 0.038426, loss: 2.4045
2022-03-03 21:09:37 - train: epoch 0118, iter [03600, 05004], lr: 0.038426, loss: 2.3969
2022-03-03 21:10:09 - train: epoch 0118, iter [03700, 05004], lr: 0.038426, loss: 2.1982
2022-03-03 21:10:41 - train: epoch 0118, iter [03800, 05004], lr: 0.038426, loss: 2.1334
2022-03-03 21:11:14 - train: epoch 0118, iter [03900, 05004], lr: 0.038426, loss: 2.2583
2022-03-03 21:11:47 - train: epoch 0118, iter [04000, 05004], lr: 0.038426, loss: 2.4954
2022-03-03 21:12:19 - train: epoch 0118, iter [04100, 05004], lr: 0.038426, loss: 2.4001
2022-03-03 21:12:51 - train: epoch 0118, iter [04200, 05004], lr: 0.038426, loss: 2.2570
2022-03-03 21:13:23 - train: epoch 0118, iter [04300, 05004], lr: 0.038426, loss: 2.2072
2022-03-03 21:13:56 - train: epoch 0118, iter [04400, 05004], lr: 0.038426, loss: 2.5004
2022-03-03 21:14:27 - train: epoch 0118, iter [04500, 05004], lr: 0.038426, loss: 2.2414
2022-03-03 21:15:01 - train: epoch 0118, iter [04600, 05004], lr: 0.038426, loss: 2.5468
2022-03-03 21:15:32 - train: epoch 0118, iter [04700, 05004], lr: 0.038426, loss: 2.3316
2022-03-03 21:16:05 - train: epoch 0118, iter [04800, 05004], lr: 0.038426, loss: 2.1781
2022-03-03 21:16:37 - train: epoch 0118, iter [04900, 05004], lr: 0.038426, loss: 2.3876
2022-03-03 21:17:09 - train: epoch 0118, iter [05000, 05004], lr: 0.038426, loss: 2.3239
2022-03-03 21:17:10 - train: epoch 118, train_loss: 2.3051
2022-03-03 21:18:21 - eval: epoch: 118, acc1: 61.950%, acc5: 84.926%, test_loss: 1.5709, per_image_load_time: 0.541ms, per_image_inference_time: 0.403ms
2022-03-03 21:18:22 - until epoch: 118, best_acc1: 63.124%
2022-03-03 21:18:22 - epoch 119 lr: 0.037643544661716516
2022-03-03 21:18:59 - train: epoch 0119, iter [00100, 05004], lr: 0.037644, loss: 2.6495
2022-03-03 21:19:32 - train: epoch 0119, iter [00200, 05004], lr: 0.037644, loss: 2.3556
2022-03-03 21:20:03 - train: epoch 0119, iter [00300, 05004], lr: 0.037644, loss: 2.3781
2022-03-03 21:20:36 - train: epoch 0119, iter [00400, 05004], lr: 0.037644, loss: 2.3112
2022-03-03 21:21:08 - train: epoch 0119, iter [00500, 05004], lr: 0.037644, loss: 2.2892
2022-03-03 21:21:41 - train: epoch 0119, iter [00600, 05004], lr: 0.037644, loss: 2.3154
2022-03-03 21:22:13 - train: epoch 0119, iter [00700, 05004], lr: 0.037644, loss: 2.3524
2022-03-03 21:22:45 - train: epoch 0119, iter [00800, 05004], lr: 0.037644, loss: 2.3637
2022-03-03 21:23:18 - train: epoch 0119, iter [00900, 05004], lr: 0.037644, loss: 2.1454
2022-03-03 21:23:50 - train: epoch 0119, iter [01000, 05004], lr: 0.037644, loss: 2.1319
2022-03-03 21:24:23 - train: epoch 0119, iter [01100, 05004], lr: 0.037644, loss: 2.2051
2022-03-03 21:24:56 - train: epoch 0119, iter [01200, 05004], lr: 0.037644, loss: 2.1277
2022-03-03 21:25:28 - train: epoch 0119, iter [01300, 05004], lr: 0.037644, loss: 2.4907
2022-03-03 21:26:00 - train: epoch 0119, iter [01400, 05004], lr: 0.037644, loss: 2.4083
2022-03-03 21:26:32 - train: epoch 0119, iter [01500, 05004], lr: 0.037644, loss: 2.3774
2022-03-03 21:27:05 - train: epoch 0119, iter [01600, 05004], lr: 0.037644, loss: 2.2832
2022-03-03 21:27:37 - train: epoch 0119, iter [01700, 05004], lr: 0.037644, loss: 2.1315
2022-03-03 21:28:10 - train: epoch 0119, iter [01800, 05004], lr: 0.037644, loss: 2.6494
2022-03-03 21:28:41 - train: epoch 0119, iter [01900, 05004], lr: 0.037644, loss: 2.3519
2022-03-03 21:29:15 - train: epoch 0119, iter [02000, 05004], lr: 0.037644, loss: 2.1565
2022-03-03 21:29:47 - train: epoch 0119, iter [02100, 05004], lr: 0.037644, loss: 2.5457
2022-03-03 21:30:20 - train: epoch 0119, iter [02200, 05004], lr: 0.037644, loss: 2.2839
2022-03-03 21:30:52 - train: epoch 0119, iter [02300, 05004], lr: 0.037644, loss: 2.0660
2022-03-03 21:31:24 - train: epoch 0119, iter [02400, 05004], lr: 0.037644, loss: 2.3614
2022-03-03 21:31:56 - train: epoch 0119, iter [02500, 05004], lr: 0.037644, loss: 2.4928
2022-03-03 21:32:28 - train: epoch 0119, iter [02600, 05004], lr: 0.037644, loss: 2.4591
2022-03-03 21:33:01 - train: epoch 0119, iter [02700, 05004], lr: 0.037644, loss: 2.1681
2022-03-03 21:33:34 - train: epoch 0119, iter [02800, 05004], lr: 0.037644, loss: 2.4910
2022-03-03 21:34:06 - train: epoch 0119, iter [02900, 05004], lr: 0.037644, loss: 2.3511
2022-03-03 21:34:39 - train: epoch 0119, iter [03000, 05004], lr: 0.037644, loss: 1.9419
2022-03-03 21:35:11 - train: epoch 0119, iter [03100, 05004], lr: 0.037644, loss: 2.1943
2022-03-03 21:35:44 - train: epoch 0119, iter [03200, 05004], lr: 0.037644, loss: 2.0257
2022-03-03 21:36:16 - train: epoch 0119, iter [03300, 05004], lr: 0.037644, loss: 2.0526
2022-03-03 21:36:48 - train: epoch 0119, iter [03400, 05004], lr: 0.037644, loss: 2.5258
2022-03-03 21:37:20 - train: epoch 0119, iter [03500, 05004], lr: 0.037644, loss: 2.1944
2022-03-03 21:37:53 - train: epoch 0119, iter [03600, 05004], lr: 0.037644, loss: 2.4954
2022-03-03 21:38:25 - train: epoch 0119, iter [03700, 05004], lr: 0.037644, loss: 2.4874
2022-03-03 21:38:58 - train: epoch 0119, iter [03800, 05004], lr: 0.037644, loss: 2.2749
2022-03-03 21:39:31 - train: epoch 0119, iter [03900, 05004], lr: 0.037644, loss: 2.4863
2022-03-03 21:40:03 - train: epoch 0119, iter [04000, 05004], lr: 0.037644, loss: 2.1065
2022-03-03 21:40:35 - train: epoch 0119, iter [04100, 05004], lr: 0.037644, loss: 2.3757
2022-03-03 21:41:08 - train: epoch 0119, iter [04200, 05004], lr: 0.037644, loss: 2.2635
2022-03-03 21:41:41 - train: epoch 0119, iter [04300, 05004], lr: 0.037644, loss: 2.2609
2022-03-03 21:42:12 - train: epoch 0119, iter [04400, 05004], lr: 0.037644, loss: 2.0664
2022-03-03 21:42:44 - train: epoch 0119, iter [04500, 05004], lr: 0.037644, loss: 2.6584
2022-03-03 21:43:17 - train: epoch 0119, iter [04600, 05004], lr: 0.037644, loss: 2.3294
2022-03-03 21:43:49 - train: epoch 0119, iter [04700, 05004], lr: 0.037644, loss: 2.5708
2022-03-03 21:44:22 - train: epoch 0119, iter [04800, 05004], lr: 0.037644, loss: 2.1628
2022-03-03 21:44:53 - train: epoch 0119, iter [04900, 05004], lr: 0.037644, loss: 2.3497
2022-03-03 21:45:25 - train: epoch 0119, iter [05000, 05004], lr: 0.037644, loss: 2.3859
2022-03-03 21:45:26 - train: epoch 119, train_loss: 2.2995
2022-03-03 21:46:37 - eval: epoch: 119, acc1: 61.824%, acc5: 84.744%, test_loss: 1.5857, per_image_load_time: 0.780ms, per_image_inference_time: 0.410ms
2022-03-03 21:46:37 - until epoch: 119, best_acc1: 63.124%
2022-03-03 21:46:37 - epoch 120 lr: 0.036864631090070654
2022-03-03 21:47:14 - train: epoch 0120, iter [00100, 05004], lr: 0.036865, loss: 2.4813
2022-03-03 21:47:47 - train: epoch 0120, iter [00200, 05004], lr: 0.036865, loss: 2.0978
2022-03-03 21:48:19 - train: epoch 0120, iter [00300, 05004], lr: 0.036865, loss: 2.2990
2022-03-03 21:48:52 - train: epoch 0120, iter [00400, 05004], lr: 0.036865, loss: 2.5535
2022-03-03 21:49:24 - train: epoch 0120, iter [00500, 05004], lr: 0.036865, loss: 2.2337
2022-03-03 21:49:56 - train: epoch 0120, iter [00600, 05004], lr: 0.036865, loss: 1.8916
2022-03-03 21:50:28 - train: epoch 0120, iter [00700, 05004], lr: 0.036865, loss: 2.2939
2022-03-03 21:51:01 - train: epoch 0120, iter [00800, 05004], lr: 0.036865, loss: 2.2795
2022-03-03 21:51:32 - train: epoch 0120, iter [00900, 05004], lr: 0.036865, loss: 2.2997
2022-03-03 21:52:05 - train: epoch 0120, iter [01000, 05004], lr: 0.036865, loss: 2.2700
2022-03-03 21:52:37 - train: epoch 0120, iter [01100, 05004], lr: 0.036865, loss: 2.3212
2022-03-03 21:53:09 - train: epoch 0120, iter [01200, 05004], lr: 0.036865, loss: 2.3039
2022-03-03 21:53:42 - train: epoch 0120, iter [01300, 05004], lr: 0.036865, loss: 2.4065
2022-03-03 21:54:14 - train: epoch 0120, iter [01400, 05004], lr: 0.036865, loss: 2.4989
2022-03-03 21:54:47 - train: epoch 0120, iter [01500, 05004], lr: 0.036865, loss: 2.4603
2022-03-03 21:55:19 - train: epoch 0120, iter [01600, 05004], lr: 0.036865, loss: 2.1142
2022-03-03 21:55:52 - train: epoch 0120, iter [01700, 05004], lr: 0.036865, loss: 2.6522
2022-03-03 21:56:24 - train: epoch 0120, iter [01800, 05004], lr: 0.036865, loss: 2.3468
2022-03-03 21:56:57 - train: epoch 0120, iter [01900, 05004], lr: 0.036865, loss: 2.2664
2022-03-03 21:57:29 - train: epoch 0120, iter [02000, 05004], lr: 0.036865, loss: 2.1357
2022-03-03 21:58:02 - train: epoch 0120, iter [02100, 05004], lr: 0.036865, loss: 2.5179
2022-03-03 21:58:33 - train: epoch 0120, iter [02200, 05004], lr: 0.036865, loss: 2.2329
2022-03-03 21:59:06 - train: epoch 0120, iter [02300, 05004], lr: 0.036865, loss: 2.2206
2022-03-03 21:59:37 - train: epoch 0120, iter [02400, 05004], lr: 0.036865, loss: 2.5846
2022-03-03 22:00:10 - train: epoch 0120, iter [02500, 05004], lr: 0.036865, loss: 2.4231
2022-03-03 22:00:42 - train: epoch 0120, iter [02600, 05004], lr: 0.036865, loss: 2.1725
2022-03-03 22:01:14 - train: epoch 0120, iter [02700, 05004], lr: 0.036865, loss: 2.1074
2022-03-03 22:01:47 - train: epoch 0120, iter [02800, 05004], lr: 0.036865, loss: 2.0626
2022-03-03 22:02:20 - train: epoch 0120, iter [02900, 05004], lr: 0.036865, loss: 2.1777
2022-03-03 22:02:52 - train: epoch 0120, iter [03000, 05004], lr: 0.036865, loss: 2.2048
2022-03-03 22:03:24 - train: epoch 0120, iter [03100, 05004], lr: 0.036865, loss: 2.2097
2022-03-03 22:03:56 - train: epoch 0120, iter [03200, 05004], lr: 0.036865, loss: 2.3402
2022-03-03 22:04:28 - train: epoch 0120, iter [03300, 05004], lr: 0.036865, loss: 2.3792
2022-03-03 22:05:00 - train: epoch 0120, iter [03400, 05004], lr: 0.036865, loss: 2.3350
2022-03-03 22:05:33 - train: epoch 0120, iter [03500, 05004], lr: 0.036865, loss: 2.3324
2022-03-03 22:06:05 - train: epoch 0120, iter [03600, 05004], lr: 0.036865, loss: 2.1619
2022-03-03 22:06:38 - train: epoch 0120, iter [03700, 05004], lr: 0.036865, loss: 2.2525
2022-03-03 22:07:10 - train: epoch 0120, iter [03800, 05004], lr: 0.036865, loss: 2.1737
2022-03-03 22:07:43 - train: epoch 0120, iter [03900, 05004], lr: 0.036865, loss: 2.3512
2022-03-03 22:08:15 - train: epoch 0120, iter [04000, 05004], lr: 0.036865, loss: 2.2820
2022-03-03 22:08:47 - train: epoch 0120, iter [04100, 05004], lr: 0.036865, loss: 2.2834
2022-03-03 22:09:19 - train: epoch 0120, iter [04200, 05004], lr: 0.036865, loss: 2.5286
2022-03-03 22:09:51 - train: epoch 0120, iter [04300, 05004], lr: 0.036865, loss: 2.3576
2022-03-03 22:10:23 - train: epoch 0120, iter [04400, 05004], lr: 0.036865, loss: 2.4537
2022-03-03 22:10:55 - train: epoch 0120, iter [04500, 05004], lr: 0.036865, loss: 2.2925
2022-03-03 22:11:29 - train: epoch 0120, iter [04600, 05004], lr: 0.036865, loss: 2.2218
2022-03-03 22:12:01 - train: epoch 0120, iter [04700, 05004], lr: 0.036865, loss: 2.1402
2022-03-03 22:12:34 - train: epoch 0120, iter [04800, 05004], lr: 0.036865, loss: 2.4437
2022-03-03 22:13:05 - train: epoch 0120, iter [04900, 05004], lr: 0.036865, loss: 2.5921
2022-03-03 22:13:37 - train: epoch 0120, iter [05000, 05004], lr: 0.036865, loss: 2.1342
2022-03-03 22:13:38 - train: epoch 120, train_loss: 2.2882
2022-03-03 22:14:49 - eval: epoch: 120, acc1: 61.692%, acc5: 84.940%, test_loss: 1.5778, per_image_load_time: 1.111ms, per_image_inference_time: 0.361ms
2022-03-03 22:14:50 - until epoch: 120, best_acc1: 63.124%
2022-03-03 22:14:50 - epoch 121 lr: 0.03608912680417737
2022-03-03 22:15:27 - train: epoch 0121, iter [00100, 05004], lr: 0.036089, loss: 2.1984
2022-03-03 22:16:00 - train: epoch 0121, iter [00200, 05004], lr: 0.036089, loss: 1.9709
2022-03-03 22:16:32 - train: epoch 0121, iter [00300, 05004], lr: 0.036089, loss: 2.3835
2022-03-03 22:17:06 - train: epoch 0121, iter [00400, 05004], lr: 0.036089, loss: 2.0147
2022-03-03 22:17:38 - train: epoch 0121, iter [00500, 05004], lr: 0.036089, loss: 2.4050
2022-03-03 22:18:10 - train: epoch 0121, iter [00600, 05004], lr: 0.036089, loss: 2.3475
2022-03-03 22:18:43 - train: epoch 0121, iter [00700, 05004], lr: 0.036089, loss: 2.1631
2022-03-03 22:19:16 - train: epoch 0121, iter [00800, 05004], lr: 0.036089, loss: 2.2340
2022-03-03 22:19:48 - train: epoch 0121, iter [00900, 05004], lr: 0.036089, loss: 2.3516
2022-03-03 22:20:20 - train: epoch 0121, iter [01000, 05004], lr: 0.036089, loss: 2.3840
2022-03-03 22:20:52 - train: epoch 0121, iter [01100, 05004], lr: 0.036089, loss: 2.2035
2022-03-03 22:21:25 - train: epoch 0121, iter [01200, 05004], lr: 0.036089, loss: 2.5258
2022-03-03 22:21:58 - train: epoch 0121, iter [01300, 05004], lr: 0.036089, loss: 2.2603
2022-03-03 22:22:31 - train: epoch 0121, iter [01400, 05004], lr: 0.036089, loss: 2.3145
2022-03-03 22:23:03 - train: epoch 0121, iter [01500, 05004], lr: 0.036089, loss: 2.2280
2022-03-03 22:23:36 - train: epoch 0121, iter [01600, 05004], lr: 0.036089, loss: 2.1290
2022-03-03 22:24:08 - train: epoch 0121, iter [01700, 05004], lr: 0.036089, loss: 2.3032
2022-03-03 22:24:41 - train: epoch 0121, iter [01800, 05004], lr: 0.036089, loss: 2.5738
2022-03-03 22:25:12 - train: epoch 0121, iter [01900, 05004], lr: 0.036089, loss: 2.4858
2022-03-03 22:25:45 - train: epoch 0121, iter [02000, 05004], lr: 0.036089, loss: 2.5008
2022-03-03 22:26:18 - train: epoch 0121, iter [02100, 05004], lr: 0.036089, loss: 2.3370
2022-03-03 22:26:50 - train: epoch 0121, iter [02200, 05004], lr: 0.036089, loss: 1.9460
2022-03-03 22:27:23 - train: epoch 0121, iter [02300, 05004], lr: 0.036089, loss: 1.8932
2022-03-03 22:27:55 - train: epoch 0121, iter [02400, 05004], lr: 0.036089, loss: 2.2936
2022-03-03 22:28:28 - train: epoch 0121, iter [02500, 05004], lr: 0.036089, loss: 2.0105
2022-03-03 22:29:00 - train: epoch 0121, iter [02600, 05004], lr: 0.036089, loss: 2.4629
2022-03-03 22:29:33 - train: epoch 0121, iter [02700, 05004], lr: 0.036089, loss: 2.2857
2022-03-03 22:30:05 - train: epoch 0121, iter [02800, 05004], lr: 0.036089, loss: 2.5038
2022-03-03 22:30:38 - train: epoch 0121, iter [02900, 05004], lr: 0.036089, loss: 2.4249
2022-03-03 22:31:09 - train: epoch 0121, iter [03000, 05004], lr: 0.036089, loss: 2.0485
2022-03-03 22:31:43 - train: epoch 0121, iter [03100, 05004], lr: 0.036089, loss: 2.0223
2022-03-03 22:32:14 - train: epoch 0121, iter [03200, 05004], lr: 0.036089, loss: 2.1306
2022-03-03 22:32:48 - train: epoch 0121, iter [03300, 05004], lr: 0.036089, loss: 2.2618
2022-03-03 22:33:20 - train: epoch 0121, iter [03400, 05004], lr: 0.036089, loss: 2.3169
2022-03-03 22:33:52 - train: epoch 0121, iter [03500, 05004], lr: 0.036089, loss: 2.1564
2022-03-03 22:34:24 - train: epoch 0121, iter [03600, 05004], lr: 0.036089, loss: 1.9869
2022-03-03 22:34:57 - train: epoch 0121, iter [03700, 05004], lr: 0.036089, loss: 2.3788
2022-03-03 22:35:30 - train: epoch 0121, iter [03800, 05004], lr: 0.036089, loss: 2.3487
2022-03-03 22:36:02 - train: epoch 0121, iter [03900, 05004], lr: 0.036089, loss: 2.2934
2022-03-03 22:36:34 - train: epoch 0121, iter [04000, 05004], lr: 0.036089, loss: 2.4651
2022-03-03 22:37:07 - train: epoch 0121, iter [04100, 05004], lr: 0.036089, loss: 2.1579
2022-03-03 22:37:39 - train: epoch 0121, iter [04200, 05004], lr: 0.036089, loss: 2.1409
2022-03-03 22:38:12 - train: epoch 0121, iter [04300, 05004], lr: 0.036089, loss: 2.4120
2022-03-03 22:38:45 - train: epoch 0121, iter [04400, 05004], lr: 0.036089, loss: 2.5006
2022-03-03 22:39:16 - train: epoch 0121, iter [04500, 05004], lr: 0.036089, loss: 2.2837
2022-03-03 22:39:49 - train: epoch 0121, iter [04600, 05004], lr: 0.036089, loss: 2.1852
2022-03-03 22:40:22 - train: epoch 0121, iter [04700, 05004], lr: 0.036089, loss: 2.5123
2022-03-03 22:40:54 - train: epoch 0121, iter [04800, 05004], lr: 0.036089, loss: 2.4317
2022-03-03 22:41:27 - train: epoch 0121, iter [04900, 05004], lr: 0.036089, loss: 2.2009
2022-03-03 22:41:58 - train: epoch 0121, iter [05000, 05004], lr: 0.036089, loss: 2.6003
2022-03-03 22:41:59 - train: epoch 121, train_loss: 2.2810
2022-03-03 22:43:10 - eval: epoch: 121, acc1: 62.812%, acc5: 85.546%, test_loss: 1.5259, per_image_load_time: 1.406ms, per_image_inference_time: 0.411ms
2022-03-03 22:43:11 - until epoch: 121, best_acc1: 63.124%
2022-03-03 22:43:11 - epoch 122 lr: 0.03531723308621847
2022-03-03 22:43:48 - train: epoch 0122, iter [00100, 05004], lr: 0.035317, loss: 2.1814
2022-03-03 22:44:21 - train: epoch 0122, iter [00200, 05004], lr: 0.035317, loss: 2.1776
2022-03-03 22:44:54 - train: epoch 0122, iter [00300, 05004], lr: 0.035317, loss: 2.3252
2022-03-03 22:45:25 - train: epoch 0122, iter [00400, 05004], lr: 0.035317, loss: 2.1022
2022-03-03 22:45:58 - train: epoch 0122, iter [00500, 05004], lr: 0.035317, loss: 2.2014
2022-03-03 22:46:30 - train: epoch 0122, iter [00600, 05004], lr: 0.035317, loss: 2.2221
2022-03-03 22:47:02 - train: epoch 0122, iter [00700, 05004], lr: 0.035317, loss: 1.8211
2022-03-03 22:47:34 - train: epoch 0122, iter [00800, 05004], lr: 0.035317, loss: 2.2854
2022-03-03 22:48:07 - train: epoch 0122, iter [00900, 05004], lr: 0.035317, loss: 2.2821
2022-03-03 22:48:39 - train: epoch 0122, iter [01000, 05004], lr: 0.035317, loss: 2.1554
2022-03-03 22:49:12 - train: epoch 0122, iter [01100, 05004], lr: 0.035317, loss: 1.8620
2022-03-03 22:49:45 - train: epoch 0122, iter [01200, 05004], lr: 0.035317, loss: 2.4910
2022-03-03 22:50:17 - train: epoch 0122, iter [01300, 05004], lr: 0.035317, loss: 2.2153
2022-03-03 22:50:49 - train: epoch 0122, iter [01400, 05004], lr: 0.035317, loss: 2.5356
2022-03-03 22:51:21 - train: epoch 0122, iter [01500, 05004], lr: 0.035317, loss: 2.2759
2022-03-03 22:51:52 - train: epoch 0122, iter [01600, 05004], lr: 0.035317, loss: 2.1920
2022-03-03 22:52:25 - train: epoch 0122, iter [01700, 05004], lr: 0.035317, loss: 2.0539
2022-03-03 22:52:58 - train: epoch 0122, iter [01800, 05004], lr: 0.035317, loss: 2.2302
2022-03-03 22:53:30 - train: epoch 0122, iter [01900, 05004], lr: 0.035317, loss: 2.4663
2022-03-03 22:54:03 - train: epoch 0122, iter [02000, 05004], lr: 0.035317, loss: 2.4579
2022-03-03 22:54:36 - train: epoch 0122, iter [02100, 05004], lr: 0.035317, loss: 2.2165
2022-03-03 22:55:08 - train: epoch 0122, iter [02200, 05004], lr: 0.035317, loss: 2.3004
2022-03-03 22:55:41 - train: epoch 0122, iter [02300, 05004], lr: 0.035317, loss: 2.4390
2022-03-03 22:56:13 - train: epoch 0122, iter [02400, 05004], lr: 0.035317, loss: 2.2933
2022-03-03 22:56:44 - train: epoch 0122, iter [02500, 05004], lr: 0.035317, loss: 2.2399
2022-03-03 22:57:18 - train: epoch 0122, iter [02600, 05004], lr: 0.035317, loss: 2.2543
2022-03-03 22:57:50 - train: epoch 0122, iter [02700, 05004], lr: 0.035317, loss: 2.0905
2022-03-03 22:58:23 - train: epoch 0122, iter [02800, 05004], lr: 0.035317, loss: 2.2402
2022-03-03 22:58:56 - train: epoch 0122, iter [02900, 05004], lr: 0.035317, loss: 2.2172
2022-03-03 22:59:29 - train: epoch 0122, iter [03000, 05004], lr: 0.035317, loss: 2.5700
2022-03-03 23:00:01 - train: epoch 0122, iter [03100, 05004], lr: 0.035317, loss: 2.1731
2022-03-03 23:00:33 - train: epoch 0122, iter [03200, 05004], lr: 0.035317, loss: 2.4928
2022-03-03 23:01:06 - train: epoch 0122, iter [03300, 05004], lr: 0.035317, loss: 2.3332
2022-03-03 23:01:37 - train: epoch 0122, iter [03400, 05004], lr: 0.035317, loss: 2.3767
2022-03-03 23:02:11 - train: epoch 0122, iter [03500, 05004], lr: 0.035317, loss: 2.2992
2022-03-03 23:02:44 - train: epoch 0122, iter [03600, 05004], lr: 0.035317, loss: 2.4059
2022-03-03 23:03:18 - train: epoch 0122, iter [03700, 05004], lr: 0.035317, loss: 2.1836
2022-03-03 23:03:50 - train: epoch 0122, iter [03800, 05004], lr: 0.035317, loss: 2.3292
2022-03-03 23:04:23 - train: epoch 0122, iter [03900, 05004], lr: 0.035317, loss: 2.3928
2022-03-03 23:04:56 - train: epoch 0122, iter [04000, 05004], lr: 0.035317, loss: 2.2971
2022-03-03 23:05:28 - train: epoch 0122, iter [04100, 05004], lr: 0.035317, loss: 2.3162
2022-03-03 23:06:01 - train: epoch 0122, iter [04200, 05004], lr: 0.035317, loss: 2.3250
2022-03-03 23:06:33 - train: epoch 0122, iter [04300, 05004], lr: 0.035317, loss: 2.5238
2022-03-03 23:07:05 - train: epoch 0122, iter [04400, 05004], lr: 0.035317, loss: 2.3470
2022-03-03 23:07:38 - train: epoch 0122, iter [04500, 05004], lr: 0.035317, loss: 1.9933
2022-03-03 23:08:10 - train: epoch 0122, iter [04600, 05004], lr: 0.035317, loss: 2.4671
2022-03-03 23:08:43 - train: epoch 0122, iter [04700, 05004], lr: 0.035317, loss: 2.1932
2022-03-03 23:09:16 - train: epoch 0122, iter [04800, 05004], lr: 0.035317, loss: 2.4824
2022-03-03 23:09:49 - train: epoch 0122, iter [04900, 05004], lr: 0.035317, loss: 2.1430
2022-03-03 23:10:21 - train: epoch 0122, iter [05000, 05004], lr: 0.035317, loss: 2.4907
2022-03-03 23:10:22 - train: epoch 122, train_loss: 2.2699
