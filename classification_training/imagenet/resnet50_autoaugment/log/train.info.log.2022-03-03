2022-03-03 23:11:32 - eval: epoch: 122, acc1: 64.068%, acc5: 86.484%, test_loss: 1.4678, per_image_load_time: 0.691ms, per_image_inference_time: 0.387ms
2022-03-03 23:11:33 - until epoch: 122, best_acc1: 64.068%
2022-03-03 23:11:33 - epoch 123 lr: 0.03454915028125263
2022-03-03 23:12:11 - train: epoch 0123, iter [00100, 05004], lr: 0.034549, loss: 2.2937
2022-03-03 23:12:44 - train: epoch 0123, iter [00200, 05004], lr: 0.034549, loss: 2.1978
2022-03-03 23:13:16 - train: epoch 0123, iter [00300, 05004], lr: 0.034549, loss: 2.0749
2022-03-03 23:13:49 - train: epoch 0123, iter [00400, 05004], lr: 0.034549, loss: 2.1901
2022-03-03 23:14:21 - train: epoch 0123, iter [00500, 05004], lr: 0.034549, loss: 2.0570
2022-03-03 23:14:55 - train: epoch 0123, iter [00600, 05004], lr: 0.034549, loss: 2.1712
2022-03-03 23:15:27 - train: epoch 0123, iter [00700, 05004], lr: 0.034549, loss: 2.2185
2022-03-03 23:15:59 - train: epoch 0123, iter [00800, 05004], lr: 0.034549, loss: 2.1790
2022-03-03 23:16:32 - train: epoch 0123, iter [00900, 05004], lr: 0.034549, loss: 2.3560
2022-03-03 23:17:04 - train: epoch 0123, iter [01000, 05004], lr: 0.034549, loss: 2.2868
2022-03-03 23:17:36 - train: epoch 0123, iter [01100, 05004], lr: 0.034549, loss: 2.4942
2022-03-03 23:18:09 - train: epoch 0123, iter [01200, 05004], lr: 0.034549, loss: 2.1937
2022-03-03 23:18:41 - train: epoch 0123, iter [01300, 05004], lr: 0.034549, loss: 2.1386
2022-03-03 23:19:14 - train: epoch 0123, iter [01400, 05004], lr: 0.034549, loss: 2.1644
2022-03-03 23:19:47 - train: epoch 0123, iter [01500, 05004], lr: 0.034549, loss: 2.2257
2022-03-03 23:20:19 - train: epoch 0123, iter [01600, 05004], lr: 0.034549, loss: 2.2981
2022-03-03 23:20:51 - train: epoch 0123, iter [01700, 05004], lr: 0.034549, loss: 2.3776
2022-03-03 23:21:23 - train: epoch 0123, iter [01800, 05004], lr: 0.034549, loss: 2.0590
2022-03-03 23:21:56 - train: epoch 0123, iter [01900, 05004], lr: 0.034549, loss: 2.5374
2022-03-03 23:22:29 - train: epoch 0123, iter [02000, 05004], lr: 0.034549, loss: 2.3803
2022-03-03 23:23:01 - train: epoch 0123, iter [02100, 05004], lr: 0.034549, loss: 2.3424
2022-03-03 23:23:33 - train: epoch 0123, iter [02200, 05004], lr: 0.034549, loss: 2.2652
2022-03-03 23:24:07 - train: epoch 0123, iter [02300, 05004], lr: 0.034549, loss: 2.3112
2022-03-03 23:24:39 - train: epoch 0123, iter [02400, 05004], lr: 0.034549, loss: 2.4287
2022-03-03 23:25:12 - train: epoch 0123, iter [02500, 05004], lr: 0.034549, loss: 2.2222
2022-03-03 23:25:45 - train: epoch 0123, iter [02600, 05004], lr: 0.034549, loss: 2.3204
2022-03-03 23:26:16 - train: epoch 0123, iter [02700, 05004], lr: 0.034549, loss: 2.3469
2022-03-03 23:26:49 - train: epoch 0123, iter [02800, 05004], lr: 0.034549, loss: 2.1376
2022-03-03 23:27:22 - train: epoch 0123, iter [02900, 05004], lr: 0.034549, loss: 2.4072
2022-03-03 23:27:55 - train: epoch 0123, iter [03000, 05004], lr: 0.034549, loss: 2.1410
2022-03-03 23:28:28 - train: epoch 0123, iter [03100, 05004], lr: 0.034549, loss: 2.2862
2022-03-03 23:29:01 - train: epoch 0123, iter [03200, 05004], lr: 0.034549, loss: 2.0681
2022-03-03 23:29:33 - train: epoch 0123, iter [03300, 05004], lr: 0.034549, loss: 2.0702
2022-03-03 23:30:06 - train: epoch 0123, iter [03400, 05004], lr: 0.034549, loss: 2.0986
2022-03-03 23:30:39 - train: epoch 0123, iter [03500, 05004], lr: 0.034549, loss: 2.0531
2022-03-03 23:31:12 - train: epoch 0123, iter [03600, 05004], lr: 0.034549, loss: 2.3959
2022-03-03 23:31:44 - train: epoch 0123, iter [03700, 05004], lr: 0.034549, loss: 2.1927
2022-03-03 23:32:16 - train: epoch 0123, iter [03800, 05004], lr: 0.034549, loss: 2.5338
2022-03-03 23:32:50 - train: epoch 0123, iter [03900, 05004], lr: 0.034549, loss: 2.3161
2022-03-03 23:33:22 - train: epoch 0123, iter [04000, 05004], lr: 0.034549, loss: 2.2876
2022-03-03 23:33:55 - train: epoch 0123, iter [04100, 05004], lr: 0.034549, loss: 2.3805
2022-03-03 23:34:28 - train: epoch 0123, iter [04200, 05004], lr: 0.034549, loss: 2.3414
2022-03-03 23:35:01 - train: epoch 0123, iter [04300, 05004], lr: 0.034549, loss: 2.3739
2022-03-03 23:35:34 - train: epoch 0123, iter [04400, 05004], lr: 0.034549, loss: 2.1297
2022-03-03 23:36:05 - train: epoch 0123, iter [04500, 05004], lr: 0.034549, loss: 2.1431
2022-03-03 23:36:38 - train: epoch 0123, iter [04600, 05004], lr: 0.034549, loss: 2.3014
2022-03-03 23:37:09 - train: epoch 0123, iter [04700, 05004], lr: 0.034549, loss: 2.2307
2022-03-03 23:37:42 - train: epoch 0123, iter [04800, 05004], lr: 0.034549, loss: 2.1929
2022-03-03 23:38:16 - train: epoch 0123, iter [04900, 05004], lr: 0.034549, loss: 2.1083
2022-03-03 23:38:47 - train: epoch 0123, iter [05000, 05004], lr: 0.034549, loss: 2.5493
2022-03-03 23:38:48 - train: epoch 123, train_loss: 2.2602
2022-03-03 23:39:59 - eval: epoch: 123, acc1: 62.924%, acc5: 85.398%, test_loss: 1.5404, per_image_load_time: 0.780ms, per_image_inference_time: 0.390ms
2022-03-03 23:40:00 - until epoch: 123, best_acc1: 64.068%
2022-03-03 23:40:00 - epoch 124 lr: 0.03378507774521587
2022-03-03 23:40:38 - train: epoch 0124, iter [00100, 05004], lr: 0.033785, loss: 2.0586
2022-03-03 23:41:09 - train: epoch 0124, iter [00200, 05004], lr: 0.033785, loss: 2.3281
2022-03-03 23:41:42 - train: epoch 0124, iter [00300, 05004], lr: 0.033785, loss: 2.2392
2022-03-03 23:42:15 - train: epoch 0124, iter [00400, 05004], lr: 0.033785, loss: 2.0509
2022-03-03 23:42:48 - train: epoch 0124, iter [00500, 05004], lr: 0.033785, loss: 2.0776
2022-03-03 23:43:21 - train: epoch 0124, iter [00600, 05004], lr: 0.033785, loss: 2.4376
2022-03-03 23:43:54 - train: epoch 0124, iter [00700, 05004], lr: 0.033785, loss: 2.1062
2022-03-03 23:44:26 - train: epoch 0124, iter [00800, 05004], lr: 0.033785, loss: 2.3573
2022-03-03 23:44:59 - train: epoch 0124, iter [00900, 05004], lr: 0.033785, loss: 2.3612
2022-03-03 23:45:31 - train: epoch 0124, iter [01000, 05004], lr: 0.033785, loss: 2.3221
2022-03-03 23:46:04 - train: epoch 0124, iter [01100, 05004], lr: 0.033785, loss: 2.0441
2022-03-03 23:46:36 - train: epoch 0124, iter [01200, 05004], lr: 0.033785, loss: 1.9951
2022-03-03 23:47:10 - train: epoch 0124, iter [01300, 05004], lr: 0.033785, loss: 2.2257
2022-03-03 23:47:42 - train: epoch 0124, iter [01400, 05004], lr: 0.033785, loss: 2.1985
2022-03-03 23:48:15 - train: epoch 0124, iter [01500, 05004], lr: 0.033785, loss: 2.2560
2022-03-03 23:48:49 - train: epoch 0124, iter [01600, 05004], lr: 0.033785, loss: 2.2479
2022-03-03 23:49:22 - train: epoch 0124, iter [01700, 05004], lr: 0.033785, loss: 2.3180
2022-03-03 23:49:56 - train: epoch 0124, iter [01800, 05004], lr: 0.033785, loss: 2.2481
2022-03-03 23:50:28 - train: epoch 0124, iter [01900, 05004], lr: 0.033785, loss: 2.3998
2022-03-03 23:51:00 - train: epoch 0124, iter [02000, 05004], lr: 0.033785, loss: 2.2516
2022-03-03 23:51:32 - train: epoch 0124, iter [02100, 05004], lr: 0.033785, loss: 2.3150
2022-03-03 23:52:04 - train: epoch 0124, iter [02200, 05004], lr: 0.033785, loss: 2.0919
2022-03-03 23:52:38 - train: epoch 0124, iter [02300, 05004], lr: 0.033785, loss: 2.3537
2022-03-03 23:53:10 - train: epoch 0124, iter [02400, 05004], lr: 0.033785, loss: 2.0450
2022-03-03 23:53:43 - train: epoch 0124, iter [02500, 05004], lr: 0.033785, loss: 2.0863
2022-03-03 23:54:16 - train: epoch 0124, iter [02600, 05004], lr: 0.033785, loss: 2.1834
2022-03-03 23:54:49 - train: epoch 0124, iter [02700, 05004], lr: 0.033785, loss: 2.4471
2022-03-03 23:55:22 - train: epoch 0124, iter [02800, 05004], lr: 0.033785, loss: 2.3390
2022-03-03 23:55:53 - train: epoch 0124, iter [02900, 05004], lr: 0.033785, loss: 2.1805
2022-03-03 23:56:27 - train: epoch 0124, iter [03000, 05004], lr: 0.033785, loss: 2.1446
2022-03-03 23:56:59 - train: epoch 0124, iter [03100, 05004], lr: 0.033785, loss: 2.1825
2022-03-03 23:57:31 - train: epoch 0124, iter [03200, 05004], lr: 0.033785, loss: 2.0918
2022-03-03 23:58:05 - train: epoch 0124, iter [03300, 05004], lr: 0.033785, loss: 2.1072
2022-03-03 23:58:38 - train: epoch 0124, iter [03400, 05004], lr: 0.033785, loss: 2.0496
2022-03-03 23:59:11 - train: epoch 0124, iter [03500, 05004], lr: 0.033785, loss: 2.4855
2022-03-03 23:59:43 - train: epoch 0124, iter [03600, 05004], lr: 0.033785, loss: 2.2380
2022-03-04 00:00:17 - train: epoch 0124, iter [03700, 05004], lr: 0.033785, loss: 2.3838
2022-03-04 00:00:49 - train: epoch 0124, iter [03800, 05004], lr: 0.033785, loss: 2.2207
2022-03-04 00:01:21 - train: epoch 0124, iter [03900, 05004], lr: 0.033785, loss: 2.2882
2022-03-04 00:01:54 - train: epoch 0124, iter [04000, 05004], lr: 0.033785, loss: 2.2506
2022-03-04 00:02:26 - train: epoch 0124, iter [04100, 05004], lr: 0.033785, loss: 2.3536
2022-03-04 00:02:59 - train: epoch 0124, iter [04200, 05004], lr: 0.033785, loss: 2.3801
2022-03-04 00:03:32 - train: epoch 0124, iter [04300, 05004], lr: 0.033785, loss: 2.1750
2022-03-04 00:04:06 - train: epoch 0124, iter [04400, 05004], lr: 0.033785, loss: 2.4462
2022-03-04 00:04:38 - train: epoch 0124, iter [04500, 05004], lr: 0.033785, loss: 2.1473
2022-03-04 00:05:11 - train: epoch 0124, iter [04600, 05004], lr: 0.033785, loss: 2.2976
2022-03-04 00:05:45 - train: epoch 0124, iter [04700, 05004], lr: 0.033785, loss: 2.3752
2022-03-04 00:06:17 - train: epoch 0124, iter [04800, 05004], lr: 0.033785, loss: 2.1369
2022-03-04 00:06:50 - train: epoch 0124, iter [04900, 05004], lr: 0.033785, loss: 1.8855
2022-03-04 00:07:25 - train: epoch 0124, iter [05000, 05004], lr: 0.033785, loss: 2.3238
2022-03-04 00:07:26 - train: epoch 124, train_loss: 2.2541
2022-03-04 00:08:37 - eval: epoch: 124, acc1: 64.480%, acc5: 86.746%, test_loss: 1.4461, per_image_load_time: 0.596ms, per_image_inference_time: 0.393ms
2022-03-04 00:08:38 - until epoch: 124, best_acc1: 64.480%
2022-03-04 00:08:38 - epoch 125 lr: 0.033025213793178645
2022-03-04 00:09:16 - train: epoch 0125, iter [00100, 05004], lr: 0.033025, loss: 2.1166
2022-03-04 00:09:48 - train: epoch 0125, iter [00200, 05004], lr: 0.033025, loss: 2.4534
2022-03-04 00:10:21 - train: epoch 0125, iter [00300, 05004], lr: 0.033025, loss: 2.2610
2022-03-04 00:10:53 - train: epoch 0125, iter [00400, 05004], lr: 0.033025, loss: 2.0813
2022-03-04 00:11:26 - train: epoch 0125, iter [00500, 05004], lr: 0.033025, loss: 2.1756
2022-03-04 00:11:57 - train: epoch 0125, iter [00600, 05004], lr: 0.033025, loss: 2.1229
2022-03-04 00:12:30 - train: epoch 0125, iter [00700, 05004], lr: 0.033025, loss: 2.0135
2022-03-04 00:13:03 - train: epoch 0125, iter [00800, 05004], lr: 0.033025, loss: 2.3103
2022-03-04 00:13:35 - train: epoch 0125, iter [00900, 05004], lr: 0.033025, loss: 2.0777
2022-03-04 00:14:07 - train: epoch 0125, iter [01000, 05004], lr: 0.033025, loss: 2.1252
2022-03-04 00:14:40 - train: epoch 0125, iter [01100, 05004], lr: 0.033025, loss: 2.0061
2022-03-04 00:15:13 - train: epoch 0125, iter [01200, 05004], lr: 0.033025, loss: 2.2275
2022-03-04 00:15:46 - train: epoch 0125, iter [01300, 05004], lr: 0.033025, loss: 2.2766
2022-03-04 00:16:18 - train: epoch 0125, iter [01400, 05004], lr: 0.033025, loss: 2.3905
2022-03-04 00:16:50 - train: epoch 0125, iter [01500, 05004], lr: 0.033025, loss: 2.5064
2022-03-04 00:17:22 - train: epoch 0125, iter [01600, 05004], lr: 0.033025, loss: 2.1353
2022-03-04 00:17:56 - train: epoch 0125, iter [01700, 05004], lr: 0.033025, loss: 2.2805
2022-03-04 00:18:27 - train: epoch 0125, iter [01800, 05004], lr: 0.033025, loss: 2.3481
2022-03-04 00:19:00 - train: epoch 0125, iter [01900, 05004], lr: 0.033025, loss: 2.2322
2022-03-04 00:19:32 - train: epoch 0125, iter [02000, 05004], lr: 0.033025, loss: 2.3170
2022-03-04 00:20:04 - train: epoch 0125, iter [02100, 05004], lr: 0.033025, loss: 2.4049
2022-03-04 00:20:37 - train: epoch 0125, iter [02200, 05004], lr: 0.033025, loss: 2.3190
2022-03-04 00:21:11 - train: epoch 0125, iter [02300, 05004], lr: 0.033025, loss: 1.9456
2022-03-04 00:21:42 - train: epoch 0125, iter [02400, 05004], lr: 0.033025, loss: 2.4377
2022-03-04 00:22:15 - train: epoch 0125, iter [02500, 05004], lr: 0.033025, loss: 2.1488
2022-03-04 00:22:47 - train: epoch 0125, iter [02600, 05004], lr: 0.033025, loss: 2.3235
2022-03-04 00:23:20 - train: epoch 0125, iter [02700, 05004], lr: 0.033025, loss: 2.0431
2022-03-04 00:23:53 - train: epoch 0125, iter [02800, 05004], lr: 0.033025, loss: 2.4431
2022-03-04 00:24:25 - train: epoch 0125, iter [02900, 05004], lr: 0.033025, loss: 2.0793
2022-03-04 00:24:58 - train: epoch 0125, iter [03000, 05004], lr: 0.033025, loss: 2.1296
2022-03-04 00:25:30 - train: epoch 0125, iter [03100, 05004], lr: 0.033025, loss: 2.3614
2022-03-04 00:26:03 - train: epoch 0125, iter [03200, 05004], lr: 0.033025, loss: 2.2186
2022-03-04 00:26:35 - train: epoch 0125, iter [03300, 05004], lr: 0.033025, loss: 2.1277
2022-03-04 00:27:07 - train: epoch 0125, iter [03400, 05004], lr: 0.033025, loss: 2.2052
2022-03-04 00:27:38 - train: epoch 0125, iter [03500, 05004], lr: 0.033025, loss: 2.1410
2022-03-04 00:28:11 - train: epoch 0125, iter [03600, 05004], lr: 0.033025, loss: 2.0256
2022-03-04 00:28:42 - train: epoch 0125, iter [03700, 05004], lr: 0.033025, loss: 2.4742
2022-03-04 00:29:15 - train: epoch 0125, iter [03800, 05004], lr: 0.033025, loss: 2.1461
2022-03-04 00:29:46 - train: epoch 0125, iter [03900, 05004], lr: 0.033025, loss: 2.2349
2022-03-04 00:30:18 - train: epoch 0125, iter [04000, 05004], lr: 0.033025, loss: 2.1836
2022-03-04 00:30:50 - train: epoch 0125, iter [04100, 05004], lr: 0.033025, loss: 2.2775
2022-03-04 00:31:23 - train: epoch 0125, iter [04200, 05004], lr: 0.033025, loss: 2.4125
2022-03-04 00:31:55 - train: epoch 0125, iter [04300, 05004], lr: 0.033025, loss: 2.0720
2022-03-04 00:32:27 - train: epoch 0125, iter [04400, 05004], lr: 0.033025, loss: 2.1221
2022-03-04 00:32:58 - train: epoch 0125, iter [04500, 05004], lr: 0.033025, loss: 2.4623
2022-03-04 00:33:31 - train: epoch 0125, iter [04600, 05004], lr: 0.033025, loss: 2.0371
2022-03-04 00:34:02 - train: epoch 0125, iter [04700, 05004], lr: 0.033025, loss: 2.2180
2022-03-04 00:34:34 - train: epoch 0125, iter [04800, 05004], lr: 0.033025, loss: 2.4283
2022-03-04 00:35:05 - train: epoch 0125, iter [04900, 05004], lr: 0.033025, loss: 2.3840
2022-03-04 00:35:36 - train: epoch 0125, iter [05000, 05004], lr: 0.033025, loss: 2.0352
2022-03-04 00:35:37 - train: epoch 125, train_loss: 2.2421
2022-03-04 00:36:47 - eval: epoch: 125, acc1: 62.928%, acc5: 85.494%, test_loss: 1.5383, per_image_load_time: 0.545ms, per_image_inference_time: 0.372ms
2022-03-04 00:36:47 - until epoch: 125, best_acc1: 64.480%
2022-03-04 00:36:47 - epoch 126 lr: 0.03226975564787322
2022-03-04 00:37:25 - train: epoch 0126, iter [00100, 05004], lr: 0.032270, loss: 2.0949
2022-03-04 00:37:57 - train: epoch 0126, iter [00200, 05004], lr: 0.032270, loss: 2.2588
2022-03-04 00:38:28 - train: epoch 0126, iter [00300, 05004], lr: 0.032270, loss: 2.1633
2022-03-04 00:39:01 - train: epoch 0126, iter [00400, 05004], lr: 0.032270, loss: 2.1036
2022-03-04 00:39:33 - train: epoch 0126, iter [00500, 05004], lr: 0.032270, loss: 2.0054
2022-03-04 00:40:05 - train: epoch 0126, iter [00600, 05004], lr: 0.032270, loss: 2.3335
2022-03-04 00:40:37 - train: epoch 0126, iter [00700, 05004], lr: 0.032270, loss: 2.1455
2022-03-04 00:41:09 - train: epoch 0126, iter [00800, 05004], lr: 0.032270, loss: 1.9988
2022-03-04 00:41:40 - train: epoch 0126, iter [00900, 05004], lr: 0.032270, loss: 2.3230
2022-03-04 00:42:13 - train: epoch 0126, iter [01000, 05004], lr: 0.032270, loss: 1.9952
2022-03-04 00:42:44 - train: epoch 0126, iter [01100, 05004], lr: 0.032270, loss: 2.5928
2022-03-04 00:43:16 - train: epoch 0126, iter [01200, 05004], lr: 0.032270, loss: 2.1852
2022-03-04 00:43:49 - train: epoch 0126, iter [01300, 05004], lr: 0.032270, loss: 2.2327
2022-03-04 00:44:22 - train: epoch 0126, iter [01400, 05004], lr: 0.032270, loss: 2.2732
2022-03-04 00:44:52 - train: epoch 0126, iter [01500, 05004], lr: 0.032270, loss: 2.2138
2022-03-04 00:45:25 - train: epoch 0126, iter [01600, 05004], lr: 0.032270, loss: 2.0953
2022-03-04 00:45:59 - train: epoch 0126, iter [01700, 05004], lr: 0.032270, loss: 2.1212
2022-03-04 00:46:29 - train: epoch 0126, iter [01800, 05004], lr: 0.032270, loss: 2.3322
2022-03-04 00:47:02 - train: epoch 0126, iter [01900, 05004], lr: 0.032270, loss: 2.0834
2022-03-04 00:47:33 - train: epoch 0126, iter [02000, 05004], lr: 0.032270, loss: 2.1915
2022-03-04 00:48:06 - train: epoch 0126, iter [02100, 05004], lr: 0.032270, loss: 2.5251
2022-03-04 00:48:37 - train: epoch 0126, iter [02200, 05004], lr: 0.032270, loss: 2.3995
2022-03-04 00:49:10 - train: epoch 0126, iter [02300, 05004], lr: 0.032270, loss: 2.7066
2022-03-04 00:49:42 - train: epoch 0126, iter [02400, 05004], lr: 0.032270, loss: 2.3649
2022-03-04 00:50:14 - train: epoch 0126, iter [02500, 05004], lr: 0.032270, loss: 2.6253
2022-03-04 00:50:46 - train: epoch 0126, iter [02600, 05004], lr: 0.032270, loss: 2.4537
2022-03-04 00:51:19 - train: epoch 0126, iter [02700, 05004], lr: 0.032270, loss: 2.1881
2022-03-04 00:51:53 - train: epoch 0126, iter [02800, 05004], lr: 0.032270, loss: 2.1613
2022-03-04 00:52:24 - train: epoch 0126, iter [02900, 05004], lr: 0.032270, loss: 2.2529
2022-03-04 00:52:56 - train: epoch 0126, iter [03000, 05004], lr: 0.032270, loss: 2.1058
2022-03-04 00:53:28 - train: epoch 0126, iter [03100, 05004], lr: 0.032270, loss: 2.3907
2022-03-04 00:54:00 - train: epoch 0126, iter [03200, 05004], lr: 0.032270, loss: 2.3690
2022-03-04 00:54:31 - train: epoch 0126, iter [03300, 05004], lr: 0.032270, loss: 2.2594
2022-03-04 00:55:04 - train: epoch 0126, iter [03400, 05004], lr: 0.032270, loss: 2.3307
2022-03-04 00:55:35 - train: epoch 0126, iter [03500, 05004], lr: 0.032270, loss: 2.0605
2022-03-04 00:56:07 - train: epoch 0126, iter [03600, 05004], lr: 0.032270, loss: 2.2920
2022-03-04 00:56:39 - train: epoch 0126, iter [03700, 05004], lr: 0.032270, loss: 2.2126
2022-03-04 00:57:10 - train: epoch 0126, iter [03800, 05004], lr: 0.032270, loss: 2.0939
2022-03-04 00:57:44 - train: epoch 0126, iter [03900, 05004], lr: 0.032270, loss: 2.0981
2022-03-04 00:58:16 - train: epoch 0126, iter [04000, 05004], lr: 0.032270, loss: 2.1857
2022-03-04 00:58:47 - train: epoch 0126, iter [04100, 05004], lr: 0.032270, loss: 2.2509
2022-03-04 00:59:19 - train: epoch 0126, iter [04200, 05004], lr: 0.032270, loss: 2.2790
2022-03-04 00:59:51 - train: epoch 0126, iter [04300, 05004], lr: 0.032270, loss: 1.9810
2022-03-04 01:00:22 - train: epoch 0126, iter [04400, 05004], lr: 0.032270, loss: 2.1439
2022-03-04 01:00:54 - train: epoch 0126, iter [04500, 05004], lr: 0.032270, loss: 2.0677
2022-03-04 01:01:25 - train: epoch 0126, iter [04600, 05004], lr: 0.032270, loss: 2.2771
2022-03-04 01:01:58 - train: epoch 0126, iter [04700, 05004], lr: 0.032270, loss: 2.1327
2022-03-04 01:02:29 - train: epoch 0126, iter [04800, 05004], lr: 0.032270, loss: 2.4486
2022-03-04 01:03:01 - train: epoch 0126, iter [04900, 05004], lr: 0.032270, loss: 2.4454
2022-03-04 01:03:32 - train: epoch 0126, iter [05000, 05004], lr: 0.032270, loss: 2.0417
2022-03-04 01:03:33 - train: epoch 126, train_loss: 2.2358
2022-03-04 01:04:43 - eval: epoch: 126, acc1: 64.456%, acc5: 86.652%, test_loss: 1.4521, per_image_load_time: 1.137ms, per_image_inference_time: 0.337ms
2022-03-04 01:04:44 - until epoch: 126, best_acc1: 64.480%
2022-03-04 01:04:44 - epoch 127 lr: 0.031518899388504454
2022-03-04 01:05:20 - train: epoch 0127, iter [00100, 05004], lr: 0.031519, loss: 2.0991
2022-03-04 01:05:53 - train: epoch 0127, iter [00200, 05004], lr: 0.031519, loss: 1.9604
2022-03-04 01:06:25 - train: epoch 0127, iter [00300, 05004], lr: 0.031519, loss: 2.1718
2022-03-04 01:06:56 - train: epoch 0127, iter [00400, 05004], lr: 0.031519, loss: 2.1772
2022-03-04 01:07:28 - train: epoch 0127, iter [00500, 05004], lr: 0.031519, loss: 2.2989
2022-03-04 01:08:01 - train: epoch 0127, iter [00600, 05004], lr: 0.031519, loss: 2.3583
2022-03-04 01:08:31 - train: epoch 0127, iter [00700, 05004], lr: 0.031519, loss: 2.1431
2022-03-04 01:09:04 - train: epoch 0127, iter [00800, 05004], lr: 0.031519, loss: 2.1975
2022-03-04 01:09:36 - train: epoch 0127, iter [00900, 05004], lr: 0.031519, loss: 2.0298
2022-03-04 01:10:08 - train: epoch 0127, iter [01000, 05004], lr: 0.031519, loss: 2.3087
2022-03-04 01:10:40 - train: epoch 0127, iter [01100, 05004], lr: 0.031519, loss: 2.0461
2022-03-04 01:11:12 - train: epoch 0127, iter [01200, 05004], lr: 0.031519, loss: 2.1148
2022-03-04 01:11:43 - train: epoch 0127, iter [01300, 05004], lr: 0.031519, loss: 2.1338
2022-03-04 01:12:15 - train: epoch 0127, iter [01400, 05004], lr: 0.031519, loss: 2.1833
2022-03-04 01:12:47 - train: epoch 0127, iter [01500, 05004], lr: 0.031519, loss: 2.5152
2022-03-04 01:13:19 - train: epoch 0127, iter [01600, 05004], lr: 0.031519, loss: 2.0843
2022-03-04 01:13:50 - train: epoch 0127, iter [01700, 05004], lr: 0.031519, loss: 2.3573
2022-03-04 01:14:22 - train: epoch 0127, iter [01800, 05004], lr: 0.031519, loss: 2.0627
2022-03-04 01:14:54 - train: epoch 0127, iter [01900, 05004], lr: 0.031519, loss: 2.3862
2022-03-04 01:15:26 - train: epoch 0127, iter [02000, 05004], lr: 0.031519, loss: 1.9585
2022-03-04 01:15:58 - train: epoch 0127, iter [02100, 05004], lr: 0.031519, loss: 2.2922
2022-03-04 01:16:30 - train: epoch 0127, iter [02200, 05004], lr: 0.031519, loss: 2.2118
2022-03-04 01:17:01 - train: epoch 0127, iter [02300, 05004], lr: 0.031519, loss: 2.1443
2022-03-04 01:17:32 - train: epoch 0127, iter [02400, 05004], lr: 0.031519, loss: 2.2971
2022-03-04 01:18:04 - train: epoch 0127, iter [02500, 05004], lr: 0.031519, loss: 2.2357
2022-03-04 01:18:36 - train: epoch 0127, iter [02600, 05004], lr: 0.031519, loss: 2.4415
2022-03-04 01:19:08 - train: epoch 0127, iter [02700, 05004], lr: 0.031519, loss: 2.0514
2022-03-04 01:19:40 - train: epoch 0127, iter [02800, 05004], lr: 0.031519, loss: 2.3287
2022-03-04 01:20:11 - train: epoch 0127, iter [02900, 05004], lr: 0.031519, loss: 2.1871
2022-03-04 01:20:44 - train: epoch 0127, iter [03000, 05004], lr: 0.031519, loss: 2.3701
2022-03-04 01:21:15 - train: epoch 0127, iter [03100, 05004], lr: 0.031519, loss: 2.1151
2022-03-04 01:21:47 - train: epoch 0127, iter [03200, 05004], lr: 0.031519, loss: 2.0256
2022-03-04 01:22:19 - train: epoch 0127, iter [03300, 05004], lr: 0.031519, loss: 2.3277
2022-03-04 01:22:50 - train: epoch 0127, iter [03400, 05004], lr: 0.031519, loss: 2.3188
2022-03-04 01:23:22 - train: epoch 0127, iter [03500, 05004], lr: 0.031519, loss: 2.3536
2022-03-04 01:23:54 - train: epoch 0127, iter [03600, 05004], lr: 0.031519, loss: 2.2452
2022-03-04 01:24:26 - train: epoch 0127, iter [03700, 05004], lr: 0.031519, loss: 2.5845
2022-03-04 01:24:57 - train: epoch 0127, iter [03800, 05004], lr: 0.031519, loss: 2.0737
2022-03-04 01:25:30 - train: epoch 0127, iter [03900, 05004], lr: 0.031519, loss: 2.4668
2022-03-04 01:26:01 - train: epoch 0127, iter [04000, 05004], lr: 0.031519, loss: 1.9440
2022-03-04 01:26:33 - train: epoch 0127, iter [04100, 05004], lr: 0.031519, loss: 2.3939
2022-03-04 01:27:05 - train: epoch 0127, iter [04200, 05004], lr: 0.031519, loss: 2.2834
2022-03-04 01:27:37 - train: epoch 0127, iter [04300, 05004], lr: 0.031519, loss: 1.9825
2022-03-04 01:28:09 - train: epoch 0127, iter [04400, 05004], lr: 0.031519, loss: 2.0909
2022-03-04 01:28:41 - train: epoch 0127, iter [04500, 05004], lr: 0.031519, loss: 2.4729
2022-03-04 01:29:13 - train: epoch 0127, iter [04600, 05004], lr: 0.031519, loss: 2.3573
2022-03-04 01:29:45 - train: epoch 0127, iter [04700, 05004], lr: 0.031519, loss: 2.1109
2022-03-04 01:30:17 - train: epoch 0127, iter [04800, 05004], lr: 0.031519, loss: 2.1191
2022-03-04 01:30:49 - train: epoch 0127, iter [04900, 05004], lr: 0.031519, loss: 2.1711
2022-03-04 01:31:19 - train: epoch 0127, iter [05000, 05004], lr: 0.031519, loss: 2.4325
2022-03-04 01:31:20 - train: epoch 127, train_loss: 2.2239
2022-03-04 01:32:30 - eval: epoch: 127, acc1: 64.834%, acc5: 86.816%, test_loss: 1.4408, per_image_load_time: 0.764ms, per_image_inference_time: 0.358ms
2022-03-04 01:32:31 - until epoch: 127, best_acc1: 64.834%
2022-03-04 01:32:31 - epoch 128 lr: 0.030772839899857463
2022-03-04 01:33:08 - train: epoch 0128, iter [00100, 05004], lr: 0.030773, loss: 1.9912
2022-03-04 01:33:39 - train: epoch 0128, iter [00200, 05004], lr: 0.030773, loss: 2.1976
2022-03-04 01:34:11 - train: epoch 0128, iter [00300, 05004], lr: 0.030773, loss: 2.2166
2022-03-04 01:34:43 - train: epoch 0128, iter [00400, 05004], lr: 0.030773, loss: 2.1913
2022-03-04 01:35:14 - train: epoch 0128, iter [00500, 05004], lr: 0.030773, loss: 2.3806
2022-03-04 01:35:47 - train: epoch 0128, iter [00600, 05004], lr: 0.030773, loss: 1.9866
2022-03-04 01:36:19 - train: epoch 0128, iter [00700, 05004], lr: 0.030773, loss: 2.3526
2022-03-04 01:36:50 - train: epoch 0128, iter [00800, 05004], lr: 0.030773, loss: 2.1316
2022-03-04 01:37:22 - train: epoch 0128, iter [00900, 05004], lr: 0.030773, loss: 2.2059
2022-03-04 01:37:54 - train: epoch 0128, iter [01000, 05004], lr: 0.030773, loss: 2.4065
2022-03-04 01:38:26 - train: epoch 0128, iter [01100, 05004], lr: 0.030773, loss: 2.3077
2022-03-04 01:38:57 - train: epoch 0128, iter [01200, 05004], lr: 0.030773, loss: 2.3079
2022-03-04 01:39:29 - train: epoch 0128, iter [01300, 05004], lr: 0.030773, loss: 2.1618
2022-03-04 01:40:02 - train: epoch 0128, iter [01400, 05004], lr: 0.030773, loss: 2.2275
2022-03-04 01:40:33 - train: epoch 0128, iter [01500, 05004], lr: 0.030773, loss: 1.9302
2022-03-04 01:41:05 - train: epoch 0128, iter [01600, 05004], lr: 0.030773, loss: 2.0585
2022-03-04 01:41:37 - train: epoch 0128, iter [01700, 05004], lr: 0.030773, loss: 2.1411
2022-03-04 01:42:09 - train: epoch 0128, iter [01800, 05004], lr: 0.030773, loss: 2.3549
2022-03-04 01:42:40 - train: epoch 0128, iter [01900, 05004], lr: 0.030773, loss: 2.1799
2022-03-04 01:43:13 - train: epoch 0128, iter [02000, 05004], lr: 0.030773, loss: 2.0430
2022-03-04 01:43:45 - train: epoch 0128, iter [02100, 05004], lr: 0.030773, loss: 2.4529
2022-03-04 01:44:17 - train: epoch 0128, iter [02200, 05004], lr: 0.030773, loss: 2.2853
2022-03-04 01:44:49 - train: epoch 0128, iter [02300, 05004], lr: 0.030773, loss: 1.9760
2022-03-04 01:45:20 - train: epoch 0128, iter [02400, 05004], lr: 0.030773, loss: 2.1978
2022-03-04 01:45:53 - train: epoch 0128, iter [02500, 05004], lr: 0.030773, loss: 2.1329
2022-03-04 01:46:25 - train: epoch 0128, iter [02600, 05004], lr: 0.030773, loss: 2.2025
2022-03-04 01:46:57 - train: epoch 0128, iter [02700, 05004], lr: 0.030773, loss: 2.0907
2022-03-04 01:47:29 - train: epoch 0128, iter [02800, 05004], lr: 0.030773, loss: 2.2702
2022-03-04 01:48:01 - train: epoch 0128, iter [02900, 05004], lr: 0.030773, loss: 2.2045
2022-03-04 01:48:33 - train: epoch 0128, iter [03000, 05004], lr: 0.030773, loss: 2.0862
2022-03-04 01:49:05 - train: epoch 0128, iter [03100, 05004], lr: 0.030773, loss: 2.2156
2022-03-04 01:49:37 - train: epoch 0128, iter [03200, 05004], lr: 0.030773, loss: 1.9796
2022-03-04 01:50:09 - train: epoch 0128, iter [03300, 05004], lr: 0.030773, loss: 2.2208
2022-03-04 01:50:41 - train: epoch 0128, iter [03400, 05004], lr: 0.030773, loss: 2.2148
2022-03-04 01:51:13 - train: epoch 0128, iter [03500, 05004], lr: 0.030773, loss: 2.0235
2022-03-04 01:51:44 - train: epoch 0128, iter [03600, 05004], lr: 0.030773, loss: 2.1998
2022-03-04 01:52:17 - train: epoch 0128, iter [03700, 05004], lr: 0.030773, loss: 2.1289
2022-03-04 01:52:48 - train: epoch 0128, iter [03800, 05004], lr: 0.030773, loss: 2.2351
2022-03-04 01:53:20 - train: epoch 0128, iter [03900, 05004], lr: 0.030773, loss: 2.0023
2022-03-04 01:53:52 - train: epoch 0128, iter [04000, 05004], lr: 0.030773, loss: 2.1530
2022-03-04 01:54:24 - train: epoch 0128, iter [04100, 05004], lr: 0.030773, loss: 2.2496
2022-03-04 01:54:56 - train: epoch 0128, iter [04200, 05004], lr: 0.030773, loss: 2.3090
2022-03-04 01:55:28 - train: epoch 0128, iter [04300, 05004], lr: 0.030773, loss: 2.3040
2022-03-04 01:56:00 - train: epoch 0128, iter [04400, 05004], lr: 0.030773, loss: 2.1752
2022-03-04 01:56:31 - train: epoch 0128, iter [04500, 05004], lr: 0.030773, loss: 2.1085
2022-03-04 01:57:03 - train: epoch 0128, iter [04600, 05004], lr: 0.030773, loss: 2.0543
2022-03-04 01:57:35 - train: epoch 0128, iter [04700, 05004], lr: 0.030773, loss: 2.0854
2022-03-04 01:58:07 - train: epoch 0128, iter [04800, 05004], lr: 0.030773, loss: 1.9931
2022-03-04 01:58:38 - train: epoch 0128, iter [04900, 05004], lr: 0.030773, loss: 2.2648
2022-03-04 01:59:10 - train: epoch 0128, iter [05000, 05004], lr: 0.030773, loss: 2.2207
2022-03-04 01:59:11 - train: epoch 128, train_loss: 2.2140
2022-03-04 02:00:21 - eval: epoch: 128, acc1: 65.038%, acc5: 86.952%, test_loss: 1.4355, per_image_load_time: 0.612ms, per_image_inference_time: 0.381ms
2022-03-04 02:00:22 - until epoch: 128, best_acc1: 65.038%
2022-03-04 02:00:22 - epoch 129 lr: 0.030031770821715233
2022-03-04 02:00:59 - train: epoch 0129, iter [00100, 05004], lr: 0.030032, loss: 2.1138
2022-03-04 02:01:32 - train: epoch 0129, iter [00200, 05004], lr: 0.030032, loss: 2.6161
2022-03-04 02:02:03 - train: epoch 0129, iter [00300, 05004], lr: 0.030032, loss: 2.1731
2022-03-04 02:02:35 - train: epoch 0129, iter [00400, 05004], lr: 0.030032, loss: 2.4530
2022-03-04 02:03:07 - train: epoch 0129, iter [00500, 05004], lr: 0.030032, loss: 1.9475
2022-03-04 02:03:39 - train: epoch 0129, iter [00600, 05004], lr: 0.030032, loss: 1.9633
2022-03-04 02:04:10 - train: epoch 0129, iter [00700, 05004], lr: 0.030032, loss: 2.2021
2022-03-04 02:04:42 - train: epoch 0129, iter [00800, 05004], lr: 0.030032, loss: 2.4374
2022-03-04 02:05:15 - train: epoch 0129, iter [00900, 05004], lr: 0.030032, loss: 2.0805
2022-03-04 02:05:47 - train: epoch 0129, iter [01000, 05004], lr: 0.030032, loss: 2.2019
2022-03-04 02:06:19 - train: epoch 0129, iter [01100, 05004], lr: 0.030032, loss: 1.9267
2022-03-04 02:06:50 - train: epoch 0129, iter [01200, 05004], lr: 0.030032, loss: 2.6335
2022-03-04 02:07:22 - train: epoch 0129, iter [01300, 05004], lr: 0.030032, loss: 2.0894
2022-03-04 02:07:54 - train: epoch 0129, iter [01400, 05004], lr: 0.030032, loss: 2.0240
2022-03-04 02:08:26 - train: epoch 0129, iter [01500, 05004], lr: 0.030032, loss: 2.2519
2022-03-04 02:08:58 - train: epoch 0129, iter [01600, 05004], lr: 0.030032, loss: 2.5653
2022-03-04 02:09:30 - train: epoch 0129, iter [01700, 05004], lr: 0.030032, loss: 2.1517
2022-03-04 02:10:02 - train: epoch 0129, iter [01800, 05004], lr: 0.030032, loss: 2.1194
2022-03-04 02:10:33 - train: epoch 0129, iter [01900, 05004], lr: 0.030032, loss: 2.3044
2022-03-04 02:11:06 - train: epoch 0129, iter [02000, 05004], lr: 0.030032, loss: 2.0318
2022-03-04 02:11:37 - train: epoch 0129, iter [02100, 05004], lr: 0.030032, loss: 2.1481
2022-03-04 02:12:09 - train: epoch 0129, iter [02200, 05004], lr: 0.030032, loss: 2.0358
2022-03-04 02:12:40 - train: epoch 0129, iter [02300, 05004], lr: 0.030032, loss: 2.1782
2022-03-04 02:13:13 - train: epoch 0129, iter [02400, 05004], lr: 0.030032, loss: 2.0587
2022-03-04 02:13:45 - train: epoch 0129, iter [02500, 05004], lr: 0.030032, loss: 2.2824
2022-03-04 02:14:16 - train: epoch 0129, iter [02600, 05004], lr: 0.030032, loss: 2.2954
2022-03-04 02:14:48 - train: epoch 0129, iter [02700, 05004], lr: 0.030032, loss: 2.3251
2022-03-04 02:15:20 - train: epoch 0129, iter [02800, 05004], lr: 0.030032, loss: 2.1139
2022-03-04 02:15:52 - train: epoch 0129, iter [02900, 05004], lr: 0.030032, loss: 2.2327
2022-03-04 02:16:24 - train: epoch 0129, iter [03000, 05004], lr: 0.030032, loss: 2.2600
2022-03-04 02:16:56 - train: epoch 0129, iter [03100, 05004], lr: 0.030032, loss: 2.2461
2022-03-04 02:17:28 - train: epoch 0129, iter [03200, 05004], lr: 0.030032, loss: 2.1398
2022-03-04 02:18:00 - train: epoch 0129, iter [03300, 05004], lr: 0.030032, loss: 2.1222
2022-03-04 02:18:32 - train: epoch 0129, iter [03400, 05004], lr: 0.030032, loss: 2.4876
2022-03-04 02:19:04 - train: epoch 0129, iter [03500, 05004], lr: 0.030032, loss: 2.1445
2022-03-04 02:19:35 - train: epoch 0129, iter [03600, 05004], lr: 0.030032, loss: 2.1907
2022-03-04 02:20:08 - train: epoch 0129, iter [03700, 05004], lr: 0.030032, loss: 2.1010
2022-03-04 02:20:39 - train: epoch 0129, iter [03800, 05004], lr: 0.030032, loss: 2.2867
2022-03-04 02:21:10 - train: epoch 0129, iter [03900, 05004], lr: 0.030032, loss: 2.3465
2022-03-04 02:21:44 - train: epoch 0129, iter [04000, 05004], lr: 0.030032, loss: 2.2257
2022-03-04 02:22:15 - train: epoch 0129, iter [04100, 05004], lr: 0.030032, loss: 2.1476
2022-03-04 02:22:47 - train: epoch 0129, iter [04200, 05004], lr: 0.030032, loss: 2.2140
2022-03-04 02:23:19 - train: epoch 0129, iter [04300, 05004], lr: 0.030032, loss: 2.3038
2022-03-04 02:23:50 - train: epoch 0129, iter [04400, 05004], lr: 0.030032, loss: 2.3297
2022-03-04 02:24:22 - train: epoch 0129, iter [04500, 05004], lr: 0.030032, loss: 2.4340
2022-03-04 02:24:54 - train: epoch 0129, iter [04600, 05004], lr: 0.030032, loss: 2.2459
2022-03-04 02:25:26 - train: epoch 0129, iter [04700, 05004], lr: 0.030032, loss: 2.3807
2022-03-04 02:25:58 - train: epoch 0129, iter [04800, 05004], lr: 0.030032, loss: 2.4327
2022-03-04 02:26:29 - train: epoch 0129, iter [04900, 05004], lr: 0.030032, loss: 2.0925
2022-03-04 02:27:01 - train: epoch 0129, iter [05000, 05004], lr: 0.030032, loss: 2.2311
2022-03-04 02:27:02 - train: epoch 129, train_loss: 2.2044
2022-03-04 02:28:12 - eval: epoch: 129, acc1: 64.436%, acc5: 86.606%, test_loss: 1.4492, per_image_load_time: 1.685ms, per_image_inference_time: 0.351ms
2022-03-04 02:28:13 - until epoch: 129, best_acc1: 65.038%
2022-03-04 02:28:13 - epoch 130 lr: 0.029295884498599413
2022-03-04 02:28:49 - train: epoch 0130, iter [00100, 05004], lr: 0.029296, loss: 2.0262
2022-03-04 02:29:22 - train: epoch 0130, iter [00200, 05004], lr: 0.029296, loss: 2.0516
2022-03-04 02:29:53 - train: epoch 0130, iter [00300, 05004], lr: 0.029296, loss: 2.3328
2022-03-04 02:30:25 - train: epoch 0130, iter [00400, 05004], lr: 0.029296, loss: 1.9868
2022-03-04 02:30:58 - train: epoch 0130, iter [00500, 05004], lr: 0.029296, loss: 2.1052
2022-03-04 02:31:30 - train: epoch 0130, iter [00600, 05004], lr: 0.029296, loss: 2.0097
2022-03-04 02:32:01 - train: epoch 0130, iter [00700, 05004], lr: 0.029296, loss: 2.2954
2022-03-04 02:32:33 - train: epoch 0130, iter [00800, 05004], lr: 0.029296, loss: 2.1088
2022-03-04 02:33:05 - train: epoch 0130, iter [00900, 05004], lr: 0.029296, loss: 2.1403
2022-03-04 02:33:37 - train: epoch 0130, iter [01000, 05004], lr: 0.029296, loss: 2.4525
2022-03-04 02:34:08 - train: epoch 0130, iter [01100, 05004], lr: 0.029296, loss: 2.2225
2022-03-04 02:34:40 - train: epoch 0130, iter [01200, 05004], lr: 0.029296, loss: 2.1351
2022-03-04 02:35:13 - train: epoch 0130, iter [01300, 05004], lr: 0.029296, loss: 2.2217
2022-03-04 02:35:44 - train: epoch 0130, iter [01400, 05004], lr: 0.029296, loss: 2.3513
2022-03-04 02:36:16 - train: epoch 0130, iter [01500, 05004], lr: 0.029296, loss: 2.1183
2022-03-04 02:36:48 - train: epoch 0130, iter [01600, 05004], lr: 0.029296, loss: 2.2501
2022-03-04 02:37:21 - train: epoch 0130, iter [01700, 05004], lr: 0.029296, loss: 1.9732
2022-03-04 02:37:52 - train: epoch 0130, iter [01800, 05004], lr: 0.029296, loss: 2.2414
2022-03-04 02:38:25 - train: epoch 0130, iter [01900, 05004], lr: 0.029296, loss: 2.0451
2022-03-04 02:38:56 - train: epoch 0130, iter [02000, 05004], lr: 0.029296, loss: 2.1670
2022-03-04 02:39:28 - train: epoch 0130, iter [02100, 05004], lr: 0.029296, loss: 2.3036
2022-03-04 02:40:00 - train: epoch 0130, iter [02200, 05004], lr: 0.029296, loss: 2.1774
2022-03-04 02:40:32 - train: epoch 0130, iter [02300, 05004], lr: 0.029296, loss: 2.3548
2022-03-04 02:41:04 - train: epoch 0130, iter [02400, 05004], lr: 0.029296, loss: 2.2093
2022-03-04 02:41:35 - train: epoch 0130, iter [02500, 05004], lr: 0.029296, loss: 2.3003
2022-03-04 02:42:07 - train: epoch 0130, iter [02600, 05004], lr: 0.029296, loss: 2.2012
2022-03-04 02:42:39 - train: epoch 0130, iter [02700, 05004], lr: 0.029296, loss: 2.1911
2022-03-04 02:43:12 - train: epoch 0130, iter [02800, 05004], lr: 0.029296, loss: 2.3625
2022-03-04 02:43:43 - train: epoch 0130, iter [02900, 05004], lr: 0.029296, loss: 2.3201
2022-03-04 02:44:15 - train: epoch 0130, iter [03000, 05004], lr: 0.029296, loss: 2.0668
2022-03-04 02:44:47 - train: epoch 0130, iter [03100, 05004], lr: 0.029296, loss: 2.0994
2022-03-04 02:45:19 - train: epoch 0130, iter [03200, 05004], lr: 0.029296, loss: 2.1439
2022-03-04 02:45:51 - train: epoch 0130, iter [03300, 05004], lr: 0.029296, loss: 2.2514
2022-03-04 02:46:23 - train: epoch 0130, iter [03400, 05004], lr: 0.029296, loss: 2.2532
2022-03-04 02:46:55 - train: epoch 0130, iter [03500, 05004], lr: 0.029296, loss: 2.0775
2022-03-04 02:47:27 - train: epoch 0130, iter [03600, 05004], lr: 0.029296, loss: 2.2383
2022-03-04 02:47:58 - train: epoch 0130, iter [03700, 05004], lr: 0.029296, loss: 2.2198
2022-03-04 02:48:30 - train: epoch 0130, iter [03800, 05004], lr: 0.029296, loss: 2.3380
2022-03-04 02:49:01 - train: epoch 0130, iter [03900, 05004], lr: 0.029296, loss: 2.2147
2022-03-04 02:49:34 - train: epoch 0130, iter [04000, 05004], lr: 0.029296, loss: 2.3070
2022-03-04 02:50:05 - train: epoch 0130, iter [04100, 05004], lr: 0.029296, loss: 2.1844
2022-03-04 02:50:37 - train: epoch 0130, iter [04200, 05004], lr: 0.029296, loss: 2.3137
2022-03-04 02:51:09 - train: epoch 0130, iter [04300, 05004], lr: 0.029296, loss: 2.0160
2022-03-04 02:51:40 - train: epoch 0130, iter [04400, 05004], lr: 0.029296, loss: 2.2667
2022-03-04 02:52:13 - train: epoch 0130, iter [04500, 05004], lr: 0.029296, loss: 2.3082
2022-03-04 02:52:44 - train: epoch 0130, iter [04600, 05004], lr: 0.029296, loss: 2.4520
2022-03-04 02:53:16 - train: epoch 0130, iter [04700, 05004], lr: 0.029296, loss: 2.1220
2022-03-04 02:53:48 - train: epoch 0130, iter [04800, 05004], lr: 0.029296, loss: 2.1220
2022-03-04 02:54:20 - train: epoch 0130, iter [04900, 05004], lr: 0.029296, loss: 2.0812
2022-03-04 02:54:51 - train: epoch 0130, iter [05000, 05004], lr: 0.029296, loss: 2.2788
2022-03-04 02:54:52 - train: epoch 130, train_loss: 2.1977
2022-03-04 02:56:03 - eval: epoch: 130, acc1: 63.544%, acc5: 85.806%, test_loss: 1.5056, per_image_load_time: 2.000ms, per_image_inference_time: 0.365ms
2022-03-04 02:56:03 - until epoch: 130, best_acc1: 65.038%
2022-03-04 02:56:03 - epoch 131 lr: 0.028565371929847285
2022-03-04 02:56:40 - train: epoch 0131, iter [00100, 05004], lr: 0.028565, loss: 1.9605
2022-03-04 02:57:11 - train: epoch 0131, iter [00200, 05004], lr: 0.028565, loss: 2.4290
2022-03-04 02:57:43 - train: epoch 0131, iter [00300, 05004], lr: 0.028565, loss: 2.2271
2022-03-04 02:58:15 - train: epoch 0131, iter [00400, 05004], lr: 0.028565, loss: 2.3028
2022-03-04 02:58:48 - train: epoch 0131, iter [00500, 05004], lr: 0.028565, loss: 2.2680
2022-03-04 02:59:20 - train: epoch 0131, iter [00600, 05004], lr: 0.028565, loss: 2.2889
2022-03-04 02:59:52 - train: epoch 0131, iter [00700, 05004], lr: 0.028565, loss: 2.2592
2022-03-04 03:00:23 - train: epoch 0131, iter [00800, 05004], lr: 0.028565, loss: 1.9807
2022-03-04 03:00:56 - train: epoch 0131, iter [00900, 05004], lr: 0.028565, loss: 2.1506
2022-03-04 03:01:28 - train: epoch 0131, iter [01000, 05004], lr: 0.028565, loss: 1.7463
2022-03-04 03:02:00 - train: epoch 0131, iter [01100, 05004], lr: 0.028565, loss: 2.3398
2022-03-04 03:02:32 - train: epoch 0131, iter [01200, 05004], lr: 0.028565, loss: 1.9892
2022-03-04 03:03:04 - train: epoch 0131, iter [01300, 05004], lr: 0.028565, loss: 2.2013
2022-03-04 03:03:35 - train: epoch 0131, iter [01400, 05004], lr: 0.028565, loss: 2.1588
2022-03-04 03:04:07 - train: epoch 0131, iter [01500, 05004], lr: 0.028565, loss: 2.1960
2022-03-04 03:04:39 - train: epoch 0131, iter [01600, 05004], lr: 0.028565, loss: 2.4896
2022-03-04 03:05:11 - train: epoch 0131, iter [01700, 05004], lr: 0.028565, loss: 2.3621
2022-03-04 03:05:43 - train: epoch 0131, iter [01800, 05004], lr: 0.028565, loss: 2.1773
2022-03-04 03:06:15 - train: epoch 0131, iter [01900, 05004], lr: 0.028565, loss: 2.2275
2022-03-04 03:06:47 - train: epoch 0131, iter [02000, 05004], lr: 0.028565, loss: 2.4883
2022-03-04 03:07:19 - train: epoch 0131, iter [02100, 05004], lr: 0.028565, loss: 2.5891
2022-03-04 03:07:51 - train: epoch 0131, iter [02200, 05004], lr: 0.028565, loss: 2.1081
2022-03-04 03:08:24 - train: epoch 0131, iter [02300, 05004], lr: 0.028565, loss: 2.3817
2022-03-04 03:08:55 - train: epoch 0131, iter [02400, 05004], lr: 0.028565, loss: 2.1336
2022-03-04 03:09:27 - train: epoch 0131, iter [02500, 05004], lr: 0.028565, loss: 2.0979
2022-03-04 03:09:59 - train: epoch 0131, iter [02600, 05004], lr: 0.028565, loss: 2.2088
2022-03-04 03:10:31 - train: epoch 0131, iter [02700, 05004], lr: 0.028565, loss: 2.1971
2022-03-04 03:11:03 - train: epoch 0131, iter [02800, 05004], lr: 0.028565, loss: 1.9643
2022-03-04 03:11:35 - train: epoch 0131, iter [02900, 05004], lr: 0.028565, loss: 2.4249
2022-03-04 03:12:08 - train: epoch 0131, iter [03000, 05004], lr: 0.028565, loss: 2.0720
2022-03-04 03:12:40 - train: epoch 0131, iter [03100, 05004], lr: 0.028565, loss: 2.0683
2022-03-04 03:13:12 - train: epoch 0131, iter [03200, 05004], lr: 0.028565, loss: 2.0589
2022-03-04 03:13:44 - train: epoch 0131, iter [03300, 05004], lr: 0.028565, loss: 2.2981
2022-03-04 03:14:16 - train: epoch 0131, iter [03400, 05004], lr: 0.028565, loss: 2.1710
2022-03-04 03:14:48 - train: epoch 0131, iter [03500, 05004], lr: 0.028565, loss: 1.9405
2022-03-04 03:15:20 - train: epoch 0131, iter [03600, 05004], lr: 0.028565, loss: 2.3611
2022-03-04 03:15:52 - train: epoch 0131, iter [03700, 05004], lr: 0.028565, loss: 2.2361
2022-03-04 03:16:24 - train: epoch 0131, iter [03800, 05004], lr: 0.028565, loss: 2.1337
2022-03-04 03:16:56 - train: epoch 0131, iter [03900, 05004], lr: 0.028565, loss: 2.0421
2022-03-04 03:17:28 - train: epoch 0131, iter [04000, 05004], lr: 0.028565, loss: 2.3478
2022-03-04 03:18:00 - train: epoch 0131, iter [04100, 05004], lr: 0.028565, loss: 2.1257
2022-03-04 03:18:31 - train: epoch 0131, iter [04200, 05004], lr: 0.028565, loss: 2.1744
2022-03-04 03:19:04 - train: epoch 0131, iter [04300, 05004], lr: 0.028565, loss: 2.1325
2022-03-04 03:19:36 - train: epoch 0131, iter [04400, 05004], lr: 0.028565, loss: 2.3854
2022-03-04 03:20:09 - train: epoch 0131, iter [04500, 05004], lr: 0.028565, loss: 1.9987
2022-03-04 03:20:40 - train: epoch 0131, iter [04600, 05004], lr: 0.028565, loss: 2.1425
2022-03-04 03:21:11 - train: epoch 0131, iter [04700, 05004], lr: 0.028565, loss: 2.1081
2022-03-04 03:21:44 - train: epoch 0131, iter [04800, 05004], lr: 0.028565, loss: 2.1788
2022-03-04 03:22:15 - train: epoch 0131, iter [04900, 05004], lr: 0.028565, loss: 2.2958
2022-03-04 03:22:46 - train: epoch 0131, iter [05000, 05004], lr: 0.028565, loss: 2.0771
2022-03-04 03:22:47 - train: epoch 131, train_loss: 2.1876
2022-03-04 03:23:57 - eval: epoch: 131, acc1: 65.522%, acc5: 87.088%, test_loss: 1.4069, per_image_load_time: 1.617ms, per_image_inference_time: 0.363ms
2022-03-04 03:23:58 - until epoch: 131, best_acc1: 65.522%
2022-03-04 03:23:58 - epoch 132 lr: 0.02784042272003794
2022-03-04 03:24:36 - train: epoch 0132, iter [00100, 05004], lr: 0.027840, loss: 2.0003
2022-03-04 03:25:08 - train: epoch 0132, iter [00200, 05004], lr: 0.027840, loss: 2.0821
2022-03-04 03:25:40 - train: epoch 0132, iter [00300, 05004], lr: 0.027840, loss: 2.3968
2022-03-04 03:26:12 - train: epoch 0132, iter [00400, 05004], lr: 0.027840, loss: 2.2310
2022-03-04 03:26:45 - train: epoch 0132, iter [00500, 05004], lr: 0.027840, loss: 2.3300
2022-03-04 03:27:16 - train: epoch 0132, iter [00600, 05004], lr: 0.027840, loss: 1.9563
2022-03-04 03:27:48 - train: epoch 0132, iter [00700, 05004], lr: 0.027840, loss: 2.0896
2022-03-04 03:28:20 - train: epoch 0132, iter [00800, 05004], lr: 0.027840, loss: 2.1565
2022-03-04 03:28:52 - train: epoch 0132, iter [00900, 05004], lr: 0.027840, loss: 2.3753
2022-03-04 03:29:25 - train: epoch 0132, iter [01000, 05004], lr: 0.027840, loss: 2.2729
2022-03-04 03:29:56 - train: epoch 0132, iter [01100, 05004], lr: 0.027840, loss: 2.2071
2022-03-04 03:30:27 - train: epoch 0132, iter [01200, 05004], lr: 0.027840, loss: 2.1827
2022-03-04 03:31:00 - train: epoch 0132, iter [01300, 05004], lr: 0.027840, loss: 2.0720
2022-03-04 03:31:31 - train: epoch 0132, iter [01400, 05004], lr: 0.027840, loss: 2.1637
2022-03-04 03:32:04 - train: epoch 0132, iter [01500, 05004], lr: 0.027840, loss: 2.3739
2022-03-04 03:32:36 - train: epoch 0132, iter [01600, 05004], lr: 0.027840, loss: 2.3544
2022-03-04 03:33:09 - train: epoch 0132, iter [01700, 05004], lr: 0.027840, loss: 2.2058
2022-03-04 03:33:40 - train: epoch 0132, iter [01800, 05004], lr: 0.027840, loss: 2.6161
2022-03-04 03:34:12 - train: epoch 0132, iter [01900, 05004], lr: 0.027840, loss: 1.8537
2022-03-04 03:34:44 - train: epoch 0132, iter [02000, 05004], lr: 0.027840, loss: 1.8321
2022-03-04 03:35:16 - train: epoch 0132, iter [02100, 05004], lr: 0.027840, loss: 2.0386
2022-03-04 03:35:48 - train: epoch 0132, iter [02200, 05004], lr: 0.027840, loss: 2.1428
2022-03-04 03:36:21 - train: epoch 0132, iter [02300, 05004], lr: 0.027840, loss: 2.2399
2022-03-04 03:36:52 - train: epoch 0132, iter [02400, 05004], lr: 0.027840, loss: 2.1059
2022-03-04 03:37:25 - train: epoch 0132, iter [02500, 05004], lr: 0.027840, loss: 2.2814
2022-03-04 03:37:56 - train: epoch 0132, iter [02600, 05004], lr: 0.027840, loss: 2.3792
2022-03-04 03:38:28 - train: epoch 0132, iter [02700, 05004], lr: 0.027840, loss: 2.2609
2022-03-04 03:39:00 - train: epoch 0132, iter [02800, 05004], lr: 0.027840, loss: 2.6117
2022-03-04 03:39:33 - train: epoch 0132, iter [02900, 05004], lr: 0.027840, loss: 1.9722
2022-03-04 03:40:04 - train: epoch 0132, iter [03000, 05004], lr: 0.027840, loss: 2.4282
2022-03-04 03:40:36 - train: epoch 0132, iter [03100, 05004], lr: 0.027840, loss: 2.1262
2022-03-04 03:41:08 - train: epoch 0132, iter [03200, 05004], lr: 0.027840, loss: 2.1500
2022-03-04 03:41:40 - train: epoch 0132, iter [03300, 05004], lr: 0.027840, loss: 2.3500
2022-03-04 03:42:12 - train: epoch 0132, iter [03400, 05004], lr: 0.027840, loss: 2.2821
2022-03-04 03:42:45 - train: epoch 0132, iter [03500, 05004], lr: 0.027840, loss: 2.0844
2022-03-04 03:43:16 - train: epoch 0132, iter [03600, 05004], lr: 0.027840, loss: 2.1082
2022-03-04 03:43:48 - train: epoch 0132, iter [03700, 05004], lr: 0.027840, loss: 2.0181
2022-03-04 03:44:20 - train: epoch 0132, iter [03800, 05004], lr: 0.027840, loss: 2.2177
2022-03-04 03:44:53 - train: epoch 0132, iter [03900, 05004], lr: 0.027840, loss: 2.1911
2022-03-04 03:45:23 - train: epoch 0132, iter [04000, 05004], lr: 0.027840, loss: 2.2512
2022-03-04 03:45:56 - train: epoch 0132, iter [04100, 05004], lr: 0.027840, loss: 1.9754
2022-03-04 03:46:26 - train: epoch 0132, iter [04200, 05004], lr: 0.027840, loss: 2.5320
2022-03-04 03:47:00 - train: epoch 0132, iter [04300, 05004], lr: 0.027840, loss: 2.2609
2022-03-04 03:47:31 - train: epoch 0132, iter [04400, 05004], lr: 0.027840, loss: 1.9843
2022-03-04 03:48:03 - train: epoch 0132, iter [04500, 05004], lr: 0.027840, loss: 2.5110
2022-03-04 03:48:35 - train: epoch 0132, iter [04600, 05004], lr: 0.027840, loss: 2.3492
2022-03-04 03:49:06 - train: epoch 0132, iter [04700, 05004], lr: 0.027840, loss: 2.1992
2022-03-04 03:49:38 - train: epoch 0132, iter [04800, 05004], lr: 0.027840, loss: 2.2160
2022-03-04 03:50:11 - train: epoch 0132, iter [04900, 05004], lr: 0.027840, loss: 2.4470
2022-03-04 03:50:41 - train: epoch 0132, iter [05000, 05004], lr: 0.027840, loss: 2.2708
2022-03-04 03:50:42 - train: epoch 132, train_loss: 2.1751
2022-03-04 03:51:52 - eval: epoch: 132, acc1: 66.114%, acc5: 87.636%, test_loss: 1.3836, per_image_load_time: 0.708ms, per_image_inference_time: 0.329ms
2022-03-04 03:51:52 - until epoch: 132, best_acc1: 66.114%
2022-03-04 03:51:52 - epoch 133 lr: 0.02712122502978024
2022-03-04 03:52:29 - train: epoch 0133, iter [00100, 05004], lr: 0.027121, loss: 2.1555
2022-03-04 03:53:02 - train: epoch 0133, iter [00200, 05004], lr: 0.027121, loss: 2.2636
2022-03-04 03:53:33 - train: epoch 0133, iter [00300, 05004], lr: 0.027121, loss: 2.1331
2022-03-04 03:54:06 - train: epoch 0133, iter [00400, 05004], lr: 0.027121, loss: 2.2446
2022-03-04 03:54:37 - train: epoch 0133, iter [00500, 05004], lr: 0.027121, loss: 2.2305
2022-03-04 03:55:10 - train: epoch 0133, iter [00600, 05004], lr: 0.027121, loss: 2.2885
2022-03-04 03:55:41 - train: epoch 0133, iter [00700, 05004], lr: 0.027121, loss: 2.0142
2022-03-04 03:56:14 - train: epoch 0133, iter [00800, 05004], lr: 0.027121, loss: 1.9917
2022-03-04 03:56:46 - train: epoch 0133, iter [00900, 05004], lr: 0.027121, loss: 2.0157
2022-03-04 03:57:18 - train: epoch 0133, iter [01000, 05004], lr: 0.027121, loss: 2.0270
2022-03-04 03:57:51 - train: epoch 0133, iter [01100, 05004], lr: 0.027121, loss: 1.8204
2022-03-04 03:58:23 - train: epoch 0133, iter [01200, 05004], lr: 0.027121, loss: 2.0842
2022-03-04 03:58:54 - train: epoch 0133, iter [01300, 05004], lr: 0.027121, loss: 1.9952
2022-03-04 03:59:26 - train: epoch 0133, iter [01400, 05004], lr: 0.027121, loss: 2.0928
2022-03-04 03:59:58 - train: epoch 0133, iter [01500, 05004], lr: 0.027121, loss: 2.0837
2022-03-04 04:00:29 - train: epoch 0133, iter [01600, 05004], lr: 0.027121, loss: 2.2297
2022-03-04 04:01:01 - train: epoch 0133, iter [01700, 05004], lr: 0.027121, loss: 2.1909
2022-03-04 04:01:34 - train: epoch 0133, iter [01800, 05004], lr: 0.027121, loss: 1.9359
2022-03-04 04:02:05 - train: epoch 0133, iter [01900, 05004], lr: 0.027121, loss: 2.0094
2022-03-04 04:02:37 - train: epoch 0133, iter [02000, 05004], lr: 0.027121, loss: 1.8035
2022-03-04 04:03:10 - train: epoch 0133, iter [02100, 05004], lr: 0.027121, loss: 2.2307
2022-03-04 04:03:42 - train: epoch 0133, iter [02200, 05004], lr: 0.027121, loss: 2.2391
2022-03-04 04:04:14 - train: epoch 0133, iter [02300, 05004], lr: 0.027121, loss: 2.1995
2022-03-04 04:04:46 - train: epoch 0133, iter [02400, 05004], lr: 0.027121, loss: 2.3417
2022-03-04 04:05:18 - train: epoch 0133, iter [02500, 05004], lr: 0.027121, loss: 2.1128
2022-03-04 04:05:51 - train: epoch 0133, iter [02600, 05004], lr: 0.027121, loss: 2.2914
2022-03-04 04:06:23 - train: epoch 0133, iter [02700, 05004], lr: 0.027121, loss: 2.3244
2022-03-04 04:06:55 - train: epoch 0133, iter [02800, 05004], lr: 0.027121, loss: 2.2717
2022-03-04 04:07:27 - train: epoch 0133, iter [02900, 05004], lr: 0.027121, loss: 1.9512
2022-03-04 04:07:59 - train: epoch 0133, iter [03000, 05004], lr: 0.027121, loss: 2.3297
2022-03-04 04:08:31 - train: epoch 0133, iter [03100, 05004], lr: 0.027121, loss: 2.4018
2022-03-04 04:09:04 - train: epoch 0133, iter [03200, 05004], lr: 0.027121, loss: 1.9989
2022-03-04 04:09:35 - train: epoch 0133, iter [03300, 05004], lr: 0.027121, loss: 2.1063
2022-03-04 04:10:07 - train: epoch 0133, iter [03400, 05004], lr: 0.027121, loss: 2.1964
2022-03-04 04:10:38 - train: epoch 0133, iter [03500, 05004], lr: 0.027121, loss: 2.1019
2022-03-04 04:11:11 - train: epoch 0133, iter [03600, 05004], lr: 0.027121, loss: 2.0917
2022-03-04 04:11:43 - train: epoch 0133, iter [03700, 05004], lr: 0.027121, loss: 2.3028
2022-03-04 04:12:15 - train: epoch 0133, iter [03800, 05004], lr: 0.027121, loss: 2.0543
2022-03-04 04:12:47 - train: epoch 0133, iter [03900, 05004], lr: 0.027121, loss: 2.2906
2022-03-04 04:13:20 - train: epoch 0133, iter [04000, 05004], lr: 0.027121, loss: 2.2004
2022-03-04 04:13:51 - train: epoch 0133, iter [04100, 05004], lr: 0.027121, loss: 2.3970
2022-03-04 04:14:23 - train: epoch 0133, iter [04200, 05004], lr: 0.027121, loss: 1.9637
2022-03-04 04:14:55 - train: epoch 0133, iter [04300, 05004], lr: 0.027121, loss: 2.3398
2022-03-04 04:15:27 - train: epoch 0133, iter [04400, 05004], lr: 0.027121, loss: 1.9128
2022-03-04 04:15:59 - train: epoch 0133, iter [04500, 05004], lr: 0.027121, loss: 2.3115
2022-03-04 04:16:32 - train: epoch 0133, iter [04600, 05004], lr: 0.027121, loss: 1.8881
2022-03-04 04:17:03 - train: epoch 0133, iter [04700, 05004], lr: 0.027121, loss: 2.0633
2022-03-04 04:17:35 - train: epoch 0133, iter [04800, 05004], lr: 0.027121, loss: 2.0374
2022-03-04 04:18:07 - train: epoch 0133, iter [04900, 05004], lr: 0.027121, loss: 1.8687
2022-03-04 04:18:38 - train: epoch 0133, iter [05000, 05004], lr: 0.027121, loss: 2.0128
2022-03-04 04:18:40 - train: epoch 133, train_loss: 2.1667
2022-03-04 04:19:50 - eval: epoch: 133, acc1: 64.730%, acc5: 86.500%, test_loss: 1.4495, per_image_load_time: 2.329ms, per_image_inference_time: 0.344ms
2022-03-04 04:19:51 - until epoch: 133, best_acc1: 66.114%
2022-03-04 04:19:51 - epoch 134 lr: 0.0264079655268759
2022-03-04 04:20:27 - train: epoch 0134, iter [00100, 05004], lr: 0.026408, loss: 2.1389
2022-03-04 04:20:59 - train: epoch 0134, iter [00200, 05004], lr: 0.026408, loss: 2.1268
2022-03-04 04:21:31 - train: epoch 0134, iter [00300, 05004], lr: 0.026408, loss: 2.2315
2022-03-04 04:22:03 - train: epoch 0134, iter [00400, 05004], lr: 0.026408, loss: 2.0512
2022-03-04 04:22:35 - train: epoch 0134, iter [00500, 05004], lr: 0.026408, loss: 2.0084
2022-03-04 04:23:07 - train: epoch 0134, iter [00600, 05004], lr: 0.026408, loss: 2.1597
2022-03-04 04:23:38 - train: epoch 0134, iter [00700, 05004], lr: 0.026408, loss: 2.5008
2022-03-04 04:24:11 - train: epoch 0134, iter [00800, 05004], lr: 0.026408, loss: 2.0925
2022-03-04 04:24:43 - train: epoch 0134, iter [00900, 05004], lr: 0.026408, loss: 2.1758
2022-03-04 04:25:15 - train: epoch 0134, iter [01000, 05004], lr: 0.026408, loss: 2.2334
2022-03-04 04:25:47 - train: epoch 0134, iter [01100, 05004], lr: 0.026408, loss: 2.1342
2022-03-04 04:26:19 - train: epoch 0134, iter [01200, 05004], lr: 0.026408, loss: 1.9478
2022-03-04 04:26:52 - train: epoch 0134, iter [01300, 05004], lr: 0.026408, loss: 2.4205
2022-03-04 04:27:23 - train: epoch 0134, iter [01400, 05004], lr: 0.026408, loss: 1.8891
2022-03-04 04:27:55 - train: epoch 0134, iter [01500, 05004], lr: 0.026408, loss: 2.1950
2022-03-04 04:28:27 - train: epoch 0134, iter [01600, 05004], lr: 0.026408, loss: 2.2558
2022-03-04 04:28:58 - train: epoch 0134, iter [01700, 05004], lr: 0.026408, loss: 2.0795
2022-03-04 04:29:30 - train: epoch 0134, iter [01800, 05004], lr: 0.026408, loss: 2.3325
2022-03-04 04:30:02 - train: epoch 0134, iter [01900, 05004], lr: 0.026408, loss: 2.0567
2022-03-04 04:30:34 - train: epoch 0134, iter [02000, 05004], lr: 0.026408, loss: 2.0965
2022-03-04 04:31:06 - train: epoch 0134, iter [02100, 05004], lr: 0.026408, loss: 2.2349
2022-03-04 04:31:39 - train: epoch 0134, iter [02200, 05004], lr: 0.026408, loss: 2.1393
2022-03-04 04:32:10 - train: epoch 0134, iter [02300, 05004], lr: 0.026408, loss: 2.1736
2022-03-04 04:32:42 - train: epoch 0134, iter [02400, 05004], lr: 0.026408, loss: 1.7280
2022-03-04 04:33:14 - train: epoch 0134, iter [02500, 05004], lr: 0.026408, loss: 2.3733
2022-03-04 04:33:46 - train: epoch 0134, iter [02600, 05004], lr: 0.026408, loss: 2.4573
2022-03-04 04:34:18 - train: epoch 0134, iter [02700, 05004], lr: 0.026408, loss: 2.3716
2022-03-04 04:34:50 - train: epoch 0134, iter [02800, 05004], lr: 0.026408, loss: 1.9160
2022-03-04 04:35:23 - train: epoch 0134, iter [02900, 05004], lr: 0.026408, loss: 2.4758
2022-03-04 04:35:54 - train: epoch 0134, iter [03000, 05004], lr: 0.026408, loss: 2.1689
2022-03-04 04:36:27 - train: epoch 0134, iter [03100, 05004], lr: 0.026408, loss: 2.1872
2022-03-04 04:36:59 - train: epoch 0134, iter [03200, 05004], lr: 0.026408, loss: 2.2621
2022-03-04 04:37:31 - train: epoch 0134, iter [03300, 05004], lr: 0.026408, loss: 2.3720
2022-03-04 04:38:02 - train: epoch 0134, iter [03400, 05004], lr: 0.026408, loss: 1.9548
2022-03-04 04:38:34 - train: epoch 0134, iter [03500, 05004], lr: 0.026408, loss: 2.3445
2022-03-04 04:39:06 - train: epoch 0134, iter [03600, 05004], lr: 0.026408, loss: 2.0062
2022-03-04 04:39:38 - train: epoch 0134, iter [03700, 05004], lr: 0.026408, loss: 2.2836
2022-03-04 04:40:10 - train: epoch 0134, iter [03800, 05004], lr: 0.026408, loss: 2.0330
2022-03-04 04:40:41 - train: epoch 0134, iter [03900, 05004], lr: 0.026408, loss: 2.1871
2022-03-04 04:41:14 - train: epoch 0134, iter [04000, 05004], lr: 0.026408, loss: 2.1896
2022-03-04 04:41:45 - train: epoch 0134, iter [04100, 05004], lr: 0.026408, loss: 2.2648
2022-03-04 04:42:18 - train: epoch 0134, iter [04200, 05004], lr: 0.026408, loss: 2.2367
2022-03-04 04:42:50 - train: epoch 0134, iter [04300, 05004], lr: 0.026408, loss: 2.0281
2022-03-04 04:43:23 - train: epoch 0134, iter [04400, 05004], lr: 0.026408, loss: 2.3152
2022-03-04 04:43:54 - train: epoch 0134, iter [04500, 05004], lr: 0.026408, loss: 2.1872
2022-03-04 04:44:26 - train: epoch 0134, iter [04600, 05004], lr: 0.026408, loss: 2.0248
2022-03-04 04:44:58 - train: epoch 0134, iter [04700, 05004], lr: 0.026408, loss: 2.1791
2022-03-04 04:45:30 - train: epoch 0134, iter [04800, 05004], lr: 0.026408, loss: 2.2332
2022-03-04 04:46:01 - train: epoch 0134, iter [04900, 05004], lr: 0.026408, loss: 1.9029
2022-03-04 04:46:33 - train: epoch 0134, iter [05000, 05004], lr: 0.026408, loss: 2.3419
2022-03-04 04:46:34 - train: epoch 134, train_loss: 2.1564
2022-03-04 04:47:44 - eval: epoch: 134, acc1: 66.204%, acc5: 87.452%, test_loss: 1.3835, per_image_load_time: 1.557ms, per_image_inference_time: 0.389ms
2022-03-04 04:47:45 - until epoch: 134, best_acc1: 66.204%
2022-03-04 04:47:45 - epoch 135 lr: 0.0257008293378697
2022-03-04 04:48:22 - train: epoch 0135, iter [00100, 05004], lr: 0.025701, loss: 1.8243
2022-03-04 04:48:54 - train: epoch 0135, iter [00200, 05004], lr: 0.025701, loss: 2.2177
2022-03-04 04:49:26 - train: epoch 0135, iter [00300, 05004], lr: 0.025701, loss: 2.1823
2022-03-04 04:49:58 - train: epoch 0135, iter [00400, 05004], lr: 0.025701, loss: 2.1482
2022-03-04 04:50:30 - train: epoch 0135, iter [00500, 05004], lr: 0.025701, loss: 1.9047
2022-03-04 04:51:02 - train: epoch 0135, iter [00600, 05004], lr: 0.025701, loss: 2.1121
2022-03-04 04:51:33 - train: epoch 0135, iter [00700, 05004], lr: 0.025701, loss: 2.3052
2022-03-04 04:52:05 - train: epoch 0135, iter [00800, 05004], lr: 0.025701, loss: 2.2047
2022-03-04 04:52:37 - train: epoch 0135, iter [00900, 05004], lr: 0.025701, loss: 2.1431
2022-03-04 04:53:09 - train: epoch 0135, iter [01000, 05004], lr: 0.025701, loss: 2.3197
2022-03-04 04:53:42 - train: epoch 0135, iter [01100, 05004], lr: 0.025701, loss: 2.0210
2022-03-04 04:54:14 - train: epoch 0135, iter [01200, 05004], lr: 0.025701, loss: 2.2137
2022-03-04 04:54:46 - train: epoch 0135, iter [01300, 05004], lr: 0.025701, loss: 2.5224
2022-03-04 04:55:18 - train: epoch 0135, iter [01400, 05004], lr: 0.025701, loss: 2.4645
2022-03-04 04:55:50 - train: epoch 0135, iter [01500, 05004], lr: 0.025701, loss: 2.1395
2022-03-04 04:56:22 - train: epoch 0135, iter [01600, 05004], lr: 0.025701, loss: 1.9811
2022-03-04 04:56:54 - train: epoch 0135, iter [01700, 05004], lr: 0.025701, loss: 2.3600
2022-03-04 04:57:25 - train: epoch 0135, iter [01800, 05004], lr: 0.025701, loss: 2.2693
2022-03-04 04:57:58 - train: epoch 0135, iter [01900, 05004], lr: 0.025701, loss: 2.2751
2022-03-04 04:58:29 - train: epoch 0135, iter [02000, 05004], lr: 0.025701, loss: 2.3494
2022-03-04 04:59:02 - train: epoch 0135, iter [02100, 05004], lr: 0.025701, loss: 2.2752
2022-03-04 04:59:34 - train: epoch 0135, iter [02200, 05004], lr: 0.025701, loss: 2.0486
2022-03-04 05:00:06 - train: epoch 0135, iter [02300, 05004], lr: 0.025701, loss: 2.2195
2022-03-04 05:00:37 - train: epoch 0135, iter [02400, 05004], lr: 0.025701, loss: 2.0089
2022-03-04 05:01:09 - train: epoch 0135, iter [02500, 05004], lr: 0.025701, loss: 2.2731
2022-03-04 05:01:41 - train: epoch 0135, iter [02600, 05004], lr: 0.025701, loss: 2.1232
2022-03-04 05:02:13 - train: epoch 0135, iter [02700, 05004], lr: 0.025701, loss: 2.1809
2022-03-04 05:02:45 - train: epoch 0135, iter [02800, 05004], lr: 0.025701, loss: 2.4388
2022-03-04 05:03:17 - train: epoch 0135, iter [02900, 05004], lr: 0.025701, loss: 2.0175
2022-03-04 05:03:48 - train: epoch 0135, iter [03000, 05004], lr: 0.025701, loss: 2.2062
2022-03-04 05:04:21 - train: epoch 0135, iter [03100, 05004], lr: 0.025701, loss: 2.2126
2022-03-04 05:04:52 - train: epoch 0135, iter [03200, 05004], lr: 0.025701, loss: 2.0619
2022-03-04 05:05:25 - train: epoch 0135, iter [03300, 05004], lr: 0.025701, loss: 2.1018
2022-03-04 05:05:56 - train: epoch 0135, iter [03400, 05004], lr: 0.025701, loss: 2.4262
2022-03-04 05:06:29 - train: epoch 0135, iter [03500, 05004], lr: 0.025701, loss: 2.1170
2022-03-04 05:07:00 - train: epoch 0135, iter [03600, 05004], lr: 0.025701, loss: 1.9698
2022-03-04 05:07:33 - train: epoch 0135, iter [03700, 05004], lr: 0.025701, loss: 1.9895
2022-03-04 05:08:04 - train: epoch 0135, iter [03800, 05004], lr: 0.025701, loss: 2.0535
2022-03-04 05:08:37 - train: epoch 0135, iter [03900, 05004], lr: 0.025701, loss: 2.1112
2022-03-04 05:09:08 - train: epoch 0135, iter [04000, 05004], lr: 0.025701, loss: 2.5516
2022-03-04 05:09:40 - train: epoch 0135, iter [04100, 05004], lr: 0.025701, loss: 2.3887
2022-03-04 05:10:12 - train: epoch 0135, iter [04200, 05004], lr: 0.025701, loss: 2.2557
2022-03-04 05:10:44 - train: epoch 0135, iter [04300, 05004], lr: 0.025701, loss: 2.3426
2022-03-04 05:11:16 - train: epoch 0135, iter [04400, 05004], lr: 0.025701, loss: 2.2018
2022-03-04 05:11:48 - train: epoch 0135, iter [04500, 05004], lr: 0.025701, loss: 2.0800
2022-03-04 05:12:19 - train: epoch 0135, iter [04600, 05004], lr: 0.025701, loss: 2.3658
2022-03-04 05:12:51 - train: epoch 0135, iter [04700, 05004], lr: 0.025701, loss: 2.2780
2022-03-04 05:13:23 - train: epoch 0135, iter [04800, 05004], lr: 0.025701, loss: 1.9096
2022-03-04 05:13:55 - train: epoch 0135, iter [04900, 05004], lr: 0.025701, loss: 2.5292
2022-03-04 05:14:26 - train: epoch 0135, iter [05000, 05004], lr: 0.025701, loss: 2.0435
2022-03-04 05:14:27 - train: epoch 135, train_loss: 2.1498
2022-03-04 05:15:37 - eval: epoch: 135, acc1: 65.202%, acc5: 87.072%, test_loss: 1.4180, per_image_load_time: 0.637ms, per_image_inference_time: 0.384ms
2022-03-04 05:15:38 - until epoch: 135, best_acc1: 66.204%
2022-03-04 05:15:38 - epoch 136 lr: 0.025000000000000012
2022-03-04 05:16:14 - train: epoch 0136, iter [00100, 05004], lr: 0.025000, loss: 2.1656
2022-03-04 05:16:48 - train: epoch 0136, iter [00200, 05004], lr: 0.025000, loss: 2.2379
2022-03-04 05:17:20 - train: epoch 0136, iter [00300, 05004], lr: 0.025000, loss: 2.1626
2022-03-04 05:17:51 - train: epoch 0136, iter [00400, 05004], lr: 0.025000, loss: 2.0962
2022-03-04 05:18:23 - train: epoch 0136, iter [00500, 05004], lr: 0.025000, loss: 2.2195
2022-03-04 05:18:56 - train: epoch 0136, iter [00600, 05004], lr: 0.025000, loss: 1.7719
2022-03-04 05:19:27 - train: epoch 0136, iter [00700, 05004], lr: 0.025000, loss: 2.1000
2022-03-04 05:19:59 - train: epoch 0136, iter [00800, 05004], lr: 0.025000, loss: 2.3188
2022-03-04 05:20:30 - train: epoch 0136, iter [00900, 05004], lr: 0.025000, loss: 2.3820
2022-03-04 05:21:02 - train: epoch 0136, iter [01000, 05004], lr: 0.025000, loss: 2.0749
2022-03-04 05:21:34 - train: epoch 0136, iter [01100, 05004], lr: 0.025000, loss: 2.1048
2022-03-04 05:22:06 - train: epoch 0136, iter [01200, 05004], lr: 0.025000, loss: 2.1696
2022-03-04 05:22:38 - train: epoch 0136, iter [01300, 05004], lr: 0.025000, loss: 2.2846
2022-03-04 05:23:10 - train: epoch 0136, iter [01400, 05004], lr: 0.025000, loss: 2.1734
2022-03-04 05:23:42 - train: epoch 0136, iter [01500, 05004], lr: 0.025000, loss: 2.1663
2022-03-04 05:24:13 - train: epoch 0136, iter [01600, 05004], lr: 0.025000, loss: 2.0010
2022-03-04 05:24:46 - train: epoch 0136, iter [01700, 05004], lr: 0.025000, loss: 2.4609
2022-03-04 05:25:18 - train: epoch 0136, iter [01800, 05004], lr: 0.025000, loss: 2.2195
2022-03-04 05:25:49 - train: epoch 0136, iter [01900, 05004], lr: 0.025000, loss: 2.2702
2022-03-04 05:26:21 - train: epoch 0136, iter [02000, 05004], lr: 0.025000, loss: 2.0390
2022-03-04 05:26:53 - train: epoch 0136, iter [02100, 05004], lr: 0.025000, loss: 1.9098
2022-03-04 05:27:25 - train: epoch 0136, iter [02200, 05004], lr: 0.025000, loss: 2.1431
2022-03-04 05:27:57 - train: epoch 0136, iter [02300, 05004], lr: 0.025000, loss: 2.4823
2022-03-04 05:28:29 - train: epoch 0136, iter [02400, 05004], lr: 0.025000, loss: 1.9535
2022-03-04 05:29:01 - train: epoch 0136, iter [02500, 05004], lr: 0.025000, loss: 2.2958
2022-03-04 05:29:34 - train: epoch 0136, iter [02600, 05004], lr: 0.025000, loss: 2.0072
2022-03-04 05:30:05 - train: epoch 0136, iter [02700, 05004], lr: 0.025000, loss: 2.0820
2022-03-04 05:30:37 - train: epoch 0136, iter [02800, 05004], lr: 0.025000, loss: 2.3161
2022-03-04 05:31:09 - train: epoch 0136, iter [02900, 05004], lr: 0.025000, loss: 2.2065
2022-03-04 05:31:40 - train: epoch 0136, iter [03000, 05004], lr: 0.025000, loss: 2.1413
2022-03-04 05:32:12 - train: epoch 0136, iter [03100, 05004], lr: 0.025000, loss: 2.0128
2022-03-04 05:32:44 - train: epoch 0136, iter [03200, 05004], lr: 0.025000, loss: 2.2615
2022-03-04 05:33:16 - train: epoch 0136, iter [03300, 05004], lr: 0.025000, loss: 2.1600
2022-03-04 05:33:48 - train: epoch 0136, iter [03400, 05004], lr: 0.025000, loss: 2.0287
2022-03-04 05:34:21 - train: epoch 0136, iter [03500, 05004], lr: 0.025000, loss: 2.0569
2022-03-04 05:34:52 - train: epoch 0136, iter [03600, 05004], lr: 0.025000, loss: 2.2927
2022-03-04 05:35:24 - train: epoch 0136, iter [03700, 05004], lr: 0.025000, loss: 2.1342
2022-03-04 05:35:57 - train: epoch 0136, iter [03800, 05004], lr: 0.025000, loss: 2.2452
2022-03-04 05:36:29 - train: epoch 0136, iter [03900, 05004], lr: 0.025000, loss: 2.1617
2022-03-04 05:37:00 - train: epoch 0136, iter [04000, 05004], lr: 0.025000, loss: 2.0590
2022-03-04 05:37:33 - train: epoch 0136, iter [04100, 05004], lr: 0.025000, loss: 2.0722
2022-03-04 05:38:04 - train: epoch 0136, iter [04200, 05004], lr: 0.025000, loss: 2.1367
2022-03-04 05:38:36 - train: epoch 0136, iter [04300, 05004], lr: 0.025000, loss: 1.7900
2022-03-04 05:39:07 - train: epoch 0136, iter [04400, 05004], lr: 0.025000, loss: 1.7151
2022-03-04 05:39:40 - train: epoch 0136, iter [04500, 05004], lr: 0.025000, loss: 2.2732
2022-03-04 05:40:12 - train: epoch 0136, iter [04600, 05004], lr: 0.025000, loss: 2.1189
2022-03-04 05:40:43 - train: epoch 0136, iter [04700, 05004], lr: 0.025000, loss: 2.1812
2022-03-04 05:41:16 - train: epoch 0136, iter [04800, 05004], lr: 0.025000, loss: 2.2782
2022-03-04 05:41:47 - train: epoch 0136, iter [04900, 05004], lr: 0.025000, loss: 2.1674
2022-03-04 05:42:18 - train: epoch 0136, iter [05000, 05004], lr: 0.025000, loss: 2.4340
2022-03-04 05:42:19 - train: epoch 136, train_loss: 2.1340
2022-03-04 05:43:28 - eval: epoch: 136, acc1: 66.024%, acc5: 87.458%, test_loss: 1.3852, per_image_load_time: 1.351ms, per_image_inference_time: 0.347ms
2022-03-04 05:43:29 - until epoch: 136, best_acc1: 66.204%
2022-03-04 05:43:29 - epoch 137 lr: 0.02430565941356157
2022-03-04 05:44:05 - train: epoch 0137, iter [00100, 05004], lr: 0.024306, loss: 2.0436
2022-03-04 05:44:38 - train: epoch 0137, iter [00200, 05004], lr: 0.024306, loss: 2.2849
2022-03-04 05:45:10 - train: epoch 0137, iter [00300, 05004], lr: 0.024306, loss: 1.8970
2022-03-04 05:45:42 - train: epoch 0137, iter [00400, 05004], lr: 0.024306, loss: 1.9196
2022-03-04 05:46:15 - train: epoch 0137, iter [00500, 05004], lr: 0.024306, loss: 2.0296
2022-03-04 05:46:46 - train: epoch 0137, iter [00600, 05004], lr: 0.024306, loss: 2.3474
2022-03-04 05:47:18 - train: epoch 0137, iter [00700, 05004], lr: 0.024306, loss: 2.2888
2022-03-04 05:47:50 - train: epoch 0137, iter [00800, 05004], lr: 0.024306, loss: 2.1102
2022-03-04 05:48:22 - train: epoch 0137, iter [00900, 05004], lr: 0.024306, loss: 2.0762
2022-03-04 05:48:55 - train: epoch 0137, iter [01000, 05004], lr: 0.024306, loss: 2.1187
2022-03-04 05:49:26 - train: epoch 0137, iter [01100, 05004], lr: 0.024306, loss: 2.4291
2022-03-04 05:49:58 - train: epoch 0137, iter [01200, 05004], lr: 0.024306, loss: 1.9668
2022-03-04 05:50:29 - train: epoch 0137, iter [01300, 05004], lr: 0.024306, loss: 1.9588
2022-03-04 05:51:02 - train: epoch 0137, iter [01400, 05004], lr: 0.024306, loss: 2.2375
2022-03-04 05:51:33 - train: epoch 0137, iter [01500, 05004], lr: 0.024306, loss: 2.0949
2022-03-04 05:52:06 - train: epoch 0137, iter [01600, 05004], lr: 0.024306, loss: 2.3325
2022-03-04 05:52:38 - train: epoch 0137, iter [01700, 05004], lr: 0.024306, loss: 1.9835
2022-03-04 05:53:10 - train: epoch 0137, iter [01800, 05004], lr: 0.024306, loss: 2.3745
2022-03-04 05:53:41 - train: epoch 0137, iter [01900, 05004], lr: 0.024306, loss: 2.2270
2022-03-04 05:54:14 - train: epoch 0137, iter [02000, 05004], lr: 0.024306, loss: 2.0769
2022-03-04 05:54:45 - train: epoch 0137, iter [02100, 05004], lr: 0.024306, loss: 2.2573
2022-03-04 05:55:17 - train: epoch 0137, iter [02200, 05004], lr: 0.024306, loss: 2.3611
2022-03-04 05:55:49 - train: epoch 0137, iter [02300, 05004], lr: 0.024306, loss: 2.1188
2022-03-04 05:56:22 - train: epoch 0137, iter [02400, 05004], lr: 0.024306, loss: 2.0634
2022-03-04 05:56:54 - train: epoch 0137, iter [02500, 05004], lr: 0.024306, loss: 2.0989
2022-03-04 05:57:26 - train: epoch 0137, iter [02600, 05004], lr: 0.024306, loss: 2.1045
2022-03-04 05:57:58 - train: epoch 0137, iter [02700, 05004], lr: 0.024306, loss: 2.0055
2022-03-04 05:58:31 - train: epoch 0137, iter [02800, 05004], lr: 0.024306, loss: 1.9031
2022-03-04 05:59:02 - train: epoch 0137, iter [02900, 05004], lr: 0.024306, loss: 2.0104
2022-03-04 05:59:34 - train: epoch 0137, iter [03000, 05004], lr: 0.024306, loss: 2.1819
2022-03-04 06:00:06 - train: epoch 0137, iter [03100, 05004], lr: 0.024306, loss: 2.5824
2022-03-04 06:00:39 - train: epoch 0137, iter [03200, 05004], lr: 0.024306, loss: 2.1217
2022-03-04 06:01:09 - train: epoch 0137, iter [03300, 05004], lr: 0.024306, loss: 1.9299
2022-03-04 06:01:42 - train: epoch 0137, iter [03400, 05004], lr: 0.024306, loss: 2.1317
2022-03-04 06:02:13 - train: epoch 0137, iter [03500, 05004], lr: 0.024306, loss: 2.3034
2022-03-04 06:02:46 - train: epoch 0137, iter [03600, 05004], lr: 0.024306, loss: 2.0151
2022-03-04 06:03:18 - train: epoch 0137, iter [03700, 05004], lr: 0.024306, loss: 2.1299
2022-03-04 06:03:51 - train: epoch 0137, iter [03800, 05004], lr: 0.024306, loss: 1.9917
2022-03-04 06:04:22 - train: epoch 0137, iter [03900, 05004], lr: 0.024306, loss: 2.1146
2022-03-04 06:04:55 - train: epoch 0137, iter [04000, 05004], lr: 0.024306, loss: 1.9575
2022-03-04 06:05:26 - train: epoch 0137, iter [04100, 05004], lr: 0.024306, loss: 2.2781
2022-03-04 06:05:58 - train: epoch 0137, iter [04200, 05004], lr: 0.024306, loss: 2.1871
2022-03-04 06:06:31 - train: epoch 0137, iter [04300, 05004], lr: 0.024306, loss: 2.0245
2022-03-04 06:07:03 - train: epoch 0137, iter [04400, 05004], lr: 0.024306, loss: 2.0903
2022-03-04 06:07:36 - train: epoch 0137, iter [04500, 05004], lr: 0.024306, loss: 2.2525
2022-03-04 06:08:07 - train: epoch 0137, iter [04600, 05004], lr: 0.024306, loss: 2.2090
2022-03-04 06:08:39 - train: epoch 0137, iter [04700, 05004], lr: 0.024306, loss: 2.1214
2022-03-04 06:09:11 - train: epoch 0137, iter [04800, 05004], lr: 0.024306, loss: 1.9727
2022-03-04 06:09:44 - train: epoch 0137, iter [04900, 05004], lr: 0.024306, loss: 2.3142
2022-03-04 06:10:15 - train: epoch 0137, iter [05000, 05004], lr: 0.024306, loss: 1.9549
2022-03-04 06:10:16 - train: epoch 137, train_loss: 2.1223
2022-03-04 06:11:26 - eval: epoch: 137, acc1: 66.502%, acc5: 87.848%, test_loss: 1.3625, per_image_load_time: 1.311ms, per_image_inference_time: 0.351ms
2022-03-04 06:11:27 - until epoch: 137, best_acc1: 66.502%
2022-03-04 06:11:27 - epoch 138 lr: 0.02361798779469336
2022-03-04 06:12:04 - train: epoch 0138, iter [00100, 05004], lr: 0.023618, loss: 1.8989
2022-03-04 06:12:36 - train: epoch 0138, iter [00200, 05004], lr: 0.023618, loss: 2.2985
2022-03-04 06:13:08 - train: epoch 0138, iter [00300, 05004], lr: 0.023618, loss: 2.3206
2022-03-04 06:13:40 - train: epoch 0138, iter [00400, 05004], lr: 0.023618, loss: 2.1389
2022-03-04 06:14:13 - train: epoch 0138, iter [00500, 05004], lr: 0.023618, loss: 2.0111
2022-03-04 06:14:44 - train: epoch 0138, iter [00600, 05004], lr: 0.023618, loss: 2.0878
2022-03-04 06:15:16 - train: epoch 0138, iter [00700, 05004], lr: 0.023618, loss: 2.1318
2022-03-04 06:15:48 - train: epoch 0138, iter [00800, 05004], lr: 0.023618, loss: 2.1033
2022-03-04 06:16:20 - train: epoch 0138, iter [00900, 05004], lr: 0.023618, loss: 2.1343
2022-03-04 06:16:52 - train: epoch 0138, iter [01000, 05004], lr: 0.023618, loss: 2.1213
2022-03-04 06:17:24 - train: epoch 0138, iter [01100, 05004], lr: 0.023618, loss: 2.1901
2022-03-04 06:17:56 - train: epoch 0138, iter [01200, 05004], lr: 0.023618, loss: 2.2599
2022-03-04 06:18:28 - train: epoch 0138, iter [01300, 05004], lr: 0.023618, loss: 1.9869
2022-03-04 06:19:00 - train: epoch 0138, iter [01400, 05004], lr: 0.023618, loss: 2.1291
2022-03-04 06:19:32 - train: epoch 0138, iter [01500, 05004], lr: 0.023618, loss: 2.0441
2022-03-04 06:20:04 - train: epoch 0138, iter [01600, 05004], lr: 0.023618, loss: 2.1725
2022-03-04 06:20:36 - train: epoch 0138, iter [01700, 05004], lr: 0.023618, loss: 2.1957
2022-03-04 06:21:08 - train: epoch 0138, iter [01800, 05004], lr: 0.023618, loss: 1.9531
2022-03-04 06:21:40 - train: epoch 0138, iter [01900, 05004], lr: 0.023618, loss: 2.0374
2022-03-04 06:22:11 - train: epoch 0138, iter [02000, 05004], lr: 0.023618, loss: 2.1297
2022-03-04 06:22:44 - train: epoch 0138, iter [02100, 05004], lr: 0.023618, loss: 2.0929
2022-03-04 06:23:16 - train: epoch 0138, iter [02200, 05004], lr: 0.023618, loss: 1.7592
2022-03-04 06:23:48 - train: epoch 0138, iter [02300, 05004], lr: 0.023618, loss: 1.8234
2022-03-04 06:24:20 - train: epoch 0138, iter [02400, 05004], lr: 0.023618, loss: 1.9526
2022-03-04 06:24:52 - train: epoch 0138, iter [02500, 05004], lr: 0.023618, loss: 1.9525
2022-03-04 06:25:24 - train: epoch 0138, iter [02600, 05004], lr: 0.023618, loss: 2.0624
2022-03-04 06:25:55 - train: epoch 0138, iter [02700, 05004], lr: 0.023618, loss: 2.0531
2022-03-04 06:26:28 - train: epoch 0138, iter [02800, 05004], lr: 0.023618, loss: 2.1598
2022-03-04 06:27:00 - train: epoch 0138, iter [02900, 05004], lr: 0.023618, loss: 2.0933
2022-03-04 06:27:32 - train: epoch 0138, iter [03000, 05004], lr: 0.023618, loss: 2.0048
2022-03-04 06:28:03 - train: epoch 0138, iter [03100, 05004], lr: 0.023618, loss: 2.0082
2022-03-04 06:28:36 - train: epoch 0138, iter [03200, 05004], lr: 0.023618, loss: 1.9793
2022-03-04 06:29:08 - train: epoch 0138, iter [03300, 05004], lr: 0.023618, loss: 2.1692
2022-03-04 06:29:40 - train: epoch 0138, iter [03400, 05004], lr: 0.023618, loss: 1.9460
2022-03-04 06:30:11 - train: epoch 0138, iter [03500, 05004], lr: 0.023618, loss: 1.9292
2022-03-04 06:30:44 - train: epoch 0138, iter [03600, 05004], lr: 0.023618, loss: 2.1312
2022-03-04 06:31:15 - train: epoch 0138, iter [03700, 05004], lr: 0.023618, loss: 2.0240
2022-03-04 06:31:47 - train: epoch 0138, iter [03800, 05004], lr: 0.023618, loss: 2.1584
2022-03-04 06:32:19 - train: epoch 0138, iter [03900, 05004], lr: 0.023618, loss: 2.0954
2022-03-04 06:32:51 - train: epoch 0138, iter [04000, 05004], lr: 0.023618, loss: 2.4262
2022-03-04 06:33:23 - train: epoch 0138, iter [04100, 05004], lr: 0.023618, loss: 1.7996
2022-03-04 06:33:54 - train: epoch 0138, iter [04200, 05004], lr: 0.023618, loss: 2.1105
2022-03-04 06:34:27 - train: epoch 0138, iter [04300, 05004], lr: 0.023618, loss: 1.8385
2022-03-04 06:34:58 - train: epoch 0138, iter [04400, 05004], lr: 0.023618, loss: 2.0486
2022-03-04 06:35:30 - train: epoch 0138, iter [04500, 05004], lr: 0.023618, loss: 2.1388
2022-03-04 06:36:02 - train: epoch 0138, iter [04600, 05004], lr: 0.023618, loss: 2.0271
2022-03-04 06:36:34 - train: epoch 0138, iter [04700, 05004], lr: 0.023618, loss: 2.0966
2022-03-04 06:37:06 - train: epoch 0138, iter [04800, 05004], lr: 0.023618, loss: 2.1191
2022-03-04 06:37:38 - train: epoch 0138, iter [04900, 05004], lr: 0.023618, loss: 1.9882
2022-03-04 06:38:09 - train: epoch 0138, iter [05000, 05004], lr: 0.023618, loss: 2.5015
2022-03-04 06:38:10 - train: epoch 138, train_loss: 2.1102
2022-03-04 06:39:20 - eval: epoch: 138, acc1: 66.340%, acc5: 87.714%, test_loss: 1.3735, per_image_load_time: 1.531ms, per_image_inference_time: 0.389ms
2022-03-04 06:39:20 - until epoch: 138, best_acc1: 66.502%
2022-03-04 06:39:20 - epoch 139 lr: 0.022937163628603437
2022-03-04 06:39:56 - train: epoch 0139, iter [00100, 05004], lr: 0.022937, loss: 1.9792
2022-03-04 06:40:30 - train: epoch 0139, iter [00200, 05004], lr: 0.022937, loss: 1.9580
2022-03-04 06:41:01 - train: epoch 0139, iter [00300, 05004], lr: 0.022937, loss: 2.2265
2022-03-04 06:41:33 - train: epoch 0139, iter [00400, 05004], lr: 0.022937, loss: 2.1047
2022-03-04 06:42:06 - train: epoch 0139, iter [00500, 05004], lr: 0.022937, loss: 2.2268
2022-03-04 06:42:37 - train: epoch 0139, iter [00600, 05004], lr: 0.022937, loss: 2.1102
2022-03-04 06:43:09 - train: epoch 0139, iter [00700, 05004], lr: 0.022937, loss: 2.2516
2022-03-04 06:43:41 - train: epoch 0139, iter [00800, 05004], lr: 0.022937, loss: 2.1473
2022-03-04 06:44:12 - train: epoch 0139, iter [00900, 05004], lr: 0.022937, loss: 2.1439
2022-03-04 06:44:45 - train: epoch 0139, iter [01000, 05004], lr: 0.022937, loss: 2.1761
2022-03-04 06:45:17 - train: epoch 0139, iter [01100, 05004], lr: 0.022937, loss: 2.1207
2022-03-04 06:45:49 - train: epoch 0139, iter [01200, 05004], lr: 0.022937, loss: 1.8552
2022-03-04 06:46:21 - train: epoch 0139, iter [01300, 05004], lr: 0.022937, loss: 2.0816
2022-03-04 06:46:54 - train: epoch 0139, iter [01400, 05004], lr: 0.022937, loss: 2.0608
2022-03-04 06:47:25 - train: epoch 0139, iter [01500, 05004], lr: 0.022937, loss: 2.3541
2022-03-04 06:47:57 - train: epoch 0139, iter [01600, 05004], lr: 0.022937, loss: 1.9233
2022-03-04 06:48:28 - train: epoch 0139, iter [01700, 05004], lr: 0.022937, loss: 2.2530
2022-03-04 06:49:01 - train: epoch 0139, iter [01800, 05004], lr: 0.022937, loss: 1.9389
2022-03-04 06:49:32 - train: epoch 0139, iter [01900, 05004], lr: 0.022937, loss: 2.2983
2022-03-04 06:50:04 - train: epoch 0139, iter [02000, 05004], lr: 0.022937, loss: 2.0172
2022-03-04 06:50:37 - train: epoch 0139, iter [02100, 05004], lr: 0.022937, loss: 2.0016
2022-03-04 06:51:08 - train: epoch 0139, iter [02200, 05004], lr: 0.022937, loss: 2.0272
2022-03-04 06:51:40 - train: epoch 0139, iter [02300, 05004], lr: 0.022937, loss: 2.0360
2022-03-04 06:52:11 - train: epoch 0139, iter [02400, 05004], lr: 0.022937, loss: 2.1508
2022-03-04 06:52:44 - train: epoch 0139, iter [02500, 05004], lr: 0.022937, loss: 2.1451
2022-03-04 06:53:15 - train: epoch 0139, iter [02600, 05004], lr: 0.022937, loss: 2.2487
2022-03-04 06:53:47 - train: epoch 0139, iter [02700, 05004], lr: 0.022937, loss: 2.0969
2022-03-04 06:54:18 - train: epoch 0139, iter [02800, 05004], lr: 0.022937, loss: 2.2607
2022-03-04 06:54:50 - train: epoch 0139, iter [02900, 05004], lr: 0.022937, loss: 2.1444
2022-03-04 06:55:23 - train: epoch 0139, iter [03000, 05004], lr: 0.022937, loss: 1.8660
2022-03-04 06:55:55 - train: epoch 0139, iter [03100, 05004], lr: 0.022937, loss: 2.3732
2022-03-04 06:56:27 - train: epoch 0139, iter [03200, 05004], lr: 0.022937, loss: 2.3835
2022-03-04 06:56:59 - train: epoch 0139, iter [03300, 05004], lr: 0.022937, loss: 2.1066
2022-03-04 06:57:30 - train: epoch 0139, iter [03400, 05004], lr: 0.022937, loss: 1.9901
2022-03-04 06:58:03 - train: epoch 0139, iter [03500, 05004], lr: 0.022937, loss: 2.3487
2022-03-04 06:58:34 - train: epoch 0139, iter [03600, 05004], lr: 0.022937, loss: 1.8799
2022-03-04 06:59:07 - train: epoch 0139, iter [03700, 05004], lr: 0.022937, loss: 2.0560
2022-03-04 06:59:39 - train: epoch 0139, iter [03800, 05004], lr: 0.022937, loss: 2.2425
2022-03-04 07:00:12 - train: epoch 0139, iter [03900, 05004], lr: 0.022937, loss: 1.7942
2022-03-04 07:00:43 - train: epoch 0139, iter [04000, 05004], lr: 0.022937, loss: 2.0819
2022-03-04 07:01:15 - train: epoch 0139, iter [04100, 05004], lr: 0.022937, loss: 2.0513
2022-03-04 07:01:47 - train: epoch 0139, iter [04200, 05004], lr: 0.022937, loss: 2.2935
2022-03-04 07:02:20 - train: epoch 0139, iter [04300, 05004], lr: 0.022937, loss: 2.0741
2022-03-04 07:02:52 - train: epoch 0139, iter [04400, 05004], lr: 0.022937, loss: 2.5733
2022-03-04 07:03:22 - train: epoch 0139, iter [04500, 05004], lr: 0.022937, loss: 2.3478
2022-03-04 07:03:56 - train: epoch 0139, iter [04600, 05004], lr: 0.022937, loss: 2.1589
2022-03-04 07:04:28 - train: epoch 0139, iter [04700, 05004], lr: 0.022937, loss: 2.1523
2022-03-04 07:05:00 - train: epoch 0139, iter [04800, 05004], lr: 0.022937, loss: 2.0035
2022-03-04 07:05:32 - train: epoch 0139, iter [04900, 05004], lr: 0.022937, loss: 2.1990
2022-03-04 07:06:03 - train: epoch 0139, iter [05000, 05004], lr: 0.022937, loss: 2.4041
2022-03-04 07:06:04 - train: epoch 139, train_loss: 2.1070
2022-03-04 07:07:14 - eval: epoch: 139, acc1: 65.380%, acc5: 86.936%, test_loss: 1.4258, per_image_load_time: 0.806ms, per_image_inference_time: 0.341ms
2022-03-04 07:07:14 - until epoch: 139, best_acc1: 66.502%
2022-03-04 07:07:14 - epoch 140 lr: 0.022263363623243056
2022-03-04 07:07:51 - train: epoch 0140, iter [00100, 05004], lr: 0.022263, loss: 1.9522
2022-03-04 07:08:23 - train: epoch 0140, iter [00200, 05004], lr: 0.022263, loss: 2.2139
2022-03-04 07:08:56 - train: epoch 0140, iter [00300, 05004], lr: 0.022263, loss: 2.1149
2022-03-04 07:09:27 - train: epoch 0140, iter [00400, 05004], lr: 0.022263, loss: 1.8791
2022-03-04 07:09:59 - train: epoch 0140, iter [00500, 05004], lr: 0.022263, loss: 2.1365
2022-03-04 07:10:32 - train: epoch 0140, iter [00600, 05004], lr: 0.022263, loss: 2.1193
2022-03-04 07:11:03 - train: epoch 0140, iter [00700, 05004], lr: 0.022263, loss: 1.9977
2022-03-04 07:11:35 - train: epoch 0140, iter [00800, 05004], lr: 0.022263, loss: 2.4527
2022-03-04 07:12:08 - train: epoch 0140, iter [00900, 05004], lr: 0.022263, loss: 1.9528
2022-03-04 07:12:40 - train: epoch 0140, iter [01000, 05004], lr: 0.022263, loss: 2.0995
2022-03-04 07:13:12 - train: epoch 0140, iter [01100, 05004], lr: 0.022263, loss: 1.8778
2022-03-04 07:13:45 - train: epoch 0140, iter [01200, 05004], lr: 0.022263, loss: 2.0603
2022-03-04 07:14:16 - train: epoch 0140, iter [01300, 05004], lr: 0.022263, loss: 2.4880
2022-03-04 07:14:48 - train: epoch 0140, iter [01400, 05004], lr: 0.022263, loss: 2.1793
2022-03-04 07:15:20 - train: epoch 0140, iter [01500, 05004], lr: 0.022263, loss: 1.8967
2022-03-04 07:15:52 - train: epoch 0140, iter [01600, 05004], lr: 0.022263, loss: 2.1995
2022-03-04 07:16:24 - train: epoch 0140, iter [01700, 05004], lr: 0.022263, loss: 2.1449
2022-03-04 07:16:57 - train: epoch 0140, iter [01800, 05004], lr: 0.022263, loss: 2.0636
2022-03-04 07:17:29 - train: epoch 0140, iter [01900, 05004], lr: 0.022263, loss: 2.2966
2022-03-04 07:18:02 - train: epoch 0140, iter [02000, 05004], lr: 0.022263, loss: 1.9007
2022-03-04 07:18:33 - train: epoch 0140, iter [02100, 05004], lr: 0.022263, loss: 1.9903
2022-03-04 07:19:05 - train: epoch 0140, iter [02200, 05004], lr: 0.022263, loss: 2.2367
2022-03-04 07:19:37 - train: epoch 0140, iter [02300, 05004], lr: 0.022263, loss: 1.8678
2022-03-04 07:20:09 - train: epoch 0140, iter [02400, 05004], lr: 0.022263, loss: 1.9206
2022-03-04 07:20:41 - train: epoch 0140, iter [02500, 05004], lr: 0.022263, loss: 1.9946
2022-03-04 07:21:14 - train: epoch 0140, iter [02600, 05004], lr: 0.022263, loss: 2.0575
2022-03-04 07:21:46 - train: epoch 0140, iter [02700, 05004], lr: 0.022263, loss: 2.0459
2022-03-04 07:22:18 - train: epoch 0140, iter [02800, 05004], lr: 0.022263, loss: 1.9841
2022-03-04 07:22:50 - train: epoch 0140, iter [02900, 05004], lr: 0.022263, loss: 2.1455
2022-03-04 07:23:23 - train: epoch 0140, iter [03000, 05004], lr: 0.022263, loss: 2.0126
2022-03-04 07:23:54 - train: epoch 0140, iter [03100, 05004], lr: 0.022263, loss: 2.1742
2022-03-04 07:24:26 - train: epoch 0140, iter [03200, 05004], lr: 0.022263, loss: 2.0517
2022-03-04 07:24:58 - train: epoch 0140, iter [03300, 05004], lr: 0.022263, loss: 2.1485
2022-03-04 07:25:30 - train: epoch 0140, iter [03400, 05004], lr: 0.022263, loss: 2.1712
2022-03-04 07:26:02 - train: epoch 0140, iter [03500, 05004], lr: 0.022263, loss: 2.2016
2022-03-04 07:26:34 - train: epoch 0140, iter [03600, 05004], lr: 0.022263, loss: 2.1394
2022-03-04 07:27:06 - train: epoch 0140, iter [03700, 05004], lr: 0.022263, loss: 2.0295
2022-03-04 07:27:38 - train: epoch 0140, iter [03800, 05004], lr: 0.022263, loss: 2.3770
2022-03-04 07:28:09 - train: epoch 0140, iter [03900, 05004], lr: 0.022263, loss: 1.9393
2022-03-04 07:28:42 - train: epoch 0140, iter [04000, 05004], lr: 0.022263, loss: 2.1684
2022-03-04 07:29:13 - train: epoch 0140, iter [04100, 05004], lr: 0.022263, loss: 2.2574
2022-03-04 07:29:45 - train: epoch 0140, iter [04200, 05004], lr: 0.022263, loss: 1.8584
2022-03-04 07:30:18 - train: epoch 0140, iter [04300, 05004], lr: 0.022263, loss: 2.3842
2022-03-04 07:30:49 - train: epoch 0140, iter [04400, 05004], lr: 0.022263, loss: 2.3024
2022-03-04 07:31:21 - train: epoch 0140, iter [04500, 05004], lr: 0.022263, loss: 1.8991
2022-03-04 07:31:53 - train: epoch 0140, iter [04600, 05004], lr: 0.022263, loss: 1.9844
2022-03-04 07:32:25 - train: epoch 0140, iter [04700, 05004], lr: 0.022263, loss: 2.0607
2022-03-04 07:32:56 - train: epoch 0140, iter [04800, 05004], lr: 0.022263, loss: 2.1301
2022-03-04 07:33:29 - train: epoch 0140, iter [04900, 05004], lr: 0.022263, loss: 1.9682
2022-03-04 07:34:00 - train: epoch 0140, iter [05000, 05004], lr: 0.022263, loss: 2.1612
2022-03-04 07:34:01 - train: epoch 140, train_loss: 2.0934
2022-03-04 07:35:11 - eval: epoch: 140, acc1: 65.706%, acc5: 87.264%, test_loss: 1.3982, per_image_load_time: 0.483ms, per_image_inference_time: 0.373ms
2022-03-04 07:35:12 - until epoch: 140, best_acc1: 66.502%
2022-03-04 07:35:12 - epoch 141 lr: 0.021596762663442216
2022-03-04 07:35:49 - train: epoch 0141, iter [00100, 05004], lr: 0.021597, loss: 2.1867
2022-03-04 07:36:22 - train: epoch 0141, iter [00200, 05004], lr: 0.021597, loss: 1.8449
2022-03-04 07:36:53 - train: epoch 0141, iter [00300, 05004], lr: 0.021597, loss: 1.9468
2022-03-04 07:37:25 - train: epoch 0141, iter [00400, 05004], lr: 0.021597, loss: 1.9727
2022-03-04 07:37:58 - train: epoch 0141, iter [00500, 05004], lr: 0.021597, loss: 2.2235
2022-03-04 07:38:30 - train: epoch 0141, iter [00600, 05004], lr: 0.021597, loss: 2.4724
2022-03-04 07:39:02 - train: epoch 0141, iter [00700, 05004], lr: 0.021597, loss: 2.1092
2022-03-04 07:39:34 - train: epoch 0141, iter [00800, 05004], lr: 0.021597, loss: 1.9984
2022-03-04 07:40:06 - train: epoch 0141, iter [00900, 05004], lr: 0.021597, loss: 2.1421
2022-03-04 07:40:38 - train: epoch 0141, iter [01000, 05004], lr: 0.021597, loss: 2.0797
2022-03-04 07:41:10 - train: epoch 0141, iter [01100, 05004], lr: 0.021597, loss: 2.2987
2022-03-04 07:41:41 - train: epoch 0141, iter [01200, 05004], lr: 0.021597, loss: 2.3403
2022-03-04 07:42:14 - train: epoch 0141, iter [01300, 05004], lr: 0.021597, loss: 2.2105
2022-03-04 07:42:46 - train: epoch 0141, iter [01400, 05004], lr: 0.021597, loss: 1.9134
2022-03-04 07:43:18 - train: epoch 0141, iter [01500, 05004], lr: 0.021597, loss: 1.8385
2022-03-04 07:43:50 - train: epoch 0141, iter [01600, 05004], lr: 0.021597, loss: 1.9902
2022-03-04 07:44:22 - train: epoch 0141, iter [01700, 05004], lr: 0.021597, loss: 2.0607
2022-03-04 07:44:54 - train: epoch 0141, iter [01800, 05004], lr: 0.021597, loss: 2.1748
2022-03-04 07:45:26 - train: epoch 0141, iter [01900, 05004], lr: 0.021597, loss: 2.0714
2022-03-04 07:45:58 - train: epoch 0141, iter [02000, 05004], lr: 0.021597, loss: 1.9886
2022-03-04 07:46:30 - train: epoch 0141, iter [02100, 05004], lr: 0.021597, loss: 1.9399
2022-03-04 07:47:02 - train: epoch 0141, iter [02200, 05004], lr: 0.021597, loss: 2.0973
2022-03-04 07:47:34 - train: epoch 0141, iter [02300, 05004], lr: 0.021597, loss: 1.8007
2022-03-04 07:48:05 - train: epoch 0141, iter [02400, 05004], lr: 0.021597, loss: 2.0642
2022-03-04 07:48:38 - train: epoch 0141, iter [02500, 05004], lr: 0.021597, loss: 2.2925
2022-03-04 07:49:10 - train: epoch 0141, iter [02600, 05004], lr: 0.021597, loss: 2.1282
2022-03-04 07:49:42 - train: epoch 0141, iter [02700, 05004], lr: 0.021597, loss: 2.1328
2022-03-04 07:50:14 - train: epoch 0141, iter [02800, 05004], lr: 0.021597, loss: 2.0348
2022-03-04 07:50:46 - train: epoch 0141, iter [02900, 05004], lr: 0.021597, loss: 2.0597
2022-03-04 07:51:18 - train: epoch 0141, iter [03000, 05004], lr: 0.021597, loss: 2.1307
2022-03-04 07:51:50 - train: epoch 0141, iter [03100, 05004], lr: 0.021597, loss: 2.2207
2022-03-04 07:52:22 - train: epoch 0141, iter [03200, 05004], lr: 0.021597, loss: 2.2063
2022-03-04 07:52:54 - train: epoch 0141, iter [03300, 05004], lr: 0.021597, loss: 2.1589
2022-03-04 07:53:26 - train: epoch 0141, iter [03400, 05004], lr: 0.021597, loss: 2.3144
2022-03-04 07:53:59 - train: epoch 0141, iter [03500, 05004], lr: 0.021597, loss: 2.3806
2022-03-04 07:54:30 - train: epoch 0141, iter [03600, 05004], lr: 0.021597, loss: 1.9240
2022-03-04 07:55:03 - train: epoch 0141, iter [03700, 05004], lr: 0.021597, loss: 1.9216
2022-03-04 07:55:34 - train: epoch 0141, iter [03800, 05004], lr: 0.021597, loss: 2.0281
2022-03-04 07:56:06 - train: epoch 0141, iter [03900, 05004], lr: 0.021597, loss: 2.0468
2022-03-04 07:56:38 - train: epoch 0141, iter [04000, 05004], lr: 0.021597, loss: 2.1501
2022-03-04 07:57:10 - train: epoch 0141, iter [04100, 05004], lr: 0.021597, loss: 1.9762
2022-03-04 07:57:41 - train: epoch 0141, iter [04200, 05004], lr: 0.021597, loss: 2.2518
2022-03-04 07:58:13 - train: epoch 0141, iter [04300, 05004], lr: 0.021597, loss: 2.0907
2022-03-04 07:58:45 - train: epoch 0141, iter [04400, 05004], lr: 0.021597, loss: 2.1910
2022-03-04 07:59:17 - train: epoch 0141, iter [04500, 05004], lr: 0.021597, loss: 2.0767
2022-03-04 07:59:50 - train: epoch 0141, iter [04600, 05004], lr: 0.021597, loss: 2.0668
2022-03-04 08:00:22 - train: epoch 0141, iter [04700, 05004], lr: 0.021597, loss: 2.1786
2022-03-04 08:00:54 - train: epoch 0141, iter [04800, 05004], lr: 0.021597, loss: 1.9223
2022-03-04 08:01:26 - train: epoch 0141, iter [04900, 05004], lr: 0.021597, loss: 2.2473
2022-03-04 08:01:56 - train: epoch 0141, iter [05000, 05004], lr: 0.021597, loss: 2.1333
2022-03-04 08:01:57 - train: epoch 141, train_loss: 2.0829
2022-03-04 08:03:07 - eval: epoch: 141, acc1: 66.974%, acc5: 88.220%, test_loss: 1.3244, per_image_load_time: 0.767ms, per_image_inference_time: 0.338ms
2022-03-04 08:03:08 - until epoch: 141, best_acc1: 66.974%
2022-03-04 08:03:08 - epoch 142 lr: 0.020937533765518185
2022-03-04 08:03:45 - train: epoch 0142, iter [00100, 05004], lr: 0.020938, loss: 2.3420
2022-03-04 08:04:17 - train: epoch 0142, iter [00200, 05004], lr: 0.020938, loss: 1.9134
2022-03-04 08:04:49 - train: epoch 0142, iter [00300, 05004], lr: 0.020938, loss: 2.1567
2022-03-04 08:05:21 - train: epoch 0142, iter [00400, 05004], lr: 0.020938, loss: 1.9658
2022-03-04 08:05:54 - train: epoch 0142, iter [00500, 05004], lr: 0.020938, loss: 2.1495
2022-03-04 08:06:26 - train: epoch 0142, iter [00600, 05004], lr: 0.020938, loss: 2.0607
2022-03-04 08:06:59 - train: epoch 0142, iter [00700, 05004], lr: 0.020938, loss: 2.1740
2022-03-04 08:07:30 - train: epoch 0142, iter [00800, 05004], lr: 0.020938, loss: 2.2377
2022-03-04 08:08:02 - train: epoch 0142, iter [00900, 05004], lr: 0.020938, loss: 1.8247
2022-03-04 08:08:34 - train: epoch 0142, iter [01000, 05004], lr: 0.020938, loss: 2.2050
2022-03-04 08:09:06 - train: epoch 0142, iter [01100, 05004], lr: 0.020938, loss: 2.0042
2022-03-04 08:09:38 - train: epoch 0142, iter [01200, 05004], lr: 0.020938, loss: 1.9582
2022-03-04 08:10:10 - train: epoch 0142, iter [01300, 05004], lr: 0.020938, loss: 2.0146
2022-03-04 08:10:42 - train: epoch 0142, iter [01400, 05004], lr: 0.020938, loss: 1.9482
2022-03-04 08:11:14 - train: epoch 0142, iter [01500, 05004], lr: 0.020938, loss: 2.1374
2022-03-04 08:11:47 - train: epoch 0142, iter [01600, 05004], lr: 0.020938, loss: 2.1388
2022-03-04 08:12:19 - train: epoch 0142, iter [01700, 05004], lr: 0.020938, loss: 2.1885
2022-03-04 08:12:52 - train: epoch 0142, iter [01800, 05004], lr: 0.020938, loss: 2.0431
2022-03-04 08:13:23 - train: epoch 0142, iter [01900, 05004], lr: 0.020938, loss: 2.1900
2022-03-04 08:13:56 - train: epoch 0142, iter [02000, 05004], lr: 0.020938, loss: 2.0728
2022-03-04 08:14:27 - train: epoch 0142, iter [02100, 05004], lr: 0.020938, loss: 2.1262
2022-03-04 08:14:59 - train: epoch 0142, iter [02200, 05004], lr: 0.020938, loss: 2.0686
2022-03-04 08:15:31 - train: epoch 0142, iter [02300, 05004], lr: 0.020938, loss: 1.9570
2022-03-04 08:16:04 - train: epoch 0142, iter [02400, 05004], lr: 0.020938, loss: 2.0291
2022-03-04 08:16:36 - train: epoch 0142, iter [02500, 05004], lr: 0.020938, loss: 2.0548
2022-03-04 08:17:09 - train: epoch 0142, iter [02600, 05004], lr: 0.020938, loss: 1.9398
2022-03-04 08:17:40 - train: epoch 0142, iter [02700, 05004], lr: 0.020938, loss: 1.9923
2022-03-04 08:18:12 - train: epoch 0142, iter [02800, 05004], lr: 0.020938, loss: 2.0700
2022-03-04 08:18:44 - train: epoch 0142, iter [02900, 05004], lr: 0.020938, loss: 1.8664
2022-03-04 08:19:17 - train: epoch 0142, iter [03000, 05004], lr: 0.020938, loss: 2.0439
2022-03-04 08:19:49 - train: epoch 0142, iter [03100, 05004], lr: 0.020938, loss: 2.2024
2022-03-04 08:20:21 - train: epoch 0142, iter [03200, 05004], lr: 0.020938, loss: 2.4582
2022-03-04 08:20:53 - train: epoch 0142, iter [03300, 05004], lr: 0.020938, loss: 1.9317
2022-03-04 08:21:24 - train: epoch 0142, iter [03400, 05004], lr: 0.020938, loss: 2.0101
2022-03-04 08:21:56 - train: epoch 0142, iter [03500, 05004], lr: 0.020938, loss: 2.0801
2022-03-04 08:22:30 - train: epoch 0142, iter [03600, 05004], lr: 0.020938, loss: 2.0730
2022-03-04 08:23:01 - train: epoch 0142, iter [03700, 05004], lr: 0.020938, loss: 2.0610
2022-03-04 08:23:34 - train: epoch 0142, iter [03800, 05004], lr: 0.020938, loss: 2.4203
2022-03-04 08:24:05 - train: epoch 0142, iter [03900, 05004], lr: 0.020938, loss: 2.2141
2022-03-04 08:24:37 - train: epoch 0142, iter [04000, 05004], lr: 0.020938, loss: 2.2056
2022-03-04 08:25:10 - train: epoch 0142, iter [04100, 05004], lr: 0.020938, loss: 2.2068
2022-03-04 08:25:41 - train: epoch 0142, iter [04200, 05004], lr: 0.020938, loss: 1.9253
2022-03-04 08:26:13 - train: epoch 0142, iter [04300, 05004], lr: 0.020938, loss: 1.9736
2022-03-04 08:26:46 - train: epoch 0142, iter [04400, 05004], lr: 0.020938, loss: 2.0117
2022-03-04 08:27:16 - train: epoch 0142, iter [04500, 05004], lr: 0.020938, loss: 2.0802
2022-03-04 08:27:49 - train: epoch 0142, iter [04600, 05004], lr: 0.020938, loss: 2.4287
2022-03-04 08:28:21 - train: epoch 0142, iter [04700, 05004], lr: 0.020938, loss: 2.1911
2022-03-04 08:28:54 - train: epoch 0142, iter [04800, 05004], lr: 0.020938, loss: 1.9886
2022-03-04 08:29:26 - train: epoch 0142, iter [04900, 05004], lr: 0.020938, loss: 2.1653
2022-03-04 08:29:56 - train: epoch 0142, iter [05000, 05004], lr: 0.020938, loss: 2.1456
2022-03-04 08:29:57 - train: epoch 142, train_loss: 2.0745
2022-03-04 08:31:07 - eval: epoch: 142, acc1: 67.382%, acc5: 88.238%, test_loss: 1.3276, per_image_load_time: 1.332ms, per_image_inference_time: 0.375ms
2022-03-04 08:31:08 - until epoch: 142, best_acc1: 67.382%
2022-03-04 08:31:08 - epoch 143 lr: 0.020285848032369137
2022-03-04 08:31:45 - train: epoch 0143, iter [00100, 05004], lr: 0.020286, loss: 2.0405
2022-03-04 08:32:18 - train: epoch 0143, iter [00200, 05004], lr: 0.020286, loss: 2.0254
2022-03-04 08:32:49 - train: epoch 0143, iter [00300, 05004], lr: 0.020286, loss: 1.9715
2022-03-04 08:33:21 - train: epoch 0143, iter [00400, 05004], lr: 0.020286, loss: 1.7952
2022-03-04 08:33:53 - train: epoch 0143, iter [00500, 05004], lr: 0.020286, loss: 1.9945
2022-03-04 08:34:26 - train: epoch 0143, iter [00600, 05004], lr: 0.020286, loss: 2.1412
2022-03-04 08:34:58 - train: epoch 0143, iter [00700, 05004], lr: 0.020286, loss: 2.1698
2022-03-04 08:35:30 - train: epoch 0143, iter [00800, 05004], lr: 0.020286, loss: 2.1351
2022-03-04 08:36:01 - train: epoch 0143, iter [00900, 05004], lr: 0.020286, loss: 2.2851
2022-03-04 08:36:34 - train: epoch 0143, iter [01000, 05004], lr: 0.020286, loss: 1.9117
2022-03-04 08:37:05 - train: epoch 0143, iter [01100, 05004], lr: 0.020286, loss: 2.2087
2022-03-04 08:37:37 - train: epoch 0143, iter [01200, 05004], lr: 0.020286, loss: 1.9866
2022-03-04 08:38:09 - train: epoch 0143, iter [01300, 05004], lr: 0.020286, loss: 2.0856
2022-03-04 08:38:41 - train: epoch 0143, iter [01400, 05004], lr: 0.020286, loss: 2.0058
2022-03-04 08:39:13 - train: epoch 0143, iter [01500, 05004], lr: 0.020286, loss: 1.9915
2022-03-04 08:39:45 - train: epoch 0143, iter [01600, 05004], lr: 0.020286, loss: 1.9527
2022-03-04 08:40:18 - train: epoch 0143, iter [01700, 05004], lr: 0.020286, loss: 2.0254
2022-03-04 08:40:49 - train: epoch 0143, iter [01800, 05004], lr: 0.020286, loss: 2.2268
2022-03-04 08:41:21 - train: epoch 0143, iter [01900, 05004], lr: 0.020286, loss: 1.9312
2022-03-04 08:41:52 - train: epoch 0143, iter [02000, 05004], lr: 0.020286, loss: 2.3732
2022-03-04 08:42:25 - train: epoch 0143, iter [02100, 05004], lr: 0.020286, loss: 1.8852
2022-03-04 08:42:57 - train: epoch 0143, iter [02200, 05004], lr: 0.020286, loss: 2.1188
2022-03-04 08:43:29 - train: epoch 0143, iter [02300, 05004], lr: 0.020286, loss: 2.0656
2022-03-04 08:44:00 - train: epoch 0143, iter [02400, 05004], lr: 0.020286, loss: 2.1059
2022-03-04 08:44:33 - train: epoch 0143, iter [02500, 05004], lr: 0.020286, loss: 2.2863
2022-03-04 08:45:05 - train: epoch 0143, iter [02600, 05004], lr: 0.020286, loss: 2.3171
2022-03-04 08:45:37 - train: epoch 0143, iter [02700, 05004], lr: 0.020286, loss: 1.9016
2022-03-04 08:46:09 - train: epoch 0143, iter [02800, 05004], lr: 0.020286, loss: 2.2403
2022-03-04 08:46:41 - train: epoch 0143, iter [02900, 05004], lr: 0.020286, loss: 2.0024
2022-03-04 08:47:13 - train: epoch 0143, iter [03000, 05004], lr: 0.020286, loss: 2.1841
2022-03-04 08:47:46 - train: epoch 0143, iter [03100, 05004], lr: 0.020286, loss: 1.8955
2022-03-04 08:48:17 - train: epoch 0143, iter [03200, 05004], lr: 0.020286, loss: 2.1113
2022-03-04 08:48:49 - train: epoch 0143, iter [03300, 05004], lr: 0.020286, loss: 2.0319
2022-03-04 08:49:21 - train: epoch 0143, iter [03400, 05004], lr: 0.020286, loss: 1.9955
2022-03-04 08:49:54 - train: epoch 0143, iter [03500, 05004], lr: 0.020286, loss: 2.1336
2022-03-04 08:50:26 - train: epoch 0143, iter [03600, 05004], lr: 0.020286, loss: 2.3163
2022-03-04 08:50:58 - train: epoch 0143, iter [03700, 05004], lr: 0.020286, loss: 2.1958
2022-03-04 08:51:29 - train: epoch 0143, iter [03800, 05004], lr: 0.020286, loss: 1.8673
2022-03-04 08:52:02 - train: epoch 0143, iter [03900, 05004], lr: 0.020286, loss: 2.0339
2022-03-04 08:52:33 - train: epoch 0143, iter [04000, 05004], lr: 0.020286, loss: 2.0509
2022-03-04 08:53:05 - train: epoch 0143, iter [04100, 05004], lr: 0.020286, loss: 2.1280
2022-03-04 08:53:37 - train: epoch 0143, iter [04200, 05004], lr: 0.020286, loss: 1.8891
2022-03-04 08:54:09 - train: epoch 0143, iter [04300, 05004], lr: 0.020286, loss: 2.2883
2022-03-04 08:54:41 - train: epoch 0143, iter [04400, 05004], lr: 0.020286, loss: 2.2546
2022-03-04 08:55:13 - train: epoch 0143, iter [04500, 05004], lr: 0.020286, loss: 1.9634
2022-03-04 08:55:44 - train: epoch 0143, iter [04600, 05004], lr: 0.020286, loss: 2.1051
2022-03-04 08:56:17 - train: epoch 0143, iter [04700, 05004], lr: 0.020286, loss: 2.2396
2022-03-04 08:56:50 - train: epoch 0143, iter [04800, 05004], lr: 0.020286, loss: 1.9319
2022-03-04 08:57:22 - train: epoch 0143, iter [04900, 05004], lr: 0.020286, loss: 2.0549
2022-03-04 08:57:53 - train: epoch 0143, iter [05000, 05004], lr: 0.020286, loss: 2.1964
2022-03-04 08:57:54 - train: epoch 143, train_loss: 2.0633
2022-03-04 08:59:04 - eval: epoch: 143, acc1: 67.614%, acc5: 88.586%, test_loss: 1.3043, per_image_load_time: 1.942ms, per_image_inference_time: 0.344ms
2022-03-04 08:59:05 - until epoch: 143, best_acc1: 67.614%
2022-03-04 08:59:05 - epoch 144 lr: 0.01964187460906444
2022-03-04 08:59:42 - train: epoch 0144, iter [00100, 05004], lr: 0.019642, loss: 2.3317
2022-03-04 09:00:14 - train: epoch 0144, iter [00200, 05004], lr: 0.019642, loss: 1.7273
2022-03-04 09:00:46 - train: epoch 0144, iter [00300, 05004], lr: 0.019642, loss: 2.0560
2022-03-04 09:01:17 - train: epoch 0144, iter [00400, 05004], lr: 0.019642, loss: 2.2082
2022-03-04 09:01:49 - train: epoch 0144, iter [00500, 05004], lr: 0.019642, loss: 2.0473
2022-03-04 09:02:21 - train: epoch 0144, iter [00600, 05004], lr: 0.019642, loss: 2.1008
2022-03-04 09:02:54 - train: epoch 0144, iter [00700, 05004], lr: 0.019642, loss: 1.9860
2022-03-04 09:03:25 - train: epoch 0144, iter [00800, 05004], lr: 0.019642, loss: 2.3905
2022-03-04 09:03:57 - train: epoch 0144, iter [00900, 05004], lr: 0.019642, loss: 2.0381
2022-03-04 09:04:29 - train: epoch 0144, iter [01000, 05004], lr: 0.019642, loss: 1.8836
2022-03-04 09:05:02 - train: epoch 0144, iter [01100, 05004], lr: 0.019642, loss: 2.1358
2022-03-04 09:05:33 - train: epoch 0144, iter [01200, 05004], lr: 0.019642, loss: 2.2627
2022-03-04 09:06:05 - train: epoch 0144, iter [01300, 05004], lr: 0.019642, loss: 2.1238
2022-03-04 09:06:37 - train: epoch 0144, iter [01400, 05004], lr: 0.019642, loss: 1.9961
2022-03-04 09:07:09 - train: epoch 0144, iter [01500, 05004], lr: 0.019642, loss: 2.0326
2022-03-04 09:07:41 - train: epoch 0144, iter [01600, 05004], lr: 0.019642, loss: 1.7519
2022-03-04 09:08:13 - train: epoch 0144, iter [01700, 05004], lr: 0.019642, loss: 1.7671
2022-03-04 09:08:45 - train: epoch 0144, iter [01800, 05004], lr: 0.019642, loss: 2.2073
2022-03-04 09:09:17 - train: epoch 0144, iter [01900, 05004], lr: 0.019642, loss: 1.9770
2022-03-04 09:09:49 - train: epoch 0144, iter [02000, 05004], lr: 0.019642, loss: 2.3809
2022-03-04 09:10:20 - train: epoch 0144, iter [02100, 05004], lr: 0.019642, loss: 1.9211
2022-03-04 09:10:52 - train: epoch 0144, iter [02200, 05004], lr: 0.019642, loss: 2.1138
2022-03-04 09:11:25 - train: epoch 0144, iter [02300, 05004], lr: 0.019642, loss: 1.7120
2022-03-04 09:11:57 - train: epoch 0144, iter [02400, 05004], lr: 0.019642, loss: 2.3272
2022-03-04 09:12:28 - train: epoch 0144, iter [02500, 05004], lr: 0.019642, loss: 1.8302
2022-03-04 09:13:00 - train: epoch 0144, iter [02600, 05004], lr: 0.019642, loss: 2.1406
2022-03-04 09:13:32 - train: epoch 0144, iter [02700, 05004], lr: 0.019642, loss: 1.9532
2022-03-04 09:14:04 - train: epoch 0144, iter [02800, 05004], lr: 0.019642, loss: 1.9815
2022-03-04 09:14:36 - train: epoch 0144, iter [02900, 05004], lr: 0.019642, loss: 1.9006
2022-03-04 09:15:09 - train: epoch 0144, iter [03000, 05004], lr: 0.019642, loss: 2.3467
2022-03-04 09:15:40 - train: epoch 0144, iter [03100, 05004], lr: 0.019642, loss: 1.9320
2022-03-04 09:16:12 - train: epoch 0144, iter [03200, 05004], lr: 0.019642, loss: 2.1360
2022-03-04 09:16:44 - train: epoch 0144, iter [03300, 05004], lr: 0.019642, loss: 2.2382
2022-03-04 09:17:16 - train: epoch 0144, iter [03400, 05004], lr: 0.019642, loss: 2.0952
2022-03-04 09:17:48 - train: epoch 0144, iter [03500, 05004], lr: 0.019642, loss: 2.2205
2022-03-04 09:18:19 - train: epoch 0144, iter [03600, 05004], lr: 0.019642, loss: 2.1070
2022-03-04 09:18:51 - train: epoch 0144, iter [03700, 05004], lr: 0.019642, loss: 2.1345
2022-03-04 09:19:23 - train: epoch 0144, iter [03800, 05004], lr: 0.019642, loss: 1.9858
2022-03-04 09:19:56 - train: epoch 0144, iter [03900, 05004], lr: 0.019642, loss: 2.1024
2022-03-04 09:20:27 - train: epoch 0144, iter [04000, 05004], lr: 0.019642, loss: 1.8305
2022-03-04 09:20:59 - train: epoch 0144, iter [04100, 05004], lr: 0.019642, loss: 2.1329
2022-03-04 09:21:30 - train: epoch 0144, iter [04200, 05004], lr: 0.019642, loss: 2.0647
2022-03-04 09:22:03 - train: epoch 0144, iter [04300, 05004], lr: 0.019642, loss: 2.3993
2022-03-04 09:22:35 - train: epoch 0144, iter [04400, 05004], lr: 0.019642, loss: 2.2859
2022-03-04 09:23:07 - train: epoch 0144, iter [04500, 05004], lr: 0.019642, loss: 2.0206
2022-03-04 09:23:39 - train: epoch 0144, iter [04600, 05004], lr: 0.019642, loss: 2.2134
2022-03-04 09:24:10 - train: epoch 0144, iter [04700, 05004], lr: 0.019642, loss: 2.0132
2022-03-04 09:24:43 - train: epoch 0144, iter [04800, 05004], lr: 0.019642, loss: 2.1326
2022-03-04 09:25:14 - train: epoch 0144, iter [04900, 05004], lr: 0.019642, loss: 2.2037
2022-03-04 09:25:46 - train: epoch 0144, iter [05000, 05004], lr: 0.019642, loss: 2.3885
2022-03-04 09:25:47 - train: epoch 144, train_loss: 2.0481
2022-03-04 09:26:56 - eval: epoch: 144, acc1: 66.450%, acc5: 88.040%, test_loss: 1.3539, per_image_load_time: 1.052ms, per_image_inference_time: 0.336ms
2022-03-04 09:26:56 - until epoch: 144, best_acc1: 67.614%
2022-03-04 09:26:56 - epoch 145 lr: 0.019005780638942982
2022-03-04 09:27:33 - train: epoch 0145, iter [00100, 05004], lr: 0.019006, loss: 1.8092
2022-03-04 09:28:06 - train: epoch 0145, iter [00200, 05004], lr: 0.019006, loss: 1.8455
2022-03-04 09:28:37 - train: epoch 0145, iter [00300, 05004], lr: 0.019006, loss: 1.8656
2022-03-04 09:29:09 - train: epoch 0145, iter [00400, 05004], lr: 0.019006, loss: 1.8497
2022-03-04 09:29:41 - train: epoch 0145, iter [00500, 05004], lr: 0.019006, loss: 2.0328
2022-03-04 09:30:13 - train: epoch 0145, iter [00600, 05004], lr: 0.019006, loss: 1.9577
2022-03-04 09:30:46 - train: epoch 0145, iter [00700, 05004], lr: 0.019006, loss: 2.2875
2022-03-04 09:31:17 - train: epoch 0145, iter [00800, 05004], lr: 0.019006, loss: 2.0102
2022-03-04 09:31:50 - train: epoch 0145, iter [00900, 05004], lr: 0.019006, loss: 1.8475
2022-03-04 09:32:21 - train: epoch 0145, iter [01000, 05004], lr: 0.019006, loss: 1.8956
2022-03-04 09:32:53 - train: epoch 0145, iter [01100, 05004], lr: 0.019006, loss: 1.9689
2022-03-04 09:33:25 - train: epoch 0145, iter [01200, 05004], lr: 0.019006, loss: 2.0688
2022-03-04 09:33:57 - train: epoch 0145, iter [01300, 05004], lr: 0.019006, loss: 2.1208
2022-03-04 09:34:30 - train: epoch 0145, iter [01400, 05004], lr: 0.019006, loss: 2.0103
2022-03-04 09:35:01 - train: epoch 0145, iter [01500, 05004], lr: 0.019006, loss: 1.9944
2022-03-04 09:35:33 - train: epoch 0145, iter [01600, 05004], lr: 0.019006, loss: 2.0132
2022-03-04 09:36:04 - train: epoch 0145, iter [01700, 05004], lr: 0.019006, loss: 2.0865
2022-03-04 09:36:36 - train: epoch 0145, iter [01800, 05004], lr: 0.019006, loss: 1.8766
2022-03-04 09:37:13 - train: epoch 0145, iter [01900, 05004], lr: 0.019006, loss: 1.8086
2022-03-04 09:37:49 - train: epoch 0145, iter [02000, 05004], lr: 0.019006, loss: 1.9633
2022-03-04 09:38:20 - train: epoch 0145, iter [02100, 05004], lr: 0.019006, loss: 2.2131
2022-03-04 09:38:52 - train: epoch 0145, iter [02200, 05004], lr: 0.019006, loss: 2.0989
2022-03-04 09:39:24 - train: epoch 0145, iter [02300, 05004], lr: 0.019006, loss: 2.0640
2022-03-04 09:39:55 - train: epoch 0145, iter [02400, 05004], lr: 0.019006, loss: 1.9936
2022-03-04 09:40:28 - train: epoch 0145, iter [02500, 05004], lr: 0.019006, loss: 2.2454
2022-03-04 09:41:00 - train: epoch 0145, iter [02600, 05004], lr: 0.019006, loss: 2.0870
2022-03-04 09:41:32 - train: epoch 0145, iter [02700, 05004], lr: 0.019006, loss: 2.3971
2022-03-04 09:42:04 - train: epoch 0145, iter [02800, 05004], lr: 0.019006, loss: 2.1957
2022-03-04 09:42:36 - train: epoch 0145, iter [02900, 05004], lr: 0.019006, loss: 2.0064
2022-03-04 09:43:08 - train: epoch 0145, iter [03000, 05004], lr: 0.019006, loss: 2.0017
2022-03-04 09:43:39 - train: epoch 0145, iter [03100, 05004], lr: 0.019006, loss: 2.0158
2022-03-04 09:44:12 - train: epoch 0145, iter [03200, 05004], lr: 0.019006, loss: 2.0009
2022-03-04 09:44:44 - train: epoch 0145, iter [03300, 05004], lr: 0.019006, loss: 2.4193
2022-03-04 09:45:16 - train: epoch 0145, iter [03400, 05004], lr: 0.019006, loss: 2.2710
2022-03-04 09:45:48 - train: epoch 0145, iter [03500, 05004], lr: 0.019006, loss: 2.0827
2022-03-04 09:46:20 - train: epoch 0145, iter [03600, 05004], lr: 0.019006, loss: 2.0273
2022-03-04 09:46:51 - train: epoch 0145, iter [03700, 05004], lr: 0.019006, loss: 2.2887
2022-03-04 09:47:24 - train: epoch 0145, iter [03800, 05004], lr: 0.019006, loss: 2.0145
2022-03-04 09:47:56 - train: epoch 0145, iter [03900, 05004], lr: 0.019006, loss: 2.1745
2022-03-04 09:48:28 - train: epoch 0145, iter [04000, 05004], lr: 0.019006, loss: 1.7926
2022-03-04 09:48:59 - train: epoch 0145, iter [04100, 05004], lr: 0.019006, loss: 2.2092
2022-03-04 09:49:31 - train: epoch 0145, iter [04200, 05004], lr: 0.019006, loss: 2.3184
2022-03-04 09:50:03 - train: epoch 0145, iter [04300, 05004], lr: 0.019006, loss: 1.9606
2022-03-04 09:50:34 - train: epoch 0145, iter [04400, 05004], lr: 0.019006, loss: 1.9171
2022-03-04 09:51:06 - train: epoch 0145, iter [04500, 05004], lr: 0.019006, loss: 2.0036
2022-03-04 09:51:39 - train: epoch 0145, iter [04600, 05004], lr: 0.019006, loss: 1.9029
2022-03-04 09:52:11 - train: epoch 0145, iter [04700, 05004], lr: 0.019006, loss: 2.1820
2022-03-04 09:52:41 - train: epoch 0145, iter [04800, 05004], lr: 0.019006, loss: 1.9772
2022-03-04 09:53:13 - train: epoch 0145, iter [04900, 05004], lr: 0.019006, loss: 2.1418
2022-03-04 09:53:44 - train: epoch 0145, iter [05000, 05004], lr: 0.019006, loss: 2.0378
2022-03-04 09:53:45 - train: epoch 145, train_loss: 2.0382
2022-03-04 09:54:56 - eval: epoch: 145, acc1: 68.072%, acc5: 88.874%, test_loss: 1.2909, per_image_load_time: 1.952ms, per_image_inference_time: 0.370ms
2022-03-04 09:54:56 - until epoch: 145, best_acc1: 68.072%
2022-03-04 09:54:56 - epoch 146 lr: 0.018377731220231144
2022-03-04 09:55:33 - train: epoch 0146, iter [00100, 05004], lr: 0.018378, loss: 1.8687
2022-03-04 09:56:06 - train: epoch 0146, iter [00200, 05004], lr: 0.018378, loss: 1.9673
2022-03-04 09:56:37 - train: epoch 0146, iter [00300, 05004], lr: 0.018378, loss: 2.0865
2022-03-04 09:57:10 - train: epoch 0146, iter [00400, 05004], lr: 0.018378, loss: 2.0163
2022-03-04 09:57:41 - train: epoch 0146, iter [00500, 05004], lr: 0.018378, loss: 1.8102
2022-03-04 09:58:13 - train: epoch 0146, iter [00600, 05004], lr: 0.018378, loss: 1.7620
2022-03-04 09:58:45 - train: epoch 0146, iter [00700, 05004], lr: 0.018378, loss: 2.2880
2022-03-04 09:59:18 - train: epoch 0146, iter [00800, 05004], lr: 0.018378, loss: 2.0820
2022-03-04 09:59:49 - train: epoch 0146, iter [00900, 05004], lr: 0.018378, loss: 2.0388
2022-03-04 10:00:22 - train: epoch 0146, iter [01000, 05004], lr: 0.018378, loss: 1.8754
2022-03-04 10:00:53 - train: epoch 0146, iter [01100, 05004], lr: 0.018378, loss: 1.7347
2022-03-04 10:01:25 - train: epoch 0146, iter [01200, 05004], lr: 0.018378, loss: 1.9406
2022-03-04 10:01:57 - train: epoch 0146, iter [01300, 05004], lr: 0.018378, loss: 1.6945
2022-03-04 10:02:29 - train: epoch 0146, iter [01400, 05004], lr: 0.018378, loss: 1.9289
2022-03-04 10:03:01 - train: epoch 0146, iter [01500, 05004], lr: 0.018378, loss: 1.9410
2022-03-04 10:03:33 - train: epoch 0146, iter [01600, 05004], lr: 0.018378, loss: 1.9826
2022-03-04 10:04:05 - train: epoch 0146, iter [01700, 05004], lr: 0.018378, loss: 2.0528
2022-03-04 10:04:37 - train: epoch 0146, iter [01800, 05004], lr: 0.018378, loss: 1.9797
2022-03-04 10:05:10 - train: epoch 0146, iter [01900, 05004], lr: 0.018378, loss: 1.7116
2022-03-04 10:05:41 - train: epoch 0146, iter [02000, 05004], lr: 0.018378, loss: 2.0252
2022-03-04 10:06:13 - train: epoch 0146, iter [02100, 05004], lr: 0.018378, loss: 2.0995
2022-03-04 10:06:44 - train: epoch 0146, iter [02200, 05004], lr: 0.018378, loss: 2.1558
2022-03-04 10:07:17 - train: epoch 0146, iter [02300, 05004], lr: 0.018378, loss: 1.6806
2022-03-04 10:07:49 - train: epoch 0146, iter [02400, 05004], lr: 0.018378, loss: 2.1256
2022-03-04 10:08:22 - train: epoch 0146, iter [02500, 05004], lr: 0.018378, loss: 2.1962
2022-03-04 10:08:53 - train: epoch 0146, iter [02600, 05004], lr: 0.018378, loss: 2.2768
2022-03-04 10:09:25 - train: epoch 0146, iter [02700, 05004], lr: 0.018378, loss: 2.2948
2022-03-04 10:09:57 - train: epoch 0146, iter [02800, 05004], lr: 0.018378, loss: 2.1420
2022-03-04 10:10:29 - train: epoch 0146, iter [02900, 05004], lr: 0.018378, loss: 2.1679
2022-03-04 10:11:00 - train: epoch 0146, iter [03000, 05004], lr: 0.018378, loss: 2.1071
2022-03-04 10:11:32 - train: epoch 0146, iter [03100, 05004], lr: 0.018378, loss: 1.9856
2022-03-04 10:12:04 - train: epoch 0146, iter [03200, 05004], lr: 0.018378, loss: 1.6374
2022-03-04 10:12:36 - train: epoch 0146, iter [03300, 05004], lr: 0.018378, loss: 1.8130
2022-03-04 10:13:08 - train: epoch 0146, iter [03400, 05004], lr: 0.018378, loss: 2.2217
2022-03-04 10:13:41 - train: epoch 0146, iter [03500, 05004], lr: 0.018378, loss: 1.9360
2022-03-04 10:14:12 - train: epoch 0146, iter [03600, 05004], lr: 0.018378, loss: 1.9218
2022-03-04 10:14:44 - train: epoch 0146, iter [03700, 05004], lr: 0.018378, loss: 2.0688
2022-03-04 10:15:16 - train: epoch 0146, iter [03800, 05004], lr: 0.018378, loss: 2.0572
2022-03-04 10:15:48 - train: epoch 0146, iter [03900, 05004], lr: 0.018378, loss: 1.9773
2022-03-04 10:16:19 - train: epoch 0146, iter [04000, 05004], lr: 0.018378, loss: 2.0191
2022-03-04 10:16:51 - train: epoch 0146, iter [04100, 05004], lr: 0.018378, loss: 1.9564
2022-03-04 10:17:25 - train: epoch 0146, iter [04200, 05004], lr: 0.018378, loss: 1.8861
2022-03-04 10:17:57 - train: epoch 0146, iter [04300, 05004], lr: 0.018378, loss: 1.9424
2022-03-04 10:18:28 - train: epoch 0146, iter [04400, 05004], lr: 0.018378, loss: 2.2254
2022-03-04 10:19:00 - train: epoch 0146, iter [04500, 05004], lr: 0.018378, loss: 1.9244
2022-03-04 10:19:31 - train: epoch 0146, iter [04600, 05004], lr: 0.018378, loss: 2.2507
2022-03-04 10:20:04 - train: epoch 0146, iter [04700, 05004], lr: 0.018378, loss: 2.0391
2022-03-04 10:20:34 - train: epoch 0146, iter [04800, 05004], lr: 0.018378, loss: 2.2601
2022-03-04 10:21:06 - train: epoch 0146, iter [04900, 05004], lr: 0.018378, loss: 2.1066
2022-03-04 10:21:37 - train: epoch 0146, iter [05000, 05004], lr: 0.018378, loss: 2.1012
2022-03-04 10:21:38 - train: epoch 146, train_loss: 2.0241
2022-03-04 10:22:48 - eval: epoch: 146, acc1: 68.056%, acc5: 88.602%, test_loss: 1.3011, per_image_load_time: 2.295ms, per_image_inference_time: 0.359ms
2022-03-04 10:22:48 - until epoch: 146, best_acc1: 68.072%
2022-03-04 10:22:48 - epoch 147 lr: 0.017757889363191483
2022-03-04 10:23:25 - train: epoch 0147, iter [00100, 05004], lr: 0.017758, loss: 1.8476
2022-03-04 10:23:57 - train: epoch 0147, iter [00200, 05004], lr: 0.017758, loss: 2.0042
2022-03-04 10:24:28 - train: epoch 0147, iter [00300, 05004], lr: 0.017758, loss: 2.1903
2022-03-04 10:25:01 - train: epoch 0147, iter [00400, 05004], lr: 0.017758, loss: 2.1406
2022-03-04 10:25:32 - train: epoch 0147, iter [00500, 05004], lr: 0.017758, loss: 1.8385
2022-03-04 10:26:04 - train: epoch 0147, iter [00600, 05004], lr: 0.017758, loss: 2.3165
2022-03-04 10:26:35 - train: epoch 0147, iter [00700, 05004], lr: 0.017758, loss: 1.9202
2022-03-04 10:27:07 - train: epoch 0147, iter [00800, 05004], lr: 0.017758, loss: 2.1462
2022-03-04 10:27:39 - train: epoch 0147, iter [00900, 05004], lr: 0.017758, loss: 2.1124
2022-03-04 10:28:11 - train: epoch 0147, iter [01000, 05004], lr: 0.017758, loss: 2.0233
2022-03-04 10:28:43 - train: epoch 0147, iter [01100, 05004], lr: 0.017758, loss: 2.0905
2022-03-04 10:29:16 - train: epoch 0147, iter [01200, 05004], lr: 0.017758, loss: 1.8519
2022-03-04 10:29:48 - train: epoch 0147, iter [01300, 05004], lr: 0.017758, loss: 2.1736
2022-03-04 10:30:19 - train: epoch 0147, iter [01400, 05004], lr: 0.017758, loss: 2.0957
2022-03-04 10:30:51 - train: epoch 0147, iter [01500, 05004], lr: 0.017758, loss: 2.0914
2022-03-04 10:31:24 - train: epoch 0147, iter [01600, 05004], lr: 0.017758, loss: 2.1256
2022-03-04 10:31:55 - train: epoch 0147, iter [01700, 05004], lr: 0.017758, loss: 1.9629
2022-03-04 10:32:27 - train: epoch 0147, iter [01800, 05004], lr: 0.017758, loss: 2.2200
2022-03-04 10:32:59 - train: epoch 0147, iter [01900, 05004], lr: 0.017758, loss: 2.1984
2022-03-04 10:33:31 - train: epoch 0147, iter [02000, 05004], lr: 0.017758, loss: 1.8128
2022-03-04 10:34:03 - train: epoch 0147, iter [02100, 05004], lr: 0.017758, loss: 1.9820
2022-03-04 10:34:36 - train: epoch 0147, iter [02200, 05004], lr: 0.017758, loss: 2.3406
2022-03-04 10:35:08 - train: epoch 0147, iter [02300, 05004], lr: 0.017758, loss: 1.9626
2022-03-04 10:35:38 - train: epoch 0147, iter [02400, 05004], lr: 0.017758, loss: 1.9713
2022-03-04 10:36:11 - train: epoch 0147, iter [02500, 05004], lr: 0.017758, loss: 2.2461
2022-03-04 10:36:43 - train: epoch 0147, iter [02600, 05004], lr: 0.017758, loss: 2.2423
2022-03-04 10:37:15 - train: epoch 0147, iter [02700, 05004], lr: 0.017758, loss: 2.0868
2022-03-04 10:37:47 - train: epoch 0147, iter [02800, 05004], lr: 0.017758, loss: 1.8988
2022-03-04 10:38:19 - train: epoch 0147, iter [02900, 05004], lr: 0.017758, loss: 1.7677
2022-03-04 10:38:51 - train: epoch 0147, iter [03000, 05004], lr: 0.017758, loss: 1.9659
2022-03-04 10:39:22 - train: epoch 0147, iter [03100, 05004], lr: 0.017758, loss: 1.9987
2022-03-04 10:39:55 - train: epoch 0147, iter [03200, 05004], lr: 0.017758, loss: 1.9229
2022-03-04 10:40:26 - train: epoch 0147, iter [03300, 05004], lr: 0.017758, loss: 2.1190
2022-03-04 10:40:58 - train: epoch 0147, iter [03400, 05004], lr: 0.017758, loss: 1.8862
2022-03-04 10:41:31 - train: epoch 0147, iter [03500, 05004], lr: 0.017758, loss: 1.8285
2022-03-04 10:42:03 - train: epoch 0147, iter [03600, 05004], lr: 0.017758, loss: 1.7983
2022-03-04 10:42:35 - train: epoch 0147, iter [03700, 05004], lr: 0.017758, loss: 2.1429
2022-03-04 10:43:07 - train: epoch 0147, iter [03800, 05004], lr: 0.017758, loss: 2.2227
2022-03-04 10:43:38 - train: epoch 0147, iter [03900, 05004], lr: 0.017758, loss: 2.2159
2022-03-04 10:44:11 - train: epoch 0147, iter [04000, 05004], lr: 0.017758, loss: 2.1843
2022-03-04 10:44:42 - train: epoch 0147, iter [04100, 05004], lr: 0.017758, loss: 2.0688
2022-03-04 10:45:14 - train: epoch 0147, iter [04200, 05004], lr: 0.017758, loss: 2.2325
2022-03-04 10:45:46 - train: epoch 0147, iter [04300, 05004], lr: 0.017758, loss: 2.1356
2022-03-04 10:46:18 - train: epoch 0147, iter [04400, 05004], lr: 0.017758, loss: 1.7802
2022-03-04 10:46:50 - train: epoch 0147, iter [04500, 05004], lr: 0.017758, loss: 2.2929
2022-03-04 10:47:22 - train: epoch 0147, iter [04600, 05004], lr: 0.017758, loss: 2.1479
2022-03-04 10:47:54 - train: epoch 0147, iter [04700, 05004], lr: 0.017758, loss: 2.2292
2022-03-04 10:48:26 - train: epoch 0147, iter [04800, 05004], lr: 0.017758, loss: 1.9410
2022-03-04 10:48:58 - train: epoch 0147, iter [04900, 05004], lr: 0.017758, loss: 1.7197
2022-03-04 10:49:29 - train: epoch 0147, iter [05000, 05004], lr: 0.017758, loss: 1.8528
2022-03-04 10:49:30 - train: epoch 147, train_loss: 2.0136
2022-03-04 10:50:40 - eval: epoch: 147, acc1: 68.310%, acc5: 88.768%, test_loss: 1.2851, per_image_load_time: 0.522ms, per_image_inference_time: 0.367ms
2022-03-04 10:50:41 - until epoch: 147, best_acc1: 68.310%
2022-03-04 10:50:41 - epoch 148 lr: 0.01714641594781347
2022-03-04 10:51:18 - train: epoch 0148, iter [00100, 05004], lr: 0.017146, loss: 2.2086
2022-03-04 10:51:50 - train: epoch 0148, iter [00200, 05004], lr: 0.017146, loss: 2.0024
2022-03-04 10:52:22 - train: epoch 0148, iter [00300, 05004], lr: 0.017146, loss: 2.1506
2022-03-04 10:52:53 - train: epoch 0148, iter [00400, 05004], lr: 0.017146, loss: 1.9764
2022-03-04 10:53:26 - train: epoch 0148, iter [00500, 05004], lr: 0.017146, loss: 1.8582
2022-03-04 10:53:57 - train: epoch 0148, iter [00600, 05004], lr: 0.017146, loss: 2.0142
2022-03-04 10:54:30 - train: epoch 0148, iter [00700, 05004], lr: 0.017146, loss: 1.8132
2022-03-04 10:55:03 - train: epoch 0148, iter [00800, 05004], lr: 0.017146, loss: 1.8333
2022-03-04 10:55:34 - train: epoch 0148, iter [00900, 05004], lr: 0.017146, loss: 1.9417
2022-03-04 10:56:07 - train: epoch 0148, iter [01000, 05004], lr: 0.017146, loss: 2.0821
2022-03-04 10:56:39 - train: epoch 0148, iter [01100, 05004], lr: 0.017146, loss: 1.8005
2022-03-04 10:57:13 - train: epoch 0148, iter [01200, 05004], lr: 0.017146, loss: 1.6244
2022-03-04 10:57:45 - train: epoch 0148, iter [01300, 05004], lr: 0.017146, loss: 1.9108
2022-03-04 10:58:19 - train: epoch 0148, iter [01400, 05004], lr: 0.017146, loss: 2.0000
2022-03-04 10:58:50 - train: epoch 0148, iter [01500, 05004], lr: 0.017146, loss: 2.0951
2022-03-04 10:59:23 - train: epoch 0148, iter [01600, 05004], lr: 0.017146, loss: 1.9363
2022-03-04 10:59:55 - train: epoch 0148, iter [01700, 05004], lr: 0.017146, loss: 2.1866
2022-03-04 11:00:28 - train: epoch 0148, iter [01800, 05004], lr: 0.017146, loss: 1.9013
2022-03-04 11:01:01 - train: epoch 0148, iter [01900, 05004], lr: 0.017146, loss: 2.0502
2022-03-04 11:01:36 - train: epoch 0148, iter [02000, 05004], lr: 0.017146, loss: 2.1606
2022-03-04 11:02:08 - train: epoch 0148, iter [02100, 05004], lr: 0.017146, loss: 1.9633
2022-03-04 11:02:40 - train: epoch 0148, iter [02200, 05004], lr: 0.017146, loss: 2.0362
2022-03-04 11:03:14 - train: epoch 0148, iter [02300, 05004], lr: 0.017146, loss: 2.0906
2022-03-04 11:03:47 - train: epoch 0148, iter [02400, 05004], lr: 0.017146, loss: 1.9417
2022-03-04 11:04:20 - train: epoch 0148, iter [02500, 05004], lr: 0.017146, loss: 1.9433
2022-03-04 11:04:52 - train: epoch 0148, iter [02600, 05004], lr: 0.017146, loss: 1.9217
2022-03-04 11:05:24 - train: epoch 0148, iter [02700, 05004], lr: 0.017146, loss: 2.0567
2022-03-04 11:05:56 - train: epoch 0148, iter [02800, 05004], lr: 0.017146, loss: 2.1347
2022-03-04 11:06:31 - train: epoch 0148, iter [02900, 05004], lr: 0.017146, loss: 2.2049
2022-03-04 11:07:03 - train: epoch 0148, iter [03000, 05004], lr: 0.017146, loss: 1.9202
2022-03-04 11:07:35 - train: epoch 0148, iter [03100, 05004], lr: 0.017146, loss: 2.0553
2022-03-04 11:08:08 - train: epoch 0148, iter [03200, 05004], lr: 0.017146, loss: 1.7968
2022-03-04 11:08:41 - train: epoch 0148, iter [03300, 05004], lr: 0.017146, loss: 2.0315
2022-03-04 11:09:13 - train: epoch 0148, iter [03400, 05004], lr: 0.017146, loss: 1.7636
2022-03-04 11:09:47 - train: epoch 0148, iter [03500, 05004], lr: 0.017146, loss: 2.0200
2022-03-04 11:10:18 - train: epoch 0148, iter [03600, 05004], lr: 0.017146, loss: 1.9040
2022-03-04 11:10:51 - train: epoch 0148, iter [03700, 05004], lr: 0.017146, loss: 1.9271
2022-03-04 11:11:22 - train: epoch 0148, iter [03800, 05004], lr: 0.017146, loss: 2.1089
2022-03-04 11:11:56 - train: epoch 0148, iter [03900, 05004], lr: 0.017146, loss: 2.0033
2022-03-04 11:12:30 - train: epoch 0148, iter [04000, 05004], lr: 0.017146, loss: 2.0073
2022-03-04 11:13:03 - train: epoch 0148, iter [04100, 05004], lr: 0.017146, loss: 1.9936
2022-03-04 11:13:36 - train: epoch 0148, iter [04200, 05004], lr: 0.017146, loss: 1.6856
2022-03-04 11:14:07 - train: epoch 0148, iter [04300, 05004], lr: 0.017146, loss: 1.6652
2022-03-04 11:14:41 - train: epoch 0148, iter [04400, 05004], lr: 0.017146, loss: 1.8965
2022-03-04 11:15:12 - train: epoch 0148, iter [04500, 05004], lr: 0.017146, loss: 1.9407
2022-03-04 11:15:44 - train: epoch 0148, iter [04600, 05004], lr: 0.017146, loss: 1.8595
2022-03-04 11:16:16 - train: epoch 0148, iter [04700, 05004], lr: 0.017146, loss: 2.2055
2022-03-04 11:16:48 - train: epoch 0148, iter [04800, 05004], lr: 0.017146, loss: 1.9814
2022-03-04 11:17:20 - train: epoch 0148, iter [04900, 05004], lr: 0.017146, loss: 2.0466
2022-03-04 11:17:51 - train: epoch 0148, iter [05000, 05004], lr: 0.017146, loss: 2.1106
2022-03-04 11:17:52 - train: epoch 148, train_loss: 2.0004
2022-03-04 11:19:03 - eval: epoch: 148, acc1: 68.614%, acc5: 89.280%, test_loss: 1.2580, per_image_load_time: 0.751ms, per_image_inference_time: 0.355ms
2022-03-04 11:19:04 - until epoch: 148, best_acc1: 68.614%
2022-03-04 11:19:04 - epoch 149 lr: 0.016543469682057107
2022-03-04 11:19:42 - train: epoch 0149, iter [00100, 05004], lr: 0.016543, loss: 1.9127
2022-03-04 11:20:16 - train: epoch 0149, iter [00200, 05004], lr: 0.016543, loss: 2.0484
2022-03-04 11:20:49 - train: epoch 0149, iter [00300, 05004], lr: 0.016543, loss: 2.0742
2022-03-04 11:21:20 - train: epoch 0149, iter [00400, 05004], lr: 0.016543, loss: 1.9456
2022-03-04 11:21:53 - train: epoch 0149, iter [00500, 05004], lr: 0.016543, loss: 1.8609
2022-03-04 11:22:26 - train: epoch 0149, iter [00600, 05004], lr: 0.016543, loss: 1.8622
2022-03-04 11:22:57 - train: epoch 0149, iter [00700, 05004], lr: 0.016543, loss: 2.1921
2022-03-04 11:23:29 - train: epoch 0149, iter [00800, 05004], lr: 0.016543, loss: 2.1315
2022-03-04 11:24:02 - train: epoch 0149, iter [00900, 05004], lr: 0.016543, loss: 2.0509
2022-03-04 11:24:34 - train: epoch 0149, iter [01000, 05004], lr: 0.016543, loss: 2.1524
2022-03-04 11:25:06 - train: epoch 0149, iter [01100, 05004], lr: 0.016543, loss: 1.8928
2022-03-04 11:25:39 - train: epoch 0149, iter [01200, 05004], lr: 0.016543, loss: 1.9907
2022-03-04 11:26:11 - train: epoch 0149, iter [01300, 05004], lr: 0.016543, loss: 1.9618
2022-03-04 11:26:44 - train: epoch 0149, iter [01400, 05004], lr: 0.016543, loss: 1.9647
2022-03-04 11:27:17 - train: epoch 0149, iter [01500, 05004], lr: 0.016543, loss: 2.0478
2022-03-04 11:27:48 - train: epoch 0149, iter [01600, 05004], lr: 0.016543, loss: 2.1886
2022-03-04 11:28:20 - train: epoch 0149, iter [01700, 05004], lr: 0.016543, loss: 1.7628
2022-03-04 11:28:53 - train: epoch 0149, iter [01800, 05004], lr: 0.016543, loss: 1.9543
2022-03-04 11:29:25 - train: epoch 0149, iter [01900, 05004], lr: 0.016543, loss: 1.7989
2022-03-04 11:29:58 - train: epoch 0149, iter [02000, 05004], lr: 0.016543, loss: 1.9298
2022-03-04 11:30:30 - train: epoch 0149, iter [02100, 05004], lr: 0.016543, loss: 1.8325
2022-03-04 11:31:03 - train: epoch 0149, iter [02200, 05004], lr: 0.016543, loss: 2.2317
2022-03-04 11:31:35 - train: epoch 0149, iter [02300, 05004], lr: 0.016543, loss: 1.8413
2022-03-04 11:32:07 - train: epoch 0149, iter [02400, 05004], lr: 0.016543, loss: 1.8087
2022-03-04 11:32:39 - train: epoch 0149, iter [02500, 05004], lr: 0.016543, loss: 1.8192
2022-03-04 11:33:10 - train: epoch 0149, iter [02600, 05004], lr: 0.016543, loss: 1.8882
2022-03-04 11:33:41 - train: epoch 0149, iter [02700, 05004], lr: 0.016543, loss: 2.2517
2022-03-04 11:34:13 - train: epoch 0149, iter [02800, 05004], lr: 0.016543, loss: 2.0587
2022-03-04 11:34:45 - train: epoch 0149, iter [02900, 05004], lr: 0.016543, loss: 1.7512
2022-03-04 11:35:17 - train: epoch 0149, iter [03000, 05004], lr: 0.016543, loss: 1.9571
2022-03-04 11:35:48 - train: epoch 0149, iter [03100, 05004], lr: 0.016543, loss: 1.7940
2022-03-04 11:36:19 - train: epoch 0149, iter [03200, 05004], lr: 0.016543, loss: 2.0231
2022-03-04 11:36:51 - train: epoch 0149, iter [03300, 05004], lr: 0.016543, loss: 1.8614
2022-03-04 11:37:24 - train: epoch 0149, iter [03400, 05004], lr: 0.016543, loss: 1.8239
2022-03-04 11:37:54 - train: epoch 0149, iter [03500, 05004], lr: 0.016543, loss: 2.1392
2022-03-04 11:38:27 - train: epoch 0149, iter [03600, 05004], lr: 0.016543, loss: 1.8582
2022-03-04 11:38:58 - train: epoch 0149, iter [03700, 05004], lr: 0.016543, loss: 1.8898
2022-03-04 11:39:29 - train: epoch 0149, iter [03800, 05004], lr: 0.016543, loss: 1.8081
2022-03-04 11:40:01 - train: epoch 0149, iter [03900, 05004], lr: 0.016543, loss: 2.1728
2022-03-04 11:40:32 - train: epoch 0149, iter [04000, 05004], lr: 0.016543, loss: 2.1701
2022-03-04 11:41:04 - train: epoch 0149, iter [04100, 05004], lr: 0.016543, loss: 1.9485
2022-03-04 11:41:36 - train: epoch 0149, iter [04200, 05004], lr: 0.016543, loss: 1.8695
2022-03-04 11:42:08 - train: epoch 0149, iter [04300, 05004], lr: 0.016543, loss: 1.9200
2022-03-04 11:42:40 - train: epoch 0149, iter [04400, 05004], lr: 0.016543, loss: 1.9835
2022-03-04 11:43:11 - train: epoch 0149, iter [04500, 05004], lr: 0.016543, loss: 1.8179
2022-03-04 11:43:43 - train: epoch 0149, iter [04600, 05004], lr: 0.016543, loss: 1.8082
2022-03-04 11:44:14 - train: epoch 0149, iter [04700, 05004], lr: 0.016543, loss: 2.2921
2022-03-04 11:44:46 - train: epoch 0149, iter [04800, 05004], lr: 0.016543, loss: 2.0369
2022-03-04 11:45:17 - train: epoch 0149, iter [04900, 05004], lr: 0.016543, loss: 1.6406
2022-03-04 11:45:51 - train: epoch 0149, iter [05000, 05004], lr: 0.016543, loss: 2.1510
2022-03-04 11:45:52 - train: epoch 149, train_loss: 1.9906
2022-03-04 11:47:02 - eval: epoch: 149, acc1: 68.182%, acc5: 88.806%, test_loss: 1.2852, per_image_load_time: 1.191ms, per_image_inference_time: 0.343ms
2022-03-04 11:47:03 - until epoch: 149, best_acc1: 68.614%
2022-03-04 11:47:03 - epoch 150 lr: 0.015949207060660137
2022-03-04 11:47:39 - train: epoch 0150, iter [00100, 05004], lr: 0.015949, loss: 1.9754
2022-03-04 11:48:11 - train: epoch 0150, iter [00200, 05004], lr: 0.015949, loss: 1.9632
2022-03-04 11:48:43 - train: epoch 0150, iter [00300, 05004], lr: 0.015949, loss: 1.9350
2022-03-04 11:49:15 - train: epoch 0150, iter [00400, 05004], lr: 0.015949, loss: 1.9841
2022-03-04 11:49:48 - train: epoch 0150, iter [00500, 05004], lr: 0.015949, loss: 2.0471
2022-03-04 11:50:20 - train: epoch 0150, iter [00600, 05004], lr: 0.015949, loss: 2.0932
2022-03-04 11:50:51 - train: epoch 0150, iter [00700, 05004], lr: 0.015949, loss: 1.8692
2022-03-04 11:51:23 - train: epoch 0150, iter [00800, 05004], lr: 0.015949, loss: 1.8039
2022-03-04 11:51:55 - train: epoch 0150, iter [00900, 05004], lr: 0.015949, loss: 1.9699
2022-03-04 11:52:27 - train: epoch 0150, iter [01000, 05004], lr: 0.015949, loss: 1.8948
2022-03-04 11:52:59 - train: epoch 0150, iter [01100, 05004], lr: 0.015949, loss: 2.1537
2022-03-04 11:53:31 - train: epoch 0150, iter [01200, 05004], lr: 0.015949, loss: 1.9034
2022-03-04 11:54:02 - train: epoch 0150, iter [01300, 05004], lr: 0.015949, loss: 2.1355
2022-03-04 11:54:34 - train: epoch 0150, iter [01400, 05004], lr: 0.015949, loss: 1.9664
2022-03-04 11:55:06 - train: epoch 0150, iter [01500, 05004], lr: 0.015949, loss: 2.0728
2022-03-04 11:55:39 - train: epoch 0150, iter [01600, 05004], lr: 0.015949, loss: 1.8331
2022-03-04 11:56:11 - train: epoch 0150, iter [01700, 05004], lr: 0.015949, loss: 1.9906
2022-03-04 11:56:43 - train: epoch 0150, iter [01800, 05004], lr: 0.015949, loss: 2.0130
2022-03-04 11:57:15 - train: epoch 0150, iter [01900, 05004], lr: 0.015949, loss: 1.9964
2022-03-04 11:57:46 - train: epoch 0150, iter [02000, 05004], lr: 0.015949, loss: 2.1579
2022-03-04 11:58:18 - train: epoch 0150, iter [02100, 05004], lr: 0.015949, loss: 1.9454
2022-03-04 11:58:50 - train: epoch 0150, iter [02200, 05004], lr: 0.015949, loss: 1.8131
2022-03-04 11:59:22 - train: epoch 0150, iter [02300, 05004], lr: 0.015949, loss: 2.1940
2022-03-04 11:59:55 - train: epoch 0150, iter [02400, 05004], lr: 0.015949, loss: 1.8980
2022-03-04 12:00:27 - train: epoch 0150, iter [02500, 05004], lr: 0.015949, loss: 1.8337
2022-03-04 12:00:58 - train: epoch 0150, iter [02600, 05004], lr: 0.015949, loss: 1.9446
2022-03-04 12:01:31 - train: epoch 0150, iter [02700, 05004], lr: 0.015949, loss: 1.8745
2022-03-04 12:02:03 - train: epoch 0150, iter [02800, 05004], lr: 0.015949, loss: 1.6675
2022-03-04 12:02:35 - train: epoch 0150, iter [02900, 05004], lr: 0.015949, loss: 2.1909
2022-03-04 12:03:06 - train: epoch 0150, iter [03000, 05004], lr: 0.015949, loss: 1.9179
2022-03-04 12:03:38 - train: epoch 0150, iter [03100, 05004], lr: 0.015949, loss: 1.9232
2022-03-04 12:04:10 - train: epoch 0150, iter [03200, 05004], lr: 0.015949, loss: 2.0589
2022-03-04 12:04:42 - train: epoch 0150, iter [03300, 05004], lr: 0.015949, loss: 2.1283
2022-03-04 12:05:13 - train: epoch 0150, iter [03400, 05004], lr: 0.015949, loss: 2.0269
2022-03-04 12:05:44 - train: epoch 0150, iter [03500, 05004], lr: 0.015949, loss: 2.1756
2022-03-04 12:06:17 - train: epoch 0150, iter [03600, 05004], lr: 0.015949, loss: 1.9593
2022-03-04 12:06:48 - train: epoch 0150, iter [03700, 05004], lr: 0.015949, loss: 2.1962
2022-03-04 12:07:21 - train: epoch 0150, iter [03800, 05004], lr: 0.015949, loss: 2.0848
2022-03-04 12:07:53 - train: epoch 0150, iter [03900, 05004], lr: 0.015949, loss: 1.9549
2022-03-04 12:08:26 - train: epoch 0150, iter [04000, 05004], lr: 0.015949, loss: 1.9401
2022-03-04 12:08:58 - train: epoch 0150, iter [04100, 05004], lr: 0.015949, loss: 2.0403
2022-03-04 12:09:30 - train: epoch 0150, iter [04200, 05004], lr: 0.015949, loss: 2.0468
2022-03-04 12:10:01 - train: epoch 0150, iter [04300, 05004], lr: 0.015949, loss: 1.6239
2022-03-04 12:10:33 - train: epoch 0150, iter [04400, 05004], lr: 0.015949, loss: 2.0665
2022-03-04 12:11:05 - train: epoch 0150, iter [04500, 05004], lr: 0.015949, loss: 2.0241
2022-03-04 12:11:36 - train: epoch 0150, iter [04600, 05004], lr: 0.015949, loss: 2.2730
2022-03-04 12:12:08 - train: epoch 0150, iter [04700, 05004], lr: 0.015949, loss: 1.7262
2022-03-04 12:12:40 - train: epoch 0150, iter [04800, 05004], lr: 0.015949, loss: 2.0325
2022-03-04 12:13:12 - train: epoch 0150, iter [04900, 05004], lr: 0.015949, loss: 1.8575
2022-03-04 12:13:43 - train: epoch 0150, iter [05000, 05004], lr: 0.015949, loss: 2.0079
2022-03-04 12:13:44 - train: epoch 150, train_loss: 1.9817
2022-03-04 12:14:54 - eval: epoch: 150, acc1: 68.904%, acc5: 89.532%, test_loss: 1.2437, per_image_load_time: 1.516ms, per_image_inference_time: 0.362ms
2022-03-04 12:14:55 - until epoch: 150, best_acc1: 68.904%
2022-03-04 12:14:55 - epoch 151 lr: 0.015363782324520032
2022-03-04 12:15:32 - train: epoch 0151, iter [00100, 05004], lr: 0.015364, loss: 1.9662
2022-03-04 12:16:04 - train: epoch 0151, iter [00200, 05004], lr: 0.015364, loss: 2.0820
2022-03-04 12:16:37 - train: epoch 0151, iter [00300, 05004], lr: 0.015364, loss: 1.8569
2022-03-04 12:17:08 - train: epoch 0151, iter [00400, 05004], lr: 0.015364, loss: 2.1666
2022-03-04 12:17:40 - train: epoch 0151, iter [00500, 05004], lr: 0.015364, loss: 2.1599
2022-03-04 12:18:12 - train: epoch 0151, iter [00600, 05004], lr: 0.015364, loss: 1.7459
2022-03-04 12:18:44 - train: epoch 0151, iter [00700, 05004], lr: 0.015364, loss: 2.2581
2022-03-04 12:19:16 - train: epoch 0151, iter [00800, 05004], lr: 0.015364, loss: 1.8826
2022-03-04 12:19:48 - train: epoch 0151, iter [00900, 05004], lr: 0.015364, loss: 1.9295
2022-03-04 12:20:20 - train: epoch 0151, iter [01000, 05004], lr: 0.015364, loss: 1.8789
2022-03-04 12:20:53 - train: epoch 0151, iter [01100, 05004], lr: 0.015364, loss: 1.9308
2022-03-04 12:21:26 - train: epoch 0151, iter [01200, 05004], lr: 0.015364, loss: 2.0495
2022-03-04 12:21:58 - train: epoch 0151, iter [01300, 05004], lr: 0.015364, loss: 2.2263
2022-03-04 12:22:30 - train: epoch 0151, iter [01400, 05004], lr: 0.015364, loss: 1.8216
2022-03-04 12:23:02 - train: epoch 0151, iter [01500, 05004], lr: 0.015364, loss: 1.6745
2022-03-04 12:23:34 - train: epoch 0151, iter [01600, 05004], lr: 0.015364, loss: 2.0268
2022-03-04 12:24:06 - train: epoch 0151, iter [01700, 05004], lr: 0.015364, loss: 1.8928
2022-03-04 12:24:37 - train: epoch 0151, iter [01800, 05004], lr: 0.015364, loss: 2.1229
2022-03-04 12:25:09 - train: epoch 0151, iter [01900, 05004], lr: 0.015364, loss: 2.1386
2022-03-04 12:25:42 - train: epoch 0151, iter [02000, 05004], lr: 0.015364, loss: 2.0697
2022-03-04 12:26:13 - train: epoch 0151, iter [02100, 05004], lr: 0.015364, loss: 1.8346
2022-03-04 12:26:45 - train: epoch 0151, iter [02200, 05004], lr: 0.015364, loss: 2.0840
2022-03-04 12:27:15 - train: epoch 0151, iter [02300, 05004], lr: 0.015364, loss: 1.8367
2022-03-04 12:27:48 - train: epoch 0151, iter [02400, 05004], lr: 0.015364, loss: 2.0568
2022-03-04 12:28:20 - train: epoch 0151, iter [02500, 05004], lr: 0.015364, loss: 1.8251
2022-03-04 12:28:52 - train: epoch 0151, iter [02600, 05004], lr: 0.015364, loss: 2.0590
2022-03-04 12:29:24 - train: epoch 0151, iter [02700, 05004], lr: 0.015364, loss: 2.0752
2022-03-04 12:29:55 - train: epoch 0151, iter [02800, 05004], lr: 0.015364, loss: 2.0140
2022-03-04 12:30:28 - train: epoch 0151, iter [02900, 05004], lr: 0.015364, loss: 1.8645
2022-03-04 12:30:59 - train: epoch 0151, iter [03000, 05004], lr: 0.015364, loss: 2.0411
2022-03-04 12:31:32 - train: epoch 0151, iter [03100, 05004], lr: 0.015364, loss: 2.1219
2022-03-04 12:32:04 - train: epoch 0151, iter [03200, 05004], lr: 0.015364, loss: 1.7589
2022-03-04 12:32:35 - train: epoch 0151, iter [03300, 05004], lr: 0.015364, loss: 1.7696
2022-03-04 12:33:06 - train: epoch 0151, iter [03400, 05004], lr: 0.015364, loss: 1.9297
2022-03-04 12:33:39 - train: epoch 0151, iter [03500, 05004], lr: 0.015364, loss: 2.0421
2022-03-04 12:34:11 - train: epoch 0151, iter [03600, 05004], lr: 0.015364, loss: 2.0217
2022-03-04 12:34:43 - train: epoch 0151, iter [03700, 05004], lr: 0.015364, loss: 1.8874
2022-03-04 12:35:14 - train: epoch 0151, iter [03800, 05004], lr: 0.015364, loss: 2.0801
2022-03-04 12:35:47 - train: epoch 0151, iter [03900, 05004], lr: 0.015364, loss: 2.0088
2022-03-04 12:36:19 - train: epoch 0151, iter [04000, 05004], lr: 0.015364, loss: 1.9908
2022-03-04 12:36:51 - train: epoch 0151, iter [04100, 05004], lr: 0.015364, loss: 1.9672
2022-03-04 12:37:23 - train: epoch 0151, iter [04200, 05004], lr: 0.015364, loss: 2.0660
2022-03-04 12:37:56 - train: epoch 0151, iter [04300, 05004], lr: 0.015364, loss: 2.1817
2022-03-04 12:38:28 - train: epoch 0151, iter [04400, 05004], lr: 0.015364, loss: 1.9862
2022-03-04 12:38:58 - train: epoch 0151, iter [04500, 05004], lr: 0.015364, loss: 1.8938
2022-03-04 12:39:31 - train: epoch 0151, iter [04600, 05004], lr: 0.015364, loss: 2.0237
2022-03-04 12:40:02 - train: epoch 0151, iter [04700, 05004], lr: 0.015364, loss: 1.6959
2022-03-04 12:40:35 - train: epoch 0151, iter [04800, 05004], lr: 0.015364, loss: 1.9659
2022-03-04 12:41:08 - train: epoch 0151, iter [04900, 05004], lr: 0.015364, loss: 2.0464
2022-03-04 12:41:39 - train: epoch 0151, iter [05000, 05004], lr: 0.015364, loss: 1.9765
2022-03-04 12:41:40 - train: epoch 151, train_loss: 1.9681
2022-03-04 12:42:51 - eval: epoch: 151, acc1: 68.570%, acc5: 89.090%, test_loss: 1.2648, per_image_load_time: 0.512ms, per_image_inference_time: 0.363ms
2022-03-04 12:42:51 - until epoch: 151, best_acc1: 68.904%
2022-03-04 12:42:51 - epoch 152 lr: 0.01478734742066054
2022-03-04 12:43:29 - train: epoch 0152, iter [00100, 05004], lr: 0.014787, loss: 2.1151
2022-03-04 12:44:00 - train: epoch 0152, iter [00200, 05004], lr: 0.014787, loss: 1.8662
2022-03-04 12:44:32 - train: epoch 0152, iter [00300, 05004], lr: 0.014787, loss: 2.1113
2022-03-04 12:45:03 - train: epoch 0152, iter [00400, 05004], lr: 0.014787, loss: 1.7452
2022-03-04 12:45:35 - train: epoch 0152, iter [00500, 05004], lr: 0.014787, loss: 1.8001
2022-03-04 12:46:06 - train: epoch 0152, iter [00600, 05004], lr: 0.014787, loss: 1.9115
2022-03-04 12:46:39 - train: epoch 0152, iter [00700, 05004], lr: 0.014787, loss: 2.1179
2022-03-04 12:47:10 - train: epoch 0152, iter [00800, 05004], lr: 0.014787, loss: 1.7439
2022-03-04 12:47:43 - train: epoch 0152, iter [00900, 05004], lr: 0.014787, loss: 1.9609
2022-03-04 12:48:14 - train: epoch 0152, iter [01000, 05004], lr: 0.014787, loss: 1.9440
2022-03-04 12:48:46 - train: epoch 0152, iter [01100, 05004], lr: 0.014787, loss: 1.9635
2022-03-04 12:49:18 - train: epoch 0152, iter [01200, 05004], lr: 0.014787, loss: 1.9185
2022-03-04 12:49:52 - train: epoch 0152, iter [01300, 05004], lr: 0.014787, loss: 1.7333
2022-03-04 12:50:25 - train: epoch 0152, iter [01400, 05004], lr: 0.014787, loss: 2.0300
2022-03-04 12:50:57 - train: epoch 0152, iter [01500, 05004], lr: 0.014787, loss: 1.9519
2022-03-04 12:51:29 - train: epoch 0152, iter [01600, 05004], lr: 0.014787, loss: 2.0132
2022-03-04 12:52:02 - train: epoch 0152, iter [01700, 05004], lr: 0.014787, loss: 1.9889
2022-03-04 12:52:37 - train: epoch 0152, iter [01800, 05004], lr: 0.014787, loss: 1.9453
2022-03-04 12:53:11 - train: epoch 0152, iter [01900, 05004], lr: 0.014787, loss: 1.9034
2022-03-04 12:53:42 - train: epoch 0152, iter [02000, 05004], lr: 0.014787, loss: 1.9969
2022-03-04 12:54:16 - train: epoch 0152, iter [02100, 05004], lr: 0.014787, loss: 1.9351
2022-03-04 12:54:49 - train: epoch 0152, iter [02200, 05004], lr: 0.014787, loss: 1.9431
2022-03-04 12:55:23 - train: epoch 0152, iter [02300, 05004], lr: 0.014787, loss: 1.9847
2022-03-04 12:55:57 - train: epoch 0152, iter [02400, 05004], lr: 0.014787, loss: 1.7940
2022-03-04 12:56:31 - train: epoch 0152, iter [02500, 05004], lr: 0.014787, loss: 2.0022
2022-03-04 12:57:07 - train: epoch 0152, iter [02600, 05004], lr: 0.014787, loss: 1.9429
2022-03-04 12:57:43 - train: epoch 0152, iter [02700, 05004], lr: 0.014787, loss: 2.0710
2022-03-04 12:58:21 - train: epoch 0152, iter [02800, 05004], lr: 0.014787, loss: 2.0010
2022-03-04 12:58:57 - train: epoch 0152, iter [02900, 05004], lr: 0.014787, loss: 2.0502
2022-03-04 12:59:34 - train: epoch 0152, iter [03000, 05004], lr: 0.014787, loss: 2.4201
2022-03-04 13:00:06 - train: epoch 0152, iter [03100, 05004], lr: 0.014787, loss: 2.0536
2022-03-04 13:00:43 - train: epoch 0152, iter [03200, 05004], lr: 0.014787, loss: 1.9039
2022-03-04 13:01:16 - train: epoch 0152, iter [03300, 05004], lr: 0.014787, loss: 2.2921
2022-03-04 13:01:50 - train: epoch 0152, iter [03400, 05004], lr: 0.014787, loss: 1.8925
2022-03-04 13:02:22 - train: epoch 0152, iter [03500, 05004], lr: 0.014787, loss: 1.9135
2022-03-04 13:02:56 - train: epoch 0152, iter [03600, 05004], lr: 0.014787, loss: 1.7291
2022-03-04 13:03:29 - train: epoch 0152, iter [03700, 05004], lr: 0.014787, loss: 1.9425
2022-03-04 13:04:01 - train: epoch 0152, iter [03800, 05004], lr: 0.014787, loss: 1.8600
2022-03-04 13:04:33 - train: epoch 0152, iter [03900, 05004], lr: 0.014787, loss: 1.9614
2022-03-04 13:05:06 - train: epoch 0152, iter [04000, 05004], lr: 0.014787, loss: 1.7078
2022-03-04 13:05:42 - train: epoch 0152, iter [04100, 05004], lr: 0.014787, loss: 1.9500
2022-03-04 13:06:15 - train: epoch 0152, iter [04200, 05004], lr: 0.014787, loss: 1.9565
2022-03-04 13:06:49 - train: epoch 0152, iter [04300, 05004], lr: 0.014787, loss: 1.9546
2022-03-04 13:07:22 - train: epoch 0152, iter [04400, 05004], lr: 0.014787, loss: 1.9388
2022-03-04 13:07:54 - train: epoch 0152, iter [04500, 05004], lr: 0.014787, loss: 2.0240
2022-03-04 13:08:26 - train: epoch 0152, iter [04600, 05004], lr: 0.014787, loss: 1.9134
2022-03-04 13:08:59 - train: epoch 0152, iter [04700, 05004], lr: 0.014787, loss: 1.9054
2022-03-04 13:09:32 - train: epoch 0152, iter [04800, 05004], lr: 0.014787, loss: 2.0089
2022-03-04 13:10:04 - train: epoch 0152, iter [04900, 05004], lr: 0.014787, loss: 1.9028
2022-03-04 13:10:37 - train: epoch 0152, iter [05000, 05004], lr: 0.014787, loss: 1.9214
2022-03-04 13:10:38 - train: epoch 152, train_loss: 1.9530
2022-03-04 13:11:48 - eval: epoch: 152, acc1: 68.898%, acc5: 89.102%, test_loss: 1.2612, per_image_load_time: 1.593ms, per_image_inference_time: 0.403ms
2022-03-04 13:11:49 - until epoch: 152, best_acc1: 68.904%
2022-03-04 13:11:49 - epoch 153 lr: 0.014220051962793951
2022-03-04 13:12:27 - train: epoch 0153, iter [00100, 05004], lr: 0.014220, loss: 1.8453
2022-03-04 13:12:59 - train: epoch 0153, iter [00200, 05004], lr: 0.014220, loss: 1.9438
2022-03-04 13:13:32 - train: epoch 0153, iter [00300, 05004], lr: 0.014220, loss: 1.8216
2022-03-04 13:14:04 - train: epoch 0153, iter [00400, 05004], lr: 0.014220, loss: 1.9284
2022-03-04 13:14:37 - train: epoch 0153, iter [00500, 05004], lr: 0.014220, loss: 2.0203
2022-03-04 13:15:08 - train: epoch 0153, iter [00600, 05004], lr: 0.014220, loss: 1.9194
2022-03-04 13:15:42 - train: epoch 0153, iter [00700, 05004], lr: 0.014220, loss: 2.0411
2022-03-04 13:16:13 - train: epoch 0153, iter [00800, 05004], lr: 0.014220, loss: 1.4519
2022-03-04 13:16:47 - train: epoch 0153, iter [00900, 05004], lr: 0.014220, loss: 2.1053
2022-03-04 13:17:19 - train: epoch 0153, iter [01000, 05004], lr: 0.014220, loss: 1.7409
2022-03-04 13:17:52 - train: epoch 0153, iter [01100, 05004], lr: 0.014220, loss: 1.9987
2022-03-04 13:18:25 - train: epoch 0153, iter [01200, 05004], lr: 0.014220, loss: 1.8293
2022-03-04 13:18:57 - train: epoch 0153, iter [01300, 05004], lr: 0.014220, loss: 1.9513
2022-03-04 13:19:29 - train: epoch 0153, iter [01400, 05004], lr: 0.014220, loss: 2.1354
2022-03-04 13:20:03 - train: epoch 0153, iter [01500, 05004], lr: 0.014220, loss: 1.6862
2022-03-04 13:20:35 - train: epoch 0153, iter [01600, 05004], lr: 0.014220, loss: 1.8995
2022-03-04 13:21:08 - train: epoch 0153, iter [01700, 05004], lr: 0.014220, loss: 1.8653
2022-03-04 13:21:40 - train: epoch 0153, iter [01800, 05004], lr: 0.014220, loss: 1.6455
2022-03-04 13:22:13 - train: epoch 0153, iter [01900, 05004], lr: 0.014220, loss: 2.1460
2022-03-04 13:22:46 - train: epoch 0153, iter [02000, 05004], lr: 0.014220, loss: 1.7979
2022-03-04 13:23:20 - train: epoch 0153, iter [02100, 05004], lr: 0.014220, loss: 1.7573
2022-03-04 13:23:53 - train: epoch 0153, iter [02200, 05004], lr: 0.014220, loss: 1.8384
2022-03-04 13:24:25 - train: epoch 0153, iter [02300, 05004], lr: 0.014220, loss: 1.9294
2022-03-04 13:24:57 - train: epoch 0153, iter [02400, 05004], lr: 0.014220, loss: 1.9963
2022-03-04 13:25:30 - train: epoch 0153, iter [02500, 05004], lr: 0.014220, loss: 2.0283
2022-03-04 13:26:03 - train: epoch 0153, iter [02600, 05004], lr: 0.014220, loss: 1.8344
2022-03-04 13:26:36 - train: epoch 0153, iter [02700, 05004], lr: 0.014220, loss: 1.7487
2022-03-04 13:27:10 - train: epoch 0153, iter [02800, 05004], lr: 0.014220, loss: 1.9447
2022-03-04 13:27:43 - train: epoch 0153, iter [02900, 05004], lr: 0.014220, loss: 2.0549
2022-03-04 13:28:17 - train: epoch 0153, iter [03000, 05004], lr: 0.014220, loss: 2.0656
2022-03-04 13:28:51 - train: epoch 0153, iter [03100, 05004], lr: 0.014220, loss: 1.9150
2022-03-04 13:29:25 - train: epoch 0153, iter [03200, 05004], lr: 0.014220, loss: 1.9676
2022-03-04 13:29:58 - train: epoch 0153, iter [03300, 05004], lr: 0.014220, loss: 1.7482
2022-03-04 13:30:32 - train: epoch 0153, iter [03400, 05004], lr: 0.014220, loss: 1.6708
2022-03-04 13:31:05 - train: epoch 0153, iter [03500, 05004], lr: 0.014220, loss: 2.1024
2022-03-04 13:31:39 - train: epoch 0153, iter [03600, 05004], lr: 0.014220, loss: 1.6693
2022-03-04 13:32:14 - train: epoch 0153, iter [03700, 05004], lr: 0.014220, loss: 2.0919
2022-03-04 13:32:47 - train: epoch 0153, iter [03800, 05004], lr: 0.014220, loss: 2.0449
2022-03-04 13:33:21 - train: epoch 0153, iter [03900, 05004], lr: 0.014220, loss: 1.9141
2022-03-04 13:33:54 - train: epoch 0153, iter [04000, 05004], lr: 0.014220, loss: 1.9711
2022-03-04 13:34:27 - train: epoch 0153, iter [04100, 05004], lr: 0.014220, loss: 1.7356
2022-03-04 13:35:02 - train: epoch 0153, iter [04200, 05004], lr: 0.014220, loss: 1.9514
2022-03-04 13:35:36 - train: epoch 0153, iter [04300, 05004], lr: 0.014220, loss: 1.7848
2022-03-04 13:36:10 - train: epoch 0153, iter [04400, 05004], lr: 0.014220, loss: 1.7861
2022-03-04 13:36:44 - train: epoch 0153, iter [04500, 05004], lr: 0.014220, loss: 1.8084
2022-03-04 13:37:18 - train: epoch 0153, iter [04600, 05004], lr: 0.014220, loss: 1.8401
2022-03-04 13:37:52 - train: epoch 0153, iter [04700, 05004], lr: 0.014220, loss: 2.0368
2022-03-04 13:38:25 - train: epoch 0153, iter [04800, 05004], lr: 0.014220, loss: 1.9797
2022-03-04 13:38:59 - train: epoch 0153, iter [04900, 05004], lr: 0.014220, loss: 1.8554
2022-03-04 13:39:30 - train: epoch 0153, iter [05000, 05004], lr: 0.014220, loss: 1.8228
2022-03-04 13:39:31 - train: epoch 153, train_loss: 1.9388
2022-03-04 13:40:46 - eval: epoch: 153, acc1: 69.276%, acc5: 89.478%, test_loss: 1.2349, per_image_load_time: 0.659ms, per_image_inference_time: 0.464ms
2022-03-04 13:40:47 - until epoch: 153, best_acc1: 69.276%
2022-03-04 13:40:47 - epoch 154 lr: 0.01366204319248885
2022-03-04 13:41:26 - train: epoch 0154, iter [00100, 05004], lr: 0.013662, loss: 2.0057
2022-03-04 13:42:00 - train: epoch 0154, iter [00200, 05004], lr: 0.013662, loss: 1.8875
2022-03-04 13:42:34 - train: epoch 0154, iter [00300, 05004], lr: 0.013662, loss: 1.9784
2022-03-04 13:43:07 - train: epoch 0154, iter [00400, 05004], lr: 0.013662, loss: 1.9396
2022-03-04 13:43:42 - train: epoch 0154, iter [00500, 05004], lr: 0.013662, loss: 1.6373
2022-03-04 13:44:15 - train: epoch 0154, iter [00600, 05004], lr: 0.013662, loss: 1.7309
2022-03-04 13:44:50 - train: epoch 0154, iter [00700, 05004], lr: 0.013662, loss: 1.9986
2022-03-04 13:45:23 - train: epoch 0154, iter [00800, 05004], lr: 0.013662, loss: 1.9333
2022-03-04 13:45:57 - train: epoch 0154, iter [00900, 05004], lr: 0.013662, loss: 2.0207
2022-03-04 13:46:30 - train: epoch 0154, iter [01000, 05004], lr: 0.013662, loss: 1.7860
2022-03-04 13:47:04 - train: epoch 0154, iter [01100, 05004], lr: 0.013662, loss: 2.0701
2022-03-04 13:47:39 - train: epoch 0154, iter [01200, 05004], lr: 0.013662, loss: 2.0440
2022-03-04 13:48:12 - train: epoch 0154, iter [01300, 05004], lr: 0.013662, loss: 1.7012
2022-03-04 13:48:46 - train: epoch 0154, iter [01400, 05004], lr: 0.013662, loss: 1.7583
2022-03-04 13:49:19 - train: epoch 0154, iter [01500, 05004], lr: 0.013662, loss: 1.8068
2022-03-04 13:49:54 - train: epoch 0154, iter [01600, 05004], lr: 0.013662, loss: 1.7833
2022-03-04 13:50:26 - train: epoch 0154, iter [01700, 05004], lr: 0.013662, loss: 1.9013
2022-03-04 13:51:01 - train: epoch 0154, iter [01800, 05004], lr: 0.013662, loss: 1.9181
2022-03-04 13:51:34 - train: epoch 0154, iter [01900, 05004], lr: 0.013662, loss: 1.8392
2022-03-04 13:52:08 - train: epoch 0154, iter [02000, 05004], lr: 0.013662, loss: 1.7473
2022-03-04 13:52:41 - train: epoch 0154, iter [02100, 05004], lr: 0.013662, loss: 1.6331
2022-03-04 13:53:16 - train: epoch 0154, iter [02200, 05004], lr: 0.013662, loss: 1.9531
2022-03-04 13:53:49 - train: epoch 0154, iter [02300, 05004], lr: 0.013662, loss: 1.8379
2022-03-04 13:54:23 - train: epoch 0154, iter [02400, 05004], lr: 0.013662, loss: 2.0023
2022-03-04 13:54:57 - train: epoch 0154, iter [02500, 05004], lr: 0.013662, loss: 2.0286
2022-03-04 13:55:30 - train: epoch 0154, iter [02600, 05004], lr: 0.013662, loss: 1.7789
2022-03-04 13:56:03 - train: epoch 0154, iter [02700, 05004], lr: 0.013662, loss: 1.9712
2022-03-04 13:56:36 - train: epoch 0154, iter [02800, 05004], lr: 0.013662, loss: 2.1364
2022-03-04 13:57:10 - train: epoch 0154, iter [02900, 05004], lr: 0.013662, loss: 1.7477
2022-03-04 13:57:43 - train: epoch 0154, iter [03000, 05004], lr: 0.013662, loss: 2.0369
2022-03-04 13:58:17 - train: epoch 0154, iter [03100, 05004], lr: 0.013662, loss: 1.8753
2022-03-04 13:58:50 - train: epoch 0154, iter [03200, 05004], lr: 0.013662, loss: 2.0105
2022-03-04 13:59:24 - train: epoch 0154, iter [03300, 05004], lr: 0.013662, loss: 1.7694
2022-03-04 13:59:58 - train: epoch 0154, iter [03400, 05004], lr: 0.013662, loss: 1.9416
2022-03-04 14:00:32 - train: epoch 0154, iter [03500, 05004], lr: 0.013662, loss: 1.7553
2022-03-04 14:01:06 - train: epoch 0154, iter [03600, 05004], lr: 0.013662, loss: 1.8050
2022-03-04 14:01:39 - train: epoch 0154, iter [03700, 05004], lr: 0.013662, loss: 2.1374
2022-03-04 14:02:12 - train: epoch 0154, iter [03800, 05004], lr: 0.013662, loss: 1.8560
2022-03-04 14:02:47 - train: epoch 0154, iter [03900, 05004], lr: 0.013662, loss: 2.0228
2022-03-04 14:03:20 - train: epoch 0154, iter [04000, 05004], lr: 0.013662, loss: 1.8571
2022-03-04 14:03:54 - train: epoch 0154, iter [04100, 05004], lr: 0.013662, loss: 1.7747
2022-03-04 14:04:27 - train: epoch 0154, iter [04200, 05004], lr: 0.013662, loss: 1.9620
2022-03-04 14:05:01 - train: epoch 0154, iter [04300, 05004], lr: 0.013662, loss: 1.7616
2022-03-04 14:05:34 - train: epoch 0154, iter [04400, 05004], lr: 0.013662, loss: 2.0069
2022-03-04 14:06:08 - train: epoch 0154, iter [04500, 05004], lr: 0.013662, loss: 1.6865
2022-03-04 14:06:41 - train: epoch 0154, iter [04600, 05004], lr: 0.013662, loss: 2.0157
2022-03-04 14:07:15 - train: epoch 0154, iter [04700, 05004], lr: 0.013662, loss: 1.9297
2022-03-04 14:07:49 - train: epoch 0154, iter [04800, 05004], lr: 0.013662, loss: 1.9907
2022-03-04 14:08:22 - train: epoch 0154, iter [04900, 05004], lr: 0.013662, loss: 2.0171
2022-03-04 14:08:54 - train: epoch 0154, iter [05000, 05004], lr: 0.013662, loss: 1.7778
2022-03-04 14:08:55 - train: epoch 154, train_loss: 1.9277
2022-03-04 14:10:10 - eval: epoch: 154, acc1: 69.764%, acc5: 89.736%, test_loss: 1.2152, per_image_load_time: 0.609ms, per_image_inference_time: 0.487ms
2022-03-04 14:10:11 - until epoch: 154, best_acc1: 69.764%
2022-03-04 14:10:11 - epoch 155 lr: 0.013113465940953495
2022-03-04 14:10:51 - train: epoch 0155, iter [00100, 05004], lr: 0.013113, loss: 2.0151
2022-03-04 14:11:25 - train: epoch 0155, iter [00200, 05004], lr: 0.013113, loss: 2.0848
2022-03-04 14:11:58 - train: epoch 0155, iter [00300, 05004], lr: 0.013113, loss: 1.9430
2022-03-04 14:12:32 - train: epoch 0155, iter [00400, 05004], lr: 0.013113, loss: 2.0717
2022-03-04 14:13:07 - train: epoch 0155, iter [00500, 05004], lr: 0.013113, loss: 2.1034
2022-03-04 14:13:40 - train: epoch 0155, iter [00600, 05004], lr: 0.013113, loss: 1.8512
2022-03-04 14:14:14 - train: epoch 0155, iter [00700, 05004], lr: 0.013113, loss: 1.8971
2022-03-04 14:14:49 - train: epoch 0155, iter [00800, 05004], lr: 0.013113, loss: 1.7937
2022-03-04 14:15:22 - train: epoch 0155, iter [00900, 05004], lr: 0.013113, loss: 1.9040
2022-03-04 14:15:57 - train: epoch 0155, iter [01000, 05004], lr: 0.013113, loss: 2.2188
2022-03-04 14:16:30 - train: epoch 0155, iter [01100, 05004], lr: 0.013113, loss: 1.8192
2022-03-04 14:17:05 - train: epoch 0155, iter [01200, 05004], lr: 0.013113, loss: 1.8206
2022-03-04 14:17:38 - train: epoch 0155, iter [01300, 05004], lr: 0.013113, loss: 1.8602
2022-03-04 14:18:11 - train: epoch 0155, iter [01400, 05004], lr: 0.013113, loss: 2.0400
2022-03-04 14:18:46 - train: epoch 0155, iter [01500, 05004], lr: 0.013113, loss: 1.7218
2022-03-04 14:19:19 - train: epoch 0155, iter [01600, 05004], lr: 0.013113, loss: 1.8547
2022-03-04 14:19:53 - train: epoch 0155, iter [01700, 05004], lr: 0.013113, loss: 2.0359
2022-03-04 14:20:27 - train: epoch 0155, iter [01800, 05004], lr: 0.013113, loss: 2.0237
2022-03-04 14:21:01 - train: epoch 0155, iter [01900, 05004], lr: 0.013113, loss: 1.8789
2022-03-04 14:21:35 - train: epoch 0155, iter [02000, 05004], lr: 0.013113, loss: 2.0193
2022-03-04 14:22:09 - train: epoch 0155, iter [02100, 05004], lr: 0.013113, loss: 1.8772
2022-03-04 14:22:43 - train: epoch 0155, iter [02200, 05004], lr: 0.013113, loss: 1.8445
2022-03-04 14:23:16 - train: epoch 0155, iter [02300, 05004], lr: 0.013113, loss: 1.6153
2022-03-04 14:23:50 - train: epoch 0155, iter [02400, 05004], lr: 0.013113, loss: 2.0225
2022-03-04 14:24:24 - train: epoch 0155, iter [02500, 05004], lr: 0.013113, loss: 1.9604
2022-03-04 14:24:58 - train: epoch 0155, iter [02600, 05004], lr: 0.013113, loss: 1.9150
2022-03-04 14:25:31 - train: epoch 0155, iter [02700, 05004], lr: 0.013113, loss: 2.0313
2022-03-04 14:26:05 - train: epoch 0155, iter [02800, 05004], lr: 0.013113, loss: 2.0257
2022-03-04 14:26:39 - train: epoch 0155, iter [02900, 05004], lr: 0.013113, loss: 1.8632
2022-03-04 14:27:13 - train: epoch 0155, iter [03000, 05004], lr: 0.013113, loss: 1.6231
2022-03-04 14:27:47 - train: epoch 0155, iter [03100, 05004], lr: 0.013113, loss: 1.9908
2022-03-04 14:28:21 - train: epoch 0155, iter [03200, 05004], lr: 0.013113, loss: 2.2116
2022-03-04 14:28:55 - train: epoch 0155, iter [03300, 05004], lr: 0.013113, loss: 2.0346
2022-03-04 14:29:28 - train: epoch 0155, iter [03400, 05004], lr: 0.013113, loss: 1.9269
2022-03-04 14:30:02 - train: epoch 0155, iter [03500, 05004], lr: 0.013113, loss: 2.0263
2022-03-04 14:30:36 - train: epoch 0155, iter [03600, 05004], lr: 0.013113, loss: 2.2018
2022-03-04 14:31:10 - train: epoch 0155, iter [03700, 05004], lr: 0.013113, loss: 1.8071
2022-03-04 14:31:43 - train: epoch 0155, iter [03800, 05004], lr: 0.013113, loss: 2.1046
2022-03-04 14:32:17 - train: epoch 0155, iter [03900, 05004], lr: 0.013113, loss: 1.9251
2022-03-04 14:32:51 - train: epoch 0155, iter [04000, 05004], lr: 0.013113, loss: 2.0898
2022-03-04 14:33:26 - train: epoch 0155, iter [04100, 05004], lr: 0.013113, loss: 1.7926
2022-03-04 14:33:59 - train: epoch 0155, iter [04200, 05004], lr: 0.013113, loss: 1.9950
2022-03-04 14:34:32 - train: epoch 0155, iter [04300, 05004], lr: 0.013113, loss: 1.9626
2022-03-04 14:35:06 - train: epoch 0155, iter [04400, 05004], lr: 0.013113, loss: 1.6919
2022-03-04 14:35:40 - train: epoch 0155, iter [04500, 05004], lr: 0.013113, loss: 1.6244
2022-03-04 14:36:14 - train: epoch 0155, iter [04600, 05004], lr: 0.013113, loss: 2.1082
2022-03-04 14:36:48 - train: epoch 0155, iter [04700, 05004], lr: 0.013113, loss: 1.8922
2022-03-04 14:37:23 - train: epoch 0155, iter [04800, 05004], lr: 0.013113, loss: 1.9933
2022-03-04 14:37:56 - train: epoch 0155, iter [04900, 05004], lr: 0.013113, loss: 1.8436
2022-03-04 14:38:28 - train: epoch 0155, iter [05000, 05004], lr: 0.013113, loss: 1.7056
2022-03-04 14:38:29 - train: epoch 155, train_loss: 1.9149
2022-03-04 14:39:44 - eval: epoch: 155, acc1: 69.782%, acc5: 89.820%, test_loss: 1.2082, per_image_load_time: 0.675ms, per_image_inference_time: 0.483ms
2022-03-04 14:39:45 - until epoch: 155, best_acc1: 69.782%
2022-03-04 14:39:45 - epoch 156 lr: 0.012574462591444941
2022-03-04 14:40:23 - train: epoch 0156, iter [00100, 05004], lr: 0.012574, loss: 2.0831
2022-03-04 14:40:57 - train: epoch 0156, iter [00200, 05004], lr: 0.012574, loss: 1.9737
2022-03-04 14:41:32 - train: epoch 0156, iter [00300, 05004], lr: 0.012574, loss: 1.6457
2022-03-04 14:42:05 - train: epoch 0156, iter [00400, 05004], lr: 0.012574, loss: 1.7399
2022-03-04 14:42:39 - train: epoch 0156, iter [00500, 05004], lr: 0.012574, loss: 1.7015
2022-03-04 14:43:14 - train: epoch 0156, iter [00600, 05004], lr: 0.012574, loss: 1.8852
2022-03-04 14:43:47 - train: epoch 0156, iter [00700, 05004], lr: 0.012574, loss: 1.7413
2022-03-04 14:44:20 - train: epoch 0156, iter [00800, 05004], lr: 0.012574, loss: 2.1351
2022-03-04 14:44:55 - train: epoch 0156, iter [00900, 05004], lr: 0.012574, loss: 2.0632
2022-03-04 14:45:29 - train: epoch 0156, iter [01000, 05004], lr: 0.012574, loss: 1.8910
2022-03-04 14:46:04 - train: epoch 0156, iter [01100, 05004], lr: 0.012574, loss: 1.8964
2022-03-04 14:46:37 - train: epoch 0156, iter [01200, 05004], lr: 0.012574, loss: 1.9181
2022-03-04 14:47:11 - train: epoch 0156, iter [01300, 05004], lr: 0.012574, loss: 2.0113
2022-03-04 14:47:45 - train: epoch 0156, iter [01400, 05004], lr: 0.012574, loss: 1.7727
2022-03-04 14:48:19 - train: epoch 0156, iter [01500, 05004], lr: 0.012574, loss: 2.3002
2022-03-04 14:48:53 - train: epoch 0156, iter [01600, 05004], lr: 0.012574, loss: 1.7510
2022-03-04 14:49:27 - train: epoch 0156, iter [01700, 05004], lr: 0.012574, loss: 1.9431
2022-03-04 14:50:01 - train: epoch 0156, iter [01800, 05004], lr: 0.012574, loss: 1.7589
2022-03-04 14:50:34 - train: epoch 0156, iter [01900, 05004], lr: 0.012574, loss: 1.7348
2022-03-04 14:51:08 - train: epoch 0156, iter [02000, 05004], lr: 0.012574, loss: 1.9371
2022-03-04 14:51:41 - train: epoch 0156, iter [02100, 05004], lr: 0.012574, loss: 1.9276
2022-03-04 14:52:16 - train: epoch 0156, iter [02200, 05004], lr: 0.012574, loss: 2.1582
2022-03-04 14:52:49 - train: epoch 0156, iter [02300, 05004], lr: 0.012574, loss: 1.7991
2022-03-04 14:53:24 - train: epoch 0156, iter [02400, 05004], lr: 0.012574, loss: 2.0042
2022-03-04 14:53:57 - train: epoch 0156, iter [02500, 05004], lr: 0.012574, loss: 1.7572
2022-03-04 14:54:31 - train: epoch 0156, iter [02600, 05004], lr: 0.012574, loss: 1.8157
2022-03-04 14:55:04 - train: epoch 0156, iter [02700, 05004], lr: 0.012574, loss: 1.7455
2022-03-04 14:55:39 - train: epoch 0156, iter [02800, 05004], lr: 0.012574, loss: 2.0481
2022-03-04 14:56:12 - train: epoch 0156, iter [02900, 05004], lr: 0.012574, loss: 1.7692
2022-03-04 14:56:48 - train: epoch 0156, iter [03000, 05004], lr: 0.012574, loss: 1.6319
2022-03-04 14:57:20 - train: epoch 0156, iter [03100, 05004], lr: 0.012574, loss: 1.9963
2022-03-04 14:57:54 - train: epoch 0156, iter [03200, 05004], lr: 0.012574, loss: 1.8228
2022-03-04 14:58:28 - train: epoch 0156, iter [03300, 05004], lr: 0.012574, loss: 1.8642
2022-03-04 14:59:02 - train: epoch 0156, iter [03400, 05004], lr: 0.012574, loss: 1.8670
2022-03-04 14:59:36 - train: epoch 0156, iter [03500, 05004], lr: 0.012574, loss: 2.0037
2022-03-04 15:00:10 - train: epoch 0156, iter [03600, 05004], lr: 0.012574, loss: 2.1785
2022-03-04 15:00:43 - train: epoch 0156, iter [03700, 05004], lr: 0.012574, loss: 1.9570
2022-03-04 15:01:17 - train: epoch 0156, iter [03800, 05004], lr: 0.012574, loss: 1.8542
2022-03-04 15:01:51 - train: epoch 0156, iter [03900, 05004], lr: 0.012574, loss: 2.0198
2022-03-04 15:02:26 - train: epoch 0156, iter [04000, 05004], lr: 0.012574, loss: 1.7764
2022-03-04 15:03:01 - train: epoch 0156, iter [04100, 05004], lr: 0.012574, loss: 1.9800
2022-03-04 15:03:34 - train: epoch 0156, iter [04200, 05004], lr: 0.012574, loss: 1.8230
2022-03-04 15:04:08 - train: epoch 0156, iter [04300, 05004], lr: 0.012574, loss: 1.7189
2022-03-04 15:04:42 - train: epoch 0156, iter [04400, 05004], lr: 0.012574, loss: 1.9485
2022-03-04 15:05:16 - train: epoch 0156, iter [04500, 05004], lr: 0.012574, loss: 1.9188
2022-03-04 15:05:50 - train: epoch 0156, iter [04600, 05004], lr: 0.012574, loss: 1.9632
2022-03-04 15:06:24 - train: epoch 0156, iter [04700, 05004], lr: 0.012574, loss: 1.7885
2022-03-04 15:06:58 - train: epoch 0156, iter [04800, 05004], lr: 0.012574, loss: 2.0559
2022-03-04 15:07:32 - train: epoch 0156, iter [04900, 05004], lr: 0.012574, loss: 2.0000
2022-03-04 15:08:04 - train: epoch 0156, iter [05000, 05004], lr: 0.012574, loss: 1.7947
2022-03-04 15:08:05 - train: epoch 156, train_loss: 1.9002
2022-03-04 15:09:21 - eval: epoch: 156, acc1: 69.798%, acc5: 89.744%, test_loss: 1.2118, per_image_load_time: 0.609ms, per_image_inference_time: 0.496ms
2022-03-04 15:09:22 - until epoch: 156, best_acc1: 69.798%
2022-03-04 15:09:22 - epoch 157 lr: 0.01204517304231343
2022-03-04 15:10:01 - train: epoch 0157, iter [00100, 05004], lr: 0.012045, loss: 1.8424
2022-03-04 15:10:35 - train: epoch 0157, iter [00200, 05004], lr: 0.012045, loss: 1.9697
2022-03-04 15:11:09 - train: epoch 0157, iter [00300, 05004], lr: 0.012045, loss: 1.7818
2022-03-04 15:11:44 - train: epoch 0157, iter [00400, 05004], lr: 0.012045, loss: 2.0339
2022-03-04 15:12:18 - train: epoch 0157, iter [00500, 05004], lr: 0.012045, loss: 1.9562
2022-03-04 15:12:52 - train: epoch 0157, iter [00600, 05004], lr: 0.012045, loss: 1.9834
2022-03-04 15:13:24 - train: epoch 0157, iter [00700, 05004], lr: 0.012045, loss: 1.9707
2022-03-04 15:13:59 - train: epoch 0157, iter [00800, 05004], lr: 0.012045, loss: 1.9445
2022-03-04 15:14:34 - train: epoch 0157, iter [00900, 05004], lr: 0.012045, loss: 1.5968
2022-03-04 15:15:08 - train: epoch 0157, iter [01000, 05004], lr: 0.012045, loss: 1.8738
2022-03-04 15:15:42 - train: epoch 0157, iter [01100, 05004], lr: 0.012045, loss: 1.7796
2022-03-04 15:16:16 - train: epoch 0157, iter [01200, 05004], lr: 0.012045, loss: 1.8107
2022-03-04 15:16:49 - train: epoch 0157, iter [01300, 05004], lr: 0.012045, loss: 1.7495
2022-03-04 15:17:24 - train: epoch 0157, iter [01400, 05004], lr: 0.012045, loss: 1.8601
2022-03-04 15:17:58 - train: epoch 0157, iter [01500, 05004], lr: 0.012045, loss: 1.9957
2022-03-04 15:18:31 - train: epoch 0157, iter [01600, 05004], lr: 0.012045, loss: 1.7381
2022-03-04 15:19:05 - train: epoch 0157, iter [01700, 05004], lr: 0.012045, loss: 1.7079
2022-03-04 15:19:39 - train: epoch 0157, iter [01800, 05004], lr: 0.012045, loss: 1.7334
2022-03-04 15:20:12 - train: epoch 0157, iter [01900, 05004], lr: 0.012045, loss: 1.6973
2022-03-04 15:20:46 - train: epoch 0157, iter [02000, 05004], lr: 0.012045, loss: 1.9493
2022-03-04 15:21:20 - train: epoch 0157, iter [02100, 05004], lr: 0.012045, loss: 1.9592
2022-03-04 15:21:54 - train: epoch 0157, iter [02200, 05004], lr: 0.012045, loss: 1.8700
2022-03-04 15:22:28 - train: epoch 0157, iter [02300, 05004], lr: 0.012045, loss: 1.6902
2022-03-04 15:23:02 - train: epoch 0157, iter [02400, 05004], lr: 0.012045, loss: 1.6713
2022-03-04 15:23:35 - train: epoch 0157, iter [02500, 05004], lr: 0.012045, loss: 2.0357
2022-03-04 15:24:09 - train: epoch 0157, iter [02600, 05004], lr: 0.012045, loss: 1.8790
2022-03-04 15:24:43 - train: epoch 0157, iter [02700, 05004], lr: 0.012045, loss: 1.7955
2022-03-04 15:25:17 - train: epoch 0157, iter [02800, 05004], lr: 0.012045, loss: 1.9047
2022-03-04 15:25:51 - train: epoch 0157, iter [02900, 05004], lr: 0.012045, loss: 1.6620
2022-03-04 15:26:25 - train: epoch 0157, iter [03000, 05004], lr: 0.012045, loss: 1.8831
2022-03-04 15:26:58 - train: epoch 0157, iter [03100, 05004], lr: 0.012045, loss: 2.0074
2022-03-04 15:27:33 - train: epoch 0157, iter [03200, 05004], lr: 0.012045, loss: 1.7932
2022-03-04 15:28:06 - train: epoch 0157, iter [03300, 05004], lr: 0.012045, loss: 1.9552
2022-03-04 15:28:40 - train: epoch 0157, iter [03400, 05004], lr: 0.012045, loss: 1.6634
2022-03-04 15:29:14 - train: epoch 0157, iter [03500, 05004], lr: 0.012045, loss: 1.9500
2022-03-04 15:29:47 - train: epoch 0157, iter [03600, 05004], lr: 0.012045, loss: 2.0251
2022-03-04 15:30:21 - train: epoch 0157, iter [03700, 05004], lr: 0.012045, loss: 1.9661
2022-03-04 15:30:54 - train: epoch 0157, iter [03800, 05004], lr: 0.012045, loss: 1.7551
2022-03-04 15:31:28 - train: epoch 0157, iter [03900, 05004], lr: 0.012045, loss: 1.9408
2022-03-04 15:32:02 - train: epoch 0157, iter [04000, 05004], lr: 0.012045, loss: 1.7206
2022-03-04 15:32:35 - train: epoch 0157, iter [04100, 05004], lr: 0.012045, loss: 1.8815
2022-03-04 15:33:08 - train: epoch 0157, iter [04200, 05004], lr: 0.012045, loss: 1.6679
2022-03-04 15:33:42 - train: epoch 0157, iter [04300, 05004], lr: 0.012045, loss: 1.8855
2022-03-04 15:34:16 - train: epoch 0157, iter [04400, 05004], lr: 0.012045, loss: 2.0375
2022-03-04 15:34:50 - train: epoch 0157, iter [04500, 05004], lr: 0.012045, loss: 2.0671
2022-03-04 15:35:24 - train: epoch 0157, iter [04600, 05004], lr: 0.012045, loss: 1.8468
2022-03-04 15:35:56 - train: epoch 0157, iter [04700, 05004], lr: 0.012045, loss: 2.0270
2022-03-04 15:36:29 - train: epoch 0157, iter [04800, 05004], lr: 0.012045, loss: 1.8506
2022-03-04 15:37:03 - train: epoch 0157, iter [04900, 05004], lr: 0.012045, loss: 1.9103
2022-03-04 15:37:35 - train: epoch 0157, iter [05000, 05004], lr: 0.012045, loss: 2.1523
2022-03-04 15:37:37 - train: epoch 157, train_loss: 1.8860
2022-03-04 15:38:51 - eval: epoch: 157, acc1: 70.592%, acc5: 90.220%, test_loss: 1.1764, per_image_load_time: 0.560ms, per_image_inference_time: 0.430ms
2022-03-04 15:38:52 - until epoch: 157, best_acc1: 70.592%
2022-03-04 15:38:52 - epoch 158 lr: 0.011525734670691701
2022-03-04 15:39:31 - train: epoch 0158, iter [00100, 05004], lr: 0.011526, loss: 1.9577
2022-03-04 15:40:04 - train: epoch 0158, iter [00200, 05004], lr: 0.011526, loss: 1.9348
2022-03-04 15:40:38 - train: epoch 0158, iter [00300, 05004], lr: 0.011526, loss: 1.6644
2022-03-04 15:41:11 - train: epoch 0158, iter [00400, 05004], lr: 0.011526, loss: 1.8247
2022-03-04 15:41:45 - train: epoch 0158, iter [00500, 05004], lr: 0.011526, loss: 1.7455
2022-03-04 15:42:19 - train: epoch 0158, iter [00600, 05004], lr: 0.011526, loss: 1.7346
2022-03-04 15:42:52 - train: epoch 0158, iter [00700, 05004], lr: 0.011526, loss: 1.8109
2022-03-04 15:43:25 - train: epoch 0158, iter [00800, 05004], lr: 0.011526, loss: 1.9216
2022-03-04 15:43:59 - train: epoch 0158, iter [00900, 05004], lr: 0.011526, loss: 1.9883
2022-03-04 15:44:33 - train: epoch 0158, iter [01000, 05004], lr: 0.011526, loss: 1.8082
2022-03-04 15:45:06 - train: epoch 0158, iter [01100, 05004], lr: 0.011526, loss: 1.8833
2022-03-04 15:45:41 - train: epoch 0158, iter [01200, 05004], lr: 0.011526, loss: 1.8470
2022-03-04 15:46:14 - train: epoch 0158, iter [01300, 05004], lr: 0.011526, loss: 1.8674
2022-03-04 15:46:47 - train: epoch 0158, iter [01400, 05004], lr: 0.011526, loss: 1.6409
2022-03-04 15:47:21 - train: epoch 0158, iter [01500, 05004], lr: 0.011526, loss: 1.7302
2022-03-04 15:47:55 - train: epoch 0158, iter [01600, 05004], lr: 0.011526, loss: 2.2851
2022-03-04 15:48:28 - train: epoch 0158, iter [01700, 05004], lr: 0.011526, loss: 1.8569
2022-03-04 15:49:03 - train: epoch 0158, iter [01800, 05004], lr: 0.011526, loss: 1.8350
2022-03-04 15:49:36 - train: epoch 0158, iter [01900, 05004], lr: 0.011526, loss: 1.7815
2022-03-04 15:50:10 - train: epoch 0158, iter [02000, 05004], lr: 0.011526, loss: 1.9848
2022-03-04 15:50:43 - train: epoch 0158, iter [02100, 05004], lr: 0.011526, loss: 1.9313
2022-03-04 15:51:18 - train: epoch 0158, iter [02200, 05004], lr: 0.011526, loss: 1.8349
2022-03-04 15:51:52 - train: epoch 0158, iter [02300, 05004], lr: 0.011526, loss: 1.9897
2022-03-04 15:52:26 - train: epoch 0158, iter [02400, 05004], lr: 0.011526, loss: 1.9488
2022-03-04 15:52:59 - train: epoch 0158, iter [02500, 05004], lr: 0.011526, loss: 1.9113
2022-03-04 15:53:33 - train: epoch 0158, iter [02600, 05004], lr: 0.011526, loss: 1.9201
2022-03-04 15:54:07 - train: epoch 0158, iter [02700, 05004], lr: 0.011526, loss: 1.8881
2022-03-04 15:54:41 - train: epoch 0158, iter [02800, 05004], lr: 0.011526, loss: 2.1525
2022-03-04 15:55:15 - train: epoch 0158, iter [02900, 05004], lr: 0.011526, loss: 1.8655
2022-03-04 15:55:49 - train: epoch 0158, iter [03000, 05004], lr: 0.011526, loss: 1.5488
2022-03-04 15:56:21 - train: epoch 0158, iter [03100, 05004], lr: 0.011526, loss: 1.7853
2022-03-04 15:56:56 - train: epoch 0158, iter [03200, 05004], lr: 0.011526, loss: 1.9547
2022-03-04 15:57:30 - train: epoch 0158, iter [03300, 05004], lr: 0.011526, loss: 1.5259
2022-03-04 15:58:03 - train: epoch 0158, iter [03400, 05004], lr: 0.011526, loss: 1.7785
2022-03-04 15:58:38 - train: epoch 0158, iter [03500, 05004], lr: 0.011526, loss: 1.8005
2022-03-04 15:59:12 - train: epoch 0158, iter [03600, 05004], lr: 0.011526, loss: 1.8660
2022-03-04 15:59:44 - train: epoch 0158, iter [03700, 05004], lr: 0.011526, loss: 1.8167
2022-03-04 16:00:18 - train: epoch 0158, iter [03800, 05004], lr: 0.011526, loss: 2.0492
2022-03-04 16:00:52 - train: epoch 0158, iter [03900, 05004], lr: 0.011526, loss: 1.6911
2022-03-04 16:01:26 - train: epoch 0158, iter [04000, 05004], lr: 0.011526, loss: 1.7678
2022-03-04 16:02:00 - train: epoch 0158, iter [04100, 05004], lr: 0.011526, loss: 1.9253
2022-03-04 16:02:33 - train: epoch 0158, iter [04200, 05004], lr: 0.011526, loss: 1.9373
2022-03-04 16:03:06 - train: epoch 0158, iter [04300, 05004], lr: 0.011526, loss: 1.8247
2022-03-04 16:03:39 - train: epoch 0158, iter [04400, 05004], lr: 0.011526, loss: 1.7453
2022-03-04 16:04:12 - train: epoch 0158, iter [04500, 05004], lr: 0.011526, loss: 1.8101
2022-03-04 16:04:44 - train: epoch 0158, iter [04600, 05004], lr: 0.011526, loss: 1.7258
2022-03-04 16:05:19 - train: epoch 0158, iter [04700, 05004], lr: 0.011526, loss: 1.7021
2022-03-04 16:05:52 - train: epoch 0158, iter [04800, 05004], lr: 0.011526, loss: 2.0183
2022-03-04 16:06:25 - train: epoch 0158, iter [04900, 05004], lr: 0.011526, loss: 1.8539
2022-03-04 16:06:58 - train: epoch 0158, iter [05000, 05004], lr: 0.011526, loss: 1.9338
2022-03-04 16:06:59 - train: epoch 158, train_loss: 1.8769
2022-03-04 16:08:13 - eval: epoch: 158, acc1: 70.614%, acc5: 90.252%, test_loss: 1.1720, per_image_load_time: 2.198ms, per_image_inference_time: 0.508ms
2022-03-04 16:08:14 - until epoch: 158, best_acc1: 70.614%
2022-03-04 16:08:14 - epoch 159 lr: 0.011016282296838887
2022-03-04 16:08:54 - train: epoch 0159, iter [00100, 05004], lr: 0.011016, loss: 1.6646
2022-03-04 16:09:28 - train: epoch 0159, iter [00200, 05004], lr: 0.011016, loss: 1.8216
2022-03-04 16:10:01 - train: epoch 0159, iter [00300, 05004], lr: 0.011016, loss: 2.0745
2022-03-04 16:10:36 - train: epoch 0159, iter [00400, 05004], lr: 0.011016, loss: 1.8801
2022-03-04 16:11:09 - train: epoch 0159, iter [00500, 05004], lr: 0.011016, loss: 1.8711
2022-03-04 16:11:43 - train: epoch 0159, iter [00600, 05004], lr: 0.011016, loss: 1.6076
2022-03-04 16:12:17 - train: epoch 0159, iter [00700, 05004], lr: 0.011016, loss: 1.9008
2022-03-04 16:12:51 - train: epoch 0159, iter [00800, 05004], lr: 0.011016, loss: 1.7312
2022-03-04 16:13:24 - train: epoch 0159, iter [00900, 05004], lr: 0.011016, loss: 1.8384
2022-03-04 16:13:58 - train: epoch 0159, iter [01000, 05004], lr: 0.011016, loss: 1.9933
2022-03-04 16:14:31 - train: epoch 0159, iter [01100, 05004], lr: 0.011016, loss: 1.9758
2022-03-04 16:15:06 - train: epoch 0159, iter [01200, 05004], lr: 0.011016, loss: 1.6837
2022-03-04 16:15:40 - train: epoch 0159, iter [01300, 05004], lr: 0.011016, loss: 1.6116
2022-03-04 16:16:13 - train: epoch 0159, iter [01400, 05004], lr: 0.011016, loss: 1.9197
2022-03-04 16:16:47 - train: epoch 0159, iter [01500, 05004], lr: 0.011016, loss: 1.9466
2022-03-04 16:17:21 - train: epoch 0159, iter [01600, 05004], lr: 0.011016, loss: 1.7758
2022-03-04 16:17:54 - train: epoch 0159, iter [01700, 05004], lr: 0.011016, loss: 1.6123
2022-03-04 16:18:29 - train: epoch 0159, iter [01800, 05004], lr: 0.011016, loss: 1.8690
2022-03-04 16:19:02 - train: epoch 0159, iter [01900, 05004], lr: 0.011016, loss: 1.7260
2022-03-04 16:19:36 - train: epoch 0159, iter [02000, 05004], lr: 0.011016, loss: 1.7723
2022-03-04 16:20:10 - train: epoch 0159, iter [02100, 05004], lr: 0.011016, loss: 1.9376
2022-03-04 16:20:44 - train: epoch 0159, iter [02200, 05004], lr: 0.011016, loss: 2.0001
2022-03-04 16:21:18 - train: epoch 0159, iter [02300, 05004], lr: 0.011016, loss: 1.7890
2022-03-04 16:21:52 - train: epoch 0159, iter [02400, 05004], lr: 0.011016, loss: 1.8052
2022-03-04 16:22:25 - train: epoch 0159, iter [02500, 05004], lr: 0.011016, loss: 1.7714
2022-03-04 16:22:59 - train: epoch 0159, iter [02600, 05004], lr: 0.011016, loss: 1.9201
2022-03-04 16:23:33 - train: epoch 0159, iter [02700, 05004], lr: 0.011016, loss: 1.8324
2022-03-04 16:24:07 - train: epoch 0159, iter [02800, 05004], lr: 0.011016, loss: 1.9850
2022-03-04 16:24:41 - train: epoch 0159, iter [02900, 05004], lr: 0.011016, loss: 2.2685
2022-03-04 16:25:15 - train: epoch 0159, iter [03000, 05004], lr: 0.011016, loss: 1.7694
2022-03-04 16:25:48 - train: epoch 0159, iter [03100, 05004], lr: 0.011016, loss: 2.1131
2022-03-04 16:26:23 - train: epoch 0159, iter [03200, 05004], lr: 0.011016, loss: 1.9932
2022-03-04 16:26:56 - train: epoch 0159, iter [03300, 05004], lr: 0.011016, loss: 1.9826
2022-03-04 16:27:31 - train: epoch 0159, iter [03400, 05004], lr: 0.011016, loss: 1.8400
2022-03-04 16:28:05 - train: epoch 0159, iter [03500, 05004], lr: 0.011016, loss: 2.0238
2022-03-04 16:28:38 - train: epoch 0159, iter [03600, 05004], lr: 0.011016, loss: 1.6828
2022-03-04 16:29:13 - train: epoch 0159, iter [03700, 05004], lr: 0.011016, loss: 1.8702
2022-03-04 16:29:45 - train: epoch 0159, iter [03800, 05004], lr: 0.011016, loss: 1.8228
2022-03-04 16:30:19 - train: epoch 0159, iter [03900, 05004], lr: 0.011016, loss: 1.7014
2022-03-04 16:30:54 - train: epoch 0159, iter [04000, 05004], lr: 0.011016, loss: 2.0213
2022-03-04 16:31:28 - train: epoch 0159, iter [04100, 05004], lr: 0.011016, loss: 1.9691
2022-03-04 16:32:02 - train: epoch 0159, iter [04200, 05004], lr: 0.011016, loss: 1.8643
2022-03-04 16:32:35 - train: epoch 0159, iter [04300, 05004], lr: 0.011016, loss: 2.0218
2022-03-04 16:33:09 - train: epoch 0159, iter [04400, 05004], lr: 0.011016, loss: 1.8685
2022-03-04 16:33:43 - train: epoch 0159, iter [04500, 05004], lr: 0.011016, loss: 1.7606
2022-03-04 16:34:16 - train: epoch 0159, iter [04600, 05004], lr: 0.011016, loss: 2.1373
2022-03-04 16:34:50 - train: epoch 0159, iter [04700, 05004], lr: 0.011016, loss: 1.8885
2022-03-04 16:35:24 - train: epoch 0159, iter [04800, 05004], lr: 0.011016, loss: 1.9293
2022-03-04 16:35:57 - train: epoch 0159, iter [04900, 05004], lr: 0.011016, loss: 1.8774
2022-03-04 16:36:30 - train: epoch 0159, iter [05000, 05004], lr: 0.011016, loss: 1.9746
2022-03-04 16:36:32 - train: epoch 159, train_loss: 1.8652
2022-03-04 16:37:45 - eval: epoch: 159, acc1: 70.728%, acc5: 90.150%, test_loss: 1.1726, per_image_load_time: 0.830ms, per_image_inference_time: 0.486ms
2022-03-04 16:37:46 - until epoch: 159, best_acc1: 70.728%
2022-03-04 16:37:46 - epoch 160 lr: 0.010516948149147755
2022-03-04 16:38:26 - train: epoch 0160, iter [00100, 05004], lr: 0.010517, loss: 1.9535
2022-03-04 16:38:59 - train: epoch 0160, iter [00200, 05004], lr: 0.010517, loss: 1.9208
2022-03-04 16:39:32 - train: epoch 0160, iter [00300, 05004], lr: 0.010517, loss: 1.7814
2022-03-04 16:40:06 - train: epoch 0160, iter [00400, 05004], lr: 0.010517, loss: 1.5345
2022-03-04 16:40:41 - train: epoch 0160, iter [00500, 05004], lr: 0.010517, loss: 1.9087
2022-03-04 16:41:15 - train: epoch 0160, iter [00600, 05004], lr: 0.010517, loss: 2.1250
2022-03-04 16:41:49 - train: epoch 0160, iter [00700, 05004], lr: 0.010517, loss: 1.7567
2022-03-04 16:42:23 - train: epoch 0160, iter [00800, 05004], lr: 0.010517, loss: 1.7256
2022-03-04 16:42:56 - train: epoch 0160, iter [00900, 05004], lr: 0.010517, loss: 1.8248
2022-03-04 16:43:30 - train: epoch 0160, iter [01000, 05004], lr: 0.010517, loss: 1.8823
2022-03-04 16:44:03 - train: epoch 0160, iter [01100, 05004], lr: 0.010517, loss: 1.9026
2022-03-04 16:44:37 - train: epoch 0160, iter [01200, 05004], lr: 0.010517, loss: 1.8888
2022-03-04 16:45:11 - train: epoch 0160, iter [01300, 05004], lr: 0.010517, loss: 1.7871
2022-03-04 16:45:45 - train: epoch 0160, iter [01400, 05004], lr: 0.010517, loss: 1.9767
2022-03-04 16:46:19 - train: epoch 0160, iter [01500, 05004], lr: 0.010517, loss: 1.5421
2022-03-04 16:46:53 - train: epoch 0160, iter [01600, 05004], lr: 0.010517, loss: 1.7747
2022-03-04 16:47:27 - train: epoch 0160, iter [01700, 05004], lr: 0.010517, loss: 1.8012
2022-03-04 16:48:01 - train: epoch 0160, iter [01800, 05004], lr: 0.010517, loss: 1.9831
2022-03-04 16:48:35 - train: epoch 0160, iter [01900, 05004], lr: 0.010517, loss: 2.0230
2022-03-04 16:49:08 - train: epoch 0160, iter [02000, 05004], lr: 0.010517, loss: 1.8446
2022-03-04 16:49:42 - train: epoch 0160, iter [02100, 05004], lr: 0.010517, loss: 1.8963
2022-03-04 16:50:16 - train: epoch 0160, iter [02200, 05004], lr: 0.010517, loss: 1.9688
2022-03-04 16:50:50 - train: epoch 0160, iter [02300, 05004], lr: 0.010517, loss: 1.6266
2022-03-04 16:51:23 - train: epoch 0160, iter [02400, 05004], lr: 0.010517, loss: 1.7245
2022-03-04 16:51:58 - train: epoch 0160, iter [02500, 05004], lr: 0.010517, loss: 1.6476
2022-03-04 16:52:32 - train: epoch 0160, iter [02600, 05004], lr: 0.010517, loss: 1.7230
2022-03-04 16:53:04 - train: epoch 0160, iter [02700, 05004], lr: 0.010517, loss: 1.7025
2022-03-04 16:53:38 - train: epoch 0160, iter [02800, 05004], lr: 0.010517, loss: 1.8657
2022-03-04 16:54:12 - train: epoch 0160, iter [02900, 05004], lr: 0.010517, loss: 1.8970
2022-03-04 16:54:46 - train: epoch 0160, iter [03000, 05004], lr: 0.010517, loss: 2.0505
2022-03-04 16:55:20 - train: epoch 0160, iter [03100, 05004], lr: 0.010517, loss: 1.6865
2022-03-04 16:55:53 - train: epoch 0160, iter [03200, 05004], lr: 0.010517, loss: 2.0051
2022-03-04 16:56:26 - train: epoch 0160, iter [03300, 05004], lr: 0.010517, loss: 1.6976
2022-03-04 16:57:00 - train: epoch 0160, iter [03400, 05004], lr: 0.010517, loss: 2.0241
2022-03-04 16:57:35 - train: epoch 0160, iter [03500, 05004], lr: 0.010517, loss: 1.8145
2022-03-04 16:58:08 - train: epoch 0160, iter [03600, 05004], lr: 0.010517, loss: 1.8455
2022-03-04 16:58:42 - train: epoch 0160, iter [03700, 05004], lr: 0.010517, loss: 1.8033
2022-03-04 16:59:14 - train: epoch 0160, iter [03800, 05004], lr: 0.010517, loss: 2.0926
2022-03-04 16:59:49 - train: epoch 0160, iter [03900, 05004], lr: 0.010517, loss: 2.0311
2022-03-04 17:00:22 - train: epoch 0160, iter [04000, 05004], lr: 0.010517, loss: 1.9834
2022-03-04 17:00:57 - train: epoch 0160, iter [04100, 05004], lr: 0.010517, loss: 1.8239
2022-03-04 17:01:30 - train: epoch 0160, iter [04200, 05004], lr: 0.010517, loss: 1.6760
2022-03-04 17:02:05 - train: epoch 0160, iter [04300, 05004], lr: 0.010517, loss: 1.5464
2022-03-04 17:02:38 - train: epoch 0160, iter [04400, 05004], lr: 0.010517, loss: 2.0444
2022-03-04 17:03:12 - train: epoch 0160, iter [04500, 05004], lr: 0.010517, loss: 1.8786
2022-03-04 17:03:46 - train: epoch 0160, iter [04600, 05004], lr: 0.010517, loss: 2.0607
2022-03-04 17:04:20 - train: epoch 0160, iter [04700, 05004], lr: 0.010517, loss: 1.8374
2022-03-04 17:04:54 - train: epoch 0160, iter [04800, 05004], lr: 0.010517, loss: 1.8700
2022-03-04 17:05:29 - train: epoch 0160, iter [04900, 05004], lr: 0.010517, loss: 1.9448
2022-03-04 17:06:00 - train: epoch 0160, iter [05000, 05004], lr: 0.010517, loss: 1.7301
2022-03-04 17:06:01 - train: epoch 160, train_loss: 1.8498
2022-03-04 17:07:15 - eval: epoch: 160, acc1: 70.986%, acc5: 90.274%, test_loss: 1.1692, per_image_load_time: 1.156ms, per_image_inference_time: 0.512ms
2022-03-04 17:07:16 - until epoch: 160, best_acc1: 70.986%
2022-03-04 17:07:16 - epoch 161 lr: 0.010027861829824953
2022-03-04 17:07:55 - train: epoch 0161, iter [00100, 05004], lr: 0.010028, loss: 1.8710
2022-03-04 17:08:30 - train: epoch 0161, iter [00200, 05004], lr: 0.010028, loss: 2.1181
2022-03-04 17:09:03 - train: epoch 0161, iter [00300, 05004], lr: 0.010028, loss: 1.7598
2022-03-04 17:09:37 - train: epoch 0161, iter [00400, 05004], lr: 0.010028, loss: 1.9008
2022-03-04 17:10:10 - train: epoch 0161, iter [00500, 05004], lr: 0.010028, loss: 1.8006
2022-03-04 17:10:43 - train: epoch 0161, iter [00600, 05004], lr: 0.010028, loss: 1.8621
2022-03-04 17:11:18 - train: epoch 0161, iter [00700, 05004], lr: 0.010028, loss: 1.8016
2022-03-04 17:11:52 - train: epoch 0161, iter [00800, 05004], lr: 0.010028, loss: 1.7751
2022-03-04 17:12:25 - train: epoch 0161, iter [00900, 05004], lr: 0.010028, loss: 1.7194
2022-03-04 17:12:58 - train: epoch 0161, iter [01000, 05004], lr: 0.010028, loss: 1.7885
2022-03-04 17:13:33 - train: epoch 0161, iter [01100, 05004], lr: 0.010028, loss: 1.9291
2022-03-04 17:14:07 - train: epoch 0161, iter [01200, 05004], lr: 0.010028, loss: 1.6765
2022-03-04 17:14:41 - train: epoch 0161, iter [01300, 05004], lr: 0.010028, loss: 1.6521
2022-03-04 17:15:14 - train: epoch 0161, iter [01400, 05004], lr: 0.010028, loss: 1.8102
2022-03-04 17:15:47 - train: epoch 0161, iter [01500, 05004], lr: 0.010028, loss: 1.7795
2022-03-04 17:16:21 - train: epoch 0161, iter [01600, 05004], lr: 0.010028, loss: 1.8464
2022-03-04 17:16:56 - train: epoch 0161, iter [01700, 05004], lr: 0.010028, loss: 1.8011
2022-03-04 17:17:29 - train: epoch 0161, iter [01800, 05004], lr: 0.010028, loss: 1.6419
2022-03-04 17:18:03 - train: epoch 0161, iter [01900, 05004], lr: 0.010028, loss: 1.8228
2022-03-04 17:18:37 - train: epoch 0161, iter [02000, 05004], lr: 0.010028, loss: 2.0410
2022-03-04 17:19:11 - train: epoch 0161, iter [02100, 05004], lr: 0.010028, loss: 1.9485
2022-03-04 17:19:44 - train: epoch 0161, iter [02200, 05004], lr: 0.010028, loss: 1.7218
2022-03-04 17:20:18 - train: epoch 0161, iter [02300, 05004], lr: 0.010028, loss: 1.6165
2022-03-04 17:20:52 - train: epoch 0161, iter [02400, 05004], lr: 0.010028, loss: 1.5387
2022-03-04 17:21:26 - train: epoch 0161, iter [02500, 05004], lr: 0.010028, loss: 2.0186
2022-03-04 17:21:59 - train: epoch 0161, iter [02600, 05004], lr: 0.010028, loss: 1.8482
2022-03-04 17:22:32 - train: epoch 0161, iter [02700, 05004], lr: 0.010028, loss: 1.7894
2022-03-04 17:23:06 - train: epoch 0161, iter [02800, 05004], lr: 0.010028, loss: 1.8103
2022-03-04 17:23:40 - train: epoch 0161, iter [02900, 05004], lr: 0.010028, loss: 1.6687
2022-03-04 17:24:14 - train: epoch 0161, iter [03000, 05004], lr: 0.010028, loss: 1.6646
2022-03-04 17:24:47 - train: epoch 0161, iter [03100, 05004], lr: 0.010028, loss: 1.9573
2022-03-04 17:25:22 - train: epoch 0161, iter [03200, 05004], lr: 0.010028, loss: 1.8306
2022-03-04 17:25:54 - train: epoch 0161, iter [03300, 05004], lr: 0.010028, loss: 1.6611
2022-03-04 17:26:28 - train: epoch 0161, iter [03400, 05004], lr: 0.010028, loss: 1.6718
2022-03-04 17:27:03 - train: epoch 0161, iter [03500, 05004], lr: 0.010028, loss: 1.7874
2022-03-04 17:27:36 - train: epoch 0161, iter [03600, 05004], lr: 0.010028, loss: 2.0720
2022-03-04 17:28:11 - train: epoch 0161, iter [03700, 05004], lr: 0.010028, loss: 1.9379
2022-03-04 17:28:44 - train: epoch 0161, iter [03800, 05004], lr: 0.010028, loss: 1.8142
2022-03-04 17:29:18 - train: epoch 0161, iter [03900, 05004], lr: 0.010028, loss: 1.7712
2022-03-04 17:29:52 - train: epoch 0161, iter [04000, 05004], lr: 0.010028, loss: 1.8000
2022-03-04 17:30:26 - train: epoch 0161, iter [04100, 05004], lr: 0.010028, loss: 1.7081
2022-03-04 17:31:00 - train: epoch 0161, iter [04200, 05004], lr: 0.010028, loss: 1.9812
2022-03-04 17:31:34 - train: epoch 0161, iter [04300, 05004], lr: 0.010028, loss: 2.0029
2022-03-04 17:32:08 - train: epoch 0161, iter [04400, 05004], lr: 0.010028, loss: 1.7960
2022-03-04 17:32:41 - train: epoch 0161, iter [04500, 05004], lr: 0.010028, loss: 1.7581
2022-03-04 17:33:15 - train: epoch 0161, iter [04600, 05004], lr: 0.010028, loss: 1.9161
2022-03-04 17:33:49 - train: epoch 0161, iter [04700, 05004], lr: 0.010028, loss: 1.8458
2022-03-04 17:34:23 - train: epoch 0161, iter [04800, 05004], lr: 0.010028, loss: 2.0255
2022-03-04 17:34:57 - train: epoch 0161, iter [04900, 05004], lr: 0.010028, loss: 2.0307
2022-03-04 17:35:29 - train: epoch 0161, iter [05000, 05004], lr: 0.010028, loss: 2.1399
2022-03-04 17:35:30 - train: epoch 161, train_loss: 1.8393
2022-03-04 17:36:44 - eval: epoch: 161, acc1: 70.806%, acc5: 90.258%, test_loss: 1.1691, per_image_load_time: 1.364ms, per_image_inference_time: 0.503ms
2022-03-04 17:36:45 - until epoch: 161, best_acc1: 70.986%
2022-03-04 17:36:45 - epoch 162 lr: 0.009549150281252633
2022-03-04 17:37:24 - train: epoch 0162, iter [00100, 05004], lr: 0.009549, loss: 1.6609
2022-03-04 17:37:57 - train: epoch 0162, iter [00200, 05004], lr: 0.009549, loss: 1.6365
2022-03-04 17:38:32 - train: epoch 0162, iter [00300, 05004], lr: 0.009549, loss: 1.7206
2022-03-04 17:39:05 - train: epoch 0162, iter [00400, 05004], lr: 0.009549, loss: 1.8353
2022-03-04 17:39:39 - train: epoch 0162, iter [00500, 05004], lr: 0.009549, loss: 1.7545
2022-03-04 17:40:13 - train: epoch 0162, iter [00600, 05004], lr: 0.009549, loss: 1.8279
2022-03-04 17:40:47 - train: epoch 0162, iter [00700, 05004], lr: 0.009549, loss: 1.9329
2022-03-04 17:41:21 - train: epoch 0162, iter [00800, 05004], lr: 0.009549, loss: 1.8586
2022-03-04 17:41:55 - train: epoch 0162, iter [00900, 05004], lr: 0.009549, loss: 2.1705
2022-03-04 17:42:29 - train: epoch 0162, iter [01000, 05004], lr: 0.009549, loss: 1.8672
2022-03-04 17:43:02 - train: epoch 0162, iter [01100, 05004], lr: 0.009549, loss: 1.7285
2022-03-04 17:43:35 - train: epoch 0162, iter [01200, 05004], lr: 0.009549, loss: 1.7052
2022-03-04 17:44:10 - train: epoch 0162, iter [01300, 05004], lr: 0.009549, loss: 1.9071
2022-03-04 17:44:44 - train: epoch 0162, iter [01400, 05004], lr: 0.009549, loss: 1.8048
2022-03-04 17:45:17 - train: epoch 0162, iter [01500, 05004], lr: 0.009549, loss: 1.7570
2022-03-04 17:45:51 - train: epoch 0162, iter [01600, 05004], lr: 0.009549, loss: 2.0711
2022-03-04 17:46:25 - train: epoch 0162, iter [01700, 05004], lr: 0.009549, loss: 1.9104
2022-03-04 17:46:59 - train: epoch 0162, iter [01800, 05004], lr: 0.009549, loss: 1.8564
2022-03-04 17:47:33 - train: epoch 0162, iter [01900, 05004], lr: 0.009549, loss: 1.9915
2022-03-04 17:48:07 - train: epoch 0162, iter [02000, 05004], lr: 0.009549, loss: 1.8810
2022-03-04 17:48:41 - train: epoch 0162, iter [02100, 05004], lr: 0.009549, loss: 2.0303
2022-03-04 17:49:14 - train: epoch 0162, iter [02200, 05004], lr: 0.009549, loss: 1.4843
2022-03-04 17:49:48 - train: epoch 0162, iter [02300, 05004], lr: 0.009549, loss: 1.8214
2022-03-04 17:50:21 - train: epoch 0162, iter [02400, 05004], lr: 0.009549, loss: 1.8146
2022-03-04 17:50:55 - train: epoch 0162, iter [02500, 05004], lr: 0.009549, loss: 1.8930
2022-03-04 17:51:29 - train: epoch 0162, iter [02600, 05004], lr: 0.009549, loss: 1.8818
2022-03-04 17:52:03 - train: epoch 0162, iter [02700, 05004], lr: 0.009549, loss: 1.8520
2022-03-04 17:52:36 - train: epoch 0162, iter [02800, 05004], lr: 0.009549, loss: 1.8744
2022-03-04 17:53:10 - train: epoch 0162, iter [02900, 05004], lr: 0.009549, loss: 1.6954
2022-03-04 17:53:43 - train: epoch 0162, iter [03000, 05004], lr: 0.009549, loss: 1.6740
2022-03-04 17:54:17 - train: epoch 0162, iter [03100, 05004], lr: 0.009549, loss: 1.8567
2022-03-04 17:54:51 - train: epoch 0162, iter [03200, 05004], lr: 0.009549, loss: 1.7361
2022-03-04 17:55:24 - train: epoch 0162, iter [03300, 05004], lr: 0.009549, loss: 1.8853
2022-03-04 17:55:58 - train: epoch 0162, iter [03400, 05004], lr: 0.009549, loss: 1.6530
2022-03-04 17:56:31 - train: epoch 0162, iter [03500, 05004], lr: 0.009549, loss: 1.9018
2022-03-04 17:57:05 - train: epoch 0162, iter [03600, 05004], lr: 0.009549, loss: 2.0130
2022-03-04 17:57:39 - train: epoch 0162, iter [03700, 05004], lr: 0.009549, loss: 1.6218
2022-03-04 17:58:13 - train: epoch 0162, iter [03800, 05004], lr: 0.009549, loss: 1.6971
2022-03-04 17:58:45 - train: epoch 0162, iter [03900, 05004], lr: 0.009549, loss: 1.9798
2022-03-04 17:59:19 - train: epoch 0162, iter [04000, 05004], lr: 0.009549, loss: 1.6760
2022-03-04 17:59:53 - train: epoch 0162, iter [04100, 05004], lr: 0.009549, loss: 2.0204
2022-03-04 18:00:26 - train: epoch 0162, iter [04200, 05004], lr: 0.009549, loss: 1.8440
2022-03-04 18:01:00 - train: epoch 0162, iter [04300, 05004], lr: 0.009549, loss: 1.6441
2022-03-04 18:01:34 - train: epoch 0162, iter [04400, 05004], lr: 0.009549, loss: 1.7793
2022-03-04 18:02:07 - train: epoch 0162, iter [04500, 05004], lr: 0.009549, loss: 2.0060
2022-03-04 18:02:41 - train: epoch 0162, iter [04600, 05004], lr: 0.009549, loss: 1.7444
2022-03-04 18:03:15 - train: epoch 0162, iter [04700, 05004], lr: 0.009549, loss: 1.8734
2022-03-04 18:03:49 - train: epoch 0162, iter [04800, 05004], lr: 0.009549, loss: 1.8031
2022-03-04 18:04:23 - train: epoch 0162, iter [04900, 05004], lr: 0.009549, loss: 1.6511
2022-03-04 18:04:55 - train: epoch 0162, iter [05000, 05004], lr: 0.009549, loss: 2.0935
2022-03-04 18:04:56 - train: epoch 162, train_loss: 1.8228
2022-03-04 18:06:10 - eval: epoch: 162, acc1: 71.412%, acc5: 90.822%, test_loss: 1.1347, per_image_load_time: 0.747ms, per_image_inference_time: 0.478ms
2022-03-04 18:06:11 - until epoch: 162, best_acc1: 71.412%
2022-03-04 18:06:11 - epoch 163 lr: 0.009080937753040646
2022-03-04 18:06:50 - train: epoch 0163, iter [00100, 05004], lr: 0.009081, loss: 1.9904
2022-03-04 18:07:25 - train: epoch 0163, iter [00200, 05004], lr: 0.009081, loss: 1.9824
2022-03-04 18:07:58 - train: epoch 0163, iter [00300, 05004], lr: 0.009081, loss: 1.8308
2022-03-04 18:08:31 - train: epoch 0163, iter [00400, 05004], lr: 0.009081, loss: 1.8879
2022-03-04 18:09:05 - train: epoch 0163, iter [00500, 05004], lr: 0.009081, loss: 1.6352
2022-03-04 18:09:39 - train: epoch 0163, iter [00600, 05004], lr: 0.009081, loss: 1.8939
2022-03-04 18:10:13 - train: epoch 0163, iter [00700, 05004], lr: 0.009081, loss: 1.8695
2022-03-04 18:10:46 - train: epoch 0163, iter [00800, 05004], lr: 0.009081, loss: 1.7639
2022-03-04 18:11:20 - train: epoch 0163, iter [00900, 05004], lr: 0.009081, loss: 1.8406
2022-03-04 18:11:54 - train: epoch 0163, iter [01000, 05004], lr: 0.009081, loss: 1.6523
2022-03-04 18:12:27 - train: epoch 0163, iter [01100, 05004], lr: 0.009081, loss: 1.8260
2022-03-04 18:13:01 - train: epoch 0163, iter [01200, 05004], lr: 0.009081, loss: 1.8273
2022-03-04 18:13:35 - train: epoch 0163, iter [01300, 05004], lr: 0.009081, loss: 1.6766
2022-03-04 18:14:09 - train: epoch 0163, iter [01400, 05004], lr: 0.009081, loss: 1.5091
2022-03-04 18:14:43 - train: epoch 0163, iter [01500, 05004], lr: 0.009081, loss: 1.8769
2022-03-04 18:15:16 - train: epoch 0163, iter [01600, 05004], lr: 0.009081, loss: 2.0412
2022-03-04 18:15:51 - train: epoch 0163, iter [01700, 05004], lr: 0.009081, loss: 1.5969
2022-03-04 18:16:24 - train: epoch 0163, iter [01800, 05004], lr: 0.009081, loss: 1.8194
2022-03-04 18:16:58 - train: epoch 0163, iter [01900, 05004], lr: 0.009081, loss: 1.9275
2022-03-04 18:17:32 - train: epoch 0163, iter [02000, 05004], lr: 0.009081, loss: 1.7101
2022-03-04 18:18:06 - train: epoch 0163, iter [02100, 05004], lr: 0.009081, loss: 1.6552
2022-03-04 18:18:39 - train: epoch 0163, iter [02200, 05004], lr: 0.009081, loss: 1.7899
2022-03-04 18:19:13 - train: epoch 0163, iter [02300, 05004], lr: 0.009081, loss: 1.8032
2022-03-04 18:19:47 - train: epoch 0163, iter [02400, 05004], lr: 0.009081, loss: 1.9454
2022-03-04 18:20:21 - train: epoch 0163, iter [02500, 05004], lr: 0.009081, loss: 1.8868
2022-03-04 18:20:55 - train: epoch 0163, iter [02600, 05004], lr: 0.009081, loss: 1.8549
2022-03-04 18:21:29 - train: epoch 0163, iter [02700, 05004], lr: 0.009081, loss: 1.7646
2022-03-04 18:22:02 - train: epoch 0163, iter [02800, 05004], lr: 0.009081, loss: 1.8188
2022-03-04 18:22:36 - train: epoch 0163, iter [02900, 05004], lr: 0.009081, loss: 1.7863
2022-03-04 18:23:10 - train: epoch 0163, iter [03000, 05004], lr: 0.009081, loss: 1.9007
2022-03-04 18:23:44 - train: epoch 0163, iter [03100, 05004], lr: 0.009081, loss: 1.7446
2022-03-04 18:24:18 - train: epoch 0163, iter [03200, 05004], lr: 0.009081, loss: 1.8555
2022-03-04 18:24:51 - train: epoch 0163, iter [03300, 05004], lr: 0.009081, loss: 1.8106
2022-03-04 18:25:25 - train: epoch 0163, iter [03400, 05004], lr: 0.009081, loss: 2.0021
2022-03-04 18:25:59 - train: epoch 0163, iter [03500, 05004], lr: 0.009081, loss: 1.9370
2022-03-04 18:26:33 - train: epoch 0163, iter [03600, 05004], lr: 0.009081, loss: 1.8356
2022-03-04 18:27:08 - train: epoch 0163, iter [03700, 05004], lr: 0.009081, loss: 1.7839
2022-03-04 18:27:41 - train: epoch 0163, iter [03800, 05004], lr: 0.009081, loss: 1.8670
2022-03-04 18:28:14 - train: epoch 0163, iter [03900, 05004], lr: 0.009081, loss: 1.7102
2022-03-04 18:28:48 - train: epoch 0163, iter [04000, 05004], lr: 0.009081, loss: 1.9425
2022-03-04 18:29:22 - train: epoch 0163, iter [04100, 05004], lr: 0.009081, loss: 1.8789
2022-03-04 18:29:56 - train: epoch 0163, iter [04200, 05004], lr: 0.009081, loss: 1.8110
2022-03-04 18:30:30 - train: epoch 0163, iter [04300, 05004], lr: 0.009081, loss: 1.6440
2022-03-04 18:31:04 - train: epoch 0163, iter [04400, 05004], lr: 0.009081, loss: 1.6448
2022-03-04 18:31:37 - train: epoch 0163, iter [04500, 05004], lr: 0.009081, loss: 1.5551
2022-03-04 18:32:11 - train: epoch 0163, iter [04600, 05004], lr: 0.009081, loss: 1.9425
2022-03-04 18:32:45 - train: epoch 0163, iter [04700, 05004], lr: 0.009081, loss: 2.0297
2022-03-04 18:33:18 - train: epoch 0163, iter [04800, 05004], lr: 0.009081, loss: 1.9445
2022-03-04 18:33:52 - train: epoch 0163, iter [04900, 05004], lr: 0.009081, loss: 1.8744
2022-03-04 18:34:25 - train: epoch 0163, iter [05000, 05004], lr: 0.009081, loss: 1.6173
2022-03-04 18:34:27 - train: epoch 163, train_loss: 1.8097
2022-03-04 18:35:40 - eval: epoch: 163, acc1: 71.820%, acc5: 90.652%, test_loss: 1.1321, per_image_load_time: 0.709ms, per_image_inference_time: 0.478ms
2022-03-04 18:35:41 - until epoch: 163, best_acc1: 71.820%
2022-03-04 18:35:41 - epoch 164 lr: 0.008623345769777514
2022-03-04 18:36:19 - train: epoch 0164, iter [00100, 05004], lr: 0.008623, loss: 1.6418
2022-03-04 18:36:54 - train: epoch 0164, iter [00200, 05004], lr: 0.008623, loss: 1.6668
2022-03-04 18:37:28 - train: epoch 0164, iter [00300, 05004], lr: 0.008623, loss: 1.9908
2022-03-04 18:38:01 - train: epoch 0164, iter [00400, 05004], lr: 0.008623, loss: 1.9249
2022-03-04 18:38:35 - train: epoch 0164, iter [00500, 05004], lr: 0.008623, loss: 1.6817
2022-03-04 18:39:09 - train: epoch 0164, iter [00600, 05004], lr: 0.008623, loss: 1.8930
2022-03-04 18:39:43 - train: epoch 0164, iter [00700, 05004], lr: 0.008623, loss: 1.8916
2022-03-04 18:40:17 - train: epoch 0164, iter [00800, 05004], lr: 0.008623, loss: 1.6907
2022-03-04 18:40:50 - train: epoch 0164, iter [00900, 05004], lr: 0.008623, loss: 1.7330
2022-03-04 18:41:23 - train: epoch 0164, iter [01000, 05004], lr: 0.008623, loss: 1.6853
2022-03-04 18:41:56 - train: epoch 0164, iter [01100, 05004], lr: 0.008623, loss: 1.8159
2022-03-04 18:42:31 - train: epoch 0164, iter [01200, 05004], lr: 0.008623, loss: 1.5622
2022-03-04 18:43:05 - train: epoch 0164, iter [01300, 05004], lr: 0.008623, loss: 2.0062
2022-03-04 18:43:38 - train: epoch 0164, iter [01400, 05004], lr: 0.008623, loss: 1.7929
2022-03-04 18:44:12 - train: epoch 0164, iter [01500, 05004], lr: 0.008623, loss: 2.1810
2022-03-04 18:44:45 - train: epoch 0164, iter [01600, 05004], lr: 0.008623, loss: 1.8597
2022-03-04 18:45:19 - train: epoch 0164, iter [01700, 05004], lr: 0.008623, loss: 1.8086
2022-03-04 18:45:52 - train: epoch 0164, iter [01800, 05004], lr: 0.008623, loss: 1.6402
2022-03-04 18:46:26 - train: epoch 0164, iter [01900, 05004], lr: 0.008623, loss: 1.7927
2022-03-04 18:47:00 - train: epoch 0164, iter [02000, 05004], lr: 0.008623, loss: 1.7028
2022-03-04 18:47:34 - train: epoch 0164, iter [02100, 05004], lr: 0.008623, loss: 1.7354
2022-03-04 18:48:08 - train: epoch 0164, iter [02200, 05004], lr: 0.008623, loss: 1.6455
2022-03-04 18:48:41 - train: epoch 0164, iter [02300, 05004], lr: 0.008623, loss: 1.9829
2022-03-04 18:49:15 - train: epoch 0164, iter [02400, 05004], lr: 0.008623, loss: 1.9799
2022-03-04 18:49:49 - train: epoch 0164, iter [02500, 05004], lr: 0.008623, loss: 1.9529
2022-03-04 18:50:22 - train: epoch 0164, iter [02600, 05004], lr: 0.008623, loss: 1.6128
2022-03-04 18:50:56 - train: epoch 0164, iter [02700, 05004], lr: 0.008623, loss: 2.0697
2022-03-04 18:51:30 - train: epoch 0164, iter [02800, 05004], lr: 0.008623, loss: 1.7109
2022-03-04 18:52:03 - train: epoch 0164, iter [02900, 05004], lr: 0.008623, loss: 2.0032
2022-03-04 18:52:37 - train: epoch 0164, iter [03000, 05004], lr: 0.008623, loss: 1.9754
2022-03-04 18:53:11 - train: epoch 0164, iter [03100, 05004], lr: 0.008623, loss: 1.6964
2022-03-04 18:53:45 - train: epoch 0164, iter [03200, 05004], lr: 0.008623, loss: 1.9089
2022-03-04 18:54:19 - train: epoch 0164, iter [03300, 05004], lr: 0.008623, loss: 1.8479
2022-03-04 18:54:53 - train: epoch 0164, iter [03400, 05004], lr: 0.008623, loss: 1.5154
2022-03-04 18:55:26 - train: epoch 0164, iter [03500, 05004], lr: 0.008623, loss: 1.5197
2022-03-04 18:56:00 - train: epoch 0164, iter [03600, 05004], lr: 0.008623, loss: 1.8449
2022-03-04 18:56:34 - train: epoch 0164, iter [03700, 05004], lr: 0.008623, loss: 1.7867
2022-03-04 18:57:07 - train: epoch 0164, iter [03800, 05004], lr: 0.008623, loss: 1.6644
2022-03-04 18:57:42 - train: epoch 0164, iter [03900, 05004], lr: 0.008623, loss: 1.6691
2022-03-04 18:58:15 - train: epoch 0164, iter [04000, 05004], lr: 0.008623, loss: 1.6782
2022-03-04 18:58:49 - train: epoch 0164, iter [04100, 05004], lr: 0.008623, loss: 1.5789
2022-03-04 18:59:23 - train: epoch 0164, iter [04200, 05004], lr: 0.008623, loss: 1.9742
2022-03-04 18:59:57 - train: epoch 0164, iter [04300, 05004], lr: 0.008623, loss: 1.5669
2022-03-04 19:00:30 - train: epoch 0164, iter [04400, 05004], lr: 0.008623, loss: 1.4260
2022-03-04 19:01:04 - train: epoch 0164, iter [04500, 05004], lr: 0.008623, loss: 1.8854
2022-03-04 19:01:38 - train: epoch 0164, iter [04600, 05004], lr: 0.008623, loss: 1.8120
2022-03-04 19:02:12 - train: epoch 0164, iter [04700, 05004], lr: 0.008623, loss: 1.8906
2022-03-04 19:02:46 - train: epoch 0164, iter [04800, 05004], lr: 0.008623, loss: 1.7771
2022-03-04 19:03:20 - train: epoch 0164, iter [04900, 05004], lr: 0.008623, loss: 1.7570
2022-03-04 19:03:53 - train: epoch 0164, iter [05000, 05004], lr: 0.008623, loss: 1.6485
2022-03-04 19:03:54 - train: epoch 164, train_loss: 1.7954
2022-03-04 19:05:07 - eval: epoch: 164, acc1: 71.544%, acc5: 90.646%, test_loss: 1.1375, per_image_load_time: 1.292ms, per_image_inference_time: 0.457ms
2022-03-04 19:05:08 - until epoch: 164, best_acc1: 71.820%
2022-03-04 19:05:08 - epoch 165 lr: 0.008176493099488664
2022-03-04 19:05:47 - train: epoch 0165, iter [00100, 05004], lr: 0.008176, loss: 1.9262
2022-03-04 19:06:21 - train: epoch 0165, iter [00200, 05004], lr: 0.008176, loss: 1.9646
2022-03-04 19:06:54 - train: epoch 0165, iter [00300, 05004], lr: 0.008176, loss: 1.8214
2022-03-04 19:07:28 - train: epoch 0165, iter [00400, 05004], lr: 0.008176, loss: 1.7506
2022-03-04 19:08:02 - train: epoch 0165, iter [00500, 05004], lr: 0.008176, loss: 1.7523
2022-03-04 19:08:35 - train: epoch 0165, iter [00600, 05004], lr: 0.008176, loss: 1.7006
2022-03-04 19:09:08 - train: epoch 0165, iter [00700, 05004], lr: 0.008176, loss: 1.9293
2022-03-04 19:09:42 - train: epoch 0165, iter [00800, 05004], lr: 0.008176, loss: 1.7814
2022-03-04 19:10:16 - train: epoch 0165, iter [00900, 05004], lr: 0.008176, loss: 1.7169
2022-03-04 19:10:51 - train: epoch 0165, iter [01000, 05004], lr: 0.008176, loss: 1.6208
2022-03-04 19:11:24 - train: epoch 0165, iter [01100, 05004], lr: 0.008176, loss: 1.8538
2022-03-04 19:11:57 - train: epoch 0165, iter [01200, 05004], lr: 0.008176, loss: 1.6487
2022-03-04 19:12:31 - train: epoch 0165, iter [01300, 05004], lr: 0.008176, loss: 1.8893
2022-03-04 19:13:05 - train: epoch 0165, iter [01400, 05004], lr: 0.008176, loss: 1.6481
2022-03-04 19:13:39 - train: epoch 0165, iter [01500, 05004], lr: 0.008176, loss: 1.9084
2022-03-04 19:14:13 - train: epoch 0165, iter [01600, 05004], lr: 0.008176, loss: 1.6080
2022-03-04 19:14:47 - train: epoch 0165, iter [01700, 05004], lr: 0.008176, loss: 1.9754
2022-03-04 19:15:20 - train: epoch 0165, iter [01800, 05004], lr: 0.008176, loss: 1.6178
2022-03-04 19:15:54 - train: epoch 0165, iter [01900, 05004], lr: 0.008176, loss: 1.5848
2022-03-04 19:16:27 - train: epoch 0165, iter [02000, 05004], lr: 0.008176, loss: 1.8133
2022-03-04 19:17:02 - train: epoch 0165, iter [02100, 05004], lr: 0.008176, loss: 1.7671
2022-03-04 19:17:34 - train: epoch 0165, iter [02200, 05004], lr: 0.008176, loss: 1.5668
2022-03-04 19:18:08 - train: epoch 0165, iter [02300, 05004], lr: 0.008176, loss: 1.8744
2022-03-04 19:18:41 - train: epoch 0165, iter [02400, 05004], lr: 0.008176, loss: 1.7563
2022-03-04 19:19:16 - train: epoch 0165, iter [02500, 05004], lr: 0.008176, loss: 1.8324
2022-03-04 19:19:49 - train: epoch 0165, iter [02600, 05004], lr: 0.008176, loss: 1.8827
2022-03-04 19:20:23 - train: epoch 0165, iter [02700, 05004], lr: 0.008176, loss: 1.8315
2022-03-04 19:20:57 - train: epoch 0165, iter [02800, 05004], lr: 0.008176, loss: 1.9861
2022-03-04 19:21:30 - train: epoch 0165, iter [02900, 05004], lr: 0.008176, loss: 1.8274
2022-03-04 19:22:03 - train: epoch 0165, iter [03000, 05004], lr: 0.008176, loss: 1.7458
2022-03-04 19:22:38 - train: epoch 0165, iter [03100, 05004], lr: 0.008176, loss: 1.6841
2022-03-04 19:23:11 - train: epoch 0165, iter [03200, 05004], lr: 0.008176, loss: 1.5642
2022-03-04 19:23:45 - train: epoch 0165, iter [03300, 05004], lr: 0.008176, loss: 1.7778
2022-03-04 19:24:18 - train: epoch 0165, iter [03400, 05004], lr: 0.008176, loss: 1.9903
2022-03-04 19:24:52 - train: epoch 0165, iter [03500, 05004], lr: 0.008176, loss: 2.0068
2022-03-04 19:25:25 - train: epoch 0165, iter [03600, 05004], lr: 0.008176, loss: 2.2235
2022-03-04 19:25:59 - train: epoch 0165, iter [03700, 05004], lr: 0.008176, loss: 1.7485
2022-03-04 19:26:33 - train: epoch 0165, iter [03800, 05004], lr: 0.008176, loss: 1.6877
2022-03-04 19:27:06 - train: epoch 0165, iter [03900, 05004], lr: 0.008176, loss: 1.8662
2022-03-04 19:27:40 - train: epoch 0165, iter [04000, 05004], lr: 0.008176, loss: 1.8275
2022-03-04 19:28:14 - train: epoch 0165, iter [04100, 05004], lr: 0.008176, loss: 1.7465
2022-03-04 19:28:48 - train: epoch 0165, iter [04200, 05004], lr: 0.008176, loss: 2.1104
2022-03-04 19:29:22 - train: epoch 0165, iter [04300, 05004], lr: 0.008176, loss: 1.8349
2022-03-04 19:29:55 - train: epoch 0165, iter [04400, 05004], lr: 0.008176, loss: 1.7508
2022-03-04 19:30:28 - train: epoch 0165, iter [04500, 05004], lr: 0.008176, loss: 1.6470
2022-03-04 19:31:03 - train: epoch 0165, iter [04600, 05004], lr: 0.008176, loss: 1.8884
2022-03-04 19:31:35 - train: epoch 0165, iter [04700, 05004], lr: 0.008176, loss: 1.7515
2022-03-04 19:32:09 - train: epoch 0165, iter [04800, 05004], lr: 0.008176, loss: 1.4937
2022-03-04 19:32:43 - train: epoch 0165, iter [04900, 05004], lr: 0.008176, loss: 1.7033
2022-03-04 19:33:16 - train: epoch 0165, iter [05000, 05004], lr: 0.008176, loss: 1.7070
2022-03-04 19:33:17 - train: epoch 165, train_loss: 1.7805
2022-03-04 19:34:31 - eval: epoch: 165, acc1: 72.370%, acc5: 91.320%, test_loss: 1.0937, per_image_load_time: 2.342ms, per_image_inference_time: 0.521ms
2022-03-04 19:34:32 - until epoch: 165, best_acc1: 72.370%
2022-03-04 19:34:32 - epoch 166 lr: 0.00774049572281027
2022-03-04 19:35:10 - train: epoch 0166, iter [00100, 05004], lr: 0.007740, loss: 1.9183
2022-03-04 19:35:44 - train: epoch 0166, iter [00200, 05004], lr: 0.007740, loss: 1.6858
2022-03-04 19:36:18 - train: epoch 0166, iter [00300, 05004], lr: 0.007740, loss: 1.7178
2022-03-04 19:36:52 - train: epoch 0166, iter [00400, 05004], lr: 0.007740, loss: 1.7628
2022-03-04 19:37:26 - train: epoch 0166, iter [00500, 05004], lr: 0.007740, loss: 1.6688
2022-03-04 19:37:59 - train: epoch 0166, iter [00600, 05004], lr: 0.007740, loss: 1.8707
2022-03-04 19:38:34 - train: epoch 0166, iter [00700, 05004], lr: 0.007740, loss: 1.7942
2022-03-04 19:39:07 - train: epoch 0166, iter [00800, 05004], lr: 0.007740, loss: 1.8862
2022-03-04 19:39:41 - train: epoch 0166, iter [00900, 05004], lr: 0.007740, loss: 1.7670
2022-03-04 19:40:15 - train: epoch 0166, iter [01000, 05004], lr: 0.007740, loss: 1.7186
2022-03-04 19:40:49 - train: epoch 0166, iter [01100, 05004], lr: 0.007740, loss: 1.7452
2022-03-04 19:41:22 - train: epoch 0166, iter [01200, 05004], lr: 0.007740, loss: 1.8618
2022-03-04 19:41:56 - train: epoch 0166, iter [01300, 05004], lr: 0.007740, loss: 1.6752
2022-03-04 19:42:30 - train: epoch 0166, iter [01400, 05004], lr: 0.007740, loss: 1.7026
2022-03-04 19:43:04 - train: epoch 0166, iter [01500, 05004], lr: 0.007740, loss: 1.7691
2022-03-04 19:43:38 - train: epoch 0166, iter [01600, 05004], lr: 0.007740, loss: 1.7666
2022-03-04 19:44:12 - train: epoch 0166, iter [01700, 05004], lr: 0.007740, loss: 1.7589
2022-03-04 19:44:45 - train: epoch 0166, iter [01800, 05004], lr: 0.007740, loss: 1.8572
2022-03-04 19:45:19 - train: epoch 0166, iter [01900, 05004], lr: 0.007740, loss: 1.7245
2022-03-04 19:45:53 - train: epoch 0166, iter [02000, 05004], lr: 0.007740, loss: 1.7580
2022-03-04 19:46:27 - train: epoch 0166, iter [02100, 05004], lr: 0.007740, loss: 1.6609
2022-03-04 19:47:01 - train: epoch 0166, iter [02200, 05004], lr: 0.007740, loss: 1.5736
2022-03-04 19:47:36 - train: epoch 0166, iter [02300, 05004], lr: 0.007740, loss: 1.7672
2022-03-04 19:48:09 - train: epoch 0166, iter [02400, 05004], lr: 0.007740, loss: 1.8860
2022-03-04 19:48:42 - train: epoch 0166, iter [02500, 05004], lr: 0.007740, loss: 1.6612
2022-03-04 19:49:16 - train: epoch 0166, iter [02600, 05004], lr: 0.007740, loss: 1.4632
2022-03-04 19:49:50 - train: epoch 0166, iter [02700, 05004], lr: 0.007740, loss: 1.7296
2022-03-04 19:50:24 - train: epoch 0166, iter [02800, 05004], lr: 0.007740, loss: 1.9410
2022-03-04 19:50:57 - train: epoch 0166, iter [02900, 05004], lr: 0.007740, loss: 2.2152
2022-03-04 19:51:31 - train: epoch 0166, iter [03000, 05004], lr: 0.007740, loss: 1.7450
2022-03-04 19:52:05 - train: epoch 0166, iter [03100, 05004], lr: 0.007740, loss: 1.7567
2022-03-04 19:52:39 - train: epoch 0166, iter [03200, 05004], lr: 0.007740, loss: 1.5893
2022-03-04 19:53:13 - train: epoch 0166, iter [03300, 05004], lr: 0.007740, loss: 1.9210
2022-03-04 19:53:47 - train: epoch 0166, iter [03400, 05004], lr: 0.007740, loss: 1.6121
2022-03-04 19:54:21 - train: epoch 0166, iter [03500, 05004], lr: 0.007740, loss: 1.7317
2022-03-04 19:54:53 - train: epoch 0166, iter [03600, 05004], lr: 0.007740, loss: 1.5972
2022-03-04 19:55:27 - train: epoch 0166, iter [03700, 05004], lr: 0.007740, loss: 1.5430
2022-03-04 19:56:01 - train: epoch 0166, iter [03800, 05004], lr: 0.007740, loss: 1.4244
2022-03-04 19:56:35 - train: epoch 0166, iter [03900, 05004], lr: 0.007740, loss: 1.6817
2022-03-04 19:57:09 - train: epoch 0166, iter [04000, 05004], lr: 0.007740, loss: 1.7243
2022-03-04 19:57:43 - train: epoch 0166, iter [04100, 05004], lr: 0.007740, loss: 1.7526
2022-03-04 19:58:16 - train: epoch 0166, iter [04200, 05004], lr: 0.007740, loss: 1.6839
2022-03-04 19:58:49 - train: epoch 0166, iter [04300, 05004], lr: 0.007740, loss: 1.7997
2022-03-04 19:59:23 - train: epoch 0166, iter [04400, 05004], lr: 0.007740, loss: 1.6587
2022-03-04 19:59:57 - train: epoch 0166, iter [04500, 05004], lr: 0.007740, loss: 1.7644
2022-03-04 20:00:31 - train: epoch 0166, iter [04600, 05004], lr: 0.007740, loss: 1.7567
2022-03-04 20:01:05 - train: epoch 0166, iter [04700, 05004], lr: 0.007740, loss: 1.7068
2022-03-04 20:01:38 - train: epoch 0166, iter [04800, 05004], lr: 0.007740, loss: 1.9270
2022-03-04 20:02:12 - train: epoch 0166, iter [04900, 05004], lr: 0.007740, loss: 1.8314
2022-03-04 20:02:45 - train: epoch 0166, iter [05000, 05004], lr: 0.007740, loss: 1.8906
2022-03-04 20:02:46 - train: epoch 166, train_loss: 1.7671
2022-03-04 20:04:00 - eval: epoch: 166, acc1: 72.210%, acc5: 91.246%, test_loss: 1.0974, per_image_load_time: 1.545ms, per_image_inference_time: 0.497ms
2022-03-04 20:04:01 - until epoch: 166, best_acc1: 72.370%
2022-03-04 20:04:01 - epoch 167 lr: 0.0073154668028864
2022-03-04 20:04:39 - train: epoch 0167, iter [00100, 05004], lr: 0.007315, loss: 1.6519
2022-03-04 20:05:14 - train: epoch 0167, iter [00200, 05004], lr: 0.007315, loss: 1.8388
2022-03-04 20:05:47 - train: epoch 0167, iter [00300, 05004], lr: 0.007315, loss: 1.7630
2022-03-04 20:06:21 - train: epoch 0167, iter [00400, 05004], lr: 0.007315, loss: 1.7326
2022-03-04 20:06:56 - train: epoch 0167, iter [00500, 05004], lr: 0.007315, loss: 1.5597
2022-03-04 20:07:30 - train: epoch 0167, iter [00600, 05004], lr: 0.007315, loss: 1.8085
2022-03-04 20:08:03 - train: epoch 0167, iter [00700, 05004], lr: 0.007315, loss: 1.5815
2022-03-04 20:08:37 - train: epoch 0167, iter [00800, 05004], lr: 0.007315, loss: 1.8361
2022-03-04 20:09:11 - train: epoch 0167, iter [00900, 05004], lr: 0.007315, loss: 1.8858
2022-03-04 20:09:45 - train: epoch 0167, iter [01000, 05004], lr: 0.007315, loss: 1.9568
2022-03-04 20:10:19 - train: epoch 0167, iter [01100, 05004], lr: 0.007315, loss: 1.7925
2022-03-04 20:10:52 - train: epoch 0167, iter [01200, 05004], lr: 0.007315, loss: 1.5740
2022-03-04 20:11:26 - train: epoch 0167, iter [01300, 05004], lr: 0.007315, loss: 1.7023
2022-03-04 20:12:00 - train: epoch 0167, iter [01400, 05004], lr: 0.007315, loss: 1.6335
2022-03-04 20:12:34 - train: epoch 0167, iter [01500, 05004], lr: 0.007315, loss: 1.6373
2022-03-04 20:13:07 - train: epoch 0167, iter [01600, 05004], lr: 0.007315, loss: 1.8855
2022-03-04 20:13:41 - train: epoch 0167, iter [01700, 05004], lr: 0.007315, loss: 1.8789
2022-03-04 20:14:15 - train: epoch 0167, iter [01800, 05004], lr: 0.007315, loss: 1.8790
2022-03-04 20:14:48 - train: epoch 0167, iter [01900, 05004], lr: 0.007315, loss: 1.9289
2022-03-04 20:15:22 - train: epoch 0167, iter [02000, 05004], lr: 0.007315, loss: 1.8120
2022-03-04 20:15:56 - train: epoch 0167, iter [02100, 05004], lr: 0.007315, loss: 1.6469
2022-03-04 20:16:30 - train: epoch 0167, iter [02200, 05004], lr: 0.007315, loss: 1.5745
2022-03-04 20:17:04 - train: epoch 0167, iter [02300, 05004], lr: 0.007315, loss: 1.8330
2022-03-04 20:17:37 - train: epoch 0167, iter [02400, 05004], lr: 0.007315, loss: 1.7545
2022-03-04 20:18:11 - train: epoch 0167, iter [02500, 05004], lr: 0.007315, loss: 1.5677
2022-03-04 20:18:45 - train: epoch 0167, iter [02600, 05004], lr: 0.007315, loss: 1.5843
2022-03-04 20:19:18 - train: epoch 0167, iter [02700, 05004], lr: 0.007315, loss: 1.6309
2022-03-04 20:19:53 - train: epoch 0167, iter [02800, 05004], lr: 0.007315, loss: 1.8963
2022-03-04 20:20:27 - train: epoch 0167, iter [02900, 05004], lr: 0.007315, loss: 1.7585
2022-03-04 20:21:01 - train: epoch 0167, iter [03000, 05004], lr: 0.007315, loss: 1.8132
2022-03-04 20:21:34 - train: epoch 0167, iter [03100, 05004], lr: 0.007315, loss: 1.5567
2022-03-04 20:22:08 - train: epoch 0167, iter [03200, 05004], lr: 0.007315, loss: 1.5468
2022-03-04 20:22:42 - train: epoch 0167, iter [03300, 05004], lr: 0.007315, loss: 1.6860
2022-03-04 20:23:16 - train: epoch 0167, iter [03400, 05004], lr: 0.007315, loss: 1.7118
2022-03-04 20:23:50 - train: epoch 0167, iter [03500, 05004], lr: 0.007315, loss: 1.8352
2022-03-04 20:24:24 - train: epoch 0167, iter [03600, 05004], lr: 0.007315, loss: 1.7532
2022-03-04 20:24:57 - train: epoch 0167, iter [03700, 05004], lr: 0.007315, loss: 1.6735
2022-03-04 20:25:31 - train: epoch 0167, iter [03800, 05004], lr: 0.007315, loss: 1.8248
2022-03-04 20:26:04 - train: epoch 0167, iter [03900, 05004], lr: 0.007315, loss: 1.7234
2022-03-04 20:26:38 - train: epoch 0167, iter [04000, 05004], lr: 0.007315, loss: 1.5769
2022-03-04 20:27:12 - train: epoch 0167, iter [04100, 05004], lr: 0.007315, loss: 1.9239
2022-03-04 20:27:46 - train: epoch 0167, iter [04200, 05004], lr: 0.007315, loss: 1.7194
2022-03-04 20:28:19 - train: epoch 0167, iter [04300, 05004], lr: 0.007315, loss: 1.8272
2022-03-04 20:28:53 - train: epoch 0167, iter [04400, 05004], lr: 0.007315, loss: 1.6171
2022-03-04 20:29:28 - train: epoch 0167, iter [04500, 05004], lr: 0.007315, loss: 1.7793
2022-03-04 20:30:01 - train: epoch 0167, iter [04600, 05004], lr: 0.007315, loss: 1.6565
2022-03-04 20:30:35 - train: epoch 0167, iter [04700, 05004], lr: 0.007315, loss: 2.0820
2022-03-04 20:31:08 - train: epoch 0167, iter [04800, 05004], lr: 0.007315, loss: 1.5142
2022-03-04 20:31:42 - train: epoch 0167, iter [04900, 05004], lr: 0.007315, loss: 1.5845
2022-03-04 20:32:15 - train: epoch 0167, iter [05000, 05004], lr: 0.007315, loss: 1.7611
2022-03-04 20:32:16 - train: epoch 167, train_loss: 1.7515
2022-03-04 20:33:30 - eval: epoch: 167, acc1: 72.728%, acc5: 91.400%, test_loss: 1.0833, per_image_load_time: 1.790ms, per_image_inference_time: 0.509ms
2022-03-04 20:33:31 - until epoch: 167, best_acc1: 72.728%
2022-03-04 20:33:31 - epoch 168 lr: 0.006901516655997537
2022-03-04 20:34:10 - train: epoch 0168, iter [00100, 05004], lr: 0.006902, loss: 1.7531
2022-03-04 20:34:44 - train: epoch 0168, iter [00200, 05004], lr: 0.006902, loss: 1.7525
2022-03-04 20:35:17 - train: epoch 0168, iter [00300, 05004], lr: 0.006902, loss: 1.6717
2022-03-04 20:35:51 - train: epoch 0168, iter [00400, 05004], lr: 0.006902, loss: 1.6147
2022-03-04 20:36:24 - train: epoch 0168, iter [00500, 05004], lr: 0.006902, loss: 1.8372
2022-03-04 20:36:58 - train: epoch 0168, iter [00600, 05004], lr: 0.006902, loss: 1.8086
2022-03-04 20:37:31 - train: epoch 0168, iter [00700, 05004], lr: 0.006902, loss: 1.6910
2022-03-04 20:38:05 - train: epoch 0168, iter [00800, 05004], lr: 0.006902, loss: 1.7270
2022-03-04 20:38:39 - train: epoch 0168, iter [00900, 05004], lr: 0.006902, loss: 1.7877
2022-03-04 20:39:13 - train: epoch 0168, iter [01000, 05004], lr: 0.006902, loss: 1.4400
2022-03-04 20:39:47 - train: epoch 0168, iter [01100, 05004], lr: 0.006902, loss: 1.7026
2022-03-04 20:40:20 - train: epoch 0168, iter [01200, 05004], lr: 0.006902, loss: 1.6318
2022-03-04 20:40:54 - train: epoch 0168, iter [01300, 05004], lr: 0.006902, loss: 1.8590
2022-03-04 20:41:27 - train: epoch 0168, iter [01400, 05004], lr: 0.006902, loss: 1.7403
2022-03-04 20:42:00 - train: epoch 0168, iter [01500, 05004], lr: 0.006902, loss: 2.0108
2022-03-04 20:42:35 - train: epoch 0168, iter [01600, 05004], lr: 0.006902, loss: 1.8434
2022-03-04 20:43:09 - train: epoch 0168, iter [01700, 05004], lr: 0.006902, loss: 1.9296
2022-03-04 20:43:43 - train: epoch 0168, iter [01800, 05004], lr: 0.006902, loss: 1.6965
2022-03-04 20:44:18 - train: epoch 0168, iter [01900, 05004], lr: 0.006902, loss: 1.8832
2022-03-04 20:44:52 - train: epoch 0168, iter [02000, 05004], lr: 0.006902, loss: 1.5335
2022-03-04 20:45:26 - train: epoch 0168, iter [02100, 05004], lr: 0.006902, loss: 1.5389
2022-03-04 20:46:00 - train: epoch 0168, iter [02200, 05004], lr: 0.006902, loss: 1.8124
2022-03-04 20:46:34 - train: epoch 0168, iter [02300, 05004], lr: 0.006902, loss: 1.5651
2022-03-04 20:47:10 - train: epoch 0168, iter [02400, 05004], lr: 0.006902, loss: 2.0749
2022-03-04 20:47:43 - train: epoch 0168, iter [02500, 05004], lr: 0.006902, loss: 1.7116
2022-03-04 20:48:18 - train: epoch 0168, iter [02600, 05004], lr: 0.006902, loss: 1.6771
2022-03-04 20:48:52 - train: epoch 0168, iter [02700, 05004], lr: 0.006902, loss: 1.5764
2022-03-04 20:49:26 - train: epoch 0168, iter [02800, 05004], lr: 0.006902, loss: 1.4337
2022-03-04 20:50:01 - train: epoch 0168, iter [02900, 05004], lr: 0.006902, loss: 1.7514
2022-03-04 20:50:35 - train: epoch 0168, iter [03000, 05004], lr: 0.006902, loss: 1.5342
2022-03-04 20:51:09 - train: epoch 0168, iter [03100, 05004], lr: 0.006902, loss: 1.6508
2022-03-04 20:51:44 - train: epoch 0168, iter [03200, 05004], lr: 0.006902, loss: 1.6728
2022-03-04 20:52:18 - train: epoch 0168, iter [03300, 05004], lr: 0.006902, loss: 1.6919
2022-03-04 20:52:53 - train: epoch 0168, iter [03400, 05004], lr: 0.006902, loss: 1.8862
2022-03-04 20:53:27 - train: epoch 0168, iter [03500, 05004], lr: 0.006902, loss: 1.7836
2022-03-04 20:54:01 - train: epoch 0168, iter [03600, 05004], lr: 0.006902, loss: 1.7357
2022-03-04 20:54:35 - train: epoch 0168, iter [03700, 05004], lr: 0.006902, loss: 1.6752
2022-03-04 20:55:10 - train: epoch 0168, iter [03800, 05004], lr: 0.006902, loss: 1.6606
2022-03-04 20:55:45 - train: epoch 0168, iter [03900, 05004], lr: 0.006902, loss: 1.9129
2022-03-04 20:56:19 - train: epoch 0168, iter [04000, 05004], lr: 0.006902, loss: 1.7549
2022-03-04 20:56:54 - train: epoch 0168, iter [04100, 05004], lr: 0.006902, loss: 2.0062
2022-03-04 20:57:28 - train: epoch 0168, iter [04200, 05004], lr: 0.006902, loss: 1.8665
2022-03-04 20:58:03 - train: epoch 0168, iter [04300, 05004], lr: 0.006902, loss: 1.7115
2022-03-04 20:58:36 - train: epoch 0168, iter [04400, 05004], lr: 0.006902, loss: 1.9217
2022-03-04 20:59:11 - train: epoch 0168, iter [04500, 05004], lr: 0.006902, loss: 1.8016
2022-03-04 20:59:46 - train: epoch 0168, iter [04600, 05004], lr: 0.006902, loss: 1.5373
2022-03-04 21:00:20 - train: epoch 0168, iter [04700, 05004], lr: 0.006902, loss: 1.7811
2022-03-04 21:00:55 - train: epoch 0168, iter [04800, 05004], lr: 0.006902, loss: 1.7006
2022-03-04 21:01:29 - train: epoch 0168, iter [04900, 05004], lr: 0.006902, loss: 1.8245
2022-03-04 21:02:02 - train: epoch 0168, iter [05000, 05004], lr: 0.006902, loss: 1.8973
2022-03-04 21:02:03 - train: epoch 168, train_loss: 1.7389
2022-03-04 21:03:19 - eval: epoch: 168, acc1: 72.910%, acc5: 91.376%, test_loss: 1.0770, per_image_load_time: 0.637ms, per_image_inference_time: 0.485ms
2022-03-04 21:03:19 - until epoch: 168, best_acc1: 72.910%
2022-03-04 21:03:19 - epoch 169 lr: 0.006498752722928042
2022-03-04 21:03:59 - train: epoch 0169, iter [00100, 05004], lr: 0.006499, loss: 1.7949
2022-03-04 21:04:34 - train: epoch 0169, iter [00200, 05004], lr: 0.006499, loss: 1.7445
2022-03-04 21:05:08 - train: epoch 0169, iter [00300, 05004], lr: 0.006499, loss: 1.7882
2022-03-04 21:05:42 - train: epoch 0169, iter [00400, 05004], lr: 0.006499, loss: 1.5344
2022-03-04 21:06:16 - train: epoch 0169, iter [00500, 05004], lr: 0.006499, loss: 1.6108
2022-03-04 21:06:50 - train: epoch 0169, iter [00600, 05004], lr: 0.006499, loss: 1.6641
2022-03-04 21:07:24 - train: epoch 0169, iter [00700, 05004], lr: 0.006499, loss: 1.8997
2022-03-04 21:07:59 - train: epoch 0169, iter [00800, 05004], lr: 0.006499, loss: 1.8060
2022-03-04 21:08:34 - train: epoch 0169, iter [00900, 05004], lr: 0.006499, loss: 1.8306
2022-03-04 21:09:08 - train: epoch 0169, iter [01000, 05004], lr: 0.006499, loss: 1.6910
2022-03-04 21:09:42 - train: epoch 0169, iter [01100, 05004], lr: 0.006499, loss: 1.7565
2022-03-04 21:10:16 - train: epoch 0169, iter [01200, 05004], lr: 0.006499, loss: 1.8458
2022-03-04 21:10:50 - train: epoch 0169, iter [01300, 05004], lr: 0.006499, loss: 1.6594
2022-03-04 21:11:25 - train: epoch 0169, iter [01400, 05004], lr: 0.006499, loss: 1.8398
2022-03-04 21:11:58 - train: epoch 0169, iter [01500, 05004], lr: 0.006499, loss: 1.5299
2022-03-04 21:12:32 - train: epoch 0169, iter [01600, 05004], lr: 0.006499, loss: 1.7238
2022-03-04 21:13:07 - train: epoch 0169, iter [01700, 05004], lr: 0.006499, loss: 1.8868
2022-03-04 21:13:39 - train: epoch 0169, iter [01800, 05004], lr: 0.006499, loss: 1.9161
2022-03-04 21:14:15 - train: epoch 0169, iter [01900, 05004], lr: 0.006499, loss: 1.4802
2022-03-04 21:14:49 - train: epoch 0169, iter [02000, 05004], lr: 0.006499, loss: 2.0786
2022-03-04 21:15:23 - train: epoch 0169, iter [02100, 05004], lr: 0.006499, loss: 2.0524
2022-03-04 21:15:58 - train: epoch 0169, iter [02200, 05004], lr: 0.006499, loss: 1.6101
2022-03-04 21:16:32 - train: epoch 0169, iter [02300, 05004], lr: 0.006499, loss: 1.8682
2022-03-04 21:17:06 - train: epoch 0169, iter [02400, 05004], lr: 0.006499, loss: 1.6152
2022-03-04 21:17:42 - train: epoch 0169, iter [02500, 05004], lr: 0.006499, loss: 1.5856
2022-03-04 21:18:16 - train: epoch 0169, iter [02600, 05004], lr: 0.006499, loss: 1.5795
2022-03-04 21:18:50 - train: epoch 0169, iter [02700, 05004], lr: 0.006499, loss: 1.4048
2022-03-04 21:19:24 - train: epoch 0169, iter [02800, 05004], lr: 0.006499, loss: 1.9230
2022-03-04 21:19:58 - train: epoch 0169, iter [02900, 05004], lr: 0.006499, loss: 1.4930
2022-03-04 21:20:32 - train: epoch 0169, iter [03000, 05004], lr: 0.006499, loss: 1.5097
2022-03-04 21:21:07 - train: epoch 0169, iter [03100, 05004], lr: 0.006499, loss: 1.6422
2022-03-04 21:21:42 - train: epoch 0169, iter [03200, 05004], lr: 0.006499, loss: 2.0573
2022-03-04 21:22:15 - train: epoch 0169, iter [03300, 05004], lr: 0.006499, loss: 1.7382
2022-03-04 21:22:50 - train: epoch 0169, iter [03400, 05004], lr: 0.006499, loss: 1.5116
2022-03-04 21:23:24 - train: epoch 0169, iter [03500, 05004], lr: 0.006499, loss: 1.6169
2022-03-04 21:23:58 - train: epoch 0169, iter [03600, 05004], lr: 0.006499, loss: 1.9091
2022-03-04 21:24:32 - train: epoch 0169, iter [03700, 05004], lr: 0.006499, loss: 1.9478
2022-03-04 21:25:06 - train: epoch 0169, iter [03800, 05004], lr: 0.006499, loss: 1.6986
2022-03-04 21:25:41 - train: epoch 0169, iter [03900, 05004], lr: 0.006499, loss: 1.5372
2022-03-04 21:26:14 - train: epoch 0169, iter [04000, 05004], lr: 0.006499, loss: 1.5381
2022-03-04 21:26:49 - train: epoch 0169, iter [04100, 05004], lr: 0.006499, loss: 1.6715
2022-03-04 21:27:22 - train: epoch 0169, iter [04200, 05004], lr: 0.006499, loss: 1.6634
2022-03-04 21:27:56 - train: epoch 0169, iter [04300, 05004], lr: 0.006499, loss: 1.5522
2022-03-04 21:28:30 - train: epoch 0169, iter [04400, 05004], lr: 0.006499, loss: 1.8971
2022-03-04 21:29:04 - train: epoch 0169, iter [04500, 05004], lr: 0.006499, loss: 1.7783
2022-03-04 21:29:39 - train: epoch 0169, iter [04600, 05004], lr: 0.006499, loss: 1.9292
2022-03-04 21:30:16 - train: epoch 0169, iter [04700, 05004], lr: 0.006499, loss: 1.5591
2022-03-04 21:30:51 - train: epoch 0169, iter [04800, 05004], lr: 0.006499, loss: 1.5670
2022-03-04 21:31:26 - train: epoch 0169, iter [04900, 05004], lr: 0.006499, loss: 1.7364
2022-03-04 21:31:59 - train: epoch 0169, iter [05000, 05004], lr: 0.006499, loss: 1.7813
2022-03-04 21:32:00 - train: epoch 169, train_loss: 1.7238
2022-03-04 21:33:15 - eval: epoch: 169, acc1: 72.718%, acc5: 91.320%, test_loss: 1.0807, per_image_load_time: 1.166ms, per_image_inference_time: 0.497ms
2022-03-04 21:33:15 - until epoch: 169, best_acc1: 72.910%
2022-03-04 21:33:15 - epoch 170 lr: 0.006107279541079769
2022-03-04 21:33:55 - train: epoch 0170, iter [00100, 05004], lr: 0.006107, loss: 1.9666
2022-03-04 21:34:28 - train: epoch 0170, iter [00200, 05004], lr: 0.006107, loss: 1.8002
2022-03-04 21:35:03 - train: epoch 0170, iter [00300, 05004], lr: 0.006107, loss: 1.6732
2022-03-04 21:35:37 - train: epoch 0170, iter [00400, 05004], lr: 0.006107, loss: 1.7517
2022-03-04 21:36:11 - train: epoch 0170, iter [00500, 05004], lr: 0.006107, loss: 1.9915
2022-03-04 21:36:44 - train: epoch 0170, iter [00600, 05004], lr: 0.006107, loss: 1.7979
2022-03-04 21:37:18 - train: epoch 0170, iter [00700, 05004], lr: 0.006107, loss: 1.6473
2022-03-04 21:37:52 - train: epoch 0170, iter [00800, 05004], lr: 0.006107, loss: 1.6877
2022-03-04 21:38:26 - train: epoch 0170, iter [00900, 05004], lr: 0.006107, loss: 1.9453
2022-03-04 21:39:00 - train: epoch 0170, iter [01000, 05004], lr: 0.006107, loss: 1.7582
2022-03-04 21:39:34 - train: epoch 0170, iter [01100, 05004], lr: 0.006107, loss: 1.6156
2022-03-04 21:40:07 - train: epoch 0170, iter [01200, 05004], lr: 0.006107, loss: 1.7141
2022-03-04 21:40:41 - train: epoch 0170, iter [01300, 05004], lr: 0.006107, loss: 1.7300
2022-03-04 21:41:16 - train: epoch 0170, iter [01400, 05004], lr: 0.006107, loss: 1.7694
2022-03-04 21:41:49 - train: epoch 0170, iter [01500, 05004], lr: 0.006107, loss: 1.6988
2022-03-04 21:42:23 - train: epoch 0170, iter [01600, 05004], lr: 0.006107, loss: 2.1175
2022-03-04 21:42:58 - train: epoch 0170, iter [01700, 05004], lr: 0.006107, loss: 1.7846
2022-03-04 21:43:31 - train: epoch 0170, iter [01800, 05004], lr: 0.006107, loss: 1.6437
2022-03-04 21:44:05 - train: epoch 0170, iter [01900, 05004], lr: 0.006107, loss: 1.6891
2022-03-04 21:44:39 - train: epoch 0170, iter [02000, 05004], lr: 0.006107, loss: 1.7952
2022-03-04 21:45:13 - train: epoch 0170, iter [02100, 05004], lr: 0.006107, loss: 1.8612
2022-03-04 21:45:47 - train: epoch 0170, iter [02200, 05004], lr: 0.006107, loss: 1.8658
2022-03-04 21:46:20 - train: epoch 0170, iter [02300, 05004], lr: 0.006107, loss: 1.6073
2022-03-04 21:46:53 - train: epoch 0170, iter [02400, 05004], lr: 0.006107, loss: 1.9168
2022-03-04 21:47:28 - train: epoch 0170, iter [02500, 05004], lr: 0.006107, loss: 1.5892
2022-03-04 21:48:01 - train: epoch 0170, iter [02600, 05004], lr: 0.006107, loss: 2.0077
2022-03-04 21:48:34 - train: epoch 0170, iter [02700, 05004], lr: 0.006107, loss: 1.5027
2022-03-04 21:49:08 - train: epoch 0170, iter [02800, 05004], lr: 0.006107, loss: 1.9255
2022-03-04 21:49:42 - train: epoch 0170, iter [02900, 05004], lr: 0.006107, loss: 1.9426
2022-03-04 21:50:15 - train: epoch 0170, iter [03000, 05004], lr: 0.006107, loss: 1.5793
2022-03-04 21:50:49 - train: epoch 0170, iter [03100, 05004], lr: 0.006107, loss: 1.7077
2022-03-04 21:51:23 - train: epoch 0170, iter [03200, 05004], lr: 0.006107, loss: 2.0065
2022-03-04 21:51:57 - train: epoch 0170, iter [03300, 05004], lr: 0.006107, loss: 1.6777
2022-03-04 21:52:31 - train: epoch 0170, iter [03400, 05004], lr: 0.006107, loss: 2.0921
2022-03-04 21:53:04 - train: epoch 0170, iter [03500, 05004], lr: 0.006107, loss: 1.8642
2022-03-04 21:53:38 - train: epoch 0170, iter [03600, 05004], lr: 0.006107, loss: 1.6154
2022-03-04 21:54:12 - train: epoch 0170, iter [03700, 05004], lr: 0.006107, loss: 1.6728
2022-03-04 21:54:46 - train: epoch 0170, iter [03800, 05004], lr: 0.006107, loss: 1.9690
2022-03-04 21:55:21 - train: epoch 0170, iter [03900, 05004], lr: 0.006107, loss: 1.8000
2022-03-04 21:55:54 - train: epoch 0170, iter [04000, 05004], lr: 0.006107, loss: 1.5524
2022-03-04 21:56:29 - train: epoch 0170, iter [04100, 05004], lr: 0.006107, loss: 1.5523
2022-03-04 21:57:02 - train: epoch 0170, iter [04200, 05004], lr: 0.006107, loss: 1.8913
2022-03-04 21:57:36 - train: epoch 0170, iter [04300, 05004], lr: 0.006107, loss: 1.3638
2022-03-04 21:58:09 - train: epoch 0170, iter [04400, 05004], lr: 0.006107, loss: 1.6945
2022-03-04 21:58:44 - train: epoch 0170, iter [04500, 05004], lr: 0.006107, loss: 1.6793
2022-03-04 21:59:18 - train: epoch 0170, iter [04600, 05004], lr: 0.006107, loss: 1.7125
2022-03-04 21:59:51 - train: epoch 0170, iter [04700, 05004], lr: 0.006107, loss: 1.7640
2022-03-04 22:00:26 - train: epoch 0170, iter [04800, 05004], lr: 0.006107, loss: 1.8614
2022-03-04 22:01:00 - train: epoch 0170, iter [04900, 05004], lr: 0.006107, loss: 1.7181
2022-03-04 22:01:33 - train: epoch 0170, iter [05000, 05004], lr: 0.006107, loss: 1.7664
2022-03-04 22:01:34 - train: epoch 170, train_loss: 1.7076
2022-03-04 22:02:49 - eval: epoch: 170, acc1: 73.738%, acc5: 91.684%, test_loss: 1.0481, per_image_load_time: 2.037ms, per_image_inference_time: 0.528ms
2022-03-04 22:02:50 - until epoch: 170, best_acc1: 73.738%
2022-03-04 22:02:50 - epoch 171 lr: 0.00572719871733951
2022-03-04 22:03:30 - train: epoch 0171, iter [00100, 05004], lr: 0.005727, loss: 1.6080
2022-03-04 22:04:03 - train: epoch 0171, iter [00200, 05004], lr: 0.005727, loss: 1.7025
2022-03-04 22:04:38 - train: epoch 0171, iter [00300, 05004], lr: 0.005727, loss: 1.7438
2022-03-04 22:05:11 - train: epoch 0171, iter [00400, 05004], lr: 0.005727, loss: 2.0301
2022-03-04 22:05:45 - train: epoch 0171, iter [00500, 05004], lr: 0.005727, loss: 1.6564
2022-03-04 22:06:19 - train: epoch 0171, iter [00600, 05004], lr: 0.005727, loss: 1.8204
2022-03-04 22:06:52 - train: epoch 0171, iter [00700, 05004], lr: 0.005727, loss: 1.7014
2022-03-04 22:07:26 - train: epoch 0171, iter [00800, 05004], lr: 0.005727, loss: 1.6019
2022-03-04 22:07:59 - train: epoch 0171, iter [00900, 05004], lr: 0.005727, loss: 1.4582
2022-03-04 22:08:33 - train: epoch 0171, iter [01000, 05004], lr: 0.005727, loss: 1.8284
2022-03-04 22:09:06 - train: epoch 0171, iter [01100, 05004], lr: 0.005727, loss: 1.5096
2022-03-04 22:09:40 - train: epoch 0171, iter [01200, 05004], lr: 0.005727, loss: 1.8045
2022-03-04 22:10:13 - train: epoch 0171, iter [01300, 05004], lr: 0.005727, loss: 1.6507
2022-03-04 22:10:47 - train: epoch 0171, iter [01400, 05004], lr: 0.005727, loss: 1.6623
2022-03-04 22:11:20 - train: epoch 0171, iter [01500, 05004], lr: 0.005727, loss: 1.6679
2022-03-04 22:11:55 - train: epoch 0171, iter [01600, 05004], lr: 0.005727, loss: 1.5647
2022-03-04 22:12:28 - train: epoch 0171, iter [01700, 05004], lr: 0.005727, loss: 1.7297
2022-03-04 22:13:02 - train: epoch 0171, iter [01800, 05004], lr: 0.005727, loss: 1.7957
2022-03-04 22:13:36 - train: epoch 0171, iter [01900, 05004], lr: 0.005727, loss: 1.7530
2022-03-04 22:14:10 - train: epoch 0171, iter [02000, 05004], lr: 0.005727, loss: 1.4270
2022-03-04 22:14:44 - train: epoch 0171, iter [02100, 05004], lr: 0.005727, loss: 1.5691
2022-03-04 22:15:18 - train: epoch 0171, iter [02200, 05004], lr: 0.005727, loss: 1.7722
2022-03-04 22:15:52 - train: epoch 0171, iter [02300, 05004], lr: 0.005727, loss: 1.7829
2022-03-04 22:16:25 - train: epoch 0171, iter [02400, 05004], lr: 0.005727, loss: 1.5869
2022-03-04 22:17:00 - train: epoch 0171, iter [02500, 05004], lr: 0.005727, loss: 2.0993
2022-03-04 22:17:33 - train: epoch 0171, iter [02600, 05004], lr: 0.005727, loss: 1.8904
2022-03-04 22:18:07 - train: epoch 0171, iter [02700, 05004], lr: 0.005727, loss: 1.8544
2022-03-04 22:18:40 - train: epoch 0171, iter [02800, 05004], lr: 0.005727, loss: 1.6006
2022-03-04 22:19:15 - train: epoch 0171, iter [02900, 05004], lr: 0.005727, loss: 1.6443
2022-03-04 22:19:49 - train: epoch 0171, iter [03000, 05004], lr: 0.005727, loss: 1.8228
2022-03-04 22:20:22 - train: epoch 0171, iter [03100, 05004], lr: 0.005727, loss: 1.6596
2022-03-04 22:20:56 - train: epoch 0171, iter [03200, 05004], lr: 0.005727, loss: 1.7678
2022-03-04 22:21:30 - train: epoch 0171, iter [03300, 05004], lr: 0.005727, loss: 1.8232
2022-03-04 22:22:04 - train: epoch 0171, iter [03400, 05004], lr: 0.005727, loss: 1.6141
2022-03-04 22:22:37 - train: epoch 0171, iter [03500, 05004], lr: 0.005727, loss: 1.9107
2022-03-04 22:23:12 - train: epoch 0171, iter [03600, 05004], lr: 0.005727, loss: 1.7931
2022-03-04 22:23:45 - train: epoch 0171, iter [03700, 05004], lr: 0.005727, loss: 1.7237
2022-03-04 22:24:20 - train: epoch 0171, iter [03800, 05004], lr: 0.005727, loss: 1.5180
2022-03-04 22:24:53 - train: epoch 0171, iter [03900, 05004], lr: 0.005727, loss: 1.8838
2022-03-04 22:25:27 - train: epoch 0171, iter [04000, 05004], lr: 0.005727, loss: 1.7545
2022-03-04 22:26:00 - train: epoch 0171, iter [04100, 05004], lr: 0.005727, loss: 1.5643
2022-03-04 22:26:33 - train: epoch 0171, iter [04200, 05004], lr: 0.005727, loss: 1.6684
2022-03-04 22:27:08 - train: epoch 0171, iter [04300, 05004], lr: 0.005727, loss: 1.7759
2022-03-04 22:27:41 - train: epoch 0171, iter [04400, 05004], lr: 0.005727, loss: 1.8730
2022-03-04 22:28:15 - train: epoch 0171, iter [04500, 05004], lr: 0.005727, loss: 1.7783
2022-03-04 22:28:49 - train: epoch 0171, iter [04600, 05004], lr: 0.005727, loss: 1.6564
2022-03-04 22:29:23 - train: epoch 0171, iter [04700, 05004], lr: 0.005727, loss: 1.7550
2022-03-04 22:29:57 - train: epoch 0171, iter [04800, 05004], lr: 0.005727, loss: 1.9973
2022-03-04 22:30:30 - train: epoch 0171, iter [04900, 05004], lr: 0.005727, loss: 1.8766
2022-03-04 22:31:03 - train: epoch 0171, iter [05000, 05004], lr: 0.005727, loss: 1.8636
2022-03-04 22:31:04 - train: epoch 171, train_loss: 1.6918
2022-03-04 22:32:18 - eval: epoch: 171, acc1: 73.970%, acc5: 91.854%, test_loss: 1.0445, per_image_load_time: 0.556ms, per_image_inference_time: 0.486ms
2022-03-04 22:32:18 - until epoch: 171, best_acc1: 73.970%
2022-03-04 22:32:18 - epoch 172 lr: 0.005358608901706802
2022-03-04 22:32:57 - train: epoch 0172, iter [00100, 05004], lr: 0.005359, loss: 1.9041
2022-03-04 22:33:31 - train: epoch 0172, iter [00200, 05004], lr: 0.005359, loss: 1.5987
2022-03-04 22:34:05 - train: epoch 0172, iter [00300, 05004], lr: 0.005359, loss: 1.8142
2022-03-04 22:34:40 - train: epoch 0172, iter [00400, 05004], lr: 0.005359, loss: 1.5271
2022-03-04 22:35:14 - train: epoch 0172, iter [00500, 05004], lr: 0.005359, loss: 1.7815
2022-03-04 22:35:48 - train: epoch 0172, iter [00600, 05004], lr: 0.005359, loss: 1.5997
2022-03-04 22:36:22 - train: epoch 0172, iter [00700, 05004], lr: 0.005359, loss: 1.7925
2022-03-04 22:36:57 - train: epoch 0172, iter [00800, 05004], lr: 0.005359, loss: 1.7934
2022-03-04 22:37:29 - train: epoch 0172, iter [00900, 05004], lr: 0.005359, loss: 1.5881
2022-03-04 22:38:03 - train: epoch 0172, iter [01000, 05004], lr: 0.005359, loss: 1.3204
2022-03-04 22:38:37 - train: epoch 0172, iter [01100, 05004], lr: 0.005359, loss: 1.7050
2022-03-04 22:39:10 - train: epoch 0172, iter [01200, 05004], lr: 0.005359, loss: 1.8240
2022-03-04 22:39:43 - train: epoch 0172, iter [01300, 05004], lr: 0.005359, loss: 1.6241
2022-03-04 22:40:17 - train: epoch 0172, iter [01400, 05004], lr: 0.005359, loss: 1.8634
2022-03-04 22:40:50 - train: epoch 0172, iter [01500, 05004], lr: 0.005359, loss: 1.5102
2022-03-04 22:41:23 - train: epoch 0172, iter [01600, 05004], lr: 0.005359, loss: 1.8103
2022-03-04 22:41:57 - train: epoch 0172, iter [01700, 05004], lr: 0.005359, loss: 2.0335
2022-03-04 22:42:30 - train: epoch 0172, iter [01800, 05004], lr: 0.005359, loss: 1.8745
2022-03-04 22:43:04 - train: epoch 0172, iter [01900, 05004], lr: 0.005359, loss: 1.4858
2022-03-04 22:43:37 - train: epoch 0172, iter [02000, 05004], lr: 0.005359, loss: 1.6834
2022-03-04 22:44:10 - train: epoch 0172, iter [02100, 05004], lr: 0.005359, loss: 1.6045
2022-03-04 22:44:45 - train: epoch 0172, iter [02200, 05004], lr: 0.005359, loss: 1.4582
2022-03-04 22:45:19 - train: epoch 0172, iter [02300, 05004], lr: 0.005359, loss: 1.7413
2022-03-04 22:45:53 - train: epoch 0172, iter [02400, 05004], lr: 0.005359, loss: 1.7826
2022-03-04 22:46:26 - train: epoch 0172, iter [02500, 05004], lr: 0.005359, loss: 1.6998
2022-03-04 22:47:00 - train: epoch 0172, iter [02600, 05004], lr: 0.005359, loss: 1.6973
2022-03-04 22:47:34 - train: epoch 0172, iter [02700, 05004], lr: 0.005359, loss: 1.6465
2022-03-04 22:48:08 - train: epoch 0172, iter [02800, 05004], lr: 0.005359, loss: 1.7753
2022-03-04 22:48:42 - train: epoch 0172, iter [02900, 05004], lr: 0.005359, loss: 1.8595
2022-03-04 22:49:15 - train: epoch 0172, iter [03000, 05004], lr: 0.005359, loss: 1.5091
2022-03-04 22:49:48 - train: epoch 0172, iter [03100, 05004], lr: 0.005359, loss: 1.5603
2022-03-04 22:50:21 - train: epoch 0172, iter [03200, 05004], lr: 0.005359, loss: 1.6692
2022-03-04 22:50:55 - train: epoch 0172, iter [03300, 05004], lr: 0.005359, loss: 1.7840
2022-03-04 22:51:29 - train: epoch 0172, iter [03400, 05004], lr: 0.005359, loss: 1.8400
2022-03-04 22:52:04 - train: epoch 0172, iter [03500, 05004], lr: 0.005359, loss: 1.5100
2022-03-04 22:52:37 - train: epoch 0172, iter [03600, 05004], lr: 0.005359, loss: 1.6249
2022-03-04 22:53:13 - train: epoch 0172, iter [03700, 05004], lr: 0.005359, loss: 1.6437
2022-03-04 22:53:47 - train: epoch 0172, iter [03800, 05004], lr: 0.005359, loss: 1.7115
2022-03-04 22:54:21 - train: epoch 0172, iter [03900, 05004], lr: 0.005359, loss: 1.7064
2022-03-04 22:54:55 - train: epoch 0172, iter [04000, 05004], lr: 0.005359, loss: 1.7874
2022-03-04 22:55:29 - train: epoch 0172, iter [04100, 05004], lr: 0.005359, loss: 1.6570
2022-03-04 22:56:02 - train: epoch 0172, iter [04200, 05004], lr: 0.005359, loss: 1.8104
2022-03-04 22:56:36 - train: epoch 0172, iter [04300, 05004], lr: 0.005359, loss: 1.5672
2022-03-04 22:57:10 - train: epoch 0172, iter [04400, 05004], lr: 0.005359, loss: 1.6342
2022-03-04 22:57:44 - train: epoch 0172, iter [04500, 05004], lr: 0.005359, loss: 1.7516
2022-03-04 22:58:18 - train: epoch 0172, iter [04600, 05004], lr: 0.005359, loss: 1.7804
2022-03-04 22:58:51 - train: epoch 0172, iter [04700, 05004], lr: 0.005359, loss: 1.4968
2022-03-04 22:59:26 - train: epoch 0172, iter [04800, 05004], lr: 0.005359, loss: 1.6322
2022-03-04 22:59:59 - train: epoch 0172, iter [04900, 05004], lr: 0.005359, loss: 1.6375
2022-03-04 23:00:32 - train: epoch 0172, iter [05000, 05004], lr: 0.005359, loss: 1.3607
2022-03-04 23:00:33 - train: epoch 172, train_loss: 1.6739
2022-03-04 23:01:47 - eval: epoch: 172, acc1: 73.666%, acc5: 91.730%, test_loss: 1.0467, per_image_load_time: 0.563ms, per_image_inference_time: 0.471ms
2022-03-04 23:01:48 - until epoch: 172, best_acc1: 73.970%
2022-03-04 23:01:48 - epoch 173 lr: 0.005001605761689398
2022-03-04 23:02:27 - train: epoch 0173, iter [00100, 05004], lr: 0.005002, loss: 1.8086
2022-03-04 23:03:02 - train: epoch 0173, iter [00200, 05004], lr: 0.005002, loss: 1.8555
2022-03-04 23:03:35 - train: epoch 0173, iter [00300, 05004], lr: 0.005002, loss: 1.7006
2022-03-04 23:04:10 - train: epoch 0173, iter [00400, 05004], lr: 0.005002, loss: 1.5430
2022-03-04 23:04:43 - train: epoch 0173, iter [00500, 05004], lr: 0.005002, loss: 1.4505
2022-03-04 23:05:16 - train: epoch 0173, iter [00600, 05004], lr: 0.005002, loss: 1.5726
2022-03-04 23:05:51 - train: epoch 0173, iter [00700, 05004], lr: 0.005002, loss: 1.5781
2022-03-04 23:06:24 - train: epoch 0173, iter [00800, 05004], lr: 0.005002, loss: 1.5063
2022-03-04 23:06:58 - train: epoch 0173, iter [00900, 05004], lr: 0.005002, loss: 1.5801
2022-03-04 23:07:32 - train: epoch 0173, iter [01000, 05004], lr: 0.005002, loss: 1.8696
2022-03-04 23:08:04 - train: epoch 0173, iter [01100, 05004], lr: 0.005002, loss: 1.7081
2022-03-04 23:08:38 - train: epoch 0173, iter [01200, 05004], lr: 0.005002, loss: 1.4703
2022-03-04 23:09:12 - train: epoch 0173, iter [01300, 05004], lr: 0.005002, loss: 1.7875
2022-03-04 23:09:44 - train: epoch 0173, iter [01400, 05004], lr: 0.005002, loss: 1.6771
2022-03-04 23:10:19 - train: epoch 0173, iter [01500, 05004], lr: 0.005002, loss: 1.5321
2022-03-04 23:10:53 - train: epoch 0173, iter [01600, 05004], lr: 0.005002, loss: 1.6440
2022-03-04 23:11:25 - train: epoch 0173, iter [01700, 05004], lr: 0.005002, loss: 1.5418
