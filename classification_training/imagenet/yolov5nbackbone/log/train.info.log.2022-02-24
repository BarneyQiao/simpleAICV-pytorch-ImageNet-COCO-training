2022-02-24 08:13:01 - network: yolov5nbackbone
2022-02-24 08:13:01 - num_classes: 1000
2022-02-24 08:13:01 - input_image_size: 256
2022-02-24 08:13:01 - scale: 1.1428571428571428
2022-02-24 08:13:01 - trained_model_path: 
2022-02-24 08:13:01 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-24 08:13:01 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f2b0044ebe0>
2022-02-24 08:13:01 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f2b0044eeb0>
2022-02-24 08:13:01 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f2b0044eee0>
2022-02-24 08:13:01 - seed: 0
2022-02-24 08:13:01 - batch_size: 256
2022-02-24 08:13:01 - num_workers: 16
2022-02-24 08:13:01 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-24 08:13:01 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-24 08:13:01 - epochs: 100
2022-02-24 08:13:01 - print_interval: 100
2022-02-24 08:13:01 - distributed: True
2022-02-24 08:13:01 - sync_bn: False
2022-02-24 08:13:01 - apex: True
2022-02-24 08:13:01 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-24 08:13:01 - gpus_num: 2
2022-02-24 08:13:01 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f2ae2b09c30>
2022-02-24 08:13:06 - --------------------parameters--------------------
2022-02-24 08:13:06 - name: conv.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: conv.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: conv.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv2.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv2.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv2.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv3.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv3.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.conv3.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.2.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.2.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.2.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv2.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv2.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv2.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv3.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv3.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.conv3.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.4.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.4.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.4.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv2.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv2.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv2.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv3.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv3.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.conv3.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: middle_layers.6.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.6.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: middle_layers.6.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: sppf.conv1.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: sppf.conv1.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: sppf.conv1.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: sppf.conv2.layer.0.weight, grad: True
2022-02-24 08:13:06 - name: sppf.conv2.layer.1.weight, grad: True
2022-02-24 08:13:06 - name: sppf.conv2.layer.1.bias, grad: True
2022-02-24 08:13:06 - name: fc.weight, grad: True
2022-02-24 08:13:06 - name: fc.bias, grad: True
2022-02-24 08:13:06 - --------------------buffers--------------------
2022-02-24 08:13:06 - name: conv.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: conv.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: conv.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv2.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv2.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv3.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv3.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.2.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.2.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv2.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv2.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv3.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv3.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.4.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.4.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.4.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv2.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv2.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv3.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv3.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: middle_layers.6.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: middle_layers.6.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: middle_layers.6.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: sppf.conv1.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: sppf.conv1.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: sppf.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - name: sppf.conv2.layer.1.running_mean, grad: False
2022-02-24 08:13:06 - name: sppf.conv2.layer.1.running_var, grad: False
2022-02-24 08:13:06 - name: sppf.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 08:13:06 - epoch 001 lr: 0.1
2022-02-24 08:13:45 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8790
2022-02-24 08:14:18 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7446
2022-02-24 08:14:53 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.7031
2022-02-24 08:15:27 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5721
2022-02-24 08:16:01 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.3916
2022-02-24 08:16:34 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.2572
2022-02-24 08:17:08 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1596
2022-02-24 08:17:42 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.9854
2022-02-24 08:18:15 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.9509
2022-02-24 08:18:49 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.8265
2022-02-24 08:19:22 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.7850
2022-02-24 08:19:55 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.4681
2022-02-24 08:20:29 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.4519
2022-02-24 08:21:02 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.4472
2022-02-24 08:21:36 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.3433
2022-02-24 08:22:10 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.4692
2022-02-24 08:22:44 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.1669
2022-02-24 08:23:17 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.2151
2022-02-24 08:23:52 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.1896
2022-02-24 08:24:25 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.1223
2022-02-24 08:25:00 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.0813
2022-02-24 08:25:32 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.0557
2022-02-24 08:26:06 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.7854
2022-02-24 08:26:40 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.8524
2022-02-24 08:27:14 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.9972
2022-02-24 08:27:48 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.1307
2022-02-24 08:28:22 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.9945
2022-02-24 08:28:56 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.8275
2022-02-24 08:29:29 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.7536
2022-02-24 08:30:03 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.7436
2022-02-24 08:30:37 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.8419
2022-02-24 08:31:11 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.7652
2022-02-24 08:31:45 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.5554
2022-02-24 08:32:18 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.6349
2022-02-24 08:32:53 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.6209
2022-02-24 08:33:26 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.6821
2022-02-24 08:34:00 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.7644
2022-02-24 08:34:34 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.6261
2022-02-24 08:35:08 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.7105
2022-02-24 08:35:42 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.5897
2022-02-24 08:36:16 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.7020
2022-02-24 08:36:50 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.5731
2022-02-24 08:37:24 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.4590
2022-02-24 08:37:58 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.3584
2022-02-24 08:38:32 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.5591
2022-02-24 08:39:08 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.6494
2022-02-24 08:39:41 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.4223
2022-02-24 08:40:16 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.6787
2022-02-24 08:40:50 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.4607
2022-02-24 08:41:24 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.2113
2022-02-24 08:41:25 - train: epoch 001, train_loss: 5.1478
2022-02-24 08:42:41 - eval: epoch: 001, acc1: 17.810%, acc5: 38.462%, test_loss: 4.1936, per_image_load_time: 2.238ms, per_image_inference_time: 0.200ms
2022-02-24 08:42:41 - until epoch: 001, best_acc1: 17.810%
2022-02-24 08:42:41 - epoch 002 lr: 0.1
2022-02-24 08:43:21 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.5760
2022-02-24 08:43:54 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.2379
2022-02-24 08:44:28 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.5888
2022-02-24 08:45:02 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.4580
2022-02-24 08:45:35 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.2757
2022-02-24 08:46:09 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.1472
2022-02-24 08:46:43 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.4072
2022-02-24 08:47:17 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 4.1211
2022-02-24 08:47:50 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.9640
2022-02-24 08:48:24 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.5519
2022-02-24 08:48:58 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.3606
2022-02-24 08:49:31 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.2823
2022-02-24 08:50:05 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.2715
2022-02-24 08:50:40 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.3387
2022-02-24 08:51:14 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.4658
2022-02-24 08:51:47 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 4.1635
2022-02-24 08:52:21 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.1898
2022-02-24 08:52:55 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.2652
2022-02-24 08:53:30 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 4.0935
2022-02-24 08:54:03 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.9980
2022-02-24 08:54:37 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 4.1615
2022-02-24 08:55:10 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.8515
2022-02-24 08:55:45 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 4.2135
2022-02-24 08:56:18 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 4.0258
2022-02-24 08:56:52 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 4.1165
2022-02-24 08:57:26 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 4.0306
2022-02-24 08:58:01 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 4.2952
2022-02-24 08:58:34 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 4.2096
2022-02-24 08:59:09 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.9884
2022-02-24 08:59:42 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.9442
2022-02-24 09:00:17 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.9654
2022-02-24 09:00:50 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 4.0930
2022-02-24 09:01:25 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 4.0248
2022-02-24 09:01:58 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 4.2530
2022-02-24 09:02:33 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.8873
2022-02-24 09:03:06 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 4.0891
2022-02-24 09:03:41 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 4.1155
2022-02-24 09:04:13 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.8481
2022-02-24 09:04:48 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 4.1091
2022-02-24 09:05:22 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 4.0578
2022-02-24 09:05:56 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 4.1151
2022-02-24 09:06:31 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.9435
2022-02-24 09:07:04 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.8595
2022-02-24 09:07:38 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.8680
2022-02-24 09:08:13 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.9584
2022-02-24 09:08:47 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.9214
2022-02-24 09:09:21 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.9965
2022-02-24 09:09:56 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.9050
2022-02-24 09:10:31 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.7993
2022-02-24 09:11:04 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.8300
2022-02-24 09:11:05 - train: epoch 002, train_loss: 4.1333
2022-02-24 09:12:23 - eval: epoch: 002, acc1: 24.214%, acc5: 47.764%, test_loss: 3.7264, per_image_load_time: 1.617ms, per_image_inference_time: 0.167ms
2022-02-24 09:12:23 - until epoch: 002, best_acc1: 24.214%
2022-02-24 09:12:23 - epoch 003 lr: 0.1
2022-02-24 09:13:03 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.8144
2022-02-24 09:13:37 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.9311
2022-02-24 09:14:11 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.9781
2022-02-24 09:14:44 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.8086
2022-02-24 09:15:18 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 4.0364
2022-02-24 09:15:51 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.8138
2022-02-24 09:16:26 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 4.0548
2022-02-24 09:16:58 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.8936
2022-02-24 09:17:33 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.8820
2022-02-24 09:18:06 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.9728
2022-02-24 09:18:40 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.7655
2022-02-24 09:19:14 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.8802
2022-02-24 09:19:48 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.9409
2022-02-24 09:20:22 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.8123
2022-02-24 09:20:56 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 4.0467
2022-02-24 09:21:31 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.8007
2022-02-24 09:22:05 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.6986
2022-02-24 09:22:39 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.7404
2022-02-24 09:23:13 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.9274
2022-02-24 09:23:48 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 4.2056
2022-02-24 09:24:21 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.9749
2022-02-24 09:24:56 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 4.1520
2022-02-24 09:25:29 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.8435
2022-02-24 09:26:03 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.8097
2022-02-24 09:26:38 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 4.0357
2022-02-24 09:27:11 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.7656
2022-02-24 09:27:47 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 4.1101
2022-02-24 09:28:20 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.7550
2022-02-24 09:28:54 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.8357
2022-02-24 09:29:28 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.9761
2022-02-24 09:30:03 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.9869
2022-02-24 09:30:36 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.7856
2022-02-24 09:31:11 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.7016
2022-02-24 09:31:45 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.9707
2022-02-24 09:32:20 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.5500
2022-02-24 09:32:53 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.7840
2022-02-24 09:33:28 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.8465
2022-02-24 09:34:02 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.9073
2022-02-24 09:34:36 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.9897
2022-02-24 09:35:10 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.6315
2022-02-24 09:35:44 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.7814
2022-02-24 09:36:18 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.6824
2022-02-24 09:36:52 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 3.4903
2022-02-24 09:37:26 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 3.6722
2022-02-24 09:38:02 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.7067
2022-02-24 09:38:36 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.6603
2022-02-24 09:39:11 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.6892
2022-02-24 09:39:45 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.7867
2022-02-24 09:40:21 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.7572
2022-02-24 09:40:54 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.7176
2022-02-24 09:40:55 - train: epoch 003, train_loss: 3.8308
2022-02-24 09:42:12 - eval: epoch: 003, acc1: 26.642%, acc5: 50.702%, test_loss: 3.5628, per_image_load_time: 2.705ms, per_image_inference_time: 0.182ms
2022-02-24 09:42:12 - until epoch: 003, best_acc1: 26.642%
2022-02-24 09:42:12 - epoch 004 lr: 0.1
2022-02-24 09:42:51 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.8464
2022-02-24 09:43:25 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.6690
2022-02-24 09:43:59 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.6717
2022-02-24 09:44:34 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 3.5669
2022-02-24 09:45:07 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 3.6135
2022-02-24 09:45:40 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.8808
2022-02-24 09:46:14 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.9062
2022-02-24 09:46:48 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 3.5633
2022-02-24 09:47:22 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 3.5254
2022-02-24 09:47:56 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.8189
2022-02-24 09:48:30 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.7405
2022-02-24 09:49:05 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 3.5068
2022-02-24 09:49:38 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 3.5456
2022-02-24 09:50:12 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.6522
2022-02-24 09:50:47 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.7677
2022-02-24 09:51:21 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 3.6930
2022-02-24 09:51:54 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.6819
2022-02-24 09:52:28 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.8794
2022-02-24 09:53:03 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.8319
2022-02-24 09:53:37 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.7314
2022-02-24 09:54:11 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.6845
2022-02-24 09:54:46 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.5576
2022-02-24 09:55:19 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 3.6127
2022-02-24 09:55:54 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 3.4096
2022-02-24 09:56:29 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 3.6440
2022-02-24 09:57:03 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.8068
2022-02-24 09:57:36 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 3.5538
2022-02-24 09:58:11 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 3.5345
2022-02-24 09:58:45 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 3.6681
2022-02-24 09:59:20 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 3.6429
2022-02-24 09:59:53 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.6063
2022-02-24 10:00:28 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 3.6653
2022-02-24 10:01:02 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.6769
2022-02-24 10:01:36 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.6711
2022-02-24 10:02:10 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 3.4858
2022-02-24 10:02:45 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 3.3693
2022-02-24 10:03:19 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.6155
2022-02-24 10:03:54 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 3.5215
2022-02-24 10:04:28 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 3.4926
2022-02-24 10:05:02 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 3.3397
2022-02-24 10:05:36 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 3.6569
2022-02-24 10:06:11 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 3.6683
2022-02-24 10:06:45 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 3.4735
2022-02-24 10:07:19 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 3.3868
2022-02-24 10:07:53 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 3.1023
2022-02-24 10:08:29 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 3.6587
2022-02-24 10:09:02 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 3.5174
2022-02-24 10:09:37 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 3.4472
2022-02-24 10:10:11 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 3.6082
2022-02-24 10:10:46 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 3.6850
2022-02-24 10:10:46 - train: epoch 004, train_loss: 3.6567
2022-02-24 10:12:04 - eval: epoch: 004, acc1: 29.696%, acc5: 54.656%, test_loss: 3.3451, per_image_load_time: 2.070ms, per_image_inference_time: 0.174ms
2022-02-24 10:12:04 - until epoch: 004, best_acc1: 29.696%
2022-02-24 10:12:04 - epoch 005 lr: 0.1
2022-02-24 10:12:43 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.6427
2022-02-24 10:13:18 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.5074
2022-02-24 10:13:51 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.7774
2022-02-24 10:14:25 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 3.6226
2022-02-24 10:14:59 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 3.4056
2022-02-24 10:15:33 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 3.4897
2022-02-24 10:16:07 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 3.5615
2022-02-24 10:16:41 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.7991
2022-02-24 10:17:15 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 3.5388
2022-02-24 10:17:49 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 3.6104
2022-02-24 10:18:23 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.7270
2022-02-24 10:18:57 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.5201
2022-02-24 10:19:31 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 3.6192
2022-02-24 10:20:05 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.6956
2022-02-24 10:20:38 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 3.4167
2022-02-24 10:21:13 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 3.3152
2022-02-24 10:21:47 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 3.5207
2022-02-24 10:22:21 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 3.6519
2022-02-24 10:22:55 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 3.2362
2022-02-24 10:23:28 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 3.5092
2022-02-24 10:24:03 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 3.4141
2022-02-24 10:24:37 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 3.5823
2022-02-24 10:25:11 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 3.4980
2022-02-24 10:25:45 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 3.5361
2022-02-24 10:26:19 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 3.4609
2022-02-24 10:26:53 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.7018
2022-02-24 10:27:28 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.8341
2022-02-24 10:28:01 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 3.4506
2022-02-24 10:28:36 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 3.3304
2022-02-24 10:29:09 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 3.4571
2022-02-24 10:29:44 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 3.5170
2022-02-24 10:30:17 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.5799
2022-02-24 10:30:52 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 3.4058
2022-02-24 10:31:26 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 3.3530
2022-02-24 10:32:00 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 3.5357
2022-02-24 10:32:34 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 3.5910
2022-02-24 10:33:08 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 3.4357
2022-02-24 10:33:43 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 3.5024
2022-02-24 10:34:17 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.7173
2022-02-24 10:34:51 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 3.4886
2022-02-24 10:35:25 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 3.4002
2022-02-24 10:35:59 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 3.4302
2022-02-24 10:36:34 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 3.3752
2022-02-24 10:37:08 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 3.5396
2022-02-24 10:37:43 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 3.6107
2022-02-24 10:38:17 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 3.4534
2022-02-24 10:38:52 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 3.3239
2022-02-24 10:39:28 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 3.3749
2022-02-24 10:40:02 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 3.6968
2022-02-24 10:40:37 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 3.4638
2022-02-24 10:40:37 - train: epoch 005, train_loss: 3.5422
2022-02-24 10:41:53 - eval: epoch: 005, acc1: 30.284%, acc5: 55.400%, test_loss: 3.3043, per_image_load_time: 1.892ms, per_image_inference_time: 0.189ms
2022-02-24 10:41:53 - until epoch: 005, best_acc1: 30.284%
2022-02-24 10:41:53 - epoch 006 lr: 0.1
2022-02-24 10:42:33 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.3644
2022-02-24 10:43:07 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.5426
2022-02-24 10:43:41 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.3826
2022-02-24 10:44:15 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.5869
2022-02-24 10:44:49 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.4470
2022-02-24 10:45:23 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.4826
2022-02-24 10:45:57 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.5328
2022-02-24 10:46:32 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.6097
2022-02-24 10:47:06 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.4037
2022-02-24 10:47:40 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.1791
2022-02-24 10:48:14 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.3350
2022-02-24 10:48:48 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.5406
2022-02-24 10:49:22 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.6251
2022-02-24 10:49:57 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 3.6082
2022-02-24 10:50:31 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 3.5645
2022-02-24 10:51:05 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 3.2682
2022-02-24 10:51:39 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 3.5675
2022-02-24 10:52:13 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 3.4982
2022-02-24 10:52:47 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 3.4669
2022-02-24 10:53:21 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 3.7034
2022-02-24 10:53:55 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 3.4452
2022-02-24 10:54:28 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 3.3694
2022-02-24 10:55:04 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 3.3525
2022-02-24 10:55:37 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 3.5136
2022-02-24 10:56:12 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 3.6001
2022-02-24 10:56:45 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 3.4827
2022-02-24 10:57:20 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 3.5805
2022-02-24 10:57:53 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 3.3185
2022-02-24 10:58:29 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 3.4733
2022-02-24 10:59:02 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 3.3045
2022-02-24 10:59:36 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 3.2435
2022-02-24 11:00:11 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 3.3042
2022-02-24 11:00:44 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 3.3298
2022-02-24 11:01:19 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 3.6405
2022-02-24 11:01:52 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 3.5269
2022-02-24 11:02:28 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 3.5795
2022-02-24 11:03:01 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 3.3947
2022-02-24 11:03:36 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 3.2628
2022-02-24 11:04:09 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 3.2994
2022-02-24 11:04:44 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.7779
2022-02-24 11:05:18 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 3.3902
2022-02-24 11:05:52 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 3.4338
2022-02-24 11:06:26 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 3.2452
2022-02-24 11:07:01 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 3.3805
2022-02-24 11:07:35 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 3.5795
2022-02-24 11:08:09 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 3.4130
2022-02-24 11:08:43 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 3.5943
2022-02-24 11:09:18 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 3.5708
2022-02-24 11:09:53 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 3.5168
2022-02-24 11:10:27 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 3.3773
2022-02-24 11:10:27 - train: epoch 006, train_loss: 3.4684
2022-02-24 11:11:43 - eval: epoch: 006, acc1: 30.880%, acc5: 56.768%, test_loss: 3.2470, per_image_load_time: 2.486ms, per_image_inference_time: 0.195ms
2022-02-24 11:11:43 - until epoch: 006, best_acc1: 30.880%
2022-02-24 11:11:43 - epoch 007 lr: 0.1
2022-02-24 11:12:23 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 3.2438
2022-02-24 11:12:56 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 3.6259
2022-02-24 11:13:31 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 3.7684
2022-02-24 11:14:04 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 3.3371
2022-02-24 11:14:40 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 3.3519
2022-02-24 11:15:13 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 3.6076
2022-02-24 11:15:47 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 3.4290
2022-02-24 11:16:21 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 3.4751
2022-02-24 11:16:55 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 3.4807
2022-02-24 11:17:29 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 3.4131
2022-02-24 11:18:03 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 3.2506
2022-02-24 11:18:37 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 3.3710
2022-02-24 11:19:12 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 3.2372
2022-02-24 11:19:46 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 3.5563
2022-02-24 11:20:20 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 3.4561
2022-02-24 11:20:53 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 3.4119
2022-02-24 11:21:27 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 3.4683
2022-02-24 11:21:59 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 3.2615
2022-02-24 11:22:32 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 3.4802
2022-02-24 11:23:05 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 3.2382
2022-02-24 11:23:38 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 3.5073
2022-02-24 11:24:11 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 3.2750
2022-02-24 11:24:45 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 3.5385
2022-02-24 11:25:17 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 3.4378
2022-02-24 11:25:50 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 3.2660
2022-02-24 11:26:23 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 3.2964
2022-02-24 11:26:57 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 3.2271
2022-02-24 11:27:29 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 3.3458
2022-02-24 11:28:04 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 3.3407
2022-02-24 11:28:37 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 3.4697
2022-02-24 11:29:12 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 3.3993
2022-02-24 11:29:46 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 3.2842
2022-02-24 11:30:20 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 3.5906
2022-02-24 11:30:54 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 3.2815
2022-02-24 11:31:29 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 3.4631
2022-02-24 11:32:02 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 3.1647
2022-02-24 11:32:36 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 3.5362
2022-02-24 11:33:11 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 3.7538
2022-02-24 11:33:44 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 3.3622
2022-02-24 11:34:20 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 3.4246
2022-02-24 11:34:52 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 3.4949
2022-02-24 11:35:28 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 3.2091
2022-02-24 11:36:02 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 3.5214
2022-02-24 11:36:36 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 3.3542
2022-02-24 11:37:10 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 3.5814
2022-02-24 11:37:45 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 3.4686
2022-02-24 11:38:19 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 3.5712
2022-02-24 11:38:54 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 3.6000
2022-02-24 11:39:29 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 3.4388
2022-02-24 11:40:03 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 3.5220
2022-02-24 11:40:04 - train: epoch 007, train_loss: 3.4122
2022-02-24 11:41:20 - eval: epoch: 007, acc1: 32.918%, acc5: 58.816%, test_loss: 3.1215, per_image_load_time: 2.523ms, per_image_inference_time: 0.195ms
2022-02-24 11:41:20 - until epoch: 007, best_acc1: 32.918%
2022-02-24 11:41:20 - epoch 008 lr: 0.1
2022-02-24 11:41:59 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 3.3581
2022-02-24 11:42:33 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 3.5058
2022-02-24 11:43:06 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 3.1471
2022-02-24 11:43:41 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 3.3167
2022-02-24 11:44:14 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 3.2040
2022-02-24 11:44:49 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 3.2018
2022-02-24 11:45:22 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.6520
2022-02-24 11:45:57 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 3.2981
2022-02-24 11:46:31 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 3.3562
2022-02-24 11:47:06 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 3.3654
2022-02-24 11:47:39 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 3.1070
2022-02-24 11:48:14 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 3.1055
2022-02-24 11:48:48 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 3.4498
2022-02-24 11:49:23 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 3.2455
2022-02-24 11:49:57 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 3.5788
2022-02-24 11:50:31 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 3.5231
2022-02-24 11:51:05 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 3.3130
2022-02-24 11:51:40 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 3.3635
2022-02-24 11:52:13 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 3.1762
2022-02-24 11:52:48 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 3.5370
2022-02-24 11:53:21 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 3.3724
2022-02-24 11:53:56 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 3.3143
2022-02-24 11:54:29 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 3.3712
2022-02-24 11:55:03 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 3.4355
2022-02-24 11:55:37 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 3.3546
2022-02-24 11:56:12 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 3.4618
2022-02-24 11:56:45 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 3.5545
2022-02-24 11:57:20 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 3.4788
2022-02-24 11:57:54 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 3.3717
2022-02-24 11:58:28 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 3.4557
2022-02-24 11:59:01 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 3.4950
2022-02-24 11:59:36 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 3.4643
2022-02-24 12:00:09 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 3.5732
2022-02-24 12:00:44 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 3.5533
2022-02-24 12:01:17 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 3.4604
2022-02-24 12:01:52 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 3.5166
2022-02-24 12:02:25 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 3.3160
2022-02-24 12:03:00 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 3.2796
2022-02-24 12:03:33 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 3.3968
2022-02-24 12:04:08 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.6325
2022-02-24 12:04:41 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 3.3026
2022-02-24 12:05:16 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 3.3031
2022-02-24 12:05:51 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 3.1890
2022-02-24 12:06:24 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 3.3344
2022-02-24 12:06:58 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 3.4658
2022-02-24 12:07:32 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 3.5606
2022-02-24 12:08:07 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 3.3240
2022-02-24 12:08:41 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 3.2114
2022-02-24 12:09:16 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 3.3958
2022-02-24 12:09:49 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 3.1086
2022-02-24 12:09:50 - train: epoch 008, train_loss: 3.3707
2022-02-24 12:11:07 - eval: epoch: 008, acc1: 32.086%, acc5: 58.018%, test_loss: 3.1704, per_image_load_time: 1.674ms, per_image_inference_time: 0.178ms
2022-02-24 12:11:07 - until epoch: 008, best_acc1: 32.918%
2022-02-24 12:11:07 - epoch 009 lr: 0.1
2022-02-24 12:11:46 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 3.0640
2022-02-24 12:12:22 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 3.2702
2022-02-24 12:12:55 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 3.1309
2022-02-24 12:13:29 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 3.4759
2022-02-24 12:14:02 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 3.2870
2022-02-24 12:14:37 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 3.4616
2022-02-24 12:15:10 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 3.2509
2022-02-24 12:15:44 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 3.2176
2022-02-24 12:16:18 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 3.2191
2022-02-24 12:16:52 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 3.2479
2022-02-24 12:17:26 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 3.6960
2022-02-24 12:18:00 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 3.5225
2022-02-24 12:18:33 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 3.4053
2022-02-24 12:19:08 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 3.0676
2022-02-24 12:19:42 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 3.2870
2022-02-24 12:20:16 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 3.4245
2022-02-24 12:20:49 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 3.4253
2022-02-24 12:21:24 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 3.1736
2022-02-24 12:21:59 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 3.1160
2022-02-24 12:22:32 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 3.2729
2022-02-24 12:23:08 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 3.5709
2022-02-24 12:23:41 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 3.5413
2022-02-24 12:24:16 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 3.1576
2022-02-24 12:24:50 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 3.2911
2022-02-24 12:25:24 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 3.0509
2022-02-24 12:25:58 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 3.3817
2022-02-24 12:26:32 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 3.2166
2022-02-24 12:27:07 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 3.3452
2022-02-24 12:27:40 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 3.0607
2022-02-24 12:28:15 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 3.2427
2022-02-24 12:28:49 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 3.4935
2022-02-24 12:29:24 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 3.3442
2022-02-24 12:29:58 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 3.4258
2022-02-24 12:30:32 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 3.5786
2022-02-24 12:31:06 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 3.4629
2022-02-24 12:31:41 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 3.1907
2022-02-24 12:32:15 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 3.4718
2022-02-24 12:32:49 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 3.4858
2022-02-24 12:33:23 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 3.0089
2022-02-24 12:33:58 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 3.6047
2022-02-24 12:34:32 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 3.2651
2022-02-24 12:35:07 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 3.0632
2022-02-24 12:35:41 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 3.4661
2022-02-24 12:36:15 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 3.4094
2022-02-24 12:36:49 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 3.4059
2022-02-24 12:37:24 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 3.3886
2022-02-24 12:37:58 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 3.3551
2022-02-24 12:38:33 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 3.4513
2022-02-24 12:39:08 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 3.3153
2022-02-24 12:39:41 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 3.1996
2022-02-24 12:39:42 - train: epoch 009, train_loss: 3.3385
2022-02-24 12:40:58 - eval: epoch: 009, acc1: 33.252%, acc5: 59.366%, test_loss: 3.1108, per_image_load_time: 1.628ms, per_image_inference_time: 0.193ms
2022-02-24 12:40:58 - until epoch: 009, best_acc1: 33.252%
2022-02-24 12:40:58 - epoch 010 lr: 0.1
2022-02-24 12:41:37 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 3.4401
2022-02-24 12:42:11 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 3.4234
2022-02-24 12:42:46 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 3.3819
2022-02-24 12:43:19 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 3.3672
2022-02-24 12:43:53 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 3.2896
2022-02-24 12:44:26 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 3.4488
2022-02-24 12:45:01 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 3.3462
2022-02-24 12:45:34 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 3.1732
2022-02-24 12:46:09 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 3.1210
2022-02-24 12:46:42 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 3.1629
2022-02-24 12:47:17 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 3.4196
2022-02-24 12:47:51 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 3.1120
2022-02-24 12:48:26 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 3.2900
2022-02-24 12:48:59 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 3.4286
2022-02-24 12:49:34 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 3.0583
2022-02-24 12:50:08 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 3.2422
2022-02-24 12:50:43 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 3.5038
2022-02-24 12:51:15 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 3.2509
2022-02-24 12:51:51 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 3.1545
2022-02-24 12:52:23 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 3.3738
2022-02-24 12:52:59 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 3.3016
2022-02-24 12:53:31 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 3.4462
2022-02-24 12:54:06 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 3.5407
2022-02-24 12:54:40 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 3.4216
2022-02-24 12:55:14 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 3.3550
2022-02-24 12:55:48 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 3.3057
2022-02-24 12:56:22 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 3.3108
2022-02-24 12:56:56 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 3.3031
2022-02-24 12:57:30 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 3.5582
2022-02-24 12:58:05 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 3.1927
2022-02-24 12:58:39 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 3.4285
2022-02-24 12:59:13 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 3.4163
2022-02-24 12:59:47 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 3.4625
2022-02-24 13:00:21 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 3.5930
2022-02-24 13:00:55 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 3.4105
2022-02-24 13:01:29 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 3.5685
2022-02-24 13:02:04 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 3.2037
2022-02-24 13:02:39 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 3.4461
2022-02-24 13:03:12 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 3.0586
2022-02-24 13:03:47 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 3.1566
2022-02-24 13:04:21 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 3.1721
2022-02-24 13:04:55 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 3.4049
2022-02-24 13:05:30 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 3.2879
2022-02-24 13:06:03 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 3.2652
2022-02-24 13:06:39 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 3.2309
2022-02-24 13:07:13 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 3.3140
2022-02-24 13:07:48 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 3.3104
2022-02-24 13:08:21 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 3.1811
2022-02-24 13:08:58 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 3.0803
2022-02-24 13:09:31 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 3.0637
2022-02-24 13:09:32 - train: epoch 010, train_loss: 3.3095
2022-02-24 13:10:48 - eval: epoch: 010, acc1: 34.632%, acc5: 60.512%, test_loss: 3.0307, per_image_load_time: 1.174ms, per_image_inference_time: 0.200ms
2022-02-24 13:10:48 - until epoch: 010, best_acc1: 34.632%
2022-02-24 13:10:48 - epoch 011 lr: 0.1
2022-02-24 13:11:28 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 3.2159
2022-02-24 13:12:01 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 3.3894
2022-02-24 13:12:35 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 3.1258
2022-02-24 13:13:09 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 3.3871
2022-02-24 13:13:42 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 3.3151
2022-02-24 13:14:17 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 3.2856
2022-02-24 13:14:51 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 3.2077
2022-02-24 13:15:24 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 3.3270
2022-02-24 13:15:59 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 3.3844
2022-02-24 13:16:32 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 3.2598
2022-02-24 13:17:07 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 3.3562
2022-02-24 13:17:41 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 3.5862
2022-02-24 13:18:15 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 3.3888
2022-02-24 13:18:49 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 3.2591
2022-02-24 13:19:24 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 3.0577
2022-02-24 13:19:57 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 3.2696
2022-02-24 13:20:32 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 3.3837
2022-02-24 13:21:06 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 3.1960
2022-02-24 13:21:40 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 3.1746
2022-02-24 13:22:14 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 3.3134
2022-02-24 13:22:48 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 3.2401
2022-02-24 13:23:23 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 3.3387
2022-02-24 13:23:57 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 3.4902
2022-02-24 13:24:31 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 3.2306
2022-02-24 13:25:05 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 3.5630
2022-02-24 13:25:39 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 3.2729
2022-02-24 13:26:13 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 3.1790
2022-02-24 13:26:47 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 3.0473
2022-02-24 13:27:22 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 3.1255
2022-02-24 13:27:56 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 3.5715
2022-02-24 13:28:30 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 3.3514
2022-02-24 13:29:05 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 3.0752
2022-02-24 13:29:39 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 3.3366
2022-02-24 13:30:15 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 3.2687
2022-02-24 13:30:49 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 3.2158
2022-02-24 13:31:23 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 3.3860
2022-02-24 13:31:58 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 3.5772
2022-02-24 13:32:33 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.8359
2022-02-24 13:33:08 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 3.3369
2022-02-24 13:33:42 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 3.2694
2022-02-24 13:34:17 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 3.1709
2022-02-24 13:34:51 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 3.1235
2022-02-24 13:35:26 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 3.3321
2022-02-24 13:36:02 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 3.3587
2022-02-24 13:36:36 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 3.0652
2022-02-24 13:37:11 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 3.3515
2022-02-24 13:37:45 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.9173
2022-02-24 13:38:21 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 3.0269
2022-02-24 13:38:56 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 3.0875
2022-02-24 13:39:30 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 3.2122
2022-02-24 13:39:31 - train: epoch 011, train_loss: 3.2869
2022-02-24 13:40:47 - eval: epoch: 011, acc1: 33.656%, acc5: 59.814%, test_loss: 3.0757, per_image_load_time: 1.419ms, per_image_inference_time: 0.202ms
2022-02-24 13:40:47 - until epoch: 011, best_acc1: 34.632%
2022-02-24 13:40:47 - epoch 012 lr: 0.1
2022-02-24 13:41:27 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 3.1912
2022-02-24 13:42:00 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 3.0773
2022-02-24 13:42:35 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 3.1585
2022-02-24 13:43:10 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 3.3907
2022-02-24 13:43:45 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 3.5823
2022-02-24 13:44:19 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 3.0762
2022-02-24 13:44:54 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 3.2122
2022-02-24 13:45:28 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 3.2791
2022-02-24 13:46:02 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 3.3104
2022-02-24 13:46:37 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 3.0766
2022-02-24 13:47:10 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 3.6384
2022-02-24 13:47:45 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 3.2174
2022-02-24 13:48:19 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 3.1935
2022-02-24 13:48:53 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 3.3355
2022-02-24 13:49:28 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 3.0543
2022-02-24 13:50:03 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 3.2589
2022-02-24 13:50:37 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 3.2412
2022-02-24 13:51:12 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 3.2946
2022-02-24 13:51:46 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 3.3241
2022-02-24 13:52:21 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 3.3542
2022-02-24 13:52:54 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 3.3680
2022-02-24 13:53:29 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 3.3156
2022-02-24 13:54:03 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 3.1927
2022-02-24 13:54:39 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 3.3961
2022-02-24 13:55:13 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 3.0394
2022-02-24 13:55:47 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 3.0766
2022-02-24 13:56:22 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 3.1243
2022-02-24 13:56:57 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 3.2949
2022-02-24 13:57:31 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 3.0542
2022-02-24 13:58:05 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 3.1868
2022-02-24 13:58:40 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 3.3199
2022-02-24 13:59:13 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.8969
2022-02-24 13:59:48 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 3.1454
2022-02-24 14:00:22 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 3.2820
2022-02-24 14:00:56 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 3.3708
2022-02-24 14:01:32 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 3.3271
2022-02-24 14:02:06 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 3.1760
2022-02-24 14:02:41 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 3.1162
2022-02-24 14:03:15 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 3.0944
2022-02-24 14:03:51 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 3.0678
2022-02-24 14:04:25 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 3.2251
2022-02-24 14:04:59 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 3.1283
2022-02-24 14:05:33 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 3.4901
2022-02-24 14:06:08 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 3.0611
2022-02-24 14:06:43 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 3.2247
2022-02-24 14:07:18 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 3.6308
2022-02-24 14:07:55 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 3.1729
2022-02-24 14:08:29 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 3.3666
2022-02-24 14:09:06 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 3.3209
2022-02-24 14:09:40 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.7607
2022-02-24 14:09:40 - train: epoch 012, train_loss: 3.2689
2022-02-24 14:10:58 - eval: epoch: 012, acc1: 34.622%, acc5: 60.886%, test_loss: 3.0085, per_image_load_time: 1.998ms, per_image_inference_time: 0.210ms
2022-02-24 14:10:58 - until epoch: 012, best_acc1: 34.632%
2022-02-24 14:10:58 - epoch 013 lr: 0.1
2022-02-24 14:11:37 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 3.0735
2022-02-24 14:12:12 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 3.2614
2022-02-24 14:12:46 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 3.1312
2022-02-24 14:13:20 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 3.2783
2022-02-24 14:13:54 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 3.1484
2022-02-24 14:14:28 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 3.4775
2022-02-24 14:15:03 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 3.0064
2022-02-24 14:15:36 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 3.3480
2022-02-24 14:16:11 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 3.3361
2022-02-24 14:16:45 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 3.2781
2022-02-24 14:17:20 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 3.2541
2022-02-24 14:17:55 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 3.3370
2022-02-24 14:18:29 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 3.3725
2022-02-24 14:19:04 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 3.3274
2022-02-24 14:19:37 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 3.3595
2022-02-24 14:20:13 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.9671
2022-02-24 14:20:47 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 3.3677
2022-02-24 14:21:21 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 3.0888
2022-02-24 14:21:55 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 3.2536
2022-02-24 14:22:30 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 3.3931
2022-02-24 14:23:04 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 3.5753
2022-02-24 14:23:39 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 3.2279
2022-02-24 14:24:14 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 3.2184
2022-02-24 14:24:48 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 3.3359
2022-02-24 14:25:22 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 3.2776
2022-02-24 14:25:56 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 3.1359
2022-02-24 14:26:32 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 3.1793
2022-02-24 14:27:06 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 3.3305
2022-02-24 14:27:40 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 3.2440
2022-02-24 14:28:14 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 3.2752
2022-02-24 14:28:49 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 3.0181
2022-02-24 14:29:22 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 3.2999
2022-02-24 14:29:57 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 3.0630
2022-02-24 14:30:31 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 3.2145
2022-02-24 14:31:06 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 3.0508
2022-02-24 14:31:39 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 3.4246
2022-02-24 14:32:15 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 3.0331
2022-02-24 14:32:50 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 3.2787
2022-02-24 14:33:24 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 3.3005
2022-02-24 14:33:59 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 3.2506
2022-02-24 14:34:33 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 3.2269
2022-02-24 14:35:08 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 3.2985
2022-02-24 14:35:42 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 3.2758
2022-02-24 14:36:18 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 3.2026
2022-02-24 14:36:52 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 3.3524
2022-02-24 14:37:27 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 3.2554
2022-02-24 14:38:02 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 3.3845
2022-02-24 14:38:37 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 3.2853
2022-02-24 14:39:11 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 3.2716
2022-02-24 14:39:47 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 3.2629
2022-02-24 14:39:48 - train: epoch 013, train_loss: 3.2510
2022-02-24 14:41:04 - eval: epoch: 013, acc1: 34.854%, acc5: 60.794%, test_loss: 3.0125, per_image_load_time: 1.619ms, per_image_inference_time: 0.204ms
2022-02-24 14:41:04 - until epoch: 013, best_acc1: 34.854%
2022-02-24 14:41:04 - epoch 014 lr: 0.1
2022-02-24 14:41:44 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 3.2083
2022-02-24 14:42:18 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 3.4032
2022-02-24 14:42:52 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.9193
2022-02-24 14:43:25 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 3.0866
2022-02-24 14:44:00 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 3.2273
2022-02-24 14:44:34 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 3.3206
2022-02-24 14:45:08 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 3.2104
2022-02-24 14:45:42 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 3.2674
2022-02-24 14:46:17 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 3.3362
2022-02-24 14:46:51 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 3.2460
2022-02-24 14:47:26 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 3.1972
2022-02-24 14:47:59 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 3.1704
2022-02-24 14:48:34 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 3.2219
2022-02-24 14:49:08 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 3.3874
2022-02-24 14:49:43 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 3.4016
2022-02-24 14:50:17 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 3.0879
2022-02-24 14:50:52 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 3.3357
2022-02-24 14:51:26 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 3.3889
2022-02-24 14:52:01 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 3.0669
2022-02-24 14:52:34 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 3.2336
2022-02-24 14:53:09 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 3.3233
2022-02-24 14:53:44 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 3.2937
2022-02-24 14:54:18 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 3.1979
2022-02-24 14:54:52 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 3.5638
2022-02-24 14:55:27 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 3.4183
2022-02-24 14:56:01 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 3.1858
2022-02-24 14:56:37 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 3.1934
2022-02-24 14:57:10 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 3.3945
2022-02-24 14:57:45 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 3.1514
2022-02-24 14:58:20 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 3.4778
2022-02-24 14:58:54 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 3.1691
2022-02-24 14:59:29 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 3.1687
2022-02-24 15:00:04 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 3.0162
2022-02-24 15:00:38 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 3.1405
2022-02-24 15:01:12 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 3.2742
2022-02-24 15:01:47 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 3.1551
2022-02-24 15:02:22 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 3.1531
2022-02-24 15:02:57 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 3.4689
2022-02-24 15:03:31 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 3.2088
2022-02-24 15:04:06 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 3.3764
2022-02-24 15:04:40 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 3.2175
2022-02-24 15:05:14 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 3.0134
2022-02-24 15:05:49 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 3.2038
2022-02-24 15:06:25 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 3.0409
2022-02-24 15:06:58 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 3.2150
2022-02-24 15:07:34 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 3.1880
2022-02-24 15:08:10 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 3.2110
2022-02-24 15:08:46 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 3.1728
2022-02-24 15:09:20 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 3.0570
2022-02-24 15:09:54 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 3.2012
2022-02-24 15:09:55 - train: epoch 014, train_loss: 3.2358
2022-02-24 15:11:11 - eval: epoch: 014, acc1: 34.828%, acc5: 60.770%, test_loss: 3.0248, per_image_load_time: 2.690ms, per_image_inference_time: 0.212ms
2022-02-24 15:11:11 - until epoch: 014, best_acc1: 34.854%
2022-02-24 15:11:11 - epoch 015 lr: 0.1
2022-02-24 15:11:51 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 3.0824
2022-02-24 15:12:25 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 3.3099
2022-02-24 15:13:00 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 3.3501
2022-02-24 15:13:34 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 3.2805
2022-02-24 15:14:07 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 3.1546
2022-02-24 15:14:41 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 3.4358
2022-02-24 15:15:15 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 3.2832
2022-02-24 15:15:48 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 3.1486
2022-02-24 15:16:23 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 3.1141
2022-02-24 15:16:56 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 3.3295
2022-02-24 15:17:30 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 3.1444
2022-02-24 15:18:03 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 3.2648
2022-02-24 15:18:37 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 3.4287
2022-02-24 15:19:11 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 3.1315
2022-02-24 15:19:46 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 3.0264
2022-02-24 15:20:20 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 3.1152
2022-02-24 15:20:54 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 3.4924
2022-02-24 15:21:27 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 3.0507
2022-02-24 15:22:01 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 3.1690
2022-02-24 15:22:35 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 3.1505
2022-02-24 15:23:10 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 3.0337
2022-02-24 15:23:43 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 3.2640
2022-02-24 15:24:18 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.9639
2022-02-24 15:24:51 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 3.2308
2022-02-24 15:25:26 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 3.2812
2022-02-24 15:25:59 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 3.0727
2022-02-24 15:26:34 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 3.2254
2022-02-24 15:27:08 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 3.4362
2022-02-24 15:27:42 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 3.1864
2022-02-24 15:28:15 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.9591
2022-02-24 15:28:49 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 3.0991
2022-02-24 15:29:24 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 3.1638
2022-02-24 15:29:58 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 3.0751
2022-02-24 15:30:32 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 3.1520
2022-02-24 15:31:06 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 3.4313
2022-02-24 15:31:39 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 3.2005
2022-02-24 15:32:14 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 3.1473
2022-02-24 15:32:48 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 3.1590
2022-02-24 15:33:23 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 3.4065
2022-02-24 15:33:56 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 3.3210
2022-02-24 15:34:30 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 3.5066
2022-02-24 15:35:04 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.8959
2022-02-24 15:35:38 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 3.3310
2022-02-24 15:36:12 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 3.0942
2022-02-24 15:36:47 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 3.2717
2022-02-24 15:37:21 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 3.1525
2022-02-24 15:37:56 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 3.3506
2022-02-24 15:38:31 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 3.2506
2022-02-24 15:39:05 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 3.3349
2022-02-24 15:39:39 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 3.4183
2022-02-24 15:39:40 - train: epoch 015, train_loss: 3.2226
2022-02-24 15:40:56 - eval: epoch: 015, acc1: 35.814%, acc5: 61.738%, test_loss: 2.9619, per_image_load_time: 2.791ms, per_image_inference_time: 0.191ms
2022-02-24 15:40:56 - until epoch: 015, best_acc1: 35.814%
2022-02-24 15:40:56 - epoch 016 lr: 0.1
2022-02-24 15:41:35 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 3.0772
2022-02-24 15:42:09 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.9751
2022-02-24 15:42:42 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 3.2903
2022-02-24 15:43:17 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 3.4396
2022-02-24 15:43:50 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.9699
2022-02-24 15:44:25 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 3.4120
2022-02-24 15:44:58 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 3.0025
2022-02-24 15:45:32 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 3.1165
2022-02-24 15:46:06 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 3.4004
2022-02-24 15:46:40 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 3.1290
2022-02-24 15:47:15 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 3.1278
2022-02-24 15:47:48 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 3.0162
2022-02-24 15:48:23 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 3.2733
2022-02-24 15:48:57 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 3.0348
2022-02-24 15:49:32 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 3.2187
2022-02-24 15:50:06 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 3.3580
2022-02-24 15:50:40 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 3.2399
2022-02-24 15:51:15 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 3.1435
2022-02-24 15:51:48 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 3.2728
2022-02-24 15:52:22 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.7763
2022-02-24 15:52:58 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 3.3766
2022-02-24 15:53:31 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 3.2854
2022-02-24 15:54:06 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 3.3545
2022-02-24 15:54:40 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 3.3951
2022-02-24 15:55:13 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 3.0580
2022-02-24 15:55:48 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 3.2709
2022-02-24 15:56:21 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 3.1556
2022-02-24 15:56:56 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 3.0030
2022-02-24 15:57:30 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 3.1179
2022-02-24 15:58:04 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 3.4920
2022-02-24 15:58:38 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 3.2825
2022-02-24 15:59:13 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 3.2753
2022-02-24 15:59:47 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 3.3691
2022-02-24 16:00:21 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 3.2140
2022-02-24 16:00:55 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 3.1155
2022-02-24 16:01:29 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 3.0974
2022-02-24 16:02:04 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 3.4144
2022-02-24 16:02:38 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 3.5697
2022-02-24 16:03:12 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 3.2357
2022-02-24 16:03:47 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 3.1702
2022-02-24 16:04:20 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 3.1130
2022-02-24 16:04:54 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 3.2588
2022-02-24 16:05:29 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 3.1067
2022-02-24 16:06:04 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 3.1394
2022-02-24 16:06:38 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 3.3026
2022-02-24 16:07:13 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 3.1035
2022-02-24 16:07:47 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 3.3718
2022-02-24 16:08:21 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 3.0444
2022-02-24 16:08:56 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 3.2036
2022-02-24 16:09:30 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 3.3226
2022-02-24 16:09:31 - train: epoch 016, train_loss: 3.2109
2022-02-24 16:10:48 - eval: epoch: 016, acc1: 34.876%, acc5: 61.282%, test_loss: 2.9941, per_image_load_time: 2.415ms, per_image_inference_time: 0.199ms
2022-02-24 16:10:48 - until epoch: 016, best_acc1: 35.814%
2022-02-24 16:10:48 - epoch 017 lr: 0.1
2022-02-24 16:11:28 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 3.0851
2022-02-24 16:12:02 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 3.3625
2022-02-24 16:12:36 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 3.3536
2022-02-24 16:13:09 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.8580
2022-02-24 16:13:44 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 3.1211
2022-02-24 16:14:17 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 3.5123
2022-02-24 16:14:51 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 3.2642
2022-02-24 16:15:25 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 3.1660
2022-02-24 16:16:00 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 3.1172
2022-02-24 16:16:33 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 3.1296
2022-02-24 16:17:08 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 3.5668
2022-02-24 16:17:41 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 3.4333
2022-02-24 16:18:15 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 3.1777
2022-02-24 16:18:49 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 3.3261
2022-02-24 16:19:23 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.9473
2022-02-24 16:19:58 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 3.1624
2022-02-24 16:20:33 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 3.1133
2022-02-24 16:21:06 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 3.2643
2022-02-24 16:21:40 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 3.0810
2022-02-24 16:22:15 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 3.2487
2022-02-24 16:22:49 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 3.1935
2022-02-24 16:23:22 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.9781
2022-02-24 16:23:57 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 3.1286
2022-02-24 16:24:31 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 3.0677
2022-02-24 16:25:05 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 3.4370
2022-02-24 16:25:39 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 3.1249
2022-02-24 16:26:13 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 3.1851
2022-02-24 16:26:48 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 3.4304
2022-02-24 16:27:22 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 3.4153
2022-02-24 16:27:56 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.9983
2022-02-24 16:28:30 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 3.3478
2022-02-24 16:29:04 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 3.0336
2022-02-24 16:29:39 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 3.2087
2022-02-24 16:30:13 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 3.0712
2022-02-24 16:30:46 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 3.3007
2022-02-24 16:31:21 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 3.4973
2022-02-24 16:31:55 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 3.1517
2022-02-24 16:32:29 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 3.4928
2022-02-24 16:33:04 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 3.1474
2022-02-24 16:33:38 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 3.0525
2022-02-24 16:34:12 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 3.2093
2022-02-24 16:34:48 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 3.1062
2022-02-24 16:35:21 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 3.1706
2022-02-24 16:35:55 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 3.2781
2022-02-24 16:36:29 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 3.2484
2022-02-24 16:37:04 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 3.2328
2022-02-24 16:37:39 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 3.3459
2022-02-24 16:38:13 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 3.2732
2022-02-24 16:38:48 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 3.0875
2022-02-24 16:39:21 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 3.0114
2022-02-24 16:39:22 - train: epoch 017, train_loss: 3.1994
2022-02-24 16:40:38 - eval: epoch: 017, acc1: 35.886%, acc5: 61.746%, test_loss: 2.9635, per_image_load_time: 2.665ms, per_image_inference_time: 0.204ms
2022-02-24 16:40:38 - until epoch: 017, best_acc1: 35.886%
2022-02-24 16:40:38 - epoch 018 lr: 0.1
2022-02-24 16:41:17 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 3.1325
2022-02-24 16:41:52 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 3.2039
2022-02-24 16:42:26 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 3.4051
2022-02-24 16:43:00 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 3.2904
2022-02-24 16:43:34 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 3.1645
2022-02-24 16:44:08 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 3.3803
2022-02-24 16:44:42 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.9728
2022-02-24 16:45:16 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 3.2002
2022-02-24 16:45:51 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 3.2221
2022-02-24 16:46:24 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 3.1933
2022-02-24 16:46:59 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 3.4996
2022-02-24 16:47:32 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 3.1797
2022-02-24 16:48:07 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 3.3888
2022-02-24 16:48:41 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 3.2861
2022-02-24 16:49:14 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 3.4363
2022-02-24 16:49:49 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 3.1513
2022-02-24 16:50:23 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.9549
2022-02-24 16:50:58 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 3.0393
2022-02-24 16:51:33 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 3.2555
2022-02-24 16:52:06 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 3.5464
2022-02-24 16:52:42 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 3.3983
2022-02-24 16:53:16 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 3.1379
2022-02-24 16:53:51 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 3.3670
2022-02-24 16:54:25 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 3.1560
2022-02-24 16:55:00 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.9201
2022-02-24 16:55:34 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 3.0595
2022-02-24 16:56:09 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 3.3046
2022-02-24 16:56:43 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.9593
2022-02-24 16:57:16 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 3.3392
2022-02-24 16:57:52 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.9944
2022-02-24 16:58:26 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 3.4645
2022-02-24 16:59:01 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 3.0184
2022-02-24 16:59:35 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 3.1284
2022-02-24 17:00:11 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 3.0919
2022-02-24 17:00:44 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 3.2802
2022-02-24 17:01:20 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 3.3191
2022-02-24 17:01:54 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 3.5522
2022-02-24 17:02:29 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 3.2317
2022-02-24 17:03:02 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 3.2518
2022-02-24 17:03:37 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 3.1472
2022-02-24 17:04:12 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.9985
2022-02-24 17:04:47 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 3.2557
2022-02-24 17:05:21 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.8654
2022-02-24 17:05:56 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 3.3727
2022-02-24 17:06:30 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 3.1250
2022-02-24 17:07:07 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 3.0042
2022-02-24 17:07:39 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 3.3170
2022-02-24 17:08:16 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 3.2854
2022-02-24 17:08:49 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 3.2488
2022-02-24 17:09:24 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 3.1596
2022-02-24 17:09:25 - train: epoch 018, train_loss: 3.1872
2022-02-24 17:10:42 - eval: epoch: 018, acc1: 35.750%, acc5: 61.772%, test_loss: 2.9600, per_image_load_time: 1.690ms, per_image_inference_time: 0.207ms
2022-02-24 17:10:42 - until epoch: 018, best_acc1: 35.886%
2022-02-24 17:10:42 - epoch 019 lr: 0.1
2022-02-24 17:11:23 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 3.0561
2022-02-24 17:11:57 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 3.4190
2022-02-24 17:12:32 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 3.4256
2022-02-24 17:13:05 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 3.1474
2022-02-24 17:13:40 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 3.1328
2022-02-24 17:14:15 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 3.1897
2022-02-24 17:14:49 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.9269
2022-02-24 17:15:23 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 3.3342
2022-02-24 17:15:59 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 3.1868
2022-02-24 17:16:32 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 3.3239
2022-02-24 17:17:08 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.8529
2022-02-24 17:17:41 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 3.1968
2022-02-24 17:18:17 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 3.3336
2022-02-24 17:18:50 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 3.2039
2022-02-24 17:19:26 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 3.5283
2022-02-24 17:20:00 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 3.0227
2022-02-24 17:20:34 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 3.2716
2022-02-24 17:21:09 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 3.0268
2022-02-24 17:21:43 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 3.4469
2022-02-24 17:22:19 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 3.2760
2022-02-24 17:22:52 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 3.2134
2022-02-24 17:23:27 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 3.0606
2022-02-24 17:24:02 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 3.1558
2022-02-24 17:24:36 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 3.3019
2022-02-24 17:25:10 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 3.0407
2022-02-24 17:25:46 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 3.2977
2022-02-24 17:26:20 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 3.3706
2022-02-24 17:26:55 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 3.1663
2022-02-24 17:27:29 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 3.1194
2022-02-24 17:28:04 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 3.4158
2022-02-24 17:28:39 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 3.2513
2022-02-24 17:29:12 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.8184
2022-02-24 17:29:47 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 3.1829
2022-02-24 17:30:20 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 3.2819
2022-02-24 17:30:55 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 3.3040
2022-02-24 17:31:29 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 3.0675
2022-02-24 17:32:04 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 3.3137
2022-02-24 17:32:38 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 3.3555
2022-02-24 17:33:13 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 3.1114
2022-02-24 17:33:48 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 3.1365
2022-02-24 17:34:22 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 3.2421
2022-02-24 17:34:57 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 3.2366
2022-02-24 17:35:32 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 3.2373
2022-02-24 17:36:06 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 3.3122
2022-02-24 17:36:40 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 3.4171
2022-02-24 17:37:14 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 3.0552
2022-02-24 17:37:50 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 3.1807
2022-02-24 17:38:25 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 3.1883
2022-02-24 17:39:01 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 3.0321
2022-02-24 17:39:34 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 3.2021
2022-02-24 17:39:34 - train: epoch 019, train_loss: 3.1851
2022-02-24 17:40:54 - eval: epoch: 019, acc1: 35.688%, acc5: 62.186%, test_loss: 2.9504, per_image_load_time: 2.911ms, per_image_inference_time: 0.167ms
2022-02-24 17:40:54 - until epoch: 019, best_acc1: 35.886%
2022-02-24 17:40:54 - epoch 020 lr: 0.1
2022-02-24 17:41:34 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 3.3966
2022-02-24 17:42:08 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.9684
2022-02-24 17:42:44 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 3.3204
2022-02-24 17:43:17 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.8784
2022-02-24 17:43:53 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 3.1482
2022-02-24 17:44:27 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 3.4524
2022-02-24 17:45:01 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 3.1055
2022-02-24 17:45:37 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 3.1830
2022-02-24 17:46:11 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 3.3721
2022-02-24 17:46:45 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 3.1078
2022-02-24 17:47:20 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 3.0252
2022-02-24 17:47:55 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 3.0119
2022-02-24 17:48:30 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 3.0054
2022-02-24 17:49:06 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 3.2493
2022-02-24 17:49:39 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 3.1168
2022-02-24 17:50:13 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.9490
2022-02-24 17:50:47 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.9602
2022-02-24 17:51:23 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 3.0656
2022-02-24 17:51:57 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 3.0055
2022-02-24 17:52:32 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 3.1215
2022-02-24 17:53:06 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 3.3342
2022-02-24 17:53:40 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 3.0302
2022-02-24 17:54:15 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 3.0843
2022-02-24 17:54:49 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 3.2967
2022-02-24 17:55:25 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 3.1424
2022-02-24 17:55:58 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.8977
2022-02-24 17:56:33 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 3.1632
2022-02-24 17:57:08 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 3.3530
2022-02-24 17:57:41 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 3.1745
2022-02-24 17:58:17 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 3.0570
2022-02-24 17:58:51 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 3.2651
2022-02-24 17:59:25 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 3.4896
2022-02-24 18:00:00 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 3.1425
2022-02-24 18:00:34 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 3.1300
2022-02-24 18:01:10 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 3.0783
2022-02-24 18:01:44 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 3.1676
2022-02-24 18:02:18 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 3.1743
2022-02-24 18:02:53 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 3.1468
2022-02-24 18:03:28 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 3.1485
2022-02-24 18:04:04 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 3.0914
2022-02-24 18:04:37 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 3.0222
2022-02-24 18:05:12 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 3.0907
2022-02-24 18:05:46 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 3.1971
2022-02-24 18:06:21 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 3.2069
2022-02-24 18:06:55 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 3.2558
2022-02-24 18:07:29 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 3.3549
2022-02-24 18:08:04 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 3.0025
2022-02-24 18:08:40 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 3.3312
2022-02-24 18:09:14 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 3.1553
2022-02-24 18:09:48 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 3.0763
2022-02-24 18:09:49 - train: epoch 020, train_loss: 3.1722
2022-02-24 18:11:08 - eval: epoch: 020, acc1: 35.784%, acc5: 62.014%, test_loss: 2.9618, per_image_load_time: 2.676ms, per_image_inference_time: 0.173ms
2022-02-24 18:11:08 - until epoch: 020, best_acc1: 35.886%
2022-02-24 18:11:08 - epoch 021 lr: 0.1
2022-02-24 18:11:47 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 3.0297
2022-02-24 18:12:22 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 3.1630
2022-02-24 18:12:56 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.8451
2022-02-24 18:13:31 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 3.0684
2022-02-24 18:14:05 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.8256
2022-02-24 18:14:41 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 3.0394
2022-02-24 18:15:14 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 3.0117
2022-02-24 18:15:50 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 3.3244
2022-02-24 18:16:26 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 3.2443
2022-02-24 18:16:59 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.9298
2022-02-24 18:17:35 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 3.1261
2022-02-24 18:18:09 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 3.0965
2022-02-24 18:18:44 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 3.1700
2022-02-24 18:19:19 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 3.0900
2022-02-24 18:19:53 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.9647
2022-02-24 18:20:29 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 3.1900
2022-02-24 18:21:02 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 3.1623
2022-02-24 18:21:37 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.9531
2022-02-24 18:22:12 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 3.2401
2022-02-24 18:22:47 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 3.2773
2022-02-24 18:23:22 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.9976
2022-02-24 18:23:56 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 3.3012
2022-02-24 18:24:31 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 3.0966
2022-02-24 18:25:06 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 3.0228
2022-02-24 18:25:40 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 3.1138
2022-02-24 18:26:16 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 3.3166
2022-02-24 18:26:51 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 3.1586
2022-02-24 18:27:26 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 3.2815
2022-02-24 18:28:00 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 3.2393
2022-02-24 18:28:35 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 3.3627
2022-02-24 18:29:09 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 3.2381
2022-02-24 18:29:45 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 3.3231
2022-02-24 18:30:19 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 3.3912
2022-02-24 18:30:54 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 3.2491
2022-02-24 18:31:29 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 3.0133
2022-02-24 18:32:03 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 3.1219
2022-02-24 18:32:38 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 3.2361
2022-02-24 18:33:13 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.9146
2022-02-24 18:33:47 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 3.0216
2022-02-24 18:34:22 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 3.3819
2022-02-24 18:34:57 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 3.0950
2022-02-24 18:35:31 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 3.1514
2022-02-24 18:36:07 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 3.2664
2022-02-24 18:36:40 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 3.2731
2022-02-24 18:37:16 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 3.2413
2022-02-24 18:37:49 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 3.1006
2022-02-24 18:38:25 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 3.2282
2022-02-24 18:39:00 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 3.2397
2022-02-24 18:39:36 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.9626
2022-02-24 18:40:09 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 3.0535
2022-02-24 18:40:10 - train: epoch 021, train_loss: 3.1658
2022-02-24 18:41:28 - eval: epoch: 021, acc1: 36.038%, acc5: 62.326%, test_loss: 2.9362, per_image_load_time: 1.402ms, per_image_inference_time: 0.181ms
2022-02-24 18:41:28 - until epoch: 021, best_acc1: 36.038%
2022-02-24 18:41:28 - epoch 022 lr: 0.1
2022-02-24 18:42:09 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.9564
2022-02-24 18:42:43 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 3.0218
2022-02-24 18:43:17 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.8050
2022-02-24 18:43:53 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 3.1438
2022-02-24 18:44:27 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 3.1167
2022-02-24 18:45:02 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 3.1287
2022-02-24 18:45:36 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 3.2106
2022-02-24 18:46:11 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 3.2346
2022-02-24 18:46:45 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 3.2433
2022-02-24 18:47:20 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 3.1449
2022-02-24 18:47:54 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 3.3190
2022-02-24 18:48:29 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.8480
2022-02-24 18:49:03 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 3.2492
2022-02-24 18:49:39 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 3.2079
2022-02-24 18:50:13 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 3.1480
2022-02-24 18:50:48 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 3.0222
2022-02-24 18:51:21 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.9204
2022-02-24 18:51:57 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 3.4082
2022-02-24 18:52:31 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.9216
2022-02-24 18:53:06 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 3.2038
2022-02-24 18:53:39 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 3.0773
2022-02-24 18:54:14 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.9317
2022-02-24 18:54:47 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 3.3349
2022-02-24 18:55:22 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 3.0929
2022-02-24 18:55:55 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 3.1323
2022-02-24 18:56:30 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.8901
2022-02-24 18:57:03 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.9192
2022-02-24 18:57:38 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 3.5515
2022-02-24 18:58:11 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 3.0047
2022-02-24 18:58:46 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 3.2034
2022-02-24 18:59:19 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 3.3469
2022-02-24 18:59:54 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 3.1237
2022-02-24 19:00:28 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 3.2676
2022-02-24 19:01:04 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 3.0091
2022-02-24 19:01:37 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 3.3792
2022-02-24 19:02:12 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 3.2777
2022-02-24 19:02:47 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 3.1883
2022-02-24 19:03:22 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 3.2232
2022-02-24 19:03:56 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 3.1114
2022-02-24 19:04:32 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 3.2170
2022-02-24 19:05:05 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 3.0146
2022-02-24 19:05:40 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 3.1805
2022-02-24 19:06:15 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 3.2139
2022-02-24 19:06:49 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 3.2231
2022-02-24 19:07:24 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 3.2007
2022-02-24 19:07:59 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 3.3254
2022-02-24 19:08:32 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 3.3326
2022-02-24 19:09:09 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.8852
2022-02-24 19:09:43 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 3.0394
2022-02-24 19:10:17 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 3.0215
2022-02-24 19:10:18 - train: epoch 022, train_loss: 3.1618
2022-02-24 19:11:35 - eval: epoch: 022, acc1: 35.488%, acc5: 61.728%, test_loss: 2.9694, per_image_load_time: 1.923ms, per_image_inference_time: 0.191ms
2022-02-24 19:11:35 - until epoch: 022, best_acc1: 36.038%
2022-02-24 19:11:35 - epoch 023 lr: 0.1
2022-02-24 19:12:15 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 3.1583
2022-02-24 19:12:49 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.8480
2022-02-24 19:13:24 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 3.0993
2022-02-24 19:13:59 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 3.2995
2022-02-24 19:14:33 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 3.0982
2022-02-24 19:15:08 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 3.1967
2022-02-24 19:15:43 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 3.0466
2022-02-24 19:16:17 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 3.1033
2022-02-24 19:16:51 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 3.1269
2022-02-24 19:17:26 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.9928
2022-02-24 19:18:02 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 3.2197
2022-02-24 19:18:36 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.9549
2022-02-24 19:19:11 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 3.0215
2022-02-24 19:19:44 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 3.1686
2022-02-24 19:20:20 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 3.0630
2022-02-24 19:20:54 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 3.0710
2022-02-24 19:21:29 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 3.4507
2022-02-24 19:22:03 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 3.1152
2022-02-24 19:22:38 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 3.2443
2022-02-24 19:23:12 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.7348
2022-02-24 19:23:47 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 3.2184
2022-02-24 19:24:22 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.8663
2022-02-24 19:24:57 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.9884
2022-02-24 19:25:31 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 3.1268
2022-02-24 19:26:05 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 3.2714
2022-02-24 19:26:41 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 3.0749
2022-02-24 19:27:16 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 3.0988
2022-02-24 19:27:50 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 3.2910
2022-02-24 19:28:25 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 3.2946
2022-02-24 19:29:01 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 3.2060
2022-02-24 19:29:35 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 3.1669
2022-02-24 19:30:09 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 3.2639
2022-02-24 19:30:43 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 3.3473
2022-02-24 19:31:19 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 3.4316
2022-02-24 19:31:53 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 3.1448
2022-02-24 19:32:28 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.9849
2022-02-24 19:33:02 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 3.0666
2022-02-24 19:33:36 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 3.1241
2022-02-24 19:34:11 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 3.1307
2022-02-24 19:34:46 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 3.1346
2022-02-24 19:35:21 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 3.0823
2022-02-24 19:35:56 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 3.0271
2022-02-24 19:36:30 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.9000
2022-02-24 19:37:05 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.9735
2022-02-24 19:37:40 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 3.1237
2022-02-24 19:38:15 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 3.0855
2022-02-24 19:38:49 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.9555
2022-02-24 19:39:25 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 3.0339
2022-02-24 19:40:00 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 3.0666
2022-02-24 19:40:34 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 3.1545
2022-02-24 19:40:34 - train: epoch 023, train_loss: 3.1545
2022-02-24 19:41:52 - eval: epoch: 023, acc1: 36.102%, acc5: 62.110%, test_loss: 2.9353, per_image_load_time: 1.993ms, per_image_inference_time: 0.179ms
2022-02-24 19:41:52 - until epoch: 023, best_acc1: 36.102%
2022-02-24 19:41:52 - epoch 024 lr: 0.1
2022-02-24 19:42:33 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 3.1900
2022-02-24 19:43:07 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 3.2135
2022-02-24 19:43:42 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 3.1421
2022-02-24 19:44:15 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 3.1002
2022-02-24 19:44:50 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 3.1579
2022-02-24 19:45:24 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.8717
2022-02-24 19:45:59 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 3.2025
2022-02-24 19:46:34 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 3.0055
2022-02-24 19:47:09 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 3.1092
2022-02-24 19:47:42 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 3.2225
2022-02-24 19:48:17 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.8804
2022-02-24 19:48:51 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 3.0620
2022-02-24 19:49:26 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 3.4194
2022-02-24 19:50:01 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.9443
2022-02-24 19:50:35 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 3.3667
2022-02-24 19:51:10 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 3.0937
2022-02-24 19:51:44 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 3.0761
2022-02-24 19:52:19 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 3.2654
2022-02-24 19:52:54 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.8820
2022-02-24 19:53:28 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 3.1983
2022-02-24 19:54:03 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 3.0789
2022-02-24 19:54:37 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.9415
2022-02-24 19:55:13 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 3.1091
2022-02-24 19:55:47 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 3.0802
2022-02-24 19:56:22 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 3.0365
2022-02-24 19:56:56 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 3.0548
2022-02-24 19:57:31 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 3.2218
2022-02-24 19:58:05 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 3.2749
2022-02-24 19:58:40 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 3.2044
2022-02-24 19:59:13 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 3.0968
2022-02-24 19:59:49 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 3.1272
2022-02-24 20:00:22 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 3.4054
2022-02-24 20:00:58 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.9072
2022-02-24 20:01:32 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 3.2680
2022-02-24 20:02:07 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 3.0217
2022-02-24 20:02:41 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 3.1307
2022-02-24 20:03:16 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 3.1633
2022-02-24 20:03:51 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 3.4262
2022-02-24 20:04:25 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 3.2715
2022-02-24 20:04:59 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 3.0645
2022-02-24 20:05:34 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 3.0315
2022-02-24 20:06:08 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 3.1025
2022-02-24 20:06:43 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.9807
2022-02-24 20:07:17 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 3.1959
2022-02-24 20:07:51 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.8402
2022-02-24 20:08:25 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 3.1771
2022-02-24 20:09:00 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 3.0299
2022-02-24 20:09:35 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 3.1138
2022-02-24 20:10:11 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 3.1039
2022-02-24 20:10:44 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 3.2951
2022-02-24 20:10:45 - train: epoch 024, train_loss: 3.1498
2022-02-24 20:12:00 - eval: epoch: 024, acc1: 35.848%, acc5: 62.298%, test_loss: 2.9404, per_image_load_time: 2.622ms, per_image_inference_time: 0.232ms
2022-02-24 20:12:00 - until epoch: 024, best_acc1: 36.102%
2022-02-24 20:12:00 - epoch 025 lr: 0.1
2022-02-24 20:12:39 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 3.0883
2022-02-24 20:13:14 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.8795
2022-02-24 20:13:49 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.9453
2022-02-24 20:14:23 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 3.2100
2022-02-24 20:14:57 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 3.0575
2022-02-24 20:15:33 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 3.1889
2022-02-24 20:16:07 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 3.1598
2022-02-24 20:16:42 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 3.2127
2022-02-24 20:17:16 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 3.0213
2022-02-24 20:17:51 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 3.0528
2022-02-24 20:18:25 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 3.1724
2022-02-24 20:19:01 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 3.1548
2022-02-24 20:19:35 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 3.0653
2022-02-24 20:20:09 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 3.1192
2022-02-24 20:20:45 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 3.0124
2022-02-24 20:21:19 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.9109
2022-02-24 20:21:54 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 3.1994
2022-02-24 20:22:27 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.9759
2022-02-24 20:23:03 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 3.0135
2022-02-24 20:23:37 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 3.1750
2022-02-24 20:24:12 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.8714
2022-02-24 20:24:46 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 3.0281
2022-02-24 20:25:21 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 3.1493
2022-02-24 20:25:56 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.9792
2022-02-24 20:26:31 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 3.2727
2022-02-24 20:27:06 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 3.0796
2022-02-24 20:27:41 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 3.1474
2022-02-24 20:28:13 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 3.0565
2022-02-24 20:28:49 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 3.2332
2022-02-24 20:29:22 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 3.3996
2022-02-24 20:29:57 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.9644
2022-02-24 20:30:31 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 3.1931
2022-02-24 20:31:05 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 3.3199
2022-02-24 20:31:39 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 3.1936
2022-02-24 20:32:14 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.9044
2022-02-24 20:32:48 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 3.1673
2022-02-24 20:33:22 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 3.1722
2022-02-24 20:33:56 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 3.2989
2022-02-24 20:34:30 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 3.2781
2022-02-24 20:35:04 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 3.2417
2022-02-24 20:35:40 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 3.3293
2022-02-24 20:36:14 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 3.2339
2022-02-24 20:36:48 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 3.0732
2022-02-24 20:37:23 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.9268
2022-02-24 20:37:57 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 3.0662
2022-02-24 20:38:32 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.9619
2022-02-24 20:39:07 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 3.0424
2022-02-24 20:39:42 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.9572
2022-02-24 20:40:16 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 3.1091
2022-02-24 20:40:50 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 3.1373
2022-02-24 20:40:51 - train: epoch 025, train_loss: 3.1448
2022-02-24 20:42:08 - eval: epoch: 025, acc1: 36.184%, acc5: 62.392%, test_loss: 2.9314, per_image_load_time: 2.806ms, per_image_inference_time: 0.204ms
2022-02-24 20:42:08 - until epoch: 025, best_acc1: 36.184%
2022-02-24 20:42:08 - epoch 026 lr: 0.1
2022-02-24 20:42:48 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 3.0991
2022-02-24 20:43:22 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 3.0203
2022-02-24 20:43:56 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.9388
2022-02-24 20:44:28 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 3.1885
2022-02-24 20:45:03 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 3.1823
2022-02-24 20:45:36 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 3.2255
2022-02-24 20:46:10 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 3.0863
2022-02-24 20:46:44 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.9888
2022-02-24 20:47:19 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 3.3465
2022-02-24 20:47:53 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 3.0688
2022-02-24 20:48:27 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 3.2326
2022-02-24 20:49:02 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 3.0289
2022-02-24 20:49:35 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 3.0942
2022-02-24 20:50:09 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 3.1359
2022-02-24 20:50:44 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 3.0272
2022-02-24 20:51:19 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 3.3232
2022-02-24 20:51:52 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 3.1095
2022-02-24 20:52:26 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 3.3526
2022-02-24 20:53:01 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 3.4955
2022-02-24 20:53:34 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 3.4285
2022-02-24 20:54:09 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 3.1262
2022-02-24 20:54:42 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 3.1948
2022-02-24 20:55:17 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 3.0568
2022-02-24 20:55:52 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 3.3373
2022-02-24 20:56:25 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 3.3726
2022-02-24 20:57:00 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 3.0323
2022-02-24 20:57:33 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 3.1352
2022-02-24 20:58:07 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.9650
2022-02-24 20:58:42 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 3.3846
2022-02-24 20:59:16 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 3.0667
2022-02-24 20:59:51 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 3.0745
2022-02-24 21:00:25 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 3.2500
2022-02-24 21:00:59 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 3.0990
2022-02-24 21:01:33 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 3.1850
2022-02-24 21:02:08 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 3.1026
2022-02-24 21:02:42 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 3.0771
2022-02-24 21:03:16 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 3.2011
2022-02-24 21:03:51 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 3.3256
2022-02-24 21:04:25 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 3.3079
2022-02-24 21:05:00 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 3.4021
2022-02-24 21:05:33 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 3.1155
2022-02-24 21:06:08 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 3.1829
2022-02-24 21:06:41 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 3.2273
2022-02-24 21:07:17 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 3.2165
2022-02-24 21:07:50 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 3.3991
2022-02-24 21:08:26 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 3.1632
2022-02-24 21:09:00 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.9230
2022-02-24 21:09:35 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 3.0013
2022-02-24 21:10:10 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 3.2394
2022-02-24 21:10:43 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 3.2516
2022-02-24 21:10:44 - train: epoch 026, train_loss: 3.1400
2022-02-24 21:12:00 - eval: epoch: 026, acc1: 36.986%, acc5: 63.194%, test_loss: 2.8762, per_image_load_time: 2.747ms, per_image_inference_time: 0.200ms
2022-02-24 21:12:00 - until epoch: 026, best_acc1: 36.986%
2022-02-24 21:12:00 - epoch 027 lr: 0.1
2022-02-24 21:12:40 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 3.3696
2022-02-24 21:13:14 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 3.0229
2022-02-24 21:13:47 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 3.2175
2022-02-24 21:14:22 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 3.3542
2022-02-24 21:14:55 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 3.2308
2022-02-24 21:15:29 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 3.3452
2022-02-24 21:16:03 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 3.4207
2022-02-24 21:16:37 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 3.1666
2022-02-24 21:17:10 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 3.0707
2022-02-24 21:17:46 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 3.2664
2022-02-24 21:18:19 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.9747
2022-02-24 21:18:54 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 3.3668
2022-02-24 21:19:28 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 3.1847
2022-02-24 21:20:03 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 3.4115
2022-02-24 21:20:37 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 3.0729
2022-02-24 21:21:11 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 3.2621
2022-02-24 21:21:46 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 3.0513
2022-02-24 21:22:20 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 3.1596
2022-02-24 21:22:55 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 3.2188
2022-02-24 21:23:29 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 3.1089
2022-02-24 21:24:04 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 3.3749
2022-02-24 21:24:39 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 3.3365
2022-02-24 21:25:13 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 3.4461
2022-02-24 21:25:49 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.9539
2022-02-24 21:26:23 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 3.1607
2022-02-24 21:26:57 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 3.3479
2022-02-24 21:27:31 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 3.1191
2022-02-24 21:28:06 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 3.1570
2022-02-24 21:28:40 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 3.2028
2022-02-24 21:29:15 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.9439
2022-02-24 21:29:49 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.8795
2022-02-24 21:30:23 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.9501
2022-02-24 21:30:58 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 3.1632
2022-02-24 21:31:33 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 3.0852
2022-02-24 21:32:07 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 3.3863
2022-02-24 21:32:42 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 3.1861
2022-02-24 21:33:16 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.9763
2022-02-24 21:33:51 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.9662
2022-02-24 21:34:25 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 3.0206
2022-02-24 21:34:59 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 3.3672
2022-02-24 21:35:34 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 3.1544
2022-02-24 21:36:09 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 3.1562
2022-02-24 21:36:43 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 3.1379
2022-02-24 21:37:19 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 3.0084
2022-02-24 21:37:53 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 3.1606
2022-02-24 21:38:28 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 3.0272
2022-02-24 21:39:03 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 3.1752
2022-02-24 21:39:38 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 3.2849
2022-02-24 21:40:14 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 3.1268
2022-02-24 21:40:47 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.8114
2022-02-24 21:40:48 - train: epoch 027, train_loss: 3.1351
2022-02-24 21:42:05 - eval: epoch: 027, acc1: 37.248%, acc5: 63.652%, test_loss: 2.8541, per_image_load_time: 2.741ms, per_image_inference_time: 0.217ms
2022-02-24 21:42:05 - until epoch: 027, best_acc1: 37.248%
2022-02-24 22:53:32 - epoch 028 lr: 0.1
2022-02-24 22:54:11 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 3.0741
2022-02-24 22:54:47 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 3.1178
2022-02-24 22:55:20 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 3.1113
2022-02-24 22:55:55 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 3.0465
2022-02-24 22:56:30 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.9949
2022-02-24 22:57:03 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 3.1532
2022-02-24 22:57:38 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 3.3834
2022-02-24 22:58:12 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 3.0953
2022-02-24 22:58:46 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.9277
2022-02-24 22:59:20 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 3.1197
2022-02-24 22:59:54 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 3.1060
2022-02-24 23:00:29 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 3.1366
2022-02-24 23:01:02 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 3.0907
2022-02-24 23:01:37 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 3.4139
2022-02-24 23:02:11 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 3.0271
2022-02-24 23:02:46 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 3.1096
2022-02-24 23:03:19 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 3.2010
2022-02-24 23:03:54 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 3.1672
2022-02-24 23:04:28 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 3.1127
2022-02-24 23:05:03 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 3.1627
2022-02-24 23:05:37 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 3.3082
2022-02-24 23:06:11 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 3.2637
2022-02-24 23:06:45 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 3.4130
2022-02-24 23:07:20 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 3.4135
2022-02-24 23:07:54 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 3.2023
2022-02-24 23:08:29 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 3.1652
2022-02-24 23:09:02 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 3.1349
2022-02-24 23:09:37 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 3.1916
2022-02-24 23:10:10 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 3.0015
2022-02-24 23:10:46 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 3.1253
2022-02-24 23:11:19 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 3.3491
2022-02-24 23:11:53 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.9545
2022-02-24 23:12:27 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 3.1041
2022-02-24 23:13:02 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 3.2313
2022-02-24 23:13:36 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 3.0181
2022-02-24 23:14:11 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 3.1629
2022-02-24 23:14:45 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 3.3400
2022-02-24 23:15:20 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.7847
2022-02-24 23:15:54 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 3.3348
2022-02-24 23:16:29 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 3.2278
2022-02-24 23:17:02 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 3.1767
2022-02-24 23:17:37 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 3.3433
2022-02-24 23:18:12 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 3.0251
2022-02-24 23:18:47 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 3.0624
2022-02-24 23:19:21 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 3.2642
2022-02-24 23:19:56 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 3.1265
2022-02-24 23:20:30 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 3.1615
2022-02-24 23:21:07 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 3.1088
2022-02-24 23:21:41 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 3.0540
2022-02-24 23:22:16 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.9107
2022-02-24 23:22:16 - train: epoch 028, train_loss: 3.1300
2022-02-24 23:23:35 - eval: epoch: 028, acc1: 37.318%, acc5: 64.102%, test_loss: 2.8417, per_image_load_time: 2.875ms, per_image_inference_time: 0.187ms
2022-02-24 23:23:35 - until epoch: 028, best_acc1: 37.318%
2022-02-24 23:23:35 - epoch 029 lr: 0.1
2022-02-24 23:24:15 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 3.0713
2022-02-24 23:24:49 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 3.0002
2022-02-24 23:25:24 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 3.0772
2022-02-24 23:25:58 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 3.3286
2022-02-24 23:26:33 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 3.1163
2022-02-24 23:27:07 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 3.1004
2022-02-24 23:27:42 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 2.9924
2022-02-24 23:28:16 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 3.0345
2022-02-24 23:28:50 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 3.1058
2022-02-24 23:29:25 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 3.2151
2022-02-24 23:29:59 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.8895
2022-02-24 23:30:33 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 3.1867
2022-02-24 23:31:08 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 3.1330
2022-02-24 23:31:42 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 3.0682
2022-02-24 23:32:16 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 3.2193
2022-02-24 23:32:51 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 3.4331
2022-02-24 23:33:24 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 3.0424
2022-02-24 23:33:59 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 3.3204
2022-02-24 23:34:34 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.9141
2022-02-24 23:35:07 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 3.1174
2022-02-24 23:35:42 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 3.0182
2022-02-24 23:36:15 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 3.0968
2022-02-24 23:36:50 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.9781
2022-02-24 23:37:24 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 3.0538
2022-02-24 23:37:58 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 3.0386
2022-02-24 23:38:32 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 3.2726
2022-02-24 23:39:06 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.9591
2022-02-24 23:39:40 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 3.1469
2022-02-24 23:40:16 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.8881
2022-02-24 23:40:50 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.9767
2022-02-24 23:41:24 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 3.1015
2022-02-24 23:41:59 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 3.1072
2022-02-24 23:42:34 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 3.0409
2022-02-24 23:43:08 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.9190
2022-02-24 23:43:42 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 3.5451
2022-02-24 23:44:16 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.8705
2022-02-24 23:44:51 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.9319
2022-02-24 23:45:25 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.8167
2022-02-24 23:46:00 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 3.0056
2022-02-24 23:46:34 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 3.1993
2022-02-24 23:47:09 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 3.1140
2022-02-24 23:47:43 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 3.1214
2022-02-24 23:48:19 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 3.1152
2022-02-24 23:48:53 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 3.1899
2022-02-24 23:49:28 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 3.2253
2022-02-24 23:50:03 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 3.1533
2022-02-24 23:50:38 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 3.0577
2022-02-24 23:51:13 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 3.1603
2022-02-24 23:51:49 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 3.3303
2022-02-24 23:52:24 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.8111
2022-02-24 23:52:24 - train: epoch 029, train_loss: 3.1250
2022-02-24 23:53:42 - eval: epoch: 029, acc1: 36.782%, acc5: 62.710%, test_loss: 2.9046, per_image_load_time: 2.859ms, per_image_inference_time: 0.180ms
2022-02-24 23:53:42 - until epoch: 029, best_acc1: 37.318%
2022-02-24 23:53:42 - epoch 030 lr: 0.1
2022-02-24 23:54:22 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 3.1113
2022-02-24 23:54:57 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 3.2010
2022-02-24 23:55:31 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 3.1074
2022-02-24 23:56:04 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 3.0133
2022-02-24 23:56:38 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 3.0444
2022-02-24 23:57:12 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 3.1271
2022-02-24 23:57:46 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 3.1756
2022-02-24 23:58:20 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 3.0486
2022-02-24 23:58:54 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.9247
2022-02-24 23:59:29 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.8655
2022-02-25 00:00:04 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 3.0445
2022-02-25 00:00:37 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 3.1216
2022-02-25 00:01:12 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.9900
2022-02-25 00:01:47 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 3.0698
2022-02-25 00:02:21 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.9854
2022-02-25 00:02:56 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 3.1828
2022-02-25 00:03:30 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 3.3994
2022-02-25 00:04:05 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 3.1636
2022-02-25 00:04:40 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 3.0735
2022-02-25 00:05:14 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 3.0578
2022-02-25 00:05:49 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 3.3233
2022-02-25 00:06:23 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 3.0341
2022-02-25 00:06:59 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 3.1218
2022-02-25 00:07:33 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 3.0974
2022-02-25 00:08:07 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 3.3065
2022-02-25 00:08:41 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 3.0853
2022-02-25 00:09:16 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 3.0813
2022-02-25 00:09:51 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 3.0486
2022-02-25 00:10:25 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.8668
2022-02-25 00:11:01 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 3.2372
2022-02-25 00:11:34 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 3.2942
2022-02-25 00:12:08 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.9142
2022-02-25 00:12:43 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 3.3229
2022-02-25 00:13:18 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 3.0819
2022-02-25 00:13:54 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.9383
2022-02-25 00:14:28 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 3.0332
2022-02-25 00:15:04 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 3.0096
2022-02-25 00:15:37 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.9494
2022-02-25 00:16:14 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.8107
2022-02-25 00:16:48 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 3.0264
2022-02-25 00:17:23 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 3.0510
2022-02-25 00:17:57 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 3.1111
2022-02-25 00:18:32 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 3.1061
2022-02-25 00:19:07 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 3.0210
2022-02-25 00:19:43 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 3.2356
2022-02-25 00:20:17 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.8543
2022-02-25 00:20:53 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 3.1049
2022-02-25 00:21:28 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 3.3093
2022-02-25 00:22:04 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 3.0386
2022-02-25 00:22:38 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 3.2568
2022-02-25 00:22:39 - train: epoch 030, train_loss: 3.1218
2022-02-25 00:23:55 - eval: epoch: 030, acc1: 37.670%, acc5: 64.094%, test_loss: 2.8240, per_image_load_time: 2.746ms, per_image_inference_time: 0.211ms
2022-02-25 00:23:55 - until epoch: 030, best_acc1: 37.670%
2022-02-25 00:23:55 - epoch 031 lr: 0.010000000000000002
2022-02-25 00:24:35 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 3.0321
2022-02-25 00:25:09 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.8396
2022-02-25 00:25:43 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 2.6437
2022-02-25 00:26:18 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.7274
2022-02-25 00:26:52 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 2.7635
2022-02-25 00:27:26 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 2.7134
2022-02-25 00:27:59 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 2.7848
2022-02-25 00:28:35 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 2.8949
2022-02-25 00:29:08 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 2.7337
2022-02-25 00:29:43 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.8226
2022-02-25 00:30:17 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.6591
2022-02-25 00:30:51 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 2.7051
2022-02-25 00:31:26 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 2.3726
2022-02-25 00:32:01 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 2.5325
2022-02-25 00:32:35 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.8371
2022-02-25 00:33:10 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 2.6216
2022-02-25 00:33:44 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 2.4947
2022-02-25 00:34:18 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 2.4840
2022-02-25 00:34:54 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 2.7407
2022-02-25 00:35:26 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 2.8642
2022-02-25 00:36:01 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 2.6090
2022-02-25 00:36:36 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 2.4634
2022-02-25 00:37:11 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 2.6402
2022-02-25 00:37:45 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 2.6551
2022-02-25 00:38:19 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 2.6344
2022-02-25 00:38:53 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 2.5391
2022-02-25 00:39:29 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 2.9327
2022-02-25 00:40:03 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 3.2092
2022-02-25 00:40:38 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 2.6878
2022-02-25 00:41:12 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.9608
2022-02-25 00:41:46 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 2.7078
2022-02-25 00:42:20 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.8177
2022-02-25 00:42:55 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 2.5339
2022-02-25 00:43:31 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.8078
2022-02-25 00:44:03 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 2.6937
2022-02-25 00:44:38 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 2.4778
2022-02-25 00:45:13 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 2.7851
2022-02-25 00:45:48 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 2.5993
2022-02-25 00:46:23 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 2.7077
2022-02-25 00:46:57 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 2.6513
2022-02-25 00:47:32 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 2.6641
2022-02-25 00:48:06 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 2.6127
2022-02-25 00:48:41 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 2.6040
2022-02-25 00:49:16 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 2.7180
2022-02-25 00:49:52 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 2.6171
2022-02-25 00:50:26 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 2.8683
2022-02-25 00:51:02 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 2.5404
2022-02-25 00:51:36 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 2.5111
2022-02-25 00:52:13 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 2.8008
2022-02-25 00:52:47 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 2.8801
2022-02-25 00:52:48 - train: epoch 031, train_loss: 2.6977
2022-02-25 00:54:06 - eval: epoch: 031, acc1: 48.036%, acc5: 73.040%, test_loss: 2.2871, per_image_load_time: 2.199ms, per_image_inference_time: 0.183ms
2022-02-25 00:54:06 - until epoch: 031, best_acc1: 48.036%
2022-02-25 00:54:06 - epoch 032 lr: 0.010000000000000002
2022-02-25 00:54:46 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 2.4768
2022-02-25 00:55:19 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 2.6521
2022-02-25 00:55:54 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 2.4472
2022-02-25 00:56:28 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 2.7990
2022-02-25 00:57:02 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 2.5891
2022-02-25 00:57:38 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 2.6484
2022-02-25 00:58:12 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 2.5718
2022-02-25 00:58:45 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 2.5581
2022-02-25 00:59:19 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 2.5407
2022-02-25 00:59:54 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 2.7590
2022-02-25 01:00:28 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 2.8231
2022-02-25 01:01:02 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 2.7541
2022-02-25 01:01:38 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 2.4907
2022-02-25 01:02:10 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 2.8051
2022-02-25 01:02:45 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 2.5623
2022-02-25 01:03:20 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 2.7429
2022-02-25 01:03:55 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 2.6915
2022-02-25 01:04:29 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.9612
2022-02-25 01:05:02 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 2.4854
2022-02-25 01:05:37 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 2.4174
2022-02-25 01:06:12 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 2.3564
2022-02-25 01:06:46 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 2.6322
2022-02-25 01:07:20 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 2.6997
2022-02-25 01:07:55 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 2.4146
2022-02-25 01:08:30 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 2.6641
2022-02-25 01:09:04 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 2.4826
2022-02-25 01:09:39 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 2.6010
2022-02-25 01:10:13 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 2.6937
2022-02-25 01:10:49 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 2.6870
2022-02-25 01:11:23 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 2.5155
2022-02-25 01:11:57 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 2.7368
2022-02-25 01:12:31 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 2.6889
2022-02-25 01:13:06 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 2.5613
2022-02-25 01:13:41 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 2.2758
2022-02-25 01:14:16 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 2.7071
2022-02-25 01:14:49 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 2.6509
2022-02-25 01:15:25 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 2.6598
2022-02-25 01:15:59 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 2.4279
2022-02-25 01:16:33 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 2.5066
2022-02-25 01:17:09 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 2.5447
2022-02-25 01:17:43 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 2.5623
2022-02-25 01:18:18 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 2.3543
2022-02-25 01:18:52 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.6844
2022-02-25 01:19:27 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 2.5006
2022-02-25 01:20:00 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 2.7405
2022-02-25 01:20:36 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 2.6113
2022-02-25 01:21:10 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 2.7375
2022-02-25 01:21:46 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 2.3893
2022-02-25 01:22:21 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 2.5802
2022-02-25 01:22:55 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 2.6532
2022-02-25 01:22:56 - train: epoch 032, train_loss: 2.6054
2022-02-25 01:24:15 - eval: epoch: 032, acc1: 49.006%, acc5: 73.796%, test_loss: 2.2457, per_image_load_time: 2.839ms, per_image_inference_time: 0.194ms
2022-02-25 01:24:15 - until epoch: 032, best_acc1: 49.006%
2022-02-25 01:24:15 - epoch 033 lr: 0.010000000000000002
2022-02-25 01:24:55 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 2.4621
2022-02-25 01:25:29 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 2.7878
2022-02-25 01:26:03 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 2.4281
2022-02-25 01:26:36 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 2.3195
2022-02-25 01:27:11 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 2.8542
2022-02-25 01:27:44 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 2.3193
2022-02-25 01:28:19 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 2.8968
2022-02-25 01:28:52 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 2.6682
2022-02-25 01:29:27 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 2.6480
2022-02-25 01:30:02 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 2.6807
2022-02-25 01:30:36 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 2.3897
2022-02-25 01:31:11 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 2.8421
2022-02-25 01:31:44 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 2.4794
2022-02-25 01:32:19 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 2.6553
2022-02-25 01:32:53 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 2.7611
2022-02-25 01:33:27 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 2.7160
2022-02-25 01:34:02 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 2.5288
2022-02-25 01:34:36 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 2.5627
2022-02-25 01:35:11 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 2.4146
2022-02-25 01:35:45 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 2.3391
2022-02-25 01:36:19 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 2.5873
2022-02-25 01:36:53 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 2.7297
2022-02-25 01:37:28 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 2.3658
2022-02-25 01:38:02 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.6988
2022-02-25 01:38:37 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 2.5998
2022-02-25 01:39:11 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 2.4576
2022-02-25 01:39:46 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 2.6101
2022-02-25 01:40:20 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 2.6671
2022-02-25 01:40:54 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 2.7294
2022-02-25 01:41:29 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 2.3431
2022-02-25 01:42:02 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 2.5785
2022-02-25 01:42:38 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 2.4507
2022-02-25 01:43:11 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 2.5524
2022-02-25 01:43:46 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 2.4462
2022-02-25 01:44:20 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 2.6012
2022-02-25 01:44:56 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 2.8283
2022-02-25 01:45:30 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 2.6478
2022-02-25 01:46:05 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 2.5716
2022-02-25 01:46:39 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 2.8539
2022-02-25 01:47:14 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 2.6847
2022-02-25 01:47:48 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 2.5990
2022-02-25 01:48:23 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 2.6032
2022-02-25 01:48:57 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 2.4834
2022-02-25 01:49:31 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 2.6171
2022-02-25 01:50:07 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 2.7998
2022-02-25 01:50:41 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 2.5229
2022-02-25 01:51:16 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 2.4714
2022-02-25 01:51:51 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.8444
2022-02-25 01:52:27 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 2.2581
2022-02-25 01:53:01 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 2.6338
2022-02-25 01:53:02 - train: epoch 033, train_loss: 2.5749
2022-02-25 01:54:19 - eval: epoch: 033, acc1: 49.486%, acc5: 74.228%, test_loss: 2.2202, per_image_load_time: 2.773ms, per_image_inference_time: 0.214ms
2022-02-25 01:54:19 - until epoch: 033, best_acc1: 49.486%
2022-02-25 01:54:19 - epoch 034 lr: 0.010000000000000002
2022-02-25 01:54:58 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 2.5363
2022-02-25 01:55:33 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 2.3503
2022-02-25 01:56:07 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 2.4890
2022-02-25 01:56:40 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 2.3858
2022-02-25 01:57:15 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 2.7074
2022-02-25 01:57:49 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 2.6418
2022-02-25 01:58:23 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 2.4172
2022-02-25 01:58:58 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 2.7980
2022-02-25 01:59:31 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 2.4765
2022-02-25 02:00:06 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 2.6597
2022-02-25 02:00:40 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 2.6226
2022-02-25 02:01:15 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 2.6402
2022-02-25 02:01:50 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 2.5492
2022-02-25 02:02:25 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 2.6519
2022-02-25 02:02:59 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 2.5814
2022-02-25 02:03:33 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 2.4965
2022-02-25 02:04:08 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 2.7624
2022-02-25 02:04:43 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 2.6296
2022-02-25 02:05:17 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 2.6490
2022-02-25 02:05:52 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 2.3818
2022-02-25 02:06:26 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 2.4885
2022-02-25 02:07:01 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 2.2924
2022-02-25 02:07:35 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 2.4887
2022-02-25 02:08:10 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 2.2482
2022-02-25 02:08:44 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 2.4740
2022-02-25 02:09:18 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 2.6973
2022-02-25 02:09:53 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 2.6599
2022-02-25 02:10:27 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 2.4160
2022-02-25 02:11:01 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 2.3856
2022-02-25 02:11:37 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 2.4700
2022-02-25 02:12:11 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 2.5050
2022-02-25 02:12:45 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 2.5048
2022-02-25 02:13:20 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 2.4304
2022-02-25 02:13:55 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 2.5688
2022-02-25 02:14:29 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 2.3542
2022-02-25 02:15:05 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 2.3894
2022-02-25 02:15:39 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 2.5029
2022-02-25 02:16:14 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 2.5780
2022-02-25 02:16:49 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 2.8252
2022-02-25 02:17:24 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 2.6312
2022-02-25 02:17:58 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 2.7593
2022-02-25 02:18:32 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 2.4784
2022-02-25 02:19:08 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 2.6963
2022-02-25 02:19:43 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 2.4102
2022-02-25 02:20:17 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 2.5256
2022-02-25 02:20:52 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 2.5433
2022-02-25 02:21:27 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 2.5439
2022-02-25 02:22:02 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 2.7073
2022-02-25 02:22:38 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 2.4885
2022-02-25 02:23:13 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 2.4705
2022-02-25 02:23:14 - train: epoch 034, train_loss: 2.5554
2022-02-25 02:24:31 - eval: epoch: 034, acc1: 49.648%, acc5: 74.294%, test_loss: 2.2124, per_image_load_time: 1.224ms, per_image_inference_time: 0.205ms
2022-02-25 02:24:31 - until epoch: 034, best_acc1: 49.648%
2022-02-25 02:24:31 - epoch 035 lr: 0.010000000000000002
2022-02-25 02:25:10 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 2.4416
2022-02-25 02:25:45 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 2.3052
2022-02-25 02:26:20 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 2.7254
2022-02-25 02:26:53 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 2.6412
2022-02-25 02:27:27 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 2.5653
2022-02-25 02:28:02 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 2.5699
2022-02-25 02:28:35 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 2.4611
2022-02-25 02:29:10 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 2.5849
2022-02-25 02:29:44 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 2.7697
2022-02-25 02:30:19 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 2.5235
2022-02-25 02:30:53 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 2.5896
2022-02-25 02:31:28 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 2.3600
2022-02-25 02:32:03 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 2.6642
2022-02-25 02:32:37 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 2.7060
2022-02-25 02:33:12 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 2.5376
2022-02-25 02:33:45 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 2.4205
2022-02-25 02:34:20 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 2.4888
2022-02-25 02:34:54 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 2.6385
2022-02-25 02:35:29 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 2.4235
2022-02-25 02:36:03 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 2.7736
2022-02-25 02:36:38 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 2.5687
2022-02-25 02:37:13 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 2.5441
2022-02-25 02:37:48 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 2.6154
2022-02-25 02:38:22 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 2.5816
2022-02-25 02:38:56 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 2.4697
2022-02-25 02:39:31 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 2.6404
2022-02-25 02:40:06 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 2.4934
2022-02-25 02:40:40 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 2.7113
2022-02-25 02:41:15 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 2.5398
2022-02-25 02:41:50 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 2.2075
2022-02-25 02:42:24 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 2.7423
2022-02-25 02:42:59 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 2.4674
2022-02-25 02:43:34 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 2.5257
2022-02-25 02:44:08 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 2.6540
2022-02-25 02:44:43 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 2.2722
2022-02-25 02:45:18 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 2.3787
2022-02-25 02:45:53 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 2.5563
2022-02-25 02:46:28 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 2.4687
2022-02-25 02:47:02 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 2.5935
2022-02-25 02:47:38 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 2.4389
2022-02-25 02:48:12 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 2.6408
2022-02-25 02:48:46 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 2.2955
2022-02-25 02:49:21 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 2.5589
2022-02-25 02:49:56 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 2.8029
2022-02-25 02:50:32 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 2.6456
2022-02-25 02:51:06 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 2.4316
2022-02-25 02:51:41 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 2.6430
2022-02-25 02:52:17 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 2.5777
2022-02-25 02:52:53 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 2.6704
2022-02-25 02:53:27 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 2.4316
2022-02-25 02:53:28 - train: epoch 035, train_loss: 2.5460
2022-02-25 02:54:47 - eval: epoch: 035, acc1: 49.754%, acc5: 74.706%, test_loss: 2.2046, per_image_load_time: 2.917ms, per_image_inference_time: 0.177ms
2022-02-25 02:54:47 - until epoch: 035, best_acc1: 49.754%
2022-02-25 02:54:47 - epoch 036 lr: 0.010000000000000002
2022-02-25 02:55:27 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 2.6029
2022-02-25 02:56:01 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 2.3510
2022-02-25 02:56:35 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 2.3893
2022-02-25 02:57:10 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 2.4686
2022-02-25 02:57:43 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 2.3211
2022-02-25 02:58:18 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 2.4526
2022-02-25 02:58:51 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 2.6277
2022-02-25 02:59:26 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 2.3883
2022-02-25 02:59:59 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 2.6361
2022-02-25 03:00:34 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 2.3924
2022-02-25 03:01:08 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 2.4714
2022-02-25 03:01:42 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 2.4658
2022-02-25 03:02:16 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 2.6430
2022-02-25 03:02:50 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 2.5046
2022-02-25 03:03:25 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 2.5453
2022-02-25 03:03:59 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 2.7385
2022-02-25 03:04:34 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 2.6714
2022-02-25 03:05:09 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 2.4482
2022-02-25 03:05:43 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 2.7469
2022-02-25 03:06:16 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 2.4809
2022-02-25 03:06:51 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 2.5319
2022-02-25 03:07:26 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 2.6368
2022-02-25 03:08:00 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 2.5488
2022-02-25 03:08:34 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 2.8108
2022-02-25 03:09:09 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 2.3200
2022-02-25 03:09:44 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 2.6649
2022-02-25 03:10:19 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 2.5159
2022-02-25 03:10:52 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 2.4185
2022-02-25 03:11:27 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 2.4131
2022-02-25 03:12:02 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 2.4013
2022-02-25 03:12:36 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 2.7041
2022-02-25 03:13:10 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 2.6249
2022-02-25 03:13:45 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 2.2293
2022-02-25 03:14:19 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 2.3075
2022-02-25 03:14:53 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 2.7493
2022-02-25 03:15:28 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 2.7827
2022-02-25 03:16:03 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 2.6390
2022-02-25 03:16:38 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 2.4839
2022-02-25 03:17:10 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 2.4364
2022-02-25 03:17:47 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 2.6621
2022-02-25 03:18:21 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 2.4999
2022-02-25 03:18:56 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 2.5556
2022-02-25 03:19:30 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 2.5439
2022-02-25 03:20:04 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 2.6567
2022-02-25 03:20:39 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 2.1804
2022-02-25 03:21:14 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 2.4357
2022-02-25 03:21:49 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 2.3983
2022-02-25 03:22:24 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 2.5999
2022-02-25 03:23:00 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 2.4149
2022-02-25 03:23:34 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 2.5263
2022-02-25 03:23:35 - train: epoch 036, train_loss: 2.5361
2022-02-25 03:24:53 - eval: epoch: 036, acc1: 50.034%, acc5: 74.736%, test_loss: 2.1885, per_image_load_time: 2.818ms, per_image_inference_time: 0.202ms
2022-02-25 03:24:53 - until epoch: 036, best_acc1: 50.034%
2022-02-25 03:24:53 - epoch 037 lr: 0.010000000000000002
2022-02-25 03:25:33 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 2.4186
2022-02-25 03:26:07 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 2.2270
2022-02-25 03:26:41 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 2.3389
2022-02-25 03:27:16 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 2.2615
2022-02-25 03:27:50 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 2.6127
2022-02-25 03:28:23 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 2.8997
2022-02-25 03:28:57 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 2.6015
2022-02-25 03:29:31 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 2.6661
2022-02-25 03:30:06 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.7825
2022-02-25 03:30:41 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 2.4335
2022-02-25 03:31:14 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 2.6465
2022-02-25 03:31:49 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 2.6407
2022-02-25 03:32:23 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 2.5000
2022-02-25 03:32:56 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 2.7766
2022-02-25 03:33:31 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 2.3137
2022-02-25 03:34:06 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 2.3730
2022-02-25 03:34:39 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 2.6273
2022-02-25 03:35:15 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 2.5995
2022-02-25 03:35:48 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 2.5825
2022-02-25 03:36:23 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 2.5944
2022-02-25 03:36:57 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 2.5261
2022-02-25 03:37:32 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 2.4843
2022-02-25 03:38:05 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 2.5641
2022-02-25 03:38:40 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 2.4289
2022-02-25 03:39:14 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 2.7532
2022-02-25 03:39:49 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 2.6129
2022-02-25 03:40:23 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 2.5842
2022-02-25 03:41:00 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 2.5046
2022-02-25 03:41:33 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 2.6430
2022-02-25 03:42:07 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 2.6125
2022-02-25 03:42:42 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 2.4493
2022-02-25 03:43:17 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 2.7061
2022-02-25 03:43:50 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 2.5745
2022-02-25 03:44:25 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 2.3920
2022-02-25 03:44:59 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 2.4495
2022-02-25 03:45:34 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 2.4646
2022-02-25 03:46:09 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 2.5264
2022-02-25 03:46:43 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 2.5511
2022-02-25 03:47:17 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 2.6068
2022-02-25 03:47:53 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 2.4835
2022-02-25 03:48:27 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 2.5200
2022-02-25 03:49:02 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 2.4799
2022-02-25 03:49:35 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 2.5104
2022-02-25 03:50:11 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 2.5451
2022-02-25 03:50:45 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 2.3911
2022-02-25 03:51:19 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 2.5354
2022-02-25 03:51:54 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 2.4863
2022-02-25 03:52:30 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 2.3064
2022-02-25 03:53:05 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 2.4613
2022-02-25 03:53:39 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 2.5020
2022-02-25 03:53:40 - train: epoch 037, train_loss: 2.5349
2022-02-25 03:54:56 - eval: epoch: 037, acc1: 49.778%, acc5: 74.802%, test_loss: 2.1934, per_image_load_time: 2.617ms, per_image_inference_time: 0.214ms
2022-02-25 03:54:56 - until epoch: 037, best_acc1: 50.034%
2022-02-25 03:54:56 - epoch 038 lr: 0.010000000000000002
2022-02-25 03:55:34 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 2.3094
2022-02-25 03:56:10 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 2.3922
2022-02-25 03:56:42 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 2.3901
2022-02-25 03:57:17 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 2.4162
2022-02-25 03:57:51 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 2.3752
2022-02-25 03:58:26 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 2.8013
2022-02-25 03:59:00 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 2.3294
2022-02-25 03:59:33 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 2.4485
2022-02-25 04:00:08 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 2.5367
2022-02-25 04:00:42 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 2.8023
2022-02-25 04:01:18 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 2.5158
2022-02-25 04:01:51 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 2.4642
2022-02-25 04:02:25 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 2.5515
2022-02-25 04:03:01 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 2.5815
2022-02-25 04:03:35 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 2.2174
2022-02-25 04:04:10 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 2.6510
2022-02-25 04:04:44 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 2.6681
2022-02-25 04:05:18 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 2.4117
2022-02-25 04:05:53 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 2.3616
2022-02-25 04:06:28 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 2.4434
2022-02-25 04:07:02 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 2.4563
2022-02-25 04:07:37 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 2.2333
2022-02-25 04:08:11 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 2.7399
2022-02-25 04:08:46 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 2.7111
2022-02-25 04:09:20 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 2.4108
2022-02-25 04:09:55 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 2.4806
2022-02-25 04:10:28 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 2.4291
2022-02-25 04:11:03 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 2.7212
2022-02-25 04:11:39 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 2.5125
2022-02-25 04:12:13 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 2.3691
2022-02-25 04:12:47 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 2.5114
2022-02-25 04:13:21 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 2.4913
2022-02-25 04:13:57 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 2.3503
2022-02-25 04:14:32 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 2.5296
2022-02-25 04:15:06 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 2.5422
2022-02-25 04:15:41 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 2.6125
2022-02-25 04:16:14 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 2.4925
2022-02-25 04:16:50 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 2.7308
2022-02-25 04:17:24 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 2.3621
2022-02-25 04:18:00 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 2.5684
2022-02-25 04:18:34 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 2.4019
2022-02-25 04:19:09 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 2.3677
2022-02-25 04:19:44 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 2.4540
2022-02-25 04:20:19 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 2.7006
2022-02-25 04:20:53 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 2.6361
2022-02-25 04:21:28 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 2.3964
2022-02-25 04:22:03 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 2.4284
2022-02-25 04:22:40 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 2.4196
2022-02-25 04:23:14 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 2.5693
2022-02-25 04:23:48 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 2.4010
2022-02-25 04:23:49 - train: epoch 038, train_loss: 2.5320
2022-02-25 04:25:06 - eval: epoch: 038, acc1: 49.732%, acc5: 74.758%, test_loss: 2.1963, per_image_load_time: 2.747ms, per_image_inference_time: 0.208ms
2022-02-25 04:25:06 - until epoch: 038, best_acc1: 50.034%
2022-02-25 04:25:06 - epoch 039 lr: 0.010000000000000002
2022-02-25 04:25:45 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 2.4989
2022-02-25 04:26:20 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 2.6186
2022-02-25 04:26:54 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 2.5696
2022-02-25 04:27:29 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 2.7030
2022-02-25 04:28:03 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 2.5778
2022-02-25 04:28:36 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 2.3738
2022-02-25 04:29:11 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 2.6675
2022-02-25 04:29:45 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 2.3911
2022-02-25 04:30:18 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 2.6996
2022-02-25 04:30:52 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 2.2391
2022-02-25 04:31:26 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 2.7279
2022-02-25 04:32:02 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 2.5988
2022-02-25 04:32:35 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 2.6369
2022-02-25 04:33:09 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 2.7043
2022-02-25 04:33:43 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 2.2777
2022-02-25 04:34:18 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 2.4763
2022-02-25 04:34:52 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 2.3979
2022-02-25 04:35:26 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 2.5222
2022-02-25 04:36:00 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 2.2185
2022-02-25 04:36:35 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 2.4395
2022-02-25 04:37:10 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 2.7378
2022-02-25 04:37:44 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 2.6064
2022-02-25 04:38:18 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 2.7492
2022-02-25 04:38:52 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 2.5925
2022-02-25 04:39:28 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 2.4783
2022-02-25 04:40:00 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 2.4588
2022-02-25 04:40:36 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 2.4639
2022-02-25 04:41:11 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 2.6756
2022-02-25 04:41:45 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 2.3764
2022-02-25 04:42:21 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 2.7281
2022-02-25 04:42:54 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 2.3127
2022-02-25 04:43:29 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 2.5518
2022-02-25 04:44:04 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 2.6159
2022-02-25 04:44:38 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 2.6376
2022-02-25 04:45:13 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 2.9097
2022-02-25 04:45:47 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 2.5500
2022-02-25 04:46:22 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 2.4678
2022-02-25 04:46:57 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 2.2834
2022-02-25 04:47:31 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 2.7784
2022-02-25 04:48:04 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 2.4266
2022-02-25 04:48:40 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 2.4629
2022-02-25 04:49:14 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 2.6600
2022-02-25 04:49:49 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 2.4511
2022-02-25 04:50:23 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 2.3399
2022-02-25 04:50:58 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 2.3694
2022-02-25 04:51:33 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 2.5343
2022-02-25 04:52:08 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 2.6643
2022-02-25 04:52:44 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 2.4754
2022-02-25 04:53:20 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 2.4129
2022-02-25 04:53:53 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 2.5392
2022-02-25 04:53:54 - train: epoch 039, train_loss: 2.5311
2022-02-25 04:55:11 - eval: epoch: 039, acc1: 49.784%, acc5: 74.782%, test_loss: 2.1942, per_image_load_time: 2.729ms, per_image_inference_time: 0.219ms
2022-02-25 04:55:11 - until epoch: 039, best_acc1: 50.034%
2022-02-25 04:55:11 - epoch 040 lr: 0.010000000000000002
2022-02-25 04:55:50 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 2.8437
2022-02-25 04:56:24 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 2.5417
2022-02-25 04:56:59 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 2.8058
2022-02-25 04:57:32 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 2.3782
2022-02-25 04:58:07 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 2.3149
2022-02-25 04:58:41 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 2.6203
2022-02-25 04:59:15 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 2.6554
2022-02-25 04:59:49 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 2.4840
2022-02-25 05:00:23 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 2.3663
2022-02-25 05:00:58 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 2.3528
2022-02-25 05:01:31 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 2.4687
2022-02-25 05:02:06 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 2.4730
2022-02-25 05:02:40 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 2.5798
2022-02-25 05:03:14 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 2.5778
2022-02-25 05:03:48 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 2.4180
2022-02-25 05:04:22 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 2.5917
2022-02-25 05:04:57 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 2.4707
2022-02-25 05:05:32 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 2.4290
2022-02-25 05:06:06 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 2.4153
2022-02-25 05:06:42 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 2.5215
2022-02-25 05:07:15 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 2.5677
2022-02-25 05:07:50 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 2.4082
2022-02-25 05:08:23 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 2.7783
2022-02-25 05:08:59 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 2.6129
2022-02-25 05:09:33 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 2.5936
2022-02-25 05:10:08 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 2.3839
2022-02-25 05:10:41 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 2.6158
2022-02-25 05:11:15 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 2.5662
2022-02-25 05:11:49 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 2.7880
2022-02-25 05:12:26 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 2.5000
2022-02-25 05:12:59 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 2.7280
2022-02-25 05:13:34 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 2.6252
2022-02-25 05:14:08 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 2.6107
2022-02-25 05:14:42 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 2.4076
2022-02-25 05:15:16 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 2.6981
2022-02-25 05:15:51 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 2.4401
2022-02-25 05:16:25 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 2.5175
2022-02-25 05:17:00 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 2.5238
2022-02-25 05:17:34 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 2.4285
2022-02-25 05:18:10 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 2.6170
2022-02-25 05:18:44 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 2.5084
2022-02-25 05:19:19 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 2.4962
2022-02-25 05:19:53 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 2.4188
2022-02-25 05:20:27 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 2.3028
2022-02-25 05:21:03 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 2.3779
2022-02-25 05:21:37 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 2.4982
2022-02-25 05:22:12 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 2.6138
2022-02-25 05:22:47 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 2.5097
2022-02-25 05:23:24 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 2.5879
2022-02-25 05:23:57 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 2.6506
2022-02-25 05:23:59 - train: epoch 040, train_loss: 2.5315
2022-02-25 05:25:15 - eval: epoch: 040, acc1: 49.874%, acc5: 74.728%, test_loss: 2.1984, per_image_load_time: 2.665ms, per_image_inference_time: 0.240ms
2022-02-25 05:25:15 - until epoch: 040, best_acc1: 50.034%
2022-02-25 05:25:15 - epoch 041 lr: 0.010000000000000002
2022-02-25 05:25:55 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 2.7156
2022-02-25 05:26:29 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 2.8372
2022-02-25 05:27:03 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 2.6786
2022-02-25 05:27:37 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 2.4573
2022-02-25 05:28:11 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 2.2760
2022-02-25 05:28:46 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 2.6400
2022-02-25 05:29:20 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 2.5666
2022-02-25 05:29:54 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 2.5374
2022-02-25 05:30:28 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 2.2951
2022-02-25 05:31:03 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 2.7814
2022-02-25 05:31:36 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 2.3615
2022-02-25 05:32:11 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 2.2708
2022-02-25 05:32:45 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 2.3089
2022-02-25 05:33:21 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 2.6239
2022-02-25 05:33:54 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.8113
2022-02-25 05:34:28 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 2.4351
2022-02-25 05:35:03 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 2.5754
2022-02-25 05:35:37 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 2.4281
2022-02-25 05:36:13 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 2.4414
2022-02-25 05:36:47 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 2.5694
2022-02-25 05:37:21 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 2.3801
2022-02-25 05:37:56 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 2.2164
2022-02-25 05:38:30 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 2.4174
2022-02-25 05:39:04 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 2.7948
2022-02-25 05:39:39 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 2.5459
2022-02-25 05:40:13 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 2.7361
2022-02-25 05:40:47 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 2.5136
2022-02-25 05:41:22 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 2.6640
2022-02-25 05:41:56 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 2.7952
2022-02-25 05:42:31 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 2.5809
2022-02-25 05:43:06 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 2.5599
2022-02-25 05:43:41 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 2.5768
2022-02-25 05:44:14 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 2.3628
2022-02-25 05:44:49 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 2.1876
2022-02-25 05:45:24 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 2.5618
2022-02-25 05:45:58 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 2.7461
2022-02-25 05:46:33 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 2.6841
2022-02-25 05:47:08 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 2.3779
2022-02-25 05:47:42 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 2.4263
2022-02-25 05:48:18 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 2.4575
2022-02-25 05:48:52 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 2.6548
2022-02-25 05:49:27 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 2.5092
2022-02-25 05:50:01 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 2.5149
2022-02-25 05:50:37 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 2.3519
2022-02-25 05:51:10 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 2.5675
2022-02-25 05:51:46 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 2.5509
2022-02-25 05:52:21 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 2.5767
2022-02-25 05:52:57 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 2.7212
2022-02-25 05:53:33 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 2.5612
2022-02-25 05:54:07 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 2.6388
2022-02-25 05:54:08 - train: epoch 041, train_loss: 2.5325
2022-02-25 05:55:25 - eval: epoch: 041, acc1: 49.456%, acc5: 74.656%, test_loss: 2.2044, per_image_load_time: 2.732ms, per_image_inference_time: 0.241ms
2022-02-25 05:55:25 - until epoch: 041, best_acc1: 50.034%
2022-02-25 05:55:25 - epoch 042 lr: 0.010000000000000002
2022-02-25 05:56:03 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 2.2924
2022-02-25 05:56:38 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 2.3687
2022-02-25 05:57:13 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 2.5527
2022-02-25 05:57:46 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 2.2376
2022-02-25 05:58:20 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 2.5617
2022-02-25 05:58:54 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 2.4453
2022-02-25 05:59:28 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 2.5914
2022-02-25 06:00:03 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 2.5600
2022-02-25 06:00:36 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 2.7273
2022-02-25 06:01:10 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 2.7992
2022-02-25 06:01:45 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 2.4033
2022-02-25 06:02:19 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 2.2317
2022-02-25 06:02:54 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 2.5485
2022-02-25 06:03:28 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 2.7136
2022-02-25 06:04:03 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 2.6232
2022-02-25 06:04:37 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 2.6898
2022-02-25 06:05:11 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 2.5456
2022-02-25 06:05:46 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 2.4231
2022-02-25 06:06:20 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 2.7366
2022-02-25 06:06:54 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 2.5626
2022-02-25 06:07:28 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 2.4821
2022-02-25 06:08:03 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 2.6072
2022-02-25 06:08:38 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 2.4739
2022-02-25 06:09:12 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 2.5972
2022-02-25 06:09:47 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 2.6076
2022-02-25 06:10:21 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 2.5815
2022-02-25 06:10:55 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 2.3414
2022-02-25 06:11:29 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 2.4353
2022-02-25 06:12:04 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 2.5549
2022-02-25 06:12:38 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 2.5759
2022-02-25 06:13:13 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 2.6455
2022-02-25 06:13:47 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 2.6870
2022-02-25 06:14:21 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 2.5296
2022-02-25 06:14:55 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 2.3214
2022-02-25 06:15:30 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 2.6719
2022-02-25 06:16:05 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 2.7027
2022-02-25 06:16:39 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 2.7062
2022-02-25 06:17:15 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 2.3491
2022-02-25 06:17:48 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 2.4479
2022-02-25 06:18:24 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 2.4168
2022-02-25 06:18:57 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 2.6568
2022-02-25 06:19:33 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 2.5142
2022-02-25 06:20:07 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 2.3366
2022-02-25 06:20:42 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 2.3534
2022-02-25 06:21:17 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 2.3937
2022-02-25 06:21:51 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 2.5019
2022-02-25 06:22:26 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 2.4631
2022-02-25 06:23:01 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 2.5045
2022-02-25 06:23:36 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 2.6018
2022-02-25 06:24:10 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 2.4722
2022-02-25 06:24:11 - train: epoch 042, train_loss: 2.5319
2022-02-25 06:25:28 - eval: epoch: 042, acc1: 49.626%, acc5: 74.590%, test_loss: 2.2028, per_image_load_time: 2.721ms, per_image_inference_time: 0.234ms
2022-02-25 06:25:28 - until epoch: 042, best_acc1: 50.034%
2022-02-25 06:25:28 - epoch 043 lr: 0.010000000000000002
2022-02-25 06:26:07 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 2.6338
2022-02-25 06:26:41 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 2.7920
2022-02-25 06:27:15 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 2.2816
2022-02-25 06:27:49 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 2.5172
2022-02-25 06:28:24 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 2.4496
2022-02-25 06:28:59 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 2.5628
2022-02-25 06:29:33 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 2.4704
2022-02-25 06:30:07 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 2.4156
2022-02-25 06:30:41 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 2.4429
2022-02-25 06:31:15 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 2.6084
2022-02-25 06:31:49 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 2.4703
2022-02-25 06:32:24 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 2.4152
2022-02-25 06:32:59 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 2.5835
2022-02-25 06:33:32 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 2.3075
2022-02-25 06:34:07 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 2.5954
2022-02-25 06:34:41 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 2.4616
2022-02-25 06:35:16 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 2.7061
2022-02-25 06:35:50 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 2.9977
2022-02-25 06:36:24 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 2.7460
2022-02-25 06:36:58 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 2.4401
2022-02-25 06:37:33 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 2.4432
2022-02-25 06:38:07 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 2.6827
2022-02-25 06:38:41 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 2.5818
2022-02-25 06:39:17 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 2.4334
2022-02-25 06:39:51 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 2.7792
2022-02-25 06:40:26 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 2.3937
2022-02-25 06:41:00 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 2.5959
2022-02-25 06:41:34 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 2.3621
2022-02-25 06:42:09 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 2.4874
2022-02-25 06:42:43 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 2.8561
2022-02-25 06:43:18 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 2.7416
2022-02-25 06:43:53 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 2.4291
2022-02-25 06:44:26 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 2.6484
2022-02-25 06:45:02 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 2.4231
2022-02-25 06:45:36 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 2.4388
2022-02-25 06:46:11 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 2.6654
2022-02-25 06:46:45 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 2.3382
2022-02-25 06:47:19 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 2.6862
2022-02-25 06:47:53 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 2.6343
2022-02-25 06:48:28 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 2.5338
2022-02-25 06:49:02 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 2.8395
2022-02-25 06:49:37 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 2.5852
2022-02-25 06:50:10 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 2.4720
2022-02-25 06:50:46 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 2.6603
2022-02-25 06:51:20 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 2.4847
2022-02-25 06:51:55 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 2.5182
2022-02-25 06:52:29 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 2.6794
2022-02-25 06:53:04 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 2.4269
2022-02-25 06:53:40 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 2.5746
2022-02-25 06:54:15 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 2.5857
2022-02-25 06:54:16 - train: epoch 043, train_loss: 2.5332
2022-02-25 06:55:32 - eval: epoch: 043, acc1: 49.310%, acc5: 74.830%, test_loss: 2.2032, per_image_load_time: 2.131ms, per_image_inference_time: 0.234ms
2022-02-25 06:55:32 - until epoch: 043, best_acc1: 50.034%
2022-02-25 06:55:32 - epoch 044 lr: 0.010000000000000002
2022-02-25 06:56:11 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 2.5464
2022-02-25 06:56:45 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 2.5196
2022-02-25 06:57:20 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 2.3886
2022-02-25 06:57:55 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 2.1685
2022-02-25 06:58:29 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 2.6258
2022-02-25 06:59:03 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 2.4407
2022-02-25 06:59:36 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 2.4297
2022-02-25 07:00:11 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 2.5215
2022-02-25 07:00:45 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 2.4432
2022-02-25 07:01:19 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 2.4815
2022-02-25 07:01:54 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 2.2125
2022-02-25 07:02:28 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 2.4380
2022-02-25 07:03:03 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 2.4949
2022-02-25 07:03:37 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 2.3412
2022-02-25 07:04:12 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 2.5914
2022-02-25 07:04:46 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 2.4753
2022-02-25 07:05:20 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 2.6779
2022-02-25 07:05:55 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 2.4449
2022-02-25 07:06:30 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 2.6055
2022-02-25 07:07:05 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 2.5163
2022-02-25 07:07:40 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 2.7709
2022-02-25 07:08:14 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 2.5902
2022-02-25 07:08:48 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 2.6460
2022-02-25 07:09:22 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 2.6393
2022-02-25 07:09:56 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 2.5106
2022-02-25 07:10:31 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 2.3309
2022-02-25 07:11:04 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 2.4668
2022-02-25 07:11:39 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 2.3436
2022-02-25 07:12:14 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 2.4477
2022-02-25 07:12:48 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 2.4352
2022-02-25 07:13:24 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 2.3546
2022-02-25 07:13:57 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 2.4642
2022-02-25 07:14:32 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 2.3987
2022-02-25 07:15:07 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 2.5446
2022-02-25 07:15:41 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 2.6198
2022-02-25 07:16:16 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 2.7562
2022-02-25 07:16:50 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 2.5209
2022-02-25 07:17:25 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 2.3224
2022-02-25 07:18:00 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 2.4635
2022-02-25 07:18:34 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 2.4531
2022-02-25 07:19:09 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 2.3965
2022-02-25 07:19:43 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 2.6766
2022-02-25 07:20:19 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 2.5269
2022-02-25 07:20:53 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 2.6079
2022-02-25 07:21:29 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 2.7737
2022-02-25 07:22:03 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 2.5643
2022-02-25 07:22:38 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 2.5378
2022-02-25 07:23:13 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 2.5928
2022-02-25 07:23:49 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 2.5183
2022-02-25 07:24:23 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 2.5891
2022-02-25 07:24:24 - train: epoch 044, train_loss: 2.5326
2022-02-25 07:25:42 - eval: epoch: 044, acc1: 49.690%, acc5: 74.652%, test_loss: 2.2022, per_image_load_time: 2.786ms, per_image_inference_time: 0.191ms
2022-02-25 07:25:42 - until epoch: 044, best_acc1: 50.034%
2022-02-25 07:25:42 - epoch 045 lr: 0.010000000000000002
2022-02-25 07:26:21 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 2.6620
2022-02-25 07:26:55 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 2.3505
2022-02-25 07:27:31 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 2.4867
2022-02-25 07:28:03 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 2.6013
2022-02-25 07:28:38 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 2.6452
2022-02-25 07:29:13 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 2.7883
2022-02-25 07:29:48 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 2.4900
2022-02-25 07:30:21 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 2.3593
2022-02-25 07:30:56 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 2.4539
2022-02-25 07:31:31 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 2.4335
2022-02-25 07:32:05 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 2.4616
2022-02-25 07:32:40 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 2.5447
2022-02-25 07:33:14 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 2.5320
2022-02-25 07:33:49 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 2.4684
2022-02-25 07:34:24 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 2.5383
2022-02-25 07:34:58 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 2.4601
2022-02-25 07:35:32 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 2.3351
2022-02-25 07:36:07 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 2.5238
2022-02-25 07:36:42 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 2.7495
2022-02-25 07:37:17 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 2.6630
2022-02-25 07:37:51 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 2.6012
2022-02-25 07:38:26 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 2.6392
2022-02-25 07:39:00 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 2.3030
2022-02-25 07:39:34 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 2.7055
2022-02-25 07:40:09 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 2.6151
2022-02-25 07:40:44 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 2.4359
2022-02-25 07:41:19 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 2.4083
2022-02-25 07:41:54 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 2.4099
2022-02-25 07:42:26 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 2.5177
2022-02-25 07:43:02 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 2.7746
2022-02-25 07:43:36 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 2.5224
2022-02-25 07:44:11 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 2.6034
2022-02-25 07:44:45 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 2.5744
2022-02-25 07:45:20 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 2.5306
2022-02-25 07:45:54 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 2.7344
2022-02-25 07:46:29 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 2.6061
2022-02-25 07:47:04 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 2.5756
2022-02-25 07:47:39 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 2.3650
2022-02-25 07:48:13 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 2.5054
2022-02-25 07:48:47 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 2.6977
2022-02-25 07:49:22 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 2.5258
2022-02-25 07:49:57 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 2.6711
2022-02-25 07:50:31 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 2.4934
2022-02-25 07:51:06 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 2.4454
2022-02-25 07:51:41 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 2.3467
2022-02-25 07:52:15 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 2.6095
2022-02-25 07:52:51 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 2.6339
2022-02-25 07:53:25 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 2.4125
2022-02-25 07:54:01 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 2.5785
2022-02-25 07:54:35 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 2.4828
2022-02-25 07:54:37 - train: epoch 045, train_loss: 2.5311
2022-02-25 07:55:54 - eval: epoch: 045, acc1: 49.814%, acc5: 74.802%, test_loss: 2.1932, per_image_load_time: 2.739ms, per_image_inference_time: 0.210ms
2022-02-25 07:55:54 - until epoch: 045, best_acc1: 50.034%
2022-02-25 07:55:54 - epoch 046 lr: 0.010000000000000002
2022-02-25 07:56:34 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 2.2444
2022-02-25 07:57:08 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 2.3831
2022-02-25 07:57:43 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 2.2565
2022-02-25 07:58:16 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 2.4063
2022-02-25 07:58:50 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 2.3498
2022-02-25 07:59:25 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 2.5811
2022-02-25 07:59:59 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 2.3892
2022-02-25 08:00:34 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 2.5237
2022-02-25 08:01:07 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 2.5024
2022-02-25 08:01:42 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 2.2704
2022-02-25 08:02:16 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 2.5039
2022-02-25 08:02:50 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 2.4869
2022-02-25 08:03:23 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 2.8112
2022-02-25 08:03:59 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 2.5218
2022-02-25 08:04:32 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 2.4200
2022-02-25 08:05:07 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 2.6017
2022-02-25 08:05:41 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 2.4345
2022-02-25 08:06:16 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 2.6313
2022-02-25 08:06:50 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 2.3644
2022-02-25 08:07:24 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 2.3616
2022-02-25 08:07:59 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 2.6839
2022-02-25 08:08:33 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 2.1689
2022-02-25 08:09:09 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 2.6620
2022-02-25 08:09:42 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 2.6269
2022-02-25 08:10:17 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 2.4665
2022-02-25 08:10:53 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 2.5460
2022-02-25 08:11:26 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 2.5029
2022-02-25 08:12:01 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 2.4024
2022-02-25 08:12:36 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 2.4815
2022-02-25 08:13:09 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 2.2473
2022-02-25 08:13:43 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 2.4613
2022-02-25 08:14:18 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 2.6283
2022-02-25 08:14:52 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 2.5314
2022-02-25 08:15:28 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 2.6989
2022-02-25 08:16:03 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 2.7367
2022-02-25 08:16:37 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 2.5492
2022-02-25 08:17:12 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 2.2282
2022-02-25 08:17:46 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 2.4888
2022-02-25 08:18:20 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 2.4879
2022-02-25 08:18:55 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 2.3478
2022-02-25 08:19:31 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 2.6110
2022-02-25 08:20:04 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 2.2884
2022-02-25 08:20:39 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 2.7292
2022-02-25 08:21:12 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 2.8102
2022-02-25 08:21:48 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 2.5583
2022-02-25 08:22:22 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 2.4495
2022-02-25 08:22:58 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 2.5581
2022-02-25 08:23:32 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 2.1752
2022-02-25 08:24:08 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 2.6507
2022-02-25 08:24:42 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 2.3945
2022-02-25 08:24:43 - train: epoch 046, train_loss: 2.5326
2022-02-25 08:26:00 - eval: epoch: 046, acc1: 49.834%, acc5: 74.802%, test_loss: 2.1925, per_image_load_time: 2.289ms, per_image_inference_time: 0.207ms
2022-02-25 08:26:01 - until epoch: 046, best_acc1: 50.034%
2022-02-25 08:26:01 - epoch 047 lr: 0.010000000000000002
2022-02-25 08:26:40 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 2.5603
2022-02-25 08:27:14 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 2.8728
2022-02-25 08:27:49 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 2.2922
2022-02-25 08:28:22 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 2.2698
2022-02-25 08:28:58 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 2.2450
2022-02-25 08:29:31 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 2.5679
2022-02-25 08:30:05 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 2.7382
2022-02-25 08:30:40 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 2.6213
2022-02-25 08:31:13 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 2.8727
2022-02-25 08:31:48 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 2.5443
2022-02-25 08:32:22 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 2.8126
2022-02-25 08:32:57 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 2.4677
2022-02-25 08:33:30 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 2.6978
2022-02-25 08:34:06 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 2.5089
2022-02-25 08:34:39 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 2.4683
2022-02-25 08:35:13 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 2.3962
2022-02-25 08:35:48 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 2.5763
2022-02-25 08:36:23 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 2.5877
2022-02-25 08:36:58 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 2.3464
2022-02-25 08:37:32 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 2.5907
2022-02-25 08:38:07 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 2.7374
2022-02-25 08:38:41 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 2.4622
2022-02-25 08:39:15 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 2.7337
2022-02-25 08:39:49 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 2.5253
2022-02-25 08:40:24 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 2.4591
2022-02-25 08:40:59 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 2.6806
2022-02-25 08:41:33 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 2.3350
2022-02-25 08:42:07 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 2.5037
2022-02-25 08:42:42 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 2.4294
2022-02-25 08:43:17 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 2.7017
2022-02-25 08:43:52 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 2.3548
2022-02-25 08:44:27 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 2.6408
2022-02-25 08:45:01 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 2.4515
2022-02-25 08:45:35 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 2.3529
2022-02-25 08:46:09 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 2.6787
2022-02-25 08:46:44 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 2.5565
2022-02-25 08:47:18 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 2.5948
2022-02-25 08:47:53 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 2.5366
2022-02-25 08:48:28 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 2.5558
2022-02-25 08:49:02 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 2.5842
2022-02-25 08:49:37 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 2.5144
2022-02-25 08:50:11 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 2.5178
2022-02-25 08:50:45 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 2.4281
2022-02-25 08:51:21 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 2.6159
2022-02-25 08:51:55 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 2.5526
2022-02-25 08:52:28 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 2.3930
2022-02-25 08:53:04 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 2.7037
2022-02-25 08:53:40 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 2.5366
2022-02-25 08:54:14 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 2.5007
2022-02-25 08:54:49 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 2.5133
2022-02-25 08:54:51 - train: epoch 047, train_loss: 2.5299
2022-02-25 08:56:08 - eval: epoch: 047, acc1: 49.970%, acc5: 75.056%, test_loss: 2.1914, per_image_load_time: 1.684ms, per_image_inference_time: 0.220ms
2022-02-25 08:56:08 - until epoch: 047, best_acc1: 50.034%
2022-02-25 08:56:08 - epoch 048 lr: 0.010000000000000002
2022-02-25 08:56:47 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 2.5661
2022-02-25 08:57:23 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.7905
2022-02-25 08:57:55 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 2.6645
2022-02-25 08:58:29 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 2.5444
2022-02-25 08:59:05 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 2.6237
2022-02-25 08:59:37 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 2.5687
2022-02-25 09:00:13 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 2.4403
2022-02-25 09:00:45 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 2.6896
2022-02-25 09:01:21 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 2.6093
2022-02-25 09:01:54 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 2.6864
2022-02-25 09:02:29 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 2.8791
2022-02-25 09:03:03 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 2.5750
2022-02-25 09:03:37 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 2.2983
2022-02-25 09:04:12 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 2.5618
2022-02-25 09:04:46 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 2.5110
2022-02-25 09:05:21 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 2.5115
2022-02-25 09:05:55 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 2.5586
2022-02-25 09:06:29 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 2.4591
2022-02-25 09:07:04 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 2.5753
2022-02-25 09:07:38 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 2.6100
2022-02-25 09:08:13 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 2.5040
2022-02-25 09:08:48 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 2.7022
2022-02-25 09:09:21 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 2.2925
2022-02-25 09:09:56 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 2.7119
2022-02-25 09:10:30 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 2.6667
2022-02-25 09:11:05 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 2.4446
2022-02-25 09:11:39 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 2.6134
2022-02-25 09:12:13 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 2.3601
2022-02-25 09:12:47 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 2.6347
2022-02-25 09:13:22 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 2.5533
2022-02-25 09:13:56 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 2.5733
2022-02-25 09:14:30 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 2.5191
2022-02-25 09:15:05 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 2.5588
2022-02-25 09:15:39 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 2.5589
2022-02-25 09:16:14 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 2.8231
2022-02-25 09:16:49 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 2.5362
2022-02-25 09:17:23 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 2.6682
2022-02-25 09:17:57 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 2.6173
2022-02-25 09:18:32 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 2.5567
2022-02-25 09:19:06 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 2.5330
2022-02-25 09:19:41 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 2.6637
2022-02-25 09:20:15 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 2.4767
2022-02-25 09:20:48 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 2.3099
2022-02-25 09:21:23 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 2.4790
2022-02-25 09:21:59 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 2.5978
2022-02-25 09:22:33 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 2.4271
2022-02-25 09:23:08 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 2.7222
2022-02-25 09:23:43 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 2.7354
2022-02-25 09:24:19 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 2.3779
2022-02-25 09:24:52 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 2.3839
2022-02-25 09:24:54 - train: epoch 048, train_loss: 2.5311
2022-02-25 09:26:11 - eval: epoch: 048, acc1: 49.416%, acc5: 74.532%, test_loss: 2.2128, per_image_load_time: 2.712ms, per_image_inference_time: 0.226ms
2022-02-25 09:26:11 - until epoch: 048, best_acc1: 50.034%
2022-02-25 09:26:11 - epoch 049 lr: 0.010000000000000002
2022-02-25 09:26:51 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 2.8052
2022-02-25 09:27:25 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 2.3577
2022-02-25 09:27:59 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 2.4818
2022-02-25 09:28:34 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 2.6081
2022-02-25 09:29:07 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 2.6016
2022-02-25 09:29:42 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 2.5774
2022-02-25 09:30:16 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 2.5083
2022-02-25 09:30:50 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 2.8694
2022-02-25 09:31:24 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 2.4322
2022-02-25 09:31:59 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 2.6162
2022-02-25 09:32:34 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 2.6054
2022-02-25 09:33:07 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 2.3066
2022-02-25 09:33:42 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 2.5366
2022-02-25 09:34:16 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 2.6392
2022-02-25 09:34:51 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 2.5010
2022-02-25 09:35:25 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 2.7023
2022-02-25 09:36:00 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 2.6683
2022-02-25 09:36:34 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 2.4385
2022-02-25 09:37:09 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 2.3520
2022-02-25 09:37:43 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 2.3915
2022-02-25 09:38:17 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 2.5160
2022-02-25 09:38:52 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 2.5884
2022-02-25 09:39:25 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 2.5955
2022-02-25 09:40:00 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 2.7061
2022-02-25 09:40:34 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 2.6498
2022-02-25 09:41:10 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 2.5197
2022-02-25 09:41:44 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 2.6354
2022-02-25 09:42:18 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 2.4487
2022-02-25 09:42:52 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 2.6430
2022-02-25 09:43:27 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 2.5268
2022-02-25 09:44:00 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 2.6091
2022-02-25 09:44:35 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 2.5990
2022-02-25 09:45:08 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 2.4535
2022-02-25 09:45:43 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 2.6720
2022-02-25 09:46:17 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 2.5807
2022-02-25 09:46:52 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 2.4990
2022-02-25 09:47:27 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 2.5324
2022-02-25 09:48:03 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 2.5023
2022-02-25 09:48:36 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 2.7007
2022-02-25 09:49:11 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 2.2627
2022-02-25 09:49:45 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 2.4591
2022-02-25 09:50:20 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 2.6476
2022-02-25 09:50:53 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 2.6187
2022-02-25 09:51:28 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 2.5332
2022-02-25 09:52:03 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 2.5033
2022-02-25 09:52:40 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 2.6528
2022-02-25 09:53:15 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 2.6574
2022-02-25 09:53:50 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 2.5017
2022-02-25 09:54:25 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 2.6158
2022-02-25 09:54:59 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 2.5209
2022-02-25 09:55:01 - train: epoch 049, train_loss: 2.5268
2022-02-25 09:56:19 - eval: epoch: 049, acc1: 49.316%, acc5: 74.424%, test_loss: 2.2090, per_image_load_time: 2.733ms, per_image_inference_time: 0.220ms
2022-02-25 09:56:19 - until epoch: 049, best_acc1: 50.034%
2022-02-25 09:56:19 - epoch 050 lr: 0.010000000000000002
2022-02-25 09:56:58 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 2.6704
2022-02-25 09:57:32 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 2.4307
2022-02-25 09:58:06 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 2.5359
2022-02-25 09:58:41 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 2.5051
2022-02-25 09:59:14 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 2.5439
2022-02-25 09:59:49 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 2.6092
2022-02-25 10:00:23 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 2.3489
2022-02-25 10:00:57 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 2.2711
2022-02-25 10:01:32 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 2.6381
2022-02-25 10:02:06 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 2.5854
2022-02-25 10:02:41 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 2.8321
2022-02-25 10:03:15 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 2.4808
2022-02-25 10:03:49 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 2.2893
2022-02-25 10:04:23 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 2.6368
2022-02-25 10:04:59 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 2.4542
2022-02-25 10:05:32 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 2.6123
2022-02-25 10:06:08 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 2.5858
2022-02-25 10:06:42 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.6495
2022-02-25 10:07:16 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 2.3095
2022-02-25 10:07:50 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 2.3695
2022-02-25 10:08:26 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 2.3674
2022-02-25 10:09:00 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 2.4762
2022-02-25 10:09:35 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 2.3820
2022-02-25 10:10:09 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 2.6562
2022-02-25 10:10:44 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 2.7487
2022-02-25 10:11:18 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 2.3815
2022-02-25 10:11:52 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 2.4612
2022-02-25 10:12:27 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 2.4048
2022-02-25 10:13:03 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 2.6421
2022-02-25 10:13:36 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 2.9211
2022-02-25 10:14:13 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 2.5832
2022-02-25 10:14:47 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 2.7286
2022-02-25 10:15:22 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 2.4002
2022-02-25 10:15:56 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 2.6134
2022-02-25 10:16:31 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 2.5027
2022-02-25 10:17:05 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 2.5370
2022-02-25 10:17:40 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 2.9193
2022-02-25 10:18:14 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 2.4938
2022-02-25 10:18:49 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 2.4220
2022-02-25 10:19:23 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 2.6613
2022-02-25 10:19:58 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 2.5327
2022-02-25 10:20:32 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 2.6776
2022-02-25 10:21:07 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 2.7528
2022-02-25 10:21:41 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 2.4174
2022-02-25 10:22:17 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 2.6109
2022-02-25 10:22:51 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 2.4865
2022-02-25 10:23:24 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 2.6563
2022-02-25 10:24:00 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 2.4439
2022-02-25 10:24:36 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 2.4046
2022-02-25 10:25:10 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 2.4443
2022-02-25 10:25:12 - train: epoch 050, train_loss: 2.5288
2022-02-25 10:26:29 - eval: epoch: 050, acc1: 49.724%, acc5: 75.016%, test_loss: 2.1937, per_image_load_time: 2.187ms, per_image_inference_time: 0.212ms
2022-02-25 10:26:29 - until epoch: 050, best_acc1: 50.034%
2022-02-25 10:26:29 - epoch 051 lr: 0.010000000000000002
2022-02-25 10:27:10 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 2.8144
2022-02-25 10:27:44 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 2.8266
2022-02-25 10:28:17 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 2.4565
2022-02-25 10:28:51 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 2.3333
2022-02-25 10:29:26 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 2.5543
2022-02-25 10:30:00 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 2.3029
2022-02-25 10:30:34 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 2.4272
2022-02-25 10:31:09 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 2.5560
2022-02-25 10:31:43 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 2.2984
2022-02-25 10:32:18 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.6628
2022-02-25 10:32:52 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 2.5279
2022-02-25 10:33:26 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 2.3027
2022-02-25 10:34:00 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 2.3995
2022-02-25 10:34:34 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 2.5326
2022-02-25 10:35:09 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 2.5212
2022-02-25 10:35:44 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 2.2405
2022-02-25 10:36:17 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 2.5003
2022-02-25 10:36:52 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 2.5266
2022-02-25 10:37:26 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 2.5400
2022-02-25 10:38:00 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 2.4333
2022-02-25 10:38:35 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 2.4362
2022-02-25 10:39:09 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 2.4996
2022-02-25 10:39:44 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 2.6167
2022-02-25 10:40:17 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 2.4544
2022-02-25 10:40:51 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 2.4744
2022-02-25 10:41:26 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 2.3005
2022-02-25 10:42:01 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 2.3864
2022-02-25 10:42:35 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 2.4555
2022-02-25 10:43:10 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 2.5305
2022-02-25 10:43:44 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 2.2981
2022-02-25 10:44:19 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 2.5206
2022-02-25 10:44:52 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 2.4602
2022-02-25 10:45:27 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 2.7285
2022-02-25 10:46:01 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 2.4395
2022-02-25 10:46:35 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 2.5095
2022-02-25 10:47:09 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 2.4731
2022-02-25 10:47:44 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 2.6651
2022-02-25 10:48:18 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 2.4602
2022-02-25 10:48:53 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 2.6400
2022-02-25 10:49:27 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 2.7770
2022-02-25 10:50:02 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 2.5506
2022-02-25 10:50:36 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 2.6906
2022-02-25 10:51:12 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 2.4761
2022-02-25 10:51:45 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 2.5227
2022-02-25 10:52:20 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 2.2029
2022-02-25 10:52:55 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 2.5964
2022-02-25 10:53:30 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 2.5037
2022-02-25 10:54:06 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 2.7260
2022-02-25 10:54:40 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 2.5503
2022-02-25 10:55:15 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 2.3865
2022-02-25 10:55:16 - train: epoch 051, train_loss: 2.5287
2022-02-25 10:56:34 - eval: epoch: 051, acc1: 49.836%, acc5: 74.784%, test_loss: 2.1996, per_image_load_time: 2.129ms, per_image_inference_time: 0.211ms
2022-02-25 10:56:34 - until epoch: 051, best_acc1: 50.034%
2022-02-25 10:56:34 - epoch 052 lr: 0.010000000000000002
2022-02-25 10:57:14 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 2.5738
2022-02-25 10:57:48 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 2.4559
2022-02-25 10:58:22 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 2.6001
2022-02-25 10:58:56 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 2.5143
2022-02-25 10:59:30 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 2.4787
2022-02-25 11:00:04 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 2.6277
2022-02-25 11:00:39 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 2.5110
2022-02-25 11:01:12 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 2.3200
2022-02-25 11:01:47 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 2.6939
2022-02-25 11:02:21 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 2.5482
2022-02-25 11:02:56 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 2.6857
2022-02-25 11:03:29 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 2.3513
2022-02-25 11:04:04 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 2.5024
2022-02-25 11:04:39 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 2.7034
2022-02-25 11:05:12 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 2.3101
2022-02-25 11:05:47 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 2.5164
2022-02-25 11:06:21 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 2.4148
2022-02-25 11:06:55 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 2.5757
2022-02-25 11:07:31 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 2.3747
2022-02-25 11:08:05 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 2.7744
2022-02-25 11:08:39 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 2.5315
2022-02-25 11:09:14 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 2.6995
2022-02-25 11:09:49 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 2.5113
2022-02-25 11:10:23 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 2.2665
2022-02-25 11:10:58 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 2.3119
2022-02-25 11:11:33 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 2.3206
2022-02-25 11:12:06 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 2.5775
2022-02-25 11:12:40 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 2.5434
2022-02-25 11:13:15 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 2.3101
2022-02-25 11:13:50 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 2.4602
2022-02-25 11:14:24 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 2.2416
2022-02-25 11:14:58 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 2.5101
2022-02-25 11:15:33 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 2.4899
2022-02-25 11:16:08 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 2.6234
2022-02-25 11:16:42 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 2.4834
2022-02-25 11:17:17 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 2.6704
2022-02-25 11:17:52 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 2.7558
2022-02-25 11:18:25 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 2.5220
2022-02-25 11:19:00 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 2.3603
2022-02-25 11:19:35 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 2.5850
2022-02-25 11:20:10 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 2.4145
2022-02-25 11:20:45 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 2.5978
2022-02-25 11:21:20 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 2.4662
2022-02-25 11:21:54 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 2.6215
2022-02-25 11:22:29 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 2.5658
2022-02-25 11:23:03 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 2.4009
2022-02-25 11:23:38 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 2.4600
2022-02-25 11:24:13 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 2.3520
2022-02-25 11:24:49 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 2.4861
2022-02-25 11:25:23 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 2.4934
2022-02-25 11:25:25 - train: epoch 052, train_loss: 2.5268
2022-02-25 11:26:42 - eval: epoch: 052, acc1: 49.622%, acc5: 74.740%, test_loss: 2.2050, per_image_load_time: 2.730ms, per_image_inference_time: 0.211ms
2022-02-25 11:26:42 - until epoch: 052, best_acc1: 50.034%
2022-02-25 11:26:42 - epoch 053 lr: 0.010000000000000002
2022-02-25 11:27:22 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 2.5032
2022-02-25 11:27:56 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 2.6474
2022-02-25 11:28:29 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 2.6340
2022-02-25 11:29:05 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 2.7511
2022-02-25 11:29:38 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 2.2994
2022-02-25 11:30:13 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 2.6760
2022-02-25 11:30:47 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 2.6388
2022-02-25 11:31:21 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 2.5200
2022-02-25 11:31:56 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 2.5111
2022-02-25 11:32:29 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 2.7030
2022-02-25 11:33:04 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 2.6386
2022-02-25 11:33:38 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 2.3986
2022-02-25 11:34:13 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 2.5494
2022-02-25 11:34:47 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 2.7410
2022-02-25 11:35:22 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 2.3357
2022-02-25 11:35:55 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 2.5896
2022-02-25 11:36:30 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 2.6483
2022-02-25 11:37:04 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 2.5037
2022-02-25 11:37:39 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 2.4914
2022-02-25 11:38:13 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 2.5427
2022-02-25 11:38:48 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 2.7670
2022-02-25 11:39:23 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 2.4089
2022-02-25 11:39:59 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 2.6012
2022-02-25 11:40:33 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 2.4308
2022-02-25 11:41:07 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 2.6962
2022-02-25 11:41:43 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 2.6456
2022-02-25 11:42:17 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 2.6252
2022-02-25 11:42:51 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 2.7301
2022-02-25 11:43:26 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 2.2672
2022-02-25 11:44:00 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 2.2887
2022-02-25 11:44:34 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 2.9111
2022-02-25 11:45:08 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 2.7353
2022-02-25 11:45:43 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 2.4862
2022-02-25 11:46:17 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 2.4488
2022-02-25 11:46:52 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 2.4566
2022-02-25 11:47:27 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 2.4290
2022-02-25 11:48:00 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 2.6142
2022-02-25 11:48:36 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 2.3923
2022-02-25 11:49:10 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 2.6800
2022-02-25 11:49:45 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 2.6765
2022-02-25 11:50:19 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 2.5778
2022-02-25 11:50:54 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 2.6190
2022-02-25 11:51:27 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 2.8922
2022-02-25 11:52:03 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 2.4828
2022-02-25 11:52:38 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 2.5473
2022-02-25 11:53:13 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 2.5515
2022-02-25 11:53:48 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 2.4470
2022-02-25 11:54:22 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 2.7883
2022-02-25 11:54:58 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 2.5817
2022-02-25 11:55:31 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 2.5071
2022-02-25 11:55:34 - train: epoch 053, train_loss: 2.5256
2022-02-25 11:56:51 - eval: epoch: 053, acc1: 49.628%, acc5: 74.984%, test_loss: 2.1961, per_image_load_time: 2.615ms, per_image_inference_time: 0.238ms
2022-02-25 11:56:51 - until epoch: 053, best_acc1: 50.034%
2022-02-25 11:56:51 - epoch 054 lr: 0.010000000000000002
2022-02-25 11:57:31 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 2.4941
2022-02-25 11:58:05 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 2.4751
2022-02-25 11:58:39 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 2.6954
2022-02-25 11:59:14 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 2.3975
2022-02-25 11:59:47 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 2.4492
2022-02-25 12:00:22 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 2.5081
2022-02-25 12:00:56 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 2.7406
2022-02-25 12:01:31 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 2.8004
2022-02-25 12:02:05 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 2.3130
2022-02-25 12:02:39 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 2.2841
2022-02-25 12:03:14 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 2.2995
2022-02-25 12:03:47 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 2.6878
2022-02-25 12:04:21 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 2.3178
2022-02-25 12:04:56 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 2.7869
2022-02-25 12:05:31 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 2.5810
2022-02-25 12:06:05 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 2.4781
2022-02-25 12:06:40 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 2.4696
2022-02-25 12:07:14 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 2.4563
2022-02-25 12:07:49 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.6489
2022-02-25 12:08:22 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 2.6977
2022-02-25 12:08:56 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 2.2574
2022-02-25 12:09:31 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 2.6904
2022-02-25 12:10:07 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 2.2797
2022-02-25 12:10:40 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 2.7394
2022-02-25 12:11:15 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 2.6972
2022-02-25 12:11:49 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 2.6817
2022-02-25 12:12:24 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 2.4234
2022-02-25 12:12:57 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 2.6876
2022-02-25 12:13:33 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 2.2630
2022-02-25 12:14:07 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 2.6758
2022-02-25 12:14:42 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 2.6256
2022-02-25 12:15:16 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 2.6849
2022-02-25 12:15:51 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 2.5444
2022-02-25 12:16:25 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 2.4179
2022-02-25 12:17:00 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 2.5908
2022-02-25 12:17:34 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 2.5511
2022-02-25 12:18:08 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 2.4176
2022-02-25 12:18:42 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 2.4700
2022-02-25 12:19:16 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 2.6146
2022-02-25 12:19:52 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 2.4638
2022-02-25 12:20:26 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 2.4511
2022-02-25 12:21:01 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 2.7760
2022-02-25 12:21:35 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 2.7089
2022-02-25 12:22:10 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 2.2540
2022-02-25 12:22:43 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 2.6443
2022-02-25 12:23:18 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 2.7136
2022-02-25 12:23:53 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 2.8509
2022-02-25 12:24:29 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 2.6405
2022-02-25 12:25:04 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 2.5126
2022-02-25 12:25:38 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 2.3998
2022-02-25 12:25:40 - train: epoch 054, train_loss: 2.5278
2022-02-25 12:26:57 - eval: epoch: 054, acc1: 49.464%, acc5: 74.678%, test_loss: 2.2067, per_image_load_time: 2.714ms, per_image_inference_time: 0.210ms
2022-02-25 12:26:57 - until epoch: 054, best_acc1: 50.034%
2022-02-25 12:26:57 - epoch 055 lr: 0.010000000000000002
2022-02-25 12:27:36 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 2.4774
2022-02-25 12:28:11 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 2.7440
2022-02-25 12:28:45 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 2.4036
2022-02-25 12:29:18 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 2.5398
2022-02-25 12:29:53 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 2.5271
2022-02-25 12:30:27 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 2.5238
2022-02-25 12:31:02 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 2.6082
2022-02-25 12:31:35 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 2.2753
2022-02-25 12:32:10 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 2.6923
2022-02-25 12:32:44 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 2.4996
2022-02-25 12:33:19 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 2.4957
2022-02-25 12:33:53 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 2.5678
2022-02-25 12:34:28 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 2.8256
2022-02-25 12:35:02 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 2.4370
2022-02-25 12:35:36 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 2.4490
2022-02-25 12:36:11 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 2.6098
2022-02-25 12:36:46 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 2.6655
2022-02-25 12:37:19 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 2.5334
2022-02-25 12:37:54 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 2.5886
2022-02-25 12:38:29 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 2.4805
2022-02-25 12:39:04 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 2.2085
2022-02-25 12:39:39 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 2.7465
2022-02-25 12:40:13 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 2.4042
2022-02-25 12:40:48 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 2.3550
2022-02-25 12:41:23 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 2.5329
2022-02-25 12:41:56 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 2.6433
2022-02-25 12:42:31 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 2.4954
2022-02-25 12:43:05 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 2.7566
2022-02-25 12:43:41 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 2.5887
2022-02-25 12:44:15 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 2.5828
2022-02-25 12:44:50 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 2.5171
2022-02-25 12:45:24 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 2.4604
2022-02-25 12:46:00 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 2.0445
2022-02-25 12:46:34 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 2.4713
2022-02-25 12:47:09 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 2.3459
2022-02-25 12:47:42 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 2.5923
2022-02-25 12:48:18 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 2.3739
2022-02-25 12:48:52 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 2.4810
2022-02-25 12:49:26 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 2.6652
2022-02-25 12:50:01 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 2.4141
2022-02-25 12:50:36 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 2.4074
2022-02-25 12:51:11 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 2.6825
2022-02-25 12:51:46 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 2.6041
2022-02-25 12:52:20 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 2.4878
2022-02-25 12:52:56 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 2.6031
2022-02-25 12:53:30 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 2.3756
2022-02-25 12:54:05 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 2.3217
2022-02-25 12:54:40 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 2.7017
2022-02-25 12:55:15 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 2.4188
2022-02-25 12:55:49 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 2.5921
2022-02-25 12:55:51 - train: epoch 055, train_loss: 2.5230
2022-02-25 12:57:08 - eval: epoch: 055, acc1: 50.002%, acc5: 74.922%, test_loss: 2.1857, per_image_load_time: 2.714ms, per_image_inference_time: 0.205ms
2022-02-25 12:57:08 - until epoch: 055, best_acc1: 50.034%
2022-02-25 12:57:08 - epoch 056 lr: 0.010000000000000002
2022-02-25 12:57:48 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 2.5632
2022-02-25 12:58:22 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 2.6447
2022-02-25 12:58:55 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 2.4896
2022-02-25 12:59:30 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 2.5743
2022-02-25 13:00:05 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 2.4871
2022-02-25 13:00:39 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 2.6903
2022-02-25 13:01:13 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 2.4415
2022-02-25 13:01:47 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 2.7841
2022-02-25 13:02:23 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 2.4311
2022-02-25 13:02:57 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 2.3059
2022-02-25 13:03:30 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 2.3502
2022-02-25 13:04:05 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 2.7533
2022-02-25 13:04:40 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 2.5722
2022-02-25 13:05:13 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 2.7335
2022-02-25 13:05:48 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 2.8356
2022-02-25 13:06:23 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 2.2138
2022-02-25 13:06:56 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 2.7736
2022-02-25 13:07:31 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 2.6738
2022-02-25 13:08:06 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 2.6511
2022-02-25 13:08:40 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 2.5019
2022-02-25 13:09:14 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 2.6261
2022-02-25 13:09:49 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 2.5304
2022-02-25 13:10:24 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 2.6889
2022-02-25 13:10:58 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 2.4735
2022-02-25 13:11:33 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 2.5968
2022-02-25 13:12:07 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 2.4966
2022-02-25 13:12:41 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 2.4338
2022-02-25 13:13:16 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 2.4459
2022-02-25 13:13:50 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 2.5675
2022-02-25 13:14:25 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 2.5914
2022-02-25 13:14:58 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 2.6085
2022-02-25 13:15:33 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 2.5944
2022-02-25 13:16:07 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 2.6301
2022-02-25 13:16:42 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 2.4306
2022-02-25 13:17:16 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 2.3369
2022-02-25 13:17:51 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 2.3688
2022-02-25 13:18:25 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 2.6096
2022-02-25 13:19:00 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 2.5627
2022-02-25 13:19:34 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 2.8875
2022-02-25 13:20:09 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 2.5972
2022-02-25 13:20:44 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 2.7946
2022-02-25 13:21:18 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 2.5349
2022-02-25 13:21:52 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 2.4617
2022-02-25 13:22:27 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 2.5367
2022-02-25 13:23:02 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 2.5073
2022-02-25 13:23:36 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 2.4856
2022-02-25 13:24:10 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 2.4054
2022-02-25 13:24:44 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 2.4564
2022-02-25 13:25:20 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 2.6239
2022-02-25 13:25:53 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 2.6694
2022-02-25 13:25:56 - train: epoch 056, train_loss: 2.5220
2022-02-25 13:27:13 - eval: epoch: 056, acc1: 49.880%, acc5: 74.984%, test_loss: 2.1877, per_image_load_time: 2.715ms, per_image_inference_time: 0.229ms
2022-02-25 13:27:13 - until epoch: 056, best_acc1: 50.034%
2022-02-25 13:27:13 - epoch 057 lr: 0.010000000000000002
2022-02-25 13:27:53 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 2.5626
2022-02-25 13:28:28 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 2.5732
2022-02-25 13:29:01 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 2.6090
2022-02-25 13:29:35 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 2.4427
2022-02-25 13:30:10 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 2.6436
2022-02-25 13:30:43 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 2.4155
2022-02-25 13:31:18 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 2.3173
2022-02-25 13:31:52 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 2.4255
2022-02-25 13:32:26 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 2.4544
2022-02-25 13:33:00 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 2.4157
2022-02-25 13:33:34 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 2.2055
2022-02-25 13:34:09 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 2.5159
2022-02-25 13:34:43 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 2.5888
2022-02-25 13:35:17 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 2.6861
2022-02-25 13:35:51 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 2.5818
2022-02-25 13:36:26 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 2.5324
2022-02-25 13:37:00 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 2.5197
2022-02-25 13:37:33 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 2.5446
2022-02-25 13:38:07 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 2.4941
2022-02-25 13:38:42 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 2.5772
2022-02-25 13:39:16 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 2.5055
2022-02-25 13:39:51 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 2.3005
2022-02-25 13:40:25 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 2.7655
2022-02-25 13:41:00 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 2.5014
2022-02-25 13:41:34 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 2.6980
2022-02-25 13:42:09 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 2.4203
2022-02-25 13:42:43 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 2.1511
2022-02-25 13:43:17 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 2.4143
2022-02-25 13:43:51 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 2.5391
2022-02-25 13:44:26 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 2.6712
2022-02-25 13:45:01 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 2.6236
2022-02-25 13:45:34 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 2.5807
2022-02-25 13:46:10 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 2.6867
2022-02-25 13:46:43 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 2.7482
2022-02-25 13:47:17 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 2.5213
2022-02-25 13:47:52 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 2.7063
2022-02-25 13:48:27 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 2.4690
2022-02-25 13:49:01 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 2.5208
2022-02-25 13:49:34 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 2.7206
2022-02-25 13:50:10 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 2.3955
2022-02-25 13:50:45 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 2.6192
2022-02-25 13:51:19 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 2.5355
2022-02-25 13:51:53 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 2.4691
2022-02-25 13:52:28 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 2.6466
2022-02-25 13:53:02 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 2.7062
2022-02-25 13:53:37 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 2.5414
2022-02-25 13:54:11 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 2.5785
2022-02-25 13:54:47 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 2.7333
2022-02-25 13:55:21 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 2.6822
2022-02-25 13:55:56 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 2.5365
2022-02-25 13:55:57 - train: epoch 057, train_loss: 2.5194
2022-02-25 13:57:15 - eval: epoch: 057, acc1: 49.936%, acc5: 74.872%, test_loss: 2.1911, per_image_load_time: 2.653ms, per_image_inference_time: 0.215ms
2022-02-25 13:57:15 - until epoch: 057, best_acc1: 50.034%
2022-02-25 13:57:15 - epoch 058 lr: 0.010000000000000002
2022-02-25 13:57:54 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 2.5246
2022-02-25 13:58:28 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 2.5761
2022-02-25 13:59:02 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 2.6615
2022-02-25 13:59:35 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 2.4490
2022-02-25 14:00:10 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 2.3448
2022-02-25 14:00:45 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 2.4372
2022-02-25 14:01:20 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 2.3321
2022-02-25 14:01:53 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 2.3363
2022-02-25 14:02:27 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 2.3314
2022-02-25 14:03:01 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 2.6153
2022-02-25 14:03:35 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 2.3390
2022-02-25 14:04:10 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 2.5853
2022-02-25 14:04:44 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.6799
2022-02-25 14:05:19 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 2.6053
2022-02-25 14:05:53 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 2.3920
2022-02-25 14:06:27 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 2.5722
2022-02-25 14:07:01 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 2.4966
2022-02-25 14:07:36 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 2.6378
2022-02-25 14:08:10 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 2.5978
2022-02-25 14:08:45 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 2.6996
2022-02-25 14:09:19 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 2.6252
2022-02-25 14:09:54 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 2.4299
2022-02-25 14:10:28 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 2.6391
2022-02-25 14:11:02 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 2.8281
2022-02-25 14:11:36 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 2.6169
2022-02-25 14:12:11 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 2.3203
2022-02-25 14:12:45 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 2.7537
2022-02-25 14:13:20 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 2.4226
2022-02-25 14:13:54 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 2.3037
2022-02-25 14:14:28 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 2.7477
2022-02-25 14:15:02 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 2.4785
2022-02-25 14:15:37 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 2.3889
2022-02-25 14:16:11 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 2.3963
2022-02-25 14:16:45 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 2.4638
2022-02-25 14:17:20 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 2.4031
2022-02-25 14:17:54 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 2.2833
2022-02-25 14:18:29 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 2.3898
2022-02-25 14:19:02 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 2.4862
2022-02-25 14:19:38 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 2.5470
2022-02-25 14:20:12 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 2.5602
2022-02-25 14:20:47 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 2.5012
2022-02-25 14:21:20 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 2.4235
2022-02-25 14:21:55 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 2.4077
2022-02-25 14:22:29 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 2.3882
2022-02-25 14:23:04 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 2.3517
2022-02-25 14:23:39 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 2.2661
2022-02-25 14:24:14 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 2.7168
2022-02-25 14:24:49 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 2.6432
2022-02-25 14:25:24 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 2.2649
2022-02-25 14:25:58 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 2.4335
2022-02-25 14:26:00 - train: epoch 058, train_loss: 2.5190
2022-02-25 14:27:18 - eval: epoch: 058, acc1: 49.892%, acc5: 74.924%, test_loss: 2.1875, per_image_load_time: 2.733ms, per_image_inference_time: 0.234ms
2022-02-25 14:27:18 - until epoch: 058, best_acc1: 50.034%
2022-02-25 14:27:18 - epoch 059 lr: 0.010000000000000002
2022-02-25 14:27:57 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 2.4892
2022-02-25 14:28:31 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 2.4561
2022-02-25 14:29:06 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 2.5352
2022-02-25 14:29:40 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 2.6693
2022-02-25 14:30:13 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 2.4440
2022-02-25 14:30:47 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 2.5027
2022-02-25 14:31:22 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 2.4248
2022-02-25 14:31:57 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 2.5076
2022-02-25 14:32:30 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 2.3971
2022-02-25 14:33:04 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 2.6050
2022-02-25 14:33:39 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 2.9158
2022-02-25 14:34:14 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 2.2565
2022-02-25 14:34:48 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 2.8230
2022-02-25 14:35:22 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 2.5850
2022-02-25 14:35:56 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 2.4693
2022-02-25 14:36:31 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 2.4031
2022-02-25 14:37:06 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 2.6123
2022-02-25 14:37:39 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 2.3551
2022-02-25 14:38:14 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 2.6953
2022-02-25 14:38:49 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 2.2446
2022-02-25 14:39:24 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 2.5040
2022-02-25 14:39:57 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 2.6011
2022-02-25 14:40:32 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 2.4700
2022-02-25 14:41:06 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 2.7191
2022-02-25 14:41:41 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 2.5283
2022-02-25 14:42:15 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 2.3766
2022-02-25 14:42:49 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 2.6403
2022-02-25 14:43:24 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 2.7701
2022-02-25 14:43:58 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 2.3852
2022-02-25 14:44:32 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.8265
2022-02-25 14:45:08 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 2.3797
2022-02-25 14:45:41 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 2.6859
2022-02-25 14:46:16 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 2.4716
2022-02-25 14:46:51 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.8800
2022-02-25 14:47:26 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 2.4491
2022-02-25 14:48:00 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 2.5767
2022-02-25 14:48:35 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 2.6699
2022-02-25 14:49:10 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 2.3991
2022-02-25 14:49:44 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 2.2455
2022-02-25 14:50:18 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 2.6459
2022-02-25 14:50:53 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 2.5479
2022-02-25 14:51:29 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 2.6917
2022-02-25 14:52:03 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 2.6408
2022-02-25 14:52:38 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 2.6137
2022-02-25 14:53:11 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 2.6805
2022-02-25 14:53:47 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 2.5515
2022-02-25 14:54:22 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 2.4600
2022-02-25 14:54:57 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 2.4895
2022-02-25 14:55:33 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 2.5907
2022-02-25 14:56:06 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 2.9012
2022-02-25 14:56:09 - train: epoch 059, train_loss: 2.5186
2022-02-25 14:57:26 - eval: epoch: 059, acc1: 49.752%, acc5: 74.894%, test_loss: 2.1981, per_image_load_time: 2.720ms, per_image_inference_time: 0.249ms
2022-02-25 14:57:26 - until epoch: 059, best_acc1: 50.034%
2022-02-25 14:57:26 - epoch 060 lr: 0.010000000000000002
2022-02-25 14:58:06 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 2.5254
2022-02-25 14:58:41 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 2.5670
2022-02-25 14:59:13 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 2.4766
2022-02-25 14:59:48 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 2.7692
2022-02-25 15:00:23 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 2.8837
2022-02-25 15:00:56 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 2.5544
2022-02-25 15:01:31 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 2.5963
2022-02-25 15:02:05 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 2.7080
2022-02-25 15:02:39 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 2.4795
2022-02-25 15:03:15 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 2.2368
2022-02-25 15:03:48 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 2.3712
2022-02-25 15:04:21 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 2.3247
2022-02-25 15:04:57 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 2.6274
2022-02-25 15:05:30 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 2.3859
2022-02-25 15:06:05 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 2.6836
2022-02-25 15:06:38 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 2.3824
2022-02-25 15:07:13 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 2.5186
2022-02-25 15:07:47 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 2.7213
2022-02-25 15:08:22 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 2.6170
2022-02-25 15:08:57 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 2.4480
2022-02-25 15:09:31 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 2.5792
2022-02-25 15:10:05 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 2.6042
2022-02-25 15:10:40 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 2.3334
2022-02-25 15:11:14 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 2.5050
2022-02-25 15:11:49 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 2.4636
2022-02-25 15:12:23 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 2.6690
2022-02-25 15:12:58 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 2.4148
2022-02-25 15:13:31 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 2.6630
2022-02-25 15:14:06 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 2.4590
2022-02-25 15:14:40 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 2.5849
2022-02-25 15:15:14 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 2.5893
2022-02-25 15:15:50 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 2.3684
2022-02-25 15:16:23 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 2.2725
2022-02-25 15:16:57 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 2.6818
2022-02-25 15:17:31 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 2.5378
2022-02-25 15:18:04 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 2.6277
2022-02-25 15:18:39 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 2.6887
2022-02-25 15:19:13 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 2.5381
2022-02-25 15:19:48 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 2.6899
2022-02-25 15:20:22 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 2.5803
2022-02-25 15:20:57 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 2.5535
2022-02-25 15:21:31 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 2.5669
2022-02-25 15:22:06 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 2.3998
2022-02-25 15:22:41 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 2.6650
2022-02-25 15:23:15 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 2.5124
2022-02-25 15:23:50 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 2.4198
2022-02-25 15:24:24 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 2.6561
2022-02-25 15:25:00 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 2.3659
2022-02-25 15:25:34 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 2.4952
2022-02-25 15:26:09 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 2.5337
2022-02-25 15:26:10 - train: epoch 060, train_loss: 2.5173
2022-02-25 15:27:29 - eval: epoch: 060, acc1: 49.760%, acc5: 74.622%, test_loss: 2.1937, per_image_load_time: 1.994ms, per_image_inference_time: 0.205ms
2022-02-25 15:27:29 - until epoch: 060, best_acc1: 50.034%
2022-02-25 15:27:29 - epoch 061 lr: 0.0010000000000000002
2022-02-25 15:28:08 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 2.3069
2022-02-25 15:28:42 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 2.3564
2022-02-25 15:29:16 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 2.2407
2022-02-25 15:29:50 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 2.4199
2022-02-25 15:30:24 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 2.3751
2022-02-25 15:30:57 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 2.4602
2022-02-25 15:31:32 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 2.3697
2022-02-25 15:32:06 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 2.3349
2022-02-25 15:32:41 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 2.3661
2022-02-25 15:33:15 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 2.3024
2022-02-25 15:33:49 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 2.2384
2022-02-25 15:34:22 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 2.1677
2022-02-25 15:34:57 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 2.0959
2022-02-25 15:35:31 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 2.2603
2022-02-25 15:36:05 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 2.3631
2022-02-25 15:36:40 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 2.1420
2022-02-25 15:37:14 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 2.3351
2022-02-25 15:37:49 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 2.5262
2022-02-25 15:38:22 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 2.4960
2022-02-25 15:38:57 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 2.5339
2022-02-25 15:39:31 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 2.5359
2022-02-25 15:40:06 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 2.2552
2022-02-25 15:40:39 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 2.1455
2022-02-25 15:41:14 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 2.2821
2022-02-25 15:41:49 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 2.2678
2022-02-25 15:42:23 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 2.3782
2022-02-25 15:42:57 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 2.4070
2022-02-25 15:43:32 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 2.2810
2022-02-25 15:44:05 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 2.2614
2022-02-25 15:44:40 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 2.4598
2022-02-25 15:45:16 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 2.4535
2022-02-25 15:45:50 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 2.1922
2022-02-25 15:46:24 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 2.5679
2022-02-25 15:46:58 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 2.2650
2022-02-25 15:47:33 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 2.3968
2022-02-25 15:48:07 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 2.2713
2022-02-25 15:48:43 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 2.2499
2022-02-25 15:49:16 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 2.3688
2022-02-25 15:49:51 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 2.2637
2022-02-25 15:50:25 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 2.1660
2022-02-25 15:51:00 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 2.4156
2022-02-25 15:51:34 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 2.3632
2022-02-25 15:52:09 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 2.2993
2022-02-25 15:52:43 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 2.3631
2022-02-25 15:53:18 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 2.3129
2022-02-25 15:53:53 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 2.1617
2022-02-25 15:54:27 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 2.3286
2022-02-25 15:55:03 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 2.3009
2022-02-25 15:55:37 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 2.2206
2022-02-25 15:56:11 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 2.5947
2022-02-25 15:56:13 - train: epoch 061, train_loss: 2.3602
2022-02-25 15:57:31 - eval: epoch: 061, acc1: 53.406%, acc5: 77.676%, test_loss: 2.0168, per_image_load_time: 2.736ms, per_image_inference_time: 0.231ms
2022-02-25 15:57:31 - until epoch: 061, best_acc1: 53.406%
2022-02-25 15:57:31 - epoch 062 lr: 0.0010000000000000002
2022-02-25 15:58:11 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 2.4833
2022-02-25 15:58:44 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 2.4552
2022-02-25 15:59:19 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 2.5230
2022-02-25 15:59:53 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 2.1871
2022-02-25 16:00:28 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 2.4206
2022-02-25 16:01:01 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 2.1145
2022-02-25 16:01:35 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 2.3168
2022-02-25 16:02:10 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 2.3706
2022-02-25 16:02:45 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 2.4257
2022-02-25 16:03:19 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 2.6788
2022-02-25 16:03:53 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 2.3427
2022-02-25 16:04:28 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 2.4032
2022-02-25 16:05:01 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 2.4082
2022-02-25 16:05:35 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 2.2302
2022-02-25 16:06:09 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 2.2613
2022-02-25 16:06:45 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 2.7289
2022-02-25 16:07:19 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 2.3551
2022-02-25 16:07:53 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 2.2583
2022-02-25 16:08:28 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 2.3519
2022-02-25 16:09:02 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 2.4619
2022-02-25 16:09:37 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 2.5440
2022-02-25 16:10:10 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 2.1184
2022-02-25 16:10:46 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 2.1761
2022-02-25 16:11:19 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 2.2831
2022-02-25 16:11:54 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 2.4363
2022-02-25 16:12:28 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 2.3011
2022-02-25 16:13:03 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 2.4312
2022-02-25 16:13:38 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 2.3438
2022-02-25 16:14:12 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 2.3183
2022-02-25 16:14:46 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 2.6528
2022-02-25 16:15:21 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 2.2557
2022-02-25 16:15:55 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 2.1596
2022-02-25 16:16:30 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 2.3682
2022-02-25 16:17:03 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 2.6049
2022-02-25 16:17:38 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 2.4673
2022-02-25 16:18:12 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 2.4294
2022-02-25 16:18:47 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 2.1553
2022-02-25 16:19:21 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 2.2821
2022-02-25 16:19:56 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 2.5223
2022-02-25 16:20:30 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.9748
2022-02-25 16:21:05 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 2.3350
2022-02-25 16:21:39 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 2.5101
2022-02-25 16:22:14 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 2.3448
2022-02-25 16:22:48 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 2.4911
2022-02-25 16:23:24 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 2.1542
2022-02-25 16:23:58 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 2.3659
2022-02-25 16:24:34 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 2.2134
2022-02-25 16:25:08 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 2.3816
2022-02-25 16:25:44 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 2.2640
2022-02-25 16:26:17 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 2.5689
2022-02-25 16:26:20 - train: epoch 062, train_loss: 2.3306
2022-02-25 16:27:38 - eval: epoch: 062, acc1: 53.726%, acc5: 77.920%, test_loss: 1.9993, per_image_load_time: 2.366ms, per_image_inference_time: 0.231ms
2022-02-25 16:27:38 - until epoch: 062, best_acc1: 53.726%
2022-02-25 16:27:38 - epoch 063 lr: 0.0010000000000000002
2022-02-25 16:28:17 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 2.5039
2022-02-25 16:28:52 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 2.2513
2022-02-25 16:29:26 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 2.4310
2022-02-25 16:29:59 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 2.1909
2022-02-25 16:30:33 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 2.2077
2022-02-25 16:31:08 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 2.3465
2022-02-25 16:31:41 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 2.1935
2022-02-25 16:32:15 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 2.2401
2022-02-25 16:32:50 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 2.6375
2022-02-25 16:33:24 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 2.4290
2022-02-25 16:33:58 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 2.3986
2022-02-25 16:34:32 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 2.5077
2022-02-25 16:35:07 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 2.0545
2022-02-25 16:35:42 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 2.4730
2022-02-25 16:36:15 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 2.2156
2022-02-25 16:36:50 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 2.3198
2022-02-25 16:37:24 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 2.2694
2022-02-25 16:37:59 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 2.3666
2022-02-25 16:38:34 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 2.3526
2022-02-25 16:39:08 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 2.0931
2022-02-25 16:39:41 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 2.0413
2022-02-25 16:40:16 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 2.5699
2022-02-25 16:40:50 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 2.5859
2022-02-25 16:41:25 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 2.3960
2022-02-25 16:41:59 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 2.2861
2022-02-25 16:42:35 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 2.1711
2022-02-25 16:43:08 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 2.4854
2022-02-25 16:43:43 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 2.2541
2022-02-25 16:44:18 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 2.4198
2022-02-25 16:44:53 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 2.6348
2022-02-25 16:45:27 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 2.5435
2022-02-25 16:46:02 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 2.5692
2022-02-25 16:46:36 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 2.1843
2022-02-25 16:47:10 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 2.2372
2022-02-25 16:47:45 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 2.5263
2022-02-25 16:48:19 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 2.4251
2022-02-25 16:48:54 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 2.2254
2022-02-25 16:49:29 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 2.3440
2022-02-25 16:50:03 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 2.1487
2022-02-25 16:50:37 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 2.1721
2022-02-25 16:51:12 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 2.3952
2022-02-25 16:51:47 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 2.2800
2022-02-25 16:52:21 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 2.4633
2022-02-25 16:52:56 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 2.3759
2022-02-25 16:53:30 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 2.2901
2022-02-25 16:54:06 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 2.3213
2022-02-25 16:54:41 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 2.1601
2022-02-25 16:55:16 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 2.2853
2022-02-25 16:55:51 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 2.2051
2022-02-25 16:56:25 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 2.2634
2022-02-25 16:56:27 - train: epoch 063, train_loss: 2.3195
2022-02-25 16:57:45 - eval: epoch: 063, acc1: 53.746%, acc5: 78.108%, test_loss: 1.9934, per_image_load_time: 2.764ms, per_image_inference_time: 0.222ms
2022-02-25 16:57:45 - until epoch: 063, best_acc1: 53.746%
2022-02-25 16:57:45 - epoch 064 lr: 0.0010000000000000002
2022-02-25 16:58:25 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 2.3381
2022-02-25 16:58:59 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 2.2874
2022-02-25 16:59:33 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 2.2350
2022-02-25 17:00:07 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 2.2868
2022-02-25 17:00:41 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 2.2248
2022-02-25 17:01:16 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 2.4041
2022-02-25 17:01:50 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 2.2156
2022-02-25 17:02:25 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 2.1365
2022-02-25 17:02:58 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 2.3696
2022-02-25 17:03:33 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 2.3132
2022-02-25 17:04:08 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 2.4583
2022-02-25 17:04:41 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.9685
2022-02-25 17:05:17 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 2.2551
2022-02-25 17:05:51 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 2.4889
2022-02-25 17:06:25 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 2.5243
2022-02-25 17:06:59 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 2.2442
2022-02-25 17:07:32 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 2.2712
2022-02-25 17:08:07 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 2.1016
2022-02-25 17:08:41 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 2.2820
2022-02-25 17:09:16 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 2.2006
2022-02-25 17:09:50 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 2.4857
2022-02-25 17:10:25 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 2.4163
2022-02-25 17:10:59 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 2.2864
2022-02-25 17:11:33 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 2.2091
2022-02-25 17:12:08 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 2.2466
2022-02-25 17:12:42 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 2.1940
2022-02-25 17:13:17 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 2.1281
2022-02-25 17:13:52 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 2.2618
2022-02-25 17:14:25 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 2.4420
2022-02-25 17:15:00 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 2.4497
2022-02-25 17:15:34 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 2.3893
2022-02-25 17:16:09 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 2.4129
2022-02-25 17:16:42 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 2.3426
2022-02-25 17:17:17 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 2.4472
2022-02-25 17:17:52 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 2.2082
2022-02-25 17:18:27 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 2.3457
2022-02-25 17:19:01 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 2.0197
2022-02-25 17:19:37 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 2.3196
2022-02-25 17:20:11 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 2.1785
2022-02-25 17:20:46 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 2.1983
2022-02-25 17:21:20 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 2.2375
2022-02-25 17:21:55 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 2.0338
2022-02-25 17:22:28 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 2.4339
2022-02-25 17:23:04 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 2.2295
2022-02-25 17:23:37 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 2.3999
2022-02-25 17:24:14 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 2.3803
2022-02-25 17:24:48 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 2.6749
2022-02-25 17:25:23 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 2.3259
2022-02-25 17:25:57 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 2.5445
2022-02-25 17:26:32 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 2.0869
2022-02-25 17:26:34 - train: epoch 064, train_loss: 2.3135
2022-02-25 17:27:52 - eval: epoch: 064, acc1: 53.968%, acc5: 78.104%, test_loss: 1.9872, per_image_load_time: 1.700ms, per_image_inference_time: 0.212ms
2022-02-25 17:27:52 - until epoch: 064, best_acc1: 53.968%
2022-02-25 17:27:52 - epoch 065 lr: 0.0010000000000000002
2022-02-25 17:28:31 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 2.3455
2022-02-25 17:29:06 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 2.3583
2022-02-25 17:29:40 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 2.2236
2022-02-25 17:30:14 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 2.3386
2022-02-25 17:30:48 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 2.5114
2022-02-25 17:31:23 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 2.5611
2022-02-25 17:31:57 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 2.3759
2022-02-25 17:32:31 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 2.2156
2022-02-25 17:33:05 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 2.1330
2022-02-25 17:33:40 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 2.3725
2022-02-25 17:34:14 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 2.1868
2022-02-25 17:34:49 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 2.5571
2022-02-25 17:35:23 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 2.4608
2022-02-25 17:35:58 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 2.3015
2022-02-25 17:36:32 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 2.3833
2022-02-25 17:37:06 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 2.4484
2022-02-25 17:37:41 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 2.2611
2022-02-25 17:38:14 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 2.1189
2022-02-25 17:38:49 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 2.1382
2022-02-25 17:39:23 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 2.3432
2022-02-25 17:39:56 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 2.5405
2022-02-25 17:40:31 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 2.2571
2022-02-25 17:41:06 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 2.3159
2022-02-25 17:41:41 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 2.2702
2022-02-25 17:42:15 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 2.2919
2022-02-25 17:42:50 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 2.2649
2022-02-25 17:43:23 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 2.3581
2022-02-25 17:43:59 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 2.3197
2022-02-25 17:44:32 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 2.3914
2022-02-25 17:45:07 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 2.2776
2022-02-25 17:45:42 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 2.1061
2022-02-25 17:46:15 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 2.3271
2022-02-25 17:46:49 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 2.3261
2022-02-25 17:47:24 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 2.2212
2022-02-25 17:47:58 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 2.6776
2022-02-25 17:48:33 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 2.1665
2022-02-25 17:49:07 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 2.1362
2022-02-25 17:49:42 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 2.2613
2022-02-25 17:50:16 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 2.6379
2022-02-25 17:50:50 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 2.1957
2022-02-25 17:51:26 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 2.2719
2022-02-25 17:52:00 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 2.3416
2022-02-25 17:52:35 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 2.3736
2022-02-25 17:53:10 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 2.3793
2022-02-25 17:53:44 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 2.2967
2022-02-25 17:54:18 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 2.5171
2022-02-25 17:54:53 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 2.5547
2022-02-25 17:55:28 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 2.1640
2022-02-25 17:56:04 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 2.3853
2022-02-25 17:56:38 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 2.2608
2022-02-25 17:56:39 - train: epoch 065, train_loss: 2.3093
2022-02-25 17:57:56 - eval: epoch: 065, acc1: 54.200%, acc5: 78.230%, test_loss: 1.9821, per_image_load_time: 2.716ms, per_image_inference_time: 0.236ms
2022-02-25 17:57:56 - until epoch: 065, best_acc1: 54.200%
2022-02-25 17:57:56 - epoch 066 lr: 0.0010000000000000002
2022-02-25 17:58:35 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 2.0588
2022-02-25 17:59:10 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 2.5187
2022-02-25 17:59:45 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 2.1184
2022-02-25 18:00:18 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 2.0555
2022-02-25 18:00:54 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 2.2488
2022-02-25 18:01:27 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 2.1727
2022-02-25 18:02:01 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 2.4379
2022-02-25 18:02:35 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 2.4798
2022-02-25 18:03:10 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 2.2974
2022-02-25 18:03:45 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 2.2238
2022-02-25 18:04:18 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 2.4171
2022-02-25 18:04:53 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 2.4254
2022-02-25 18:05:28 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 2.5078
2022-02-25 18:06:02 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 2.2229
2022-02-25 18:06:36 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 2.3718
2022-02-25 18:07:10 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 2.4732
2022-02-25 18:07:45 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 2.1209
2022-02-25 18:08:19 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.9662
2022-02-25 18:08:53 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 2.4091
2022-02-25 18:09:28 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 2.0811
2022-02-25 18:10:03 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 2.4804
2022-02-25 18:10:38 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 2.1525
2022-02-25 18:11:12 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 2.2203
2022-02-25 18:11:46 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 2.4722
2022-02-25 18:12:21 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 2.2368
2022-02-25 18:12:55 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 2.2805
2022-02-25 18:13:30 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 2.2680
2022-02-25 18:14:04 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 2.1915
2022-02-25 18:14:38 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 2.3672
2022-02-25 18:15:12 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 2.2801
2022-02-25 18:15:48 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 2.4937
2022-02-25 18:16:22 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 2.3767
2022-02-25 18:16:57 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 2.3522
2022-02-25 18:17:31 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 2.4383
2022-02-25 18:18:05 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 2.4211
2022-02-25 18:18:41 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 2.5564
2022-02-25 18:19:14 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 2.3243
2022-02-25 18:19:50 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 2.3937
2022-02-25 18:20:23 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 2.4503
2022-02-25 18:20:58 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 2.4015
2022-02-25 18:21:32 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 2.3835
2022-02-25 18:22:07 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 2.1545
2022-02-25 18:22:40 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 2.0646
2022-02-25 18:23:15 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 2.0750
2022-02-25 18:23:49 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 2.5549
2022-02-25 18:24:25 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 2.4357
2022-02-25 18:25:00 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 2.2042
2022-02-25 18:25:34 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 2.2943
2022-02-25 18:26:10 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 2.2436
2022-02-25 18:26:44 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 2.2177
2022-02-25 18:26:46 - train: epoch 066, train_loss: 2.3031
2022-02-25 18:28:03 - eval: epoch: 066, acc1: 54.272%, acc5: 78.190%, test_loss: 1.9798, per_image_load_time: 2.745ms, per_image_inference_time: 0.197ms
2022-02-25 18:28:03 - until epoch: 066, best_acc1: 54.272%
2022-02-25 18:28:03 - epoch 067 lr: 0.0010000000000000002
2022-02-25 18:28:43 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 2.3873
2022-02-25 18:29:18 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 2.3920
2022-02-25 18:29:51 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 2.3966
2022-02-25 18:30:25 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 2.1485
2022-02-25 18:31:00 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 2.1027
2022-02-25 18:31:35 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 2.2899
2022-02-25 18:32:08 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 2.3741
2022-02-25 18:32:43 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 2.6791
2022-02-25 18:33:17 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 2.5795
2022-02-25 18:33:51 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 2.2477
2022-02-25 18:34:26 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 2.4307
2022-02-25 18:35:01 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 2.0526
2022-02-25 18:35:35 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 2.2113
2022-02-25 18:36:10 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 2.4015
2022-02-25 18:36:43 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 2.1285
2022-02-25 18:37:18 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 2.3187
2022-02-25 18:37:53 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 2.2120
2022-02-25 18:38:27 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 2.6650
2022-02-25 18:39:02 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 2.2274
2022-02-25 18:39:36 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 2.2839
2022-02-25 18:40:10 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 2.0493
2022-02-25 18:40:45 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 2.3711
2022-02-25 18:41:21 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 2.2651
2022-02-25 18:41:55 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 2.2595
2022-02-25 18:42:30 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 2.3533
2022-02-25 18:43:04 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 2.3461
2022-02-25 18:43:39 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 2.0841
2022-02-25 18:44:13 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 2.5361
2022-02-25 18:44:48 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 2.3430
2022-02-25 18:45:23 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 2.1289
2022-02-25 18:45:57 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 2.2604
2022-02-25 18:46:31 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 2.4871
2022-02-25 18:47:05 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 2.2533
2022-02-25 18:47:39 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 2.3741
2022-02-25 18:48:14 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 2.4505
2022-02-25 18:48:49 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 2.2157
2022-02-25 18:49:23 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 2.3890
2022-02-25 18:49:58 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 2.1311
2022-02-25 18:50:32 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 2.7417
2022-02-25 18:51:07 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 2.4080
2022-02-25 18:51:42 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 2.3861
2022-02-25 18:52:15 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 2.2616
2022-02-25 18:52:51 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.9925
2022-02-25 18:53:24 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 2.1885
2022-02-25 18:53:59 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 2.3746
2022-02-25 18:54:35 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.9724
2022-02-25 18:55:09 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 2.4323
2022-02-25 18:55:45 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 2.2143
2022-02-25 18:56:19 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 2.2325
2022-02-25 18:56:54 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 2.4934
2022-02-25 18:56:56 - train: epoch 067, train_loss: 2.3000
2022-02-25 18:58:14 - eval: epoch: 067, acc1: 54.244%, acc5: 78.266%, test_loss: 1.9773, per_image_load_time: 2.721ms, per_image_inference_time: 0.222ms
2022-02-25 18:58:14 - until epoch: 067, best_acc1: 54.272%
2022-02-25 18:58:14 - epoch 068 lr: 0.0010000000000000002
2022-02-25 18:58:53 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 2.3396
2022-02-25 18:59:28 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 2.4067
2022-02-25 19:00:02 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 2.2459
2022-02-25 19:00:36 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 2.2566
2022-02-25 19:01:10 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 2.1846
2022-02-25 19:01:44 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 2.2232
2022-02-25 19:02:18 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 2.4042
2022-02-25 19:02:52 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 2.0342
2022-02-25 19:03:27 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 2.3808
2022-02-25 19:04:01 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 2.3751
2022-02-25 19:04:34 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 2.4666
2022-02-25 19:05:09 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.9577
2022-02-25 19:05:43 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 2.2124
2022-02-25 19:06:18 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 2.2666
2022-02-25 19:06:52 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 2.6272
2022-02-25 19:07:27 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 2.1196
2022-02-25 19:08:01 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 2.3135
2022-02-25 19:08:36 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 2.3004
2022-02-25 19:09:09 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 2.1309
2022-02-25 19:09:44 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 2.5184
2022-02-25 19:10:19 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 2.3355
2022-02-25 19:10:53 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 2.2310
2022-02-25 19:11:28 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 2.1366
2022-02-25 19:12:02 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 2.3614
2022-02-25 19:12:37 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 2.1885
2022-02-25 19:13:11 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 2.3716
2022-02-25 19:13:45 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 2.3396
2022-02-25 19:14:20 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 2.4078
2022-02-25 19:14:54 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 2.5106
2022-02-25 19:15:29 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 2.3133
2022-02-25 19:16:04 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 2.0286
2022-02-25 19:16:37 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 2.3978
2022-02-25 19:17:12 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 2.4371
2022-02-25 19:17:47 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 2.1434
2022-02-25 19:18:21 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 2.3711
2022-02-25 19:18:56 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 2.3082
2022-02-25 19:19:31 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 2.2262
2022-02-25 19:20:05 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 2.3876
2022-02-25 19:20:39 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 2.3835
2022-02-25 19:21:14 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 2.1987
2022-02-25 19:21:49 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 2.2121
2022-02-25 19:22:24 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 2.2068
2022-02-25 19:22:58 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 2.4115
2022-02-25 19:23:33 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 2.3412
2022-02-25 19:24:06 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 2.3257
2022-02-25 19:24:41 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 2.2124
2022-02-25 19:25:16 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 2.4091
2022-02-25 19:25:51 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 2.3323
2022-02-25 19:26:27 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 2.2347
2022-02-25 19:27:01 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 2.1689
2022-02-25 19:27:03 - train: epoch 068, train_loss: 2.2991
2022-02-25 19:28:20 - eval: epoch: 068, acc1: 54.448%, acc5: 78.424%, test_loss: 1.9717, per_image_load_time: 1.640ms, per_image_inference_time: 0.224ms
2022-02-25 19:28:20 - until epoch: 068, best_acc1: 54.448%
2022-02-25 19:28:20 - epoch 069 lr: 0.0010000000000000002
2022-02-25 19:29:00 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 2.4579
2022-02-25 19:29:34 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 2.3376
2022-02-25 19:30:08 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 2.2216
2022-02-25 19:30:42 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 2.0623
2022-02-25 19:31:17 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 2.4598
2022-02-25 19:31:51 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 2.1473
2022-02-25 19:32:25 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 2.5215
2022-02-25 19:33:00 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 2.3481
2022-02-25 19:33:34 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 2.2707
2022-02-25 19:34:09 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 2.3440
2022-02-25 19:34:43 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 2.1726
2022-02-25 19:35:17 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 2.1707
2022-02-25 19:35:52 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 2.3305
2022-02-25 19:36:27 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 2.2376
2022-02-25 19:37:00 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 2.1554
2022-02-25 19:37:35 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 2.4281
2022-02-25 19:38:09 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 2.3858
2022-02-25 19:38:43 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 2.1216
2022-02-25 19:39:17 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 2.2519
2022-02-25 19:39:52 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 2.0689
2022-02-25 19:40:26 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 2.1925
2022-02-25 19:41:00 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 2.3042
2022-02-25 19:41:34 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 2.1985
2022-02-25 19:42:09 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 2.4642
2022-02-25 19:42:43 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 2.1269
2022-02-25 19:43:17 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 2.1819
2022-02-25 19:43:52 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 2.3198
2022-02-25 19:44:27 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 2.6194
2022-02-25 19:45:01 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 2.3069
2022-02-25 19:45:35 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 2.1642
2022-02-25 19:46:11 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 2.2631
2022-02-25 19:46:46 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 2.2110
2022-02-25 19:47:20 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 2.1786
2022-02-25 19:47:54 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 2.2024
2022-02-25 19:48:29 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 2.1810
2022-02-25 19:49:03 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 2.0097
2022-02-25 19:49:37 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 2.3448
2022-02-25 19:50:12 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 2.4611
2022-02-25 19:50:46 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 2.2856
2022-02-25 19:51:21 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 2.4672
2022-02-25 19:51:56 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 2.2347
2022-02-25 19:52:30 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 2.0695
2022-02-25 19:53:05 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 2.3026
2022-02-25 19:53:38 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 2.2099
2022-02-25 19:54:14 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 2.3920
2022-02-25 19:54:48 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 2.2572
2022-02-25 19:55:23 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 2.2716
2022-02-25 19:55:58 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 2.2880
2022-02-25 19:56:33 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 2.2338
2022-02-25 19:57:07 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 2.3246
2022-02-25 19:57:09 - train: epoch 069, train_loss: 2.2925
2022-02-25 19:58:26 - eval: epoch: 069, acc1: 54.364%, acc5: 78.284%, test_loss: 1.9742, per_image_load_time: 2.615ms, per_image_inference_time: 0.249ms
2022-02-25 19:58:26 - until epoch: 069, best_acc1: 54.448%
2022-02-25 19:58:26 - epoch 070 lr: 0.0010000000000000002
2022-02-25 19:59:05 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 2.4949
2022-02-25 19:59:39 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 2.2551
2022-02-25 20:00:14 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 2.3557
2022-02-25 20:00:48 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 2.2702
2022-02-25 20:01:23 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 2.2437
2022-02-25 20:01:57 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 2.3114
2022-02-25 20:02:32 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 2.0519
2022-02-25 20:03:06 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 2.2295
2022-02-25 20:03:40 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 2.1786
2022-02-25 20:04:14 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 2.4357
2022-02-25 20:04:49 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 2.6190
2022-02-25 20:05:24 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 2.5516
2022-02-25 20:05:58 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 2.2218
2022-02-25 20:06:31 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 2.1318
2022-02-25 20:07:06 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 2.3244
2022-02-25 20:07:40 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 2.4030
2022-02-25 20:08:14 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 2.3230
2022-02-25 20:08:48 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 2.0467
2022-02-25 20:09:22 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 2.1328
2022-02-25 20:09:57 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 2.1855
2022-02-25 20:10:30 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 2.5426
2022-02-25 20:11:05 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 2.6339
2022-02-25 20:11:39 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 2.5874
2022-02-25 20:12:14 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 2.4287
2022-02-25 20:12:48 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 2.1370
2022-02-25 20:13:23 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 2.2792
2022-02-25 20:13:57 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 2.3105
2022-02-25 20:14:32 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 2.3769
2022-02-25 20:15:06 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 2.1636
2022-02-25 20:15:40 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 2.4078
2022-02-25 20:16:15 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 2.3300
2022-02-25 20:16:50 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 2.3162
2022-02-25 20:17:24 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 2.3777
2022-02-25 20:17:58 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 2.1655
2022-02-25 20:18:34 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 2.3763
2022-02-25 20:19:08 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 2.3584
2022-02-25 20:19:42 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 2.2449
2022-02-25 20:20:16 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 2.2421
2022-02-25 20:20:52 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 2.1663
2022-02-25 20:21:26 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 2.3465
2022-02-25 20:22:01 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 2.2014
2022-02-25 20:22:35 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 2.1380
2022-02-25 20:23:09 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 2.3780
2022-02-25 20:23:45 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 2.3917
2022-02-25 20:24:18 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 2.3103
2022-02-25 20:24:52 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 2.4090
2022-02-25 20:25:28 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 2.4174
2022-02-25 20:26:02 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 2.3223
2022-02-25 20:26:37 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 2.2682
2022-02-25 20:27:12 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 2.4438
2022-02-25 20:27:14 - train: epoch 070, train_loss: 2.2919
2022-02-25 20:28:32 - eval: epoch: 070, acc1: 54.420%, acc5: 78.364%, test_loss: 1.9692, per_image_load_time: 2.710ms, per_image_inference_time: 0.238ms
2022-02-25 20:28:32 - until epoch: 070, best_acc1: 54.448%
2022-02-25 20:28:32 - epoch 071 lr: 0.0010000000000000002
2022-02-25 20:29:12 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 2.2030
2022-02-25 20:29:46 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 2.0666
2022-02-25 20:30:21 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 2.4624
2022-02-25 20:30:54 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 2.1917
2022-02-25 20:31:28 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 2.5424
2022-02-25 20:32:04 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 2.2860
2022-02-25 20:32:37 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 2.3282
2022-02-25 20:33:12 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 2.2607
2022-02-25 20:33:47 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 2.3858
2022-02-25 20:34:20 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 2.3676
2022-02-25 20:34:55 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 2.5417
2022-02-25 20:35:29 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 2.3271
2022-02-25 20:36:04 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 2.1815
2022-02-25 20:36:37 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 2.3330
2022-02-25 20:37:12 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 2.1283
2022-02-25 20:37:47 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 2.3105
2022-02-25 20:38:21 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 2.4006
2022-02-25 20:38:56 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 2.4287
2022-02-25 20:39:30 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 2.2487
2022-02-25 20:40:05 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 2.3555
2022-02-25 20:40:39 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 2.1931
2022-02-25 20:41:14 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 2.1479
2022-02-25 20:41:48 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 2.2779
2022-02-25 20:42:23 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 2.2127
2022-02-25 20:42:57 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 2.5105
2022-02-25 20:43:32 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 2.2088
2022-02-25 20:44:07 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 2.4961
2022-02-25 20:44:41 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 2.2627
2022-02-25 20:45:15 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 2.2086
2022-02-25 20:45:51 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 2.5433
2022-02-25 20:46:25 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.9183
2022-02-25 20:47:00 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 2.2081
2022-02-25 20:47:34 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 2.3437
2022-02-25 20:48:09 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 2.2143
2022-02-25 20:48:43 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 2.5352
2022-02-25 20:49:18 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 2.2941
2022-02-25 20:49:52 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 2.3024
2022-02-25 20:50:27 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 2.3316
2022-02-25 20:51:01 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 2.2077
2022-02-25 20:51:36 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 2.2631
2022-02-25 20:52:10 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 2.1626
2022-02-25 20:52:44 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 2.4053
2022-02-25 20:53:18 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 2.1512
2022-02-25 20:53:54 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 2.3944
2022-02-25 20:54:28 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 2.3279
2022-02-25 20:55:04 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 2.4016
2022-02-25 20:55:38 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 2.2686
2022-02-25 20:56:13 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 2.4199
2022-02-25 20:56:50 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.9891
2022-02-25 20:57:23 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 2.3074
2022-02-25 20:57:25 - train: epoch 071, train_loss: 2.2889
2022-02-25 20:58:43 - eval: epoch: 071, acc1: 54.384%, acc5: 78.514%, test_loss: 1.9674, per_image_load_time: 2.732ms, per_image_inference_time: 0.205ms
2022-02-25 20:58:43 - until epoch: 071, best_acc1: 54.448%
2022-02-25 20:58:43 - epoch 072 lr: 0.0010000000000000002
2022-02-25 20:59:22 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 2.4465
2022-02-25 20:59:56 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 2.1537
2022-02-25 21:00:31 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 2.1254
2022-02-25 21:01:04 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 2.3775
2022-02-25 21:01:39 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 2.1417
2022-02-25 21:02:14 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 2.0905
2022-02-25 21:02:48 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 2.1821
2022-02-25 21:03:22 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 2.5556
2022-02-25 21:03:57 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 2.1316
2022-02-25 21:04:31 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 2.0786
2022-02-25 21:05:06 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 2.5021
2022-02-25 21:05:41 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 2.1053
2022-02-25 21:06:14 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 2.3901
2022-02-25 21:06:49 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 2.3632
2022-02-25 21:07:23 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 2.3811
2022-02-25 21:07:58 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 2.1910
2022-02-25 21:08:32 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 2.2939
2022-02-25 21:09:08 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 2.2566
2022-02-25 21:09:41 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 2.4052
2022-02-25 21:10:17 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 2.2859
2022-02-25 21:10:50 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 2.3242
2022-02-25 21:11:25 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 2.3812
2022-02-25 21:11:59 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 2.4031
2022-02-25 21:12:34 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 2.3310
2022-02-25 21:13:09 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 2.2029
2022-02-25 21:13:43 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 2.4194
2022-02-25 21:14:18 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 2.6014
2022-02-25 21:14:53 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 2.4080
2022-02-25 21:15:26 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 2.0786
2022-02-25 21:16:02 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 2.2255
2022-02-25 21:16:36 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 2.3469
2022-02-25 21:17:11 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 2.1001
2022-02-25 21:17:45 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 2.2985
2022-02-25 21:18:20 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 2.5590
2022-02-25 21:18:55 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 2.5045
2022-02-25 21:19:29 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 2.3975
2022-02-25 21:20:03 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 2.4600
2022-02-25 21:20:38 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 2.2920
2022-02-25 21:21:13 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 2.5205
2022-02-25 21:21:48 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 2.3009
2022-02-25 21:22:22 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 2.4968
2022-02-25 21:22:58 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 2.1282
2022-02-25 21:23:32 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 2.2139
2022-02-25 21:24:06 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 2.1894
2022-02-25 21:24:42 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 2.4197
2022-02-25 21:25:16 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 2.3500
2022-02-25 21:25:50 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 2.3650
2022-02-25 21:26:27 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 2.3908
2022-02-25 21:27:01 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 2.4018
2022-02-25 21:27:35 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 2.2647
2022-02-25 21:27:37 - train: epoch 072, train_loss: 2.2871
2022-02-25 21:28:55 - eval: epoch: 072, acc1: 54.518%, acc5: 78.410%, test_loss: 1.9665, per_image_load_time: 2.355ms, per_image_inference_time: 0.211ms
2022-02-25 21:28:55 - until epoch: 072, best_acc1: 54.518%
2022-02-25 21:28:55 - epoch 073 lr: 0.0010000000000000002
2022-02-25 21:29:35 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 2.5352
2022-02-25 21:30:09 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 2.4401
2022-02-25 21:30:43 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 2.2125
2022-02-25 21:31:17 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 2.0089
2022-02-25 21:31:52 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 2.3746
2022-02-25 21:32:26 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 2.1370
2022-02-25 21:33:01 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 2.2910
2022-02-25 21:33:35 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 2.2185
2022-02-25 21:34:09 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 2.2100
2022-02-25 21:34:43 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 2.2928
2022-02-25 21:35:17 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 2.4388
2022-02-25 21:35:52 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 2.3711
2022-02-25 21:36:26 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 2.2939
2022-02-25 21:37:01 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 2.1463
2022-02-25 21:37:35 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 2.0136
2022-02-25 21:38:09 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 2.1837
2022-02-25 21:38:44 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 2.5192
2022-02-25 21:39:18 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 2.0258
2022-02-25 21:39:52 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 2.4344
2022-02-25 21:40:26 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 2.1661
2022-02-25 21:41:00 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 2.0843
2022-02-25 21:41:35 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 2.2301
2022-02-25 21:42:09 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 2.3370
2022-02-25 21:42:44 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 2.2906
2022-02-25 21:43:19 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 2.2895
2022-02-25 21:43:53 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 2.0694
2022-02-25 21:44:28 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 2.2537
2022-02-25 21:45:01 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 2.2805
2022-02-25 21:45:36 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 2.3872
2022-02-25 21:46:11 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 2.2736
2022-02-25 21:46:45 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 2.2728
2022-02-25 21:47:19 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 2.1536
2022-02-25 21:47:53 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 2.3009
2022-02-25 21:48:28 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 2.4856
2022-02-25 21:49:03 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 2.4182
2022-02-25 21:49:37 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 2.1357
2022-02-25 21:50:11 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 2.4267
2022-02-25 21:50:45 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 2.3800
2022-02-25 21:51:20 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 2.2035
2022-02-25 21:51:55 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 2.2995
2022-02-25 21:52:29 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 2.1570
2022-02-25 21:53:03 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 2.5501
2022-02-25 21:53:38 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 2.3114
2022-02-25 21:54:13 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 2.3462
2022-02-25 21:54:48 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 2.2267
2022-02-25 21:55:23 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 2.4169
2022-02-25 21:55:57 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 2.2043
2022-02-25 21:56:31 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 2.1377
2022-02-25 21:57:07 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 2.2169
2022-02-25 21:57:41 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 2.4795
2022-02-25 21:57:43 - train: epoch 073, train_loss: 2.2882
