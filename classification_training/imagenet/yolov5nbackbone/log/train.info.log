2022-02-25 21:59:01 - eval: epoch: 073, acc1: 54.450%, acc5: 78.620%, test_loss: 1.9621, per_image_load_time: 1.925ms, per_image_inference_time: 0.219ms
2022-02-25 21:59:01 - until epoch: 073, best_acc1: 54.518%
2022-02-25 21:59:01 - epoch 074 lr: 0.0010000000000000002
2022-02-25 21:59:40 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 2.4637
2022-02-25 22:00:15 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 2.5502
2022-02-25 22:00:49 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 2.3648
2022-02-25 22:01:22 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 2.4165
2022-02-25 22:01:56 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 2.4107
2022-02-25 22:02:30 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 2.3677
2022-02-25 22:03:03 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 2.2474
2022-02-25 22:03:37 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 2.3350
2022-02-25 22:04:11 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 2.1061
2022-02-25 22:04:45 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 2.5076
2022-02-25 22:05:18 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 2.2192
2022-02-25 22:05:52 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 2.3696
2022-02-25 22:06:26 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 2.3144
2022-02-25 22:07:01 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 2.1808
2022-02-25 22:07:34 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 2.2732
2022-02-25 22:08:07 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 2.1498
2022-02-25 22:08:41 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 2.3882
2022-02-25 22:09:15 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 2.5492
2022-02-25 22:09:49 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 2.2674
2022-02-25 22:10:25 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 2.0407
2022-02-25 22:10:57 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 2.2129
2022-02-25 22:11:31 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 2.4981
2022-02-25 22:12:06 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 2.4479
2022-02-25 22:12:40 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 2.1916
2022-02-25 22:13:13 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.9590
2022-02-25 22:13:46 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 2.2558
2022-02-25 22:14:21 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 2.3544
2022-02-25 22:14:54 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 2.6808
2022-02-25 22:15:28 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 2.4164
2022-02-25 22:16:02 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 2.2612
2022-02-25 22:16:35 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 2.2166
2022-02-25 22:17:08 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 2.1356
2022-02-25 22:17:42 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 2.4546
2022-02-25 22:18:16 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 2.2053
2022-02-25 22:18:49 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 2.3626
2022-02-25 22:19:23 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 2.5300
2022-02-25 22:19:58 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 2.3718
2022-02-25 22:20:31 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 2.0336
2022-02-25 22:21:06 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 2.0311
2022-02-25 22:21:39 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 2.3553
2022-02-25 22:22:14 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 2.3267
2022-02-25 22:22:47 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 2.2676
2022-02-25 22:23:22 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 2.2783
2022-02-25 22:23:55 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 2.3965
2022-02-25 22:24:30 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 2.4154
2022-02-25 22:25:03 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 2.4308
2022-02-25 22:25:37 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 2.1849
2022-02-25 22:26:11 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 2.2875
2022-02-25 22:26:48 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 2.3522
2022-02-25 22:27:20 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 2.2868
2022-02-25 22:27:22 - train: epoch 074, train_loss: 2.2835
2022-02-25 22:28:39 - eval: epoch: 074, acc1: 54.568%, acc5: 78.528%, test_loss: 1.9600, per_image_load_time: 2.756ms, per_image_inference_time: 0.177ms
2022-02-25 22:28:39 - until epoch: 074, best_acc1: 54.568%
2022-02-25 22:28:39 - epoch 075 lr: 0.0010000000000000002
2022-02-25 22:29:18 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 2.4720
2022-02-25 22:29:53 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 2.5480
2022-02-25 22:30:26 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 2.3832
2022-02-25 22:31:00 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 2.0977
2022-02-25 22:31:33 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 2.3054
2022-02-25 22:32:07 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 2.2462
2022-02-25 22:32:41 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 2.5847
2022-02-25 22:33:14 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 2.3088
2022-02-25 22:33:48 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 2.3285
2022-02-25 22:34:21 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.9913
2022-02-25 22:34:55 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 2.3742
2022-02-25 22:35:29 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 2.1604
2022-02-25 22:36:03 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 2.3645
2022-02-25 22:36:36 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 2.1038
2022-02-25 22:37:11 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 2.1229
2022-02-25 22:37:45 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.9947
2022-02-25 22:38:19 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 2.4787
2022-02-25 22:38:52 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 2.2974
2022-02-25 22:39:27 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 2.2961
2022-02-25 22:40:00 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 2.2608
2022-02-25 22:40:33 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 2.0957
2022-02-25 22:41:08 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 2.4569
2022-02-25 22:41:41 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 2.3759
2022-02-25 22:42:15 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 2.3858
2022-02-25 22:42:50 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 2.3994
2022-02-25 22:43:23 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 2.4641
2022-02-25 22:43:58 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.9589
2022-02-25 22:44:31 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 2.2621
2022-02-25 22:45:04 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 2.3164
2022-02-25 22:45:39 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 2.4784
2022-02-25 22:46:12 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 2.3648
2022-02-25 22:46:47 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 2.2854
2022-02-25 22:47:20 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 2.2941
2022-02-25 22:47:55 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 2.3568
2022-02-25 22:48:28 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 2.2724
2022-02-25 22:49:03 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 2.2600
2022-02-25 22:49:36 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 2.2587
2022-02-25 22:50:11 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 2.2013
2022-02-25 22:50:44 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 2.2938
2022-02-25 22:51:19 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 2.1068
2022-02-25 22:51:53 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 2.1954
2022-02-25 22:52:27 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 2.2636
2022-02-25 22:53:01 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 2.4019
2022-02-25 22:53:36 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 2.3695
2022-02-25 22:54:09 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 2.2209
2022-02-25 22:54:44 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 2.1772
2022-02-25 22:55:18 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 2.4639
2022-02-25 22:55:52 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 2.2658
2022-02-25 22:56:27 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 2.2661
2022-02-25 22:57:00 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 2.2409
2022-02-25 22:57:02 - train: epoch 075, train_loss: 2.2845
2022-02-25 22:58:19 - eval: epoch: 075, acc1: 54.630%, acc5: 78.708%, test_loss: 1.9593, per_image_load_time: 2.715ms, per_image_inference_time: 0.219ms
2022-02-25 22:58:19 - until epoch: 075, best_acc1: 54.630%
2022-02-25 22:58:19 - epoch 076 lr: 0.0010000000000000002
2022-02-25 22:58:59 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 2.0422
2022-02-25 22:59:32 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 2.2999
2022-02-25 23:00:06 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 2.3273
2022-02-25 23:00:39 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 2.4555
2022-02-25 23:01:13 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 2.2259
2022-02-25 23:01:47 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 2.2910
2022-02-25 23:02:22 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 2.1630
2022-02-25 23:02:57 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 2.2112
2022-02-25 23:03:30 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 2.2381
2022-02-25 23:04:04 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 2.2481
2022-02-25 23:04:38 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 2.1393
2022-02-25 23:05:12 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 2.1692
2022-02-25 23:05:46 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 2.2744
2022-02-25 23:06:20 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 2.2861
2022-02-25 23:06:53 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 2.2186
2022-02-25 23:07:28 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 2.3289
2022-02-25 23:08:01 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 2.1740
2022-02-25 23:08:35 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 2.1379
2022-02-25 23:09:08 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 2.2652
2022-02-25 23:09:43 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 2.3684
2022-02-25 23:10:17 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 2.2455
2022-02-25 23:10:51 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 2.2133
2022-02-25 23:11:25 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 2.0899
2022-02-25 23:11:59 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 2.4584
2022-02-25 23:12:33 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 2.1480
2022-02-25 23:13:07 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 2.5230
2022-02-25 23:13:40 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 2.2755
2022-02-25 23:14:15 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 2.3378
2022-02-25 23:14:48 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 2.3529
2022-02-25 23:15:23 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 2.0907
2022-02-25 23:15:56 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 2.1706
2022-02-25 23:16:31 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 2.3131
2022-02-25 23:17:05 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 2.0520
2022-02-25 23:17:39 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 2.2638
2022-02-25 23:18:13 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 2.1219
2022-02-25 23:18:47 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 2.0167
2022-02-25 23:19:21 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 2.2652
2022-02-25 23:19:56 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 2.2020
2022-02-25 23:20:29 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 2.1553
2022-02-25 23:21:03 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 2.4425
2022-02-25 23:21:37 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 2.1999
2022-02-25 23:22:12 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 2.3004
2022-02-25 23:22:46 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 2.3873
2022-02-25 23:23:20 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 2.1639
2022-02-25 23:23:54 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 2.2892
2022-02-25 23:24:28 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 2.0808
2022-02-25 23:25:02 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 2.3663
2022-02-25 23:25:37 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 2.0556
2022-02-25 23:26:12 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 2.2211
2022-02-25 23:26:45 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 2.3902
2022-02-25 23:26:47 - train: epoch 076, train_loss: 2.2782
2022-02-25 23:28:04 - eval: epoch: 076, acc1: 54.362%, acc5: 78.574%, test_loss: 1.9604, per_image_load_time: 1.869ms, per_image_inference_time: 0.202ms
2022-02-25 23:28:04 - until epoch: 076, best_acc1: 54.630%
2022-02-25 23:28:04 - epoch 077 lr: 0.0010000000000000002
2022-02-25 23:28:43 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 2.3043
2022-02-25 23:29:17 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 2.3008
2022-02-25 23:29:50 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 2.1441
2022-02-25 23:30:25 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 2.3162
2022-02-25 23:30:58 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 2.1441
2022-02-25 23:31:32 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.9570
2022-02-25 23:32:07 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 2.4041
2022-02-25 23:32:40 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 2.3250
2022-02-25 23:33:15 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 2.4642
2022-02-25 23:33:48 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 2.2172
2022-02-25 23:34:22 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 2.5398
2022-02-25 23:34:55 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 2.4188
2022-02-25 23:35:29 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 2.1457
2022-02-25 23:36:03 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 2.1575
2022-02-25 23:36:38 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 2.1160
2022-02-25 23:37:11 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 2.3011
2022-02-25 23:37:46 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 2.6206
2022-02-25 23:38:20 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 2.1883
2022-02-25 23:38:53 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 2.1283
2022-02-25 23:39:28 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 2.2057
2022-02-25 23:40:02 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 2.3374
2022-02-25 23:40:36 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 2.2119
2022-02-25 23:41:11 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 2.3513
2022-02-25 23:41:44 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 2.1569
2022-02-25 23:42:18 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 2.4367
2022-02-25 23:42:52 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 2.1284
2022-02-25 23:43:26 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 2.3894
2022-02-25 23:44:00 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 2.3520
2022-02-25 23:44:34 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 2.4292
2022-02-25 23:45:08 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 2.2400
2022-02-25 23:45:42 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 2.1966
2022-02-25 23:46:15 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 2.4410
2022-02-25 23:46:50 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 2.2127
2022-02-25 23:47:23 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 2.4132
2022-02-25 23:47:58 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 2.1967
2022-02-25 23:48:33 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 2.2344
2022-02-25 23:49:06 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 2.1299
2022-02-25 23:49:40 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 2.1260
2022-02-25 23:50:14 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 2.2290
2022-02-25 23:50:49 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 2.4348
2022-02-25 23:51:22 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 2.3971
2022-02-25 23:51:56 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 2.1919
2022-02-25 23:52:29 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 2.4263
2022-02-25 23:53:03 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 2.1540
2022-02-25 23:53:38 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 2.6048
2022-02-25 23:54:11 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 2.1701
2022-02-25 23:54:46 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 2.3352
2022-02-25 23:55:21 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 2.4256
2022-02-25 23:55:55 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 2.5365
2022-02-25 23:56:28 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 2.2947
2022-02-25 23:56:30 - train: epoch 077, train_loss: 2.2766
2022-02-25 23:57:48 - eval: epoch: 077, acc1: 54.530%, acc5: 78.474%, test_loss: 1.9593, per_image_load_time: 2.827ms, per_image_inference_time: 0.177ms
2022-02-25 23:57:48 - until epoch: 077, best_acc1: 54.630%
2022-02-25 23:57:48 - epoch 078 lr: 0.0010000000000000002
2022-02-25 23:58:27 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 2.4187
2022-02-25 23:59:02 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 2.3898
2022-02-25 23:59:35 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 2.5249
2022-02-26 00:00:10 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 2.3734
2022-02-26 00:00:42 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.9751
2022-02-26 00:01:16 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 2.1719
2022-02-26 00:01:50 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 2.3051
2022-02-26 00:02:24 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 2.4571
2022-02-26 00:02:57 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 2.3229
2022-02-26 00:03:31 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 2.3299
2022-02-26 00:04:05 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 2.0827
2022-02-26 00:04:38 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 2.2726
2022-02-26 00:05:13 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 2.0614
2022-02-26 00:05:47 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 2.0599
2022-02-26 00:06:21 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 2.2608
2022-02-26 00:06:54 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 2.3540
2022-02-26 00:07:28 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 2.3625
2022-02-26 00:08:03 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 2.4697
2022-02-26 00:08:37 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 2.3196
2022-02-26 00:09:11 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 2.2491
2022-02-26 00:09:44 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 2.2562
2022-02-26 00:10:19 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 2.2598
2022-02-26 00:10:53 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 2.3502
2022-02-26 00:11:26 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 2.3933
2022-02-26 00:12:01 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 2.1529
2022-02-26 00:12:34 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 2.3602
2022-02-26 00:13:09 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 2.2620
2022-02-26 00:13:42 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 2.2318
2022-02-26 00:14:16 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 2.0804
2022-02-26 00:14:50 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 2.2779
2022-02-26 00:15:24 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 2.2762
2022-02-26 00:15:58 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 2.3476
2022-02-26 00:16:33 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 2.4577
2022-02-26 00:17:07 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 2.0825
2022-02-26 00:17:41 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 2.3349
2022-02-26 00:18:15 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 2.4016
2022-02-26 00:18:49 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 2.3188
2022-02-26 00:19:23 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 2.2037
2022-02-26 00:19:57 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 2.3925
2022-02-26 00:20:31 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 2.2273
2022-02-26 00:21:06 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 2.2932
2022-02-26 00:21:39 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 2.4889
2022-02-26 00:22:14 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 2.3848
2022-02-26 00:22:48 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 2.2876
2022-02-26 00:23:22 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 2.1697
2022-02-26 00:23:57 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 2.1756
2022-02-26 00:24:31 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 2.2699
2022-02-26 00:25:06 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 2.4958
2022-02-26 00:25:40 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 2.3046
2022-02-26 00:26:14 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 2.3756
2022-02-26 00:26:16 - train: epoch 078, train_loss: 2.2766
2022-02-26 00:27:33 - eval: epoch: 078, acc1: 54.656%, acc5: 78.610%, test_loss: 1.9541, per_image_load_time: 2.771ms, per_image_inference_time: 0.205ms
2022-02-26 00:27:33 - until epoch: 078, best_acc1: 54.656%
2022-02-26 00:27:33 - epoch 079 lr: 0.0010000000000000002
2022-02-26 00:28:12 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 2.1650
2022-02-26 00:28:47 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 2.2443
2022-02-26 00:29:20 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 2.3889
2022-02-26 00:29:54 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 2.2305
2022-02-26 00:30:28 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 2.1692
2022-02-26 00:31:01 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 2.2346
2022-02-26 00:31:35 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 2.1518
2022-02-26 00:32:09 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 2.2963
2022-02-26 00:32:43 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 2.2262
2022-02-26 00:33:16 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 2.3633
2022-02-26 00:33:50 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 2.5562
2022-02-26 00:34:24 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 2.2270
2022-02-26 00:34:58 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 2.2238
2022-02-26 00:35:32 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 2.3011
2022-02-26 00:36:06 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 2.3520
2022-02-26 00:36:40 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 2.2686
2022-02-26 00:37:14 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 2.2373
2022-02-26 00:37:49 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 2.2756
2022-02-26 00:38:23 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 2.1851
2022-02-26 00:38:57 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 2.4369
2022-02-26 00:39:30 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 2.2220
2022-02-26 00:40:05 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 2.2948
2022-02-26 00:40:38 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 2.1885
2022-02-26 00:41:13 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 2.2170
2022-02-26 00:41:47 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 2.2022
2022-02-26 00:42:21 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 2.1856
2022-02-26 00:42:54 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 2.0328
2022-02-26 00:43:28 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 2.2602
2022-02-26 00:44:01 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 2.5160
2022-02-26 00:44:36 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 2.3438
2022-02-26 00:45:10 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 2.2374
2022-02-26 00:45:45 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 2.5504
2022-02-26 00:46:18 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 2.1230
2022-02-26 00:46:53 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 2.2303
2022-02-26 00:47:26 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 2.5095
2022-02-26 00:48:00 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 2.2360
2022-02-26 00:48:34 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 2.1383
2022-02-26 00:49:08 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 2.3976
2022-02-26 00:49:42 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 2.2935
2022-02-26 00:50:15 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 2.1072
2022-02-26 00:50:49 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 2.3251
2022-02-26 00:51:23 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.9999
2022-02-26 00:51:57 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.9987
2022-02-26 00:52:31 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 2.4311
2022-02-26 00:53:05 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 2.4452
2022-02-26 00:53:40 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 2.3235
2022-02-26 00:54:14 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 2.0958
2022-02-26 00:54:48 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 2.3186
2022-02-26 00:55:23 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 2.3899
2022-02-26 00:55:57 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 2.0548
2022-02-26 00:55:59 - train: epoch 079, train_loss: 2.2741
2022-02-26 00:57:16 - eval: epoch: 079, acc1: 54.638%, acc5: 78.544%, test_loss: 1.9567, per_image_load_time: 2.743ms, per_image_inference_time: 0.197ms
2022-02-26 00:57:16 - until epoch: 079, best_acc1: 54.656%
2022-02-26 00:57:16 - epoch 080 lr: 0.0010000000000000002
2022-02-26 00:57:55 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 2.2036
2022-02-26 00:58:29 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 2.2206
2022-02-26 00:59:03 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 2.1501
2022-02-26 00:59:36 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 2.1253
2022-02-26 01:00:10 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 2.0124
2022-02-26 01:00:44 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 2.1587
2022-02-26 01:01:18 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 2.0484
2022-02-26 01:01:53 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 2.1012
2022-02-26 01:02:26 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 2.2242
2022-02-26 01:03:01 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 2.3476
2022-02-26 01:03:34 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 2.0782
2022-02-26 01:04:08 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 2.3399
2022-02-26 01:04:42 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.9894
2022-02-26 01:05:17 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 2.1911
2022-02-26 01:05:50 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 2.1259
2022-02-26 01:06:25 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 2.4450
2022-02-26 01:06:58 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 2.3411
2022-02-26 01:07:32 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 2.2915
2022-02-26 01:08:06 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 2.1714
2022-02-26 01:08:40 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 2.2398
2022-02-26 01:09:14 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 2.3684
2022-02-26 01:09:48 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 2.3903
2022-02-26 01:10:22 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 2.2834
2022-02-26 01:10:56 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 2.5883
2022-02-26 01:11:30 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 2.0465
2022-02-26 01:12:04 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 2.3043
2022-02-26 01:12:38 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 2.2703
2022-02-26 01:13:13 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 2.3710
2022-02-26 01:13:46 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 2.3182
2022-02-26 01:14:21 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 2.2443
2022-02-26 01:14:55 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 2.4004
2022-02-26 01:15:29 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 2.0958
2022-02-26 01:16:03 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 2.2167
2022-02-26 01:16:37 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 2.2843
2022-02-26 01:17:11 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 2.0803
2022-02-26 01:17:45 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 2.2451
2022-02-26 01:18:18 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 2.2204
2022-02-26 01:18:53 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 2.2691
2022-02-26 01:19:27 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 2.0188
2022-02-26 01:20:02 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 2.3747
2022-02-26 01:20:36 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 2.3302
2022-02-26 01:21:10 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 2.1632
2022-02-26 01:21:44 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 2.6603
2022-02-26 01:22:18 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 2.1587
2022-02-26 01:22:52 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 2.2973
2022-02-26 01:23:27 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 2.3106
2022-02-26 01:24:00 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 2.1593
2022-02-26 01:24:36 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 2.1066
2022-02-26 01:25:09 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 2.6028
2022-02-26 01:25:44 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 2.4097
2022-02-26 01:25:46 - train: epoch 080, train_loss: 2.2746
2022-02-26 01:27:02 - eval: epoch: 080, acc1: 54.468%, acc5: 78.572%, test_loss: 1.9530, per_image_load_time: 2.459ms, per_image_inference_time: 0.190ms
2022-02-26 01:27:02 - until epoch: 080, best_acc1: 54.656%
2022-02-26 01:27:02 - epoch 081 lr: 0.0010000000000000002
2022-02-26 01:27:42 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 2.2160
2022-02-26 01:28:15 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 2.1946
2022-02-26 01:28:48 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 2.4722
2022-02-26 01:29:22 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 2.3152
2022-02-26 01:29:56 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 2.5437
2022-02-26 01:30:30 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 2.2285
2022-02-26 01:31:03 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 2.2828
2022-02-26 01:31:38 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 2.3745
2022-02-26 01:32:10 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 2.7162
2022-02-26 01:32:45 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 2.3780
2022-02-26 01:33:19 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 2.2538
2022-02-26 01:33:53 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 2.3646
2022-02-26 01:34:26 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 2.2519
2022-02-26 01:35:01 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 2.1090
2022-02-26 01:35:33 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 2.5105
2022-02-26 01:36:08 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 2.0510
2022-02-26 01:36:42 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 2.2313
2022-02-26 01:37:17 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 2.1887
2022-02-26 01:37:51 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 2.2138
2022-02-26 01:38:25 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 2.2928
2022-02-26 01:38:59 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 2.2190
2022-02-26 01:39:33 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 2.0661
2022-02-26 01:40:07 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 2.2183
2022-02-26 01:40:41 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 2.2616
2022-02-26 01:41:15 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 2.5484
2022-02-26 01:41:48 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 2.3222
2022-02-26 01:42:23 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 2.4745
2022-02-26 01:42:57 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 2.2149
2022-02-26 01:43:31 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 2.2399
2022-02-26 01:44:06 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 2.3168
2022-02-26 01:44:39 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 2.1302
2022-02-26 01:45:14 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 2.2904
2022-02-26 01:45:47 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 2.2298
2022-02-26 01:46:22 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 2.3895
2022-02-26 01:46:56 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 2.2999
2022-02-26 01:47:30 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 2.0466
2022-02-26 01:48:05 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 2.5464
2022-02-26 01:48:38 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 2.2117
2022-02-26 01:49:12 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 2.5041
2022-02-26 01:49:45 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 2.2360
2022-02-26 01:50:20 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 2.2123
2022-02-26 01:50:53 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 2.3117
2022-02-26 01:51:28 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 2.1996
2022-02-26 01:52:02 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 2.4739
2022-02-26 01:52:36 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 2.4042
2022-02-26 01:53:10 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 2.1680
2022-02-26 01:53:44 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 2.3152
2022-02-26 01:54:19 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 2.3200
2022-02-26 01:54:52 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 2.3743
2022-02-26 01:55:26 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 2.3180
2022-02-26 01:55:28 - train: epoch 081, train_loss: 2.2743
2022-02-26 01:56:44 - eval: epoch: 081, acc1: 54.710%, acc5: 78.556%, test_loss: 1.9518, per_image_load_time: 1.670ms, per_image_inference_time: 0.209ms
2022-02-26 01:56:45 - until epoch: 081, best_acc1: 54.710%
2022-02-26 01:56:45 - epoch 082 lr: 0.0010000000000000002
2022-02-26 01:57:23 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 2.0114
2022-02-26 01:57:58 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 2.2064
2022-02-26 01:58:32 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 2.3364
2022-02-26 01:59:05 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 2.3344
2022-02-26 01:59:39 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 2.3592
2022-02-26 02:00:13 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 2.3407
2022-02-26 02:00:46 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 2.4281
2022-02-26 02:01:20 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 2.2565
2022-02-26 02:01:53 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 2.3653
2022-02-26 02:02:27 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 2.3407
2022-02-26 02:03:01 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 2.3907
2022-02-26 02:03:35 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 2.3877
2022-02-26 02:04:09 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 2.4136
2022-02-26 02:04:42 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 2.0259
2022-02-26 02:05:17 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 2.3779
2022-02-26 02:05:50 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 2.3475
2022-02-26 02:06:25 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 2.2040
2022-02-26 02:06:59 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.9974
2022-02-26 02:07:33 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 2.2362
2022-02-26 02:08:07 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 2.2082
2022-02-26 02:08:41 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 2.2439
2022-02-26 02:09:14 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 2.4561
2022-02-26 02:09:49 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 2.2079
2022-02-26 02:10:23 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 2.5214
2022-02-26 02:10:56 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 2.3551
2022-02-26 02:11:31 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 2.3008
2022-02-26 02:12:05 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 2.1517
2022-02-26 02:12:39 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 2.0082
2022-02-26 02:13:13 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.9942
2022-02-26 02:13:47 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 2.1695
2022-02-26 02:14:20 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 2.2678
2022-02-26 02:14:55 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 2.2040
2022-02-26 02:15:28 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 2.2074
2022-02-26 02:16:03 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 2.1412
2022-02-26 02:16:36 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 2.1862
2022-02-26 02:17:11 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 2.3001
2022-02-26 02:17:44 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 2.0196
2022-02-26 02:18:18 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 2.3152
2022-02-26 02:18:52 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 2.2469
2022-02-26 02:19:26 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 2.3866
2022-02-26 02:19:59 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 2.3399
2022-02-26 02:20:34 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 2.3706
2022-02-26 02:21:07 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 2.2429
2022-02-26 02:21:41 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 2.1808
2022-02-26 02:22:15 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 2.2122
2022-02-26 02:22:49 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 2.3873
2022-02-26 02:23:23 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 2.1013
2022-02-26 02:23:58 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 2.0967
2022-02-26 02:24:32 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 2.3088
2022-02-26 02:25:06 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 2.1858
2022-02-26 02:25:08 - train: epoch 082, train_loss: 2.2724
2022-02-26 02:26:25 - eval: epoch: 082, acc1: 54.692%, acc5: 78.674%, test_loss: 1.9524, per_image_load_time: 2.771ms, per_image_inference_time: 0.192ms
2022-02-26 02:26:25 - until epoch: 082, best_acc1: 54.710%
2022-02-26 02:26:25 - epoch 083 lr: 0.0010000000000000002
2022-02-26 02:27:04 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.9457
2022-02-26 02:27:38 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 2.2673
2022-02-26 02:28:11 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 2.2426
2022-02-26 02:28:46 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 2.2794
2022-02-26 02:29:20 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 2.2955
2022-02-26 02:29:53 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 2.2952
2022-02-26 02:30:27 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 2.2744
2022-02-26 02:31:01 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 2.2109
2022-02-26 02:31:34 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 2.1831
2022-02-26 02:32:08 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 2.5405
2022-02-26 02:32:41 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 2.3952
2022-02-26 02:33:16 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 2.3381
2022-02-26 02:33:49 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 2.1115
2022-02-26 02:34:22 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 2.3548
2022-02-26 02:34:57 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 2.2810
2022-02-26 02:35:30 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 2.3280
2022-02-26 02:36:04 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 2.3568
2022-02-26 02:36:38 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 2.4205
2022-02-26 02:37:12 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 2.3508
2022-02-26 02:37:46 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.9412
2022-02-26 02:38:19 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 2.3477
2022-02-26 02:38:54 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 2.1541
2022-02-26 02:39:27 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 2.2548
2022-02-26 02:40:01 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 2.2046
2022-02-26 02:40:35 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 2.0699
2022-02-26 02:41:09 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 2.1783
2022-02-26 02:41:43 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 2.2698
2022-02-26 02:42:16 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 2.2709
2022-02-26 02:42:51 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 2.4785
2022-02-26 02:43:24 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 2.2600
2022-02-26 02:43:59 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 2.1576
2022-02-26 02:44:32 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 2.1965
2022-02-26 02:45:07 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 2.1462
2022-02-26 02:45:41 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 2.0516
2022-02-26 02:46:15 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 2.2471
2022-02-26 02:46:48 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 2.1889
2022-02-26 02:47:22 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 2.0434
2022-02-26 02:47:56 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 2.1882
2022-02-26 02:48:30 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 2.4813
2022-02-26 02:49:04 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 2.4682
2022-02-26 02:49:39 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 2.2274
2022-02-26 02:50:12 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 2.4544
2022-02-26 02:50:46 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 2.1935
2022-02-26 02:51:19 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 2.3881
2022-02-26 02:51:54 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 2.2300
2022-02-26 02:52:28 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 2.0437
2022-02-26 02:53:02 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 2.4368
2022-02-26 02:53:36 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 2.4217
2022-02-26 02:54:11 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 2.2460
2022-02-26 02:54:45 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 2.2500
2022-02-26 02:54:46 - train: epoch 083, train_loss: 2.2707
2022-02-26 02:56:04 - eval: epoch: 083, acc1: 54.722%, acc5: 78.728%, test_loss: 1.9524, per_image_load_time: 2.723ms, per_image_inference_time: 0.184ms
2022-02-26 02:56:04 - until epoch: 083, best_acc1: 54.722%
2022-02-26 02:56:04 - epoch 084 lr: 0.0010000000000000002
2022-02-26 02:56:43 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 2.3186
2022-02-26 02:57:16 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 2.1676
2022-02-26 02:57:50 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 2.1089
2022-02-26 02:58:25 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 2.4004
2022-02-26 02:58:58 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 2.3859
2022-02-26 02:59:33 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 2.7177
2022-02-26 03:00:06 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 2.5102
2022-02-26 03:00:40 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 2.3755
2022-02-26 03:01:13 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 2.3251
2022-02-26 03:01:47 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 2.3349
2022-02-26 03:02:21 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 2.3216
2022-02-26 03:02:54 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 2.5767
2022-02-26 03:03:28 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 2.4488
2022-02-26 03:04:03 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 2.2950
2022-02-26 03:04:36 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 2.3800
2022-02-26 03:05:11 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 2.1851
2022-02-26 03:05:44 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 2.5242
2022-02-26 03:06:19 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 2.4244
2022-02-26 03:06:52 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 2.1865
2022-02-26 03:07:27 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.9921
2022-02-26 03:08:00 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 2.1315
2022-02-26 03:08:35 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 2.2031
2022-02-26 03:09:08 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 2.3601
2022-02-26 03:09:43 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 2.1236
2022-02-26 03:10:16 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 2.4197
2022-02-26 03:10:51 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 2.3111
2022-02-26 03:11:25 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 2.2744
2022-02-26 03:11:59 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 2.2148
2022-02-26 03:12:32 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 2.2284
2022-02-26 03:13:07 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 2.3922
2022-02-26 03:13:41 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 2.3361
2022-02-26 03:14:15 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 2.2270
2022-02-26 03:14:49 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 2.3387
2022-02-26 03:15:23 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 2.5072
2022-02-26 03:15:57 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 2.2165
2022-02-26 03:16:31 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 2.3056
2022-02-26 03:17:04 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 2.3687
2022-02-26 03:17:39 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 2.3853
2022-02-26 03:18:13 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 2.3573
2022-02-26 03:18:47 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 2.3441
2022-02-26 03:19:20 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 2.2229
2022-02-26 03:19:56 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 2.3705
2022-02-26 03:20:29 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 2.2429
2022-02-26 03:21:03 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 2.4847
2022-02-26 03:21:36 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 2.1352
2022-02-26 03:22:12 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 2.1159
2022-02-26 03:22:45 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 2.1213
2022-02-26 03:23:19 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 2.1586
2022-02-26 03:23:54 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 2.1035
2022-02-26 03:24:28 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 2.2069
2022-02-26 03:24:30 - train: epoch 084, train_loss: 2.2697
2022-02-26 03:25:47 - eval: epoch: 084, acc1: 54.672%, acc5: 78.630%, test_loss: 1.9546, per_image_load_time: 1.946ms, per_image_inference_time: 0.197ms
2022-02-26 03:25:47 - until epoch: 084, best_acc1: 54.722%
2022-02-26 03:25:47 - epoch 085 lr: 0.0010000000000000002
2022-02-26 03:26:26 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 2.1040
2022-02-26 03:26:59 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 2.4348
2022-02-26 03:27:33 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 2.4307
2022-02-26 03:28:08 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 2.3311
2022-02-26 03:28:41 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 2.3289
2022-02-26 03:29:15 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 2.1574
2022-02-26 03:29:48 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 2.4089
2022-02-26 03:30:21 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 2.3280
2022-02-26 03:30:55 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.9808
2022-02-26 03:31:30 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 2.1683
2022-02-26 03:32:03 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 2.2740
2022-02-26 03:32:37 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 2.0945
2022-02-26 03:33:11 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 2.4735
2022-02-26 03:33:45 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 2.2021
2022-02-26 03:34:19 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 2.1867
2022-02-26 03:34:52 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 2.1455
2022-02-26 03:35:27 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 2.2883
2022-02-26 03:36:00 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 2.0872
2022-02-26 03:36:34 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 2.1204
2022-02-26 03:37:08 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 2.2641
2022-02-26 03:37:43 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 2.3008
2022-02-26 03:38:17 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 2.4656
2022-02-26 03:38:50 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 2.2124
2022-02-26 03:39:25 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 2.3306
2022-02-26 03:39:59 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 2.4783
2022-02-26 03:40:33 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 2.3613
2022-02-26 03:41:07 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 2.2363
2022-02-26 03:41:40 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 2.4856
2022-02-26 03:42:15 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 2.3499
2022-02-26 03:42:49 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 2.4116
2022-02-26 03:43:22 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 2.4213
2022-02-26 03:43:56 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 2.3447
2022-02-26 03:44:29 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 2.3354
2022-02-26 03:45:03 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 2.3317
2022-02-26 03:45:38 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 2.2155
2022-02-26 03:46:12 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 2.4321
2022-02-26 03:46:46 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.9525
2022-02-26 03:47:19 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 2.2739
2022-02-26 03:47:53 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 2.3462
2022-02-26 03:48:27 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 2.5603
2022-02-26 03:49:02 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 2.2306
2022-02-26 03:49:35 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 2.4209
2022-02-26 03:50:09 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 2.2553
2022-02-26 03:50:43 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 2.2640
2022-02-26 03:51:17 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 2.4259
2022-02-26 03:51:51 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 2.4497
2022-02-26 03:52:24 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 2.5586
2022-02-26 03:53:00 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 2.0805
2022-02-26 03:53:34 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 2.3494
2022-02-26 03:54:08 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 2.1404
2022-02-26 03:54:10 - train: epoch 085, train_loss: 2.2688
2022-02-26 03:55:26 - eval: epoch: 085, acc1: 54.742%, acc5: 78.636%, test_loss: 1.9505, per_image_load_time: 2.715ms, per_image_inference_time: 0.215ms
2022-02-26 03:55:26 - until epoch: 085, best_acc1: 54.742%
2022-02-26 03:55:26 - epoch 086 lr: 0.0010000000000000002
2022-02-26 03:56:05 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 2.2600
2022-02-26 03:56:39 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 2.1355
2022-02-26 03:57:12 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 2.4743
2022-02-26 03:57:46 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 2.6064
2022-02-26 03:58:21 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 2.6890
2022-02-26 03:58:55 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 2.3212
2022-02-26 03:59:29 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 2.3185
2022-02-26 04:00:03 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 2.4051
2022-02-26 04:00:37 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 2.2311
2022-02-26 04:01:10 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 2.3803
2022-02-26 04:01:45 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 2.2995
2022-02-26 04:02:18 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 2.3743
2022-02-26 04:02:52 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 2.4453
2022-02-26 04:03:25 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 2.2847
2022-02-26 04:03:59 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 2.0550
2022-02-26 04:04:33 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 2.3432
2022-02-26 04:05:07 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 2.2067
2022-02-26 04:05:40 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 2.1755
2022-02-26 04:06:14 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 2.3519
2022-02-26 04:06:48 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 2.3693
2022-02-26 04:07:22 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.9691
2022-02-26 04:07:56 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 2.3686
2022-02-26 04:08:30 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 2.3230
2022-02-26 04:09:04 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 2.1270
2022-02-26 04:09:40 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 2.1825
2022-02-26 04:10:13 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 2.3314
2022-02-26 04:10:48 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 2.1975
2022-02-26 04:11:23 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 2.2374
2022-02-26 04:11:57 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 2.2252
2022-02-26 04:12:32 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 2.2658
2022-02-26 04:13:06 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 2.3922
2022-02-26 04:13:41 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 2.5397
2022-02-26 04:14:16 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 2.4210
2022-02-26 04:14:51 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 2.4836
2022-02-26 04:15:26 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 2.3258
2022-02-26 04:16:01 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 2.2283
2022-02-26 04:16:35 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 2.3768
2022-02-26 04:17:11 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 2.2332
2022-02-26 04:17:45 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 2.2634
2022-02-26 04:18:20 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 2.2376
2022-02-26 04:18:54 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 2.1330
2022-02-26 04:19:29 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 2.3880
2022-02-26 04:20:03 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 2.2476
2022-02-26 04:20:38 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 2.2504
2022-02-26 04:21:13 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 2.1581
2022-02-26 04:21:48 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 2.2072
2022-02-26 04:22:22 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 2.2961
2022-02-26 04:22:58 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 2.3302
2022-02-26 04:23:32 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 2.2457
2022-02-26 04:24:07 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 2.1893
2022-02-26 04:24:09 - train: epoch 086, train_loss: 2.2684
2022-02-26 04:25:26 - eval: epoch: 086, acc1: 54.724%, acc5: 78.588%, test_loss: 1.9533, per_image_load_time: 2.743ms, per_image_inference_time: 0.212ms
2022-02-26 04:25:26 - until epoch: 086, best_acc1: 54.742%
2022-02-26 04:25:26 - epoch 087 lr: 0.0010000000000000002
2022-02-26 04:26:06 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 2.3431
2022-02-26 04:26:39 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 2.3002
2022-02-26 04:27:14 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 2.3103
2022-02-26 04:27:47 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 2.4009
2022-02-26 04:28:22 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 2.2008
2022-02-26 04:28:56 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 2.5461
2022-02-26 04:29:31 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 2.1557
2022-02-26 04:30:05 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 2.1693
2022-02-26 04:30:41 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 2.2575
2022-02-26 04:31:14 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 2.1421
2022-02-26 04:31:48 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 2.3889
2022-02-26 04:32:23 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 2.1977
2022-02-26 04:32:57 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 2.5775
2022-02-26 04:33:31 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.9013
2022-02-26 04:34:06 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 2.2229
2022-02-26 04:34:40 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 2.2055
2022-02-26 04:35:16 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 2.3909
2022-02-26 04:35:49 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 2.3879
2022-02-26 04:36:24 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 2.4083
2022-02-26 04:36:58 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 2.1644
2022-02-26 04:37:33 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 2.4434
2022-02-26 04:38:07 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 2.4529
2022-02-26 04:38:41 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 2.3590
2022-02-26 04:39:16 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 2.3313
2022-02-26 04:39:50 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 2.2029
2022-02-26 04:40:24 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 2.3221
2022-02-26 04:40:59 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 2.3226
2022-02-26 04:41:33 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 2.0440
2022-02-26 04:42:08 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 2.1025
2022-02-26 04:42:41 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 2.4601
2022-02-26 04:43:16 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 2.2376
2022-02-26 04:43:51 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 2.4089
2022-02-26 04:44:25 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 2.2170
2022-02-26 04:45:00 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 2.1553
2022-02-26 04:45:33 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 2.1068
2022-02-26 04:46:08 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 2.2004
2022-02-26 04:46:43 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 2.1508
2022-02-26 04:47:17 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 2.3650
2022-02-26 04:47:51 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 2.2544
2022-02-26 04:48:26 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 2.1713
2022-02-26 04:49:00 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 2.5394
2022-02-26 04:49:34 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 2.3575
2022-02-26 04:50:09 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 2.2963
2022-02-26 04:50:43 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 2.2363
2022-02-26 04:51:19 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 2.2133
2022-02-26 04:51:53 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 2.2926
2022-02-26 04:52:28 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 2.3851
2022-02-26 04:53:03 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 2.2175
2022-02-26 04:53:39 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 2.1570
2022-02-26 04:54:12 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 2.4499
2022-02-26 04:54:14 - train: epoch 087, train_loss: 2.2654
2022-02-26 04:55:32 - eval: epoch: 087, acc1: 54.848%, acc5: 78.836%, test_loss: 1.9454, per_image_load_time: 2.135ms, per_image_inference_time: 0.227ms
2022-02-26 04:55:32 - until epoch: 087, best_acc1: 54.848%
2022-02-26 04:55:32 - epoch 088 lr: 0.0010000000000000002
2022-02-26 04:56:11 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 2.0584
2022-02-26 04:56:45 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 2.1616
2022-02-26 04:57:19 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 2.5422
2022-02-26 04:57:53 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 2.3943
2022-02-26 04:58:27 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 2.3150
2022-02-26 04:59:02 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 2.3493
2022-02-26 04:59:36 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 2.3456
2022-02-26 05:00:11 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 2.2157
2022-02-26 05:00:44 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 2.1295
2022-02-26 05:01:20 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 2.1713
2022-02-26 05:01:54 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 2.3648
2022-02-26 05:02:28 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 2.3182
2022-02-26 05:03:03 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 2.4774
2022-02-26 05:03:37 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 2.2527
2022-02-26 05:04:12 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 2.2385
2022-02-26 05:04:46 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 2.2024
2022-02-26 05:05:20 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 2.2881
2022-02-26 05:05:54 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 2.0868
2022-02-26 05:06:29 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 2.1907
2022-02-26 05:07:02 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 2.3542
2022-02-26 05:07:36 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 2.4403
2022-02-26 05:08:11 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 2.3383
2022-02-26 05:08:45 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 2.1327
2022-02-26 05:09:19 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 2.3918
2022-02-26 05:09:53 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 2.4651
2022-02-26 05:10:28 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 2.4510
2022-02-26 05:11:02 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 2.3480
2022-02-26 05:11:36 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 2.3643
2022-02-26 05:12:11 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 2.0635
2022-02-26 05:12:45 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 2.2829
2022-02-26 05:13:20 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 2.0828
2022-02-26 05:13:53 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 2.2844
2022-02-26 05:14:28 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 2.1925
2022-02-26 05:15:02 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 2.2694
2022-02-26 05:15:37 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 2.1429
2022-02-26 05:16:11 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 2.1033
2022-02-26 05:16:46 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 2.3654
2022-02-26 05:17:21 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 2.4632
2022-02-26 05:17:55 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 2.2449
2022-02-26 05:18:30 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.9283
2022-02-26 05:19:04 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 2.3512
2022-02-26 05:19:40 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 2.0920
2022-02-26 05:20:14 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 2.3805
2022-02-26 05:20:48 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 2.0238
2022-02-26 05:21:23 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 2.3153
2022-02-26 05:21:57 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 2.5492
2022-02-26 05:22:32 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 2.3059
2022-02-26 05:23:07 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 2.2929
2022-02-26 05:23:42 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 2.0078
2022-02-26 05:24:16 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 2.3688
2022-02-26 05:24:18 - train: epoch 088, train_loss: 2.2676
2022-02-26 05:25:35 - eval: epoch: 088, acc1: 54.748%, acc5: 78.880%, test_loss: 1.9438, per_image_load_time: 2.536ms, per_image_inference_time: 0.214ms
2022-02-26 05:25:35 - until epoch: 088, best_acc1: 54.848%
2022-02-26 05:25:35 - epoch 089 lr: 0.0010000000000000002
2022-02-26 05:26:15 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 2.5012
2022-02-26 05:26:50 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 2.1753
2022-02-26 05:27:24 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 2.2392
2022-02-26 05:27:59 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 2.1140
2022-02-26 05:28:33 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 2.3416
2022-02-26 05:29:07 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 2.0829
2022-02-26 05:29:41 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 2.5593
2022-02-26 05:30:15 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 2.7071
2022-02-26 05:30:50 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 2.1489
2022-02-26 05:31:24 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 2.4721
2022-02-26 05:31:59 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 2.1672
2022-02-26 05:32:33 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 2.4979
2022-02-26 05:33:07 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 2.2674
2022-02-26 05:33:42 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 2.3967
2022-02-26 05:34:17 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 2.5334
2022-02-26 05:34:50 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 2.3559
2022-02-26 05:35:25 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 2.1658
2022-02-26 05:35:59 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 2.2472
2022-02-26 05:36:34 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 2.2141
2022-02-26 05:37:08 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 2.2996
2022-02-26 05:37:42 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 2.1207
2022-02-26 05:38:17 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 2.2047
2022-02-26 05:38:51 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 2.2597
2022-02-26 05:39:26 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 2.4624
2022-02-26 05:40:01 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 2.4547
2022-02-26 05:40:35 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 2.1845
2022-02-26 05:41:10 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 2.2164
2022-02-26 05:41:44 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 2.5172
2022-02-26 05:42:19 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 2.2578
2022-02-26 05:42:53 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 2.2772
2022-02-26 05:43:28 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 2.3129
2022-02-26 05:44:02 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 2.4821
2022-02-26 05:44:37 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 2.4150
2022-02-26 05:45:12 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 2.2200
2022-02-26 05:45:47 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.8651
2022-02-26 05:46:21 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 2.1998
2022-02-26 05:46:55 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 2.4615
2022-02-26 05:47:30 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 2.2781
2022-02-26 05:48:05 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 2.4198
2022-02-26 05:48:39 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 2.2058
2022-02-26 05:49:14 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 2.2825
2022-02-26 05:49:48 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 2.1550
2022-02-26 05:50:23 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 2.2252
2022-02-26 05:50:58 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 2.3378
2022-02-26 05:51:33 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 2.0162
2022-02-26 05:52:07 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 2.1299
2022-02-26 05:52:43 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 2.2224
2022-02-26 05:53:19 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.9590
2022-02-26 05:53:54 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 2.1890
2022-02-26 05:54:28 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.9770
2022-02-26 05:54:30 - train: epoch 089, train_loss: 2.2649
2022-02-26 05:55:47 - eval: epoch: 089, acc1: 54.690%, acc5: 78.678%, test_loss: 1.9495, per_image_load_time: 2.740ms, per_image_inference_time: 0.211ms
2022-02-26 05:55:47 - until epoch: 089, best_acc1: 54.848%
2022-02-26 05:55:47 - epoch 090 lr: 0.0010000000000000002
2022-02-26 05:56:27 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 2.2290
2022-02-26 05:57:01 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 2.2205
2022-02-26 05:57:34 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 2.1866
2022-02-26 05:58:09 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 2.1132
2022-02-26 05:58:43 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 2.2227
2022-02-26 05:59:18 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 2.3701
2022-02-26 05:59:51 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 2.3177
2022-02-26 06:00:25 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 2.3663
2022-02-26 06:01:00 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 2.0923
2022-02-26 06:01:34 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.9759
2022-02-26 06:02:09 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 2.1426
2022-02-26 06:02:43 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 2.1026
2022-02-26 06:03:18 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 2.0922
2022-02-26 06:03:53 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 2.2544
2022-02-26 06:04:27 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 2.4784
2022-02-26 06:05:02 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 2.2365
2022-02-26 06:05:35 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 2.0353
2022-02-26 06:06:09 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 2.0617
2022-02-26 06:06:44 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 2.2023
2022-02-26 06:07:18 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 2.3456
2022-02-26 06:07:53 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 2.4684
2022-02-26 06:08:28 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 2.3876
2022-02-26 06:09:02 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 2.4965
2022-02-26 06:09:36 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 2.4890
2022-02-26 06:10:11 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 2.1491
2022-02-26 06:10:45 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 2.1248
2022-02-26 06:11:20 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 2.0041
2022-02-26 06:11:54 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 2.1841
2022-02-26 06:12:29 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 2.1958
2022-02-26 06:13:03 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 2.3729
2022-02-26 06:13:38 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 2.1678
2022-02-26 06:14:12 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 2.3391
2022-02-26 06:14:47 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 2.6346
2022-02-26 06:15:21 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 2.1594
2022-02-26 06:15:55 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 2.1612
2022-02-26 06:16:30 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 2.1395
2022-02-26 06:17:04 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 2.5594
2022-02-26 06:17:38 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 2.2049
2022-02-26 06:18:13 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 2.0758
2022-02-26 06:18:48 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 2.3002
2022-02-26 06:19:22 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 2.4873
2022-02-26 06:19:57 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 2.5230
2022-02-26 06:20:32 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 2.1187
2022-02-26 06:21:06 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 2.1846
2022-02-26 06:21:41 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 2.1172
2022-02-26 06:22:15 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 2.3514
2022-02-26 06:22:50 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 2.1438
2022-02-26 06:23:25 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 2.5445
2022-02-26 06:24:01 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 2.2476
2022-02-26 06:24:34 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 2.0887
2022-02-26 06:24:36 - train: epoch 090, train_loss: 2.2629
2022-02-26 06:25:54 - eval: epoch: 090, acc1: 54.814%, acc5: 78.766%, test_loss: 1.9418, per_image_load_time: 2.710ms, per_image_inference_time: 0.206ms
2022-02-26 06:25:54 - until epoch: 090, best_acc1: 54.848%
2022-02-26 06:25:54 - epoch 091 lr: 0.00010000000000000003
2022-02-26 06:26:33 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 2.0688
2022-02-26 06:27:07 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 2.2203
2022-02-26 06:27:41 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 2.2017
2022-02-26 06:28:17 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 2.2556
2022-02-26 06:28:50 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 2.1551
2022-02-26 06:29:26 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 2.5643
2022-02-26 06:29:59 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 2.3250
2022-02-26 06:30:33 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 2.0353
2022-02-26 06:31:07 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 2.2862
2022-02-26 06:31:42 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 2.0650
2022-02-26 06:32:16 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.9251
2022-02-26 06:32:51 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 2.1983
2022-02-26 06:33:26 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 2.3586
2022-02-26 06:34:00 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 2.1725
2022-02-26 06:34:34 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 2.1938
2022-02-26 06:35:09 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 2.2301
2022-02-26 06:35:43 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 2.2680
2022-02-26 06:36:17 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 2.3814
2022-02-26 06:36:51 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 2.3050
2022-02-26 06:37:25 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 2.2147
2022-02-26 06:38:00 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 2.1968
2022-02-26 06:38:35 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 2.3731
2022-02-26 06:39:09 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 2.2617
2022-02-26 06:39:44 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 2.2004
2022-02-26 06:40:18 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 2.1731
2022-02-26 06:40:53 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 2.2196
2022-02-26 06:41:27 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 2.2523
2022-02-26 06:42:01 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 2.2970
2022-02-26 06:42:36 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 2.1399
2022-02-26 06:43:11 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 2.4333
2022-02-26 06:43:45 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 2.2536
2022-02-26 06:44:19 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 2.4153
2022-02-26 06:44:52 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 2.2253
2022-02-26 06:45:28 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 2.3646
2022-02-26 06:46:01 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 2.1645
2022-02-26 06:46:36 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 2.3524
2022-02-26 06:47:11 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 2.4115
2022-02-26 06:47:45 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 2.3336
2022-02-26 06:48:19 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 2.0814
2022-02-26 06:48:54 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 2.0538
2022-02-26 06:49:28 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 2.1384
2022-02-26 06:50:03 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 2.2087
2022-02-26 06:50:37 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 2.0583
2022-02-26 06:51:13 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.9959
2022-02-26 06:51:47 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 2.1452
2022-02-26 06:52:22 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 2.3175
2022-02-26 06:52:56 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 2.4035
2022-02-26 06:53:32 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 2.3180
2022-02-26 06:54:06 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 2.2290
2022-02-26 06:54:41 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 2.5049
2022-02-26 06:54:43 - train: epoch 091, train_loss: 2.2402
2022-02-26 06:56:01 - eval: epoch: 091, acc1: 55.368%, acc5: 79.114%, test_loss: 1.9228, per_image_load_time: 2.730ms, per_image_inference_time: 0.222ms
2022-02-26 06:56:01 - until epoch: 091, best_acc1: 55.368%
2022-02-26 06:56:01 - epoch 092 lr: 0.00010000000000000003
2022-02-26 06:56:41 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 2.0441
2022-02-26 06:57:16 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 2.1559
2022-02-26 06:57:50 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 2.2874
2022-02-26 06:58:25 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 2.2912
2022-02-26 06:58:59 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 2.2128
2022-02-26 06:59:33 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 2.2052
2022-02-26 07:00:07 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 2.2386
2022-02-26 07:00:41 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 2.3777
2022-02-26 07:01:15 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 2.4154
2022-02-26 07:01:50 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 2.1523
2022-02-26 07:02:24 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 2.2903
2022-02-26 07:02:58 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 2.1787
2022-02-26 07:03:32 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 2.0878
2022-02-26 07:04:08 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 2.1944
2022-02-26 07:04:41 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 2.0751
2022-02-26 07:05:17 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 2.1700
2022-02-26 07:05:50 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 2.0817
2022-02-26 07:06:25 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 2.1745
2022-02-26 07:06:59 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 2.1068
2022-02-26 07:07:33 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 2.0718
2022-02-26 07:08:08 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 2.1786
2022-02-26 07:08:42 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 2.1312
2022-02-26 07:09:17 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 2.3995
2022-02-26 07:09:52 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 2.2254
2022-02-26 07:10:25 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 2.1637
2022-02-26 07:11:00 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 2.1528
2022-02-26 07:11:34 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 2.4131
2022-02-26 07:12:09 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.9691
2022-02-26 07:12:43 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 2.4018
2022-02-26 07:13:17 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 2.1372
2022-02-26 07:13:52 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 2.3094
2022-02-26 07:14:26 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 2.1225
2022-02-26 07:15:02 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 2.2720
2022-02-26 07:15:37 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 2.3320
2022-02-26 07:16:10 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 2.3159
2022-02-26 07:16:45 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 2.1404
2022-02-26 07:17:19 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 2.1454
2022-02-26 07:17:54 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 2.2233
2022-02-26 07:18:28 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 2.2735
2022-02-26 07:19:03 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 2.3315
2022-02-26 07:19:37 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 2.0507
2022-02-26 07:20:12 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 2.4528
2022-02-26 07:20:47 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 2.1368
2022-02-26 07:21:21 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 2.1961
2022-02-26 07:21:56 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 2.2514
2022-02-26 07:22:30 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 2.1527
2022-02-26 07:23:06 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 2.2340
2022-02-26 07:23:41 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 2.2245
2022-02-26 07:24:16 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 2.0036
2022-02-26 07:24:50 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 2.1853
2022-02-26 07:24:53 - train: epoch 092, train_loss: 2.2360
2022-02-26 07:26:10 - eval: epoch: 092, acc1: 55.226%, acc5: 79.070%, test_loss: 1.9230, per_image_load_time: 2.353ms, per_image_inference_time: 0.235ms
2022-02-26 07:26:10 - until epoch: 092, best_acc1: 55.368%
2022-02-26 07:26:10 - epoch 093 lr: 0.00010000000000000003
2022-02-26 07:26:49 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 2.2021
2022-02-26 07:27:23 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 2.1359
2022-02-26 07:27:57 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 2.2466
2022-02-26 07:28:32 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 2.3989
2022-02-26 07:29:06 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 2.1727
2022-02-26 07:29:41 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 2.1284
2022-02-26 07:30:15 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 2.2554
2022-02-26 07:30:50 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 2.1564
2022-02-26 07:31:24 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 2.2176
2022-02-26 07:31:59 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 2.1757
2022-02-26 07:32:32 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 2.2191
2022-02-26 07:33:07 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 2.2899
2022-02-26 07:33:40 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 2.1879
2022-02-26 07:34:16 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 2.0038
2022-02-26 07:34:50 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 2.4885
2022-02-26 07:35:25 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 2.1838
2022-02-26 07:35:59 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 2.4769
2022-02-26 07:36:33 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 2.3444
2022-02-26 07:37:08 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.9486
2022-02-26 07:37:42 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 2.2262
2022-02-26 07:38:18 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 2.1672
2022-02-26 07:38:52 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 2.3843
2022-02-26 07:39:27 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 2.1779
2022-02-26 07:40:00 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 2.0040
2022-02-26 07:40:35 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 2.2468
2022-02-26 07:41:11 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 2.3564
2022-02-26 07:41:45 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 2.2354
2022-02-26 07:42:21 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 2.0355
2022-02-26 07:42:55 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.9660
2022-02-26 07:43:30 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 2.1646
2022-02-26 07:44:04 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 2.4897
2022-02-26 07:44:38 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 2.1934
2022-02-26 07:45:13 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 2.1982
2022-02-26 07:45:48 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 2.5010
2022-02-26 07:46:24 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 2.2766
2022-02-26 07:46:58 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 2.2278
2022-02-26 07:47:32 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 2.1302
2022-02-26 07:48:07 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 2.1678
2022-02-26 07:48:42 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 2.2127
2022-02-26 07:49:17 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 2.3778
2022-02-26 07:49:52 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 2.2932
2022-02-26 07:50:27 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 2.1925
2022-02-26 07:51:02 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 2.3023
2022-02-26 07:51:36 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 2.1667
2022-02-26 07:52:11 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 2.1025
2022-02-26 07:52:46 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 2.0225
2022-02-26 07:53:21 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 2.6922
2022-02-26 07:53:56 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 2.1808
2022-02-26 07:54:31 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 2.1396
2022-02-26 07:55:05 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 2.1982
2022-02-26 07:55:07 - train: epoch 093, train_loss: 2.2308
2022-02-26 07:56:25 - eval: epoch: 093, acc1: 55.296%, acc5: 79.022%, test_loss: 1.9226, per_image_load_time: 2.052ms, per_image_inference_time: 0.234ms
2022-02-26 07:56:25 - until epoch: 093, best_acc1: 55.368%
2022-02-26 20:01:05 - epoch 094 lr: 0.00010000000000000003
2022-02-26 20:02:18 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 2.2651
2022-02-26 20:02:52 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 2.5517
2022-02-26 20:03:25 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 2.2130
2022-02-26 20:03:58 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 2.0964
2022-02-26 20:04:31 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 2.1193
2022-02-26 20:05:05 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 2.2948
2022-02-26 20:05:38 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 2.2098
2022-02-26 20:06:11 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 2.2076
2022-02-26 20:06:45 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 2.2479
2022-02-26 20:07:19 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 2.2789
2022-02-26 20:07:52 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.9816
2022-02-26 20:08:26 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 2.2077
2022-02-26 20:08:59 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 2.1520
2022-02-26 20:09:33 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 2.2027
2022-02-26 20:10:05 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 2.5482
2022-02-26 20:10:40 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 2.0969
2022-02-26 20:11:13 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 2.1500
2022-02-26 20:11:47 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 2.0838
2022-02-26 20:12:20 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 2.0250
2022-02-26 20:12:54 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 2.0831
2022-02-26 20:13:28 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.9570
2022-02-26 20:14:02 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 2.1552
2022-02-26 20:14:35 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 2.3192
2022-02-26 20:15:09 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 2.2295
2022-02-26 20:15:43 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 2.1298
2022-02-26 20:16:16 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 2.1940
2022-02-26 20:16:49 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 2.5133
2022-02-26 20:17:23 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.8930
2022-02-26 20:17:58 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 2.2374
2022-02-26 20:18:31 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 2.5451
2022-02-26 20:19:06 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 2.2267
2022-02-26 20:19:39 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 2.3449
2022-02-26 20:20:14 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 2.1984
2022-02-26 20:20:46 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 2.4848
2022-02-26 20:21:21 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 2.0984
2022-02-26 20:21:54 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 2.3372
2022-02-26 20:22:28 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.9212
2022-02-26 20:23:01 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 2.2876
2022-02-26 20:23:35 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 2.2884
2022-02-26 20:24:09 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 2.3840
2022-02-26 20:24:43 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 2.2963
2022-02-26 20:25:16 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 2.5429
2022-02-26 20:25:50 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 2.3529
2022-02-26 20:26:25 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 2.1346
2022-02-26 20:26:58 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 2.3870
2022-02-26 20:27:33 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 2.3482
2022-02-26 20:28:06 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 2.3259
2022-02-26 20:28:41 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 2.2850
2022-02-26 20:29:12 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 2.1739
2022-02-26 20:29:13 - train: epoch 094, train_loss: 2.2322
2022-02-26 20:30:29 - eval: epoch: 094, acc1: 55.288%, acc5: 79.274%, test_loss: 1.9205, per_image_load_time: 2.639ms, per_image_inference_time: 0.209ms
2022-02-26 20:30:29 - until epoch: 094, best_acc1: 55.368%
2022-02-26 20:30:29 - epoch 095 lr: 0.00010000000000000003
2022-02-26 20:31:09 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 2.5123
2022-02-26 20:31:42 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 2.3525
2022-02-26 20:32:15 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 2.2687
2022-02-26 20:32:49 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 2.2272
2022-02-26 20:33:23 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 2.1062
2022-02-26 20:33:58 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 2.2158
2022-02-26 20:34:31 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 2.2216
2022-02-26 20:35:05 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 2.2555
2022-02-26 20:35:37 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 2.3622
2022-02-26 20:36:10 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 2.3599
2022-02-26 20:36:43 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.8879
2022-02-26 20:37:18 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 2.2365
2022-02-26 20:37:52 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 2.2516
2022-02-26 20:38:26 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 2.1503
2022-02-26 20:38:59 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 2.1147
2022-02-26 20:39:33 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 1.9270
2022-02-26 20:40:07 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 2.1996
2022-02-26 20:40:40 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 2.2914
2022-02-26 20:41:14 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 2.0023
2022-02-26 20:41:47 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 2.2825
2022-02-26 20:42:20 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 2.2152
2022-02-26 20:42:53 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 1.9998
2022-02-26 20:43:28 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 2.1695
2022-02-26 20:44:01 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 2.2488
2022-02-26 20:44:36 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 2.2001
2022-02-26 20:45:09 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 2.1762
2022-02-26 20:45:43 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 2.2933
2022-02-26 20:46:17 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 2.1266
2022-02-26 20:46:51 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 2.4327
2022-02-26 20:47:24 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 2.3027
2022-02-26 20:47:58 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 2.2147
2022-02-26 20:48:31 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 2.2410
2022-02-26 20:49:05 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 2.2981
2022-02-26 20:49:39 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.9338
2022-02-26 20:50:13 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 2.2120
2022-02-26 20:50:46 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.9466
2022-02-26 20:51:20 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 2.2599
2022-02-26 20:51:54 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 2.3454
2022-02-26 20:52:29 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 2.2938
2022-02-26 20:53:02 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.9906
2022-02-26 20:53:36 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 2.1562
2022-02-26 20:54:09 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 2.1697
2022-02-26 20:54:43 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 2.4228
2022-02-26 20:55:16 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 2.4402
2022-02-26 20:55:50 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 2.2423
2022-02-26 20:56:24 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 2.2649
2022-02-26 20:56:58 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 2.0628
2022-02-26 20:57:33 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 2.1385
2022-02-26 20:58:06 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 2.0076
2022-02-26 20:58:40 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 2.0866
2022-02-26 20:58:41 - train: epoch 095, train_loss: 2.2305
2022-02-26 20:59:58 - eval: epoch: 095, acc1: 55.464%, acc5: 79.192%, test_loss: 1.9191, per_image_load_time: 2.762ms, per_image_inference_time: 0.191ms
2022-02-26 20:59:58 - until epoch: 095, best_acc1: 55.464%
2022-02-26 20:59:58 - epoch 096 lr: 0.00010000000000000003
2022-02-26 21:00:38 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 2.3522
2022-02-26 21:01:11 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.9317
2022-02-26 21:01:43 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 2.1002
2022-02-26 21:02:17 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 2.0344
2022-02-26 21:02:50 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 2.2427
2022-02-26 21:03:25 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 2.4185
2022-02-26 21:03:58 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 2.1931
2022-02-26 21:04:32 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 2.2135
2022-02-26 21:05:06 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 2.2124
2022-02-26 21:05:40 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 2.0600
2022-02-26 21:06:12 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 2.3722
2022-02-26 21:06:47 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 2.2191
2022-02-26 21:07:21 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 2.2550
2022-02-26 21:07:54 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 2.2568
2022-02-26 21:08:27 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 2.3058
2022-02-26 21:09:00 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.9771
2022-02-26 21:09:34 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 2.0987
2022-02-26 21:10:09 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 2.2333
2022-02-26 21:10:42 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 2.4095
2022-02-26 21:11:16 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 2.2334
2022-02-26 21:11:50 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 2.2549
2022-02-26 21:12:23 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.8968
2022-02-26 21:12:58 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 2.2297
2022-02-26 21:13:31 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 2.0921
2022-02-26 21:14:05 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 2.0447
2022-02-26 21:14:37 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 2.2248
2022-02-26 21:15:12 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 2.6584
2022-02-26 21:15:46 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 2.1746
2022-02-26 21:16:20 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 2.4175
2022-02-26 21:16:54 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 2.1958
2022-02-26 21:17:28 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 2.1894
2022-02-26 21:18:02 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 2.4110
2022-02-26 21:18:36 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 2.5389
2022-02-26 21:19:11 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 2.1222
2022-02-26 21:19:44 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 2.1262
2022-02-26 21:20:17 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 2.0128
2022-02-26 21:20:51 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 2.3002
2022-02-26 21:21:24 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 2.0092
2022-02-26 21:21:59 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 2.0406
2022-02-26 21:22:34 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 2.4244
2022-02-26 21:23:07 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 2.1881
2022-02-26 21:23:42 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 2.2194
2022-02-26 21:24:15 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.8413
2022-02-26 21:24:50 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 2.0693
2022-02-26 21:25:23 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 2.3414
2022-02-26 21:25:59 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 2.0206
2022-02-26 21:26:32 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 2.4289
2022-02-26 21:27:06 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 2.2175
2022-02-26 21:27:39 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 2.5338
2022-02-26 21:28:12 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 2.2166
2022-02-26 21:28:13 - train: epoch 096, train_loss: 2.2317
2022-02-26 21:29:29 - eval: epoch: 096, acc1: 55.408%, acc5: 79.202%, test_loss: 1.9182, per_image_load_time: 2.438ms, per_image_inference_time: 0.186ms
2022-02-26 21:29:29 - until epoch: 096, best_acc1: 55.464%
2022-02-26 21:29:29 - epoch 097 lr: 0.00010000000000000003
2022-02-26 21:30:08 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 2.2201
2022-02-26 21:30:42 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 2.2211
2022-02-26 21:31:15 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 2.2432
2022-02-26 21:31:50 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 2.1566
2022-02-26 21:32:23 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 2.2596
2022-02-26 21:32:57 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 2.2485
2022-02-26 21:33:30 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 2.1678
2022-02-26 21:34:03 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.9984
2022-02-26 21:34:37 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 2.0480
2022-02-26 21:35:11 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 2.3483
2022-02-26 21:35:45 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.9977
2022-02-26 21:36:18 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 2.2142
2022-02-26 21:36:52 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 2.3751
2022-02-26 21:37:26 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 2.3313
2022-02-26 21:38:00 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 2.1565
2022-02-26 21:38:33 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 2.0669
2022-02-26 21:39:08 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 2.1669
2022-02-26 21:39:41 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 2.4287
2022-02-26 21:40:14 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 2.2510
2022-02-26 21:40:47 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 2.1779
2022-02-26 21:41:21 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 2.1327
2022-02-26 21:41:54 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 2.1677
2022-02-26 21:42:28 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 2.0472
2022-02-26 21:43:02 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 2.2787
2022-02-26 21:43:37 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 2.2704
2022-02-26 21:44:10 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 2.2300
2022-02-26 21:44:45 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 2.0859
2022-02-26 21:45:18 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 2.0646
2022-02-26 21:45:52 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 2.3053
2022-02-26 21:46:25 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 2.2051
2022-02-26 21:46:58 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 2.2256
2022-02-26 21:47:32 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 2.3154
2022-02-26 21:48:05 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 2.2892
2022-02-26 21:48:40 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 2.2716
2022-02-26 21:49:13 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 2.0202
2022-02-26 21:49:47 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 2.2062
2022-02-26 21:50:20 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.8503
2022-02-26 21:50:54 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 2.1103
2022-02-26 21:51:27 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 2.0919
2022-02-26 21:52:02 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 2.3093
2022-02-26 21:52:36 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 2.3077
2022-02-26 21:53:09 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 2.1058
2022-02-26 21:53:42 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 2.2814
2022-02-26 21:54:16 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 2.2676
2022-02-26 21:54:50 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 2.1838
2022-02-26 21:55:24 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 2.2305
2022-02-26 21:55:59 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 2.1596
2022-02-26 21:56:32 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 2.1997
2022-02-26 21:57:09 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 2.3438
2022-02-26 21:57:41 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 2.0013
2022-02-26 21:57:42 - train: epoch 097, train_loss: 2.2298
2022-02-26 21:58:58 - eval: epoch: 097, acc1: 55.414%, acc5: 79.182%, test_loss: 1.9190, per_image_load_time: 0.966ms, per_image_inference_time: 0.219ms
2022-02-26 21:58:58 - until epoch: 097, best_acc1: 55.464%
2022-02-26 21:58:58 - epoch 098 lr: 0.00010000000000000003
2022-02-26 21:59:36 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 2.0462
2022-02-26 22:00:09 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 2.4089
2022-02-26 22:00:43 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 2.2436
2022-02-26 22:01:16 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 2.2461
2022-02-26 22:01:50 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 2.3364
2022-02-26 22:02:24 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 2.2453
2022-02-26 22:02:58 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 2.1332
2022-02-26 22:03:31 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 2.2500
2022-02-26 22:04:05 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 2.2750
2022-02-26 22:04:39 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 2.2080
2022-02-26 22:05:12 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 2.1641
2022-02-26 22:05:45 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 2.2598
2022-02-26 22:06:18 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 2.2074
2022-02-26 22:06:52 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 2.1409
2022-02-26 22:07:25 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 2.1874
2022-02-26 22:07:59 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 2.0777
2022-02-26 22:08:32 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 2.2258
2022-02-26 22:09:07 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 2.2983
2022-02-26 22:09:41 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.9864
2022-02-26 22:10:14 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 2.1909
2022-02-26 22:10:48 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 2.3833
2022-02-26 22:11:22 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 2.1792
2022-02-26 22:11:56 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 2.2197
2022-02-26 22:12:28 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 2.2905
2022-02-26 22:13:02 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 2.2426
2022-02-26 22:13:35 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 2.2124
2022-02-26 22:14:09 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 2.2657
2022-02-26 22:14:43 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 2.2703
2022-02-26 22:15:17 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 2.1178
2022-02-26 22:15:52 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 2.1906
2022-02-26 22:16:25 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 2.1592
2022-02-26 22:16:59 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.9617
2022-02-26 22:17:33 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 2.0891
2022-02-26 22:18:07 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 2.3401
2022-02-26 22:18:40 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 2.1875
2022-02-26 22:19:13 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 2.4893
2022-02-26 22:19:46 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 2.2178
2022-02-26 22:20:21 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 2.2273
2022-02-26 22:20:54 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 2.1403
2022-02-26 22:21:29 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 2.2967
2022-02-26 22:22:02 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 2.3535
2022-02-26 22:22:36 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 2.1791
2022-02-26 22:23:09 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 2.1774
2022-02-26 22:23:43 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 2.4189
2022-02-26 22:24:17 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 2.1718
2022-02-26 22:24:52 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 2.1642
2022-02-26 22:25:25 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 2.0704
2022-02-26 22:25:58 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 2.0558
2022-02-26 22:26:33 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 2.1888
2022-02-26 22:27:07 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 2.3700
2022-02-26 22:27:08 - train: epoch 098, train_loss: 2.2296
2022-02-26 22:28:24 - eval: epoch: 098, acc1: 55.510%, acc5: 79.200%, test_loss: 1.9165, per_image_load_time: 1.711ms, per_image_inference_time: 0.187ms
2022-02-26 22:28:24 - until epoch: 098, best_acc1: 55.510%
2022-02-26 22:28:24 - epoch 099 lr: 0.00010000000000000003
2022-02-26 22:29:04 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 2.2230
2022-02-26 22:29:37 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 2.1889
2022-02-26 22:30:09 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 2.0055
2022-02-26 22:30:44 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 2.1068
2022-02-26 22:31:17 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 2.2349
2022-02-26 22:31:51 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 2.1812
2022-02-26 22:32:23 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 2.4958
2022-02-26 22:32:57 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 2.3113
2022-02-26 22:33:30 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 2.3004
2022-02-26 22:34:04 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 2.3261
2022-02-26 22:34:38 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 2.3898
2022-02-26 22:35:13 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 2.0838
2022-02-26 22:35:46 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 2.4557
2022-02-26 22:36:20 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 2.2612
2022-02-26 22:36:53 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 2.0699
2022-02-26 22:37:27 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 2.2714
2022-02-26 22:38:00 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 2.1373
2022-02-26 22:38:33 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 2.1655
2022-02-26 22:39:06 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 2.1920
2022-02-26 22:39:40 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 2.0532
2022-02-26 22:40:13 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 2.1142
2022-02-26 22:40:48 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 2.0495
2022-02-26 22:41:21 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.9823
2022-02-26 22:41:55 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 2.6776
2022-02-26 22:42:28 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 2.3079
2022-02-26 22:43:02 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 2.2960
2022-02-26 22:43:37 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 2.3521
2022-02-26 22:44:10 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 2.1645
2022-02-26 22:44:43 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 2.2276
2022-02-26 22:45:15 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 2.3672
2022-02-26 22:45:49 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 2.0818
2022-02-26 22:46:22 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 2.5879
2022-02-26 22:46:57 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 2.2452
2022-02-26 22:47:30 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 2.2700
2022-02-26 22:48:05 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 2.3843
2022-02-26 22:48:38 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 2.1575
2022-02-26 22:49:12 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 1.9873
2022-02-26 22:49:46 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 2.3553
2022-02-26 22:50:20 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 2.1577
2022-02-26 22:50:54 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 2.1694
2022-02-26 22:51:27 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 2.1308
2022-02-26 22:52:00 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 2.2910
2022-02-26 22:52:34 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 2.1663
2022-02-26 22:53:08 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 2.1841
2022-02-26 22:53:42 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 2.3579
2022-02-26 22:54:16 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 2.3100
2022-02-26 22:54:51 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 2.3048
2022-02-26 22:55:26 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 2.3042
2022-02-26 22:56:00 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 2.1850
2022-02-26 22:56:33 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 2.3809
2022-02-26 22:56:34 - train: epoch 099, train_loss: 2.2298
2022-02-26 22:57:50 - eval: epoch: 099, acc1: 55.414%, acc5: 79.164%, test_loss: 1.9189, per_image_load_time: 2.179ms, per_image_inference_time: 0.196ms
2022-02-26 22:57:50 - until epoch: 099, best_acc1: 55.510%
2022-02-26 22:57:50 - epoch 100 lr: 0.00010000000000000003
2022-02-26 22:58:28 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 2.1897
2022-02-26 22:59:01 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 2.3395
2022-02-26 22:59:35 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 2.4277
2022-02-26 23:00:08 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 1.9774
2022-02-26 23:00:43 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 2.3329
2022-02-26 23:01:17 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 2.2461
2022-02-26 23:01:50 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 2.1478
2022-02-26 23:02:23 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 2.1811
2022-02-26 23:02:57 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 2.1654
2022-02-26 23:03:31 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 2.1818
2022-02-26 23:04:05 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 2.1002
2022-02-26 23:04:38 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 2.3867
2022-02-26 23:05:11 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 2.2619
2022-02-26 23:05:45 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 2.3840
2022-02-26 23:06:19 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 2.3293
2022-02-26 23:06:52 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 2.0763
2022-02-26 23:07:26 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 2.0529
2022-02-26 23:08:00 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 2.1528
2022-02-26 23:08:33 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 2.3451
2022-02-26 23:09:07 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 2.4069
2022-02-26 23:09:41 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 2.1063
2022-02-26 23:10:15 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 2.3162
2022-02-26 23:10:49 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 2.1569
2022-02-26 23:11:21 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 2.2537
2022-02-26 23:11:55 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 2.3932
2022-02-26 23:12:29 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 2.2387
2022-02-26 23:13:03 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 2.1648
2022-02-26 23:13:36 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 2.0630
2022-02-26 23:14:11 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 2.1022
2022-02-26 23:14:44 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 2.0374
2022-02-26 23:15:18 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 2.1058
2022-02-26 23:15:52 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 2.4559
2022-02-26 23:16:26 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 2.2156
2022-02-26 23:17:01 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 2.4718
2022-02-26 23:17:33 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 2.0356
2022-02-26 23:18:06 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 2.1465
2022-02-26 23:18:40 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 2.1122
2022-02-26 23:19:14 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 2.1442
2022-02-26 23:19:48 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 2.2092
2022-02-26 23:20:20 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 2.3107
2022-02-26 23:20:55 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 2.1389
2022-02-26 23:21:29 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 2.2256
2022-02-26 23:22:03 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 2.1391
2022-02-26 23:22:38 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 2.4230
2022-02-26 23:23:12 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 2.0761
2022-02-26 23:23:45 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 2.2666
2022-02-26 23:24:18 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 2.4337
2022-02-26 23:24:53 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 2.1958
2022-02-26 23:25:27 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 2.1277
2022-02-26 23:26:00 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 2.1925
2022-02-26 23:26:01 - train: epoch 100, train_loss: 2.2288
2022-02-26 23:27:16 - eval: epoch: 100, acc1: 55.350%, acc5: 79.116%, test_loss: 1.9182, per_image_load_time: 2.028ms, per_image_inference_time: 0.222ms
2022-02-26 23:27:16 - until epoch: 100, best_acc1: 55.510%
2022-02-26 23:27:16 - train done. model: yolov5nbackbone, macs: 205.613M, params: 937.480K, train time: 49.967 hours, best_acc1: 55.510%
