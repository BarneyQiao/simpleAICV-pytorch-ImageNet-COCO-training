2022-02-22 08:05:24 - network: resnet50half
2022-02-22 08:05:24 - num_classes: 1000
2022-02-22 08:05:24 - input_image_size: 224
2022-02-22 08:05:24 - scale: 1.1428571428571428
2022-02-22 08:05:24 - trained_model_path: 
2022-02-22 08:05:24 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-22 08:05:24 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f99c5c74250>
2022-02-22 08:05:24 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f99c5c74520>
2022-02-22 08:05:24 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f99c5c74550>
2022-02-22 08:05:24 - seed: 0
2022-02-22 08:05:24 - batch_size: 256
2022-02-22 08:05:24 - num_workers: 16
2022-02-22 08:05:24 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-22 08:05:24 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-22 08:05:24 - epochs: 100
2022-02-22 08:05:24 - print_interval: 100
2022-02-22 08:05:24 - distributed: True
2022-02-22 08:05:24 - sync_bn: False
2022-02-22 08:05:24 - apex: True
2022-02-22 08:05:24 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-22 08:05:24 - gpus_num: 2
2022-02-22 08:05:24 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f99a66e7230>
2022-02-22 08:05:28 - --------------------parameters--------------------
2022-02-22 08:05:28 - name: conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-02-22 08:05:28 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-02-22 08:05:28 - name: fc.weight, grad: True
2022-02-22 08:05:28 - name: fc.bias, grad: True
2022-02-22 08:05:28 - --------------------buffers--------------------
2022-02-22 08:05:28 - name: conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-02-22 08:05:28 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-02-22 08:05:28 - epoch 001 lr: 0.1
2022-02-22 08:06:07 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9076
2022-02-22 08:06:41 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9021
2022-02-22 08:07:15 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8758
2022-02-22 08:07:49 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.7577
2022-02-22 08:08:22 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.7396
2022-02-22 08:08:57 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.5785
2022-02-22 08:09:29 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.4862
2022-02-22 08:10:03 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.3107
2022-02-22 08:10:36 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.1084
2022-02-22 08:11:11 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.1592
2022-02-22 08:11:44 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.0310
2022-02-22 08:12:17 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.8603
2022-02-22 08:12:51 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.8486
2022-02-22 08:13:24 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.7739
2022-02-22 08:13:58 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.6556
2022-02-22 08:14:31 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.7129
2022-02-22 08:15:05 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.5589
2022-02-22 08:15:38 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.5917
2022-02-22 08:16:12 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.4066
2022-02-22 08:16:46 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.4355
2022-02-22 08:17:19 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.3504
2022-02-22 08:17:53 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.2493
2022-02-22 08:18:26 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.2181
2022-02-22 08:19:01 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.1969
2022-02-22 08:19:34 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.1644
2022-02-22 08:20:08 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.3973
2022-02-22 08:20:41 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.1805
2022-02-22 08:21:15 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.0260
2022-02-22 08:21:48 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.9770
2022-02-22 08:22:23 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.0685
2022-02-22 08:22:55 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.0809
2022-02-22 08:23:29 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.9638
2022-02-22 08:24:03 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.7736
2022-02-22 08:24:37 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.7825
2022-02-22 08:25:11 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.8234
2022-02-22 08:25:45 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.8476
2022-02-22 08:26:18 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.9219
2022-02-22 08:26:52 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.6257
2022-02-22 08:27:24 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.7962
2022-02-22 08:27:59 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.6595
2022-02-22 08:28:33 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.7256
2022-02-22 08:29:06 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.6018
2022-02-22 08:29:40 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.5015
2022-02-22 08:30:14 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.3563
2022-02-22 08:30:47 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.6065
2022-02-22 08:31:21 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.6635
2022-02-22 08:31:55 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.3183
2022-02-22 08:32:29 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.4932
2022-02-22 08:33:04 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.4448
2022-02-22 08:33:36 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.2291
2022-02-22 08:33:37 - train: epoch 001, train_loss: 5.3528
2022-02-22 08:34:52 - eval: epoch: 001, acc1: 15.960%, acc5: 35.432%, test_loss: 4.3976, per_image_load_time: 2.658ms, per_image_inference_time: 0.280ms
2022-02-22 08:34:52 - until epoch: 001, best_acc1: 15.960%
2022-02-22 08:34:52 - epoch 002 lr: 0.1
2022-02-22 08:35:31 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.4085
2022-02-22 08:36:06 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.0881
2022-02-22 08:36:40 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.4571
2022-02-22 08:37:12 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.3227
2022-02-22 08:37:46 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0469
2022-02-22 08:38:19 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0783
2022-02-22 08:38:54 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.2219
2022-02-22 08:39:28 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.9863
2022-02-22 08:40:01 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.7193
2022-02-22 08:40:35 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.2685
2022-02-22 08:41:09 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.2531
2022-02-22 08:41:42 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.1266
2022-02-22 08:42:15 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.1266
2022-02-22 08:42:48 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1330
2022-02-22 08:43:22 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.0635
2022-02-22 08:43:56 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.9478
2022-02-22 08:44:30 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 4.0135
2022-02-22 08:45:04 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.0862
2022-02-22 08:45:37 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8281
2022-02-22 08:46:11 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6390
2022-02-22 08:46:45 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.9066
2022-02-22 08:47:19 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.6653
2022-02-22 08:47:53 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.8230
2022-02-22 08:48:26 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.7198
2022-02-22 08:49:00 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.6279
2022-02-22 08:49:33 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.6794
2022-02-22 08:50:07 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.9200
2022-02-22 08:50:40 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.7702
2022-02-22 08:51:14 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6204
2022-02-22 08:51:47 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5834
2022-02-22 08:52:21 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.7035
2022-02-22 08:52:54 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.6748
2022-02-22 08:53:28 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.7786
2022-02-22 08:54:02 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8194
2022-02-22 08:54:36 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6213
2022-02-22 08:55:09 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.6053
2022-02-22 08:55:43 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.6845
2022-02-22 08:56:16 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.3730
2022-02-22 08:56:50 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.6336
2022-02-22 08:57:24 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5083
2022-02-22 08:57:58 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.5961
2022-02-22 08:58:32 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.4935
2022-02-22 08:59:06 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.4566
2022-02-22 08:59:39 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.3960
2022-02-22 09:00:13 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.3809
2022-02-22 09:00:47 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.3929
2022-02-22 09:01:21 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.4572
2022-02-22 09:01:55 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.5486
2022-02-22 09:02:29 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.3536
2022-02-22 09:03:02 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.4165
2022-02-22 09:03:03 - train: epoch 002, train_loss: 3.8109
2022-02-22 09:04:17 - eval: epoch: 002, acc1: 29.006%, acc5: 54.868%, test_loss: 3.3446, per_image_load_time: 2.651ms, per_image_inference_time: 0.245ms
2022-02-22 09:04:17 - until epoch: 002, best_acc1: 29.006%
2022-02-22 09:04:17 - epoch 003 lr: 0.1
2022-02-22 09:04:55 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.5146
2022-02-22 09:05:29 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.4392
2022-02-22 09:06:03 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.3860
2022-02-22 09:06:37 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.4816
2022-02-22 09:07:11 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.4898
2022-02-22 09:07:44 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.3214
2022-02-22 09:08:18 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.5809
2022-02-22 09:08:51 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4430
2022-02-22 09:09:25 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.2510
2022-02-22 09:09:59 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.3292
2022-02-22 09:10:34 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1958
2022-02-22 09:11:07 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.4287
2022-02-22 09:11:41 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.2817
2022-02-22 09:12:15 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.2619
2022-02-22 09:12:49 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.6237
2022-02-22 09:13:23 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.3483
2022-02-22 09:13:56 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.3221
2022-02-22 09:14:29 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1692
2022-02-22 09:15:04 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.3780
2022-02-22 09:15:37 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.5298
2022-02-22 09:16:11 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4788
2022-02-22 09:16:45 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.7108
2022-02-22 09:17:18 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.3132
2022-02-22 09:17:52 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.2146
2022-02-22 09:18:26 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3158
2022-02-22 09:19:00 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.3068
2022-02-22 09:19:34 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5644
2022-02-22 09:20:07 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.2044
2022-02-22 09:20:41 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.1745
2022-02-22 09:21:14 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.4490
2022-02-22 09:21:49 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.3111
2022-02-22 09:22:21 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2857
2022-02-22 09:22:55 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.2471
2022-02-22 09:23:28 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.4118
2022-02-22 09:24:02 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.0085
2022-02-22 09:24:36 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.1561
2022-02-22 09:25:09 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.1018
2022-02-22 09:25:43 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.2720
2022-02-22 09:26:18 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.3085
2022-02-22 09:26:51 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1498
2022-02-22 09:27:25 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.2059
2022-02-22 09:27:58 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.1910
2022-02-22 09:28:32 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.7602
2022-02-22 09:29:05 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9667
2022-02-22 09:29:39 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.1536
2022-02-22 09:30:13 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0966
2022-02-22 09:30:47 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 3.1128
2022-02-22 09:31:20 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.2265
2022-02-22 09:31:54 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.2151
2022-02-22 09:32:26 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.2306
2022-02-22 09:32:28 - train: epoch 003, train_loss: 3.2640
2022-02-22 09:33:42 - eval: epoch: 003, acc1: 33.962%, acc5: 60.156%, test_loss: 3.0663, per_image_load_time: 2.268ms, per_image_inference_time: 0.274ms
2022-02-22 09:33:43 - until epoch: 003, best_acc1: 33.962%
2022-02-22 09:33:43 - epoch 004 lr: 0.1
2022-02-22 09:34:21 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.1464
2022-02-22 09:34:55 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.0054
2022-02-22 09:35:29 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.2163
2022-02-22 09:36:02 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.9959
2022-02-22 09:36:36 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.9237
2022-02-22 09:37:09 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.2825
2022-02-22 09:37:43 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.2127
2022-02-22 09:38:17 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.9624
2022-02-22 09:38:50 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.8590
2022-02-22 09:39:24 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.0068
2022-02-22 09:39:58 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.1268
2022-02-22 09:40:32 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.8056
2022-02-22 09:41:05 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.7750
2022-02-22 09:41:38 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0911
2022-02-22 09:42:12 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.1019
2022-02-22 09:42:46 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.9283
2022-02-22 09:43:20 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1404
2022-02-22 09:43:54 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.2573
2022-02-22 09:44:27 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.1175
2022-02-22 09:45:03 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.9889
2022-02-22 09:45:35 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 3.0609
2022-02-22 09:46:09 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.9341
2022-02-22 09:46:42 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.7820
2022-02-22 09:47:16 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8388
2022-02-22 09:47:50 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.8978
2022-02-22 09:48:23 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.0417
2022-02-22 09:48:56 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.8224
2022-02-22 09:49:30 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.9544
2022-02-22 09:50:05 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.9982
2022-02-22 09:50:38 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.9766
2022-02-22 09:51:12 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.0635
2022-02-22 09:51:46 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.8567
2022-02-22 09:52:19 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 3.1429
2022-02-22 09:52:53 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 3.0230
2022-02-22 09:53:27 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.9482
2022-02-22 09:54:00 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.7935
2022-02-22 09:54:35 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.0263
2022-02-22 09:55:08 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.9613
2022-02-22 09:55:42 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.8770
2022-02-22 09:56:16 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.7076
2022-02-22 09:56:49 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.9644
2022-02-22 09:57:23 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.7134
2022-02-22 09:57:57 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.7538
2022-02-22 09:58:30 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.9049
2022-02-22 09:59:04 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.4156
2022-02-22 09:59:39 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.8963
2022-02-22 10:00:12 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.8441
2022-02-22 10:00:46 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.8009
2022-02-22 10:01:19 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.9263
2022-02-22 10:01:52 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.9861
2022-02-22 10:01:53 - train: epoch 004, train_loss: 3.0099
2022-02-22 10:03:09 - eval: epoch: 004, acc1: 37.936%, acc5: 64.476%, test_loss: 2.8490, per_image_load_time: 1.890ms, per_image_inference_time: 0.247ms
2022-02-22 10:03:09 - until epoch: 004, best_acc1: 37.936%
2022-02-22 10:03:09 - epoch 005 lr: 0.1
2022-02-22 10:03:48 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 3.0009
2022-02-22 10:04:22 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.9617
2022-02-22 10:04:55 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0372
2022-02-22 10:05:28 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.9184
2022-02-22 10:06:02 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.7254
2022-02-22 10:06:35 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.8582
2022-02-22 10:07:09 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.8997
2022-02-22 10:07:42 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.1343
2022-02-22 10:08:16 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.8203
2022-02-22 10:08:49 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.9290
2022-02-22 10:09:24 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.9564
2022-02-22 10:09:57 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 3.0856
2022-02-22 10:10:30 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.8186
2022-02-22 10:11:04 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.9688
2022-02-22 10:11:38 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.7142
2022-02-22 10:12:11 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.6539
2022-02-22 10:12:45 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.7738
2022-02-22 10:13:19 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.9682
2022-02-22 10:13:52 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6919
2022-02-22 10:14:26 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.9146
2022-02-22 10:15:00 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7009
2022-02-22 10:15:34 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.7850
2022-02-22 10:16:08 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.7199
2022-02-22 10:16:41 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.8768
2022-02-22 10:17:14 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.9587
2022-02-22 10:17:49 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.0520
2022-02-22 10:18:23 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 3.0380
2022-02-22 10:18:57 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.9183
2022-02-22 10:19:31 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.7958
2022-02-22 10:20:04 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.7193
2022-02-22 10:20:38 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.8078
2022-02-22 10:21:12 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.0109
2022-02-22 10:21:46 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.5752
2022-02-22 10:22:19 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.6624
2022-02-22 10:22:53 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.9263
2022-02-22 10:23:26 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8957
2022-02-22 10:24:00 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.7611
2022-02-22 10:24:34 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.5936
2022-02-22 10:25:09 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.0524
2022-02-22 10:25:41 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.7604
2022-02-22 10:26:16 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.8248
2022-02-22 10:26:49 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.8532
2022-02-22 10:27:24 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.6828
2022-02-22 10:27:56 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.8806
2022-02-22 10:28:30 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.9173
2022-02-22 10:29:04 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.8021
2022-02-22 10:29:38 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.6578
2022-02-22 10:30:12 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.7039
2022-02-22 10:30:45 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8982
2022-02-22 10:31:18 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.8100
2022-02-22 10:31:19 - train: epoch 005, train_loss: 2.8595
2022-02-22 10:32:34 - eval: epoch: 005, acc1: 39.772%, acc5: 67.004%, test_loss: 2.7016, per_image_load_time: 2.658ms, per_image_inference_time: 0.280ms
2022-02-22 10:32:34 - until epoch: 005, best_acc1: 39.772%
2022-02-22 10:32:34 - epoch 006 lr: 0.1
2022-02-22 10:33:13 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.7340
2022-02-22 10:33:47 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.8882
2022-02-22 10:34:20 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.7041
2022-02-22 10:34:53 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.0040
2022-02-22 10:35:27 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.8147
2022-02-22 10:36:00 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.7766
2022-02-22 10:36:35 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.9214
2022-02-22 10:37:08 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.6761
2022-02-22 10:37:42 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.7168
2022-02-22 10:38:15 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.7152
2022-02-22 10:38:49 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.8226
2022-02-22 10:39:22 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.8693
2022-02-22 10:39:57 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.9624
2022-02-22 10:40:31 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.9552
2022-02-22 10:41:05 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.8648
2022-02-22 10:41:38 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.5906
2022-02-22 10:42:13 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8812
2022-02-22 10:42:47 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8783
2022-02-22 10:43:21 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.7389
2022-02-22 10:43:54 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.9356
2022-02-22 10:44:28 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.8180
2022-02-22 10:45:02 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.6515
2022-02-22 10:45:36 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.5550
2022-02-22 10:46:09 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7584
2022-02-22 10:46:43 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9642
2022-02-22 10:47:16 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.7031
2022-02-22 10:47:51 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.7634
2022-02-22 10:48:23 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.5547
2022-02-22 10:48:58 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.8399
2022-02-22 10:49:30 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6759
2022-02-22 10:50:04 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.5210
2022-02-22 10:50:37 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.6909
2022-02-22 10:51:10 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5788
2022-02-22 10:51:45 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.9321
2022-02-22 10:52:18 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.7265
2022-02-22 10:52:52 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.7741
2022-02-22 10:53:25 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.6730
2022-02-22 10:53:59 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.6009
2022-02-22 10:54:33 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5687
2022-02-22 10:55:06 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.9764
2022-02-22 10:55:41 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.7192
2022-02-22 10:56:14 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.5793
2022-02-22 10:56:47 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6859
2022-02-22 10:57:22 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.6973
2022-02-22 10:57:55 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.7347
2022-02-22 10:58:29 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.6856
2022-02-22 10:59:02 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.7313
2022-02-22 10:59:37 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.7055
2022-02-22 11:00:11 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.7271
2022-02-22 11:00:44 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.5667
2022-02-22 11:00:45 - train: epoch 006, train_loss: 2.7614
2022-02-22 11:01:59 - eval: epoch: 006, acc1: 40.560%, acc5: 67.372%, test_loss: 2.6656, per_image_load_time: 2.440ms, per_image_inference_time: 0.294ms
2022-02-22 11:01:59 - until epoch: 006, best_acc1: 40.560%
2022-02-22 11:01:59 - epoch 007 lr: 0.1
2022-02-22 11:02:38 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.6303
2022-02-22 11:03:12 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.9176
2022-02-22 11:03:46 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9401
2022-02-22 11:04:20 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7389
2022-02-22 11:04:53 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.6421
2022-02-22 11:05:27 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.8729
2022-02-22 11:06:01 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.7260
2022-02-22 11:06:34 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.7699
2022-02-22 11:07:09 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.6901
2022-02-22 11:07:42 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.7765
2022-02-22 11:08:15 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.5863
2022-02-22 11:08:50 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.7628
2022-02-22 11:09:24 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.7277
2022-02-22 11:09:58 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.6926
2022-02-22 11:10:31 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8107
2022-02-22 11:11:05 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.7198
2022-02-22 11:11:38 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7928
2022-02-22 11:12:12 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.6414
2022-02-22 11:12:45 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.7059
2022-02-22 11:13:19 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.5770
2022-02-22 11:13:53 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.7752
2022-02-22 11:14:27 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.6864
2022-02-22 11:15:00 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.7740
2022-02-22 11:15:34 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.7816
2022-02-22 11:16:08 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.5951
2022-02-22 11:16:41 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.6230
2022-02-22 11:17:15 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.5680
2022-02-22 11:17:47 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6825
2022-02-22 11:18:21 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.5830
2022-02-22 11:18:56 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.7158
2022-02-22 11:19:29 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.5497
2022-02-22 11:20:03 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.5982
2022-02-22 11:20:36 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.8746
2022-02-22 11:21:11 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.5544
2022-02-22 11:21:44 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.7358
2022-02-22 11:22:18 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.4817
2022-02-22 11:22:51 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.7541
2022-02-22 11:23:25 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.7826
2022-02-22 11:23:58 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.5733
2022-02-22 11:24:32 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.8313
2022-02-22 11:25:05 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.6268
2022-02-22 11:25:40 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.5716
2022-02-22 11:26:13 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.7923
2022-02-22 11:26:47 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.4811
2022-02-22 11:27:20 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.8656
2022-02-22 11:27:54 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.6268
2022-02-22 11:28:27 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.7775
2022-02-22 11:29:02 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.9658
2022-02-22 11:29:36 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.6757
2022-02-22 11:30:08 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.7454
2022-02-22 11:30:09 - train: epoch 007, train_loss: 2.6902
2022-02-22 11:31:23 - eval: epoch: 007, acc1: 39.860%, acc5: 66.716%, test_loss: 2.6966, per_image_load_time: 2.513ms, per_image_inference_time: 0.250ms
2022-02-22 11:31:24 - until epoch: 007, best_acc1: 40.560%
2022-02-22 11:31:24 - epoch 008 lr: 0.1
2022-02-22 11:32:03 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6097
2022-02-22 11:32:37 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.8593
2022-02-22 11:33:09 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.4630
2022-02-22 11:33:44 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.5569
2022-02-22 11:34:17 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.6033
2022-02-22 11:34:51 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.5438
2022-02-22 11:35:25 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.8621
2022-02-22 11:35:59 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.4103
2022-02-22 11:36:33 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.6631
2022-02-22 11:37:07 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.6917
2022-02-22 11:37:40 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.4100
2022-02-22 11:38:14 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.4317
2022-02-22 11:38:47 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.7377
2022-02-22 11:39:22 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.5195
2022-02-22 11:39:56 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.6670
2022-02-22 11:40:30 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.6840
2022-02-22 11:41:03 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.6067
2022-02-22 11:41:37 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.8150
2022-02-22 11:42:10 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.3880
2022-02-22 11:42:44 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.6952
2022-02-22 11:43:18 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.6435
2022-02-22 11:43:52 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.4658
2022-02-22 11:44:25 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.6867
2022-02-22 11:44:59 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.5662
2022-02-22 11:45:32 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.5760
2022-02-22 11:46:05 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.6708
2022-02-22 11:46:38 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.6950
2022-02-22 11:47:12 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.6808
2022-02-22 11:47:45 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.6363
2022-02-22 11:48:19 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.7937
2022-02-22 11:48:52 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.5752
2022-02-22 11:49:26 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.8511
2022-02-22 11:49:59 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.8840
2022-02-22 11:50:32 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.8185
2022-02-22 11:51:05 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.5596
2022-02-22 11:51:40 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7028
2022-02-22 11:52:13 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.6159
2022-02-22 11:52:47 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.5524
2022-02-22 11:53:20 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7452
2022-02-22 11:53:54 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.9100
2022-02-22 11:54:28 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5703
2022-02-22 11:55:01 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.6175
2022-02-22 11:55:35 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.3802
2022-02-22 11:56:08 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.5825
2022-02-22 11:56:42 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.8417
2022-02-22 11:57:15 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.8347
2022-02-22 11:57:49 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.6238
2022-02-22 11:58:23 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.6296
2022-02-22 11:58:57 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.6652
2022-02-22 11:59:31 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.6031
2022-02-22 11:59:32 - train: epoch 008, train_loss: 2.6355
2022-02-22 12:00:47 - eval: epoch: 008, acc1: 44.708%, acc5: 71.166%, test_loss: 2.4609, per_image_load_time: 2.653ms, per_image_inference_time: 0.266ms
2022-02-22 12:00:48 - until epoch: 008, best_acc1: 44.708%
2022-02-22 12:00:48 - epoch 009 lr: 0.1
2022-02-22 12:01:26 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3986
2022-02-22 12:02:00 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.4300
2022-02-22 12:02:34 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3047
2022-02-22 12:03:08 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.8098
2022-02-22 12:03:41 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.5826
2022-02-22 12:04:14 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.6063
2022-02-22 12:04:48 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.5375
2022-02-22 12:05:22 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.4458
2022-02-22 12:05:56 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.3578
2022-02-22 12:06:29 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.4783
2022-02-22 12:07:03 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.8730
2022-02-22 12:07:37 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.6190
2022-02-22 12:08:10 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.7368
2022-02-22 12:08:43 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.3228
2022-02-22 12:09:18 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.4811
2022-02-22 12:09:52 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.6690
2022-02-22 12:10:26 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.8055
2022-02-22 12:10:59 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.5477
2022-02-22 12:11:33 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.3273
2022-02-22 12:12:08 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.4263
2022-02-22 12:12:41 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.6239
2022-02-22 12:13:15 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.6805
2022-02-22 12:13:49 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.3492
2022-02-22 12:14:22 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.4834
2022-02-22 12:14:57 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.4580
2022-02-22 12:15:30 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.6207
2022-02-22 12:16:04 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.5649
2022-02-22 12:16:39 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.6285
2022-02-22 12:17:12 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.2897
2022-02-22 12:17:45 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.5330
2022-02-22 12:18:20 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.6276
2022-02-22 12:18:53 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.5765
2022-02-22 12:19:27 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5791
2022-02-22 12:20:01 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8123
2022-02-22 12:20:34 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6957
2022-02-22 12:21:08 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.5384
2022-02-22 12:21:41 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.7665
2022-02-22 12:22:15 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.7938
2022-02-22 12:22:48 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.3176
2022-02-22 12:23:22 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.7884
2022-02-22 12:23:55 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.6317
2022-02-22 12:24:29 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.5822
2022-02-22 12:25:03 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.7014
2022-02-22 12:25:36 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.5898
2022-02-22 12:26:10 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.5727
2022-02-22 12:26:44 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.6349
2022-02-22 12:27:17 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.8078
2022-02-22 12:27:51 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.8542
2022-02-22 12:28:25 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.6956
2022-02-22 12:28:58 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.4781
2022-02-22 12:28:59 - train: epoch 009, train_loss: 2.5953
2022-02-22 12:30:14 - eval: epoch: 009, acc1: 44.424%, acc5: 71.112%, test_loss: 2.4532, per_image_load_time: 2.646ms, per_image_inference_time: 0.265ms
2022-02-22 12:30:14 - until epoch: 009, best_acc1: 44.708%
2022-02-22 12:30:14 - epoch 010 lr: 0.1
2022-02-22 12:30:54 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.5348
2022-02-22 12:31:27 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.7247
2022-02-22 12:32:01 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.5401
2022-02-22 12:32:35 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.6378
2022-02-22 12:33:07 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.5467
2022-02-22 12:33:42 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.6709
2022-02-22 12:34:15 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.6737
2022-02-22 12:34:49 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.5123
2022-02-22 12:35:22 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.4298
2022-02-22 12:35:57 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.4046
2022-02-22 12:36:30 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.5143
2022-02-22 12:37:03 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.4897
2022-02-22 12:37:38 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.4232
2022-02-22 12:38:11 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.6708
2022-02-22 12:38:45 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.3263
2022-02-22 12:39:18 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.6066
2022-02-22 12:39:51 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.6676
2022-02-22 12:40:24 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5141
2022-02-22 12:40:57 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.6223
2022-02-22 12:41:32 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.5828
2022-02-22 12:42:05 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.4529
2022-02-22 12:42:39 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.7391
2022-02-22 12:43:13 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.7357
2022-02-22 12:43:47 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.7028
2022-02-22 12:44:21 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.5720
2022-02-22 12:44:54 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.6804
2022-02-22 12:45:28 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.4105
2022-02-22 12:46:01 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5551
2022-02-22 12:46:35 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.7396
2022-02-22 12:47:09 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.5950
2022-02-22 12:47:43 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.7422
2022-02-22 12:48:17 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.5363
2022-02-22 12:48:50 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.6472
2022-02-22 12:49:24 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.7696
2022-02-22 12:49:57 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.7700
2022-02-22 12:50:31 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.8110
2022-02-22 12:51:04 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.5196
2022-02-22 12:51:38 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.6949
2022-02-22 12:52:12 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.2452
2022-02-22 12:52:46 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.6029
2022-02-22 12:53:20 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.3485
2022-02-22 12:53:53 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.6576
2022-02-22 12:54:27 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.4999
2022-02-22 12:55:01 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.5367
2022-02-22 12:55:35 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.3749
2022-02-22 12:56:09 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6901
2022-02-22 12:56:42 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.6766
2022-02-22 12:57:15 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.4350
2022-02-22 12:57:50 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.4204
2022-02-22 12:58:22 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.3570
2022-02-22 12:58:24 - train: epoch 010, train_loss: 2.5636
2022-02-22 12:59:39 - eval: epoch: 010, acc1: 40.146%, acc5: 66.224%, test_loss: 2.7391, per_image_load_time: 2.609ms, per_image_inference_time: 0.254ms
2022-02-22 12:59:39 - until epoch: 010, best_acc1: 44.708%
2022-02-22 12:59:39 - epoch 011 lr: 0.1
2022-02-22 13:00:18 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.3590
2022-02-22 13:00:52 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.6976
2022-02-22 13:01:25 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.3383
2022-02-22 13:01:58 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.6093
2022-02-22 13:02:33 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.4857
2022-02-22 13:03:06 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.5906
2022-02-22 13:03:39 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.5461
2022-02-22 13:04:13 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5246
2022-02-22 13:04:46 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.5799
2022-02-22 13:05:20 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.4212
2022-02-22 13:05:54 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.5477
2022-02-22 13:06:28 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.9290
2022-02-22 13:07:01 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.6723
2022-02-22 13:07:36 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.6438
2022-02-22 13:08:08 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.4518
2022-02-22 13:08:42 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.5480
2022-02-22 13:09:16 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.6795
2022-02-22 13:09:50 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.4367
2022-02-22 13:10:25 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.3909
2022-02-22 13:10:58 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.7211
2022-02-22 13:11:31 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.5829
2022-02-22 13:12:06 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.4947
2022-02-22 13:12:39 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.8445
2022-02-22 13:13:14 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.3773
2022-02-22 13:13:47 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.7631
2022-02-22 13:14:21 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.5856
2022-02-22 13:14:54 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.5081
2022-02-22 13:15:28 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.1878
2022-02-22 13:16:02 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.6087
2022-02-22 13:16:36 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.7197
2022-02-22 13:17:09 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.5365
2022-02-22 13:17:43 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.3356
2022-02-22 13:18:17 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.6295
2022-02-22 13:18:51 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.5723
2022-02-22 13:19:24 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.6488
2022-02-22 13:19:58 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.6869
2022-02-22 13:20:32 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.6026
2022-02-22 13:21:06 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.2988
2022-02-22 13:21:40 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.6057
2022-02-22 13:22:13 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.5802
2022-02-22 13:22:46 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.2944
2022-02-22 13:23:20 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.4204
2022-02-22 13:23:54 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.5761
2022-02-22 13:24:27 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.5241
2022-02-22 13:25:02 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.3548
2022-02-22 13:25:35 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.4097
2022-02-22 13:26:09 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.1818
2022-02-22 13:26:42 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.2983
2022-02-22 13:27:16 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.3173
2022-02-22 13:27:50 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.4719
2022-02-22 13:27:51 - train: epoch 011, train_loss: 2.5364
2022-02-22 13:29:04 - eval: epoch: 011, acc1: 47.026%, acc5: 73.578%, test_loss: 2.3045, per_image_load_time: 1.685ms, per_image_inference_time: 0.283ms
2022-02-22 13:29:04 - until epoch: 011, best_acc1: 47.026%
2022-02-22 13:29:04 - epoch 012 lr: 0.1
2022-02-22 13:29:43 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.4977
2022-02-22 13:30:16 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4013
2022-02-22 13:30:49 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.4797
2022-02-22 13:31:24 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.6645
2022-02-22 13:31:57 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.7590
2022-02-22 13:32:31 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.2222
2022-02-22 13:33:05 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.4212
2022-02-22 13:33:39 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.5205
2022-02-22 13:34:12 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.5985
2022-02-22 13:34:47 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.3057
2022-02-22 13:35:20 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8852
2022-02-22 13:35:54 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.3109
2022-02-22 13:36:29 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.4603
2022-02-22 13:37:02 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.6260
2022-02-22 13:37:35 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.2851
2022-02-22 13:38:10 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.4528
2022-02-22 13:38:42 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.3859
2022-02-22 13:39:16 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.5688
2022-02-22 13:39:50 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.5731
2022-02-22 13:40:23 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.7126
2022-02-22 13:40:57 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.5444
2022-02-22 13:41:30 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.6684
2022-02-22 13:42:04 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.5093
2022-02-22 13:42:38 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.5606
2022-02-22 13:43:12 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.3843
2022-02-22 13:43:46 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.3363
2022-02-22 13:44:20 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.4462
2022-02-22 13:44:53 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.4920
2022-02-22 13:45:27 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3266
2022-02-22 13:46:01 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.4075
2022-02-22 13:46:35 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.7084
2022-02-22 13:47:08 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.2962
2022-02-22 13:47:42 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.4293
2022-02-22 13:48:15 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.4828
2022-02-22 13:48:49 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.6863
2022-02-22 13:49:23 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.5351
2022-02-22 13:49:57 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.5192
2022-02-22 13:50:30 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.5259
2022-02-22 13:51:04 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.4052
2022-02-22 13:51:38 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.4295
2022-02-22 13:52:12 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.4919
2022-02-22 13:52:46 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.3777
2022-02-22 13:53:20 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.5327
2022-02-22 13:53:53 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.3168
2022-02-22 13:54:26 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.4633
2022-02-22 13:55:00 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.7841
2022-02-22 13:55:35 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.4869
2022-02-22 13:56:08 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.5014
2022-02-22 13:56:43 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5064
2022-02-22 13:57:16 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.1615
2022-02-22 13:57:17 - train: epoch 012, train_loss: 2.5159
2022-02-22 13:58:32 - eval: epoch: 012, acc1: 47.680%, acc5: 74.194%, test_loss: 2.2827, per_image_load_time: 1.301ms, per_image_inference_time: 0.267ms
2022-02-22 13:58:32 - until epoch: 012, best_acc1: 47.680%
2022-02-22 13:58:32 - epoch 013 lr: 0.1
2022-02-22 13:59:11 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.3071
2022-02-22 13:59:45 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.4970
2022-02-22 14:00:19 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.3722
2022-02-22 14:00:51 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.4273
2022-02-22 14:01:25 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.4559
2022-02-22 14:01:59 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.6166
2022-02-22 14:02:32 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.3789
2022-02-22 14:03:05 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.6173
2022-02-22 14:03:40 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.4010
2022-02-22 14:04:13 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.5046
2022-02-22 14:04:47 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.5099
2022-02-22 14:05:20 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.6399
2022-02-22 14:05:55 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.6153
2022-02-22 14:06:29 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.3708
2022-02-22 14:07:03 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.5698
2022-02-22 14:07:36 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.2750
2022-02-22 14:08:10 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.5099
2022-02-22 14:08:44 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.5011
2022-02-22 14:09:19 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.5864
2022-02-22 14:09:52 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.7087
2022-02-22 14:10:26 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.8232
2022-02-22 14:11:00 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.4804
2022-02-22 14:11:33 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.5406
2022-02-22 14:12:07 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.5208
2022-02-22 14:12:41 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.3034
2022-02-22 14:13:14 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.4148
2022-02-22 14:13:48 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.4185
2022-02-22 14:14:21 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.5541
2022-02-22 14:14:55 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.5966
2022-02-22 14:15:28 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.3997
2022-02-22 14:16:03 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.3319
2022-02-22 14:16:36 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.5540
2022-02-22 14:17:10 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.3462
2022-02-22 14:17:44 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.3669
2022-02-22 14:18:18 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.3609
2022-02-22 14:18:51 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.7636
2022-02-22 14:19:25 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.2026
2022-02-22 14:19:59 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.6748
2022-02-22 14:20:33 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.5793
2022-02-22 14:21:07 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.5584
2022-02-22 14:21:40 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.3892
2022-02-22 14:22:14 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.4590
2022-02-22 14:22:48 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.4572
2022-02-22 14:23:22 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.3621
2022-02-22 14:23:56 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.4572
2022-02-22 14:24:30 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.5224
2022-02-22 14:25:03 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5712
2022-02-22 14:25:36 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.6119
2022-02-22 14:26:11 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.5171
2022-02-22 14:26:45 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.5868
2022-02-22 14:26:45 - train: epoch 013, train_loss: 2.4991
2022-02-22 14:28:00 - eval: epoch: 013, acc1: 46.304%, acc5: 72.508%, test_loss: 2.3642, per_image_load_time: 2.645ms, per_image_inference_time: 0.285ms
2022-02-22 14:28:00 - until epoch: 013, best_acc1: 47.680%
2022-02-22 14:28:00 - epoch 014 lr: 0.1
2022-02-22 14:28:39 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.4592
2022-02-22 14:29:13 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.6393
2022-02-22 14:29:47 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.2089
2022-02-22 14:30:20 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.3785
2022-02-22 14:30:54 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.4526
2022-02-22 14:31:28 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.4098
2022-02-22 14:32:01 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.4671
2022-02-22 14:32:35 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4597
2022-02-22 14:33:09 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5505
2022-02-22 14:33:43 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.6635
2022-02-22 14:34:16 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.4630
2022-02-22 14:34:50 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.5336
2022-02-22 14:35:24 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4812
2022-02-22 14:35:57 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.4586
2022-02-22 14:36:31 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.5468
2022-02-22 14:37:05 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.4823
2022-02-22 14:37:38 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.6859
2022-02-22 14:38:13 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.6475
2022-02-22 14:38:47 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.3856
2022-02-22 14:39:20 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.4754
2022-02-22 14:39:54 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.5878
2022-02-22 14:40:28 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.5170
2022-02-22 14:41:02 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.6067
2022-02-22 14:41:35 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6259
2022-02-22 14:42:09 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.4610
2022-02-22 14:42:43 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.4232
2022-02-22 14:43:16 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.5660
2022-02-22 14:43:50 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.8115
2022-02-22 14:44:24 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.3517
2022-02-22 14:44:57 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.5305
2022-02-22 14:45:31 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.3162
2022-02-22 14:46:04 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.4236
2022-02-22 14:46:39 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.3393
2022-02-22 14:47:13 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.3435
2022-02-22 14:47:46 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.4374
2022-02-22 14:48:20 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.3138
2022-02-22 14:48:54 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.3069
2022-02-22 14:49:27 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.5694
2022-02-22 14:50:01 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.3562
2022-02-22 14:50:34 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.6077
2022-02-22 14:51:08 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.3070
2022-02-22 14:51:42 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2886
2022-02-22 14:52:16 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.3423
2022-02-22 14:52:50 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.2679
2022-02-22 14:53:24 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.4342
2022-02-22 14:53:58 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.2992
2022-02-22 14:54:32 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.4770
2022-02-22 14:55:06 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.4556
2022-02-22 14:55:41 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.2315
2022-02-22 14:56:14 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.4798
2022-02-22 14:56:15 - train: epoch 014, train_loss: 2.4823
2022-02-22 14:57:28 - eval: epoch: 014, acc1: 46.262%, acc5: 73.124%, test_loss: 2.3531, per_image_load_time: 0.681ms, per_image_inference_time: 0.262ms
2022-02-22 14:57:29 - until epoch: 014, best_acc1: 47.680%
2022-02-22 14:57:29 - epoch 015 lr: 0.1
2022-02-22 14:58:08 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.2477
2022-02-22 14:58:41 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.7027
2022-02-22 14:59:15 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6305
2022-02-22 14:59:49 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.3834
2022-02-22 15:00:22 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.3894
2022-02-22 15:00:56 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.5981
2022-02-22 15:01:30 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.4777
2022-02-22 15:02:04 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.3007
2022-02-22 15:02:37 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.2661
2022-02-22 15:03:11 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.4610
2022-02-22 15:03:45 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.3761
2022-02-22 15:04:19 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.4534
2022-02-22 15:04:53 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.7092
2022-02-22 15:05:27 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2283
2022-02-22 15:06:00 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.1568
2022-02-22 15:06:33 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.4410
2022-02-22 15:07:07 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.7381
2022-02-22 15:07:40 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2838
2022-02-22 15:08:14 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.3380
2022-02-22 15:08:48 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.3428
2022-02-22 15:09:22 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.3750
2022-02-22 15:09:55 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.6829
2022-02-22 15:10:28 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3358
2022-02-22 15:11:03 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4182
2022-02-22 15:11:36 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.6657
2022-02-22 15:12:09 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.3406
2022-02-22 15:12:43 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.5490
2022-02-22 15:13:16 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5957
2022-02-22 15:13:50 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.4613
2022-02-22 15:14:23 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.3268
2022-02-22 15:14:57 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.3317
2022-02-22 15:15:30 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.4599
2022-02-22 15:16:04 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.2618
2022-02-22 15:16:39 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.4188
2022-02-22 15:17:12 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.6227
2022-02-22 15:17:46 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.5046
2022-02-22 15:18:20 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.3369
2022-02-22 15:18:54 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.3132
2022-02-22 15:19:27 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.6473
2022-02-22 15:20:01 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4422
2022-02-22 15:20:36 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.6778
2022-02-22 15:21:09 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.2294
2022-02-22 15:21:43 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.4479
2022-02-22 15:22:17 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.4784
2022-02-22 15:22:51 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.5219
2022-02-22 15:23:24 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.3231
2022-02-22 15:23:58 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.6325
2022-02-22 15:24:32 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.4449
2022-02-22 15:25:06 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.4275
2022-02-22 15:25:40 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.5649
2022-02-22 15:25:41 - train: epoch 015, train_loss: 2.4706
2022-02-22 15:26:54 - eval: epoch: 015, acc1: 47.668%, acc5: 74.130%, test_loss: 2.2810, per_image_load_time: 2.077ms, per_image_inference_time: 0.274ms
2022-02-22 15:26:54 - until epoch: 015, best_acc1: 47.680%
2022-02-22 15:26:54 - epoch 016 lr: 0.1
2022-02-22 15:27:33 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.4358
2022-02-22 15:28:07 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.3358
2022-02-22 15:28:40 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.4962
2022-02-22 15:29:15 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.6526
2022-02-22 15:29:48 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.2646
2022-02-22 15:30:21 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.5411
2022-02-22 15:30:55 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.2653
2022-02-22 15:31:30 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.3854
2022-02-22 15:32:03 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.5861
2022-02-22 15:32:37 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.3265
2022-02-22 15:33:10 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.4422
2022-02-22 15:33:44 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2840
2022-02-22 15:34:17 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.5879
2022-02-22 15:34:51 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.3542
2022-02-22 15:35:26 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.5628
2022-02-22 15:35:59 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.5179
2022-02-22 15:36:33 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.3785
2022-02-22 15:37:07 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.3134
2022-02-22 15:37:41 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.3963
2022-02-22 15:38:15 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.1802
2022-02-22 15:38:48 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.4437
2022-02-22 15:39:21 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.4743
2022-02-22 15:39:55 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6063
2022-02-22 15:40:29 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.5928
2022-02-22 15:41:03 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.3815
2022-02-22 15:41:36 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.6762
2022-02-22 15:42:09 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.5361
2022-02-22 15:42:43 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.2670
2022-02-22 15:43:16 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.6182
2022-02-22 15:43:49 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.7309
2022-02-22 15:44:22 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.6517
2022-02-22 15:44:55 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.5591
2022-02-22 15:45:28 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.6004
2022-02-22 15:46:01 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.4259
2022-02-22 15:46:34 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3727
2022-02-22 15:47:07 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.2918
2022-02-22 15:47:40 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.6418
2022-02-22 15:48:14 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.8415
2022-02-22 15:48:46 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.5191
2022-02-22 15:49:20 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.5935
2022-02-22 15:49:53 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.3919
2022-02-22 15:50:26 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.5298
2022-02-22 15:51:00 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.3083
2022-02-22 15:51:33 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.3419
2022-02-22 15:52:06 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.5137
2022-02-22 15:52:39 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.4721
2022-02-22 15:53:12 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.7224
2022-02-22 15:53:45 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.3620
2022-02-22 15:54:20 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5323
2022-02-22 15:54:51 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5689
2022-02-22 15:54:52 - train: epoch 016, train_loss: 2.4594
2022-02-22 15:56:06 - eval: epoch: 016, acc1: 48.502%, acc5: 75.078%, test_loss: 2.2238, per_image_load_time: 1.962ms, per_image_inference_time: 0.269ms
2022-02-22 15:56:06 - until epoch: 016, best_acc1: 48.502%
2022-02-22 15:56:06 - epoch 017 lr: 0.1
2022-02-22 15:56:44 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.3646
2022-02-22 15:57:18 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.5520
2022-02-22 15:57:51 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.6457
2022-02-22 15:58:24 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.1960
2022-02-22 15:58:57 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4151
2022-02-22 15:59:30 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.7394
2022-02-22 16:00:04 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.4310
2022-02-22 16:00:37 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.3742
2022-02-22 16:01:10 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.3164
2022-02-22 16:01:43 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.3514
2022-02-22 16:02:17 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.8412
2022-02-22 16:02:49 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.6164
2022-02-22 16:03:23 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.5390
2022-02-22 16:03:57 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4481
2022-02-22 16:04:28 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.1806
2022-02-22 16:05:02 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.2643
2022-02-22 16:05:35 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.4075
2022-02-22 16:06:07 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.4203
2022-02-22 16:06:41 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2867
2022-02-22 16:07:14 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.6550
2022-02-22 16:07:47 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.4189
2022-02-22 16:08:20 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.3192
2022-02-22 16:08:53 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.4059
2022-02-22 16:09:27 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.3816
2022-02-22 16:10:00 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.5775
2022-02-22 16:10:32 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.4116
2022-02-22 16:11:05 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3639
2022-02-22 16:11:38 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.6025
2022-02-22 16:12:11 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.6747
2022-02-22 16:12:44 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.2591
2022-02-22 16:13:18 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.7014
2022-02-22 16:13:50 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.3625
2022-02-22 16:14:23 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.5563
2022-02-22 16:14:57 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.3494
2022-02-22 16:15:29 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.4853
2022-02-22 16:16:02 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.6952
2022-02-22 16:16:35 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.4358
2022-02-22 16:17:08 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.6159
2022-02-22 16:17:41 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.3406
2022-02-22 16:18:13 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.2805
2022-02-22 16:18:46 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.4629
2022-02-22 16:19:19 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.5176
2022-02-22 16:19:52 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.4917
2022-02-22 16:20:25 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.4365
2022-02-22 16:20:58 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.4968
2022-02-22 16:21:32 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.4507
2022-02-22 16:22:05 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.5511
2022-02-22 16:22:39 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.5118
2022-02-22 16:23:11 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.1928
2022-02-22 16:23:43 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.3051
2022-02-22 16:23:44 - train: epoch 017, train_loss: 2.4491
2022-02-22 16:24:58 - eval: epoch: 017, acc1: 47.790%, acc5: 74.582%, test_loss: 2.2562, per_image_load_time: 1.968ms, per_image_inference_time: 0.251ms
2022-02-22 16:24:58 - until epoch: 017, best_acc1: 48.502%
2022-02-22 16:24:58 - epoch 018 lr: 0.1
2022-02-22 16:25:36 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.4900
2022-02-22 16:26:09 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.5048
2022-02-22 16:26:42 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5661
2022-02-22 16:27:14 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.6104
2022-02-22 16:27:48 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.3658
2022-02-22 16:28:20 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.5220
2022-02-22 16:28:54 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.2517
2022-02-22 16:29:27 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.3957
2022-02-22 16:30:00 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.5297
2022-02-22 16:30:33 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.2859
2022-02-22 16:31:06 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.6654
2022-02-22 16:31:39 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.4601
2022-02-22 16:32:13 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.7664
2022-02-22 16:32:46 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.4540
2022-02-22 16:33:19 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.6503
2022-02-22 16:33:52 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.4180
2022-02-22 16:34:26 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.2872
2022-02-22 16:34:59 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.1203
2022-02-22 16:35:33 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.4772
2022-02-22 16:36:06 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.7456
2022-02-22 16:36:39 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.6094
2022-02-22 16:37:13 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.3936
2022-02-22 16:37:46 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.4085
2022-02-22 16:38:20 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.2933
2022-02-22 16:38:53 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.1407
2022-02-22 16:39:26 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.4048
2022-02-22 16:40:00 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.5926
2022-02-22 16:40:33 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.1573
2022-02-22 16:41:05 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.4443
2022-02-22 16:41:38 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.3691
2022-02-22 16:42:11 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.8564
2022-02-22 16:42:44 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.3307
2022-02-22 16:43:17 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.4116
2022-02-22 16:43:50 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.4174
2022-02-22 16:44:23 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5939
2022-02-22 16:44:55 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.4277
2022-02-22 16:45:28 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.7677
2022-02-22 16:46:02 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.5229
2022-02-22 16:46:35 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.5742
2022-02-22 16:47:08 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.3822
2022-02-22 16:47:40 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.3025
2022-02-22 16:48:14 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.4625
2022-02-22 16:48:47 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1562
2022-02-22 16:49:19 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.7031
2022-02-22 16:49:53 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.5402
2022-02-22 16:50:27 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.2963
2022-02-22 16:51:00 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.7100
2022-02-22 16:51:34 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.5922
2022-02-22 16:52:07 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.4611
2022-02-22 16:52:39 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.5412
2022-02-22 16:52:40 - train: epoch 018, train_loss: 2.4399
2022-02-22 16:53:54 - eval: epoch: 018, acc1: 48.846%, acc5: 74.858%, test_loss: 2.2220, per_image_load_time: 1.088ms, per_image_inference_time: 0.253ms
2022-02-22 16:53:54 - until epoch: 018, best_acc1: 48.846%
2022-02-22 16:53:54 - epoch 019 lr: 0.1
2022-02-22 16:54:34 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.2411
2022-02-22 16:55:06 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.6080
2022-02-22 16:55:39 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.6605
2022-02-22 16:56:12 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.3417
2022-02-22 16:56:45 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3885
2022-02-22 16:57:17 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.3746
2022-02-22 16:57:50 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.2088
2022-02-22 16:58:24 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.5886
2022-02-22 16:58:56 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.3825
2022-02-22 16:59:29 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.6689
2022-02-22 17:00:02 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.3054
2022-02-22 17:00:36 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.3791
2022-02-22 17:01:09 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.5231
2022-02-22 17:01:42 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.2926
2022-02-22 17:02:16 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.7979
2022-02-22 17:02:48 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.3076
2022-02-22 17:03:21 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.5250
2022-02-22 17:03:54 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.3119
2022-02-22 17:04:27 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.6515
2022-02-22 17:05:01 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.4381
2022-02-22 17:05:34 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.3086
2022-02-22 17:06:07 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.4888
2022-02-22 17:06:40 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.3389
2022-02-22 17:07:13 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.5392
2022-02-22 17:07:46 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.4199
2022-02-22 17:08:19 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.4117
2022-02-22 17:08:53 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.5762
2022-02-22 17:09:25 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4380
2022-02-22 17:09:59 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.4152
2022-02-22 17:10:31 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.6717
2022-02-22 17:11:05 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.4891
2022-02-22 17:11:37 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.2825
2022-02-22 17:12:11 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.4907
2022-02-22 17:12:44 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4193
2022-02-22 17:13:18 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.5051
2022-02-22 17:13:50 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.2161
2022-02-22 17:14:24 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.5042
2022-02-22 17:14:57 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.6346
2022-02-22 17:15:30 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.4726
2022-02-22 17:16:02 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2326
2022-02-22 17:16:36 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.5374
2022-02-22 17:17:09 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.4547
2022-02-22 17:17:42 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.3496
2022-02-22 17:18:15 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.5718
2022-02-22 17:18:47 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.7172
2022-02-22 17:19:21 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.3691
2022-02-22 17:19:54 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.3668
2022-02-22 17:20:27 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.4656
2022-02-22 17:21:00 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3932
2022-02-22 17:21:33 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.4421
2022-02-22 17:21:34 - train: epoch 019, train_loss: 2.4353
2022-02-22 17:22:48 - eval: epoch: 019, acc1: 48.506%, acc5: 74.706%, test_loss: 2.2343, per_image_load_time: 2.610ms, per_image_inference_time: 0.257ms
2022-02-22 17:22:48 - until epoch: 019, best_acc1: 48.846%
2022-02-22 17:22:48 - epoch 020 lr: 0.1
2022-02-22 17:23:27 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.4806
2022-02-22 17:24:00 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.2281
2022-02-22 17:24:33 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.5243
2022-02-22 17:25:07 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.2107
2022-02-22 17:25:39 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3097
2022-02-22 17:26:12 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.6072
2022-02-22 17:26:45 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.2187
2022-02-22 17:27:19 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.4055
2022-02-22 17:27:51 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.6546
2022-02-22 17:28:25 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.5118
2022-02-22 17:28:58 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.3720
2022-02-22 17:29:31 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.2190
2022-02-22 17:30:05 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.3688
2022-02-22 17:30:38 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.5920
2022-02-22 17:31:11 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.4463
2022-02-22 17:31:44 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.2931
2022-02-22 17:32:18 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.1423
2022-02-22 17:32:51 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.3333
2022-02-22 17:33:24 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.2846
2022-02-22 17:33:57 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.4090
2022-02-22 17:34:30 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.5563
2022-02-22 17:35:03 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.1841
2022-02-22 17:35:37 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.2664
2022-02-22 17:36:09 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.5317
2022-02-22 17:36:43 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.3392
2022-02-22 17:37:16 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.1371
2022-02-22 17:37:49 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4382
2022-02-22 17:38:22 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.5721
2022-02-22 17:38:55 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5549
2022-02-22 17:39:28 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.2784
2022-02-22 17:40:01 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.4760
2022-02-22 17:40:33 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.6601
2022-02-22 17:41:06 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.3545
2022-02-22 17:41:40 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.4380
2022-02-22 17:42:13 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.2883
2022-02-22 17:42:47 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.4388
2022-02-22 17:43:19 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.3751
2022-02-22 17:43:53 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.4333
2022-02-22 17:44:26 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.4837
2022-02-22 17:44:59 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.3035
2022-02-22 17:45:31 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2989
2022-02-22 17:46:05 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.3174
2022-02-22 17:46:37 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.4590
2022-02-22 17:47:11 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.4617
2022-02-22 17:47:44 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.4093
2022-02-22 17:48:18 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.5401
2022-02-22 17:48:50 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.2200
2022-02-22 17:49:24 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.5107
2022-02-22 17:49:57 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.4978
2022-02-22 17:50:30 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.2928
2022-02-22 17:50:31 - train: epoch 020, train_loss: 2.4222
2022-02-22 17:51:45 - eval: epoch: 020, acc1: 49.106%, acc5: 75.490%, test_loss: 2.1973, per_image_load_time: 2.517ms, per_image_inference_time: 0.236ms
2022-02-22 17:51:45 - until epoch: 020, best_acc1: 49.106%
2022-02-22 17:51:45 - epoch 021 lr: 0.1
2022-02-22 17:52:24 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.3274
2022-02-22 17:52:57 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.4886
2022-02-22 17:53:31 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.0454
2022-02-22 17:54:03 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.5104
2022-02-22 17:54:36 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.2971
2022-02-22 17:55:09 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.3017
2022-02-22 17:55:42 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.2190
2022-02-22 17:56:15 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.5089
2022-02-22 17:56:48 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.4025
2022-02-22 17:57:21 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.3296
2022-02-22 17:57:54 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.3427
2022-02-22 17:58:28 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.2888
2022-02-22 17:59:01 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.3822
2022-02-22 17:59:34 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.2997
2022-02-22 18:00:06 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.3071
2022-02-22 18:00:40 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.4302
2022-02-22 18:01:13 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.4119
2022-02-22 18:01:46 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.2556
2022-02-22 18:02:20 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.5597
2022-02-22 18:02:52 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.5998
2022-02-22 18:03:26 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.2913
2022-02-22 18:03:59 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.5321
2022-02-22 18:04:33 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.2924
2022-02-22 18:05:05 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2816
2022-02-22 18:05:38 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.4128
2022-02-22 18:06:12 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.5779
2022-02-22 18:06:45 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.3172
2022-02-22 18:07:19 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.3452
2022-02-22 18:07:51 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.4183
2022-02-22 18:08:24 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.5948
2022-02-22 18:08:57 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.5229
2022-02-22 18:09:31 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.4266
2022-02-22 18:10:03 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.6750
2022-02-22 18:10:37 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.6185
2022-02-22 18:11:11 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.3983
2022-02-22 18:11:45 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.3361
2022-02-22 18:12:17 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.5752
2022-02-22 18:12:51 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.3450
2022-02-22 18:13:25 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.3558
2022-02-22 18:13:58 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.6429
2022-02-22 18:14:31 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.2280
2022-02-22 18:15:04 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.3167
2022-02-22 18:15:37 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.3436
2022-02-22 18:16:09 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.4688
2022-02-22 18:16:43 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.6913
2022-02-22 18:17:16 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.3783
2022-02-22 18:17:49 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.6885
2022-02-22 18:18:23 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.4446
2022-02-22 18:18:56 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.2717
2022-02-22 18:19:29 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.2953
2022-02-22 18:19:30 - train: epoch 021, train_loss: 2.4171
2022-02-22 18:20:43 - eval: epoch: 021, acc1: 49.240%, acc5: 75.522%, test_loss: 2.1864, per_image_load_time: 2.531ms, per_image_inference_time: 0.250ms
2022-02-22 18:20:44 - until epoch: 021, best_acc1: 49.240%
2022-02-22 18:20:44 - epoch 022 lr: 0.1
2022-02-22 18:21:22 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1685
2022-02-22 18:21:56 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.3110
2022-02-22 18:22:28 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.1291
2022-02-22 18:23:01 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.2840
2022-02-22 18:23:34 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.4570
2022-02-22 18:24:08 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.5102
2022-02-22 18:24:42 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.5210
2022-02-22 18:25:15 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.4736
2022-02-22 18:25:48 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.5672
2022-02-22 18:26:22 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.4577
2022-02-22 18:26:56 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.4215
2022-02-22 18:27:29 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.0630
2022-02-22 18:28:02 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.4749
2022-02-22 18:28:36 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.4679
2022-02-22 18:29:10 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.3110
2022-02-22 18:29:42 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.0823
2022-02-22 18:30:16 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.1144
2022-02-22 18:30:49 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.5709
2022-02-22 18:31:23 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.3414
2022-02-22 18:31:56 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4094
2022-02-22 18:32:30 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.4817
2022-02-22 18:33:02 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.1355
2022-02-22 18:33:36 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.5492
2022-02-22 18:34:08 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.3937
2022-02-22 18:34:42 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.3590
2022-02-22 18:35:15 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.2195
2022-02-22 18:35:48 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.2981
2022-02-22 18:36:21 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.7755
2022-02-22 18:36:55 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.2363
2022-02-22 18:37:28 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.4291
2022-02-22 18:38:01 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.6344
2022-02-22 18:38:34 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.4364
2022-02-22 18:39:07 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3497
2022-02-22 18:39:41 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.2364
2022-02-22 18:40:15 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4841
2022-02-22 18:40:47 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.4683
2022-02-22 18:41:20 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.5090
2022-02-22 18:41:53 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.5351
2022-02-22 18:42:27 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.4101
2022-02-22 18:43:00 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.5554
2022-02-22 18:43:34 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.2826
2022-02-22 18:44:07 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.3815
2022-02-22 18:44:40 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4721
2022-02-22 18:45:13 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.4932
2022-02-22 18:45:47 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.4378
2022-02-22 18:46:20 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.6765
2022-02-22 18:46:53 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.4810
2022-02-22 18:47:26 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.2277
2022-02-22 18:48:01 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.2167
2022-02-22 18:48:32 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.3812
2022-02-22 18:48:34 - train: epoch 022, train_loss: 2.4133
2022-02-22 18:49:49 - eval: epoch: 022, acc1: 45.494%, acc5: 72.216%, test_loss: 2.4023, per_image_load_time: 2.669ms, per_image_inference_time: 0.253ms
2022-02-22 18:49:49 - until epoch: 022, best_acc1: 49.240%
2022-02-22 18:49:49 - epoch 023 lr: 0.1
2022-02-22 18:50:27 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2484
2022-02-22 18:51:00 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.2197
2022-02-22 18:51:33 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.2673
2022-02-22 18:52:06 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.4683
2022-02-22 18:52:40 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.4769
2022-02-22 18:53:13 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.4076
2022-02-22 18:53:47 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.1054
2022-02-22 18:54:19 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.2939
2022-02-22 18:54:54 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.3962
2022-02-22 18:55:27 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.2331
2022-02-22 18:56:00 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.4171
2022-02-22 18:56:34 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.2162
2022-02-22 18:57:08 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.4786
2022-02-22 18:57:41 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.4962
2022-02-22 18:58:14 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.2085
2022-02-22 18:58:48 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3443
2022-02-22 18:59:21 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.4797
2022-02-22 18:59:56 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.3312
2022-02-22 19:00:28 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.4735
2022-02-22 19:01:02 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.0718
2022-02-22 19:01:35 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.5418
2022-02-22 19:02:09 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 2.1426
2022-02-22 19:02:43 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.2888
2022-02-22 19:03:16 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.4236
2022-02-22 19:03:50 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.4815
2022-02-22 19:04:23 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.5161
2022-02-22 19:04:58 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.3938
2022-02-22 19:05:31 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.3737
2022-02-22 19:06:04 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.4484
2022-02-22 19:06:38 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.6307
2022-02-22 19:07:11 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.5527
2022-02-22 19:07:45 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.5081
2022-02-22 19:08:16 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.5180
2022-02-22 19:08:51 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.5899
2022-02-22 19:09:23 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.2939
2022-02-22 19:09:57 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.1362
2022-02-22 19:10:30 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.4783
2022-02-22 19:11:04 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.3874
2022-02-22 19:11:38 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.3938
2022-02-22 19:12:11 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.4255
2022-02-22 19:12:45 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.2730
2022-02-22 19:13:18 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.3399
2022-02-22 19:13:52 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.3022
2022-02-22 19:14:25 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1628
2022-02-22 19:14:58 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.2028
2022-02-22 19:15:32 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.4267
2022-02-22 19:16:06 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.2067
2022-02-22 19:16:39 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.2680
2022-02-22 19:17:13 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.3121
2022-02-22 19:17:45 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.5180
2022-02-22 19:17:46 - train: epoch 023, train_loss: 2.4076
2022-02-22 19:19:00 - eval: epoch: 023, acc1: 46.664%, acc5: 73.304%, test_loss: 2.3252, per_image_load_time: 1.256ms, per_image_inference_time: 0.231ms
2022-02-22 19:19:00 - until epoch: 023, best_acc1: 49.240%
2022-02-22 19:19:00 - epoch 024 lr: 0.1
2022-02-22 19:19:39 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.2897
2022-02-22 19:20:13 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.4139
2022-02-22 19:20:46 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.4068
2022-02-22 19:21:19 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.4883
2022-02-22 19:21:53 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.2706
2022-02-22 19:22:26 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1262
2022-02-22 19:22:59 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.2626
2022-02-22 19:23:33 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.3131
2022-02-22 19:24:07 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.3978
2022-02-22 19:24:40 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.3659
2022-02-22 19:25:13 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.0140
2022-02-22 19:25:46 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.2170
2022-02-22 19:26:20 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.6326
2022-02-22 19:26:52 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.2869
2022-02-22 19:27:26 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.6433
2022-02-22 19:27:59 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.5462
2022-02-22 19:28:33 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.3777
2022-02-22 19:29:06 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.7046
2022-02-22 19:29:40 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.2639
2022-02-22 19:30:13 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.4968
2022-02-22 19:30:47 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.3616
2022-02-22 19:31:20 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.3015
2022-02-22 19:31:53 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.3646
2022-02-22 19:32:26 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.3454
2022-02-22 19:33:00 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.3755
2022-02-22 19:33:33 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.3695
2022-02-22 19:34:07 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.4392
2022-02-22 19:34:39 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.4882
2022-02-22 19:35:13 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.5435
2022-02-22 19:35:45 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.3487
2022-02-22 19:36:19 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.2783
2022-02-22 19:36:52 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.5603
2022-02-22 19:37:26 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.0775
2022-02-22 19:37:59 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.3613
2022-02-22 19:38:32 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.3087
2022-02-22 19:39:05 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.4909
2022-02-22 19:39:39 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.4237
2022-02-22 19:40:12 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.4953
2022-02-22 19:40:46 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.2673
2022-02-22 19:41:19 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.4600
2022-02-22 19:41:52 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.4128
2022-02-22 19:42:26 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.5148
2022-02-22 19:42:58 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3951
2022-02-22 19:43:32 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.3950
2022-02-22 19:44:05 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.1346
2022-02-22 19:44:39 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4948
2022-02-22 19:45:12 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.3998
2022-02-22 19:45:48 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.2639
2022-02-22 19:46:22 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3783
2022-02-22 19:46:54 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.5459
2022-02-22 19:46:55 - train: epoch 024, train_loss: 2.4030
2022-02-22 19:48:10 - eval: epoch: 024, acc1: 50.408%, acc5: 76.558%, test_loss: 2.1322, per_image_load_time: 2.606ms, per_image_inference_time: 0.267ms
2022-02-22 19:48:10 - until epoch: 024, best_acc1: 50.408%
2022-02-22 19:48:10 - epoch 025 lr: 0.1
2022-02-22 19:48:49 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.2598
2022-02-22 19:49:22 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.1584
2022-02-22 19:49:55 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.2115
2022-02-22 19:50:29 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.3008
2022-02-22 19:51:03 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.2999
2022-02-22 19:51:36 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.3688
2022-02-22 19:52:09 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.4164
2022-02-22 19:52:43 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.3088
2022-02-22 19:53:17 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.2431
2022-02-22 19:53:50 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.2639
2022-02-22 19:54:23 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.4324
2022-02-22 19:54:57 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.4543
2022-02-22 19:55:30 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.3054
2022-02-22 19:56:03 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4671
2022-02-22 19:56:37 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.3375
2022-02-22 19:57:11 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.1256
2022-02-22 19:57:44 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3442
2022-02-22 19:58:17 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.3052
2022-02-22 19:58:50 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.2176
2022-02-22 19:59:24 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.3448
2022-02-22 19:59:58 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.2019
2022-02-22 20:00:31 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.2507
2022-02-22 20:01:04 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.3653
2022-02-22 20:01:37 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.1512
2022-02-22 20:02:11 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.4311
2022-02-22 20:02:44 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.5795
2022-02-22 20:03:18 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.4048
2022-02-22 20:03:51 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.3284
2022-02-22 20:04:24 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.6075
2022-02-22 20:04:57 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.6416
2022-02-22 20:05:31 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.3586
2022-02-22 20:06:04 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.6216
2022-02-22 20:06:38 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.3800
2022-02-22 20:07:11 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.6040
2022-02-22 20:07:45 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.1264
2022-02-22 20:08:18 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.5709
2022-02-22 20:08:52 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3968
2022-02-22 20:09:25 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.4089
2022-02-22 20:09:58 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.5201
2022-02-22 20:10:32 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4812
2022-02-22 20:11:06 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.5760
2022-02-22 20:11:40 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.4816
2022-02-22 20:12:13 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.2651
2022-02-22 20:12:47 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.3076
2022-02-22 20:13:20 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.3333
2022-02-22 20:13:54 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.3877
2022-02-22 20:14:27 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.4097
2022-02-22 20:15:01 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.2494
2022-02-22 20:15:34 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.2971
2022-02-22 20:16:07 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.6584
2022-02-22 20:16:08 - train: epoch 025, train_loss: 2.3983
2022-02-22 20:17:23 - eval: epoch: 025, acc1: 47.446%, acc5: 73.832%, test_loss: 2.3029, per_image_load_time: 2.619ms, per_image_inference_time: 0.273ms
2022-02-22 20:17:23 - until epoch: 025, best_acc1: 50.408%
2022-02-22 20:17:23 - epoch 026 lr: 0.1
2022-02-22 20:18:02 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1859
2022-02-22 20:18:35 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.1741
2022-02-22 20:19:09 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.3951
2022-02-22 20:19:41 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.4111
2022-02-22 20:20:15 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.4549
2022-02-22 20:20:48 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.5380
2022-02-22 20:21:21 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.3361
2022-02-22 20:21:54 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.1684
2022-02-22 20:22:27 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.6979
2022-02-22 20:23:01 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.2898
2022-02-22 20:23:34 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3707
2022-02-22 20:24:07 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.4731
2022-02-22 20:24:40 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2565
2022-02-22 20:25:13 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.4493
2022-02-22 20:25:47 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3697
2022-02-22 20:26:20 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.4861
2022-02-22 20:26:54 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.3482
2022-02-22 20:27:28 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.4637
2022-02-22 20:28:01 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6860
2022-02-22 20:28:34 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.5869
2022-02-22 20:29:07 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.4860
2022-02-22 20:29:40 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.4355
2022-02-22 20:30:13 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.4035
2022-02-22 20:30:47 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.6395
2022-02-22 20:31:20 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.6880
2022-02-22 20:31:54 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.4495
2022-02-22 20:32:26 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.3545
2022-02-22 20:33:00 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.2504
2022-02-22 20:33:34 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.5561
2022-02-22 20:34:08 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.2075
2022-02-22 20:34:41 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.3400
2022-02-22 20:35:14 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.4036
2022-02-22 20:35:48 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.3239
2022-02-22 20:36:21 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.6424
2022-02-22 20:36:54 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.5246
2022-02-22 20:37:28 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.3022
2022-02-22 20:38:01 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.5560
2022-02-22 20:38:35 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.6190
2022-02-22 20:39:08 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.5127
2022-02-22 20:39:42 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.6467
2022-02-22 20:40:15 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.4110
2022-02-22 20:40:49 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.5400
2022-02-22 20:41:21 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.3751
2022-02-22 20:41:56 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.6215
2022-02-22 20:42:28 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.6923
2022-02-22 20:43:02 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.3468
2022-02-22 20:43:35 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.2345
2022-02-22 20:44:09 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.3262
2022-02-22 20:44:42 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.4656
2022-02-22 20:45:14 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.6188
2022-02-22 20:45:15 - train: epoch 026, train_loss: 2.3931
2022-02-22 20:46:30 - eval: epoch: 026, acc1: 45.374%, acc5: 72.048%, test_loss: 2.3981, per_image_load_time: 2.544ms, per_image_inference_time: 0.276ms
2022-02-22 20:46:30 - until epoch: 026, best_acc1: 50.408%
2022-02-22 20:46:30 - epoch 027 lr: 0.1
2022-02-22 20:47:09 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.5855
2022-02-22 20:47:42 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.3168
2022-02-22 20:48:16 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.5098
2022-02-22 20:48:49 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.4945
2022-02-22 20:49:23 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.4460
2022-02-22 20:49:56 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.6139
2022-02-22 20:50:30 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.6221
2022-02-22 20:51:03 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.4567
2022-02-22 20:51:37 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.3159
2022-02-22 20:52:11 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.4474
2022-02-22 20:52:44 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.3532
2022-02-22 20:53:18 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.7004
2022-02-22 20:53:51 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.4745
2022-02-22 20:54:26 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.6388
2022-02-22 20:54:59 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.4677
2022-02-22 20:55:32 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3606
2022-02-22 20:56:06 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.3472
2022-02-22 20:56:39 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.4331
2022-02-22 20:57:13 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.5571
2022-02-22 20:57:46 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.2558
2022-02-22 20:58:20 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.6696
2022-02-22 20:58:53 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.4714
2022-02-22 20:59:26 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.7299
2022-02-22 21:00:00 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.2916
2022-02-22 21:00:33 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.3463
2022-02-22 21:01:06 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.6209
2022-02-22 21:01:40 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.3869
2022-02-22 21:02:12 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.4319
2022-02-22 21:02:47 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.4082
2022-02-22 21:03:20 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.2372
2022-02-22 21:03:53 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.3018
2022-02-22 21:04:26 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.3071
2022-02-22 21:05:00 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.3589
2022-02-22 21:05:33 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.3474
2022-02-22 21:06:07 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.6091
2022-02-22 21:06:40 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.2379
2022-02-22 21:07:14 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.3413
2022-02-22 21:07:47 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.1189
2022-02-22 21:08:21 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.2008
2022-02-22 21:08:54 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.4564
2022-02-22 21:09:28 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.3698
2022-02-22 21:10:01 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.4599
2022-02-22 21:10:34 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.2651
2022-02-22 21:11:08 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.4496
2022-02-22 21:11:40 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.1684
2022-02-22 21:12:14 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.3450
2022-02-22 21:12:48 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.4769
2022-02-22 21:13:21 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.6332
2022-02-22 21:13:56 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.5453
2022-02-22 21:14:28 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.0985
2022-02-22 21:14:29 - train: epoch 027, train_loss: 2.3890
2022-02-22 21:15:44 - eval: epoch: 027, acc1: 49.790%, acc5: 75.938%, test_loss: 2.1642, per_image_load_time: 2.451ms, per_image_inference_time: 0.287ms
2022-02-22 21:15:44 - until epoch: 027, best_acc1: 50.408%
2022-02-22 21:15:44 - epoch 028 lr: 0.1
2022-02-22 21:16:22 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.2017
2022-02-22 21:16:56 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2369
2022-02-22 21:17:29 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.4217
2022-02-22 21:18:03 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.1816
2022-02-22 21:18:36 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.3569
2022-02-22 21:19:10 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.4696
2022-02-22 21:19:43 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.5802
2022-02-22 21:20:17 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.0701
2022-02-22 21:20:50 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.1683
2022-02-22 21:21:23 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.5065
2022-02-22 21:21:57 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.3964
2022-02-22 21:22:31 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.1901
2022-02-22 21:23:04 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.3159
2022-02-22 21:23:38 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.8479
2022-02-22 21:24:12 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.3339
2022-02-22 21:24:45 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.3540
2022-02-22 21:25:19 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.2993
2022-02-22 21:25:52 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.3076
2022-02-22 21:26:25 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.3667
2022-02-22 21:26:59 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.4851
2022-02-22 21:27:32 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.4103
2022-02-22 21:28:06 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.4830
2022-02-22 21:28:38 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.5034
2022-02-22 21:29:12 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.6342
2022-02-22 21:29:46 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.3523
2022-02-22 21:30:19 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.1964
2022-02-22 21:30:53 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.3246
2022-02-22 21:31:26 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.4512
2022-02-22 21:32:01 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.4171
2022-02-22 21:32:34 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.4749
2022-02-22 21:33:08 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.6545
2022-02-22 21:33:42 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.3189
2022-02-22 21:34:16 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.4831
2022-02-22 21:34:49 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.3554
2022-02-22 21:35:23 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.3020
2022-02-22 21:35:56 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.2578
2022-02-22 21:36:30 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.3532
2022-02-22 21:37:03 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.1117
2022-02-22 21:37:37 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.3807
2022-02-22 21:38:10 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.5179
2022-02-22 21:38:44 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.4951
2022-02-22 21:39:18 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.4785
2022-02-22 21:39:51 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.3381
2022-02-22 21:40:24 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.3541
2022-02-22 21:40:59 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.3645
2022-02-22 21:41:32 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.5038
2022-02-22 21:42:05 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.4101
2022-02-22 21:42:39 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.2887
2022-02-22 21:43:13 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.2598
2022-02-22 21:43:46 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.3358
2022-02-22 21:43:47 - train: epoch 028, train_loss: 2.3831
2022-02-22 21:45:02 - eval: epoch: 028, acc1: 50.766%, acc5: 76.574%, test_loss: 2.1285, per_image_load_time: 1.612ms, per_image_inference_time: 0.289ms
2022-02-22 21:45:02 - until epoch: 028, best_acc1: 50.766%
2022-02-22 21:45:02 - epoch 029 lr: 0.1
2022-02-22 21:45:42 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.4618
2022-02-22 21:46:14 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4172
2022-02-22 21:46:48 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.4261
2022-02-22 21:47:21 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.2914
2022-02-22 21:47:54 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3343
2022-02-22 21:48:27 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.7329
2022-02-22 21:49:00 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9491
2022-02-22 21:49:34 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.5016
2022-02-22 21:50:08 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.3154
2022-02-22 21:50:41 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.2034
2022-02-22 21:51:15 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.2890
2022-02-22 21:51:47 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.4556
2022-02-22 21:52:21 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.4003
2022-02-22 21:52:54 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.6356
2022-02-22 21:53:27 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.4895
2022-02-22 21:54:00 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.3278
2022-02-22 21:54:34 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.4797
2022-02-22 21:55:07 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.4755
2022-02-22 21:55:40 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.2276
2022-02-22 21:56:13 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.6197
2022-02-22 21:56:47 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.2604
2022-02-22 21:57:20 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.3808
2022-02-22 21:57:54 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.4009
2022-02-22 21:58:27 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.3127
2022-02-22 21:59:01 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2398
2022-02-22 21:59:34 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.4646
2022-02-22 22:00:07 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.2709
2022-02-22 22:00:40 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.2122
2022-02-22 22:01:13 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.3809
2022-02-22 22:01:46 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.2572
2022-02-22 22:02:19 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.2849
2022-02-22 22:02:52 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.3122
2022-02-22 22:03:25 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.2613
2022-02-22 22:03:59 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.1682
2022-02-22 22:04:33 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.4900
2022-02-22 22:05:06 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.3383
2022-02-22 22:05:40 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.2903
2022-02-22 22:06:12 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.3436
2022-02-22 22:06:47 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.2068
2022-02-22 22:07:19 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.2693
2022-02-22 22:07:53 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.3746
2022-02-22 22:08:26 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.3395
2022-02-22 22:09:01 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.5366
2022-02-22 22:09:33 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.3359
2022-02-22 22:10:07 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.4695
2022-02-22 22:10:40 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.5583
2022-02-22 22:11:14 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.2301
2022-02-22 22:11:48 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.5069
2022-02-22 22:12:21 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.6805
2022-02-22 22:12:53 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.0737
2022-02-22 22:12:55 - train: epoch 029, train_loss: 2.3814
2022-02-22 22:14:09 - eval: epoch: 029, acc1: 49.720%, acc5: 75.846%, test_loss: 2.1682, per_image_load_time: 1.070ms, per_image_inference_time: 0.271ms
2022-02-22 22:14:09 - until epoch: 029, best_acc1: 50.766%
2022-02-22 22:14:09 - epoch 030 lr: 0.1
2022-02-22 22:14:49 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.5462
2022-02-22 22:15:22 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.5931
2022-02-22 22:15:56 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.2998
2022-02-22 22:16:29 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.1582
2022-02-22 22:17:03 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.5952
2022-02-22 22:17:37 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.2933
2022-02-22 22:18:10 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.3187
2022-02-22 22:18:44 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.4768
2022-02-22 22:19:17 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.4430
2022-02-22 22:19:51 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.1297
2022-02-22 22:20:24 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.2653
2022-02-22 22:20:57 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.4039
2022-02-22 22:21:31 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.1646
2022-02-22 22:22:04 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.1972
2022-02-22 22:22:38 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.4410
2022-02-22 22:23:12 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.3885
2022-02-22 22:23:45 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.4985
2022-02-22 22:24:19 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.3668
2022-02-22 22:24:53 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.4613
2022-02-22 22:25:26 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.3019
2022-02-22 22:25:59 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.5782
2022-02-22 22:26:32 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.3081
2022-02-22 22:27:06 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.4997
2022-02-22 22:27:40 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.6471
2022-02-22 22:28:13 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.4893
2022-02-22 22:28:47 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.3540
2022-02-22 22:29:20 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.3804
2022-02-22 22:29:54 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3543
2022-02-22 22:30:27 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.3651
2022-02-22 22:31:01 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.6503
2022-02-22 22:31:34 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.3836
2022-02-22 22:32:08 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.1591
2022-02-22 22:32:42 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.6417
2022-02-22 22:33:15 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.3566
2022-02-22 22:33:50 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.4908
2022-02-22 22:34:22 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.2343
2022-02-22 22:34:56 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.3119
2022-02-22 22:35:30 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.5244
2022-02-22 22:36:03 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3884
2022-02-22 22:36:36 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.2133
2022-02-22 22:37:09 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.3204
2022-02-22 22:37:43 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5969
2022-02-22 22:38:17 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.2602
2022-02-22 22:38:50 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.4007
2022-02-22 22:39:24 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.6015
2022-02-22 22:39:57 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.0434
2022-02-22 22:40:30 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.3900
2022-02-22 22:41:05 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.3736
2022-02-22 22:41:39 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.5801
2022-02-22 22:42:11 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.4712
2022-02-22 22:42:13 - train: epoch 030, train_loss: 2.3825
2022-02-22 22:43:28 - eval: epoch: 030, acc1: 47.492%, acc5: 73.942%, test_loss: 2.2860, per_image_load_time: 2.630ms, per_image_inference_time: 0.279ms
2022-02-22 22:43:28 - until epoch: 030, best_acc1: 50.766%
2022-02-22 22:43:28 - epoch 031 lr: 0.010000000000000002
2022-02-22 22:44:07 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.2186
2022-02-22 22:44:40 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.9648
2022-02-22 22:45:14 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.9761
2022-02-22 22:45:47 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.0516
2022-02-22 22:46:20 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.9140
2022-02-22 22:46:53 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.8885
2022-02-22 22:47:27 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.8195
2022-02-22 22:48:00 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.7837
2022-02-22 22:48:34 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.8607
2022-02-22 22:49:07 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.1386
2022-02-22 22:49:41 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.1467
2022-02-22 22:50:15 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.9314
2022-02-22 22:50:48 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.6010
2022-02-22 22:51:22 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.8210
2022-02-22 22:51:55 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 2.0840
2022-02-22 22:52:28 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.7339
2022-02-22 22:53:02 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.6728
2022-02-22 22:53:34 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.8586
2022-02-22 22:54:08 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.9538
2022-02-22 22:54:41 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.8903
2022-02-22 22:55:14 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.7180
2022-02-22 22:55:47 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.6571
2022-02-22 22:56:20 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.6632
2022-02-22 22:56:53 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.8031
2022-02-22 22:57:26 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.7576
2022-02-22 22:58:00 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.8713
2022-02-22 22:58:32 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.8192
2022-02-22 22:59:07 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.0091
2022-02-22 22:59:40 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.8220
2022-02-22 23:00:13 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.8927
2022-02-22 23:00:46 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.7162
2022-02-22 23:01:20 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.8981
2022-02-22 23:01:53 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8545
2022-02-22 23:02:27 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.9574
2022-02-22 23:03:00 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.8938
2022-02-22 23:03:33 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7936
2022-02-22 23:04:07 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.7579
2022-02-22 23:04:41 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.6424
2022-02-22 23:05:14 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.7831
2022-02-22 23:05:47 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.6849
2022-02-22 23:06:21 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.7719
2022-02-22 23:06:55 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.7989
2022-02-22 23:07:29 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.7219
2022-02-22 23:08:02 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.9365
2022-02-22 23:08:36 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.9175
2022-02-22 23:09:10 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7771
2022-02-22 23:09:44 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.8328
2022-02-22 23:10:17 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.6930
2022-02-22 23:10:51 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.6550
2022-02-22 23:11:24 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.7311
2022-02-22 23:11:25 - train: epoch 031, train_loss: 1.8521
2022-02-22 23:12:40 - eval: epoch: 031, acc1: 64.368%, acc5: 86.272%, test_loss: 1.4648, per_image_load_time: 1.734ms, per_image_inference_time: 0.267ms
2022-02-22 23:12:41 - until epoch: 031, best_acc1: 64.368%
2022-02-22 23:12:41 - epoch 032 lr: 0.010000000000000002
2022-02-22 23:13:19 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.6741
2022-02-22 23:13:53 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.8193
2022-02-22 23:14:26 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.7658
2022-02-22 23:15:00 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.8722
2022-02-22 23:15:33 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.7958
2022-02-22 23:16:06 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.9039
2022-02-22 23:16:40 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.7639
2022-02-22 23:17:13 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.8228
2022-02-22 23:17:47 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.7410
2022-02-22 23:18:21 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.8613
2022-02-22 23:18:54 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.8979
2022-02-22 23:19:27 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.8044
2022-02-22 23:20:01 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.7550
2022-02-22 23:20:35 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.8769
2022-02-22 23:21:09 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.8785
2022-02-22 23:21:43 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.8729
2022-02-22 23:22:16 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.9278
2022-02-22 23:22:50 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.0636
2022-02-22 23:23:23 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.6593
2022-02-22 23:23:56 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.7242
2022-02-22 23:24:29 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.6523
2022-02-22 23:25:02 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.6337
2022-02-22 23:25:36 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.7331
2022-02-22 23:26:10 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.6510
2022-02-22 23:26:43 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.8379
2022-02-22 23:27:17 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.5009
2022-02-22 23:27:50 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.7258
2022-02-22 23:28:23 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.7670
2022-02-22 23:28:56 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.5217
2022-02-22 23:29:31 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.5400
2022-02-22 23:30:03 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.8546
2022-02-22 23:30:37 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.6712
2022-02-22 23:31:10 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.5854
2022-02-22 23:31:44 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.5246
2022-02-22 23:32:18 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.7889
2022-02-22 23:32:52 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.7616
2022-02-22 23:33:25 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.8190
2022-02-22 23:33:59 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.5280
2022-02-22 23:34:32 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.7718
2022-02-22 23:35:05 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.6559
2022-02-22 23:35:39 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.7024
2022-02-22 23:36:12 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.5850
2022-02-22 23:36:45 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.9371
2022-02-22 23:37:18 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.6868
2022-02-22 23:37:52 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8847
2022-02-22 23:38:25 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.7151
2022-02-22 23:38:59 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.8370
2022-02-22 23:39:33 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.6334
2022-02-22 23:40:07 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.8621
2022-02-22 23:40:39 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.7803
2022-02-22 23:40:41 - train: epoch 032, train_loss: 1.7408
2022-02-22 23:41:54 - eval: epoch: 032, acc1: 65.106%, acc5: 86.640%, test_loss: 1.4234, per_image_load_time: 2.397ms, per_image_inference_time: 0.287ms
2022-02-22 23:41:55 - until epoch: 032, best_acc1: 65.106%
2022-02-22 23:41:55 - epoch 033 lr: 0.010000000000000002
2022-02-22 23:42:33 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.5799
2022-02-22 23:43:06 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.8672
2022-02-22 23:43:40 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.5844
2022-02-22 23:44:12 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4907
2022-02-22 23:44:46 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7810
2022-02-22 23:45:19 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.5327
2022-02-22 23:45:53 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.8318
2022-02-22 23:46:26 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.8278
2022-02-22 23:46:59 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.7129
2022-02-22 23:47:32 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.7714
2022-02-22 23:48:06 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.5959
2022-02-22 23:48:39 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.6714
2022-02-22 23:49:12 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.6239
2022-02-22 23:49:46 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.8652
2022-02-22 23:50:19 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.9578
2022-02-22 23:50:52 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.9020
2022-02-22 23:51:25 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.5373
2022-02-22 23:51:59 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.9093
2022-02-22 23:52:32 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.7806
2022-02-22 23:53:05 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.5904
2022-02-22 23:53:39 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.7257
2022-02-22 23:54:13 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.7882
2022-02-22 23:54:45 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.5204
2022-02-22 23:55:19 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.9776
2022-02-22 23:55:52 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.6617
2022-02-22 23:56:26 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.5527
2022-02-22 23:56:59 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.9806
2022-02-22 23:57:33 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.6980
2022-02-22 23:58:06 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.7963
2022-02-22 23:58:39 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.7422
2022-02-22 23:59:12 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.6984
2022-02-22 23:59:46 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.6352
2022-02-23 00:00:19 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.7391
2022-02-23 00:00:52 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.5926
2022-02-23 00:01:25 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.7599
2022-02-23 00:01:59 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.9339
2022-02-23 00:02:31 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.7081
2022-02-23 00:03:05 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.5692
2022-02-23 00:03:38 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.7915
2022-02-23 00:04:11 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.8646
2022-02-23 00:04:44 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.7165
2022-02-23 00:05:17 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.6465
2022-02-23 00:05:51 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.8717
2022-02-23 00:06:24 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.6713
2022-02-23 00:06:57 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.8932
2022-02-23 00:07:31 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.6271
2022-02-23 00:08:05 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.6159
2022-02-23 00:08:39 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.0353
2022-02-23 00:09:13 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.5259
2022-02-23 00:09:46 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.5823
2022-02-23 00:09:47 - train: epoch 033, train_loss: 1.6976
2022-02-23 00:11:02 - eval: epoch: 033, acc1: 65.106%, acc5: 86.668%, test_loss: 1.4213, per_image_load_time: 2.291ms, per_image_inference_time: 0.285ms
2022-02-23 00:11:02 - until epoch: 033, best_acc1: 65.106%
2022-02-23 00:11:02 - epoch 034 lr: 0.010000000000000002
2022-02-23 00:11:41 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.7704
2022-02-23 00:12:13 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.6435
2022-02-23 00:12:46 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.6217
2022-02-23 00:13:19 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.4897
2022-02-23 00:13:51 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.6708
2022-02-23 00:14:24 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.9948
2022-02-23 00:14:58 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.6383
2022-02-23 00:15:31 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.5851
2022-02-23 00:16:05 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.6305
2022-02-23 00:16:38 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.5587
2022-02-23 00:17:11 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.7090
2022-02-23 00:17:45 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.7051
2022-02-23 00:18:17 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.6197
2022-02-23 00:18:50 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.6402
2022-02-23 00:19:23 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.5972
2022-02-23 00:19:56 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.5533
2022-02-23 00:20:29 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.6641
2022-02-23 00:21:03 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.8374
2022-02-23 00:21:36 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.7900
2022-02-23 00:22:10 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.7727
2022-02-23 00:22:42 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.8464
2022-02-23 00:23:17 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.5884
2022-02-23 00:23:49 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.7556
2022-02-23 00:24:23 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.5987
2022-02-23 00:24:56 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.6972
2022-02-23 00:25:29 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.6947
2022-02-23 00:26:02 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.6758
2022-02-23 00:26:35 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.5359
2022-02-23 00:27:08 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.3975
2022-02-23 00:27:41 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.5082
2022-02-23 00:28:14 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.6696
2022-02-23 00:28:47 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.6368
2022-02-23 00:29:20 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.6177
2022-02-23 00:29:54 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.7135
2022-02-23 00:30:27 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.5716
2022-02-23 00:31:01 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.5336
2022-02-23 00:31:34 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.5783
2022-02-23 00:32:07 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.6313
2022-02-23 00:32:40 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.7651
2022-02-23 00:33:14 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.4988
2022-02-23 00:33:47 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.6626
2022-02-23 00:34:20 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.5991
2022-02-23 00:34:53 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.6947
2022-02-23 00:35:26 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.7595
2022-02-23 00:36:00 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.7883
2022-02-23 00:36:33 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.7624
2022-02-23 00:37:06 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.7141
2022-02-23 00:37:40 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.6670
2022-02-23 00:38:13 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.7164
2022-02-23 00:38:46 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.4969
2022-02-23 00:38:47 - train: epoch 034, train_loss: 1.6738
2022-02-23 00:40:01 - eval: epoch: 034, acc1: 65.514%, acc5: 87.046%, test_loss: 1.3963, per_image_load_time: 2.435ms, per_image_inference_time: 0.259ms
2022-02-23 00:40:01 - until epoch: 034, best_acc1: 65.514%
2022-02-23 00:40:01 - epoch 035 lr: 0.010000000000000002
2022-02-23 00:40:40 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.4804
2022-02-23 00:41:12 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.4712
2022-02-23 00:41:47 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.8118
2022-02-23 00:42:19 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.6084
2022-02-23 00:42:53 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.7348
2022-02-23 00:43:25 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.6304
2022-02-23 00:43:59 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.5930
2022-02-23 00:44:32 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.6859
2022-02-23 00:45:06 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.7249
2022-02-23 00:45:39 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.7891
2022-02-23 00:46:12 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.7485
2022-02-23 00:46:45 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.6932
2022-02-23 00:47:19 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.6939
2022-02-23 00:47:52 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.6340
2022-02-23 00:48:26 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.7214
2022-02-23 00:48:59 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.6259
2022-02-23 00:49:33 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.4998
2022-02-23 00:50:06 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.6741
2022-02-23 00:50:39 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.6759
2022-02-23 00:51:12 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.6891
2022-02-23 00:51:46 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.6016
2022-02-23 00:52:19 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.7663
2022-02-23 00:52:53 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.6673
2022-02-23 00:53:26 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.7775
2022-02-23 00:54:00 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.5529
2022-02-23 00:54:33 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.8811
2022-02-23 00:55:06 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.7166
2022-02-23 00:55:40 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.5965
2022-02-23 00:56:13 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.5944
2022-02-23 00:56:47 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.7359
2022-02-23 00:57:19 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.6360
2022-02-23 00:57:53 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.5481
2022-02-23 00:58:26 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.5429
2022-02-23 00:58:59 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.5963
2022-02-23 00:59:33 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.3720
2022-02-23 01:00:06 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.6666
2022-02-23 01:00:40 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.4167
2022-02-23 01:01:13 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.6045
2022-02-23 01:01:46 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.7489
2022-02-23 01:02:20 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.4224
2022-02-23 01:02:53 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.9963
2022-02-23 01:03:26 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.5817
2022-02-23 01:04:00 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.6087
2022-02-23 01:04:33 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.7580
2022-02-23 01:05:07 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.7830
2022-02-23 01:05:41 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5747
2022-02-23 01:06:14 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.9871
2022-02-23 01:06:47 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.9245
2022-02-23 01:07:22 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.7018
2022-02-23 01:07:54 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.4903
2022-02-23 01:07:55 - train: epoch 035, train_loss: 1.6586
2022-02-23 01:09:10 - eval: epoch: 035, acc1: 65.970%, acc5: 87.134%, test_loss: 1.3853, per_image_load_time: 2.659ms, per_image_inference_time: 0.248ms
2022-02-23 01:09:11 - until epoch: 035, best_acc1: 65.970%
2022-02-23 01:09:11 - epoch 036 lr: 0.010000000000000002
2022-02-23 01:09:50 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.8170
2022-02-23 01:10:23 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.4568
2022-02-23 01:10:56 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.5085
2022-02-23 01:11:29 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.7040
2022-02-23 01:12:03 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.7595
2022-02-23 01:12:36 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.5490
2022-02-23 01:13:09 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.4331
2022-02-23 01:13:43 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.4168
2022-02-23 01:14:16 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.7207
2022-02-23 01:14:49 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.6868
2022-02-23 01:15:22 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.7894
2022-02-23 01:15:55 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.5433
2022-02-23 01:16:28 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.6183
2022-02-23 01:17:01 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8011
2022-02-23 01:17:35 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.8451
2022-02-23 01:18:08 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.6895
2022-02-23 01:18:41 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.6643
2022-02-23 01:19:15 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.5677
2022-02-23 01:19:48 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.6093
2022-02-23 01:20:21 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.5226
2022-02-23 01:20:55 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.6975
2022-02-23 01:21:28 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.5891
2022-02-23 01:22:01 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.6963
2022-02-23 01:22:34 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.7771
2022-02-23 01:23:08 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.4962
2022-02-23 01:23:41 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.6924
2022-02-23 01:24:14 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.5159
2022-02-23 01:24:48 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.5053
2022-02-23 01:25:21 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.5758
2022-02-23 01:25:54 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.8119
2022-02-23 01:26:27 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.6712
2022-02-23 01:27:01 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.6308
2022-02-23 01:27:34 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.4778
2022-02-23 01:28:07 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.5336
2022-02-23 01:28:40 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.6942
2022-02-23 01:29:14 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.7767
2022-02-23 01:29:47 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.6250
2022-02-23 01:30:21 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.6076
2022-02-23 01:30:55 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.6128
2022-02-23 01:31:28 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.6822
2022-02-23 01:32:01 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.5938
2022-02-23 01:32:34 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.6243
2022-02-23 01:33:07 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.6702
2022-02-23 01:33:40 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.6689
2022-02-23 01:34:14 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.3731
2022-02-23 01:34:47 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.5138
2022-02-23 01:35:21 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.6483
2022-02-23 01:35:55 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.6420
2022-02-23 01:36:28 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.6575
2022-02-23 01:37:01 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.6334
2022-02-23 01:37:02 - train: epoch 036, train_loss: 1.6474
2022-02-23 01:38:16 - eval: epoch: 036, acc1: 65.850%, acc5: 87.220%, test_loss: 1.3892, per_image_load_time: 2.515ms, per_image_inference_time: 0.293ms
2022-02-23 01:38:17 - until epoch: 036, best_acc1: 65.970%
2022-02-23 01:38:17 - epoch 037 lr: 0.010000000000000002
2022-02-23 01:38:55 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.4142
2022-02-23 01:39:28 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3635
2022-02-23 01:40:02 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.5622
2022-02-23 01:40:35 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.7229
2022-02-23 01:41:07 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.6028
2022-02-23 01:41:41 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.7398
2022-02-23 01:42:14 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.7152
2022-02-23 01:42:48 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.6342
2022-02-23 01:43:21 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.0240
2022-02-23 01:43:54 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.5374
2022-02-23 01:44:28 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.6922
2022-02-23 01:45:01 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.6019
2022-02-23 01:45:35 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.6851
2022-02-23 01:46:08 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.6865
2022-02-23 01:46:41 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.5096
2022-02-23 01:47:14 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.3951
2022-02-23 01:47:47 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.9289
2022-02-23 01:48:21 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.7105
2022-02-23 01:48:54 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.7044
2022-02-23 01:49:27 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.6375
2022-02-23 01:50:00 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.7145
2022-02-23 01:50:34 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.6123
2022-02-23 01:51:08 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.4728
2022-02-23 01:51:40 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.7031
2022-02-23 01:52:14 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.5912
2022-02-23 01:52:47 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.8612
2022-02-23 01:53:21 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.6977
2022-02-23 01:53:54 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.7480
2022-02-23 01:54:28 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.6679
2022-02-23 01:55:01 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.7584
2022-02-23 01:55:34 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.5441
2022-02-23 01:56:06 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.7229
2022-02-23 01:56:40 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.5479
2022-02-23 01:57:12 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.5179
2022-02-23 01:57:45 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.6245
2022-02-23 01:58:19 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.7348
2022-02-23 01:58:53 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.7710
2022-02-23 01:59:26 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.5808
2022-02-23 01:59:59 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.8113
2022-02-23 02:00:32 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.5457
2022-02-23 02:01:06 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.6131
2022-02-23 02:01:39 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.8601
2022-02-23 02:02:12 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.7330
2022-02-23 02:02:45 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.5943
2022-02-23 02:03:18 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.5570
2022-02-23 02:03:51 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.7233
2022-02-23 02:04:25 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.6659
2022-02-23 02:04:59 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.6383
2022-02-23 02:05:32 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.6519
2022-02-23 02:06:05 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.6252
2022-02-23 02:06:06 - train: epoch 037, train_loss: 1.6436
2022-02-23 02:07:21 - eval: epoch: 037, acc1: 65.332%, acc5: 87.062%, test_loss: 1.4029, per_image_load_time: 2.563ms, per_image_inference_time: 0.288ms
2022-02-23 02:07:21 - until epoch: 037, best_acc1: 65.970%
2022-02-23 02:07:21 - epoch 038 lr: 0.010000000000000002
2022-02-23 02:07:59 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.5766
2022-02-23 02:08:33 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.3597
2022-02-23 02:09:05 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.3457
2022-02-23 02:09:39 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.5192
2022-02-23 02:10:12 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.4929
2022-02-23 02:10:45 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.6932
2022-02-23 02:11:20 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.4773
2022-02-23 02:11:53 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.5995
2022-02-23 02:12:26 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.6025
2022-02-23 02:12:59 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.8464
2022-02-23 02:13:33 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5092
2022-02-23 02:14:06 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.6515
2022-02-23 02:14:40 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.7699
2022-02-23 02:15:12 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.7236
2022-02-23 02:15:46 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.6764
2022-02-23 02:16:19 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.7451
2022-02-23 02:16:52 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.8196
2022-02-23 02:17:27 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.6981
2022-02-23 02:18:00 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.6634
2022-02-23 02:18:33 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.5863
2022-02-23 02:19:06 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.7404
2022-02-23 02:19:40 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.5748
2022-02-23 02:20:13 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.7424
2022-02-23 02:20:46 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.8867
2022-02-23 02:21:20 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.5758
2022-02-23 02:21:52 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.6435
2022-02-23 02:22:25 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.7676
2022-02-23 02:23:00 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.7755
2022-02-23 02:23:34 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.6996
2022-02-23 02:24:06 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.6206
2022-02-23 02:24:40 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.6540
2022-02-23 02:25:13 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.3848
2022-02-23 02:25:47 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.3923
2022-02-23 02:26:20 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.5887
2022-02-23 02:26:54 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.6658
2022-02-23 02:27:28 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.7123
2022-02-23 02:28:00 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.4451
2022-02-23 02:28:34 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.7083
2022-02-23 02:29:07 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.6170
2022-02-23 02:29:40 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.6003
2022-02-23 02:30:14 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.6089
2022-02-23 02:30:47 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.4766
2022-02-23 02:31:21 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.7818
2022-02-23 02:31:55 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.7115
2022-02-23 02:32:28 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.7242
2022-02-23 02:33:01 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.7255
2022-02-23 02:33:36 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.5060
2022-02-23 02:34:09 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.6965
2022-02-23 02:34:43 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.6572
2022-02-23 02:35:15 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.4786
2022-02-23 02:35:16 - train: epoch 038, train_loss: 1.6414
2022-02-23 02:36:31 - eval: epoch: 038, acc1: 65.530%, acc5: 86.878%, test_loss: 1.4072, per_image_load_time: 2.616ms, per_image_inference_time: 0.259ms
2022-02-23 02:36:31 - until epoch: 038, best_acc1: 65.970%
2022-02-23 02:36:31 - epoch 039 lr: 0.010000000000000002
2022-02-23 02:37:09 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.6507
2022-02-23 02:37:43 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.7971
2022-02-23 02:38:16 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.4315
2022-02-23 02:38:49 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.6311
2022-02-23 02:39:23 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.5696
2022-02-23 02:39:56 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.4908
2022-02-23 02:40:30 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.8021
2022-02-23 02:41:03 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.6064
2022-02-23 02:41:36 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.7462
2022-02-23 02:42:11 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.5289
2022-02-23 02:42:44 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.7745
2022-02-23 02:43:17 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.7183
2022-02-23 02:43:51 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.8630
2022-02-23 02:44:25 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.6320
2022-02-23 02:44:58 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.6470
2022-02-23 02:45:31 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.8406
2022-02-23 02:46:05 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.4308
2022-02-23 02:46:38 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.6183
2022-02-23 02:47:12 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.3970
2022-02-23 02:47:45 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.5937
2022-02-23 02:48:19 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.6829
2022-02-23 02:48:52 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.5731
2022-02-23 02:49:26 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.9106
2022-02-23 02:49:59 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.7920
2022-02-23 02:50:32 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.5204
2022-02-23 02:51:05 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.7014
2022-02-23 02:51:39 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.8591
2022-02-23 02:52:13 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.6209
2022-02-23 02:52:47 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.4503
2022-02-23 02:53:20 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.6199
2022-02-23 02:53:53 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.5826
2022-02-23 02:54:27 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.6394
2022-02-23 02:55:00 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.6864
2022-02-23 02:55:33 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.7586
2022-02-23 02:56:07 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.8770
2022-02-23 02:56:40 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.8149
2022-02-23 02:57:13 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.5744
2022-02-23 02:57:46 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.3689
2022-02-23 02:58:20 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.7446
2022-02-23 02:58:54 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.6508
2022-02-23 02:59:27 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.6750
2022-02-23 03:00:01 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.7304
2022-02-23 03:00:35 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.6654
2022-02-23 03:01:08 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.3732
2022-02-23 03:01:41 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.5507
2022-02-23 03:02:15 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.7828
2022-02-23 03:02:48 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.7336
2022-02-23 03:03:22 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.6305
2022-02-23 03:03:56 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.5120
2022-02-23 03:04:28 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.5595
2022-02-23 03:04:30 - train: epoch 039, train_loss: 1.6407
2022-02-23 03:05:44 - eval: epoch: 039, acc1: 65.616%, acc5: 87.112%, test_loss: 1.3974, per_image_load_time: 2.033ms, per_image_inference_time: 0.276ms
2022-02-23 03:05:45 - until epoch: 039, best_acc1: 65.970%
2022-02-23 03:05:45 - epoch 040 lr: 0.010000000000000002
2022-02-23 03:06:23 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.8587
2022-02-23 03:06:56 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.8403
2022-02-23 03:07:30 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.7336
2022-02-23 03:08:03 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.5869
2022-02-23 03:08:37 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.5507
2022-02-23 03:09:09 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.7278
2022-02-23 03:09:43 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.6340
2022-02-23 03:10:16 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.9018
2022-02-23 03:10:50 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.5158
2022-02-23 03:11:23 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.2757
2022-02-23 03:11:56 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.6578
2022-02-23 03:12:30 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.5686
2022-02-23 03:13:03 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.4995
2022-02-23 03:13:36 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.5154
2022-02-23 03:14:08 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.7999
2022-02-23 03:14:42 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.6793
2022-02-23 03:15:15 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.6705
2022-02-23 03:15:48 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.4462
2022-02-23 03:16:22 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.6576
2022-02-23 03:16:55 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.6806
2022-02-23 03:17:29 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.4437
2022-02-23 03:18:01 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.5286
2022-02-23 03:18:35 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.5822
2022-02-23 03:19:08 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.6259
2022-02-23 03:19:41 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.7114
2022-02-23 03:20:15 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.5425
2022-02-23 03:20:47 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.7883
2022-02-23 03:21:20 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.5949
2022-02-23 03:21:54 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.8967
2022-02-23 03:22:27 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.7697
2022-02-23 03:23:00 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.5686
2022-02-23 03:23:34 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.8761
2022-02-23 03:24:07 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.7445
2022-02-23 03:24:41 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.7204
2022-02-23 03:25:14 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.6733
2022-02-23 03:25:47 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.7268
2022-02-23 03:26:20 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.6737
2022-02-23 03:26:53 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.5015
2022-02-23 03:27:27 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.6587
2022-02-23 03:28:00 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.7517
2022-02-23 03:28:33 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.6995
2022-02-23 03:29:06 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.5354
2022-02-23 03:29:39 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.6506
2022-02-23 03:30:12 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.6834
2022-02-23 03:30:45 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.4281
2022-02-23 03:31:18 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.7239
2022-02-23 03:31:53 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.6413
2022-02-23 03:32:25 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.5557
2022-02-23 03:33:00 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.6870
2022-02-23 03:33:32 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.7157
2022-02-23 03:33:34 - train: epoch 040, train_loss: 1.6405
2022-02-23 03:34:47 - eval: epoch: 040, acc1: 65.734%, acc5: 87.266%, test_loss: 1.3902, per_image_load_time: 2.456ms, per_image_inference_time: 0.275ms
2022-02-23 03:34:48 - until epoch: 040, best_acc1: 65.970%
2022-02-23 03:34:48 - epoch 041 lr: 0.010000000000000002
2022-02-23 03:35:26 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.7374
2022-02-23 03:35:59 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.7954
2022-02-23 03:36:32 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.5840
2022-02-23 03:37:05 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.6441
2022-02-23 03:37:39 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.4509
2022-02-23 03:38:12 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.7787
2022-02-23 03:38:45 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.4910
2022-02-23 03:39:18 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3114
2022-02-23 03:39:52 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.3906
2022-02-23 03:40:25 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.8393
2022-02-23 03:40:58 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.5889
2022-02-23 03:41:32 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.3952
2022-02-23 03:42:05 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.6423
2022-02-23 03:42:39 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.7097
2022-02-23 03:43:12 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.9500
2022-02-23 03:43:45 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.3356
2022-02-23 03:44:19 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.6731
2022-02-23 03:44:53 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.5764
2022-02-23 03:45:26 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.6747
2022-02-23 03:45:58 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.6052
2022-02-23 03:46:32 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.4592
2022-02-23 03:47:05 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.4218
2022-02-23 03:47:38 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.2777
2022-02-23 03:48:11 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.8664
2022-02-23 03:48:44 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.6063
2022-02-23 03:49:18 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.8255
2022-02-23 03:49:51 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.8558
2022-02-23 03:50:25 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.5996
2022-02-23 03:50:59 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7555
2022-02-23 03:51:32 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.7363
2022-02-23 03:52:06 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.8210
2022-02-23 03:52:39 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.5489
2022-02-23 03:53:13 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.5402
2022-02-23 03:53:46 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.5514
2022-02-23 03:54:19 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.7108
2022-02-23 03:54:52 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.7623
2022-02-23 03:55:25 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.6968
2022-02-23 03:55:58 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.6158
2022-02-23 03:56:32 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.6388
2022-02-23 03:57:05 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.5538
2022-02-23 03:57:38 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.6458
2022-02-23 03:58:11 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.5469
2022-02-23 03:58:45 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.7700
2022-02-23 03:59:18 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.5044
2022-02-23 03:59:52 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.6402
2022-02-23 04:00:25 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.7397
2022-02-23 04:00:59 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.8993
2022-02-23 04:01:33 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.6385
2022-02-23 04:02:06 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.6178
2022-02-23 04:02:38 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.6488
2022-02-23 04:02:40 - train: epoch 041, train_loss: 1.6431
2022-02-23 04:03:55 - eval: epoch: 041, acc1: 64.746%, acc5: 86.472%, test_loss: 1.4376, per_image_load_time: 2.637ms, per_image_inference_time: 0.259ms
2022-02-23 04:03:55 - until epoch: 041, best_acc1: 65.970%
2022-02-23 04:03:55 - epoch 042 lr: 0.010000000000000002
2022-02-23 04:04:33 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.3067
2022-02-23 04:05:07 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.6811
2022-02-23 04:05:40 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.6953
2022-02-23 04:06:14 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.3598
2022-02-23 04:06:47 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.7114
2022-02-23 04:07:20 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.4717
2022-02-23 04:07:52 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.6656
2022-02-23 04:08:26 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.5831
2022-02-23 04:08:59 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.8414
2022-02-23 04:09:32 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.7780
2022-02-23 04:10:05 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.3968
2022-02-23 04:10:38 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.5896
2022-02-23 04:11:12 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.6836
2022-02-23 04:11:46 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.9656
2022-02-23 04:12:19 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.5872
2022-02-23 04:12:52 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.7362
2022-02-23 04:13:25 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.6500
2022-02-23 04:13:58 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.6287
2022-02-23 04:14:31 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.7368
2022-02-23 04:15:04 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.5550
2022-02-23 04:15:37 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.4893
2022-02-23 04:16:11 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.5355
2022-02-23 04:16:45 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.5630
2022-02-23 04:17:18 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.9681
2022-02-23 04:17:51 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.8039
2022-02-23 04:18:25 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.7469
2022-02-23 04:18:57 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.6044
2022-02-23 04:19:30 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.5491
2022-02-23 04:20:04 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.6189
2022-02-23 04:20:38 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.6661
2022-02-23 04:21:11 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.7237
2022-02-23 04:21:44 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.7376
2022-02-23 04:22:16 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.8110
2022-02-23 04:22:50 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.5616
2022-02-23 04:23:23 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.5817
2022-02-23 04:23:56 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.8047
2022-02-23 04:24:29 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.5994
2022-02-23 04:25:03 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.4518
2022-02-23 04:25:36 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.5715
2022-02-23 04:26:10 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.7179
2022-02-23 04:26:42 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.8291
2022-02-23 04:27:16 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.6374
2022-02-23 04:27:48 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.4441
2022-02-23 04:28:22 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.5630
2022-02-23 04:28:55 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.6063
2022-02-23 04:29:29 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.8975
2022-02-23 04:30:01 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.4913
2022-02-23 04:30:36 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.7787
2022-02-23 04:31:09 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.7330
2022-02-23 04:31:42 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.5810
2022-02-23 04:31:43 - train: epoch 042, train_loss: 1.6440
2022-02-23 04:32:58 - eval: epoch: 042, acc1: 65.568%, acc5: 87.120%, test_loss: 1.4034, per_image_load_time: 2.122ms, per_image_inference_time: 0.281ms
2022-02-23 04:32:58 - until epoch: 042, best_acc1: 65.970%
2022-02-23 04:32:58 - epoch 043 lr: 0.010000000000000002
2022-02-23 04:33:36 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.6031
2022-02-23 04:34:10 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.7295
2022-02-23 04:34:43 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.4381
2022-02-23 04:35:16 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.6145
2022-02-23 04:35:49 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.5982
2022-02-23 04:36:23 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.6042
2022-02-23 04:36:56 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.6666
2022-02-23 04:37:30 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.8369
2022-02-23 04:38:03 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.6123
2022-02-23 04:38:37 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.8388
2022-02-23 04:39:10 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.7080
2022-02-23 04:39:43 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.6935
2022-02-23 04:40:16 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.7285
2022-02-23 04:40:50 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.6326
2022-02-23 04:41:23 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.4433
2022-02-23 04:41:56 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.6927
2022-02-23 04:42:30 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.7358
2022-02-23 04:43:03 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.6961
2022-02-23 04:43:36 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.6245
2022-02-23 04:44:09 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.4640
2022-02-23 04:44:43 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.5839
2022-02-23 04:45:16 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.7118
2022-02-23 04:45:49 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.8035
2022-02-23 04:46:22 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.5287
2022-02-23 04:46:56 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.8330
2022-02-23 04:47:30 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.3665
2022-02-23 04:48:02 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.7147
2022-02-23 04:48:35 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.5271
2022-02-23 04:49:09 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.7143
2022-02-23 04:49:42 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.9021
2022-02-23 04:50:15 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.9164
2022-02-23 04:50:49 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.6444
2022-02-23 04:51:22 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.7598
2022-02-23 04:51:56 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.6006
2022-02-23 04:52:29 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.6554
2022-02-23 04:53:02 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.6496
2022-02-23 04:53:36 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.6403
2022-02-23 04:54:08 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.7829
2022-02-23 04:54:42 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.8426
2022-02-23 04:55:15 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.6493
2022-02-23 04:55:49 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.9879
2022-02-23 04:56:22 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.6008
2022-02-23 04:56:55 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.6037
2022-02-23 04:57:29 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.6010
2022-02-23 04:58:02 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.5962
2022-02-23 04:58:36 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.6820
2022-02-23 04:59:10 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.5534
2022-02-23 04:59:43 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.6719
2022-02-23 05:00:16 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.5651
2022-02-23 05:00:50 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.5715
2022-02-23 05:00:52 - train: epoch 043, train_loss: 1.6445
2022-02-23 05:02:07 - eval: epoch: 043, acc1: 65.468%, acc5: 86.872%, test_loss: 1.4025, per_image_load_time: 2.292ms, per_image_inference_time: 0.265ms
2022-02-23 05:02:07 - until epoch: 043, best_acc1: 65.970%
2022-02-23 05:02:07 - epoch 044 lr: 0.010000000000000002
2022-02-23 05:02:46 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.7190
2022-02-23 05:03:18 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.7748
2022-02-23 05:03:51 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.6806
2022-02-23 05:04:24 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.2765
2022-02-23 05:04:57 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.5789
2022-02-23 05:05:30 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.4544
2022-02-23 05:06:04 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.4746
2022-02-23 05:06:37 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.5472
2022-02-23 05:07:11 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.7059
2022-02-23 05:07:44 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.7222
2022-02-23 05:08:17 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.5455
2022-02-23 05:08:51 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.5772
2022-02-23 05:09:24 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.6988
2022-02-23 05:09:58 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.5179
2022-02-23 05:10:31 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.6606
2022-02-23 05:11:04 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.5302
2022-02-23 05:11:38 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.5350
2022-02-23 05:12:11 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.5542
2022-02-23 05:12:45 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.7909
2022-02-23 05:13:18 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.5971
2022-02-23 05:13:52 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.7791
2022-02-23 05:14:25 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.6383
2022-02-23 05:14:58 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.8416
2022-02-23 05:15:31 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.7249
2022-02-23 05:16:05 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.7585
2022-02-23 05:16:38 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.7493
2022-02-23 05:17:11 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.7813
2022-02-23 05:17:44 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.4382
2022-02-23 05:18:18 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.4966
2022-02-23 05:18:51 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.6263
2022-02-23 05:19:25 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.6802
2022-02-23 05:19:58 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.3341
2022-02-23 05:20:31 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.5917
2022-02-23 05:21:04 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.8242
2022-02-23 05:21:38 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.6109
2022-02-23 05:22:11 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.8271
2022-02-23 05:22:44 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.7731
2022-02-23 05:23:17 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.3387
2022-02-23 05:23:51 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.7414
2022-02-23 05:24:24 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.8197
2022-02-23 05:24:58 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.5226
2022-02-23 05:25:31 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.7275
2022-02-23 05:26:04 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.7677
2022-02-23 05:26:37 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.4282
2022-02-23 05:27:11 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.6976
2022-02-23 05:27:45 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.5454
2022-02-23 05:28:18 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.6473
2022-02-23 05:28:51 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.8564
2022-02-23 05:29:25 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.5315
2022-02-23 05:29:57 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.7661
2022-02-23 05:29:59 - train: epoch 044, train_loss: 1.6470
2022-02-23 05:31:14 - eval: epoch: 044, acc1: 65.030%, acc5: 86.884%, test_loss: 1.4153, per_image_load_time: 2.649ms, per_image_inference_time: 0.246ms
2022-02-23 05:31:14 - until epoch: 044, best_acc1: 65.970%
2022-02-23 05:31:14 - epoch 045 lr: 0.010000000000000002
2022-02-23 05:31:53 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.4891
2022-02-23 05:32:25 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.6260
2022-02-23 05:32:59 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.6654
2022-02-23 05:33:31 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.4557
2022-02-23 05:34:04 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.8950
2022-02-23 05:34:36 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.7180
2022-02-23 05:35:11 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.3329
2022-02-23 05:35:43 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.5499
2022-02-23 05:36:17 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.6544
2022-02-23 05:36:50 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.5751
2022-02-23 05:37:23 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.7737
2022-02-23 05:37:55 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.6703
2022-02-23 05:38:29 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.7481
2022-02-23 05:39:03 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.6334
2022-02-23 05:39:36 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.6874
2022-02-23 05:40:09 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.4709
2022-02-23 05:40:43 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.5267
2022-02-23 05:41:16 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.5363
2022-02-23 05:41:50 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.6757
2022-02-23 05:42:23 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.8165
2022-02-23 05:42:57 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.7220
2022-02-23 05:43:30 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.6759
2022-02-23 05:44:04 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.4930
2022-02-23 05:44:37 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.7564
2022-02-23 05:45:11 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.6798
2022-02-23 05:45:44 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.7133
2022-02-23 05:46:17 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.4591
2022-02-23 05:46:51 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.6046
2022-02-23 05:47:24 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.7710
2022-02-23 05:47:57 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.8679
2022-02-23 05:48:30 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.5912
2022-02-23 05:49:03 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.8001
2022-02-23 05:49:37 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.6002
2022-02-23 05:50:09 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.4493
2022-02-23 05:50:43 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.8413
2022-02-23 05:51:16 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.7280
2022-02-23 05:51:50 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.5768
2022-02-23 05:52:22 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.5437
2022-02-23 05:52:56 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.7185
2022-02-23 05:53:29 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.8311
2022-02-23 05:54:03 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.6238
2022-02-23 05:54:36 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.7731
2022-02-23 05:55:09 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.8604
2022-02-23 05:55:43 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.7848
2022-02-23 05:56:15 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.5352
2022-02-23 05:56:49 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.8615
2022-02-23 05:57:22 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.7713
2022-02-23 05:57:55 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.6258
2022-02-23 05:58:30 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.5954
2022-02-23 05:59:02 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.6298
2022-02-23 05:59:03 - train: epoch 045, train_loss: 1.6449
2022-02-23 06:00:19 - eval: epoch: 045, acc1: 65.462%, acc5: 86.854%, test_loss: 1.4074, per_image_load_time: 2.607ms, per_image_inference_time: 0.266ms
2022-02-23 06:00:19 - until epoch: 045, best_acc1: 65.970%
2022-02-23 06:00:19 - epoch 046 lr: 0.010000000000000002
2022-02-23 06:00:58 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.3645
2022-02-23 06:01:31 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.5422
2022-02-23 06:02:05 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.6391
2022-02-23 06:02:37 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.8143
2022-02-23 06:03:11 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.4804
2022-02-23 06:03:44 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.7504
2022-02-23 06:04:17 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.6007
2022-02-23 06:04:51 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.7500
2022-02-23 06:05:24 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.7168
2022-02-23 06:05:57 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.4564
2022-02-23 06:06:30 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.6412
2022-02-23 06:07:04 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.6103
2022-02-23 06:07:37 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.6328
2022-02-23 06:08:11 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.8849
2022-02-23 06:08:44 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.6097
2022-02-23 06:09:18 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.6884
2022-02-23 06:09:51 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.5902
2022-02-23 06:10:25 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.6972
2022-02-23 06:10:58 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.4592
2022-02-23 06:11:31 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.5869
2022-02-23 06:12:04 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.7111
2022-02-23 06:12:38 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.4216
2022-02-23 06:13:12 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.7971
2022-02-23 06:13:45 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.6877
2022-02-23 06:14:19 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.6854
2022-02-23 06:14:53 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.7904
2022-02-23 06:15:26 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.5257
2022-02-23 06:15:59 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.5560
2022-02-23 06:16:32 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.6894
2022-02-23 06:17:05 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.5747
2022-02-23 06:17:39 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.4710
2022-02-23 06:18:12 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.8787
2022-02-23 06:18:45 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.7808
2022-02-23 06:19:19 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.7405
2022-02-23 06:19:52 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.6813
2022-02-23 06:20:26 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.5159
2022-02-23 06:20:59 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.5730
2022-02-23 06:21:33 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.7567
2022-02-23 06:22:06 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.6245
2022-02-23 06:22:39 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.5517
2022-02-23 06:23:12 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.9299
2022-02-23 06:23:46 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.5270
2022-02-23 06:24:20 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.7991
2022-02-23 06:24:53 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.7637
2022-02-23 06:25:27 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.5321
2022-02-23 06:26:00 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.7581
2022-02-23 06:26:33 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.5493
2022-02-23 06:27:08 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.6840
2022-02-23 06:27:42 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.6801
2022-02-23 06:28:14 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.6355
2022-02-23 06:28:16 - train: epoch 046, train_loss: 1.6451
2022-02-23 06:29:30 - eval: epoch: 046, acc1: 64.798%, acc5: 86.544%, test_loss: 1.4314, per_image_load_time: 1.476ms, per_image_inference_time: 0.247ms
2022-02-23 06:29:31 - until epoch: 046, best_acc1: 65.970%
2022-02-23 06:29:31 - epoch 047 lr: 0.010000000000000002
2022-02-23 06:30:09 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.5327
2022-02-23 06:30:43 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.8257
2022-02-23 06:31:16 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.5630
2022-02-23 06:31:50 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.5403
2022-02-23 06:32:23 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.6633
2022-02-23 06:32:56 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.7035
2022-02-23 06:33:29 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.7490
2022-02-23 06:34:03 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.4952
2022-02-23 06:34:36 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.7782
2022-02-23 06:35:10 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.6584
2022-02-23 06:35:43 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.6745
2022-02-23 06:36:16 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.6119
2022-02-23 06:36:49 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.7073
2022-02-23 06:37:23 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.6675
2022-02-23 06:37:56 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.4784
2022-02-23 06:38:30 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.5396
2022-02-23 06:39:03 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.4863
2022-02-23 06:39:37 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.6079
2022-02-23 06:40:11 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.5787
2022-02-23 06:40:44 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.5638
2022-02-23 06:41:18 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.8095
2022-02-23 06:41:52 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.7983
2022-02-23 06:42:25 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.6359
2022-02-23 06:42:58 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.4431
2022-02-23 06:43:31 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.6652
2022-02-23 06:44:05 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.9218
2022-02-23 06:44:38 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.6171
2022-02-23 06:45:11 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.8257
2022-02-23 06:45:45 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.6078
2022-02-23 06:46:19 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.7026
2022-02-23 06:46:51 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.5809
2022-02-23 06:47:25 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.9173
2022-02-23 06:47:58 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.5458
2022-02-23 06:48:30 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.6628
2022-02-23 06:49:04 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.7483
2022-02-23 06:49:37 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.6588
2022-02-23 06:50:10 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.8123
2022-02-23 06:50:44 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.6545
2022-02-23 06:51:17 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.5085
2022-02-23 06:51:52 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.7311
2022-02-23 06:52:26 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.6136
2022-02-23 06:52:59 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.6361
2022-02-23 06:53:34 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.5297
2022-02-23 06:54:08 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.6155
2022-02-23 06:54:42 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.7161
2022-02-23 06:55:15 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.6353
2022-02-23 06:55:49 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.6620
2022-02-23 06:56:24 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.5474
2022-02-23 06:56:57 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.7309
2022-02-23 06:57:30 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.5281
2022-02-23 06:57:32 - train: epoch 047, train_loss: 1.6473
2022-02-23 06:58:48 - eval: epoch: 047, acc1: 64.810%, acc5: 86.492%, test_loss: 1.4361, per_image_load_time: 1.936ms, per_image_inference_time: 0.261ms
2022-02-23 06:58:48 - until epoch: 047, best_acc1: 65.970%
2022-02-23 06:58:48 - epoch 048 lr: 0.010000000000000002
2022-02-23 06:59:27 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.7637
2022-02-23 07:00:01 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.0490
2022-02-23 07:00:34 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.6869
2022-02-23 07:01:08 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.6015
2022-02-23 07:01:40 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.5063
2022-02-23 07:02:15 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.5959
2022-02-23 07:02:50 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.6359
2022-02-23 07:03:23 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.7255
2022-02-23 07:03:57 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.8251
2022-02-23 07:04:31 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.6950
2022-02-23 07:05:06 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.5851
2022-02-23 07:05:39 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.7877
2022-02-23 07:06:13 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.4284
2022-02-23 07:06:47 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.5551
2022-02-23 07:07:21 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.6759
2022-02-23 07:07:55 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.6373
2022-02-23 07:08:29 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.8261
2022-02-23 07:09:03 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.6471
2022-02-23 07:09:38 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.7410
2022-02-23 07:10:11 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.8471
2022-02-23 07:10:45 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.6937
2022-02-23 07:11:19 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.8466
2022-02-23 07:11:52 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.4547
2022-02-23 07:12:27 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.7659
2022-02-23 07:13:01 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.6556
2022-02-23 07:13:34 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.8236
2022-02-23 07:14:08 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.7846
2022-02-23 07:14:43 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.6141
2022-02-23 07:15:16 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.7935
2022-02-23 07:15:50 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.6204
2022-02-23 07:16:24 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.5992
2022-02-23 07:16:58 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.4850
2022-02-23 07:17:31 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.7424
2022-02-23 07:18:06 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.6348
2022-02-23 07:18:39 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.8514
2022-02-23 07:19:14 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.7176
2022-02-23 07:19:47 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.7047
2022-02-23 07:20:21 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.6258
2022-02-23 07:20:56 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.7682
2022-02-23 07:21:30 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.4607
2022-02-23 07:22:04 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.8883
2022-02-23 07:22:37 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.5979
2022-02-23 07:23:10 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.5720
2022-02-23 07:23:45 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.5370
2022-02-23 07:24:19 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.6665
2022-02-23 07:24:52 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.6036
2022-02-23 07:25:25 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.7462
2022-02-23 07:25:56 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.9238
2022-02-23 07:26:31 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.7036
2022-02-23 07:27:05 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.6849
2022-02-23 07:27:06 - train: epoch 048, train_loss: 1.6485
2022-02-23 07:28:21 - eval: epoch: 048, acc1: 65.344%, acc5: 87.048%, test_loss: 1.4019, per_image_load_time: 0.790ms, per_image_inference_time: 0.266ms
2022-02-23 07:28:22 - until epoch: 048, best_acc1: 65.970%
2022-02-23 07:50:40 - epoch 049 lr: 0.010000000000000002
2022-02-23 07:51:20 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6629
2022-02-23 07:51:53 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.6781
2022-02-23 07:52:27 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.8489
2022-02-23 07:53:00 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.5087
2022-02-23 07:53:33 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.6455
2022-02-23 07:54:07 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.4683
2022-02-23 07:54:40 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.4646
2022-02-23 07:55:14 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.7861
2022-02-23 07:55:47 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.4440
2022-02-23 07:56:20 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.5751
2022-02-23 07:56:52 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.5824
2022-02-23 07:57:25 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.5372
2022-02-23 07:57:56 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.7991
2022-02-23 07:58:30 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.8753
2022-02-23 07:59:03 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.6621
2022-02-23 07:59:36 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.6217
2022-02-23 08:00:10 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.6456
2022-02-23 08:00:43 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.7205
2022-02-23 08:01:17 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.4596
2022-02-23 08:01:50 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.5935
2022-02-23 08:02:23 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.5233
2022-02-23 08:02:58 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.4371
2022-02-23 08:03:31 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.6245
2022-02-23 08:04:04 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.6971
2022-02-23 08:04:37 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.8155
2022-02-23 08:05:11 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.8297
2022-02-23 08:05:45 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.5984
2022-02-23 08:06:19 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.8831
2022-02-23 08:06:52 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.8082
2022-02-23 08:07:25 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.6599
2022-02-23 08:07:59 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.5818
2022-02-23 08:08:32 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.7919
2022-02-23 08:09:06 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.5104
2022-02-23 08:09:39 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.5731
2022-02-23 08:10:12 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.6681
2022-02-23 08:10:46 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.6536
2022-02-23 08:11:19 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.6132
2022-02-23 08:11:54 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.7426
2022-02-23 08:12:26 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.8474
2022-02-23 08:13:00 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.7238
2022-02-23 08:13:33 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.5859
2022-02-23 08:14:06 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.8944
2022-02-23 08:14:41 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.7767
2022-02-23 08:15:14 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.6601
2022-02-23 08:15:48 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.5196
2022-02-23 08:16:21 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.6452
2022-02-23 08:16:55 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.9283
2022-02-23 08:17:30 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.6132
2022-02-23 08:18:03 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.6012
2022-02-23 08:18:36 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.6586
2022-02-23 08:18:37 - train: epoch 049, train_loss: 1.6418
2022-02-23 08:19:53 - eval: epoch: 049, acc1: 65.152%, acc5: 86.802%, test_loss: 1.4182, per_image_load_time: 2.637ms, per_image_inference_time: 0.296ms
2022-02-23 08:19:53 - until epoch: 049, best_acc1: 65.970%
2022-02-23 08:19:53 - epoch 050 lr: 0.010000000000000002
2022-02-23 08:20:32 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.6360
2022-02-23 08:21:06 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.7380
2022-02-23 08:21:40 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.5949
2022-02-23 08:22:14 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.6540
2022-02-23 08:22:47 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.5394
2022-02-23 08:23:21 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.9237
2022-02-23 08:23:54 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.4425
2022-02-23 08:24:29 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.4248
2022-02-23 08:25:03 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.5406
2022-02-23 08:25:36 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.8217
2022-02-23 08:26:10 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.5845
2022-02-23 08:26:44 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.5718
2022-02-23 08:27:17 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.6131
2022-02-23 08:27:52 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.7085
2022-02-23 08:28:24 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.4676
2022-02-23 08:28:56 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.4916
2022-02-23 08:29:29 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.7834
2022-02-23 08:30:02 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 2.0099
2022-02-23 08:30:35 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.7178
2022-02-23 08:31:10 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.6355
2022-02-23 08:31:43 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.6852
2022-02-23 08:32:16 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.7881
2022-02-23 08:32:49 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.6461
2022-02-23 08:33:23 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.7796
2022-02-23 08:33:56 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.6952
2022-02-23 08:34:30 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.4655
2022-02-23 08:35:04 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.6066
2022-02-23 08:35:37 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.6654
2022-02-23 08:36:12 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.9315
2022-02-23 08:36:45 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.7368
2022-02-23 08:37:18 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.7671
2022-02-23 08:37:52 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.6366
2022-02-23 08:38:26 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.5106
2022-02-23 08:39:00 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.6813
2022-02-23 08:39:33 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.7197
2022-02-23 08:40:07 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.5229
2022-02-23 08:40:40 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.7011
2022-02-23 08:41:14 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.5910
2022-02-23 08:41:47 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.3656
2022-02-23 08:42:21 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.6343
2022-02-23 08:42:54 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.5892
2022-02-23 08:43:28 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.7608
2022-02-23 08:44:02 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.8330
2022-02-23 08:44:35 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.5601
2022-02-23 08:45:10 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.4983
2022-02-23 08:45:43 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.6426
2022-02-23 08:46:17 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.7147
2022-02-23 08:46:50 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.5779
2022-02-23 08:47:24 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.4261
2022-02-23 08:47:57 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.3987
2022-02-23 08:47:58 - train: epoch 050, train_loss: 1.6450
2022-02-23 08:49:13 - eval: epoch: 050, acc1: 64.978%, acc5: 86.866%, test_loss: 1.4153, per_image_load_time: 1.030ms, per_image_inference_time: 0.281ms
2022-02-23 08:49:13 - until epoch: 050, best_acc1: 65.970%
2022-02-23 08:49:13 - epoch 051 lr: 0.010000000000000002
2022-02-23 08:49:52 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.9548
2022-02-23 08:50:26 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.7550
2022-02-23 08:50:59 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.6713
2022-02-23 08:51:33 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.3844
2022-02-23 08:52:08 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.6403
2022-02-23 08:52:40 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.5623
2022-02-23 08:53:14 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.6369
2022-02-23 08:53:48 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.7849
2022-02-23 08:54:21 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.4566
2022-02-23 08:54:55 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.8730
2022-02-23 08:55:28 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.8328
2022-02-23 08:56:02 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.4444
2022-02-23 08:56:36 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.2775
2022-02-23 08:57:09 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.4173
2022-02-23 08:57:43 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.6306
2022-02-23 08:58:16 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.3377
2022-02-23 08:58:50 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.7608
2022-02-23 08:59:24 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.6567
2022-02-23 08:59:57 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.5183
2022-02-23 09:00:28 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.6172
2022-02-23 09:01:00 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.6750
2022-02-23 09:01:34 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.5491
2022-02-23 09:02:07 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.8171
2022-02-23 09:02:41 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.6462
2022-02-23 09:03:14 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.5966
2022-02-23 09:03:48 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.3783
2022-02-23 09:04:22 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.5090
2022-02-23 09:04:56 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.4972
2022-02-23 09:05:30 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.6223
2022-02-23 09:06:03 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.6102
2022-02-23 09:06:37 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.6528
2022-02-23 09:07:10 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.6242
2022-02-23 09:07:44 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.8514
2022-02-23 09:08:17 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.6796
2022-02-23 09:08:51 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.6320
2022-02-23 09:09:24 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.6763
2022-02-23 09:09:57 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.8480
2022-02-23 09:10:31 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.5834
2022-02-23 09:11:05 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.8509
2022-02-23 09:11:39 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.5908
2022-02-23 09:12:13 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.7730
2022-02-23 09:12:46 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.7133
2022-02-23 09:13:19 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.4585
2022-02-23 09:13:53 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.9061
2022-02-23 09:14:26 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.5399
2022-02-23 09:15:00 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.7619
2022-02-23 09:15:35 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.5876
2022-02-23 09:16:08 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.6641
2022-02-23 09:16:42 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.8067
2022-02-23 09:17:15 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.5605
2022-02-23 09:17:16 - train: epoch 051, train_loss: 1.6472
2022-02-23 09:18:31 - eval: epoch: 051, acc1: 64.610%, acc5: 86.302%, test_loss: 1.4504, per_image_load_time: 2.401ms, per_image_inference_time: 0.283ms
2022-02-23 09:18:31 - until epoch: 051, best_acc1: 65.970%
2022-02-23 09:18:31 - epoch 052 lr: 0.010000000000000002
2022-02-23 09:19:09 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.6049
2022-02-23 09:19:43 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.7274
2022-02-23 09:20:17 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.7017
2022-02-23 09:20:50 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.6949
2022-02-23 09:21:25 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.6914
2022-02-23 09:21:58 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.6259
2022-02-23 09:22:32 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.7724
2022-02-23 09:23:05 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.5408
2022-02-23 09:23:39 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.6275
2022-02-23 09:24:13 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.7743
2022-02-23 09:24:47 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.5334
2022-02-23 09:25:20 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.7897
2022-02-23 09:25:54 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.4444
2022-02-23 09:26:28 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.9546
2022-02-23 09:27:01 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.6307
2022-02-23 09:27:36 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.4305
2022-02-23 09:28:10 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.6397
2022-02-23 09:28:43 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.7389
2022-02-23 09:29:17 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.4910
2022-02-23 09:29:51 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.7494
2022-02-23 09:30:24 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.4548
2022-02-23 09:30:58 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.8897
2022-02-23 09:31:32 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.4914
2022-02-23 09:32:04 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.3565
2022-02-23 09:32:36 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.5381
2022-02-23 09:33:08 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.5145
2022-02-23 09:33:42 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.4305
2022-02-23 09:34:15 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.6299
2022-02-23 09:34:50 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.5186
2022-02-23 09:35:23 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.5048
2022-02-23 09:35:57 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.7559
2022-02-23 09:36:31 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.5149
2022-02-23 09:37:05 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.6304
2022-02-23 09:37:38 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.7870
2022-02-23 09:38:12 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.5858
2022-02-23 09:38:45 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.7807
2022-02-23 09:39:18 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.7623
2022-02-23 09:39:53 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.8865
2022-02-23 09:40:26 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.3914
2022-02-23 09:41:00 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.8770
2022-02-23 09:41:34 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.5525
2022-02-23 09:42:07 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.5397
2022-02-23 09:42:42 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.6212
2022-02-23 09:43:14 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.7582
2022-02-23 09:43:48 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.5074
2022-02-23 09:44:22 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.6595
2022-02-23 09:44:55 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.7848
2022-02-23 09:45:30 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.4429
2022-02-23 09:46:04 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.7079
2022-02-23 09:46:36 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.5729
2022-02-23 09:46:37 - train: epoch 052, train_loss: 1.6406
2022-02-23 09:47:52 - eval: epoch: 052, acc1: 64.998%, acc5: 86.738%, test_loss: 1.4274, per_image_load_time: 2.597ms, per_image_inference_time: 0.275ms
2022-02-23 09:47:53 - until epoch: 052, best_acc1: 65.970%
2022-02-23 09:47:53 - epoch 053 lr: 0.010000000000000002
2022-02-23 09:48:31 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.6138
2022-02-23 09:49:05 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.7503
2022-02-23 09:49:39 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.5969
2022-02-23 09:50:13 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.5696
2022-02-23 09:50:46 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.5840
2022-02-23 09:51:20 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.4642
2022-02-23 09:51:53 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.6167
2022-02-23 09:52:26 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.6659
2022-02-23 09:53:00 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.5893
2022-02-23 09:53:33 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.6658
2022-02-23 09:54:08 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.7371
2022-02-23 09:54:41 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.4176
2022-02-23 09:55:14 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.7828
2022-02-23 09:55:47 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.9211
2022-02-23 09:56:21 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.5216
2022-02-23 09:56:55 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.6282
2022-02-23 09:57:29 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.8175
2022-02-23 09:58:02 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.7591
2022-02-23 09:58:36 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.4775
2022-02-23 09:59:09 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.5795
2022-02-23 09:59:43 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.8168
2022-02-23 10:00:17 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.6183
2022-02-23 10:00:51 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.5555
2022-02-23 10:01:24 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.5510
2022-02-23 10:01:58 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.8329
2022-02-23 10:02:31 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.8435
2022-02-23 10:03:05 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.7177
2022-02-23 10:03:38 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.7759
2022-02-23 10:04:11 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.4897
2022-02-23 10:04:43 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.4842
2022-02-23 10:05:15 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.7575
2022-02-23 10:05:50 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.7828
2022-02-23 10:06:24 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.7151
2022-02-23 10:06:57 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.6436
2022-02-23 10:07:31 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.6617
2022-02-23 10:08:04 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.5265
2022-02-23 10:08:39 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.7562
2022-02-23 10:09:11 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.6076
2022-02-23 10:09:46 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.8654
2022-02-23 10:10:19 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.6504
2022-02-23 10:10:53 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.6040
2022-02-23 10:11:26 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.6509
2022-02-23 10:12:00 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.9111
2022-02-23 10:12:33 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.5821
2022-02-23 10:13:06 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.8621
2022-02-23 10:13:40 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.5739
2022-02-23 10:14:15 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.7432
2022-02-23 10:14:48 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.7715
2022-02-23 10:15:23 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.6463
2022-02-23 10:15:55 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.5471
2022-02-23 10:15:57 - train: epoch 053, train_loss: 1.6380
2022-02-23 10:17:12 - eval: epoch: 053, acc1: 63.950%, acc5: 85.936%, test_loss: 1.4825, per_image_load_time: 2.025ms, per_image_inference_time: 0.254ms
2022-02-23 10:17:12 - until epoch: 053, best_acc1: 65.970%
2022-02-23 10:17:12 - epoch 054 lr: 0.010000000000000002
2022-02-23 10:17:51 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.4454
2022-02-23 10:18:24 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.8167
2022-02-23 10:18:58 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.7604
2022-02-23 10:19:31 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.5208
2022-02-23 10:20:05 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.5732
2022-02-23 10:20:38 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.6848
2022-02-23 10:21:12 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.9615
2022-02-23 10:21:46 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.8724
2022-02-23 10:22:20 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.6620
2022-02-23 10:22:53 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.4649
2022-02-23 10:23:26 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.5430
2022-02-23 10:24:01 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.7271
2022-02-23 10:24:35 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.5413
2022-02-23 10:25:08 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.7170
2022-02-23 10:25:42 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.6661
2022-02-23 10:26:15 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.5520
2022-02-23 10:26:49 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.8860
2022-02-23 10:27:23 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.3938
2022-02-23 10:27:57 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.7095
2022-02-23 10:28:31 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.5992
2022-02-23 10:29:04 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.5995
2022-02-23 10:29:38 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.7470
2022-02-23 10:30:12 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.5886
2022-02-23 10:30:46 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.7634
2022-02-23 10:31:20 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.6246
2022-02-23 10:31:54 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.4441
2022-02-23 10:32:27 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.8050
2022-02-23 10:33:00 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.8254
2022-02-23 10:33:34 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.5435
2022-02-23 10:34:07 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.6576
2022-02-23 10:34:41 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.7353
2022-02-23 10:35:13 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.5987
2022-02-23 10:35:45 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.5599
2022-02-23 10:36:17 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.4764
2022-02-23 10:36:50 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.7237
2022-02-23 10:37:24 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.5497
2022-02-23 10:37:57 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.6435
2022-02-23 10:38:31 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.4988
2022-02-23 10:39:05 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.5995
2022-02-23 10:39:38 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.4787
2022-02-23 10:40:13 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.6645
2022-02-23 10:40:46 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.7883
2022-02-23 10:41:20 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.6345
2022-02-23 10:41:53 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.2729
2022-02-23 10:42:27 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.5876
2022-02-23 10:43:01 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.7104
2022-02-23 10:43:35 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.8334
2022-02-23 10:44:08 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.7013
2022-02-23 10:44:43 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.3237
2022-02-23 10:45:16 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.6440
2022-02-23 10:45:17 - train: epoch 054, train_loss: 1.6410
2022-02-23 10:46:32 - eval: epoch: 054, acc1: 64.052%, acc5: 86.372%, test_loss: 1.4572, per_image_load_time: 2.283ms, per_image_inference_time: 0.295ms
2022-02-23 10:46:32 - until epoch: 054, best_acc1: 65.970%
2022-02-23 10:46:32 - epoch 055 lr: 0.010000000000000002
2022-02-23 10:47:12 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.5414
2022-02-23 10:47:44 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.6514
2022-02-23 10:48:19 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.4337
2022-02-23 10:48:54 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.5007
2022-02-23 10:49:26 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.5182
2022-02-23 10:50:00 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.7052
2022-02-23 10:50:34 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.7279
2022-02-23 10:51:07 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.3664
2022-02-23 10:51:42 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.7695
2022-02-23 10:52:15 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.5369
2022-02-23 10:52:49 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.6640
2022-02-23 10:53:22 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.5695
2022-02-23 10:53:57 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.7229
2022-02-23 10:54:31 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.4702
2022-02-23 10:55:04 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.4124
2022-02-23 10:55:38 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.7353
2022-02-23 10:56:12 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.7973
2022-02-23 10:56:46 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.5566
2022-02-23 10:57:19 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.7037
2022-02-23 10:57:52 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.6458
2022-02-23 10:58:27 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.3718
2022-02-23 10:59:00 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.7079
2022-02-23 10:59:34 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.5931
2022-02-23 11:00:07 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.6065
2022-02-23 11:00:42 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.6020
2022-02-23 11:01:15 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.4460
2022-02-23 11:01:48 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.6971
2022-02-23 11:02:22 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.8038
2022-02-23 11:02:55 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.5630
2022-02-23 11:03:29 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.5776
2022-02-23 11:04:03 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.7769
2022-02-23 11:04:36 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.5373
2022-02-23 11:05:09 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.6307
2022-02-23 11:05:43 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.5358
2022-02-23 11:06:17 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.4220
2022-02-23 11:06:51 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.8356
2022-02-23 11:07:22 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.5522
2022-02-23 11:07:54 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.7340
2022-02-23 11:08:27 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.8275
2022-02-23 11:09:01 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.7075
2022-02-23 11:09:35 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.6558
2022-02-23 11:10:09 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.7870
2022-02-23 11:10:42 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.8941
2022-02-23 11:11:16 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.7061
2022-02-23 11:11:50 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.6137
2022-02-23 11:12:23 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.7193
2022-02-23 11:12:57 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.5241
2022-02-23 11:13:31 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.6409
2022-02-23 11:14:06 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.6085
2022-02-23 11:14:39 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.7830
2022-02-23 11:14:40 - train: epoch 055, train_loss: 1.6343
2022-02-23 11:15:55 - eval: epoch: 055, acc1: 65.320%, acc5: 87.074%, test_loss: 1.4009, per_image_load_time: 2.600ms, per_image_inference_time: 0.298ms
2022-02-23 11:15:55 - until epoch: 055, best_acc1: 65.970%
2022-02-23 11:15:55 - epoch 056 lr: 0.010000000000000002
2022-02-23 11:16:34 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.7459
2022-02-23 11:17:08 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.6348
2022-02-23 11:17:42 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.4715
2022-02-23 11:18:15 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.5226
2022-02-23 11:18:50 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.7606
2022-02-23 11:19:24 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.6518
2022-02-23 11:19:58 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.6299
2022-02-23 11:20:31 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.7736
2022-02-23 11:21:06 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.6688
2022-02-23 11:21:39 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.7308
2022-02-23 11:22:14 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.4868
2022-02-23 11:22:48 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.5915
2022-02-23 11:23:22 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.8000
2022-02-23 11:23:55 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.5117
2022-02-23 11:24:29 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.8981
2022-02-23 11:25:03 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.6349
2022-02-23 11:25:37 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.6028
2022-02-23 11:26:11 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.7770
2022-02-23 11:26:44 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.6537
2022-02-23 11:27:18 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.7374
2022-02-23 11:27:51 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.7361
2022-02-23 11:28:25 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.6449
2022-02-23 11:28:59 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.6041
2022-02-23 11:29:32 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.6759
2022-02-23 11:30:06 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.7479
2022-02-23 11:30:39 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.8030
2022-02-23 11:31:13 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.6995
2022-02-23 11:31:46 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.5829
2022-02-23 11:32:20 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.6222
2022-02-23 11:32:55 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.7209
2022-02-23 11:33:29 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.4546
2022-02-23 11:34:02 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.4649
2022-02-23 11:34:36 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.6826
2022-02-23 11:35:09 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.6886
2022-02-23 11:35:43 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.5779
2022-02-23 11:36:17 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.5031
2022-02-23 11:36:51 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.5170
2022-02-23 11:37:24 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.7011
2022-02-23 11:37:57 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.6536
2022-02-23 11:38:31 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.6719
2022-02-23 11:39:03 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.7697
2022-02-23 11:39:36 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.7235
2022-02-23 11:40:08 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.7418
2022-02-23 11:40:42 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.7509
2022-02-23 11:41:16 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.6018
2022-02-23 11:41:49 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.7952
2022-02-23 11:42:23 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.6004
2022-02-23 11:42:56 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.4878
2022-02-23 11:43:30 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.6730
2022-02-23 11:44:04 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.5548
2022-02-23 11:44:05 - train: epoch 056, train_loss: 1.6342
2022-02-23 11:45:20 - eval: epoch: 056, acc1: 65.346%, acc5: 86.974%, test_loss: 1.4130, per_image_load_time: 2.629ms, per_image_inference_time: 0.295ms
2022-02-23 11:45:20 - until epoch: 056, best_acc1: 65.970%
2022-02-23 11:45:20 - epoch 057 lr: 0.010000000000000002
2022-02-23 11:45:59 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.5754
2022-02-23 11:46:33 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.5645
2022-02-23 11:47:06 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.5231
2022-02-23 11:47:40 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.5449
2022-02-23 11:48:14 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.5229
2022-02-23 11:48:48 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.6797
2022-02-23 11:49:22 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.5037
2022-02-23 11:49:55 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.6230
2022-02-23 11:50:28 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.6492
2022-02-23 11:51:03 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.3853
2022-02-23 11:51:36 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.5666
2022-02-23 11:52:10 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.6021
2022-02-23 11:52:43 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.6396
2022-02-23 11:53:17 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.6107
2022-02-23 11:53:50 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.6658
2022-02-23 11:54:23 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.8541
2022-02-23 11:54:57 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.7590
2022-02-23 11:55:31 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.8126
2022-02-23 11:56:04 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.7084
2022-02-23 11:56:38 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.6520
2022-02-23 11:57:11 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.5247
2022-02-23 11:57:44 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.5564
2022-02-23 11:58:19 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.7922
2022-02-23 11:58:52 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.6424
2022-02-23 11:59:25 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.8626
2022-02-23 11:59:59 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.5447
2022-02-23 12:00:33 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.4733
2022-02-23 12:01:06 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.3425
2022-02-23 12:01:40 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.8225
2022-02-23 12:02:13 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.8720
2022-02-23 12:02:47 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.5817
2022-02-23 12:03:21 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.7326
2022-02-23 12:03:55 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.7279
2022-02-23 12:04:28 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.6102
2022-02-23 12:05:02 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.8015
2022-02-23 12:05:35 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.6888
2022-02-23 12:06:10 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.6086
2022-02-23 12:06:44 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.6826
2022-02-23 12:07:17 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.8015
2022-02-23 12:07:50 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.5539
2022-02-23 12:08:24 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.7334
2022-02-23 12:08:57 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.6145
2022-02-23 12:09:31 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.4454
2022-02-23 12:10:05 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.7346
2022-02-23 12:10:38 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.7501
2022-02-23 12:11:09 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.5234
2022-02-23 12:11:41 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.6131
2022-02-23 12:12:17 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.7182
2022-02-23 12:12:51 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.7614
2022-02-23 12:13:24 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.7737
2022-02-23 12:13:25 - train: epoch 057, train_loss: 1.6332
2022-02-23 12:14:40 - eval: epoch: 057, acc1: 65.396%, acc5: 86.978%, test_loss: 1.4071, per_image_load_time: 2.407ms, per_image_inference_time: 0.299ms
2022-02-23 12:14:40 - until epoch: 057, best_acc1: 65.970%
2022-02-23 12:14:40 - epoch 058 lr: 0.010000000000000002
2022-02-23 12:15:19 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.6585
2022-02-23 12:15:52 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.4709
2022-02-23 12:16:26 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.4897
2022-02-23 12:17:00 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.5981
2022-02-23 12:17:33 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.5753
2022-02-23 12:18:07 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.9322
2022-02-23 12:18:42 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.8246
2022-02-23 12:19:14 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.5365
2022-02-23 12:19:48 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.5424
2022-02-23 12:20:22 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.7456
2022-02-23 12:20:55 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.4800
2022-02-23 12:21:30 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.5047
2022-02-23 12:22:03 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 2.0139
2022-02-23 12:22:36 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.7363
2022-02-23 12:23:10 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.5664
2022-02-23 12:23:43 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.5620
2022-02-23 12:24:18 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.8086
2022-02-23 12:24:51 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.8735
2022-02-23 12:25:25 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.7601
2022-02-23 12:25:58 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.8437
2022-02-23 12:26:32 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.5083
2022-02-23 12:27:06 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.5375
2022-02-23 12:27:39 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.5809
2022-02-23 12:28:13 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.5240
2022-02-23 12:28:46 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.6270
2022-02-23 12:29:19 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.5536
2022-02-23 12:29:54 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.6897
2022-02-23 12:30:27 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.6455
2022-02-23 12:31:00 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.5819
2022-02-23 12:31:33 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.7076
2022-02-23 12:32:07 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.7321
2022-02-23 12:32:41 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.3887
2022-02-23 12:33:15 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.6414
2022-02-23 12:33:48 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.5505
2022-02-23 12:34:21 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.5038
2022-02-23 12:34:55 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.5813
2022-02-23 12:35:28 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.4802
2022-02-23 12:36:01 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.6168
2022-02-23 12:36:35 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.5487
2022-02-23 12:37:09 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.6625
2022-02-23 12:37:43 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.5376
2022-02-23 12:38:16 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.5443
2022-02-23 12:38:50 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.7119
2022-02-23 12:39:23 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.5963
2022-02-23 12:39:57 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.4779
2022-02-23 12:40:31 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.5232
2022-02-23 12:41:05 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.5915
2022-02-23 12:41:38 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.6966
2022-02-23 12:42:12 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.2825
2022-02-23 12:42:44 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.4379
2022-02-23 12:42:45 - train: epoch 058, train_loss: 1.6310
2022-02-23 12:43:59 - eval: epoch: 058, acc1: 65.798%, acc5: 87.196%, test_loss: 1.3944, per_image_load_time: 2.525ms, per_image_inference_time: 0.228ms
2022-02-23 12:43:59 - until epoch: 058, best_acc1: 65.970%
2022-02-23 12:43:59 - epoch 059 lr: 0.010000000000000002
2022-02-23 12:44:38 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.4039
2022-02-23 12:45:12 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.4435
2022-02-23 12:45:45 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.4140
2022-02-23 12:46:18 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.7726
2022-02-23 12:46:52 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.6298
2022-02-23 12:47:26 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.4698
2022-02-23 12:47:59 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.4386
2022-02-23 12:48:32 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.7034
2022-02-23 12:49:06 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.5628
2022-02-23 12:49:39 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.8316
2022-02-23 12:50:13 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.9048
2022-02-23 12:50:47 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.5964
2022-02-23 12:51:20 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.9770
2022-02-23 12:51:54 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.7348
2022-02-23 12:52:27 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.5880
2022-02-23 12:53:02 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.5324
2022-02-23 12:53:35 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.7834
2022-02-23 12:54:08 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.5527
2022-02-23 12:54:42 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.4973
2022-02-23 12:55:16 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.3885
2022-02-23 12:55:50 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.6283
2022-02-23 12:56:23 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.6540
2022-02-23 12:56:56 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.5774
2022-02-23 12:57:31 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.7679
2022-02-23 12:58:03 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.6403
2022-02-23 12:58:38 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.5608
2022-02-23 12:59:11 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.6289
2022-02-23 12:59:45 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.7953
2022-02-23 13:00:17 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.5439
2022-02-23 13:00:51 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.8388
2022-02-23 13:01:25 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.6808
2022-02-23 13:01:58 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.9288
2022-02-23 13:02:33 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.5269
2022-02-23 13:03:06 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.9916
2022-02-23 13:03:41 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.4986
2022-02-23 13:04:14 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.8674
2022-02-23 13:04:48 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.7548
2022-02-23 13:05:21 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.6280
2022-02-23 13:05:56 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.3815
2022-02-23 13:06:29 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.7605
2022-02-23 13:07:03 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.4645
2022-02-23 13:07:36 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.6648
2022-02-23 13:08:12 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.5957
2022-02-23 13:08:45 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.5518
2022-02-23 13:09:18 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.6541
2022-02-23 13:09:53 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.7105
2022-02-23 13:10:27 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.6506
2022-02-23 13:11:00 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.4088
2022-02-23 13:11:35 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.6236
2022-02-23 13:12:07 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.8120
2022-02-23 13:12:08 - train: epoch 059, train_loss: 1.6291
2022-02-23 13:13:23 - eval: epoch: 059, acc1: 65.250%, acc5: 86.874%, test_loss: 1.4095, per_image_load_time: 1.803ms, per_image_inference_time: 0.281ms
2022-02-23 13:13:23 - until epoch: 059, best_acc1: 65.970%
2022-02-23 13:13:23 - epoch 060 lr: 0.010000000000000002
2022-02-23 13:14:01 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.6510
2022-02-23 13:14:33 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.5967
2022-02-23 13:15:06 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.5227
2022-02-23 13:15:38 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.6934
2022-02-23 13:16:12 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.7817
2022-02-23 13:16:45 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.8730
2022-02-23 13:17:19 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.5895
2022-02-23 13:17:52 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.7896
2022-02-23 13:18:27 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.4850
2022-02-23 13:19:01 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.3601
2022-02-23 13:19:34 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.5741
2022-02-23 13:20:08 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.5916
2022-02-23 13:20:42 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.5200
2022-02-23 13:21:16 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.7892
2022-02-23 13:21:50 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.6583
2022-02-23 13:22:23 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.5532
2022-02-23 13:22:57 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.5357
2022-02-23 13:23:31 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.7803
2022-02-23 13:24:04 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.7339
2022-02-23 13:24:39 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.7386
2022-02-23 13:25:13 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.7543
2022-02-23 13:25:47 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.6046
2022-02-23 13:26:20 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.7020
2022-02-23 13:26:54 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.5905
2022-02-23 13:27:27 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.7103
2022-02-23 13:28:02 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.5709
2022-02-23 13:28:35 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.6427
2022-02-23 13:29:08 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.6900
2022-02-23 13:29:42 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.7245
2022-02-23 13:30:16 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.6925
2022-02-23 13:30:50 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.8318
2022-02-23 13:31:23 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.5965
2022-02-23 13:31:57 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.4045
2022-02-23 13:32:30 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.7649
2022-02-23 13:33:04 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.7515
2022-02-23 13:33:37 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.6184
2022-02-23 13:34:10 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.6450
2022-02-23 13:34:43 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.5638
2022-02-23 13:35:18 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.8258
2022-02-23 13:35:51 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.7238
2022-02-23 13:36:24 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.8976
2022-02-23 13:36:59 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.5954
2022-02-23 13:37:32 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.4881
2022-02-23 13:38:06 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.8789
2022-02-23 13:38:40 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.7257
2022-02-23 13:39:13 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.5522
2022-02-23 13:39:47 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.4598
2022-02-23 13:40:21 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.5471
2022-02-23 13:40:54 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.7793
2022-02-23 13:41:28 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.7015
2022-02-23 13:41:29 - train: epoch 060, train_loss: 1.6264
2022-02-23 13:42:44 - eval: epoch: 060, acc1: 65.526%, acc5: 86.978%, test_loss: 1.4010, per_image_load_time: 2.619ms, per_image_inference_time: 0.266ms
2022-02-23 13:42:44 - until epoch: 060, best_acc1: 65.970%
2022-02-23 13:42:44 - epoch 061 lr: 0.0010000000000000002
2022-02-23 13:43:22 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.3186
2022-02-23 13:43:56 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.4382
2022-02-23 13:44:30 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.3741
2022-02-23 13:45:03 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.6864
2022-02-23 13:45:37 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.2440
2022-02-23 13:46:10 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.5181
2022-02-23 13:46:42 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.3863
2022-02-23 13:47:13 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.4386
2022-02-23 13:47:47 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.6396
2022-02-23 13:48:20 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.3668
2022-02-23 13:48:55 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.2675
2022-02-23 13:49:28 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.2870
2022-02-23 13:50:01 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.4966
2022-02-23 13:50:34 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.2738
2022-02-23 13:51:08 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.2926
2022-02-23 13:51:41 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.1964
2022-02-23 13:52:14 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.5196
2022-02-23 13:52:48 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.3603
2022-02-23 13:53:22 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.5744
2022-02-23 13:53:55 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.2957
2022-02-23 13:54:30 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.5196
2022-02-23 13:55:03 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.4433
2022-02-23 13:55:36 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.4004
2022-02-23 13:56:10 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.4370
2022-02-23 13:56:43 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.3243
2022-02-23 13:57:17 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.5122
2022-02-23 13:57:51 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.6650
2022-02-23 13:58:24 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.3990
2022-02-23 13:58:58 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.5277
2022-02-23 13:59:32 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.3929
2022-02-23 14:00:06 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.5789
2022-02-23 14:00:40 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.3854
2022-02-23 14:01:14 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.3594
2022-02-23 14:01:47 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.2522
2022-02-23 14:02:21 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.5661
2022-02-23 14:02:55 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.4885
2022-02-23 14:03:29 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.4626
2022-02-23 14:04:02 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.4960
2022-02-23 14:04:36 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.5049
2022-02-23 14:05:10 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.4220
2022-02-23 14:05:44 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.5476
2022-02-23 14:06:17 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.4377
2022-02-23 14:06:51 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.4008
2022-02-23 14:07:25 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.3822
2022-02-23 14:07:59 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.4389
2022-02-23 14:08:32 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.3821
2022-02-23 14:09:06 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.4787
2022-02-23 14:09:41 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.4531
2022-02-23 14:10:14 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.3745
2022-02-23 14:10:48 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.4674
2022-02-23 14:10:49 - train: epoch 061, train_loss: 1.4303
2022-02-23 14:12:04 - eval: epoch: 061, acc1: 69.824%, acc5: 89.456%, test_loss: 1.2077, per_image_load_time: 1.876ms, per_image_inference_time: 0.279ms
2022-02-23 14:12:04 - until epoch: 061, best_acc1: 69.824%
2022-02-23 14:12:04 - epoch 062 lr: 0.0010000000000000002
2022-02-23 14:12:43 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.4207
2022-02-23 14:13:17 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.6443
2022-02-23 14:13:51 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.4084
2022-02-23 14:14:24 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.1559
2022-02-23 14:14:58 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.3130
2022-02-23 14:15:32 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.3537
2022-02-23 14:16:05 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.2743
2022-02-23 14:16:38 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.4020
2022-02-23 14:17:11 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.4829
2022-02-23 14:17:44 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.5031
2022-02-23 14:18:17 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.4757
2022-02-23 14:18:50 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.4768
2022-02-23 14:19:22 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.3214
2022-02-23 14:19:56 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.3128
2022-02-23 14:20:30 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.3788
2022-02-23 14:21:03 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.6063
2022-02-23 14:21:37 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.3570
2022-02-23 14:22:10 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.2805
2022-02-23 14:22:44 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.1723
2022-02-23 14:23:18 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.5197
2022-02-23 14:23:51 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.5502
2022-02-23 14:24:25 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.1510
2022-02-23 14:24:58 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.4684
2022-02-23 14:25:32 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.3716
2022-02-23 14:26:06 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.3990
2022-02-23 14:26:39 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.2061
2022-02-23 14:27:13 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.3766
2022-02-23 14:27:46 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.4951
2022-02-23 14:28:20 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.2509
2022-02-23 14:28:54 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.5507
2022-02-23 14:29:27 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.3915
2022-02-23 14:30:00 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.4023
2022-02-23 14:30:34 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.3922
2022-02-23 14:31:09 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.4100
2022-02-23 14:31:42 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.4297
2022-02-23 14:32:16 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.4771
2022-02-23 14:32:49 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.4668
2022-02-23 14:33:23 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.3035
2022-02-23 14:33:57 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.3839
2022-02-23 14:34:30 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.4135
2022-02-23 14:35:03 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.4349
2022-02-23 14:35:37 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.2223
2022-02-23 14:36:10 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.3898
2022-02-23 14:36:44 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.2353
2022-02-23 14:37:18 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.4090
2022-02-23 14:37:51 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.1625
2022-02-23 14:38:25 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.2751
2022-02-23 14:38:59 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.1849
2022-02-23 14:39:33 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.3218
2022-02-23 14:40:07 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.6080
2022-02-23 14:40:08 - train: epoch 062, train_loss: 1.3817
2022-02-23 14:41:23 - eval: epoch: 062, acc1: 70.220%, acc5: 89.780%, test_loss: 1.1890, per_image_load_time: 2.524ms, per_image_inference_time: 0.276ms
2022-02-23 14:41:23 - until epoch: 062, best_acc1: 70.220%
2022-02-23 14:41:23 - epoch 063 lr: 0.0010000000000000002
2022-02-23 14:42:01 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.3414
2022-02-23 14:42:35 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.2857
2022-02-23 14:43:09 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.4094
2022-02-23 14:43:42 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.5793
2022-02-23 14:44:16 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.2460
2022-02-23 14:44:49 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.5291
2022-02-23 14:45:22 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.4895
2022-02-23 14:45:56 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.4217
2022-02-23 14:46:30 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.4035
2022-02-23 14:47:03 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.5008
2022-02-23 14:47:37 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.2727
2022-02-23 14:48:10 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.3965
2022-02-23 14:48:44 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.0969
2022-02-23 14:49:17 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.4987
2022-02-23 14:49:50 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.3932
2022-02-23 14:50:22 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.3739
2022-02-23 14:50:54 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.1978
2022-02-23 14:51:28 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.3560
2022-02-23 14:52:01 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.5869
2022-02-23 14:52:35 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.2524
2022-02-23 14:53:08 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.3724
2022-02-23 14:53:43 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.5974
2022-02-23 14:54:16 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.6299
2022-02-23 14:54:50 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.3851
2022-02-23 14:55:23 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.4004
2022-02-23 14:55:58 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.3165
2022-02-23 14:56:31 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.6150
2022-02-23 14:57:05 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.2859
2022-02-23 14:57:39 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.4785
2022-02-23 14:58:13 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.6306
2022-02-23 14:58:47 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.4297
2022-02-23 14:59:20 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.4888
2022-02-23 14:59:54 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.1942
2022-02-23 15:00:27 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.3574
2022-02-23 15:01:01 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.4547
2022-02-23 15:01:35 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.2955
2022-02-23 15:02:09 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.5523
2022-02-23 15:02:43 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.2643
2022-02-23 15:03:16 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.2657
2022-02-23 15:03:50 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.2114
2022-02-23 15:04:23 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.4027
2022-02-23 15:04:57 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.3835
2022-02-23 15:05:31 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.4892
2022-02-23 15:06:05 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.3269
2022-02-23 15:06:38 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.4315
2022-02-23 15:07:12 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.2068
2022-02-23 15:07:46 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.2974
2022-02-23 15:08:21 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.1413
2022-02-23 15:08:55 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.4028
2022-02-23 15:09:27 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.3503
2022-02-23 15:09:28 - train: epoch 063, train_loss: 1.3636
2022-02-23 15:10:43 - eval: epoch: 063, acc1: 70.546%, acc5: 89.900%, test_loss: 1.1811, per_image_load_time: 2.426ms, per_image_inference_time: 0.292ms
2022-02-23 15:10:44 - until epoch: 063, best_acc1: 70.546%
2022-02-23 15:10:44 - epoch 064 lr: 0.0010000000000000002
2022-02-23 15:11:23 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.2301
2022-02-23 15:11:56 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.3395
2022-02-23 15:12:30 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.2191
2022-02-23 15:13:04 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.3859
2022-02-23 15:13:37 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.3097
2022-02-23 15:14:11 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.4827
2022-02-23 15:14:44 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.4353
2022-02-23 15:15:18 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.3470
2022-02-23 15:15:52 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.2324
2022-02-23 15:16:26 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.3972
2022-02-23 15:17:00 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.2750
2022-02-23 15:17:34 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.1589
2022-02-23 15:18:08 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.3931
2022-02-23 15:18:41 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.5849
2022-02-23 15:19:15 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.5030
2022-02-23 15:19:48 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.2879
2022-02-23 15:20:22 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.1908
2022-02-23 15:20:56 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.2174
2022-02-23 15:21:28 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.4378
2022-02-23 15:22:00 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.1915
2022-02-23 15:22:32 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.4828
2022-02-23 15:23:04 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.4523
2022-02-23 15:23:39 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.4323
2022-02-23 15:24:11 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.4576
2022-02-23 15:24:45 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.1258
2022-02-23 15:25:18 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.2487
2022-02-23 15:25:52 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.4195
2022-02-23 15:26:26 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.3961
2022-02-23 15:26:59 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.3293
2022-02-23 15:27:33 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.3904
2022-02-23 15:28:06 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.3238
2022-02-23 15:28:40 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.4208
2022-02-23 15:29:13 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.2218
2022-02-23 15:29:48 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.3721
2022-02-23 15:30:21 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.2748
2022-02-23 15:30:55 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.2023
2022-02-23 15:31:28 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.1302
2022-02-23 15:32:02 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.4159
2022-02-23 15:32:35 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.2110
2022-02-23 15:33:09 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.3334
2022-02-23 15:33:42 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.4152
2022-02-23 15:34:15 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.3611
2022-02-23 15:34:50 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.3772
2022-02-23 15:35:23 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.1747
2022-02-23 15:35:56 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.3739
2022-02-23 15:36:30 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.5279
2022-02-23 15:37:03 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.7615
2022-02-23 15:37:38 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.3942
2022-02-23 15:38:11 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.3282
2022-02-23 15:38:45 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.1500
2022-02-23 15:38:46 - train: epoch 064, train_loss: 1.3522
2022-02-23 15:40:01 - eval: epoch: 064, acc1: 70.606%, acc5: 89.998%, test_loss: 1.1728, per_image_load_time: 1.790ms, per_image_inference_time: 0.279ms
2022-02-23 15:40:01 - until epoch: 064, best_acc1: 70.606%
2022-02-23 15:40:01 - epoch 065 lr: 0.0010000000000000002
2022-02-23 15:40:41 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.3567
2022-02-23 15:41:14 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.3358
2022-02-23 15:41:47 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.3185
2022-02-23 15:42:21 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.3595
2022-02-23 15:42:54 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.5820
2022-02-23 15:43:29 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.4597
2022-02-23 15:44:02 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.5900
2022-02-23 15:44:35 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.3993
2022-02-23 15:45:09 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.3513
2022-02-23 15:45:43 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.3466
2022-02-23 15:46:16 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.1783
2022-02-23 15:46:51 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.7095
2022-02-23 15:47:25 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.3455
2022-02-23 15:47:59 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.3388
2022-02-23 15:48:33 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.2995
2022-02-23 15:49:07 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.3967
2022-02-23 15:49:40 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.5663
2022-02-23 15:50:14 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.3234
2022-02-23 15:50:49 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.2190
2022-02-23 15:51:22 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.4368
2022-02-23 15:51:56 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.2041
2022-02-23 15:52:30 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.3881
2022-02-23 15:53:03 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.3600
2022-02-23 15:53:36 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.2865
2022-02-23 15:54:07 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.3466
2022-02-23 15:54:40 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.5537
2022-02-23 15:55:14 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.3729
2022-02-23 15:55:48 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.3554
2022-02-23 15:56:21 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.2080
2022-02-23 15:56:55 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.3199
2022-02-23 15:57:28 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.3172
2022-02-23 15:58:01 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.4025
2022-02-23 15:58:35 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.2511
2022-02-23 15:59:09 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.2326
2022-02-23 15:59:42 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.7620
2022-02-23 16:00:16 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.3015
2022-02-23 16:00:49 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.2882
2022-02-23 16:01:23 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.4101
2022-02-23 16:01:57 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.4550
2022-02-23 16:02:30 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.4630
2022-02-23 16:03:04 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.2813
2022-02-23 16:03:37 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.2769
2022-02-23 16:04:11 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.4683
2022-02-23 16:04:45 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.3447
2022-02-23 16:05:19 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.3555
2022-02-23 16:05:53 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.3189
2022-02-23 16:06:26 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.3673
2022-02-23 16:07:01 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.2636
2022-02-23 16:07:35 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.4194
2022-02-23 16:08:07 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.2845
2022-02-23 16:08:08 - train: epoch 065, train_loss: 1.3409
2022-02-23 16:09:23 - eval: epoch: 065, acc1: 70.706%, acc5: 90.066%, test_loss: 1.1713, per_image_load_time: 2.640ms, per_image_inference_time: 0.257ms
2022-02-23 16:09:23 - until epoch: 065, best_acc1: 70.706%
2022-02-23 16:09:23 - epoch 066 lr: 0.0010000000000000002
2022-02-23 16:10:03 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.2919
2022-02-23 16:10:36 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.4976
2022-02-23 16:11:10 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.2533
2022-02-23 16:11:43 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.2353
2022-02-23 16:12:17 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.3805
2022-02-23 16:12:51 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.2409
2022-02-23 16:13:24 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.4114
2022-02-23 16:13:58 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.3020
2022-02-23 16:14:32 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.3595
2022-02-23 16:15:06 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.1989
2022-02-23 16:15:39 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3820
2022-02-23 16:16:13 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.5271
2022-02-23 16:16:46 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.3485
2022-02-23 16:17:20 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.2870
2022-02-23 16:17:54 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.3719
2022-02-23 16:18:27 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.2532
2022-02-23 16:19:01 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.3451
2022-02-23 16:19:34 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.0594
2022-02-23 16:20:08 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.3882
2022-02-23 16:20:42 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.1039
2022-02-23 16:21:15 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.2410
2022-02-23 16:21:49 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.2543
2022-02-23 16:22:23 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.3823
2022-02-23 16:22:56 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.2962
2022-02-23 16:23:30 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.2687
2022-02-23 16:24:03 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.2704
2022-02-23 16:24:37 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.4945
2022-02-23 16:25:09 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.3969
2022-02-23 16:25:42 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.3071
2022-02-23 16:26:14 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.5150
2022-02-23 16:26:48 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.4990
2022-02-23 16:27:21 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.4012
2022-02-23 16:27:55 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.3994
2022-02-23 16:28:29 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.4531
2022-02-23 16:29:03 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.4831
2022-02-23 16:29:37 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.3212
2022-02-23 16:30:11 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.3755
2022-02-23 16:30:44 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.3447
2022-02-23 16:31:18 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.3436
2022-02-23 16:31:51 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.3079
2022-02-23 16:32:24 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.3470
2022-02-23 16:32:58 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.4158
2022-02-23 16:33:32 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.1982
2022-02-23 16:34:04 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.2375
2022-02-23 16:34:38 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.4586
2022-02-23 16:35:12 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.5255
2022-02-23 16:35:46 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.3106
2022-02-23 16:36:20 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.3177
2022-02-23 16:36:54 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.2543
2022-02-23 16:37:27 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.2856
2022-02-23 16:37:28 - train: epoch 066, train_loss: 1.3338
2022-02-23 16:38:43 - eval: epoch: 066, acc1: 70.842%, acc5: 89.974%, test_loss: 1.1664, per_image_load_time: 2.302ms, per_image_inference_time: 0.288ms
2022-02-23 16:38:44 - until epoch: 066, best_acc1: 70.842%
2022-02-23 16:38:44 - epoch 067 lr: 0.0010000000000000002
2022-02-23 16:39:22 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.3166
2022-02-23 16:39:56 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.2949
2022-02-23 16:40:30 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.2859
2022-02-23 16:41:03 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.3368
2022-02-23 16:41:37 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.2497
2022-02-23 16:42:11 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.2743
2022-02-23 16:42:45 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.4125
2022-02-23 16:43:19 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.2989
2022-02-23 16:43:53 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.5492
2022-02-23 16:44:25 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.4082
2022-02-23 16:45:00 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.4050
2022-02-23 16:45:33 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.3833
2022-02-23 16:46:06 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.2969
2022-02-23 16:46:41 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.4221
2022-02-23 16:47:14 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.1672
2022-02-23 16:47:47 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.3378
2022-02-23 16:48:20 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.3100
2022-02-23 16:48:55 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.3617
2022-02-23 16:49:29 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.2008
2022-02-23 16:50:03 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.4150
2022-02-23 16:50:37 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.0790
2022-02-23 16:51:11 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.2759
2022-02-23 16:51:44 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.2885
2022-02-23 16:52:18 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.1931
2022-02-23 16:52:52 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.5129
2022-02-23 16:53:25 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.2200
2022-02-23 16:53:59 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.3090
2022-02-23 16:54:33 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.4181
2022-02-23 16:55:07 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.2804
2022-02-23 16:55:40 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.0744
2022-02-23 16:56:13 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.3023
2022-02-23 16:56:46 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.3405
2022-02-23 16:57:18 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.2215
2022-02-23 16:57:50 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.4309
2022-02-23 16:58:23 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.3662
2022-02-23 16:58:58 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.2748
2022-02-23 16:59:31 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.4413
2022-02-23 17:00:04 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.4294
2022-02-23 17:00:38 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.4951
2022-02-23 17:01:12 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.3679
2022-02-23 17:01:45 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.3228
2022-02-23 17:02:19 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.3066
2022-02-23 17:02:52 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.2910
2022-02-23 17:03:25 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.1388
2022-02-23 17:03:59 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.3218
2022-02-23 17:04:33 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.0655
2022-02-23 17:05:08 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.4159
2022-02-23 17:05:41 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.0167
2022-02-23 17:06:15 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.3709
2022-02-23 17:06:48 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.3688
2022-02-23 17:06:49 - train: epoch 067, train_loss: 1.3288
2022-02-23 17:08:04 - eval: epoch: 067, acc1: 70.906%, acc5: 90.088%, test_loss: 1.1627, per_image_load_time: 1.198ms, per_image_inference_time: 0.287ms
2022-02-23 17:08:04 - until epoch: 067, best_acc1: 70.906%
2022-02-23 17:08:04 - epoch 068 lr: 0.0010000000000000002
2022-02-23 17:08:42 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.4045
2022-02-23 17:09:16 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.5571
2022-02-23 17:09:50 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.3648
2022-02-23 17:10:23 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.2636
2022-02-23 17:10:57 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.1737
2022-02-23 17:11:31 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.3586
2022-02-23 17:12:03 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.5246
2022-02-23 17:12:37 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.3154
2022-02-23 17:13:10 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.2362
2022-02-23 17:13:43 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.2033
2022-02-23 17:14:17 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.6806
2022-02-23 17:14:50 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.1557
2022-02-23 17:15:23 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.2301
2022-02-23 17:15:57 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.1918
2022-02-23 17:16:30 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.3421
2022-02-23 17:17:03 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.3205
2022-02-23 17:17:36 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.2337
2022-02-23 17:18:08 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.3545
2022-02-23 17:18:41 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.2965
2022-02-23 17:19:14 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.4573
2022-02-23 17:19:47 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.2693
2022-02-23 17:20:21 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.4311
2022-02-23 17:20:54 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.1724
2022-02-23 17:21:29 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.3119
2022-02-23 17:22:02 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.3548
2022-02-23 17:22:37 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.1598
2022-02-23 17:23:10 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.2563
2022-02-23 17:23:44 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.4850
2022-02-23 17:24:18 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.3235
2022-02-23 17:24:52 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.3112
2022-02-23 17:25:26 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.2031
2022-02-23 17:25:59 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.3673
2022-02-23 17:26:33 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.1627
2022-02-23 17:27:07 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.1392
2022-02-23 17:27:40 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.3419
2022-02-23 17:28:15 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.3163
2022-02-23 17:28:47 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.3416
2022-02-23 17:29:18 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.3311
2022-02-23 17:29:51 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.4667
2022-02-23 17:30:24 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.2391
2022-02-23 17:30:58 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.2648
2022-02-23 17:31:31 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.2530
2022-02-23 17:32:05 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.3326
2022-02-23 17:32:38 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.2768
2022-02-23 17:33:13 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.2527
2022-02-23 17:33:46 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.3973
2022-02-23 17:34:21 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.3508
2022-02-23 17:34:54 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.4353
2022-02-23 17:35:29 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.2658
2022-02-23 17:36:01 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.2901
2022-02-23 17:36:02 - train: epoch 068, train_loss: 1.3240
2022-02-23 17:37:17 - eval: epoch: 068, acc1: 70.956%, acc5: 90.182%, test_loss: 1.1586, per_image_load_time: 2.609ms, per_image_inference_time: 0.275ms
2022-02-23 17:37:18 - until epoch: 068, best_acc1: 70.956%
2022-02-23 17:37:18 - epoch 069 lr: 0.0010000000000000002
2022-02-23 17:37:57 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.4347
2022-02-23 17:38:29 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.3788
2022-02-23 17:39:03 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.3503
2022-02-23 17:39:37 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.2870
2022-02-23 17:40:11 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.4280
2022-02-23 17:40:44 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.2506
2022-02-23 17:41:18 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.4213
2022-02-23 17:41:52 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.5696
2022-02-23 17:42:25 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.0474
2022-02-23 17:42:59 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.2056
2022-02-23 17:43:32 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.1912
2022-02-23 17:44:06 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.2048
2022-02-23 17:44:39 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.4322
2022-02-23 17:45:13 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.4153
2022-02-23 17:45:47 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.4263
2022-02-23 17:46:21 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.3161
2022-02-23 17:46:54 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.2966
2022-02-23 17:47:29 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.3320
2022-02-23 17:48:02 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.2739
2022-02-23 17:48:36 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.2656
2022-02-23 17:49:10 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.3178
2022-02-23 17:49:43 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.3630
2022-02-23 17:50:16 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.2635
2022-02-23 17:50:50 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.3532
2022-02-23 17:51:24 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.1038
2022-02-23 17:51:57 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.2672
2022-02-23 17:52:31 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.3134
2022-02-23 17:53:05 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.3589
2022-02-23 17:53:39 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.1348
2022-02-23 17:54:13 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.4194
2022-02-23 17:54:47 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.3604
2022-02-23 17:55:20 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.2431
2022-02-23 17:55:55 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.2654
2022-02-23 17:56:29 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.3077
2022-02-23 17:57:03 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.1119
2022-02-23 17:57:36 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.0940
2022-02-23 17:58:10 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.4340
2022-02-23 17:58:44 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.2347
2022-02-23 17:59:17 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.4386
2022-02-23 17:59:50 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.4353
2022-02-23 18:00:24 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.2486
2022-02-23 18:00:58 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.2840
2022-02-23 18:01:31 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.3902
2022-02-23 18:02:06 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.3566
2022-02-23 18:02:40 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.4305
2022-02-23 18:03:13 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.5224
2022-02-23 18:03:47 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.3391
2022-02-23 18:04:21 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.4556
2022-02-23 18:04:55 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.3796
2022-02-23 18:05:27 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.1042
2022-02-23 18:05:29 - train: epoch 069, train_loss: 1.3155
2022-02-23 18:06:45 - eval: epoch: 069, acc1: 70.886%, acc5: 90.142%, test_loss: 1.1611, per_image_load_time: 2.687ms, per_image_inference_time: 0.244ms
2022-02-23 18:06:45 - until epoch: 069, best_acc1: 70.956%
2022-02-23 18:06:45 - epoch 070 lr: 0.0010000000000000002
2022-02-23 18:07:27 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.3496
2022-02-23 18:08:01 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.2164
2022-02-23 18:08:35 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.5454
2022-02-23 18:09:08 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.1774
2022-02-23 18:09:41 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.3922
2022-02-23 18:10:15 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.3258
2022-02-23 18:10:48 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.2429
2022-02-23 18:11:22 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.3458
2022-02-23 18:11:55 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.3688
2022-02-23 18:12:28 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.4106
2022-02-23 18:13:02 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.4607
2022-02-23 18:13:36 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.2763
2022-02-23 18:14:10 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.2573
2022-02-23 18:14:42 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.2889
2022-02-23 18:15:16 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.2877
2022-02-23 18:15:50 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.3369
2022-02-23 18:16:23 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.3012
2022-02-23 18:16:57 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.2173
2022-02-23 18:17:30 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.3696
2022-02-23 18:18:04 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.3035
2022-02-23 18:18:38 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.3338
2022-02-23 18:19:10 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.3130
2022-02-23 18:19:44 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.4212
2022-02-23 18:20:18 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.4323
2022-02-23 18:20:50 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.2766
2022-02-23 18:21:24 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.1435
2022-02-23 18:21:57 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.5523
2022-02-23 18:22:30 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.6377
2022-02-23 18:23:03 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.4140
2022-02-23 18:23:37 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.0995
2022-02-23 18:24:10 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.2549
2022-02-23 18:24:44 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.3589
2022-02-23 18:25:17 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.2679
2022-02-23 18:25:51 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.1985
2022-02-23 18:26:24 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.2316
2022-02-23 18:26:59 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.3061
2022-02-23 18:27:32 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.2984
2022-02-23 18:28:05 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.2157
2022-02-23 18:28:39 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.1175
2022-02-23 18:29:13 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.2943
2022-02-23 18:29:47 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.3101
2022-02-23 18:30:21 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.3737
2022-02-23 18:30:55 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.2370
2022-02-23 18:31:29 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.3811
2022-02-23 18:32:01 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.3079
2022-02-23 18:32:36 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.4128
2022-02-23 18:33:11 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.4541
2022-02-23 18:33:44 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.3077
2022-02-23 18:34:17 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.3768
2022-02-23 18:34:51 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.2576
2022-02-23 18:34:52 - train: epoch 070, train_loss: 1.3119
2022-02-23 18:36:09 - eval: epoch: 070, acc1: 71.048%, acc5: 90.260%, test_loss: 1.1559, per_image_load_time: 2.707ms, per_image_inference_time: 0.256ms
2022-02-23 18:36:09 - until epoch: 070, best_acc1: 71.048%
2022-02-23 18:36:09 - epoch 071 lr: 0.0010000000000000002
2022-02-23 18:36:48 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.1496
2022-02-23 18:37:22 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.2958
2022-02-23 18:37:55 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.4756
2022-02-23 18:38:27 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.3728
2022-02-23 18:39:01 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.5284
2022-02-23 18:39:34 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.3432
2022-02-23 18:40:08 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.2862
2022-02-23 18:40:42 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.1923
2022-02-23 18:41:16 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.3592
2022-02-23 18:41:49 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.2516
2022-02-23 18:42:24 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.4756
2022-02-23 18:42:58 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.2450
2022-02-23 18:43:32 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.2132
2022-02-23 18:44:04 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.2748
2022-02-23 18:44:39 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.2342
2022-02-23 18:45:13 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.3298
2022-02-23 18:45:47 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.1088
2022-02-23 18:46:21 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.4303
2022-02-23 18:46:54 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.2093
2022-02-23 18:47:29 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.5463
2022-02-23 18:48:02 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.2114
2022-02-23 18:48:36 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.0429
2022-02-23 18:49:09 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.1939
2022-02-23 18:49:43 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.1680
2022-02-23 18:50:17 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.5898
2022-02-23 18:50:52 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.1961
2022-02-23 18:51:25 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.3334
2022-02-23 18:51:59 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.3386
2022-02-23 18:52:33 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.2835
2022-02-23 18:53:07 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.3044
2022-02-23 18:53:40 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.2111
2022-02-23 18:54:13 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.1794
2022-02-23 18:54:47 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.4721
2022-02-23 18:55:20 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.0993
2022-02-23 18:55:55 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.4532
2022-02-23 18:56:29 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.3825
2022-02-23 18:57:03 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.3641
2022-02-23 18:57:38 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.4269
2022-02-23 18:58:11 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.3521
2022-02-23 18:58:45 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.2525
2022-02-23 18:59:19 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.2064
2022-02-23 18:59:53 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.2544
2022-02-23 19:00:26 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.0308
2022-02-23 19:00:59 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.4202
2022-02-23 19:01:34 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.4423
2022-02-23 19:02:08 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.3822
2022-02-23 19:02:42 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.3338
2022-02-23 19:03:16 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.2323
2022-02-23 19:03:51 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.3330
2022-02-23 19:04:23 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.1811
2022-02-23 19:04:25 - train: epoch 071, train_loss: 1.3066
2022-02-23 19:05:41 - eval: epoch: 071, acc1: 71.040%, acc5: 90.296%, test_loss: 1.1514, per_image_load_time: 2.681ms, per_image_inference_time: 0.261ms
2022-02-23 19:05:41 - until epoch: 071, best_acc1: 71.048%
2022-02-23 19:05:41 - epoch 072 lr: 0.0010000000000000002
2022-02-23 19:06:21 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.3126
2022-02-23 19:06:55 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.2776
2022-02-23 19:07:28 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.1761
2022-02-23 19:08:03 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.4372
2022-02-23 19:08:36 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.3205
2022-02-23 19:09:10 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.1881
2022-02-23 19:09:43 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.2473
2022-02-23 19:10:18 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.3813
2022-02-23 19:10:52 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.1620
2022-02-23 19:11:26 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.0662
2022-02-23 19:12:00 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.2534
2022-02-23 19:12:34 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.1963
2022-02-23 19:13:08 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.4613
2022-02-23 19:13:41 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.3389
2022-02-23 19:14:15 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.3760
2022-02-23 19:14:48 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.2540
2022-02-23 19:15:22 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.1916
2022-02-23 19:15:55 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.3126
2022-02-23 19:16:29 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.2925
2022-02-23 19:17:02 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.2503
2022-02-23 19:17:36 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.5921
2022-02-23 19:18:10 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.3979
2022-02-23 19:18:45 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.6072
2022-02-23 19:19:18 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.1578
2022-02-23 19:19:52 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.4075
2022-02-23 19:20:26 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.2982
2022-02-23 19:21:00 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.4239
2022-02-23 19:21:33 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.3796
2022-02-23 19:22:06 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.1020
2022-02-23 19:22:40 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.2660
2022-02-23 19:23:14 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.3566
2022-02-23 19:23:48 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.4047
2022-02-23 19:24:21 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.3104
2022-02-23 19:24:55 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.3598
2022-02-23 19:25:28 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.3248
2022-02-23 19:26:02 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.1482
2022-02-23 19:26:35 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.4903
2022-02-23 19:27:09 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.1324
2022-02-23 19:27:42 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.3254
2022-02-23 19:28:16 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.3219
2022-02-23 19:28:49 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.4494
2022-02-23 19:29:23 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.2962
2022-02-23 19:29:57 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.3486
2022-02-23 19:30:31 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.2000
2022-02-23 19:31:04 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.5022
2022-02-23 19:31:38 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.2256
2022-02-23 19:32:11 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.3537
2022-02-23 19:32:46 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.3581
2022-02-23 19:33:19 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.3797
2022-02-23 19:33:52 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.3265
2022-02-23 19:33:54 - train: epoch 072, train_loss: 1.3042
2022-02-23 19:35:10 - eval: epoch: 072, acc1: 71.014%, acc5: 90.284%, test_loss: 1.1524, per_image_load_time: 2.080ms, per_image_inference_time: 0.278ms
2022-02-23 19:35:10 - until epoch: 072, best_acc1: 71.048%
2022-02-23 19:35:10 - epoch 073 lr: 0.0010000000000000002
2022-02-23 19:35:50 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.6185
2022-02-23 19:36:23 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.3547
2022-02-23 19:36:58 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.4229
2022-02-23 19:37:31 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.1028
2022-02-23 19:38:05 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.2573
2022-02-23 19:38:40 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.2690
2022-02-23 19:39:13 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.2751
2022-02-23 19:39:47 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.2291
2022-02-23 19:40:20 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.1731
2022-02-23 19:40:53 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.1269
2022-02-23 19:41:27 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.4337
2022-02-23 19:42:00 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.4038
2022-02-23 19:42:32 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.1776
2022-02-23 19:43:06 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.1231
2022-02-23 19:43:40 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.3185
2022-02-23 19:44:12 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.3424
2022-02-23 19:44:46 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.5980
2022-02-23 19:45:19 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.2852
2022-02-23 19:45:53 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.3658
2022-02-23 19:46:27 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.2563
2022-02-23 19:47:00 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.3166
2022-02-23 19:47:33 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.2445
2022-02-23 19:48:06 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.2747
2022-02-23 19:48:40 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.4008
2022-02-23 19:49:14 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.1554
2022-02-23 19:49:47 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.2078
2022-02-23 19:50:21 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.3002
2022-02-23 19:50:55 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.1765
2022-02-23 19:51:29 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.4504
2022-02-23 19:52:02 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.0594
2022-02-23 19:52:36 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.3041
2022-02-23 19:53:09 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.1922
2022-02-23 19:53:43 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.3281
2022-02-23 19:54:16 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.2373
2022-02-23 19:54:50 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.2570
2022-02-23 19:55:24 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.3204
2022-02-23 19:55:58 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.1803
2022-02-23 19:56:31 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.5653
2022-02-23 19:57:04 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.3068
2022-02-23 19:57:37 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.3894
2022-02-23 19:58:10 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.2708
2022-02-23 19:58:43 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.3226
2022-02-23 19:59:17 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.1208
2022-02-23 19:59:50 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.5374
2022-02-23 20:00:24 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.3410
2022-02-23 20:00:58 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.3689
2022-02-23 20:01:31 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.1409
2022-02-23 20:02:05 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.5048
2022-02-23 20:02:39 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.2112
2022-02-23 20:03:10 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.3600
2022-02-23 20:03:12 - train: epoch 073, train_loss: 1.2990
2022-02-23 20:04:28 - eval: epoch: 073, acc1: 70.982%, acc5: 90.182%, test_loss: 1.1525, per_image_load_time: 1.768ms, per_image_inference_time: 0.285ms
2022-02-23 20:04:28 - until epoch: 073, best_acc1: 71.048%
2022-02-23 20:04:28 - epoch 074 lr: 0.0010000000000000002
2022-02-23 20:05:07 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.1873
2022-02-23 20:05:42 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.2750
2022-02-23 20:06:15 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.1787
2022-02-23 20:06:48 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.5782
2022-02-23 20:07:22 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2708
2022-02-23 20:07:56 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.2800
2022-02-23 20:08:30 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.0987
2022-02-23 20:09:03 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.2790
2022-02-23 20:09:36 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.3113
2022-02-23 20:10:11 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.3804
2022-02-23 20:10:44 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.2258
2022-02-23 20:11:18 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.2657
2022-02-23 20:11:53 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.2401
2022-02-23 20:12:26 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.1845
2022-02-23 20:13:00 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.3109
2022-02-23 20:13:33 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.3761
2022-02-23 20:14:07 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.3585
2022-02-23 20:14:40 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.3997
2022-02-23 20:15:14 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.3131
2022-02-23 20:15:46 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.9920
2022-02-23 20:16:19 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.2880
2022-02-23 20:16:52 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.5295
2022-02-23 20:17:26 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.2739
2022-02-23 20:18:00 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.3946
2022-02-23 20:18:32 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.3635
2022-02-23 20:19:06 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.0965
2022-02-23 20:19:41 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.2298
2022-02-23 20:20:14 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.4630
2022-02-23 20:20:47 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.2405
2022-02-23 20:21:21 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.3491
2022-02-23 20:21:55 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.4055
2022-02-23 20:22:28 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.4186
2022-02-23 20:23:02 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.2798
2022-02-23 20:23:36 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.3817
2022-02-23 20:24:09 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.1023
2022-02-23 20:24:43 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.3243
2022-02-23 20:25:17 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.3026
2022-02-23 20:25:51 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.2034
2022-02-23 20:26:25 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.1039
2022-02-23 20:26:58 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.3792
2022-02-23 20:27:33 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.3606
2022-02-23 20:28:06 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.3098
2022-02-23 20:28:40 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.2591
2022-02-23 20:29:13 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.1207
2022-02-23 20:29:48 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.1409
2022-02-23 20:30:21 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.3766
2022-02-23 20:30:55 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.0304
2022-02-23 20:31:29 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.2931
2022-02-23 20:32:02 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.5234
2022-02-23 20:32:35 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.2665
2022-02-23 20:32:37 - train: epoch 074, train_loss: 1.2956
2022-02-23 20:33:53 - eval: epoch: 074, acc1: 71.154%, acc5: 90.264%, test_loss: 1.1481, per_image_load_time: 1.592ms, per_image_inference_time: 0.265ms
2022-02-23 20:33:53 - until epoch: 074, best_acc1: 71.154%
2022-02-23 20:33:53 - epoch 075 lr: 0.0010000000000000002
2022-02-23 20:34:32 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.5067
2022-02-23 20:35:06 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.1687
2022-02-23 20:35:40 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.1699
2022-02-23 20:36:14 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.1464
2022-02-23 20:36:48 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.3848
2022-02-23 20:37:21 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.3376
2022-02-23 20:37:55 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.6132
2022-02-23 20:38:29 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.3735
2022-02-23 20:39:02 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.4402
2022-02-23 20:39:36 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.1352
2022-02-23 20:40:11 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.4104
2022-02-23 20:40:44 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.2732
2022-02-23 20:41:17 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.1390
2022-02-23 20:41:51 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.1264
2022-02-23 20:42:26 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.3469
2022-02-23 20:42:59 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.1903
2022-02-23 20:43:33 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.2972
2022-02-23 20:44:06 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.3336
2022-02-23 20:44:40 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.1427
2022-02-23 20:45:13 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.2985
2022-02-23 20:45:47 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.3041
2022-02-23 20:46:21 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.2289
2022-02-23 20:46:55 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 0.9952
2022-02-23 20:47:29 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.3769
2022-02-23 20:48:01 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.3447
2022-02-23 20:48:34 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.3931
2022-02-23 20:49:08 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.2791
2022-02-23 20:49:40 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.3376
2022-02-23 20:50:14 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.5754
2022-02-23 20:50:48 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.4338
2022-02-23 20:51:22 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.4027
2022-02-23 20:51:55 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.4515
2022-02-23 20:52:29 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.4490
2022-02-23 20:53:03 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.3141
2022-02-23 20:53:37 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.2302
2022-02-23 20:54:11 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.4126
2022-02-23 20:54:45 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.4256
2022-02-23 20:55:19 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.2034
2022-02-23 20:55:52 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.3614
2022-02-23 20:56:25 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.1393
2022-02-23 20:57:00 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.2358
2022-02-23 20:57:33 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.1472
2022-02-23 20:58:07 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.4943
2022-02-23 20:58:40 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.0849
2022-02-23 20:59:15 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.2798
2022-02-23 20:59:49 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.1194
2022-02-23 21:00:23 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.4355
2022-02-23 21:00:56 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.3462
2022-02-23 21:01:30 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.2011
2022-02-23 21:02:03 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.1965
2022-02-23 21:02:04 - train: epoch 075, train_loss: 1.2937
2022-02-23 21:03:20 - eval: epoch: 075, acc1: 71.024%, acc5: 90.184%, test_loss: 1.1527, per_image_load_time: 1.901ms, per_image_inference_time: 0.272ms
2022-02-23 21:03:21 - until epoch: 075, best_acc1: 71.154%
2022-02-23 21:03:21 - epoch 076 lr: 0.0010000000000000002
2022-02-23 21:04:00 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.2395
2022-02-23 21:04:33 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.3992
2022-02-23 21:05:07 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.3477
2022-02-23 21:05:41 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.3066
2022-02-23 21:06:15 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.1189
2022-02-23 21:06:49 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.2868
2022-02-23 21:07:23 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.4091
2022-02-23 21:07:56 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.4334
2022-02-23 21:08:31 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.1641
2022-02-23 21:09:04 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.3581
2022-02-23 21:09:38 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.3365
2022-02-23 21:10:11 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.3820
2022-02-23 21:10:45 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.4465
2022-02-23 21:11:19 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.4999
2022-02-23 21:11:52 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.2988
2022-02-23 21:12:26 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.2978
2022-02-23 21:13:00 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.3646
2022-02-23 21:13:32 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.1876
2022-02-23 21:14:06 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.3895
2022-02-23 21:14:40 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.3471
2022-02-23 21:15:13 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.2742
2022-02-23 21:15:47 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.3415
2022-02-23 21:16:21 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.2072
2022-02-23 21:16:54 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.4416
2022-02-23 21:17:28 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.2585
2022-02-23 21:18:01 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.3544
2022-02-23 21:18:35 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.3244
2022-02-23 21:19:09 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.1444
2022-02-23 21:19:42 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.3684
2022-02-23 21:20:14 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.1358
2022-02-23 21:20:46 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.3752
2022-02-23 21:21:17 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.4339
2022-02-23 21:21:49 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.3207
2022-02-23 21:22:22 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.2767
2022-02-23 21:22:55 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.3231
2022-02-23 21:23:29 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.3750
2022-02-23 21:24:03 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.1853
2022-02-23 21:24:36 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.1295
2022-02-23 21:25:10 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.0379
2022-02-23 21:25:43 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.2230
2022-02-23 21:26:18 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.3179
2022-02-23 21:26:51 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.3613
2022-02-23 21:27:25 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.4442
2022-02-23 21:27:58 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.3020
2022-02-23 21:28:31 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.3194
2022-02-23 21:29:05 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.2881
2022-02-23 21:29:40 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.3280
2022-02-23 21:30:13 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.2108
2022-02-23 21:30:47 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.1307
2022-02-23 21:31:19 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.3769
2022-02-23 21:31:20 - train: epoch 076, train_loss: 1.2906
2022-02-23 21:32:36 - eval: epoch: 076, acc1: 71.152%, acc5: 90.382%, test_loss: 1.1457, per_image_load_time: 1.012ms, per_image_inference_time: 0.235ms
2022-02-23 21:32:36 - until epoch: 076, best_acc1: 71.154%
2022-02-23 21:32:36 - epoch 077 lr: 0.0010000000000000002
2022-02-23 21:33:14 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.4456
2022-02-23 21:33:47 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.2773
2022-02-23 21:34:21 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.1348
2022-02-23 21:34:54 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.2288
2022-02-23 21:35:28 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.1386
2022-02-23 21:36:01 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.2348
2022-02-23 21:36:34 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.2233
2022-02-23 21:37:07 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.3293
2022-02-23 21:37:41 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.3865
2022-02-23 21:38:14 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.1872
2022-02-23 21:38:47 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.4396
2022-02-23 21:39:20 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.5005
2022-02-23 21:39:54 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.1852
2022-02-23 21:40:27 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.5159
2022-02-23 21:41:01 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.1173
2022-02-23 21:41:34 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.3129
2022-02-23 21:42:08 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.4813
2022-02-23 21:42:41 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.2373
2022-02-23 21:43:15 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.0538
2022-02-23 21:43:48 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.2600
2022-02-23 21:44:22 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.4943
2022-02-23 21:44:56 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.2094
2022-02-23 21:45:30 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.3959
2022-02-23 21:46:03 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.2083
2022-02-23 21:46:36 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.1666
2022-02-23 21:47:09 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.1547
2022-02-23 21:47:42 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.2389
2022-02-23 21:48:16 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.3758
2022-02-23 21:48:50 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.4687
2022-02-23 21:49:23 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.1663
2022-02-23 21:49:56 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.1973
2022-02-23 21:50:29 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.4090
2022-02-23 21:51:03 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.2129
2022-02-23 21:51:36 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.3870
2022-02-23 21:52:10 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.3291
2022-02-23 21:52:42 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.0585
2022-02-23 21:53:14 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.3495
2022-02-23 21:53:46 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.2193
2022-02-23 21:54:18 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.2332
2022-02-23 21:54:50 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.2709
2022-02-23 21:55:24 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.3279
2022-02-23 21:55:56 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.3515
2022-02-23 21:56:30 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.3253
2022-02-23 21:57:02 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.2806
2022-02-23 21:57:35 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.3457
2022-02-23 21:58:08 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.2313
2022-02-23 21:58:42 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.2110
2022-02-23 21:59:16 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.1715
2022-02-23 21:59:48 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.6596
2022-02-23 22:00:20 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.2108
2022-02-23 22:00:21 - train: epoch 077, train_loss: 1.2853
2022-02-23 22:01:37 - eval: epoch: 077, acc1: 71.204%, acc5: 90.412%, test_loss: 1.1423, per_image_load_time: 1.258ms, per_image_inference_time: 0.255ms
2022-02-23 22:01:37 - until epoch: 077, best_acc1: 71.204%
2022-02-23 22:01:37 - epoch 078 lr: 0.0010000000000000002
2022-02-23 22:02:16 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.3627
2022-02-23 22:02:50 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.2805
2022-02-23 22:03:23 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.3219
2022-02-23 22:03:56 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.3620
2022-02-23 22:04:29 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.1766
2022-02-23 22:05:03 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.2478
2022-02-23 22:05:35 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.4427
2022-02-23 22:06:09 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.6505
2022-02-23 22:06:41 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.3498
2022-02-23 22:07:14 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.1968
2022-02-23 22:07:48 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.2744
2022-02-23 22:08:20 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.1012
2022-02-23 22:08:54 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.1750
2022-02-23 22:09:27 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.1846
2022-02-23 22:10:01 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.2625
2022-02-23 22:10:33 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.3551
2022-02-23 22:11:06 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.3639
2022-02-23 22:11:39 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.3193
2022-02-23 22:12:13 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.4660
2022-02-23 22:12:46 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.3052
2022-02-23 22:13:19 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.4224
2022-02-23 22:13:53 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.1845
2022-02-23 22:14:27 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.1914
2022-02-23 22:14:59 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.2543
2022-02-23 22:15:33 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.2867
2022-02-23 22:16:05 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.2966
2022-02-23 22:16:39 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.2799
2022-02-23 22:17:11 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.3722
2022-02-23 22:17:45 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.2912
2022-02-23 22:18:17 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.4239
2022-02-23 22:18:51 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.2967
2022-02-23 22:19:23 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.3017
2022-02-23 22:19:56 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.5333
2022-02-23 22:20:30 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.3139
2022-02-23 22:21:03 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.4356
2022-02-23 22:21:36 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.4083
2022-02-23 22:22:08 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.3404
2022-02-23 22:22:42 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.3844
2022-02-23 22:23:15 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.3614
2022-02-23 22:23:48 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.4108
2022-02-23 22:24:20 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.3284
2022-02-23 22:24:54 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.1919
2022-02-23 22:25:26 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.1179
2022-02-23 22:25:59 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.2535
2022-02-23 22:26:31 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.3345
2022-02-23 22:27:04 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.1725
2022-02-23 22:27:38 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.3482
2022-02-23 22:28:12 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.4579
2022-02-23 22:28:46 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.2475
2022-02-23 22:29:18 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.2747
2022-02-23 22:29:20 - train: epoch 078, train_loss: 1.2832
2022-02-23 22:30:35 - eval: epoch: 078, acc1: 71.178%, acc5: 90.332%, test_loss: 1.1470, per_image_load_time: 0.975ms, per_image_inference_time: 0.226ms
2022-02-23 22:30:36 - until epoch: 078, best_acc1: 71.204%
2022-02-23 22:30:36 - epoch 079 lr: 0.0010000000000000002
2022-02-23 22:31:14 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.3203
2022-02-23 22:31:48 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.2168
2022-02-23 22:32:21 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.3257
2022-02-23 22:32:54 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.3856
2022-02-23 22:33:27 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.1005
2022-02-23 22:34:00 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 0.9756
2022-02-23 22:34:34 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.1438
2022-02-23 22:35:07 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.1507
2022-02-23 22:35:41 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.2440
2022-02-23 22:36:14 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.2216
2022-02-23 22:36:48 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.4001
2022-02-23 22:37:21 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.4432
2022-02-23 22:37:54 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.1954
2022-02-23 22:38:28 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.3097
2022-02-23 22:39:00 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.3820
2022-02-23 22:39:34 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.1382
2022-02-23 22:40:07 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.2539
2022-02-23 22:40:42 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.3946
2022-02-23 22:41:15 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.2687
2022-02-23 22:41:49 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.4401
2022-02-23 22:42:21 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.2594
2022-02-23 22:42:54 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.1966
2022-02-23 22:43:28 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.1946
2022-02-23 22:44:01 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.3735
2022-02-23 22:44:34 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.2355
2022-02-23 22:45:07 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.0753
2022-02-23 22:45:40 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.9801
2022-02-23 22:46:13 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.3005
2022-02-23 22:46:45 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.2646
2022-02-23 22:47:18 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.3350
2022-02-23 22:47:52 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.3506
2022-02-23 22:48:24 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.3432
2022-02-23 22:48:57 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.4297
2022-02-23 22:49:31 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.2866
2022-02-23 22:50:03 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.2588
2022-02-23 22:50:36 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.2746
2022-02-23 22:51:09 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.2815
2022-02-23 22:51:42 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.3629
2022-02-23 22:52:15 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.1757
2022-02-23 22:52:48 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.1146
2022-02-23 22:53:22 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.3074
2022-02-23 22:53:55 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.2522
2022-02-23 22:54:28 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.2198
2022-02-23 22:55:01 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.4557
2022-02-23 22:55:34 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.3617
2022-02-23 22:56:06 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.3516
2022-02-23 22:56:40 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.2699
2022-02-23 22:57:13 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.4258
2022-02-23 22:57:46 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.2697
2022-02-23 22:58:17 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.0828
2022-02-23 22:58:19 - train: epoch 079, train_loss: 1.2807
2022-02-23 22:59:32 - eval: epoch: 079, acc1: 70.988%, acc5: 90.318%, test_loss: 1.1491, per_image_load_time: 1.108ms, per_image_inference_time: 0.215ms
2022-02-23 22:59:32 - until epoch: 079, best_acc1: 71.204%
2022-02-23 22:59:32 - epoch 080 lr: 0.0010000000000000002
2022-02-23 23:00:09 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.2328
2022-02-23 23:00:43 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.2796
2022-02-23 23:01:16 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.2898
2022-02-23 23:01:48 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.2893
2022-02-23 23:02:21 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.1770
2022-02-23 23:02:54 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.3063
2022-02-23 23:03:27 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.3161
2022-02-23 23:04:01 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.1846
2022-02-23 23:04:34 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.2253
2022-02-23 23:05:07 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.1567
2022-02-23 23:05:39 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.2143
2022-02-23 23:06:13 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.3502
2022-02-23 23:06:46 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.1451
2022-02-23 23:07:18 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.2237
2022-02-23 23:07:52 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.3110
2022-02-23 23:08:25 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.2676
2022-02-23 23:08:58 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.5587
2022-02-23 23:09:32 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.4721
2022-02-23 23:10:04 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.2738
2022-02-23 23:10:37 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.2715
2022-02-23 23:11:10 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.4678
2022-02-23 23:11:44 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.4474
2022-02-23 23:12:16 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.2880
2022-02-23 23:12:50 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.3819
2022-02-23 23:13:22 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.0662
2022-02-23 23:13:56 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.3472
2022-02-23 23:14:29 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.1543
2022-02-23 23:15:03 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.2095
2022-02-23 23:15:36 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.3284
2022-02-23 23:16:10 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.3343
2022-02-23 23:16:42 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.3530
2022-02-23 23:17:16 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.0441
2022-02-23 23:17:48 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.4175
2022-02-23 23:18:22 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.1534
2022-02-23 23:18:55 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.1714
2022-02-23 23:19:29 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.3119
2022-02-23 23:20:02 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.4924
2022-02-23 23:20:35 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.5321
2022-02-23 23:21:08 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.2201
2022-02-23 23:21:41 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.4368
2022-02-23 23:22:13 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.4360
2022-02-23 23:22:47 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.2475
2022-02-23 23:23:21 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.5674
2022-02-23 23:23:52 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.2000
2022-02-23 23:24:27 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.1439
2022-02-23 23:25:00 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.3470
2022-02-23 23:25:33 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.2427
2022-02-23 23:26:07 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.3105
2022-02-23 23:26:40 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.4000
2022-02-23 23:27:13 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.0828
2022-02-23 23:27:15 - train: epoch 080, train_loss: 1.2806
2022-02-23 23:28:31 - eval: epoch: 080, acc1: 71.368%, acc5: 90.280%, test_loss: 1.1444, per_image_load_time: 1.255ms, per_image_inference_time: 0.231ms
2022-02-23 23:28:31 - until epoch: 080, best_acc1: 71.368%
2022-02-23 23:28:31 - epoch 081 lr: 0.0010000000000000002
2022-02-23 23:29:10 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.3147
2022-02-23 23:29:43 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.2016
2022-02-23 23:30:15 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.1550
2022-02-23 23:30:49 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.2693
2022-02-23 23:31:21 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.4504
2022-02-23 23:31:55 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.2350
2022-02-23 23:32:26 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.1645
2022-02-23 23:33:00 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.3914
2022-02-23 23:33:33 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.2668
2022-02-23 23:34:06 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.6498
2022-02-23 23:34:41 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.0324
2022-02-23 23:35:14 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.2630
2022-02-23 23:35:47 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.2301
2022-02-23 23:36:20 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.2190
2022-02-23 23:36:53 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.3695
2022-02-23 23:37:26 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 0.9918
2022-02-23 23:38:00 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.2303
2022-02-23 23:38:33 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.3114
2022-02-23 23:39:05 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.0841
2022-02-23 23:39:38 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.2759
2022-02-23 23:40:12 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.2312
2022-02-23 23:40:45 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.1788
2022-02-23 23:41:18 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.1274
2022-02-23 23:41:52 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.0326
2022-02-23 23:42:24 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.3182
2022-02-23 23:42:57 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.2073
2022-02-23 23:43:31 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.3139
2022-02-23 23:44:04 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.1064
2022-02-23 23:44:37 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.1919
2022-02-23 23:45:10 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.3210
2022-02-23 23:45:43 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.1466
2022-02-23 23:46:16 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.2690
2022-02-23 23:46:48 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.2154
2022-02-23 23:47:21 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.5406
2022-02-23 23:47:55 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.2657
2022-02-23 23:48:28 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.2584
2022-02-23 23:49:01 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.2315
2022-02-23 23:49:35 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.2459
2022-02-23 23:50:08 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.4677
2022-02-23 23:50:41 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.1772
2022-02-23 23:51:14 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.2431
2022-02-23 23:51:48 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.2107
2022-02-23 23:52:20 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.2654
2022-02-23 23:52:54 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.5751
2022-02-23 23:53:27 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.2549
2022-02-23 23:53:59 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.1365
2022-02-23 23:54:33 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.3235
2022-02-23 23:55:07 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.3118
2022-02-23 23:55:39 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.3297
2022-02-23 23:56:11 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.9957
2022-02-23 23:56:13 - train: epoch 081, train_loss: 1.2758
2022-02-23 23:57:29 - eval: epoch: 081, acc1: 71.294%, acc5: 90.302%, test_loss: 1.1462, per_image_load_time: 0.948ms, per_image_inference_time: 0.218ms
2022-02-23 23:57:29 - until epoch: 081, best_acc1: 71.368%
2022-02-23 23:57:29 - epoch 082 lr: 0.0010000000000000002
2022-02-23 23:58:08 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.2307
2022-02-23 23:58:42 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.2916
2022-02-23 23:59:15 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.2145
2022-02-23 23:59:48 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.2034
2022-02-24 00:00:21 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.2751
2022-02-24 00:00:54 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.2411
2022-02-24 00:01:27 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.4828
2022-02-24 00:02:00 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.1058
2022-02-24 00:02:34 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.3240
2022-02-24 00:03:06 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.2681
2022-02-24 00:03:39 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.4434
2022-02-24 00:04:11 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.3017
2022-02-24 00:04:44 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.3635
2022-02-24 00:05:17 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.0740
2022-02-24 00:05:51 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.3551
2022-02-24 00:06:24 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.2206
2022-02-24 00:06:58 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.1562
2022-02-24 00:07:31 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.2616
2022-02-24 00:08:05 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.3748
2022-02-24 00:08:38 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.1646
2022-02-24 00:09:12 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.3007
2022-02-24 00:09:45 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.3280
2022-02-24 00:10:19 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.2478
2022-02-24 00:10:52 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.3589
2022-02-24 00:11:26 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.2567
2022-02-24 00:11:59 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.2284
2022-02-24 00:12:33 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.1642
2022-02-24 00:13:07 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.0677
2022-02-24 00:13:40 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.1710
2022-02-24 00:14:14 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.2257
2022-02-24 00:14:47 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.2428
2022-02-24 00:15:20 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.3421
2022-02-24 00:15:54 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.1719
2022-02-24 00:16:27 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.2679
2022-02-24 00:17:00 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.0886
2022-02-24 00:17:33 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.2816
2022-02-24 00:18:06 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.1600
2022-02-24 00:18:40 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.3113
2022-02-24 00:19:13 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.1362
2022-02-24 00:19:47 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.4106
2022-02-24 00:20:19 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.2412
2022-02-24 00:20:53 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.3475
2022-02-24 00:21:26 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.2841
2022-02-24 00:21:59 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.1219
2022-02-24 00:22:33 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.2323
2022-02-24 00:23:06 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.2957
2022-02-24 00:23:39 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.2242
2022-02-24 00:24:12 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.1288
2022-02-24 00:24:46 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.1598
2022-02-24 00:25:19 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.2661
2022-02-24 00:25:21 - train: epoch 082, train_loss: 1.2755
2022-02-24 00:26:38 - eval: epoch: 082, acc1: 71.376%, acc5: 90.338%, test_loss: 1.1419, per_image_load_time: 1.714ms, per_image_inference_time: 0.241ms
2022-02-24 00:26:38 - until epoch: 082, best_acc1: 71.376%
2022-02-24 00:26:38 - epoch 083 lr: 0.0010000000000000002
2022-02-24 00:27:17 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.2009
2022-02-24 00:27:51 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.1607
2022-02-24 00:28:24 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.2588
2022-02-24 00:28:57 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.2762
2022-02-24 00:29:31 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.2190
2022-02-24 00:30:04 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.1832
2022-02-24 00:30:38 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.1623
2022-02-24 00:31:11 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.2528
2022-02-24 00:31:45 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.3132
2022-02-24 00:32:17 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.4409
2022-02-24 00:32:52 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.2795
2022-02-24 00:33:25 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.3287
2022-02-24 00:33:59 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.4342
2022-02-24 00:34:33 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.3660
2022-02-24 00:35:06 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.0869
2022-02-24 00:35:40 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.2062
2022-02-24 00:36:11 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.4110
2022-02-24 00:36:44 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.3056
2022-02-24 00:37:17 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.2877
2022-02-24 00:37:50 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.0776
2022-02-24 00:38:22 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.1055
2022-02-24 00:38:55 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.2532
2022-02-24 00:39:28 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.2293
2022-02-24 00:40:01 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.1612
2022-02-24 00:40:35 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.1581
2022-02-24 00:41:08 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.2205
2022-02-24 00:41:41 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.2455
2022-02-24 00:42:15 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.1827
2022-02-24 00:42:48 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.2441
2022-02-24 00:43:21 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.4577
2022-02-24 00:43:54 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.2418
2022-02-24 00:44:27 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.1171
2022-02-24 00:45:01 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.3116
2022-02-24 00:45:34 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.2821
2022-02-24 00:46:07 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.3735
2022-02-24 00:46:40 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.2878
2022-02-24 00:47:14 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.1706
2022-02-24 00:47:47 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.3293
2022-02-24 00:48:21 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.2930
2022-02-24 00:48:53 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.4376
2022-02-24 00:49:27 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.2843
2022-02-24 00:50:00 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.5127
2022-02-24 00:50:34 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.1811
2022-02-24 00:51:06 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.2021
2022-02-24 00:51:40 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.1908
2022-02-24 00:52:13 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.2993
2022-02-24 00:52:47 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.5350
2022-02-24 00:53:19 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.3086
2022-02-24 00:53:55 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.4343
2022-02-24 00:54:26 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.2029
2022-02-24 00:54:28 - train: epoch 083, train_loss: 1.2726
2022-02-24 00:55:43 - eval: epoch: 083, acc1: 71.344%, acc5: 90.396%, test_loss: 1.1389, per_image_load_time: 1.174ms, per_image_inference_time: 0.238ms
2022-02-24 00:55:44 - until epoch: 083, best_acc1: 71.376%
2022-02-24 00:55:44 - epoch 084 lr: 0.0010000000000000002
2022-02-24 00:56:22 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.2281
2022-02-24 00:56:55 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.3032
2022-02-24 00:57:29 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.1230
2022-02-24 00:58:03 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.2641
2022-02-24 00:58:35 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.2789
2022-02-24 00:59:09 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.3671
2022-02-24 00:59:42 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.4527
2022-02-24 01:00:15 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.4034
2022-02-24 01:00:49 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.2458
2022-02-24 01:01:22 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.2208
2022-02-24 01:01:56 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.2537
2022-02-24 01:02:29 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.5350
2022-02-24 01:03:03 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.2187
2022-02-24 01:03:38 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.2024
2022-02-24 01:04:10 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.2821
2022-02-24 01:04:43 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.2273
2022-02-24 01:05:17 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.4707
2022-02-24 01:05:49 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.3913
2022-02-24 01:06:23 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.2130
2022-02-24 01:06:55 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.1354
2022-02-24 01:07:28 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.1070
2022-02-24 01:08:00 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.1578
2022-02-24 01:08:33 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.2836
2022-02-24 01:09:04 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.1863
2022-02-24 01:09:36 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.3987
2022-02-24 01:10:07 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.1681
2022-02-24 01:10:39 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.2236
2022-02-24 01:11:12 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.1344
2022-02-24 01:11:45 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.4050
2022-02-24 01:12:17 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.2304
2022-02-24 01:12:51 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.3471
2022-02-24 01:13:23 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.1757
2022-02-24 01:13:56 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.2010
2022-02-24 01:14:29 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.3595
2022-02-24 01:15:02 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 0.9605
2022-02-24 01:15:34 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.2348
2022-02-24 01:16:07 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.4230
2022-02-24 01:16:40 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.3532
2022-02-24 01:17:13 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.3459
2022-02-24 01:17:46 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.4464
2022-02-24 01:18:20 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.2128
2022-02-24 01:18:52 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.4392
2022-02-24 01:19:25 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.3703
2022-02-24 01:19:58 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.4559
2022-02-24 01:20:32 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.1793
2022-02-24 01:21:04 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.1879
2022-02-24 01:21:38 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.2359
2022-02-24 01:22:10 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.2876
2022-02-24 01:22:44 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.2024
2022-02-24 01:23:16 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.1919
2022-02-24 01:23:18 - train: epoch 084, train_loss: 1.2681
2022-02-24 01:24:33 - eval: epoch: 084, acc1: 71.266%, acc5: 90.352%, test_loss: 1.1396, per_image_load_time: 1.386ms, per_image_inference_time: 0.281ms
2022-02-24 01:24:33 - until epoch: 084, best_acc1: 71.376%
2022-02-24 01:24:33 - epoch 085 lr: 0.0010000000000000002
2022-02-24 01:25:11 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.2078
2022-02-24 01:25:44 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.1984
2022-02-24 01:26:17 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.1992
2022-02-24 01:26:50 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.1953
2022-02-24 01:27:23 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.4039
2022-02-24 01:27:56 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.1331
2022-02-24 01:28:28 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.3273
2022-02-24 01:29:01 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.1967
2022-02-24 01:29:34 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.2842
2022-02-24 01:30:08 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.3183
2022-02-24 01:30:41 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.1652
2022-02-24 01:31:15 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.2371
2022-02-24 01:31:48 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.2499
2022-02-24 01:32:21 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.0724
2022-02-24 01:32:53 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.3247
2022-02-24 01:33:27 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.2526
2022-02-24 01:34:00 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.6035
2022-02-24 01:34:33 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.9832
2022-02-24 01:35:06 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.1803
2022-02-24 01:35:39 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.2064
2022-02-24 01:36:12 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 1.1733
2022-02-24 01:36:45 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.2322
2022-02-24 01:37:18 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.1522
2022-02-24 01:37:51 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.4711
2022-02-24 01:38:24 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.4205
2022-02-24 01:38:57 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.4341
2022-02-24 01:39:30 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.2120
2022-02-24 01:40:03 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.4203
2022-02-24 01:40:36 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.2544
2022-02-24 01:41:07 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.4375
2022-02-24 01:41:39 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.3315
2022-02-24 01:42:11 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.4940
2022-02-24 01:42:41 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.3894
2022-02-24 01:43:14 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.3511
2022-02-24 01:43:47 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.2678
2022-02-24 01:44:20 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.3397
2022-02-24 01:44:53 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.2198
2022-02-24 01:45:26 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.2046
2022-02-24 01:45:58 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.1422
2022-02-24 01:46:31 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.5264
2022-02-24 01:47:04 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.3252
2022-02-24 01:47:37 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.4679
2022-02-24 01:48:10 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.2536
2022-02-24 01:48:43 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.2897
2022-02-24 01:49:15 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.4514
2022-02-24 01:49:49 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.2512
2022-02-24 01:50:21 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.5564
2022-02-24 01:50:55 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.1912
2022-02-24 01:51:28 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.3449
2022-02-24 01:52:00 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.2320
2022-02-24 01:52:02 - train: epoch 085, train_loss: 1.2684
2022-02-24 01:53:16 - eval: epoch: 085, acc1: 71.148%, acc5: 90.366%, test_loss: 1.1404, per_image_load_time: 1.508ms, per_image_inference_time: 0.242ms
2022-02-24 01:53:16 - until epoch: 085, best_acc1: 71.376%
2022-02-24 01:53:16 - epoch 086 lr: 0.0010000000000000002
2022-02-24 01:53:55 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.2778
2022-02-24 01:54:28 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.0924
2022-02-24 01:55:00 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.3413
2022-02-24 01:55:34 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.4966
2022-02-24 01:56:07 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.3531
2022-02-24 01:56:39 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.2226
2022-02-24 01:57:12 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.3081
2022-02-24 01:57:45 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.4287
2022-02-24 01:58:18 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.2452
2022-02-24 01:58:50 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.2912
2022-02-24 01:59:24 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.4105
2022-02-24 01:59:57 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.1556
2022-02-24 02:00:30 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.3263
2022-02-24 02:01:03 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.3084
2022-02-24 02:01:36 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 0.9584
2022-02-24 02:02:08 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.3646
2022-02-24 02:02:41 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.2594
2022-02-24 02:03:15 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.3821
2022-02-24 02:03:47 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.2758
2022-02-24 02:04:21 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.4433
2022-02-24 02:04:53 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.0791
2022-02-24 02:05:26 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.3479
2022-02-24 02:05:59 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.2295
2022-02-24 02:06:32 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.1483
2022-02-24 02:07:05 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.2241
2022-02-24 02:07:38 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.3568
2022-02-24 02:08:10 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.2395
2022-02-24 02:08:44 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.3427
2022-02-24 02:09:17 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.1671
2022-02-24 02:09:50 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.0797
2022-02-24 02:10:23 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.1788
2022-02-24 02:10:56 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.4790
2022-02-24 02:11:28 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.2088
2022-02-24 02:12:02 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.3285
2022-02-24 02:12:35 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.3691
2022-02-24 02:13:08 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.3476
2022-02-24 02:13:39 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.3295
2022-02-24 02:14:11 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.3827
2022-02-24 02:14:42 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.3552
2022-02-24 02:15:14 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.3406
2022-02-24 02:15:46 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.0401
2022-02-24 02:16:19 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.2864
2022-02-24 02:16:52 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.3990
2022-02-24 02:17:25 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.2949
2022-02-24 02:17:58 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.1468
2022-02-24 02:18:32 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.3953
2022-02-24 02:19:05 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.3682
2022-02-24 02:19:38 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.2727
2022-02-24 02:20:11 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.3139
2022-02-24 02:20:42 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.3087
2022-02-24 02:20:44 - train: epoch 086, train_loss: 1.2653
2022-02-24 02:21:59 - eval: epoch: 086, acc1: 71.324%, acc5: 90.298%, test_loss: 1.1442, per_image_load_time: 2.057ms, per_image_inference_time: 0.258ms
2022-02-24 02:21:59 - until epoch: 086, best_acc1: 71.376%
2022-02-24 02:21:59 - epoch 087 lr: 0.0010000000000000002
2022-02-24 02:22:37 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.1205
2022-02-24 02:23:10 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.1129
2022-02-24 02:23:43 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.2347
2022-02-24 02:24:17 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.2880
2022-02-24 02:24:49 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.2027
2022-02-24 02:25:22 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.3231
2022-02-24 02:25:54 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.3787
2022-02-24 02:26:28 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.3564
2022-02-24 02:27:01 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.2362
2022-02-24 02:27:34 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.1691
2022-02-24 02:28:06 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.5159
2022-02-24 02:28:40 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.3605
2022-02-24 02:29:12 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.5777
2022-02-24 02:29:46 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.2446
2022-02-24 02:30:18 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.2804
2022-02-24 02:30:51 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.2608
2022-02-24 02:31:24 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.4978
2022-02-24 02:31:57 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.2443
2022-02-24 02:32:30 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.2951
2022-02-24 02:33:03 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.2736
2022-02-24 02:33:37 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.2028
2022-02-24 02:34:10 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.1782
2022-02-24 02:34:44 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.2723
2022-02-24 02:35:16 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.1182
2022-02-24 02:35:49 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.1481
2022-02-24 02:36:22 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.3714
2022-02-24 02:36:55 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.3530
2022-02-24 02:37:28 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.1442
2022-02-24 02:38:01 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.2436
2022-02-24 02:38:34 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.1962
2022-02-24 02:39:07 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.2274
2022-02-24 02:39:40 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.2691
2022-02-24 02:40:13 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.2912
2022-02-24 02:40:46 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.1599
2022-02-24 02:41:19 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.1064
2022-02-24 02:41:51 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.1905
2022-02-24 02:42:25 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.1697
2022-02-24 02:42:58 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.3121
2022-02-24 02:43:32 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.3764
2022-02-24 02:44:04 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.2814
2022-02-24 02:44:37 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.3985
2022-02-24 02:45:10 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.2300
2022-02-24 02:45:43 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.2722
2022-02-24 02:46:14 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.2135
2022-02-24 02:46:46 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.2125
2022-02-24 02:47:17 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.3942
2022-02-24 02:47:49 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.3159
2022-02-24 02:48:23 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.2070
2022-02-24 02:48:57 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.1386
2022-02-24 02:49:29 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.2958
2022-02-24 02:49:31 - train: epoch 087, train_loss: 1.2649
2022-02-24 02:50:46 - eval: epoch: 087, acc1: 71.286%, acc5: 90.526%, test_loss: 1.1372, per_image_load_time: 2.574ms, per_image_inference_time: 0.290ms
2022-02-24 02:50:46 - until epoch: 087, best_acc1: 71.376%
2022-02-24 02:50:46 - epoch 088 lr: 0.0010000000000000002
2022-02-24 02:51:24 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.0875
2022-02-24 02:51:57 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.2156
2022-02-24 02:52:29 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.5817
2022-02-24 02:53:02 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.3098
2022-02-24 02:53:35 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.2922
2022-02-24 02:54:08 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.2503
2022-02-24 02:54:42 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.3495
2022-02-24 02:55:14 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.2849
2022-02-24 02:55:47 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.2226
2022-02-24 02:56:20 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.2942
2022-02-24 02:56:53 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.1498
2022-02-24 02:57:26 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.3096
2022-02-24 02:58:00 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.0798
2022-02-24 02:58:32 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.0633
2022-02-24 02:59:06 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.3374
2022-02-24 02:59:39 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.0387
2022-02-24 03:00:12 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.2743
2022-02-24 03:00:47 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.1125
2022-02-24 03:01:19 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.4429
2022-02-24 03:01:52 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.1787
2022-02-24 03:02:25 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.3271
2022-02-24 03:02:58 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.3079
2022-02-24 03:03:33 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.2691
2022-02-24 03:04:05 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.3450
2022-02-24 03:04:38 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.4154
2022-02-24 03:05:11 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.3071
2022-02-24 03:05:45 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.2200
2022-02-24 03:06:18 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.3827
2022-02-24 03:06:51 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.1108
2022-02-24 03:07:24 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.2836
2022-02-24 03:07:57 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.3011
2022-02-24 03:08:30 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.0710
2022-02-24 03:09:03 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.1235
2022-02-24 03:09:36 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.1217
2022-02-24 03:10:10 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.3701
2022-02-24 03:10:42 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.2335
2022-02-24 03:11:15 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.4430
2022-02-24 03:11:48 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.3876
2022-02-24 03:12:21 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.2520
2022-02-24 03:12:55 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.1076
2022-02-24 03:13:29 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.2081
2022-02-24 03:14:01 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.2554
2022-02-24 03:14:34 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.1185
2022-02-24 03:15:07 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.2271
2022-02-24 03:15:41 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.1786
2022-02-24 03:16:13 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.5222
2022-02-24 03:16:47 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.3248
2022-02-24 03:17:19 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.3181
2022-02-24 03:17:53 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.3009
2022-02-24 03:18:24 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.2674
2022-02-24 03:18:26 - train: epoch 088, train_loss: 1.2636
2022-02-24 03:19:39 - eval: epoch: 088, acc1: 71.316%, acc5: 90.316%, test_loss: 1.1405, per_image_load_time: 1.643ms, per_image_inference_time: 0.229ms
2022-02-24 03:19:39 - until epoch: 088, best_acc1: 71.376%
2022-02-24 03:19:39 - epoch 089 lr: 0.0010000000000000002
2022-02-24 03:20:16 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.3244
2022-02-24 03:20:49 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.0700
2022-02-24 03:21:22 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.2948
2022-02-24 03:21:55 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.1203
2022-02-24 03:22:28 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.1337
2022-02-24 03:23:01 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.1551
2022-02-24 03:23:34 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.5499
2022-02-24 03:24:07 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.3620
2022-02-24 03:24:40 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.2392
2022-02-24 03:25:13 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.3652
2022-02-24 03:25:46 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.2342
2022-02-24 03:26:19 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.2646
2022-02-24 03:26:52 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.2844
2022-02-24 03:27:25 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.4995
2022-02-24 03:27:58 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.3914
2022-02-24 03:28:31 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.0718
2022-02-24 03:29:05 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.2907
2022-02-24 03:29:37 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.3834
2022-02-24 03:30:11 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.1109
2022-02-24 03:30:44 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.1335
2022-02-24 03:31:17 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.3369
2022-02-24 03:31:50 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.2280
2022-02-24 03:32:24 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.2629
2022-02-24 03:32:57 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.3518
2022-02-24 03:33:30 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.2986
2022-02-24 03:34:04 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.2230
2022-02-24 03:34:37 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.1662
2022-02-24 03:35:10 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.3093
2022-02-24 03:35:43 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.3701
2022-02-24 03:36:17 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.2915
2022-02-24 03:36:50 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.3863
2022-02-24 03:37:23 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.4243
2022-02-24 03:37:56 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.5440
2022-02-24 03:38:30 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.2442
2022-02-24 03:39:03 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.1545
2022-02-24 03:39:36 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.1814
2022-02-24 03:40:09 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.2976
2022-02-24 03:40:43 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.1820
2022-02-24 03:41:17 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.3880
2022-02-24 03:41:50 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.1530
2022-02-24 03:42:24 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.3536
2022-02-24 03:42:58 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.3504
2022-02-24 03:43:32 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.3347
2022-02-24 03:44:05 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.3254
2022-02-24 03:44:39 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.0505
2022-02-24 03:45:12 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.2102
2022-02-24 03:45:46 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.2508
2022-02-24 03:46:19 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.2322
2022-02-24 03:46:53 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.3046
2022-02-24 03:47:25 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 0.9700
2022-02-24 03:47:28 - train: epoch 089, train_loss: 1.2623
2022-02-24 03:48:43 - eval: epoch: 089, acc1: 71.406%, acc5: 90.344%, test_loss: 1.1428, per_image_load_time: 1.291ms, per_image_inference_time: 0.274ms
2022-02-24 03:48:44 - until epoch: 089, best_acc1: 71.406%
2022-02-24 03:48:44 - epoch 090 lr: 0.0010000000000000002
2022-02-24 03:49:22 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.2837
2022-02-24 03:49:55 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.1786
2022-02-24 03:50:29 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.2215
2022-02-24 03:51:02 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.1603
2022-02-24 03:51:35 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.1472
2022-02-24 03:52:09 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.4490
2022-02-24 03:52:42 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.2241
2022-02-24 03:53:15 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.3284
2022-02-24 03:53:49 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.3498
2022-02-24 03:54:22 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.0631
2022-02-24 03:54:56 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.3382
2022-02-24 03:55:28 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.1685
2022-02-24 03:56:02 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.2855
2022-02-24 03:56:36 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.2283
2022-02-24 03:57:09 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.3931
2022-02-24 03:57:43 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.2979
2022-02-24 03:58:16 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.1595
2022-02-24 03:58:49 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.1790
2022-02-24 03:59:23 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.2292
2022-02-24 03:59:56 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.2859
2022-02-24 04:00:29 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.2538
2022-02-24 04:01:02 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.3101
2022-02-24 04:01:36 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.2437
2022-02-24 04:02:10 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.3122
2022-02-24 04:02:43 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.3551
2022-02-24 04:03:16 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.1499
2022-02-24 04:03:50 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.2498
2022-02-24 04:04:23 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.3414
2022-02-24 04:04:57 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.1984
2022-02-24 04:05:30 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.1997
2022-02-24 04:06:03 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.0538
2022-02-24 04:06:37 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.2605
2022-02-24 04:07:10 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.4180
2022-02-24 04:07:44 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.0965
2022-02-24 04:08:17 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.3145
2022-02-24 04:08:51 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.1159
2022-02-24 04:09:25 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.4176
2022-02-24 04:09:57 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.1047
2022-02-24 04:10:31 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.0619
2022-02-24 04:11:04 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.3364
2022-02-24 04:11:38 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.5846
2022-02-24 04:12:12 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.2814
2022-02-24 04:12:45 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.2055
2022-02-24 04:13:19 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.2771
2022-02-24 04:13:52 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.1304
2022-02-24 04:14:26 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.2791
2022-02-24 04:15:01 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.1031
2022-02-24 04:15:34 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.6057
2022-02-24 04:16:07 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.0777
2022-02-24 04:16:40 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.2291
2022-02-24 04:16:42 - train: epoch 090, train_loss: 1.2602
2022-02-24 04:17:58 - eval: epoch: 090, acc1: 71.334%, acc5: 90.384%, test_loss: 1.1409, per_image_load_time: 1.245ms, per_image_inference_time: 0.275ms
2022-02-24 04:17:59 - until epoch: 090, best_acc1: 71.406%
2022-02-24 04:17:59 - epoch 091 lr: 0.00010000000000000003
2022-02-24 04:18:36 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.2183
2022-02-24 04:19:11 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.2951
2022-02-24 04:19:44 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.4379
2022-02-24 04:20:18 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.3346
2022-02-24 04:20:51 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.2571
2022-02-24 04:21:24 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.2270
2022-02-24 04:21:57 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.4148
2022-02-24 04:22:30 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 0.8853
2022-02-24 04:23:04 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.3951
2022-02-24 04:23:38 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.2529
2022-02-24 04:24:11 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.1313
2022-02-24 04:24:44 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.3823
2022-02-24 04:25:18 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.0842
2022-02-24 04:25:52 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.2950
2022-02-24 04:26:25 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.1749
2022-02-24 04:26:59 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.3581
2022-02-24 04:27:33 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.2258
2022-02-24 04:28:06 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.2525
2022-02-24 04:28:39 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.2723
2022-02-24 04:29:13 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.0435
2022-02-24 04:29:47 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.1647
2022-02-24 04:30:19 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.4414
2022-02-24 04:30:52 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.3065
2022-02-24 04:31:26 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.0999
2022-02-24 04:31:59 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.3456
2022-02-24 04:32:33 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.2584
2022-02-24 04:33:05 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.2112
2022-02-24 04:33:39 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.2627
2022-02-24 04:34:11 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.4007
2022-02-24 04:34:45 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.3064
2022-02-24 04:35:18 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.1944
2022-02-24 04:35:51 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.4504
2022-02-24 04:36:24 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.0760
2022-02-24 04:36:57 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.2506
2022-02-24 04:37:30 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.1577
2022-02-24 04:38:05 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.3820
2022-02-24 04:38:38 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.3839
2022-02-24 04:39:12 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.1372
2022-02-24 04:39:45 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.1749
2022-02-24 04:40:18 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.3484
2022-02-24 04:40:51 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.1950
2022-02-24 04:41:25 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.4088
2022-02-24 04:41:58 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.2625
2022-02-24 04:42:32 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.0611
2022-02-24 04:43:05 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.0593
2022-02-24 04:43:38 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.2294
2022-02-24 04:44:11 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.2784
2022-02-24 04:44:45 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.1330
2022-02-24 04:45:18 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.1920
2022-02-24 04:45:51 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.4229
2022-02-24 04:45:53 - train: epoch 091, train_loss: 1.2281
2022-02-24 04:47:09 - eval: epoch: 091, acc1: 71.782%, acc5: 90.716%, test_loss: 1.1166, per_image_load_time: 1.259ms, per_image_inference_time: 0.278ms
2022-02-24 04:47:09 - until epoch: 091, best_acc1: 71.782%
2022-02-24 04:47:09 - epoch 092 lr: 0.00010000000000000003
2022-02-24 04:47:47 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.2792
2022-02-24 04:48:21 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.3947
2022-02-24 04:48:55 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.1919
2022-02-24 04:49:28 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.0698
2022-02-24 04:50:01 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.1414
2022-02-24 04:50:34 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.1188
2022-02-24 04:51:08 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.2076
2022-02-24 04:51:42 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.4360
2022-02-24 04:52:15 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.2680
2022-02-24 04:52:48 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.2136
2022-02-24 04:53:22 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.0764
2022-02-24 04:53:55 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.3771
2022-02-24 04:54:28 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.0793
2022-02-24 04:55:02 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.0410
2022-02-24 04:55:36 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.1603
2022-02-24 04:56:09 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.2399
2022-02-24 04:56:43 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.3079
2022-02-24 04:57:16 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.1520
2022-02-24 04:57:50 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.0916
2022-02-24 04:58:23 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.0939
2022-02-24 04:58:56 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.2439
2022-02-24 04:59:30 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.3309
2022-02-24 05:00:03 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.2672
2022-02-24 05:00:37 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.1372
2022-02-24 05:01:11 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.2466
2022-02-24 05:01:43 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.3128
2022-02-24 05:02:17 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.2878
2022-02-24 05:02:50 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.0980
2022-02-24 05:03:24 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.2716
2022-02-24 05:03:57 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.2382
2022-02-24 05:04:30 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.2382
2022-02-24 05:05:04 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.1616
2022-02-24 05:05:37 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.3978
2022-02-24 05:06:11 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.1734
2022-02-24 05:06:43 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.2304
2022-02-24 05:07:16 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.2435
2022-02-24 05:07:50 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.1349
2022-02-24 05:08:24 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.2655
2022-02-24 05:08:57 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.1896
2022-02-24 05:09:31 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.2724
2022-02-24 05:10:04 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.1865
2022-02-24 05:10:38 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.1884
2022-02-24 05:11:11 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.2128
2022-02-24 05:11:44 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.2052
2022-02-24 05:12:17 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.1439
2022-02-24 05:12:52 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.2191
2022-02-24 05:13:24 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 1.0961
2022-02-24 05:13:58 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.3145
2022-02-24 05:14:32 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.0451
2022-02-24 05:15:04 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.1441
2022-02-24 05:15:06 - train: epoch 092, train_loss: 1.2183
2022-02-24 05:16:22 - eval: epoch: 092, acc1: 71.888%, acc5: 90.664%, test_loss: 1.1148, per_image_load_time: 1.175ms, per_image_inference_time: 0.272ms
2022-02-24 05:16:22 - until epoch: 092, best_acc1: 71.888%
2022-02-24 05:16:22 - epoch 093 lr: 0.00010000000000000003
2022-02-24 05:17:00 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.2655
2022-02-24 05:17:33 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.2625
2022-02-24 05:18:07 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.2569
2022-02-24 05:18:40 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.3653
2022-02-24 05:19:14 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.3025
2022-02-24 05:19:46 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.2247
2022-02-24 05:20:20 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.0971
2022-02-24 05:20:53 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 0.9734
2022-02-24 05:21:27 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.3687
2022-02-24 05:22:01 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.0034
2022-02-24 05:22:35 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.2116
2022-02-24 05:23:08 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.3495
2022-02-24 05:23:41 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.1156
2022-02-24 05:24:15 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.2512
2022-02-24 05:24:48 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.5803
2022-02-24 05:25:22 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.3122
2022-02-24 05:25:56 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.3008
2022-02-24 05:26:29 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.0751
2022-02-24 05:27:03 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.1058
2022-02-24 05:27:36 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.1982
2022-02-24 05:28:10 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.1079
2022-02-24 05:28:43 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.4031
2022-02-24 05:29:16 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.1984
2022-02-24 05:29:50 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.1933
2022-02-24 05:30:24 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.2120
2022-02-24 05:30:57 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.4612
2022-02-24 05:31:31 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.2188
2022-02-24 05:32:04 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.4895
2022-02-24 05:32:38 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.1013
2022-02-24 05:33:11 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.0044
2022-02-24 05:33:44 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.1695
2022-02-24 05:34:18 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.0990
2022-02-24 05:34:51 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.1661
2022-02-24 05:35:26 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.2790
2022-02-24 05:35:58 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.1978
2022-02-24 05:36:32 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.3668
2022-02-24 05:37:04 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.1411
2022-02-24 05:37:38 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.1055
2022-02-24 05:38:10 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.2987
2022-02-24 05:38:45 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.3797
2022-02-24 05:39:18 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.4185
2022-02-24 05:39:52 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.2429
2022-02-24 05:40:25 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.3750
2022-02-24 05:40:59 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.1418
2022-02-24 05:41:32 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.2227
2022-02-24 05:42:05 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.0960
2022-02-24 05:42:39 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.4384
2022-02-24 05:43:12 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.1996
2022-02-24 05:43:46 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.2927
2022-02-24 05:44:19 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.1309
2022-02-24 05:44:21 - train: epoch 093, train_loss: 1.2161
2022-02-24 05:45:37 - eval: epoch: 093, acc1: 71.980%, acc5: 90.690%, test_loss: 1.1115, per_image_load_time: 0.990ms, per_image_inference_time: 0.263ms
2022-02-24 05:45:37 - until epoch: 093, best_acc1: 71.980%
2022-02-24 05:45:37 - epoch 094 lr: 0.00010000000000000003
2022-02-24 05:46:16 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.2138
2022-02-24 05:46:49 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.2605
2022-02-24 05:47:23 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.2878
2022-02-24 05:47:56 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.2599
2022-02-24 05:48:29 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.1416
2022-02-24 05:49:03 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.0459
2022-02-24 05:49:36 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.1571
2022-02-24 05:50:10 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.1238
2022-02-24 05:50:43 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.2671
2022-02-24 05:51:17 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.2094
2022-02-24 05:51:50 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.4019
2022-02-24 05:52:23 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.0705
2022-02-24 05:52:57 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.2033
2022-02-24 05:53:30 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.2833
2022-02-24 05:54:04 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.3857
2022-02-24 05:54:37 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.2553
2022-02-24 05:55:11 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.1575
2022-02-24 05:55:45 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.0437
2022-02-24 05:56:18 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.1053
2022-02-24 05:56:52 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 0.9679
2022-02-24 05:57:25 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.1276
2022-02-24 05:57:59 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.2024
2022-02-24 05:58:32 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.2025
2022-02-24 05:59:06 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.2320
2022-02-24 05:59:39 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.3748
2022-02-24 06:00:13 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.2285
2022-02-24 06:00:47 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.1636
2022-02-24 06:01:21 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.2887
2022-02-24 06:01:54 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.0793
2022-02-24 06:02:28 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.2838
2022-02-24 06:03:00 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.4661
2022-02-24 06:03:35 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.2213
2022-02-24 06:04:08 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.2818
2022-02-24 06:04:42 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.4360
2022-02-24 06:05:15 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.3267
2022-02-24 06:05:49 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.1895
2022-02-24 06:06:22 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.4021
2022-02-24 06:06:56 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.0943
2022-02-24 06:07:29 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 0.9899
2022-02-24 06:08:03 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.2526
2022-02-24 06:08:36 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.2885
2022-02-24 06:09:10 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.1584
2022-02-24 06:09:43 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.2713
2022-02-24 06:10:16 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.4611
2022-02-24 06:10:50 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.1313
2022-02-24 06:11:23 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.5095
2022-02-24 06:11:58 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.5290
2022-02-24 06:12:31 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.0753
2022-02-24 06:13:06 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.3511
2022-02-24 06:13:37 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.1308
2022-02-24 06:13:40 - train: epoch 094, train_loss: 1.2126
2022-02-24 06:14:55 - eval: epoch: 094, acc1: 72.048%, acc5: 90.782%, test_loss: 1.1112, per_image_load_time: 1.716ms, per_image_inference_time: 0.261ms
2022-02-24 06:14:55 - until epoch: 094, best_acc1: 72.048%
2022-02-24 06:14:55 - epoch 095 lr: 0.00010000000000000003
2022-02-24 06:15:34 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.4373
2022-02-24 06:16:07 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.3196
2022-02-24 06:16:40 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.1677
2022-02-24 06:17:14 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.1338
2022-02-24 06:17:47 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.2516
2022-02-24 06:18:21 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.2776
2022-02-24 06:18:54 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.2216
2022-02-24 06:19:28 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.1070
2022-02-24 06:20:00 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.5087
2022-02-24 06:20:34 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.2240
2022-02-24 06:21:07 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.0477
2022-02-24 06:21:41 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.1647
2022-02-24 06:22:15 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.1836
2022-02-24 06:22:48 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.1934
2022-02-24 06:23:21 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.1377
2022-02-24 06:23:55 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.8618
2022-02-24 06:24:28 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.0489
2022-02-24 06:25:02 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.2511
2022-02-24 06:25:34 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.2980
2022-02-24 06:26:08 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.1749
2022-02-24 06:26:41 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.1402
2022-02-24 06:27:15 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.9670
2022-02-24 06:27:49 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.1711
2022-02-24 06:28:22 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.1489
2022-02-24 06:28:56 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.1973
2022-02-24 06:29:28 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.1677
2022-02-24 06:30:03 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.1787
2022-02-24 06:30:35 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.1125
2022-02-24 06:31:09 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.1867
2022-02-24 06:31:42 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.3317
2022-02-24 06:32:16 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.3420
2022-02-24 06:32:49 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.1463
2022-02-24 06:33:22 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.0886
2022-02-24 06:33:56 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 0.9935
2022-02-24 06:34:30 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.1279
2022-02-24 06:35:02 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.0520
2022-02-24 06:35:35 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.3260
2022-02-24 06:36:09 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 0.9882
2022-02-24 06:36:42 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.1288
2022-02-24 06:37:15 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.0699
2022-02-24 06:37:48 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.0676
2022-02-24 06:38:22 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.0677
2022-02-24 06:38:55 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.3419
2022-02-24 06:39:29 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.3539
2022-02-24 06:40:02 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.1475
2022-02-24 06:40:35 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.1982
2022-02-24 06:41:07 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.2749
2022-02-24 06:41:41 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.2324
2022-02-24 06:42:15 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.1873
2022-02-24 06:42:47 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.0811
2022-02-24 06:42:50 - train: epoch 095, train_loss: 1.2080
2022-02-24 06:44:05 - eval: epoch: 095, acc1: 72.052%, acc5: 90.766%, test_loss: 1.1102, per_image_load_time: 1.425ms, per_image_inference_time: 0.278ms
2022-02-24 06:44:06 - until epoch: 095, best_acc1: 72.052%
2022-02-24 06:44:06 - epoch 096 lr: 0.00010000000000000003
2022-02-24 06:44:44 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.1564
2022-02-24 06:45:16 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.2115
2022-02-24 06:45:50 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.1946
2022-02-24 06:46:24 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.0513
2022-02-24 06:46:56 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.2755
2022-02-24 06:47:30 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.3293
2022-02-24 06:48:03 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.3033
2022-02-24 06:48:36 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.1432
2022-02-24 06:49:08 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.3574
2022-02-24 06:49:42 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.2185
2022-02-24 06:50:15 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.2601
2022-02-24 06:50:49 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 0.9769
2022-02-24 06:51:22 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.1734
2022-02-24 06:51:56 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.2505
2022-02-24 06:52:29 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.2515
2022-02-24 06:53:02 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 1.0256
2022-02-24 06:53:36 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.1010
2022-02-24 06:54:08 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.2103
2022-02-24 06:54:42 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.2389
2022-02-24 06:55:15 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.2081
2022-02-24 06:55:49 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.3468
2022-02-24 06:56:23 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 0.9496
2022-02-24 06:56:56 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.3262
2022-02-24 06:57:29 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.1558
2022-02-24 06:58:03 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.3617
2022-02-24 06:58:36 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.3007
2022-02-24 06:59:09 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.3440
2022-02-24 06:59:43 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.3597
2022-02-24 07:00:16 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.3191
2022-02-24 07:00:50 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.4120
2022-02-24 07:01:23 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.2978
2022-02-24 07:01:57 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.2779
2022-02-24 07:02:30 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.2618
2022-02-24 07:03:03 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.1044
2022-02-24 07:03:36 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.1703
2022-02-24 07:04:10 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.0948
2022-02-24 07:04:43 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.2652
2022-02-24 07:05:16 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.1187
2022-02-24 07:05:50 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.0929
2022-02-24 07:06:23 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.3274
2022-02-24 07:06:56 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.1242
2022-02-24 07:07:30 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.1737
2022-02-24 07:08:02 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 0.9607
2022-02-24 07:08:36 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.1664
2022-02-24 07:09:09 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.1772
2022-02-24 07:09:43 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.0998
2022-02-24 07:10:16 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.2886
2022-02-24 07:10:50 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.3245
2022-02-24 07:11:23 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.3400
2022-02-24 07:11:56 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.2347
2022-02-24 07:11:58 - train: epoch 096, train_loss: 1.2078
2022-02-24 07:13:13 - eval: epoch: 096, acc1: 72.074%, acc5: 90.762%, test_loss: 1.1099, per_image_load_time: 2.123ms, per_image_inference_time: 0.281ms
2022-02-24 07:13:13 - until epoch: 096, best_acc1: 72.074%
2022-02-24 07:13:13 - epoch 097 lr: 0.00010000000000000003
2022-02-24 07:13:52 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.2363
2022-02-24 07:14:24 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.2718
2022-02-24 07:14:58 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.1909
2022-02-24 07:15:32 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.2943
2022-02-24 07:16:05 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.3102
2022-02-24 07:16:39 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.2111
2022-02-24 07:17:12 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.2288
2022-02-24 07:17:45 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.0391
2022-02-24 07:18:18 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.1268
2022-02-24 07:18:51 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.2875
2022-02-24 07:19:24 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.0606
2022-02-24 07:19:58 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.2735
2022-02-24 07:20:31 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.0554
2022-02-24 07:21:05 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.2605
2022-02-24 07:21:38 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.0451
2022-02-24 07:22:11 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.1903
2022-02-24 07:22:46 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 1.2084
2022-02-24 07:23:18 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.2110
2022-02-24 07:23:52 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.0828
2022-02-24 07:24:25 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.1088
2022-02-24 07:24:58 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.2107
2022-02-24 07:25:31 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.1479
2022-02-24 07:26:05 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.0498
2022-02-24 07:26:38 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.1715
2022-02-24 07:27:12 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.1885
2022-02-24 07:27:45 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.4243
2022-02-24 07:28:18 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.0286
2022-02-24 07:28:52 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.2457
2022-02-24 07:29:26 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.2818
2022-02-24 07:30:00 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.1715
2022-02-24 07:30:33 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.2168
2022-02-24 07:31:06 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.3059
2022-02-24 07:31:40 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.4510
2022-02-24 07:32:13 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.2473
2022-02-24 07:32:46 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.3113
2022-02-24 07:33:20 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.1692
2022-02-24 07:33:52 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.1308
2022-02-24 07:34:26 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.1678
2022-02-24 07:34:58 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.2799
2022-02-24 07:35:32 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.4459
2022-02-24 07:36:05 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.1371
2022-02-24 07:36:38 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.0783
2022-02-24 07:37:11 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.0851
2022-02-24 07:37:45 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.2631
2022-02-24 07:38:17 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.0975
2022-02-24 07:38:51 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.2224
2022-02-24 07:39:24 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.2132
2022-02-24 07:39:58 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.2071
2022-02-24 07:40:32 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.2849
2022-02-24 07:41:04 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.0815
2022-02-24 07:41:06 - train: epoch 097, train_loss: 1.2093
2022-02-24 07:42:22 - eval: epoch: 097, acc1: 72.006%, acc5: 90.772%, test_loss: 1.1088, per_image_load_time: 2.070ms, per_image_inference_time: 0.308ms
2022-02-24 07:42:22 - until epoch: 097, best_acc1: 72.074%
2022-02-24 07:42:22 - epoch 098 lr: 0.00010000000000000003
2022-02-24 07:43:00 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.4173
2022-02-24 07:43:33 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.2723
2022-02-24 07:44:07 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.3222
2022-02-24 07:44:41 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.1739
2022-02-24 07:45:13 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.1610
2022-02-24 07:45:48 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.1539
2022-02-24 07:46:21 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.2083
2022-02-24 07:46:54 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.3140
2022-02-24 07:47:27 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.1510
2022-02-24 07:48:01 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.2000
2022-02-24 07:48:35 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.3202
2022-02-24 07:49:08 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.1612
2022-02-24 07:49:42 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.2893
