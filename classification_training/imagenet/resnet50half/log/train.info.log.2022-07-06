2022-07-06 21:17:04 - network: resnet50half
2022-07-06 21:17:04 - num_classes: 1000
2022-07-06 21:17:04 - input_image_size: 224
2022-07-06 21:17:04 - scale: 1.1428571428571428
2022-07-06 21:17:04 - trained_model_path: 
2022-07-06 21:17:04 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-06 21:17:04 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-06 21:17:04 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f1b05ad7850>
2022-07-06 21:17:04 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f1b05ad7b20>
2022-07-06 21:17:04 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f1b05ad7b50>
2022-07-06 21:17:04 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f1b05ad7bb0>
2022-07-06 21:17:04 - seed: 0
2022-07-06 21:17:04 - batch_size: 256
2022-07-06 21:17:04 - num_workers: 16
2022-07-06 21:17:04 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-06 21:17:04 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-07-06 21:17:04 - epochs: 100
2022-07-06 21:17:04 - print_interval: 100
2022-07-06 21:17:04 - sync_bn: False
2022-07-06 21:17:04 - apex: True
2022-07-06 21:17:04 - use_ema_model: False
2022-07-06 21:17:04 - ema_model_decay: 0.9999
2022-07-06 21:17:04 - gpus_type: NVIDIA RTX A5000
2022-07-06 21:17:04 - gpus_num: 2
2022-07-06 21:17:04 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f1ad2c748f0>
2022-07-06 21:17:04 - --------------------parameters--------------------
2022-07-06 21:17:04 - name: conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-07-06 21:17:04 - name: fc.weight, grad: True
2022-07-06 21:17:04 - name: fc.bias, grad: True
2022-07-06 21:17:04 - --------------------buffers--------------------
2022-07-06 21:17:04 - name: conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-06 21:17:04 - -----------no weight decay layers--------------
2022-07-06 21:17:04 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-06 21:17:04 - -------------weight decay layers---------------
2022-07-06 21:17:04 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-06 21:17:04 - epoch 001 lr: 0.100000
2022-07-06 21:17:43 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9141
2022-07-06 21:18:16 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.9020
2022-07-06 21:18:48 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.8704
2022-07-06 21:19:20 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.7999
2022-07-06 21:19:52 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.7032
2022-07-06 21:20:26 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.4773
2022-07-06 21:20:58 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.5570
2022-07-06 21:21:30 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.3520
2022-07-06 21:22:03 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 6.2921
2022-07-06 21:22:37 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 6.2445
2022-07-06 21:23:10 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 6.1451
2022-07-06 21:23:42 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.9642
2022-07-06 21:24:15 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.9490
2022-07-06 21:24:48 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.9197
2022-07-06 21:25:22 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.7900
2022-07-06 21:25:55 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.7946
2022-07-06 21:26:28 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 5.5779
2022-07-06 21:27:01 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.6383
2022-07-06 21:27:35 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 5.4534
2022-07-06 21:28:07 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 5.3911
2022-07-06 21:28:41 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 5.3690
2022-07-06 21:29:14 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 5.2643
2022-07-06 21:29:46 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 5.1789
2022-07-06 21:30:20 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 5.2092
2022-07-06 21:30:54 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 5.2189
2022-07-06 21:31:26 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 5.3320
2022-07-06 21:32:00 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 5.2807
2022-07-06 21:32:32 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 5.0023
2022-07-06 21:33:06 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.9137
2022-07-06 21:33:39 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 5.0221
2022-07-06 21:34:13 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 5.1369
2022-07-06 21:34:46 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.9950
2022-07-06 21:35:19 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.7043
2022-07-06 21:35:52 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.7174
2022-07-06 21:36:26 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.7668
2022-07-06 21:36:58 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.8176
2022-07-06 21:37:32 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.8785
2022-07-06 21:38:06 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.6587
2022-07-06 21:38:39 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.6967
2022-07-06 21:39:12 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.7043
2022-07-06 21:39:45 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.7426
2022-07-06 21:40:19 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 4.5298
2022-07-06 21:40:52 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 4.5157
2022-07-06 21:41:26 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 4.2252
2022-07-06 21:42:00 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.4319
2022-07-06 21:42:33 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.6583
2022-07-06 21:43:06 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 4.2413
2022-07-06 21:43:40 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.5799
2022-07-06 21:44:13 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 4.3842
2022-07-06 21:44:44 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 4.2969
2022-07-06 21:44:45 - train: epoch 001, train_loss: 5.3563
2022-07-06 21:45:57 - eval: epoch: 001, acc1: 14.174%, acc5: 32.458%, test_loss: 5.5176, per_image_load_time: 1.963ms, per_image_inference_time: 0.273ms
2022-07-06 21:45:58 - until epoch: 001, best_acc1: 14.174%
2022-07-06 21:45:58 - epoch 002 lr: 0.100000
2022-07-06 21:46:36 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.4024
2022-07-06 21:47:09 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 4.1563
2022-07-06 21:47:41 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 4.4907
2022-07-06 21:48:15 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 4.2957
2022-07-06 21:48:48 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 4.0194
2022-07-06 21:49:19 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 4.0785
2022-07-06 21:49:53 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 4.3564
2022-07-06 21:50:26 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.8857
2022-07-06 21:50:59 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.6749
2022-07-06 21:51:32 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 4.2069
2022-07-06 21:52:05 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 4.2508
2022-07-06 21:52:38 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 4.0087
2022-07-06 21:53:11 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 4.0851
2022-07-06 21:53:45 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 4.1231
2022-07-06 21:54:18 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 4.0147
2022-07-06 21:54:52 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.8777
2022-07-06 21:55:24 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.9983
2022-07-06 21:55:57 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 4.0174
2022-07-06 21:56:31 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.8046
2022-07-06 21:57:04 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.6696
2022-07-06 21:57:38 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.8853
2022-07-06 21:58:11 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.5605
2022-07-06 21:58:45 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.7861
2022-07-06 21:59:18 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.6393
2022-07-06 21:59:51 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.7787
2022-07-06 22:00:24 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.6734
2022-07-06 22:00:57 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.8764
2022-07-06 22:01:30 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.6325
2022-07-06 22:02:04 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.6677
2022-07-06 22:02:37 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.5056
2022-07-06 22:03:10 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.6558
2022-07-06 22:03:43 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.6065
2022-07-06 22:04:17 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.6419
2022-07-06 22:04:49 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.8346
2022-07-06 22:05:23 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.6056
2022-07-06 22:05:56 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.5601
2022-07-06 22:06:30 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.7464
2022-07-06 22:07:02 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.4161
2022-07-06 22:07:35 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.5437
2022-07-06 22:08:08 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.5601
2022-07-06 22:08:42 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.5798
2022-07-06 22:09:15 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.5124
2022-07-06 22:09:49 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.3993
2022-07-06 22:10:22 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.3254
2022-07-06 22:10:54 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.3735
2022-07-06 22:11:27 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.3816
2022-07-06 22:12:01 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.4857
2022-07-06 22:12:34 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.5636
2022-07-06 22:13:07 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.3628
2022-07-06 22:13:39 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.3013
2022-07-06 22:13:40 - train: epoch 002, train_loss: 3.7895
2022-07-06 22:14:54 - eval: epoch: 002, acc1: 28.764%, acc5: 53.830%, test_loss: 3.6611, per_image_load_time: 1.944ms, per_image_inference_time: 0.269ms
2022-07-06 22:14:54 - until epoch: 002, best_acc1: 28.764%
2022-07-06 22:14:54 - epoch 003 lr: 0.100000
2022-07-06 22:15:32 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.4721
2022-07-06 22:16:05 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.5391
2022-07-06 22:16:38 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.3635
2022-07-06 22:17:11 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.3152
2022-07-06 22:17:43 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.5363
2022-07-06 22:18:17 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.2725
2022-07-06 22:18:49 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.5038
2022-07-06 22:19:22 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.4704
2022-07-06 22:19:56 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.2675
2022-07-06 22:20:28 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.3139
2022-07-06 22:21:02 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1842
2022-07-06 22:21:34 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.2480
2022-07-06 22:22:08 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.2321
2022-07-06 22:22:40 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.1950
2022-07-06 22:23:14 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.4735
2022-07-06 22:23:47 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.2289
2022-07-06 22:24:20 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.1647
2022-07-06 22:24:53 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.1675
2022-07-06 22:25:27 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.2977
2022-07-06 22:26:00 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.4957
2022-07-06 22:26:33 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4875
2022-07-06 22:27:07 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.7214
2022-07-06 22:27:39 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.2543
2022-07-06 22:28:12 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.0820
2022-07-06 22:28:46 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.1749
2022-07-06 22:29:19 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.2295
2022-07-06 22:29:53 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.5568
2022-07-06 22:30:26 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0965
2022-07-06 22:31:00 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.2231
2022-07-06 22:31:32 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.2595
2022-07-06 22:32:06 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.3679
2022-07-06 22:32:39 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2610
2022-07-06 22:33:12 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1639
2022-07-06 22:33:46 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3153
2022-07-06 22:34:18 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 3.0082
2022-07-06 22:34:52 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.1098
2022-07-06 22:35:26 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.0991
2022-07-06 22:35:58 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.1723
2022-07-06 22:36:32 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.2463
2022-07-06 22:37:05 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1179
2022-07-06 22:37:39 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.3066
2022-07-06 22:38:12 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.0353
2022-07-06 22:38:46 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.7795
2022-07-06 22:39:19 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9196
2022-07-06 22:39:53 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.1386
2022-07-06 22:40:26 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0914
2022-07-06 22:40:59 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.8885
2022-07-06 22:41:33 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.2051
2022-07-06 22:42:06 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.1574
2022-07-06 22:42:38 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.1382
2022-07-06 22:42:39 - train: epoch 003, train_loss: 3.2156
2022-07-06 22:43:52 - eval: epoch: 003, acc1: 38.886%, acc5: 64.820%, test_loss: 2.8012, per_image_load_time: 1.940ms, per_image_inference_time: 0.275ms
2022-07-06 22:43:52 - until epoch: 003, best_acc1: 38.886%
2022-07-06 22:43:52 - epoch 004 lr: 0.100000
2022-07-06 22:44:30 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.0710
2022-07-06 22:45:03 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.9088
2022-07-06 22:45:35 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 3.1420
2022-07-06 22:46:08 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.9597
2022-07-06 22:46:40 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.8597
2022-07-06 22:47:13 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.1849
2022-07-06 22:47:47 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.1136
2022-07-06 22:48:19 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.8910
2022-07-06 22:48:52 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.7489
2022-07-06 22:49:25 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.9525
2022-07-06 22:49:58 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.1265
2022-07-06 22:50:32 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.7957
2022-07-06 22:51:06 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.7546
2022-07-06 22:51:38 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0700
2022-07-06 22:52:12 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.0821
2022-07-06 22:52:45 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.8931
2022-07-06 22:53:18 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.9838
2022-07-06 22:53:52 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.1578
2022-07-06 22:54:25 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.0460
2022-07-06 22:54:59 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.9294
2022-07-06 22:55:32 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9980
2022-07-06 22:56:05 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.9408
2022-07-06 22:56:38 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.7224
2022-07-06 22:57:12 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.7551
2022-07-06 22:57:44 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.9104
2022-07-06 22:58:17 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.0234
2022-07-06 22:58:51 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.6986
2022-07-06 22:59:24 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.9672
2022-07-06 22:59:58 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.8719
2022-07-06 23:00:31 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.8529
2022-07-06 23:01:04 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.9246
2022-07-06 23:01:38 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.9134
2022-07-06 23:02:12 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9701
2022-07-06 23:02:45 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.9331
2022-07-06 23:03:17 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.8696
2022-07-06 23:03:51 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.7510
2022-07-06 23:04:24 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.9150
2022-07-06 23:04:58 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.8541
2022-07-06 23:05:32 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.9042
2022-07-06 23:06:05 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.5776
2022-07-06 23:06:38 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.9282
2022-07-06 23:07:12 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.7885
2022-07-06 23:07:45 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.6466
2022-07-06 23:08:18 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.6658
2022-07-06 23:08:52 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.2847
2022-07-06 23:09:25 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.7881
2022-07-06 23:09:59 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.7460
2022-07-06 23:10:32 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.7449
2022-07-06 23:11:05 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.8244
2022-07-06 23:11:37 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.9747
2022-07-06 23:11:38 - train: epoch 004, train_loss: 2.9306
2022-07-06 23:12:51 - eval: epoch: 004, acc1: 38.386%, acc5: 63.906%, test_loss: 3.5241, per_image_load_time: 2.426ms, per_image_inference_time: 0.252ms
2022-07-06 23:12:51 - until epoch: 004, best_acc1: 38.886%
2022-07-06 23:12:51 - epoch 005 lr: 0.100000
2022-07-06 23:13:29 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.9214
2022-07-06 23:14:02 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 3.0180
2022-07-06 23:14:35 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.8690
2022-07-06 23:15:07 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.8626
2022-07-06 23:15:40 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.6219
2022-07-06 23:16:12 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.8508
2022-07-06 23:16:45 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.8833
2022-07-06 23:17:18 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.9900
2022-07-06 23:17:51 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.7952
2022-07-06 23:18:23 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.8164
2022-07-06 23:18:56 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.8651
2022-07-06 23:19:29 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.8582
2022-07-06 23:20:02 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.7307
2022-07-06 23:20:35 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.8392
2022-07-06 23:21:08 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.6164
2022-07-06 23:21:41 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.5604
2022-07-06 23:22:14 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.7167
2022-07-06 23:22:47 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.8061
2022-07-06 23:23:20 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6220
2022-07-06 23:23:54 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.7764
2022-07-06 23:24:27 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.5447
2022-07-06 23:25:00 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.5594
2022-07-06 23:25:34 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.3718
2022-07-06 23:26:06 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.6706
2022-07-06 23:26:40 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.8788
2022-07-06 23:27:13 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.9384
2022-07-06 23:27:47 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.9180
2022-07-06 23:28:20 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.7305
2022-07-06 23:28:54 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.5869
2022-07-06 23:29:27 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.6191
2022-07-06 23:30:00 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.7539
2022-07-06 23:30:33 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.7925
2022-07-06 23:31:07 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.4686
2022-07-06 23:31:40 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.6296
2022-07-06 23:32:13 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.6556
2022-07-06 23:32:46 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8090
2022-07-06 23:33:19 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.6124
2022-07-06 23:33:53 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.4983
2022-07-06 23:34:26 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.9944
2022-07-06 23:34:59 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.6756
2022-07-06 23:35:32 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.7552
2022-07-06 23:36:05 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.7400
2022-07-06 23:36:39 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.5100
2022-07-06 23:37:12 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.7524
2022-07-06 23:37:46 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.7588
2022-07-06 23:38:19 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.6688
2022-07-06 23:38:52 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.5736
2022-07-06 23:39:25 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.5677
2022-07-06 23:39:59 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8428
2022-07-06 23:40:30 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.7426
2022-07-06 23:40:31 - train: epoch 005, train_loss: 2.7611
2022-07-06 23:41:45 - eval: epoch: 005, acc1: 43.482%, acc5: 70.148%, test_loss: 2.5065, per_image_load_time: 2.237ms, per_image_inference_time: 0.260ms
2022-07-06 23:41:45 - until epoch: 005, best_acc1: 43.482%
2022-07-06 23:41:45 - epoch 006 lr: 0.100000
2022-07-06 23:42:24 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.5997
2022-07-06 23:42:56 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.7060
2022-07-06 23:43:29 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.4995
2022-07-06 23:44:02 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.8269
2022-07-06 23:44:35 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.6354
2022-07-06 23:45:08 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.7386
2022-07-06 23:45:42 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.8195
2022-07-06 23:46:15 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.6706
2022-07-06 23:46:49 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.6704
2022-07-06 23:47:21 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.6290
2022-07-06 23:47:55 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.5816
2022-07-06 23:48:27 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.8067
2022-07-06 23:49:01 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.7413
2022-07-06 23:49:34 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.8610
2022-07-06 23:50:07 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.8143
2022-07-06 23:50:41 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.5595
2022-07-06 23:51:14 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.7679
2022-07-06 23:51:47 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8837
2022-07-06 23:52:20 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.6214
2022-07-06 23:52:53 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.8455
2022-07-06 23:53:27 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.7117
2022-07-06 23:54:00 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.5753
2022-07-06 23:54:33 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.5167
2022-07-06 23:55:06 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.5865
2022-07-06 23:55:39 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.8867
2022-07-06 23:56:13 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.6823
2022-07-06 23:56:46 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.7084
2022-07-06 23:57:19 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.4807
2022-07-06 23:57:52 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.7862
2022-07-06 23:58:26 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.6780
2022-07-06 23:58:59 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.5038
2022-07-06 23:59:32 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.5132
2022-07-07 00:00:06 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.4638
2022-07-07 00:00:39 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.7433
2022-07-07 00:01:12 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.6880
2022-07-07 00:01:46 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.6327
2022-07-07 00:02:18 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.6436
2022-07-07 00:02:51 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.5114
2022-07-07 00:03:25 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5101
2022-07-07 00:03:58 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.8404
2022-07-07 00:04:32 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5278
2022-07-07 00:05:05 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.5329
2022-07-07 00:05:38 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.6031
2022-07-07 00:06:13 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.5721
2022-07-07 00:06:45 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.7000
2022-07-07 00:07:18 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.6160
2022-07-07 00:07:52 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.5592
2022-07-07 00:08:25 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.6051
2022-07-07 00:08:58 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.6251
2022-07-07 00:09:30 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.5746
2022-07-07 00:09:31 - train: epoch 006, train_loss: 2.6564
2022-07-07 00:10:44 - eval: epoch: 006, acc1: 46.766%, acc5: 72.930%, test_loss: 2.3429, per_image_load_time: 2.166ms, per_image_inference_time: 0.269ms
2022-07-07 00:10:44 - until epoch: 006, best_acc1: 46.766%
2022-07-07 00:10:44 - epoch 007 lr: 0.100000
2022-07-07 00:11:23 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.4145
2022-07-07 00:11:56 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.8587
2022-07-07 00:12:28 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.8066
2022-07-07 00:13:00 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.6937
2022-07-07 00:13:34 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.5276
2022-07-07 00:14:06 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.7372
2022-07-07 00:14:40 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.5313
2022-07-07 00:15:13 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.7071
2022-07-07 00:15:46 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.5545
2022-07-07 00:16:19 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.6222
2022-07-07 00:16:52 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.3967
2022-07-07 00:17:26 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.5619
2022-07-07 00:17:59 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.4839
2022-07-07 00:18:32 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.5790
2022-07-07 00:19:05 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.7149
2022-07-07 00:19:39 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.5332
2022-07-07 00:20:13 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.6131
2022-07-07 00:20:46 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.4656
2022-07-07 00:21:20 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.5825
2022-07-07 00:21:53 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.4757
2022-07-07 00:22:27 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.6537
2022-07-07 00:22:59 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.5000
2022-07-07 00:23:33 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.6697
2022-07-07 00:24:06 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.6732
2022-07-07 00:24:40 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.6563
2022-07-07 00:25:12 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.5032
2022-07-07 00:25:46 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.4840
2022-07-07 00:26:20 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6421
2022-07-07 00:26:53 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.5291
2022-07-07 00:27:26 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.6074
2022-07-07 00:28:00 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.4650
2022-07-07 00:28:34 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.5275
2022-07-07 00:29:07 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.7556
2022-07-07 00:29:40 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.4006
2022-07-07 00:30:13 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.5839
2022-07-07 00:30:47 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.4093
2022-07-07 00:31:20 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.4776
2022-07-07 00:31:54 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.7191
2022-07-07 00:32:27 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.4690
2022-07-07 00:33:00 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.7872
2022-07-07 00:33:33 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.4995
2022-07-07 00:34:07 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.4559
2022-07-07 00:34:40 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.7118
2022-07-07 00:35:13 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.3157
2022-07-07 00:35:47 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.7355
2022-07-07 00:36:20 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.7324
2022-07-07 00:36:53 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.6628
2022-07-07 00:37:26 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.9415
2022-07-07 00:38:01 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.5943
2022-07-07 00:38:32 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.5408
2022-07-07 00:38:33 - train: epoch 007, train_loss: 2.5780
2022-07-07 00:39:46 - eval: epoch: 007, acc1: 41.584%, acc5: 67.768%, test_loss: 2.6565, per_image_load_time: 1.875ms, per_image_inference_time: 0.260ms
2022-07-07 00:39:46 - until epoch: 007, best_acc1: 46.766%
2022-07-07 00:39:46 - epoch 008 lr: 0.100000
2022-07-07 00:40:25 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.4150
2022-07-07 00:40:58 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.6738
2022-07-07 00:41:31 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.2486
2022-07-07 00:42:03 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.3843
2022-07-07 00:42:35 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.4069
2022-07-07 00:43:08 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.5234
2022-07-07 00:43:41 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.8383
2022-07-07 00:44:14 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.2912
2022-07-07 00:44:47 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.4987
2022-07-07 00:45:20 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.4447
2022-07-07 00:45:53 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.3535
2022-07-07 00:46:26 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.3624
2022-07-07 00:46:59 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.4420
2022-07-07 00:47:33 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.4583
2022-07-07 00:48:05 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.5193
2022-07-07 00:48:39 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.6158
2022-07-07 00:49:12 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.4388
2022-07-07 00:49:46 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7002
2022-07-07 00:50:17 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.3117
2022-07-07 00:50:51 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.6040
2022-07-07 00:51:25 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.5012
2022-07-07 00:51:58 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.4717
2022-07-07 00:52:30 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.5604
2022-07-07 00:53:04 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.5616
2022-07-07 00:53:37 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.4522
2022-07-07 00:54:11 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.6230
2022-07-07 00:54:44 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.6418
2022-07-07 00:55:17 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.6076
2022-07-07 00:55:49 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.4227
2022-07-07 00:56:22 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.5828
2022-07-07 00:56:55 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.5422
2022-07-07 00:57:30 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.7696
2022-07-07 00:58:02 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.7187
2022-07-07 00:58:35 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.6869
2022-07-07 00:59:08 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.4694
2022-07-07 00:59:42 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7037
2022-07-07 01:00:15 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.5073
2022-07-07 01:00:48 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.4459
2022-07-07 01:01:22 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.6130
2022-07-07 01:01:56 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.7598
2022-07-07 01:02:29 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.4971
2022-07-07 01:03:02 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.4006
2022-07-07 01:03:35 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.2233
2022-07-07 01:04:08 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.3771
2022-07-07 01:04:41 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.7554
2022-07-07 01:05:15 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.6619
2022-07-07 01:05:48 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.4391
2022-07-07 01:06:21 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.4862
2022-07-07 01:06:54 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.6352
2022-07-07 01:07:26 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.4945
2022-07-07 01:07:27 - train: epoch 008, train_loss: 2.5182
2022-07-07 01:08:40 - eval: epoch: 008, acc1: 49.008%, acc5: 74.770%, test_loss: 2.2234, per_image_load_time: 2.468ms, per_image_inference_time: 0.282ms
2022-07-07 01:08:40 - until epoch: 008, best_acc1: 49.008%
2022-07-07 01:08:40 - epoch 009 lr: 0.100000
2022-07-07 01:09:19 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.2039
2022-07-07 01:09:51 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.3763
2022-07-07 01:10:23 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.2144
2022-07-07 01:10:57 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7078
2022-07-07 01:11:29 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.4305
2022-07-07 01:12:02 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.4624
2022-07-07 01:12:35 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.5032
2022-07-07 01:13:07 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.2827
2022-07-07 01:13:41 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.2983
2022-07-07 01:14:14 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.3895
2022-07-07 01:14:47 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.7768
2022-07-07 01:15:21 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.5657
2022-07-07 01:15:54 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.6667
2022-07-07 01:16:27 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.2641
2022-07-07 01:17:00 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.3852
2022-07-07 01:17:33 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.4706
2022-07-07 01:18:06 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.7277
2022-07-07 01:18:40 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.4317
2022-07-07 01:19:13 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.2652
2022-07-07 01:19:47 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.1506
2022-07-07 01:20:19 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.5195
2022-07-07 01:20:53 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.5190
2022-07-07 01:21:26 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.1714
2022-07-07 01:21:59 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.4052
2022-07-07 01:22:32 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.3119
2022-07-07 01:23:05 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.5128
2022-07-07 01:23:38 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.3938
2022-07-07 01:24:12 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.5489
2022-07-07 01:24:44 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.1336
2022-07-07 01:25:17 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.4186
2022-07-07 01:25:51 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.5736
2022-07-07 01:26:24 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.4925
2022-07-07 01:26:58 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.4061
2022-07-07 01:27:31 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.6529
2022-07-07 01:28:04 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.5652
2022-07-07 01:28:37 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.3181
2022-07-07 01:29:12 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.7103
2022-07-07 01:29:44 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.6865
2022-07-07 01:30:17 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.1315
2022-07-07 01:30:50 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.5970
2022-07-07 01:31:24 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.4827
2022-07-07 01:31:57 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.3372
2022-07-07 01:32:30 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.5747
2022-07-07 01:33:04 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.4587
2022-07-07 01:33:37 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.4573
2022-07-07 01:34:10 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.5800
2022-07-07 01:34:43 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.7112
2022-07-07 01:35:16 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.6723
2022-07-07 01:35:50 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.5004
2022-07-07 01:36:21 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.4230
2022-07-07 01:36:22 - train: epoch 009, train_loss: 2.4746
2022-07-07 01:37:35 - eval: epoch: 009, acc1: 46.504%, acc5: 72.600%, test_loss: 2.3594, per_image_load_time: 2.546ms, per_image_inference_time: 0.261ms
2022-07-07 01:37:35 - until epoch: 009, best_acc1: 49.008%
2022-07-07 01:37:35 - epoch 010 lr: 0.100000
2022-07-07 01:38:13 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.5381
2022-07-07 01:38:46 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.6447
2022-07-07 01:39:19 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.4002
2022-07-07 01:39:51 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5645
2022-07-07 01:40:25 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.2880
2022-07-07 01:40:57 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.5640
2022-07-07 01:41:30 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.5133
2022-07-07 01:42:03 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.2572
2022-07-07 01:42:36 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.2185
2022-07-07 01:43:09 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.2734
2022-07-07 01:43:43 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.4277
2022-07-07 01:44:16 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.3455
2022-07-07 01:44:50 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.2871
2022-07-07 01:45:23 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.5196
2022-07-07 01:45:56 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.2088
2022-07-07 01:46:29 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.5730
2022-07-07 01:47:03 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.5698
2022-07-07 01:47:35 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.4347
2022-07-07 01:48:09 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.4021
2022-07-07 01:48:41 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.4595
2022-07-07 01:49:14 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.3048
2022-07-07 01:49:47 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.6209
2022-07-07 01:50:20 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.6989
2022-07-07 01:50:53 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.5498
2022-07-07 01:51:27 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.4290
2022-07-07 01:52:00 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.4808
2022-07-07 01:52:34 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.2326
2022-07-07 01:53:06 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5040
2022-07-07 01:53:40 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.5411
2022-07-07 01:54:13 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.5121
2022-07-07 01:54:46 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.7284
2022-07-07 01:55:20 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.4700
2022-07-07 01:55:54 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.6024
2022-07-07 01:56:26 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.6594
2022-07-07 01:56:59 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.5992
2022-07-07 01:57:33 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.6987
2022-07-07 01:58:06 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.3515
2022-07-07 01:58:40 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.4002
2022-07-07 01:59:13 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.1253
2022-07-07 01:59:47 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.4915
2022-07-07 02:00:20 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.2893
2022-07-07 02:00:53 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.5019
2022-07-07 02:01:26 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.4504
2022-07-07 02:02:00 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.4836
2022-07-07 02:02:34 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.2749
2022-07-07 02:03:06 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.4727
2022-07-07 02:03:40 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.4906
2022-07-07 02:04:14 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.3383
2022-07-07 02:04:47 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.3563
2022-07-07 02:05:19 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.2143
2022-07-07 02:05:19 - train: epoch 010, train_loss: 2.4379
2022-07-07 02:06:33 - eval: epoch: 010, acc1: 47.068%, acc5: 73.254%, test_loss: 2.3222, per_image_load_time: 1.693ms, per_image_inference_time: 0.270ms
2022-07-07 02:06:33 - until epoch: 010, best_acc1: 49.008%
2022-07-07 02:06:33 - epoch 011 lr: 0.100000
2022-07-07 02:07:11 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.1523
2022-07-07 02:07:44 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.5627
2022-07-07 02:08:16 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.1452
2022-07-07 02:08:50 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.6047
2022-07-07 02:09:23 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.3777
2022-07-07 02:09:56 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.4857
2022-07-07 02:10:28 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4530
2022-07-07 02:11:01 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.3978
2022-07-07 02:11:34 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.4880
2022-07-07 02:12:07 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.4009
2022-07-07 02:12:40 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.4986
2022-07-07 02:13:13 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.7316
2022-07-07 02:13:46 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.5466
2022-07-07 02:14:19 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.4804
2022-07-07 02:14:51 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.2090
2022-07-07 02:15:24 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.4431
2022-07-07 02:15:58 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.4597
2022-07-07 02:16:30 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.2582
2022-07-07 02:17:04 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.1710
2022-07-07 02:17:37 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.5842
2022-07-07 02:18:10 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.4378
2022-07-07 02:18:43 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.3806
2022-07-07 02:19:17 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.5899
2022-07-07 02:19:50 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.2165
2022-07-07 02:20:23 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.6150
2022-07-07 02:20:56 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.4190
2022-07-07 02:21:30 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.4063
2022-07-07 02:22:02 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.1086
2022-07-07 02:22:36 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.4515
2022-07-07 02:23:10 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.5999
2022-07-07 02:23:43 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.3854
2022-07-07 02:24:15 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.2231
2022-07-07 02:24:49 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.5140
2022-07-07 02:25:22 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.2990
2022-07-07 02:25:54 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.4456
2022-07-07 02:26:29 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.4175
2022-07-07 02:27:02 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.5625
2022-07-07 02:27:35 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.1417
2022-07-07 02:28:08 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.4915
2022-07-07 02:28:41 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.4390
2022-07-07 02:29:15 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.1819
2022-07-07 02:29:48 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.3223
2022-07-07 02:30:22 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4459
2022-07-07 02:30:56 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.3234
2022-07-07 02:31:28 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.3451
2022-07-07 02:32:02 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.3327
2022-07-07 02:32:35 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.1750
2022-07-07 02:33:08 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.2147
2022-07-07 02:33:42 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.2632
2022-07-07 02:34:14 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.2972
2022-07-07 02:34:14 - train: epoch 011, train_loss: 2.4056
2022-07-07 02:35:28 - eval: epoch: 011, acc1: 48.810%, acc5: 74.842%, test_loss: 2.2311, per_image_load_time: 2.555ms, per_image_inference_time: 0.273ms
2022-07-07 02:35:28 - until epoch: 011, best_acc1: 49.008%
2022-07-07 02:35:28 - epoch 012 lr: 0.100000
2022-07-07 02:36:07 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.3440
2022-07-07 02:36:40 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4069
2022-07-07 02:37:12 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.3107
2022-07-07 02:37:46 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.5055
2022-07-07 02:38:19 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.5751
2022-07-07 02:38:51 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.0513
2022-07-07 02:39:24 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.1971
2022-07-07 02:39:58 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.3304
2022-07-07 02:40:30 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.4944
2022-07-07 02:41:04 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.1071
2022-07-07 02:41:37 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8232
2022-07-07 02:42:10 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.2444
2022-07-07 02:42:43 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.3150
2022-07-07 02:43:17 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.5302
2022-07-07 02:43:50 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.2060
2022-07-07 02:44:23 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.3248
2022-07-07 02:44:56 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.2413
2022-07-07 02:45:29 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.3512
2022-07-07 02:46:03 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.4490
2022-07-07 02:46:35 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6308
2022-07-07 02:47:09 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.3395
2022-07-07 02:47:41 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.5982
2022-07-07 02:48:15 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.4546
2022-07-07 02:48:49 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.4900
2022-07-07 02:49:22 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.2165
2022-07-07 02:49:55 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.0956
2022-07-07 02:50:28 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.3582
2022-07-07 02:51:01 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.3675
2022-07-07 02:51:34 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.1771
2022-07-07 02:52:07 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.4110
2022-07-07 02:52:40 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.4977
2022-07-07 02:53:13 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.0971
2022-07-07 02:53:47 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3332
2022-07-07 02:54:20 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.3009
2022-07-07 02:54:52 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.5476
2022-07-07 02:55:27 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.3833
2022-07-07 02:55:58 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.4245
2022-07-07 02:56:32 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.4518
2022-07-07 02:57:05 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.4003
2022-07-07 02:57:38 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.2865
2022-07-07 02:58:12 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.2183
2022-07-07 02:58:45 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.2246
2022-07-07 02:59:19 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.3589
2022-07-07 02:59:52 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.2275
2022-07-07 03:00:25 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.3080
2022-07-07 03:00:58 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.7793
2022-07-07 03:01:32 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.3215
2022-07-07 03:02:05 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.5134
2022-07-07 03:02:38 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.4035
2022-07-07 03:03:09 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.1305
2022-07-07 03:03:10 - train: epoch 012, train_loss: 2.3826
2022-07-07 03:04:23 - eval: epoch: 012, acc1: 50.236%, acc5: 75.846%, test_loss: 2.1563, per_image_load_time: 1.630ms, per_image_inference_time: 0.270ms
2022-07-07 03:04:23 - until epoch: 012, best_acc1: 50.236%
2022-07-07 03:04:23 - epoch 013 lr: 0.100000
2022-07-07 03:05:01 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.1123
2022-07-07 03:05:34 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.2813
2022-07-07 03:06:07 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.2526
2022-07-07 03:06:40 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.2905
2022-07-07 03:07:13 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.3609
2022-07-07 03:07:45 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.5245
2022-07-07 03:08:17 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.2284
2022-07-07 03:08:51 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.4365
2022-07-07 03:09:23 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.3117
2022-07-07 03:09:57 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.4698
2022-07-07 03:10:30 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.4206
2022-07-07 03:11:04 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.4443
2022-07-07 03:11:36 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.4393
2022-07-07 03:12:10 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.1740
2022-07-07 03:12:44 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.4853
2022-07-07 03:13:16 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1322
2022-07-07 03:13:50 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.3615
2022-07-07 03:14:23 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.3739
2022-07-07 03:14:57 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.3734
2022-07-07 03:15:30 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.5936
2022-07-07 03:16:03 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.6042
2022-07-07 03:16:36 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3384
2022-07-07 03:17:10 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.4080
2022-07-07 03:17:43 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.4503
2022-07-07 03:18:17 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.2668
2022-07-07 03:18:50 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.2535
2022-07-07 03:19:23 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.2667
2022-07-07 03:19:57 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4748
2022-07-07 03:20:29 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.4823
2022-07-07 03:21:02 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.2856
2022-07-07 03:21:35 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.1855
2022-07-07 03:22:08 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.4460
2022-07-07 03:22:41 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.1268
2022-07-07 03:23:15 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.2276
2022-07-07 03:23:47 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.2085
2022-07-07 03:24:20 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6751
2022-07-07 03:24:54 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.0979
2022-07-07 03:25:26 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.4654
2022-07-07 03:26:01 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.4263
2022-07-07 03:26:34 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.3903
2022-07-07 03:27:07 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.3244
2022-07-07 03:27:40 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.3622
2022-07-07 03:28:14 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.2960
2022-07-07 03:28:46 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.2719
2022-07-07 03:29:19 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.2392
2022-07-07 03:29:53 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.3488
2022-07-07 03:30:27 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.4409
2022-07-07 03:30:59 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.4000
2022-07-07 03:31:34 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.4541
2022-07-07 03:32:05 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.4649
2022-07-07 03:32:06 - train: epoch 013, train_loss: 2.3620
2022-07-07 03:33:20 - eval: epoch: 013, acc1: 48.346%, acc5: 74.212%, test_loss: 2.2751, per_image_load_time: 2.548ms, per_image_inference_time: 0.280ms
2022-07-07 03:33:20 - until epoch: 013, best_acc1: 50.236%
2022-07-07 03:33:20 - epoch 014 lr: 0.100000
2022-07-07 03:34:00 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.3605
2022-07-07 03:34:32 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.3391
2022-07-07 03:35:05 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.9895
2022-07-07 03:35:38 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.3003
2022-07-07 03:36:11 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.2683
2022-07-07 03:36:44 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.3809
2022-07-07 03:37:17 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.2598
2022-07-07 03:37:51 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.3689
2022-07-07 03:38:24 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.4654
2022-07-07 03:38:57 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.5121
2022-07-07 03:39:31 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.2586
2022-07-07 03:40:04 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.3055
2022-07-07 03:40:37 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.3739
2022-07-07 03:41:10 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.4444
2022-07-07 03:41:43 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.4937
2022-07-07 03:42:16 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.2638
2022-07-07 03:42:49 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.5552
2022-07-07 03:43:23 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.5765
2022-07-07 03:43:56 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2651
2022-07-07 03:44:29 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3359
2022-07-07 03:45:02 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.5033
2022-07-07 03:45:36 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.4182
2022-07-07 03:46:09 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.2922
2022-07-07 03:46:42 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.4899
2022-07-07 03:47:15 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.2951
2022-07-07 03:47:48 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.3153
2022-07-07 03:48:21 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.2615
2022-07-07 03:48:55 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.6455
2022-07-07 03:49:28 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.2665
2022-07-07 03:50:02 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.3969
2022-07-07 03:50:34 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.1996
2022-07-07 03:51:07 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.2693
2022-07-07 03:51:41 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.1675
2022-07-07 03:52:15 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.2414
2022-07-07 03:52:47 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.3242
2022-07-07 03:53:21 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.2118
2022-07-07 03:53:54 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.2777
2022-07-07 03:54:27 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.5456
2022-07-07 03:55:01 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.2354
2022-07-07 03:55:34 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.4105
2022-07-07 03:56:08 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.2650
2022-07-07 03:56:41 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.2285
2022-07-07 03:57:15 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.1446
2022-07-07 03:57:47 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.1608
2022-07-07 03:58:21 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.3595
2022-07-07 03:58:53 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.2611
2022-07-07 03:59:27 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.2406
2022-07-07 04:00:00 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.2164
2022-07-07 04:00:33 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.1645
2022-07-07 04:01:05 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.3600
2022-07-07 04:01:06 - train: epoch 014, train_loss: 2.3418
2022-07-07 04:02:20 - eval: epoch: 014, acc1: 48.558%, acc5: 74.950%, test_loss: 2.2413, per_image_load_time: 2.580ms, per_image_inference_time: 0.251ms
2022-07-07 04:02:20 - until epoch: 014, best_acc1: 50.236%
2022-07-07 04:02:20 - epoch 015 lr: 0.100000
2022-07-07 04:02:57 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.1495
2022-07-07 04:03:31 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.4486
2022-07-07 04:04:03 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.4386
2022-07-07 04:04:36 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.2452
2022-07-07 04:05:10 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.3062
2022-07-07 04:05:42 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.3515
2022-07-07 04:06:15 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.2130
2022-07-07 04:06:49 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.0773
2022-07-07 04:07:22 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.1916
2022-07-07 04:07:55 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.3501
2022-07-07 04:08:28 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.1793
2022-07-07 04:09:02 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.2263
2022-07-07 04:09:35 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5962
2022-07-07 04:10:09 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.2476
2022-07-07 04:10:42 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.0646
2022-07-07 04:11:15 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.3250
2022-07-07 04:11:48 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.4853
2022-07-07 04:12:21 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.2334
2022-07-07 04:12:54 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.2317
2022-07-07 04:13:28 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.3072
2022-07-07 04:14:01 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.1742
2022-07-07 04:14:34 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.5199
2022-07-07 04:15:07 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.1210
2022-07-07 04:15:41 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.4683
2022-07-07 04:16:13 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.3586
2022-07-07 04:16:46 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.0795
2022-07-07 04:17:19 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.2205
2022-07-07 04:17:53 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.4481
2022-07-07 04:18:26 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.3020
2022-07-07 04:19:00 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.2045
2022-07-07 04:19:33 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.1994
2022-07-07 04:20:07 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3313
2022-07-07 04:20:40 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.1131
2022-07-07 04:21:14 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.3171
2022-07-07 04:21:46 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.4797
2022-07-07 04:22:20 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.2944
2022-07-07 04:22:53 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.2899
2022-07-07 04:23:27 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.2744
2022-07-07 04:23:59 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5894
2022-07-07 04:24:33 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.3060
2022-07-07 04:25:07 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.4950
2022-07-07 04:25:39 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.0680
2022-07-07 04:26:13 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.3662
2022-07-07 04:26:46 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.3250
2022-07-07 04:27:20 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.3508
2022-07-07 04:27:53 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.1891
2022-07-07 04:28:26 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.4792
2022-07-07 04:29:00 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.2568
2022-07-07 04:29:33 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.2751
2022-07-07 04:30:04 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.4486
2022-07-07 04:30:05 - train: epoch 015, train_loss: 2.3279
2022-07-07 04:31:18 - eval: epoch: 015, acc1: 50.660%, acc5: 76.032%, test_loss: 2.1606, per_image_load_time: 1.484ms, per_image_inference_time: 0.301ms
2022-07-07 04:31:18 - until epoch: 015, best_acc1: 50.660%
2022-07-07 04:31:18 - epoch 016 lr: 0.100000
2022-07-07 04:31:56 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.2129
2022-07-07 04:32:29 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.1776
2022-07-07 04:33:02 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.3117
2022-07-07 04:33:35 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.5165
2022-07-07 04:34:08 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.1140
2022-07-07 04:34:41 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.3774
2022-07-07 04:35:14 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.0594
2022-07-07 04:35:47 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.2234
2022-07-07 04:36:20 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.4329
2022-07-07 04:36:52 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.1506
2022-07-07 04:37:25 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.2195
2022-07-07 04:37:59 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2831
2022-07-07 04:38:32 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.3704
2022-07-07 04:39:05 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.2444
2022-07-07 04:39:39 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.3521
2022-07-07 04:40:13 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.4600
2022-07-07 04:40:45 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.2226
2022-07-07 04:41:19 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.1667
2022-07-07 04:41:51 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.3106
2022-07-07 04:42:25 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.0408
2022-07-07 04:42:58 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.3566
2022-07-07 04:43:31 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.3821
2022-07-07 04:44:04 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.5851
2022-07-07 04:44:38 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.5531
2022-07-07 04:45:10 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.2163
2022-07-07 04:45:44 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.4670
2022-07-07 04:46:17 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.2552
2022-07-07 04:46:51 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.1363
2022-07-07 04:47:24 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.5055
2022-07-07 04:47:58 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.6272
2022-07-07 04:48:31 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.4858
2022-07-07 04:49:04 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.4747
2022-07-07 04:49:37 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.3472
2022-07-07 04:50:11 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.1744
2022-07-07 04:50:44 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3082
2022-07-07 04:51:17 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.1778
2022-07-07 04:51:50 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.4420
2022-07-07 04:52:23 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.7353
2022-07-07 04:52:58 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.3414
2022-07-07 04:53:30 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.4618
2022-07-07 04:54:03 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.3088
2022-07-07 04:54:37 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4171
2022-07-07 04:55:10 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.1319
2022-07-07 04:55:45 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.2188
2022-07-07 04:56:17 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.4346
2022-07-07 04:56:51 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.2227
2022-07-07 04:57:24 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.6450
2022-07-07 04:57:58 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.3356
2022-07-07 04:58:31 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.4475
2022-07-07 04:59:02 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5353
2022-07-07 04:59:03 - train: epoch 016, train_loss: 2.3131
2022-07-07 05:00:17 - eval: epoch: 016, acc1: 48.206%, acc5: 73.814%, test_loss: 2.2874, per_image_load_time: 2.632ms, per_image_inference_time: 0.248ms
2022-07-07 05:00:17 - until epoch: 016, best_acc1: 50.660%
2022-07-07 05:00:17 - epoch 017 lr: 0.100000
2022-07-07 05:00:56 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.1464
2022-07-07 05:01:28 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.3336
2022-07-07 05:02:01 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.4928
2022-07-07 05:02:34 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.0248
2022-07-07 05:03:07 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.2270
2022-07-07 05:03:39 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.6648
2022-07-07 05:04:12 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.3082
2022-07-07 05:04:45 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.2197
2022-07-07 05:05:18 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.2387
2022-07-07 05:05:51 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.2730
2022-07-07 05:06:24 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.6159
2022-07-07 05:06:57 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.3987
2022-07-07 05:07:30 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.2693
2022-07-07 05:08:04 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4158
2022-07-07 05:08:37 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.0000
2022-07-07 05:09:11 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.1173
2022-07-07 05:09:43 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.2014
2022-07-07 05:10:16 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.3808
2022-07-07 05:10:49 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.1982
2022-07-07 05:11:23 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.4186
2022-07-07 05:11:56 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.3345
2022-07-07 05:12:29 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.2902
2022-07-07 05:13:02 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.2312
2022-07-07 05:13:35 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.1539
2022-07-07 05:14:08 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.4342
2022-07-07 05:14:42 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.2360
2022-07-07 05:15:14 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.2060
2022-07-07 05:15:48 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.5813
2022-07-07 05:16:21 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.5448
2022-07-07 05:16:54 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.0537
2022-07-07 05:17:28 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.5174
2022-07-07 05:18:01 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.1434
2022-07-07 05:18:34 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.3609
2022-07-07 05:19:07 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.0708
2022-07-07 05:19:40 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.2881
2022-07-07 05:20:14 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.5142
2022-07-07 05:20:47 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3176
2022-07-07 05:21:19 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.3859
2022-07-07 05:21:53 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.1880
2022-07-07 05:22:27 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.1598
2022-07-07 05:23:00 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.3111
2022-07-07 05:23:33 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.3871
2022-07-07 05:24:06 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.3271
2022-07-07 05:24:40 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.3039
2022-07-07 05:25:13 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.4119
2022-07-07 05:25:46 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.2868
2022-07-07 05:26:20 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.3484
2022-07-07 05:26:53 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.4110
2022-07-07 05:27:26 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.0358
2022-07-07 05:27:57 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.1049
2022-07-07 05:27:58 - train: epoch 017, train_loss: 2.2989
2022-07-07 05:29:11 - eval: epoch: 017, acc1: 51.870%, acc5: 77.370%, test_loss: 2.0718, per_image_load_time: 2.179ms, per_image_inference_time: 0.277ms
2022-07-07 05:29:12 - until epoch: 017, best_acc1: 51.870%
2022-07-07 05:29:12 - epoch 018 lr: 0.100000
2022-07-07 05:29:50 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.2236
2022-07-07 05:30:24 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.2770
2022-07-07 05:30:56 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.4783
2022-07-07 05:31:28 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.3914
2022-07-07 05:32:02 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.2756
2022-07-07 05:32:34 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.3745
2022-07-07 05:33:07 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.0560
2022-07-07 05:33:40 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.2536
2022-07-07 05:34:13 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4392
2022-07-07 05:34:46 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.1454
2022-07-07 05:35:19 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.5341
2022-07-07 05:35:52 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.3221
2022-07-07 05:36:25 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.6871
2022-07-07 05:36:58 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.2898
2022-07-07 05:37:30 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.5335
2022-07-07 05:38:04 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.2013
2022-07-07 05:38:37 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.1787
2022-07-07 05:39:11 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.0823
2022-07-07 05:39:43 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.2572
2022-07-07 05:40:17 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.5209
2022-07-07 05:40:50 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.4777
2022-07-07 05:41:24 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.2972
2022-07-07 05:41:56 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.2640
2022-07-07 05:42:30 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.1855
2022-07-07 05:43:03 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.9759
2022-07-07 05:43:38 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.1802
2022-07-07 05:44:09 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.3895
2022-07-07 05:44:43 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.0065
2022-07-07 05:45:16 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.2423
2022-07-07 05:45:49 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2326
2022-07-07 05:46:23 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.5940
2022-07-07 05:46:56 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.1919
2022-07-07 05:47:29 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.1715
2022-07-07 05:48:02 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.2985
2022-07-07 05:48:36 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5136
2022-07-07 05:49:09 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.3337
2022-07-07 05:49:42 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.6274
2022-07-07 05:50:16 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.4428
2022-07-07 05:50:50 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.3989
2022-07-07 05:51:23 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.1164
2022-07-07 05:51:55 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.1399
2022-07-07 05:52:30 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.2607
2022-07-07 05:53:03 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.0415
2022-07-07 05:53:36 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.3981
2022-07-07 05:54:09 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.2877
2022-07-07 05:54:44 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.0741
2022-07-07 05:55:16 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.5177
2022-07-07 05:55:50 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.4388
2022-07-07 05:56:23 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.2086
2022-07-07 05:56:55 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.4717
2022-07-07 05:56:55 - train: epoch 018, train_loss: 2.2857
2022-07-07 05:58:09 - eval: epoch: 018, acc1: 52.604%, acc5: 77.760%, test_loss: 2.0520, per_image_load_time: 0.825ms, per_image_inference_time: 0.259ms
2022-07-07 05:58:10 - until epoch: 018, best_acc1: 52.604%
2022-07-07 05:58:10 - epoch 019 lr: 0.100000
2022-07-07 05:58:49 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.0699
2022-07-07 05:59:20 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.4863
2022-07-07 05:59:53 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.5335
2022-07-07 06:00:26 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.2178
2022-07-07 06:00:59 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.1769
2022-07-07 06:01:32 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.1504
2022-07-07 06:02:06 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.0569
2022-07-07 06:02:38 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.3706
2022-07-07 06:03:11 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.2783
2022-07-07 06:03:44 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.3508
2022-07-07 06:04:17 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.2085
2022-07-07 06:04:51 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.2374
2022-07-07 06:05:24 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.3649
2022-07-07 06:05:57 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.2111
2022-07-07 06:06:29 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.6423
2022-07-07 06:07:02 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.2019
2022-07-07 06:07:36 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.3032
2022-07-07 06:08:10 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.1940
2022-07-07 06:08:42 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.4341
2022-07-07 06:09:17 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.2738
2022-07-07 06:09:50 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.1780
2022-07-07 06:10:24 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.3051
2022-07-07 06:10:56 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.2902
2022-07-07 06:11:30 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.3804
2022-07-07 06:12:02 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.1735
2022-07-07 06:12:37 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.2831
2022-07-07 06:13:09 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.3424
2022-07-07 06:13:42 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.2719
2022-07-07 06:14:16 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.2500
2022-07-07 06:14:49 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.5071
2022-07-07 06:15:23 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.2869
2022-07-07 06:15:55 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.9712
2022-07-07 06:16:29 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.1268
2022-07-07 06:17:02 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.2996
2022-07-07 06:17:36 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.2653
2022-07-07 06:18:09 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.0997
2022-07-07 06:18:42 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.3816
2022-07-07 06:19:15 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.4913
2022-07-07 06:19:49 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.2175
2022-07-07 06:20:22 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.1259
2022-07-07 06:20:56 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.3057
2022-07-07 06:21:29 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.2509
2022-07-07 06:22:02 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.1490
2022-07-07 06:22:35 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.4847
2022-07-07 06:23:09 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.6968
2022-07-07 06:23:42 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.2998
2022-07-07 06:24:14 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.1847
2022-07-07 06:24:48 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3448
2022-07-07 06:25:21 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.2888
2022-07-07 06:25:53 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.2983
2022-07-07 06:25:54 - train: epoch 019, train_loss: 2.2802
2022-07-07 06:27:07 - eval: epoch: 019, acc1: 52.070%, acc5: 77.336%, test_loss: 2.0679, per_image_load_time: 2.232ms, per_image_inference_time: 0.280ms
2022-07-07 06:27:07 - until epoch: 019, best_acc1: 52.604%
2022-07-07 06:27:07 - epoch 020 lr: 0.100000
2022-07-07 06:27:45 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.3819
2022-07-07 06:28:18 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.0804
2022-07-07 06:28:51 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.3074
2022-07-07 06:29:24 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.0588
2022-07-07 06:29:57 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.2030
2022-07-07 06:30:30 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.3851
2022-07-07 06:31:03 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.0116
2022-07-07 06:31:36 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.2052
2022-07-07 06:32:09 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.6148
2022-07-07 06:32:42 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.3232
2022-07-07 06:33:16 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.2674
2022-07-07 06:33:50 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.0205
2022-07-07 06:34:22 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.1836
2022-07-07 06:34:55 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.3877
2022-07-07 06:35:29 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.3346
2022-07-07 06:36:02 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.0887
2022-07-07 06:36:36 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.9561
2022-07-07 06:37:08 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.2387
2022-07-07 06:37:42 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.1092
2022-07-07 06:38:15 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.3245
2022-07-07 06:38:48 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.3860
2022-07-07 06:39:21 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.0738
2022-07-07 06:39:55 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.0919
2022-07-07 06:40:28 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.4247
2022-07-07 06:41:01 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.1211
2022-07-07 06:41:34 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.0777
2022-07-07 06:42:07 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.3520
2022-07-07 06:42:41 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.3350
2022-07-07 06:43:14 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.4324
2022-07-07 06:43:47 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.1382
2022-07-07 06:44:20 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.3738
2022-07-07 06:44:53 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.4740
2022-07-07 06:45:26 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.1513
2022-07-07 06:46:00 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.2818
2022-07-07 06:46:32 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.1098
2022-07-07 06:47:06 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.2814
2022-07-07 06:47:39 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.2069
2022-07-07 06:48:13 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.1806
2022-07-07 06:48:46 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.3342
2022-07-07 06:49:19 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.1400
2022-07-07 06:49:52 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.2102
2022-07-07 06:50:26 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.1406
2022-07-07 06:50:58 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.3353
2022-07-07 06:51:32 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.3261
2022-07-07 06:52:05 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.2610
2022-07-07 06:52:39 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.3200
2022-07-07 06:53:12 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.1190
2022-07-07 06:53:45 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.3392
2022-07-07 06:54:18 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.3597
2022-07-07 06:54:50 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.0232
2022-07-07 06:54:51 - train: epoch 020, train_loss: 2.2660
2022-07-07 06:56:04 - eval: epoch: 020, acc1: 51.898%, acc5: 77.422%, test_loss: 2.0752, per_image_load_time: 2.090ms, per_image_inference_time: 0.296ms
2022-07-07 06:56:04 - until epoch: 020, best_acc1: 52.604%
2022-07-07 06:56:04 - epoch 021 lr: 0.100000
2022-07-07 06:56:42 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.0939
2022-07-07 06:57:17 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.3017
2022-07-07 06:57:48 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.8251
2022-07-07 06:58:21 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.3192
2022-07-07 06:58:54 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.1322
2022-07-07 06:59:27 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.0429
2022-07-07 07:00:00 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.0861
2022-07-07 07:00:33 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.3476
2022-07-07 07:01:05 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.2995
2022-07-07 07:01:39 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.1645
2022-07-07 07:02:12 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.1596
2022-07-07 07:02:45 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.1416
2022-07-07 07:03:17 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.1038
2022-07-07 07:03:51 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.1494
2022-07-07 07:04:24 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.1217
2022-07-07 07:04:57 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.3233
2022-07-07 07:05:30 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.3005
2022-07-07 07:06:04 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.0624
2022-07-07 07:06:37 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.4818
2022-07-07 07:07:10 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.4029
2022-07-07 07:07:43 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1008
2022-07-07 07:08:16 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.1725
2022-07-07 07:08:50 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.1331
2022-07-07 07:09:22 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.1797
2022-07-07 07:09:56 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.2718
2022-07-07 07:10:30 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.4336
2022-07-07 07:11:03 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.1280
2022-07-07 07:11:37 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.2473
2022-07-07 07:12:09 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.2203
2022-07-07 07:12:43 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.4867
2022-07-07 07:13:16 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.3431
2022-07-07 07:13:50 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.2994
2022-07-07 07:14:22 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.4534
2022-07-07 07:14:56 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.4260
2022-07-07 07:15:30 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.1379
2022-07-07 07:16:04 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.1554
2022-07-07 07:16:37 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2934
2022-07-07 07:17:11 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.2175
2022-07-07 07:17:44 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.1585
2022-07-07 07:18:18 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.4791
2022-07-07 07:18:51 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.1924
2022-07-07 07:19:24 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.2199
2022-07-07 07:19:57 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.2230
2022-07-07 07:20:31 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.3823
2022-07-07 07:21:03 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.4982
2022-07-07 07:21:37 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.1813
2022-07-07 07:22:10 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.5115
2022-07-07 07:22:44 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.4687
2022-07-07 07:23:17 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.0223
2022-07-07 07:23:49 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.1670
2022-07-07 07:23:50 - train: epoch 021, train_loss: 2.2598
2022-07-07 07:25:04 - eval: epoch: 021, acc1: 50.128%, acc5: 76.096%, test_loss: 2.1622, per_image_load_time: 1.384ms, per_image_inference_time: 0.249ms
2022-07-07 07:25:04 - until epoch: 021, best_acc1: 52.604%
2022-07-07 07:25:04 - epoch 022 lr: 0.100000
2022-07-07 07:25:42 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.9477
2022-07-07 07:26:14 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1296
2022-07-07 07:26:47 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.9489
2022-07-07 07:27:20 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.1169
2022-07-07 07:27:54 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3060
2022-07-07 07:28:27 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.3603
2022-07-07 07:29:00 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.3760
2022-07-07 07:29:32 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.3542
2022-07-07 07:30:06 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4027
2022-07-07 07:30:38 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.2941
2022-07-07 07:31:12 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.3364
2022-07-07 07:31:45 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.0212
2022-07-07 07:32:19 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.2263
2022-07-07 07:32:52 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.2635
2022-07-07 07:33:25 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.1294
2022-07-07 07:33:59 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.0692
2022-07-07 07:34:32 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.1041
2022-07-07 07:35:05 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.3524
2022-07-07 07:35:38 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.0752
2022-07-07 07:36:11 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.2096
2022-07-07 07:36:43 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.3677
2022-07-07 07:37:17 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.0878
2022-07-07 07:37:51 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.4074
2022-07-07 07:38:23 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.3361
2022-07-07 07:38:57 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.2070
2022-07-07 07:39:30 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.9578
2022-07-07 07:40:02 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.1237
2022-07-07 07:40:36 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.5806
2022-07-07 07:41:08 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.1373
2022-07-07 07:41:42 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.3016
2022-07-07 07:42:15 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.4092
2022-07-07 07:42:49 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.3244
2022-07-07 07:43:22 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.2001
2022-07-07 07:43:55 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.1036
2022-07-07 07:44:28 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.3301
2022-07-07 07:45:02 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.3163
2022-07-07 07:45:35 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.3746
2022-07-07 07:46:08 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.4280
2022-07-07 07:46:41 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.1729
2022-07-07 07:47:15 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.3083
2022-07-07 07:47:49 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.2554
2022-07-07 07:48:22 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.2127
2022-07-07 07:48:56 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.4879
2022-07-07 07:49:28 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.3012
2022-07-07 07:50:02 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.2585
2022-07-07 07:50:35 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.5194
2022-07-07 07:51:08 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.3638
2022-07-07 07:51:41 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.9598
2022-07-07 07:52:14 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.0903
2022-07-07 07:52:47 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.1694
2022-07-07 07:52:48 - train: epoch 022, train_loss: 2.2521
2022-07-07 07:54:01 - eval: epoch: 022, acc1: 49.966%, acc5: 75.478%, test_loss: 2.1846, per_image_load_time: 2.550ms, per_image_inference_time: 0.265ms
2022-07-07 07:54:01 - until epoch: 022, best_acc1: 52.604%
2022-07-07 07:54:01 - epoch 023 lr: 0.100000
2022-07-07 07:54:39 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.1286
2022-07-07 07:55:13 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.8953
2022-07-07 07:55:46 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.1025
2022-07-07 07:56:19 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.2864
2022-07-07 07:56:52 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.2584
2022-07-07 07:57:25 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.1256
2022-07-07 07:57:58 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.9491
2022-07-07 07:58:31 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.1534
2022-07-07 07:59:05 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.2970
2022-07-07 07:59:38 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.0966
2022-07-07 08:00:10 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.2897
2022-07-07 08:00:44 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.0552
2022-07-07 08:01:17 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.1583
2022-07-07 08:01:49 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3077
2022-07-07 08:02:23 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.0626
2022-07-07 08:02:57 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.2098
2022-07-07 08:03:29 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.3768
2022-07-07 08:04:02 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.1166
2022-07-07 08:04:35 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.2486
2022-07-07 08:05:09 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.0206
2022-07-07 08:05:42 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.3753
2022-07-07 08:06:15 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.9909
2022-07-07 08:06:48 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.1274
2022-07-07 08:07:21 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.3008
2022-07-07 08:07:54 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.3013
2022-07-07 08:08:27 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.3115
2022-07-07 08:09:00 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.1860
2022-07-07 08:09:33 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.3708
2022-07-07 08:10:06 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.3516
2022-07-07 08:10:40 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.4192
2022-07-07 08:11:13 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.3928
2022-07-07 08:11:47 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.4110
2022-07-07 08:12:18 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.2769
2022-07-07 08:12:53 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.3821
2022-07-07 08:13:25 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.2668
2022-07-07 08:13:58 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.9781
2022-07-07 08:14:32 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.2330
2022-07-07 08:15:05 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.3120
2022-07-07 08:15:37 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.1881
2022-07-07 08:16:11 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.1981
2022-07-07 08:16:44 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.1425
2022-07-07 08:17:17 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.1326
2022-07-07 08:17:51 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.9646
2022-07-07 08:18:24 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.0173
2022-07-07 08:18:57 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.0861
2022-07-07 08:19:30 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.3836
2022-07-07 08:20:02 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.0974
2022-07-07 08:20:36 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.1754
2022-07-07 08:21:09 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.1366
2022-07-07 08:21:41 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.3595
2022-07-07 08:21:42 - train: epoch 023, train_loss: 2.2457
2022-07-07 08:22:56 - eval: epoch: 023, acc1: 47.328%, acc5: 73.308%, test_loss: 2.3290, per_image_load_time: 1.203ms, per_image_inference_time: 0.250ms
2022-07-07 08:22:56 - until epoch: 023, best_acc1: 52.604%
2022-07-07 08:22:56 - epoch 024 lr: 0.100000
2022-07-07 08:23:33 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.1320
2022-07-07 08:24:08 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.1813
2022-07-07 08:24:39 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.2186
2022-07-07 08:25:12 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.3602
2022-07-07 08:25:45 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.1151
2022-07-07 08:26:18 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 1.9965
2022-07-07 08:26:51 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.1669
2022-07-07 08:27:24 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.1424
2022-07-07 08:27:57 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.2012
2022-07-07 08:28:30 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.2353
2022-07-07 08:29:03 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.0000
2022-07-07 08:29:36 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.1070
2022-07-07 08:30:09 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.4934
2022-07-07 08:30:42 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.0940
2022-07-07 08:31:15 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.4882
2022-07-07 08:31:49 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.3275
2022-07-07 08:32:22 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.0884
2022-07-07 08:32:54 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.4161
2022-07-07 08:33:28 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.1164
2022-07-07 08:34:01 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.2529
2022-07-07 08:34:35 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.1608
2022-07-07 08:35:07 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.0614
2022-07-07 08:35:41 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.1578
2022-07-07 08:36:14 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.2421
2022-07-07 08:36:47 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.1497
2022-07-07 08:37:20 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.1873
2022-07-07 08:37:54 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.3845
2022-07-07 08:38:27 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.3810
2022-07-07 08:39:00 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.3077
2022-07-07 08:39:33 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.1535
2022-07-07 08:40:06 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.0563
2022-07-07 08:40:39 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.3235
2022-07-07 08:41:13 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.9211
2022-07-07 08:41:46 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.2438
2022-07-07 08:42:19 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.1899
2022-07-07 08:42:52 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.3139
2022-07-07 08:43:26 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.1725
2022-07-07 08:43:59 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.5049
2022-07-07 08:44:32 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.1544
2022-07-07 08:45:05 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.3419
2022-07-07 08:45:39 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.2981
2022-07-07 08:46:12 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.2933
2022-07-07 08:46:45 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.2183
2022-07-07 08:47:18 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.3215
2022-07-07 08:47:52 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.0510
2022-07-07 08:48:25 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4480
2022-07-07 08:48:59 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.1361
2022-07-07 08:49:32 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.1923
2022-07-07 08:50:06 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.2837
2022-07-07 08:50:36 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.3846
2022-07-07 08:50:37 - train: epoch 024, train_loss: 2.2390
2022-07-07 08:51:50 - eval: epoch: 024, acc1: 47.764%, acc5: 73.890%, test_loss: 2.2961, per_image_load_time: 2.082ms, per_image_inference_time: 0.296ms
2022-07-07 08:51:50 - until epoch: 024, best_acc1: 52.604%
2022-07-07 08:51:50 - epoch 025 lr: 0.100000
2022-07-07 08:52:29 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.0367
2022-07-07 08:53:02 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.0597
2022-07-07 08:53:35 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.0623
2022-07-07 08:54:07 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.1165
2022-07-07 08:54:40 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.1043
2022-07-07 08:55:13 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.2588
2022-07-07 08:55:45 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.2469
2022-07-07 08:56:18 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.1952
2022-07-07 08:56:51 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.0763
2022-07-07 08:57:25 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.1238
2022-07-07 08:57:57 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.1860
2022-07-07 08:58:31 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.2736
2022-07-07 08:59:03 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.1878
2022-07-07 08:59:37 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.2752
2022-07-07 09:00:10 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.1918
2022-07-07 09:00:43 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.9355
2022-07-07 09:01:16 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.2382
2022-07-07 09:01:49 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.1742
2022-07-07 09:02:23 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.0631
2022-07-07 09:02:55 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.1170
2022-07-07 09:03:29 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1457
2022-07-07 09:04:02 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.9908
2022-07-07 09:04:35 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.2457
2022-07-07 09:05:09 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.0792
2022-07-07 09:05:43 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.2728
2022-07-07 09:06:15 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.3980
2022-07-07 09:06:49 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.3046
2022-07-07 09:07:21 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.1838
2022-07-07 09:07:55 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.4788
2022-07-07 09:08:28 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.4293
2022-07-07 09:09:01 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.1995
2022-07-07 09:09:34 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.5052
2022-07-07 09:10:08 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.1718
2022-07-07 09:10:41 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4050
2022-07-07 09:11:14 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.0220
2022-07-07 09:11:47 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.3989
2022-07-07 09:12:21 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.1585
2022-07-07 09:12:54 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.2522
2022-07-07 09:13:26 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.3629
2022-07-07 09:14:00 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.3636
2022-07-07 09:14:33 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.5253
2022-07-07 09:15:07 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.3017
2022-07-07 09:15:40 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1966
2022-07-07 09:16:13 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.2251
2022-07-07 09:16:46 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.1810
2022-07-07 09:17:20 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.2092
2022-07-07 09:17:53 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.1607
2022-07-07 09:18:26 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.9710
2022-07-07 09:19:00 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.2074
2022-07-07 09:19:31 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.3808
2022-07-07 09:19:31 - train: epoch 025, train_loss: 2.2336
2022-07-07 09:20:44 - eval: epoch: 025, acc1: 52.566%, acc5: 77.896%, test_loss: 2.0404, per_image_load_time: 1.314ms, per_image_inference_time: 0.291ms
2022-07-07 09:20:44 - until epoch: 025, best_acc1: 52.604%
2022-07-07 09:20:44 - epoch 026 lr: 0.100000
2022-07-07 09:21:22 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.1048
2022-07-07 09:21:55 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.0804
2022-07-07 09:22:27 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.1977
2022-07-07 09:22:59 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.2107
2022-07-07 09:23:32 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.3017
2022-07-07 09:24:05 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.3687
2022-07-07 09:24:38 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.1070
2022-07-07 09:25:11 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.9372
2022-07-07 09:25:45 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.4150
2022-07-07 09:26:18 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.0720
2022-07-07 09:26:52 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.1885
2022-07-07 09:27:25 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.2106
2022-07-07 09:27:58 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2586
2022-07-07 09:28:31 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.2071
2022-07-07 09:29:05 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.1461
2022-07-07 09:29:38 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.3213
2022-07-07 09:30:11 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.2537
2022-07-07 09:30:45 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.3377
2022-07-07 09:31:18 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6175
2022-07-07 09:31:51 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.3348
2022-07-07 09:32:24 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.3548
2022-07-07 09:32:57 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.2669
2022-07-07 09:33:31 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.2036
2022-07-07 09:34:04 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.5029
2022-07-07 09:34:38 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.3241
2022-07-07 09:35:11 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.3041
2022-07-07 09:35:44 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.1510
2022-07-07 09:36:17 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.1157
2022-07-07 09:36:51 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.2632
2022-07-07 09:37:23 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.1657
2022-07-07 09:37:57 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.2315
2022-07-07 09:38:30 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.1829
2022-07-07 09:39:04 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.1624
2022-07-07 09:39:37 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.4293
2022-07-07 09:40:11 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.4348
2022-07-07 09:40:44 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.0877
2022-07-07 09:41:18 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.2521
2022-07-07 09:41:51 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.4877
2022-07-07 09:42:23 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.3864
2022-07-07 09:42:57 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.3947
2022-07-07 09:43:30 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.2873
2022-07-07 09:44:03 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.3939
2022-07-07 09:44:37 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.3652
2022-07-07 09:45:10 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.5093
2022-07-07 09:45:44 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.4278
2022-07-07 09:46:17 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.2476
2022-07-07 09:46:51 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.0771
2022-07-07 09:47:23 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.1672
2022-07-07 09:47:57 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.1856
2022-07-07 09:48:29 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.4507
2022-07-07 09:48:30 - train: epoch 026, train_loss: 2.2263
2022-07-07 09:49:43 - eval: epoch: 026, acc1: 48.274%, acc5: 74.072%, test_loss: 2.2676, per_image_load_time: 2.584ms, per_image_inference_time: 0.264ms
2022-07-07 09:49:43 - until epoch: 026, best_acc1: 52.604%
2022-07-07 09:49:43 - epoch 027 lr: 0.100000
2022-07-07 09:50:22 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.3221
2022-07-07 09:50:55 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.1766
2022-07-07 09:51:27 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.2599
2022-07-07 09:52:00 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.4510
2022-07-07 09:52:32 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.3305
2022-07-07 09:53:05 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.4267
2022-07-07 09:53:37 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.2837
2022-07-07 09:54:10 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.1947
2022-07-07 09:54:43 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.1047
2022-07-07 09:55:16 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.3518
2022-07-07 09:55:49 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.1617
2022-07-07 09:56:23 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.4962
2022-07-07 09:56:56 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.2832
2022-07-07 09:57:30 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.3876
2022-07-07 09:58:04 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.1826
2022-07-07 09:58:36 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.1714
2022-07-07 09:59:10 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.1993
2022-07-07 09:59:44 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.1868
2022-07-07 10:00:17 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.4495
2022-07-07 10:00:50 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.1375
2022-07-07 10:01:24 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.4286
2022-07-07 10:01:57 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.3470
2022-07-07 10:02:31 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.5531
2022-07-07 10:03:05 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.1914
2022-07-07 10:03:37 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.0735
2022-07-07 10:04:11 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.3750
2022-07-07 10:04:43 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.1540
2022-07-07 10:05:16 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.3694
2022-07-07 10:05:49 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.2198
2022-07-07 10:06:23 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.1412
2022-07-07 10:06:56 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.2025
2022-07-07 10:07:30 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.1275
2022-07-07 10:08:03 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.2881
2022-07-07 10:08:36 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.1994
2022-07-07 10:09:09 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.6200
2022-07-07 10:09:43 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.0317
2022-07-07 10:10:16 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.1549
2022-07-07 10:10:49 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.9478
2022-07-07 10:11:22 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.0170
2022-07-07 10:11:55 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.2736
2022-07-07 10:12:29 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.1703
2022-07-07 10:13:02 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.2934
2022-07-07 10:13:35 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.1534
2022-07-07 10:14:10 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.1433
2022-07-07 10:14:42 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.0573
2022-07-07 10:15:17 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.1547
2022-07-07 10:15:49 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.1377
2022-07-07 10:16:23 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.4070
2022-07-07 10:16:55 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.2796
2022-07-07 10:17:27 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.8641
2022-07-07 10:17:28 - train: epoch 027, train_loss: 2.2205
2022-07-07 10:18:41 - eval: epoch: 027, acc1: 55.114%, acc5: 79.864%, test_loss: 1.9141, per_image_load_time: 1.494ms, per_image_inference_time: 0.243ms
2022-07-07 10:18:41 - until epoch: 027, best_acc1: 55.114%
2022-07-07 10:18:41 - epoch 028 lr: 0.100000
2022-07-07 10:19:20 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.9893
2022-07-07 10:19:52 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.1280
2022-07-07 10:20:26 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.1842
2022-07-07 10:20:58 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.0248
2022-07-07 10:21:31 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.1822
2022-07-07 10:22:05 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.2823
2022-07-07 10:22:37 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.4184
2022-07-07 10:23:10 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.8581
2022-07-07 10:23:43 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.0253
2022-07-07 10:24:16 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.2071
2022-07-07 10:24:49 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.1826
2022-07-07 10:25:23 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.0473
2022-07-07 10:25:55 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.1893
2022-07-07 10:26:29 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.5566
2022-07-07 10:27:02 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.1639
2022-07-07 10:27:36 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.2425
2022-07-07 10:28:08 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.0761
2022-07-07 10:28:41 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.1229
2022-07-07 10:29:14 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.1817
2022-07-07 10:29:48 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.3612
2022-07-07 10:30:20 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.3624
2022-07-07 10:30:54 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.2719
2022-07-07 10:31:27 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.3793
2022-07-07 10:32:00 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.4984
2022-07-07 10:32:34 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.3229
2022-07-07 10:33:07 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.0037
2022-07-07 10:33:41 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.1093
2022-07-07 10:34:13 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.1604
2022-07-07 10:34:47 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.2396
2022-07-07 10:35:21 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.3642
2022-07-07 10:35:54 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.4518
2022-07-07 10:36:27 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.2626
2022-07-07 10:37:01 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.2284
2022-07-07 10:37:35 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.1870
2022-07-07 10:38:06 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.0861
2022-07-07 10:38:41 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.0836
2022-07-07 10:39:13 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.1804
2022-07-07 10:39:47 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.9207
2022-07-07 10:40:20 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.2525
2022-07-07 10:40:54 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.3872
2022-07-07 10:41:26 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.3514
2022-07-07 10:42:00 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.3283
2022-07-07 10:42:33 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.1512
2022-07-07 10:43:06 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.1856
2022-07-07 10:43:41 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.2410
2022-07-07 10:44:13 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.2968
2022-07-07 10:44:47 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.2726
2022-07-07 10:45:20 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.1150
2022-07-07 10:45:53 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.0556
2022-07-07 10:46:25 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.1933
2022-07-07 10:46:26 - train: epoch 028, train_loss: 2.2142
2022-07-07 10:47:39 - eval: epoch: 028, acc1: 53.360%, acc5: 78.454%, test_loss: 2.0069, per_image_load_time: 1.781ms, per_image_inference_time: 0.277ms
2022-07-07 10:47:39 - until epoch: 028, best_acc1: 55.114%
2022-07-07 10:47:39 - epoch 029 lr: 0.100000
2022-07-07 10:48:18 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.1884
2022-07-07 10:48:50 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.1990
2022-07-07 10:49:24 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.3408
2022-07-07 10:49:55 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.1261
2022-07-07 10:50:28 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.1407
2022-07-07 10:51:01 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.4371
2022-07-07 10:51:35 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.7167
2022-07-07 10:52:08 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.3516
2022-07-07 10:52:40 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1481
2022-07-07 10:53:14 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.9704
2022-07-07 10:53:47 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.1392
2022-07-07 10:54:19 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.2695
2022-07-07 10:54:53 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.1689
2022-07-07 10:55:27 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.4513
2022-07-07 10:56:00 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.2230
2022-07-07 10:56:33 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.1293
2022-07-07 10:57:07 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.2447
2022-07-07 10:57:40 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.3109
2022-07-07 10:58:13 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.0170
2022-07-07 10:58:46 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.3605
2022-07-07 10:59:20 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.1221
2022-07-07 10:59:53 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.2376
2022-07-07 11:00:27 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.2573
2022-07-07 11:01:00 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.1376
2022-07-07 11:01:33 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.0671
2022-07-07 11:02:06 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.2769
2022-07-07 11:02:39 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.1182
2022-07-07 11:03:12 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.0910
2022-07-07 11:03:46 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.1392
2022-07-07 11:04:19 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.1588
2022-07-07 11:04:52 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.2389
2022-07-07 11:05:26 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.2017
2022-07-07 11:05:59 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.0906
2022-07-07 11:06:32 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.0632
2022-07-07 11:07:05 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.3298
2022-07-07 11:07:38 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.0940
2022-07-07 11:08:12 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.1986
2022-07-07 11:08:45 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.1819
2022-07-07 11:09:19 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.9991
2022-07-07 11:09:52 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.0589
2022-07-07 11:10:26 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.1701
2022-07-07 11:10:59 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.1899
2022-07-07 11:11:32 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.2564
2022-07-07 11:12:05 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.2042
2022-07-07 11:12:39 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.4009
2022-07-07 11:13:12 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.4553
2022-07-07 11:13:45 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.8898
2022-07-07 11:14:18 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.2618
2022-07-07 11:14:51 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.5008
2022-07-07 11:15:23 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.9774
2022-07-07 11:15:24 - train: epoch 029, train_loss: 2.2106
2022-07-07 11:16:37 - eval: epoch: 029, acc1: 53.884%, acc5: 78.968%, test_loss: 1.9708, per_image_load_time: 1.776ms, per_image_inference_time: 0.264ms
2022-07-07 11:16:37 - until epoch: 029, best_acc1: 55.114%
2022-07-07 11:16:37 - epoch 030 lr: 0.100000
2022-07-07 11:17:15 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.3393
2022-07-07 11:17:48 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.2985
2022-07-07 11:18:21 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.1088
2022-07-07 11:18:53 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.8946
2022-07-07 11:19:26 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.5954
2022-07-07 11:19:59 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0485
2022-07-07 11:20:33 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.0749
2022-07-07 11:21:06 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.4221
2022-07-07 11:21:39 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.2147
2022-07-07 11:22:11 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.8696
2022-07-07 11:22:45 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.0498
2022-07-07 11:23:18 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.2056
2022-07-07 11:23:51 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.0129
2022-07-07 11:24:24 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.0903
2022-07-07 11:24:58 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.1588
2022-07-07 11:25:30 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.1728
2022-07-07 11:26:03 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.3484
2022-07-07 11:26:37 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.2711
2022-07-07 11:27:10 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.3876
2022-07-07 11:27:42 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.1959
2022-07-07 11:28:16 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.2967
2022-07-07 11:28:48 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.0974
2022-07-07 11:29:22 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.2468
2022-07-07 11:29:55 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.4506
2022-07-07 11:30:28 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2691
2022-07-07 11:31:01 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.1457
2022-07-07 11:31:34 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.1066
2022-07-07 11:32:08 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.2397
2022-07-07 11:32:41 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.2048
2022-07-07 11:33:15 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.3818
2022-07-07 11:33:48 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.1320
2022-07-07 11:34:21 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.0465
2022-07-07 11:34:54 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.3858
2022-07-07 11:35:27 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.1985
2022-07-07 11:36:02 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.3213
2022-07-07 11:36:35 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.0968
2022-07-07 11:37:08 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.1527
2022-07-07 11:37:42 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.2971
2022-07-07 11:38:15 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.3957
2022-07-07 11:38:49 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.9641
2022-07-07 11:39:22 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.0172
2022-07-07 11:39:55 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5194
2022-07-07 11:40:29 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.2312
2022-07-07 11:41:02 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.1727
2022-07-07 11:41:35 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.3830
2022-07-07 11:42:09 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.9631
2022-07-07 11:42:41 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.1112
2022-07-07 11:43:15 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.2333
2022-07-07 11:43:49 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.4130
2022-07-07 11:44:21 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.3187
2022-07-07 11:44:22 - train: epoch 030, train_loss: 2.2092
2022-07-07 11:45:36 - eval: epoch: 030, acc1: 54.448%, acc5: 79.626%, test_loss: 1.9314, per_image_load_time: 2.459ms, per_image_inference_time: 0.277ms
2022-07-07 11:45:36 - until epoch: 030, best_acc1: 55.114%
2022-07-07 11:45:36 - epoch 031 lr: 0.010000
2022-07-07 11:46:14 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.1223
2022-07-07 11:46:47 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.8337
2022-07-07 11:47:20 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.7893
2022-07-07 11:47:52 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.0402
2022-07-07 11:48:25 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.7772
2022-07-07 11:48:58 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.7903
2022-07-07 11:49:32 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.7724
2022-07-07 11:50:04 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.7610
2022-07-07 11:50:37 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.7325
2022-07-07 11:51:11 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.0007
2022-07-07 11:51:43 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.0289
2022-07-07 11:52:17 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.7473
2022-07-07 11:52:51 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.5025
2022-07-07 11:53:24 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.7108
2022-07-07 11:53:57 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.9102
2022-07-07 11:54:30 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.6270
2022-07-07 11:55:03 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.5749
2022-07-07 11:55:37 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.6840
2022-07-07 11:56:10 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.7799
2022-07-07 11:56:43 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.7386
2022-07-07 11:57:16 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.6203
2022-07-07 11:57:49 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.5738
2022-07-07 11:58:23 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.5658
2022-07-07 11:58:56 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.6555
2022-07-07 11:59:29 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.5989
2022-07-07 12:00:02 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.8201
2022-07-07 12:00:35 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.7415
2022-07-07 12:01:09 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.9459
2022-07-07 12:01:42 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.7486
2022-07-07 12:02:15 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.9134
2022-07-07 12:02:48 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6085
2022-07-07 12:03:22 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.8000
2022-07-07 12:03:55 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8194
2022-07-07 12:04:29 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.8426
2022-07-07 12:05:03 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.8550
2022-07-07 12:05:35 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.7484
2022-07-07 12:06:09 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.6682
2022-07-07 12:06:42 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.5543
2022-07-07 12:07:16 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.6540
2022-07-07 12:07:49 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.6164
2022-07-07 12:08:23 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.5848
2022-07-07 12:08:55 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.8022
2022-07-07 12:09:29 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.5845
2022-07-07 12:10:03 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.8643
2022-07-07 12:10:37 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.7450
2022-07-07 12:11:10 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.7569
2022-07-07 12:11:42 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.7790
2022-07-07 12:12:16 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.6353
2022-07-07 12:12:49 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.5597
2022-07-07 12:13:21 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.6411
2022-07-07 12:13:22 - train: epoch 031, train_loss: 1.7384
2022-07-07 12:14:35 - eval: epoch: 031, acc1: 66.188%, acc5: 87.000%, test_loss: 1.3937, per_image_load_time: 2.530ms, per_image_inference_time: 0.285ms
2022-07-07 12:14:35 - until epoch: 031, best_acc1: 66.188%
2022-07-07 12:14:35 - epoch 032 lr: 0.010000
2022-07-07 12:15:13 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.5636
2022-07-07 12:15:46 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.7468
2022-07-07 12:16:19 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.6321
2022-07-07 12:16:51 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.8027
2022-07-07 12:17:24 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.7389
2022-07-07 12:17:57 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.6848
2022-07-07 12:18:30 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.6699
2022-07-07 12:19:03 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.7032
2022-07-07 12:19:36 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.6581
2022-07-07 12:20:09 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.7739
2022-07-07 12:20:42 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.7820
2022-07-07 12:21:15 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.6563
2022-07-07 12:21:49 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.6242
2022-07-07 12:22:21 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.8749
2022-07-07 12:22:54 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.7597
2022-07-07 12:23:27 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.8417
2022-07-07 12:24:00 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.8040
2022-07-07 12:24:35 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.9746
2022-07-07 12:25:07 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.5470
2022-07-07 12:25:40 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.5683
2022-07-07 12:26:13 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.5263
2022-07-07 12:26:47 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.5786
2022-07-07 12:27:20 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.6948
2022-07-07 12:27:53 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.5034
2022-07-07 12:28:26 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.7703
2022-07-07 12:29:00 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.4475
2022-07-07 12:29:33 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.6700
2022-07-07 12:30:07 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.7547
2022-07-07 12:30:40 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.4707
2022-07-07 12:31:14 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.4290
2022-07-07 12:31:46 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.6539
2022-07-07 12:32:20 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.5368
2022-07-07 12:32:54 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.4849
2022-07-07 12:33:26 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.4286
2022-07-07 12:34:00 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.5603
2022-07-07 12:34:34 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.6229
2022-07-07 12:35:06 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.5929
2022-07-07 12:35:40 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.4825
2022-07-07 12:36:13 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.6083
2022-07-07 12:36:46 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.4891
2022-07-07 12:37:19 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.6024
2022-07-07 12:37:52 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.5176
2022-07-07 12:38:26 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.8450
2022-07-07 12:38:59 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.5728
2022-07-07 12:39:32 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.7739
2022-07-07 12:40:06 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.6003
2022-07-07 12:40:39 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.6985
2022-07-07 12:41:12 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.4668
2022-07-07 12:41:45 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.8303
2022-07-07 12:42:17 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6050
2022-07-07 12:42:18 - train: epoch 032, train_loss: 1.6254
2022-07-07 12:43:30 - eval: epoch: 032, acc1: 66.822%, acc5: 87.496%, test_loss: 1.3588, per_image_load_time: 2.187ms, per_image_inference_time: 0.293ms
2022-07-07 12:43:31 - until epoch: 032, best_acc1: 66.822%
2022-07-07 12:43:31 - epoch 033 lr: 0.010000
2022-07-07 12:44:08 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.3580
2022-07-07 12:44:42 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.7092
2022-07-07 12:45:15 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.4466
2022-07-07 12:45:47 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.4308
2022-07-07 12:46:20 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.6993
2022-07-07 12:46:52 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.4967
2022-07-07 12:47:25 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.7560
2022-07-07 12:47:58 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.7074
2022-07-07 12:48:31 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.6656
2022-07-07 12:49:05 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.6124
2022-07-07 12:49:37 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.5162
2022-07-07 12:50:10 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.5723
2022-07-07 12:50:43 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.4946
2022-07-07 12:51:16 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.6941
2022-07-07 12:51:49 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.8079
2022-07-07 12:52:21 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.7020
2022-07-07 12:52:55 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.4390
2022-07-07 12:53:28 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.7520
2022-07-07 12:54:01 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.6577
2022-07-07 12:54:34 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.4618
2022-07-07 12:55:07 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.6324
2022-07-07 12:55:40 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.6997
2022-07-07 12:56:13 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.4593
2022-07-07 12:56:47 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.8529
2022-07-07 12:57:20 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.5536
2022-07-07 12:57:53 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.4763
2022-07-07 12:58:26 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.8240
2022-07-07 12:58:59 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.5468
2022-07-07 12:59:32 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.7546
2022-07-07 13:00:05 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.6338
2022-07-07 13:00:38 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.5506
2022-07-07 13:01:11 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.5240
2022-07-07 13:01:45 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.6669
2022-07-07 13:02:18 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.4877
2022-07-07 13:02:51 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.6918
2022-07-07 13:03:25 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.7711
2022-07-07 13:03:58 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.5555
2022-07-07 13:04:31 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.4217
2022-07-07 13:05:04 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.6920
2022-07-07 13:05:38 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.6194
2022-07-07 13:06:11 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.5336
2022-07-07 13:06:45 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.5140
2022-07-07 13:07:18 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.7521
2022-07-07 13:07:51 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.5877
2022-07-07 13:08:23 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.7663
2022-07-07 13:08:56 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.4786
2022-07-07 13:09:29 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.4081
2022-07-07 13:10:03 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.9553
2022-07-07 13:10:36 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.3711
2022-07-07 13:11:07 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.4843
2022-07-07 13:11:08 - train: epoch 033, train_loss: 1.5793
2022-07-07 13:12:21 - eval: epoch: 033, acc1: 67.598%, acc5: 87.882%, test_loss: 1.3302, per_image_load_time: 2.541ms, per_image_inference_time: 0.286ms
2022-07-07 13:12:22 - until epoch: 033, best_acc1: 67.598%
2022-07-07 13:12:22 - epoch 034 lr: 0.010000
2022-07-07 13:13:01 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.5198
2022-07-07 13:13:33 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.5480
2022-07-07 13:14:07 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.5427
2022-07-07 13:14:38 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.3637
2022-07-07 13:15:12 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.5099
2022-07-07 13:15:44 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.8306
2022-07-07 13:16:18 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.5623
2022-07-07 13:16:50 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.5278
2022-07-07 13:17:23 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.5077
2022-07-07 13:17:57 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.4618
2022-07-07 13:18:30 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.5346
2022-07-07 13:19:03 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.5082
2022-07-07 13:19:36 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.5008
2022-07-07 13:20:09 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.5829
2022-07-07 13:20:42 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.4967
2022-07-07 13:21:15 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.4265
2022-07-07 13:21:49 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.5939
2022-07-07 13:22:22 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.7354
2022-07-07 13:22:55 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.6290
2022-07-07 13:23:28 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.6162
2022-07-07 13:24:02 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.7439
2022-07-07 13:24:35 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.4482
2022-07-07 13:25:08 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.6440
2022-07-07 13:25:41 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.4298
2022-07-07 13:26:15 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.6148
2022-07-07 13:26:48 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.6164
2022-07-07 13:27:21 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.5642
2022-07-07 13:27:54 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.4073
2022-07-07 13:28:28 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.2968
2022-07-07 13:29:01 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.3714
2022-07-07 13:29:34 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.3954
2022-07-07 13:30:07 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.5309
2022-07-07 13:30:40 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.4011
2022-07-07 13:31:13 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.6493
2022-07-07 13:31:47 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.4290
2022-07-07 13:32:20 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.4147
2022-07-07 13:32:54 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.4480
2022-07-07 13:33:27 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.5234
2022-07-07 13:34:00 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.7045
2022-07-07 13:34:34 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.4064
2022-07-07 13:35:07 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.5547
2022-07-07 13:35:40 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.5255
2022-07-07 13:36:13 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.5428
2022-07-07 13:36:46 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6358
2022-07-07 13:37:20 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.7486
2022-07-07 13:37:54 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.6064
2022-07-07 13:38:27 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.5174
2022-07-07 13:39:00 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.5359
2022-07-07 13:39:34 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.5329
2022-07-07 13:40:06 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.3792
2022-07-07 13:40:07 - train: epoch 034, train_loss: 1.5523
2022-07-07 13:41:20 - eval: epoch: 034, acc1: 67.604%, acc5: 88.042%, test_loss: 1.3217, per_image_load_time: 2.090ms, per_image_inference_time: 0.301ms
2022-07-07 13:41:20 - until epoch: 034, best_acc1: 67.604%
2022-07-07 13:41:20 - epoch 035 lr: 0.010000
2022-07-07 13:41:59 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.3731
2022-07-07 13:42:31 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.3197
2022-07-07 13:43:04 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.7082
2022-07-07 13:43:36 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.4432
2022-07-07 13:44:10 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.5783
2022-07-07 13:44:42 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.4957
2022-07-07 13:45:15 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.5143
2022-07-07 13:45:49 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.5612
2022-07-07 13:46:22 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.6613
2022-07-07 13:46:55 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.6808
2022-07-07 13:47:28 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.6684
2022-07-07 13:48:01 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.4840
2022-07-07 13:48:34 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.5807
2022-07-07 13:49:07 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.6159
2022-07-07 13:49:40 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.6289
2022-07-07 13:50:15 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.5556
2022-07-07 13:50:47 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.3950
2022-07-07 13:51:21 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.5639
2022-07-07 13:51:54 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.6267
2022-07-07 13:52:28 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.5213
2022-07-07 13:53:00 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.5167
2022-07-07 13:53:34 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.6218
2022-07-07 13:54:08 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.4861
2022-07-07 13:54:40 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.5989
2022-07-07 13:55:14 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.3944
2022-07-07 13:55:47 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.7533
2022-07-07 13:56:20 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.6635
2022-07-07 13:56:53 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.4410
2022-07-07 13:57:25 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.4944
2022-07-07 13:57:59 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.6391
2022-07-07 13:58:31 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.5794
2022-07-07 13:59:05 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.4304
2022-07-07 13:59:38 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.4658
2022-07-07 14:00:11 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.6015
2022-07-07 14:00:44 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.2991
2022-07-07 14:01:18 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.5718
2022-07-07 14:01:50 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.3947
2022-07-07 14:02:24 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.6028
2022-07-07 14:02:57 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.5816
2022-07-07 14:03:30 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.3278
2022-07-07 14:04:03 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.8132
2022-07-07 14:04:37 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.4885
2022-07-07 14:05:10 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.5908
2022-07-07 14:05:43 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.6187
2022-07-07 14:06:17 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.6316
2022-07-07 14:06:51 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5130
2022-07-07 14:07:23 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.7647
2022-07-07 14:07:56 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.7578
2022-07-07 14:08:30 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.6590
2022-07-07 14:09:02 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.4678
2022-07-07 14:09:02 - train: epoch 035, train_loss: 1.5343
2022-07-07 14:10:16 - eval: epoch: 035, acc1: 67.926%, acc5: 88.232%, test_loss: 1.3077, per_image_load_time: 2.158ms, per_image_inference_time: 0.274ms
2022-07-07 14:10:16 - until epoch: 035, best_acc1: 67.926%
2022-07-07 14:10:16 - epoch 036 lr: 0.010000
2022-07-07 14:10:55 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.7431
2022-07-07 14:11:28 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.2969
2022-07-07 14:12:00 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.3958
2022-07-07 14:12:32 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.4863
2022-07-07 14:13:05 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.5191
2022-07-07 14:13:38 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.4380
2022-07-07 14:14:11 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.3666
2022-07-07 14:14:45 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.2464
2022-07-07 14:15:17 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.4642
2022-07-07 14:15:50 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.5287
2022-07-07 14:16:23 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.4708
2022-07-07 14:16:56 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.4925
2022-07-07 14:17:29 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.5274
2022-07-07 14:18:02 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.7156
2022-07-07 14:18:35 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.6622
2022-07-07 14:19:09 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.5870
2022-07-07 14:19:42 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.5010
2022-07-07 14:20:15 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.3569
2022-07-07 14:20:48 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.5928
2022-07-07 14:21:22 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.4887
2022-07-07 14:21:55 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.6042
2022-07-07 14:22:27 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.4033
2022-07-07 14:23:01 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.6087
2022-07-07 14:23:34 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.6770
2022-07-07 14:24:07 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.3816
2022-07-07 14:24:40 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.5478
2022-07-07 14:25:13 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.4379
2022-07-07 14:25:46 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.4265
2022-07-07 14:26:19 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.3910
2022-07-07 14:26:53 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.7113
2022-07-07 14:27:24 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.5753
2022-07-07 14:27:59 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.5071
2022-07-07 14:28:32 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.4368
2022-07-07 14:29:05 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.4345
2022-07-07 14:29:38 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.5601
2022-07-07 14:30:11 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.6880
2022-07-07 14:30:44 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.4958
2022-07-07 14:31:18 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.4178
2022-07-07 14:31:50 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.5181
2022-07-07 14:32:25 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.5165
2022-07-07 14:32:57 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.4840
2022-07-07 14:33:31 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.4778
2022-07-07 14:34:04 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.4943
2022-07-07 14:34:38 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.6115
2022-07-07 14:35:11 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.3124
2022-07-07 14:35:45 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.3315
2022-07-07 14:36:18 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.4913
2022-07-07 14:36:51 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.4897
2022-07-07 14:37:24 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.4925
2022-07-07 14:37:55 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.5010
2022-07-07 14:37:56 - train: epoch 036, train_loss: 1.5188
2022-07-07 14:39:10 - eval: epoch: 036, acc1: 67.752%, acc5: 88.160%, test_loss: 1.3171, per_image_load_time: 1.849ms, per_image_inference_time: 0.269ms
2022-07-07 14:39:11 - until epoch: 036, best_acc1: 67.926%
2022-07-07 14:39:11 - epoch 037 lr: 0.010000
2022-07-07 14:39:50 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.2701
2022-07-07 14:40:22 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.2375
2022-07-07 14:40:54 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.4272
2022-07-07 14:41:27 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.5994
2022-07-07 14:42:00 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.4670
2022-07-07 14:42:32 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.6317
2022-07-07 14:43:05 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.5434
2022-07-07 14:43:39 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5143
2022-07-07 14:44:12 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.7750
2022-07-07 14:44:45 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.4173
2022-07-07 14:45:19 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.5686
2022-07-07 14:45:51 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.5073
2022-07-07 14:46:24 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.4600
2022-07-07 14:46:58 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.6014
2022-07-07 14:47:31 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.4568
2022-07-07 14:48:04 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.3010
2022-07-07 14:48:38 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.7957
2022-07-07 14:49:10 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.6927
2022-07-07 14:49:44 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.5815
2022-07-07 14:50:17 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.5192
2022-07-07 14:50:51 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.6103
2022-07-07 14:51:24 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.4763
2022-07-07 14:51:56 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.3770
2022-07-07 14:52:30 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.5133
2022-07-07 14:53:03 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.4386
2022-07-07 14:53:36 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.7298
2022-07-07 14:54:10 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.5828
2022-07-07 14:54:43 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.5342
2022-07-07 14:55:16 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.6131
2022-07-07 14:55:50 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.7313
2022-07-07 14:56:23 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.4219
2022-07-07 14:56:56 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.5537
2022-07-07 14:57:30 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.4134
2022-07-07 14:58:03 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.3327
2022-07-07 14:58:37 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.5243
2022-07-07 14:59:11 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.5579
2022-07-07 14:59:44 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.6504
2022-07-07 15:00:17 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.3863
2022-07-07 15:00:51 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.7129
2022-07-07 15:01:23 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.4104
2022-07-07 15:01:57 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.5093
2022-07-07 15:02:30 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.7437
2022-07-07 15:03:04 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.5285
2022-07-07 15:03:37 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.5120
2022-07-07 15:04:10 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.4599
2022-07-07 15:04:43 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.5303
2022-07-07 15:05:17 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.4970
2022-07-07 15:05:49 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.5309
2022-07-07 15:06:22 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.5943
2022-07-07 15:06:55 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.4731
2022-07-07 15:06:56 - train: epoch 037, train_loss: 1.5125
2022-07-07 15:08:08 - eval: epoch: 037, acc1: 67.334%, acc5: 87.734%, test_loss: 1.3416, per_image_load_time: 2.514ms, per_image_inference_time: 0.282ms
2022-07-07 15:08:08 - until epoch: 037, best_acc1: 67.926%
2022-07-07 15:08:08 - epoch 038 lr: 0.010000
2022-07-07 15:08:47 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.4628
2022-07-07 15:09:19 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.2877
2022-07-07 15:09:51 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.1448
2022-07-07 15:10:25 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.3726
2022-07-07 15:10:57 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.2858
2022-07-07 15:11:30 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.5641
2022-07-07 15:12:03 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.3811
2022-07-07 15:12:36 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.3155
2022-07-07 15:13:10 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.4592
2022-07-07 15:13:42 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.6915
2022-07-07 15:14:16 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.4126
2022-07-07 15:14:48 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.5346
2022-07-07 15:15:22 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.6740
2022-07-07 15:15:55 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.5439
2022-07-07 15:16:28 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.5739
2022-07-07 15:17:00 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.6191
2022-07-07 15:17:34 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.6747
2022-07-07 15:18:07 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.6118
2022-07-07 15:18:41 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.5865
2022-07-07 15:19:14 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.4609
2022-07-07 15:19:47 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.4780
2022-07-07 15:20:21 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.4460
2022-07-07 15:20:53 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.5705
2022-07-07 15:21:26 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.8139
2022-07-07 15:21:59 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.4602
2022-07-07 15:22:33 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.5009
2022-07-07 15:23:06 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.6593
2022-07-07 15:23:40 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.6704
2022-07-07 15:24:12 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.7119
2022-07-07 15:24:45 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.4888
2022-07-07 15:25:18 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.4724
2022-07-07 15:25:51 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.2365
2022-07-07 15:26:25 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.2718
2022-07-07 15:26:58 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.4727
2022-07-07 15:27:32 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.5622
2022-07-07 15:28:05 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.5298
2022-07-07 15:28:38 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.2992
2022-07-07 15:29:11 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.6363
2022-07-07 15:29:44 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.4310
2022-07-07 15:30:18 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.4326
2022-07-07 15:30:51 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.4882
2022-07-07 15:31:25 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.3506
2022-07-07 15:31:58 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.6439
2022-07-07 15:32:31 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.5427
2022-07-07 15:33:05 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.5672
2022-07-07 15:33:37 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.6184
2022-07-07 15:34:11 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.3258
2022-07-07 15:34:43 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.5529
2022-07-07 15:35:17 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.4368
2022-07-07 15:35:49 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.3047
2022-07-07 15:35:49 - train: epoch 038, train_loss: 1.5052
2022-07-07 15:37:02 - eval: epoch: 038, acc1: 67.576%, acc5: 87.934%, test_loss: 1.3284, per_image_load_time: 2.470ms, per_image_inference_time: 0.298ms
2022-07-07 15:37:02 - until epoch: 038, best_acc1: 67.926%
2022-07-07 15:37:02 - epoch 039 lr: 0.010000
2022-07-07 15:37:41 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.5845
2022-07-07 15:38:14 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.6005
2022-07-07 15:38:47 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.3148
2022-07-07 15:39:20 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.5397
2022-07-07 15:39:53 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.4906
2022-07-07 15:40:26 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.3622
2022-07-07 15:40:59 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.6514
2022-07-07 15:41:33 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.4718
2022-07-07 15:42:07 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.5940
2022-07-07 15:42:40 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.3715
2022-07-07 15:43:13 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.5377
2022-07-07 15:43:46 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.5602
2022-07-07 15:44:20 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.7145
2022-07-07 15:44:52 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.6319
2022-07-07 15:45:26 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.4772
2022-07-07 15:45:58 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.4794
2022-07-07 15:46:32 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.2801
2022-07-07 15:47:05 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.3877
2022-07-07 15:47:39 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.2265
2022-07-07 15:48:12 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.4185
2022-07-07 15:48:45 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.4971
2022-07-07 15:49:18 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.4528
2022-07-07 15:49:51 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.7664
2022-07-07 15:50:25 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.6205
2022-07-07 15:50:58 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.3631
2022-07-07 15:51:32 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.4987
2022-07-07 15:52:04 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.6937
2022-07-07 15:52:38 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.3671
2022-07-07 15:53:11 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.3273
2022-07-07 15:53:45 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.6145
2022-07-07 15:54:17 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.3646
2022-07-07 15:54:51 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.5531
2022-07-07 15:55:25 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.5951
2022-07-07 15:55:58 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.5972
2022-07-07 15:56:31 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.6971
2022-07-07 15:57:04 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.6692
2022-07-07 15:57:38 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.4338
2022-07-07 15:58:11 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.3660
2022-07-07 15:58:45 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.6140
2022-07-07 15:59:17 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.5629
2022-07-07 15:59:52 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.5673
2022-07-07 16:00:24 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.5274
2022-07-07 16:00:58 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.5968
2022-07-07 16:01:31 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.1657
2022-07-07 16:02:05 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.4041
2022-07-07 16:02:38 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.7293
2022-07-07 16:03:11 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.4951
2022-07-07 16:03:44 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.5659
2022-07-07 16:04:18 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.3063
2022-07-07 16:04:49 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.3801
2022-07-07 16:04:50 - train: epoch 039, train_loss: 1.5011
2022-07-07 16:06:03 - eval: epoch: 039, acc1: 67.302%, acc5: 88.088%, test_loss: 1.3297, per_image_load_time: 2.586ms, per_image_inference_time: 0.242ms
2022-07-07 16:06:03 - until epoch: 039, best_acc1: 67.926%
2022-07-07 16:06:03 - epoch 040 lr: 0.010000
2022-07-07 16:06:42 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.6461
2022-07-07 16:07:14 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.7111
2022-07-07 16:07:48 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.6064
2022-07-07 16:08:20 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.4438
2022-07-07 16:08:53 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.4024
2022-07-07 16:09:26 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.5829
2022-07-07 16:09:59 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.4673
2022-07-07 16:10:32 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.6849
2022-07-07 16:11:05 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.3569
2022-07-07 16:11:37 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.1733
2022-07-07 16:12:10 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.5025
2022-07-07 16:12:43 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.4292
2022-07-07 16:13:16 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.4018
2022-07-07 16:13:50 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.3840
2022-07-07 16:14:22 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.6654
2022-07-07 16:14:56 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.5392
2022-07-07 16:15:29 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.6031
2022-07-07 16:16:02 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.2873
2022-07-07 16:16:36 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.4231
2022-07-07 16:17:09 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.5750
2022-07-07 16:17:42 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.3451
2022-07-07 16:18:15 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.2855
2022-07-07 16:18:48 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.4378
2022-07-07 16:19:22 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.4959
2022-07-07 16:19:54 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.5777
2022-07-07 16:20:28 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.3287
2022-07-07 16:21:01 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.6486
2022-07-07 16:21:34 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.5749
2022-07-07 16:22:07 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.8116
2022-07-07 16:22:41 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.6711
2022-07-07 16:23:14 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.4055
2022-07-07 16:23:47 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.6960
2022-07-07 16:24:20 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.5861
2022-07-07 16:24:53 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.4799
2022-07-07 16:25:26 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.5667
2022-07-07 16:25:59 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.5239
2022-07-07 16:26:32 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.5491
2022-07-07 16:27:05 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.3809
2022-07-07 16:27:38 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.6300
2022-07-07 16:28:12 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.6266
2022-07-07 16:28:45 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.5106
2022-07-07 16:29:18 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.3737
2022-07-07 16:29:52 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.5083
2022-07-07 16:30:25 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.5668
2022-07-07 16:30:58 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.3093
2022-07-07 16:31:31 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.5129
2022-07-07 16:32:05 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.4515
2022-07-07 16:32:38 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.4206
2022-07-07 16:33:12 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.5497
2022-07-07 16:33:43 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.5437
2022-07-07 16:33:44 - train: epoch 040, train_loss: 1.4983
2022-07-07 16:34:57 - eval: epoch: 040, acc1: 67.592%, acc5: 88.128%, test_loss: 1.3153, per_image_load_time: 1.262ms, per_image_inference_time: 0.287ms
2022-07-07 16:34:57 - until epoch: 040, best_acc1: 67.926%
2022-07-07 16:34:57 - epoch 041 lr: 0.010000
2022-07-07 16:35:35 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.5519
2022-07-07 16:36:08 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.6609
2022-07-07 16:36:41 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.4461
2022-07-07 16:37:14 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.4165
2022-07-07 16:37:47 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.2619
2022-07-07 16:38:20 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.6677
2022-07-07 16:38:52 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.3957
2022-07-07 16:39:26 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.2330
2022-07-07 16:39:59 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.2869
2022-07-07 16:40:32 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.6671
2022-07-07 16:41:05 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.3015
2022-07-07 16:41:38 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.2960
2022-07-07 16:42:10 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.4290
2022-07-07 16:42:45 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.5545
2022-07-07 16:43:17 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.7833
2022-07-07 16:43:50 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.2949
2022-07-07 16:44:24 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.5401
2022-07-07 16:44:57 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.5296
2022-07-07 16:45:30 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.6395
2022-07-07 16:46:03 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.4596
2022-07-07 16:46:37 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.3318
2022-07-07 16:47:10 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.2996
2022-07-07 16:47:44 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.2108
2022-07-07 16:48:15 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.7067
2022-07-07 16:48:49 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.4576
2022-07-07 16:49:22 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.6151
2022-07-07 16:49:56 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.5803
2022-07-07 16:50:29 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.4744
2022-07-07 16:51:03 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.6215
2022-07-07 16:51:36 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.5741
2022-07-07 16:52:09 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.6358
2022-07-07 16:52:42 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.3673
2022-07-07 16:53:16 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.3091
2022-07-07 16:53:50 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.4301
2022-07-07 16:54:23 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.6593
2022-07-07 16:54:56 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.5405
2022-07-07 16:55:29 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.4963
2022-07-07 16:56:02 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.4295
2022-07-07 16:56:37 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.5710
2022-07-07 16:57:09 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.3535
2022-07-07 16:57:43 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.5432
2022-07-07 16:58:15 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.4348
2022-07-07 16:58:50 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.5358
2022-07-07 16:59:22 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.4828
2022-07-07 16:59:56 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.5170
2022-07-07 17:00:29 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.5335
2022-07-07 17:01:03 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.7046
2022-07-07 17:01:36 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.5108
2022-07-07 17:02:10 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.4327
2022-07-07 17:02:41 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.5921
2022-07-07 17:02:42 - train: epoch 041, train_loss: 1.4998
2022-07-07 17:03:55 - eval: epoch: 041, acc1: 67.798%, acc5: 88.082%, test_loss: 1.3138, per_image_load_time: 2.414ms, per_image_inference_time: 0.284ms
2022-07-07 17:03:55 - until epoch: 041, best_acc1: 67.926%
2022-07-07 17:03:55 - epoch 042 lr: 0.010000
2022-07-07 17:04:34 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.1549
2022-07-07 17:05:05 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.5660
2022-07-07 17:05:39 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.4796
2022-07-07 17:06:11 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.2216
2022-07-07 17:06:44 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.5646
2022-07-07 17:07:17 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.3638
2022-07-07 17:07:49 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.5313
2022-07-07 17:08:23 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.4360
2022-07-07 17:08:56 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.7414
2022-07-07 17:09:29 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.6054
2022-07-07 17:10:03 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.3489
2022-07-07 17:10:35 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.4528
2022-07-07 17:11:10 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.5905
2022-07-07 17:11:43 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.8563
2022-07-07 17:12:16 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.4445
2022-07-07 17:12:48 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.6077
2022-07-07 17:13:22 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.5136
2022-07-07 17:13:55 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.4449
2022-07-07 17:14:29 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.5817
2022-07-07 17:15:01 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.3460
2022-07-07 17:15:35 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.3166
2022-07-07 17:16:08 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.4268
2022-07-07 17:16:41 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.4371
2022-07-07 17:17:14 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.8103
2022-07-07 17:17:48 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.5992
2022-07-07 17:18:22 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.5652
2022-07-07 17:18:55 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.4241
2022-07-07 17:19:28 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.4399
2022-07-07 17:20:01 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.4829
2022-07-07 17:20:34 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.4973
2022-07-07 17:21:07 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.5063
2022-07-07 17:21:41 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.5552
2022-07-07 17:22:14 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.7417
2022-07-07 17:22:47 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.3903
2022-07-07 17:23:20 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.4797
2022-07-07 17:23:53 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.6733
2022-07-07 17:24:27 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.4775
2022-07-07 17:25:01 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.3364
2022-07-07 17:25:34 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.4775
2022-07-07 17:26:08 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.4989
2022-07-07 17:26:41 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.6916
2022-07-07 17:27:15 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.5843
2022-07-07 17:27:48 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.3699
2022-07-07 17:28:21 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.4291
2022-07-07 17:28:54 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.4475
2022-07-07 17:29:27 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.7618
2022-07-07 17:30:01 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.3209
2022-07-07 17:30:34 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.6019
2022-07-07 17:31:08 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.5885
2022-07-07 17:31:40 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.5150
2022-07-07 17:31:41 - train: epoch 042, train_loss: 1.4978
2022-07-07 17:32:54 - eval: epoch: 042, acc1: 67.544%, acc5: 88.004%, test_loss: 1.3259, per_image_load_time: 2.096ms, per_image_inference_time: 0.269ms
2022-07-07 17:32:54 - until epoch: 042, best_acc1: 67.926%
2022-07-07 17:32:54 - epoch 043 lr: 0.010000
2022-07-07 17:33:33 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.5131
2022-07-07 17:34:05 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.4972
2022-07-07 17:34:38 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.1921
2022-07-07 17:35:10 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.4091
2022-07-07 17:35:43 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.4238
2022-07-07 17:36:16 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.4118
2022-07-07 17:36:49 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.6074
2022-07-07 17:37:21 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.6917
2022-07-07 17:37:54 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.4387
2022-07-07 17:38:28 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.6796
2022-07-07 17:39:01 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.5424
2022-07-07 17:39:35 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.5471
2022-07-07 17:40:07 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.6173
2022-07-07 17:40:41 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.4405
2022-07-07 17:41:14 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.3605
2022-07-07 17:41:48 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.4776
2022-07-07 17:42:20 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.5732
2022-07-07 17:42:53 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.6239
2022-07-07 17:43:26 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.5388
2022-07-07 17:43:59 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.3039
2022-07-07 17:44:33 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.4590
2022-07-07 17:45:06 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.5495
2022-07-07 17:45:38 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.6266
2022-07-07 17:46:12 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.3507
2022-07-07 17:46:45 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.6267
2022-07-07 17:47:20 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.1984
2022-07-07 17:47:52 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.5060
2022-07-07 17:48:26 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.4321
2022-07-07 17:48:59 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.5675
2022-07-07 17:49:31 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.7583
2022-07-07 17:50:05 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.6887
2022-07-07 17:50:38 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.5885
2022-07-07 17:51:11 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.6266
2022-07-07 17:51:45 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.5381
2022-07-07 17:52:18 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.4851
2022-07-07 17:52:52 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.5294
2022-07-07 17:53:26 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.4802
2022-07-07 17:53:58 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.6012
2022-07-07 17:54:32 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.6327
2022-07-07 17:55:05 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.5012
2022-07-07 17:55:39 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.8189
2022-07-07 17:56:12 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.6216
2022-07-07 17:56:46 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.4658
2022-07-07 17:57:19 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.4754
2022-07-07 17:57:52 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.4908
2022-07-07 17:58:25 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.5434
2022-07-07 17:58:58 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.4101
2022-07-07 17:59:31 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.4842
2022-07-07 18:00:05 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.4168
2022-07-07 18:00:36 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.4408
2022-07-07 18:00:37 - train: epoch 043, train_loss: 1.4970
2022-07-07 18:01:51 - eval: epoch: 043, acc1: 65.930%, acc5: 87.230%, test_loss: 1.3943, per_image_load_time: 1.299ms, per_image_inference_time: 0.283ms
2022-07-07 18:01:51 - until epoch: 043, best_acc1: 67.926%
2022-07-07 18:01:51 - epoch 044 lr: 0.010000
2022-07-07 18:02:30 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.5640
2022-07-07 18:03:03 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.5764
2022-07-07 18:03:36 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.5228
2022-07-07 18:04:08 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.1919
2022-07-07 18:04:41 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.4951
2022-07-07 18:05:13 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.2872
2022-07-07 18:05:46 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.3201
2022-07-07 18:06:19 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.4211
2022-07-07 18:06:51 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.4384
2022-07-07 18:07:24 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.4609
2022-07-07 18:07:57 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.4684
2022-07-07 18:08:30 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.3850
2022-07-07 18:09:04 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.6101
2022-07-07 18:09:37 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.3755
2022-07-07 18:10:10 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.4605
2022-07-07 18:10:43 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.4041
2022-07-07 18:11:17 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.3871
2022-07-07 18:11:49 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.5439
2022-07-07 18:12:23 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.6980
2022-07-07 18:12:56 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.3687
2022-07-07 18:13:30 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.6272
2022-07-07 18:14:03 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.4760
2022-07-07 18:14:36 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.6470
2022-07-07 18:15:08 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.4787
2022-07-07 18:15:42 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.5687
2022-07-07 18:16:15 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.4346
2022-07-07 18:16:49 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.5522
2022-07-07 18:17:22 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.3081
2022-07-07 18:17:55 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.3580
2022-07-07 18:18:28 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.2825
2022-07-07 18:19:02 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.5391
2022-07-07 18:19:35 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.1400
2022-07-07 18:20:08 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.4641
2022-07-07 18:20:41 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.6856
2022-07-07 18:21:15 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.3854
2022-07-07 18:21:49 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.7631
2022-07-07 18:22:21 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.6381
2022-07-07 18:22:54 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.1830
2022-07-07 18:23:28 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.5714
2022-07-07 18:24:01 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.5973
2022-07-07 18:24:35 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.3779
2022-07-07 18:25:08 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.6203
2022-07-07 18:25:41 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.7889
2022-07-07 18:26:14 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.2844
2022-07-07 18:26:48 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.5492
2022-07-07 18:27:21 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.6402
2022-07-07 18:27:54 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.5427
2022-07-07 18:28:27 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.6948
2022-07-07 18:29:00 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.3165
2022-07-07 18:29:32 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.5813
2022-07-07 18:29:32 - train: epoch 044, train_loss: 1.4981
2022-07-07 18:30:46 - eval: epoch: 044, acc1: 67.206%, acc5: 87.868%, test_loss: 1.3420, per_image_load_time: 2.594ms, per_image_inference_time: 0.261ms
2022-07-07 18:30:46 - until epoch: 044, best_acc1: 67.926%
2022-07-07 18:30:46 - epoch 045 lr: 0.010000
2022-07-07 18:31:25 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.3322
2022-07-07 18:31:58 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.5454
2022-07-07 18:32:32 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.6029
2022-07-07 18:33:04 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.3706
2022-07-07 18:33:37 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.6653
2022-07-07 18:34:10 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.4974
2022-07-07 18:34:44 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.1775
2022-07-07 18:35:16 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.4360
2022-07-07 18:35:49 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.5203
2022-07-07 18:36:22 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.4552
2022-07-07 18:36:56 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.5970
2022-07-07 18:37:28 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.5339
2022-07-07 18:38:01 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.5697
2022-07-07 18:38:34 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.4221
2022-07-07 18:39:07 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.5867
2022-07-07 18:39:41 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.4722
2022-07-07 18:40:14 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.4012
2022-07-07 18:40:47 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.3550
2022-07-07 18:41:21 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.5633
2022-07-07 18:41:53 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.5607
2022-07-07 18:42:27 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.6274
2022-07-07 18:42:59 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.5346
2022-07-07 18:43:33 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.4045
2022-07-07 18:44:06 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.6730
2022-07-07 18:44:38 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.5036
2022-07-07 18:45:12 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.5218
2022-07-07 18:45:45 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.4027
2022-07-07 18:46:19 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.5004
2022-07-07 18:46:52 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.6530
2022-07-07 18:47:25 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.7284
2022-07-07 18:47:59 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.5150
2022-07-07 18:48:32 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.5871
2022-07-07 18:49:05 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.5427
2022-07-07 18:49:38 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.2602
2022-07-07 18:50:12 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.6137
2022-07-07 18:50:45 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.5716
2022-07-07 18:51:18 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.5101
2022-07-07 18:51:51 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.3198
2022-07-07 18:52:25 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.6656
2022-07-07 18:52:58 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.7006
2022-07-07 18:53:31 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.3700
2022-07-07 18:54:04 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.6482
2022-07-07 18:54:38 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.8031
2022-07-07 18:55:11 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.6660
2022-07-07 18:55:45 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.3972
2022-07-07 18:56:18 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.6381
2022-07-07 18:56:52 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.6752
2022-07-07 18:57:24 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.4944
2022-07-07 18:57:58 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.4484
2022-07-07 18:58:29 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.5532
2022-07-07 18:58:30 - train: epoch 045, train_loss: 1.4955
2022-07-07 18:59:43 - eval: epoch: 045, acc1: 66.318%, acc5: 87.480%, test_loss: 1.3757, per_image_load_time: 2.454ms, per_image_inference_time: 0.276ms
2022-07-07 18:59:43 - until epoch: 045, best_acc1: 67.926%
2022-07-07 18:59:43 - epoch 046 lr: 0.010000
2022-07-07 19:00:21 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.2507
2022-07-07 19:00:54 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.3924
2022-07-07 19:01:26 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.5810
2022-07-07 19:02:00 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.6348
2022-07-07 19:02:32 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.2906
2022-07-07 19:03:05 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.6841
2022-07-07 19:03:38 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.4305
2022-07-07 19:04:11 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.6033
2022-07-07 19:04:45 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.5495
2022-07-07 19:05:16 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.3067
2022-07-07 19:05:50 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.4330
2022-07-07 19:06:23 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.4913
2022-07-07 19:06:56 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.4611
2022-07-07 19:07:29 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.6959
2022-07-07 19:08:03 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.4323
2022-07-07 19:08:36 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.4755
2022-07-07 19:09:09 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.3941
2022-07-07 19:09:43 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.4559
2022-07-07 19:10:16 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.3033
2022-07-07 19:10:49 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.4874
2022-07-07 19:11:23 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.6638
2022-07-07 19:11:56 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.3094
2022-07-07 19:12:30 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.6867
2022-07-07 19:13:03 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.4952
2022-07-07 19:13:37 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.5247
2022-07-07 19:14:09 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.7163
2022-07-07 19:14:43 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.3638
2022-07-07 19:15:16 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.3042
2022-07-07 19:15:50 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.5075
2022-07-07 19:16:23 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.3420
2022-07-07 19:16:56 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.3230
2022-07-07 19:17:29 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.6968
2022-07-07 19:18:03 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.5362
2022-07-07 19:18:35 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.6088
2022-07-07 19:19:10 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.5493
2022-07-07 19:19:42 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.3879
2022-07-07 19:20:15 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.3628
2022-07-07 19:20:48 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.6910
2022-07-07 19:21:22 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.5160
2022-07-07 19:21:55 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.3895
2022-07-07 19:22:28 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.6876
2022-07-07 19:23:02 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.3737
2022-07-07 19:23:36 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.6763
2022-07-07 19:24:08 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.5460
2022-07-07 19:24:42 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.4214
2022-07-07 19:25:14 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.5773
2022-07-07 19:25:48 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.4408
2022-07-07 19:26:22 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.4959
2022-07-07 19:26:54 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.5299
2022-07-07 19:27:27 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.4340
2022-07-07 19:27:28 - train: epoch 046, train_loss: 1.4947
2022-07-07 19:28:41 - eval: epoch: 046, acc1: 67.324%, acc5: 87.858%, test_loss: 1.3434, per_image_load_time: 2.532ms, per_image_inference_time: 0.285ms
2022-07-07 19:28:41 - until epoch: 046, best_acc1: 67.926%
2022-07-07 19:28:41 - epoch 047 lr: 0.010000
2022-07-07 19:29:19 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.4433
2022-07-07 19:29:51 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.6157
2022-07-07 19:30:25 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.4114
2022-07-07 19:30:57 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.3169
2022-07-07 19:31:30 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.6208
2022-07-07 19:32:03 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.5587
2022-07-07 19:32:36 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.6629
2022-07-07 19:33:09 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.3469
2022-07-07 19:33:42 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.6379
2022-07-07 19:34:16 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.5198
2022-07-07 19:34:49 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.5371
2022-07-07 19:35:22 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.3649
2022-07-07 19:35:55 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.4630
2022-07-07 19:36:28 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.4331
2022-07-07 19:37:02 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.4417
2022-07-07 19:37:34 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.3716
2022-07-07 19:38:07 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.3089
2022-07-07 19:38:40 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.5102
2022-07-07 19:39:14 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.3444
2022-07-07 19:39:47 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.5143
2022-07-07 19:40:20 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.5675
2022-07-07 19:40:54 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.6561
2022-07-07 19:41:28 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.4650
2022-07-07 19:42:00 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.2869
2022-07-07 19:42:34 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.5281
2022-07-07 19:43:06 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.8015
2022-07-07 19:43:40 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.4011
2022-07-07 19:44:13 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.6917
2022-07-07 19:44:46 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.4634
2022-07-07 19:45:20 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.6067
2022-07-07 19:45:52 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.4026
2022-07-07 19:46:26 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.7621
2022-07-07 19:46:59 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.3619
2022-07-07 19:47:32 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.5028
2022-07-07 19:48:05 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.6544
2022-07-07 19:48:39 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.6086
2022-07-07 19:49:12 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.6600
2022-07-07 19:49:46 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.5513
2022-07-07 19:50:19 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.4159
2022-07-07 19:50:53 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.5392
2022-07-07 19:51:27 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.5236
2022-07-07 19:52:00 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.4581
2022-07-07 19:52:34 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.3635
2022-07-07 19:53:07 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.4422
2022-07-07 19:53:40 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.5964
2022-07-07 19:54:13 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.5318
2022-07-07 19:54:46 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.5443
2022-07-07 19:55:20 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.3758
2022-07-07 19:55:54 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.5420
2022-07-07 19:56:25 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.3827
2022-07-07 19:56:26 - train: epoch 047, train_loss: 1.4970
2022-07-07 19:57:39 - eval: epoch: 047, acc1: 66.930%, acc5: 87.786%, test_loss: 1.3558, per_image_load_time: 2.172ms, per_image_inference_time: 0.272ms
2022-07-07 19:57:39 - until epoch: 047, best_acc1: 67.926%
2022-07-07 19:57:39 - epoch 048 lr: 0.010000
2022-07-07 19:58:17 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.6858
2022-07-07 19:58:50 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.8240
2022-07-07 19:59:23 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.4469
2022-07-07 19:59:55 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.4792
2022-07-07 20:00:28 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.3164
2022-07-07 20:01:01 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.4640
2022-07-07 20:01:33 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.5457
2022-07-07 20:02:07 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.5548
2022-07-07 20:02:40 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.5391
2022-07-07 20:03:13 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.5100
2022-07-07 20:03:46 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.4933
2022-07-07 20:04:19 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.5888
2022-07-07 20:04:53 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.3155
2022-07-07 20:05:25 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.4239
2022-07-07 20:05:58 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.5981
2022-07-07 20:06:32 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.4569
2022-07-07 20:07:04 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.6047
2022-07-07 20:07:39 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.5107
2022-07-07 20:08:11 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.5354
2022-07-07 20:08:44 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.7327
2022-07-07 20:09:18 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.4919
2022-07-07 20:09:51 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.6865
2022-07-07 20:10:24 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.3862
2022-07-07 20:10:58 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.5616
2022-07-07 20:11:31 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.5140
2022-07-07 20:12:04 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.6796
2022-07-07 20:12:38 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.6680
2022-07-07 20:13:11 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.5049
2022-07-07 20:13:44 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.5909
2022-07-07 20:14:18 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.5704
2022-07-07 20:14:51 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.3733
2022-07-07 20:15:24 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.3505
2022-07-07 20:15:57 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.6287
2022-07-07 20:16:30 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.4864
2022-07-07 20:17:04 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.7501
2022-07-07 20:17:37 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.5561
2022-07-07 20:18:10 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.5504
2022-07-07 20:18:45 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.4464
2022-07-07 20:19:17 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.6157
2022-07-07 20:19:50 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.3171
2022-07-07 20:20:24 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.6651
2022-07-07 20:20:57 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.4160
2022-07-07 20:21:29 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.5087
2022-07-07 20:22:03 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.4392
2022-07-07 20:22:37 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.5028
2022-07-07 20:23:10 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.4979
2022-07-07 20:23:43 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.5542
2022-07-07 20:24:15 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.9040
2022-07-07 20:24:49 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.5515
2022-07-07 20:25:21 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.5051
2022-07-07 20:25:21 - train: epoch 048, train_loss: 1.4976
2022-07-07 20:26:34 - eval: epoch: 048, acc1: 66.618%, acc5: 87.638%, test_loss: 1.3657, per_image_load_time: 2.236ms, per_image_inference_time: 0.276ms
2022-07-07 20:26:34 - until epoch: 048, best_acc1: 67.926%
2022-07-07 20:26:34 - epoch 049 lr: 0.010000
2022-07-07 20:27:13 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6376
2022-07-07 20:27:45 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.4823
2022-07-07 20:28:18 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.5628
2022-07-07 20:28:52 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.4735
2022-07-07 20:29:24 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.4731
2022-07-07 20:29:57 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.4335
2022-07-07 20:30:30 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.5476
2022-07-07 20:31:04 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.8543
2022-07-07 20:31:36 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.4562
2022-07-07 20:32:10 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.5013
2022-07-07 20:32:43 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.3918
2022-07-07 20:33:16 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.1859
2022-07-07 20:33:49 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.6158
2022-07-07 20:34:23 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.6913
2022-07-07 20:34:56 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.4584
2022-07-07 20:35:29 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.6849
2022-07-07 20:36:02 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.5615
2022-07-07 20:36:36 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.4242
2022-07-07 20:37:08 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.4975
2022-07-07 20:37:42 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.3353
2022-07-07 20:38:15 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.4423
2022-07-07 20:38:48 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.4959
2022-07-07 20:39:21 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.4049
2022-07-07 20:39:54 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.6038
2022-07-07 20:40:27 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.4634
2022-07-07 20:41:01 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.5041
2022-07-07 20:41:33 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.3750
2022-07-07 20:42:07 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.3633
2022-07-07 20:42:40 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.6466
2022-07-07 20:43:13 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.6446
2022-07-07 20:43:47 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.5647
2022-07-07 20:44:20 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.5451
2022-07-07 20:44:53 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.5439
2022-07-07 20:45:27 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.6849
2022-07-07 20:46:00 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.5698
2022-07-07 20:46:33 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.7075
2022-07-07 20:47:06 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.5837
2022-07-07 20:47:40 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.7144
2022-07-07 20:48:13 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.7352
2022-07-07 20:48:46 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.5313
2022-07-07 20:49:19 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.3905
2022-07-07 20:49:52 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.5295
2022-07-07 20:50:26 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.8414
2022-07-07 20:50:59 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.5056
2022-07-07 20:51:32 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.4411
2022-07-07 20:52:06 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.5327
2022-07-07 20:52:39 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.7186
2022-07-07 20:53:13 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.3348
2022-07-07 20:53:46 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.3747
2022-07-07 20:54:17 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.4003
2022-07-07 20:54:18 - train: epoch 049, train_loss: 1.4934
2022-07-07 20:55:32 - eval: epoch: 049, acc1: 67.330%, acc5: 87.936%, test_loss: 1.3328, per_image_load_time: 2.543ms, per_image_inference_time: 0.245ms
2022-07-07 20:55:32 - until epoch: 049, best_acc1: 67.926%
2022-07-07 20:55:32 - epoch 050 lr: 0.010000
2022-07-07 20:56:10 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.7148
2022-07-07 20:56:43 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.4750
2022-07-07 20:57:16 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.5234
2022-07-07 20:57:48 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.3337
2022-07-07 20:58:21 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.4972
2022-07-07 20:58:53 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.5313
2022-07-07 20:59:27 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.2541
2022-07-07 20:59:59 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.2753
2022-07-07 21:00:32 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.4362
2022-07-07 21:01:05 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.6209
2022-07-07 21:01:39 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.5796
2022-07-07 21:02:12 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.4609
2022-07-07 21:02:45 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.3937
2022-07-07 21:03:18 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.5507
2022-07-07 21:03:51 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.5350
2022-07-07 21:04:25 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.4768
2022-07-07 21:04:58 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.5540
2022-07-07 21:05:31 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.5376
2022-07-07 21:06:04 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.4901
2022-07-07 21:06:38 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.5234
2022-07-07 21:07:11 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.2746
2022-07-07 21:07:44 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.5372
2022-07-07 21:08:18 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.3566
2022-07-07 21:08:52 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.4877
2022-07-07 21:09:24 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.5887
2022-07-07 21:09:58 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.4061
2022-07-07 21:10:31 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.4807
2022-07-07 21:11:04 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.6723
2022-07-07 21:11:38 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.6530
2022-07-07 21:12:10 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.6145
2022-07-07 21:12:44 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.3839
2022-07-07 21:13:17 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.5055
2022-07-07 21:13:51 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.3374
2022-07-07 21:14:23 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.3900
2022-07-07 21:14:57 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.5032
2022-07-07 21:15:30 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.5824
2022-07-07 21:16:03 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.5804
2022-07-07 21:16:36 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.3480
