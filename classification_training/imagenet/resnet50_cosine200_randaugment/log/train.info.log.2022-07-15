2022-07-14 21:26:16 - train: epoch 0061, iter [00600, 05004], lr: 0.081547, loss: 2.9392
2022-07-14 21:26:50 - train: epoch 0061, iter [00700, 05004], lr: 0.081535, loss: 2.6523
2022-07-14 21:27:24 - train: epoch 0061, iter [00800, 05004], lr: 0.081522, loss: 2.9994
2022-07-14 21:27:58 - train: epoch 0061, iter [00900, 05004], lr: 0.081510, loss: 2.9108
2022-07-14 21:28:33 - train: epoch 0061, iter [01000, 05004], lr: 0.081497, loss: 2.8726
2022-07-14 21:29:07 - train: epoch 0061, iter [01100, 05004], lr: 0.081485, loss: 2.5121
2022-07-14 21:29:42 - train: epoch 0061, iter [01200, 05004], lr: 0.081472, loss: 2.7488
2022-07-14 21:30:16 - train: epoch 0061, iter [01300, 05004], lr: 0.081460, loss: 2.6398
2022-07-14 21:30:51 - train: epoch 0061, iter [01400, 05004], lr: 0.081447, loss: 2.6928
2022-07-14 21:31:25 - train: epoch 0061, iter [01500, 05004], lr: 0.081435, loss: 2.9395
2022-07-14 21:32:00 - train: epoch 0061, iter [01600, 05004], lr: 0.081422, loss: 2.3591
2022-07-14 21:32:33 - train: epoch 0061, iter [01700, 05004], lr: 0.081410, loss: 2.8461
2022-07-14 21:33:08 - train: epoch 0061, iter [01800, 05004], lr: 0.081397, loss: 2.8112
2022-07-14 21:33:42 - train: epoch 0061, iter [01900, 05004], lr: 0.081385, loss: 3.0323
2022-07-14 21:34:16 - train: epoch 0061, iter [02000, 05004], lr: 0.081372, loss: 2.5835
2022-07-14 21:34:49 - train: epoch 0061, iter [02100, 05004], lr: 0.081360, loss: 2.7565
2022-07-14 21:35:24 - train: epoch 0061, iter [02200, 05004], lr: 0.081347, loss: 2.8226
2022-07-14 21:35:58 - train: epoch 0061, iter [02300, 05004], lr: 0.081335, loss: 2.7661
2022-07-14 21:36:31 - train: epoch 0061, iter [02400, 05004], lr: 0.081322, loss: 2.6523
2022-07-14 21:37:05 - train: epoch 0061, iter [02500, 05004], lr: 0.081310, loss: 2.5793
2022-07-14 21:37:39 - train: epoch 0061, iter [02600, 05004], lr: 0.081297, loss: 2.7989
2022-07-14 21:38:13 - train: epoch 0061, iter [02700, 05004], lr: 0.081284, loss: 2.8302
2022-07-14 21:38:47 - train: epoch 0061, iter [02800, 05004], lr: 0.081272, loss: 2.7344
2022-07-14 21:39:20 - train: epoch 0061, iter [02900, 05004], lr: 0.081259, loss: 3.0067
2022-07-14 21:39:55 - train: epoch 0061, iter [03000, 05004], lr: 0.081247, loss: 2.9569
2022-07-14 21:40:28 - train: epoch 0061, iter [03100, 05004], lr: 0.081234, loss: 2.8441
2022-07-14 21:41:02 - train: epoch 0061, iter [03200, 05004], lr: 0.081222, loss: 2.5582
2022-07-14 21:41:36 - train: epoch 0061, iter [03300, 05004], lr: 0.081209, loss: 2.7171
2022-07-14 21:42:09 - train: epoch 0061, iter [03400, 05004], lr: 0.081196, loss: 2.7365
2022-07-14 21:42:43 - train: epoch 0061, iter [03500, 05004], lr: 0.081184, loss: 2.7196
2022-07-14 21:43:17 - train: epoch 0061, iter [03600, 05004], lr: 0.081171, loss: 2.7124
2022-07-14 21:43:51 - train: epoch 0061, iter [03700, 05004], lr: 0.081159, loss: 2.6069
2022-07-14 21:44:25 - train: epoch 0061, iter [03800, 05004], lr: 0.081146, loss: 2.7148
2022-07-14 21:44:59 - train: epoch 0061, iter [03900, 05004], lr: 0.081133, loss: 2.7286
2022-07-14 21:45:33 - train: epoch 0061, iter [04000, 05004], lr: 0.081121, loss: 2.7433
2022-07-14 21:46:08 - train: epoch 0061, iter [04100, 05004], lr: 0.081108, loss: 2.8203
2022-07-14 21:46:42 - train: epoch 0061, iter [04200, 05004], lr: 0.081096, loss: 2.7776
2022-07-14 21:47:15 - train: epoch 0061, iter [04300, 05004], lr: 0.081083, loss: 2.5033
2022-07-14 21:47:50 - train: epoch 0061, iter [04400, 05004], lr: 0.081070, loss: 2.6906
2022-07-14 21:48:23 - train: epoch 0061, iter [04500, 05004], lr: 0.081058, loss: 2.8457
2022-07-14 21:48:59 - train: epoch 0061, iter [04600, 05004], lr: 0.081045, loss: 2.7968
2022-07-14 21:49:32 - train: epoch 0061, iter [04700, 05004], lr: 0.081033, loss: 2.9462
2022-07-14 21:50:06 - train: epoch 0061, iter [04800, 05004], lr: 0.081020, loss: 2.5086
2022-07-14 21:50:41 - train: epoch 0061, iter [04900, 05004], lr: 0.081007, loss: 2.8160
2022-07-14 21:51:13 - train: epoch 0061, iter [05000, 05004], lr: 0.080995, loss: 2.9221
2022-07-14 21:51:14 - train: epoch 061, train_loss: 2.7590
2022-07-14 21:52:27 - eval: epoch: 061, acc1: 59.502%, acc5: 83.246%, test_loss: 1.7092, per_image_load_time: 2.354ms, per_image_inference_time: 0.464ms
2022-07-14 21:52:27 - until epoch: 061, best_acc1: 59.502%
2022-07-14 21:52:27 - epoch 062 lr: 0.080994
2022-07-14 21:53:07 - train: epoch 0062, iter [00100, 05004], lr: 0.080982, loss: 2.7267
2022-07-14 21:53:40 - train: epoch 0062, iter [00200, 05004], lr: 0.080969, loss: 2.7671
2022-07-14 21:54:14 - train: epoch 0062, iter [00300, 05004], lr: 0.080956, loss: 2.5977
2022-07-14 21:54:47 - train: epoch 0062, iter [00400, 05004], lr: 0.080944, loss: 2.6572
2022-07-14 21:55:20 - train: epoch 0062, iter [00500, 05004], lr: 0.080931, loss: 2.6884
2022-07-14 21:55:53 - train: epoch 0062, iter [00600, 05004], lr: 0.080918, loss: 2.5469
2022-07-14 21:56:26 - train: epoch 0062, iter [00700, 05004], lr: 0.080906, loss: 2.7573
2022-07-14 21:56:59 - train: epoch 0062, iter [00800, 05004], lr: 0.080893, loss: 2.9073
2022-07-14 21:57:32 - train: epoch 0062, iter [00900, 05004], lr: 0.080880, loss: 2.8768
2022-07-14 21:58:05 - train: epoch 0062, iter [01000, 05004], lr: 0.080868, loss: 2.9565
2022-07-14 21:58:39 - train: epoch 0062, iter [01100, 05004], lr: 0.080855, loss: 2.8403
2022-07-14 21:59:12 - train: epoch 0062, iter [01200, 05004], lr: 0.080842, loss: 3.0164
2022-07-14 21:59:45 - train: epoch 0062, iter [01300, 05004], lr: 0.080830, loss: 2.6876
2022-07-14 22:00:18 - train: epoch 0062, iter [01400, 05004], lr: 0.080817, loss: 2.5996
2022-07-14 22:00:52 - train: epoch 0062, iter [01500, 05004], lr: 0.080804, loss: 2.7166
2022-07-14 22:01:24 - train: epoch 0062, iter [01600, 05004], lr: 0.080792, loss: 3.1831
2022-07-14 22:01:57 - train: epoch 0062, iter [01700, 05004], lr: 0.080779, loss: 2.8819
2022-07-14 22:02:30 - train: epoch 0062, iter [01800, 05004], lr: 0.080766, loss: 2.7986
2022-07-14 22:03:04 - train: epoch 0062, iter [01900, 05004], lr: 0.080754, loss: 2.5714
2022-07-14 22:03:38 - train: epoch 0062, iter [02000, 05004], lr: 0.080741, loss: 2.8588
2022-07-14 22:04:12 - train: epoch 0062, iter [02100, 05004], lr: 0.080728, loss: 2.8592
2022-07-14 22:04:45 - train: epoch 0062, iter [02200, 05004], lr: 0.080716, loss: 2.5117
2022-07-14 22:05:18 - train: epoch 0062, iter [02300, 05004], lr: 0.080703, loss: 2.6910
2022-07-14 22:05:51 - train: epoch 0062, iter [02400, 05004], lr: 0.080690, loss: 2.8682
2022-07-14 22:06:25 - train: epoch 0062, iter [02500, 05004], lr: 0.080677, loss: 2.6582
2022-07-14 22:06:59 - train: epoch 0062, iter [02600, 05004], lr: 0.080665, loss: 2.6209
2022-07-14 22:07:32 - train: epoch 0062, iter [02700, 05004], lr: 0.080652, loss: 2.8477
2022-07-14 22:08:06 - train: epoch 0062, iter [02800, 05004], lr: 0.080639, loss: 2.6812
2022-07-14 22:08:39 - train: epoch 0062, iter [02900, 05004], lr: 0.080627, loss: 3.0040
2022-07-14 22:09:13 - train: epoch 0062, iter [03000, 05004], lr: 0.080614, loss: 2.9462
2022-07-14 22:09:46 - train: epoch 0062, iter [03100, 05004], lr: 0.080601, loss: 2.8459
2022-07-14 22:10:20 - train: epoch 0062, iter [03200, 05004], lr: 0.080588, loss: 2.7400
2022-07-14 22:10:53 - train: epoch 0062, iter [03300, 05004], lr: 0.080576, loss: 3.1048
2022-07-14 22:11:27 - train: epoch 0062, iter [03400, 05004], lr: 0.080563, loss: 2.6934
2022-07-14 22:12:00 - train: epoch 0062, iter [03500, 05004], lr: 0.080550, loss: 2.5119
2022-07-14 22:12:33 - train: epoch 0062, iter [03600, 05004], lr: 0.080537, loss: 2.9210
2022-07-14 22:13:08 - train: epoch 0062, iter [03700, 05004], lr: 0.080525, loss: 2.6894
2022-07-14 22:13:41 - train: epoch 0062, iter [03800, 05004], lr: 0.080512, loss: 2.9478
2022-07-14 22:14:15 - train: epoch 0062, iter [03900, 05004], lr: 0.080499, loss: 2.7608
2022-07-14 22:14:48 - train: epoch 0062, iter [04000, 05004], lr: 0.080486, loss: 2.7063
2022-07-14 22:15:23 - train: epoch 0062, iter [04100, 05004], lr: 0.080474, loss: 2.8472
2022-07-14 22:15:55 - train: epoch 0062, iter [04200, 05004], lr: 0.080461, loss: 2.8252
2022-07-14 22:16:29 - train: epoch 0062, iter [04300, 05004], lr: 0.080448, loss: 2.9284
2022-07-14 22:17:03 - train: epoch 0062, iter [04400, 05004], lr: 0.080435, loss: 2.4818
2022-07-14 22:17:37 - train: epoch 0062, iter [04500, 05004], lr: 0.080423, loss: 2.7980
2022-07-14 22:18:10 - train: epoch 0062, iter [04600, 05004], lr: 0.080410, loss: 2.5058
2022-07-14 22:18:43 - train: epoch 0062, iter [04700, 05004], lr: 0.080397, loss: 2.7261
2022-07-14 22:19:16 - train: epoch 0062, iter [04800, 05004], lr: 0.080384, loss: 2.6401
2022-07-14 22:19:50 - train: epoch 0062, iter [04900, 05004], lr: 0.080371, loss: 2.7828
2022-07-14 22:20:22 - train: epoch 0062, iter [05000, 05004], lr: 0.080359, loss: 2.8198
2022-07-14 22:20:23 - train: epoch 062, train_loss: 2.7520
2022-07-14 22:21:36 - eval: epoch: 062, acc1: 58.436%, acc5: 82.674%, test_loss: 1.7383, per_image_load_time: 2.143ms, per_image_inference_time: 0.463ms
2022-07-14 22:21:36 - until epoch: 062, best_acc1: 59.502%
2022-07-14 22:21:36 - epoch 063 lr: 0.080358
2022-07-14 22:22:15 - train: epoch 0063, iter [00100, 05004], lr: 0.080345, loss: 2.5550
2022-07-14 22:22:49 - train: epoch 0063, iter [00200, 05004], lr: 0.080333, loss: 2.7042
2022-07-14 22:23:22 - train: epoch 0063, iter [00300, 05004], lr: 0.080320, loss: 2.9206
2022-07-14 22:23:55 - train: epoch 0063, iter [00400, 05004], lr: 0.080307, loss: 2.6159
2022-07-14 22:24:28 - train: epoch 0063, iter [00500, 05004], lr: 0.080294, loss: 2.7276
2022-07-14 22:25:01 - train: epoch 0063, iter [00600, 05004], lr: 0.080281, loss: 2.9330
2022-07-14 22:25:34 - train: epoch 0063, iter [00700, 05004], lr: 0.080269, loss: 2.8091
2022-07-14 22:26:06 - train: epoch 0063, iter [00800, 05004], lr: 0.080256, loss: 2.7578
2022-07-14 22:26:39 - train: epoch 0063, iter [00900, 05004], lr: 0.080243, loss: 3.0412
2022-07-14 22:27:12 - train: epoch 0063, iter [01000, 05004], lr: 0.080230, loss: 2.8647
2022-07-14 22:27:46 - train: epoch 0063, iter [01100, 05004], lr: 0.080217, loss: 2.6571
2022-07-14 22:28:18 - train: epoch 0063, iter [01200, 05004], lr: 0.080204, loss: 2.7466
2022-07-14 22:28:51 - train: epoch 0063, iter [01300, 05004], lr: 0.080192, loss: 2.4440
2022-07-14 22:29:25 - train: epoch 0063, iter [01400, 05004], lr: 0.080179, loss: 2.6957
2022-07-14 22:29:58 - train: epoch 0063, iter [01500, 05004], lr: 0.080166, loss: 2.7329
2022-07-14 22:30:31 - train: epoch 0063, iter [01600, 05004], lr: 0.080153, loss: 2.6977
2022-07-14 22:31:03 - train: epoch 0063, iter [01700, 05004], lr: 0.080140, loss: 2.7567
2022-07-14 22:31:36 - train: epoch 0063, iter [01800, 05004], lr: 0.080127, loss: 2.9275
2022-07-14 22:32:09 - train: epoch 0063, iter [01900, 05004], lr: 0.080115, loss: 2.6838
2022-07-14 22:32:43 - train: epoch 0063, iter [02000, 05004], lr: 0.080102, loss: 2.3965
2022-07-14 22:33:15 - train: epoch 0063, iter [02100, 05004], lr: 0.080089, loss: 2.6962
2022-07-14 22:33:49 - train: epoch 0063, iter [02200, 05004], lr: 0.080076, loss: 2.9639
2022-07-14 22:34:22 - train: epoch 0063, iter [02300, 05004], lr: 0.080063, loss: 3.1305
2022-07-14 22:34:55 - train: epoch 0063, iter [02400, 05004], lr: 0.080050, loss: 2.8409
2022-07-14 22:35:29 - train: epoch 0063, iter [02500, 05004], lr: 0.080037, loss: 2.8666
2022-07-14 22:36:02 - train: epoch 0063, iter [02600, 05004], lr: 0.080024, loss: 2.7498
2022-07-14 22:36:36 - train: epoch 0063, iter [02700, 05004], lr: 0.080012, loss: 3.0244
2022-07-14 22:37:09 - train: epoch 0063, iter [02800, 05004], lr: 0.079999, loss: 2.8472
2022-07-14 22:37:42 - train: epoch 0063, iter [02900, 05004], lr: 0.079986, loss: 3.0642
2022-07-14 22:38:16 - train: epoch 0063, iter [03000, 05004], lr: 0.079973, loss: 2.8763
2022-07-14 22:38:49 - train: epoch 0063, iter [03100, 05004], lr: 0.079960, loss: 2.8516
2022-07-14 22:39:22 - train: epoch 0063, iter [03200, 05004], lr: 0.079947, loss: 2.7967
2022-07-14 22:39:55 - train: epoch 0063, iter [03300, 05004], lr: 0.079934, loss: 2.7356
2022-07-14 22:40:29 - train: epoch 0063, iter [03400, 05004], lr: 0.079921, loss: 2.6190
2022-07-14 22:41:02 - train: epoch 0063, iter [03500, 05004], lr: 0.079909, loss: 2.8163
2022-07-14 22:41:35 - train: epoch 0063, iter [03600, 05004], lr: 0.079896, loss: 2.6123
2022-07-14 22:42:08 - train: epoch 0063, iter [03700, 05004], lr: 0.079883, loss: 2.9020
2022-07-14 22:42:42 - train: epoch 0063, iter [03800, 05004], lr: 0.079870, loss: 2.8258
2022-07-14 22:43:15 - train: epoch 0063, iter [03900, 05004], lr: 0.079857, loss: 2.8285
2022-07-14 22:43:49 - train: epoch 0063, iter [04000, 05004], lr: 0.079844, loss: 2.7526
2022-07-14 22:44:22 - train: epoch 0063, iter [04100, 05004], lr: 0.079831, loss: 2.7950
2022-07-14 22:44:56 - train: epoch 0063, iter [04200, 05004], lr: 0.079818, loss: 2.8265
2022-07-14 22:45:30 - train: epoch 0063, iter [04300, 05004], lr: 0.079805, loss: 2.7891
2022-07-14 22:46:03 - train: epoch 0063, iter [04400, 05004], lr: 0.079792, loss: 2.6944
2022-07-14 22:46:36 - train: epoch 0063, iter [04500, 05004], lr: 0.079779, loss: 2.9387
2022-07-14 22:47:09 - train: epoch 0063, iter [04600, 05004], lr: 0.079766, loss: 2.8373
2022-07-14 22:47:42 - train: epoch 0063, iter [04700, 05004], lr: 0.079753, loss: 2.7678
2022-07-14 22:48:15 - train: epoch 0063, iter [04800, 05004], lr: 0.079741, loss: 2.5525
2022-07-14 22:48:49 - train: epoch 0063, iter [04900, 05004], lr: 0.079728, loss: 2.6173
2022-07-14 22:49:20 - train: epoch 0063, iter [05000, 05004], lr: 0.079715, loss: 2.8244
2022-07-14 22:49:21 - train: epoch 063, train_loss: 2.7476
2022-07-14 22:50:34 - eval: epoch: 063, acc1: 59.348%, acc5: 83.150%, test_loss: 1.7028, per_image_load_time: 1.947ms, per_image_inference_time: 0.477ms
2022-07-14 22:50:34 - until epoch: 063, best_acc1: 59.502%
2022-07-14 22:50:34 - epoch 064 lr: 0.079714
2022-07-14 22:51:13 - train: epoch 0064, iter [00100, 05004], lr: 0.079701, loss: 2.7092
2022-07-14 22:51:46 - train: epoch 0064, iter [00200, 05004], lr: 0.079688, loss: 2.7304
2022-07-14 22:52:20 - train: epoch 0064, iter [00300, 05004], lr: 0.079675, loss: 2.3669
2022-07-14 22:52:53 - train: epoch 0064, iter [00400, 05004], lr: 0.079662, loss: 2.5395
2022-07-14 22:53:25 - train: epoch 0064, iter [00500, 05004], lr: 0.079649, loss: 2.5772
2022-07-14 22:53:59 - train: epoch 0064, iter [00600, 05004], lr: 0.079636, loss: 2.8216
2022-07-14 22:54:32 - train: epoch 0064, iter [00700, 05004], lr: 0.079623, loss: 2.8076
2022-07-14 22:55:06 - train: epoch 0064, iter [00800, 05004], lr: 0.079610, loss: 2.7597
2022-07-14 22:55:39 - train: epoch 0064, iter [00900, 05004], lr: 0.079598, loss: 2.7697
2022-07-14 22:56:13 - train: epoch 0064, iter [01000, 05004], lr: 0.079585, loss: 2.6625
2022-07-14 22:56:46 - train: epoch 0064, iter [01100, 05004], lr: 0.079572, loss: 2.7404
2022-07-14 22:57:19 - train: epoch 0064, iter [01200, 05004], lr: 0.079559, loss: 2.5822
2022-07-14 22:57:53 - train: epoch 0064, iter [01300, 05004], lr: 0.079546, loss: 2.8354
2022-07-14 22:58:26 - train: epoch 0064, iter [01400, 05004], lr: 0.079533, loss: 2.9884
2022-07-14 22:59:00 - train: epoch 0064, iter [01500, 05004], lr: 0.079520, loss: 2.7388
2022-07-14 22:59:33 - train: epoch 0064, iter [01600, 05004], lr: 0.079507, loss: 2.4893
2022-07-14 23:00:06 - train: epoch 0064, iter [01700, 05004], lr: 0.079494, loss: 2.8699
2022-07-14 23:00:39 - train: epoch 0064, iter [01800, 05004], lr: 0.079481, loss: 2.8602
2022-07-14 23:01:12 - train: epoch 0064, iter [01900, 05004], lr: 0.079468, loss: 2.7316
2022-07-14 23:01:45 - train: epoch 0064, iter [02000, 05004], lr: 0.079455, loss: 2.7919
2022-07-14 23:02:19 - train: epoch 0064, iter [02100, 05004], lr: 0.079442, loss: 2.8940
2022-07-14 23:02:52 - train: epoch 0064, iter [02200, 05004], lr: 0.079429, loss: 2.7800
2022-07-14 23:03:26 - train: epoch 0064, iter [02300, 05004], lr: 0.079416, loss: 2.7612
2022-07-14 23:03:58 - train: epoch 0064, iter [02400, 05004], lr: 0.079403, loss: 2.8302
2022-07-14 23:04:32 - train: epoch 0064, iter [02500, 05004], lr: 0.079390, loss: 2.8553
2022-07-14 23:05:04 - train: epoch 0064, iter [02600, 05004], lr: 0.079376, loss: 2.6769
2022-07-14 23:05:38 - train: epoch 0064, iter [02700, 05004], lr: 0.079363, loss: 2.7428
2022-07-14 23:06:11 - train: epoch 0064, iter [02800, 05004], lr: 0.079350, loss: 2.7349
2022-07-14 23:06:44 - train: epoch 0064, iter [02900, 05004], lr: 0.079337, loss: 2.7224
2022-07-14 23:07:18 - train: epoch 0064, iter [03000, 05004], lr: 0.079324, loss: 2.8392
2022-07-14 23:07:50 - train: epoch 0064, iter [03100, 05004], lr: 0.079311, loss: 2.6704
2022-07-14 23:08:24 - train: epoch 0064, iter [03200, 05004], lr: 0.079298, loss: 2.5974
2022-07-14 23:08:57 - train: epoch 0064, iter [03300, 05004], lr: 0.079285, loss: 2.5954
2022-07-14 23:09:31 - train: epoch 0064, iter [03400, 05004], lr: 0.079272, loss: 2.7990
2022-07-14 23:10:03 - train: epoch 0064, iter [03500, 05004], lr: 0.079259, loss: 2.8074
2022-07-14 23:10:37 - train: epoch 0064, iter [03600, 05004], lr: 0.079246, loss: 2.5760
2022-07-14 23:11:10 - train: epoch 0064, iter [03700, 05004], lr: 0.079233, loss: 2.7181
2022-07-14 23:11:44 - train: epoch 0064, iter [03800, 05004], lr: 0.079220, loss: 2.9144
2022-07-14 23:12:17 - train: epoch 0064, iter [03900, 05004], lr: 0.079207, loss: 2.6736
2022-07-14 23:12:51 - train: epoch 0064, iter [04000, 05004], lr: 0.079194, loss: 2.4867
2022-07-14 23:13:25 - train: epoch 0064, iter [04100, 05004], lr: 0.079181, loss: 2.8090
2022-07-14 23:13:59 - train: epoch 0064, iter [04200, 05004], lr: 0.079168, loss: 2.8598
2022-07-14 23:14:33 - train: epoch 0064, iter [04300, 05004], lr: 0.079155, loss: 3.1354
2022-07-14 23:15:07 - train: epoch 0064, iter [04400, 05004], lr: 0.079142, loss: 2.5192
2022-07-14 23:15:41 - train: epoch 0064, iter [04500, 05004], lr: 0.079128, loss: 2.7085
2022-07-14 23:16:15 - train: epoch 0064, iter [04600, 05004], lr: 0.079115, loss: 2.8388
2022-07-14 23:16:49 - train: epoch 0064, iter [04700, 05004], lr: 0.079102, loss: 3.1184
2022-07-14 23:17:23 - train: epoch 0064, iter [04800, 05004], lr: 0.079089, loss: 2.7032
2022-07-14 23:17:57 - train: epoch 0064, iter [04900, 05004], lr: 0.079076, loss: 2.9622
2022-07-14 23:18:30 - train: epoch 0064, iter [05000, 05004], lr: 0.079063, loss: 2.7324
2022-07-14 23:18:31 - train: epoch 064, train_loss: 2.7450
2022-07-14 23:19:45 - eval: epoch: 064, acc1: 55.066%, acc5: 79.614%, test_loss: 1.9395, per_image_load_time: 2.322ms, per_image_inference_time: 0.468ms
2022-07-14 23:19:45 - until epoch: 064, best_acc1: 59.502%
2022-07-14 23:19:45 - epoch 065 lr: 0.079062
2022-07-14 23:20:25 - train: epoch 0065, iter [00100, 05004], lr: 0.079049, loss: 2.7640
2022-07-14 23:20:59 - train: epoch 0065, iter [00200, 05004], lr: 0.079036, loss: 2.7402
2022-07-14 23:21:33 - train: epoch 0065, iter [00300, 05004], lr: 0.079023, loss: 2.6420
2022-07-14 23:22:07 - train: epoch 0065, iter [00400, 05004], lr: 0.079010, loss: 2.7094
2022-07-14 23:22:41 - train: epoch 0065, iter [00500, 05004], lr: 0.078997, loss: 2.9584
2022-07-14 23:23:16 - train: epoch 0065, iter [00600, 05004], lr: 0.078984, loss: 2.7405
2022-07-14 23:23:51 - train: epoch 0065, iter [00700, 05004], lr: 0.078971, loss: 2.9361
2022-07-14 23:24:26 - train: epoch 0065, iter [00800, 05004], lr: 0.078958, loss: 2.6502
2022-07-14 23:25:00 - train: epoch 0065, iter [00900, 05004], lr: 0.078944, loss: 2.7704
2022-07-14 23:25:35 - train: epoch 0065, iter [01000, 05004], lr: 0.078931, loss: 2.6627
2022-07-14 23:26:09 - train: epoch 0065, iter [01100, 05004], lr: 0.078918, loss: 2.5120
2022-07-14 23:26:43 - train: epoch 0065, iter [01200, 05004], lr: 0.078905, loss: 2.8763
2022-07-14 23:27:18 - train: epoch 0065, iter [01300, 05004], lr: 0.078892, loss: 2.6595
2022-07-14 23:27:53 - train: epoch 0065, iter [01400, 05004], lr: 0.078879, loss: 2.6328
2022-07-14 23:28:28 - train: epoch 0065, iter [01500, 05004], lr: 0.078866, loss: 2.4756
2022-07-14 23:29:02 - train: epoch 0065, iter [01600, 05004], lr: 0.078852, loss: 3.1237
2022-07-14 23:29:36 - train: epoch 0065, iter [01700, 05004], lr: 0.078839, loss: 2.7725
2022-07-14 23:30:11 - train: epoch 0065, iter [01800, 05004], lr: 0.078826, loss: 2.5197
2022-07-14 23:30:45 - train: epoch 0065, iter [01900, 05004], lr: 0.078813, loss: 2.4632
2022-07-14 23:31:21 - train: epoch 0065, iter [02000, 05004], lr: 0.078800, loss: 2.7314
2022-07-14 23:31:56 - train: epoch 0065, iter [02100, 05004], lr: 0.078787, loss: 2.7071
2022-07-14 23:32:31 - train: epoch 0065, iter [02200, 05004], lr: 0.078774, loss: 2.7100
2022-07-14 23:33:06 - train: epoch 0065, iter [02300, 05004], lr: 0.078760, loss: 2.7855
2022-07-14 23:33:41 - train: epoch 0065, iter [02400, 05004], lr: 0.078747, loss: 2.7479
2022-07-14 23:34:15 - train: epoch 0065, iter [02500, 05004], lr: 0.078734, loss: 2.7568
2022-07-14 23:34:51 - train: epoch 0065, iter [02600, 05004], lr: 0.078721, loss: 2.7980
2022-07-14 23:35:25 - train: epoch 0065, iter [02700, 05004], lr: 0.078708, loss: 2.5948
2022-07-14 23:36:00 - train: epoch 0065, iter [02800, 05004], lr: 0.078695, loss: 2.9866
2022-07-14 23:36:35 - train: epoch 0065, iter [02900, 05004], lr: 0.078681, loss: 2.6579
2022-07-14 23:37:09 - train: epoch 0065, iter [03000, 05004], lr: 0.078668, loss: 2.4894
2022-07-14 23:37:44 - train: epoch 0065, iter [03100, 05004], lr: 0.078655, loss: 2.9677
2022-07-14 23:38:19 - train: epoch 0065, iter [03200, 05004], lr: 0.078642, loss: 2.8461
2022-07-14 23:38:53 - train: epoch 0065, iter [03300, 05004], lr: 0.078629, loss: 2.8319
2022-07-14 23:39:27 - train: epoch 0065, iter [03400, 05004], lr: 0.078615, loss: 2.8700
2022-07-14 23:40:02 - train: epoch 0065, iter [03500, 05004], lr: 0.078602, loss: 2.8856
2022-07-14 23:40:36 - train: epoch 0065, iter [03600, 05004], lr: 0.078589, loss: 2.7248
2022-07-14 23:41:11 - train: epoch 0065, iter [03700, 05004], lr: 0.078576, loss: 2.7173
2022-07-14 23:41:45 - train: epoch 0065, iter [03800, 05004], lr: 0.078563, loss: 2.8076
2022-07-14 23:42:20 - train: epoch 0065, iter [03900, 05004], lr: 0.078549, loss: 2.7653
2022-07-14 23:42:54 - train: epoch 0065, iter [04000, 05004], lr: 0.078536, loss: 2.9760
2022-07-14 23:43:29 - train: epoch 0065, iter [04100, 05004], lr: 0.078523, loss: 2.5813
2022-07-14 23:44:03 - train: epoch 0065, iter [04200, 05004], lr: 0.078510, loss: 2.6990
2022-07-14 23:44:39 - train: epoch 0065, iter [04300, 05004], lr: 0.078496, loss: 2.8124
2022-07-14 23:45:13 - train: epoch 0065, iter [04400, 05004], lr: 0.078483, loss: 2.8078
2022-07-14 23:45:47 - train: epoch 0065, iter [04500, 05004], lr: 0.078470, loss: 2.7616
2022-07-14 23:46:22 - train: epoch 0065, iter [04600, 05004], lr: 0.078457, loss: 2.6060
2022-07-14 23:46:56 - train: epoch 0065, iter [04700, 05004], lr: 0.078443, loss: 2.4754
2022-07-14 23:47:31 - train: epoch 0065, iter [04800, 05004], lr: 0.078430, loss: 2.7132
2022-07-14 23:48:06 - train: epoch 0065, iter [04900, 05004], lr: 0.078417, loss: 2.7918
2022-07-14 23:48:38 - train: epoch 0065, iter [05000, 05004], lr: 0.078404, loss: 2.9195
2022-07-14 23:48:39 - train: epoch 065, train_loss: 2.7355
2022-07-14 23:49:53 - eval: epoch: 065, acc1: 58.796%, acc5: 82.990%, test_loss: 1.7257, per_image_load_time: 1.797ms, per_image_inference_time: 0.471ms
2022-07-14 23:49:53 - until epoch: 065, best_acc1: 59.502%
2022-07-14 23:49:53 - epoch 066 lr: 0.078403
2022-07-14 23:50:33 - train: epoch 0066, iter [00100, 05004], lr: 0.078390, loss: 2.5472
2022-07-14 23:51:07 - train: epoch 0066, iter [00200, 05004], lr: 0.078377, loss: 3.1060
2022-07-14 23:51:42 - train: epoch 0066, iter [00300, 05004], lr: 0.078363, loss: 3.0405
2022-07-14 23:52:16 - train: epoch 0066, iter [00400, 05004], lr: 0.078350, loss: 2.5775
2022-07-14 23:52:50 - train: epoch 0066, iter [00500, 05004], lr: 0.078337, loss: 2.5223
2022-07-14 23:53:24 - train: epoch 0066, iter [00600, 05004], lr: 0.078324, loss: 2.5480
2022-07-14 23:53:58 - train: epoch 0066, iter [00700, 05004], lr: 0.078310, loss: 2.7833
2022-07-14 23:54:33 - train: epoch 0066, iter [00800, 05004], lr: 0.078297, loss: 2.9457
2022-07-14 23:55:08 - train: epoch 0066, iter [00900, 05004], lr: 0.078284, loss: 2.7351
2022-07-14 23:55:41 - train: epoch 0066, iter [01000, 05004], lr: 0.078271, loss: 2.4696
2022-07-14 23:56:16 - train: epoch 0066, iter [01100, 05004], lr: 0.078257, loss: 2.9713
2022-07-14 23:56:50 - train: epoch 0066, iter [01200, 05004], lr: 0.078244, loss: 2.7800
2022-07-14 23:57:25 - train: epoch 0066, iter [01300, 05004], lr: 0.078231, loss: 2.8192
2022-07-14 23:57:59 - train: epoch 0066, iter [01400, 05004], lr: 0.078217, loss: 2.5543
2022-07-14 23:58:35 - train: epoch 0066, iter [01500, 05004], lr: 0.078204, loss: 2.6165
2022-07-14 23:59:08 - train: epoch 0066, iter [01600, 05004], lr: 0.078191, loss: 2.9036
2022-07-14 23:59:42 - train: epoch 0066, iter [01700, 05004], lr: 0.078178, loss: 2.8386
2022-07-15 00:00:17 - train: epoch 0066, iter [01800, 05004], lr: 0.078164, loss: 2.5414
2022-07-15 00:00:52 - train: epoch 0066, iter [01900, 05004], lr: 0.078151, loss: 2.7944
2022-07-15 00:01:26 - train: epoch 0066, iter [02000, 05004], lr: 0.078138, loss: 2.6996
2022-07-15 00:02:00 - train: epoch 0066, iter [02100, 05004], lr: 0.078124, loss: 2.5096
2022-07-15 00:02:34 - train: epoch 0066, iter [02200, 05004], lr: 0.078111, loss: 2.7071
2022-07-15 00:03:09 - train: epoch 0066, iter [02300, 05004], lr: 0.078098, loss: 2.6990
2022-07-15 00:03:44 - train: epoch 0066, iter [02400, 05004], lr: 0.078084, loss: 2.8712
2022-07-15 00:04:18 - train: epoch 0066, iter [02500, 05004], lr: 0.078071, loss: 2.7405
2022-07-15 00:04:52 - train: epoch 0066, iter [02600, 05004], lr: 0.078058, loss: 2.4800
2022-07-15 00:05:27 - train: epoch 0066, iter [02700, 05004], lr: 0.078044, loss: 2.8292
2022-07-15 00:06:02 - train: epoch 0066, iter [02800, 05004], lr: 0.078031, loss: 2.8565
2022-07-15 00:06:37 - train: epoch 0066, iter [02900, 05004], lr: 0.078018, loss: 3.0458
2022-07-15 00:07:11 - train: epoch 0066, iter [03000, 05004], lr: 0.078004, loss: 2.4623
2022-07-15 00:07:46 - train: epoch 0066, iter [03100, 05004], lr: 0.077991, loss: 2.7503
2022-07-15 00:08:19 - train: epoch 0066, iter [03200, 05004], lr: 0.077978, loss: 2.6117
2022-07-15 00:08:55 - train: epoch 0066, iter [03300, 05004], lr: 0.077964, loss: 2.4446
2022-07-15 00:09:29 - train: epoch 0066, iter [03400, 05004], lr: 0.077951, loss: 2.8956
2022-07-15 00:10:04 - train: epoch 0066, iter [03500, 05004], lr: 0.077938, loss: 2.8681
2022-07-15 00:10:39 - train: epoch 0066, iter [03600, 05004], lr: 0.077924, loss: 2.6539
2022-07-15 00:11:14 - train: epoch 0066, iter [03700, 05004], lr: 0.077911, loss: 2.7708
2022-07-15 00:11:49 - train: epoch 0066, iter [03800, 05004], lr: 0.077898, loss: 2.7707
2022-07-15 00:12:23 - train: epoch 0066, iter [03900, 05004], lr: 0.077884, loss: 2.5744
2022-07-15 00:12:58 - train: epoch 0066, iter [04000, 05004], lr: 0.077871, loss: 2.6479
2022-07-15 00:13:32 - train: epoch 0066, iter [04100, 05004], lr: 0.077858, loss: 2.6967
2022-07-15 00:14:06 - train: epoch 0066, iter [04200, 05004], lr: 0.077844, loss: 2.6857
2022-07-15 00:14:41 - train: epoch 0066, iter [04300, 05004], lr: 0.077831, loss: 2.4859
2022-07-15 00:15:17 - train: epoch 0066, iter [04400, 05004], lr: 0.077817, loss: 2.7330
2022-07-15 00:15:51 - train: epoch 0066, iter [04500, 05004], lr: 0.077804, loss: 3.0193
2022-07-15 00:16:26 - train: epoch 0066, iter [04600, 05004], lr: 0.077791, loss: 2.9574
2022-07-15 00:17:00 - train: epoch 0066, iter [04700, 05004], lr: 0.077777, loss: 2.6070
2022-07-15 00:17:35 - train: epoch 0066, iter [04800, 05004], lr: 0.077764, loss: 2.7831
2022-07-15 00:18:09 - train: epoch 0066, iter [04900, 05004], lr: 0.077751, loss: 2.7261
2022-07-15 00:18:42 - train: epoch 0066, iter [05000, 05004], lr: 0.077737, loss: 2.7686
2022-07-15 00:18:43 - train: epoch 066, train_loss: 2.7301
2022-07-15 00:19:57 - eval: epoch: 066, acc1: 56.408%, acc5: 81.096%, test_loss: 1.8384, per_image_load_time: 1.860ms, per_image_inference_time: 0.481ms
2022-07-15 00:19:57 - until epoch: 066, best_acc1: 59.502%
2022-07-15 00:19:57 - epoch 067 lr: 0.077737
2022-07-15 00:20:37 - train: epoch 0067, iter [00100, 05004], lr: 0.077723, loss: 2.4702
2022-07-15 00:21:11 - train: epoch 0067, iter [00200, 05004], lr: 0.077710, loss: 2.6173
2022-07-15 00:21:45 - train: epoch 0067, iter [00300, 05004], lr: 0.077696, loss: 2.7157
2022-07-15 00:22:20 - train: epoch 0067, iter [00400, 05004], lr: 0.077683, loss: 2.7624
2022-07-15 00:22:54 - train: epoch 0067, iter [00500, 05004], lr: 0.077670, loss: 2.7315
2022-07-15 00:23:29 - train: epoch 0067, iter [00600, 05004], lr: 0.077656, loss: 2.5701
2022-07-15 00:24:03 - train: epoch 0067, iter [00700, 05004], lr: 0.077643, loss: 2.7114
2022-07-15 00:24:37 - train: epoch 0067, iter [00800, 05004], lr: 0.077629, loss: 2.6530
2022-07-15 00:25:11 - train: epoch 0067, iter [00900, 05004], lr: 0.077616, loss: 3.0819
2022-07-15 00:25:46 - train: epoch 0067, iter [01000, 05004], lr: 0.077603, loss: 2.6014
2022-07-15 00:26:20 - train: epoch 0067, iter [01100, 05004], lr: 0.077589, loss: 2.6276
2022-07-15 00:26:53 - train: epoch 0067, iter [01200, 05004], lr: 0.077576, loss: 2.7529
2022-07-15 00:27:28 - train: epoch 0067, iter [01300, 05004], lr: 0.077562, loss: 2.7628
2022-07-15 00:28:02 - train: epoch 0067, iter [01400, 05004], lr: 0.077549, loss: 2.7387
2022-07-15 00:28:37 - train: epoch 0067, iter [01500, 05004], lr: 0.077535, loss: 2.5111
2022-07-15 00:29:10 - train: epoch 0067, iter [01600, 05004], lr: 0.077522, loss: 2.6350
2022-07-15 00:29:45 - train: epoch 0067, iter [01700, 05004], lr: 0.077509, loss: 2.7192
2022-07-15 00:30:20 - train: epoch 0067, iter [01800, 05004], lr: 0.077495, loss: 3.0410
2022-07-15 00:30:54 - train: epoch 0067, iter [01900, 05004], lr: 0.077482, loss: 2.7036
2022-07-15 00:31:28 - train: epoch 0067, iter [02000, 05004], lr: 0.077468, loss: 2.9083
2022-07-15 00:32:03 - train: epoch 0067, iter [02100, 05004], lr: 0.077455, loss: 2.5509
2022-07-15 00:32:38 - train: epoch 0067, iter [02200, 05004], lr: 0.077441, loss: 2.7453
2022-07-15 00:33:13 - train: epoch 0067, iter [02300, 05004], lr: 0.077428, loss: 2.8030
2022-07-15 00:33:47 - train: epoch 0067, iter [02400, 05004], lr: 0.077414, loss: 2.7553
2022-07-15 00:34:21 - train: epoch 0067, iter [02500, 05004], lr: 0.077401, loss: 2.7486
2022-07-15 00:34:56 - train: epoch 0067, iter [02600, 05004], lr: 0.077387, loss: 2.8560
2022-07-15 00:35:31 - train: epoch 0067, iter [02700, 05004], lr: 0.077374, loss: 2.4432
2022-07-15 00:36:05 - train: epoch 0067, iter [02800, 05004], lr: 0.077360, loss: 2.9748
2022-07-15 00:36:39 - train: epoch 0067, iter [02900, 05004], lr: 0.077347, loss: 2.9060
2022-07-15 00:37:13 - train: epoch 0067, iter [03000, 05004], lr: 0.077334, loss: 2.5516
2022-07-15 00:37:48 - train: epoch 0067, iter [03100, 05004], lr: 0.077320, loss: 2.6396
2022-07-15 00:38:21 - train: epoch 0067, iter [03200, 05004], lr: 0.077307, loss: 2.7915
2022-07-15 00:38:56 - train: epoch 0067, iter [03300, 05004], lr: 0.077293, loss: 2.7308
2022-07-15 00:39:30 - train: epoch 0067, iter [03400, 05004], lr: 0.077280, loss: 2.8496
2022-07-15 00:40:04 - train: epoch 0067, iter [03500, 05004], lr: 0.077266, loss: 2.8972
2022-07-15 00:40:37 - train: epoch 0067, iter [03600, 05004], lr: 0.077253, loss: 2.7002
2022-07-15 00:41:11 - train: epoch 0067, iter [03700, 05004], lr: 0.077239, loss: 2.5453
2022-07-15 00:41:46 - train: epoch 0067, iter [03800, 05004], lr: 0.077226, loss: 2.9461
2022-07-15 00:42:21 - train: epoch 0067, iter [03900, 05004], lr: 0.077212, loss: 2.8201
2022-07-15 00:42:54 - train: epoch 0067, iter [04000, 05004], lr: 0.077199, loss: 3.0260
2022-07-15 00:43:28 - train: epoch 0067, iter [04100, 05004], lr: 0.077185, loss: 2.7797
2022-07-15 00:44:03 - train: epoch 0067, iter [04200, 05004], lr: 0.077172, loss: 2.7743
2022-07-15 00:44:37 - train: epoch 0067, iter [04300, 05004], lr: 0.077158, loss: 2.6186
2022-07-15 00:45:11 - train: epoch 0067, iter [04400, 05004], lr: 0.077145, loss: 2.9213
2022-07-15 00:45:45 - train: epoch 0067, iter [04500, 05004], lr: 0.077131, loss: 2.8621
2022-07-15 00:46:19 - train: epoch 0067, iter [04600, 05004], lr: 0.077117, loss: 2.6912
2022-07-15 00:46:54 - train: epoch 0067, iter [04700, 05004], lr: 0.077104, loss: 2.7915
2022-07-15 00:47:27 - train: epoch 0067, iter [04800, 05004], lr: 0.077090, loss: 2.4388
2022-07-15 00:48:02 - train: epoch 0067, iter [04900, 05004], lr: 0.077077, loss: 2.9903
2022-07-15 00:48:35 - train: epoch 0067, iter [05000, 05004], lr: 0.077063, loss: 2.8165
2022-07-15 00:48:36 - train: epoch 067, train_loss: 2.7304
2022-07-15 00:49:50 - eval: epoch: 067, acc1: 56.618%, acc5: 81.292%, test_loss: 1.8433, per_image_load_time: 1.060ms, per_image_inference_time: 0.469ms
2022-07-15 00:49:50 - until epoch: 067, best_acc1: 59.502%
2022-07-15 00:49:50 - epoch 068 lr: 0.077063
2022-07-15 00:50:28 - train: epoch 0068, iter [00100, 05004], lr: 0.077049, loss: 2.8136
2022-07-15 00:51:03 - train: epoch 0068, iter [00200, 05004], lr: 0.077036, loss: 2.6721
2022-07-15 00:51:36 - train: epoch 0068, iter [00300, 05004], lr: 0.077022, loss: 2.5568
2022-07-15 00:52:10 - train: epoch 0068, iter [00400, 05004], lr: 0.077009, loss: 2.6741
2022-07-15 00:52:44 - train: epoch 0068, iter [00500, 05004], lr: 0.076995, loss: 2.4480
2022-07-15 00:53:18 - train: epoch 0068, iter [00600, 05004], lr: 0.076982, loss: 2.9376
2022-07-15 00:53:53 - train: epoch 0068, iter [00700, 05004], lr: 0.076968, loss: 2.9369
2022-07-15 00:54:27 - train: epoch 0068, iter [00800, 05004], lr: 0.076954, loss: 2.7935
2022-07-15 00:55:00 - train: epoch 0068, iter [00900, 05004], lr: 0.076941, loss: 2.7198
2022-07-15 00:55:35 - train: epoch 0068, iter [01000, 05004], lr: 0.076927, loss: 2.4683
2022-07-15 00:56:08 - train: epoch 0068, iter [01100, 05004], lr: 0.076914, loss: 2.7443
2022-07-15 00:56:42 - train: epoch 0068, iter [01200, 05004], lr: 0.076900, loss: 2.3636
2022-07-15 00:57:16 - train: epoch 0068, iter [01300, 05004], lr: 0.076887, loss: 2.6388
2022-07-15 00:57:50 - train: epoch 0068, iter [01400, 05004], lr: 0.076873, loss: 2.8145
2022-07-15 00:58:24 - train: epoch 0068, iter [01500, 05004], lr: 0.076859, loss: 2.9205
2022-07-15 00:58:58 - train: epoch 0068, iter [01600, 05004], lr: 0.076846, loss: 2.6891
2022-07-15 00:59:32 - train: epoch 0068, iter [01700, 05004], lr: 0.076832, loss: 2.5972
2022-07-15 01:00:06 - train: epoch 0068, iter [01800, 05004], lr: 0.076819, loss: 2.8546
2022-07-15 01:00:40 - train: epoch 0068, iter [01900, 05004], lr: 0.076805, loss: 2.6339
2022-07-15 01:01:14 - train: epoch 0068, iter [02000, 05004], lr: 0.076792, loss: 2.6508
2022-07-15 01:01:48 - train: epoch 0068, iter [02100, 05004], lr: 0.076778, loss: 2.5925
2022-07-15 01:02:21 - train: epoch 0068, iter [02200, 05004], lr: 0.076764, loss: 2.7585
2022-07-15 01:02:56 - train: epoch 0068, iter [02300, 05004], lr: 0.076751, loss: 2.7866
2022-07-15 01:03:31 - train: epoch 0068, iter [02400, 05004], lr: 0.076737, loss: 2.4967
2022-07-15 01:04:05 - train: epoch 0068, iter [02500, 05004], lr: 0.076724, loss: 2.9096
2022-07-15 01:04:39 - train: epoch 0068, iter [02600, 05004], lr: 0.076710, loss: 2.3419
2022-07-15 01:05:14 - train: epoch 0068, iter [02700, 05004], lr: 0.076696, loss: 2.6147
2022-07-15 01:05:48 - train: epoch 0068, iter [02800, 05004], lr: 0.076683, loss: 2.8006
2022-07-15 01:06:22 - train: epoch 0068, iter [02900, 05004], lr: 0.076669, loss: 2.6694
2022-07-15 01:06:56 - train: epoch 0068, iter [03000, 05004], lr: 0.076656, loss: 2.7297
2022-07-15 01:07:30 - train: epoch 0068, iter [03100, 05004], lr: 0.076642, loss: 2.7533
2022-07-15 01:08:04 - train: epoch 0068, iter [03200, 05004], lr: 0.076628, loss: 2.5356
2022-07-15 01:08:39 - train: epoch 0068, iter [03300, 05004], lr: 0.076615, loss: 2.8661
2022-07-15 01:09:12 - train: epoch 0068, iter [03400, 05004], lr: 0.076601, loss: 2.7329
2022-07-15 01:09:47 - train: epoch 0068, iter [03500, 05004], lr: 0.076587, loss: 2.6308
2022-07-15 01:10:21 - train: epoch 0068, iter [03600, 05004], lr: 0.076574, loss: 2.6027
2022-07-15 01:10:55 - train: epoch 0068, iter [03700, 05004], lr: 0.076560, loss: 2.4670
2022-07-15 01:11:29 - train: epoch 0068, iter [03800, 05004], lr: 0.076546, loss: 2.8539
2022-07-15 01:12:03 - train: epoch 0068, iter [03900, 05004], lr: 0.076533, loss: 2.6423
2022-07-15 01:12:36 - train: epoch 0068, iter [04000, 05004], lr: 0.076519, loss: 2.7348
2022-07-15 01:13:10 - train: epoch 0068, iter [04100, 05004], lr: 0.076506, loss: 2.4303
2022-07-15 01:13:44 - train: epoch 0068, iter [04200, 05004], lr: 0.076492, loss: 2.4887
2022-07-15 01:14:18 - train: epoch 0068, iter [04300, 05004], lr: 0.076478, loss: 2.8222
2022-07-15 01:14:52 - train: epoch 0068, iter [04400, 05004], lr: 0.076465, loss: 2.6683
2022-07-15 01:15:26 - train: epoch 0068, iter [04500, 05004], lr: 0.076451, loss: 2.6682
2022-07-15 01:16:00 - train: epoch 0068, iter [04600, 05004], lr: 0.076437, loss: 3.0402
2022-07-15 01:16:34 - train: epoch 0068, iter [04700, 05004], lr: 0.076424, loss: 2.7688
2022-07-15 01:17:07 - train: epoch 0068, iter [04800, 05004], lr: 0.076410, loss: 2.8636
2022-07-15 01:17:42 - train: epoch 0068, iter [04900, 05004], lr: 0.076396, loss: 2.6604
2022-07-15 01:18:14 - train: epoch 0068, iter [05000, 05004], lr: 0.076383, loss: 2.6677
2022-07-15 01:18:15 - train: epoch 068, train_loss: 2.7198
2022-07-15 01:19:29 - eval: epoch: 068, acc1: 55.124%, acc5: 79.862%, test_loss: 1.9158, per_image_load_time: 0.989ms, per_image_inference_time: 0.482ms
2022-07-15 01:19:29 - until epoch: 068, best_acc1: 59.502%
2022-07-15 01:19:29 - epoch 069 lr: 0.076382
2022-07-15 01:20:09 - train: epoch 0069, iter [00100, 05004], lr: 0.076368, loss: 2.9519
2022-07-15 01:20:43 - train: epoch 0069, iter [00200, 05004], lr: 0.076355, loss: 3.0076
2022-07-15 01:21:16 - train: epoch 0069, iter [00300, 05004], lr: 0.076341, loss: 2.7290
2022-07-15 01:21:50 - train: epoch 0069, iter [00400, 05004], lr: 0.076327, loss: 2.3803
2022-07-15 01:22:25 - train: epoch 0069, iter [00500, 05004], lr: 0.076314, loss: 2.9722
2022-07-15 01:22:59 - train: epoch 0069, iter [00600, 05004], lr: 0.076300, loss: 2.6016
2022-07-15 01:23:33 - train: epoch 0069, iter [00700, 05004], lr: 0.076286, loss: 2.7951
2022-07-15 01:24:06 - train: epoch 0069, iter [00800, 05004], lr: 0.076273, loss: 2.7688
2022-07-15 01:24:40 - train: epoch 0069, iter [00900, 05004], lr: 0.076259, loss: 2.6454
2022-07-15 01:25:14 - train: epoch 0069, iter [01000, 05004], lr: 0.076245, loss: 2.7905
2022-07-15 01:25:48 - train: epoch 0069, iter [01100, 05004], lr: 0.076231, loss: 2.7643
2022-07-15 01:26:22 - train: epoch 0069, iter [01200, 05004], lr: 0.076218, loss: 2.8403
2022-07-15 01:26:56 - train: epoch 0069, iter [01300, 05004], lr: 0.076204, loss: 2.6852
2022-07-15 01:27:30 - train: epoch 0069, iter [01400, 05004], lr: 0.076190, loss: 2.6259
2022-07-15 01:28:04 - train: epoch 0069, iter [01500, 05004], lr: 0.076177, loss: 2.5464
2022-07-15 01:28:39 - train: epoch 0069, iter [01600, 05004], lr: 0.076163, loss: 2.7803
2022-07-15 01:29:14 - train: epoch 0069, iter [01700, 05004], lr: 0.076149, loss: 2.7718
2022-07-15 01:29:46 - train: epoch 0069, iter [01800, 05004], lr: 0.076135, loss: 2.5921
2022-07-15 01:30:21 - train: epoch 0069, iter [01900, 05004], lr: 0.076122, loss: 2.7567
2022-07-15 01:30:54 - train: epoch 0069, iter [02000, 05004], lr: 0.076108, loss: 2.5089
2022-07-15 01:31:28 - train: epoch 0069, iter [02100, 05004], lr: 0.076094, loss: 2.4631
2022-07-15 01:32:03 - train: epoch 0069, iter [02200, 05004], lr: 0.076081, loss: 2.7702
2022-07-15 01:32:35 - train: epoch 0069, iter [02300, 05004], lr: 0.076067, loss: 2.4965
2022-07-15 01:33:09 - train: epoch 0069, iter [02400, 05004], lr: 0.076053, loss: 2.6862
2022-07-15 01:33:43 - train: epoch 0069, iter [02500, 05004], lr: 0.076039, loss: 2.5040
2022-07-15 01:34:18 - train: epoch 0069, iter [02600, 05004], lr: 0.076026, loss: 2.9655
2022-07-15 01:34:52 - train: epoch 0069, iter [02700, 05004], lr: 0.076012, loss: 2.9628
2022-07-15 01:35:26 - train: epoch 0069, iter [02800, 05004], lr: 0.075998, loss: 2.6924
2022-07-15 01:36:00 - train: epoch 0069, iter [02900, 05004], lr: 0.075984, loss: 2.8941
2022-07-15 01:36:34 - train: epoch 0069, iter [03000, 05004], lr: 0.075971, loss: 2.8025
2022-07-15 01:37:08 - train: epoch 0069, iter [03100, 05004], lr: 0.075957, loss: 2.8921
2022-07-15 01:37:42 - train: epoch 0069, iter [03200, 05004], lr: 0.075943, loss: 2.5942
2022-07-15 01:38:15 - train: epoch 0069, iter [03300, 05004], lr: 0.075929, loss: 2.7964
2022-07-15 01:38:50 - train: epoch 0069, iter [03400, 05004], lr: 0.075916, loss: 2.5457
2022-07-15 01:39:24 - train: epoch 0069, iter [03500, 05004], lr: 0.075902, loss: 2.6195
2022-07-15 01:39:57 - train: epoch 0069, iter [03600, 05004], lr: 0.075888, loss: 2.7342
2022-07-15 01:40:32 - train: epoch 0069, iter [03700, 05004], lr: 0.075874, loss: 2.9209
2022-07-15 01:41:07 - train: epoch 0069, iter [03800, 05004], lr: 0.075860, loss: 2.9094
2022-07-15 01:41:40 - train: epoch 0069, iter [03900, 05004], lr: 0.075847, loss: 2.6247
2022-07-15 01:42:15 - train: epoch 0069, iter [04000, 05004], lr: 0.075833, loss: 2.7551
2022-07-15 01:42:48 - train: epoch 0069, iter [04100, 05004], lr: 0.075819, loss: 2.8757
2022-07-15 01:43:22 - train: epoch 0069, iter [04200, 05004], lr: 0.075805, loss: 2.5452
2022-07-15 01:43:56 - train: epoch 0069, iter [04300, 05004], lr: 0.075791, loss: 2.4789
2022-07-15 01:44:30 - train: epoch 0069, iter [04400, 05004], lr: 0.075778, loss: 2.6785
2022-07-15 01:45:05 - train: epoch 0069, iter [04500, 05004], lr: 0.075764, loss: 2.7428
2022-07-15 01:45:39 - train: epoch 0069, iter [04600, 05004], lr: 0.075750, loss: 2.8347
2022-07-15 01:46:13 - train: epoch 0069, iter [04700, 05004], lr: 0.075736, loss: 2.6760
2022-07-15 01:46:47 - train: epoch 0069, iter [04800, 05004], lr: 0.075723, loss: 2.9405
2022-07-15 01:47:21 - train: epoch 0069, iter [04900, 05004], lr: 0.075709, loss: 2.7154
2022-07-15 01:47:53 - train: epoch 0069, iter [05000, 05004], lr: 0.075695, loss: 2.8810
2022-07-15 01:47:54 - train: epoch 069, train_loss: 2.7183
2022-07-15 01:49:09 - eval: epoch: 069, acc1: 60.090%, acc5: 83.828%, test_loss: 1.6653, per_image_load_time: 1.557ms, per_image_inference_time: 0.474ms
2022-07-15 01:49:09 - until epoch: 069, best_acc1: 60.090%
2022-07-15 01:49:09 - epoch 070 lr: 0.075694
2022-07-15 01:49:48 - train: epoch 0070, iter [00100, 05004], lr: 0.075681, loss: 2.5950
2022-07-15 01:50:23 - train: epoch 0070, iter [00200, 05004], lr: 0.075667, loss: 2.6637
2022-07-15 01:50:57 - train: epoch 0070, iter [00300, 05004], lr: 0.075653, loss: 2.6893
2022-07-15 01:51:31 - train: epoch 0070, iter [00400, 05004], lr: 0.075639, loss: 2.3808
2022-07-15 01:52:05 - train: epoch 0070, iter [00500, 05004], lr: 0.075625, loss: 2.8222
2022-07-15 01:52:39 - train: epoch 0070, iter [00600, 05004], lr: 0.075611, loss: 2.5039
2022-07-15 01:53:13 - train: epoch 0070, iter [00700, 05004], lr: 0.075598, loss: 3.0025
2022-07-15 01:53:46 - train: epoch 0070, iter [00800, 05004], lr: 0.075584, loss: 2.6489
2022-07-15 01:54:21 - train: epoch 0070, iter [00900, 05004], lr: 0.075570, loss: 2.6925
2022-07-15 01:54:55 - train: epoch 0070, iter [01000, 05004], lr: 0.075556, loss: 2.6730
2022-07-15 01:55:30 - train: epoch 0070, iter [01100, 05004], lr: 0.075542, loss: 2.8566
2022-07-15 01:56:04 - train: epoch 0070, iter [01200, 05004], lr: 0.075528, loss: 2.7906
2022-07-15 01:56:38 - train: epoch 0070, iter [01300, 05004], lr: 0.075515, loss: 2.6171
2022-07-15 01:57:12 - train: epoch 0070, iter [01400, 05004], lr: 0.075501, loss: 2.5654
2022-07-15 01:57:45 - train: epoch 0070, iter [01500, 05004], lr: 0.075487, loss: 2.7935
2022-07-15 01:58:19 - train: epoch 0070, iter [01600, 05004], lr: 0.075473, loss: 2.5869
2022-07-15 01:58:53 - train: epoch 0070, iter [01700, 05004], lr: 0.075459, loss: 2.7671
2022-07-15 01:59:27 - train: epoch 0070, iter [01800, 05004], lr: 0.075445, loss: 2.5129
2022-07-15 02:00:01 - train: epoch 0070, iter [01900, 05004], lr: 0.075431, loss: 2.6772
2022-07-15 02:00:35 - train: epoch 0070, iter [02000, 05004], lr: 0.075418, loss: 2.7446
2022-07-15 02:01:10 - train: epoch 0070, iter [02100, 05004], lr: 0.075404, loss: 2.8691
2022-07-15 02:01:44 - train: epoch 0070, iter [02200, 05004], lr: 0.075390, loss: 2.8211
2022-07-15 02:02:17 - train: epoch 0070, iter [02300, 05004], lr: 0.075376, loss: 2.9792
2022-07-15 02:02:52 - train: epoch 0070, iter [02400, 05004], lr: 0.075362, loss: 2.6989
2022-07-15 02:03:25 - train: epoch 0070, iter [02500, 05004], lr: 0.075348, loss: 2.5670
2022-07-15 02:03:59 - train: epoch 0070, iter [02600, 05004], lr: 0.075334, loss: 2.4816
2022-07-15 02:04:33 - train: epoch 0070, iter [02700, 05004], lr: 0.075321, loss: 2.7737
2022-07-15 02:05:06 - train: epoch 0070, iter [02800, 05004], lr: 0.075307, loss: 2.7979
2022-07-15 02:05:40 - train: epoch 0070, iter [02900, 05004], lr: 0.075293, loss: 2.7611
2022-07-15 02:06:14 - train: epoch 0070, iter [03000, 05004], lr: 0.075279, loss: 2.6813
2022-07-15 02:06:48 - train: epoch 0070, iter [03100, 05004], lr: 0.075265, loss: 2.6791
2022-07-15 02:07:21 - train: epoch 0070, iter [03200, 05004], lr: 0.075251, loss: 2.7568
2022-07-15 02:07:55 - train: epoch 0070, iter [03300, 05004], lr: 0.075237, loss: 2.7623
2022-07-15 02:08:29 - train: epoch 0070, iter [03400, 05004], lr: 0.075223, loss: 2.7914
2022-07-15 02:09:03 - train: epoch 0070, iter [03500, 05004], lr: 0.075209, loss: 2.6009
2022-07-15 02:09:38 - train: epoch 0070, iter [03600, 05004], lr: 0.075195, loss: 2.6155
2022-07-15 02:10:12 - train: epoch 0070, iter [03700, 05004], lr: 0.075182, loss: 2.7249
2022-07-15 02:10:44 - train: epoch 0070, iter [03800, 05004], lr: 0.075168, loss: 2.6207
2022-07-15 02:11:19 - train: epoch 0070, iter [03900, 05004], lr: 0.075154, loss: 2.4730
2022-07-15 02:11:53 - train: epoch 0070, iter [04000, 05004], lr: 0.075140, loss: 2.8305
2022-07-15 02:12:27 - train: epoch 0070, iter [04100, 05004], lr: 0.075126, loss: 2.6362
2022-07-15 02:13:00 - train: epoch 0070, iter [04200, 05004], lr: 0.075112, loss: 2.7158
2022-07-15 02:13:35 - train: epoch 0070, iter [04300, 05004], lr: 0.075098, loss: 2.8275
2022-07-15 02:14:09 - train: epoch 0070, iter [04400, 05004], lr: 0.075084, loss: 2.8244
2022-07-15 02:14:44 - train: epoch 0070, iter [04500, 05004], lr: 0.075070, loss: 2.8106
2022-07-15 02:15:17 - train: epoch 0070, iter [04600, 05004], lr: 0.075056, loss: 2.8777
2022-07-15 02:15:50 - train: epoch 0070, iter [04700, 05004], lr: 0.075042, loss: 2.7180
2022-07-15 02:16:25 - train: epoch 0070, iter [04800, 05004], lr: 0.075028, loss: 2.6614
2022-07-15 02:16:59 - train: epoch 0070, iter [04900, 05004], lr: 0.075014, loss: 2.7994
2022-07-15 02:17:31 - train: epoch 0070, iter [05000, 05004], lr: 0.075001, loss: 2.6718
2022-07-15 02:17:32 - train: epoch 070, train_loss: 2.7112
2022-07-15 02:18:47 - eval: epoch: 070, acc1: 59.042%, acc5: 83.104%, test_loss: 1.7016, per_image_load_time: 2.345ms, per_image_inference_time: 0.492ms
2022-07-15 02:18:47 - until epoch: 070, best_acc1: 60.090%
2022-07-15 02:18:47 - epoch 071 lr: 0.075000
2022-07-15 02:19:27 - train: epoch 0071, iter [00100, 05004], lr: 0.074986, loss: 2.7209
2022-07-15 02:20:02 - train: epoch 0071, iter [00200, 05004], lr: 0.074972, loss: 2.6679
2022-07-15 02:20:36 - train: epoch 0071, iter [00300, 05004], lr: 0.074958, loss: 2.6392
2022-07-15 02:21:09 - train: epoch 0071, iter [00400, 05004], lr: 0.074944, loss: 2.6615
2022-07-15 02:21:42 - train: epoch 0071, iter [00500, 05004], lr: 0.074930, loss: 2.9573
2022-07-15 02:22:15 - train: epoch 0071, iter [00600, 05004], lr: 0.074916, loss: 2.5675
2022-07-15 02:22:49 - train: epoch 0071, iter [00700, 05004], lr: 0.074902, loss: 2.8567
2022-07-15 02:23:22 - train: epoch 0071, iter [00800, 05004], lr: 0.074888, loss: 2.7113
2022-07-15 02:23:56 - train: epoch 0071, iter [00900, 05004], lr: 0.074874, loss: 2.9529
2022-07-15 02:24:30 - train: epoch 0071, iter [01000, 05004], lr: 0.074860, loss: 2.5422
2022-07-15 02:25:03 - train: epoch 0071, iter [01100, 05004], lr: 0.074846, loss: 2.9596
2022-07-15 02:25:37 - train: epoch 0071, iter [01200, 05004], lr: 0.074833, loss: 2.8546
2022-07-15 02:26:10 - train: epoch 0071, iter [01300, 05004], lr: 0.074819, loss: 2.6365
2022-07-15 02:26:44 - train: epoch 0071, iter [01400, 05004], lr: 0.074805, loss: 2.6615
2022-07-15 02:27:17 - train: epoch 0071, iter [01500, 05004], lr: 0.074791, loss: 2.5246
2022-07-15 02:27:51 - train: epoch 0071, iter [01600, 05004], lr: 0.074777, loss: 2.4350
2022-07-15 02:28:25 - train: epoch 0071, iter [01700, 05004], lr: 0.074763, loss: 2.7321
2022-07-15 02:28:59 - train: epoch 0071, iter [01800, 05004], lr: 0.074749, loss: 2.6887
2022-07-15 02:29:33 - train: epoch 0071, iter [01900, 05004], lr: 0.074735, loss: 2.5631
2022-07-15 02:30:06 - train: epoch 0071, iter [02000, 05004], lr: 0.074721, loss: 2.5965
2022-07-15 02:30:40 - train: epoch 0071, iter [02100, 05004], lr: 0.074707, loss: 2.7134
2022-07-15 02:31:15 - train: epoch 0071, iter [02200, 05004], lr: 0.074693, loss: 2.6877
2022-07-15 02:31:48 - train: epoch 0071, iter [02300, 05004], lr: 0.074679, loss: 2.8249
2022-07-15 02:32:22 - train: epoch 0071, iter [02400, 05004], lr: 0.074665, loss: 2.7072
2022-07-15 02:32:56 - train: epoch 0071, iter [02500, 05004], lr: 0.074651, loss: 2.7774
2022-07-15 02:33:30 - train: epoch 0071, iter [02600, 05004], lr: 0.074637, loss: 2.5398
2022-07-15 02:34:04 - train: epoch 0071, iter [02700, 05004], lr: 0.074623, loss: 2.8313
2022-07-15 02:34:37 - train: epoch 0071, iter [02800, 05004], lr: 0.074609, loss: 2.7240
2022-07-15 02:35:12 - train: epoch 0071, iter [02900, 05004], lr: 0.074595, loss: 2.6207
2022-07-15 02:35:46 - train: epoch 0071, iter [03000, 05004], lr: 0.074581, loss: 2.8729
2022-07-15 02:36:19 - train: epoch 0071, iter [03100, 05004], lr: 0.074567, loss: 2.6561
2022-07-15 02:36:54 - train: epoch 0071, iter [03200, 05004], lr: 0.074553, loss: 2.4378
2022-07-15 02:37:27 - train: epoch 0071, iter [03300, 05004], lr: 0.074539, loss: 2.7528
2022-07-15 02:38:01 - train: epoch 0071, iter [03400, 05004], lr: 0.074525, loss: 2.6580
2022-07-15 02:38:35 - train: epoch 0071, iter [03500, 05004], lr: 0.074510, loss: 2.9169
2022-07-15 02:39:09 - train: epoch 0071, iter [03600, 05004], lr: 0.074496, loss: 2.7847
2022-07-15 02:39:43 - train: epoch 0071, iter [03700, 05004], lr: 0.074482, loss: 2.8305
2022-07-15 02:40:18 - train: epoch 0071, iter [03800, 05004], lr: 0.074468, loss: 2.7250
2022-07-15 02:40:52 - train: epoch 0071, iter [03900, 05004], lr: 0.074454, loss: 2.8859
2022-07-15 02:41:27 - train: epoch 0071, iter [04000, 05004], lr: 0.074440, loss: 2.5205
2022-07-15 02:42:00 - train: epoch 0071, iter [04100, 05004], lr: 0.074426, loss: 2.5077
2022-07-15 02:42:35 - train: epoch 0071, iter [04200, 05004], lr: 0.074412, loss: 2.8720
2022-07-15 02:43:09 - train: epoch 0071, iter [04300, 05004], lr: 0.074398, loss: 2.6114
2022-07-15 02:43:43 - train: epoch 0071, iter [04400, 05004], lr: 0.074384, loss: 2.7916
2022-07-15 02:44:17 - train: epoch 0071, iter [04500, 05004], lr: 0.074370, loss: 2.6013
2022-07-15 02:44:52 - train: epoch 0071, iter [04600, 05004], lr: 0.074356, loss: 2.7649
2022-07-15 02:45:26 - train: epoch 0071, iter [04700, 05004], lr: 0.074342, loss: 2.6954
2022-07-15 02:46:00 - train: epoch 0071, iter [04800, 05004], lr: 0.074328, loss: 2.6716
2022-07-15 02:46:34 - train: epoch 0071, iter [04900, 05004], lr: 0.074314, loss: 2.5842
2022-07-15 02:47:06 - train: epoch 0071, iter [05000, 05004], lr: 0.074300, loss: 2.4860
2022-07-15 02:47:08 - train: epoch 071, train_loss: 2.7039
2022-07-15 02:48:22 - eval: epoch: 071, acc1: 59.532%, acc5: 83.430%, test_loss: 1.6922, per_image_load_time: 2.189ms, per_image_inference_time: 0.482ms
2022-07-15 02:48:22 - until epoch: 071, best_acc1: 60.090%
2022-07-15 02:48:22 - epoch 072 lr: 0.074299
2022-07-15 02:49:02 - train: epoch 0072, iter [00100, 05004], lr: 0.074285, loss: 2.4720
2022-07-15 02:49:36 - train: epoch 0072, iter [00200, 05004], lr: 0.074271, loss: 2.7064
2022-07-15 02:50:11 - train: epoch 0072, iter [00300, 05004], lr: 0.074257, loss: 2.2707
2022-07-15 02:50:44 - train: epoch 0072, iter [00400, 05004], lr: 0.074243, loss: 2.7982
2022-07-15 02:51:18 - train: epoch 0072, iter [00500, 05004], lr: 0.074229, loss: 2.6704
2022-07-15 02:51:53 - train: epoch 0072, iter [00600, 05004], lr: 0.074215, loss: 2.6878
2022-07-15 02:52:26 - train: epoch 0072, iter [00700, 05004], lr: 0.074201, loss: 2.8329
2022-07-15 02:53:00 - train: epoch 0072, iter [00800, 05004], lr: 0.074187, loss: 2.6574
2022-07-15 02:53:34 - train: epoch 0072, iter [00900, 05004], lr: 0.074172, loss: 2.6287
2022-07-15 02:54:08 - train: epoch 0072, iter [01000, 05004], lr: 0.074158, loss: 2.4681
2022-07-15 02:54:42 - train: epoch 0072, iter [01100, 05004], lr: 0.074144, loss: 2.9637
2022-07-15 02:55:15 - train: epoch 0072, iter [01200, 05004], lr: 0.074130, loss: 2.3591
2022-07-15 02:55:50 - train: epoch 0072, iter [01300, 05004], lr: 0.074116, loss: 2.5709
2022-07-15 02:56:24 - train: epoch 0072, iter [01400, 05004], lr: 0.074102, loss: 2.6834
2022-07-15 02:56:58 - train: epoch 0072, iter [01500, 05004], lr: 0.074088, loss: 2.3935
2022-07-15 02:57:32 - train: epoch 0072, iter [01600, 05004], lr: 0.074074, loss: 2.6102
2022-07-15 02:58:06 - train: epoch 0072, iter [01700, 05004], lr: 0.074060, loss: 2.7059
2022-07-15 02:58:39 - train: epoch 0072, iter [01800, 05004], lr: 0.074046, loss: 2.3758
2022-07-15 02:59:13 - train: epoch 0072, iter [01900, 05004], lr: 0.074031, loss: 2.5939
2022-07-15 02:59:47 - train: epoch 0072, iter [02000, 05004], lr: 0.074017, loss: 3.1301
2022-07-15 03:00:21 - train: epoch 0072, iter [02100, 05004], lr: 0.074003, loss: 2.8989
2022-07-15 03:00:56 - train: epoch 0072, iter [02200, 05004], lr: 0.073989, loss: 2.8103
2022-07-15 03:01:30 - train: epoch 0072, iter [02300, 05004], lr: 0.073975, loss: 2.6272
2022-07-15 03:02:03 - train: epoch 0072, iter [02400, 05004], lr: 0.073961, loss: 2.7669
2022-07-15 03:02:38 - train: epoch 0072, iter [02500, 05004], lr: 0.073947, loss: 2.6948
2022-07-15 03:03:12 - train: epoch 0072, iter [02600, 05004], lr: 0.073933, loss: 2.8525
2022-07-15 03:03:45 - train: epoch 0072, iter [02700, 05004], lr: 0.073918, loss: 2.6167
2022-07-15 03:04:19 - train: epoch 0072, iter [02800, 05004], lr: 0.073904, loss: 2.4875
2022-07-15 03:04:53 - train: epoch 0072, iter [02900, 05004], lr: 0.073890, loss: 2.6134
2022-07-15 03:05:27 - train: epoch 0072, iter [03000, 05004], lr: 0.073876, loss: 2.6810
2022-07-15 03:06:01 - train: epoch 0072, iter [03100, 05004], lr: 0.073862, loss: 2.9674
2022-07-15 03:06:35 - train: epoch 0072, iter [03200, 05004], lr: 0.073848, loss: 2.5176
2022-07-15 03:07:09 - train: epoch 0072, iter [03300, 05004], lr: 0.073834, loss: 2.7120
2022-07-15 03:07:43 - train: epoch 0072, iter [03400, 05004], lr: 0.073819, loss: 3.0256
2022-07-15 03:08:17 - train: epoch 0072, iter [03500, 05004], lr: 0.073805, loss: 2.6587
2022-07-15 03:08:51 - train: epoch 0072, iter [03600, 05004], lr: 0.073791, loss: 2.7220
2022-07-15 03:09:24 - train: epoch 0072, iter [03700, 05004], lr: 0.073777, loss: 2.8679
2022-07-15 03:09:59 - train: epoch 0072, iter [03800, 05004], lr: 0.073763, loss: 2.9288
2022-07-15 03:10:32 - train: epoch 0072, iter [03900, 05004], lr: 0.073749, loss: 2.6020
2022-07-15 03:11:06 - train: epoch 0072, iter [04000, 05004], lr: 0.073734, loss: 2.8237
2022-07-15 03:11:40 - train: epoch 0072, iter [04100, 05004], lr: 0.073720, loss: 2.8030
2022-07-15 03:12:14 - train: epoch 0072, iter [04200, 05004], lr: 0.073706, loss: 2.8349
2022-07-15 03:12:49 - train: epoch 0072, iter [04300, 05004], lr: 0.073692, loss: 2.6994
2022-07-15 03:13:23 - train: epoch 0072, iter [04400, 05004], lr: 0.073678, loss: 2.6081
2022-07-15 03:13:57 - train: epoch 0072, iter [04500, 05004], lr: 0.073664, loss: 2.7240
2022-07-15 03:14:30 - train: epoch 0072, iter [04600, 05004], lr: 0.073649, loss: 2.8785
2022-07-15 03:15:05 - train: epoch 0072, iter [04700, 05004], lr: 0.073635, loss: 2.8636
2022-07-15 03:15:39 - train: epoch 0072, iter [04800, 05004], lr: 0.073621, loss: 2.6039
2022-07-15 03:16:12 - train: epoch 0072, iter [04900, 05004], lr: 0.073607, loss: 2.7511
2022-07-15 03:16:45 - train: epoch 0072, iter [05000, 05004], lr: 0.073593, loss: 2.7110
2022-07-15 03:16:46 - train: epoch 072, train_loss: 2.7051
2022-07-15 03:18:01 - eval: epoch: 072, acc1: 59.460%, acc5: 83.326%, test_loss: 1.7008, per_image_load_time: 2.464ms, per_image_inference_time: 0.460ms
2022-07-15 03:18:02 - until epoch: 072, best_acc1: 60.090%
2022-07-15 03:18:02 - epoch 073 lr: 0.073592
2022-07-15 03:18:41 - train: epoch 0073, iter [00100, 05004], lr: 0.073578, loss: 2.9533
2022-07-15 03:19:15 - train: epoch 0073, iter [00200, 05004], lr: 0.073564, loss: 2.7570
2022-07-15 03:19:49 - train: epoch 0073, iter [00300, 05004], lr: 0.073549, loss: 2.3565
2022-07-15 03:20:23 - train: epoch 0073, iter [00400, 05004], lr: 0.073535, loss: 2.4592
2022-07-15 03:20:56 - train: epoch 0073, iter [00500, 05004], lr: 0.073521, loss: 2.5561
2022-07-15 03:21:30 - train: epoch 0073, iter [00600, 05004], lr: 0.073507, loss: 2.8453
2022-07-15 03:22:04 - train: epoch 0073, iter [00700, 05004], lr: 0.073493, loss: 2.9219
2022-07-15 03:22:38 - train: epoch 0073, iter [00800, 05004], lr: 0.073478, loss: 2.4286
2022-07-15 03:23:13 - train: epoch 0073, iter [00900, 05004], lr: 0.073464, loss: 2.7502
2022-07-15 03:23:46 - train: epoch 0073, iter [01000, 05004], lr: 0.073450, loss: 2.6989
2022-07-15 03:24:20 - train: epoch 0073, iter [01100, 05004], lr: 0.073436, loss: 2.7735
2022-07-15 03:24:55 - train: epoch 0073, iter [01200, 05004], lr: 0.073422, loss: 2.7203
2022-07-15 03:25:29 - train: epoch 0073, iter [01300, 05004], lr: 0.073407, loss: 2.7719
2022-07-15 03:26:03 - train: epoch 0073, iter [01400, 05004], lr: 0.073393, loss: 2.6240
2022-07-15 03:26:37 - train: epoch 0073, iter [01500, 05004], lr: 0.073379, loss: 2.9035
2022-07-15 03:27:11 - train: epoch 0073, iter [01600, 05004], lr: 0.073365, loss: 2.3983
2022-07-15 03:27:45 - train: epoch 0073, iter [01700, 05004], lr: 0.073350, loss: 2.7730
2022-07-15 03:28:19 - train: epoch 0073, iter [01800, 05004], lr: 0.073336, loss: 2.7021
2022-07-15 03:28:54 - train: epoch 0073, iter [01900, 05004], lr: 0.073322, loss: 3.0454
2022-07-15 03:29:28 - train: epoch 0073, iter [02000, 05004], lr: 0.073308, loss: 2.7911
2022-07-15 03:30:02 - train: epoch 0073, iter [02100, 05004], lr: 0.073293, loss: 2.6474
2022-07-15 03:30:36 - train: epoch 0073, iter [02200, 05004], lr: 0.073279, loss: 2.7767
2022-07-15 03:31:10 - train: epoch 0073, iter [02300, 05004], lr: 0.073265, loss: 2.7304
2022-07-15 03:31:44 - train: epoch 0073, iter [02400, 05004], lr: 0.073251, loss: 2.6387
2022-07-15 03:32:19 - train: epoch 0073, iter [02500, 05004], lr: 0.073236, loss: 2.7413
2022-07-15 03:32:52 - train: epoch 0073, iter [02600, 05004], lr: 0.073222, loss: 2.5962
2022-07-15 03:33:27 - train: epoch 0073, iter [02700, 05004], lr: 0.073208, loss: 2.9608
2022-07-15 03:34:01 - train: epoch 0073, iter [02800, 05004], lr: 0.073194, loss: 2.6454
2022-07-15 03:34:35 - train: epoch 0073, iter [02900, 05004], lr: 0.073179, loss: 2.6994
2022-07-15 03:35:09 - train: epoch 0073, iter [03000, 05004], lr: 0.073165, loss: 2.4977
2022-07-15 03:35:44 - train: epoch 0073, iter [03100, 05004], lr: 0.073151, loss: 2.6777
2022-07-15 03:36:18 - train: epoch 0073, iter [03200, 05004], lr: 0.073137, loss: 2.8635
2022-07-15 03:36:52 - train: epoch 0073, iter [03300, 05004], lr: 0.073122, loss: 2.5980
2022-07-15 03:37:26 - train: epoch 0073, iter [03400, 05004], lr: 0.073108, loss: 2.8106
2022-07-15 03:38:00 - train: epoch 0073, iter [03500, 05004], lr: 0.073094, loss: 2.7769
2022-07-15 03:38:34 - train: epoch 0073, iter [03600, 05004], lr: 0.073080, loss: 2.5896
2022-07-15 03:39:08 - train: epoch 0073, iter [03700, 05004], lr: 0.073065, loss: 2.7050
2022-07-15 03:39:43 - train: epoch 0073, iter [03800, 05004], lr: 0.073051, loss: 2.8366
2022-07-15 03:40:17 - train: epoch 0073, iter [03900, 05004], lr: 0.073037, loss: 2.6504
2022-07-15 03:40:52 - train: epoch 0073, iter [04000, 05004], lr: 0.073022, loss: 2.7211
2022-07-15 03:41:26 - train: epoch 0073, iter [04100, 05004], lr: 0.073008, loss: 2.6726
2022-07-15 03:41:59 - train: epoch 0073, iter [04200, 05004], lr: 0.072994, loss: 2.9602
2022-07-15 03:42:34 - train: epoch 0073, iter [04300, 05004], lr: 0.072979, loss: 2.9121
2022-07-15 03:43:08 - train: epoch 0073, iter [04400, 05004], lr: 0.072965, loss: 2.9019
2022-07-15 03:43:43 - train: epoch 0073, iter [04500, 05004], lr: 0.072951, loss: 2.7092
2022-07-15 03:44:16 - train: epoch 0073, iter [04600, 05004], lr: 0.072937, loss: 2.5903
2022-07-15 03:44:51 - train: epoch 0073, iter [04700, 05004], lr: 0.072922, loss: 2.4816
2022-07-15 03:45:25 - train: epoch 0073, iter [04800, 05004], lr: 0.072908, loss: 2.8531
2022-07-15 03:45:59 - train: epoch 0073, iter [04900, 05004], lr: 0.072894, loss: 2.6116
2022-07-15 03:46:32 - train: epoch 0073, iter [05000, 05004], lr: 0.072879, loss: 2.6744
2022-07-15 03:46:33 - train: epoch 073, train_loss: 2.6983
2022-07-15 03:47:48 - eval: epoch: 073, acc1: 60.336%, acc5: 83.674%, test_loss: 1.6717, per_image_load_time: 1.858ms, per_image_inference_time: 0.513ms
2022-07-15 03:47:48 - until epoch: 073, best_acc1: 60.336%
2022-07-15 03:47:48 - epoch 074 lr: 0.072879
2022-07-15 03:48:28 - train: epoch 0074, iter [00100, 05004], lr: 0.072864, loss: 2.7033
2022-07-15 03:49:02 - train: epoch 0074, iter [00200, 05004], lr: 0.072850, loss: 2.6686
2022-07-15 03:49:36 - train: epoch 0074, iter [00300, 05004], lr: 0.072836, loss: 2.5697
2022-07-15 03:50:11 - train: epoch 0074, iter [00400, 05004], lr: 0.072822, loss: 2.8197
2022-07-15 03:50:45 - train: epoch 0074, iter [00500, 05004], lr: 0.072807, loss: 2.5901
2022-07-15 03:51:19 - train: epoch 0074, iter [00600, 05004], lr: 0.072793, loss: 2.6605
2022-07-15 03:51:53 - train: epoch 0074, iter [00700, 05004], lr: 0.072779, loss: 2.6920
2022-07-15 03:52:26 - train: epoch 0074, iter [00800, 05004], lr: 0.072764, loss: 2.6440
2022-07-15 03:53:00 - train: epoch 0074, iter [00900, 05004], lr: 0.072750, loss: 2.3524
2022-07-15 03:53:34 - train: epoch 0074, iter [01000, 05004], lr: 0.072736, loss: 2.7367
2022-07-15 03:54:08 - train: epoch 0074, iter [01100, 05004], lr: 0.072721, loss: 2.6347
2022-07-15 03:54:43 - train: epoch 0074, iter [01200, 05004], lr: 0.072707, loss: 2.7895
2022-07-15 03:55:17 - train: epoch 0074, iter [01300, 05004], lr: 0.072692, loss: 2.6790
2022-07-15 03:55:51 - train: epoch 0074, iter [01400, 05004], lr: 0.072678, loss: 2.7372
2022-07-15 03:56:25 - train: epoch 0074, iter [01500, 05004], lr: 0.072664, loss: 2.8579
2022-07-15 03:56:58 - train: epoch 0074, iter [01600, 05004], lr: 0.072649, loss: 2.6799
2022-07-15 03:57:33 - train: epoch 0074, iter [01700, 05004], lr: 0.072635, loss: 2.6137
2022-07-15 03:58:08 - train: epoch 0074, iter [01800, 05004], lr: 0.072621, loss: 2.7472
2022-07-15 03:58:41 - train: epoch 0074, iter [01900, 05004], lr: 0.072606, loss: 2.4499
2022-07-15 03:59:16 - train: epoch 0074, iter [02000, 05004], lr: 0.072592, loss: 2.3664
2022-07-15 03:59:49 - train: epoch 0074, iter [02100, 05004], lr: 0.072578, loss: 3.0408
2022-07-15 04:00:23 - train: epoch 0074, iter [02200, 05004], lr: 0.072563, loss: 2.8837
2022-07-15 04:00:58 - train: epoch 0074, iter [02300, 05004], lr: 0.072549, loss: 2.7164
2022-07-15 04:01:33 - train: epoch 0074, iter [02400, 05004], lr: 0.072535, loss: 2.6713
2022-07-15 04:02:07 - train: epoch 0074, iter [02500, 05004], lr: 0.072520, loss: 2.6939
2022-07-15 04:02:40 - train: epoch 0074, iter [02600, 05004], lr: 0.072506, loss: 2.6341
2022-07-15 04:03:15 - train: epoch 0074, iter [02700, 05004], lr: 0.072491, loss: 2.6988
2022-07-15 04:03:49 - train: epoch 0074, iter [02800, 05004], lr: 0.072477, loss: 3.0262
2022-07-15 04:04:22 - train: epoch 0074, iter [02900, 05004], lr: 0.072463, loss: 2.9529
2022-07-15 04:04:56 - train: epoch 0074, iter [03000, 05004], lr: 0.072448, loss: 2.4951
2022-07-15 04:05:30 - train: epoch 0074, iter [03100, 05004], lr: 0.072434, loss: 2.8894
2022-07-15 04:06:05 - train: epoch 0074, iter [03200, 05004], lr: 0.072420, loss: 2.4171
2022-07-15 04:06:38 - train: epoch 0074, iter [03300, 05004], lr: 0.072405, loss: 2.6707
2022-07-15 04:07:13 - train: epoch 0074, iter [03400, 05004], lr: 0.072391, loss: 2.5122
2022-07-15 04:07:46 - train: epoch 0074, iter [03500, 05004], lr: 0.072376, loss: 2.5884
2022-07-15 04:08:21 - train: epoch 0074, iter [03600, 05004], lr: 0.072362, loss: 2.7334
2022-07-15 04:08:54 - train: epoch 0074, iter [03700, 05004], lr: 0.072348, loss: 2.7464
2022-07-15 04:09:28 - train: epoch 0074, iter [03800, 05004], lr: 0.072333, loss: 2.5270
2022-07-15 04:10:03 - train: epoch 0074, iter [03900, 05004], lr: 0.072319, loss: 2.3624
2022-07-15 04:10:37 - train: epoch 0074, iter [04000, 05004], lr: 0.072304, loss: 2.6232
2022-07-15 04:11:11 - train: epoch 0074, iter [04100, 05004], lr: 0.072290, loss: 2.5503
2022-07-15 04:11:45 - train: epoch 0074, iter [04200, 05004], lr: 0.072276, loss: 2.7098
2022-07-15 04:12:20 - train: epoch 0074, iter [04300, 05004], lr: 0.072261, loss: 2.5805
2022-07-15 04:12:55 - train: epoch 0074, iter [04400, 05004], lr: 0.072247, loss: 2.7413
2022-07-15 04:13:28 - train: epoch 0074, iter [04500, 05004], lr: 0.072232, loss: 2.6079
2022-07-15 04:14:02 - train: epoch 0074, iter [04600, 05004], lr: 0.072218, loss: 2.9359
2022-07-15 04:14:36 - train: epoch 0074, iter [04700, 05004], lr: 0.072203, loss: 2.7332
2022-07-15 04:15:11 - train: epoch 0074, iter [04800, 05004], lr: 0.072189, loss: 2.8499
2022-07-15 04:15:46 - train: epoch 0074, iter [04900, 05004], lr: 0.072175, loss: 3.0139
2022-07-15 04:16:18 - train: epoch 0074, iter [05000, 05004], lr: 0.072160, loss: 2.8836
2022-07-15 04:16:20 - train: epoch 074, train_loss: 2.6953
2022-07-15 04:17:34 - eval: epoch: 074, acc1: 56.540%, acc5: 80.748%, test_loss: 1.8482, per_image_load_time: 2.233ms, per_image_inference_time: 0.486ms
2022-07-15 04:17:34 - until epoch: 074, best_acc1: 60.336%
2022-07-15 04:17:34 - epoch 075 lr: 0.072159
2022-07-15 04:18:13 - train: epoch 0075, iter [00100, 05004], lr: 0.072145, loss: 2.6967
2022-07-15 04:18:48 - train: epoch 0075, iter [00200, 05004], lr: 0.072131, loss: 2.5118
2022-07-15 04:19:22 - train: epoch 0075, iter [00300, 05004], lr: 0.072116, loss: 2.4508
2022-07-15 04:19:56 - train: epoch 0075, iter [00400, 05004], lr: 0.072102, loss: 2.7343
2022-07-15 04:20:30 - train: epoch 0075, iter [00500, 05004], lr: 0.072087, loss: 2.5941
2022-07-15 04:21:03 - train: epoch 0075, iter [00600, 05004], lr: 0.072073, loss: 2.6021
2022-07-15 04:21:37 - train: epoch 0075, iter [00700, 05004], lr: 0.072059, loss: 2.7921
2022-07-15 04:22:11 - train: epoch 0075, iter [00800, 05004], lr: 0.072044, loss: 2.8342
2022-07-15 04:22:46 - train: epoch 0075, iter [00900, 05004], lr: 0.072030, loss: 2.7220
2022-07-15 04:23:20 - train: epoch 0075, iter [01000, 05004], lr: 0.072015, loss: 2.7099
2022-07-15 04:23:53 - train: epoch 0075, iter [01100, 05004], lr: 0.072001, loss: 2.4322
2022-07-15 04:24:27 - train: epoch 0075, iter [01200, 05004], lr: 0.071986, loss: 2.4391
2022-07-15 04:25:00 - train: epoch 0075, iter [01300, 05004], lr: 0.071972, loss: 2.3607
2022-07-15 04:25:35 - train: epoch 0075, iter [01400, 05004], lr: 0.071957, loss: 2.6023
2022-07-15 04:26:09 - train: epoch 0075, iter [01500, 05004], lr: 0.071943, loss: 2.7063
2022-07-15 04:26:43 - train: epoch 0075, iter [01600, 05004], lr: 0.071928, loss: 2.7337
2022-07-15 04:27:16 - train: epoch 0075, iter [01700, 05004], lr: 0.071914, loss: 2.7664
2022-07-15 04:27:51 - train: epoch 0075, iter [01800, 05004], lr: 0.071899, loss: 2.4773
2022-07-15 04:28:24 - train: epoch 0075, iter [01900, 05004], lr: 0.071885, loss: 2.6772
2022-07-15 04:28:57 - train: epoch 0075, iter [02000, 05004], lr: 0.071871, loss: 2.4727
2022-07-15 04:29:32 - train: epoch 0075, iter [02100, 05004], lr: 0.071856, loss: 2.4579
2022-07-15 04:30:06 - train: epoch 0075, iter [02200, 05004], lr: 0.071842, loss: 2.5256
2022-07-15 04:30:39 - train: epoch 0075, iter [02300, 05004], lr: 0.071827, loss: 2.4325
2022-07-15 04:31:14 - train: epoch 0075, iter [02400, 05004], lr: 0.071813, loss: 2.5814
2022-07-15 04:31:47 - train: epoch 0075, iter [02500, 05004], lr: 0.071798, loss: 2.7335
2022-07-15 04:32:21 - train: epoch 0075, iter [02600, 05004], lr: 0.071784, loss: 2.8785
2022-07-15 04:32:55 - train: epoch 0075, iter [02700, 05004], lr: 0.071769, loss: 2.5726
2022-07-15 04:33:30 - train: epoch 0075, iter [02800, 05004], lr: 0.071755, loss: 2.6584
2022-07-15 04:34:04 - train: epoch 0075, iter [02900, 05004], lr: 0.071740, loss: 2.7733
2022-07-15 04:34:38 - train: epoch 0075, iter [03000, 05004], lr: 0.071726, loss: 2.6688
2022-07-15 04:35:12 - train: epoch 0075, iter [03100, 05004], lr: 0.071711, loss: 2.5774
2022-07-15 04:35:46 - train: epoch 0075, iter [03200, 05004], lr: 0.071697, loss: 2.8300
2022-07-15 04:36:20 - train: epoch 0075, iter [03300, 05004], lr: 0.071682, loss: 2.8323
2022-07-15 04:36:54 - train: epoch 0075, iter [03400, 05004], lr: 0.071668, loss: 2.6727
2022-07-15 04:37:28 - train: epoch 0075, iter [03500, 05004], lr: 0.071653, loss: 2.6668
2022-07-15 04:38:03 - train: epoch 0075, iter [03600, 05004], lr: 0.071639, loss: 2.8195
2022-07-15 04:38:36 - train: epoch 0075, iter [03700, 05004], lr: 0.071624, loss: 2.8620
2022-07-15 04:39:10 - train: epoch 0075, iter [03800, 05004], lr: 0.071610, loss: 2.7161
2022-07-15 04:39:45 - train: epoch 0075, iter [03900, 05004], lr: 0.071595, loss: 2.6972
2022-07-15 04:40:19 - train: epoch 0075, iter [04000, 05004], lr: 0.071581, loss: 2.4830
2022-07-15 04:40:53 - train: epoch 0075, iter [04100, 05004], lr: 0.071566, loss: 2.5020
2022-07-15 04:41:26 - train: epoch 0075, iter [04200, 05004], lr: 0.071551, loss: 2.7375
2022-07-15 04:42:00 - train: epoch 0075, iter [04300, 05004], lr: 0.071537, loss: 2.7927
2022-07-15 04:42:34 - train: epoch 0075, iter [04400, 05004], lr: 0.071522, loss: 2.7146
2022-07-15 04:43:08 - train: epoch 0075, iter [04500, 05004], lr: 0.071508, loss: 2.8096
2022-07-15 04:43:42 - train: epoch 0075, iter [04600, 05004], lr: 0.071493, loss: 2.8143
2022-07-15 04:44:16 - train: epoch 0075, iter [04700, 05004], lr: 0.071479, loss: 2.6847
2022-07-15 04:44:50 - train: epoch 0075, iter [04800, 05004], lr: 0.071464, loss: 2.5900
2022-07-15 04:45:24 - train: epoch 0075, iter [04900, 05004], lr: 0.071450, loss: 2.6932
2022-07-15 04:45:57 - train: epoch 0075, iter [05000, 05004], lr: 0.071435, loss: 2.5493
2022-07-15 04:45:58 - train: epoch 075, train_loss: 2.6861
2022-07-15 04:47:12 - eval: epoch: 075, acc1: 59.788%, acc5: 83.526%, test_loss: 1.6815, per_image_load_time: 0.995ms, per_image_inference_time: 0.478ms
2022-07-15 04:47:13 - until epoch: 075, best_acc1: 60.336%
2022-07-15 04:47:13 - epoch 076 lr: 0.071434
2022-07-15 04:47:52 - train: epoch 0076, iter [00100, 05004], lr: 0.071420, loss: 3.0210
2022-07-15 04:48:26 - train: epoch 0076, iter [00200, 05004], lr: 0.071406, loss: 2.5410
2022-07-15 04:49:00 - train: epoch 0076, iter [00300, 05004], lr: 0.071391, loss: 2.9641
2022-07-15 04:49:34 - train: epoch 0076, iter [00400, 05004], lr: 0.071376, loss: 2.6675
2022-07-15 04:50:09 - train: epoch 0076, iter [00500, 05004], lr: 0.071362, loss: 2.6584
2022-07-15 04:50:43 - train: epoch 0076, iter [00600, 05004], lr: 0.071347, loss: 2.6020
2022-07-15 04:51:17 - train: epoch 0076, iter [00700, 05004], lr: 0.071333, loss: 2.5446
2022-07-15 04:51:51 - train: epoch 0076, iter [00800, 05004], lr: 0.071318, loss: 2.5534
2022-07-15 04:52:25 - train: epoch 0076, iter [00900, 05004], lr: 0.071304, loss: 3.1384
2022-07-15 04:53:00 - train: epoch 0076, iter [01000, 05004], lr: 0.071289, loss: 2.8670
2022-07-15 04:53:34 - train: epoch 0076, iter [01100, 05004], lr: 0.071275, loss: 2.8379
2022-07-15 04:54:07 - train: epoch 0076, iter [01200, 05004], lr: 0.071260, loss: 2.7787
2022-07-15 04:54:42 - train: epoch 0076, iter [01300, 05004], lr: 0.071245, loss: 2.7944
2022-07-15 04:55:17 - train: epoch 0076, iter [01400, 05004], lr: 0.071231, loss: 2.6114
2022-07-15 04:55:50 - train: epoch 0076, iter [01500, 05004], lr: 0.071216, loss: 2.8446
2022-07-15 04:56:25 - train: epoch 0076, iter [01600, 05004], lr: 0.071202, loss: 2.7225
2022-07-15 04:56:59 - train: epoch 0076, iter [01700, 05004], lr: 0.071187, loss: 2.4581
2022-07-15 04:57:33 - train: epoch 0076, iter [01800, 05004], lr: 0.071172, loss: 2.8558
2022-07-15 04:58:07 - train: epoch 0076, iter [01900, 05004], lr: 0.071158, loss: 2.8497
2022-07-15 04:58:41 - train: epoch 0076, iter [02000, 05004], lr: 0.071143, loss: 2.6434
2022-07-15 04:59:15 - train: epoch 0076, iter [02100, 05004], lr: 0.071129, loss: 2.7873
2022-07-15 04:59:49 - train: epoch 0076, iter [02200, 05004], lr: 0.071114, loss: 2.6575
2022-07-15 05:00:24 - train: epoch 0076, iter [02300, 05004], lr: 0.071100, loss: 2.5644
2022-07-15 05:00:58 - train: epoch 0076, iter [02400, 05004], lr: 0.071085, loss: 2.7618
2022-07-15 05:01:31 - train: epoch 0076, iter [02500, 05004], lr: 0.071070, loss: 2.6959
2022-07-15 05:02:06 - train: epoch 0076, iter [02600, 05004], lr: 0.071056, loss: 3.0748
2022-07-15 05:02:40 - train: epoch 0076, iter [02700, 05004], lr: 0.071041, loss: 2.6594
2022-07-15 05:03:14 - train: epoch 0076, iter [02800, 05004], lr: 0.071027, loss: 2.7039
2022-07-15 05:03:49 - train: epoch 0076, iter [02900, 05004], lr: 0.071012, loss: 2.5186
2022-07-15 05:04:23 - train: epoch 0076, iter [03000, 05004], lr: 0.070997, loss: 2.5968
2022-07-15 05:04:57 - train: epoch 0076, iter [03100, 05004], lr: 0.070983, loss: 2.3322
2022-07-15 05:05:32 - train: epoch 0076, iter [03200, 05004], lr: 0.070968, loss: 2.6936
2022-07-15 05:06:06 - train: epoch 0076, iter [03300, 05004], lr: 0.070953, loss: 2.7792
2022-07-15 05:06:41 - train: epoch 0076, iter [03400, 05004], lr: 0.070939, loss: 2.5053
2022-07-15 05:07:15 - train: epoch 0076, iter [03500, 05004], lr: 0.070924, loss: 2.7570
2022-07-15 05:07:50 - train: epoch 0076, iter [03600, 05004], lr: 0.070910, loss: 2.5274
2022-07-15 05:08:24 - train: epoch 0076, iter [03700, 05004], lr: 0.070895, loss: 2.3849
2022-07-15 05:08:58 - train: epoch 0076, iter [03800, 05004], lr: 0.070880, loss: 2.6430
2022-07-15 05:09:32 - train: epoch 0076, iter [03900, 05004], lr: 0.070866, loss: 2.6162
2022-07-15 05:10:06 - train: epoch 0076, iter [04000, 05004], lr: 0.070851, loss: 2.6171
2022-07-15 05:10:39 - train: epoch 0076, iter [04100, 05004], lr: 0.070836, loss: 2.9492
2022-07-15 05:11:14 - train: epoch 0076, iter [04200, 05004], lr: 0.070822, loss: 2.7394
2022-07-15 05:11:48 - train: epoch 0076, iter [04300, 05004], lr: 0.070807, loss: 2.7781
2022-07-15 05:12:22 - train: epoch 0076, iter [04400, 05004], lr: 0.070793, loss: 2.5222
2022-07-15 05:12:55 - train: epoch 0076, iter [04500, 05004], lr: 0.070778, loss: 2.4983
2022-07-15 05:13:29 - train: epoch 0076, iter [04600, 05004], lr: 0.070763, loss: 2.9387
2022-07-15 05:14:03 - train: epoch 0076, iter [04700, 05004], lr: 0.070749, loss: 2.9304
2022-07-15 05:14:37 - train: epoch 0076, iter [04800, 05004], lr: 0.070734, loss: 2.7859
2022-07-15 05:15:12 - train: epoch 0076, iter [04900, 05004], lr: 0.070719, loss: 2.6828
2022-07-15 05:15:45 - train: epoch 0076, iter [05000, 05004], lr: 0.070705, loss: 2.6948
2022-07-15 05:15:46 - train: epoch 076, train_loss: 2.6830
2022-07-15 05:17:00 - eval: epoch: 076, acc1: 55.668%, acc5: 80.156%, test_loss: 1.8912, per_image_load_time: 1.650ms, per_image_inference_time: 0.489ms
2022-07-15 05:17:01 - until epoch: 076, best_acc1: 60.336%
2022-07-15 05:17:01 - epoch 077 lr: 0.070704
2022-07-15 05:17:41 - train: epoch 0077, iter [00100, 05004], lr: 0.070689, loss: 3.0502
2022-07-15 05:18:15 - train: epoch 0077, iter [00200, 05004], lr: 0.070675, loss: 2.6020
2022-07-15 05:18:49 - train: epoch 0077, iter [00300, 05004], lr: 0.070660, loss: 2.7348
2022-07-15 05:19:23 - train: epoch 0077, iter [00400, 05004], lr: 0.070645, loss: 2.5286
2022-07-15 05:19:57 - train: epoch 0077, iter [00500, 05004], lr: 0.070631, loss: 2.4602
2022-07-15 05:20:31 - train: epoch 0077, iter [00600, 05004], lr: 0.070616, loss: 2.4814
2022-07-15 05:21:06 - train: epoch 0077, iter [00700, 05004], lr: 0.070601, loss: 2.9070
2022-07-15 05:21:40 - train: epoch 0077, iter [00800, 05004], lr: 0.070587, loss: 2.6642
2022-07-15 05:22:14 - train: epoch 0077, iter [00900, 05004], lr: 0.070572, loss: 2.6707
2022-07-15 05:22:48 - train: epoch 0077, iter [01000, 05004], lr: 0.070557, loss: 2.8410
2022-07-15 05:23:23 - train: epoch 0077, iter [01100, 05004], lr: 0.070543, loss: 2.6461
2022-07-15 05:23:56 - train: epoch 0077, iter [01200, 05004], lr: 0.070528, loss: 2.8789
2022-07-15 05:24:31 - train: epoch 0077, iter [01300, 05004], lr: 0.070513, loss: 2.4909
2022-07-15 05:25:05 - train: epoch 0077, iter [01400, 05004], lr: 0.070499, loss: 2.5233
2022-07-15 05:25:40 - train: epoch 0077, iter [01500, 05004], lr: 0.070484, loss: 2.6054
2022-07-15 05:26:14 - train: epoch 0077, iter [01600, 05004], lr: 0.070469, loss: 2.4613
2022-07-15 05:26:48 - train: epoch 0077, iter [01700, 05004], lr: 0.070455, loss: 2.8375
2022-07-15 05:27:22 - train: epoch 0077, iter [01800, 05004], lr: 0.070440, loss: 2.6756
2022-07-15 05:27:56 - train: epoch 0077, iter [01900, 05004], lr: 0.070425, loss: 2.3398
2022-07-15 05:28:31 - train: epoch 0077, iter [02000, 05004], lr: 0.070411, loss: 2.6939
2022-07-15 05:29:05 - train: epoch 0077, iter [02100, 05004], lr: 0.070396, loss: 2.8511
2022-07-15 05:29:39 - train: epoch 0077, iter [02200, 05004], lr: 0.070381, loss: 2.5887
2022-07-15 05:30:14 - train: epoch 0077, iter [02300, 05004], lr: 0.070367, loss: 2.8434
2022-07-15 05:30:48 - train: epoch 0077, iter [02400, 05004], lr: 0.070352, loss: 2.6689
2022-07-15 05:31:23 - train: epoch 0077, iter [02500, 05004], lr: 0.070337, loss: 2.9836
2022-07-15 05:31:57 - train: epoch 0077, iter [02600, 05004], lr: 0.070322, loss: 2.3978
2022-07-15 05:32:31 - train: epoch 0077, iter [02700, 05004], lr: 0.070308, loss: 2.7343
2022-07-15 05:33:05 - train: epoch 0077, iter [02800, 05004], lr: 0.070293, loss: 2.6760
2022-07-15 05:33:40 - train: epoch 0077, iter [02900, 05004], lr: 0.070278, loss: 2.5195
2022-07-15 05:34:14 - train: epoch 0077, iter [03000, 05004], lr: 0.070264, loss: 2.5458
2022-07-15 05:34:49 - train: epoch 0077, iter [03100, 05004], lr: 0.070249, loss: 2.6020
2022-07-15 05:35:23 - train: epoch 0077, iter [03200, 05004], lr: 0.070234, loss: 2.8626
2022-07-15 05:35:58 - train: epoch 0077, iter [03300, 05004], lr: 0.070219, loss: 2.6991
2022-07-15 05:36:33 - train: epoch 0077, iter [03400, 05004], lr: 0.070205, loss: 2.9499
2022-07-15 05:37:07 - train: epoch 0077, iter [03500, 05004], lr: 0.070190, loss: 2.6412
2022-07-15 05:37:42 - train: epoch 0077, iter [03600, 05004], lr: 0.070175, loss: 2.9293
2022-07-15 05:38:16 - train: epoch 0077, iter [03700, 05004], lr: 0.070161, loss: 2.6251
2022-07-15 05:38:50 - train: epoch 0077, iter [03800, 05004], lr: 0.070146, loss: 2.6349
2022-07-15 05:39:25 - train: epoch 0077, iter [03900, 05004], lr: 0.070131, loss: 2.4401
2022-07-15 05:40:00 - train: epoch 0077, iter [04000, 05004], lr: 0.070116, loss: 2.5995
2022-07-15 05:40:35 - train: epoch 0077, iter [04100, 05004], lr: 0.070102, loss: 2.8638
2022-07-15 05:41:09 - train: epoch 0077, iter [04200, 05004], lr: 0.070087, loss: 2.7849
2022-07-15 05:41:43 - train: epoch 0077, iter [04300, 05004], lr: 0.070072, loss: 2.5542
2022-07-15 05:42:18 - train: epoch 0077, iter [04400, 05004], lr: 0.070057, loss: 2.7202
2022-07-15 05:42:53 - train: epoch 0077, iter [04500, 05004], lr: 0.070043, loss: 2.5504
2022-07-15 05:43:27 - train: epoch 0077, iter [04600, 05004], lr: 0.070028, loss: 2.4835
2022-07-15 05:44:01 - train: epoch 0077, iter [04700, 05004], lr: 0.070013, loss: 2.7884
2022-07-15 05:44:36 - train: epoch 0077, iter [04800, 05004], lr: 0.069998, loss: 2.5097
2022-07-15 05:45:10 - train: epoch 0077, iter [04900, 05004], lr: 0.069984, loss: 2.6903
2022-07-15 05:45:43 - train: epoch 0077, iter [05000, 05004], lr: 0.069969, loss: 2.5146
2022-07-15 05:45:44 - train: epoch 077, train_loss: 2.6787
2022-07-15 05:46:59 - eval: epoch: 077, acc1: 61.028%, acc5: 84.472%, test_loss: 1.6203, per_image_load_time: 0.996ms, per_image_inference_time: 0.492ms
2022-07-15 05:46:59 - until epoch: 077, best_acc1: 61.028%
2022-07-15 05:46:59 - epoch 078 lr: 0.069968
2022-07-15 05:47:39 - train: epoch 0078, iter [00100, 05004], lr: 0.069953, loss: 2.7795
2022-07-15 05:48:13 - train: epoch 0078, iter [00200, 05004], lr: 0.069939, loss: 2.8832
2022-07-15 05:48:47 - train: epoch 0078, iter [00300, 05004], lr: 0.069924, loss: 2.7601
2022-07-15 05:49:21 - train: epoch 0078, iter [00400, 05004], lr: 0.069909, loss: 2.5703
2022-07-15 05:49:56 - train: epoch 0078, iter [00500, 05004], lr: 0.069894, loss: 2.5982
2022-07-15 05:50:30 - train: epoch 0078, iter [00600, 05004], lr: 0.069880, loss: 2.5555
2022-07-15 05:51:04 - train: epoch 0078, iter [00700, 05004], lr: 0.069865, loss: 2.2993
2022-07-15 05:51:39 - train: epoch 0078, iter [00800, 05004], lr: 0.069850, loss: 2.8050
2022-07-15 05:52:13 - train: epoch 0078, iter [00900, 05004], lr: 0.069835, loss: 2.9113
2022-07-15 05:52:47 - train: epoch 0078, iter [01000, 05004], lr: 0.069821, loss: 2.6091
2022-07-15 05:53:23 - train: epoch 0078, iter [01100, 05004], lr: 0.069806, loss: 2.7000
2022-07-15 05:53:56 - train: epoch 0078, iter [01200, 05004], lr: 0.069791, loss: 2.5524
2022-07-15 05:54:31 - train: epoch 0078, iter [01300, 05004], lr: 0.069776, loss: 2.4954
2022-07-15 05:55:05 - train: epoch 0078, iter [01400, 05004], lr: 0.069761, loss: 2.5278
2022-07-15 05:55:39 - train: epoch 0078, iter [01500, 05004], lr: 0.069747, loss: 2.6493
2022-07-15 05:56:14 - train: epoch 0078, iter [01600, 05004], lr: 0.069732, loss: 2.8516
2022-07-15 05:56:48 - train: epoch 0078, iter [01700, 05004], lr: 0.069717, loss: 2.6345
2022-07-15 05:57:23 - train: epoch 0078, iter [01800, 05004], lr: 0.069702, loss: 3.0638
2022-07-15 05:57:57 - train: epoch 0078, iter [01900, 05004], lr: 0.069687, loss: 2.3384
2022-07-15 05:58:32 - train: epoch 0078, iter [02000, 05004], lr: 0.069673, loss: 2.7827
2022-07-15 05:59:07 - train: epoch 0078, iter [02100, 05004], lr: 0.069658, loss: 2.6898
2022-07-15 05:59:41 - train: epoch 0078, iter [02200, 05004], lr: 0.069643, loss: 2.5641
2022-07-15 06:00:15 - train: epoch 0078, iter [02300, 05004], lr: 0.069628, loss: 2.8236
2022-07-15 06:00:49 - train: epoch 0078, iter [02400, 05004], lr: 0.069613, loss: 2.4255
2022-07-15 06:01:24 - train: epoch 0078, iter [02500, 05004], lr: 0.069599, loss: 2.5285
2022-07-15 06:01:59 - train: epoch 0078, iter [02600, 05004], lr: 0.069584, loss: 2.7218
2022-07-15 06:02:35 - train: epoch 0078, iter [02700, 05004], lr: 0.069569, loss: 2.6374
2022-07-15 06:03:08 - train: epoch 0078, iter [02800, 05004], lr: 0.069554, loss: 2.5376
2022-07-15 06:03:42 - train: epoch 0078, iter [02900, 05004], lr: 0.069539, loss: 2.4364
2022-07-15 06:04:16 - train: epoch 0078, iter [03000, 05004], lr: 0.069525, loss: 3.1243
2022-07-15 06:04:50 - train: epoch 0078, iter [03100, 05004], lr: 0.069510, loss: 2.6289
2022-07-15 06:05:24 - train: epoch 0078, iter [03200, 05004], lr: 0.069495, loss: 2.7354
2022-07-15 06:05:59 - train: epoch 0078, iter [03300, 05004], lr: 0.069480, loss: 2.9757
2022-07-15 06:06:33 - train: epoch 0078, iter [03400, 05004], lr: 0.069465, loss: 2.5440
2022-07-15 06:07:08 - train: epoch 0078, iter [03500, 05004], lr: 0.069450, loss: 2.8860
2022-07-15 06:07:43 - train: epoch 0078, iter [03600, 05004], lr: 0.069436, loss: 2.7193
2022-07-15 06:08:17 - train: epoch 0078, iter [03700, 05004], lr: 0.069421, loss: 2.5830
2022-07-15 06:08:51 - train: epoch 0078, iter [03800, 05004], lr: 0.069406, loss: 2.2439
2022-07-15 06:09:25 - train: epoch 0078, iter [03900, 05004], lr: 0.069391, loss: 2.4081
2022-07-15 06:10:01 - train: epoch 0078, iter [04000, 05004], lr: 0.069376, loss: 2.5737
2022-07-15 06:10:35 - train: epoch 0078, iter [04100, 05004], lr: 0.069361, loss: 2.9343
2022-07-15 06:11:09 - train: epoch 0078, iter [04200, 05004], lr: 0.069347, loss: 2.7337
2022-07-15 06:11:43 - train: epoch 0078, iter [04300, 05004], lr: 0.069332, loss: 2.8098
2022-07-15 06:12:18 - train: epoch 0078, iter [04400, 05004], lr: 0.069317, loss: 2.5719
2022-07-15 06:12:52 - train: epoch 0078, iter [04500, 05004], lr: 0.069302, loss: 2.6286
2022-07-15 06:13:27 - train: epoch 0078, iter [04600, 05004], lr: 0.069287, loss: 2.6784
2022-07-15 06:14:01 - train: epoch 0078, iter [04700, 05004], lr: 0.069272, loss: 2.6848
2022-07-15 06:14:35 - train: epoch 0078, iter [04800, 05004], lr: 0.069257, loss: 2.7377
2022-07-15 06:15:09 - train: epoch 0078, iter [04900, 05004], lr: 0.069243, loss: 2.7277
2022-07-15 06:15:42 - train: epoch 0078, iter [05000, 05004], lr: 0.069228, loss: 2.6673
2022-07-15 06:15:43 - train: epoch 078, train_loss: 2.6734
2022-07-15 06:16:57 - eval: epoch: 078, acc1: 60.218%, acc5: 83.914%, test_loss: 1.6630, per_image_load_time: 2.374ms, per_image_inference_time: 0.456ms
2022-07-15 06:16:57 - until epoch: 078, best_acc1: 61.028%
2022-07-15 06:16:57 - epoch 079 lr: 0.069227
2022-07-15 06:17:36 - train: epoch 0079, iter [00100, 05004], lr: 0.069212, loss: 2.6401
2022-07-15 06:18:10 - train: epoch 0079, iter [00200, 05004], lr: 0.069197, loss: 2.2224
2022-07-15 06:18:44 - train: epoch 0079, iter [00300, 05004], lr: 0.069183, loss: 2.5439
2022-07-15 06:19:18 - train: epoch 0079, iter [00400, 05004], lr: 0.069168, loss: 2.7191
2022-07-15 06:19:52 - train: epoch 0079, iter [00500, 05004], lr: 0.069153, loss: 2.2902
2022-07-15 06:20:26 - train: epoch 0079, iter [00600, 05004], lr: 0.069138, loss: 2.8891
2022-07-15 06:21:01 - train: epoch 0079, iter [00700, 05004], lr: 0.069123, loss: 2.6711
2022-07-15 06:21:35 - train: epoch 0079, iter [00800, 05004], lr: 0.069108, loss: 2.8823
2022-07-15 06:22:08 - train: epoch 0079, iter [00900, 05004], lr: 0.069093, loss: 2.6002
2022-07-15 06:22:43 - train: epoch 0079, iter [01000, 05004], lr: 0.069078, loss: 2.6432
2022-07-15 06:23:16 - train: epoch 0079, iter [01100, 05004], lr: 0.069064, loss: 2.8255
2022-07-15 06:23:51 - train: epoch 0079, iter [01200, 05004], lr: 0.069049, loss: 3.0812
2022-07-15 06:24:26 - train: epoch 0079, iter [01300, 05004], lr: 0.069034, loss: 2.4753
2022-07-15 06:24:59 - train: epoch 0079, iter [01400, 05004], lr: 0.069019, loss: 2.7333
2022-07-15 06:25:32 - train: epoch 0079, iter [01500, 05004], lr: 0.069004, loss: 2.6323
2022-07-15 06:26:06 - train: epoch 0079, iter [01600, 05004], lr: 0.068989, loss: 2.2382
2022-07-15 06:26:40 - train: epoch 0079, iter [01700, 05004], lr: 0.068974, loss: 2.7373
2022-07-15 06:27:14 - train: epoch 0079, iter [01800, 05004], lr: 0.068959, loss: 3.0034
2022-07-15 06:27:48 - train: epoch 0079, iter [01900, 05004], lr: 0.068944, loss: 2.7218
2022-07-15 06:28:22 - train: epoch 0079, iter [02000, 05004], lr: 0.068930, loss: 2.8913
2022-07-15 06:28:56 - train: epoch 0079, iter [02100, 05004], lr: 0.068915, loss: 2.4555
2022-07-15 06:29:30 - train: epoch 0079, iter [02200, 05004], lr: 0.068900, loss: 2.4679
2022-07-15 06:30:04 - train: epoch 0079, iter [02300, 05004], lr: 0.068885, loss: 2.7051
2022-07-15 06:30:37 - train: epoch 0079, iter [02400, 05004], lr: 0.068870, loss: 2.7396
2022-07-15 06:31:11 - train: epoch 0079, iter [02500, 05004], lr: 0.068855, loss: 2.7540
2022-07-15 06:31:45 - train: epoch 0079, iter [02600, 05004], lr: 0.068840, loss: 2.6600
2022-07-15 06:32:19 - train: epoch 0079, iter [02700, 05004], lr: 0.068825, loss: 2.3631
2022-07-15 06:32:52 - train: epoch 0079, iter [02800, 05004], lr: 0.068810, loss: 2.6050
2022-07-15 06:33:26 - train: epoch 0079, iter [02900, 05004], lr: 0.068795, loss: 2.1683
2022-07-15 06:34:01 - train: epoch 0079, iter [03000, 05004], lr: 0.068780, loss: 2.8523
2022-07-15 06:34:35 - train: epoch 0079, iter [03100, 05004], lr: 0.068766, loss: 2.5474
2022-07-15 06:35:09 - train: epoch 0079, iter [03200, 05004], lr: 0.068751, loss: 2.6898
2022-07-15 06:35:42 - train: epoch 0079, iter [03300, 05004], lr: 0.068736, loss: 2.5892
2022-07-15 06:36:17 - train: epoch 0079, iter [03400, 05004], lr: 0.068721, loss: 2.8046
2022-07-15 06:36:50 - train: epoch 0079, iter [03500, 05004], lr: 0.068706, loss: 2.8022
2022-07-15 06:37:25 - train: epoch 0079, iter [03600, 05004], lr: 0.068691, loss: 2.5516
2022-07-15 06:37:59 - train: epoch 0079, iter [03700, 05004], lr: 0.068676, loss: 2.6103
2022-07-15 06:38:33 - train: epoch 0079, iter [03800, 05004], lr: 0.068661, loss: 2.6466
2022-07-15 06:39:07 - train: epoch 0079, iter [03900, 05004], lr: 0.068646, loss: 2.6419
2022-07-15 06:39:41 - train: epoch 0079, iter [04000, 05004], lr: 0.068631, loss: 2.6936
2022-07-15 06:40:16 - train: epoch 0079, iter [04100, 05004], lr: 0.068616, loss: 2.8515
2022-07-15 06:40:50 - train: epoch 0079, iter [04200, 05004], lr: 0.068601, loss: 2.5439
2022-07-15 06:41:24 - train: epoch 0079, iter [04300, 05004], lr: 0.068586, loss: 2.7353
2022-07-15 06:41:58 - train: epoch 0079, iter [04400, 05004], lr: 0.068571, loss: 3.0854
2022-07-15 06:42:32 - train: epoch 0079, iter [04500, 05004], lr: 0.068556, loss: 2.8084
2022-07-15 06:43:06 - train: epoch 0079, iter [04600, 05004], lr: 0.068542, loss: 2.7636
2022-07-15 06:43:40 - train: epoch 0079, iter [04700, 05004], lr: 0.068527, loss: 2.4421
2022-07-15 06:44:14 - train: epoch 0079, iter [04800, 05004], lr: 0.068512, loss: 2.8776
2022-07-15 06:44:47 - train: epoch 0079, iter [04900, 05004], lr: 0.068497, loss: 2.6999
2022-07-15 06:45:20 - train: epoch 0079, iter [05000, 05004], lr: 0.068482, loss: 2.2173
2022-07-15 06:45:21 - train: epoch 079, train_loss: 2.6678
2022-07-15 06:46:35 - eval: epoch: 079, acc1: 58.204%, acc5: 82.038%, test_loss: 1.7713, per_image_load_time: 1.714ms, per_image_inference_time: 0.480ms
2022-07-15 06:46:35 - until epoch: 079, best_acc1: 61.028%
2022-07-15 06:46:35 - epoch 080 lr: 0.068481
2022-07-15 06:47:15 - train: epoch 0080, iter [00100, 05004], lr: 0.068466, loss: 2.2747
2022-07-15 06:47:50 - train: epoch 0080, iter [00200, 05004], lr: 0.068451, loss: 2.9026
2022-07-15 06:48:24 - train: epoch 0080, iter [00300, 05004], lr: 0.068436, loss: 2.6875
2022-07-15 06:48:58 - train: epoch 0080, iter [00400, 05004], lr: 0.068421, loss: 2.4492
2022-07-15 06:49:32 - train: epoch 0080, iter [00500, 05004], lr: 0.068406, loss: 2.4574
2022-07-15 06:50:07 - train: epoch 0080, iter [00600, 05004], lr: 0.068391, loss: 2.7438
2022-07-15 06:50:41 - train: epoch 0080, iter [00700, 05004], lr: 0.068376, loss: 2.5673
2022-07-15 06:51:15 - train: epoch 0080, iter [00800, 05004], lr: 0.068361, loss: 2.3841
2022-07-15 06:51:50 - train: epoch 0080, iter [00900, 05004], lr: 0.068346, loss: 2.8692
2022-07-15 06:52:23 - train: epoch 0080, iter [01000, 05004], lr: 0.068331, loss: 2.5356
2022-07-15 06:52:57 - train: epoch 0080, iter [01100, 05004], lr: 0.068316, loss: 2.8431
2022-07-15 06:53:32 - train: epoch 0080, iter [01200, 05004], lr: 0.068301, loss: 2.8463
2022-07-15 06:54:07 - train: epoch 0080, iter [01300, 05004], lr: 0.068286, loss: 2.5525
2022-07-15 06:54:41 - train: epoch 0080, iter [01400, 05004], lr: 0.068272, loss: 2.7783
2022-07-15 06:55:16 - train: epoch 0080, iter [01500, 05004], lr: 0.068257, loss: 2.5159
2022-07-15 06:55:50 - train: epoch 0080, iter [01600, 05004], lr: 0.068242, loss: 2.4217
2022-07-15 06:56:25 - train: epoch 0080, iter [01700, 05004], lr: 0.068227, loss: 2.9238
2022-07-15 06:56:59 - train: epoch 0080, iter [01800, 05004], lr: 0.068212, loss: 2.4697
2022-07-15 06:57:34 - train: epoch 0080, iter [01900, 05004], lr: 0.068197, loss: 2.6455
2022-07-15 06:58:09 - train: epoch 0080, iter [02000, 05004], lr: 0.068182, loss: 2.5977
2022-07-15 06:58:43 - train: epoch 0080, iter [02100, 05004], lr: 0.068167, loss: 2.8008
2022-07-15 06:59:18 - train: epoch 0080, iter [02200, 05004], lr: 0.068152, loss: 2.6957
2022-07-15 06:59:52 - train: epoch 0080, iter [02300, 05004], lr: 0.068137, loss: 2.5660
2022-07-15 07:00:27 - train: epoch 0080, iter [02400, 05004], lr: 0.068122, loss: 2.6029
2022-07-15 07:01:01 - train: epoch 0080, iter [02500, 05004], lr: 0.068107, loss: 2.3948
2022-07-15 07:01:36 - train: epoch 0080, iter [02600, 05004], lr: 0.068092, loss: 2.6343
2022-07-15 07:02:10 - train: epoch 0080, iter [02700, 05004], lr: 0.068077, loss: 2.7750
2022-07-15 07:02:45 - train: epoch 0080, iter [02800, 05004], lr: 0.068062, loss: 2.7452
2022-07-15 07:03:19 - train: epoch 0080, iter [02900, 05004], lr: 0.068047, loss: 2.6565
2022-07-15 07:03:54 - train: epoch 0080, iter [03000, 05004], lr: 0.068032, loss: 2.5704
2022-07-15 07:04:28 - train: epoch 0080, iter [03100, 05004], lr: 0.068016, loss: 2.4572
2022-07-15 07:05:02 - train: epoch 0080, iter [03200, 05004], lr: 0.068001, loss: 2.7209
2022-07-15 07:05:37 - train: epoch 0080, iter [03300, 05004], lr: 0.067986, loss: 2.6207
2022-07-15 07:06:12 - train: epoch 0080, iter [03400, 05004], lr: 0.067971, loss: 2.5881
2022-07-15 07:06:46 - train: epoch 0080, iter [03500, 05004], lr: 0.067956, loss: 3.0026
2022-07-15 07:07:21 - train: epoch 0080, iter [03600, 05004], lr: 0.067941, loss: 2.6389
2022-07-15 07:07:55 - train: epoch 0080, iter [03700, 05004], lr: 0.067926, loss: 2.5826
2022-07-15 07:08:30 - train: epoch 0080, iter [03800, 05004], lr: 0.067911, loss: 2.6182
2022-07-15 07:09:04 - train: epoch 0080, iter [03900, 05004], lr: 0.067896, loss: 2.4416
2022-07-15 07:09:39 - train: epoch 0080, iter [04000, 05004], lr: 0.067881, loss: 2.7254
2022-07-15 07:10:13 - train: epoch 0080, iter [04100, 05004], lr: 0.067866, loss: 2.7246
2022-07-15 07:10:48 - train: epoch 0080, iter [04200, 05004], lr: 0.067851, loss: 2.5692
2022-07-15 07:11:22 - train: epoch 0080, iter [04300, 05004], lr: 0.067836, loss: 3.2017
2022-07-15 07:11:57 - train: epoch 0080, iter [04400, 05004], lr: 0.067821, loss: 2.9926
2022-07-15 07:12:31 - train: epoch 0080, iter [04500, 05004], lr: 0.067806, loss: 2.3395
2022-07-15 07:13:05 - train: epoch 0080, iter [04600, 05004], lr: 0.067791, loss: 2.5655
2022-07-15 07:13:40 - train: epoch 0080, iter [04700, 05004], lr: 0.067776, loss: 2.7288
2022-07-15 07:14:14 - train: epoch 0080, iter [04800, 05004], lr: 0.067761, loss: 2.7385
2022-07-15 07:14:49 - train: epoch 0080, iter [04900, 05004], lr: 0.067746, loss: 2.8731
2022-07-15 07:15:23 - train: epoch 0080, iter [05000, 05004], lr: 0.067731, loss: 2.7771
2022-07-15 07:15:24 - train: epoch 080, train_loss: 2.6650
2022-07-15 07:16:39 - eval: epoch: 080, acc1: 60.884%, acc5: 84.242%, test_loss: 1.6300, per_image_load_time: 2.012ms, per_image_inference_time: 0.470ms
2022-07-15 07:16:39 - until epoch: 080, best_acc1: 61.028%
2022-07-15 08:33:15 - epoch 081 lr: 0.067730
2022-07-15 08:33:55 - train: epoch 0081, iter [00100, 05004], lr: 0.067715, loss: 2.3210
2022-07-15 08:34:29 - train: epoch 0081, iter [00200, 05004], lr: 0.067700, loss: 2.2203
2022-07-15 08:35:03 - train: epoch 0081, iter [00300, 05004], lr: 0.067685, loss: 2.6177
2022-07-15 08:35:37 - train: epoch 0081, iter [00400, 05004], lr: 0.067670, loss: 2.3652
2022-07-15 08:36:11 - train: epoch 0081, iter [00500, 05004], lr: 0.067655, loss: 2.8559
2022-07-15 08:36:44 - train: epoch 0081, iter [00600, 05004], lr: 0.067640, loss: 2.5409
2022-07-15 08:37:18 - train: epoch 0081, iter [00700, 05004], lr: 0.067625, loss: 2.6568
2022-07-15 08:37:51 - train: epoch 0081, iter [00800, 05004], lr: 0.067610, loss: 2.6018
2022-07-15 08:38:25 - train: epoch 0081, iter [00900, 05004], lr: 0.067595, loss: 2.7518
2022-07-15 08:38:58 - train: epoch 0081, iter [01000, 05004], lr: 0.067580, loss: 2.7659
2022-07-15 08:39:31 - train: epoch 0081, iter [01100, 05004], lr: 0.067565, loss: 2.4648
2022-07-15 08:40:05 - train: epoch 0081, iter [01200, 05004], lr: 0.067549, loss: 2.8802
2022-07-15 08:40:38 - train: epoch 0081, iter [01300, 05004], lr: 0.067534, loss: 2.6141
2022-07-15 08:41:12 - train: epoch 0081, iter [01400, 05004], lr: 0.067519, loss: 2.5460
2022-07-15 08:41:46 - train: epoch 0081, iter [01500, 05004], lr: 0.067504, loss: 2.7654
2022-07-15 08:42:20 - train: epoch 0081, iter [01600, 05004], lr: 0.067489, loss: 2.7109
2022-07-15 08:42:53 - train: epoch 0081, iter [01700, 05004], lr: 0.067474, loss: 2.6266
2022-07-15 08:43:28 - train: epoch 0081, iter [01800, 05004], lr: 0.067459, loss: 2.7664
2022-07-15 08:44:01 - train: epoch 0081, iter [01900, 05004], lr: 0.067444, loss: 2.6524
2022-07-15 08:44:34 - train: epoch 0081, iter [02000, 05004], lr: 0.067429, loss: 2.8523
2022-07-15 08:45:08 - train: epoch 0081, iter [02100, 05004], lr: 0.067414, loss: 2.7646
2022-07-15 08:45:42 - train: epoch 0081, iter [02200, 05004], lr: 0.067399, loss: 2.6834
2022-07-15 08:46:16 - train: epoch 0081, iter [02300, 05004], lr: 0.067384, loss: 2.6564
2022-07-15 08:46:50 - train: epoch 0081, iter [02400, 05004], lr: 0.067368, loss: 2.6932
2022-07-15 08:47:24 - train: epoch 0081, iter [02500, 05004], lr: 0.067353, loss: 2.7859
2022-07-15 08:47:57 - train: epoch 0081, iter [02600, 05004], lr: 0.067338, loss: 2.8839
2022-07-15 08:48:31 - train: epoch 0081, iter [02700, 05004], lr: 0.067323, loss: 2.7521
2022-07-15 08:49:05 - train: epoch 0081, iter [02800, 05004], lr: 0.067308, loss: 2.3409
2022-07-15 08:49:38 - train: epoch 0081, iter [02900, 05004], lr: 0.067293, loss: 2.5055
2022-07-15 08:50:12 - train: epoch 0081, iter [03000, 05004], lr: 0.067278, loss: 2.5984
2022-07-15 08:50:46 - train: epoch 0081, iter [03100, 05004], lr: 0.067263, loss: 2.6158
2022-07-15 08:51:20 - train: epoch 0081, iter [03200, 05004], lr: 0.067248, loss: 2.5995
2022-07-15 08:51:54 - train: epoch 0081, iter [03300, 05004], lr: 0.067233, loss: 2.7859
2022-07-15 08:52:29 - train: epoch 0081, iter [03400, 05004], lr: 0.067217, loss: 2.7709
2022-07-15 08:53:02 - train: epoch 0081, iter [03500, 05004], lr: 0.067202, loss: 2.7920
2022-07-15 08:53:36 - train: epoch 0081, iter [03600, 05004], lr: 0.067187, loss: 2.5086
2022-07-15 08:54:10 - train: epoch 0081, iter [03700, 05004], lr: 0.067172, loss: 2.7980
2022-07-15 08:54:42 - train: epoch 0081, iter [03800, 05004], lr: 0.067157, loss: 2.8651
2022-07-15 08:55:16 - train: epoch 0081, iter [03900, 05004], lr: 0.067142, loss: 3.0341
2022-07-15 08:55:51 - train: epoch 0081, iter [04000, 05004], lr: 0.067127, loss: 2.6690
2022-07-15 08:56:24 - train: epoch 0081, iter [04100, 05004], lr: 0.067112, loss: 2.7132
2022-07-15 08:56:57 - train: epoch 0081, iter [04200, 05004], lr: 0.067096, loss: 2.5652
2022-07-15 08:57:31 - train: epoch 0081, iter [04300, 05004], lr: 0.067081, loss: 2.6710
2022-07-15 08:58:04 - train: epoch 0081, iter [04400, 05004], lr: 0.067066, loss: 2.7135
2022-07-15 08:58:38 - train: epoch 0081, iter [04500, 05004], lr: 0.067051, loss: 2.8144
2022-07-15 08:59:12 - train: epoch 0081, iter [04600, 05004], lr: 0.067036, loss: 2.5840
2022-07-15 08:59:46 - train: epoch 0081, iter [04700, 05004], lr: 0.067021, loss: 2.9738
2022-07-15 09:00:20 - train: epoch 0081, iter [04800, 05004], lr: 0.067006, loss: 2.6903
2022-07-15 09:00:53 - train: epoch 0081, iter [04900, 05004], lr: 0.066991, loss: 2.6526
2022-07-15 09:01:26 - train: epoch 0081, iter [05000, 05004], lr: 0.066975, loss: 2.7620
2022-07-15 09:01:27 - train: epoch 081, train_loss: 2.6612
2022-07-15 09:02:41 - eval: epoch: 081, acc1: 60.398%, acc5: 83.866%, test_loss: 1.6608, per_image_load_time: 1.938ms, per_image_inference_time: 0.494ms
2022-07-15 09:02:42 - until epoch: 081, best_acc1: 61.028%
2022-07-15 09:02:42 - epoch 082 lr: 0.066975
2022-07-15 09:03:21 - train: epoch 0082, iter [00100, 05004], lr: 0.066960, loss: 2.7044
2022-07-15 09:03:55 - train: epoch 0082, iter [00200, 05004], lr: 0.066944, loss: 2.5584
2022-07-15 09:04:31 - train: epoch 0082, iter [00300, 05004], lr: 0.066929, loss: 2.6309
2022-07-15 09:05:04 - train: epoch 0082, iter [00400, 05004], lr: 0.066914, loss: 2.6724
2022-07-15 09:05:38 - train: epoch 0082, iter [00500, 05004], lr: 0.066899, loss: 2.7055
2022-07-15 09:06:13 - train: epoch 0082, iter [00600, 05004], lr: 0.066884, loss: 2.6346
2022-07-15 09:06:46 - train: epoch 0082, iter [00700, 05004], lr: 0.066869, loss: 2.5605
2022-07-15 09:07:21 - train: epoch 0082, iter [00800, 05004], lr: 0.066854, loss: 2.5816
2022-07-15 09:07:56 - train: epoch 0082, iter [00900, 05004], lr: 0.066838, loss: 2.8688
2022-07-15 09:08:30 - train: epoch 0082, iter [01000, 05004], lr: 0.066823, loss: 2.8523
2022-07-15 09:09:04 - train: epoch 0082, iter [01100, 05004], lr: 0.066808, loss: 2.5507
2022-07-15 09:09:39 - train: epoch 0082, iter [01200, 05004], lr: 0.066793, loss: 2.5402
2022-07-15 09:10:12 - train: epoch 0082, iter [01300, 05004], lr: 0.066778, loss: 2.8446
2022-07-15 09:10:47 - train: epoch 0082, iter [01400, 05004], lr: 0.066763, loss: 2.4497
2022-07-15 09:11:21 - train: epoch 0082, iter [01500, 05004], lr: 0.066747, loss: 2.7516
2022-07-15 09:11:55 - train: epoch 0082, iter [01600, 05004], lr: 0.066732, loss: 2.4276
2022-07-15 09:12:30 - train: epoch 0082, iter [01700, 05004], lr: 0.066717, loss: 2.5480
2022-07-15 09:13:04 - train: epoch 0082, iter [01800, 05004], lr: 0.066702, loss: 2.3905
2022-07-15 09:13:38 - train: epoch 0082, iter [01900, 05004], lr: 0.066687, loss: 2.6880
2022-07-15 09:14:13 - train: epoch 0082, iter [02000, 05004], lr: 0.066672, loss: 2.9406
2022-07-15 09:14:47 - train: epoch 0082, iter [02100, 05004], lr: 0.066656, loss: 2.8921
2022-07-15 09:15:21 - train: epoch 0082, iter [02200, 05004], lr: 0.066641, loss: 2.4614
2022-07-15 09:15:56 - train: epoch 0082, iter [02300, 05004], lr: 0.066626, loss: 2.5834
2022-07-15 09:16:31 - train: epoch 0082, iter [02400, 05004], lr: 0.066611, loss: 2.7191
2022-07-15 09:17:05 - train: epoch 0082, iter [02500, 05004], lr: 0.066596, loss: 2.6176
2022-07-15 09:17:39 - train: epoch 0082, iter [02600, 05004], lr: 0.066581, loss: 2.8886
2022-07-15 09:18:14 - train: epoch 0082, iter [02700, 05004], lr: 0.066565, loss: 2.4448
2022-07-15 09:18:49 - train: epoch 0082, iter [02800, 05004], lr: 0.066550, loss: 2.4823
2022-07-15 09:19:23 - train: epoch 0082, iter [02900, 05004], lr: 0.066535, loss: 2.4878
2022-07-15 09:19:57 - train: epoch 0082, iter [03000, 05004], lr: 0.066520, loss: 2.5570
2022-07-15 09:20:32 - train: epoch 0082, iter [03100, 05004], lr: 0.066505, loss: 2.5305
2022-07-15 09:21:06 - train: epoch 0082, iter [03200, 05004], lr: 0.066489, loss: 2.7918
2022-07-15 09:21:40 - train: epoch 0082, iter [03300, 05004], lr: 0.066474, loss: 2.5653
2022-07-15 09:22:15 - train: epoch 0082, iter [03400, 05004], lr: 0.066459, loss: 2.4705
2022-07-15 09:22:49 - train: epoch 0082, iter [03500, 05004], lr: 0.066444, loss: 2.7404
2022-07-15 09:23:24 - train: epoch 0082, iter [03600, 05004], lr: 0.066429, loss: 2.6593
2022-07-15 09:23:58 - train: epoch 0082, iter [03700, 05004], lr: 0.066413, loss: 2.4197
2022-07-15 09:24:32 - train: epoch 0082, iter [03800, 05004], lr: 0.066398, loss: 2.6417
2022-07-15 09:25:06 - train: epoch 0082, iter [03900, 05004], lr: 0.066383, loss: 2.5752
2022-07-15 09:25:40 - train: epoch 0082, iter [04000, 05004], lr: 0.066368, loss: 2.4931
2022-07-15 09:26:15 - train: epoch 0082, iter [04100, 05004], lr: 0.066353, loss: 2.7565
2022-07-15 09:26:50 - train: epoch 0082, iter [04200, 05004], lr: 0.066337, loss: 2.8073
2022-07-15 09:27:25 - train: epoch 0082, iter [04300, 05004], lr: 0.066322, loss: 2.7046
2022-07-15 09:27:59 - train: epoch 0082, iter [04400, 05004], lr: 0.066307, loss: 2.7803
2022-07-15 09:28:33 - train: epoch 0082, iter [04500, 05004], lr: 0.066292, loss: 2.5725
2022-07-15 09:29:08 - train: epoch 0082, iter [04600, 05004], lr: 0.066276, loss: 2.7128
2022-07-15 09:29:42 - train: epoch 0082, iter [04700, 05004], lr: 0.066261, loss: 2.6022
2022-07-15 09:30:16 - train: epoch 0082, iter [04800, 05004], lr: 0.066246, loss: 2.5063
2022-07-15 09:30:50 - train: epoch 0082, iter [04900, 05004], lr: 0.066231, loss: 2.4918
2022-07-15 09:31:23 - train: epoch 0082, iter [05000, 05004], lr: 0.066216, loss: 2.5471
2022-07-15 09:31:24 - train: epoch 082, train_loss: 2.6530
2022-07-15 09:32:38 - eval: epoch: 082, acc1: 61.336%, acc5: 84.512%, test_loss: 1.6102, per_image_load_time: 2.343ms, per_image_inference_time: 0.485ms
2022-07-15 09:32:39 - until epoch: 082, best_acc1: 61.336%
2022-07-15 09:32:39 - epoch 083 lr: 0.066215
2022-07-15 09:33:18 - train: epoch 0083, iter [00100, 05004], lr: 0.066200, loss: 2.6839
2022-07-15 09:33:52 - train: epoch 0083, iter [00200, 05004], lr: 0.066184, loss: 2.7761
2022-07-15 09:34:26 - train: epoch 0083, iter [00300, 05004], lr: 0.066169, loss: 2.6063
2022-07-15 09:34:59 - train: epoch 0083, iter [00400, 05004], lr: 0.066154, loss: 2.6761
2022-07-15 09:35:32 - train: epoch 0083, iter [00500, 05004], lr: 0.066139, loss: 2.6229
2022-07-15 09:36:06 - train: epoch 0083, iter [00600, 05004], lr: 0.066124, loss: 2.6959
2022-07-15 09:36:39 - train: epoch 0083, iter [00700, 05004], lr: 0.066108, loss: 2.3971
2022-07-15 09:37:13 - train: epoch 0083, iter [00800, 05004], lr: 0.066093, loss: 2.5314
2022-07-15 09:37:47 - train: epoch 0083, iter [00900, 05004], lr: 0.066078, loss: 2.6055
2022-07-15 09:38:20 - train: epoch 0083, iter [01000, 05004], lr: 0.066063, loss: 2.9748
2022-07-15 09:38:54 - train: epoch 0083, iter [01100, 05004], lr: 0.066047, loss: 2.6595
2022-07-15 09:39:27 - train: epoch 0083, iter [01200, 05004], lr: 0.066032, loss: 2.6324
2022-07-15 09:40:00 - train: epoch 0083, iter [01300, 05004], lr: 0.066017, loss: 2.5310
2022-07-15 09:40:34 - train: epoch 0083, iter [01400, 05004], lr: 0.066002, loss: 2.6058
2022-07-15 09:41:08 - train: epoch 0083, iter [01500, 05004], lr: 0.065986, loss: 2.5059
2022-07-15 09:41:41 - train: epoch 0083, iter [01600, 05004], lr: 0.065971, loss: 2.5949
2022-07-15 09:42:15 - train: epoch 0083, iter [01700, 05004], lr: 0.065956, loss: 2.7943
2022-07-15 09:42:49 - train: epoch 0083, iter [01800, 05004], lr: 0.065941, loss: 2.6870
2022-07-15 09:43:22 - train: epoch 0083, iter [01900, 05004], lr: 0.065925, loss: 2.7274
2022-07-15 09:43:57 - train: epoch 0083, iter [02000, 05004], lr: 0.065910, loss: 2.5257
2022-07-15 09:44:30 - train: epoch 0083, iter [02100, 05004], lr: 0.065895, loss: 2.3112
2022-07-15 09:45:04 - train: epoch 0083, iter [02200, 05004], lr: 0.065880, loss: 2.4277
2022-07-15 09:45:37 - train: epoch 0083, iter [02300, 05004], lr: 0.065864, loss: 2.5668
2022-07-15 09:46:10 - train: epoch 0083, iter [02400, 05004], lr: 0.065849, loss: 2.5057
2022-07-15 09:46:44 - train: epoch 0083, iter [02500, 05004], lr: 0.065834, loss: 2.4805
2022-07-15 09:47:18 - train: epoch 0083, iter [02600, 05004], lr: 0.065818, loss: 2.2407
2022-07-15 09:47:52 - train: epoch 0083, iter [02700, 05004], lr: 0.065803, loss: 2.5116
2022-07-15 09:48:26 - train: epoch 0083, iter [02800, 05004], lr: 0.065788, loss: 2.5287
2022-07-15 09:48:59 - train: epoch 0083, iter [02900, 05004], lr: 0.065773, loss: 2.7898
2022-07-15 09:49:33 - train: epoch 0083, iter [03000, 05004], lr: 0.065757, loss: 2.6154
2022-07-15 09:50:06 - train: epoch 0083, iter [03100, 05004], lr: 0.065742, loss: 2.4669
2022-07-15 09:50:41 - train: epoch 0083, iter [03200, 05004], lr: 0.065727, loss: 2.8700
2022-07-15 09:51:14 - train: epoch 0083, iter [03300, 05004], lr: 0.065711, loss: 2.8477
2022-07-15 09:51:47 - train: epoch 0083, iter [03400, 05004], lr: 0.065696, loss: 2.5068
2022-07-15 09:52:21 - train: epoch 0083, iter [03500, 05004], lr: 0.065681, loss: 2.5907
2022-07-15 09:52:55 - train: epoch 0083, iter [03600, 05004], lr: 0.065666, loss: 2.6880
2022-07-15 09:53:29 - train: epoch 0083, iter [03700, 05004], lr: 0.065650, loss: 2.2455
2022-07-15 09:54:02 - train: epoch 0083, iter [03800, 05004], lr: 0.065635, loss: 2.5369
2022-07-15 09:54:36 - train: epoch 0083, iter [03900, 05004], lr: 0.065620, loss: 2.8660
2022-07-15 09:55:10 - train: epoch 0083, iter [04000, 05004], lr: 0.065604, loss: 3.0844
2022-07-15 09:55:42 - train: epoch 0083, iter [04100, 05004], lr: 0.065589, loss: 2.6988
2022-07-15 09:56:16 - train: epoch 0083, iter [04200, 05004], lr: 0.065574, loss: 3.0302
2022-07-15 09:56:51 - train: epoch 0083, iter [04300, 05004], lr: 0.065559, loss: 2.5302
2022-07-15 09:57:25 - train: epoch 0083, iter [04400, 05004], lr: 0.065543, loss: 2.4091
2022-07-15 09:57:59 - train: epoch 0083, iter [04500, 05004], lr: 0.065528, loss: 2.8713
2022-07-15 09:58:32 - train: epoch 0083, iter [04600, 05004], lr: 0.065513, loss: 2.8159
2022-07-15 09:59:05 - train: epoch 0083, iter [04700, 05004], lr: 0.065497, loss: 2.9551
2022-07-15 09:59:40 - train: epoch 0083, iter [04800, 05004], lr: 0.065482, loss: 2.4677
2022-07-15 10:00:13 - train: epoch 0083, iter [04900, 05004], lr: 0.065467, loss: 2.5979
2022-07-15 10:00:45 - train: epoch 0083, iter [05000, 05004], lr: 0.065451, loss: 2.6504
2022-07-15 10:00:46 - train: epoch 083, train_loss: 2.6487
2022-07-15 10:02:00 - eval: epoch: 083, acc1: 62.056%, acc5: 84.800%, test_loss: 1.5821, per_image_load_time: 2.429ms, per_image_inference_time: 0.456ms
2022-07-15 10:02:01 - until epoch: 083, best_acc1: 62.056%
2022-07-15 10:02:01 - epoch 084 lr: 0.065451
2022-07-15 10:02:39 - train: epoch 0084, iter [00100, 05004], lr: 0.065436, loss: 2.3664
2022-07-15 10:03:15 - train: epoch 0084, iter [00200, 05004], lr: 0.065420, loss: 2.5666
2022-07-15 10:03:48 - train: epoch 0084, iter [00300, 05004], lr: 0.065405, loss: 2.6020
2022-07-15 10:04:23 - train: epoch 0084, iter [00400, 05004], lr: 0.065390, loss: 2.4733
2022-07-15 10:04:56 - train: epoch 0084, iter [00500, 05004], lr: 0.065374, loss: 2.8224
2022-07-15 10:05:30 - train: epoch 0084, iter [00600, 05004], lr: 0.065359, loss: 2.9380
2022-07-15 10:06:03 - train: epoch 0084, iter [00700, 05004], lr: 0.065344, loss: 2.7317
2022-07-15 10:06:37 - train: epoch 0084, iter [00800, 05004], lr: 0.065328, loss: 2.6091
2022-07-15 10:07:11 - train: epoch 0084, iter [00900, 05004], lr: 0.065313, loss: 2.8393
2022-07-15 10:07:45 - train: epoch 0084, iter [01000, 05004], lr: 0.065298, loss: 2.8755
2022-07-15 10:08:19 - train: epoch 0084, iter [01100, 05004], lr: 0.065282, loss: 2.6593
2022-07-15 10:08:53 - train: epoch 0084, iter [01200, 05004], lr: 0.065267, loss: 2.6360
2022-07-15 10:09:26 - train: epoch 0084, iter [01300, 05004], lr: 0.065252, loss: 2.7792
2022-07-15 10:10:01 - train: epoch 0084, iter [01400, 05004], lr: 0.065236, loss: 2.5201
2022-07-15 10:10:35 - train: epoch 0084, iter [01500, 05004], lr: 0.065221, loss: 2.3450
2022-07-15 10:11:09 - train: epoch 0084, iter [01600, 05004], lr: 0.065206, loss: 3.0400
2022-07-15 10:11:43 - train: epoch 0084, iter [01700, 05004], lr: 0.065190, loss: 2.8320
2022-07-15 10:12:17 - train: epoch 0084, iter [01800, 05004], lr: 0.065175, loss: 2.6338
2022-07-15 10:12:50 - train: epoch 0084, iter [01900, 05004], lr: 0.065160, loss: 2.4709
2022-07-15 10:13:24 - train: epoch 0084, iter [02000, 05004], lr: 0.065144, loss: 2.6578
2022-07-15 10:13:58 - train: epoch 0084, iter [02100, 05004], lr: 0.065129, loss: 2.5980
2022-07-15 10:14:32 - train: epoch 0084, iter [02200, 05004], lr: 0.065114, loss: 2.7431
2022-07-15 10:15:05 - train: epoch 0084, iter [02300, 05004], lr: 0.065098, loss: 2.6165
2022-07-15 10:15:40 - train: epoch 0084, iter [02400, 05004], lr: 0.065083, loss: 2.6858
2022-07-15 10:16:14 - train: epoch 0084, iter [02500, 05004], lr: 0.065068, loss: 2.6740
2022-07-15 10:16:48 - train: epoch 0084, iter [02600, 05004], lr: 0.065052, loss: 2.4023
2022-07-15 10:17:22 - train: epoch 0084, iter [02700, 05004], lr: 0.065037, loss: 2.6433
2022-07-15 10:17:56 - train: epoch 0084, iter [02800, 05004], lr: 0.065022, loss: 2.4667
2022-07-15 10:18:30 - train: epoch 0084, iter [02900, 05004], lr: 0.065006, loss: 2.5626
2022-07-15 10:19:04 - train: epoch 0084, iter [03000, 05004], lr: 0.064991, loss: 2.6702
2022-07-15 10:19:38 - train: epoch 0084, iter [03100, 05004], lr: 0.064975, loss: 2.6092
2022-07-15 10:20:13 - train: epoch 0084, iter [03200, 05004], lr: 0.064960, loss: 2.4687
2022-07-15 10:20:47 - train: epoch 0084, iter [03300, 05004], lr: 0.064945, loss: 2.5501
2022-07-15 10:21:21 - train: epoch 0084, iter [03400, 05004], lr: 0.064929, loss: 2.4941
2022-07-15 10:21:55 - train: epoch 0084, iter [03500, 05004], lr: 0.064914, loss: 2.3884
2022-07-15 10:22:29 - train: epoch 0084, iter [03600, 05004], lr: 0.064899, loss: 2.5520
2022-07-15 10:23:02 - train: epoch 0084, iter [03700, 05004], lr: 0.064883, loss: 2.8720
2022-07-15 10:23:36 - train: epoch 0084, iter [03800, 05004], lr: 0.064868, loss: 2.4701
2022-07-15 10:24:11 - train: epoch 0084, iter [03900, 05004], lr: 0.064853, loss: 2.7075
2022-07-15 10:24:44 - train: epoch 0084, iter [04000, 05004], lr: 0.064837, loss: 2.6473
2022-07-15 10:25:19 - train: epoch 0084, iter [04100, 05004], lr: 0.064822, loss: 2.5195
2022-07-15 10:25:52 - train: epoch 0084, iter [04200, 05004], lr: 0.064806, loss: 2.6111
2022-07-15 10:26:26 - train: epoch 0084, iter [04300, 05004], lr: 0.064791, loss: 2.8454
2022-07-15 10:26:59 - train: epoch 0084, iter [04400, 05004], lr: 0.064776, loss: 2.9270
2022-07-15 10:27:34 - train: epoch 0084, iter [04500, 05004], lr: 0.064760, loss: 2.1806
2022-07-15 10:28:09 - train: epoch 0084, iter [04600, 05004], lr: 0.064745, loss: 2.6295
2022-07-15 10:28:43 - train: epoch 0084, iter [04700, 05004], lr: 0.064730, loss: 2.6319
2022-07-15 10:29:18 - train: epoch 0084, iter [04800, 05004], lr: 0.064714, loss: 2.3485
2022-07-15 10:29:51 - train: epoch 0084, iter [04900, 05004], lr: 0.064699, loss: 2.5234
2022-07-15 10:30:25 - train: epoch 0084, iter [05000, 05004], lr: 0.064683, loss: 2.6007
2022-07-15 10:30:26 - train: epoch 084, train_loss: 2.6426
2022-07-15 10:31:41 - eval: epoch: 084, acc1: 61.240%, acc5: 84.636%, test_loss: 1.6020, per_image_load_time: 1.370ms, per_image_inference_time: 0.455ms
2022-07-15 10:31:41 - until epoch: 084, best_acc1: 62.056%
2022-07-15 10:31:41 - epoch 085 lr: 0.064683
2022-07-15 10:32:21 - train: epoch 0085, iter [00100, 05004], lr: 0.064667, loss: 2.4399
2022-07-15 10:32:55 - train: epoch 0085, iter [00200, 05004], lr: 0.064652, loss: 2.6613
2022-07-15 10:33:30 - train: epoch 0085, iter [00300, 05004], lr: 0.064637, loss: 2.5838
2022-07-15 10:34:05 - train: epoch 0085, iter [00400, 05004], lr: 0.064621, loss: 2.8711
2022-07-15 10:34:39 - train: epoch 0085, iter [00500, 05004], lr: 0.064606, loss: 2.7218
2022-07-15 10:35:12 - train: epoch 0085, iter [00600, 05004], lr: 0.064590, loss: 2.4428
2022-07-15 10:35:47 - train: epoch 0085, iter [00700, 05004], lr: 0.064575, loss: 2.8042
2022-07-15 10:36:22 - train: epoch 0085, iter [00800, 05004], lr: 0.064560, loss: 2.5801
2022-07-15 10:36:55 - train: epoch 0085, iter [00900, 05004], lr: 0.064544, loss: 2.5639
2022-07-15 10:37:30 - train: epoch 0085, iter [01000, 05004], lr: 0.064529, loss: 2.7325
2022-07-15 10:38:04 - train: epoch 0085, iter [01100, 05004], lr: 0.064513, loss: 2.8258
2022-07-15 10:38:39 - train: epoch 0085, iter [01200, 05004], lr: 0.064498, loss: 2.6604
2022-07-15 10:39:14 - train: epoch 0085, iter [01300, 05004], lr: 0.064483, loss: 2.5927
2022-07-15 10:39:48 - train: epoch 0085, iter [01400, 05004], lr: 0.064467, loss: 2.6901
2022-07-15 10:40:23 - train: epoch 0085, iter [01500, 05004], lr: 0.064452, loss: 2.6035
2022-07-15 10:40:57 - train: epoch 0085, iter [01600, 05004], lr: 0.064436, loss: 2.7800
2022-07-15 10:41:31 - train: epoch 0085, iter [01700, 05004], lr: 0.064421, loss: 2.8642
2022-07-15 10:42:05 - train: epoch 0085, iter [01800, 05004], lr: 0.064406, loss: 2.2879
2022-07-15 10:42:41 - train: epoch 0085, iter [01900, 05004], lr: 0.064390, loss: 2.3372
2022-07-15 10:43:15 - train: epoch 0085, iter [02000, 05004], lr: 0.064375, loss: 2.4429
2022-07-15 10:43:50 - train: epoch 0085, iter [02100, 05004], lr: 0.064359, loss: 2.3774
2022-07-15 10:44:25 - train: epoch 0085, iter [02200, 05004], lr: 0.064344, loss: 2.5383
2022-07-15 10:44:58 - train: epoch 0085, iter [02300, 05004], lr: 0.064328, loss: 2.4031
2022-07-15 10:45:33 - train: epoch 0085, iter [02400, 05004], lr: 0.064313, loss: 2.6391
2022-07-15 10:46:08 - train: epoch 0085, iter [02500, 05004], lr: 0.064298, loss: 2.6071
2022-07-15 10:46:43 - train: epoch 0085, iter [02600, 05004], lr: 0.064282, loss: 2.8973
2022-07-15 10:47:18 - train: epoch 0085, iter [02700, 05004], lr: 0.064267, loss: 2.5754
2022-07-15 10:47:52 - train: epoch 0085, iter [02800, 05004], lr: 0.064251, loss: 2.6945
2022-07-15 10:48:28 - train: epoch 0085, iter [02900, 05004], lr: 0.064236, loss: 2.5280
2022-07-15 10:49:03 - train: epoch 0085, iter [03000, 05004], lr: 0.064220, loss: 2.8336
2022-07-15 10:49:37 - train: epoch 0085, iter [03100, 05004], lr: 0.064205, loss: 2.5612
2022-07-15 10:50:13 - train: epoch 0085, iter [03200, 05004], lr: 0.064190, loss: 2.7419
2022-07-15 10:50:46 - train: epoch 0085, iter [03300, 05004], lr: 0.064174, loss: 2.5887
2022-07-15 10:51:22 - train: epoch 0085, iter [03400, 05004], lr: 0.064159, loss: 2.6798
2022-07-15 10:51:57 - train: epoch 0085, iter [03500, 05004], lr: 0.064143, loss: 2.7210
2022-07-15 10:52:32 - train: epoch 0085, iter [03600, 05004], lr: 0.064128, loss: 2.7390
2022-07-15 10:53:07 - train: epoch 0085, iter [03700, 05004], lr: 0.064112, loss: 2.5177
2022-07-15 10:53:42 - train: epoch 0085, iter [03800, 05004], lr: 0.064097, loss: 2.6476
2022-07-15 10:54:17 - train: epoch 0085, iter [03900, 05004], lr: 0.064081, loss: 2.8429
2022-07-15 10:54:52 - train: epoch 0085, iter [04000, 05004], lr: 0.064066, loss: 2.6908
2022-07-15 10:55:27 - train: epoch 0085, iter [04100, 05004], lr: 0.064051, loss: 2.5328
2022-07-15 10:56:02 - train: epoch 0085, iter [04200, 05004], lr: 0.064035, loss: 2.7465
2022-07-15 10:56:38 - train: epoch 0085, iter [04300, 05004], lr: 0.064020, loss: 2.6876
2022-07-15 10:57:13 - train: epoch 0085, iter [04400, 05004], lr: 0.064004, loss: 2.5576
2022-07-15 10:57:48 - train: epoch 0085, iter [04500, 05004], lr: 0.063989, loss: 3.0008
2022-07-15 10:58:22 - train: epoch 0085, iter [04600, 05004], lr: 0.063973, loss: 2.5877
2022-07-15 10:58:58 - train: epoch 0085, iter [04700, 05004], lr: 0.063958, loss: 2.8267
2022-07-15 10:59:32 - train: epoch 0085, iter [04800, 05004], lr: 0.063942, loss: 2.3787
2022-07-15 11:00:07 - train: epoch 0085, iter [04900, 05004], lr: 0.063927, loss: 2.7261
2022-07-15 11:00:40 - train: epoch 0085, iter [05000, 05004], lr: 0.063911, loss: 2.4141
2022-07-15 11:00:42 - train: epoch 085, train_loss: 2.6351
2022-07-15 11:01:57 - eval: epoch: 085, acc1: 61.092%, acc5: 84.496%, test_loss: 1.6184, per_image_load_time: 0.822ms, per_image_inference_time: 0.459ms
2022-07-15 11:01:57 - until epoch: 085, best_acc1: 62.056%
2022-07-15 11:01:57 - epoch 086 lr: 0.063911
2022-07-15 11:02:37 - train: epoch 0086, iter [00100, 05004], lr: 0.063895, loss: 2.6057
2022-07-15 11:03:12 - train: epoch 0086, iter [00200, 05004], lr: 0.063880, loss: 2.5434
2022-07-15 11:03:46 - train: epoch 0086, iter [00300, 05004], lr: 0.063864, loss: 2.6610
2022-07-15 11:04:21 - train: epoch 0086, iter [00400, 05004], lr: 0.063849, loss: 2.7905
2022-07-15 11:04:55 - train: epoch 0086, iter [00500, 05004], lr: 0.063834, loss: 2.5629
2022-07-15 11:05:29 - train: epoch 0086, iter [00600, 05004], lr: 0.063818, loss: 2.5293
2022-07-15 11:06:04 - train: epoch 0086, iter [00700, 05004], lr: 0.063803, loss: 2.6023
2022-07-15 11:06:38 - train: epoch 0086, iter [00800, 05004], lr: 0.063787, loss: 2.6501
2022-07-15 11:07:12 - train: epoch 0086, iter [00900, 05004], lr: 0.063772, loss: 2.7617
2022-07-15 11:07:46 - train: epoch 0086, iter [01000, 05004], lr: 0.063756, loss: 2.4777
2022-07-15 11:08:21 - train: epoch 0086, iter [01100, 05004], lr: 0.063741, loss: 2.6174
2022-07-15 11:08:55 - train: epoch 0086, iter [01200, 05004], lr: 0.063725, loss: 2.5545
2022-07-15 11:09:29 - train: epoch 0086, iter [01300, 05004], lr: 0.063710, loss: 2.7284
2022-07-15 11:10:05 - train: epoch 0086, iter [01400, 05004], lr: 0.063694, loss: 2.5478
2022-07-15 11:10:38 - train: epoch 0086, iter [01500, 05004], lr: 0.063679, loss: 2.4514
2022-07-15 11:11:13 - train: epoch 0086, iter [01600, 05004], lr: 0.063663, loss: 2.5334
2022-07-15 11:11:48 - train: epoch 0086, iter [01700, 05004], lr: 0.063648, loss: 2.5612
2022-07-15 11:12:22 - train: epoch 0086, iter [01800, 05004], lr: 0.063632, loss: 2.5026
2022-07-15 11:12:57 - train: epoch 0086, iter [01900, 05004], lr: 0.063617, loss: 2.5119
2022-07-15 11:13:32 - train: epoch 0086, iter [02000, 05004], lr: 0.063601, loss: 2.5471
2022-07-15 11:14:06 - train: epoch 0086, iter [02100, 05004], lr: 0.063586, loss: 2.4343
2022-07-15 11:14:41 - train: epoch 0086, iter [02200, 05004], lr: 0.063570, loss: 2.9377
2022-07-15 11:15:15 - train: epoch 0086, iter [02300, 05004], lr: 0.063555, loss: 2.7452
2022-07-15 11:15:50 - train: epoch 0086, iter [02400, 05004], lr: 0.063539, loss: 2.8597
2022-07-15 11:16:25 - train: epoch 0086, iter [02500, 05004], lr: 0.063524, loss: 2.7716
2022-07-15 11:17:00 - train: epoch 0086, iter [02600, 05004], lr: 0.063508, loss: 2.7026
2022-07-15 11:17:34 - train: epoch 0086, iter [02700, 05004], lr: 0.063493, loss: 2.6372
2022-07-15 11:18:09 - train: epoch 0086, iter [02800, 05004], lr: 0.063477, loss: 2.7578
2022-07-15 11:18:43 - train: epoch 0086, iter [02900, 05004], lr: 0.063462, loss: 2.4893
2022-07-15 11:19:18 - train: epoch 0086, iter [03000, 05004], lr: 0.063446, loss: 2.4527
2022-07-15 11:19:53 - train: epoch 0086, iter [03100, 05004], lr: 0.063431, loss: 2.7273
2022-07-15 11:20:28 - train: epoch 0086, iter [03200, 05004], lr: 0.063415, loss: 2.8822
2022-07-15 11:21:03 - train: epoch 0086, iter [03300, 05004], lr: 0.063400, loss: 2.5212
2022-07-15 11:21:38 - train: epoch 0086, iter [03400, 05004], lr: 0.063384, loss: 2.7560
2022-07-15 11:22:13 - train: epoch 0086, iter [03500, 05004], lr: 0.063369, loss: 2.7917
2022-07-15 11:22:49 - train: epoch 0086, iter [03600, 05004], lr: 0.063353, loss: 2.7236
2022-07-15 11:23:24 - train: epoch 0086, iter [03700, 05004], lr: 0.063338, loss: 2.5210
2022-07-15 11:23:58 - train: epoch 0086, iter [03800, 05004], lr: 0.063322, loss: 2.6182
2022-07-15 11:24:34 - train: epoch 0086, iter [03900, 05004], lr: 0.063307, loss: 2.9633
2022-07-15 11:25:08 - train: epoch 0086, iter [04000, 05004], lr: 0.063291, loss: 2.6099
2022-07-15 11:25:43 - train: epoch 0086, iter [04100, 05004], lr: 0.063276, loss: 2.3868
2022-07-15 11:26:18 - train: epoch 0086, iter [04200, 05004], lr: 0.063260, loss: 2.0944
2022-07-15 11:26:53 - train: epoch 0086, iter [04300, 05004], lr: 0.063245, loss: 2.6467
2022-07-15 11:27:28 - train: epoch 0086, iter [04400, 05004], lr: 0.063229, loss: 2.7948
2022-07-15 11:28:03 - train: epoch 0086, iter [04500, 05004], lr: 0.063214, loss: 2.7096
2022-07-15 11:28:38 - train: epoch 0086, iter [04600, 05004], lr: 0.063198, loss: 2.6606
2022-07-15 11:29:13 - train: epoch 0086, iter [04700, 05004], lr: 0.063183, loss: 2.5166
2022-07-15 11:29:48 - train: epoch 0086, iter [04800, 05004], lr: 0.063167, loss: 2.2908
2022-07-15 11:30:24 - train: epoch 0086, iter [04900, 05004], lr: 0.063152, loss: 2.5569
2022-07-15 11:30:57 - train: epoch 0086, iter [05000, 05004], lr: 0.063136, loss: 2.3203
2022-07-15 11:30:58 - train: epoch 086, train_loss: 2.6312
2022-07-15 11:32:14 - eval: epoch: 086, acc1: 61.564%, acc5: 84.706%, test_loss: 1.5978, per_image_load_time: 2.197ms, per_image_inference_time: 0.458ms
2022-07-15 11:32:14 - until epoch: 086, best_acc1: 62.056%
2022-07-15 11:32:14 - epoch 087 lr: 0.063135
2022-07-15 11:32:55 - train: epoch 0087, iter [00100, 05004], lr: 0.063120, loss: 2.5480
2022-07-15 11:33:29 - train: epoch 0087, iter [00200, 05004], lr: 0.063104, loss: 2.3785
2022-07-15 11:34:04 - train: epoch 0087, iter [00300, 05004], lr: 0.063089, loss: 2.6251
2022-07-15 11:34:39 - train: epoch 0087, iter [00400, 05004], lr: 0.063073, loss: 2.7023
2022-07-15 11:35:13 - train: epoch 0087, iter [00500, 05004], lr: 0.063058, loss: 2.6850
2022-07-15 11:35:48 - train: epoch 0087, iter [00600, 05004], lr: 0.063042, loss: 2.6010
2022-07-15 11:36:23 - train: epoch 0087, iter [00700, 05004], lr: 0.063027, loss: 2.6151
2022-07-15 11:36:57 - train: epoch 0087, iter [00800, 05004], lr: 0.063011, loss: 2.2683
2022-07-15 11:37:32 - train: epoch 0087, iter [00900, 05004], lr: 0.062996, loss: 2.5453
2022-07-15 11:38:07 - train: epoch 0087, iter [01000, 05004], lr: 0.062980, loss: 2.6368
2022-07-15 11:38:41 - train: epoch 0087, iter [01100, 05004], lr: 0.062964, loss: 2.5605
2022-07-15 11:39:16 - train: epoch 0087, iter [01200, 05004], lr: 0.062949, loss: 2.5328
2022-07-15 11:39:51 - train: epoch 0087, iter [01300, 05004], lr: 0.062933, loss: 2.6477
2022-07-15 11:40:26 - train: epoch 0087, iter [01400, 05004], lr: 0.062918, loss: 2.5865
2022-07-15 11:41:00 - train: epoch 0087, iter [01500, 05004], lr: 0.062902, loss: 2.6399
2022-07-15 11:41:35 - train: epoch 0087, iter [01600, 05004], lr: 0.062887, loss: 2.5184
2022-07-15 11:42:09 - train: epoch 0087, iter [01700, 05004], lr: 0.062871, loss: 2.9563
2022-07-15 11:42:43 - train: epoch 0087, iter [01800, 05004], lr: 0.062856, loss: 2.5303
2022-07-15 11:43:18 - train: epoch 0087, iter [01900, 05004], lr: 0.062840, loss: 2.5515
2022-07-15 11:43:52 - train: epoch 0087, iter [02000, 05004], lr: 0.062824, loss: 2.8582
2022-07-15 11:44:27 - train: epoch 0087, iter [02100, 05004], lr: 0.062809, loss: 2.6220
2022-07-15 11:45:02 - train: epoch 0087, iter [02200, 05004], lr: 0.062793, loss: 2.6597
2022-07-15 11:45:36 - train: epoch 0087, iter [02300, 05004], lr: 0.062778, loss: 2.7288
2022-07-15 11:46:11 - train: epoch 0087, iter [02400, 05004], lr: 0.062762, loss: 2.5360
2022-07-15 11:46:44 - train: epoch 0087, iter [02500, 05004], lr: 0.062747, loss: 2.7239
2022-07-15 11:47:18 - train: epoch 0087, iter [02600, 05004], lr: 0.062731, loss: 2.8264
2022-07-15 11:47:53 - train: epoch 0087, iter [02700, 05004], lr: 0.062716, loss: 2.4668
2022-07-15 11:48:27 - train: epoch 0087, iter [02800, 05004], lr: 0.062700, loss: 2.5287
2022-07-15 11:49:01 - train: epoch 0087, iter [02900, 05004], lr: 0.062684, loss: 2.8558
2022-07-15 11:49:36 - train: epoch 0087, iter [03000, 05004], lr: 0.062669, loss: 2.5808
2022-07-15 11:50:11 - train: epoch 0087, iter [03100, 05004], lr: 0.062653, loss: 2.5796
2022-07-15 11:50:44 - train: epoch 0087, iter [03200, 05004], lr: 0.062638, loss: 2.7069
2022-07-15 11:51:19 - train: epoch 0087, iter [03300, 05004], lr: 0.062622, loss: 2.8168
2022-07-15 11:51:53 - train: epoch 0087, iter [03400, 05004], lr: 0.062606, loss: 2.6501
2022-07-15 11:52:27 - train: epoch 0087, iter [03500, 05004], lr: 0.062591, loss: 2.6310
2022-07-15 11:53:01 - train: epoch 0087, iter [03600, 05004], lr: 0.062575, loss: 2.5732
2022-07-15 11:53:35 - train: epoch 0087, iter [03700, 05004], lr: 0.062560, loss: 2.5304
2022-07-15 11:54:09 - train: epoch 0087, iter [03800, 05004], lr: 0.062544, loss: 2.4992
2022-07-15 11:54:44 - train: epoch 0087, iter [03900, 05004], lr: 0.062529, loss: 2.6400
2022-07-15 11:55:18 - train: epoch 0087, iter [04000, 05004], lr: 0.062513, loss: 2.4924
2022-07-15 11:55:52 - train: epoch 0087, iter [04100, 05004], lr: 0.062497, loss: 2.6243
2022-07-15 11:56:27 - train: epoch 0087, iter [04200, 05004], lr: 0.062482, loss: 2.8332
2022-07-15 11:57:02 - train: epoch 0087, iter [04300, 05004], lr: 0.062466, loss: 2.9250
2022-07-15 11:57:36 - train: epoch 0087, iter [04400, 05004], lr: 0.062451, loss: 2.5235
2022-07-15 11:58:11 - train: epoch 0087, iter [04500, 05004], lr: 0.062435, loss: 2.4255
2022-07-15 11:58:45 - train: epoch 0087, iter [04600, 05004], lr: 0.062419, loss: 2.3990
2022-07-15 11:59:20 - train: epoch 0087, iter [04700, 05004], lr: 0.062404, loss: 2.8704
2022-07-15 11:59:55 - train: epoch 0087, iter [04800, 05004], lr: 0.062388, loss: 2.7640
2022-07-15 12:00:29 - train: epoch 0087, iter [04900, 05004], lr: 0.062373, loss: 2.4757
2022-07-15 12:01:02 - train: epoch 0087, iter [05000, 05004], lr: 0.062357, loss: 2.6725
2022-07-15 12:01:03 - train: epoch 087, train_loss: 2.6210
2022-07-15 12:02:19 - eval: epoch: 087, acc1: 62.046%, acc5: 84.860%, test_loss: 1.5825, per_image_load_time: 2.107ms, per_image_inference_time: 0.475ms
2022-07-15 12:02:19 - until epoch: 087, best_acc1: 62.056%
2022-07-15 12:02:19 - epoch 088 lr: 0.062356
2022-07-15 12:02:59 - train: epoch 0088, iter [00100, 05004], lr: 0.062341, loss: 2.3012
2022-07-15 12:03:34 - train: epoch 0088, iter [00200, 05004], lr: 0.062325, loss: 2.6976
2022-07-15 12:04:08 - train: epoch 0088, iter [00300, 05004], lr: 0.062310, loss: 2.8217
2022-07-15 12:04:42 - train: epoch 0088, iter [00400, 05004], lr: 0.062294, loss: 2.4428
2022-07-15 12:05:16 - train: epoch 0088, iter [00500, 05004], lr: 0.062278, loss: 2.7914
2022-07-15 12:05:50 - train: epoch 0088, iter [00600, 05004], lr: 0.062263, loss: 2.7634
2022-07-15 12:06:25 - train: epoch 0088, iter [00700, 05004], lr: 0.062247, loss: 2.3245
2022-07-15 12:06:58 - train: epoch 0088, iter [00800, 05004], lr: 0.062232, loss: 2.6776
2022-07-15 12:07:33 - train: epoch 0088, iter [00900, 05004], lr: 0.062216, loss: 2.8120
2022-07-15 12:08:07 - train: epoch 0088, iter [01000, 05004], lr: 0.062200, loss: 2.3482
2022-07-15 12:08:41 - train: epoch 0088, iter [01100, 05004], lr: 0.062185, loss: 2.5175
2022-07-15 12:09:15 - train: epoch 0088, iter [01200, 05004], lr: 0.062169, loss: 2.4887
2022-07-15 12:09:49 - train: epoch 0088, iter [01300, 05004], lr: 0.062154, loss: 2.4931
2022-07-15 12:10:24 - train: epoch 0088, iter [01400, 05004], lr: 0.062138, loss: 2.3913
2022-07-15 12:10:58 - train: epoch 0088, iter [01500, 05004], lr: 0.062122, loss: 2.5223
2022-07-15 12:11:33 - train: epoch 0088, iter [01600, 05004], lr: 0.062107, loss: 2.6573
2022-07-15 12:12:06 - train: epoch 0088, iter [01700, 05004], lr: 0.062091, loss: 2.7252
2022-07-15 12:12:40 - train: epoch 0088, iter [01800, 05004], lr: 0.062075, loss: 2.3736
2022-07-15 12:13:15 - train: epoch 0088, iter [01900, 05004], lr: 0.062060, loss: 2.5515
2022-07-15 12:13:49 - train: epoch 0088, iter [02000, 05004], lr: 0.062044, loss: 2.6438
2022-07-15 12:14:23 - train: epoch 0088, iter [02100, 05004], lr: 0.062029, loss: 2.6146
2022-07-15 12:14:57 - train: epoch 0088, iter [02200, 05004], lr: 0.062013, loss: 2.5917
2022-07-15 12:15:32 - train: epoch 0088, iter [02300, 05004], lr: 0.061997, loss: 2.5080
2022-07-15 12:16:06 - train: epoch 0088, iter [02400, 05004], lr: 0.061982, loss: 2.5136
2022-07-15 12:16:41 - train: epoch 0088, iter [02500, 05004], lr: 0.061966, loss: 2.9464
2022-07-15 12:17:15 - train: epoch 0088, iter [02600, 05004], lr: 0.061950, loss: 2.7739
2022-07-15 12:17:49 - train: epoch 0088, iter [02700, 05004], lr: 0.061935, loss: 2.7509
2022-07-15 12:18:23 - train: epoch 0088, iter [02800, 05004], lr: 0.061919, loss: 2.8014
2022-07-15 12:18:57 - train: epoch 0088, iter [02900, 05004], lr: 0.061904, loss: 2.5561
2022-07-15 12:19:32 - train: epoch 0088, iter [03000, 05004], lr: 0.061888, loss: 2.5845
2022-07-15 12:20:06 - train: epoch 0088, iter [03100, 05004], lr: 0.061872, loss: 2.5686
2022-07-15 12:20:40 - train: epoch 0088, iter [03200, 05004], lr: 0.061857, loss: 2.3312
2022-07-15 12:21:15 - train: epoch 0088, iter [03300, 05004], lr: 0.061841, loss: 2.4109
2022-07-15 12:21:49 - train: epoch 0088, iter [03400, 05004], lr: 0.061825, loss: 2.5712
2022-07-15 12:22:23 - train: epoch 0088, iter [03500, 05004], lr: 0.061810, loss: 2.4748
2022-07-15 12:22:57 - train: epoch 0088, iter [03600, 05004], lr: 0.061794, loss: 2.8492
2022-07-15 12:23:31 - train: epoch 0088, iter [03700, 05004], lr: 0.061778, loss: 2.7083
2022-07-15 12:24:06 - train: epoch 0088, iter [03800, 05004], lr: 0.061763, loss: 2.5828
2022-07-15 12:24:39 - train: epoch 0088, iter [03900, 05004], lr: 0.061747, loss: 2.6037
2022-07-15 12:25:14 - train: epoch 0088, iter [04000, 05004], lr: 0.061732, loss: 2.4526
2022-07-15 12:25:48 - train: epoch 0088, iter [04100, 05004], lr: 0.061716, loss: 2.3682
2022-07-15 12:26:23 - train: epoch 0088, iter [04200, 05004], lr: 0.061700, loss: 2.6777
2022-07-15 12:26:57 - train: epoch 0088, iter [04300, 05004], lr: 0.061685, loss: 2.5621
2022-07-15 12:27:32 - train: epoch 0088, iter [04400, 05004], lr: 0.061669, loss: 2.5970
2022-07-15 12:28:06 - train: epoch 0088, iter [04500, 05004], lr: 0.061653, loss: 2.6295
2022-07-15 12:28:39 - train: epoch 0088, iter [04600, 05004], lr: 0.061638, loss: 3.0978
2022-07-15 12:29:13 - train: epoch 0088, iter [04700, 05004], lr: 0.061622, loss: 2.5260
2022-07-15 12:29:47 - train: epoch 0088, iter [04800, 05004], lr: 0.061606, loss: 2.7880
2022-07-15 12:30:22 - train: epoch 0088, iter [04900, 05004], lr: 0.061591, loss: 2.7244
2022-07-15 12:30:55 - train: epoch 0088, iter [05000, 05004], lr: 0.061575, loss: 2.6373
2022-07-15 12:30:56 - train: epoch 088, train_loss: 2.6180
2022-07-15 12:32:11 - eval: epoch: 088, acc1: 58.208%, acc5: 82.160%, test_loss: 1.7645, per_image_load_time: 0.814ms, per_image_inference_time: 0.477ms
2022-07-15 12:32:12 - until epoch: 088, best_acc1: 62.056%
2022-07-15 12:32:12 - epoch 089 lr: 0.061574
2022-07-15 12:32:52 - train: epoch 0089, iter [00100, 05004], lr: 0.061559, loss: 2.7595
2022-07-15 12:33:27 - train: epoch 0089, iter [00200, 05004], lr: 0.061543, loss: 2.4725
2022-07-15 12:34:02 - train: epoch 0089, iter [00300, 05004], lr: 0.061527, loss: 2.6365
2022-07-15 12:34:36 - train: epoch 0089, iter [00400, 05004], lr: 0.061512, loss: 2.4830
2022-07-15 12:35:10 - train: epoch 0089, iter [00500, 05004], lr: 0.061496, loss: 2.6038
2022-07-15 12:35:45 - train: epoch 0089, iter [00600, 05004], lr: 0.061480, loss: 2.5939
2022-07-15 12:36:19 - train: epoch 0089, iter [00700, 05004], lr: 0.061465, loss: 2.8831
2022-07-15 12:36:54 - train: epoch 0089, iter [00800, 05004], lr: 0.061449, loss: 2.5750
2022-07-15 12:37:29 - train: epoch 0089, iter [00900, 05004], lr: 0.061433, loss: 2.5118
2022-07-15 12:38:02 - train: epoch 0089, iter [01000, 05004], lr: 0.061418, loss: 2.8919
2022-07-15 12:38:37 - train: epoch 0089, iter [01100, 05004], lr: 0.061402, loss: 2.4522
2022-07-15 12:39:11 - train: epoch 0089, iter [01200, 05004], lr: 0.061386, loss: 2.8858
2022-07-15 12:39:46 - train: epoch 0089, iter [01300, 05004], lr: 0.061371, loss: 2.5691
2022-07-15 12:40:20 - train: epoch 0089, iter [01400, 05004], lr: 0.061355, loss: 2.5720
2022-07-15 12:40:54 - train: epoch 0089, iter [01500, 05004], lr: 0.061339, loss: 2.8487
2022-07-15 12:41:29 - train: epoch 0089, iter [01600, 05004], lr: 0.061324, loss: 2.3857
2022-07-15 12:42:04 - train: epoch 0089, iter [01700, 05004], lr: 0.061308, loss: 2.8697
2022-07-15 12:42:38 - train: epoch 0089, iter [01800, 05004], lr: 0.061292, loss: 2.7038
2022-07-15 12:43:13 - train: epoch 0089, iter [01900, 05004], lr: 0.061277, loss: 2.5615
2022-07-15 12:43:47 - train: epoch 0089, iter [02000, 05004], lr: 0.061261, loss: 2.3175
2022-07-15 12:44:23 - train: epoch 0089, iter [02100, 05004], lr: 0.061245, loss: 2.6340
2022-07-15 12:44:57 - train: epoch 0089, iter [02200, 05004], lr: 0.061230, loss: 2.6656
2022-07-15 12:45:32 - train: epoch 0089, iter [02300, 05004], lr: 0.061214, loss: 2.5732
2022-07-15 12:46:07 - train: epoch 0089, iter [02400, 05004], lr: 0.061198, loss: 2.6338
2022-07-15 12:46:42 - train: epoch 0089, iter [02500, 05004], lr: 0.061182, loss: 2.5065
2022-07-15 12:47:16 - train: epoch 0089, iter [02600, 05004], lr: 0.061167, loss: 2.6097
2022-07-15 12:47:51 - train: epoch 0089, iter [02700, 05004], lr: 0.061151, loss: 2.6654
2022-07-15 12:48:25 - train: epoch 0089, iter [02800, 05004], lr: 0.061135, loss: 2.5525
2022-07-15 12:49:00 - train: epoch 0089, iter [02900, 05004], lr: 0.061120, loss: 2.8193
2022-07-15 12:49:34 - train: epoch 0089, iter [03000, 05004], lr: 0.061104, loss: 2.5707
2022-07-15 12:50:09 - train: epoch 0089, iter [03100, 05004], lr: 0.061088, loss: 2.7630
2022-07-15 12:50:43 - train: epoch 0089, iter [03200, 05004], lr: 0.061073, loss: 2.7548
2022-07-15 12:51:19 - train: epoch 0089, iter [03300, 05004], lr: 0.061057, loss: 2.7318
2022-07-15 12:51:53 - train: epoch 0089, iter [03400, 05004], lr: 0.061041, loss: 2.5359
2022-07-15 12:52:27 - train: epoch 0089, iter [03500, 05004], lr: 0.061025, loss: 2.4580
2022-07-15 12:53:02 - train: epoch 0089, iter [03600, 05004], lr: 0.061010, loss: 2.5453
2022-07-15 12:53:37 - train: epoch 0089, iter [03700, 05004], lr: 0.060994, loss: 2.7171
2022-07-15 12:54:11 - train: epoch 0089, iter [03800, 05004], lr: 0.060978, loss: 2.6811
2022-07-15 12:54:46 - train: epoch 0089, iter [03900, 05004], lr: 0.060963, loss: 2.8200
2022-07-15 12:55:20 - train: epoch 0089, iter [04000, 05004], lr: 0.060947, loss: 2.6322
2022-07-15 12:55:55 - train: epoch 0089, iter [04100, 05004], lr: 0.060931, loss: 2.8910
2022-07-15 12:56:30 - train: epoch 0089, iter [04200, 05004], lr: 0.060916, loss: 2.8388
2022-07-15 12:57:05 - train: epoch 0089, iter [04300, 05004], lr: 0.060900, loss: 2.9123
2022-07-15 12:57:39 - train: epoch 0089, iter [04400, 05004], lr: 0.060884, loss: 2.8797
2022-07-15 12:58:13 - train: epoch 0089, iter [04500, 05004], lr: 0.060868, loss: 2.3539
2022-07-15 12:58:48 - train: epoch 0089, iter [04600, 05004], lr: 0.060853, loss: 2.4524
2022-07-15 12:59:23 - train: epoch 0089, iter [04700, 05004], lr: 0.060837, loss: 2.7173
2022-07-15 12:59:57 - train: epoch 0089, iter [04800, 05004], lr: 0.060821, loss: 2.4625
2022-07-15 13:00:32 - train: epoch 0089, iter [04900, 05004], lr: 0.060806, loss: 2.9792
2022-07-15 13:01:05 - train: epoch 0089, iter [05000, 05004], lr: 0.060790, loss: 2.5126
2022-07-15 13:01:06 - train: epoch 089, train_loss: 2.6123
2022-07-15 13:02:23 - eval: epoch: 089, acc1: 61.848%, acc5: 84.876%, test_loss: 1.5733, per_image_load_time: 2.495ms, per_image_inference_time: 0.471ms
2022-07-15 13:02:23 - until epoch: 089, best_acc1: 62.056%
2022-07-15 13:02:23 - epoch 090 lr: 0.060789
2022-07-15 13:03:02 - train: epoch 0090, iter [00100, 05004], lr: 0.060773, loss: 2.5271
2022-07-15 13:03:37 - train: epoch 0090, iter [00200, 05004], lr: 0.060758, loss: 2.3158
2022-07-15 13:04:11 - train: epoch 0090, iter [00300, 05004], lr: 0.060742, loss: 2.4071
2022-07-15 13:04:45 - train: epoch 0090, iter [00400, 05004], lr: 0.060726, loss: 2.6192
2022-07-15 13:05:20 - train: epoch 0090, iter [00500, 05004], lr: 0.060711, loss: 2.3176
2022-07-15 13:05:55 - train: epoch 0090, iter [00600, 05004], lr: 0.060695, loss: 2.8950
2022-07-15 13:06:29 - train: epoch 0090, iter [00700, 05004], lr: 0.060679, loss: 2.6286
2022-07-15 13:07:04 - train: epoch 0090, iter [00800, 05004], lr: 0.060663, loss: 2.5347
2022-07-15 13:07:40 - train: epoch 0090, iter [00900, 05004], lr: 0.060648, loss: 2.6258
2022-07-15 13:08:13 - train: epoch 0090, iter [01000, 05004], lr: 0.060632, loss: 2.5782
2022-07-15 13:08:47 - train: epoch 0090, iter [01100, 05004], lr: 0.060616, loss: 2.8759
2022-07-15 13:09:23 - train: epoch 0090, iter [01200, 05004], lr: 0.060601, loss: 2.4475
2022-07-15 13:09:57 - train: epoch 0090, iter [01300, 05004], lr: 0.060585, loss: 2.4835
2022-07-15 13:10:31 - train: epoch 0090, iter [01400, 05004], lr: 0.060569, loss: 2.5112
2022-07-15 13:11:06 - train: epoch 0090, iter [01500, 05004], lr: 0.060553, loss: 2.6905
2022-07-15 13:11:41 - train: epoch 0090, iter [01600, 05004], lr: 0.060538, loss: 2.5063
2022-07-15 13:12:15 - train: epoch 0090, iter [01700, 05004], lr: 0.060522, loss: 2.4120
2022-07-15 13:12:50 - train: epoch 0090, iter [01800, 05004], lr: 0.060506, loss: 2.6938
2022-07-15 13:13:23 - train: epoch 0090, iter [01900, 05004], lr: 0.060490, loss: 2.5018
2022-07-15 13:13:58 - train: epoch 0090, iter [02000, 05004], lr: 0.060475, loss: 2.7839
2022-07-15 13:14:32 - train: epoch 0090, iter [02100, 05004], lr: 0.060459, loss: 2.7450
2022-07-15 13:15:07 - train: epoch 0090, iter [02200, 05004], lr: 0.060443, loss: 2.5778
2022-07-15 13:15:41 - train: epoch 0090, iter [02300, 05004], lr: 0.060427, loss: 2.7799
2022-07-15 13:16:17 - train: epoch 0090, iter [02400, 05004], lr: 0.060412, loss: 2.6841
2022-07-15 13:16:52 - train: epoch 0090, iter [02500, 05004], lr: 0.060396, loss: 2.8364
2022-07-15 13:17:26 - train: epoch 0090, iter [02600, 05004], lr: 0.060380, loss: 2.5434
2022-07-15 13:17:59 - train: epoch 0090, iter [02700, 05004], lr: 0.060364, loss: 2.5470
2022-07-15 13:18:35 - train: epoch 0090, iter [02800, 05004], lr: 0.060349, loss: 2.9867
2022-07-15 13:19:09 - train: epoch 0090, iter [02900, 05004], lr: 0.060333, loss: 2.5789
2022-07-15 13:19:44 - train: epoch 0090, iter [03000, 05004], lr: 0.060317, loss: 2.5927
2022-07-15 13:20:18 - train: epoch 0090, iter [03100, 05004], lr: 0.060301, loss: 2.3980
2022-07-15 13:20:54 - train: epoch 0090, iter [03200, 05004], lr: 0.060286, loss: 2.4034
2022-07-15 13:21:28 - train: epoch 0090, iter [03300, 05004], lr: 0.060270, loss: 2.9399
2022-07-15 13:22:02 - train: epoch 0090, iter [03400, 05004], lr: 0.060254, loss: 2.5583
2022-07-15 13:22:36 - train: epoch 0090, iter [03500, 05004], lr: 0.060238, loss: 2.5238
2022-07-15 13:23:11 - train: epoch 0090, iter [03600, 05004], lr: 0.060223, loss: 2.7039
2022-07-15 13:23:45 - train: epoch 0090, iter [03700, 05004], lr: 0.060207, loss: 2.6952
2022-07-15 13:24:20 - train: epoch 0090, iter [03800, 05004], lr: 0.060191, loss: 2.6974
2022-07-15 13:24:54 - train: epoch 0090, iter [03900, 05004], lr: 0.060175, loss: 2.2489
2022-07-15 13:25:29 - train: epoch 0090, iter [04000, 05004], lr: 0.060160, loss: 2.6309
2022-07-15 13:26:04 - train: epoch 0090, iter [04100, 05004], lr: 0.060144, loss: 2.7335
2022-07-15 13:26:39 - train: epoch 0090, iter [04200, 05004], lr: 0.060128, loss: 3.0488
2022-07-15 13:27:13 - train: epoch 0090, iter [04300, 05004], lr: 0.060112, loss: 2.5981
2022-07-15 13:27:48 - train: epoch 0090, iter [04400, 05004], lr: 0.060097, loss: 2.6898
2022-07-15 13:28:23 - train: epoch 0090, iter [04500, 05004], lr: 0.060081, loss: 2.1215
2022-07-15 13:28:58 - train: epoch 0090, iter [04600, 05004], lr: 0.060065, loss: 2.5797
2022-07-15 13:29:33 - train: epoch 0090, iter [04700, 05004], lr: 0.060049, loss: 2.4566
2022-07-15 13:30:07 - train: epoch 0090, iter [04800, 05004], lr: 0.060033, loss: 2.8667
2022-07-15 13:30:43 - train: epoch 0090, iter [04900, 05004], lr: 0.060018, loss: 2.5537
2022-07-15 13:31:15 - train: epoch 0090, iter [05000, 05004], lr: 0.060002, loss: 2.7374
2022-07-15 13:31:16 - train: epoch 090, train_loss: 2.6028
2022-07-15 13:32:32 - eval: epoch: 090, acc1: 61.044%, acc5: 83.944%, test_loss: 1.6302, per_image_load_time: 2.314ms, per_image_inference_time: 0.464ms
2022-07-15 13:32:32 - until epoch: 090, best_acc1: 62.056%
2022-07-15 13:32:32 - epoch 091 lr: 0.060001
2022-07-15 13:33:12 - train: epoch 0091, iter [00100, 05004], lr: 0.059986, loss: 2.5549
2022-07-15 13:33:47 - train: epoch 0091, iter [00200, 05004], lr: 0.059970, loss: 2.5881
2022-07-15 13:34:22 - train: epoch 0091, iter [00300, 05004], lr: 0.059954, loss: 2.4985
2022-07-15 13:34:56 - train: epoch 0091, iter [00400, 05004], lr: 0.059938, loss: 2.6349
2022-07-15 13:35:30 - train: epoch 0091, iter [00500, 05004], lr: 0.059922, loss: 2.5275
2022-07-15 13:36:04 - train: epoch 0091, iter [00600, 05004], lr: 0.059907, loss: 2.5269
2022-07-15 13:36:39 - train: epoch 0091, iter [00700, 05004], lr: 0.059891, loss: 2.6173
2022-07-15 13:37:13 - train: epoch 0091, iter [00800, 05004], lr: 0.059875, loss: 2.1621
2022-07-15 13:37:47 - train: epoch 0091, iter [00900, 05004], lr: 0.059859, loss: 2.6076
2022-07-15 13:38:22 - train: epoch 0091, iter [01000, 05004], lr: 0.059844, loss: 2.4683
2022-07-15 13:38:56 - train: epoch 0091, iter [01100, 05004], lr: 0.059828, loss: 2.3266
2022-07-15 13:39:30 - train: epoch 0091, iter [01200, 05004], lr: 0.059812, loss: 2.6329
2022-07-15 13:40:04 - train: epoch 0091, iter [01300, 05004], lr: 0.059796, loss: 2.3802
2022-07-15 13:40:38 - train: epoch 0091, iter [01400, 05004], lr: 0.059780, loss: 2.5465
2022-07-15 13:41:12 - train: epoch 0091, iter [01500, 05004], lr: 0.059765, loss: 2.7663
2022-07-15 13:41:45 - train: epoch 0091, iter [01600, 05004], lr: 0.059749, loss: 2.4410
2022-07-15 13:42:19 - train: epoch 0091, iter [01700, 05004], lr: 0.059733, loss: 2.6822
2022-07-15 13:42:54 - train: epoch 0091, iter [01800, 05004], lr: 0.059717, loss: 2.4946
2022-07-15 13:43:27 - train: epoch 0091, iter [01900, 05004], lr: 0.059701, loss: 2.4073
2022-07-15 13:44:01 - train: epoch 0091, iter [02000, 05004], lr: 0.059686, loss: 2.5504
2022-07-15 13:44:35 - train: epoch 0091, iter [02100, 05004], lr: 0.059670, loss: 2.4988
2022-07-15 13:45:09 - train: epoch 0091, iter [02200, 05004], lr: 0.059654, loss: 2.6977
2022-07-15 13:45:41 - train: epoch 0091, iter [02300, 05004], lr: 0.059638, loss: 2.3577
2022-07-15 13:46:15 - train: epoch 0091, iter [02400, 05004], lr: 0.059622, loss: 2.5592
2022-07-15 13:46:50 - train: epoch 0091, iter [02500, 05004], lr: 0.059607, loss: 2.5766
2022-07-15 13:47:23 - train: epoch 0091, iter [02600, 05004], lr: 0.059591, loss: 2.5714
2022-07-15 13:47:58 - train: epoch 0091, iter [02700, 05004], lr: 0.059575, loss: 2.6829
2022-07-15 13:48:31 - train: epoch 0091, iter [02800, 05004], lr: 0.059559, loss: 2.6744
2022-07-15 13:49:05 - train: epoch 0091, iter [02900, 05004], lr: 0.059543, loss: 2.6769
2022-07-15 13:49:38 - train: epoch 0091, iter [03000, 05004], lr: 0.059528, loss: 2.9858
2022-07-15 13:50:12 - train: epoch 0091, iter [03100, 05004], lr: 0.059512, loss: 2.5178
2022-07-15 13:50:46 - train: epoch 0091, iter [03200, 05004], lr: 0.059496, loss: 3.2499
2022-07-15 13:51:20 - train: epoch 0091, iter [03300, 05004], lr: 0.059480, loss: 2.7572
2022-07-15 13:51:54 - train: epoch 0091, iter [03400, 05004], lr: 0.059464, loss: 2.5701
2022-07-15 13:52:28 - train: epoch 0091, iter [03500, 05004], lr: 0.059449, loss: 2.5892
2022-07-15 13:53:02 - train: epoch 0091, iter [03600, 05004], lr: 0.059433, loss: 2.6371
2022-07-15 13:53:36 - train: epoch 0091, iter [03700, 05004], lr: 0.059417, loss: 2.6379
2022-07-15 13:54:10 - train: epoch 0091, iter [03800, 05004], lr: 0.059401, loss: 2.9761
2022-07-15 13:54:44 - train: epoch 0091, iter [03900, 05004], lr: 0.059385, loss: 2.3362
2022-07-15 13:55:18 - train: epoch 0091, iter [04000, 05004], lr: 0.059370, loss: 2.6048
2022-07-15 13:55:52 - train: epoch 0091, iter [04100, 05004], lr: 0.059354, loss: 2.5121
2022-07-15 13:56:25 - train: epoch 0091, iter [04200, 05004], lr: 0.059338, loss: 2.9760
2022-07-15 13:57:00 - train: epoch 0091, iter [04300, 05004], lr: 0.059322, loss: 2.6877
2022-07-15 13:57:33 - train: epoch 0091, iter [04400, 05004], lr: 0.059306, loss: 2.5136
2022-07-15 13:58:07 - train: epoch 0091, iter [04500, 05004], lr: 0.059290, loss: 2.4879
2022-07-15 13:58:41 - train: epoch 0091, iter [04600, 05004], lr: 0.059275, loss: 2.6070
2022-07-15 13:59:14 - train: epoch 0091, iter [04700, 05004], lr: 0.059259, loss: 2.7002
2022-07-15 13:59:48 - train: epoch 0091, iter [04800, 05004], lr: 0.059243, loss: 2.6466
2022-07-15 14:00:22 - train: epoch 0091, iter [04900, 05004], lr: 0.059227, loss: 2.5696
2022-07-15 14:00:55 - train: epoch 0091, iter [05000, 05004], lr: 0.059211, loss: 2.9290
2022-07-15 14:00:56 - train: epoch 091, train_loss: 2.6000
2022-07-15 14:02:11 - eval: epoch: 091, acc1: 60.704%, acc5: 84.012%, test_loss: 1.6510, per_image_load_time: 2.194ms, per_image_inference_time: 0.454ms
2022-07-15 14:02:11 - until epoch: 091, best_acc1: 62.056%
2022-07-15 14:02:11 - epoch 092 lr: 0.059211
2022-07-15 14:02:51 - train: epoch 0092, iter [00100, 05004], lr: 0.059195, loss: 2.6318
2022-07-15 14:03:26 - train: epoch 0092, iter [00200, 05004], lr: 0.059179, loss: 2.6421
2022-07-15 14:04:00 - train: epoch 0092, iter [00300, 05004], lr: 0.059163, loss: 2.8031
2022-07-15 14:04:33 - train: epoch 0092, iter [00400, 05004], lr: 0.059147, loss: 2.4659
2022-07-15 14:05:06 - train: epoch 0092, iter [00500, 05004], lr: 0.059132, loss: 2.6975
2022-07-15 14:05:42 - train: epoch 0092, iter [00600, 05004], lr: 0.059116, loss: 2.8407
2022-07-15 14:06:15 - train: epoch 0092, iter [00700, 05004], lr: 0.059100, loss: 2.5869
2022-07-15 14:06:50 - train: epoch 0092, iter [00800, 05004], lr: 0.059084, loss: 2.6242
2022-07-15 14:07:22 - train: epoch 0092, iter [00900, 05004], lr: 0.059068, loss: 2.8802
2022-07-15 14:07:55 - train: epoch 0092, iter [01000, 05004], lr: 0.059052, loss: 2.5580
2022-07-15 14:08:29 - train: epoch 0092, iter [01100, 05004], lr: 0.059037, loss: 2.4294
2022-07-15 14:09:02 - train: epoch 0092, iter [01200, 05004], lr: 0.059021, loss: 2.3310
2022-07-15 14:09:36 - train: epoch 0092, iter [01300, 05004], lr: 0.059005, loss: 2.6761
2022-07-15 14:10:10 - train: epoch 0092, iter [01400, 05004], lr: 0.058989, loss: 2.6313
2022-07-15 14:10:43 - train: epoch 0092, iter [01500, 05004], lr: 0.058973, loss: 2.4466
2022-07-15 14:11:17 - train: epoch 0092, iter [01600, 05004], lr: 0.058957, loss: 2.4951
2022-07-15 14:11:50 - train: epoch 0092, iter [01700, 05004], lr: 0.058942, loss: 2.6504
2022-07-15 14:12:24 - train: epoch 0092, iter [01800, 05004], lr: 0.058926, loss: 2.8670
2022-07-15 14:12:58 - train: epoch 0092, iter [01900, 05004], lr: 0.058910, loss: 2.4865
2022-07-15 14:13:33 - train: epoch 0092, iter [02000, 05004], lr: 0.058894, loss: 2.5776
2022-07-15 14:14:05 - train: epoch 0092, iter [02100, 05004], lr: 0.058878, loss: 2.5237
2022-07-15 14:14:38 - train: epoch 0092, iter [02200, 05004], lr: 0.058862, loss: 2.5943
2022-07-15 14:15:12 - train: epoch 0092, iter [02300, 05004], lr: 0.058847, loss: 2.9100
2022-07-15 14:15:46 - train: epoch 0092, iter [02400, 05004], lr: 0.058831, loss: 2.6779
2022-07-15 14:16:20 - train: epoch 0092, iter [02500, 05004], lr: 0.058815, loss: 2.7512
2022-07-15 14:16:53 - train: epoch 0092, iter [02600, 05004], lr: 0.058799, loss: 2.4807
2022-07-15 14:17:27 - train: epoch 0092, iter [02700, 05004], lr: 0.058783, loss: 2.6414
2022-07-15 14:18:01 - train: epoch 0092, iter [02800, 05004], lr: 0.058767, loss: 2.7651
2022-07-15 14:18:34 - train: epoch 0092, iter [02900, 05004], lr: 0.058752, loss: 2.8330
2022-07-15 14:19:08 - train: epoch 0092, iter [03000, 05004], lr: 0.058736, loss: 2.6819
2022-07-15 14:19:42 - train: epoch 0092, iter [03100, 05004], lr: 0.058720, loss: 2.8168
2022-07-15 14:20:15 - train: epoch 0092, iter [03200, 05004], lr: 0.058704, loss: 2.4831
2022-07-15 14:20:49 - train: epoch 0092, iter [03300, 05004], lr: 0.058688, loss: 2.2891
2022-07-15 14:21:23 - train: epoch 0092, iter [03400, 05004], lr: 0.058672, loss: 2.8272
2022-07-15 14:21:56 - train: epoch 0092, iter [03500, 05004], lr: 0.058656, loss: 2.6160
2022-07-15 14:22:29 - train: epoch 0092, iter [03600, 05004], lr: 0.058641, loss: 2.5298
2022-07-15 14:23:04 - train: epoch 0092, iter [03700, 05004], lr: 0.058625, loss: 2.5127
2022-07-15 14:23:38 - train: epoch 0092, iter [03800, 05004], lr: 0.058609, loss: 2.7278
2022-07-15 14:24:12 - train: epoch 0092, iter [03900, 05004], lr: 0.058593, loss: 2.7251
2022-07-15 14:24:46 - train: epoch 0092, iter [04000, 05004], lr: 0.058577, loss: 2.5651
2022-07-15 14:25:20 - train: epoch 0092, iter [04100, 05004], lr: 0.058561, loss: 2.4372
2022-07-15 14:25:54 - train: epoch 0092, iter [04200, 05004], lr: 0.058545, loss: 2.6726
2022-07-15 14:26:28 - train: epoch 0092, iter [04300, 05004], lr: 0.058530, loss: 2.4540
2022-07-15 14:27:01 - train: epoch 0092, iter [04400, 05004], lr: 0.058514, loss: 2.6440
2022-07-15 14:27:35 - train: epoch 0092, iter [04500, 05004], lr: 0.058498, loss: 2.7919
2022-07-15 14:28:09 - train: epoch 0092, iter [04600, 05004], lr: 0.058482, loss: 2.7783
2022-07-15 14:28:43 - train: epoch 0092, iter [04700, 05004], lr: 0.058466, loss: 2.4027
2022-07-15 14:29:18 - train: epoch 0092, iter [04800, 05004], lr: 0.058450, loss: 2.6973
2022-07-15 14:29:51 - train: epoch 0092, iter [04900, 05004], lr: 0.058434, loss: 2.7275
2022-07-15 14:30:24 - train: epoch 0092, iter [05000, 05004], lr: 0.058418, loss: 2.4502
2022-07-15 14:30:25 - train: epoch 092, train_loss: 2.5950
2022-07-15 14:31:40 - eval: epoch: 092, acc1: 62.646%, acc5: 85.226%, test_loss: 1.5523, per_image_load_time: 2.404ms, per_image_inference_time: 0.490ms
2022-07-15 14:31:41 - until epoch: 092, best_acc1: 62.646%
2022-07-15 14:31:41 - epoch 093 lr: 0.058418
2022-07-15 14:32:20 - train: epoch 0093, iter [00100, 05004], lr: 0.058402, loss: 2.5529
2022-07-15 14:32:54 - train: epoch 0093, iter [00200, 05004], lr: 0.058386, loss: 2.3511
2022-07-15 14:33:28 - train: epoch 0093, iter [00300, 05004], lr: 0.058370, loss: 2.4368
2022-07-15 14:34:03 - train: epoch 0093, iter [00400, 05004], lr: 0.058354, loss: 2.6176
2022-07-15 14:34:37 - train: epoch 0093, iter [00500, 05004], lr: 0.058339, loss: 2.6269
2022-07-15 14:35:10 - train: epoch 0093, iter [00600, 05004], lr: 0.058323, loss: 2.6296
2022-07-15 14:35:46 - train: epoch 0093, iter [00700, 05004], lr: 0.058307, loss: 2.4222
2022-07-15 14:36:19 - train: epoch 0093, iter [00800, 05004], lr: 0.058291, loss: 2.4416
2022-07-15 14:36:53 - train: epoch 0093, iter [00900, 05004], lr: 0.058275, loss: 2.6896
2022-07-15 14:37:28 - train: epoch 0093, iter [01000, 05004], lr: 0.058259, loss: 2.4297
2022-07-15 14:38:02 - train: epoch 0093, iter [01100, 05004], lr: 0.058243, loss: 2.5467
2022-07-15 14:38:36 - train: epoch 0093, iter [01200, 05004], lr: 0.058227, loss: 2.4225
2022-07-15 14:39:11 - train: epoch 0093, iter [01300, 05004], lr: 0.058211, loss: 2.5851
2022-07-15 14:39:45 - train: epoch 0093, iter [01400, 05004], lr: 0.058196, loss: 2.6202
2022-07-15 14:40:20 - train: epoch 0093, iter [01500, 05004], lr: 0.058180, loss: 2.7365
2022-07-15 14:40:54 - train: epoch 0093, iter [01600, 05004], lr: 0.058164, loss: 2.5177
2022-07-15 14:41:28 - train: epoch 0093, iter [01700, 05004], lr: 0.058148, loss: 2.6937
2022-07-15 14:42:03 - train: epoch 0093, iter [01800, 05004], lr: 0.058132, loss: 2.5950
2022-07-15 14:42:37 - train: epoch 0093, iter [01900, 05004], lr: 0.058116, loss: 2.3622
2022-07-15 14:43:11 - train: epoch 0093, iter [02000, 05004], lr: 0.058100, loss: 2.5133
2022-07-15 14:43:45 - train: epoch 0093, iter [02100, 05004], lr: 0.058084, loss: 2.4295
2022-07-15 14:44:20 - train: epoch 0093, iter [02200, 05004], lr: 0.058069, loss: 2.8110
2022-07-15 14:44:54 - train: epoch 0093, iter [02300, 05004], lr: 0.058053, loss: 2.6555
2022-07-15 14:45:29 - train: epoch 0093, iter [02400, 05004], lr: 0.058037, loss: 2.6706
2022-07-15 14:46:03 - train: epoch 0093, iter [02500, 05004], lr: 0.058021, loss: 2.7326
2022-07-15 14:46:37 - train: epoch 0093, iter [02600, 05004], lr: 0.058005, loss: 2.9366
2022-07-15 14:47:12 - train: epoch 0093, iter [02700, 05004], lr: 0.057989, loss: 2.4914
2022-07-15 14:47:46 - train: epoch 0093, iter [02800, 05004], lr: 0.057973, loss: 2.6295
2022-07-15 14:48:20 - train: epoch 0093, iter [02900, 05004], lr: 0.057957, loss: 2.5668
2022-07-15 14:48:54 - train: epoch 0093, iter [03000, 05004], lr: 0.057941, loss: 2.4423
2022-07-15 14:49:29 - train: epoch 0093, iter [03100, 05004], lr: 0.057926, loss: 2.5867
2022-07-15 14:50:03 - train: epoch 0093, iter [03200, 05004], lr: 0.057910, loss: 2.5818
2022-07-15 14:50:37 - train: epoch 0093, iter [03300, 05004], lr: 0.057894, loss: 2.4684
2022-07-15 14:51:11 - train: epoch 0093, iter [03400, 05004], lr: 0.057878, loss: 2.6600
2022-07-15 14:51:45 - train: epoch 0093, iter [03500, 05004], lr: 0.057862, loss: 2.2384
2022-07-15 14:52:19 - train: epoch 0093, iter [03600, 05004], lr: 0.057846, loss: 2.6417
2022-07-15 14:52:54 - train: epoch 0093, iter [03700, 05004], lr: 0.057830, loss: 2.5111
2022-07-15 14:53:28 - train: epoch 0093, iter [03800, 05004], lr: 0.057814, loss: 2.3961
2022-07-15 14:54:02 - train: epoch 0093, iter [03900, 05004], lr: 0.057798, loss: 2.5546
2022-07-15 14:54:36 - train: epoch 0093, iter [04000, 05004], lr: 0.057782, loss: 2.8090
2022-07-15 14:55:10 - train: epoch 0093, iter [04100, 05004], lr: 0.057767, loss: 2.7956
2022-07-15 14:55:44 - train: epoch 0093, iter [04200, 05004], lr: 0.057751, loss: 2.5449
2022-07-15 14:56:18 - train: epoch 0093, iter [04300, 05004], lr: 0.057735, loss: 2.6205
2022-07-15 14:56:52 - train: epoch 0093, iter [04400, 05004], lr: 0.057719, loss: 2.4446
2022-07-15 14:57:26 - train: epoch 0093, iter [04500, 05004], lr: 0.057703, loss: 2.4114
2022-07-15 14:58:00 - train: epoch 0093, iter [04600, 05004], lr: 0.057687, loss: 2.6505
2022-07-15 14:58:34 - train: epoch 0093, iter [04700, 05004], lr: 0.057671, loss: 3.0614
2022-07-15 14:59:08 - train: epoch 0093, iter [04800, 05004], lr: 0.057655, loss: 2.5696
2022-07-15 14:59:41 - train: epoch 0093, iter [04900, 05004], lr: 0.057639, loss: 2.5646
2022-07-15 15:00:14 - train: epoch 0093, iter [05000, 05004], lr: 0.057623, loss: 2.2427
2022-07-15 15:00:15 - train: epoch 093, train_loss: 2.5897
2022-07-15 15:01:29 - eval: epoch: 093, acc1: 62.202%, acc5: 85.062%, test_loss: 1.5685, per_image_load_time: 2.231ms, per_image_inference_time: 0.477ms
2022-07-15 15:01:30 - until epoch: 093, best_acc1: 62.646%
2022-07-15 15:01:30 - epoch 094 lr: 0.057623
2022-07-15 15:02:09 - train: epoch 0094, iter [00100, 05004], lr: 0.057607, loss: 2.4378
2022-07-15 15:02:43 - train: epoch 0094, iter [00200, 05004], lr: 0.057591, loss: 2.4178
2022-07-15 15:03:16 - train: epoch 0094, iter [00300, 05004], lr: 0.057575, loss: 2.6057
2022-07-15 15:03:50 - train: epoch 0094, iter [00400, 05004], lr: 0.057559, loss: 2.3415
2022-07-15 15:04:25 - train: epoch 0094, iter [00500, 05004], lr: 0.057543, loss: 2.3665
2022-07-15 15:04:59 - train: epoch 0094, iter [00600, 05004], lr: 0.057527, loss: 2.6903
2022-07-15 15:05:33 - train: epoch 0094, iter [00700, 05004], lr: 0.057511, loss: 2.8609
2022-07-15 15:06:06 - train: epoch 0094, iter [00800, 05004], lr: 0.057495, loss: 2.5831
2022-07-15 15:06:41 - train: epoch 0094, iter [00900, 05004], lr: 0.057480, loss: 2.5120
2022-07-15 15:07:14 - train: epoch 0094, iter [01000, 05004], lr: 0.057464, loss: 2.6113
2022-07-15 15:07:48 - train: epoch 0094, iter [01100, 05004], lr: 0.057448, loss: 2.6331
2022-07-15 15:08:22 - train: epoch 0094, iter [01200, 05004], lr: 0.057432, loss: 2.6141
2022-07-15 15:08:56 - train: epoch 0094, iter [01300, 05004], lr: 0.057416, loss: 2.6218
2022-07-15 15:09:30 - train: epoch 0094, iter [01400, 05004], lr: 0.057400, loss: 2.4864
2022-07-15 15:10:04 - train: epoch 0094, iter [01500, 05004], lr: 0.057384, loss: 2.5256
2022-07-15 15:10:37 - train: epoch 0094, iter [01600, 05004], lr: 0.057368, loss: 2.7287
2022-07-15 15:11:12 - train: epoch 0094, iter [01700, 05004], lr: 0.057352, loss: 2.4331
2022-07-15 15:11:45 - train: epoch 0094, iter [01800, 05004], lr: 0.057336, loss: 2.5512
2022-07-15 15:12:19 - train: epoch 0094, iter [01900, 05004], lr: 0.057320, loss: 2.5288
2022-07-15 15:12:53 - train: epoch 0094, iter [02000, 05004], lr: 0.057304, loss: 2.3949
2022-07-15 15:13:27 - train: epoch 0094, iter [02100, 05004], lr: 0.057288, loss: 2.5192
2022-07-15 15:14:01 - train: epoch 0094, iter [02200, 05004], lr: 0.057273, loss: 2.6867
2022-07-15 15:14:36 - train: epoch 0094, iter [02300, 05004], lr: 0.057257, loss: 2.3555
2022-07-15 15:15:10 - train: epoch 0094, iter [02400, 05004], lr: 0.057241, loss: 2.7039
2022-07-15 15:15:43 - train: epoch 0094, iter [02500, 05004], lr: 0.057225, loss: 2.5382
2022-07-15 15:16:18 - train: epoch 0094, iter [02600, 05004], lr: 0.057209, loss: 2.5006
2022-07-15 15:16:52 - train: epoch 0094, iter [02700, 05004], lr: 0.057193, loss: 2.6961
2022-07-15 15:17:27 - train: epoch 0094, iter [02800, 05004], lr: 0.057177, loss: 2.6591
2022-07-15 15:18:00 - train: epoch 0094, iter [02900, 05004], lr: 0.057161, loss: 2.4177
2022-07-15 15:18:34 - train: epoch 0094, iter [03000, 05004], lr: 0.057145, loss: 2.6341
2022-07-15 15:19:08 - train: epoch 0094, iter [03100, 05004], lr: 0.057129, loss: 3.0269
2022-07-15 15:19:42 - train: epoch 0094, iter [03200, 05004], lr: 0.057113, loss: 2.4988
2022-07-15 15:20:15 - train: epoch 0094, iter [03300, 05004], lr: 0.057097, loss: 2.4330
2022-07-15 15:20:50 - train: epoch 0094, iter [03400, 05004], lr: 0.057081, loss: 2.8523
2022-07-15 15:21:24 - train: epoch 0094, iter [03500, 05004], lr: 0.057065, loss: 2.8536
2022-07-15 15:21:58 - train: epoch 0094, iter [03600, 05004], lr: 0.057050, loss: 2.4420
2022-07-15 15:22:31 - train: epoch 0094, iter [03700, 05004], lr: 0.057034, loss: 2.7392
2022-07-15 15:23:06 - train: epoch 0094, iter [03800, 05004], lr: 0.057018, loss: 2.6666
2022-07-15 15:23:39 - train: epoch 0094, iter [03900, 05004], lr: 0.057002, loss: 2.4708
2022-07-15 15:24:13 - train: epoch 0094, iter [04000, 05004], lr: 0.056986, loss: 2.6552
2022-07-15 15:24:47 - train: epoch 0094, iter [04100, 05004], lr: 0.056970, loss: 2.6628
2022-07-15 15:25:22 - train: epoch 0094, iter [04200, 05004], lr: 0.056954, loss: 2.8787
2022-07-15 15:25:56 - train: epoch 0094, iter [04300, 05004], lr: 0.056938, loss: 2.6677
2022-07-15 15:26:30 - train: epoch 0094, iter [04400, 05004], lr: 0.056922, loss: 2.8258
2022-07-15 15:27:04 - train: epoch 0094, iter [04500, 05004], lr: 0.056906, loss: 2.6223
2022-07-15 15:27:38 - train: epoch 0094, iter [04600, 05004], lr: 0.056890, loss: 2.4882
2022-07-15 15:28:13 - train: epoch 0094, iter [04700, 05004], lr: 0.056874, loss: 2.8023
2022-07-15 15:28:46 - train: epoch 0094, iter [04800, 05004], lr: 0.056858, loss: 2.5856
2022-07-15 15:29:21 - train: epoch 0094, iter [04900, 05004], lr: 0.056842, loss: 2.7732
2022-07-15 15:29:53 - train: epoch 0094, iter [05000, 05004], lr: 0.056826, loss: 2.4752
2022-07-15 15:29:54 - train: epoch 094, train_loss: 2.5861
2022-07-15 15:31:09 - eval: epoch: 094, acc1: 59.820%, acc5: 83.534%, test_loss: 1.6789, per_image_load_time: 2.316ms, per_image_inference_time: 0.458ms
2022-07-15 15:31:10 - until epoch: 094, best_acc1: 62.646%
2022-07-15 15:31:10 - epoch 095 lr: 0.056826
2022-07-15 15:31:51 - train: epoch 0095, iter [00100, 05004], lr: 0.056810, loss: 2.8176
2022-07-15 15:32:24 - train: epoch 0095, iter [00200, 05004], lr: 0.056794, loss: 2.6575
2022-07-15 15:32:58 - train: epoch 0095, iter [00300, 05004], lr: 0.056778, loss: 2.3536
2022-07-15 15:33:33 - train: epoch 0095, iter [00400, 05004], lr: 0.056762, loss: 2.4913
2022-07-15 15:34:06 - train: epoch 0095, iter [00500, 05004], lr: 0.056746, loss: 2.5415
2022-07-15 15:34:41 - train: epoch 0095, iter [00600, 05004], lr: 0.056730, loss: 2.7419
2022-07-15 15:35:16 - train: epoch 0095, iter [00700, 05004], lr: 0.056714, loss: 2.7613
2022-07-15 15:35:49 - train: epoch 0095, iter [00800, 05004], lr: 0.056698, loss: 2.5592
2022-07-15 15:36:24 - train: epoch 0095, iter [00900, 05004], lr: 0.056682, loss: 2.8732
2022-07-15 15:36:58 - train: epoch 0095, iter [01000, 05004], lr: 0.056666, loss: 2.5133
2022-07-15 15:37:32 - train: epoch 0095, iter [01100, 05004], lr: 0.056650, loss: 2.4822
2022-07-15 15:38:06 - train: epoch 0095, iter [01200, 05004], lr: 0.056634, loss: 2.3090
2022-07-15 15:38:40 - train: epoch 0095, iter [01300, 05004], lr: 0.056618, loss: 2.5661
2022-07-15 15:39:14 - train: epoch 0095, iter [01400, 05004], lr: 0.056602, loss: 2.3590
2022-07-15 15:39:48 - train: epoch 0095, iter [01500, 05004], lr: 0.056586, loss: 2.4547
2022-07-15 15:40:21 - train: epoch 0095, iter [01600, 05004], lr: 0.056570, loss: 2.5429
2022-07-15 15:40:55 - train: epoch 0095, iter [01700, 05004], lr: 0.056554, loss: 2.6315
2022-07-15 15:41:28 - train: epoch 0095, iter [01800, 05004], lr: 0.056539, loss: 2.7747
2022-07-15 15:42:01 - train: epoch 0095, iter [01900, 05004], lr: 0.056523, loss: 2.6462
2022-07-15 15:42:35 - train: epoch 0095, iter [02000, 05004], lr: 0.056507, loss: 2.5493
2022-07-15 15:43:08 - train: epoch 0095, iter [02100, 05004], lr: 0.056491, loss: 2.6668
2022-07-15 15:43:42 - train: epoch 0095, iter [02200, 05004], lr: 0.056475, loss: 2.2428
2022-07-15 15:44:16 - train: epoch 0095, iter [02300, 05004], lr: 0.056459, loss: 2.5087
2022-07-15 15:44:50 - train: epoch 0095, iter [02400, 05004], lr: 0.056443, loss: 2.7669
2022-07-15 15:45:23 - train: epoch 0095, iter [02500, 05004], lr: 0.056427, loss: 2.4417
2022-07-15 15:45:57 - train: epoch 0095, iter [02600, 05004], lr: 0.056411, loss: 2.6444
2022-07-15 15:46:31 - train: epoch 0095, iter [02700, 05004], lr: 0.056395, loss: 2.5467
2022-07-15 15:47:04 - train: epoch 0095, iter [02800, 05004], lr: 0.056379, loss: 2.5950
2022-07-15 15:47:38 - train: epoch 0095, iter [02900, 05004], lr: 0.056363, loss: 2.5847
2022-07-15 15:48:12 - train: epoch 0095, iter [03000, 05004], lr: 0.056347, loss: 2.9355
2022-07-15 15:48:45 - train: epoch 0095, iter [03100, 05004], lr: 0.056331, loss: 2.5898
2022-07-15 15:49:19 - train: epoch 0095, iter [03200, 05004], lr: 0.056315, loss: 2.6783
2022-07-15 15:49:52 - train: epoch 0095, iter [03300, 05004], lr: 0.056299, loss: 2.5098
2022-07-15 15:50:27 - train: epoch 0095, iter [03400, 05004], lr: 0.056283, loss: 2.2191
2022-07-15 15:51:00 - train: epoch 0095, iter [03500, 05004], lr: 0.056267, loss: 2.2760
2022-07-15 15:51:35 - train: epoch 0095, iter [03600, 05004], lr: 0.056251, loss: 2.4784
2022-07-15 15:52:08 - train: epoch 0095, iter [03700, 05004], lr: 0.056235, loss: 2.6525
2022-07-15 15:52:42 - train: epoch 0095, iter [03800, 05004], lr: 0.056219, loss: 2.5768
2022-07-15 15:53:16 - train: epoch 0095, iter [03900, 05004], lr: 0.056203, loss: 2.4946
2022-07-15 15:53:50 - train: epoch 0095, iter [04000, 05004], lr: 0.056187, loss: 2.4840
2022-07-15 15:54:24 - train: epoch 0095, iter [04100, 05004], lr: 0.056171, loss: 2.5642
2022-07-15 15:54:58 - train: epoch 0095, iter [04200, 05004], lr: 0.056155, loss: 2.7226
2022-07-15 15:55:31 - train: epoch 0095, iter [04300, 05004], lr: 0.056139, loss: 2.6963
2022-07-15 15:56:06 - train: epoch 0095, iter [04400, 05004], lr: 0.056123, loss: 2.6642
2022-07-15 15:56:40 - train: epoch 0095, iter [04500, 05004], lr: 0.056107, loss: 2.8182
2022-07-15 15:57:15 - train: epoch 0095, iter [04600, 05004], lr: 0.056091, loss: 2.6321
2022-07-15 15:57:48 - train: epoch 0095, iter [04700, 05004], lr: 0.056075, loss: 2.5055
2022-07-15 15:58:22 - train: epoch 0095, iter [04800, 05004], lr: 0.056059, loss: 2.6425
2022-07-15 15:58:56 - train: epoch 0095, iter [04900, 05004], lr: 0.056043, loss: 2.2460
2022-07-15 15:59:29 - train: epoch 0095, iter [05000, 05004], lr: 0.056027, loss: 2.4273
2022-07-15 15:59:30 - train: epoch 095, train_loss: 2.5781
2022-07-15 16:00:44 - eval: epoch: 095, acc1: 59.690%, acc5: 83.682%, test_loss: 1.6773, per_image_load_time: 1.432ms, per_image_inference_time: 0.477ms
2022-07-15 16:00:44 - until epoch: 095, best_acc1: 62.646%
2022-07-15 16:00:44 - epoch 096 lr: 0.056027
2022-07-15 16:01:23 - train: epoch 0096, iter [00100, 05004], lr: 0.056011, loss: 2.7939
2022-07-15 16:01:57 - train: epoch 0096, iter [00200, 05004], lr: 0.055995, loss: 2.5048
2022-07-15 16:02:31 - train: epoch 0096, iter [00300, 05004], lr: 0.055979, loss: 2.5992
2022-07-15 16:03:04 - train: epoch 0096, iter [00400, 05004], lr: 0.055963, loss: 2.3634
2022-07-15 16:03:39 - train: epoch 0096, iter [00500, 05004], lr: 0.055947, loss: 2.3698
2022-07-15 16:04:13 - train: epoch 0096, iter [00600, 05004], lr: 0.055931, loss: 2.5531
2022-07-15 16:04:47 - train: epoch 0096, iter [00700, 05004], lr: 0.055915, loss: 2.5490
2022-07-15 16:05:21 - train: epoch 0096, iter [00800, 05004], lr: 0.055899, loss: 2.4371
2022-07-15 16:05:55 - train: epoch 0096, iter [00900, 05004], lr: 0.055883, loss: 2.5528
2022-07-15 16:06:29 - train: epoch 0096, iter [01000, 05004], lr: 0.055867, loss: 2.7306
2022-07-15 16:07:02 - train: epoch 0096, iter [01100, 05004], lr: 0.055851, loss: 2.5514
2022-07-15 16:07:36 - train: epoch 0096, iter [01200, 05004], lr: 0.055835, loss: 2.4075
2022-07-15 16:08:09 - train: epoch 0096, iter [01300, 05004], lr: 0.055819, loss: 2.5338
2022-07-15 16:08:42 - train: epoch 0096, iter [01400, 05004], lr: 0.055803, loss: 2.2432
2022-07-15 16:09:16 - train: epoch 0096, iter [01500, 05004], lr: 0.055787, loss: 2.5119
2022-07-15 16:09:50 - train: epoch 0096, iter [01600, 05004], lr: 0.055771, loss: 2.4700
2022-07-15 16:10:24 - train: epoch 0096, iter [01700, 05004], lr: 0.055755, loss: 2.0979
2022-07-15 16:10:57 - train: epoch 0096, iter [01800, 05004], lr: 0.055739, loss: 2.5274
2022-07-15 16:11:30 - train: epoch 0096, iter [01900, 05004], lr: 0.055723, loss: 2.7118
2022-07-15 16:12:04 - train: epoch 0096, iter [02000, 05004], lr: 0.055707, loss: 2.4669
2022-07-15 16:12:38 - train: epoch 0096, iter [02100, 05004], lr: 0.055691, loss: 2.8470
2022-07-15 16:13:12 - train: epoch 0096, iter [02200, 05004], lr: 0.055675, loss: 2.1504
2022-07-15 16:13:45 - train: epoch 0096, iter [02300, 05004], lr: 0.055659, loss: 2.2902
2022-07-15 16:14:18 - train: epoch 0096, iter [02400, 05004], lr: 0.055643, loss: 2.7870
2022-07-15 16:14:52 - train: epoch 0096, iter [02500, 05004], lr: 0.055627, loss: 2.8111
2022-07-15 16:15:26 - train: epoch 0096, iter [02600, 05004], lr: 0.055611, loss: 2.3956
2022-07-15 16:16:04 - train: epoch 0096, iter [02700, 05004], lr: 0.055595, loss: 2.7483
2022-07-15 16:16:41 - train: epoch 0096, iter [02800, 05004], lr: 0.055579, loss: 2.6831
2022-07-15 16:17:18 - train: epoch 0096, iter [02900, 05004], lr: 0.055563, loss: 2.7344
2022-07-15 16:17:56 - train: epoch 0096, iter [03000, 05004], lr: 0.055547, loss: 2.8544
2022-07-15 16:18:32 - train: epoch 0096, iter [03100, 05004], lr: 0.055531, loss: 2.8971
2022-07-15 16:19:10 - train: epoch 0096, iter [03200, 05004], lr: 0.055515, loss: 2.8378
2022-07-15 16:19:46 - train: epoch 0096, iter [03300, 05004], lr: 0.055499, loss: 2.6492
2022-07-15 16:20:24 - train: epoch 0096, iter [03400, 05004], lr: 0.055483, loss: 2.2747
2022-07-15 16:21:01 - train: epoch 0096, iter [03500, 05004], lr: 0.055467, loss: 2.1689
2022-07-15 16:21:39 - train: epoch 0096, iter [03600, 05004], lr: 0.055451, loss: 2.5352
2022-07-15 16:22:15 - train: epoch 0096, iter [03700, 05004], lr: 0.055435, loss: 2.5966
2022-07-15 16:22:53 - train: epoch 0096, iter [03800, 05004], lr: 0.055419, loss: 2.4186
2022-07-15 16:23:29 - train: epoch 0096, iter [03900, 05004], lr: 0.055403, loss: 2.5282
2022-07-15 16:24:05 - train: epoch 0096, iter [04000, 05004], lr: 0.055387, loss: 2.5930
2022-07-15 16:24:42 - train: epoch 0096, iter [04100, 05004], lr: 0.055371, loss: 2.6535
2022-07-15 16:25:18 - train: epoch 0096, iter [04200, 05004], lr: 0.055355, loss: 2.6185
2022-07-15 16:25:56 - train: epoch 0096, iter [04300, 05004], lr: 0.055339, loss: 2.1968
2022-07-15 16:26:33 - train: epoch 0096, iter [04400, 05004], lr: 0.055323, loss: 2.4893
2022-07-15 16:27:09 - train: epoch 0096, iter [04500, 05004], lr: 0.055307, loss: 2.5319
2022-07-15 16:27:48 - train: epoch 0096, iter [04600, 05004], lr: 0.055291, loss: 2.3944
2022-07-15 16:28:24 - train: epoch 0096, iter [04700, 05004], lr: 0.055275, loss: 2.6239
2022-07-15 16:29:00 - train: epoch 0096, iter [04800, 05004], lr: 0.055259, loss: 2.4649
2022-07-15 16:29:38 - train: epoch 0096, iter [04900, 05004], lr: 0.055243, loss: 2.5341
2022-07-15 16:30:13 - train: epoch 0096, iter [05000, 05004], lr: 0.055227, loss: 2.6857
2022-07-15 16:30:14 - train: epoch 096, train_loss: 2.5720
2022-07-15 16:31:35 - eval: epoch: 096, acc1: 62.206%, acc5: 85.096%, test_loss: 1.5656, per_image_load_time: 2.610ms, per_image_inference_time: 0.445ms
2022-07-15 16:31:35 - until epoch: 096, best_acc1: 62.646%
2022-07-15 16:31:35 - epoch 097 lr: 0.055226
2022-07-15 16:32:16 - train: epoch 0097, iter [00100, 05004], lr: 0.055210, loss: 2.4320
2022-07-15 16:32:54 - train: epoch 0097, iter [00200, 05004], lr: 0.055194, loss: 2.1081
2022-07-15 16:33:31 - train: epoch 0097, iter [00300, 05004], lr: 0.055178, loss: 2.7060
2022-07-15 16:34:08 - train: epoch 0097, iter [00400, 05004], lr: 0.055162, loss: 2.5933
2022-07-15 16:34:46 - train: epoch 0097, iter [00500, 05004], lr: 0.055146, loss: 2.7133
2022-07-15 16:35:23 - train: epoch 0097, iter [00600, 05004], lr: 0.055130, loss: 2.5636
2022-07-15 16:36:01 - train: epoch 0097, iter [00700, 05004], lr: 0.055114, loss: 2.6295
2022-07-15 16:36:38 - train: epoch 0097, iter [00800, 05004], lr: 0.055098, loss: 2.5350
2022-07-15 16:37:13 - train: epoch 0097, iter [00900, 05004], lr: 0.055082, loss: 2.6303
2022-07-15 16:37:47 - train: epoch 0097, iter [01000, 05004], lr: 0.055066, loss: 2.5550
2022-07-15 16:38:20 - train: epoch 0097, iter [01100, 05004], lr: 0.055050, loss: 2.4257
2022-07-15 16:38:54 - train: epoch 0097, iter [01200, 05004], lr: 0.055034, loss: 2.4668
2022-07-15 16:39:29 - train: epoch 0097, iter [01300, 05004], lr: 0.055018, loss: 2.4934
2022-07-15 16:40:04 - train: epoch 0097, iter [01400, 05004], lr: 0.055002, loss: 2.2725
2022-07-15 16:40:37 - train: epoch 0097, iter [01500, 05004], lr: 0.054986, loss: 2.7456
2022-07-15 16:41:12 - train: epoch 0097, iter [01600, 05004], lr: 0.054970, loss: 2.3410
2022-07-15 16:41:46 - train: epoch 0097, iter [01700, 05004], lr: 0.054954, loss: 2.4853
2022-07-15 16:42:20 - train: epoch 0097, iter [01800, 05004], lr: 0.054938, loss: 2.5670
2022-07-15 16:42:55 - train: epoch 0097, iter [01900, 05004], lr: 0.054922, loss: 2.4777
2022-07-15 16:43:29 - train: epoch 0097, iter [02000, 05004], lr: 0.054906, loss: 2.5599
2022-07-15 16:44:03 - train: epoch 0097, iter [02100, 05004], lr: 0.054890, loss: 2.7698
2022-07-15 16:44:38 - train: epoch 0097, iter [02200, 05004], lr: 0.054874, loss: 2.5831
2022-07-15 16:45:12 - train: epoch 0097, iter [02300, 05004], lr: 0.054858, loss: 2.3470
2022-07-15 16:45:46 - train: epoch 0097, iter [02400, 05004], lr: 0.054842, loss: 2.5226
2022-07-15 16:46:20 - train: epoch 0097, iter [02500, 05004], lr: 0.054826, loss: 2.4465
2022-07-15 16:46:55 - train: epoch 0097, iter [02600, 05004], lr: 0.054810, loss: 2.8196
2022-07-15 16:47:29 - train: epoch 0097, iter [02700, 05004], lr: 0.054794, loss: 2.7114
2022-07-15 16:48:03 - train: epoch 0097, iter [02800, 05004], lr: 0.054778, loss: 2.5653
2022-07-15 16:48:38 - train: epoch 0097, iter [02900, 05004], lr: 0.054762, loss: 2.6441
2022-07-15 16:49:12 - train: epoch 0097, iter [03000, 05004], lr: 0.054746, loss: 2.5952
2022-07-15 16:49:46 - train: epoch 0097, iter [03100, 05004], lr: 0.054730, loss: 2.3315
2022-07-15 16:50:22 - train: epoch 0097, iter [03200, 05004], lr: 0.054714, loss: 2.8488
2022-07-15 16:50:55 - train: epoch 0097, iter [03300, 05004], lr: 0.054698, loss: 2.5407
2022-07-15 16:51:30 - train: epoch 0097, iter [03400, 05004], lr: 0.054682, loss: 2.8226
2022-07-15 16:52:04 - train: epoch 0097, iter [03500, 05004], lr: 0.054666, loss: 2.6036
2022-07-15 16:52:40 - train: epoch 0097, iter [03600, 05004], lr: 0.054650, loss: 2.6129
2022-07-15 16:53:13 - train: epoch 0097, iter [03700, 05004], lr: 0.054634, loss: 2.4426
2022-07-15 16:53:48 - train: epoch 0097, iter [03800, 05004], lr: 0.054618, loss: 2.4400
2022-07-15 16:54:21 - train: epoch 0097, iter [03900, 05004], lr: 0.054602, loss: 2.5185
2022-07-15 16:54:56 - train: epoch 0097, iter [04000, 05004], lr: 0.054586, loss: 2.7553
2022-07-15 16:55:29 - train: epoch 0097, iter [04100, 05004], lr: 0.054570, loss: 2.5718
2022-07-15 16:56:04 - train: epoch 0097, iter [04200, 05004], lr: 0.054554, loss: 2.6762
2022-07-15 16:56:37 - train: epoch 0097, iter [04300, 05004], lr: 0.054538, loss: 2.3858
2022-07-15 16:57:11 - train: epoch 0097, iter [04400, 05004], lr: 0.054521, loss: 2.5273
2022-07-15 16:57:45 - train: epoch 0097, iter [04500, 05004], lr: 0.054505, loss: 2.6367
2022-07-15 16:58:19 - train: epoch 0097, iter [04600, 05004], lr: 0.054489, loss: 2.5689
2022-07-15 16:58:54 - train: epoch 0097, iter [04700, 05004], lr: 0.054473, loss: 2.5395
2022-07-15 16:59:28 - train: epoch 0097, iter [04800, 05004], lr: 0.054457, loss: 2.5301
2022-07-15 17:00:02 - train: epoch 0097, iter [04900, 05004], lr: 0.054441, loss: 2.9809
2022-07-15 17:00:35 - train: epoch 0097, iter [05000, 05004], lr: 0.054425, loss: 2.6150
2022-07-15 17:00:36 - train: epoch 097, train_loss: 2.5658
2022-07-15 17:01:50 - eval: epoch: 097, acc1: 62.564%, acc5: 85.736%, test_loss: 1.5278, per_image_load_time: 1.973ms, per_image_inference_time: 0.475ms
2022-07-15 17:01:50 - until epoch: 097, best_acc1: 62.646%
2022-07-15 17:01:50 - epoch 098 lr: 0.054424
2022-07-15 17:02:29 - train: epoch 0098, iter [00100, 05004], lr: 0.054409, loss: 2.5341
2022-07-15 17:03:04 - train: epoch 0098, iter [00200, 05004], lr: 0.054393, loss: 2.6939
2022-07-15 17:03:38 - train: epoch 0098, iter [00300, 05004], lr: 0.054377, loss: 2.7231
2022-07-15 17:04:11 - train: epoch 0098, iter [00400, 05004], lr: 0.054361, loss: 2.5274
2022-07-15 17:04:46 - train: epoch 0098, iter [00500, 05004], lr: 0.054344, loss: 2.3602
2022-07-15 17:05:20 - train: epoch 0098, iter [00600, 05004], lr: 0.054328, loss: 2.5409
2022-07-15 17:05:53 - train: epoch 0098, iter [00700, 05004], lr: 0.054312, loss: 2.3165
2022-07-15 17:06:27 - train: epoch 0098, iter [00800, 05004], lr: 0.054296, loss: 2.6668
2022-07-15 17:07:01 - train: epoch 0098, iter [00900, 05004], lr: 0.054280, loss: 2.5273
2022-07-15 17:07:35 - train: epoch 0098, iter [01000, 05004], lr: 0.054264, loss: 2.2778
2022-07-15 17:08:09 - train: epoch 0098, iter [01100, 05004], lr: 0.054248, loss: 2.4653
2022-07-15 17:08:43 - train: epoch 0098, iter [01200, 05004], lr: 0.054232, loss: 2.5291
2022-07-15 17:09:17 - train: epoch 0098, iter [01300, 05004], lr: 0.054216, loss: 2.5991
2022-07-15 17:09:51 - train: epoch 0098, iter [01400, 05004], lr: 0.054200, loss: 2.5879
2022-07-15 17:10:25 - train: epoch 0098, iter [01500, 05004], lr: 0.054184, loss: 2.5067
2022-07-15 17:10:59 - train: epoch 0098, iter [01600, 05004], lr: 0.054168, loss: 2.3443
2022-07-15 17:11:33 - train: epoch 0098, iter [01700, 05004], lr: 0.054152, loss: 2.4980
2022-07-15 17:12:07 - train: epoch 0098, iter [01800, 05004], lr: 0.054136, loss: 2.4415
2022-07-15 17:12:41 - train: epoch 0098, iter [01900, 05004], lr: 0.054120, loss: 2.4097
2022-07-15 17:13:16 - train: epoch 0098, iter [02000, 05004], lr: 0.054104, loss: 2.5042
2022-07-15 17:13:49 - train: epoch 0098, iter [02100, 05004], lr: 0.054088, loss: 2.4261
2022-07-15 17:14:24 - train: epoch 0098, iter [02200, 05004], lr: 0.054072, loss: 2.4266
2022-07-15 17:14:57 - train: epoch 0098, iter [02300, 05004], lr: 0.054056, loss: 2.2595
2022-07-15 17:15:31 - train: epoch 0098, iter [02400, 05004], lr: 0.054040, loss: 2.7290
2022-07-15 17:16:06 - train: epoch 0098, iter [02500, 05004], lr: 0.054024, loss: 2.6155
2022-07-15 17:16:40 - train: epoch 0098, iter [02600, 05004], lr: 0.054008, loss: 2.3415
2022-07-15 17:17:14 - train: epoch 0098, iter [02700, 05004], lr: 0.053992, loss: 2.5543
2022-07-15 17:17:49 - train: epoch 0098, iter [02800, 05004], lr: 0.053976, loss: 2.4747
2022-07-15 17:18:23 - train: epoch 0098, iter [02900, 05004], lr: 0.053959, loss: 2.5095
2022-07-15 17:18:57 - train: epoch 0098, iter [03000, 05004], lr: 0.053943, loss: 2.5842
2022-07-15 17:19:32 - train: epoch 0098, iter [03100, 05004], lr: 0.053927, loss: 2.3529
2022-07-15 17:20:07 - train: epoch 0098, iter [03200, 05004], lr: 0.053911, loss: 2.3882
2022-07-15 17:20:41 - train: epoch 0098, iter [03300, 05004], lr: 0.053895, loss: 2.5991
2022-07-15 17:21:15 - train: epoch 0098, iter [03400, 05004], lr: 0.053879, loss: 2.6316
2022-07-15 17:21:49 - train: epoch 0098, iter [03500, 05004], lr: 0.053863, loss: 2.5825
2022-07-15 17:22:23 - train: epoch 0098, iter [03600, 05004], lr: 0.053847, loss: 2.7906
2022-07-15 17:22:58 - train: epoch 0098, iter [03700, 05004], lr: 0.053831, loss: 2.6206
2022-07-15 17:23:33 - train: epoch 0098, iter [03800, 05004], lr: 0.053815, loss: 2.6790
2022-07-15 17:24:07 - train: epoch 0098, iter [03900, 05004], lr: 0.053799, loss: 2.6630
2022-07-15 17:24:41 - train: epoch 0098, iter [04000, 05004], lr: 0.053783, loss: 2.5110
2022-07-15 17:25:16 - train: epoch 0098, iter [04100, 05004], lr: 0.053767, loss: 2.5414
2022-07-15 17:25:50 - train: epoch 0098, iter [04200, 05004], lr: 0.053751, loss: 2.7299
2022-07-15 17:26:25 - train: epoch 0098, iter [04300, 05004], lr: 0.053735, loss: 2.4399
2022-07-15 17:27:00 - train: epoch 0098, iter [04400, 05004], lr: 0.053719, loss: 2.5783
2022-07-15 17:27:34 - train: epoch 0098, iter [04500, 05004], lr: 0.053703, loss: 2.3411
2022-07-15 17:28:09 - train: epoch 0098, iter [04600, 05004], lr: 0.053687, loss: 2.4782
2022-07-15 17:28:43 - train: epoch 0098, iter [04700, 05004], lr: 0.053671, loss: 2.6630
2022-07-15 17:29:17 - train: epoch 0098, iter [04800, 05004], lr: 0.053654, loss: 2.6297
2022-07-15 17:29:51 - train: epoch 0098, iter [04900, 05004], lr: 0.053638, loss: 2.5062
2022-07-15 17:30:24 - train: epoch 0098, iter [05000, 05004], lr: 0.053622, loss: 2.6738
2022-07-15 17:30:25 - train: epoch 098, train_loss: 2.5578
2022-07-15 17:31:39 - eval: epoch: 098, acc1: 61.840%, acc5: 84.920%, test_loss: 1.5722, per_image_load_time: 0.741ms, per_image_inference_time: 0.420ms
2022-07-15 17:31:39 - until epoch: 098, best_acc1: 62.646%
2022-07-15 17:31:39 - epoch 099 lr: 0.053622
2022-07-15 17:32:19 - train: epoch 0099, iter [00100, 05004], lr: 0.053606, loss: 2.4928
2022-07-15 17:32:53 - train: epoch 0099, iter [00200, 05004], lr: 0.053590, loss: 2.5629
2022-07-15 17:33:27 - train: epoch 0099, iter [00300, 05004], lr: 0.053574, loss: 2.2831
2022-07-15 17:34:00 - train: epoch 0099, iter [00400, 05004], lr: 0.053558, loss: 2.6801
2022-07-15 17:34:34 - train: epoch 0099, iter [00500, 05004], lr: 0.053541, loss: 2.5908
2022-07-15 17:35:07 - train: epoch 0099, iter [00600, 05004], lr: 0.053525, loss: 2.4138
2022-07-15 17:35:41 - train: epoch 0099, iter [00700, 05004], lr: 0.053509, loss: 2.5304
2022-07-15 17:36:15 - train: epoch 0099, iter [00800, 05004], lr: 0.053493, loss: 2.4458
2022-07-15 17:36:48 - train: epoch 0099, iter [00900, 05004], lr: 0.053477, loss: 2.6239
2022-07-15 17:37:22 - train: epoch 0099, iter [01000, 05004], lr: 0.053461, loss: 2.5431
2022-07-15 17:37:56 - train: epoch 0099, iter [01100, 05004], lr: 0.053445, loss: 2.5493
2022-07-15 17:38:29 - train: epoch 0099, iter [01200, 05004], lr: 0.053429, loss: 2.3100
2022-07-15 17:39:03 - train: epoch 0099, iter [01300, 05004], lr: 0.053413, loss: 2.4471
2022-07-15 17:39:37 - train: epoch 0099, iter [01400, 05004], lr: 0.053397, loss: 2.5230
2022-07-15 17:40:11 - train: epoch 0099, iter [01500, 05004], lr: 0.053381, loss: 2.4593
2022-07-15 17:40:45 - train: epoch 0099, iter [01600, 05004], lr: 0.053365, loss: 2.5215
2022-07-15 17:41:18 - train: epoch 0099, iter [01700, 05004], lr: 0.053349, loss: 2.2559
2022-07-15 17:41:51 - train: epoch 0099, iter [01800, 05004], lr: 0.053333, loss: 2.3581
2022-07-15 17:42:26 - train: epoch 0099, iter [01900, 05004], lr: 0.053317, loss: 2.7221
2022-07-15 17:42:59 - train: epoch 0099, iter [02000, 05004], lr: 0.053301, loss: 2.3923
2022-07-15 17:43:33 - train: epoch 0099, iter [02100, 05004], lr: 0.053284, loss: 2.2867
2022-07-15 17:44:06 - train: epoch 0099, iter [02200, 05004], lr: 0.053268, loss: 2.2998
2022-07-15 17:44:40 - train: epoch 0099, iter [02300, 05004], lr: 0.053252, loss: 2.4145
2022-07-15 17:45:14 - train: epoch 0099, iter [02400, 05004], lr: 0.053236, loss: 2.6897
2022-07-15 17:45:47 - train: epoch 0099, iter [02500, 05004], lr: 0.053220, loss: 2.2713
2022-07-15 17:46:21 - train: epoch 0099, iter [02600, 05004], lr: 0.053204, loss: 2.5810
2022-07-15 17:46:54 - train: epoch 0099, iter [02700, 05004], lr: 0.053188, loss: 2.7299
2022-07-15 17:47:28 - train: epoch 0099, iter [02800, 05004], lr: 0.053172, loss: 2.5287
2022-07-15 17:48:01 - train: epoch 0099, iter [02900, 05004], lr: 0.053156, loss: 2.6723
2022-07-15 17:48:35 - train: epoch 0099, iter [03000, 05004], lr: 0.053140, loss: 2.8092
2022-07-15 17:49:09 - train: epoch 0099, iter [03100, 05004], lr: 0.053124, loss: 2.5513
2022-07-15 17:49:42 - train: epoch 0099, iter [03200, 05004], lr: 0.053108, loss: 2.7709
2022-07-15 17:50:16 - train: epoch 0099, iter [03300, 05004], lr: 0.053092, loss: 2.3690
2022-07-15 17:50:49 - train: epoch 0099, iter [03400, 05004], lr: 0.053076, loss: 2.3661
2022-07-15 17:51:22 - train: epoch 0099, iter [03500, 05004], lr: 0.053060, loss: 2.7747
2022-07-15 17:51:56 - train: epoch 0099, iter [03600, 05004], lr: 0.053044, loss: 2.4566
2022-07-15 17:52:29 - train: epoch 0099, iter [03700, 05004], lr: 0.053027, loss: 2.1179
2022-07-15 17:53:03 - train: epoch 0099, iter [03800, 05004], lr: 0.053011, loss: 2.4990
2022-07-15 17:53:38 - train: epoch 0099, iter [03900, 05004], lr: 0.052995, loss: 2.4929
2022-07-15 17:54:10 - train: epoch 0099, iter [04000, 05004], lr: 0.052979, loss: 2.5044
2022-07-15 17:54:44 - train: epoch 0099, iter [04100, 05004], lr: 0.052963, loss: 2.4925
2022-07-15 17:55:17 - train: epoch 0099, iter [04200, 05004], lr: 0.052947, loss: 2.6024
2022-07-15 17:55:50 - train: epoch 0099, iter [04300, 05004], lr: 0.052931, loss: 2.5345
2022-07-15 17:56:23 - train: epoch 0099, iter [04400, 05004], lr: 0.052915, loss: 2.5798
2022-07-15 17:56:58 - train: epoch 0099, iter [04500, 05004], lr: 0.052899, loss: 2.6981
2022-07-15 17:57:32 - train: epoch 0099, iter [04600, 05004], lr: 0.052883, loss: 2.5355
2022-07-15 17:58:06 - train: epoch 0099, iter [04700, 05004], lr: 0.052867, loss: 2.4862
2022-07-15 17:58:40 - train: epoch 0099, iter [04800, 05004], lr: 0.052851, loss: 2.6164
2022-07-15 17:59:14 - train: epoch 0099, iter [04900, 05004], lr: 0.052835, loss: 2.6241
2022-07-15 17:59:46 - train: epoch 0099, iter [05000, 05004], lr: 0.052819, loss: 2.6267
2022-07-15 17:59:47 - train: epoch 099, train_loss: 2.5475
2022-07-15 18:01:01 - eval: epoch: 099, acc1: 63.688%, acc5: 85.870%, test_loss: 1.5023, per_image_load_time: 2.230ms, per_image_inference_time: 0.456ms
2022-07-15 18:01:01 - until epoch: 099, best_acc1: 63.688%
2022-07-15 18:01:01 - epoch 100 lr: 0.052818
2022-07-15 18:01:41 - train: epoch 0100, iter [00100, 05004], lr: 0.052802, loss: 2.8281
2022-07-15 18:02:15 - train: epoch 0100, iter [00200, 05004], lr: 0.052786, loss: 2.5198
2022-07-15 18:02:49 - train: epoch 0100, iter [00300, 05004], lr: 0.052770, loss: 2.1339
2022-07-15 18:03:22 - train: epoch 0100, iter [00400, 05004], lr: 0.052754, loss: 2.4397
2022-07-15 18:03:56 - train: epoch 0100, iter [00500, 05004], lr: 0.052738, loss: 2.4015
2022-07-15 18:04:29 - train: epoch 0100, iter [00600, 05004], lr: 0.052721, loss: 2.6511
2022-07-15 18:05:03 - train: epoch 0100, iter [00700, 05004], lr: 0.052705, loss: 2.5386
2022-07-15 18:05:37 - train: epoch 0100, iter [00800, 05004], lr: 0.052689, loss: 2.3758
2022-07-15 18:06:11 - train: epoch 0100, iter [00900, 05004], lr: 0.052673, loss: 2.3587
2022-07-15 18:06:45 - train: epoch 0100, iter [01000, 05004], lr: 0.052657, loss: 2.4565
2022-07-15 18:07:20 - train: epoch 0100, iter [01100, 05004], lr: 0.052641, loss: 2.2162
2022-07-15 18:07:54 - train: epoch 0100, iter [01200, 05004], lr: 0.052625, loss: 2.5739
2022-07-15 18:08:28 - train: epoch 0100, iter [01300, 05004], lr: 0.052609, loss: 2.5364
2022-07-15 18:09:03 - train: epoch 0100, iter [01400, 05004], lr: 0.052593, loss: 2.4039
2022-07-15 18:09:37 - train: epoch 0100, iter [01500, 05004], lr: 0.052577, loss: 2.3910
2022-07-15 18:10:11 - train: epoch 0100, iter [01600, 05004], lr: 0.052561, loss: 2.2831
2022-07-15 18:10:45 - train: epoch 0100, iter [01700, 05004], lr: 0.052545, loss: 2.3461
2022-07-15 18:11:20 - train: epoch 0100, iter [01800, 05004], lr: 0.052529, loss: 2.3482
2022-07-15 18:11:54 - train: epoch 0100, iter [01900, 05004], lr: 0.052512, loss: 2.3129
2022-07-15 18:12:29 - train: epoch 0100, iter [02000, 05004], lr: 0.052496, loss: 2.7836
2022-07-15 18:13:03 - train: epoch 0100, iter [02100, 05004], lr: 0.052480, loss: 2.3519
2022-07-15 18:13:37 - train: epoch 0100, iter [02200, 05004], lr: 0.052464, loss: 2.6408
2022-07-15 18:14:11 - train: epoch 0100, iter [02300, 05004], lr: 0.052448, loss: 2.6142
2022-07-15 18:14:46 - train: epoch 0100, iter [02400, 05004], lr: 0.052432, loss: 2.6880
2022-07-15 18:15:21 - train: epoch 0100, iter [02500, 05004], lr: 0.052416, loss: 2.6787
2022-07-15 18:15:55 - train: epoch 0100, iter [02600, 05004], lr: 0.052400, loss: 2.3602
2022-07-15 18:16:28 - train: epoch 0100, iter [02700, 05004], lr: 0.052384, loss: 2.4873
2022-07-15 18:17:02 - train: epoch 0100, iter [02800, 05004], lr: 0.052368, loss: 2.6205
2022-07-15 18:17:37 - train: epoch 0100, iter [02900, 05004], lr: 0.052352, loss: 2.5656
2022-07-15 18:18:11 - train: epoch 0100, iter [03000, 05004], lr: 0.052336, loss: 2.3850
2022-07-15 18:18:45 - train: epoch 0100, iter [03100, 05004], lr: 0.052320, loss: 2.5136
2022-07-15 18:19:19 - train: epoch 0100, iter [03200, 05004], lr: 0.052303, loss: 2.8131
2022-07-15 18:19:53 - train: epoch 0100, iter [03300, 05004], lr: 0.052287, loss: 2.5493
2022-07-15 18:20:27 - train: epoch 0100, iter [03400, 05004], lr: 0.052271, loss: 2.3512
2022-07-15 18:21:01 - train: epoch 0100, iter [03500, 05004], lr: 0.052255, loss: 2.2734
2022-07-15 18:21:35 - train: epoch 0100, iter [03600, 05004], lr: 0.052239, loss: 2.7590
2022-07-15 18:22:10 - train: epoch 0100, iter [03700, 05004], lr: 0.052223, loss: 2.6867
2022-07-15 18:22:44 - train: epoch 0100, iter [03800, 05004], lr: 0.052207, loss: 2.4020
2022-07-15 18:23:19 - train: epoch 0100, iter [03900, 05004], lr: 0.052191, loss: 2.7881
2022-07-15 18:23:53 - train: epoch 0100, iter [04000, 05004], lr: 0.052175, loss: 2.2219
2022-07-15 18:24:26 - train: epoch 0100, iter [04100, 05004], lr: 0.052159, loss: 2.5070
2022-07-15 18:25:01 - train: epoch 0100, iter [04200, 05004], lr: 0.052143, loss: 2.5734
2022-07-15 18:25:35 - train: epoch 0100, iter [04300, 05004], lr: 0.052127, loss: 2.4285
2022-07-15 18:26:10 - train: epoch 0100, iter [04400, 05004], lr: 0.052110, loss: 2.4652
2022-07-15 18:26:44 - train: epoch 0100, iter [04500, 05004], lr: 0.052094, loss: 2.1388
2022-07-15 18:27:18 - train: epoch 0100, iter [04600, 05004], lr: 0.052078, loss: 2.5028
2022-07-15 18:27:52 - train: epoch 0100, iter [04700, 05004], lr: 0.052062, loss: 2.4058
2022-07-15 18:28:26 - train: epoch 0100, iter [04800, 05004], lr: 0.052046, loss: 2.2770
2022-07-15 18:29:00 - train: epoch 0100, iter [04900, 05004], lr: 0.052030, loss: 2.3163
2022-07-15 18:29:33 - train: epoch 0100, iter [05000, 05004], lr: 0.052014, loss: 2.5759
2022-07-15 18:29:34 - train: epoch 100, train_loss: 2.5448
2022-07-15 18:30:50 - eval: epoch: 100, acc1: 63.742%, acc5: 86.168%, test_loss: 1.4921, per_image_load_time: 2.454ms, per_image_inference_time: 0.454ms
2022-07-15 18:30:50 - until epoch: 100, best_acc1: 63.742%
2022-07-15 18:30:50 - epoch 101 lr: 0.052013
2022-07-15 18:31:31 - train: epoch 0101, iter [00100, 05004], lr: 0.051997, loss: 2.3692
2022-07-15 18:32:06 - train: epoch 0101, iter [00200, 05004], lr: 0.051981, loss: 2.3901
2022-07-15 18:32:39 - train: epoch 0101, iter [00300, 05004], lr: 0.051965, loss: 2.6303
2022-07-15 18:33:15 - train: epoch 0101, iter [00400, 05004], lr: 0.051949, loss: 2.6581
2022-07-15 18:33:49 - train: epoch 0101, iter [00500, 05004], lr: 0.051933, loss: 2.6982
2022-07-15 18:34:23 - train: epoch 0101, iter [00600, 05004], lr: 0.051917, loss: 2.4805
2022-07-15 18:34:59 - train: epoch 0101, iter [00700, 05004], lr: 0.051901, loss: 2.9698
2022-07-15 18:35:34 - train: epoch 0101, iter [00800, 05004], lr: 0.051885, loss: 2.5626
2022-07-15 18:36:08 - train: epoch 0101, iter [00900, 05004], lr: 0.051869, loss: 2.6288
2022-07-15 18:36:43 - train: epoch 0101, iter [01000, 05004], lr: 0.051852, loss: 2.7998
2022-07-15 18:37:17 - train: epoch 0101, iter [01100, 05004], lr: 0.051836, loss: 2.6763
2022-07-15 18:37:51 - train: epoch 0101, iter [01200, 05004], lr: 0.051820, loss: 2.4894
2022-07-15 18:38:26 - train: epoch 0101, iter [01300, 05004], lr: 0.051804, loss: 3.1177
2022-07-15 18:39:00 - train: epoch 0101, iter [01400, 05004], lr: 0.051788, loss: 2.6330
2022-07-15 18:39:35 - train: epoch 0101, iter [01500, 05004], lr: 0.051772, loss: 2.5775
2022-07-15 18:40:10 - train: epoch 0101, iter [01600, 05004], lr: 0.051756, loss: 2.4341
2022-07-15 18:40:44 - train: epoch 0101, iter [01700, 05004], lr: 0.051740, loss: 2.5776
2022-07-15 18:41:19 - train: epoch 0101, iter [01800, 05004], lr: 0.051724, loss: 2.4305
2022-07-15 18:41:54 - train: epoch 0101, iter [01900, 05004], lr: 0.051708, loss: 2.5102
2022-07-15 18:42:28 - train: epoch 0101, iter [02000, 05004], lr: 0.051692, loss: 2.3704
2022-07-15 18:43:03 - train: epoch 0101, iter [02100, 05004], lr: 0.051675, loss: 2.5496
2022-07-15 18:43:37 - train: epoch 0101, iter [02200, 05004], lr: 0.051659, loss: 2.1649
2022-07-15 18:44:12 - train: epoch 0101, iter [02300, 05004], lr: 0.051643, loss: 2.6346
2022-07-15 18:44:47 - train: epoch 0101, iter [02400, 05004], lr: 0.051627, loss: 2.6175
2022-07-15 18:45:21 - train: epoch 0101, iter [02500, 05004], lr: 0.051611, loss: 2.7187
2022-07-15 18:45:56 - train: epoch 0101, iter [02600, 05004], lr: 0.051595, loss: 2.2367
2022-07-15 18:46:31 - train: epoch 0101, iter [02700, 05004], lr: 0.051579, loss: 2.6152
2022-07-15 18:47:05 - train: epoch 0101, iter [02800, 05004], lr: 0.051563, loss: 2.2201
2022-07-15 18:47:41 - train: epoch 0101, iter [02900, 05004], lr: 0.051547, loss: 2.4058
2022-07-15 18:48:15 - train: epoch 0101, iter [03000, 05004], lr: 0.051531, loss: 2.3987
2022-07-15 18:48:50 - train: epoch 0101, iter [03100, 05004], lr: 0.051515, loss: 2.6471
2022-07-15 18:49:25 - train: epoch 0101, iter [03200, 05004], lr: 0.051498, loss: 2.8249
2022-07-15 18:50:00 - train: epoch 0101, iter [03300, 05004], lr: 0.051482, loss: 2.4272
2022-07-15 18:50:35 - train: epoch 0101, iter [03400, 05004], lr: 0.051466, loss: 2.4262
2022-07-15 18:51:10 - train: epoch 0101, iter [03500, 05004], lr: 0.051450, loss: 2.4308
2022-07-15 18:51:44 - train: epoch 0101, iter [03600, 05004], lr: 0.051434, loss: 2.2316
2022-07-15 18:52:19 - train: epoch 0101, iter [03700, 05004], lr: 0.051418, loss: 2.8067
2022-07-15 18:52:53 - train: epoch 0101, iter [03800, 05004], lr: 0.051402, loss: 2.6552
2022-07-15 18:53:29 - train: epoch 0101, iter [03900, 05004], lr: 0.051386, loss: 2.7556
2022-07-15 18:54:04 - train: epoch 0101, iter [04000, 05004], lr: 0.051370, loss: 2.5081
2022-07-15 18:54:39 - train: epoch 0101, iter [04100, 05004], lr: 0.051354, loss: 2.2910
2022-07-15 18:55:13 - train: epoch 0101, iter [04200, 05004], lr: 0.051338, loss: 2.4583
2022-07-15 18:55:49 - train: epoch 0101, iter [04300, 05004], lr: 0.051321, loss: 2.4565
2022-07-15 18:56:24 - train: epoch 0101, iter [04400, 05004], lr: 0.051305, loss: 2.5912
2022-07-15 18:56:59 - train: epoch 0101, iter [04500, 05004], lr: 0.051289, loss: 2.7147
2022-07-15 18:57:34 - train: epoch 0101, iter [04600, 05004], lr: 0.051273, loss: 2.7364
2022-07-15 18:58:09 - train: epoch 0101, iter [04700, 05004], lr: 0.051257, loss: 2.4418
2022-07-15 18:58:45 - train: epoch 0101, iter [04800, 05004], lr: 0.051241, loss: 2.4585
2022-07-15 18:59:19 - train: epoch 0101, iter [04900, 05004], lr: 0.051225, loss: 2.5120
2022-07-15 18:59:53 - train: epoch 0101, iter [05000, 05004], lr: 0.051209, loss: 2.4237
2022-07-15 18:59:54 - train: epoch 101, train_loss: 2.5327
2022-07-15 19:01:09 - eval: epoch: 101, acc1: 62.616%, acc5: 85.464%, test_loss: 1.5550, per_image_load_time: 1.830ms, per_image_inference_time: 0.490ms
2022-07-15 19:01:09 - until epoch: 101, best_acc1: 63.742%
2022-07-15 19:01:09 - epoch 102 lr: 0.051208
2022-07-15 19:01:49 - train: epoch 0102, iter [00100, 05004], lr: 0.051192, loss: 2.2740
2022-07-15 19:02:24 - train: epoch 0102, iter [00200, 05004], lr: 0.051176, loss: 2.3342
2022-07-15 19:02:59 - train: epoch 0102, iter [00300, 05004], lr: 0.051160, loss: 2.2164
2022-07-15 19:03:34 - train: epoch 0102, iter [00400, 05004], lr: 0.051144, loss: 2.3616
2022-07-15 19:04:08 - train: epoch 0102, iter [00500, 05004], lr: 0.051128, loss: 2.7680
2022-07-15 19:04:42 - train: epoch 0102, iter [00600, 05004], lr: 0.051112, loss: 2.3685
2022-07-15 19:05:17 - train: epoch 0102, iter [00700, 05004], lr: 0.051096, loss: 2.9731
2022-07-15 19:05:52 - train: epoch 0102, iter [00800, 05004], lr: 0.051079, loss: 2.5392
2022-07-15 19:06:27 - train: epoch 0102, iter [00900, 05004], lr: 0.051063, loss: 2.6628
2022-07-15 19:07:01 - train: epoch 0102, iter [01000, 05004], lr: 0.051047, loss: 2.5787
2022-07-15 19:07:36 - train: epoch 0102, iter [01100, 05004], lr: 0.051031, loss: 2.5566
2022-07-15 19:08:11 - train: epoch 0102, iter [01200, 05004], lr: 0.051015, loss: 2.6093
2022-07-15 19:08:46 - train: epoch 0102, iter [01300, 05004], lr: 0.050999, loss: 2.4631
2022-07-15 19:09:20 - train: epoch 0102, iter [01400, 05004], lr: 0.050983, loss: 2.4822
2022-07-15 19:09:55 - train: epoch 0102, iter [01500, 05004], lr: 0.050967, loss: 2.7974
2022-07-15 19:10:29 - train: epoch 0102, iter [01600, 05004], lr: 0.050951, loss: 2.5087
2022-07-15 19:11:04 - train: epoch 0102, iter [01700, 05004], lr: 0.050935, loss: 2.5501
2022-07-15 19:11:40 - train: epoch 0102, iter [01800, 05004], lr: 0.050918, loss: 2.5496
2022-07-15 19:12:15 - train: epoch 0102, iter [01900, 05004], lr: 0.050902, loss: 2.4430
2022-07-15 19:12:49 - train: epoch 0102, iter [02000, 05004], lr: 0.050886, loss: 2.5225
2022-07-15 19:13:24 - train: epoch 0102, iter [02100, 05004], lr: 0.050870, loss: 2.3212
2022-07-15 19:13:58 - train: epoch 0102, iter [02200, 05004], lr: 0.050854, loss: 2.4280
2022-07-15 19:14:33 - train: epoch 0102, iter [02300, 05004], lr: 0.050838, loss: 2.6231
2022-07-15 19:15:08 - train: epoch 0102, iter [02400, 05004], lr: 0.050822, loss: 2.6852
2022-07-15 19:15:42 - train: epoch 0102, iter [02500, 05004], lr: 0.050806, loss: 2.2504
2022-07-15 19:16:17 - train: epoch 0102, iter [02600, 05004], lr: 0.050790, loss: 2.5851
2022-07-15 19:16:52 - train: epoch 0102, iter [02700, 05004], lr: 0.050774, loss: 2.7967
2022-07-15 19:17:27 - train: epoch 0102, iter [02800, 05004], lr: 0.050758, loss: 2.7671
2022-07-15 19:18:02 - train: epoch 0102, iter [02900, 05004], lr: 0.050741, loss: 2.4463
2022-07-15 19:18:36 - train: epoch 0102, iter [03000, 05004], lr: 0.050725, loss: 2.4329
2022-07-15 19:19:11 - train: epoch 0102, iter [03100, 05004], lr: 0.050709, loss: 2.4312
2022-07-15 19:19:46 - train: epoch 0102, iter [03200, 05004], lr: 0.050693, loss: 2.3117
2022-07-15 19:20:21 - train: epoch 0102, iter [03300, 05004], lr: 0.050677, loss: 2.3364
2022-07-15 19:20:55 - train: epoch 0102, iter [03400, 05004], lr: 0.050661, loss: 2.7790
2022-07-15 19:21:30 - train: epoch 0102, iter [03500, 05004], lr: 0.050645, loss: 2.6085
2022-07-15 19:22:06 - train: epoch 0102, iter [03600, 05004], lr: 0.050629, loss: 2.0851
2022-07-15 19:22:40 - train: epoch 0102, iter [03700, 05004], lr: 0.050613, loss: 2.4190
2022-07-15 19:23:15 - train: epoch 0102, iter [03800, 05004], lr: 0.050597, loss: 2.5223
2022-07-15 19:23:49 - train: epoch 0102, iter [03900, 05004], lr: 0.050580, loss: 2.6374
2022-07-15 19:24:25 - train: epoch 0102, iter [04000, 05004], lr: 0.050564, loss: 2.5532
2022-07-15 19:24:59 - train: epoch 0102, iter [04100, 05004], lr: 0.050548, loss: 2.2008
2022-07-15 19:25:34 - train: epoch 0102, iter [04200, 05004], lr: 0.050532, loss: 2.8062
2022-07-15 19:26:08 - train: epoch 0102, iter [04300, 05004], lr: 0.050516, loss: 2.3634
2022-07-15 19:26:43 - train: epoch 0102, iter [04400, 05004], lr: 0.050500, loss: 2.5884
2022-07-15 19:27:18 - train: epoch 0102, iter [04500, 05004], lr: 0.050484, loss: 2.4663
2022-07-15 19:27:53 - train: epoch 0102, iter [04600, 05004], lr: 0.050468, loss: 2.7717
2022-07-15 19:28:28 - train: epoch 0102, iter [04700, 05004], lr: 0.050452, loss: 2.7321
2022-07-15 19:29:03 - train: epoch 0102, iter [04800, 05004], lr: 0.050436, loss: 2.5390
2022-07-15 19:29:38 - train: epoch 0102, iter [04900, 05004], lr: 0.050420, loss: 2.5067
2022-07-15 19:30:12 - train: epoch 0102, iter [05000, 05004], lr: 0.050403, loss: 2.4066
2022-07-15 19:30:13 - train: epoch 102, train_loss: 2.5304
2022-07-15 19:31:29 - eval: epoch: 102, acc1: 61.552%, acc5: 84.808%, test_loss: 1.5990, per_image_load_time: 2.213ms, per_image_inference_time: 0.467ms
2022-07-15 19:31:29 - until epoch: 102, best_acc1: 63.742%
2022-07-15 19:31:29 - epoch 103 lr: 0.050403
2022-07-15 19:32:09 - train: epoch 0103, iter [00100, 05004], lr: 0.050387, loss: 2.2707
2022-07-15 19:32:44 - train: epoch 0103, iter [00200, 05004], lr: 0.050371, loss: 2.5846
2022-07-15 19:33:19 - train: epoch 0103, iter [00300, 05004], lr: 0.050354, loss: 2.6724
2022-07-15 19:33:54 - train: epoch 0103, iter [00400, 05004], lr: 0.050338, loss: 2.5928
2022-07-15 19:34:29 - train: epoch 0103, iter [00500, 05004], lr: 0.050322, loss: 2.3688
2022-07-15 19:35:03 - train: epoch 0103, iter [00600, 05004], lr: 0.050306, loss: 2.4697
2022-07-15 19:35:38 - train: epoch 0103, iter [00700, 05004], lr: 0.050290, loss: 2.6174
2022-07-15 19:36:12 - train: epoch 0103, iter [00800, 05004], lr: 0.050274, loss: 2.5991
2022-07-15 19:36:47 - train: epoch 0103, iter [00900, 05004], lr: 0.050258, loss: 2.5112
2022-07-15 19:37:21 - train: epoch 0103, iter [01000, 05004], lr: 0.050242, loss: 2.5063
2022-07-15 19:37:56 - train: epoch 0103, iter [01100, 05004], lr: 0.050226, loss: 2.6173
2022-07-15 19:38:31 - train: epoch 0103, iter [01200, 05004], lr: 0.050210, loss: 2.6612
2022-07-15 19:39:06 - train: epoch 0103, iter [01300, 05004], lr: 0.050193, loss: 2.4611
2022-07-15 19:39:40 - train: epoch 0103, iter [01400, 05004], lr: 0.050177, loss: 2.5860
2022-07-15 19:40:15 - train: epoch 0103, iter [01500, 05004], lr: 0.050161, loss: 2.8049
2022-07-15 19:40:50 - train: epoch 0103, iter [01600, 05004], lr: 0.050145, loss: 2.2862
2022-07-15 19:41:25 - train: epoch 0103, iter [01700, 05004], lr: 0.050129, loss: 2.4336
2022-07-15 19:41:58 - train: epoch 0103, iter [01800, 05004], lr: 0.050113, loss: 2.3443
2022-07-15 19:42:33 - train: epoch 0103, iter [01900, 05004], lr: 0.050097, loss: 2.9216
2022-07-15 19:43:08 - train: epoch 0103, iter [02000, 05004], lr: 0.050081, loss: 2.2327
2022-07-15 19:43:43 - train: epoch 0103, iter [02100, 05004], lr: 0.050065, loss: 2.6202
2022-07-15 19:44:18 - train: epoch 0103, iter [02200, 05004], lr: 0.050049, loss: 2.5257
2022-07-15 19:44:53 - train: epoch 0103, iter [02300, 05004], lr: 0.050033, loss: 2.4332
2022-07-15 19:45:28 - train: epoch 0103, iter [02400, 05004], lr: 0.050016, loss: 2.8713
2022-07-15 19:46:03 - train: epoch 0103, iter [02500, 05004], lr: 0.050000, loss: 2.1756
2022-07-15 19:46:37 - train: epoch 0103, iter [02600, 05004], lr: 0.049984, loss: 2.6687
2022-07-15 19:47:12 - train: epoch 0103, iter [02700, 05004], lr: 0.049968, loss: 2.2779
2022-07-15 19:47:46 - train: epoch 0103, iter [02800, 05004], lr: 0.049952, loss: 2.3506
2022-07-15 19:48:22 - train: epoch 0103, iter [02900, 05004], lr: 0.049936, loss: 2.4574
2022-07-15 19:48:56 - train: epoch 0103, iter [03000, 05004], lr: 0.049920, loss: 2.4961
2022-07-15 19:49:31 - train: epoch 0103, iter [03100, 05004], lr: 0.049904, loss: 2.8911
2022-07-15 19:50:06 - train: epoch 0103, iter [03200, 05004], lr: 0.049888, loss: 2.6288
2022-07-15 19:50:41 - train: epoch 0103, iter [03300, 05004], lr: 0.049872, loss: 2.6899
2022-07-15 19:51:16 - train: epoch 0103, iter [03400, 05004], lr: 0.049855, loss: 2.5245
2022-07-15 19:51:50 - train: epoch 0103, iter [03500, 05004], lr: 0.049839, loss: 2.6507
2022-07-15 19:52:26 - train: epoch 0103, iter [03600, 05004], lr: 0.049823, loss: 2.3174
2022-07-15 19:53:01 - train: epoch 0103, iter [03700, 05004], lr: 0.049807, loss: 2.6661
2022-07-15 19:53:36 - train: epoch 0103, iter [03800, 05004], lr: 0.049791, loss: 2.4598
2022-07-15 19:54:11 - train: epoch 0103, iter [03900, 05004], lr: 0.049775, loss: 2.5276
2022-07-15 19:54:46 - train: epoch 0103, iter [04000, 05004], lr: 0.049759, loss: 2.5373
2022-07-15 19:55:21 - train: epoch 0103, iter [04100, 05004], lr: 0.049743, loss: 2.4812
2022-07-15 19:55:56 - train: epoch 0103, iter [04200, 05004], lr: 0.049727, loss: 2.5784
2022-07-15 19:56:31 - train: epoch 0103, iter [04300, 05004], lr: 0.049711, loss: 2.2398
2022-07-15 19:57:05 - train: epoch 0103, iter [04400, 05004], lr: 0.049694, loss: 2.6412
2022-07-15 19:57:41 - train: epoch 0103, iter [04500, 05004], lr: 0.049678, loss: 2.4915
2022-07-15 19:58:15 - train: epoch 0103, iter [04600, 05004], lr: 0.049662, loss: 2.7234
2022-07-15 19:58:51 - train: epoch 0103, iter [04700, 05004], lr: 0.049646, loss: 2.3476
2022-07-15 19:59:25 - train: epoch 0103, iter [04800, 05004], lr: 0.049630, loss: 2.5309
2022-07-15 20:00:00 - train: epoch 0103, iter [04900, 05004], lr: 0.049614, loss: 2.4756
2022-07-15 20:00:34 - train: epoch 0103, iter [05000, 05004], lr: 0.049598, loss: 2.5782
2022-07-15 20:00:35 - train: epoch 103, train_loss: 2.5300
2022-07-15 20:01:49 - eval: epoch: 103, acc1: 62.776%, acc5: 85.126%, test_loss: 1.5512, per_image_load_time: 1.650ms, per_image_inference_time: 0.492ms
2022-07-15 20:01:49 - until epoch: 103, best_acc1: 63.742%
2022-07-15 20:01:49 - epoch 104 lr: 0.049597
2022-07-15 20:02:29 - train: epoch 0104, iter [00100, 05004], lr: 0.049581, loss: 2.4128
2022-07-15 20:03:04 - train: epoch 0104, iter [00200, 05004], lr: 0.049565, loss: 2.5559
2022-07-15 20:03:38 - train: epoch 0104, iter [00300, 05004], lr: 0.049549, loss: 2.4390
2022-07-15 20:04:12 - train: epoch 0104, iter [00400, 05004], lr: 0.049533, loss: 2.5618
2022-07-15 20:04:47 - train: epoch 0104, iter [00500, 05004], lr: 0.049517, loss: 2.4083
2022-07-15 20:05:22 - train: epoch 0104, iter [00600, 05004], lr: 0.049501, loss: 2.4455
2022-07-15 20:05:57 - train: epoch 0104, iter [00700, 05004], lr: 0.049485, loss: 2.4503
2022-07-15 20:06:31 - train: epoch 0104, iter [00800, 05004], lr: 0.049468, loss: 2.4198
2022-07-15 20:07:05 - train: epoch 0104, iter [00900, 05004], lr: 0.049452, loss: 2.3271
2022-07-15 20:07:38 - train: epoch 0104, iter [01000, 05004], lr: 0.049436, loss: 2.6651
2022-07-15 20:08:11 - train: epoch 0104, iter [01100, 05004], lr: 0.049420, loss: 2.5733
2022-07-15 20:08:44 - train: epoch 0104, iter [01200, 05004], lr: 0.049404, loss: 2.5646
2022-07-15 20:09:17 - train: epoch 0104, iter [01300, 05004], lr: 0.049388, loss: 2.5464
2022-07-15 20:09:50 - train: epoch 0104, iter [01400, 05004], lr: 0.049372, loss: 2.2951
2022-07-15 20:10:25 - train: epoch 0104, iter [01500, 05004], lr: 0.049356, loss: 2.5452
2022-07-15 20:10:58 - train: epoch 0104, iter [01600, 05004], lr: 0.049340, loss: 2.3554
2022-07-15 20:11:33 - train: epoch 0104, iter [01700, 05004], lr: 0.049324, loss: 2.2962
2022-07-15 20:12:08 - train: epoch 0104, iter [01800, 05004], lr: 0.049307, loss: 2.4716
2022-07-15 20:12:42 - train: epoch 0104, iter [01900, 05004], lr: 0.049291, loss: 2.2591
2022-07-15 20:13:16 - train: epoch 0104, iter [02000, 05004], lr: 0.049275, loss: 2.5396
2022-07-15 20:13:51 - train: epoch 0104, iter [02100, 05004], lr: 0.049259, loss: 2.2443
2022-07-15 20:14:26 - train: epoch 0104, iter [02200, 05004], lr: 0.049243, loss: 2.4878
2022-07-15 20:15:01 - train: epoch 0104, iter [02300, 05004], lr: 0.049227, loss: 2.5467
2022-07-15 20:15:35 - train: epoch 0104, iter [02400, 05004], lr: 0.049211, loss: 2.6924
2022-07-15 20:16:11 - train: epoch 0104, iter [02500, 05004], lr: 0.049195, loss: 2.3449
2022-07-15 20:16:45 - train: epoch 0104, iter [02600, 05004], lr: 0.049179, loss: 2.4101
2022-07-15 20:17:20 - train: epoch 0104, iter [02700, 05004], lr: 0.049163, loss: 2.3342
2022-07-15 20:17:54 - train: epoch 0104, iter [02800, 05004], lr: 0.049147, loss: 2.2925
2022-07-15 20:18:30 - train: epoch 0104, iter [02900, 05004], lr: 0.049130, loss: 2.3160
2022-07-15 20:19:04 - train: epoch 0104, iter [03000, 05004], lr: 0.049114, loss: 2.4337
2022-07-15 20:19:38 - train: epoch 0104, iter [03100, 05004], lr: 0.049098, loss: 2.4014
2022-07-15 20:20:12 - train: epoch 0104, iter [03200, 05004], lr: 0.049082, loss: 2.6090
2022-07-15 20:20:48 - train: epoch 0104, iter [03300, 05004], lr: 0.049066, loss: 2.7190
2022-07-15 20:21:23 - train: epoch 0104, iter [03400, 05004], lr: 0.049050, loss: 2.5774
2022-07-15 20:21:58 - train: epoch 0104, iter [03500, 05004], lr: 0.049034, loss: 2.6176
2022-07-15 20:22:33 - train: epoch 0104, iter [03600, 05004], lr: 0.049018, loss: 2.6774
2022-07-15 20:23:08 - train: epoch 0104, iter [03700, 05004], lr: 0.049002, loss: 2.9335
2022-07-15 20:23:43 - train: epoch 0104, iter [03800, 05004], lr: 0.048986, loss: 2.1844
2022-07-15 20:24:17 - train: epoch 0104, iter [03900, 05004], lr: 0.048969, loss: 2.3717
2022-07-15 20:24:52 - train: epoch 0104, iter [04000, 05004], lr: 0.048953, loss: 2.6166
2022-07-15 20:25:27 - train: epoch 0104, iter [04100, 05004], lr: 0.048937, loss: 2.4260
2022-07-15 20:26:02 - train: epoch 0104, iter [04200, 05004], lr: 0.048921, loss: 2.3363
2022-07-15 20:26:36 - train: epoch 0104, iter [04300, 05004], lr: 0.048905, loss: 2.7234
2022-07-15 20:27:10 - train: epoch 0104, iter [04400, 05004], lr: 0.048889, loss: 2.5053
2022-07-15 20:27:45 - train: epoch 0104, iter [04500, 05004], lr: 0.048873, loss: 2.3308
2022-07-15 20:28:20 - train: epoch 0104, iter [04600, 05004], lr: 0.048857, loss: 2.5123
2022-07-15 20:28:54 - train: epoch 0104, iter [04700, 05004], lr: 0.048841, loss: 2.6682
2022-07-15 20:29:28 - train: epoch 0104, iter [04800, 05004], lr: 0.048825, loss: 2.5623
2022-07-15 20:30:03 - train: epoch 0104, iter [04900, 05004], lr: 0.048809, loss: 2.5400
2022-07-15 20:30:36 - train: epoch 0104, iter [05000, 05004], lr: 0.048792, loss: 2.3928
2022-07-15 20:30:37 - train: epoch 104, train_loss: 2.5188
2022-07-15 20:31:53 - eval: epoch: 104, acc1: 61.880%, acc5: 84.872%, test_loss: 1.5895, per_image_load_time: 2.050ms, per_image_inference_time: 0.486ms
2022-07-15 20:31:53 - until epoch: 104, best_acc1: 63.742%
2022-07-15 20:31:53 - epoch 105 lr: 0.048792
2022-07-15 20:32:33 - train: epoch 0105, iter [00100, 05004], lr: 0.048776, loss: 2.9197
2022-07-15 20:33:07 - train: epoch 0105, iter [00200, 05004], lr: 0.048760, loss: 2.5569
2022-07-15 20:33:41 - train: epoch 0105, iter [00300, 05004], lr: 0.048744, loss: 2.5457
2022-07-15 20:34:16 - train: epoch 0105, iter [00400, 05004], lr: 0.048727, loss: 2.5647
2022-07-15 20:34:50 - train: epoch 0105, iter [00500, 05004], lr: 0.048711, loss: 2.5950
2022-07-15 20:35:25 - train: epoch 0105, iter [00600, 05004], lr: 0.048695, loss: 2.3004
2022-07-15 20:35:59 - train: epoch 0105, iter [00700, 05004], lr: 0.048679, loss: 2.4698
2022-07-15 20:36:34 - train: epoch 0105, iter [00800, 05004], lr: 0.048663, loss: 2.5686
2022-07-15 20:37:08 - train: epoch 0105, iter [00900, 05004], lr: 0.048647, loss: 2.2186
2022-07-15 20:37:43 - train: epoch 0105, iter [01000, 05004], lr: 0.048631, loss: 2.3554
2022-07-15 20:38:17 - train: epoch 0105, iter [01100, 05004], lr: 0.048615, loss: 2.8022
2022-07-15 20:38:52 - train: epoch 0105, iter [01200, 05004], lr: 0.048599, loss: 2.4639
2022-07-15 20:39:27 - train: epoch 0105, iter [01300, 05004], lr: 0.048583, loss: 2.3715
2022-07-15 20:40:02 - train: epoch 0105, iter [01400, 05004], lr: 0.048567, loss: 2.6955
2022-07-15 20:40:36 - train: epoch 0105, iter [01500, 05004], lr: 0.048550, loss: 2.3531
2022-07-15 20:41:11 - train: epoch 0105, iter [01600, 05004], lr: 0.048534, loss: 2.1010
2022-07-15 20:41:46 - train: epoch 0105, iter [01700, 05004], lr: 0.048518, loss: 2.3019
2022-07-15 20:42:21 - train: epoch 0105, iter [01800, 05004], lr: 0.048502, loss: 2.8495
2022-07-15 20:42:55 - train: epoch 0105, iter [01900, 05004], lr: 0.048486, loss: 2.5268
2022-07-15 20:43:30 - train: epoch 0105, iter [02000, 05004], lr: 0.048470, loss: 2.5216
2022-07-15 20:44:05 - train: epoch 0105, iter [02100, 05004], lr: 0.048454, loss: 2.4779
2022-07-15 20:44:40 - train: epoch 0105, iter [02200, 05004], lr: 0.048438, loss: 2.8954
2022-07-15 20:45:14 - train: epoch 0105, iter [02300, 05004], lr: 0.048422, loss: 2.4723
2022-07-15 20:45:49 - train: epoch 0105, iter [02400, 05004], lr: 0.048406, loss: 2.3742
2022-07-15 20:46:24 - train: epoch 0105, iter [02500, 05004], lr: 0.048390, loss: 2.3250
2022-07-15 20:46:59 - train: epoch 0105, iter [02600, 05004], lr: 0.048373, loss: 2.4071
2022-07-15 20:47:34 - train: epoch 0105, iter [02700, 05004], lr: 0.048357, loss: 2.6838
2022-07-15 20:48:08 - train: epoch 0105, iter [02800, 05004], lr: 0.048341, loss: 2.5166
2022-07-15 20:48:42 - train: epoch 0105, iter [02900, 05004], lr: 0.048325, loss: 2.5435
2022-07-15 20:49:18 - train: epoch 0105, iter [03000, 05004], lr: 0.048309, loss: 2.5389
2022-07-15 20:49:52 - train: epoch 0105, iter [03100, 05004], lr: 0.048293, loss: 2.6743
2022-07-15 20:50:27 - train: epoch 0105, iter [03200, 05004], lr: 0.048277, loss: 2.9355
2022-07-15 20:51:02 - train: epoch 0105, iter [03300, 05004], lr: 0.048261, loss: 2.2237
2022-07-15 20:51:36 - train: epoch 0105, iter [03400, 05004], lr: 0.048245, loss: 2.4965
2022-07-15 20:52:11 - train: epoch 0105, iter [03500, 05004], lr: 0.048229, loss: 2.6531
2022-07-15 20:52:47 - train: epoch 0105, iter [03600, 05004], lr: 0.048213, loss: 2.4816
2022-07-15 20:53:21 - train: epoch 0105, iter [03700, 05004], lr: 0.048196, loss: 2.5081
2022-07-15 20:53:56 - train: epoch 0105, iter [03800, 05004], lr: 0.048180, loss: 2.5528
2022-07-15 20:54:30 - train: epoch 0105, iter [03900, 05004], lr: 0.048164, loss: 2.5811
2022-07-15 20:55:06 - train: epoch 0105, iter [04000, 05004], lr: 0.048148, loss: 2.5576
2022-07-15 20:55:40 - train: epoch 0105, iter [04100, 05004], lr: 0.048132, loss: 2.6509
2022-07-15 20:56:16 - train: epoch 0105, iter [04200, 05004], lr: 0.048116, loss: 2.4755
2022-07-15 20:56:51 - train: epoch 0105, iter [04300, 05004], lr: 0.048100, loss: 2.9344
2022-07-15 20:57:25 - train: epoch 0105, iter [04400, 05004], lr: 0.048084, loss: 2.4078
2022-07-15 20:58:01 - train: epoch 0105, iter [04500, 05004], lr: 0.048068, loss: 2.9389
2022-07-15 20:58:36 - train: epoch 0105, iter [04600, 05004], lr: 0.048052, loss: 2.6690
2022-07-15 20:59:11 - train: epoch 0105, iter [04700, 05004], lr: 0.048036, loss: 2.7385
2022-07-15 20:59:46 - train: epoch 0105, iter [04800, 05004], lr: 0.048020, loss: 2.4221
2022-07-15 21:00:21 - train: epoch 0105, iter [04900, 05004], lr: 0.048003, loss: 2.6070
2022-07-15 21:00:55 - train: epoch 0105, iter [05000, 05004], lr: 0.047987, loss: 2.3186
2022-07-15 21:00:56 - train: epoch 105, train_loss: 2.5103
2022-07-15 21:02:12 - eval: epoch: 105, acc1: 64.650%, acc5: 86.264%, test_loss: 1.4658, per_image_load_time: 2.078ms, per_image_inference_time: 0.485ms
2022-07-15 21:02:12 - until epoch: 105, best_acc1: 64.650%
2022-07-15 21:02:12 - epoch 106 lr: 0.047987
2022-07-15 21:02:53 - train: epoch 0106, iter [00100, 05004], lr: 0.047971, loss: 2.2312
2022-07-15 21:03:28 - train: epoch 0106, iter [00200, 05004], lr: 0.047955, loss: 2.5212
2022-07-15 21:04:02 - train: epoch 0106, iter [00300, 05004], lr: 0.047938, loss: 1.9540
2022-07-15 21:04:38 - train: epoch 0106, iter [00400, 05004], lr: 0.047922, loss: 2.3642
2022-07-15 21:05:12 - train: epoch 0106, iter [00500, 05004], lr: 0.047906, loss: 2.4035
2022-07-15 21:05:48 - train: epoch 0106, iter [00600, 05004], lr: 0.047890, loss: 2.6268
2022-07-15 21:06:22 - train: epoch 0106, iter [00700, 05004], lr: 0.047874, loss: 2.4374
2022-07-15 21:06:57 - train: epoch 0106, iter [00800, 05004], lr: 0.047858, loss: 2.3495
2022-07-15 21:07:32 - train: epoch 0106, iter [00900, 05004], lr: 0.047842, loss: 2.3546
2022-07-15 21:08:07 - train: epoch 0106, iter [01000, 05004], lr: 0.047826, loss: 2.7487
2022-07-15 21:08:42 - train: epoch 0106, iter [01100, 05004], lr: 0.047810, loss: 2.4278
2022-07-15 21:09:17 - train: epoch 0106, iter [01200, 05004], lr: 0.047794, loss: 2.4089
2022-07-15 21:09:51 - train: epoch 0106, iter [01300, 05004], lr: 0.047778, loss: 2.2993
2022-07-15 21:10:27 - train: epoch 0106, iter [01400, 05004], lr: 0.047762, loss: 2.4490
2022-07-15 21:11:02 - train: epoch 0106, iter [01500, 05004], lr: 0.047745, loss: 2.6216
2022-07-15 21:11:37 - train: epoch 0106, iter [01600, 05004], lr: 0.047729, loss: 2.3301
2022-07-15 21:12:12 - train: epoch 0106, iter [01700, 05004], lr: 0.047713, loss: 2.5811
2022-07-15 21:12:47 - train: epoch 0106, iter [01800, 05004], lr: 0.047697, loss: 2.3415
2022-07-15 21:13:21 - train: epoch 0106, iter [01900, 05004], lr: 0.047681, loss: 2.5446
2022-07-15 21:13:57 - train: epoch 0106, iter [02000, 05004], lr: 0.047665, loss: 2.5026
2022-07-15 21:14:32 - train: epoch 0106, iter [02100, 05004], lr: 0.047649, loss: 2.2491
2022-07-15 21:15:07 - train: epoch 0106, iter [02200, 05004], lr: 0.047633, loss: 2.4688
2022-07-15 21:15:42 - train: epoch 0106, iter [02300, 05004], lr: 0.047617, loss: 2.6384
2022-07-15 21:16:17 - train: epoch 0106, iter [02400, 05004], lr: 0.047601, loss: 2.6054
2022-07-15 21:16:52 - train: epoch 0106, iter [02500, 05004], lr: 0.047585, loss: 2.5631
2022-07-15 21:17:27 - train: epoch 0106, iter [02600, 05004], lr: 0.047569, loss: 2.4691
2022-07-15 21:18:02 - train: epoch 0106, iter [02700, 05004], lr: 0.047552, loss: 2.6493
2022-07-15 21:18:37 - train: epoch 0106, iter [02800, 05004], lr: 0.047536, loss: 2.4463
2022-07-15 21:19:12 - train: epoch 0106, iter [02900, 05004], lr: 0.047520, loss: 2.2276
2022-07-15 21:19:47 - train: epoch 0106, iter [03000, 05004], lr: 0.047504, loss: 2.6231
2022-07-15 21:20:22 - train: epoch 0106, iter [03100, 05004], lr: 0.047488, loss: 2.6622
2022-07-15 21:20:58 - train: epoch 0106, iter [03200, 05004], lr: 0.047472, loss: 2.4449
2022-07-15 21:21:33 - train: epoch 0106, iter [03300, 05004], lr: 0.047456, loss: 2.5576
2022-07-15 21:22:08 - train: epoch 0106, iter [03400, 05004], lr: 0.047440, loss: 2.3882
2022-07-15 21:22:44 - train: epoch 0106, iter [03500, 05004], lr: 0.047424, loss: 2.3611
2022-07-15 21:23:19 - train: epoch 0106, iter [03600, 05004], lr: 0.047408, loss: 2.4370
2022-07-15 21:23:54 - train: epoch 0106, iter [03700, 05004], lr: 0.047392, loss: 2.4223
2022-07-15 21:24:30 - train: epoch 0106, iter [03800, 05004], lr: 0.047376, loss: 2.6760
2022-07-15 21:25:05 - train: epoch 0106, iter [03900, 05004], lr: 0.047360, loss: 2.4781
2022-07-15 21:25:40 - train: epoch 0106, iter [04000, 05004], lr: 0.047343, loss: 2.3595
2022-07-15 21:26:14 - train: epoch 0106, iter [04100, 05004], lr: 0.047327, loss: 2.6962
2022-07-15 21:26:50 - train: epoch 0106, iter [04200, 05004], lr: 0.047311, loss: 2.6039
2022-07-15 21:27:25 - train: epoch 0106, iter [04300, 05004], lr: 0.047295, loss: 2.2123
2022-07-15 21:28:01 - train: epoch 0106, iter [04400, 05004], lr: 0.047279, loss: 2.4189
2022-07-15 21:28:36 - train: epoch 0106, iter [04500, 05004], lr: 0.047263, loss: 2.6426
2022-07-15 21:29:11 - train: epoch 0106, iter [04600, 05004], lr: 0.047247, loss: 2.5738
2022-07-15 21:29:47 - train: epoch 0106, iter [04700, 05004], lr: 0.047231, loss: 2.7709
2022-07-15 21:30:22 - train: epoch 0106, iter [04800, 05004], lr: 0.047215, loss: 2.5805
2022-07-15 21:30:58 - train: epoch 0106, iter [04900, 05004], lr: 0.047199, loss: 2.2381
2022-07-15 21:31:31 - train: epoch 0106, iter [05000, 05004], lr: 0.047183, loss: 2.4854
2022-07-15 21:31:32 - train: epoch 106, train_loss: 2.5057
2022-07-15 21:32:48 - eval: epoch: 106, acc1: 62.754%, acc5: 85.438%, test_loss: 1.5363, per_image_load_time: 2.161ms, per_image_inference_time: 0.478ms
2022-07-15 21:32:48 - until epoch: 106, best_acc1: 64.650%
2022-07-15 21:32:48 - epoch 107 lr: 0.047182
2022-07-15 21:33:28 - train: epoch 0107, iter [00100, 05004], lr: 0.047166, loss: 2.2082
2022-07-15 21:34:03 - train: epoch 0107, iter [00200, 05004], lr: 0.047150, loss: 2.7632
2022-07-15 21:34:37 - train: epoch 0107, iter [00300, 05004], lr: 0.047134, loss: 2.3642
2022-07-15 21:35:12 - train: epoch 0107, iter [00400, 05004], lr: 0.047118, loss: 2.6383
2022-07-15 21:35:47 - train: epoch 0107, iter [00500, 05004], lr: 0.047102, loss: 2.2058
2022-07-15 21:36:22 - train: epoch 0107, iter [00600, 05004], lr: 0.047086, loss: 2.5888
2022-07-15 21:36:57 - train: epoch 0107, iter [00700, 05004], lr: 0.047070, loss: 2.2441
2022-07-15 21:37:32 - train: epoch 0107, iter [00800, 05004], lr: 0.047054, loss: 2.3729
2022-07-15 21:38:06 - train: epoch 0107, iter [00900, 05004], lr: 0.047037, loss: 2.5577
2022-07-15 21:38:42 - train: epoch 0107, iter [01000, 05004], lr: 0.047021, loss: 2.4426
2022-07-15 21:39:16 - train: epoch 0107, iter [01100, 05004], lr: 0.047005, loss: 2.4520
2022-07-15 21:39:52 - train: epoch 0107, iter [01200, 05004], lr: 0.046989, loss: 2.4520
2022-07-15 21:40:26 - train: epoch 0107, iter [01300, 05004], lr: 0.046973, loss: 2.2720
2022-07-15 21:41:02 - train: epoch 0107, iter [01400, 05004], lr: 0.046957, loss: 2.2436
2022-07-15 21:41:36 - train: epoch 0107, iter [01500, 05004], lr: 0.046941, loss: 2.3872
2022-07-15 21:42:12 - train: epoch 0107, iter [01600, 05004], lr: 0.046925, loss: 2.4254
2022-07-15 21:42:47 - train: epoch 0107, iter [01700, 05004], lr: 0.046909, loss: 2.4565
2022-07-15 21:43:22 - train: epoch 0107, iter [01800, 05004], lr: 0.046893, loss: 2.3107
2022-07-15 21:43:57 - train: epoch 0107, iter [01900, 05004], lr: 0.046877, loss: 2.6171
2022-07-15 21:44:32 - train: epoch 0107, iter [02000, 05004], lr: 0.046861, loss: 2.5912
2022-07-15 21:45:08 - train: epoch 0107, iter [02100, 05004], lr: 0.046845, loss: 2.6965
2022-07-15 21:45:43 - train: epoch 0107, iter [02200, 05004], lr: 0.046829, loss: 2.6113
2022-07-15 21:46:19 - train: epoch 0107, iter [02300, 05004], lr: 0.046813, loss: 2.3251
2022-07-15 21:46:54 - train: epoch 0107, iter [02400, 05004], lr: 0.046796, loss: 2.3764
2022-07-15 21:47:29 - train: epoch 0107, iter [02500, 05004], lr: 0.046780, loss: 2.4674
2022-07-15 21:48:04 - train: epoch 0107, iter [02600, 05004], lr: 0.046764, loss: 2.8979
2022-07-15 21:48:40 - train: epoch 0107, iter [02700, 05004], lr: 0.046748, loss: 2.3103
2022-07-15 21:49:15 - train: epoch 0107, iter [02800, 05004], lr: 0.046732, loss: 2.5128
2022-07-15 21:49:50 - train: epoch 0107, iter [02900, 05004], lr: 0.046716, loss: 2.6419
2022-07-15 21:50:25 - train: epoch 0107, iter [03000, 05004], lr: 0.046700, loss: 2.4611
2022-07-15 21:51:01 - train: epoch 0107, iter [03100, 05004], lr: 0.046684, loss: 2.5684
2022-07-15 21:51:36 - train: epoch 0107, iter [03200, 05004], lr: 0.046668, loss: 2.3925
2022-07-15 21:52:11 - train: epoch 0107, iter [03300, 05004], lr: 0.046652, loss: 2.4439
2022-07-15 21:52:46 - train: epoch 0107, iter [03400, 05004], lr: 0.046636, loss: 2.3324
2022-07-15 21:53:22 - train: epoch 0107, iter [03500, 05004], lr: 0.046620, loss: 2.4373
2022-07-15 21:53:58 - train: epoch 0107, iter [03600, 05004], lr: 0.046604, loss: 2.5189
2022-07-15 21:54:32 - train: epoch 0107, iter [03700, 05004], lr: 0.046588, loss: 2.3955
2022-07-15 21:55:07 - train: epoch 0107, iter [03800, 05004], lr: 0.046572, loss: 2.3307
2022-07-15 21:55:42 - train: epoch 0107, iter [03900, 05004], lr: 0.046556, loss: 2.4097
2022-07-15 21:56:16 - train: epoch 0107, iter [04000, 05004], lr: 0.046539, loss: 2.5808
2022-07-15 21:56:51 - train: epoch 0107, iter [04100, 05004], lr: 0.046523, loss: 2.2983
2022-07-15 21:57:26 - train: epoch 0107, iter [04200, 05004], lr: 0.046507, loss: 2.7418
2022-07-15 21:58:01 - train: epoch 0107, iter [04300, 05004], lr: 0.046491, loss: 2.6496
2022-07-15 21:58:36 - train: epoch 0107, iter [04400, 05004], lr: 0.046475, loss: 2.5730
2022-07-15 21:59:11 - train: epoch 0107, iter [04500, 05004], lr: 0.046459, loss: 2.7747
2022-07-15 21:59:46 - train: epoch 0107, iter [04600, 05004], lr: 0.046443, loss: 2.3509
2022-07-15 22:00:21 - train: epoch 0107, iter [04700, 05004], lr: 0.046427, loss: 2.5878
2022-07-15 22:00:55 - train: epoch 0107, iter [04800, 05004], lr: 0.046411, loss: 2.3676
2022-07-15 22:01:30 - train: epoch 0107, iter [04900, 05004], lr: 0.046395, loss: 2.7426
2022-07-15 22:02:03 - train: epoch 0107, iter [05000, 05004], lr: 0.046379, loss: 2.7370
2022-07-15 22:02:04 - train: epoch 107, train_loss: 2.5009
2022-07-15 22:03:19 - eval: epoch: 107, acc1: 62.162%, acc5: 85.012%, test_loss: 1.5658, per_image_load_time: 1.729ms, per_image_inference_time: 0.491ms
2022-07-15 22:03:19 - until epoch: 107, best_acc1: 64.650%
2022-07-15 22:03:19 - epoch 108 lr: 0.046378
2022-07-15 22:03:58 - train: epoch 0108, iter [00100, 05004], lr: 0.046362, loss: 2.1225
2022-07-15 22:04:33 - train: epoch 0108, iter [00200, 05004], lr: 0.046346, loss: 2.2260
2022-07-15 22:05:07 - train: epoch 0108, iter [00300, 05004], lr: 0.046330, loss: 2.7737
2022-07-15 22:05:42 - train: epoch 0108, iter [00400, 05004], lr: 0.046314, loss: 2.3735
2022-07-15 22:06:17 - train: epoch 0108, iter [00500, 05004], lr: 0.046298, loss: 2.5789
2022-07-15 22:06:52 - train: epoch 0108, iter [00600, 05004], lr: 0.046282, loss: 2.8205
2022-07-15 22:07:27 - train: epoch 0108, iter [00700, 05004], lr: 0.046266, loss: 2.2739
2022-07-15 22:08:01 - train: epoch 0108, iter [00800, 05004], lr: 0.046250, loss: 2.2867
2022-07-15 22:08:36 - train: epoch 0108, iter [00900, 05004], lr: 0.046234, loss: 2.4633
2022-07-15 22:09:09 - train: epoch 0108, iter [01000, 05004], lr: 0.046218, loss: 2.5465
2022-07-15 22:09:45 - train: epoch 0108, iter [01100, 05004], lr: 0.046202, loss: 2.4661
2022-07-15 22:10:19 - train: epoch 0108, iter [01200, 05004], lr: 0.046186, loss: 2.7084
2022-07-15 22:10:54 - train: epoch 0108, iter [01300, 05004], lr: 0.046170, loss: 2.7083
2022-07-15 22:11:29 - train: epoch 0108, iter [01400, 05004], lr: 0.046154, loss: 2.4621
2022-07-15 22:12:04 - train: epoch 0108, iter [01500, 05004], lr: 0.046137, loss: 2.4137
2022-07-15 22:12:38 - train: epoch 0108, iter [01600, 05004], lr: 0.046121, loss: 2.5171
2022-07-15 22:13:13 - train: epoch 0108, iter [01700, 05004], lr: 0.046105, loss: 2.3754
2022-07-15 22:13:48 - train: epoch 0108, iter [01800, 05004], lr: 0.046089, loss: 2.3111
2022-07-15 22:14:22 - train: epoch 0108, iter [01900, 05004], lr: 0.046073, loss: 2.2612
2022-07-15 22:14:57 - train: epoch 0108, iter [02000, 05004], lr: 0.046057, loss: 2.5881
2022-07-15 22:15:32 - train: epoch 0108, iter [02100, 05004], lr: 0.046041, loss: 2.6048
2022-07-15 22:16:07 - train: epoch 0108, iter [02200, 05004], lr: 0.046025, loss: 2.4517
2022-07-15 22:16:41 - train: epoch 0108, iter [02300, 05004], lr: 0.046009, loss: 2.6475
2022-07-15 22:17:17 - train: epoch 0108, iter [02400, 05004], lr: 0.045993, loss: 2.3597
2022-07-15 22:17:51 - train: epoch 0108, iter [02500, 05004], lr: 0.045977, loss: 2.5549
2022-07-15 22:18:25 - train: epoch 0108, iter [02600, 05004], lr: 0.045961, loss: 2.6403
2022-07-15 22:19:00 - train: epoch 0108, iter [02700, 05004], lr: 0.045945, loss: 2.3738
2022-07-15 22:19:34 - train: epoch 0108, iter [02800, 05004], lr: 0.045929, loss: 2.4293
2022-07-15 22:20:08 - train: epoch 0108, iter [02900, 05004], lr: 0.045913, loss: 2.3669
2022-07-15 22:20:41 - train: epoch 0108, iter [03000, 05004], lr: 0.045897, loss: 2.5927
2022-07-15 22:21:14 - train: epoch 0108, iter [03100, 05004], lr: 0.045881, loss: 2.5693
2022-07-15 22:21:49 - train: epoch 0108, iter [03200, 05004], lr: 0.045865, loss: 2.1962
2022-07-15 22:22:21 - train: epoch 0108, iter [03300, 05004], lr: 0.045849, loss: 2.2143
2022-07-15 22:22:54 - train: epoch 0108, iter [03400, 05004], lr: 0.045833, loss: 2.5985
2022-07-15 22:23:28 - train: epoch 0108, iter [03500, 05004], lr: 0.045817, loss: 2.3612
2022-07-15 22:24:01 - train: epoch 0108, iter [03600, 05004], lr: 0.045801, loss: 2.3141
2022-07-15 22:24:35 - train: epoch 0108, iter [03700, 05004], lr: 0.045784, loss: 2.5853
2022-07-15 22:25:08 - train: epoch 0108, iter [03800, 05004], lr: 0.045768, loss: 2.3242
2022-07-15 22:25:42 - train: epoch 0108, iter [03900, 05004], lr: 0.045752, loss: 2.4557
2022-07-15 22:26:16 - train: epoch 0108, iter [04000, 05004], lr: 0.045736, loss: 2.5529
2022-07-15 22:26:49 - train: epoch 0108, iter [04100, 05004], lr: 0.045720, loss: 2.5654
2022-07-15 22:27:22 - train: epoch 0108, iter [04200, 05004], lr: 0.045704, loss: 2.5449
2022-07-15 22:27:56 - train: epoch 0108, iter [04300, 05004], lr: 0.045688, loss: 2.4128
2022-07-15 22:28:30 - train: epoch 0108, iter [04400, 05004], lr: 0.045672, loss: 2.5692
2022-07-15 22:29:04 - train: epoch 0108, iter [04500, 05004], lr: 0.045656, loss: 2.5543
2022-07-15 22:29:37 - train: epoch 0108, iter [04600, 05004], lr: 0.045640, loss: 2.5218
2022-07-15 22:30:11 - train: epoch 0108, iter [04700, 05004], lr: 0.045624, loss: 2.5899
2022-07-15 22:30:45 - train: epoch 0108, iter [04800, 05004], lr: 0.045608, loss: 2.5358
2022-07-15 22:31:19 - train: epoch 0108, iter [04900, 05004], lr: 0.045592, loss: 2.6191
2022-07-15 22:31:51 - train: epoch 0108, iter [05000, 05004], lr: 0.045576, loss: 2.2591
2022-07-15 22:31:52 - train: epoch 108, train_loss: 2.4924
2022-07-15 22:33:05 - eval: epoch: 108, acc1: 64.044%, acc5: 85.944%, test_loss: 1.4927, per_image_load_time: 0.857ms, per_image_inference_time: 0.415ms
2022-07-15 22:33:06 - until epoch: 108, best_acc1: 64.650%
2022-07-15 22:33:06 - epoch 109 lr: 0.045575
2022-07-15 22:33:44 - train: epoch 0109, iter [00100, 05004], lr: 0.045559, loss: 2.3787
2022-07-15 22:34:18 - train: epoch 0109, iter [00200, 05004], lr: 0.045543, loss: 2.2895
2022-07-15 22:34:51 - train: epoch 0109, iter [00300, 05004], lr: 0.045527, loss: 2.5707
2022-07-15 22:35:24 - train: epoch 0109, iter [00400, 05004], lr: 0.045511, loss: 2.1495
2022-07-15 22:35:58 - train: epoch 0109, iter [00500, 05004], lr: 0.045495, loss: 2.5346
2022-07-15 22:36:31 - train: epoch 0109, iter [00600, 05004], lr: 0.045479, loss: 2.4664
2022-07-15 22:37:04 - train: epoch 0109, iter [00700, 05004], lr: 0.045463, loss: 2.5743
2022-07-15 22:37:38 - train: epoch 0109, iter [00800, 05004], lr: 0.045447, loss: 2.5062
2022-07-15 22:38:11 - train: epoch 0109, iter [00900, 05004], lr: 0.045431, loss: 2.5067
2022-07-15 22:38:45 - train: epoch 0109, iter [01000, 05004], lr: 0.045415, loss: 2.1975
2022-07-15 22:39:18 - train: epoch 0109, iter [01100, 05004], lr: 0.045399, loss: 2.7279
2022-07-15 22:39:51 - train: epoch 0109, iter [01200, 05004], lr: 0.045383, loss: 2.6473
2022-07-15 22:40:26 - train: epoch 0109, iter [01300, 05004], lr: 0.045367, loss: 2.3498
2022-07-15 22:40:58 - train: epoch 0109, iter [01400, 05004], lr: 0.045351, loss: 2.5092
2022-07-15 22:41:32 - train: epoch 0109, iter [01500, 05004], lr: 0.045335, loss: 2.7072
2022-07-15 22:42:05 - train: epoch 0109, iter [01600, 05004], lr: 0.045319, loss: 2.0925
2022-07-15 22:42:39 - train: epoch 0109, iter [01700, 05004], lr: 0.045303, loss: 2.6752
2022-07-15 22:43:13 - train: epoch 0109, iter [01800, 05004], lr: 0.045287, loss: 2.4932
2022-07-15 22:43:46 - train: epoch 0109, iter [01900, 05004], lr: 0.045271, loss: 2.6309
2022-07-15 22:44:20 - train: epoch 0109, iter [02000, 05004], lr: 0.045255, loss: 2.7258
2022-07-15 22:44:53 - train: epoch 0109, iter [02100, 05004], lr: 0.045239, loss: 2.4715
2022-07-15 22:45:26 - train: epoch 0109, iter [02200, 05004], lr: 0.045223, loss: 2.3777
2022-07-15 22:46:00 - train: epoch 0109, iter [02300, 05004], lr: 0.045207, loss: 2.4570
2022-07-15 22:46:33 - train: epoch 0109, iter [02400, 05004], lr: 0.045191, loss: 2.5307
2022-07-15 22:47:08 - train: epoch 0109, iter [02500, 05004], lr: 0.045175, loss: 2.6977
2022-07-15 22:47:41 - train: epoch 0109, iter [02600, 05004], lr: 0.045159, loss: 2.4863
2022-07-15 22:48:15 - train: epoch 0109, iter [02700, 05004], lr: 0.045143, loss: 2.4245
2022-07-15 22:48:48 - train: epoch 0109, iter [02800, 05004], lr: 0.045127, loss: 2.4143
2022-07-15 22:49:22 - train: epoch 0109, iter [02900, 05004], lr: 0.045111, loss: 2.4747
2022-07-15 22:49:55 - train: epoch 0109, iter [03000, 05004], lr: 0.045095, loss: 2.7032
2022-07-15 22:50:29 - train: epoch 0109, iter [03100, 05004], lr: 0.045078, loss: 2.4487
2022-07-15 22:51:01 - train: epoch 0109, iter [03200, 05004], lr: 0.045062, loss: 2.6180
2022-07-15 22:51:35 - train: epoch 0109, iter [03300, 05004], lr: 0.045046, loss: 2.5287
2022-07-15 22:52:09 - train: epoch 0109, iter [03400, 05004], lr: 0.045030, loss: 2.3308
2022-07-15 22:52:42 - train: epoch 0109, iter [03500, 05004], lr: 0.045014, loss: 2.5222
2022-07-15 22:53:16 - train: epoch 0109, iter [03600, 05004], lr: 0.044998, loss: 2.2473
2022-07-15 22:53:49 - train: epoch 0109, iter [03700, 05004], lr: 0.044982, loss: 2.2285
2022-07-15 22:54:22 - train: epoch 0109, iter [03800, 05004], lr: 0.044966, loss: 2.6695
2022-07-15 22:54:56 - train: epoch 0109, iter [03900, 05004], lr: 0.044950, loss: 2.5447
2022-07-15 22:55:29 - train: epoch 0109, iter [04000, 05004], lr: 0.044934, loss: 2.7309
2022-07-15 22:56:03 - train: epoch 0109, iter [04100, 05004], lr: 0.044918, loss: 2.3935
2022-07-15 22:56:36 - train: epoch 0109, iter [04200, 05004], lr: 0.044902, loss: 2.3713
2022-07-15 22:57:09 - train: epoch 0109, iter [04300, 05004], lr: 0.044886, loss: 2.4744
2022-07-15 22:57:43 - train: epoch 0109, iter [04400, 05004], lr: 0.044870, loss: 2.4372
2022-07-15 22:58:16 - train: epoch 0109, iter [04500, 05004], lr: 0.044854, loss: 2.3387
2022-07-15 22:58:50 - train: epoch 0109, iter [04600, 05004], lr: 0.044838, loss: 2.4767
2022-07-15 22:59:23 - train: epoch 0109, iter [04700, 05004], lr: 0.044822, loss: 2.2737
2022-07-15 22:59:56 - train: epoch 0109, iter [04800, 05004], lr: 0.044806, loss: 2.6056
2022-07-15 23:00:30 - train: epoch 0109, iter [04900, 05004], lr: 0.044790, loss: 2.2882
2022-07-15 23:01:02 - train: epoch 0109, iter [05000, 05004], lr: 0.044774, loss: 2.4046
2022-07-15 23:01:03 - train: epoch 109, train_loss: 2.4863
2022-07-15 23:02:16 - eval: epoch: 109, acc1: 63.316%, acc5: 85.782%, test_loss: 1.5137, per_image_load_time: 1.110ms, per_image_inference_time: 0.429ms
2022-07-15 23:02:16 - until epoch: 109, best_acc1: 64.650%
2022-07-15 23:02:16 - epoch 110 lr: 0.044773
2022-07-15 23:02:56 - train: epoch 0110, iter [00100, 05004], lr: 0.044758, loss: 2.3088
2022-07-15 23:03:29 - train: epoch 0110, iter [00200, 05004], lr: 0.044742, loss: 2.2031
2022-07-15 23:04:02 - train: epoch 0110, iter [00300, 05004], lr: 0.044726, loss: 2.7246
2022-07-15 23:04:36 - train: epoch 0110, iter [00400, 05004], lr: 0.044710, loss: 2.3817
2022-07-15 23:05:08 - train: epoch 0110, iter [00500, 05004], lr: 0.044694, loss: 2.8064
2022-07-15 23:05:42 - train: epoch 0110, iter [00600, 05004], lr: 0.044678, loss: 2.3984
2022-07-15 23:06:16 - train: epoch 0110, iter [00700, 05004], lr: 0.044662, loss: 2.7592
2022-07-15 23:06:49 - train: epoch 0110, iter [00800, 05004], lr: 0.044646, loss: 2.8649
2022-07-15 23:07:22 - train: epoch 0110, iter [00900, 05004], lr: 0.044630, loss: 2.2576
2022-07-15 23:07:56 - train: epoch 0110, iter [01000, 05004], lr: 0.044614, loss: 2.4958
2022-07-15 23:08:30 - train: epoch 0110, iter [01100, 05004], lr: 0.044598, loss: 2.6790
2022-07-15 23:09:03 - train: epoch 0110, iter [01200, 05004], lr: 0.044582, loss: 2.5641
2022-07-15 23:09:38 - train: epoch 0110, iter [01300, 05004], lr: 0.044565, loss: 2.3207
2022-07-15 23:10:11 - train: epoch 0110, iter [01400, 05004], lr: 0.044549, loss: 2.5104
2022-07-15 23:10:45 - train: epoch 0110, iter [01500, 05004], lr: 0.044533, loss: 2.3168
2022-07-15 23:11:19 - train: epoch 0110, iter [01600, 05004], lr: 0.044517, loss: 2.7172
2022-07-15 23:11:52 - train: epoch 0110, iter [01700, 05004], lr: 0.044501, loss: 2.4736
2022-07-15 23:12:26 - train: epoch 0110, iter [01800, 05004], lr: 0.044485, loss: 2.6639
2022-07-15 23:13:00 - train: epoch 0110, iter [01900, 05004], lr: 0.044469, loss: 2.6300
2022-07-15 23:13:34 - train: epoch 0110, iter [02000, 05004], lr: 0.044453, loss: 2.4195
2022-07-15 23:14:07 - train: epoch 0110, iter [02100, 05004], lr: 0.044437, loss: 2.7367
2022-07-15 23:14:40 - train: epoch 0110, iter [02200, 05004], lr: 0.044421, loss: 2.4225
2022-07-15 23:15:14 - train: epoch 0110, iter [02300, 05004], lr: 0.044406, loss: 2.4362
2022-07-15 23:15:48 - train: epoch 0110, iter [02400, 05004], lr: 0.044390, loss: 2.4862
2022-07-15 23:16:22 - train: epoch 0110, iter [02500, 05004], lr: 0.044374, loss: 2.4266
2022-07-15 23:16:56 - train: epoch 0110, iter [02600, 05004], lr: 0.044358, loss: 2.5165
2022-07-15 23:17:30 - train: epoch 0110, iter [02700, 05004], lr: 0.044342, loss: 2.5389
2022-07-15 23:18:03 - train: epoch 0110, iter [02800, 05004], lr: 0.044326, loss: 2.4602
2022-07-15 23:18:38 - train: epoch 0110, iter [02900, 05004], lr: 0.044310, loss: 2.5478
2022-07-15 23:19:11 - train: epoch 0110, iter [03000, 05004], lr: 0.044294, loss: 2.4173
2022-07-15 23:19:45 - train: epoch 0110, iter [03100, 05004], lr: 0.044278, loss: 2.3645
2022-07-15 23:20:19 - train: epoch 0110, iter [03200, 05004], lr: 0.044262, loss: 2.6797
2022-07-15 23:20:52 - train: epoch 0110, iter [03300, 05004], lr: 0.044246, loss: 2.2588
2022-07-15 23:21:26 - train: epoch 0110, iter [03400, 05004], lr: 0.044230, loss: 2.4049
2022-07-15 23:22:00 - train: epoch 0110, iter [03500, 05004], lr: 0.044214, loss: 2.9052
2022-07-15 23:22:33 - train: epoch 0110, iter [03600, 05004], lr: 0.044198, loss: 2.2247
2022-07-15 23:23:08 - train: epoch 0110, iter [03700, 05004], lr: 0.044182, loss: 2.4772
2022-07-15 23:23:41 - train: epoch 0110, iter [03800, 05004], lr: 0.044166, loss: 2.6005
2022-07-15 23:24:15 - train: epoch 0110, iter [03900, 05004], lr: 0.044150, loss: 2.2658
2022-07-15 23:24:48 - train: epoch 0110, iter [04000, 05004], lr: 0.044134, loss: 2.4011
2022-07-15 23:25:22 - train: epoch 0110, iter [04100, 05004], lr: 0.044118, loss: 2.6033
2022-07-15 23:25:55 - train: epoch 0110, iter [04200, 05004], lr: 0.044102, loss: 2.3925
2022-07-15 23:26:29 - train: epoch 0110, iter [04300, 05004], lr: 0.044086, loss: 2.3669
2022-07-15 23:27:03 - train: epoch 0110, iter [04400, 05004], lr: 0.044070, loss: 2.3579
2022-07-15 23:27:36 - train: epoch 0110, iter [04500, 05004], lr: 0.044054, loss: 2.4029
2022-07-15 23:28:10 - train: epoch 0110, iter [04600, 05004], lr: 0.044038, loss: 2.2616
2022-07-15 23:28:44 - train: epoch 0110, iter [04700, 05004], lr: 0.044022, loss: 2.6688
2022-07-15 23:29:18 - train: epoch 0110, iter [04800, 05004], lr: 0.044006, loss: 2.2662
2022-07-15 23:29:51 - train: epoch 0110, iter [04900, 05004], lr: 0.043990, loss: 2.6225
2022-07-15 23:30:23 - train: epoch 0110, iter [05000, 05004], lr: 0.043974, loss: 2.4921
2022-07-15 23:30:25 - train: epoch 110, train_loss: 2.4764
2022-07-15 23:31:38 - eval: epoch: 110, acc1: 63.920%, acc5: 86.240%, test_loss: 1.4826, per_image_load_time: 0.869ms, per_image_inference_time: 0.430ms
2022-07-15 23:31:39 - until epoch: 110, best_acc1: 64.650%
2022-07-15 23:31:39 - epoch 111 lr: 0.043973
2022-07-15 23:32:18 - train: epoch 0111, iter [00100, 05004], lr: 0.043957, loss: 2.5910
2022-07-15 23:32:52 - train: epoch 0111, iter [00200, 05004], lr: 0.043941, loss: 2.7895
2022-07-15 23:33:25 - train: epoch 0111, iter [00300, 05004], lr: 0.043925, loss: 2.4855
2022-07-15 23:33:58 - train: epoch 0111, iter [00400, 05004], lr: 0.043909, loss: 2.3393
2022-07-15 23:34:32 - train: epoch 0111, iter [00500, 05004], lr: 0.043893, loss: 2.3991
2022-07-15 23:35:06 - train: epoch 0111, iter [00600, 05004], lr: 0.043877, loss: 2.3235
2022-07-15 23:35:39 - train: epoch 0111, iter [00700, 05004], lr: 0.043861, loss: 2.8491
2022-07-15 23:36:13 - train: epoch 0111, iter [00800, 05004], lr: 0.043845, loss: 2.3323
2022-07-15 23:36:46 - train: epoch 0111, iter [00900, 05004], lr: 0.043829, loss: 2.7336
2022-07-15 23:37:19 - train: epoch 0111, iter [01000, 05004], lr: 0.043813, loss: 2.3455
2022-07-15 23:37:54 - train: epoch 0111, iter [01100, 05004], lr: 0.043797, loss: 2.2714
2022-07-15 23:38:27 - train: epoch 0111, iter [01200, 05004], lr: 0.043781, loss: 2.4894
2022-07-15 23:39:00 - train: epoch 0111, iter [01300, 05004], lr: 0.043765, loss: 2.5479
2022-07-15 23:39:35 - train: epoch 0111, iter [01400, 05004], lr: 0.043750, loss: 2.2849
2022-07-15 23:40:08 - train: epoch 0111, iter [01500, 05004], lr: 0.043734, loss: 2.4259
2022-07-15 23:40:41 - train: epoch 0111, iter [01600, 05004], lr: 0.043718, loss: 2.4479
2022-07-15 23:41:15 - train: epoch 0111, iter [01700, 05004], lr: 0.043702, loss: 2.3243
2022-07-15 23:41:48 - train: epoch 0111, iter [01800, 05004], lr: 0.043686, loss: 2.0497
2022-07-15 23:42:22 - train: epoch 0111, iter [01900, 05004], lr: 0.043670, loss: 2.3903
2022-07-15 23:42:56 - train: epoch 0111, iter [02000, 05004], lr: 0.043654, loss: 2.5836
2022-07-15 23:43:29 - train: epoch 0111, iter [02100, 05004], lr: 0.043638, loss: 2.8200
2022-07-15 23:44:03 - train: epoch 0111, iter [02200, 05004], lr: 0.043622, loss: 2.6300
2022-07-15 23:44:37 - train: epoch 0111, iter [02300, 05004], lr: 0.043606, loss: 2.5819
2022-07-15 23:45:11 - train: epoch 0111, iter [02400, 05004], lr: 0.043590, loss: 2.5467
2022-07-15 23:45:46 - train: epoch 0111, iter [02500, 05004], lr: 0.043574, loss: 2.3771
2022-07-15 23:46:20 - train: epoch 0111, iter [02600, 05004], lr: 0.043558, loss: 2.4841
2022-07-15 23:46:54 - train: epoch 0111, iter [02700, 05004], lr: 0.043542, loss: 2.2736
2022-07-15 23:47:27 - train: epoch 0111, iter [02800, 05004], lr: 0.043526, loss: 2.3294
2022-07-15 23:48:00 - train: epoch 0111, iter [02900, 05004], lr: 0.043510, loss: 2.3991
2022-07-15 23:48:33 - train: epoch 0111, iter [03000, 05004], lr: 0.043494, loss: 2.5095
2022-07-15 23:49:06 - train: epoch 0111, iter [03100, 05004], lr: 0.043478, loss: 2.2933
2022-07-15 23:49:40 - train: epoch 0111, iter [03200, 05004], lr: 0.043462, loss: 2.7353
2022-07-15 23:50:12 - train: epoch 0111, iter [03300, 05004], lr: 0.043446, loss: 2.5709
2022-07-15 23:50:45 - train: epoch 0111, iter [03400, 05004], lr: 0.043430, loss: 2.8473
2022-07-15 23:51:18 - train: epoch 0111, iter [03500, 05004], lr: 0.043414, loss: 2.5316
2022-07-15 23:51:52 - train: epoch 0111, iter [03600, 05004], lr: 0.043398, loss: 2.2356
2022-07-15 23:52:24 - train: epoch 0111, iter [03700, 05004], lr: 0.043382, loss: 2.3638
2022-07-15 23:52:58 - train: epoch 0111, iter [03800, 05004], lr: 0.043366, loss: 2.4193
2022-07-15 23:53:30 - train: epoch 0111, iter [03900, 05004], lr: 0.043350, loss: 2.1856
2022-07-15 23:54:04 - train: epoch 0111, iter [04000, 05004], lr: 0.043334, loss: 2.7022
2022-07-15 23:54:37 - train: epoch 0111, iter [04100, 05004], lr: 0.043319, loss: 2.7098
2022-07-15 23:55:11 - train: epoch 0111, iter [04200, 05004], lr: 0.043303, loss: 2.1344
2022-07-15 23:55:43 - train: epoch 0111, iter [04300, 05004], lr: 0.043287, loss: 2.4379
2022-07-15 23:56:17 - train: epoch 0111, iter [04400, 05004], lr: 0.043271, loss: 2.5905
2022-07-15 23:56:51 - train: epoch 0111, iter [04500, 05004], lr: 0.043255, loss: 2.6518
2022-07-15 23:57:24 - train: epoch 0111, iter [04600, 05004], lr: 0.043239, loss: 2.2631
2022-07-15 23:57:57 - train: epoch 0111, iter [04700, 05004], lr: 0.043223, loss: 2.4909
2022-07-15 23:58:29 - train: epoch 0111, iter [04800, 05004], lr: 0.043207, loss: 2.4321
2022-07-15 23:59:03 - train: epoch 0111, iter [04900, 05004], lr: 0.043191, loss: 2.5174
2022-07-15 23:59:35 - train: epoch 0111, iter [05000, 05004], lr: 0.043175, loss: 2.8744
2022-07-15 23:59:36 - train: epoch 111, train_loss: 2.4709
2022-07-16 00:00:49 - eval: epoch: 111, acc1: 64.030%, acc5: 86.012%, test_loss: 1.4894, per_image_load_time: 2.049ms, per_image_inference_time: 0.434ms
2022-07-16 00:00:49 - until epoch: 111, best_acc1: 64.650%
2022-07-16 00:00:49 - epoch 112 lr: 0.043174
2022-07-16 00:01:28 - train: epoch 0112, iter [00100, 05004], lr: 0.043158, loss: 2.3737
2022-07-16 00:02:01 - train: epoch 0112, iter [00200, 05004], lr: 0.043142, loss: 2.2699
2022-07-16 00:02:35 - train: epoch 0112, iter [00300, 05004], lr: 0.043126, loss: 2.6700
2022-07-16 00:03:08 - train: epoch 0112, iter [00400, 05004], lr: 0.043111, loss: 2.5635
2022-07-16 00:03:41 - train: epoch 0112, iter [00500, 05004], lr: 0.043095, loss: 2.6751
2022-07-16 00:04:15 - train: epoch 0112, iter [00600, 05004], lr: 0.043079, loss: 2.2948
2022-07-16 00:04:48 - train: epoch 0112, iter [00700, 05004], lr: 0.043063, loss: 2.9914
2022-07-16 00:05:21 - train: epoch 0112, iter [00800, 05004], lr: 0.043047, loss: 2.1179
2022-07-16 00:05:55 - train: epoch 0112, iter [00900, 05004], lr: 0.043031, loss: 2.3136
2022-07-16 00:06:28 - train: epoch 0112, iter [01000, 05004], lr: 0.043015, loss: 2.4267
2022-07-16 00:07:03 - train: epoch 0112, iter [01100, 05004], lr: 0.042999, loss: 2.5631
2022-07-16 00:07:37 - train: epoch 0112, iter [01200, 05004], lr: 0.042983, loss: 2.6893
2022-07-16 00:08:12 - train: epoch 0112, iter [01300, 05004], lr: 0.042967, loss: 2.5482
2022-07-16 00:08:47 - train: epoch 0112, iter [01400, 05004], lr: 0.042951, loss: 2.4812
2022-07-16 00:09:21 - train: epoch 0112, iter [01500, 05004], lr: 0.042935, loss: 2.8233
2022-07-16 00:09:56 - train: epoch 0112, iter [01600, 05004], lr: 0.042919, loss: 2.6249
2022-07-16 00:10:31 - train: epoch 0112, iter [01700, 05004], lr: 0.042903, loss: 2.4947
2022-07-16 00:11:06 - train: epoch 0112, iter [01800, 05004], lr: 0.042887, loss: 2.2891
2022-07-16 00:11:40 - train: epoch 0112, iter [01900, 05004], lr: 0.042871, loss: 2.1230
2022-07-16 00:12:15 - train: epoch 0112, iter [02000, 05004], lr: 0.042856, loss: 2.6083
2022-07-16 00:12:50 - train: epoch 0112, iter [02100, 05004], lr: 0.042840, loss: 2.7806
2022-07-16 00:13:25 - train: epoch 0112, iter [02200, 05004], lr: 0.042824, loss: 2.5838
2022-07-16 00:13:59 - train: epoch 0112, iter [02300, 05004], lr: 0.042808, loss: 2.7225
2022-07-16 00:14:34 - train: epoch 0112, iter [02400, 05004], lr: 0.042792, loss: 2.2267
2022-07-16 00:15:08 - train: epoch 0112, iter [02500, 05004], lr: 0.042776, loss: 2.6777
2022-07-16 00:15:43 - train: epoch 0112, iter [02600, 05004], lr: 0.042760, loss: 2.3603
2022-07-16 00:16:18 - train: epoch 0112, iter [02700, 05004], lr: 0.042744, loss: 2.6231
2022-07-16 00:16:52 - train: epoch 0112, iter [02800, 05004], lr: 0.042728, loss: 2.4877
2022-07-16 00:17:27 - train: epoch 0112, iter [02900, 05004], lr: 0.042712, loss: 2.7528
2022-07-16 00:18:02 - train: epoch 0112, iter [03000, 05004], lr: 0.042696, loss: 2.6639
2022-07-16 00:18:37 - train: epoch 0112, iter [03100, 05004], lr: 0.042680, loss: 2.5771
2022-07-16 00:19:10 - train: epoch 0112, iter [03200, 05004], lr: 0.042664, loss: 2.3582
2022-07-16 00:19:45 - train: epoch 0112, iter [03300, 05004], lr: 0.042648, loss: 2.3732
2022-07-16 00:20:20 - train: epoch 0112, iter [03400, 05004], lr: 0.042633, loss: 2.6494
2022-07-16 00:20:55 - train: epoch 0112, iter [03500, 05004], lr: 0.042617, loss: 2.4046
2022-07-16 00:21:29 - train: epoch 0112, iter [03600, 05004], lr: 0.042601, loss: 2.4179
2022-07-16 00:22:03 - train: epoch 0112, iter [03700, 05004], lr: 0.042585, loss: 2.4511
2022-07-16 00:22:36 - train: epoch 0112, iter [03800, 05004], lr: 0.042569, loss: 2.5834
2022-07-16 00:23:11 - train: epoch 0112, iter [03900, 05004], lr: 0.042553, loss: 2.0643
2022-07-16 00:23:45 - train: epoch 0112, iter [04000, 05004], lr: 0.042537, loss: 2.4567
2022-07-16 00:24:19 - train: epoch 0112, iter [04100, 05004], lr: 0.042521, loss: 2.3052
2022-07-16 00:24:53 - train: epoch 0112, iter [04200, 05004], lr: 0.042505, loss: 2.3927
2022-07-16 00:25:26 - train: epoch 0112, iter [04300, 05004], lr: 0.042489, loss: 2.5730
2022-07-16 00:26:00 - train: epoch 0112, iter [04400, 05004], lr: 0.042473, loss: 2.6203
2022-07-16 00:26:34 - train: epoch 0112, iter [04500, 05004], lr: 0.042457, loss: 2.3890
2022-07-16 00:27:07 - train: epoch 0112, iter [04600, 05004], lr: 0.042442, loss: 2.3967
2022-07-16 00:27:41 - train: epoch 0112, iter [04700, 05004], lr: 0.042426, loss: 2.4070
2022-07-16 00:28:14 - train: epoch 0112, iter [04800, 05004], lr: 0.042410, loss: 2.5641
2022-07-16 00:28:48 - train: epoch 0112, iter [04900, 05004], lr: 0.042394, loss: 2.0997
2022-07-16 00:29:20 - train: epoch 0112, iter [05000, 05004], lr: 0.042378, loss: 2.3684
2022-07-16 00:29:22 - train: epoch 112, train_loss: 2.4641
2022-07-16 00:30:35 - eval: epoch: 112, acc1: 65.082%, acc5: 87.180%, test_loss: 1.4272, per_image_load_time: 2.138ms, per_image_inference_time: 0.443ms
2022-07-16 00:30:35 - until epoch: 112, best_acc1: 65.082%
2022-07-16 00:30:35 - epoch 113 lr: 0.042377
2022-07-16 00:31:14 - train: epoch 0113, iter [00100, 05004], lr: 0.042361, loss: 2.5416
2022-07-16 00:31:48 - train: epoch 0113, iter [00200, 05004], lr: 0.042345, loss: 2.3013
2022-07-16 00:32:21 - train: epoch 0113, iter [00300, 05004], lr: 0.042330, loss: 2.5078
2022-07-16 00:32:54 - train: epoch 0113, iter [00400, 05004], lr: 0.042314, loss: 2.2308
2022-07-16 00:33:28 - train: epoch 0113, iter [00500, 05004], lr: 0.042298, loss: 2.5050
2022-07-16 00:34:01 - train: epoch 0113, iter [00600, 05004], lr: 0.042282, loss: 2.3138
2022-07-16 00:34:34 - train: epoch 0113, iter [00700, 05004], lr: 0.042266, loss: 2.6000
2022-07-16 00:35:07 - train: epoch 0113, iter [00800, 05004], lr: 0.042250, loss: 2.4658
2022-07-16 00:35:41 - train: epoch 0113, iter [00900, 05004], lr: 0.042234, loss: 2.6073
2022-07-16 00:36:14 - train: epoch 0113, iter [01000, 05004], lr: 0.042218, loss: 2.2874
2022-07-16 00:36:47 - train: epoch 0113, iter [01100, 05004], lr: 0.042202, loss: 2.1936
2022-07-16 00:37:21 - train: epoch 0113, iter [01200, 05004], lr: 0.042186, loss: 2.4300
2022-07-16 00:37:54 - train: epoch 0113, iter [01300, 05004], lr: 0.042170, loss: 2.4959
2022-07-16 00:38:27 - train: epoch 0113, iter [01400, 05004], lr: 0.042155, loss: 2.3498
2022-07-16 00:38:59 - train: epoch 0113, iter [01500, 05004], lr: 0.042139, loss: 2.7206
2022-07-16 00:39:33 - train: epoch 0113, iter [01600, 05004], lr: 0.042123, loss: 2.3797
2022-07-16 00:40:06 - train: epoch 0113, iter [01700, 05004], lr: 0.042107, loss: 2.5497
2022-07-16 00:40:40 - train: epoch 0113, iter [01800, 05004], lr: 0.042091, loss: 2.2010
2022-07-16 00:41:14 - train: epoch 0113, iter [01900, 05004], lr: 0.042075, loss: 2.5664
2022-07-16 00:41:46 - train: epoch 0113, iter [02000, 05004], lr: 0.042059, loss: 2.5304
2022-07-16 00:42:19 - train: epoch 0113, iter [02100, 05004], lr: 0.042043, loss: 2.8109
2022-07-16 00:42:53 - train: epoch 0113, iter [02200, 05004], lr: 0.042027, loss: 2.4833
2022-07-16 00:43:27 - train: epoch 0113, iter [02300, 05004], lr: 0.042012, loss: 2.2472
2022-07-16 00:43:59 - train: epoch 0113, iter [02400, 05004], lr: 0.041996, loss: 2.3722
2022-07-16 00:44:33 - train: epoch 0113, iter [02500, 05004], lr: 0.041980, loss: 2.4463
2022-07-16 00:45:06 - train: epoch 0113, iter [02600, 05004], lr: 0.041964, loss: 2.2994
2022-07-16 00:45:38 - train: epoch 0113, iter [02700, 05004], lr: 0.041948, loss: 2.1548
2022-07-16 00:46:12 - train: epoch 0113, iter [02800, 05004], lr: 0.041932, loss: 2.4048
2022-07-16 00:46:45 - train: epoch 0113, iter [02900, 05004], lr: 0.041916, loss: 2.3407
2022-07-16 00:47:19 - train: epoch 0113, iter [03000, 05004], lr: 0.041900, loss: 2.2417
2022-07-16 00:47:52 - train: epoch 0113, iter [03100, 05004], lr: 0.041884, loss: 2.5089
2022-07-16 00:48:26 - train: epoch 0113, iter [03200, 05004], lr: 0.041869, loss: 2.2341
2022-07-16 00:48:58 - train: epoch 0113, iter [03300, 05004], lr: 0.041853, loss: 2.5129
2022-07-16 00:49:32 - train: epoch 0113, iter [03400, 05004], lr: 0.041837, loss: 2.6568
2022-07-16 00:50:05 - train: epoch 0113, iter [03500, 05004], lr: 0.041821, loss: 2.3196
2022-07-16 00:50:39 - train: epoch 0113, iter [03600, 05004], lr: 0.041805, loss: 2.4638
2022-07-16 00:51:12 - train: epoch 0113, iter [03700, 05004], lr: 0.041789, loss: 2.3966
2022-07-16 00:51:46 - train: epoch 0113, iter [03800, 05004], lr: 0.041773, loss: 2.3271
2022-07-16 00:52:19 - train: epoch 0113, iter [03900, 05004], lr: 0.041757, loss: 2.6171
2022-07-16 00:52:53 - train: epoch 0113, iter [04000, 05004], lr: 0.041742, loss: 2.6384
2022-07-16 00:53:26 - train: epoch 0113, iter [04100, 05004], lr: 0.041726, loss: 2.2710
2022-07-16 00:54:00 - train: epoch 0113, iter [04200, 05004], lr: 0.041710, loss: 2.5620
2022-07-16 00:54:33 - train: epoch 0113, iter [04300, 05004], lr: 0.041694, loss: 2.6404
2022-07-16 00:55:06 - train: epoch 0113, iter [04400, 05004], lr: 0.041678, loss: 2.4665
2022-07-16 00:55:41 - train: epoch 0113, iter [04500, 05004], lr: 0.041662, loss: 2.3155
2022-07-16 00:56:15 - train: epoch 0113, iter [04600, 05004], lr: 0.041646, loss: 2.5274
2022-07-16 00:56:49 - train: epoch 0113, iter [04700, 05004], lr: 0.041630, loss: 2.4643
2022-07-16 00:57:23 - train: epoch 0113, iter [04800, 05004], lr: 0.041615, loss: 2.5523
2022-07-16 00:57:58 - train: epoch 0113, iter [04900, 05004], lr: 0.041599, loss: 2.4558
2022-07-16 00:58:30 - train: epoch 0113, iter [05000, 05004], lr: 0.041583, loss: 2.3015
2022-07-16 00:58:31 - train: epoch 113, train_loss: 2.4576
2022-07-16 00:59:46 - eval: epoch: 113, acc1: 64.374%, acc5: 86.488%, test_loss: 1.4695, per_image_load_time: 2.084ms, per_image_inference_time: 0.472ms
2022-07-16 00:59:46 - until epoch: 113, best_acc1: 65.082%
2022-07-16 00:59:46 - epoch 114 lr: 0.041582
2022-07-16 01:00:25 - train: epoch 0114, iter [00100, 05004], lr: 0.041566, loss: 2.6929
2022-07-16 01:01:00 - train: epoch 0114, iter [00200, 05004], lr: 0.041550, loss: 2.2077
2022-07-16 01:01:34 - train: epoch 0114, iter [00300, 05004], lr: 0.041535, loss: 2.1772
2022-07-16 01:02:09 - train: epoch 0114, iter [00400, 05004], lr: 0.041519, loss: 2.3889
2022-07-16 01:02:42 - train: epoch 0114, iter [00500, 05004], lr: 0.041503, loss: 2.2958
2022-07-16 01:03:18 - train: epoch 0114, iter [00600, 05004], lr: 0.041487, loss: 2.4348
2022-07-16 01:03:51 - train: epoch 0114, iter [00700, 05004], lr: 0.041471, loss: 2.4649
2022-07-16 01:04:26 - train: epoch 0114, iter [00800, 05004], lr: 0.041455, loss: 2.5249
2022-07-16 01:05:00 - train: epoch 0114, iter [00900, 05004], lr: 0.041439, loss: 2.1475
2022-07-16 01:05:35 - train: epoch 0114, iter [01000, 05004], lr: 0.041424, loss: 2.5756
2022-07-16 01:06:10 - train: epoch 0114, iter [01100, 05004], lr: 0.041408, loss: 2.4483
2022-07-16 01:06:44 - train: epoch 0114, iter [01200, 05004], lr: 0.041392, loss: 2.6443
2022-07-16 01:07:20 - train: epoch 0114, iter [01300, 05004], lr: 0.041376, loss: 2.3175
2022-07-16 01:07:54 - train: epoch 0114, iter [01400, 05004], lr: 0.041360, loss: 2.6567
2022-07-16 01:08:29 - train: epoch 0114, iter [01500, 05004], lr: 0.041344, loss: 2.2602
2022-07-16 01:09:03 - train: epoch 0114, iter [01600, 05004], lr: 0.041328, loss: 2.3702
2022-07-16 01:09:37 - train: epoch 0114, iter [01700, 05004], lr: 0.041313, loss: 2.2107
2022-07-16 01:10:12 - train: epoch 0114, iter [01800, 05004], lr: 0.041297, loss: 2.4606
2022-07-16 01:10:46 - train: epoch 0114, iter [01900, 05004], lr: 0.041281, loss: 2.2724
2022-07-16 01:11:22 - train: epoch 0114, iter [02000, 05004], lr: 0.041265, loss: 2.4918
2022-07-16 01:11:56 - train: epoch 0114, iter [02100, 05004], lr: 0.041249, loss: 2.3575
2022-07-16 01:12:31 - train: epoch 0114, iter [02200, 05004], lr: 0.041233, loss: 2.1358
2022-07-16 01:13:06 - train: epoch 0114, iter [02300, 05004], lr: 0.041217, loss: 2.7684
2022-07-16 01:13:39 - train: epoch 0114, iter [02400, 05004], lr: 0.041202, loss: 2.6495
2022-07-16 01:14:15 - train: epoch 0114, iter [02500, 05004], lr: 0.041186, loss: 2.3850
2022-07-16 01:14:50 - train: epoch 0114, iter [02600, 05004], lr: 0.041170, loss: 2.5225
2022-07-16 01:15:24 - train: epoch 0114, iter [02700, 05004], lr: 0.041154, loss: 2.8040
2022-07-16 01:15:59 - train: epoch 0114, iter [02800, 05004], lr: 0.041138, loss: 2.4434
2022-07-16 01:16:34 - train: epoch 0114, iter [02900, 05004], lr: 0.041122, loss: 2.1981
2022-07-16 01:17:09 - train: epoch 0114, iter [03000, 05004], lr: 0.041107, loss: 2.1731
2022-07-16 01:17:42 - train: epoch 0114, iter [03100, 05004], lr: 0.041091, loss: 2.7468
2022-07-16 01:18:18 - train: epoch 0114, iter [03200, 05004], lr: 0.041075, loss: 2.5449
2022-07-16 01:18:53 - train: epoch 0114, iter [03300, 05004], lr: 0.041059, loss: 2.3604
2022-07-16 01:19:28 - train: epoch 0114, iter [03400, 05004], lr: 0.041043, loss: 2.4786
2022-07-16 01:20:02 - train: epoch 0114, iter [03500, 05004], lr: 0.041027, loss: 2.0299
2022-07-16 01:20:37 - train: epoch 0114, iter [03600, 05004], lr: 0.041011, loss: 2.5854
2022-07-16 01:21:12 - train: epoch 0114, iter [03700, 05004], lr: 0.040996, loss: 2.3634
2022-07-16 01:21:47 - train: epoch 0114, iter [03800, 05004], lr: 0.040980, loss: 2.4599
2022-07-16 01:22:21 - train: epoch 0114, iter [03900, 05004], lr: 0.040964, loss: 2.6610
2022-07-16 01:22:56 - train: epoch 0114, iter [04000, 05004], lr: 0.040948, loss: 2.4237
2022-07-16 01:23:30 - train: epoch 0114, iter [04100, 05004], lr: 0.040932, loss: 2.2812
2022-07-16 01:24:04 - train: epoch 0114, iter [04200, 05004], lr: 0.040916, loss: 2.6855
2022-07-16 01:24:39 - train: epoch 0114, iter [04300, 05004], lr: 0.040901, loss: 2.2910
2022-07-16 01:25:14 - train: epoch 0114, iter [04400, 05004], lr: 0.040885, loss: 2.4368
2022-07-16 01:25:49 - train: epoch 0114, iter [04500, 05004], lr: 0.040869, loss: 2.8203
2022-07-16 01:26:24 - train: epoch 0114, iter [04600, 05004], lr: 0.040853, loss: 2.6913
2022-07-16 01:27:00 - train: epoch 0114, iter [04700, 05004], lr: 0.040837, loss: 2.3677
2022-07-16 01:27:34 - train: epoch 0114, iter [04800, 05004], lr: 0.040822, loss: 2.7120
2022-07-16 01:28:08 - train: epoch 0114, iter [04900, 05004], lr: 0.040806, loss: 2.3456
2022-07-16 01:28:42 - train: epoch 0114, iter [05000, 05004], lr: 0.040790, loss: 2.3888
2022-07-16 01:28:43 - train: epoch 114, train_loss: 2.4507
2022-07-16 01:29:58 - eval: epoch: 114, acc1: 65.434%, acc5: 87.084%, test_loss: 1.4290, per_image_load_time: 0.942ms, per_image_inference_time: 0.462ms
2022-07-16 01:29:58 - until epoch: 114, best_acc1: 65.434%
2022-07-16 01:29:58 - epoch 115 lr: 0.040789
2022-07-16 01:30:37 - train: epoch 0115, iter [00100, 05004], lr: 0.040773, loss: 2.3827
2022-07-16 01:31:12 - train: epoch 0115, iter [00200, 05004], lr: 0.040758, loss: 2.2652
2022-07-16 01:31:47 - train: epoch 0115, iter [00300, 05004], lr: 0.040742, loss: 2.3934
2022-07-16 01:32:22 - train: epoch 0115, iter [00400, 05004], lr: 0.040726, loss: 2.3531
2022-07-16 01:32:56 - train: epoch 0115, iter [00500, 05004], lr: 0.040710, loss: 2.4470
2022-07-16 01:33:30 - train: epoch 0115, iter [00600, 05004], lr: 0.040694, loss: 2.1541
2022-07-16 01:34:05 - train: epoch 0115, iter [00700, 05004], lr: 0.040679, loss: 2.5608
2022-07-16 01:34:40 - train: epoch 0115, iter [00800, 05004], lr: 0.040663, loss: 2.4443
2022-07-16 01:35:15 - train: epoch 0115, iter [00900, 05004], lr: 0.040647, loss: 2.7332
2022-07-16 01:35:49 - train: epoch 0115, iter [01000, 05004], lr: 0.040631, loss: 2.1310
2022-07-16 01:36:23 - train: epoch 0115, iter [01100, 05004], lr: 0.040615, loss: 2.3391
2022-07-16 01:36:57 - train: epoch 0115, iter [01200, 05004], lr: 0.040599, loss: 2.3750
2022-07-16 01:37:32 - train: epoch 0115, iter [01300, 05004], lr: 0.040584, loss: 2.7322
2022-07-16 01:38:06 - train: epoch 0115, iter [01400, 05004], lr: 0.040568, loss: 2.1450
2022-07-16 01:38:41 - train: epoch 0115, iter [01500, 05004], lr: 0.040552, loss: 2.6287
2022-07-16 01:39:15 - train: epoch 0115, iter [01600, 05004], lr: 0.040536, loss: 2.4084
2022-07-16 01:39:50 - train: epoch 0115, iter [01700, 05004], lr: 0.040520, loss: 2.6302
2022-07-16 01:40:25 - train: epoch 0115, iter [01800, 05004], lr: 0.040505, loss: 2.4145
2022-07-16 01:40:59 - train: epoch 0115, iter [01900, 05004], lr: 0.040489, loss: 2.6428
2022-07-16 01:41:33 - train: epoch 0115, iter [02000, 05004], lr: 0.040473, loss: 2.7519
2022-07-16 01:42:08 - train: epoch 0115, iter [02100, 05004], lr: 0.040457, loss: 2.3404
2022-07-16 01:42:43 - train: epoch 0115, iter [02200, 05004], lr: 0.040441, loss: 2.5962
2022-07-16 01:43:18 - train: epoch 0115, iter [02300, 05004], lr: 0.040426, loss: 2.5662
2022-07-16 01:43:53 - train: epoch 0115, iter [02400, 05004], lr: 0.040410, loss: 2.3736
2022-07-16 01:44:27 - train: epoch 0115, iter [02500, 05004], lr: 0.040394, loss: 2.1044
2022-07-16 01:45:01 - train: epoch 0115, iter [02600, 05004], lr: 0.040378, loss: 2.2123
2022-07-16 01:45:36 - train: epoch 0115, iter [02700, 05004], lr: 0.040362, loss: 2.1467
2022-07-16 01:46:10 - train: epoch 0115, iter [02800, 05004], lr: 0.040347, loss: 2.5151
2022-07-16 01:46:45 - train: epoch 0115, iter [02900, 05004], lr: 0.040331, loss: 2.3813
2022-07-16 01:47:19 - train: epoch 0115, iter [03000, 05004], lr: 0.040315, loss: 2.1668
2022-07-16 01:47:54 - train: epoch 0115, iter [03100, 05004], lr: 0.040299, loss: 2.5541
2022-07-16 01:48:29 - train: epoch 0115, iter [03200, 05004], lr: 0.040283, loss: 2.4580
2022-07-16 01:49:03 - train: epoch 0115, iter [03300, 05004], lr: 0.040268, loss: 2.2694
2022-07-16 01:49:38 - train: epoch 0115, iter [03400, 05004], lr: 0.040252, loss: 2.5907
2022-07-16 01:50:12 - train: epoch 0115, iter [03500, 05004], lr: 0.040236, loss: 2.5217
2022-07-16 01:50:48 - train: epoch 0115, iter [03600, 05004], lr: 0.040220, loss: 2.8757
2022-07-16 01:51:22 - train: epoch 0115, iter [03700, 05004], lr: 0.040204, loss: 2.5888
2022-07-16 01:51:56 - train: epoch 0115, iter [03800, 05004], lr: 0.040189, loss: 2.5397
2022-07-16 01:52:31 - train: epoch 0115, iter [03900, 05004], lr: 0.040173, loss: 2.6526
2022-07-16 01:53:04 - train: epoch 0115, iter [04000, 05004], lr: 0.040157, loss: 2.7746
2022-07-16 01:53:40 - train: epoch 0115, iter [04100, 05004], lr: 0.040141, loss: 2.3163
2022-07-16 01:54:14 - train: epoch 0115, iter [04200, 05004], lr: 0.040126, loss: 2.5552
2022-07-16 01:54:49 - train: epoch 0115, iter [04300, 05004], lr: 0.040110, loss: 2.4105
2022-07-16 01:55:23 - train: epoch 0115, iter [04400, 05004], lr: 0.040094, loss: 2.4357
2022-07-16 01:55:58 - train: epoch 0115, iter [04500, 05004], lr: 0.040078, loss: 2.3090
2022-07-16 01:56:32 - train: epoch 0115, iter [04600, 05004], lr: 0.040062, loss: 2.4886
2022-07-16 01:57:06 - train: epoch 0115, iter [04700, 05004], lr: 0.040047, loss: 2.6266
2022-07-16 01:57:41 - train: epoch 0115, iter [04800, 05004], lr: 0.040031, loss: 2.4383
2022-07-16 01:58:16 - train: epoch 0115, iter [04900, 05004], lr: 0.040015, loss: 2.4213
2022-07-16 01:58:49 - train: epoch 0115, iter [05000, 05004], lr: 0.039999, loss: 2.4496
2022-07-16 01:58:50 - train: epoch 115, train_loss: 2.4426
2022-07-16 02:00:05 - eval: epoch: 115, acc1: 65.162%, acc5: 87.098%, test_loss: 1.4208, per_image_load_time: 1.692ms, per_image_inference_time: 0.477ms
2022-07-16 02:00:05 - until epoch: 115, best_acc1: 65.434%
2022-07-16 02:00:05 - epoch 116 lr: 0.039999
2022-07-16 02:00:45 - train: epoch 0116, iter [00100, 05004], lr: 0.039983, loss: 2.4254
2022-07-16 02:01:19 - train: epoch 0116, iter [00200, 05004], lr: 0.039967, loss: 2.6026
2022-07-16 02:01:55 - train: epoch 0116, iter [00300, 05004], lr: 0.039951, loss: 2.4039
2022-07-16 02:02:28 - train: epoch 0116, iter [00400, 05004], lr: 0.039936, loss: 2.5694
2022-07-16 02:03:04 - train: epoch 0116, iter [00500, 05004], lr: 0.039920, loss: 2.3103
2022-07-16 02:03:37 - train: epoch 0116, iter [00600, 05004], lr: 0.039904, loss: 2.4834
2022-07-16 02:04:12 - train: epoch 0116, iter [00700, 05004], lr: 0.039888, loss: 2.5168
2022-07-16 02:04:47 - train: epoch 0116, iter [00800, 05004], lr: 0.039873, loss: 2.6112
2022-07-16 02:05:21 - train: epoch 0116, iter [00900, 05004], lr: 0.039857, loss: 2.6030
2022-07-16 02:05:56 - train: epoch 0116, iter [01000, 05004], lr: 0.039841, loss: 2.6520
2022-07-16 02:06:30 - train: epoch 0116, iter [01100, 05004], lr: 0.039825, loss: 2.3431
2022-07-16 02:07:04 - train: epoch 0116, iter [01200, 05004], lr: 0.039810, loss: 2.5119
2022-07-16 02:07:39 - train: epoch 0116, iter [01300, 05004], lr: 0.039794, loss: 2.5362
2022-07-16 02:08:12 - train: epoch 0116, iter [01400, 05004], lr: 0.039778, loss: 2.5608
2022-07-16 02:08:48 - train: epoch 0116, iter [01500, 05004], lr: 0.039762, loss: 2.5567
2022-07-16 02:09:22 - train: epoch 0116, iter [01600, 05004], lr: 0.039746, loss: 2.4940
2022-07-16 02:09:57 - train: epoch 0116, iter [01700, 05004], lr: 0.039731, loss: 2.2986
2022-07-16 02:10:31 - train: epoch 0116, iter [01800, 05004], lr: 0.039715, loss: 2.6257
2022-07-16 02:11:06 - train: epoch 0116, iter [01900, 05004], lr: 0.039699, loss: 2.2798
2022-07-16 02:11:40 - train: epoch 0116, iter [02000, 05004], lr: 0.039683, loss: 2.2958
2022-07-16 02:12:15 - train: epoch 0116, iter [02100, 05004], lr: 0.039668, loss: 2.4602
2022-07-16 02:12:51 - train: epoch 0116, iter [02200, 05004], lr: 0.039652, loss: 2.6130
2022-07-16 02:13:25 - train: epoch 0116, iter [02300, 05004], lr: 0.039636, loss: 2.5598
2022-07-16 02:14:01 - train: epoch 0116, iter [02400, 05004], lr: 0.039620, loss: 2.3538
2022-07-16 02:14:35 - train: epoch 0116, iter [02500, 05004], lr: 0.039605, loss: 2.5865
2022-07-16 02:15:10 - train: epoch 0116, iter [02600, 05004], lr: 0.039589, loss: 2.6855
2022-07-16 02:15:44 - train: epoch 0116, iter [02700, 05004], lr: 0.039573, loss: 2.7598
2022-07-16 02:16:19 - train: epoch 0116, iter [02800, 05004], lr: 0.039557, loss: 2.6243
2022-07-16 02:16:54 - train: epoch 0116, iter [02900, 05004], lr: 0.039542, loss: 2.7485
2022-07-16 02:17:29 - train: epoch 0116, iter [03000, 05004], lr: 0.039526, loss: 2.5067
2022-07-16 02:18:04 - train: epoch 0116, iter [03100, 05004], lr: 0.039510, loss: 2.6345
2022-07-16 02:18:38 - train: epoch 0116, iter [03200, 05004], lr: 0.039495, loss: 2.4980
2022-07-16 02:19:13 - train: epoch 0116, iter [03300, 05004], lr: 0.039479, loss: 2.7736
2022-07-16 02:19:48 - train: epoch 0116, iter [03400, 05004], lr: 0.039463, loss: 2.3523
2022-07-16 02:20:23 - train: epoch 0116, iter [03500, 05004], lr: 0.039447, loss: 2.5321
2022-07-16 02:20:57 - train: epoch 0116, iter [03600, 05004], lr: 0.039432, loss: 2.5303
2022-07-16 02:21:32 - train: epoch 0116, iter [03700, 05004], lr: 0.039416, loss: 2.5166
2022-07-16 02:22:07 - train: epoch 0116, iter [03800, 05004], lr: 0.039400, loss: 2.2573
2022-07-16 02:22:41 - train: epoch 0116, iter [03900, 05004], lr: 0.039384, loss: 2.3648
2022-07-16 02:23:16 - train: epoch 0116, iter [04000, 05004], lr: 0.039369, loss: 2.4820
2022-07-16 02:23:51 - train: epoch 0116, iter [04100, 05004], lr: 0.039353, loss: 2.5513
2022-07-16 02:24:25 - train: epoch 0116, iter [04200, 05004], lr: 0.039337, loss: 2.2796
2022-07-16 02:24:59 - train: epoch 0116, iter [04300, 05004], lr: 0.039321, loss: 2.5653
2022-07-16 02:25:34 - train: epoch 0116, iter [04400, 05004], lr: 0.039306, loss: 2.4095
2022-07-16 02:26:08 - train: epoch 0116, iter [04500, 05004], lr: 0.039290, loss: 2.4597
2022-07-16 02:26:44 - train: epoch 0116, iter [04600, 05004], lr: 0.039274, loss: 2.5586
2022-07-16 02:27:18 - train: epoch 0116, iter [04700, 05004], lr: 0.039259, loss: 2.5073
2022-07-16 02:27:53 - train: epoch 0116, iter [04800, 05004], lr: 0.039243, loss: 2.3612
2022-07-16 02:28:27 - train: epoch 0116, iter [04900, 05004], lr: 0.039227, loss: 2.6112
2022-07-16 02:29:00 - train: epoch 0116, iter [05000, 05004], lr: 0.039211, loss: 2.3037
2022-07-16 02:29:01 - train: epoch 116, train_loss: 2.4381
2022-07-16 02:30:17 - eval: epoch: 116, acc1: 64.580%, acc5: 86.362%, test_loss: 1.4659, per_image_load_time: 2.201ms, per_image_inference_time: 0.490ms
2022-07-16 02:30:17 - until epoch: 116, best_acc1: 65.434%
2022-07-16 02:30:17 - epoch 117 lr: 0.039211
2022-07-16 02:30:57 - train: epoch 0117, iter [00100, 05004], lr: 0.039195, loss: 2.5424
2022-07-16 02:31:31 - train: epoch 0117, iter [00200, 05004], lr: 0.039179, loss: 2.4776
2022-07-16 02:32:05 - train: epoch 0117, iter [00300, 05004], lr: 0.039164, loss: 2.6389
2022-07-16 02:32:40 - train: epoch 0117, iter [00400, 05004], lr: 0.039148, loss: 2.4485
2022-07-16 02:33:14 - train: epoch 0117, iter [00500, 05004], lr: 0.039132, loss: 2.4097
2022-07-16 02:33:49 - train: epoch 0117, iter [00600, 05004], lr: 0.039116, loss: 2.4282
2022-07-16 02:34:22 - train: epoch 0117, iter [00700, 05004], lr: 0.039101, loss: 2.4861
2022-07-16 02:34:57 - train: epoch 0117, iter [00800, 05004], lr: 0.039085, loss: 2.2251
2022-07-16 02:35:32 - train: epoch 0117, iter [00900, 05004], lr: 0.039069, loss: 2.4678
2022-07-16 02:36:07 - train: epoch 0117, iter [01000, 05004], lr: 0.039054, loss: 2.2364
2022-07-16 02:36:41 - train: epoch 0117, iter [01100, 05004], lr: 0.039038, loss: 2.4978
2022-07-16 02:37:15 - train: epoch 0117, iter [01200, 05004], lr: 0.039022, loss: 2.5435
2022-07-16 02:37:51 - train: epoch 0117, iter [01300, 05004], lr: 0.039007, loss: 2.1717
2022-07-16 02:38:25 - train: epoch 0117, iter [01400, 05004], lr: 0.038991, loss: 2.5005
2022-07-16 02:39:00 - train: epoch 0117, iter [01500, 05004], lr: 0.038975, loss: 2.5946
2022-07-16 02:39:34 - train: epoch 0117, iter [01600, 05004], lr: 0.038959, loss: 2.2303
2022-07-16 02:40:09 - train: epoch 0117, iter [01700, 05004], lr: 0.038944, loss: 2.7167
2022-07-16 02:40:43 - train: epoch 0117, iter [01800, 05004], lr: 0.038928, loss: 2.5558
2022-07-16 02:41:18 - train: epoch 0117, iter [01900, 05004], lr: 0.038912, loss: 2.0427
2022-07-16 02:41:52 - train: epoch 0117, iter [02000, 05004], lr: 0.038897, loss: 2.4623
2022-07-16 02:42:27 - train: epoch 0117, iter [02100, 05004], lr: 0.038881, loss: 2.3104
2022-07-16 02:43:02 - train: epoch 0117, iter [02200, 05004], lr: 0.038865, loss: 2.4094
2022-07-16 02:43:36 - train: epoch 0117, iter [02300, 05004], lr: 0.038850, loss: 2.2841
2022-07-16 02:44:11 - train: epoch 0117, iter [02400, 05004], lr: 0.038834, loss: 2.2503
2022-07-16 02:44:45 - train: epoch 0117, iter [02500, 05004], lr: 0.038818, loss: 2.4304
2022-07-16 02:45:20 - train: epoch 0117, iter [02600, 05004], lr: 0.038802, loss: 2.4787
2022-07-16 02:45:55 - train: epoch 0117, iter [02700, 05004], lr: 0.038787, loss: 2.4948
2022-07-16 02:46:30 - train: epoch 0117, iter [02800, 05004], lr: 0.038771, loss: 2.3905
2022-07-16 02:47:04 - train: epoch 0117, iter [02900, 05004], lr: 0.038755, loss: 2.5360
2022-07-16 02:47:39 - train: epoch 0117, iter [03000, 05004], lr: 0.038740, loss: 2.1471
2022-07-16 02:48:13 - train: epoch 0117, iter [03100, 05004], lr: 0.038724, loss: 2.7354
2022-07-16 02:48:48 - train: epoch 0117, iter [03200, 05004], lr: 0.038708, loss: 2.3738
2022-07-16 02:49:23 - train: epoch 0117, iter [03300, 05004], lr: 0.038693, loss: 2.5971
2022-07-16 02:49:58 - train: epoch 0117, iter [03400, 05004], lr: 0.038677, loss: 2.2464
2022-07-16 02:50:32 - train: epoch 0117, iter [03500, 05004], lr: 0.038661, loss: 2.3382
2022-07-16 02:51:07 - train: epoch 0117, iter [03600, 05004], lr: 0.038646, loss: 2.4484
2022-07-16 02:51:41 - train: epoch 0117, iter [03700, 05004], lr: 0.038630, loss: 2.6241
2022-07-16 02:52:16 - train: epoch 0117, iter [03800, 05004], lr: 0.038614, loss: 2.4120
2022-07-16 02:52:50 - train: epoch 0117, iter [03900, 05004], lr: 0.038599, loss: 2.4692
2022-07-16 02:53:25 - train: epoch 0117, iter [04000, 05004], lr: 0.038583, loss: 2.4654
2022-07-16 02:54:00 - train: epoch 0117, iter [04100, 05004], lr: 0.038567, loss: 2.6701
2022-07-16 02:54:35 - train: epoch 0117, iter [04200, 05004], lr: 0.038552, loss: 2.5657
2022-07-16 02:55:10 - train: epoch 0117, iter [04300, 05004], lr: 0.038536, loss: 2.2776
2022-07-16 02:55:44 - train: epoch 0117, iter [04400, 05004], lr: 0.038520, loss: 2.3207
2022-07-16 02:56:20 - train: epoch 0117, iter [04500, 05004], lr: 0.038505, loss: 2.6779
2022-07-16 02:56:54 - train: epoch 0117, iter [04600, 05004], lr: 0.038489, loss: 2.3186
2022-07-16 02:57:28 - train: epoch 0117, iter [04700, 05004], lr: 0.038473, loss: 2.2354
2022-07-16 02:58:03 - train: epoch 0117, iter [04800, 05004], lr: 0.038458, loss: 2.2951
2022-07-16 02:58:38 - train: epoch 0117, iter [04900, 05004], lr: 0.038442, loss: 2.7652
2022-07-16 02:59:12 - train: epoch 0117, iter [05000, 05004], lr: 0.038426, loss: 2.5409
2022-07-16 02:59:13 - train: epoch 117, train_loss: 2.4284
2022-07-16 03:00:28 - eval: epoch: 117, acc1: 65.254%, acc5: 86.920%, test_loss: 1.4289, per_image_load_time: 0.980ms, per_image_inference_time: 0.489ms
2022-07-16 03:00:28 - until epoch: 117, best_acc1: 65.434%
2022-07-16 03:00:28 - epoch 118 lr: 0.038426
2022-07-16 03:01:08 - train: epoch 0118, iter [00100, 05004], lr: 0.038410, loss: 2.1700
2022-07-16 03:01:43 - train: epoch 0118, iter [00200, 05004], lr: 0.038394, loss: 2.3368
2022-07-16 03:02:18 - train: epoch 0118, iter [00300, 05004], lr: 0.038379, loss: 2.4559
2022-07-16 03:02:51 - train: epoch 0118, iter [00400, 05004], lr: 0.038363, loss: 2.4643
2022-07-16 03:03:27 - train: epoch 0118, iter [00500, 05004], lr: 0.038347, loss: 2.5565
2022-07-16 03:04:02 - train: epoch 0118, iter [00600, 05004], lr: 0.038332, loss: 2.3946
2022-07-16 03:04:35 - train: epoch 0118, iter [00700, 05004], lr: 0.038316, loss: 2.1753
2022-07-16 03:05:09 - train: epoch 0118, iter [00800, 05004], lr: 0.038300, loss: 2.5264
2022-07-16 03:05:45 - train: epoch 0118, iter [00900, 05004], lr: 0.038285, loss: 2.3109
2022-07-16 03:06:19 - train: epoch 0118, iter [01000, 05004], lr: 0.038269, loss: 2.3065
2022-07-16 03:06:53 - train: epoch 0118, iter [01100, 05004], lr: 0.038253, loss: 2.5512
2022-07-16 03:07:27 - train: epoch 0118, iter [01200, 05004], lr: 0.038238, loss: 2.4988
2022-07-16 03:08:03 - train: epoch 0118, iter [01300, 05004], lr: 0.038222, loss: 2.5495
2022-07-16 03:08:37 - train: epoch 0118, iter [01400, 05004], lr: 0.038207, loss: 2.2544
2022-07-16 03:09:11 - train: epoch 0118, iter [01500, 05004], lr: 0.038191, loss: 2.2585
2022-07-16 03:09:45 - train: epoch 0118, iter [01600, 05004], lr: 0.038175, loss: 2.5884
2022-07-16 03:10:21 - train: epoch 0118, iter [01700, 05004], lr: 0.038160, loss: 2.0490
2022-07-16 03:10:55 - train: epoch 0118, iter [01800, 05004], lr: 0.038144, loss: 2.4338
2022-07-16 03:11:30 - train: epoch 0118, iter [01900, 05004], lr: 0.038128, loss: 2.5750
2022-07-16 03:12:05 - train: epoch 0118, iter [02000, 05004], lr: 0.038113, loss: 2.3042
2022-07-16 03:12:39 - train: epoch 0118, iter [02100, 05004], lr: 0.038097, loss: 2.1793
2022-07-16 03:13:14 - train: epoch 0118, iter [02200, 05004], lr: 0.038081, loss: 2.3044
2022-07-16 03:13:48 - train: epoch 0118, iter [02300, 05004], lr: 0.038066, loss: 2.7135
2022-07-16 03:14:22 - train: epoch 0118, iter [02400, 05004], lr: 0.038050, loss: 2.2834
2022-07-16 03:14:57 - train: epoch 0118, iter [02500, 05004], lr: 0.038035, loss: 2.5749
2022-07-16 03:15:31 - train: epoch 0118, iter [02600, 05004], lr: 0.038019, loss: 2.2765
2022-07-16 03:16:05 - train: epoch 0118, iter [02700, 05004], lr: 0.038003, loss: 2.5793
2022-07-16 03:16:40 - train: epoch 0118, iter [02800, 05004], lr: 0.037988, loss: 2.5641
2022-07-16 03:17:15 - train: epoch 0118, iter [02900, 05004], lr: 0.037972, loss: 2.3818
2022-07-16 03:17:49 - train: epoch 0118, iter [03000, 05004], lr: 0.037956, loss: 2.2206
2022-07-16 03:18:23 - train: epoch 0118, iter [03100, 05004], lr: 0.037941, loss: 2.6597
2022-07-16 03:18:58 - train: epoch 0118, iter [03200, 05004], lr: 0.037925, loss: 2.5324
2022-07-16 03:19:34 - train: epoch 0118, iter [03300, 05004], lr: 0.037910, loss: 2.2776
2022-07-16 03:20:08 - train: epoch 0118, iter [03400, 05004], lr: 0.037894, loss: 2.3616
2022-07-16 03:20:43 - train: epoch 0118, iter [03500, 05004], lr: 0.037878, loss: 2.6992
2022-07-16 03:21:19 - train: epoch 0118, iter [03600, 05004], lr: 0.037863, loss: 2.7658
2022-07-16 03:21:53 - train: epoch 0118, iter [03700, 05004], lr: 0.037847, loss: 2.4360
2022-07-16 03:22:27 - train: epoch 0118, iter [03800, 05004], lr: 0.037831, loss: 2.4789
2022-07-16 03:23:02 - train: epoch 0118, iter [03900, 05004], lr: 0.037816, loss: 2.7761
2022-07-16 03:23:36 - train: epoch 0118, iter [04000, 05004], lr: 0.037800, loss: 2.4661
2022-07-16 03:24:11 - train: epoch 0118, iter [04100, 05004], lr: 0.037785, loss: 2.4468
2022-07-16 03:24:46 - train: epoch 0118, iter [04200, 05004], lr: 0.037769, loss: 2.2562
2022-07-16 03:25:19 - train: epoch 0118, iter [04300, 05004], lr: 0.037753, loss: 2.5537
2022-07-16 03:25:55 - train: epoch 0118, iter [04400, 05004], lr: 0.037738, loss: 2.4519
2022-07-16 03:26:29 - train: epoch 0118, iter [04500, 05004], lr: 0.037722, loss: 2.6735
2022-07-16 03:27:03 - train: epoch 0118, iter [04600, 05004], lr: 0.037707, loss: 2.3039
2022-07-16 03:27:38 - train: epoch 0118, iter [04700, 05004], lr: 0.037691, loss: 2.3219
2022-07-16 03:28:12 - train: epoch 0118, iter [04800, 05004], lr: 0.037675, loss: 2.3215
2022-07-16 03:28:48 - train: epoch 0118, iter [04900, 05004], lr: 0.037660, loss: 2.5114
2022-07-16 03:29:21 - train: epoch 0118, iter [05000, 05004], lr: 0.037644, loss: 2.6632
2022-07-16 03:29:22 - train: epoch 118, train_loss: 2.4206
2022-07-16 03:30:37 - eval: epoch: 118, acc1: 64.544%, acc5: 86.550%, test_loss: 1.4569, per_image_load_time: 2.010ms, per_image_inference_time: 0.485ms
2022-07-16 03:30:37 - until epoch: 118, best_acc1: 65.434%
2022-07-16 03:30:37 - epoch 119 lr: 0.037643
2022-07-16 03:31:17 - train: epoch 0119, iter [00100, 05004], lr: 0.037628, loss: 2.4077
2022-07-16 03:31:50 - train: epoch 0119, iter [00200, 05004], lr: 0.037612, loss: 2.2624
2022-07-16 03:32:26 - train: epoch 0119, iter [00300, 05004], lr: 0.037597, loss: 2.3797
2022-07-16 03:33:00 - train: epoch 0119, iter [00400, 05004], lr: 0.037581, loss: 2.2860
2022-07-16 03:33:33 - train: epoch 0119, iter [00500, 05004], lr: 0.037566, loss: 2.1671
2022-07-16 03:34:08 - train: epoch 0119, iter [00600, 05004], lr: 0.037550, loss: 2.4298
2022-07-16 03:34:43 - train: epoch 0119, iter [00700, 05004], lr: 0.037534, loss: 2.2478
2022-07-16 03:35:17 - train: epoch 0119, iter [00800, 05004], lr: 0.037519, loss: 2.3078
2022-07-16 03:35:51 - train: epoch 0119, iter [00900, 05004], lr: 0.037503, loss: 2.5074
2022-07-16 03:36:25 - train: epoch 0119, iter [01000, 05004], lr: 0.037488, loss: 2.2483
2022-07-16 03:37:00 - train: epoch 0119, iter [01100, 05004], lr: 0.037472, loss: 2.7352
2022-07-16 03:37:34 - train: epoch 0119, iter [01200, 05004], lr: 0.037456, loss: 2.3316
2022-07-16 03:38:10 - train: epoch 0119, iter [01300, 05004], lr: 0.037441, loss: 2.6824
2022-07-16 03:38:44 - train: epoch 0119, iter [01400, 05004], lr: 0.037425, loss: 2.5344
2022-07-16 03:39:19 - train: epoch 0119, iter [01500, 05004], lr: 0.037410, loss: 2.3614
2022-07-16 03:39:54 - train: epoch 0119, iter [01600, 05004], lr: 0.037394, loss: 2.4125
2022-07-16 03:40:30 - train: epoch 0119, iter [01700, 05004], lr: 0.037379, loss: 2.3634
2022-07-16 03:41:03 - train: epoch 0119, iter [01800, 05004], lr: 0.037363, loss: 2.4595
2022-07-16 03:41:38 - train: epoch 0119, iter [01900, 05004], lr: 0.037347, loss: 2.3097
2022-07-16 03:42:13 - train: epoch 0119, iter [02000, 05004], lr: 0.037332, loss: 2.5594
2022-07-16 03:42:47 - train: epoch 0119, iter [02100, 05004], lr: 0.037316, loss: 2.7277
2022-07-16 03:43:22 - train: epoch 0119, iter [02200, 05004], lr: 0.037301, loss: 2.7291
2022-07-16 03:43:57 - train: epoch 0119, iter [02300, 05004], lr: 0.037285, loss: 2.3068
2022-07-16 03:44:31 - train: epoch 0119, iter [02400, 05004], lr: 0.037270, loss: 2.4946
2022-07-16 03:45:06 - train: epoch 0119, iter [02500, 05004], lr: 0.037254, loss: 2.7928
2022-07-16 03:45:41 - train: epoch 0119, iter [02600, 05004], lr: 0.037238, loss: 2.4036
2022-07-16 03:46:16 - train: epoch 0119, iter [02700, 05004], lr: 0.037223, loss: 2.3485
2022-07-16 03:46:51 - train: epoch 0119, iter [02800, 05004], lr: 0.037207, loss: 2.6775
2022-07-16 03:47:26 - train: epoch 0119, iter [02900, 05004], lr: 0.037192, loss: 2.2841
2022-07-16 03:48:01 - train: epoch 0119, iter [03000, 05004], lr: 0.037176, loss: 2.3006
2022-07-16 03:48:36 - train: epoch 0119, iter [03100, 05004], lr: 0.037161, loss: 2.3872
2022-07-16 03:49:10 - train: epoch 0119, iter [03200, 05004], lr: 0.037145, loss: 2.1883
2022-07-16 03:49:45 - train: epoch 0119, iter [03300, 05004], lr: 0.037129, loss: 2.1160
2022-07-16 03:50:20 - train: epoch 0119, iter [03400, 05004], lr: 0.037114, loss: 2.3026
2022-07-16 03:50:54 - train: epoch 0119, iter [03500, 05004], lr: 0.037098, loss: 2.5523
2022-07-16 03:51:29 - train: epoch 0119, iter [03600, 05004], lr: 0.037083, loss: 2.4734
2022-07-16 03:52:04 - train: epoch 0119, iter [03700, 05004], lr: 0.037067, loss: 2.8096
2022-07-16 03:52:39 - train: epoch 0119, iter [03800, 05004], lr: 0.037052, loss: 2.4760
2022-07-16 03:53:14 - train: epoch 0119, iter [03900, 05004], lr: 0.037036, loss: 2.6591
2022-07-16 03:53:48 - train: epoch 0119, iter [04000, 05004], lr: 0.037021, loss: 2.0860
2022-07-16 03:54:23 - train: epoch 0119, iter [04100, 05004], lr: 0.037005, loss: 2.5784
2022-07-16 03:54:58 - train: epoch 0119, iter [04200, 05004], lr: 0.036990, loss: 2.6237
2022-07-16 03:55:33 - train: epoch 0119, iter [04300, 05004], lr: 0.036974, loss: 2.3639
2022-07-16 03:56:07 - train: epoch 0119, iter [04400, 05004], lr: 0.036958, loss: 2.4747
2022-07-16 03:56:44 - train: epoch 0119, iter [04500, 05004], lr: 0.036943, loss: 2.4955
2022-07-16 03:57:18 - train: epoch 0119, iter [04600, 05004], lr: 0.036927, loss: 2.4213
2022-07-16 03:57:53 - train: epoch 0119, iter [04700, 05004], lr: 0.036912, loss: 2.4423
2022-07-16 03:58:27 - train: epoch 0119, iter [04800, 05004], lr: 0.036896, loss: 2.1959
2022-07-16 03:59:02 - train: epoch 0119, iter [04900, 05004], lr: 0.036881, loss: 2.4422
2022-07-16 03:59:35 - train: epoch 0119, iter [05000, 05004], lr: 0.036865, loss: 2.7151
2022-07-16 03:59:36 - train: epoch 119, train_loss: 2.4107
2022-07-16 04:00:51 - eval: epoch: 119, acc1: 66.140%, acc5: 87.578%, test_loss: 1.3867, per_image_load_time: 1.529ms, per_image_inference_time: 0.465ms
2022-07-16 04:00:51 - until epoch: 119, best_acc1: 66.140%
2022-07-16 04:00:51 - epoch 120 lr: 0.036864
2022-07-16 04:01:30 - train: epoch 0120, iter [00100, 05004], lr: 0.036849, loss: 2.3578
2022-07-16 04:02:05 - train: epoch 0120, iter [00200, 05004], lr: 0.036834, loss: 2.4299
2022-07-16 04:02:40 - train: epoch 0120, iter [00300, 05004], lr: 0.036818, loss: 2.1517
2022-07-16 04:03:14 - train: epoch 0120, iter [00400, 05004], lr: 0.036803, loss: 2.2858
2022-07-16 04:03:49 - train: epoch 0120, iter [00500, 05004], lr: 0.036787, loss: 2.3564
2022-07-16 04:04:22 - train: epoch 0120, iter [00600, 05004], lr: 0.036771, loss: 2.0468
2022-07-16 04:04:57 - train: epoch 0120, iter [00700, 05004], lr: 0.036756, loss: 2.3794
2022-07-16 04:05:31 - train: epoch 0120, iter [00800, 05004], lr: 0.036740, loss: 2.4245
2022-07-16 04:06:06 - train: epoch 0120, iter [00900, 05004], lr: 0.036725, loss: 2.5703
2022-07-16 04:06:41 - train: epoch 0120, iter [01000, 05004], lr: 0.036709, loss: 2.3988
2022-07-16 04:07:15 - train: epoch 0120, iter [01100, 05004], lr: 0.036694, loss: 2.1911
2022-07-16 04:07:49 - train: epoch 0120, iter [01200, 05004], lr: 0.036678, loss: 2.4067
2022-07-16 04:08:23 - train: epoch 0120, iter [01300, 05004], lr: 0.036663, loss: 2.1270
2022-07-16 04:08:58 - train: epoch 0120, iter [01400, 05004], lr: 0.036647, loss: 2.3045
2022-07-16 04:09:33 - train: epoch 0120, iter [01500, 05004], lr: 0.036632, loss: 2.4892
2022-07-16 04:10:08 - train: epoch 0120, iter [01600, 05004], lr: 0.036616, loss: 2.2508
2022-07-16 04:10:42 - train: epoch 0120, iter [01700, 05004], lr: 0.036601, loss: 2.6120
2022-07-16 04:11:18 - train: epoch 0120, iter [01800, 05004], lr: 0.036585, loss: 2.3374
2022-07-16 04:11:51 - train: epoch 0120, iter [01900, 05004], lr: 0.036570, loss: 2.6020
2022-07-16 04:12:26 - train: epoch 0120, iter [02000, 05004], lr: 0.036554, loss: 2.5338
2022-07-16 04:13:01 - train: epoch 0120, iter [02100, 05004], lr: 0.036539, loss: 2.5736
2022-07-16 04:13:36 - train: epoch 0120, iter [02200, 05004], lr: 0.036523, loss: 2.4770
2022-07-16 04:14:10 - train: epoch 0120, iter [02300, 05004], lr: 0.036508, loss: 2.1020
2022-07-16 04:14:45 - train: epoch 0120, iter [02400, 05004], lr: 0.036492, loss: 2.5784
2022-07-16 04:15:19 - train: epoch 0120, iter [02500, 05004], lr: 0.036477, loss: 2.3757
2022-07-16 04:15:54 - train: epoch 0120, iter [02600, 05004], lr: 0.036461, loss: 2.5221
2022-07-16 04:16:29 - train: epoch 0120, iter [02700, 05004], lr: 0.036446, loss: 2.3748
2022-07-16 04:17:03 - train: epoch 0120, iter [02800, 05004], lr: 0.036430, loss: 2.2857
2022-07-16 04:17:38 - train: epoch 0120, iter [02900, 05004], lr: 0.036415, loss: 2.4300
2022-07-16 04:18:12 - train: epoch 0120, iter [03000, 05004], lr: 0.036399, loss: 2.4460
2022-07-16 04:18:47 - train: epoch 0120, iter [03100, 05004], lr: 0.036384, loss: 2.3728
2022-07-16 04:19:22 - train: epoch 0120, iter [03200, 05004], lr: 0.036368, loss: 2.6224
2022-07-16 04:19:57 - train: epoch 0120, iter [03300, 05004], lr: 0.036353, loss: 2.3164
2022-07-16 04:20:31 - train: epoch 0120, iter [03400, 05004], lr: 0.036337, loss: 2.4265
2022-07-16 04:21:05 - train: epoch 0120, iter [03500, 05004], lr: 0.036322, loss: 2.8507
2022-07-16 04:21:41 - train: epoch 0120, iter [03600, 05004], lr: 0.036306, loss: 2.1504
2022-07-16 04:22:14 - train: epoch 0120, iter [03700, 05004], lr: 0.036291, loss: 2.2222
2022-07-16 04:22:49 - train: epoch 0120, iter [03800, 05004], lr: 0.036275, loss: 2.0653
2022-07-16 04:23:25 - train: epoch 0120, iter [03900, 05004], lr: 0.036260, loss: 2.6710
2022-07-16 04:24:00 - train: epoch 0120, iter [04000, 05004], lr: 0.036244, loss: 2.3698
2022-07-16 04:24:33 - train: epoch 0120, iter [04100, 05004], lr: 0.036229, loss: 2.6231
2022-07-16 04:25:08 - train: epoch 0120, iter [04200, 05004], lr: 0.036213, loss: 1.9780
2022-07-16 04:25:43 - train: epoch 0120, iter [04300, 05004], lr: 0.036198, loss: 2.3625
2022-07-16 04:26:18 - train: epoch 0120, iter [04400, 05004], lr: 0.036183, loss: 2.4011
2022-07-16 04:26:53 - train: epoch 0120, iter [04500, 05004], lr: 0.036167, loss: 2.3357
2022-07-16 04:27:27 - train: epoch 0120, iter [04600, 05004], lr: 0.036152, loss: 2.4132
2022-07-16 04:28:02 - train: epoch 0120, iter [04700, 05004], lr: 0.036136, loss: 2.4491
2022-07-16 04:28:37 - train: epoch 0120, iter [04800, 05004], lr: 0.036121, loss: 2.5033
2022-07-16 04:29:12 - train: epoch 0120, iter [04900, 05004], lr: 0.036105, loss: 2.4296
2022-07-16 04:29:45 - train: epoch 0120, iter [05000, 05004], lr: 0.036090, loss: 2.6887
2022-07-16 04:29:46 - train: epoch 120, train_loss: 2.4021
2022-07-16 04:31:01 - eval: epoch: 120, acc1: 66.398%, acc5: 87.722%, test_loss: 1.3757, per_image_load_time: 1.700ms, per_image_inference_time: 0.479ms
2022-07-16 04:31:02 - until epoch: 120, best_acc1: 66.398%
2022-07-16 04:31:02 - epoch 121 lr: 0.036089
2022-07-16 04:31:41 - train: epoch 0121, iter [00100, 05004], lr: 0.036074, loss: 2.4110
2022-07-16 04:32:16 - train: epoch 0121, iter [00200, 05004], lr: 0.036058, loss: 2.4650
2022-07-16 04:32:51 - train: epoch 0121, iter [00300, 05004], lr: 0.036043, loss: 2.4500
2022-07-16 04:33:25 - train: epoch 0121, iter [00400, 05004], lr: 0.036027, loss: 2.0419
2022-07-16 04:34:00 - train: epoch 0121, iter [00500, 05004], lr: 0.036012, loss: 2.3183
2022-07-16 04:34:34 - train: epoch 0121, iter [00600, 05004], lr: 0.035996, loss: 2.3444
2022-07-16 04:35:09 - train: epoch 0121, iter [00700, 05004], lr: 0.035981, loss: 2.3681
2022-07-16 04:35:44 - train: epoch 0121, iter [00800, 05004], lr: 0.035965, loss: 2.4787
2022-07-16 04:36:18 - train: epoch 0121, iter [00900, 05004], lr: 0.035950, loss: 2.4120
2022-07-16 04:36:52 - train: epoch 0121, iter [01000, 05004], lr: 0.035935, loss: 2.4262
2022-07-16 04:37:28 - train: epoch 0121, iter [01100, 05004], lr: 0.035919, loss: 2.4552
2022-07-16 04:38:02 - train: epoch 0121, iter [01200, 05004], lr: 0.035904, loss: 2.3953
2022-07-16 04:38:36 - train: epoch 0121, iter [01300, 05004], lr: 0.035888, loss: 2.5290
2022-07-16 04:39:11 - train: epoch 0121, iter [01400, 05004], lr: 0.035873, loss: 2.4010
2022-07-16 04:39:45 - train: epoch 0121, iter [01500, 05004], lr: 0.035857, loss: 2.3583
2022-07-16 04:40:19 - train: epoch 0121, iter [01600, 05004], lr: 0.035842, loss: 2.1625
2022-07-16 04:40:54 - train: epoch 0121, iter [01700, 05004], lr: 0.035826, loss: 2.4389
2022-07-16 04:41:28 - train: epoch 0121, iter [01800, 05004], lr: 0.035811, loss: 2.4937
2022-07-16 04:42:02 - train: epoch 0121, iter [01900, 05004], lr: 0.035796, loss: 2.7372
2022-07-16 04:42:37 - train: epoch 0121, iter [02000, 05004], lr: 0.035780, loss: 2.7102
2022-07-16 04:43:11 - train: epoch 0121, iter [02100, 05004], lr: 0.035765, loss: 2.6730
2022-07-16 04:43:46 - train: epoch 0121, iter [02200, 05004], lr: 0.035749, loss: 2.1851
2022-07-16 04:44:20 - train: epoch 0121, iter [02300, 05004], lr: 0.035734, loss: 2.0772
2022-07-16 04:44:56 - train: epoch 0121, iter [02400, 05004], lr: 0.035718, loss: 2.4330
2022-07-16 04:45:29 - train: epoch 0121, iter [02500, 05004], lr: 0.035703, loss: 2.2339
2022-07-16 04:46:04 - train: epoch 0121, iter [02600, 05004], lr: 0.035688, loss: 2.5545
2022-07-16 04:46:39 - train: epoch 0121, iter [02700, 05004], lr: 0.035672, loss: 2.2398
2022-07-16 04:47:14 - train: epoch 0121, iter [02800, 05004], lr: 0.035657, loss: 2.2862
2022-07-16 04:47:48 - train: epoch 0121, iter [02900, 05004], lr: 0.035641, loss: 2.6657
2022-07-16 04:48:22 - train: epoch 0121, iter [03000, 05004], lr: 0.035626, loss: 1.9530
2022-07-16 04:48:57 - train: epoch 0121, iter [03100, 05004], lr: 0.035610, loss: 2.5769
2022-07-16 04:49:32 - train: epoch 0121, iter [03200, 05004], lr: 0.035595, loss: 2.3417
2022-07-16 04:50:06 - train: epoch 0121, iter [03300, 05004], lr: 0.035580, loss: 2.3119
2022-07-16 04:50:41 - train: epoch 0121, iter [03400, 05004], lr: 0.035564, loss: 2.4968
2022-07-16 04:51:15 - train: epoch 0121, iter [03500, 05004], lr: 0.035549, loss: 2.2582
2022-07-16 04:51:50 - train: epoch 0121, iter [03600, 05004], lr: 0.035533, loss: 2.3898
2022-07-16 04:52:24 - train: epoch 0121, iter [03700, 05004], lr: 0.035518, loss: 2.4582
2022-07-16 04:52:59 - train: epoch 0121, iter [03800, 05004], lr: 0.035503, loss: 2.6826
2022-07-16 04:53:34 - train: epoch 0121, iter [03900, 05004], lr: 0.035487, loss: 2.6309
2022-07-16 04:54:09 - train: epoch 0121, iter [04000, 05004], lr: 0.035472, loss: 2.1322
2022-07-16 04:54:43 - train: epoch 0121, iter [04100, 05004], lr: 0.035456, loss: 2.2655
2022-07-16 04:55:18 - train: epoch 0121, iter [04200, 05004], lr: 0.035441, loss: 2.6892
2022-07-16 04:55:53 - train: epoch 0121, iter [04300, 05004], lr: 0.035426, loss: 2.2025
2022-07-16 04:56:27 - train: epoch 0121, iter [04400, 05004], lr: 0.035410, loss: 2.6362
2022-07-16 04:57:01 - train: epoch 0121, iter [04500, 05004], lr: 0.035395, loss: 2.5513
2022-07-16 04:57:36 - train: epoch 0121, iter [04600, 05004], lr: 0.035379, loss: 2.1202
2022-07-16 04:58:12 - train: epoch 0121, iter [04700, 05004], lr: 0.035364, loss: 2.6946
2022-07-16 04:58:46 - train: epoch 0121, iter [04800, 05004], lr: 0.035349, loss: 2.4265
2022-07-16 04:59:21 - train: epoch 0121, iter [04900, 05004], lr: 0.035333, loss: 2.5008
2022-07-16 04:59:54 - train: epoch 0121, iter [05000, 05004], lr: 0.035318, loss: 2.3110
2022-07-16 04:59:55 - train: epoch 121, train_loss: 2.3914
2022-07-16 05:01:10 - eval: epoch: 121, acc1: 66.782%, acc5: 87.902%, test_loss: 1.3548, per_image_load_time: 1.787ms, per_image_inference_time: 0.494ms
2022-07-16 05:01:11 - until epoch: 121, best_acc1: 66.782%
2022-07-16 05:01:11 - epoch 122 lr: 0.035317
2022-07-16 05:01:51 - train: epoch 0122, iter [00100, 05004], lr: 0.035302, loss: 2.4688
2022-07-16 05:02:25 - train: epoch 0122, iter [00200, 05004], lr: 0.035286, loss: 2.1074
2022-07-16 05:03:01 - train: epoch 0122, iter [00300, 05004], lr: 0.035271, loss: 2.3922
2022-07-16 05:03:35 - train: epoch 0122, iter [00400, 05004], lr: 0.035256, loss: 1.9234
2022-07-16 05:04:10 - train: epoch 0122, iter [00500, 05004], lr: 0.035240, loss: 2.4314
2022-07-16 05:04:44 - train: epoch 0122, iter [00600, 05004], lr: 0.035225, loss: 2.5514
2022-07-16 05:05:18 - train: epoch 0122, iter [00700, 05004], lr: 0.035210, loss: 2.3397
2022-07-16 05:05:53 - train: epoch 0122, iter [00800, 05004], lr: 0.035194, loss: 2.2901
2022-07-16 05:06:28 - train: epoch 0122, iter [00900, 05004], lr: 0.035179, loss: 2.3429
2022-07-16 05:07:02 - train: epoch 0122, iter [01000, 05004], lr: 0.035163, loss: 2.1196
2022-07-16 05:07:38 - train: epoch 0122, iter [01100, 05004], lr: 0.035148, loss: 2.1628
2022-07-16 05:08:12 - train: epoch 0122, iter [01200, 05004], lr: 0.035133, loss: 2.5941
2022-07-16 05:08:47 - train: epoch 0122, iter [01300, 05004], lr: 0.035117, loss: 2.3624
2022-07-16 05:09:22 - train: epoch 0122, iter [01400, 05004], lr: 0.035102, loss: 2.5220
2022-07-16 05:09:57 - train: epoch 0122, iter [01500, 05004], lr: 0.035087, loss: 2.1663
2022-07-16 05:10:32 - train: epoch 0122, iter [01600, 05004], lr: 0.035071, loss: 2.2487
2022-07-16 05:11:06 - train: epoch 0122, iter [01700, 05004], lr: 0.035056, loss: 2.4578
2022-07-16 05:11:40 - train: epoch 0122, iter [01800, 05004], lr: 0.035040, loss: 2.2540
2022-07-16 05:12:15 - train: epoch 0122, iter [01900, 05004], lr: 0.035025, loss: 2.2384
2022-07-16 05:12:50 - train: epoch 0122, iter [02000, 05004], lr: 0.035010, loss: 2.3305
2022-07-16 05:13:24 - train: epoch 0122, iter [02100, 05004], lr: 0.034994, loss: 2.4326
2022-07-16 05:13:59 - train: epoch 0122, iter [02200, 05004], lr: 0.034979, loss: 2.6008
2022-07-16 05:14:34 - train: epoch 0122, iter [02300, 05004], lr: 0.034964, loss: 2.4359
2022-07-16 05:15:08 - train: epoch 0122, iter [02400, 05004], lr: 0.034948, loss: 2.4314
2022-07-16 05:15:43 - train: epoch 0122, iter [02500, 05004], lr: 0.034933, loss: 2.4676
2022-07-16 05:16:17 - train: epoch 0122, iter [02600, 05004], lr: 0.034918, loss: 2.2556
2022-07-16 05:16:51 - train: epoch 0122, iter [02700, 05004], lr: 0.034902, loss: 2.5387
2022-07-16 05:17:25 - train: epoch 0122, iter [02800, 05004], lr: 0.034887, loss: 2.4380
2022-07-16 05:17:59 - train: epoch 0122, iter [02900, 05004], lr: 0.034872, loss: 2.4933
2022-07-16 05:18:35 - train: epoch 0122, iter [03000, 05004], lr: 0.034856, loss: 2.7137
2022-07-16 05:19:10 - train: epoch 0122, iter [03100, 05004], lr: 0.034841, loss: 2.4872
2022-07-16 05:19:44 - train: epoch 0122, iter [03200, 05004], lr: 0.034826, loss: 2.7141
2022-07-16 05:20:18 - train: epoch 0122, iter [03300, 05004], lr: 0.034810, loss: 2.2586
2022-07-16 05:20:54 - train: epoch 0122, iter [03400, 05004], lr: 0.034795, loss: 2.3685
2022-07-16 05:21:28 - train: epoch 0122, iter [03500, 05004], lr: 0.034780, loss: 2.4137
2022-07-16 05:22:03 - train: epoch 0122, iter [03600, 05004], lr: 0.034764, loss: 2.6064
2022-07-16 05:22:37 - train: epoch 0122, iter [03700, 05004], lr: 0.034749, loss: 2.3992
2022-07-16 05:23:12 - train: epoch 0122, iter [03800, 05004], lr: 0.034734, loss: 2.3944
2022-07-16 05:23:47 - train: epoch 0122, iter [03900, 05004], lr: 0.034718, loss: 2.5201
2022-07-16 05:24:22 - train: epoch 0122, iter [04000, 05004], lr: 0.034703, loss: 2.4406
2022-07-16 05:24:56 - train: epoch 0122, iter [04100, 05004], lr: 0.034688, loss: 2.3026
2022-07-16 05:25:30 - train: epoch 0122, iter [04200, 05004], lr: 0.034672, loss: 2.2490
2022-07-16 05:26:05 - train: epoch 0122, iter [04300, 05004], lr: 0.034657, loss: 2.4499
2022-07-16 05:26:39 - train: epoch 0122, iter [04400, 05004], lr: 0.034642, loss: 2.7861
2022-07-16 05:27:14 - train: epoch 0122, iter [04500, 05004], lr: 0.034626, loss: 2.3169
2022-07-16 05:27:48 - train: epoch 0122, iter [04600, 05004], lr: 0.034611, loss: 2.4461
2022-07-16 05:28:24 - train: epoch 0122, iter [04700, 05004], lr: 0.034596, loss: 2.2455
2022-07-16 05:28:57 - train: epoch 0122, iter [04800, 05004], lr: 0.034580, loss: 2.3424
2022-07-16 05:29:32 - train: epoch 0122, iter [04900, 05004], lr: 0.034565, loss: 2.5808
2022-07-16 05:30:06 - train: epoch 0122, iter [05000, 05004], lr: 0.034550, loss: 2.6754
2022-07-16 05:30:07 - train: epoch 122, train_loss: 2.3897
2022-07-16 05:31:22 - eval: epoch: 122, acc1: 64.992%, acc5: 86.654%, test_loss: 1.4398, per_image_load_time: 1.554ms, per_image_inference_time: 0.497ms
2022-07-16 05:31:22 - until epoch: 122, best_acc1: 66.782%
2022-07-16 05:31:22 - epoch 123 lr: 0.034549
2022-07-16 05:32:02 - train: epoch 0123, iter [00100, 05004], lr: 0.034534, loss: 2.5878
2022-07-16 05:32:37 - train: epoch 0123, iter [00200, 05004], lr: 0.034519, loss: 2.5210
2022-07-16 05:33:12 - train: epoch 0123, iter [00300, 05004], lr: 0.034503, loss: 2.5491
2022-07-16 05:33:46 - train: epoch 0123, iter [00400, 05004], lr: 0.034488, loss: 2.2673
2022-07-16 05:34:21 - train: epoch 0123, iter [00500, 05004], lr: 0.034473, loss: 2.3177
2022-07-16 05:34:55 - train: epoch 0123, iter [00600, 05004], lr: 0.034457, loss: 2.0348
2022-07-16 05:35:30 - train: epoch 0123, iter [00700, 05004], lr: 0.034442, loss: 2.4842
2022-07-16 05:36:05 - train: epoch 0123, iter [00800, 05004], lr: 0.034427, loss: 2.2718
2022-07-16 05:36:40 - train: epoch 0123, iter [00900, 05004], lr: 0.034411, loss: 2.4737
2022-07-16 05:37:14 - train: epoch 0123, iter [01000, 05004], lr: 0.034396, loss: 2.3927
2022-07-16 05:37:49 - train: epoch 0123, iter [01100, 05004], lr: 0.034381, loss: 2.4004
2022-07-16 05:38:23 - train: epoch 0123, iter [01200, 05004], lr: 0.034366, loss: 2.3146
2022-07-16 05:38:57 - train: epoch 0123, iter [01300, 05004], lr: 0.034350, loss: 2.2012
2022-07-16 05:39:32 - train: epoch 0123, iter [01400, 05004], lr: 0.034335, loss: 2.3439
2022-07-16 05:40:07 - train: epoch 0123, iter [01500, 05004], lr: 0.034320, loss: 2.4070
2022-07-16 05:40:42 - train: epoch 0123, iter [01600, 05004], lr: 0.034304, loss: 2.4542
2022-07-16 05:41:16 - train: epoch 0123, iter [01700, 05004], lr: 0.034289, loss: 2.1959
2022-07-16 05:41:51 - train: epoch 0123, iter [01800, 05004], lr: 0.034274, loss: 2.2630
2022-07-16 05:42:26 - train: epoch 0123, iter [01900, 05004], lr: 0.034259, loss: 2.4896
2022-07-16 05:43:01 - train: epoch 0123, iter [02000, 05004], lr: 0.034243, loss: 2.4548
2022-07-16 05:43:34 - train: epoch 0123, iter [02100, 05004], lr: 0.034228, loss: 2.3436
2022-07-16 05:44:10 - train: epoch 0123, iter [02200, 05004], lr: 0.034213, loss: 2.1446
2022-07-16 05:44:44 - train: epoch 0123, iter [02300, 05004], lr: 0.034197, loss: 2.8288
2022-07-16 05:45:19 - train: epoch 0123, iter [02400, 05004], lr: 0.034182, loss: 2.3670
2022-07-16 05:45:53 - train: epoch 0123, iter [02500, 05004], lr: 0.034167, loss: 2.3134
2022-07-16 05:46:28 - train: epoch 0123, iter [02600, 05004], lr: 0.034152, loss: 2.3775
2022-07-16 05:47:02 - train: epoch 0123, iter [02700, 05004], lr: 0.034136, loss: 2.6345
2022-07-16 05:47:38 - train: epoch 0123, iter [02800, 05004], lr: 0.034121, loss: 2.4672
2022-07-16 05:48:12 - train: epoch 0123, iter [02900, 05004], lr: 0.034106, loss: 2.3729
2022-07-16 05:48:47 - train: epoch 0123, iter [03000, 05004], lr: 0.034091, loss: 2.5445
2022-07-16 05:49:21 - train: epoch 0123, iter [03100, 05004], lr: 0.034075, loss: 2.3317
2022-07-16 05:49:55 - train: epoch 0123, iter [03200, 05004], lr: 0.034060, loss: 2.2801
2022-07-16 05:50:30 - train: epoch 0123, iter [03300, 05004], lr: 0.034045, loss: 2.2873
2022-07-16 05:51:05 - train: epoch 0123, iter [03400, 05004], lr: 0.034030, loss: 2.5261
2022-07-16 05:51:39 - train: epoch 0123, iter [03500, 05004], lr: 0.034014, loss: 2.7876
2022-07-16 05:52:14 - train: epoch 0123, iter [03600, 05004], lr: 0.033999, loss: 2.3787
2022-07-16 05:52:49 - train: epoch 0123, iter [03700, 05004], lr: 0.033984, loss: 2.4227
2022-07-16 05:53:23 - train: epoch 0123, iter [03800, 05004], lr: 0.033969, loss: 2.4542
2022-07-16 05:53:59 - train: epoch 0123, iter [03900, 05004], lr: 0.033953, loss: 2.4437
2022-07-16 05:54:32 - train: epoch 0123, iter [04000, 05004], lr: 0.033938, loss: 2.3990
2022-07-16 05:55:07 - train: epoch 0123, iter [04100, 05004], lr: 0.033923, loss: 2.3662
2022-07-16 05:55:42 - train: epoch 0123, iter [04200, 05004], lr: 0.033908, loss: 2.6075
2022-07-16 05:56:17 - train: epoch 0123, iter [04300, 05004], lr: 0.033892, loss: 2.3490
2022-07-16 05:56:51 - train: epoch 0123, iter [04400, 05004], lr: 0.033877, loss: 2.4383
2022-07-16 05:57:26 - train: epoch 0123, iter [04500, 05004], lr: 0.033862, loss: 2.5246
2022-07-16 05:58:01 - train: epoch 0123, iter [04600, 05004], lr: 0.033847, loss: 2.5755
2022-07-16 05:58:35 - train: epoch 0123, iter [04700, 05004], lr: 0.033831, loss: 2.2511
2022-07-16 05:59:09 - train: epoch 0123, iter [04800, 05004], lr: 0.033816, loss: 2.3494
2022-07-16 05:59:44 - train: epoch 0123, iter [04900, 05004], lr: 0.033801, loss: 2.3712
2022-07-16 06:00:18 - train: epoch 0123, iter [05000, 05004], lr: 0.033786, loss: 2.7174
2022-07-16 06:00:19 - train: epoch 123, train_loss: 2.3819
2022-07-16 06:01:34 - eval: epoch: 123, acc1: 65.162%, acc5: 86.956%, test_loss: 1.4342, per_image_load_time: 2.144ms, per_image_inference_time: 0.488ms
2022-07-16 06:01:35 - until epoch: 123, best_acc1: 66.782%
2022-07-16 06:01:35 - epoch 124 lr: 0.033785
2022-07-16 06:02:14 - train: epoch 0124, iter [00100, 05004], lr: 0.033770, loss: 2.1171
2022-07-16 06:02:49 - train: epoch 0124, iter [00200, 05004], lr: 0.033755, loss: 2.3894
2022-07-16 06:03:23 - train: epoch 0124, iter [00300, 05004], lr: 0.033739, loss: 2.1363
2022-07-16 06:03:58 - train: epoch 0124, iter [00400, 05004], lr: 0.033724, loss: 2.1101
2022-07-16 06:04:33 - train: epoch 0124, iter [00500, 05004], lr: 0.033709, loss: 2.2234
2022-07-16 06:05:07 - train: epoch 0124, iter [00600, 05004], lr: 0.033694, loss: 2.5523
2022-07-16 06:05:42 - train: epoch 0124, iter [00700, 05004], lr: 0.033679, loss: 2.4645
2022-07-16 06:06:16 - train: epoch 0124, iter [00800, 05004], lr: 0.033663, loss: 2.1679
2022-07-16 06:06:51 - train: epoch 0124, iter [00900, 05004], lr: 0.033648, loss: 2.6472
2022-07-16 06:07:26 - train: epoch 0124, iter [01000, 05004], lr: 0.033633, loss: 2.3930
2022-07-16 06:07:59 - train: epoch 0124, iter [01100, 05004], lr: 0.033618, loss: 2.3943
2022-07-16 06:08:36 - train: epoch 0124, iter [01200, 05004], lr: 0.033602, loss: 2.1616
2022-07-16 06:09:10 - train: epoch 0124, iter [01300, 05004], lr: 0.033587, loss: 2.3666
2022-07-16 06:09:44 - train: epoch 0124, iter [01400, 05004], lr: 0.033572, loss: 2.1211
2022-07-16 06:10:19 - train: epoch 0124, iter [01500, 05004], lr: 0.033557, loss: 2.2376
2022-07-16 06:10:53 - train: epoch 0124, iter [01600, 05004], lr: 0.033542, loss: 2.3689
2022-07-16 06:11:28 - train: epoch 0124, iter [01700, 05004], lr: 0.033526, loss: 2.3007
2022-07-16 06:12:02 - train: epoch 0124, iter [01800, 05004], lr: 0.033511, loss: 2.4204
2022-07-16 06:12:36 - train: epoch 0124, iter [01900, 05004], lr: 0.033496, loss: 2.4674
2022-07-16 06:13:10 - train: epoch 0124, iter [02000, 05004], lr: 0.033481, loss: 2.5280
2022-07-16 06:13:46 - train: epoch 0124, iter [02100, 05004], lr: 0.033466, loss: 2.3442
2022-07-16 06:14:20 - train: epoch 0124, iter [02200, 05004], lr: 0.033450, loss: 2.5074
2022-07-16 06:14:54 - train: epoch 0124, iter [02300, 05004], lr: 0.033435, loss: 2.2873
2022-07-16 06:15:29 - train: epoch 0124, iter [02400, 05004], lr: 0.033420, loss: 2.3860
2022-07-16 06:16:04 - train: epoch 0124, iter [02500, 05004], lr: 0.033405, loss: 2.3889
2022-07-16 06:16:38 - train: epoch 0124, iter [02600, 05004], lr: 0.033390, loss: 2.3023
2022-07-16 06:17:12 - train: epoch 0124, iter [02700, 05004], lr: 0.033375, loss: 2.5066
2022-07-16 06:17:47 - train: epoch 0124, iter [02800, 05004], lr: 0.033359, loss: 2.2354
2022-07-16 06:18:21 - train: epoch 0124, iter [02900, 05004], lr: 0.033344, loss: 2.3930
2022-07-16 06:18:56 - train: epoch 0124, iter [03000, 05004], lr: 0.033329, loss: 2.2873
2022-07-16 06:19:31 - train: epoch 0124, iter [03100, 05004], lr: 0.033314, loss: 2.5527
2022-07-16 06:20:06 - train: epoch 0124, iter [03200, 05004], lr: 0.033299, loss: 2.2990
2022-07-16 06:20:40 - train: epoch 0124, iter [03300, 05004], lr: 0.033283, loss: 2.4502
2022-07-16 06:21:15 - train: epoch 0124, iter [03400, 05004], lr: 0.033268, loss: 2.4913
2022-07-16 06:21:49 - train: epoch 0124, iter [03500, 05004], lr: 0.033253, loss: 2.4924
2022-07-16 06:22:24 - train: epoch 0124, iter [03600, 05004], lr: 0.033238, loss: 2.3643
2022-07-16 06:22:59 - train: epoch 0124, iter [03700, 05004], lr: 0.033223, loss: 2.4382
2022-07-16 06:23:34 - train: epoch 0124, iter [03800, 05004], lr: 0.033208, loss: 2.2036
2022-07-16 06:24:09 - train: epoch 0124, iter [03900, 05004], lr: 0.033192, loss: 2.2367
2022-07-16 06:24:43 - train: epoch 0124, iter [04000, 05004], lr: 0.033177, loss: 2.0756
2022-07-16 06:25:18 - train: epoch 0124, iter [04100, 05004], lr: 0.033162, loss: 2.2315
2022-07-16 06:25:52 - train: epoch 0124, iter [04200, 05004], lr: 0.033147, loss: 2.3007
2022-07-16 06:26:27 - train: epoch 0124, iter [04300, 05004], lr: 0.033132, loss: 2.4122
2022-07-16 06:27:02 - train: epoch 0124, iter [04400, 05004], lr: 0.033117, loss: 2.3660
2022-07-16 06:27:37 - train: epoch 0124, iter [04500, 05004], lr: 0.033102, loss: 2.2674
2022-07-16 06:28:12 - train: epoch 0124, iter [04600, 05004], lr: 0.033086, loss: 2.8411
2022-07-16 06:28:47 - train: epoch 0124, iter [04700, 05004], lr: 0.033071, loss: 2.4253
2022-07-16 06:29:21 - train: epoch 0124, iter [04800, 05004], lr: 0.033056, loss: 2.5628
2022-07-16 06:29:56 - train: epoch 0124, iter [04900, 05004], lr: 0.033041, loss: 2.0602
2022-07-16 06:30:29 - train: epoch 0124, iter [05000, 05004], lr: 0.033026, loss: 2.4206
2022-07-16 06:30:30 - train: epoch 124, train_loss: 2.3726
2022-07-16 06:31:46 - eval: epoch: 124, acc1: 64.682%, acc5: 86.660%, test_loss: 1.4496, per_image_load_time: 1.103ms, per_image_inference_time: 0.484ms
2022-07-16 06:31:46 - until epoch: 124, best_acc1: 66.782%
2022-07-16 06:31:46 - epoch 125 lr: 0.033025
2022-07-16 06:32:27 - train: epoch 0125, iter [00100, 05004], lr: 0.033010, loss: 2.2872
2022-07-16 06:33:01 - train: epoch 0125, iter [00200, 05004], lr: 0.032995, loss: 2.4930
2022-07-16 06:33:37 - train: epoch 0125, iter [00300, 05004], lr: 0.032980, loss: 2.2433
2022-07-16 06:34:11 - train: epoch 0125, iter [00400, 05004], lr: 0.032965, loss: 2.2877
2022-07-16 06:34:46 - train: epoch 0125, iter [00500, 05004], lr: 0.032950, loss: 2.4866
2022-07-16 06:35:20 - train: epoch 0125, iter [00600, 05004], lr: 0.032934, loss: 1.9865
2022-07-16 06:35:54 - train: epoch 0125, iter [00700, 05004], lr: 0.032919, loss: 2.2060
2022-07-16 06:36:29 - train: epoch 0125, iter [00800, 05004], lr: 0.032904, loss: 2.2136
2022-07-16 06:37:03 - train: epoch 0125, iter [00900, 05004], lr: 0.032889, loss: 2.3491
2022-07-16 06:37:37 - train: epoch 0125, iter [01000, 05004], lr: 0.032874, loss: 2.2853
2022-07-16 06:38:10 - train: epoch 0125, iter [01100, 05004], lr: 0.032859, loss: 2.3363
2022-07-16 06:38:46 - train: epoch 0125, iter [01200, 05004], lr: 0.032844, loss: 2.4817
2022-07-16 06:39:19 - train: epoch 0125, iter [01300, 05004], lr: 0.032829, loss: 2.2047
2022-07-16 06:39:54 - train: epoch 0125, iter [01400, 05004], lr: 0.032813, loss: 2.6212
2022-07-16 06:40:27 - train: epoch 0125, iter [01500, 05004], lr: 0.032798, loss: 2.5804
2022-07-16 06:41:01 - train: epoch 0125, iter [01600, 05004], lr: 0.032783, loss: 2.2558
2022-07-16 06:41:35 - train: epoch 0125, iter [01700, 05004], lr: 0.032768, loss: 2.6579
2022-07-16 06:42:10 - train: epoch 0125, iter [01800, 05004], lr: 0.032753, loss: 2.4094
2022-07-16 06:42:44 - train: epoch 0125, iter [01900, 05004], lr: 0.032738, loss: 2.2586
2022-07-16 06:43:18 - train: epoch 0125, iter [02000, 05004], lr: 0.032723, loss: 2.4240
2022-07-16 06:43:52 - train: epoch 0125, iter [02100, 05004], lr: 0.032708, loss: 2.4651
2022-07-16 06:44:26 - train: epoch 0125, iter [02200, 05004], lr: 0.032693, loss: 2.7209
2022-07-16 06:45:01 - train: epoch 0125, iter [02300, 05004], lr: 0.032677, loss: 2.1803
2022-07-16 06:45:36 - train: epoch 0125, iter [02400, 05004], lr: 0.032662, loss: 2.4661
2022-07-16 06:46:09 - train: epoch 0125, iter [02500, 05004], lr: 0.032647, loss: 2.2463
2022-07-16 06:46:44 - train: epoch 0125, iter [02600, 05004], lr: 0.032632, loss: 2.1580
2022-07-16 06:47:18 - train: epoch 0125, iter [02700, 05004], lr: 0.032617, loss: 2.4364
2022-07-16 06:47:52 - train: epoch 0125, iter [02800, 05004], lr: 0.032602, loss: 2.9124
2022-07-16 06:48:27 - train: epoch 0125, iter [02900, 05004], lr: 0.032587, loss: 2.1627
2022-07-16 06:49:02 - train: epoch 0125, iter [03000, 05004], lr: 0.032572, loss: 2.3480
2022-07-16 06:49:36 - train: epoch 0125, iter [03100, 05004], lr: 0.032557, loss: 2.7589
2022-07-16 06:50:10 - train: epoch 0125, iter [03200, 05004], lr: 0.032542, loss: 2.3818
2022-07-16 06:50:45 - train: epoch 0125, iter [03300, 05004], lr: 0.032527, loss: 2.1500
2022-07-16 06:51:19 - train: epoch 0125, iter [03400, 05004], lr: 0.032511, loss: 2.4302
2022-07-16 06:51:53 - train: epoch 0125, iter [03500, 05004], lr: 0.032496, loss: 2.3960
2022-07-16 06:52:28 - train: epoch 0125, iter [03600, 05004], lr: 0.032481, loss: 2.2313
2022-07-16 06:53:03 - train: epoch 0125, iter [03700, 05004], lr: 0.032466, loss: 2.3866
2022-07-16 06:53:37 - train: epoch 0125, iter [03800, 05004], lr: 0.032451, loss: 2.0647
2022-07-16 06:54:12 - train: epoch 0125, iter [03900, 05004], lr: 0.032436, loss: 2.3034
2022-07-16 06:54:45 - train: epoch 0125, iter [04000, 05004], lr: 0.032421, loss: 2.3978
2022-07-16 06:55:20 - train: epoch 0125, iter [04100, 05004], lr: 0.032406, loss: 2.2443
2022-07-16 06:55:54 - train: epoch 0125, iter [04200, 05004], lr: 0.032391, loss: 2.7293
2022-07-16 06:56:29 - train: epoch 0125, iter [04300, 05004], lr: 0.032376, loss: 2.2361
2022-07-16 06:57:03 - train: epoch 0125, iter [04400, 05004], lr: 0.032361, loss: 2.6067
2022-07-16 06:57:37 - train: epoch 0125, iter [04500, 05004], lr: 0.032346, loss: 2.6117
2022-07-16 06:58:12 - train: epoch 0125, iter [04600, 05004], lr: 0.032331, loss: 2.2136
2022-07-16 06:58:47 - train: epoch 0125, iter [04700, 05004], lr: 0.032316, loss: 2.5733
2022-07-16 06:59:22 - train: epoch 0125, iter [04800, 05004], lr: 0.032300, loss: 2.3284
2022-07-16 06:59:56 - train: epoch 0125, iter [04900, 05004], lr: 0.032285, loss: 2.4639
2022-07-16 07:00:30 - train: epoch 0125, iter [05000, 05004], lr: 0.032270, loss: 2.4482
2022-07-16 07:00:31 - train: epoch 125, train_loss: 2.3636
2022-07-16 07:01:46 - eval: epoch: 125, acc1: 66.722%, acc5: 87.914%, test_loss: 1.3595, per_image_load_time: 1.314ms, per_image_inference_time: 0.472ms
2022-07-16 07:01:46 - until epoch: 125, best_acc1: 66.782%
2022-07-16 07:01:46 - epoch 126 lr: 0.032270
2022-07-16 07:02:26 - train: epoch 0126, iter [00100, 05004], lr: 0.032255, loss: 2.4516
2022-07-16 07:03:00 - train: epoch 0126, iter [00200, 05004], lr: 0.032240, loss: 2.2765
2022-07-16 07:03:35 - train: epoch 0126, iter [00300, 05004], lr: 0.032225, loss: 2.2540
2022-07-16 07:04:10 - train: epoch 0126, iter [00400, 05004], lr: 0.032210, loss: 2.5272
2022-07-16 07:04:45 - train: epoch 0126, iter [00500, 05004], lr: 0.032195, loss: 2.5245
2022-07-16 07:05:19 - train: epoch 0126, iter [00600, 05004], lr: 0.032179, loss: 2.0831
2022-07-16 07:05:54 - train: epoch 0126, iter [00700, 05004], lr: 0.032164, loss: 2.7455
2022-07-16 07:06:29 - train: epoch 0126, iter [00800, 05004], lr: 0.032149, loss: 2.4541
2022-07-16 07:07:04 - train: epoch 0126, iter [00900, 05004], lr: 0.032134, loss: 2.4214
2022-07-16 07:07:38 - train: epoch 0126, iter [01000, 05004], lr: 0.032119, loss: 2.2014
2022-07-16 07:08:13 - train: epoch 0126, iter [01100, 05004], lr: 0.032104, loss: 2.5174
2022-07-16 07:08:47 - train: epoch 0126, iter [01200, 05004], lr: 0.032089, loss: 2.2789
2022-07-16 07:09:23 - train: epoch 0126, iter [01300, 05004], lr: 0.032074, loss: 2.3331
2022-07-16 07:09:57 - train: epoch 0126, iter [01400, 05004], lr: 0.032059, loss: 2.6019
2022-07-16 07:10:32 - train: epoch 0126, iter [01500, 05004], lr: 0.032044, loss: 2.2979
2022-07-16 07:11:07 - train: epoch 0126, iter [01600, 05004], lr: 0.032029, loss: 2.2886
2022-07-16 07:11:41 - train: epoch 0126, iter [01700, 05004], lr: 0.032014, loss: 2.4093
2022-07-16 07:12:16 - train: epoch 0126, iter [01800, 05004], lr: 0.031999, loss: 2.1870
2022-07-16 07:12:51 - train: epoch 0126, iter [01900, 05004], lr: 0.031984, loss: 2.1875
2022-07-16 07:13:26 - train: epoch 0126, iter [02000, 05004], lr: 0.031969, loss: 2.4577
2022-07-16 07:14:00 - train: epoch 0126, iter [02100, 05004], lr: 0.031954, loss: 2.3060
2022-07-16 07:14:35 - train: epoch 0126, iter [02200, 05004], lr: 0.031939, loss: 2.5842
2022-07-16 07:15:10 - train: epoch 0126, iter [02300, 05004], lr: 0.031924, loss: 2.1952
2022-07-16 07:15:44 - train: epoch 0126, iter [02400, 05004], lr: 0.031909, loss: 2.5189
2022-07-16 07:16:19 - train: epoch 0126, iter [02500, 05004], lr: 0.031894, loss: 2.6926
2022-07-16 07:16:54 - train: epoch 0126, iter [02600, 05004], lr: 0.031879, loss: 2.6864
2022-07-16 07:17:29 - train: epoch 0126, iter [02700, 05004], lr: 0.031864, loss: 2.1971
2022-07-16 07:18:03 - train: epoch 0126, iter [02800, 05004], lr: 0.031849, loss: 2.3537
2022-07-16 07:18:38 - train: epoch 0126, iter [02900, 05004], lr: 0.031834, loss: 2.7385
2022-07-16 07:19:12 - train: epoch 0126, iter [03000, 05004], lr: 0.031819, loss: 2.1370
2022-07-16 07:19:47 - train: epoch 0126, iter [03100, 05004], lr: 0.031804, loss: 2.5448
2022-07-16 07:20:22 - train: epoch 0126, iter [03200, 05004], lr: 0.031789, loss: 2.2414
2022-07-16 07:20:56 - train: epoch 0126, iter [03300, 05004], lr: 0.031774, loss: 2.4689
2022-07-16 07:21:33 - train: epoch 0126, iter [03400, 05004], lr: 0.031759, loss: 2.3564
2022-07-16 07:22:06 - train: epoch 0126, iter [03500, 05004], lr: 0.031744, loss: 2.8060
2022-07-16 07:22:41 - train: epoch 0126, iter [03600, 05004], lr: 0.031729, loss: 2.1355
2022-07-16 07:23:16 - train: epoch 0126, iter [03700, 05004], lr: 0.031714, loss: 2.1618
2022-07-16 07:23:51 - train: epoch 0126, iter [03800, 05004], lr: 0.031699, loss: 2.2316
2022-07-16 07:24:25 - train: epoch 0126, iter [03900, 05004], lr: 0.031684, loss: 2.3156
2022-07-16 07:25:00 - train: epoch 0126, iter [04000, 05004], lr: 0.031669, loss: 2.2374
2022-07-16 07:25:34 - train: epoch 0126, iter [04100, 05004], lr: 0.031654, loss: 2.5227
2022-07-16 07:26:09 - train: epoch 0126, iter [04200, 05004], lr: 0.031639, loss: 2.3183
2022-07-16 07:26:44 - train: epoch 0126, iter [04300, 05004], lr: 0.031624, loss: 2.3212
2022-07-16 07:27:19 - train: epoch 0126, iter [04400, 05004], lr: 0.031609, loss: 2.3192
2022-07-16 07:27:53 - train: epoch 0126, iter [04500, 05004], lr: 0.031594, loss: 2.2267
2022-07-16 07:28:27 - train: epoch 0126, iter [04600, 05004], lr: 0.031579, loss: 2.3640
2022-07-16 07:29:03 - train: epoch 0126, iter [04700, 05004], lr: 0.031564, loss: 2.1948
2022-07-16 07:29:38 - train: epoch 0126, iter [04800, 05004], lr: 0.031549, loss: 2.4587
2022-07-16 07:30:13 - train: epoch 0126, iter [04900, 05004], lr: 0.031534, loss: 2.2822
2022-07-16 07:30:46 - train: epoch 0126, iter [05000, 05004], lr: 0.031519, loss: 2.4324
2022-07-16 07:30:47 - train: epoch 126, train_loss: 2.3525
2022-07-16 07:32:02 - eval: epoch: 126, acc1: 65.812%, acc5: 87.374%, test_loss: 1.3954, per_image_load_time: 1.469ms, per_image_inference_time: 0.483ms
2022-07-16 07:32:02 - until epoch: 126, best_acc1: 66.782%
2022-07-16 07:32:02 - epoch 127 lr: 0.031519
2022-07-16 07:32:42 - train: epoch 0127, iter [00100, 05004], lr: 0.031504, loss: 2.7487
2022-07-16 07:33:17 - train: epoch 0127, iter [00200, 05004], lr: 0.031489, loss: 1.9830
2022-07-16 07:33:52 - train: epoch 0127, iter [00300, 05004], lr: 0.031474, loss: 2.4404
2022-07-16 07:34:26 - train: epoch 0127, iter [00400, 05004], lr: 0.031459, loss: 2.5089
2022-07-16 07:35:01 - train: epoch 0127, iter [00500, 05004], lr: 0.031444, loss: 2.1224
2022-07-16 07:35:36 - train: epoch 0127, iter [00600, 05004], lr: 0.031429, loss: 2.6835
2022-07-16 07:36:09 - train: epoch 0127, iter [00700, 05004], lr: 0.031414, loss: 2.2297
2022-07-16 07:36:44 - train: epoch 0127, iter [00800, 05004], lr: 0.031399, loss: 2.2667
2022-07-16 07:37:18 - train: epoch 0127, iter [00900, 05004], lr: 0.031384, loss: 2.0085
2022-07-16 07:37:53 - train: epoch 0127, iter [01000, 05004], lr: 0.031369, loss: 2.5213
2022-07-16 07:38:27 - train: epoch 0127, iter [01100, 05004], lr: 0.031354, loss: 2.1718
2022-07-16 07:39:01 - train: epoch 0127, iter [01200, 05004], lr: 0.031340, loss: 2.3508
2022-07-16 07:39:35 - train: epoch 0127, iter [01300, 05004], lr: 0.031325, loss: 2.2844
2022-07-16 07:40:10 - train: epoch 0127, iter [01400, 05004], lr: 0.031310, loss: 2.3751
2022-07-16 07:40:43 - train: epoch 0127, iter [01500, 05004], lr: 0.031295, loss: 2.2935
