2022-07-13 08:28:20 - network: resnet50
2022-07-13 08:28:20 - num_classes: 1000
2022-07-13 08:28:20 - input_image_size: 224
2022-07-13 08:28:20 - scale: 1.1428571428571428
2022-07-13 08:28:20 - trained_model_path: 
2022-07-13 08:28:20 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-13 08:28:20 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-13 08:28:20 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fa3e3118df0>
2022-07-13 08:28:20 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fa3c794a100>
2022-07-13 08:28:20 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fa3c794a130>
2022-07-13 08:28:20 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fa3c794a190>
2022-07-13 08:28:20 - seed: 0
2022-07-13 08:28:20 - batch_size: 256
2022-07-13 08:28:20 - num_workers: 16
2022-07-13 08:28:20 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-13 08:28:20 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-07-13 08:28:20 - epochs: 200
2022-07-13 08:28:20 - print_interval: 100
2022-07-13 08:28:20 - accumulation_steps: 1
2022-07-13 08:28:20 - sync_bn: False
2022-07-13 08:28:20 - apex: True
2022-07-13 08:28:20 - use_ema_model: False
2022-07-13 08:28:20 - ema_model_decay: 0.9999
2022-07-13 08:28:20 - gpus_type: NVIDIA RTX A5000
2022-07-13 08:28:20 - gpus_num: 2
2022-07-13 08:28:20 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fa3c58bfcf0>
2022-07-13 08:28:20 - --------------------parameters--------------------
2022-07-13 08:28:20 - name: conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-07-13 08:28:20 - name: fc.weight, grad: True
2022-07-13 08:28:20 - name: fc.bias, grad: True
2022-07-13 08:28:20 - --------------------buffers--------------------
2022-07-13 08:28:20 - name: conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-07-13 08:28:20 - -----------no weight decay layers--------------
2022-07-13 08:28:20 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-13 08:28:20 - -------------weight decay layers---------------
2022-07-13 08:28:20 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-13 08:28:20 - epoch 001 lr: 0.100000
2022-07-13 08:29:02 - train: epoch 0001, iter [00100, 05004], lr: 0.020400, loss: 6.9974
2022-07-13 08:29:39 - train: epoch 0001, iter [00200, 05004], lr: 0.020799, loss: 6.9291
2022-07-13 08:30:15 - train: epoch 0001, iter [00300, 05004], lr: 0.021199, loss: 6.9142
2022-07-13 08:30:51 - train: epoch 0001, iter [00400, 05004], lr: 0.021599, loss: 6.8825
2022-07-13 08:31:27 - train: epoch 0001, iter [00500, 05004], lr: 0.021998, loss: 6.8876
2022-07-13 08:32:03 - train: epoch 0001, iter [00600, 05004], lr: 0.022398, loss: 6.7771
2022-07-13 08:32:37 - train: epoch 0001, iter [00700, 05004], lr: 0.022798, loss: 6.7742
2022-07-13 08:33:12 - train: epoch 0001, iter [00800, 05004], lr: 0.023197, loss: 6.7772
2022-07-13 08:33:48 - train: epoch 0001, iter [00900, 05004], lr: 0.023597, loss: 6.7003
2022-07-13 08:34:22 - train: epoch 0001, iter [01000, 05004], lr: 0.023997, loss: 6.6305
2022-07-13 08:34:57 - train: epoch 0001, iter [01100, 05004], lr: 0.024396, loss: 6.6997
2022-07-13 08:35:32 - train: epoch 0001, iter [01200, 05004], lr: 0.024796, loss: 6.5260
2022-07-13 08:36:07 - train: epoch 0001, iter [01300, 05004], lr: 0.025196, loss: 6.6599
2022-07-13 08:36:41 - train: epoch 0001, iter [01400, 05004], lr: 0.025596, loss: 6.4527
2022-07-13 08:37:16 - train: epoch 0001, iter [01500, 05004], lr: 0.025995, loss: 6.3946
2022-07-13 08:37:50 - train: epoch 0001, iter [01600, 05004], lr: 0.026395, loss: 6.5440
2022-07-13 08:38:24 - train: epoch 0001, iter [01700, 05004], lr: 0.026795, loss: 6.3307
2022-07-13 08:38:59 - train: epoch 0001, iter [01800, 05004], lr: 0.027194, loss: 6.3404
2022-07-13 08:39:34 - train: epoch 0001, iter [01900, 05004], lr: 0.027594, loss: 6.2214
2022-07-13 08:40:08 - train: epoch 0001, iter [02000, 05004], lr: 0.027994, loss: 6.0928
2022-07-13 08:40:42 - train: epoch 0001, iter [02100, 05004], lr: 0.028393, loss: 6.1379
2022-07-13 08:41:15 - train: epoch 0001, iter [02200, 05004], lr: 0.028793, loss: 6.1424
2022-07-13 08:41:49 - train: epoch 0001, iter [02300, 05004], lr: 0.029193, loss: 6.0519
2022-07-13 08:42:23 - train: epoch 0001, iter [02400, 05004], lr: 0.029592, loss: 6.0889
2022-07-13 08:42:58 - train: epoch 0001, iter [02500, 05004], lr: 0.029992, loss: 6.0338
2022-07-13 08:43:31 - train: epoch 0001, iter [02600, 05004], lr: 0.030392, loss: 5.9947
2022-07-13 08:44:05 - train: epoch 0001, iter [02700, 05004], lr: 0.030791, loss: 6.1033
2022-07-13 08:44:38 - train: epoch 0001, iter [02800, 05004], lr: 0.031191, loss: 5.9810
2022-07-13 08:45:12 - train: epoch 0001, iter [02900, 05004], lr: 0.031591, loss: 5.9875
2022-07-13 08:45:45 - train: epoch 0001, iter [03000, 05004], lr: 0.031990, loss: 5.9352
2022-07-13 08:46:19 - train: epoch 0001, iter [03100, 05004], lr: 0.032390, loss: 5.9953
2022-07-13 08:46:53 - train: epoch 0001, iter [03200, 05004], lr: 0.032790, loss: 5.9582
2022-07-13 08:47:27 - train: epoch 0001, iter [03300, 05004], lr: 0.033189, loss: 5.9087
2022-07-13 08:47:59 - train: epoch 0001, iter [03400, 05004], lr: 0.033589, loss: 5.8999
2022-07-13 08:48:34 - train: epoch 0001, iter [03500, 05004], lr: 0.033989, loss: 5.7098
2022-07-13 08:49:07 - train: epoch 0001, iter [03600, 05004], lr: 0.034388, loss: 5.9217
2022-07-13 08:49:40 - train: epoch 0001, iter [03700, 05004], lr: 0.034788, loss: 5.8556
2022-07-13 08:50:15 - train: epoch 0001, iter [03800, 05004], lr: 0.035188, loss: 5.6292
2022-07-13 08:50:47 - train: epoch 0001, iter [03900, 05004], lr: 0.035588, loss: 5.8338
2022-07-13 08:51:21 - train: epoch 0001, iter [04000, 05004], lr: 0.035987, loss: 5.6678
2022-07-13 08:51:55 - train: epoch 0001, iter [04100, 05004], lr: 0.036387, loss: 5.7552
2022-07-13 08:52:29 - train: epoch 0001, iter [04200, 05004], lr: 0.036787, loss: 5.6103
2022-07-13 08:53:01 - train: epoch 0001, iter [04300, 05004], lr: 0.037186, loss: 5.6961
2022-07-13 08:53:35 - train: epoch 0001, iter [04400, 05004], lr: 0.037586, loss: 5.4945
2022-07-13 08:54:10 - train: epoch 0001, iter [04500, 05004], lr: 0.037986, loss: 5.7179
2022-07-13 08:54:43 - train: epoch 0001, iter [04600, 05004], lr: 0.038385, loss: 5.6879
2022-07-13 08:55:17 - train: epoch 0001, iter [04700, 05004], lr: 0.038785, loss: 5.5916
2022-07-13 08:55:51 - train: epoch 0001, iter [04800, 05004], lr: 0.039185, loss: 5.7535
2022-07-13 08:56:24 - train: epoch 0001, iter [04900, 05004], lr: 0.039584, loss: 5.4613
2022-07-13 08:56:57 - train: epoch 0001, iter [05000, 05004], lr: 0.039984, loss: 5.5488
2022-07-13 08:56:58 - train: epoch 001, train_loss: 6.1611
2022-07-13 08:58:14 - eval: epoch: 001, acc1: 8.264%, acc5: 22.534%, test_loss: 5.1970, per_image_load_time: 2.342ms, per_image_inference_time: 0.470ms
2022-07-13 08:58:14 - until epoch: 001, best_acc1: 8.264%
2022-07-13 08:58:14 - epoch 002 lr: 0.040004
2022-07-13 08:58:55 - train: epoch 0002, iter [00100, 05004], lr: 0.040400, loss: 5.4508
2022-07-13 08:59:29 - train: epoch 0002, iter [00200, 05004], lr: 0.040799, loss: 5.2311
2022-07-13 09:00:03 - train: epoch 0002, iter [00300, 05004], lr: 0.041199, loss: 5.5199
2022-07-13 09:00:38 - train: epoch 0002, iter [00400, 05004], lr: 0.041599, loss: 5.3695
2022-07-13 09:01:12 - train: epoch 0002, iter [00500, 05004], lr: 0.041998, loss: 5.2565
2022-07-13 09:01:46 - train: epoch 0002, iter [00600, 05004], lr: 0.042398, loss: 5.2960
2022-07-13 09:02:20 - train: epoch 0002, iter [00700, 05004], lr: 0.042798, loss: 5.3372
2022-07-13 09:02:54 - train: epoch 0002, iter [00800, 05004], lr: 0.043197, loss: 5.1115
2022-07-13 09:03:29 - train: epoch 0002, iter [00900, 05004], lr: 0.043597, loss: 5.0508
2022-07-13 09:04:03 - train: epoch 0002, iter [01000, 05004], lr: 0.043997, loss: 5.4486
2022-07-13 09:04:36 - train: epoch 0002, iter [01100, 05004], lr: 0.044396, loss: 5.2534
2022-07-13 09:05:10 - train: epoch 0002, iter [01200, 05004], lr: 0.044796, loss: 5.4122
2022-07-13 09:05:45 - train: epoch 0002, iter [01300, 05004], lr: 0.045196, loss: 5.0366
2022-07-13 09:06:20 - train: epoch 0002, iter [01400, 05004], lr: 0.045596, loss: 5.2626
2022-07-13 09:06:54 - train: epoch 0002, iter [01500, 05004], lr: 0.045995, loss: 5.2150
2022-07-13 09:07:29 - train: epoch 0002, iter [01600, 05004], lr: 0.046395, loss: 5.1335
2022-07-13 09:08:02 - train: epoch 0002, iter [01700, 05004], lr: 0.046795, loss: 5.0396
2022-07-13 09:08:36 - train: epoch 0002, iter [01800, 05004], lr: 0.047194, loss: 5.1988
2022-07-13 09:09:10 - train: epoch 0002, iter [01900, 05004], lr: 0.047594, loss: 5.1798
2022-07-13 09:09:44 - train: epoch 0002, iter [02000, 05004], lr: 0.047994, loss: 4.8979
2022-07-13 09:10:18 - train: epoch 0002, iter [02100, 05004], lr: 0.048393, loss: 4.9915
2022-07-13 09:10:53 - train: epoch 0002, iter [02200, 05004], lr: 0.048793, loss: 4.8397
2022-07-13 09:11:27 - train: epoch 0002, iter [02300, 05004], lr: 0.049193, loss: 4.9385
2022-07-13 09:12:01 - train: epoch 0002, iter [02400, 05004], lr: 0.049592, loss: 4.9176
2022-07-13 09:12:35 - train: epoch 0002, iter [02500, 05004], lr: 0.049992, loss: 5.0308
2022-07-13 09:13:09 - train: epoch 0002, iter [02600, 05004], lr: 0.050392, loss: 5.0408
2022-07-13 09:13:43 - train: epoch 0002, iter [02700, 05004], lr: 0.050791, loss: 4.9861
2022-07-13 09:14:17 - train: epoch 0002, iter [02800, 05004], lr: 0.051191, loss: 4.8507
2022-07-13 09:14:52 - train: epoch 0002, iter [02900, 05004], lr: 0.051591, loss: 4.8078
2022-07-13 09:15:26 - train: epoch 0002, iter [03000, 05004], lr: 0.051990, loss: 4.8988
2022-07-13 09:16:01 - train: epoch 0002, iter [03100, 05004], lr: 0.052390, loss: 4.7466
2022-07-13 09:16:35 - train: epoch 0002, iter [03200, 05004], lr: 0.052790, loss: 4.8840
2022-07-13 09:17:09 - train: epoch 0002, iter [03300, 05004], lr: 0.053189, loss: 4.7214
2022-07-13 09:17:43 - train: epoch 0002, iter [03400, 05004], lr: 0.053589, loss: 4.9148
2022-07-13 09:18:17 - train: epoch 0002, iter [03500, 05004], lr: 0.053989, loss: 4.8034
2022-07-13 09:18:52 - train: epoch 0002, iter [03600, 05004], lr: 0.054388, loss: 4.7662
2022-07-13 09:19:26 - train: epoch 0002, iter [03700, 05004], lr: 0.054788, loss: 4.8279
2022-07-13 09:20:00 - train: epoch 0002, iter [03800, 05004], lr: 0.055188, loss: 4.5513
2022-07-13 09:20:35 - train: epoch 0002, iter [03900, 05004], lr: 0.055588, loss: 4.6561
2022-07-13 09:21:11 - train: epoch 0002, iter [04000, 05004], lr: 0.055987, loss: 4.6063
2022-07-13 09:21:45 - train: epoch 0002, iter [04100, 05004], lr: 0.056387, loss: 4.7698
2022-07-13 09:22:20 - train: epoch 0002, iter [04200, 05004], lr: 0.056787, loss: 4.5928
2022-07-13 09:22:54 - train: epoch 0002, iter [04300, 05004], lr: 0.057186, loss: 4.6905
2022-07-13 09:23:29 - train: epoch 0002, iter [04400, 05004], lr: 0.057586, loss: 4.4452
2022-07-13 09:24:03 - train: epoch 0002, iter [04500, 05004], lr: 0.057986, loss: 4.5198
2022-07-13 09:24:38 - train: epoch 0002, iter [04600, 05004], lr: 0.058385, loss: 4.5430
2022-07-13 09:25:12 - train: epoch 0002, iter [04700, 05004], lr: 0.058785, loss: 4.4374
2022-07-13 09:25:47 - train: epoch 0002, iter [04800, 05004], lr: 0.059185, loss: 4.5558
2022-07-13 09:26:21 - train: epoch 0002, iter [04900, 05004], lr: 0.059584, loss: 4.4914
2022-07-13 09:26:54 - train: epoch 0002, iter [05000, 05004], lr: 0.059984, loss: 4.6446
2022-07-13 09:26:55 - train: epoch 002, train_loss: 4.9793
2022-07-13 09:28:09 - eval: epoch: 002, acc1: 19.206%, acc5: 41.746%, test_loss: 4.0532, per_image_load_time: 2.211ms, per_image_inference_time: 0.476ms
2022-07-13 09:28:10 - until epoch: 002, best_acc1: 19.206%
2022-07-13 09:28:10 - epoch 003 lr: 0.060004
2022-07-13 09:28:50 - train: epoch 0003, iter [00100, 05004], lr: 0.060400, loss: 4.6370
2022-07-13 09:29:24 - train: epoch 0003, iter [00200, 05004], lr: 0.060799, loss: 4.4961
2022-07-13 09:29:58 - train: epoch 0003, iter [00300, 05004], lr: 0.061199, loss: 4.7083
2022-07-13 09:30:32 - train: epoch 0003, iter [00400, 05004], lr: 0.061599, loss: 4.4483
2022-07-13 09:31:06 - train: epoch 0003, iter [00500, 05004], lr: 0.061998, loss: 4.4627
2022-07-13 09:31:41 - train: epoch 0003, iter [00600, 05004], lr: 0.062398, loss: 4.3297
2022-07-13 09:32:15 - train: epoch 0003, iter [00700, 05004], lr: 0.062798, loss: 4.5604
2022-07-13 09:32:50 - train: epoch 0003, iter [00800, 05004], lr: 0.063197, loss: 4.5733
2022-07-13 09:33:24 - train: epoch 0003, iter [00900, 05004], lr: 0.063597, loss: 4.3800
2022-07-13 09:33:59 - train: epoch 0003, iter [01000, 05004], lr: 0.063997, loss: 4.3676
2022-07-13 09:34:33 - train: epoch 0003, iter [01100, 05004], lr: 0.064396, loss: 4.5227
2022-07-13 09:35:08 - train: epoch 0003, iter [01200, 05004], lr: 0.064796, loss: 4.1181
2022-07-13 09:35:42 - train: epoch 0003, iter [01300, 05004], lr: 0.065196, loss: 4.2557
2022-07-13 09:36:17 - train: epoch 0003, iter [01400, 05004], lr: 0.065596, loss: 4.2856
2022-07-13 09:36:52 - train: epoch 0003, iter [01500, 05004], lr: 0.065995, loss: 4.3644
2022-07-13 09:37:27 - train: epoch 0003, iter [01600, 05004], lr: 0.066395, loss: 4.3127
2022-07-13 09:38:02 - train: epoch 0003, iter [01700, 05004], lr: 0.066795, loss: 4.4066
2022-07-13 09:38:36 - train: epoch 0003, iter [01800, 05004], lr: 0.067194, loss: 4.2127
2022-07-13 09:39:11 - train: epoch 0003, iter [01900, 05004], lr: 0.067594, loss: 4.4788
2022-07-13 09:39:45 - train: epoch 0003, iter [02000, 05004], lr: 0.067994, loss: 4.4437
2022-07-13 09:40:21 - train: epoch 0003, iter [02100, 05004], lr: 0.068393, loss: 4.5094
2022-07-13 09:40:56 - train: epoch 0003, iter [02200, 05004], lr: 0.068793, loss: 4.7385
2022-07-13 09:41:30 - train: epoch 0003, iter [02300, 05004], lr: 0.069193, loss: 4.3195
2022-07-13 09:42:05 - train: epoch 0003, iter [02400, 05004], lr: 0.069592, loss: 4.0341
2022-07-13 09:42:40 - train: epoch 0003, iter [02500, 05004], lr: 0.069992, loss: 4.4100
2022-07-13 09:43:15 - train: epoch 0003, iter [02600, 05004], lr: 0.070392, loss: 4.3329
2022-07-13 09:43:50 - train: epoch 0003, iter [02700, 05004], lr: 0.070791, loss: 4.5362
2022-07-13 09:44:25 - train: epoch 0003, iter [02800, 05004], lr: 0.071191, loss: 4.2807
2022-07-13 09:45:00 - train: epoch 0003, iter [02900, 05004], lr: 0.071591, loss: 4.1433
2022-07-13 09:45:35 - train: epoch 0003, iter [03000, 05004], lr: 0.071990, loss: 4.3424
2022-07-13 09:46:09 - train: epoch 0003, iter [03100, 05004], lr: 0.072390, loss: 4.3877
2022-07-13 09:46:45 - train: epoch 0003, iter [03200, 05004], lr: 0.072790, loss: 4.1949
2022-07-13 09:47:19 - train: epoch 0003, iter [03300, 05004], lr: 0.073189, loss: 4.2273
2022-07-13 09:47:54 - train: epoch 0003, iter [03400, 05004], lr: 0.073589, loss: 4.2888
2022-07-13 09:48:28 - train: epoch 0003, iter [03500, 05004], lr: 0.073989, loss: 3.9825
2022-07-13 09:49:02 - train: epoch 0003, iter [03600, 05004], lr: 0.074388, loss: 4.1568
2022-07-13 09:49:37 - train: epoch 0003, iter [03700, 05004], lr: 0.074788, loss: 4.0937
2022-07-13 09:50:11 - train: epoch 0003, iter [03800, 05004], lr: 0.075188, loss: 4.3744
2022-07-13 09:50:46 - train: epoch 0003, iter [03900, 05004], lr: 0.075588, loss: 4.3703
2022-07-13 09:51:20 - train: epoch 0003, iter [04000, 05004], lr: 0.075987, loss: 4.1317
2022-07-13 09:51:54 - train: epoch 0003, iter [04100, 05004], lr: 0.076387, loss: 4.0682
2022-07-13 09:52:28 - train: epoch 0003, iter [04200, 05004], lr: 0.076787, loss: 4.1289
2022-07-13 09:53:03 - train: epoch 0003, iter [04300, 05004], lr: 0.077186, loss: 3.8601
2022-07-13 09:53:38 - train: epoch 0003, iter [04400, 05004], lr: 0.077586, loss: 3.9934
2022-07-13 09:54:12 - train: epoch 0003, iter [04500, 05004], lr: 0.077986, loss: 4.0998
2022-07-13 09:54:47 - train: epoch 0003, iter [04600, 05004], lr: 0.078385, loss: 4.2705
2022-07-13 09:55:20 - train: epoch 0003, iter [04700, 05004], lr: 0.078785, loss: 4.0329
2022-07-13 09:55:55 - train: epoch 0003, iter [04800, 05004], lr: 0.079185, loss: 4.2658
2022-07-13 09:56:29 - train: epoch 0003, iter [04900, 05004], lr: 0.079584, loss: 4.0623
2022-07-13 09:57:02 - train: epoch 0003, iter [05000, 05004], lr: 0.079984, loss: 4.0529
2022-07-13 09:57:03 - train: epoch 003, train_loss: 4.2928
2022-07-13 09:58:18 - eval: epoch: 003, acc1: 30.694%, acc5: 56.934%, test_loss: 3.2467, per_image_load_time: 2.386ms, per_image_inference_time: 0.479ms
2022-07-13 09:58:18 - until epoch: 003, best_acc1: 30.694%
2022-07-13 09:58:18 - epoch 004 lr: 0.080004
2022-07-13 09:58:58 - train: epoch 0004, iter [00100, 05004], lr: 0.080400, loss: 4.0495
2022-07-13 09:59:31 - train: epoch 0004, iter [00200, 05004], lr: 0.080799, loss: 4.1813
2022-07-13 10:00:06 - train: epoch 0004, iter [00300, 05004], lr: 0.081199, loss: 4.0127
2022-07-13 10:00:40 - train: epoch 0004, iter [00400, 05004], lr: 0.081599, loss: 3.9528
2022-07-13 10:01:15 - train: epoch 0004, iter [00500, 05004], lr: 0.081998, loss: 3.7641
2022-07-13 10:01:50 - train: epoch 0004, iter [00600, 05004], lr: 0.082398, loss: 4.3271
2022-07-13 10:02:25 - train: epoch 0004, iter [00700, 05004], lr: 0.082798, loss: 4.3679
2022-07-13 10:02:58 - train: epoch 0004, iter [00800, 05004], lr: 0.083197, loss: 3.7864
2022-07-13 10:03:33 - train: epoch 0004, iter [00900, 05004], lr: 0.083597, loss: 3.8360
2022-07-13 10:04:07 - train: epoch 0004, iter [01000, 05004], lr: 0.083997, loss: 4.1392
2022-07-13 10:04:42 - train: epoch 0004, iter [01100, 05004], lr: 0.084396, loss: 4.0117
2022-07-13 10:05:17 - train: epoch 0004, iter [01200, 05004], lr: 0.084796, loss: 3.7419
2022-07-13 10:05:52 - train: epoch 0004, iter [01300, 05004], lr: 0.085196, loss: 3.8771
2022-07-13 10:06:26 - train: epoch 0004, iter [01400, 05004], lr: 0.085596, loss: 4.2116
2022-07-13 10:07:01 - train: epoch 0004, iter [01500, 05004], lr: 0.085995, loss: 3.9917
2022-07-13 10:07:36 - train: epoch 0004, iter [01600, 05004], lr: 0.086395, loss: 4.0863
2022-07-13 10:08:11 - train: epoch 0004, iter [01700, 05004], lr: 0.086795, loss: 3.9702
2022-07-13 10:08:45 - train: epoch 0004, iter [01800, 05004], lr: 0.087194, loss: 4.1726
2022-07-13 10:09:20 - train: epoch 0004, iter [01900, 05004], lr: 0.087594, loss: 3.9427
2022-07-13 10:09:55 - train: epoch 0004, iter [02000, 05004], lr: 0.087994, loss: 3.8504
2022-07-13 10:10:29 - train: epoch 0004, iter [02100, 05004], lr: 0.088393, loss: 3.9440
2022-07-13 10:11:03 - train: epoch 0004, iter [02200, 05004], lr: 0.088793, loss: 3.8064
2022-07-13 10:11:37 - train: epoch 0004, iter [02300, 05004], lr: 0.089193, loss: 3.6941
2022-07-13 10:12:11 - train: epoch 0004, iter [02400, 05004], lr: 0.089592, loss: 3.5875
2022-07-13 10:12:46 - train: epoch 0004, iter [02500, 05004], lr: 0.089992, loss: 3.7779
2022-07-13 10:13:21 - train: epoch 0004, iter [02600, 05004], lr: 0.090392, loss: 4.0063
2022-07-13 10:13:55 - train: epoch 0004, iter [02700, 05004], lr: 0.090791, loss: 3.8194
2022-07-13 10:14:29 - train: epoch 0004, iter [02800, 05004], lr: 0.091191, loss: 3.7319
2022-07-13 10:15:03 - train: epoch 0004, iter [02900, 05004], lr: 0.091591, loss: 3.9705
2022-07-13 10:15:38 - train: epoch 0004, iter [03000, 05004], lr: 0.091990, loss: 3.8565
2022-07-13 10:16:12 - train: epoch 0004, iter [03100, 05004], lr: 0.092390, loss: 3.9451
2022-07-13 10:16:46 - train: epoch 0004, iter [03200, 05004], lr: 0.092790, loss: 3.9177
2022-07-13 10:17:21 - train: epoch 0004, iter [03300, 05004], lr: 0.093189, loss: 3.8384
2022-07-13 10:17:55 - train: epoch 0004, iter [03400, 05004], lr: 0.093589, loss: 4.0791
2022-07-13 10:18:29 - train: epoch 0004, iter [03500, 05004], lr: 0.093989, loss: 3.9912
2022-07-13 10:19:04 - train: epoch 0004, iter [03600, 05004], lr: 0.094388, loss: 3.8984
2022-07-13 10:19:38 - train: epoch 0004, iter [03700, 05004], lr: 0.094788, loss: 3.7969
2022-07-13 10:20:14 - train: epoch 0004, iter [03800, 05004], lr: 0.095188, loss: 3.6124
2022-07-13 10:20:48 - train: epoch 0004, iter [03900, 05004], lr: 0.095588, loss: 3.6875
2022-07-13 10:21:22 - train: epoch 0004, iter [04000, 05004], lr: 0.095987, loss: 3.7050
2022-07-13 10:21:56 - train: epoch 0004, iter [04100, 05004], lr: 0.096387, loss: 3.8045
2022-07-13 10:22:31 - train: epoch 0004, iter [04200, 05004], lr: 0.096787, loss: 3.9196
2022-07-13 10:23:05 - train: epoch 0004, iter [04300, 05004], lr: 0.097186, loss: 3.7962
2022-07-13 10:23:39 - train: epoch 0004, iter [04400, 05004], lr: 0.097586, loss: 3.6846
2022-07-13 10:24:14 - train: epoch 0004, iter [04500, 05004], lr: 0.097986, loss: 3.6512
2022-07-13 10:24:48 - train: epoch 0004, iter [04600, 05004], lr: 0.098385, loss: 3.7916
2022-07-13 10:25:22 - train: epoch 0004, iter [04700, 05004], lr: 0.098785, loss: 3.7724
2022-07-13 10:25:56 - train: epoch 0004, iter [04800, 05004], lr: 0.099185, loss: 3.8482
2022-07-13 10:26:30 - train: epoch 0004, iter [04900, 05004], lr: 0.099584, loss: 3.7752
2022-07-13 10:27:04 - train: epoch 0004, iter [05000, 05004], lr: 0.099984, loss: 3.8496
2022-07-13 10:27:05 - train: epoch 004, train_loss: 3.9359
2022-07-13 10:28:20 - eval: epoch: 004, acc1: 33.654%, acc5: 60.594%, test_loss: 3.0555, per_image_load_time: 2.354ms, per_image_inference_time: 0.464ms
2022-07-13 10:28:20 - until epoch: 004, best_acc1: 33.654%
2022-07-13 10:28:20 - epoch 005 lr: 0.100004
2022-07-13 10:28:59 - train: epoch 0005, iter [00100, 05004], lr: 0.100400, loss: 3.7447
2022-07-13 10:29:34 - train: epoch 0005, iter [00200, 05004], lr: 0.100799, loss: 3.6874
2022-07-13 10:30:09 - train: epoch 0005, iter [00300, 05004], lr: 0.101199, loss: 3.9893
2022-07-13 10:30:43 - train: epoch 0005, iter [00400, 05004], lr: 0.101599, loss: 3.7368
2022-07-13 10:31:16 - train: epoch 0005, iter [00500, 05004], lr: 0.101998, loss: 3.8370
2022-07-13 10:31:50 - train: epoch 0005, iter [00600, 05004], lr: 0.102398, loss: 3.7920
2022-07-13 10:32:24 - train: epoch 0005, iter [00700, 05004], lr: 0.102798, loss: 3.8454
2022-07-13 10:32:59 - train: epoch 0005, iter [00800, 05004], lr: 0.103197, loss: 3.8747
2022-07-13 10:33:33 - train: epoch 0005, iter [00900, 05004], lr: 0.103597, loss: 3.5352
2022-07-13 10:34:06 - train: epoch 0005, iter [01000, 05004], lr: 0.103997, loss: 3.9130
2022-07-13 10:34:41 - train: epoch 0005, iter [01100, 05004], lr: 0.104396, loss: 3.8647
2022-07-13 10:35:15 - train: epoch 0005, iter [01200, 05004], lr: 0.104796, loss: 3.9277
2022-07-13 10:35:49 - train: epoch 0005, iter [01300, 05004], lr: 0.105196, loss: 3.4874
2022-07-13 10:36:23 - train: epoch 0005, iter [01400, 05004], lr: 0.105596, loss: 3.8370
2022-07-13 10:36:57 - train: epoch 0005, iter [01500, 05004], lr: 0.105995, loss: 3.8186
2022-07-13 10:37:32 - train: epoch 0005, iter [01600, 05004], lr: 0.106395, loss: 3.4449
2022-07-13 10:38:06 - train: epoch 0005, iter [01700, 05004], lr: 0.106795, loss: 3.5943
2022-07-13 10:38:40 - train: epoch 0005, iter [01800, 05004], lr: 0.107194, loss: 4.0201
2022-07-13 10:39:14 - train: epoch 0005, iter [01900, 05004], lr: 0.107594, loss: 3.5814
2022-07-13 10:39:49 - train: epoch 0005, iter [02000, 05004], lr: 0.107994, loss: 3.8130
2022-07-13 10:40:24 - train: epoch 0005, iter [02100, 05004], lr: 0.108393, loss: 3.5344
2022-07-13 10:40:58 - train: epoch 0005, iter [02200, 05004], lr: 0.108793, loss: 3.6129
2022-07-13 10:41:33 - train: epoch 0005, iter [02300, 05004], lr: 0.109193, loss: 3.5065
2022-07-13 10:42:07 - train: epoch 0005, iter [02400, 05004], lr: 0.109592, loss: 3.5929
2022-07-13 10:42:42 - train: epoch 0005, iter [02500, 05004], lr: 0.109992, loss: 4.1246
2022-07-13 10:43:15 - train: epoch 0005, iter [02600, 05004], lr: 0.110392, loss: 3.9946
2022-07-13 10:43:50 - train: epoch 0005, iter [02700, 05004], lr: 0.110791, loss: 3.9724
2022-07-13 10:44:25 - train: epoch 0005, iter [02800, 05004], lr: 0.111191, loss: 3.7687
2022-07-13 10:44:59 - train: epoch 0005, iter [02900, 05004], lr: 0.111591, loss: 3.5392
2022-07-13 10:45:33 - train: epoch 0005, iter [03000, 05004], lr: 0.111990, loss: 4.0394
2022-07-13 10:46:08 - train: epoch 0005, iter [03100, 05004], lr: 0.112390, loss: 3.7940
2022-07-13 10:46:43 - train: epoch 0005, iter [03200, 05004], lr: 0.112790, loss: 3.7706
2022-07-13 10:47:17 - train: epoch 0005, iter [03300, 05004], lr: 0.113189, loss: 3.6990
2022-07-13 10:47:51 - train: epoch 0005, iter [03400, 05004], lr: 0.113589, loss: 3.7341
2022-07-13 10:48:25 - train: epoch 0005, iter [03500, 05004], lr: 0.113989, loss: 3.6654
2022-07-13 10:48:59 - train: epoch 0005, iter [03600, 05004], lr: 0.114388, loss: 3.6051
2022-07-13 10:49:34 - train: epoch 0005, iter [03700, 05004], lr: 0.114788, loss: 3.6577
2022-07-13 10:50:08 - train: epoch 0005, iter [03800, 05004], lr: 0.115188, loss: 3.3701
2022-07-13 10:50:42 - train: epoch 0005, iter [03900, 05004], lr: 0.115588, loss: 3.8756
2022-07-13 10:51:17 - train: epoch 0005, iter [04000, 05004], lr: 0.115987, loss: 3.5099
2022-07-13 10:51:52 - train: epoch 0005, iter [04100, 05004], lr: 0.116387, loss: 3.8394
2022-07-13 10:52:26 - train: epoch 0005, iter [04200, 05004], lr: 0.116787, loss: 3.6618
2022-07-13 10:53:01 - train: epoch 0005, iter [04300, 05004], lr: 0.117186, loss: 3.4061
2022-07-13 10:53:35 - train: epoch 0005, iter [04400, 05004], lr: 0.117586, loss: 3.7803
2022-07-13 10:54:09 - train: epoch 0005, iter [04500, 05004], lr: 0.117986, loss: 3.6685
2022-07-13 10:54:44 - train: epoch 0005, iter [04600, 05004], lr: 0.118385, loss: 3.5098
2022-07-13 10:55:18 - train: epoch 0005, iter [04700, 05004], lr: 0.118785, loss: 3.4915
2022-07-13 10:55:53 - train: epoch 0005, iter [04800, 05004], lr: 0.119185, loss: 3.7081
2022-07-13 10:56:27 - train: epoch 0005, iter [04900, 05004], lr: 0.119584, loss: 3.9746
2022-07-13 10:57:00 - train: epoch 0005, iter [05000, 05004], lr: 0.119984, loss: 3.4464
2022-07-13 10:57:01 - train: epoch 005, train_loss: 3.7419
2022-07-13 10:58:15 - eval: epoch: 005, acc1: 35.738%, acc5: 63.002%, test_loss: 2.9191, per_image_load_time: 1.171ms, per_image_inference_time: 0.470ms
2022-07-13 10:58:15 - until epoch: 005, best_acc1: 35.738%
2022-07-13 10:58:15 - epoch 006 lr: 0.100000
2022-07-13 10:58:55 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 3.6630
2022-07-13 10:59:29 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 3.6692
2022-07-13 11:00:02 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 3.2414
2022-07-13 11:00:37 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 3.6980
2022-07-13 11:01:11 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 3.3533
2022-07-13 11:01:45 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 3.5779
2022-07-13 11:02:19 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 3.5840
2022-07-13 11:02:54 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 3.6136
2022-07-13 11:03:27 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 3.3849
2022-07-13 11:04:02 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 3.5843
2022-07-13 11:04:35 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 3.5753
2022-07-13 11:05:09 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 3.5594
2022-07-13 11:05:43 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 3.8364
2022-07-13 11:06:17 - train: epoch 0006, iter [01400, 05004], lr: 0.099999, loss: 3.7128
2022-07-13 11:06:51 - train: epoch 0006, iter [01500, 05004], lr: 0.099999, loss: 3.6702
2022-07-13 11:07:24 - train: epoch 0006, iter [01600, 05004], lr: 0.099999, loss: 3.3844
2022-07-13 11:07:59 - train: epoch 0006, iter [01700, 05004], lr: 0.099999, loss: 3.6450
2022-07-13 11:08:33 - train: epoch 0006, iter [01800, 05004], lr: 0.099999, loss: 3.7158
2022-07-13 11:09:07 - train: epoch 0006, iter [01900, 05004], lr: 0.099999, loss: 3.5038
2022-07-13 11:09:41 - train: epoch 0006, iter [02000, 05004], lr: 0.099999, loss: 3.6196
2022-07-13 11:10:15 - train: epoch 0006, iter [02100, 05004], lr: 0.099999, loss: 3.6448
2022-07-13 11:10:49 - train: epoch 0006, iter [02200, 05004], lr: 0.099999, loss: 3.5389
2022-07-13 11:11:22 - train: epoch 0006, iter [02300, 05004], lr: 0.099999, loss: 3.2977
2022-07-13 11:11:57 - train: epoch 0006, iter [02400, 05004], lr: 0.099999, loss: 3.3541
2022-07-13 11:12:31 - train: epoch 0006, iter [02500, 05004], lr: 0.099998, loss: 3.7398
2022-07-13 11:13:05 - train: epoch 0006, iter [02600, 05004], lr: 0.099998, loss: 3.2269
2022-07-13 11:13:39 - train: epoch 0006, iter [02700, 05004], lr: 0.099998, loss: 3.6839
2022-07-13 11:14:13 - train: epoch 0006, iter [02800, 05004], lr: 0.099998, loss: 3.5406
2022-07-13 11:14:47 - train: epoch 0006, iter [02900, 05004], lr: 0.099998, loss: 3.4768
2022-07-13 11:15:21 - train: epoch 0006, iter [03000, 05004], lr: 0.099998, loss: 3.6071
2022-07-13 11:15:55 - train: epoch 0006, iter [03100, 05004], lr: 0.099998, loss: 3.2967
2022-07-13 11:16:29 - train: epoch 0006, iter [03200, 05004], lr: 0.099997, loss: 3.6452
2022-07-13 11:17:03 - train: epoch 0006, iter [03300, 05004], lr: 0.099997, loss: 3.3339
2022-07-13 11:17:38 - train: epoch 0006, iter [03400, 05004], lr: 0.099997, loss: 3.5698
2022-07-13 11:18:12 - train: epoch 0006, iter [03500, 05004], lr: 0.099997, loss: 3.5092
2022-07-13 11:18:46 - train: epoch 0006, iter [03600, 05004], lr: 0.099997, loss: 3.3882
2022-07-13 11:19:20 - train: epoch 0006, iter [03700, 05004], lr: 0.099996, loss: 3.2680
2022-07-13 11:19:54 - train: epoch 0006, iter [03800, 05004], lr: 0.099996, loss: 3.3085
2022-07-13 11:20:26 - train: epoch 0006, iter [03900, 05004], lr: 0.099996, loss: 3.3135
2022-07-13 11:21:01 - train: epoch 0006, iter [04000, 05004], lr: 0.099996, loss: 3.7635
2022-07-13 11:21:34 - train: epoch 0006, iter [04100, 05004], lr: 0.099996, loss: 3.5210
2022-07-13 11:22:08 - train: epoch 0006, iter [04200, 05004], lr: 0.099995, loss: 3.3813
2022-07-13 11:22:42 - train: epoch 0006, iter [04300, 05004], lr: 0.099995, loss: 3.3630
2022-07-13 11:23:15 - train: epoch 0006, iter [04400, 05004], lr: 0.099995, loss: 3.3265
2022-07-13 11:23:49 - train: epoch 0006, iter [04500, 05004], lr: 0.099995, loss: 3.4396
2022-07-13 11:24:22 - train: epoch 0006, iter [04600, 05004], lr: 0.099995, loss: 3.5125
2022-07-13 11:24:56 - train: epoch 0006, iter [04700, 05004], lr: 0.099994, loss: 3.3382
2022-07-13 11:25:30 - train: epoch 0006, iter [04800, 05004], lr: 0.099994, loss: 3.6172
2022-07-13 11:26:04 - train: epoch 0006, iter [04900, 05004], lr: 0.099994, loss: 3.5689
2022-07-13 11:26:36 - train: epoch 0006, iter [05000, 05004], lr: 0.099994, loss: 3.4298
2022-07-13 11:26:37 - train: epoch 006, train_loss: 3.5146
2022-07-13 11:27:51 - eval: epoch: 006, acc1: 43.454%, acc5: 69.900%, test_loss: 2.5296, per_image_load_time: 1.632ms, per_image_inference_time: 0.491ms
2022-07-13 11:27:51 - until epoch: 006, best_acc1: 43.454%
2022-07-13 11:27:51 - epoch 007 lr: 0.099994
2022-07-13 11:28:30 - train: epoch 0007, iter [00100, 05004], lr: 0.099993, loss: 3.1284
2022-07-13 11:29:05 - train: epoch 0007, iter [00200, 05004], lr: 0.099993, loss: 3.6238
2022-07-13 11:29:39 - train: epoch 0007, iter [00300, 05004], lr: 0.099993, loss: 3.7162
2022-07-13 11:30:14 - train: epoch 0007, iter [00400, 05004], lr: 0.099992, loss: 3.5338
2022-07-13 11:30:47 - train: epoch 0007, iter [00500, 05004], lr: 0.099992, loss: 3.3740
2022-07-13 11:31:22 - train: epoch 0007, iter [00600, 05004], lr: 0.099992, loss: 3.6739
2022-07-13 11:31:57 - train: epoch 0007, iter [00700, 05004], lr: 0.099992, loss: 3.4613
2022-07-13 11:32:30 - train: epoch 0007, iter [00800, 05004], lr: 0.099991, loss: 3.4145
2022-07-13 11:33:04 - train: epoch 0007, iter [00900, 05004], lr: 0.099991, loss: 3.3530
2022-07-13 11:33:38 - train: epoch 0007, iter [01000, 05004], lr: 0.099991, loss: 3.5758
2022-07-13 11:34:12 - train: epoch 0007, iter [01100, 05004], lr: 0.099990, loss: 3.3906
2022-07-13 11:34:46 - train: epoch 0007, iter [01200, 05004], lr: 0.099990, loss: 3.5399
2022-07-13 11:35:20 - train: epoch 0007, iter [01300, 05004], lr: 0.099990, loss: 3.3978
2022-07-13 11:35:56 - train: epoch 0007, iter [01400, 05004], lr: 0.099989, loss: 3.2404
2022-07-13 11:36:30 - train: epoch 0007, iter [01500, 05004], lr: 0.099989, loss: 3.4871
2022-07-13 11:37:04 - train: epoch 0007, iter [01600, 05004], lr: 0.099989, loss: 3.2845
2022-07-13 11:37:38 - train: epoch 0007, iter [01700, 05004], lr: 0.099988, loss: 3.4495
2022-07-13 11:38:13 - train: epoch 0007, iter [01800, 05004], lr: 0.099988, loss: 3.3636
2022-07-13 11:38:47 - train: epoch 0007, iter [01900, 05004], lr: 0.099988, loss: 3.5676
2022-07-13 11:39:22 - train: epoch 0007, iter [02000, 05004], lr: 0.099987, loss: 3.0286
2022-07-13 11:39:57 - train: epoch 0007, iter [02100, 05004], lr: 0.099987, loss: 3.4750
2022-07-13 11:40:30 - train: epoch 0007, iter [02200, 05004], lr: 0.099987, loss: 3.4954
2022-07-13 11:41:06 - train: epoch 0007, iter [02300, 05004], lr: 0.099986, loss: 3.4610
2022-07-13 11:41:40 - train: epoch 0007, iter [02400, 05004], lr: 0.099986, loss: 3.4221
2022-07-13 11:42:14 - train: epoch 0007, iter [02500, 05004], lr: 0.099985, loss: 3.4352
2022-07-13 11:42:48 - train: epoch 0007, iter [02600, 05004], lr: 0.099985, loss: 3.3375
2022-07-13 11:43:23 - train: epoch 0007, iter [02700, 05004], lr: 0.099985, loss: 3.3275
2022-07-13 11:43:58 - train: epoch 0007, iter [02800, 05004], lr: 0.099984, loss: 3.4638
2022-07-13 11:44:33 - train: epoch 0007, iter [02900, 05004], lr: 0.099984, loss: 3.4364
2022-07-13 11:45:07 - train: epoch 0007, iter [03000, 05004], lr: 0.099983, loss: 3.5786
2022-07-13 11:45:42 - train: epoch 0007, iter [03100, 05004], lr: 0.099983, loss: 3.1446
2022-07-13 11:46:17 - train: epoch 0007, iter [03200, 05004], lr: 0.099983, loss: 3.3094
2022-07-13 11:46:52 - train: epoch 0007, iter [03300, 05004], lr: 0.099982, loss: 3.6791
2022-07-13 11:47:27 - train: epoch 0007, iter [03400, 05004], lr: 0.099982, loss: 3.2954
2022-07-13 11:48:02 - train: epoch 0007, iter [03500, 05004], lr: 0.099981, loss: 3.4978
2022-07-13 11:48:37 - train: epoch 0007, iter [03600, 05004], lr: 0.099981, loss: 3.2745
2022-07-13 11:49:11 - train: epoch 0007, iter [03700, 05004], lr: 0.099980, loss: 3.2705
2022-07-13 11:49:46 - train: epoch 0007, iter [03800, 05004], lr: 0.099980, loss: 3.4999
2022-07-13 11:50:21 - train: epoch 0007, iter [03900, 05004], lr: 0.099979, loss: 3.4191
2022-07-13 11:50:55 - train: epoch 0007, iter [04000, 05004], lr: 0.099979, loss: 3.3961
2022-07-13 11:51:31 - train: epoch 0007, iter [04100, 05004], lr: 0.099979, loss: 3.1449
2022-07-13 11:52:06 - train: epoch 0007, iter [04200, 05004], lr: 0.099978, loss: 3.1269
2022-07-13 11:52:41 - train: epoch 0007, iter [04300, 05004], lr: 0.099978, loss: 3.3310
2022-07-13 11:53:15 - train: epoch 0007, iter [04400, 05004], lr: 0.099977, loss: 3.1234
2022-07-13 11:53:50 - train: epoch 0007, iter [04500, 05004], lr: 0.099977, loss: 3.5033
2022-07-13 11:54:25 - train: epoch 0007, iter [04600, 05004], lr: 0.099976, loss: 3.5228
2022-07-13 11:55:01 - train: epoch 0007, iter [04700, 05004], lr: 0.099976, loss: 3.5552
2022-07-13 11:55:35 - train: epoch 0007, iter [04800, 05004], lr: 0.099975, loss: 3.6932
2022-07-13 11:56:09 - train: epoch 0007, iter [04900, 05004], lr: 0.099975, loss: 3.3091
2022-07-13 11:56:42 - train: epoch 0007, iter [05000, 05004], lr: 0.099974, loss: 3.3706
2022-07-13 11:56:44 - train: epoch 007, train_loss: 3.4158
2022-07-13 11:57:58 - eval: epoch: 007, acc1: 45.732%, acc5: 72.116%, test_loss: 2.4069, per_image_load_time: 2.391ms, per_image_inference_time: 0.460ms
2022-07-13 11:57:58 - until epoch: 007, best_acc1: 45.732%
2022-07-13 11:57:58 - epoch 008 lr: 0.099974
2022-07-13 11:58:37 - train: epoch 0008, iter [00100, 05004], lr: 0.099974, loss: 3.3859
2022-07-13 11:59:12 - train: epoch 0008, iter [00200, 05004], lr: 0.099973, loss: 3.4591
2022-07-13 11:59:44 - train: epoch 0008, iter [00300, 05004], lr: 0.099972, loss: 3.1828
2022-07-13 12:00:19 - train: epoch 0008, iter [00400, 05004], lr: 0.099972, loss: 3.4355
2022-07-13 12:00:52 - train: epoch 0008, iter [00500, 05004], lr: 0.099971, loss: 3.3322
2022-07-13 12:01:26 - train: epoch 0008, iter [00600, 05004], lr: 0.099971, loss: 3.1322
2022-07-13 12:02:00 - train: epoch 0008, iter [00700, 05004], lr: 0.099970, loss: 3.6077
2022-07-13 12:02:33 - train: epoch 0008, iter [00800, 05004], lr: 0.099970, loss: 3.0877
2022-07-13 12:03:07 - train: epoch 0008, iter [00900, 05004], lr: 0.099969, loss: 3.3183
2022-07-13 12:03:41 - train: epoch 0008, iter [01000, 05004], lr: 0.099969, loss: 3.4940
2022-07-13 12:04:14 - train: epoch 0008, iter [01100, 05004], lr: 0.099968, loss: 3.1196
2022-07-13 12:04:48 - train: epoch 0008, iter [01200, 05004], lr: 0.099967, loss: 3.0471
2022-07-13 12:05:22 - train: epoch 0008, iter [01300, 05004], lr: 0.099967, loss: 3.1999
2022-07-13 12:05:56 - train: epoch 0008, iter [01400, 05004], lr: 0.099966, loss: 3.1571
2022-07-13 12:06:30 - train: epoch 0008, iter [01500, 05004], lr: 0.099966, loss: 3.3375
2022-07-13 12:07:03 - train: epoch 0008, iter [01600, 05004], lr: 0.099965, loss: 3.3532
2022-07-13 12:07:38 - train: epoch 0008, iter [01700, 05004], lr: 0.099964, loss: 3.2391
2022-07-13 12:08:12 - train: epoch 0008, iter [01800, 05004], lr: 0.099964, loss: 3.5496
2022-07-13 12:08:45 - train: epoch 0008, iter [01900, 05004], lr: 0.099963, loss: 3.1930
2022-07-13 12:09:19 - train: epoch 0008, iter [02000, 05004], lr: 0.099963, loss: 3.4725
2022-07-13 12:09:53 - train: epoch 0008, iter [02100, 05004], lr: 0.099962, loss: 3.2605
2022-07-13 12:10:28 - train: epoch 0008, iter [02200, 05004], lr: 0.099961, loss: 3.1280
2022-07-13 12:11:02 - train: epoch 0008, iter [02300, 05004], lr: 0.099961, loss: 3.5054
2022-07-13 12:11:36 - train: epoch 0008, iter [02400, 05004], lr: 0.099960, loss: 3.1778
2022-07-13 12:12:10 - train: epoch 0008, iter [02500, 05004], lr: 0.099959, loss: 3.1649
2022-07-13 12:12:44 - train: epoch 0008, iter [02600, 05004], lr: 0.099959, loss: 3.6979
2022-07-13 12:13:18 - train: epoch 0008, iter [02700, 05004], lr: 0.099958, loss: 3.4515
2022-07-13 12:13:52 - train: epoch 0008, iter [02800, 05004], lr: 0.099957, loss: 3.4197
2022-07-13 12:14:25 - train: epoch 0008, iter [02900, 05004], lr: 0.099957, loss: 3.3077
2022-07-13 12:15:00 - train: epoch 0008, iter [03000, 05004], lr: 0.099956, loss: 3.4453
2022-07-13 12:15:33 - train: epoch 0008, iter [03100, 05004], lr: 0.099955, loss: 3.6292
2022-07-13 12:16:08 - train: epoch 0008, iter [03200, 05004], lr: 0.099955, loss: 3.7289
2022-07-13 12:16:42 - train: epoch 0008, iter [03300, 05004], lr: 0.099954, loss: 3.4378
2022-07-13 12:17:15 - train: epoch 0008, iter [03400, 05004], lr: 0.099953, loss: 3.2923
2022-07-13 12:17:50 - train: epoch 0008, iter [03500, 05004], lr: 0.099953, loss: 3.3189
2022-07-13 12:18:24 - train: epoch 0008, iter [03600, 05004], lr: 0.099952, loss: 3.4600
2022-07-13 12:18:58 - train: epoch 0008, iter [03700, 05004], lr: 0.099951, loss: 3.2487
2022-07-13 12:19:32 - train: epoch 0008, iter [03800, 05004], lr: 0.099951, loss: 3.0603
2022-07-13 12:20:06 - train: epoch 0008, iter [03900, 05004], lr: 0.099950, loss: 3.4160
2022-07-13 12:20:40 - train: epoch 0008, iter [04000, 05004], lr: 0.099949, loss: 3.5091
2022-07-13 12:21:14 - train: epoch 0008, iter [04100, 05004], lr: 0.099948, loss: 3.2147
2022-07-13 12:21:47 - train: epoch 0008, iter [04200, 05004], lr: 0.099948, loss: 3.3031
2022-07-13 12:22:21 - train: epoch 0008, iter [04300, 05004], lr: 0.099947, loss: 2.7529
2022-07-13 12:22:55 - train: epoch 0008, iter [04400, 05004], lr: 0.099946, loss: 3.1610
2022-07-13 12:23:30 - train: epoch 0008, iter [04500, 05004], lr: 0.099945, loss: 3.5984
2022-07-13 12:24:04 - train: epoch 0008, iter [04600, 05004], lr: 0.099945, loss: 3.6727
2022-07-13 12:24:37 - train: epoch 0008, iter [04700, 05004], lr: 0.099944, loss: 3.1247
2022-07-13 12:25:12 - train: epoch 0008, iter [04800, 05004], lr: 0.099943, loss: 3.4600
2022-07-13 12:25:46 - train: epoch 0008, iter [04900, 05004], lr: 0.099942, loss: 3.3077
2022-07-13 12:26:19 - train: epoch 0008, iter [05000, 05004], lr: 0.099942, loss: 3.4608
2022-07-13 12:26:20 - train: epoch 008, train_loss: 3.3366
2022-07-13 12:27:36 - eval: epoch: 008, acc1: 48.258%, acc5: 74.178%, test_loss: 2.2615, per_image_load_time: 2.432ms, per_image_inference_time: 0.468ms
2022-07-13 12:27:36 - until epoch: 008, best_acc1: 48.258%
2022-07-13 12:27:36 - epoch 009 lr: 0.099942
2022-07-13 12:28:16 - train: epoch 0009, iter [00100, 05004], lr: 0.099941, loss: 2.9769
2022-07-13 12:28:50 - train: epoch 0009, iter [00200, 05004], lr: 0.099940, loss: 3.2886
2022-07-13 12:29:25 - train: epoch 0009, iter [00300, 05004], lr: 0.099939, loss: 2.8730
2022-07-13 12:30:00 - train: epoch 0009, iter [00400, 05004], lr: 0.099938, loss: 3.5322
2022-07-13 12:30:34 - train: epoch 0009, iter [00500, 05004], lr: 0.099938, loss: 3.2047
2022-07-13 12:31:08 - train: epoch 0009, iter [00600, 05004], lr: 0.099937, loss: 3.2047
2022-07-13 12:31:43 - train: epoch 0009, iter [00700, 05004], lr: 0.099936, loss: 3.3915
2022-07-13 12:32:18 - train: epoch 0009, iter [00800, 05004], lr: 0.099935, loss: 3.3057
2022-07-13 12:32:53 - train: epoch 0009, iter [00900, 05004], lr: 0.099934, loss: 3.2316
2022-07-13 12:33:26 - train: epoch 0009, iter [01000, 05004], lr: 0.099934, loss: 3.0283
2022-07-13 12:34:02 - train: epoch 0009, iter [01100, 05004], lr: 0.099933, loss: 3.6262
2022-07-13 12:34:37 - train: epoch 0009, iter [01200, 05004], lr: 0.099932, loss: 3.3152
2022-07-13 12:35:12 - train: epoch 0009, iter [01300, 05004], lr: 0.099931, loss: 3.3683
2022-07-13 12:35:46 - train: epoch 0009, iter [01400, 05004], lr: 0.099930, loss: 2.9207
2022-07-13 12:36:21 - train: epoch 0009, iter [01500, 05004], lr: 0.099929, loss: 3.2706
2022-07-13 12:36:56 - train: epoch 0009, iter [01600, 05004], lr: 0.099929, loss: 3.2194
2022-07-13 12:37:30 - train: epoch 0009, iter [01700, 05004], lr: 0.099928, loss: 3.1989
2022-07-13 12:38:05 - train: epoch 0009, iter [01800, 05004], lr: 0.099927, loss: 3.4411
2022-07-13 12:38:40 - train: epoch 0009, iter [01900, 05004], lr: 0.099926, loss: 2.9438
2022-07-13 12:39:15 - train: epoch 0009, iter [02000, 05004], lr: 0.099925, loss: 3.0015
2022-07-13 12:39:50 - train: epoch 0009, iter [02100, 05004], lr: 0.099924, loss: 3.4318
2022-07-13 12:40:25 - train: epoch 0009, iter [02200, 05004], lr: 0.099923, loss: 3.2581
2022-07-13 12:41:00 - train: epoch 0009, iter [02300, 05004], lr: 0.099922, loss: 3.2929
2022-07-13 12:41:35 - train: epoch 0009, iter [02400, 05004], lr: 0.099921, loss: 3.3208
2022-07-13 12:42:10 - train: epoch 0009, iter [02500, 05004], lr: 0.099921, loss: 3.1840
2022-07-13 12:42:45 - train: epoch 0009, iter [02600, 05004], lr: 0.099920, loss: 3.2921
2022-07-13 12:43:19 - train: epoch 0009, iter [02700, 05004], lr: 0.099919, loss: 3.3253
2022-07-13 12:43:54 - train: epoch 0009, iter [02800, 05004], lr: 0.099918, loss: 3.2274
2022-07-13 12:44:30 - train: epoch 0009, iter [02900, 05004], lr: 0.099917, loss: 3.0369
2022-07-13 12:45:05 - train: epoch 0009, iter [03000, 05004], lr: 0.099916, loss: 3.0841
2022-07-13 12:45:39 - train: epoch 0009, iter [03100, 05004], lr: 0.099915, loss: 3.3044
2022-07-13 12:46:15 - train: epoch 0009, iter [03200, 05004], lr: 0.099914, loss: 3.1458
2022-07-13 12:46:51 - train: epoch 0009, iter [03300, 05004], lr: 0.099913, loss: 3.2482
2022-07-13 12:47:25 - train: epoch 0009, iter [03400, 05004], lr: 0.099912, loss: 3.5630
2022-07-13 12:48:00 - train: epoch 0009, iter [03500, 05004], lr: 0.099911, loss: 3.5438
2022-07-13 12:48:36 - train: epoch 0009, iter [03600, 05004], lr: 0.099910, loss: 3.1294
2022-07-13 12:49:11 - train: epoch 0009, iter [03700, 05004], lr: 0.099909, loss: 3.4168
2022-07-13 12:49:46 - train: epoch 0009, iter [03800, 05004], lr: 0.099908, loss: 3.4724
2022-07-13 12:50:22 - train: epoch 0009, iter [03900, 05004], lr: 0.099907, loss: 3.0952
2022-07-13 12:50:57 - train: epoch 0009, iter [04000, 05004], lr: 0.099906, loss: 3.5047
2022-07-13 12:51:32 - train: epoch 0009, iter [04100, 05004], lr: 0.099905, loss: 3.4949
2022-07-13 12:52:07 - train: epoch 0009, iter [04200, 05004], lr: 0.099904, loss: 3.4385
2022-07-13 12:52:41 - train: epoch 0009, iter [04300, 05004], lr: 0.099903, loss: 3.0356
2022-07-13 12:53:14 - train: epoch 0009, iter [04400, 05004], lr: 0.099902, loss: 3.1463
2022-07-13 12:53:49 - train: epoch 0009, iter [04500, 05004], lr: 0.099901, loss: 3.3968
2022-07-13 12:54:23 - train: epoch 0009, iter [04600, 05004], lr: 0.099900, loss: 3.3637
2022-07-13 12:54:57 - train: epoch 0009, iter [04700, 05004], lr: 0.099899, loss: 3.1464
2022-07-13 12:55:31 - train: epoch 0009, iter [04800, 05004], lr: 0.099898, loss: 3.3168
2022-07-13 12:56:05 - train: epoch 0009, iter [04900, 05004], lr: 0.099897, loss: 3.3471
2022-07-13 12:56:37 - train: epoch 0009, iter [05000, 05004], lr: 0.099896, loss: 3.1076
2022-07-13 12:56:38 - train: epoch 009, train_loss: 3.2789
2022-07-13 12:57:53 - eval: epoch: 009, acc1: 48.220%, acc5: 74.278%, test_loss: 2.2707, per_image_load_time: 2.421ms, per_image_inference_time: 0.451ms
2022-07-13 12:57:53 - until epoch: 009, best_acc1: 48.258%
2022-07-13 12:57:53 - epoch 010 lr: 0.099896
2022-07-13 12:58:33 - train: epoch 0010, iter [00100, 05004], lr: 0.099895, loss: 3.0703
2022-07-13 12:59:06 - train: epoch 0010, iter [00200, 05004], lr: 0.099894, loss: 3.2328
2022-07-13 12:59:42 - train: epoch 0010, iter [00300, 05004], lr: 0.099893, loss: 3.2645
2022-07-13 13:00:17 - train: epoch 0010, iter [00400, 05004], lr: 0.099892, loss: 3.4849
2022-07-13 13:00:50 - train: epoch 0010, iter [00500, 05004], lr: 0.099891, loss: 3.1994
2022-07-13 13:01:25 - train: epoch 0010, iter [00600, 05004], lr: 0.099890, loss: 3.3540
2022-07-13 13:02:00 - train: epoch 0010, iter [00700, 05004], lr: 0.099889, loss: 3.3726
2022-07-13 13:02:34 - train: epoch 0010, iter [00800, 05004], lr: 0.099888, loss: 3.0696
2022-07-13 13:03:09 - train: epoch 0010, iter [00900, 05004], lr: 0.099887, loss: 3.0700
2022-07-13 13:03:43 - train: epoch 0010, iter [01000, 05004], lr: 0.099886, loss: 2.9417
2022-07-13 13:04:18 - train: epoch 0010, iter [01100, 05004], lr: 0.099884, loss: 3.3034
2022-07-13 13:04:53 - train: epoch 0010, iter [01200, 05004], lr: 0.099883, loss: 3.0613
2022-07-13 13:05:28 - train: epoch 0010, iter [01300, 05004], lr: 0.099882, loss: 3.2924
2022-07-13 13:06:03 - train: epoch 0010, iter [01400, 05004], lr: 0.099881, loss: 3.2364
2022-07-13 13:06:37 - train: epoch 0010, iter [01500, 05004], lr: 0.099880, loss: 3.1301
2022-07-13 13:07:12 - train: epoch 0010, iter [01600, 05004], lr: 0.099879, loss: 3.3375
2022-07-13 13:07:47 - train: epoch 0010, iter [01700, 05004], lr: 0.099878, loss: 3.3830
2022-07-13 13:08:21 - train: epoch 0010, iter [01800, 05004], lr: 0.099877, loss: 3.2734
2022-07-13 13:08:57 - train: epoch 0010, iter [01900, 05004], lr: 0.099876, loss: 3.1088
2022-07-13 13:09:32 - train: epoch 0010, iter [02000, 05004], lr: 0.099874, loss: 3.2066
2022-07-13 13:10:06 - train: epoch 0010, iter [02100, 05004], lr: 0.099873, loss: 3.0630
2022-07-13 13:10:41 - train: epoch 0010, iter [02200, 05004], lr: 0.099872, loss: 3.4052
2022-07-13 13:11:16 - train: epoch 0010, iter [02300, 05004], lr: 0.099871, loss: 3.3094
2022-07-13 13:11:51 - train: epoch 0010, iter [02400, 05004], lr: 0.099870, loss: 3.5882
2022-07-13 13:12:25 - train: epoch 0010, iter [02500, 05004], lr: 0.099869, loss: 3.4104
2022-07-13 13:13:01 - train: epoch 0010, iter [02600, 05004], lr: 0.099868, loss: 3.4606
2022-07-13 13:13:34 - train: epoch 0010, iter [02700, 05004], lr: 0.099866, loss: 2.9379
2022-07-13 13:14:07 - train: epoch 0010, iter [02800, 05004], lr: 0.099865, loss: 3.2454
2022-07-13 13:14:41 - train: epoch 0010, iter [02900, 05004], lr: 0.099864, loss: 3.3183
2022-07-13 13:15:15 - train: epoch 0010, iter [03000, 05004], lr: 0.099863, loss: 3.0329
2022-07-13 13:15:49 - train: epoch 0010, iter [03100, 05004], lr: 0.099862, loss: 3.4697
2022-07-13 13:16:23 - train: epoch 0010, iter [03200, 05004], lr: 0.099860, loss: 3.1495
2022-07-13 13:16:58 - train: epoch 0010, iter [03300, 05004], lr: 0.099859, loss: 3.3296
2022-07-13 13:17:32 - train: epoch 0010, iter [03400, 05004], lr: 0.099858, loss: 3.2627
2022-07-13 13:18:06 - train: epoch 0010, iter [03500, 05004], lr: 0.099857, loss: 3.3737
2022-07-13 13:18:40 - train: epoch 0010, iter [03600, 05004], lr: 0.099856, loss: 3.3924
2022-07-13 13:19:14 - train: epoch 0010, iter [03700, 05004], lr: 0.099854, loss: 3.3277
2022-07-13 13:19:48 - train: epoch 0010, iter [03800, 05004], lr: 0.099853, loss: 3.1485
2022-07-13 13:20:23 - train: epoch 0010, iter [03900, 05004], lr: 0.099852, loss: 3.0864
2022-07-13 13:20:57 - train: epoch 0010, iter [04000, 05004], lr: 0.099851, loss: 3.0230
2022-07-13 13:21:31 - train: epoch 0010, iter [04100, 05004], lr: 0.099849, loss: 3.1155
2022-07-13 13:22:05 - train: epoch 0010, iter [04200, 05004], lr: 0.099848, loss: 3.2913
2022-07-13 13:22:40 - train: epoch 0010, iter [04300, 05004], lr: 0.099847, loss: 3.3347
2022-07-13 13:23:14 - train: epoch 0010, iter [04400, 05004], lr: 0.099846, loss: 3.1734
2022-07-13 13:23:49 - train: epoch 0010, iter [04500, 05004], lr: 0.099844, loss: 3.1142
2022-07-13 13:24:22 - train: epoch 0010, iter [04600, 05004], lr: 0.099843, loss: 3.3071
2022-07-13 13:24:56 - train: epoch 0010, iter [04700, 05004], lr: 0.099842, loss: 3.3246
2022-07-13 13:25:30 - train: epoch 0010, iter [04800, 05004], lr: 0.099840, loss: 2.9713
2022-07-13 13:26:05 - train: epoch 0010, iter [04900, 05004], lr: 0.099839, loss: 3.1610
2022-07-13 13:26:38 - train: epoch 0010, iter [05000, 05004], lr: 0.099838, loss: 3.0075
2022-07-13 13:26:39 - train: epoch 010, train_loss: 3.2280
2022-07-13 13:27:54 - eval: epoch: 010, acc1: 50.086%, acc5: 75.908%, test_loss: 2.1824, per_image_load_time: 2.472ms, per_image_inference_time: 0.465ms
2022-07-13 13:27:55 - until epoch: 010, best_acc1: 50.086%
2022-07-13 13:27:55 - epoch 011 lr: 0.099838
2022-07-13 13:28:35 - train: epoch 0011, iter [00100, 05004], lr: 0.099837, loss: 3.0546
2022-07-13 13:29:09 - train: epoch 0011, iter [00200, 05004], lr: 0.099835, loss: 3.1527
2022-07-13 13:29:44 - train: epoch 0011, iter [00300, 05004], lr: 0.099834, loss: 2.9836
2022-07-13 13:30:18 - train: epoch 0011, iter [00400, 05004], lr: 0.099833, loss: 3.4709
2022-07-13 13:30:52 - train: epoch 0011, iter [00500, 05004], lr: 0.099831, loss: 2.9957
2022-07-13 13:31:27 - train: epoch 0011, iter [00600, 05004], lr: 0.099830, loss: 3.1389
2022-07-13 13:32:01 - train: epoch 0011, iter [00700, 05004], lr: 0.099829, loss: 3.1657
2022-07-13 13:32:36 - train: epoch 0011, iter [00800, 05004], lr: 0.099827, loss: 3.2046
2022-07-13 13:33:10 - train: epoch 0011, iter [00900, 05004], lr: 0.099826, loss: 3.2635
2022-07-13 13:33:44 - train: epoch 0011, iter [01000, 05004], lr: 0.099825, loss: 3.3244
2022-07-13 13:34:19 - train: epoch 0011, iter [01100, 05004], lr: 0.099823, loss: 3.2592
2022-07-13 13:34:53 - train: epoch 0011, iter [01200, 05004], lr: 0.099822, loss: 3.6322
2022-07-13 13:35:27 - train: epoch 0011, iter [01300, 05004], lr: 0.099821, loss: 3.3589
2022-07-13 13:36:01 - train: epoch 0011, iter [01400, 05004], lr: 0.099819, loss: 3.0847
2022-07-13 13:36:37 - train: epoch 0011, iter [01500, 05004], lr: 0.099818, loss: 3.0425
2022-07-13 13:37:12 - train: epoch 0011, iter [01600, 05004], lr: 0.099816, loss: 3.1010
2022-07-13 13:37:47 - train: epoch 0011, iter [01700, 05004], lr: 0.099815, loss: 3.3023
2022-07-13 13:38:21 - train: epoch 0011, iter [01800, 05004], lr: 0.099814, loss: 3.2021
2022-07-13 13:38:56 - train: epoch 0011, iter [01900, 05004], lr: 0.099812, loss: 2.9080
2022-07-13 13:39:31 - train: epoch 0011, iter [02000, 05004], lr: 0.099811, loss: 3.4422
2022-07-13 13:40:07 - train: epoch 0011, iter [02100, 05004], lr: 0.099810, loss: 3.0609
2022-07-13 13:40:42 - train: epoch 0011, iter [02200, 05004], lr: 0.099808, loss: 3.1738
2022-07-13 13:41:17 - train: epoch 0011, iter [02300, 05004], lr: 0.099807, loss: 3.2594
2022-07-13 13:41:52 - train: epoch 0011, iter [02400, 05004], lr: 0.099805, loss: 2.8298
2022-07-13 13:42:26 - train: epoch 0011, iter [02500, 05004], lr: 0.099804, loss: 3.5664
2022-07-13 13:43:01 - train: epoch 0011, iter [02600, 05004], lr: 0.099802, loss: 3.3180
2022-07-13 13:43:36 - train: epoch 0011, iter [02700, 05004], lr: 0.099801, loss: 3.2541
2022-07-13 13:44:10 - train: epoch 0011, iter [02800, 05004], lr: 0.099800, loss: 3.0169
2022-07-13 13:44:43 - train: epoch 0011, iter [02900, 05004], lr: 0.099798, loss: 3.2722
2022-07-13 13:45:17 - train: epoch 0011, iter [03000, 05004], lr: 0.099797, loss: 3.2849
2022-07-13 13:45:50 - train: epoch 0011, iter [03100, 05004], lr: 0.099795, loss: 3.1249
2022-07-13 13:46:24 - train: epoch 0011, iter [03200, 05004], lr: 0.099794, loss: 3.0064
2022-07-13 13:46:58 - train: epoch 0011, iter [03300, 05004], lr: 0.099792, loss: 3.2082
2022-07-13 13:47:32 - train: epoch 0011, iter [03400, 05004], lr: 0.099791, loss: 3.3401
2022-07-13 13:48:07 - train: epoch 0011, iter [03500, 05004], lr: 0.099789, loss: 3.2387
2022-07-13 13:48:41 - train: epoch 0011, iter [03600, 05004], lr: 0.099788, loss: 3.1321
2022-07-13 13:49:14 - train: epoch 0011, iter [03700, 05004], lr: 0.099786, loss: 3.3279
2022-07-13 13:49:48 - train: epoch 0011, iter [03800, 05004], lr: 0.099785, loss: 3.1164
2022-07-13 13:50:22 - train: epoch 0011, iter [03900, 05004], lr: 0.099783, loss: 3.3767
2022-07-13 13:50:54 - train: epoch 0011, iter [04000, 05004], lr: 0.099782, loss: 2.9738
2022-07-13 13:51:29 - train: epoch 0011, iter [04100, 05004], lr: 0.099780, loss: 2.9013
2022-07-13 13:52:03 - train: epoch 0011, iter [04200, 05004], lr: 0.099779, loss: 3.0121
2022-07-13 13:52:37 - train: epoch 0011, iter [04300, 05004], lr: 0.099777, loss: 3.3252
2022-07-13 13:53:10 - train: epoch 0011, iter [04400, 05004], lr: 0.099776, loss: 3.1575
2022-07-13 13:53:44 - train: epoch 0011, iter [04500, 05004], lr: 0.099774, loss: 2.9351
2022-07-13 13:54:17 - train: epoch 0011, iter [04600, 05004], lr: 0.099773, loss: 3.0138
2022-07-13 13:54:52 - train: epoch 0011, iter [04700, 05004], lr: 0.099771, loss: 2.7440
2022-07-13 13:55:25 - train: epoch 0011, iter [04800, 05004], lr: 0.099770, loss: 3.0146
2022-07-13 13:55:59 - train: epoch 0011, iter [04900, 05004], lr: 0.099768, loss: 2.9803
2022-07-13 13:56:31 - train: epoch 0011, iter [05000, 05004], lr: 0.099767, loss: 3.0657
2022-07-13 13:56:32 - train: epoch 011, train_loss: 3.1830
2022-07-13 13:57:45 - eval: epoch: 011, acc1: 50.914%, acc5: 76.822%, test_loss: 2.1107, per_image_load_time: 1.532ms, per_image_inference_time: 0.449ms
2022-07-13 13:57:45 - until epoch: 011, best_acc1: 50.914%
2022-07-13 13:57:45 - epoch 012 lr: 0.099767
2022-07-13 13:58:24 - train: epoch 0012, iter [00100, 05004], lr: 0.099765, loss: 3.3112
2022-07-13 13:58:57 - train: epoch 0012, iter [00200, 05004], lr: 0.099763, loss: 3.1011
2022-07-13 13:59:30 - train: epoch 0012, iter [00300, 05004], lr: 0.099762, loss: 3.0081
2022-07-13 14:00:04 - train: epoch 0012, iter [00400, 05004], lr: 0.099760, loss: 3.3527
2022-07-13 14:00:38 - train: epoch 0012, iter [00500, 05004], lr: 0.099759, loss: 3.3758
2022-07-13 14:01:12 - train: epoch 0012, iter [00600, 05004], lr: 0.099757, loss: 2.8829
2022-07-13 14:01:46 - train: epoch 0012, iter [00700, 05004], lr: 0.099756, loss: 3.3040
2022-07-13 14:02:20 - train: epoch 0012, iter [00800, 05004], lr: 0.099754, loss: 3.2304
2022-07-13 14:02:55 - train: epoch 0012, iter [00900, 05004], lr: 0.099752, loss: 3.2298
2022-07-13 14:03:28 - train: epoch 0012, iter [01000, 05004], lr: 0.099751, loss: 2.8828
2022-07-13 14:04:02 - train: epoch 0012, iter [01100, 05004], lr: 0.099749, loss: 3.4665
2022-07-13 14:04:37 - train: epoch 0012, iter [01200, 05004], lr: 0.099748, loss: 2.9054
2022-07-13 14:05:11 - train: epoch 0012, iter [01300, 05004], lr: 0.099746, loss: 3.1246
2022-07-13 14:05:45 - train: epoch 0012, iter [01400, 05004], lr: 0.099744, loss: 3.3341
2022-07-13 14:06:19 - train: epoch 0012, iter [01500, 05004], lr: 0.099743, loss: 2.8620
2022-07-13 14:06:53 - train: epoch 0012, iter [01600, 05004], lr: 0.099741, loss: 3.1148
2022-07-13 14:07:28 - train: epoch 0012, iter [01700, 05004], lr: 0.099739, loss: 3.0189
2022-07-13 14:08:02 - train: epoch 0012, iter [01800, 05004], lr: 0.099738, loss: 3.2170
2022-07-13 14:08:37 - train: epoch 0012, iter [01900, 05004], lr: 0.099736, loss: 3.3928
2022-07-13 14:09:12 - train: epoch 0012, iter [02000, 05004], lr: 0.099734, loss: 3.2478
2022-07-13 14:09:46 - train: epoch 0012, iter [02100, 05004], lr: 0.099733, loss: 3.2379
2022-07-13 14:10:20 - train: epoch 0012, iter [02200, 05004], lr: 0.099731, loss: 3.2995
2022-07-13 14:10:55 - train: epoch 0012, iter [02300, 05004], lr: 0.099729, loss: 3.2913
2022-07-13 14:11:29 - train: epoch 0012, iter [02400, 05004], lr: 0.099728, loss: 3.1153
2022-07-13 14:12:04 - train: epoch 0012, iter [02500, 05004], lr: 0.099726, loss: 3.3951
2022-07-13 14:12:39 - train: epoch 0012, iter [02600, 05004], lr: 0.099724, loss: 2.9525
2022-07-13 14:13:14 - train: epoch 0012, iter [02700, 05004], lr: 0.099723, loss: 2.9961
2022-07-13 14:13:48 - train: epoch 0012, iter [02800, 05004], lr: 0.099721, loss: 3.1953
2022-07-13 14:14:22 - train: epoch 0012, iter [02900, 05004], lr: 0.099719, loss: 2.9314
2022-07-13 14:14:57 - train: epoch 0012, iter [03000, 05004], lr: 0.099718, loss: 2.9306
2022-07-13 14:15:31 - train: epoch 0012, iter [03100, 05004], lr: 0.099716, loss: 3.0816
2022-07-13 14:16:06 - train: epoch 0012, iter [03200, 05004], lr: 0.099714, loss: 2.7597
2022-07-13 14:16:40 - train: epoch 0012, iter [03300, 05004], lr: 0.099713, loss: 3.1333
2022-07-13 14:17:14 - train: epoch 0012, iter [03400, 05004], lr: 0.099711, loss: 3.0516
2022-07-13 14:17:49 - train: epoch 0012, iter [03500, 05004], lr: 0.099709, loss: 3.3288
2022-07-13 14:18:23 - train: epoch 0012, iter [03600, 05004], lr: 0.099707, loss: 3.0030
2022-07-13 14:18:57 - train: epoch 0012, iter [03700, 05004], lr: 0.099706, loss: 3.1252
2022-07-13 14:19:32 - train: epoch 0012, iter [03800, 05004], lr: 0.099704, loss: 3.0148
2022-07-13 14:20:07 - train: epoch 0012, iter [03900, 05004], lr: 0.099702, loss: 3.0726
2022-07-13 14:20:41 - train: epoch 0012, iter [04000, 05004], lr: 0.099700, loss: 3.2004
2022-07-13 14:21:15 - train: epoch 0012, iter [04100, 05004], lr: 0.099699, loss: 3.1132
2022-07-13 14:21:50 - train: epoch 0012, iter [04200, 05004], lr: 0.099697, loss: 2.9276
2022-07-13 14:22:25 - train: epoch 0012, iter [04300, 05004], lr: 0.099695, loss: 3.1785
2022-07-13 14:22:59 - train: epoch 0012, iter [04400, 05004], lr: 0.099693, loss: 2.9160
2022-07-13 14:23:34 - train: epoch 0012, iter [04500, 05004], lr: 0.099691, loss: 2.9315
2022-07-13 14:24:09 - train: epoch 0012, iter [04600, 05004], lr: 0.099690, loss: 3.5128
2022-07-13 14:24:44 - train: epoch 0012, iter [04700, 05004], lr: 0.099688, loss: 3.1423
2022-07-13 14:25:18 - train: epoch 0012, iter [04800, 05004], lr: 0.099686, loss: 3.3051
2022-07-13 14:25:52 - train: epoch 0012, iter [04900, 05004], lr: 0.099684, loss: 3.0416
2022-07-13 14:26:26 - train: epoch 0012, iter [05000, 05004], lr: 0.099682, loss: 2.9761
2022-07-13 14:26:27 - train: epoch 012, train_loss: 3.1518
2022-07-13 14:27:42 - eval: epoch: 012, acc1: 48.948%, acc5: 75.102%, test_loss: 2.2173, per_image_load_time: 2.361ms, per_image_inference_time: 0.478ms
2022-07-13 14:27:43 - until epoch: 012, best_acc1: 50.914%
2022-07-13 14:27:43 - epoch 013 lr: 0.099682
2022-07-13 14:28:22 - train: epoch 0013, iter [00100, 05004], lr: 0.099681, loss: 2.8149
2022-07-13 14:28:57 - train: epoch 0013, iter [00200, 05004], lr: 0.099679, loss: 3.2409
2022-07-13 14:29:32 - train: epoch 0013, iter [00300, 05004], lr: 0.099677, loss: 2.8741
2022-07-13 14:30:06 - train: epoch 0013, iter [00400, 05004], lr: 0.099675, loss: 3.1484
2022-07-13 14:30:41 - train: epoch 0013, iter [00500, 05004], lr: 0.099673, loss: 3.4191
2022-07-13 14:31:14 - train: epoch 0013, iter [00600, 05004], lr: 0.099671, loss: 3.4658
2022-07-13 14:31:48 - train: epoch 0013, iter [00700, 05004], lr: 0.099670, loss: 3.0272
2022-07-13 14:32:22 - train: epoch 0013, iter [00800, 05004], lr: 0.099668, loss: 3.0577
2022-07-13 14:32:56 - train: epoch 0013, iter [00900, 05004], lr: 0.099666, loss: 3.0845
2022-07-13 14:33:31 - train: epoch 0013, iter [01000, 05004], lr: 0.099664, loss: 2.9363
2022-07-13 14:34:05 - train: epoch 0013, iter [01100, 05004], lr: 0.099662, loss: 3.1935
2022-07-13 14:34:39 - train: epoch 0013, iter [01200, 05004], lr: 0.099660, loss: 3.5786
2022-07-13 14:35:13 - train: epoch 0013, iter [01300, 05004], lr: 0.099658, loss: 3.2718
2022-07-13 14:35:46 - train: epoch 0013, iter [01400, 05004], lr: 0.099657, loss: 3.1573
2022-07-13 14:36:21 - train: epoch 0013, iter [01500, 05004], lr: 0.099655, loss: 3.1474
2022-07-13 14:36:55 - train: epoch 0013, iter [01600, 05004], lr: 0.099653, loss: 2.9821
2022-07-13 14:37:29 - train: epoch 0013, iter [01700, 05004], lr: 0.099651, loss: 3.0366
2022-07-13 14:38:03 - train: epoch 0013, iter [01800, 05004], lr: 0.099649, loss: 3.3230
2022-07-13 14:38:38 - train: epoch 0013, iter [01900, 05004], lr: 0.099647, loss: 3.0094
2022-07-13 14:39:12 - train: epoch 0013, iter [02000, 05004], lr: 0.099645, loss: 3.1774
2022-07-13 14:39:46 - train: epoch 0013, iter [02100, 05004], lr: 0.099643, loss: 3.2401
2022-07-13 14:40:20 - train: epoch 0013, iter [02200, 05004], lr: 0.099641, loss: 3.0933
2022-07-13 14:40:54 - train: epoch 0013, iter [02300, 05004], lr: 0.099639, loss: 2.9150
2022-07-13 14:41:29 - train: epoch 0013, iter [02400, 05004], lr: 0.099637, loss: 3.0388
2022-07-13 14:42:03 - train: epoch 0013, iter [02500, 05004], lr: 0.099635, loss: 2.9510
2022-07-13 14:42:37 - train: epoch 0013, iter [02600, 05004], lr: 0.099634, loss: 3.0471
2022-07-13 14:43:11 - train: epoch 0013, iter [02700, 05004], lr: 0.099632, loss: 2.8932
2022-07-13 14:43:46 - train: epoch 0013, iter [02800, 05004], lr: 0.099630, loss: 3.1463
2022-07-13 14:44:21 - train: epoch 0013, iter [02900, 05004], lr: 0.099628, loss: 3.1581
2022-07-13 14:44:54 - train: epoch 0013, iter [03000, 05004], lr: 0.099626, loss: 3.0696
2022-07-13 14:45:28 - train: epoch 0013, iter [03100, 05004], lr: 0.099624, loss: 3.1643
2022-07-13 14:46:02 - train: epoch 0013, iter [03200, 05004], lr: 0.099622, loss: 3.2560
2022-07-13 14:46:36 - train: epoch 0013, iter [03300, 05004], lr: 0.099620, loss: 2.9927
2022-07-13 14:47:12 - train: epoch 0013, iter [03400, 05004], lr: 0.099618, loss: 3.0326
2022-07-13 14:47:45 - train: epoch 0013, iter [03500, 05004], lr: 0.099616, loss: 2.9843
2022-07-13 14:48:20 - train: epoch 0013, iter [03600, 05004], lr: 0.099614, loss: 3.4559
2022-07-13 14:48:54 - train: epoch 0013, iter [03700, 05004], lr: 0.099612, loss: 2.8695
2022-07-13 14:49:29 - train: epoch 0013, iter [03800, 05004], lr: 0.099610, loss: 3.1985
2022-07-13 14:50:03 - train: epoch 0013, iter [03900, 05004], lr: 0.099608, loss: 3.0231
2022-07-13 14:50:38 - train: epoch 0013, iter [04000, 05004], lr: 0.099606, loss: 3.2538
2022-07-13 14:51:13 - train: epoch 0013, iter [04100, 05004], lr: 0.099604, loss: 3.2104
2022-07-13 14:51:47 - train: epoch 0013, iter [04200, 05004], lr: 0.099602, loss: 3.1265
2022-07-13 14:52:22 - train: epoch 0013, iter [04300, 05004], lr: 0.099600, loss: 2.8671
2022-07-13 14:52:56 - train: epoch 0013, iter [04400, 05004], lr: 0.099598, loss: 3.0280
2022-07-13 14:53:31 - train: epoch 0013, iter [04500, 05004], lr: 0.099596, loss: 3.1790
2022-07-13 14:54:06 - train: epoch 0013, iter [04600, 05004], lr: 0.099594, loss: 3.0862
2022-07-13 14:54:41 - train: epoch 0013, iter [04700, 05004], lr: 0.099592, loss: 3.1897
2022-07-13 14:55:15 - train: epoch 0013, iter [04800, 05004], lr: 0.099589, loss: 3.1452
2022-07-13 14:55:51 - train: epoch 0013, iter [04900, 05004], lr: 0.099587, loss: 3.2583
2022-07-13 14:56:24 - train: epoch 0013, iter [05000, 05004], lr: 0.099585, loss: 3.2725
2022-07-13 14:56:25 - train: epoch 013, train_loss: 3.1251
2022-07-13 14:57:41 - eval: epoch: 013, acc1: 52.320%, acc5: 77.782%, test_loss: 2.0590, per_image_load_time: 2.245ms, per_image_inference_time: 0.491ms
2022-07-13 14:57:41 - until epoch: 013, best_acc1: 52.320%
2022-07-13 14:57:41 - epoch 014 lr: 0.099585
2022-07-13 14:58:21 - train: epoch 0014, iter [00100, 05004], lr: 0.099583, loss: 2.9628
2022-07-13 14:58:56 - train: epoch 0014, iter [00200, 05004], lr: 0.099581, loss: 3.0511
2022-07-13 14:59:31 - train: epoch 0014, iter [00300, 05004], lr: 0.099579, loss: 2.9226
2022-07-13 15:00:05 - train: epoch 0014, iter [00400, 05004], lr: 0.099577, loss: 2.7369
2022-07-13 15:00:40 - train: epoch 0014, iter [00500, 05004], lr: 0.099575, loss: 3.1901
2022-07-13 15:01:15 - train: epoch 0014, iter [00600, 05004], lr: 0.099573, loss: 3.1067
2022-07-13 15:01:50 - train: epoch 0014, iter [00700, 05004], lr: 0.099571, loss: 3.0166
2022-07-13 15:02:24 - train: epoch 0014, iter [00800, 05004], lr: 0.099569, loss: 3.0224
2022-07-13 15:02:59 - train: epoch 0014, iter [00900, 05004], lr: 0.099566, loss: 3.1796
2022-07-13 15:03:34 - train: epoch 0014, iter [01000, 05004], lr: 0.099564, loss: 3.1963
2022-07-13 15:04:10 - train: epoch 0014, iter [01100, 05004], lr: 0.099562, loss: 3.2544
2022-07-13 15:04:45 - train: epoch 0014, iter [01200, 05004], lr: 0.099560, loss: 2.9845
2022-07-13 15:05:20 - train: epoch 0014, iter [01300, 05004], lr: 0.099558, loss: 3.0049
2022-07-13 15:05:54 - train: epoch 0014, iter [01400, 05004], lr: 0.099556, loss: 3.2417
2022-07-13 15:06:29 - train: epoch 0014, iter [01500, 05004], lr: 0.099554, loss: 3.0843
2022-07-13 15:07:04 - train: epoch 0014, iter [01600, 05004], lr: 0.099552, loss: 3.2888
2022-07-13 15:07:39 - train: epoch 0014, iter [01700, 05004], lr: 0.099549, loss: 3.4796
2022-07-13 15:08:14 - train: epoch 0014, iter [01800, 05004], lr: 0.099547, loss: 3.1707
2022-07-13 15:08:50 - train: epoch 0014, iter [01900, 05004], lr: 0.099545, loss: 2.9976
2022-07-13 15:09:23 - train: epoch 0014, iter [02000, 05004], lr: 0.099543, loss: 2.9095
2022-07-13 15:09:58 - train: epoch 0014, iter [02100, 05004], lr: 0.099541, loss: 3.1340
2022-07-13 15:10:33 - train: epoch 0014, iter [02200, 05004], lr: 0.099539, loss: 3.2085
2022-07-13 15:11:08 - train: epoch 0014, iter [02300, 05004], lr: 0.099536, loss: 3.1313
2022-07-13 15:11:43 - train: epoch 0014, iter [02400, 05004], lr: 0.099534, loss: 3.2369
2022-07-13 15:12:18 - train: epoch 0014, iter [02500, 05004], lr: 0.099532, loss: 3.0268
2022-07-13 15:12:53 - train: epoch 0014, iter [02600, 05004], lr: 0.099530, loss: 3.0726
2022-07-13 15:13:28 - train: epoch 0014, iter [02700, 05004], lr: 0.099528, loss: 2.9892
2022-07-13 15:14:03 - train: epoch 0014, iter [02800, 05004], lr: 0.099525, loss: 3.2383
2022-07-13 15:14:39 - train: epoch 0014, iter [02900, 05004], lr: 0.099523, loss: 2.9638
2022-07-13 15:15:14 - train: epoch 0014, iter [03000, 05004], lr: 0.099521, loss: 3.1286
2022-07-13 15:15:48 - train: epoch 0014, iter [03100, 05004], lr: 0.099519, loss: 2.9977
2022-07-13 15:16:23 - train: epoch 0014, iter [03200, 05004], lr: 0.099516, loss: 3.3440
2022-07-13 15:16:58 - train: epoch 0014, iter [03300, 05004], lr: 0.099514, loss: 2.9562
2022-07-13 15:17:32 - train: epoch 0014, iter [03400, 05004], lr: 0.099512, loss: 3.0401
2022-07-13 15:18:07 - train: epoch 0014, iter [03500, 05004], lr: 0.099510, loss: 3.2432
2022-07-13 15:18:41 - train: epoch 0014, iter [03600, 05004], lr: 0.099507, loss: 3.1159
2022-07-13 15:19:16 - train: epoch 0014, iter [03700, 05004], lr: 0.099505, loss: 3.1788
2022-07-13 15:19:50 - train: epoch 0014, iter [03800, 05004], lr: 0.099503, loss: 3.3671
2022-07-13 15:20:23 - train: epoch 0014, iter [03900, 05004], lr: 0.099501, loss: 3.0513
2022-07-13 15:20:58 - train: epoch 0014, iter [04000, 05004], lr: 0.099498, loss: 2.9106
2022-07-13 15:21:32 - train: epoch 0014, iter [04100, 05004], lr: 0.099496, loss: 3.0087
2022-07-13 15:22:07 - train: epoch 0014, iter [04200, 05004], lr: 0.099494, loss: 3.1201
2022-07-13 15:22:42 - train: epoch 0014, iter [04300, 05004], lr: 0.099492, loss: 2.9412
2022-07-13 15:23:16 - train: epoch 0014, iter [04400, 05004], lr: 0.099489, loss: 2.8676
2022-07-13 15:23:51 - train: epoch 0014, iter [04500, 05004], lr: 0.099487, loss: 2.9539
2022-07-13 15:24:25 - train: epoch 0014, iter [04600, 05004], lr: 0.099485, loss: 2.8503
2022-07-13 15:24:59 - train: epoch 0014, iter [04700, 05004], lr: 0.099482, loss: 3.0690
2022-07-13 15:25:33 - train: epoch 0014, iter [04800, 05004], lr: 0.099480, loss: 3.1589
2022-07-13 15:26:07 - train: epoch 0014, iter [04900, 05004], lr: 0.099478, loss: 2.8200
2022-07-13 15:26:41 - train: epoch 0014, iter [05000, 05004], lr: 0.099475, loss: 2.9169
2022-07-13 15:26:42 - train: epoch 014, train_loss: 3.0968
2022-07-13 15:27:57 - eval: epoch: 014, acc1: 48.972%, acc5: 75.040%, test_loss: 2.2277, per_image_load_time: 1.879ms, per_image_inference_time: 0.473ms
2022-07-13 15:27:57 - until epoch: 014, best_acc1: 52.320%
2022-07-13 15:27:57 - epoch 015 lr: 0.099475
2022-07-13 15:28:37 - train: epoch 0015, iter [00100, 05004], lr: 0.099473, loss: 2.8533
2022-07-13 15:29:12 - train: epoch 0015, iter [00200, 05004], lr: 0.099471, loss: 3.1254
2022-07-13 15:29:46 - train: epoch 0015, iter [00300, 05004], lr: 0.099468, loss: 3.2343
2022-07-13 15:30:20 - train: epoch 0015, iter [00400, 05004], lr: 0.099466, loss: 3.0293
2022-07-13 15:30:54 - train: epoch 0015, iter [00500, 05004], lr: 0.099464, loss: 3.1657
2022-07-13 15:31:27 - train: epoch 0015, iter [00600, 05004], lr: 0.099461, loss: 3.0876
2022-07-13 15:32:02 - train: epoch 0015, iter [00700, 05004], lr: 0.099459, loss: 3.1999
2022-07-13 15:32:36 - train: epoch 0015, iter [00800, 05004], lr: 0.099457, loss: 2.8383
2022-07-13 15:33:11 - train: epoch 0015, iter [00900, 05004], lr: 0.099454, loss: 3.1214
2022-07-13 15:33:45 - train: epoch 0015, iter [01000, 05004], lr: 0.099452, loss: 3.2621
2022-07-13 15:34:20 - train: epoch 0015, iter [01100, 05004], lr: 0.099449, loss: 2.9617
2022-07-13 15:34:54 - train: epoch 0015, iter [01200, 05004], lr: 0.099447, loss: 3.0688
2022-07-13 15:35:29 - train: epoch 0015, iter [01300, 05004], lr: 0.099445, loss: 3.3026
2022-07-13 15:36:04 - train: epoch 0015, iter [01400, 05004], lr: 0.099442, loss: 2.9894
2022-07-13 15:36:38 - train: epoch 0015, iter [01500, 05004], lr: 0.099440, loss: 2.6336
2022-07-13 15:37:13 - train: epoch 0015, iter [01600, 05004], lr: 0.099437, loss: 3.0881
2022-07-13 15:37:46 - train: epoch 0015, iter [01700, 05004], lr: 0.099435, loss: 3.2561
2022-07-13 15:38:22 - train: epoch 0015, iter [01800, 05004], lr: 0.099433, loss: 2.9738
2022-07-13 15:38:56 - train: epoch 0015, iter [01900, 05004], lr: 0.099430, loss: 2.9669
2022-07-13 15:39:31 - train: epoch 0015, iter [02000, 05004], lr: 0.099428, loss: 3.1928
2022-07-13 15:40:05 - train: epoch 0015, iter [02100, 05004], lr: 0.099425, loss: 2.9720
2022-07-13 15:40:40 - train: epoch 0015, iter [02200, 05004], lr: 0.099423, loss: 3.1806
2022-07-13 15:41:14 - train: epoch 0015, iter [02300, 05004], lr: 0.099420, loss: 2.9803
2022-07-13 15:41:49 - train: epoch 0015, iter [02400, 05004], lr: 0.099418, loss: 3.2889
2022-07-13 15:42:23 - train: epoch 0015, iter [02500, 05004], lr: 0.099416, loss: 2.9719
2022-07-13 15:42:58 - train: epoch 0015, iter [02600, 05004], lr: 0.099413, loss: 2.8515
2022-07-13 15:43:33 - train: epoch 0015, iter [02700, 05004], lr: 0.099411, loss: 3.1421
2022-07-13 15:44:08 - train: epoch 0015, iter [02800, 05004], lr: 0.099408, loss: 3.5775
2022-07-13 15:44:42 - train: epoch 0015, iter [02900, 05004], lr: 0.099406, loss: 2.8770
2022-07-13 15:45:17 - train: epoch 0015, iter [03000, 05004], lr: 0.099403, loss: 2.9750
2022-07-13 15:45:52 - train: epoch 0015, iter [03100, 05004], lr: 0.099401, loss: 3.0707
2022-07-13 15:46:26 - train: epoch 0015, iter [03200, 05004], lr: 0.099398, loss: 2.8277
2022-07-13 15:47:02 - train: epoch 0015, iter [03300, 05004], lr: 0.099396, loss: 2.9059
2022-07-13 15:47:36 - train: epoch 0015, iter [03400, 05004], lr: 0.099393, loss: 3.0992
2022-07-13 15:48:11 - train: epoch 0015, iter [03500, 05004], lr: 0.099391, loss: 3.0172
2022-07-13 15:48:45 - train: epoch 0015, iter [03600, 05004], lr: 0.099388, loss: 3.0186
2022-07-13 15:49:20 - train: epoch 0015, iter [03700, 05004], lr: 0.099386, loss: 3.0830
2022-07-13 15:49:55 - train: epoch 0015, iter [03800, 05004], lr: 0.099383, loss: 3.2234
2022-07-13 15:50:30 - train: epoch 0015, iter [03900, 05004], lr: 0.099381, loss: 3.3201
2022-07-13 15:51:04 - train: epoch 0015, iter [04000, 05004], lr: 0.099378, loss: 3.1970
2022-07-13 15:51:40 - train: epoch 0015, iter [04100, 05004], lr: 0.099376, loss: 3.4267
2022-07-13 15:52:13 - train: epoch 0015, iter [04200, 05004], lr: 0.099373, loss: 2.6888
2022-07-13 15:52:49 - train: epoch 0015, iter [04300, 05004], lr: 0.099371, loss: 3.1555
2022-07-13 15:53:23 - train: epoch 0015, iter [04400, 05004], lr: 0.099368, loss: 3.0053
2022-07-13 15:53:57 - train: epoch 0015, iter [04500, 05004], lr: 0.099365, loss: 3.0929
2022-07-13 15:54:32 - train: epoch 0015, iter [04600, 05004], lr: 0.099363, loss: 3.1450
2022-07-13 15:55:05 - train: epoch 0015, iter [04700, 05004], lr: 0.099360, loss: 3.0741
2022-07-13 15:55:40 - train: epoch 0015, iter [04800, 05004], lr: 0.099358, loss: 3.2024
2022-07-13 15:56:14 - train: epoch 0015, iter [04900, 05004], lr: 0.099355, loss: 2.8223
2022-07-13 15:56:47 - train: epoch 0015, iter [05000, 05004], lr: 0.099353, loss: 2.9231
2022-07-13 15:56:48 - train: epoch 015, train_loss: 3.0753
2022-07-13 15:58:02 - eval: epoch: 015, acc1: 52.410%, acc5: 78.052%, test_loss: 2.0393, per_image_load_time: 1.752ms, per_image_inference_time: 0.478ms
2022-07-13 15:58:03 - until epoch: 015, best_acc1: 52.410%
2022-07-13 15:58:03 - epoch 016 lr: 0.099352
2022-07-13 15:58:41 - train: epoch 0016, iter [00100, 05004], lr: 0.099350, loss: 2.8446
2022-07-13 15:59:15 - train: epoch 0016, iter [00200, 05004], lr: 0.099347, loss: 2.9967
2022-07-13 15:59:50 - train: epoch 0016, iter [00300, 05004], lr: 0.099345, loss: 2.9971
2022-07-13 16:00:23 - train: epoch 0016, iter [00400, 05004], lr: 0.099342, loss: 3.2154
2022-07-13 16:00:58 - train: epoch 0016, iter [00500, 05004], lr: 0.099340, loss: 2.9157
2022-07-13 16:01:32 - train: epoch 0016, iter [00600, 05004], lr: 0.099337, loss: 3.1538
2022-07-13 16:02:06 - train: epoch 0016, iter [00700, 05004], lr: 0.099334, loss: 2.9767
2022-07-13 16:02:41 - train: epoch 0016, iter [00800, 05004], lr: 0.099332, loss: 3.0910
2022-07-13 16:03:14 - train: epoch 0016, iter [00900, 05004], lr: 0.099329, loss: 2.9359
2022-07-13 16:03:48 - train: epoch 0016, iter [01000, 05004], lr: 0.099326, loss: 2.9857
2022-07-13 16:04:22 - train: epoch 0016, iter [01100, 05004], lr: 0.099324, loss: 2.8946
2022-07-13 16:04:56 - train: epoch 0016, iter [01200, 05004], lr: 0.099321, loss: 3.1679
2022-07-13 16:05:30 - train: epoch 0016, iter [01300, 05004], lr: 0.099319, loss: 3.0967
2022-07-13 16:06:04 - train: epoch 0016, iter [01400, 05004], lr: 0.099316, loss: 2.9460
2022-07-13 16:06:38 - train: epoch 0016, iter [01500, 05004], lr: 0.099313, loss: 3.1395
2022-07-13 16:07:13 - train: epoch 0016, iter [01600, 05004], lr: 0.099311, loss: 3.0889
2022-07-13 16:07:46 - train: epoch 0016, iter [01700, 05004], lr: 0.099308, loss: 2.9587
2022-07-13 16:08:21 - train: epoch 0016, iter [01800, 05004], lr: 0.099305, loss: 3.0788
2022-07-13 16:08:55 - train: epoch 0016, iter [01900, 05004], lr: 0.099303, loss: 3.1407
2022-07-13 16:09:30 - train: epoch 0016, iter [02000, 05004], lr: 0.099300, loss: 2.8199
2022-07-13 16:10:04 - train: epoch 0016, iter [02100, 05004], lr: 0.099297, loss: 3.0956
2022-07-13 16:10:39 - train: epoch 0016, iter [02200, 05004], lr: 0.099294, loss: 3.0330
2022-07-13 16:11:12 - train: epoch 0016, iter [02300, 05004], lr: 0.099292, loss: 3.3007
2022-07-13 16:11:47 - train: epoch 0016, iter [02400, 05004], lr: 0.099289, loss: 3.3185
2022-07-13 16:12:20 - train: epoch 0016, iter [02500, 05004], lr: 0.099286, loss: 3.0762
2022-07-13 16:12:54 - train: epoch 0016, iter [02600, 05004], lr: 0.099284, loss: 3.2766
2022-07-13 16:13:28 - train: epoch 0016, iter [02700, 05004], lr: 0.099281, loss: 3.0713
2022-07-13 16:14:02 - train: epoch 0016, iter [02800, 05004], lr: 0.099278, loss: 3.0759
2022-07-13 16:14:36 - train: epoch 0016, iter [02900, 05004], lr: 0.099275, loss: 3.0770
2022-07-13 16:15:10 - train: epoch 0016, iter [03000, 05004], lr: 0.099273, loss: 3.3794
2022-07-13 16:15:44 - train: epoch 0016, iter [03100, 05004], lr: 0.099270, loss: 3.1872
2022-07-13 16:16:19 - train: epoch 0016, iter [03200, 05004], lr: 0.099267, loss: 3.3517
2022-07-13 16:16:53 - train: epoch 0016, iter [03300, 05004], lr: 0.099265, loss: 2.9997
2022-07-13 16:17:28 - train: epoch 0016, iter [03400, 05004], lr: 0.099262, loss: 2.9418
2022-07-13 16:18:01 - train: epoch 0016, iter [03500, 05004], lr: 0.099259, loss: 3.1537
2022-07-13 16:18:36 - train: epoch 0016, iter [03600, 05004], lr: 0.099256, loss: 3.0040
2022-07-13 16:19:11 - train: epoch 0016, iter [03700, 05004], lr: 0.099253, loss: 3.2245
2022-07-13 16:19:44 - train: epoch 0016, iter [03800, 05004], lr: 0.099251, loss: 3.3419
2022-07-13 16:20:19 - train: epoch 0016, iter [03900, 05004], lr: 0.099248, loss: 3.2232
2022-07-13 16:20:52 - train: epoch 0016, iter [04000, 05004], lr: 0.099245, loss: 2.9500
2022-07-13 16:21:26 - train: epoch 0016, iter [04100, 05004], lr: 0.099242, loss: 3.0285
2022-07-13 16:22:00 - train: epoch 0016, iter [04200, 05004], lr: 0.099240, loss: 2.9964
2022-07-13 16:22:35 - train: epoch 0016, iter [04300, 05004], lr: 0.099237, loss: 2.9804
2022-07-13 16:23:09 - train: epoch 0016, iter [04400, 05004], lr: 0.099234, loss: 3.2140
2022-07-13 16:23:42 - train: epoch 0016, iter [04500, 05004], lr: 0.099231, loss: 3.2317
2022-07-13 16:24:17 - train: epoch 0016, iter [04600, 05004], lr: 0.099228, loss: 3.0170
2022-07-13 16:24:50 - train: epoch 0016, iter [04700, 05004], lr: 0.099226, loss: 3.1973
2022-07-13 16:25:25 - train: epoch 0016, iter [04800, 05004], lr: 0.099223, loss: 3.0191
2022-07-13 16:25:59 - train: epoch 0016, iter [04900, 05004], lr: 0.099220, loss: 2.9512
2022-07-13 16:26:32 - train: epoch 0016, iter [05000, 05004], lr: 0.099217, loss: 3.0685
2022-07-13 16:26:33 - train: epoch 016, train_loss: 3.0571
2022-07-13 16:27:47 - eval: epoch: 016, acc1: 51.390%, acc5: 77.086%, test_loss: 2.0892, per_image_load_time: 2.402ms, per_image_inference_time: 0.479ms
2022-07-13 16:27:48 - until epoch: 016, best_acc1: 52.410%
2022-07-13 16:27:48 - epoch 017 lr: 0.099217
2022-07-13 16:28:26 - train: epoch 0017, iter [00100, 05004], lr: 0.099214, loss: 3.0041
2022-07-13 16:29:00 - train: epoch 0017, iter [00200, 05004], lr: 0.099211, loss: 3.1708
2022-07-13 16:29:33 - train: epoch 0017, iter [00300, 05004], lr: 0.099208, loss: 3.2435
2022-07-13 16:30:07 - train: epoch 0017, iter [00400, 05004], lr: 0.099206, loss: 2.9624
2022-07-13 16:30:41 - train: epoch 0017, iter [00500, 05004], lr: 0.099203, loss: 2.9771
2022-07-13 16:31:15 - train: epoch 0017, iter [00600, 05004], lr: 0.099200, loss: 3.4659
2022-07-13 16:31:48 - train: epoch 0017, iter [00700, 05004], lr: 0.099197, loss: 3.2703
2022-07-13 16:32:21 - train: epoch 0017, iter [00800, 05004], lr: 0.099194, loss: 2.9437
2022-07-13 16:32:54 - train: epoch 0017, iter [00900, 05004], lr: 0.099191, loss: 3.1780
2022-07-13 16:33:28 - train: epoch 0017, iter [01000, 05004], lr: 0.099188, loss: 2.9459
2022-07-13 16:34:01 - train: epoch 0017, iter [01100, 05004], lr: 0.099185, loss: 3.2363
2022-07-13 16:34:34 - train: epoch 0017, iter [01200, 05004], lr: 0.099182, loss: 3.3166
2022-07-13 16:35:08 - train: epoch 0017, iter [01300, 05004], lr: 0.099180, loss: 3.0617
2022-07-13 16:35:41 - train: epoch 0017, iter [01400, 05004], lr: 0.099177, loss: 3.0314
2022-07-13 16:36:15 - train: epoch 0017, iter [01500, 05004], lr: 0.099174, loss: 2.6833
2022-07-13 16:36:48 - train: epoch 0017, iter [01600, 05004], lr: 0.099171, loss: 2.6618
2022-07-13 16:37:23 - train: epoch 0017, iter [01700, 05004], lr: 0.099168, loss: 3.1038
2022-07-13 16:37:56 - train: epoch 0017, iter [01800, 05004], lr: 0.099165, loss: 3.4901
2022-07-13 16:38:30 - train: epoch 0017, iter [01900, 05004], lr: 0.099162, loss: 2.6640
2022-07-13 16:39:02 - train: epoch 0017, iter [02000, 05004], lr: 0.099159, loss: 3.3095
2022-07-13 16:39:36 - train: epoch 0017, iter [02100, 05004], lr: 0.099156, loss: 2.9471
2022-07-13 16:40:10 - train: epoch 0017, iter [02200, 05004], lr: 0.099153, loss: 2.8418
2022-07-13 16:40:44 - train: epoch 0017, iter [02300, 05004], lr: 0.099150, loss: 2.8263
2022-07-13 16:41:17 - train: epoch 0017, iter [02400, 05004], lr: 0.099147, loss: 2.8243
2022-07-13 16:41:50 - train: epoch 0017, iter [02500, 05004], lr: 0.099144, loss: 3.2577
2022-07-13 16:42:24 - train: epoch 0017, iter [02600, 05004], lr: 0.099141, loss: 2.9048
2022-07-13 16:42:59 - train: epoch 0017, iter [02700, 05004], lr: 0.099138, loss: 3.0386
2022-07-13 16:43:32 - train: epoch 0017, iter [02800, 05004], lr: 0.099135, loss: 3.1379
2022-07-13 16:44:06 - train: epoch 0017, iter [02900, 05004], lr: 0.099132, loss: 3.2115
2022-07-13 16:44:40 - train: epoch 0017, iter [03000, 05004], lr: 0.099129, loss: 2.7633
2022-07-13 16:45:13 - train: epoch 0017, iter [03100, 05004], lr: 0.099126, loss: 3.1782
2022-07-13 16:45:47 - train: epoch 0017, iter [03200, 05004], lr: 0.099123, loss: 2.8855
2022-07-13 16:46:21 - train: epoch 0017, iter [03300, 05004], lr: 0.099120, loss: 3.1724
2022-07-13 16:46:55 - train: epoch 0017, iter [03400, 05004], lr: 0.099117, loss: 2.7328
2022-07-13 16:47:29 - train: epoch 0017, iter [03500, 05004], lr: 0.099114, loss: 2.8450
2022-07-13 16:48:03 - train: epoch 0017, iter [03600, 05004], lr: 0.099111, loss: 3.3554
2022-07-13 16:48:37 - train: epoch 0017, iter [03700, 05004], lr: 0.099108, loss: 2.9122
2022-07-13 16:49:11 - train: epoch 0017, iter [03800, 05004], lr: 0.099105, loss: 3.1932
2022-07-13 16:49:45 - train: epoch 0017, iter [03900, 05004], lr: 0.099102, loss: 2.8509
2022-07-13 16:50:19 - train: epoch 0017, iter [04000, 05004], lr: 0.099099, loss: 2.9842
2022-07-13 16:50:53 - train: epoch 0017, iter [04100, 05004], lr: 0.099096, loss: 3.0994
2022-07-13 16:51:28 - train: epoch 0017, iter [04200, 05004], lr: 0.099093, loss: 3.0919
2022-07-13 16:52:02 - train: epoch 0017, iter [04300, 05004], lr: 0.099090, loss: 3.0256
2022-07-13 16:52:35 - train: epoch 0017, iter [04400, 05004], lr: 0.099087, loss: 3.0816
2022-07-13 16:53:09 - train: epoch 0017, iter [04500, 05004], lr: 0.099084, loss: 2.9287
2022-07-13 16:53:43 - train: epoch 0017, iter [04600, 05004], lr: 0.099081, loss: 3.0082
2022-07-13 16:54:18 - train: epoch 0017, iter [04700, 05004], lr: 0.099078, loss: 3.0439
2022-07-13 16:54:52 - train: epoch 0017, iter [04800, 05004], lr: 0.099075, loss: 3.0990
2022-07-13 16:55:26 - train: epoch 0017, iter [04900, 05004], lr: 0.099072, loss: 2.8128
2022-07-13 16:55:58 - train: epoch 0017, iter [05000, 05004], lr: 0.099069, loss: 2.7842
2022-07-13 16:55:59 - train: epoch 017, train_loss: 3.0435
2022-07-13 16:57:12 - eval: epoch: 017, acc1: 50.188%, acc5: 76.038%, test_loss: 2.1549, per_image_load_time: 1.460ms, per_image_inference_time: 0.500ms
2022-07-13 16:57:13 - until epoch: 017, best_acc1: 52.410%
2022-07-13 16:57:13 - epoch 018 lr: 0.099068
2022-07-13 16:57:52 - train: epoch 0018, iter [00100, 05004], lr: 0.099065, loss: 3.0432
2022-07-13 16:58:26 - train: epoch 0018, iter [00200, 05004], lr: 0.099062, loss: 3.0247
2022-07-13 16:59:00 - train: epoch 0018, iter [00300, 05004], lr: 0.099059, loss: 3.2349
2022-07-13 16:59:34 - train: epoch 0018, iter [00400, 05004], lr: 0.099056, loss: 2.9762
2022-07-13 17:00:08 - train: epoch 0018, iter [00500, 05004], lr: 0.099053, loss: 2.7503
2022-07-13 17:00:43 - train: epoch 0018, iter [00600, 05004], lr: 0.099050, loss: 2.9556
2022-07-13 17:01:17 - train: epoch 0018, iter [00700, 05004], lr: 0.099047, loss: 2.7612
2022-07-13 17:01:50 - train: epoch 0018, iter [00800, 05004], lr: 0.099044, loss: 2.9651
2022-07-13 17:02:25 - train: epoch 0018, iter [00900, 05004], lr: 0.099040, loss: 3.3552
2022-07-13 17:02:59 - train: epoch 0018, iter [01000, 05004], lr: 0.099037, loss: 2.9226
2022-07-13 17:03:33 - train: epoch 0018, iter [01100, 05004], lr: 0.099034, loss: 3.2437
2022-07-13 17:04:07 - train: epoch 0018, iter [01200, 05004], lr: 0.099031, loss: 3.0072
2022-07-13 17:04:41 - train: epoch 0018, iter [01300, 05004], lr: 0.099028, loss: 3.1652
2022-07-13 17:05:15 - train: epoch 0018, iter [01400, 05004], lr: 0.099025, loss: 3.0206
2022-07-13 17:05:50 - train: epoch 0018, iter [01500, 05004], lr: 0.099022, loss: 3.2150
2022-07-13 17:06:22 - train: epoch 0018, iter [01600, 05004], lr: 0.099018, loss: 3.1435
2022-07-13 17:06:57 - train: epoch 0018, iter [01700, 05004], lr: 0.099015, loss: 3.0269
2022-07-13 17:07:31 - train: epoch 0018, iter [01800, 05004], lr: 0.099012, loss: 2.9089
2022-07-13 17:08:06 - train: epoch 0018, iter [01900, 05004], lr: 0.099009, loss: 2.9751
2022-07-13 17:08:39 - train: epoch 0018, iter [02000, 05004], lr: 0.099006, loss: 3.1066
2022-07-13 17:09:14 - train: epoch 0018, iter [02100, 05004], lr: 0.099002, loss: 2.9843
2022-07-13 17:09:48 - train: epoch 0018, iter [02200, 05004], lr: 0.098999, loss: 2.9778
2022-07-13 17:10:23 - train: epoch 0018, iter [02300, 05004], lr: 0.098996, loss: 2.9163
2022-07-13 17:10:57 - train: epoch 0018, iter [02400, 05004], lr: 0.098993, loss: 2.8513
2022-07-13 17:11:31 - train: epoch 0018, iter [02500, 05004], lr: 0.098990, loss: 2.9900
2022-07-13 17:12:04 - train: epoch 0018, iter [02600, 05004], lr: 0.098986, loss: 2.6894
2022-07-13 17:12:38 - train: epoch 0018, iter [02700, 05004], lr: 0.098983, loss: 3.1608
2022-07-13 17:13:13 - train: epoch 0018, iter [02800, 05004], lr: 0.098980, loss: 2.9379
2022-07-13 17:13:46 - train: epoch 0018, iter [02900, 05004], lr: 0.098977, loss: 3.0478
2022-07-13 17:14:21 - train: epoch 0018, iter [03000, 05004], lr: 0.098973, loss: 3.0343
2022-07-13 17:14:55 - train: epoch 0018, iter [03100, 05004], lr: 0.098970, loss: 3.0850
2022-07-13 17:15:29 - train: epoch 0018, iter [03200, 05004], lr: 0.098967, loss: 2.8911
2022-07-13 17:16:02 - train: epoch 0018, iter [03300, 05004], lr: 0.098964, loss: 2.9142
2022-07-13 17:16:35 - train: epoch 0018, iter [03400, 05004], lr: 0.098960, loss: 3.0322
2022-07-13 17:17:09 - train: epoch 0018, iter [03500, 05004], lr: 0.098957, loss: 3.1199
2022-07-13 17:17:42 - train: epoch 0018, iter [03600, 05004], lr: 0.098954, loss: 3.1593
2022-07-13 17:18:15 - train: epoch 0018, iter [03700, 05004], lr: 0.098951, loss: 3.3162
2022-07-13 17:18:48 - train: epoch 0018, iter [03800, 05004], lr: 0.098947, loss: 3.1358
2022-07-13 17:19:22 - train: epoch 0018, iter [03900, 05004], lr: 0.098944, loss: 2.9393
2022-07-13 17:19:55 - train: epoch 0018, iter [04000, 05004], lr: 0.098941, loss: 3.0272
2022-07-13 17:20:29 - train: epoch 0018, iter [04100, 05004], lr: 0.098937, loss: 3.0837
2022-07-13 17:21:03 - train: epoch 0018, iter [04200, 05004], lr: 0.098934, loss: 2.8867
2022-07-13 17:21:36 - train: epoch 0018, iter [04300, 05004], lr: 0.098931, loss: 2.6514
2022-07-13 17:22:10 - train: epoch 0018, iter [04400, 05004], lr: 0.098928, loss: 3.0862
2022-07-13 17:22:44 - train: epoch 0018, iter [04500, 05004], lr: 0.098924, loss: 3.2940
2022-07-13 17:23:17 - train: epoch 0018, iter [04600, 05004], lr: 0.098921, loss: 3.0453
2022-07-13 17:23:52 - train: epoch 0018, iter [04700, 05004], lr: 0.098918, loss: 3.3094
2022-07-13 17:24:25 - train: epoch 0018, iter [04800, 05004], lr: 0.098914, loss: 3.1828
2022-07-13 17:24:59 - train: epoch 0018, iter [04900, 05004], lr: 0.098911, loss: 3.1618
2022-07-13 17:25:32 - train: epoch 0018, iter [05000, 05004], lr: 0.098908, loss: 3.2076
2022-07-13 17:25:33 - train: epoch 018, train_loss: 3.0274
2022-07-13 17:26:47 - eval: epoch: 018, acc1: 51.820%, acc5: 77.108%, test_loss: 2.0908, per_image_load_time: 2.336ms, per_image_inference_time: 0.485ms
2022-07-13 17:26:47 - until epoch: 018, best_acc1: 52.410%
2022-07-13 17:26:47 - epoch 019 lr: 0.098907
2022-07-13 17:27:27 - train: epoch 0019, iter [00100, 05004], lr: 0.098904, loss: 2.5034
2022-07-13 17:28:00 - train: epoch 0019, iter [00200, 05004], lr: 0.098901, loss: 3.1808
2022-07-13 17:28:34 - train: epoch 0019, iter [00300, 05004], lr: 0.098897, loss: 3.2841
2022-07-13 17:29:08 - train: epoch 0019, iter [00400, 05004], lr: 0.098894, loss: 2.8440
2022-07-13 17:29:43 - train: epoch 0019, iter [00500, 05004], lr: 0.098891, loss: 3.0899
2022-07-13 17:30:17 - train: epoch 0019, iter [00600, 05004], lr: 0.098887, loss: 3.0057
2022-07-13 17:30:52 - train: epoch 0019, iter [00700, 05004], lr: 0.098884, loss: 2.9145
2022-07-13 17:31:26 - train: epoch 0019, iter [00800, 05004], lr: 0.098880, loss: 3.1386
2022-07-13 17:32:00 - train: epoch 0019, iter [00900, 05004], lr: 0.098877, loss: 2.9798
2022-07-13 17:32:34 - train: epoch 0019, iter [01000, 05004], lr: 0.098874, loss: 3.2381
2022-07-13 17:33:09 - train: epoch 0019, iter [01100, 05004], lr: 0.098870, loss: 2.9478
2022-07-13 17:33:42 - train: epoch 0019, iter [01200, 05004], lr: 0.098867, loss: 3.1408
2022-07-13 17:34:17 - train: epoch 0019, iter [01300, 05004], lr: 0.098863, loss: 3.0586
2022-07-13 17:34:51 - train: epoch 0019, iter [01400, 05004], lr: 0.098860, loss: 2.9869
2022-07-13 17:35:26 - train: epoch 0019, iter [01500, 05004], lr: 0.098857, loss: 3.3378
2022-07-13 17:36:00 - train: epoch 0019, iter [01600, 05004], lr: 0.098853, loss: 3.0057
2022-07-13 17:36:35 - train: epoch 0019, iter [01700, 05004], lr: 0.098850, loss: 3.0857
2022-07-13 17:37:09 - train: epoch 0019, iter [01800, 05004], lr: 0.098846, loss: 2.9765
2022-07-13 17:37:43 - train: epoch 0019, iter [01900, 05004], lr: 0.098843, loss: 3.0781
2022-07-13 17:38:18 - train: epoch 0019, iter [02000, 05004], lr: 0.098839, loss: 2.9557
2022-07-13 17:38:53 - train: epoch 0019, iter [02100, 05004], lr: 0.098836, loss: 2.6639
2022-07-13 17:39:27 - train: epoch 0019, iter [02200, 05004], lr: 0.098833, loss: 3.0488
2022-07-13 17:40:01 - train: epoch 0019, iter [02300, 05004], lr: 0.098829, loss: 3.1467
2022-07-13 17:40:36 - train: epoch 0019, iter [02400, 05004], lr: 0.098826, loss: 3.0587
2022-07-13 17:41:10 - train: epoch 0019, iter [02500, 05004], lr: 0.098822, loss: 2.7527
2022-07-13 17:41:45 - train: epoch 0019, iter [02600, 05004], lr: 0.098819, loss: 3.0228
2022-07-13 17:42:19 - train: epoch 0019, iter [02700, 05004], lr: 0.098815, loss: 2.9103
2022-07-13 17:42:54 - train: epoch 0019, iter [02800, 05004], lr: 0.098812, loss: 2.9327
2022-07-13 17:43:28 - train: epoch 0019, iter [02900, 05004], lr: 0.098808, loss: 2.9870
2022-07-13 17:44:02 - train: epoch 0019, iter [03000, 05004], lr: 0.098805, loss: 3.1212
2022-07-13 17:44:37 - train: epoch 0019, iter [03100, 05004], lr: 0.098801, loss: 2.9944
2022-07-13 17:45:11 - train: epoch 0019, iter [03200, 05004], lr: 0.098798, loss: 2.8661
2022-07-13 17:45:46 - train: epoch 0019, iter [03300, 05004], lr: 0.098794, loss: 2.8634
2022-07-13 17:46:20 - train: epoch 0019, iter [03400, 05004], lr: 0.098791, loss: 3.0870
2022-07-13 17:46:53 - train: epoch 0019, iter [03500, 05004], lr: 0.098787, loss: 3.2396
2022-07-13 17:47:28 - train: epoch 0019, iter [03600, 05004], lr: 0.098784, loss: 2.9864
2022-07-13 17:48:03 - train: epoch 0019, iter [03700, 05004], lr: 0.098780, loss: 3.1927
2022-07-13 17:48:38 - train: epoch 0019, iter [03800, 05004], lr: 0.098777, loss: 3.1283
2022-07-13 17:49:12 - train: epoch 0019, iter [03900, 05004], lr: 0.098773, loss: 2.9452
2022-07-13 17:49:47 - train: epoch 0019, iter [04000, 05004], lr: 0.098769, loss: 2.9879
2022-07-13 17:50:20 - train: epoch 0019, iter [04100, 05004], lr: 0.098766, loss: 2.9733
2022-07-13 17:50:55 - train: epoch 0019, iter [04200, 05004], lr: 0.098762, loss: 2.9713
2022-07-13 17:51:30 - train: epoch 0019, iter [04300, 05004], lr: 0.098759, loss: 2.8116
2022-07-13 17:52:04 - train: epoch 0019, iter [04400, 05004], lr: 0.098755, loss: 3.1836
2022-07-13 17:52:39 - train: epoch 0019, iter [04500, 05004], lr: 0.098752, loss: 3.2358
2022-07-13 17:53:13 - train: epoch 0019, iter [04600, 05004], lr: 0.098748, loss: 3.1150
2022-07-13 17:53:47 - train: epoch 0019, iter [04700, 05004], lr: 0.098744, loss: 2.9189
2022-07-13 17:54:22 - train: epoch 0019, iter [04800, 05004], lr: 0.098741, loss: 2.9277
2022-07-13 17:54:56 - train: epoch 0019, iter [04900, 05004], lr: 0.098737, loss: 2.8454
2022-07-13 17:55:29 - train: epoch 0019, iter [05000, 05004], lr: 0.098734, loss: 2.9804
2022-07-13 17:55:30 - train: epoch 019, train_loss: 3.0194
2022-07-13 17:56:43 - eval: epoch: 019, acc1: 54.738%, acc5: 79.516%, test_loss: 1.9382, per_image_load_time: 2.375ms, per_image_inference_time: 0.477ms
2022-07-13 17:56:43 - until epoch: 019, best_acc1: 54.738%
2022-07-13 17:56:43 - epoch 020 lr: 0.098734
2022-07-13 17:57:22 - train: epoch 0020, iter [00100, 05004], lr: 0.098730, loss: 2.9804
2022-07-13 17:57:56 - train: epoch 0020, iter [00200, 05004], lr: 0.098726, loss: 2.8082
2022-07-13 17:58:30 - train: epoch 0020, iter [00300, 05004], lr: 0.098723, loss: 3.0216
2022-07-13 17:59:03 - train: epoch 0020, iter [00400, 05004], lr: 0.098719, loss: 2.9317
2022-07-13 17:59:37 - train: epoch 0020, iter [00500, 05004], lr: 0.098715, loss: 2.8249
2022-07-13 18:00:11 - train: epoch 0020, iter [00600, 05004], lr: 0.098712, loss: 3.3074
2022-07-13 18:00:46 - train: epoch 0020, iter [00700, 05004], lr: 0.098708, loss: 2.7763
2022-07-13 18:01:20 - train: epoch 0020, iter [00800, 05004], lr: 0.098705, loss: 3.2914
2022-07-13 18:01:53 - train: epoch 0020, iter [00900, 05004], lr: 0.098701, loss: 3.3606
2022-07-13 18:02:28 - train: epoch 0020, iter [01000, 05004], lr: 0.098697, loss: 2.9876
2022-07-13 18:03:01 - train: epoch 0020, iter [01100, 05004], lr: 0.098694, loss: 3.1564
2022-07-13 18:03:35 - train: epoch 0020, iter [01200, 05004], lr: 0.098690, loss: 2.8363
2022-07-13 18:04:10 - train: epoch 0020, iter [01300, 05004], lr: 0.098686, loss: 3.0508
2022-07-13 18:04:43 - train: epoch 0020, iter [01400, 05004], lr: 0.098683, loss: 2.8861
2022-07-13 18:05:17 - train: epoch 0020, iter [01500, 05004], lr: 0.098679, loss: 3.0181
2022-07-13 18:05:51 - train: epoch 0020, iter [01600, 05004], lr: 0.098675, loss: 2.9687
2022-07-13 18:06:25 - train: epoch 0020, iter [01700, 05004], lr: 0.098672, loss: 2.5617
2022-07-13 18:07:00 - train: epoch 0020, iter [01800, 05004], lr: 0.098668, loss: 2.9916
2022-07-13 18:07:33 - train: epoch 0020, iter [01900, 05004], lr: 0.098664, loss: 2.9993
2022-07-13 18:08:07 - train: epoch 0020, iter [02000, 05004], lr: 0.098661, loss: 3.1409
2022-07-13 18:08:41 - train: epoch 0020, iter [02100, 05004], lr: 0.098657, loss: 3.2011
2022-07-13 18:09:16 - train: epoch 0020, iter [02200, 05004], lr: 0.098653, loss: 2.9591
2022-07-13 18:09:50 - train: epoch 0020, iter [02300, 05004], lr: 0.098649, loss: 2.9001
2022-07-13 18:10:24 - train: epoch 0020, iter [02400, 05004], lr: 0.098646, loss: 3.0078
2022-07-13 18:10:58 - train: epoch 0020, iter [02500, 05004], lr: 0.098642, loss: 2.8031
2022-07-13 18:11:33 - train: epoch 0020, iter [02600, 05004], lr: 0.098638, loss: 2.9890
2022-07-13 18:12:07 - train: epoch 0020, iter [02700, 05004], lr: 0.098635, loss: 3.0358
2022-07-13 18:12:42 - train: epoch 0020, iter [02800, 05004], lr: 0.098631, loss: 3.1189
2022-07-13 18:13:16 - train: epoch 0020, iter [02900, 05004], lr: 0.098627, loss: 3.0041
2022-07-13 18:13:50 - train: epoch 0020, iter [03000, 05004], lr: 0.098623, loss: 2.9088
2022-07-13 18:14:24 - train: epoch 0020, iter [03100, 05004], lr: 0.098620, loss: 2.8989
2022-07-13 18:14:59 - train: epoch 0020, iter [03200, 05004], lr: 0.098616, loss: 3.2468
2022-07-13 18:15:34 - train: epoch 0020, iter [03300, 05004], lr: 0.098612, loss: 2.8830
2022-07-13 18:16:09 - train: epoch 0020, iter [03400, 05004], lr: 0.098608, loss: 3.0253
2022-07-13 18:16:43 - train: epoch 0020, iter [03500, 05004], lr: 0.098604, loss: 2.8851
2022-07-13 18:17:17 - train: epoch 0020, iter [03600, 05004], lr: 0.098601, loss: 3.0137
2022-07-13 18:17:51 - train: epoch 0020, iter [03700, 05004], lr: 0.098597, loss: 3.1323
2022-07-13 18:18:27 - train: epoch 0020, iter [03800, 05004], lr: 0.098593, loss: 3.0647
2022-07-13 18:19:01 - train: epoch 0020, iter [03900, 05004], lr: 0.098589, loss: 3.1143
2022-07-13 18:19:36 - train: epoch 0020, iter [04000, 05004], lr: 0.098586, loss: 2.9249
2022-07-13 18:20:10 - train: epoch 0020, iter [04100, 05004], lr: 0.098582, loss: 2.7747
2022-07-13 18:20:45 - train: epoch 0020, iter [04200, 05004], lr: 0.098578, loss: 3.0701
2022-07-13 18:21:19 - train: epoch 0020, iter [04300, 05004], lr: 0.098574, loss: 2.8546
2022-07-13 18:21:53 - train: epoch 0020, iter [04400, 05004], lr: 0.098570, loss: 3.0032
2022-07-13 18:22:28 - train: epoch 0020, iter [04500, 05004], lr: 0.098566, loss: 3.0246
2022-07-13 18:23:03 - train: epoch 0020, iter [04600, 05004], lr: 0.098563, loss: 2.9904
2022-07-13 18:23:38 - train: epoch 0020, iter [04700, 05004], lr: 0.098559, loss: 2.8746
2022-07-13 18:24:12 - train: epoch 0020, iter [04800, 05004], lr: 0.098555, loss: 2.7921
2022-07-13 18:24:47 - train: epoch 0020, iter [04900, 05004], lr: 0.098551, loss: 3.1329
2022-07-13 18:25:21 - train: epoch 0020, iter [05000, 05004], lr: 0.098547, loss: 2.8476
2022-07-13 18:25:22 - train: epoch 020, train_loss: 2.9986
2022-07-13 18:26:37 - eval: epoch: 020, acc1: 53.962%, acc5: 78.968%, test_loss: 1.9778, per_image_load_time: 2.406ms, per_image_inference_time: 0.474ms
2022-07-13 18:26:37 - until epoch: 020, best_acc1: 54.738%
2022-07-13 18:26:37 - epoch 021 lr: 0.098547
2022-07-13 18:27:16 - train: epoch 0021, iter [00100, 05004], lr: 0.098543, loss: 2.7111
2022-07-13 18:27:50 - train: epoch 0021, iter [00200, 05004], lr: 0.098539, loss: 3.1989
2022-07-13 18:28:24 - train: epoch 0021, iter [00300, 05004], lr: 0.098536, loss: 2.7317
2022-07-13 18:28:57 - train: epoch 0021, iter [00400, 05004], lr: 0.098532, loss: 3.1520
2022-07-13 18:29:31 - train: epoch 0021, iter [00500, 05004], lr: 0.098528, loss: 2.7965
2022-07-13 18:30:05 - train: epoch 0021, iter [00600, 05004], lr: 0.098524, loss: 2.9853
2022-07-13 18:30:39 - train: epoch 0021, iter [00700, 05004], lr: 0.098520, loss: 2.8931
2022-07-13 18:31:13 - train: epoch 0021, iter [00800, 05004], lr: 0.098516, loss: 3.0971
2022-07-13 18:31:46 - train: epoch 0021, iter [00900, 05004], lr: 0.098512, loss: 2.8227
2022-07-13 18:32:20 - train: epoch 0021, iter [01000, 05004], lr: 0.098508, loss: 2.8868
2022-07-13 18:32:55 - train: epoch 0021, iter [01100, 05004], lr: 0.098504, loss: 3.0889
2022-07-13 18:33:28 - train: epoch 0021, iter [01200, 05004], lr: 0.098500, loss: 2.8919
2022-07-13 18:34:01 - train: epoch 0021, iter [01300, 05004], lr: 0.098497, loss: 2.8568
2022-07-13 18:34:36 - train: epoch 0021, iter [01400, 05004], lr: 0.098493, loss: 3.0935
2022-07-13 18:35:10 - train: epoch 0021, iter [01500, 05004], lr: 0.098489, loss: 2.9880
2022-07-13 18:35:44 - train: epoch 0021, iter [01600, 05004], lr: 0.098485, loss: 2.9662
2022-07-13 18:36:19 - train: epoch 0021, iter [01700, 05004], lr: 0.098481, loss: 3.1597
2022-07-13 18:36:52 - train: epoch 0021, iter [01800, 05004], lr: 0.098477, loss: 2.8450
2022-07-13 18:37:27 - train: epoch 0021, iter [01900, 05004], lr: 0.098473, loss: 3.0298
2022-07-13 18:38:01 - train: epoch 0021, iter [02000, 05004], lr: 0.098469, loss: 3.0811
2022-07-13 18:38:35 - train: epoch 0021, iter [02100, 05004], lr: 0.098465, loss: 2.7514
2022-07-13 18:39:09 - train: epoch 0021, iter [02200, 05004], lr: 0.098461, loss: 2.8526
2022-07-13 18:39:43 - train: epoch 0021, iter [02300, 05004], lr: 0.098457, loss: 2.7036
2022-07-13 18:40:17 - train: epoch 0021, iter [02400, 05004], lr: 0.098453, loss: 2.9708
2022-07-13 18:40:51 - train: epoch 0021, iter [02500, 05004], lr: 0.098449, loss: 3.0042
2022-07-13 18:41:26 - train: epoch 0021, iter [02600, 05004], lr: 0.098445, loss: 3.2881
2022-07-13 18:42:00 - train: epoch 0021, iter [02700, 05004], lr: 0.098441, loss: 2.8634
2022-07-13 18:42:34 - train: epoch 0021, iter [02800, 05004], lr: 0.098437, loss: 2.9881
2022-07-13 18:43:08 - train: epoch 0021, iter [02900, 05004], lr: 0.098433, loss: 3.2173
2022-07-13 18:43:42 - train: epoch 0021, iter [03000, 05004], lr: 0.098429, loss: 3.0676
2022-07-13 18:44:17 - train: epoch 0021, iter [03100, 05004], lr: 0.098425, loss: 3.1948
2022-07-13 18:44:51 - train: epoch 0021, iter [03200, 05004], lr: 0.098421, loss: 3.0927
2022-07-13 18:45:26 - train: epoch 0021, iter [03300, 05004], lr: 0.098417, loss: 3.2003
2022-07-13 18:46:00 - train: epoch 0021, iter [03400, 05004], lr: 0.098413, loss: 3.1405
2022-07-13 18:46:34 - train: epoch 0021, iter [03500, 05004], lr: 0.098409, loss: 2.9759
2022-07-13 18:47:09 - train: epoch 0021, iter [03600, 05004], lr: 0.098405, loss: 2.7966
2022-07-13 18:47:44 - train: epoch 0021, iter [03700, 05004], lr: 0.098401, loss: 3.0783
2022-07-13 18:48:18 - train: epoch 0021, iter [03800, 05004], lr: 0.098397, loss: 2.8147
2022-07-13 18:48:53 - train: epoch 0021, iter [03900, 05004], lr: 0.098393, loss: 2.9044
2022-07-13 18:49:27 - train: epoch 0021, iter [04000, 05004], lr: 0.098389, loss: 3.2019
2022-07-13 18:50:02 - train: epoch 0021, iter [04100, 05004], lr: 0.098385, loss: 2.8677
2022-07-13 18:50:36 - train: epoch 0021, iter [04200, 05004], lr: 0.098381, loss: 2.9560
2022-07-13 18:51:10 - train: epoch 0021, iter [04300, 05004], lr: 0.098377, loss: 3.0203
2022-07-13 18:51:45 - train: epoch 0021, iter [04400, 05004], lr: 0.098373, loss: 2.8176
2022-07-13 18:52:19 - train: epoch 0021, iter [04500, 05004], lr: 0.098369, loss: 2.9540
2022-07-13 18:52:55 - train: epoch 0021, iter [04600, 05004], lr: 0.098365, loss: 2.7988
2022-07-13 18:53:29 - train: epoch 0021, iter [04700, 05004], lr: 0.098360, loss: 3.1507
2022-07-13 18:54:04 - train: epoch 0021, iter [04800, 05004], lr: 0.098356, loss: 3.1257
2022-07-13 18:54:39 - train: epoch 0021, iter [04900, 05004], lr: 0.098352, loss: 3.0937
2022-07-13 18:55:12 - train: epoch 0021, iter [05000, 05004], lr: 0.098348, loss: 3.0417
2022-07-13 18:55:13 - train: epoch 021, train_loss: 2.9914
2022-07-13 18:56:29 - eval: epoch: 021, acc1: 50.800%, acc5: 76.620%, test_loss: 2.1204, per_image_load_time: 2.442ms, per_image_inference_time: 0.473ms
2022-07-13 18:56:29 - until epoch: 021, best_acc1: 54.738%
2022-07-13 18:56:29 - epoch 022 lr: 0.098348
2022-07-13 18:57:09 - train: epoch 0022, iter [00100, 05004], lr: 0.098344, loss: 2.6153
2022-07-13 18:57:42 - train: epoch 0022, iter [00200, 05004], lr: 0.098340, loss: 2.8731
2022-07-13 18:58:16 - train: epoch 0022, iter [00300, 05004], lr: 0.098336, loss: 2.6915
2022-07-13 18:58:51 - train: epoch 0022, iter [00400, 05004], lr: 0.098332, loss: 2.7997
2022-07-13 18:59:25 - train: epoch 0022, iter [00500, 05004], lr: 0.098327, loss: 2.9401
2022-07-13 18:59:59 - train: epoch 0022, iter [00600, 05004], lr: 0.098323, loss: 3.1432
2022-07-13 19:00:33 - train: epoch 0022, iter [00700, 05004], lr: 0.098319, loss: 3.0661
2022-07-13 19:01:07 - train: epoch 0022, iter [00800, 05004], lr: 0.098315, loss: 2.9933
2022-07-13 19:01:40 - train: epoch 0022, iter [00900, 05004], lr: 0.098311, loss: 3.0636
2022-07-13 19:02:14 - train: epoch 0022, iter [01000, 05004], lr: 0.098307, loss: 2.8559
2022-07-13 19:02:47 - train: epoch 0022, iter [01100, 05004], lr: 0.098303, loss: 3.0543
2022-07-13 19:03:21 - train: epoch 0022, iter [01200, 05004], lr: 0.098298, loss: 2.6225
2022-07-13 19:03:54 - train: epoch 0022, iter [01300, 05004], lr: 0.098294, loss: 2.9452
2022-07-13 19:04:28 - train: epoch 0022, iter [01400, 05004], lr: 0.098290, loss: 3.0220
2022-07-13 19:05:01 - train: epoch 0022, iter [01500, 05004], lr: 0.098286, loss: 3.0254
2022-07-13 19:05:35 - train: epoch 0022, iter [01600, 05004], lr: 0.098282, loss: 2.6832
2022-07-13 19:06:10 - train: epoch 0022, iter [01700, 05004], lr: 0.098278, loss: 2.4844
2022-07-13 19:06:43 - train: epoch 0022, iter [01800, 05004], lr: 0.098273, loss: 2.9755
2022-07-13 19:07:17 - train: epoch 0022, iter [01900, 05004], lr: 0.098269, loss: 2.7150
2022-07-13 19:07:50 - train: epoch 0022, iter [02000, 05004], lr: 0.098265, loss: 3.1422
2022-07-13 19:08:24 - train: epoch 0022, iter [02100, 05004], lr: 0.098261, loss: 3.0918
2022-07-13 19:08:58 - train: epoch 0022, iter [02200, 05004], lr: 0.098257, loss: 2.7535
2022-07-13 19:09:32 - train: epoch 0022, iter [02300, 05004], lr: 0.098252, loss: 3.0430
2022-07-13 19:10:06 - train: epoch 0022, iter [02400, 05004], lr: 0.098248, loss: 3.1155
2022-07-13 19:10:40 - train: epoch 0022, iter [02500, 05004], lr: 0.098244, loss: 3.0503
2022-07-13 19:11:15 - train: epoch 0022, iter [02600, 05004], lr: 0.098240, loss: 2.8279
2022-07-13 19:11:49 - train: epoch 0022, iter [02700, 05004], lr: 0.098235, loss: 2.9345
2022-07-13 19:12:22 - train: epoch 0022, iter [02800, 05004], lr: 0.098231, loss: 3.1032
2022-07-13 19:12:56 - train: epoch 0022, iter [02900, 05004], lr: 0.098227, loss: 2.8086
2022-07-13 19:13:30 - train: epoch 0022, iter [03000, 05004], lr: 0.098223, loss: 3.0930
2022-07-13 19:14:04 - train: epoch 0022, iter [03100, 05004], lr: 0.098218, loss: 3.0928
2022-07-13 19:14:38 - train: epoch 0022, iter [03200, 05004], lr: 0.098214, loss: 2.8183
2022-07-13 19:15:13 - train: epoch 0022, iter [03300, 05004], lr: 0.098210, loss: 3.0030
2022-07-13 19:15:47 - train: epoch 0022, iter [03400, 05004], lr: 0.098206, loss: 2.7992
2022-07-13 19:16:21 - train: epoch 0022, iter [03500, 05004], lr: 0.098201, loss: 2.9952
2022-07-13 19:16:55 - train: epoch 0022, iter [03600, 05004], lr: 0.098197, loss: 3.1919
2022-07-13 19:17:29 - train: epoch 0022, iter [03700, 05004], lr: 0.098193, loss: 3.0730
2022-07-13 19:18:03 - train: epoch 0022, iter [03800, 05004], lr: 0.098188, loss: 2.9622
2022-07-13 19:18:39 - train: epoch 0022, iter [03900, 05004], lr: 0.098184, loss: 2.7870
2022-07-13 19:19:13 - train: epoch 0022, iter [04000, 05004], lr: 0.098180, loss: 2.9673
2022-07-13 19:19:46 - train: epoch 0022, iter [04100, 05004], lr: 0.098176, loss: 2.8316
2022-07-13 19:20:22 - train: epoch 0022, iter [04200, 05004], lr: 0.098171, loss: 2.9476
2022-07-13 19:20:56 - train: epoch 0022, iter [04300, 05004], lr: 0.098167, loss: 3.1566
2022-07-13 19:21:30 - train: epoch 0022, iter [04400, 05004], lr: 0.098163, loss: 3.1416
2022-07-13 19:22:05 - train: epoch 0022, iter [04500, 05004], lr: 0.098158, loss: 2.9478
2022-07-13 19:22:39 - train: epoch 0022, iter [04600, 05004], lr: 0.098154, loss: 3.0761
2022-07-13 19:23:13 - train: epoch 0022, iter [04700, 05004], lr: 0.098150, loss: 3.0026
2022-07-13 19:23:47 - train: epoch 0022, iter [04800, 05004], lr: 0.098145, loss: 2.6748
2022-07-13 19:24:21 - train: epoch 0022, iter [04900, 05004], lr: 0.098141, loss: 2.8064
2022-07-13 19:24:54 - train: epoch 0022, iter [05000, 05004], lr: 0.098137, loss: 2.7722
2022-07-13 19:24:55 - train: epoch 022, train_loss: 2.9810
2022-07-13 19:26:10 - eval: epoch: 022, acc1: 54.156%, acc5: 79.298%, test_loss: 1.9710, per_image_load_time: 2.433ms, per_image_inference_time: 0.459ms
2022-07-13 19:26:10 - until epoch: 022, best_acc1: 54.738%
2022-07-13 19:26:10 - epoch 023 lr: 0.098136
2022-07-13 19:26:49 - train: epoch 0023, iter [00100, 05004], lr: 0.098132, loss: 2.8549
2022-07-13 19:27:24 - train: epoch 0023, iter [00200, 05004], lr: 0.098128, loss: 2.7614
2022-07-13 19:27:57 - train: epoch 0023, iter [00300, 05004], lr: 0.098123, loss: 2.9544
2022-07-13 19:28:32 - train: epoch 0023, iter [00400, 05004], lr: 0.098119, loss: 3.3312
2022-07-13 19:29:06 - train: epoch 0023, iter [00500, 05004], lr: 0.098115, loss: 3.0119
2022-07-13 19:29:41 - train: epoch 0023, iter [00600, 05004], lr: 0.098110, loss: 3.0957
2022-07-13 19:30:14 - train: epoch 0023, iter [00700, 05004], lr: 0.098106, loss: 2.7671
2022-07-13 19:30:48 - train: epoch 0023, iter [00800, 05004], lr: 0.098101, loss: 3.0581
2022-07-13 19:31:22 - train: epoch 0023, iter [00900, 05004], lr: 0.098097, loss: 2.9564
2022-07-13 19:31:57 - train: epoch 0023, iter [01000, 05004], lr: 0.098093, loss: 2.7740
2022-07-13 19:32:30 - train: epoch 0023, iter [01100, 05004], lr: 0.098088, loss: 3.1906
2022-07-13 19:33:05 - train: epoch 0023, iter [01200, 05004], lr: 0.098084, loss: 2.8595
2022-07-13 19:33:39 - train: epoch 0023, iter [01300, 05004], lr: 0.098079, loss: 3.0528
2022-07-13 19:34:13 - train: epoch 0023, iter [01400, 05004], lr: 0.098075, loss: 2.8508
2022-07-13 19:34:46 - train: epoch 0023, iter [01500, 05004], lr: 0.098071, loss: 2.8833
2022-07-13 19:35:21 - train: epoch 0023, iter [01600, 05004], lr: 0.098066, loss: 2.8918
2022-07-13 19:35:55 - train: epoch 0023, iter [01700, 05004], lr: 0.098062, loss: 2.9109
2022-07-13 19:36:29 - train: epoch 0023, iter [01800, 05004], lr: 0.098057, loss: 2.7474
2022-07-13 19:37:02 - train: epoch 0023, iter [01900, 05004], lr: 0.098053, loss: 3.1817
2022-07-13 19:37:37 - train: epoch 0023, iter [02000, 05004], lr: 0.098048, loss: 2.5925
2022-07-13 19:38:10 - train: epoch 0023, iter [02100, 05004], lr: 0.098044, loss: 3.0267
2022-07-13 19:38:44 - train: epoch 0023, iter [02200, 05004], lr: 0.098039, loss: 2.9802
2022-07-13 19:39:17 - train: epoch 0023, iter [02300, 05004], lr: 0.098035, loss: 2.8803
2022-07-13 19:39:51 - train: epoch 0023, iter [02400, 05004], lr: 0.098030, loss: 3.0243
2022-07-13 19:40:26 - train: epoch 0023, iter [02500, 05004], lr: 0.098026, loss: 2.9993
2022-07-13 19:40:59 - train: epoch 0023, iter [02600, 05004], lr: 0.098022, loss: 2.9283
2022-07-13 19:41:34 - train: epoch 0023, iter [02700, 05004], lr: 0.098017, loss: 2.8880
2022-07-13 19:42:08 - train: epoch 0023, iter [02800, 05004], lr: 0.098013, loss: 2.9468
2022-07-13 19:42:41 - train: epoch 0023, iter [02900, 05004], lr: 0.098008, loss: 3.1036
2022-07-13 19:43:15 - train: epoch 0023, iter [03000, 05004], lr: 0.098004, loss: 3.0667
2022-07-13 19:43:49 - train: epoch 0023, iter [03100, 05004], lr: 0.097999, loss: 3.0911
2022-07-13 19:44:23 - train: epoch 0023, iter [03200, 05004], lr: 0.097995, loss: 2.9767
2022-07-13 19:44:56 - train: epoch 0023, iter [03300, 05004], lr: 0.097990, loss: 3.2909
2022-07-13 19:45:31 - train: epoch 0023, iter [03400, 05004], lr: 0.097985, loss: 3.1782
2022-07-13 19:46:03 - train: epoch 0023, iter [03500, 05004], lr: 0.097981, loss: 2.7755
2022-07-13 19:46:39 - train: epoch 0023, iter [03600, 05004], lr: 0.097976, loss: 2.8712
2022-07-13 19:47:13 - train: epoch 0023, iter [03700, 05004], lr: 0.097972, loss: 3.1298
2022-07-13 19:47:47 - train: epoch 0023, iter [03800, 05004], lr: 0.097967, loss: 3.0697
2022-07-13 19:48:21 - train: epoch 0023, iter [03900, 05004], lr: 0.097963, loss: 2.9471
2022-07-13 19:48:55 - train: epoch 0023, iter [04000, 05004], lr: 0.097958, loss: 3.0575
2022-07-13 19:49:29 - train: epoch 0023, iter [04100, 05004], lr: 0.097954, loss: 2.7791
2022-07-13 19:50:03 - train: epoch 0023, iter [04200, 05004], lr: 0.097949, loss: 2.8914
2022-07-13 19:50:37 - train: epoch 0023, iter [04300, 05004], lr: 0.097945, loss: 2.6725
2022-07-13 19:51:11 - train: epoch 0023, iter [04400, 05004], lr: 0.097940, loss: 2.7475
2022-07-13 19:51:45 - train: epoch 0023, iter [04500, 05004], lr: 0.097935, loss: 2.7292
2022-07-13 19:52:20 - train: epoch 0023, iter [04600, 05004], lr: 0.097931, loss: 2.9270
2022-07-13 19:52:54 - train: epoch 0023, iter [04700, 05004], lr: 0.097926, loss: 2.9024
2022-07-13 19:53:29 - train: epoch 0023, iter [04800, 05004], lr: 0.097922, loss: 2.9251
2022-07-13 19:54:03 - train: epoch 0023, iter [04900, 05004], lr: 0.097917, loss: 3.0961
2022-07-13 19:54:36 - train: epoch 0023, iter [05000, 05004], lr: 0.097912, loss: 2.9748
2022-07-13 19:54:37 - train: epoch 023, train_loss: 2.9722
2022-07-13 19:55:51 - eval: epoch: 023, acc1: 53.310%, acc5: 78.648%, test_loss: 2.0107, per_image_load_time: 2.267ms, per_image_inference_time: 0.464ms
2022-07-13 19:55:51 - until epoch: 023, best_acc1: 54.738%
2022-07-13 19:55:51 - epoch 024 lr: 0.097912
2022-07-13 19:56:30 - train: epoch 0024, iter [00100, 05004], lr: 0.097908, loss: 2.7979
2022-07-13 19:57:05 - train: epoch 0024, iter [00200, 05004], lr: 0.097903, loss: 2.9572
2022-07-13 19:57:39 - train: epoch 0024, iter [00300, 05004], lr: 0.097898, loss: 3.0505
2022-07-13 19:58:14 - train: epoch 0024, iter [00400, 05004], lr: 0.097894, loss: 3.0824
2022-07-13 19:58:48 - train: epoch 0024, iter [00500, 05004], lr: 0.097889, loss: 2.9719
2022-07-13 19:59:22 - train: epoch 0024, iter [00600, 05004], lr: 0.097885, loss: 2.8981
2022-07-13 19:59:56 - train: epoch 0024, iter [00700, 05004], lr: 0.097880, loss: 2.9550
2022-07-13 20:00:31 - train: epoch 0024, iter [00800, 05004], lr: 0.097875, loss: 2.6560
2022-07-13 20:01:05 - train: epoch 0024, iter [00900, 05004], lr: 0.097871, loss: 3.0420
2022-07-13 20:01:40 - train: epoch 0024, iter [01000, 05004], lr: 0.097866, loss: 3.0163
2022-07-13 20:02:14 - train: epoch 0024, iter [01100, 05004], lr: 0.097861, loss: 2.5835
2022-07-13 20:02:48 - train: epoch 0024, iter [01200, 05004], lr: 0.097857, loss: 2.8924
2022-07-13 20:03:22 - train: epoch 0024, iter [01300, 05004], lr: 0.097852, loss: 3.1159
2022-07-13 20:03:57 - train: epoch 0024, iter [01400, 05004], lr: 0.097847, loss: 2.7188
2022-07-13 20:04:31 - train: epoch 0024, iter [01500, 05004], lr: 0.097843, loss: 2.9856
2022-07-13 20:05:06 - train: epoch 0024, iter [01600, 05004], lr: 0.097838, loss: 3.0440
2022-07-13 20:05:41 - train: epoch 0024, iter [01700, 05004], lr: 0.097833, loss: 3.0171
2022-07-13 20:06:15 - train: epoch 0024, iter [01800, 05004], lr: 0.097829, loss: 2.9760
2022-07-13 20:06:49 - train: epoch 0024, iter [01900, 05004], lr: 0.097824, loss: 2.8432
2022-07-13 20:07:24 - train: epoch 0024, iter [02000, 05004], lr: 0.097819, loss: 3.1775
2022-07-13 20:07:58 - train: epoch 0024, iter [02100, 05004], lr: 0.097815, loss: 2.9409
2022-07-13 20:08:33 - train: epoch 0024, iter [02200, 05004], lr: 0.097810, loss: 2.9688
2022-07-13 20:09:07 - train: epoch 0024, iter [02300, 05004], lr: 0.097805, loss: 2.9473
2022-07-13 20:09:42 - train: epoch 0024, iter [02400, 05004], lr: 0.097800, loss: 3.0171
2022-07-13 20:10:16 - train: epoch 0024, iter [02500, 05004], lr: 0.097796, loss: 2.8393
2022-07-13 20:10:50 - train: epoch 0024, iter [02600, 05004], lr: 0.097791, loss: 2.9867
2022-07-13 20:11:23 - train: epoch 0024, iter [02700, 05004], lr: 0.097786, loss: 2.9184
2022-07-13 20:11:58 - train: epoch 0024, iter [02800, 05004], lr: 0.097781, loss: 3.0225
2022-07-13 20:12:31 - train: epoch 0024, iter [02900, 05004], lr: 0.097777, loss: 2.8688
2022-07-13 20:13:05 - train: epoch 0024, iter [03000, 05004], lr: 0.097772, loss: 2.8843
2022-07-13 20:13:39 - train: epoch 0024, iter [03100, 05004], lr: 0.097767, loss: 3.1874
2022-07-13 20:14:13 - train: epoch 0024, iter [03200, 05004], lr: 0.097762, loss: 3.1027
2022-07-13 20:14:47 - train: epoch 0024, iter [03300, 05004], lr: 0.097758, loss: 2.8471
2022-07-13 20:15:22 - train: epoch 0024, iter [03400, 05004], lr: 0.097753, loss: 2.9433
2022-07-13 20:15:55 - train: epoch 0024, iter [03500, 05004], lr: 0.097748, loss: 2.9593
2022-07-13 20:16:29 - train: epoch 0024, iter [03600, 05004], lr: 0.097743, loss: 2.9505
2022-07-13 20:17:04 - train: epoch 0024, iter [03700, 05004], lr: 0.097739, loss: 2.8517
2022-07-13 20:17:38 - train: epoch 0024, iter [03800, 05004], lr: 0.097734, loss: 3.0253
2022-07-13 20:18:12 - train: epoch 0024, iter [03900, 05004], lr: 0.097729, loss: 3.0708
2022-07-13 20:18:46 - train: epoch 0024, iter [04000, 05004], lr: 0.097724, loss: 2.9522
2022-07-13 20:19:21 - train: epoch 0024, iter [04100, 05004], lr: 0.097719, loss: 2.8298
2022-07-13 20:19:55 - train: epoch 0024, iter [04200, 05004], lr: 0.097715, loss: 2.7945
2022-07-13 20:20:30 - train: epoch 0024, iter [04300, 05004], lr: 0.097710, loss: 2.8734
2022-07-13 20:21:04 - train: epoch 0024, iter [04400, 05004], lr: 0.097705, loss: 3.0745
2022-07-13 20:21:38 - train: epoch 0024, iter [04500, 05004], lr: 0.097700, loss: 2.9623
2022-07-13 20:22:13 - train: epoch 0024, iter [04600, 05004], lr: 0.097695, loss: 3.0159
2022-07-13 20:22:47 - train: epoch 0024, iter [04700, 05004], lr: 0.097690, loss: 2.8978
2022-07-13 20:23:21 - train: epoch 0024, iter [04800, 05004], lr: 0.097686, loss: 2.7414
2022-07-13 20:23:57 - train: epoch 0024, iter [04900, 05004], lr: 0.097681, loss: 2.8626
2022-07-13 20:24:29 - train: epoch 0024, iter [05000, 05004], lr: 0.097676, loss: 2.9887
2022-07-13 20:24:30 - train: epoch 024, train_loss: 2.9605
2022-07-13 20:25:45 - eval: epoch: 024, acc1: 54.836%, acc5: 79.912%, test_loss: 1.9161, per_image_load_time: 2.435ms, per_image_inference_time: 0.471ms
2022-07-13 20:25:45 - until epoch: 024, best_acc1: 54.836%
2022-07-13 20:25:45 - epoch 025 lr: 0.097676
2022-07-13 20:26:25 - train: epoch 0025, iter [00100, 05004], lr: 0.097671, loss: 2.8678
2022-07-13 20:26:59 - train: epoch 0025, iter [00200, 05004], lr: 0.097666, loss: 2.6770
2022-07-13 20:27:33 - train: epoch 0025, iter [00300, 05004], lr: 0.097661, loss: 2.9327
2022-07-13 20:28:07 - train: epoch 0025, iter [00400, 05004], lr: 0.097656, loss: 2.7928
2022-07-13 20:28:41 - train: epoch 0025, iter [00500, 05004], lr: 0.097651, loss: 2.8130
2022-07-13 20:29:16 - train: epoch 0025, iter [00600, 05004], lr: 0.097647, loss: 3.0199
2022-07-13 20:29:51 - train: epoch 0025, iter [00700, 05004], lr: 0.097642, loss: 2.7308
2022-07-13 20:30:24 - train: epoch 0025, iter [00800, 05004], lr: 0.097637, loss: 3.0281
2022-07-13 20:30:58 - train: epoch 0025, iter [00900, 05004], lr: 0.097632, loss: 2.7490
2022-07-13 20:31:33 - train: epoch 0025, iter [01000, 05004], lr: 0.097627, loss: 2.8825
2022-07-13 20:32:07 - train: epoch 0025, iter [01100, 05004], lr: 0.097622, loss: 2.9418
2022-07-13 20:32:41 - train: epoch 0025, iter [01200, 05004], lr: 0.097617, loss: 2.7651
2022-07-13 20:33:16 - train: epoch 0025, iter [01300, 05004], lr: 0.097612, loss: 2.9237
2022-07-13 20:33:51 - train: epoch 0025, iter [01400, 05004], lr: 0.097607, loss: 3.1649
2022-07-13 20:34:25 - train: epoch 0025, iter [01500, 05004], lr: 0.097602, loss: 2.8107
2022-07-13 20:34:59 - train: epoch 0025, iter [01600, 05004], lr: 0.097597, loss: 2.6846
2022-07-13 20:35:33 - train: epoch 0025, iter [01700, 05004], lr: 0.097593, loss: 2.7855
2022-07-13 20:36:08 - train: epoch 0025, iter [01800, 05004], lr: 0.097588, loss: 2.7962
2022-07-13 20:36:43 - train: epoch 0025, iter [01900, 05004], lr: 0.097583, loss: 2.8216
2022-07-13 20:37:18 - train: epoch 0025, iter [02000, 05004], lr: 0.097578, loss: 2.8321
2022-07-13 20:37:52 - train: epoch 0025, iter [02100, 05004], lr: 0.097573, loss: 2.8983
2022-07-13 20:38:28 - train: epoch 0025, iter [02200, 05004], lr: 0.097568, loss: 2.7117
2022-07-13 20:39:03 - train: epoch 0025, iter [02300, 05004], lr: 0.097563, loss: 3.1317
2022-07-13 20:39:38 - train: epoch 0025, iter [02400, 05004], lr: 0.097558, loss: 2.6713
2022-07-13 20:40:12 - train: epoch 0025, iter [02500, 05004], lr: 0.097553, loss: 3.0264
2022-07-13 20:40:47 - train: epoch 0025, iter [02600, 05004], lr: 0.097548, loss: 2.9308
2022-07-13 20:41:21 - train: epoch 0025, iter [02700, 05004], lr: 0.097543, loss: 3.1361
2022-07-13 20:41:57 - train: epoch 0025, iter [02800, 05004], lr: 0.097538, loss: 2.8015
2022-07-13 20:42:31 - train: epoch 0025, iter [02900, 05004], lr: 0.097533, loss: 3.1448
2022-07-13 20:43:06 - train: epoch 0025, iter [03000, 05004], lr: 0.097528, loss: 2.9819
2022-07-13 20:43:39 - train: epoch 0025, iter [03100, 05004], lr: 0.097523, loss: 2.8659
2022-07-13 20:44:13 - train: epoch 0025, iter [03200, 05004], lr: 0.097518, loss: 3.2871
2022-07-13 20:44:47 - train: epoch 0025, iter [03300, 05004], lr: 0.097513, loss: 2.9666
2022-07-13 20:45:21 - train: epoch 0025, iter [03400, 05004], lr: 0.097508, loss: 3.2615
2022-07-13 20:45:55 - train: epoch 0025, iter [03500, 05004], lr: 0.097503, loss: 2.8118
2022-07-13 20:46:28 - train: epoch 0025, iter [03600, 05004], lr: 0.097498, loss: 2.9651
2022-07-13 20:47:02 - train: epoch 0025, iter [03700, 05004], lr: 0.097493, loss: 3.0742
2022-07-13 20:47:35 - train: epoch 0025, iter [03800, 05004], lr: 0.097488, loss: 3.2315
2022-07-13 20:48:10 - train: epoch 0025, iter [03900, 05004], lr: 0.097483, loss: 3.2490
2022-07-13 20:48:43 - train: epoch 0025, iter [04000, 05004], lr: 0.097478, loss: 2.8730
2022-07-13 20:49:18 - train: epoch 0025, iter [04100, 05004], lr: 0.097473, loss: 3.3082
2022-07-13 20:49:52 - train: epoch 0025, iter [04200, 05004], lr: 0.097468, loss: 2.9138
2022-07-13 20:50:26 - train: epoch 0025, iter [04300, 05004], lr: 0.097463, loss: 2.8231
2022-07-13 20:51:00 - train: epoch 0025, iter [04400, 05004], lr: 0.097458, loss: 3.0873
2022-07-13 20:51:34 - train: epoch 0025, iter [04500, 05004], lr: 0.097452, loss: 3.0070
2022-07-13 20:52:08 - train: epoch 0025, iter [04600, 05004], lr: 0.097447, loss: 2.8561
2022-07-13 20:52:42 - train: epoch 0025, iter [04700, 05004], lr: 0.097442, loss: 2.7066
2022-07-13 20:53:16 - train: epoch 0025, iter [04800, 05004], lr: 0.097437, loss: 2.9448
2022-07-13 20:53:51 - train: epoch 0025, iter [04900, 05004], lr: 0.097432, loss: 2.9073
2022-07-13 20:54:23 - train: epoch 0025, iter [05000, 05004], lr: 0.097427, loss: 3.1303
2022-07-13 20:54:24 - train: epoch 025, train_loss: 2.9530
2022-07-13 20:55:38 - eval: epoch: 025, acc1: 49.922%, acc5: 75.572%, test_loss: 2.1876, per_image_load_time: 2.189ms, per_image_inference_time: 0.470ms
2022-07-13 20:55:38 - until epoch: 025, best_acc1: 54.836%
2022-07-13 20:55:38 - epoch 026 lr: 0.097427
2022-07-13 20:56:17 - train: epoch 0026, iter [00100, 05004], lr: 0.097422, loss: 2.8611
2022-07-13 20:56:51 - train: epoch 0026, iter [00200, 05004], lr: 0.097417, loss: 2.7649
2022-07-13 20:57:25 - train: epoch 0026, iter [00300, 05004], lr: 0.097412, loss: 2.8183
2022-07-13 20:57:58 - train: epoch 0026, iter [00400, 05004], lr: 0.097406, loss: 3.0909
2022-07-13 20:58:33 - train: epoch 0026, iter [00500, 05004], lr: 0.097401, loss: 2.9079
2022-07-13 20:59:06 - train: epoch 0026, iter [00600, 05004], lr: 0.097396, loss: 3.0303
2022-07-13 20:59:41 - train: epoch 0026, iter [00700, 05004], lr: 0.097391, loss: 3.1074
2022-07-13 21:00:15 - train: epoch 0026, iter [00800, 05004], lr: 0.097386, loss: 2.8485
2022-07-13 21:00:50 - train: epoch 0026, iter [00900, 05004], lr: 0.097381, loss: 3.1547
2022-07-13 21:01:24 - train: epoch 0026, iter [01000, 05004], lr: 0.097376, loss: 2.6628
2022-07-13 21:01:58 - train: epoch 0026, iter [01100, 05004], lr: 0.097370, loss: 2.9672
2022-07-13 21:02:33 - train: epoch 0026, iter [01200, 05004], lr: 0.097365, loss: 3.0090
2022-07-13 21:03:06 - train: epoch 0026, iter [01300, 05004], lr: 0.097360, loss: 2.6893
2022-07-13 21:03:41 - train: epoch 0026, iter [01400, 05004], lr: 0.097355, loss: 3.1176
2022-07-13 21:04:16 - train: epoch 0026, iter [01500, 05004], lr: 0.097350, loss: 3.0137
2022-07-13 21:04:50 - train: epoch 0026, iter [01600, 05004], lr: 0.097345, loss: 2.9775
2022-07-13 21:05:24 - train: epoch 0026, iter [01700, 05004], lr: 0.097339, loss: 2.8583
2022-07-13 21:05:59 - train: epoch 0026, iter [01800, 05004], lr: 0.097334, loss: 2.9925
2022-07-13 21:06:33 - train: epoch 0026, iter [01900, 05004], lr: 0.097329, loss: 3.0538
2022-07-13 21:07:07 - train: epoch 0026, iter [02000, 05004], lr: 0.097324, loss: 3.0526
2022-07-13 21:07:41 - train: epoch 0026, iter [02100, 05004], lr: 0.097319, loss: 2.9614
2022-07-13 21:08:16 - train: epoch 0026, iter [02200, 05004], lr: 0.097313, loss: 2.8552
2022-07-13 21:08:51 - train: epoch 0026, iter [02300, 05004], lr: 0.097308, loss: 2.8668
2022-07-13 21:09:25 - train: epoch 0026, iter [02400, 05004], lr: 0.097303, loss: 3.3446
2022-07-13 21:09:59 - train: epoch 0026, iter [02500, 05004], lr: 0.097298, loss: 3.1534
2022-07-13 21:10:34 - train: epoch 0026, iter [02600, 05004], lr: 0.097293, loss: 3.0618
2022-07-13 21:11:07 - train: epoch 0026, iter [02700, 05004], lr: 0.097287, loss: 2.8489
2022-07-13 21:11:42 - train: epoch 0026, iter [02800, 05004], lr: 0.097282, loss: 2.8692
2022-07-13 21:12:17 - train: epoch 0026, iter [02900, 05004], lr: 0.097277, loss: 3.0295
2022-07-13 21:12:50 - train: epoch 0026, iter [03000, 05004], lr: 0.097272, loss: 2.9810
2022-07-13 21:13:26 - train: epoch 0026, iter [03100, 05004], lr: 0.097266, loss: 2.9798
2022-07-13 21:13:59 - train: epoch 0026, iter [03200, 05004], lr: 0.097261, loss: 3.0396
2022-07-13 21:14:34 - train: epoch 0026, iter [03300, 05004], lr: 0.097256, loss: 2.7459
2022-07-13 21:15:08 - train: epoch 0026, iter [03400, 05004], lr: 0.097251, loss: 2.9867
2022-07-13 21:15:43 - train: epoch 0026, iter [03500, 05004], lr: 0.097245, loss: 3.1158
2022-07-13 21:16:16 - train: epoch 0026, iter [03600, 05004], lr: 0.097240, loss: 3.1733
2022-07-13 21:16:51 - train: epoch 0026, iter [03700, 05004], lr: 0.097235, loss: 2.9730
2022-07-13 21:17:25 - train: epoch 0026, iter [03800, 05004], lr: 0.097230, loss: 3.1592
2022-07-13 21:18:01 - train: epoch 0026, iter [03900, 05004], lr: 0.097224, loss: 3.1612
2022-07-13 21:18:34 - train: epoch 0026, iter [04000, 05004], lr: 0.097219, loss: 3.0807
2022-07-13 21:19:08 - train: epoch 0026, iter [04100, 05004], lr: 0.097214, loss: 3.4166
2022-07-13 21:19:42 - train: epoch 0026, iter [04200, 05004], lr: 0.097208, loss: 3.0994
2022-07-13 21:20:16 - train: epoch 0026, iter [04300, 05004], lr: 0.097203, loss: 2.8690
2022-07-13 21:20:50 - train: epoch 0026, iter [04400, 05004], lr: 0.097198, loss: 3.2633
2022-07-13 21:21:25 - train: epoch 0026, iter [04500, 05004], lr: 0.097192, loss: 3.2150
2022-07-13 21:21:59 - train: epoch 0026, iter [04600, 05004], lr: 0.097187, loss: 2.8806
2022-07-13 21:22:34 - train: epoch 0026, iter [04700, 05004], lr: 0.097182, loss: 2.8207
2022-07-13 21:23:08 - train: epoch 0026, iter [04800, 05004], lr: 0.097176, loss: 2.9523
2022-07-13 21:23:43 - train: epoch 0026, iter [04900, 05004], lr: 0.097171, loss: 2.9114
2022-07-13 21:24:17 - train: epoch 0026, iter [05000, 05004], lr: 0.097166, loss: 3.1175
2022-07-13 21:24:18 - train: epoch 026, train_loss: 2.9449
2022-07-13 21:25:32 - eval: epoch: 026, acc1: 52.000%, acc5: 77.482%, test_loss: 2.0760, per_image_load_time: 1.302ms, per_image_inference_time: 0.468ms
2022-07-13 21:25:32 - until epoch: 026, best_acc1: 54.836%
2022-07-14 04:31:33 - epoch 027 lr: 0.097166
2022-07-14 04:32:15 - train: epoch 0027, iter [00100, 05004], lr: 0.097160, loss: 2.8638
2022-07-14 04:32:48 - train: epoch 0027, iter [00200, 05004], lr: 0.097155, loss: 2.8215
2022-07-14 04:33:21 - train: epoch 0027, iter [00300, 05004], lr: 0.097150, loss: 3.1548
2022-07-14 04:33:55 - train: epoch 0027, iter [00400, 05004], lr: 0.097144, loss: 3.0145
2022-07-14 04:34:28 - train: epoch 0027, iter [00500, 05004], lr: 0.097139, loss: 3.2065
2022-07-14 04:35:02 - train: epoch 0027, iter [00600, 05004], lr: 0.097133, loss: 3.0975
2022-07-14 04:35:33 - train: epoch 0027, iter [00700, 05004], lr: 0.097128, loss: 2.7450
2022-07-14 04:36:07 - train: epoch 0027, iter [00800, 05004], lr: 0.097123, loss: 2.8826
2022-07-14 04:36:40 - train: epoch 0027, iter [00900, 05004], lr: 0.097117, loss: 2.8704
2022-07-14 04:37:13 - train: epoch 0027, iter [01000, 05004], lr: 0.097112, loss: 3.1903
2022-07-14 04:37:47 - train: epoch 0027, iter [01100, 05004], lr: 0.097107, loss: 3.1219
2022-07-14 04:38:19 - train: epoch 0027, iter [01200, 05004], lr: 0.097101, loss: 3.1510
2022-07-14 04:38:52 - train: epoch 0027, iter [01300, 05004], lr: 0.097096, loss: 2.8895
2022-07-14 04:39:26 - train: epoch 0027, iter [01400, 05004], lr: 0.097090, loss: 3.1777
2022-07-14 04:40:00 - train: epoch 0027, iter [01500, 05004], lr: 0.097085, loss: 2.7475
2022-07-14 04:40:32 - train: epoch 0027, iter [01600, 05004], lr: 0.097079, loss: 2.8702
2022-07-14 04:41:05 - train: epoch 0027, iter [01700, 05004], lr: 0.097074, loss: 2.9080
2022-07-14 04:41:38 - train: epoch 0027, iter [01800, 05004], lr: 0.097069, loss: 2.9509
2022-07-14 04:42:12 - train: epoch 0027, iter [01900, 05004], lr: 0.097063, loss: 2.9470
2022-07-14 04:42:45 - train: epoch 0027, iter [02000, 05004], lr: 0.097058, loss: 2.8349
2022-07-14 04:43:19 - train: epoch 0027, iter [02100, 05004], lr: 0.097052, loss: 2.8561
2022-07-14 04:43:52 - train: epoch 0027, iter [02200, 05004], lr: 0.097047, loss: 3.0745
2022-07-14 04:44:26 - train: epoch 0027, iter [02300, 05004], lr: 0.097041, loss: 3.0288
2022-07-14 04:45:00 - train: epoch 0027, iter [02400, 05004], lr: 0.097036, loss: 3.0826
2022-07-14 04:45:33 - train: epoch 0027, iter [02500, 05004], lr: 0.097030, loss: 2.8474
2022-07-14 04:46:07 - train: epoch 0027, iter [02600, 05004], lr: 0.097025, loss: 2.7642
2022-07-14 04:46:40 - train: epoch 0027, iter [02700, 05004], lr: 0.097020, loss: 3.0365
2022-07-14 04:47:14 - train: epoch 0027, iter [02800, 05004], lr: 0.097014, loss: 2.8822
2022-07-14 04:47:48 - train: epoch 0027, iter [02900, 05004], lr: 0.097009, loss: 2.9530
2022-07-14 04:48:22 - train: epoch 0027, iter [03000, 05004], lr: 0.097003, loss: 2.6843
2022-07-14 04:48:55 - train: epoch 0027, iter [03100, 05004], lr: 0.096998, loss: 2.9608
2022-07-14 04:49:29 - train: epoch 0027, iter [03200, 05004], lr: 0.096992, loss: 2.9298
2022-07-14 04:50:03 - train: epoch 0027, iter [03300, 05004], lr: 0.096987, loss: 2.9327
2022-07-14 04:50:36 - train: epoch 0027, iter [03400, 05004], lr: 0.096981, loss: 2.8839
2022-07-14 04:51:10 - train: epoch 0027, iter [03500, 05004], lr: 0.096976, loss: 3.1864
2022-07-14 04:51:44 - train: epoch 0027, iter [03600, 05004], lr: 0.096970, loss: 2.9608
2022-07-14 04:52:18 - train: epoch 0027, iter [03700, 05004], lr: 0.096965, loss: 2.8772
2022-07-14 04:52:52 - train: epoch 0027, iter [03800, 05004], lr: 0.096959, loss: 2.9187
2022-07-14 04:53:26 - train: epoch 0027, iter [03900, 05004], lr: 0.096954, loss: 2.8825
2022-07-14 04:54:00 - train: epoch 0027, iter [04000, 05004], lr: 0.096948, loss: 3.1546
2022-07-14 04:54:33 - train: epoch 0027, iter [04100, 05004], lr: 0.096942, loss: 3.2183
2022-07-14 04:55:07 - train: epoch 0027, iter [04200, 05004], lr: 0.096937, loss: 3.0268
2022-07-14 04:55:42 - train: epoch 0027, iter [04300, 05004], lr: 0.096931, loss: 3.1348
2022-07-14 04:56:15 - train: epoch 0027, iter [04400, 05004], lr: 0.096926, loss: 2.8149
2022-07-14 04:56:49 - train: epoch 0027, iter [04500, 05004], lr: 0.096920, loss: 2.8985
2022-07-14 04:57:22 - train: epoch 0027, iter [04600, 05004], lr: 0.096915, loss: 2.9574
2022-07-14 04:57:56 - train: epoch 0027, iter [04700, 05004], lr: 0.096909, loss: 3.2706
2022-07-14 04:58:30 - train: epoch 0027, iter [04800, 05004], lr: 0.096904, loss: 3.0603
2022-07-14 04:59:04 - train: epoch 0027, iter [04900, 05004], lr: 0.096898, loss: 2.7766
2022-07-14 04:59:36 - train: epoch 0027, iter [05000, 05004], lr: 0.096892, loss: 2.6743
2022-07-14 04:59:37 - train: epoch 027, train_loss: 2.9349
2022-07-14 05:00:51 - eval: epoch: 027, acc1: 54.774%, acc5: 80.060%, test_loss: 1.9198, per_image_load_time: 2.439ms, per_image_inference_time: 0.461ms
2022-07-14 05:00:52 - until epoch: 027, best_acc1: 54.836%
2022-07-14 05:00:52 - epoch 028 lr: 0.096892
2022-07-14 05:01:31 - train: epoch 0028, iter [00100, 05004], lr: 0.096887, loss: 2.6256
2022-07-14 05:02:05 - train: epoch 0028, iter [00200, 05004], lr: 0.096881, loss: 3.0872
2022-07-14 05:02:39 - train: epoch 0028, iter [00300, 05004], lr: 0.096875, loss: 2.9658
2022-07-14 05:03:12 - train: epoch 0028, iter [00400, 05004], lr: 0.096870, loss: 3.0164
2022-07-14 05:03:46 - train: epoch 0028, iter [00500, 05004], lr: 0.096864, loss: 2.9258
2022-07-14 05:04:20 - train: epoch 0028, iter [00600, 05004], lr: 0.096859, loss: 3.0827
2022-07-14 05:04:53 - train: epoch 0028, iter [00700, 05004], lr: 0.096853, loss: 3.1819
2022-07-14 05:05:27 - train: epoch 0028, iter [00800, 05004], lr: 0.096847, loss: 2.6180
2022-07-14 05:06:01 - train: epoch 0028, iter [00900, 05004], lr: 0.096842, loss: 2.7718
2022-07-14 05:06:36 - train: epoch 0028, iter [01000, 05004], lr: 0.096836, loss: 3.0963
2022-07-14 05:07:10 - train: epoch 0028, iter [01100, 05004], lr: 0.096830, loss: 2.9155
2022-07-14 05:07:43 - train: epoch 0028, iter [01200, 05004], lr: 0.096825, loss: 2.9957
2022-07-14 05:08:18 - train: epoch 0028, iter [01300, 05004], lr: 0.096819, loss: 2.8889
2022-07-14 05:08:51 - train: epoch 0028, iter [01400, 05004], lr: 0.096813, loss: 3.0339
2022-07-14 05:09:26 - train: epoch 0028, iter [01500, 05004], lr: 0.096808, loss: 2.9991
2022-07-14 05:10:00 - train: epoch 0028, iter [01600, 05004], lr: 0.096802, loss: 3.1122
2022-07-14 05:10:34 - train: epoch 0028, iter [01700, 05004], lr: 0.096796, loss: 2.9387
2022-07-14 05:11:08 - train: epoch 0028, iter [01800, 05004], lr: 0.096791, loss: 2.7322
2022-07-14 05:11:42 - train: epoch 0028, iter [01900, 05004], lr: 0.096785, loss: 2.8484
2022-07-14 05:12:16 - train: epoch 0028, iter [02000, 05004], lr: 0.096779, loss: 3.0431
2022-07-14 05:12:50 - train: epoch 0028, iter [02100, 05004], lr: 0.096774, loss: 3.1240
2022-07-14 05:13:24 - train: epoch 0028, iter [02200, 05004], lr: 0.096768, loss: 3.0763
2022-07-14 05:13:59 - train: epoch 0028, iter [02300, 05004], lr: 0.096762, loss: 3.2592
2022-07-14 05:14:33 - train: epoch 0028, iter [02400, 05004], lr: 0.096757, loss: 2.8399
2022-07-14 05:15:08 - train: epoch 0028, iter [02500, 05004], lr: 0.096751, loss: 2.8377
2022-07-14 05:15:41 - train: epoch 0028, iter [02600, 05004], lr: 0.096745, loss: 3.1852
2022-07-14 05:16:16 - train: epoch 0028, iter [02700, 05004], lr: 0.096740, loss: 2.8142
2022-07-14 05:16:50 - train: epoch 0028, iter [02800, 05004], lr: 0.096734, loss: 3.1995
2022-07-14 05:17:25 - train: epoch 0028, iter [02900, 05004], lr: 0.096728, loss: 3.1476
2022-07-14 05:17:58 - train: epoch 0028, iter [03000, 05004], lr: 0.096722, loss: 2.8429
2022-07-14 05:18:33 - train: epoch 0028, iter [03100, 05004], lr: 0.096717, loss: 2.9637
2022-07-14 05:19:08 - train: epoch 0028, iter [03200, 05004], lr: 0.096711, loss: 2.9863
2022-07-14 05:19:42 - train: epoch 0028, iter [03300, 05004], lr: 0.096705, loss: 2.9949
2022-07-14 05:20:17 - train: epoch 0028, iter [03400, 05004], lr: 0.096699, loss: 3.0078
2022-07-14 05:20:50 - train: epoch 0028, iter [03500, 05004], lr: 0.096694, loss: 2.9569
2022-07-14 05:21:25 - train: epoch 0028, iter [03600, 05004], lr: 0.096688, loss: 3.0345
2022-07-14 05:21:59 - train: epoch 0028, iter [03700, 05004], lr: 0.096682, loss: 2.9883
2022-07-14 05:22:34 - train: epoch 0028, iter [03800, 05004], lr: 0.096676, loss: 2.8626
2022-07-14 05:23:08 - train: epoch 0028, iter [03900, 05004], lr: 0.096671, loss: 2.6219
2022-07-14 05:23:43 - train: epoch 0028, iter [04000, 05004], lr: 0.096665, loss: 3.0048
2022-07-14 05:24:16 - train: epoch 0028, iter [04100, 05004], lr: 0.096659, loss: 3.1260
2022-07-14 05:24:51 - train: epoch 0028, iter [04200, 05004], lr: 0.096653, loss: 3.0635
2022-07-14 05:25:25 - train: epoch 0028, iter [04300, 05004], lr: 0.096647, loss: 2.9632
2022-07-14 05:26:00 - train: epoch 0028, iter [04400, 05004], lr: 0.096642, loss: 2.9730
2022-07-14 05:26:34 - train: epoch 0028, iter [04500, 05004], lr: 0.096636, loss: 2.9240
2022-07-14 05:27:09 - train: epoch 0028, iter [04600, 05004], lr: 0.096630, loss: 2.8730
2022-07-14 05:27:43 - train: epoch 0028, iter [04700, 05004], lr: 0.096624, loss: 3.1215
2022-07-14 05:28:17 - train: epoch 0028, iter [04800, 05004], lr: 0.096618, loss: 3.4171
2022-07-14 05:28:53 - train: epoch 0028, iter [04900, 05004], lr: 0.096613, loss: 3.0868
2022-07-14 05:29:26 - train: epoch 0028, iter [05000, 05004], lr: 0.096607, loss: 2.6913
2022-07-14 05:29:27 - train: epoch 028, train_loss: 2.9256
2022-07-14 05:30:42 - eval: epoch: 028, acc1: 54.988%, acc5: 79.678%, test_loss: 1.9325, per_image_load_time: 2.418ms, per_image_inference_time: 0.462ms
2022-07-14 05:30:43 - until epoch: 028, best_acc1: 54.988%
2022-07-14 05:30:43 - epoch 029 lr: 0.096606
2022-07-14 05:31:22 - train: epoch 0029, iter [00100, 05004], lr: 0.096601, loss: 2.9356
2022-07-14 05:31:56 - train: epoch 0029, iter [00200, 05004], lr: 0.096595, loss: 2.8791
2022-07-14 05:32:30 - train: epoch 0029, iter [00300, 05004], lr: 0.096589, loss: 2.9111
2022-07-14 05:33:03 - train: epoch 0029, iter [00400, 05004], lr: 0.096583, loss: 2.8155
2022-07-14 05:33:36 - train: epoch 0029, iter [00500, 05004], lr: 0.096577, loss: 2.9477
2022-07-14 05:34:10 - train: epoch 0029, iter [00600, 05004], lr: 0.096571, loss: 2.9581
2022-07-14 05:34:43 - train: epoch 0029, iter [00700, 05004], lr: 0.096566, loss: 2.7183
2022-07-14 05:35:16 - train: epoch 0029, iter [00800, 05004], lr: 0.096560, loss: 2.7753
2022-07-14 05:35:50 - train: epoch 0029, iter [00900, 05004], lr: 0.096554, loss: 2.9934
2022-07-14 05:36:24 - train: epoch 0029, iter [01000, 05004], lr: 0.096548, loss: 3.0467
2022-07-14 05:36:58 - train: epoch 0029, iter [01100, 05004], lr: 0.096542, loss: 2.7347
2022-07-14 05:37:31 - train: epoch 0029, iter [01200, 05004], lr: 0.096536, loss: 2.9145
2022-07-14 05:38:05 - train: epoch 0029, iter [01300, 05004], lr: 0.096530, loss: 2.7795
2022-07-14 05:38:38 - train: epoch 0029, iter [01400, 05004], lr: 0.096524, loss: 3.1070
2022-07-14 05:39:12 - train: epoch 0029, iter [01500, 05004], lr: 0.096518, loss: 2.8737
2022-07-14 05:39:45 - train: epoch 0029, iter [01600, 05004], lr: 0.096513, loss: 2.7593
2022-07-14 05:40:19 - train: epoch 0029, iter [01700, 05004], lr: 0.096507, loss: 3.0319
2022-07-14 05:40:52 - train: epoch 0029, iter [01800, 05004], lr: 0.096501, loss: 3.0549
2022-07-14 05:41:26 - train: epoch 0029, iter [01900, 05004], lr: 0.096495, loss: 2.8179
2022-07-14 05:42:00 - train: epoch 0029, iter [02000, 05004], lr: 0.096489, loss: 2.7502
2022-07-14 05:42:34 - train: epoch 0029, iter [02100, 05004], lr: 0.096483, loss: 2.8543
2022-07-14 05:43:07 - train: epoch 0029, iter [02200, 05004], lr: 0.096477, loss: 3.0892
2022-07-14 05:43:41 - train: epoch 0029, iter [02300, 05004], lr: 0.096471, loss: 3.1458
2022-07-14 05:44:15 - train: epoch 0029, iter [02400, 05004], lr: 0.096465, loss: 2.7005
2022-07-14 05:44:48 - train: epoch 0029, iter [02500, 05004], lr: 0.096459, loss: 2.8715
2022-07-14 05:45:23 - train: epoch 0029, iter [02600, 05004], lr: 0.096453, loss: 3.0830
2022-07-14 05:45:55 - train: epoch 0029, iter [02700, 05004], lr: 0.096447, loss: 2.8358
2022-07-14 05:46:30 - train: epoch 0029, iter [02800, 05004], lr: 0.096441, loss: 2.8714
2022-07-14 05:47:04 - train: epoch 0029, iter [02900, 05004], lr: 0.096435, loss: 2.9284
2022-07-14 05:47:38 - train: epoch 0029, iter [03000, 05004], lr: 0.096429, loss: 2.7286
2022-07-14 05:48:11 - train: epoch 0029, iter [03100, 05004], lr: 0.096423, loss: 2.8121
2022-07-14 05:48:45 - train: epoch 0029, iter [03200, 05004], lr: 0.096417, loss: 3.0215
2022-07-14 05:49:19 - train: epoch 0029, iter [03300, 05004], lr: 0.096411, loss: 3.0316
2022-07-14 05:49:53 - train: epoch 0029, iter [03400, 05004], lr: 0.096405, loss: 2.6797
2022-07-14 05:50:27 - train: epoch 0029, iter [03500, 05004], lr: 0.096399, loss: 3.2122
2022-07-14 05:51:01 - train: epoch 0029, iter [03600, 05004], lr: 0.096393, loss: 2.8457
2022-07-14 05:51:35 - train: epoch 0029, iter [03700, 05004], lr: 0.096387, loss: 3.1440
2022-07-14 05:52:09 - train: epoch 0029, iter [03800, 05004], lr: 0.096381, loss: 2.9370
2022-07-14 05:52:43 - train: epoch 0029, iter [03900, 05004], lr: 0.096375, loss: 2.7765
2022-07-14 05:53:17 - train: epoch 0029, iter [04000, 05004], lr: 0.096369, loss: 3.1465
2022-07-14 05:53:50 - train: epoch 0029, iter [04100, 05004], lr: 0.096363, loss: 2.7830
2022-07-14 05:54:25 - train: epoch 0029, iter [04200, 05004], lr: 0.096357, loss: 2.7375
2022-07-14 05:54:59 - train: epoch 0029, iter [04300, 05004], lr: 0.096351, loss: 2.7740
2022-07-14 05:55:33 - train: epoch 0029, iter [04400, 05004], lr: 0.096345, loss: 2.9308
2022-07-14 05:56:07 - train: epoch 0029, iter [04500, 05004], lr: 0.096339, loss: 3.0643
2022-07-14 05:56:41 - train: epoch 0029, iter [04600, 05004], lr: 0.096333, loss: 2.7322
2022-07-14 05:57:15 - train: epoch 0029, iter [04700, 05004], lr: 0.096327, loss: 2.4942
2022-07-14 05:57:49 - train: epoch 0029, iter [04800, 05004], lr: 0.096321, loss: 3.0229
2022-07-14 05:58:23 - train: epoch 0029, iter [04900, 05004], lr: 0.096315, loss: 3.1396
2022-07-14 05:58:55 - train: epoch 0029, iter [05000, 05004], lr: 0.096309, loss: 2.6579
2022-07-14 05:58:56 - train: epoch 029, train_loss: 2.9152
2022-07-14 06:00:11 - eval: epoch: 029, acc1: 53.552%, acc5: 78.812%, test_loss: 1.9832, per_image_load_time: 2.414ms, per_image_inference_time: 0.453ms
2022-07-14 06:00:11 - until epoch: 029, best_acc1: 54.988%
2022-07-14 06:00:11 - epoch 030 lr: 0.096309
2022-07-14 06:00:51 - train: epoch 0030, iter [00100, 05004], lr: 0.096303, loss: 2.9235
2022-07-14 06:01:25 - train: epoch 0030, iter [00200, 05004], lr: 0.096297, loss: 3.0400
2022-07-14 06:01:58 - train: epoch 0030, iter [00300, 05004], lr: 0.096290, loss: 3.0285
2022-07-14 06:02:33 - train: epoch 0030, iter [00400, 05004], lr: 0.096284, loss: 2.8888
2022-07-14 06:03:08 - train: epoch 0030, iter [00500, 05004], lr: 0.096278, loss: 3.0234
2022-07-14 06:03:42 - train: epoch 0030, iter [00600, 05004], lr: 0.096272, loss: 2.9345
2022-07-14 06:04:17 - train: epoch 0030, iter [00700, 05004], lr: 0.096266, loss: 3.0212
2022-07-14 06:04:50 - train: epoch 0030, iter [00800, 05004], lr: 0.096260, loss: 3.1397
2022-07-14 06:05:24 - train: epoch 0030, iter [00900, 05004], lr: 0.096254, loss: 3.1343
2022-07-14 06:05:57 - train: epoch 0030, iter [01000, 05004], lr: 0.096248, loss: 2.6979
2022-07-14 06:06:31 - train: epoch 0030, iter [01100, 05004], lr: 0.096242, loss: 2.8278
2022-07-14 06:07:04 - train: epoch 0030, iter [01200, 05004], lr: 0.096236, loss: 2.8503
2022-07-14 06:07:39 - train: epoch 0030, iter [01300, 05004], lr: 0.096229, loss: 2.7773
2022-07-14 06:08:12 - train: epoch 0030, iter [01400, 05004], lr: 0.096223, loss: 2.6732
2022-07-14 06:08:46 - train: epoch 0030, iter [01500, 05004], lr: 0.096217, loss: 2.9041
2022-07-14 06:09:20 - train: epoch 0030, iter [01600, 05004], lr: 0.096211, loss: 2.8564
2022-07-14 06:09:53 - train: epoch 0030, iter [01700, 05004], lr: 0.096205, loss: 3.0131
2022-07-14 06:10:27 - train: epoch 0030, iter [01800, 05004], lr: 0.096199, loss: 2.9200
2022-07-14 06:11:01 - train: epoch 0030, iter [01900, 05004], lr: 0.096193, loss: 3.0530
2022-07-14 06:11:35 - train: epoch 0030, iter [02000, 05004], lr: 0.096186, loss: 2.6907
2022-07-14 06:12:10 - train: epoch 0030, iter [02100, 05004], lr: 0.096180, loss: 3.0911
2022-07-14 06:12:44 - train: epoch 0030, iter [02200, 05004], lr: 0.096174, loss: 3.0022
2022-07-14 06:13:17 - train: epoch 0030, iter [02300, 05004], lr: 0.096168, loss: 2.9145
2022-07-14 06:13:51 - train: epoch 0030, iter [02400, 05004], lr: 0.096162, loss: 3.0784
2022-07-14 06:14:26 - train: epoch 0030, iter [02500, 05004], lr: 0.096155, loss: 3.0892
2022-07-14 06:15:00 - train: epoch 0030, iter [02600, 05004], lr: 0.096149, loss: 2.7268
2022-07-14 06:15:34 - train: epoch 0030, iter [02700, 05004], lr: 0.096143, loss: 2.9514
2022-07-14 06:16:08 - train: epoch 0030, iter [02800, 05004], lr: 0.096137, loss: 3.0519
2022-07-14 06:16:43 - train: epoch 0030, iter [02900, 05004], lr: 0.096131, loss: 2.7875
2022-07-14 06:17:17 - train: epoch 0030, iter [03000, 05004], lr: 0.096124, loss: 2.8552
2022-07-14 06:17:50 - train: epoch 0030, iter [03100, 05004], lr: 0.096118, loss: 2.9998
2022-07-14 06:18:24 - train: epoch 0030, iter [03200, 05004], lr: 0.096112, loss: 2.9012
2022-07-14 06:18:59 - train: epoch 0030, iter [03300, 05004], lr: 0.096106, loss: 3.0995
2022-07-14 06:19:33 - train: epoch 0030, iter [03400, 05004], lr: 0.096100, loss: 2.7826
2022-07-14 06:20:07 - train: epoch 0030, iter [03500, 05004], lr: 0.096093, loss: 2.9298
2022-07-14 06:20:41 - train: epoch 0030, iter [03600, 05004], lr: 0.096087, loss: 2.8064
2022-07-14 06:21:15 - train: epoch 0030, iter [03700, 05004], lr: 0.096081, loss: 2.7650
2022-07-14 06:21:49 - train: epoch 0030, iter [03800, 05004], lr: 0.096075, loss: 2.9658
2022-07-14 06:22:24 - train: epoch 0030, iter [03900, 05004], lr: 0.096068, loss: 2.8467
2022-07-14 06:22:58 - train: epoch 0030, iter [04000, 05004], lr: 0.096062, loss: 3.0203
2022-07-14 06:23:31 - train: epoch 0030, iter [04100, 05004], lr: 0.096056, loss: 2.7962
2022-07-14 06:24:06 - train: epoch 0030, iter [04200, 05004], lr: 0.096050, loss: 2.9213
2022-07-14 06:24:40 - train: epoch 0030, iter [04300, 05004], lr: 0.096043, loss: 3.1043
2022-07-14 06:25:15 - train: epoch 0030, iter [04400, 05004], lr: 0.096037, loss: 2.6656
2022-07-14 06:25:50 - train: epoch 0030, iter [04500, 05004], lr: 0.096031, loss: 3.1581
2022-07-14 06:26:24 - train: epoch 0030, iter [04600, 05004], lr: 0.096024, loss: 2.6254
2022-07-14 06:26:58 - train: epoch 0030, iter [04700, 05004], lr: 0.096018, loss: 2.7824
2022-07-14 06:27:32 - train: epoch 0030, iter [04800, 05004], lr: 0.096012, loss: 3.0232
2022-07-14 06:28:07 - train: epoch 0030, iter [04900, 05004], lr: 0.096006, loss: 2.9695
2022-07-14 06:28:41 - train: epoch 0030, iter [05000, 05004], lr: 0.095999, loss: 3.0066
2022-07-14 06:28:42 - train: epoch 030, train_loss: 2.9188
2022-07-14 06:29:55 - eval: epoch: 030, acc1: 54.200%, acc5: 79.088%, test_loss: 1.9691, per_image_load_time: 2.322ms, per_image_inference_time: 0.469ms
2022-07-14 06:29:56 - until epoch: 030, best_acc1: 54.988%
2022-07-14 06:29:56 - epoch 031 lr: 0.095999
2022-07-14 06:30:35 - train: epoch 0031, iter [00100, 05004], lr: 0.095993, loss: 2.9791
2022-07-14 06:31:09 - train: epoch 0031, iter [00200, 05004], lr: 0.095986, loss: 2.9393
2022-07-14 06:31:44 - train: epoch 0031, iter [00300, 05004], lr: 0.095980, loss: 2.7534
2022-07-14 06:32:18 - train: epoch 0031, iter [00400, 05004], lr: 0.095974, loss: 2.5791
2022-07-14 06:32:52 - train: epoch 0031, iter [00500, 05004], lr: 0.095967, loss: 2.7251
2022-07-14 06:33:26 - train: epoch 0031, iter [00600, 05004], lr: 0.095961, loss: 2.7903
2022-07-14 06:34:01 - train: epoch 0031, iter [00700, 05004], lr: 0.095955, loss: 3.1063
2022-07-14 06:34:35 - train: epoch 0031, iter [00800, 05004], lr: 0.095948, loss: 3.1191
2022-07-14 06:35:09 - train: epoch 0031, iter [00900, 05004], lr: 0.095942, loss: 2.8057
2022-07-14 06:35:45 - train: epoch 0031, iter [01000, 05004], lr: 0.095936, loss: 3.1821
2022-07-14 06:36:18 - train: epoch 0031, iter [01100, 05004], lr: 0.095929, loss: 3.0394
2022-07-14 06:36:51 - train: epoch 0031, iter [01200, 05004], lr: 0.095923, loss: 2.7528
2022-07-14 06:37:26 - train: epoch 0031, iter [01300, 05004], lr: 0.095917, loss: 2.7235
2022-07-14 06:38:00 - train: epoch 0031, iter [01400, 05004], lr: 0.095910, loss: 2.9104
2022-07-14 06:38:34 - train: epoch 0031, iter [01500, 05004], lr: 0.095904, loss: 3.0045
2022-07-14 06:39:08 - train: epoch 0031, iter [01600, 05004], lr: 0.095897, loss: 2.9625
2022-07-14 06:39:41 - train: epoch 0031, iter [01700, 05004], lr: 0.095891, loss: 2.8041
2022-07-14 06:40:15 - train: epoch 0031, iter [01800, 05004], lr: 0.095885, loss: 2.7878
2022-07-14 06:40:49 - train: epoch 0031, iter [01900, 05004], lr: 0.095878, loss: 2.8528
2022-07-14 06:41:22 - train: epoch 0031, iter [02000, 05004], lr: 0.095872, loss: 2.8584
2022-07-14 06:41:56 - train: epoch 0031, iter [02100, 05004], lr: 0.095865, loss: 2.9611
2022-07-14 06:42:29 - train: epoch 0031, iter [02200, 05004], lr: 0.095859, loss: 2.6719
2022-07-14 06:43:03 - train: epoch 0031, iter [02300, 05004], lr: 0.095853, loss: 2.8734
2022-07-14 06:43:36 - train: epoch 0031, iter [02400, 05004], lr: 0.095846, loss: 2.7587
2022-07-14 06:44:10 - train: epoch 0031, iter [02500, 05004], lr: 0.095840, loss: 2.6108
2022-07-14 06:44:44 - train: epoch 0031, iter [02600, 05004], lr: 0.095833, loss: 2.8115
2022-07-14 06:45:18 - train: epoch 0031, iter [02700, 05004], lr: 0.095827, loss: 2.9638
2022-07-14 06:45:52 - train: epoch 0031, iter [02800, 05004], lr: 0.095820, loss: 3.2399
2022-07-14 06:46:26 - train: epoch 0031, iter [02900, 05004], lr: 0.095814, loss: 2.9015
2022-07-14 06:46:59 - train: epoch 0031, iter [03000, 05004], lr: 0.095808, loss: 3.1536
2022-07-14 06:47:32 - train: epoch 0031, iter [03100, 05004], lr: 0.095801, loss: 2.7874
2022-07-14 06:48:06 - train: epoch 0031, iter [03200, 05004], lr: 0.095795, loss: 3.0579
2022-07-14 06:48:40 - train: epoch 0031, iter [03300, 05004], lr: 0.095788, loss: 2.9122
2022-07-14 06:49:15 - train: epoch 0031, iter [03400, 05004], lr: 0.095782, loss: 3.1752
2022-07-14 06:49:49 - train: epoch 0031, iter [03500, 05004], lr: 0.095775, loss: 3.1670
2022-07-14 06:50:22 - train: epoch 0031, iter [03600, 05004], lr: 0.095769, loss: 2.6574
2022-07-14 06:50:57 - train: epoch 0031, iter [03700, 05004], lr: 0.095762, loss: 2.6513
2022-07-14 06:51:30 - train: epoch 0031, iter [03800, 05004], lr: 0.095756, loss: 2.6412
2022-07-14 06:52:05 - train: epoch 0031, iter [03900, 05004], lr: 0.095749, loss: 2.8400
2022-07-14 06:52:39 - train: epoch 0031, iter [04000, 05004], lr: 0.095743, loss: 3.2277
2022-07-14 06:53:13 - train: epoch 0031, iter [04100, 05004], lr: 0.095736, loss: 2.6464
2022-07-14 06:53:47 - train: epoch 0031, iter [04200, 05004], lr: 0.095730, loss: 3.0075
2022-07-14 06:54:20 - train: epoch 0031, iter [04300, 05004], lr: 0.095723, loss: 3.1106
2022-07-14 06:54:55 - train: epoch 0031, iter [04400, 05004], lr: 0.095717, loss: 2.9424
2022-07-14 06:55:30 - train: epoch 0031, iter [04500, 05004], lr: 0.095710, loss: 2.7443
2022-07-14 06:56:03 - train: epoch 0031, iter [04600, 05004], lr: 0.095704, loss: 2.9049
2022-07-14 06:56:38 - train: epoch 0031, iter [04700, 05004], lr: 0.095697, loss: 2.7944
2022-07-14 06:57:13 - train: epoch 0031, iter [04800, 05004], lr: 0.095691, loss: 2.5570
2022-07-14 06:57:46 - train: epoch 0031, iter [04900, 05004], lr: 0.095684, loss: 2.6343
2022-07-14 06:58:19 - train: epoch 0031, iter [05000, 05004], lr: 0.095678, loss: 3.0901
2022-07-14 06:58:20 - train: epoch 031, train_loss: 2.9065
2022-07-14 06:59:34 - eval: epoch: 031, acc1: 55.792%, acc5: 80.746%, test_loss: 1.8751, per_image_load_time: 2.379ms, per_image_inference_time: 0.485ms
2022-07-14 06:59:34 - until epoch: 031, best_acc1: 55.792%
2022-07-14 06:59:34 - epoch 032 lr: 0.095677
2022-07-14 07:00:14 - train: epoch 0032, iter [00100, 05004], lr: 0.095671, loss: 2.9411
2022-07-14 07:00:48 - train: epoch 0032, iter [00200, 05004], lr: 0.095664, loss: 2.9327
2022-07-14 07:01:22 - train: epoch 0032, iter [00300, 05004], lr: 0.095658, loss: 2.5577
2022-07-14 07:01:57 - train: epoch 0032, iter [00400, 05004], lr: 0.095651, loss: 2.7103
2022-07-14 07:02:31 - train: epoch 0032, iter [00500, 05004], lr: 0.095644, loss: 3.1142
2022-07-14 07:03:05 - train: epoch 0032, iter [00600, 05004], lr: 0.095638, loss: 2.6926
2022-07-14 07:03:40 - train: epoch 0032, iter [00700, 05004], lr: 0.095631, loss: 2.9926
2022-07-14 07:04:14 - train: epoch 0032, iter [00800, 05004], lr: 0.095625, loss: 2.8669
2022-07-14 07:04:49 - train: epoch 0032, iter [00900, 05004], lr: 0.095618, loss: 3.1077
2022-07-14 07:05:23 - train: epoch 0032, iter [01000, 05004], lr: 0.095612, loss: 3.1388
2022-07-14 07:05:58 - train: epoch 0032, iter [01100, 05004], lr: 0.095605, loss: 2.9638
2022-07-14 07:06:32 - train: epoch 0032, iter [01200, 05004], lr: 0.095598, loss: 2.9721
2022-07-14 07:07:07 - train: epoch 0032, iter [01300, 05004], lr: 0.095592, loss: 2.9856
2022-07-14 07:07:41 - train: epoch 0032, iter [01400, 05004], lr: 0.095585, loss: 2.9362
2022-07-14 07:08:15 - train: epoch 0032, iter [01500, 05004], lr: 0.095579, loss: 2.9140
2022-07-14 07:08:48 - train: epoch 0032, iter [01600, 05004], lr: 0.095572, loss: 2.9146
2022-07-14 07:09:22 - train: epoch 0032, iter [01700, 05004], lr: 0.095565, loss: 3.1131
2022-07-14 07:09:56 - train: epoch 0032, iter [01800, 05004], lr: 0.095559, loss: 3.2489
2022-07-14 07:10:30 - train: epoch 0032, iter [01900, 05004], lr: 0.095552, loss: 2.5545
2022-07-14 07:11:03 - train: epoch 0032, iter [02000, 05004], lr: 0.095545, loss: 2.6311
2022-07-14 07:11:38 - train: epoch 0032, iter [02100, 05004], lr: 0.095539, loss: 3.3626
2022-07-14 07:12:12 - train: epoch 0032, iter [02200, 05004], lr: 0.095532, loss: 3.0533
2022-07-14 07:12:46 - train: epoch 0032, iter [02300, 05004], lr: 0.095525, loss: 3.0041
2022-07-14 07:13:19 - train: epoch 0032, iter [02400, 05004], lr: 0.095519, loss: 3.0661
2022-07-14 07:13:54 - train: epoch 0032, iter [02500, 05004], lr: 0.095512, loss: 2.7995
2022-07-14 07:14:27 - train: epoch 0032, iter [02600, 05004], lr: 0.095505, loss: 2.7296
2022-07-14 07:15:01 - train: epoch 0032, iter [02700, 05004], lr: 0.095499, loss: 2.7623
2022-07-14 07:15:35 - train: epoch 0032, iter [02800, 05004], lr: 0.095492, loss: 2.7807
2022-07-14 07:16:09 - train: epoch 0032, iter [02900, 05004], lr: 0.095485, loss: 2.9421
2022-07-14 07:16:43 - train: epoch 0032, iter [03000, 05004], lr: 0.095479, loss: 2.9698
2022-07-14 07:17:16 - train: epoch 0032, iter [03100, 05004], lr: 0.095472, loss: 2.8523
2022-07-14 07:17:51 - train: epoch 0032, iter [03200, 05004], lr: 0.095465, loss: 3.2576
2022-07-14 07:18:26 - train: epoch 0032, iter [03300, 05004], lr: 0.095459, loss: 2.8173
2022-07-14 07:19:00 - train: epoch 0032, iter [03400, 05004], lr: 0.095452, loss: 2.8392
2022-07-14 07:19:34 - train: epoch 0032, iter [03500, 05004], lr: 0.095445, loss: 2.5773
2022-07-14 07:20:08 - train: epoch 0032, iter [03600, 05004], lr: 0.095438, loss: 3.1404
2022-07-14 07:20:42 - train: epoch 0032, iter [03700, 05004], lr: 0.095432, loss: 3.1336
2022-07-14 07:21:17 - train: epoch 0032, iter [03800, 05004], lr: 0.095425, loss: 2.8358
2022-07-14 07:21:51 - train: epoch 0032, iter [03900, 05004], lr: 0.095418, loss: 3.0237
2022-07-14 07:22:25 - train: epoch 0032, iter [04000, 05004], lr: 0.095412, loss: 2.8305
2022-07-14 07:23:00 - train: epoch 0032, iter [04100, 05004], lr: 0.095405, loss: 2.7526
2022-07-14 07:23:34 - train: epoch 0032, iter [04200, 05004], lr: 0.095398, loss: 2.7664
2022-07-14 07:24:07 - train: epoch 0032, iter [04300, 05004], lr: 0.095391, loss: 3.0488
2022-07-14 07:24:41 - train: epoch 0032, iter [04400, 05004], lr: 0.095385, loss: 2.9138
2022-07-14 07:25:15 - train: epoch 0032, iter [04500, 05004], lr: 0.095378, loss: 3.0326
2022-07-14 07:25:50 - train: epoch 0032, iter [04600, 05004], lr: 0.095371, loss: 2.8741
2022-07-14 07:26:24 - train: epoch 0032, iter [04700, 05004], lr: 0.095364, loss: 3.0390
2022-07-14 07:26:58 - train: epoch 0032, iter [04800, 05004], lr: 0.095358, loss: 2.6881
2022-07-14 07:27:33 - train: epoch 0032, iter [04900, 05004], lr: 0.095351, loss: 2.7722
2022-07-14 07:28:06 - train: epoch 0032, iter [05000, 05004], lr: 0.095344, loss: 2.7992
2022-07-14 07:28:07 - train: epoch 032, train_loss: 2.9015
2022-07-14 07:29:20 - eval: epoch: 032, acc1: 54.210%, acc5: 78.858%, test_loss: 1.9729, per_image_load_time: 1.814ms, per_image_inference_time: 0.469ms
2022-07-14 07:29:20 - until epoch: 032, best_acc1: 55.792%
2022-07-14 07:29:20 - epoch 033 lr: 0.095344
2022-07-14 07:29:59 - train: epoch 0033, iter [00100, 05004], lr: 0.095337, loss: 2.6245
2022-07-14 07:30:33 - train: epoch 0033, iter [00200, 05004], lr: 0.095330, loss: 2.6843
2022-07-14 07:31:07 - train: epoch 0033, iter [00300, 05004], lr: 0.095323, loss: 2.7959
2022-07-14 07:31:41 - train: epoch 0033, iter [00400, 05004], lr: 0.095317, loss: 2.7066
2022-07-14 07:32:16 - train: epoch 0033, iter [00500, 05004], lr: 0.095310, loss: 2.7769
2022-07-14 07:32:50 - train: epoch 0033, iter [00600, 05004], lr: 0.095303, loss: 3.0752
2022-07-14 07:33:24 - train: epoch 0033, iter [00700, 05004], lr: 0.095296, loss: 3.1984
2022-07-14 07:33:59 - train: epoch 0033, iter [00800, 05004], lr: 0.095289, loss: 3.1492
2022-07-14 07:34:33 - train: epoch 0033, iter [00900, 05004], lr: 0.095282, loss: 3.0190
2022-07-14 07:35:07 - train: epoch 0033, iter [01000, 05004], lr: 0.095276, loss: 3.3174
2022-07-14 07:35:41 - train: epoch 0033, iter [01100, 05004], lr: 0.095269, loss: 2.6193
2022-07-14 07:36:16 - train: epoch 0033, iter [01200, 05004], lr: 0.095262, loss: 3.0038
2022-07-14 07:36:50 - train: epoch 0033, iter [01300, 05004], lr: 0.095255, loss: 2.9234
2022-07-14 07:37:25 - train: epoch 0033, iter [01400, 05004], lr: 0.095248, loss: 3.0826
2022-07-14 07:38:00 - train: epoch 0033, iter [01500, 05004], lr: 0.095241, loss: 2.9179
2022-07-14 07:38:33 - train: epoch 0033, iter [01600, 05004], lr: 0.095235, loss: 3.1273
2022-07-14 07:39:07 - train: epoch 0033, iter [01700, 05004], lr: 0.095228, loss: 3.0299
2022-07-14 07:39:42 - train: epoch 0033, iter [01800, 05004], lr: 0.095221, loss: 2.8485
2022-07-14 07:40:17 - train: epoch 0033, iter [01900, 05004], lr: 0.095214, loss: 2.7880
2022-07-14 07:40:51 - train: epoch 0033, iter [02000, 05004], lr: 0.095207, loss: 2.6796
2022-07-14 07:41:25 - train: epoch 0033, iter [02100, 05004], lr: 0.095200, loss: 3.1113
2022-07-14 07:41:58 - train: epoch 0033, iter [02200, 05004], lr: 0.095193, loss: 3.1418
2022-07-14 07:42:32 - train: epoch 0033, iter [02300, 05004], lr: 0.095186, loss: 2.8337
2022-07-14 07:43:06 - train: epoch 0033, iter [02400, 05004], lr: 0.095180, loss: 2.9440
2022-07-14 07:43:40 - train: epoch 0033, iter [02500, 05004], lr: 0.095173, loss: 2.6261
2022-07-14 07:44:13 - train: epoch 0033, iter [02600, 05004], lr: 0.095166, loss: 2.6615
2022-07-14 07:44:47 - train: epoch 0033, iter [02700, 05004], lr: 0.095159, loss: 2.9199
2022-07-14 07:45:21 - train: epoch 0033, iter [02800, 05004], lr: 0.095152, loss: 2.9753
2022-07-14 07:45:54 - train: epoch 0033, iter [02900, 05004], lr: 0.095145, loss: 2.9304
2022-07-14 07:46:28 - train: epoch 0033, iter [03000, 05004], lr: 0.095138, loss: 2.9546
2022-07-14 07:47:01 - train: epoch 0033, iter [03100, 05004], lr: 0.095131, loss: 2.8886
2022-07-14 07:47:35 - train: epoch 0033, iter [03200, 05004], lr: 0.095124, loss: 2.9952
2022-07-14 07:48:08 - train: epoch 0033, iter [03300, 05004], lr: 0.095117, loss: 2.8415
2022-07-14 07:48:42 - train: epoch 0033, iter [03400, 05004], lr: 0.095110, loss: 2.5371
2022-07-14 07:49:16 - train: epoch 0033, iter [03500, 05004], lr: 0.095103, loss: 3.1778
2022-07-14 07:49:49 - train: epoch 0033, iter [03600, 05004], lr: 0.095096, loss: 3.0388
2022-07-14 07:50:23 - train: epoch 0033, iter [03700, 05004], lr: 0.095090, loss: 2.8246
2022-07-14 07:50:57 - train: epoch 0033, iter [03800, 05004], lr: 0.095083, loss: 2.9836
2022-07-14 07:51:31 - train: epoch 0033, iter [03900, 05004], lr: 0.095076, loss: 2.8261
2022-07-14 07:52:05 - train: epoch 0033, iter [04000, 05004], lr: 0.095069, loss: 3.2611
2022-07-14 07:52:39 - train: epoch 0033, iter [04100, 05004], lr: 0.095062, loss: 2.7397
2022-07-14 07:53:12 - train: epoch 0033, iter [04200, 05004], lr: 0.095055, loss: 2.6180
2022-07-14 07:53:46 - train: epoch 0033, iter [04300, 05004], lr: 0.095048, loss: 3.2394
2022-07-14 07:54:20 - train: epoch 0033, iter [04400, 05004], lr: 0.095041, loss: 3.0338
2022-07-14 07:54:52 - train: epoch 0033, iter [04500, 05004], lr: 0.095034, loss: 3.0818
2022-07-14 07:55:26 - train: epoch 0033, iter [04600, 05004], lr: 0.095027, loss: 2.8573
2022-07-14 07:56:00 - train: epoch 0033, iter [04700, 05004], lr: 0.095020, loss: 2.7667
2022-07-14 07:56:34 - train: epoch 0033, iter [04800, 05004], lr: 0.095013, loss: 3.1091
2022-07-14 07:57:08 - train: epoch 0033, iter [04900, 05004], lr: 0.095006, loss: 2.6684
2022-07-14 07:57:41 - train: epoch 0033, iter [05000, 05004], lr: 0.094999, loss: 2.8753
2022-07-14 07:57:42 - train: epoch 033, train_loss: 2.8964
2022-07-14 07:58:56 - eval: epoch: 033, acc1: 53.720%, acc5: 78.842%, test_loss: 1.9746, per_image_load_time: 2.393ms, per_image_inference_time: 0.479ms
2022-07-14 07:58:56 - until epoch: 033, best_acc1: 55.792%
2022-07-14 07:58:56 - epoch 034 lr: 0.094998
2022-07-14 07:59:36 - train: epoch 0034, iter [00100, 05004], lr: 0.094991, loss: 2.8462
2022-07-14 08:00:09 - train: epoch 0034, iter [00200, 05004], lr: 0.094984, loss: 2.7308
2022-07-14 08:00:44 - train: epoch 0034, iter [00300, 05004], lr: 0.094977, loss: 2.6685
2022-07-14 08:01:18 - train: epoch 0034, iter [00400, 05004], lr: 0.094970, loss: 2.6250
2022-07-14 08:01:53 - train: epoch 0034, iter [00500, 05004], lr: 0.094963, loss: 2.6984
2022-07-14 08:02:27 - train: epoch 0034, iter [00600, 05004], lr: 0.094956, loss: 2.8926
2022-07-14 08:03:01 - train: epoch 0034, iter [00700, 05004], lr: 0.094949, loss: 2.9581
2022-07-14 08:03:34 - train: epoch 0034, iter [00800, 05004], lr: 0.094942, loss: 3.1531
2022-07-14 08:04:09 - train: epoch 0034, iter [00900, 05004], lr: 0.094935, loss: 2.9519
2022-07-14 08:04:43 - train: epoch 0034, iter [01000, 05004], lr: 0.094928, loss: 2.7094
2022-07-14 08:05:19 - train: epoch 0034, iter [01100, 05004], lr: 0.094921, loss: 3.0564
2022-07-14 08:05:52 - train: epoch 0034, iter [01200, 05004], lr: 0.094914, loss: 2.9592
2022-07-14 08:06:28 - train: epoch 0034, iter [01300, 05004], lr: 0.094907, loss: 3.0618
2022-07-14 08:07:02 - train: epoch 0034, iter [01400, 05004], lr: 0.094900, loss: 2.9343
2022-07-14 08:07:36 - train: epoch 0034, iter [01500, 05004], lr: 0.094893, loss: 2.9863
2022-07-14 08:08:11 - train: epoch 0034, iter [01600, 05004], lr: 0.094886, loss: 2.7303
2022-07-14 08:08:45 - train: epoch 0034, iter [01700, 05004], lr: 0.094878, loss: 2.7652
2022-07-14 08:09:20 - train: epoch 0034, iter [01800, 05004], lr: 0.094871, loss: 3.0052
2022-07-14 08:09:54 - train: epoch 0034, iter [01900, 05004], lr: 0.094864, loss: 3.0332
2022-07-14 08:10:29 - train: epoch 0034, iter [02000, 05004], lr: 0.094857, loss: 3.1232
2022-07-14 08:11:03 - train: epoch 0034, iter [02100, 05004], lr: 0.094850, loss: 3.0120
2022-07-14 08:11:38 - train: epoch 0034, iter [02200, 05004], lr: 0.094843, loss: 2.5323
2022-07-14 08:12:12 - train: epoch 0034, iter [02300, 05004], lr: 0.094836, loss: 2.7458
2022-07-14 08:12:46 - train: epoch 0034, iter [02400, 05004], lr: 0.094829, loss: 3.0794
2022-07-14 08:13:20 - train: epoch 0034, iter [02500, 05004], lr: 0.094821, loss: 2.9430
2022-07-14 08:13:54 - train: epoch 0034, iter [02600, 05004], lr: 0.094814, loss: 2.8406
2022-07-14 08:14:28 - train: epoch 0034, iter [02700, 05004], lr: 0.094807, loss: 3.0325
2022-07-14 08:15:01 - train: epoch 0034, iter [02800, 05004], lr: 0.094800, loss: 2.7681
2022-07-14 08:15:35 - train: epoch 0034, iter [02900, 05004], lr: 0.094793, loss: 2.8743
2022-07-14 08:16:08 - train: epoch 0034, iter [03000, 05004], lr: 0.094786, loss: 2.7402
2022-07-14 08:16:42 - train: epoch 0034, iter [03100, 05004], lr: 0.094779, loss: 2.9033
2022-07-14 08:17:16 - train: epoch 0034, iter [03200, 05004], lr: 0.094771, loss: 2.9020
2022-07-14 08:17:50 - train: epoch 0034, iter [03300, 05004], lr: 0.094764, loss: 2.8837
2022-07-14 08:18:24 - train: epoch 0034, iter [03400, 05004], lr: 0.094757, loss: 3.3733
2022-07-14 08:18:58 - train: epoch 0034, iter [03500, 05004], lr: 0.094750, loss: 2.9658
2022-07-14 08:19:32 - train: epoch 0034, iter [03600, 05004], lr: 0.094743, loss: 2.9027
2022-07-14 08:20:06 - train: epoch 0034, iter [03700, 05004], lr: 0.094736, loss: 2.8321
2022-07-14 08:20:40 - train: epoch 0034, iter [03800, 05004], lr: 0.094728, loss: 2.9799
2022-07-14 08:21:14 - train: epoch 0034, iter [03900, 05004], lr: 0.094721, loss: 3.0944
2022-07-14 08:21:47 - train: epoch 0034, iter [04000, 05004], lr: 0.094714, loss: 3.0636
2022-07-14 08:22:23 - train: epoch 0034, iter [04100, 05004], lr: 0.094707, loss: 3.0965
2022-07-14 08:22:57 - train: epoch 0034, iter [04200, 05004], lr: 0.094700, loss: 2.9195
2022-07-14 08:23:32 - train: epoch 0034, iter [04300, 05004], lr: 0.094692, loss: 2.7701
2022-07-14 08:24:04 - train: epoch 0034, iter [04400, 05004], lr: 0.094685, loss: 2.7025
2022-07-14 08:24:39 - train: epoch 0034, iter [04500, 05004], lr: 0.094678, loss: 2.9117
2022-07-14 08:25:13 - train: epoch 0034, iter [04600, 05004], lr: 0.094671, loss: 2.9903
2022-07-14 08:25:48 - train: epoch 0034, iter [04700, 05004], lr: 0.094663, loss: 2.8442
2022-07-14 08:26:20 - train: epoch 0034, iter [04800, 05004], lr: 0.094656, loss: 2.9127
2022-07-14 08:26:55 - train: epoch 0034, iter [04900, 05004], lr: 0.094649, loss: 2.9504
2022-07-14 08:27:28 - train: epoch 0034, iter [05000, 05004], lr: 0.094642, loss: 2.9057
2022-07-14 08:27:29 - train: epoch 034, train_loss: 2.8890
2022-07-14 08:28:42 - eval: epoch: 034, acc1: 55.688%, acc5: 80.886%, test_loss: 1.8666, per_image_load_time: 2.195ms, per_image_inference_time: 0.470ms
2022-07-14 08:28:42 - until epoch: 034, best_acc1: 55.792%
2022-07-14 08:28:42 - epoch 035 lr: 0.094641
2022-07-14 08:29:21 - train: epoch 0035, iter [00100, 05004], lr: 0.094634, loss: 2.7157
2022-07-14 08:29:56 - train: epoch 0035, iter [00200, 05004], lr: 0.094627, loss: 2.8913
2022-07-14 08:30:28 - train: epoch 0035, iter [00300, 05004], lr: 0.094620, loss: 2.8522
2022-07-14 08:31:03 - train: epoch 0035, iter [00400, 05004], lr: 0.094612, loss: 2.6190
2022-07-14 08:31:37 - train: epoch 0035, iter [00500, 05004], lr: 0.094605, loss: 2.6579
2022-07-14 08:32:11 - train: epoch 0035, iter [00600, 05004], lr: 0.094598, loss: 2.7998
2022-07-14 08:32:45 - train: epoch 0035, iter [00700, 05004], lr: 0.094591, loss: 3.2104
2022-07-14 08:33:20 - train: epoch 0035, iter [00800, 05004], lr: 0.094583, loss: 2.6548
2022-07-14 08:33:54 - train: epoch 0035, iter [00900, 05004], lr: 0.094576, loss: 2.8768
2022-07-14 08:34:29 - train: epoch 0035, iter [01000, 05004], lr: 0.094569, loss: 2.8561
2022-07-14 08:35:03 - train: epoch 0035, iter [01100, 05004], lr: 0.094561, loss: 2.8401
2022-07-14 08:35:37 - train: epoch 0035, iter [01200, 05004], lr: 0.094554, loss: 2.8824
2022-07-14 08:36:11 - train: epoch 0035, iter [01300, 05004], lr: 0.094547, loss: 3.1052
2022-07-14 08:36:45 - train: epoch 0035, iter [01400, 05004], lr: 0.094539, loss: 2.9769
2022-07-14 08:37:19 - train: epoch 0035, iter [01500, 05004], lr: 0.094532, loss: 2.8651
2022-07-14 08:37:54 - train: epoch 0035, iter [01600, 05004], lr: 0.094525, loss: 2.9057
2022-07-14 08:38:27 - train: epoch 0035, iter [01700, 05004], lr: 0.094517, loss: 2.6771
2022-07-14 08:39:02 - train: epoch 0035, iter [01800, 05004], lr: 0.094510, loss: 2.9251
2022-07-14 08:39:36 - train: epoch 0035, iter [01900, 05004], lr: 0.094503, loss: 3.0462
2022-07-14 08:40:11 - train: epoch 0035, iter [02000, 05004], lr: 0.094495, loss: 3.0047
2022-07-14 08:40:45 - train: epoch 0035, iter [02100, 05004], lr: 0.094488, loss: 2.6846
2022-07-14 08:41:20 - train: epoch 0035, iter [02200, 05004], lr: 0.094481, loss: 3.1850
2022-07-14 08:41:54 - train: epoch 0035, iter [02300, 05004], lr: 0.094473, loss: 2.6049
2022-07-14 08:42:28 - train: epoch 0035, iter [02400, 05004], lr: 0.094466, loss: 3.1286
2022-07-14 08:43:03 - train: epoch 0035, iter [02500, 05004], lr: 0.094459, loss: 2.7399
2022-07-14 08:43:37 - train: epoch 0035, iter [02600, 05004], lr: 0.094451, loss: 2.7848
2022-07-14 08:44:11 - train: epoch 0035, iter [02700, 05004], lr: 0.094444, loss: 2.8981
2022-07-14 08:44:45 - train: epoch 0035, iter [02800, 05004], lr: 0.094437, loss: 3.0615
2022-07-14 08:45:20 - train: epoch 0035, iter [02900, 05004], lr: 0.094429, loss: 2.8661
2022-07-14 08:45:55 - train: epoch 0035, iter [03000, 05004], lr: 0.094422, loss: 2.5288
2022-07-14 08:46:29 - train: epoch 0035, iter [03100, 05004], lr: 0.094414, loss: 2.8382
2022-07-14 08:47:04 - train: epoch 0035, iter [03200, 05004], lr: 0.094407, loss: 3.0958
2022-07-14 08:47:39 - train: epoch 0035, iter [03300, 05004], lr: 0.094400, loss: 2.6657
2022-07-14 08:48:12 - train: epoch 0035, iter [03400, 05004], lr: 0.094392, loss: 2.9099
2022-07-14 08:48:47 - train: epoch 0035, iter [03500, 05004], lr: 0.094385, loss: 2.6820
2022-07-14 08:49:21 - train: epoch 0035, iter [03600, 05004], lr: 0.094377, loss: 2.7988
2022-07-14 08:49:56 - train: epoch 0035, iter [03700, 05004], lr: 0.094370, loss: 2.7185
2022-07-14 08:50:29 - train: epoch 0035, iter [03800, 05004], lr: 0.094363, loss: 3.0706
2022-07-14 08:51:03 - train: epoch 0035, iter [03900, 05004], lr: 0.094355, loss: 2.7228
2022-07-14 08:51:37 - train: epoch 0035, iter [04000, 05004], lr: 0.094348, loss: 2.6247
2022-07-14 08:52:11 - train: epoch 0035, iter [04100, 05004], lr: 0.094340, loss: 3.0296
2022-07-14 08:52:44 - train: epoch 0035, iter [04200, 05004], lr: 0.094333, loss: 2.7348
2022-07-14 08:53:18 - train: epoch 0035, iter [04300, 05004], lr: 0.094325, loss: 3.2578
2022-07-14 08:53:52 - train: epoch 0035, iter [04400, 05004], lr: 0.094318, loss: 2.7155
2022-07-14 08:54:26 - train: epoch 0035, iter [04500, 05004], lr: 0.094310, loss: 2.9526
2022-07-14 08:54:59 - train: epoch 0035, iter [04600, 05004], lr: 0.094303, loss: 2.8219
2022-07-14 08:55:33 - train: epoch 0035, iter [04700, 05004], lr: 0.094296, loss: 2.9920
2022-07-14 08:56:07 - train: epoch 0035, iter [04800, 05004], lr: 0.094288, loss: 2.9861
2022-07-14 08:56:41 - train: epoch 0035, iter [04900, 05004], lr: 0.094281, loss: 2.8004
2022-07-14 08:57:14 - train: epoch 0035, iter [05000, 05004], lr: 0.094273, loss: 2.7186
2022-07-14 08:57:15 - train: epoch 035, train_loss: 2.8809
2022-07-14 08:58:28 - eval: epoch: 035, acc1: 55.506%, acc5: 80.058%, test_loss: 1.9010, per_image_load_time: 2.267ms, per_image_inference_time: 0.463ms
2022-07-14 08:58:28 - until epoch: 035, best_acc1: 55.792%
2022-07-14 08:58:28 - epoch 036 lr: 0.094273
2022-07-14 08:59:07 - train: epoch 0036, iter [00100, 05004], lr: 0.094265, loss: 3.0478
2022-07-14 08:59:41 - train: epoch 0036, iter [00200, 05004], lr: 0.094258, loss: 2.6699
2022-07-14 09:00:14 - train: epoch 0036, iter [00300, 05004], lr: 0.094250, loss: 2.8186
2022-07-14 09:00:47 - train: epoch 0036, iter [00400, 05004], lr: 0.094243, loss: 2.7243
2022-07-14 09:01:21 - train: epoch 0036, iter [00500, 05004], lr: 0.094235, loss: 2.9129
2022-07-14 09:01:55 - train: epoch 0036, iter [00600, 05004], lr: 0.094228, loss: 2.7954
2022-07-14 09:02:29 - train: epoch 0036, iter [00700, 05004], lr: 0.094220, loss: 2.7293
2022-07-14 09:03:03 - train: epoch 0036, iter [00800, 05004], lr: 0.094213, loss: 2.5431
2022-07-14 09:03:37 - train: epoch 0036, iter [00900, 05004], lr: 0.094205, loss: 2.8061
2022-07-14 09:04:12 - train: epoch 0036, iter [01000, 05004], lr: 0.094198, loss: 3.0778
2022-07-14 09:04:46 - train: epoch 0036, iter [01100, 05004], lr: 0.094190, loss: 2.9636
2022-07-14 09:05:20 - train: epoch 0036, iter [01200, 05004], lr: 0.094183, loss: 2.6954
2022-07-14 09:05:54 - train: epoch 0036, iter [01300, 05004], lr: 0.094175, loss: 2.7534
2022-07-14 09:06:28 - train: epoch 0036, iter [01400, 05004], lr: 0.094168, loss: 2.8136
2022-07-14 09:07:02 - train: epoch 0036, iter [01500, 05004], lr: 0.094160, loss: 2.8055
2022-07-14 09:07:37 - train: epoch 0036, iter [01600, 05004], lr: 0.094153, loss: 3.0356
2022-07-14 09:08:12 - train: epoch 0036, iter [01700, 05004], lr: 0.094145, loss: 2.8505
2022-07-14 09:08:46 - train: epoch 0036, iter [01800, 05004], lr: 0.094137, loss: 2.9053
2022-07-14 09:09:20 - train: epoch 0036, iter [01900, 05004], lr: 0.094130, loss: 2.7509
2022-07-14 09:09:55 - train: epoch 0036, iter [02000, 05004], lr: 0.094122, loss: 3.0519
2022-07-14 09:10:29 - train: epoch 0036, iter [02100, 05004], lr: 0.094115, loss: 2.6755
2022-07-14 09:11:03 - train: epoch 0036, iter [02200, 05004], lr: 0.094107, loss: 2.9200
2022-07-14 09:11:37 - train: epoch 0036, iter [02300, 05004], lr: 0.094100, loss: 3.0682
2022-07-14 09:12:12 - train: epoch 0036, iter [02400, 05004], lr: 0.094092, loss: 3.0266
2022-07-14 09:12:47 - train: epoch 0036, iter [02500, 05004], lr: 0.094084, loss: 2.7028
2022-07-14 09:13:21 - train: epoch 0036, iter [02600, 05004], lr: 0.094077, loss: 2.7837
2022-07-14 09:13:55 - train: epoch 0036, iter [02700, 05004], lr: 0.094069, loss: 2.5130
2022-07-14 09:14:30 - train: epoch 0036, iter [02800, 05004], lr: 0.094062, loss: 2.6727
2022-07-14 09:15:05 - train: epoch 0036, iter [02900, 05004], lr: 0.094054, loss: 2.9733
2022-07-14 09:15:39 - train: epoch 0036, iter [03000, 05004], lr: 0.094046, loss: 2.9050
2022-07-14 09:16:14 - train: epoch 0036, iter [03100, 05004], lr: 0.094039, loss: 2.9742
2022-07-14 09:16:48 - train: epoch 0036, iter [03200, 05004], lr: 0.094031, loss: 2.8574
2022-07-14 09:17:23 - train: epoch 0036, iter [03300, 05004], lr: 0.094023, loss: 2.8741
2022-07-14 09:17:57 - train: epoch 0036, iter [03400, 05004], lr: 0.094016, loss: 2.7990
2022-07-14 09:18:31 - train: epoch 0036, iter [03500, 05004], lr: 0.094008, loss: 2.8826
2022-07-14 09:19:06 - train: epoch 0036, iter [03600, 05004], lr: 0.094001, loss: 2.8034
2022-07-14 09:19:41 - train: epoch 0036, iter [03700, 05004], lr: 0.093993, loss: 2.7888
2022-07-14 09:20:15 - train: epoch 0036, iter [03800, 05004], lr: 0.093985, loss: 2.8334
2022-07-14 09:20:49 - train: epoch 0036, iter [03900, 05004], lr: 0.093978, loss: 2.8013
2022-07-14 09:21:25 - train: epoch 0036, iter [04000, 05004], lr: 0.093970, loss: 2.9008
2022-07-14 09:21:59 - train: epoch 0036, iter [04100, 05004], lr: 0.093962, loss: 2.7027
2022-07-14 09:22:34 - train: epoch 0036, iter [04200, 05004], lr: 0.093955, loss: 2.8193
2022-07-14 09:23:08 - train: epoch 0036, iter [04300, 05004], lr: 0.093947, loss: 3.0405
2022-07-14 09:23:41 - train: epoch 0036, iter [04400, 05004], lr: 0.093939, loss: 2.9963
2022-07-14 09:24:16 - train: epoch 0036, iter [04500, 05004], lr: 0.093932, loss: 2.7720
2022-07-14 09:24:50 - train: epoch 0036, iter [04600, 05004], lr: 0.093924, loss: 2.6815
2022-07-14 09:25:23 - train: epoch 0036, iter [04700, 05004], lr: 0.093916, loss: 2.7552
2022-07-14 09:25:58 - train: epoch 0036, iter [04800, 05004], lr: 0.093908, loss: 2.8872
2022-07-14 09:26:31 - train: epoch 0036, iter [04900, 05004], lr: 0.093901, loss: 2.6664
2022-07-14 09:27:04 - train: epoch 0036, iter [05000, 05004], lr: 0.093893, loss: 3.1393
2022-07-14 09:27:05 - train: epoch 036, train_loss: 2.8786
2022-07-14 09:28:18 - eval: epoch: 036, acc1: 56.642%, acc5: 81.404%, test_loss: 1.8344, per_image_load_time: 2.339ms, per_image_inference_time: 0.457ms
2022-07-14 09:28:18 - until epoch: 036, best_acc1: 56.642%
2022-07-14 09:28:18 - epoch 037 lr: 0.093893
2022-07-14 09:28:57 - train: epoch 0037, iter [00100, 05004], lr: 0.093885, loss: 2.7822
2022-07-14 09:29:31 - train: epoch 0037, iter [00200, 05004], lr: 0.093877, loss: 2.5710
2022-07-14 09:30:05 - train: epoch 0037, iter [00300, 05004], lr: 0.093870, loss: 2.9260
2022-07-14 09:30:38 - train: epoch 0037, iter [00400, 05004], lr: 0.093862, loss: 2.6662
2022-07-14 09:31:12 - train: epoch 0037, iter [00500, 05004], lr: 0.093854, loss: 3.0199
2022-07-14 09:31:46 - train: epoch 0037, iter [00600, 05004], lr: 0.093846, loss: 3.0286
2022-07-14 09:32:20 - train: epoch 0037, iter [00700, 05004], lr: 0.093839, loss: 2.7907
2022-07-14 09:32:54 - train: epoch 0037, iter [00800, 05004], lr: 0.093831, loss: 2.7514
2022-07-14 09:33:28 - train: epoch 0037, iter [00900, 05004], lr: 0.093823, loss: 3.1681
2022-07-14 09:34:03 - train: epoch 0037, iter [01000, 05004], lr: 0.093815, loss: 2.6060
2022-07-14 09:34:38 - train: epoch 0037, iter [01100, 05004], lr: 0.093808, loss: 3.1253
2022-07-14 09:35:12 - train: epoch 0037, iter [01200, 05004], lr: 0.093800, loss: 3.0296
2022-07-14 09:35:45 - train: epoch 0037, iter [01300, 05004], lr: 0.093792, loss: 2.9570
2022-07-14 09:36:20 - train: epoch 0037, iter [01400, 05004], lr: 0.093784, loss: 3.2680
2022-07-14 09:36:55 - train: epoch 0037, iter [01500, 05004], lr: 0.093777, loss: 2.9733
2022-07-14 09:37:29 - train: epoch 0037, iter [01600, 05004], lr: 0.093769, loss: 2.4784
2022-07-14 09:38:03 - train: epoch 0037, iter [01700, 05004], lr: 0.093761, loss: 2.8690
2022-07-14 09:38:37 - train: epoch 0037, iter [01800, 05004], lr: 0.093753, loss: 2.8939
2022-07-14 09:39:11 - train: epoch 0037, iter [01900, 05004], lr: 0.093745, loss: 3.1543
2022-07-14 09:39:45 - train: epoch 0037, iter [02000, 05004], lr: 0.093738, loss: 2.9013
2022-07-14 09:40:19 - train: epoch 0037, iter [02100, 05004], lr: 0.093730, loss: 2.9921
2022-07-14 09:40:54 - train: epoch 0037, iter [02200, 05004], lr: 0.093722, loss: 2.7470
2022-07-14 09:41:27 - train: epoch 0037, iter [02300, 05004], lr: 0.093714, loss: 2.9447
2022-07-14 09:42:02 - train: epoch 0037, iter [02400, 05004], lr: 0.093706, loss: 3.0673
2022-07-14 09:42:36 - train: epoch 0037, iter [02500, 05004], lr: 0.093699, loss: 2.7652
2022-07-14 09:43:10 - train: epoch 0037, iter [02600, 05004], lr: 0.093691, loss: 2.9693
2022-07-14 09:43:45 - train: epoch 0037, iter [02700, 05004], lr: 0.093683, loss: 3.1028
2022-07-14 09:44:19 - train: epoch 0037, iter [02800, 05004], lr: 0.093675, loss: 2.8584
2022-07-14 09:44:53 - train: epoch 0037, iter [02900, 05004], lr: 0.093667, loss: 2.8620
2022-07-14 09:45:27 - train: epoch 0037, iter [03000, 05004], lr: 0.093659, loss: 3.1219
2022-07-14 09:46:02 - train: epoch 0037, iter [03100, 05004], lr: 0.093652, loss: 3.1158
2022-07-14 09:46:37 - train: epoch 0037, iter [03200, 05004], lr: 0.093644, loss: 2.8688
2022-07-14 09:47:11 - train: epoch 0037, iter [03300, 05004], lr: 0.093636, loss: 2.7909
2022-07-14 09:47:45 - train: epoch 0037, iter [03400, 05004], lr: 0.093628, loss: 2.7997
2022-07-14 09:48:20 - train: epoch 0037, iter [03500, 05004], lr: 0.093620, loss: 2.6081
2022-07-14 09:48:54 - train: epoch 0037, iter [03600, 05004], lr: 0.093612, loss: 2.8442
2022-07-14 09:49:29 - train: epoch 0037, iter [03700, 05004], lr: 0.093604, loss: 2.5384
2022-07-14 09:50:02 - train: epoch 0037, iter [03800, 05004], lr: 0.093596, loss: 2.9105
2022-07-14 09:50:37 - train: epoch 0037, iter [03900, 05004], lr: 0.093589, loss: 2.7634
2022-07-14 09:51:11 - train: epoch 0037, iter [04000, 05004], lr: 0.093581, loss: 2.9572
2022-07-14 09:51:45 - train: epoch 0037, iter [04100, 05004], lr: 0.093573, loss: 2.6453
2022-07-14 09:52:19 - train: epoch 0037, iter [04200, 05004], lr: 0.093565, loss: 2.9508
2022-07-14 09:52:54 - train: epoch 0037, iter [04300, 05004], lr: 0.093557, loss: 2.9017
2022-07-14 09:53:27 - train: epoch 0037, iter [04400, 05004], lr: 0.093549, loss: 2.8340
2022-07-14 09:54:02 - train: epoch 0037, iter [04500, 05004], lr: 0.093541, loss: 2.7251
2022-07-14 09:54:36 - train: epoch 0037, iter [04600, 05004], lr: 0.093533, loss: 2.9676
2022-07-14 09:55:11 - train: epoch 0037, iter [04700, 05004], lr: 0.093525, loss: 3.0445
2022-07-14 09:55:45 - train: epoch 0037, iter [04800, 05004], lr: 0.093517, loss: 2.8831
2022-07-14 09:56:20 - train: epoch 0037, iter [04900, 05004], lr: 0.093509, loss: 2.9282
2022-07-14 09:56:53 - train: epoch 0037, iter [05000, 05004], lr: 0.093502, loss: 3.0083
2022-07-14 09:56:54 - train: epoch 037, train_loss: 2.8725
2022-07-14 09:58:10 - eval: epoch: 037, acc1: 54.702%, acc5: 79.676%, test_loss: 1.9358, per_image_load_time: 2.496ms, per_image_inference_time: 0.430ms
2022-07-14 09:58:10 - until epoch: 037, best_acc1: 56.642%
2022-07-14 09:58:10 - epoch 038 lr: 0.093501
2022-07-14 09:58:49 - train: epoch 0038, iter [00100, 05004], lr: 0.093493, loss: 2.9282
2022-07-14 09:59:24 - train: epoch 0038, iter [00200, 05004], lr: 0.093485, loss: 2.8147
2022-07-14 09:59:58 - train: epoch 0038, iter [00300, 05004], lr: 0.093477, loss: 2.6413
2022-07-14 10:00:33 - train: epoch 0038, iter [00400, 05004], lr: 0.093469, loss: 2.8773
2022-07-14 10:01:07 - train: epoch 0038, iter [00500, 05004], lr: 0.093462, loss: 2.6394
2022-07-14 10:01:41 - train: epoch 0038, iter [00600, 05004], lr: 0.093454, loss: 3.1318
2022-07-14 10:02:16 - train: epoch 0038, iter [00700, 05004], lr: 0.093446, loss: 2.9281
2022-07-14 10:02:51 - train: epoch 0038, iter [00800, 05004], lr: 0.093438, loss: 2.7965
2022-07-14 10:03:25 - train: epoch 0038, iter [00900, 05004], lr: 0.093430, loss: 3.0083
2022-07-14 10:03:59 - train: epoch 0038, iter [01000, 05004], lr: 0.093422, loss: 3.1455
2022-07-14 10:04:34 - train: epoch 0038, iter [01100, 05004], lr: 0.093414, loss: 2.6992
2022-07-14 10:05:09 - train: epoch 0038, iter [01200, 05004], lr: 0.093406, loss: 3.0503
2022-07-14 10:05:43 - train: epoch 0038, iter [01300, 05004], lr: 0.093398, loss: 2.6644
2022-07-14 10:06:18 - train: epoch 0038, iter [01400, 05004], lr: 0.093390, loss: 2.9987
2022-07-14 10:06:52 - train: epoch 0038, iter [01500, 05004], lr: 0.093382, loss: 2.7549
2022-07-14 10:07:27 - train: epoch 0038, iter [01600, 05004], lr: 0.093374, loss: 2.8930
2022-07-14 10:08:02 - train: epoch 0038, iter [01700, 05004], lr: 0.093366, loss: 3.0114
2022-07-14 10:08:36 - train: epoch 0038, iter [01800, 05004], lr: 0.093358, loss: 2.6715
2022-07-14 10:09:11 - train: epoch 0038, iter [01900, 05004], lr: 0.093350, loss: 2.7806
2022-07-14 10:09:45 - train: epoch 0038, iter [02000, 05004], lr: 0.093342, loss: 2.8779
2022-07-14 10:10:20 - train: epoch 0038, iter [02100, 05004], lr: 0.093334, loss: 2.8500
2022-07-14 10:10:54 - train: epoch 0038, iter [02200, 05004], lr: 0.093326, loss: 2.9022
2022-07-14 10:11:28 - train: epoch 0038, iter [02300, 05004], lr: 0.093318, loss: 3.3493
2022-07-14 10:12:04 - train: epoch 0038, iter [02400, 05004], lr: 0.093309, loss: 2.9544
2022-07-14 10:12:39 - train: epoch 0038, iter [02500, 05004], lr: 0.093301, loss: 2.7894
2022-07-14 10:13:12 - train: epoch 0038, iter [02600, 05004], lr: 0.093293, loss: 2.8625
2022-07-14 10:13:48 - train: epoch 0038, iter [02700, 05004], lr: 0.093285, loss: 2.7599
2022-07-14 10:14:22 - train: epoch 0038, iter [02800, 05004], lr: 0.093277, loss: 3.1031
2022-07-14 10:14:57 - train: epoch 0038, iter [02900, 05004], lr: 0.093269, loss: 3.0626
2022-07-14 10:15:32 - train: epoch 0038, iter [03000, 05004], lr: 0.093261, loss: 2.8103
2022-07-14 10:16:06 - train: epoch 0038, iter [03100, 05004], lr: 0.093253, loss: 2.9546
2022-07-14 10:16:40 - train: epoch 0038, iter [03200, 05004], lr: 0.093245, loss: 2.6013
2022-07-14 10:17:15 - train: epoch 0038, iter [03300, 05004], lr: 0.093237, loss: 2.7504
2022-07-14 10:17:50 - train: epoch 0038, iter [03400, 05004], lr: 0.093229, loss: 2.6218
2022-07-14 10:18:25 - train: epoch 0038, iter [03500, 05004], lr: 0.093221, loss: 3.3340
2022-07-14 10:18:58 - train: epoch 0038, iter [03600, 05004], lr: 0.093213, loss: 2.9212
2022-07-14 10:19:34 - train: epoch 0038, iter [03700, 05004], lr: 0.093205, loss: 2.7491
2022-07-14 10:20:08 - train: epoch 0038, iter [03800, 05004], lr: 0.093196, loss: 2.9110
2022-07-14 10:20:43 - train: epoch 0038, iter [03900, 05004], lr: 0.093188, loss: 3.0517
2022-07-14 10:21:18 - train: epoch 0038, iter [04000, 05004], lr: 0.093180, loss: 2.6618
2022-07-14 10:21:52 - train: epoch 0038, iter [04100, 05004], lr: 0.093172, loss: 2.9868
2022-07-14 10:22:27 - train: epoch 0038, iter [04200, 05004], lr: 0.093164, loss: 2.5525
2022-07-14 10:23:01 - train: epoch 0038, iter [04300, 05004], lr: 0.093156, loss: 2.9762
2022-07-14 10:23:36 - train: epoch 0038, iter [04400, 05004], lr: 0.093148, loss: 2.6888
2022-07-14 10:24:11 - train: epoch 0038, iter [04500, 05004], lr: 0.093140, loss: 2.9697
2022-07-14 10:24:45 - train: epoch 0038, iter [04600, 05004], lr: 0.093131, loss: 2.9737
2022-07-14 10:25:20 - train: epoch 0038, iter [04700, 05004], lr: 0.093123, loss: 2.8000
2022-07-14 10:25:55 - train: epoch 0038, iter [04800, 05004], lr: 0.093115, loss: 2.8240
2022-07-14 10:26:29 - train: epoch 0038, iter [04900, 05004], lr: 0.093107, loss: 2.8879
2022-07-14 10:27:03 - train: epoch 0038, iter [05000, 05004], lr: 0.093099, loss: 2.6814
2022-07-14 10:27:04 - train: epoch 038, train_loss: 2.8642
2022-07-14 10:28:18 - eval: epoch: 038, acc1: 57.648%, acc5: 81.924%, test_loss: 1.7846, per_image_load_time: 2.306ms, per_image_inference_time: 0.482ms
2022-07-14 10:28:19 - until epoch: 038, best_acc1: 57.648%
2022-07-14 10:28:19 - epoch 039 lr: 0.093098
2022-07-14 10:28:57 - train: epoch 0039, iter [00100, 05004], lr: 0.093090, loss: 2.6993
2022-07-14 10:29:32 - train: epoch 0039, iter [00200, 05004], lr: 0.093082, loss: 2.9700
2022-07-14 10:30:07 - train: epoch 0039, iter [00300, 05004], lr: 0.093074, loss: 2.6442
2022-07-14 10:30:41 - train: epoch 0039, iter [00400, 05004], lr: 0.093066, loss: 2.7679
2022-07-14 10:31:15 - train: epoch 0039, iter [00500, 05004], lr: 0.093058, loss: 2.6710
2022-07-14 10:31:50 - train: epoch 0039, iter [00600, 05004], lr: 0.093049, loss: 2.9251
2022-07-14 10:32:25 - train: epoch 0039, iter [00700, 05004], lr: 0.093041, loss: 3.2025
2022-07-14 10:32:59 - train: epoch 0039, iter [00800, 05004], lr: 0.093033, loss: 2.5090
2022-07-14 10:33:33 - train: epoch 0039, iter [00900, 05004], lr: 0.093025, loss: 3.1452
2022-07-14 10:34:07 - train: epoch 0039, iter [01000, 05004], lr: 0.093017, loss: 2.8637
2022-07-14 10:34:41 - train: epoch 0039, iter [01100, 05004], lr: 0.093008, loss: 2.8081
2022-07-14 10:35:14 - train: epoch 0039, iter [01200, 05004], lr: 0.093000, loss: 2.8613
2022-07-14 10:35:47 - train: epoch 0039, iter [01300, 05004], lr: 0.092992, loss: 3.0731
2022-07-14 10:36:21 - train: epoch 0039, iter [01400, 05004], lr: 0.092984, loss: 2.7765
2022-07-14 10:36:54 - train: epoch 0039, iter [01500, 05004], lr: 0.092976, loss: 2.7531
2022-07-14 10:37:28 - train: epoch 0039, iter [01600, 05004], lr: 0.092967, loss: 2.9131
2022-07-14 10:38:02 - train: epoch 0039, iter [01700, 05004], lr: 0.092959, loss: 2.9237
2022-07-14 10:38:36 - train: epoch 0039, iter [01800, 05004], lr: 0.092951, loss: 2.8929
2022-07-14 10:39:10 - train: epoch 0039, iter [01900, 05004], lr: 0.092943, loss: 2.7156
2022-07-14 10:39:44 - train: epoch 0039, iter [02000, 05004], lr: 0.092934, loss: 2.8704
2022-07-14 10:40:18 - train: epoch 0039, iter [02100, 05004], lr: 0.092926, loss: 3.0023
2022-07-14 10:40:51 - train: epoch 0039, iter [02200, 05004], lr: 0.092918, loss: 2.7966
2022-07-14 10:41:24 - train: epoch 0039, iter [02300, 05004], lr: 0.092910, loss: 3.0584
2022-07-14 10:41:59 - train: epoch 0039, iter [02400, 05004], lr: 0.092901, loss: 2.8641
2022-07-14 10:42:32 - train: epoch 0039, iter [02500, 05004], lr: 0.092893, loss: 2.6081
2022-07-14 10:43:06 - train: epoch 0039, iter [02600, 05004], lr: 0.092885, loss: 2.8143
2022-07-14 10:43:40 - train: epoch 0039, iter [02700, 05004], lr: 0.092877, loss: 2.9235
2022-07-14 10:44:13 - train: epoch 0039, iter [02800, 05004], lr: 0.092868, loss: 2.8180
2022-07-14 10:44:48 - train: epoch 0039, iter [02900, 05004], lr: 0.092860, loss: 2.6800
2022-07-14 10:45:21 - train: epoch 0039, iter [03000, 05004], lr: 0.092852, loss: 2.6712
2022-07-14 10:45:55 - train: epoch 0039, iter [03100, 05004], lr: 0.092843, loss: 2.7143
2022-07-14 10:46:29 - train: epoch 0039, iter [03200, 05004], lr: 0.092835, loss: 2.7902
2022-07-14 10:47:03 - train: epoch 0039, iter [03300, 05004], lr: 0.092827, loss: 2.7940
2022-07-14 10:47:37 - train: epoch 0039, iter [03400, 05004], lr: 0.092818, loss: 2.6447
2022-07-14 10:48:12 - train: epoch 0039, iter [03500, 05004], lr: 0.092810, loss: 3.1559
2022-07-14 10:48:46 - train: epoch 0039, iter [03600, 05004], lr: 0.092802, loss: 3.0784
2022-07-14 10:49:20 - train: epoch 0039, iter [03700, 05004], lr: 0.092793, loss: 2.8418
2022-07-14 10:49:54 - train: epoch 0039, iter [03800, 05004], lr: 0.092785, loss: 2.8398
2022-07-14 10:50:27 - train: epoch 0039, iter [03900, 05004], lr: 0.092777, loss: 2.9403
2022-07-14 10:51:02 - train: epoch 0039, iter [04000, 05004], lr: 0.092768, loss: 2.7512
2022-07-14 10:51:37 - train: epoch 0039, iter [04100, 05004], lr: 0.092760, loss: 2.8376
2022-07-14 10:52:10 - train: epoch 0039, iter [04200, 05004], lr: 0.092752, loss: 2.7342
2022-07-14 10:52:44 - train: epoch 0039, iter [04300, 05004], lr: 0.092743, loss: 2.7514
2022-07-14 10:53:18 - train: epoch 0039, iter [04400, 05004], lr: 0.092735, loss: 2.8376
2022-07-14 10:53:52 - train: epoch 0039, iter [04500, 05004], lr: 0.092727, loss: 2.9928
2022-07-14 10:54:27 - train: epoch 0039, iter [04600, 05004], lr: 0.092718, loss: 2.9984
2022-07-14 10:55:01 - train: epoch 0039, iter [04700, 05004], lr: 0.092710, loss: 2.8514
2022-07-14 10:55:35 - train: epoch 0039, iter [04800, 05004], lr: 0.092702, loss: 3.0790
2022-07-14 10:56:09 - train: epoch 0039, iter [04900, 05004], lr: 0.092693, loss: 2.7290
2022-07-14 10:56:42 - train: epoch 0039, iter [05000, 05004], lr: 0.092685, loss: 2.8822
2022-07-14 10:56:44 - train: epoch 039, train_loss: 2.8617
2022-07-14 10:57:58 - eval: epoch: 039, acc1: 54.722%, acc5: 79.960%, test_loss: 1.9122, per_image_load_time: 2.421ms, per_image_inference_time: 0.451ms
2022-07-14 10:57:58 - until epoch: 039, best_acc1: 57.648%
2022-07-14 10:57:58 - epoch 040 lr: 0.092684
2022-07-14 10:58:38 - train: epoch 0040, iter [00100, 05004], lr: 0.092676, loss: 3.0159
2022-07-14 10:59:12 - train: epoch 0040, iter [00200, 05004], lr: 0.092668, loss: 2.6903
2022-07-14 10:59:47 - train: epoch 0040, iter [00300, 05004], lr: 0.092659, loss: 3.0174
2022-07-14 11:00:21 - train: epoch 0040, iter [00400, 05004], lr: 0.092651, loss: 2.6124
2022-07-14 11:00:55 - train: epoch 0040, iter [00500, 05004], lr: 0.092643, loss: 2.8056
2022-07-14 11:01:30 - train: epoch 0040, iter [00600, 05004], lr: 0.092634, loss: 2.6405
2022-07-14 11:02:04 - train: epoch 0040, iter [00700, 05004], lr: 0.092626, loss: 2.9023
2022-07-14 11:02:39 - train: epoch 0040, iter [00800, 05004], lr: 0.092617, loss: 2.8027
2022-07-14 11:03:13 - train: epoch 0040, iter [00900, 05004], lr: 0.092609, loss: 2.6479
2022-07-14 11:03:47 - train: epoch 0040, iter [01000, 05004], lr: 0.092600, loss: 2.5728
2022-07-14 11:04:22 - train: epoch 0040, iter [01100, 05004], lr: 0.092592, loss: 2.8053
2022-07-14 11:04:56 - train: epoch 0040, iter [01200, 05004], lr: 0.092584, loss: 2.8975
2022-07-14 11:05:31 - train: epoch 0040, iter [01300, 05004], lr: 0.092575, loss: 2.6971
2022-07-14 11:06:06 - train: epoch 0040, iter [01400, 05004], lr: 0.092567, loss: 2.8281
2022-07-14 11:06:39 - train: epoch 0040, iter [01500, 05004], lr: 0.092558, loss: 2.8878
2022-07-14 11:07:13 - train: epoch 0040, iter [01600, 05004], lr: 0.092550, loss: 2.8519
2022-07-14 11:07:46 - train: epoch 0040, iter [01700, 05004], lr: 0.092541, loss: 3.0836
2022-07-14 11:08:20 - train: epoch 0040, iter [01800, 05004], lr: 0.092533, loss: 2.8167
2022-07-14 11:08:55 - train: epoch 0040, iter [01900, 05004], lr: 0.092524, loss: 2.7956
2022-07-14 11:09:29 - train: epoch 0040, iter [02000, 05004], lr: 0.092516, loss: 2.9728
2022-07-14 11:10:02 - train: epoch 0040, iter [02100, 05004], lr: 0.092508, loss: 2.7314
2022-07-14 11:10:36 - train: epoch 0040, iter [02200, 05004], lr: 0.092499, loss: 2.7129
2022-07-14 11:11:10 - train: epoch 0040, iter [02300, 05004], lr: 0.092491, loss: 2.7499
2022-07-14 11:11:45 - train: epoch 0040, iter [02400, 05004], lr: 0.092482, loss: 2.9417
2022-07-14 11:12:18 - train: epoch 0040, iter [02500, 05004], lr: 0.092474, loss: 3.0306
2022-07-14 11:12:53 - train: epoch 0040, iter [02600, 05004], lr: 0.092465, loss: 3.0659
2022-07-14 11:13:27 - train: epoch 0040, iter [02700, 05004], lr: 0.092457, loss: 2.8549
2022-07-14 11:14:01 - train: epoch 0040, iter [02800, 05004], lr: 0.092448, loss: 2.6615
2022-07-14 11:14:35 - train: epoch 0040, iter [02900, 05004], lr: 0.092440, loss: 3.0061
2022-07-14 11:15:09 - train: epoch 0040, iter [03000, 05004], lr: 0.092431, loss: 2.6590
2022-07-14 11:15:43 - train: epoch 0040, iter [03100, 05004], lr: 0.092423, loss: 2.8421
2022-07-14 11:16:17 - train: epoch 0040, iter [03200, 05004], lr: 0.092414, loss: 3.0053
2022-07-14 11:16:50 - train: epoch 0040, iter [03300, 05004], lr: 0.092405, loss: 2.7757
2022-07-14 11:17:25 - train: epoch 0040, iter [03400, 05004], lr: 0.092397, loss: 2.7451
2022-07-14 11:17:59 - train: epoch 0040, iter [03500, 05004], lr: 0.092388, loss: 2.8471
2022-07-14 11:18:33 - train: epoch 0040, iter [03600, 05004], lr: 0.092380, loss: 2.9216
2022-07-14 11:19:06 - train: epoch 0040, iter [03700, 05004], lr: 0.092371, loss: 2.9586
2022-07-14 11:19:40 - train: epoch 0040, iter [03800, 05004], lr: 0.092363, loss: 2.9775
2022-07-14 11:20:14 - train: epoch 0040, iter [03900, 05004], lr: 0.092354, loss: 2.8181
2022-07-14 11:20:48 - train: epoch 0040, iter [04000, 05004], lr: 0.092346, loss: 2.8879
2022-07-14 11:21:21 - train: epoch 0040, iter [04100, 05004], lr: 0.092337, loss: 2.6056
2022-07-14 11:21:56 - train: epoch 0040, iter [04200, 05004], lr: 0.092329, loss: 2.9911
2022-07-14 11:22:29 - train: epoch 0040, iter [04300, 05004], lr: 0.092320, loss: 2.6763
2022-07-14 11:23:04 - train: epoch 0040, iter [04400, 05004], lr: 0.092311, loss: 2.9603
2022-07-14 11:23:38 - train: epoch 0040, iter [04500, 05004], lr: 0.092303, loss: 2.9301
2022-07-14 11:24:13 - train: epoch 0040, iter [04600, 05004], lr: 0.092294, loss: 2.9730
2022-07-14 11:24:47 - train: epoch 0040, iter [04700, 05004], lr: 0.092286, loss: 2.7101
2022-07-14 11:25:21 - train: epoch 0040, iter [04800, 05004], lr: 0.092277, loss: 2.9868
2022-07-14 11:25:55 - train: epoch 0040, iter [04900, 05004], lr: 0.092268, loss: 2.7304
2022-07-14 11:26:27 - train: epoch 0040, iter [05000, 05004], lr: 0.092260, loss: 2.8156
2022-07-14 11:26:29 - train: epoch 040, train_loss: 2.8590
2022-07-14 11:27:42 - eval: epoch: 040, acc1: 55.872%, acc5: 80.812%, test_loss: 1.8650, per_image_load_time: 2.315ms, per_image_inference_time: 0.459ms
2022-07-14 11:27:42 - until epoch: 040, best_acc1: 57.648%
2022-07-14 11:27:42 - epoch 041 lr: 0.092259
2022-07-14 11:28:21 - train: epoch 0041, iter [00100, 05004], lr: 0.092251, loss: 3.1904
2022-07-14 11:28:55 - train: epoch 0041, iter [00200, 05004], lr: 0.092242, loss: 3.1838
2022-07-14 11:29:29 - train: epoch 0041, iter [00300, 05004], lr: 0.092234, loss: 3.2256
2022-07-14 11:30:03 - train: epoch 0041, iter [00400, 05004], lr: 0.092225, loss: 2.8167
2022-07-14 11:30:37 - train: epoch 0041, iter [00500, 05004], lr: 0.092216, loss: 2.7829
2022-07-14 11:31:12 - train: epoch 0041, iter [00600, 05004], lr: 0.092208, loss: 3.0703
2022-07-14 11:31:45 - train: epoch 0041, iter [00700, 05004], lr: 0.092199, loss: 2.8840
2022-07-14 11:32:19 - train: epoch 0041, iter [00800, 05004], lr: 0.092191, loss: 2.6770
2022-07-14 11:32:52 - train: epoch 0041, iter [00900, 05004], lr: 0.092182, loss: 2.4436
2022-07-14 11:33:26 - train: epoch 0041, iter [01000, 05004], lr: 0.092173, loss: 2.9975
2022-07-14 11:34:00 - train: epoch 0041, iter [01100, 05004], lr: 0.092165, loss: 2.8010
2022-07-14 11:34:33 - train: epoch 0041, iter [01200, 05004], lr: 0.092156, loss: 2.6948
2022-07-14 11:35:07 - train: epoch 0041, iter [01300, 05004], lr: 0.092147, loss: 2.7185
2022-07-14 11:35:41 - train: epoch 0041, iter [01400, 05004], lr: 0.092139, loss: 2.9145
2022-07-14 11:36:15 - train: epoch 0041, iter [01500, 05004], lr: 0.092130, loss: 3.2006
2022-07-14 11:36:50 - train: epoch 0041, iter [01600, 05004], lr: 0.092121, loss: 2.8824
2022-07-14 11:37:23 - train: epoch 0041, iter [01700, 05004], lr: 0.092113, loss: 2.9733
2022-07-14 11:37:57 - train: epoch 0041, iter [01800, 05004], lr: 0.092104, loss: 2.6910
2022-07-14 11:38:32 - train: epoch 0041, iter [01900, 05004], lr: 0.092095, loss: 3.0473
2022-07-14 11:39:06 - train: epoch 0041, iter [02000, 05004], lr: 0.092087, loss: 2.7757
2022-07-14 11:39:40 - train: epoch 0041, iter [02100, 05004], lr: 0.092078, loss: 2.5483
2022-07-14 11:40:14 - train: epoch 0041, iter [02200, 05004], lr: 0.092069, loss: 2.7215
2022-07-14 11:40:49 - train: epoch 0041, iter [02300, 05004], lr: 0.092060, loss: 2.8373
2022-07-14 11:41:23 - train: epoch 0041, iter [02400, 05004], lr: 0.092052, loss: 3.1836
2022-07-14 11:41:57 - train: epoch 0041, iter [02500, 05004], lr: 0.092043, loss: 2.6996
2022-07-14 11:42:31 - train: epoch 0041, iter [02600, 05004], lr: 0.092034, loss: 3.1312
2022-07-14 11:43:05 - train: epoch 0041, iter [02700, 05004], lr: 0.092026, loss: 3.0287
2022-07-14 11:43:40 - train: epoch 0041, iter [02800, 05004], lr: 0.092017, loss: 2.8934
2022-07-14 11:44:14 - train: epoch 0041, iter [02900, 05004], lr: 0.092008, loss: 2.9207
2022-07-14 11:44:48 - train: epoch 0041, iter [03000, 05004], lr: 0.091999, loss: 2.9231
2022-07-14 11:45:22 - train: epoch 0041, iter [03100, 05004], lr: 0.091991, loss: 3.0088
2022-07-14 11:45:56 - train: epoch 0041, iter [03200, 05004], lr: 0.091982, loss: 2.8985
2022-07-14 11:46:30 - train: epoch 0041, iter [03300, 05004], lr: 0.091973, loss: 2.7765
2022-07-14 11:47:04 - train: epoch 0041, iter [03400, 05004], lr: 0.091964, loss: 2.8337
2022-07-14 11:47:38 - train: epoch 0041, iter [03500, 05004], lr: 0.091956, loss: 2.7709
2022-07-14 11:48:12 - train: epoch 0041, iter [03600, 05004], lr: 0.091947, loss: 2.9307
2022-07-14 11:48:46 - train: epoch 0041, iter [03700, 05004], lr: 0.091938, loss: 2.9669
2022-07-14 11:49:19 - train: epoch 0041, iter [03800, 05004], lr: 0.091929, loss: 2.5828
2022-07-14 11:49:52 - train: epoch 0041, iter [03900, 05004], lr: 0.091921, loss: 2.7760
2022-07-14 11:50:26 - train: epoch 0041, iter [04000, 05004], lr: 0.091912, loss: 2.8252
2022-07-14 11:50:59 - train: epoch 0041, iter [04100, 05004], lr: 0.091903, loss: 2.9469
2022-07-14 11:51:32 - train: epoch 0041, iter [04200, 05004], lr: 0.091894, loss: 3.0850
2022-07-14 11:52:06 - train: epoch 0041, iter [04300, 05004], lr: 0.091886, loss: 2.7341
2022-07-14 11:52:40 - train: epoch 0041, iter [04400, 05004], lr: 0.091877, loss: 2.6705
2022-07-14 11:53:13 - train: epoch 0041, iter [04500, 05004], lr: 0.091868, loss: 2.8076
2022-07-14 11:53:47 - train: epoch 0041, iter [04600, 05004], lr: 0.091859, loss: 2.7324
2022-07-14 11:54:21 - train: epoch 0041, iter [04700, 05004], lr: 0.091850, loss: 2.8952
2022-07-14 11:54:55 - train: epoch 0041, iter [04800, 05004], lr: 0.091841, loss: 2.8667
2022-07-14 11:55:28 - train: epoch 0041, iter [04900, 05004], lr: 0.091833, loss: 2.9698
2022-07-14 11:56:01 - train: epoch 0041, iter [05000, 05004], lr: 0.091824, loss: 3.0181
2022-07-14 11:56:02 - train: epoch 041, train_loss: 2.8516
2022-07-14 11:57:15 - eval: epoch: 041, acc1: 55.466%, acc5: 80.548%, test_loss: 1.8860, per_image_load_time: 2.406ms, per_image_inference_time: 0.438ms
2022-07-14 11:57:15 - until epoch: 041, best_acc1: 57.648%
2022-07-14 11:57:15 - epoch 042 lr: 0.091823
2022-07-14 11:57:54 - train: epoch 0042, iter [00100, 05004], lr: 0.091815, loss: 2.3934
2022-07-14 11:58:29 - train: epoch 0042, iter [00200, 05004], lr: 0.091806, loss: 2.7855
2022-07-14 11:59:02 - train: epoch 0042, iter [00300, 05004], lr: 0.091797, loss: 2.9562
2022-07-14 11:59:37 - train: epoch 0042, iter [00400, 05004], lr: 0.091788, loss: 2.9786
2022-07-14 12:00:10 - train: epoch 0042, iter [00500, 05004], lr: 0.091779, loss: 2.8782
2022-07-14 12:00:43 - train: epoch 0042, iter [00600, 05004], lr: 0.091770, loss: 3.0580
2022-07-14 12:01:17 - train: epoch 0042, iter [00700, 05004], lr: 0.091762, loss: 2.9655
2022-07-14 12:01:51 - train: epoch 0042, iter [00800, 05004], lr: 0.091753, loss: 2.6689
2022-07-14 12:02:24 - train: epoch 0042, iter [00900, 05004], lr: 0.091744, loss: 2.7266
2022-07-14 12:02:58 - train: epoch 0042, iter [01000, 05004], lr: 0.091735, loss: 3.1741
2022-07-14 12:03:33 - train: epoch 0042, iter [01100, 05004], lr: 0.091726, loss: 2.7751
2022-07-14 12:04:07 - train: epoch 0042, iter [01200, 05004], lr: 0.091717, loss: 2.8546
2022-07-14 12:04:42 - train: epoch 0042, iter [01300, 05004], lr: 0.091708, loss: 3.0613
2022-07-14 12:05:15 - train: epoch 0042, iter [01400, 05004], lr: 0.091700, loss: 2.8552
2022-07-14 12:05:49 - train: epoch 0042, iter [01500, 05004], lr: 0.091691, loss: 2.5392
2022-07-14 12:06:23 - train: epoch 0042, iter [01600, 05004], lr: 0.091682, loss: 2.8483
2022-07-14 12:06:56 - train: epoch 0042, iter [01700, 05004], lr: 0.091673, loss: 2.6321
2022-07-14 12:07:30 - train: epoch 0042, iter [01800, 05004], lr: 0.091664, loss: 2.9205
2022-07-14 12:08:04 - train: epoch 0042, iter [01900, 05004], lr: 0.091655, loss: 3.1410
2022-07-14 12:08:38 - train: epoch 0042, iter [02000, 05004], lr: 0.091646, loss: 2.7486
2022-07-14 12:09:12 - train: epoch 0042, iter [02100, 05004], lr: 0.091637, loss: 2.5989
2022-07-14 12:09:47 - train: epoch 0042, iter [02200, 05004], lr: 0.091628, loss: 2.7119
2022-07-14 12:10:20 - train: epoch 0042, iter [02300, 05004], lr: 0.091619, loss: 3.0012
2022-07-14 12:10:55 - train: epoch 0042, iter [02400, 05004], lr: 0.091611, loss: 2.5386
2022-07-14 12:11:29 - train: epoch 0042, iter [02500, 05004], lr: 0.091602, loss: 2.9147
2022-07-14 12:12:04 - train: epoch 0042, iter [02600, 05004], lr: 0.091593, loss: 2.9400
2022-07-14 12:12:38 - train: epoch 0042, iter [02700, 05004], lr: 0.091584, loss: 2.7061
2022-07-14 12:13:13 - train: epoch 0042, iter [02800, 05004], lr: 0.091575, loss: 2.9164
2022-07-14 12:13:47 - train: epoch 0042, iter [02900, 05004], lr: 0.091566, loss: 3.1696
2022-07-14 12:14:21 - train: epoch 0042, iter [03000, 05004], lr: 0.091557, loss: 2.8351
2022-07-14 12:14:56 - train: epoch 0042, iter [03100, 05004], lr: 0.091548, loss: 2.8671
2022-07-14 12:15:30 - train: epoch 0042, iter [03200, 05004], lr: 0.091539, loss: 2.7635
2022-07-14 12:16:04 - train: epoch 0042, iter [03300, 05004], lr: 0.091530, loss: 2.9813
2022-07-14 12:16:39 - train: epoch 0042, iter [03400, 05004], lr: 0.091521, loss: 2.8282
2022-07-14 12:17:13 - train: epoch 0042, iter [03500, 05004], lr: 0.091512, loss: 2.7479
2022-07-14 12:17:47 - train: epoch 0042, iter [03600, 05004], lr: 0.091503, loss: 2.8184
2022-07-14 12:18:21 - train: epoch 0042, iter [03700, 05004], lr: 0.091494, loss: 2.6642
2022-07-14 12:18:57 - train: epoch 0042, iter [03800, 05004], lr: 0.091485, loss: 2.6962
2022-07-14 12:19:30 - train: epoch 0042, iter [03900, 05004], lr: 0.091476, loss: 2.9141
2022-07-14 12:20:05 - train: epoch 0042, iter [04000, 05004], lr: 0.091467, loss: 2.4818
2022-07-14 12:20:40 - train: epoch 0042, iter [04100, 05004], lr: 0.091458, loss: 2.7156
2022-07-14 12:21:13 - train: epoch 0042, iter [04200, 05004], lr: 0.091449, loss: 2.9722
2022-07-14 12:21:48 - train: epoch 0042, iter [04300, 05004], lr: 0.091440, loss: 2.4843
2022-07-14 12:22:22 - train: epoch 0042, iter [04400, 05004], lr: 0.091431, loss: 2.8620
2022-07-14 12:22:56 - train: epoch 0042, iter [04500, 05004], lr: 0.091422, loss: 2.7463
2022-07-14 12:23:30 - train: epoch 0042, iter [04600, 05004], lr: 0.091413, loss: 3.0377
2022-07-14 12:24:04 - train: epoch 0042, iter [04700, 05004], lr: 0.091404, loss: 2.7757
2022-07-14 12:24:39 - train: epoch 0042, iter [04800, 05004], lr: 0.091395, loss: 2.7860
2022-07-14 12:25:12 - train: epoch 0042, iter [04900, 05004], lr: 0.091386, loss: 2.6930
2022-07-14 12:25:45 - train: epoch 0042, iter [05000, 05004], lr: 0.091377, loss: 2.8846
2022-07-14 12:25:46 - train: epoch 042, train_loss: 2.8447
2022-07-14 12:26:59 - eval: epoch: 042, acc1: 57.872%, acc5: 81.796%, test_loss: 1.7820, per_image_load_time: 2.407ms, per_image_inference_time: 0.447ms
2022-07-14 12:26:59 - until epoch: 042, best_acc1: 57.872%
2022-07-14 12:26:59 - epoch 043 lr: 0.091377
2022-07-14 12:27:39 - train: epoch 0043, iter [00100, 05004], lr: 0.091368, loss: 2.8567
2022-07-14 12:28:13 - train: epoch 0043, iter [00200, 05004], lr: 0.091359, loss: 3.0405
2022-07-14 12:28:47 - train: epoch 0043, iter [00300, 05004], lr: 0.091350, loss: 2.5146
2022-07-14 12:29:22 - train: epoch 0043, iter [00400, 05004], lr: 0.091340, loss: 2.9328
2022-07-14 12:29:56 - train: epoch 0043, iter [00500, 05004], lr: 0.091331, loss: 2.9054
2022-07-14 12:30:30 - train: epoch 0043, iter [00600, 05004], lr: 0.091322, loss: 2.9532
2022-07-14 12:31:04 - train: epoch 0043, iter [00700, 05004], lr: 0.091313, loss: 2.7879
2022-07-14 12:31:38 - train: epoch 0043, iter [00800, 05004], lr: 0.091304, loss: 2.8654
2022-07-14 12:32:12 - train: epoch 0043, iter [00900, 05004], lr: 0.091295, loss: 2.7930
2022-07-14 12:32:46 - train: epoch 0043, iter [01000, 05004], lr: 0.091286, loss: 2.8650
2022-07-14 12:33:20 - train: epoch 0043, iter [01100, 05004], lr: 0.091277, loss: 2.6652
2022-07-14 12:33:54 - train: epoch 0043, iter [01200, 05004], lr: 0.091268, loss: 2.8736
2022-07-14 12:34:28 - train: epoch 0043, iter [01300, 05004], lr: 0.091259, loss: 2.8471
2022-07-14 12:35:01 - train: epoch 0043, iter [01400, 05004], lr: 0.091250, loss: 2.5681
2022-07-14 12:35:35 - train: epoch 0043, iter [01500, 05004], lr: 0.091241, loss: 2.7024
2022-07-14 12:36:08 - train: epoch 0043, iter [01600, 05004], lr: 0.091232, loss: 2.8016
2022-07-14 12:36:42 - train: epoch 0043, iter [01700, 05004], lr: 0.091222, loss: 3.0437
2022-07-14 12:37:16 - train: epoch 0043, iter [01800, 05004], lr: 0.091213, loss: 2.9484
2022-07-14 12:37:50 - train: epoch 0043, iter [01900, 05004], lr: 0.091204, loss: 2.6973
2022-07-14 12:38:24 - train: epoch 0043, iter [02000, 05004], lr: 0.091195, loss: 2.8042
2022-07-14 12:38:58 - train: epoch 0043, iter [02100, 05004], lr: 0.091186, loss: 2.4916
2022-07-14 12:39:32 - train: epoch 0043, iter [02200, 05004], lr: 0.091177, loss: 3.1045
2022-07-14 12:40:06 - train: epoch 0043, iter [02300, 05004], lr: 0.091168, loss: 2.9967
2022-07-14 12:40:41 - train: epoch 0043, iter [02400, 05004], lr: 0.091159, loss: 2.4297
2022-07-14 12:41:15 - train: epoch 0043, iter [02500, 05004], lr: 0.091149, loss: 2.9830
2022-07-14 12:41:49 - train: epoch 0043, iter [02600, 05004], lr: 0.091140, loss: 2.9568
2022-07-14 12:42:22 - train: epoch 0043, iter [02700, 05004], lr: 0.091131, loss: 3.0170
2022-07-14 12:42:57 - train: epoch 0043, iter [02800, 05004], lr: 0.091122, loss: 2.5562
2022-07-14 12:43:31 - train: epoch 0043, iter [02900, 05004], lr: 0.091113, loss: 2.6809
2022-07-14 12:44:05 - train: epoch 0043, iter [03000, 05004], lr: 0.091104, loss: 2.6970
2022-07-14 12:44:40 - train: epoch 0043, iter [03100, 05004], lr: 0.091094, loss: 3.0956
2022-07-14 12:45:15 - train: epoch 0043, iter [03200, 05004], lr: 0.091085, loss: 3.1426
2022-07-14 12:45:49 - train: epoch 0043, iter [03300, 05004], lr: 0.091076, loss: 2.7026
2022-07-14 12:46:23 - train: epoch 0043, iter [03400, 05004], lr: 0.091067, loss: 2.6569
2022-07-14 12:46:57 - train: epoch 0043, iter [03500, 05004], lr: 0.091058, loss: 2.8873
2022-07-14 12:47:31 - train: epoch 0043, iter [03600, 05004], lr: 0.091049, loss: 3.1990
2022-07-14 12:48:06 - train: epoch 0043, iter [03700, 05004], lr: 0.091039, loss: 2.7717
2022-07-14 12:48:40 - train: epoch 0043, iter [03800, 05004], lr: 0.091030, loss: 2.9839
2022-07-14 12:49:14 - train: epoch 0043, iter [03900, 05004], lr: 0.091021, loss: 2.8551
2022-07-14 12:49:49 - train: epoch 0043, iter [04000, 05004], lr: 0.091012, loss: 3.0296
2022-07-14 12:50:23 - train: epoch 0043, iter [04100, 05004], lr: 0.091003, loss: 2.9108
2022-07-14 12:50:57 - train: epoch 0043, iter [04200, 05004], lr: 0.090993, loss: 2.6890
2022-07-14 12:51:32 - train: epoch 0043, iter [04300, 05004], lr: 0.090984, loss: 2.9478
2022-07-14 12:52:06 - train: epoch 0043, iter [04400, 05004], lr: 0.090975, loss: 2.7805
2022-07-14 12:52:40 - train: epoch 0043, iter [04500, 05004], lr: 0.090966, loss: 2.8059
2022-07-14 12:53:14 - train: epoch 0043, iter [04600, 05004], lr: 0.090956, loss: 3.1120
2022-07-14 12:53:49 - train: epoch 0043, iter [04700, 05004], lr: 0.090947, loss: 2.9686
2022-07-14 12:54:23 - train: epoch 0043, iter [04800, 05004], lr: 0.090938, loss: 2.6447
2022-07-14 12:54:57 - train: epoch 0043, iter [04900, 05004], lr: 0.090929, loss: 2.8509
2022-07-14 12:55:31 - train: epoch 0043, iter [05000, 05004], lr: 0.090919, loss: 2.6893
2022-07-14 12:55:32 - train: epoch 043, train_loss: 2.8389
2022-07-14 12:56:45 - eval: epoch: 043, acc1: 53.338%, acc5: 78.836%, test_loss: 1.9930, per_image_load_time: 2.366ms, per_image_inference_time: 0.465ms
2022-07-14 12:56:46 - until epoch: 043, best_acc1: 57.872%
2022-07-14 12:56:46 - epoch 044 lr: 0.090919
2022-07-14 12:57:26 - train: epoch 0044, iter [00100, 05004], lr: 0.090910, loss: 2.6827
2022-07-14 12:57:59 - train: epoch 0044, iter [00200, 05004], lr: 0.090901, loss: 2.6736
2022-07-14 12:58:33 - train: epoch 0044, iter [00300, 05004], lr: 0.090891, loss: 2.9929
2022-07-14 12:59:06 - train: epoch 0044, iter [00400, 05004], lr: 0.090882, loss: 2.4492
2022-07-14 12:59:38 - train: epoch 0044, iter [00500, 05004], lr: 0.090873, loss: 2.7201
2022-07-14 13:00:12 - train: epoch 0044, iter [00600, 05004], lr: 0.090863, loss: 2.7842
2022-07-14 13:00:45 - train: epoch 0044, iter [00700, 05004], lr: 0.090854, loss: 2.8116
2022-07-14 13:01:18 - train: epoch 0044, iter [00800, 05004], lr: 0.090845, loss: 2.6678
2022-07-14 13:01:52 - train: epoch 0044, iter [00900, 05004], lr: 0.090836, loss: 2.6179
2022-07-14 13:02:25 - train: epoch 0044, iter [01000, 05004], lr: 0.090826, loss: 2.6842
2022-07-14 13:02:59 - train: epoch 0044, iter [01100, 05004], lr: 0.090817, loss: 2.6483
2022-07-14 13:03:32 - train: epoch 0044, iter [01200, 05004], lr: 0.090808, loss: 2.6167
2022-07-14 13:04:05 - train: epoch 0044, iter [01300, 05004], lr: 0.090798, loss: 2.8852
2022-07-14 13:04:39 - train: epoch 0044, iter [01400, 05004], lr: 0.090789, loss: 3.0534
2022-07-14 13:05:12 - train: epoch 0044, iter [01500, 05004], lr: 0.090780, loss: 2.8817
2022-07-14 13:05:47 - train: epoch 0044, iter [01600, 05004], lr: 0.090771, loss: 3.0605
2022-07-14 13:06:20 - train: epoch 0044, iter [01700, 05004], lr: 0.090761, loss: 2.6992
2022-07-14 13:06:55 - train: epoch 0044, iter [01800, 05004], lr: 0.090752, loss: 2.9324
2022-07-14 13:07:27 - train: epoch 0044, iter [01900, 05004], lr: 0.090743, loss: 3.1023
2022-07-14 13:08:02 - train: epoch 0044, iter [02000, 05004], lr: 0.090733, loss: 2.6879
2022-07-14 13:08:35 - train: epoch 0044, iter [02100, 05004], lr: 0.090724, loss: 2.9529
2022-07-14 13:09:09 - train: epoch 0044, iter [02200, 05004], lr: 0.090715, loss: 3.0102
2022-07-14 13:09:43 - train: epoch 0044, iter [02300, 05004], lr: 0.090705, loss: 2.9367
2022-07-14 13:10:16 - train: epoch 0044, iter [02400, 05004], lr: 0.090696, loss: 2.6554
2022-07-14 13:10:51 - train: epoch 0044, iter [02500, 05004], lr: 0.090686, loss: 2.9098
2022-07-14 13:11:24 - train: epoch 0044, iter [02600, 05004], lr: 0.090677, loss: 2.9923
2022-07-14 13:11:58 - train: epoch 0044, iter [02700, 05004], lr: 0.090668, loss: 3.0457
2022-07-14 13:12:32 - train: epoch 0044, iter [02800, 05004], lr: 0.090658, loss: 2.4466
2022-07-14 13:13:06 - train: epoch 0044, iter [02900, 05004], lr: 0.090649, loss: 2.8356
2022-07-14 13:13:39 - train: epoch 0044, iter [03000, 05004], lr: 0.090640, loss: 2.7771
2022-07-14 13:14:14 - train: epoch 0044, iter [03100, 05004], lr: 0.090630, loss: 2.7391
2022-07-14 13:14:46 - train: epoch 0044, iter [03200, 05004], lr: 0.090621, loss: 2.6065
2022-07-14 13:15:21 - train: epoch 0044, iter [03300, 05004], lr: 0.090611, loss: 2.7105
2022-07-14 13:15:55 - train: epoch 0044, iter [03400, 05004], lr: 0.090602, loss: 2.9975
2022-07-14 13:16:29 - train: epoch 0044, iter [03500, 05004], lr: 0.090593, loss: 2.9277
2022-07-14 13:17:03 - train: epoch 0044, iter [03600, 05004], lr: 0.090583, loss: 3.0006
2022-07-14 13:17:36 - train: epoch 0044, iter [03700, 05004], lr: 0.090574, loss: 2.8922
2022-07-14 13:18:12 - train: epoch 0044, iter [03800, 05004], lr: 0.090564, loss: 2.8637
2022-07-14 13:18:45 - train: epoch 0044, iter [03900, 05004], lr: 0.090555, loss: 2.8627
2022-07-14 13:19:20 - train: epoch 0044, iter [04000, 05004], lr: 0.090546, loss: 2.8686
2022-07-14 13:19:53 - train: epoch 0044, iter [04100, 05004], lr: 0.090536, loss: 2.5507
2022-07-14 13:20:28 - train: epoch 0044, iter [04200, 05004], lr: 0.090527, loss: 2.9918
2022-07-14 13:21:02 - train: epoch 0044, iter [04300, 05004], lr: 0.090517, loss: 2.9578
2022-07-14 13:21:36 - train: epoch 0044, iter [04400, 05004], lr: 0.090508, loss: 2.9023
2022-07-14 13:22:10 - train: epoch 0044, iter [04500, 05004], lr: 0.090498, loss: 2.6797
2022-07-14 13:22:43 - train: epoch 0044, iter [04600, 05004], lr: 0.090489, loss: 3.1734
2022-07-14 13:23:18 - train: epoch 0044, iter [04700, 05004], lr: 0.090480, loss: 3.0287
2022-07-14 13:23:51 - train: epoch 0044, iter [04800, 05004], lr: 0.090470, loss: 2.9622
2022-07-14 13:24:26 - train: epoch 0044, iter [04900, 05004], lr: 0.090461, loss: 2.6309
2022-07-14 13:24:58 - train: epoch 0044, iter [05000, 05004], lr: 0.090451, loss: 3.0698
2022-07-14 13:25:00 - train: epoch 044, train_loss: 2.8388
2022-07-14 13:26:13 - eval: epoch: 044, acc1: 52.940%, acc5: 78.228%, test_loss: 2.0272, per_image_load_time: 2.396ms, per_image_inference_time: 0.460ms
2022-07-14 13:26:14 - until epoch: 044, best_acc1: 57.872%
2022-07-14 13:26:14 - epoch 045 lr: 0.090451
2022-07-14 13:26:54 - train: epoch 0045, iter [00100, 05004], lr: 0.090441, loss: 2.6536
2022-07-14 13:27:28 - train: epoch 0045, iter [00200, 05004], lr: 0.090432, loss: 2.6768
2022-07-14 13:28:02 - train: epoch 0045, iter [00300, 05004], lr: 0.090422, loss: 2.6470
2022-07-14 13:28:36 - train: epoch 0045, iter [00400, 05004], lr: 0.090413, loss: 2.6767
2022-07-14 13:29:10 - train: epoch 0045, iter [00500, 05004], lr: 0.090403, loss: 3.0692
2022-07-14 13:29:45 - train: epoch 0045, iter [00600, 05004], lr: 0.090394, loss: 2.6598
2022-07-14 13:30:20 - train: epoch 0045, iter [00700, 05004], lr: 0.090385, loss: 2.7268
2022-07-14 13:30:54 - train: epoch 0045, iter [00800, 05004], lr: 0.090375, loss: 2.8448
2022-07-14 13:31:27 - train: epoch 0045, iter [00900, 05004], lr: 0.090366, loss: 2.4934
2022-07-14 13:32:01 - train: epoch 0045, iter [01000, 05004], lr: 0.090356, loss: 2.7771
2022-07-14 13:32:35 - train: epoch 0045, iter [01100, 05004], lr: 0.090347, loss: 2.8887
2022-07-14 13:33:09 - train: epoch 0045, iter [01200, 05004], lr: 0.090337, loss: 2.7602
2022-07-14 13:33:43 - train: epoch 0045, iter [01300, 05004], lr: 0.090327, loss: 2.9292
2022-07-14 13:34:17 - train: epoch 0045, iter [01400, 05004], lr: 0.090318, loss: 2.6517
2022-07-14 13:34:50 - train: epoch 0045, iter [01500, 05004], lr: 0.090308, loss: 2.7867
2022-07-14 13:35:24 - train: epoch 0045, iter [01600, 05004], lr: 0.090299, loss: 2.5115
2022-07-14 13:35:58 - train: epoch 0045, iter [01700, 05004], lr: 0.090289, loss: 2.5007
2022-07-14 13:36:31 - train: epoch 0045, iter [01800, 05004], lr: 0.090280, loss: 2.9448
2022-07-14 13:37:05 - train: epoch 0045, iter [01900, 05004], lr: 0.090270, loss: 2.7895
2022-07-14 13:37:39 - train: epoch 0045, iter [02000, 05004], lr: 0.090261, loss: 2.8997
2022-07-14 13:38:13 - train: epoch 0045, iter [02100, 05004], lr: 0.090251, loss: 2.9025
2022-07-14 13:38:47 - train: epoch 0045, iter [02200, 05004], lr: 0.090242, loss: 2.9680
2022-07-14 13:39:21 - train: epoch 0045, iter [02300, 05004], lr: 0.090232, loss: 2.9250
2022-07-14 13:39:55 - train: epoch 0045, iter [02400, 05004], lr: 0.090223, loss: 2.6244
2022-07-14 13:40:28 - train: epoch 0045, iter [02500, 05004], lr: 0.090213, loss: 2.7563
2022-07-14 13:41:02 - train: epoch 0045, iter [02600, 05004], lr: 0.090203, loss: 2.7735
2022-07-14 13:41:37 - train: epoch 0045, iter [02700, 05004], lr: 0.090194, loss: 2.8790
2022-07-14 13:42:10 - train: epoch 0045, iter [02800, 05004], lr: 0.090184, loss: 2.9527
2022-07-14 13:42:43 - train: epoch 0045, iter [02900, 05004], lr: 0.090175, loss: 3.2073
2022-07-14 13:43:17 - train: epoch 0045, iter [03000, 05004], lr: 0.090165, loss: 2.8555
2022-07-14 13:43:52 - train: epoch 0045, iter [03100, 05004], lr: 0.090156, loss: 2.8893
2022-07-14 13:44:26 - train: epoch 0045, iter [03200, 05004], lr: 0.090146, loss: 3.0079
2022-07-14 13:45:00 - train: epoch 0045, iter [03300, 05004], lr: 0.090136, loss: 2.8273
2022-07-14 13:45:33 - train: epoch 0045, iter [03400, 05004], lr: 0.090127, loss: 2.6826
2022-07-14 13:46:08 - train: epoch 0045, iter [03500, 05004], lr: 0.090117, loss: 2.6014
2022-07-14 13:46:42 - train: epoch 0045, iter [03600, 05004], lr: 0.090108, loss: 3.0380
2022-07-14 13:47:16 - train: epoch 0045, iter [03700, 05004], lr: 0.090098, loss: 2.7895
2022-07-14 13:47:51 - train: epoch 0045, iter [03800, 05004], lr: 0.090088, loss: 2.5931
2022-07-14 13:48:26 - train: epoch 0045, iter [03900, 05004], lr: 0.090079, loss: 2.9039
2022-07-14 13:48:59 - train: epoch 0045, iter [04000, 05004], lr: 0.090069, loss: 3.2158
2022-07-14 13:49:34 - train: epoch 0045, iter [04100, 05004], lr: 0.090059, loss: 2.8899
2022-07-14 13:50:07 - train: epoch 0045, iter [04200, 05004], lr: 0.090050, loss: 3.0724
2022-07-14 13:50:42 - train: epoch 0045, iter [04300, 05004], lr: 0.090040, loss: 2.9451
2022-07-14 13:51:16 - train: epoch 0045, iter [04400, 05004], lr: 0.090030, loss: 2.9369
2022-07-14 13:51:51 - train: epoch 0045, iter [04500, 05004], lr: 0.090021, loss: 2.6214
2022-07-14 13:52:25 - train: epoch 0045, iter [04600, 05004], lr: 0.090011, loss: 2.9354
2022-07-14 13:52:59 - train: epoch 0045, iter [04700, 05004], lr: 0.090002, loss: 2.8136
2022-07-14 13:53:33 - train: epoch 0045, iter [04800, 05004], lr: 0.089992, loss: 2.6673
2022-07-14 13:54:08 - train: epoch 0045, iter [04900, 05004], lr: 0.089982, loss: 2.9443
2022-07-14 13:54:41 - train: epoch 0045, iter [05000, 05004], lr: 0.089973, loss: 3.2522
2022-07-14 13:54:42 - train: epoch 045, train_loss: 2.8313
2022-07-14 13:55:56 - eval: epoch: 045, acc1: 56.202%, acc5: 80.832%, test_loss: 1.8574, per_image_load_time: 2.392ms, per_image_inference_time: 0.458ms
2022-07-14 13:55:56 - until epoch: 045, best_acc1: 57.872%
2022-07-14 13:55:56 - epoch 046 lr: 0.089972
2022-07-14 13:56:35 - train: epoch 0046, iter [00100, 05004], lr: 0.089962, loss: 2.9064
2022-07-14 13:57:09 - train: epoch 0046, iter [00200, 05004], lr: 0.089953, loss: 2.7595
2022-07-14 13:57:43 - train: epoch 0046, iter [00300, 05004], lr: 0.089943, loss: 2.6858
2022-07-14 13:58:17 - train: epoch 0046, iter [00400, 05004], lr: 0.089933, loss: 2.8394
2022-07-14 13:58:51 - train: epoch 0046, iter [00500, 05004], lr: 0.089924, loss: 2.6204
2022-07-14 13:59:26 - train: epoch 0046, iter [00600, 05004], lr: 0.089914, loss: 2.8453
2022-07-14 14:00:00 - train: epoch 0046, iter [00700, 05004], lr: 0.089904, loss: 2.9383
2022-07-14 14:00:35 - train: epoch 0046, iter [00800, 05004], lr: 0.089895, loss: 2.6681
2022-07-14 14:01:09 - train: epoch 0046, iter [00900, 05004], lr: 0.089885, loss: 2.7676
2022-07-14 14:01:43 - train: epoch 0046, iter [01000, 05004], lr: 0.089875, loss: 2.6324
2022-07-14 14:02:17 - train: epoch 0046, iter [01100, 05004], lr: 0.089866, loss: 2.5781
2022-07-14 14:02:52 - train: epoch 0046, iter [01200, 05004], lr: 0.089856, loss: 2.6696
2022-07-14 14:03:27 - train: epoch 0046, iter [01300, 05004], lr: 0.089846, loss: 2.7607
2022-07-14 14:04:00 - train: epoch 0046, iter [01400, 05004], lr: 0.089836, loss: 2.9698
2022-07-14 14:04:34 - train: epoch 0046, iter [01500, 05004], lr: 0.089827, loss: 2.6030
2022-07-14 14:05:07 - train: epoch 0046, iter [01600, 05004], lr: 0.089817, loss: 2.6374
2022-07-14 14:05:41 - train: epoch 0046, iter [01700, 05004], lr: 0.089807, loss: 2.7595
2022-07-14 14:06:15 - train: epoch 0046, iter [01800, 05004], lr: 0.089797, loss: 2.9331
2022-07-14 14:06:49 - train: epoch 0046, iter [01900, 05004], lr: 0.089788, loss: 2.9517
2022-07-14 14:07:22 - train: epoch 0046, iter [02000, 05004], lr: 0.089778, loss: 2.8033
2022-07-14 14:07:56 - train: epoch 0046, iter [02100, 05004], lr: 0.089768, loss: 3.2683
2022-07-14 14:08:30 - train: epoch 0046, iter [02200, 05004], lr: 0.089758, loss: 2.5735
2022-07-14 14:09:04 - train: epoch 0046, iter [02300, 05004], lr: 0.089749, loss: 3.2803
2022-07-14 14:09:39 - train: epoch 0046, iter [02400, 05004], lr: 0.089739, loss: 2.6473
2022-07-14 14:10:12 - train: epoch 0046, iter [02500, 05004], lr: 0.089729, loss: 2.7865
2022-07-14 14:10:46 - train: epoch 0046, iter [02600, 05004], lr: 0.089719, loss: 3.3593
2022-07-14 14:11:20 - train: epoch 0046, iter [02700, 05004], lr: 0.089710, loss: 2.8659
2022-07-14 14:11:53 - train: epoch 0046, iter [02800, 05004], lr: 0.089700, loss: 2.9209
2022-07-14 14:12:27 - train: epoch 0046, iter [02900, 05004], lr: 0.089690, loss: 2.9555
2022-07-14 14:13:02 - train: epoch 0046, iter [03000, 05004], lr: 0.089680, loss: 2.5956
2022-07-14 14:13:36 - train: epoch 0046, iter [03100, 05004], lr: 0.089670, loss: 2.5888
2022-07-14 14:14:10 - train: epoch 0046, iter [03200, 05004], lr: 0.089661, loss: 2.9006
2022-07-14 14:14:44 - train: epoch 0046, iter [03300, 05004], lr: 0.089651, loss: 3.0641
2022-07-14 14:15:19 - train: epoch 0046, iter [03400, 05004], lr: 0.089641, loss: 2.8059
2022-07-14 14:15:53 - train: epoch 0046, iter [03500, 05004], lr: 0.089631, loss: 3.0739
2022-07-14 14:16:26 - train: epoch 0046, iter [03600, 05004], lr: 0.089621, loss: 3.0123
2022-07-14 14:17:01 - train: epoch 0046, iter [03700, 05004], lr: 0.089611, loss: 2.9171
2022-07-14 14:17:35 - train: epoch 0046, iter [03800, 05004], lr: 0.089602, loss: 2.6322
2022-07-14 14:18:09 - train: epoch 0046, iter [03900, 05004], lr: 0.089592, loss: 3.0006
2022-07-14 14:18:43 - train: epoch 0046, iter [04000, 05004], lr: 0.089582, loss: 2.8010
2022-07-14 14:19:17 - train: epoch 0046, iter [04100, 05004], lr: 0.089572, loss: 2.9423
2022-07-14 14:19:50 - train: epoch 0046, iter [04200, 05004], lr: 0.089562, loss: 2.9866
2022-07-14 14:20:25 - train: epoch 0046, iter [04300, 05004], lr: 0.089552, loss: 2.8213
2022-07-14 14:20:59 - train: epoch 0046, iter [04400, 05004], lr: 0.089543, loss: 2.9476
2022-07-14 14:21:33 - train: epoch 0046, iter [04500, 05004], lr: 0.089533, loss: 2.7678
2022-07-14 14:22:07 - train: epoch 0046, iter [04600, 05004], lr: 0.089523, loss: 2.9095
2022-07-14 14:22:40 - train: epoch 0046, iter [04700, 05004], lr: 0.089513, loss: 2.5507
2022-07-14 14:23:15 - train: epoch 0046, iter [04800, 05004], lr: 0.089503, loss: 2.7998
2022-07-14 14:23:49 - train: epoch 0046, iter [04900, 05004], lr: 0.089493, loss: 3.2210
2022-07-14 14:24:21 - train: epoch 0046, iter [05000, 05004], lr: 0.089483, loss: 2.8715
2022-07-14 14:24:23 - train: epoch 046, train_loss: 2.8243
2022-07-14 14:25:36 - eval: epoch: 046, acc1: 57.760%, acc5: 81.812%, test_loss: 1.7874, per_image_load_time: 2.285ms, per_image_inference_time: 0.450ms
2022-07-14 14:25:36 - until epoch: 046, best_acc1: 57.872%
2022-07-14 14:25:36 - epoch 047 lr: 0.089483
2022-07-14 14:26:15 - train: epoch 0047, iter [00100, 05004], lr: 0.089473, loss: 2.5914
2022-07-14 14:26:49 - train: epoch 0047, iter [00200, 05004], lr: 0.089463, loss: 2.8676
2022-07-14 14:27:24 - train: epoch 0047, iter [00300, 05004], lr: 0.089453, loss: 2.6554
2022-07-14 14:27:58 - train: epoch 0047, iter [00400, 05004], lr: 0.089444, loss: 2.5390
2022-07-14 14:28:33 - train: epoch 0047, iter [00500, 05004], lr: 0.089434, loss: 3.0344
2022-07-14 14:29:07 - train: epoch 0047, iter [00600, 05004], lr: 0.089424, loss: 2.6448
2022-07-14 14:29:41 - train: epoch 0047, iter [00700, 05004], lr: 0.089414, loss: 3.0138
2022-07-14 14:30:14 - train: epoch 0047, iter [00800, 05004], lr: 0.089404, loss: 2.7303
2022-07-14 14:30:48 - train: epoch 0047, iter [00900, 05004], lr: 0.089394, loss: 2.7901
2022-07-14 14:31:23 - train: epoch 0047, iter [01000, 05004], lr: 0.089384, loss: 2.9191
2022-07-14 14:31:58 - train: epoch 0047, iter [01100, 05004], lr: 0.089374, loss: 3.0728
2022-07-14 14:32:32 - train: epoch 0047, iter [01200, 05004], lr: 0.089364, loss: 2.8780
2022-07-14 14:33:06 - train: epoch 0047, iter [01300, 05004], lr: 0.089354, loss: 2.8101
2022-07-14 14:33:40 - train: epoch 0047, iter [01400, 05004], lr: 0.089344, loss: 2.8405
2022-07-14 14:34:14 - train: epoch 0047, iter [01500, 05004], lr: 0.089334, loss: 2.8865
2022-07-14 14:34:49 - train: epoch 0047, iter [01600, 05004], lr: 0.089325, loss: 2.8851
2022-07-14 14:35:24 - train: epoch 0047, iter [01700, 05004], lr: 0.089315, loss: 2.8469
2022-07-14 14:35:58 - train: epoch 0047, iter [01800, 05004], lr: 0.089305, loss: 2.8973
2022-07-14 14:36:32 - train: epoch 0047, iter [01900, 05004], lr: 0.089295, loss: 2.5684
2022-07-14 14:37:06 - train: epoch 0047, iter [02000, 05004], lr: 0.089285, loss: 2.8420
2022-07-14 14:37:41 - train: epoch 0047, iter [02100, 05004], lr: 0.089275, loss: 3.3779
2022-07-14 14:38:14 - train: epoch 0047, iter [02200, 05004], lr: 0.089265, loss: 3.0257
2022-07-14 14:38:48 - train: epoch 0047, iter [02300, 05004], lr: 0.089255, loss: 2.9623
2022-07-14 14:39:22 - train: epoch 0047, iter [02400, 05004], lr: 0.089245, loss: 2.4357
2022-07-14 14:39:56 - train: epoch 0047, iter [02500, 05004], lr: 0.089235, loss: 3.1462
2022-07-14 14:40:30 - train: epoch 0047, iter [02600, 05004], lr: 0.089225, loss: 3.0327
2022-07-14 14:41:04 - train: epoch 0047, iter [02700, 05004], lr: 0.089215, loss: 2.2712
2022-07-14 14:41:38 - train: epoch 0047, iter [02800, 05004], lr: 0.089205, loss: 3.0744
2022-07-14 14:42:12 - train: epoch 0047, iter [02900, 05004], lr: 0.089195, loss: 2.7651
2022-07-14 14:42:46 - train: epoch 0047, iter [03000, 05004], lr: 0.089185, loss: 3.1105
2022-07-14 14:43:20 - train: epoch 0047, iter [03100, 05004], lr: 0.089175, loss: 2.9553
2022-07-14 14:43:55 - train: epoch 0047, iter [03200, 05004], lr: 0.089165, loss: 3.1124
2022-07-14 14:44:28 - train: epoch 0047, iter [03300, 05004], lr: 0.089155, loss: 2.6470
2022-07-14 14:45:02 - train: epoch 0047, iter [03400, 05004], lr: 0.089145, loss: 2.5929
2022-07-14 14:45:36 - train: epoch 0047, iter [03500, 05004], lr: 0.089135, loss: 3.1254
2022-07-14 14:46:11 - train: epoch 0047, iter [03600, 05004], lr: 0.089125, loss: 3.0339
2022-07-14 14:46:45 - train: epoch 0047, iter [03700, 05004], lr: 0.089115, loss: 2.9808
2022-07-14 14:47:19 - train: epoch 0047, iter [03800, 05004], lr: 0.089105, loss: 2.6473
2022-07-14 14:47:54 - train: epoch 0047, iter [03900, 05004], lr: 0.089095, loss: 2.6930
2022-07-14 14:48:27 - train: epoch 0047, iter [04000, 05004], lr: 0.089085, loss: 2.9814
2022-07-14 14:49:02 - train: epoch 0047, iter [04100, 05004], lr: 0.089075, loss: 2.8304
2022-07-14 14:49:37 - train: epoch 0047, iter [04200, 05004], lr: 0.089065, loss: 2.7907
2022-07-14 14:50:10 - train: epoch 0047, iter [04300, 05004], lr: 0.089055, loss: 2.6334
2022-07-14 14:50:45 - train: epoch 0047, iter [04400, 05004], lr: 0.089045, loss: 2.8229
2022-07-14 14:51:20 - train: epoch 0047, iter [04500, 05004], lr: 0.089034, loss: 2.9636
2022-07-14 14:51:53 - train: epoch 0047, iter [04600, 05004], lr: 0.089024, loss: 2.9245
2022-07-14 14:52:27 - train: epoch 0047, iter [04700, 05004], lr: 0.089014, loss: 3.1105
2022-07-14 14:53:02 - train: epoch 0047, iter [04800, 05004], lr: 0.089004, loss: 2.5821
2022-07-14 14:53:36 - train: epoch 0047, iter [04900, 05004], lr: 0.088994, loss: 2.6537
2022-07-14 14:54:09 - train: epoch 0047, iter [05000, 05004], lr: 0.088984, loss: 2.9513
2022-07-14 14:54:10 - train: epoch 047, train_loss: 2.8222
2022-07-14 14:55:23 - eval: epoch: 047, acc1: 56.280%, acc5: 80.984%, test_loss: 1.8484, per_image_load_time: 2.376ms, per_image_inference_time: 0.463ms
2022-07-14 14:55:23 - until epoch: 047, best_acc1: 57.872%
2022-07-14 14:55:23 - epoch 048 lr: 0.088984
2022-07-14 14:56:03 - train: epoch 0048, iter [00100, 05004], lr: 0.088974, loss: 2.8950
2022-07-14 14:56:37 - train: epoch 0048, iter [00200, 05004], lr: 0.088964, loss: 3.0663
2022-07-14 14:57:10 - train: epoch 0048, iter [00300, 05004], lr: 0.088953, loss: 2.9387
2022-07-14 14:57:44 - train: epoch 0048, iter [00400, 05004], lr: 0.088943, loss: 2.5450
2022-07-14 14:58:19 - train: epoch 0048, iter [00500, 05004], lr: 0.088933, loss: 3.0543
2022-07-14 14:58:52 - train: epoch 0048, iter [00600, 05004], lr: 0.088923, loss: 3.0225
2022-07-14 14:59:26 - train: epoch 0048, iter [00700, 05004], lr: 0.088913, loss: 3.0691
2022-07-14 15:00:01 - train: epoch 0048, iter [00800, 05004], lr: 0.088903, loss: 3.1855
2022-07-14 15:00:36 - train: epoch 0048, iter [00900, 05004], lr: 0.088893, loss: 2.8807
2022-07-14 15:01:10 - train: epoch 0048, iter [01000, 05004], lr: 0.088883, loss: 2.9840
2022-07-14 15:01:45 - train: epoch 0048, iter [01100, 05004], lr: 0.088873, loss: 2.9578
2022-07-14 15:02:19 - train: epoch 0048, iter [01200, 05004], lr: 0.088862, loss: 2.7650
2022-07-14 15:02:54 - train: epoch 0048, iter [01300, 05004], lr: 0.088852, loss: 2.8441
2022-07-14 15:03:28 - train: epoch 0048, iter [01400, 05004], lr: 0.088842, loss: 2.6798
2022-07-14 15:04:03 - train: epoch 0048, iter [01500, 05004], lr: 0.088832, loss: 2.8686
2022-07-14 15:04:37 - train: epoch 0048, iter [01600, 05004], lr: 0.088822, loss: 2.9497
2022-07-14 15:05:12 - train: epoch 0048, iter [01700, 05004], lr: 0.088812, loss: 2.6866
2022-07-14 15:05:46 - train: epoch 0048, iter [01800, 05004], lr: 0.088802, loss: 2.5544
2022-07-14 15:06:21 - train: epoch 0048, iter [01900, 05004], lr: 0.088791, loss: 3.1027
2022-07-14 15:06:55 - train: epoch 0048, iter [02000, 05004], lr: 0.088781, loss: 2.5029
2022-07-14 15:07:30 - train: epoch 0048, iter [02100, 05004], lr: 0.088771, loss: 2.8325
2022-07-14 15:08:04 - train: epoch 0048, iter [02200, 05004], lr: 0.088761, loss: 2.7659
2022-07-14 15:08:38 - train: epoch 0048, iter [02300, 05004], lr: 0.088751, loss: 2.7145
2022-07-14 15:09:11 - train: epoch 0048, iter [02400, 05004], lr: 0.088741, loss: 2.7515
2022-07-14 15:09:44 - train: epoch 0048, iter [02500, 05004], lr: 0.088730, loss: 2.6584
2022-07-14 15:10:18 - train: epoch 0048, iter [02600, 05004], lr: 0.088720, loss: 2.8322
2022-07-14 15:10:52 - train: epoch 0048, iter [02700, 05004], lr: 0.088710, loss: 2.8907
2022-07-14 15:11:26 - train: epoch 0048, iter [02800, 05004], lr: 0.088700, loss: 2.8148
2022-07-14 15:12:00 - train: epoch 0048, iter [02900, 05004], lr: 0.088690, loss: 2.8943
2022-07-14 15:12:34 - train: epoch 0048, iter [03000, 05004], lr: 0.088679, loss: 2.7290
2022-07-14 15:13:08 - train: epoch 0048, iter [03100, 05004], lr: 0.088669, loss: 2.9626
2022-07-14 15:13:42 - train: epoch 0048, iter [03200, 05004], lr: 0.088659, loss: 2.7157
2022-07-14 15:14:15 - train: epoch 0048, iter [03300, 05004], lr: 0.088649, loss: 2.8634
2022-07-14 15:14:48 - train: epoch 0048, iter [03400, 05004], lr: 0.088639, loss: 2.7716
2022-07-14 15:15:23 - train: epoch 0048, iter [03500, 05004], lr: 0.088628, loss: 3.1440
2022-07-14 15:15:57 - train: epoch 0048, iter [03600, 05004], lr: 0.088618, loss: 2.5358
2022-07-14 15:16:31 - train: epoch 0048, iter [03700, 05004], lr: 0.088608, loss: 2.7354
2022-07-14 15:17:05 - train: epoch 0048, iter [03800, 05004], lr: 0.088598, loss: 3.0081
2022-07-14 15:17:39 - train: epoch 0048, iter [03900, 05004], lr: 0.088588, loss: 2.7610
2022-07-14 15:18:13 - train: epoch 0048, iter [04000, 05004], lr: 0.088577, loss: 2.7212
2022-07-14 15:18:47 - train: epoch 0048, iter [04100, 05004], lr: 0.088567, loss: 3.1207
2022-07-14 15:19:21 - train: epoch 0048, iter [04200, 05004], lr: 0.088557, loss: 2.8196
2022-07-14 15:19:54 - train: epoch 0048, iter [04300, 05004], lr: 0.088547, loss: 2.8569
2022-07-14 15:20:29 - train: epoch 0048, iter [04400, 05004], lr: 0.088536, loss: 2.9158
2022-07-14 15:21:02 - train: epoch 0048, iter [04500, 05004], lr: 0.088526, loss: 3.0437
2022-07-14 15:21:37 - train: epoch 0048, iter [04600, 05004], lr: 0.088516, loss: 3.0506
2022-07-14 15:22:11 - train: epoch 0048, iter [04700, 05004], lr: 0.088506, loss: 2.9029
2022-07-14 15:22:44 - train: epoch 0048, iter [04800, 05004], lr: 0.088495, loss: 3.1813
2022-07-14 15:23:19 - train: epoch 0048, iter [04900, 05004], lr: 0.088485, loss: 3.0415
2022-07-14 15:23:52 - train: epoch 0048, iter [05000, 05004], lr: 0.088475, loss: 2.8936
2022-07-14 15:23:53 - train: epoch 048, train_loss: 2.8198
2022-07-14 15:25:07 - eval: epoch: 048, acc1: 55.020%, acc5: 79.832%, test_loss: 1.9164, per_image_load_time: 2.353ms, per_image_inference_time: 0.488ms
2022-07-14 15:25:07 - until epoch: 048, best_acc1: 57.872%
2022-07-14 15:25:07 - epoch 049 lr: 0.088474
2022-07-14 15:25:47 - train: epoch 0049, iter [00100, 05004], lr: 0.088464, loss: 2.6275
2022-07-14 15:26:21 - train: epoch 0049, iter [00200, 05004], lr: 0.088454, loss: 2.7588
2022-07-14 15:26:56 - train: epoch 0049, iter [00300, 05004], lr: 0.088443, loss: 3.0490
2022-07-14 15:27:30 - train: epoch 0049, iter [00400, 05004], lr: 0.088433, loss: 2.8256
2022-07-14 15:28:04 - train: epoch 0049, iter [00500, 05004], lr: 0.088423, loss: 2.7075
2022-07-14 15:28:38 - train: epoch 0049, iter [00600, 05004], lr: 0.088413, loss: 2.3391
2022-07-14 15:29:13 - train: epoch 0049, iter [00700, 05004], lr: 0.088402, loss: 2.6387
2022-07-14 15:29:48 - train: epoch 0049, iter [00800, 05004], lr: 0.088392, loss: 3.0840
2022-07-14 15:30:22 - train: epoch 0049, iter [00900, 05004], lr: 0.088382, loss: 2.4820
2022-07-14 15:30:56 - train: epoch 0049, iter [01000, 05004], lr: 0.088371, loss: 2.8181
2022-07-14 15:31:30 - train: epoch 0049, iter [01100, 05004], lr: 0.088361, loss: 2.8213
2022-07-14 15:32:06 - train: epoch 0049, iter [01200, 05004], lr: 0.088351, loss: 2.6430
2022-07-14 15:32:39 - train: epoch 0049, iter [01300, 05004], lr: 0.088340, loss: 2.8293
2022-07-14 15:33:14 - train: epoch 0049, iter [01400, 05004], lr: 0.088330, loss: 2.8577
2022-07-14 15:33:48 - train: epoch 0049, iter [01500, 05004], lr: 0.088320, loss: 2.9229
2022-07-14 15:34:23 - train: epoch 0049, iter [01600, 05004], lr: 0.088309, loss: 2.9870
2022-07-14 15:34:57 - train: epoch 0049, iter [01700, 05004], lr: 0.088299, loss: 2.8296
2022-07-14 15:35:33 - train: epoch 0049, iter [01800, 05004], lr: 0.088289, loss: 2.7683
2022-07-14 15:36:07 - train: epoch 0049, iter [01900, 05004], lr: 0.088278, loss: 2.7905
2022-07-14 15:36:42 - train: epoch 0049, iter [02000, 05004], lr: 0.088268, loss: 2.5291
2022-07-14 15:37:16 - train: epoch 0049, iter [02100, 05004], lr: 0.088257, loss: 2.6969
2022-07-14 15:37:50 - train: epoch 0049, iter [02200, 05004], lr: 0.088247, loss: 3.1046
2022-07-14 15:38:24 - train: epoch 0049, iter [02300, 05004], lr: 0.088237, loss: 2.7978
2022-07-14 15:39:00 - train: epoch 0049, iter [02400, 05004], lr: 0.088226, loss: 2.9719
2022-07-14 15:39:34 - train: epoch 0049, iter [02500, 05004], lr: 0.088216, loss: 3.0246
2022-07-14 15:40:08 - train: epoch 0049, iter [02600, 05004], lr: 0.088206, loss: 3.0515
2022-07-14 15:40:42 - train: epoch 0049, iter [02700, 05004], lr: 0.088195, loss: 2.8014
2022-07-14 15:41:17 - train: epoch 0049, iter [02800, 05004], lr: 0.088185, loss: 2.7304
2022-07-14 15:41:50 - train: epoch 0049, iter [02900, 05004], lr: 0.088174, loss: 2.8745
2022-07-14 15:42:26 - train: epoch 0049, iter [03000, 05004], lr: 0.088164, loss: 2.8336
2022-07-14 15:43:00 - train: epoch 0049, iter [03100, 05004], lr: 0.088154, loss: 2.7038
2022-07-14 15:43:35 - train: epoch 0049, iter [03200, 05004], lr: 0.088143, loss: 2.7615
2022-07-14 15:44:10 - train: epoch 0049, iter [03300, 05004], lr: 0.088133, loss: 2.8391
2022-07-14 15:44:43 - train: epoch 0049, iter [03400, 05004], lr: 0.088122, loss: 2.9794
2022-07-14 15:45:18 - train: epoch 0049, iter [03500, 05004], lr: 0.088112, loss: 2.9044
2022-07-14 15:45:53 - train: epoch 0049, iter [03600, 05004], lr: 0.088102, loss: 2.9499
2022-07-14 15:46:28 - train: epoch 0049, iter [03700, 05004], lr: 0.088091, loss: 2.6047
2022-07-14 15:47:02 - train: epoch 0049, iter [03800, 05004], lr: 0.088081, loss: 2.7837
2022-07-14 15:47:37 - train: epoch 0049, iter [03900, 05004], lr: 0.088070, loss: 3.0078
2022-07-14 15:48:12 - train: epoch 0049, iter [04000, 05004], lr: 0.088060, loss: 2.7493
2022-07-14 15:48:46 - train: epoch 0049, iter [04100, 05004], lr: 0.088049, loss: 2.6462
2022-07-14 15:49:21 - train: epoch 0049, iter [04200, 05004], lr: 0.088039, loss: 2.7666
2022-07-14 15:49:56 - train: epoch 0049, iter [04300, 05004], lr: 0.088029, loss: 2.9991
2022-07-14 15:50:30 - train: epoch 0049, iter [04400, 05004], lr: 0.088018, loss: 2.7909
2022-07-14 15:51:06 - train: epoch 0049, iter [04500, 05004], lr: 0.088008, loss: 2.7808
2022-07-14 15:51:40 - train: epoch 0049, iter [04600, 05004], lr: 0.087997, loss: 3.0563
2022-07-14 15:52:14 - train: epoch 0049, iter [04700, 05004], lr: 0.087987, loss: 2.9252
2022-07-14 15:52:48 - train: epoch 0049, iter [04800, 05004], lr: 0.087976, loss: 2.7187
2022-07-14 15:53:23 - train: epoch 0049, iter [04900, 05004], lr: 0.087966, loss: 2.8041
2022-07-14 15:53:56 - train: epoch 0049, iter [05000, 05004], lr: 0.087955, loss: 2.7549
2022-07-14 15:53:57 - train: epoch 049, train_loss: 2.8125
2022-07-14 15:55:11 - eval: epoch: 049, acc1: 57.508%, acc5: 81.932%, test_loss: 1.7899, per_image_load_time: 2.424ms, per_image_inference_time: 0.447ms
2022-07-14 15:55:11 - until epoch: 049, best_acc1: 57.872%
2022-07-14 15:55:11 - epoch 050 lr: 0.087955
2022-07-14 15:55:50 - train: epoch 0050, iter [00100, 05004], lr: 0.087944, loss: 2.6092
2022-07-14 15:56:24 - train: epoch 0050, iter [00200, 05004], lr: 0.087934, loss: 3.0193
2022-07-14 15:56:57 - train: epoch 0050, iter [00300, 05004], lr: 0.087923, loss: 2.6790
2022-07-14 15:57:31 - train: epoch 0050, iter [00400, 05004], lr: 0.087913, loss: 2.7000
2022-07-14 15:58:04 - train: epoch 0050, iter [00500, 05004], lr: 0.087902, loss: 2.6895
2022-07-14 15:58:39 - train: epoch 0050, iter [00600, 05004], lr: 0.087892, loss: 2.9786
2022-07-14 15:59:13 - train: epoch 0050, iter [00700, 05004], lr: 0.087881, loss: 2.6010
2022-07-14 15:59:47 - train: epoch 0050, iter [00800, 05004], lr: 0.087871, loss: 2.6176
2022-07-14 16:00:20 - train: epoch 0050, iter [00900, 05004], lr: 0.087860, loss: 2.8450
2022-07-14 16:00:55 - train: epoch 0050, iter [01000, 05004], lr: 0.087850, loss: 2.8063
2022-07-14 16:01:29 - train: epoch 0050, iter [01100, 05004], lr: 0.087839, loss: 2.6678
2022-07-14 16:02:02 - train: epoch 0050, iter [01200, 05004], lr: 0.087829, loss: 2.9130
2022-07-14 16:02:37 - train: epoch 0050, iter [01300, 05004], lr: 0.087818, loss: 2.7085
2022-07-14 16:03:11 - train: epoch 0050, iter [01400, 05004], lr: 0.087808, loss: 2.8257
2022-07-14 16:03:46 - train: epoch 0050, iter [01500, 05004], lr: 0.087797, loss: 2.9138
2022-07-14 16:04:19 - train: epoch 0050, iter [01600, 05004], lr: 0.087787, loss: 2.6210
2022-07-14 16:04:54 - train: epoch 0050, iter [01700, 05004], lr: 0.087776, loss: 2.7621
2022-07-14 16:05:28 - train: epoch 0050, iter [01800, 05004], lr: 0.087766, loss: 3.1482
2022-07-14 16:06:01 - train: epoch 0050, iter [01900, 05004], lr: 0.087755, loss: 2.5868
2022-07-14 16:06:36 - train: epoch 0050, iter [02000, 05004], lr: 0.087744, loss: 2.6825
2022-07-14 16:07:09 - train: epoch 0050, iter [02100, 05004], lr: 0.087734, loss: 2.6756
2022-07-14 16:07:43 - train: epoch 0050, iter [02200, 05004], lr: 0.087723, loss: 2.7503
2022-07-14 16:08:18 - train: epoch 0050, iter [02300, 05004], lr: 0.087713, loss: 2.4674
2022-07-14 16:08:51 - train: epoch 0050, iter [02400, 05004], lr: 0.087702, loss: 2.8255
2022-07-14 16:09:25 - train: epoch 0050, iter [02500, 05004], lr: 0.087692, loss: 3.0035
2022-07-14 16:10:00 - train: epoch 0050, iter [02600, 05004], lr: 0.087681, loss: 2.5888
2022-07-14 16:10:33 - train: epoch 0050, iter [02700, 05004], lr: 0.087670, loss: 2.8446
2022-07-14 16:11:08 - train: epoch 0050, iter [02800, 05004], lr: 0.087660, loss: 2.7770
2022-07-14 16:11:42 - train: epoch 0050, iter [02900, 05004], lr: 0.087649, loss: 3.2554
2022-07-14 16:12:17 - train: epoch 0050, iter [03000, 05004], lr: 0.087639, loss: 2.8518
2022-07-14 16:12:51 - train: epoch 0050, iter [03100, 05004], lr: 0.087628, loss: 2.8723
2022-07-14 16:13:25 - train: epoch 0050, iter [03200, 05004], lr: 0.087617, loss: 2.8252
2022-07-14 16:13:59 - train: epoch 0050, iter [03300, 05004], lr: 0.087607, loss: 2.8014
2022-07-14 16:14:33 - train: epoch 0050, iter [03400, 05004], lr: 0.087596, loss: 2.8280
2022-07-14 16:15:07 - train: epoch 0050, iter [03500, 05004], lr: 0.087586, loss: 2.6619
2022-07-14 16:15:41 - train: epoch 0050, iter [03600, 05004], lr: 0.087575, loss: 2.9495
2022-07-14 16:16:15 - train: epoch 0050, iter [03700, 05004], lr: 0.087564, loss: 2.9473
2022-07-14 16:16:49 - train: epoch 0050, iter [03800, 05004], lr: 0.087554, loss: 2.5267
2022-07-14 16:17:23 - train: epoch 0050, iter [03900, 05004], lr: 0.087543, loss: 2.3618
2022-07-14 16:17:57 - train: epoch 0050, iter [04000, 05004], lr: 0.087533, loss: 2.9228
2022-07-14 16:18:31 - train: epoch 0050, iter [04100, 05004], lr: 0.087522, loss: 2.8422
2022-07-14 16:19:06 - train: epoch 0050, iter [04200, 05004], lr: 0.087511, loss: 2.9228
2022-07-14 16:19:40 - train: epoch 0050, iter [04300, 05004], lr: 0.087501, loss: 2.9243
2022-07-14 16:20:14 - train: epoch 0050, iter [04400, 05004], lr: 0.087490, loss: 2.7917
2022-07-14 16:20:48 - train: epoch 0050, iter [04500, 05004], lr: 0.087479, loss: 2.8281
2022-07-14 16:21:22 - train: epoch 0050, iter [04600, 05004], lr: 0.087469, loss: 2.9714
2022-07-14 16:21:57 - train: epoch 0050, iter [04700, 05004], lr: 0.087458, loss: 3.0581
2022-07-14 16:22:31 - train: epoch 0050, iter [04800, 05004], lr: 0.087447, loss: 2.9314
2022-07-14 16:23:06 - train: epoch 0050, iter [04900, 05004], lr: 0.087437, loss: 2.5514
2022-07-14 16:23:39 - train: epoch 0050, iter [05000, 05004], lr: 0.087426, loss: 2.8141
2022-07-14 16:23:41 - train: epoch 050, train_loss: 2.8072
2022-07-14 16:24:54 - eval: epoch: 050, acc1: 55.808%, acc5: 80.528%, test_loss: 1.8783, per_image_load_time: 2.399ms, per_image_inference_time: 0.463ms
2022-07-14 16:24:55 - until epoch: 050, best_acc1: 57.872%
2022-07-14 16:24:55 - epoch 051 lr: 0.087425
2022-07-14 16:25:34 - train: epoch 0051, iter [00100, 05004], lr: 0.087415, loss: 2.8463
2022-07-14 16:26:08 - train: epoch 0051, iter [00200, 05004], lr: 0.087404, loss: 2.5545
2022-07-14 16:26:42 - train: epoch 0051, iter [00300, 05004], lr: 0.087393, loss: 2.8796
2022-07-14 16:27:16 - train: epoch 0051, iter [00400, 05004], lr: 0.087383, loss: 2.6820
2022-07-14 16:27:51 - train: epoch 0051, iter [00500, 05004], lr: 0.087372, loss: 2.7569
2022-07-14 16:28:25 - train: epoch 0051, iter [00600, 05004], lr: 0.087361, loss: 2.6176
2022-07-14 16:29:00 - train: epoch 0051, iter [00700, 05004], lr: 0.087351, loss: 2.9267
2022-07-14 16:29:34 - train: epoch 0051, iter [00800, 05004], lr: 0.087340, loss: 3.0441
2022-07-14 16:30:09 - train: epoch 0051, iter [00900, 05004], lr: 0.087329, loss: 2.6854
2022-07-14 16:30:43 - train: epoch 0051, iter [01000, 05004], lr: 0.087319, loss: 2.8044
2022-07-14 16:31:17 - train: epoch 0051, iter [01100, 05004], lr: 0.087308, loss: 2.9350
2022-07-14 16:31:52 - train: epoch 0051, iter [01200, 05004], lr: 0.087297, loss: 2.7686
2022-07-14 16:32:27 - train: epoch 0051, iter [01300, 05004], lr: 0.087286, loss: 2.7433
2022-07-14 16:33:01 - train: epoch 0051, iter [01400, 05004], lr: 0.087276, loss: 2.6867
2022-07-14 16:33:36 - train: epoch 0051, iter [01500, 05004], lr: 0.087265, loss: 2.7578
2022-07-14 16:34:10 - train: epoch 0051, iter [01600, 05004], lr: 0.087254, loss: 2.6413
2022-07-14 16:34:44 - train: epoch 0051, iter [01700, 05004], lr: 0.087244, loss: 2.6809
2022-07-14 16:35:18 - train: epoch 0051, iter [01800, 05004], lr: 0.087233, loss: 2.8117
2022-07-14 16:35:52 - train: epoch 0051, iter [01900, 05004], lr: 0.087222, loss: 2.7505
2022-07-14 16:36:26 - train: epoch 0051, iter [02000, 05004], lr: 0.087211, loss: 2.6416
2022-07-14 16:36:59 - train: epoch 0051, iter [02100, 05004], lr: 0.087201, loss: 2.8485
2022-07-14 16:37:33 - train: epoch 0051, iter [02200, 05004], lr: 0.087190, loss: 2.7361
2022-07-14 16:38:07 - train: epoch 0051, iter [02300, 05004], lr: 0.087179, loss: 2.9105
2022-07-14 16:38:40 - train: epoch 0051, iter [02400, 05004], lr: 0.087168, loss: 2.5329
2022-07-14 16:39:14 - train: epoch 0051, iter [02500, 05004], lr: 0.087157, loss: 2.9034
2022-07-14 16:39:48 - train: epoch 0051, iter [02600, 05004], lr: 0.087147, loss: 2.6953
2022-07-14 16:40:22 - train: epoch 0051, iter [02700, 05004], lr: 0.087136, loss: 2.9696
2022-07-14 16:40:55 - train: epoch 0051, iter [02800, 05004], lr: 0.087125, loss: 2.5566
2022-07-14 16:41:29 - train: epoch 0051, iter [02900, 05004], lr: 0.087114, loss: 3.0434
2022-07-14 16:42:03 - train: epoch 0051, iter [03000, 05004], lr: 0.087104, loss: 2.8842
2022-07-14 16:42:37 - train: epoch 0051, iter [03100, 05004], lr: 0.087093, loss: 2.8065
2022-07-14 16:43:10 - train: epoch 0051, iter [03200, 05004], lr: 0.087082, loss: 2.9667
2022-07-14 16:43:44 - train: epoch 0051, iter [03300, 05004], lr: 0.087071, loss: 2.8511
2022-07-14 16:44:18 - train: epoch 0051, iter [03400, 05004], lr: 0.087060, loss: 2.9691
2022-07-14 16:44:53 - train: epoch 0051, iter [03500, 05004], lr: 0.087050, loss: 2.9487
2022-07-14 16:45:25 - train: epoch 0051, iter [03600, 05004], lr: 0.087039, loss: 2.7668
2022-07-14 16:45:59 - train: epoch 0051, iter [03700, 05004], lr: 0.087028, loss: 2.8586
2022-07-14 16:46:32 - train: epoch 0051, iter [03800, 05004], lr: 0.087017, loss: 2.7450
2022-07-14 16:47:06 - train: epoch 0051, iter [03900, 05004], lr: 0.087006, loss: 3.0507
2022-07-14 16:47:41 - train: epoch 0051, iter [04000, 05004], lr: 0.086995, loss: 2.8170
2022-07-14 16:48:15 - train: epoch 0051, iter [04100, 05004], lr: 0.086985, loss: 2.5575
2022-07-14 16:48:49 - train: epoch 0051, iter [04200, 05004], lr: 0.086974, loss: 2.6855
2022-07-14 16:49:23 - train: epoch 0051, iter [04300, 05004], lr: 0.086963, loss: 2.7870
2022-07-14 16:49:57 - train: epoch 0051, iter [04400, 05004], lr: 0.086952, loss: 3.0854
2022-07-14 16:50:31 - train: epoch 0051, iter [04500, 05004], lr: 0.086941, loss: 2.5638
2022-07-14 16:51:05 - train: epoch 0051, iter [04600, 05004], lr: 0.086930, loss: 3.0627
2022-07-14 16:51:39 - train: epoch 0051, iter [04700, 05004], lr: 0.086920, loss: 3.0897
2022-07-14 16:52:13 - train: epoch 0051, iter [04800, 05004], lr: 0.086909, loss: 3.1104
2022-07-14 16:52:47 - train: epoch 0051, iter [04900, 05004], lr: 0.086898, loss: 2.8003
2022-07-14 16:53:20 - train: epoch 0051, iter [05000, 05004], lr: 0.086887, loss: 2.5690
2022-07-14 16:53:21 - train: epoch 051, train_loss: 2.8042
2022-07-14 16:54:34 - eval: epoch: 051, acc1: 56.776%, acc5: 81.190%, test_loss: 1.8259, per_image_load_time: 2.369ms, per_image_inference_time: 0.466ms
2022-07-14 16:54:34 - until epoch: 051, best_acc1: 57.872%
2022-07-14 16:54:34 - epoch 052 lr: 0.086886
2022-07-14 16:55:13 - train: epoch 0052, iter [00100, 05004], lr: 0.086876, loss: 2.9448
2022-07-14 16:55:46 - train: epoch 0052, iter [00200, 05004], lr: 0.086865, loss: 2.9868
2022-07-14 16:56:19 - train: epoch 0052, iter [00300, 05004], lr: 0.086854, loss: 2.6504
2022-07-14 16:56:52 - train: epoch 0052, iter [00400, 05004], lr: 0.086843, loss: 2.8736
2022-07-14 16:57:25 - train: epoch 0052, iter [00500, 05004], lr: 0.086832, loss: 2.9262
2022-07-14 16:57:59 - train: epoch 0052, iter [00600, 05004], lr: 0.086821, loss: 2.7997
2022-07-14 16:58:32 - train: epoch 0052, iter [00700, 05004], lr: 0.086810, loss: 2.7826
2022-07-14 16:59:05 - train: epoch 0052, iter [00800, 05004], lr: 0.086799, loss: 3.0048
2022-07-14 16:59:38 - train: epoch 0052, iter [00900, 05004], lr: 0.086789, loss: 2.9604
2022-07-14 17:00:11 - train: epoch 0052, iter [01000, 05004], lr: 0.086778, loss: 2.9426
2022-07-14 17:00:44 - train: epoch 0052, iter [01100, 05004], lr: 0.086767, loss: 2.8045
2022-07-14 17:01:17 - train: epoch 0052, iter [01200, 05004], lr: 0.086756, loss: 2.7240
2022-07-14 17:01:50 - train: epoch 0052, iter [01300, 05004], lr: 0.086745, loss: 2.4667
2022-07-14 17:02:24 - train: epoch 0052, iter [01400, 05004], lr: 0.086734, loss: 3.1595
2022-07-14 17:02:56 - train: epoch 0052, iter [01500, 05004], lr: 0.086723, loss: 2.5414
2022-07-14 17:03:30 - train: epoch 0052, iter [01600, 05004], lr: 0.086712, loss: 2.4679
2022-07-14 17:04:03 - train: epoch 0052, iter [01700, 05004], lr: 0.086701, loss: 2.4929
2022-07-14 17:04:36 - train: epoch 0052, iter [01800, 05004], lr: 0.086690, loss: 2.9249
2022-07-14 17:05:09 - train: epoch 0052, iter [01900, 05004], lr: 0.086679, loss: 2.9173
2022-07-14 17:05:43 - train: epoch 0052, iter [02000, 05004], lr: 0.086668, loss: 2.8176
2022-07-14 17:06:15 - train: epoch 0052, iter [02100, 05004], lr: 0.086657, loss: 2.8004
2022-07-14 17:06:50 - train: epoch 0052, iter [02200, 05004], lr: 0.086647, loss: 2.8533
2022-07-14 17:07:22 - train: epoch 0052, iter [02300, 05004], lr: 0.086636, loss: 2.6973
2022-07-14 17:07:56 - train: epoch 0052, iter [02400, 05004], lr: 0.086625, loss: 2.6123
2022-07-14 17:08:29 - train: epoch 0052, iter [02500, 05004], lr: 0.086614, loss: 2.4706
2022-07-14 17:09:03 - train: epoch 0052, iter [02600, 05004], lr: 0.086603, loss: 2.6452
2022-07-14 17:09:36 - train: epoch 0052, iter [02700, 05004], lr: 0.086592, loss: 2.6642
2022-07-14 17:10:10 - train: epoch 0052, iter [02800, 05004], lr: 0.086581, loss: 2.8724
2022-07-14 17:10:44 - train: epoch 0052, iter [02900, 05004], lr: 0.086570, loss: 2.5715
2022-07-14 17:11:17 - train: epoch 0052, iter [03000, 05004], lr: 0.086559, loss: 2.6047
2022-07-14 17:11:50 - train: epoch 0052, iter [03100, 05004], lr: 0.086548, loss: 3.1258
2022-07-14 17:12:24 - train: epoch 0052, iter [03200, 05004], lr: 0.086537, loss: 2.8236
2022-07-14 17:12:57 - train: epoch 0052, iter [03300, 05004], lr: 0.086526, loss: 2.8814
2022-07-14 17:13:31 - train: epoch 0052, iter [03400, 05004], lr: 0.086515, loss: 2.8502
2022-07-14 17:14:04 - train: epoch 0052, iter [03500, 05004], lr: 0.086504, loss: 2.7886
2022-07-14 17:14:38 - train: epoch 0052, iter [03600, 05004], lr: 0.086493, loss: 2.8420
2022-07-14 17:15:12 - train: epoch 0052, iter [03700, 05004], lr: 0.086482, loss: 2.8980
2022-07-14 17:15:45 - train: epoch 0052, iter [03800, 05004], lr: 0.086471, loss: 3.0038
2022-07-14 17:16:19 - train: epoch 0052, iter [03900, 05004], lr: 0.086460, loss: 2.5575
2022-07-14 17:16:52 - train: epoch 0052, iter [04000, 05004], lr: 0.086449, loss: 2.6208
2022-07-14 17:17:26 - train: epoch 0052, iter [04100, 05004], lr: 0.086438, loss: 2.8933
2022-07-14 17:18:00 - train: epoch 0052, iter [04200, 05004], lr: 0.086427, loss: 2.9740
2022-07-14 17:18:33 - train: epoch 0052, iter [04300, 05004], lr: 0.086416, loss: 2.8944
2022-07-14 17:19:07 - train: epoch 0052, iter [04400, 05004], lr: 0.086405, loss: 3.0130
2022-07-14 17:19:40 - train: epoch 0052, iter [04500, 05004], lr: 0.086394, loss: 2.8962
2022-07-14 17:20:14 - train: epoch 0052, iter [04600, 05004], lr: 0.086383, loss: 2.9362
2022-07-14 17:20:47 - train: epoch 0052, iter [04700, 05004], lr: 0.086372, loss: 2.9552
2022-07-14 17:21:21 - train: epoch 0052, iter [04800, 05004], lr: 0.086361, loss: 2.6053
2022-07-14 17:21:54 - train: epoch 0052, iter [04900, 05004], lr: 0.086349, loss: 2.8508
2022-07-14 17:22:26 - train: epoch 0052, iter [05000, 05004], lr: 0.086338, loss: 2.7500
2022-07-14 17:22:27 - train: epoch 052, train_loss: 2.8004
2022-07-14 17:23:40 - eval: epoch: 052, acc1: 57.590%, acc5: 82.090%, test_loss: 1.7770, per_image_load_time: 2.372ms, per_image_inference_time: 0.485ms
2022-07-14 17:23:41 - until epoch: 052, best_acc1: 57.872%
2022-07-14 17:23:41 - epoch 053 lr: 0.086338
2022-07-14 17:24:21 - train: epoch 0053, iter [00100, 05004], lr: 0.086327, loss: 2.8207
2022-07-14 17:24:55 - train: epoch 0053, iter [00200, 05004], lr: 0.086316, loss: 2.8543
2022-07-14 17:25:28 - train: epoch 0053, iter [00300, 05004], lr: 0.086305, loss: 3.0890
2022-07-14 17:26:03 - train: epoch 0053, iter [00400, 05004], lr: 0.086294, loss: 3.1769
2022-07-14 17:26:36 - train: epoch 0053, iter [00500, 05004], lr: 0.086283, loss: 2.9071
2022-07-14 17:27:11 - train: epoch 0053, iter [00600, 05004], lr: 0.086272, loss: 2.8694
2022-07-14 17:27:46 - train: epoch 0053, iter [00700, 05004], lr: 0.086260, loss: 2.6257
2022-07-14 17:28:20 - train: epoch 0053, iter [00800, 05004], lr: 0.086249, loss: 2.7751
2022-07-14 17:28:54 - train: epoch 0053, iter [00900, 05004], lr: 0.086238, loss: 2.8658
2022-07-14 17:29:29 - train: epoch 0053, iter [01000, 05004], lr: 0.086227, loss: 2.7678
2022-07-14 17:30:03 - train: epoch 0053, iter [01100, 05004], lr: 0.086216, loss: 2.4525
2022-07-14 17:30:37 - train: epoch 0053, iter [01200, 05004], lr: 0.086205, loss: 2.2826
2022-07-14 17:31:10 - train: epoch 0053, iter [01300, 05004], lr: 0.086194, loss: 2.5533
2022-07-14 17:31:44 - train: epoch 0053, iter [01400, 05004], lr: 0.086183, loss: 3.0097
2022-07-14 17:32:18 - train: epoch 0053, iter [01500, 05004], lr: 0.086172, loss: 2.5937
2022-07-14 17:32:53 - train: epoch 0053, iter [01600, 05004], lr: 0.086161, loss: 2.9801
2022-07-14 17:33:27 - train: epoch 0053, iter [01700, 05004], lr: 0.086149, loss: 2.8474
2022-07-14 17:34:01 - train: epoch 0053, iter [01800, 05004], lr: 0.086138, loss: 2.6980
2022-07-14 17:34:36 - train: epoch 0053, iter [01900, 05004], lr: 0.086127, loss: 2.8747
2022-07-14 17:35:09 - train: epoch 0053, iter [02000, 05004], lr: 0.086116, loss: 2.9303
2022-07-14 17:35:44 - train: epoch 0053, iter [02100, 05004], lr: 0.086105, loss: 2.7818
2022-07-14 17:36:18 - train: epoch 0053, iter [02200, 05004], lr: 0.086094, loss: 2.6115
2022-07-14 17:36:53 - train: epoch 0053, iter [02300, 05004], lr: 0.086083, loss: 3.0177
2022-07-14 17:37:27 - train: epoch 0053, iter [02400, 05004], lr: 0.086071, loss: 2.7556
2022-07-14 17:38:02 - train: epoch 0053, iter [02500, 05004], lr: 0.086060, loss: 2.8287
2022-07-14 17:38:35 - train: epoch 0053, iter [02600, 05004], lr: 0.086049, loss: 3.1694
2022-07-14 17:39:10 - train: epoch 0053, iter [02700, 05004], lr: 0.086038, loss: 3.1492
2022-07-14 17:39:44 - train: epoch 0053, iter [02800, 05004], lr: 0.086027, loss: 2.8977
2022-07-14 17:40:18 - train: epoch 0053, iter [02900, 05004], lr: 0.086016, loss: 2.9263
2022-07-14 17:40:53 - train: epoch 0053, iter [03000, 05004], lr: 0.086005, loss: 2.3780
2022-07-14 17:41:27 - train: epoch 0053, iter [03100, 05004], lr: 0.085993, loss: 3.0560
2022-07-14 17:42:02 - train: epoch 0053, iter [03200, 05004], lr: 0.085982, loss: 2.8729
2022-07-14 17:42:36 - train: epoch 0053, iter [03300, 05004], lr: 0.085971, loss: 2.5862
2022-07-14 17:43:10 - train: epoch 0053, iter [03400, 05004], lr: 0.085960, loss: 2.7697
2022-07-14 17:43:45 - train: epoch 0053, iter [03500, 05004], lr: 0.085949, loss: 2.7639
2022-07-14 17:44:19 - train: epoch 0053, iter [03600, 05004], lr: 0.085937, loss: 2.8800
2022-07-14 17:44:54 - train: epoch 0053, iter [03700, 05004], lr: 0.085926, loss: 3.0727
2022-07-14 17:45:28 - train: epoch 0053, iter [03800, 05004], lr: 0.085915, loss: 2.5480
2022-07-14 17:46:02 - train: epoch 0053, iter [03900, 05004], lr: 0.085904, loss: 2.9928
2022-07-14 17:46:36 - train: epoch 0053, iter [04000, 05004], lr: 0.085893, loss: 2.8945
2022-07-14 17:47:12 - train: epoch 0053, iter [04100, 05004], lr: 0.085881, loss: 2.7912
2022-07-14 17:47:45 - train: epoch 0053, iter [04200, 05004], lr: 0.085870, loss: 2.8076
2022-07-14 17:48:20 - train: epoch 0053, iter [04300, 05004], lr: 0.085859, loss: 2.9212
2022-07-14 17:48:55 - train: epoch 0053, iter [04400, 05004], lr: 0.085848, loss: 2.7185
2022-07-14 17:49:30 - train: epoch 0053, iter [04500, 05004], lr: 0.085837, loss: 2.7992
2022-07-14 17:50:04 - train: epoch 0053, iter [04600, 05004], lr: 0.085825, loss: 2.3219
2022-07-14 17:50:38 - train: epoch 0053, iter [04700, 05004], lr: 0.085814, loss: 2.4752
2022-07-14 17:51:12 - train: epoch 0053, iter [04800, 05004], lr: 0.085803, loss: 2.9802
2022-07-14 17:51:46 - train: epoch 0053, iter [04900, 05004], lr: 0.085792, loss: 3.0464
2022-07-14 17:52:20 - train: epoch 0053, iter [05000, 05004], lr: 0.085780, loss: 3.0521
2022-07-14 17:52:21 - train: epoch 053, train_loss: 2.7958
2022-07-14 17:53:36 - eval: epoch: 053, acc1: 56.484%, acc5: 81.000%, test_loss: 1.8409, per_image_load_time: 2.403ms, per_image_inference_time: 0.478ms
2022-07-14 17:53:36 - until epoch: 053, best_acc1: 57.872%
2022-07-14 17:53:36 - epoch 054 lr: 0.085780
2022-07-14 17:54:16 - train: epoch 0054, iter [00100, 05004], lr: 0.085769, loss: 2.6215
2022-07-14 17:54:50 - train: epoch 0054, iter [00200, 05004], lr: 0.085757, loss: 2.9461
2022-07-14 17:55:24 - train: epoch 0054, iter [00300, 05004], lr: 0.085746, loss: 2.8608
2022-07-14 17:55:58 - train: epoch 0054, iter [00400, 05004], lr: 0.085735, loss: 2.8679
2022-07-14 17:56:33 - train: epoch 0054, iter [00500, 05004], lr: 0.085724, loss: 2.7868
2022-07-14 17:57:06 - train: epoch 0054, iter [00600, 05004], lr: 0.085712, loss: 2.5520
2022-07-14 17:57:41 - train: epoch 0054, iter [00700, 05004], lr: 0.085701, loss: 3.0101
2022-07-14 17:58:16 - train: epoch 0054, iter [00800, 05004], lr: 0.085690, loss: 2.8459
2022-07-14 17:58:51 - train: epoch 0054, iter [00900, 05004], lr: 0.085679, loss: 2.3751
2022-07-14 17:59:24 - train: epoch 0054, iter [01000, 05004], lr: 0.085667, loss: 2.6690
2022-07-14 17:59:59 - train: epoch 0054, iter [01100, 05004], lr: 0.085656, loss: 2.4245
2022-07-14 18:00:34 - train: epoch 0054, iter [01200, 05004], lr: 0.085645, loss: 2.7423
2022-07-14 18:01:08 - train: epoch 0054, iter [01300, 05004], lr: 0.085633, loss: 2.8667
2022-07-14 18:01:43 - train: epoch 0054, iter [01400, 05004], lr: 0.085622, loss: 2.7199
2022-07-14 18:02:18 - train: epoch 0054, iter [01500, 05004], lr: 0.085611, loss: 2.6602
2022-07-14 18:02:52 - train: epoch 0054, iter [01600, 05004], lr: 0.085600, loss: 2.6874
2022-07-14 18:03:26 - train: epoch 0054, iter [01700, 05004], lr: 0.085588, loss: 3.0517
2022-07-14 18:04:01 - train: epoch 0054, iter [01800, 05004], lr: 0.085577, loss: 2.6731
2022-07-14 18:04:36 - train: epoch 0054, iter [01900, 05004], lr: 0.085566, loss: 2.9034
2022-07-14 18:05:11 - train: epoch 0054, iter [02000, 05004], lr: 0.085554, loss: 2.8959
2022-07-14 18:05:45 - train: epoch 0054, iter [02100, 05004], lr: 0.085543, loss: 2.8422
2022-07-14 18:06:19 - train: epoch 0054, iter [02200, 05004], lr: 0.085532, loss: 2.9033
2022-07-14 18:06:53 - train: epoch 0054, iter [02300, 05004], lr: 0.085520, loss: 2.6907
2022-07-14 18:07:28 - train: epoch 0054, iter [02400, 05004], lr: 0.085509, loss: 2.7164
2022-07-14 18:08:03 - train: epoch 0054, iter [02500, 05004], lr: 0.085498, loss: 2.9835
2022-07-14 18:08:37 - train: epoch 0054, iter [02600, 05004], lr: 0.085486, loss: 2.7446
2022-07-14 18:09:11 - train: epoch 0054, iter [02700, 05004], lr: 0.085475, loss: 2.7619
2022-07-14 18:09:45 - train: epoch 0054, iter [02800, 05004], lr: 0.085464, loss: 3.2938
2022-07-14 18:10:20 - train: epoch 0054, iter [02900, 05004], lr: 0.085452, loss: 2.6435
2022-07-14 18:10:54 - train: epoch 0054, iter [03000, 05004], lr: 0.085441, loss: 2.8680
2022-07-14 18:11:29 - train: epoch 0054, iter [03100, 05004], lr: 0.085430, loss: 3.0479
2022-07-14 18:12:03 - train: epoch 0054, iter [03200, 05004], lr: 0.085418, loss: 2.7491
2022-07-14 18:12:38 - train: epoch 0054, iter [03300, 05004], lr: 0.085407, loss: 2.5863
2022-07-14 18:13:12 - train: epoch 0054, iter [03400, 05004], lr: 0.085395, loss: 2.4048
2022-07-14 18:13:46 - train: epoch 0054, iter [03500, 05004], lr: 0.085384, loss: 3.0318
2022-07-14 18:14:21 - train: epoch 0054, iter [03600, 05004], lr: 0.085373, loss: 2.9854
2022-07-14 18:14:56 - train: epoch 0054, iter [03700, 05004], lr: 0.085361, loss: 2.7107
2022-07-14 18:15:30 - train: epoch 0054, iter [03800, 05004], lr: 0.085350, loss: 2.7094
2022-07-14 18:16:05 - train: epoch 0054, iter [03900, 05004], lr: 0.085339, loss: 3.0280
2022-07-14 18:16:39 - train: epoch 0054, iter [04000, 05004], lr: 0.085327, loss: 3.0379
2022-07-14 18:17:13 - train: epoch 0054, iter [04100, 05004], lr: 0.085316, loss: 2.9057
2022-07-14 18:17:48 - train: epoch 0054, iter [04200, 05004], lr: 0.085304, loss: 2.7357
2022-07-14 18:18:22 - train: epoch 0054, iter [04300, 05004], lr: 0.085293, loss: 2.9409
2022-07-14 18:18:56 - train: epoch 0054, iter [04400, 05004], lr: 0.085282, loss: 2.7081
2022-07-14 18:19:30 - train: epoch 0054, iter [04500, 05004], lr: 0.085270, loss: 2.5756
2022-07-14 18:20:04 - train: epoch 0054, iter [04600, 05004], lr: 0.085259, loss: 2.8700
2022-07-14 18:20:39 - train: epoch 0054, iter [04700, 05004], lr: 0.085247, loss: 3.0116
2022-07-14 18:21:13 - train: epoch 0054, iter [04800, 05004], lr: 0.085236, loss: 2.9272
2022-07-14 18:21:48 - train: epoch 0054, iter [04900, 05004], lr: 0.085225, loss: 2.6835
2022-07-14 18:22:21 - train: epoch 0054, iter [05000, 05004], lr: 0.085213, loss: 2.7823
2022-07-14 18:22:22 - train: epoch 054, train_loss: 2.7901
2022-07-14 18:23:36 - eval: epoch: 054, acc1: 57.602%, acc5: 81.528%, test_loss: 1.7962, per_image_load_time: 2.095ms, per_image_inference_time: 0.495ms
2022-07-14 18:23:36 - until epoch: 054, best_acc1: 57.872%
2022-07-14 18:23:36 - epoch 055 lr: 0.085213
2022-07-14 18:24:16 - train: epoch 0055, iter [00100, 05004], lr: 0.085201, loss: 2.7479
2022-07-14 18:24:50 - train: epoch 0055, iter [00200, 05004], lr: 0.085190, loss: 2.6574
2022-07-14 18:25:24 - train: epoch 0055, iter [00300, 05004], lr: 0.085178, loss: 2.4137
2022-07-14 18:25:59 - train: epoch 0055, iter [00400, 05004], lr: 0.085167, loss: 2.5539
2022-07-14 18:26:33 - train: epoch 0055, iter [00500, 05004], lr: 0.085155, loss: 2.6761
2022-07-14 18:27:07 - train: epoch 0055, iter [00600, 05004], lr: 0.085144, loss: 2.8994
2022-07-14 18:27:41 - train: epoch 0055, iter [00700, 05004], lr: 0.085133, loss: 2.7184
2022-07-14 18:28:15 - train: epoch 0055, iter [00800, 05004], lr: 0.085121, loss: 2.7236
2022-07-14 18:28:50 - train: epoch 0055, iter [00900, 05004], lr: 0.085110, loss: 2.8870
2022-07-14 18:29:23 - train: epoch 0055, iter [01000, 05004], lr: 0.085098, loss: 2.5969
2022-07-14 18:29:57 - train: epoch 0055, iter [01100, 05004], lr: 0.085087, loss: 2.8590
2022-07-14 18:30:31 - train: epoch 0055, iter [01200, 05004], lr: 0.085075, loss: 2.7371
2022-07-14 18:31:05 - train: epoch 0055, iter [01300, 05004], lr: 0.085064, loss: 2.6480
2022-07-14 18:31:39 - train: epoch 0055, iter [01400, 05004], lr: 0.085052, loss: 2.8511
2022-07-14 18:32:13 - train: epoch 0055, iter [01500, 05004], lr: 0.085041, loss: 2.9006
2022-07-14 18:32:47 - train: epoch 0055, iter [01600, 05004], lr: 0.085029, loss: 2.8313
2022-07-14 18:33:22 - train: epoch 0055, iter [01700, 05004], lr: 0.085018, loss: 2.8826
2022-07-14 18:33:57 - train: epoch 0055, iter [01800, 05004], lr: 0.085006, loss: 2.8080
2022-07-14 18:34:30 - train: epoch 0055, iter [01900, 05004], lr: 0.084995, loss: 2.9819
2022-07-14 18:35:05 - train: epoch 0055, iter [02000, 05004], lr: 0.084983, loss: 2.8432
2022-07-14 18:35:38 - train: epoch 0055, iter [02100, 05004], lr: 0.084972, loss: 2.5543
2022-07-14 18:36:13 - train: epoch 0055, iter [02200, 05004], lr: 0.084960, loss: 2.9603
2022-07-14 18:36:47 - train: epoch 0055, iter [02300, 05004], lr: 0.084949, loss: 2.6531
2022-07-14 18:37:21 - train: epoch 0055, iter [02400, 05004], lr: 0.084937, loss: 2.2828
2022-07-14 18:37:55 - train: epoch 0055, iter [02500, 05004], lr: 0.084926, loss: 2.7646
2022-07-14 18:38:29 - train: epoch 0055, iter [02600, 05004], lr: 0.084914, loss: 2.7658
2022-07-14 18:39:02 - train: epoch 0055, iter [02700, 05004], lr: 0.084903, loss: 2.8136
2022-07-14 18:39:37 - train: epoch 0055, iter [02800, 05004], lr: 0.084891, loss: 2.9120
2022-07-14 18:40:11 - train: epoch 0055, iter [02900, 05004], lr: 0.084880, loss: 2.9056
2022-07-14 18:40:45 - train: epoch 0055, iter [03000, 05004], lr: 0.084868, loss: 3.0592
2022-07-14 18:41:19 - train: epoch 0055, iter [03100, 05004], lr: 0.084857, loss: 3.3416
2022-07-14 18:41:53 - train: epoch 0055, iter [03200, 05004], lr: 0.084845, loss: 2.6365
2022-07-14 18:42:26 - train: epoch 0055, iter [03300, 05004], lr: 0.084834, loss: 2.5106
2022-07-14 18:43:01 - train: epoch 0055, iter [03400, 05004], lr: 0.084822, loss: 2.7924
2022-07-14 18:43:35 - train: epoch 0055, iter [03500, 05004], lr: 0.084810, loss: 2.6926
2022-07-14 18:44:09 - train: epoch 0055, iter [03600, 05004], lr: 0.084799, loss: 2.7622
2022-07-14 18:44:43 - train: epoch 0055, iter [03700, 05004], lr: 0.084787, loss: 2.8568
2022-07-14 18:45:17 - train: epoch 0055, iter [03800, 05004], lr: 0.084776, loss: 2.7206
2022-07-14 18:45:51 - train: epoch 0055, iter [03900, 05004], lr: 0.084764, loss: 2.9522
2022-07-14 18:46:25 - train: epoch 0055, iter [04000, 05004], lr: 0.084753, loss: 2.5266
2022-07-14 18:46:59 - train: epoch 0055, iter [04100, 05004], lr: 0.084741, loss: 2.9343
2022-07-14 18:47:32 - train: epoch 0055, iter [04200, 05004], lr: 0.084729, loss: 3.0456
2022-07-14 18:48:07 - train: epoch 0055, iter [04300, 05004], lr: 0.084718, loss: 2.8998
2022-07-14 18:48:41 - train: epoch 0055, iter [04400, 05004], lr: 0.084706, loss: 2.8585
2022-07-14 18:49:15 - train: epoch 0055, iter [04500, 05004], lr: 0.084695, loss: 2.8036
2022-07-14 18:49:49 - train: epoch 0055, iter [04600, 05004], lr: 0.084683, loss: 2.8226
2022-07-14 18:50:23 - train: epoch 0055, iter [04700, 05004], lr: 0.084671, loss: 2.8921
2022-07-14 18:50:57 - train: epoch 0055, iter [04800, 05004], lr: 0.084660, loss: 2.7019
2022-07-14 18:51:31 - train: epoch 0055, iter [04900, 05004], lr: 0.084648, loss: 2.7858
2022-07-14 18:52:04 - train: epoch 0055, iter [05000, 05004], lr: 0.084637, loss: 2.9284
2022-07-14 18:52:05 - train: epoch 055, train_loss: 2.7842
2022-07-14 18:53:19 - eval: epoch: 055, acc1: 54.402%, acc5: 79.376%, test_loss: 1.9444, per_image_load_time: 2.410ms, per_image_inference_time: 0.449ms
2022-07-14 18:53:19 - until epoch: 055, best_acc1: 57.872%
2022-07-14 18:53:19 - epoch 056 lr: 0.084636
2022-07-14 18:53:58 - train: epoch 0056, iter [00100, 05004], lr: 0.084625, loss: 2.9490
2022-07-14 18:54:32 - train: epoch 0056, iter [00200, 05004], lr: 0.084613, loss: 2.6769
2022-07-14 18:55:06 - train: epoch 0056, iter [00300, 05004], lr: 0.084601, loss: 2.5460
2022-07-14 18:55:40 - train: epoch 0056, iter [00400, 05004], lr: 0.084590, loss: 2.7376
2022-07-14 18:56:14 - train: epoch 0056, iter [00500, 05004], lr: 0.084578, loss: 2.7826
2022-07-14 18:56:49 - train: epoch 0056, iter [00600, 05004], lr: 0.084566, loss: 2.8420
2022-07-14 18:57:23 - train: epoch 0056, iter [00700, 05004], lr: 0.084555, loss: 2.5650
2022-07-14 18:57:57 - train: epoch 0056, iter [00800, 05004], lr: 0.084543, loss: 2.9281
2022-07-14 18:58:31 - train: epoch 0056, iter [00900, 05004], lr: 0.084532, loss: 2.7709
2022-07-14 18:59:06 - train: epoch 0056, iter [01000, 05004], lr: 0.084520, loss: 2.6057
2022-07-14 18:59:41 - train: epoch 0056, iter [01100, 05004], lr: 0.084508, loss: 3.0091
2022-07-14 19:00:15 - train: epoch 0056, iter [01200, 05004], lr: 0.084497, loss: 2.4876
2022-07-14 19:00:49 - train: epoch 0056, iter [01300, 05004], lr: 0.084485, loss: 2.8425
2022-07-14 19:01:23 - train: epoch 0056, iter [01400, 05004], lr: 0.084473, loss: 2.6727
2022-07-14 19:01:58 - train: epoch 0056, iter [01500, 05004], lr: 0.084462, loss: 2.8948
2022-07-14 19:02:33 - train: epoch 0056, iter [01600, 05004], lr: 0.084450, loss: 2.7433
2022-07-14 19:03:07 - train: epoch 0056, iter [01700, 05004], lr: 0.084438, loss: 2.9143
2022-07-14 19:03:42 - train: epoch 0056, iter [01800, 05004], lr: 0.084427, loss: 2.8848
2022-07-14 19:04:16 - train: epoch 0056, iter [01900, 05004], lr: 0.084415, loss: 2.9626
2022-07-14 19:04:51 - train: epoch 0056, iter [02000, 05004], lr: 0.084403, loss: 3.0171
2022-07-14 19:05:25 - train: epoch 0056, iter [02100, 05004], lr: 0.084392, loss: 2.8309
2022-07-14 19:06:00 - train: epoch 0056, iter [02200, 05004], lr: 0.084380, loss: 2.8699
2022-07-14 19:06:34 - train: epoch 0056, iter [02300, 05004], lr: 0.084368, loss: 2.7347
2022-07-14 19:07:08 - train: epoch 0056, iter [02400, 05004], lr: 0.084357, loss: 2.9872
2022-07-14 19:07:43 - train: epoch 0056, iter [02500, 05004], lr: 0.084345, loss: 2.8035
2022-07-14 19:08:17 - train: epoch 0056, iter [02600, 05004], lr: 0.084333, loss: 2.6970
2022-07-14 19:08:52 - train: epoch 0056, iter [02700, 05004], lr: 0.084321, loss: 2.8114
2022-07-14 19:09:27 - train: epoch 0056, iter [02800, 05004], lr: 0.084310, loss: 2.8760
2022-07-14 19:10:01 - train: epoch 0056, iter [02900, 05004], lr: 0.084298, loss: 2.9187
2022-07-14 19:10:35 - train: epoch 0056, iter [03000, 05004], lr: 0.084286, loss: 2.8593
2022-07-14 19:11:10 - train: epoch 0056, iter [03100, 05004], lr: 0.084275, loss: 2.5472
2022-07-14 19:11:45 - train: epoch 0056, iter [03200, 05004], lr: 0.084263, loss: 2.6865
2022-07-14 19:12:20 - train: epoch 0056, iter [03300, 05004], lr: 0.084251, loss: 2.7765
2022-07-14 19:12:54 - train: epoch 0056, iter [03400, 05004], lr: 0.084239, loss: 2.8290
2022-07-14 19:13:29 - train: epoch 0056, iter [03500, 05004], lr: 0.084228, loss: 2.4990
2022-07-14 19:14:03 - train: epoch 0056, iter [03600, 05004], lr: 0.084216, loss: 2.5645
2022-07-14 19:14:37 - train: epoch 0056, iter [03700, 05004], lr: 0.084204, loss: 2.5800
2022-07-14 19:15:12 - train: epoch 0056, iter [03800, 05004], lr: 0.084192, loss: 2.5669
2022-07-14 19:15:47 - train: epoch 0056, iter [03900, 05004], lr: 0.084181, loss: 2.8973
2022-07-14 19:16:21 - train: epoch 0056, iter [04000, 05004], lr: 0.084169, loss: 2.9148
2022-07-14 19:16:55 - train: epoch 0056, iter [04100, 05004], lr: 0.084157, loss: 2.9588
2022-07-14 19:17:30 - train: epoch 0056, iter [04200, 05004], lr: 0.084145, loss: 2.8166
2022-07-14 19:18:05 - train: epoch 0056, iter [04300, 05004], lr: 0.084134, loss: 2.7704
2022-07-14 19:18:39 - train: epoch 0056, iter [04400, 05004], lr: 0.084122, loss: 2.8880
2022-07-14 19:19:14 - train: epoch 0056, iter [04500, 05004], lr: 0.084110, loss: 2.8907
2022-07-14 19:19:49 - train: epoch 0056, iter [04600, 05004], lr: 0.084098, loss: 2.8838
2022-07-14 19:20:23 - train: epoch 0056, iter [04700, 05004], lr: 0.084087, loss: 2.9853
2022-07-14 19:20:58 - train: epoch 0056, iter [04800, 05004], lr: 0.084075, loss: 2.9974
2022-07-14 19:21:32 - train: epoch 0056, iter [04900, 05004], lr: 0.084063, loss: 3.1782
2022-07-14 19:22:06 - train: epoch 0056, iter [05000, 05004], lr: 0.084051, loss: 2.7330
2022-07-14 19:22:08 - train: epoch 056, train_loss: 2.7819
2022-07-14 19:23:23 - eval: epoch: 056, acc1: 58.142%, acc5: 82.698%, test_loss: 1.7474, per_image_load_time: 2.485ms, per_image_inference_time: 0.489ms
2022-07-14 19:23:24 - until epoch: 056, best_acc1: 58.142%
2022-07-14 19:23:24 - epoch 057 lr: 0.084051
2022-07-14 19:24:04 - train: epoch 0057, iter [00100, 05004], lr: 0.084039, loss: 2.8429
2022-07-14 19:24:37 - train: epoch 0057, iter [00200, 05004], lr: 0.084027, loss: 2.8103
2022-07-14 19:25:11 - train: epoch 0057, iter [00300, 05004], lr: 0.084015, loss: 2.8367
2022-07-14 19:25:45 - train: epoch 0057, iter [00400, 05004], lr: 0.084004, loss: 2.7574
2022-07-14 19:26:19 - train: epoch 0057, iter [00500, 05004], lr: 0.083992, loss: 2.3266
2022-07-14 19:26:52 - train: epoch 0057, iter [00600, 05004], lr: 0.083980, loss: 2.9586
2022-07-14 19:27:26 - train: epoch 0057, iter [00700, 05004], lr: 0.083968, loss: 2.7043
2022-07-14 19:28:01 - train: epoch 0057, iter [00800, 05004], lr: 0.083956, loss: 2.6836
2022-07-14 19:28:35 - train: epoch 0057, iter [00900, 05004], lr: 0.083945, loss: 2.8064
2022-07-14 19:29:08 - train: epoch 0057, iter [01000, 05004], lr: 0.083933, loss: 2.5997
2022-07-14 19:29:43 - train: epoch 0057, iter [01100, 05004], lr: 0.083921, loss: 2.8171
2022-07-14 19:30:17 - train: epoch 0057, iter [01200, 05004], lr: 0.083909, loss: 2.6289
2022-07-14 19:30:52 - train: epoch 0057, iter [01300, 05004], lr: 0.083897, loss: 2.9261
2022-07-14 19:31:25 - train: epoch 0057, iter [01400, 05004], lr: 0.083885, loss: 2.7828
2022-07-14 19:32:00 - train: epoch 0057, iter [01500, 05004], lr: 0.083874, loss: 2.8893
2022-07-14 19:32:34 - train: epoch 0057, iter [01600, 05004], lr: 0.083862, loss: 3.0017
2022-07-14 19:33:08 - train: epoch 0057, iter [01700, 05004], lr: 0.083850, loss: 2.9120
2022-07-14 19:33:43 - train: epoch 0057, iter [01800, 05004], lr: 0.083838, loss: 2.9805
2022-07-14 19:34:16 - train: epoch 0057, iter [01900, 05004], lr: 0.083826, loss: 2.9141
2022-07-14 19:34:51 - train: epoch 0057, iter [02000, 05004], lr: 0.083814, loss: 2.8469
2022-07-14 19:35:25 - train: epoch 0057, iter [02100, 05004], lr: 0.083802, loss: 2.9177
2022-07-14 19:35:59 - train: epoch 0057, iter [02200, 05004], lr: 0.083791, loss: 2.7879
2022-07-14 19:36:33 - train: epoch 0057, iter [02300, 05004], lr: 0.083779, loss: 2.6173
2022-07-14 19:37:08 - train: epoch 0057, iter [02400, 05004], lr: 0.083767, loss: 2.5008
2022-07-14 19:37:42 - train: epoch 0057, iter [02500, 05004], lr: 0.083755, loss: 2.8319
2022-07-14 19:38:17 - train: epoch 0057, iter [02600, 05004], lr: 0.083743, loss: 2.7033
2022-07-14 19:38:52 - train: epoch 0057, iter [02700, 05004], lr: 0.083731, loss: 2.5088
2022-07-14 19:39:26 - train: epoch 0057, iter [02800, 05004], lr: 0.083719, loss: 2.5667
2022-07-14 19:40:01 - train: epoch 0057, iter [02900, 05004], lr: 0.083707, loss: 2.7856
2022-07-14 19:40:35 - train: epoch 0057, iter [03000, 05004], lr: 0.083696, loss: 2.8682
2022-07-14 19:41:10 - train: epoch 0057, iter [03100, 05004], lr: 0.083684, loss: 2.9021
2022-07-14 19:41:44 - train: epoch 0057, iter [03200, 05004], lr: 0.083672, loss: 2.8370
2022-07-14 19:42:19 - train: epoch 0057, iter [03300, 05004], lr: 0.083660, loss: 2.8161
2022-07-14 19:42:54 - train: epoch 0057, iter [03400, 05004], lr: 0.083648, loss: 2.7738
2022-07-14 19:43:28 - train: epoch 0057, iter [03500, 05004], lr: 0.083636, loss: 2.9601
2022-07-14 19:44:02 - train: epoch 0057, iter [03600, 05004], lr: 0.083624, loss: 2.7971
2022-07-14 19:44:38 - train: epoch 0057, iter [03700, 05004], lr: 0.083612, loss: 2.7291
2022-07-14 19:45:12 - train: epoch 0057, iter [03800, 05004], lr: 0.083600, loss: 2.5242
2022-07-14 19:45:46 - train: epoch 0057, iter [03900, 05004], lr: 0.083588, loss: 3.0310
2022-07-14 19:46:21 - train: epoch 0057, iter [04000, 05004], lr: 0.083576, loss: 2.8971
2022-07-14 19:46:56 - train: epoch 0057, iter [04100, 05004], lr: 0.083565, loss: 2.7350
2022-07-14 19:47:31 - train: epoch 0057, iter [04200, 05004], lr: 0.083553, loss: 2.7450
2022-07-14 19:48:06 - train: epoch 0057, iter [04300, 05004], lr: 0.083541, loss: 2.3952
2022-07-14 19:48:40 - train: epoch 0057, iter [04400, 05004], lr: 0.083529, loss: 2.7804
2022-07-14 19:49:15 - train: epoch 0057, iter [04500, 05004], lr: 0.083517, loss: 2.7342
2022-07-14 19:49:49 - train: epoch 0057, iter [04600, 05004], lr: 0.083505, loss: 2.9046
2022-07-14 19:50:24 - train: epoch 0057, iter [04700, 05004], lr: 0.083493, loss: 2.6746
2022-07-14 19:50:59 - train: epoch 0057, iter [04800, 05004], lr: 0.083481, loss: 2.9233
2022-07-14 19:51:33 - train: epoch 0057, iter [04900, 05004], lr: 0.083469, loss: 2.8183
2022-07-14 19:52:06 - train: epoch 0057, iter [05000, 05004], lr: 0.083457, loss: 3.0061
2022-07-14 19:52:07 - train: epoch 057, train_loss: 2.7774
2022-07-14 19:53:21 - eval: epoch: 057, acc1: 56.726%, acc5: 81.180%, test_loss: 1.8385, per_image_load_time: 2.336ms, per_image_inference_time: 0.508ms
2022-07-14 19:53:21 - until epoch: 057, best_acc1: 58.142%
2022-07-14 19:53:21 - epoch 058 lr: 0.083456
2022-07-14 19:54:01 - train: epoch 0058, iter [00100, 05004], lr: 0.083445, loss: 2.8495
2022-07-14 19:54:35 - train: epoch 0058, iter [00200, 05004], lr: 0.083433, loss: 2.9046
2022-07-14 19:55:09 - train: epoch 0058, iter [00300, 05004], lr: 0.083421, loss: 2.5803
2022-07-14 19:55:43 - train: epoch 0058, iter [00400, 05004], lr: 0.083409, loss: 2.5797
2022-07-14 19:56:17 - train: epoch 0058, iter [00500, 05004], lr: 0.083397, loss: 2.5332
2022-07-14 19:56:52 - train: epoch 0058, iter [00600, 05004], lr: 0.083385, loss: 2.8852
2022-07-14 19:57:26 - train: epoch 0058, iter [00700, 05004], lr: 0.083373, loss: 2.8356
2022-07-14 19:58:01 - train: epoch 0058, iter [00800, 05004], lr: 0.083361, loss: 2.8512
2022-07-14 19:58:36 - train: epoch 0058, iter [00900, 05004], lr: 0.083349, loss: 2.6061
2022-07-14 19:59:09 - train: epoch 0058, iter [01000, 05004], lr: 0.083337, loss: 2.4744
2022-07-14 19:59:44 - train: epoch 0058, iter [01100, 05004], lr: 0.083325, loss: 2.6924
2022-07-14 20:00:19 - train: epoch 0058, iter [01200, 05004], lr: 0.083313, loss: 2.7384
2022-07-14 20:00:54 - train: epoch 0058, iter [01300, 05004], lr: 0.083301, loss: 2.7887
2022-07-14 20:01:29 - train: epoch 0058, iter [01400, 05004], lr: 0.083289, loss: 2.8728
2022-07-14 20:02:04 - train: epoch 0058, iter [01500, 05004], lr: 0.083277, loss: 2.8662
2022-07-14 20:02:38 - train: epoch 0058, iter [01600, 05004], lr: 0.083265, loss: 2.7430
2022-07-14 20:03:12 - train: epoch 0058, iter [01700, 05004], lr: 0.083253, loss: 2.8470
2022-07-14 20:03:47 - train: epoch 0058, iter [01800, 05004], lr: 0.083241, loss: 2.9468
2022-07-14 20:04:21 - train: epoch 0058, iter [01900, 05004], lr: 0.083229, loss: 2.6780
2022-07-14 20:04:57 - train: epoch 0058, iter [02000, 05004], lr: 0.083217, loss: 2.9492
2022-07-14 20:05:31 - train: epoch 0058, iter [02100, 05004], lr: 0.083205, loss: 2.6710
2022-07-14 20:06:05 - train: epoch 0058, iter [02200, 05004], lr: 0.083193, loss: 2.6957
2022-07-14 20:06:40 - train: epoch 0058, iter [02300, 05004], lr: 0.083180, loss: 2.9968
2022-07-14 20:07:15 - train: epoch 0058, iter [02400, 05004], lr: 0.083168, loss: 2.8765
2022-07-14 20:07:50 - train: epoch 0058, iter [02500, 05004], lr: 0.083156, loss: 2.6546
2022-07-14 20:08:24 - train: epoch 0058, iter [02600, 05004], lr: 0.083144, loss: 2.7810
2022-07-14 20:08:58 - train: epoch 0058, iter [02700, 05004], lr: 0.083132, loss: 2.9654
2022-07-14 20:09:32 - train: epoch 0058, iter [02800, 05004], lr: 0.083120, loss: 2.6694
2022-07-14 20:10:08 - train: epoch 0058, iter [02900, 05004], lr: 0.083108, loss: 2.7898
2022-07-14 20:10:42 - train: epoch 0058, iter [03000, 05004], lr: 0.083096, loss: 3.0179
2022-07-14 20:11:16 - train: epoch 0058, iter [03100, 05004], lr: 0.083084, loss: 2.7704
2022-07-14 20:11:51 - train: epoch 0058, iter [03200, 05004], lr: 0.083072, loss: 2.5503
2022-07-14 20:12:25 - train: epoch 0058, iter [03300, 05004], lr: 0.083060, loss: 2.6007
2022-07-14 20:12:59 - train: epoch 0058, iter [03400, 05004], lr: 0.083048, loss: 2.6797
2022-07-14 20:13:35 - train: epoch 0058, iter [03500, 05004], lr: 0.083036, loss: 2.3875
2022-07-14 20:14:09 - train: epoch 0058, iter [03600, 05004], lr: 0.083024, loss: 2.7720
2022-07-14 20:14:43 - train: epoch 0058, iter [03700, 05004], lr: 0.083012, loss: 2.7762
2022-07-14 20:15:18 - train: epoch 0058, iter [03800, 05004], lr: 0.082999, loss: 2.6112
2022-07-14 20:15:53 - train: epoch 0058, iter [03900, 05004], lr: 0.082987, loss: 2.7122
2022-07-14 20:16:28 - train: epoch 0058, iter [04000, 05004], lr: 0.082975, loss: 2.7690
2022-07-14 20:17:01 - train: epoch 0058, iter [04100, 05004], lr: 0.082963, loss: 2.8072
2022-07-14 20:17:36 - train: epoch 0058, iter [04200, 05004], lr: 0.082951, loss: 2.6886
2022-07-14 20:18:10 - train: epoch 0058, iter [04300, 05004], lr: 0.082939, loss: 2.7417
2022-07-14 20:18:45 - train: epoch 0058, iter [04400, 05004], lr: 0.082927, loss: 2.7524
2022-07-14 20:19:18 - train: epoch 0058, iter [04500, 05004], lr: 0.082915, loss: 2.6281
2022-07-14 20:19:53 - train: epoch 0058, iter [04600, 05004], lr: 0.082903, loss: 2.6613
2022-07-14 20:20:28 - train: epoch 0058, iter [04700, 05004], lr: 0.082890, loss: 2.7231
2022-07-14 20:21:02 - train: epoch 0058, iter [04800, 05004], lr: 0.082878, loss: 2.8874
2022-07-14 20:21:36 - train: epoch 0058, iter [04900, 05004], lr: 0.082866, loss: 2.5096
2022-07-14 20:22:09 - train: epoch 0058, iter [05000, 05004], lr: 0.082854, loss: 2.2683
2022-07-14 20:22:10 - train: epoch 058, train_loss: 2.7686
2022-07-14 20:23:24 - eval: epoch: 058, acc1: 58.160%, acc5: 81.986%, test_loss: 1.7697, per_image_load_time: 2.395ms, per_image_inference_time: 0.440ms
2022-07-14 20:23:25 - until epoch: 058, best_acc1: 58.160%
2022-07-14 20:23:25 - epoch 059 lr: 0.082853
2022-07-14 20:24:04 - train: epoch 0059, iter [00100, 05004], lr: 0.082841, loss: 2.6788
2022-07-14 20:24:39 - train: epoch 0059, iter [00200, 05004], lr: 0.082829, loss: 2.6325
2022-07-14 20:25:12 - train: epoch 0059, iter [00300, 05004], lr: 0.082817, loss: 2.5041
2022-07-14 20:25:46 - train: epoch 0059, iter [00400, 05004], lr: 0.082805, loss: 2.9491
2022-07-14 20:26:21 - train: epoch 0059, iter [00500, 05004], lr: 0.082793, loss: 2.7692
2022-07-14 20:26:54 - train: epoch 0059, iter [00600, 05004], lr: 0.082781, loss: 2.8247
2022-07-14 20:27:28 - train: epoch 0059, iter [00700, 05004], lr: 0.082769, loss: 2.7445
2022-07-14 20:28:02 - train: epoch 0059, iter [00800, 05004], lr: 0.082756, loss: 2.7310
2022-07-14 20:28:35 - train: epoch 0059, iter [00900, 05004], lr: 0.082744, loss: 2.9271
2022-07-14 20:29:10 - train: epoch 0059, iter [01000, 05004], lr: 0.082732, loss: 2.8592
2022-07-14 20:29:44 - train: epoch 0059, iter [01100, 05004], lr: 0.082720, loss: 3.0044
2022-07-14 20:30:17 - train: epoch 0059, iter [01200, 05004], lr: 0.082708, loss: 2.5021
2022-07-14 20:30:52 - train: epoch 0059, iter [01300, 05004], lr: 0.082696, loss: 2.9610
2022-07-14 20:31:26 - train: epoch 0059, iter [01400, 05004], lr: 0.082683, loss: 2.6463
2022-07-14 20:31:59 - train: epoch 0059, iter [01500, 05004], lr: 0.082671, loss: 2.9787
2022-07-14 20:32:34 - train: epoch 0059, iter [01600, 05004], lr: 0.082659, loss: 2.7320
2022-07-14 20:33:07 - train: epoch 0059, iter [01700, 05004], lr: 0.082647, loss: 2.9010
2022-07-14 20:33:41 - train: epoch 0059, iter [01800, 05004], lr: 0.082635, loss: 2.6656
2022-07-14 20:34:15 - train: epoch 0059, iter [01900, 05004], lr: 0.082622, loss: 2.3993
2022-07-14 20:34:50 - train: epoch 0059, iter [02000, 05004], lr: 0.082610, loss: 2.6421
2022-07-14 20:35:24 - train: epoch 0059, iter [02100, 05004], lr: 0.082598, loss: 2.7803
2022-07-14 20:35:57 - train: epoch 0059, iter [02200, 05004], lr: 0.082586, loss: 2.7410
2022-07-14 20:36:31 - train: epoch 0059, iter [02300, 05004], lr: 0.082574, loss: 2.5488
2022-07-14 20:37:05 - train: epoch 0059, iter [02400, 05004], lr: 0.082561, loss: 2.9018
2022-07-14 20:37:39 - train: epoch 0059, iter [02500, 05004], lr: 0.082549, loss: 2.6218
2022-07-14 20:38:13 - train: epoch 0059, iter [02600, 05004], lr: 0.082537, loss: 2.7905
2022-07-14 20:38:47 - train: epoch 0059, iter [02700, 05004], lr: 0.082525, loss: 2.7619
2022-07-14 20:39:22 - train: epoch 0059, iter [02800, 05004], lr: 0.082512, loss: 2.8957
2022-07-14 20:39:55 - train: epoch 0059, iter [02900, 05004], lr: 0.082500, loss: 2.3989
2022-07-14 20:40:29 - train: epoch 0059, iter [03000, 05004], lr: 0.082488, loss: 2.8978
2022-07-14 20:41:04 - train: epoch 0059, iter [03100, 05004], lr: 0.082476, loss: 2.6354
2022-07-14 20:41:38 - train: epoch 0059, iter [03200, 05004], lr: 0.082464, loss: 2.4495
2022-07-14 20:42:11 - train: epoch 0059, iter [03300, 05004], lr: 0.082451, loss: 2.9797
2022-07-14 20:42:46 - train: epoch 0059, iter [03400, 05004], lr: 0.082439, loss: 3.2014
2022-07-14 20:43:20 - train: epoch 0059, iter [03500, 05004], lr: 0.082427, loss: 2.7638
2022-07-14 20:43:54 - train: epoch 0059, iter [03600, 05004], lr: 0.082415, loss: 2.7124
2022-07-14 20:44:29 - train: epoch 0059, iter [03700, 05004], lr: 0.082402, loss: 2.8662
2022-07-14 20:45:03 - train: epoch 0059, iter [03800, 05004], lr: 0.082390, loss: 2.5990
2022-07-14 20:45:37 - train: epoch 0059, iter [03900, 05004], lr: 0.082378, loss: 2.7454
2022-07-14 20:46:11 - train: epoch 0059, iter [04000, 05004], lr: 0.082365, loss: 3.1249
2022-07-14 20:46:45 - train: epoch 0059, iter [04100, 05004], lr: 0.082353, loss: 2.6291
2022-07-14 20:47:19 - train: epoch 0059, iter [04200, 05004], lr: 0.082341, loss: 2.7337
2022-07-14 20:47:54 - train: epoch 0059, iter [04300, 05004], lr: 0.082329, loss: 3.1335
2022-07-14 20:48:27 - train: epoch 0059, iter [04400, 05004], lr: 0.082316, loss: 2.7312
2022-07-14 20:49:01 - train: epoch 0059, iter [04500, 05004], lr: 0.082304, loss: 2.7320
2022-07-14 20:49:36 - train: epoch 0059, iter [04600, 05004], lr: 0.082292, loss: 2.8566
2022-07-14 20:50:10 - train: epoch 0059, iter [04700, 05004], lr: 0.082279, loss: 2.9089
2022-07-14 20:50:44 - train: epoch 0059, iter [04800, 05004], lr: 0.082267, loss: 2.7940
2022-07-14 20:51:18 - train: epoch 0059, iter [04900, 05004], lr: 0.082255, loss: 2.7060
2022-07-14 20:51:50 - train: epoch 0059, iter [05000, 05004], lr: 0.082243, loss: 2.7520
2022-07-14 20:51:52 - train: epoch 059, train_loss: 2.7673
2022-07-14 20:53:05 - eval: epoch: 059, acc1: 59.190%, acc5: 83.158%, test_loss: 1.7061, per_image_load_time: 2.113ms, per_image_inference_time: 0.456ms
2022-07-14 20:53:05 - until epoch: 059, best_acc1: 59.190%
2022-07-14 20:53:05 - epoch 060 lr: 0.082242
2022-07-14 20:53:44 - train: epoch 0060, iter [00100, 05004], lr: 0.082230, loss: 2.9786
2022-07-14 20:54:18 - train: epoch 0060, iter [00200, 05004], lr: 0.082217, loss: 2.7226
2022-07-14 20:54:51 - train: epoch 0060, iter [00300, 05004], lr: 0.082205, loss: 2.7417
2022-07-14 20:55:25 - train: epoch 0060, iter [00400, 05004], lr: 0.082193, loss: 2.8756
2022-07-14 20:55:59 - train: epoch 0060, iter [00500, 05004], lr: 0.082181, loss: 2.8482
2022-07-14 20:56:32 - train: epoch 0060, iter [00600, 05004], lr: 0.082168, loss: 3.2877
2022-07-14 20:57:06 - train: epoch 0060, iter [00700, 05004], lr: 0.082156, loss: 3.1257
2022-07-14 20:57:40 - train: epoch 0060, iter [00800, 05004], lr: 0.082144, loss: 2.8808
2022-07-14 20:58:14 - train: epoch 0060, iter [00900, 05004], lr: 0.082131, loss: 2.6827
2022-07-14 20:58:49 - train: epoch 0060, iter [01000, 05004], lr: 0.082119, loss: 2.5405
2022-07-14 20:59:23 - train: epoch 0060, iter [01100, 05004], lr: 0.082107, loss: 2.6545
2022-07-14 20:59:57 - train: epoch 0060, iter [01200, 05004], lr: 0.082094, loss: 2.4838
2022-07-14 21:00:31 - train: epoch 0060, iter [01300, 05004], lr: 0.082082, loss: 2.9145
2022-07-14 21:01:04 - train: epoch 0060, iter [01400, 05004], lr: 0.082070, loss: 2.6234
2022-07-14 21:01:39 - train: epoch 0060, iter [01500, 05004], lr: 0.082057, loss: 2.7725
2022-07-14 21:02:13 - train: epoch 0060, iter [01600, 05004], lr: 0.082045, loss: 2.4453
2022-07-14 21:02:47 - train: epoch 0060, iter [01700, 05004], lr: 0.082032, loss: 2.7082
2022-07-14 21:03:22 - train: epoch 0060, iter [01800, 05004], lr: 0.082020, loss: 2.9177
2022-07-14 21:03:55 - train: epoch 0060, iter [01900, 05004], lr: 0.082008, loss: 2.7928
2022-07-14 21:04:30 - train: epoch 0060, iter [02000, 05004], lr: 0.081995, loss: 2.6788
2022-07-14 21:05:04 - train: epoch 0060, iter [02100, 05004], lr: 0.081983, loss: 2.9138
2022-07-14 21:05:38 - train: epoch 0060, iter [02200, 05004], lr: 0.081971, loss: 2.8990
2022-07-14 21:06:11 - train: epoch 0060, iter [02300, 05004], lr: 0.081958, loss: 2.3712
2022-07-14 21:06:45 - train: epoch 0060, iter [02400, 05004], lr: 0.081946, loss: 2.9610
2022-07-14 21:07:19 - train: epoch 0060, iter [02500, 05004], lr: 0.081933, loss: 2.7664
2022-07-14 21:07:52 - train: epoch 0060, iter [02600, 05004], lr: 0.081921, loss: 2.8966
2022-07-14 21:08:26 - train: epoch 0060, iter [02700, 05004], lr: 0.081909, loss: 3.0292
2022-07-14 21:09:01 - train: epoch 0060, iter [02800, 05004], lr: 0.081896, loss: 2.9821
2022-07-14 21:09:35 - train: epoch 0060, iter [02900, 05004], lr: 0.081884, loss: 2.6316
2022-07-14 21:10:09 - train: epoch 0060, iter [03000, 05004], lr: 0.081871, loss: 2.8101
2022-07-14 21:10:42 - train: epoch 0060, iter [03100, 05004], lr: 0.081859, loss: 2.9116
2022-07-14 21:11:16 - train: epoch 0060, iter [03200, 05004], lr: 0.081847, loss: 2.7201
2022-07-14 21:11:50 - train: epoch 0060, iter [03300, 05004], lr: 0.081834, loss: 2.4884
2022-07-14 21:12:24 - train: epoch 0060, iter [03400, 05004], lr: 0.081822, loss: 2.6323
2022-07-14 21:12:57 - train: epoch 0060, iter [03500, 05004], lr: 0.081809, loss: 3.1016
2022-07-14 21:13:31 - train: epoch 0060, iter [03600, 05004], lr: 0.081797, loss: 3.0220
2022-07-14 21:14:05 - train: epoch 0060, iter [03700, 05004], lr: 0.081785, loss: 2.8280
2022-07-14 21:14:39 - train: epoch 0060, iter [03800, 05004], lr: 0.081772, loss: 2.7552
2022-07-14 21:15:13 - train: epoch 0060, iter [03900, 05004], lr: 0.081760, loss: 2.7464
2022-07-14 21:15:47 - train: epoch 0060, iter [04000, 05004], lr: 0.081747, loss: 2.8419
2022-07-14 21:16:20 - train: epoch 0060, iter [04100, 05004], lr: 0.081735, loss: 3.0129
2022-07-14 21:16:55 - train: epoch 0060, iter [04200, 05004], lr: 0.081722, loss: 3.0459
2022-07-14 21:17:29 - train: epoch 0060, iter [04300, 05004], lr: 0.081710, loss: 2.6184
2022-07-14 21:18:04 - train: epoch 0060, iter [04400, 05004], lr: 0.081698, loss: 2.5753
2022-07-14 21:18:38 - train: epoch 0060, iter [04500, 05004], lr: 0.081685, loss: 2.9212
2022-07-14 21:19:13 - train: epoch 0060, iter [04600, 05004], lr: 0.081673, loss: 2.6707
2022-07-14 21:19:48 - train: epoch 0060, iter [04700, 05004], lr: 0.081660, loss: 2.6808
2022-07-14 21:20:22 - train: epoch 0060, iter [04800, 05004], lr: 0.081648, loss: 2.8999
2022-07-14 21:20:56 - train: epoch 0060, iter [04900, 05004], lr: 0.081635, loss: 2.7085
2022-07-14 21:21:29 - train: epoch 0060, iter [05000, 05004], lr: 0.081623, loss: 2.9090
2022-07-14 21:21:30 - train: epoch 060, train_loss: 2.7627
2022-07-14 21:22:44 - eval: epoch: 060, acc1: 58.458%, acc5: 82.688%, test_loss: 1.7303, per_image_load_time: 2.398ms, per_image_inference_time: 0.475ms
2022-07-14 21:22:45 - until epoch: 060, best_acc1: 59.190%
2022-07-14 21:22:45 - epoch 061 lr: 0.081622
2022-07-14 21:23:24 - train: epoch 0061, iter [00100, 05004], lr: 0.081610, loss: 2.6865
2022-07-14 21:23:59 - train: epoch 0061, iter [00200, 05004], lr: 0.081597, loss: 2.9527
2022-07-14 21:24:33 - train: epoch 0061, iter [00300, 05004], lr: 0.081585, loss: 2.8972
2022-07-14 21:25:07 - train: epoch 0061, iter [00400, 05004], lr: 0.081572, loss: 3.0009
2022-07-14 21:25:41 - train: epoch 0061, iter [00500, 05004], lr: 0.081560, loss: 2.7309
