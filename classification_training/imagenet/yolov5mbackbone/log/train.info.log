2022-02-26 22:58:49 - train: epoch 0089, iter [04300, 05004], lr: 0.001000, loss: 1.3470
2022-02-26 22:59:25 - train: epoch 0089, iter [04400, 05004], lr: 0.001000, loss: 1.2615
2022-02-26 23:00:00 - train: epoch 0089, iter [04500, 05004], lr: 0.001000, loss: 1.1087
2022-02-26 23:00:38 - train: epoch 0089, iter [04600, 05004], lr: 0.001000, loss: 1.2102
2022-02-26 23:01:15 - train: epoch 0089, iter [04700, 05004], lr: 0.001000, loss: 1.2922
2022-02-26 23:01:52 - train: epoch 0089, iter [04800, 05004], lr: 0.001000, loss: 1.3557
2022-02-26 23:02:28 - train: epoch 0089, iter [04900, 05004], lr: 0.001000, loss: 1.3064
2022-02-26 23:03:06 - train: epoch 0089, iter [05000, 05004], lr: 0.001000, loss: 1.0674
2022-02-26 23:03:07 - train: epoch 089, train_loss: 1.2954
2022-02-26 23:04:31 - eval: epoch: 089, acc1: 71.552%, acc5: 90.230%, test_loss: 1.1402, per_image_load_time: 1.039ms, per_image_inference_time: 0.575ms
2022-02-26 23:04:31 - until epoch: 089, best_acc1: 71.592%
2022-02-26 23:04:31 - epoch 090 lr: 0.0010000000000000002
2022-02-26 23:05:15 - train: epoch 0090, iter [00100, 05004], lr: 0.001000, loss: 1.3114
2022-02-26 23:05:52 - train: epoch 0090, iter [00200, 05004], lr: 0.001000, loss: 1.3270
2022-02-26 23:06:29 - train: epoch 0090, iter [00300, 05004], lr: 0.001000, loss: 1.2468
2022-02-26 23:07:06 - train: epoch 0090, iter [00400, 05004], lr: 0.001000, loss: 1.1425
2022-02-26 23:07:43 - train: epoch 0090, iter [00500, 05004], lr: 0.001000, loss: 1.3250
2022-02-26 23:08:23 - train: epoch 0090, iter [00600, 05004], lr: 0.001000, loss: 1.4692
2022-02-26 23:09:02 - train: epoch 0090, iter [00700, 05004], lr: 0.001000, loss: 1.3661
2022-02-26 23:09:41 - train: epoch 0090, iter [00800, 05004], lr: 0.001000, loss: 1.1951
2022-02-26 23:10:20 - train: epoch 0090, iter [00900, 05004], lr: 0.001000, loss: 1.2556
2022-02-26 23:10:57 - train: epoch 0090, iter [01000, 05004], lr: 0.001000, loss: 1.0419
2022-02-26 23:11:35 - train: epoch 0090, iter [01100, 05004], lr: 0.001000, loss: 1.4033
2022-02-26 23:12:15 - train: epoch 0090, iter [01200, 05004], lr: 0.001000, loss: 1.2962
2022-02-26 23:12:54 - train: epoch 0090, iter [01300, 05004], lr: 0.001000, loss: 1.5000
2022-02-26 23:13:34 - train: epoch 0090, iter [01400, 05004], lr: 0.001000, loss: 1.1175
2022-02-26 23:14:13 - train: epoch 0090, iter [01500, 05004], lr: 0.001000, loss: 1.6325
2022-02-26 23:14:52 - train: epoch 0090, iter [01600, 05004], lr: 0.001000, loss: 1.2059
2022-02-26 23:15:29 - train: epoch 0090, iter [01700, 05004], lr: 0.001000, loss: 1.1200
2022-02-26 23:16:06 - train: epoch 0090, iter [01800, 05004], lr: 0.001000, loss: 1.2040
2022-02-26 23:16:43 - train: epoch 0090, iter [01900, 05004], lr: 0.001000, loss: 1.2152
2022-02-26 23:17:20 - train: epoch 0090, iter [02000, 05004], lr: 0.001000, loss: 1.4485
2022-02-26 23:17:59 - train: epoch 0090, iter [02100, 05004], lr: 0.001000, loss: 1.4401
2022-02-26 23:18:39 - train: epoch 0090, iter [02200, 05004], lr: 0.001000, loss: 1.3242
2022-02-26 23:19:16 - train: epoch 0090, iter [02300, 05004], lr: 0.001000, loss: 1.4256
2022-02-26 23:19:54 - train: epoch 0090, iter [02400, 05004], lr: 0.001000, loss: 1.3061
2022-02-26 23:20:32 - train: epoch 0090, iter [02500, 05004], lr: 0.001000, loss: 1.4044
2022-02-26 23:21:08 - train: epoch 0090, iter [02600, 05004], lr: 0.001000, loss: 1.3340
2022-02-26 23:21:45 - train: epoch 0090, iter [02700, 05004], lr: 0.001000, loss: 1.2010
2022-02-26 23:22:22 - train: epoch 0090, iter [02800, 05004], lr: 0.001000, loss: 1.3787
2022-02-26 23:23:00 - train: epoch 0090, iter [02900, 05004], lr: 0.001000, loss: 1.2433
2022-02-26 23:23:41 - train: epoch 0090, iter [03000, 05004], lr: 0.001000, loss: 1.2145
2022-02-26 23:24:18 - train: epoch 0090, iter [03100, 05004], lr: 0.001000, loss: 1.1681
2022-02-26 23:24:56 - train: epoch 0090, iter [03200, 05004], lr: 0.001000, loss: 1.2787
2022-02-26 23:25:33 - train: epoch 0090, iter [03300, 05004], lr: 0.001000, loss: 1.4524
2022-02-26 23:26:10 - train: epoch 0090, iter [03400, 05004], lr: 0.001000, loss: 1.1429
2022-02-26 23:26:48 - train: epoch 0090, iter [03500, 05004], lr: 0.001000, loss: 1.3044
2022-02-26 23:27:24 - train: epoch 0090, iter [03600, 05004], lr: 0.001000, loss: 1.2730
2022-02-26 23:28:02 - train: epoch 0090, iter [03700, 05004], lr: 0.001000, loss: 1.2888
2022-02-26 23:28:42 - train: epoch 0090, iter [03800, 05004], lr: 0.001000, loss: 1.3183
2022-02-26 23:29:20 - train: epoch 0090, iter [03900, 05004], lr: 0.001000, loss: 1.0967
2022-02-26 23:29:58 - train: epoch 0090, iter [04000, 05004], lr: 0.001000, loss: 1.1604
2022-02-26 23:30:37 - train: epoch 0090, iter [04100, 05004], lr: 0.001000, loss: 1.6141
2022-02-26 23:31:14 - train: epoch 0090, iter [04200, 05004], lr: 0.001000, loss: 1.4544
2022-02-26 23:31:50 - train: epoch 0090, iter [04300, 05004], lr: 0.001000, loss: 1.3497
2022-02-26 23:32:26 - train: epoch 0090, iter [04400, 05004], lr: 0.001000, loss: 1.2696
2022-02-26 23:33:04 - train: epoch 0090, iter [04500, 05004], lr: 0.001000, loss: 1.3023
2022-02-26 23:33:45 - train: epoch 0090, iter [04600, 05004], lr: 0.001000, loss: 1.2718
2022-02-26 23:34:24 - train: epoch 0090, iter [04700, 05004], lr: 0.001000, loss: 1.2472
2022-02-26 23:35:00 - train: epoch 0090, iter [04800, 05004], lr: 0.001000, loss: 1.3911
2022-02-26 23:35:38 - train: epoch 0090, iter [04900, 05004], lr: 0.001000, loss: 1.2238
2022-02-26 23:36:15 - train: epoch 0090, iter [05000, 05004], lr: 0.001000, loss: 1.3138
2022-02-26 23:36:17 - train: epoch 090, train_loss: 1.2938
2022-02-26 23:37:38 - eval: epoch: 090, acc1: 71.498%, acc5: 90.370%, test_loss: 1.1361, per_image_load_time: 2.297ms, per_image_inference_time: 0.583ms
2022-02-26 23:37:38 - until epoch: 090, best_acc1: 71.592%
2022-02-26 23:37:38 - epoch 091 lr: 0.00010000000000000003
2022-02-26 23:38:24 - train: epoch 0091, iter [00100, 05004], lr: 0.000100, loss: 1.2317
2022-02-26 23:39:03 - train: epoch 0091, iter [00200, 05004], lr: 0.000100, loss: 1.3202
2022-02-26 23:39:42 - train: epoch 0091, iter [00300, 05004], lr: 0.000100, loss: 1.2454
2022-02-26 23:40:21 - train: epoch 0091, iter [00400, 05004], lr: 0.000100, loss: 1.2021
2022-02-26 23:40:59 - train: epoch 0091, iter [00500, 05004], lr: 0.000100, loss: 1.2109
2022-02-26 23:41:37 - train: epoch 0091, iter [00600, 05004], lr: 0.000100, loss: 1.2694
2022-02-26 23:42:14 - train: epoch 0091, iter [00700, 05004], lr: 0.000100, loss: 1.2154
2022-02-26 23:42:52 - train: epoch 0091, iter [00800, 05004], lr: 0.000100, loss: 1.1148
2022-02-26 23:43:32 - train: epoch 0091, iter [00900, 05004], lr: 0.000100, loss: 1.3164
2022-02-26 23:44:09 - train: epoch 0091, iter [01000, 05004], lr: 0.000100, loss: 1.1931
2022-02-26 23:44:47 - train: epoch 0091, iter [01100, 05004], lr: 0.000100, loss: 1.1128
2022-02-26 23:45:25 - train: epoch 0091, iter [01200, 05004], lr: 0.000100, loss: 1.3130
2022-02-26 23:46:02 - train: epoch 0091, iter [01300, 05004], lr: 0.000100, loss: 1.2011
2022-02-26 23:46:39 - train: epoch 0091, iter [01400, 05004], lr: 0.000100, loss: 1.3461
2022-02-26 23:47:15 - train: epoch 0091, iter [01500, 05004], lr: 0.000100, loss: 1.2629
2022-02-26 23:47:53 - train: epoch 0091, iter [01600, 05004], lr: 0.000100, loss: 1.2608
2022-02-26 23:48:32 - train: epoch 0091, iter [01700, 05004], lr: 0.000100, loss: 1.3802
2022-02-26 23:49:11 - train: epoch 0091, iter [01800, 05004], lr: 0.000100, loss: 1.2891
2022-02-26 23:49:49 - train: epoch 0091, iter [01900, 05004], lr: 0.000100, loss: 1.1839
2022-02-26 23:50:26 - train: epoch 0091, iter [02000, 05004], lr: 0.000100, loss: 1.2877
2022-02-26 23:51:05 - train: epoch 0091, iter [02100, 05004], lr: 0.000100, loss: 1.0446
2022-02-26 23:51:42 - train: epoch 0091, iter [02200, 05004], lr: 0.000100, loss: 1.3493
2022-02-26 23:52:18 - train: epoch 0091, iter [02300, 05004], lr: 0.000100, loss: 1.3944
2022-02-26 23:52:56 - train: epoch 0091, iter [02400, 05004], lr: 0.000100, loss: 1.2367
2022-02-26 23:53:36 - train: epoch 0091, iter [02500, 05004], lr: 0.000100, loss: 1.2671
2022-02-26 23:54:13 - train: epoch 0091, iter [02600, 05004], lr: 0.000100, loss: 1.2398
2022-02-26 23:54:51 - train: epoch 0091, iter [02700, 05004], lr: 0.000100, loss: 1.1391
2022-02-26 23:55:28 - train: epoch 0091, iter [02800, 05004], lr: 0.000100, loss: 1.2998
2022-02-26 23:56:06 - train: epoch 0091, iter [02900, 05004], lr: 0.000100, loss: 1.3797
2022-02-26 23:56:42 - train: epoch 0091, iter [03000, 05004], lr: 0.000100, loss: 1.3924
2022-02-26 23:57:18 - train: epoch 0091, iter [03100, 05004], lr: 0.000100, loss: 1.2210
2022-02-26 23:57:56 - train: epoch 0091, iter [03200, 05004], lr: 0.000100, loss: 1.3752
2022-02-26 23:58:36 - train: epoch 0091, iter [03300, 05004], lr: 0.000100, loss: 1.1995
2022-02-26 23:59:14 - train: epoch 0091, iter [03400, 05004], lr: 0.000100, loss: 1.1564
2022-02-26 23:59:52 - train: epoch 0091, iter [03500, 05004], lr: 0.000100, loss: 1.2722
2022-02-27 00:00:29 - train: epoch 0091, iter [03600, 05004], lr: 0.000100, loss: 1.2007
2022-02-27 00:01:06 - train: epoch 0091, iter [03700, 05004], lr: 0.000100, loss: 1.2964
2022-02-27 00:01:42 - train: epoch 0091, iter [03800, 05004], lr: 0.000100, loss: 1.2670
2022-02-27 00:02:19 - train: epoch 0091, iter [03900, 05004], lr: 0.000100, loss: 1.1831
2022-02-27 00:02:56 - train: epoch 0091, iter [04000, 05004], lr: 0.000100, loss: 1.2610
2022-02-27 00:03:36 - train: epoch 0091, iter [04100, 05004], lr: 0.000100, loss: 1.2633
2022-02-27 00:04:13 - train: epoch 0091, iter [04200, 05004], lr: 0.000100, loss: 1.2920
2022-02-27 00:04:50 - train: epoch 0091, iter [04300, 05004], lr: 0.000100, loss: 1.3545
2022-02-27 00:05:27 - train: epoch 0091, iter [04400, 05004], lr: 0.000100, loss: 1.0837
2022-02-27 00:06:04 - train: epoch 0091, iter [04500, 05004], lr: 0.000100, loss: 1.2119
2022-02-27 00:06:41 - train: epoch 0091, iter [04600, 05004], lr: 0.000100, loss: 1.2858
2022-02-27 00:07:18 - train: epoch 0091, iter [04700, 05004], lr: 0.000100, loss: 1.3162
2022-02-27 00:07:55 - train: epoch 0091, iter [04800, 05004], lr: 0.000100, loss: 1.3364
2022-02-27 00:08:35 - train: epoch 0091, iter [04900, 05004], lr: 0.000100, loss: 1.2289
2022-02-27 00:09:12 - train: epoch 0091, iter [05000, 05004], lr: 0.000100, loss: 1.4606
2022-02-27 00:09:14 - train: epoch 091, train_loss: 1.2634
2022-02-27 00:10:38 - eval: epoch: 091, acc1: 71.930%, acc5: 90.570%, test_loss: 1.1198, per_image_load_time: 1.109ms, per_image_inference_time: 0.574ms
2022-02-27 00:10:38 - until epoch: 091, best_acc1: 71.930%
2022-02-27 00:10:38 - epoch 092 lr: 0.00010000000000000003
2022-02-27 00:11:22 - train: epoch 0092, iter [00100, 05004], lr: 0.000100, loss: 1.2723
2022-02-27 00:11:58 - train: epoch 0092, iter [00200, 05004], lr: 0.000100, loss: 1.3646
2022-02-27 00:12:38 - train: epoch 0092, iter [00300, 05004], lr: 0.000100, loss: 1.2680
2022-02-27 00:13:18 - train: epoch 0092, iter [00400, 05004], lr: 0.000100, loss: 1.1198
2022-02-27 00:13:59 - train: epoch 0092, iter [00500, 05004], lr: 0.000100, loss: 1.4004
2022-02-27 00:14:38 - train: epoch 0092, iter [00600, 05004], lr: 0.000100, loss: 1.2326
2022-02-27 00:15:17 - train: epoch 0092, iter [00700, 05004], lr: 0.000100, loss: 1.2824
2022-02-27 00:15:55 - train: epoch 0092, iter [00800, 05004], lr: 0.000100, loss: 1.3645
2022-02-27 00:16:34 - train: epoch 0092, iter [00900, 05004], lr: 0.000100, loss: 1.2649
2022-02-27 00:17:13 - train: epoch 0092, iter [01000, 05004], lr: 0.000100, loss: 1.4445
2022-02-27 00:17:53 - train: epoch 0092, iter [01100, 05004], lr: 0.000100, loss: 1.1733
2022-02-27 00:18:31 - train: epoch 0092, iter [01200, 05004], lr: 0.000100, loss: 1.1436
2022-02-27 00:19:09 - train: epoch 0092, iter [01300, 05004], lr: 0.000100, loss: 1.2146
2022-02-27 00:19:50 - train: epoch 0092, iter [01400, 05004], lr: 0.000100, loss: 1.1584
2022-02-27 00:20:30 - train: epoch 0092, iter [01500, 05004], lr: 0.000100, loss: 1.0929
2022-02-27 00:21:09 - train: epoch 0092, iter [01600, 05004], lr: 0.000100, loss: 1.2174
2022-02-27 00:21:48 - train: epoch 0092, iter [01700, 05004], lr: 0.000100, loss: 1.3293
2022-02-27 00:22:28 - train: epoch 0092, iter [01800, 05004], lr: 0.000100, loss: 1.2614
2022-02-27 00:23:07 - train: epoch 0092, iter [01900, 05004], lr: 0.000100, loss: 1.1086
2022-02-27 00:23:45 - train: epoch 0092, iter [02000, 05004], lr: 0.000100, loss: 1.1797
2022-02-27 00:24:24 - train: epoch 0092, iter [02100, 05004], lr: 0.000100, loss: 1.2506
2022-02-27 00:25:04 - train: epoch 0092, iter [02200, 05004], lr: 0.000100, loss: 1.3636
2022-02-27 00:25:44 - train: epoch 0092, iter [02300, 05004], lr: 0.000100, loss: 1.2750
2022-02-27 00:26:24 - train: epoch 0092, iter [02400, 05004], lr: 0.000100, loss: 1.1361
2022-02-27 00:27:03 - train: epoch 0092, iter [02500, 05004], lr: 0.000100, loss: 1.2037
2022-02-27 00:27:43 - train: epoch 0092, iter [02600, 05004], lr: 0.000100, loss: 1.3930
2022-02-27 00:28:22 - train: epoch 0092, iter [02700, 05004], lr: 0.000100, loss: 1.1500
2022-02-27 00:29:01 - train: epoch 0092, iter [02800, 05004], lr: 0.000100, loss: 1.1392
2022-02-27 00:29:39 - train: epoch 0092, iter [02900, 05004], lr: 0.000100, loss: 1.2960
2022-02-27 00:30:20 - train: epoch 0092, iter [03000, 05004], lr: 0.000100, loss: 1.1780
2022-02-27 00:31:00 - train: epoch 0092, iter [03100, 05004], lr: 0.000100, loss: 1.2399
2022-02-27 00:31:40 - train: epoch 0092, iter [03200, 05004], lr: 0.000100, loss: 1.0975
2022-02-27 00:32:19 - train: epoch 0092, iter [03300, 05004], lr: 0.000100, loss: 1.2306
2022-02-27 00:32:58 - train: epoch 0092, iter [03400, 05004], lr: 0.000100, loss: 1.4721
2022-02-27 00:33:37 - train: epoch 0092, iter [03500, 05004], lr: 0.000100, loss: 1.2045
2022-02-27 00:34:17 - train: epoch 0092, iter [03600, 05004], lr: 0.000100, loss: 1.1779
2022-02-27 00:34:55 - train: epoch 0092, iter [03700, 05004], lr: 0.000100, loss: 1.1825
2022-02-27 00:35:34 - train: epoch 0092, iter [03800, 05004], lr: 0.000100, loss: 1.4489
2022-02-27 00:36:15 - train: epoch 0092, iter [03900, 05004], lr: 0.000100, loss: 1.3134
2022-02-27 00:36:55 - train: epoch 0092, iter [04000, 05004], lr: 0.000100, loss: 1.3426
2022-02-27 00:37:34 - train: epoch 0092, iter [04100, 05004], lr: 0.000100, loss: 1.1113
2022-02-27 00:38:13 - train: epoch 0092, iter [04200, 05004], lr: 0.000100, loss: 1.2555
2022-02-27 00:38:52 - train: epoch 0092, iter [04300, 05004], lr: 0.000100, loss: 1.2940
2022-02-27 00:39:30 - train: epoch 0092, iter [04400, 05004], lr: 0.000100, loss: 1.1895
2022-02-27 00:40:08 - train: epoch 0092, iter [04500, 05004], lr: 0.000100, loss: 1.1981
2022-02-27 00:40:47 - train: epoch 0092, iter [04600, 05004], lr: 0.000100, loss: 1.1359
2022-02-27 00:41:28 - train: epoch 0092, iter [04700, 05004], lr: 0.000100, loss: 0.9438
2022-02-27 00:42:08 - train: epoch 0092, iter [04800, 05004], lr: 0.000100, loss: 1.1545
2022-02-27 00:42:47 - train: epoch 0092, iter [04900, 05004], lr: 0.000100, loss: 1.1747
2022-02-27 00:43:25 - train: epoch 0092, iter [05000, 05004], lr: 0.000100, loss: 1.4262
2022-02-27 00:43:27 - train: epoch 092, train_loss: 1.2546
2022-02-27 00:44:54 - eval: epoch: 092, acc1: 71.976%, acc5: 90.592%, test_loss: 1.1179, per_image_load_time: 2.766ms, per_image_inference_time: 0.558ms
2022-02-27 00:44:54 - until epoch: 092, best_acc1: 71.976%
2022-02-27 00:44:54 - epoch 093 lr: 0.00010000000000000003
2022-02-27 00:45:39 - train: epoch 0093, iter [00100, 05004], lr: 0.000100, loss: 1.1278
2022-02-27 00:46:19 - train: epoch 0093, iter [00200, 05004], lr: 0.000100, loss: 1.1388
2022-02-27 00:47:00 - train: epoch 0093, iter [00300, 05004], lr: 0.000100, loss: 1.1980
2022-02-27 00:47:38 - train: epoch 0093, iter [00400, 05004], lr: 0.000100, loss: 1.2937
2022-02-27 00:48:18 - train: epoch 0093, iter [00500, 05004], lr: 0.000100, loss: 1.3781
2022-02-27 00:48:57 - train: epoch 0093, iter [00600, 05004], lr: 0.000100, loss: 1.1291
2022-02-27 00:49:35 - train: epoch 0093, iter [00700, 05004], lr: 0.000100, loss: 1.3619
2022-02-27 00:50:16 - train: epoch 0093, iter [00800, 05004], lr: 0.000100, loss: 1.1314
2022-02-27 00:50:54 - train: epoch 0093, iter [00900, 05004], lr: 0.000100, loss: 1.1598
2022-02-27 00:51:33 - train: epoch 0093, iter [01000, 05004], lr: 0.000100, loss: 1.1313
2022-02-27 00:52:13 - train: epoch 0093, iter [01100, 05004], lr: 0.000100, loss: 1.1322
2022-02-27 00:52:53 - train: epoch 0093, iter [01200, 05004], lr: 0.000100, loss: 1.1949
2022-02-27 00:53:33 - train: epoch 0093, iter [01300, 05004], lr: 0.000100, loss: 1.2489
2022-02-27 00:54:12 - train: epoch 0093, iter [01400, 05004], lr: 0.000100, loss: 1.1264
2022-02-27 00:54:52 - train: epoch 0093, iter [01500, 05004], lr: 0.000100, loss: 1.4038
2022-02-27 00:55:31 - train: epoch 0093, iter [01600, 05004], lr: 0.000100, loss: 1.2828
2022-02-27 00:56:08 - train: epoch 0093, iter [01700, 05004], lr: 0.000100, loss: 1.1401
2022-02-27 00:56:47 - train: epoch 0093, iter [01800, 05004], lr: 0.000100, loss: 1.2657
2022-02-27 00:57:28 - train: epoch 0093, iter [01900, 05004], lr: 0.000100, loss: 1.1992
2022-02-27 00:58:07 - train: epoch 0093, iter [02000, 05004], lr: 0.000100, loss: 1.1822
2022-02-27 00:58:46 - train: epoch 0093, iter [02100, 05004], lr: 0.000100, loss: 1.2103
2022-02-27 00:59:26 - train: epoch 0093, iter [02200, 05004], lr: 0.000100, loss: 1.4476
2022-02-27 01:00:05 - train: epoch 0093, iter [02300, 05004], lr: 0.000100, loss: 1.3735
2022-02-27 01:00:44 - train: epoch 0093, iter [02400, 05004], lr: 0.000100, loss: 1.1524
2022-02-27 01:01:23 - train: epoch 0093, iter [02500, 05004], lr: 0.000100, loss: 1.2472
2022-02-27 01:02:01 - train: epoch 0093, iter [02600, 05004], lr: 0.000100, loss: 1.5289
2022-02-27 01:02:42 - train: epoch 0093, iter [02700, 05004], lr: 0.000100, loss: 1.1867
2022-02-27 01:03:20 - train: epoch 0093, iter [02800, 05004], lr: 0.000100, loss: 1.1946
2022-02-27 01:03:57 - train: epoch 0093, iter [02900, 05004], lr: 0.000100, loss: 1.2621
2022-02-27 01:04:35 - train: epoch 0093, iter [03000, 05004], lr: 0.000100, loss: 1.1697
2022-02-27 01:05:11 - train: epoch 0093, iter [03100, 05004], lr: 0.000100, loss: 1.4477
2022-02-27 01:05:49 - train: epoch 0093, iter [03200, 05004], lr: 0.000100, loss: 1.1819
2022-02-27 01:06:25 - train: epoch 0093, iter [03300, 05004], lr: 0.000100, loss: 1.2488
2022-02-27 01:07:01 - train: epoch 0093, iter [03400, 05004], lr: 0.000100, loss: 1.4913
2022-02-27 01:07:39 - train: epoch 0093, iter [03500, 05004], lr: 0.000100, loss: 1.0348
2022-02-27 01:08:19 - train: epoch 0093, iter [03600, 05004], lr: 0.000100, loss: 1.4197
2022-02-27 01:08:56 - train: epoch 0093, iter [03700, 05004], lr: 0.000100, loss: 1.1580
2022-02-27 01:09:34 - train: epoch 0093, iter [03800, 05004], lr: 0.000100, loss: 1.1136
2022-02-27 01:10:12 - train: epoch 0093, iter [03900, 05004], lr: 0.000100, loss: 1.3394
2022-02-27 01:10:50 - train: epoch 0093, iter [04000, 05004], lr: 0.000100, loss: 1.4382
2022-02-27 01:11:26 - train: epoch 0093, iter [04100, 05004], lr: 0.000100, loss: 1.2986
2022-02-27 01:12:02 - train: epoch 0093, iter [04200, 05004], lr: 0.000100, loss: 1.3521
2022-02-27 01:12:42 - train: epoch 0093, iter [04300, 05004], lr: 0.000100, loss: 1.4859
2022-02-27 01:13:21 - train: epoch 0093, iter [04400, 05004], lr: 0.000100, loss: 1.1602
2022-02-27 01:13:59 - train: epoch 0093, iter [04500, 05004], lr: 0.000100, loss: 1.1503
2022-02-27 01:14:37 - train: epoch 0093, iter [04600, 05004], lr: 0.000100, loss: 1.1139
2022-02-27 01:15:15 - train: epoch 0093, iter [04700, 05004], lr: 0.000100, loss: 1.5996
2022-02-27 01:15:53 - train: epoch 0093, iter [04800, 05004], lr: 0.000100, loss: 1.1887
2022-02-27 01:16:29 - train: epoch 0093, iter [04900, 05004], lr: 0.000100, loss: 1.3672
2022-02-27 01:17:05 - train: epoch 0093, iter [05000, 05004], lr: 0.000100, loss: 1.2228
2022-02-27 01:17:06 - train: epoch 093, train_loss: 1.2523
2022-02-27 01:18:33 - eval: epoch: 093, acc1: 72.002%, acc5: 90.666%, test_loss: 1.1176, per_image_load_time: 2.020ms, per_image_inference_time: 0.582ms
2022-02-27 01:18:34 - until epoch: 093, best_acc1: 72.002%
2022-02-27 01:18:34 - epoch 094 lr: 0.00010000000000000003
2022-02-27 01:19:18 - train: epoch 0094, iter [00100, 05004], lr: 0.000100, loss: 1.2843
2022-02-27 01:19:56 - train: epoch 0094, iter [00200, 05004], lr: 0.000100, loss: 1.2730
2022-02-27 01:20:33 - train: epoch 0094, iter [00300, 05004], lr: 0.000100, loss: 1.3694
2022-02-27 01:21:10 - train: epoch 0094, iter [00400, 05004], lr: 0.000100, loss: 1.4062
2022-02-27 01:21:47 - train: epoch 0094, iter [00500, 05004], lr: 0.000100, loss: 1.1353
2022-02-27 01:22:24 - train: epoch 0094, iter [00600, 05004], lr: 0.000100, loss: 1.1637
2022-02-27 01:23:05 - train: epoch 0094, iter [00700, 05004], lr: 0.000100, loss: 1.3744
2022-02-27 01:23:44 - train: epoch 0094, iter [00800, 05004], lr: 0.000100, loss: 1.2611
2022-02-27 01:24:22 - train: epoch 0094, iter [00900, 05004], lr: 0.000100, loss: 1.1373
2022-02-27 01:25:00 - train: epoch 0094, iter [01000, 05004], lr: 0.000100, loss: 1.3370
2022-02-27 01:25:38 - train: epoch 0094, iter [01100, 05004], lr: 0.000100, loss: 1.3300
2022-02-27 01:26:15 - train: epoch 0094, iter [01200, 05004], lr: 0.000100, loss: 1.2572
2022-02-27 01:26:53 - train: epoch 0094, iter [01300, 05004], lr: 0.000100, loss: 1.2090
2022-02-27 01:27:30 - train: epoch 0094, iter [01400, 05004], lr: 0.000100, loss: 1.2150
2022-02-27 01:28:11 - train: epoch 0094, iter [01500, 05004], lr: 0.000100, loss: 1.3779
2022-02-27 01:28:50 - train: epoch 0094, iter [01600, 05004], lr: 0.000100, loss: 1.6042
2022-02-27 01:29:28 - train: epoch 0094, iter [01700, 05004], lr: 0.000100, loss: 1.1593
2022-02-27 01:30:06 - train: epoch 0094, iter [01800, 05004], lr: 0.000100, loss: 1.2535
2022-02-27 01:30:44 - train: epoch 0094, iter [01900, 05004], lr: 0.000100, loss: 1.2699
2022-02-27 01:31:22 - train: epoch 0094, iter [02000, 05004], lr: 0.000100, loss: 1.1304
2022-02-27 01:31:58 - train: epoch 0094, iter [02100, 05004], lr: 0.000100, loss: 1.3147
2022-02-27 01:32:36 - train: epoch 0094, iter [02200, 05004], lr: 0.000100, loss: 1.1672
2022-02-27 01:33:17 - train: epoch 0094, iter [02300, 05004], lr: 0.000100, loss: 1.0832
2022-02-27 01:33:56 - train: epoch 0094, iter [02400, 05004], lr: 0.000100, loss: 1.2083
2022-02-27 01:34:35 - train: epoch 0094, iter [02500, 05004], lr: 0.000100, loss: 1.1689
2022-02-27 01:35:14 - train: epoch 0094, iter [02600, 05004], lr: 0.000100, loss: 1.0489
2022-02-27 01:35:53 - train: epoch 0094, iter [02700, 05004], lr: 0.000100, loss: 1.1654
2022-02-27 01:36:30 - train: epoch 0094, iter [02800, 05004], lr: 0.000100, loss: 1.2235
2022-02-27 01:37:08 - train: epoch 0094, iter [02900, 05004], lr: 0.000100, loss: 1.1897
2022-02-27 01:37:46 - train: epoch 0094, iter [03000, 05004], lr: 0.000100, loss: 1.1834
2022-02-27 01:38:27 - train: epoch 0094, iter [03100, 05004], lr: 0.000100, loss: 1.4126
2022-02-27 01:39:05 - train: epoch 0094, iter [03200, 05004], lr: 0.000100, loss: 1.2981
2022-02-27 01:39:44 - train: epoch 0094, iter [03300, 05004], lr: 0.000100, loss: 1.1982
2022-02-27 01:40:22 - train: epoch 0094, iter [03400, 05004], lr: 0.000100, loss: 1.2392
2022-02-27 01:41:00 - train: epoch 0094, iter [03500, 05004], lr: 0.000100, loss: 1.4597
2022-02-27 01:41:38 - train: epoch 0094, iter [03600, 05004], lr: 0.000100, loss: 1.2844
2022-02-27 01:42:15 - train: epoch 0094, iter [03700, 05004], lr: 0.000100, loss: 1.4411
2022-02-27 01:42:56 - train: epoch 0094, iter [03800, 05004], lr: 0.000100, loss: 1.1924
2022-02-27 01:43:36 - train: epoch 0094, iter [03900, 05004], lr: 0.000100, loss: 1.2175
2022-02-27 01:44:14 - train: epoch 0094, iter [04000, 05004], lr: 0.000100, loss: 1.3908
2022-02-27 01:44:52 - train: epoch 0094, iter [04100, 05004], lr: 0.000100, loss: 1.5167
2022-02-27 01:45:31 - train: epoch 0094, iter [04200, 05004], lr: 0.000100, loss: 1.0070
2022-02-27 01:46:09 - train: epoch 0094, iter [04300, 05004], lr: 0.000100, loss: 1.3271
2022-02-27 01:46:46 - train: epoch 0094, iter [04400, 05004], lr: 0.000100, loss: 1.3679
2022-02-27 01:47:24 - train: epoch 0094, iter [04500, 05004], lr: 0.000100, loss: 1.2671
2022-02-27 01:48:06 - train: epoch 0094, iter [04600, 05004], lr: 0.000100, loss: 1.3669
2022-02-27 01:48:44 - train: epoch 0094, iter [04700, 05004], lr: 0.000100, loss: 1.2348
2022-02-27 01:49:22 - train: epoch 0094, iter [04800, 05004], lr: 0.000100, loss: 1.2777
2022-02-27 01:50:01 - train: epoch 0094, iter [04900, 05004], lr: 0.000100, loss: 1.1297
2022-02-27 01:50:38 - train: epoch 0094, iter [05000, 05004], lr: 0.000100, loss: 1.1756
2022-02-27 01:50:39 - train: epoch 094, train_loss: 1.2515
2022-02-27 01:52:04 - eval: epoch: 094, acc1: 71.938%, acc5: 90.686%, test_loss: 1.1168, per_image_load_time: 2.576ms, per_image_inference_time: 0.568ms
2022-02-27 01:52:05 - until epoch: 094, best_acc1: 72.002%
2022-02-27 01:52:05 - epoch 095 lr: 0.00010000000000000003
2022-02-27 01:52:53 - train: epoch 0095, iter [00100, 05004], lr: 0.000100, loss: 1.2071
2022-02-27 01:53:31 - train: epoch 0095, iter [00200, 05004], lr: 0.000100, loss: 1.4674
2022-02-27 01:54:10 - train: epoch 0095, iter [00300, 05004], lr: 0.000100, loss: 1.2441
2022-02-27 01:54:48 - train: epoch 0095, iter [00400, 05004], lr: 0.000100, loss: 1.3250
2022-02-27 01:55:26 - train: epoch 0095, iter [00500, 05004], lr: 0.000100, loss: 1.2646
2022-02-27 01:56:04 - train: epoch 0095, iter [00600, 05004], lr: 0.000100, loss: 1.2550
2022-02-27 01:56:41 - train: epoch 0095, iter [00700, 05004], lr: 0.000100, loss: 1.3552
2022-02-27 01:57:19 - train: epoch 0095, iter [00800, 05004], lr: 0.000100, loss: 1.2568
2022-02-27 01:58:00 - train: epoch 0095, iter [00900, 05004], lr: 0.000100, loss: 1.3973
2022-02-27 01:58:39 - train: epoch 0095, iter [01000, 05004], lr: 0.000100, loss: 1.3057
2022-02-27 01:59:17 - train: epoch 0095, iter [01100, 05004], lr: 0.000100, loss: 1.1159
2022-02-27 01:59:57 - train: epoch 0095, iter [01200, 05004], lr: 0.000100, loss: 1.1172
2022-02-27 02:00:36 - train: epoch 0095, iter [01300, 05004], lr: 0.000100, loss: 1.2306
2022-02-27 02:01:13 - train: epoch 0095, iter [01400, 05004], lr: 0.000100, loss: 1.2815
2022-02-27 02:01:51 - train: epoch 0095, iter [01500, 05004], lr: 0.000100, loss: 1.1947
2022-02-27 02:02:28 - train: epoch 0095, iter [01600, 05004], lr: 0.000100, loss: 0.8373
2022-02-27 02:03:10 - train: epoch 0095, iter [01700, 05004], lr: 0.000100, loss: 1.1776
2022-02-27 02:03:50 - train: epoch 0095, iter [01800, 05004], lr: 0.000100, loss: 1.3470
2022-02-27 02:04:28 - train: epoch 0095, iter [01900, 05004], lr: 0.000100, loss: 1.1507
2022-02-27 02:05:06 - train: epoch 0095, iter [02000, 05004], lr: 0.000100, loss: 1.3047
2022-02-27 02:05:43 - train: epoch 0095, iter [02100, 05004], lr: 0.000100, loss: 1.2456
2022-02-27 02:06:20 - train: epoch 0095, iter [02200, 05004], lr: 0.000100, loss: 0.9885
2022-02-27 02:06:58 - train: epoch 0095, iter [02300, 05004], lr: 0.000100, loss: 1.2524
2022-02-27 02:07:37 - train: epoch 0095, iter [02400, 05004], lr: 0.000100, loss: 1.3729
2022-02-27 02:08:16 - train: epoch 0095, iter [02500, 05004], lr: 0.000100, loss: 1.2290
2022-02-27 02:08:54 - train: epoch 0095, iter [02600, 05004], lr: 0.000100, loss: 1.3262
2022-02-27 02:09:32 - train: epoch 0095, iter [02700, 05004], lr: 0.000100, loss: 1.2464
2022-02-27 02:10:10 - train: epoch 0095, iter [02800, 05004], lr: 0.000100, loss: 1.2303
2022-02-27 02:10:48 - train: epoch 0095, iter [02900, 05004], lr: 0.000100, loss: 1.1954
2022-02-27 02:11:25 - train: epoch 0095, iter [03000, 05004], lr: 0.000100, loss: 1.4222
2022-02-27 02:12:02 - train: epoch 0095, iter [03100, 05004], lr: 0.000100, loss: 1.4513
2022-02-27 02:12:39 - train: epoch 0095, iter [03200, 05004], lr: 0.000100, loss: 1.2766
2022-02-27 02:13:20 - train: epoch 0095, iter [03300, 05004], lr: 0.000100, loss: 1.2703
2022-02-27 02:13:59 - train: epoch 0095, iter [03400, 05004], lr: 0.000100, loss: 1.1579
2022-02-27 02:14:35 - train: epoch 0095, iter [03500, 05004], lr: 0.000100, loss: 1.1897
2022-02-27 02:15:14 - train: epoch 0095, iter [03600, 05004], lr: 0.000100, loss: 1.1337
2022-02-27 02:15:52 - train: epoch 0095, iter [03700, 05004], lr: 0.000100, loss: 1.1768
2022-02-27 02:16:28 - train: epoch 0095, iter [03800, 05004], lr: 0.000100, loss: 1.0023
2022-02-27 02:17:05 - train: epoch 0095, iter [03900, 05004], lr: 0.000100, loss: 1.3480
2022-02-27 02:17:42 - train: epoch 0095, iter [04000, 05004], lr: 0.000100, loss: 1.1367
2022-02-27 02:18:22 - train: epoch 0095, iter [04100, 05004], lr: 0.000100, loss: 1.3197
2022-02-27 02:19:01 - train: epoch 0095, iter [04200, 05004], lr: 0.000100, loss: 1.0760
2022-02-27 02:19:38 - train: epoch 0095, iter [04300, 05004], lr: 0.000100, loss: 1.2554
2022-02-27 02:20:16 - train: epoch 0095, iter [04400, 05004], lr: 0.000100, loss: 1.3094
2022-02-27 02:20:54 - train: epoch 0095, iter [04500, 05004], lr: 0.000100, loss: 1.1580
2022-02-27 02:21:31 - train: epoch 0095, iter [04600, 05004], lr: 0.000100, loss: 1.2689
2022-02-27 02:22:08 - train: epoch 0095, iter [04700, 05004], lr: 0.000100, loss: 1.2567
2022-02-27 02:22:46 - train: epoch 0095, iter [04800, 05004], lr: 0.000100, loss: 1.3226
2022-02-27 02:23:26 - train: epoch 0095, iter [04900, 05004], lr: 0.000100, loss: 1.1544
2022-02-27 02:24:03 - train: epoch 0095, iter [05000, 05004], lr: 0.000100, loss: 1.0611
2022-02-27 02:24:04 - train: epoch 095, train_loss: 1.2525
2022-02-27 02:25:27 - eval: epoch: 095, acc1: 72.062%, acc5: 90.698%, test_loss: 1.1164, per_image_load_time: 1.889ms, per_image_inference_time: 0.585ms
2022-02-27 02:25:28 - until epoch: 095, best_acc1: 72.062%
2022-02-27 02:25:28 - epoch 096 lr: 0.00010000000000000003
2022-02-27 02:26:11 - train: epoch 0096, iter [00100, 05004], lr: 0.000100, loss: 1.2695
2022-02-27 02:26:48 - train: epoch 0096, iter [00200, 05004], lr: 0.000100, loss: 1.3307
2022-02-27 02:27:25 - train: epoch 0096, iter [00300, 05004], lr: 0.000100, loss: 1.2433
2022-02-27 02:28:06 - train: epoch 0096, iter [00400, 05004], lr: 0.000100, loss: 1.0163
2022-02-27 02:28:44 - train: epoch 0096, iter [00500, 05004], lr: 0.000100, loss: 1.1984
2022-02-27 02:29:22 - train: epoch 0096, iter [00600, 05004], lr: 0.000100, loss: 1.4009
2022-02-27 02:29:59 - train: epoch 0096, iter [00700, 05004], lr: 0.000100, loss: 1.1722
2022-02-27 02:30:37 - train: epoch 0096, iter [00800, 05004], lr: 0.000100, loss: 1.1275
2022-02-27 02:31:14 - train: epoch 0096, iter [00900, 05004], lr: 0.000100, loss: 1.1892
2022-02-27 02:31:51 - train: epoch 0096, iter [01000, 05004], lr: 0.000100, loss: 1.1949
2022-02-27 02:32:28 - train: epoch 0096, iter [01100, 05004], lr: 0.000100, loss: 1.2334
2022-02-27 02:33:09 - train: epoch 0096, iter [01200, 05004], lr: 0.000100, loss: 1.3150
2022-02-27 02:33:46 - train: epoch 0096, iter [01300, 05004], lr: 0.000100, loss: 1.3437
2022-02-27 02:34:24 - train: epoch 0096, iter [01400, 05004], lr: 0.000100, loss: 1.1690
2022-02-27 02:35:02 - train: epoch 0096, iter [01500, 05004], lr: 0.000100, loss: 1.1585
2022-02-27 02:35:40 - train: epoch 0096, iter [01600, 05004], lr: 0.000100, loss: 0.9679
2022-02-27 02:36:17 - train: epoch 0096, iter [01700, 05004], lr: 0.000100, loss: 1.2744
2022-02-27 02:36:53 - train: epoch 0096, iter [01800, 05004], lr: 0.000100, loss: 1.3402
2022-02-27 02:37:31 - train: epoch 0096, iter [01900, 05004], lr: 0.000100, loss: 1.3383
2022-02-27 02:38:10 - train: epoch 0096, iter [02000, 05004], lr: 0.000100, loss: 1.1768
2022-02-27 02:38:48 - train: epoch 0096, iter [02100, 05004], lr: 0.000100, loss: 1.1887
2022-02-27 02:39:26 - train: epoch 0096, iter [02200, 05004], lr: 0.000100, loss: 1.0515
2022-02-27 02:40:04 - train: epoch 0096, iter [02300, 05004], lr: 0.000100, loss: 1.1587
2022-02-27 02:40:42 - train: epoch 0096, iter [02400, 05004], lr: 0.000100, loss: 1.1848
2022-02-27 02:41:18 - train: epoch 0096, iter [02500, 05004], lr: 0.000100, loss: 1.1136
2022-02-27 02:41:55 - train: epoch 0096, iter [02600, 05004], lr: 0.000100, loss: 1.2022
2022-02-27 02:42:33 - train: epoch 0096, iter [02700, 05004], lr: 0.000100, loss: 1.3483
2022-02-27 02:43:13 - train: epoch 0096, iter [02800, 05004], lr: 0.000100, loss: 1.3422
2022-02-27 02:43:49 - train: epoch 0096, iter [02900, 05004], lr: 0.000100, loss: 1.2450
2022-02-27 02:44:27 - train: epoch 0096, iter [03000, 05004], lr: 0.000100, loss: 1.2732
2022-02-27 02:45:05 - train: epoch 0096, iter [03100, 05004], lr: 0.000100, loss: 1.3174
2022-02-27 02:45:43 - train: epoch 0096, iter [03200, 05004], lr: 0.000100, loss: 1.2845
2022-02-27 02:46:19 - train: epoch 0096, iter [03300, 05004], lr: 0.000100, loss: 1.3854
2022-02-27 02:46:55 - train: epoch 0096, iter [03400, 05004], lr: 0.000100, loss: 1.0511
2022-02-27 02:47:33 - train: epoch 0096, iter [03500, 05004], lr: 0.000100, loss: 1.1120
2022-02-27 02:48:13 - train: epoch 0096, iter [03600, 05004], lr: 0.000100, loss: 1.0925
2022-02-27 02:48:51 - train: epoch 0096, iter [03700, 05004], lr: 0.000100, loss: 1.2506
2022-02-27 02:49:28 - train: epoch 0096, iter [03800, 05004], lr: 0.000100, loss: 1.2751
2022-02-27 02:50:05 - train: epoch 0096, iter [03900, 05004], lr: 0.000100, loss: 1.2046
2022-02-27 02:50:43 - train: epoch 0096, iter [04000, 05004], lr: 0.000100, loss: 1.0907
2022-02-27 02:51:19 - train: epoch 0096, iter [04100, 05004], lr: 0.000100, loss: 1.1757
2022-02-27 02:51:56 - train: epoch 0096, iter [04200, 05004], lr: 0.000100, loss: 1.3284
2022-02-27 02:52:37 - train: epoch 0096, iter [04300, 05004], lr: 0.000100, loss: 1.0041
2022-02-27 02:53:15 - train: epoch 0096, iter [04400, 05004], lr: 0.000100, loss: 1.2348
2022-02-27 02:53:52 - train: epoch 0096, iter [04500, 05004], lr: 0.000100, loss: 1.1769
2022-02-27 02:54:29 - train: epoch 0096, iter [04600, 05004], lr: 0.000100, loss: 1.1919
2022-02-27 02:55:06 - train: epoch 0096, iter [04700, 05004], lr: 0.000100, loss: 1.2912
2022-02-27 02:55:45 - train: epoch 0096, iter [04800, 05004], lr: 0.000100, loss: 1.3147
2022-02-27 02:56:22 - train: epoch 0096, iter [04900, 05004], lr: 0.000100, loss: 1.3912
2022-02-27 02:56:58 - train: epoch 0096, iter [05000, 05004], lr: 0.000100, loss: 1.2003
2022-02-27 02:56:59 - train: epoch 096, train_loss: 1.2475
2022-02-27 02:58:25 - eval: epoch: 096, acc1: 72.038%, acc5: 90.672%, test_loss: 1.1151, per_image_load_time: 1.562ms, per_image_inference_time: 0.576ms
2022-02-27 02:58:26 - until epoch: 096, best_acc1: 72.062%
2022-02-27 02:58:26 - epoch 097 lr: 0.00010000000000000003
2022-02-27 02:59:09 - train: epoch 0097, iter [00100, 05004], lr: 0.000100, loss: 1.3309
2022-02-27 02:59:45 - train: epoch 0097, iter [00200, 05004], lr: 0.000100, loss: 1.0450
2022-02-27 03:00:23 - train: epoch 0097, iter [00300, 05004], lr: 0.000100, loss: 1.3228
2022-02-27 03:00:59 - train: epoch 0097, iter [00400, 05004], lr: 0.000100, loss: 1.5071
2022-02-27 03:01:35 - train: epoch 0097, iter [00500, 05004], lr: 0.000100, loss: 1.3024
2022-02-27 03:02:14 - train: epoch 0097, iter [00600, 05004], lr: 0.000100, loss: 1.3440
2022-02-27 03:02:52 - train: epoch 0097, iter [00700, 05004], lr: 0.000100, loss: 1.1950
2022-02-27 03:03:30 - train: epoch 0097, iter [00800, 05004], lr: 0.000100, loss: 1.0486
2022-02-27 03:04:07 - train: epoch 0097, iter [00900, 05004], lr: 0.000100, loss: 1.1498
2022-02-27 03:04:46 - train: epoch 0097, iter [01000, 05004], lr: 0.000100, loss: 1.2825
2022-02-27 03:05:23 - train: epoch 0097, iter [01100, 05004], lr: 0.000100, loss: 1.1192
2022-02-27 03:05:59 - train: epoch 0097, iter [01200, 05004], lr: 0.000100, loss: 1.2212
2022-02-27 03:06:36 - train: epoch 0097, iter [01300, 05004], lr: 0.000100, loss: 1.1308
2022-02-27 03:07:15 - train: epoch 0097, iter [01400, 05004], lr: 0.000100, loss: 1.1774
2022-02-27 03:07:54 - train: epoch 0097, iter [01500, 05004], lr: 0.000100, loss: 1.1928
2022-02-27 03:08:32 - train: epoch 0097, iter [01600, 05004], lr: 0.000100, loss: 1.1688
2022-02-27 03:09:10 - train: epoch 0097, iter [01700, 05004], lr: 0.000100, loss: 1.1700
2022-02-27 03:09:48 - train: epoch 0097, iter [01800, 05004], lr: 0.000100, loss: 1.3868
2022-02-27 03:10:24 - train: epoch 0097, iter [01900, 05004], lr: 0.000100, loss: 1.1987
2022-02-27 03:11:00 - train: epoch 0097, iter [02000, 05004], lr: 0.000100, loss: 1.2077
2022-02-27 03:11:37 - train: epoch 0097, iter [02100, 05004], lr: 0.000100, loss: 1.2305
2022-02-27 03:12:17 - train: epoch 0097, iter [02200, 05004], lr: 0.000100, loss: 1.3591
2022-02-27 03:12:56 - train: epoch 0097, iter [02300, 05004], lr: 0.000100, loss: 1.1439
2022-02-27 03:13:33 - train: epoch 0097, iter [02400, 05004], lr: 0.000100, loss: 1.2469
2022-02-27 03:14:11 - train: epoch 0097, iter [02500, 05004], lr: 0.000100, loss: 1.2803
2022-02-27 03:14:48 - train: epoch 0097, iter [02600, 05004], lr: 0.000100, loss: 1.3558
2022-02-27 03:15:25 - train: epoch 0097, iter [02700, 05004], lr: 0.000100, loss: 1.0734
2022-02-27 03:16:02 - train: epoch 0097, iter [02800, 05004], lr: 0.000100, loss: 1.1697
2022-02-27 03:16:39 - train: epoch 0097, iter [02900, 05004], lr: 0.000100, loss: 1.5039
2022-02-27 03:17:19 - train: epoch 0097, iter [03000, 05004], lr: 0.000100, loss: 1.1337
2022-02-27 03:17:56 - train: epoch 0097, iter [03100, 05004], lr: 0.000100, loss: 1.2270
2022-02-27 03:18:34 - train: epoch 0097, iter [03200, 05004], lr: 0.000100, loss: 1.3225
2022-02-27 03:19:12 - train: epoch 0097, iter [03300, 05004], lr: 0.000100, loss: 1.4049
2022-02-27 03:19:49 - train: epoch 0097, iter [03400, 05004], lr: 0.000100, loss: 1.3170
2022-02-27 03:20:26 - train: epoch 0097, iter [03500, 05004], lr: 0.000100, loss: 1.2905
2022-02-27 03:21:04 - train: epoch 0097, iter [03600, 05004], lr: 0.000100, loss: 1.2924
2022-02-27 03:21:40 - train: epoch 0097, iter [03700, 05004], lr: 0.000100, loss: 1.0563
2022-02-27 03:22:21 - train: epoch 0097, iter [03800, 05004], lr: 0.000100, loss: 1.0613
2022-02-27 03:22:57 - train: epoch 0097, iter [03900, 05004], lr: 0.000100, loss: 1.1816
2022-02-27 03:23:37 - train: epoch 0097, iter [04000, 05004], lr: 0.000100, loss: 1.3736
2022-02-27 03:24:15 - train: epoch 0097, iter [04100, 05004], lr: 0.000100, loss: 1.1538
2022-02-27 03:24:53 - train: epoch 0097, iter [04200, 05004], lr: 0.000100, loss: 1.2000
2022-02-27 03:25:31 - train: epoch 0097, iter [04300, 05004], lr: 0.000100, loss: 1.1348
2022-02-27 03:26:08 - train: epoch 0097, iter [04400, 05004], lr: 0.000100, loss: 1.1143
2022-02-27 03:26:44 - train: epoch 0097, iter [04500, 05004], lr: 0.000100, loss: 1.2952
2022-02-27 03:27:25 - train: epoch 0097, iter [04600, 05004], lr: 0.000100, loss: 1.3203
2022-02-27 03:28:04 - train: epoch 0097, iter [04700, 05004], lr: 0.000100, loss: 1.2324
2022-02-27 03:28:42 - train: epoch 0097, iter [04800, 05004], lr: 0.000100, loss: 1.2082
2022-02-27 03:29:20 - train: epoch 0097, iter [04900, 05004], lr: 0.000100, loss: 1.5058
2022-02-27 03:29:56 - train: epoch 0097, iter [05000, 05004], lr: 0.000100, loss: 1.2171
2022-02-27 03:29:58 - train: epoch 097, train_loss: 1.2476
2022-02-27 03:31:21 - eval: epoch: 097, acc1: 72.040%, acc5: 90.692%, test_loss: 1.1134, per_image_load_time: 2.557ms, per_image_inference_time: 0.584ms
2022-02-27 03:31:21 - until epoch: 097, best_acc1: 72.062%
2022-02-27 03:31:21 - epoch 098 lr: 0.00010000000000000003
2022-02-27 03:32:07 - train: epoch 0098, iter [00100, 05004], lr: 0.000100, loss: 1.3453
2022-02-27 03:32:47 - train: epoch 0098, iter [00200, 05004], lr: 0.000100, loss: 1.3663
2022-02-27 03:33:25 - train: epoch 0098, iter [00300, 05004], lr: 0.000100, loss: 1.4983
2022-02-27 03:34:04 - train: epoch 0098, iter [00400, 05004], lr: 0.000100, loss: 1.1210
2022-02-27 03:34:43 - train: epoch 0098, iter [00500, 05004], lr: 0.000100, loss: 1.1749
2022-02-27 03:35:21 - train: epoch 0098, iter [00600, 05004], lr: 0.000100, loss: 1.3039
2022-02-27 03:35:59 - train: epoch 0098, iter [00700, 05004], lr: 0.000100, loss: 1.2054
2022-02-27 03:36:36 - train: epoch 0098, iter [00800, 05004], lr: 0.000100, loss: 1.3015
2022-02-27 03:37:18 - train: epoch 0098, iter [00900, 05004], lr: 0.000100, loss: 1.2410
2022-02-27 03:37:55 - train: epoch 0098, iter [01000, 05004], lr: 0.000100, loss: 1.1092
2022-02-27 03:38:34 - train: epoch 0098, iter [01100, 05004], lr: 0.000100, loss: 1.0424
2022-02-27 03:39:11 - train: epoch 0098, iter [01200, 05004], lr: 0.000100, loss: 1.2009
2022-02-27 03:39:50 - train: epoch 0098, iter [01300, 05004], lr: 0.000100, loss: 1.3469
2022-02-27 03:40:27 - train: epoch 0098, iter [01400, 05004], lr: 0.000100, loss: 1.2097
2022-02-27 03:41:04 - train: epoch 0098, iter [01500, 05004], lr: 0.000100, loss: 1.0927
2022-02-27 03:41:41 - train: epoch 0098, iter [01600, 05004], lr: 0.000100, loss: 1.2268
2022-02-27 03:42:22 - train: epoch 0098, iter [01700, 05004], lr: 0.000100, loss: 1.2426
2022-02-27 03:43:01 - train: epoch 0098, iter [01800, 05004], lr: 0.000100, loss: 1.1955
2022-02-27 03:43:39 - train: epoch 0098, iter [01900, 05004], lr: 0.000100, loss: 1.1000
2022-02-27 03:44:18 - train: epoch 0098, iter [02000, 05004], lr: 0.000100, loss: 1.3616
2022-02-27 03:44:56 - train: epoch 0098, iter [02100, 05004], lr: 0.000100, loss: 1.1407
2022-02-27 03:45:34 - train: epoch 0098, iter [02200, 05004], lr: 0.000100, loss: 1.1554
2022-02-27 03:46:11 - train: epoch 0098, iter [02300, 05004], lr: 0.000100, loss: 1.1885
2022-02-27 03:46:47 - train: epoch 0098, iter [02400, 05004], lr: 0.000100, loss: 1.2279
2022-02-27 03:47:29 - train: epoch 0098, iter [02500, 05004], lr: 0.000100, loss: 1.4104
2022-02-27 03:48:08 - train: epoch 0098, iter [02600, 05004], lr: 0.000100, loss: 1.1674
2022-02-27 03:48:46 - train: epoch 0098, iter [02700, 05004], lr: 0.000100, loss: 1.3050
2022-02-27 03:49:24 - train: epoch 0098, iter [02800, 05004], lr: 0.000100, loss: 1.1549
2022-02-27 03:50:02 - train: epoch 0098, iter [02900, 05004], lr: 0.000100, loss: 1.2614
2022-02-27 03:50:40 - train: epoch 0098, iter [03000, 05004], lr: 0.000100, loss: 1.1885
2022-02-27 03:51:16 - train: epoch 0098, iter [03100, 05004], lr: 0.000100, loss: 1.2424
2022-02-27 03:51:54 - train: epoch 0098, iter [03200, 05004], lr: 0.000100, loss: 1.0469
2022-02-27 03:52:35 - train: epoch 0098, iter [03300, 05004], lr: 0.000100, loss: 1.1213
2022-02-27 03:53:13 - train: epoch 0098, iter [03400, 05004], lr: 0.000100, loss: 1.2985
2022-02-27 03:53:51 - train: epoch 0098, iter [03500, 05004], lr: 0.000100, loss: 1.2120
2022-02-27 03:54:29 - train: epoch 0098, iter [03600, 05004], lr: 0.000100, loss: 1.4963
2022-02-27 03:55:07 - train: epoch 0098, iter [03700, 05004], lr: 0.000100, loss: 1.3508
2022-02-27 03:55:44 - train: epoch 0098, iter [03800, 05004], lr: 0.000100, loss: 1.2758
2022-02-27 03:56:23 - train: epoch 0098, iter [03900, 05004], lr: 0.000100, loss: 1.2413
2022-02-27 03:56:59 - train: epoch 0098, iter [04000, 05004], lr: 0.000100, loss: 1.2221
2022-02-27 03:57:40 - train: epoch 0098, iter [04100, 05004], lr: 0.000100, loss: 1.3705
2022-02-27 03:58:19 - train: epoch 0098, iter [04200, 05004], lr: 0.000100, loss: 1.2584
2022-02-27 03:58:58 - train: epoch 0098, iter [04300, 05004], lr: 0.000100, loss: 1.1143
2022-02-27 03:59:36 - train: epoch 0098, iter [04400, 05004], lr: 0.000100, loss: 1.4785
2022-02-27 04:00:13 - train: epoch 0098, iter [04500, 05004], lr: 0.000100, loss: 1.3664
2022-02-27 04:00:52 - train: epoch 0098, iter [04600, 05004], lr: 0.000100, loss: 1.4119
2022-02-27 04:01:29 - train: epoch 0098, iter [04700, 05004], lr: 0.000100, loss: 1.1788
2022-02-27 04:02:07 - train: epoch 0098, iter [04800, 05004], lr: 0.000100, loss: 1.0440
2022-02-27 04:02:48 - train: epoch 0098, iter [04900, 05004], lr: 0.000100, loss: 1.3050
2022-02-27 04:03:25 - train: epoch 0098, iter [05000, 05004], lr: 0.000100, loss: 1.2789
2022-02-27 04:03:27 - train: epoch 098, train_loss: 1.2466
2022-02-27 04:04:54 - eval: epoch: 098, acc1: 72.104%, acc5: 90.700%, test_loss: 1.1138, per_image_load_time: 2.768ms, per_image_inference_time: 0.571ms
2022-02-27 04:04:54 - until epoch: 098, best_acc1: 72.104%
2022-02-27 04:04:54 - epoch 099 lr: 0.00010000000000000003
2022-02-27 04:05:39 - train: epoch 0099, iter [00100, 05004], lr: 0.000100, loss: 1.0991
2022-02-27 04:06:17 - train: epoch 0099, iter [00200, 05004], lr: 0.000100, loss: 1.1730
2022-02-27 04:06:54 - train: epoch 0099, iter [00300, 05004], lr: 0.000100, loss: 1.1767
2022-02-27 04:07:35 - train: epoch 0099, iter [00400, 05004], lr: 0.000100, loss: 1.2896
2022-02-27 04:08:14 - train: epoch 0099, iter [00500, 05004], lr: 0.000100, loss: 1.3191
2022-02-27 04:08:53 - train: epoch 0099, iter [00600, 05004], lr: 0.000100, loss: 1.1925
2022-02-27 04:09:33 - train: epoch 0099, iter [00700, 05004], lr: 0.000100, loss: 1.2764
2022-02-27 04:10:12 - train: epoch 0099, iter [00800, 05004], lr: 0.000100, loss: 1.2374
2022-02-27 04:10:48 - train: epoch 0099, iter [00900, 05004], lr: 0.000100, loss: 1.1626
2022-02-27 04:11:24 - train: epoch 0099, iter [01000, 05004], lr: 0.000100, loss: 1.2682
2022-02-27 04:12:02 - train: epoch 0099, iter [01100, 05004], lr: 0.000100, loss: 1.2789
2022-02-27 04:12:42 - train: epoch 0099, iter [01200, 05004], lr: 0.000100, loss: 1.1079
2022-02-27 04:13:19 - train: epoch 0099, iter [01300, 05004], lr: 0.000100, loss: 1.2966
2022-02-27 04:13:56 - train: epoch 0099, iter [01400, 05004], lr: 0.000100, loss: 1.3136
2022-02-27 04:14:34 - train: epoch 0099, iter [01500, 05004], lr: 0.000100, loss: 1.1840
2022-02-27 04:15:11 - train: epoch 0099, iter [01600, 05004], lr: 0.000100, loss: 1.4158
2022-02-27 04:15:48 - train: epoch 0099, iter [01700, 05004], lr: 0.000100, loss: 1.1345
2022-02-27 04:16:25 - train: epoch 0099, iter [01800, 05004], lr: 0.000100, loss: 1.1772
2022-02-27 04:17:03 - train: epoch 0099, iter [01900, 05004], lr: 0.000100, loss: 1.1767
2022-02-27 04:17:43 - train: epoch 0099, iter [02000, 05004], lr: 0.000100, loss: 1.0956
2022-02-27 04:18:20 - train: epoch 0099, iter [02100, 05004], lr: 0.000100, loss: 1.0339
2022-02-27 04:18:58 - train: epoch 0099, iter [02200, 05004], lr: 0.000100, loss: 1.2962
2022-02-27 04:19:37 - train: epoch 0099, iter [02300, 05004], lr: 0.000100, loss: 1.3011
2022-02-27 04:20:13 - train: epoch 0099, iter [02400, 05004], lr: 0.000100, loss: 1.4198
2022-02-27 04:20:51 - train: epoch 0099, iter [02500, 05004], lr: 0.000100, loss: 1.2139
2022-02-27 04:21:28 - train: epoch 0099, iter [02600, 05004], lr: 0.000100, loss: 1.1627
2022-02-27 04:22:06 - train: epoch 0099, iter [02700, 05004], lr: 0.000100, loss: 1.2397
2022-02-27 04:22:46 - train: epoch 0099, iter [02800, 05004], lr: 0.000100, loss: 1.4339
2022-02-27 04:23:23 - train: epoch 0099, iter [02900, 05004], lr: 0.000100, loss: 1.2290
2022-02-27 04:24:00 - train: epoch 0099, iter [03000, 05004], lr: 0.000100, loss: 1.3832
2022-02-27 04:24:38 - train: epoch 0099, iter [03100, 05004], lr: 0.000100, loss: 1.2188
2022-02-27 04:25:16 - train: epoch 0099, iter [03200, 05004], lr: 0.000100, loss: 1.3054
2022-02-27 04:25:53 - train: epoch 0099, iter [03300, 05004], lr: 0.000100, loss: 1.0828
2022-02-27 04:26:29 - train: epoch 0099, iter [03400, 05004], lr: 0.000100, loss: 1.2416
2022-02-27 04:27:07 - train: epoch 0099, iter [03500, 05004], lr: 0.000100, loss: 1.3425
2022-02-27 04:27:46 - train: epoch 0099, iter [03600, 05004], lr: 0.000100, loss: 1.1553
2022-02-27 04:28:24 - train: epoch 0099, iter [03700, 05004], lr: 0.000100, loss: 1.0939
2022-02-27 04:29:03 - train: epoch 0099, iter [03800, 05004], lr: 0.000100, loss: 1.4035
2022-02-27 04:29:40 - train: epoch 0099, iter [03900, 05004], lr: 0.000100, loss: 1.1772
2022-02-27 04:30:17 - train: epoch 0099, iter [04000, 05004], lr: 0.000100, loss: 1.3347
2022-02-27 04:30:55 - train: epoch 0099, iter [04100, 05004], lr: 0.000100, loss: 1.1088
2022-02-27 04:31:31 - train: epoch 0099, iter [04200, 05004], lr: 0.000100, loss: 1.2625
2022-02-27 04:32:11 - train: epoch 0099, iter [04300, 05004], lr: 0.000100, loss: 1.2524
2022-02-27 04:32:50 - train: epoch 0099, iter [04400, 05004], lr: 0.000100, loss: 1.3665
2022-02-27 04:33:26 - train: epoch 0099, iter [04500, 05004], lr: 0.000100, loss: 1.3708
2022-02-27 04:34:04 - train: epoch 0099, iter [04600, 05004], lr: 0.000100, loss: 1.4359
2022-02-27 04:34:41 - train: epoch 0099, iter [04700, 05004], lr: 0.000100, loss: 1.1216
2022-02-27 04:35:18 - train: epoch 0099, iter [04800, 05004], lr: 0.000100, loss: 1.4025
2022-02-27 04:35:55 - train: epoch 0099, iter [04900, 05004], lr: 0.000100, loss: 1.2108
2022-02-27 04:36:31 - train: epoch 0099, iter [05000, 05004], lr: 0.000100, loss: 1.3298
2022-02-27 04:36:33 - train: epoch 099, train_loss: 1.2470
2022-02-27 04:38:00 - eval: epoch: 099, acc1: 72.056%, acc5: 90.646%, test_loss: 1.1138, per_image_load_time: 2.388ms, per_image_inference_time: 0.578ms
2022-02-27 04:38:00 - until epoch: 099, best_acc1: 72.104%
2022-02-27 04:38:00 - epoch 100 lr: 0.00010000000000000003
2022-02-27 04:38:44 - train: epoch 0100, iter [00100, 05004], lr: 0.000100, loss: 1.2082
2022-02-27 04:39:22 - train: epoch 0100, iter [00200, 05004], lr: 0.000100, loss: 1.3387
2022-02-27 04:39:59 - train: epoch 0100, iter [00300, 05004], lr: 0.000100, loss: 1.1204
2022-02-27 04:40:36 - train: epoch 0100, iter [00400, 05004], lr: 0.000100, loss: 0.9880
2022-02-27 04:41:13 - train: epoch 0100, iter [00500, 05004], lr: 0.000100, loss: 1.3375
2022-02-27 04:41:52 - train: epoch 0100, iter [00600, 05004], lr: 0.000100, loss: 1.5224
2022-02-27 04:42:31 - train: epoch 0100, iter [00700, 05004], lr: 0.000100, loss: 1.1128
2022-02-27 04:43:09 - train: epoch 0100, iter [00800, 05004], lr: 0.000100, loss: 1.3573
2022-02-27 04:43:47 - train: epoch 0100, iter [00900, 05004], lr: 0.000100, loss: 1.0671
2022-02-27 04:44:25 - train: epoch 0100, iter [01000, 05004], lr: 0.000100, loss: 1.2621
2022-02-27 04:45:03 - train: epoch 0100, iter [01100, 05004], lr: 0.000100, loss: 1.1775
2022-02-27 04:45:40 - train: epoch 0100, iter [01200, 05004], lr: 0.000100, loss: 1.3744
2022-02-27 04:46:16 - train: epoch 0100, iter [01300, 05004], lr: 0.000100, loss: 1.3127
2022-02-27 04:46:56 - train: epoch 0100, iter [01400, 05004], lr: 0.000100, loss: 1.3050
2022-02-27 04:47:34 - train: epoch 0100, iter [01500, 05004], lr: 0.000100, loss: 1.2492
2022-02-27 04:48:13 - train: epoch 0100, iter [01600, 05004], lr: 0.000100, loss: 1.1559
2022-02-27 04:48:51 - train: epoch 0100, iter [01700, 05004], lr: 0.000100, loss: 1.2639
2022-02-27 04:49:29 - train: epoch 0100, iter [01800, 05004], lr: 0.000100, loss: 1.3017
2022-02-27 04:50:06 - train: epoch 0100, iter [01900, 05004], lr: 0.000100, loss: 1.1951
2022-02-27 04:50:42 - train: epoch 0100, iter [02000, 05004], lr: 0.000100, loss: 1.2017
2022-02-27 04:51:19 - train: epoch 0100, iter [02100, 05004], lr: 0.000100, loss: 1.1946
2022-02-27 04:51:59 - train: epoch 0100, iter [02200, 05004], lr: 0.000100, loss: 1.4174
2022-02-27 04:52:38 - train: epoch 0100, iter [02300, 05004], lr: 0.000100, loss: 1.2597
2022-02-27 04:53:16 - train: epoch 0100, iter [02400, 05004], lr: 0.000100, loss: 1.2257
2022-02-27 04:53:53 - train: epoch 0100, iter [02500, 05004], lr: 0.000100, loss: 1.2008
2022-02-27 04:54:30 - train: epoch 0100, iter [02600, 05004], lr: 0.000100, loss: 1.3396
2022-02-27 04:55:06 - train: epoch 0100, iter [02700, 05004], lr: 0.000100, loss: 1.0921
2022-02-27 04:55:43 - train: epoch 0100, iter [02800, 05004], lr: 0.000100, loss: 1.2500
2022-02-27 04:56:21 - train: epoch 0100, iter [02900, 05004], lr: 0.000100, loss: 1.2590
2022-02-27 04:57:00 - train: epoch 0100, iter [03000, 05004], lr: 0.000100, loss: 1.1261
2022-02-27 04:57:38 - train: epoch 0100, iter [03100, 05004], lr: 0.000100, loss: 1.1993
2022-02-27 04:58:15 - train: epoch 0100, iter [03200, 05004], lr: 0.000100, loss: 1.2991
2022-02-27 04:58:52 - train: epoch 0100, iter [03300, 05004], lr: 0.000100, loss: 1.3689
2022-02-27 04:59:30 - train: epoch 0100, iter [03400, 05004], lr: 0.000100, loss: 1.3508
2022-02-27 05:00:06 - train: epoch 0100, iter [03500, 05004], lr: 0.000100, loss: 1.0605
2022-02-27 05:00:43 - train: epoch 0100, iter [03600, 05004], lr: 0.000100, loss: 1.2861
2022-02-27 05:01:21 - train: epoch 0100, iter [03700, 05004], lr: 0.000100, loss: 1.1658
2022-02-27 05:02:00 - train: epoch 0100, iter [03800, 05004], lr: 0.000100, loss: 1.2325
2022-02-27 05:02:38 - train: epoch 0100, iter [03900, 05004], lr: 0.000100, loss: 1.2830
2022-02-27 05:03:15 - train: epoch 0100, iter [04000, 05004], lr: 0.000100, loss: 1.2248
2022-02-27 05:03:53 - train: epoch 0100, iter [04100, 05004], lr: 0.000100, loss: 1.3720
2022-02-27 05:04:31 - train: epoch 0100, iter [04200, 05004], lr: 0.000100, loss: 1.2080
2022-02-27 05:05:07 - train: epoch 0100, iter [04300, 05004], lr: 0.000100, loss: 1.1801
2022-02-27 05:05:44 - train: epoch 0100, iter [04400, 05004], lr: 0.000100, loss: 1.4550
2022-02-27 05:06:21 - train: epoch 0100, iter [04500, 05004], lr: 0.000100, loss: 1.1796
2022-02-27 05:07:01 - train: epoch 0100, iter [04600, 05004], lr: 0.000100, loss: 1.1016
2022-02-27 05:07:38 - train: epoch 0100, iter [04700, 05004], lr: 0.000100, loss: 1.2123
2022-02-27 05:08:15 - train: epoch 0100, iter [04800, 05004], lr: 0.000100, loss: 1.0833
2022-02-27 05:08:54 - train: epoch 0100, iter [04900, 05004], lr: 0.000100, loss: 1.1528
2022-02-27 05:09:30 - train: epoch 0100, iter [05000, 05004], lr: 0.000100, loss: 1.3828
2022-02-27 05:09:32 - train: epoch 100, train_loss: 1.2455
2022-02-27 05:10:52 - eval: epoch: 100, acc1: 72.036%, acc5: 90.588%, test_loss: 1.1145, per_image_load_time: 1.745ms, per_image_inference_time: 0.587ms
2022-02-27 05:10:53 - until epoch: 100, best_acc1: 72.104%
2022-02-27 05:10:53 - train done. model: yolov5mbackbone, macs: 2.230G, params: 7.556M, train time: 54.196 hours, best_acc1: 72.104%
