2022-02-24 22:58:36 - network: yolov5mbackbone
2022-02-24 22:58:36 - num_classes: 1000
2022-02-24 22:58:36 - input_image_size: 256
2022-02-24 22:58:36 - scale: 1.1428571428571428
2022-02-24 22:58:36 - trained_model_path: 
2022-02-24 22:58:36 - criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-02-24 22:58:36 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f2842d0c9a0>
2022-02-24 22:58:36 - val_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f2842d0cc70>
2022-02-24 22:58:36 - collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f2842d0cca0>
2022-02-24 22:58:36 - seed: 0
2022-02-24 22:58:36 - batch_size: 256
2022-02-24 22:58:36 - num_workers: 16
2022-02-24 22:58:36 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001})
2022-02-24 22:58:36 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-02-24 22:58:36 - epochs: 100
2022-02-24 22:58:36 - print_interval: 100
2022-02-24 22:58:36 - distributed: True
2022-02-24 22:58:36 - sync_bn: False
2022-02-24 22:58:36 - apex: True
2022-02-24 22:58:36 - gpus_type: NVIDIA GeForce RTX 3090
2022-02-24 22:58:36 - gpus_num: 2
2022-02-24 22:58:36 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f2822c9eeb0>
2022-02-24 22:58:42 - --------------------parameters--------------------
2022-02-24 22:58:42 - name: conv.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: conv.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: conv.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv2.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv2.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv2.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv3.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv3.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.conv3.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.2.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.2.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.2.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv2.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv2.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv2.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv3.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv3.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.conv3.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.4.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.4.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.4.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv2.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv2.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv2.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv3.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv3.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.conv3.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.0.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: middle_layers.6.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.6.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: middle_layers.6.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: sppf.conv1.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: sppf.conv1.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: sppf.conv1.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: sppf.conv2.layer.0.weight, grad: True
2022-02-24 22:58:42 - name: sppf.conv2.layer.1.weight, grad: True
2022-02-24 22:58:42 - name: sppf.conv2.layer.1.bias, grad: True
2022-02-24 22:58:42 - name: fc.weight, grad: True
2022-02-24 22:58:42 - name: fc.bias, grad: True
2022-02-24 22:58:42 - --------------------buffers--------------------
2022-02-24 22:58:42 - name: conv.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: conv.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: conv.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv2.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv2.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv3.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv3.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.conv3.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.1.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.2.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.2.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.2.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv2.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv2.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv3.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv3.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.conv3.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.2.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.3.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.4.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.3.bottlenecks.5.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.4.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.4.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.4.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv2.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv2.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv3.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv3.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.conv3.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.0.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.0.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.5.bottlenecks.1.conv.1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: middle_layers.6.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: middle_layers.6.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: middle_layers.6.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: sppf.conv1.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: sppf.conv1.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: sppf.conv1.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:42 - name: sppf.conv2.layer.1.running_mean, grad: False
2022-02-24 22:58:42 - name: sppf.conv2.layer.1.running_var, grad: False
2022-02-24 22:58:42 - name: sppf.conv2.layer.1.num_batches_tracked, grad: False
2022-02-24 22:58:43 - epoch 001 lr: 0.1
2022-02-24 22:59:26 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.9504
2022-02-24 23:00:03 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7715
2022-02-24 23:00:43 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.7750
2022-02-24 23:01:21 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.6287
2022-02-24 23:01:57 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4936
2022-02-24 23:02:35 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.2234
2022-02-24 23:03:14 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.2060
2022-02-24 23:03:51 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.0449
2022-02-24 23:04:26 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 5.8674
2022-02-24 23:05:06 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 5.7683
2022-02-24 23:05:44 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 5.7007
2022-02-24 23:06:20 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 5.4505
2022-02-24 23:06:56 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 5.3072
2022-02-24 23:07:36 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 5.3588
2022-02-24 23:08:14 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 5.1395
2022-02-24 23:08:49 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 5.1648
2022-02-24 23:09:28 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 4.8834
2022-02-24 23:10:08 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 5.0382
2022-02-24 23:10:45 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 4.8276
2022-02-24 23:11:22 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 4.7677
2022-02-24 23:12:02 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 4.7594
2022-02-24 23:12:42 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 4.7858
2022-02-24 23:13:20 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 4.5691
2022-02-24 23:13:57 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 4.6833
2022-02-24 23:14:38 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 4.6889
2022-02-24 23:15:18 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 4.5983
2022-02-24 23:15:55 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 4.6695
2022-02-24 23:16:33 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 4.5073
2022-02-24 23:17:14 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 4.3146
2022-02-24 23:17:52 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 4.4798
2022-02-24 23:18:30 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 4.6208
2022-02-24 23:19:10 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 4.4088
2022-02-24 23:19:50 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 4.1299
2022-02-24 23:20:27 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 4.2673
2022-02-24 23:21:05 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 4.3349
2022-02-24 23:21:46 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 4.3156
2022-02-24 23:22:25 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 4.4654
2022-02-24 23:23:02 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 4.1980
2022-02-24 23:23:40 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 4.2467
2022-02-24 23:24:21 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 4.0240
2022-02-24 23:24:58 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 4.1994
2022-02-24 23:25:36 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 3.9217
2022-02-24 23:26:16 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 3.9259
2022-02-24 23:26:57 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 3.8635
2022-02-24 23:27:33 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 4.1843
2022-02-24 23:28:11 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 4.2098
2022-02-24 23:28:51 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 3.9190
2022-02-24 23:29:31 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 4.1451
2022-02-24 23:30:07 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 3.9310
2022-02-24 23:30:44 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 3.7862
2022-02-24 23:30:45 - train: epoch 001, train_loss: 4.8858
2022-02-24 23:32:13 - eval: epoch: 001, acc1: 23.956%, acc5: 47.666%, test_loss: 3.7251, per_image_load_time: 2.668ms, per_image_inference_time: 0.539ms
2022-02-24 23:32:14 - until epoch: 001, best_acc1: 23.956%
2022-02-24 23:32:14 - epoch 002 lr: 0.1
2022-02-24 23:32:58 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 4.0127
2022-02-24 23:33:39 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 3.7852
2022-02-24 23:34:18 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 3.9998
2022-02-24 23:34:55 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 3.9115
2022-02-24 23:35:32 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 3.5993
2022-02-24 23:36:11 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 3.5809
2022-02-24 23:36:47 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 3.8317
2022-02-24 23:37:24 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 3.6273
2022-02-24 23:38:01 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 3.4534
2022-02-24 23:38:39 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 3.9800
2022-02-24 23:39:17 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 3.8206
2022-02-24 23:39:53 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 3.7258
2022-02-24 23:40:29 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 3.6523
2022-02-24 23:41:09 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 3.8426
2022-02-24 23:41:47 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 3.9421
2022-02-24 23:42:23 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 3.6015
2022-02-24 23:42:59 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 3.7378
2022-02-24 23:43:39 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 3.6491
2022-02-24 23:44:15 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 3.5706
2022-02-24 23:44:52 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 3.4339
2022-02-24 23:45:29 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 3.5240
2022-02-24 23:46:09 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 3.3527
2022-02-24 23:46:45 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 3.6711
2022-02-24 23:47:21 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 3.4483
2022-02-24 23:48:01 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 3.5340
2022-02-24 23:48:38 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 3.3956
2022-02-24 23:49:14 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 3.7267
2022-02-24 23:49:51 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 3.6157
2022-02-24 23:50:31 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 3.4302
2022-02-24 23:51:07 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 3.3820
2022-02-24 23:51:43 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 3.3613
2022-02-24 23:52:21 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 3.4683
2022-02-24 23:53:00 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 3.4366
2022-02-24 23:53:36 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 3.5610
2022-02-24 23:54:12 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 3.2856
2022-02-24 23:54:52 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 3.5000
2022-02-24 23:55:29 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 3.6201
2022-02-24 23:56:05 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 3.2711
2022-02-24 23:56:43 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 3.4030
2022-02-24 23:57:23 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 3.3063
2022-02-24 23:57:59 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 3.5297
2022-02-24 23:58:35 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 3.3875
2022-02-24 23:59:15 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 3.3059
2022-02-24 23:59:53 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 3.2339
2022-02-25 00:00:28 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 3.3302
2022-02-25 00:01:06 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 3.3517
2022-02-25 00:01:46 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 3.3350
2022-02-25 00:02:23 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 3.3343
2022-02-25 00:02:58 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 3.2904
2022-02-25 00:03:36 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 3.2662
2022-02-25 00:03:37 - train: epoch 002, train_loss: 3.5576
2022-02-25 00:05:03 - eval: epoch: 002, acc1: 33.768%, acc5: 59.414%, test_loss: 3.0987, per_image_load_time: 2.833ms, per_image_inference_time: 0.531ms
2022-02-25 00:05:03 - until epoch: 002, best_acc1: 33.768%
2022-02-25 00:05:03 - epoch 003 lr: 0.1
2022-02-25 00:05:48 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 3.2616
2022-02-25 00:06:28 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 3.2357
2022-02-25 00:07:03 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 3.2065
2022-02-25 00:07:39 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 3.2153
2022-02-25 00:08:18 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 3.3661
2022-02-25 00:08:57 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 3.0355
2022-02-25 00:09:31 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 3.3343
2022-02-25 00:10:08 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 3.2417
2022-02-25 00:10:48 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 3.1732
2022-02-25 00:11:25 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 3.2151
2022-02-25 00:12:00 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 3.1350
2022-02-25 00:12:38 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 3.3781
2022-02-25 00:13:17 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 3.2420
2022-02-25 00:13:52 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 3.2518
2022-02-25 00:14:27 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 3.5123
2022-02-25 00:15:07 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 3.1233
2022-02-25 00:15:45 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 3.0882
2022-02-25 00:16:20 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 3.0816
2022-02-25 00:16:58 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 3.1730
2022-02-25 00:17:37 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 3.5137
2022-02-25 00:18:12 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 3.4385
2022-02-25 00:18:48 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 3.5506
2022-02-25 00:19:28 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 3.2379
2022-02-25 00:20:06 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 3.1330
2022-02-25 00:20:41 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 3.3062
2022-02-25 00:21:17 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 3.1872
2022-02-25 00:21:58 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 3.3419
2022-02-25 00:22:34 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 3.0897
2022-02-25 00:23:09 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 3.0483
2022-02-25 00:23:48 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 3.3705
2022-02-25 00:24:27 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 3.3081
2022-02-25 00:25:01 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 3.2062
2022-02-25 00:25:37 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 3.1569
2022-02-25 00:26:17 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 3.3224
2022-02-25 00:26:54 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.8715
2022-02-25 00:27:29 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 3.0442
2022-02-25 00:28:09 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 3.0950
2022-02-25 00:28:47 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 3.2912
2022-02-25 00:29:23 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 3.3192
2022-02-25 00:29:58 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 3.1398
2022-02-25 00:30:39 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 3.0894
2022-02-25 00:31:14 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 3.0556
2022-02-25 00:31:50 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.8501
2022-02-25 00:32:29 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.9736
2022-02-25 00:33:08 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 3.0832
2022-02-25 00:33:42 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 3.0714
2022-02-25 00:34:18 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.8863
2022-02-25 00:34:59 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 3.1319
2022-02-25 00:35:36 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 3.2577
2022-02-25 00:36:09 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 3.1697
2022-02-25 00:36:11 - train: epoch 003, train_loss: 3.1710
2022-02-25 00:37:36 - eval: epoch: 003, acc1: 38.416%, acc5: 64.572%, test_loss: 2.8112, per_image_load_time: 1.950ms, per_image_inference_time: 0.548ms
2022-02-25 00:37:37 - until epoch: 003, best_acc1: 38.416%
2022-02-25 00:37:37 - epoch 004 lr: 0.1
2022-02-25 00:38:17 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 3.0889
2022-02-25 00:38:54 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 3.0243
2022-02-25 00:39:35 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.9799
2022-02-25 00:40:11 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.9208
2022-02-25 00:40:46 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.8167
2022-02-25 00:41:25 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 3.2483
2022-02-25 00:42:04 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 3.0741
2022-02-25 00:42:39 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.8609
2022-02-25 00:43:16 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 2.7363
2022-02-25 00:43:56 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 3.0919
2022-02-25 00:44:32 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 3.0410
2022-02-25 00:45:08 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.8072
2022-02-25 00:45:48 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.9128
2022-02-25 00:46:27 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 3.0615
2022-02-25 00:47:02 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 3.1586
2022-02-25 00:47:38 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 2.7861
2022-02-25 00:48:18 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 3.1424
2022-02-25 00:48:55 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 3.1149
2022-02-25 00:49:30 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 3.0393
2022-02-25 00:50:09 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 3.0313
2022-02-25 00:50:47 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.9493
2022-02-25 00:51:22 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 3.0188
2022-02-25 00:51:58 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 2.7928
2022-02-25 00:52:38 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.8367
2022-02-25 00:53:15 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.8763
2022-02-25 00:53:51 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 3.0427
2022-02-25 00:54:28 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.7673
2022-02-25 00:55:08 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.9101
2022-02-25 00:55:42 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.9823
2022-02-25 00:56:18 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.8551
2022-02-25 00:56:57 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 3.0005
2022-02-25 00:57:35 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.8860
2022-02-25 00:58:10 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.9582
2022-02-25 00:58:47 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.9228
2022-02-25 00:59:26 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.8988
2022-02-25 01:00:02 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.8241
2022-02-25 01:00:37 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 3.0292
2022-02-25 01:01:16 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 2.8265
2022-02-25 01:01:54 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 2.7929
2022-02-25 01:02:29 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 2.6376
2022-02-25 01:03:05 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 2.9017
2022-02-25 01:03:45 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.7677
2022-02-25 01:04:22 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 2.7850
2022-02-25 01:04:56 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.6574
2022-02-25 01:05:35 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 2.4466
2022-02-25 01:06:13 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.8258
2022-02-25 01:06:48 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.7625
2022-02-25 01:07:23 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.7264
2022-02-25 01:08:02 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.8628
2022-02-25 01:08:40 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.9787
2022-02-25 01:08:41 - train: epoch 004, train_loss: 2.9641
2022-02-25 01:10:03 - eval: epoch: 004, acc1: 42.538%, acc5: 68.876%, test_loss: 2.5691, per_image_load_time: 2.613ms, per_image_inference_time: 0.563ms
2022-02-25 01:10:03 - until epoch: 004, best_acc1: 42.538%
2022-02-25 01:10:03 - epoch 005 lr: 0.1
2022-02-25 01:10:47 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.9859
2022-02-25 01:11:22 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.9600
2022-02-25 01:11:59 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 3.0764
2022-02-25 01:12:38 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.8760
2022-02-25 01:13:14 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.6326
2022-02-25 01:13:50 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.8420
2022-02-25 01:14:29 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.8374
2022-02-25 01:15:07 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 3.1447
2022-02-25 01:15:42 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.8821
2022-02-25 01:16:18 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.9204
2022-02-25 01:16:57 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 3.0019
2022-02-25 01:17:33 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.8120
2022-02-25 01:18:08 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 2.8032
2022-02-25 01:18:45 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 3.0267
2022-02-25 01:19:24 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 2.7358
2022-02-25 01:19:59 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 2.6097
2022-02-25 01:20:35 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.8596
2022-02-25 01:21:14 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.9147
2022-02-25 01:21:52 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.6806
2022-02-25 01:22:27 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.8106
2022-02-25 01:23:04 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 2.7099
2022-02-25 01:23:44 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.7828
2022-02-25 01:24:19 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 2.7644
2022-02-25 01:24:55 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.6753
2022-02-25 01:25:34 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.8687
2022-02-25 01:26:12 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 3.1389
2022-02-25 01:26:47 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.8466
2022-02-25 01:27:23 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.8443
2022-02-25 01:28:03 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.7231
2022-02-25 01:28:39 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 2.8503
2022-02-25 01:29:14 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.9260
2022-02-25 01:29:53 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 3.0074
2022-02-25 01:30:32 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.6226
2022-02-25 01:31:07 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.6797
2022-02-25 01:31:43 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.9030
2022-02-25 01:32:23 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.8038
2022-02-25 01:32:59 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.8500
2022-02-25 01:33:35 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 2.7197
2022-02-25 01:34:12 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 3.0123
2022-02-25 01:34:53 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.8262
2022-02-25 01:35:27 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 2.6659
2022-02-25 01:36:03 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.8895
2022-02-25 01:36:42 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.8608
2022-02-25 01:37:20 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.9476
2022-02-25 01:37:55 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.9461
2022-02-25 01:38:32 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 2.6959
2022-02-25 01:39:12 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.5700
2022-02-25 01:39:47 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.6146
2022-02-25 01:40:22 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.8459
2022-02-25 01:41:01 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.7876
2022-02-25 01:41:03 - train: epoch 005, train_loss: 2.8326
2022-02-25 01:42:23 - eval: epoch: 005, acc1: 43.492%, acc5: 69.940%, test_loss: 2.5113, per_image_load_time: 1.819ms, per_image_inference_time: 0.554ms
2022-02-25 01:42:23 - until epoch: 005, best_acc1: 43.492%
2022-02-25 01:42:23 - epoch 006 lr: 0.1
2022-02-25 01:43:07 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.7777
2022-02-25 01:43:46 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.7788
2022-02-25 01:44:21 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.6008
2022-02-25 01:44:57 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.7666
2022-02-25 01:45:37 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 2.7356
2022-02-25 01:46:14 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.8241
2022-02-25 01:46:47 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.7649
2022-02-25 01:47:26 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.8534
2022-02-25 01:48:06 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.5889
2022-02-25 01:48:42 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.6718
2022-02-25 01:49:17 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.6923
2022-02-25 01:49:56 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.8506
2022-02-25 01:50:33 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.9254
2022-02-25 01:51:08 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.8186
2022-02-25 01:51:45 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.9177
2022-02-25 01:52:25 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.5934
2022-02-25 01:53:00 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.8781
2022-02-25 01:53:35 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.8755
2022-02-25 01:54:15 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 2.6331
2022-02-25 01:54:54 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.8265
2022-02-25 01:55:28 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.7825
2022-02-25 01:56:04 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 2.6515
2022-02-25 01:56:45 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.6045
2022-02-25 01:57:21 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.7876
2022-02-25 01:57:56 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.9114
2022-02-25 01:58:35 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 2.6652
2022-02-25 01:59:14 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.8768
2022-02-25 01:59:48 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 2.7039
2022-02-25 02:00:25 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.8441
2022-02-25 02:01:05 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.7052
2022-02-25 02:01:42 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 2.5372
2022-02-25 02:02:17 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.5946
2022-02-25 02:02:55 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 2.5468
2022-02-25 02:03:34 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.8486
2022-02-25 02:04:10 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.8766
2022-02-25 02:04:45 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.7645
2022-02-25 02:05:25 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.8262
2022-02-25 02:06:02 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 2.6478
2022-02-25 02:06:37 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.5968
2022-02-25 02:07:15 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 3.0302
2022-02-25 02:07:54 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.5828
2022-02-25 02:08:29 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.6693
2022-02-25 02:09:05 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.5705
2022-02-25 02:09:45 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 2.7812
2022-02-25 02:10:22 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.6436
2022-02-25 02:10:57 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.6651
2022-02-25 02:11:34 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.8428
2022-02-25 02:12:14 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.7413
2022-02-25 02:12:48 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.7523
2022-02-25 02:13:23 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 2.5321
2022-02-25 02:13:25 - train: epoch 006, train_loss: 2.7473
2022-02-25 02:14:50 - eval: epoch: 006, acc1: 46.044%, acc5: 71.778%, test_loss: 2.3822, per_image_load_time: 2.754ms, per_image_inference_time: 0.551ms
2022-02-25 02:14:51 - until epoch: 006, best_acc1: 46.044%
2022-02-25 02:14:51 - epoch 007 lr: 0.1
2022-02-25 02:15:31 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 2.5079
2022-02-25 02:16:10 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.9856
2022-02-25 02:16:49 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.9945
2022-02-25 02:17:24 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.7229
2022-02-25 02:17:59 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.5493
2022-02-25 02:18:38 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.8991
2022-02-25 02:19:16 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 2.7012
2022-02-25 02:19:50 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.6282
2022-02-25 02:20:29 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.6825
2022-02-25 02:21:08 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.8017
2022-02-25 02:21:42 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 2.5792
2022-02-25 02:22:19 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.7421
2022-02-25 02:22:57 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 2.5520
2022-02-25 02:23:36 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.7993
2022-02-25 02:24:11 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.8091
2022-02-25 02:24:47 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.7374
2022-02-25 02:25:27 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.7506
2022-02-25 02:26:02 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.6059
2022-02-25 02:26:38 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.6561
2022-02-25 02:27:17 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 2.5154
2022-02-25 02:27:55 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.8170
2022-02-25 02:28:30 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 2.5172
2022-02-25 02:29:07 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.8363
2022-02-25 02:29:46 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.5631
2022-02-25 02:30:21 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.6031
2022-02-25 02:30:57 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 2.6667
2022-02-25 02:31:35 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 2.5137
2022-02-25 02:32:14 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.6754
2022-02-25 02:32:49 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 2.5741
2022-02-25 02:33:24 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.7474
2022-02-25 02:34:05 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 2.5745
2022-02-25 02:34:42 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 2.5285
2022-02-25 02:35:16 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.9714
2022-02-25 02:35:54 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.6949
2022-02-25 02:36:33 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.8124
2022-02-25 02:37:08 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 2.5681
2022-02-25 02:37:43 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.7686
2022-02-25 02:38:22 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.8828
2022-02-25 02:39:01 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.5343
2022-02-25 02:39:35 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.8118
2022-02-25 02:40:11 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.5187
2022-02-25 02:40:52 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.4541
2022-02-25 02:41:27 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.7392
2022-02-25 02:42:01 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 2.6137
2022-02-25 02:42:41 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.9056
2022-02-25 02:43:19 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.7523
2022-02-25 02:43:53 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 2.8071
2022-02-25 02:44:30 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.9664
2022-02-25 02:45:10 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.7182
2022-02-25 02:45:47 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.7023
2022-02-25 02:45:48 - train: epoch 007, train_loss: 2.6806
2022-02-25 02:47:11 - eval: epoch: 007, acc1: 46.664%, acc5: 72.614%, test_loss: 2.3408, per_image_load_time: 2.508ms, per_image_inference_time: 0.549ms
2022-02-25 02:47:11 - until epoch: 007, best_acc1: 46.664%
2022-02-25 02:47:11 - epoch 008 lr: 0.1
2022-02-25 02:47:55 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.6987
2022-02-25 02:48:29 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.7710
2022-02-25 02:49:06 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.4954
2022-02-25 02:49:45 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 2.4670
2022-02-25 02:50:21 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 2.4468
2022-02-25 02:50:56 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.6033
2022-02-25 02:51:35 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 3.1151
2022-02-25 02:52:13 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 2.5412
2022-02-25 02:52:49 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.6583
2022-02-25 02:53:25 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.5836
2022-02-25 02:54:04 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.5600
2022-02-25 02:54:41 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.4714
2022-02-25 02:55:16 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 2.7081
2022-02-25 02:55:54 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 2.4616
2022-02-25 02:56:34 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 2.8126
2022-02-25 02:57:09 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.6569
2022-02-25 02:57:44 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.6665
2022-02-25 02:58:23 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 2.7301
2022-02-25 02:59:01 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 2.5560
2022-02-25 02:59:36 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.6588
2022-02-25 03:00:13 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.6394
2022-02-25 03:00:53 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 2.5522
2022-02-25 03:01:29 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 2.6817
2022-02-25 03:02:04 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.5056
2022-02-25 03:02:42 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.6321
2022-02-25 03:03:21 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 2.9069
2022-02-25 03:03:56 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.8274
2022-02-25 03:04:32 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.7704
2022-02-25 03:05:11 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.7041
2022-02-25 03:05:48 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.7061
2022-02-25 03:06:24 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 2.4522
2022-02-25 03:07:02 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.9141
2022-02-25 03:07:41 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.8144
2022-02-25 03:08:16 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.8323
2022-02-25 03:08:51 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 2.6694
2022-02-25 03:09:31 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.7159
2022-02-25 03:10:08 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.5518
2022-02-25 03:10:45 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 2.5084
2022-02-25 03:11:20 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.7231
2022-02-25 03:11:57 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 3.0180
2022-02-25 03:12:36 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 2.5831
2022-02-25 03:13:11 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.5502
2022-02-25 03:13:47 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 2.3752
2022-02-25 03:14:25 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.5867
2022-02-25 03:15:04 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 2.7764
2022-02-25 03:15:39 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.8263
2022-02-25 03:16:14 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.5533
2022-02-25 03:16:54 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.6272
2022-02-25 03:17:30 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.5898
2022-02-25 03:18:04 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.5121
2022-02-25 03:18:06 - train: epoch 008, train_loss: 2.6323
2022-02-25 03:19:32 - eval: epoch: 008, acc1: 47.866%, acc5: 73.686%, test_loss: 2.2823, per_image_load_time: 2.772ms, per_image_inference_time: 0.537ms
2022-02-25 03:19:32 - until epoch: 008, best_acc1: 47.866%
2022-02-25 03:19:32 - epoch 009 lr: 0.1
2022-02-25 03:20:12 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 2.3129
2022-02-25 03:20:49 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.4465
2022-02-25 03:21:29 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 2.3293
2022-02-25 03:22:05 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.7422
2022-02-25 03:22:39 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 2.5511
2022-02-25 03:23:18 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 2.5756
2022-02-25 03:23:56 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 2.5336
2022-02-25 03:24:31 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.4212
2022-02-25 03:25:08 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 2.4683
2022-02-25 03:25:47 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 2.4463
2022-02-25 03:26:22 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.9145
2022-02-25 03:26:57 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.7027
2022-02-25 03:27:36 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.7509
2022-02-25 03:28:15 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 2.3132
2022-02-25 03:28:50 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 2.5358
2022-02-25 03:29:27 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.5659
2022-02-25 03:30:07 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.6797
2022-02-25 03:30:43 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.5511
2022-02-25 03:31:18 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 2.3507
2022-02-25 03:31:57 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 2.3794
2022-02-25 03:32:35 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 2.6431
2022-02-25 03:33:11 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.6741
2022-02-25 03:33:46 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.3825
2022-02-25 03:34:27 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.5788
2022-02-25 03:35:03 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 2.4158
2022-02-25 03:35:37 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.7157
2022-02-25 03:36:16 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.5166
2022-02-25 03:36:55 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.7278
2022-02-25 03:37:30 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 2.2878
2022-02-25 03:38:05 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 2.5242
2022-02-25 03:38:44 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.6687
2022-02-25 03:39:22 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.6403
2022-02-25 03:39:57 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.5920
2022-02-25 03:40:34 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.8105
2022-02-25 03:41:14 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.6879
2022-02-25 03:41:49 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 2.5605
2022-02-25 03:42:25 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.6488
2022-02-25 03:43:03 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 2.7146
2022-02-25 03:43:41 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 2.2792
2022-02-25 03:44:16 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.8948
2022-02-25 03:44:54 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.5652
2022-02-25 03:45:32 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 2.4254
2022-02-25 03:46:08 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 2.7090
2022-02-25 03:46:44 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.6307
2022-02-25 03:47:23 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.6180
2022-02-25 03:48:01 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.6526
2022-02-25 03:48:36 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.6248
2022-02-25 03:49:12 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.6752
2022-02-25 03:49:52 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.5814
2022-02-25 03:50:28 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 2.5474
2022-02-25 03:50:29 - train: epoch 009, train_loss: 2.5941
2022-02-25 03:51:52 - eval: epoch: 009, acc1: 48.026%, acc5: 73.898%, test_loss: 2.2657, per_image_load_time: 2.382ms, per_image_inference_time: 0.558ms
2022-02-25 03:51:53 - until epoch: 009, best_acc1: 48.026%
2022-02-25 03:51:53 - epoch 010 lr: 0.1
2022-02-25 03:52:36 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 2.4664
2022-02-25 03:53:11 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 2.6860
2022-02-25 03:53:50 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 2.6042
2022-02-25 03:54:29 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.5633
2022-02-25 03:55:03 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 2.5501
2022-02-25 03:55:40 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.5237
2022-02-25 03:56:20 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.5593
2022-02-25 03:56:56 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 2.4514
2022-02-25 03:57:31 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 2.3681
2022-02-25 03:58:09 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 2.3850
2022-02-25 03:58:49 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 2.6090
2022-02-25 03:59:24 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.4488
2022-02-25 04:00:00 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 2.4060
2022-02-25 04:00:40 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.6038
2022-02-25 04:01:17 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 2.1606
2022-02-25 04:01:51 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.7266
2022-02-25 04:02:31 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.7708
2022-02-25 04:03:09 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 2.5264
2022-02-25 04:03:45 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.5069
2022-02-25 04:04:21 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.6581
2022-02-25 04:05:00 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.4550
2022-02-25 04:05:37 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.6832
2022-02-25 04:06:11 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.8497
2022-02-25 04:06:48 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.7517
2022-02-25 04:07:28 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.5309
2022-02-25 04:08:04 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 2.5577
2022-02-25 04:08:39 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 2.3277
2022-02-25 04:09:19 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.5891
2022-02-25 04:09:57 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.6942
2022-02-25 04:10:31 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.4699
2022-02-25 04:11:08 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.6335
2022-02-25 04:11:48 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.4592
2022-02-25 04:12:24 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.7302
2022-02-25 04:12:58 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.8671
2022-02-25 04:13:38 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.7062
2022-02-25 04:14:15 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.8743
2022-02-25 04:14:50 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.4880
2022-02-25 04:15:26 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.5582
2022-02-25 04:16:06 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 2.2074
2022-02-25 04:16:42 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 2.4874
2022-02-25 04:17:17 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 2.4777
2022-02-25 04:17:55 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.6533
2022-02-25 04:18:34 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 2.6059
2022-02-25 04:19:10 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 2.4645
2022-02-25 04:19:46 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 2.3939
2022-02-25 04:20:26 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 2.6676
2022-02-25 04:21:03 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.5539
2022-02-25 04:21:38 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.5025
2022-02-25 04:22:16 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.3608
2022-02-25 04:22:53 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 2.3172
2022-02-25 04:22:55 - train: epoch 010, train_loss: 2.5621
2022-02-25 04:24:14 - eval: epoch: 010, acc1: 48.830%, acc5: 74.546%, test_loss: 2.2333, per_image_load_time: 2.372ms, per_image_inference_time: 0.553ms
2022-02-25 04:24:14 - until epoch: 010, best_acc1: 48.830%
2022-02-25 04:24:14 - epoch 011 lr: 0.1
2022-02-25 04:24:59 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 2.3846
2022-02-25 04:25:35 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 2.7190
2022-02-25 04:26:10 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.3513
2022-02-25 04:26:49 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 2.6018
2022-02-25 04:27:27 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 2.4383
2022-02-25 04:28:02 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.5709
2022-02-25 04:28:39 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 2.4879
2022-02-25 04:29:18 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.5364
2022-02-25 04:29:53 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.7449
2022-02-25 04:30:29 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.5090
2022-02-25 04:31:07 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.6513
2022-02-25 04:31:45 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.9028
2022-02-25 04:32:21 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.6990
2022-02-25 04:32:56 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 2.5052
2022-02-25 04:33:37 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 2.3799
2022-02-25 04:34:15 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.5224
2022-02-25 04:34:49 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.5874
2022-02-25 04:35:27 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 2.4660
2022-02-25 04:36:06 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.3429
2022-02-25 04:36:40 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.6880
2022-02-25 04:37:16 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 2.4990
2022-02-25 04:37:56 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.4600
2022-02-25 04:38:34 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.8313
2022-02-25 04:39:09 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 2.3539
2022-02-25 04:39:45 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 2.8092
2022-02-25 04:40:24 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.5148
2022-02-25 04:41:01 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.4612
2022-02-25 04:41:37 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 2.2460
2022-02-25 04:42:14 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.6953
2022-02-25 04:42:54 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.7968
2022-02-25 04:43:28 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 2.5651
2022-02-25 04:44:04 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 2.3257
2022-02-25 04:44:45 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 2.5390
2022-02-25 04:45:22 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.5298
2022-02-25 04:45:57 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 2.5299
2022-02-25 04:46:35 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 2.6023
2022-02-25 04:47:14 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.6360
2022-02-25 04:47:49 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.1993
2022-02-25 04:48:25 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 2.5053
2022-02-25 04:49:04 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 2.5599
2022-02-25 04:49:41 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 2.3896
2022-02-25 04:50:16 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.6159
2022-02-25 04:50:54 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.4785
2022-02-25 04:51:33 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.5203
2022-02-25 04:52:08 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 2.4062
2022-02-25 04:52:43 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 2.4730
2022-02-25 04:53:23 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 2.2268
2022-02-25 04:54:02 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 2.3736
2022-02-25 04:54:36 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 2.2518
2022-02-25 04:55:11 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.4445
2022-02-25 04:55:13 - train: epoch 011, train_loss: 2.5353
2022-02-25 04:56:37 - eval: epoch: 011, acc1: 49.078%, acc5: 74.838%, test_loss: 2.2163, per_image_load_time: 2.437ms, per_image_inference_time: 0.561ms
2022-02-25 04:56:37 - until epoch: 011, best_acc1: 49.078%
2022-02-25 04:56:37 - epoch 012 lr: 0.1
2022-02-25 04:57:17 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 2.4468
2022-02-25 04:57:56 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 2.4862
2022-02-25 04:58:34 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 2.5755
2022-02-25 04:59:10 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 2.5507
2022-02-25 04:59:46 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.7324
2022-02-25 05:00:26 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 2.3725
2022-02-25 05:01:02 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 2.4019
2022-02-25 05:01:37 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.4688
2022-02-25 05:02:16 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 2.6455
2022-02-25 05:02:55 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 2.3408
2022-02-25 05:03:30 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.8892
2022-02-25 05:04:05 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 2.3918
2022-02-25 05:04:44 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 2.3875
2022-02-25 05:05:21 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.6071
2022-02-25 05:05:56 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 2.3501
2022-02-25 05:06:34 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 2.4197
2022-02-25 05:07:13 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 2.3269
2022-02-25 05:07:49 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 2.5618
2022-02-25 05:08:23 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.5187
2022-02-25 05:09:04 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.6909
2022-02-25 05:09:41 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.5731
2022-02-25 05:10:16 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 2.7582
2022-02-25 05:10:53 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 2.5629
2022-02-25 05:11:32 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.6006
2022-02-25 05:12:08 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 2.2882
2022-02-25 05:12:43 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 2.2733
2022-02-25 05:13:23 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.4767
2022-02-25 05:14:02 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.3909
2022-02-25 05:14:37 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.3497
2022-02-25 05:15:12 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 2.3897
2022-02-25 05:15:51 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.6931
2022-02-25 05:16:28 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1084
2022-02-25 05:17:04 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.3932
2022-02-25 05:17:41 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 2.5129
2022-02-25 05:18:20 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 2.7638
2022-02-25 05:18:56 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 2.5357
2022-02-25 05:19:32 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.4211
2022-02-25 05:20:12 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.4592
2022-02-25 05:20:49 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 2.3235
2022-02-25 05:21:24 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.4086
2022-02-25 05:22:01 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 2.4089
2022-02-25 05:22:41 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 2.3626
2022-02-25 05:23:18 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.4956
2022-02-25 05:23:53 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 2.3808
2022-02-25 05:24:32 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 2.4918
2022-02-25 05:25:10 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 2.8625
2022-02-25 05:25:45 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 2.4315
2022-02-25 05:26:22 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 2.6206
2022-02-25 05:27:02 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 2.5105
2022-02-25 05:27:37 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 2.1794
2022-02-25 05:27:39 - train: epoch 012, train_loss: 2.5129
2022-02-25 05:29:02 - eval: epoch: 012, acc1: 49.924%, acc5: 75.810%, test_loss: 2.1526, per_image_load_time: 2.133ms, per_image_inference_time: 0.553ms
2022-02-25 05:29:02 - until epoch: 012, best_acc1: 49.924%
2022-02-25 05:29:02 - epoch 013 lr: 0.1
2022-02-25 05:29:45 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 2.3433
2022-02-25 05:30:19 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 2.5415
2022-02-25 05:30:58 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 2.4543
2022-02-25 05:31:37 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 2.3614
2022-02-25 05:32:11 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.4343
2022-02-25 05:32:47 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.6371
2022-02-25 05:33:26 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.4003
2022-02-25 05:34:04 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.6077
2022-02-25 05:34:38 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 2.4666
2022-02-25 05:35:16 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 2.6940
2022-02-25 05:35:55 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.5726
2022-02-25 05:36:32 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 2.6385
2022-02-25 05:37:07 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 2.6763
2022-02-25 05:37:45 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 2.5030
2022-02-25 05:38:25 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.5722
2022-02-25 05:38:59 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 2.1694
2022-02-25 05:39:35 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 2.6004
2022-02-25 05:40:15 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 2.4495
2022-02-25 05:40:51 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.4806
2022-02-25 05:41:27 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.7207
2022-02-25 05:42:05 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.7621
2022-02-25 05:42:45 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 2.3624
2022-02-25 05:43:20 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 2.5348
2022-02-25 05:43:56 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.4900
2022-02-25 05:44:36 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 2.4512
2022-02-25 05:45:13 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 2.3963
2022-02-25 05:45:47 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 2.4555
2022-02-25 05:46:26 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.4735
2022-02-25 05:47:05 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.4819
2022-02-25 05:47:40 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 2.3781
2022-02-25 05:48:14 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 2.3681
2022-02-25 05:48:55 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.4244
2022-02-25 05:49:33 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 2.2795
2022-02-25 05:50:08 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 2.3794
2022-02-25 05:50:44 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 2.3014
2022-02-25 05:51:23 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.6692
2022-02-25 05:52:00 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 2.3049
2022-02-25 05:52:34 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.6166
2022-02-25 05:53:13 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.5489
2022-02-25 05:53:51 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.4654
2022-02-25 05:54:26 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.4164
2022-02-25 05:55:02 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.4736
2022-02-25 05:55:42 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 2.3559
2022-02-25 05:56:17 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.4041
2022-02-25 05:56:54 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 2.4728
2022-02-25 05:57:32 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.4403
2022-02-25 05:58:11 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.5966
2022-02-25 05:58:46 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 2.5370
2022-02-25 05:59:23 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.5017
2022-02-25 06:00:01 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.5202
2022-02-25 06:00:03 - train: epoch 013, train_loss: 2.4934
2022-02-25 06:01:22 - eval: epoch: 013, acc1: 49.628%, acc5: 75.294%, test_loss: 2.1774, per_image_load_time: 2.432ms, per_image_inference_time: 0.548ms
2022-02-25 06:01:22 - until epoch: 013, best_acc1: 49.924%
2022-02-25 06:01:22 - epoch 014 lr: 0.1
2022-02-25 06:02:08 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 2.2469
2022-02-25 06:02:45 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.5376
2022-02-25 06:03:19 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 2.0827
2022-02-25 06:03:57 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 2.3721
2022-02-25 06:04:36 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 2.3855
2022-02-25 06:05:13 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.5002
2022-02-25 06:05:47 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 2.3427
2022-02-25 06:06:26 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 2.4633
2022-02-25 06:07:04 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.5932
2022-02-25 06:07:38 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.6173
2022-02-25 06:08:14 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 2.4266
2022-02-25 06:08:54 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.5921
2022-02-25 06:09:31 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.4298
2022-02-25 06:10:05 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.5330
2022-02-25 06:10:45 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.5963
2022-02-25 06:11:24 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 2.4017
2022-02-25 06:11:59 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.6020
2022-02-25 06:12:34 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.7620
2022-02-25 06:13:14 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 2.2004
2022-02-25 06:13:52 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 2.3418
2022-02-25 06:14:26 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.6233
2022-02-25 06:15:03 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.3487
2022-02-25 06:15:43 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 2.5721
2022-02-25 06:16:19 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.6307
2022-02-25 06:16:55 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 2.4992
2022-02-25 06:17:33 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 2.5283
2022-02-25 06:18:12 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 2.3920
2022-02-25 06:18:46 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.6547
2022-02-25 06:19:24 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.3585
2022-02-25 06:20:02 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.5364
2022-02-25 06:20:39 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.3215
2022-02-25 06:21:15 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 2.4401
2022-02-25 06:21:52 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 2.3240
2022-02-25 06:22:32 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.3515
2022-02-25 06:23:08 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.3832
2022-02-25 06:23:43 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 2.3922
2022-02-25 06:24:23 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.4504
2022-02-25 06:25:00 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.6781
2022-02-25 06:25:36 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.4188
2022-02-25 06:26:13 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.7120
2022-02-25 06:26:53 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.4847
2022-02-25 06:27:29 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 2.3398
2022-02-25 06:28:04 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 2.3607
2022-02-25 06:28:45 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.3088
2022-02-25 06:29:22 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.5733
2022-02-25 06:29:58 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 2.4007
2022-02-25 06:30:35 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 2.3736
2022-02-25 06:31:14 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 2.4440
2022-02-25 06:31:50 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 2.2231
2022-02-25 06:32:24 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.5906
2022-02-25 06:32:25 - train: epoch 014, train_loss: 2.4767
2022-02-25 06:33:51 - eval: epoch: 014, acc1: 50.084%, acc5: 75.700%, test_loss: 2.1596, per_image_load_time: 2.716ms, per_image_inference_time: 0.543ms
2022-02-25 06:33:51 - until epoch: 014, best_acc1: 50.084%
2022-02-25 06:33:51 - epoch 015 lr: 0.1
2022-02-25 06:34:32 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 2.3726
2022-02-25 06:35:09 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.7330
2022-02-25 06:35:49 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.6295
2022-02-25 06:36:23 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.5114
2022-02-25 06:37:00 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 2.3648
2022-02-25 06:37:40 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.5680
2022-02-25 06:38:17 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 2.4159
2022-02-25 06:38:51 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.3521
2022-02-25 06:39:30 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 2.3446
2022-02-25 06:40:09 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 2.5348
2022-02-25 06:40:43 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 2.3416
2022-02-25 06:41:20 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.4841
2022-02-25 06:41:55 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.5988
2022-02-25 06:42:33 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 2.3910
2022-02-25 06:43:12 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 2.1684
2022-02-25 06:43:47 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 2.4611
2022-02-25 06:44:23 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 2.6579
2022-02-25 06:45:03 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 2.3234
2022-02-25 06:45:40 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 2.4145
2022-02-25 06:46:14 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 2.4073
2022-02-25 06:46:53 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 2.3327
2022-02-25 06:47:32 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.5472
2022-02-25 06:48:06 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 2.3331
2022-02-25 06:48:42 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.5863
2022-02-25 06:49:22 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.5756
2022-02-25 06:49:58 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 2.2370
2022-02-25 06:50:34 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.5255
2022-02-25 06:51:11 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.5793
2022-02-25 06:51:51 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 2.4124
2022-02-25 06:52:25 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 2.2868
2022-02-25 06:53:02 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 2.4711
2022-02-25 06:53:41 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 2.3306
2022-02-25 06:54:19 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 2.2972
2022-02-25 06:54:54 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 2.5117
2022-02-25 06:55:28 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.6249
2022-02-25 06:56:08 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 2.6005
2022-02-25 06:56:46 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 2.3787
2022-02-25 06:57:20 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 2.4758
2022-02-25 06:57:58 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.5975
2022-02-25 06:58:38 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.4054
2022-02-25 06:59:13 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 2.7557
2022-02-25 06:59:48 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 2.1437
2022-02-25 07:00:28 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 2.4294
2022-02-25 07:01:05 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 2.4508
2022-02-25 07:01:39 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.4259
2022-02-25 07:02:17 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.4138
2022-02-25 07:02:57 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 2.6068
2022-02-25 07:03:32 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 2.4598
2022-02-25 07:04:07 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 2.6148
2022-02-25 07:04:44 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.6032
2022-02-25 07:04:46 - train: epoch 015, train_loss: 2.4623
2022-02-25 07:06:07 - eval: epoch: 015, acc1: 51.252%, acc5: 76.586%, test_loss: 2.0956, per_image_load_time: 2.623ms, per_image_inference_time: 0.552ms
2022-02-25 07:06:08 - until epoch: 015, best_acc1: 51.252%
2022-02-25 07:06:08 - epoch 016 lr: 0.1
2022-02-25 07:06:52 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 2.3789
2022-02-25 07:07:31 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 2.2591
2022-02-25 07:08:05 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 2.4416
2022-02-25 07:08:41 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.8100
2022-02-25 07:09:22 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 2.3250
2022-02-25 07:09:58 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 2.7246
2022-02-25 07:10:34 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 2.1410
2022-02-25 07:11:11 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 2.4460
2022-02-25 07:11:50 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 2.4850
2022-02-25 07:12:26 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 2.3352
2022-02-25 07:13:02 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 2.2778
2022-02-25 07:13:42 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 2.2676
2022-02-25 07:14:18 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 2.5366
2022-02-25 07:14:54 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 2.4120
2022-02-25 07:15:30 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.5559
2022-02-25 07:16:11 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.6470
2022-02-25 07:16:46 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.3324
2022-02-25 07:17:21 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.4241
2022-02-25 07:18:00 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 2.4078
2022-02-25 07:18:39 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 2.0881
2022-02-25 07:19:14 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.7123
2022-02-25 07:19:51 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 2.5696
2022-02-25 07:20:30 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.6858
2022-02-25 07:21:07 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 2.6251
2022-02-25 07:21:41 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 2.2852
2022-02-25 07:22:21 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.5270
2022-02-25 07:22:59 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 2.4550
2022-02-25 07:23:34 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 2.2275
2022-02-25 07:24:10 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 2.5212
2022-02-25 07:24:50 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.6763
2022-02-25 07:25:25 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.5024
2022-02-25 07:26:01 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.5630
2022-02-25 07:26:40 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 2.5866
2022-02-25 07:27:18 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 2.3799
2022-02-25 07:27:53 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 2.3001
2022-02-25 07:28:30 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 2.3251
2022-02-25 07:29:09 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.5920
2022-02-25 07:29:46 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.8500
2022-02-25 07:30:21 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.6671
2022-02-25 07:30:59 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.5154
2022-02-25 07:31:38 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.4812
2022-02-25 07:32:13 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 2.4743
2022-02-25 07:32:49 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 2.3615
2022-02-25 07:33:28 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 2.2653
2022-02-25 07:34:05 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.4732
2022-02-25 07:34:41 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 2.2736
2022-02-25 07:35:18 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.7522
2022-02-25 07:35:57 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 2.3863
2022-02-25 07:36:32 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 2.5910
2022-02-25 07:37:06 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.5685
2022-02-25 07:37:08 - train: epoch 016, train_loss: 2.4494
2022-02-25 07:38:33 - eval: epoch: 016, acc1: 50.134%, acc5: 75.760%, test_loss: 2.1505, per_image_load_time: 1.048ms, per_image_inference_time: 0.541ms
2022-02-25 07:38:33 - until epoch: 016, best_acc1: 51.252%
2022-02-25 07:38:33 - epoch 017 lr: 0.1
2022-02-25 07:39:14 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 2.2949
2022-02-25 07:39:53 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 2.5992
2022-02-25 07:40:31 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.5822
2022-02-25 07:41:07 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 2.2542
2022-02-25 07:41:44 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 2.4536
2022-02-25 07:42:24 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.8481
2022-02-25 07:43:00 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.4576
2022-02-25 07:43:35 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 2.3141
2022-02-25 07:44:13 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 2.4114
2022-02-25 07:44:52 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 2.3538
2022-02-25 07:45:27 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.6841
2022-02-25 07:46:03 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 2.7278
2022-02-25 07:46:43 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 2.5230
2022-02-25 07:47:19 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.4721
2022-02-25 07:47:53 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 2.1653
2022-02-25 07:48:33 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 2.2257
2022-02-25 07:49:11 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 2.3349
2022-02-25 07:49:45 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.5279
2022-02-25 07:50:21 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 2.2378
2022-02-25 07:51:00 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.5621
2022-02-25 07:51:38 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 2.4011
2022-02-25 07:52:12 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 2.3204
2022-02-25 07:52:49 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 2.2758
2022-02-25 07:53:29 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 2.4312
2022-02-25 07:54:04 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.5885
2022-02-25 07:54:39 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 2.3912
2022-02-25 07:55:18 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 2.3478
2022-02-25 07:55:56 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 2.6452
2022-02-25 07:56:31 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.7526
2022-02-25 07:57:06 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 2.2826
2022-02-25 07:57:46 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.4906
2022-02-25 07:58:23 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 2.3546
2022-02-25 07:58:58 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.4406
2022-02-25 07:59:36 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 2.1350
2022-02-25 08:00:14 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 2.4054
2022-02-25 08:00:49 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 2.6825
2022-02-25 08:01:24 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 2.3833
2022-02-25 08:02:05 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 2.6122
2022-02-25 08:02:42 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 2.2776
2022-02-25 08:03:17 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.3034
2022-02-25 08:03:53 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.5171
2022-02-25 08:04:33 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 2.3386
2022-02-25 08:05:09 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 2.4591
2022-02-25 08:05:45 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 2.5364
2022-02-25 08:06:24 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 2.6004
2022-02-25 08:07:01 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 2.4131
2022-02-25 08:07:36 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.4309
2022-02-25 08:08:14 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.5522
2022-02-25 08:08:53 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 2.2268
2022-02-25 08:09:28 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 2.2717
2022-02-25 08:09:30 - train: epoch 017, train_loss: 2.4381
2022-02-25 08:10:52 - eval: epoch: 017, acc1: 51.364%, acc5: 76.762%, test_loss: 2.0977, per_image_load_time: 2.132ms, per_image_inference_time: 0.566ms
2022-02-25 08:10:52 - until epoch: 017, best_acc1: 51.364%
2022-02-25 08:10:52 - epoch 018 lr: 0.1
2022-02-25 08:11:34 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 2.3365
2022-02-25 08:12:09 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.3158
2022-02-25 08:12:48 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.5604
2022-02-25 08:13:26 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.4440
2022-02-25 08:14:02 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.4558
2022-02-25 08:14:38 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 2.6526
2022-02-25 08:15:18 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 2.1210
2022-02-25 08:15:54 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 2.4230
2022-02-25 08:16:30 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.4250
2022-02-25 08:17:07 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 2.2714
2022-02-25 08:17:46 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.6936
2022-02-25 08:18:22 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 2.4823
2022-02-25 08:18:57 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.7816
2022-02-25 08:19:36 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.3953
2022-02-25 08:20:15 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.6803
2022-02-25 08:20:49 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 2.3701
2022-02-25 08:21:26 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.3041
2022-02-25 08:22:05 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 2.2691
2022-02-25 08:22:41 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 2.3955
2022-02-25 08:23:15 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.6298
2022-02-25 08:23:54 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 2.7004
2022-02-25 08:24:33 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.5332
2022-02-25 08:25:08 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 2.4620
2022-02-25 08:25:44 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 2.2703
2022-02-25 08:26:23 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 2.0834
2022-02-25 08:27:00 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.2681
2022-02-25 08:27:35 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 2.6327
2022-02-25 08:28:13 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.1680
2022-02-25 08:28:52 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.5042
2022-02-25 08:29:27 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 2.2485
2022-02-25 08:30:02 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.6894
2022-02-25 08:30:42 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 2.2897
2022-02-25 08:31:18 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 2.3572
2022-02-25 08:31:54 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.4520
2022-02-25 08:32:31 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.5114
2022-02-25 08:33:10 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.4975
2022-02-25 08:33:46 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.7785
2022-02-25 08:34:21 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.4462
2022-02-25 08:35:00 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.5469
2022-02-25 08:35:38 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.3681
2022-02-25 08:36:12 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.3586
2022-02-25 08:36:48 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 2.4320
2022-02-25 08:37:28 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 2.1962
2022-02-25 08:38:05 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 2.5488
2022-02-25 08:38:40 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.4069
2022-02-25 08:39:18 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 2.2194
2022-02-25 08:39:58 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.6531
2022-02-25 08:40:33 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.4892
2022-02-25 08:41:09 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.4637
2022-02-25 08:41:47 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.5971
2022-02-25 08:41:48 - train: epoch 018, train_loss: 2.4256
2022-02-25 08:43:08 - eval: epoch: 018, acc1: 50.388%, acc5: 75.726%, test_loss: 2.1541, per_image_load_time: 2.111ms, per_image_inference_time: 0.561ms
2022-02-25 08:43:08 - until epoch: 018, best_acc1: 51.364%
2022-02-25 08:43:08 - epoch 019 lr: 0.1
2022-02-25 08:43:53 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 2.2428
2022-02-25 08:44:32 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.5934
2022-02-25 08:45:06 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.5715
2022-02-25 08:45:42 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.3835
2022-02-25 08:46:22 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 2.3481
2022-02-25 08:46:58 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 2.3470
2022-02-25 08:47:33 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 2.1690
2022-02-25 08:48:11 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.4941
2022-02-25 08:48:49 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 2.5189
2022-02-25 08:49:26 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 2.5808
2022-02-25 08:50:00 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 2.3065
2022-02-25 08:50:40 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 2.5232
2022-02-25 08:51:18 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.5512
2022-02-25 08:51:54 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 2.3315
2022-02-25 08:52:30 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.7395
2022-02-25 08:53:09 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 2.3774
2022-02-25 08:53:46 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 2.4673
2022-02-25 08:54:21 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 2.3391
2022-02-25 08:54:59 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.7314
2022-02-25 08:55:39 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.3642
2022-02-25 08:56:14 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 2.3062
2022-02-25 08:56:51 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.4598
2022-02-25 08:57:31 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.3912
2022-02-25 08:58:07 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 2.5262
2022-02-25 08:58:42 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 2.3626
2022-02-25 08:59:21 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 2.5570
2022-02-25 09:00:00 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 2.5502
2022-02-25 09:00:35 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 2.4101
2022-02-25 09:01:12 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.4781
2022-02-25 09:01:51 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 2.6683
2022-02-25 09:02:27 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 2.4056
2022-02-25 09:03:02 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 2.2110
2022-02-25 09:03:40 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 2.3451
2022-02-25 09:04:19 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 2.4565
2022-02-25 09:04:54 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.4993
2022-02-25 09:05:31 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 2.2694
2022-02-25 09:06:11 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.5009
2022-02-25 09:06:49 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.5564
2022-02-25 09:07:24 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 2.2319
2022-02-25 09:08:01 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 2.2291
2022-02-25 09:08:42 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.4705
2022-02-25 09:09:17 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 2.4446
2022-02-25 09:09:53 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 2.3564
2022-02-25 09:10:32 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.4686
2022-02-25 09:11:10 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.7113
2022-02-25 09:11:45 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 2.4075
2022-02-25 09:12:21 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.2960
2022-02-25 09:13:02 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 2.3996
2022-02-25 09:13:38 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 2.3533
2022-02-25 09:14:14 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.4487
2022-02-25 09:14:15 - train: epoch 019, train_loss: 2.4201
2022-02-25 09:15:41 - eval: epoch: 019, acc1: 51.946%, acc5: 77.054%, test_loss: 2.0759, per_image_load_time: 2.453ms, per_image_inference_time: 0.528ms
2022-02-25 09:15:42 - until epoch: 019, best_acc1: 51.946%
2022-02-25 09:15:42 - epoch 020 lr: 0.1
2022-02-25 09:16:24 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.5306
2022-02-25 09:17:01 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 2.2312
2022-02-25 09:17:40 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 2.5440
2022-02-25 09:18:16 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 2.2012
2022-02-25 09:18:52 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 2.3633
2022-02-25 09:19:33 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 2.5228
2022-02-25 09:20:11 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 2.2862
2022-02-25 09:20:48 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.4791
2022-02-25 09:21:26 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.7399
2022-02-25 09:22:05 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.5461
2022-02-25 09:22:41 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 2.4102
2022-02-25 09:23:17 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 2.2064
2022-02-25 09:23:57 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 2.3231
2022-02-25 09:24:36 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.4392
2022-02-25 09:25:12 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.5142
2022-02-25 09:25:47 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.3684
2022-02-25 09:26:27 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 2.1056
2022-02-25 09:27:04 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.3246
2022-02-25 09:27:40 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 2.2548
2022-02-25 09:28:17 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 2.4705
2022-02-25 09:28:57 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.5804
2022-02-25 09:29:33 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 2.2558
2022-02-25 09:30:09 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 2.2497
2022-02-25 09:30:49 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.4306
2022-02-25 09:31:28 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 2.3343
2022-02-25 09:32:03 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 2.2926
2022-02-25 09:32:40 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.4719
2022-02-25 09:33:21 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 2.4515
2022-02-25 09:33:59 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.5642
2022-02-25 09:34:38 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.3280
2022-02-25 09:35:17 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.5184
2022-02-25 09:35:59 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.6651
2022-02-25 09:36:34 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 2.2371
2022-02-25 09:37:12 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 2.4014
2022-02-25 09:37:54 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 2.3364
2022-02-25 09:38:35 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 2.3159
2022-02-25 09:39:12 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 2.3421
2022-02-25 09:39:51 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 2.3654
2022-02-25 09:40:34 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.4294
2022-02-25 09:41:13 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 2.2331
2022-02-25 09:41:51 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 2.3209
2022-02-25 09:42:33 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.3334
2022-02-25 09:43:12 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 2.4027
2022-02-25 09:43:50 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 2.3876
2022-02-25 09:44:30 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.4176
2022-02-25 09:45:13 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 2.5785
2022-02-25 09:45:51 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 2.3303
2022-02-25 09:46:29 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 2.4400
2022-02-25 09:47:10 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.5013
2022-02-25 09:47:51 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 2.2140
2022-02-25 09:47:52 - train: epoch 020, train_loss: 2.4076
2022-02-25 09:49:20 - eval: epoch: 020, acc1: 52.246%, acc5: 77.720%, test_loss: 2.0474, per_image_load_time: 2.829ms, per_image_inference_time: 0.553ms
2022-02-25 09:49:20 - until epoch: 020, best_acc1: 52.246%
2022-02-25 09:49:20 - epoch 021 lr: 0.1
2022-02-25 09:50:07 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 2.1349
2022-02-25 09:50:46 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.5268
2022-02-25 09:51:25 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 2.1498
2022-02-25 09:52:07 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 2.4028
2022-02-25 09:52:45 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 2.2816
2022-02-25 09:53:24 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 2.2916
2022-02-25 09:54:05 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 2.3319
2022-02-25 09:54:47 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.5698
2022-02-25 09:55:27 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 2.5404
2022-02-25 09:56:05 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 2.2035
2022-02-25 09:56:49 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 2.2191
2022-02-25 09:57:28 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 2.2981
2022-02-25 09:58:07 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 2.2675
2022-02-25 09:58:48 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 2.2995
2022-02-25 09:59:29 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 2.2241
2022-02-25 10:00:07 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 2.3796
2022-02-25 10:00:47 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 2.3469
2022-02-25 10:01:30 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 2.3566
2022-02-25 10:02:08 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.5724
2022-02-25 10:02:46 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.6220
2022-02-25 10:03:28 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 2.1844
2022-02-25 10:04:09 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.4416
2022-02-25 10:04:48 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 2.2957
2022-02-25 10:05:27 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 2.2442
2022-02-25 10:06:11 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.4265
2022-02-25 10:06:51 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.6486
2022-02-25 10:07:31 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 2.2516
2022-02-25 10:08:13 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 2.3384
2022-02-25 10:08:54 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 2.2697
2022-02-25 10:09:34 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.7126
2022-02-25 10:10:14 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 2.5172
2022-02-25 10:10:57 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 2.3874
2022-02-25 10:11:36 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.8155
2022-02-25 10:12:14 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.5053
2022-02-25 10:12:57 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.2473
2022-02-25 10:13:38 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 2.4131
2022-02-25 10:14:17 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 2.2902
2022-02-25 10:14:59 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.2508
2022-02-25 10:15:39 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.2826
2022-02-25 10:16:21 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.6799
2022-02-25 10:17:02 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 2.2782
2022-02-25 10:17:42 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 2.3927
2022-02-25 10:18:24 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 2.4362
2022-02-25 10:19:06 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 2.5207
2022-02-25 10:19:44 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 2.5733
2022-02-25 10:20:27 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.3908
2022-02-25 10:21:09 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 2.5891
2022-02-25 10:21:48 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.5143
2022-02-25 10:22:27 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 2.2139
2022-02-25 10:23:08 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 2.2925
2022-02-25 10:23:09 - train: epoch 021, train_loss: 2.4024
2022-02-25 10:24:36 - eval: epoch: 021, acc1: 51.368%, acc5: 76.966%, test_loss: 2.0835, per_image_load_time: 2.808ms, per_image_inference_time: 0.548ms
2022-02-25 10:24:36 - until epoch: 021, best_acc1: 52.246%
2022-02-25 10:24:36 - epoch 022 lr: 0.1
2022-02-25 10:25:24 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 2.1428
2022-02-25 10:26:05 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 2.1611
2022-02-25 10:26:44 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 2.0623
2022-02-25 10:27:25 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 2.2783
2022-02-25 10:28:07 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.3666
2022-02-25 10:28:48 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 2.3983
2022-02-25 10:29:27 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.4133
2022-02-25 10:30:09 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.4065
2022-02-25 10:30:50 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.4652
2022-02-25 10:31:29 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 2.5034
2022-02-25 10:32:11 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.5067
2022-02-25 10:32:53 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 2.0976
2022-02-25 10:33:32 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 2.4482
2022-02-25 10:34:13 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 2.4827
2022-02-25 10:34:55 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.3993
2022-02-25 10:35:35 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 2.3151
2022-02-25 10:36:15 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.0794
2022-02-25 10:36:56 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.6009
2022-02-25 10:37:38 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 2.2054
2022-02-25 10:38:17 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 2.4045
2022-02-25 10:38:57 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 2.4280
2022-02-25 10:39:40 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 2.2360
2022-02-25 10:40:19 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.5516
2022-02-25 10:40:58 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.4209
2022-02-25 10:41:41 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 2.4303
2022-02-25 10:42:24 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 2.0589
2022-02-25 10:43:03 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 2.2365
2022-02-25 10:43:43 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.7007
2022-02-25 10:44:26 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 2.3229
2022-02-25 10:45:06 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 2.4988
2022-02-25 10:45:46 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.5468
2022-02-25 10:46:28 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.4802
2022-02-25 10:47:08 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 2.3892
2022-02-25 10:47:46 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 2.2571
2022-02-25 10:48:27 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.4427
2022-02-25 10:49:11 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 2.5088
2022-02-25 10:49:49 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.5697
2022-02-25 10:50:29 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.6473
2022-02-25 10:51:11 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 2.3302
2022-02-25 10:51:51 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 2.4829
2022-02-25 10:52:30 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 2.3480
2022-02-25 10:53:12 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.3660
2022-02-25 10:53:52 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.6172
2022-02-25 10:54:31 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 2.3926
2022-02-25 10:55:10 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 2.4106
2022-02-25 10:55:52 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.5400
2022-02-25 10:56:31 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 2.4092
2022-02-25 10:57:07 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 2.1348
2022-02-25 10:57:48 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 2.2375
2022-02-25 10:58:28 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 2.2434
2022-02-25 10:58:30 - train: epoch 022, train_loss: 2.3958
2022-02-25 10:59:56 - eval: epoch: 022, acc1: 48.354%, acc5: 73.652%, test_loss: 2.2883, per_image_load_time: 2.797ms, per_image_inference_time: 0.554ms
2022-02-25 10:59:56 - until epoch: 022, best_acc1: 52.246%
2022-02-25 10:59:56 - epoch 023 lr: 0.1
2022-02-25 11:00:43 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 2.2761
2022-02-25 11:01:19 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 2.0467
2022-02-25 11:01:58 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 2.2476
2022-02-25 11:02:39 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 2.5322
2022-02-25 11:03:17 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 2.4615
2022-02-25 11:03:54 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 2.2970
2022-02-25 11:04:34 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 2.1732
2022-02-25 11:05:16 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.3434
2022-02-25 11:05:53 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 2.4138
2022-02-25 11:06:32 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 2.1239
2022-02-25 11:07:15 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 2.5646
2022-02-25 11:07:51 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 2.2415
2022-02-25 11:08:28 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.3186
2022-02-25 11:09:09 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 2.3162
2022-02-25 11:09:49 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 2.3033
2022-02-25 11:10:26 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 2.3539
2022-02-25 11:11:05 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 2.6504
2022-02-25 11:11:46 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 2.2898
2022-02-25 11:12:23 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.3911
2022-02-25 11:13:01 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 2.0356
2022-02-25 11:13:42 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 2.5385
2022-02-25 11:14:22 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.9980
2022-02-25 11:14:57 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.3028
2022-02-25 11:15:38 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.3377
2022-02-25 11:16:19 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.4546
2022-02-25 11:16:55 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.2810
2022-02-25 11:17:34 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 2.3293
2022-02-25 11:18:17 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.3626
2022-02-25 11:18:53 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 2.4146
2022-02-25 11:19:31 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 2.6000
2022-02-25 11:20:13 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.4471
2022-02-25 11:20:51 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.5148
2022-02-25 11:21:27 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 2.5974
2022-02-25 11:22:07 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.4014
2022-02-25 11:22:48 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 2.3130
2022-02-25 11:23:24 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 2.2549
2022-02-25 11:24:04 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 2.3842
2022-02-25 11:24:45 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 2.3470
2022-02-25 11:25:22 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 2.4491
2022-02-25 11:25:59 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 2.4041
2022-02-25 11:26:41 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 2.2686
2022-02-25 11:27:20 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 2.2850
2022-02-25 11:27:55 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 2.1856
2022-02-25 11:28:35 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 2.1780
2022-02-25 11:29:16 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 2.2560
2022-02-25 11:29:53 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 2.3092
2022-02-25 11:30:31 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 2.1379
2022-02-25 11:31:13 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 2.2727
2022-02-25 11:31:51 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 2.3501
2022-02-25 11:32:27 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.4819
2022-02-25 11:32:28 - train: epoch 023, train_loss: 2.3873
2022-02-25 11:33:55 - eval: epoch: 023, acc1: 52.176%, acc5: 77.136%, test_loss: 2.0679, per_image_load_time: 1.520ms, per_image_inference_time: 0.570ms
2022-02-25 11:33:55 - until epoch: 023, best_acc1: 52.246%
2022-02-25 11:33:55 - epoch 024 lr: 0.1
2022-02-25 11:34:39 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.3618
2022-02-25 11:35:21 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 2.4370
2022-02-25 11:35:59 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 2.3180
2022-02-25 11:36:36 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 2.4487
2022-02-25 11:37:16 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 2.3170
2022-02-25 11:37:58 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.1315
2022-02-25 11:38:34 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 2.3217
2022-02-25 11:39:13 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 2.3293
2022-02-25 11:39:53 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 2.4615
2022-02-25 11:40:31 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 2.3818
2022-02-25 11:41:07 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 2.1060
2022-02-25 11:41:49 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 2.2623
2022-02-25 11:42:29 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.7957
2022-02-25 11:43:05 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 2.2983
2022-02-25 11:43:44 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.5108
2022-02-25 11:44:26 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 2.4083
2022-02-25 11:45:03 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 2.3627
2022-02-25 11:45:42 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.5581
2022-02-25 11:46:23 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 2.2213
2022-02-25 11:47:01 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.4487
2022-02-25 11:47:37 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 2.4382
2022-02-25 11:48:18 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 2.2605
2022-02-25 11:48:58 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.4115
2022-02-25 11:49:35 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.3162
2022-02-25 11:50:14 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 2.2237
2022-02-25 11:50:55 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 2.2991
2022-02-25 11:51:32 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.3451
2022-02-25 11:52:10 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.6025
2022-02-25 11:52:51 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.5464
2022-02-25 11:53:30 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 2.3753
2022-02-25 11:54:07 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 2.3059
2022-02-25 11:54:48 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.5750
2022-02-25 11:55:28 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 2.1370
2022-02-25 11:56:05 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 2.4024
2022-02-25 11:56:44 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 2.1945
2022-02-25 11:57:25 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.4241
2022-02-25 11:58:03 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.3198
2022-02-25 11:58:42 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.6622
2022-02-25 11:59:22 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 2.4170
2022-02-25 12:00:01 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.3079
2022-02-25 12:00:38 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 2.3429
2022-02-25 12:01:19 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 2.3446
2022-02-25 12:01:59 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 2.3483
2022-02-25 12:02:36 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 2.3561
2022-02-25 12:03:16 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 2.2093
2022-02-25 12:03:57 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 2.4647
2022-02-25 12:04:34 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.3677
2022-02-25 12:05:11 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 2.3429
2022-02-25 12:05:53 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 2.3970
2022-02-25 12:06:31 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.5244
2022-02-25 12:06:32 - train: epoch 024, train_loss: 2.3825
2022-02-25 12:08:01 - eval: epoch: 024, acc1: 51.628%, acc5: 77.276%, test_loss: 2.0670, per_image_load_time: 2.231ms, per_image_inference_time: 0.559ms
2022-02-25 12:08:01 - until epoch: 024, best_acc1: 52.246%
2022-02-25 12:08:01 - epoch 025 lr: 0.1
2022-02-25 12:08:46 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 2.2599
2022-02-25 12:09:23 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 2.1239
2022-02-25 12:10:05 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.3425
2022-02-25 12:10:45 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.4511
2022-02-25 12:11:20 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 2.2833
2022-02-25 12:12:01 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 2.3451
2022-02-25 12:12:42 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.3228
2022-02-25 12:13:18 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 2.2060
2022-02-25 12:13:57 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 2.1983
2022-02-25 12:14:38 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 2.3094
2022-02-25 12:15:16 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 2.3989
2022-02-25 12:15:52 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 2.3633
2022-02-25 12:16:34 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.2666
2022-02-25 12:17:13 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.4972
2022-02-25 12:17:50 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 2.2977
2022-02-25 12:18:29 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 2.1201
2022-02-25 12:19:09 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 2.3740
2022-02-25 12:19:47 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 2.2822
2022-02-25 12:20:26 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 2.1672
2022-02-25 12:21:07 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 2.3826
2022-02-25 12:21:46 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 2.1365
2022-02-25 12:22:22 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 2.2336
2022-02-25 12:23:04 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 2.4038
2022-02-25 12:23:43 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 2.1166
2022-02-25 12:24:21 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 2.4202
2022-02-25 12:25:01 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.3780
2022-02-25 12:25:42 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.3980
2022-02-25 12:26:18 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 2.2435
2022-02-25 12:26:57 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 2.4487
2022-02-25 12:27:39 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.5705
2022-02-25 12:28:16 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 2.2943
2022-02-25 12:28:52 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 2.6045
2022-02-25 12:29:34 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 2.5050
2022-02-25 12:30:13 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 2.4846
2022-02-25 12:30:50 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 2.1076
2022-02-25 12:31:29 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.5440
2022-02-25 12:32:10 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.3149
2022-02-25 12:32:47 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 2.4481
2022-02-25 12:33:24 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.6061
2022-02-25 12:34:06 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 2.4275
2022-02-25 12:34:44 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.5145
2022-02-25 12:35:20 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 2.5210
2022-02-25 12:36:01 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 2.1423
2022-02-25 12:36:42 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 2.3057
2022-02-25 12:37:17 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 2.4070
2022-02-25 12:37:58 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 2.3615
2022-02-25 12:38:38 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 2.3566
2022-02-25 12:39:16 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 2.1354
2022-02-25 12:39:53 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 2.4151
2022-02-25 12:40:34 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 2.5135
2022-02-25 12:40:35 - train: epoch 025, train_loss: 2.3752
2022-02-25 12:41:59 - eval: epoch: 025, acc1: 52.478%, acc5: 77.806%, test_loss: 2.0342, per_image_load_time: 2.226ms, per_image_inference_time: 0.553ms
2022-02-25 12:41:59 - until epoch: 025, best_acc1: 52.478%
2022-02-25 12:41:59 - epoch 026 lr: 0.1
2022-02-25 12:42:47 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 2.0794
2022-02-25 12:43:24 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 2.1417
2022-02-25 12:44:01 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 2.4062
2022-02-25 12:44:42 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 2.3457
2022-02-25 12:45:22 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 2.3015
2022-02-25 12:45:59 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.4718
2022-02-25 12:46:39 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 2.2947
2022-02-25 12:47:21 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 2.1807
2022-02-25 12:47:57 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 2.7202
2022-02-25 12:48:35 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 2.2052
2022-02-25 12:49:17 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.3563
2022-02-25 12:49:56 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 2.3916
2022-02-25 12:50:32 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 2.2054
2022-02-25 12:51:15 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 2.3674
2022-02-25 12:51:53 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 2.3507
2022-02-25 12:52:30 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 2.5021
2022-02-25 12:53:11 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.3588
2022-02-25 12:53:52 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.5022
2022-02-25 12:54:29 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 2.6796
2022-02-25 12:55:08 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 2.5591
2022-02-25 12:55:50 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 2.4226
2022-02-25 12:56:27 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.4536
2022-02-25 12:57:05 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 2.2857
2022-02-25 12:57:48 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.6342
2022-02-25 12:58:27 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 2.5857
2022-02-25 12:59:04 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 2.3817
2022-02-25 12:59:44 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 2.2860
2022-02-25 13:00:25 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 2.2075
2022-02-25 13:01:02 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 2.5876
2022-02-25 13:01:41 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 2.2552
2022-02-25 13:02:23 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.3137
2022-02-25 13:03:01 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 2.3866
2022-02-25 13:03:38 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 2.2476
2022-02-25 13:04:19 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.4724
2022-02-25 13:04:59 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.3866
2022-02-25 13:05:35 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 2.2818
2022-02-25 13:06:17 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 2.4013
2022-02-25 13:06:58 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.5662
2022-02-25 13:07:35 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.4866
2022-02-25 13:08:14 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.6059
2022-02-25 13:08:56 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 2.4059
2022-02-25 13:09:34 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 2.3915
2022-02-25 13:10:10 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.5687
2022-02-25 13:10:52 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.5044
2022-02-25 13:11:31 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.6736
2022-02-25 13:12:09 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.3364
2022-02-25 13:12:50 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 2.3038
2022-02-25 13:13:30 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 2.2988
2022-02-25 13:14:07 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.4837
2022-02-25 13:14:44 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 2.5797
2022-02-25 13:14:46 - train: epoch 026, train_loss: 2.3710
2022-02-25 13:16:14 - eval: epoch: 026, acc1: 52.610%, acc5: 78.024%, test_loss: 2.0157, per_image_load_time: 2.382ms, per_image_inference_time: 0.567ms
2022-02-25 13:16:14 - until epoch: 026, best_acc1: 52.610%
2022-02-25 13:16:14 - epoch 027 lr: 0.1
2022-02-25 13:16:59 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 2.5534
2022-02-25 13:17:39 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 2.2731
2022-02-25 13:18:17 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 2.5017
2022-02-25 13:18:57 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 2.4814
2022-02-25 13:19:37 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 2.4538
2022-02-25 13:20:14 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.5781
2022-02-25 13:20:51 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 2.7282
2022-02-25 13:21:32 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 2.3966
2022-02-25 13:22:10 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 2.3369
2022-02-25 13:22:48 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 2.3554
2022-02-25 13:23:28 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 2.3212
2022-02-25 13:24:07 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.7075
2022-02-25 13:24:44 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.4983
2022-02-25 13:25:24 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 2.6526
2022-02-25 13:26:04 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 2.4390
2022-02-25 13:26:41 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.3744
2022-02-25 13:27:19 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 2.3616
2022-02-25 13:27:59 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 2.4247
2022-02-25 13:28:36 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.5402
2022-02-25 13:29:13 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 2.3542
2022-02-25 13:29:55 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 2.6692
2022-02-25 13:30:31 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.5376
2022-02-25 13:31:07 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.6306
2022-02-25 13:31:47 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 2.3374
2022-02-25 13:32:27 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 2.2634
2022-02-25 13:33:04 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 2.6032
2022-02-25 13:33:45 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 2.3034
2022-02-25 13:34:23 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 2.4191
2022-02-25 13:35:00 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.3529
2022-02-25 13:35:38 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 2.0978
2022-02-25 13:36:18 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 2.2245
2022-02-25 13:36:57 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 2.2244
2022-02-25 13:37:35 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 2.3553
2022-02-25 13:38:16 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 2.3454
2022-02-25 13:38:53 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.5097
2022-02-25 13:39:28 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.3481
2022-02-25 13:40:08 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 2.2613
2022-02-25 13:40:48 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 2.2176
2022-02-25 13:41:28 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 2.2001
2022-02-25 13:42:08 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.5685
2022-02-25 13:42:46 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 2.4403
2022-02-25 13:43:22 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 2.3862
2022-02-25 13:43:58 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 2.3132
2022-02-25 13:44:35 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 2.3786
2022-02-25 13:45:16 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.2719
2022-02-25 13:45:53 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 2.2306
2022-02-25 13:46:32 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 2.3552
2022-02-25 13:47:11 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.6384
2022-02-25 13:47:50 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 2.4018
2022-02-25 13:48:26 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 2.0111
2022-02-25 13:48:27 - train: epoch 027, train_loss: 2.3662
2022-02-25 13:49:56 - eval: epoch: 027, acc1: 52.294%, acc5: 77.564%, test_loss: 2.0404, per_image_load_time: 2.792ms, per_image_inference_time: 0.567ms
2022-02-25 13:49:56 - until epoch: 027, best_acc1: 52.610%
2022-02-25 13:49:56 - epoch 028 lr: 0.1
2022-02-25 13:50:41 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 2.1205
2022-02-25 13:51:21 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 2.2037
2022-02-25 13:51:57 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 2.2090
2022-02-25 13:52:33 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 2.3534
2022-02-25 13:53:12 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 2.3059
2022-02-25 13:53:52 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.3875
2022-02-25 13:54:30 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.5903
2022-02-25 13:55:10 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 2.1434
2022-02-25 13:55:49 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 2.1608
2022-02-25 13:56:28 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.3975
2022-02-25 13:57:07 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 2.4341
2022-02-25 13:57:48 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 2.2196
2022-02-25 13:58:24 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 2.2519
2022-02-25 13:59:01 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.6819
2022-02-25 13:59:41 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 2.3532
2022-02-25 14:00:19 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 2.2636
2022-02-25 14:00:56 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.2810
2022-02-25 14:01:35 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 2.1522
2022-02-25 14:02:14 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.2756
2022-02-25 14:02:50 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 2.5196
2022-02-25 14:03:28 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.4236
2022-02-25 14:04:09 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.4159
2022-02-25 14:04:46 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.5571
2022-02-25 14:05:23 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.5245
2022-02-25 14:06:03 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 2.3152
2022-02-25 14:06:41 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 2.1971
2022-02-25 14:07:17 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.2386
2022-02-25 14:07:55 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 2.3267
2022-02-25 14:08:35 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 2.4131
2022-02-25 14:09:11 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 2.3764
2022-02-25 14:09:48 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.5204
2022-02-25 14:10:29 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 2.2938
2022-02-25 14:11:05 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 2.4194
2022-02-25 14:11:41 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.2419
2022-02-25 14:12:20 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 2.2135
2022-02-25 14:13:00 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 2.1710
2022-02-25 14:13:35 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 2.4097
2022-02-25 14:14:12 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 2.0395
2022-02-25 14:14:53 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 2.4028
2022-02-25 14:15:30 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.5101
2022-02-25 14:16:06 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 2.4706
2022-02-25 14:16:46 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.5875
2022-02-25 14:17:24 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 2.3119
2022-02-25 14:17:59 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 2.3260
2022-02-25 14:18:38 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.3215
2022-02-25 14:19:18 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 2.4050
2022-02-25 14:19:53 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.3299
2022-02-25 14:20:30 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 2.2587
2022-02-25 14:21:11 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 2.1861
2022-02-25 14:21:47 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 2.2568
2022-02-25 14:21:49 - train: epoch 028, train_loss: 2.3591
2022-02-25 14:23:15 - eval: epoch: 028, acc1: 52.106%, acc5: 77.086%, test_loss: 2.0707, per_image_load_time: 2.779ms, per_image_inference_time: 0.553ms
2022-02-25 14:23:15 - until epoch: 028, best_acc1: 52.610%
2022-02-25 14:23:15 - epoch 029 lr: 0.1
2022-02-25 14:23:58 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 2.3978
2022-02-25 14:24:35 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 2.4502
2022-02-25 14:25:16 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.5431
2022-02-25 14:25:53 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 2.2050
2022-02-25 14:26:29 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 2.3666
2022-02-25 14:27:08 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 2.7015
2022-02-25 14:27:48 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.9714
2022-02-25 14:28:24 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 2.5209
2022-02-25 14:29:00 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.1920
2022-02-25 14:29:41 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 2.2021
2022-02-25 14:30:19 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 2.2630
2022-02-25 14:30:55 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 2.4541
2022-02-25 14:31:35 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 2.4246
2022-02-25 14:32:14 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 2.6090
2022-02-25 14:32:50 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 2.5108
2022-02-25 14:33:30 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 2.3206
2022-02-25 14:34:09 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 2.3463
2022-02-25 14:34:45 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 2.4646
2022-02-25 14:35:23 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 2.1753
2022-02-25 14:36:03 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 2.6119
2022-02-25 14:36:41 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.3422
2022-02-25 14:37:18 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.4375
2022-02-25 14:37:58 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 2.3332
2022-02-25 14:38:38 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 2.2912
2022-02-25 14:39:14 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 2.2740
2022-02-25 14:39:53 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.4770
2022-02-25 14:40:33 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 2.4144
2022-02-25 14:41:09 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.2816
2022-02-25 14:41:47 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.3038
2022-02-25 14:42:27 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 2.3525
2022-02-25 14:43:05 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.3605
2022-02-25 14:43:41 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.4243
2022-02-25 14:44:21 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 2.2383
2022-02-25 14:45:00 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 2.0785
2022-02-25 14:45:36 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.3043
2022-02-25 14:46:14 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 2.2859
2022-02-25 14:46:55 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 2.2206
2022-02-25 14:47:31 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 2.3219
2022-02-25 14:48:09 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 2.1632
2022-02-25 14:48:49 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 2.3764
2022-02-25 14:49:27 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.2111
2022-02-25 14:50:02 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 2.2669
2022-02-25 14:50:42 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.4537
2022-02-25 14:51:20 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.3466
2022-02-25 14:51:57 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 2.4381
2022-02-25 14:52:34 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.5782
2022-02-25 14:53:15 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 2.1457
2022-02-25 14:53:53 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 2.3889
2022-02-25 14:54:30 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.7728
2022-02-25 14:55:08 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 2.0763
2022-02-25 14:55:10 - train: epoch 029, train_loss: 2.3566
2022-02-25 14:56:38 - eval: epoch: 029, acc1: 52.786%, acc5: 77.886%, test_loss: 2.0211, per_image_load_time: 2.608ms, per_image_inference_time: 0.558ms
2022-02-25 14:56:38 - until epoch: 029, best_acc1: 52.786%
2022-02-25 14:56:38 - epoch 030 lr: 0.1
2022-02-25 14:57:25 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 2.6089
2022-02-25 14:58:04 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.3440
2022-02-25 14:58:45 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 2.3391
2022-02-25 14:59:24 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 2.0836
2022-02-25 15:00:03 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 2.6434
2022-02-25 15:00:42 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 2.0974
2022-02-25 15:01:23 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 2.4449
2022-02-25 15:02:02 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.4697
2022-02-25 15:02:38 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.4288
2022-02-25 15:03:19 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 2.1172
2022-02-25 15:03:57 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 2.1587
2022-02-25 15:04:33 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 2.3715
2022-02-25 15:05:12 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 2.0216
2022-02-25 15:05:51 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 2.2838
2022-02-25 15:06:29 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 2.2565
2022-02-25 15:07:10 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 2.2707
2022-02-25 15:07:50 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.6582
2022-02-25 15:08:28 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 2.3427
2022-02-25 15:09:09 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 2.3854
2022-02-25 15:09:50 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 2.3733
2022-02-25 15:10:30 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 2.4282
2022-02-25 15:11:09 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.2375
2022-02-25 15:11:51 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 2.3682
2022-02-25 15:12:30 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 2.5812
2022-02-25 15:13:10 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.3190
2022-02-25 15:13:51 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 2.3150
2022-02-25 15:14:30 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 2.2309
2022-02-25 15:15:10 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 2.3987
2022-02-25 15:15:50 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 2.1911
2022-02-25 15:16:29 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 2.5818
2022-02-25 15:17:08 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 2.2999
2022-02-25 15:17:49 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 2.1147
2022-02-25 15:18:30 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.5684
2022-02-25 15:19:09 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 2.4197
2022-02-25 15:19:49 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 2.3558
2022-02-25 15:20:30 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 2.1783
2022-02-25 15:21:08 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 2.2537
2022-02-25 15:21:49 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 2.4660
2022-02-25 15:22:29 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 2.4062
2022-02-25 15:23:08 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 2.1841
2022-02-25 15:23:48 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 2.2296
2022-02-25 15:24:29 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.5225
2022-02-25 15:25:08 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.3412
2022-02-25 15:25:48 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 2.2911
2022-02-25 15:26:30 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 2.4775
2022-02-25 15:27:07 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 2.1202
2022-02-25 15:27:47 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.3925
2022-02-25 15:28:29 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 2.4903
2022-02-25 15:29:07 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 2.4210
2022-02-25 15:29:46 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.4590
2022-02-25 15:29:47 - train: epoch 030, train_loss: 2.3549
2022-02-25 15:31:20 - eval: epoch: 030, acc1: 52.332%, acc5: 77.842%, test_loss: 2.0324, per_image_load_time: 3.032ms, per_image_inference_time: 0.565ms
2022-02-25 15:31:20 - until epoch: 030, best_acc1: 52.786%
2022-02-25 15:31:20 - epoch 031 lr: 0.010000000000000002
2022-02-25 15:32:07 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 2.2400
2022-02-25 15:32:49 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 2.0025
2022-02-25 15:33:27 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.9810
2022-02-25 15:34:08 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 2.1005
2022-02-25 15:34:49 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.8895
2022-02-25 15:35:25 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.9321
2022-02-25 15:36:03 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.8186
2022-02-25 15:36:45 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.9190
2022-02-25 15:37:23 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.8846
2022-02-25 15:38:00 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 2.1347
2022-02-25 15:38:42 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 2.1840
2022-02-25 15:39:21 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.9174
2022-02-25 15:39:58 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.6475
2022-02-25 15:40:40 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.8997
2022-02-25 15:41:19 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.9789
2022-02-25 15:41:57 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.7881
2022-02-25 15:42:37 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.7633
2022-02-25 15:43:18 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.9041
2022-02-25 15:43:55 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.9615
2022-02-25 15:44:33 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.9365
2022-02-25 15:45:14 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.7474
2022-02-25 15:45:54 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.6914
2022-02-25 15:46:30 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.7618
2022-02-25 15:47:11 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.8665
2022-02-25 15:47:53 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.7478
2022-02-25 15:48:29 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.8746
2022-02-25 15:49:07 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.9249
2022-02-25 15:49:49 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 2.0876
2022-02-25 15:50:28 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.8846
2022-02-25 15:51:05 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 2.0208
2022-02-25 15:51:47 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.6653
2022-02-25 15:52:27 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 2.0566
2022-02-25 15:53:05 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.8264
2022-02-25 15:53:46 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 2.0475
2022-02-25 15:54:27 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.9423
2022-02-25 15:55:04 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.8481
2022-02-25 15:55:44 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.7490
2022-02-25 15:56:25 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.6592
2022-02-25 15:57:04 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.8826
2022-02-25 15:57:41 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.7833
2022-02-25 15:58:22 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.6624
2022-02-25 15:59:04 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.8616
2022-02-25 15:59:43 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.7983
2022-02-25 16:00:24 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.9537
2022-02-25 16:01:05 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.8259
2022-02-25 16:01:43 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.9486
2022-02-25 16:02:24 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.7846
2022-02-25 16:03:06 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.8493
2022-02-25 16:03:45 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.6783
2022-02-25 16:04:22 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.8161
2022-02-25 16:04:24 - train: epoch 031, train_loss: 1.8750
2022-02-25 16:05:56 - eval: epoch: 031, acc1: 64.380%, acc5: 85.892%, test_loss: 1.4682, per_image_load_time: 2.960ms, per_image_inference_time: 0.548ms
2022-02-25 16:05:57 - until epoch: 031, best_acc1: 64.380%
2022-02-25 16:05:57 - epoch 032 lr: 0.010000000000000002
2022-02-25 16:06:45 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.7353
2022-02-25 16:07:28 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.8269
2022-02-25 16:08:09 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.7895
2022-02-25 16:08:50 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.9252
2022-02-25 16:09:33 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.8040
2022-02-25 16:10:14 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.8822
2022-02-25 16:10:55 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.8943
2022-02-25 16:11:38 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.7743
2022-02-25 16:12:19 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.7390
2022-02-25 16:12:58 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.9369
2022-02-25 16:13:41 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.8711
2022-02-25 16:14:22 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.7836
2022-02-25 16:15:02 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.7522
2022-02-25 16:15:43 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.9232
2022-02-25 16:16:24 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.8085
2022-02-25 16:17:03 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.8816
2022-02-25 16:17:44 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.9048
2022-02-25 16:18:28 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 2.0505
2022-02-25 16:19:07 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.5842
2022-02-25 16:19:48 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.7345
2022-02-25 16:20:30 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.6876
2022-02-25 16:21:11 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.6354
2022-02-25 16:21:52 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.7534
2022-02-25 16:22:33 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.7823
2022-02-25 16:23:14 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.8789
2022-02-25 16:23:55 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.5793
2022-02-25 16:24:37 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.8085
2022-02-25 16:25:18 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.8730
2022-02-25 16:25:59 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.5902
2022-02-25 16:26:42 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.5558
2022-02-25 16:27:21 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.7137
2022-02-25 16:28:01 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.7834
2022-02-25 16:28:44 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.5544
2022-02-25 16:29:26 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.6583
2022-02-25 16:30:06 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.7881
2022-02-25 16:30:48 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.8441
2022-02-25 16:31:30 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.7918
2022-02-25 16:32:10 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.5822
2022-02-25 16:32:52 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.7754
2022-02-25 16:33:34 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.6755
2022-02-25 16:34:14 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.7386
2022-02-25 16:34:54 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.6168
2022-02-25 16:35:37 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 2.0283
2022-02-25 16:36:17 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.7046
2022-02-25 16:36:57 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.8699
2022-02-25 16:37:39 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.7208
2022-02-25 16:38:18 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.8410
2022-02-25 16:38:58 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.5892
2022-02-25 16:39:40 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.8964
2022-02-25 16:40:18 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.6602
2022-02-25 16:40:20 - train: epoch 032, train_loss: 1.7590
2022-02-25 16:41:51 - eval: epoch: 032, acc1: 65.498%, acc5: 86.488%, test_loss: 1.4261, per_image_load_time: 2.950ms, per_image_inference_time: 0.541ms
2022-02-25 16:41:51 - until epoch: 032, best_acc1: 65.498%
2022-02-25 16:41:51 - epoch 033 lr: 0.010000000000000002
2022-02-25 16:42:39 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.6145
2022-02-25 16:43:22 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.8351
2022-02-25 16:44:05 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.5876
2022-02-25 16:44:46 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.5578
2022-02-25 16:45:27 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.7883
2022-02-25 16:46:09 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.6458
2022-02-25 16:46:49 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.8760
2022-02-25 16:47:28 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.8501
2022-02-25 16:48:10 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.8538
2022-02-25 16:48:51 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.7518
2022-02-25 16:49:27 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.6709
2022-02-25 16:50:09 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.7180
2022-02-25 16:50:50 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.6607
2022-02-25 16:51:28 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.9195
2022-02-25 16:52:09 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.9399
2022-02-25 16:52:50 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.8761
2022-02-25 16:53:28 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.6379
2022-02-25 16:54:08 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.9211
2022-02-25 16:54:50 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.7128
2022-02-25 16:55:30 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.5875
2022-02-25 16:56:08 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.6883
2022-02-25 16:56:51 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.7563
2022-02-25 16:57:31 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.5170
2022-02-25 16:58:10 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 2.0035
2022-02-25 16:58:51 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.6970
2022-02-25 16:59:33 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.5538
2022-02-25 17:00:11 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.8056
2022-02-25 17:00:51 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.7306
2022-02-25 17:01:33 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.8486
2022-02-25 17:02:11 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.7298
2022-02-25 17:02:49 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.7359
2022-02-25 17:03:32 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.6298
2022-02-25 17:04:10 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.7685
2022-02-25 17:04:48 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.6567
2022-02-25 17:05:29 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.7726
2022-02-25 17:06:10 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.9178
2022-02-25 17:06:47 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.6164
2022-02-25 17:07:26 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.5256
2022-02-25 17:08:08 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.8370
2022-02-25 17:08:48 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.7566
2022-02-25 17:09:25 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.7263
2022-02-25 17:10:03 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.6198
2022-02-25 17:10:44 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.8429
2022-02-25 17:11:24 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.7872
2022-02-25 17:12:01 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.8841
2022-02-25 17:12:42 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.6121
2022-02-25 17:13:23 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.5764
2022-02-25 17:14:00 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 2.0577
2022-02-25 17:14:41 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.6103
2022-02-25 17:15:21 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.6197
2022-02-25 17:15:22 - train: epoch 033, train_loss: 1.7155
2022-02-25 17:16:50 - eval: epoch: 033, acc1: 65.874%, acc5: 86.830%, test_loss: 1.3982, per_image_load_time: 2.276ms, per_image_inference_time: 0.544ms
2022-02-25 17:16:50 - until epoch: 033, best_acc1: 65.874%
2022-02-25 17:16:50 - epoch 034 lr: 0.010000000000000002
2022-02-25 17:17:38 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.7196
2022-02-25 17:18:18 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.6752
2022-02-25 17:18:59 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.6535
2022-02-25 17:19:41 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.4113
2022-02-25 17:20:19 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.6536
2022-02-25 17:20:59 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.9598
2022-02-25 17:21:42 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.5473
2022-02-25 17:22:20 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.6200
2022-02-25 17:23:00 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.6184
2022-02-25 17:23:42 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.6962
2022-02-25 17:24:20 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.7711
2022-02-25 17:24:57 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.6275
2022-02-25 17:25:38 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.5966
2022-02-25 17:26:18 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.6748
2022-02-25 17:26:54 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.6345
2022-02-25 17:27:33 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.4827
2022-02-25 17:28:14 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.7138
2022-02-25 17:28:51 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.8132
2022-02-25 17:29:29 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.8373
2022-02-25 17:30:10 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.8453
2022-02-25 17:30:48 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.9117
2022-02-25 17:31:25 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.5837
2022-02-25 17:32:04 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.7694
2022-02-25 17:32:45 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.5904
2022-02-25 17:33:22 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.6794
2022-02-25 17:34:00 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.6858
2022-02-25 17:34:41 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.7188
2022-02-25 17:35:19 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 1.5104
2022-02-25 17:35:56 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.4474
2022-02-25 17:36:38 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.5170
2022-02-25 17:37:16 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.6302
2022-02-25 17:37:52 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.7230
2022-02-25 17:38:32 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.6694
2022-02-25 17:39:14 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.7512
2022-02-25 17:39:52 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.5145
2022-02-25 17:40:31 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.5341
2022-02-25 17:41:13 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.5464
2022-02-25 17:41:52 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.5548
2022-02-25 17:42:29 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.7385
2022-02-25 17:43:11 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.5525
2022-02-25 17:43:52 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.6803
2022-02-25 17:44:29 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.6735
2022-02-25 17:45:10 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.7109
2022-02-25 17:45:51 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.6762
2022-02-25 17:46:29 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.7887
2022-02-25 17:47:09 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.7779
2022-02-25 17:47:51 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.6966
2022-02-25 17:48:31 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.6236
2022-02-25 17:49:09 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.6463
2022-02-25 17:49:50 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.5138
2022-02-25 17:49:52 - train: epoch 034, train_loss: 1.6917
2022-02-25 17:51:18 - eval: epoch: 034, acc1: 66.278%, acc5: 86.932%, test_loss: 1.3852, per_image_load_time: 2.722ms, per_image_inference_time: 0.554ms
2022-02-25 17:51:18 - until epoch: 034, best_acc1: 66.278%
2022-02-25 17:51:18 - epoch 035 lr: 0.010000000000000002
2022-02-25 17:52:07 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 1.5854
2022-02-25 17:52:45 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 1.4659
2022-02-25 17:53:24 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.7948
2022-02-25 17:54:06 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.5982
2022-02-25 17:54:46 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.6869
2022-02-25 17:55:24 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.6259
2022-02-25 17:56:06 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.6622
2022-02-25 17:56:46 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.6146
2022-02-25 17:57:24 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.8814
2022-02-25 17:58:04 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.8132
2022-02-25 17:58:46 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.7409
2022-02-25 17:59:23 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.5615
2022-02-25 18:00:02 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.6964
2022-02-25 18:00:45 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.6008
2022-02-25 18:01:23 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.7604
2022-02-25 18:02:01 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.7069
2022-02-25 18:02:42 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.5297
2022-02-25 18:03:20 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.7371
2022-02-25 18:03:58 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.6981
2022-02-25 18:04:39 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.7311
2022-02-25 18:05:19 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.5692
2022-02-25 18:05:56 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.8350
2022-02-25 18:06:34 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.6648
2022-02-25 18:07:16 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.7959
2022-02-25 18:07:54 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.6240
2022-02-25 18:08:32 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.7702
2022-02-25 18:09:13 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.7438
2022-02-25 18:09:53 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.6755
2022-02-25 18:10:30 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.7073
2022-02-25 18:11:08 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.7411
2022-02-25 18:11:50 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.6754
2022-02-25 18:12:27 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.5241
2022-02-25 18:13:04 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.5972
2022-02-25 18:13:45 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.6434
2022-02-25 18:14:25 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.4763
2022-02-25 18:15:03 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.6766
2022-02-25 18:15:42 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.4413
2022-02-25 18:16:23 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.6608
2022-02-25 18:17:01 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.8010
2022-02-25 18:17:39 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 1.4511
2022-02-25 18:18:21 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.8159
2022-02-25 18:19:00 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.6147
2022-02-25 18:19:37 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.7115
2022-02-25 18:20:16 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.8133
2022-02-25 18:20:58 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.7670
2022-02-25 18:21:35 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.5720
2022-02-25 18:22:14 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.9740
2022-02-25 18:22:55 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.8148
2022-02-25 18:23:35 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.6960
2022-02-25 18:24:11 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.5450
2022-02-25 18:24:12 - train: epoch 035, train_loss: 1.6754
2022-02-25 18:25:42 - eval: epoch: 035, acc1: 66.476%, acc5: 86.992%, test_loss: 1.3792, per_image_load_time: 2.835ms, per_image_inference_time: 0.581ms
2022-02-25 18:25:42 - until epoch: 035, best_acc1: 66.476%
2022-02-25 18:25:42 - epoch 036 lr: 0.010000000000000002
2022-02-25 18:26:25 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.7579
2022-02-25 18:27:06 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.3600
2022-02-25 18:27:46 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.6658
2022-02-25 18:28:23 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.7022
2022-02-25 18:29:01 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.6819
2022-02-25 18:29:43 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.6077
2022-02-25 18:30:19 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.4767
2022-02-25 18:30:59 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.3863
2022-02-25 18:31:40 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.6805
2022-02-25 18:32:18 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.6693
2022-02-25 18:32:55 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.7964
2022-02-25 18:33:35 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.6283
2022-02-25 18:34:16 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.5049
2022-02-25 18:34:54 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.8996
2022-02-25 18:35:32 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.8768
2022-02-25 18:36:13 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.8022
2022-02-25 18:36:52 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.7267
2022-02-25 18:37:29 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 1.5199
2022-02-25 18:38:10 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.7082
2022-02-25 18:38:50 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.6189
2022-02-25 18:39:26 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 1.6311
2022-02-25 18:40:06 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.6126
2022-02-25 18:40:46 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.7546
2022-02-25 18:41:23 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.7604
2022-02-25 18:42:01 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.5158
2022-02-25 18:42:43 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.7709
2022-02-25 18:43:21 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.6383
2022-02-25 18:43:58 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.6028
2022-02-25 18:44:37 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.5089
2022-02-25 18:45:18 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.7522
2022-02-25 18:45:54 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.6879
2022-02-25 18:46:31 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.6918
2022-02-25 18:47:11 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.5829
2022-02-25 18:47:48 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.5123
2022-02-25 18:48:25 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.6722
2022-02-25 18:49:05 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.8100
2022-02-25 18:49:45 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.6433
2022-02-25 18:50:20 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.5574
2022-02-25 18:50:57 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.5580
2022-02-25 18:51:38 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.6764
2022-02-25 18:52:15 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.7153
2022-02-25 18:52:51 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.6565
2022-02-25 18:53:33 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.6124
2022-02-25 18:54:11 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.6715
2022-02-25 18:54:47 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.3698
2022-02-25 18:55:25 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.5185
2022-02-25 18:56:05 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.6663
2022-02-25 18:56:42 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.6850
2022-02-25 18:57:17 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.6304
2022-02-25 18:57:56 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.6391
2022-02-25 18:57:57 - train: epoch 036, train_loss: 1.6641
2022-02-25 18:59:22 - eval: epoch: 036, acc1: 66.340%, acc5: 87.014%, test_loss: 1.3805, per_image_load_time: 2.103ms, per_image_inference_time: 0.562ms
2022-02-25 18:59:23 - until epoch: 036, best_acc1: 66.476%
2022-02-25 18:59:23 - epoch 037 lr: 0.010000000000000002
2022-02-25 19:00:09 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.5162
2022-02-25 19:00:49 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.3640
2022-02-25 19:01:25 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.5473
2022-02-25 19:02:02 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.7706
2022-02-25 19:02:43 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.5970
2022-02-25 19:03:20 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.7545
2022-02-25 19:03:55 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.7665
2022-02-25 19:04:36 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.5970
2022-02-25 19:05:15 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 2.0207
2022-02-25 19:05:51 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.4215
2022-02-25 19:06:29 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.7145
2022-02-25 19:07:10 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.6881
2022-02-25 19:07:46 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.6822
2022-02-25 19:08:22 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.7075
2022-02-25 19:09:03 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.4651
2022-02-25 19:09:41 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.4766
2022-02-25 19:10:17 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.8855
2022-02-25 19:10:55 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.7774
2022-02-25 19:11:37 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.7294
2022-02-25 19:12:14 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.7430
2022-02-25 19:12:50 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.6892
2022-02-25 19:13:30 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.6181
2022-02-25 19:14:10 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.5196
2022-02-25 19:14:46 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.5784
2022-02-25 19:15:24 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.5561
2022-02-25 19:16:06 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.8908
2022-02-25 19:16:42 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.7538
2022-02-25 19:17:19 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.7173
2022-02-25 19:17:59 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.6749
2022-02-25 19:18:38 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.8375
2022-02-25 19:19:14 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.6245
2022-02-25 19:19:53 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.7948
2022-02-25 19:20:34 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.5134
2022-02-25 19:21:11 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.5194
2022-02-25 19:21:48 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.7160
2022-02-25 19:22:28 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.8085
2022-02-25 19:23:07 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.6986
2022-02-25 19:23:43 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.5468
2022-02-25 19:24:22 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.8240
2022-02-25 19:25:03 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.5985
2022-02-25 19:25:38 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.6339
2022-02-25 19:26:16 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.8066
2022-02-25 19:26:56 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.7248
2022-02-25 19:27:33 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.5393
2022-02-25 19:28:09 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.6613
2022-02-25 19:28:48 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.7448
2022-02-25 19:29:28 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.7477
2022-02-25 19:30:03 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.5900
2022-02-25 19:30:41 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.7030
2022-02-25 19:31:21 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.6714
2022-02-25 19:31:22 - train: epoch 037, train_loss: 1.6602
2022-02-25 19:32:45 - eval: epoch: 037, acc1: 66.400%, acc5: 87.150%, test_loss: 1.3704, per_image_load_time: 1.158ms, per_image_inference_time: 0.558ms
2022-02-25 19:32:45 - until epoch: 037, best_acc1: 66.476%
2022-02-25 19:32:45 - epoch 038 lr: 0.010000000000000002
2022-02-25 19:33:32 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 1.6580
2022-02-25 19:34:09 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.4465
2022-02-25 19:34:45 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.4100
2022-02-25 19:35:24 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.6273
2022-02-25 19:36:04 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.5096
2022-02-25 19:36:38 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.8165
2022-02-25 19:37:15 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.4253
2022-02-25 19:37:56 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.5962
2022-02-25 19:38:32 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.6165
2022-02-25 19:39:09 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.8807
2022-02-25 19:39:48 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.5433
2022-02-25 19:40:28 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.6860
2022-02-25 19:41:04 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.7983
2022-02-25 19:41:41 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.7479
2022-02-25 19:42:21 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.6662
2022-02-25 19:42:59 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.7349
2022-02-25 19:43:34 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.7469
2022-02-25 19:44:14 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.7178
2022-02-25 19:44:53 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.6432
2022-02-25 19:45:28 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.6073
2022-02-25 19:46:07 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.7227
2022-02-25 19:46:47 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.5439
2022-02-25 19:47:22 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.8381
2022-02-25 19:47:59 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.9325
2022-02-25 19:48:40 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 1.4908
2022-02-25 19:49:17 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.7547
2022-02-25 19:49:53 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.8049
2022-02-25 19:50:34 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.8815
2022-02-25 19:51:13 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.7372
2022-02-25 19:51:50 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.6032
2022-02-25 19:52:30 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.6632
2022-02-25 19:53:09 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.4560
2022-02-25 19:53:48 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.3734
2022-02-25 19:54:27 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.6147
2022-02-25 19:55:08 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.7080
2022-02-25 19:55:46 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.7225
2022-02-25 19:56:24 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.4972
2022-02-25 19:57:05 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.7520
2022-02-25 19:57:43 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.5957
2022-02-25 19:58:20 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.7413
2022-02-25 19:59:00 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.6505
2022-02-25 19:59:39 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.4274
2022-02-25 20:00:17 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.7757
2022-02-25 20:00:58 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.7328
2022-02-25 20:01:37 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.8398
2022-02-25 20:02:15 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.8625
2022-02-25 20:02:56 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.5593
2022-02-25 20:03:36 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.6755
2022-02-25 20:04:14 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.6712
2022-02-25 20:04:52 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 1.5203
2022-02-25 20:04:53 - train: epoch 038, train_loss: 1.6546
2022-02-25 20:06:23 - eval: epoch: 038, acc1: 66.474%, acc5: 87.246%, test_loss: 1.3696, per_image_load_time: 2.816ms, per_image_inference_time: 0.558ms
2022-02-25 20:06:23 - until epoch: 038, best_acc1: 66.476%
2022-02-25 20:06:23 - epoch 039 lr: 0.010000000000000002
2022-02-25 20:07:10 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.7081
2022-02-25 20:07:49 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.8843
2022-02-25 20:08:27 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 1.4636
2022-02-25 20:09:07 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.7401
2022-02-25 20:09:46 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.6501
2022-02-25 20:10:23 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.4682
2022-02-25 20:11:03 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.7651
2022-02-25 20:11:43 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.5069
2022-02-25 20:12:20 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.8230
2022-02-25 20:13:00 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.4729
2022-02-25 20:13:40 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.7445
2022-02-25 20:14:19 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.7529
2022-02-25 20:14:58 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.8994
2022-02-25 20:15:38 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.6233
2022-02-25 20:16:17 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.6247
2022-02-25 20:16:55 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.7335
2022-02-25 20:17:36 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.4451
2022-02-25 20:18:14 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.7695
2022-02-25 20:18:53 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 1.3864
2022-02-25 20:19:33 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.5939
2022-02-25 20:20:12 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.6536
2022-02-25 20:20:51 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.6476
2022-02-25 20:21:31 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.8879
2022-02-25 20:22:10 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.8498
2022-02-25 20:22:45 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.5485
2022-02-25 20:23:23 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.6249
2022-02-25 20:24:03 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.8630
2022-02-25 20:24:42 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.5575
2022-02-25 20:25:21 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 1.5179
2022-02-25 20:26:02 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.7120
2022-02-25 20:26:40 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 1.5892
2022-02-25 20:27:18 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.7148
2022-02-25 20:27:57 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.6466
2022-02-25 20:28:36 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.7361
2022-02-25 20:29:13 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.8623
2022-02-25 20:29:54 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.7353
2022-02-25 20:30:33 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.6487
2022-02-25 20:31:11 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.5196
2022-02-25 20:31:51 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.7470
2022-02-25 20:32:33 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.5931
2022-02-25 20:33:13 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.6157
2022-02-25 20:33:54 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.6644
2022-02-25 20:34:34 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.7049
2022-02-25 20:35:15 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.4534
2022-02-25 20:35:55 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.4762
2022-02-25 20:36:37 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.8392
2022-02-25 20:37:17 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.7214
2022-02-25 20:37:57 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.6588
2022-02-25 20:38:38 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.5905
2022-02-25 20:39:19 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.5491
2022-02-25 20:39:20 - train: epoch 039, train_loss: 1.6523
2022-02-25 20:40:52 - eval: epoch: 039, acc1: 65.978%, acc5: 86.980%, test_loss: 1.3827, per_image_load_time: 2.863ms, per_image_inference_time: 0.559ms
2022-02-25 20:40:52 - until epoch: 039, best_acc1: 66.476%
2022-02-25 20:40:52 - epoch 040 lr: 0.010000000000000002
2022-02-25 20:41:39 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.9106
2022-02-25 20:42:20 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.8689
2022-02-25 20:43:03 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.7344
2022-02-25 20:43:43 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.6681
2022-02-25 20:44:25 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.5309
2022-02-25 20:45:07 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.7268
2022-02-25 20:45:49 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.6969
2022-02-25 20:46:30 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.7591
2022-02-25 20:47:13 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.5702
2022-02-25 20:47:55 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 1.3330
2022-02-25 20:48:36 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 1.5649
2022-02-25 20:49:17 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.5469
2022-02-25 20:49:59 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 1.6096
2022-02-25 20:50:41 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.5577
2022-02-25 20:51:23 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.8854
2022-02-25 20:52:03 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.6602
2022-02-25 20:52:45 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 1.6101
2022-02-25 20:53:27 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.5462
2022-02-25 20:54:09 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.6571
2022-02-25 20:54:51 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.7538
2022-02-25 20:55:31 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 1.5620
2022-02-25 20:56:10 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 1.4649
2022-02-25 20:56:51 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 1.5725
2022-02-25 20:57:31 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.6909
2022-02-25 20:58:09 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.7216
2022-02-25 20:58:50 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.5829
2022-02-25 20:59:30 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.8385
2022-02-25 21:00:08 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.6243
2022-02-25 21:00:49 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.9870
2022-02-25 21:01:30 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.7588
2022-02-25 21:02:08 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.5934
2022-02-25 21:02:46 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.7776
2022-02-25 21:03:26 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.7826
2022-02-25 21:04:04 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.6793
2022-02-25 21:04:41 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.6901
2022-02-25 21:05:22 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.7295
2022-02-25 21:06:01 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.7432
2022-02-25 21:06:37 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.4819
2022-02-25 21:07:17 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.6110
2022-02-25 21:07:56 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.7898
2022-02-25 21:08:33 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.7075
2022-02-25 21:09:12 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.5564
2022-02-25 21:09:52 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.6603
2022-02-25 21:10:29 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 1.6097
2022-02-25 21:11:07 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 1.4761
2022-02-25 21:11:47 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.7168
2022-02-25 21:12:27 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.6852
2022-02-25 21:13:02 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 1.5079
2022-02-25 21:13:42 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.7166
2022-02-25 21:14:21 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.6899
2022-02-25 21:14:22 - train: epoch 040, train_loss: 1.6507
2022-02-25 21:15:47 - eval: epoch: 040, acc1: 65.362%, acc5: 86.456%, test_loss: 1.4185, per_image_load_time: 2.412ms, per_image_inference_time: 0.549ms
2022-02-25 21:15:48 - until epoch: 040, best_acc1: 66.476%
2022-02-25 21:15:48 - epoch 041 lr: 0.010000000000000002
2022-02-25 21:16:33 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.8170
2022-02-25 21:17:10 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.7193
2022-02-25 21:17:47 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.6546
2022-02-25 21:18:28 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.6698
2022-02-25 21:19:05 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.4400
2022-02-25 21:19:41 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 1.7730
2022-02-25 21:20:22 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.5428
2022-02-25 21:21:00 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.3992
2022-02-25 21:21:37 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.4259
2022-02-25 21:22:16 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.8725
2022-02-25 21:22:56 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.5545
2022-02-25 21:23:34 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 1.4011
2022-02-25 21:24:11 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 1.6350
2022-02-25 21:24:52 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.6428
2022-02-25 21:25:30 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 2.0182
2022-02-25 21:26:07 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.3786
2022-02-25 21:26:48 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.6242
2022-02-25 21:27:27 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.5612
2022-02-25 21:28:03 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.7816
2022-02-25 21:28:42 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.5461
2022-02-25 21:29:22 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.6050
2022-02-25 21:29:59 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.5195
2022-02-25 21:30:36 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.3391
2022-02-25 21:31:16 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.8764
2022-02-25 21:31:55 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.5864
2022-02-25 21:32:32 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.8217
2022-02-25 21:33:12 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.8973
2022-02-25 21:33:52 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.6321
2022-02-25 21:34:29 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.7576
2022-02-25 21:35:08 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.7276
2022-02-25 21:35:48 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.7489
2022-02-25 21:36:25 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 1.5746
2022-02-25 21:37:02 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.5058
2022-02-25 21:37:43 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 1.4686
2022-02-25 21:38:21 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.5927
2022-02-25 21:38:57 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.8444
2022-02-25 21:39:37 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.6420
2022-02-25 21:40:17 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.5240
2022-02-25 21:40:54 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.6224
2022-02-25 21:41:31 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 1.5918
2022-02-25 21:42:13 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.6637
2022-02-25 21:42:50 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.5588
2022-02-25 21:43:27 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.6904
2022-02-25 21:44:08 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.5968
2022-02-25 21:44:47 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.6692
2022-02-25 21:45:24 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.7653
2022-02-25 21:46:06 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.8021
2022-02-25 21:46:48 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.6665
2022-02-25 21:47:27 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.6635
2022-02-25 21:48:06 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.7257
2022-02-25 21:48:07 - train: epoch 041, train_loss: 1.6520
2022-02-25 21:49:35 - eval: epoch: 041, acc1: 66.354%, acc5: 87.034%, test_loss: 1.3716, per_image_load_time: 2.854ms, per_image_inference_time: 0.560ms
2022-02-25 21:49:35 - until epoch: 041, best_acc1: 66.476%
2022-02-25 21:49:35 - epoch 042 lr: 0.010000000000000002
2022-02-25 21:50:22 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 1.4206
2022-02-25 21:51:03 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.6534
2022-02-25 21:51:40 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.7176
2022-02-25 21:52:20 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.3190
2022-02-25 21:53:00 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.7098
2022-02-25 21:53:37 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.5377
2022-02-25 21:54:14 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.7450
2022-02-25 21:54:56 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.5171
2022-02-25 21:55:34 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.8723
2022-02-25 21:56:11 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.7847
2022-02-25 21:56:51 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.4880
2022-02-25 21:57:31 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.6195
2022-02-25 21:58:11 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.5932
2022-02-25 21:58:50 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.9380
2022-02-25 21:59:31 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.5712
2022-02-25 22:00:07 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.6555
2022-02-25 22:00:44 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.6747
2022-02-25 22:01:26 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.6236
2022-02-25 22:02:04 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.8067
2022-02-25 22:02:41 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.5403
2022-02-25 22:03:21 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 1.4009
2022-02-25 22:04:00 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.4560
2022-02-25 22:04:36 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.4784
2022-02-25 22:05:14 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.9704
2022-02-25 22:05:54 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.8474
2022-02-25 22:06:30 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.7191
2022-02-25 22:07:05 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 1.5612
2022-02-25 22:07:45 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.5635
2022-02-25 22:08:24 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.6129
2022-02-25 22:08:59 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.7151
2022-02-25 22:09:37 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.5884
2022-02-25 22:10:16 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.8156
2022-02-25 22:10:53 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.8013
2022-02-25 22:11:29 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.4499
2022-02-25 22:12:10 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.7026
2022-02-25 22:12:49 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.8361
2022-02-25 22:13:25 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.6041
2022-02-25 22:14:04 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 1.4930
2022-02-25 22:14:45 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.5871
2022-02-25 22:15:25 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.6768
2022-02-25 22:16:04 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.7724
2022-02-25 22:16:46 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.7126
2022-02-25 22:17:23 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 1.4943
2022-02-25 22:18:00 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.5543
2022-02-25 22:18:35 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.6024
2022-02-25 22:19:11 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.8791
2022-02-25 22:19:46 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.5568
2022-02-25 22:20:21 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.7995
2022-02-25 22:20:58 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.6805
2022-02-25 22:21:32 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.5536
2022-02-25 22:21:34 - train: epoch 042, train_loss: 1.6504
2022-02-25 22:22:55 - eval: epoch: 042, acc1: 66.450%, acc5: 87.256%, test_loss: 1.3684, per_image_load_time: 2.110ms, per_image_inference_time: 0.553ms
2022-02-25 22:22:55 - until epoch: 042, best_acc1: 66.476%
2022-02-25 22:22:55 - epoch 043 lr: 0.010000000000000002
2022-02-25 22:23:39 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.5972
2022-02-25 22:24:23 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.6772
2022-02-25 22:25:06 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 1.4215
2022-02-25 22:25:45 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 1.6266
2022-02-25 22:26:23 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.6454
2022-02-25 22:26:59 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.7225
2022-02-25 22:27:36 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.6791
2022-02-25 22:28:12 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.8350
2022-02-25 22:28:48 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.5859
2022-02-25 22:29:24 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.8080
2022-02-25 22:29:59 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.6690
2022-02-25 22:30:36 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.6935
2022-02-25 22:31:13 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.6626
2022-02-25 22:31:50 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 1.5640
2022-02-25 22:32:26 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.5283
2022-02-25 22:33:03 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.7044
2022-02-25 22:33:39 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.7281
2022-02-25 22:34:14 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.7890
2022-02-25 22:34:50 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.8156
2022-02-25 22:35:27 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 1.4278
2022-02-25 22:36:05 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.5738
2022-02-25 22:36:42 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.7388
2022-02-25 22:37:17 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.8389
2022-02-25 22:37:53 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 1.4903
2022-02-25 22:38:30 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.8550
2022-02-25 22:39:05 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 1.3581
2022-02-25 22:39:40 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.6781
2022-02-25 22:40:20 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.4759
2022-02-25 22:40:57 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.7137
2022-02-25 22:41:34 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.9175
2022-02-25 22:42:11 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.8622
2022-02-25 22:42:48 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.7516
2022-02-25 22:43:24 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.8493
2022-02-25 22:44:00 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.7061
2022-02-25 22:44:36 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.7412
2022-02-25 22:45:15 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.6669
2022-02-25 22:45:52 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.6219
2022-02-25 22:46:29 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.8491
2022-02-25 22:47:05 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.7639
2022-02-25 22:47:41 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.6806
2022-02-25 22:48:18 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.9671
2022-02-25 22:48:53 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.7258
2022-02-25 22:49:31 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.6488
2022-02-25 22:50:10 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.6658
2022-02-25 22:50:45 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.6308
2022-02-25 22:51:22 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.6580
2022-02-25 22:51:58 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.6544
2022-02-25 22:52:34 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.5512
2022-02-25 22:53:10 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.5280
2022-02-25 22:53:45 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.5344
2022-02-25 22:53:46 - train: epoch 043, train_loss: 1.6479
2022-02-25 22:55:10 - eval: epoch: 043, acc1: 66.496%, acc5: 87.142%, test_loss: 1.3768, per_image_load_time: 2.550ms, per_image_inference_time: 0.542ms
2022-02-25 22:55:10 - until epoch: 043, best_acc1: 66.496%
2022-02-25 22:55:10 - epoch 044 lr: 0.010000000000000002
2022-02-25 22:55:51 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.6920
2022-02-25 22:56:27 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.6489
2022-02-25 22:57:02 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.7059
2022-02-25 22:57:37 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 1.3453
2022-02-25 22:58:11 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.5617
