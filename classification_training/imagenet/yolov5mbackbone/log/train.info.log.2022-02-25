2022-02-25 22:58:48 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.4606
2022-02-25 22:59:26 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.4168
2022-02-25 23:00:01 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.5486
2022-02-25 23:00:37 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.6544
2022-02-25 23:01:13 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.6189
2022-02-25 23:01:47 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 1.5947
2022-02-25 23:02:23 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.6164
2022-02-25 23:02:58 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.6394
2022-02-25 23:03:36 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.4944
2022-02-25 23:04:12 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.5458
2022-02-25 23:04:47 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.5927
2022-02-25 23:05:23 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.5681
2022-02-25 23:05:59 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.6266
2022-02-25 23:06:34 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.7115
2022-02-25 23:07:09 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 1.6045
2022-02-25 23:07:44 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.8430
2022-02-25 23:08:22 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.7432
2022-02-25 23:08:58 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.8010
2022-02-25 23:09:35 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.8052
2022-02-25 23:10:11 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.7068
2022-02-25 23:10:46 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.7817
2022-02-25 23:11:21 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.7752
2022-02-25 23:11:56 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 1.4728
2022-02-25 23:12:31 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 1.5466
2022-02-25 23:13:09 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.6332
2022-02-25 23:13:46 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.7792
2022-02-25 23:14:21 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.3765
2022-02-25 23:14:56 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.5284
2022-02-25 23:15:32 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.8452
2022-02-25 23:16:08 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.6671
2022-02-25 23:16:43 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.7618
2022-02-25 23:17:17 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.7881
2022-02-25 23:17:56 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.4738
2022-02-25 23:18:32 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.8638
2022-02-25 23:19:08 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.8644
2022-02-25 23:19:43 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.5307
2022-02-25 23:20:19 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.6269
2022-02-25 23:20:54 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.8339
2022-02-25 23:21:29 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.4416
2022-02-25 23:22:04 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.8545
2022-02-25 23:22:43 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.5972
2022-02-25 23:23:18 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.7281
2022-02-25 23:23:53 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.8015
2022-02-25 23:24:29 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.5325
2022-02-25 23:25:03 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.6931
2022-02-25 23:25:05 - train: epoch 044, train_loss: 1.6493
2022-02-25 23:26:23 - eval: epoch: 044, acc1: 66.578%, acc5: 87.210%, test_loss: 1.3699, per_image_load_time: 2.388ms, per_image_inference_time: 0.539ms
2022-02-25 23:26:24 - until epoch: 044, best_acc1: 66.578%
2022-02-25 23:26:24 - epoch 045 lr: 0.010000000000000002
2022-02-25 23:27:05 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.4954
2022-02-25 23:27:43 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.6305
2022-02-25 23:28:20 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.6767
2022-02-25 23:28:54 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.5546
2022-02-25 23:29:30 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.8452
2022-02-25 23:30:05 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.6646
2022-02-25 23:30:40 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 1.4058
2022-02-25 23:31:16 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.6867
2022-02-25 23:31:50 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.5802
2022-02-25 23:32:28 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.6791
2022-02-25 23:33:05 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.8409
2022-02-25 23:33:40 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.7188
2022-02-25 23:34:15 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.7292
2022-02-25 23:34:52 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.6673
2022-02-25 23:35:27 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.7557
2022-02-25 23:36:01 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.6659
2022-02-25 23:36:36 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.5551
2022-02-25 23:37:15 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.4878
2022-02-25 23:37:51 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.7544
2022-02-25 23:38:26 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.7966
2022-02-25 23:39:01 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.7128
2022-02-25 23:39:37 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.7372
2022-02-25 23:40:13 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.5135
2022-02-25 23:40:47 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.7802
2022-02-25 23:41:22 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.6773
2022-02-25 23:42:01 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.6219
2022-02-25 23:42:37 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 1.5105
2022-02-25 23:43:13 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 1.6191
2022-02-25 23:43:49 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.8408
2022-02-25 23:44:24 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.8758
2022-02-25 23:44:59 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.6032
2022-02-25 23:45:34 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.7479
2022-02-25 23:46:10 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.6020
2022-02-25 23:46:47 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.3441
2022-02-25 23:47:23 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.8360
2022-02-25 23:47:59 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.7605
2022-02-25 23:48:34 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.6086
2022-02-25 23:49:10 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.4830
2022-02-25 23:49:43 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.7738
2022-02-25 23:50:19 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.7737
2022-02-25 23:50:53 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.5369
2022-02-25 23:51:33 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.8030
2022-02-25 23:52:07 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.7767
2022-02-25 23:52:43 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.8652
2022-02-25 23:53:18 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.5482
2022-02-25 23:53:54 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.8107
2022-02-25 23:54:29 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.6445
2022-02-25 23:55:04 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.4295
2022-02-25 23:55:39 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.6336
2022-02-25 23:56:16 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.5716
2022-02-25 23:56:17 - train: epoch 045, train_loss: 1.6455
2022-02-25 23:57:37 - eval: epoch: 045, acc1: 66.252%, acc5: 87.232%, test_loss: 1.3680, per_image_load_time: 2.286ms, per_image_inference_time: 0.562ms
2022-02-25 23:57:38 - until epoch: 045, best_acc1: 66.578%
2022-02-25 23:57:38 - epoch 046 lr: 0.010000000000000002
2022-02-25 23:58:19 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.4025
2022-02-25 23:58:55 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 1.4783
2022-02-25 23:59:29 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.6719
2022-02-26 00:00:05 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.8362
2022-02-26 00:00:40 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.3849
2022-02-26 00:01:18 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.6996
2022-02-26 00:01:54 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 1.5197
2022-02-26 00:02:30 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.6668
2022-02-26 00:03:05 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.6282
2022-02-26 00:03:40 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 1.4136
2022-02-26 00:04:14 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.6546
2022-02-26 00:04:50 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.6708
2022-02-26 00:05:26 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.6967
2022-02-26 00:06:04 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.8174
2022-02-26 00:06:39 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.5989
2022-02-26 00:07:15 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.6791
2022-02-26 00:07:51 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.5194
2022-02-26 00:08:26 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.5575
2022-02-26 00:09:02 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.4634
2022-02-26 00:09:37 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.6603
2022-02-26 00:10:12 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.7079
2022-02-26 00:10:49 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.4364
2022-02-26 00:11:26 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.7412
2022-02-26 00:12:01 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.6326
2022-02-26 00:12:37 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.7569
2022-02-26 00:13:12 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.8744
2022-02-26 00:13:47 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.6068
2022-02-26 00:14:23 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 1.4467
2022-02-26 00:14:57 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.6627
2022-02-26 00:15:34 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 1.4414
2022-02-26 00:16:11 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.4914
2022-02-26 00:16:46 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.8131
2022-02-26 00:17:22 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.6707
2022-02-26 00:17:57 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.6335
2022-02-26 00:18:33 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.6550
2022-02-26 00:19:08 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.5830
2022-02-26 00:19:43 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.4591
2022-02-26 00:20:19 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.8269
2022-02-26 00:20:56 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.6452
2022-02-26 00:21:32 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.6057
2022-02-26 00:22:07 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.8195
2022-02-26 00:22:43 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.4862
2022-02-26 00:23:18 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.8012
2022-02-26 00:23:53 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.6897
2022-02-26 00:24:28 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.5477
2022-02-26 00:25:06 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.7237
2022-02-26 00:25:43 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.5861
2022-02-26 00:26:19 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.6163
2022-02-26 00:26:54 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.6456
2022-02-26 00:27:29 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.6070
2022-02-26 00:27:30 - train: epoch 046, train_loss: 1.6456
2022-02-26 00:28:49 - eval: epoch: 046, acc1: 66.376%, acc5: 87.310%, test_loss: 1.3709, per_image_load_time: 2.500ms, per_image_inference_time: 0.553ms
2022-02-26 00:28:49 - until epoch: 046, best_acc1: 66.578%
2022-02-26 00:28:49 - epoch 047 lr: 0.010000000000000002
2022-02-26 00:29:30 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.5366
2022-02-26 00:30:08 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.7449
2022-02-26 00:30:44 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 1.4756
2022-02-26 00:31:20 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.4779
2022-02-26 00:31:55 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.7688
2022-02-26 00:32:31 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 1.6188
2022-02-26 00:33:05 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.7316
2022-02-26 00:33:41 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.5394
2022-02-26 00:34:16 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.8143
2022-02-26 00:34:54 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.6755
2022-02-26 00:35:29 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.7613
2022-02-26 00:36:05 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.6399
2022-02-26 00:36:41 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.8289
2022-02-26 00:37:17 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 1.5920
2022-02-26 00:37:52 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.6031
2022-02-26 00:38:26 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.5202
2022-02-26 00:39:01 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.4484
2022-02-26 00:39:39 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.5855
2022-02-26 00:40:15 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.4612
2022-02-26 00:40:52 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.6025
2022-02-26 00:41:26 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.8302
2022-02-26 00:42:02 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.7208
2022-02-26 00:42:37 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.7039
2022-02-26 00:43:11 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.4184
2022-02-26 00:43:47 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.6068
2022-02-26 00:44:25 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.8921
2022-02-26 00:45:01 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.7533
2022-02-26 00:45:36 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.8133
2022-02-26 00:46:11 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.5773
2022-02-26 00:46:47 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.7426
2022-02-26 00:47:22 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.5446
2022-02-26 00:47:57 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.9121
2022-02-26 00:48:33 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 1.4543
2022-02-26 00:49:09 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.6652
2022-02-26 00:49:45 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.8480
2022-02-26 00:50:20 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.6653
2022-02-26 00:50:56 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.8905
2022-02-26 00:51:32 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.5544
2022-02-26 00:52:06 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 1.5359
2022-02-26 00:52:42 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.6299
2022-02-26 00:53:18 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.6614
2022-02-26 00:53:55 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.6100
2022-02-26 00:54:31 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 1.5202
2022-02-26 00:55:08 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.6310
2022-02-26 00:55:43 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.8546
2022-02-26 00:56:18 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.6536
2022-02-26 00:56:52 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.7201
2022-02-26 00:57:28 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.4723
2022-02-26 00:58:02 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.7367
2022-02-26 00:58:40 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.4480
2022-02-26 00:58:41 - train: epoch 047, train_loss: 1.6452
2022-02-26 01:00:00 - eval: epoch: 047, acc1: 66.600%, acc5: 87.200%, test_loss: 1.3659, per_image_load_time: 1.931ms, per_image_inference_time: 0.565ms
2022-02-26 01:00:00 - until epoch: 047, best_acc1: 66.600%
2022-02-26 01:00:00 - epoch 048 lr: 0.010000000000000002
2022-02-26 01:00:41 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.8231
2022-02-26 01:01:16 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 2.0575
2022-02-26 01:01:51 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.6196
2022-02-26 01:02:25 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.6089
2022-02-26 01:03:04 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.4461
2022-02-26 01:03:40 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.5635
2022-02-26 01:04:16 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.5630
2022-02-26 01:04:52 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.6564
2022-02-26 01:05:27 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.8168
2022-02-26 01:06:03 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.7054
2022-02-26 01:06:37 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.7857
2022-02-26 01:07:12 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.6694
2022-02-26 01:07:49 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 1.4535
2022-02-26 01:08:26 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.6498
2022-02-26 01:09:02 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.7226
2022-02-26 01:09:38 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.6020
2022-02-26 01:10:13 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.7939
2022-02-26 01:10:49 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.6785
2022-02-26 01:11:24 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.7593
2022-02-26 01:11:58 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.9058
2022-02-26 01:12:36 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.6697
2022-02-26 01:13:13 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.8172
2022-02-26 01:13:47 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.4704
2022-02-26 01:14:24 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.7890
2022-02-26 01:14:59 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.7308
2022-02-26 01:15:35 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.8854
2022-02-26 01:16:09 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.8556
2022-02-26 01:16:44 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.6669
2022-02-26 01:17:21 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.7525
2022-02-26 01:17:59 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.6480
2022-02-26 01:18:34 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.5578
2022-02-26 01:19:09 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.4972
2022-02-26 01:19:44 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.7636
2022-02-26 01:20:20 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.5812
2022-02-26 01:20:55 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.8831
2022-02-26 01:21:30 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.6674
2022-02-26 01:22:07 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.6879
2022-02-26 01:22:44 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.5049
2022-02-26 01:23:19 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.7250
2022-02-26 01:23:55 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.4131
2022-02-26 01:24:31 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.8608
2022-02-26 01:25:06 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.5615
2022-02-26 01:25:41 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.6494
2022-02-26 01:26:15 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.5106
2022-02-26 01:26:52 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.6296
2022-02-26 01:27:29 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.6389
2022-02-26 01:28:06 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.6827
2022-02-26 01:28:40 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.9789
2022-02-26 01:29:16 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.7473
2022-02-26 01:29:50 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.7257
2022-02-26 01:29:52 - train: epoch 048, train_loss: 1.6468
2022-02-26 01:31:10 - eval: epoch: 048, acc1: 66.470%, acc5: 87.214%, test_loss: 1.3749, per_image_load_time: 2.138ms, per_image_inference_time: 0.563ms
2022-02-26 01:31:10 - until epoch: 048, best_acc1: 66.600%
2022-02-26 01:31:10 - epoch 049 lr: 0.010000000000000002
2022-02-26 01:31:55 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.6823
2022-02-26 01:32:30 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.6703
2022-02-26 01:33:05 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.6633
2022-02-26 01:33:41 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.7519
2022-02-26 01:34:15 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.5098
2022-02-26 01:34:52 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.5177
2022-02-26 01:35:26 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.6298
2022-02-26 01:36:01 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.9887
2022-02-26 01:36:39 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 1.5003
2022-02-26 01:37:15 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.6523
2022-02-26 01:37:51 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 1.5283
2022-02-26 01:38:27 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.4084
2022-02-26 01:39:02 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.7899
2022-02-26 01:39:37 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.7536
2022-02-26 01:40:11 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.4996
2022-02-26 01:40:47 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.8622
2022-02-26 01:41:26 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.6411
2022-02-26 01:42:02 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.5611
2022-02-26 01:42:37 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.5646
2022-02-26 01:43:13 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.5565
2022-02-26 01:43:48 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.5698
2022-02-26 01:44:23 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.6403
2022-02-26 01:44:58 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 1.5874
2022-02-26 01:45:33 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.7726
2022-02-26 01:46:11 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.7566
2022-02-26 01:46:47 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.6337
2022-02-26 01:47:23 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.4769
2022-02-26 01:47:58 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.5546
2022-02-26 01:48:34 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.7516
2022-02-26 01:49:09 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.6195
2022-02-26 01:49:44 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.7283
2022-02-26 01:50:20 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.7797
2022-02-26 01:50:58 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.6981
2022-02-26 01:51:34 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.7897
2022-02-26 01:52:09 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.6750
2022-02-26 01:52:45 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.7676
2022-02-26 01:53:21 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.6886
2022-02-26 01:53:56 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.7504
2022-02-26 01:54:31 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.9084
2022-02-26 01:55:05 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 1.6879
2022-02-26 01:55:43 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.6131
2022-02-26 01:56:19 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.7105
2022-02-26 01:56:55 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.9505
2022-02-26 01:57:30 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.7242
2022-02-26 01:58:05 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.6337
2022-02-26 01:58:40 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.7586
2022-02-26 01:59:15 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.7958
2022-02-26 01:59:51 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.4037
2022-02-26 02:00:28 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.5403
2022-02-26 02:01:03 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.5894
2022-02-26 02:01:04 - train: epoch 049, train_loss: 1.6396
2022-02-26 02:02:23 - eval: epoch: 049, acc1: 66.246%, acc5: 87.188%, test_loss: 1.3771, per_image_load_time: 2.533ms, per_image_inference_time: 0.566ms
2022-02-26 02:02:24 - until epoch: 049, best_acc1: 66.600%
2022-02-26 02:02:24 - epoch 050 lr: 0.010000000000000002
2022-02-26 02:03:05 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.8261
2022-02-26 02:03:40 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.5699
2022-02-26 02:04:15 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.5853
2022-02-26 02:04:50 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.5119
2022-02-26 02:05:28 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.6365
2022-02-26 02:06:05 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.7790
2022-02-26 02:06:41 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 1.4222
2022-02-26 02:07:16 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 1.4430
2022-02-26 02:07:52 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.4367
2022-02-26 02:08:27 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 1.7631
2022-02-26 02:09:01 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.7855
2022-02-26 02:09:38 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.6642
2022-02-26 02:10:15 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.4842
2022-02-26 02:10:50 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.5648
2022-02-26 02:11:26 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.5935
2022-02-26 02:12:01 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 1.5598
2022-02-26 02:12:37 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.7820
2022-02-26 02:13:12 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.7022
2022-02-26 02:13:47 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.6009
2022-02-26 02:14:24 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 1.6268
2022-02-26 02:15:01 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.3557
2022-02-26 02:15:37 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.7240
2022-02-26 02:16:13 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 1.6527
2022-02-26 02:16:48 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.6412
2022-02-26 02:17:24 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.7087
2022-02-26 02:17:59 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.6309
2022-02-26 02:18:34 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.7198
2022-02-26 02:19:11 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.8358
2022-02-26 02:19:49 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.8277
2022-02-26 02:20:26 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.7168
2022-02-26 02:21:03 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.4916
2022-02-26 02:21:39 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.6122
2022-02-26 02:22:16 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.6045
2022-02-26 02:22:51 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.5071
2022-02-26 02:23:27 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.6903
2022-02-26 02:24:07 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.6110
2022-02-26 02:24:43 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.7794
2022-02-26 02:25:19 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.4252
2022-02-26 02:25:55 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 1.4459
2022-02-26 02:26:30 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.8094
2022-02-26 02:27:04 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.5286
2022-02-26 02:27:40 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.7498
2022-02-26 02:28:14 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.5621
2022-02-26 02:28:52 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 1.6061
2022-02-26 02:29:29 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.5467
2022-02-26 02:30:03 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.7073
2022-02-26 02:30:38 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.6876
2022-02-26 02:31:15 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.4903
2022-02-26 02:31:50 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.4563
2022-02-26 02:32:25 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.5625
2022-02-26 02:32:26 - train: epoch 050, train_loss: 1.6380
2022-02-26 02:33:48 - eval: epoch: 050, acc1: 66.440%, acc5: 87.256%, test_loss: 1.3693, per_image_load_time: 2.314ms, per_image_inference_time: 0.551ms
2022-02-26 02:33:48 - until epoch: 050, best_acc1: 66.600%
2022-02-26 02:33:48 - epoch 051 lr: 0.010000000000000002
2022-02-26 02:34:30 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.8328
2022-02-26 02:35:05 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.9786
2022-02-26 02:35:40 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.7260
2022-02-26 02:36:16 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.3551
2022-02-26 02:36:50 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 1.7417
2022-02-26 02:37:25 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 1.5384
2022-02-26 02:38:01 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 1.5350
2022-02-26 02:38:39 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.8675
2022-02-26 02:39:16 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.5384
2022-02-26 02:39:51 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 2.0091
2022-02-26 02:40:26 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.7656
2022-02-26 02:41:02 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 1.7147
2022-02-26 02:41:38 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 1.3359
2022-02-26 02:42:13 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.5197
2022-02-26 02:42:49 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.5848
2022-02-26 02:43:25 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 1.5062
2022-02-26 02:44:01 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.7653
2022-02-26 02:44:37 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.6898
2022-02-26 02:45:13 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.4222
2022-02-26 02:45:49 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.6750
2022-02-26 02:46:24 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 1.6626
2022-02-26 02:46:59 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.5989
2022-02-26 02:47:35 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.7545
2022-02-26 02:48:13 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.7152
2022-02-26 02:48:48 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 1.5156
2022-02-26 02:49:23 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.4362
2022-02-26 02:49:59 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.7492
2022-02-26 02:50:35 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.5966
2022-02-26 02:51:10 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.6270
2022-02-26 02:51:45 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.5438
2022-02-26 02:52:22 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.5788
2022-02-26 02:52:59 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.5961
2022-02-26 02:53:36 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.6595
2022-02-26 02:54:11 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.6230
2022-02-26 02:54:46 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.5723
2022-02-26 02:55:22 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.5628
2022-02-26 02:55:57 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.7240
2022-02-26 02:56:32 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.5736
2022-02-26 02:57:09 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.6719
2022-02-26 02:57:45 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.6535
2022-02-26 02:58:22 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.6670
2022-02-26 02:58:58 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.9225
2022-02-26 02:59:33 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.5519
2022-02-26 03:00:08 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.6585
2022-02-26 03:00:43 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.3766
2022-02-26 03:01:18 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.6964
2022-02-26 03:01:55 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.6534
2022-02-26 03:02:32 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.5822
2022-02-26 03:03:08 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.8192
2022-02-26 03:03:42 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.6038
2022-02-26 03:03:44 - train: epoch 051, train_loss: 1.6375
2022-02-26 03:05:03 - eval: epoch: 051, acc1: 66.402%, acc5: 87.276%, test_loss: 1.3651, per_image_load_time: 1.395ms, per_image_inference_time: 0.568ms
2022-02-26 03:05:03 - until epoch: 051, best_acc1: 66.600%
2022-02-26 03:05:03 - epoch 052 lr: 0.010000000000000002
2022-02-26 03:05:44 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.4864
2022-02-26 03:06:20 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.6780
2022-02-26 03:06:58 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.6808
2022-02-26 03:07:33 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.6940
2022-02-26 03:08:09 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.6675
2022-02-26 03:08:45 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.6074
2022-02-26 03:09:21 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.7849
2022-02-26 03:09:56 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.6686
2022-02-26 03:10:31 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.6953
2022-02-26 03:11:06 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.9101
2022-02-26 03:11:44 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.5680
2022-02-26 03:12:20 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.4956
2022-02-26 03:12:56 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 1.5357
2022-02-26 03:13:31 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.9709
2022-02-26 03:14:06 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.5206
2022-02-26 03:14:42 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.4886
2022-02-26 03:15:16 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 1.5115
2022-02-26 03:15:52 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.4392
2022-02-26 03:16:29 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.6538
2022-02-26 03:17:05 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.7480
2022-02-26 03:17:39 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.4502
2022-02-26 03:18:15 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.6077
2022-02-26 03:18:52 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.3636
2022-02-26 03:19:27 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 1.4642
2022-02-26 03:20:00 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.3827
2022-02-26 03:20:38 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 1.4975
2022-02-26 03:21:14 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.6263
2022-02-26 03:21:50 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.6703
2022-02-26 03:22:25 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.4402
2022-02-26 03:23:01 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.5871
2022-02-26 03:23:37 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.7163
2022-02-26 03:24:12 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.7594
2022-02-26 03:24:46 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.5768
2022-02-26 03:25:22 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.6796
2022-02-26 03:26:00 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.7454
2022-02-26 03:26:36 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.6997
2022-02-26 03:27:12 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.8769
2022-02-26 03:27:48 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.5127
2022-02-26 03:28:23 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 1.5819
2022-02-26 03:28:58 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.8614
2022-02-26 03:29:32 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.3556
2022-02-26 03:30:08 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.5181
2022-02-26 03:30:47 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.6107
2022-02-26 03:31:22 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.7320
2022-02-26 03:31:57 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.4954
2022-02-26 03:32:33 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.7581
2022-02-26 03:33:08 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.6486
2022-02-26 03:33:45 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 1.3836
2022-02-26 03:34:19 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.5129
2022-02-26 03:34:53 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.6929
2022-02-26 03:34:55 - train: epoch 052, train_loss: 1.6346
2022-02-26 03:36:16 - eval: epoch: 052, acc1: 66.134%, acc5: 87.226%, test_loss: 1.3724, per_image_load_time: 1.700ms, per_image_inference_time: 0.556ms
2022-02-26 03:36:16 - until epoch: 052, best_acc1: 66.600%
2022-02-26 03:36:16 - epoch 053 lr: 0.010000000000000002
2022-02-26 03:36:57 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.6469
2022-02-26 03:37:33 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.9231
2022-02-26 03:38:09 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.6405
2022-02-26 03:38:45 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.7244
2022-02-26 03:39:19 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.8034
2022-02-26 03:39:54 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.5886
2022-02-26 03:40:33 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.5125
2022-02-26 03:41:08 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.7932
2022-02-26 03:41:44 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.5514
2022-02-26 03:42:20 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.6470
2022-02-26 03:42:55 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.5746
2022-02-26 03:43:30 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 1.6258
2022-02-26 03:44:05 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.7038
2022-02-26 03:44:40 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.9537
2022-02-26 03:45:19 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.6364
2022-02-26 03:45:54 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.8223
2022-02-26 03:46:30 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.9059
2022-02-26 03:47:05 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.6882
2022-02-26 03:47:42 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 1.4361
2022-02-26 03:48:17 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 1.7135
2022-02-26 03:48:50 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.6545
2022-02-26 03:49:26 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.5566
2022-02-26 03:50:04 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.6640
2022-02-26 03:50:40 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.5941
2022-02-26 03:51:16 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.8824
2022-02-26 03:51:52 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.6550
2022-02-26 03:52:28 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.8807
2022-02-26 03:53:03 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.8201
2022-02-26 03:53:37 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.4630
2022-02-26 03:54:13 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.5180
2022-02-26 03:54:51 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.7623
2022-02-26 03:55:27 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.8890
2022-02-26 03:56:03 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.6105
2022-02-26 03:56:38 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.7628
2022-02-26 03:57:14 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.5850
2022-02-26 03:57:49 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.6791
2022-02-26 03:58:23 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.7911
2022-02-26 03:59:01 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.6007
2022-02-26 03:59:37 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.7849
2022-02-26 04:00:13 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.6360
2022-02-26 04:00:49 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.6867
2022-02-26 04:01:25 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.6123
2022-02-26 04:02:00 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.8545
2022-02-26 04:02:36 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.4970
2022-02-26 04:03:10 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.7446
2022-02-26 04:03:48 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.5727
2022-02-26 04:04:24 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.7428
2022-02-26 04:05:00 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.8002
2022-02-26 04:05:36 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.5590
2022-02-26 04:06:11 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.6633
2022-02-26 04:06:12 - train: epoch 053, train_loss: 1.6314
2022-02-26 04:07:31 - eval: epoch: 053, acc1: 66.626%, acc5: 87.536%, test_loss: 1.3529, per_image_load_time: 2.373ms, per_image_inference_time: 0.590ms
2022-02-26 04:07:31 - until epoch: 053, best_acc1: 66.626%
2022-02-26 04:07:31 - epoch 054 lr: 0.010000000000000002
2022-02-26 04:08:12 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.4431
2022-02-26 04:08:50 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.9044
2022-02-26 04:09:26 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.6781
2022-02-26 04:10:01 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.4628
2022-02-26 04:10:36 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.6499
2022-02-26 04:11:11 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.4325
2022-02-26 04:11:46 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.6768
2022-02-26 04:12:21 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.6960
2022-02-26 04:12:57 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 1.4041
2022-02-26 04:13:34 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 1.4300
2022-02-26 04:14:10 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 1.3921
2022-02-26 04:14:47 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.7530
2022-02-26 04:15:22 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.5261
2022-02-26 04:15:57 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.6995
2022-02-26 04:16:32 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.5977
2022-02-26 04:17:07 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 1.2619
2022-02-26 04:17:42 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.6183
2022-02-26 04:18:20 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.5744
2022-02-26 04:18:56 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 2.0133
2022-02-26 04:19:32 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.7040
2022-02-26 04:20:08 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.3651
2022-02-26 04:20:43 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.6962
2022-02-26 04:21:18 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 1.6062
2022-02-26 04:21:53 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.5734
2022-02-26 04:22:29 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.7054
2022-02-26 04:23:05 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.6314
2022-02-26 04:23:41 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.7134
2022-02-26 04:24:16 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.9525
2022-02-26 04:24:52 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.4495
2022-02-26 04:25:27 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.6958
2022-02-26 04:26:01 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.6716
2022-02-26 04:26:36 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.8003
2022-02-26 04:27:12 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.6339
2022-02-26 04:27:50 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.7314
2022-02-26 04:28:25 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.7597
2022-02-26 04:29:01 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.6312
2022-02-26 04:29:35 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.5214
2022-02-26 04:30:12 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.7097
2022-02-26 04:30:47 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.5751
2022-02-26 04:31:22 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.5018
2022-02-26 04:31:57 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.6802
2022-02-26 04:32:35 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.5836
2022-02-26 04:33:10 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.6678
2022-02-26 04:33:47 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.4586
2022-02-26 04:34:22 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.5637
2022-02-26 04:34:58 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.6806
2022-02-26 04:35:32 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.8374
2022-02-26 04:36:07 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.8205
2022-02-26 04:36:43 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.4737
2022-02-26 04:37:20 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.6312
2022-02-26 04:37:22 - train: epoch 054, train_loss: 1.6316
2022-02-26 04:38:42 - eval: epoch: 054, acc1: 66.416%, acc5: 87.474%, test_loss: 1.3625, per_image_load_time: 1.840ms, per_image_inference_time: 0.590ms
2022-02-26 04:38:42 - until epoch: 054, best_acc1: 66.626%
2022-02-26 04:38:42 - epoch 055 lr: 0.010000000000000002
2022-02-26 04:39:23 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 1.5684
2022-02-26 04:39:58 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.6147
2022-02-26 04:40:34 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.4342
2022-02-26 04:41:10 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 1.6133
2022-02-26 04:41:47 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 1.4464
2022-02-26 04:42:23 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.4787
2022-02-26 04:42:59 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.7131
2022-02-26 04:43:34 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.4507
2022-02-26 04:44:11 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.6348
2022-02-26 04:44:45 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.5688
2022-02-26 04:45:20 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 1.6332
2022-02-26 04:45:55 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.6674
2022-02-26 04:46:33 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.7972
2022-02-26 04:47:09 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 1.5515
2022-02-26 04:47:43 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 1.6886
2022-02-26 04:48:20 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.7382
2022-02-26 04:48:55 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.7223
2022-02-26 04:49:30 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.7943
2022-02-26 04:50:05 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.7692
2022-02-26 04:50:40 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.6118
2022-02-26 04:51:18 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 1.4057
2022-02-26 04:51:53 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.7664
2022-02-26 04:52:29 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.5788
2022-02-26 04:53:05 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 1.4598
2022-02-26 04:53:40 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.6549
2022-02-26 04:54:15 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 1.5607
2022-02-26 04:54:49 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.5177
2022-02-26 04:55:24 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.5925
2022-02-26 04:56:02 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.7746
2022-02-26 04:56:37 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.5391
2022-02-26 04:57:13 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.6907
2022-02-26 04:57:48 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.6438
2022-02-26 04:58:24 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 1.4178
2022-02-26 04:59:00 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.4489
2022-02-26 04:59:34 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 1.3660
2022-02-26 05:00:09 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.7388
2022-02-26 05:00:47 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 1.6567
2022-02-26 05:01:23 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.7222
2022-02-26 05:01:58 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.9947
2022-02-26 05:02:33 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.5187
2022-02-26 05:03:08 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.5558
2022-02-26 05:03:44 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.6703
2022-02-26 05:04:19 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.6898
2022-02-26 05:04:54 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.8306
2022-02-26 05:05:31 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.6506
2022-02-26 05:06:08 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.6679
2022-02-26 05:06:43 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 1.4283
2022-02-26 05:07:19 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.7226
2022-02-26 05:07:54 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.5821
2022-02-26 05:08:29 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.7079
2022-02-26 05:08:30 - train: epoch 055, train_loss: 1.6265
2022-02-26 05:09:49 - eval: epoch: 055, acc1: 66.542%, acc5: 87.382%, test_loss: 1.3608, per_image_load_time: 2.397ms, per_image_inference_time: 0.580ms
2022-02-26 05:09:49 - until epoch: 055, best_acc1: 66.626%
2022-02-26 05:09:49 - epoch 056 lr: 0.010000000000000002
2022-02-26 05:10:33 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.6114
2022-02-26 05:11:08 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 1.7680
2022-02-26 05:11:44 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 1.5651
2022-02-26 05:12:20 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.5787
2022-02-26 05:12:57 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.6226
2022-02-26 05:13:32 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.6482
2022-02-26 05:14:05 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.7413
2022-02-26 05:14:41 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.8755
2022-02-26 05:15:20 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.6927
2022-02-26 05:15:55 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.5957
2022-02-26 05:16:31 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.4827
2022-02-26 05:17:06 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.5478
2022-02-26 05:17:41 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.7077
2022-02-26 05:18:17 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 1.6003
2022-02-26 05:18:52 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.9284
2022-02-26 05:19:27 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 1.5064
2022-02-26 05:20:03 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.7286
2022-02-26 05:20:39 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.8647
2022-02-26 05:21:16 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.6680
2022-02-26 05:21:51 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.5293
2022-02-26 05:22:26 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.6179
2022-02-26 05:23:03 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.7531
2022-02-26 05:23:36 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.6818
2022-02-26 05:24:12 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.5809
2022-02-26 05:24:48 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.7294
2022-02-26 05:25:25 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.5438
2022-02-26 05:26:01 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 1.6196
2022-02-26 05:26:37 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.5433
2022-02-26 05:27:13 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.7174
2022-02-26 05:27:48 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.7949
2022-02-26 05:28:24 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 1.5408
2022-02-26 05:28:58 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.4480
2022-02-26 05:29:35 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.9056
2022-02-26 05:30:12 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.5048
2022-02-26 05:30:48 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.6331
2022-02-26 05:31:23 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.3758
2022-02-26 05:31:58 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.5785
2022-02-26 05:32:34 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.4617
2022-02-26 05:33:09 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.7945
2022-02-26 05:33:45 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.5415
2022-02-26 05:34:23 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.8647
2022-02-26 05:34:58 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.5879
2022-02-26 05:35:34 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.5124
2022-02-26 05:36:08 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.7358
2022-02-26 05:36:44 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.6757
2022-02-26 05:37:20 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.6958
2022-02-26 05:37:55 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.6959
2022-02-26 05:38:30 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.8663
2022-02-26 05:39:07 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.6510
2022-02-26 05:39:44 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.7001
2022-02-26 05:39:45 - train: epoch 056, train_loss: 1.6235
2022-02-26 05:41:05 - eval: epoch: 056, acc1: 66.398%, acc5: 87.508%, test_loss: 1.3625, per_image_load_time: 2.450ms, per_image_inference_time: 0.589ms
2022-02-26 05:41:06 - until epoch: 056, best_acc1: 66.626%
2022-02-26 05:41:06 - epoch 057 lr: 0.010000000000000002
2022-02-26 05:41:47 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.7216
2022-02-26 05:42:22 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.6258
2022-02-26 05:42:57 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.5274
2022-02-26 05:43:33 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.5399
2022-02-26 05:44:11 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 1.4223
2022-02-26 05:44:46 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.7891
2022-02-26 05:45:22 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 1.2852
2022-02-26 05:45:58 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.6351
2022-02-26 05:46:34 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.6614
2022-02-26 05:47:09 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 1.5387
2022-02-26 05:47:43 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 1.5542
2022-02-26 05:48:19 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.5502
2022-02-26 05:48:57 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.6503
2022-02-26 05:49:34 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.7199
2022-02-26 05:50:09 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.7597
2022-02-26 05:50:44 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.7401
2022-02-26 05:51:20 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.7761
2022-02-26 05:51:55 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.7119
2022-02-26 05:52:30 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.5106
2022-02-26 05:53:07 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.6139
2022-02-26 05:53:43 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.6636
2022-02-26 05:54:20 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 1.7386
2022-02-26 05:54:55 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.6511
2022-02-26 05:55:30 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.4949
2022-02-26 05:56:06 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.5437
2022-02-26 05:56:41 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.4862
2022-02-26 05:57:16 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 1.3783
2022-02-26 05:57:52 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 1.2960
2022-02-26 05:58:31 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.7409
2022-02-26 05:59:05 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.6816
2022-02-26 05:59:41 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.7924
2022-02-26 06:00:17 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.6936
2022-02-26 06:00:53 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.7063
2022-02-26 06:01:27 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.6767
2022-02-26 06:02:02 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.6909
2022-02-26 06:02:39 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.4991
2022-02-26 06:03:17 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.5931
2022-02-26 06:03:53 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.4871
2022-02-26 06:04:29 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.7122
2022-02-26 06:05:04 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.5652
2022-02-26 06:05:40 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.6785
2022-02-26 06:06:14 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.7251
2022-02-26 06:06:49 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.5546
2022-02-26 06:07:27 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.6327
2022-02-26 06:08:04 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.7247
2022-02-26 06:08:40 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.7697
2022-02-26 06:09:16 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.5447
2022-02-26 06:09:51 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.8126
2022-02-26 06:10:26 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.8113
2022-02-26 06:11:00 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.7764
2022-02-26 06:11:01 - train: epoch 057, train_loss: 1.6227
2022-02-26 06:12:22 - eval: epoch: 057, acc1: 66.622%, acc5: 87.336%, test_loss: 1.3617, per_image_load_time: 2.456ms, per_image_inference_time: 0.583ms
2022-02-26 06:12:22 - until epoch: 057, best_acc1: 66.626%
2022-02-26 06:12:22 - epoch 058 lr: 0.010000000000000002
2022-02-26 06:13:03 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.7027
2022-02-26 06:13:39 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.3980
2022-02-26 06:14:15 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.6616
2022-02-26 06:14:51 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 1.6995
2022-02-26 06:15:25 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.4496
2022-02-26 06:16:00 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.7463
2022-02-26 06:16:36 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.6235
2022-02-26 06:17:14 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.5048
2022-02-26 06:17:50 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.6068
2022-02-26 06:18:26 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.7607
2022-02-26 06:19:01 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.4159
2022-02-26 06:19:37 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.5261
2022-02-26 06:20:13 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.7337
2022-02-26 06:20:48 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.5936
2022-02-26 06:21:23 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.5862
2022-02-26 06:22:01 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.4977
2022-02-26 06:22:36 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.8092
2022-02-26 06:23:12 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.6590
2022-02-26 06:23:48 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.7245
2022-02-26 06:24:23 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.7242
2022-02-26 06:24:58 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.5556
2022-02-26 06:25:33 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.3528
2022-02-26 06:26:09 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.5891
2022-02-26 06:26:46 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.7344
2022-02-26 06:27:22 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.8059
2022-02-26 06:27:58 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.4797
2022-02-26 06:28:33 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.8184
2022-02-26 06:29:07 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.3985
2022-02-26 06:29:44 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.5453
2022-02-26 06:30:18 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.8120
2022-02-26 06:30:54 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.5054
2022-02-26 06:31:31 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 1.5400
2022-02-26 06:32:07 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.6323
2022-02-26 06:32:43 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.4639
2022-02-26 06:33:18 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.5079
2022-02-26 06:33:54 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.5376
2022-02-26 06:34:29 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.6104
2022-02-26 06:35:04 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.6399
2022-02-26 06:35:39 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.7481
2022-02-26 06:36:18 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.7438
2022-02-26 06:36:54 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.7494
2022-02-26 06:37:29 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.3678
2022-02-26 06:38:04 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.8685
2022-02-26 06:38:39 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.4123
2022-02-26 06:39:15 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.7454
2022-02-26 06:39:50 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.5411
2022-02-26 06:40:25 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.7163
2022-02-26 06:41:03 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 1.7283
2022-02-26 06:41:37 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.5112
2022-02-26 06:42:13 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 1.6062
2022-02-26 06:42:15 - train: epoch 058, train_loss: 1.6182
2022-02-26 06:43:35 - eval: epoch: 058, acc1: 66.348%, acc5: 87.292%, test_loss: 1.3622, per_image_load_time: 2.359ms, per_image_inference_time: 0.593ms
2022-02-26 06:43:35 - until epoch: 058, best_acc1: 66.626%
2022-02-26 06:43:35 - epoch 059 lr: 0.010000000000000002
2022-02-26 06:44:15 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.7754
2022-02-26 06:44:51 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.5075
2022-02-26 06:45:27 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 1.5529
2022-02-26 06:46:04 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.7096
2022-02-26 06:46:40 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.7518
2022-02-26 06:47:16 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.5453
2022-02-26 06:47:51 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.4723
2022-02-26 06:48:26 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.6019
2022-02-26 06:49:01 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.5251
2022-02-26 06:49:36 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.5946
2022-02-26 06:50:13 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.8005
2022-02-26 06:50:51 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.3947
2022-02-26 06:51:27 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.8396
2022-02-26 06:52:03 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.7631
2022-02-26 06:52:38 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.8595
2022-02-26 06:53:13 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.5234
2022-02-26 06:53:48 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 1.6915
2022-02-26 06:54:23 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.6044
2022-02-26 06:55:01 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.7162
2022-02-26 06:55:36 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 1.6101
2022-02-26 06:56:12 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.8824
2022-02-26 06:56:47 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.7451
2022-02-26 06:57:22 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.5581
2022-02-26 06:57:58 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.6377
2022-02-26 06:58:33 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.6657
2022-02-26 06:59:08 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.6070
2022-02-26 06:59:42 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.5152
2022-02-26 07:00:22 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.7870
2022-02-26 07:00:56 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 1.6067
2022-02-26 07:01:32 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 2.0043
2022-02-26 07:02:08 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 1.5045
2022-02-26 07:02:43 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.5826
2022-02-26 07:03:19 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.5956
2022-02-26 07:03:54 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 2.0347
2022-02-26 07:04:29 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 1.4363
2022-02-26 07:05:07 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.6083
2022-02-26 07:05:43 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.4606
2022-02-26 07:06:18 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.5709
2022-02-26 07:06:55 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.4840
2022-02-26 07:07:30 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.8882
2022-02-26 07:08:06 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.6997
2022-02-26 07:08:41 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.6433
2022-02-26 07:09:16 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.7891
2022-02-26 07:09:54 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.7829
2022-02-26 07:10:30 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.6501
2022-02-26 07:11:05 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.6702
2022-02-26 07:11:41 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.5619
2022-02-26 07:12:16 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.5946
2022-02-26 07:12:51 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.8716
2022-02-26 07:13:25 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.7836
2022-02-26 07:13:26 - train: epoch 059, train_loss: 1.6170
2022-02-26 07:14:49 - eval: epoch: 059, acc1: 66.856%, acc5: 87.544%, test_loss: 1.3560, per_image_load_time: 2.602ms, per_image_inference_time: 0.565ms
2022-02-26 07:14:49 - until epoch: 059, best_acc1: 66.856%
2022-02-26 07:14:49 - epoch 060 lr: 0.010000000000000002
2022-02-26 07:15:30 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 1.5621
2022-02-26 07:16:05 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 1.6399
2022-02-26 07:16:42 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.5510
2022-02-26 07:17:16 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.6550
2022-02-26 07:17:52 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.7514
2022-02-26 07:18:27 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.7473
2022-02-26 07:19:04 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.6878
2022-02-26 07:19:40 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.8897
2022-02-26 07:20:16 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.4271
2022-02-26 07:20:51 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 1.3405
2022-02-26 07:21:28 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.5179
2022-02-26 07:22:03 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.6179
2022-02-26 07:22:38 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.5092
2022-02-26 07:23:13 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.6421
2022-02-26 07:23:49 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.6647
2022-02-26 07:24:26 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 1.6037
2022-02-26 07:25:03 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.6138
2022-02-26 07:25:38 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.5838
2022-02-26 07:26:14 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.7139
2022-02-26 07:26:49 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 1.5347
2022-02-26 07:27:24 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.6748
2022-02-26 07:27:59 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.7167
2022-02-26 07:28:37 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.4761
2022-02-26 07:29:13 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 1.5798
2022-02-26 07:29:48 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.5524
2022-02-26 07:30:25 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.7385
2022-02-26 07:30:59 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.5470
2022-02-26 07:31:35 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.5255
2022-02-26 07:32:10 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.7144
2022-02-26 07:32:45 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.9110
2022-02-26 07:33:23 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.6911
2022-02-26 07:33:58 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.5682
2022-02-26 07:34:34 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 1.4789
2022-02-26 07:35:10 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.6611
2022-02-26 07:35:45 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.7043
2022-02-26 07:36:21 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.6375
2022-02-26 07:36:55 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.5371
2022-02-26 07:37:30 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.6412
2022-02-26 07:38:08 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.9010
2022-02-26 07:38:45 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.6258
2022-02-26 07:39:20 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.7825
2022-02-26 07:39:56 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.8158
2022-02-26 07:40:31 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.5702
2022-02-26 07:41:06 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.7793
2022-02-26 07:41:41 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.5941
2022-02-26 07:42:15 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.4884
2022-02-26 07:42:55 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.6066
2022-02-26 07:43:30 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.4685
2022-02-26 07:44:05 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.5713
2022-02-26 07:44:40 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.6308
2022-02-26 07:44:41 - train: epoch 060, train_loss: 1.6140
2022-02-26 07:46:01 - eval: epoch: 060, acc1: 66.582%, acc5: 87.596%, test_loss: 1.3522, per_image_load_time: 2.383ms, per_image_inference_time: 0.586ms
2022-02-26 07:46:01 - until epoch: 060, best_acc1: 66.856%
2022-02-26 07:46:01 - epoch 061 lr: 0.0010000000000000002
2022-02-26 07:46:41 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 1.3417
2022-02-26 07:47:18 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 1.5442
2022-02-26 07:47:55 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 1.3481
2022-02-26 07:48:31 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.6769
2022-02-26 07:49:07 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 1.4249
2022-02-26 07:49:42 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 1.5191
2022-02-26 07:50:18 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 1.2841
2022-02-26 07:50:53 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 1.6303
2022-02-26 07:51:27 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 1.4229
2022-02-26 07:52:06 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 1.2899
2022-02-26 07:52:43 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.3205
2022-02-26 07:53:18 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 1.3108
2022-02-26 07:53:54 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 1.3964
2022-02-26 07:54:29 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 1.2905
2022-02-26 07:55:04 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 1.5248
2022-02-26 07:55:39 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 1.2722
2022-02-26 07:56:14 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 1.5130
2022-02-26 07:56:52 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.4486
2022-02-26 07:57:28 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 1.6397
2022-02-26 07:58:03 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 1.3435
2022-02-26 07:58:39 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.8145
2022-02-26 07:59:15 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 1.2913
2022-02-26 07:59:51 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 1.3806
2022-02-26 08:00:25 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 1.2131
2022-02-26 08:01:01 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 1.3788
2022-02-26 08:01:37 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 1.5113
2022-02-26 08:02:13 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 1.6083
2022-02-26 08:02:49 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.4612
2022-02-26 08:03:25 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.5689
2022-02-26 08:04:01 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 1.3533
2022-02-26 08:04:37 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 1.5206
2022-02-26 08:05:12 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 1.4342
2022-02-26 08:05:46 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 1.4390
2022-02-26 08:06:22 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 1.2807
2022-02-26 08:07:00 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 1.4304
2022-02-26 08:07:36 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 1.4327
2022-02-26 08:08:11 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 1.5906
2022-02-26 08:08:47 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 1.4374
2022-02-26 08:09:23 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.3509
2022-02-26 08:09:57 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.4362
2022-02-26 08:10:33 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.6566
2022-02-26 08:11:09 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 1.2905
2022-02-26 08:11:47 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 1.4220
2022-02-26 08:12:22 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 1.4311
2022-02-26 08:12:58 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 1.4819
2022-02-26 08:13:34 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 1.4995
2022-02-26 08:14:10 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 1.3200
2022-02-26 08:14:44 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 1.3993
2022-02-26 08:15:19 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 1.4551
2022-02-26 08:15:54 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 1.4883
2022-02-26 08:15:55 - train: epoch 061, train_loss: 1.4381
2022-02-26 08:17:15 - eval: epoch: 061, acc1: 70.100%, acc5: 89.450%, test_loss: 1.2059, per_image_load_time: 1.504ms, per_image_inference_time: 0.579ms
2022-02-26 08:17:16 - until epoch: 061, best_acc1: 70.100%
2022-02-26 08:17:16 - epoch 062 lr: 0.0010000000000000002
2022-02-26 08:17:56 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.4159
2022-02-26 08:18:33 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.4628
2022-02-26 08:19:08 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 1.3460
2022-02-26 08:19:43 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 1.3583
2022-02-26 08:20:18 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 1.2963
2022-02-26 08:20:56 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 1.2907
2022-02-26 08:21:31 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 1.6430
2022-02-26 08:22:08 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 1.4028
2022-02-26 08:22:43 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 1.3658
2022-02-26 08:23:18 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 1.4986
2022-02-26 08:23:53 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 1.2773
2022-02-26 08:24:28 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 1.6822
2022-02-26 08:25:03 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 1.4758
2022-02-26 08:25:41 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 1.4230
2022-02-26 08:26:16 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 1.4677
2022-02-26 08:26:52 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.4954
2022-02-26 08:27:28 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 1.4618
2022-02-26 08:28:03 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 1.3448
2022-02-26 08:28:39 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 1.3524
2022-02-26 08:29:15 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 1.3464
2022-02-26 08:29:49 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 1.4831
2022-02-26 08:30:28 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 1.2638
2022-02-26 08:31:04 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.3721
2022-02-26 08:31:39 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 1.2979
2022-02-26 08:32:14 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 1.4863
2022-02-26 08:32:50 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 1.2747
2022-02-26 08:33:25 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 1.4346
2022-02-26 08:33:59 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 1.4743
2022-02-26 08:34:34 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 1.4922
2022-02-26 08:35:12 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 1.5336
2022-02-26 08:35:49 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 1.5024
2022-02-26 08:36:24 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 1.1477
2022-02-26 08:36:59 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 1.4980
2022-02-26 08:37:34 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 1.3973
2022-02-26 08:38:09 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 1.4430
2022-02-26 08:38:43 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 1.4379
2022-02-26 08:39:19 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 1.4358
2022-02-26 08:39:58 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 1.4915
2022-02-26 08:40:33 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 1.3767
2022-02-26 08:41:09 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 1.2634
2022-02-26 08:41:44 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 1.4237
2022-02-26 08:42:20 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 1.1844
2022-02-26 08:42:55 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 1.4057
2022-02-26 08:43:30 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 1.2723
2022-02-26 08:44:05 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 1.2811
2022-02-26 08:44:42 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 1.1667
2022-02-26 08:45:18 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 1.3504
2022-02-26 08:45:54 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 1.2947
2022-02-26 08:46:31 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 1.3211
2022-02-26 08:47:05 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.4344
2022-02-26 08:47:06 - train: epoch 062, train_loss: 1.3950
2022-02-26 08:48:25 - eval: epoch: 062, acc1: 70.400%, acc5: 89.660%, test_loss: 1.1887, per_image_load_time: 2.480ms, per_image_inference_time: 0.578ms
2022-02-26 08:48:25 - until epoch: 062, best_acc1: 70.400%
2022-02-26 08:48:25 - epoch 063 lr: 0.0010000000000000002
2022-02-26 08:49:06 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 1.3245
2022-02-26 08:49:44 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 1.3474
2022-02-26 08:50:21 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 1.5285
2022-02-26 08:50:56 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 1.4507
2022-02-26 08:51:32 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 1.3038
2022-02-26 08:52:07 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 1.5048
2022-02-26 08:52:42 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 1.3965
2022-02-26 08:53:17 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 1.4218
2022-02-26 08:53:53 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 1.5445
2022-02-26 08:54:30 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.4282
2022-02-26 08:55:06 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 1.1814
2022-02-26 08:55:42 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.3713
2022-02-26 08:56:18 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 1.4242
2022-02-26 08:56:54 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.7158
2022-02-26 08:57:28 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 1.4242
2022-02-26 08:58:04 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 1.3019
2022-02-26 08:58:41 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 1.3166
2022-02-26 08:59:18 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 1.4004
2022-02-26 08:59:54 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 1.5556
2022-02-26 09:00:31 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 1.2848
2022-02-26 09:01:06 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 1.2925
2022-02-26 09:01:42 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.6781
2022-02-26 09:02:17 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.4922
2022-02-26 09:02:52 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 1.5795
2022-02-26 09:03:29 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 1.3541
2022-02-26 09:04:06 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 1.2289
2022-02-26 09:04:42 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.6071
2022-02-26 09:05:18 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 1.3891
2022-02-26 09:05:54 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 1.3508
2022-02-26 09:06:29 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.4948
2022-02-26 09:07:04 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 1.6919
2022-02-26 09:07:39 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 1.5079
2022-02-26 09:08:17 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 1.3121
2022-02-26 09:08:53 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 1.1481
2022-02-26 09:09:28 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 1.5000
2022-02-26 09:10:04 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 1.2290
2022-02-26 09:10:40 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 1.5342
2022-02-26 09:11:15 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 1.6078
2022-02-26 09:11:50 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 1.0884
2022-02-26 09:12:25 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 1.0791
2022-02-26 09:13:03 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 1.5010
2022-02-26 09:13:39 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 1.2937
2022-02-26 09:14:16 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 1.4684
2022-02-26 09:14:51 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 1.4225
2022-02-26 09:15:27 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 1.3005
2022-02-26 09:16:03 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 1.4451
2022-02-26 09:16:38 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 1.3006
2022-02-26 09:17:13 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 1.4865
2022-02-26 09:17:52 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 1.2977
2022-02-26 09:18:27 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 1.2867
2022-02-26 09:18:28 - train: epoch 063, train_loss: 1.3807
2022-02-26 09:19:48 - eval: epoch: 063, acc1: 70.616%, acc5: 89.748%, test_loss: 1.1807, per_image_load_time: 1.980ms, per_image_inference_time: 0.591ms
2022-02-26 09:19:48 - until epoch: 063, best_acc1: 70.616%
2022-02-26 09:19:48 - epoch 064 lr: 0.0010000000000000002
2022-02-26 09:20:29 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 1.3101
2022-02-26 09:21:05 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 1.3613
2022-02-26 09:21:40 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 1.1647
2022-02-26 09:22:16 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 1.4772
2022-02-26 09:22:53 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 1.2269
2022-02-26 09:23:29 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 1.6298
2022-02-26 09:24:06 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 1.4423
2022-02-26 09:24:42 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 1.2565
2022-02-26 09:25:17 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 1.3704
2022-02-26 09:25:53 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 1.3370
2022-02-26 09:26:28 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 1.2986
2022-02-26 09:27:04 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 1.2164
2022-02-26 09:27:43 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 1.4546
2022-02-26 09:28:18 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 1.6783
2022-02-26 09:28:54 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 1.4973
2022-02-26 09:29:30 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 1.2891
2022-02-26 09:30:05 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 1.1682
2022-02-26 09:30:40 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 1.2587
2022-02-26 09:31:15 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 1.3243
2022-02-26 09:31:51 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 1.2599
2022-02-26 09:32:30 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 1.3689
2022-02-26 09:33:05 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 1.4825
2022-02-26 09:33:41 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 1.4937
2022-02-26 09:34:16 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 1.3878
2022-02-26 09:34:51 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 1.3095
2022-02-26 09:35:27 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 1.2879
2022-02-26 09:36:01 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 1.3304
2022-02-26 09:36:38 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 1.2386
2022-02-26 09:37:16 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 1.5804
2022-02-26 09:37:52 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 1.3834
2022-02-26 09:38:27 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 1.3331
2022-02-26 09:39:03 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 1.4004
2022-02-26 09:39:39 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 1.3011
2022-02-26 09:40:13 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 1.5451
2022-02-26 09:40:49 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 1.3345
2022-02-26 09:41:26 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 1.3231
2022-02-26 09:42:03 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 1.0350
2022-02-26 09:42:38 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 1.5534
2022-02-26 09:43:14 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 1.2517
2022-02-26 09:43:49 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 1.2265
2022-02-26 09:44:25 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 1.4549
2022-02-26 09:45:00 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 1.2263
2022-02-26 09:45:34 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 1.5615
2022-02-26 09:46:12 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 1.3400
2022-02-26 09:46:48 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 1.2098
2022-02-26 09:47:25 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.4484
2022-02-26 09:48:00 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.6950
2022-02-26 09:48:35 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 1.3989
2022-02-26 09:49:11 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 1.6369
2022-02-26 09:49:45 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 1.2574
2022-02-26 09:49:46 - train: epoch 064, train_loss: 1.3713
2022-02-26 09:51:07 - eval: epoch: 064, acc1: 70.880%, acc5: 89.854%, test_loss: 1.1738, per_image_load_time: 2.476ms, per_image_inference_time: 0.582ms
2022-02-26 09:51:07 - until epoch: 064, best_acc1: 70.880%
2022-02-26 09:51:07 - epoch 065 lr: 0.0010000000000000002
2022-02-26 09:51:49 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 1.5241
2022-02-26 09:52:25 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 1.2932
2022-02-26 09:53:01 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 1.3669
2022-02-26 09:53:36 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 1.4656
2022-02-26 09:54:12 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 1.3383
2022-02-26 09:54:47 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 1.4663
2022-02-26 09:55:22 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 1.4057
2022-02-26 09:55:59 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 1.2925
2022-02-26 09:56:36 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 1.2513
2022-02-26 09:57:11 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 1.3845
2022-02-26 09:57:46 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 1.3397
2022-02-26 09:58:22 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 1.5487
2022-02-26 09:58:57 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 1.3121
2022-02-26 09:59:32 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 1.1501
2022-02-26 10:00:07 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 1.2976
2022-02-26 10:00:44 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 1.3336
2022-02-26 10:01:21 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 1.3615
2022-02-26 10:01:57 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 1.2486
2022-02-26 10:02:33 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 1.1848
2022-02-26 10:03:08 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 1.2502
2022-02-26 10:03:43 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 1.3109
2022-02-26 10:04:19 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 1.4723
2022-02-26 10:04:54 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 1.2216
2022-02-26 10:05:31 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 1.3339
2022-02-26 10:06:08 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 1.3079
2022-02-26 10:06:44 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 1.4680
2022-02-26 10:07:19 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 1.4857
2022-02-26 10:07:54 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 1.1966
2022-02-26 10:08:29 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 1.4137
2022-02-26 10:09:05 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 1.2704
2022-02-26 10:09:40 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 1.3843
2022-02-26 10:10:15 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 1.4203
2022-02-26 10:10:53 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 1.3239
2022-02-26 10:11:29 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 1.2592
2022-02-26 10:12:04 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 1.6470
2022-02-26 10:12:41 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 1.4272
2022-02-26 10:13:16 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 1.2184
2022-02-26 10:13:51 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 1.1610
2022-02-26 10:14:26 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 1.4683
2022-02-26 10:15:05 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 1.3894
2022-02-26 10:15:41 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 1.2500
2022-02-26 10:16:16 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 1.3713
2022-02-26 10:16:52 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 1.2734
2022-02-26 10:17:27 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 1.3155
2022-02-26 10:18:03 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 1.3404
2022-02-26 10:18:38 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 1.3859
2022-02-26 10:19:12 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 1.4440
2022-02-26 10:19:50 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 1.1222
2022-02-26 10:20:27 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 1.2326
2022-02-26 10:21:01 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 1.4554
2022-02-26 10:21:02 - train: epoch 065, train_loss: 1.3623
2022-02-26 10:22:22 - eval: epoch: 065, acc1: 70.930%, acc5: 89.926%, test_loss: 1.1695, per_image_load_time: 2.027ms, per_image_inference_time: 0.586ms
2022-02-26 10:22:22 - until epoch: 065, best_acc1: 70.930%
2022-02-26 10:22:22 - epoch 066 lr: 0.0010000000000000002
2022-02-26 10:23:03 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 1.2824
2022-02-26 10:23:38 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 1.5652
2022-02-26 10:24:14 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 1.1439
2022-02-26 10:24:52 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 1.1241
2022-02-26 10:25:28 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 1.3989
2022-02-26 10:26:04 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 1.3680
2022-02-26 10:26:39 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 1.3242
2022-02-26 10:27:15 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 1.6763
2022-02-26 10:27:51 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 1.4812
2022-02-26 10:28:26 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 1.4964
2022-02-26 10:29:00 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 1.3650
2022-02-26 10:29:36 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 1.5012
2022-02-26 10:30:14 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 1.4817
2022-02-26 10:30:50 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 1.1918
2022-02-26 10:31:26 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 1.3670
2022-02-26 10:32:01 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.3719
2022-02-26 10:32:38 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 1.2895
2022-02-26 10:33:12 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 1.2344
2022-02-26 10:33:47 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 1.5686
2022-02-26 10:34:24 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 1.4741
2022-02-26 10:35:01 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 1.3723
2022-02-26 10:35:38 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 1.2880
2022-02-26 10:36:13 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 1.4933
2022-02-26 10:36:49 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 1.2625
2022-02-26 10:37:24 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 1.2972
2022-02-26 10:37:58 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 1.3173
2022-02-26 10:38:34 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 1.5432
2022-02-26 10:39:10 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 1.4953
2022-02-26 10:39:48 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 1.3081
2022-02-26 10:40:24 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 1.3309
2022-02-26 10:40:59 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.5009
2022-02-26 10:41:34 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 1.2578
2022-02-26 10:42:10 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 1.2894
2022-02-26 10:42:45 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 1.5929
2022-02-26 10:43:20 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 1.4351
2022-02-26 10:43:56 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 1.4745
2022-02-26 10:44:33 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 1.3499
2022-02-26 10:45:09 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 1.2510
2022-02-26 10:45:45 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 1.2344
2022-02-26 10:46:20 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 1.4605
2022-02-26 10:46:55 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 1.1700
2022-02-26 10:47:31 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 1.1846
2022-02-26 10:48:06 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 1.2071
2022-02-26 10:48:42 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 1.2987
2022-02-26 10:49:19 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 1.4201
2022-02-26 10:49:55 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 1.4985
2022-02-26 10:50:31 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 1.2264
2022-02-26 10:51:07 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 1.2705
2022-02-26 10:51:44 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 1.3317
2022-02-26 10:52:19 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 1.2798
2022-02-26 10:52:21 - train: epoch 066, train_loss: 1.3553
2022-02-26 10:53:44 - eval: epoch: 066, acc1: 70.952%, acc5: 89.982%, test_loss: 1.1652, per_image_load_time: 2.488ms, per_image_inference_time: 0.560ms
2022-02-26 10:53:45 - until epoch: 066, best_acc1: 70.952%
2022-02-26 10:53:45 - epoch 067 lr: 0.0010000000000000002
2022-02-26 10:54:27 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 1.2594
2022-02-26 10:55:05 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 1.3377
2022-02-26 10:55:42 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 1.6180
2022-02-26 10:56:18 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 1.3475
2022-02-26 10:56:55 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 1.2589
2022-02-26 10:57:31 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 1.2451
2022-02-26 10:58:07 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 1.3057
2022-02-26 10:58:46 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 1.4537
2022-02-26 10:59:24 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 1.5239
2022-02-26 11:00:00 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 1.1878
2022-02-26 11:00:37 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 1.3954
2022-02-26 11:01:15 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 1.2951
2022-02-26 11:01:51 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 1.5344
2022-02-26 11:02:27 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 1.3117
2022-02-26 11:03:03 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 1.3541
2022-02-26 11:03:43 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 1.3463
2022-02-26 11:04:20 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 1.2155
2022-02-26 11:04:57 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.6497
2022-02-26 11:05:34 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.3360
2022-02-26 11:06:12 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.6248
2022-02-26 11:06:48 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 1.1876
2022-02-26 11:07:24 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 1.3871
2022-02-26 11:08:01 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 1.3504
2022-02-26 11:08:40 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 1.2375
2022-02-26 11:09:16 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 1.2585
2022-02-26 11:09:53 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 1.2555
2022-02-26 11:10:30 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 1.2511
2022-02-26 11:11:07 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 1.4145
2022-02-26 11:11:44 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 1.2462
2022-02-26 11:12:19 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.2915
2022-02-26 11:12:55 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 1.1472
2022-02-26 11:13:32 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 1.5242
2022-02-26 11:14:10 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 1.1144
2022-02-26 11:14:47 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 1.3833
2022-02-26 11:15:24 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 1.2138
2022-02-26 11:16:00 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 1.2754
2022-02-26 11:16:37 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 1.4915
2022-02-26 11:17:13 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 1.3358
2022-02-26 11:17:49 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 1.4251
2022-02-26 11:18:25 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 1.4449
2022-02-26 11:19:04 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 1.5261
2022-02-26 11:19:40 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 1.5301
2022-02-26 11:20:17 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 1.1171
2022-02-26 11:20:54 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 1.3996
2022-02-26 11:21:30 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 1.1803
2022-02-26 11:22:08 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 1.0809
2022-02-26 11:22:45 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 1.2549
2022-02-26 11:23:20 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 1.1076
2022-02-26 11:24:00 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 1.4118
2022-02-26 11:24:35 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 1.3929
2022-02-26 11:24:37 - train: epoch 067, train_loss: 1.3502
2022-02-26 11:26:00 - eval: epoch: 067, acc1: 71.056%, acc5: 90.054%, test_loss: 1.1633, per_image_load_time: 1.744ms, per_image_inference_time: 0.572ms
2022-02-26 11:26:01 - until epoch: 067, best_acc1: 71.056%
2022-02-26 11:26:01 - epoch 068 lr: 0.0010000000000000002
2022-02-26 11:26:43 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 1.3773
2022-02-26 11:27:20 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 1.3372
2022-02-26 11:27:56 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 1.5172
2022-02-26 11:28:33 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 1.2824
2022-02-26 11:29:12 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 1.2038
2022-02-26 11:29:49 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 1.3421
2022-02-26 11:30:26 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 1.6266
2022-02-26 11:31:02 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 1.3261
2022-02-26 11:31:39 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 1.2496
2022-02-26 11:32:15 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 1.3146
2022-02-26 11:32:51 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 1.5421
2022-02-26 11:33:27 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 1.1923
2022-02-26 11:34:05 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 1.1967
2022-02-26 11:34:42 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 1.3703
2022-02-26 11:35:19 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 1.4301
2022-02-26 11:35:56 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 1.4410
2022-02-26 11:36:32 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 1.4596
2022-02-26 11:37:08 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 1.4412
2022-02-26 11:37:44 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 1.3506
2022-02-26 11:38:19 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 1.4353
2022-02-26 11:38:56 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 1.3810
2022-02-26 11:39:34 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 1.4536
2022-02-26 11:40:11 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 1.2457
2022-02-26 11:40:47 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 1.4332
2022-02-26 11:41:24 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 1.4525
2022-02-26 11:41:59 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 1.3398
2022-02-26 11:42:35 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 1.4231
2022-02-26 11:43:11 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 1.5196
2022-02-26 11:43:45 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 1.4851
2022-02-26 11:44:25 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 1.5771
2022-02-26 11:45:02 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 1.1805
2022-02-26 11:45:39 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 1.5558
2022-02-26 11:46:15 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 1.3205
2022-02-26 11:46:51 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 1.2065
2022-02-26 11:47:27 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 1.3555
2022-02-26 11:48:02 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 1.1951
2022-02-26 11:48:39 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 1.3697
2022-02-26 11:49:17 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 1.5026
2022-02-26 11:49:55 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 1.4329
2022-02-26 11:50:32 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 1.3048
2022-02-26 11:51:08 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 1.1446
2022-02-26 11:51:45 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 1.5132
2022-02-26 11:52:21 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 1.4356
2022-02-26 11:52:56 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 1.4221
2022-02-26 11:53:31 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 1.3926
2022-02-26 11:54:10 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 1.4140
2022-02-26 11:54:45 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 1.6078
2022-02-26 11:55:21 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 1.4653
2022-02-26 11:55:57 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 1.4728
2022-02-26 11:56:31 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 1.3472
2022-02-26 11:56:33 - train: epoch 068, train_loss: 1.3452
2022-02-26 11:57:53 - eval: epoch: 068, acc1: 71.092%, acc5: 90.060%, test_loss: 1.1585, per_image_load_time: 2.460ms, per_image_inference_time: 0.556ms
2022-02-26 11:57:53 - until epoch: 068, best_acc1: 71.092%
2022-02-26 11:57:53 - epoch 069 lr: 0.0010000000000000002
2022-02-26 11:58:35 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 1.5458
2022-02-26 11:59:12 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 1.4987
2022-02-26 11:59:48 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 1.3632
2022-02-26 12:00:25 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 1.2535
2022-02-26 12:01:00 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 1.3148
2022-02-26 12:01:36 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 1.2205
2022-02-26 12:02:11 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 1.3271
2022-02-26 12:02:46 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 1.3818
2022-02-26 12:03:22 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 1.3189
2022-02-26 12:03:59 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 1.3105
2022-02-26 12:04:35 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 1.2655
2022-02-26 12:05:11 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 1.3145
2022-02-26 12:05:48 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 1.7081
2022-02-26 12:06:24 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 1.4664
2022-02-26 12:06:58 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 1.3156
2022-02-26 12:07:34 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 1.4814
2022-02-26 12:08:09 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 1.2279
2022-02-26 12:08:47 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 1.1413
2022-02-26 12:09:24 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 1.2695
2022-02-26 12:09:59 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 1.2175
2022-02-26 12:10:35 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 1.4120
2022-02-26 12:11:11 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 1.3709
2022-02-26 12:11:47 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 1.3300
2022-02-26 12:12:22 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 1.4902
2022-02-26 12:12:57 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 1.3302
2022-02-26 12:13:33 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 1.3462
2022-02-26 12:14:10 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 1.5979
2022-02-26 12:14:47 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 1.5050
2022-02-26 12:15:22 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 1.2967
2022-02-26 12:15:58 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 1.2166
2022-02-26 12:16:34 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 1.1705
2022-02-26 12:17:09 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 1.2618
2022-02-26 12:17:44 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 1.2065
2022-02-26 12:18:19 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 1.1428
2022-02-26 12:18:57 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 1.2364
2022-02-26 12:19:33 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 1.1834
2022-02-26 12:20:08 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 1.4328
2022-02-26 12:20:44 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 1.3923
2022-02-26 12:21:20 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 1.2810
2022-02-26 12:21:56 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 1.4095
2022-02-26 12:22:31 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 1.5037
2022-02-26 12:23:06 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 1.3150
2022-02-26 12:23:43 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 1.3916
2022-02-26 12:24:20 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 1.3264
2022-02-26 12:24:56 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 1.3272
2022-02-26 12:25:31 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 1.5106
2022-02-26 12:26:07 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 1.3887
2022-02-26 12:26:43 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 1.3118
2022-02-26 12:27:18 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 1.2202
2022-02-26 12:27:52 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 1.3970
2022-02-26 12:27:53 - train: epoch 069, train_loss: 1.3404
2022-02-26 12:29:17 - eval: epoch: 069, acc1: 71.042%, acc5: 90.004%, test_loss: 1.1595, per_image_load_time: 2.565ms, per_image_inference_time: 0.537ms
2022-02-26 12:29:17 - until epoch: 069, best_acc1: 71.092%
2022-02-26 12:29:17 - epoch 070 lr: 0.0010000000000000002
2022-02-26 12:29:59 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 1.4313
2022-02-26 12:30:35 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 1.4187
2022-02-26 12:31:10 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 1.4706
2022-02-26 12:31:45 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 1.2575
2022-02-26 12:32:21 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 1.3335
2022-02-26 12:32:56 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 1.2707
2022-02-26 12:33:34 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 1.2645
2022-02-26 12:34:09 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 1.2978
2022-02-26 12:34:45 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 1.4253
2022-02-26 12:35:21 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 1.3244
2022-02-26 12:35:57 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.6322
2022-02-26 12:36:32 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 1.1718
2022-02-26 12:37:07 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 1.2922
2022-02-26 12:37:42 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 1.3003
2022-02-26 12:38:20 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 1.3656
2022-02-26 12:38:55 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 1.3814
2022-02-26 12:39:32 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 1.4615
2022-02-26 12:40:07 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 1.1448
2022-02-26 12:40:43 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.3275
2022-02-26 12:41:18 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 1.3908
2022-02-26 12:41:54 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 1.4602
2022-02-26 12:42:29 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 1.4573
2022-02-26 12:43:07 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 1.3404
2022-02-26 12:43:43 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 1.4935
2022-02-26 12:44:19 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 1.2811
2022-02-26 12:44:55 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 1.3904
2022-02-26 12:45:30 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 1.3148
2022-02-26 12:46:05 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 1.5001
2022-02-26 12:46:40 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 1.5449
2022-02-26 12:47:17 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 1.3121
2022-02-26 12:47:55 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 1.2815
2022-02-26 12:48:30 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 1.4493
2022-02-26 12:49:06 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 1.3380
2022-02-26 12:49:41 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 1.3233
2022-02-26 12:50:17 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 1.3582
2022-02-26 12:50:51 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 1.4032
2022-02-26 12:51:27 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 1.3587
2022-02-26 12:52:03 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 1.2785
2022-02-26 12:52:40 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 1.1267
2022-02-26 12:53:16 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 1.1875
2022-02-26 12:53:52 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 1.2816
2022-02-26 12:54:28 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 1.4092
2022-02-26 12:55:03 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 1.3674
2022-02-26 12:55:38 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 1.3694
2022-02-26 12:56:13 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 1.2567
2022-02-26 12:56:50 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 1.4861
2022-02-26 12:57:27 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 1.4350
2022-02-26 12:58:03 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 1.4777
2022-02-26 12:58:39 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 1.4245
2022-02-26 12:59:13 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 1.3660
2022-02-26 12:59:15 - train: epoch 070, train_loss: 1.3364
2022-02-26 13:00:33 - eval: epoch: 070, acc1: 71.232%, acc5: 89.992%, test_loss: 1.1563, per_image_load_time: 2.421ms, per_image_inference_time: 0.584ms
2022-02-26 13:00:33 - until epoch: 070, best_acc1: 71.232%
2022-02-26 13:00:33 - epoch 071 lr: 0.0010000000000000002
2022-02-26 13:01:14 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 1.2506
2022-02-26 13:01:53 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 1.2787
2022-02-26 13:02:29 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 1.2531
2022-02-26 13:03:05 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 1.3987
2022-02-26 13:03:40 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 1.3965
2022-02-26 13:04:16 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 1.4526
2022-02-26 13:04:51 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 1.5110
2022-02-26 13:05:27 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 1.2843
2022-02-26 13:06:02 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 1.3375
2022-02-26 13:06:40 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 1.4092
2022-02-26 13:07:16 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 1.4782
2022-02-26 13:07:51 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 1.3355
2022-02-26 13:08:27 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 1.4119
2022-02-26 13:09:02 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 1.3484
2022-02-26 13:09:37 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 1.1569
2022-02-26 13:10:12 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 1.2437
2022-02-26 13:10:49 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 1.2517
2022-02-26 13:11:26 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 1.2810
2022-02-26 13:12:02 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 1.2714
2022-02-26 13:12:38 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 1.4228
2022-02-26 13:13:13 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 1.1997
2022-02-26 13:13:48 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 1.1699
2022-02-26 13:14:24 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 1.3191
2022-02-26 13:14:58 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 1.2915
2022-02-26 13:15:35 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 1.4823
2022-02-26 13:16:13 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 1.2508
2022-02-26 13:16:48 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 1.4148
2022-02-26 13:17:24 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 1.5238
2022-02-26 13:17:59 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 1.3059
2022-02-26 13:18:35 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 1.4206
2022-02-26 13:19:10 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 1.2934
2022-02-26 13:19:45 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 1.1762
2022-02-26 13:20:21 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 1.1941
2022-02-26 13:20:59 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 1.3380
2022-02-26 13:21:34 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 1.4913
2022-02-26 13:22:09 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 1.3993
2022-02-26 13:22:46 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 1.2895
2022-02-26 13:23:23 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 1.2983
2022-02-26 13:23:57 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 1.2977
2022-02-26 13:24:32 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 1.5555
2022-02-26 13:25:08 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 1.2305
2022-02-26 13:25:46 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 1.4320
2022-02-26 13:26:22 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 1.2841
2022-02-26 13:26:57 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 1.3790
2022-02-26 13:27:33 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 1.2450
2022-02-26 13:28:08 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 1.4191
2022-02-26 13:28:44 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 1.1406
2022-02-26 13:29:19 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 1.2531
2022-02-26 13:29:55 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 1.0863
2022-02-26 13:30:31 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 1.2662
2022-02-26 13:30:33 - train: epoch 071, train_loss: 1.3336
2022-02-26 13:31:53 - eval: epoch: 071, acc1: 71.178%, acc5: 90.052%, test_loss: 1.1553, per_image_load_time: 2.503ms, per_image_inference_time: 0.571ms
2022-02-26 13:31:54 - until epoch: 071, best_acc1: 71.232%
2022-02-26 13:31:54 - epoch 072 lr: 0.0010000000000000002
2022-02-26 13:32:35 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 1.4267
2022-02-26 13:33:11 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 1.1333
2022-02-26 13:33:47 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 1.2696
2022-02-26 13:34:21 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 1.3363
2022-02-26 13:34:58 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 1.2469
2022-02-26 13:35:35 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 1.2937
2022-02-26 13:36:12 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 1.2847
2022-02-26 13:36:47 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 1.5114
2022-02-26 13:37:24 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 1.1818
2022-02-26 13:38:00 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 1.2685
2022-02-26 13:38:35 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 1.3820
2022-02-26 13:39:10 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 1.2395
2022-02-26 13:39:46 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 1.3946
2022-02-26 13:40:24 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 1.3863
2022-02-26 13:41:00 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 1.2135
2022-02-26 13:41:36 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 1.4169
2022-02-26 13:42:11 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 1.2433
2022-02-26 13:42:48 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 1.2468
2022-02-26 13:43:23 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 1.2178
2022-02-26 13:43:58 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 1.3609
2022-02-26 13:44:33 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 1.3589
2022-02-26 13:45:12 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 1.3711
2022-02-26 13:45:47 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.4099
2022-02-26 13:46:23 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 1.2834
2022-02-26 13:46:58 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 1.2509
2022-02-26 13:47:35 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 1.1673
2022-02-26 13:48:10 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 1.2920
2022-02-26 13:48:45 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 1.3808
2022-02-26 13:49:20 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 1.3067
2022-02-26 13:49:58 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 1.2321
2022-02-26 13:50:33 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 1.4849
2022-02-26 13:51:09 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 1.2658
2022-02-26 13:51:45 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 1.2588
2022-02-26 13:52:20 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 1.5158
2022-02-26 13:52:56 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 1.3344
2022-02-26 13:53:31 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 1.2841
2022-02-26 13:54:06 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 1.4906
2022-02-26 13:54:41 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 1.3692
2022-02-26 13:55:19 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 1.3852
2022-02-26 13:55:55 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 1.2999
2022-02-26 13:56:31 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 1.4980
2022-02-26 13:57:07 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 1.2888
2022-02-26 13:57:43 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 1.3487
2022-02-26 13:58:18 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 1.2150
2022-02-26 13:58:52 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 1.3554
2022-02-26 13:59:28 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 1.2148
2022-02-26 14:00:06 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 1.3171
2022-02-26 14:00:41 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 1.5124
2022-02-26 14:01:18 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 1.4026
2022-02-26 14:01:52 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 1.3195
2022-02-26 14:01:54 - train: epoch 072, train_loss: 1.3316
2022-02-26 14:03:13 - eval: epoch: 072, acc1: 71.230%, acc5: 90.062%, test_loss: 1.1545, per_image_load_time: 2.439ms, per_image_inference_time: 0.575ms
2022-02-26 14:03:13 - until epoch: 072, best_acc1: 71.232%
2022-02-26 14:03:13 - epoch 073 lr: 0.0010000000000000002
2022-02-26 14:03:53 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.5461
2022-02-26 14:04:31 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 1.3351
2022-02-26 14:05:07 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 1.4087
2022-02-26 14:05:43 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 1.0716
2022-02-26 14:06:18 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 1.2352
2022-02-26 14:06:54 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 1.1199
2022-02-26 14:07:30 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 1.2846
2022-02-26 14:08:05 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 1.2548
2022-02-26 14:08:40 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 1.1678
2022-02-26 14:09:17 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 1.3294
2022-02-26 14:09:54 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 1.3561
2022-02-26 14:10:29 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 1.3162
2022-02-26 14:11:05 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 1.3809
2022-02-26 14:11:41 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 1.2971
2022-02-26 14:12:16 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 1.3070
2022-02-26 14:12:51 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 1.4037
2022-02-26 14:13:26 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 1.6038
2022-02-26 14:14:04 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 1.1393
2022-02-26 14:14:40 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.4630
2022-02-26 14:15:15 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 1.0759
2022-02-26 14:15:51 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 1.3098
2022-02-26 14:16:26 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 1.4576
2022-02-26 14:17:01 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 1.3811
2022-02-26 14:17:36 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 1.3112
2022-02-26 14:18:11 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 1.4516
2022-02-26 14:18:49 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 1.3396
2022-02-26 14:19:25 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 1.5166
2022-02-26 14:20:01 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 1.3697
2022-02-26 14:20:37 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 1.4462
2022-02-26 14:21:13 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 1.1500
2022-02-26 14:21:48 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 1.3017
2022-02-26 14:22:23 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 1.2112
2022-02-26 14:22:58 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 1.4571
2022-02-26 14:23:36 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 1.2845
2022-02-26 14:24:12 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 1.4501
2022-02-26 14:24:47 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 1.2484
2022-02-26 14:25:23 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 1.3114
2022-02-26 14:25:59 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 1.5698
2022-02-26 14:26:35 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 1.2811
2022-02-26 14:27:10 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 1.3513
2022-02-26 14:27:45 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 1.3777
2022-02-26 14:28:24 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 1.4634
2022-02-26 14:29:00 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 1.2255
2022-02-26 14:29:35 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 1.3480
2022-02-26 14:30:10 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 1.2524
2022-02-26 14:30:46 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 1.5039
2022-02-26 14:31:22 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 1.1731
2022-02-26 14:31:56 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 1.4560
2022-02-26 14:32:31 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 1.2420
2022-02-26 14:33:09 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 1.4324
2022-02-26 14:33:10 - train: epoch 073, train_loss: 1.3277
2022-02-26 14:34:30 - eval: epoch: 073, acc1: 71.314%, acc5: 90.162%, test_loss: 1.1500, per_image_load_time: 2.389ms, per_image_inference_time: 0.586ms
2022-02-26 14:34:30 - until epoch: 073, best_acc1: 71.314%
2022-02-26 14:34:30 - epoch 074 lr: 0.0010000000000000002
2022-02-26 14:35:11 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 1.2870
2022-02-26 14:35:48 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 1.3949
2022-02-26 14:36:23 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 1.4047
2022-02-26 14:36:59 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 1.5988
2022-02-26 14:37:34 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 1.2895
2022-02-26 14:38:12 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 1.3973
2022-02-26 14:38:47 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 1.3040
2022-02-26 14:39:22 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 1.5071
2022-02-26 14:39:59 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 1.2404
2022-02-26 14:40:35 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 1.4347
2022-02-26 14:41:09 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 1.4733
2022-02-26 14:41:44 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 1.1933
2022-02-26 14:42:21 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 1.4195
2022-02-26 14:42:58 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 1.1843
2022-02-26 14:43:33 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 1.2900
2022-02-26 14:44:09 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 1.0730
2022-02-26 14:44:45 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 1.5408
2022-02-26 14:45:20 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 1.7114
2022-02-26 14:45:55 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 1.4426
2022-02-26 14:46:31 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 1.1082
2022-02-26 14:47:07 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 1.3471
2022-02-26 14:47:45 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 1.6593
2022-02-26 14:48:20 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 1.5081
2022-02-26 14:48:56 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 1.3513
2022-02-26 14:49:31 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 1.2109
2022-02-26 14:50:07 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 1.1060
2022-02-26 14:50:42 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 1.2917
2022-02-26 14:51:16 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 1.5310
2022-02-26 14:51:51 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 1.2851
2022-02-26 14:52:29 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 1.3816
2022-02-26 14:53:05 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 1.3166
2022-02-26 14:53:40 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 1.2388
2022-02-26 14:54:16 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 1.4536
2022-02-26 14:54:52 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 1.3631
2022-02-26 14:55:27 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 1.1515
2022-02-26 14:56:01 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 1.3962
2022-02-26 14:56:36 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 1.3514
2022-02-26 14:57:14 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 1.2657
2022-02-26 14:57:50 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 1.1005
2022-02-26 14:58:26 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 1.2841
2022-02-26 14:59:01 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 1.5115
2022-02-26 14:59:36 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 1.2995
2022-02-26 15:00:11 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 1.2844
2022-02-26 15:00:46 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 1.1863
2022-02-26 15:01:20 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 1.0585
2022-02-26 15:01:59 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 1.3884
2022-02-26 15:02:35 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 1.3813
2022-02-26 15:03:09 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 1.3297
2022-02-26 15:03:46 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 1.5058
2022-02-26 15:04:20 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 1.2987
2022-02-26 15:04:22 - train: epoch 074, train_loss: 1.3232
2022-02-26 15:05:40 - eval: epoch: 074, acc1: 71.290%, acc5: 90.180%, test_loss: 1.1470, per_image_load_time: 2.443ms, per_image_inference_time: 0.576ms
2022-02-26 15:05:41 - until epoch: 074, best_acc1: 71.314%
2022-02-26 15:05:41 - epoch 075 lr: 0.0010000000000000002
2022-02-26 15:06:24 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 1.3867
2022-02-26 15:07:00 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 1.2783
2022-02-26 15:07:36 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 1.3092
2022-02-26 15:08:12 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 1.3746
2022-02-26 15:08:47 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 1.2029
2022-02-26 15:09:23 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 1.1877
2022-02-26 15:09:58 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 1.4844
2022-02-26 15:10:32 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 1.4885
2022-02-26 15:11:10 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 1.4818
2022-02-26 15:11:47 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 1.1708
2022-02-26 15:12:23 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 1.4128
2022-02-26 15:12:58 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 1.2291
2022-02-26 15:13:34 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 1.1993
2022-02-26 15:14:09 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 1.2361
2022-02-26 15:14:45 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 1.3333
2022-02-26 15:15:20 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 1.0731
2022-02-26 15:15:58 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 1.3226
2022-02-26 15:16:35 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 1.2389
2022-02-26 15:17:12 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 1.2073
2022-02-26 15:17:48 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 1.3858
2022-02-26 15:18:25 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 1.2108
2022-02-26 15:19:01 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 1.3525
2022-02-26 15:19:36 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 1.2969
2022-02-26 15:20:12 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 1.3061
2022-02-26 15:20:50 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 1.3328
2022-02-26 15:21:28 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 1.3674
2022-02-26 15:22:06 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 1.0202
2022-02-26 15:22:41 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 1.3442
2022-02-26 15:23:18 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 1.4058
2022-02-26 15:23:53 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 1.4790
2022-02-26 15:24:29 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 1.3460
2022-02-26 15:25:05 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 1.2984
2022-02-26 15:25:46 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 1.3336
2022-02-26 15:26:22 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 1.1374
2022-02-26 15:26:59 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 1.2368
2022-02-26 15:27:36 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 1.2966
2022-02-26 15:28:12 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 1.4283
2022-02-26 15:28:49 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 1.2379
2022-02-26 15:29:25 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 1.5028
2022-02-26 15:30:02 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 1.2396
2022-02-26 15:30:40 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 1.0435
2022-02-26 15:31:18 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 1.4310
2022-02-26 15:31:54 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 1.5817
2022-02-26 15:32:32 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 1.1655
2022-02-26 15:33:09 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 1.3389
2022-02-26 15:33:45 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 1.2153
2022-02-26 15:34:21 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 1.4977
2022-02-26 15:34:59 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 1.2888
2022-02-26 15:35:38 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 1.1642
2022-02-26 15:36:14 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 1.3740
2022-02-26 15:36:15 - train: epoch 075, train_loss: 1.3231
2022-02-26 15:37:38 - eval: epoch: 075, acc1: 71.256%, acc5: 90.158%, test_loss: 1.1465, per_image_load_time: 2.155ms, per_image_inference_time: 0.593ms
2022-02-26 15:37:38 - until epoch: 075, best_acc1: 71.314%
2022-02-26 15:37:38 - epoch 076 lr: 0.0010000000000000002
2022-02-26 15:38:20 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 1.1011
2022-02-26 15:38:56 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 1.2748
2022-02-26 15:39:32 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 1.3777
2022-02-26 15:40:11 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 1.3391
2022-02-26 15:40:48 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 1.3393
2022-02-26 15:41:25 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 1.3393
2022-02-26 15:42:03 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 1.3351
2022-02-26 15:42:40 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 1.1804
2022-02-26 15:43:18 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 1.1539
2022-02-26 15:43:53 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 1.0957
2022-02-26 15:44:30 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 1.1691
2022-02-26 15:45:08 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 1.3193
2022-02-26 15:45:47 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 1.2550
2022-02-26 15:46:24 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 1.2383
2022-02-26 15:47:00 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 1.2385
2022-02-26 15:47:39 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 1.2015
2022-02-26 15:48:15 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 1.3668
2022-02-26 15:48:51 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 1.0751
2022-02-26 15:49:27 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 1.4088
2022-02-26 15:50:06 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 1.3329
2022-02-26 15:50:44 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 1.4533
2022-02-26 15:51:21 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 1.4912
2022-02-26 15:51:58 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 1.2504
2022-02-26 15:52:36 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.4884
2022-02-26 15:53:11 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 1.2449
2022-02-26 15:53:47 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 1.5679
2022-02-26 15:54:24 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 1.4429
2022-02-26 15:55:03 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 1.2328
2022-02-26 15:55:40 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 1.3086
2022-02-26 15:56:17 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 1.1222
2022-02-26 15:56:54 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 1.3326
2022-02-26 15:57:31 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 1.3653
2022-02-26 15:58:07 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 1.2493
2022-02-26 15:58:43 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 1.3109
2022-02-26 15:59:21 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 1.1721
2022-02-26 15:59:59 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 1.2274
2022-02-26 16:00:37 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 1.2014
2022-02-26 16:01:13 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 1.1368
2022-02-26 16:01:50 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 1.2339
2022-02-26 16:02:27 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 1.3096
2022-02-26 16:03:03 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 1.2870
2022-02-26 16:03:38 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 1.5018
2022-02-26 16:04:17 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 1.3891
2022-02-26 16:04:55 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 1.2454
2022-02-26 16:05:33 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 1.2823
2022-02-26 16:06:09 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 1.2377
2022-02-26 16:06:46 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 1.4973
2022-02-26 16:07:22 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 1.0805
2022-02-26 16:07:59 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 1.3065
2022-02-26 16:08:34 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 1.2994
2022-02-26 16:08:35 - train: epoch 076, train_loss: 1.3197
2022-02-26 16:09:59 - eval: epoch: 076, acc1: 71.228%, acc5: 90.132%, test_loss: 1.1490, per_image_load_time: 1.227ms, per_image_inference_time: 0.585ms
2022-02-26 16:09:59 - until epoch: 076, best_acc1: 71.314%
2022-02-26 16:09:59 - epoch 077 lr: 0.0010000000000000002
2022-02-26 16:10:41 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 1.4314
2022-02-26 16:11:18 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 1.4347
2022-02-26 16:11:55 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 1.1661
2022-02-26 16:12:31 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 1.4474
2022-02-26 16:13:06 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 1.1842
2022-02-26 16:13:42 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 1.1016
2022-02-26 16:14:23 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 1.4406
2022-02-26 16:15:00 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 1.4204
2022-02-26 16:15:37 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 1.5073
2022-02-26 16:16:14 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 1.0641
2022-02-26 16:16:51 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 1.4234
2022-02-26 16:17:27 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 1.4203
2022-02-26 16:18:03 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 1.1125
2022-02-26 16:18:41 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 1.3400
2022-02-26 16:19:19 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 1.2316
2022-02-26 16:19:56 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 1.2722
2022-02-26 16:20:34 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 1.6197
2022-02-26 16:21:10 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 1.1568
2022-02-26 16:21:46 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 1.3363
2022-02-26 16:22:22 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 1.2296
2022-02-26 16:22:58 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 1.3471
2022-02-26 16:23:36 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 1.4272
2022-02-26 16:24:14 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 1.4541
2022-02-26 16:24:51 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 1.2983
2022-02-26 16:25:27 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 1.4567
2022-02-26 16:26:05 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 1.1217
2022-02-26 16:26:42 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 1.3319
2022-02-26 16:27:18 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 1.5215
2022-02-26 16:27:53 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 1.6477
2022-02-26 16:28:32 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 1.1783
2022-02-26 16:29:10 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 1.3523
2022-02-26 16:29:48 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 1.4972
2022-02-26 16:30:25 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 1.0942
2022-02-26 16:31:01 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 1.5447
2022-02-26 16:31:38 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 1.1681
2022-02-26 16:32:14 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 1.2617
2022-02-26 16:32:50 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 1.3630
2022-02-26 16:33:30 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 1.3171
2022-02-26 16:34:07 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 1.3638
2022-02-26 16:34:44 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 1.2793
2022-02-26 16:35:21 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 1.3833
2022-02-26 16:35:58 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 1.4611
2022-02-26 16:36:33 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 1.2585
2022-02-26 16:37:10 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 1.2918
2022-02-26 16:37:46 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 1.4562
2022-02-26 16:38:25 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 1.2586
2022-02-26 16:39:02 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 1.0717
2022-02-26 16:39:39 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 1.2734
2022-02-26 16:40:16 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 1.4221
2022-02-26 16:40:51 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 1.5229
2022-02-26 16:40:53 - train: epoch 077, train_loss: 1.3150
2022-02-26 16:42:14 - eval: epoch: 077, acc1: 71.346%, acc5: 90.332%, test_loss: 1.1457, per_image_load_time: 2.535ms, per_image_inference_time: 0.582ms
2022-02-26 16:42:14 - until epoch: 077, best_acc1: 71.346%
2022-02-26 16:42:14 - epoch 078 lr: 0.0010000000000000002
2022-02-26 16:42:59 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 1.3490
2022-02-26 16:43:37 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 1.4346
2022-02-26 16:44:15 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 1.4151
2022-02-26 16:44:51 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 1.3343
2022-02-26 16:45:28 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 1.3295
2022-02-26 16:46:03 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 1.2834
2022-02-26 16:46:39 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 1.5973
2022-02-26 16:47:16 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 1.4649
2022-02-26 16:47:54 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 1.4375
2022-02-26 16:48:32 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 1.4012
2022-02-26 16:49:09 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 1.1061
2022-02-26 16:49:46 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 1.1013
2022-02-26 16:50:23 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 1.3696
2022-02-26 16:50:59 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 1.1301
2022-02-26 16:51:36 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 1.3131
2022-02-26 16:52:12 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 1.3510
2022-02-26 16:52:51 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 1.3532
2022-02-26 16:53:29 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 1.2220
2022-02-26 16:54:05 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 1.2050
2022-02-26 16:54:43 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 1.2262
2022-02-26 16:55:20 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 1.5967
2022-02-26 16:55:55 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 1.2629
2022-02-26 16:56:31 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 1.3911
2022-02-26 16:57:07 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 1.2625
2022-02-26 16:57:46 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 1.1694
2022-02-26 16:58:22 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 1.2578
2022-02-26 16:59:00 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 1.2511
2022-02-26 16:59:35 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 1.3644
2022-02-26 17:00:13 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 1.1855
2022-02-26 17:00:48 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 1.3623
2022-02-26 17:01:24 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 1.4169
2022-02-26 17:02:02 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 1.2249
2022-02-26 17:02:41 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.4166
2022-02-26 17:03:17 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 1.3653
2022-02-26 17:03:54 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 1.2673
2022-02-26 17:04:31 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 1.3562
2022-02-26 17:05:07 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 1.1676
2022-02-26 17:05:43 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 1.3178
2022-02-26 17:06:19 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 1.3621
2022-02-26 17:06:57 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 1.3067
2022-02-26 17:07:35 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 1.4127
2022-02-26 17:08:12 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 1.4724
2022-02-26 17:08:49 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 1.4040
2022-02-26 17:09:25 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 1.3274
2022-02-26 17:10:02 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 1.3554
2022-02-26 17:10:38 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 1.1239
2022-02-26 17:11:13 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 1.4317
2022-02-26 17:11:53 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 1.5311
2022-02-26 17:12:31 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 1.3773
2022-02-26 17:13:05 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 1.3698
2022-02-26 17:13:07 - train: epoch 078, train_loss: 1.3129
2022-02-26 17:14:29 - eval: epoch: 078, acc1: 71.418%, acc5: 90.232%, test_loss: 1.1457, per_image_load_time: 2.583ms, per_image_inference_time: 0.590ms
2022-02-26 17:14:30 - until epoch: 078, best_acc1: 71.418%
2022-02-26 17:14:30 - epoch 079 lr: 0.0010000000000000002
2022-02-26 17:15:11 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 1.1773
2022-02-26 17:15:47 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 1.2551
2022-02-26 17:16:26 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 1.3936
2022-02-26 17:17:04 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 1.4161
2022-02-26 17:17:41 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 1.2892
2022-02-26 17:18:17 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 1.2323
2022-02-26 17:18:55 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 1.1668
2022-02-26 17:19:32 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 1.3510
2022-02-26 17:20:08 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 1.2653
2022-02-26 17:20:44 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 1.1791
2022-02-26 17:21:20 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 1.4225
2022-02-26 17:22:00 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 1.5678
2022-02-26 17:22:38 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 1.0799
2022-02-26 17:23:14 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 1.2341
2022-02-26 17:23:52 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 1.1323
2022-02-26 17:24:28 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 1.1385
2022-02-26 17:25:05 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 1.2413
2022-02-26 17:25:41 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 1.2669
2022-02-26 17:26:18 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 1.3602
2022-02-26 17:26:57 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 1.4329
2022-02-26 17:27:33 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 1.1806
2022-02-26 17:28:10 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 1.3820
2022-02-26 17:28:46 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 1.2527
2022-02-26 17:29:23 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 1.2756
2022-02-26 17:29:59 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 1.3144
2022-02-26 17:30:35 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 1.2142
2022-02-26 17:31:13 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 1.1109
2022-02-26 17:31:49 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 1.2625
2022-02-26 17:32:26 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 1.1682
2022-02-26 17:33:04 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 1.2873
2022-02-26 17:33:41 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 1.3075
2022-02-26 17:34:17 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 1.5072
2022-02-26 17:34:52 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 1.2955
2022-02-26 17:35:29 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 1.1605
2022-02-26 17:36:06 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 1.4481
2022-02-26 17:36:44 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 1.2306
2022-02-26 17:37:22 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 1.1054
2022-02-26 17:37:59 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 1.3980
2022-02-26 17:38:35 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 1.1584
2022-02-26 17:39:12 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 1.1653
2022-02-26 17:39:48 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 1.4649
2022-02-26 17:40:25 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 1.1881
2022-02-26 17:41:04 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 1.0682
2022-02-26 17:41:41 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 1.4847
2022-02-26 17:42:18 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.5061
2022-02-26 17:42:55 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 1.5073
2022-02-26 17:43:32 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 1.3291
2022-02-26 17:44:09 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 1.3804
2022-02-26 17:44:46 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 1.4627
2022-02-26 17:45:20 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 1.1925
2022-02-26 17:45:22 - train: epoch 079, train_loss: 1.3120
2022-02-26 17:46:47 - eval: epoch: 079, acc1: 71.490%, acc5: 90.256%, test_loss: 1.1422, per_image_load_time: 1.104ms, per_image_inference_time: 0.573ms
2022-02-26 17:46:47 - until epoch: 079, best_acc1: 71.490%
2022-02-26 17:46:47 - epoch 080 lr: 0.0010000000000000002
2022-02-26 17:47:30 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 1.1359
2022-02-26 17:48:07 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 1.2751
2022-02-26 17:48:44 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.3788
2022-02-26 17:49:20 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 1.1241
2022-02-26 17:49:56 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 1.3063
2022-02-26 17:50:32 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 1.2348
2022-02-26 17:51:11 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 1.3109
2022-02-26 17:51:49 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 1.1854
2022-02-26 17:52:26 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 1.3347
2022-02-26 17:53:02 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 1.2837
2022-02-26 17:53:38 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 1.3172
2022-02-26 17:54:15 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 1.3125
2022-02-26 17:54:50 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 1.2469
2022-02-26 17:55:27 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 1.3277
2022-02-26 17:56:06 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 1.1741
2022-02-26 17:56:43 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 1.2105
2022-02-26 17:57:19 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 1.2803
2022-02-26 17:57:55 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 1.4445
2022-02-26 17:58:32 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 1.2831
2022-02-26 17:59:07 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 1.4107
2022-02-26 17:59:43 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 1.3433
2022-02-26 18:00:19 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 1.3268
2022-02-26 18:00:58 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 1.1732
2022-02-26 18:01:35 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 1.2525
2022-02-26 18:02:11 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 1.2780
2022-02-26 18:02:48 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 1.3368
2022-02-26 18:03:24 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 1.3970
2022-02-26 18:04:00 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 1.4108
2022-02-26 18:04:36 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 1.1211
2022-02-26 18:05:13 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 1.3258
2022-02-26 18:05:52 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 1.4878
2022-02-26 18:06:28 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 1.0095
2022-02-26 18:07:04 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 1.2563
2022-02-26 18:07:42 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 1.1850
2022-02-26 18:08:17 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 1.2018
2022-02-26 18:08:53 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 1.3321
2022-02-26 18:09:29 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 1.2110
2022-02-26 18:10:07 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 1.3407
2022-02-26 18:10:45 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 1.1707
2022-02-26 18:11:23 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 1.3964
2022-02-26 18:11:59 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 1.3000
2022-02-26 18:12:35 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 1.3254
2022-02-26 18:13:11 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 1.4119
2022-02-26 18:13:47 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 1.2789
2022-02-26 18:14:23 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 1.2353
2022-02-26 18:15:01 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 1.3147
2022-02-26 18:15:38 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 1.4680
2022-02-26 18:16:14 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 1.3444
2022-02-26 18:16:50 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 1.4611
2022-02-26 18:17:25 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 1.1512
2022-02-26 18:17:27 - train: epoch 080, train_loss: 1.3105
2022-02-26 18:18:49 - eval: epoch: 080, acc1: 71.516%, acc5: 90.208%, test_loss: 1.1453, per_image_load_time: 2.599ms, per_image_inference_time: 0.565ms
2022-02-26 18:18:49 - until epoch: 080, best_acc1: 71.516%
2022-02-26 18:18:49 - epoch 081 lr: 0.0010000000000000002
2022-02-26 18:19:32 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 1.2128
2022-02-26 18:20:09 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 1.3205
2022-02-26 18:20:45 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 1.2697
2022-02-26 18:21:22 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 1.3965
2022-02-26 18:21:58 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 1.3934
2022-02-26 18:22:34 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 1.2699
2022-02-26 18:23:11 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 1.1660
2022-02-26 18:23:46 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 1.4879
2022-02-26 18:24:24 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 1.2114
2022-02-26 18:25:02 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 1.2600
2022-02-26 18:25:38 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 1.2412
2022-02-26 18:26:14 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 1.3230
2022-02-26 18:26:51 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 1.2307
2022-02-26 18:27:27 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 1.0230
2022-02-26 18:28:01 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 1.4295
2022-02-26 18:28:38 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 1.2434
2022-02-26 18:29:17 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 1.4232
2022-02-26 18:29:54 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 1.2313
2022-02-26 18:30:31 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 1.1826
2022-02-26 18:31:07 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 1.3934
2022-02-26 18:31:44 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 1.2779
2022-02-26 18:32:20 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 1.2755
2022-02-26 18:32:55 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 1.1688
2022-02-26 18:33:31 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 1.2740
2022-02-26 18:34:10 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 1.4781
2022-02-26 18:34:47 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 1.4602
2022-02-26 18:35:23 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 1.2179
2022-02-26 18:35:59 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 1.2532
2022-02-26 18:36:36 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 1.1207
2022-02-26 18:37:11 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 1.3253
2022-02-26 18:37:47 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 1.2097
2022-02-26 18:38:22 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 1.2855
2022-02-26 18:39:01 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 1.3243
2022-02-26 18:39:37 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 1.3720
2022-02-26 18:40:13 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 1.4238
2022-02-26 18:40:49 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 1.1610
2022-02-26 18:41:26 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 1.4100
2022-02-26 18:42:01 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 1.1253
2022-02-26 18:42:36 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 1.3551
2022-02-26 18:43:11 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 1.1635
2022-02-26 18:43:50 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 1.3652
2022-02-26 18:44:28 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 1.3581
2022-02-26 18:45:05 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 1.2731
2022-02-26 18:45:40 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 1.3885
2022-02-26 18:46:16 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 1.3940
2022-02-26 18:46:51 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 1.2943
2022-02-26 18:47:27 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 1.3954
2022-02-26 18:48:02 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 1.3900
2022-02-26 18:48:41 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 1.2640
2022-02-26 18:49:17 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 1.0728
2022-02-26 18:49:18 - train: epoch 081, train_loss: 1.3072
2022-02-26 18:50:41 - eval: epoch: 081, acc1: 71.438%, acc5: 90.336%, test_loss: 1.1436, per_image_load_time: 2.594ms, per_image_inference_time: 0.568ms
2022-02-26 18:50:41 - until epoch: 081, best_acc1: 71.516%
2022-02-26 18:50:41 - epoch 082 lr: 0.0010000000000000002
2022-02-26 18:51:23 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 1.1357
2022-02-26 18:51:57 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 1.2007
2022-02-26 18:52:32 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 1.3730
2022-02-26 18:53:11 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 1.4405
2022-02-26 18:53:48 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 1.4036
2022-02-26 18:54:24 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 1.1539
2022-02-26 18:55:00 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 1.3955
2022-02-26 18:55:37 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 1.3360
2022-02-26 18:56:12 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 1.3498
2022-02-26 18:56:47 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 1.3910
2022-02-26 18:57:22 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 1.4199
2022-02-26 18:58:01 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 1.3187
2022-02-26 18:58:37 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 1.4459
2022-02-26 18:59:13 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 1.2103
2022-02-26 18:59:49 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 1.2127
2022-02-26 19:00:25 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 1.2719
2022-02-26 19:01:00 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 1.2357
2022-02-26 19:01:35 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 1.2300
2022-02-26 19:02:11 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 1.2142
2022-02-26 19:02:50 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 1.1156
2022-02-26 19:03:27 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 1.3150
2022-02-26 19:04:02 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 1.3484
2022-02-26 19:04:37 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 1.3340
2022-02-26 19:05:12 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 1.4198
2022-02-26 19:05:48 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 1.2385
2022-02-26 19:06:24 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 1.2316
2022-02-26 19:06:59 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 1.2320
2022-02-26 19:07:35 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 1.1515
2022-02-26 19:08:13 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 1.1959
2022-02-26 19:08:49 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 1.3037
2022-02-26 19:09:25 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 1.2200
2022-02-26 19:10:01 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 1.4204
2022-02-26 19:10:37 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 1.3413
2022-02-26 19:11:12 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 1.2432
2022-02-26 19:11:47 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 1.2264
2022-02-26 19:12:24 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 1.2659
2022-02-26 19:13:02 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 1.2182
2022-02-26 19:13:38 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 1.6401
2022-02-26 19:14:15 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 1.3231
2022-02-26 19:14:51 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 1.3073
2022-02-26 19:15:27 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 1.3248
2022-02-26 19:16:03 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 1.4109
2022-02-26 19:16:38 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 1.1488
2022-02-26 19:17:16 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 1.2629
2022-02-26 19:17:53 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 1.3158
2022-02-26 19:18:29 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 1.4611
2022-02-26 19:19:05 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 1.0995
2022-02-26 19:19:41 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 1.2093
2022-02-26 19:20:17 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 1.3618
2022-02-26 19:20:51 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 1.2762
2022-02-26 19:20:53 - train: epoch 082, train_loss: 1.3043
2022-02-26 19:22:15 - eval: epoch: 082, acc1: 71.404%, acc5: 90.252%, test_loss: 1.1418, per_image_load_time: 1.873ms, per_image_inference_time: 0.587ms
2022-02-26 19:22:16 - until epoch: 082, best_acc1: 71.516%
2022-02-26 19:22:16 - epoch 083 lr: 0.0010000000000000002
2022-02-26 19:22:56 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 1.2515
2022-02-26 19:23:34 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 1.2915
2022-02-26 19:24:09 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 1.3395
2022-02-26 19:24:45 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 1.5081
2022-02-26 19:25:21 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 1.2428
2022-02-26 19:25:55 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 1.1902
2022-02-26 19:26:33 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 1.2426
2022-02-26 19:27:11 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 1.1688
2022-02-26 19:27:48 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 1.4755
2022-02-26 19:28:24 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 1.3790
2022-02-26 19:29:01 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 1.3460
2022-02-26 19:29:36 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 1.2527
2022-02-26 19:30:12 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 1.2262
2022-02-26 19:30:47 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 1.4194
2022-02-26 19:31:26 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 1.2470
2022-02-26 19:32:03 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 1.3550
2022-02-26 19:32:39 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 1.3999
2022-02-26 19:33:15 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 1.5177
2022-02-26 19:33:51 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 1.1331
2022-02-26 19:34:27 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 1.0073
2022-02-26 19:35:03 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 1.1778
2022-02-26 19:35:38 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 1.2662
2022-02-26 19:36:17 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 1.3746
2022-02-26 19:36:54 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 1.2336
2022-02-26 19:37:30 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 1.3914
2022-02-26 19:38:06 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 1.1796
2022-02-26 19:38:43 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 1.2692
2022-02-26 19:39:19 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 1.2290
2022-02-26 19:39:54 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 1.2398
2022-02-26 19:40:29 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 1.5029
2022-02-26 19:41:08 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 1.1242
2022-02-26 19:41:46 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 1.2189
2022-02-26 19:42:22 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 1.5115
2022-02-26 19:42:58 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 1.3847
2022-02-26 19:43:34 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 1.3469
2022-02-26 19:44:10 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 1.3723
2022-02-26 19:44:46 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 1.3028
2022-02-26 19:45:22 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 1.1985
2022-02-26 19:46:01 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 1.2978
2022-02-26 19:46:37 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 1.5052
2022-02-26 19:47:15 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 1.2187
2022-02-26 19:47:51 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 1.5446
2022-02-26 19:48:28 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 1.2716
2022-02-26 19:49:04 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 1.1924
2022-02-26 19:49:39 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 1.3892
2022-02-26 19:50:15 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 1.4990
2022-02-26 19:50:54 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 1.3939
2022-02-26 19:51:31 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 1.4304
2022-02-26 19:52:07 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 1.5625
2022-02-26 19:52:43 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 1.3634
2022-02-26 19:52:44 - train: epoch 083, train_loss: 1.3035
2022-02-26 19:54:04 - eval: epoch: 083, acc1: 71.592%, acc5: 90.258%, test_loss: 1.1427, per_image_load_time: 2.492ms, per_image_inference_time: 0.585ms
2022-02-26 19:54:05 - until epoch: 083, best_acc1: 71.592%
2022-02-26 19:54:05 - epoch 084 lr: 0.0010000000000000002
2022-02-26 19:54:46 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 1.3883
2022-02-26 19:55:25 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 1.3042
2022-02-26 19:56:01 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 1.1463
2022-02-26 19:56:38 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 1.3493
2022-02-26 19:57:15 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 1.3783
2022-02-26 19:57:51 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 1.5379
2022-02-26 19:58:27 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 1.3281
2022-02-26 19:59:02 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 1.4095
2022-02-26 19:59:38 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 1.2804
2022-02-26 20:00:17 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 1.4419
2022-02-26 20:00:53 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 1.3219
2022-02-26 20:01:29 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 1.5646
2022-02-26 20:02:06 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 1.2757
2022-02-26 20:02:42 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 1.4105
2022-02-26 20:03:17 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 1.3930
2022-02-26 20:03:52 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 1.1805
2022-02-26 20:04:29 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 1.3732
2022-02-26 20:05:08 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 1.3188
2022-02-26 20:05:44 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 1.4226
2022-02-26 20:06:20 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 1.3922
2022-02-26 20:06:56 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 1.2348
2022-02-26 20:07:32 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 1.0059
2022-02-26 20:08:08 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 1.2968
2022-02-26 20:08:44 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 1.1869
2022-02-26 20:09:20 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 1.3418
2022-02-26 20:09:59 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 1.4049
2022-02-26 20:10:35 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 1.2756
2022-02-26 20:11:11 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 1.3993
2022-02-26 20:11:47 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 1.2351
2022-02-26 20:12:24 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 1.2789
2022-02-26 20:12:59 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 1.1750
2022-02-26 20:13:35 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 1.1742
2022-02-26 20:14:11 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 1.2874
2022-02-26 20:14:49 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 1.2014
2022-02-26 20:15:26 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 1.1272
2022-02-26 20:16:02 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 1.3051
2022-02-26 20:16:38 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 1.4039
2022-02-26 20:17:14 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 1.5312
2022-02-26 20:17:50 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 1.3767
2022-02-26 20:18:25 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 1.4605
2022-02-26 20:19:01 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 1.2604
2022-02-26 20:19:39 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 1.4110
2022-02-26 20:20:15 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 1.2949
2022-02-26 20:20:50 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.5162
2022-02-26 20:21:26 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 1.2302
2022-02-26 20:22:02 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 1.1634
2022-02-26 20:22:36 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 1.5290
2022-02-26 20:23:11 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 1.1882
2022-02-26 20:23:47 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 1.1000
2022-02-26 20:24:23 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 1.3334
2022-02-26 20:24:25 - train: epoch 084, train_loss: 1.3008
2022-02-26 20:25:44 - eval: epoch: 084, acc1: 71.446%, acc5: 90.306%, test_loss: 1.1422, per_image_load_time: 1.787ms, per_image_inference_time: 0.590ms
2022-02-26 20:25:44 - until epoch: 084, best_acc1: 71.592%
2022-02-26 20:25:44 - epoch 085 lr: 0.0010000000000000002
2022-02-26 20:26:25 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 1.2124
2022-02-26 20:27:00 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 1.1393
2022-02-26 20:27:35 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 1.4100
2022-02-26 20:28:11 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 1.1958
2022-02-26 20:28:49 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 1.4761
2022-02-26 20:29:24 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 1.0608
2022-02-26 20:30:00 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 1.1607
2022-02-26 20:30:36 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 1.2051
2022-02-26 20:31:11 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 1.2268
2022-02-26 20:31:45 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 1.3700
2022-02-26 20:32:20 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 1.3311
2022-02-26 20:32:55 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 1.2028
2022-02-26 20:33:33 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 1.2766
2022-02-26 20:34:09 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 1.3372
2022-02-26 20:34:45 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 1.2174
2022-02-26 20:35:20 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 1.2335
2022-02-26 20:35:56 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 1.6114
2022-02-26 20:36:31 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 1.0731
2022-02-26 20:37:07 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 1.1731
2022-02-26 20:37:41 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 1.2229
2022-02-26 20:38:19 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.9842
2022-02-26 20:38:55 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 1.3643
2022-02-26 20:39:31 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 1.0641
2022-02-26 20:40:06 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 1.4399
2022-02-26 20:40:42 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 1.4837
2022-02-26 20:41:18 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 1.3279
2022-02-26 20:41:53 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 1.2334
2022-02-26 20:42:28 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 1.4490
2022-02-26 20:43:05 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 1.4684
2022-02-26 20:43:42 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 1.4452
2022-02-26 20:44:18 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 1.2328
2022-02-26 20:44:54 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 1.2360
2022-02-26 20:45:30 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 1.3650
2022-02-26 20:46:06 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 1.4020
2022-02-26 20:46:43 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 1.3627
2022-02-26 20:47:16 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 1.3377
2022-02-26 20:47:52 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 1.1765
2022-02-26 20:48:30 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 1.1717
2022-02-26 20:49:06 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 1.2011
2022-02-26 20:49:41 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 1.4837
2022-02-26 20:50:17 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 1.3684
2022-02-26 20:50:52 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 1.3768
2022-02-26 20:51:28 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 1.1529
2022-02-26 20:52:03 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 1.3296
2022-02-26 20:52:38 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 1.3700
2022-02-26 20:53:17 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 1.4155
2022-02-26 20:53:52 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.5248
2022-02-26 20:54:29 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 1.1166
2022-02-26 20:55:05 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 1.3098
2022-02-26 20:55:40 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 1.2177
2022-02-26 20:55:41 - train: epoch 085, train_loss: 1.3028
2022-02-26 20:57:01 - eval: epoch: 085, acc1: 71.386%, acc5: 90.210%, test_loss: 1.1437, per_image_load_time: 2.466ms, per_image_inference_time: 0.580ms
2022-02-26 20:57:01 - until epoch: 085, best_acc1: 71.592%
2022-02-26 20:57:01 - epoch 086 lr: 0.0010000000000000002
2022-02-26 20:57:45 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 1.1706
2022-02-26 20:58:22 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 1.2219
2022-02-26 20:58:58 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 1.4184
2022-02-26 20:59:35 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 1.4477
2022-02-26 21:00:12 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 1.2508
2022-02-26 21:00:49 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 1.2250
2022-02-26 21:01:25 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 1.2107
2022-02-26 21:02:00 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 1.4522
2022-02-26 21:02:40 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 1.3926
2022-02-26 21:03:17 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 1.3473
2022-02-26 21:03:55 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 1.3513
2022-02-26 21:04:32 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 1.1837
2022-02-26 21:05:08 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 1.2973
2022-02-26 21:05:45 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 1.2187
2022-02-26 21:06:21 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 1.1983
2022-02-26 21:06:59 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 1.3106
2022-02-26 21:07:37 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 1.2033
2022-02-26 21:08:13 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 1.3904
2022-02-26 21:08:49 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 1.4802
2022-02-26 21:09:25 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.3253
2022-02-26 21:10:01 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 1.2377
2022-02-26 21:10:37 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 1.3450
2022-02-26 21:11:13 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 1.2361
2022-02-26 21:11:52 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 1.1232
2022-02-26 21:12:29 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 1.2781
2022-02-26 21:13:06 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 1.3676
2022-02-26 21:13:43 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 1.1538
2022-02-26 21:14:19 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 1.4185
2022-02-26 21:14:55 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 1.1368
2022-02-26 21:15:32 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 1.3393
2022-02-26 21:16:08 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 1.1480
2022-02-26 21:16:46 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 1.3361
2022-02-26 21:17:24 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 1.3768
2022-02-26 21:18:00 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 1.4492
2022-02-26 21:18:36 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 1.3501
2022-02-26 21:19:13 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 1.3546
2022-02-26 21:19:50 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 1.4178
2022-02-26 21:20:25 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 1.3143
2022-02-26 21:21:01 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 1.3566
2022-02-26 21:21:40 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 1.2868
2022-02-26 21:22:16 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 1.3514
2022-02-26 21:22:53 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 1.4020
2022-02-26 21:23:30 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 1.3464
2022-02-26 21:24:06 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 1.2449
2022-02-26 21:24:42 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 1.1236
2022-02-26 21:25:18 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 1.2297
2022-02-26 21:25:55 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 1.2863
2022-02-26 21:26:33 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 1.2597
2022-02-26 21:27:09 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 1.2973
2022-02-26 21:27:45 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 1.2571
2022-02-26 21:27:47 - train: epoch 086, train_loss: 1.2993
2022-02-26 21:29:09 - eval: epoch: 086, acc1: 71.482%, acc5: 90.240%, test_loss: 1.1455, per_image_load_time: 2.249ms, per_image_inference_time: 0.587ms
2022-02-26 21:29:09 - until epoch: 086, best_acc1: 71.592%
2022-02-26 21:29:09 - epoch 087 lr: 0.0010000000000000002
2022-02-26 21:29:50 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 1.3783
2022-02-26 21:30:26 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 1.0914
2022-02-26 21:31:03 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 1.3385
2022-02-26 21:31:41 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 1.4566
2022-02-26 21:32:19 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 1.0337
2022-02-26 21:32:56 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 1.3111
2022-02-26 21:33:33 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 1.1519
2022-02-26 21:34:11 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 1.2906
2022-02-26 21:34:47 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 1.3873
2022-02-26 21:35:23 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 1.2441
2022-02-26 21:36:02 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 1.2990
2022-02-26 21:36:39 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 1.4305
2022-02-26 21:37:15 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 1.4208
2022-02-26 21:37:52 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 1.2993
2022-02-26 21:38:30 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 1.1789
2022-02-26 21:39:09 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 1.1069
2022-02-26 21:39:50 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 1.5313
2022-02-26 21:40:28 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 1.3324
2022-02-26 21:41:06 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 1.3992
2022-02-26 21:41:43 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 1.2918
2022-02-26 21:42:19 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 1.3353
2022-02-26 21:42:55 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 1.3485
2022-02-26 21:43:32 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 1.3363
2022-02-26 21:44:07 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 1.2428
2022-02-26 21:44:41 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 1.2784
2022-02-26 21:45:18 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 1.2565
2022-02-26 21:45:56 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 1.2151
2022-02-26 21:46:32 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 1.0820
2022-02-26 21:47:08 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 1.1658
2022-02-26 21:47:44 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 1.3148
2022-02-26 21:48:21 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 1.2756
2022-02-26 21:48:57 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 1.2246
2022-02-26 21:49:31 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 1.2925
2022-02-26 21:50:07 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 1.2829
2022-02-26 21:50:45 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 1.0812
2022-02-26 21:51:21 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 1.1752
2022-02-26 21:51:57 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 1.2671
2022-02-26 21:52:33 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 1.2713
2022-02-26 21:53:09 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 1.2805
2022-02-26 21:53:45 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 1.4187
2022-02-26 21:54:20 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 1.4109
2022-02-26 21:54:55 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 1.3624
2022-02-26 21:55:34 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 1.3212
2022-02-26 21:56:10 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 1.3275
2022-02-26 21:56:46 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 1.3081
2022-02-26 21:57:23 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 1.3525
2022-02-26 21:57:58 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 1.3245
2022-02-26 21:58:34 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 1.2088
2022-02-26 21:59:09 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 1.2428
2022-02-26 21:59:43 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 1.3002
2022-02-26 21:59:45 - train: epoch 087, train_loss: 1.2968
2022-02-26 22:01:09 - eval: epoch: 087, acc1: 71.574%, acc5: 90.222%, test_loss: 1.1391, per_image_load_time: 1.924ms, per_image_inference_time: 0.576ms
2022-02-26 22:01:09 - until epoch: 087, best_acc1: 71.592%
2022-02-26 22:01:09 - epoch 088 lr: 0.0010000000000000002
2022-02-26 22:01:52 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 1.2324
2022-02-26 22:02:28 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 1.2694
2022-02-26 22:03:04 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 1.3449
2022-02-26 22:03:40 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 1.3051
2022-02-26 22:04:14 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 1.2841
2022-02-26 22:04:52 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 1.3057
2022-02-26 22:05:30 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 1.2545
2022-02-26 22:06:05 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 1.2568
2022-02-26 22:06:42 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 1.2782
2022-02-26 22:07:19 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 1.3100
2022-02-26 22:07:55 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 1.3201
2022-02-26 22:08:32 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 1.2568
2022-02-26 22:09:07 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 1.4147
2022-02-26 22:09:44 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 1.1774
2022-02-26 22:10:23 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 1.3703
2022-02-26 22:10:59 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 1.2391
2022-02-26 22:11:36 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 1.4007
2022-02-26 22:12:12 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 1.2696
2022-02-26 22:12:48 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 1.3574
2022-02-26 22:13:23 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 1.1443
2022-02-26 22:13:58 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 1.2453
2022-02-26 22:14:36 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 1.1828
2022-02-26 22:15:13 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 1.0657
2022-02-26 22:15:51 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 1.3883
2022-02-26 22:16:27 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 1.4511
2022-02-26 22:17:03 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 1.4625
2022-02-26 22:17:39 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 1.3082
2022-02-26 22:18:15 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 1.2792
2022-02-26 22:18:50 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 1.2095
2022-02-26 22:19:28 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 1.3777
2022-02-26 22:20:05 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 1.1877
2022-02-26 22:20:42 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 1.1742
2022-02-26 22:21:18 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 1.1397
2022-02-26 22:21:54 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 1.1464
2022-02-26 22:22:30 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 1.2070
2022-02-26 22:23:06 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 1.2890
2022-02-26 22:23:41 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 1.4234
2022-02-26 22:24:19 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 1.2590
2022-02-26 22:24:57 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 1.4346
2022-02-26 22:25:34 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 1.2060
2022-02-26 22:26:09 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 1.3386
2022-02-26 22:26:45 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 1.4172
2022-02-26 22:27:21 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 1.2452
2022-02-26 22:27:56 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 1.2026
2022-02-26 22:28:31 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 1.3429
2022-02-26 22:29:09 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 1.3368
2022-02-26 22:29:45 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 1.4009
2022-02-26 22:30:21 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 1.1653
2022-02-26 22:30:57 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 1.0748
2022-02-26 22:31:32 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 1.2546
2022-02-26 22:31:33 - train: epoch 088, train_loss: 1.2956
2022-02-26 22:32:53 - eval: epoch: 088, acc1: 71.582%, acc5: 90.308%, test_loss: 1.1374, per_image_load_time: 2.389ms, per_image_inference_time: 0.576ms
2022-02-26 22:32:54 - until epoch: 088, best_acc1: 71.592%
2022-02-26 22:32:54 - epoch 089 lr: 0.0010000000000000002
2022-02-26 22:33:34 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 1.5615
2022-02-26 22:34:10 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 1.1143
2022-02-26 22:34:48 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 1.3647
2022-02-26 22:35:24 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 1.1230
2022-02-26 22:35:59 - train: epoch 0089, iter [00500, 05004], lr: 0.001000, loss: 1.2534
2022-02-26 22:36:36 - train: epoch 0089, iter [00600, 05004], lr: 0.001000, loss: 1.1610
2022-02-26 22:37:12 - train: epoch 0089, iter [00700, 05004], lr: 0.001000, loss: 1.3503
2022-02-26 22:37:47 - train: epoch 0089, iter [00800, 05004], lr: 0.001000, loss: 1.4713
2022-02-26 22:38:22 - train: epoch 0089, iter [00900, 05004], lr: 0.001000, loss: 1.3080
2022-02-26 22:38:57 - train: epoch 0089, iter [01000, 05004], lr: 0.001000, loss: 1.5143
2022-02-26 22:39:35 - train: epoch 0089, iter [01100, 05004], lr: 0.001000, loss: 1.2541
2022-02-26 22:40:11 - train: epoch 0089, iter [01200, 05004], lr: 0.001000, loss: 1.4346
2022-02-26 22:40:47 - train: epoch 0089, iter [01300, 05004], lr: 0.001000, loss: 1.3271
2022-02-26 22:41:23 - train: epoch 0089, iter [01400, 05004], lr: 0.001000, loss: 1.3910
2022-02-26 22:41:58 - train: epoch 0089, iter [01500, 05004], lr: 0.001000, loss: 1.2695
2022-02-26 22:42:33 - train: epoch 0089, iter [01600, 05004], lr: 0.001000, loss: 1.1811
2022-02-26 22:43:10 - train: epoch 0089, iter [01700, 05004], lr: 0.001000, loss: 1.3283
2022-02-26 22:43:45 - train: epoch 0089, iter [01800, 05004], lr: 0.001000, loss: 1.2886
2022-02-26 22:44:23 - train: epoch 0089, iter [01900, 05004], lr: 0.001000, loss: 1.1781
2022-02-26 22:45:01 - train: epoch 0089, iter [02000, 05004], lr: 0.001000, loss: 1.2865
2022-02-26 22:45:36 - train: epoch 0089, iter [02100, 05004], lr: 0.001000, loss: 1.3871
2022-02-26 22:46:10 - train: epoch 0089, iter [02200, 05004], lr: 0.001000, loss: 1.3653
2022-02-26 22:46:47 - train: epoch 0089, iter [02300, 05004], lr: 0.001000, loss: 1.1677
2022-02-26 22:47:23 - train: epoch 0089, iter [02400, 05004], lr: 0.001000, loss: 1.4550
2022-02-26 22:47:58 - train: epoch 0089, iter [02500, 05004], lr: 0.001000, loss: 1.2496
2022-02-26 22:48:34 - train: epoch 0089, iter [02600, 05004], lr: 0.001000, loss: 1.2721
2022-02-26 22:49:13 - train: epoch 0089, iter [02700, 05004], lr: 0.001000, loss: 1.2765
2022-02-26 22:49:49 - train: epoch 0089, iter [02800, 05004], lr: 0.001000, loss: 1.3673
2022-02-26 22:50:25 - train: epoch 0089, iter [02900, 05004], lr: 0.001000, loss: 1.2682
2022-02-26 22:51:01 - train: epoch 0089, iter [03000, 05004], lr: 0.001000, loss: 1.3383
2022-02-26 22:51:38 - train: epoch 0089, iter [03100, 05004], lr: 0.001000, loss: 1.4277
2022-02-26 22:52:14 - train: epoch 0089, iter [03200, 05004], lr: 0.001000, loss: 1.4385
2022-02-26 22:52:48 - train: epoch 0089, iter [03300, 05004], lr: 0.001000, loss: 1.4294
2022-02-26 22:53:26 - train: epoch 0089, iter [03400, 05004], lr: 0.001000, loss: 1.2693
2022-02-26 22:54:03 - train: epoch 0089, iter [03500, 05004], lr: 0.001000, loss: 1.0776
2022-02-26 22:54:38 - train: epoch 0089, iter [03600, 05004], lr: 0.001000, loss: 1.2836
2022-02-26 22:55:14 - train: epoch 0089, iter [03700, 05004], lr: 0.001000, loss: 1.4674
2022-02-26 22:55:49 - train: epoch 0089, iter [03800, 05004], lr: 0.001000, loss: 1.2440
2022-02-26 22:56:26 - train: epoch 0089, iter [03900, 05004], lr: 0.001000, loss: 1.3688
2022-02-26 22:57:00 - train: epoch 0089, iter [04000, 05004], lr: 0.001000, loss: 1.1001
2022-02-26 22:57:35 - train: epoch 0089, iter [04100, 05004], lr: 0.001000, loss: 1.5189
2022-02-26 22:58:12 - train: epoch 0089, iter [04200, 05004], lr: 0.001000, loss: 1.2128
