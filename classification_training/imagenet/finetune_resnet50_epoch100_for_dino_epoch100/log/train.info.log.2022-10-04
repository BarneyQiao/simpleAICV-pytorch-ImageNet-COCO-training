2022-10-08 21:59:02 - network: resnet50
2022-10-08 21:59:02 - num_classes: 1000
2022-10-08 21:59:02 - input_image_size: 224
2022-10-08 21:59:02 - scale: 1.1428571428571428
2022-10-08 21:59:02 - trained_model_path: /root/code/SimpleAICV-ImageNet-CIFAR-COCO-VOC-training/contrastive_learning_training/imagenet/dino_resnet50_epoch100/checkpoints/resnet50_dino_pretrain_model-student-loss2.457.pth
2022-10-08 21:59:02 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-10-08 21:59:02 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-10-08 21:59:02 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5c7807ab80>
2022-10-08 21:59:02 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f5c7807a6d0>
2022-10-08 21:59:02 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5c7807a640>
2022-10-08 21:59:02 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f5c7807a580>
2022-10-08 21:59:02 - seed: 0
2022-10-08 21:59:02 - batch_size: 256
2022-10-08 21:59:02 - num_workers: 20
2022-10-08 21:59:02 - accumulation_steps: 1
2022-10-08 21:59:02 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-10-08 21:59:02 - scheduler: ('MultiStepLR', {'warm_up_epochs': 0, 'gamma': 0.1, 'milestones': [30, 60, 90]})
2022-10-08 21:59:02 - epochs: 100
2022-10-08 21:59:02 - print_interval: 100
2022-10-08 21:59:02 - sync_bn: False
2022-10-08 21:59:02 - apex: True
2022-10-08 21:59:02 - use_ema_model: False
2022-10-08 21:59:02 - ema_model_decay: 0.9999
2022-10-08 21:59:02 - gpus_type: NVIDIA RTX A5000
2022-10-08 21:59:02 - gpus_num: 2
2022-10-08 21:59:02 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f5c57e0b3f0>
2022-10-08 21:59:02 - --------------------parameters--------------------
2022-10-08 21:59:02 - name: conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.0.weight, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.weight, grad: True
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.bias, grad: True
2022-10-08 21:59:02 - name: fc.weight, grad: True
2022-10-08 21:59:02 - name: fc.bias, grad: True
2022-10-08 21:59:02 - --------------------buffers--------------------
2022-10-08 21:59:02 - name: conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.running_mean, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.running_var, grad: False
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.num_batches_tracked, grad: False
2022-10-08 21:59:02 - -----------no weight decay layers--------------
2022-10-08 21:59:02 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-10-08 21:59:02 - -------------weight decay layers---------------
2022-10-08 21:59:02 - name: conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer1.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer2.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.3.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.4.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer3.5.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.0.downsample_conv.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.1.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv1.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv2.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: layer4.2.conv3.layer.0.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-10-08 21:59:02 - epoch 001 lr: 0.100000
2022-10-08 21:59:42 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 5.5496
2022-10-08 22:00:18 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 4.8412
2022-10-08 22:00:53 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 4.4554
2022-10-08 22:01:26 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 4.1731
2022-10-08 22:02:01 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 4.0922
2022-10-08 22:02:35 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 4.0097
2022-10-08 22:03:10 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 3.7936
2022-10-08 22:03:44 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 3.6802
2022-10-08 22:04:19 - train: epoch 0001, iter [00900, 05004], lr: 0.100000, loss: 3.6284
2022-10-08 22:04:52 - train: epoch 0001, iter [01000, 05004], lr: 0.100000, loss: 3.4272
2022-10-08 22:05:25 - train: epoch 0001, iter [01100, 05004], lr: 0.100000, loss: 3.6518
2022-10-08 22:06:00 - train: epoch 0001, iter [01200, 05004], lr: 0.100000, loss: 3.2242
2022-10-08 22:06:35 - train: epoch 0001, iter [01300, 05004], lr: 0.100000, loss: 3.2648
2022-10-08 22:07:09 - train: epoch 0001, iter [01400, 05004], lr: 0.100000, loss: 3.2185
2022-10-08 22:07:43 - train: epoch 0001, iter [01500, 05004], lr: 0.100000, loss: 3.1131
2022-10-08 22:08:17 - train: epoch 0001, iter [01600, 05004], lr: 0.100000, loss: 3.4484
2022-10-08 22:08:52 - train: epoch 0001, iter [01700, 05004], lr: 0.100000, loss: 2.8494
2022-10-08 22:09:26 - train: epoch 0001, iter [01800, 05004], lr: 0.100000, loss: 3.2226
2022-10-08 22:10:00 - train: epoch 0001, iter [01900, 05004], lr: 0.100000, loss: 2.8399
2022-10-08 22:10:35 - train: epoch 0001, iter [02000, 05004], lr: 0.100000, loss: 3.2995
2022-10-08 22:11:08 - train: epoch 0001, iter [02100, 05004], lr: 0.100000, loss: 2.9252
2022-10-08 22:11:42 - train: epoch 0001, iter [02200, 05004], lr: 0.100000, loss: 3.1120
2022-10-08 22:12:16 - train: epoch 0001, iter [02300, 05004], lr: 0.100000, loss: 2.8956
2022-10-08 22:12:50 - train: epoch 0001, iter [02400, 05004], lr: 0.100000, loss: 2.8880
2022-10-08 22:13:23 - train: epoch 0001, iter [02500, 05004], lr: 0.100000, loss: 3.0500
2022-10-08 22:13:58 - train: epoch 0001, iter [02600, 05004], lr: 0.100000, loss: 3.0184
2022-10-08 22:14:32 - train: epoch 0001, iter [02700, 05004], lr: 0.100000, loss: 2.8779
2022-10-08 22:15:06 - train: epoch 0001, iter [02800, 05004], lr: 0.100000, loss: 2.7824
2022-10-08 22:15:41 - train: epoch 0001, iter [02900, 05004], lr: 0.100000, loss: 2.9590
2022-10-08 22:16:15 - train: epoch 0001, iter [03000, 05004], lr: 0.100000, loss: 2.8310
2022-10-08 22:16:48 - train: epoch 0001, iter [03100, 05004], lr: 0.100000, loss: 2.7777
2022-10-08 22:17:23 - train: epoch 0001, iter [03200, 05004], lr: 0.100000, loss: 2.8668
2022-10-08 22:17:56 - train: epoch 0001, iter [03300, 05004], lr: 0.100000, loss: 2.8752
2022-10-08 22:18:31 - train: epoch 0001, iter [03400, 05004], lr: 0.100000, loss: 2.9535
2022-10-08 22:19:05 - train: epoch 0001, iter [03500, 05004], lr: 0.100000, loss: 2.5669
2022-10-08 22:19:39 - train: epoch 0001, iter [03600, 05004], lr: 0.100000, loss: 2.4583
2022-10-08 22:20:13 - train: epoch 0001, iter [03700, 05004], lr: 0.100000, loss: 2.8854
2022-10-08 22:20:47 - train: epoch 0001, iter [03800, 05004], lr: 0.100000, loss: 2.6915
2022-10-08 22:21:21 - train: epoch 0001, iter [03900, 05004], lr: 0.100000, loss: 2.7181
2022-10-08 22:21:55 - train: epoch 0001, iter [04000, 05004], lr: 0.100000, loss: 2.5959
2022-10-08 22:22:29 - train: epoch 0001, iter [04100, 05004], lr: 0.100000, loss: 2.8670
2022-10-08 22:23:03 - train: epoch 0001, iter [04200, 05004], lr: 0.100000, loss: 2.5121
2022-10-08 22:23:37 - train: epoch 0001, iter [04300, 05004], lr: 0.100000, loss: 2.5993
2022-10-08 22:24:11 - train: epoch 0001, iter [04400, 05004], lr: 0.100000, loss: 2.4483
2022-10-08 22:24:45 - train: epoch 0001, iter [04500, 05004], lr: 0.100000, loss: 2.7103
2022-10-08 22:25:19 - train: epoch 0001, iter [04600, 05004], lr: 0.100000, loss: 2.8650
2022-10-08 22:25:53 - train: epoch 0001, iter [04700, 05004], lr: 0.100000, loss: 2.8683
2022-10-08 22:26:29 - train: epoch 0001, iter [04800, 05004], lr: 0.100000, loss: 2.7952
2022-10-08 22:27:03 - train: epoch 0001, iter [04900, 05004], lr: 0.100000, loss: 2.8338
2022-10-08 22:27:35 - train: epoch 0001, iter [05000, 05004], lr: 0.100000, loss: 2.6134
2022-10-08 22:27:37 - train: epoch 001, train_loss: 3.1407
2022-10-08 22:28:55 - eval: epoch: 001, acc1: 44.748%, acc5: 72.468%, test_loss: 2.3874, per_image_load_time: 2.040ms, per_image_inference_time: 0.576ms
2022-10-08 22:28:55 - until epoch: 001, best_acc1: 44.748%
2022-10-08 22:28:55 - epoch 002 lr: 0.100000
2022-10-08 22:29:35 - train: epoch 0002, iter [00100, 05004], lr: 0.100000, loss: 2.4652
2022-10-08 22:30:10 - train: epoch 0002, iter [00200, 05004], lr: 0.100000, loss: 2.4081
2022-10-08 22:30:42 - train: epoch 0002, iter [00300, 05004], lr: 0.100000, loss: 2.6634
2022-10-08 22:31:16 - train: epoch 0002, iter [00400, 05004], lr: 0.100000, loss: 2.4440
2022-10-08 22:31:51 - train: epoch 0002, iter [00500, 05004], lr: 0.100000, loss: 2.5593
2022-10-08 22:32:24 - train: epoch 0002, iter [00600, 05004], lr: 0.100000, loss: 2.3708
2022-10-08 22:32:59 - train: epoch 0002, iter [00700, 05004], lr: 0.100000, loss: 2.6966
2022-10-08 22:33:33 - train: epoch 0002, iter [00800, 05004], lr: 0.100000, loss: 2.3155
2022-10-08 22:34:08 - train: epoch 0002, iter [00900, 05004], lr: 0.100000, loss: 2.3132
2022-10-08 22:34:42 - train: epoch 0002, iter [01000, 05004], lr: 0.100000, loss: 2.4981
2022-10-08 22:35:16 - train: epoch 0002, iter [01100, 05004], lr: 0.100000, loss: 2.5532
2022-10-08 22:35:50 - train: epoch 0002, iter [01200, 05004], lr: 0.100000, loss: 2.4369
2022-10-08 22:36:25 - train: epoch 0002, iter [01300, 05004], lr: 0.100000, loss: 2.5601
2022-10-08 22:36:58 - train: epoch 0002, iter [01400, 05004], lr: 0.100000, loss: 2.5927
2022-10-08 22:37:33 - train: epoch 0002, iter [01500, 05004], lr: 0.100000, loss: 2.4618
2022-10-08 22:38:07 - train: epoch 0002, iter [01600, 05004], lr: 0.100000, loss: 2.3931
2022-10-08 22:38:41 - train: epoch 0002, iter [01700, 05004], lr: 0.100000, loss: 2.5351
2022-10-08 22:39:15 - train: epoch 0002, iter [01800, 05004], lr: 0.100000, loss: 2.5691
2022-10-08 22:39:49 - train: epoch 0002, iter [01900, 05004], lr: 0.100000, loss: 2.5985
2022-10-08 22:40:23 - train: epoch 0002, iter [02000, 05004], lr: 0.100000, loss: 2.1145
2022-10-08 22:40:57 - train: epoch 0002, iter [02100, 05004], lr: 0.100000, loss: 2.5498
2022-10-08 22:41:32 - train: epoch 0002, iter [02200, 05004], lr: 0.100000, loss: 2.2619
2022-10-08 22:42:06 - train: epoch 0002, iter [02300, 05004], lr: 0.100000, loss: 2.2931
2022-10-08 22:42:40 - train: epoch 0002, iter [02400, 05004], lr: 0.100000, loss: 2.2686
2022-10-08 22:43:15 - train: epoch 0002, iter [02500, 05004], lr: 0.100000, loss: 2.3448
2022-10-08 22:43:48 - train: epoch 0002, iter [02600, 05004], lr: 0.100000, loss: 2.4186
2022-10-08 22:44:23 - train: epoch 0002, iter [02700, 05004], lr: 0.100000, loss: 2.5765
2022-10-08 22:44:57 - train: epoch 0002, iter [02800, 05004], lr: 0.100000, loss: 2.3213
2022-10-08 22:45:31 - train: epoch 0002, iter [02900, 05004], lr: 0.100000, loss: 2.3129
2022-10-08 22:46:06 - train: epoch 0002, iter [03000, 05004], lr: 0.100000, loss: 2.3725
2022-10-08 22:46:40 - train: epoch 0002, iter [03100, 05004], lr: 0.100000, loss: 2.2601
2022-10-08 22:47:14 - train: epoch 0002, iter [03200, 05004], lr: 0.100000, loss: 2.2709
2022-10-08 22:47:48 - train: epoch 0002, iter [03300, 05004], lr: 0.100000, loss: 2.3179
2022-10-08 22:48:23 - train: epoch 0002, iter [03400, 05004], lr: 0.100000, loss: 2.1781
2022-10-08 22:48:57 - train: epoch 0002, iter [03500, 05004], lr: 0.100000, loss: 2.1865
2022-10-08 22:49:31 - train: epoch 0002, iter [03600, 05004], lr: 0.100000, loss: 2.6081
2022-10-08 22:50:06 - train: epoch 0002, iter [03700, 05004], lr: 0.100000, loss: 2.4186
2022-10-08 22:50:40 - train: epoch 0002, iter [03800, 05004], lr: 0.100000, loss: 2.3275
2022-10-08 22:51:15 - train: epoch 0002, iter [03900, 05004], lr: 0.100000, loss: 2.3706
2022-10-08 22:51:48 - train: epoch 0002, iter [04000, 05004], lr: 0.100000, loss: 2.1776
2022-10-08 22:52:23 - train: epoch 0002, iter [04100, 05004], lr: 0.100000, loss: 2.5584
2022-10-08 22:52:57 - train: epoch 0002, iter [04200, 05004], lr: 0.100000, loss: 2.3962
2022-10-08 22:53:32 - train: epoch 0002, iter [04300, 05004], lr: 0.100000, loss: 2.3759
2022-10-08 22:54:06 - train: epoch 0002, iter [04400, 05004], lr: 0.100000, loss: 2.1759
2022-10-08 22:54:40 - train: epoch 0002, iter [04500, 05004], lr: 0.100000, loss: 2.2073
2022-10-08 22:55:16 - train: epoch 0002, iter [04600, 05004], lr: 0.100000, loss: 2.0735
2022-10-08 22:56:00 - train: epoch 0002, iter [04700, 05004], lr: 0.100000, loss: 2.5116
2022-10-08 22:56:57 - train: epoch 0002, iter [04800, 05004], lr: 0.100000, loss: 2.3307
2022-10-08 22:57:44 - train: epoch 0002, iter [04900, 05004], lr: 0.100000, loss: 2.3443
2022-10-08 22:58:19 - train: epoch 0002, iter [05000, 05004], lr: 0.100000, loss: 2.3256
2022-10-08 22:58:21 - train: epoch 002, train_loss: 2.4016
2022-10-08 22:59:38 - eval: epoch: 002, acc1: 48.742%, acc5: 75.260%, test_loss: 2.2145, per_image_load_time: 0.954ms, per_image_inference_time: 0.590ms
2022-10-08 22:59:38 - until epoch: 002, best_acc1: 48.742%
2022-10-08 22:59:38 - epoch 003 lr: 0.100000
2022-10-08 23:00:17 - train: epoch 0003, iter [00100, 05004], lr: 0.100000, loss: 2.4011
2022-10-08 23:00:52 - train: epoch 0003, iter [00200, 05004], lr: 0.100000, loss: 2.2919
2022-10-08 23:01:27 - train: epoch 0003, iter [00300, 05004], lr: 0.100000, loss: 2.1298
2022-10-08 23:02:01 - train: epoch 0003, iter [00400, 05004], lr: 0.100000, loss: 2.4208
2022-10-08 23:02:34 - train: epoch 0003, iter [00500, 05004], lr: 0.100000, loss: 2.4184
2022-10-08 23:03:09 - train: epoch 0003, iter [00600, 05004], lr: 0.100000, loss: 2.1898
2022-10-08 23:03:43 - train: epoch 0003, iter [00700, 05004], lr: 0.100000, loss: 2.3111
2022-10-08 23:04:18 - train: epoch 0003, iter [00800, 05004], lr: 0.100000, loss: 2.3682
2022-10-08 23:04:53 - train: epoch 0003, iter [00900, 05004], lr: 0.100000, loss: 2.2594
2022-10-08 23:05:27 - train: epoch 0003, iter [01000, 05004], lr: 0.100000, loss: 2.2041
2022-10-08 23:06:01 - train: epoch 0003, iter [01100, 05004], lr: 0.100000, loss: 2.2169
2022-10-08 23:06:35 - train: epoch 0003, iter [01200, 05004], lr: 0.100000, loss: 2.3154
2022-10-08 23:07:09 - train: epoch 0003, iter [01300, 05004], lr: 0.100000, loss: 2.2151
2022-10-08 23:07:43 - train: epoch 0003, iter [01400, 05004], lr: 0.100000, loss: 2.2472
2022-10-08 23:08:17 - train: epoch 0003, iter [01500, 05004], lr: 0.100000, loss: 2.4148
2022-10-08 23:08:51 - train: epoch 0003, iter [01600, 05004], lr: 0.100000, loss: 2.0556
2022-10-08 23:09:26 - train: epoch 0003, iter [01700, 05004], lr: 0.100000, loss: 2.2398
2022-10-08 23:10:00 - train: epoch 0003, iter [01800, 05004], lr: 0.100000, loss: 2.2145
2022-10-08 23:10:34 - train: epoch 0003, iter [01900, 05004], lr: 0.100000, loss: 2.2551
2022-10-08 23:11:08 - train: epoch 0003, iter [02000, 05004], lr: 0.100000, loss: 2.2987
2022-10-08 23:11:43 - train: epoch 0003, iter [02100, 05004], lr: 0.100000, loss: 2.1598
2022-10-08 23:12:17 - train: epoch 0003, iter [02200, 05004], lr: 0.100000, loss: 2.5719
2022-10-08 23:12:51 - train: epoch 0003, iter [02300, 05004], lr: 0.100000, loss: 2.1737
2022-10-08 23:13:25 - train: epoch 0003, iter [02400, 05004], lr: 0.100000, loss: 1.9819
2022-10-08 23:14:00 - train: epoch 0003, iter [02500, 05004], lr: 0.100000, loss: 2.1555
2022-10-08 23:14:34 - train: epoch 0003, iter [02600, 05004], lr: 0.100000, loss: 2.2000
2022-10-08 23:15:08 - train: epoch 0003, iter [02700, 05004], lr: 0.100000, loss: 2.7084
2022-10-08 23:15:43 - train: epoch 0003, iter [02800, 05004], lr: 0.100000, loss: 2.1992
2022-10-08 23:16:17 - train: epoch 0003, iter [02900, 05004], lr: 0.100000, loss: 2.2262
2022-10-08 23:16:51 - train: epoch 0003, iter [03000, 05004], lr: 0.100000, loss: 2.3012
2022-10-08 23:17:25 - train: epoch 0003, iter [03100, 05004], lr: 0.100000, loss: 2.5317
2022-10-08 23:18:00 - train: epoch 0003, iter [03200, 05004], lr: 0.100000, loss: 2.2652
2022-10-08 23:18:36 - train: epoch 0003, iter [03300, 05004], lr: 0.100000, loss: 2.1915
2022-10-08 23:19:10 - train: epoch 0003, iter [03400, 05004], lr: 0.100000, loss: 2.3514
2022-10-08 23:19:44 - train: epoch 0003, iter [03500, 05004], lr: 0.100000, loss: 2.1869
2022-10-08 23:20:19 - train: epoch 0003, iter [03600, 05004], lr: 0.100000, loss: 2.1773
2022-10-08 23:20:56 - train: epoch 0003, iter [03700, 05004], lr: 0.100000, loss: 2.4188
2022-10-08 23:22:02 - train: epoch 0003, iter [03800, 05004], lr: 0.100000, loss: 2.3883
2022-10-08 23:24:39 - train: epoch 0003, iter [03900, 05004], lr: 0.100000, loss: 2.5636
2022-10-08 23:26:58 - train: epoch 0003, iter [04000, 05004], lr: 0.100000, loss: 2.0663
2022-10-08 23:29:43 - train: epoch 0003, iter [04100, 05004], lr: 0.100000, loss: 2.1035
2022-10-08 23:31:39 - train: epoch 0003, iter [04200, 05004], lr: 0.100000, loss: 2.3047
2022-10-08 23:33:23 - train: epoch 0003, iter [04300, 05004], lr: 0.100000, loss: 2.2670
2022-10-08 23:34:15 - train: epoch 0003, iter [04400, 05004], lr: 0.100000, loss: 2.2954
2022-10-08 23:34:55 - train: epoch 0003, iter [04500, 05004], lr: 0.100000, loss: 2.2698
2022-10-08 23:35:33 - train: epoch 0003, iter [04600, 05004], lr: 0.100000, loss: 2.1769
2022-10-08 23:36:20 - train: epoch 0003, iter [04700, 05004], lr: 0.100000, loss: 2.1873
2022-10-08 23:36:55 - train: epoch 0003, iter [04800, 05004], lr: 0.100000, loss: 2.1863
2022-10-08 23:37:47 - train: epoch 0003, iter [04900, 05004], lr: 0.100000, loss: 2.4025
2022-10-08 23:38:26 - train: epoch 0003, iter [05000, 05004], lr: 0.100000, loss: 2.2129
2022-10-08 23:38:28 - train: epoch 003, train_loss: 2.2537
2022-10-08 23:39:46 - eval: epoch: 003, acc1: 50.756%, acc5: 76.984%, test_loss: 2.1012, per_image_load_time: 2.292ms, per_image_inference_time: 0.575ms
2022-10-08 23:39:46 - until epoch: 003, best_acc1: 50.756%
2022-10-08 23:39:46 - epoch 004 lr: 0.100000
2022-10-08 23:40:26 - train: epoch 0004, iter [00100, 05004], lr: 0.100000, loss: 2.1086
2022-10-08 23:41:01 - train: epoch 0004, iter [00200, 05004], lr: 0.100000, loss: 2.1309
2022-10-08 23:41:35 - train: epoch 0004, iter [00300, 05004], lr: 0.100000, loss: 2.1831
2022-10-08 23:42:09 - train: epoch 0004, iter [00400, 05004], lr: 0.100000, loss: 2.3052
2022-10-08 23:42:43 - train: epoch 0004, iter [00500, 05004], lr: 0.100000, loss: 2.0747
2022-10-08 23:43:17 - train: epoch 0004, iter [00600, 05004], lr: 0.100000, loss: 2.3457
2022-10-08 23:43:51 - train: epoch 0004, iter [00700, 05004], lr: 0.100000, loss: 2.2469
2022-10-08 23:44:26 - train: epoch 0004, iter [00800, 05004], lr: 0.100000, loss: 2.1110
2022-10-08 23:45:01 - train: epoch 0004, iter [00900, 05004], lr: 0.100000, loss: 1.9259
2022-10-08 23:45:35 - train: epoch 0004, iter [01000, 05004], lr: 0.100000, loss: 2.4006
2022-10-08 23:46:10 - train: epoch 0004, iter [01100, 05004], lr: 0.100000, loss: 2.3132
2022-10-08 23:46:44 - train: epoch 0004, iter [01200, 05004], lr: 0.100000, loss: 2.0670
2022-10-08 23:47:19 - train: epoch 0004, iter [01300, 05004], lr: 0.100000, loss: 2.0760
2022-10-08 23:47:52 - train: epoch 0004, iter [01400, 05004], lr: 0.100000, loss: 2.1769
2022-10-08 23:48:27 - train: epoch 0004, iter [01500, 05004], lr: 0.100000, loss: 2.4177
2022-10-08 23:49:01 - train: epoch 0004, iter [01600, 05004], lr: 0.100000, loss: 1.9927
2022-10-08 23:49:35 - train: epoch 0004, iter [01700, 05004], lr: 0.100000, loss: 2.3039
2022-10-08 23:50:10 - train: epoch 0004, iter [01800, 05004], lr: 0.100000, loss: 2.1900
2022-10-08 23:50:44 - train: epoch 0004, iter [01900, 05004], lr: 0.100000, loss: 2.0981
2022-10-08 23:51:18 - train: epoch 0004, iter [02000, 05004], lr: 0.100000, loss: 2.3085
2022-10-08 23:51:54 - train: epoch 0004, iter [02100, 05004], lr: 0.100000, loss: 2.1847
2022-10-08 23:52:28 - train: epoch 0004, iter [02200, 05004], lr: 0.100000, loss: 2.2066
2022-10-08 23:53:02 - train: epoch 0004, iter [02300, 05004], lr: 0.100000, loss: 1.8654
2022-10-08 23:53:37 - train: epoch 0004, iter [02400, 05004], lr: 0.100000, loss: 2.0096
2022-10-08 23:54:11 - train: epoch 0004, iter [02500, 05004], lr: 0.100000, loss: 2.2968
2022-10-08 23:54:45 - train: epoch 0004, iter [02600, 05004], lr: 0.100000, loss: 2.2742
2022-10-08 23:55:20 - train: epoch 0004, iter [02700, 05004], lr: 0.100000, loss: 2.0471
2022-10-08 23:55:55 - train: epoch 0004, iter [02800, 05004], lr: 0.100000, loss: 2.2954
2022-10-08 23:56:29 - train: epoch 0004, iter [02900, 05004], lr: 0.100000, loss: 2.1509
2022-10-08 23:57:03 - train: epoch 0004, iter [03000, 05004], lr: 0.100000, loss: 2.1220
2022-10-08 23:57:38 - train: epoch 0004, iter [03100, 05004], lr: 0.100000, loss: 2.0196
2022-10-08 23:58:12 - train: epoch 0004, iter [03200, 05004], lr: 0.100000, loss: 2.2070
2022-10-08 23:58:47 - train: epoch 0004, iter [03300, 05004], lr: 0.100000, loss: 2.2689
2022-10-08 23:59:21 - train: epoch 0004, iter [03400, 05004], lr: 0.100000, loss: 2.2772
2022-10-08 23:59:55 - train: epoch 0004, iter [03500, 05004], lr: 0.100000, loss: 2.2766
2022-10-09 00:00:29 - train: epoch 0004, iter [03600, 05004], lr: 0.100000, loss: 2.1164
2022-10-09 00:01:04 - train: epoch 0004, iter [03700, 05004], lr: 0.100000, loss: 2.1404
2022-10-09 00:01:38 - train: epoch 0004, iter [03800, 05004], lr: 0.100000, loss: 1.9034
2022-10-09 00:02:13 - train: epoch 0004, iter [03900, 05004], lr: 0.100000, loss: 1.9611
2022-10-09 00:02:47 - train: epoch 0004, iter [04000, 05004], lr: 0.100000, loss: 1.8982
2022-10-09 00:03:21 - train: epoch 0004, iter [04100, 05004], lr: 0.100000, loss: 1.9975
2022-10-09 00:03:55 - train: epoch 0004, iter [04200, 05004], lr: 0.100000, loss: 2.1496
2022-10-09 00:04:29 - train: epoch 0004, iter [04300, 05004], lr: 0.100000, loss: 1.9402
2022-10-09 00:05:03 - train: epoch 0004, iter [04400, 05004], lr: 0.100000, loss: 2.0821
2022-10-09 00:05:37 - train: epoch 0004, iter [04500, 05004], lr: 0.100000, loss: 1.7444
2022-10-09 00:06:12 - train: epoch 0004, iter [04600, 05004], lr: 0.100000, loss: 2.0915
2022-10-09 00:06:46 - train: epoch 0004, iter [04700, 05004], lr: 0.100000, loss: 2.2206
2022-10-09 00:07:19 - train: epoch 0004, iter [04800, 05004], lr: 0.100000, loss: 2.1311
2022-10-09 00:07:53 - train: epoch 0004, iter [04900, 05004], lr: 0.100000, loss: 2.1145
2022-10-09 00:08:26 - train: epoch 0004, iter [05000, 05004], lr: 0.100000, loss: 2.1105
2022-10-09 00:08:28 - train: epoch 004, train_loss: 2.1775
2022-10-09 00:09:45 - eval: epoch: 004, acc1: 52.714%, acc5: 78.666%, test_loss: 2.0089, per_image_load_time: 1.302ms, per_image_inference_time: 0.597ms
2022-10-09 00:09:45 - until epoch: 004, best_acc1: 52.714%
2022-10-09 00:09:45 - epoch 005 lr: 0.100000
2022-10-09 00:10:24 - train: epoch 0005, iter [00100, 05004], lr: 0.100000, loss: 2.1552
2022-10-09 00:10:59 - train: epoch 0005, iter [00200, 05004], lr: 0.100000, loss: 2.3046
2022-10-09 00:11:33 - train: epoch 0005, iter [00300, 05004], lr: 0.100000, loss: 2.4921
2022-10-09 00:12:08 - train: epoch 0005, iter [00400, 05004], lr: 0.100000, loss: 2.1877
2022-10-09 00:12:41 - train: epoch 0005, iter [00500, 05004], lr: 0.100000, loss: 2.1177
2022-10-09 00:13:15 - train: epoch 0005, iter [00600, 05004], lr: 0.100000, loss: 2.1855
2022-10-09 00:13:50 - train: epoch 0005, iter [00700, 05004], lr: 0.100000, loss: 2.2397
2022-10-09 00:14:25 - train: epoch 0005, iter [00800, 05004], lr: 0.100000, loss: 2.1383
2022-10-09 00:14:59 - train: epoch 0005, iter [00900, 05004], lr: 0.100000, loss: 2.1499
2022-10-09 00:15:33 - train: epoch 0005, iter [01000, 05004], lr: 0.100000, loss: 2.1671
2022-10-09 00:16:08 - train: epoch 0005, iter [01100, 05004], lr: 0.100000, loss: 2.2229
2022-10-09 00:16:41 - train: epoch 0005, iter [01200, 05004], lr: 0.100000, loss: 2.0940
2022-10-09 00:17:16 - train: epoch 0005, iter [01300, 05004], lr: 0.100000, loss: 1.9147
2022-10-09 00:17:52 - train: epoch 0005, iter [01400, 05004], lr: 0.100000, loss: 2.1071
2022-10-09 00:18:26 - train: epoch 0005, iter [01500, 05004], lr: 0.100000, loss: 1.7983
2022-10-09 00:19:00 - train: epoch 0005, iter [01600, 05004], lr: 0.100000, loss: 1.8887
2022-10-09 00:19:34 - train: epoch 0005, iter [01700, 05004], lr: 0.100000, loss: 2.0951
2022-10-09 00:20:09 - train: epoch 0005, iter [01800, 05004], lr: 0.100000, loss: 2.0639
2022-10-09 00:20:43 - train: epoch 0005, iter [01900, 05004], lr: 0.100000, loss: 2.0235
2022-10-09 00:21:17 - train: epoch 0005, iter [02000, 05004], lr: 0.100000, loss: 2.2923
2022-10-09 00:21:52 - train: epoch 0005, iter [02100, 05004], lr: 0.100000, loss: 1.9222
2022-10-09 00:22:26 - train: epoch 0005, iter [02200, 05004], lr: 0.100000, loss: 2.1315
2022-10-09 00:23:01 - train: epoch 0005, iter [02300, 05004], lr: 0.100000, loss: 1.9986
2022-10-09 00:23:36 - train: epoch 0005, iter [02400, 05004], lr: 0.100000, loss: 2.2213
2022-10-09 00:24:10 - train: epoch 0005, iter [02500, 05004], lr: 0.100000, loss: 2.0358
2022-10-09 00:24:45 - train: epoch 0005, iter [02600, 05004], lr: 0.100000, loss: 2.2732
2022-10-09 00:25:20 - train: epoch 0005, iter [02700, 05004], lr: 0.100000, loss: 2.2018
2022-10-09 00:25:54 - train: epoch 0005, iter [02800, 05004], lr: 0.100000, loss: 2.0669
2022-10-09 00:26:29 - train: epoch 0005, iter [02900, 05004], lr: 0.100000, loss: 2.0418
2022-10-09 00:27:03 - train: epoch 0005, iter [03000, 05004], lr: 0.100000, loss: 1.9024
2022-10-09 00:27:37 - train: epoch 0005, iter [03100, 05004], lr: 0.100000, loss: 2.4007
2022-10-09 00:28:14 - train: epoch 0005, iter [03200, 05004], lr: 0.100000, loss: 2.0591
2022-10-09 00:28:48 - train: epoch 0005, iter [03300, 05004], lr: 0.100000, loss: 2.1684
2022-10-09 00:29:26 - train: epoch 0005, iter [03400, 05004], lr: 0.100000, loss: 2.0161
2022-10-09 00:29:59 - train: epoch 0005, iter [03500, 05004], lr: 0.100000, loss: 2.3282
2022-10-09 00:30:34 - train: epoch 0005, iter [03600, 05004], lr: 0.100000, loss: 2.1419
2022-10-09 00:31:14 - train: epoch 0005, iter [03700, 05004], lr: 0.100000, loss: 2.1292
2022-10-09 00:31:50 - train: epoch 0005, iter [03800, 05004], lr: 0.100000, loss: 1.9868
2022-10-09 00:32:25 - train: epoch 0005, iter [03900, 05004], lr: 0.100000, loss: 2.3924
2022-10-09 00:32:59 - train: epoch 0005, iter [04000, 05004], lr: 0.100000, loss: 2.2269
2022-10-09 00:33:33 - train: epoch 0005, iter [04100, 05004], lr: 0.100000, loss: 1.9986
2022-10-09 00:34:08 - train: epoch 0005, iter [04200, 05004], lr: 0.100000, loss: 2.2866
2022-10-09 00:34:42 - train: epoch 0005, iter [04300, 05004], lr: 0.100000, loss: 2.2400
2022-10-09 00:35:17 - train: epoch 0005, iter [04400, 05004], lr: 0.100000, loss: 2.1887
2022-10-09 00:35:51 - train: epoch 0005, iter [04500, 05004], lr: 0.100000, loss: 2.1970
2022-10-09 00:36:25 - train: epoch 0005, iter [04600, 05004], lr: 0.100000, loss: 1.8627
2022-10-09 00:36:59 - train: epoch 0005, iter [04700, 05004], lr: 0.100000, loss: 2.0251
2022-10-09 00:37:35 - train: epoch 0005, iter [04800, 05004], lr: 0.100000, loss: 2.0007
2022-10-09 00:38:09 - train: epoch 0005, iter [04900, 05004], lr: 0.100000, loss: 2.3901
2022-10-09 00:38:41 - train: epoch 0005, iter [05000, 05004], lr: 0.100000, loss: 2.1866
2022-10-09 00:38:44 - train: epoch 005, train_loss: 2.1327
2022-10-09 00:40:00 - eval: epoch: 005, acc1: 53.088%, acc5: 79.114%, test_loss: 1.9752, per_image_load_time: 1.299ms, per_image_inference_time: 0.584ms
2022-10-09 00:40:00 - until epoch: 005, best_acc1: 53.088%
2022-10-09 00:40:00 - epoch 006 lr: 0.100000
2022-10-09 00:40:41 - train: epoch 0006, iter [00100, 05004], lr: 0.100000, loss: 2.0444
2022-10-09 00:41:15 - train: epoch 0006, iter [00200, 05004], lr: 0.100000, loss: 2.2471
2022-10-09 00:41:50 - train: epoch 0006, iter [00300, 05004], lr: 0.100000, loss: 2.0315
2022-10-09 00:42:22 - train: epoch 0006, iter [00400, 05004], lr: 0.100000, loss: 2.0826
2022-10-09 00:42:57 - train: epoch 0006, iter [00500, 05004], lr: 0.100000, loss: 1.7790
2022-10-09 00:43:32 - train: epoch 0006, iter [00600, 05004], lr: 0.100000, loss: 2.2365
2022-10-09 00:44:06 - train: epoch 0006, iter [00700, 05004], lr: 0.100000, loss: 2.2310
2022-10-09 00:44:40 - train: epoch 0006, iter [00800, 05004], lr: 0.100000, loss: 2.1199
2022-10-09 00:45:15 - train: epoch 0006, iter [00900, 05004], lr: 0.100000, loss: 2.0717
2022-10-09 00:45:49 - train: epoch 0006, iter [01000, 05004], lr: 0.100000, loss: 2.0380
2022-10-09 00:46:22 - train: epoch 0006, iter [01100, 05004], lr: 0.100000, loss: 2.1909
2022-10-09 00:46:56 - train: epoch 0006, iter [01200, 05004], lr: 0.100000, loss: 2.2866
2022-10-09 00:47:31 - train: epoch 0006, iter [01300, 05004], lr: 0.100000, loss: 2.0121
2022-10-09 00:48:05 - train: epoch 0006, iter [01400, 05004], lr: 0.100000, loss: 2.0459
2022-10-09 00:48:39 - train: epoch 0006, iter [01500, 05004], lr: 0.100000, loss: 2.3879
2022-10-09 00:49:12 - train: epoch 0006, iter [01600, 05004], lr: 0.100000, loss: 2.0884
2022-10-09 00:49:46 - train: epoch 0006, iter [01700, 05004], lr: 0.100000, loss: 2.2104
2022-10-09 00:50:21 - train: epoch 0006, iter [01800, 05004], lr: 0.100000, loss: 2.0550
2022-10-09 00:50:55 - train: epoch 0006, iter [01900, 05004], lr: 0.100000, loss: 1.9041
2022-10-09 00:51:28 - train: epoch 0006, iter [02000, 05004], lr: 0.100000, loss: 2.0475
2022-10-09 00:52:03 - train: epoch 0006, iter [02100, 05004], lr: 0.100000, loss: 2.2140
2022-10-09 00:52:37 - train: epoch 0006, iter [02200, 05004], lr: 0.100000, loss: 1.9135
2022-10-09 00:53:11 - train: epoch 0006, iter [02300, 05004], lr: 0.100000, loss: 2.0153
2022-10-09 00:53:45 - train: epoch 0006, iter [02400, 05004], lr: 0.100000, loss: 2.0966
2022-10-09 00:54:19 - train: epoch 0006, iter [02500, 05004], lr: 0.100000, loss: 2.2268
2022-10-09 00:54:53 - train: epoch 0006, iter [02600, 05004], lr: 0.100000, loss: 1.9905
2022-10-09 00:55:28 - train: epoch 0006, iter [02700, 05004], lr: 0.100000, loss: 2.2895
2022-10-09 00:56:01 - train: epoch 0006, iter [02800, 05004], lr: 0.100000, loss: 1.8707
2022-10-09 00:56:36 - train: epoch 0006, iter [02900, 05004], lr: 0.100000, loss: 2.1307
2022-10-09 00:57:10 - train: epoch 0006, iter [03000, 05004], lr: 0.100000, loss: 2.1494
2022-10-09 00:57:43 - train: epoch 0006, iter [03100, 05004], lr: 0.100000, loss: 1.9878
2022-10-09 00:58:18 - train: epoch 0006, iter [03200, 05004], lr: 0.100000, loss: 2.1293
2022-10-09 00:58:52 - train: epoch 0006, iter [03300, 05004], lr: 0.100000, loss: 1.9772
2022-10-09 00:59:26 - train: epoch 0006, iter [03400, 05004], lr: 0.100000, loss: 2.2016
2022-10-09 01:00:00 - train: epoch 0006, iter [03500, 05004], lr: 0.100000, loss: 2.2103
2022-10-09 01:00:36 - train: epoch 0006, iter [03600, 05004], lr: 0.100000, loss: 2.2901
2022-10-09 01:01:12 - train: epoch 0006, iter [03700, 05004], lr: 0.100000, loss: 2.0645
2022-10-09 01:01:45 - train: epoch 0006, iter [03800, 05004], lr: 0.100000, loss: 1.9664
2022-10-09 01:02:18 - train: epoch 0006, iter [03900, 05004], lr: 0.100000, loss: 2.1152
2022-10-09 01:02:53 - train: epoch 0006, iter [04000, 05004], lr: 0.100000, loss: 2.1341
2022-10-09 01:03:27 - train: epoch 0006, iter [04100, 05004], lr: 0.100000, loss: 2.1724
2022-10-09 01:04:02 - train: epoch 0006, iter [04200, 05004], lr: 0.100000, loss: 2.0431
2022-10-09 01:04:35 - train: epoch 0006, iter [04300, 05004], lr: 0.100000, loss: 2.2089
2022-10-09 01:05:11 - train: epoch 0006, iter [04400, 05004], lr: 0.100000, loss: 1.8016
2022-10-09 01:05:43 - train: epoch 0006, iter [04500, 05004], lr: 0.100000, loss: 2.0278
2022-10-09 01:06:18 - train: epoch 0006, iter [04600, 05004], lr: 0.100000, loss: 2.1735
2022-10-09 01:06:52 - train: epoch 0006, iter [04700, 05004], lr: 0.100000, loss: 2.0881
2022-10-09 01:07:26 - train: epoch 0006, iter [04800, 05004], lr: 0.100000, loss: 2.3017
2022-10-09 01:08:00 - train: epoch 0006, iter [04900, 05004], lr: 0.100000, loss: 2.4041
2022-10-09 01:08:32 - train: epoch 0006, iter [05000, 05004], lr: 0.100000, loss: 1.8644
2022-10-09 01:08:34 - train: epoch 006, train_loss: 2.1051
2022-10-09 01:09:52 - eval: epoch: 006, acc1: 51.792%, acc5: 77.462%, test_loss: 2.0717, per_image_load_time: 1.656ms, per_image_inference_time: 0.574ms
2022-10-09 01:09:52 - until epoch: 006, best_acc1: 53.088%
2022-10-09 01:09:52 - epoch 007 lr: 0.100000
2022-10-09 01:10:32 - train: epoch 0007, iter [00100, 05004], lr: 0.100000, loss: 1.8860
2022-10-09 01:11:06 - train: epoch 0007, iter [00200, 05004], lr: 0.100000, loss: 2.0202
2022-10-09 01:11:41 - train: epoch 0007, iter [00300, 05004], lr: 0.100000, loss: 2.1979
2022-10-09 01:12:15 - train: epoch 0007, iter [00400, 05004], lr: 0.100000, loss: 2.0434
2022-10-09 01:12:50 - train: epoch 0007, iter [00500, 05004], lr: 0.100000, loss: 2.0008
2022-10-09 01:13:25 - train: epoch 0007, iter [00600, 05004], lr: 0.100000, loss: 2.1357
2022-10-09 01:13:59 - train: epoch 0007, iter [00700, 05004], lr: 0.100000, loss: 1.8606
2022-10-09 01:14:33 - train: epoch 0007, iter [00800, 05004], lr: 0.100000, loss: 2.0332
2022-10-09 01:15:06 - train: epoch 0007, iter [00900, 05004], lr: 0.100000, loss: 2.1632
2022-10-09 01:15:40 - train: epoch 0007, iter [01000, 05004], lr: 0.100000, loss: 2.1477
2022-10-09 01:16:14 - train: epoch 0007, iter [01100, 05004], lr: 0.100000, loss: 1.9151
2022-10-09 01:16:49 - train: epoch 0007, iter [01200, 05004], lr: 0.100000, loss: 2.1419
2022-10-09 01:17:23 - train: epoch 0007, iter [01300, 05004], lr: 0.100000, loss: 1.8149
2022-10-09 01:17:58 - train: epoch 0007, iter [01400, 05004], lr: 0.100000, loss: 2.1823
2022-10-09 01:18:32 - train: epoch 0007, iter [01500, 05004], lr: 0.100000, loss: 2.1417
2022-10-09 01:19:07 - train: epoch 0007, iter [01600, 05004], lr: 0.100000, loss: 2.0303
2022-10-09 01:19:41 - train: epoch 0007, iter [01700, 05004], lr: 0.100000, loss: 2.0650
2022-10-09 01:20:15 - train: epoch 0007, iter [01800, 05004], lr: 0.100000, loss: 2.1732
2022-10-09 01:20:49 - train: epoch 0007, iter [01900, 05004], lr: 0.100000, loss: 2.1209
2022-10-09 01:21:23 - train: epoch 0007, iter [02000, 05004], lr: 0.100000, loss: 1.9247
2022-10-09 01:21:57 - train: epoch 0007, iter [02100, 05004], lr: 0.100000, loss: 2.1858
2022-10-09 01:22:31 - train: epoch 0007, iter [02200, 05004], lr: 0.100000, loss: 1.9363
2022-10-09 01:23:06 - train: epoch 0007, iter [02300, 05004], lr: 0.100000, loss: 2.0918
2022-10-09 01:23:40 - train: epoch 0007, iter [02400, 05004], lr: 0.100000, loss: 2.2602
2022-10-09 01:24:14 - train: epoch 0007, iter [02500, 05004], lr: 0.100000, loss: 2.0975
2022-10-09 01:24:49 - train: epoch 0007, iter [02600, 05004], lr: 0.100000, loss: 1.8727
2022-10-09 01:25:24 - train: epoch 0007, iter [02700, 05004], lr: 0.100000, loss: 1.9214
2022-10-09 01:25:58 - train: epoch 0007, iter [02800, 05004], lr: 0.100000, loss: 2.0054
2022-10-09 01:26:32 - train: epoch 0007, iter [02900, 05004], lr: 0.100000, loss: 1.8910
2022-10-09 01:27:07 - train: epoch 0007, iter [03000, 05004], lr: 0.100000, loss: 2.0441
2022-10-09 01:27:43 - train: epoch 0007, iter [03100, 05004], lr: 0.100000, loss: 1.8673
2022-10-09 01:28:16 - train: epoch 0007, iter [03200, 05004], lr: 0.100000, loss: 1.9644
2022-10-09 01:28:50 - train: epoch 0007, iter [03300, 05004], lr: 0.100000, loss: 2.1739
2022-10-09 01:29:25 - train: epoch 0007, iter [03400, 05004], lr: 0.100000, loss: 2.1399
2022-10-09 01:30:00 - train: epoch 0007, iter [03500, 05004], lr: 0.100000, loss: 2.2192
2022-10-09 01:30:35 - train: epoch 0007, iter [03600, 05004], lr: 0.100000, loss: 1.9146
2022-10-09 01:31:09 - train: epoch 0007, iter [03700, 05004], lr: 0.100000, loss: 2.1521
2022-10-09 01:31:43 - train: epoch 0007, iter [03800, 05004], lr: 0.100000, loss: 2.1169
2022-10-09 01:32:18 - train: epoch 0007, iter [03900, 05004], lr: 0.100000, loss: 2.0173
2022-10-09 01:32:53 - train: epoch 0007, iter [04000, 05004], lr: 0.100000, loss: 2.2855
2022-10-09 01:33:27 - train: epoch 0007, iter [04100, 05004], lr: 0.100000, loss: 2.2519
2022-10-09 01:34:02 - train: epoch 0007, iter [04200, 05004], lr: 0.100000, loss: 2.0714
2022-10-09 01:34:37 - train: epoch 0007, iter [04300, 05004], lr: 0.100000, loss: 2.4425
2022-10-09 01:35:10 - train: epoch 0007, iter [04400, 05004], lr: 0.100000, loss: 1.9106
2022-10-09 01:35:46 - train: epoch 0007, iter [04500, 05004], lr: 0.100000, loss: 2.0937
2022-10-09 01:36:21 - train: epoch 0007, iter [04600, 05004], lr: 0.100000, loss: 2.2331
2022-10-09 01:36:54 - train: epoch 0007, iter [04700, 05004], lr: 0.100000, loss: 1.9556
2022-10-09 01:37:29 - train: epoch 0007, iter [04800, 05004], lr: 0.100000, loss: 2.2462
2022-10-09 01:38:03 - train: epoch 0007, iter [04900, 05004], lr: 0.100000, loss: 2.0553
2022-10-09 01:38:36 - train: epoch 0007, iter [05000, 05004], lr: 0.100000, loss: 2.0403
2022-10-09 01:38:38 - train: epoch 007, train_loss: 2.0770
2022-10-09 01:39:56 - eval: epoch: 007, acc1: 44.184%, acc5: 70.106%, test_loss: 2.6008, per_image_load_time: 2.338ms, per_image_inference_time: 0.533ms
2022-10-09 01:39:57 - until epoch: 007, best_acc1: 53.088%
2022-10-09 01:39:57 - epoch 008 lr: 0.100000
2022-10-09 01:40:38 - train: epoch 0008, iter [00100, 05004], lr: 0.100000, loss: 2.1269
2022-10-09 01:41:15 - train: epoch 0008, iter [00200, 05004], lr: 0.100000, loss: 2.2032
2022-10-09 01:41:50 - train: epoch 0008, iter [00300, 05004], lr: 0.100000, loss: 2.1438
2022-10-09 01:42:24 - train: epoch 0008, iter [00400, 05004], lr: 0.100000, loss: 1.9647
2022-10-09 01:43:00 - train: epoch 0008, iter [00500, 05004], lr: 0.100000, loss: 1.8540
2022-10-09 01:43:34 - train: epoch 0008, iter [00600, 05004], lr: 0.100000, loss: 2.0565
2022-10-09 01:44:09 - train: epoch 0008, iter [00700, 05004], lr: 0.100000, loss: 2.0753
2022-10-09 01:44:44 - train: epoch 0008, iter [00800, 05004], lr: 0.100000, loss: 1.7035
2022-10-09 01:45:19 - train: epoch 0008, iter [00900, 05004], lr: 0.100000, loss: 2.0793
2022-10-09 01:45:53 - train: epoch 0008, iter [01000, 05004], lr: 0.100000, loss: 2.1011
2022-10-09 01:46:28 - train: epoch 0008, iter [01100, 05004], lr: 0.100000, loss: 2.0418
2022-10-09 01:47:02 - train: epoch 0008, iter [01200, 05004], lr: 0.100000, loss: 2.0510
2022-10-09 01:47:37 - train: epoch 0008, iter [01300, 05004], lr: 0.100000, loss: 1.9661
2022-10-09 01:48:12 - train: epoch 0008, iter [01400, 05004], lr: 0.100000, loss: 1.9599
2022-10-09 01:48:46 - train: epoch 0008, iter [01500, 05004], lr: 0.100000, loss: 1.9956
2022-10-09 01:49:21 - train: epoch 0008, iter [01600, 05004], lr: 0.100000, loss: 2.0675
2022-10-09 01:49:55 - train: epoch 0008, iter [01700, 05004], lr: 0.100000, loss: 2.0854
2022-10-09 01:50:31 - train: epoch 0008, iter [01800, 05004], lr: 0.100000, loss: 1.9745
2022-10-09 01:51:04 - train: epoch 0008, iter [01900, 05004], lr: 0.100000, loss: 1.9724
2022-10-09 01:51:39 - train: epoch 0008, iter [02000, 05004], lr: 0.100000, loss: 2.0995
2022-10-09 01:52:12 - train: epoch 0008, iter [02100, 05004], lr: 0.100000, loss: 2.0918
2022-10-09 01:52:48 - train: epoch 0008, iter [02200, 05004], lr: 0.100000, loss: 1.9789
2022-10-09 01:53:21 - train: epoch 0008, iter [02300, 05004], lr: 0.100000, loss: 1.9913
2022-10-09 01:53:56 - train: epoch 0008, iter [02400, 05004], lr: 0.100000, loss: 2.0640
2022-10-09 01:54:31 - train: epoch 0008, iter [02500, 05004], lr: 0.100000, loss: 2.0583
2022-10-09 01:55:05 - train: epoch 0008, iter [02600, 05004], lr: 0.100000, loss: 1.9678
2022-10-09 01:55:40 - train: epoch 0008, iter [02700, 05004], lr: 0.100000, loss: 2.1819
2022-10-09 01:56:13 - train: epoch 0008, iter [02800, 05004], lr: 0.100000, loss: 2.0595
2022-10-09 01:56:48 - train: epoch 0008, iter [02900, 05004], lr: 0.100000, loss: 2.0073
2022-10-09 01:57:22 - train: epoch 0008, iter [03000, 05004], lr: 0.100000, loss: 2.2009
2022-10-09 01:57:57 - train: epoch 0008, iter [03100, 05004], lr: 0.100000, loss: 1.9826
2022-10-09 01:58:44 - train: epoch 0008, iter [03200, 05004], lr: 0.100000, loss: 2.3400
2022-10-09 01:59:17 - train: epoch 0008, iter [03300, 05004], lr: 0.100000, loss: 2.1438
2022-10-09 01:59:54 - train: epoch 0008, iter [03400, 05004], lr: 0.100000, loss: 2.0729
2022-10-09 02:00:28 - train: epoch 0008, iter [03500, 05004], lr: 0.100000, loss: 1.9790
2022-10-09 02:01:05 - train: epoch 0008, iter [03600, 05004], lr: 0.100000, loss: 2.1347
2022-10-09 02:01:39 - train: epoch 0008, iter [03700, 05004], lr: 0.100000, loss: 2.0552
2022-10-09 02:02:29 - train: epoch 0008, iter [03800, 05004], lr: 0.100000, loss: 1.8805
2022-10-09 02:03:03 - train: epoch 0008, iter [03900, 05004], lr: 0.100000, loss: 2.2135
2022-10-09 02:03:38 - train: epoch 0008, iter [04000, 05004], lr: 0.100000, loss: 2.4008
2022-10-09 02:04:13 - train: epoch 0008, iter [04100, 05004], lr: 0.100000, loss: 1.9462
2022-10-09 02:04:47 - train: epoch 0008, iter [04200, 05004], lr: 0.100000, loss: 2.0241
2022-10-09 02:05:21 - train: epoch 0008, iter [04300, 05004], lr: 0.100000, loss: 1.8670
2022-10-09 02:05:55 - train: epoch 0008, iter [04400, 05004], lr: 0.100000, loss: 2.0673
2022-10-09 02:06:30 - train: epoch 0008, iter [04500, 05004], lr: 0.100000, loss: 1.8700
2022-10-09 02:07:05 - train: epoch 0008, iter [04600, 05004], lr: 0.100000, loss: 2.0641
2022-10-09 02:07:40 - train: epoch 0008, iter [04700, 05004], lr: 0.100000, loss: 2.0516
2022-10-09 02:08:43 - train: epoch 0008, iter [04800, 05004], lr: 0.100000, loss: 2.0903
2022-10-09 02:09:43 - train: epoch 0008, iter [04900, 05004], lr: 0.100000, loss: 2.0890
2022-10-09 02:10:56 - train: epoch 0008, iter [05000, 05004], lr: 0.100000, loss: 2.0408
2022-10-09 02:10:58 - train: epoch 008, train_loss: 2.0569
2022-10-09 02:12:16 - eval: epoch: 008, acc1: 53.658%, acc5: 79.312%, test_loss: 1.9573, per_image_load_time: 2.296ms, per_image_inference_time: 0.565ms
2022-10-09 02:12:16 - until epoch: 008, best_acc1: 53.658%
2022-10-09 02:12:16 - epoch 009 lr: 0.100000
2022-10-09 02:12:56 - train: epoch 0009, iter [00100, 05004], lr: 0.100000, loss: 1.6890
2022-10-09 02:13:31 - train: epoch 0009, iter [00200, 05004], lr: 0.100000, loss: 2.1469
2022-10-09 02:14:05 - train: epoch 0009, iter [00300, 05004], lr: 0.100000, loss: 1.8371
2022-10-09 02:14:39 - train: epoch 0009, iter [00400, 05004], lr: 0.100000, loss: 2.2654
2022-10-09 02:15:14 - train: epoch 0009, iter [00500, 05004], lr: 0.100000, loss: 1.9337
2022-10-09 02:15:48 - train: epoch 0009, iter [00600, 05004], lr: 0.100000, loss: 1.9819
2022-10-09 02:16:22 - train: epoch 0009, iter [00700, 05004], lr: 0.100000, loss: 1.9291
2022-10-09 02:16:57 - train: epoch 0009, iter [00800, 05004], lr: 0.100000, loss: 2.0667
2022-10-09 02:17:31 - train: epoch 0009, iter [00900, 05004], lr: 0.100000, loss: 1.9944
2022-10-09 02:18:06 - train: epoch 0009, iter [01000, 05004], lr: 0.100000, loss: 1.8578
2022-10-09 02:18:40 - train: epoch 0009, iter [01100, 05004], lr: 0.100000, loss: 2.1226
2022-10-09 02:19:15 - train: epoch 0009, iter [01200, 05004], lr: 0.100000, loss: 2.0891
2022-10-09 02:19:50 - train: epoch 0009, iter [01300, 05004], lr: 0.100000, loss: 2.1378
2022-10-09 02:20:24 - train: epoch 0009, iter [01400, 05004], lr: 0.100000, loss: 1.8914
2022-10-09 02:20:58 - train: epoch 0009, iter [01500, 05004], lr: 0.100000, loss: 1.9973
2022-10-09 02:21:34 - train: epoch 0009, iter [01600, 05004], lr: 0.100000, loss: 2.1237
2022-10-09 02:22:09 - train: epoch 0009, iter [01700, 05004], lr: 0.100000, loss: 2.0408
2022-10-09 02:22:44 - train: epoch 0009, iter [01800, 05004], lr: 0.100000, loss: 2.0458
2022-10-09 02:23:18 - train: epoch 0009, iter [01900, 05004], lr: 0.100000, loss: 1.9875
2022-10-09 02:23:54 - train: epoch 0009, iter [02000, 05004], lr: 0.100000, loss: 1.9632
2022-10-09 02:24:28 - train: epoch 0009, iter [02100, 05004], lr: 0.100000, loss: 1.9897
2022-10-09 02:25:03 - train: epoch 0009, iter [02200, 05004], lr: 0.100000, loss: 2.1438
2022-10-09 02:25:37 - train: epoch 0009, iter [02300, 05004], lr: 0.100000, loss: 2.0249
2022-10-09 02:26:11 - train: epoch 0009, iter [02400, 05004], lr: 0.100000, loss: 2.0439
2022-10-09 02:26:46 - train: epoch 0009, iter [02500, 05004], lr: 0.100000, loss: 1.8106
2022-10-09 02:27:20 - train: epoch 0009, iter [02600, 05004], lr: 0.100000, loss: 2.2392
2022-10-09 02:27:54 - train: epoch 0009, iter [02700, 05004], lr: 0.100000, loss: 2.0529
2022-10-09 02:28:28 - train: epoch 0009, iter [02800, 05004], lr: 0.100000, loss: 2.3454
2022-10-09 02:29:03 - train: epoch 0009, iter [02900, 05004], lr: 0.100000, loss: 1.8166
2022-10-09 02:29:39 - train: epoch 0009, iter [03000, 05004], lr: 0.100000, loss: 1.9521
2022-10-09 02:30:13 - train: epoch 0009, iter [03100, 05004], lr: 0.100000, loss: 2.1600
2022-10-09 02:30:51 - train: epoch 0009, iter [03200, 05004], lr: 0.100000, loss: 2.1244
2022-10-09 02:31:43 - train: epoch 0009, iter [03300, 05004], lr: 0.100000, loss: 2.0485
2022-10-09 02:32:34 - train: epoch 0009, iter [03400, 05004], lr: 0.100000, loss: 2.2253
2022-10-09 02:33:32 - train: epoch 0009, iter [03500, 05004], lr: 0.100000, loss: 2.1333
2022-10-09 02:34:21 - train: epoch 0009, iter [03600, 05004], lr: 0.100000, loss: 1.9376
2022-10-09 02:35:03 - train: epoch 0009, iter [03700, 05004], lr: 0.100000, loss: 2.0390
2022-10-09 02:35:46 - train: epoch 0009, iter [03800, 05004], lr: 0.100000, loss: 1.9159
2022-10-09 02:36:20 - train: epoch 0009, iter [03900, 05004], lr: 0.100000, loss: 1.8750
2022-10-09 02:37:01 - train: epoch 0009, iter [04000, 05004], lr: 0.100000, loss: 2.1678
2022-10-09 02:37:37 - train: epoch 0009, iter [04100, 05004], lr: 0.100000, loss: 2.0642
2022-10-09 02:38:16 - train: epoch 0009, iter [04200, 05004], lr: 0.100000, loss: 1.8142
2022-10-09 02:38:53 - train: epoch 0009, iter [04300, 05004], lr: 0.100000, loss: 1.9827
2022-10-09 02:39:29 - train: epoch 0009, iter [04400, 05004], lr: 0.100000, loss: 2.2395
2022-10-09 02:40:05 - train: epoch 0009, iter [04500, 05004], lr: 0.100000, loss: 2.0299
2022-10-09 02:40:39 - train: epoch 0009, iter [04600, 05004], lr: 0.100000, loss: 2.0456
2022-10-09 02:41:14 - train: epoch 0009, iter [04700, 05004], lr: 0.100000, loss: 2.3465
2022-10-09 02:41:48 - train: epoch 0009, iter [04800, 05004], lr: 0.100000, loss: 2.0406
2022-10-09 02:42:22 - train: epoch 0009, iter [04900, 05004], lr: 0.100000, loss: 2.1476
2022-10-09 02:42:55 - train: epoch 0009, iter [05000, 05004], lr: 0.100000, loss: 1.7984
2022-10-09 02:42:57 - train: epoch 009, train_loss: 2.0408
2022-10-09 02:44:16 - eval: epoch: 009, acc1: 54.622%, acc5: 80.068%, test_loss: 1.9092, per_image_load_time: 2.420ms, per_image_inference_time: 0.550ms
2022-10-09 02:44:17 - until epoch: 009, best_acc1: 54.622%
2022-10-09 02:44:17 - epoch 010 lr: 0.100000
2022-10-09 02:44:57 - train: epoch 0010, iter [00100, 05004], lr: 0.100000, loss: 1.9286
2022-10-09 02:45:32 - train: epoch 0010, iter [00200, 05004], lr: 0.100000, loss: 1.9956
2022-10-09 02:46:06 - train: epoch 0010, iter [00300, 05004], lr: 0.100000, loss: 1.9500
2022-10-09 02:46:42 - train: epoch 0010, iter [00400, 05004], lr: 0.100000, loss: 2.2353
2022-10-09 02:47:16 - train: epoch 0010, iter [00500, 05004], lr: 0.100000, loss: 1.9715
2022-10-09 02:47:50 - train: epoch 0010, iter [00600, 05004], lr: 0.100000, loss: 2.1580
2022-10-09 02:48:24 - train: epoch 0010, iter [00700, 05004], lr: 0.100000, loss: 2.0923
2022-10-09 02:48:59 - train: epoch 0010, iter [00800, 05004], lr: 0.100000, loss: 1.9863
2022-10-09 02:49:33 - train: epoch 0010, iter [00900, 05004], lr: 0.100000, loss: 1.8551
2022-10-09 02:50:07 - train: epoch 0010, iter [01000, 05004], lr: 0.100000, loss: 1.8177
2022-10-09 02:50:42 - train: epoch 0010, iter [01100, 05004], lr: 0.100000, loss: 1.9374
2022-10-09 02:51:16 - train: epoch 0010, iter [01200, 05004], lr: 0.100000, loss: 2.0646
2022-10-09 02:51:51 - train: epoch 0010, iter [01300, 05004], lr: 0.100000, loss: 1.9078
2022-10-09 02:52:26 - train: epoch 0010, iter [01400, 05004], lr: 0.100000, loss: 2.2516
2022-10-09 02:53:01 - train: epoch 0010, iter [01500, 05004], lr: 0.100000, loss: 1.7646
2022-10-09 02:53:35 - train: epoch 0010, iter [01600, 05004], lr: 0.100000, loss: 2.0104
2022-10-09 02:54:10 - train: epoch 0010, iter [01700, 05004], lr: 0.100000, loss: 2.2601
2022-10-09 02:54:44 - train: epoch 0010, iter [01800, 05004], lr: 0.100000, loss: 1.8869
2022-10-09 02:55:20 - train: epoch 0010, iter [01900, 05004], lr: 0.100000, loss: 2.0303
2022-10-09 02:55:54 - train: epoch 0010, iter [02000, 05004], lr: 0.100000, loss: 2.0980
2022-10-09 02:56:29 - train: epoch 0010, iter [02100, 05004], lr: 0.100000, loss: 2.0377
2022-10-09 02:57:07 - train: epoch 0010, iter [02200, 05004], lr: 0.100000, loss: 2.1098
2022-10-09 02:57:45 - train: epoch 0010, iter [02300, 05004], lr: 0.100000, loss: 2.1424
2022-10-09 02:58:21 - train: epoch 0010, iter [02400, 05004], lr: 0.100000, loss: 2.3312
2022-10-09 02:58:59 - train: epoch 0010, iter [02500, 05004], lr: 0.100000, loss: 2.1158
2022-10-09 02:59:32 - train: epoch 0010, iter [02600, 05004], lr: 0.100000, loss: 1.9698
2022-10-09 03:00:07 - train: epoch 0010, iter [02700, 05004], lr: 0.100000, loss: 1.8729
2022-10-09 03:00:42 - train: epoch 0010, iter [02800, 05004], lr: 0.100000, loss: 2.0775
2022-10-09 03:01:17 - train: epoch 0010, iter [02900, 05004], lr: 0.100000, loss: 2.2055
2022-10-09 03:01:52 - train: epoch 0010, iter [03000, 05004], lr: 0.100000, loss: 2.1739
2022-10-09 03:02:26 - train: epoch 0010, iter [03100, 05004], lr: 0.100000, loss: 2.0963
2022-10-09 03:03:01 - train: epoch 0010, iter [03200, 05004], lr: 0.100000, loss: 2.1500
2022-10-09 03:03:36 - train: epoch 0010, iter [03300, 05004], lr: 0.100000, loss: 2.3394
2022-10-09 03:04:10 - train: epoch 0010, iter [03400, 05004], lr: 0.100000, loss: 2.0424
2022-10-09 03:04:44 - train: epoch 0010, iter [03500, 05004], lr: 0.100000, loss: 2.2651
2022-10-09 03:05:18 - train: epoch 0010, iter [03600, 05004], lr: 0.100000, loss: 2.1936
2022-10-09 03:05:53 - train: epoch 0010, iter [03700, 05004], lr: 0.100000, loss: 2.1225
2022-10-09 03:06:28 - train: epoch 0010, iter [03800, 05004], lr: 0.100000, loss: 2.2054
2022-10-09 03:07:03 - train: epoch 0010, iter [03900, 05004], lr: 0.100000, loss: 1.7879
2022-10-09 03:07:38 - train: epoch 0010, iter [04000, 05004], lr: 0.100000, loss: 1.9469
2022-10-09 03:08:12 - train: epoch 0010, iter [04100, 05004], lr: 0.100000, loss: 1.9387
2022-10-09 03:08:47 - train: epoch 0010, iter [04200, 05004], lr: 0.100000, loss: 2.1197
2022-10-09 03:09:20 - train: epoch 0010, iter [04300, 05004], lr: 0.100000, loss: 1.9545
2022-10-09 03:09:55 - train: epoch 0010, iter [04400, 05004], lr: 0.100000, loss: 1.8492
2022-10-09 03:10:30 - train: epoch 0010, iter [04500, 05004], lr: 0.100000, loss: 1.9806
2022-10-09 03:11:05 - train: epoch 0010, iter [04600, 05004], lr: 0.100000, loss: 1.8822
2022-10-09 03:11:40 - train: epoch 0010, iter [04700, 05004], lr: 0.100000, loss: 2.0350
2022-10-09 03:12:15 - train: epoch 0010, iter [04800, 05004], lr: 0.100000, loss: 2.0374
2022-10-09 03:12:49 - train: epoch 0010, iter [04900, 05004], lr: 0.100000, loss: 2.1552
2022-10-09 03:13:22 - train: epoch 0010, iter [05000, 05004], lr: 0.100000, loss: 1.8353
2022-10-09 03:13:24 - train: epoch 010, train_loss: 2.0283
2022-10-09 03:14:42 - eval: epoch: 010, acc1: 54.990%, acc5: 79.760%, test_loss: 1.9172, per_image_load_time: 2.269ms, per_image_inference_time: 0.585ms
2022-10-09 03:14:42 - until epoch: 010, best_acc1: 54.990%
2022-10-09 03:14:42 - epoch 011 lr: 0.100000
2022-10-09 03:15:23 - train: epoch 0011, iter [00100, 05004], lr: 0.100000, loss: 1.8221
2022-10-09 03:15:56 - train: epoch 0011, iter [00200, 05004], lr: 0.100000, loss: 1.9440
2022-10-09 03:16:31 - train: epoch 0011, iter [00300, 05004], lr: 0.100000, loss: 2.0708
2022-10-09 03:17:05 - train: epoch 0011, iter [00400, 05004], lr: 0.100000, loss: 1.9407
2022-10-09 03:17:39 - train: epoch 0011, iter [00500, 05004], lr: 0.100000, loss: 1.9601
2022-10-09 03:18:14 - train: epoch 0011, iter [00600, 05004], lr: 0.100000, loss: 2.0028
2022-10-09 03:18:48 - train: epoch 0011, iter [00700, 05004], lr: 0.100000, loss: 1.9783
2022-10-09 03:19:22 - train: epoch 0011, iter [00800, 05004], lr: 0.100000, loss: 2.3640
2022-10-09 03:19:55 - train: epoch 0011, iter [00900, 05004], lr: 0.100000, loss: 2.0836
2022-10-09 03:20:31 - train: epoch 0011, iter [01000, 05004], lr: 0.100000, loss: 2.0693
2022-10-09 03:21:04 - train: epoch 0011, iter [01100, 05004], lr: 0.100000, loss: 2.1501
2022-10-09 03:21:39 - train: epoch 0011, iter [01200, 05004], lr: 0.100000, loss: 2.0480
2022-10-09 03:22:14 - train: epoch 0011, iter [01300, 05004], lr: 0.100000, loss: 2.1869
2022-10-09 03:22:48 - train: epoch 0011, iter [01400, 05004], lr: 0.100000, loss: 1.8573
2022-10-09 03:23:24 - train: epoch 0011, iter [01500, 05004], lr: 0.100000, loss: 1.8375
2022-10-09 03:23:57 - train: epoch 0011, iter [01600, 05004], lr: 0.100000, loss: 2.1100
2022-10-09 03:24:33 - train: epoch 0011, iter [01700, 05004], lr: 0.100000, loss: 2.0846
2022-10-09 03:25:09 - train: epoch 0011, iter [01800, 05004], lr: 0.100000, loss: 1.8286
2022-10-09 03:25:43 - train: epoch 0011, iter [01900, 05004], lr: 0.100000, loss: 2.0543
2022-10-09 03:26:18 - train: epoch 0011, iter [02000, 05004], lr: 0.100000, loss: 2.1255
2022-10-09 03:26:53 - train: epoch 0011, iter [02100, 05004], lr: 0.100000, loss: 1.9736
2022-10-09 03:27:27 - train: epoch 0011, iter [02200, 05004], lr: 0.100000, loss: 2.2101
2022-10-09 03:28:02 - train: epoch 0011, iter [02300, 05004], lr: 0.100000, loss: 2.0666
2022-10-09 03:28:37 - train: epoch 0011, iter [02400, 05004], lr: 0.100000, loss: 1.9552
2022-10-09 03:29:13 - train: epoch 0011, iter [02500, 05004], lr: 0.100000, loss: 1.9994
2022-10-09 03:29:48 - train: epoch 0011, iter [02600, 05004], lr: 0.100000, loss: 2.0796
2022-10-09 03:30:24 - train: epoch 0011, iter [02700, 05004], lr: 0.100000, loss: 2.0745
2022-10-09 03:30:58 - train: epoch 0011, iter [02800, 05004], lr: 0.100000, loss: 1.8743
2022-10-09 03:31:33 - train: epoch 0011, iter [02900, 05004], lr: 0.100000, loss: 2.0724
2022-10-09 03:32:07 - train: epoch 0011, iter [03000, 05004], lr: 0.100000, loss: 2.2513
2022-10-09 03:32:41 - train: epoch 0011, iter [03100, 05004], lr: 0.100000, loss: 1.8924
2022-10-09 03:33:16 - train: epoch 0011, iter [03200, 05004], lr: 0.100000, loss: 1.9164
2022-10-09 03:33:54 - train: epoch 0011, iter [03300, 05004], lr: 0.100000, loss: 1.9993
2022-10-09 03:34:28 - train: epoch 0011, iter [03400, 05004], lr: 0.100000, loss: 2.1583
2022-10-09 03:35:03 - train: epoch 0011, iter [03500, 05004], lr: 0.100000, loss: 1.9734
2022-10-09 03:35:38 - train: epoch 0011, iter [03600, 05004], lr: 0.100000, loss: 1.9888
2022-10-09 03:36:13 - train: epoch 0011, iter [03700, 05004], lr: 0.100000, loss: 2.1755
2022-10-09 03:36:48 - train: epoch 0011, iter [03800, 05004], lr: 0.100000, loss: 2.0742
2022-10-09 03:37:22 - train: epoch 0011, iter [03900, 05004], lr: 0.100000, loss: 1.9818
2022-10-09 03:37:58 - train: epoch 0011, iter [04000, 05004], lr: 0.100000, loss: 1.9828
2022-10-09 03:38:32 - train: epoch 0011, iter [04100, 05004], lr: 0.100000, loss: 1.7496
2022-10-09 03:39:08 - train: epoch 0011, iter [04200, 05004], lr: 0.100000, loss: 2.0341
2022-10-09 03:39:53 - train: epoch 0011, iter [04300, 05004], lr: 0.100000, loss: 2.1361
2022-10-09 03:42:25 - train: epoch 0011, iter [04400, 05004], lr: 0.100000, loss: 2.1447
2022-10-09 03:44:26 - train: epoch 0011, iter [04500, 05004], lr: 0.100000, loss: 1.9398
2022-10-09 03:45:11 - train: epoch 0011, iter [04600, 05004], lr: 0.100000, loss: 1.9365
2022-10-09 03:45:57 - train: epoch 0011, iter [04700, 05004], lr: 0.100000, loss: 1.8001
2022-10-09 03:46:40 - train: epoch 0011, iter [04800, 05004], lr: 0.100000, loss: 1.8124
2022-10-09 03:47:20 - train: epoch 0011, iter [04900, 05004], lr: 0.100000, loss: 1.9844
2022-10-09 03:47:55 - train: epoch 0011, iter [05000, 05004], lr: 0.100000, loss: 2.0833
2022-10-09 03:47:57 - train: epoch 011, train_loss: 2.0155
2022-10-09 03:49:15 - eval: epoch: 011, acc1: 55.454%, acc5: 80.440%, test_loss: 1.8876, per_image_load_time: 2.397ms, per_image_inference_time: 0.584ms
2022-10-09 03:49:15 - until epoch: 011, best_acc1: 55.454%
2022-10-09 03:49:15 - epoch 012 lr: 0.100000
2022-10-09 03:49:56 - train: epoch 0012, iter [00100, 05004], lr: 0.100000, loss: 1.8366
2022-10-09 03:50:30 - train: epoch 0012, iter [00200, 05004], lr: 0.100000, loss: 1.7852
2022-10-09 03:51:05 - train: epoch 0012, iter [00300, 05004], lr: 0.100000, loss: 1.8595
2022-10-09 03:51:38 - train: epoch 0012, iter [00400, 05004], lr: 0.100000, loss: 1.9057
2022-10-09 03:52:14 - train: epoch 0012, iter [00500, 05004], lr: 0.100000, loss: 2.0793
2022-10-09 03:52:47 - train: epoch 0012, iter [00600, 05004], lr: 0.100000, loss: 1.9888
2022-10-09 03:53:22 - train: epoch 0012, iter [00700, 05004], lr: 0.100000, loss: 1.8109
2022-10-09 03:53:55 - train: epoch 0012, iter [00800, 05004], lr: 0.100000, loss: 2.1046
2022-10-09 03:54:30 - train: epoch 0012, iter [00900, 05004], lr: 0.100000, loss: 1.9305
2022-10-09 03:55:04 - train: epoch 0012, iter [01000, 05004], lr: 0.100000, loss: 1.9382
2022-10-09 03:55:38 - train: epoch 0012, iter [01100, 05004], lr: 0.100000, loss: 2.3129
2022-10-09 03:56:11 - train: epoch 0012, iter [01200, 05004], lr: 0.100000, loss: 1.7826
2022-10-09 03:56:47 - train: epoch 0012, iter [01300, 05004], lr: 0.100000, loss: 1.8275
2022-10-09 03:57:22 - train: epoch 0012, iter [01400, 05004], lr: 0.100000, loss: 2.0246
2022-10-09 03:57:57 - train: epoch 0012, iter [01500, 05004], lr: 0.100000, loss: 1.8694
2022-10-09 03:58:32 - train: epoch 0012, iter [01600, 05004], lr: 0.100000, loss: 1.8206
2022-10-09 03:59:06 - train: epoch 0012, iter [01700, 05004], lr: 0.100000, loss: 1.8451
2022-10-09 03:59:43 - train: epoch 0012, iter [01800, 05004], lr: 0.100000, loss: 1.9858
2022-10-09 04:00:18 - train: epoch 0012, iter [01900, 05004], lr: 0.100000, loss: 2.1591
2022-10-09 04:00:54 - train: epoch 0012, iter [02000, 05004], lr: 0.100000, loss: 2.0415
2022-10-09 04:01:27 - train: epoch 0012, iter [02100, 05004], lr: 0.100000, loss: 2.0578
2022-10-09 04:02:02 - train: epoch 0012, iter [02200, 05004], lr: 0.100000, loss: 1.9443
2022-10-09 04:02:36 - train: epoch 0012, iter [02300, 05004], lr: 0.100000, loss: 1.9163
2022-10-09 04:03:10 - train: epoch 0012, iter [02400, 05004], lr: 0.100000, loss: 2.1382
2022-10-09 04:03:45 - train: epoch 0012, iter [02500, 05004], lr: 0.100000, loss: 1.7606
2022-10-09 04:04:20 - train: epoch 0012, iter [02600, 05004], lr: 0.100000, loss: 1.9192
2022-10-09 04:04:54 - train: epoch 0012, iter [02700, 05004], lr: 0.100000, loss: 2.1235
2022-10-09 04:05:30 - train: epoch 0012, iter [02800, 05004], lr: 0.100000, loss: 2.1764
2022-10-09 04:06:11 - train: epoch 0012, iter [02900, 05004], lr: 0.100000, loss: 2.0603
2022-10-09 04:06:47 - train: epoch 0012, iter [03000, 05004], lr: 0.100000, loss: 1.9072
2022-10-09 04:07:21 - train: epoch 0012, iter [03100, 05004], lr: 0.100000, loss: 2.0832
2022-10-09 04:07:54 - train: epoch 0012, iter [03200, 05004], lr: 0.100000, loss: 2.1426
2022-10-09 04:08:29 - train: epoch 0012, iter [03300, 05004], lr: 0.100000, loss: 2.0211
2022-10-09 04:09:04 - train: epoch 0012, iter [03400, 05004], lr: 0.100000, loss: 1.9623
2022-10-09 04:09:37 - train: epoch 0012, iter [03500, 05004], lr: 0.100000, loss: 1.8389
2022-10-09 04:10:11 - train: epoch 0012, iter [03600, 05004], lr: 0.100000, loss: 1.8117
2022-10-09 04:10:45 - train: epoch 0012, iter [03700, 05004], lr: 0.100000, loss: 2.0783
2022-10-09 04:11:20 - train: epoch 0012, iter [03800, 05004], lr: 0.100000, loss: 2.0923
2022-10-09 04:11:53 - train: epoch 0012, iter [03900, 05004], lr: 0.100000, loss: 1.8354
2022-10-09 04:12:29 - train: epoch 0012, iter [04000, 05004], lr: 0.100000, loss: 2.1010
2022-10-09 04:13:03 - train: epoch 0012, iter [04100, 05004], lr: 0.100000, loss: 1.9198
2022-10-09 04:13:37 - train: epoch 0012, iter [04200, 05004], lr: 0.100000, loss: 1.8644
2022-10-09 04:14:12 - train: epoch 0012, iter [04300, 05004], lr: 0.100000, loss: 2.0174
2022-10-09 04:14:45 - train: epoch 0012, iter [04400, 05004], lr: 0.100000, loss: 1.9823
2022-10-09 04:15:21 - train: epoch 0012, iter [04500, 05004], lr: 0.100000, loss: 1.9343
2022-10-09 04:15:55 - train: epoch 0012, iter [04600, 05004], lr: 0.100000, loss: 1.9673
2022-10-09 04:16:30 - train: epoch 0012, iter [04700, 05004], lr: 0.100000, loss: 1.8844
2022-10-09 04:17:04 - train: epoch 0012, iter [04800, 05004], lr: 0.100000, loss: 1.9940
2022-10-09 04:17:37 - train: epoch 0012, iter [04900, 05004], lr: 0.100000, loss: 1.8553
2022-10-09 04:18:10 - train: epoch 0012, iter [05000, 05004], lr: 0.100000, loss: 1.7340
2022-10-09 04:18:12 - train: epoch 012, train_loss: 2.0034
2022-10-09 04:19:30 - eval: epoch: 012, acc1: 49.480%, acc5: 75.002%, test_loss: 2.2162, per_image_load_time: 2.318ms, per_image_inference_time: 0.560ms
2022-10-09 04:19:30 - until epoch: 012, best_acc1: 55.454%
2022-10-09 04:19:30 - epoch 013 lr: 0.100000
2022-10-09 04:20:10 - train: epoch 0013, iter [00100, 05004], lr: 0.100000, loss: 1.8589
2022-10-09 04:20:45 - train: epoch 0013, iter [00200, 05004], lr: 0.100000, loss: 1.8597
2022-10-09 04:21:19 - train: epoch 0013, iter [00300, 05004], lr: 0.100000, loss: 1.9023
2022-10-09 04:21:53 - train: epoch 0013, iter [00400, 05004], lr: 0.100000, loss: 1.8580
2022-10-09 04:22:28 - train: epoch 0013, iter [00500, 05004], lr: 0.100000, loss: 2.0251
2022-10-09 04:23:02 - train: epoch 0013, iter [00600, 05004], lr: 0.100000, loss: 2.0348
2022-10-09 04:23:37 - train: epoch 0013, iter [00700, 05004], lr: 0.100000, loss: 2.0125
2022-10-09 04:24:11 - train: epoch 0013, iter [00800, 05004], lr: 0.100000, loss: 2.2201
2022-10-09 04:24:46 - train: epoch 0013, iter [00900, 05004], lr: 0.100000, loss: 1.7611
2022-10-09 04:25:21 - train: epoch 0013, iter [01000, 05004], lr: 0.100000, loss: 1.7677
2022-10-09 04:25:55 - train: epoch 0013, iter [01100, 05004], lr: 0.100000, loss: 2.0727
2022-10-09 04:26:30 - train: epoch 0013, iter [01200, 05004], lr: 0.100000, loss: 1.9604
2022-10-09 04:27:04 - train: epoch 0013, iter [01300, 05004], lr: 0.100000, loss: 1.8912
2022-10-09 04:27:39 - train: epoch 0013, iter [01400, 05004], lr: 0.100000, loss: 1.8068
2022-10-09 04:28:14 - train: epoch 0013, iter [01500, 05004], lr: 0.100000, loss: 2.2620
2022-10-09 04:28:48 - train: epoch 0013, iter [01600, 05004], lr: 0.100000, loss: 1.8911
2022-10-09 04:29:23 - train: epoch 0013, iter [01700, 05004], lr: 0.100000, loss: 1.8565
2022-10-09 04:29:57 - train: epoch 0013, iter [01800, 05004], lr: 0.100000, loss: 1.8459
2022-10-09 04:30:32 - train: epoch 0013, iter [01900, 05004], lr: 0.100000, loss: 2.0844
2022-10-09 04:31:06 - train: epoch 0013, iter [02000, 05004], lr: 0.100000, loss: 2.0037
2022-10-09 04:31:41 - train: epoch 0013, iter [02100, 05004], lr: 0.100000, loss: 2.1380
2022-10-09 04:32:16 - train: epoch 0013, iter [02200, 05004], lr: 0.100000, loss: 1.7831
2022-10-09 04:32:50 - train: epoch 0013, iter [02300, 05004], lr: 0.100000, loss: 1.8630
2022-10-09 04:33:25 - train: epoch 0013, iter [02400, 05004], lr: 0.100000, loss: 2.1519
2022-10-09 04:34:00 - train: epoch 0013, iter [02500, 05004], lr: 0.100000, loss: 1.9990
2022-10-09 04:34:38 - train: epoch 0013, iter [02600, 05004], lr: 0.100000, loss: 1.7073
2022-10-09 04:35:12 - train: epoch 0013, iter [02700, 05004], lr: 0.100000, loss: 1.8701
2022-10-09 04:35:48 - train: epoch 0013, iter [02800, 05004], lr: 0.100000, loss: 2.0914
2022-10-09 04:36:20 - train: epoch 0013, iter [02900, 05004], lr: 0.100000, loss: 2.0028
2022-10-09 04:36:55 - train: epoch 0013, iter [03000, 05004], lr: 0.100000, loss: 1.8583
2022-10-09 04:37:30 - train: epoch 0013, iter [03100, 05004], lr: 0.100000, loss: 1.9633
2022-10-09 04:38:04 - train: epoch 0013, iter [03200, 05004], lr: 0.100000, loss: 2.0606
2022-10-09 04:38:39 - train: epoch 0013, iter [03300, 05004], lr: 0.100000, loss: 1.8904
2022-10-09 04:39:13 - train: epoch 0013, iter [03400, 05004], lr: 0.100000, loss: 1.9005
2022-10-09 04:39:47 - train: epoch 0013, iter [03500, 05004], lr: 0.100000, loss: 1.7774
2022-10-09 04:40:21 - train: epoch 0013, iter [03600, 05004], lr: 0.100000, loss: 2.2004
2022-10-09 04:40:57 - train: epoch 0013, iter [03700, 05004], lr: 0.100000, loss: 1.8018
2022-10-09 04:41:31 - train: epoch 0013, iter [03800, 05004], lr: 0.100000, loss: 2.2802
2022-10-09 04:42:06 - train: epoch 0013, iter [03900, 05004], lr: 0.100000, loss: 2.1930
2022-10-09 04:42:40 - train: epoch 0013, iter [04000, 05004], lr: 0.100000, loss: 2.0740
2022-10-09 04:43:16 - train: epoch 0013, iter [04100, 05004], lr: 0.100000, loss: 2.1106
2022-10-09 04:43:49 - train: epoch 0013, iter [04200, 05004], lr: 0.100000, loss: 2.0441
2022-10-09 04:44:24 - train: epoch 0013, iter [04300, 05004], lr: 0.100000, loss: 1.9075
2022-10-09 04:44:59 - train: epoch 0013, iter [04400, 05004], lr: 0.100000, loss: 2.0035
2022-10-09 04:45:34 - train: epoch 0013, iter [04500, 05004], lr: 0.100000, loss: 1.8728
2022-10-09 04:46:09 - train: epoch 0013, iter [04600, 05004], lr: 0.100000, loss: 2.0611
2022-10-09 04:46:43 - train: epoch 0013, iter [04700, 05004], lr: 0.100000, loss: 2.0109
2022-10-09 04:47:17 - train: epoch 0013, iter [04800, 05004], lr: 0.100000, loss: 1.9219
2022-10-09 04:47:52 - train: epoch 0013, iter [04900, 05004], lr: 0.100000, loss: 2.1113
2022-10-09 04:48:24 - train: epoch 0013, iter [05000, 05004], lr: 0.100000, loss: 2.1002
2022-10-09 04:48:26 - train: epoch 013, train_loss: 1.9957
2022-10-09 04:49:45 - eval: epoch: 013, acc1: 57.284%, acc5: 81.824%, test_loss: 1.7998, per_image_load_time: 1.860ms, per_image_inference_time: 0.572ms
2022-10-09 04:49:45 - until epoch: 013, best_acc1: 57.284%
2022-10-09 04:49:45 - epoch 014 lr: 0.100000
2022-10-09 04:50:25 - train: epoch 0014, iter [00100, 05004], lr: 0.100000, loss: 1.9726
2022-10-09 04:51:00 - train: epoch 0014, iter [00200, 05004], lr: 0.100000, loss: 2.1418
2022-10-09 04:51:34 - train: epoch 0014, iter [00300, 05004], lr: 0.100000, loss: 1.8824
2022-10-09 04:52:08 - train: epoch 0014, iter [00400, 05004], lr: 0.100000, loss: 1.8742
2022-10-09 04:52:43 - train: epoch 0014, iter [00500, 05004], lr: 0.100000, loss: 1.8369
2022-10-09 04:53:18 - train: epoch 0014, iter [00600, 05004], lr: 0.100000, loss: 2.0160
2022-10-09 04:53:53 - train: epoch 0014, iter [00700, 05004], lr: 0.100000, loss: 1.8951
2022-10-09 04:54:28 - train: epoch 0014, iter [00800, 05004], lr: 0.100000, loss: 1.8431
2022-10-09 04:55:02 - train: epoch 0014, iter [00900, 05004], lr: 0.100000, loss: 2.0074
2022-10-09 04:55:37 - train: epoch 0014, iter [01000, 05004], lr: 0.100000, loss: 2.2701
2022-10-09 04:56:12 - train: epoch 0014, iter [01100, 05004], lr: 0.100000, loss: 1.8226
2022-10-09 04:56:46 - train: epoch 0014, iter [01200, 05004], lr: 0.100000, loss: 2.1848
2022-10-09 04:57:21 - train: epoch 0014, iter [01300, 05004], lr: 0.100000, loss: 2.1963
2022-10-09 04:57:56 - train: epoch 0014, iter [01400, 05004], lr: 0.100000, loss: 2.0075
2022-10-09 04:58:31 - train: epoch 0014, iter [01500, 05004], lr: 0.100000, loss: 2.1226
2022-10-09 04:59:04 - train: epoch 0014, iter [01600, 05004], lr: 0.100000, loss: 1.8980
2022-10-09 04:59:39 - train: epoch 0014, iter [01700, 05004], lr: 0.100000, loss: 2.1308
2022-10-09 05:00:13 - train: epoch 0014, iter [01800, 05004], lr: 0.100000, loss: 2.0718
2022-10-09 05:00:48 - train: epoch 0014, iter [01900, 05004], lr: 0.100000, loss: 1.8783
2022-10-09 05:01:22 - train: epoch 0014, iter [02000, 05004], lr: 0.100000, loss: 1.9538
2022-10-09 05:01:57 - train: epoch 0014, iter [02100, 05004], lr: 0.100000, loss: 2.1912
2022-10-09 05:02:31 - train: epoch 0014, iter [02200, 05004], lr: 0.100000, loss: 2.0133
2022-10-09 05:03:06 - train: epoch 0014, iter [02300, 05004], lr: 0.100000, loss: 1.9507
2022-10-09 05:03:41 - train: epoch 0014, iter [02400, 05004], lr: 0.100000, loss: 2.0389
2022-10-09 05:04:16 - train: epoch 0014, iter [02500, 05004], lr: 0.100000, loss: 1.8183
2022-10-09 05:04:51 - train: epoch 0014, iter [02600, 05004], lr: 0.100000, loss: 1.9416
2022-10-09 05:05:26 - train: epoch 0014, iter [02700, 05004], lr: 0.100000, loss: 1.9190
2022-10-09 05:06:02 - train: epoch 0014, iter [02800, 05004], lr: 0.100000, loss: 2.3015
2022-10-09 05:06:35 - train: epoch 0014, iter [02900, 05004], lr: 0.100000, loss: 2.0660
2022-10-09 05:07:09 - train: epoch 0014, iter [03000, 05004], lr: 0.100000, loss: 2.0079
2022-10-09 05:07:44 - train: epoch 0014, iter [03100, 05004], lr: 0.100000, loss: 2.1388
2022-10-09 05:08:19 - train: epoch 0014, iter [03200, 05004], lr: 0.100000, loss: 1.9042
2022-10-09 05:08:53 - train: epoch 0014, iter [03300, 05004], lr: 0.100000, loss: 1.8611
2022-10-09 05:09:27 - train: epoch 0014, iter [03400, 05004], lr: 0.100000, loss: 2.0273
2022-10-09 05:10:01 - train: epoch 0014, iter [03500, 05004], lr: 0.100000, loss: 2.1258
2022-10-09 05:10:37 - train: epoch 0014, iter [03600, 05004], lr: 0.100000, loss: 1.7400
2022-10-09 05:11:10 - train: epoch 0014, iter [03700, 05004], lr: 0.100000, loss: 2.0840
2022-10-09 05:11:46 - train: epoch 0014, iter [03800, 05004], lr: 0.100000, loss: 2.0553
2022-10-09 05:12:21 - train: epoch 0014, iter [03900, 05004], lr: 0.100000, loss: 2.2184
2022-10-09 05:12:56 - train: epoch 0014, iter [04000, 05004], lr: 0.100000, loss: 2.0428
2022-10-09 05:13:31 - train: epoch 0014, iter [04100, 05004], lr: 0.100000, loss: 2.0328
2022-10-09 05:14:08 - train: epoch 0014, iter [04200, 05004], lr: 0.100000, loss: 1.9854
2022-10-09 05:14:42 - train: epoch 0014, iter [04300, 05004], lr: 0.100000, loss: 1.6862
2022-10-09 05:15:17 - train: epoch 0014, iter [04400, 05004], lr: 0.100000, loss: 2.0644
2022-10-09 05:15:53 - train: epoch 0014, iter [04500, 05004], lr: 0.100000, loss: 2.0001
2022-10-09 05:16:27 - train: epoch 0014, iter [04600, 05004], lr: 0.100000, loss: 1.7343
2022-10-09 05:17:02 - train: epoch 0014, iter [04700, 05004], lr: 0.100000, loss: 1.9677
2022-10-09 05:17:37 - train: epoch 0014, iter [04800, 05004], lr: 0.100000, loss: 1.9405
2022-10-09 05:18:12 - train: epoch 0014, iter [04900, 05004], lr: 0.100000, loss: 1.9272
2022-10-09 05:18:44 - train: epoch 0014, iter [05000, 05004], lr: 0.100000, loss: 2.0842
2022-10-09 05:18:46 - train: epoch 014, train_loss: 1.9875
2022-10-09 05:20:04 - eval: epoch: 014, acc1: 55.616%, acc5: 80.896%, test_loss: 1.8698, per_image_load_time: 1.403ms, per_image_inference_time: 0.601ms
2022-10-09 05:20:05 - until epoch: 014, best_acc1: 57.284%
2022-10-09 05:20:05 - epoch 015 lr: 0.100000
2022-10-09 05:20:45 - train: epoch 0015, iter [00100, 05004], lr: 0.100000, loss: 1.8679
2022-10-09 05:21:21 - train: epoch 0015, iter [00200, 05004], lr: 0.100000, loss: 2.1752
2022-10-09 05:21:55 - train: epoch 0015, iter [00300, 05004], lr: 0.100000, loss: 2.2883
2022-10-09 05:22:30 - train: epoch 0015, iter [00400, 05004], lr: 0.100000, loss: 2.0675
2022-10-09 05:23:06 - train: epoch 0015, iter [00500, 05004], lr: 0.100000, loss: 1.7866
2022-10-09 05:23:40 - train: epoch 0015, iter [00600, 05004], lr: 0.100000, loss: 2.1178
2022-10-09 05:24:15 - train: epoch 0015, iter [00700, 05004], lr: 0.100000, loss: 1.9166
2022-10-09 05:24:49 - train: epoch 0015, iter [00800, 05004], lr: 0.100000, loss: 2.0188
2022-10-09 05:25:23 - train: epoch 0015, iter [00900, 05004], lr: 0.100000, loss: 1.9805
2022-10-09 05:26:00 - train: epoch 0015, iter [01000, 05004], lr: 0.100000, loss: 1.7551
2022-10-09 05:26:33 - train: epoch 0015, iter [01100, 05004], lr: 0.100000, loss: 1.7993
2022-10-09 05:27:07 - train: epoch 0015, iter [01200, 05004], lr: 0.100000, loss: 2.0228
2022-10-09 05:27:42 - train: epoch 0015, iter [01300, 05004], lr: 0.100000, loss: 2.0722
2022-10-09 05:28:16 - train: epoch 0015, iter [01400, 05004], lr: 0.100000, loss: 1.9210
2022-10-09 05:28:51 - train: epoch 0015, iter [01500, 05004], lr: 0.100000, loss: 1.9571
2022-10-09 05:29:26 - train: epoch 0015, iter [01600, 05004], lr: 0.100000, loss: 1.9599
2022-10-09 05:30:00 - train: epoch 0015, iter [01700, 05004], lr: 0.100000, loss: 1.9491
2022-10-09 05:30:36 - train: epoch 0015, iter [01800, 05004], lr: 0.100000, loss: 1.9629
2022-10-09 05:31:15 - train: epoch 0015, iter [01900, 05004], lr: 0.100000, loss: 1.9654
2022-10-09 05:31:48 - train: epoch 0015, iter [02000, 05004], lr: 0.100000, loss: 1.9660
2022-10-09 05:32:23 - train: epoch 0015, iter [02100, 05004], lr: 0.100000, loss: 1.7577
2022-10-09 05:32:57 - train: epoch 0015, iter [02200, 05004], lr: 0.100000, loss: 2.0462
2022-10-09 05:33:31 - train: epoch 0015, iter [02300, 05004], lr: 0.100000, loss: 1.9226
2022-10-09 05:34:05 - train: epoch 0015, iter [02400, 05004], lr: 0.100000, loss: 2.0108
2022-10-09 05:34:41 - train: epoch 0015, iter [02500, 05004], lr: 0.100000, loss: 2.0567
2022-10-09 05:35:15 - train: epoch 0015, iter [02600, 05004], lr: 0.100000, loss: 1.8028
2022-10-09 05:35:49 - train: epoch 0015, iter [02700, 05004], lr: 0.100000, loss: 2.0599
2022-10-09 05:36:23 - train: epoch 0015, iter [02800, 05004], lr: 0.100000, loss: 2.0342
2022-10-09 05:36:58 - train: epoch 0015, iter [02900, 05004], lr: 0.100000, loss: 1.9798
2022-10-09 05:37:32 - train: epoch 0015, iter [03000, 05004], lr: 0.100000, loss: 1.8101
2022-10-09 05:38:05 - train: epoch 0015, iter [03100, 05004], lr: 0.100000, loss: 1.8179
2022-10-09 05:38:39 - train: epoch 0015, iter [03200, 05004], lr: 0.100000, loss: 1.9754
2022-10-09 05:39:14 - train: epoch 0015, iter [03300, 05004], lr: 0.100000, loss: 1.8199
2022-10-09 05:39:49 - train: epoch 0015, iter [03400, 05004], lr: 0.100000, loss: 1.9494
2022-10-09 05:40:23 - train: epoch 0015, iter [03500, 05004], lr: 0.100000, loss: 2.1929
2022-10-09 05:40:57 - train: epoch 0015, iter [03600, 05004], lr: 0.100000, loss: 1.9655
2022-10-09 05:41:30 - train: epoch 0015, iter [03700, 05004], lr: 0.100000, loss: 1.9371
2022-10-09 05:42:05 - train: epoch 0015, iter [03800, 05004], lr: 0.100000, loss: 1.8005
2022-10-09 05:42:41 - train: epoch 0015, iter [03900, 05004], lr: 0.100000, loss: 2.0003
2022-10-09 05:43:15 - train: epoch 0015, iter [04000, 05004], lr: 0.100000, loss: 2.1022
2022-10-09 05:43:50 - train: epoch 0015, iter [04100, 05004], lr: 0.100000, loss: 1.7952
2022-10-09 05:44:25 - train: epoch 0015, iter [04200, 05004], lr: 0.100000, loss: 1.8324
2022-10-09 05:44:59 - train: epoch 0015, iter [04300, 05004], lr: 0.100000, loss: 1.7447
2022-10-09 05:45:34 - train: epoch 0015, iter [04400, 05004], lr: 0.100000, loss: 1.8719
2022-10-09 05:46:08 - train: epoch 0015, iter [04500, 05004], lr: 0.100000, loss: 2.0938
2022-10-09 05:46:43 - train: epoch 0015, iter [04600, 05004], lr: 0.100000, loss: 2.0210
2022-10-09 05:47:19 - train: epoch 0015, iter [04700, 05004], lr: 0.100000, loss: 1.8882
2022-10-09 05:47:55 - train: epoch 0015, iter [04800, 05004], lr: 0.100000, loss: 1.8765
2022-10-09 05:48:30 - train: epoch 0015, iter [04900, 05004], lr: 0.100000, loss: 1.9996
2022-10-09 05:49:03 - train: epoch 0015, iter [05000, 05004], lr: 0.100000, loss: 2.1569
2022-10-09 05:49:05 - train: epoch 015, train_loss: 1.9798
2022-10-09 05:50:23 - eval: epoch: 015, acc1: 54.800%, acc5: 79.560%, test_loss: 1.9298, per_image_load_time: 2.327ms, per_image_inference_time: 0.589ms
2022-10-09 05:50:23 - until epoch: 015, best_acc1: 57.284%
2022-10-09 05:50:23 - epoch 016 lr: 0.100000
2022-10-09 05:51:02 - train: epoch 0016, iter [00100, 05004], lr: 0.100000, loss: 1.9216
2022-10-09 05:51:37 - train: epoch 0016, iter [00200, 05004], lr: 0.100000, loss: 1.8860
2022-10-09 05:52:13 - train: epoch 0016, iter [00300, 05004], lr: 0.100000, loss: 1.8230
2022-10-09 05:52:46 - train: epoch 0016, iter [00400, 05004], lr: 0.100000, loss: 2.0815
2022-10-09 05:53:20 - train: epoch 0016, iter [00500, 05004], lr: 0.100000, loss: 1.8478
2022-10-09 05:53:54 - train: epoch 0016, iter [00600, 05004], lr: 0.100000, loss: 1.9283
2022-10-09 05:54:28 - train: epoch 0016, iter [00700, 05004], lr: 0.100000, loss: 1.8555
2022-10-09 05:55:02 - train: epoch 0016, iter [00800, 05004], lr: 0.100000, loss: 1.8378
2022-10-09 05:55:38 - train: epoch 0016, iter [00900, 05004], lr: 0.100000, loss: 1.9030
2022-10-09 05:56:13 - train: epoch 0016, iter [01000, 05004], lr: 0.100000, loss: 1.7964
2022-10-09 05:56:47 - train: epoch 0016, iter [01100, 05004], lr: 0.100000, loss: 1.8179
2022-10-09 05:57:21 - train: epoch 0016, iter [01200, 05004], lr: 0.100000, loss: 1.9721
2022-10-09 05:57:56 - train: epoch 0016, iter [01300, 05004], lr: 0.100000, loss: 1.9389
2022-10-09 05:58:31 - train: epoch 0016, iter [01400, 05004], lr: 0.100000, loss: 1.8571
2022-10-09 05:59:05 - train: epoch 0016, iter [01500, 05004], lr: 0.100000, loss: 2.2110
2022-10-09 05:59:40 - train: epoch 0016, iter [01600, 05004], lr: 0.100000, loss: 2.0784
2022-10-09 06:00:14 - train: epoch 0016, iter [01700, 05004], lr: 0.100000, loss: 2.1084
2022-10-09 06:00:48 - train: epoch 0016, iter [01800, 05004], lr: 0.100000, loss: 2.2174
2022-10-09 06:01:23 - train: epoch 0016, iter [01900, 05004], lr: 0.100000, loss: 1.9245
2022-10-09 06:01:57 - train: epoch 0016, iter [02000, 05004], lr: 0.100000, loss: 1.6962
2022-10-09 06:02:31 - train: epoch 0016, iter [02100, 05004], lr: 0.100000, loss: 2.1866
2022-10-09 06:03:05 - train: epoch 0016, iter [02200, 05004], lr: 0.100000, loss: 1.9791
2022-10-09 06:03:39 - train: epoch 0016, iter [02300, 05004], lr: 0.100000, loss: 2.1721
2022-10-09 06:04:14 - train: epoch 0016, iter [02400, 05004], lr: 0.100000, loss: 1.8207
2022-10-09 06:04:49 - train: epoch 0016, iter [02500, 05004], lr: 0.100000, loss: 1.8319
2022-10-09 06:05:24 - train: epoch 0016, iter [02600, 05004], lr: 0.100000, loss: 2.0386
2022-10-09 06:05:57 - train: epoch 0016, iter [02700, 05004], lr: 0.100000, loss: 1.8240
2022-10-09 06:06:33 - train: epoch 0016, iter [02800, 05004], lr: 0.100000, loss: 1.8849
2022-10-09 06:07:07 - train: epoch 0016, iter [02900, 05004], lr: 0.100000, loss: 1.9175
2022-10-09 06:07:42 - train: epoch 0016, iter [03000, 05004], lr: 0.100000, loss: 2.0376
2022-10-09 06:08:14 - train: epoch 0016, iter [03100, 05004], lr: 0.100000, loss: 2.0631
2022-10-09 06:08:49 - train: epoch 0016, iter [03200, 05004], lr: 0.100000, loss: 2.3115
2022-10-09 06:09:24 - train: epoch 0016, iter [03300, 05004], lr: 0.100000, loss: 1.9682
2022-10-09 06:09:59 - train: epoch 0016, iter [03400, 05004], lr: 0.100000, loss: 1.8961
2022-10-09 06:10:35 - train: epoch 0016, iter [03500, 05004], lr: 0.100000, loss: 1.9935
2022-10-09 06:11:08 - train: epoch 0016, iter [03600, 05004], lr: 0.100000, loss: 1.9328
2022-10-09 06:11:45 - train: epoch 0016, iter [03700, 05004], lr: 0.100000, loss: 2.0133
2022-10-09 06:12:24 - train: epoch 0016, iter [03800, 05004], lr: 0.100000, loss: 2.2629
2022-10-09 06:13:03 - train: epoch 0016, iter [03900, 05004], lr: 0.100000, loss: 2.0400
2022-10-09 06:13:40 - train: epoch 0016, iter [04000, 05004], lr: 0.100000, loss: 2.1970
2022-10-09 06:14:27 - train: epoch 0016, iter [04100, 05004], lr: 0.100000, loss: 2.0537
2022-10-09 06:15:02 - train: epoch 0016, iter [04200, 05004], lr: 0.100000, loss: 1.9066
2022-10-09 06:15:37 - train: epoch 0016, iter [04300, 05004], lr: 0.100000, loss: 1.9604
2022-10-09 06:16:12 - train: epoch 0016, iter [04400, 05004], lr: 0.100000, loss: 1.8894
2022-10-09 06:16:45 - train: epoch 0016, iter [04500, 05004], lr: 0.100000, loss: 2.1350
2022-10-09 06:17:25 - train: epoch 0016, iter [04600, 05004], lr: 0.100000, loss: 1.8958
2022-10-09 06:17:58 - train: epoch 0016, iter [04700, 05004], lr: 0.100000, loss: 2.1938
2022-10-09 06:18:33 - train: epoch 0016, iter [04800, 05004], lr: 0.100000, loss: 1.9692
2022-10-09 06:19:09 - train: epoch 0016, iter [04900, 05004], lr: 0.100000, loss: 1.9360
2022-10-09 06:19:49 - train: epoch 0016, iter [05000, 05004], lr: 0.100000, loss: 2.2141
2022-10-09 06:19:51 - train: epoch 016, train_loss: 1.9755
2022-10-09 06:21:08 - eval: epoch: 016, acc1: 54.610%, acc5: 79.692%, test_loss: 1.9259, per_image_load_time: 2.289ms, per_image_inference_time: 0.592ms
2022-10-09 06:21:08 - until epoch: 016, best_acc1: 57.284%
2022-10-09 06:21:08 - epoch 017 lr: 0.100000
2022-10-09 06:21:48 - train: epoch 0017, iter [00100, 05004], lr: 0.100000, loss: 1.9620
2022-10-09 06:22:23 - train: epoch 0017, iter [00200, 05004], lr: 0.100000, loss: 1.9388
2022-10-09 06:22:57 - train: epoch 0017, iter [00300, 05004], lr: 0.100000, loss: 2.1298
2022-10-09 06:23:30 - train: epoch 0017, iter [00400, 05004], lr: 0.100000, loss: 1.6702
2022-10-09 06:24:05 - train: epoch 0017, iter [00500, 05004], lr: 0.100000, loss: 1.9595
2022-10-09 06:24:39 - train: epoch 0017, iter [00600, 05004], lr: 0.100000, loss: 2.3232
2022-10-09 06:25:13 - train: epoch 0017, iter [00700, 05004], lr: 0.100000, loss: 2.0058
2022-10-09 06:25:48 - train: epoch 0017, iter [00800, 05004], lr: 0.100000, loss: 1.8638
2022-10-09 06:26:22 - train: epoch 0017, iter [00900, 05004], lr: 0.100000, loss: 1.6621
2022-10-09 06:26:57 - train: epoch 0017, iter [01000, 05004], lr: 0.100000, loss: 1.9908
2022-10-09 06:27:30 - train: epoch 0017, iter [01100, 05004], lr: 0.100000, loss: 2.1935
2022-10-09 06:28:05 - train: epoch 0017, iter [01200, 05004], lr: 0.100000, loss: 1.9766
2022-10-09 06:28:40 - train: epoch 0017, iter [01300, 05004], lr: 0.100000, loss: 1.9239
2022-10-09 06:29:14 - train: epoch 0017, iter [01400, 05004], lr: 0.100000, loss: 2.0916
2022-10-09 06:29:50 - train: epoch 0017, iter [01500, 05004], lr: 0.100000, loss: 1.8706
2022-10-09 06:30:25 - train: epoch 0017, iter [01600, 05004], lr: 0.100000, loss: 1.7010
2022-10-09 06:31:00 - train: epoch 0017, iter [01700, 05004], lr: 0.100000, loss: 1.7817
2022-10-09 06:31:35 - train: epoch 0017, iter [01800, 05004], lr: 0.100000, loss: 2.0050
2022-10-09 06:32:09 - train: epoch 0017, iter [01900, 05004], lr: 0.100000, loss: 1.9818
2022-10-09 06:32:43 - train: epoch 0017, iter [02000, 05004], lr: 0.100000, loss: 2.0309
2022-10-09 06:33:18 - train: epoch 0017, iter [02100, 05004], lr: 0.100000, loss: 1.9968
2022-10-09 06:33:53 - train: epoch 0017, iter [02200, 05004], lr: 0.100000, loss: 1.9517
2022-10-09 06:34:27 - train: epoch 0017, iter [02300, 05004], lr: 0.100000, loss: 1.9656
2022-10-09 06:35:01 - train: epoch 0017, iter [02400, 05004], lr: 0.100000, loss: 1.7622
2022-10-09 06:35:35 - train: epoch 0017, iter [02500, 05004], lr: 0.100000, loss: 2.3007
2022-10-09 06:36:09 - train: epoch 0017, iter [02600, 05004], lr: 0.100000, loss: 1.9857
2022-10-09 06:36:44 - train: epoch 0017, iter [02700, 05004], lr: 0.100000, loss: 1.8479
2022-10-09 06:37:25 - train: epoch 0017, iter [02800, 05004], lr: 0.100000, loss: 1.9293
2022-10-09 06:37:59 - train: epoch 0017, iter [02900, 05004], lr: 0.100000, loss: 2.1863
2022-10-09 06:38:33 - train: epoch 0017, iter [03000, 05004], lr: 0.100000, loss: 1.7724
2022-10-09 06:39:07 - train: epoch 0017, iter [03100, 05004], lr: 0.100000, loss: 2.2725
2022-10-09 06:39:42 - train: epoch 0017, iter [03200, 05004], lr: 0.100000, loss: 1.9348
2022-10-09 06:40:16 - train: epoch 0017, iter [03300, 05004], lr: 0.100000, loss: 2.0525
2022-10-09 06:40:52 - train: epoch 0017, iter [03400, 05004], lr: 0.100000, loss: 1.8260
2022-10-09 06:41:27 - train: epoch 0017, iter [03500, 05004], lr: 0.100000, loss: 1.8539
2022-10-09 06:42:01 - train: epoch 0017, iter [03600, 05004], lr: 0.100000, loss: 1.9473
2022-10-09 06:42:35 - train: epoch 0017, iter [03700, 05004], lr: 0.100000, loss: 1.8931
2022-10-09 06:43:10 - train: epoch 0017, iter [03800, 05004], lr: 0.100000, loss: 1.8803
2022-10-09 06:43:44 - train: epoch 0017, iter [03900, 05004], lr: 0.100000, loss: 1.8491
2022-10-09 06:44:18 - train: epoch 0017, iter [04000, 05004], lr: 0.100000, loss: 2.0453
2022-10-09 06:44:53 - train: epoch 0017, iter [04100, 05004], lr: 0.100000, loss: 2.1555
2022-10-09 06:45:27 - train: epoch 0017, iter [04200, 05004], lr: 0.100000, loss: 1.9868
2022-10-09 06:46:02 - train: epoch 0017, iter [04300, 05004], lr: 0.100000, loss: 1.9177
2022-10-09 06:46:36 - train: epoch 0017, iter [04400, 05004], lr: 0.100000, loss: 1.9663
2022-10-09 06:47:12 - train: epoch 0017, iter [04500, 05004], lr: 0.100000, loss: 1.9766
2022-10-09 06:47:46 - train: epoch 0017, iter [04600, 05004], lr: 0.100000, loss: 1.8888
2022-10-09 06:48:21 - train: epoch 0017, iter [04700, 05004], lr: 0.100000, loss: 2.1560
2022-10-09 06:48:57 - train: epoch 0017, iter [04800, 05004], lr: 0.100000, loss: 2.0277
2022-10-09 06:49:31 - train: epoch 0017, iter [04900, 05004], lr: 0.100000, loss: 1.9324
2022-10-09 06:50:03 - train: epoch 0017, iter [05000, 05004], lr: 0.100000, loss: 1.9125
2022-10-09 06:50:05 - train: epoch 017, train_loss: 1.9666
2022-10-09 06:51:22 - eval: epoch: 017, acc1: 57.876%, acc5: 82.550%, test_loss: 1.7434, per_image_load_time: 1.484ms, per_image_inference_time: 0.587ms
2022-10-09 06:51:22 - until epoch: 017, best_acc1: 57.876%
2022-10-09 06:51:22 - epoch 018 lr: 0.100000
2022-10-09 06:52:03 - train: epoch 0018, iter [00100, 05004], lr: 0.100000, loss: 1.7980
2022-10-09 06:52:37 - train: epoch 0018, iter [00200, 05004], lr: 0.100000, loss: 2.0625
2022-10-09 06:53:12 - train: epoch 0018, iter [00300, 05004], lr: 0.100000, loss: 2.0361
2022-10-09 06:53:46 - train: epoch 0018, iter [00400, 05004], lr: 0.100000, loss: 2.1343
2022-10-09 06:54:20 - train: epoch 0018, iter [00500, 05004], lr: 0.100000, loss: 2.0623
2022-10-09 06:54:54 - train: epoch 0018, iter [00600, 05004], lr: 0.100000, loss: 1.9465
2022-10-09 06:55:28 - train: epoch 0018, iter [00700, 05004], lr: 0.100000, loss: 1.7908
2022-10-09 06:56:03 - train: epoch 0018, iter [00800, 05004], lr: 0.100000, loss: 1.9038
2022-10-09 06:56:37 - train: epoch 0018, iter [00900, 05004], lr: 0.100000, loss: 2.0330
2022-10-09 06:57:12 - train: epoch 0018, iter [01000, 05004], lr: 0.100000, loss: 1.8212
2022-10-09 06:57:45 - train: epoch 0018, iter [01100, 05004], lr: 0.100000, loss: 2.0444
2022-10-09 06:58:21 - train: epoch 0018, iter [01200, 05004], lr: 0.100000, loss: 1.9434
2022-10-09 06:58:55 - train: epoch 0018, iter [01300, 05004], lr: 0.100000, loss: 2.1975
2022-10-09 06:59:32 - train: epoch 0018, iter [01400, 05004], lr: 0.100000, loss: 2.0407
2022-10-09 07:00:05 - train: epoch 0018, iter [01500, 05004], lr: 0.100000, loss: 2.2316
2022-10-09 07:00:43 - train: epoch 0018, iter [01600, 05004], lr: 0.100000, loss: 1.9504
2022-10-09 07:01:23 - train: epoch 0018, iter [01700, 05004], lr: 0.100000, loss: 2.0510
2022-10-09 07:01:59 - train: epoch 0018, iter [01800, 05004], lr: 0.100000, loss: 1.8669
2022-10-09 07:02:35 - train: epoch 0018, iter [01900, 05004], lr: 0.100000, loss: 1.9223
2022-10-09 07:03:12 - train: epoch 0018, iter [02000, 05004], lr: 0.100000, loss: 2.3479
2022-10-09 07:03:47 - train: epoch 0018, iter [02100, 05004], lr: 0.100000, loss: 1.9850
2022-10-09 07:04:25 - train: epoch 0018, iter [02200, 05004], lr: 0.100000, loss: 2.0206
2022-10-09 07:05:04 - train: epoch 0018, iter [02300, 05004], lr: 0.100000, loss: 1.8534
2022-10-09 07:05:37 - train: epoch 0018, iter [02400, 05004], lr: 0.100000, loss: 1.9799
2022-10-09 07:06:15 - train: epoch 0018, iter [02500, 05004], lr: 0.100000, loss: 1.8985
2022-10-09 07:06:48 - train: epoch 0018, iter [02600, 05004], lr: 0.100000, loss: 2.0381
2022-10-09 07:07:26 - train: epoch 0018, iter [02700, 05004], lr: 0.100000, loss: 1.9705
2022-10-09 07:08:02 - train: epoch 0018, iter [02800, 05004], lr: 0.100000, loss: 2.0515
2022-10-09 07:08:35 - train: epoch 0018, iter [02900, 05004], lr: 0.100000, loss: 2.0427
2022-10-09 07:09:10 - train: epoch 0018, iter [03000, 05004], lr: 0.100000, loss: 1.9553
2022-10-09 07:09:44 - train: epoch 0018, iter [03100, 05004], lr: 0.100000, loss: 2.2379
2022-10-09 07:10:20 - train: epoch 0018, iter [03200, 05004], lr: 0.100000, loss: 1.9314
2022-10-09 07:10:54 - train: epoch 0018, iter [03300, 05004], lr: 0.100000, loss: 1.8911
2022-10-09 07:11:29 - train: epoch 0018, iter [03400, 05004], lr: 0.100000, loss: 2.0309
2022-10-09 07:12:13 - train: epoch 0018, iter [03500, 05004], lr: 0.100000, loss: 2.0488
2022-10-09 07:12:56 - train: epoch 0018, iter [03600, 05004], lr: 0.100000, loss: 2.0459
2022-10-09 07:13:42 - train: epoch 0018, iter [03700, 05004], lr: 0.100000, loss: 2.2413
2022-10-09 07:14:26 - train: epoch 0018, iter [03800, 05004], lr: 0.100000, loss: 2.1422
2022-10-09 07:15:02 - train: epoch 0018, iter [03900, 05004], lr: 0.100000, loss: 2.0040
2022-10-09 07:15:42 - train: epoch 0018, iter [04000, 05004], lr: 0.100000, loss: 2.0088
2022-10-09 07:16:22 - train: epoch 0018, iter [04100, 05004], lr: 0.100000, loss: 2.1210
2022-10-09 07:16:58 - train: epoch 0018, iter [04200, 05004], lr: 0.100000, loss: 1.9595
2022-10-09 07:17:58 - train: epoch 0018, iter [04300, 05004], lr: 0.100000, loss: 1.8932
2022-10-09 07:19:07 - train: epoch 0018, iter [04400, 05004], lr: 0.100000, loss: 1.9773
2022-10-09 07:20:21 - train: epoch 0018, iter [04500, 05004], lr: 0.100000, loss: 2.0164
2022-10-09 07:21:17 - train: epoch 0018, iter [04600, 05004], lr: 0.100000, loss: 1.9955
2022-10-09 07:21:58 - train: epoch 0018, iter [04700, 05004], lr: 0.100000, loss: 2.0326
2022-10-09 07:23:00 - train: epoch 0018, iter [04800, 05004], lr: 0.100000, loss: 2.0031
2022-10-09 07:23:41 - train: epoch 0018, iter [04900, 05004], lr: 0.100000, loss: 2.0372
2022-10-09 07:24:28 - train: epoch 0018, iter [05000, 05004], lr: 0.100000, loss: 2.0549
2022-10-09 07:24:30 - train: epoch 018, train_loss: 1.9622
2022-10-09 07:25:48 - eval: epoch: 018, acc1: 57.490%, acc5: 81.942%, test_loss: 1.7900, per_image_load_time: 1.953ms, per_image_inference_time: 0.573ms
2022-10-09 07:25:49 - until epoch: 018, best_acc1: 57.876%
2022-10-09 07:25:49 - epoch 019 lr: 0.100000
2022-10-09 07:26:30 - train: epoch 0019, iter [00100, 05004], lr: 0.100000, loss: 1.6057
2022-10-09 07:27:04 - train: epoch 0019, iter [00200, 05004], lr: 0.100000, loss: 2.1025
2022-10-09 07:27:38 - train: epoch 0019, iter [00300, 05004], lr: 0.100000, loss: 2.0633
2022-10-09 07:28:11 - train: epoch 0019, iter [00400, 05004], lr: 0.100000, loss: 2.1059
2022-10-09 07:28:46 - train: epoch 0019, iter [00500, 05004], lr: 0.100000, loss: 1.8884
2022-10-09 07:29:21 - train: epoch 0019, iter [00600, 05004], lr: 0.100000, loss: 1.8219
2022-10-09 07:29:54 - train: epoch 0019, iter [00700, 05004], lr: 0.100000, loss: 1.6082
2022-10-09 07:30:29 - train: epoch 0019, iter [00800, 05004], lr: 0.100000, loss: 2.1422
2022-10-09 07:31:03 - train: epoch 0019, iter [00900, 05004], lr: 0.100000, loss: 1.9937
2022-10-09 07:31:38 - train: epoch 0019, iter [01000, 05004], lr: 0.100000, loss: 1.8905
2022-10-09 07:32:14 - train: epoch 0019, iter [01100, 05004], lr: 0.100000, loss: 1.9131
2022-10-09 07:32:48 - train: epoch 0019, iter [01200, 05004], lr: 0.100000, loss: 1.7781
2022-10-09 07:33:22 - train: epoch 0019, iter [01300, 05004], lr: 0.100000, loss: 2.0017
2022-10-09 07:33:57 - train: epoch 0019, iter [01400, 05004], lr: 0.100000, loss: 1.5810
2022-10-09 07:34:32 - train: epoch 0019, iter [01500, 05004], lr: 0.100000, loss: 2.1455
2022-10-09 07:35:07 - train: epoch 0019, iter [01600, 05004], lr: 0.100000, loss: 1.9467
2022-10-09 07:35:41 - train: epoch 0019, iter [01700, 05004], lr: 0.100000, loss: 1.8726
2022-10-09 07:36:16 - train: epoch 0019, iter [01800, 05004], lr: 0.100000, loss: 1.9470
2022-10-09 07:36:50 - train: epoch 0019, iter [01900, 05004], lr: 0.100000, loss: 2.2053
2022-10-09 07:37:25 - train: epoch 0019, iter [02000, 05004], lr: 0.100000, loss: 2.0608
2022-10-09 07:37:59 - train: epoch 0019, iter [02100, 05004], lr: 0.100000, loss: 1.6891
2022-10-09 07:38:34 - train: epoch 0019, iter [02200, 05004], lr: 0.100000, loss: 2.0000
2022-10-09 07:39:09 - train: epoch 0019, iter [02300, 05004], lr: 0.100000, loss: 2.1522
2022-10-09 07:39:44 - train: epoch 0019, iter [02400, 05004], lr: 0.100000, loss: 1.9713
2022-10-09 07:40:20 - train: epoch 0019, iter [02500, 05004], lr: 0.100000, loss: 1.8735
2022-10-09 07:40:55 - train: epoch 0019, iter [02600, 05004], lr: 0.100000, loss: 1.9338
2022-10-09 07:41:36 - train: epoch 0019, iter [02700, 05004], lr: 0.100000, loss: 1.8425
2022-10-09 07:42:14 - train: epoch 0019, iter [02800, 05004], lr: 0.100000, loss: 1.8525
2022-10-09 07:42:54 - train: epoch 0019, iter [02900, 05004], lr: 0.100000, loss: 2.2160
2022-10-09 07:43:37 - train: epoch 0019, iter [03000, 05004], lr: 0.100000, loss: 1.9789
2022-10-09 07:44:14 - train: epoch 0019, iter [03100, 05004], lr: 0.100000, loss: 1.8700
2022-10-09 07:44:47 - train: epoch 0019, iter [03200, 05004], lr: 0.100000, loss: 1.6717
2022-10-09 07:45:23 - train: epoch 0019, iter [03300, 05004], lr: 0.100000, loss: 1.7905
2022-10-09 07:45:58 - train: epoch 0019, iter [03400, 05004], lr: 0.100000, loss: 1.7880
2022-10-09 07:46:32 - train: epoch 0019, iter [03500, 05004], lr: 0.100000, loss: 2.0043
2022-10-09 07:47:06 - train: epoch 0019, iter [03600, 05004], lr: 0.100000, loss: 1.6595
2022-10-09 07:47:46 - train: epoch 0019, iter [03700, 05004], lr: 0.100000, loss: 2.0484
2022-10-09 07:48:24 - train: epoch 0019, iter [03800, 05004], lr: 0.100000, loss: 2.1383
2022-10-09 07:49:00 - train: epoch 0019, iter [03900, 05004], lr: 0.100000, loss: 1.8619
2022-10-09 07:49:34 - train: epoch 0019, iter [04000, 05004], lr: 0.100000, loss: 1.9284
2022-10-09 07:50:10 - train: epoch 0019, iter [04100, 05004], lr: 0.100000, loss: 2.1157
2022-10-09 07:50:44 - train: epoch 0019, iter [04200, 05004], lr: 0.100000, loss: 1.9264
2022-10-09 07:51:22 - train: epoch 0019, iter [04300, 05004], lr: 0.100000, loss: 1.8602
2022-10-09 07:51:57 - train: epoch 0019, iter [04400, 05004], lr: 0.100000, loss: 2.0146
2022-10-09 07:52:30 - train: epoch 0019, iter [04500, 05004], lr: 0.100000, loss: 2.3066
2022-10-09 07:53:06 - train: epoch 0019, iter [04600, 05004], lr: 0.100000, loss: 1.9412
2022-10-09 07:53:41 - train: epoch 0019, iter [04700, 05004], lr: 0.100000, loss: 2.0060
2022-10-09 07:54:16 - train: epoch 0019, iter [04800, 05004], lr: 0.100000, loss: 1.8234
2022-10-09 07:54:50 - train: epoch 0019, iter [04900, 05004], lr: 0.100000, loss: 1.7574
2022-10-09 07:55:23 - train: epoch 0019, iter [05000, 05004], lr: 0.100000, loss: 2.0052
2022-10-09 07:55:25 - train: epoch 019, train_loss: 1.9554
2022-10-09 07:56:44 - eval: epoch: 019, acc1: 54.674%, acc5: 79.856%, test_loss: 1.9167, per_image_load_time: 2.440ms, per_image_inference_time: 0.556ms
2022-10-09 07:56:44 - until epoch: 019, best_acc1: 57.876%
2022-10-09 07:56:44 - epoch 020 lr: 0.100000
2022-10-09 07:57:27 - train: epoch 0020, iter [00100, 05004], lr: 0.100000, loss: 2.0297
2022-10-09 07:58:01 - train: epoch 0020, iter [00200, 05004], lr: 0.100000, loss: 1.7424
2022-10-09 07:58:36 - train: epoch 0020, iter [00300, 05004], lr: 0.100000, loss: 1.9460
2022-10-09 07:59:10 - train: epoch 0020, iter [00400, 05004], lr: 0.100000, loss: 1.6830
2022-10-09 07:59:44 - train: epoch 0020, iter [00500, 05004], lr: 0.100000, loss: 1.6540
2022-10-09 08:00:19 - train: epoch 0020, iter [00600, 05004], lr: 0.100000, loss: 1.9414
2022-10-09 08:00:54 - train: epoch 0020, iter [00700, 05004], lr: 0.100000, loss: 1.6703
2022-10-09 08:01:29 - train: epoch 0020, iter [00800, 05004], lr: 0.100000, loss: 2.0685
2022-10-09 08:02:03 - train: epoch 0020, iter [00900, 05004], lr: 0.100000, loss: 2.1622
2022-10-09 08:02:38 - train: epoch 0020, iter [01000, 05004], lr: 0.100000, loss: 2.0092
2022-10-09 08:03:13 - train: epoch 0020, iter [01100, 05004], lr: 0.100000, loss: 1.7237
2022-10-09 08:03:48 - train: epoch 0020, iter [01200, 05004], lr: 0.100000, loss: 1.8013
2022-10-09 08:04:21 - train: epoch 0020, iter [01300, 05004], lr: 0.100000, loss: 1.8923
2022-10-09 08:05:00 - train: epoch 0020, iter [01400, 05004], lr: 0.100000, loss: 2.0057
2022-10-09 08:05:35 - train: epoch 0020, iter [01500, 05004], lr: 0.100000, loss: 2.0692
2022-10-09 08:06:09 - train: epoch 0020, iter [01600, 05004], lr: 0.100000, loss: 2.0242
2022-10-09 08:06:44 - train: epoch 0020, iter [01700, 05004], lr: 0.100000, loss: 1.7555
2022-10-09 08:07:19 - train: epoch 0020, iter [01800, 05004], lr: 0.100000, loss: 2.0343
2022-10-09 08:07:56 - train: epoch 0020, iter [01900, 05004], lr: 0.100000, loss: 1.9658
2022-10-09 08:08:32 - train: epoch 0020, iter [02000, 05004], lr: 0.100000, loss: 1.7701
2022-10-09 08:09:07 - train: epoch 0020, iter [02100, 05004], lr: 0.100000, loss: 2.0960
2022-10-09 08:09:42 - train: epoch 0020, iter [02200, 05004], lr: 0.100000, loss: 1.9160
2022-10-09 08:10:19 - train: epoch 0020, iter [02300, 05004], lr: 0.100000, loss: 1.7367
2022-10-09 08:10:55 - train: epoch 0020, iter [02400, 05004], lr: 0.100000, loss: 2.1898
2022-10-09 08:11:33 - train: epoch 0020, iter [02500, 05004], lr: 0.100000, loss: 1.8834
2022-10-09 08:12:07 - train: epoch 0020, iter [02600, 05004], lr: 0.100000, loss: 1.8808
2022-10-09 08:12:42 - train: epoch 0020, iter [02700, 05004], lr: 0.100000, loss: 2.0836
2022-10-09 08:13:16 - train: epoch 0020, iter [02800, 05004], lr: 0.100000, loss: 1.9136
2022-10-09 08:13:50 - train: epoch 0020, iter [02900, 05004], lr: 0.100000, loss: 2.0813
2022-10-09 08:14:25 - train: epoch 0020, iter [03000, 05004], lr: 0.100000, loss: 2.2367
2022-10-09 08:15:08 - train: epoch 0020, iter [03100, 05004], lr: 0.100000, loss: 2.0185
2022-10-09 08:15:56 - train: epoch 0020, iter [03200, 05004], lr: 0.100000, loss: 2.2959
2022-10-09 08:16:39 - train: epoch 0020, iter [03300, 05004], lr: 0.100000, loss: 1.6764
2022-10-09 08:17:19 - train: epoch 0020, iter [03400, 05004], lr: 0.100000, loss: 1.9223
2022-10-09 08:18:02 - train: epoch 0020, iter [03500, 05004], lr: 0.100000, loss: 1.9225
2022-10-09 08:19:07 - train: epoch 0020, iter [03600, 05004], lr: 0.100000, loss: 1.9819
2022-10-09 08:20:10 - train: epoch 0020, iter [03700, 05004], lr: 0.100000, loss: 1.9351
2022-10-09 08:21:04 - train: epoch 0020, iter [03800, 05004], lr: 0.100000, loss: 1.9492
2022-10-09 08:22:13 - train: epoch 0020, iter [03900, 05004], lr: 0.100000, loss: 2.2139
2022-10-09 08:23:06 - train: epoch 0020, iter [04000, 05004], lr: 0.100000, loss: 1.8660
2022-10-09 08:24:07 - train: epoch 0020, iter [04100, 05004], lr: 0.100000, loss: 1.8671
2022-10-09 08:25:04 - train: epoch 0020, iter [04200, 05004], lr: 0.100000, loss: 2.0282
2022-10-09 08:26:02 - train: epoch 0020, iter [04300, 05004], lr: 0.100000, loss: 1.9731
2022-10-09 08:26:50 - train: epoch 0020, iter [04400, 05004], lr: 0.100000, loss: 1.9049
2022-10-09 08:27:26 - train: epoch 0020, iter [04500, 05004], lr: 0.100000, loss: 2.2497
2022-10-09 08:27:59 - train: epoch 0020, iter [04600, 05004], lr: 0.100000, loss: 1.9518
2022-10-09 08:28:34 - train: epoch 0020, iter [04700, 05004], lr: 0.100000, loss: 1.8662
2022-10-09 08:29:08 - train: epoch 0020, iter [04800, 05004], lr: 0.100000, loss: 1.9382
2022-10-09 08:29:42 - train: epoch 0020, iter [04900, 05004], lr: 0.100000, loss: 2.0576
2022-10-09 08:30:15 - train: epoch 0020, iter [05000, 05004], lr: 0.100000, loss: 1.8054
2022-10-09 08:30:17 - train: epoch 020, train_loss: 1.9504
2022-10-09 08:31:36 - eval: epoch: 020, acc1: 57.576%, acc5: 81.812%, test_loss: 1.7898, per_image_load_time: 2.279ms, per_image_inference_time: 0.568ms
2022-10-09 08:31:36 - until epoch: 020, best_acc1: 57.876%
2022-10-09 08:31:36 - epoch 021 lr: 0.100000
2022-10-09 08:32:18 - train: epoch 0021, iter [00100, 05004], lr: 0.100000, loss: 1.9827
2022-10-09 08:32:51 - train: epoch 0021, iter [00200, 05004], lr: 0.100000, loss: 2.0477
2022-10-09 08:33:27 - train: epoch 0021, iter [00300, 05004], lr: 0.100000, loss: 1.6614
2022-10-09 08:34:01 - train: epoch 0021, iter [00400, 05004], lr: 0.100000, loss: 1.9774
2022-10-09 08:34:34 - train: epoch 0021, iter [00500, 05004], lr: 0.100000, loss: 1.8883
2022-10-09 08:35:10 - train: epoch 0021, iter [00600, 05004], lr: 0.100000, loss: 1.7986
2022-10-09 08:35:45 - train: epoch 0021, iter [00700, 05004], lr: 0.100000, loss: 1.8787
2022-10-09 08:36:19 - train: epoch 0021, iter [00800, 05004], lr: 0.100000, loss: 2.1622
2022-10-09 08:36:53 - train: epoch 0021, iter [00900, 05004], lr: 0.100000, loss: 1.8793
2022-10-09 08:37:28 - train: epoch 0021, iter [01000, 05004], lr: 0.100000, loss: 1.7064
2022-10-09 08:38:04 - train: epoch 0021, iter [01100, 05004], lr: 0.100000, loss: 1.7319
2022-10-09 08:38:43 - train: epoch 0021, iter [01200, 05004], lr: 0.100000, loss: 1.5992
2022-10-09 08:39:20 - train: epoch 0021, iter [01300, 05004], lr: 0.100000, loss: 1.8476
2022-10-09 08:39:54 - train: epoch 0021, iter [01400, 05004], lr: 0.100000, loss: 1.6279
2022-10-09 08:40:31 - train: epoch 0021, iter [01500, 05004], lr: 0.100000, loss: 1.8234
2022-10-09 08:41:07 - train: epoch 0021, iter [01600, 05004], lr: 0.100000, loss: 1.7864
2022-10-09 08:41:42 - train: epoch 0021, iter [01700, 05004], lr: 0.100000, loss: 1.9817
2022-10-09 08:42:18 - train: epoch 0021, iter [01800, 05004], lr: 0.100000, loss: 1.8345
2022-10-09 08:42:54 - train: epoch 0021, iter [01900, 05004], lr: 0.100000, loss: 2.2337
2022-10-09 08:43:29 - train: epoch 0021, iter [02000, 05004], lr: 0.100000, loss: 2.0059
2022-10-09 08:44:03 - train: epoch 0021, iter [02100, 05004], lr: 0.100000, loss: 1.6553
2022-10-09 08:44:38 - train: epoch 0021, iter [02200, 05004], lr: 0.100000, loss: 2.0023
2022-10-09 08:45:13 - train: epoch 0021, iter [02300, 05004], lr: 0.100000, loss: 1.9687
2022-10-09 08:45:48 - train: epoch 0021, iter [02400, 05004], lr: 0.100000, loss: 1.6777
2022-10-09 08:46:21 - train: epoch 0021, iter [02500, 05004], lr: 0.100000, loss: 2.0842
2022-10-09 08:46:56 - train: epoch 0021, iter [02600, 05004], lr: 0.100000, loss: 2.1137
2022-10-09 08:47:31 - train: epoch 0021, iter [02700, 05004], lr: 0.100000, loss: 1.8478
2022-10-09 08:48:05 - train: epoch 0021, iter [02800, 05004], lr: 0.100000, loss: 1.7887
2022-10-09 08:48:38 - train: epoch 0021, iter [02900, 05004], lr: 0.100000, loss: 1.8379
2022-10-09 08:49:14 - train: epoch 0021, iter [03000, 05004], lr: 0.100000, loss: 2.0545
2022-10-09 08:49:47 - train: epoch 0021, iter [03100, 05004], lr: 0.100000, loss: 1.9167
2022-10-09 08:50:29 - train: epoch 0021, iter [03200, 05004], lr: 0.100000, loss: 1.7298
2022-10-09 08:51:02 - train: epoch 0021, iter [03300, 05004], lr: 0.100000, loss: 2.1522
2022-10-09 08:51:36 - train: epoch 0021, iter [03400, 05004], lr: 0.100000, loss: 2.0498
2022-10-09 08:52:11 - train: epoch 0021, iter [03500, 05004], lr: 0.100000, loss: 2.0457
2022-10-09 08:52:45 - train: epoch 0021, iter [03600, 05004], lr: 0.100000, loss: 1.8250
2022-10-09 08:53:23 - train: epoch 0021, iter [03700, 05004], lr: 0.100000, loss: 1.8816
2022-10-09 08:54:04 - train: epoch 0021, iter [03800, 05004], lr: 0.100000, loss: 2.0375
2022-10-09 08:54:51 - train: epoch 0021, iter [03900, 05004], lr: 0.100000, loss: 2.0798
2022-10-09 08:56:00 - train: epoch 0021, iter [04000, 05004], lr: 0.100000, loss: 2.0908
2022-10-09 08:56:52 - train: epoch 0021, iter [04100, 05004], lr: 0.100000, loss: 1.8017
2022-10-09 08:58:03 - train: epoch 0021, iter [04200, 05004], lr: 0.100000, loss: 1.9132
2022-10-09 08:58:52 - train: epoch 0021, iter [04300, 05004], lr: 0.100000, loss: 1.9616
2022-10-09 08:59:56 - train: epoch 0021, iter [04400, 05004], lr: 0.100000, loss: 1.9876
2022-10-09 09:00:35 - train: epoch 0021, iter [04500, 05004], lr: 0.100000, loss: 1.9112
2022-10-09 09:01:09 - train: epoch 0021, iter [04600, 05004], lr: 0.100000, loss: 2.1063
2022-10-09 09:01:43 - train: epoch 0021, iter [04700, 05004], lr: 0.100000, loss: 1.9746
2022-10-09 09:02:18 - train: epoch 0021, iter [04800, 05004], lr: 0.100000, loss: 2.1782
2022-10-09 09:02:52 - train: epoch 0021, iter [04900, 05004], lr: 0.100000, loss: 1.7447
2022-10-09 09:03:25 - train: epoch 0021, iter [05000, 05004], lr: 0.100000, loss: 1.8545
2022-10-09 09:03:27 - train: epoch 021, train_loss: 1.9460
2022-10-09 09:04:45 - eval: epoch: 021, acc1: 52.736%, acc5: 78.044%, test_loss: 2.0410, per_image_load_time: 2.386ms, per_image_inference_time: 0.569ms
2022-10-09 09:04:45 - until epoch: 021, best_acc1: 57.876%
2022-10-09 09:04:45 - epoch 022 lr: 0.100000
2022-10-09 09:05:26 - train: epoch 0022, iter [00100, 05004], lr: 0.100000, loss: 1.6717
2022-10-09 09:06:00 - train: epoch 0022, iter [00200, 05004], lr: 0.100000, loss: 1.9490
2022-10-09 09:06:39 - train: epoch 0022, iter [00300, 05004], lr: 0.100000, loss: 1.7710
2022-10-09 09:07:13 - train: epoch 0022, iter [00400, 05004], lr: 0.100000, loss: 1.8782
2022-10-09 09:07:51 - train: epoch 0022, iter [00500, 05004], lr: 0.100000, loss: 2.0532
2022-10-09 09:08:24 - train: epoch 0022, iter [00600, 05004], lr: 0.100000, loss: 1.9923
2022-10-09 09:08:58 - train: epoch 0022, iter [00700, 05004], lr: 0.100000, loss: 2.0440
2022-10-09 09:09:32 - train: epoch 0022, iter [00800, 05004], lr: 0.100000, loss: 2.0804
2022-10-09 09:10:05 - train: epoch 0022, iter [00900, 05004], lr: 0.100000, loss: 2.0002
2022-10-09 09:10:41 - train: epoch 0022, iter [01000, 05004], lr: 0.100000, loss: 1.9866
2022-10-09 09:11:15 - train: epoch 0022, iter [01100, 05004], lr: 0.100000, loss: 2.1326
2022-10-09 09:11:50 - train: epoch 0022, iter [01200, 05004], lr: 0.100000, loss: 1.6235
2022-10-09 09:12:24 - train: epoch 0022, iter [01300, 05004], lr: 0.100000, loss: 1.9079
2022-10-09 09:12:58 - train: epoch 0022, iter [01400, 05004], lr: 0.100000, loss: 1.6581
2022-10-09 09:13:33 - train: epoch 0022, iter [01500, 05004], lr: 0.100000, loss: 2.0798
2022-10-09 09:14:08 - train: epoch 0022, iter [01600, 05004], lr: 0.100000, loss: 1.9723
2022-10-09 09:14:42 - train: epoch 0022, iter [01700, 05004], lr: 0.100000, loss: 2.2469
2022-10-09 09:15:17 - train: epoch 0022, iter [01800, 05004], lr: 0.100000, loss: 2.0663
2022-10-09 09:15:52 - train: epoch 0022, iter [01900, 05004], lr: 0.100000, loss: 1.9030
2022-10-09 09:16:26 - train: epoch 0022, iter [02000, 05004], lr: 0.100000, loss: 1.7820
2022-10-09 09:17:01 - train: epoch 0022, iter [02100, 05004], lr: 0.100000, loss: 1.8949
2022-10-09 09:17:34 - train: epoch 0022, iter [02200, 05004], lr: 0.100000, loss: 1.7502
2022-10-09 09:18:10 - train: epoch 0022, iter [02300, 05004], lr: 0.100000, loss: 2.0372
2022-10-09 09:18:45 - train: epoch 0022, iter [02400, 05004], lr: 0.100000, loss: 2.1441
2022-10-09 09:19:20 - train: epoch 0022, iter [02500, 05004], lr: 0.100000, loss: 1.9195
2022-10-09 09:19:54 - train: epoch 0022, iter [02600, 05004], lr: 0.100000, loss: 1.7546
2022-10-09 09:20:28 - train: epoch 0022, iter [02700, 05004], lr: 0.100000, loss: 1.8424
2022-10-09 09:21:04 - train: epoch 0022, iter [02800, 05004], lr: 0.100000, loss: 2.1696
2022-10-09 09:21:38 - train: epoch 0022, iter [02900, 05004], lr: 0.100000, loss: 1.8051
2022-10-09 09:22:12 - train: epoch 0022, iter [03000, 05004], lr: 0.100000, loss: 1.9990
2022-10-09 09:22:46 - train: epoch 0022, iter [03100, 05004], lr: 0.100000, loss: 2.1280
2022-10-09 09:23:21 - train: epoch 0022, iter [03200, 05004], lr: 0.100000, loss: 2.0848
2022-10-09 09:23:55 - train: epoch 0022, iter [03300, 05004], lr: 0.100000, loss: 1.8877
2022-10-09 09:24:29 - train: epoch 0022, iter [03400, 05004], lr: 0.100000, loss: 1.8586
2022-10-09 09:25:04 - train: epoch 0022, iter [03500, 05004], lr: 0.100000, loss: 2.1430
2022-10-09 09:25:38 - train: epoch 0022, iter [03600, 05004], lr: 0.100000, loss: 1.9980
2022-10-09 09:26:12 - train: epoch 0022, iter [03700, 05004], lr: 0.100000, loss: 2.1318
2022-10-09 09:26:47 - train: epoch 0022, iter [03800, 05004], lr: 0.100000, loss: 2.1339
2022-10-09 09:27:21 - train: epoch 0022, iter [03900, 05004], lr: 0.100000, loss: 1.8572
2022-10-09 09:27:55 - train: epoch 0022, iter [04000, 05004], lr: 0.100000, loss: 1.9190
2022-10-09 09:28:30 - train: epoch 0022, iter [04100, 05004], lr: 0.100000, loss: 1.9756
2022-10-09 09:29:05 - train: epoch 0022, iter [04200, 05004], lr: 0.100000, loss: 2.1027
2022-10-09 09:29:41 - train: epoch 0022, iter [04300, 05004], lr: 0.100000, loss: 2.0724
2022-10-09 09:30:15 - train: epoch 0022, iter [04400, 05004], lr: 0.100000, loss: 1.8103
2022-10-09 09:30:49 - train: epoch 0022, iter [04500, 05004], lr: 0.100000, loss: 1.9862
2022-10-09 09:31:23 - train: epoch 0022, iter [04600, 05004], lr: 0.100000, loss: 2.1896
2022-10-09 09:31:57 - train: epoch 0022, iter [04700, 05004], lr: 0.100000, loss: 1.9226
2022-10-09 09:32:31 - train: epoch 0022, iter [04800, 05004], lr: 0.100000, loss: 1.8018
2022-10-09 09:33:07 - train: epoch 0022, iter [04900, 05004], lr: 0.100000, loss: 1.8310
2022-10-09 09:33:39 - train: epoch 0022, iter [05000, 05004], lr: 0.100000, loss: 1.8531
2022-10-09 09:33:41 - train: epoch 022, train_loss: 1.9393
2022-10-09 09:34:58 - eval: epoch: 022, acc1: 57.384%, acc5: 81.782%, test_loss: 1.7905, per_image_load_time: 1.794ms, per_image_inference_time: 0.571ms
2022-10-09 09:34:59 - until epoch: 022, best_acc1: 57.876%
2022-10-09 09:34:59 - epoch 023 lr: 0.100000
2022-10-09 09:35:39 - train: epoch 0023, iter [00100, 05004], lr: 0.100000, loss: 1.8448
2022-10-09 09:36:12 - train: epoch 0023, iter [00200, 05004], lr: 0.100000, loss: 1.8506
2022-10-09 09:36:47 - train: epoch 0023, iter [00300, 05004], lr: 0.100000, loss: 1.9750
2022-10-09 09:37:21 - train: epoch 0023, iter [00400, 05004], lr: 0.100000, loss: 1.8689
2022-10-09 09:38:02 - train: epoch 0023, iter [00500, 05004], lr: 0.100000, loss: 1.8614
2022-10-09 09:38:45 - train: epoch 0023, iter [00600, 05004], lr: 0.100000, loss: 1.9448
2022-10-09 09:39:34 - train: epoch 0023, iter [00700, 05004], lr: 0.100000, loss: 1.9089
2022-10-09 09:40:16 - train: epoch 0023, iter [00800, 05004], lr: 0.100000, loss: 2.0379
2022-10-09 09:40:54 - train: epoch 0023, iter [00900, 05004], lr: 0.100000, loss: 1.8039
2022-10-09 09:41:31 - train: epoch 0023, iter [01000, 05004], lr: 0.100000, loss: 1.9615
2022-10-09 09:42:05 - train: epoch 0023, iter [01100, 05004], lr: 0.100000, loss: 1.7994
2022-10-09 09:42:40 - train: epoch 0023, iter [01200, 05004], lr: 0.100000, loss: 1.7954
2022-10-09 09:43:14 - train: epoch 0023, iter [01300, 05004], lr: 0.100000, loss: 2.0421
2022-10-09 09:43:49 - train: epoch 0023, iter [01400, 05004], lr: 0.100000, loss: 1.9717
2022-10-09 09:44:23 - train: epoch 0023, iter [01500, 05004], lr: 0.100000, loss: 1.7878
2022-10-09 09:44:58 - train: epoch 0023, iter [01600, 05004], lr: 0.100000, loss: 1.8212
2022-10-09 09:45:35 - train: epoch 0023, iter [01700, 05004], lr: 0.100000, loss: 1.7926
2022-10-09 09:46:10 - train: epoch 0023, iter [01800, 05004], lr: 0.100000, loss: 1.9270
2022-10-09 09:46:44 - train: epoch 0023, iter [01900, 05004], lr: 0.100000, loss: 2.0081
2022-10-09 09:47:20 - train: epoch 0023, iter [02000, 05004], lr: 0.100000, loss: 1.5985
2022-10-09 09:47:56 - train: epoch 0023, iter [02100, 05004], lr: 0.100000, loss: 1.7640
2022-10-09 09:48:30 - train: epoch 0023, iter [02200, 05004], lr: 0.100000, loss: 1.7582
2022-10-09 09:49:05 - train: epoch 0023, iter [02300, 05004], lr: 0.100000, loss: 2.0482
2022-10-09 09:49:39 - train: epoch 0023, iter [02400, 05004], lr: 0.100000, loss: 2.0898
2022-10-09 09:50:14 - train: epoch 0023, iter [02500, 05004], lr: 0.100000, loss: 2.0280
2022-10-09 09:50:49 - train: epoch 0023, iter [02600, 05004], lr: 0.100000, loss: 2.0447
2022-10-09 09:51:23 - train: epoch 0023, iter [02700, 05004], lr: 0.100000, loss: 1.9306
2022-10-09 09:51:57 - train: epoch 0023, iter [02800, 05004], lr: 0.100000, loss: 2.1008
2022-10-09 09:52:31 - train: epoch 0023, iter [02900, 05004], lr: 0.100000, loss: 1.9680
2022-10-09 09:53:06 - train: epoch 0023, iter [03000, 05004], lr: 0.100000, loss: 1.9176
2022-10-09 09:53:41 - train: epoch 0023, iter [03100, 05004], lr: 0.100000, loss: 2.0362
2022-10-09 09:54:15 - train: epoch 0023, iter [03200, 05004], lr: 0.100000, loss: 2.0541
2022-10-09 09:54:49 - train: epoch 0023, iter [03300, 05004], lr: 0.100000, loss: 1.9879
2022-10-09 09:55:22 - train: epoch 0023, iter [03400, 05004], lr: 0.100000, loss: 2.1958
2022-10-09 09:55:58 - train: epoch 0023, iter [03500, 05004], lr: 0.100000, loss: 1.8925
2022-10-09 09:56:32 - train: epoch 0023, iter [03600, 05004], lr: 0.100000, loss: 1.9403
2022-10-09 09:57:07 - train: epoch 0023, iter [03700, 05004], lr: 0.100000, loss: 1.9220
2022-10-09 09:57:41 - train: epoch 0023, iter [03800, 05004], lr: 0.100000, loss: 1.9367
2022-10-09 09:58:15 - train: epoch 0023, iter [03900, 05004], lr: 0.100000, loss: 1.9255
2022-10-09 09:58:50 - train: epoch 0023, iter [04000, 05004], lr: 0.100000, loss: 1.8851
2022-10-09 09:59:24 - train: epoch 0023, iter [04100, 05004], lr: 0.100000, loss: 1.9766
2022-10-09 09:59:59 - train: epoch 0023, iter [04200, 05004], lr: 0.100000, loss: 1.7903
2022-10-09 10:00:34 - train: epoch 0023, iter [04300, 05004], lr: 0.100000, loss: 1.8066
2022-10-09 10:01:10 - train: epoch 0023, iter [04400, 05004], lr: 0.100000, loss: 1.8779
2022-10-09 10:01:44 - train: epoch 0023, iter [04500, 05004], lr: 0.100000, loss: 1.8831
2022-10-09 10:02:19 - train: epoch 0023, iter [04600, 05004], lr: 0.100000, loss: 1.8898
2022-10-09 10:02:54 - train: epoch 0023, iter [04700, 05004], lr: 0.100000, loss: 1.8137
2022-10-09 10:03:29 - train: epoch 0023, iter [04800, 05004], lr: 0.100000, loss: 1.9207
2022-10-09 10:04:05 - train: epoch 0023, iter [04900, 05004], lr: 0.100000, loss: 1.7962
2022-10-09 10:04:37 - train: epoch 0023, iter [05000, 05004], lr: 0.100000, loss: 2.1997
2022-10-09 10:04:40 - train: epoch 023, train_loss: 1.9346
2022-10-09 10:05:57 - eval: epoch: 023, acc1: 56.304%, acc5: 80.924%, test_loss: 1.8487, per_image_load_time: 2.356ms, per_image_inference_time: 0.595ms
2022-10-09 10:05:58 - until epoch: 023, best_acc1: 57.876%
2022-10-09 10:05:58 - epoch 024 lr: 0.100000
2022-10-09 10:06:39 - train: epoch 0024, iter [00100, 05004], lr: 0.100000, loss: 2.0018
2022-10-09 10:07:14 - train: epoch 0024, iter [00200, 05004], lr: 0.100000, loss: 1.7818
2022-10-09 10:07:47 - train: epoch 0024, iter [00300, 05004], lr: 0.100000, loss: 1.7658
2022-10-09 10:08:22 - train: epoch 0024, iter [00400, 05004], lr: 0.100000, loss: 1.9826
2022-10-09 10:08:56 - train: epoch 0024, iter [00500, 05004], lr: 0.100000, loss: 1.8930
2022-10-09 10:09:31 - train: epoch 0024, iter [00600, 05004], lr: 0.100000, loss: 2.0628
2022-10-09 10:10:06 - train: epoch 0024, iter [00700, 05004], lr: 0.100000, loss: 1.8744
2022-10-09 10:10:40 - train: epoch 0024, iter [00800, 05004], lr: 0.100000, loss: 1.6821
2022-10-09 10:11:15 - train: epoch 0024, iter [00900, 05004], lr: 0.100000, loss: 1.9426
2022-10-09 10:11:49 - train: epoch 0024, iter [01000, 05004], lr: 0.100000, loss: 1.9360
2022-10-09 10:12:24 - train: epoch 0024, iter [01100, 05004], lr: 0.100000, loss: 1.7323
2022-10-09 10:12:58 - train: epoch 0024, iter [01200, 05004], lr: 0.100000, loss: 1.8642
2022-10-09 10:13:31 - train: epoch 0024, iter [01300, 05004], lr: 0.100000, loss: 2.3101
2022-10-09 10:14:07 - train: epoch 0024, iter [01400, 05004], lr: 0.100000, loss: 1.9076
2022-10-09 10:14:42 - train: epoch 0024, iter [01500, 05004], lr: 0.100000, loss: 2.0604
2022-10-09 10:15:17 - train: epoch 0024, iter [01600, 05004], lr: 0.100000, loss: 1.9265
2022-10-09 10:15:51 - train: epoch 0024, iter [01700, 05004], lr: 0.100000, loss: 1.8251
2022-10-09 10:16:25 - train: epoch 0024, iter [01800, 05004], lr: 0.100000, loss: 2.2428
2022-10-09 10:17:00 - train: epoch 0024, iter [01900, 05004], lr: 0.100000, loss: 1.7249
2022-10-09 10:17:34 - train: epoch 0024, iter [02000, 05004], lr: 0.100000, loss: 2.0343
2022-10-09 10:18:09 - train: epoch 0024, iter [02100, 05004], lr: 0.100000, loss: 1.8662
2022-10-09 10:18:42 - train: epoch 0024, iter [02200, 05004], lr: 0.100000, loss: 1.7749
2022-10-09 10:19:16 - train: epoch 0024, iter [02300, 05004], lr: 0.100000, loss: 2.0917
2022-10-09 10:19:51 - train: epoch 0024, iter [02400, 05004], lr: 0.100000, loss: 2.3743
2022-10-09 10:20:25 - train: epoch 0024, iter [02500, 05004], lr: 0.100000, loss: 1.9749
2022-10-09 10:21:00 - train: epoch 0024, iter [02600, 05004], lr: 0.100000, loss: 1.9666
2022-10-09 10:21:34 - train: epoch 0024, iter [02700, 05004], lr: 0.100000, loss: 2.1124
2022-10-09 10:22:09 - train: epoch 0024, iter [02800, 05004], lr: 0.100000, loss: 2.0269
2022-10-09 10:22:43 - train: epoch 0024, iter [02900, 05004], lr: 0.100000, loss: 2.0479
2022-10-09 10:23:16 - train: epoch 0024, iter [03000, 05004], lr: 0.100000, loss: 1.9091
2022-10-09 10:23:51 - train: epoch 0024, iter [03100, 05004], lr: 0.100000, loss: 1.8323
2022-10-09 10:24:26 - train: epoch 0024, iter [03200, 05004], lr: 0.100000, loss: 2.0493
2022-10-09 10:25:05 - train: epoch 0024, iter [03300, 05004], lr: 0.100000, loss: 1.5548
2022-10-09 10:25:40 - train: epoch 0024, iter [03400, 05004], lr: 0.100000, loss: 1.9759
2022-10-09 10:26:14 - train: epoch 0024, iter [03500, 05004], lr: 0.100000, loss: 1.9207
2022-10-09 10:26:48 - train: epoch 0024, iter [03600, 05004], lr: 0.100000, loss: 2.0207
2022-10-09 10:27:23 - train: epoch 0024, iter [03700, 05004], lr: 0.100000, loss: 2.1054
2022-10-09 10:27:58 - train: epoch 0024, iter [03800, 05004], lr: 0.100000, loss: 2.1838
2022-10-09 10:28:32 - train: epoch 0024, iter [03900, 05004], lr: 0.100000, loss: 1.8987
2022-10-09 10:29:06 - train: epoch 0024, iter [04000, 05004], lr: 0.100000, loss: 2.0448
2022-10-09 10:29:40 - train: epoch 0024, iter [04100, 05004], lr: 0.100000, loss: 1.8522
2022-10-09 10:30:15 - train: epoch 0024, iter [04200, 05004], lr: 0.100000, loss: 1.8352
2022-10-09 10:30:50 - train: epoch 0024, iter [04300, 05004], lr: 0.100000, loss: 1.9462
2022-10-09 10:31:24 - train: epoch 0024, iter [04400, 05004], lr: 0.100000, loss: 1.8745
2022-10-09 10:31:59 - train: epoch 0024, iter [04500, 05004], lr: 0.100000, loss: 1.7514
2022-10-09 10:32:33 - train: epoch 0024, iter [04600, 05004], lr: 0.100000, loss: 1.9433
2022-10-09 10:33:07 - train: epoch 0024, iter [04700, 05004], lr: 0.100000, loss: 2.0999
2022-10-09 10:33:41 - train: epoch 0024, iter [04800, 05004], lr: 0.100000, loss: 1.7847
2022-10-09 10:34:16 - train: epoch 0024, iter [04900, 05004], lr: 0.100000, loss: 1.9682
2022-10-09 10:34:48 - train: epoch 0024, iter [05000, 05004], lr: 0.100000, loss: 2.0973
2022-10-09 10:34:50 - train: epoch 024, train_loss: 1.9337
2022-10-09 10:36:08 - eval: epoch: 024, acc1: 58.602%, acc5: 82.732%, test_loss: 1.7417, per_image_load_time: 2.323ms, per_image_inference_time: 0.582ms
2022-10-09 10:36:08 - until epoch: 024, best_acc1: 58.602%
2022-10-09 10:36:08 - epoch 025 lr: 0.100000
2022-10-09 10:36:49 - train: epoch 0025, iter [00100, 05004], lr: 0.100000, loss: 1.8502
2022-10-09 10:37:24 - train: epoch 0025, iter [00200, 05004], lr: 0.100000, loss: 1.7375
2022-10-09 10:37:57 - train: epoch 0025, iter [00300, 05004], lr: 0.100000, loss: 2.1088
2022-10-09 10:38:31 - train: epoch 0025, iter [00400, 05004], lr: 0.100000, loss: 2.1309
2022-10-09 10:39:13 - train: epoch 0025, iter [00500, 05004], lr: 0.100000, loss: 1.6882
2022-10-09 10:39:46 - train: epoch 0025, iter [00600, 05004], lr: 0.100000, loss: 1.7428
2022-10-09 10:40:25 - train: epoch 0025, iter [00700, 05004], lr: 0.100000, loss: 2.0749
2022-10-09 10:41:09 - train: epoch 0025, iter [00800, 05004], lr: 0.100000, loss: 1.9402
2022-10-09 10:41:41 - train: epoch 0025, iter [00900, 05004], lr: 0.100000, loss: 1.5794
2022-10-09 10:42:16 - train: epoch 0025, iter [01000, 05004], lr: 0.100000, loss: 1.9609
2022-10-09 10:42:50 - train: epoch 0025, iter [01100, 05004], lr: 0.100000, loss: 1.7492
2022-10-09 10:43:25 - train: epoch 0025, iter [01200, 05004], lr: 0.100000, loss: 1.9485
2022-10-09 10:44:00 - train: epoch 0025, iter [01300, 05004], lr: 0.100000, loss: 2.0550
2022-10-09 10:44:34 - train: epoch 0025, iter [01400, 05004], lr: 0.100000, loss: 2.0093
2022-10-09 10:45:09 - train: epoch 0025, iter [01500, 05004], lr: 0.100000, loss: 1.8832
2022-10-09 10:45:44 - train: epoch 0025, iter [01600, 05004], lr: 0.100000, loss: 1.6910
2022-10-09 10:46:18 - train: epoch 0025, iter [01700, 05004], lr: 0.100000, loss: 1.9693
2022-10-09 10:46:57 - train: epoch 0025, iter [01800, 05004], lr: 0.100000, loss: 1.8828
2022-10-09 10:47:31 - train: epoch 0025, iter [01900, 05004], lr: 0.100000, loss: 1.9958
2022-10-09 10:48:06 - train: epoch 0025, iter [02000, 05004], lr: 0.100000, loss: 1.9372
2022-10-09 10:48:43 - train: epoch 0025, iter [02100, 05004], lr: 0.100000, loss: 1.7655
2022-10-09 10:49:18 - train: epoch 0025, iter [02200, 05004], lr: 0.100000, loss: 1.5881
2022-10-09 10:49:53 - train: epoch 0025, iter [02300, 05004], lr: 0.100000, loss: 1.7929
2022-10-09 10:50:27 - train: epoch 0025, iter [02400, 05004], lr: 0.100000, loss: 1.6131
2022-10-09 10:51:01 - train: epoch 0025, iter [02500, 05004], lr: 0.100000, loss: 1.8697
2022-10-09 10:51:35 - train: epoch 0025, iter [02600, 05004], lr: 0.100000, loss: 2.1917
2022-10-09 10:52:10 - train: epoch 0025, iter [02700, 05004], lr: 0.100000, loss: 2.0284
2022-10-09 10:52:45 - train: epoch 0025, iter [02800, 05004], lr: 0.100000, loss: 1.9257
2022-10-09 10:53:20 - train: epoch 0025, iter [02900, 05004], lr: 0.100000, loss: 1.9828
2022-10-09 10:53:53 - train: epoch 0025, iter [03000, 05004], lr: 0.100000, loss: 2.0401
2022-10-09 10:54:28 - train: epoch 0025, iter [03100, 05004], lr: 0.100000, loss: 1.9318
2022-10-09 10:55:03 - train: epoch 0025, iter [03200, 05004], lr: 0.100000, loss: 1.8868
2022-10-09 10:55:38 - train: epoch 0025, iter [03300, 05004], lr: 0.100000, loss: 1.9250
2022-10-09 10:56:12 - train: epoch 0025, iter [03400, 05004], lr: 0.100000, loss: 1.9248
2022-10-09 10:56:47 - train: epoch 0025, iter [03500, 05004], lr: 0.100000, loss: 1.8092
2022-10-09 10:57:22 - train: epoch 0025, iter [03600, 05004], lr: 0.100000, loss: 2.0200
2022-10-09 10:57:56 - train: epoch 0025, iter [03700, 05004], lr: 0.100000, loss: 2.0866
2022-10-09 10:58:31 - train: epoch 0025, iter [03800, 05004], lr: 0.100000, loss: 1.9027
2022-10-09 10:59:05 - train: epoch 0025, iter [03900, 05004], lr: 0.100000, loss: 2.0520
2022-10-09 11:00:10 - train: epoch 0025, iter [04000, 05004], lr: 0.100000, loss: 1.9544
2022-10-09 11:00:58 - train: epoch 0025, iter [04100, 05004], lr: 0.100000, loss: 2.0622
2022-10-09 11:01:35 - train: epoch 0025, iter [04200, 05004], lr: 0.100000, loss: 1.9942
2022-10-09 11:02:15 - train: epoch 0025, iter [04300, 05004], lr: 0.100000, loss: 1.8120
2022-10-09 11:03:05 - train: epoch 0025, iter [04400, 05004], lr: 0.100000, loss: 1.6763
2022-10-09 11:03:52 - train: epoch 0025, iter [04500, 05004], lr: 0.100000, loss: 1.9678
2022-10-09 11:04:46 - train: epoch 0025, iter [04600, 05004], lr: 0.100000, loss: 1.8270
2022-10-09 11:05:27 - train: epoch 0025, iter [04700, 05004], lr: 0.100000, loss: 1.8292
2022-10-09 11:06:05 - train: epoch 0025, iter [04800, 05004], lr: 0.100000, loss: 1.7362
2022-10-09 11:06:40 - train: epoch 0025, iter [04900, 05004], lr: 0.100000, loss: 1.8645
2022-10-09 11:07:12 - train: epoch 0025, iter [05000, 05004], lr: 0.100000, loss: 1.9486
2022-10-09 11:07:14 - train: epoch 025, train_loss: 1.9294
2022-10-09 11:08:31 - eval: epoch: 025, acc1: 54.554%, acc5: 79.708%, test_loss: 1.9226, per_image_load_time: 1.680ms, per_image_inference_time: 0.581ms
2022-10-09 11:08:31 - until epoch: 025, best_acc1: 58.602%
2022-10-09 11:08:31 - epoch 026 lr: 0.100000
2022-10-09 11:09:11 - train: epoch 0026, iter [00100, 05004], lr: 0.100000, loss: 1.9228
2022-10-09 11:09:45 - train: epoch 0026, iter [00200, 05004], lr: 0.100000, loss: 1.6709
2022-10-09 11:10:19 - train: epoch 0026, iter [00300, 05004], lr: 0.100000, loss: 1.9262
2022-10-09 11:10:53 - train: epoch 0026, iter [00400, 05004], lr: 0.100000, loss: 1.6108
2022-10-09 11:11:27 - train: epoch 0026, iter [00500, 05004], lr: 0.100000, loss: 1.7690
2022-10-09 11:12:01 - train: epoch 0026, iter [00600, 05004], lr: 0.100000, loss: 2.2525
2022-10-09 11:12:36 - train: epoch 0026, iter [00700, 05004], lr: 0.100000, loss: 1.9435
2022-10-09 11:13:10 - train: epoch 0026, iter [00800, 05004], lr: 0.100000, loss: 1.7014
2022-10-09 11:13:44 - train: epoch 0026, iter [00900, 05004], lr: 0.100000, loss: 1.7297
2022-10-09 11:14:21 - train: epoch 0026, iter [01000, 05004], lr: 0.100000, loss: 1.8522
2022-10-09 11:14:55 - train: epoch 0026, iter [01100, 05004], lr: 0.100000, loss: 2.0103
2022-10-09 11:15:51 - train: epoch 0026, iter [01200, 05004], lr: 0.100000, loss: 1.8669
2022-10-09 11:16:26 - train: epoch 0026, iter [01300, 05004], lr: 0.100000, loss: 1.7311
2022-10-09 11:17:00 - train: epoch 0026, iter [01400, 05004], lr: 0.100000, loss: 1.6860
2022-10-09 11:17:35 - train: epoch 0026, iter [01500, 05004], lr: 0.100000, loss: 1.9874
2022-10-09 11:18:08 - train: epoch 0026, iter [01600, 05004], lr: 0.100000, loss: 1.9798
2022-10-09 11:18:42 - train: epoch 0026, iter [01700, 05004], lr: 0.100000, loss: 2.1623
2022-10-09 11:19:18 - train: epoch 0026, iter [01800, 05004], lr: 0.100000, loss: 2.1955
2022-10-09 11:19:51 - train: epoch 0026, iter [01900, 05004], lr: 0.100000, loss: 1.9184
2022-10-09 11:20:27 - train: epoch 0026, iter [02000, 05004], lr: 0.100000, loss: 1.9133
2022-10-09 11:21:02 - train: epoch 0026, iter [02100, 05004], lr: 0.100000, loss: 1.8386
2022-10-09 11:21:35 - train: epoch 0026, iter [02200, 05004], lr: 0.100000, loss: 2.0722
2022-10-09 11:22:16 - train: epoch 0026, iter [02300, 05004], lr: 0.100000, loss: 1.7674
2022-10-09 11:22:50 - train: epoch 0026, iter [02400, 05004], lr: 0.100000, loss: 2.1697
2022-10-09 11:23:24 - train: epoch 0026, iter [02500, 05004], lr: 0.100000, loss: 1.9164
2022-10-09 11:23:59 - train: epoch 0026, iter [02600, 05004], lr: 0.100000, loss: 1.9094
2022-10-09 11:24:33 - train: epoch 0026, iter [02700, 05004], lr: 0.100000, loss: 1.9576
2022-10-09 11:25:07 - train: epoch 0026, iter [02800, 05004], lr: 0.100000, loss: 1.9690
2022-10-09 11:25:42 - train: epoch 0026, iter [02900, 05004], lr: 0.100000, loss: 1.8640
2022-10-09 11:26:16 - train: epoch 0026, iter [03000, 05004], lr: 0.100000, loss: 1.9705
2022-10-09 11:26:50 - train: epoch 0026, iter [03100, 05004], lr: 0.100000, loss: 2.0877
2022-10-09 11:27:36 - train: epoch 0026, iter [03200, 05004], lr: 0.100000, loss: 1.6748
2022-10-09 11:28:31 - train: epoch 0026, iter [03300, 05004], lr: 0.100000, loss: 1.8460
2022-10-09 11:29:05 - train: epoch 0026, iter [03400, 05004], lr: 0.100000, loss: 2.0259
2022-10-09 11:29:40 - train: epoch 0026, iter [03500, 05004], lr: 0.100000, loss: 2.0189
2022-10-09 11:30:18 - train: epoch 0026, iter [03600, 05004], lr: 0.100000, loss: 1.8535
2022-10-09 11:30:52 - train: epoch 0026, iter [03700, 05004], lr: 0.100000, loss: 1.9147
2022-10-09 11:31:28 - train: epoch 0026, iter [03800, 05004], lr: 0.100000, loss: 2.0982
2022-10-09 11:32:05 - train: epoch 0026, iter [03900, 05004], lr: 0.100000, loss: 2.1217
2022-10-09 11:32:37 - train: epoch 0026, iter [04000, 05004], lr: 0.100000, loss: 2.0152
2022-10-09 11:33:12 - train: epoch 0026, iter [04100, 05004], lr: 0.100000, loss: 1.8969
2022-10-09 11:33:48 - train: epoch 0026, iter [04200, 05004], lr: 0.100000, loss: 1.9764
2022-10-09 11:34:23 - train: epoch 0026, iter [04300, 05004], lr: 0.100000, loss: 2.2279
2022-10-09 11:34:59 - train: epoch 0026, iter [04400, 05004], lr: 0.100000, loss: 2.1443
2022-10-09 11:35:33 - train: epoch 0026, iter [04500, 05004], lr: 0.100000, loss: 2.1880
2022-10-09 11:36:09 - train: epoch 0026, iter [04600, 05004], lr: 0.100000, loss: 2.0352
2022-10-09 11:36:43 - train: epoch 0026, iter [04700, 05004], lr: 0.100000, loss: 1.7502
2022-10-09 11:37:17 - train: epoch 0026, iter [04800, 05004], lr: 0.100000, loss: 1.8659
2022-10-09 11:37:52 - train: epoch 0026, iter [04900, 05004], lr: 0.100000, loss: 2.0428
2022-10-09 11:38:26 - train: epoch 0026, iter [05000, 05004], lr: 0.100000, loss: 1.9561
2022-10-09 11:38:27 - train: epoch 026, train_loss: 1.9282
2022-10-09 11:39:46 - eval: epoch: 026, acc1: 59.536%, acc5: 83.386%, test_loss: 1.6774, per_image_load_time: 2.128ms, per_image_inference_time: 0.577ms
2022-10-09 11:39:46 - until epoch: 026, best_acc1: 59.536%
2022-10-09 11:39:46 - epoch 027 lr: 0.100000
2022-10-09 11:40:27 - train: epoch 0027, iter [00100, 05004], lr: 0.100000, loss: 1.9323
2022-10-09 11:41:01 - train: epoch 0027, iter [00200, 05004], lr: 0.100000, loss: 1.8553
2022-10-09 11:41:36 - train: epoch 0027, iter [00300, 05004], lr: 0.100000, loss: 1.8150
2022-10-09 11:42:10 - train: epoch 0027, iter [00400, 05004], lr: 0.100000, loss: 1.9391
2022-10-09 11:42:45 - train: epoch 0027, iter [00500, 05004], lr: 0.100000, loss: 1.8441
2022-10-09 11:43:20 - train: epoch 0027, iter [00600, 05004], lr: 0.100000, loss: 2.1111
2022-10-09 11:43:55 - train: epoch 0027, iter [00700, 05004], lr: 0.100000, loss: 1.8406
2022-10-09 11:44:29 - train: epoch 0027, iter [00800, 05004], lr: 0.100000, loss: 1.8777
2022-10-09 11:45:02 - train: epoch 0027, iter [00900, 05004], lr: 0.100000, loss: 1.8810
2022-10-09 11:45:38 - train: epoch 0027, iter [01000, 05004], lr: 0.100000, loss: 1.9326
2022-10-09 11:46:11 - train: epoch 0027, iter [01100, 05004], lr: 0.100000, loss: 1.9327
2022-10-09 11:46:45 - train: epoch 0027, iter [01200, 05004], lr: 0.100000, loss: 2.0269
2022-10-09 11:47:21 - train: epoch 0027, iter [01300, 05004], lr: 0.100000, loss: 2.0240
2022-10-09 11:47:56 - train: epoch 0027, iter [01400, 05004], lr: 0.100000, loss: 1.9511
2022-10-09 11:48:29 - train: epoch 0027, iter [01500, 05004], lr: 0.100000, loss: 1.9746
2022-10-09 11:49:03 - train: epoch 0027, iter [01600, 05004], lr: 0.100000, loss: 2.0479
2022-10-09 11:49:38 - train: epoch 0027, iter [01700, 05004], lr: 0.100000, loss: 1.8292
2022-10-09 11:50:15 - train: epoch 0027, iter [01800, 05004], lr: 0.100000, loss: 1.8105
2022-10-09 11:50:49 - train: epoch 0027, iter [01900, 05004], lr: 0.100000, loss: 2.0375
2022-10-09 11:51:23 - train: epoch 0027, iter [02000, 05004], lr: 0.100000, loss: 1.9394
2022-10-09 11:51:57 - train: epoch 0027, iter [02100, 05004], lr: 0.100000, loss: 1.8855
2022-10-09 11:52:32 - train: epoch 0027, iter [02200, 05004], lr: 0.100000, loss: 2.0718
2022-10-09 11:53:07 - train: epoch 0027, iter [02300, 05004], lr: 0.100000, loss: 2.1922
2022-10-09 11:53:41 - train: epoch 0027, iter [02400, 05004], lr: 0.100000, loss: 1.9320
2022-10-09 11:54:16 - train: epoch 0027, iter [02500, 05004], lr: 0.100000, loss: 1.8831
2022-10-09 11:54:51 - train: epoch 0027, iter [02600, 05004], lr: 0.100000, loss: 1.7004
2022-10-09 11:55:25 - train: epoch 0027, iter [02700, 05004], lr: 0.100000, loss: 1.8005
2022-10-09 11:55:58 - train: epoch 0027, iter [02800, 05004], lr: 0.100000, loss: 1.8141
2022-10-09 11:56:34 - train: epoch 0027, iter [02900, 05004], lr: 0.100000, loss: 2.0897
2022-10-09 11:57:09 - train: epoch 0027, iter [03000, 05004], lr: 0.100000, loss: 1.7272
2022-10-09 11:57:43 - train: epoch 0027, iter [03100, 05004], lr: 0.100000, loss: 1.8023
2022-10-09 11:58:18 - train: epoch 0027, iter [03200, 05004], lr: 0.100000, loss: 1.7570
2022-10-09 11:58:51 - train: epoch 0027, iter [03300, 05004], lr: 0.100000, loss: 1.9746
2022-10-09 11:59:26 - train: epoch 0027, iter [03400, 05004], lr: 0.100000, loss: 1.8287
2022-10-09 11:59:59 - train: epoch 0027, iter [03500, 05004], lr: 0.100000, loss: 2.1171
2022-10-09 12:00:34 - train: epoch 0027, iter [03600, 05004], lr: 0.100000, loss: 2.1673
2022-10-09 12:01:09 - train: epoch 0027, iter [03700, 05004], lr: 0.100000, loss: 1.7901
2022-10-09 12:01:44 - train: epoch 0027, iter [03800, 05004], lr: 0.100000, loss: 1.8104
2022-10-09 12:02:17 - train: epoch 0027, iter [03900, 05004], lr: 0.100000, loss: 1.7049
2022-10-09 12:03:00 - train: epoch 0027, iter [04000, 05004], lr: 0.100000, loss: 2.3230
2022-10-09 12:03:35 - train: epoch 0027, iter [04100, 05004], lr: 0.100000, loss: 1.8415
2022-10-09 12:04:13 - train: epoch 0027, iter [04200, 05004], lr: 0.100000, loss: 1.9362
2022-10-09 12:04:50 - train: epoch 0027, iter [04300, 05004], lr: 0.100000, loss: 1.8325
2022-10-09 12:05:26 - train: epoch 0027, iter [04400, 05004], lr: 0.100000, loss: 1.7303
2022-10-09 12:06:01 - train: epoch 0027, iter [04500, 05004], lr: 0.100000, loss: 2.0048
2022-10-09 12:06:36 - train: epoch 0027, iter [04600, 05004], lr: 0.100000, loss: 1.8139
2022-10-09 12:07:13 - train: epoch 0027, iter [04700, 05004], lr: 0.100000, loss: 1.9035
2022-10-09 12:07:48 - train: epoch 0027, iter [04800, 05004], lr: 0.100000, loss: 2.0071
2022-10-09 12:08:21 - train: epoch 0027, iter [04900, 05004], lr: 0.100000, loss: 1.9617
2022-10-09 12:08:53 - train: epoch 0027, iter [05000, 05004], lr: 0.100000, loss: 1.6356
2022-10-09 12:08:55 - train: epoch 027, train_loss: 1.9240
2022-10-09 12:10:14 - eval: epoch: 027, acc1: 59.492%, acc5: 83.204%, test_loss: 1.6997, per_image_load_time: 2.370ms, per_image_inference_time: 0.567ms
2022-10-09 12:10:14 - until epoch: 027, best_acc1: 59.536%
2022-10-09 12:10:14 - epoch 028 lr: 0.100000
2022-10-09 12:10:55 - train: epoch 0028, iter [00100, 05004], lr: 0.100000, loss: 1.6594
2022-10-09 12:11:29 - train: epoch 0028, iter [00200, 05004], lr: 0.100000, loss: 1.9061
2022-10-09 12:12:05 - train: epoch 0028, iter [00300, 05004], lr: 0.100000, loss: 1.9466
2022-10-09 12:12:39 - train: epoch 0028, iter [00400, 05004], lr: 0.100000, loss: 1.8985
2022-10-09 12:13:13 - train: epoch 0028, iter [00500, 05004], lr: 0.100000, loss: 1.6986
2022-10-09 12:13:49 - train: epoch 0028, iter [00600, 05004], lr: 0.100000, loss: 2.0430
2022-10-09 12:14:22 - train: epoch 0028, iter [00700, 05004], lr: 0.100000, loss: 2.0215
2022-10-09 12:14:56 - train: epoch 0028, iter [00800, 05004], lr: 0.100000, loss: 1.8064
2022-10-09 12:15:32 - train: epoch 0028, iter [00900, 05004], lr: 0.100000, loss: 1.8088
2022-10-09 12:16:06 - train: epoch 0028, iter [01000, 05004], lr: 0.100000, loss: 2.0170
2022-10-09 12:16:40 - train: epoch 0028, iter [01100, 05004], lr: 0.100000, loss: 1.9272
2022-10-09 12:17:15 - train: epoch 0028, iter [01200, 05004], lr: 0.100000, loss: 1.9926
2022-10-09 12:17:50 - train: epoch 0028, iter [01300, 05004], lr: 0.100000, loss: 1.8651
2022-10-09 12:18:26 - train: epoch 0028, iter [01400, 05004], lr: 0.100000, loss: 2.2225
2022-10-09 12:19:02 - train: epoch 0028, iter [01500, 05004], lr: 0.100000, loss: 1.6699
2022-10-09 12:19:37 - train: epoch 0028, iter [01600, 05004], lr: 0.100000, loss: 1.9314
2022-10-09 12:20:12 - train: epoch 0028, iter [01700, 05004], lr: 0.100000, loss: 2.0544
2022-10-09 12:20:47 - train: epoch 0028, iter [01800, 05004], lr: 0.100000, loss: 1.8831
2022-10-09 12:21:20 - train: epoch 0028, iter [01900, 05004], lr: 0.100000, loss: 2.0755
2022-10-09 12:21:55 - train: epoch 0028, iter [02000, 05004], lr: 0.100000, loss: 1.8960
2022-10-09 12:22:30 - train: epoch 0028, iter [02100, 05004], lr: 0.100000, loss: 2.0844
2022-10-09 12:23:03 - train: epoch 0028, iter [02200, 05004], lr: 0.100000, loss: 2.1018
2022-10-09 12:23:39 - train: epoch 0028, iter [02300, 05004], lr: 0.100000, loss: 2.0111
2022-10-09 12:24:13 - train: epoch 0028, iter [02400, 05004], lr: 0.100000, loss: 2.0775
2022-10-09 12:24:48 - train: epoch 0028, iter [02500, 05004], lr: 0.100000, loss: 1.9131
2022-10-09 12:25:22 - train: epoch 0028, iter [02600, 05004], lr: 0.100000, loss: 1.9638
2022-10-09 12:25:57 - train: epoch 0028, iter [02700, 05004], lr: 0.100000, loss: 2.0534
2022-10-09 12:26:32 - train: epoch 0028, iter [02800, 05004], lr: 0.100000, loss: 1.8276
2022-10-09 12:27:07 - train: epoch 0028, iter [02900, 05004], lr: 0.100000, loss: 1.8811
2022-10-09 12:27:41 - train: epoch 0028, iter [03000, 05004], lr: 0.100000, loss: 1.9324
2022-10-09 12:28:16 - train: epoch 0028, iter [03100, 05004], lr: 0.100000, loss: 2.0933
2022-10-09 12:28:52 - train: epoch 0028, iter [03200, 05004], lr: 0.100000, loss: 1.8193
2022-10-09 12:29:26 - train: epoch 0028, iter [03300, 05004], lr: 0.100000, loss: 1.8471
2022-10-09 12:30:02 - train: epoch 0028, iter [03400, 05004], lr: 0.100000, loss: 2.0118
2022-10-09 12:30:36 - train: epoch 0028, iter [03500, 05004], lr: 0.100000, loss: 1.8969
2022-10-09 12:31:11 - train: epoch 0028, iter [03600, 05004], lr: 0.100000, loss: 1.6969
2022-10-09 12:31:46 - train: epoch 0028, iter [03700, 05004], lr: 0.100000, loss: 1.9759
2022-10-09 12:32:21 - train: epoch 0028, iter [03800, 05004], lr: 0.100000, loss: 1.6526
2022-10-09 12:32:55 - train: epoch 0028, iter [03900, 05004], lr: 0.100000, loss: 1.9487
2022-10-09 12:33:31 - train: epoch 0028, iter [04000, 05004], lr: 0.100000, loss: 2.0612
2022-10-09 12:34:07 - train: epoch 0028, iter [04100, 05004], lr: 0.100000, loss: 1.9338
2022-10-09 12:34:42 - train: epoch 0028, iter [04200, 05004], lr: 0.100000, loss: 2.0896
2022-10-09 12:35:17 - train: epoch 0028, iter [04300, 05004], lr: 0.100000, loss: 1.8636
2022-10-09 12:35:52 - train: epoch 0028, iter [04400, 05004], lr: 0.100000, loss: 1.9281
2022-10-09 12:36:27 - train: epoch 0028, iter [04500, 05004], lr: 0.100000, loss: 2.0575
2022-10-09 12:37:01 - train: epoch 0028, iter [04600, 05004], lr: 0.100000, loss: 1.8529
2022-10-09 12:37:35 - train: epoch 0028, iter [04700, 05004], lr: 0.100000, loss: 2.0270
2022-10-09 12:38:10 - train: epoch 0028, iter [04800, 05004], lr: 0.100000, loss: 1.8415
2022-10-09 12:38:45 - train: epoch 0028, iter [04900, 05004], lr: 0.100000, loss: 1.9089
2022-10-09 12:39:16 - train: epoch 0028, iter [05000, 05004], lr: 0.100000, loss: 1.7592
2022-10-09 12:39:18 - train: epoch 028, train_loss: 1.9208
2022-10-09 12:40:37 - eval: epoch: 028, acc1: 59.264%, acc5: 82.870%, test_loss: 1.7140, per_image_load_time: 2.386ms, per_image_inference_time: 0.569ms
2022-10-09 12:40:37 - until epoch: 028, best_acc1: 59.536%
2022-10-09 12:40:37 - epoch 029 lr: 0.100000
2022-10-09 12:41:19 - train: epoch 0029, iter [00100, 05004], lr: 0.100000, loss: 1.9131
2022-10-09 12:41:51 - train: epoch 0029, iter [00200, 05004], lr: 0.100000, loss: 1.9027
2022-10-09 12:42:28 - train: epoch 0029, iter [00300, 05004], lr: 0.100000, loss: 2.0190
2022-10-09 12:43:03 - train: epoch 0029, iter [00400, 05004], lr: 0.100000, loss: 1.8411
2022-10-09 12:43:37 - train: epoch 0029, iter [00500, 05004], lr: 0.100000, loss: 1.8584
2022-10-09 12:44:12 - train: epoch 0029, iter [00600, 05004], lr: 0.100000, loss: 1.8983
2022-10-09 12:44:45 - train: epoch 0029, iter [00700, 05004], lr: 0.100000, loss: 1.6293
2022-10-09 12:45:19 - train: epoch 0029, iter [00800, 05004], lr: 0.100000, loss: 1.9088
2022-10-09 12:45:54 - train: epoch 0029, iter [00900, 05004], lr: 0.100000, loss: 2.0659
2022-10-09 12:46:28 - train: epoch 0029, iter [01000, 05004], lr: 0.100000, loss: 1.7826
2022-10-09 12:47:03 - train: epoch 0029, iter [01100, 05004], lr: 0.100000, loss: 1.5905
2022-10-09 12:47:38 - train: epoch 0029, iter [01200, 05004], lr: 0.100000, loss: 1.8937
2022-10-09 12:48:11 - train: epoch 0029, iter [01300, 05004], lr: 0.100000, loss: 1.8377
2022-10-09 12:48:47 - train: epoch 0029, iter [01400, 05004], lr: 0.100000, loss: 1.9355
2022-10-09 12:49:21 - train: epoch 0029, iter [01500, 05004], lr: 0.100000, loss: 1.8245
2022-10-09 12:49:56 - train: epoch 0029, iter [01600, 05004], lr: 0.100000, loss: 1.8919
2022-10-09 12:50:30 - train: epoch 0029, iter [01700, 05004], lr: 0.100000, loss: 1.9940
2022-10-09 12:51:05 - train: epoch 0029, iter [01800, 05004], lr: 0.100000, loss: 1.9198
2022-10-09 12:51:39 - train: epoch 0029, iter [01900, 05004], lr: 0.100000, loss: 1.7678
2022-10-09 12:52:14 - train: epoch 0029, iter [02000, 05004], lr: 0.100000, loss: 1.9967
2022-10-09 12:52:48 - train: epoch 0029, iter [02100, 05004], lr: 0.100000, loss: 2.0178
2022-10-09 12:53:22 - train: epoch 0029, iter [02200, 05004], lr: 0.100000, loss: 2.0813
2022-10-09 12:53:58 - train: epoch 0029, iter [02300, 05004], lr: 0.100000, loss: 1.9452
2022-10-09 12:54:32 - train: epoch 0029, iter [02400, 05004], lr: 0.100000, loss: 1.7524
2022-10-09 12:55:06 - train: epoch 0029, iter [02500, 05004], lr: 0.100000, loss: 1.8640
2022-10-09 12:55:41 - train: epoch 0029, iter [02600, 05004], lr: 0.100000, loss: 2.0478
2022-10-09 12:56:16 - train: epoch 0029, iter [02700, 05004], lr: 0.100000, loss: 1.9618
2022-10-09 12:56:50 - train: epoch 0029, iter [02800, 05004], lr: 0.100000, loss: 2.0195
2022-10-09 12:57:24 - train: epoch 0029, iter [02900, 05004], lr: 0.100000, loss: 2.1190
2022-10-09 12:57:58 - train: epoch 0029, iter [03000, 05004], lr: 0.100000, loss: 1.7626
2022-10-09 12:58:32 - train: epoch 0029, iter [03100, 05004], lr: 0.100000, loss: 2.0189
2022-10-09 12:59:07 - train: epoch 0029, iter [03200, 05004], lr: 0.100000, loss: 2.0659
2022-10-09 12:59:41 - train: epoch 0029, iter [03300, 05004], lr: 0.100000, loss: 1.7534
2022-10-09 13:00:15 - train: epoch 0029, iter [03400, 05004], lr: 0.100000, loss: 1.8811
2022-10-09 13:00:50 - train: epoch 0029, iter [03500, 05004], lr: 0.100000, loss: 2.0195
2022-10-09 13:01:23 - train: epoch 0029, iter [03600, 05004], lr: 0.100000, loss: 1.7509
2022-10-09 13:01:58 - train: epoch 0029, iter [03700, 05004], lr: 0.100000, loss: 1.9696
2022-10-09 13:02:33 - train: epoch 0029, iter [03800, 05004], lr: 0.100000, loss: 1.8631
2022-10-09 13:03:08 - train: epoch 0029, iter [03900, 05004], lr: 0.100000, loss: 1.7534
2022-10-09 13:03:46 - train: epoch 0029, iter [04000, 05004], lr: 0.100000, loss: 1.9411
2022-10-09 13:04:21 - train: epoch 0029, iter [04100, 05004], lr: 0.100000, loss: 2.1274
2022-10-09 13:05:00 - train: epoch 0029, iter [04200, 05004], lr: 0.100000, loss: 1.6961
2022-10-09 13:05:50 - train: epoch 0029, iter [04300, 05004], lr: 0.100000, loss: 2.0487
2022-10-09 13:06:22 - train: epoch 0029, iter [04400, 05004], lr: 0.100000, loss: 2.0027
2022-10-09 13:06:57 - train: epoch 0029, iter [04500, 05004], lr: 0.100000, loss: 1.8863
2022-10-09 13:07:34 - train: epoch 0029, iter [04600, 05004], lr: 0.100000, loss: 2.0264
2022-10-09 13:08:07 - train: epoch 0029, iter [04700, 05004], lr: 0.100000, loss: 1.6959
2022-10-09 13:08:44 - train: epoch 0029, iter [04800, 05004], lr: 0.100000, loss: 1.8676
2022-10-09 13:09:39 - train: epoch 0029, iter [04900, 05004], lr: 0.100000, loss: 2.1397
2022-10-09 13:10:45 - train: epoch 0029, iter [05000, 05004], lr: 0.100000, loss: 1.6522
2022-10-09 13:10:46 - train: epoch 029, train_loss: 1.9168
2022-10-09 13:12:04 - eval: epoch: 029, acc1: 58.420%, acc5: 82.392%, test_loss: 1.7354, per_image_load_time: 1.328ms, per_image_inference_time: 0.551ms
2022-10-09 13:12:04 - until epoch: 029, best_acc1: 59.536%
2022-10-09 13:12:04 - epoch 030 lr: 0.100000
2022-10-09 13:12:44 - train: epoch 0030, iter [00100, 05004], lr: 0.100000, loss: 1.9404
2022-10-09 13:13:19 - train: epoch 0030, iter [00200, 05004], lr: 0.100000, loss: 2.0680
2022-10-09 13:13:54 - train: epoch 0030, iter [00300, 05004], lr: 0.100000, loss: 1.6999
2022-10-09 13:14:28 - train: epoch 0030, iter [00400, 05004], lr: 0.100000, loss: 1.7749
2022-10-09 13:15:03 - train: epoch 0030, iter [00500, 05004], lr: 0.100000, loss: 1.9767
2022-10-09 13:15:37 - train: epoch 0030, iter [00600, 05004], lr: 0.100000, loss: 1.9966
2022-10-09 13:16:13 - train: epoch 0030, iter [00700, 05004], lr: 0.100000, loss: 1.8950
2022-10-09 13:16:47 - train: epoch 0030, iter [00800, 05004], lr: 0.100000, loss: 2.0253
2022-10-09 13:17:21 - train: epoch 0030, iter [00900, 05004], lr: 0.100000, loss: 2.0599
2022-10-09 13:17:56 - train: epoch 0030, iter [01000, 05004], lr: 0.100000, loss: 1.7632
2022-10-09 13:18:30 - train: epoch 0030, iter [01100, 05004], lr: 0.100000, loss: 1.7753
2022-10-09 13:19:05 - train: epoch 0030, iter [01200, 05004], lr: 0.100000, loss: 1.9768
2022-10-09 13:19:40 - train: epoch 0030, iter [01300, 05004], lr: 0.100000, loss: 1.8993
2022-10-09 13:20:14 - train: epoch 0030, iter [01400, 05004], lr: 0.100000, loss: 1.7050
2022-10-09 13:20:48 - train: epoch 0030, iter [01500, 05004], lr: 0.100000, loss: 1.6940
2022-10-09 13:21:23 - train: epoch 0030, iter [01600, 05004], lr: 0.100000, loss: 1.9060
2022-10-09 13:21:58 - train: epoch 0030, iter [01700, 05004], lr: 0.100000, loss: 2.0415
2022-10-09 13:22:32 - train: epoch 0030, iter [01800, 05004], lr: 0.100000, loss: 1.9369
2022-10-09 13:23:07 - train: epoch 0030, iter [01900, 05004], lr: 0.100000, loss: 1.6810
2022-10-09 13:23:43 - train: epoch 0030, iter [02000, 05004], lr: 0.100000, loss: 1.8358
2022-10-09 13:24:18 - train: epoch 0030, iter [02100, 05004], lr: 0.100000, loss: 1.9423
2022-10-09 13:24:52 - train: epoch 0030, iter [02200, 05004], lr: 0.100000, loss: 2.0971
2022-10-09 13:25:26 - train: epoch 0030, iter [02300, 05004], lr: 0.100000, loss: 1.8535
2022-10-09 13:26:01 - train: epoch 0030, iter [02400, 05004], lr: 0.100000, loss: 1.9371
2022-10-09 13:26:35 - train: epoch 0030, iter [02500, 05004], lr: 0.100000, loss: 2.2490
2022-10-09 13:27:11 - train: epoch 0030, iter [02600, 05004], lr: 0.100000, loss: 1.9007
2022-10-09 13:27:47 - train: epoch 0030, iter [02700, 05004], lr: 0.100000, loss: 1.9017
2022-10-09 13:28:19 - train: epoch 0030, iter [02800, 05004], lr: 0.100000, loss: 1.8665
2022-10-09 13:28:55 - train: epoch 0030, iter [02900, 05004], lr: 0.100000, loss: 1.7309
2022-10-09 13:29:29 - train: epoch 0030, iter [03000, 05004], lr: 0.100000, loss: 1.9690
2022-10-09 13:30:03 - train: epoch 0030, iter [03100, 05004], lr: 0.100000, loss: 1.9210
2022-10-09 13:30:38 - train: epoch 0030, iter [03200, 05004], lr: 0.100000, loss: 1.9774
2022-10-09 13:31:13 - train: epoch 0030, iter [03300, 05004], lr: 0.100000, loss: 2.0303
2022-10-09 13:31:48 - train: epoch 0030, iter [03400, 05004], lr: 0.100000, loss: 1.8752
2022-10-09 13:32:22 - train: epoch 0030, iter [03500, 05004], lr: 0.100000, loss: 1.7736
2022-10-09 13:32:57 - train: epoch 0030, iter [03600, 05004], lr: 0.100000, loss: 1.9303
2022-10-09 13:33:31 - train: epoch 0030, iter [03700, 05004], lr: 0.100000, loss: 1.8378
2022-10-09 13:34:06 - train: epoch 0030, iter [03800, 05004], lr: 0.100000, loss: 1.9645
2022-10-09 13:34:41 - train: epoch 0030, iter [03900, 05004], lr: 0.100000, loss: 1.7626
2022-10-09 13:35:16 - train: epoch 0030, iter [04000, 05004], lr: 0.100000, loss: 1.7774
2022-10-09 13:35:52 - train: epoch 0030, iter [04100, 05004], lr: 0.100000, loss: 1.9276
2022-10-09 13:36:26 - train: epoch 0030, iter [04200, 05004], lr: 0.100000, loss: 2.2471
2022-10-09 13:37:02 - train: epoch 0030, iter [04300, 05004], lr: 0.100000, loss: 2.0337
2022-10-09 13:37:36 - train: epoch 0030, iter [04400, 05004], lr: 0.100000, loss: 1.9191
2022-10-09 13:38:10 - train: epoch 0030, iter [04500, 05004], lr: 0.100000, loss: 1.9453
2022-10-09 13:38:45 - train: epoch 0030, iter [04600, 05004], lr: 0.100000, loss: 1.7297
2022-10-09 13:39:19 - train: epoch 0030, iter [04700, 05004], lr: 0.100000, loss: 2.1223
2022-10-09 13:39:53 - train: epoch 0030, iter [04800, 05004], lr: 0.100000, loss: 1.8356
2022-10-09 13:40:29 - train: epoch 0030, iter [04900, 05004], lr: 0.100000, loss: 1.8961
2022-10-09 13:41:01 - train: epoch 0030, iter [05000, 05004], lr: 0.100000, loss: 2.0650
2022-10-09 13:41:03 - train: epoch 030, train_loss: 1.9149
2022-10-09 13:42:21 - eval: epoch: 030, acc1: 56.640%, acc5: 81.280%, test_loss: 1.8225, per_image_load_time: 2.124ms, per_image_inference_time: 0.549ms
2022-10-09 13:42:21 - until epoch: 030, best_acc1: 59.536%
2022-10-09 13:42:21 - epoch 031 lr: 0.010000
2022-10-09 13:43:02 - train: epoch 0031, iter [00100, 05004], lr: 0.010000, loss: 1.6857
2022-10-09 13:43:38 - train: epoch 0031, iter [00200, 05004], lr: 0.010000, loss: 1.3932
2022-10-09 13:44:12 - train: epoch 0031, iter [00300, 05004], lr: 0.010000, loss: 1.4410
2022-10-09 13:44:46 - train: epoch 0031, iter [00400, 05004], lr: 0.010000, loss: 1.6359
2022-10-09 13:45:21 - train: epoch 0031, iter [00500, 05004], lr: 0.010000, loss: 1.6626
2022-10-09 13:45:54 - train: epoch 0031, iter [00600, 05004], lr: 0.010000, loss: 1.4217
2022-10-09 13:46:28 - train: epoch 0031, iter [00700, 05004], lr: 0.010000, loss: 1.6424
2022-10-09 13:47:02 - train: epoch 0031, iter [00800, 05004], lr: 0.010000, loss: 1.5183
2022-10-09 13:47:36 - train: epoch 0031, iter [00900, 05004], lr: 0.010000, loss: 1.4712
2022-10-09 13:48:11 - train: epoch 0031, iter [01000, 05004], lr: 0.010000, loss: 1.6755
2022-10-09 13:48:54 - train: epoch 0031, iter [01100, 05004], lr: 0.010000, loss: 1.5169
2022-10-09 13:49:34 - train: epoch 0031, iter [01200, 05004], lr: 0.010000, loss: 1.5134
2022-10-09 13:50:15 - train: epoch 0031, iter [01300, 05004], lr: 0.010000, loss: 1.3105
2022-10-09 13:50:49 - train: epoch 0031, iter [01400, 05004], lr: 0.010000, loss: 1.4587
2022-10-09 13:51:23 - train: epoch 0031, iter [01500, 05004], lr: 0.010000, loss: 1.4657
2022-10-09 13:52:00 - train: epoch 0031, iter [01600, 05004], lr: 0.010000, loss: 1.3803
2022-10-09 13:52:37 - train: epoch 0031, iter [01700, 05004], lr: 0.010000, loss: 1.5057
2022-10-09 13:53:10 - train: epoch 0031, iter [01800, 05004], lr: 0.010000, loss: 1.3448
2022-10-09 13:53:46 - train: epoch 0031, iter [01900, 05004], lr: 0.010000, loss: 1.3412
2022-10-09 13:54:34 - train: epoch 0031, iter [02000, 05004], lr: 0.010000, loss: 1.4962
2022-10-09 13:55:11 - train: epoch 0031, iter [02100, 05004], lr: 0.010000, loss: 1.3626
2022-10-09 13:55:48 - train: epoch 0031, iter [02200, 05004], lr: 0.010000, loss: 1.1291
2022-10-09 13:56:23 - train: epoch 0031, iter [02300, 05004], lr: 0.010000, loss: 1.2588
2022-10-09 13:56:58 - train: epoch 0031, iter [02400, 05004], lr: 0.010000, loss: 1.4084
2022-10-09 13:57:32 - train: epoch 0031, iter [02500, 05004], lr: 0.010000, loss: 1.2092
2022-10-09 13:58:07 - train: epoch 0031, iter [02600, 05004], lr: 0.010000, loss: 1.2244
2022-10-09 13:58:41 - train: epoch 0031, iter [02700, 05004], lr: 0.010000, loss: 1.4539
2022-10-09 13:59:15 - train: epoch 0031, iter [02800, 05004], lr: 0.010000, loss: 1.6908
2022-10-09 13:59:50 - train: epoch 0031, iter [02900, 05004], lr: 0.010000, loss: 1.4474
2022-10-09 14:00:24 - train: epoch 0031, iter [03000, 05004], lr: 0.010000, loss: 1.5407
2022-10-09 14:01:00 - train: epoch 0031, iter [03100, 05004], lr: 0.010000, loss: 1.2737
2022-10-09 14:01:36 - train: epoch 0031, iter [03200, 05004], lr: 0.010000, loss: 1.6733
2022-10-09 14:02:13 - train: epoch 0031, iter [03300, 05004], lr: 0.010000, loss: 1.5554
2022-10-09 14:02:47 - train: epoch 0031, iter [03400, 05004], lr: 0.010000, loss: 1.4444
2022-10-09 14:03:23 - train: epoch 0031, iter [03500, 05004], lr: 0.010000, loss: 1.6504
2022-10-09 14:03:55 - train: epoch 0031, iter [03600, 05004], lr: 0.010000, loss: 1.3423
2022-10-09 14:04:31 - train: epoch 0031, iter [03700, 05004], lr: 0.010000, loss: 1.2389
2022-10-09 14:05:04 - train: epoch 0031, iter [03800, 05004], lr: 0.010000, loss: 1.2720
2022-10-09 14:05:40 - train: epoch 0031, iter [03900, 05004], lr: 0.010000, loss: 1.2450
2022-10-09 14:06:16 - train: epoch 0031, iter [04000, 05004], lr: 0.010000, loss: 1.4026
2022-10-09 14:06:50 - train: epoch 0031, iter [04100, 05004], lr: 0.010000, loss: 1.4413
2022-10-09 14:07:26 - train: epoch 0031, iter [04200, 05004], lr: 0.010000, loss: 1.4072
2022-10-09 14:08:00 - train: epoch 0031, iter [04300, 05004], lr: 0.010000, loss: 1.3259
2022-10-09 14:08:34 - train: epoch 0031, iter [04400, 05004], lr: 0.010000, loss: 1.3161
2022-10-09 14:09:08 - train: epoch 0031, iter [04500, 05004], lr: 0.010000, loss: 1.3693
2022-10-09 14:09:42 - train: epoch 0031, iter [04600, 05004], lr: 0.010000, loss: 1.4555
2022-10-09 14:10:17 - train: epoch 0031, iter [04700, 05004], lr: 0.010000, loss: 1.0953
2022-10-09 14:10:51 - train: epoch 0031, iter [04800, 05004], lr: 0.010000, loss: 1.4830
2022-10-09 14:11:26 - train: epoch 0031, iter [04900, 05004], lr: 0.010000, loss: 1.4628
2022-10-09 14:11:58 - train: epoch 0031, iter [05000, 05004], lr: 0.010000, loss: 1.3771
2022-10-09 14:12:00 - train: epoch 031, train_loss: 1.4336
2022-10-09 14:13:18 - eval: epoch: 031, acc1: 71.550%, acc5: 90.464%, test_loss: 1.1362, per_image_load_time: 2.113ms, per_image_inference_time: 0.580ms
2022-10-09 14:13:19 - until epoch: 031, best_acc1: 71.550%
2022-10-09 14:13:19 - epoch 032 lr: 0.010000
2022-10-09 14:13:58 - train: epoch 0032, iter [00100, 05004], lr: 0.010000, loss: 1.2465
2022-10-09 14:14:34 - train: epoch 0032, iter [00200, 05004], lr: 0.010000, loss: 1.1795
2022-10-09 14:15:09 - train: epoch 0032, iter [00300, 05004], lr: 0.010000, loss: 1.1367
2022-10-09 14:15:43 - train: epoch 0032, iter [00400, 05004], lr: 0.010000, loss: 1.3829
2022-10-09 14:16:17 - train: epoch 0032, iter [00500, 05004], lr: 0.010000, loss: 1.2834
2022-10-09 14:16:51 - train: epoch 0032, iter [00600, 05004], lr: 0.010000, loss: 1.3583
2022-10-09 14:17:26 - train: epoch 0032, iter [00700, 05004], lr: 0.010000, loss: 1.3009
2022-10-09 14:17:59 - train: epoch 0032, iter [00800, 05004], lr: 0.010000, loss: 1.2862
2022-10-09 14:18:34 - train: epoch 0032, iter [00900, 05004], lr: 0.010000, loss: 1.3514
2022-10-09 14:19:08 - train: epoch 0032, iter [01000, 05004], lr: 0.010000, loss: 1.5411
2022-10-09 14:19:42 - train: epoch 0032, iter [01100, 05004], lr: 0.010000, loss: 1.3934
2022-10-09 14:20:17 - train: epoch 0032, iter [01200, 05004], lr: 0.010000, loss: 1.2953
2022-10-09 14:20:51 - train: epoch 0032, iter [01300, 05004], lr: 0.010000, loss: 1.2843
2022-10-09 14:21:27 - train: epoch 0032, iter [01400, 05004], lr: 0.010000, loss: 1.6206
2022-10-09 14:22:01 - train: epoch 0032, iter [01500, 05004], lr: 0.010000, loss: 1.3125
2022-10-09 14:22:35 - train: epoch 0032, iter [01600, 05004], lr: 0.010000, loss: 1.4104
2022-10-09 14:23:09 - train: epoch 0032, iter [01700, 05004], lr: 0.010000, loss: 1.3471
2022-10-09 14:23:44 - train: epoch 0032, iter [01800, 05004], lr: 0.010000, loss: 1.6228
2022-10-09 14:24:18 - train: epoch 0032, iter [01900, 05004], lr: 0.010000, loss: 1.2110
2022-10-09 14:24:54 - train: epoch 0032, iter [02000, 05004], lr: 0.010000, loss: 1.2439
2022-10-09 14:25:27 - train: epoch 0032, iter [02100, 05004], lr: 0.010000, loss: 1.1753
2022-10-09 14:26:02 - train: epoch 0032, iter [02200, 05004], lr: 0.010000, loss: 1.3880
2022-10-09 14:26:36 - train: epoch 0032, iter [02300, 05004], lr: 0.010000, loss: 1.1655
2022-10-09 14:27:10 - train: epoch 0032, iter [02400, 05004], lr: 0.010000, loss: 1.3367
2022-10-09 14:27:44 - train: epoch 0032, iter [02500, 05004], lr: 0.010000, loss: 1.2610
2022-10-09 14:28:20 - train: epoch 0032, iter [02600, 05004], lr: 0.010000, loss: 1.0799
2022-10-09 14:28:55 - train: epoch 0032, iter [02700, 05004], lr: 0.010000, loss: 1.2493
2022-10-09 14:29:29 - train: epoch 0032, iter [02800, 05004], lr: 0.010000, loss: 1.3398
2022-10-09 14:30:04 - train: epoch 0032, iter [02900, 05004], lr: 0.010000, loss: 1.2802
2022-10-09 14:30:38 - train: epoch 0032, iter [03000, 05004], lr: 0.010000, loss: 1.1519
2022-10-09 14:31:12 - train: epoch 0032, iter [03100, 05004], lr: 0.010000, loss: 1.3057
2022-10-09 14:31:47 - train: epoch 0032, iter [03200, 05004], lr: 0.010000, loss: 1.4208
2022-10-09 14:32:22 - train: epoch 0032, iter [03300, 05004], lr: 0.010000, loss: 1.3044
2022-10-09 14:32:56 - train: epoch 0032, iter [03400, 05004], lr: 0.010000, loss: 1.0821
2022-10-09 14:33:32 - train: epoch 0032, iter [03500, 05004], lr: 0.010000, loss: 1.2417
2022-10-09 14:34:07 - train: epoch 0032, iter [03600, 05004], lr: 0.010000, loss: 1.3097
2022-10-09 14:34:42 - train: epoch 0032, iter [03700, 05004], lr: 0.010000, loss: 1.4709
2022-10-09 14:35:16 - train: epoch 0032, iter [03800, 05004], lr: 0.010000, loss: 1.2197
2022-10-09 14:35:52 - train: epoch 0032, iter [03900, 05004], lr: 0.010000, loss: 1.3390
2022-10-09 14:36:26 - train: epoch 0032, iter [04000, 05004], lr: 0.010000, loss: 1.1601
2022-10-09 14:37:00 - train: epoch 0032, iter [04100, 05004], lr: 0.010000, loss: 1.0923
2022-10-09 14:37:33 - train: epoch 0032, iter [04200, 05004], lr: 0.010000, loss: 1.1193
2022-10-09 14:38:07 - train: epoch 0032, iter [04300, 05004], lr: 0.010000, loss: 1.4722
2022-10-09 14:38:42 - train: epoch 0032, iter [04400, 05004], lr: 0.010000, loss: 1.3656
2022-10-09 14:39:18 - train: epoch 0032, iter [04500, 05004], lr: 0.010000, loss: 1.5103
2022-10-09 14:40:00 - train: epoch 0032, iter [04600, 05004], lr: 0.010000, loss: 1.2362
2022-10-09 14:40:43 - train: epoch 0032, iter [04700, 05004], lr: 0.010000, loss: 1.4428
2022-10-09 14:41:30 - train: epoch 0032, iter [04800, 05004], lr: 0.010000, loss: 1.2030
2022-10-09 14:42:09 - train: epoch 0032, iter [04900, 05004], lr: 0.010000, loss: 1.2773
2022-10-09 14:43:06 - train: epoch 0032, iter [05000, 05004], lr: 0.010000, loss: 1.3196
2022-10-09 14:43:08 - train: epoch 032, train_loss: 1.3137
2022-10-09 14:44:27 - eval: epoch: 032, acc1: 72.270%, acc5: 91.074%, test_loss: 1.0978, per_image_load_time: 2.383ms, per_image_inference_time: 0.561ms
2022-10-09 14:44:27 - until epoch: 032, best_acc1: 72.270%
2022-10-09 14:44:27 - epoch 033 lr: 0.010000
2022-10-09 14:45:07 - train: epoch 0033, iter [00100, 05004], lr: 0.010000, loss: 1.1476
2022-10-09 14:45:41 - train: epoch 0033, iter [00200, 05004], lr: 0.010000, loss: 1.5707
2022-10-09 14:46:15 - train: epoch 0033, iter [00300, 05004], lr: 0.010000, loss: 1.1931
2022-10-09 14:46:50 - train: epoch 0033, iter [00400, 05004], lr: 0.010000, loss: 1.3629
2022-10-09 14:47:24 - train: epoch 0033, iter [00500, 05004], lr: 0.010000, loss: 1.1944
2022-10-09 14:47:59 - train: epoch 0033, iter [00600, 05004], lr: 0.010000, loss: 1.0457
2022-10-09 14:48:32 - train: epoch 0033, iter [00700, 05004], lr: 0.010000, loss: 1.3762
2022-10-09 14:49:07 - train: epoch 0033, iter [00800, 05004], lr: 0.010000, loss: 1.3338
2022-10-09 14:49:41 - train: epoch 0033, iter [00900, 05004], lr: 0.010000, loss: 1.3941
2022-10-09 14:50:15 - train: epoch 0033, iter [01000, 05004], lr: 0.010000, loss: 1.2559
2022-10-09 14:50:50 - train: epoch 0033, iter [01100, 05004], lr: 0.010000, loss: 1.1572
2022-10-09 14:51:24 - train: epoch 0033, iter [01200, 05004], lr: 0.010000, loss: 1.4365
2022-10-09 14:51:58 - train: epoch 0033, iter [01300, 05004], lr: 0.010000, loss: 1.1819
2022-10-09 14:52:33 - train: epoch 0033, iter [01400, 05004], lr: 0.010000, loss: 1.0953
2022-10-09 14:53:07 - train: epoch 0033, iter [01500, 05004], lr: 0.010000, loss: 1.5319
2022-10-09 14:53:41 - train: epoch 0033, iter [01600, 05004], lr: 0.010000, loss: 1.4368
2022-10-09 14:54:15 - train: epoch 0033, iter [01700, 05004], lr: 0.010000, loss: 1.2269
2022-10-09 14:54:50 - train: epoch 0033, iter [01800, 05004], lr: 0.010000, loss: 1.3874
2022-10-09 14:55:25 - train: epoch 0033, iter [01900, 05004], lr: 0.010000, loss: 1.3339
2022-10-09 14:55:58 - train: epoch 0033, iter [02000, 05004], lr: 0.010000, loss: 1.3038
2022-10-09 14:56:32 - train: epoch 0033, iter [02100, 05004], lr: 0.010000, loss: 1.2448
2022-10-09 14:57:07 - train: epoch 0033, iter [02200, 05004], lr: 0.010000, loss: 1.4230
2022-10-09 14:57:41 - train: epoch 0033, iter [02300, 05004], lr: 0.010000, loss: 1.3453
2022-10-09 14:58:15 - train: epoch 0033, iter [02400, 05004], lr: 0.010000, loss: 1.4729
2022-10-09 14:58:49 - train: epoch 0033, iter [02500, 05004], lr: 0.010000, loss: 1.1693
2022-10-09 14:59:28 - train: epoch 0033, iter [02600, 05004], lr: 0.010000, loss: 1.3553
2022-10-09 15:00:08 - train: epoch 0033, iter [02700, 05004], lr: 0.010000, loss: 1.3085
2022-10-09 15:00:55 - train: epoch 0033, iter [02800, 05004], lr: 0.010000, loss: 1.3105
2022-10-09 15:01:29 - train: epoch 0033, iter [02900, 05004], lr: 0.010000, loss: 1.3238
2022-10-09 15:02:03 - train: epoch 0033, iter [03000, 05004], lr: 0.010000, loss: 1.3069
2022-10-09 15:02:38 - train: epoch 0033, iter [03100, 05004], lr: 0.010000, loss: 1.2231
2022-10-09 15:03:18 - train: epoch 0033, iter [03200, 05004], lr: 0.010000, loss: 1.3265
2022-10-09 15:04:36 - train: epoch 0033, iter [03300, 05004], lr: 0.010000, loss: 1.2197
2022-10-09 15:05:55 - train: epoch 0033, iter [03400, 05004], lr: 0.010000, loss: 1.2479
2022-10-09 15:06:32 - train: epoch 0033, iter [03500, 05004], lr: 0.010000, loss: 1.2248
2022-10-09 15:07:11 - train: epoch 0033, iter [03600, 05004], lr: 0.010000, loss: 1.4693
2022-10-09 15:07:45 - train: epoch 0033, iter [03700, 05004], lr: 0.010000, loss: 1.2556
2022-10-09 15:08:24 - train: epoch 0033, iter [03800, 05004], lr: 0.010000, loss: 1.3147
2022-10-09 15:09:01 - train: epoch 0033, iter [03900, 05004], lr: 0.010000, loss: 1.4701
2022-10-09 15:09:42 - train: epoch 0033, iter [04000, 05004], lr: 0.010000, loss: 1.3975
2022-10-09 15:10:21 - train: epoch 0033, iter [04100, 05004], lr: 0.010000, loss: 1.2149
2022-10-09 15:11:02 - train: epoch 0033, iter [04200, 05004], lr: 0.010000, loss: 1.2526
2022-10-09 15:11:38 - train: epoch 0033, iter [04300, 05004], lr: 0.010000, loss: 1.4413
2022-10-09 15:12:12 - train: epoch 0033, iter [04400, 05004], lr: 0.010000, loss: 1.2961
2022-10-09 15:12:47 - train: epoch 0033, iter [04500, 05004], lr: 0.010000, loss: 1.3812
2022-10-09 15:13:21 - train: epoch 0033, iter [04600, 05004], lr: 0.010000, loss: 1.2468
2022-10-09 15:13:55 - train: epoch 0033, iter [04700, 05004], lr: 0.010000, loss: 1.1447
2022-10-09 15:14:30 - train: epoch 0033, iter [04800, 05004], lr: 0.010000, loss: 1.5099
2022-10-09 15:15:05 - train: epoch 0033, iter [04900, 05004], lr: 0.010000, loss: 1.1668
2022-10-09 15:15:38 - train: epoch 0033, iter [05000, 05004], lr: 0.010000, loss: 1.1688
2022-10-09 15:15:40 - train: epoch 033, train_loss: 1.2633
2022-10-09 15:16:58 - eval: epoch: 033, acc1: 72.848%, acc5: 91.278%, test_loss: 1.0766, per_image_load_time: 2.137ms, per_image_inference_time: 0.569ms
2022-10-09 15:16:58 - until epoch: 033, best_acc1: 72.848%
2022-10-09 15:16:58 - epoch 034 lr: 0.010000
2022-10-09 15:17:38 - train: epoch 0034, iter [00100, 05004], lr: 0.010000, loss: 1.2244
2022-10-09 15:18:12 - train: epoch 0034, iter [00200, 05004], lr: 0.010000, loss: 1.1927
2022-10-09 15:18:46 - train: epoch 0034, iter [00300, 05004], lr: 0.010000, loss: 1.3663
2022-10-09 15:19:20 - train: epoch 0034, iter [00400, 05004], lr: 0.010000, loss: 1.0944
2022-10-09 15:19:54 - train: epoch 0034, iter [00500, 05004], lr: 0.010000, loss: 1.1838
2022-10-09 15:20:28 - train: epoch 0034, iter [00600, 05004], lr: 0.010000, loss: 1.4972
2022-10-09 15:21:01 - train: epoch 0034, iter [00700, 05004], lr: 0.010000, loss: 1.0324
2022-10-09 15:21:36 - train: epoch 0034, iter [00800, 05004], lr: 0.010000, loss: 1.4674
2022-10-09 15:22:10 - train: epoch 0034, iter [00900, 05004], lr: 0.010000, loss: 1.1520
2022-10-09 15:22:45 - train: epoch 0034, iter [01000, 05004], lr: 0.010000, loss: 1.1952
2022-10-09 15:23:22 - train: epoch 0034, iter [01100, 05004], lr: 0.010000, loss: 1.1389
2022-10-09 15:23:54 - train: epoch 0034, iter [01200, 05004], lr: 0.010000, loss: 1.3873
2022-10-09 15:24:29 - train: epoch 0034, iter [01300, 05004], lr: 0.010000, loss: 1.1123
2022-10-09 15:25:05 - train: epoch 0034, iter [01400, 05004], lr: 0.010000, loss: 1.0712
2022-10-09 15:25:41 - train: epoch 0034, iter [01500, 05004], lr: 0.010000, loss: 1.1477
2022-10-09 15:26:15 - train: epoch 0034, iter [01600, 05004], lr: 0.010000, loss: 1.2791
2022-10-09 15:26:49 - train: epoch 0034, iter [01700, 05004], lr: 0.010000, loss: 1.3298
2022-10-09 15:27:24 - train: epoch 0034, iter [01800, 05004], lr: 0.010000, loss: 1.3082
2022-10-09 15:27:58 - train: epoch 0034, iter [01900, 05004], lr: 0.010000, loss: 1.3308
2022-10-09 15:28:32 - train: epoch 0034, iter [02000, 05004], lr: 0.010000, loss: 1.2705
2022-10-09 15:29:06 - train: epoch 0034, iter [02100, 05004], lr: 0.010000, loss: 1.2612
2022-10-09 15:29:40 - train: epoch 0034, iter [02200, 05004], lr: 0.010000, loss: 1.1727
2022-10-09 15:30:15 - train: epoch 0034, iter [02300, 05004], lr: 0.010000, loss: 1.3316
2022-10-09 15:30:51 - train: epoch 0034, iter [02400, 05004], lr: 0.010000, loss: 1.2033
2022-10-09 15:31:25 - train: epoch 0034, iter [02500, 05004], lr: 0.010000, loss: 1.2672
2022-10-09 15:32:02 - train: epoch 0034, iter [02600, 05004], lr: 0.010000, loss: 1.4249
2022-10-09 15:32:41 - train: epoch 0034, iter [02700, 05004], lr: 0.010000, loss: 1.2534
2022-10-09 15:33:18 - train: epoch 0034, iter [02800, 05004], lr: 0.010000, loss: 0.9998
2022-10-09 15:33:53 - train: epoch 0034, iter [02900, 05004], lr: 0.010000, loss: 1.0598
2022-10-09 15:34:28 - train: epoch 0034, iter [03000, 05004], lr: 0.010000, loss: 1.1653
2022-10-09 15:35:04 - train: epoch 0034, iter [03100, 05004], lr: 0.010000, loss: 1.2082
2022-10-09 15:35:42 - train: epoch 0034, iter [03200, 05004], lr: 0.010000, loss: 1.3706
2022-10-09 15:36:21 - train: epoch 0034, iter [03300, 05004], lr: 0.010000, loss: 1.1015
2022-10-09 15:36:59 - train: epoch 0034, iter [03400, 05004], lr: 0.010000, loss: 1.3512
2022-10-09 15:37:38 - train: epoch 0034, iter [03500, 05004], lr: 0.010000, loss: 1.0998
2022-10-09 15:38:13 - train: epoch 0034, iter [03600, 05004], lr: 0.010000, loss: 1.2235
2022-10-09 15:38:46 - train: epoch 0034, iter [03700, 05004], lr: 0.010000, loss: 1.2640
2022-10-09 15:39:23 - train: epoch 0034, iter [03800, 05004], lr: 0.010000, loss: 1.1060
2022-10-09 15:40:40 - train: epoch 0034, iter [03900, 05004], lr: 0.010000, loss: 1.4132
2022-10-09 15:42:00 - train: epoch 0034, iter [04000, 05004], lr: 0.010000, loss: 1.0756
2022-10-09 15:43:16 - train: epoch 0034, iter [04100, 05004], lr: 0.010000, loss: 1.4259
2022-10-09 15:44:34 - train: epoch 0034, iter [04200, 05004], lr: 0.010000, loss: 1.1682
2022-10-09 15:45:18 - train: epoch 0034, iter [04300, 05004], lr: 0.010000, loss: 1.1067
2022-10-09 15:45:53 - train: epoch 0034, iter [04400, 05004], lr: 0.010000, loss: 1.1685
2022-10-09 15:46:26 - train: epoch 0034, iter [04500, 05004], lr: 0.010000, loss: 1.3023
2022-10-09 15:47:03 - train: epoch 0034, iter [04600, 05004], lr: 0.010000, loss: 1.2482
2022-10-09 15:47:37 - train: epoch 0034, iter [04700, 05004], lr: 0.010000, loss: 1.2136
2022-10-09 15:48:10 - train: epoch 0034, iter [04800, 05004], lr: 0.010000, loss: 1.2177
2022-10-09 15:48:47 - train: epoch 0034, iter [04900, 05004], lr: 0.010000, loss: 1.3556
2022-10-09 15:49:20 - train: epoch 0034, iter [05000, 05004], lr: 0.010000, loss: 1.0690
2022-10-09 15:49:22 - train: epoch 034, train_loss: 1.2323
2022-10-09 15:50:39 - eval: epoch: 034, acc1: 72.842%, acc5: 91.402%, test_loss: 1.0700, per_image_load_time: 2.330ms, per_image_inference_time: 0.576ms
2022-10-09 15:50:40 - until epoch: 034, best_acc1: 72.848%
2022-10-09 15:50:40 - epoch 035 lr: 0.010000
2022-10-09 15:51:21 - train: epoch 0035, iter [00100, 05004], lr: 0.010000, loss: 0.9778
2022-10-09 15:51:55 - train: epoch 0035, iter [00200, 05004], lr: 0.010000, loss: 0.9551
2022-10-09 15:52:29 - train: epoch 0035, iter [00300, 05004], lr: 0.010000, loss: 1.3691
2022-10-09 15:53:04 - train: epoch 0035, iter [00400, 05004], lr: 0.010000, loss: 1.0323
2022-10-09 15:53:38 - train: epoch 0035, iter [00500, 05004], lr: 0.010000, loss: 1.3518
2022-10-09 15:54:12 - train: epoch 0035, iter [00600, 05004], lr: 0.010000, loss: 1.1164
2022-10-09 15:54:46 - train: epoch 0035, iter [00700, 05004], lr: 0.010000, loss: 1.1891
2022-10-09 15:55:21 - train: epoch 0035, iter [00800, 05004], lr: 0.010000, loss: 1.1432
2022-10-09 15:55:54 - train: epoch 0035, iter [00900, 05004], lr: 0.010000, loss: 1.2686
2022-10-09 15:56:30 - train: epoch 0035, iter [01000, 05004], lr: 0.010000, loss: 1.1489
2022-10-09 15:57:03 - train: epoch 0035, iter [01100, 05004], lr: 0.010000, loss: 1.3582
2022-10-09 15:57:37 - train: epoch 0035, iter [01200, 05004], lr: 0.010000, loss: 1.1046
2022-10-09 15:58:12 - train: epoch 0035, iter [01300, 05004], lr: 0.010000, loss: 1.2953
2022-10-09 15:58:45 - train: epoch 0035, iter [01400, 05004], lr: 0.010000, loss: 1.2277
2022-10-09 15:59:20 - train: epoch 0035, iter [01500, 05004], lr: 0.010000, loss: 1.2672
2022-10-09 15:59:56 - train: epoch 0035, iter [01600, 05004], lr: 0.010000, loss: 1.2370
2022-10-09 16:00:30 - train: epoch 0035, iter [01700, 05004], lr: 0.010000, loss: 1.1506
2022-10-09 16:01:06 - train: epoch 0035, iter [01800, 05004], lr: 0.010000, loss: 1.2635
2022-10-09 16:01:43 - train: epoch 0035, iter [01900, 05004], lr: 0.010000, loss: 1.1888
2022-10-09 16:02:19 - train: epoch 0035, iter [02000, 05004], lr: 0.010000, loss: 1.2579
2022-10-09 16:02:53 - train: epoch 0035, iter [02100, 05004], lr: 0.010000, loss: 1.0939
2022-10-09 16:03:29 - train: epoch 0035, iter [02200, 05004], lr: 0.010000, loss: 1.2708
2022-10-09 16:04:02 - train: epoch 0035, iter [02300, 05004], lr: 0.010000, loss: 1.3091
2022-10-09 16:04:37 - train: epoch 0035, iter [02400, 05004], lr: 0.010000, loss: 1.2769
2022-10-09 16:05:12 - train: epoch 0035, iter [02500, 05004], lr: 0.010000, loss: 1.0603
2022-10-09 16:05:46 - train: epoch 0035, iter [02600, 05004], lr: 0.010000, loss: 1.3415
2022-10-09 16:06:21 - train: epoch 0035, iter [02700, 05004], lr: 0.010000, loss: 1.2757
2022-10-09 16:06:56 - train: epoch 0035, iter [02800, 05004], lr: 0.010000, loss: 1.2098
2022-10-09 16:07:34 - train: epoch 0035, iter [02900, 05004], lr: 0.010000, loss: 1.1986
2022-10-09 16:08:07 - train: epoch 0035, iter [03000, 05004], lr: 0.010000, loss: 1.0956
2022-10-09 16:08:41 - train: epoch 0035, iter [03100, 05004], lr: 0.010000, loss: 1.0404
2022-10-09 16:09:15 - train: epoch 0035, iter [03200, 05004], lr: 0.010000, loss: 1.1773
2022-10-09 16:09:50 - train: epoch 0035, iter [03300, 05004], lr: 0.010000, loss: 1.1285
2022-10-09 16:10:40 - train: epoch 0035, iter [03400, 05004], lr: 0.010000, loss: 1.2735
2022-10-09 16:11:44 - train: epoch 0035, iter [03500, 05004], lr: 0.010000, loss: 1.1216
2022-10-09 16:12:30 - train: epoch 0035, iter [03600, 05004], lr: 0.010000, loss: 1.2565
2022-10-09 16:13:12 - train: epoch 0035, iter [03700, 05004], lr: 0.010000, loss: 1.0517
2022-10-09 16:14:19 - train: epoch 0035, iter [03800, 05004], lr: 0.010000, loss: 1.2236
2022-10-09 16:15:13 - train: epoch 0035, iter [03900, 05004], lr: 0.010000, loss: 1.2042
2022-10-09 16:16:04 - train: epoch 0035, iter [04000, 05004], lr: 0.010000, loss: 0.9611
2022-10-09 16:17:10 - train: epoch 0035, iter [04100, 05004], lr: 0.010000, loss: 1.0909
2022-10-09 16:18:09 - train: epoch 0035, iter [04200, 05004], lr: 0.010000, loss: 1.1297
2022-10-09 16:19:23 - train: epoch 0035, iter [04300, 05004], lr: 0.010000, loss: 1.3121
2022-10-09 16:19:59 - train: epoch 0035, iter [04400, 05004], lr: 0.010000, loss: 1.4394
2022-10-09 16:20:38 - train: epoch 0035, iter [04500, 05004], lr: 0.010000, loss: 1.2196
2022-10-09 16:21:23 - train: epoch 0035, iter [04600, 05004], lr: 0.010000, loss: 1.4144
2022-10-09 16:21:55 - train: epoch 0035, iter [04700, 05004], lr: 0.010000, loss: 1.3154
2022-10-09 16:22:31 - train: epoch 0035, iter [04800, 05004], lr: 0.010000, loss: 1.2947
2022-10-09 16:23:05 - train: epoch 0035, iter [04900, 05004], lr: 0.010000, loss: 1.3283
2022-10-09 16:23:37 - train: epoch 0035, iter [05000, 05004], lr: 0.010000, loss: 1.2787
2022-10-09 16:23:39 - train: epoch 035, train_loss: 1.2078
2022-10-09 16:24:57 - eval: epoch: 035, acc1: 73.316%, acc5: 91.630%, test_loss: 1.0544, per_image_load_time: 2.366ms, per_image_inference_time: 0.579ms
2022-10-09 16:24:57 - until epoch: 035, best_acc1: 73.316%
2022-10-09 16:24:57 - epoch 036 lr: 0.010000
2022-10-09 16:25:40 - train: epoch 0036, iter [00100, 05004], lr: 0.010000, loss: 1.3223
2022-10-09 16:26:11 - train: epoch 0036, iter [00200, 05004], lr: 0.010000, loss: 1.1184
2022-10-09 16:26:45 - train: epoch 0036, iter [00300, 05004], lr: 0.010000, loss: 1.0303
2022-10-09 16:27:21 - train: epoch 0036, iter [00400, 05004], lr: 0.010000, loss: 1.0718
2022-10-09 16:27:55 - train: epoch 0036, iter [00500, 05004], lr: 0.010000, loss: 1.0349
2022-10-09 16:28:28 - train: epoch 0036, iter [00600, 05004], lr: 0.010000, loss: 1.2362
2022-10-09 16:29:03 - train: epoch 0036, iter [00700, 05004], lr: 0.010000, loss: 1.1304
2022-10-09 16:29:38 - train: epoch 0036, iter [00800, 05004], lr: 0.010000, loss: 1.0856
2022-10-09 16:30:10 - train: epoch 0036, iter [00900, 05004], lr: 0.010000, loss: 1.1313
2022-10-09 16:30:46 - train: epoch 0036, iter [01000, 05004], lr: 0.010000, loss: 1.0304
2022-10-09 16:31:20 - train: epoch 0036, iter [01100, 05004], lr: 0.010000, loss: 1.2038
2022-10-09 16:31:54 - train: epoch 0036, iter [01200, 05004], lr: 0.010000, loss: 1.1543
2022-10-09 16:32:29 - train: epoch 0036, iter [01300, 05004], lr: 0.010000, loss: 1.1552
2022-10-09 16:33:03 - train: epoch 0036, iter [01400, 05004], lr: 0.010000, loss: 1.3165
2022-10-09 16:33:37 - train: epoch 0036, iter [01500, 05004], lr: 0.010000, loss: 1.4631
2022-10-09 16:34:11 - train: epoch 0036, iter [01600, 05004], lr: 0.010000, loss: 1.2444
2022-10-09 16:34:45 - train: epoch 0036, iter [01700, 05004], lr: 0.010000, loss: 1.0939
2022-10-09 16:35:20 - train: epoch 0036, iter [01800, 05004], lr: 0.010000, loss: 0.9913
2022-10-09 16:35:57 - train: epoch 0036, iter [01900, 05004], lr: 0.010000, loss: 1.3234
2022-10-09 16:36:37 - train: epoch 0036, iter [02000, 05004], lr: 0.010000, loss: 1.2032
2022-10-09 16:37:11 - train: epoch 0036, iter [02100, 05004], lr: 0.010000, loss: 0.9867
2022-10-09 16:37:46 - train: epoch 0036, iter [02200, 05004], lr: 0.010000, loss: 1.2487
2022-10-09 16:38:29 - train: epoch 0036, iter [02300, 05004], lr: 0.010000, loss: 1.3574
2022-10-09 16:39:36 - train: epoch 0036, iter [02400, 05004], lr: 0.010000, loss: 1.2633
2022-10-09 16:40:36 - train: epoch 0036, iter [02500, 05004], lr: 0.010000, loss: 1.0861
2022-10-09 16:41:24 - train: epoch 0036, iter [02600, 05004], lr: 0.010000, loss: 1.1071
2022-10-09 16:42:31 - train: epoch 0036, iter [02700, 05004], lr: 0.010000, loss: 1.1867
2022-10-09 16:44:25 - train: epoch 0036, iter [02800, 05004], lr: 0.010000, loss: 1.1113
2022-10-09 16:46:04 - train: epoch 0036, iter [02900, 05004], lr: 0.010000, loss: 1.0139
2022-10-09 16:47:19 - train: epoch 0036, iter [03000, 05004], lr: 0.010000, loss: 1.0921
2022-10-09 16:48:04 - train: epoch 0036, iter [03100, 05004], lr: 0.010000, loss: 1.3572
2022-10-09 16:48:40 - train: epoch 0036, iter [03200, 05004], lr: 0.010000, loss: 1.2658
2022-10-09 16:49:18 - train: epoch 0036, iter [03300, 05004], lr: 0.010000, loss: 1.1576
2022-10-09 16:49:55 - train: epoch 0036, iter [03400, 05004], lr: 0.010000, loss: 1.1357
2022-10-09 16:50:27 - train: epoch 0036, iter [03500, 05004], lr: 0.010000, loss: 1.3410
2022-10-09 16:51:02 - train: epoch 0036, iter [03600, 05004], lr: 0.010000, loss: 1.3159
2022-10-09 16:51:36 - train: epoch 0036, iter [03700, 05004], lr: 0.010000, loss: 1.0017
2022-10-09 16:52:10 - train: epoch 0036, iter [03800, 05004], lr: 0.010000, loss: 1.2750
2022-10-09 16:52:46 - train: epoch 0036, iter [03900, 05004], lr: 0.010000, loss: 1.2312
2022-10-09 16:53:20 - train: epoch 0036, iter [04000, 05004], lr: 0.010000, loss: 1.1410
2022-10-09 16:53:54 - train: epoch 0036, iter [04100, 05004], lr: 0.010000, loss: 1.2004
2022-10-09 16:54:28 - train: epoch 0036, iter [04200, 05004], lr: 0.010000, loss: 1.2477
2022-10-09 16:55:02 - train: epoch 0036, iter [04300, 05004], lr: 0.010000, loss: 1.0959
2022-10-09 16:55:59 - train: epoch 0036, iter [04400, 05004], lr: 0.010000, loss: 1.2817
2022-10-09 16:57:00 - train: epoch 0036, iter [04500, 05004], lr: 0.010000, loss: 1.0935
2022-10-09 16:58:01 - train: epoch 0036, iter [04600, 05004], lr: 0.010000, loss: 1.2341
2022-10-09 16:59:29 - train: epoch 0036, iter [04700, 05004], lr: 0.010000, loss: 1.0317
2022-10-09 17:00:21 - train: epoch 0036, iter [04800, 05004], lr: 0.010000, loss: 1.0967
2022-10-09 17:00:56 - train: epoch 0036, iter [04900, 05004], lr: 0.010000, loss: 1.4103
2022-10-09 17:01:28 - train: epoch 0036, iter [05000, 05004], lr: 0.010000, loss: 1.1082
2022-10-09 17:01:30 - train: epoch 036, train_loss: 1.1913
2022-10-09 17:02:46 - eval: epoch: 036, acc1: 73.148%, acc5: 91.432%, test_loss: 1.0675, per_image_load_time: 0.916ms, per_image_inference_time: 0.580ms
2022-10-09 17:02:46 - until epoch: 036, best_acc1: 73.316%
2022-10-09 17:02:46 - epoch 037 lr: 0.010000
2022-10-09 17:03:26 - train: epoch 0037, iter [00100, 05004], lr: 0.010000, loss: 1.0912
2022-10-09 17:04:00 - train: epoch 0037, iter [00200, 05004], lr: 0.010000, loss: 1.0006
2022-10-09 17:04:35 - train: epoch 0037, iter [00300, 05004], lr: 0.010000, loss: 1.2531
2022-10-09 17:05:09 - train: epoch 0037, iter [00400, 05004], lr: 0.010000, loss: 1.0768
2022-10-09 17:05:43 - train: epoch 0037, iter [00500, 05004], lr: 0.010000, loss: 1.0996
2022-10-09 17:06:17 - train: epoch 0037, iter [00600, 05004], lr: 0.010000, loss: 1.2036
2022-10-09 17:06:52 - train: epoch 0037, iter [00700, 05004], lr: 0.010000, loss: 1.1472
2022-10-09 17:07:25 - train: epoch 0037, iter [00800, 05004], lr: 0.010000, loss: 1.1128
2022-10-09 17:08:00 - train: epoch 0037, iter [00900, 05004], lr: 0.010000, loss: 1.2224
2022-10-09 17:08:34 - train: epoch 0037, iter [01000, 05004], lr: 0.010000, loss: 1.2421
2022-10-09 17:09:07 - train: epoch 0037, iter [01100, 05004], lr: 0.010000, loss: 1.1173
2022-10-09 17:09:43 - train: epoch 0037, iter [01200, 05004], lr: 0.010000, loss: 1.2219
2022-10-09 17:10:17 - train: epoch 0037, iter [01300, 05004], lr: 0.010000, loss: 1.3493
2022-10-09 17:10:51 - train: epoch 0037, iter [01400, 05004], lr: 0.010000, loss: 1.3097
2022-10-09 17:11:25 - train: epoch 0037, iter [01500, 05004], lr: 0.010000, loss: 1.1669
2022-10-09 17:12:00 - train: epoch 0037, iter [01600, 05004], lr: 0.010000, loss: 1.0777
2022-10-09 17:12:34 - train: epoch 0037, iter [01700, 05004], lr: 0.010000, loss: 1.2280
2022-10-09 17:13:08 - train: epoch 0037, iter [01800, 05004], lr: 0.010000, loss: 1.1083
2022-10-09 17:13:42 - train: epoch 0037, iter [01900, 05004], lr: 0.010000, loss: 1.0724
2022-10-09 17:14:16 - train: epoch 0037, iter [02000, 05004], lr: 0.010000, loss: 1.0585
2022-10-09 17:14:51 - train: epoch 0037, iter [02100, 05004], lr: 0.010000, loss: 1.1839
2022-10-09 17:15:23 - train: epoch 0037, iter [02200, 05004], lr: 0.010000, loss: 1.2527
2022-10-09 17:15:57 - train: epoch 0037, iter [02300, 05004], lr: 0.010000, loss: 1.1207
2022-10-09 17:16:32 - train: epoch 0037, iter [02400, 05004], lr: 0.010000, loss: 1.3063
2022-10-09 17:17:08 - train: epoch 0037, iter [02500, 05004], lr: 0.010000, loss: 1.0388
2022-10-09 17:17:41 - train: epoch 0037, iter [02600, 05004], lr: 0.010000, loss: 1.3127
2022-10-09 17:18:15 - train: epoch 0037, iter [02700, 05004], lr: 0.010000, loss: 1.2382
2022-10-09 17:18:51 - train: epoch 0037, iter [02800, 05004], lr: 0.010000, loss: 1.1978
2022-10-09 17:19:24 - train: epoch 0037, iter [02900, 05004], lr: 0.010000, loss: 1.0947
2022-10-09 17:20:00 - train: epoch 0037, iter [03000, 05004], lr: 0.010000, loss: 1.4122
2022-10-09 17:20:36 - train: epoch 0037, iter [03100, 05004], lr: 0.010000, loss: 1.2195
2022-10-09 17:21:11 - train: epoch 0037, iter [03200, 05004], lr: 0.010000, loss: 1.1899
2022-10-09 17:21:45 - train: epoch 0037, iter [03300, 05004], lr: 0.010000, loss: 1.1555
2022-10-09 17:22:20 - train: epoch 0037, iter [03400, 05004], lr: 0.010000, loss: 1.2550
2022-10-09 17:22:55 - train: epoch 0037, iter [03500, 05004], lr: 0.010000, loss: 1.1662
2022-10-09 17:23:30 - train: epoch 0037, iter [03600, 05004], lr: 0.010000, loss: 1.3178
2022-10-09 17:24:06 - train: epoch 0037, iter [03700, 05004], lr: 0.010000, loss: 1.1397
2022-10-09 17:24:40 - train: epoch 0037, iter [03800, 05004], lr: 0.010000, loss: 1.3391
2022-10-09 17:25:15 - train: epoch 0037, iter [03900, 05004], lr: 0.010000, loss: 1.3717
2022-10-09 17:25:49 - train: epoch 0037, iter [04000, 05004], lr: 0.010000, loss: 1.1475
2022-10-09 17:26:24 - train: epoch 0037, iter [04100, 05004], lr: 0.010000, loss: 1.2611
2022-10-09 17:26:59 - train: epoch 0037, iter [04200, 05004], lr: 0.010000, loss: 1.2472
2022-10-09 17:27:33 - train: epoch 0037, iter [04300, 05004], lr: 0.010000, loss: 1.1111
2022-10-09 17:28:08 - train: epoch 0037, iter [04400, 05004], lr: 0.010000, loss: 1.1675
2022-10-09 17:28:45 - train: epoch 0037, iter [04500, 05004], lr: 0.010000, loss: 1.3044
2022-10-09 17:29:32 - train: epoch 0037, iter [04600, 05004], lr: 0.010000, loss: 1.3257
2022-10-09 17:30:08 - train: epoch 0037, iter [04700, 05004], lr: 0.010000, loss: 1.3514
2022-10-09 17:30:43 - train: epoch 0037, iter [04800, 05004], lr: 0.010000, loss: 1.2544
2022-10-09 17:31:46 - train: epoch 0037, iter [04900, 05004], lr: 0.010000, loss: 1.2368
2022-10-09 17:32:27 - train: epoch 0037, iter [05000, 05004], lr: 0.010000, loss: 1.2104
2022-10-09 17:32:29 - train: epoch 037, train_loss: 1.1779
2022-10-09 17:33:47 - eval: epoch: 037, acc1: 73.328%, acc5: 91.644%, test_loss: 1.0546, per_image_load_time: 2.210ms, per_image_inference_time: 0.559ms
2022-10-09 17:33:48 - until epoch: 037, best_acc1: 73.328%
2022-10-09 17:33:48 - epoch 038 lr: 0.010000
2022-10-09 17:34:27 - train: epoch 0038, iter [00100, 05004], lr: 0.010000, loss: 0.9239
2022-10-09 17:35:03 - train: epoch 0038, iter [00200, 05004], lr: 0.010000, loss: 1.1183
2022-10-09 17:35:37 - train: epoch 0038, iter [00300, 05004], lr: 0.010000, loss: 1.0106
2022-10-09 17:36:10 - train: epoch 0038, iter [00400, 05004], lr: 0.010000, loss: 1.1514
2022-10-09 17:36:45 - train: epoch 0038, iter [00500, 05004], lr: 0.010000, loss: 1.0877
2022-10-09 17:37:19 - train: epoch 0038, iter [00600, 05004], lr: 0.010000, loss: 1.2525
2022-10-09 17:37:53 - train: epoch 0038, iter [00700, 05004], lr: 0.010000, loss: 1.0465
2022-10-09 17:38:27 - train: epoch 0038, iter [00800, 05004], lr: 0.010000, loss: 1.1298
2022-10-09 17:39:02 - train: epoch 0038, iter [00900, 05004], lr: 0.010000, loss: 1.1929
2022-10-09 17:39:37 - train: epoch 0038, iter [01000, 05004], lr: 0.010000, loss: 1.4061
2022-10-09 17:40:12 - train: epoch 0038, iter [01100, 05004], lr: 0.010000, loss: 1.1115
2022-10-09 17:40:47 - train: epoch 0038, iter [01200, 05004], lr: 0.010000, loss: 1.2307
2022-10-09 17:41:21 - train: epoch 0038, iter [01300, 05004], lr: 0.010000, loss: 1.0315
2022-10-09 17:41:54 - train: epoch 0038, iter [01400, 05004], lr: 0.010000, loss: 1.2084
2022-10-09 17:42:30 - train: epoch 0038, iter [01500, 05004], lr: 0.010000, loss: 1.0817
2022-10-09 17:43:04 - train: epoch 0038, iter [01600, 05004], lr: 0.010000, loss: 1.1258
2022-10-09 17:43:38 - train: epoch 0038, iter [01700, 05004], lr: 0.010000, loss: 1.3640
2022-10-09 17:44:14 - train: epoch 0038, iter [01800, 05004], lr: 0.010000, loss: 1.3533
2022-10-09 17:44:48 - train: epoch 0038, iter [01900, 05004], lr: 0.010000, loss: 1.1892
2022-10-09 17:45:22 - train: epoch 0038, iter [02000, 05004], lr: 0.010000, loss: 1.2945
2022-10-09 17:45:57 - train: epoch 0038, iter [02100, 05004], lr: 0.010000, loss: 1.2187
2022-10-09 17:46:31 - train: epoch 0038, iter [02200, 05004], lr: 0.010000, loss: 1.1750
2022-10-09 17:47:06 - train: epoch 0038, iter [02300, 05004], lr: 0.010000, loss: 1.2853
2022-10-09 17:47:40 - train: epoch 0038, iter [02400, 05004], lr: 0.010000, loss: 1.3965
2022-10-09 17:48:14 - train: epoch 0038, iter [02500, 05004], lr: 0.010000, loss: 0.9190
2022-10-09 17:48:50 - train: epoch 0038, iter [02600, 05004], lr: 0.010000, loss: 1.1582
2022-10-09 17:49:24 - train: epoch 0038, iter [02700, 05004], lr: 0.010000, loss: 1.0139
2022-10-09 17:49:59 - train: epoch 0038, iter [02800, 05004], lr: 0.010000, loss: 1.2024
2022-10-09 17:50:34 - train: epoch 0038, iter [02900, 05004], lr: 0.010000, loss: 1.3508
2022-10-09 17:51:08 - train: epoch 0038, iter [03000, 05004], lr: 0.010000, loss: 1.1816
2022-10-09 17:51:46 - train: epoch 0038, iter [03100, 05004], lr: 0.010000, loss: 1.3776
2022-10-09 17:52:22 - train: epoch 0038, iter [03200, 05004], lr: 0.010000, loss: 1.0845
2022-10-09 17:52:58 - train: epoch 0038, iter [03300, 05004], lr: 0.010000, loss: 1.1741
2022-10-09 17:53:32 - train: epoch 0038, iter [03400, 05004], lr: 0.010000, loss: 1.1267
2022-10-09 17:54:10 - train: epoch 0038, iter [03500, 05004], lr: 0.010000, loss: 1.1752
2022-10-09 17:55:02 - train: epoch 0038, iter [03600, 05004], lr: 0.010000, loss: 1.1959
2022-10-09 17:55:39 - train: epoch 0038, iter [03700, 05004], lr: 0.010000, loss: 1.1429
2022-10-09 17:56:26 - train: epoch 0038, iter [03800, 05004], lr: 0.010000, loss: 1.3209
2022-10-09 17:57:03 - train: epoch 0038, iter [03900, 05004], lr: 0.010000, loss: 1.2763
2022-10-09 17:57:50 - train: epoch 0038, iter [04000, 05004], lr: 0.010000, loss: 1.2843
2022-10-09 17:58:28 - train: epoch 0038, iter [04100, 05004], lr: 0.010000, loss: 1.1010
2022-10-09 17:59:11 - train: epoch 0038, iter [04200, 05004], lr: 0.010000, loss: 1.0115
2022-10-09 18:00:01 - train: epoch 0038, iter [04300, 05004], lr: 0.010000, loss: 1.2864
2022-10-09 18:00:38 - train: epoch 0038, iter [04400, 05004], lr: 0.010000, loss: 1.3837
2022-10-09 18:01:18 - train: epoch 0038, iter [04500, 05004], lr: 0.010000, loss: 1.2475
2022-10-09 18:02:11 - train: epoch 0038, iter [04600, 05004], lr: 0.010000, loss: 1.1564
2022-10-09 18:03:08 - train: epoch 0038, iter [04700, 05004], lr: 0.010000, loss: 1.0751
2022-10-09 18:03:52 - train: epoch 0038, iter [04800, 05004], lr: 0.010000, loss: 1.1277
2022-10-09 18:04:24 - train: epoch 0038, iter [04900, 05004], lr: 0.010000, loss: 1.1606
2022-10-09 18:04:57 - train: epoch 0038, iter [05000, 05004], lr: 0.010000, loss: 0.9016
2022-10-09 18:04:59 - train: epoch 038, train_loss: 1.1678
2022-10-09 18:06:18 - eval: epoch: 038, acc1: 73.280%, acc5: 91.536%, test_loss: 1.0616, per_image_load_time: 2.404ms, per_image_inference_time: 0.583ms
2022-10-09 18:06:18 - until epoch: 038, best_acc1: 73.328%
2022-10-09 18:06:18 - epoch 039 lr: 0.010000
2022-10-09 18:06:58 - train: epoch 0039, iter [00100, 05004], lr: 0.010000, loss: 1.2620
2022-10-09 18:07:33 - train: epoch 0039, iter [00200, 05004], lr: 0.010000, loss: 1.2740
2022-10-09 18:08:07 - train: epoch 0039, iter [00300, 05004], lr: 0.010000, loss: 0.8921
2022-10-09 18:08:42 - train: epoch 0039, iter [00400, 05004], lr: 0.010000, loss: 1.1848
2022-10-09 18:09:16 - train: epoch 0039, iter [00500, 05004], lr: 0.010000, loss: 1.1608
2022-10-09 18:09:50 - train: epoch 0039, iter [00600, 05004], lr: 0.010000, loss: 1.1669
2022-10-09 18:10:24 - train: epoch 0039, iter [00700, 05004], lr: 0.010000, loss: 1.3642
2022-10-09 18:10:59 - train: epoch 0039, iter [00800, 05004], lr: 0.010000, loss: 1.1272
2022-10-09 18:11:33 - train: epoch 0039, iter [00900, 05004], lr: 0.010000, loss: 1.1957
2022-10-09 18:12:07 - train: epoch 0039, iter [01000, 05004], lr: 0.010000, loss: 1.2348
2022-10-09 18:12:41 - train: epoch 0039, iter [01100, 05004], lr: 0.010000, loss: 1.0934
2022-10-09 18:13:16 - train: epoch 0039, iter [01200, 05004], lr: 0.010000, loss: 1.2128
2022-10-09 18:13:51 - train: epoch 0039, iter [01300, 05004], lr: 0.010000, loss: 1.2914
2022-10-09 18:14:25 - train: epoch 0039, iter [01400, 05004], lr: 0.010000, loss: 1.2662
2022-10-09 18:14:59 - train: epoch 0039, iter [01500, 05004], lr: 0.010000, loss: 1.1024
2022-10-09 18:15:35 - train: epoch 0039, iter [01600, 05004], lr: 0.010000, loss: 1.0390
2022-10-09 18:16:08 - train: epoch 0039, iter [01700, 05004], lr: 0.010000, loss: 1.0736
2022-10-09 18:16:48 - train: epoch 0039, iter [01800, 05004], lr: 0.010000, loss: 1.3318
2022-10-09 18:17:27 - train: epoch 0039, iter [01900, 05004], lr: 0.010000, loss: 0.8861
2022-10-09 18:18:01 - train: epoch 0039, iter [02000, 05004], lr: 0.010000, loss: 1.0802
2022-10-09 18:18:35 - train: epoch 0039, iter [02100, 05004], lr: 0.010000, loss: 1.3617
2022-10-09 18:19:10 - train: epoch 0039, iter [02200, 05004], lr: 0.010000, loss: 1.0117
2022-10-09 18:19:44 - train: epoch 0039, iter [02300, 05004], lr: 0.010000, loss: 1.4661
2022-10-09 18:20:19 - train: epoch 0039, iter [02400, 05004], lr: 0.010000, loss: 1.1551
2022-10-09 18:20:54 - train: epoch 0039, iter [02500, 05004], lr: 0.010000, loss: 1.1458
2022-10-09 18:21:27 - train: epoch 0039, iter [02600, 05004], lr: 0.010000, loss: 1.0387
2022-10-09 18:22:01 - train: epoch 0039, iter [02700, 05004], lr: 0.010000, loss: 1.1132
2022-10-09 18:22:36 - train: epoch 0039, iter [02800, 05004], lr: 0.010000, loss: 1.2922
2022-10-09 18:23:10 - train: epoch 0039, iter [02900, 05004], lr: 0.010000, loss: 0.9884
2022-10-09 18:23:45 - train: epoch 0039, iter [03000, 05004], lr: 0.010000, loss: 1.1764
2022-10-09 18:24:19 - train: epoch 0039, iter [03100, 05004], lr: 0.010000, loss: 0.9972
2022-10-09 18:24:54 - train: epoch 0039, iter [03200, 05004], lr: 0.010000, loss: 1.2412
2022-10-09 18:25:28 - train: epoch 0039, iter [03300, 05004], lr: 0.010000, loss: 1.2393
2022-10-09 18:26:02 - train: epoch 0039, iter [03400, 05004], lr: 0.010000, loss: 1.2406
2022-10-09 18:26:36 - train: epoch 0039, iter [03500, 05004], lr: 0.010000, loss: 1.2718
2022-10-09 18:27:12 - train: epoch 0039, iter [03600, 05004], lr: 0.010000, loss: 1.2001
2022-10-09 18:27:46 - train: epoch 0039, iter [03700, 05004], lr: 0.010000, loss: 1.1244
2022-10-09 18:28:20 - train: epoch 0039, iter [03800, 05004], lr: 0.010000, loss: 1.0795
2022-10-09 18:28:55 - train: epoch 0039, iter [03900, 05004], lr: 0.010000, loss: 1.0270
2022-10-09 18:29:29 - train: epoch 0039, iter [04000, 05004], lr: 0.010000, loss: 1.1663
2022-10-09 18:30:12 - train: epoch 0039, iter [04100, 05004], lr: 0.010000, loss: 1.1028
2022-10-09 18:31:02 - train: epoch 0039, iter [04200, 05004], lr: 0.010000, loss: 1.1529
2022-10-09 18:31:55 - train: epoch 0039, iter [04300, 05004], lr: 0.010000, loss: 1.1986
2022-10-09 18:32:32 - train: epoch 0039, iter [04400, 05004], lr: 0.010000, loss: 1.2152
2022-10-09 18:33:15 - train: epoch 0039, iter [04500, 05004], lr: 0.010000, loss: 1.0662
2022-10-09 18:33:52 - train: epoch 0039, iter [04600, 05004], lr: 0.010000, loss: 1.1038
2022-10-09 18:35:00 - train: epoch 0039, iter [04700, 05004], lr: 0.010000, loss: 1.2170
2022-10-09 18:35:48 - train: epoch 0039, iter [04800, 05004], lr: 0.010000, loss: 1.0619
2022-10-09 18:36:45 - train: epoch 0039, iter [04900, 05004], lr: 0.010000, loss: 1.2145
2022-10-09 18:37:43 - train: epoch 0039, iter [05000, 05004], lr: 0.010000, loss: 1.1868
2022-10-09 18:37:45 - train: epoch 039, train_loss: 1.1629
2022-10-09 18:39:03 - eval: epoch: 039, acc1: 73.268%, acc5: 91.654%, test_loss: 1.0545, per_image_load_time: 1.198ms, per_image_inference_time: 0.565ms
2022-10-09 18:39:03 - until epoch: 039, best_acc1: 73.328%
2022-10-09 18:39:03 - epoch 040 lr: 0.010000
2022-10-09 18:39:43 - train: epoch 0040, iter [00100, 05004], lr: 0.010000, loss: 1.3131
2022-10-09 18:40:17 - train: epoch 0040, iter [00200, 05004], lr: 0.010000, loss: 1.2462
2022-10-09 18:40:52 - train: epoch 0040, iter [00300, 05004], lr: 0.010000, loss: 1.2272
2022-10-09 18:41:26 - train: epoch 0040, iter [00400, 05004], lr: 0.010000, loss: 1.0296
2022-10-09 18:42:00 - train: epoch 0040, iter [00500, 05004], lr: 0.010000, loss: 1.1892
2022-10-09 18:42:34 - train: epoch 0040, iter [00600, 05004], lr: 0.010000, loss: 1.1906
2022-10-09 18:43:09 - train: epoch 0040, iter [00700, 05004], lr: 0.010000, loss: 1.0282
2022-10-09 18:43:43 - train: epoch 0040, iter [00800, 05004], lr: 0.010000, loss: 1.1936
2022-10-09 18:44:17 - train: epoch 0040, iter [00900, 05004], lr: 0.010000, loss: 1.1669
2022-10-09 18:44:51 - train: epoch 0040, iter [01000, 05004], lr: 0.010000, loss: 0.8636
2022-10-09 18:45:25 - train: epoch 0040, iter [01100, 05004], lr: 0.010000, loss: 0.9124
2022-10-09 18:46:00 - train: epoch 0040, iter [01200, 05004], lr: 0.010000, loss: 1.0290
2022-10-09 18:46:34 - train: epoch 0040, iter [01300, 05004], lr: 0.010000, loss: 0.9955
2022-10-09 18:47:07 - train: epoch 0040, iter [01400, 05004], lr: 0.010000, loss: 1.3349
2022-10-09 18:47:42 - train: epoch 0040, iter [01500, 05004], lr: 0.010000, loss: 1.3111
2022-10-09 18:48:15 - train: epoch 0040, iter [01600, 05004], lr: 0.010000, loss: 1.2882
2022-10-09 18:48:50 - train: epoch 0040, iter [01700, 05004], lr: 0.010000, loss: 0.9977
2022-10-09 18:49:25 - train: epoch 0040, iter [01800, 05004], lr: 0.010000, loss: 1.0931
2022-10-09 18:50:00 - train: epoch 0040, iter [01900, 05004], lr: 0.010000, loss: 1.0826
2022-10-09 18:50:34 - train: epoch 0040, iter [02000, 05004], lr: 0.010000, loss: 1.2874
2022-10-09 18:51:08 - train: epoch 0040, iter [02100, 05004], lr: 0.010000, loss: 0.9542
2022-10-09 18:51:42 - train: epoch 0040, iter [02200, 05004], lr: 0.010000, loss: 0.9891
2022-10-09 18:52:17 - train: epoch 0040, iter [02300, 05004], lr: 0.010000, loss: 0.9740
2022-10-09 18:52:51 - train: epoch 0040, iter [02400, 05004], lr: 0.010000, loss: 1.2693
2022-10-09 18:53:36 - train: epoch 0040, iter [02500, 05004], lr: 0.010000, loss: 1.2143
2022-10-09 18:54:24 - train: epoch 0040, iter [02600, 05004], lr: 0.010000, loss: 1.2319
2022-10-09 18:55:04 - train: epoch 0040, iter [02700, 05004], lr: 0.010000, loss: 1.3245
2022-10-09 18:55:39 - train: epoch 0040, iter [02800, 05004], lr: 0.010000, loss: 1.2491
2022-10-09 18:56:13 - train: epoch 0040, iter [02900, 05004], lr: 0.010000, loss: 1.3554
2022-10-09 18:57:47 - train: epoch 0040, iter [03000, 05004], lr: 0.010000, loss: 1.2266
2022-10-09 19:00:13 - train: epoch 0040, iter [03100, 05004], lr: 0.010000, loss: 1.2508
2022-10-09 19:02:31 - train: epoch 0040, iter [03200, 05004], lr: 0.010000, loss: 1.2189
2022-10-09 19:04:23 - train: epoch 0040, iter [03300, 05004], lr: 0.010000, loss: 1.1478
2022-10-09 19:07:03 - train: epoch 0040, iter [03400, 05004], lr: 0.010000, loss: 1.1391
2022-10-09 19:08:05 - train: epoch 0040, iter [03500, 05004], lr: 0.010000, loss: 1.1868
2022-10-09 19:08:53 - train: epoch 0040, iter [03600, 05004], lr: 0.010000, loss: 1.1660
2022-10-09 19:09:31 - train: epoch 0040, iter [03700, 05004], lr: 0.010000, loss: 1.0735
2022-10-09 19:10:09 - train: epoch 0040, iter [03800, 05004], lr: 0.010000, loss: 1.0176
2022-10-09 19:10:45 - train: epoch 0040, iter [03900, 05004], lr: 0.010000, loss: 1.2176
2022-10-09 19:11:21 - train: epoch 0040, iter [04000, 05004], lr: 0.010000, loss: 1.0710
2022-10-09 19:11:56 - train: epoch 0040, iter [04100, 05004], lr: 0.010000, loss: 1.1667
2022-10-09 19:12:30 - train: epoch 0040, iter [04200, 05004], lr: 0.010000, loss: 1.0774
2022-10-09 19:13:07 - train: epoch 0040, iter [04300, 05004], lr: 0.010000, loss: 1.1600
2022-10-09 19:13:42 - train: epoch 0040, iter [04400, 05004], lr: 0.010000, loss: 0.9391
2022-10-09 19:14:19 - train: epoch 0040, iter [04500, 05004], lr: 0.010000, loss: 0.8485
2022-10-09 19:14:51 - train: epoch 0040, iter [04600, 05004], lr: 0.010000, loss: 1.2945
2022-10-09 19:15:26 - train: epoch 0040, iter [04700, 05004], lr: 0.010000, loss: 1.1939
2022-10-09 19:16:01 - train: epoch 0040, iter [04800, 05004], lr: 0.010000, loss: 0.9931
2022-10-09 19:16:36 - train: epoch 0040, iter [04900, 05004], lr: 0.010000, loss: 1.1746
2022-10-09 19:17:08 - train: epoch 0040, iter [05000, 05004], lr: 0.010000, loss: 1.1487
2022-10-09 19:17:10 - train: epoch 040, train_loss: 1.1548
2022-10-09 19:18:29 - eval: epoch: 040, acc1: 73.008%, acc5: 91.510%, test_loss: 1.0693, per_image_load_time: 2.362ms, per_image_inference_time: 0.564ms
2022-10-09 19:18:29 - until epoch: 040, best_acc1: 73.328%
2022-10-09 19:18:29 - epoch 041 lr: 0.010000
2022-10-09 19:19:10 - train: epoch 0041, iter [00100, 05004], lr: 0.010000, loss: 1.2176
2022-10-09 19:19:44 - train: epoch 0041, iter [00200, 05004], lr: 0.010000, loss: 1.3557
2022-10-09 19:20:19 - train: epoch 0041, iter [00300, 05004], lr: 0.010000, loss: 1.1507
2022-10-09 19:20:53 - train: epoch 0041, iter [00400, 05004], lr: 0.010000, loss: 1.2405
2022-10-09 19:21:28 - train: epoch 0041, iter [00500, 05004], lr: 0.010000, loss: 1.0420
2022-10-09 19:22:01 - train: epoch 0041, iter [00600, 05004], lr: 0.010000, loss: 0.9977
2022-10-09 19:22:37 - train: epoch 0041, iter [00700, 05004], lr: 0.010000, loss: 1.1832
2022-10-09 19:23:11 - train: epoch 0041, iter [00800, 05004], lr: 0.010000, loss: 1.2786
2022-10-09 19:23:45 - train: epoch 0041, iter [00900, 05004], lr: 0.010000, loss: 1.1040
2022-10-09 19:24:19 - train: epoch 0041, iter [01000, 05004], lr: 0.010000, loss: 1.2765
2022-10-09 19:24:53 - train: epoch 0041, iter [01100, 05004], lr: 0.010000, loss: 1.0005
2022-10-09 19:25:27 - train: epoch 0041, iter [01200, 05004], lr: 0.010000, loss: 0.9926
2022-10-09 19:26:02 - train: epoch 0041, iter [01300, 05004], lr: 0.010000, loss: 0.9306
2022-10-09 19:26:38 - train: epoch 0041, iter [01400, 05004], lr: 0.010000, loss: 1.2424
2022-10-09 19:27:11 - train: epoch 0041, iter [01500, 05004], lr: 0.010000, loss: 1.3268
2022-10-09 19:27:47 - train: epoch 0041, iter [01600, 05004], lr: 0.010000, loss: 1.2439
2022-10-09 19:28:22 - train: epoch 0041, iter [01700, 05004], lr: 0.010000, loss: 1.0256
2022-10-09 19:28:56 - train: epoch 0041, iter [01800, 05004], lr: 0.010000, loss: 1.0318
2022-10-09 19:29:32 - train: epoch 0041, iter [01900, 05004], lr: 0.010000, loss: 1.2625
2022-10-09 19:30:06 - train: epoch 0041, iter [02000, 05004], lr: 0.010000, loss: 1.0029
2022-10-09 19:30:41 - train: epoch 0041, iter [02100, 05004], lr: 0.010000, loss: 1.1180
2022-10-09 19:31:15 - train: epoch 0041, iter [02200, 05004], lr: 0.010000, loss: 1.0785
2022-10-09 19:31:50 - train: epoch 0041, iter [02300, 05004], lr: 0.010000, loss: 1.2118
2022-10-09 19:32:25 - train: epoch 0041, iter [02400, 05004], lr: 0.010000, loss: 1.1853
2022-10-09 19:32:59 - train: epoch 0041, iter [02500, 05004], lr: 0.010000, loss: 1.0681
2022-10-09 19:33:34 - train: epoch 0041, iter [02600, 05004], lr: 0.010000, loss: 1.1981
2022-10-09 19:34:09 - train: epoch 0041, iter [02700, 05004], lr: 0.010000, loss: 1.3734
2022-10-09 19:34:43 - train: epoch 0041, iter [02800, 05004], lr: 0.010000, loss: 1.1252
2022-10-09 19:35:18 - train: epoch 0041, iter [02900, 05004], lr: 0.010000, loss: 1.2714
2022-10-09 19:35:54 - train: epoch 0041, iter [03000, 05004], lr: 0.010000, loss: 1.1913
2022-10-09 19:36:28 - train: epoch 0041, iter [03100, 05004], lr: 0.010000, loss: 1.1151
2022-10-09 19:37:04 - train: epoch 0041, iter [03200, 05004], lr: 0.010000, loss: 0.9272
2022-10-09 19:37:40 - train: epoch 0041, iter [03300, 05004], lr: 0.010000, loss: 1.1873
2022-10-09 19:38:17 - train: epoch 0041, iter [03400, 05004], lr: 0.010000, loss: 0.9883
2022-10-09 19:38:53 - train: epoch 0041, iter [03500, 05004], lr: 0.010000, loss: 1.0900
2022-10-09 19:39:26 - train: epoch 0041, iter [03600, 05004], lr: 0.010000, loss: 1.3814
2022-10-09 19:40:02 - train: epoch 0041, iter [03700, 05004], lr: 0.010000, loss: 1.1893
2022-10-09 19:40:37 - train: epoch 0041, iter [03800, 05004], lr: 0.010000, loss: 1.1018
2022-10-09 19:41:14 - train: epoch 0041, iter [03900, 05004], lr: 0.010000, loss: 1.2221
2022-10-09 19:41:49 - train: epoch 0041, iter [04000, 05004], lr: 0.010000, loss: 0.9707
2022-10-09 19:42:23 - train: epoch 0041, iter [04100, 05004], lr: 0.010000, loss: 1.0286
2022-10-09 19:42:57 - train: epoch 0041, iter [04200, 05004], lr: 0.010000, loss: 1.0180
2022-10-09 19:43:33 - train: epoch 0041, iter [04300, 05004], lr: 0.010000, loss: 1.0359
2022-10-09 19:44:06 - train: epoch 0041, iter [04400, 05004], lr: 0.010000, loss: 1.1742
2022-10-09 19:44:41 - train: epoch 0041, iter [04500, 05004], lr: 0.010000, loss: 1.1787
2022-10-09 19:45:16 - train: epoch 0041, iter [04600, 05004], lr: 0.010000, loss: 1.2348
2022-10-09 19:45:50 - train: epoch 0041, iter [04700, 05004], lr: 0.010000, loss: 1.1287
2022-10-09 19:46:25 - train: epoch 0041, iter [04800, 05004], lr: 0.010000, loss: 1.2274
2022-10-09 19:47:00 - train: epoch 0041, iter [04900, 05004], lr: 0.010000, loss: 1.2038
2022-10-09 19:47:31 - train: epoch 0041, iter [05000, 05004], lr: 0.010000, loss: 1.0165
2022-10-09 19:47:33 - train: epoch 041, train_loss: 1.1549
2022-10-09 19:48:50 - eval: epoch: 041, acc1: 72.428%, acc5: 91.052%, test_loss: 1.0955, per_image_load_time: 0.858ms, per_image_inference_time: 0.574ms
2022-10-09 19:48:50 - until epoch: 041, best_acc1: 73.328%
2022-10-09 19:48:50 - epoch 042 lr: 0.010000
2022-10-09 19:49:30 - train: epoch 0042, iter [00100, 05004], lr: 0.010000, loss: 0.8863
2022-10-09 19:50:05 - train: epoch 0042, iter [00200, 05004], lr: 0.010000, loss: 1.0421
2022-10-09 19:50:39 - train: epoch 0042, iter [00300, 05004], lr: 0.010000, loss: 1.1620
2022-10-09 19:51:14 - train: epoch 0042, iter [00400, 05004], lr: 0.010000, loss: 1.0436
2022-10-09 19:51:47 - train: epoch 0042, iter [00500, 05004], lr: 0.010000, loss: 1.0437
2022-10-09 19:52:22 - train: epoch 0042, iter [00600, 05004], lr: 0.010000, loss: 1.0949
2022-10-09 19:52:57 - train: epoch 0042, iter [00700, 05004], lr: 0.010000, loss: 1.0749
2022-10-09 19:53:31 - train: epoch 0042, iter [00800, 05004], lr: 0.010000, loss: 1.2683
2022-10-09 19:54:05 - train: epoch 0042, iter [00900, 05004], lr: 0.010000, loss: 1.1366
2022-10-09 19:54:38 - train: epoch 0042, iter [01000, 05004], lr: 0.010000, loss: 1.3962
2022-10-09 19:55:13 - train: epoch 0042, iter [01100, 05004], lr: 0.010000, loss: 1.0103
2022-10-09 19:55:47 - train: epoch 0042, iter [01200, 05004], lr: 0.010000, loss: 1.0439
2022-10-09 19:56:21 - train: epoch 0042, iter [01300, 05004], lr: 0.010000, loss: 1.1252
2022-10-09 19:56:56 - train: epoch 0042, iter [01400, 05004], lr: 0.010000, loss: 1.1870
2022-10-09 19:57:30 - train: epoch 0042, iter [01500, 05004], lr: 0.010000, loss: 1.1219
2022-10-09 19:58:06 - train: epoch 0042, iter [01600, 05004], lr: 0.010000, loss: 1.3133
2022-10-09 19:58:40 - train: epoch 0042, iter [01700, 05004], lr: 0.010000, loss: 1.1599
2022-10-09 19:59:14 - train: epoch 0042, iter [01800, 05004], lr: 0.010000, loss: 1.0748
2022-10-09 19:59:48 - train: epoch 0042, iter [01900, 05004], lr: 0.010000, loss: 1.1076
2022-10-09 20:00:22 - train: epoch 0042, iter [02000, 05004], lr: 0.010000, loss: 1.1166
2022-10-09 20:00:58 - train: epoch 0042, iter [02100, 05004], lr: 0.010000, loss: 0.9572
2022-10-09 20:01:31 - train: epoch 0042, iter [02200, 05004], lr: 0.010000, loss: 1.2370
2022-10-09 20:02:05 - train: epoch 0042, iter [02300, 05004], lr: 0.010000, loss: 1.1293
2022-10-09 20:02:39 - train: epoch 0042, iter [02400, 05004], lr: 0.010000, loss: 1.1864
2022-10-09 20:03:14 - train: epoch 0042, iter [02500, 05004], lr: 0.010000, loss: 1.1957
2022-10-09 20:03:48 - train: epoch 0042, iter [02600, 05004], lr: 0.010000, loss: 1.2970
2022-10-09 20:04:22 - train: epoch 0042, iter [02700, 05004], lr: 0.010000, loss: 0.9949
2022-10-09 20:04:56 - train: epoch 0042, iter [02800, 05004], lr: 0.010000, loss: 1.1365
2022-10-09 20:05:31 - train: epoch 0042, iter [02900, 05004], lr: 0.010000, loss: 1.1857
2022-10-09 20:06:06 - train: epoch 0042, iter [03000, 05004], lr: 0.010000, loss: 1.0377
2022-10-09 20:06:39 - train: epoch 0042, iter [03100, 05004], lr: 0.010000, loss: 1.0817
2022-10-09 20:07:15 - train: epoch 0042, iter [03200, 05004], lr: 0.010000, loss: 1.3681
2022-10-09 20:07:47 - train: epoch 0042, iter [03300, 05004], lr: 0.010000, loss: 1.2032
2022-10-09 20:08:23 - train: epoch 0042, iter [03400, 05004], lr: 0.010000, loss: 1.0553
2022-10-09 20:08:58 - train: epoch 0042, iter [03500, 05004], lr: 0.010000, loss: 1.3180
2022-10-09 20:09:32 - train: epoch 0042, iter [03600, 05004], lr: 0.010000, loss: 1.3071
2022-10-09 20:10:06 - train: epoch 0042, iter [03700, 05004], lr: 0.010000, loss: 1.1602
2022-10-09 20:10:41 - train: epoch 0042, iter [03800, 05004], lr: 0.010000, loss: 0.9214
2022-10-09 20:11:15 - train: epoch 0042, iter [03900, 05004], lr: 0.010000, loss: 1.2333
2022-10-09 20:11:50 - train: epoch 0042, iter [04000, 05004], lr: 0.010000, loss: 1.0663
2022-10-09 20:12:26 - train: epoch 0042, iter [04100, 05004], lr: 0.010000, loss: 1.0040
2022-10-09 20:12:59 - train: epoch 0042, iter [04200, 05004], lr: 0.010000, loss: 1.0859
2022-10-09 20:13:35 - train: epoch 0042, iter [04300, 05004], lr: 0.010000, loss: 0.8536
2022-10-09 20:14:10 - train: epoch 0042, iter [04400, 05004], lr: 0.010000, loss: 1.1133
2022-10-09 20:14:44 - train: epoch 0042, iter [04500, 05004], lr: 0.010000, loss: 1.1608
2022-10-09 20:15:19 - train: epoch 0042, iter [04600, 05004], lr: 0.010000, loss: 1.1657
2022-10-09 20:15:53 - train: epoch 0042, iter [04700, 05004], lr: 0.010000, loss: 1.1927
2022-10-09 20:16:28 - train: epoch 0042, iter [04800, 05004], lr: 0.010000, loss: 1.2914
2022-10-09 20:17:01 - train: epoch 0042, iter [04900, 05004], lr: 0.010000, loss: 1.2283
2022-10-09 20:17:34 - train: epoch 0042, iter [05000, 05004], lr: 0.010000, loss: 1.1488
2022-10-09 20:17:36 - train: epoch 042, train_loss: 1.1506
2022-10-09 20:18:53 - eval: epoch: 042, acc1: 72.638%, acc5: 91.212%, test_loss: 1.0899, per_image_load_time: 1.307ms, per_image_inference_time: 0.578ms
2022-10-09 20:18:53 - until epoch: 042, best_acc1: 73.328%
2022-10-09 20:18:53 - epoch 043 lr: 0.010000
2022-10-09 20:19:33 - train: epoch 0043, iter [00100, 05004], lr: 0.010000, loss: 1.2092
2022-10-09 20:20:08 - train: epoch 0043, iter [00200, 05004], lr: 0.010000, loss: 1.1142
2022-10-09 20:20:41 - train: epoch 0043, iter [00300, 05004], lr: 0.010000, loss: 0.9195
2022-10-09 20:21:15 - train: epoch 0043, iter [00400, 05004], lr: 0.010000, loss: 0.9417
2022-10-09 20:21:49 - train: epoch 0043, iter [00500, 05004], lr: 0.010000, loss: 1.0962
2022-10-09 20:22:23 - train: epoch 0043, iter [00600, 05004], lr: 0.010000, loss: 1.0846
2022-10-09 20:22:56 - train: epoch 0043, iter [00700, 05004], lr: 0.010000, loss: 1.2017
2022-10-09 20:23:31 - train: epoch 0043, iter [00800, 05004], lr: 0.010000, loss: 1.2363
2022-10-09 20:24:06 - train: epoch 0043, iter [00900, 05004], lr: 0.010000, loss: 1.1397
2022-10-09 20:24:40 - train: epoch 0043, iter [01000, 05004], lr: 0.010000, loss: 1.2326
2022-10-09 20:25:15 - train: epoch 0043, iter [01100, 05004], lr: 0.010000, loss: 1.3479
2022-10-09 20:25:48 - train: epoch 0043, iter [01200, 05004], lr: 0.010000, loss: 1.0464
2022-10-09 20:26:22 - train: epoch 0043, iter [01300, 05004], lr: 0.010000, loss: 1.3710
2022-10-09 20:26:56 - train: epoch 0043, iter [01400, 05004], lr: 0.010000, loss: 0.9775
2022-10-09 20:27:32 - train: epoch 0043, iter [01500, 05004], lr: 0.010000, loss: 1.0399
2022-10-09 20:28:06 - train: epoch 0043, iter [01600, 05004], lr: 0.010000, loss: 1.0121
2022-10-09 20:28:42 - train: epoch 0043, iter [01700, 05004], lr: 0.010000, loss: 1.3464
2022-10-09 20:29:17 - train: epoch 0043, iter [01800, 05004], lr: 0.010000, loss: 1.3680
2022-10-09 20:29:51 - train: epoch 0043, iter [01900, 05004], lr: 0.010000, loss: 1.2458
2022-10-09 20:30:27 - train: epoch 0043, iter [02000, 05004], lr: 0.010000, loss: 0.8749
2022-10-09 20:31:01 - train: epoch 0043, iter [02100, 05004], lr: 0.010000, loss: 1.1324
2022-10-09 20:31:36 - train: epoch 0043, iter [02200, 05004], lr: 0.010000, loss: 1.1606
2022-10-09 20:32:08 - train: epoch 0043, iter [02300, 05004], lr: 0.010000, loss: 1.2959
2022-10-09 20:32:43 - train: epoch 0043, iter [02400, 05004], lr: 0.010000, loss: 0.9557
2022-10-09 20:33:17 - train: epoch 0043, iter [02500, 05004], lr: 0.010000, loss: 1.1099
2022-10-09 20:33:51 - train: epoch 0043, iter [02600, 05004], lr: 0.010000, loss: 0.9426
2022-10-09 20:34:25 - train: epoch 0043, iter [02700, 05004], lr: 0.010000, loss: 1.1491
2022-10-09 20:35:12 - train: epoch 0043, iter [02800, 05004], lr: 0.010000, loss: 1.0327
2022-10-09 20:36:22 - train: epoch 0043, iter [02900, 05004], lr: 0.010000, loss: 1.1539
2022-10-09 20:37:14 - train: epoch 0043, iter [03000, 05004], lr: 0.010000, loss: 1.3523
2022-10-09 20:37:56 - train: epoch 0043, iter [03100, 05004], lr: 0.010000, loss: 1.2119
2022-10-09 20:39:23 - train: epoch 0043, iter [03200, 05004], lr: 0.010000, loss: 1.2547
2022-10-09 20:40:29 - train: epoch 0043, iter [03300, 05004], lr: 0.010000, loss: 1.2564
2022-10-09 20:41:34 - train: epoch 0043, iter [03400, 05004], lr: 0.010000, loss: 1.1140
2022-10-09 20:42:41 - train: epoch 0043, iter [03500, 05004], lr: 0.010000, loss: 1.1677
2022-10-09 20:43:13 - train: epoch 0043, iter [03600, 05004], lr: 0.010000, loss: 1.1937
2022-10-09 20:44:15 - train: epoch 0043, iter [03700, 05004], lr: 0.010000, loss: 1.1370
2022-10-09 20:45:43 - train: epoch 0043, iter [03800, 05004], lr: 0.010000, loss: 1.2916
2022-10-09 20:47:14 - train: epoch 0043, iter [03900, 05004], lr: 0.010000, loss: 1.1005
2022-10-09 20:49:11 - train: epoch 0043, iter [04000, 05004], lr: 0.010000, loss: 1.2589
2022-10-09 20:50:51 - train: epoch 0043, iter [04100, 05004], lr: 0.010000, loss: 1.3301
2022-10-09 20:52:40 - train: epoch 0043, iter [04200, 05004], lr: 0.010000, loss: 1.3741
2022-10-09 20:54:21 - train: epoch 0043, iter [04300, 05004], lr: 0.010000, loss: 1.1074
2022-10-09 20:55:32 - train: epoch 0043, iter [04400, 05004], lr: 0.010000, loss: 1.1857
2022-10-09 20:57:01 - train: epoch 0043, iter [04500, 05004], lr: 0.010000, loss: 1.2563
2022-10-09 20:57:45 - train: epoch 0043, iter [04600, 05004], lr: 0.010000, loss: 1.0938
2022-10-09 20:58:21 - train: epoch 0043, iter [04700, 05004], lr: 0.010000, loss: 1.2356
2022-10-09 20:58:56 - train: epoch 0043, iter [04800, 05004], lr: 0.010000, loss: 1.1952
2022-10-09 20:59:29 - train: epoch 0043, iter [04900, 05004], lr: 0.010000, loss: 1.0753
2022-10-09 21:00:07 - train: epoch 0043, iter [05000, 05004], lr: 0.010000, loss: 1.1525
2022-10-09 21:00:09 - train: epoch 043, train_loss: 1.1456
2022-10-09 21:01:28 - eval: epoch: 043, acc1: 72.294%, acc5: 91.122%, test_loss: 1.1005, per_image_load_time: 2.389ms, per_image_inference_time: 0.580ms
2022-10-09 21:01:28 - until epoch: 043, best_acc1: 73.328%
2022-10-09 21:01:28 - epoch 044 lr: 0.010000
2022-10-09 21:02:09 - train: epoch 0044, iter [00100, 05004], lr: 0.010000, loss: 1.1768
2022-10-09 21:02:43 - train: epoch 0044, iter [00200, 05004], lr: 0.010000, loss: 1.0012
2022-10-09 21:03:19 - train: epoch 0044, iter [00300, 05004], lr: 0.010000, loss: 1.2684
2022-10-09 21:03:53 - train: epoch 0044, iter [00400, 05004], lr: 0.010000, loss: 0.8525
2022-10-09 21:04:28 - train: epoch 0044, iter [00500, 05004], lr: 0.010000, loss: 1.0916
2022-10-09 21:05:02 - train: epoch 0044, iter [00600, 05004], lr: 0.010000, loss: 1.0350
2022-10-09 21:05:36 - train: epoch 0044, iter [00700, 05004], lr: 0.010000, loss: 1.0334
2022-10-09 21:06:11 - train: epoch 0044, iter [00800, 05004], lr: 0.010000, loss: 1.1664
2022-10-09 21:06:45 - train: epoch 0044, iter [00900, 05004], lr: 0.010000, loss: 1.1415
2022-10-09 21:07:19 - train: epoch 0044, iter [01000, 05004], lr: 0.010000, loss: 1.1812
2022-10-09 21:07:54 - train: epoch 0044, iter [01100, 05004], lr: 0.010000, loss: 0.8959
2022-10-09 21:08:35 - train: epoch 0044, iter [01200, 05004], lr: 0.010000, loss: 1.2287
2022-10-09 21:09:10 - train: epoch 0044, iter [01300, 05004], lr: 0.010000, loss: 1.3394
2022-10-09 21:09:43 - train: epoch 0044, iter [01400, 05004], lr: 0.010000, loss: 1.0848
2022-10-09 21:10:18 - train: epoch 0044, iter [01500, 05004], lr: 0.010000, loss: 1.0144
2022-10-09 21:10:52 - train: epoch 0044, iter [01600, 05004], lr: 0.010000, loss: 1.1799
2022-10-09 21:11:26 - train: epoch 0044, iter [01700, 05004], lr: 0.010000, loss: 1.2303
2022-10-09 21:12:01 - train: epoch 0044, iter [01800, 05004], lr: 0.010000, loss: 1.1556
2022-10-09 21:12:36 - train: epoch 0044, iter [01900, 05004], lr: 0.010000, loss: 1.3477
2022-10-09 21:13:11 - train: epoch 0044, iter [02000, 05004], lr: 0.010000, loss: 0.9685
2022-10-09 21:13:45 - train: epoch 0044, iter [02100, 05004], lr: 0.010000, loss: 1.1706
2022-10-09 21:14:19 - train: epoch 0044, iter [02200, 05004], lr: 0.010000, loss: 1.0949
2022-10-09 21:14:56 - train: epoch 0044, iter [02300, 05004], lr: 0.010000, loss: 1.3363
2022-10-09 21:15:30 - train: epoch 0044, iter [02400, 05004], lr: 0.010000, loss: 1.1107
2022-10-09 21:16:08 - train: epoch 0044, iter [02500, 05004], lr: 0.010000, loss: 1.0956
2022-10-09 21:16:41 - train: epoch 0044, iter [02600, 05004], lr: 0.010000, loss: 1.1240
2022-10-09 21:17:18 - train: epoch 0044, iter [02700, 05004], lr: 0.010000, loss: 1.1873
2022-10-09 21:18:01 - train: epoch 0044, iter [02800, 05004], lr: 0.010000, loss: 0.9527
2022-10-09 21:18:43 - train: epoch 0044, iter [02900, 05004], lr: 0.010000, loss: 0.9261
2022-10-09 21:19:29 - train: epoch 0044, iter [03000, 05004], lr: 0.010000, loss: 1.1890
2022-10-09 21:20:04 - train: epoch 0044, iter [03100, 05004], lr: 0.010000, loss: 1.1711
2022-10-09 21:20:40 - train: epoch 0044, iter [03200, 05004], lr: 0.010000, loss: 1.0738
2022-10-09 21:21:20 - train: epoch 0044, iter [03300, 05004], lr: 0.010000, loss: 1.0983
2022-10-09 21:22:00 - train: epoch 0044, iter [03400, 05004], lr: 0.010000, loss: 1.2720
2022-10-09 21:22:39 - train: epoch 0044, iter [03500, 05004], lr: 0.010000, loss: 1.0713
2022-10-09 21:23:18 - train: epoch 0044, iter [03600, 05004], lr: 0.010000, loss: 1.3877
2022-10-09 21:23:58 - train: epoch 0044, iter [03700, 05004], lr: 0.010000, loss: 1.1756
2022-10-09 21:24:46 - train: epoch 0044, iter [03800, 05004], lr: 0.010000, loss: 1.0530
2022-10-09 21:25:30 - train: epoch 0044, iter [03900, 05004], lr: 0.010000, loss: 1.3915
2022-10-09 21:26:22 - train: epoch 0044, iter [04000, 05004], lr: 0.010000, loss: 1.1472
2022-10-09 21:27:15 - train: epoch 0044, iter [04100, 05004], lr: 0.010000, loss: 1.0610
2022-10-09 21:28:05 - train: epoch 0044, iter [04200, 05004], lr: 0.010000, loss: 1.2416
2022-10-09 21:28:56 - train: epoch 0044, iter [04300, 05004], lr: 0.010000, loss: 1.1614
2022-10-09 21:29:49 - train: epoch 0044, iter [04400, 05004], lr: 0.010000, loss: 1.1894
2022-10-09 21:30:40 - train: epoch 0044, iter [04500, 05004], lr: 0.010000, loss: 1.1838
2022-10-09 21:31:24 - train: epoch 0044, iter [04600, 05004], lr: 0.010000, loss: 1.0003
2022-10-09 21:32:06 - train: epoch 0044, iter [04700, 05004], lr: 0.010000, loss: 1.2751
2022-10-09 21:32:40 - train: epoch 0044, iter [04800, 05004], lr: 0.010000, loss: 1.0446
2022-10-09 21:33:15 - train: epoch 0044, iter [04900, 05004], lr: 0.010000, loss: 1.1637
2022-10-09 21:33:47 - train: epoch 0044, iter [05000, 05004], lr: 0.010000, loss: 1.1389
2022-10-09 21:33:49 - train: epoch 044, train_loss: 1.1492
2022-10-09 21:35:08 - eval: epoch: 044, acc1: 72.688%, acc5: 91.448%, test_loss: 1.0825, per_image_load_time: 1.681ms, per_image_inference_time: 0.582ms
2022-10-09 21:35:08 - until epoch: 044, best_acc1: 73.328%
2022-10-09 21:35:08 - epoch 045 lr: 0.010000
2022-10-09 21:35:50 - train: epoch 0045, iter [00100, 05004], lr: 0.010000, loss: 1.0915
2022-10-09 21:36:24 - train: epoch 0045, iter [00200, 05004], lr: 0.010000, loss: 1.0429
2022-10-09 21:36:58 - train: epoch 0045, iter [00300, 05004], lr: 0.010000, loss: 1.0717
2022-10-09 21:37:33 - train: epoch 0045, iter [00400, 05004], lr: 0.010000, loss: 1.2372
2022-10-09 21:38:07 - train: epoch 0045, iter [00500, 05004], lr: 0.010000, loss: 1.2404
2022-10-09 21:38:41 - train: epoch 0045, iter [00600, 05004], lr: 0.010000, loss: 1.3179
2022-10-09 21:39:17 - train: epoch 0045, iter [00700, 05004], lr: 0.010000, loss: 0.9129
2022-10-09 21:39:51 - train: epoch 0045, iter [00800, 05004], lr: 0.010000, loss: 1.2062
2022-10-09 21:40:25 - train: epoch 0045, iter [00900, 05004], lr: 0.010000, loss: 1.0489
2022-10-09 21:40:59 - train: epoch 0045, iter [01000, 05004], lr: 0.010000, loss: 1.1576
2022-10-09 21:41:34 - train: epoch 0045, iter [01100, 05004], lr: 0.010000, loss: 1.2294
2022-10-09 21:42:09 - train: epoch 0045, iter [01200, 05004], lr: 0.010000, loss: 1.1250
2022-10-09 21:42:43 - train: epoch 0045, iter [01300, 05004], lr: 0.010000, loss: 1.1345
2022-10-09 21:43:18 - train: epoch 0045, iter [01400, 05004], lr: 0.010000, loss: 1.0233
2022-10-09 21:43:52 - train: epoch 0045, iter [01500, 05004], lr: 0.010000, loss: 1.2611
2022-10-09 21:44:32 - train: epoch 0045, iter [01600, 05004], lr: 0.010000, loss: 1.1812
2022-10-09 21:45:06 - train: epoch 0045, iter [01700, 05004], lr: 0.010000, loss: 1.1399
2022-10-09 21:45:41 - train: epoch 0045, iter [01800, 05004], lr: 0.010000, loss: 1.0983
2022-10-09 21:46:23 - train: epoch 0045, iter [01900, 05004], lr: 0.010000, loss: 1.0692
2022-10-09 21:47:06 - train: epoch 0045, iter [02000, 05004], lr: 0.010000, loss: 1.2032
2022-10-09 21:48:27 - train: epoch 0045, iter [02100, 05004], lr: 0.010000, loss: 1.1399
2022-10-09 21:49:35 - train: epoch 0045, iter [02200, 05004], lr: 0.010000, loss: 1.1504
2022-10-09 21:50:25 - train: epoch 0045, iter [02300, 05004], lr: 0.010000, loss: 1.0543
2022-10-09 21:51:00 - train: epoch 0045, iter [02400, 05004], lr: 0.010000, loss: 1.1552
2022-10-09 21:51:34 - train: epoch 0045, iter [02500, 05004], lr: 0.010000, loss: 1.2895
2022-10-09 21:52:15 - train: epoch 0045, iter [02600, 05004], lr: 0.010000, loss: 1.2858
2022-10-09 21:52:51 - train: epoch 0045, iter [02700, 05004], lr: 0.010000, loss: 0.8965
2022-10-09 21:53:25 - train: epoch 0045, iter [02800, 05004], lr: 0.010000, loss: 0.9689
2022-10-09 21:53:59 - train: epoch 0045, iter [02900, 05004], lr: 0.010000, loss: 1.1873
2022-10-09 21:54:33 - train: epoch 0045, iter [03000, 05004], lr: 0.010000, loss: 1.0529
2022-10-09 21:55:08 - train: epoch 0045, iter [03100, 05004], lr: 0.010000, loss: 1.1599
2022-10-09 21:55:42 - train: epoch 0045, iter [03200, 05004], lr: 0.010000, loss: 1.3793
2022-10-09 21:56:16 - train: epoch 0045, iter [03300, 05004], lr: 0.010000, loss: 1.1554
2022-10-09 21:56:50 - train: epoch 0045, iter [03400, 05004], lr: 0.010000, loss: 1.1342
2022-10-09 21:57:27 - train: epoch 0045, iter [03500, 05004], lr: 0.010000, loss: 1.3338
2022-10-09 21:58:01 - train: epoch 0045, iter [03600, 05004], lr: 0.010000, loss: 1.1333
2022-10-09 21:58:36 - train: epoch 0045, iter [03700, 05004], lr: 0.010000, loss: 1.0094
2022-10-09 21:59:11 - train: epoch 0045, iter [03800, 05004], lr: 0.010000, loss: 1.1521
2022-10-09 21:59:44 - train: epoch 0045, iter [03900, 05004], lr: 0.010000, loss: 1.2647
2022-10-09 22:00:19 - train: epoch 0045, iter [04000, 05004], lr: 0.010000, loss: 1.1347
2022-10-09 22:00:53 - train: epoch 0045, iter [04100, 05004], lr: 0.010000, loss: 1.2063
2022-10-09 22:01:28 - train: epoch 0045, iter [04200, 05004], lr: 0.010000, loss: 1.2919
2022-10-09 22:02:03 - train: epoch 0045, iter [04300, 05004], lr: 0.010000, loss: 1.2519
2022-10-09 22:02:37 - train: epoch 0045, iter [04400, 05004], lr: 0.010000, loss: 1.1179
2022-10-09 22:03:12 - train: epoch 0045, iter [04500, 05004], lr: 0.010000, loss: 1.1373
2022-10-09 22:03:45 - train: epoch 0045, iter [04600, 05004], lr: 0.010000, loss: 1.3777
2022-10-09 22:04:19 - train: epoch 0045, iter [04700, 05004], lr: 0.010000, loss: 1.1410
2022-10-09 22:04:55 - train: epoch 0045, iter [04800, 05004], lr: 0.010000, loss: 1.0589
2022-10-09 22:05:30 - train: epoch 0045, iter [04900, 05004], lr: 0.010000, loss: 1.2542
2022-10-09 22:06:01 - train: epoch 0045, iter [05000, 05004], lr: 0.010000, loss: 1.0668
2022-10-09 22:06:03 - train: epoch 045, train_loss: 1.1428
2022-10-09 22:07:21 - eval: epoch: 045, acc1: 72.438%, acc5: 91.322%, test_loss: 1.0846, per_image_load_time: 0.735ms, per_image_inference_time: 0.574ms
2022-10-09 22:07:21 - until epoch: 045, best_acc1: 73.328%
2022-10-09 22:07:21 - epoch 046 lr: 0.010000
2022-10-09 22:08:02 - train: epoch 0046, iter [00100, 05004], lr: 0.010000, loss: 1.0169
2022-10-09 22:08:36 - train: epoch 0046, iter [00200, 05004], lr: 0.010000, loss: 0.9695
2022-10-09 22:09:10 - train: epoch 0046, iter [00300, 05004], lr: 0.010000, loss: 1.2169
2022-10-09 22:09:44 - train: epoch 0046, iter [00400, 05004], lr: 0.010000, loss: 1.2207
2022-10-09 22:10:18 - train: epoch 0046, iter [00500, 05004], lr: 0.010000, loss: 1.1949
2022-10-09 22:10:51 - train: epoch 0046, iter [00600, 05004], lr: 0.010000, loss: 1.0180
2022-10-09 22:11:26 - train: epoch 0046, iter [00700, 05004], lr: 0.010000, loss: 0.9277
2022-10-09 22:12:00 - train: epoch 0046, iter [00800, 05004], lr: 0.010000, loss: 1.1378
2022-10-09 22:12:34 - train: epoch 0046, iter [00900, 05004], lr: 0.010000, loss: 1.0806
2022-10-09 22:13:09 - train: epoch 0046, iter [01000, 05004], lr: 0.010000, loss: 0.9187
2022-10-09 22:13:42 - train: epoch 0046, iter [01100, 05004], lr: 0.010000, loss: 1.1556
2022-10-09 22:14:16 - train: epoch 0046, iter [01200, 05004], lr: 0.010000, loss: 1.2073
2022-10-09 22:14:51 - train: epoch 0046, iter [01300, 05004], lr: 0.010000, loss: 1.1480
2022-10-09 22:15:27 - train: epoch 0046, iter [01400, 05004], lr: 0.010000, loss: 1.2334
2022-10-09 22:16:00 - train: epoch 0046, iter [01500, 05004], lr: 0.010000, loss: 1.0359
2022-10-09 22:16:40 - train: epoch 0046, iter [01600, 05004], lr: 0.010000, loss: 1.1037
2022-10-09 22:17:14 - train: epoch 0046, iter [01700, 05004], lr: 0.010000, loss: 1.1861
2022-10-09 22:17:49 - train: epoch 0046, iter [01800, 05004], lr: 0.010000, loss: 1.3719
2022-10-09 22:18:26 - train: epoch 0046, iter [01900, 05004], lr: 0.010000, loss: 1.0703
2022-10-09 22:18:59 - train: epoch 0046, iter [02000, 05004], lr: 0.010000, loss: 1.2459
2022-10-09 22:19:52 - train: epoch 0046, iter [02100, 05004], lr: 0.010000, loss: 1.2601
2022-10-09 22:21:23 - train: epoch 0046, iter [02200, 05004], lr: 0.010000, loss: 1.0536
2022-10-09 22:23:40 - train: epoch 0046, iter [02300, 05004], lr: 0.010000, loss: 1.1636
2022-10-09 22:25:31 - train: epoch 0046, iter [02400, 05004], lr: 0.010000, loss: 1.1843
2022-10-09 22:28:02 - train: epoch 0046, iter [02500, 05004], lr: 0.010000, loss: 1.0381
2022-10-09 22:30:16 - train: epoch 0046, iter [02600, 05004], lr: 0.010000, loss: 1.2213
2022-10-09 22:31:42 - train: epoch 0046, iter [02700, 05004], lr: 0.010000, loss: 1.0611
2022-10-09 22:34:08 - train: epoch 0046, iter [02800, 05004], lr: 0.010000, loss: 0.9417
2022-10-09 22:36:20 - train: epoch 0046, iter [02900, 05004], lr: 0.010000, loss: 1.3104
2022-10-09 22:38:14 - train: epoch 0046, iter [03000, 05004], lr: 0.010000, loss: 0.9478
2022-10-09 22:40:36 - train: epoch 0046, iter [03100, 05004], lr: 0.010000, loss: 1.0907
2022-10-09 22:42:39 - train: epoch 0046, iter [03200, 05004], lr: 0.010000, loss: 1.1249
2022-10-09 22:44:21 - train: epoch 0046, iter [03300, 05004], lr: 0.010000, loss: 1.1419
2022-10-09 22:45:12 - train: epoch 0046, iter [03400, 05004], lr: 0.010000, loss: 1.2621
2022-10-09 22:46:00 - train: epoch 0046, iter [03500, 05004], lr: 0.010000, loss: 1.1122
2022-10-09 22:46:34 - train: epoch 0046, iter [03600, 05004], lr: 0.010000, loss: 1.3968
2022-10-09 22:47:10 - train: epoch 0046, iter [03700, 05004], lr: 0.010000, loss: 1.0242
2022-10-09 22:47:45 - train: epoch 0046, iter [03800, 05004], lr: 0.010000, loss: 1.1955
2022-10-09 22:48:26 - train: epoch 0046, iter [03900, 05004], lr: 0.010000, loss: 1.2779
2022-10-09 22:49:07 - train: epoch 0046, iter [04000, 05004], lr: 0.010000, loss: 1.0442
2022-10-09 22:50:28 - train: epoch 0046, iter [04100, 05004], lr: 0.010000, loss: 1.3331
2022-10-09 22:51:32 - train: epoch 0046, iter [04200, 05004], lr: 0.010000, loss: 1.0383
2022-10-09 22:53:24 - train: epoch 0046, iter [04300, 05004], lr: 0.010000, loss: 1.3746
2022-10-09 22:57:24 - train: epoch 0046, iter [04400, 05004], lr: 0.010000, loss: 1.3339
2022-10-09 23:01:41 - train: epoch 0046, iter [04500, 05004], lr: 0.010000, loss: 1.0480
2022-10-09 23:05:06 - train: epoch 0046, iter [04600, 05004], lr: 0.010000, loss: 1.1427
2022-10-09 23:08:25 - train: epoch 0046, iter [04700, 05004], lr: 0.010000, loss: 1.2136
2022-10-09 23:10:25 - train: epoch 0046, iter [04800, 05004], lr: 0.010000, loss: 1.1848
2022-10-09 23:12:19 - train: epoch 0046, iter [04900, 05004], lr: 0.010000, loss: 1.1706
2022-10-09 23:13:11 - train: epoch 0046, iter [05000, 05004], lr: 0.010000, loss: 1.2209
2022-10-09 23:13:13 - train: epoch 046, train_loss: 1.1448
2022-10-09 23:14:31 - eval: epoch: 046, acc1: 72.906%, acc5: 91.456%, test_loss: 1.0806, per_image_load_time: 0.870ms, per_image_inference_time: 0.564ms
2022-10-09 23:14:31 - until epoch: 046, best_acc1: 73.328%
2022-10-09 23:14:31 - epoch 047 lr: 0.010000
2022-10-09 23:15:11 - train: epoch 0047, iter [00100, 05004], lr: 0.010000, loss: 1.1837
2022-10-09 23:15:45 - train: epoch 0047, iter [00200, 05004], lr: 0.010000, loss: 1.1609
2022-10-09 23:16:19 - train: epoch 0047, iter [00300, 05004], lr: 0.010000, loss: 0.9944
2022-10-09 23:16:54 - train: epoch 0047, iter [00400, 05004], lr: 0.010000, loss: 1.1096
2022-10-09 23:17:28 - train: epoch 0047, iter [00500, 05004], lr: 0.010000, loss: 1.0708
2022-10-09 23:18:02 - train: epoch 0047, iter [00600, 05004], lr: 0.010000, loss: 0.9544
2022-10-09 23:18:37 - train: epoch 0047, iter [00700, 05004], lr: 0.010000, loss: 1.2412
2022-10-09 23:19:10 - train: epoch 0047, iter [00800, 05004], lr: 0.010000, loss: 1.1491
2022-10-09 23:19:44 - train: epoch 0047, iter [00900, 05004], lr: 0.010000, loss: 1.1998
2022-10-09 23:20:19 - train: epoch 0047, iter [01000, 05004], lr: 0.010000, loss: 1.1332
2022-10-09 23:20:52 - train: epoch 0047, iter [01100, 05004], lr: 0.010000, loss: 1.3233
2022-10-09 23:21:27 - train: epoch 0047, iter [01200, 05004], lr: 0.010000, loss: 1.1356
2022-10-09 23:22:01 - train: epoch 0047, iter [01300, 05004], lr: 0.010000, loss: 1.0339
2022-10-09 23:22:54 - train: epoch 0047, iter [01400, 05004], lr: 0.010000, loss: 0.9904
2022-10-09 23:23:32 - train: epoch 0047, iter [01500, 05004], lr: 0.010000, loss: 1.2700
2022-10-09 23:24:05 - train: epoch 0047, iter [01600, 05004], lr: 0.010000, loss: 1.1870
2022-10-09 23:24:40 - train: epoch 0047, iter [01700, 05004], lr: 0.010000, loss: 1.0483
2022-10-09 23:25:21 - train: epoch 0047, iter [01800, 05004], lr: 0.010000, loss: 1.1833
2022-10-09 23:25:56 - train: epoch 0047, iter [01900, 05004], lr: 0.010000, loss: 1.0155
2022-10-09 23:26:30 - train: epoch 0047, iter [02000, 05004], lr: 0.010000, loss: 1.0405
2022-10-09 23:27:04 - train: epoch 0047, iter [02100, 05004], lr: 0.010000, loss: 1.2223
2022-10-09 23:27:38 - train: epoch 0047, iter [02200, 05004], lr: 0.010000, loss: 1.1392
2022-10-09 23:28:12 - train: epoch 0047, iter [02300, 05004], lr: 0.010000, loss: 1.1044
2022-10-09 23:28:46 - train: epoch 0047, iter [02400, 05004], lr: 0.010000, loss: 1.1068
2022-10-09 23:29:52 - train: epoch 0047, iter [02500, 05004], lr: 0.010000, loss: 1.2163
2022-10-09 23:30:55 - train: epoch 0047, iter [02600, 05004], lr: 0.010000, loss: 1.2442
2022-10-09 23:31:30 - train: epoch 0047, iter [02700, 05004], lr: 0.010000, loss: 1.0663
2022-10-09 23:32:04 - train: epoch 0047, iter [02800, 05004], lr: 0.010000, loss: 1.1891
2022-10-09 23:32:39 - train: epoch 0047, iter [02900, 05004], lr: 0.010000, loss: 1.0513
2022-10-09 23:33:21 - train: epoch 0047, iter [03000, 05004], lr: 0.010000, loss: 1.1599
2022-10-09 23:34:11 - train: epoch 0047, iter [03100, 05004], lr: 0.010000, loss: 1.1350
2022-10-09 23:35:00 - train: epoch 0047, iter [03200, 05004], lr: 0.010000, loss: 1.2004
2022-10-09 23:35:55 - train: epoch 0047, iter [03300, 05004], lr: 0.010000, loss: 0.9941
2022-10-09 23:36:58 - train: epoch 0047, iter [03400, 05004], lr: 0.010000, loss: 1.1040
2022-10-09 23:37:33 - train: epoch 0047, iter [03500, 05004], lr: 0.010000, loss: 1.1447
2022-10-09 23:38:07 - train: epoch 0047, iter [03600, 05004], lr: 0.010000, loss: 1.0686
2022-10-09 23:38:49 - train: epoch 0047, iter [03700, 05004], lr: 0.010000, loss: 1.1964
2022-10-09 23:39:26 - train: epoch 0047, iter [03800, 05004], lr: 0.010000, loss: 1.0009
2022-10-09 23:39:58 - train: epoch 0047, iter [03900, 05004], lr: 0.010000, loss: 0.9721
2022-10-09 23:40:33 - train: epoch 0047, iter [04000, 05004], lr: 0.010000, loss: 1.2982
2022-10-09 23:41:13 - train: epoch 0047, iter [04100, 05004], lr: 0.010000, loss: 1.3631
2022-10-09 23:42:04 - train: epoch 0047, iter [04200, 05004], lr: 0.010000, loss: 1.0837
2022-10-09 23:42:49 - train: epoch 0047, iter [04300, 05004], lr: 0.010000, loss: 0.9457
2022-10-09 23:43:40 - train: epoch 0047, iter [04400, 05004], lr: 0.010000, loss: 1.1890
2022-10-09 23:44:13 - train: epoch 0047, iter [04500, 05004], lr: 0.010000, loss: 1.2255
2022-10-09 23:45:01 - train: epoch 0047, iter [04600, 05004], lr: 0.010000, loss: 1.1887
2022-10-09 23:45:47 - train: epoch 0047, iter [04700, 05004], lr: 0.010000, loss: 1.4286
2022-10-09 23:46:39 - train: epoch 0047, iter [04800, 05004], lr: 0.010000, loss: 1.1549
2022-10-09 23:47:24 - train: epoch 0047, iter [04900, 05004], lr: 0.010000, loss: 1.1808
2022-10-09 23:48:12 - train: epoch 0047, iter [05000, 05004], lr: 0.010000, loss: 1.2197
2022-10-09 23:48:15 - train: epoch 047, train_loss: 1.1423
2022-10-09 23:49:33 - eval: epoch: 047, acc1: 72.700%, acc5: 91.518%, test_loss: 1.0741, per_image_load_time: 2.278ms, per_image_inference_time: 0.574ms
2022-10-09 23:49:33 - until epoch: 047, best_acc1: 73.328%
2022-10-09 23:49:33 - epoch 048 lr: 0.010000
2022-10-09 23:50:14 - train: epoch 0048, iter [00100, 05004], lr: 0.010000, loss: 1.1870
2022-10-09 23:50:48 - train: epoch 0048, iter [00200, 05004], lr: 0.010000, loss: 1.2386
2022-10-09 23:51:23 - train: epoch 0048, iter [00300, 05004], lr: 0.010000, loss: 1.1339
2022-10-09 23:51:57 - train: epoch 0048, iter [00400, 05004], lr: 0.010000, loss: 1.1436
2022-10-09 23:52:31 - train: epoch 0048, iter [00500, 05004], lr: 0.010000, loss: 1.1371
2022-10-09 23:53:05 - train: epoch 0048, iter [00600, 05004], lr: 0.010000, loss: 1.1605
2022-10-09 23:53:39 - train: epoch 0048, iter [00700, 05004], lr: 0.010000, loss: 1.0887
2022-10-09 23:54:13 - train: epoch 0048, iter [00800, 05004], lr: 0.010000, loss: 1.1342
2022-10-09 23:54:48 - train: epoch 0048, iter [00900, 05004], lr: 0.010000, loss: 1.2234
2022-10-09 23:55:22 - train: epoch 0048, iter [01000, 05004], lr: 0.010000, loss: 1.2097
2022-10-09 23:55:57 - train: epoch 0048, iter [01100, 05004], lr: 0.010000, loss: 1.3513
2022-10-09 23:56:31 - train: epoch 0048, iter [01200, 05004], lr: 0.010000, loss: 1.1520
2022-10-09 23:57:06 - train: epoch 0048, iter [01300, 05004], lr: 0.010000, loss: 0.9505
2022-10-09 23:57:39 - train: epoch 0048, iter [01400, 05004], lr: 0.010000, loss: 1.0129
2022-10-09 23:58:14 - train: epoch 0048, iter [01500, 05004], lr: 0.010000, loss: 1.2809
2022-10-09 23:58:48 - train: epoch 0048, iter [01600, 05004], lr: 0.010000, loss: 1.0294
2022-10-09 23:59:23 - train: epoch 0048, iter [01700, 05004], lr: 0.010000, loss: 1.0086
2022-10-09 23:59:58 - train: epoch 0048, iter [01800, 05004], lr: 0.010000, loss: 1.2440
2022-10-10 00:00:31 - train: epoch 0048, iter [01900, 05004], lr: 0.010000, loss: 1.1518
2022-10-10 00:01:06 - train: epoch 0048, iter [02000, 05004], lr: 0.010000, loss: 1.2889
2022-10-10 00:01:41 - train: epoch 0048, iter [02100, 05004], lr: 0.010000, loss: 1.2278
2022-10-10 00:02:15 - train: epoch 0048, iter [02200, 05004], lr: 0.010000, loss: 1.2457
2022-10-10 00:02:49 - train: epoch 0048, iter [02300, 05004], lr: 0.010000, loss: 1.0034
2022-10-10 00:03:46 - train: epoch 0048, iter [02400, 05004], lr: 0.010000, loss: 1.3564
2022-10-10 00:04:25 - train: epoch 0048, iter [02500, 05004], lr: 0.010000, loss: 1.3101
2022-10-10 00:05:01 - train: epoch 0048, iter [02600, 05004], lr: 0.010000, loss: 1.1104
2022-10-10 00:05:38 - train: epoch 0048, iter [02700, 05004], lr: 0.010000, loss: 1.3171
2022-10-10 00:06:13 - train: epoch 0048, iter [02800, 05004], lr: 0.010000, loss: 1.1851
2022-10-10 00:06:47 - train: epoch 0048, iter [02900, 05004], lr: 0.010000, loss: 1.1426
2022-10-10 00:07:25 - train: epoch 0048, iter [03000, 05004], lr: 0.010000, loss: 1.0647
2022-10-10 00:08:02 - train: epoch 0048, iter [03100, 05004], lr: 0.010000, loss: 1.2333
2022-10-10 00:08:36 - train: epoch 0048, iter [03200, 05004], lr: 0.010000, loss: 1.0213
2022-10-10 00:09:16 - train: epoch 0048, iter [03300, 05004], lr: 0.010000, loss: 1.3481
2022-10-10 00:09:55 - train: epoch 0048, iter [03400, 05004], lr: 0.010000, loss: 1.0796
2022-10-10 00:10:35 - train: epoch 0048, iter [03500, 05004], lr: 0.010000, loss: 1.2153
2022-10-10 00:11:13 - train: epoch 0048, iter [03600, 05004], lr: 0.010000, loss: 1.1476
2022-10-10 00:11:52 - train: epoch 0048, iter [03700, 05004], lr: 0.010000, loss: 1.1689
2022-10-10 00:13:31 - train: epoch 0048, iter [03800, 05004], lr: 0.010000, loss: 1.1112
2022-10-10 00:15:28 - train: epoch 0048, iter [03900, 05004], lr: 0.010000, loss: 1.3054
2022-10-10 00:17:38 - train: epoch 0048, iter [04000, 05004], lr: 0.010000, loss: 1.0997
2022-10-10 00:20:19 - train: epoch 0048, iter [04100, 05004], lr: 0.010000, loss: 1.3575
2022-10-10 00:23:10 - train: epoch 0048, iter [04200, 05004], lr: 0.010000, loss: 1.1500
2022-10-10 00:25:31 - train: epoch 0048, iter [04300, 05004], lr: 0.010000, loss: 1.1766
2022-10-10 00:27:07 - train: epoch 0048, iter [04400, 05004], lr: 0.010000, loss: 1.1906
2022-10-10 00:27:54 - train: epoch 0048, iter [04500, 05004], lr: 0.010000, loss: 1.2351
2022-10-10 00:28:30 - train: epoch 0048, iter [04600, 05004], lr: 0.010000, loss: 1.1223
2022-10-10 00:29:09 - train: epoch 0048, iter [04700, 05004], lr: 0.010000, loss: 1.2911
2022-10-10 00:29:44 - train: epoch 0048, iter [04800, 05004], lr: 0.010000, loss: 1.4378
2022-10-10 00:30:21 - train: epoch 0048, iter [04900, 05004], lr: 0.010000, loss: 1.3644
2022-10-10 00:30:53 - train: epoch 0048, iter [05000, 05004], lr: 0.010000, loss: 1.1282
2022-10-10 00:30:56 - train: epoch 048, train_loss: 1.1407
2022-10-10 00:32:15 - eval: epoch: 048, acc1: 72.290%, acc5: 91.260%, test_loss: 1.0903, per_image_load_time: 2.265ms, per_image_inference_time: 0.577ms
2022-10-10 00:32:15 - until epoch: 048, best_acc1: 73.328%
2022-10-10 00:32:15 - epoch 049 lr: 0.010000
2022-10-10 00:32:56 - train: epoch 0049, iter [00100, 05004], lr: 0.010000, loss: 1.1347
2022-10-10 00:33:32 - train: epoch 0049, iter [00200, 05004], lr: 0.010000, loss: 1.0385
2022-10-10 00:34:05 - train: epoch 0049, iter [00300, 05004], lr: 0.010000, loss: 1.2350
2022-10-10 00:34:41 - train: epoch 0049, iter [00400, 05004], lr: 0.010000, loss: 1.1564
2022-10-10 00:35:16 - train: epoch 0049, iter [00500, 05004], lr: 0.010000, loss: 1.1206
2022-10-10 00:35:51 - train: epoch 0049, iter [00600, 05004], lr: 0.010000, loss: 1.0068
2022-10-10 00:36:25 - train: epoch 0049, iter [00700, 05004], lr: 0.010000, loss: 1.0590
2022-10-10 00:37:00 - train: epoch 0049, iter [00800, 05004], lr: 0.010000, loss: 1.4148
2022-10-10 00:37:32 - train: epoch 0049, iter [00900, 05004], lr: 0.010000, loss: 0.8958
2022-10-10 00:38:07 - train: epoch 0049, iter [01000, 05004], lr: 0.010000, loss: 1.2303
2022-10-10 00:38:41 - train: epoch 0049, iter [01100, 05004], lr: 0.010000, loss: 0.9737
2022-10-10 00:39:15 - train: epoch 0049, iter [01200, 05004], lr: 0.010000, loss: 1.0809
2022-10-10 00:39:50 - train: epoch 0049, iter [01300, 05004], lr: 0.010000, loss: 1.2752
2022-10-10 00:40:23 - train: epoch 0049, iter [01400, 05004], lr: 0.010000, loss: 1.2144
2022-10-10 00:40:58 - train: epoch 0049, iter [01500, 05004], lr: 0.010000, loss: 1.0637
2022-10-10 00:41:32 - train: epoch 0049, iter [01600, 05004], lr: 0.010000, loss: 1.1078
2022-10-10 00:42:07 - train: epoch 0049, iter [01700, 05004], lr: 0.010000, loss: 1.1964
2022-10-10 00:42:41 - train: epoch 0049, iter [01800, 05004], lr: 0.010000, loss: 1.0834
2022-10-10 00:43:16 - train: epoch 0049, iter [01900, 05004], lr: 0.010000, loss: 1.1272
2022-10-10 00:43:50 - train: epoch 0049, iter [02000, 05004], lr: 0.010000, loss: 1.3196
2022-10-10 00:44:24 - train: epoch 0049, iter [02100, 05004], lr: 0.010000, loss: 1.0785
2022-10-10 00:44:59 - train: epoch 0049, iter [02200, 05004], lr: 0.010000, loss: 1.0762
2022-10-10 00:45:33 - train: epoch 0049, iter [02300, 05004], lr: 0.010000, loss: 0.8957
2022-10-10 00:46:08 - train: epoch 0049, iter [02400, 05004], lr: 0.010000, loss: 1.1193
2022-10-10 00:46:41 - train: epoch 0049, iter [02500, 05004], lr: 0.010000, loss: 1.1727
2022-10-10 00:47:17 - train: epoch 0049, iter [02600, 05004], lr: 0.010000, loss: 1.2059
2022-10-10 00:47:51 - train: epoch 0049, iter [02700, 05004], lr: 0.010000, loss: 1.0707
2022-10-10 00:48:26 - train: epoch 0049, iter [02800, 05004], lr: 0.010000, loss: 1.0970
2022-10-10 00:49:00 - train: epoch 0049, iter [02900, 05004], lr: 0.010000, loss: 1.3419
2022-10-10 00:49:35 - train: epoch 0049, iter [03000, 05004], lr: 0.010000, loss: 1.2269
2022-10-10 00:50:10 - train: epoch 0049, iter [03100, 05004], lr: 0.010000, loss: 1.2467
2022-10-10 00:50:43 - train: epoch 0049, iter [03200, 05004], lr: 0.010000, loss: 1.1731
2022-10-10 00:51:19 - train: epoch 0049, iter [03300, 05004], lr: 0.010000, loss: 1.1069
2022-10-10 00:51:53 - train: epoch 0049, iter [03400, 05004], lr: 0.010000, loss: 1.2204
2022-10-10 00:52:29 - train: epoch 0049, iter [03500, 05004], lr: 0.010000, loss: 1.1528
2022-10-10 00:53:04 - train: epoch 0049, iter [03600, 05004], lr: 0.010000, loss: 1.2914
2022-10-10 00:53:38 - train: epoch 0049, iter [03700, 05004], lr: 0.010000, loss: 1.0595
2022-10-10 00:54:15 - train: epoch 0049, iter [03800, 05004], lr: 0.010000, loss: 1.2882
2022-10-10 00:54:50 - train: epoch 0049, iter [03900, 05004], lr: 0.010000, loss: 1.3369
2022-10-10 00:55:24 - train: epoch 0049, iter [04000, 05004], lr: 0.010000, loss: 0.9699
2022-10-10 00:55:57 - train: epoch 0049, iter [04100, 05004], lr: 0.010000, loss: 1.1509
2022-10-10 00:56:33 - train: epoch 0049, iter [04200, 05004], lr: 0.010000, loss: 1.1846
2022-10-10 00:57:07 - train: epoch 0049, iter [04300, 05004], lr: 0.010000, loss: 1.2439
2022-10-10 00:57:42 - train: epoch 0049, iter [04400, 05004], lr: 0.010000, loss: 1.2450
2022-10-10 00:58:17 - train: epoch 0049, iter [04500, 05004], lr: 0.010000, loss: 1.0066
2022-10-10 00:58:52 - train: epoch 0049, iter [04600, 05004], lr: 0.010000, loss: 1.2818
2022-10-10 00:59:26 - train: epoch 0049, iter [04700, 05004], lr: 0.010000, loss: 1.0491
2022-10-10 01:00:01 - train: epoch 0049, iter [04800, 05004], lr: 0.010000, loss: 1.0357
2022-10-10 01:00:36 - train: epoch 0049, iter [04900, 05004], lr: 0.010000, loss: 1.0125
2022-10-10 01:01:09 - train: epoch 0049, iter [05000, 05004], lr: 0.010000, loss: 1.2942
2022-10-10 01:01:11 - train: epoch 049, train_loss: 1.1373
2022-10-10 01:02:29 - eval: epoch: 049, acc1: 71.684%, acc5: 90.800%, test_loss: 1.1302, per_image_load_time: 1.350ms, per_image_inference_time: 0.575ms
2022-10-10 01:02:29 - until epoch: 049, best_acc1: 73.328%
2022-10-10 01:02:29 - epoch 050 lr: 0.010000
2022-10-10 01:03:10 - train: epoch 0050, iter [00100, 05004], lr: 0.010000, loss: 1.0172
2022-10-10 01:03:45 - train: epoch 0050, iter [00200, 05004], lr: 0.010000, loss: 1.1895
2022-10-10 01:04:19 - train: epoch 0050, iter [00300, 05004], lr: 0.010000, loss: 1.0519
2022-10-10 01:04:54 - train: epoch 0050, iter [00400, 05004], lr: 0.010000, loss: 1.0184
2022-10-10 01:05:28 - train: epoch 0050, iter [00500, 05004], lr: 0.010000, loss: 1.1516
2022-10-10 01:06:03 - train: epoch 0050, iter [00600, 05004], lr: 0.010000, loss: 1.3810
2022-10-10 01:06:38 - train: epoch 0050, iter [00700, 05004], lr: 0.010000, loss: 0.9872
2022-10-10 01:07:12 - train: epoch 0050, iter [00800, 05004], lr: 0.010000, loss: 0.9916
2022-10-10 01:07:48 - train: epoch 0050, iter [00900, 05004], lr: 0.010000, loss: 1.0815
2022-10-10 01:08:22 - train: epoch 0050, iter [01000, 05004], lr: 0.010000, loss: 0.9834
2022-10-10 01:08:57 - train: epoch 0050, iter [01100, 05004], lr: 0.010000, loss: 1.3100
2022-10-10 01:09:31 - train: epoch 0050, iter [01200, 05004], lr: 0.010000, loss: 1.1119
2022-10-10 01:10:07 - train: epoch 0050, iter [01300, 05004], lr: 0.010000, loss: 1.2280
2022-10-10 01:10:42 - train: epoch 0050, iter [01400, 05004], lr: 0.010000, loss: 1.2046
2022-10-10 01:11:16 - train: epoch 0050, iter [01500, 05004], lr: 0.010000, loss: 1.0798
2022-10-10 01:11:52 - train: epoch 0050, iter [01600, 05004], lr: 0.010000, loss: 0.9548
2022-10-10 01:12:26 - train: epoch 0050, iter [01700, 05004], lr: 0.010000, loss: 1.3428
2022-10-10 01:13:02 - train: epoch 0050, iter [01800, 05004], lr: 0.010000, loss: 1.2770
2022-10-10 01:13:36 - train: epoch 0050, iter [01900, 05004], lr: 0.010000, loss: 1.0345
2022-10-10 01:14:11 - train: epoch 0050, iter [02000, 05004], lr: 0.010000, loss: 0.9908
2022-10-10 01:14:45 - train: epoch 0050, iter [02100, 05004], lr: 0.010000, loss: 1.0423
2022-10-10 01:15:19 - train: epoch 0050, iter [02200, 05004], lr: 0.010000, loss: 1.2585
2022-10-10 01:15:53 - train: epoch 0050, iter [02300, 05004], lr: 0.010000, loss: 0.9034
2022-10-10 01:16:28 - train: epoch 0050, iter [02400, 05004], lr: 0.010000, loss: 1.3472
2022-10-10 01:17:03 - train: epoch 0050, iter [02500, 05004], lr: 0.010000, loss: 1.0592
2022-10-10 01:17:38 - train: epoch 0050, iter [02600, 05004], lr: 0.010000, loss: 1.0277
2022-10-10 01:18:12 - train: epoch 0050, iter [02700, 05004], lr: 0.010000, loss: 1.2092
2022-10-10 01:18:47 - train: epoch 0050, iter [02800, 05004], lr: 0.010000, loss: 1.3319
2022-10-10 01:19:21 - train: epoch 0050, iter [02900, 05004], lr: 0.010000, loss: 1.3785
2022-10-10 01:19:56 - train: epoch 0050, iter [03000, 05004], lr: 0.010000, loss: 1.2328
2022-10-10 01:20:30 - train: epoch 0050, iter [03100, 05004], lr: 0.010000, loss: 1.1929
2022-10-10 01:21:05 - train: epoch 0050, iter [03200, 05004], lr: 0.010000, loss: 1.2146
2022-10-10 01:21:39 - train: epoch 0050, iter [03300, 05004], lr: 0.010000, loss: 1.1745
2022-10-10 01:22:13 - train: epoch 0050, iter [03400, 05004], lr: 0.010000, loss: 1.1428
2022-10-10 01:22:48 - train: epoch 0050, iter [03500, 05004], lr: 0.010000, loss: 1.1390
2022-10-10 01:23:23 - train: epoch 0050, iter [03600, 05004], lr: 0.010000, loss: 1.0199
2022-10-10 01:23:56 - train: epoch 0050, iter [03700, 05004], lr: 0.010000, loss: 1.3140
2022-10-10 01:24:31 - train: epoch 0050, iter [03800, 05004], lr: 0.010000, loss: 1.0389
2022-10-10 01:25:06 - train: epoch 0050, iter [03900, 05004], lr: 0.010000, loss: 0.9593
2022-10-10 01:25:41 - train: epoch 0050, iter [04000, 05004], lr: 0.010000, loss: 1.2048
2022-10-10 01:26:16 - train: epoch 0050, iter [04100, 05004], lr: 0.010000, loss: 1.1774
2022-10-10 01:26:51 - train: epoch 0050, iter [04200, 05004], lr: 0.010000, loss: 1.1173
2022-10-10 01:27:25 - train: epoch 0050, iter [04300, 05004], lr: 0.010000, loss: 1.3009
2022-10-10 01:28:00 - train: epoch 0050, iter [04400, 05004], lr: 0.010000, loss: 0.9644
2022-10-10 01:28:34 - train: epoch 0050, iter [04500, 05004], lr: 0.010000, loss: 1.1370
2022-10-10 01:29:09 - train: epoch 0050, iter [04600, 05004], lr: 0.010000, loss: 1.2669
2022-10-10 01:29:44 - train: epoch 0050, iter [04700, 05004], lr: 0.010000, loss: 1.0907
2022-10-10 01:30:19 - train: epoch 0050, iter [04800, 05004], lr: 0.010000, loss: 1.2607
2022-10-10 01:30:53 - train: epoch 0050, iter [04900, 05004], lr: 0.010000, loss: 1.0416
2022-10-10 01:31:26 - train: epoch 0050, iter [05000, 05004], lr: 0.010000, loss: 1.0513
2022-10-10 01:31:28 - train: epoch 050, train_loss: 1.1361
2022-10-10 01:32:48 - eval: epoch: 050, acc1: 72.624%, acc5: 91.318%, test_loss: 1.0917, per_image_load_time: 2.443ms, per_image_inference_time: 0.553ms
2022-10-10 01:32:48 - until epoch: 050, best_acc1: 73.328%
2022-10-10 01:32:48 - epoch 051 lr: 0.010000
2022-10-10 01:33:30 - train: epoch 0051, iter [00100, 05004], lr: 0.010000, loss: 1.1654
2022-10-10 01:34:06 - train: epoch 0051, iter [00200, 05004], lr: 0.010000, loss: 1.2132
2022-10-10 01:34:41 - train: epoch 0051, iter [00300, 05004], lr: 0.010000, loss: 1.0835
2022-10-10 01:35:17 - train: epoch 0051, iter [00400, 05004], lr: 0.010000, loss: 1.0103
2022-10-10 01:35:52 - train: epoch 0051, iter [00500, 05004], lr: 0.010000, loss: 0.9826
2022-10-10 01:36:27 - train: epoch 0051, iter [00600, 05004], lr: 0.010000, loss: 0.9966
2022-10-10 01:37:02 - train: epoch 0051, iter [00700, 05004], lr: 0.010000, loss: 0.9701
2022-10-10 01:37:38 - train: epoch 0051, iter [00800, 05004], lr: 0.010000, loss: 1.4470
2022-10-10 01:38:15 - train: epoch 0051, iter [00900, 05004], lr: 0.010000, loss: 1.1165
2022-10-10 01:38:48 - train: epoch 0051, iter [01000, 05004], lr: 0.010000, loss: 1.4520
2022-10-10 01:39:25 - train: epoch 0051, iter [01100, 05004], lr: 0.010000, loss: 1.0011
2022-10-10 01:40:00 - train: epoch 0051, iter [01200, 05004], lr: 0.010000, loss: 0.9401
2022-10-10 01:40:36 - train: epoch 0051, iter [01300, 05004], lr: 0.010000, loss: 0.8783
2022-10-10 01:41:14 - train: epoch 0051, iter [01400, 05004], lr: 0.010000, loss: 1.0291
2022-10-10 01:41:50 - train: epoch 0051, iter [01500, 05004], lr: 0.010000, loss: 1.0811
2022-10-10 01:42:24 - train: epoch 0051, iter [01600, 05004], lr: 0.010000, loss: 0.8365
2022-10-10 01:42:59 - train: epoch 0051, iter [01700, 05004], lr: 0.010000, loss: 1.1050
2022-10-10 01:43:33 - train: epoch 0051, iter [01800, 05004], lr: 0.010000, loss: 1.1252
2022-10-10 01:44:07 - train: epoch 0051, iter [01900, 05004], lr: 0.010000, loss: 1.0539
2022-10-10 01:44:42 - train: epoch 0051, iter [02000, 05004], lr: 0.010000, loss: 1.0401
2022-10-10 01:45:15 - train: epoch 0051, iter [02100, 05004], lr: 0.010000, loss: 0.9843
2022-10-10 01:45:51 - train: epoch 0051, iter [02200, 05004], lr: 0.010000, loss: 1.0912
2022-10-10 01:46:26 - train: epoch 0051, iter [02300, 05004], lr: 0.010000, loss: 1.0368
2022-10-10 01:47:01 - train: epoch 0051, iter [02400, 05004], lr: 0.010000, loss: 1.0509
2022-10-10 01:47:36 - train: epoch 0051, iter [02500, 05004], lr: 0.010000, loss: 0.9357
2022-10-10 01:48:10 - train: epoch 0051, iter [02600, 05004], lr: 0.010000, loss: 1.1619
2022-10-10 01:48:46 - train: epoch 0051, iter [02700, 05004], lr: 0.010000, loss: 1.2334
2022-10-10 01:49:22 - train: epoch 0051, iter [02800, 05004], lr: 0.010000, loss: 1.0521
2022-10-10 01:49:55 - train: epoch 0051, iter [02900, 05004], lr: 0.010000, loss: 1.0949
2022-10-10 01:50:30 - train: epoch 0051, iter [03000, 05004], lr: 0.010000, loss: 1.1629
2022-10-10 01:51:05 - train: epoch 0051, iter [03100, 05004], lr: 0.010000, loss: 1.0642
2022-10-10 01:51:39 - train: epoch 0051, iter [03200, 05004], lr: 0.010000, loss: 1.0661
2022-10-10 01:52:13 - train: epoch 0051, iter [03300, 05004], lr: 0.010000, loss: 1.2216
2022-10-10 01:52:47 - train: epoch 0051, iter [03400, 05004], lr: 0.010000, loss: 1.1758
2022-10-10 01:53:21 - train: epoch 0051, iter [03500, 05004], lr: 0.010000, loss: 1.0343
2022-10-10 01:53:55 - train: epoch 0051, iter [03600, 05004], lr: 0.010000, loss: 1.2204
2022-10-10 01:54:30 - train: epoch 0051, iter [03700, 05004], lr: 0.010000, loss: 1.2416
2022-10-10 01:55:04 - train: epoch 0051, iter [03800, 05004], lr: 0.010000, loss: 1.2598
2022-10-10 01:55:38 - train: epoch 0051, iter [03900, 05004], lr: 0.010000, loss: 1.2466
2022-10-10 01:56:13 - train: epoch 0051, iter [04000, 05004], lr: 0.010000, loss: 1.1218
2022-10-10 01:56:47 - train: epoch 0051, iter [04100, 05004], lr: 0.010000, loss: 1.1521
2022-10-10 01:57:21 - train: epoch 0051, iter [04200, 05004], lr: 0.010000, loss: 1.2629
2022-10-10 01:57:56 - train: epoch 0051, iter [04300, 05004], lr: 0.010000, loss: 1.0899
2022-10-10 01:58:30 - train: epoch 0051, iter [04400, 05004], lr: 0.010000, loss: 1.3225
2022-10-10 01:59:05 - train: epoch 0051, iter [04500, 05004], lr: 0.010000, loss: 1.0740
2022-10-10 01:59:39 - train: epoch 0051, iter [04600, 05004], lr: 0.010000, loss: 1.4228
2022-10-10 02:00:14 - train: epoch 0051, iter [04700, 05004], lr: 0.010000, loss: 1.2044
2022-10-10 02:00:48 - train: epoch 0051, iter [04800, 05004], lr: 0.010000, loss: 1.1696
2022-10-10 02:01:22 - train: epoch 0051, iter [04900, 05004], lr: 0.010000, loss: 1.0539
2022-10-10 02:01:54 - train: epoch 0051, iter [05000, 05004], lr: 0.010000, loss: 1.1224
2022-10-10 02:01:57 - train: epoch 051, train_loss: 1.1342
2022-10-10 02:03:15 - eval: epoch: 051, acc1: 72.504%, acc5: 91.256%, test_loss: 1.0944, per_image_load_time: 2.200ms, per_image_inference_time: 0.565ms
2022-10-10 02:03:15 - until epoch: 051, best_acc1: 73.328%
2022-10-10 02:03:15 - epoch 052 lr: 0.010000
2022-10-10 02:03:56 - train: epoch 0052, iter [00100, 05004], lr: 0.010000, loss: 1.1197
2022-10-10 02:04:30 - train: epoch 0052, iter [00200, 05004], lr: 0.010000, loss: 1.1991
2022-10-10 02:05:06 - train: epoch 0052, iter [00300, 05004], lr: 0.010000, loss: 1.0533
2022-10-10 02:05:40 - train: epoch 0052, iter [00400, 05004], lr: 0.010000, loss: 1.2296
2022-10-10 02:06:14 - train: epoch 0052, iter [00500, 05004], lr: 0.010000, loss: 1.2341
2022-10-10 02:06:51 - train: epoch 0052, iter [00600, 05004], lr: 0.010000, loss: 1.0180
2022-10-10 02:07:27 - train: epoch 0052, iter [00700, 05004], lr: 0.010000, loss: 1.1340
2022-10-10 02:08:03 - train: epoch 0052, iter [00800, 05004], lr: 0.010000, loss: 1.1700
2022-10-10 02:08:38 - train: epoch 0052, iter [00900, 05004], lr: 0.010000, loss: 1.0536
2022-10-10 02:09:13 - train: epoch 0052, iter [01000, 05004], lr: 0.010000, loss: 1.3339
2022-10-10 02:09:48 - train: epoch 0052, iter [01100, 05004], lr: 0.010000, loss: 1.2536
2022-10-10 02:10:33 - train: epoch 0052, iter [01200, 05004], lr: 0.010000, loss: 1.0293
2022-10-10 02:11:14 - train: epoch 0052, iter [01300, 05004], lr: 0.010000, loss: 0.9310
2022-10-10 02:11:46 - train: epoch 0052, iter [01400, 05004], lr: 0.010000, loss: 1.0717
2022-10-10 02:12:23 - train: epoch 0052, iter [01500, 05004], lr: 0.010000, loss: 1.0497
2022-10-10 02:12:57 - train: epoch 0052, iter [01600, 05004], lr: 0.010000, loss: 1.0477
2022-10-10 02:13:32 - train: epoch 0052, iter [01700, 05004], lr: 0.010000, loss: 0.9804
2022-10-10 02:14:13 - train: epoch 0052, iter [01800, 05004], lr: 0.010000, loss: 1.1812
2022-10-10 02:14:49 - train: epoch 0052, iter [01900, 05004], lr: 0.010000, loss: 1.0135
2022-10-10 02:15:23 - train: epoch 0052, iter [02000, 05004], lr: 0.010000, loss: 1.1758
2022-10-10 02:16:00 - train: epoch 0052, iter [02100, 05004], lr: 0.010000, loss: 1.0394
2022-10-10 02:16:36 - train: epoch 0052, iter [02200, 05004], lr: 0.010000, loss: 1.2404
2022-10-10 02:17:11 - train: epoch 0052, iter [02300, 05004], lr: 0.010000, loss: 1.1048
2022-10-10 02:17:44 - train: epoch 0052, iter [02400, 05004], lr: 0.010000, loss: 0.8797
2022-10-10 02:18:19 - train: epoch 0052, iter [02500, 05004], lr: 0.010000, loss: 1.0056
2022-10-10 02:18:53 - train: epoch 0052, iter [02600, 05004], lr: 0.010000, loss: 0.9860
2022-10-10 02:19:27 - train: epoch 0052, iter [02700, 05004], lr: 0.010000, loss: 1.1876
2022-10-10 02:20:01 - train: epoch 0052, iter [02800, 05004], lr: 0.010000, loss: 1.1027
2022-10-10 02:20:35 - train: epoch 0052, iter [02900, 05004], lr: 0.010000, loss: 1.0336
2022-10-10 02:21:09 - train: epoch 0052, iter [03000, 05004], lr: 0.010000, loss: 1.0612
2022-10-10 02:21:44 - train: epoch 0052, iter [03100, 05004], lr: 0.010000, loss: 1.2274
2022-10-10 02:22:19 - train: epoch 0052, iter [03200, 05004], lr: 0.010000, loss: 1.0821
2022-10-10 02:22:52 - train: epoch 0052, iter [03300, 05004], lr: 0.010000, loss: 1.2103
2022-10-10 02:23:27 - train: epoch 0052, iter [03400, 05004], lr: 0.010000, loss: 1.1979
2022-10-10 02:24:02 - train: epoch 0052, iter [03500, 05004], lr: 0.010000, loss: 1.1933
2022-10-10 02:24:38 - train: epoch 0052, iter [03600, 05004], lr: 0.010000, loss: 1.2848
2022-10-10 02:25:12 - train: epoch 0052, iter [03700, 05004], lr: 0.010000, loss: 1.2005
2022-10-10 02:25:47 - train: epoch 0052, iter [03800, 05004], lr: 0.010000, loss: 1.1222
2022-10-10 02:26:22 - train: epoch 0052, iter [03900, 05004], lr: 0.010000, loss: 0.9482
2022-10-10 02:26:56 - train: epoch 0052, iter [04000, 05004], lr: 0.010000, loss: 1.2364
2022-10-10 02:27:31 - train: epoch 0052, iter [04100, 05004], lr: 0.010000, loss: 1.0520
2022-10-10 02:28:50 - train: epoch 0052, iter [04200, 05004], lr: 0.010000, loss: 1.1366
2022-10-10 02:30:03 - train: epoch 0052, iter [04300, 05004], lr: 0.010000, loss: 1.1973
2022-10-10 02:30:51 - train: epoch 0052, iter [04400, 05004], lr: 0.010000, loss: 1.1185
2022-10-10 02:31:31 - train: epoch 0052, iter [04500, 05004], lr: 0.010000, loss: 1.1164
2022-10-10 02:33:00 - train: epoch 0052, iter [04600, 05004], lr: 0.010000, loss: 1.1833
2022-10-10 02:34:13 - train: epoch 0052, iter [04700, 05004], lr: 0.010000, loss: 1.2557
2022-10-10 02:35:12 - train: epoch 0052, iter [04800, 05004], lr: 0.010000, loss: 0.9096
2022-10-10 02:36:45 - train: epoch 0052, iter [04900, 05004], lr: 0.010000, loss: 1.0458
2022-10-10 02:38:00 - train: epoch 0052, iter [05000, 05004], lr: 0.010000, loss: 1.0801
2022-10-10 02:38:02 - train: epoch 052, train_loss: 1.1282
2022-10-10 02:39:20 - eval: epoch: 052, acc1: 72.214%, acc5: 91.034%, test_loss: 1.1035, per_image_load_time: 2.298ms, per_image_inference_time: 0.579ms
2022-10-10 02:39:20 - until epoch: 052, best_acc1: 73.328%
2022-10-10 02:39:20 - epoch 053 lr: 0.010000
2022-10-10 02:40:01 - train: epoch 0053, iter [00100, 05004], lr: 0.010000, loss: 1.0718
2022-10-10 02:40:35 - train: epoch 0053, iter [00200, 05004], lr: 0.010000, loss: 1.0749
2022-10-10 02:41:09 - train: epoch 0053, iter [00300, 05004], lr: 0.010000, loss: 1.0010
2022-10-10 02:41:42 - train: epoch 0053, iter [00400, 05004], lr: 0.010000, loss: 1.1690
2022-10-10 02:42:17 - train: epoch 0053, iter [00500, 05004], lr: 0.010000, loss: 1.0278
2022-10-10 02:42:51 - train: epoch 0053, iter [00600, 05004], lr: 0.010000, loss: 1.0827
2022-10-10 02:43:25 - train: epoch 0053, iter [00700, 05004], lr: 0.010000, loss: 1.0603
2022-10-10 02:43:59 - train: epoch 0053, iter [00800, 05004], lr: 0.010000, loss: 1.1539
2022-10-10 02:44:32 - train: epoch 0053, iter [00900, 05004], lr: 0.010000, loss: 1.2688
2022-10-10 02:45:06 - train: epoch 0053, iter [01000, 05004], lr: 0.010000, loss: 1.1239
2022-10-10 02:45:40 - train: epoch 0053, iter [01100, 05004], lr: 0.010000, loss: 1.0946
2022-10-10 02:46:14 - train: epoch 0053, iter [01200, 05004], lr: 0.010000, loss: 0.9516
2022-10-10 02:46:48 - train: epoch 0053, iter [01300, 05004], lr: 0.010000, loss: 1.0842
2022-10-10 02:47:22 - train: epoch 0053, iter [01400, 05004], lr: 0.010000, loss: 1.0993
2022-10-10 02:47:56 - train: epoch 0053, iter [01500, 05004], lr: 0.010000, loss: 1.0166
2022-10-10 02:48:30 - train: epoch 0053, iter [01600, 05004], lr: 0.010000, loss: 1.3779
2022-10-10 02:49:06 - train: epoch 0053, iter [01700, 05004], lr: 0.010000, loss: 1.2432
2022-10-10 02:49:40 - train: epoch 0053, iter [01800, 05004], lr: 0.010000, loss: 1.3233
2022-10-10 02:50:13 - train: epoch 0053, iter [01900, 05004], lr: 0.010000, loss: 0.9752
2022-10-10 02:50:49 - train: epoch 0053, iter [02000, 05004], lr: 0.010000, loss: 0.9830
2022-10-10 02:51:25 - train: epoch 0053, iter [02100, 05004], lr: 0.010000, loss: 1.2366
2022-10-10 02:51:59 - train: epoch 0053, iter [02200, 05004], lr: 0.010000, loss: 1.0703
2022-10-10 02:52:34 - train: epoch 0053, iter [02300, 05004], lr: 0.010000, loss: 1.1173
2022-10-10 02:53:09 - train: epoch 0053, iter [02400, 05004], lr: 0.010000, loss: 1.0368
2022-10-10 02:53:43 - train: epoch 0053, iter [02500, 05004], lr: 0.010000, loss: 1.1977
2022-10-10 02:54:22 - train: epoch 0053, iter [02600, 05004], lr: 0.010000, loss: 1.2776
2022-10-10 02:55:00 - train: epoch 0053, iter [02700, 05004], lr: 0.010000, loss: 1.1054
2022-10-10 02:55:35 - train: epoch 0053, iter [02800, 05004], lr: 0.010000, loss: 1.1837
2022-10-10 02:56:11 - train: epoch 0053, iter [02900, 05004], lr: 0.010000, loss: 1.0535
2022-10-10 02:56:47 - train: epoch 0053, iter [03000, 05004], lr: 0.010000, loss: 1.0735
2022-10-10 02:57:23 - train: epoch 0053, iter [03100, 05004], lr: 0.010000, loss: 1.2468
2022-10-10 02:58:04 - train: epoch 0053, iter [03200, 05004], lr: 0.010000, loss: 1.2321
2022-10-10 02:58:40 - train: epoch 0053, iter [03300, 05004], lr: 0.010000, loss: 1.2625
2022-10-10 02:59:16 - train: epoch 0053, iter [03400, 05004], lr: 0.010000, loss: 1.0930
2022-10-10 02:59:50 - train: epoch 0053, iter [03500, 05004], lr: 0.010000, loss: 1.0780
2022-10-10 03:00:23 - train: epoch 0053, iter [03600, 05004], lr: 0.010000, loss: 1.1964
2022-10-10 03:00:58 - train: epoch 0053, iter [03700, 05004], lr: 0.010000, loss: 1.0543
2022-10-10 03:01:36 - train: epoch 0053, iter [03800, 05004], lr: 0.010000, loss: 1.3437
2022-10-10 03:02:11 - train: epoch 0053, iter [03900, 05004], lr: 0.010000, loss: 1.4141
2022-10-10 03:02:45 - train: epoch 0053, iter [04000, 05004], lr: 0.010000, loss: 1.3970
2022-10-10 03:03:19 - train: epoch 0053, iter [04100, 05004], lr: 0.010000, loss: 1.0829
2022-10-10 03:03:53 - train: epoch 0053, iter [04200, 05004], lr: 0.010000, loss: 1.2699
2022-10-10 03:04:27 - train: epoch 0053, iter [04300, 05004], lr: 0.010000, loss: 1.2026
2022-10-10 03:05:01 - train: epoch 0053, iter [04400, 05004], lr: 0.010000, loss: 1.1997
2022-10-10 03:05:36 - train: epoch 0053, iter [04500, 05004], lr: 0.010000, loss: 1.1699
2022-10-10 03:06:10 - train: epoch 0053, iter [04600, 05004], lr: 0.010000, loss: 1.0455
2022-10-10 03:06:44 - train: epoch 0053, iter [04700, 05004], lr: 0.010000, loss: 1.1162
2022-10-10 03:07:18 - train: epoch 0053, iter [04800, 05004], lr: 0.010000, loss: 1.2885
2022-10-10 03:07:53 - train: epoch 0053, iter [04900, 05004], lr: 0.010000, loss: 1.1171
2022-10-10 03:08:24 - train: epoch 0053, iter [05000, 05004], lr: 0.010000, loss: 1.0301
2022-10-10 03:08:27 - train: epoch 053, train_loss: 1.1303
2022-10-10 03:09:44 - eval: epoch: 053, acc1: 71.578%, acc5: 90.754%, test_loss: 1.1357, per_image_load_time: 1.253ms, per_image_inference_time: 0.574ms
2022-10-10 03:09:44 - until epoch: 053, best_acc1: 73.328%
2022-10-10 03:09:44 - epoch 054 lr: 0.010000
2022-10-10 03:10:25 - train: epoch 0054, iter [00100, 05004], lr: 0.010000, loss: 1.1168
2022-10-10 03:10:59 - train: epoch 0054, iter [00200, 05004], lr: 0.010000, loss: 1.0939
2022-10-10 03:11:34 - train: epoch 0054, iter [00300, 05004], lr: 0.010000, loss: 1.0383
2022-10-10 03:12:08 - train: epoch 0054, iter [00400, 05004], lr: 0.010000, loss: 1.0802
2022-10-10 03:12:42 - train: epoch 0054, iter [00500, 05004], lr: 0.010000, loss: 1.1045
2022-10-10 03:13:16 - train: epoch 0054, iter [00600, 05004], lr: 0.010000, loss: 1.0702
2022-10-10 03:13:51 - train: epoch 0054, iter [00700, 05004], lr: 0.010000, loss: 1.3494
2022-10-10 03:14:25 - train: epoch 0054, iter [00800, 05004], lr: 0.010000, loss: 1.1062
2022-10-10 03:15:00 - train: epoch 0054, iter [00900, 05004], lr: 0.010000, loss: 0.8708
2022-10-10 03:15:35 - train: epoch 0054, iter [01000, 05004], lr: 0.010000, loss: 0.9416
2022-10-10 03:16:09 - train: epoch 0054, iter [01100, 05004], lr: 0.010000, loss: 0.9833
2022-10-10 03:16:45 - train: epoch 0054, iter [01200, 05004], lr: 0.010000, loss: 1.2370
2022-10-10 03:17:21 - train: epoch 0054, iter [01300, 05004], lr: 0.010000, loss: 1.0200
2022-10-10 03:17:55 - train: epoch 0054, iter [01400, 05004], lr: 0.010000, loss: 1.1984
2022-10-10 03:18:29 - train: epoch 0054, iter [01500, 05004], lr: 0.010000, loss: 1.1502
2022-10-10 03:19:04 - train: epoch 0054, iter [01600, 05004], lr: 0.010000, loss: 0.8699
2022-10-10 03:19:39 - train: epoch 0054, iter [01700, 05004], lr: 0.010000, loss: 1.1786
2022-10-10 03:20:13 - train: epoch 0054, iter [01800, 05004], lr: 0.010000, loss: 1.1329
2022-10-10 03:20:48 - train: epoch 0054, iter [01900, 05004], lr: 0.010000, loss: 1.3358
2022-10-10 03:21:23 - train: epoch 0054, iter [02000, 05004], lr: 0.010000, loss: 1.3644
2022-10-10 03:21:58 - train: epoch 0054, iter [02100, 05004], lr: 0.010000, loss: 1.0334
2022-10-10 03:22:33 - train: epoch 0054, iter [02200, 05004], lr: 0.010000, loss: 1.1284
2022-10-10 03:23:06 - train: epoch 0054, iter [02300, 05004], lr: 0.010000, loss: 0.8229
2022-10-10 03:23:42 - train: epoch 0054, iter [02400, 05004], lr: 0.010000, loss: 1.1849
2022-10-10 03:24:15 - train: epoch 0054, iter [02500, 05004], lr: 0.010000, loss: 1.2546
2022-10-10 03:24:52 - train: epoch 0054, iter [02600, 05004], lr: 0.010000, loss: 1.0503
2022-10-10 03:25:29 - train: epoch 0054, iter [02700, 05004], lr: 0.010000, loss: 1.0398
2022-10-10 03:26:08 - train: epoch 0054, iter [02800, 05004], lr: 0.010000, loss: 1.1762
2022-10-10 03:26:43 - train: epoch 0054, iter [02900, 05004], lr: 0.010000, loss: 1.0951
2022-10-10 03:27:20 - train: epoch 0054, iter [03000, 05004], lr: 0.010000, loss: 1.2005
2022-10-10 03:28:03 - train: epoch 0054, iter [03100, 05004], lr: 0.010000, loss: 1.1842
2022-10-10 03:28:47 - train: epoch 0054, iter [03200, 05004], lr: 0.010000, loss: 1.3107
2022-10-10 03:29:37 - train: epoch 0054, iter [03300, 05004], lr: 0.010000, loss: 1.1966
2022-10-10 03:30:34 - train: epoch 0054, iter [03400, 05004], lr: 0.010000, loss: 1.1218
2022-10-10 03:31:25 - train: epoch 0054, iter [03500, 05004], lr: 0.010000, loss: 1.1202
2022-10-10 03:32:04 - train: epoch 0054, iter [03600, 05004], lr: 0.010000, loss: 1.1067
2022-10-10 03:32:45 - train: epoch 0054, iter [03700, 05004], lr: 0.010000, loss: 1.1299
2022-10-10 03:33:25 - train: epoch 0054, iter [03800, 05004], lr: 0.010000, loss: 1.1255
2022-10-10 03:35:02 - train: epoch 0054, iter [03900, 05004], lr: 0.010000, loss: 1.1814
2022-10-10 03:38:28 - train: epoch 0054, iter [04000, 05004], lr: 0.010000, loss: 1.1257
2022-10-10 03:40:08 - train: epoch 0054, iter [04100, 05004], lr: 0.010000, loss: 1.2951
2022-10-10 03:42:07 - train: epoch 0054, iter [04200, 05004], lr: 0.010000, loss: 1.1570
2022-10-10 03:44:03 - train: epoch 0054, iter [04300, 05004], lr: 0.010000, loss: 1.1523
2022-10-10 03:45:36 - train: epoch 0054, iter [04400, 05004], lr: 0.010000, loss: 1.0297
2022-10-10 03:46:14 - train: epoch 0054, iter [04500, 05004], lr: 0.010000, loss: 1.1262
2022-10-10 03:46:47 - train: epoch 0054, iter [04600, 05004], lr: 0.010000, loss: 1.2360
2022-10-10 03:47:23 - train: epoch 0054, iter [04700, 05004], lr: 0.010000, loss: 1.2207
2022-10-10 03:47:57 - train: epoch 0054, iter [04800, 05004], lr: 0.010000, loss: 1.2288
2022-10-10 03:48:31 - train: epoch 0054, iter [04900, 05004], lr: 0.010000, loss: 1.1499
2022-10-10 03:49:03 - train: epoch 0054, iter [05000, 05004], lr: 0.010000, loss: 1.2531
2022-10-10 03:49:05 - train: epoch 054, train_loss: 1.1251
2022-10-10 03:50:23 - eval: epoch: 054, acc1: 72.554%, acc5: 91.122%, test_loss: 1.0981, per_image_load_time: 0.798ms, per_image_inference_time: 0.550ms
2022-10-10 03:50:23 - until epoch: 054, best_acc1: 73.328%
2022-10-10 03:50:23 - epoch 055 lr: 0.010000
2022-10-10 03:51:03 - train: epoch 0055, iter [00100, 05004], lr: 0.010000, loss: 0.9616
2022-10-10 03:51:38 - train: epoch 0055, iter [00200, 05004], lr: 0.010000, loss: 1.0953
2022-10-10 03:52:12 - train: epoch 0055, iter [00300, 05004], lr: 0.010000, loss: 1.1402
2022-10-10 03:52:46 - train: epoch 0055, iter [00400, 05004], lr: 0.010000, loss: 0.9401
2022-10-10 03:53:20 - train: epoch 0055, iter [00500, 05004], lr: 0.010000, loss: 0.9501
2022-10-10 03:53:54 - train: epoch 0055, iter [00600, 05004], lr: 0.010000, loss: 1.2565
2022-10-10 03:54:29 - train: epoch 0055, iter [00700, 05004], lr: 0.010000, loss: 1.0233
2022-10-10 03:55:03 - train: epoch 0055, iter [00800, 05004], lr: 0.010000, loss: 1.0564
2022-10-10 03:55:36 - train: epoch 0055, iter [00900, 05004], lr: 0.010000, loss: 1.1080
2022-10-10 03:56:12 - train: epoch 0055, iter [01000, 05004], lr: 0.010000, loss: 1.1999
2022-10-10 03:56:45 - train: epoch 0055, iter [01100, 05004], lr: 0.010000, loss: 0.9857
2022-10-10 03:57:19 - train: epoch 0055, iter [01200, 05004], lr: 0.010000, loss: 1.0161
2022-10-10 03:57:54 - train: epoch 0055, iter [01300, 05004], lr: 0.010000, loss: 1.1920
2022-10-10 03:58:28 - train: epoch 0055, iter [01400, 05004], lr: 0.010000, loss: 0.9970
2022-10-10 03:59:03 - train: epoch 0055, iter [01500, 05004], lr: 0.010000, loss: 0.8305
2022-10-10 03:59:36 - train: epoch 0055, iter [01600, 05004], lr: 0.010000, loss: 1.3597
2022-10-10 04:00:11 - train: epoch 0055, iter [01700, 05004], lr: 0.010000, loss: 1.1865
2022-10-10 04:00:45 - train: epoch 0055, iter [01800, 05004], lr: 0.010000, loss: 1.1192
2022-10-10 04:01:20 - train: epoch 0055, iter [01900, 05004], lr: 0.010000, loss: 1.0855
2022-10-10 04:01:53 - train: epoch 0055, iter [02000, 05004], lr: 0.010000, loss: 1.1687
2022-10-10 04:02:29 - train: epoch 0055, iter [02100, 05004], lr: 0.010000, loss: 0.9657
2022-10-10 04:03:02 - train: epoch 0055, iter [02200, 05004], lr: 0.010000, loss: 1.3012
2022-10-10 04:03:36 - train: epoch 0055, iter [02300, 05004], lr: 0.010000, loss: 1.1084
2022-10-10 04:04:11 - train: epoch 0055, iter [02400, 05004], lr: 0.010000, loss: 0.9893
2022-10-10 04:04:48 - train: epoch 0055, iter [02500, 05004], lr: 0.010000, loss: 1.1458
2022-10-10 04:05:21 - train: epoch 0055, iter [02600, 05004], lr: 0.010000, loss: 0.8591
2022-10-10 04:05:55 - train: epoch 0055, iter [02700, 05004], lr: 0.010000, loss: 1.1087
2022-10-10 04:06:30 - train: epoch 0055, iter [02800, 05004], lr: 0.010000, loss: 1.0514
2022-10-10 04:07:04 - train: epoch 0055, iter [02900, 05004], lr: 0.010000, loss: 1.1661
2022-10-10 04:07:38 - train: epoch 0055, iter [03000, 05004], lr: 0.010000, loss: 1.0086
2022-10-10 04:08:12 - train: epoch 0055, iter [03100, 05004], lr: 0.010000, loss: 1.1487
2022-10-10 04:08:46 - train: epoch 0055, iter [03200, 05004], lr: 0.010000, loss: 1.0046
2022-10-10 04:09:21 - train: epoch 0055, iter [03300, 05004], lr: 0.010000, loss: 0.9300
2022-10-10 04:09:55 - train: epoch 0055, iter [03400, 05004], lr: 0.010000, loss: 1.1136
2022-10-10 04:10:29 - train: epoch 0055, iter [03500, 05004], lr: 0.010000, loss: 0.9778
2022-10-10 04:11:03 - train: epoch 0055, iter [03600, 05004], lr: 0.010000, loss: 1.2076
2022-10-10 04:11:39 - train: epoch 0055, iter [03700, 05004], lr: 0.010000, loss: 0.9657
2022-10-10 04:12:13 - train: epoch 0055, iter [03800, 05004], lr: 0.010000, loss: 1.1451
2022-10-10 04:12:47 - train: epoch 0055, iter [03900, 05004], lr: 0.010000, loss: 1.3131
2022-10-10 04:13:21 - train: epoch 0055, iter [04000, 05004], lr: 0.010000, loss: 1.1320
2022-10-10 04:13:57 - train: epoch 0055, iter [04100, 05004], lr: 0.010000, loss: 1.1766
2022-10-10 04:14:30 - train: epoch 0055, iter [04200, 05004], lr: 0.010000, loss: 1.0966
2022-10-10 04:15:04 - train: epoch 0055, iter [04300, 05004], lr: 0.010000, loss: 1.3134
2022-10-10 04:15:40 - train: epoch 0055, iter [04400, 05004], lr: 0.010000, loss: 1.1924
2022-10-10 04:16:43 - train: epoch 0055, iter [04500, 05004], lr: 0.010000, loss: 1.0834
2022-10-10 04:17:35 - train: epoch 0055, iter [04600, 05004], lr: 0.010000, loss: 1.1774
2022-10-10 04:18:23 - train: epoch 0055, iter [04700, 05004], lr: 0.010000, loss: 0.9458
2022-10-10 04:18:58 - train: epoch 0055, iter [04800, 05004], lr: 0.010000, loss: 1.1334
2022-10-10 04:19:34 - train: epoch 0055, iter [04900, 05004], lr: 0.010000, loss: 1.3397
2022-10-10 04:20:07 - train: epoch 0055, iter [05000, 05004], lr: 0.010000, loss: 1.1301
2022-10-10 04:20:09 - train: epoch 055, train_loss: 1.1210
2022-10-10 04:21:26 - eval: epoch: 055, acc1: 72.798%, acc5: 91.358%, test_loss: 1.0820, per_image_load_time: 1.070ms, per_image_inference_time: 0.595ms
2022-10-10 04:21:27 - until epoch: 055, best_acc1: 73.328%
2022-10-10 04:21:27 - epoch 056 lr: 0.010000
2022-10-10 04:22:07 - train: epoch 0056, iter [00100, 05004], lr: 0.010000, loss: 1.1490
2022-10-10 04:22:41 - train: epoch 0056, iter [00200, 05004], lr: 0.010000, loss: 0.9895
2022-10-10 04:23:15 - train: epoch 0056, iter [00300, 05004], lr: 0.010000, loss: 0.8642
2022-10-10 04:23:48 - train: epoch 0056, iter [00400, 05004], lr: 0.010000, loss: 1.0112
2022-10-10 04:24:24 - train: epoch 0056, iter [00500, 05004], lr: 0.010000, loss: 1.0958
2022-10-10 04:24:57 - train: epoch 0056, iter [00600, 05004], lr: 0.010000, loss: 1.1385
2022-10-10 04:25:31 - train: epoch 0056, iter [00700, 05004], lr: 0.010000, loss: 1.2789
2022-10-10 04:26:06 - train: epoch 0056, iter [00800, 05004], lr: 0.010000, loss: 1.2660
2022-10-10 04:26:39 - train: epoch 0056, iter [00900, 05004], lr: 0.010000, loss: 1.2999
2022-10-10 04:27:13 - train: epoch 0056, iter [01000, 05004], lr: 0.010000, loss: 1.0739
2022-10-10 04:27:48 - train: epoch 0056, iter [01100, 05004], lr: 0.010000, loss: 1.0351
2022-10-10 04:28:23 - train: epoch 0056, iter [01200, 05004], lr: 0.010000, loss: 1.0582
2022-10-10 04:28:57 - train: epoch 0056, iter [01300, 05004], lr: 0.010000, loss: 1.1839
2022-10-10 04:29:32 - train: epoch 0056, iter [01400, 05004], lr: 0.010000, loss: 0.9835
2022-10-10 04:30:06 - train: epoch 0056, iter [01500, 05004], lr: 0.010000, loss: 1.3433
2022-10-10 04:30:41 - train: epoch 0056, iter [01600, 05004], lr: 0.010000, loss: 0.9928
2022-10-10 04:31:16 - train: epoch 0056, iter [01700, 05004], lr: 0.010000, loss: 1.3463
2022-10-10 04:31:51 - train: epoch 0056, iter [01800, 05004], lr: 0.010000, loss: 1.2503
2022-10-10 04:32:25 - train: epoch 0056, iter [01900, 05004], lr: 0.010000, loss: 1.3237
2022-10-10 04:33:00 - train: epoch 0056, iter [02000, 05004], lr: 0.010000, loss: 1.0889
2022-10-10 04:33:34 - train: epoch 0056, iter [02100, 05004], lr: 0.010000, loss: 1.1095
2022-10-10 04:34:09 - train: epoch 0056, iter [02200, 05004], lr: 0.010000, loss: 1.0607
2022-10-10 04:34:45 - train: epoch 0056, iter [02300, 05004], lr: 0.010000, loss: 1.2061
2022-10-10 04:35:19 - train: epoch 0056, iter [02400, 05004], lr: 0.010000, loss: 1.1875
2022-10-10 04:35:55 - train: epoch 0056, iter [02500, 05004], lr: 0.010000, loss: 1.3118
2022-10-10 04:36:29 - train: epoch 0056, iter [02600, 05004], lr: 0.010000, loss: 1.0733
2022-10-10 04:37:03 - train: epoch 0056, iter [02700, 05004], lr: 0.010000, loss: 0.9510
2022-10-10 04:37:38 - train: epoch 0056, iter [02800, 05004], lr: 0.010000, loss: 1.1539
2022-10-10 04:38:12 - train: epoch 0056, iter [02900, 05004], lr: 0.010000, loss: 1.2918
2022-10-10 04:38:48 - train: epoch 0056, iter [03000, 05004], lr: 0.010000, loss: 1.1159
2022-10-10 04:39:23 - train: epoch 0056, iter [03100, 05004], lr: 0.010000, loss: 0.9743
2022-10-10 04:39:59 - train: epoch 0056, iter [03200, 05004], lr: 0.010000, loss: 1.1880
2022-10-10 04:40:36 - train: epoch 0056, iter [03300, 05004], lr: 0.010000, loss: 1.1280
2022-10-10 04:41:11 - train: epoch 0056, iter [03400, 05004], lr: 0.010000, loss: 1.0516
2022-10-10 04:41:45 - train: epoch 0056, iter [03500, 05004], lr: 0.010000, loss: 1.2330
2022-10-10 04:42:22 - train: epoch 0056, iter [03600, 05004], lr: 0.010000, loss: 1.0898
2022-10-10 04:42:56 - train: epoch 0056, iter [03700, 05004], lr: 0.010000, loss: 1.0846
2022-10-10 04:43:32 - train: epoch 0056, iter [03800, 05004], lr: 0.010000, loss: 1.0883
2022-10-10 04:44:07 - train: epoch 0056, iter [03900, 05004], lr: 0.010000, loss: 1.2743
2022-10-10 04:44:41 - train: epoch 0056, iter [04000, 05004], lr: 0.010000, loss: 1.0911
2022-10-10 04:45:15 - train: epoch 0056, iter [04100, 05004], lr: 0.010000, loss: 1.2024
2022-10-10 04:45:50 - train: epoch 0056, iter [04200, 05004], lr: 0.010000, loss: 1.1344
2022-10-10 04:46:25 - train: epoch 0056, iter [04300, 05004], lr: 0.010000, loss: 1.1546
2022-10-10 04:47:00 - train: epoch 0056, iter [04400, 05004], lr: 0.010000, loss: 1.0759
2022-10-10 04:47:35 - train: epoch 0056, iter [04500, 05004], lr: 0.010000, loss: 1.1052
2022-10-10 04:48:10 - train: epoch 0056, iter [04600, 05004], lr: 0.010000, loss: 1.1076
2022-10-10 04:48:46 - train: epoch 0056, iter [04700, 05004], lr: 0.010000, loss: 1.2835
2022-10-10 04:49:19 - train: epoch 0056, iter [04800, 05004], lr: 0.010000, loss: 1.2248
2022-10-10 04:49:54 - train: epoch 0056, iter [04900, 05004], lr: 0.010000, loss: 1.2044
2022-10-10 04:50:26 - train: epoch 0056, iter [05000, 05004], lr: 0.010000, loss: 1.3118
2022-10-10 04:50:29 - train: epoch 056, train_loss: 1.1219
2022-10-10 04:51:46 - eval: epoch: 056, acc1: 71.116%, acc5: 90.550%, test_loss: 1.1547, per_image_load_time: 0.887ms, per_image_inference_time: 0.578ms
2022-10-10 04:51:46 - until epoch: 056, best_acc1: 73.328%
2022-10-10 04:51:46 - epoch 057 lr: 0.010000
2022-10-10 04:52:28 - train: epoch 0057, iter [00100, 05004], lr: 0.010000, loss: 1.1603
2022-10-10 04:53:03 - train: epoch 0057, iter [00200, 05004], lr: 0.010000, loss: 1.1585
2022-10-10 04:53:38 - train: epoch 0057, iter [00300, 05004], lr: 0.010000, loss: 1.1208
2022-10-10 04:54:13 - train: epoch 0057, iter [00400, 05004], lr: 0.010000, loss: 1.0098
2022-10-10 04:54:49 - train: epoch 0057, iter [00500, 05004], lr: 0.010000, loss: 0.9342
2022-10-10 04:55:24 - train: epoch 0057, iter [00600, 05004], lr: 0.010000, loss: 1.2357
2022-10-10 04:55:59 - train: epoch 0057, iter [00700, 05004], lr: 0.010000, loss: 0.9189
2022-10-10 04:56:34 - train: epoch 0057, iter [00800, 05004], lr: 0.010000, loss: 1.1059
2022-10-10 04:57:09 - train: epoch 0057, iter [00900, 05004], lr: 0.010000, loss: 1.0332
2022-10-10 04:57:45 - train: epoch 0057, iter [01000, 05004], lr: 0.010000, loss: 0.8807
2022-10-10 04:58:20 - train: epoch 0057, iter [01100, 05004], lr: 0.010000, loss: 0.9896
2022-10-10 04:58:56 - train: epoch 0057, iter [01200, 05004], lr: 0.010000, loss: 1.2840
2022-10-10 04:59:30 - train: epoch 0057, iter [01300, 05004], lr: 0.010000, loss: 1.1128
2022-10-10 05:00:06 - train: epoch 0057, iter [01400, 05004], lr: 0.010000, loss: 1.2088
2022-10-10 05:00:42 - train: epoch 0057, iter [01500, 05004], lr: 0.010000, loss: 1.1857
2022-10-10 05:01:17 - train: epoch 0057, iter [01600, 05004], lr: 0.010000, loss: 1.2393
2022-10-10 05:01:52 - train: epoch 0057, iter [01700, 05004], lr: 0.010000, loss: 1.1992
2022-10-10 05:02:28 - train: epoch 0057, iter [01800, 05004], lr: 0.010000, loss: 1.0888
2022-10-10 05:03:09 - train: epoch 0057, iter [01900, 05004], lr: 0.010000, loss: 1.0834
2022-10-10 05:03:58 - train: epoch 0057, iter [02000, 05004], lr: 0.010000, loss: 1.2730
2022-10-10 05:04:50 - train: epoch 0057, iter [02100, 05004], lr: 0.010000, loss: 1.0327
2022-10-10 05:05:40 - train: epoch 0057, iter [02200, 05004], lr: 0.010000, loss: 0.8665
2022-10-10 05:06:41 - train: epoch 0057, iter [02300, 05004], lr: 0.010000, loss: 1.1366
2022-10-10 05:07:16 - train: epoch 0057, iter [02400, 05004], lr: 0.010000, loss: 1.0262
2022-10-10 05:08:08 - train: epoch 0057, iter [02500, 05004], lr: 0.010000, loss: 1.1990
2022-10-10 05:08:41 - train: epoch 0057, iter [02600, 05004], lr: 0.010000, loss: 1.0863
2022-10-10 05:09:17 - train: epoch 0057, iter [02700, 05004], lr: 0.010000, loss: 0.7667
2022-10-10 05:09:55 - train: epoch 0057, iter [02800, 05004], lr: 0.010000, loss: 0.9189
2022-10-10 05:10:31 - train: epoch 0057, iter [02900, 05004], lr: 0.010000, loss: 1.0655
2022-10-10 05:11:07 - train: epoch 0057, iter [03000, 05004], lr: 0.010000, loss: 1.1716
2022-10-10 05:11:42 - train: epoch 0057, iter [03100, 05004], lr: 0.010000, loss: 1.2062
2022-10-10 05:12:21 - train: epoch 0057, iter [03200, 05004], lr: 0.010000, loss: 1.2136
2022-10-10 05:13:03 - train: epoch 0057, iter [03300, 05004], lr: 0.010000, loss: 1.1815
2022-10-10 05:13:55 - train: epoch 0057, iter [03400, 05004], lr: 0.010000, loss: 1.0762
2022-10-10 05:15:23 - train: epoch 0057, iter [03500, 05004], lr: 0.010000, loss: 1.2065
2022-10-10 05:16:43 - train: epoch 0057, iter [03600, 05004], lr: 0.010000, loss: 1.1703
2022-10-10 05:18:07 - train: epoch 0057, iter [03700, 05004], lr: 0.010000, loss: 1.0879
2022-10-10 05:19:08 - train: epoch 0057, iter [03800, 05004], lr: 0.010000, loss: 1.0634
2022-10-10 05:20:18 - train: epoch 0057, iter [03900, 05004], lr: 0.010000, loss: 1.2710
2022-10-10 05:21:23 - train: epoch 0057, iter [04000, 05004], lr: 0.010000, loss: 1.0809
2022-10-10 05:22:36 - train: epoch 0057, iter [04100, 05004], lr: 0.010000, loss: 1.0328
2022-10-10 05:23:33 - train: epoch 0057, iter [04200, 05004], lr: 0.010000, loss: 1.1580
2022-10-10 05:25:00 - train: epoch 0057, iter [04300, 05004], lr: 0.010000, loss: 1.0069
2022-10-10 05:26:02 - train: epoch 0057, iter [04400, 05004], lr: 0.010000, loss: 1.3381
2022-10-10 05:27:35 - train: epoch 0057, iter [04500, 05004], lr: 0.010000, loss: 1.0048
2022-10-10 05:28:40 - train: epoch 0057, iter [04600, 05004], lr: 0.010000, loss: 1.0690
2022-10-10 05:30:15 - train: epoch 0057, iter [04700, 05004], lr: 0.010000, loss: 1.0969
2022-10-10 05:31:43 - train: epoch 0057, iter [04800, 05004], lr: 0.010000, loss: 1.3717
2022-10-10 05:33:42 - train: epoch 0057, iter [04900, 05004], lr: 0.010000, loss: 1.2733
2022-10-10 05:35:51 - train: epoch 0057, iter [05000, 05004], lr: 0.010000, loss: 1.1945
2022-10-10 05:35:55 - train: epoch 057, train_loss: 1.1190
2022-10-10 05:37:13 - eval: epoch: 057, acc1: 71.850%, acc5: 91.112%, test_loss: 1.1193, per_image_load_time: 2.372ms, per_image_inference_time: 0.554ms
2022-10-10 05:37:13 - until epoch: 057, best_acc1: 73.328%
2022-10-10 05:37:13 - epoch 058 lr: 0.010000
2022-10-10 05:37:54 - train: epoch 0058, iter [00100, 05004], lr: 0.010000, loss: 1.2177
2022-10-10 05:38:28 - train: epoch 0058, iter [00200, 05004], lr: 0.010000, loss: 1.0202
2022-10-10 05:39:02 - train: epoch 0058, iter [00300, 05004], lr: 0.010000, loss: 1.2807
2022-10-10 05:39:36 - train: epoch 0058, iter [00400, 05004], lr: 0.010000, loss: 0.9611
2022-10-10 05:40:10 - train: epoch 0058, iter [00500, 05004], lr: 0.010000, loss: 1.0900
2022-10-10 05:40:43 - train: epoch 0058, iter [00600, 05004], lr: 0.010000, loss: 1.2740
2022-10-10 05:41:17 - train: epoch 0058, iter [00700, 05004], lr: 0.010000, loss: 1.0488
2022-10-10 05:41:52 - train: epoch 0058, iter [00800, 05004], lr: 0.010000, loss: 1.0835
2022-10-10 05:42:25 - train: epoch 0058, iter [00900, 05004], lr: 0.010000, loss: 1.0442
2022-10-10 05:43:00 - train: epoch 0058, iter [01000, 05004], lr: 0.010000, loss: 1.0543
2022-10-10 05:43:34 - train: epoch 0058, iter [01100, 05004], lr: 0.010000, loss: 1.0904
2022-10-10 05:44:08 - train: epoch 0058, iter [01200, 05004], lr: 0.010000, loss: 1.0973
2022-10-10 05:44:42 - train: epoch 0058, iter [01300, 05004], lr: 0.010000, loss: 1.1237
2022-10-10 05:45:16 - train: epoch 0058, iter [01400, 05004], lr: 0.010000, loss: 1.2133
2022-10-10 05:45:50 - train: epoch 0058, iter [01500, 05004], lr: 0.010000, loss: 1.0385
2022-10-10 05:46:24 - train: epoch 0058, iter [01600, 05004], lr: 0.010000, loss: 1.1288
2022-10-10 05:46:58 - train: epoch 0058, iter [01700, 05004], lr: 0.010000, loss: 1.2011
2022-10-10 05:47:32 - train: epoch 0058, iter [01800, 05004], lr: 0.010000, loss: 1.3897
2022-10-10 05:48:06 - train: epoch 0058, iter [01900, 05004], lr: 0.010000, loss: 1.0720
2022-10-10 05:48:40 - train: epoch 0058, iter [02000, 05004], lr: 0.010000, loss: 1.2064
2022-10-10 05:49:14 - train: epoch 0058, iter [02100, 05004], lr: 0.010000, loss: 1.1787
2022-10-10 05:49:48 - train: epoch 0058, iter [02200, 05004], lr: 0.010000, loss: 1.0248
2022-10-10 05:50:39 - train: epoch 0058, iter [02300, 05004], lr: 0.010000, loss: 1.1578
2022-10-10 05:51:11 - train: epoch 0058, iter [02400, 05004], lr: 0.010000, loss: 1.1126
2022-10-10 05:51:48 - train: epoch 0058, iter [02500, 05004], lr: 0.010000, loss: 1.3261
2022-10-10 05:52:35 - train: epoch 0058, iter [02600, 05004], lr: 0.010000, loss: 1.0530
2022-10-10 05:53:18 - train: epoch 0058, iter [02700, 05004], lr: 0.010000, loss: 1.3376
2022-10-10 05:53:58 - train: epoch 0058, iter [02800, 05004], lr: 0.010000, loss: 1.1544
2022-10-10 05:54:57 - train: epoch 0058, iter [02900, 05004], lr: 0.010000, loss: 1.0986
2022-10-10 05:55:38 - train: epoch 0058, iter [03000, 05004], lr: 0.010000, loss: 1.2101
2022-10-10 05:56:33 - train: epoch 0058, iter [03100, 05004], lr: 0.010000, loss: 1.0162
2022-10-10 05:57:27 - train: epoch 0058, iter [03200, 05004], lr: 0.010000, loss: 0.9978
2022-10-10 05:58:20 - train: epoch 0058, iter [03300, 05004], lr: 0.010000, loss: 1.0889
2022-10-10 05:59:06 - train: epoch 0058, iter [03400, 05004], lr: 0.010000, loss: 1.1137
2022-10-10 05:59:51 - train: epoch 0058, iter [03500, 05004], lr: 0.010000, loss: 1.0617
2022-10-10 06:00:41 - train: epoch 0058, iter [03600, 05004], lr: 0.010000, loss: 1.0444
2022-10-10 06:01:15 - train: epoch 0058, iter [03700, 05004], lr: 0.010000, loss: 1.0772
2022-10-10 06:01:51 - train: epoch 0058, iter [03800, 05004], lr: 0.010000, loss: 1.1631
2022-10-10 06:02:25 - train: epoch 0058, iter [03900, 05004], lr: 0.010000, loss: 1.0679
2022-10-10 06:03:00 - train: epoch 0058, iter [04000, 05004], lr: 0.010000, loss: 1.0735
2022-10-10 06:03:35 - train: epoch 0058, iter [04100, 05004], lr: 0.010000, loss: 1.0762
2022-10-10 06:04:11 - train: epoch 0058, iter [04200, 05004], lr: 0.010000, loss: 1.1534
2022-10-10 06:04:44 - train: epoch 0058, iter [04300, 05004], lr: 0.010000, loss: 1.2324
2022-10-10 06:05:20 - train: epoch 0058, iter [04400, 05004], lr: 0.010000, loss: 1.0486
2022-10-10 06:05:57 - train: epoch 0058, iter [04500, 05004], lr: 0.010000, loss: 1.1147
2022-10-10 06:06:33 - train: epoch 0058, iter [04600, 05004], lr: 0.010000, loss: 1.0652
2022-10-10 06:07:07 - train: epoch 0058, iter [04700, 05004], lr: 0.010000, loss: 1.1757
2022-10-10 06:07:43 - train: epoch 0058, iter [04800, 05004], lr: 0.010000, loss: 0.9576
2022-10-10 06:08:21 - train: epoch 0058, iter [04900, 05004], lr: 0.010000, loss: 1.0655
2022-10-10 06:08:56 - train: epoch 0058, iter [05000, 05004], lr: 0.010000, loss: 0.9165
2022-10-10 06:08:59 - train: epoch 058, train_loss: 1.1169
2022-10-10 06:10:17 - eval: epoch: 058, acc1: 72.618%, acc5: 91.326%, test_loss: 1.0954, per_image_load_time: 1.120ms, per_image_inference_time: 0.588ms
2022-10-10 06:10:17 - until epoch: 058, best_acc1: 73.328%
2022-10-10 06:10:17 - epoch 059 lr: 0.010000
2022-10-10 06:10:59 - train: epoch 0059, iter [00100, 05004], lr: 0.010000, loss: 1.1837
2022-10-10 06:11:34 - train: epoch 0059, iter [00200, 05004], lr: 0.010000, loss: 1.0137
2022-10-10 06:12:07 - train: epoch 0059, iter [00300, 05004], lr: 0.010000, loss: 0.7912
2022-10-10 06:12:43 - train: epoch 0059, iter [00400, 05004], lr: 0.010000, loss: 1.0615
2022-10-10 06:13:17 - train: epoch 0059, iter [00500, 05004], lr: 0.010000, loss: 1.0944
2022-10-10 06:13:51 - train: epoch 0059, iter [00600, 05004], lr: 0.010000, loss: 1.1161
2022-10-10 06:14:25 - train: epoch 0059, iter [00700, 05004], lr: 0.010000, loss: 1.1012
2022-10-10 06:15:01 - train: epoch 0059, iter [00800, 05004], lr: 0.010000, loss: 1.0692
2022-10-10 06:15:35 - train: epoch 0059, iter [00900, 05004], lr: 0.010000, loss: 1.2035
2022-10-10 06:16:10 - train: epoch 0059, iter [01000, 05004], lr: 0.010000, loss: 1.2064
2022-10-10 06:16:45 - train: epoch 0059, iter [01100, 05004], lr: 0.010000, loss: 1.3476
2022-10-10 06:17:21 - train: epoch 0059, iter [01200, 05004], lr: 0.010000, loss: 1.0981
2022-10-10 06:17:55 - train: epoch 0059, iter [01300, 05004], lr: 0.010000, loss: 1.2593
2022-10-10 06:18:31 - train: epoch 0059, iter [01400, 05004], lr: 0.010000, loss: 1.3374
2022-10-10 06:19:07 - train: epoch 0059, iter [01500, 05004], lr: 0.010000, loss: 1.0609
2022-10-10 06:19:45 - train: epoch 0059, iter [01600, 05004], lr: 0.010000, loss: 1.1151
2022-10-10 06:20:18 - train: epoch 0059, iter [01700, 05004], lr: 0.010000, loss: 0.9789
2022-10-10 06:20:51 - train: epoch 0059, iter [01800, 05004], lr: 0.010000, loss: 1.1987
2022-10-10 06:21:26 - train: epoch 0059, iter [01900, 05004], lr: 0.010000, loss: 1.1768
2022-10-10 06:22:01 - train: epoch 0059, iter [02000, 05004], lr: 0.010000, loss: 0.9235
2022-10-10 06:22:35 - train: epoch 0059, iter [02100, 05004], lr: 0.010000, loss: 1.1221
2022-10-10 06:23:10 - train: epoch 0059, iter [02200, 05004], lr: 0.010000, loss: 1.0628
2022-10-10 06:23:45 - train: epoch 0059, iter [02300, 05004], lr: 0.010000, loss: 1.1922
2022-10-10 06:24:20 - train: epoch 0059, iter [02400, 05004], lr: 0.010000, loss: 1.2806
2022-10-10 06:24:56 - train: epoch 0059, iter [02500, 05004], lr: 0.010000, loss: 1.3396
2022-10-10 06:25:29 - train: epoch 0059, iter [02600, 05004], lr: 0.010000, loss: 1.1392
2022-10-10 06:26:09 - train: epoch 0059, iter [02700, 05004], lr: 0.010000, loss: 1.0828
2022-10-10 06:26:45 - train: epoch 0059, iter [02800, 05004], lr: 0.010000, loss: 1.3970
2022-10-10 06:27:21 - train: epoch 0059, iter [02900, 05004], lr: 0.010000, loss: 0.9794
2022-10-10 06:27:56 - train: epoch 0059, iter [03000, 05004], lr: 0.010000, loss: 1.2951
2022-10-10 06:28:31 - train: epoch 0059, iter [03100, 05004], lr: 0.010000, loss: 0.9384
2022-10-10 06:29:07 - train: epoch 0059, iter [03200, 05004], lr: 0.010000, loss: 1.3463
2022-10-10 06:29:49 - train: epoch 0059, iter [03300, 05004], lr: 0.010000, loss: 1.1107
2022-10-10 06:30:26 - train: epoch 0059, iter [03400, 05004], lr: 0.010000, loss: 1.2341
2022-10-10 06:31:15 - train: epoch 0059, iter [03500, 05004], lr: 0.010000, loss: 0.9727
2022-10-10 06:31:53 - train: epoch 0059, iter [03600, 05004], lr: 0.010000, loss: 1.2644
2022-10-10 06:32:29 - train: epoch 0059, iter [03700, 05004], lr: 0.010000, loss: 1.1189
2022-10-10 06:33:02 - train: epoch 0059, iter [03800, 05004], lr: 0.010000, loss: 1.1722
2022-10-10 06:33:38 - train: epoch 0059, iter [03900, 05004], lr: 0.010000, loss: 1.0563
2022-10-10 06:34:13 - train: epoch 0059, iter [04000, 05004], lr: 0.010000, loss: 1.3482
2022-10-10 06:34:47 - train: epoch 0059, iter [04100, 05004], lr: 0.010000, loss: 1.0638
2022-10-10 06:35:22 - train: epoch 0059, iter [04200, 05004], lr: 0.010000, loss: 1.0947
2022-10-10 06:35:57 - train: epoch 0059, iter [04300, 05004], lr: 0.010000, loss: 1.0585
2022-10-10 06:36:32 - train: epoch 0059, iter [04400, 05004], lr: 0.010000, loss: 1.2145
2022-10-10 06:37:07 - train: epoch 0059, iter [04500, 05004], lr: 0.010000, loss: 1.1542
2022-10-10 06:37:41 - train: epoch 0059, iter [04600, 05004], lr: 0.010000, loss: 1.3326
2022-10-10 06:38:16 - train: epoch 0059, iter [04700, 05004], lr: 0.010000, loss: 1.1735
2022-10-10 06:38:50 - train: epoch 0059, iter [04800, 05004], lr: 0.010000, loss: 1.0836
2022-10-10 06:39:25 - train: epoch 0059, iter [04900, 05004], lr: 0.010000, loss: 1.1915
2022-10-10 06:39:57 - train: epoch 0059, iter [05000, 05004], lr: 0.010000, loss: 1.4122
2022-10-10 06:40:00 - train: epoch 059, train_loss: 1.1118
2022-10-10 06:41:18 - eval: epoch: 059, acc1: 72.442%, acc5: 91.236%, test_loss: 1.0994, per_image_load_time: 1.263ms, per_image_inference_time: 0.633ms
2022-10-10 06:41:18 - until epoch: 059, best_acc1: 73.328%
2022-10-10 06:41:18 - epoch 060 lr: 0.010000
2022-10-10 06:42:00 - train: epoch 0060, iter [00100, 05004], lr: 0.010000, loss: 0.9260
2022-10-10 06:42:34 - train: epoch 0060, iter [00200, 05004], lr: 0.010000, loss: 0.9874
2022-10-10 06:43:08 - train: epoch 0060, iter [00300, 05004], lr: 0.010000, loss: 1.0062
2022-10-10 06:43:42 - train: epoch 0060, iter [00400, 05004], lr: 0.010000, loss: 1.2624
2022-10-10 06:44:17 - train: epoch 0060, iter [00500, 05004], lr: 0.010000, loss: 1.3061
2022-10-10 06:44:52 - train: epoch 0060, iter [00600, 05004], lr: 0.010000, loss: 1.1709
2022-10-10 06:45:25 - train: epoch 0060, iter [00700, 05004], lr: 0.010000, loss: 1.2024
2022-10-10 06:46:00 - train: epoch 0060, iter [00800, 05004], lr: 0.010000, loss: 1.2161
2022-10-10 06:46:35 - train: epoch 0060, iter [00900, 05004], lr: 0.010000, loss: 1.0435
2022-10-10 06:47:09 - train: epoch 0060, iter [01000, 05004], lr: 0.010000, loss: 0.8883
2022-10-10 06:47:44 - train: epoch 0060, iter [01100, 05004], lr: 0.010000, loss: 1.0831
2022-10-10 06:48:19 - train: epoch 0060, iter [01200, 05004], lr: 0.010000, loss: 1.0321
2022-10-10 06:48:53 - train: epoch 0060, iter [01300, 05004], lr: 0.010000, loss: 1.1929
2022-10-10 06:49:28 - train: epoch 0060, iter [01400, 05004], lr: 0.010000, loss: 1.0790
2022-10-10 06:50:02 - train: epoch 0060, iter [01500, 05004], lr: 0.010000, loss: 1.0778
2022-10-10 06:50:37 - train: epoch 0060, iter [01600, 05004], lr: 0.010000, loss: 0.9656
2022-10-10 06:51:12 - train: epoch 0060, iter [01700, 05004], lr: 0.010000, loss: 1.0788
2022-10-10 06:51:47 - train: epoch 0060, iter [01800, 05004], lr: 0.010000, loss: 1.1507
2022-10-10 06:52:22 - train: epoch 0060, iter [01900, 05004], lr: 0.010000, loss: 1.2243
2022-10-10 06:52:55 - train: epoch 0060, iter [02000, 05004], lr: 0.010000, loss: 0.9875
2022-10-10 06:53:31 - train: epoch 0060, iter [02100, 05004], lr: 0.010000, loss: 1.2595
2022-10-10 06:54:06 - train: epoch 0060, iter [02200, 05004], lr: 0.010000, loss: 1.1834
2022-10-10 06:54:39 - train: epoch 0060, iter [02300, 05004], lr: 0.010000, loss: 1.0413
2022-10-10 06:55:14 - train: epoch 0060, iter [02400, 05004], lr: 0.010000, loss: 0.9034
2022-10-10 06:55:48 - train: epoch 0060, iter [02500, 05004], lr: 0.010000, loss: 1.0657
2022-10-10 06:56:25 - train: epoch 0060, iter [02600, 05004], lr: 0.010000, loss: 1.1610
2022-10-10 06:56:58 - train: epoch 0060, iter [02700, 05004], lr: 0.010000, loss: 1.0546
2022-10-10 06:57:32 - train: epoch 0060, iter [02800, 05004], lr: 0.010000, loss: 1.1812
2022-10-10 06:58:06 - train: epoch 0060, iter [02900, 05004], lr: 0.010000, loss: 1.1644
2022-10-10 06:58:40 - train: epoch 0060, iter [03000, 05004], lr: 0.010000, loss: 1.0597
2022-10-10 06:59:14 - train: epoch 0060, iter [03100, 05004], lr: 0.010000, loss: 1.0721
2022-10-10 06:59:50 - train: epoch 0060, iter [03200, 05004], lr: 0.010000, loss: 1.0291
2022-10-10 07:00:23 - train: epoch 0060, iter [03300, 05004], lr: 0.010000, loss: 0.9589
2022-10-10 07:00:57 - train: epoch 0060, iter [03400, 05004], lr: 0.010000, loss: 1.2056
2022-10-10 07:01:32 - train: epoch 0060, iter [03500, 05004], lr: 0.010000, loss: 1.2134
2022-10-10 07:02:06 - train: epoch 0060, iter [03600, 05004], lr: 0.010000, loss: 1.0239
2022-10-10 07:02:42 - train: epoch 0060, iter [03700, 05004], lr: 0.010000, loss: 1.1276
2022-10-10 07:03:17 - train: epoch 0060, iter [03800, 05004], lr: 0.010000, loss: 1.1939
2022-10-10 07:03:51 - train: epoch 0060, iter [03900, 05004], lr: 0.010000, loss: 1.2091
2022-10-10 07:04:25 - train: epoch 0060, iter [04000, 05004], lr: 0.010000, loss: 1.0451
2022-10-10 07:05:00 - train: epoch 0060, iter [04100, 05004], lr: 0.010000, loss: 1.3488
2022-10-10 07:05:48 - train: epoch 0060, iter [04200, 05004], lr: 0.010000, loss: 1.1472
2022-10-10 07:06:37 - train: epoch 0060, iter [04300, 05004], lr: 0.010000, loss: 1.0916
2022-10-10 07:07:17 - train: epoch 0060, iter [04400, 05004], lr: 0.010000, loss: 1.2617
2022-10-10 07:08:19 - train: epoch 0060, iter [04500, 05004], lr: 0.010000, loss: 1.2011
2022-10-10 07:09:44 - train: epoch 0060, iter [04600, 05004], lr: 0.010000, loss: 1.0031
2022-10-10 07:11:17 - train: epoch 0060, iter [04700, 05004], lr: 0.010000, loss: 1.2110
2022-10-10 07:11:53 - train: epoch 0060, iter [04800, 05004], lr: 0.010000, loss: 1.0307
2022-10-10 07:12:30 - train: epoch 0060, iter [04900, 05004], lr: 0.010000, loss: 1.0891
2022-10-10 07:13:03 - train: epoch 0060, iter [05000, 05004], lr: 0.010000, loss: 1.1989
2022-10-10 07:13:06 - train: epoch 060, train_loss: 1.1068
2022-10-10 07:14:23 - eval: epoch: 060, acc1: 72.102%, acc5: 90.982%, test_loss: 1.1222, per_image_load_time: 2.088ms, per_image_inference_time: 0.569ms
2022-10-10 07:14:24 - until epoch: 060, best_acc1: 73.328%
2022-10-10 07:14:24 - epoch 061 lr: 0.001000
2022-10-10 07:15:04 - train: epoch 0061, iter [00100, 05004], lr: 0.001000, loss: 0.8083
2022-10-10 07:15:39 - train: epoch 0061, iter [00200, 05004], lr: 0.001000, loss: 0.9175
2022-10-10 07:16:14 - train: epoch 0061, iter [00300, 05004], lr: 0.001000, loss: 0.8197
2022-10-10 07:16:48 - train: epoch 0061, iter [00400, 05004], lr: 0.001000, loss: 1.1377
2022-10-10 07:17:22 - train: epoch 0061, iter [00500, 05004], lr: 0.001000, loss: 0.9439
2022-10-10 07:17:57 - train: epoch 0061, iter [00600, 05004], lr: 0.001000, loss: 0.8063
2022-10-10 07:18:31 - train: epoch 0061, iter [00700, 05004], lr: 0.001000, loss: 0.9203
2022-10-10 07:19:06 - train: epoch 0061, iter [00800, 05004], lr: 0.001000, loss: 0.7778
2022-10-10 07:19:40 - train: epoch 0061, iter [00900, 05004], lr: 0.001000, loss: 0.9518
2022-10-10 07:20:14 - train: epoch 0061, iter [01000, 05004], lr: 0.001000, loss: 0.9051
2022-10-10 07:20:48 - train: epoch 0061, iter [01100, 05004], lr: 0.001000, loss: 1.0600
2022-10-10 07:21:23 - train: epoch 0061, iter [01200, 05004], lr: 0.001000, loss: 0.8726
2022-10-10 07:21:57 - train: epoch 0061, iter [01300, 05004], lr: 0.001000, loss: 0.9336
2022-10-10 07:22:32 - train: epoch 0061, iter [01400, 05004], lr: 0.001000, loss: 0.9297
2022-10-10 07:23:07 - train: epoch 0061, iter [01500, 05004], lr: 0.001000, loss: 0.9121
2022-10-10 07:23:49 - train: epoch 0061, iter [01600, 05004], lr: 0.001000, loss: 0.7800
2022-10-10 07:24:23 - train: epoch 0061, iter [01700, 05004], lr: 0.001000, loss: 0.9688
2022-10-10 07:24:58 - train: epoch 0061, iter [01800, 05004], lr: 0.001000, loss: 1.0285
2022-10-10 07:25:30 - train: epoch 0061, iter [01900, 05004], lr: 0.001000, loss: 0.9226
2022-10-10 07:26:05 - train: epoch 0061, iter [02000, 05004], lr: 0.001000, loss: 0.8493
2022-10-10 07:26:42 - train: epoch 0061, iter [02100, 05004], lr: 0.001000, loss: 1.0729
2022-10-10 07:27:16 - train: epoch 0061, iter [02200, 05004], lr: 0.001000, loss: 0.9098
2022-10-10 07:28:00 - train: epoch 0061, iter [02300, 05004], lr: 0.001000, loss: 0.9938
2022-10-10 07:28:44 - train: epoch 0061, iter [02400, 05004], lr: 0.001000, loss: 0.8224
2022-10-10 07:29:52 - train: epoch 0061, iter [02500, 05004], lr: 0.001000, loss: 0.8462
2022-10-10 07:30:48 - train: epoch 0061, iter [02600, 05004], lr: 0.001000, loss: 0.9404
2022-10-10 07:31:52 - train: epoch 0061, iter [02700, 05004], lr: 0.001000, loss: 0.9371
2022-10-10 07:32:45 - train: epoch 0061, iter [02800, 05004], lr: 0.001000, loss: 1.0388
2022-10-10 07:33:36 - train: epoch 0061, iter [02900, 05004], lr: 0.001000, loss: 1.0542
2022-10-10 07:34:19 - train: epoch 0061, iter [03000, 05004], lr: 0.001000, loss: 0.8482
2022-10-10 07:34:59 - train: epoch 0061, iter [03100, 05004], lr: 0.001000, loss: 0.8943
2022-10-10 07:35:38 - train: epoch 0061, iter [03200, 05004], lr: 0.001000, loss: 0.8372
2022-10-10 07:36:16 - train: epoch 0061, iter [03300, 05004], lr: 0.001000, loss: 0.8680
2022-10-10 07:36:55 - train: epoch 0061, iter [03400, 05004], lr: 0.001000, loss: 0.8486
2022-10-10 07:37:31 - train: epoch 0061, iter [03500, 05004], lr: 0.001000, loss: 0.8800
2022-10-10 07:38:06 - train: epoch 0061, iter [03600, 05004], lr: 0.001000, loss: 0.9494
2022-10-10 07:38:41 - train: epoch 0061, iter [03700, 05004], lr: 0.001000, loss: 0.8089
2022-10-10 07:39:16 - train: epoch 0061, iter [03800, 05004], lr: 0.001000, loss: 0.8845
2022-10-10 07:39:52 - train: epoch 0061, iter [03900, 05004], lr: 0.001000, loss: 1.1532
2022-10-10 07:40:28 - train: epoch 0061, iter [04000, 05004], lr: 0.001000, loss: 1.0172
2022-10-10 07:41:03 - train: epoch 0061, iter [04100, 05004], lr: 0.001000, loss: 1.0783
2022-10-10 07:41:38 - train: epoch 0061, iter [04200, 05004], lr: 0.001000, loss: 0.8911
2022-10-10 07:42:13 - train: epoch 0061, iter [04300, 05004], lr: 0.001000, loss: 0.8275
2022-10-10 07:42:51 - train: epoch 0061, iter [04400, 05004], lr: 0.001000, loss: 0.8272
2022-10-10 07:43:27 - train: epoch 0061, iter [04500, 05004], lr: 0.001000, loss: 0.7876
2022-10-10 07:44:04 - train: epoch 0061, iter [04600, 05004], lr: 0.001000, loss: 0.9607
2022-10-10 07:44:41 - train: epoch 0061, iter [04700, 05004], lr: 0.001000, loss: 0.8692
2022-10-10 07:45:18 - train: epoch 0061, iter [04800, 05004], lr: 0.001000, loss: 0.9009
2022-10-10 07:45:54 - train: epoch 0061, iter [04900, 05004], lr: 0.001000, loss: 0.9867
2022-10-10 07:46:28 - train: epoch 0061, iter [05000, 05004], lr: 0.001000, loss: 0.9088
2022-10-10 07:46:31 - train: epoch 061, train_loss: 0.9350
2022-10-10 07:47:50 - eval: epoch: 061, acc1: 75.806%, acc5: 92.906%, test_loss: 0.9557, per_image_load_time: 2.352ms, per_image_inference_time: 0.604ms
2022-10-10 07:47:50 - until epoch: 061, best_acc1: 75.806%
2022-10-10 07:47:50 - epoch 062 lr: 0.001000
2022-10-10 07:48:31 - train: epoch 0062, iter [00100, 05004], lr: 0.001000, loss: 1.0771
2022-10-10 07:49:07 - train: epoch 0062, iter [00200, 05004], lr: 0.001000, loss: 1.1800
2022-10-10 07:49:40 - train: epoch 0062, iter [00300, 05004], lr: 0.001000, loss: 0.8431
2022-10-10 07:50:15 - train: epoch 0062, iter [00400, 05004], lr: 0.001000, loss: 0.7561
2022-10-10 07:50:49 - train: epoch 0062, iter [00500, 05004], lr: 0.001000, loss: 0.8666
2022-10-10 07:51:25 - train: epoch 0062, iter [00600, 05004], lr: 0.001000, loss: 0.8442
2022-10-10 07:51:59 - train: epoch 0062, iter [00700, 05004], lr: 0.001000, loss: 0.8525
2022-10-10 07:52:34 - train: epoch 0062, iter [00800, 05004], lr: 0.001000, loss: 0.9647
2022-10-10 07:53:08 - train: epoch 0062, iter [00900, 05004], lr: 0.001000, loss: 0.8326
2022-10-10 07:53:43 - train: epoch 0062, iter [01000, 05004], lr: 0.001000, loss: 0.9946
2022-10-10 07:54:17 - train: epoch 0062, iter [01100, 05004], lr: 0.001000, loss: 0.8703
2022-10-10 07:54:53 - train: epoch 0062, iter [01200, 05004], lr: 0.001000, loss: 0.8878
2022-10-10 07:55:27 - train: epoch 0062, iter [01300, 05004], lr: 0.001000, loss: 0.8036
2022-10-10 07:56:01 - train: epoch 0062, iter [01400, 05004], lr: 0.001000, loss: 0.8726
2022-10-10 07:56:36 - train: epoch 0062, iter [01500, 05004], lr: 0.001000, loss: 0.9454
2022-10-10 07:57:11 - train: epoch 0062, iter [01600, 05004], lr: 0.001000, loss: 1.0853
2022-10-10 07:57:47 - train: epoch 0062, iter [01700, 05004], lr: 0.001000, loss: 0.8873
2022-10-10 07:58:21 - train: epoch 0062, iter [01800, 05004], lr: 0.001000, loss: 0.7260
2022-10-10 07:58:57 - train: epoch 0062, iter [01900, 05004], lr: 0.001000, loss: 0.7286
2022-10-10 07:59:33 - train: epoch 0062, iter [02000, 05004], lr: 0.001000, loss: 0.9554
2022-10-10 08:00:11 - train: epoch 0062, iter [02100, 05004], lr: 0.001000, loss: 0.9633
2022-10-10 08:00:45 - train: epoch 0062, iter [02200, 05004], lr: 0.001000, loss: 0.8065
2022-10-10 08:01:20 - train: epoch 0062, iter [02300, 05004], lr: 0.001000, loss: 1.0220
2022-10-10 08:01:57 - train: epoch 0062, iter [02400, 05004], lr: 0.001000, loss: 0.9294
2022-10-10 08:02:33 - train: epoch 0062, iter [02500, 05004], lr: 0.001000, loss: 0.9662
2022-10-10 08:03:07 - train: epoch 0062, iter [02600, 05004], lr: 0.001000, loss: 0.8158
2022-10-10 08:03:56 - train: epoch 0062, iter [02700, 05004], lr: 0.001000, loss: 0.7839
2022-10-10 08:04:32 - train: epoch 0062, iter [02800, 05004], lr: 0.001000, loss: 0.9491
2022-10-10 08:05:05 - train: epoch 0062, iter [02900, 05004], lr: 0.001000, loss: 0.9440
2022-10-10 08:05:39 - train: epoch 0062, iter [03000, 05004], lr: 0.001000, loss: 0.9071
2022-10-10 08:06:20 - train: epoch 0062, iter [03100, 05004], lr: 0.001000, loss: 0.8208
2022-10-10 08:07:20 - train: epoch 0062, iter [03200, 05004], lr: 0.001000, loss: 0.8593
2022-10-10 08:08:21 - train: epoch 0062, iter [03300, 05004], lr: 0.001000, loss: 0.8585
2022-10-10 08:09:28 - train: epoch 0062, iter [03400, 05004], lr: 0.001000, loss: 0.8343
2022-10-10 08:10:22 - train: epoch 0062, iter [03500, 05004], lr: 0.001000, loss: 0.9349
2022-10-10 08:11:12 - train: epoch 0062, iter [03600, 05004], lr: 0.001000, loss: 0.9680
2022-10-10 08:11:56 - train: epoch 0062, iter [03700, 05004], lr: 0.001000, loss: 0.7566
2022-10-10 08:13:01 - train: epoch 0062, iter [03800, 05004], lr: 0.001000, loss: 0.8370
2022-10-10 08:13:51 - train: epoch 0062, iter [03900, 05004], lr: 0.001000, loss: 0.9334
2022-10-10 08:14:37 - train: epoch 0062, iter [04000, 05004], lr: 0.001000, loss: 0.8373
2022-10-10 08:15:52 - train: epoch 0062, iter [04100, 05004], lr: 0.001000, loss: 0.7218
2022-10-10 08:16:47 - train: epoch 0062, iter [04200, 05004], lr: 0.001000, loss: 0.9002
2022-10-10 08:17:50 - train: epoch 0062, iter [04300, 05004], lr: 0.001000, loss: 0.8212
2022-10-10 08:19:07 - train: epoch 0062, iter [04400, 05004], lr: 0.001000, loss: 0.8063
2022-10-10 08:19:47 - train: epoch 0062, iter [04500, 05004], lr: 0.001000, loss: 0.8302
2022-10-10 08:20:51 - train: epoch 0062, iter [04600, 05004], lr: 0.001000, loss: 0.8244
2022-10-10 08:21:41 - train: epoch 0062, iter [04700, 05004], lr: 0.001000, loss: 0.9073
2022-10-10 08:23:08 - train: epoch 0062, iter [04800, 05004], lr: 0.001000, loss: 0.9925
2022-10-10 08:24:04 - train: epoch 0062, iter [04900, 05004], lr: 0.001000, loss: 0.9568
2022-10-10 08:25:05 - train: epoch 0062, iter [05000, 05004], lr: 0.001000, loss: 1.0717
2022-10-10 08:25:07 - train: epoch 062, train_loss: 0.8844
2022-10-10 08:26:25 - eval: epoch: 062, acc1: 76.132%, acc5: 93.066%, test_loss: 0.9408, per_image_load_time: 2.110ms, per_image_inference_time: 0.571ms
2022-10-10 08:26:26 - until epoch: 062, best_acc1: 76.132%
2022-10-10 08:26:26 - epoch 063 lr: 0.001000
2022-10-10 08:27:06 - train: epoch 0063, iter [00100, 05004], lr: 0.001000, loss: 0.9575
2022-10-10 08:27:40 - train: epoch 0063, iter [00200, 05004], lr: 0.001000, loss: 0.8743
2022-10-10 08:28:15 - train: epoch 0063, iter [00300, 05004], lr: 0.001000, loss: 0.8531
2022-10-10 08:28:50 - train: epoch 0063, iter [00400, 05004], lr: 0.001000, loss: 0.8712
2022-10-10 08:29:25 - train: epoch 0063, iter [00500, 05004], lr: 0.001000, loss: 0.9000
2022-10-10 08:29:59 - train: epoch 0063, iter [00600, 05004], lr: 0.001000, loss: 0.8515
2022-10-10 08:30:33 - train: epoch 0063, iter [00700, 05004], lr: 0.001000, loss: 0.9688
2022-10-10 08:31:08 - train: epoch 0063, iter [00800, 05004], lr: 0.001000, loss: 0.7564
2022-10-10 08:31:43 - train: epoch 0063, iter [00900, 05004], lr: 0.001000, loss: 0.9178
2022-10-10 08:32:17 - train: epoch 0063, iter [01000, 05004], lr: 0.001000, loss: 1.0237
2022-10-10 08:32:51 - train: epoch 0063, iter [01100, 05004], lr: 0.001000, loss: 0.8077
2022-10-10 08:33:26 - train: epoch 0063, iter [01200, 05004], lr: 0.001000, loss: 1.0015
2022-10-10 08:34:01 - train: epoch 0063, iter [01300, 05004], lr: 0.001000, loss: 0.7442
2022-10-10 08:34:36 - train: epoch 0063, iter [01400, 05004], lr: 0.001000, loss: 1.0857
2022-10-10 08:35:09 - train: epoch 0063, iter [01500, 05004], lr: 0.001000, loss: 0.8858
2022-10-10 08:35:44 - train: epoch 0063, iter [01600, 05004], lr: 0.001000, loss: 0.9292
2022-10-10 08:36:19 - train: epoch 0063, iter [01700, 05004], lr: 0.001000, loss: 0.7290
2022-10-10 08:36:53 - train: epoch 0063, iter [01800, 05004], lr: 0.001000, loss: 0.8772
2022-10-10 08:37:26 - train: epoch 0063, iter [01900, 05004], lr: 0.001000, loss: 0.9413
2022-10-10 08:38:01 - train: epoch 0063, iter [02000, 05004], lr: 0.001000, loss: 0.9509
2022-10-10 08:38:35 - train: epoch 0063, iter [02100, 05004], lr: 0.001000, loss: 0.7972
2022-10-10 08:39:09 - train: epoch 0063, iter [02200, 05004], lr: 0.001000, loss: 1.0267
2022-10-10 08:39:43 - train: epoch 0063, iter [02300, 05004], lr: 0.001000, loss: 1.0323
2022-10-10 08:40:18 - train: epoch 0063, iter [02400, 05004], lr: 0.001000, loss: 0.6967
2022-10-10 08:40:52 - train: epoch 0063, iter [02500, 05004], lr: 0.001000, loss: 0.7911
2022-10-10 08:41:26 - train: epoch 0063, iter [02600, 05004], lr: 0.001000, loss: 0.9532
2022-10-10 08:42:02 - train: epoch 0063, iter [02700, 05004], lr: 0.001000, loss: 1.0466
2022-10-10 08:42:37 - train: epoch 0063, iter [02800, 05004], lr: 0.001000, loss: 0.9220
2022-10-10 08:43:11 - train: epoch 0063, iter [02900, 05004], lr: 0.001000, loss: 0.9926
2022-10-10 08:43:46 - train: epoch 0063, iter [03000, 05004], lr: 0.001000, loss: 1.1099
2022-10-10 08:44:20 - train: epoch 0063, iter [03100, 05004], lr: 0.001000, loss: 0.9313
2022-10-10 08:44:55 - train: epoch 0063, iter [03200, 05004], lr: 0.001000, loss: 0.9687
2022-10-10 08:45:30 - train: epoch 0063, iter [03300, 05004], lr: 0.001000, loss: 0.7716
2022-10-10 08:46:05 - train: epoch 0063, iter [03400, 05004], lr: 0.001000, loss: 0.6871
2022-10-10 08:46:41 - train: epoch 0063, iter [03500, 05004], lr: 0.001000, loss: 0.9387
2022-10-10 08:47:26 - train: epoch 0063, iter [03600, 05004], lr: 0.001000, loss: 0.8436
2022-10-10 08:47:59 - train: epoch 0063, iter [03700, 05004], lr: 0.001000, loss: 0.7996
2022-10-10 08:48:35 - train: epoch 0063, iter [03800, 05004], lr: 0.001000, loss: 0.9275
2022-10-10 08:49:09 - train: epoch 0063, iter [03900, 05004], lr: 0.001000, loss: 0.8561
2022-10-10 08:49:44 - train: epoch 0063, iter [04000, 05004], lr: 0.001000, loss: 0.7222
2022-10-10 08:50:19 - train: epoch 0063, iter [04100, 05004], lr: 0.001000, loss: 0.9029
2022-10-10 08:50:54 - train: epoch 0063, iter [04200, 05004], lr: 0.001000, loss: 0.9096
2022-10-10 08:51:29 - train: epoch 0063, iter [04300, 05004], lr: 0.001000, loss: 0.9309
2022-10-10 08:52:05 - train: epoch 0063, iter [04400, 05004], lr: 0.001000, loss: 0.8286
2022-10-10 08:52:39 - train: epoch 0063, iter [04500, 05004], lr: 0.001000, loss: 0.8762
2022-10-10 08:53:14 - train: epoch 0063, iter [04600, 05004], lr: 0.001000, loss: 0.7000
2022-10-10 08:53:48 - train: epoch 0063, iter [04700, 05004], lr: 0.001000, loss: 0.6724
2022-10-10 08:54:23 - train: epoch 0063, iter [04800, 05004], lr: 0.001000, loss: 0.6972
2022-10-10 08:54:58 - train: epoch 0063, iter [04900, 05004], lr: 0.001000, loss: 0.9169
2022-10-10 08:55:30 - train: epoch 0063, iter [05000, 05004], lr: 0.001000, loss: 0.8477
2022-10-10 08:55:32 - train: epoch 063, train_loss: 0.8648
2022-10-10 08:56:51 - eval: epoch: 063, acc1: 76.220%, acc5: 93.110%, test_loss: 0.9366, per_image_load_time: 2.045ms, per_image_inference_time: 0.602ms
2022-10-10 08:56:51 - until epoch: 063, best_acc1: 76.220%
2022-10-10 08:56:51 - epoch 064 lr: 0.001000
2022-10-10 08:57:32 - train: epoch 0064, iter [00100, 05004], lr: 0.001000, loss: 0.7708
2022-10-10 08:58:06 - train: epoch 0064, iter [00200, 05004], lr: 0.001000, loss: 0.9016
2022-10-10 08:58:41 - train: epoch 0064, iter [00300, 05004], lr: 0.001000, loss: 0.7868
2022-10-10 08:59:16 - train: epoch 0064, iter [00400, 05004], lr: 0.001000, loss: 0.9675
2022-10-10 08:59:50 - train: epoch 0064, iter [00500, 05004], lr: 0.001000, loss: 0.9373
2022-10-10 09:00:24 - train: epoch 0064, iter [00600, 05004], lr: 0.001000, loss: 0.9245
2022-10-10 09:01:02 - train: epoch 0064, iter [00700, 05004], lr: 0.001000, loss: 0.9442
2022-10-10 09:01:36 - train: epoch 0064, iter [00800, 05004], lr: 0.001000, loss: 0.8155
2022-10-10 09:02:10 - train: epoch 0064, iter [00900, 05004], lr: 0.001000, loss: 0.9392
2022-10-10 09:02:50 - train: epoch 0064, iter [01000, 05004], lr: 0.001000, loss: 0.7767
2022-10-10 09:03:22 - train: epoch 0064, iter [01100, 05004], lr: 0.001000, loss: 0.9436
2022-10-10 09:03:57 - train: epoch 0064, iter [01200, 05004], lr: 0.001000, loss: 0.6626
2022-10-10 09:04:32 - train: epoch 0064, iter [01300, 05004], lr: 0.001000, loss: 0.9549
2022-10-10 09:05:05 - train: epoch 0064, iter [01400, 05004], lr: 0.001000, loss: 0.9160
2022-10-10 09:05:41 - train: epoch 0064, iter [01500, 05004], lr: 0.001000, loss: 0.9109
2022-10-10 09:06:15 - train: epoch 0064, iter [01600, 05004], lr: 0.001000, loss: 0.8045
2022-10-10 09:06:50 - train: epoch 0064, iter [01700, 05004], lr: 0.001000, loss: 0.7613
2022-10-10 09:07:25 - train: epoch 0064, iter [01800, 05004], lr: 0.001000, loss: 0.7142
2022-10-10 09:08:18 - train: epoch 0064, iter [01900, 05004], lr: 0.001000, loss: 0.7714
2022-10-10 09:09:13 - train: epoch 0064, iter [02000, 05004], lr: 0.001000, loss: 0.8645
2022-10-10 09:10:03 - train: epoch 0064, iter [02100, 05004], lr: 0.001000, loss: 0.8741
2022-10-10 09:11:08 - train: epoch 0064, iter [02200, 05004], lr: 0.001000, loss: 0.8153
2022-10-10 09:12:16 - train: epoch 0064, iter [02300, 05004], lr: 0.001000, loss: 0.9010
2022-10-10 09:13:25 - train: epoch 0064, iter [02400, 05004], lr: 0.001000, loss: 0.7912
2022-10-10 09:14:12 - train: epoch 0064, iter [02500, 05004], lr: 0.001000, loss: 0.9411
2022-10-10 09:15:03 - train: epoch 0064, iter [02600, 05004], lr: 0.001000, loss: 0.8224
2022-10-10 09:15:42 - train: epoch 0064, iter [02700, 05004], lr: 0.001000, loss: 0.7712
2022-10-10 09:16:28 - train: epoch 0064, iter [02800, 05004], lr: 0.001000, loss: 0.8042
2022-10-10 09:17:03 - train: epoch 0064, iter [02900, 05004], lr: 0.001000, loss: 0.8538
2022-10-10 09:17:42 - train: epoch 0064, iter [03000, 05004], lr: 0.001000, loss: 0.9052
2022-10-10 09:18:29 - train: epoch 0064, iter [03100, 05004], lr: 0.001000, loss: 0.8832
2022-10-10 09:19:09 - train: epoch 0064, iter [03200, 05004], lr: 0.001000, loss: 0.9542
2022-10-10 09:19:44 - train: epoch 0064, iter [03300, 05004], lr: 0.001000, loss: 0.8327
2022-10-10 09:20:19 - train: epoch 0064, iter [03400, 05004], lr: 0.001000, loss: 0.9367
2022-10-10 09:20:53 - train: epoch 0064, iter [03500, 05004], lr: 0.001000, loss: 0.7159
2022-10-10 09:21:28 - train: epoch 0064, iter [03600, 05004], lr: 0.001000, loss: 0.7131
2022-10-10 09:22:04 - train: epoch 0064, iter [03700, 05004], lr: 0.001000, loss: 0.6867
2022-10-10 09:22:38 - train: epoch 0064, iter [03800, 05004], lr: 0.001000, loss: 0.9135
2022-10-10 09:23:12 - train: epoch 0064, iter [03900, 05004], lr: 0.001000, loss: 0.8124
2022-10-10 09:23:46 - train: epoch 0064, iter [04000, 05004], lr: 0.001000, loss: 0.7721
2022-10-10 09:24:21 - train: epoch 0064, iter [04100, 05004], lr: 0.001000, loss: 0.8806
2022-10-10 09:24:58 - train: epoch 0064, iter [04200, 05004], lr: 0.001000, loss: 0.7307
2022-10-10 09:25:34 - train: epoch 0064, iter [04300, 05004], lr: 0.001000, loss: 0.9811
2022-10-10 09:26:09 - train: epoch 0064, iter [04400, 05004], lr: 0.001000, loss: 0.7459
2022-10-10 09:26:46 - train: epoch 0064, iter [04500, 05004], lr: 0.001000, loss: 0.8182
2022-10-10 09:27:25 - train: epoch 0064, iter [04600, 05004], lr: 0.001000, loss: 1.0284
2022-10-10 09:28:01 - train: epoch 0064, iter [04700, 05004], lr: 0.001000, loss: 1.0848
2022-10-10 09:28:36 - train: epoch 0064, iter [04800, 05004], lr: 0.001000, loss: 0.8947
2022-10-10 09:29:09 - train: epoch 0064, iter [04900, 05004], lr: 0.001000, loss: 0.9504
2022-10-10 09:29:50 - train: epoch 0064, iter [05000, 05004], lr: 0.001000, loss: 0.7479
2022-10-10 09:29:52 - train: epoch 064, train_loss: 0.8496
2022-10-10 09:31:10 - eval: epoch: 064, acc1: 76.384%, acc5: 93.184%, test_loss: 0.9347, per_image_load_time: 2.095ms, per_image_inference_time: 0.565ms
2022-10-10 09:31:10 - until epoch: 064, best_acc1: 76.384%
2022-10-10 09:31:10 - epoch 065 lr: 0.001000
2022-10-10 09:31:50 - train: epoch 0065, iter [00100, 05004], lr: 0.001000, loss: 0.8570
2022-10-10 09:32:25 - train: epoch 0065, iter [00200, 05004], lr: 0.001000, loss: 0.8132
2022-10-10 09:33:04 - train: epoch 0065, iter [00300, 05004], lr: 0.001000, loss: 0.8523
2022-10-10 09:33:38 - train: epoch 0065, iter [00400, 05004], lr: 0.001000, loss: 0.9550
2022-10-10 09:34:12 - train: epoch 0065, iter [00500, 05004], lr: 0.001000, loss: 0.9089
2022-10-10 09:34:46 - train: epoch 0065, iter [00600, 05004], lr: 0.001000, loss: 0.9525
2022-10-10 09:35:20 - train: epoch 0065, iter [00700, 05004], lr: 0.001000, loss: 0.8009
2022-10-10 09:35:54 - train: epoch 0065, iter [00800, 05004], lr: 0.001000, loss: 0.7108
2022-10-10 09:36:29 - train: epoch 0065, iter [00900, 05004], lr: 0.001000, loss: 0.9002
2022-10-10 09:37:04 - train: epoch 0065, iter [01000, 05004], lr: 0.001000, loss: 0.8503
2022-10-10 09:37:38 - train: epoch 0065, iter [01100, 05004], lr: 0.001000, loss: 0.7878
2022-10-10 09:38:14 - train: epoch 0065, iter [01200, 05004], lr: 0.001000, loss: 0.9249
2022-10-10 09:38:49 - train: epoch 0065, iter [01300, 05004], lr: 0.001000, loss: 0.7812
2022-10-10 09:39:25 - train: epoch 0065, iter [01400, 05004], lr: 0.001000, loss: 0.6666
2022-10-10 09:39:58 - train: epoch 0065, iter [01500, 05004], lr: 0.001000, loss: 0.7989
2022-10-10 09:40:33 - train: epoch 0065, iter [01600, 05004], lr: 0.001000, loss: 0.8661
2022-10-10 09:41:06 - train: epoch 0065, iter [01700, 05004], lr: 0.001000, loss: 0.7095
2022-10-10 09:41:44 - train: epoch 0065, iter [01800, 05004], lr: 0.001000, loss: 0.8258
2022-10-10 09:42:26 - train: epoch 0065, iter [01900, 05004], lr: 0.001000, loss: 0.6759
2022-10-10 09:43:00 - train: epoch 0065, iter [02000, 05004], lr: 0.001000, loss: 0.8656
2022-10-10 09:43:36 - train: epoch 0065, iter [02100, 05004], lr: 0.001000, loss: 0.8522
2022-10-10 09:44:14 - train: epoch 0065, iter [02200, 05004], lr: 0.001000, loss: 0.8590
2022-10-10 09:44:47 - train: epoch 0065, iter [02300, 05004], lr: 0.001000, loss: 0.8728
2022-10-10 09:45:21 - train: epoch 0065, iter [02400, 05004], lr: 0.001000, loss: 0.6975
2022-10-10 09:45:56 - train: epoch 0065, iter [02500, 05004], lr: 0.001000, loss: 0.7361
2022-10-10 09:46:30 - train: epoch 0065, iter [02600, 05004], lr: 0.001000, loss: 0.9546
2022-10-10 09:47:05 - train: epoch 0065, iter [02700, 05004], lr: 0.001000, loss: 0.8223
2022-10-10 09:47:40 - train: epoch 0065, iter [02800, 05004], lr: 0.001000, loss: 0.7546
2022-10-10 09:48:17 - train: epoch 0065, iter [02900, 05004], lr: 0.001000, loss: 0.7241
2022-10-10 09:48:51 - train: epoch 0065, iter [03000, 05004], lr: 0.001000, loss: 0.6182
2022-10-10 09:49:35 - train: epoch 0065, iter [03100, 05004], lr: 0.001000, loss: 0.8696
2022-10-10 09:50:12 - train: epoch 0065, iter [03200, 05004], lr: 0.001000, loss: 0.9534
2022-10-10 09:51:06 - train: epoch 0065, iter [03300, 05004], lr: 0.001000, loss: 0.7808
2022-10-10 09:51:37 - train: epoch 0065, iter [03400, 05004], lr: 0.001000, loss: 0.7161
2022-10-10 09:52:13 - train: epoch 0065, iter [03500, 05004], lr: 0.001000, loss: 0.9731
2022-10-10 09:52:46 - train: epoch 0065, iter [03600, 05004], lr: 0.001000, loss: 0.8598
2022-10-10 09:53:24 - train: epoch 0065, iter [03700, 05004], lr: 0.001000, loss: 0.7403
2022-10-10 09:53:58 - train: epoch 0065, iter [03800, 05004], lr: 0.001000, loss: 0.8605
2022-10-10 09:54:34 - train: epoch 0065, iter [03900, 05004], lr: 0.001000, loss: 0.8528
2022-10-10 09:55:15 - train: epoch 0065, iter [04000, 05004], lr: 0.001000, loss: 0.9054
2022-10-10 09:56:51 - train: epoch 0065, iter [04100, 05004], lr: 0.001000, loss: 0.7239
2022-10-10 09:58:54 - train: epoch 0065, iter [04200, 05004], lr: 0.001000, loss: 0.9220
2022-10-10 10:00:19 - train: epoch 0065, iter [04300, 05004], lr: 0.001000, loss: 0.7507
2022-10-10 10:01:46 - train: epoch 0065, iter [04400, 05004], lr: 0.001000, loss: 0.7891
2022-10-10 10:03:18 - train: epoch 0065, iter [04500, 05004], lr: 0.001000, loss: 0.8960
2022-10-10 10:05:01 - train: epoch 0065, iter [04600, 05004], lr: 0.001000, loss: 0.7992
2022-10-10 10:06:44 - train: epoch 0065, iter [04700, 05004], lr: 0.001000, loss: 0.8637
2022-10-10 10:09:02 - train: epoch 0065, iter [04800, 05004], lr: 0.001000, loss: 0.8564
2022-10-10 10:11:20 - train: epoch 0065, iter [04900, 05004], lr: 0.001000, loss: 0.8505
2022-10-10 10:13:26 - train: epoch 0065, iter [05000, 05004], lr: 0.001000, loss: 0.7875
2022-10-10 10:13:29 - train: epoch 065, train_loss: 0.8382
2022-10-10 10:14:47 - eval: epoch: 065, acc1: 76.304%, acc5: 93.184%, test_loss: 0.9309, per_image_load_time: 2.234ms, per_image_inference_time: 0.583ms
2022-10-10 10:14:47 - until epoch: 065, best_acc1: 76.384%
2022-10-10 10:14:47 - epoch 066 lr: 0.001000
2022-10-10 10:15:27 - train: epoch 0066, iter [00100, 05004], lr: 0.001000, loss: 0.6387
2022-10-10 10:16:01 - train: epoch 0066, iter [00200, 05004], lr: 0.001000, loss: 0.8846
2022-10-10 10:16:36 - train: epoch 0066, iter [00300, 05004], lr: 0.001000, loss: 0.8189
2022-10-10 10:17:10 - train: epoch 0066, iter [00400, 05004], lr: 0.001000, loss: 0.6678
2022-10-10 10:17:43 - train: epoch 0066, iter [00500, 05004], lr: 0.001000, loss: 0.8793
2022-10-10 10:18:17 - train: epoch 0066, iter [00600, 05004], lr: 0.001000, loss: 0.7517
2022-10-10 10:18:51 - train: epoch 0066, iter [00700, 05004], lr: 0.001000, loss: 0.8826
2022-10-10 10:19:25 - train: epoch 0066, iter [00800, 05004], lr: 0.001000, loss: 0.9168
2022-10-10 10:20:02 - train: epoch 0066, iter [00900, 05004], lr: 0.001000, loss: 0.8047
2022-10-10 10:20:38 - train: epoch 0066, iter [01000, 05004], lr: 0.001000, loss: 0.9079
2022-10-10 10:21:11 - train: epoch 0066, iter [01100, 05004], lr: 0.001000, loss: 0.9878
2022-10-10 10:21:45 - train: epoch 0066, iter [01200, 05004], lr: 0.001000, loss: 0.9950
2022-10-10 10:22:19 - train: epoch 0066, iter [01300, 05004], lr: 0.001000, loss: 0.7335
2022-10-10 10:22:56 - train: epoch 0066, iter [01400, 05004], lr: 0.001000, loss: 0.9005
2022-10-10 10:23:32 - train: epoch 0066, iter [01500, 05004], lr: 0.001000, loss: 0.8806
2022-10-10 10:24:15 - train: epoch 0066, iter [01600, 05004], lr: 0.001000, loss: 1.0079
2022-10-10 10:25:20 - train: epoch 0066, iter [01700, 05004], lr: 0.001000, loss: 0.7748
2022-10-10 10:26:12 - train: epoch 0066, iter [01800, 05004], lr: 0.001000, loss: 0.8641
2022-10-10 10:26:57 - train: epoch 0066, iter [01900, 05004], lr: 0.001000, loss: 0.7945
2022-10-10 10:27:32 - train: epoch 0066, iter [02000, 05004], lr: 0.001000, loss: 0.8597
2022-10-10 10:28:06 - train: epoch 0066, iter [02100, 05004], lr: 0.001000, loss: 0.7541
2022-10-10 10:29:10 - train: epoch 0066, iter [02200, 05004], lr: 0.001000, loss: 0.7870
2022-10-10 10:30:09 - train: epoch 0066, iter [02300, 05004], lr: 0.001000, loss: 0.7948
2022-10-10 10:31:11 - train: epoch 0066, iter [02400, 05004], lr: 0.001000, loss: 0.8438
2022-10-10 10:32:15 - train: epoch 0066, iter [02500, 05004], lr: 0.001000, loss: 0.9029
2022-10-10 10:33:28 - train: epoch 0066, iter [02600, 05004], lr: 0.001000, loss: 0.8841
2022-10-10 10:34:30 - train: epoch 0066, iter [02700, 05004], lr: 0.001000, loss: 0.8650
2022-10-10 10:35:57 - train: epoch 0066, iter [02800, 05004], lr: 0.001000, loss: 0.7792
2022-10-10 10:36:55 - train: epoch 0066, iter [02900, 05004], lr: 0.001000, loss: 0.8953
2022-10-10 10:37:30 - train: epoch 0066, iter [03000, 05004], lr: 0.001000, loss: 0.8703
2022-10-10 10:38:05 - train: epoch 0066, iter [03100, 05004], lr: 0.001000, loss: 1.0297
2022-10-10 10:38:41 - train: epoch 0066, iter [03200, 05004], lr: 0.001000, loss: 0.7633
2022-10-10 10:39:16 - train: epoch 0066, iter [03300, 05004], lr: 0.001000, loss: 0.8551
2022-10-10 10:39:52 - train: epoch 0066, iter [03400, 05004], lr: 0.001000, loss: 0.8619
2022-10-10 10:40:28 - train: epoch 0066, iter [03500, 05004], lr: 0.001000, loss: 0.9153
2022-10-10 10:41:03 - train: epoch 0066, iter [03600, 05004], lr: 0.001000, loss: 0.9420
2022-10-10 10:41:39 - train: epoch 0066, iter [03700, 05004], lr: 0.001000, loss: 0.7575
2022-10-10 10:42:15 - train: epoch 0066, iter [03800, 05004], lr: 0.001000, loss: 0.8088
2022-10-10 10:42:50 - train: epoch 0066, iter [03900, 05004], lr: 0.001000, loss: 0.6554
2022-10-10 10:43:24 - train: epoch 0066, iter [04000, 05004], lr: 0.001000, loss: 0.9361
2022-10-10 10:44:01 - train: epoch 0066, iter [04100, 05004], lr: 0.001000, loss: 0.8389
2022-10-10 10:44:36 - train: epoch 0066, iter [04200, 05004], lr: 0.001000, loss: 0.6619
2022-10-10 10:45:11 - train: epoch 0066, iter [04300, 05004], lr: 0.001000, loss: 0.8592
2022-10-10 10:46:01 - train: epoch 0066, iter [04400, 05004], lr: 0.001000, loss: 0.9900
2022-10-10 10:46:36 - train: epoch 0066, iter [04500, 05004], lr: 0.001000, loss: 0.9313
2022-10-10 10:47:34 - train: epoch 0066, iter [04600, 05004], lr: 0.001000, loss: 0.8925
2022-10-10 10:49:04 - train: epoch 0066, iter [04700, 05004], lr: 0.001000, loss: 0.6727
2022-10-10 10:50:17 - train: epoch 0066, iter [04800, 05004], lr: 0.001000, loss: 0.8325
2022-10-10 10:51:38 - train: epoch 0066, iter [04900, 05004], lr: 0.001000, loss: 0.9057
2022-10-10 10:52:33 - train: epoch 0066, iter [05000, 05004], lr: 0.001000, loss: 0.8384
2022-10-10 10:52:36 - train: epoch 066, train_loss: 0.8297
2022-10-10 10:53:54 - eval: epoch: 066, acc1: 76.350%, acc5: 93.262%, test_loss: 0.9294, per_image_load_time: 2.269ms, per_image_inference_time: 0.581ms
2022-10-10 10:53:55 - until epoch: 066, best_acc1: 76.384%
2022-10-10 10:53:55 - epoch 067 lr: 0.001000
2022-10-10 10:54:35 - train: epoch 0067, iter [00100, 05004], lr: 0.001000, loss: 0.7520
2022-10-10 10:55:09 - train: epoch 0067, iter [00200, 05004], lr: 0.001000, loss: 0.8311
2022-10-10 10:55:44 - train: epoch 0067, iter [00300, 05004], lr: 0.001000, loss: 0.7365
2022-10-10 10:56:18 - train: epoch 0067, iter [00400, 05004], lr: 0.001000, loss: 0.7899
2022-10-10 10:56:51 - train: epoch 0067, iter [00500, 05004], lr: 0.001000, loss: 0.9169
2022-10-10 10:57:27 - train: epoch 0067, iter [00600, 05004], lr: 0.001000, loss: 0.8058
2022-10-10 10:58:01 - train: epoch 0067, iter [00700, 05004], lr: 0.001000, loss: 0.8374
2022-10-10 10:58:36 - train: epoch 0067, iter [00800, 05004], lr: 0.001000, loss: 0.7969
2022-10-10 10:59:11 - train: epoch 0067, iter [00900, 05004], lr: 0.001000, loss: 0.9836
2022-10-10 10:59:45 - train: epoch 0067, iter [01000, 05004], lr: 0.001000, loss: 0.9771
2022-10-10 11:00:24 - train: epoch 0067, iter [01100, 05004], lr: 0.001000, loss: 0.7082
2022-10-10 11:00:58 - train: epoch 0067, iter [01200, 05004], lr: 0.001000, loss: 0.8129
2022-10-10 11:01:32 - train: epoch 0067, iter [01300, 05004], lr: 0.001000, loss: 0.8676
2022-10-10 11:02:06 - train: epoch 0067, iter [01400, 05004], lr: 0.001000, loss: 0.8523
2022-10-10 11:02:40 - train: epoch 0067, iter [01500, 05004], lr: 0.001000, loss: 0.7918
2022-10-10 11:03:15 - train: epoch 0067, iter [01600, 05004], lr: 0.001000, loss: 0.7528
2022-10-10 11:03:49 - train: epoch 0067, iter [01700, 05004], lr: 0.001000, loss: 0.8030
2022-10-10 11:04:23 - train: epoch 0067, iter [01800, 05004], lr: 0.001000, loss: 1.0499
2022-10-10 11:05:19 - train: epoch 0067, iter [01900, 05004], lr: 0.001000, loss: 1.0510
2022-10-10 11:06:16 - train: epoch 0067, iter [02000, 05004], lr: 0.001000, loss: 1.0218
2022-10-10 11:07:39 - train: epoch 0067, iter [02100, 05004], lr: 0.001000, loss: 0.8610
2022-10-10 11:08:26 - train: epoch 0067, iter [02200, 05004], lr: 0.001000, loss: 0.8655
2022-10-10 11:09:00 - train: epoch 0067, iter [02300, 05004], lr: 0.001000, loss: 0.8542
2022-10-10 11:09:34 - train: epoch 0067, iter [02400, 05004], lr: 0.001000, loss: 0.7582
2022-10-10 11:10:08 - train: epoch 0067, iter [02500, 05004], lr: 0.001000, loss: 0.7935
2022-10-10 11:10:42 - train: epoch 0067, iter [02600, 05004], lr: 0.001000, loss: 0.8747
2022-10-10 11:11:18 - train: epoch 0067, iter [02700, 05004], lr: 0.001000, loss: 0.7551
2022-10-10 11:12:21 - train: epoch 0067, iter [02800, 05004], lr: 0.001000, loss: 0.8023
2022-10-10 11:14:19 - train: epoch 0067, iter [02900, 05004], lr: 0.001000, loss: 0.8876
2022-10-10 11:15:56 - train: epoch 0067, iter [03000, 05004], lr: 0.001000, loss: 1.0488
2022-10-10 11:17:16 - train: epoch 0067, iter [03100, 05004], lr: 0.001000, loss: 0.8349
2022-10-10 11:18:39 - train: epoch 0067, iter [03200, 05004], lr: 0.001000, loss: 0.8461
2022-10-10 11:19:13 - train: epoch 0067, iter [03300, 05004], lr: 0.001000, loss: 0.7538
2022-10-10 11:19:47 - train: epoch 0067, iter [03400, 05004], lr: 0.001000, loss: 0.9237
2022-10-10 11:20:23 - train: epoch 0067, iter [03500, 05004], lr: 0.001000, loss: 0.6322
2022-10-10 11:20:59 - train: epoch 0067, iter [03600, 05004], lr: 0.001000, loss: 0.8139
2022-10-10 11:21:34 - train: epoch 0067, iter [03700, 05004], lr: 0.001000, loss: 0.8738
2022-10-10 11:22:10 - train: epoch 0067, iter [03800, 05004], lr: 0.001000, loss: 0.7993
2022-10-10 11:22:45 - train: epoch 0067, iter [03900, 05004], lr: 0.001000, loss: 0.9232
2022-10-10 11:23:18 - train: epoch 0067, iter [04000, 05004], lr: 0.001000, loss: 0.9881
2022-10-10 11:23:53 - train: epoch 0067, iter [04100, 05004], lr: 0.001000, loss: 0.8920
2022-10-10 11:24:28 - train: epoch 0067, iter [04200, 05004], lr: 0.001000, loss: 0.8778
2022-10-10 11:25:02 - train: epoch 0067, iter [04300, 05004], lr: 0.001000, loss: 0.9090
2022-10-10 11:25:38 - train: epoch 0067, iter [04400, 05004], lr: 0.001000, loss: 0.6735
2022-10-10 11:26:14 - train: epoch 0067, iter [04500, 05004], lr: 0.001000, loss: 0.8654
2022-10-10 11:26:49 - train: epoch 0067, iter [04600, 05004], lr: 0.001000, loss: 0.7607
2022-10-10 11:27:24 - train: epoch 0067, iter [04700, 05004], lr: 0.001000, loss: 0.8810
2022-10-10 11:27:59 - train: epoch 0067, iter [04800, 05004], lr: 0.001000, loss: 0.8599
2022-10-10 11:28:34 - train: epoch 0067, iter [04900, 05004], lr: 0.001000, loss: 0.8184
2022-10-10 11:29:07 - train: epoch 0067, iter [05000, 05004], lr: 0.001000, loss: 0.8111
2022-10-10 11:29:09 - train: epoch 067, train_loss: 0.8202
2022-10-10 11:30:28 - eval: epoch: 067, acc1: 76.430%, acc5: 93.228%, test_loss: 0.9344, per_image_load_time: 2.296ms, per_image_inference_time: 0.576ms
2022-10-10 11:30:28 - until epoch: 067, best_acc1: 76.430%
2022-10-10 11:30:28 - epoch 068 lr: 0.001000
2022-10-10 11:31:09 - train: epoch 0068, iter [00100, 05004], lr: 0.001000, loss: 0.9504
2022-10-10 11:31:44 - train: epoch 0068, iter [00200, 05004], lr: 0.001000, loss: 0.8858
2022-10-10 11:32:18 - train: epoch 0068, iter [00300, 05004], lr: 0.001000, loss: 0.8015
2022-10-10 11:32:53 - train: epoch 0068, iter [00400, 05004], lr: 0.001000, loss: 0.8677
2022-10-10 11:33:27 - train: epoch 0068, iter [00500, 05004], lr: 0.001000, loss: 0.9810
2022-10-10 11:34:02 - train: epoch 0068, iter [00600, 05004], lr: 0.001000, loss: 0.8862
2022-10-10 11:34:35 - train: epoch 0068, iter [00700, 05004], lr: 0.001000, loss: 0.7791
2022-10-10 11:35:12 - train: epoch 0068, iter [00800, 05004], lr: 0.001000, loss: 0.7562
2022-10-10 11:35:47 - train: epoch 0068, iter [00900, 05004], lr: 0.001000, loss: 0.6884
2022-10-10 11:36:20 - train: epoch 0068, iter [01000, 05004], lr: 0.001000, loss: 0.6932
2022-10-10 11:36:56 - train: epoch 0068, iter [01100, 05004], lr: 0.001000, loss: 0.9688
2022-10-10 11:37:29 - train: epoch 0068, iter [01200, 05004], lr: 0.001000, loss: 0.6551
2022-10-10 11:38:03 - train: epoch 0068, iter [01300, 05004], lr: 0.001000, loss: 0.8534
2022-10-10 11:38:37 - train: epoch 0068, iter [01400, 05004], lr: 0.001000, loss: 0.7095
2022-10-10 11:39:12 - train: epoch 0068, iter [01500, 05004], lr: 0.001000, loss: 0.8489
2022-10-10 11:39:47 - train: epoch 0068, iter [01600, 05004], lr: 0.001000, loss: 0.9622
2022-10-10 11:40:21 - train: epoch 0068, iter [01700, 05004], lr: 0.001000, loss: 0.7225
2022-10-10 11:40:55 - train: epoch 0068, iter [01800, 05004], lr: 0.001000, loss: 0.7795
2022-10-10 11:41:28 - train: epoch 0068, iter [01900, 05004], lr: 0.001000, loss: 0.6047
2022-10-10 11:42:02 - train: epoch 0068, iter [02000, 05004], lr: 0.001000, loss: 0.8173
2022-10-10 11:42:36 - train: epoch 0068, iter [02100, 05004], lr: 0.001000, loss: 0.6635
2022-10-10 11:43:21 - train: epoch 0068, iter [02200, 05004], lr: 0.001000, loss: 0.8073
2022-10-10 11:43:56 - train: epoch 0068, iter [02300, 05004], lr: 0.001000, loss: 0.7779
2022-10-10 11:44:50 - train: epoch 0068, iter [02400, 05004], lr: 0.001000, loss: 0.8407
2022-10-10 11:45:37 - train: epoch 0068, iter [02500, 05004], lr: 0.001000, loss: 0.6539
2022-10-10 11:46:19 - train: epoch 0068, iter [02600, 05004], lr: 0.001000, loss: 0.7072
2022-10-10 11:47:33 - train: epoch 0068, iter [02700, 05004], lr: 0.001000, loss: 0.7302
2022-10-10 11:48:35 - train: epoch 0068, iter [02800, 05004], lr: 0.001000, loss: 0.7594
2022-10-10 11:49:11 - train: epoch 0068, iter [02900, 05004], lr: 0.001000, loss: 0.9561
2022-10-10 11:49:45 - train: epoch 0068, iter [03000, 05004], lr: 0.001000, loss: 0.8646
2022-10-10 11:50:19 - train: epoch 0068, iter [03100, 05004], lr: 0.001000, loss: 0.6522
2022-10-10 11:50:53 - train: epoch 0068, iter [03200, 05004], lr: 0.001000, loss: 0.9079
2022-10-10 11:51:33 - train: epoch 0068, iter [03300, 05004], lr: 0.001000, loss: 0.7523
2022-10-10 11:52:08 - train: epoch 0068, iter [03400, 05004], lr: 0.001000, loss: 0.7958
2022-10-10 11:52:42 - train: epoch 0068, iter [03500, 05004], lr: 0.001000, loss: 0.8769
2022-10-10 11:53:16 - train: epoch 0068, iter [03600, 05004], lr: 0.001000, loss: 0.8648
2022-10-10 11:53:51 - train: epoch 0068, iter [03700, 05004], lr: 0.001000, loss: 0.8217
2022-10-10 11:54:25 - train: epoch 0068, iter [03800, 05004], lr: 0.001000, loss: 0.9365
2022-10-10 11:54:59 - train: epoch 0068, iter [03900, 05004], lr: 0.001000, loss: 0.8055
2022-10-10 11:55:34 - train: epoch 0068, iter [04000, 05004], lr: 0.001000, loss: 0.8250
2022-10-10 11:56:08 - train: epoch 0068, iter [04100, 05004], lr: 0.001000, loss: 0.7664
2022-10-10 11:56:43 - train: epoch 0068, iter [04200, 05004], lr: 0.001000, loss: 0.8677
2022-10-10 11:57:17 - train: epoch 0068, iter [04300, 05004], lr: 0.001000, loss: 0.8525
2022-10-10 11:57:52 - train: epoch 0068, iter [04400, 05004], lr: 0.001000, loss: 0.6874
2022-10-10 11:58:26 - train: epoch 0068, iter [04500, 05004], lr: 0.001000, loss: 0.8166
2022-10-10 11:59:03 - train: epoch 0068, iter [04600, 05004], lr: 0.001000, loss: 0.8193
2022-10-10 11:59:37 - train: epoch 0068, iter [04700, 05004], lr: 0.001000, loss: 0.8809
2022-10-10 12:00:12 - train: epoch 0068, iter [04800, 05004], lr: 0.001000, loss: 0.8622
2022-10-10 12:00:46 - train: epoch 0068, iter [04900, 05004], lr: 0.001000, loss: 0.9097
2022-10-10 12:01:19 - train: epoch 0068, iter [05000, 05004], lr: 0.001000, loss: 0.6376
2022-10-10 12:01:21 - train: epoch 068, train_loss: 0.8149
2022-10-10 12:02:40 - eval: epoch: 068, acc1: 76.526%, acc5: 93.312%, test_loss: 0.9291, per_image_load_time: 1.633ms, per_image_inference_time: 0.624ms
2022-10-10 12:02:40 - until epoch: 068, best_acc1: 76.526%
2022-10-10 12:02:40 - epoch 069 lr: 0.001000
2022-10-10 12:03:27 - train: epoch 0069, iter [00100, 05004], lr: 0.001000, loss: 0.8305
2022-10-10 12:04:13 - train: epoch 0069, iter [00200, 05004], lr: 0.001000, loss: 0.9849
2022-10-10 12:04:47 - train: epoch 0069, iter [00300, 05004], lr: 0.001000, loss: 0.7192
2022-10-10 12:05:25 - train: epoch 0069, iter [00400, 05004], lr: 0.001000, loss: 0.8233
2022-10-10 12:05:59 - train: epoch 0069, iter [00500, 05004], lr: 0.001000, loss: 0.9936
2022-10-10 12:06:35 - train: epoch 0069, iter [00600, 05004], lr: 0.001000, loss: 0.8209
2022-10-10 12:07:10 - train: epoch 0069, iter [00700, 05004], lr: 0.001000, loss: 0.7499
2022-10-10 12:07:44 - train: epoch 0069, iter [00800, 05004], lr: 0.001000, loss: 0.8596
2022-10-10 12:08:20 - train: epoch 0069, iter [00900, 05004], lr: 0.001000, loss: 0.7218
2022-10-10 12:08:54 - train: epoch 0069, iter [01000, 05004], lr: 0.001000, loss: 0.8218
2022-10-10 12:09:29 - train: epoch 0069, iter [01100, 05004], lr: 0.001000, loss: 0.8134
2022-10-10 12:10:29 - train: epoch 0069, iter [01200, 05004], lr: 0.001000, loss: 0.7247
2022-10-10 12:11:30 - train: epoch 0069, iter [01300, 05004], lr: 0.001000, loss: 0.7720
2022-10-10 12:12:34 - train: epoch 0069, iter [01400, 05004], lr: 0.001000, loss: 0.7977
2022-10-10 12:13:58 - train: epoch 0069, iter [01500, 05004], lr: 0.001000, loss: 0.6956
2022-10-10 12:15:04 - train: epoch 0069, iter [01600, 05004], lr: 0.001000, loss: 0.9018
2022-10-10 12:15:56 - train: epoch 0069, iter [01700, 05004], lr: 0.001000, loss: 0.8382
2022-10-10 12:16:32 - train: epoch 0069, iter [01800, 05004], lr: 0.001000, loss: 0.7776
2022-10-10 12:17:06 - train: epoch 0069, iter [01900, 05004], lr: 0.001000, loss: 0.6729
2022-10-10 12:17:42 - train: epoch 0069, iter [02000, 05004], lr: 0.001000, loss: 0.5914
2022-10-10 12:18:17 - train: epoch 0069, iter [02100, 05004], lr: 0.001000, loss: 0.8111
2022-10-10 12:18:51 - train: epoch 0069, iter [02200, 05004], lr: 0.001000, loss: 0.8166
2022-10-10 12:19:25 - train: epoch 0069, iter [02300, 05004], lr: 0.001000, loss: 0.7672
2022-10-10 12:20:05 - train: epoch 0069, iter [02400, 05004], lr: 0.001000, loss: 0.8069
2022-10-10 12:20:39 - train: epoch 0069, iter [02500, 05004], lr: 0.001000, loss: 0.9017
2022-10-10 12:21:14 - train: epoch 0069, iter [02600, 05004], lr: 0.001000, loss: 0.7361
2022-10-10 12:21:51 - train: epoch 0069, iter [02700, 05004], lr: 0.001000, loss: 0.8433
2022-10-10 12:22:26 - train: epoch 0069, iter [02800, 05004], lr: 0.001000, loss: 0.8842
2022-10-10 12:23:00 - train: epoch 0069, iter [02900, 05004], lr: 0.001000, loss: 0.7939
2022-10-10 12:23:34 - train: epoch 0069, iter [03000, 05004], lr: 0.001000, loss: 0.8401
2022-10-10 12:24:09 - train: epoch 0069, iter [03100, 05004], lr: 0.001000, loss: 0.8841
2022-10-10 12:24:43 - train: epoch 0069, iter [03200, 05004], lr: 0.001000, loss: 0.8825
2022-10-10 12:25:18 - train: epoch 0069, iter [03300, 05004], lr: 0.001000, loss: 0.7376
2022-10-10 12:25:53 - train: epoch 0069, iter [03400, 05004], lr: 0.001000, loss: 0.6675
2022-10-10 12:26:29 - train: epoch 0069, iter [03500, 05004], lr: 0.001000, loss: 0.6535
2022-10-10 12:27:02 - train: epoch 0069, iter [03600, 05004], lr: 0.001000, loss: 0.4819
2022-10-10 12:27:37 - train: epoch 0069, iter [03700, 05004], lr: 0.001000, loss: 0.8338
2022-10-10 12:28:12 - train: epoch 0069, iter [03800, 05004], lr: 0.001000, loss: 0.7991
2022-10-10 12:28:50 - train: epoch 0069, iter [03900, 05004], lr: 0.001000, loss: 0.9695
2022-10-10 12:29:24 - train: epoch 0069, iter [04000, 05004], lr: 0.001000, loss: 0.7748
2022-10-10 12:30:00 - train: epoch 0069, iter [04100, 05004], lr: 0.001000, loss: 0.7796
2022-10-10 12:30:33 - train: epoch 0069, iter [04200, 05004], lr: 0.001000, loss: 0.7755
2022-10-10 12:31:09 - train: epoch 0069, iter [04300, 05004], lr: 0.001000, loss: 0.7995
2022-10-10 12:31:44 - train: epoch 0069, iter [04400, 05004], lr: 0.001000, loss: 0.7391
2022-10-10 12:32:18 - train: epoch 0069, iter [04500, 05004], lr: 0.001000, loss: 0.9093
2022-10-10 12:32:53 - train: epoch 0069, iter [04600, 05004], lr: 0.001000, loss: 0.8983
2022-10-10 12:33:33 - train: epoch 0069, iter [04700, 05004], lr: 0.001000, loss: 0.8503
2022-10-10 12:34:11 - train: epoch 0069, iter [04800, 05004], lr: 0.001000, loss: 0.9030
2022-10-10 12:34:47 - train: epoch 0069, iter [04900, 05004], lr: 0.001000, loss: 0.8775
2022-10-10 12:35:19 - train: epoch 0069, iter [05000, 05004], lr: 0.001000, loss: 0.7575
2022-10-10 12:35:21 - train: epoch 069, train_loss: 0.8083
2022-10-10 12:36:40 - eval: epoch: 069, acc1: 76.414%, acc5: 93.314%, test_loss: 0.9309, per_image_load_time: 2.284ms, per_image_inference_time: 0.582ms
2022-10-10 12:36:40 - until epoch: 069, best_acc1: 76.526%
2022-10-10 12:36:40 - epoch 070 lr: 0.001000
2022-10-10 12:37:23 - train: epoch 0070, iter [00100, 05004], lr: 0.001000, loss: 0.9503
2022-10-10 12:37:57 - train: epoch 0070, iter [00200, 05004], lr: 0.001000, loss: 0.8509
2022-10-10 12:38:35 - train: epoch 0070, iter [00300, 05004], lr: 0.001000, loss: 0.8396
2022-10-10 12:39:07 - train: epoch 0070, iter [00400, 05004], lr: 0.001000, loss: 0.8850
2022-10-10 12:39:42 - train: epoch 0070, iter [00500, 05004], lr: 0.001000, loss: 0.6724
2022-10-10 12:40:16 - train: epoch 0070, iter [00600, 05004], lr: 0.001000, loss: 0.6783
2022-10-10 12:40:50 - train: epoch 0070, iter [00700, 05004], lr: 0.001000, loss: 0.7620
2022-10-10 12:41:31 - train: epoch 0070, iter [00800, 05004], lr: 0.001000, loss: 0.8858
2022-10-10 12:42:09 - train: epoch 0070, iter [00900, 05004], lr: 0.001000, loss: 0.7538
2022-10-10 12:42:57 - train: epoch 0070, iter [01000, 05004], lr: 0.001000, loss: 0.8915
2022-10-10 12:43:42 - train: epoch 0070, iter [01100, 05004], lr: 0.001000, loss: 1.0644
2022-10-10 12:44:37 - train: epoch 0070, iter [01200, 05004], lr: 0.001000, loss: 0.7115
2022-10-10 12:45:10 - train: epoch 0070, iter [01300, 05004], lr: 0.001000, loss: 0.7302
2022-10-10 12:45:47 - train: epoch 0070, iter [01400, 05004], lr: 0.001000, loss: 0.7107
2022-10-10 12:46:37 - train: epoch 0070, iter [01500, 05004], lr: 0.001000, loss: 0.8569
2022-10-10 12:47:21 - train: epoch 0070, iter [01600, 05004], lr: 0.001000, loss: 0.8042
2022-10-10 12:48:01 - train: epoch 0070, iter [01700, 05004], lr: 0.001000, loss: 0.7013
2022-10-10 12:48:38 - train: epoch 0070, iter [01800, 05004], lr: 0.001000, loss: 0.7982
2022-10-10 12:49:23 - train: epoch 0070, iter [01900, 05004], lr: 0.001000, loss: 1.0140
2022-10-10 12:50:01 - train: epoch 0070, iter [02000, 05004], lr: 0.001000, loss: 0.8491
2022-10-10 12:50:44 - train: epoch 0070, iter [02100, 05004], lr: 0.001000, loss: 0.9337
2022-10-10 12:51:21 - train: epoch 0070, iter [02200, 05004], lr: 0.001000, loss: 0.7863
2022-10-10 12:51:59 - train: epoch 0070, iter [02300, 05004], lr: 0.001000, loss: 0.9060
2022-10-10 12:52:33 - train: epoch 0070, iter [02400, 05004], lr: 0.001000, loss: 0.8686
2022-10-10 12:53:08 - train: epoch 0070, iter [02500, 05004], lr: 0.001000, loss: 0.8671
2022-10-10 12:53:43 - train: epoch 0070, iter [02600, 05004], lr: 0.001000, loss: 0.6402
2022-10-10 12:54:16 - train: epoch 0070, iter [02700, 05004], lr: 0.001000, loss: 0.7488
2022-10-10 12:54:50 - train: epoch 0070, iter [02800, 05004], lr: 0.001000, loss: 0.8740
2022-10-10 12:55:26 - train: epoch 0070, iter [02900, 05004], lr: 0.001000, loss: 0.8582
2022-10-10 12:56:02 - train: epoch 0070, iter [03000, 05004], lr: 0.001000, loss: 0.7930
2022-10-10 12:56:36 - train: epoch 0070, iter [03100, 05004], lr: 0.001000, loss: 0.7673
2022-10-10 12:57:11 - train: epoch 0070, iter [03200, 05004], lr: 0.001000, loss: 0.9372
2022-10-10 12:57:46 - train: epoch 0070, iter [03300, 05004], lr: 0.001000, loss: 0.8449
2022-10-10 12:58:21 - train: epoch 0070, iter [03400, 05004], lr: 0.001000, loss: 0.8808
2022-10-10 12:58:55 - train: epoch 0070, iter [03500, 05004], lr: 0.001000, loss: 0.8276
2022-10-10 12:59:29 - train: epoch 0070, iter [03600, 05004], lr: 0.001000, loss: 0.9122
2022-10-10 13:00:03 - train: epoch 0070, iter [03700, 05004], lr: 0.001000, loss: 0.8694
2022-10-10 13:00:38 - train: epoch 0070, iter [03800, 05004], lr: 0.001000, loss: 0.6561
2022-10-10 13:01:13 - train: epoch 0070, iter [03900, 05004], lr: 0.001000, loss: 0.7892
2022-10-10 13:01:48 - train: epoch 0070, iter [04000, 05004], lr: 0.001000, loss: 0.7800
2022-10-10 13:02:24 - train: epoch 0070, iter [04100, 05004], lr: 0.001000, loss: 0.6787
2022-10-10 13:03:00 - train: epoch 0070, iter [04200, 05004], lr: 0.001000, loss: 0.7606
2022-10-10 13:03:55 - train: epoch 0070, iter [04300, 05004], lr: 0.001000, loss: 0.8317
2022-10-10 13:05:46 - train: epoch 0070, iter [04400, 05004], lr: 0.001000, loss: 0.7420
2022-10-10 13:07:17 - train: epoch 0070, iter [04500, 05004], lr: 0.001000, loss: 0.8293
2022-10-10 13:08:29 - train: epoch 0070, iter [04600, 05004], lr: 0.001000, loss: 0.9276
2022-10-10 13:09:43 - train: epoch 0070, iter [04700, 05004], lr: 0.001000, loss: 0.8316
2022-10-10 13:11:28 - train: epoch 0070, iter [04800, 05004], lr: 0.001000, loss: 0.7791
2022-10-10 13:13:03 - train: epoch 0070, iter [04900, 05004], lr: 0.001000, loss: 0.8117
2022-10-10 13:14:30 - train: epoch 0070, iter [05000, 05004], lr: 0.001000, loss: 0.7233
2022-10-10 13:14:32 - train: epoch 070, train_loss: 0.7997
2022-10-10 13:15:51 - eval: epoch: 070, acc1: 76.450%, acc5: 93.262%, test_loss: 0.9296, per_image_load_time: 2.377ms, per_image_inference_time: 0.564ms
2022-10-10 13:15:51 - until epoch: 070, best_acc1: 76.526%
2022-10-10 13:15:51 - epoch 071 lr: 0.001000
2022-10-10 13:16:34 - train: epoch 0071, iter [00100, 05004], lr: 0.001000, loss: 0.5945
2022-10-10 13:17:09 - train: epoch 0071, iter [00200, 05004], lr: 0.001000, loss: 0.7050
2022-10-10 13:17:44 - train: epoch 0071, iter [00300, 05004], lr: 0.001000, loss: 0.7977
2022-10-10 13:18:18 - train: epoch 0071, iter [00400, 05004], lr: 0.001000, loss: 0.8382
2022-10-10 13:18:55 - train: epoch 0071, iter [00500, 05004], lr: 0.001000, loss: 0.9029
2022-10-10 13:19:30 - train: epoch 0071, iter [00600, 05004], lr: 0.001000, loss: 0.8853
2022-10-10 13:20:04 - train: epoch 0071, iter [00700, 05004], lr: 0.001000, loss: 0.7067
2022-10-10 13:20:38 - train: epoch 0071, iter [00800, 05004], lr: 0.001000, loss: 0.9035
2022-10-10 13:21:13 - train: epoch 0071, iter [00900, 05004], lr: 0.001000, loss: 0.8686
2022-10-10 13:21:47 - train: epoch 0071, iter [01000, 05004], lr: 0.001000, loss: 0.8261
2022-10-10 13:22:21 - train: epoch 0071, iter [01100, 05004], lr: 0.001000, loss: 0.9952
2022-10-10 13:22:55 - train: epoch 0071, iter [01200, 05004], lr: 0.001000, loss: 0.6465
2022-10-10 13:23:29 - train: epoch 0071, iter [01300, 05004], lr: 0.001000, loss: 0.8688
2022-10-10 13:24:05 - train: epoch 0071, iter [01400, 05004], lr: 0.001000, loss: 0.7309
2022-10-10 13:24:43 - train: epoch 0071, iter [01500, 05004], lr: 0.001000, loss: 0.6575
2022-10-10 13:25:19 - train: epoch 0071, iter [01600, 05004], lr: 0.001000, loss: 0.8123
2022-10-10 13:25:53 - train: epoch 0071, iter [01700, 05004], lr: 0.001000, loss: 0.7143
2022-10-10 13:26:39 - train: epoch 0071, iter [01800, 05004], lr: 0.001000, loss: 0.8744
2022-10-10 13:27:36 - train: epoch 0071, iter [01900, 05004], lr: 0.001000, loss: 0.9910
2022-10-10 13:28:10 - train: epoch 0071, iter [02000, 05004], lr: 0.001000, loss: 0.7865
2022-10-10 13:28:44 - train: epoch 0071, iter [02100, 05004], lr: 0.001000, loss: 0.6637
2022-10-10 13:29:21 - train: epoch 0071, iter [02200, 05004], lr: 0.001000, loss: 0.7058
2022-10-10 13:30:00 - train: epoch 0071, iter [02300, 05004], lr: 0.001000, loss: 0.6825
2022-10-10 13:30:32 - train: epoch 0071, iter [02400, 05004], lr: 0.001000, loss: 0.5780
2022-10-10 13:31:12 - train: epoch 0071, iter [02500, 05004], lr: 0.001000, loss: 0.9715
2022-10-10 13:31:45 - train: epoch 0071, iter [02600, 05004], lr: 0.001000, loss: 0.7893
2022-10-10 13:32:20 - train: epoch 0071, iter [02700, 05004], lr: 0.001000, loss: 0.9151
2022-10-10 13:33:16 - train: epoch 0071, iter [02800, 05004], lr: 0.001000, loss: 0.9552
2022-10-10 13:35:13 - train: epoch 0071, iter [02900, 05004], lr: 0.001000, loss: 0.8113
2022-10-10 13:37:20 - train: epoch 0071, iter [03000, 05004], lr: 0.001000, loss: 0.8381
2022-10-10 13:38:57 - train: epoch 0071, iter [03100, 05004], lr: 0.001000, loss: 0.7436
2022-10-10 13:39:35 - train: epoch 0071, iter [03200, 05004], lr: 0.001000, loss: 0.6848
2022-10-10 13:40:17 - train: epoch 0071, iter [03300, 05004], lr: 0.001000, loss: 0.8134
2022-10-10 13:40:55 - train: epoch 0071, iter [03400, 05004], lr: 0.001000, loss: 0.6837
2022-10-10 13:41:34 - train: epoch 0071, iter [03500, 05004], lr: 0.001000, loss: 0.9317
2022-10-10 13:42:12 - train: epoch 0071, iter [03600, 05004], lr: 0.001000, loss: 0.8472
2022-10-10 13:42:55 - train: epoch 0071, iter [03700, 05004], lr: 0.001000, loss: 0.7689
2022-10-10 13:43:30 - train: epoch 0071, iter [03800, 05004], lr: 0.001000, loss: 0.7989
2022-10-10 13:44:03 - train: epoch 0071, iter [03900, 05004], lr: 0.001000, loss: 0.8684
2022-10-10 13:44:39 - train: epoch 0071, iter [04000, 05004], lr: 0.001000, loss: 0.8734
2022-10-10 13:45:15 - train: epoch 0071, iter [04100, 05004], lr: 0.001000, loss: 0.7695
2022-10-10 13:45:48 - train: epoch 0071, iter [04200, 05004], lr: 0.001000, loss: 0.8151
2022-10-10 13:46:22 - train: epoch 0071, iter [04300, 05004], lr: 0.001000, loss: 0.7456
2022-10-10 13:46:57 - train: epoch 0071, iter [04400, 05004], lr: 0.001000, loss: 0.7899
2022-10-10 13:47:32 - train: epoch 0071, iter [04500, 05004], lr: 0.001000, loss: 0.7263
2022-10-10 13:48:27 - train: epoch 0071, iter [04600, 05004], lr: 0.001000, loss: 0.8883
2022-10-10 13:49:05 - train: epoch 0071, iter [04700, 05004], lr: 0.001000, loss: 0.9611
2022-10-10 13:49:42 - train: epoch 0071, iter [04800, 05004], lr: 0.001000, loss: 0.7663
2022-10-10 13:50:18 - train: epoch 0071, iter [04900, 05004], lr: 0.001000, loss: 0.9479
2022-10-10 13:50:51 - train: epoch 0071, iter [05000, 05004], lr: 0.001000, loss: 0.8520
2022-10-10 13:50:53 - train: epoch 071, train_loss: 0.7961
2022-10-10 13:52:10 - eval: epoch: 071, acc1: 76.564%, acc5: 93.314%, test_loss: 0.9256, per_image_load_time: 0.354ms, per_image_inference_time: 0.582ms
2022-10-10 13:52:10 - until epoch: 071, best_acc1: 76.564%
2022-10-10 13:52:10 - epoch 072 lr: 0.001000
2022-10-10 13:52:51 - train: epoch 0072, iter [00100, 05004], lr: 0.001000, loss: 0.7257
2022-10-10 13:53:26 - train: epoch 0072, iter [00200, 05004], lr: 0.001000, loss: 0.7699
2022-10-10 13:54:01 - train: epoch 0072, iter [00300, 05004], lr: 0.001000, loss: 0.7531
2022-10-10 13:54:35 - train: epoch 0072, iter [00400, 05004], lr: 0.001000, loss: 0.8295
2022-10-10 13:55:09 - train: epoch 0072, iter [00500, 05004], lr: 0.001000, loss: 0.8386
2022-10-10 13:55:44 - train: epoch 0072, iter [00600, 05004], lr: 0.001000, loss: 0.6757
2022-10-10 13:56:19 - train: epoch 0072, iter [00700, 05004], lr: 0.001000, loss: 0.7863
2022-10-10 13:56:53 - train: epoch 0072, iter [00800, 05004], lr: 0.001000, loss: 0.7559
2022-10-10 13:57:28 - train: epoch 0072, iter [00900, 05004], lr: 0.001000, loss: 0.6616
2022-10-10 13:58:03 - train: epoch 0072, iter [01000, 05004], lr: 0.001000, loss: 0.6233
2022-10-10 13:58:37 - train: epoch 0072, iter [01100, 05004], lr: 0.001000, loss: 0.6193
2022-10-10 13:59:12 - train: epoch 0072, iter [01200, 05004], lr: 0.001000, loss: 0.8458
2022-10-10 13:59:47 - train: epoch 0072, iter [01300, 05004], lr: 0.001000, loss: 0.9316
2022-10-10 14:00:23 - train: epoch 0072, iter [01400, 05004], lr: 0.001000, loss: 0.6516
2022-10-10 14:00:57 - train: epoch 0072, iter [01500, 05004], lr: 0.001000, loss: 0.7766
2022-10-10 14:01:32 - train: epoch 0072, iter [01600, 05004], lr: 0.001000, loss: 0.8724
2022-10-10 14:02:05 - train: epoch 0072, iter [01700, 05004], lr: 0.001000, loss: 0.9564
2022-10-10 14:02:40 - train: epoch 0072, iter [01800, 05004], lr: 0.001000, loss: 0.7214
2022-10-10 14:03:14 - train: epoch 0072, iter [01900, 05004], lr: 0.001000, loss: 0.9462
2022-10-10 14:03:50 - train: epoch 0072, iter [02000, 05004], lr: 0.001000, loss: 0.8465
2022-10-10 14:04:24 - train: epoch 0072, iter [02100, 05004], lr: 0.001000, loss: 0.7637
2022-10-10 14:04:58 - train: epoch 0072, iter [02200, 05004], lr: 0.001000, loss: 0.6897
2022-10-10 14:05:33 - train: epoch 0072, iter [02300, 05004], lr: 0.001000, loss: 1.1509
2022-10-10 14:06:07 - train: epoch 0072, iter [02400, 05004], lr: 0.001000, loss: 0.7400
2022-10-10 14:06:43 - train: epoch 0072, iter [02500, 05004], lr: 0.001000, loss: 0.7361
2022-10-10 14:07:18 - train: epoch 0072, iter [02600, 05004], lr: 0.001000, loss: 0.7900
2022-10-10 14:07:52 - train: epoch 0072, iter [02700, 05004], lr: 0.001000, loss: 0.8009
2022-10-10 14:08:26 - train: epoch 0072, iter [02800, 05004], lr: 0.001000, loss: 0.8967
2022-10-10 14:09:02 - train: epoch 0072, iter [02900, 05004], lr: 0.001000, loss: 0.7453
2022-10-10 14:09:37 - train: epoch 0072, iter [03000, 05004], lr: 0.001000, loss: 0.6791
2022-10-10 14:10:12 - train: epoch 0072, iter [03100, 05004], lr: 0.001000, loss: 0.8793
2022-10-10 14:10:48 - train: epoch 0072, iter [03200, 05004], lr: 0.001000, loss: 0.8867
2022-10-10 14:11:24 - train: epoch 0072, iter [03300, 05004], lr: 0.001000, loss: 0.7714
2022-10-10 14:12:00 - train: epoch 0072, iter [03400, 05004], lr: 0.001000, loss: 0.8278
2022-10-10 14:12:36 - train: epoch 0072, iter [03500, 05004], lr: 0.001000, loss: 0.7157
2022-10-10 14:13:13 - train: epoch 0072, iter [03600, 05004], lr: 0.001000, loss: 0.8780
2022-10-10 14:13:48 - train: epoch 0072, iter [03700, 05004], lr: 0.001000, loss: 0.8243
2022-10-10 14:14:26 - train: epoch 0072, iter [03800, 05004], lr: 0.001000, loss: 0.6681
2022-10-10 14:15:00 - train: epoch 0072, iter [03900, 05004], lr: 0.001000, loss: 0.8246
2022-10-10 14:15:34 - train: epoch 0072, iter [04000, 05004], lr: 0.001000, loss: 0.7409
2022-10-10 14:16:09 - train: epoch 0072, iter [04100, 05004], lr: 0.001000, loss: 0.8603
2022-10-10 14:16:44 - train: epoch 0072, iter [04200, 05004], lr: 0.001000, loss: 0.5898
2022-10-10 14:17:19 - train: epoch 0072, iter [04300, 05004], lr: 0.001000, loss: 0.7613
2022-10-10 14:18:01 - train: epoch 0072, iter [04400, 05004], lr: 0.001000, loss: 0.6874
2022-10-10 14:18:52 - train: epoch 0072, iter [04500, 05004], lr: 0.001000, loss: 0.7697
2022-10-10 14:19:40 - train: epoch 0072, iter [04600, 05004], lr: 0.001000, loss: 0.7406
2022-10-10 14:20:29 - train: epoch 0072, iter [04700, 05004], lr: 0.001000, loss: 0.9251
2022-10-10 14:21:24 - train: epoch 0072, iter [04800, 05004], lr: 0.001000, loss: 0.7700
2022-10-10 14:22:09 - train: epoch 0072, iter [04900, 05004], lr: 0.001000, loss: 0.8573
2022-10-10 14:22:49 - train: epoch 0072, iter [05000, 05004], lr: 0.001000, loss: 0.7109
2022-10-10 14:22:51 - train: epoch 072, train_loss: 0.7935
2022-10-10 14:24:09 - eval: epoch: 072, acc1: 76.606%, acc5: 93.310%, test_loss: 0.9289, per_image_load_time: 1.656ms, per_image_inference_time: 0.599ms
2022-10-10 14:24:10 - until epoch: 072, best_acc1: 76.606%
2022-10-10 14:24:10 - epoch 073 lr: 0.001000
2022-10-10 14:24:51 - train: epoch 0073, iter [00100, 05004], lr: 0.001000, loss: 1.0365
2022-10-10 14:25:25 - train: epoch 0073, iter [00200, 05004], lr: 0.001000, loss: 0.7912
2022-10-10 14:25:59 - train: epoch 0073, iter [00300, 05004], lr: 0.001000, loss: 0.9080
2022-10-10 14:26:35 - train: epoch 0073, iter [00400, 05004], lr: 0.001000, loss: 0.7874
2022-10-10 14:27:10 - train: epoch 0073, iter [00500, 05004], lr: 0.001000, loss: 0.7530
2022-10-10 14:27:44 - train: epoch 0073, iter [00600, 05004], lr: 0.001000, loss: 0.8630
2022-10-10 14:28:18 - train: epoch 0073, iter [00700, 05004], lr: 0.001000, loss: 0.7748
2022-10-10 14:28:55 - train: epoch 0073, iter [00800, 05004], lr: 0.001000, loss: 0.8524
2022-10-10 14:29:29 - train: epoch 0073, iter [00900, 05004], lr: 0.001000, loss: 0.7246
2022-10-10 14:30:07 - train: epoch 0073, iter [01000, 05004], lr: 0.001000, loss: 0.6225
2022-10-10 14:30:56 - train: epoch 0073, iter [01100, 05004], lr: 0.001000, loss: 0.8881
2022-10-10 14:31:55 - train: epoch 0073, iter [01200, 05004], lr: 0.001000, loss: 0.7001
2022-10-10 14:32:43 - train: epoch 0073, iter [01300, 05004], lr: 0.001000, loss: 0.9856
2022-10-10 14:33:29 - train: epoch 0073, iter [01400, 05004], lr: 0.001000, loss: 0.7635
2022-10-10 14:34:02 - train: epoch 0073, iter [01500, 05004], lr: 0.001000, loss: 0.6835
2022-10-10 14:34:38 - train: epoch 0073, iter [01600, 05004], lr: 0.001000, loss: 0.8880
2022-10-10 14:35:13 - train: epoch 0073, iter [01700, 05004], lr: 0.001000, loss: 0.8526
2022-10-10 14:35:47 - train: epoch 0073, iter [01800, 05004], lr: 0.001000, loss: 0.7998
2022-10-10 14:36:22 - train: epoch 0073, iter [01900, 05004], lr: 0.001000, loss: 1.0284
2022-10-10 14:36:56 - train: epoch 0073, iter [02000, 05004], lr: 0.001000, loss: 0.8172
2022-10-10 14:37:31 - train: epoch 0073, iter [02100, 05004], lr: 0.001000, loss: 0.7972
2022-10-10 14:38:04 - train: epoch 0073, iter [02200, 05004], lr: 0.001000, loss: 0.7283
2022-10-10 14:38:40 - train: epoch 0073, iter [02300, 05004], lr: 0.001000, loss: 0.6566
2022-10-10 14:39:12 - train: epoch 0073, iter [02400, 05004], lr: 0.001000, loss: 0.9409
2022-10-10 14:39:48 - train: epoch 0073, iter [02500, 05004], lr: 0.001000, loss: 0.9657
2022-10-10 14:40:23 - train: epoch 0073, iter [02600, 05004], lr: 0.001000, loss: 0.8761
2022-10-10 14:40:56 - train: epoch 0073, iter [02700, 05004], lr: 0.001000, loss: 0.7499
2022-10-10 14:41:31 - train: epoch 0073, iter [02800, 05004], lr: 0.001000, loss: 0.7617
2022-10-10 14:42:10 - train: epoch 0073, iter [02900, 05004], lr: 0.001000, loss: 0.8242
2022-10-10 14:43:03 - train: epoch 0073, iter [03000, 05004], lr: 0.001000, loss: 0.6299
2022-10-10 14:44:03 - train: epoch 0073, iter [03100, 05004], lr: 0.001000, loss: 0.7310
2022-10-10 14:45:40 - train: epoch 0073, iter [03200, 05004], lr: 0.001000, loss: 0.8812
2022-10-10 14:47:40 - train: epoch 0073, iter [03300, 05004], lr: 0.001000, loss: 0.5943
2022-10-10 14:48:46 - train: epoch 0073, iter [03400, 05004], lr: 0.001000, loss: 0.7647
2022-10-10 14:49:49 - train: epoch 0073, iter [03500, 05004], lr: 0.001000, loss: 0.7886
2022-10-10 14:51:08 - train: epoch 0073, iter [03600, 05004], lr: 0.001000, loss: 0.6595
2022-10-10 14:52:40 - train: epoch 0073, iter [03700, 05004], lr: 0.001000, loss: 0.9092
2022-10-10 14:55:01 - train: epoch 0073, iter [03800, 05004], lr: 0.001000, loss: 0.7180
2022-10-10 14:56:57 - train: epoch 0073, iter [03900, 05004], lr: 0.001000, loss: 0.6995
2022-10-10 14:57:55 - train: epoch 0073, iter [04000, 05004], lr: 0.001000, loss: 0.8020
2022-10-10 14:59:08 - train: epoch 0073, iter [04100, 05004], lr: 0.001000, loss: 0.7386
2022-10-10 14:59:58 - train: epoch 0073, iter [04200, 05004], lr: 0.001000, loss: 0.8499
2022-10-10 15:01:51 - train: epoch 0073, iter [04300, 05004], lr: 0.001000, loss: 0.7348
2022-10-10 15:03:24 - train: epoch 0073, iter [04400, 05004], lr: 0.001000, loss: 0.7193
2022-10-10 15:05:14 - train: epoch 0073, iter [04500, 05004], lr: 0.001000, loss: 0.8472
2022-10-10 15:06:22 - train: epoch 0073, iter [04600, 05004], lr: 0.001000, loss: 0.9730
2022-10-10 15:07:11 - train: epoch 0073, iter [04700, 05004], lr: 0.001000, loss: 0.7723
2022-10-10 15:07:57 - train: epoch 0073, iter [04800, 05004], lr: 0.001000, loss: 0.7111
2022-10-10 15:08:46 - train: epoch 0073, iter [04900, 05004], lr: 0.001000, loss: 0.7732
2022-10-10 15:09:18 - train: epoch 0073, iter [05000, 05004], lr: 0.001000, loss: 0.8188
2022-10-10 15:09:20 - train: epoch 073, train_loss: 0.7866
2022-10-10 15:10:37 - eval: epoch: 073, acc1: 76.590%, acc5: 93.324%, test_loss: 0.9288, per_image_load_time: 0.860ms, per_image_inference_time: 0.581ms
2022-10-10 15:10:37 - until epoch: 073, best_acc1: 76.606%
2022-10-10 15:10:37 - epoch 074 lr: 0.001000
2022-10-10 15:11:16 - train: epoch 0074, iter [00100, 05004], lr: 0.001000, loss: 0.7861
2022-10-10 15:11:51 - train: epoch 0074, iter [00200, 05004], lr: 0.001000, loss: 0.8422
2022-10-10 15:12:25 - train: epoch 0074, iter [00300, 05004], lr: 0.001000, loss: 0.9469
2022-10-10 15:12:59 - train: epoch 0074, iter [00400, 05004], lr: 0.001000, loss: 0.9116
2022-10-10 15:13:33 - train: epoch 0074, iter [00500, 05004], lr: 0.001000, loss: 0.6977
2022-10-10 15:14:08 - train: epoch 0074, iter [00600, 05004], lr: 0.001000, loss: 0.7601
2022-10-10 15:14:41 - train: epoch 0074, iter [00700, 05004], lr: 0.001000, loss: 0.6391
2022-10-10 15:15:15 - train: epoch 0074, iter [00800, 05004], lr: 0.001000, loss: 0.9798
2022-10-10 15:15:49 - train: epoch 0074, iter [00900, 05004], lr: 0.001000, loss: 0.8030
2022-10-10 15:16:23 - train: epoch 0074, iter [01000, 05004], lr: 0.001000, loss: 0.9494
2022-10-10 15:16:57 - train: epoch 0074, iter [01100, 05004], lr: 0.001000, loss: 0.7257
2022-10-10 15:17:31 - train: epoch 0074, iter [01200, 05004], lr: 0.001000, loss: 0.6252
2022-10-10 15:18:12 - train: epoch 0074, iter [01300, 05004], lr: 0.001000, loss: 0.8106
2022-10-10 15:19:19 - train: epoch 0074, iter [01400, 05004], lr: 0.001000, loss: 0.8329
2022-10-10 15:21:05 - train: epoch 0074, iter [01500, 05004], lr: 0.001000, loss: 0.7636
2022-10-10 15:22:36 - train: epoch 0074, iter [01600, 05004], lr: 0.001000, loss: 0.7937
2022-10-10 15:24:01 - train: epoch 0074, iter [01700, 05004], lr: 0.001000, loss: 0.8919
2022-10-10 15:24:58 - train: epoch 0074, iter [01800, 05004], lr: 0.001000, loss: 0.9472
2022-10-10 15:25:53 - train: epoch 0074, iter [01900, 05004], lr: 0.001000, loss: 0.8172
2022-10-10 15:26:42 - train: epoch 0074, iter [02000, 05004], lr: 0.001000, loss: 0.7017
2022-10-10 15:27:16 - train: epoch 0074, iter [02100, 05004], lr: 0.001000, loss: 0.8235
2022-10-10 15:27:50 - train: epoch 0074, iter [02200, 05004], lr: 0.001000, loss: 0.8524
2022-10-10 15:28:24 - train: epoch 0074, iter [02300, 05004], lr: 0.001000, loss: 0.8231
2022-10-10 15:28:59 - train: epoch 0074, iter [02400, 05004], lr: 0.001000, loss: 0.5807
2022-10-10 15:29:35 - train: epoch 0074, iter [02500, 05004], lr: 0.001000, loss: 0.7589
2022-10-10 15:30:09 - train: epoch 0074, iter [02600, 05004], lr: 0.001000, loss: 0.8299
2022-10-10 15:30:52 - train: epoch 0074, iter [02700, 05004], lr: 0.001000, loss: 0.7954
2022-10-10 15:32:37 - train: epoch 0074, iter [02800, 05004], lr: 0.001000, loss: 0.8391
2022-10-10 15:34:38 - train: epoch 0074, iter [02900, 05004], lr: 0.001000, loss: 0.7416
2022-10-10 15:37:03 - train: epoch 0074, iter [03000, 05004], lr: 0.001000, loss: 0.9476
2022-10-10 15:39:17 - train: epoch 0074, iter [03100, 05004], lr: 0.001000, loss: 0.8005
2022-10-10 15:40:47 - train: epoch 0074, iter [03200, 05004], lr: 0.001000, loss: 0.7718
2022-10-10 15:42:23 - train: epoch 0074, iter [03300, 05004], lr: 0.001000, loss: 0.7584
2022-10-10 15:43:37 - train: epoch 0074, iter [03400, 05004], lr: 0.001000, loss: 0.9086
2022-10-10 15:44:50 - train: epoch 0074, iter [03500, 05004], lr: 0.001000, loss: 0.6857
2022-10-10 15:46:15 - train: epoch 0074, iter [03600, 05004], lr: 0.001000, loss: 0.8235
2022-10-10 15:48:36 - train: epoch 0074, iter [03700, 05004], lr: 0.001000, loss: 0.7901
2022-10-10 15:50:12 - train: epoch 0074, iter [03800, 05004], lr: 0.001000, loss: 0.7668
2022-10-10 15:51:15 - train: epoch 0074, iter [03900, 05004], lr: 0.001000, loss: 0.7828
2022-10-10 15:52:13 - train: epoch 0074, iter [04000, 05004], lr: 0.001000, loss: 0.9281
2022-10-10 15:53:07 - train: epoch 0074, iter [04100, 05004], lr: 0.001000, loss: 0.6790
2022-10-10 15:54:02 - train: epoch 0074, iter [04200, 05004], lr: 0.001000, loss: 0.7714
2022-10-10 15:55:08 - train: epoch 0074, iter [04300, 05004], lr: 0.001000, loss: 0.9075
2022-10-10 15:56:35 - train: epoch 0074, iter [04400, 05004], lr: 0.001000, loss: 0.6817
2022-10-10 15:57:42 - train: epoch 0074, iter [04500, 05004], lr: 0.001000, loss: 0.7767
2022-10-10 15:58:41 - train: epoch 0074, iter [04600, 05004], lr: 0.001000, loss: 0.8390
2022-10-10 15:59:33 - train: epoch 0074, iter [04700, 05004], lr: 0.001000, loss: 0.7328
2022-10-10 16:00:34 - train: epoch 0074, iter [04800, 05004], lr: 0.001000, loss: 0.8166
2022-10-10 16:01:47 - train: epoch 0074, iter [04900, 05004], lr: 0.001000, loss: 0.9438
2022-10-10 16:03:21 - train: epoch 0074, iter [05000, 05004], lr: 0.001000, loss: 0.8627
2022-10-10 16:03:23 - train: epoch 074, train_loss: 0.7813
2022-10-10 16:04:41 - eval: epoch: 074, acc1: 76.576%, acc5: 93.342%, test_loss: 0.9281, per_image_load_time: 2.268ms, per_image_inference_time: 0.572ms
2022-10-10 16:04:41 - until epoch: 074, best_acc1: 76.606%
2022-10-10 16:04:41 - epoch 075 lr: 0.001000
2022-10-10 16:05:20 - train: epoch 0075, iter [00100, 05004], lr: 0.001000, loss: 0.8170
2022-10-10 16:05:55 - train: epoch 0075, iter [00200, 05004], lr: 0.001000, loss: 0.8149
2022-10-10 16:06:30 - train: epoch 0075, iter [00300, 05004], lr: 0.001000, loss: 0.7941
2022-10-10 16:07:04 - train: epoch 0075, iter [00400, 05004], lr: 0.001000, loss: 0.7781
2022-10-10 16:07:37 - train: epoch 0075, iter [00500, 05004], lr: 0.001000, loss: 0.7153
2022-10-10 16:08:12 - train: epoch 0075, iter [00600, 05004], lr: 0.001000, loss: 0.7294
2022-10-10 16:08:45 - train: epoch 0075, iter [00700, 05004], lr: 0.001000, loss: 0.9295
2022-10-10 16:09:21 - train: epoch 0075, iter [00800, 05004], lr: 0.001000, loss: 0.7933
2022-10-10 16:09:55 - train: epoch 0075, iter [00900, 05004], lr: 0.001000, loss: 0.9297
2022-10-10 16:10:29 - train: epoch 0075, iter [01000, 05004], lr: 0.001000, loss: 0.6156
2022-10-10 16:11:04 - train: epoch 0075, iter [01100, 05004], lr: 0.001000, loss: 0.8355
2022-10-10 16:11:39 - train: epoch 0075, iter [01200, 05004], lr: 0.001000, loss: 0.7027
2022-10-10 16:12:12 - train: epoch 0075, iter [01300, 05004], lr: 0.001000, loss: 0.8217
2022-10-10 16:12:47 - train: epoch 0075, iter [01400, 05004], lr: 0.001000, loss: 0.7640
2022-10-10 16:13:21 - train: epoch 0075, iter [01500, 05004], lr: 0.001000, loss: 0.9756
2022-10-10 16:13:56 - train: epoch 0075, iter [01600, 05004], lr: 0.001000, loss: 0.7537
2022-10-10 16:14:30 - train: epoch 0075, iter [01700, 05004], lr: 0.001000, loss: 0.7691
2022-10-10 16:15:04 - train: epoch 0075, iter [01800, 05004], lr: 0.001000, loss: 0.7853
2022-10-10 16:15:37 - train: epoch 0075, iter [01900, 05004], lr: 0.001000, loss: 0.7139
2022-10-10 16:16:12 - train: epoch 0075, iter [02000, 05004], lr: 0.001000, loss: 0.8017
2022-10-10 16:16:46 - train: epoch 0075, iter [02100, 05004], lr: 0.001000, loss: 0.7479
2022-10-10 16:17:21 - train: epoch 0075, iter [02200, 05004], lr: 0.001000, loss: 0.7659
2022-10-10 16:17:55 - train: epoch 0075, iter [02300, 05004], lr: 0.001000, loss: 0.8983
2022-10-10 16:18:30 - train: epoch 0075, iter [02400, 05004], lr: 0.001000, loss: 0.7296
2022-10-10 16:19:05 - train: epoch 0075, iter [02500, 05004], lr: 0.001000, loss: 0.6304
2022-10-10 16:19:38 - train: epoch 0075, iter [02600, 05004], lr: 0.001000, loss: 0.7737
2022-10-10 16:20:14 - train: epoch 0075, iter [02700, 05004], lr: 0.001000, loss: 0.7422
2022-10-10 16:20:48 - train: epoch 0075, iter [02800, 05004], lr: 0.001000, loss: 0.6081
2022-10-10 16:21:22 - train: epoch 0075, iter [02900, 05004], lr: 0.001000, loss: 0.8273
2022-10-10 16:21:57 - train: epoch 0075, iter [03000, 05004], lr: 0.001000, loss: 0.9053
2022-10-10 16:22:46 - train: epoch 0075, iter [03100, 05004], lr: 0.001000, loss: 0.7349
2022-10-10 16:23:52 - train: epoch 0075, iter [03200, 05004], lr: 0.001000, loss: 0.7638
2022-10-10 16:24:27 - train: epoch 0075, iter [03300, 05004], lr: 0.001000, loss: 0.8153
2022-10-10 16:25:01 - train: epoch 0075, iter [03400, 05004], lr: 0.001000, loss: 0.7431
2022-10-10 16:25:36 - train: epoch 0075, iter [03500, 05004], lr: 0.001000, loss: 0.6909
2022-10-10 16:26:14 - train: epoch 0075, iter [03600, 05004], lr: 0.001000, loss: 0.7568
2022-10-10 16:26:50 - train: epoch 0075, iter [03700, 05004], lr: 0.001000, loss: 0.9089
2022-10-10 16:27:23 - train: epoch 0075, iter [03800, 05004], lr: 0.001000, loss: 0.7003
2022-10-10 16:27:57 - train: epoch 0075, iter [03900, 05004], lr: 0.001000, loss: 0.8275
2022-10-10 16:28:47 - train: epoch 0075, iter [04000, 05004], lr: 0.001000, loss: 0.7686
2022-10-10 16:30:11 - train: epoch 0075, iter [04100, 05004], lr: 0.001000, loss: 0.7797
2022-10-10 16:32:05 - train: epoch 0075, iter [04200, 05004], lr: 0.001000, loss: 0.8804
2022-10-10 16:33:02 - train: epoch 0075, iter [04300, 05004], lr: 0.001000, loss: 0.8948
2022-10-10 16:33:47 - train: epoch 0075, iter [04400, 05004], lr: 0.001000, loss: 0.7141
2022-10-10 16:35:18 - train: epoch 0075, iter [04500, 05004], lr: 0.001000, loss: 0.8634
2022-10-10 16:36:28 - train: epoch 0075, iter [04600, 05004], lr: 0.001000, loss: 0.6876
2022-10-10 16:37:38 - train: epoch 0075, iter [04700, 05004], lr: 0.001000, loss: 0.8119
2022-10-10 16:38:39 - train: epoch 0075, iter [04800, 05004], lr: 0.001000, loss: 0.6714
2022-10-10 16:39:26 - train: epoch 0075, iter [04900, 05004], lr: 0.001000, loss: 0.7518
2022-10-10 16:40:46 - train: epoch 0075, iter [05000, 05004], lr: 0.001000, loss: 0.7136
2022-10-10 16:40:48 - train: epoch 075, train_loss: 0.7770
2022-10-10 16:42:06 - eval: epoch: 075, acc1: 76.584%, acc5: 93.384%, test_loss: 0.9274, per_image_load_time: 2.202ms, per_image_inference_time: 0.561ms
2022-10-10 16:42:07 - until epoch: 075, best_acc1: 76.606%
2022-10-10 16:42:07 - epoch 076 lr: 0.001000
2022-10-10 16:42:46 - train: epoch 0076, iter [00100, 05004], lr: 0.001000, loss: 0.8793
2022-10-10 16:43:21 - train: epoch 0076, iter [00200, 05004], lr: 0.001000, loss: 0.7063
2022-10-10 16:43:56 - train: epoch 0076, iter [00300, 05004], lr: 0.001000, loss: 0.8624
2022-10-10 16:44:30 - train: epoch 0076, iter [00400, 05004], lr: 0.001000, loss: 0.8326
2022-10-10 16:45:04 - train: epoch 0076, iter [00500, 05004], lr: 0.001000, loss: 0.6258
2022-10-10 16:45:38 - train: epoch 0076, iter [00600, 05004], lr: 0.001000, loss: 0.8611
2022-10-10 16:46:12 - train: epoch 0076, iter [00700, 05004], lr: 0.001000, loss: 0.6460
2022-10-10 16:46:47 - train: epoch 0076, iter [00800, 05004], lr: 0.001000, loss: 0.7806
2022-10-10 16:47:21 - train: epoch 0076, iter [00900, 05004], lr: 0.001000, loss: 0.7521
2022-10-10 16:47:55 - train: epoch 0076, iter [01000, 05004], lr: 0.001000, loss: 0.7211
2022-10-10 16:48:29 - train: epoch 0076, iter [01100, 05004], lr: 0.001000, loss: 0.7226
2022-10-10 16:49:03 - train: epoch 0076, iter [01200, 05004], lr: 0.001000, loss: 0.7156
2022-10-10 16:49:37 - train: epoch 0076, iter [01300, 05004], lr: 0.001000, loss: 0.6051
2022-10-10 16:50:12 - train: epoch 0076, iter [01400, 05004], lr: 0.001000, loss: 0.7712
2022-10-10 16:50:46 - train: epoch 0076, iter [01500, 05004], lr: 0.001000, loss: 0.7176
2022-10-10 16:51:20 - train: epoch 0076, iter [01600, 05004], lr: 0.001000, loss: 0.7372
2022-10-10 16:51:55 - train: epoch 0076, iter [01700, 05004], lr: 0.001000, loss: 0.7094
2022-10-10 16:52:29 - train: epoch 0076, iter [01800, 05004], lr: 0.001000, loss: 0.6672
2022-10-10 16:53:04 - train: epoch 0076, iter [01900, 05004], lr: 0.001000, loss: 0.6813
2022-10-10 16:53:37 - train: epoch 0076, iter [02000, 05004], lr: 0.001000, loss: 0.6575
2022-10-10 16:54:12 - train: epoch 0076, iter [02100, 05004], lr: 0.001000, loss: 0.7537
2022-10-10 16:54:46 - train: epoch 0076, iter [02200, 05004], lr: 0.001000, loss: 0.7165
2022-10-10 16:55:20 - train: epoch 0076, iter [02300, 05004], lr: 0.001000, loss: 0.8769
2022-10-10 16:55:54 - train: epoch 0076, iter [02400, 05004], lr: 0.001000, loss: 1.1022
2022-10-10 16:56:28 - train: epoch 0076, iter [02500, 05004], lr: 0.001000, loss: 0.8269
2022-10-10 16:57:03 - train: epoch 0076, iter [02600, 05004], lr: 0.001000, loss: 0.8016
2022-10-10 16:57:37 - train: epoch 0076, iter [02700, 05004], lr: 0.001000, loss: 0.8109
2022-10-10 16:58:11 - train: epoch 0076, iter [02800, 05004], lr: 0.001000, loss: 0.7388
2022-10-10 16:58:45 - train: epoch 0076, iter [02900, 05004], lr: 0.001000, loss: 0.7883
2022-10-10 16:59:19 - train: epoch 0076, iter [03000, 05004], lr: 0.001000, loss: 0.6394
2022-10-10 16:59:53 - train: epoch 0076, iter [03100, 05004], lr: 0.001000, loss: 0.8007
2022-10-10 17:00:27 - train: epoch 0076, iter [03200, 05004], lr: 0.001000, loss: 0.8048
2022-10-10 17:01:02 - train: epoch 0076, iter [03300, 05004], lr: 0.001000, loss: 0.6915
2022-10-10 17:01:36 - train: epoch 0076, iter [03400, 05004], lr: 0.001000, loss: 0.8344
2022-10-10 17:02:10 - train: epoch 0076, iter [03500, 05004], lr: 0.001000, loss: 0.8165
2022-10-10 17:02:45 - train: epoch 0076, iter [03600, 05004], lr: 0.001000, loss: 0.7956
2022-10-10 17:03:19 - train: epoch 0076, iter [03700, 05004], lr: 0.001000, loss: 0.6563
2022-10-10 17:03:54 - train: epoch 0076, iter [03800, 05004], lr: 0.001000, loss: 0.7269
2022-10-10 17:04:28 - train: epoch 0076, iter [03900, 05004], lr: 0.001000, loss: 0.6855
2022-10-10 17:05:24 - train: epoch 0076, iter [04000, 05004], lr: 0.001000, loss: 0.8445
2022-10-10 17:06:21 - train: epoch 0076, iter [04100, 05004], lr: 0.001000, loss: 0.6768
2022-10-10 17:07:20 - train: epoch 0076, iter [04200, 05004], lr: 0.001000, loss: 0.7250
2022-10-10 17:08:08 - train: epoch 0076, iter [04300, 05004], lr: 0.001000, loss: 0.8675
2022-10-10 17:09:23 - train: epoch 0076, iter [04400, 05004], lr: 0.001000, loss: 0.9057
2022-10-10 17:11:23 - train: epoch 0076, iter [04500, 05004], lr: 0.001000, loss: 0.6726
2022-10-10 17:12:54 - train: epoch 0076, iter [04600, 05004], lr: 0.001000, loss: 0.7012
2022-10-10 17:14:43 - train: epoch 0076, iter [04700, 05004], lr: 0.001000, loss: 0.8490
2022-10-10 17:15:18 - train: epoch 0076, iter [04800, 05004], lr: 0.001000, loss: 0.5538
2022-10-10 17:15:53 - train: epoch 0076, iter [04900, 05004], lr: 0.001000, loss: 0.7943
2022-10-10 17:16:25 - train: epoch 0076, iter [05000, 05004], lr: 0.001000, loss: 0.9671
2022-10-10 17:16:28 - train: epoch 076, train_loss: 0.7729
2022-10-10 17:17:46 - eval: epoch: 076, acc1: 76.534%, acc5: 93.314%, test_loss: 0.9288, per_image_load_time: 0.416ms, per_image_inference_time: 0.580ms
2022-10-10 17:17:46 - until epoch: 076, best_acc1: 76.606%
2022-10-10 17:17:46 - epoch 077 lr: 0.001000
2022-10-10 17:18:26 - train: epoch 0077, iter [00100, 05004], lr: 0.001000, loss: 0.8522
2022-10-10 17:19:01 - train: epoch 0077, iter [00200, 05004], lr: 0.001000, loss: 0.8988
2022-10-10 17:19:35 - train: epoch 0077, iter [00300, 05004], lr: 0.001000, loss: 0.5771
2022-10-10 17:20:09 - train: epoch 0077, iter [00400, 05004], lr: 0.001000, loss: 0.7886
2022-10-10 17:20:44 - train: epoch 0077, iter [00500, 05004], lr: 0.001000, loss: 0.7588
2022-10-10 17:21:18 - train: epoch 0077, iter [00600, 05004], lr: 0.001000, loss: 0.6296
2022-10-10 17:21:54 - train: epoch 0077, iter [00700, 05004], lr: 0.001000, loss: 0.8330
2022-10-10 17:22:28 - train: epoch 0077, iter [00800, 05004], lr: 0.001000, loss: 0.8845
2022-10-10 17:23:03 - train: epoch 0077, iter [00900, 05004], lr: 0.001000, loss: 0.7467
2022-10-10 17:23:37 - train: epoch 0077, iter [01000, 05004], lr: 0.001000, loss: 0.6517
2022-10-10 17:24:11 - train: epoch 0077, iter [01100, 05004], lr: 0.001000, loss: 0.7995
2022-10-10 17:24:46 - train: epoch 0077, iter [01200, 05004], lr: 0.001000, loss: 0.8237
2022-10-10 17:25:21 - train: epoch 0077, iter [01300, 05004], lr: 0.001000, loss: 0.7210
2022-10-10 17:25:55 - train: epoch 0077, iter [01400, 05004], lr: 0.001000, loss: 0.7511
2022-10-10 17:26:30 - train: epoch 0077, iter [01500, 05004], lr: 0.001000, loss: 0.7027
2022-10-10 17:27:04 - train: epoch 0077, iter [01600, 05004], lr: 0.001000, loss: 0.8744
2022-10-10 17:27:38 - train: epoch 0077, iter [01700, 05004], lr: 0.001000, loss: 0.9644
2022-10-10 17:28:12 - train: epoch 0077, iter [01800, 05004], lr: 0.001000, loss: 0.6881
2022-10-10 17:28:47 - train: epoch 0077, iter [01900, 05004], lr: 0.001000, loss: 0.6687
2022-10-10 17:29:21 - train: epoch 0077, iter [02000, 05004], lr: 0.001000, loss: 0.6240
2022-10-10 17:29:55 - train: epoch 0077, iter [02100, 05004], lr: 0.001000, loss: 0.9332
2022-10-10 17:30:30 - train: epoch 0077, iter [02200, 05004], lr: 0.001000, loss: 0.8173
2022-10-10 17:31:12 - train: epoch 0077, iter [02300, 05004], lr: 0.001000, loss: 0.8414
2022-10-10 17:31:47 - train: epoch 0077, iter [02400, 05004], lr: 0.001000, loss: 0.7677
2022-10-10 17:32:31 - train: epoch 0077, iter [02500, 05004], lr: 0.001000, loss: 0.8837
2022-10-10 17:33:05 - train: epoch 0077, iter [02600, 05004], lr: 0.001000, loss: 0.6942
2022-10-10 17:33:39 - train: epoch 0077, iter [02700, 05004], lr: 0.001000, loss: 0.8037
2022-10-10 17:34:14 - train: epoch 0077, iter [02800, 05004], lr: 0.001000, loss: 0.8281
2022-10-10 17:34:48 - train: epoch 0077, iter [02900, 05004], lr: 0.001000, loss: 0.7527
2022-10-10 17:35:23 - train: epoch 0077, iter [03000, 05004], lr: 0.001000, loss: 0.7345
2022-10-10 17:36:00 - train: epoch 0077, iter [03100, 05004], lr: 0.001000, loss: 0.9301
2022-10-10 17:36:42 - train: epoch 0077, iter [03200, 05004], lr: 0.001000, loss: 0.8912
2022-10-10 17:37:15 - train: epoch 0077, iter [03300, 05004], lr: 0.001000, loss: 0.7851
2022-10-10 17:37:56 - train: epoch 0077, iter [03400, 05004], lr: 0.001000, loss: 0.9438
2022-10-10 17:38:31 - train: epoch 0077, iter [03500, 05004], lr: 0.001000, loss: 0.6670
2022-10-10 17:39:22 - train: epoch 0077, iter [03600, 05004], lr: 0.001000, loss: 0.6664
2022-10-10 17:39:57 - train: epoch 0077, iter [03700, 05004], lr: 0.001000, loss: 0.6293
2022-10-10 17:40:38 - train: epoch 0077, iter [03800, 05004], lr: 0.001000, loss: 0.7606
2022-10-10 17:41:50 - train: epoch 0077, iter [03900, 05004], lr: 0.001000, loss: 0.6040
2022-10-10 17:43:11 - train: epoch 0077, iter [04000, 05004], lr: 0.001000, loss: 0.8304
2022-10-10 17:44:51 - train: epoch 0077, iter [04100, 05004], lr: 0.001000, loss: 0.9860
2022-10-10 17:46:48 - train: epoch 0077, iter [04200, 05004], lr: 0.001000, loss: 0.9438
2022-10-10 17:47:44 - train: epoch 0077, iter [04300, 05004], lr: 0.001000, loss: 0.8303
2022-10-10 17:48:19 - train: epoch 0077, iter [04400, 05004], lr: 0.001000, loss: 0.8567
2022-10-10 17:50:16 - train: epoch 0077, iter [04500, 05004], lr: 0.001000, loss: 0.7487
2022-10-10 17:51:54 - train: epoch 0077, iter [04600, 05004], lr: 0.001000, loss: 0.8378
2022-10-10 17:53:28 - train: epoch 0077, iter [04700, 05004], lr: 0.001000, loss: 0.7526
2022-10-10 17:55:28 - train: epoch 0077, iter [04800, 05004], lr: 0.001000, loss: 0.6833
2022-10-10 17:56:13 - train: epoch 0077, iter [04900, 05004], lr: 0.001000, loss: 0.8589
2022-10-10 17:57:10 - train: epoch 0077, iter [05000, 05004], lr: 0.001000, loss: 0.7623
2022-10-10 17:57:13 - train: epoch 077, train_loss: 0.7699
2022-10-10 17:58:30 - eval: epoch: 077, acc1: 76.536%, acc5: 93.308%, test_loss: 0.9295, per_image_load_time: 1.402ms, per_image_inference_time: 0.615ms
2022-10-10 17:58:31 - until epoch: 077, best_acc1: 76.606%
2022-10-10 17:58:31 - epoch 078 lr: 0.001000
2022-10-10 17:59:11 - train: epoch 0078, iter [00100, 05004], lr: 0.001000, loss: 0.8159
2022-10-10 17:59:46 - train: epoch 0078, iter [00200, 05004], lr: 0.001000, loss: 0.9407
2022-10-10 18:00:21 - train: epoch 0078, iter [00300, 05004], lr: 0.001000, loss: 0.7313
2022-10-10 18:00:55 - train: epoch 0078, iter [00400, 05004], lr: 0.001000, loss: 0.7985
2022-10-10 18:01:30 - train: epoch 0078, iter [00500, 05004], lr: 0.001000, loss: 0.6884
2022-10-10 18:02:04 - train: epoch 0078, iter [00600, 05004], lr: 0.001000, loss: 0.8683
2022-10-10 18:02:39 - train: epoch 0078, iter [00700, 05004], lr: 0.001000, loss: 0.6614
2022-10-10 18:03:13 - train: epoch 0078, iter [00800, 05004], lr: 0.001000, loss: 0.7821
2022-10-10 18:03:48 - train: epoch 0078, iter [00900, 05004], lr: 0.001000, loss: 0.6592
2022-10-10 18:04:23 - train: epoch 0078, iter [01000, 05004], lr: 0.001000, loss: 0.8461
2022-10-10 18:04:57 - train: epoch 0078, iter [01100, 05004], lr: 0.001000, loss: 0.7915
2022-10-10 18:05:32 - train: epoch 0078, iter [01200, 05004], lr: 0.001000, loss: 0.7572
2022-10-10 18:06:22 - train: epoch 0078, iter [01300, 05004], lr: 0.001000, loss: 0.5804
2022-10-10 18:06:57 - train: epoch 0078, iter [01400, 05004], lr: 0.001000, loss: 0.6822
2022-10-10 18:07:31 - train: epoch 0078, iter [01500, 05004], lr: 0.001000, loss: 0.6975
2022-10-10 18:08:26 - train: epoch 0078, iter [01600, 05004], lr: 0.001000, loss: 0.7384
2022-10-10 18:09:12 - train: epoch 0078, iter [01700, 05004], lr: 0.001000, loss: 0.7565
2022-10-10 18:09:49 - train: epoch 0078, iter [01800, 05004], lr: 0.001000, loss: 0.8038
2022-10-10 18:10:30 - train: epoch 0078, iter [01900, 05004], lr: 0.001000, loss: 0.7953
2022-10-10 18:11:04 - train: epoch 0078, iter [02000, 05004], lr: 0.001000, loss: 0.8190
2022-10-10 18:11:38 - train: epoch 0078, iter [02100, 05004], lr: 0.001000, loss: 0.8482
2022-10-10 18:12:13 - train: epoch 0078, iter [02200, 05004], lr: 0.001000, loss: 0.8000
2022-10-10 18:12:48 - train: epoch 0078, iter [02300, 05004], lr: 0.001000, loss: 0.8546
2022-10-10 18:13:39 - train: epoch 0078, iter [02400, 05004], lr: 0.001000, loss: 0.7896
2022-10-10 18:14:13 - train: epoch 0078, iter [02500, 05004], lr: 0.001000, loss: 0.7045
2022-10-10 18:14:48 - train: epoch 0078, iter [02600, 05004], lr: 0.001000, loss: 0.8051
2022-10-10 18:15:23 - train: epoch 0078, iter [02700, 05004], lr: 0.001000, loss: 0.8801
2022-10-10 18:15:58 - train: epoch 0078, iter [02800, 05004], lr: 0.001000, loss: 0.6825
2022-10-10 18:16:47 - train: epoch 0078, iter [02900, 05004], lr: 0.001000, loss: 0.6617
2022-10-10 18:17:21 - train: epoch 0078, iter [03000, 05004], lr: 0.001000, loss: 0.6935
2022-10-10 18:17:56 - train: epoch 0078, iter [03100, 05004], lr: 0.001000, loss: 0.8709
2022-10-10 18:18:31 - train: epoch 0078, iter [03200, 05004], lr: 0.001000, loss: 0.7909
2022-10-10 18:19:06 - train: epoch 0078, iter [03300, 05004], lr: 0.001000, loss: 1.0279
2022-10-10 18:19:40 - train: epoch 0078, iter [03400, 05004], lr: 0.001000, loss: 0.9230
2022-10-10 18:20:15 - train: epoch 0078, iter [03500, 05004], lr: 0.001000, loss: 0.8615
2022-10-10 18:20:49 - train: epoch 0078, iter [03600, 05004], lr: 0.001000, loss: 0.8113
2022-10-10 18:21:25 - train: epoch 0078, iter [03700, 05004], lr: 0.001000, loss: 0.8781
2022-10-10 18:21:59 - train: epoch 0078, iter [03800, 05004], lr: 0.001000, loss: 0.7965
2022-10-10 18:22:33 - train: epoch 0078, iter [03900, 05004], lr: 0.001000, loss: 0.8078
2022-10-10 18:23:08 - train: epoch 0078, iter [04000, 05004], lr: 0.001000, loss: 0.7144
2022-10-10 18:23:42 - train: epoch 0078, iter [04100, 05004], lr: 0.001000, loss: 0.9305
2022-10-10 18:24:16 - train: epoch 0078, iter [04200, 05004], lr: 0.001000, loss: 0.6747
2022-10-10 18:24:52 - train: epoch 0078, iter [04300, 05004], lr: 0.001000, loss: 0.8603
2022-10-10 18:25:25 - train: epoch 0078, iter [04400, 05004], lr: 0.001000, loss: 0.7378
2022-10-10 18:26:00 - train: epoch 0078, iter [04500, 05004], lr: 0.001000, loss: 0.8205
2022-10-10 18:26:34 - train: epoch 0078, iter [04600, 05004], lr: 0.001000, loss: 0.6793
2022-10-10 18:27:09 - train: epoch 0078, iter [04700, 05004], lr: 0.001000, loss: 0.7641
2022-10-10 18:27:43 - train: epoch 0078, iter [04800, 05004], lr: 0.001000, loss: 0.7844
2022-10-10 18:28:18 - train: epoch 0078, iter [04900, 05004], lr: 0.001000, loss: 0.5666
2022-10-10 18:28:51 - train: epoch 0078, iter [05000, 05004], lr: 0.001000, loss: 0.7126
2022-10-10 18:28:53 - train: epoch 078, train_loss: 0.7689
2022-10-10 18:30:10 - eval: epoch: 078, acc1: 76.632%, acc5: 93.302%, test_loss: 0.9275, per_image_load_time: 0.711ms, per_image_inference_time: 0.563ms
2022-10-10 18:30:11 - until epoch: 078, best_acc1: 76.632%
2022-10-10 18:30:11 - epoch 079 lr: 0.001000
2022-10-10 18:30:50 - train: epoch 0079, iter [00100, 05004], lr: 0.001000, loss: 0.6629
2022-10-10 18:31:24 - train: epoch 0079, iter [00200, 05004], lr: 0.001000, loss: 0.8639
2022-10-10 18:32:00 - train: epoch 0079, iter [00300, 05004], lr: 0.001000, loss: 0.6946
2022-10-10 18:32:35 - train: epoch 0079, iter [00400, 05004], lr: 0.001000, loss: 0.8776
2022-10-10 18:33:08 - train: epoch 0079, iter [00500, 05004], lr: 0.001000, loss: 0.7685
2022-10-10 18:33:42 - train: epoch 0079, iter [00600, 05004], lr: 0.001000, loss: 0.5858
2022-10-10 18:34:17 - train: epoch 0079, iter [00700, 05004], lr: 0.001000, loss: 0.7305
2022-10-10 18:34:50 - train: epoch 0079, iter [00800, 05004], lr: 0.001000, loss: 0.6748
2022-10-10 18:35:25 - train: epoch 0079, iter [00900, 05004], lr: 0.001000, loss: 0.7602
2022-10-10 18:35:59 - train: epoch 0079, iter [01000, 05004], lr: 0.001000, loss: 0.7857
2022-10-10 18:36:33 - train: epoch 0079, iter [01100, 05004], lr: 0.001000, loss: 0.9330
2022-10-10 18:37:11 - train: epoch 0079, iter [01200, 05004], lr: 0.001000, loss: 0.7615
2022-10-10 18:37:44 - train: epoch 0079, iter [01300, 05004], lr: 0.001000, loss: 0.6939
2022-10-10 18:38:18 - train: epoch 0079, iter [01400, 05004], lr: 0.001000, loss: 0.7432
2022-10-10 18:38:52 - train: epoch 0079, iter [01500, 05004], lr: 0.001000, loss: 0.8299
2022-10-10 18:39:26 - train: epoch 0079, iter [01600, 05004], lr: 0.001000, loss: 0.6728
2022-10-10 18:40:00 - train: epoch 0079, iter [01700, 05004], lr: 0.001000, loss: 0.9027
2022-10-10 18:40:35 - train: epoch 0079, iter [01800, 05004], lr: 0.001000, loss: 0.6755
2022-10-10 18:41:10 - train: epoch 0079, iter [01900, 05004], lr: 0.001000, loss: 0.6816
2022-10-10 18:41:44 - train: epoch 0079, iter [02000, 05004], lr: 0.001000, loss: 0.9432
2022-10-10 18:42:19 - train: epoch 0079, iter [02100, 05004], lr: 0.001000, loss: 0.7786
2022-10-10 18:42:53 - train: epoch 0079, iter [02200, 05004], lr: 0.001000, loss: 0.7804
2022-10-10 18:43:29 - train: epoch 0079, iter [02300, 05004], lr: 0.001000, loss: 0.6876
2022-10-10 18:44:03 - train: epoch 0079, iter [02400, 05004], lr: 0.001000, loss: 0.8294
2022-10-10 18:44:38 - train: epoch 0079, iter [02500, 05004], lr: 0.001000, loss: 0.7055
2022-10-10 18:45:13 - train: epoch 0079, iter [02600, 05004], lr: 0.001000, loss: 0.7564
2022-10-10 18:45:48 - train: epoch 0079, iter [02700, 05004], lr: 0.001000, loss: 0.6007
2022-10-10 18:46:23 - train: epoch 0079, iter [02800, 05004], lr: 0.001000, loss: 0.7241
2022-10-10 18:46:57 - train: epoch 0079, iter [02900, 05004], lr: 0.001000, loss: 0.6845
2022-10-10 18:47:37 - train: epoch 0079, iter [03000, 05004], lr: 0.001000, loss: 0.9095
2022-10-10 18:48:13 - train: epoch 0079, iter [03100, 05004], lr: 0.001000, loss: 0.8302
2022-10-10 18:48:47 - train: epoch 0079, iter [03200, 05004], lr: 0.001000, loss: 0.7002
2022-10-10 18:49:22 - train: epoch 0079, iter [03300, 05004], lr: 0.001000, loss: 0.7998
2022-10-10 18:49:56 - train: epoch 0079, iter [03400, 05004], lr: 0.001000, loss: 0.7824
2022-10-10 18:50:32 - train: epoch 0079, iter [03500, 05004], lr: 0.001000, loss: 0.7913
2022-10-10 18:51:06 - train: epoch 0079, iter [03600, 05004], lr: 0.001000, loss: 0.7294
2022-10-10 18:51:40 - train: epoch 0079, iter [03700, 05004], lr: 0.001000, loss: 0.8145
2022-10-10 18:52:15 - train: epoch 0079, iter [03800, 05004], lr: 0.001000, loss: 0.8210
2022-10-10 18:52:49 - train: epoch 0079, iter [03900, 05004], lr: 0.001000, loss: 0.7509
2022-10-10 18:53:47 - train: epoch 0079, iter [04000, 05004], lr: 0.001000, loss: 0.7116
2022-10-10 18:54:39 - train: epoch 0079, iter [04100, 05004], lr: 0.001000, loss: 0.9095
2022-10-10 18:55:13 - train: epoch 0079, iter [04200, 05004], lr: 0.001000, loss: 0.6906
2022-10-10 18:55:48 - train: epoch 0079, iter [04300, 05004], lr: 0.001000, loss: 0.6253
2022-10-10 18:56:22 - train: epoch 0079, iter [04400, 05004], lr: 0.001000, loss: 0.8432
2022-10-10 18:56:55 - train: epoch 0079, iter [04500, 05004], lr: 0.001000, loss: 1.0387
2022-10-10 18:57:45 - train: epoch 0079, iter [04600, 05004], lr: 0.001000, loss: 0.6136
2022-10-10 18:58:39 - train: epoch 0079, iter [04700, 05004], lr: 0.001000, loss: 0.6664
2022-10-10 18:59:37 - train: epoch 0079, iter [04800, 05004], lr: 0.001000, loss: 0.8327
2022-10-10 19:01:12 - train: epoch 0079, iter [04900, 05004], lr: 0.001000, loss: 0.8088
2022-10-10 19:02:31 - train: epoch 0079, iter [05000, 05004], lr: 0.001000, loss: 0.5750
2022-10-10 19:02:33 - train: epoch 079, train_loss: 0.7630
2022-10-10 19:03:50 - eval: epoch: 079, acc1: 76.526%, acc5: 93.326%, test_loss: 0.9305, per_image_load_time: 0.752ms, per_image_inference_time: 0.561ms
2022-10-10 19:03:50 - until epoch: 079, best_acc1: 76.632%
2022-10-10 19:03:50 - epoch 080 lr: 0.001000
2022-10-10 19:04:30 - train: epoch 0080, iter [00100, 05004], lr: 0.001000, loss: 0.7693
2022-10-10 19:05:04 - train: epoch 0080, iter [00200, 05004], lr: 0.001000, loss: 0.8857
2022-10-10 19:05:39 - train: epoch 0080, iter [00300, 05004], lr: 0.001000, loss: 1.0657
2022-10-10 19:06:15 - train: epoch 0080, iter [00400, 05004], lr: 0.001000, loss: 0.6712
2022-10-10 19:06:49 - train: epoch 0080, iter [00500, 05004], lr: 0.001000, loss: 0.7117
2022-10-10 19:07:22 - train: epoch 0080, iter [00600, 05004], lr: 0.001000, loss: 0.7250
2022-10-10 19:07:57 - train: epoch 0080, iter [00700, 05004], lr: 0.001000, loss: 0.6551
2022-10-10 19:08:32 - train: epoch 0080, iter [00800, 05004], lr: 0.001000, loss: 0.6455
2022-10-10 19:09:05 - train: epoch 0080, iter [00900, 05004], lr: 0.001000, loss: 0.7120
2022-10-10 19:09:40 - train: epoch 0080, iter [01000, 05004], lr: 0.001000, loss: 0.6925
2022-10-10 19:10:15 - train: epoch 0080, iter [01100, 05004], lr: 0.001000, loss: 0.7871
2022-10-10 19:10:50 - train: epoch 0080, iter [01200, 05004], lr: 0.001000, loss: 0.8357
2022-10-10 19:11:28 - train: epoch 0080, iter [01300, 05004], lr: 0.001000, loss: 0.7364
2022-10-10 19:12:03 - train: epoch 0080, iter [01400, 05004], lr: 0.001000, loss: 0.7893
2022-10-10 19:12:40 - train: epoch 0080, iter [01500, 05004], lr: 0.001000, loss: 0.7401
2022-10-10 19:13:14 - train: epoch 0080, iter [01600, 05004], lr: 0.001000, loss: 0.8308
2022-10-10 19:14:00 - train: epoch 0080, iter [01700, 05004], lr: 0.001000, loss: 0.8303
2022-10-10 19:14:41 - train: epoch 0080, iter [01800, 05004], lr: 0.001000, loss: 0.7339
2022-10-10 19:15:34 - train: epoch 0080, iter [01900, 05004], lr: 0.001000, loss: 0.6455
2022-10-10 19:16:24 - train: epoch 0080, iter [02000, 05004], lr: 0.001000, loss: 0.7285
2022-10-10 19:16:59 - train: epoch 0080, iter [02100, 05004], lr: 0.001000, loss: 0.7950
2022-10-10 19:17:33 - train: epoch 0080, iter [02200, 05004], lr: 0.001000, loss: 0.5613
2022-10-10 19:18:07 - train: epoch 0080, iter [02300, 05004], lr: 0.001000, loss: 0.6931
2022-10-10 19:18:42 - train: epoch 0080, iter [02400, 05004], lr: 0.001000, loss: 0.8242
2022-10-10 19:19:17 - train: epoch 0080, iter [02500, 05004], lr: 0.001000, loss: 0.6525
2022-10-10 19:19:52 - train: epoch 0080, iter [02600, 05004], lr: 0.001000, loss: 0.7117
2022-10-10 19:20:26 - train: epoch 0080, iter [02700, 05004], lr: 0.001000, loss: 0.6667
2022-10-10 19:21:05 - train: epoch 0080, iter [02800, 05004], lr: 0.001000, loss: 0.7578
2022-10-10 19:21:40 - train: epoch 0080, iter [02900, 05004], lr: 0.001000, loss: 0.7976
2022-10-10 19:22:15 - train: epoch 0080, iter [03000, 05004], lr: 0.001000, loss: 0.8363
2022-10-10 19:22:50 - train: epoch 0080, iter [03100, 05004], lr: 0.001000, loss: 0.7695
2022-10-10 19:23:24 - train: epoch 0080, iter [03200, 05004], lr: 0.001000, loss: 0.6084
2022-10-10 19:23:59 - train: epoch 0080, iter [03300, 05004], lr: 0.001000, loss: 0.8776
2022-10-10 19:24:35 - train: epoch 0080, iter [03400, 05004], lr: 0.001000, loss: 0.6515
2022-10-10 19:25:09 - train: epoch 0080, iter [03500, 05004], lr: 0.001000, loss: 0.7155
2022-10-10 19:25:44 - train: epoch 0080, iter [03600, 05004], lr: 0.001000, loss: 0.7293
2022-10-10 19:26:19 - train: epoch 0080, iter [03700, 05004], lr: 0.001000, loss: 0.8662
2022-10-10 19:26:54 - train: epoch 0080, iter [03800, 05004], lr: 0.001000, loss: 0.7961
2022-10-10 19:27:28 - train: epoch 0080, iter [03900, 05004], lr: 0.001000, loss: 0.7163
2022-10-10 19:28:03 - train: epoch 0080, iter [04000, 05004], lr: 0.001000, loss: 0.9382
2022-10-10 19:28:37 - train: epoch 0080, iter [04100, 05004], lr: 0.001000, loss: 0.8723
2022-10-10 19:29:11 - train: epoch 0080, iter [04200, 05004], lr: 0.001000, loss: 0.8561
2022-10-10 19:29:47 - train: epoch 0080, iter [04300, 05004], lr: 0.001000, loss: 0.9565
2022-10-10 19:30:22 - train: epoch 0080, iter [04400, 05004], lr: 0.001000, loss: 0.7572
2022-10-10 19:30:57 - train: epoch 0080, iter [04500, 05004], lr: 0.001000, loss: 0.7725
2022-10-10 19:31:30 - train: epoch 0080, iter [04600, 05004], lr: 0.001000, loss: 0.7900
2022-10-10 19:32:05 - train: epoch 0080, iter [04700, 05004], lr: 0.001000, loss: 0.7373
2022-10-10 19:32:40 - train: epoch 0080, iter [04800, 05004], lr: 0.001000, loss: 0.7692
2022-10-10 19:33:15 - train: epoch 0080, iter [04900, 05004], lr: 0.001000, loss: 0.7135
2022-10-10 19:33:48 - train: epoch 0080, iter [05000, 05004], lr: 0.001000, loss: 0.7729
2022-10-10 19:33:51 - train: epoch 080, train_loss: 0.7596
2022-10-10 19:35:10 - eval: epoch: 080, acc1: 76.514%, acc5: 93.262%, test_loss: 0.9330, per_image_load_time: 1.489ms, per_image_inference_time: 0.600ms
2022-10-10 19:35:10 - until epoch: 080, best_acc1: 76.632%
2022-10-10 19:35:10 - epoch 081 lr: 0.001000
2022-10-10 19:35:56 - train: epoch 0081, iter [00100, 05004], lr: 0.001000, loss: 0.6895
2022-10-10 19:36:30 - train: epoch 0081, iter [00200, 05004], lr: 0.001000, loss: 0.7026
2022-10-10 19:37:05 - train: epoch 0081, iter [00300, 05004], lr: 0.001000, loss: 0.7506
2022-10-10 19:37:41 - train: epoch 0081, iter [00400, 05004], lr: 0.001000, loss: 0.6763
2022-10-10 19:38:15 - train: epoch 0081, iter [00500, 05004], lr: 0.001000, loss: 0.8264
2022-10-10 19:38:49 - train: epoch 0081, iter [00600, 05004], lr: 0.001000, loss: 0.6108
2022-10-10 19:39:23 - train: epoch 0081, iter [00700, 05004], lr: 0.001000, loss: 0.6622
2022-10-10 19:39:58 - train: epoch 0081, iter [00800, 05004], lr: 0.001000, loss: 0.7440
2022-10-10 19:40:32 - train: epoch 0081, iter [00900, 05004], lr: 0.001000, loss: 0.7885
2022-10-10 19:41:07 - train: epoch 0081, iter [01000, 05004], lr: 0.001000, loss: 0.7992
2022-10-10 19:41:41 - train: epoch 0081, iter [01100, 05004], lr: 0.001000, loss: 0.7427
2022-10-10 19:42:15 - train: epoch 0081, iter [01200, 05004], lr: 0.001000, loss: 0.6981
2022-10-10 19:42:50 - train: epoch 0081, iter [01300, 05004], lr: 0.001000, loss: 0.7420
2022-10-10 19:43:25 - train: epoch 0081, iter [01400, 05004], lr: 0.001000, loss: 0.5855
2022-10-10 19:43:59 - train: epoch 0081, iter [01500, 05004], lr: 0.001000, loss: 0.8554
2022-10-10 19:44:34 - train: epoch 0081, iter [01600, 05004], lr: 0.001000, loss: 0.8141
2022-10-10 19:45:08 - train: epoch 0081, iter [01700, 05004], lr: 0.001000, loss: 0.6902
2022-10-10 19:45:43 - train: epoch 0081, iter [01800, 05004], lr: 0.001000, loss: 0.8407
2022-10-10 19:46:17 - train: epoch 0081, iter [01900, 05004], lr: 0.001000, loss: 0.8626
2022-10-10 19:46:51 - train: epoch 0081, iter [02000, 05004], lr: 0.001000, loss: 0.9662
2022-10-10 19:47:27 - train: epoch 0081, iter [02100, 05004], lr: 0.001000, loss: 0.7511
2022-10-10 19:48:00 - train: epoch 0081, iter [02200, 05004], lr: 0.001000, loss: 0.8308
2022-10-10 19:48:34 - train: epoch 0081, iter [02300, 05004], lr: 0.001000, loss: 0.7919
2022-10-10 19:49:08 - train: epoch 0081, iter [02400, 05004], lr: 0.001000, loss: 0.7479
2022-10-10 19:49:44 - train: epoch 0081, iter [02500, 05004], lr: 0.001000, loss: 0.8622
2022-10-10 19:50:18 - train: epoch 0081, iter [02600, 05004], lr: 0.001000, loss: 0.7303
2022-10-10 19:50:53 - train: epoch 0081, iter [02700, 05004], lr: 0.001000, loss: 0.7510
2022-10-10 19:51:28 - train: epoch 0081, iter [02800, 05004], lr: 0.001000, loss: 0.8767
2022-10-10 19:52:02 - train: epoch 0081, iter [02900, 05004], lr: 0.001000, loss: 0.8682
2022-10-10 19:52:37 - train: epoch 0081, iter [03000, 05004], lr: 0.001000, loss: 0.6415
2022-10-10 19:53:11 - train: epoch 0081, iter [03100, 05004], lr: 0.001000, loss: 0.6940
2022-10-10 19:53:46 - train: epoch 0081, iter [03200, 05004], lr: 0.001000, loss: 0.7997
2022-10-10 19:54:20 - train: epoch 0081, iter [03300, 05004], lr: 0.001000, loss: 0.8014
2022-10-10 19:54:59 - train: epoch 0081, iter [03400, 05004], lr: 0.001000, loss: 0.8367
2022-10-10 19:56:54 - train: epoch 0081, iter [03500, 05004], lr: 0.001000, loss: 0.9288
2022-10-10 19:58:21 - train: epoch 0081, iter [03600, 05004], lr: 0.001000, loss: 0.7658
2022-10-10 19:59:31 - train: epoch 0081, iter [03700, 05004], lr: 0.001000, loss: 0.7630
2022-10-10 20:00:04 - train: epoch 0081, iter [03800, 05004], lr: 0.001000, loss: 0.8114
2022-10-10 20:00:39 - train: epoch 0081, iter [03900, 05004], lr: 0.001000, loss: 0.8118
2022-10-10 20:01:13 - train: epoch 0081, iter [04000, 05004], lr: 0.001000, loss: 0.6543
2022-10-10 20:01:47 - train: epoch 0081, iter [04100, 05004], lr: 0.001000, loss: 0.6794
2022-10-10 20:02:21 - train: epoch 0081, iter [04200, 05004], lr: 0.001000, loss: 0.9096
2022-10-10 20:02:55 - train: epoch 0081, iter [04300, 05004], lr: 0.001000, loss: 0.8851
2022-10-10 20:03:31 - train: epoch 0081, iter [04400, 05004], lr: 0.001000, loss: 0.8010
2022-10-10 20:04:07 - train: epoch 0081, iter [04500, 05004], lr: 0.001000, loss: 0.7857
2022-10-10 20:04:42 - train: epoch 0081, iter [04600, 05004], lr: 0.001000, loss: 0.7370
2022-10-10 20:05:16 - train: epoch 0081, iter [04700, 05004], lr: 0.001000, loss: 0.8519
2022-10-10 20:05:52 - train: epoch 0081, iter [04800, 05004], lr: 0.001000, loss: 0.6773
2022-10-10 20:06:26 - train: epoch 0081, iter [04900, 05004], lr: 0.001000, loss: 0.5769
2022-10-10 20:06:59 - train: epoch 0081, iter [05000, 05004], lr: 0.001000, loss: 0.7874
2022-10-10 20:07:01 - train: epoch 081, train_loss: 0.7557
2022-10-10 20:08:20 - eval: epoch: 081, acc1: 76.460%, acc5: 93.290%, test_loss: 0.9322, per_image_load_time: 1.863ms, per_image_inference_time: 0.613ms
2022-10-10 20:08:20 - until epoch: 081, best_acc1: 76.632%
2022-10-10 20:08:20 - epoch 082 lr: 0.001000
2022-10-10 20:09:02 - train: epoch 0082, iter [00100, 05004], lr: 0.001000, loss: 0.6989
2022-10-10 20:09:37 - train: epoch 0082, iter [00200, 05004], lr: 0.001000, loss: 0.7725
2022-10-10 20:10:12 - train: epoch 0082, iter [00300, 05004], lr: 0.001000, loss: 0.8695
2022-10-10 20:10:48 - train: epoch 0082, iter [00400, 05004], lr: 0.001000, loss: 0.7413
2022-10-10 20:11:24 - train: epoch 0082, iter [00500, 05004], lr: 0.001000, loss: 0.8722
2022-10-10 20:11:59 - train: epoch 0082, iter [00600, 05004], lr: 0.001000, loss: 0.7003
2022-10-10 20:12:35 - train: epoch 0082, iter [00700, 05004], lr: 0.001000, loss: 0.9218
2022-10-10 20:13:09 - train: epoch 0082, iter [00800, 05004], lr: 0.001000, loss: 0.8191
2022-10-10 20:13:45 - train: epoch 0082, iter [00900, 05004], lr: 0.001000, loss: 0.6989
2022-10-10 20:14:19 - train: epoch 0082, iter [01000, 05004], lr: 0.001000, loss: 0.7570
2022-10-10 20:14:54 - train: epoch 0082, iter [01100, 05004], lr: 0.001000, loss: 0.8727
2022-10-10 20:15:27 - train: epoch 0082, iter [01200, 05004], lr: 0.001000, loss: 0.7329
2022-10-10 20:16:02 - train: epoch 0082, iter [01300, 05004], lr: 0.001000, loss: 0.8876
2022-10-10 20:16:36 - train: epoch 0082, iter [01400, 05004], lr: 0.001000, loss: 0.7991
2022-10-10 20:17:10 - train: epoch 0082, iter [01500, 05004], lr: 0.001000, loss: 0.7909
2022-10-10 20:17:43 - train: epoch 0082, iter [01600, 05004], lr: 0.001000, loss: 0.8438
2022-10-10 20:18:17 - train: epoch 0082, iter [01700, 05004], lr: 0.001000, loss: 0.7479
2022-10-10 20:18:52 - train: epoch 0082, iter [01800, 05004], lr: 0.001000, loss: 0.6964
2022-10-10 20:19:27 - train: epoch 0082, iter [01900, 05004], lr: 0.001000, loss: 0.6437
2022-10-10 20:20:02 - train: epoch 0082, iter [02000, 05004], lr: 0.001000, loss: 0.6372
2022-10-10 20:20:36 - train: epoch 0082, iter [02100, 05004], lr: 0.001000, loss: 0.7265
2022-10-10 20:21:14 - train: epoch 0082, iter [02200, 05004], lr: 0.001000, loss: 0.8213
2022-10-10 20:21:49 - train: epoch 0082, iter [02300, 05004], lr: 0.001000, loss: 0.6876
2022-10-10 20:22:35 - train: epoch 0082, iter [02400, 05004], lr: 0.001000, loss: 0.9095
2022-10-10 20:24:12 - train: epoch 0082, iter [02500, 05004], lr: 0.001000, loss: 0.8151
2022-10-10 20:24:57 - train: epoch 0082, iter [02600, 05004], lr: 0.001000, loss: 0.7748
2022-10-10 20:25:31 - train: epoch 0082, iter [02700, 05004], lr: 0.001000, loss: 0.6132
2022-10-10 20:26:05 - train: epoch 0082, iter [02800, 05004], lr: 0.001000, loss: 0.6628
2022-10-10 20:26:43 - train: epoch 0082, iter [02900, 05004], lr: 0.001000, loss: 0.6194
2022-10-10 20:27:16 - train: epoch 0082, iter [03000, 05004], lr: 0.001000, loss: 0.6308
2022-10-10 20:27:55 - train: epoch 0082, iter [03100, 05004], lr: 0.001000, loss: 0.8254
2022-10-10 20:28:41 - train: epoch 0082, iter [03200, 05004], lr: 0.001000, loss: 0.7595
2022-10-10 20:29:19 - train: epoch 0082, iter [03300, 05004], lr: 0.001000, loss: 0.8393
2022-10-10 20:30:10 - train: epoch 0082, iter [03400, 05004], lr: 0.001000, loss: 0.8320
2022-10-10 20:30:52 - train: epoch 0082, iter [03500, 05004], lr: 0.001000, loss: 0.6207
2022-10-10 20:31:39 - train: epoch 0082, iter [03600, 05004], lr: 0.001000, loss: 0.6678
2022-10-10 20:32:36 - train: epoch 0082, iter [03700, 05004], lr: 0.001000, loss: 0.7454
2022-10-10 20:33:11 - train: epoch 0082, iter [03800, 05004], lr: 0.001000, loss: 0.9251
2022-10-10 20:33:47 - train: epoch 0082, iter [03900, 05004], lr: 0.001000, loss: 0.7915
2022-10-10 20:34:21 - train: epoch 0082, iter [04000, 05004], lr: 0.001000, loss: 0.7140
2022-10-10 20:35:05 - train: epoch 0082, iter [04100, 05004], lr: 0.001000, loss: 0.7417
2022-10-10 20:35:43 - train: epoch 0082, iter [04200, 05004], lr: 0.001000, loss: 0.8341
2022-10-10 20:36:17 - train: epoch 0082, iter [04300, 05004], lr: 0.001000, loss: 0.6951
2022-10-10 20:37:06 - train: epoch 0082, iter [04400, 05004], lr: 0.001000, loss: 0.7713
2022-10-10 20:37:38 - train: epoch 0082, iter [04500, 05004], lr: 0.001000, loss: 0.6293
2022-10-10 20:38:21 - train: epoch 0082, iter [04600, 05004], lr: 0.001000, loss: 0.7557
2022-10-10 20:38:56 - train: epoch 0082, iter [04700, 05004], lr: 0.001000, loss: 0.6593
2022-10-10 20:39:32 - train: epoch 0082, iter [04800, 05004], lr: 0.001000, loss: 0.6524
2022-10-10 20:40:34 - train: epoch 0082, iter [04900, 05004], lr: 0.001000, loss: 0.8323
2022-10-10 20:41:32 - train: epoch 0082, iter [05000, 05004], lr: 0.001000, loss: 0.7439
2022-10-10 20:41:34 - train: epoch 082, train_loss: 0.7515
2022-10-10 20:42:53 - eval: epoch: 082, acc1: 76.444%, acc5: 93.332%, test_loss: 0.9332, per_image_load_time: 2.326ms, per_image_inference_time: 0.562ms
2022-10-10 20:42:53 - until epoch: 082, best_acc1: 76.632%
2022-10-10 20:42:53 - epoch 083 lr: 0.001000
2022-10-10 20:43:35 - train: epoch 0083, iter [00100, 05004], lr: 0.001000, loss: 0.6696
2022-10-10 20:44:10 - train: epoch 0083, iter [00200, 05004], lr: 0.001000, loss: 0.5908
2022-10-10 20:44:45 - train: epoch 0083, iter [00300, 05004], lr: 0.001000, loss: 0.8662
2022-10-10 20:45:18 - train: epoch 0083, iter [00400, 05004], lr: 0.001000, loss: 0.9178
2022-10-10 20:45:53 - train: epoch 0083, iter [00500, 05004], lr: 0.001000, loss: 0.7075
2022-10-10 20:46:30 - train: epoch 0083, iter [00600, 05004], lr: 0.001000, loss: 0.8728
2022-10-10 20:47:06 - train: epoch 0083, iter [00700, 05004], lr: 0.001000, loss: 0.6340
2022-10-10 20:47:40 - train: epoch 0083, iter [00800, 05004], lr: 0.001000, loss: 0.6233
2022-10-10 20:48:18 - train: epoch 0083, iter [00900, 05004], lr: 0.001000, loss: 0.7303
2022-10-10 20:48:53 - train: epoch 0083, iter [01000, 05004], lr: 0.001000, loss: 0.8658
2022-10-10 20:49:27 - train: epoch 0083, iter [01100, 05004], lr: 0.001000, loss: 0.7911
2022-10-10 20:50:02 - train: epoch 0083, iter [01200, 05004], lr: 0.001000, loss: 0.6465
2022-10-10 20:50:36 - train: epoch 0083, iter [01300, 05004], lr: 0.001000, loss: 0.7480
2022-10-10 20:51:20 - train: epoch 0083, iter [01400, 05004], lr: 0.001000, loss: 0.9392
2022-10-10 20:51:57 - train: epoch 0083, iter [01500, 05004], lr: 0.001000, loss: 0.6163
2022-10-10 20:52:32 - train: epoch 0083, iter [01600, 05004], lr: 0.001000, loss: 0.8138
2022-10-10 20:53:06 - train: epoch 0083, iter [01700, 05004], lr: 0.001000, loss: 0.6677
2022-10-10 20:53:40 - train: epoch 0083, iter [01800, 05004], lr: 0.001000, loss: 0.8438
2022-10-10 20:54:14 - train: epoch 0083, iter [01900, 05004], lr: 0.001000, loss: 0.8065
2022-10-10 20:54:48 - train: epoch 0083, iter [02000, 05004], lr: 0.001000, loss: 0.4719
2022-10-10 20:55:22 - train: epoch 0083, iter [02100, 05004], lr: 0.001000, loss: 0.6448
2022-10-10 20:55:57 - train: epoch 0083, iter [02200, 05004], lr: 0.001000, loss: 0.5697
2022-10-10 20:56:32 - train: epoch 0083, iter [02300, 05004], lr: 0.001000, loss: 0.8046
2022-10-10 20:57:27 - train: epoch 0083, iter [02400, 05004], lr: 0.001000, loss: 0.8075
2022-10-10 20:58:11 - train: epoch 0083, iter [02500, 05004], lr: 0.001000, loss: 0.6039
2022-10-10 20:58:46 - train: epoch 0083, iter [02600, 05004], lr: 0.001000, loss: 0.7624
2022-10-10 20:59:20 - train: epoch 0083, iter [02700, 05004], lr: 0.001000, loss: 0.5980
2022-10-10 20:59:54 - train: epoch 0083, iter [02800, 05004], lr: 0.001000, loss: 0.7367
2022-10-10 21:00:30 - train: epoch 0083, iter [02900, 05004], lr: 0.001000, loss: 0.6438
2022-10-10 21:01:03 - train: epoch 0083, iter [03000, 05004], lr: 0.001000, loss: 0.7001
2022-10-10 21:01:37 - train: epoch 0083, iter [03100, 05004], lr: 0.001000, loss: 0.5795
2022-10-10 21:02:16 - train: epoch 0083, iter [03200, 05004], lr: 0.001000, loss: 0.7913
2022-10-10 21:02:59 - train: epoch 0083, iter [03300, 05004], lr: 0.001000, loss: 0.7747
2022-10-10 21:03:33 - train: epoch 0083, iter [03400, 05004], lr: 0.001000, loss: 0.7723
2022-10-10 21:04:14 - train: epoch 0083, iter [03500, 05004], lr: 0.001000, loss: 0.8204
2022-10-10 21:04:50 - train: epoch 0083, iter [03600, 05004], lr: 0.001000, loss: 0.8694
2022-10-10 21:05:23 - train: epoch 0083, iter [03700, 05004], lr: 0.001000, loss: 0.7045
2022-10-10 21:05:57 - train: epoch 0083, iter [03800, 05004], lr: 0.001000, loss: 0.6533
2022-10-10 21:06:32 - train: epoch 0083, iter [03900, 05004], lr: 0.001000, loss: 0.6744
2022-10-10 21:07:06 - train: epoch 0083, iter [04000, 05004], lr: 0.001000, loss: 0.8469
2022-10-10 21:07:42 - train: epoch 0083, iter [04100, 05004], lr: 0.001000, loss: 0.7843
2022-10-10 21:08:15 - train: epoch 0083, iter [04200, 05004], lr: 0.001000, loss: 0.7947
2022-10-10 21:08:50 - train: epoch 0083, iter [04300, 05004], lr: 0.001000, loss: 0.7566
2022-10-10 21:09:24 - train: epoch 0083, iter [04400, 05004], lr: 0.001000, loss: 0.7662
2022-10-10 21:10:01 - train: epoch 0083, iter [04500, 05004], lr: 0.001000, loss: 0.9189
2022-10-10 21:10:36 - train: epoch 0083, iter [04600, 05004], lr: 0.001000, loss: 0.8689
2022-10-10 21:11:11 - train: epoch 0083, iter [04700, 05004], lr: 0.001000, loss: 0.9746
2022-10-10 21:11:45 - train: epoch 0083, iter [04800, 05004], lr: 0.001000, loss: 0.8052
2022-10-10 21:12:20 - train: epoch 0083, iter [04900, 05004], lr: 0.001000, loss: 0.8279
2022-10-10 21:12:52 - train: epoch 0083, iter [05000, 05004], lr: 0.001000, loss: 0.7418
2022-10-10 21:12:54 - train: epoch 083, train_loss: 0.7474
2022-10-10 21:14:12 - eval: epoch: 083, acc1: 76.520%, acc5: 93.314%, test_loss: 0.9341, per_image_load_time: 1.446ms, per_image_inference_time: 0.574ms
2022-10-10 21:14:12 - until epoch: 083, best_acc1: 76.632%
2022-10-10 21:14:12 - epoch 084 lr: 0.001000
2022-10-10 21:14:52 - train: epoch 0084, iter [00100, 05004], lr: 0.001000, loss: 0.6360
2022-10-10 21:15:27 - train: epoch 0084, iter [00200, 05004], lr: 0.001000, loss: 0.6392
2022-10-10 21:16:01 - train: epoch 0084, iter [00300, 05004], lr: 0.001000, loss: 0.5095
2022-10-10 21:16:35 - train: epoch 0084, iter [00400, 05004], lr: 0.001000, loss: 0.6935
2022-10-10 21:17:09 - train: epoch 0084, iter [00500, 05004], lr: 0.001000, loss: 0.9070
2022-10-10 21:17:44 - train: epoch 0084, iter [00600, 05004], lr: 0.001000, loss: 0.8140
2022-10-10 21:18:18 - train: epoch 0084, iter [00700, 05004], lr: 0.001000, loss: 0.8939
2022-10-10 21:18:53 - train: epoch 0084, iter [00800, 05004], lr: 0.001000, loss: 0.7729
2022-10-10 21:19:26 - train: epoch 0084, iter [00900, 05004], lr: 0.001000, loss: 0.7776
2022-10-10 21:20:01 - train: epoch 0084, iter [01000, 05004], lr: 0.001000, loss: 0.7097
2022-10-10 21:20:36 - train: epoch 0084, iter [01100, 05004], lr: 0.001000, loss: 0.6678
2022-10-10 21:21:10 - train: epoch 0084, iter [01200, 05004], lr: 0.001000, loss: 0.9120
2022-10-10 21:21:45 - train: epoch 0084, iter [01300, 05004], lr: 0.001000, loss: 0.7555
2022-10-10 21:22:19 - train: epoch 0084, iter [01400, 05004], lr: 0.001000, loss: 0.8464
2022-10-10 21:22:54 - train: epoch 0084, iter [01500, 05004], lr: 0.001000, loss: 0.7639
2022-10-10 21:23:30 - train: epoch 0084, iter [01600, 05004], lr: 0.001000, loss: 0.6906
2022-10-10 21:24:05 - train: epoch 0084, iter [01700, 05004], lr: 0.001000, loss: 0.8153
2022-10-10 21:24:40 - train: epoch 0084, iter [01800, 05004], lr: 0.001000, loss: 0.8567
2022-10-10 21:25:14 - train: epoch 0084, iter [01900, 05004], lr: 0.001000, loss: 0.8467
2022-10-10 21:25:48 - train: epoch 0084, iter [02000, 05004], lr: 0.001000, loss: 0.5901
2022-10-10 21:26:24 - train: epoch 0084, iter [02100, 05004], lr: 0.001000, loss: 0.7514
2022-10-10 21:26:59 - train: epoch 0084, iter [02200, 05004], lr: 0.001000, loss: 0.8470
2022-10-10 21:27:34 - train: epoch 0084, iter [02300, 05004], lr: 0.001000, loss: 0.6318
2022-10-10 21:28:09 - train: epoch 0084, iter [02400, 05004], lr: 0.001000, loss: 0.5774
2022-10-10 21:28:43 - train: epoch 0084, iter [02500, 05004], lr: 0.001000, loss: 0.7961
2022-10-10 21:29:17 - train: epoch 0084, iter [02600, 05004], lr: 0.001000, loss: 0.6865
2022-10-10 21:29:52 - train: epoch 0084, iter [02700, 05004], lr: 0.001000, loss: 0.8385
2022-10-10 21:30:26 - train: epoch 0084, iter [02800, 05004], lr: 0.001000, loss: 0.7754
2022-10-10 21:31:00 - train: epoch 0084, iter [02900, 05004], lr: 0.001000, loss: 0.8994
2022-10-10 21:31:36 - train: epoch 0084, iter [03000, 05004], lr: 0.001000, loss: 0.8293
2022-10-10 21:32:10 - train: epoch 0084, iter [03100, 05004], lr: 0.001000, loss: 0.7602
2022-10-10 21:32:48 - train: epoch 0084, iter [03200, 05004], lr: 0.001000, loss: 0.7274
2022-10-10 21:33:27 - train: epoch 0084, iter [03300, 05004], lr: 0.001000, loss: 0.6089
2022-10-10 21:34:02 - train: epoch 0084, iter [03400, 05004], lr: 0.001000, loss: 0.7494
2022-10-10 21:34:36 - train: epoch 0084, iter [03500, 05004], lr: 0.001000, loss: 0.7941
2022-10-10 21:35:12 - train: epoch 0084, iter [03600, 05004], lr: 0.001000, loss: 0.7413
2022-10-10 21:35:46 - train: epoch 0084, iter [03700, 05004], lr: 0.001000, loss: 0.9152
2022-10-10 21:36:20 - train: epoch 0084, iter [03800, 05004], lr: 0.001000, loss: 0.6650
2022-10-10 21:37:01 - train: epoch 0084, iter [03900, 05004], lr: 0.001000, loss: 0.7700
2022-10-10 21:37:46 - train: epoch 0084, iter [04000, 05004], lr: 0.001000, loss: 0.6839
2022-10-10 21:38:20 - train: epoch 0084, iter [04100, 05004], lr: 0.001000, loss: 0.5409
2022-10-10 21:38:55 - train: epoch 0084, iter [04200, 05004], lr: 0.001000, loss: 0.7639
2022-10-10 21:39:48 - train: epoch 0084, iter [04300, 05004], lr: 0.001000, loss: 0.7933
2022-10-10 21:40:31 - train: epoch 0084, iter [04400, 05004], lr: 0.001000, loss: 1.0174
2022-10-10 21:41:06 - train: epoch 0084, iter [04500, 05004], lr: 0.001000, loss: 0.6102
2022-10-10 21:41:39 - train: epoch 0084, iter [04600, 05004], lr: 0.001000, loss: 0.7706
2022-10-10 21:42:33 - train: epoch 0084, iter [04700, 05004], lr: 0.001000, loss: 0.6146
2022-10-10 21:43:11 - train: epoch 0084, iter [04800, 05004], lr: 0.001000, loss: 0.7036
2022-10-10 21:44:05 - train: epoch 0084, iter [04900, 05004], lr: 0.001000, loss: 0.6095
2022-10-10 21:44:43 - train: epoch 0084, iter [05000, 05004], lr: 0.001000, loss: 0.7314
2022-10-10 21:44:45 - train: epoch 084, train_loss: 0.7471
2022-10-10 21:46:05 - eval: epoch: 084, acc1: 76.440%, acc5: 93.310%, test_loss: 0.9365, per_image_load_time: 1.886ms, per_image_inference_time: 0.600ms
2022-10-10 21:46:05 - until epoch: 084, best_acc1: 76.632%
2022-10-10 21:46:05 - epoch 085 lr: 0.001000
2022-10-10 21:46:47 - train: epoch 0085, iter [00100, 05004], lr: 0.001000, loss: 0.6902
2022-10-10 21:47:22 - train: epoch 0085, iter [00200, 05004], lr: 0.001000, loss: 0.6914
2022-10-10 21:47:56 - train: epoch 0085, iter [00300, 05004], lr: 0.001000, loss: 0.7780
2022-10-10 21:48:31 - train: epoch 0085, iter [00400, 05004], lr: 0.001000, loss: 0.7666
2022-10-10 21:49:06 - train: epoch 0085, iter [00500, 05004], lr: 0.001000, loss: 0.8130
2022-10-10 21:49:40 - train: epoch 0085, iter [00600, 05004], lr: 0.001000, loss: 0.6530
2022-10-10 21:50:14 - train: epoch 0085, iter [00700, 05004], lr: 0.001000, loss: 0.8328
2022-10-10 21:50:49 - train: epoch 0085, iter [00800, 05004], lr: 0.001000, loss: 0.6673
2022-10-10 21:51:23 - train: epoch 0085, iter [00900, 05004], lr: 0.001000, loss: 0.7521
2022-10-10 21:51:58 - train: epoch 0085, iter [01000, 05004], lr: 0.001000, loss: 0.6819
2022-10-10 21:52:32 - train: epoch 0085, iter [01100, 05004], lr: 0.001000, loss: 0.8409
2022-10-10 21:53:08 - train: epoch 0085, iter [01200, 05004], lr: 0.001000, loss: 0.6471
2022-10-10 21:53:43 - train: epoch 0085, iter [01300, 05004], lr: 0.001000, loss: 0.6557
2022-10-10 21:54:18 - train: epoch 0085, iter [01400, 05004], lr: 0.001000, loss: 0.7678
2022-10-10 21:54:52 - train: epoch 0085, iter [01500, 05004], lr: 0.001000, loss: 0.8256
2022-10-10 21:55:28 - train: epoch 0085, iter [01600, 05004], lr: 0.001000, loss: 0.6556
2022-10-10 21:56:03 - train: epoch 0085, iter [01700, 05004], lr: 0.001000, loss: 0.7869
2022-10-10 21:56:38 - train: epoch 0085, iter [01800, 05004], lr: 0.001000, loss: 0.5990
2022-10-10 21:57:13 - train: epoch 0085, iter [01900, 05004], lr: 0.001000, loss: 0.7175
2022-10-10 21:57:51 - train: epoch 0085, iter [02000, 05004], lr: 0.001000, loss: 0.5921
2022-10-10 21:58:26 - train: epoch 0085, iter [02100, 05004], lr: 0.001000, loss: 0.5977
2022-10-10 21:59:02 - train: epoch 0085, iter [02200, 05004], lr: 0.001000, loss: 0.6355
2022-10-10 21:59:44 - train: epoch 0085, iter [02300, 05004], lr: 0.001000, loss: 0.7189
2022-10-10 22:00:20 - train: epoch 0085, iter [02400, 05004], lr: 0.001000, loss: 0.7389
2022-10-10 22:01:00 - train: epoch 0085, iter [02500, 05004], lr: 0.001000, loss: 0.8773
2022-10-10 22:01:39 - train: epoch 0085, iter [02600, 05004], lr: 0.001000, loss: 0.6987
2022-10-10 22:02:33 - train: epoch 0085, iter [02700, 05004], lr: 0.001000, loss: 0.6693
2022-10-10 22:03:42 - train: epoch 0085, iter [02800, 05004], lr: 0.001000, loss: 0.7896
2022-10-10 22:04:23 - train: epoch 0085, iter [02900, 05004], lr: 0.001000, loss: 0.8793
2022-10-10 22:05:13 - train: epoch 0085, iter [03000, 05004], lr: 0.001000, loss: 0.8765
2022-10-10 22:05:50 - train: epoch 0085, iter [03100, 05004], lr: 0.001000, loss: 0.7452
2022-10-10 22:06:24 - train: epoch 0085, iter [03200, 05004], lr: 0.001000, loss: 0.7046
2022-10-10 22:06:59 - train: epoch 0085, iter [03300, 05004], lr: 0.001000, loss: 0.6939
2022-10-10 22:07:34 - train: epoch 0085, iter [03400, 05004], lr: 0.001000, loss: 0.8501
2022-10-10 22:08:13 - train: epoch 0085, iter [03500, 05004], lr: 0.001000, loss: 0.8475
2022-10-10 22:08:50 - train: epoch 0085, iter [03600, 05004], lr: 0.001000, loss: 0.7054
2022-10-10 22:09:26 - train: epoch 0085, iter [03700, 05004], lr: 0.001000, loss: 0.6329
2022-10-10 22:10:01 - train: epoch 0085, iter [03800, 05004], lr: 0.001000, loss: 0.7968
2022-10-10 22:10:36 - train: epoch 0085, iter [03900, 05004], lr: 0.001000, loss: 0.5794
2022-10-10 22:11:11 - train: epoch 0085, iter [04000, 05004], lr: 0.001000, loss: 0.9265
2022-10-10 22:11:49 - train: epoch 0085, iter [04100, 05004], lr: 0.001000, loss: 0.8777
2022-10-10 22:12:24 - train: epoch 0085, iter [04200, 05004], lr: 0.001000, loss: 0.7116
2022-10-10 22:12:58 - train: epoch 0085, iter [04300, 05004], lr: 0.001000, loss: 0.6848
2022-10-10 22:13:34 - train: epoch 0085, iter [04400, 05004], lr: 0.001000, loss: 0.7037
2022-10-10 22:14:08 - train: epoch 0085, iter [04500, 05004], lr: 0.001000, loss: 0.8228
2022-10-10 22:14:43 - train: epoch 0085, iter [04600, 05004], lr: 0.001000, loss: 0.7051
2022-10-10 22:15:17 - train: epoch 0085, iter [04700, 05004], lr: 0.001000, loss: 1.0142
2022-10-10 22:15:52 - train: epoch 0085, iter [04800, 05004], lr: 0.001000, loss: 0.6528
2022-10-10 22:16:26 - train: epoch 0085, iter [04900, 05004], lr: 0.001000, loss: 0.7365
2022-10-10 22:16:59 - train: epoch 0085, iter [05000, 05004], lr: 0.001000, loss: 0.4684
2022-10-10 22:17:02 - train: epoch 085, train_loss: 0.7417
2022-10-10 22:18:20 - eval: epoch: 085, acc1: 76.566%, acc5: 93.316%, test_loss: 0.9334, per_image_load_time: 1.352ms, per_image_inference_time: 0.584ms
2022-10-10 22:18:20 - until epoch: 085, best_acc1: 76.632%
2022-10-10 22:18:20 - epoch 086 lr: 0.001000
2022-10-10 22:19:01 - train: epoch 0086, iter [00100, 05004], lr: 0.001000, loss: 0.8055
2022-10-10 22:19:36 - train: epoch 0086, iter [00200, 05004], lr: 0.001000, loss: 0.7682
2022-10-10 22:20:10 - train: epoch 0086, iter [00300, 05004], lr: 0.001000, loss: 0.8421
2022-10-10 22:20:46 - train: epoch 0086, iter [00400, 05004], lr: 0.001000, loss: 0.8328
2022-10-10 22:21:35 - train: epoch 0086, iter [00500, 05004], lr: 0.001000, loss: 0.8048
2022-10-10 22:22:09 - train: epoch 0086, iter [00600, 05004], lr: 0.001000, loss: 0.8120
2022-10-10 22:22:44 - train: epoch 0086, iter [00700, 05004], lr: 0.001000, loss: 0.7798
2022-10-10 22:23:17 - train: epoch 0086, iter [00800, 05004], lr: 0.001000, loss: 0.8093
2022-10-10 22:23:51 - train: epoch 0086, iter [00900, 05004], lr: 0.001000, loss: 0.6537
2022-10-10 22:24:27 - train: epoch 0086, iter [01000, 05004], lr: 0.001000, loss: 0.6571
2022-10-10 22:25:02 - train: epoch 0086, iter [01100, 05004], lr: 0.001000, loss: 0.7455
2022-10-10 22:25:36 - train: epoch 0086, iter [01200, 05004], lr: 0.001000, loss: 0.7303
2022-10-10 22:26:13 - train: epoch 0086, iter [01300, 05004], lr: 0.001000, loss: 0.9030
2022-10-10 22:26:48 - train: epoch 0086, iter [01400, 05004], lr: 0.001000, loss: 0.6112
2022-10-10 22:27:22 - train: epoch 0086, iter [01500, 05004], lr: 0.001000, loss: 0.5823
2022-10-10 22:27:57 - train: epoch 0086, iter [01600, 05004], lr: 0.001000, loss: 0.8336
2022-10-10 22:28:32 - train: epoch 0086, iter [01700, 05004], lr: 0.001000, loss: 0.8744
2022-10-10 22:29:08 - train: epoch 0086, iter [01800, 05004], lr: 0.001000, loss: 0.6655
2022-10-10 22:29:42 - train: epoch 0086, iter [01900, 05004], lr: 0.001000, loss: 0.6622
2022-10-10 22:30:18 - train: epoch 0086, iter [02000, 05004], lr: 0.001000, loss: 1.0121
2022-10-10 22:30:53 - train: epoch 0086, iter [02100, 05004], lr: 0.001000, loss: 0.6482
2022-10-10 22:31:26 - train: epoch 0086, iter [02200, 05004], lr: 0.001000, loss: 0.7800
2022-10-10 22:32:03 - train: epoch 0086, iter [02300, 05004], lr: 0.001000, loss: 0.8012
2022-10-10 22:32:38 - train: epoch 0086, iter [02400, 05004], lr: 0.001000, loss: 0.5479
2022-10-10 22:33:13 - train: epoch 0086, iter [02500, 05004], lr: 0.001000, loss: 0.7412
2022-10-10 22:33:48 - train: epoch 0086, iter [02600, 05004], lr: 0.001000, loss: 0.8958
2022-10-10 22:34:23 - train: epoch 0086, iter [02700, 05004], lr: 0.001000, loss: 0.7645
2022-10-10 22:34:59 - train: epoch 0086, iter [02800, 05004], lr: 0.001000, loss: 0.9104
2022-10-10 22:35:34 - train: epoch 0086, iter [02900, 05004], lr: 0.001000, loss: 0.6078
2022-10-10 22:36:10 - train: epoch 0086, iter [03000, 05004], lr: 0.001000, loss: 0.7142
2022-10-10 22:36:45 - train: epoch 0086, iter [03100, 05004], lr: 0.001000, loss: 0.7075
2022-10-10 22:37:21 - train: epoch 0086, iter [03200, 05004], lr: 0.001000, loss: 0.8575
2022-10-10 22:37:55 - train: epoch 0086, iter [03300, 05004], lr: 0.001000, loss: 0.7054
2022-10-10 22:38:29 - train: epoch 0086, iter [03400, 05004], lr: 0.001000, loss: 0.7777
2022-10-10 22:39:03 - train: epoch 0086, iter [03500, 05004], lr: 0.001000, loss: 0.7466
2022-10-10 22:39:38 - train: epoch 0086, iter [03600, 05004], lr: 0.001000, loss: 0.7114
2022-10-10 22:40:13 - train: epoch 0086, iter [03700, 05004], lr: 0.001000, loss: 0.7227
2022-10-10 22:40:47 - train: epoch 0086, iter [03800, 05004], lr: 0.001000, loss: 0.6960
2022-10-10 22:41:22 - train: epoch 0086, iter [03900, 05004], lr: 0.001000, loss: 0.7476
2022-10-10 22:41:56 - train: epoch 0086, iter [04000, 05004], lr: 0.001000, loss: 0.5506
2022-10-10 22:42:32 - train: epoch 0086, iter [04100, 05004], lr: 0.001000, loss: 0.6765
2022-10-10 22:43:08 - train: epoch 0086, iter [04200, 05004], lr: 0.001000, loss: 0.6434
2022-10-10 22:43:42 - train: epoch 0086, iter [04300, 05004], lr: 0.001000, loss: 0.7771
2022-10-10 22:44:17 - train: epoch 0086, iter [04400, 05004], lr: 0.001000, loss: 0.8696
2022-10-10 22:44:52 - train: epoch 0086, iter [04500, 05004], lr: 0.001000, loss: 0.7002
2022-10-10 22:45:26 - train: epoch 0086, iter [04600, 05004], lr: 0.001000, loss: 0.8094
2022-10-10 22:46:01 - train: epoch 0086, iter [04700, 05004], lr: 0.001000, loss: 0.5532
2022-10-10 22:46:36 - train: epoch 0086, iter [04800, 05004], lr: 0.001000, loss: 0.6867
2022-10-10 22:47:13 - train: epoch 0086, iter [04900, 05004], lr: 0.001000, loss: 0.6542
2022-10-10 22:47:48 - train: epoch 0086, iter [05000, 05004], lr: 0.001000, loss: 0.8414
2022-10-10 22:47:52 - train: epoch 086, train_loss: 0.7391
2022-10-10 22:49:11 - eval: epoch: 086, acc1: 76.572%, acc5: 93.310%, test_loss: 0.9375, per_image_load_time: 2.332ms, per_image_inference_time: 0.551ms
2022-10-10 22:49:11 - until epoch: 086, best_acc1: 76.632%
2022-10-10 22:49:11 - epoch 087 lr: 0.001000
2022-10-10 22:49:53 - train: epoch 0087, iter [00100, 05004], lr: 0.001000, loss: 0.7660
2022-10-10 22:50:28 - train: epoch 0087, iter [00200, 05004], lr: 0.001000, loss: 0.7503
2022-10-10 22:51:03 - train: epoch 0087, iter [00300, 05004], lr: 0.001000, loss: 0.6154
2022-10-10 22:51:37 - train: epoch 0087, iter [00400, 05004], lr: 0.001000, loss: 0.7995
2022-10-10 22:52:14 - train: epoch 0087, iter [00500, 05004], lr: 0.001000, loss: 0.5903
2022-10-10 22:52:49 - train: epoch 0087, iter [00600, 05004], lr: 0.001000, loss: 0.9118
2022-10-10 22:53:24 - train: epoch 0087, iter [00700, 05004], lr: 0.001000, loss: 0.6981
2022-10-10 22:53:59 - train: epoch 0087, iter [00800, 05004], lr: 0.001000, loss: 0.7738
2022-10-10 22:54:33 - train: epoch 0087, iter [00900, 05004], lr: 0.001000, loss: 0.7001
2022-10-10 22:55:08 - train: epoch 0087, iter [01000, 05004], lr: 0.001000, loss: 0.5067
2022-10-10 22:55:44 - train: epoch 0087, iter [01100, 05004], lr: 0.001000, loss: 0.5899
2022-10-10 22:56:18 - train: epoch 0087, iter [01200, 05004], lr: 0.001000, loss: 0.6871
2022-10-10 22:56:53 - train: epoch 0087, iter [01300, 05004], lr: 0.001000, loss: 0.7393
2022-10-10 22:57:28 - train: epoch 0087, iter [01400, 05004], lr: 0.001000, loss: 0.6083
2022-10-10 22:58:02 - train: epoch 0087, iter [01500, 05004], lr: 0.001000, loss: 0.6523
2022-10-10 22:58:37 - train: epoch 0087, iter [01600, 05004], lr: 0.001000, loss: 0.6772
2022-10-10 22:59:11 - train: epoch 0087, iter [01700, 05004], lr: 0.001000, loss: 0.9047
2022-10-10 22:59:46 - train: epoch 0087, iter [01800, 05004], lr: 0.001000, loss: 0.8448
2022-10-10 23:00:20 - train: epoch 0087, iter [01900, 05004], lr: 0.001000, loss: 0.7431
2022-10-10 23:00:54 - train: epoch 0087, iter [02000, 05004], lr: 0.001000, loss: 0.8050
2022-10-10 23:01:30 - train: epoch 0087, iter [02100, 05004], lr: 0.001000, loss: 0.7142
2022-10-10 23:02:05 - train: epoch 0087, iter [02200, 05004], lr: 0.001000, loss: 0.8129
2022-10-10 23:02:44 - train: epoch 0087, iter [02300, 05004], lr: 0.001000, loss: 0.8268
2022-10-10 23:03:53 - train: epoch 0087, iter [02400, 05004], lr: 0.001000, loss: 0.6774
2022-10-10 23:05:17 - train: epoch 0087, iter [02500, 05004], lr: 0.001000, loss: 0.7434
2022-10-10 23:06:17 - train: epoch 0087, iter [02600, 05004], lr: 0.001000, loss: 0.7424
2022-10-10 23:06:57 - train: epoch 0087, iter [02700, 05004], lr: 0.001000, loss: 0.6220
2022-10-10 23:07:31 - train: epoch 0087, iter [02800, 05004], lr: 0.001000, loss: 0.6808
2022-10-10 23:08:06 - train: epoch 0087, iter [02900, 05004], lr: 0.001000, loss: 0.5817
2022-10-10 23:09:09 - train: epoch 0087, iter [03000, 05004], lr: 0.001000, loss: 0.7831
2022-10-10 23:09:43 - train: epoch 0087, iter [03100, 05004], lr: 0.001000, loss: 0.8078
2022-10-10 23:10:19 - train: epoch 0087, iter [03200, 05004], lr: 0.001000, loss: 0.7065
2022-10-10 23:10:59 - train: epoch 0087, iter [03300, 05004], lr: 0.001000, loss: 0.6751
2022-10-10 23:11:34 - train: epoch 0087, iter [03400, 05004], lr: 0.001000, loss: 0.7836
2022-10-10 23:12:08 - train: epoch 0087, iter [03500, 05004], lr: 0.001000, loss: 0.9240
2022-10-10 23:12:43 - train: epoch 0087, iter [03600, 05004], lr: 0.001000, loss: 0.6785
2022-10-10 23:13:18 - train: epoch 0087, iter [03700, 05004], lr: 0.001000, loss: 0.7142
2022-10-10 23:13:52 - train: epoch 0087, iter [03800, 05004], lr: 0.001000, loss: 0.5554
2022-10-10 23:14:26 - train: epoch 0087, iter [03900, 05004], lr: 0.001000, loss: 0.6196
2022-10-10 23:15:01 - train: epoch 0087, iter [04000, 05004], lr: 0.001000, loss: 0.7538
2022-10-10 23:15:35 - train: epoch 0087, iter [04100, 05004], lr: 0.001000, loss: 0.8685
2022-10-10 23:16:08 - train: epoch 0087, iter [04200, 05004], lr: 0.001000, loss: 0.7078
2022-10-10 23:16:46 - train: epoch 0087, iter [04300, 05004], lr: 0.001000, loss: 0.6991
2022-10-10 23:17:20 - train: epoch 0087, iter [04400, 05004], lr: 0.001000, loss: 0.7009
2022-10-10 23:17:54 - train: epoch 0087, iter [04500, 05004], lr: 0.001000, loss: 0.6722
2022-10-10 23:18:28 - train: epoch 0087, iter [04600, 05004], lr: 0.001000, loss: 0.7456
2022-10-10 23:19:02 - train: epoch 0087, iter [04700, 05004], lr: 0.001000, loss: 0.8398
2022-10-10 23:19:37 - train: epoch 0087, iter [04800, 05004], lr: 0.001000, loss: 0.7651
2022-10-10 23:20:11 - train: epoch 0087, iter [04900, 05004], lr: 0.001000, loss: 0.8152
2022-10-10 23:20:52 - train: epoch 0087, iter [05000, 05004], lr: 0.001000, loss: 0.7555
2022-10-10 23:20:54 - train: epoch 087, train_loss: 0.7378
2022-10-10 23:22:13 - eval: epoch: 087, acc1: 76.490%, acc5: 93.396%, test_loss: 0.9358, per_image_load_time: 2.300ms, per_image_inference_time: 0.594ms
2022-10-10 23:22:14 - until epoch: 087, best_acc1: 76.632%
2022-10-10 23:22:14 - epoch 088 lr: 0.001000
2022-10-10 23:22:55 - train: epoch 0088, iter [00100, 05004], lr: 0.001000, loss: 0.7035
2022-10-10 23:23:30 - train: epoch 0088, iter [00200, 05004], lr: 0.001000, loss: 0.7140
2022-10-10 23:24:04 - train: epoch 0088, iter [00300, 05004], lr: 0.001000, loss: 0.7838
2022-10-10 23:24:39 - train: epoch 0088, iter [00400, 05004], lr: 0.001000, loss: 0.6814
2022-10-10 23:25:14 - train: epoch 0088, iter [00500, 05004], lr: 0.001000, loss: 0.6957
2022-10-10 23:25:48 - train: epoch 0088, iter [00600, 05004], lr: 0.001000, loss: 0.6433
2022-10-10 23:26:22 - train: epoch 0088, iter [00700, 05004], lr: 0.001000, loss: 0.6172
2022-10-10 23:26:55 - train: epoch 0088, iter [00800, 05004], lr: 0.001000, loss: 0.8136
2022-10-10 23:27:30 - train: epoch 0088, iter [00900, 05004], lr: 0.001000, loss: 0.7995
2022-10-10 23:28:06 - train: epoch 0088, iter [01000, 05004], lr: 0.001000, loss: 0.7576
2022-10-10 23:28:40 - train: epoch 0088, iter [01100, 05004], lr: 0.001000, loss: 0.8225
2022-10-10 23:29:15 - train: epoch 0088, iter [01200, 05004], lr: 0.001000, loss: 0.6508
2022-10-10 23:29:49 - train: epoch 0088, iter [01300, 05004], lr: 0.001000, loss: 0.7821
2022-10-10 23:30:25 - train: epoch 0088, iter [01400, 05004], lr: 0.001000, loss: 0.6356
2022-10-10 23:30:59 - train: epoch 0088, iter [01500, 05004], lr: 0.001000, loss: 0.8603
2022-10-10 23:31:34 - train: epoch 0088, iter [01600, 05004], lr: 0.001000, loss: 0.6422
2022-10-10 23:32:09 - train: epoch 0088, iter [01700, 05004], lr: 0.001000, loss: 0.8215
2022-10-10 23:32:43 - train: epoch 0088, iter [01800, 05004], lr: 0.001000, loss: 0.7481
2022-10-10 23:33:19 - train: epoch 0088, iter [01900, 05004], lr: 0.001000, loss: 0.8469
2022-10-10 23:33:54 - train: epoch 0088, iter [02000, 05004], lr: 0.001000, loss: 0.7413
2022-10-10 23:34:31 - train: epoch 0088, iter [02100, 05004], lr: 0.001000, loss: 0.6372
2022-10-10 23:35:05 - train: epoch 0088, iter [02200, 05004], lr: 0.001000, loss: 0.7186
2022-10-10 23:35:42 - train: epoch 0088, iter [02300, 05004], lr: 0.001000, loss: 0.5364
2022-10-10 23:36:18 - train: epoch 0088, iter [02400, 05004], lr: 0.001000, loss: 0.7614
2022-10-10 23:36:52 - train: epoch 0088, iter [02500, 05004], lr: 0.001000, loss: 0.8726
2022-10-10 23:37:27 - train: epoch 0088, iter [02600, 05004], lr: 0.001000, loss: 0.6866
2022-10-10 23:38:02 - train: epoch 0088, iter [02700, 05004], lr: 0.001000, loss: 0.8636
2022-10-10 23:38:38 - train: epoch 0088, iter [02800, 05004], lr: 0.001000, loss: 0.7027
2022-10-10 23:39:31 - train: epoch 0088, iter [02900, 05004], lr: 0.001000, loss: 0.7065
2022-10-10 23:40:59 - train: epoch 0088, iter [03000, 05004], lr: 0.001000, loss: 0.8158
2022-10-10 23:43:09 - train: epoch 0088, iter [03100, 05004], lr: 0.001000, loss: 0.5940
2022-10-10 23:43:54 - train: epoch 0088, iter [03200, 05004], lr: 0.001000, loss: 0.6689
2022-10-10 23:44:29 - train: epoch 0088, iter [03300, 05004], lr: 0.001000, loss: 0.8314
2022-10-10 23:45:04 - train: epoch 0088, iter [03400, 05004], lr: 0.001000, loss: 0.6917
2022-10-10 23:45:39 - train: epoch 0088, iter [03500, 05004], lr: 0.001000, loss: 0.6232
2022-10-10 23:46:13 - train: epoch 0088, iter [03600, 05004], lr: 0.001000, loss: 0.6311
2022-10-10 23:46:48 - train: epoch 0088, iter [03700, 05004], lr: 0.001000, loss: 0.8606
2022-10-10 23:47:23 - train: epoch 0088, iter [03800, 05004], lr: 0.001000, loss: 0.7592
2022-10-10 23:48:05 - train: epoch 0088, iter [03900, 05004], lr: 0.001000, loss: 0.8632
2022-10-10 23:48:39 - train: epoch 0088, iter [04000, 05004], lr: 0.001000, loss: 0.5807
2022-10-10 23:49:19 - train: epoch 0088, iter [04100, 05004], lr: 0.001000, loss: 0.7465
2022-10-10 23:49:52 - train: epoch 0088, iter [04200, 05004], lr: 0.001000, loss: 0.7584
2022-10-10 23:50:33 - train: epoch 0088, iter [04300, 05004], lr: 0.001000, loss: 0.7740
2022-10-10 23:51:05 - train: epoch 0088, iter [04400, 05004], lr: 0.001000, loss: 0.7662
2022-10-10 23:51:40 - train: epoch 0088, iter [04500, 05004], lr: 0.001000, loss: 0.6492
2022-10-10 23:52:15 - train: epoch 0088, iter [04600, 05004], lr: 0.001000, loss: 0.8318
2022-10-10 23:53:47 - train: epoch 0088, iter [04700, 05004], lr: 0.001000, loss: 0.7005
2022-10-10 23:54:37 - train: epoch 0088, iter [04800, 05004], lr: 0.001000, loss: 0.7601
2022-10-10 23:55:11 - train: epoch 0088, iter [04900, 05004], lr: 0.001000, loss: 0.7119
2022-10-10 23:55:44 - train: epoch 0088, iter [05000, 05004], lr: 0.001000, loss: 0.7133
2022-10-10 23:55:46 - train: epoch 088, train_loss: 0.7346
2022-10-10 23:57:05 - eval: epoch: 088, acc1: 76.436%, acc5: 93.326%, test_loss: 0.9396, per_image_load_time: 1.864ms, per_image_inference_time: 0.595ms
2022-10-10 23:57:05 - until epoch: 088, best_acc1: 76.632%
2022-10-10 23:57:05 - epoch 089 lr: 0.001000
2022-10-10 23:57:46 - train: epoch 0089, iter [00100, 05004], lr: 0.001000, loss: 0.8444
2022-10-10 23:58:19 - train: epoch 0089, iter [00200, 05004], lr: 0.001000, loss: 0.7054
2022-10-10 23:58:55 - train: epoch 0089, iter [00300, 05004], lr: 0.001000, loss: 0.7974
2022-10-10 23:59:28 - train: epoch 0089, iter [00400, 05004], lr: 0.001000, loss: 0.5896
