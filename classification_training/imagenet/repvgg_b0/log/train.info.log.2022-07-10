2022-07-10 20:02:19 - train: epoch 0059, iter [00700, 05004], lr: 0.052434, loss: 1.5674
2022-07-10 20:02:52 - train: epoch 0059, iter [00800, 05004], lr: 0.052408, loss: 1.7868
2022-07-10 20:03:27 - train: epoch 0059, iter [00900, 05004], lr: 0.052382, loss: 1.7852
2022-07-10 20:04:00 - train: epoch 0059, iter [01000, 05004], lr: 0.052356, loss: 2.1265
2022-07-10 20:04:34 - train: epoch 0059, iter [01100, 05004], lr: 0.052329, loss: 2.1952
2022-07-10 20:05:09 - train: epoch 0059, iter [01200, 05004], lr: 0.052303, loss: 1.6043
2022-07-10 20:05:43 - train: epoch 0059, iter [01300, 05004], lr: 0.052277, loss: 2.1840
2022-07-10 20:06:18 - train: epoch 0059, iter [01400, 05004], lr: 0.052251, loss: 2.0587
2022-07-10 20:06:52 - train: epoch 0059, iter [01500, 05004], lr: 0.052225, loss: 1.9675
2022-07-10 20:07:26 - train: epoch 0059, iter [01600, 05004], lr: 0.052199, loss: 1.7253
2022-07-10 20:08:01 - train: epoch 0059, iter [01700, 05004], lr: 0.052173, loss: 1.9058
2022-07-10 20:08:35 - train: epoch 0059, iter [01800, 05004], lr: 0.052146, loss: 1.7656
2022-07-10 20:09:10 - train: epoch 0059, iter [01900, 05004], lr: 0.052120, loss: 1.6763
2022-07-10 20:09:44 - train: epoch 0059, iter [02000, 05004], lr: 0.052094, loss: 1.7284
2022-07-10 20:10:18 - train: epoch 0059, iter [02100, 05004], lr: 0.052068, loss: 1.9880
2022-07-10 20:10:53 - train: epoch 0059, iter [02200, 05004], lr: 0.052042, loss: 1.7876
2022-07-10 20:11:28 - train: epoch 0059, iter [02300, 05004], lr: 0.052016, loss: 1.7828
2022-07-10 20:12:03 - train: epoch 0059, iter [02400, 05004], lr: 0.051990, loss: 2.0365
2022-07-10 20:12:37 - train: epoch 0059, iter [02500, 05004], lr: 0.051964, loss: 2.0102
2022-07-10 20:13:12 - train: epoch 0059, iter [02600, 05004], lr: 0.051937, loss: 1.6909
2022-07-10 20:13:46 - train: epoch 0059, iter [02700, 05004], lr: 0.051911, loss: 1.8596
2022-07-10 20:14:20 - train: epoch 0059, iter [02800, 05004], lr: 0.051885, loss: 2.0246
2022-07-10 20:14:55 - train: epoch 0059, iter [02900, 05004], lr: 0.051859, loss: 1.8649
2022-07-10 20:15:29 - train: epoch 0059, iter [03000, 05004], lr: 0.051833, loss: 2.0517
2022-07-10 20:16:03 - train: epoch 0059, iter [03100, 05004], lr: 0.051807, loss: 1.6493
2022-07-10 20:16:38 - train: epoch 0059, iter [03200, 05004], lr: 0.051781, loss: 1.9039
2022-07-10 20:17:13 - train: epoch 0059, iter [03300, 05004], lr: 0.051754, loss: 1.7318
2022-07-10 20:17:46 - train: epoch 0059, iter [03400, 05004], lr: 0.051728, loss: 2.4213
2022-07-10 20:18:22 - train: epoch 0059, iter [03500, 05004], lr: 0.051702, loss: 1.7757
2022-07-10 20:18:56 - train: epoch 0059, iter [03600, 05004], lr: 0.051676, loss: 2.0228
2022-07-10 20:19:30 - train: epoch 0059, iter [03700, 05004], lr: 0.051650, loss: 1.6641
2022-07-10 20:20:05 - train: epoch 0059, iter [03800, 05004], lr: 0.051624, loss: 1.8114
2022-07-10 20:20:39 - train: epoch 0059, iter [03900, 05004], lr: 0.051598, loss: 1.7688
2022-07-10 20:21:14 - train: epoch 0059, iter [04000, 05004], lr: 0.051571, loss: 2.0763
2022-07-10 20:21:48 - train: epoch 0059, iter [04100, 05004], lr: 0.051545, loss: 1.7648
2022-07-10 20:22:23 - train: epoch 0059, iter [04200, 05004], lr: 0.051519, loss: 1.9147
2022-07-10 20:22:57 - train: epoch 0059, iter [04300, 05004], lr: 0.051493, loss: 2.1417
2022-07-10 20:23:32 - train: epoch 0059, iter [04400, 05004], lr: 0.051467, loss: 1.8471
2022-07-10 20:24:07 - train: epoch 0059, iter [04500, 05004], lr: 0.051441, loss: 1.8870
2022-07-10 20:24:41 - train: epoch 0059, iter [04600, 05004], lr: 0.051414, loss: 2.0871
2022-07-10 20:25:16 - train: epoch 0059, iter [04700, 05004], lr: 0.051388, loss: 1.8271
2022-07-10 20:25:50 - train: epoch 0059, iter [04800, 05004], lr: 0.051362, loss: 1.9476
2022-07-10 20:26:25 - train: epoch 0059, iter [04900, 05004], lr: 0.051336, loss: 1.7515
2022-07-10 20:26:58 - train: epoch 0059, iter [05000, 05004], lr: 0.051310, loss: 1.9252
2022-07-10 20:26:59 - train: epoch 059, train_loss: 1.8492
2022-07-10 20:28:14 - eval: epoch: 059, acc1: 60.224%, acc5: 83.382%, test_loss: 1.6625, per_image_load_time: 2.274ms, per_image_inference_time: 0.367ms
2022-07-10 20:28:14 - until epoch: 059, best_acc1: 61.662%
2022-07-10 20:47:26 - epoch 060 lr: 0.051309
2022-07-10 20:48:07 - train: epoch 0060, iter [00100, 05004], lr: 0.051283, loss: 1.8844
2022-07-10 20:48:41 - train: epoch 0060, iter [00200, 05004], lr: 0.051257, loss: 1.6895
2022-07-10 20:49:15 - train: epoch 0060, iter [00300, 05004], lr: 0.051230, loss: 1.7937
2022-07-10 20:49:48 - train: epoch 0060, iter [00400, 05004], lr: 0.051204, loss: 1.8007
2022-07-10 20:50:22 - train: epoch 0060, iter [00500, 05004], lr: 0.051178, loss: 2.0836
2022-07-10 20:50:55 - train: epoch 0060, iter [00600, 05004], lr: 0.051152, loss: 1.9275
2022-07-10 20:51:28 - train: epoch 0060, iter [00700, 05004], lr: 0.051126, loss: 1.7584
2022-07-10 20:52:01 - train: epoch 0060, iter [00800, 05004], lr: 0.051100, loss: 1.8615
2022-07-10 20:52:35 - train: epoch 0060, iter [00900, 05004], lr: 0.051073, loss: 1.7030
2022-07-10 20:53:09 - train: epoch 0060, iter [01000, 05004], lr: 0.051047, loss: 1.5493
2022-07-10 20:53:41 - train: epoch 0060, iter [01100, 05004], lr: 0.051021, loss: 1.7472
2022-07-10 20:54:15 - train: epoch 0060, iter [01200, 05004], lr: 0.050995, loss: 1.7710
2022-07-10 20:54:48 - train: epoch 0060, iter [01300, 05004], lr: 0.050969, loss: 1.8780
2022-07-10 20:55:22 - train: epoch 0060, iter [01400, 05004], lr: 0.050943, loss: 1.8841
2022-07-10 20:55:55 - train: epoch 0060, iter [01500, 05004], lr: 0.050917, loss: 1.9879
2022-07-10 20:56:29 - train: epoch 0060, iter [01600, 05004], lr: 0.050890, loss: 1.8214
2022-07-10 20:57:02 - train: epoch 0060, iter [01700, 05004], lr: 0.050864, loss: 2.2057
2022-07-10 20:57:36 - train: epoch 0060, iter [01800, 05004], lr: 0.050838, loss: 1.9439
2022-07-10 20:58:10 - train: epoch 0060, iter [01900, 05004], lr: 0.050812, loss: 1.9980
2022-07-10 20:58:44 - train: epoch 0060, iter [02000, 05004], lr: 0.050786, loss: 1.6620
2022-07-10 20:59:17 - train: epoch 0060, iter [02100, 05004], lr: 0.050760, loss: 1.9113
2022-07-10 20:59:51 - train: epoch 0060, iter [02200, 05004], lr: 0.050733, loss: 1.9205
2022-07-10 21:00:25 - train: epoch 0060, iter [02300, 05004], lr: 0.050707, loss: 1.6577
2022-07-10 21:00:58 - train: epoch 0060, iter [02400, 05004], lr: 0.050681, loss: 1.7542
2022-07-10 21:01:32 - train: epoch 0060, iter [02500, 05004], lr: 0.050655, loss: 2.1131
2022-07-10 21:02:05 - train: epoch 0060, iter [02600, 05004], lr: 0.050629, loss: 1.7448
2022-07-10 21:02:40 - train: epoch 0060, iter [02700, 05004], lr: 0.050603, loss: 1.7500
2022-07-10 21:03:14 - train: epoch 0060, iter [02800, 05004], lr: 0.050577, loss: 1.9676
2022-07-10 21:03:47 - train: epoch 0060, iter [02900, 05004], lr: 0.050550, loss: 1.8308
2022-07-10 21:04:21 - train: epoch 0060, iter [03000, 05004], lr: 0.050524, loss: 1.9682
2022-07-10 21:04:54 - train: epoch 0060, iter [03100, 05004], lr: 0.050498, loss: 1.9177
2022-07-10 21:05:29 - train: epoch 0060, iter [03200, 05004], lr: 0.050472, loss: 1.6697
2022-07-10 21:06:02 - train: epoch 0060, iter [03300, 05004], lr: 0.050446, loss: 1.6180
2022-07-10 21:06:36 - train: epoch 0060, iter [03400, 05004], lr: 0.050420, loss: 1.7998
2022-07-10 21:07:10 - train: epoch 0060, iter [03500, 05004], lr: 0.050393, loss: 1.8750
2022-07-10 21:07:43 - train: epoch 0060, iter [03600, 05004], lr: 0.050367, loss: 1.9328
2022-07-10 21:08:18 - train: epoch 0060, iter [03700, 05004], lr: 0.050341, loss: 1.9090
2022-07-10 21:08:51 - train: epoch 0060, iter [03800, 05004], lr: 0.050315, loss: 1.9785
2022-07-10 21:09:25 - train: epoch 0060, iter [03900, 05004], lr: 0.050289, loss: 1.9660
2022-07-10 21:10:00 - train: epoch 0060, iter [04000, 05004], lr: 0.050263, loss: 1.9079
2022-07-10 21:10:32 - train: epoch 0060, iter [04100, 05004], lr: 0.050236, loss: 1.9969
2022-07-10 21:11:06 - train: epoch 0060, iter [04200, 05004], lr: 0.050210, loss: 1.8548
2022-07-10 21:11:40 - train: epoch 0060, iter [04300, 05004], lr: 0.050184, loss: 1.7571
2022-07-10 21:12:13 - train: epoch 0060, iter [04400, 05004], lr: 0.050158, loss: 1.7602
2022-07-10 21:12:47 - train: epoch 0060, iter [04500, 05004], lr: 0.050132, loss: 1.8855
2022-07-10 21:13:21 - train: epoch 0060, iter [04600, 05004], lr: 0.050106, loss: 1.7284
2022-07-10 21:13:54 - train: epoch 0060, iter [04700, 05004], lr: 0.050080, loss: 1.6903
2022-07-10 21:14:28 - train: epoch 0060, iter [04800, 05004], lr: 0.050053, loss: 1.7725
2022-07-10 21:15:02 - train: epoch 0060, iter [04900, 05004], lr: 0.050027, loss: 1.7472
2022-07-10 21:15:34 - train: epoch 0060, iter [05000, 05004], lr: 0.050001, loss: 2.0713
2022-07-10 21:15:35 - train: epoch 060, train_loss: 1.8354
2022-07-10 21:16:51 - eval: epoch: 060, acc1: 61.784%, acc5: 84.746%, test_loss: 1.5858, per_image_load_time: 1.506ms, per_image_inference_time: 0.383ms
2022-07-10 21:16:51 - until epoch: 060, best_acc1: 61.784%
2022-07-10 21:16:51 - epoch 061 lr: 0.050000
2022-07-10 21:17:30 - train: epoch 0061, iter [00100, 05004], lr: 0.049974, loss: 1.7806
2022-07-10 21:18:05 - train: epoch 0061, iter [00200, 05004], lr: 0.049948, loss: 1.9544
2022-07-10 21:18:39 - train: epoch 0061, iter [00300, 05004], lr: 0.049922, loss: 1.6080
2022-07-10 21:19:13 - train: epoch 0061, iter [00400, 05004], lr: 0.049895, loss: 1.8485
2022-07-10 21:19:47 - train: epoch 0061, iter [00500, 05004], lr: 0.049869, loss: 1.8076
2022-07-10 21:20:21 - train: epoch 0061, iter [00600, 05004], lr: 0.049843, loss: 1.7374
2022-07-10 21:20:56 - train: epoch 0061, iter [00700, 05004], lr: 0.049817, loss: 1.6265
2022-07-10 21:21:30 - train: epoch 0061, iter [00800, 05004], lr: 0.049791, loss: 1.7461
2022-07-10 21:22:04 - train: epoch 0061, iter [00900, 05004], lr: 0.049765, loss: 1.8069
2022-07-10 21:22:38 - train: epoch 0061, iter [01000, 05004], lr: 0.049738, loss: 1.8911
2022-07-10 21:23:12 - train: epoch 0061, iter [01100, 05004], lr: 0.049712, loss: 1.6279
2022-07-10 21:23:46 - train: epoch 0061, iter [01200, 05004], lr: 0.049686, loss: 1.7928
2022-07-10 21:24:21 - train: epoch 0061, iter [01300, 05004], lr: 0.049660, loss: 1.8452
2022-07-10 21:24:55 - train: epoch 0061, iter [01400, 05004], lr: 0.049634, loss: 1.7662
2022-07-10 21:25:28 - train: epoch 0061, iter [01500, 05004], lr: 0.049608, loss: 1.7667
2022-07-10 21:26:03 - train: epoch 0061, iter [01600, 05004], lr: 0.049581, loss: 1.6567
2022-07-10 21:26:37 - train: epoch 0061, iter [01700, 05004], lr: 0.049555, loss: 1.8984
2022-07-10 21:27:11 - train: epoch 0061, iter [01800, 05004], lr: 0.049529, loss: 1.8678
2022-07-10 21:27:46 - train: epoch 0061, iter [01900, 05004], lr: 0.049503, loss: 2.0243
2022-07-10 21:28:20 - train: epoch 0061, iter [02000, 05004], lr: 0.049477, loss: 1.8407
2022-07-10 21:28:55 - train: epoch 0061, iter [02100, 05004], lr: 0.049451, loss: 2.2812
2022-07-10 21:29:28 - train: epoch 0061, iter [02200, 05004], lr: 0.049425, loss: 1.7117
2022-07-10 21:30:03 - train: epoch 0061, iter [02300, 05004], lr: 0.049398, loss: 1.8066
2022-07-10 21:30:37 - train: epoch 0061, iter [02400, 05004], lr: 0.049372, loss: 1.7452
2022-07-10 21:31:12 - train: epoch 0061, iter [02500, 05004], lr: 0.049346, loss: 1.5863
2022-07-10 21:31:46 - train: epoch 0061, iter [02600, 05004], lr: 0.049320, loss: 1.9106
2022-07-10 21:32:21 - train: epoch 0061, iter [02700, 05004], lr: 0.049294, loss: 2.0709
2022-07-10 21:32:55 - train: epoch 0061, iter [02800, 05004], lr: 0.049268, loss: 1.6991
2022-07-10 21:33:29 - train: epoch 0061, iter [02900, 05004], lr: 0.049241, loss: 2.1405
2022-07-10 21:34:04 - train: epoch 0061, iter [03000, 05004], lr: 0.049215, loss: 1.8345
2022-07-10 21:34:38 - train: epoch 0061, iter [03100, 05004], lr: 0.049189, loss: 1.8341
2022-07-10 21:35:12 - train: epoch 0061, iter [03200, 05004], lr: 0.049163, loss: 1.8332
2022-07-10 21:35:46 - train: epoch 0061, iter [03300, 05004], lr: 0.049137, loss: 1.8550
2022-07-10 21:36:20 - train: epoch 0061, iter [03400, 05004], lr: 0.049111, loss: 1.8889
2022-07-10 21:36:55 - train: epoch 0061, iter [03500, 05004], lr: 0.049084, loss: 1.8535
2022-07-10 21:37:29 - train: epoch 0061, iter [03600, 05004], lr: 0.049058, loss: 1.8056
2022-07-10 21:38:03 - train: epoch 0061, iter [03700, 05004], lr: 0.049032, loss: 1.8092
2022-07-10 21:38:38 - train: epoch 0061, iter [03800, 05004], lr: 0.049006, loss: 1.7219
2022-07-10 21:39:12 - train: epoch 0061, iter [03900, 05004], lr: 0.048980, loss: 1.8216
2022-07-10 21:39:46 - train: epoch 0061, iter [04000, 05004], lr: 0.048954, loss: 1.8361
2022-07-10 21:40:22 - train: epoch 0061, iter [04100, 05004], lr: 0.048928, loss: 2.0174
2022-07-10 21:40:56 - train: epoch 0061, iter [04200, 05004], lr: 0.048901, loss: 1.8307
2022-07-10 21:41:30 - train: epoch 0061, iter [04300, 05004], lr: 0.048875, loss: 1.8084
2022-07-10 21:42:04 - train: epoch 0061, iter [04400, 05004], lr: 0.048849, loss: 1.7854
2022-07-10 21:42:38 - train: epoch 0061, iter [04500, 05004], lr: 0.048823, loss: 1.9890
2022-07-10 21:43:13 - train: epoch 0061, iter [04600, 05004], lr: 0.048797, loss: 1.8595
2022-07-10 21:43:47 - train: epoch 0061, iter [04700, 05004], lr: 0.048771, loss: 2.0198
2022-07-10 21:44:22 - train: epoch 0061, iter [04800, 05004], lr: 0.048744, loss: 1.9272
2022-07-10 21:44:57 - train: epoch 0061, iter [04900, 05004], lr: 0.048718, loss: 1.9257
2022-07-10 21:45:30 - train: epoch 0061, iter [05000, 05004], lr: 0.048692, loss: 1.9427
2022-07-10 21:45:31 - train: epoch 061, train_loss: 1.8252
2022-07-10 21:46:46 - eval: epoch: 061, acc1: 62.512%, acc5: 85.056%, test_loss: 1.5696, per_image_load_time: 2.537ms, per_image_inference_time: 0.374ms
2022-07-10 21:46:47 - until epoch: 061, best_acc1: 62.512%
2022-07-10 21:46:47 - epoch 062 lr: 0.048691
2022-07-10 21:47:26 - train: epoch 0062, iter [00100, 05004], lr: 0.048665, loss: 1.8257
2022-07-10 21:48:01 - train: epoch 0062, iter [00200, 05004], lr: 0.048639, loss: 1.8479
2022-07-10 21:48:34 - train: epoch 0062, iter [00300, 05004], lr: 0.048613, loss: 1.9003
2022-07-10 21:49:09 - train: epoch 0062, iter [00400, 05004], lr: 0.048587, loss: 1.5701
2022-07-10 21:49:43 - train: epoch 0062, iter [00500, 05004], lr: 0.048560, loss: 1.9009
2022-07-10 21:50:18 - train: epoch 0062, iter [00600, 05004], lr: 0.048534, loss: 1.6665
2022-07-10 21:50:51 - train: epoch 0062, iter [00700, 05004], lr: 0.048508, loss: 1.8559
2022-07-10 21:51:26 - train: epoch 0062, iter [00800, 05004], lr: 0.048482, loss: 1.7840
2022-07-10 21:52:00 - train: epoch 0062, iter [00900, 05004], lr: 0.048456, loss: 1.9415
2022-07-10 21:52:34 - train: epoch 0062, iter [01000, 05004], lr: 0.048430, loss: 1.8453
2022-07-10 21:53:08 - train: epoch 0062, iter [01100, 05004], lr: 0.048404, loss: 1.6894
2022-07-10 21:53:42 - train: epoch 0062, iter [01200, 05004], lr: 0.048377, loss: 1.9265
2022-07-10 21:54:16 - train: epoch 0062, iter [01300, 05004], lr: 0.048351, loss: 1.6840
2022-07-10 21:54:51 - train: epoch 0062, iter [01400, 05004], lr: 0.048325, loss: 1.6809
2022-07-10 21:55:25 - train: epoch 0062, iter [01500, 05004], lr: 0.048299, loss: 2.0061
2022-07-10 21:56:00 - train: epoch 0062, iter [01600, 05004], lr: 0.048273, loss: 1.9402
2022-07-10 21:56:35 - train: epoch 0062, iter [01700, 05004], lr: 0.048247, loss: 1.8905
2022-07-10 21:57:09 - train: epoch 0062, iter [01800, 05004], lr: 0.048221, loss: 1.4897
2022-07-10 21:57:44 - train: epoch 0062, iter [01900, 05004], lr: 0.048194, loss: 1.6334
2022-07-10 21:58:19 - train: epoch 0062, iter [02000, 05004], lr: 0.048168, loss: 1.9859
2022-07-10 21:58:52 - train: epoch 0062, iter [02100, 05004], lr: 0.048142, loss: 2.0853
2022-07-10 21:59:28 - train: epoch 0062, iter [02200, 05004], lr: 0.048116, loss: 1.8124
2022-07-10 22:00:02 - train: epoch 0062, iter [02300, 05004], lr: 0.048090, loss: 1.7832
2022-07-10 22:00:38 - train: epoch 0062, iter [02400, 05004], lr: 0.048064, loss: 2.0183
2022-07-10 22:01:12 - train: epoch 0062, iter [02500, 05004], lr: 0.048038, loss: 2.0136
2022-07-10 22:01:46 - train: epoch 0062, iter [02600, 05004], lr: 0.048011, loss: 1.6767
2022-07-10 22:02:21 - train: epoch 0062, iter [02700, 05004], lr: 0.047985, loss: 1.7213
2022-07-10 22:02:56 - train: epoch 0062, iter [02800, 05004], lr: 0.047959, loss: 1.9780
2022-07-10 22:03:30 - train: epoch 0062, iter [02900, 05004], lr: 0.047933, loss: 1.7068
2022-07-10 22:04:05 - train: epoch 0062, iter [03000, 05004], lr: 0.047907, loss: 1.9671
2022-07-10 22:04:40 - train: epoch 0062, iter [03100, 05004], lr: 0.047881, loss: 1.6989
2022-07-10 22:05:15 - train: epoch 0062, iter [03200, 05004], lr: 0.047855, loss: 1.6460
2022-07-10 22:05:49 - train: epoch 0062, iter [03300, 05004], lr: 0.047828, loss: 1.8474
2022-07-10 22:06:24 - train: epoch 0062, iter [03400, 05004], lr: 0.047802, loss: 1.8936
2022-07-10 22:06:59 - train: epoch 0062, iter [03500, 05004], lr: 0.047776, loss: 1.6496
2022-07-10 22:07:34 - train: epoch 0062, iter [03600, 05004], lr: 0.047750, loss: 2.0250
2022-07-10 22:08:08 - train: epoch 0062, iter [03700, 05004], lr: 0.047724, loss: 1.7989
2022-07-10 22:08:43 - train: epoch 0062, iter [03800, 05004], lr: 0.047698, loss: 1.8585
2022-07-10 22:09:18 - train: epoch 0062, iter [03900, 05004], lr: 0.047672, loss: 1.7407
2022-07-10 22:09:52 - train: epoch 0062, iter [04000, 05004], lr: 0.047646, loss: 1.7415
2022-07-10 22:10:25 - train: epoch 0062, iter [04100, 05004], lr: 0.047619, loss: 1.9865
2022-07-10 22:11:00 - train: epoch 0062, iter [04200, 05004], lr: 0.047593, loss: 1.6906
2022-07-10 22:11:36 - train: epoch 0062, iter [04300, 05004], lr: 0.047567, loss: 1.8494
2022-07-10 22:12:10 - train: epoch 0062, iter [04400, 05004], lr: 0.047541, loss: 1.6193
2022-07-10 22:12:44 - train: epoch 0062, iter [04500, 05004], lr: 0.047515, loss: 1.7937
2022-07-10 22:13:19 - train: epoch 0062, iter [04600, 05004], lr: 0.047489, loss: 1.7608
2022-07-10 22:13:55 - train: epoch 0062, iter [04700, 05004], lr: 0.047463, loss: 1.5181
2022-07-10 22:14:28 - train: epoch 0062, iter [04800, 05004], lr: 0.047436, loss: 1.8556
2022-07-10 22:15:03 - train: epoch 0062, iter [04900, 05004], lr: 0.047410, loss: 1.8408
2022-07-10 22:15:36 - train: epoch 0062, iter [05000, 05004], lr: 0.047384, loss: 2.0223
2022-07-10 22:15:37 - train: epoch 062, train_loss: 1.8144
2022-07-10 22:16:54 - eval: epoch: 062, acc1: 61.836%, acc5: 84.874%, test_loss: 1.5780, per_image_load_time: 2.482ms, per_image_inference_time: 0.396ms
2022-07-10 22:16:54 - until epoch: 062, best_acc1: 62.512%
2022-07-10 22:16:54 - epoch 063 lr: 0.047383
2022-07-10 22:17:33 - train: epoch 0063, iter [00100, 05004], lr: 0.047357, loss: 1.8563
2022-07-10 22:18:07 - train: epoch 0063, iter [00200, 05004], lr: 0.047331, loss: 1.8489
2022-07-10 22:18:41 - train: epoch 0063, iter [00300, 05004], lr: 0.047305, loss: 2.0076
2022-07-10 22:19:15 - train: epoch 0063, iter [00400, 05004], lr: 0.047279, loss: 1.7715
2022-07-10 22:19:50 - train: epoch 0063, iter [00500, 05004], lr: 0.047253, loss: 1.8657
2022-07-10 22:20:24 - train: epoch 0063, iter [00600, 05004], lr: 0.047226, loss: 1.9012
2022-07-10 22:20:57 - train: epoch 0063, iter [00700, 05004], lr: 0.047200, loss: 1.9360
2022-07-10 22:21:32 - train: epoch 0063, iter [00800, 05004], lr: 0.047174, loss: 1.7839
2022-07-10 22:22:06 - train: epoch 0063, iter [00900, 05004], lr: 0.047148, loss: 1.7727
2022-07-10 22:22:40 - train: epoch 0063, iter [01000, 05004], lr: 0.047122, loss: 1.8080
2022-07-10 22:23:14 - train: epoch 0063, iter [01100, 05004], lr: 0.047096, loss: 1.8232
2022-07-10 22:23:49 - train: epoch 0063, iter [01200, 05004], lr: 0.047070, loss: 1.9204
2022-07-10 22:24:24 - train: epoch 0063, iter [01300, 05004], lr: 0.047044, loss: 1.4834
2022-07-10 22:24:58 - train: epoch 0063, iter [01400, 05004], lr: 0.047018, loss: 2.0773
2022-07-10 22:25:32 - train: epoch 0063, iter [01500, 05004], lr: 0.046991, loss: 1.6771
2022-07-10 22:26:07 - train: epoch 0063, iter [01600, 05004], lr: 0.046965, loss: 1.9212
2022-07-10 22:26:41 - train: epoch 0063, iter [01700, 05004], lr: 0.046939, loss: 1.6463
2022-07-10 22:27:15 - train: epoch 0063, iter [01800, 05004], lr: 0.046913, loss: 2.0077
2022-07-10 22:27:51 - train: epoch 0063, iter [01900, 05004], lr: 0.046887, loss: 2.0377
2022-07-10 22:28:24 - train: epoch 0063, iter [02000, 05004], lr: 0.046861, loss: 1.6361
2022-07-10 22:28:59 - train: epoch 0063, iter [02100, 05004], lr: 0.046835, loss: 1.7351
2022-07-10 22:29:34 - train: epoch 0063, iter [02200, 05004], lr: 0.046809, loss: 2.1003
2022-07-10 22:30:08 - train: epoch 0063, iter [02300, 05004], lr: 0.046783, loss: 1.9823
2022-07-10 22:30:42 - train: epoch 0063, iter [02400, 05004], lr: 0.046756, loss: 1.7935
2022-07-10 22:31:17 - train: epoch 0063, iter [02500, 05004], lr: 0.046730, loss: 2.0261
2022-07-10 22:31:52 - train: epoch 0063, iter [02600, 05004], lr: 0.046704, loss: 1.7088
2022-07-10 22:32:27 - train: epoch 0063, iter [02700, 05004], lr: 0.046678, loss: 2.1522
2022-07-10 22:33:01 - train: epoch 0063, iter [02800, 05004], lr: 0.046652, loss: 1.8669
2022-07-10 22:33:36 - train: epoch 0063, iter [02900, 05004], lr: 0.046626, loss: 1.8614
2022-07-10 22:34:09 - train: epoch 0063, iter [03000, 05004], lr: 0.046600, loss: 1.9983
2022-07-10 22:34:45 - train: epoch 0063, iter [03100, 05004], lr: 0.046574, loss: 2.1066
2022-07-10 22:35:18 - train: epoch 0063, iter [03200, 05004], lr: 0.046548, loss: 1.8311
2022-07-10 22:35:53 - train: epoch 0063, iter [03300, 05004], lr: 0.046522, loss: 1.7631
2022-07-10 22:36:28 - train: epoch 0063, iter [03400, 05004], lr: 0.046495, loss: 1.8192
2022-07-10 22:37:02 - train: epoch 0063, iter [03500, 05004], lr: 0.046469, loss: 1.8791
2022-07-10 22:37:37 - train: epoch 0063, iter [03600, 05004], lr: 0.046443, loss: 1.6245
2022-07-10 22:38:11 - train: epoch 0063, iter [03700, 05004], lr: 0.046417, loss: 1.7147
2022-07-10 22:38:46 - train: epoch 0063, iter [03800, 05004], lr: 0.046391, loss: 1.9580
2022-07-10 22:39:20 - train: epoch 0063, iter [03900, 05004], lr: 0.046365, loss: 1.8203
2022-07-10 22:39:55 - train: epoch 0063, iter [04000, 05004], lr: 0.046339, loss: 1.5040
2022-07-10 22:40:29 - train: epoch 0063, iter [04100, 05004], lr: 0.046313, loss: 1.8187
2022-07-10 22:41:03 - train: epoch 0063, iter [04200, 05004], lr: 0.046287, loss: 1.7375
2022-07-10 22:41:38 - train: epoch 0063, iter [04300, 05004], lr: 0.046261, loss: 1.9661
2022-07-10 22:42:12 - train: epoch 0063, iter [04400, 05004], lr: 0.046235, loss: 1.7921
2022-07-10 22:42:47 - train: epoch 0063, iter [04500, 05004], lr: 0.046208, loss: 1.8090
2022-07-10 22:43:21 - train: epoch 0063, iter [04600, 05004], lr: 0.046182, loss: 1.6877
2022-07-10 22:43:55 - train: epoch 0063, iter [04700, 05004], lr: 0.046156, loss: 1.7684
2022-07-10 22:44:30 - train: epoch 0063, iter [04800, 05004], lr: 0.046130, loss: 1.7250
2022-07-10 22:45:05 - train: epoch 0063, iter [04900, 05004], lr: 0.046104, loss: 1.8899
2022-07-10 22:45:38 - train: epoch 0063, iter [05000, 05004], lr: 0.046078, loss: 1.8540
2022-07-10 22:45:39 - train: epoch 063, train_loss: 1.8028
2022-07-10 22:46:55 - eval: epoch: 063, acc1: 62.982%, acc5: 85.272%, test_loss: 1.5459, per_image_load_time: 2.571ms, per_image_inference_time: 0.375ms
2022-07-10 22:46:55 - until epoch: 063, best_acc1: 62.982%
2022-07-10 22:46:55 - epoch 064 lr: 0.046077
2022-07-10 22:47:35 - train: epoch 0064, iter [00100, 05004], lr: 0.046051, loss: 1.7773
2022-07-10 22:48:09 - train: epoch 0064, iter [00200, 05004], lr: 0.046025, loss: 1.7435
2022-07-10 22:48:43 - train: epoch 0064, iter [00300, 05004], lr: 0.045999, loss: 1.5636
2022-07-10 22:49:17 - train: epoch 0064, iter [00400, 05004], lr: 0.045973, loss: 1.6676
2022-07-10 22:49:51 - train: epoch 0064, iter [00500, 05004], lr: 0.045947, loss: 1.5975
2022-07-10 22:50:26 - train: epoch 0064, iter [00600, 05004], lr: 0.045921, loss: 1.6362
2022-07-10 22:51:00 - train: epoch 0064, iter [00700, 05004], lr: 0.045895, loss: 1.8169
2022-07-10 22:51:35 - train: epoch 0064, iter [00800, 05004], lr: 0.045868, loss: 1.6950
2022-07-10 22:52:09 - train: epoch 0064, iter [00900, 05004], lr: 0.045842, loss: 1.5514
2022-07-10 22:52:43 - train: epoch 0064, iter [01000, 05004], lr: 0.045816, loss: 1.9417
2022-07-10 22:53:18 - train: epoch 0064, iter [01100, 05004], lr: 0.045790, loss: 1.5783
2022-07-10 22:53:52 - train: epoch 0064, iter [01200, 05004], lr: 0.045764, loss: 1.6832
2022-07-10 22:54:27 - train: epoch 0064, iter [01300, 05004], lr: 0.045738, loss: 1.7109
2022-07-10 22:55:01 - train: epoch 0064, iter [01400, 05004], lr: 0.045712, loss: 1.9112
2022-07-10 22:55:35 - train: epoch 0064, iter [01500, 05004], lr: 0.045686, loss: 1.9256
2022-07-10 22:56:09 - train: epoch 0064, iter [01600, 05004], lr: 0.045660, loss: 1.8150
2022-07-10 22:56:43 - train: epoch 0064, iter [01700, 05004], lr: 0.045634, loss: 1.5824
2022-07-10 22:57:18 - train: epoch 0064, iter [01800, 05004], lr: 0.045608, loss: 1.5646
2022-07-10 22:57:51 - train: epoch 0064, iter [01900, 05004], lr: 0.045582, loss: 1.8957
2022-07-10 22:58:26 - train: epoch 0064, iter [02000, 05004], lr: 0.045556, loss: 1.6229
2022-07-10 22:59:01 - train: epoch 0064, iter [02100, 05004], lr: 0.045530, loss: 1.7027
2022-07-10 22:59:35 - train: epoch 0064, iter [02200, 05004], lr: 0.045504, loss: 1.7608
2022-07-10 23:00:10 - train: epoch 0064, iter [02300, 05004], lr: 0.045478, loss: 1.7602
2022-07-10 23:00:44 - train: epoch 0064, iter [02400, 05004], lr: 0.045451, loss: 1.7211
2022-07-10 23:01:18 - train: epoch 0064, iter [02500, 05004], lr: 0.045425, loss: 1.6868
2022-07-10 23:01:52 - train: epoch 0064, iter [02600, 05004], lr: 0.045399, loss: 1.8846
2022-07-10 23:02:27 - train: epoch 0064, iter [02700, 05004], lr: 0.045373, loss: 1.7105
2022-07-10 23:03:01 - train: epoch 0064, iter [02800, 05004], lr: 0.045347, loss: 1.8150
2022-07-10 23:03:35 - train: epoch 0064, iter [02900, 05004], lr: 0.045321, loss: 1.9776
2022-07-10 23:04:10 - train: epoch 0064, iter [03000, 05004], lr: 0.045295, loss: 1.8162
2022-07-10 23:04:44 - train: epoch 0064, iter [03100, 05004], lr: 0.045269, loss: 1.8083
2022-07-10 23:05:18 - train: epoch 0064, iter [03200, 05004], lr: 0.045243, loss: 1.8647
2022-07-10 23:05:52 - train: epoch 0064, iter [03300, 05004], lr: 0.045217, loss: 1.7339
2022-07-10 23:06:28 - train: epoch 0064, iter [03400, 05004], lr: 0.045191, loss: 1.7917
2022-07-10 23:07:02 - train: epoch 0064, iter [03500, 05004], lr: 0.045165, loss: 1.8685
2022-07-10 23:07:37 - train: epoch 0064, iter [03600, 05004], lr: 0.045139, loss: 1.9529
2022-07-10 23:08:11 - train: epoch 0064, iter [03700, 05004], lr: 0.045113, loss: 1.5666
2022-07-10 23:08:45 - train: epoch 0064, iter [03800, 05004], lr: 0.045087, loss: 2.0417
2022-07-10 23:09:19 - train: epoch 0064, iter [03900, 05004], lr: 0.045061, loss: 1.6132
2022-07-10 23:09:54 - train: epoch 0064, iter [04000, 05004], lr: 0.045035, loss: 1.8231
2022-07-10 23:10:28 - train: epoch 0064, iter [04100, 05004], lr: 0.045009, loss: 1.8001
2022-07-10 23:11:02 - train: epoch 0064, iter [04200, 05004], lr: 0.044983, loss: 1.6592
2022-07-10 23:11:37 - train: epoch 0064, iter [04300, 05004], lr: 0.044957, loss: 2.0098
2022-07-10 23:12:12 - train: epoch 0064, iter [04400, 05004], lr: 0.044931, loss: 1.5858
2022-07-10 23:12:46 - train: epoch 0064, iter [04500, 05004], lr: 0.044905, loss: 1.7085
2022-07-10 23:13:20 - train: epoch 0064, iter [04600, 05004], lr: 0.044879, loss: 1.8731
2022-07-10 23:13:54 - train: epoch 0064, iter [04700, 05004], lr: 0.044853, loss: 2.2287
2022-07-10 23:14:29 - train: epoch 0064, iter [04800, 05004], lr: 0.044827, loss: 1.8680
2022-07-10 23:15:03 - train: epoch 0064, iter [04900, 05004], lr: 0.044801, loss: 1.8953
2022-07-10 23:15:36 - train: epoch 0064, iter [05000, 05004], lr: 0.044775, loss: 1.6116
2022-07-10 23:15:37 - train: epoch 064, train_loss: 1.7932
2022-07-10 23:16:53 - eval: epoch: 064, acc1: 62.746%, acc5: 85.174%, test_loss: 1.5515, per_image_load_time: 2.621ms, per_image_inference_time: 0.364ms
2022-07-10 23:16:53 - until epoch: 064, best_acc1: 62.982%
2022-07-10 23:16:53 - epoch 065 lr: 0.044773
2022-07-10 23:17:33 - train: epoch 0065, iter [00100, 05004], lr: 0.044748, loss: 1.9657
2022-07-10 23:18:07 - train: epoch 0065, iter [00200, 05004], lr: 0.044722, loss: 1.8819
2022-07-10 23:18:40 - train: epoch 0065, iter [00300, 05004], lr: 0.044696, loss: 1.5094
2022-07-10 23:19:14 - train: epoch 0065, iter [00400, 05004], lr: 0.044670, loss: 1.6765
2022-07-10 23:19:48 - train: epoch 0065, iter [00500, 05004], lr: 0.044644, loss: 1.9832
2022-07-10 23:20:23 - train: epoch 0065, iter [00600, 05004], lr: 0.044618, loss: 1.7971
2022-07-10 23:20:56 - train: epoch 0065, iter [00700, 05004], lr: 0.044592, loss: 2.0057
2022-07-10 23:21:31 - train: epoch 0065, iter [00800, 05004], lr: 0.044565, loss: 1.6067
2022-07-10 23:22:05 - train: epoch 0065, iter [00900, 05004], lr: 0.044539, loss: 1.7347
2022-07-10 23:22:39 - train: epoch 0065, iter [01000, 05004], lr: 0.044513, loss: 1.8531
2022-07-10 23:23:13 - train: epoch 0065, iter [01100, 05004], lr: 0.044487, loss: 1.8534
2022-07-10 23:23:47 - train: epoch 0065, iter [01200, 05004], lr: 0.044461, loss: 1.9420
2022-07-10 23:24:21 - train: epoch 0065, iter [01300, 05004], lr: 0.044435, loss: 1.8942
2022-07-10 23:24:55 - train: epoch 0065, iter [01400, 05004], lr: 0.044410, loss: 1.6675
2022-07-10 23:25:30 - train: epoch 0065, iter [01500, 05004], lr: 0.044384, loss: 1.7280
2022-07-10 23:26:03 - train: epoch 0065, iter [01600, 05004], lr: 0.044358, loss: 1.9274
2022-07-10 23:26:36 - train: epoch 0065, iter [01700, 05004], lr: 0.044332, loss: 1.7862
2022-07-10 23:27:11 - train: epoch 0065, iter [01800, 05004], lr: 0.044306, loss: 1.7724
2022-07-10 23:27:46 - train: epoch 0065, iter [01900, 05004], lr: 0.044280, loss: 1.5677
2022-07-10 23:28:19 - train: epoch 0065, iter [02000, 05004], lr: 0.044254, loss: 1.7394
2022-07-10 23:28:54 - train: epoch 0065, iter [02100, 05004], lr: 0.044228, loss: 1.6873
2022-07-10 23:29:27 - train: epoch 0065, iter [02200, 05004], lr: 0.044202, loss: 1.7319
2022-07-10 23:30:03 - train: epoch 0065, iter [02300, 05004], lr: 0.044176, loss: 1.9133
2022-07-10 23:30:36 - train: epoch 0065, iter [02400, 05004], lr: 0.044150, loss: 1.7969
2022-07-10 23:31:11 - train: epoch 0065, iter [02500, 05004], lr: 0.044124, loss: 1.7136
2022-07-10 23:31:45 - train: epoch 0065, iter [02600, 05004], lr: 0.044098, loss: 1.8460
2022-07-10 23:32:19 - train: epoch 0065, iter [02700, 05004], lr: 0.044072, loss: 1.7511
2022-07-10 23:32:53 - train: epoch 0065, iter [02800, 05004], lr: 0.044046, loss: 1.8124
2022-07-10 23:33:28 - train: epoch 0065, iter [02900, 05004], lr: 0.044020, loss: 1.5853
2022-07-10 23:34:02 - train: epoch 0065, iter [03000, 05004], lr: 0.043994, loss: 1.6757
2022-07-10 23:34:36 - train: epoch 0065, iter [03100, 05004], lr: 0.043968, loss: 1.8580
2022-07-10 23:35:10 - train: epoch 0065, iter [03200, 05004], lr: 0.043942, loss: 1.8885
2022-07-10 23:35:44 - train: epoch 0065, iter [03300, 05004], lr: 0.043916, loss: 1.9336
2022-07-10 23:36:18 - train: epoch 0065, iter [03400, 05004], lr: 0.043890, loss: 1.8878
2022-07-10 23:36:52 - train: epoch 0065, iter [03500, 05004], lr: 0.043864, loss: 2.1862
2022-07-10 23:37:27 - train: epoch 0065, iter [03600, 05004], lr: 0.043838, loss: 1.8820
2022-07-10 23:38:01 - train: epoch 0065, iter [03700, 05004], lr: 0.043812, loss: 1.7441
2022-07-10 23:38:35 - train: epoch 0065, iter [03800, 05004], lr: 0.043786, loss: 1.8016
2022-07-10 23:39:09 - train: epoch 0065, iter [03900, 05004], lr: 0.043760, loss: 2.0074
2022-07-10 23:39:43 - train: epoch 0065, iter [04000, 05004], lr: 0.043734, loss: 1.8860
2022-07-10 23:40:18 - train: epoch 0065, iter [04100, 05004], lr: 0.043708, loss: 1.7749
2022-07-10 23:40:52 - train: epoch 0065, iter [04200, 05004], lr: 0.043682, loss: 1.8211
2022-07-10 23:41:27 - train: epoch 0065, iter [04300, 05004], lr: 0.043656, loss: 1.6754
2022-07-10 23:42:02 - train: epoch 0065, iter [04400, 05004], lr: 0.043630, loss: 1.8271
2022-07-10 23:42:35 - train: epoch 0065, iter [04500, 05004], lr: 0.043604, loss: 1.6068
2022-07-10 23:43:10 - train: epoch 0065, iter [04600, 05004], lr: 0.043578, loss: 1.7681
2022-07-10 23:43:44 - train: epoch 0065, iter [04700, 05004], lr: 0.043553, loss: 1.7256
2022-07-10 23:44:18 - train: epoch 0065, iter [04800, 05004], lr: 0.043527, loss: 1.3980
2022-07-10 23:44:52 - train: epoch 0065, iter [04900, 05004], lr: 0.043501, loss: 1.7181
2022-07-10 23:45:25 - train: epoch 0065, iter [05000, 05004], lr: 0.043475, loss: 1.6685
2022-07-10 23:45:26 - train: epoch 065, train_loss: 1.7803
2022-07-10 23:46:42 - eval: epoch: 065, acc1: 63.130%, acc5: 85.440%, test_loss: 1.5286, per_image_load_time: 2.534ms, per_image_inference_time: 0.360ms
2022-07-10 23:46:42 - until epoch: 065, best_acc1: 63.130%
2022-07-10 23:46:42 - epoch 066 lr: 0.043473
2022-07-10 23:47:22 - train: epoch 0066, iter [00100, 05004], lr: 0.043448, loss: 1.5408
2022-07-10 23:47:56 - train: epoch 0066, iter [00200, 05004], lr: 0.043422, loss: 2.0049
2022-07-10 23:48:30 - train: epoch 0066, iter [00300, 05004], lr: 0.043396, loss: 1.8481
2022-07-10 23:49:04 - train: epoch 0066, iter [00400, 05004], lr: 0.043370, loss: 1.5805
2022-07-10 23:49:38 - train: epoch 0066, iter [00500, 05004], lr: 0.043344, loss: 1.7862
2022-07-10 23:50:12 - train: epoch 0066, iter [00600, 05004], lr: 0.043318, loss: 1.9043
2022-07-10 23:50:46 - train: epoch 0066, iter [00700, 05004], lr: 0.043292, loss: 1.7410
2022-07-10 23:51:19 - train: epoch 0066, iter [00800, 05004], lr: 0.043266, loss: 2.0340
2022-07-10 23:51:53 - train: epoch 0066, iter [00900, 05004], lr: 0.043240, loss: 1.5832
2022-07-10 23:52:28 - train: epoch 0066, iter [01000, 05004], lr: 0.043214, loss: 1.6440
2022-07-10 23:53:02 - train: epoch 0066, iter [01100, 05004], lr: 0.043189, loss: 1.7521
2022-07-10 23:53:36 - train: epoch 0066, iter [01200, 05004], lr: 0.043163, loss: 2.0028
2022-07-10 23:54:10 - train: epoch 0066, iter [01300, 05004], lr: 0.043137, loss: 1.9981
2022-07-10 23:54:45 - train: epoch 0066, iter [01400, 05004], lr: 0.043111, loss: 1.7561
2022-07-10 23:55:19 - train: epoch 0066, iter [01500, 05004], lr: 0.043085, loss: 1.7519
2022-07-10 23:55:53 - train: epoch 0066, iter [01600, 05004], lr: 0.043059, loss: 1.8167
2022-07-10 23:56:27 - train: epoch 0066, iter [01700, 05004], lr: 0.043033, loss: 1.8738
2022-07-10 23:57:02 - train: epoch 0066, iter [01800, 05004], lr: 0.043007, loss: 1.4681
2022-07-10 23:57:36 - train: epoch 0066, iter [01900, 05004], lr: 0.042981, loss: 2.0182
2022-07-10 23:58:10 - train: epoch 0066, iter [02000, 05004], lr: 0.042955, loss: 1.5753
2022-07-10 23:58:45 - train: epoch 0066, iter [02100, 05004], lr: 0.042929, loss: 1.6934
2022-07-10 23:59:19 - train: epoch 0066, iter [02200, 05004], lr: 0.042904, loss: 1.3809
2022-07-10 23:59:53 - train: epoch 0066, iter [02300, 05004], lr: 0.042878, loss: 1.8525
2022-07-11 00:00:28 - train: epoch 0066, iter [02400, 05004], lr: 0.042852, loss: 1.8241
2022-07-11 00:01:01 - train: epoch 0066, iter [02500, 05004], lr: 0.042826, loss: 1.7325
2022-07-11 00:01:36 - train: epoch 0066, iter [02600, 05004], lr: 0.042800, loss: 1.8540
2022-07-11 00:02:11 - train: epoch 0066, iter [02700, 05004], lr: 0.042774, loss: 1.9682
2022-07-11 00:02:45 - train: epoch 0066, iter [02800, 05004], lr: 0.042748, loss: 1.6074
2022-07-11 00:03:19 - train: epoch 0066, iter [02900, 05004], lr: 0.042722, loss: 1.9428
2022-07-11 00:03:53 - train: epoch 0066, iter [03000, 05004], lr: 0.042696, loss: 1.6883
2022-07-11 00:04:28 - train: epoch 0066, iter [03100, 05004], lr: 0.042671, loss: 1.8391
2022-07-11 00:05:02 - train: epoch 0066, iter [03200, 05004], lr: 0.042645, loss: 1.9156
2022-07-11 00:05:36 - train: epoch 0066, iter [03300, 05004], lr: 0.042619, loss: 1.6231
2022-07-11 00:06:11 - train: epoch 0066, iter [03400, 05004], lr: 0.042593, loss: 1.8493
2022-07-11 00:06:45 - train: epoch 0066, iter [03500, 05004], lr: 0.042567, loss: 2.0011
2022-07-11 00:07:20 - train: epoch 0066, iter [03600, 05004], lr: 0.042541, loss: 1.7340
2022-07-11 00:07:53 - train: epoch 0066, iter [03700, 05004], lr: 0.042515, loss: 1.6418
2022-07-11 00:08:27 - train: epoch 0066, iter [03800, 05004], lr: 0.042490, loss: 1.8044
2022-07-11 00:09:02 - train: epoch 0066, iter [03900, 05004], lr: 0.042464, loss: 1.5436
2022-07-11 00:09:36 - train: epoch 0066, iter [04000, 05004], lr: 0.042438, loss: 1.7160
2022-07-11 00:10:11 - train: epoch 0066, iter [04100, 05004], lr: 0.042412, loss: 1.6732
2022-07-11 00:10:45 - train: epoch 0066, iter [04200, 05004], lr: 0.042386, loss: 1.6122
2022-07-11 00:11:19 - train: epoch 0066, iter [04300, 05004], lr: 0.042360, loss: 1.8191
2022-07-11 00:11:53 - train: epoch 0066, iter [04400, 05004], lr: 0.042334, loss: 1.6356
2022-07-11 00:12:28 - train: epoch 0066, iter [04500, 05004], lr: 0.042309, loss: 1.9253
2022-07-11 00:13:02 - train: epoch 0066, iter [04600, 05004], lr: 0.042283, loss: 1.9545
2022-07-11 00:13:36 - train: epoch 0066, iter [04700, 05004], lr: 0.042257, loss: 1.5811
2022-07-11 00:14:10 - train: epoch 0066, iter [04800, 05004], lr: 0.042231, loss: 1.8453
2022-07-11 00:14:45 - train: epoch 0066, iter [04900, 05004], lr: 0.042205, loss: 1.5930
2022-07-11 00:15:17 - train: epoch 0066, iter [05000, 05004], lr: 0.042179, loss: 1.6359
2022-07-11 00:15:18 - train: epoch 066, train_loss: 1.7705
2022-07-11 00:16:34 - eval: epoch: 066, acc1: 63.126%, acc5: 85.450%, test_loss: 1.5305, per_image_load_time: 2.616ms, per_image_inference_time: 0.345ms
2022-07-11 00:16:34 - until epoch: 066, best_acc1: 63.130%
2022-07-11 00:16:34 - epoch 067 lr: 0.042178
2022-07-11 00:17:14 - train: epoch 0067, iter [00100, 05004], lr: 0.042152, loss: 1.7631
2022-07-11 00:17:47 - train: epoch 0067, iter [00200, 05004], lr: 0.042127, loss: 1.8378
2022-07-11 00:18:22 - train: epoch 0067, iter [00300, 05004], lr: 0.042101, loss: 1.5982
2022-07-11 00:18:55 - train: epoch 0067, iter [00400, 05004], lr: 0.042075, loss: 1.5540
2022-07-11 00:19:29 - train: epoch 0067, iter [00500, 05004], lr: 0.042049, loss: 1.7376
2022-07-11 00:20:02 - train: epoch 0067, iter [00600, 05004], lr: 0.042023, loss: 1.7560
2022-07-11 00:20:37 - train: epoch 0067, iter [00700, 05004], lr: 0.041997, loss: 1.7198
2022-07-11 00:21:11 - train: epoch 0067, iter [00800, 05004], lr: 0.041972, loss: 1.8786
2022-07-11 00:21:46 - train: epoch 0067, iter [00900, 05004], lr: 0.041946, loss: 1.9844
2022-07-11 00:22:19 - train: epoch 0067, iter [01000, 05004], lr: 0.041920, loss: 1.7779
2022-07-11 00:22:53 - train: epoch 0067, iter [01100, 05004], lr: 0.041894, loss: 1.6581
2022-07-11 00:23:28 - train: epoch 0067, iter [01200, 05004], lr: 0.041868, loss: 1.7029
2022-07-11 00:24:02 - train: epoch 0067, iter [01300, 05004], lr: 0.041843, loss: 1.7842
2022-07-11 00:24:33 - train: epoch 0067, iter [01400, 05004], lr: 0.041817, loss: 1.7118
2022-07-11 00:25:07 - train: epoch 0067, iter [01500, 05004], lr: 0.041791, loss: 1.5731
2022-07-11 00:25:41 - train: epoch 0067, iter [01600, 05004], lr: 0.041765, loss: 2.0381
2022-07-11 00:26:15 - train: epoch 0067, iter [01700, 05004], lr: 0.041739, loss: 1.5321
2022-07-11 00:26:50 - train: epoch 0067, iter [01800, 05004], lr: 0.041714, loss: 1.9973
2022-07-11 00:27:23 - train: epoch 0067, iter [01900, 05004], lr: 0.041688, loss: 1.7620
2022-07-11 00:27:57 - train: epoch 0067, iter [02000, 05004], lr: 0.041662, loss: 1.8470
2022-07-11 00:28:31 - train: epoch 0067, iter [02100, 05004], lr: 0.041636, loss: 1.4743
2022-07-11 00:29:06 - train: epoch 0067, iter [02200, 05004], lr: 0.041610, loss: 1.8030
2022-07-11 00:29:40 - train: epoch 0067, iter [02300, 05004], lr: 0.041585, loss: 1.8188
2022-07-11 00:30:14 - train: epoch 0067, iter [02400, 05004], lr: 0.041559, loss: 1.7670
2022-07-11 00:30:49 - train: epoch 0067, iter [02500, 05004], lr: 0.041533, loss: 1.7951
2022-07-11 00:31:22 - train: epoch 0067, iter [02600, 05004], lr: 0.041507, loss: 1.9003
2022-07-11 00:31:56 - train: epoch 0067, iter [02700, 05004], lr: 0.041481, loss: 1.6117
2022-07-11 00:32:30 - train: epoch 0067, iter [02800, 05004], lr: 0.041456, loss: 2.0865
2022-07-11 00:33:05 - train: epoch 0067, iter [02900, 05004], lr: 0.041430, loss: 1.6688
2022-07-11 00:33:38 - train: epoch 0067, iter [03000, 05004], lr: 0.041404, loss: 1.6693
2022-07-11 00:34:13 - train: epoch 0067, iter [03100, 05004], lr: 0.041378, loss: 1.8358
2022-07-11 00:34:47 - train: epoch 0067, iter [03200, 05004], lr: 0.041353, loss: 1.7970
2022-07-11 00:35:21 - train: epoch 0067, iter [03300, 05004], lr: 0.041327, loss: 1.7053
2022-07-11 00:35:55 - train: epoch 0067, iter [03400, 05004], lr: 0.041301, loss: 1.8125
2022-07-11 00:36:30 - train: epoch 0067, iter [03500, 05004], lr: 0.041275, loss: 1.8012
2022-07-11 00:37:04 - train: epoch 0067, iter [03600, 05004], lr: 0.041250, loss: 1.6532
2022-07-11 00:37:38 - train: epoch 0067, iter [03700, 05004], lr: 0.041224, loss: 1.8065
2022-07-11 00:38:13 - train: epoch 0067, iter [03800, 05004], lr: 0.041198, loss: 1.9477
2022-07-11 00:38:47 - train: epoch 0067, iter [03900, 05004], lr: 0.041172, loss: 1.9857
2022-07-11 00:39:21 - train: epoch 0067, iter [04000, 05004], lr: 0.041147, loss: 1.7745
2022-07-11 00:39:56 - train: epoch 0067, iter [04100, 05004], lr: 0.041121, loss: 1.8964
2022-07-11 00:40:30 - train: epoch 0067, iter [04200, 05004], lr: 0.041095, loss: 1.8551
2022-07-11 00:41:04 - train: epoch 0067, iter [04300, 05004], lr: 0.041069, loss: 1.6132
2022-07-11 00:41:39 - train: epoch 0067, iter [04400, 05004], lr: 0.041044, loss: 1.7481
2022-07-11 00:42:14 - train: epoch 0067, iter [04500, 05004], lr: 0.041018, loss: 1.7666
2022-07-11 00:42:48 - train: epoch 0067, iter [04600, 05004], lr: 0.040992, loss: 1.4840
2022-07-11 00:43:22 - train: epoch 0067, iter [04700, 05004], lr: 0.040966, loss: 1.6968
2022-07-11 00:43:56 - train: epoch 0067, iter [04800, 05004], lr: 0.040941, loss: 1.6155
2022-07-11 00:44:30 - train: epoch 0067, iter [04900, 05004], lr: 0.040915, loss: 1.8186
2022-07-11 00:45:03 - train: epoch 0067, iter [05000, 05004], lr: 0.040889, loss: 1.8983
2022-07-11 00:45:04 - train: epoch 067, train_loss: 1.7591
2022-07-11 00:46:20 - eval: epoch: 067, acc1: 63.650%, acc5: 85.754%, test_loss: 1.5087, per_image_load_time: 2.587ms, per_image_inference_time: 0.386ms
2022-07-11 00:46:20 - until epoch: 067, best_acc1: 63.650%
2022-07-11 00:46:20 - epoch 068 lr: 0.040888
2022-07-11 00:46:59 - train: epoch 0068, iter [00100, 05004], lr: 0.040863, loss: 1.8013
2022-07-11 00:47:34 - train: epoch 0068, iter [00200, 05004], lr: 0.040837, loss: 1.6201
2022-07-11 00:48:08 - train: epoch 0068, iter [00300, 05004], lr: 0.040811, loss: 1.6897
2022-07-11 00:48:42 - train: epoch 0068, iter [00400, 05004], lr: 0.040785, loss: 1.5478
2022-07-11 00:49:17 - train: epoch 0068, iter [00500, 05004], lr: 0.040760, loss: 1.6359
2022-07-11 00:49:51 - train: epoch 0068, iter [00600, 05004], lr: 0.040734, loss: 1.6108
2022-07-11 00:50:25 - train: epoch 0068, iter [00700, 05004], lr: 0.040708, loss: 1.7876
2022-07-11 00:50:59 - train: epoch 0068, iter [00800, 05004], lr: 0.040683, loss: 1.6344
2022-07-11 00:51:33 - train: epoch 0068, iter [00900, 05004], lr: 0.040657, loss: 1.8237
2022-07-11 00:52:07 - train: epoch 0068, iter [01000, 05004], lr: 0.040631, loss: 1.8894
2022-07-11 00:52:40 - train: epoch 0068, iter [01100, 05004], lr: 0.040605, loss: 2.0841
2022-07-11 00:53:14 - train: epoch 0068, iter [01200, 05004], lr: 0.040580, loss: 1.6746
2022-07-11 00:53:49 - train: epoch 0068, iter [01300, 05004], lr: 0.040554, loss: 1.8258
2022-07-11 00:54:22 - train: epoch 0068, iter [01400, 05004], lr: 0.040528, loss: 1.7765
2022-07-11 00:54:57 - train: epoch 0068, iter [01500, 05004], lr: 0.040503, loss: 1.6788
2022-07-11 00:55:31 - train: epoch 0068, iter [01600, 05004], lr: 0.040477, loss: 1.7752
2022-07-11 00:56:05 - train: epoch 0068, iter [01700, 05004], lr: 0.040451, loss: 1.7664
2022-07-11 00:56:39 - train: epoch 0068, iter [01800, 05004], lr: 0.040426, loss: 1.9218
2022-07-11 00:57:13 - train: epoch 0068, iter [01900, 05004], lr: 0.040400, loss: 1.7176
2022-07-11 00:57:47 - train: epoch 0068, iter [02000, 05004], lr: 0.040374, loss: 2.0445
2022-07-11 00:58:21 - train: epoch 0068, iter [02100, 05004], lr: 0.040349, loss: 1.5596
2022-07-11 00:58:56 - train: epoch 0068, iter [02200, 05004], lr: 0.040323, loss: 1.6535
2022-07-11 00:59:30 - train: epoch 0068, iter [02300, 05004], lr: 0.040297, loss: 1.5894
2022-07-11 01:00:04 - train: epoch 0068, iter [02400, 05004], lr: 0.040272, loss: 1.7258
2022-07-11 01:00:38 - train: epoch 0068, iter [02500, 05004], lr: 0.040246, loss: 1.8001
2022-07-11 01:01:13 - train: epoch 0068, iter [02600, 05004], lr: 0.040220, loss: 1.7543
2022-07-11 01:01:46 - train: epoch 0068, iter [02700, 05004], lr: 0.040195, loss: 1.7184
2022-07-11 01:02:21 - train: epoch 0068, iter [02800, 05004], lr: 0.040169, loss: 1.8110
2022-07-11 01:02:55 - train: epoch 0068, iter [02900, 05004], lr: 0.040143, loss: 1.7755
2022-07-11 01:03:29 - train: epoch 0068, iter [03000, 05004], lr: 0.040118, loss: 1.7818
2022-07-11 01:04:03 - train: epoch 0068, iter [03100, 05004], lr: 0.040092, loss: 1.5841
2022-07-11 01:04:38 - train: epoch 0068, iter [03200, 05004], lr: 0.040066, loss: 1.8529
2022-07-11 01:05:12 - train: epoch 0068, iter [03300, 05004], lr: 0.040041, loss: 1.7668
2022-07-11 01:05:45 - train: epoch 0068, iter [03400, 05004], lr: 0.040015, loss: 1.7165
2022-07-11 01:06:20 - train: epoch 0068, iter [03500, 05004], lr: 0.039990, loss: 1.6616
2022-07-11 01:06:54 - train: epoch 0068, iter [03600, 05004], lr: 0.039964, loss: 1.7282
2022-07-11 01:07:28 - train: epoch 0068, iter [03700, 05004], lr: 0.039938, loss: 1.8676
2022-07-11 01:08:03 - train: epoch 0068, iter [03800, 05004], lr: 0.039913, loss: 1.7459
2022-07-11 01:08:37 - train: epoch 0068, iter [03900, 05004], lr: 0.039887, loss: 1.8236
2022-07-11 01:09:11 - train: epoch 0068, iter [04000, 05004], lr: 0.039861, loss: 1.8008
2022-07-11 01:09:46 - train: epoch 0068, iter [04100, 05004], lr: 0.039836, loss: 1.5409
2022-07-11 01:10:20 - train: epoch 0068, iter [04200, 05004], lr: 0.039810, loss: 1.9168
2022-07-11 01:10:55 - train: epoch 0068, iter [04300, 05004], lr: 0.039785, loss: 1.8528
2022-07-11 01:11:28 - train: epoch 0068, iter [04400, 05004], lr: 0.039759, loss: 1.6780
2022-07-11 01:12:03 - train: epoch 0068, iter [04500, 05004], lr: 0.039733, loss: 1.5991
2022-07-11 01:12:37 - train: epoch 0068, iter [04600, 05004], lr: 0.039708, loss: 1.7267
2022-07-11 01:13:12 - train: epoch 0068, iter [04700, 05004], lr: 0.039682, loss: 2.0287
2022-07-11 01:13:45 - train: epoch 0068, iter [04800, 05004], lr: 0.039657, loss: 1.8190
2022-07-11 01:14:20 - train: epoch 0068, iter [04900, 05004], lr: 0.039631, loss: 1.6725
2022-07-11 01:14:53 - train: epoch 0068, iter [05000, 05004], lr: 0.039605, loss: 1.7926
2022-07-11 01:14:54 - train: epoch 068, train_loss: 1.7479
2022-07-11 01:16:10 - eval: epoch: 068, acc1: 62.996%, acc5: 85.684%, test_loss: 1.5289, per_image_load_time: 2.577ms, per_image_inference_time: 0.349ms
2022-07-11 01:16:10 - until epoch: 068, best_acc1: 63.650%
2022-07-11 01:16:10 - epoch 069 lr: 0.039604
2022-07-11 01:16:49 - train: epoch 0069, iter [00100, 05004], lr: 0.039579, loss: 1.8503
2022-07-11 01:17:24 - train: epoch 0069, iter [00200, 05004], lr: 0.039553, loss: 1.6566
2022-07-11 01:17:58 - train: epoch 0069, iter [00300, 05004], lr: 0.039528, loss: 1.8193
2022-07-11 01:18:32 - train: epoch 0069, iter [00400, 05004], lr: 0.039502, loss: 1.6155
2022-07-11 01:19:07 - train: epoch 0069, iter [00500, 05004], lr: 0.039477, loss: 1.7470
2022-07-11 01:19:41 - train: epoch 0069, iter [00600, 05004], lr: 0.039451, loss: 1.5218
2022-07-11 01:20:15 - train: epoch 0069, iter [00700, 05004], lr: 0.039425, loss: 1.8616
2022-07-11 01:20:48 - train: epoch 0069, iter [00800, 05004], lr: 0.039400, loss: 1.7707
2022-07-11 01:21:22 - train: epoch 0069, iter [00900, 05004], lr: 0.039374, loss: 1.6187
2022-07-11 01:21:57 - train: epoch 0069, iter [01000, 05004], lr: 0.039349, loss: 1.8120
2022-07-11 01:22:30 - train: epoch 0069, iter [01100, 05004], lr: 0.039323, loss: 1.7291
2022-07-11 01:23:04 - train: epoch 0069, iter [01200, 05004], lr: 0.039298, loss: 1.6736
2022-07-11 01:23:37 - train: epoch 0069, iter [01300, 05004], lr: 0.039272, loss: 1.9047
2022-07-11 01:24:11 - train: epoch 0069, iter [01400, 05004], lr: 0.039246, loss: 1.7142
2022-07-11 01:24:44 - train: epoch 0069, iter [01500, 05004], lr: 0.039221, loss: 1.8129
2022-07-11 01:25:17 - train: epoch 0069, iter [01600, 05004], lr: 0.039195, loss: 1.8814
2022-07-11 01:25:52 - train: epoch 0069, iter [01700, 05004], lr: 0.039170, loss: 1.7572
2022-07-11 01:26:25 - train: epoch 0069, iter [01800, 05004], lr: 0.039144, loss: 1.8626
2022-07-11 01:26:58 - train: epoch 0069, iter [01900, 05004], lr: 0.039119, loss: 1.7726
2022-07-11 01:27:33 - train: epoch 0069, iter [02000, 05004], lr: 0.039093, loss: 1.8161
2022-07-11 01:28:06 - train: epoch 0069, iter [02100, 05004], lr: 0.039068, loss: 1.5949
2022-07-11 01:28:40 - train: epoch 0069, iter [02200, 05004], lr: 0.039042, loss: 1.7902
2022-07-11 01:29:14 - train: epoch 0069, iter [02300, 05004], lr: 0.039017, loss: 1.6091
2022-07-11 01:29:48 - train: epoch 0069, iter [02400, 05004], lr: 0.038991, loss: 1.8932
2022-07-11 01:30:21 - train: epoch 0069, iter [02500, 05004], lr: 0.038966, loss: 1.4722
2022-07-11 01:30:55 - train: epoch 0069, iter [02600, 05004], lr: 0.038940, loss: 1.5631
2022-07-11 01:31:28 - train: epoch 0069, iter [02700, 05004], lr: 0.038915, loss: 1.8321
2022-07-11 01:32:03 - train: epoch 0069, iter [02800, 05004], lr: 0.038889, loss: 1.8683
2022-07-11 01:32:36 - train: epoch 0069, iter [02900, 05004], lr: 0.038864, loss: 1.8486
2022-07-11 01:33:10 - train: epoch 0069, iter [03000, 05004], lr: 0.038838, loss: 1.6758
2022-07-11 01:33:43 - train: epoch 0069, iter [03100, 05004], lr: 0.038813, loss: 1.9738
2022-07-11 01:34:17 - train: epoch 0069, iter [03200, 05004], lr: 0.038787, loss: 1.6660
2022-07-11 01:34:51 - train: epoch 0069, iter [03300, 05004], lr: 0.038762, loss: 1.7888
2022-07-11 01:35:26 - train: epoch 0069, iter [03400, 05004], lr: 0.038736, loss: 1.6923
2022-07-11 01:35:59 - train: epoch 0069, iter [03500, 05004], lr: 0.038711, loss: 1.5083
2022-07-11 01:36:32 - train: epoch 0069, iter [03600, 05004], lr: 0.038685, loss: 1.5921
2022-07-11 01:37:06 - train: epoch 0069, iter [03700, 05004], lr: 0.038660, loss: 1.7479
2022-07-11 01:37:41 - train: epoch 0069, iter [03800, 05004], lr: 0.038634, loss: 1.6843
2022-07-11 01:38:14 - train: epoch 0069, iter [03900, 05004], lr: 0.038609, loss: 1.7628
2022-07-11 01:38:48 - train: epoch 0069, iter [04000, 05004], lr: 0.038583, loss: 1.8193
2022-07-11 01:39:21 - train: epoch 0069, iter [04100, 05004], lr: 0.038558, loss: 1.6897
2022-07-11 01:39:55 - train: epoch 0069, iter [04200, 05004], lr: 0.038532, loss: 1.7047
2022-07-11 01:40:28 - train: epoch 0069, iter [04300, 05004], lr: 0.038507, loss: 1.6497
2022-07-11 01:41:02 - train: epoch 0069, iter [04400, 05004], lr: 0.038481, loss: 1.6467
2022-07-11 01:41:36 - train: epoch 0069, iter [04500, 05004], lr: 0.038456, loss: 1.8524
2022-07-11 01:42:10 - train: epoch 0069, iter [04600, 05004], lr: 0.038431, loss: 1.9359
2022-07-11 01:42:44 - train: epoch 0069, iter [04700, 05004], lr: 0.038405, loss: 1.8662
2022-07-11 01:43:17 - train: epoch 0069, iter [04800, 05004], lr: 0.038380, loss: 1.8459
2022-07-11 01:43:52 - train: epoch 0069, iter [04900, 05004], lr: 0.038354, loss: 1.6073
2022-07-11 01:44:24 - train: epoch 0069, iter [05000, 05004], lr: 0.038329, loss: 1.7307
2022-07-11 01:44:25 - train: epoch 069, train_loss: 1.7367
2022-07-11 01:45:40 - eval: epoch: 069, acc1: 63.658%, acc5: 85.692%, test_loss: 1.5106, per_image_load_time: 2.329ms, per_image_inference_time: 0.374ms
2022-07-11 01:45:40 - until epoch: 069, best_acc1: 63.658%
2022-07-11 01:45:40 - epoch 070 lr: 0.038327
2022-07-11 01:46:18 - train: epoch 0070, iter [00100, 05004], lr: 0.038302, loss: 1.5334
2022-07-11 01:46:52 - train: epoch 0070, iter [00200, 05004], lr: 0.038277, loss: 1.5879
2022-07-11 01:47:26 - train: epoch 0070, iter [00300, 05004], lr: 0.038251, loss: 1.8538
2022-07-11 01:47:59 - train: epoch 0070, iter [00400, 05004], lr: 0.038226, loss: 1.6655
2022-07-11 01:48:33 - train: epoch 0070, iter [00500, 05004], lr: 0.038201, loss: 1.8669
2022-07-11 01:49:07 - train: epoch 0070, iter [00600, 05004], lr: 0.038175, loss: 1.6761
2022-07-11 01:49:41 - train: epoch 0070, iter [00700, 05004], lr: 0.038150, loss: 1.5257
2022-07-11 01:50:14 - train: epoch 0070, iter [00800, 05004], lr: 0.038124, loss: 1.5899
2022-07-11 01:50:47 - train: epoch 0070, iter [00900, 05004], lr: 0.038099, loss: 1.7292
2022-07-11 01:51:21 - train: epoch 0070, iter [01000, 05004], lr: 0.038074, loss: 1.7135
2022-07-11 01:51:55 - train: epoch 0070, iter [01100, 05004], lr: 0.038048, loss: 1.8502
2022-07-11 01:52:28 - train: epoch 0070, iter [01200, 05004], lr: 0.038023, loss: 1.7107
2022-07-11 01:53:02 - train: epoch 0070, iter [01300, 05004], lr: 0.037997, loss: 1.6990
2022-07-11 01:53:36 - train: epoch 0070, iter [01400, 05004], lr: 0.037972, loss: 1.6932
2022-07-11 01:54:08 - train: epoch 0070, iter [01500, 05004], lr: 0.037947, loss: 1.7748
2022-07-11 01:54:42 - train: epoch 0070, iter [01600, 05004], lr: 0.037921, loss: 1.6843
2022-07-11 01:55:16 - train: epoch 0070, iter [01700, 05004], lr: 0.037896, loss: 1.7238
2022-07-11 01:55:50 - train: epoch 0070, iter [01800, 05004], lr: 0.037870, loss: 1.6793
2022-07-11 01:56:24 - train: epoch 0070, iter [01900, 05004], lr: 0.037845, loss: 1.5153
2022-07-11 01:56:57 - train: epoch 0070, iter [02000, 05004], lr: 0.037820, loss: 1.6450
2022-07-11 01:57:31 - train: epoch 0070, iter [02100, 05004], lr: 0.037794, loss: 1.8446
2022-07-11 01:58:05 - train: epoch 0070, iter [02200, 05004], lr: 0.037769, loss: 1.7020
2022-07-11 01:58:38 - train: epoch 0070, iter [02300, 05004], lr: 0.037744, loss: 1.8408
2022-07-11 01:59:12 - train: epoch 0070, iter [02400, 05004], lr: 0.037718, loss: 1.8347
2022-07-11 01:59:46 - train: epoch 0070, iter [02500, 05004], lr: 0.037693, loss: 1.5759
2022-07-11 02:00:20 - train: epoch 0070, iter [02600, 05004], lr: 0.037667, loss: 1.4965
2022-07-11 02:00:53 - train: epoch 0070, iter [02700, 05004], lr: 0.037642, loss: 1.9672
2022-07-11 02:01:27 - train: epoch 0070, iter [02800, 05004], lr: 0.037617, loss: 1.7470
2022-07-11 02:02:01 - train: epoch 0070, iter [02900, 05004], lr: 0.037591, loss: 1.6831
2022-07-11 02:02:34 - train: epoch 0070, iter [03000, 05004], lr: 0.037566, loss: 1.8339
2022-07-11 02:03:07 - train: epoch 0070, iter [03100, 05004], lr: 0.037541, loss: 1.6463
2022-07-11 02:03:42 - train: epoch 0070, iter [03200, 05004], lr: 0.037515, loss: 1.8000
2022-07-11 02:04:15 - train: epoch 0070, iter [03300, 05004], lr: 0.037490, loss: 1.6217
2022-07-11 02:04:49 - train: epoch 0070, iter [03400, 05004], lr: 0.037465, loss: 1.8297
2022-07-11 02:05:23 - train: epoch 0070, iter [03500, 05004], lr: 0.037439, loss: 1.7797
2022-07-11 02:05:56 - train: epoch 0070, iter [03600, 05004], lr: 0.037414, loss: 1.8776
2022-07-11 02:06:30 - train: epoch 0070, iter [03700, 05004], lr: 0.037389, loss: 1.6799
2022-07-11 02:07:04 - train: epoch 0070, iter [03800, 05004], lr: 0.037364, loss: 1.7593
2022-07-11 02:07:37 - train: epoch 0070, iter [03900, 05004], lr: 0.037338, loss: 1.4353
2022-07-11 02:08:10 - train: epoch 0070, iter [04000, 05004], lr: 0.037313, loss: 1.7996
2022-07-11 02:08:44 - train: epoch 0070, iter [04100, 05004], lr: 0.037288, loss: 1.6316
2022-07-11 02:09:17 - train: epoch 0070, iter [04200, 05004], lr: 0.037262, loss: 1.7274
2022-07-11 02:09:51 - train: epoch 0070, iter [04300, 05004], lr: 0.037237, loss: 1.6666
2022-07-11 02:10:25 - train: epoch 0070, iter [04400, 05004], lr: 0.037212, loss: 1.6804
2022-07-11 02:10:59 - train: epoch 0070, iter [04500, 05004], lr: 0.037186, loss: 1.7610
2022-07-11 02:11:32 - train: epoch 0070, iter [04600, 05004], lr: 0.037161, loss: 1.9214
2022-07-11 02:12:06 - train: epoch 0070, iter [04700, 05004], lr: 0.037136, loss: 1.5914
2022-07-11 02:12:40 - train: epoch 0070, iter [04800, 05004], lr: 0.037111, loss: 1.7141
2022-07-11 02:13:13 - train: epoch 0070, iter [04900, 05004], lr: 0.037085, loss: 1.9254
2022-07-11 02:13:46 - train: epoch 0070, iter [05000, 05004], lr: 0.037060, loss: 1.7816
2022-07-11 02:13:47 - train: epoch 070, train_loss: 1.7224
2022-07-11 02:15:01 - eval: epoch: 070, acc1: 64.044%, acc5: 86.164%, test_loss: 1.4847, per_image_load_time: 1.626ms, per_image_inference_time: 0.399ms
2022-07-11 02:15:02 - until epoch: 070, best_acc1: 64.044%
2022-07-11 02:15:02 - epoch 071 lr: 0.037059
2022-07-11 02:15:41 - train: epoch 0071, iter [00100, 05004], lr: 0.037034, loss: 1.6376
2022-07-11 02:16:14 - train: epoch 0071, iter [00200, 05004], lr: 0.037009, loss: 1.5734
2022-07-11 02:16:48 - train: epoch 0071, iter [00300, 05004], lr: 0.036983, loss: 1.9583
2022-07-11 02:17:21 - train: epoch 0071, iter [00400, 05004], lr: 0.036958, loss: 1.8008
2022-07-11 02:17:55 - train: epoch 0071, iter [00500, 05004], lr: 0.036933, loss: 1.9432
2022-07-11 02:18:28 - train: epoch 0071, iter [00600, 05004], lr: 0.036908, loss: 1.7000
2022-07-11 02:19:02 - train: epoch 0071, iter [00700, 05004], lr: 0.036882, loss: 1.8846
2022-07-11 02:19:36 - train: epoch 0071, iter [00800, 05004], lr: 0.036857, loss: 1.6272
2022-07-11 02:20:09 - train: epoch 0071, iter [00900, 05004], lr: 0.036832, loss: 1.8279
2022-07-11 02:20:43 - train: epoch 0071, iter [01000, 05004], lr: 0.036807, loss: 1.6944
2022-07-11 02:21:16 - train: epoch 0071, iter [01100, 05004], lr: 0.036781, loss: 1.9123
2022-07-11 02:21:51 - train: epoch 0071, iter [01200, 05004], lr: 0.036756, loss: 1.7227
2022-07-11 02:22:24 - train: epoch 0071, iter [01300, 05004], lr: 0.036731, loss: 1.7639
2022-07-11 02:22:58 - train: epoch 0071, iter [01400, 05004], lr: 0.036706, loss: 1.6132
2022-07-11 02:23:32 - train: epoch 0071, iter [01500, 05004], lr: 0.036680, loss: 1.6812
2022-07-11 02:24:06 - train: epoch 0071, iter [01600, 05004], lr: 0.036655, loss: 1.6613
2022-07-11 02:24:39 - train: epoch 0071, iter [01700, 05004], lr: 0.036630, loss: 1.5676
2022-07-11 02:25:13 - train: epoch 0071, iter [01800, 05004], lr: 0.036605, loss: 1.7763
2022-07-11 02:25:47 - train: epoch 0071, iter [01900, 05004], lr: 0.036580, loss: 1.5835
2022-07-11 02:26:21 - train: epoch 0071, iter [02000, 05004], lr: 0.036554, loss: 1.6511
2022-07-11 02:26:55 - train: epoch 0071, iter [02100, 05004], lr: 0.036529, loss: 1.6192
2022-07-11 02:27:29 - train: epoch 0071, iter [02200, 05004], lr: 0.036504, loss: 1.4427
2022-07-11 02:28:03 - train: epoch 0071, iter [02300, 05004], lr: 0.036479, loss: 1.5976
2022-07-11 02:28:37 - train: epoch 0071, iter [02400, 05004], lr: 0.036454, loss: 1.6155
2022-07-11 02:29:11 - train: epoch 0071, iter [02500, 05004], lr: 0.036428, loss: 1.9365
2022-07-11 02:29:44 - train: epoch 0071, iter [02600, 05004], lr: 0.036403, loss: 1.6738
2022-07-11 02:30:18 - train: epoch 0071, iter [02700, 05004], lr: 0.036378, loss: 1.6690
2022-07-11 02:30:52 - train: epoch 0071, iter [02800, 05004], lr: 0.036353, loss: 1.7824
2022-07-11 02:31:27 - train: epoch 0071, iter [02900, 05004], lr: 0.036328, loss: 1.6516
2022-07-11 02:32:00 - train: epoch 0071, iter [03000, 05004], lr: 0.036303, loss: 1.8228
2022-07-11 02:32:33 - train: epoch 0071, iter [03100, 05004], lr: 0.036277, loss: 1.5637
2022-07-11 02:33:08 - train: epoch 0071, iter [03200, 05004], lr: 0.036252, loss: 1.4636
2022-07-11 02:33:42 - train: epoch 0071, iter [03300, 05004], lr: 0.036227, loss: 1.5872
2022-07-11 02:34:16 - train: epoch 0071, iter [03400, 05004], lr: 0.036202, loss: 1.7123
2022-07-11 02:34:50 - train: epoch 0071, iter [03500, 05004], lr: 0.036177, loss: 1.8794
2022-07-11 02:35:24 - train: epoch 0071, iter [03600, 05004], lr: 0.036152, loss: 1.8378
2022-07-11 02:35:58 - train: epoch 0071, iter [03700, 05004], lr: 0.036127, loss: 1.7891
2022-07-11 02:36:32 - train: epoch 0071, iter [03800, 05004], lr: 0.036101, loss: 1.6839
2022-07-11 02:37:06 - train: epoch 0071, iter [03900, 05004], lr: 0.036076, loss: 1.7648
2022-07-11 02:37:40 - train: epoch 0071, iter [04000, 05004], lr: 0.036051, loss: 1.8162
2022-07-11 02:38:14 - train: epoch 0071, iter [04100, 05004], lr: 0.036026, loss: 1.6362
2022-07-11 02:38:49 - train: epoch 0071, iter [04200, 05004], lr: 0.036001, loss: 1.8069
2022-07-11 02:39:22 - train: epoch 0071, iter [04300, 05004], lr: 0.035976, loss: 1.5122
2022-07-11 02:39:56 - train: epoch 0071, iter [04400, 05004], lr: 0.035951, loss: 1.8600
2022-07-11 02:40:31 - train: epoch 0071, iter [04500, 05004], lr: 0.035926, loss: 1.8200
2022-07-11 02:41:04 - train: epoch 0071, iter [04600, 05004], lr: 0.035901, loss: 1.8329
2022-07-11 02:41:39 - train: epoch 0071, iter [04700, 05004], lr: 0.035875, loss: 1.6844
2022-07-11 02:42:13 - train: epoch 0071, iter [04800, 05004], lr: 0.035850, loss: 1.7728
2022-07-11 02:42:46 - train: epoch 0071, iter [04900, 05004], lr: 0.035825, loss: 1.5552
2022-07-11 02:43:18 - train: epoch 0071, iter [05000, 05004], lr: 0.035800, loss: 1.5618
2022-07-11 02:43:19 - train: epoch 071, train_loss: 1.7084
2022-07-11 02:44:34 - eval: epoch: 071, acc1: 64.518%, acc5: 86.034%, test_loss: 1.4833, per_image_load_time: 2.529ms, per_image_inference_time: 0.357ms
2022-07-11 02:44:34 - until epoch: 071, best_acc1: 64.518%
2022-07-11 02:44:34 - epoch 072 lr: 0.035799
2022-07-11 02:45:12 - train: epoch 0072, iter [00100, 05004], lr: 0.035774, loss: 1.6400
2022-07-11 02:45:46 - train: epoch 0072, iter [00200, 05004], lr: 0.035749, loss: 1.4555
2022-07-11 02:46:21 - train: epoch 0072, iter [00300, 05004], lr: 0.035724, loss: 1.5484
2022-07-11 02:46:54 - train: epoch 0072, iter [00400, 05004], lr: 0.035699, loss: 1.7320
2022-07-11 02:47:28 - train: epoch 0072, iter [00500, 05004], lr: 0.035674, loss: 1.5180
2022-07-11 02:48:01 - train: epoch 0072, iter [00600, 05004], lr: 0.035649, loss: 1.6541
2022-07-11 02:48:35 - train: epoch 0072, iter [00700, 05004], lr: 0.035624, loss: 1.5540
2022-07-11 02:49:08 - train: epoch 0072, iter [00800, 05004], lr: 0.035599, loss: 1.7345
2022-07-11 02:49:43 - train: epoch 0072, iter [00900, 05004], lr: 0.035574, loss: 1.7881
2022-07-11 02:50:16 - train: epoch 0072, iter [01000, 05004], lr: 0.035549, loss: 1.7116
2022-07-11 02:50:51 - train: epoch 0072, iter [01100, 05004], lr: 0.035524, loss: 1.9151
2022-07-11 02:51:25 - train: epoch 0072, iter [01200, 05004], lr: 0.035499, loss: 1.7700
2022-07-11 02:51:59 - train: epoch 0072, iter [01300, 05004], lr: 0.035474, loss: 1.8414
2022-07-11 02:52:32 - train: epoch 0072, iter [01400, 05004], lr: 0.035448, loss: 1.6901
2022-07-11 02:53:07 - train: epoch 0072, iter [01500, 05004], lr: 0.035423, loss: 1.7692
2022-07-11 02:53:41 - train: epoch 0072, iter [01600, 05004], lr: 0.035398, loss: 1.6678
2022-07-11 02:54:15 - train: epoch 0072, iter [01700, 05004], lr: 0.035373, loss: 1.5209
2022-07-11 02:54:49 - train: epoch 0072, iter [01800, 05004], lr: 0.035348, loss: 1.6293
2022-07-11 02:55:22 - train: epoch 0072, iter [01900, 05004], lr: 0.035323, loss: 1.6214
2022-07-11 02:55:57 - train: epoch 0072, iter [02000, 05004], lr: 0.035298, loss: 1.6692
2022-07-11 02:56:31 - train: epoch 0072, iter [02100, 05004], lr: 0.035273, loss: 1.8836
2022-07-11 02:57:05 - train: epoch 0072, iter [02200, 05004], lr: 0.035248, loss: 2.0685
2022-07-11 02:57:39 - train: epoch 0072, iter [02300, 05004], lr: 0.035223, loss: 1.8362
2022-07-11 02:58:12 - train: epoch 0072, iter [02400, 05004], lr: 0.035198, loss: 1.7720
2022-07-11 02:58:46 - train: epoch 0072, iter [02500, 05004], lr: 0.035173, loss: 1.5625
2022-07-11 02:59:20 - train: epoch 0072, iter [02600, 05004], lr: 0.035148, loss: 1.8170
2022-07-11 02:59:54 - train: epoch 0072, iter [02700, 05004], lr: 0.035123, loss: 1.7435
2022-07-11 03:00:28 - train: epoch 0072, iter [02800, 05004], lr: 0.035098, loss: 1.8617
2022-07-11 03:01:02 - train: epoch 0072, iter [02900, 05004], lr: 0.035074, loss: 1.7153
2022-07-11 03:01:36 - train: epoch 0072, iter [03000, 05004], lr: 0.035049, loss: 1.7790
2022-07-11 03:02:10 - train: epoch 0072, iter [03100, 05004], lr: 0.035024, loss: 1.8850
2022-07-11 03:02:44 - train: epoch 0072, iter [03200, 05004], lr: 0.034999, loss: 1.6397
2022-07-11 03:03:18 - train: epoch 0072, iter [03300, 05004], lr: 0.034974, loss: 1.7682
2022-07-11 03:03:52 - train: epoch 0072, iter [03400, 05004], lr: 0.034949, loss: 1.9015
2022-07-11 03:04:25 - train: epoch 0072, iter [03500, 05004], lr: 0.034924, loss: 1.7634
2022-07-11 03:04:59 - train: epoch 0072, iter [03600, 05004], lr: 0.034899, loss: 1.6628
2022-07-11 03:05:33 - train: epoch 0072, iter [03700, 05004], lr: 0.034874, loss: 1.7664
2022-07-11 03:06:07 - train: epoch 0072, iter [03800, 05004], lr: 0.034849, loss: 1.6693
2022-07-11 03:06:41 - train: epoch 0072, iter [03900, 05004], lr: 0.034824, loss: 1.6838
2022-07-11 03:07:14 - train: epoch 0072, iter [04000, 05004], lr: 0.034799, loss: 1.9433
2022-07-11 03:07:48 - train: epoch 0072, iter [04100, 05004], lr: 0.034774, loss: 1.8945
2022-07-11 03:08:22 - train: epoch 0072, iter [04200, 05004], lr: 0.034749, loss: 1.6178
2022-07-11 03:08:56 - train: epoch 0072, iter [04300, 05004], lr: 0.034724, loss: 1.5942
2022-07-11 03:09:29 - train: epoch 0072, iter [04400, 05004], lr: 0.034699, loss: 1.6163
2022-07-11 03:10:03 - train: epoch 0072, iter [04500, 05004], lr: 0.034675, loss: 1.7659
2022-07-11 03:10:37 - train: epoch 0072, iter [04600, 05004], lr: 0.034650, loss: 1.6248
2022-07-11 03:11:12 - train: epoch 0072, iter [04700, 05004], lr: 0.034625, loss: 1.7503
2022-07-11 03:11:45 - train: epoch 0072, iter [04800, 05004], lr: 0.034600, loss: 1.9357
2022-07-11 03:12:19 - train: epoch 0072, iter [04900, 05004], lr: 0.034575, loss: 1.8963
2022-07-11 03:12:51 - train: epoch 0072, iter [05000, 05004], lr: 0.034550, loss: 1.7536
2022-07-11 03:12:52 - train: epoch 072, train_loss: 1.6969
2022-07-11 03:14:07 - eval: epoch: 072, acc1: 64.196%, acc5: 86.172%, test_loss: 1.4767, per_image_load_time: 2.497ms, per_image_inference_time: 0.396ms
2022-07-11 03:14:07 - until epoch: 072, best_acc1: 64.518%
2022-07-11 03:14:07 - epoch 073 lr: 0.034549
2022-07-11 03:14:46 - train: epoch 0073, iter [00100, 05004], lr: 0.034524, loss: 1.9551
2022-07-11 03:15:20 - train: epoch 0073, iter [00200, 05004], lr: 0.034499, loss: 1.6734
2022-07-11 03:15:53 - train: epoch 0073, iter [00300, 05004], lr: 0.034475, loss: 1.7598
2022-07-11 03:16:26 - train: epoch 0073, iter [00400, 05004], lr: 0.034450, loss: 1.3925
2022-07-11 03:17:01 - train: epoch 0073, iter [00500, 05004], lr: 0.034425, loss: 1.4892
2022-07-11 03:17:35 - train: epoch 0073, iter [00600, 05004], lr: 0.034400, loss: 1.6486
2022-07-11 03:18:09 - train: epoch 0073, iter [00700, 05004], lr: 0.034375, loss: 1.8760
2022-07-11 03:18:42 - train: epoch 0073, iter [00800, 05004], lr: 0.034350, loss: 1.7185
2022-07-11 03:19:16 - train: epoch 0073, iter [00900, 05004], lr: 0.034325, loss: 1.5265
2022-07-11 03:19:48 - train: epoch 0073, iter [01000, 05004], lr: 0.034301, loss: 1.5640
2022-07-11 03:20:23 - train: epoch 0073, iter [01100, 05004], lr: 0.034276, loss: 1.7968
2022-07-11 03:20:56 - train: epoch 0073, iter [01200, 05004], lr: 0.034251, loss: 1.8525
2022-07-11 03:21:30 - train: epoch 0073, iter [01300, 05004], lr: 0.034226, loss: 1.7600
2022-07-11 03:22:03 - train: epoch 0073, iter [01400, 05004], lr: 0.034201, loss: 1.4899
2022-07-11 03:22:37 - train: epoch 0073, iter [01500, 05004], lr: 0.034176, loss: 1.7372
2022-07-11 03:23:11 - train: epoch 0073, iter [01600, 05004], lr: 0.034152, loss: 1.7534
2022-07-11 03:23:44 - train: epoch 0073, iter [01700, 05004], lr: 0.034127, loss: 1.9090
2022-07-11 03:24:18 - train: epoch 0073, iter [01800, 05004], lr: 0.034102, loss: 1.4487
2022-07-11 03:24:51 - train: epoch 0073, iter [01900, 05004], lr: 0.034077, loss: 1.8732
2022-07-11 03:25:26 - train: epoch 0073, iter [02000, 05004], lr: 0.034052, loss: 1.5786
2022-07-11 03:25:59 - train: epoch 0073, iter [02100, 05004], lr: 0.034028, loss: 1.6447
2022-07-11 03:26:33 - train: epoch 0073, iter [02200, 05004], lr: 0.034003, loss: 1.4540
2022-07-11 03:27:06 - train: epoch 0073, iter [02300, 05004], lr: 0.033978, loss: 1.8302
2022-07-11 03:27:40 - train: epoch 0073, iter [02400, 05004], lr: 0.033953, loss: 1.7501
2022-07-11 03:28:15 - train: epoch 0073, iter [02500, 05004], lr: 0.033929, loss: 1.9326
2022-07-11 03:28:47 - train: epoch 0073, iter [02600, 05004], lr: 0.033904, loss: 1.8512
2022-07-11 03:29:21 - train: epoch 0073, iter [02700, 05004], lr: 0.033879, loss: 1.5764
2022-07-11 03:29:55 - train: epoch 0073, iter [02800, 05004], lr: 0.033854, loss: 1.5713
2022-07-11 03:30:29 - train: epoch 0073, iter [02900, 05004], lr: 0.033829, loss: 1.8433
2022-07-11 03:31:03 - train: epoch 0073, iter [03000, 05004], lr: 0.033805, loss: 1.4342
2022-07-11 03:31:36 - train: epoch 0073, iter [03100, 05004], lr: 0.033780, loss: 1.7088
2022-07-11 03:32:10 - train: epoch 0073, iter [03200, 05004], lr: 0.033755, loss: 1.5535
2022-07-11 03:32:43 - train: epoch 0073, iter [03300, 05004], lr: 0.033730, loss: 1.5762
2022-07-11 03:33:16 - train: epoch 0073, iter [03400, 05004], lr: 0.033706, loss: 1.6601
2022-07-11 03:33:51 - train: epoch 0073, iter [03500, 05004], lr: 0.033681, loss: 1.6778
2022-07-11 03:34:24 - train: epoch 0073, iter [03600, 05004], lr: 0.033656, loss: 1.5948
2022-07-11 03:34:58 - train: epoch 0073, iter [03700, 05004], lr: 0.033632, loss: 1.7942
2022-07-11 03:35:32 - train: epoch 0073, iter [03800, 05004], lr: 0.033607, loss: 1.8651
2022-07-11 03:36:05 - train: epoch 0073, iter [03900, 05004], lr: 0.033582, loss: 1.6349
2022-07-11 03:36:39 - train: epoch 0073, iter [04000, 05004], lr: 0.033557, loss: 1.8101
2022-07-11 03:37:12 - train: epoch 0073, iter [04100, 05004], lr: 0.033533, loss: 1.7196
2022-07-11 03:37:47 - train: epoch 0073, iter [04200, 05004], lr: 0.033508, loss: 1.8583
2022-07-11 03:38:20 - train: epoch 0073, iter [04300, 05004], lr: 0.033483, loss: 1.7874
2022-07-11 03:38:54 - train: epoch 0073, iter [04400, 05004], lr: 0.033459, loss: 1.7343
2022-07-11 03:39:27 - train: epoch 0073, iter [04500, 05004], lr: 0.033434, loss: 1.5742
2022-07-11 03:40:01 - train: epoch 0073, iter [04600, 05004], lr: 0.033409, loss: 1.7380
2022-07-11 03:40:35 - train: epoch 0073, iter [04700, 05004], lr: 0.033385, loss: 1.5327
2022-07-11 03:41:09 - train: epoch 0073, iter [04800, 05004], lr: 0.033360, loss: 1.6211
2022-07-11 03:41:43 - train: epoch 0073, iter [04900, 05004], lr: 0.033335, loss: 1.7390
2022-07-11 03:42:14 - train: epoch 0073, iter [05000, 05004], lr: 0.033311, loss: 1.7905
2022-07-11 03:42:15 - train: epoch 073, train_loss: 1.6855
2022-07-11 03:43:29 - eval: epoch: 073, acc1: 64.454%, acc5: 86.386%, test_loss: 1.4612, per_image_load_time: 1.774ms, per_image_inference_time: 0.393ms
2022-07-11 03:43:29 - until epoch: 073, best_acc1: 64.518%
2022-07-11 03:43:29 - epoch 074 lr: 0.033309
2022-07-11 03:44:08 - train: epoch 0074, iter [00100, 05004], lr: 0.033285, loss: 1.5586
2022-07-11 03:44:41 - train: epoch 0074, iter [00200, 05004], lr: 0.033260, loss: 1.8378
2022-07-11 03:45:16 - train: epoch 0074, iter [00300, 05004], lr: 0.033236, loss: 1.9478
2022-07-11 03:45:49 - train: epoch 0074, iter [00400, 05004], lr: 0.033211, loss: 1.9482
2022-07-11 03:46:23 - train: epoch 0074, iter [00500, 05004], lr: 0.033186, loss: 1.5567
2022-07-11 03:46:56 - train: epoch 0074, iter [00600, 05004], lr: 0.033162, loss: 1.6331
2022-07-11 03:47:30 - train: epoch 0074, iter [00700, 05004], lr: 0.033137, loss: 1.5349
2022-07-11 03:48:03 - train: epoch 0074, iter [00800, 05004], lr: 0.033113, loss: 1.9223
2022-07-11 03:48:38 - train: epoch 0074, iter [00900, 05004], lr: 0.033088, loss: 1.4235
2022-07-11 03:49:12 - train: epoch 0074, iter [01000, 05004], lr: 0.033063, loss: 1.7184
2022-07-11 03:49:44 - train: epoch 0074, iter [01100, 05004], lr: 0.033039, loss: 1.7333
2022-07-11 03:50:19 - train: epoch 0074, iter [01200, 05004], lr: 0.033014, loss: 1.6218
2022-07-11 03:50:52 - train: epoch 0074, iter [01300, 05004], lr: 0.032989, loss: 1.8804
2022-07-11 03:51:26 - train: epoch 0074, iter [01400, 05004], lr: 0.032965, loss: 1.6155
2022-07-11 03:52:00 - train: epoch 0074, iter [01500, 05004], lr: 0.032940, loss: 1.8677
2022-07-11 03:52:34 - train: epoch 0074, iter [01600, 05004], lr: 0.032916, loss: 1.5849
2022-07-11 03:53:08 - train: epoch 0074, iter [01700, 05004], lr: 0.032891, loss: 1.8380
2022-07-11 03:53:41 - train: epoch 0074, iter [01800, 05004], lr: 0.032867, loss: 1.7003
2022-07-11 03:54:15 - train: epoch 0074, iter [01900, 05004], lr: 0.032842, loss: 1.5972
2022-07-11 03:54:49 - train: epoch 0074, iter [02000, 05004], lr: 0.032817, loss: 1.5544
2022-07-11 03:55:24 - train: epoch 0074, iter [02100, 05004], lr: 0.032793, loss: 1.6533
2022-07-11 03:55:57 - train: epoch 0074, iter [02200, 05004], lr: 0.032768, loss: 1.8736
2022-07-11 03:56:30 - train: epoch 0074, iter [02300, 05004], lr: 0.032744, loss: 1.8399
2022-07-11 03:57:04 - train: epoch 0074, iter [02400, 05004], lr: 0.032719, loss: 1.6970
2022-07-11 03:57:39 - train: epoch 0074, iter [02500, 05004], lr: 0.032695, loss: 1.5430
2022-07-11 03:58:12 - train: epoch 0074, iter [02600, 05004], lr: 0.032670, loss: 1.5135
2022-07-11 03:58:47 - train: epoch 0074, iter [02700, 05004], lr: 0.032646, loss: 1.6040
2022-07-11 03:59:20 - train: epoch 0074, iter [02800, 05004], lr: 0.032621, loss: 2.1124
2022-07-11 03:59:54 - train: epoch 0074, iter [02900, 05004], lr: 0.032597, loss: 1.6947
2022-07-11 04:00:28 - train: epoch 0074, iter [03000, 05004], lr: 0.032572, loss: 1.8454
2022-07-11 04:01:01 - train: epoch 0074, iter [03100, 05004], lr: 0.032547, loss: 1.6664
2022-07-11 04:01:36 - train: epoch 0074, iter [03200, 05004], lr: 0.032523, loss: 1.6177
2022-07-11 04:02:09 - train: epoch 0074, iter [03300, 05004], lr: 0.032498, loss: 1.6546
2022-07-11 04:02:43 - train: epoch 0074, iter [03400, 05004], lr: 0.032474, loss: 1.6711
2022-07-11 04:03:17 - train: epoch 0074, iter [03500, 05004], lr: 0.032449, loss: 1.5912
2022-07-11 04:03:51 - train: epoch 0074, iter [03600, 05004], lr: 0.032425, loss: 1.8602
2022-07-11 04:04:24 - train: epoch 0074, iter [03700, 05004], lr: 0.032400, loss: 1.6064
2022-07-11 04:04:59 - train: epoch 0074, iter [03800, 05004], lr: 0.032376, loss: 1.4949
2022-07-11 04:05:32 - train: epoch 0074, iter [03900, 05004], lr: 0.032352, loss: 1.5873
2022-07-11 04:06:07 - train: epoch 0074, iter [04000, 05004], lr: 0.032327, loss: 1.7177
2022-07-11 04:06:41 - train: epoch 0074, iter [04100, 05004], lr: 0.032303, loss: 1.8088
2022-07-11 04:07:14 - train: epoch 0074, iter [04200, 05004], lr: 0.032278, loss: 1.6145
2022-07-11 04:07:48 - train: epoch 0074, iter [04300, 05004], lr: 0.032254, loss: 1.6810
2022-07-11 04:08:23 - train: epoch 0074, iter [04400, 05004], lr: 0.032229, loss: 1.5189
2022-07-11 04:08:56 - train: epoch 0074, iter [04500, 05004], lr: 0.032205, loss: 1.7958
2022-07-11 04:09:30 - train: epoch 0074, iter [04600, 05004], lr: 0.032180, loss: 1.8518
2022-07-11 04:10:04 - train: epoch 0074, iter [04700, 05004], lr: 0.032156, loss: 1.4682
2022-07-11 04:10:38 - train: epoch 0074, iter [04800, 05004], lr: 0.032131, loss: 1.5547
2022-07-11 04:11:12 - train: epoch 0074, iter [04900, 05004], lr: 0.032107, loss: 2.0422
2022-07-11 04:11:44 - train: epoch 0074, iter [05000, 05004], lr: 0.032083, loss: 1.7878
2022-07-11 04:11:45 - train: epoch 074, train_loss: 1.6730
2022-07-11 04:13:00 - eval: epoch: 074, acc1: 64.328%, acc5: 86.384%, test_loss: 1.4600, per_image_load_time: 2.503ms, per_image_inference_time: 0.369ms
2022-07-11 04:13:01 - until epoch: 074, best_acc1: 64.518%
2022-07-11 04:13:01 - epoch 075 lr: 0.032081
2022-07-11 04:13:40 - train: epoch 0075, iter [00100, 05004], lr: 0.032057, loss: 1.9029
2022-07-11 04:14:13 - train: epoch 0075, iter [00200, 05004], lr: 0.032033, loss: 1.6566
2022-07-11 04:14:48 - train: epoch 0075, iter [00300, 05004], lr: 0.032008, loss: 1.5451
2022-07-11 04:15:21 - train: epoch 0075, iter [00400, 05004], lr: 0.031984, loss: 1.6878
2022-07-11 04:15:55 - train: epoch 0075, iter [00500, 05004], lr: 0.031960, loss: 1.6736
2022-07-11 04:16:29 - train: epoch 0075, iter [00600, 05004], lr: 0.031935, loss: 1.7075
2022-07-11 04:17:02 - train: epoch 0075, iter [00700, 05004], lr: 0.031911, loss: 1.7719
2022-07-11 04:17:36 - train: epoch 0075, iter [00800, 05004], lr: 0.031886, loss: 1.7917
2022-07-11 04:18:11 - train: epoch 0075, iter [00900, 05004], lr: 0.031862, loss: 1.8189
2022-07-11 04:18:45 - train: epoch 0075, iter [01000, 05004], lr: 0.031838, loss: 1.6606
2022-07-11 04:19:19 - train: epoch 0075, iter [01100, 05004], lr: 0.031813, loss: 1.5746
2022-07-11 04:19:52 - train: epoch 0075, iter [01200, 05004], lr: 0.031789, loss: 1.5426
2022-07-11 04:20:27 - train: epoch 0075, iter [01300, 05004], lr: 0.031765, loss: 1.4471
2022-07-11 04:21:01 - train: epoch 0075, iter [01400, 05004], lr: 0.031740, loss: 1.7140
2022-07-11 04:21:35 - train: epoch 0075, iter [01500, 05004], lr: 0.031716, loss: 1.9902
2022-07-11 04:22:09 - train: epoch 0075, iter [01600, 05004], lr: 0.031691, loss: 1.6726
2022-07-11 04:22:43 - train: epoch 0075, iter [01700, 05004], lr: 0.031667, loss: 1.7146
2022-07-11 04:23:18 - train: epoch 0075, iter [01800, 05004], lr: 0.031643, loss: 1.6306
2022-07-11 04:23:51 - train: epoch 0075, iter [01900, 05004], lr: 0.031618, loss: 1.5293
2022-07-11 04:24:26 - train: epoch 0075, iter [02000, 05004], lr: 0.031594, loss: 1.7942
2022-07-11 04:25:00 - train: epoch 0075, iter [02100, 05004], lr: 0.031570, loss: 1.4994
2022-07-11 04:25:34 - train: epoch 0075, iter [02200, 05004], lr: 0.031546, loss: 1.5533
2022-07-11 04:26:09 - train: epoch 0075, iter [02300, 05004], lr: 0.031521, loss: 1.5559
2022-07-11 04:26:42 - train: epoch 0075, iter [02400, 05004], lr: 0.031497, loss: 1.6458
2022-07-11 04:27:17 - train: epoch 0075, iter [02500, 05004], lr: 0.031473, loss: 1.5675
2022-07-11 04:27:51 - train: epoch 0075, iter [02600, 05004], lr: 0.031448, loss: 1.6295
2022-07-11 04:28:25 - train: epoch 0075, iter [02700, 05004], lr: 0.031424, loss: 1.6291
2022-07-11 04:28:58 - train: epoch 0075, iter [02800, 05004], lr: 0.031400, loss: 1.4669
2022-07-11 04:29:32 - train: epoch 0075, iter [02900, 05004], lr: 0.031375, loss: 1.8702
2022-07-11 04:30:07 - train: epoch 0075, iter [03000, 05004], lr: 0.031351, loss: 1.8760
2022-07-11 04:30:41 - train: epoch 0075, iter [03100, 05004], lr: 0.031327, loss: 1.6453
2022-07-11 04:31:14 - train: epoch 0075, iter [03200, 05004], lr: 0.031303, loss: 1.7657
2022-07-11 04:31:49 - train: epoch 0075, iter [03300, 05004], lr: 0.031278, loss: 1.5859
2022-07-11 04:32:23 - train: epoch 0075, iter [03400, 05004], lr: 0.031254, loss: 1.6449
2022-07-11 04:32:58 - train: epoch 0075, iter [03500, 05004], lr: 0.031230, loss: 1.7287
2022-07-11 04:33:32 - train: epoch 0075, iter [03600, 05004], lr: 0.031206, loss: 1.7922
2022-07-11 04:34:05 - train: epoch 0075, iter [03700, 05004], lr: 0.031181, loss: 1.8907
2022-07-11 04:34:39 - train: epoch 0075, iter [03800, 05004], lr: 0.031157, loss: 1.6560
2022-07-11 04:35:14 - train: epoch 0075, iter [03900, 05004], lr: 0.031133, loss: 1.6663
2022-07-11 04:35:47 - train: epoch 0075, iter [04000, 05004], lr: 0.031109, loss: 1.5837
2022-07-11 04:36:22 - train: epoch 0075, iter [04100, 05004], lr: 0.031085, loss: 1.5949
2022-07-11 04:36:55 - train: epoch 0075, iter [04200, 05004], lr: 0.031060, loss: 1.9745
2022-07-11 04:37:29 - train: epoch 0075, iter [04300, 05004], lr: 0.031036, loss: 1.7640
2022-07-11 04:38:03 - train: epoch 0075, iter [04400, 05004], lr: 0.031012, loss: 1.5320
2022-07-11 04:38:37 - train: epoch 0075, iter [04500, 05004], lr: 0.030988, loss: 1.6494
2022-07-11 04:39:11 - train: epoch 0075, iter [04600, 05004], lr: 0.030964, loss: 1.4571
2022-07-11 04:39:45 - train: epoch 0075, iter [04700, 05004], lr: 0.030939, loss: 1.7291
2022-07-11 04:40:20 - train: epoch 0075, iter [04800, 05004], lr: 0.030915, loss: 1.6679
2022-07-11 04:40:54 - train: epoch 0075, iter [04900, 05004], lr: 0.030891, loss: 1.5262
2022-07-11 04:41:26 - train: epoch 0075, iter [05000, 05004], lr: 0.030867, loss: 1.6895
2022-07-11 04:41:27 - train: epoch 075, train_loss: 1.6610
2022-07-11 04:42:43 - eval: epoch: 075, acc1: 65.206%, acc5: 86.456%, test_loss: 1.4546, per_image_load_time: 0.896ms, per_image_inference_time: 0.366ms
2022-07-11 04:42:43 - until epoch: 075, best_acc1: 65.206%
2022-07-11 04:42:43 - epoch 076 lr: 0.030866
2022-07-11 04:43:23 - train: epoch 0076, iter [00100, 05004], lr: 0.030842, loss: 1.7458
2022-07-11 04:43:57 - train: epoch 0076, iter [00200, 05004], lr: 0.030818, loss: 1.5456
2022-07-11 04:44:32 - train: epoch 0076, iter [00300, 05004], lr: 0.030793, loss: 1.5843
2022-07-11 04:45:06 - train: epoch 0076, iter [00400, 05004], lr: 0.030769, loss: 1.5722
2022-07-11 04:45:41 - train: epoch 0076, iter [00500, 05004], lr: 0.030745, loss: 1.7415
2022-07-11 04:46:15 - train: epoch 0076, iter [00600, 05004], lr: 0.030721, loss: 1.7384
2022-07-11 04:46:49 - train: epoch 0076, iter [00700, 05004], lr: 0.030697, loss: 1.6257
2022-07-11 04:47:24 - train: epoch 0076, iter [00800, 05004], lr: 0.030673, loss: 1.6175
2022-07-11 04:47:58 - train: epoch 0076, iter [00900, 05004], lr: 0.030649, loss: 1.5459
2022-07-11 04:48:32 - train: epoch 0076, iter [01000, 05004], lr: 0.030624, loss: 1.6941
2022-07-11 04:49:07 - train: epoch 0076, iter [01100, 05004], lr: 0.030600, loss: 1.5273
2022-07-11 04:49:42 - train: epoch 0076, iter [01200, 05004], lr: 0.030576, loss: 1.6035
2022-07-11 04:50:17 - train: epoch 0076, iter [01300, 05004], lr: 0.030552, loss: 1.8087
2022-07-11 04:50:52 - train: epoch 0076, iter [01400, 05004], lr: 0.030528, loss: 1.8089
2022-07-11 04:51:27 - train: epoch 0076, iter [01500, 05004], lr: 0.030504, loss: 1.6428
2022-07-11 04:52:01 - train: epoch 0076, iter [01600, 05004], lr: 0.030480, loss: 1.7174
2022-07-11 04:52:36 - train: epoch 0076, iter [01700, 05004], lr: 0.030456, loss: 1.5109
2022-07-11 04:53:11 - train: epoch 0076, iter [01800, 05004], lr: 0.030432, loss: 1.5173
2022-07-11 04:53:47 - train: epoch 0076, iter [01900, 05004], lr: 0.030408, loss: 1.6609
2022-07-11 04:54:21 - train: epoch 0076, iter [02000, 05004], lr: 0.030384, loss: 1.5207
2022-07-11 04:54:56 - train: epoch 0076, iter [02100, 05004], lr: 0.030359, loss: 1.7352
2022-07-11 04:55:31 - train: epoch 0076, iter [02200, 05004], lr: 0.030335, loss: 1.5637
2022-07-11 04:56:05 - train: epoch 0076, iter [02300, 05004], lr: 0.030311, loss: 1.4367
2022-07-11 04:56:42 - train: epoch 0076, iter [02400, 05004], lr: 0.030287, loss: 1.7050
2022-07-11 04:57:16 - train: epoch 0076, iter [02500, 05004], lr: 0.030263, loss: 1.5857
2022-07-11 04:57:52 - train: epoch 0076, iter [02600, 05004], lr: 0.030239, loss: 1.8630
2022-07-11 04:58:26 - train: epoch 0076, iter [02700, 05004], lr: 0.030215, loss: 1.7241
2022-07-11 04:59:01 - train: epoch 0076, iter [02800, 05004], lr: 0.030191, loss: 1.6238
2022-07-11 04:59:36 - train: epoch 0076, iter [02900, 05004], lr: 0.030167, loss: 1.6775
2022-07-11 05:00:11 - train: epoch 0076, iter [03000, 05004], lr: 0.030143, loss: 1.5869
2022-07-11 05:00:46 - train: epoch 0076, iter [03100, 05004], lr: 0.030119, loss: 1.7119
2022-07-11 05:01:21 - train: epoch 0076, iter [03200, 05004], lr: 0.030095, loss: 1.5856
2022-07-11 05:01:57 - train: epoch 0076, iter [03300, 05004], lr: 0.030071, loss: 1.5113
2022-07-11 05:02:32 - train: epoch 0076, iter [03400, 05004], lr: 0.030047, loss: 1.6016
2022-07-11 05:03:06 - train: epoch 0076, iter [03500, 05004], lr: 0.030023, loss: 1.6262
2022-07-11 05:03:42 - train: epoch 0076, iter [03600, 05004], lr: 0.029999, loss: 1.4364
2022-07-11 05:04:17 - train: epoch 0076, iter [03700, 05004], lr: 0.029975, loss: 1.6218
2022-07-11 05:04:52 - train: epoch 0076, iter [03800, 05004], lr: 0.029951, loss: 1.5122
2022-07-11 05:05:28 - train: epoch 0076, iter [03900, 05004], lr: 0.029927, loss: 1.6574
2022-07-11 05:06:03 - train: epoch 0076, iter [04000, 05004], lr: 0.029903, loss: 1.6776
2022-07-11 05:06:39 - train: epoch 0076, iter [04100, 05004], lr: 0.029879, loss: 1.5726
2022-07-11 05:07:14 - train: epoch 0076, iter [04200, 05004], lr: 0.029855, loss: 1.8975
2022-07-11 05:07:49 - train: epoch 0076, iter [04300, 05004], lr: 0.029832, loss: 1.8046
2022-07-11 05:08:25 - train: epoch 0076, iter [04400, 05004], lr: 0.029808, loss: 1.5649
2022-07-11 05:09:00 - train: epoch 0076, iter [04500, 05004], lr: 0.029784, loss: 1.7853
2022-07-11 05:09:34 - train: epoch 0076, iter [04600, 05004], lr: 0.029760, loss: 1.6040
2022-07-11 05:10:10 - train: epoch 0076, iter [04700, 05004], lr: 0.029736, loss: 1.5901
2022-07-11 05:10:45 - train: epoch 0076, iter [04800, 05004], lr: 0.029712, loss: 1.5119
2022-07-11 05:11:20 - train: epoch 0076, iter [04900, 05004], lr: 0.029688, loss: 1.5710
2022-07-11 05:11:53 - train: epoch 0076, iter [05000, 05004], lr: 0.029664, loss: 1.8066
2022-07-11 05:11:54 - train: epoch 076, train_loss: 1.6471
2022-07-11 05:13:11 - eval: epoch: 076, acc1: 64.856%, acc5: 86.586%, test_loss: 1.4594, per_image_load_time: 2.560ms, per_image_inference_time: 0.393ms
2022-07-11 05:13:11 - until epoch: 076, best_acc1: 65.206%
2022-07-11 05:13:11 - epoch 077 lr: 0.029663
2022-07-11 05:13:51 - train: epoch 0077, iter [00100, 05004], lr: 0.029639, loss: 1.8654
2022-07-11 05:14:25 - train: epoch 0077, iter [00200, 05004], lr: 0.029615, loss: 1.5527
2022-07-11 05:15:00 - train: epoch 0077, iter [00300, 05004], lr: 0.029592, loss: 1.4905
2022-07-11 05:15:35 - train: epoch 0077, iter [00400, 05004], lr: 0.029568, loss: 1.5750
2022-07-11 05:16:08 - train: epoch 0077, iter [00500, 05004], lr: 0.029544, loss: 1.4410
2022-07-11 05:16:44 - train: epoch 0077, iter [00600, 05004], lr: 0.029520, loss: 1.6445
2022-07-11 05:17:18 - train: epoch 0077, iter [00700, 05004], lr: 0.029496, loss: 1.5999
2022-07-11 05:17:53 - train: epoch 0077, iter [00800, 05004], lr: 0.029472, loss: 1.6162
2022-07-11 05:18:28 - train: epoch 0077, iter [00900, 05004], lr: 0.029448, loss: 1.9062
2022-07-11 05:19:02 - train: epoch 0077, iter [01000, 05004], lr: 0.029424, loss: 1.5050
2022-07-11 05:19:38 - train: epoch 0077, iter [01100, 05004], lr: 0.029401, loss: 1.7308
2022-07-11 05:20:13 - train: epoch 0077, iter [01200, 05004], lr: 0.029377, loss: 1.8544
2022-07-11 05:20:48 - train: epoch 0077, iter [01300, 05004], lr: 0.029353, loss: 1.4628
2022-07-11 05:21:22 - train: epoch 0077, iter [01400, 05004], lr: 0.029329, loss: 1.6473
2022-07-11 05:21:57 - train: epoch 0077, iter [01500, 05004], lr: 0.029305, loss: 1.4378
2022-07-11 05:22:31 - train: epoch 0077, iter [01600, 05004], lr: 0.029282, loss: 1.3080
2022-07-11 05:23:06 - train: epoch 0077, iter [01700, 05004], lr: 0.029258, loss: 1.9606
2022-07-11 05:23:41 - train: epoch 0077, iter [01800, 05004], lr: 0.029234, loss: 1.6060
2022-07-11 05:24:16 - train: epoch 0077, iter [01900, 05004], lr: 0.029210, loss: 1.3354
2022-07-11 05:24:51 - train: epoch 0077, iter [02000, 05004], lr: 0.029186, loss: 1.6311
2022-07-11 05:25:25 - train: epoch 0077, iter [02100, 05004], lr: 0.029163, loss: 1.6890
2022-07-11 05:26:01 - train: epoch 0077, iter [02200, 05004], lr: 0.029139, loss: 1.5000
2022-07-11 05:26:36 - train: epoch 0077, iter [02300, 05004], lr: 0.029115, loss: 1.8457
2022-07-11 05:27:12 - train: epoch 0077, iter [02400, 05004], lr: 0.029091, loss: 1.4769
2022-07-11 05:27:46 - train: epoch 0077, iter [02500, 05004], lr: 0.029067, loss: 2.1162
2022-07-11 05:28:21 - train: epoch 0077, iter [02600, 05004], lr: 0.029044, loss: 1.3967
2022-07-11 05:28:57 - train: epoch 0077, iter [02700, 05004], lr: 0.029020, loss: 1.6212
2022-07-11 05:29:32 - train: epoch 0077, iter [02800, 05004], lr: 0.028996, loss: 1.7749
2022-07-11 05:30:07 - train: epoch 0077, iter [02900, 05004], lr: 0.028973, loss: 1.6204
2022-07-11 05:30:42 - train: epoch 0077, iter [03000, 05004], lr: 0.028949, loss: 1.7143
2022-07-11 05:31:16 - train: epoch 0077, iter [03100, 05004], lr: 0.028925, loss: 1.6915
2022-07-11 05:31:52 - train: epoch 0077, iter [03200, 05004], lr: 0.028901, loss: 1.7622
2022-07-11 05:32:27 - train: epoch 0077, iter [03300, 05004], lr: 0.028878, loss: 1.5987
2022-07-11 05:33:02 - train: epoch 0077, iter [03400, 05004], lr: 0.028854, loss: 1.8923
2022-07-11 05:33:37 - train: epoch 0077, iter [03500, 05004], lr: 0.028830, loss: 1.5454
2022-07-11 05:34:12 - train: epoch 0077, iter [03600, 05004], lr: 0.028807, loss: 1.5587
2022-07-11 05:34:47 - train: epoch 0077, iter [03700, 05004], lr: 0.028783, loss: 1.7025
2022-07-11 05:35:22 - train: epoch 0077, iter [03800, 05004], lr: 0.028759, loss: 1.5593
2022-07-11 05:35:57 - train: epoch 0077, iter [03900, 05004], lr: 0.028735, loss: 1.6112
2022-07-11 05:36:32 - train: epoch 0077, iter [04000, 05004], lr: 0.028712, loss: 1.8020
2022-07-11 05:37:07 - train: epoch 0077, iter [04100, 05004], lr: 0.028688, loss: 1.5138
2022-07-11 05:37:42 - train: epoch 0077, iter [04200, 05004], lr: 0.028664, loss: 1.7598
2022-07-11 05:38:17 - train: epoch 0077, iter [04300, 05004], lr: 0.028641, loss: 1.6665
2022-07-11 05:38:52 - train: epoch 0077, iter [04400, 05004], lr: 0.028617, loss: 1.4230
2022-07-11 05:39:27 - train: epoch 0077, iter [04500, 05004], lr: 0.028594, loss: 1.8909
2022-07-11 05:40:01 - train: epoch 0077, iter [04600, 05004], lr: 0.028570, loss: 1.4480
2022-07-11 05:40:36 - train: epoch 0077, iter [04700, 05004], lr: 0.028546, loss: 1.4771
2022-07-11 05:41:11 - train: epoch 0077, iter [04800, 05004], lr: 0.028523, loss: 1.5085
2022-07-11 05:41:46 - train: epoch 0077, iter [04900, 05004], lr: 0.028499, loss: 1.8444
2022-07-11 05:42:19 - train: epoch 0077, iter [05000, 05004], lr: 0.028475, loss: 1.5756
2022-07-11 05:42:20 - train: epoch 077, train_loss: 1.6339
2022-07-11 05:43:37 - eval: epoch: 077, acc1: 65.722%, acc5: 87.140%, test_loss: 1.4073, per_image_load_time: 2.639ms, per_image_inference_time: 0.379ms
2022-07-11 05:43:38 - until epoch: 077, best_acc1: 65.722%
2022-07-11 05:43:38 - epoch 078 lr: 0.028474
2022-07-11 05:44:18 - train: epoch 0078, iter [00100, 05004], lr: 0.028451, loss: 1.4907
2022-07-11 05:44:52 - train: epoch 0078, iter [00200, 05004], lr: 0.028427, loss: 1.7886
2022-07-11 05:45:26 - train: epoch 0078, iter [00300, 05004], lr: 0.028404, loss: 1.6329
2022-07-11 05:46:02 - train: epoch 0078, iter [00400, 05004], lr: 0.028380, loss: 1.5266
2022-07-11 05:46:36 - train: epoch 0078, iter [00500, 05004], lr: 0.028356, loss: 1.5808
2022-07-11 05:47:11 - train: epoch 0078, iter [00600, 05004], lr: 0.028333, loss: 1.5078
2022-07-11 05:47:46 - train: epoch 0078, iter [00700, 05004], lr: 0.028309, loss: 1.8547
2022-07-11 05:48:20 - train: epoch 0078, iter [00800, 05004], lr: 0.028286, loss: 1.7530
2022-07-11 05:48:55 - train: epoch 0078, iter [00900, 05004], lr: 0.028262, loss: 1.7920
2022-07-11 05:49:29 - train: epoch 0078, iter [01000, 05004], lr: 0.028239, loss: 1.5286
2022-07-11 05:50:05 - train: epoch 0078, iter [01100, 05004], lr: 0.028215, loss: 1.6470
2022-07-11 05:50:39 - train: epoch 0078, iter [01200, 05004], lr: 0.028192, loss: 1.7005
2022-07-11 05:51:13 - train: epoch 0078, iter [01300, 05004], lr: 0.028168, loss: 1.3970
2022-07-11 05:51:49 - train: epoch 0078, iter [01400, 05004], lr: 0.028144, loss: 1.4326
2022-07-11 05:52:23 - train: epoch 0078, iter [01500, 05004], lr: 0.028121, loss: 1.7599
2022-07-11 05:52:58 - train: epoch 0078, iter [01600, 05004], lr: 0.028097, loss: 1.7380
2022-07-11 05:53:33 - train: epoch 0078, iter [01700, 05004], lr: 0.028074, loss: 1.5304
2022-07-11 05:54:08 - train: epoch 0078, iter [01800, 05004], lr: 0.028050, loss: 1.7520
2022-07-11 05:54:43 - train: epoch 0078, iter [01900, 05004], lr: 0.028027, loss: 1.4989
2022-07-11 05:55:18 - train: epoch 0078, iter [02000, 05004], lr: 0.028003, loss: 1.6028
2022-07-11 05:55:52 - train: epoch 0078, iter [02100, 05004], lr: 0.027980, loss: 1.7199
2022-07-11 05:56:27 - train: epoch 0078, iter [02200, 05004], lr: 0.027956, loss: 1.4474
2022-07-11 05:57:02 - train: epoch 0078, iter [02300, 05004], lr: 0.027933, loss: 1.5766
2022-07-11 05:57:37 - train: epoch 0078, iter [02400, 05004], lr: 0.027909, loss: 1.4413
2022-07-11 05:58:12 - train: epoch 0078, iter [02500, 05004], lr: 0.027886, loss: 1.6196
2022-07-11 05:58:47 - train: epoch 0078, iter [02600, 05004], lr: 0.027863, loss: 1.6964
2022-07-11 05:59:22 - train: epoch 0078, iter [02700, 05004], lr: 0.027839, loss: 1.7220
2022-07-11 05:59:57 - train: epoch 0078, iter [02800, 05004], lr: 0.027816, loss: 1.6730
2022-07-11 06:00:32 - train: epoch 0078, iter [02900, 05004], lr: 0.027792, loss: 1.4878
2022-07-11 06:01:06 - train: epoch 0078, iter [03000, 05004], lr: 0.027769, loss: 1.5703
2022-07-11 06:01:42 - train: epoch 0078, iter [03100, 05004], lr: 0.027745, loss: 1.6744
2022-07-11 06:02:17 - train: epoch 0078, iter [03200, 05004], lr: 0.027722, loss: 1.6943
2022-07-11 06:02:52 - train: epoch 0078, iter [03300, 05004], lr: 0.027699, loss: 1.8478
2022-07-11 06:03:26 - train: epoch 0078, iter [03400, 05004], lr: 0.027675, loss: 1.5519
2022-07-11 06:04:02 - train: epoch 0078, iter [03500, 05004], lr: 0.027652, loss: 1.7082
2022-07-11 06:04:37 - train: epoch 0078, iter [03600, 05004], lr: 0.027628, loss: 1.7077
2022-07-11 06:05:12 - train: epoch 0078, iter [03700, 05004], lr: 0.027605, loss: 1.4714
2022-07-11 06:05:46 - train: epoch 0078, iter [03800, 05004], lr: 0.027582, loss: 1.4636
2022-07-11 06:06:20 - train: epoch 0078, iter [03900, 05004], lr: 0.027558, loss: 1.7413
2022-07-11 06:06:56 - train: epoch 0078, iter [04000, 05004], lr: 0.027535, loss: 1.4525
2022-07-11 06:07:32 - train: epoch 0078, iter [04100, 05004], lr: 0.027511, loss: 1.6529
2022-07-11 06:08:07 - train: epoch 0078, iter [04200, 05004], lr: 0.027488, loss: 1.7601
2022-07-11 06:08:41 - train: epoch 0078, iter [04300, 05004], lr: 0.027465, loss: 1.6505
2022-07-11 06:09:17 - train: epoch 0078, iter [04400, 05004], lr: 0.027441, loss: 1.6823
2022-07-11 06:09:52 - train: epoch 0078, iter [04500, 05004], lr: 0.027418, loss: 1.6083
2022-07-11 06:10:26 - train: epoch 0078, iter [04600, 05004], lr: 0.027395, loss: 1.4269
2022-07-11 06:11:02 - train: epoch 0078, iter [04700, 05004], lr: 0.027371, loss: 1.6161
2022-07-11 06:11:37 - train: epoch 0078, iter [04800, 05004], lr: 0.027348, loss: 1.6564
2022-07-11 06:12:12 - train: epoch 0078, iter [04900, 05004], lr: 0.027325, loss: 1.6428
2022-07-11 06:12:45 - train: epoch 0078, iter [05000, 05004], lr: 0.027301, loss: 1.6371
2022-07-11 06:12:46 - train: epoch 078, train_loss: 1.6207
2022-07-11 06:14:03 - eval: epoch: 078, acc1: 65.818%, acc5: 87.118%, test_loss: 1.3999, per_image_load_time: 2.573ms, per_image_inference_time: 0.409ms
2022-07-11 06:14:03 - until epoch: 078, best_acc1: 65.818%
2022-07-11 06:14:03 - epoch 079 lr: 0.027300
2022-07-11 06:14:43 - train: epoch 0079, iter [00100, 05004], lr: 0.027277, loss: 1.5616
2022-07-11 06:15:18 - train: epoch 0079, iter [00200, 05004], lr: 0.027254, loss: 1.8815
2022-07-11 06:15:53 - train: epoch 0079, iter [00300, 05004], lr: 0.027231, loss: 1.5836
2022-07-11 06:16:28 - train: epoch 0079, iter [00400, 05004], lr: 0.027207, loss: 1.7135
2022-07-11 06:17:01 - train: epoch 0079, iter [00500, 05004], lr: 0.027184, loss: 1.4570
2022-07-11 06:17:37 - train: epoch 0079, iter [00600, 05004], lr: 0.027161, loss: 1.5776
2022-07-11 06:18:12 - train: epoch 0079, iter [00700, 05004], lr: 0.027137, loss: 1.7562
2022-07-11 06:18:47 - train: epoch 0079, iter [00800, 05004], lr: 0.027114, loss: 1.6042
2022-07-11 06:19:21 - train: epoch 0079, iter [00900, 05004], lr: 0.027091, loss: 1.5203
2022-07-11 06:19:57 - train: epoch 0079, iter [01000, 05004], lr: 0.027068, loss: 1.7625
2022-07-11 06:20:30 - train: epoch 0079, iter [01100, 05004], lr: 0.027044, loss: 1.8524
2022-07-11 06:21:06 - train: epoch 0079, iter [01200, 05004], lr: 0.027021, loss: 1.8127
2022-07-11 06:21:41 - train: epoch 0079, iter [01300, 05004], lr: 0.026998, loss: 1.5176
2022-07-11 06:22:15 - train: epoch 0079, iter [01400, 05004], lr: 0.026975, loss: 1.6832
2022-07-11 06:22:51 - train: epoch 0079, iter [01500, 05004], lr: 0.026952, loss: 1.4487
2022-07-11 06:23:25 - train: epoch 0079, iter [01600, 05004], lr: 0.026928, loss: 1.4290
2022-07-11 06:24:00 - train: epoch 0079, iter [01700, 05004], lr: 0.026905, loss: 1.5640
2022-07-11 06:24:36 - train: epoch 0079, iter [01800, 05004], lr: 0.026882, loss: 1.7022
2022-07-11 06:25:10 - train: epoch 0079, iter [01900, 05004], lr: 0.026859, loss: 1.5125
2022-07-11 06:25:45 - train: epoch 0079, iter [02000, 05004], lr: 0.026836, loss: 1.8886
2022-07-11 06:26:20 - train: epoch 0079, iter [02100, 05004], lr: 0.026812, loss: 1.7145
2022-07-11 06:26:55 - train: epoch 0079, iter [02200, 05004], lr: 0.026789, loss: 1.7069
2022-07-11 06:27:31 - train: epoch 0079, iter [02300, 05004], lr: 0.026766, loss: 1.5305
2022-07-11 06:28:05 - train: epoch 0079, iter [02400, 05004], lr: 0.026743, loss: 1.5102
2022-07-11 06:28:41 - train: epoch 0079, iter [02500, 05004], lr: 0.026720, loss: 1.6410
2022-07-11 06:29:16 - train: epoch 0079, iter [02600, 05004], lr: 0.026697, loss: 1.2979
2022-07-11 06:29:51 - train: epoch 0079, iter [02700, 05004], lr: 0.026673, loss: 1.3926
2022-07-11 06:30:25 - train: epoch 0079, iter [02800, 05004], lr: 0.026650, loss: 1.5820
2022-07-11 06:31:00 - train: epoch 0079, iter [02900, 05004], lr: 0.026627, loss: 1.6096
2022-07-11 06:31:35 - train: epoch 0079, iter [03000, 05004], lr: 0.026604, loss: 1.7214
2022-07-11 06:32:10 - train: epoch 0079, iter [03100, 05004], lr: 0.026581, loss: 1.5338
2022-07-11 06:32:45 - train: epoch 0079, iter [03200, 05004], lr: 0.026558, loss: 1.7469
2022-07-11 06:33:19 - train: epoch 0079, iter [03300, 05004], lr: 0.026535, loss: 1.5663
2022-07-11 06:33:56 - train: epoch 0079, iter [03400, 05004], lr: 0.026512, loss: 1.4698
2022-07-11 06:34:30 - train: epoch 0079, iter [03500, 05004], lr: 0.026489, loss: 1.5869
2022-07-11 06:35:05 - train: epoch 0079, iter [03600, 05004], lr: 0.026465, loss: 1.4134
2022-07-11 06:35:40 - train: epoch 0079, iter [03700, 05004], lr: 0.026442, loss: 1.5344
2022-07-11 06:36:15 - train: epoch 0079, iter [03800, 05004], lr: 0.026419, loss: 1.6033
2022-07-11 06:36:49 - train: epoch 0079, iter [03900, 05004], lr: 0.026396, loss: 1.4839
2022-07-11 06:37:25 - train: epoch 0079, iter [04000, 05004], lr: 0.026373, loss: 1.4643
2022-07-11 06:38:00 - train: epoch 0079, iter [04100, 05004], lr: 0.026350, loss: 1.9137
2022-07-11 06:38:35 - train: epoch 0079, iter [04200, 05004], lr: 0.026327, loss: 1.5698
2022-07-11 06:39:10 - train: epoch 0079, iter [04300, 05004], lr: 0.026304, loss: 1.2279
2022-07-11 06:39:45 - train: epoch 0079, iter [04400, 05004], lr: 0.026281, loss: 1.8771
2022-07-11 06:40:20 - train: epoch 0079, iter [04500, 05004], lr: 0.026258, loss: 1.7985
2022-07-11 06:40:55 - train: epoch 0079, iter [04600, 05004], lr: 0.026235, loss: 1.5826
2022-07-11 06:41:29 - train: epoch 0079, iter [04700, 05004], lr: 0.026212, loss: 1.6561
2022-07-11 06:42:05 - train: epoch 0079, iter [04800, 05004], lr: 0.026189, loss: 1.8490
2022-07-11 06:42:39 - train: epoch 0079, iter [04900, 05004], lr: 0.026166, loss: 1.6804
2022-07-11 06:43:13 - train: epoch 0079, iter [05000, 05004], lr: 0.026143, loss: 1.2637
2022-07-11 06:43:14 - train: epoch 079, train_loss: 1.6059
2022-07-11 06:44:30 - eval: epoch: 079, acc1: 66.184%, acc5: 87.458%, test_loss: 1.3844, per_image_load_time: 1.830ms, per_image_inference_time: 0.396ms
2022-07-11 06:44:31 - until epoch: 079, best_acc1: 66.184%
2022-07-11 06:44:31 - epoch 080 lr: 0.026142
2022-07-11 06:45:10 - train: epoch 0080, iter [00100, 05004], lr: 0.026119, loss: 1.4701
2022-07-11 06:45:45 - train: epoch 0080, iter [00200, 05004], lr: 0.026096, loss: 1.6386
2022-07-11 06:46:20 - train: epoch 0080, iter [00300, 05004], lr: 0.026073, loss: 1.6641
2022-07-11 06:46:54 - train: epoch 0080, iter [00400, 05004], lr: 0.026050, loss: 1.3384
2022-07-11 06:47:28 - train: epoch 0080, iter [00500, 05004], lr: 0.026027, loss: 1.2933
2022-07-11 06:48:03 - train: epoch 0080, iter [00600, 05004], lr: 0.026004, loss: 1.3868
2022-07-11 06:48:38 - train: epoch 0080, iter [00700, 05004], lr: 0.025981, loss: 1.6457
2022-07-11 06:49:12 - train: epoch 0080, iter [00800, 05004], lr: 0.025958, loss: 1.5320
2022-07-11 06:49:48 - train: epoch 0080, iter [00900, 05004], lr: 0.025935, loss: 1.4898
2022-07-11 06:50:21 - train: epoch 0080, iter [01000, 05004], lr: 0.025912, loss: 1.6672
2022-07-11 06:50:57 - train: epoch 0080, iter [01100, 05004], lr: 0.025890, loss: 1.6034
2022-07-11 06:51:32 - train: epoch 0080, iter [01200, 05004], lr: 0.025867, loss: 1.3960
2022-07-11 06:52:07 - train: epoch 0080, iter [01300, 05004], lr: 0.025844, loss: 1.5086
2022-07-11 06:52:41 - train: epoch 0080, iter [01400, 05004], lr: 0.025821, loss: 1.6785
2022-07-11 06:53:16 - train: epoch 0080, iter [01500, 05004], lr: 0.025798, loss: 1.5748
2022-07-11 06:53:52 - train: epoch 0080, iter [01600, 05004], lr: 0.025775, loss: 1.6822
2022-07-11 06:54:27 - train: epoch 0080, iter [01700, 05004], lr: 0.025752, loss: 1.7317
2022-07-11 06:55:01 - train: epoch 0080, iter [01800, 05004], lr: 0.025729, loss: 1.6706
2022-07-11 06:55:36 - train: epoch 0080, iter [01900, 05004], lr: 0.025706, loss: 1.4954
2022-07-11 06:56:12 - train: epoch 0080, iter [02000, 05004], lr: 0.025684, loss: 1.6470
2022-07-11 06:56:47 - train: epoch 0080, iter [02100, 05004], lr: 0.025661, loss: 1.6818
2022-07-11 06:57:22 - train: epoch 0080, iter [02200, 05004], lr: 0.025638, loss: 1.6020
2022-07-11 06:57:56 - train: epoch 0080, iter [02300, 05004], lr: 0.025615, loss: 1.6559
2022-07-11 06:58:32 - train: epoch 0080, iter [02400, 05004], lr: 0.025592, loss: 1.7631
2022-07-11 06:59:07 - train: epoch 0080, iter [02500, 05004], lr: 0.025569, loss: 1.5135
2022-07-11 06:59:42 - train: epoch 0080, iter [02600, 05004], lr: 0.025547, loss: 1.6739
2022-07-11 07:00:17 - train: epoch 0080, iter [02700, 05004], lr: 0.025524, loss: 1.4596
2022-07-11 07:00:52 - train: epoch 0080, iter [02800, 05004], lr: 0.025501, loss: 1.6529
2022-07-11 07:01:27 - train: epoch 0080, iter [02900, 05004], lr: 0.025478, loss: 1.6904
2022-07-11 07:02:01 - train: epoch 0080, iter [03000, 05004], lr: 0.025455, loss: 1.6487
2022-07-11 07:02:38 - train: epoch 0080, iter [03100, 05004], lr: 0.025433, loss: 1.5168
2022-07-11 07:03:12 - train: epoch 0080, iter [03200, 05004], lr: 0.025410, loss: 1.3904
2022-07-11 07:03:47 - train: epoch 0080, iter [03300, 05004], lr: 0.025387, loss: 1.6054
2022-07-11 07:04:22 - train: epoch 0080, iter [03400, 05004], lr: 0.025364, loss: 1.3711
2022-07-11 07:04:58 - train: epoch 0080, iter [03500, 05004], lr: 0.025341, loss: 1.5667
2022-07-11 07:05:33 - train: epoch 0080, iter [03600, 05004], lr: 0.025319, loss: 1.5094
2022-07-11 07:06:08 - train: epoch 0080, iter [03700, 05004], lr: 0.025296, loss: 1.3631
2022-07-11 07:06:42 - train: epoch 0080, iter [03800, 05004], lr: 0.025273, loss: 1.6616
2022-07-11 07:07:18 - train: epoch 0080, iter [03900, 05004], lr: 0.025251, loss: 1.4999
2022-07-11 07:07:54 - train: epoch 0080, iter [04000, 05004], lr: 0.025228, loss: 1.7132
2022-07-11 07:08:28 - train: epoch 0080, iter [04100, 05004], lr: 0.025205, loss: 1.6956
2022-07-11 07:09:03 - train: epoch 0080, iter [04200, 05004], lr: 0.025182, loss: 1.6628
2022-07-11 07:09:38 - train: epoch 0080, iter [04300, 05004], lr: 0.025160, loss: 1.8187
2022-07-11 07:10:14 - train: epoch 0080, iter [04400, 05004], lr: 0.025137, loss: 1.5660
2022-07-11 07:10:49 - train: epoch 0080, iter [04500, 05004], lr: 0.025114, loss: 1.6262
2022-07-11 07:11:24 - train: epoch 0080, iter [04600, 05004], lr: 0.025092, loss: 1.6534
2022-07-11 07:11:59 - train: epoch 0080, iter [04700, 05004], lr: 0.025069, loss: 1.5913
2022-07-11 07:12:34 - train: epoch 0080, iter [04800, 05004], lr: 0.025046, loss: 1.8050
2022-07-11 07:13:09 - train: epoch 0080, iter [04900, 05004], lr: 0.025024, loss: 1.8100
2022-07-11 07:13:43 - train: epoch 0080, iter [05000, 05004], lr: 0.025001, loss: 1.5777
2022-07-11 07:13:44 - train: epoch 080, train_loss: 1.5920
2022-07-11 07:15:01 - eval: epoch: 080, acc1: 66.316%, acc5: 87.386%, test_loss: 1.3906, per_image_load_time: 2.542ms, per_image_inference_time: 0.431ms
2022-07-11 07:15:01 - until epoch: 080, best_acc1: 66.316%
2022-07-11 07:15:01 - epoch 081 lr: 0.025000
2022-07-11 07:15:41 - train: epoch 0081, iter [00100, 05004], lr: 0.024977, loss: 1.5104
2022-07-11 07:16:16 - train: epoch 0081, iter [00200, 05004], lr: 0.024955, loss: 1.6427
2022-07-11 07:16:50 - train: epoch 0081, iter [00300, 05004], lr: 0.024932, loss: 1.4918
2022-07-11 07:17:24 - train: epoch 0081, iter [00400, 05004], lr: 0.024909, loss: 1.5384
2022-07-11 07:17:59 - train: epoch 0081, iter [00500, 05004], lr: 0.024887, loss: 1.7197
2022-07-11 07:18:33 - train: epoch 0081, iter [00600, 05004], lr: 0.024864, loss: 1.5396
2022-07-11 07:19:09 - train: epoch 0081, iter [00700, 05004], lr: 0.024842, loss: 1.5915
2022-07-11 07:19:42 - train: epoch 0081, iter [00800, 05004], lr: 0.024819, loss: 1.5666
2022-07-11 07:20:17 - train: epoch 0081, iter [00900, 05004], lr: 0.024796, loss: 1.5018
2022-07-11 07:20:52 - train: epoch 0081, iter [01000, 05004], lr: 0.024774, loss: 1.5265
2022-07-11 07:21:27 - train: epoch 0081, iter [01100, 05004], lr: 0.024751, loss: 1.4702
2022-07-11 07:22:02 - train: epoch 0081, iter [01200, 05004], lr: 0.024729, loss: 1.7010
2022-07-11 07:22:35 - train: epoch 0081, iter [01300, 05004], lr: 0.024706, loss: 1.5077
2022-07-11 07:23:10 - train: epoch 0081, iter [01400, 05004], lr: 0.024684, loss: 1.2576
2022-07-11 07:23:45 - train: epoch 0081, iter [01500, 05004], lr: 0.024661, loss: 1.7711
2022-07-11 07:24:19 - train: epoch 0081, iter [01600, 05004], lr: 0.024638, loss: 1.4964
2022-07-11 07:24:55 - train: epoch 0081, iter [01700, 05004], lr: 0.024616, loss: 1.4671
2022-07-11 07:25:30 - train: epoch 0081, iter [01800, 05004], lr: 0.024593, loss: 1.6377
2022-07-11 07:26:04 - train: epoch 0081, iter [01900, 05004], lr: 0.024571, loss: 1.4197
2022-07-11 07:26:38 - train: epoch 0081, iter [02000, 05004], lr: 0.024548, loss: 1.6351
2022-07-11 07:27:14 - train: epoch 0081, iter [02100, 05004], lr: 0.024526, loss: 1.5380
2022-07-11 07:27:49 - train: epoch 0081, iter [02200, 05004], lr: 0.024503, loss: 1.4265
2022-07-11 07:28:24 - train: epoch 0081, iter [02300, 05004], lr: 0.024481, loss: 1.4998
2022-07-11 07:28:59 - train: epoch 0081, iter [02400, 05004], lr: 0.024458, loss: 1.7044
2022-07-11 07:29:33 - train: epoch 0081, iter [02500, 05004], lr: 0.024436, loss: 1.6760
2022-07-11 07:30:09 - train: epoch 0081, iter [02600, 05004], lr: 0.024413, loss: 1.7266
2022-07-11 07:30:43 - train: epoch 0081, iter [02700, 05004], lr: 0.024391, loss: 1.6517
2022-07-11 07:31:18 - train: epoch 0081, iter [02800, 05004], lr: 0.024368, loss: 1.4743
2022-07-11 07:31:53 - train: epoch 0081, iter [02900, 05004], lr: 0.024346, loss: 1.5311
2022-07-11 07:32:28 - train: epoch 0081, iter [03000, 05004], lr: 0.024323, loss: 1.5721
2022-07-11 07:33:03 - train: epoch 0081, iter [03100, 05004], lr: 0.024301, loss: 1.6151
2022-07-11 07:33:37 - train: epoch 0081, iter [03200, 05004], lr: 0.024279, loss: 1.4387
2022-07-11 07:34:12 - train: epoch 0081, iter [03300, 05004], lr: 0.024256, loss: 1.4866
2022-07-11 07:34:47 - train: epoch 0081, iter [03400, 05004], lr: 0.024234, loss: 1.6158
2022-07-11 07:35:22 - train: epoch 0081, iter [03500, 05004], lr: 0.024211, loss: 1.6236
2022-07-11 07:35:57 - train: epoch 0081, iter [03600, 05004], lr: 0.024189, loss: 1.3178
2022-07-11 07:36:31 - train: epoch 0081, iter [03700, 05004], lr: 0.024167, loss: 1.8512
2022-07-11 07:37:06 - train: epoch 0081, iter [03800, 05004], lr: 0.024144, loss: 1.6996
2022-07-11 07:37:41 - train: epoch 0081, iter [03900, 05004], lr: 0.024122, loss: 1.7036
2022-07-11 07:38:17 - train: epoch 0081, iter [04000, 05004], lr: 0.024099, loss: 1.6431
2022-07-11 07:38:52 - train: epoch 0081, iter [04100, 05004], lr: 0.024077, loss: 1.5599
2022-07-11 07:39:27 - train: epoch 0081, iter [04200, 05004], lr: 0.024055, loss: 1.6629
2022-07-11 07:40:03 - train: epoch 0081, iter [04300, 05004], lr: 0.024032, loss: 1.6661
2022-07-11 07:40:36 - train: epoch 0081, iter [04400, 05004], lr: 0.024010, loss: 1.7790
2022-07-11 07:41:11 - train: epoch 0081, iter [04500, 05004], lr: 0.023988, loss: 1.6302
2022-07-11 07:41:46 - train: epoch 0081, iter [04600, 05004], lr: 0.023965, loss: 1.4722
2022-07-11 07:42:21 - train: epoch 0081, iter [04700, 05004], lr: 0.023943, loss: 1.6258
2022-07-11 07:42:55 - train: epoch 0081, iter [04800, 05004], lr: 0.023921, loss: 1.5414
2022-07-11 07:43:30 - train: epoch 0081, iter [04900, 05004], lr: 0.023898, loss: 1.5226
2022-07-11 07:44:03 - train: epoch 0081, iter [05000, 05004], lr: 0.023876, loss: 1.4454
2022-07-11 07:44:04 - train: epoch 081, train_loss: 1.5783
2022-07-11 07:45:21 - eval: epoch: 081, acc1: 67.132%, acc5: 87.976%, test_loss: 1.3528, per_image_load_time: 2.573ms, per_image_inference_time: 0.408ms
2022-07-11 07:45:21 - until epoch: 081, best_acc1: 67.132%
2022-07-11 07:45:21 - epoch 082 lr: 0.023875
2022-07-11 07:46:01 - train: epoch 0082, iter [00100, 05004], lr: 0.023853, loss: 1.3932
2022-07-11 07:46:36 - train: epoch 0082, iter [00200, 05004], lr: 0.023830, loss: 1.4414
2022-07-11 07:47:10 - train: epoch 0082, iter [00300, 05004], lr: 0.023808, loss: 1.6080
2022-07-11 07:47:45 - train: epoch 0082, iter [00400, 05004], lr: 0.023786, loss: 1.5263
2022-07-11 07:48:19 - train: epoch 0082, iter [00500, 05004], lr: 0.023764, loss: 1.6052
2022-07-11 07:48:54 - train: epoch 0082, iter [00600, 05004], lr: 0.023741, loss: 1.4354
2022-07-11 07:49:27 - train: epoch 0082, iter [00700, 05004], lr: 0.023719, loss: 1.7795
2022-07-11 07:50:03 - train: epoch 0082, iter [00800, 05004], lr: 0.023697, loss: 1.6313
2022-07-11 07:50:37 - train: epoch 0082, iter [00900, 05004], lr: 0.023675, loss: 1.7432
2022-07-11 07:51:12 - train: epoch 0082, iter [01000, 05004], lr: 0.023652, loss: 1.4755
2022-07-11 07:51:46 - train: epoch 0082, iter [01100, 05004], lr: 0.023630, loss: 1.7501
2022-07-11 07:52:21 - train: epoch 0082, iter [01200, 05004], lr: 0.023608, loss: 1.5278
2022-07-11 07:52:56 - train: epoch 0082, iter [01300, 05004], lr: 0.023586, loss: 1.7850
2022-07-11 07:53:31 - train: epoch 0082, iter [01400, 05004], lr: 0.023564, loss: 1.6258
2022-07-11 07:54:06 - train: epoch 0082, iter [01500, 05004], lr: 0.023541, loss: 1.3953
2022-07-11 07:54:41 - train: epoch 0082, iter [01600, 05004], lr: 0.023519, loss: 1.5728
2022-07-11 07:55:16 - train: epoch 0082, iter [01700, 05004], lr: 0.023497, loss: 1.5252
2022-07-11 07:55:51 - train: epoch 0082, iter [01800, 05004], lr: 0.023475, loss: 1.4452
2022-07-11 07:56:26 - train: epoch 0082, iter [01900, 05004], lr: 0.023453, loss: 1.5379
2022-07-11 07:57:01 - train: epoch 0082, iter [02000, 05004], lr: 0.023430, loss: 1.4508
2022-07-11 07:57:36 - train: epoch 0082, iter [02100, 05004], lr: 0.023408, loss: 1.6544
2022-07-11 07:58:10 - train: epoch 0082, iter [02200, 05004], lr: 0.023386, loss: 1.5517
2022-07-11 07:58:45 - train: epoch 0082, iter [02300, 05004], lr: 0.023364, loss: 1.7071
2022-07-11 07:59:21 - train: epoch 0082, iter [02400, 05004], lr: 0.023342, loss: 1.6312
2022-07-11 07:59:55 - train: epoch 0082, iter [02500, 05004], lr: 0.023320, loss: 1.5529
2022-07-11 08:00:30 - train: epoch 0082, iter [02600, 05004], lr: 0.023298, loss: 1.5187
2022-07-11 08:01:06 - train: epoch 0082, iter [02700, 05004], lr: 0.023275, loss: 1.3572
2022-07-11 08:01:40 - train: epoch 0082, iter [02800, 05004], lr: 0.023253, loss: 1.4211
2022-07-11 08:02:15 - train: epoch 0082, iter [02900, 05004], lr: 0.023231, loss: 1.4943
2022-07-11 08:02:50 - train: epoch 0082, iter [03000, 05004], lr: 0.023209, loss: 1.3907
2022-07-11 08:03:25 - train: epoch 0082, iter [03100, 05004], lr: 0.023187, loss: 1.6608
2022-07-11 08:04:00 - train: epoch 0082, iter [03200, 05004], lr: 0.023165, loss: 1.7817
2022-07-11 08:04:36 - train: epoch 0082, iter [03300, 05004], lr: 0.023143, loss: 1.4249
2022-07-11 08:05:12 - train: epoch 0082, iter [03400, 05004], lr: 0.023121, loss: 1.5446
2022-07-11 08:05:46 - train: epoch 0082, iter [03500, 05004], lr: 0.023099, loss: 1.5158
2022-07-11 08:06:22 - train: epoch 0082, iter [03600, 05004], lr: 0.023077, loss: 1.5857
2022-07-11 08:06:56 - train: epoch 0082, iter [03700, 05004], lr: 0.023055, loss: 1.4868
2022-07-11 08:07:32 - train: epoch 0082, iter [03800, 05004], lr: 0.023033, loss: 1.5622
2022-07-11 08:08:07 - train: epoch 0082, iter [03900, 05004], lr: 0.023011, loss: 1.7383
2022-07-11 08:08:43 - train: epoch 0082, iter [04000, 05004], lr: 0.022989, loss: 1.7063
2022-07-11 08:09:17 - train: epoch 0082, iter [04100, 05004], lr: 0.022967, loss: 1.7599
2022-07-11 08:09:52 - train: epoch 0082, iter [04200, 05004], lr: 0.022945, loss: 1.7759
2022-07-11 08:10:27 - train: epoch 0082, iter [04300, 05004], lr: 0.022923, loss: 1.4854
2022-07-11 08:11:03 - train: epoch 0082, iter [04400, 05004], lr: 0.022901, loss: 1.4392
2022-07-11 08:11:38 - train: epoch 0082, iter [04500, 05004], lr: 0.022879, loss: 1.6780
2022-07-11 08:12:13 - train: epoch 0082, iter [04600, 05004], lr: 0.022857, loss: 1.3827
2022-07-11 08:12:48 - train: epoch 0082, iter [04700, 05004], lr: 0.022835, loss: 1.5687
2022-07-11 08:13:23 - train: epoch 0082, iter [04800, 05004], lr: 0.022813, loss: 1.5603
2022-07-11 08:13:57 - train: epoch 0082, iter [04900, 05004], lr: 0.022791, loss: 1.6688
2022-07-11 08:14:31 - train: epoch 0082, iter [05000, 05004], lr: 0.022769, loss: 1.4919
2022-07-11 08:14:32 - train: epoch 082, train_loss: 1.5622
2022-07-11 08:15:49 - eval: epoch: 082, acc1: 66.600%, acc5: 87.640%, test_loss: 1.3657, per_image_load_time: 2.545ms, per_image_inference_time: 0.410ms
2022-07-11 08:15:50 - until epoch: 082, best_acc1: 67.132%
2022-07-11 08:15:50 - epoch 083 lr: 0.022768
2022-07-11 08:16:29 - train: epoch 0083, iter [00100, 05004], lr: 0.022746, loss: 1.4216
2022-07-11 08:17:05 - train: epoch 0083, iter [00200, 05004], lr: 0.022724, loss: 1.5231
2022-07-11 08:17:39 - train: epoch 0083, iter [00300, 05004], lr: 0.022702, loss: 1.5902
2022-07-11 08:18:13 - train: epoch 0083, iter [00400, 05004], lr: 0.022680, loss: 1.6227
2022-07-11 08:18:48 - train: epoch 0083, iter [00500, 05004], lr: 0.022658, loss: 1.5903
2022-07-11 08:19:23 - train: epoch 0083, iter [00600, 05004], lr: 0.022637, loss: 1.4112
2022-07-11 08:19:57 - train: epoch 0083, iter [00700, 05004], lr: 0.022615, loss: 1.5750
2022-07-11 08:20:33 - train: epoch 0083, iter [00800, 05004], lr: 0.022593, loss: 1.6501
2022-07-11 08:21:07 - train: epoch 0083, iter [00900, 05004], lr: 0.022571, loss: 1.6342
2022-07-11 08:21:42 - train: epoch 0083, iter [01000, 05004], lr: 0.022549, loss: 1.6636
2022-07-11 08:22:17 - train: epoch 0083, iter [01100, 05004], lr: 0.022527, loss: 1.6283
2022-07-11 08:22:51 - train: epoch 0083, iter [01200, 05004], lr: 0.022505, loss: 1.4570
2022-07-11 08:23:26 - train: epoch 0083, iter [01300, 05004], lr: 0.022483, loss: 1.5698
2022-07-11 08:24:00 - train: epoch 0083, iter [01400, 05004], lr: 0.022462, loss: 1.6721
2022-07-11 08:24:35 - train: epoch 0083, iter [01500, 05004], lr: 0.022440, loss: 1.5123
2022-07-11 08:25:10 - train: epoch 0083, iter [01600, 05004], lr: 0.022418, loss: 1.5864
2022-07-11 08:25:44 - train: epoch 0083, iter [01700, 05004], lr: 0.022396, loss: 1.4729
2022-07-11 08:26:19 - train: epoch 0083, iter [01800, 05004], lr: 0.022374, loss: 1.5940
2022-07-11 08:26:53 - train: epoch 0083, iter [01900, 05004], lr: 0.022353, loss: 1.5998
2022-07-11 08:27:28 - train: epoch 0083, iter [02000, 05004], lr: 0.022331, loss: 1.3674
2022-07-11 08:28:03 - train: epoch 0083, iter [02100, 05004], lr: 0.022309, loss: 1.4089
2022-07-11 08:28:38 - train: epoch 0083, iter [02200, 05004], lr: 0.022287, loss: 1.3367
2022-07-11 08:29:12 - train: epoch 0083, iter [02300, 05004], lr: 0.022265, loss: 1.4869
2022-07-11 08:29:48 - train: epoch 0083, iter [02400, 05004], lr: 0.022244, loss: 1.4899
2022-07-11 08:30:23 - train: epoch 0083, iter [02500, 05004], lr: 0.022222, loss: 1.4195
2022-07-11 08:30:57 - train: epoch 0083, iter [02600, 05004], lr: 0.022200, loss: 1.4156
2022-07-11 08:31:32 - train: epoch 0083, iter [02700, 05004], lr: 0.022178, loss: 1.5356
2022-07-11 08:32:07 - train: epoch 0083, iter [02800, 05004], lr: 0.022157, loss: 1.5392
2022-07-11 08:32:42 - train: epoch 0083, iter [02900, 05004], lr: 0.022135, loss: 1.5144
2022-07-11 08:33:17 - train: epoch 0083, iter [03000, 05004], lr: 0.022113, loss: 1.6292
2022-07-11 08:33:51 - train: epoch 0083, iter [03100, 05004], lr: 0.022092, loss: 1.5508
2022-07-11 08:34:26 - train: epoch 0083, iter [03200, 05004], lr: 0.022070, loss: 1.5143
2022-07-11 08:35:01 - train: epoch 0083, iter [03300, 05004], lr: 0.022048, loss: 1.4878
2022-07-11 08:35:37 - train: epoch 0083, iter [03400, 05004], lr: 0.022026, loss: 1.4871
2022-07-11 08:36:11 - train: epoch 0083, iter [03500, 05004], lr: 0.022005, loss: 1.5354
2022-07-11 08:36:46 - train: epoch 0083, iter [03600, 05004], lr: 0.021983, loss: 1.7074
2022-07-11 08:37:21 - train: epoch 0083, iter [03700, 05004], lr: 0.021961, loss: 1.3815
2022-07-11 08:37:55 - train: epoch 0083, iter [03800, 05004], lr: 0.021940, loss: 1.4639
2022-07-11 08:38:31 - train: epoch 0083, iter [03900, 05004], lr: 0.021918, loss: 1.6997
2022-07-11 08:39:06 - train: epoch 0083, iter [04000, 05004], lr: 0.021897, loss: 1.6702
2022-07-11 08:39:41 - train: epoch 0083, iter [04100, 05004], lr: 0.021875, loss: 1.4513
2022-07-11 08:40:15 - train: epoch 0083, iter [04200, 05004], lr: 0.021853, loss: 1.7264
2022-07-11 08:40:51 - train: epoch 0083, iter [04300, 05004], lr: 0.021832, loss: 1.6131
2022-07-11 08:41:26 - train: epoch 0083, iter [04400, 05004], lr: 0.021810, loss: 1.4272
2022-07-11 08:42:00 - train: epoch 0083, iter [04500, 05004], lr: 0.021788, loss: 1.6783
2022-07-11 08:42:35 - train: epoch 0083, iter [04600, 05004], lr: 0.021767, loss: 1.5400
2022-07-11 08:43:10 - train: epoch 0083, iter [04700, 05004], lr: 0.021745, loss: 1.7622
2022-07-11 08:43:45 - train: epoch 0083, iter [04800, 05004], lr: 0.021724, loss: 1.6574
2022-07-11 08:44:20 - train: epoch 0083, iter [04900, 05004], lr: 0.021702, loss: 1.3014
2022-07-11 08:44:53 - train: epoch 0083, iter [05000, 05004], lr: 0.021681, loss: 1.6065
2022-07-11 08:44:54 - train: epoch 083, train_loss: 1.5486
2022-07-11 08:46:11 - eval: epoch: 083, acc1: 66.216%, acc5: 87.228%, test_loss: 1.3901, per_image_load_time: 2.543ms, per_image_inference_time: 0.403ms
2022-07-11 08:46:11 - until epoch: 083, best_acc1: 67.132%
2022-07-11 08:46:11 - epoch 084 lr: 0.021679
2022-07-11 08:46:51 - train: epoch 0084, iter [00100, 05004], lr: 0.021658, loss: 1.3502
2022-07-11 08:47:26 - train: epoch 0084, iter [00200, 05004], lr: 0.021637, loss: 1.5197
2022-07-11 08:48:00 - train: epoch 0084, iter [00300, 05004], lr: 0.021615, loss: 1.2501
2022-07-11 08:48:34 - train: epoch 0084, iter [00400, 05004], lr: 0.021594, loss: 1.3780
2022-07-11 08:49:09 - train: epoch 0084, iter [00500, 05004], lr: 0.021572, loss: 1.5092
2022-07-11 08:49:44 - train: epoch 0084, iter [00600, 05004], lr: 0.021550, loss: 1.7408
2022-07-11 08:50:19 - train: epoch 0084, iter [00700, 05004], lr: 0.021529, loss: 1.5477
2022-07-11 08:50:54 - train: epoch 0084, iter [00800, 05004], lr: 0.021507, loss: 1.7182
2022-07-11 08:51:29 - train: epoch 0084, iter [00900, 05004], lr: 0.021486, loss: 1.5490
2022-07-11 08:52:03 - train: epoch 0084, iter [01000, 05004], lr: 0.021464, loss: 1.5853
2022-07-11 08:52:38 - train: epoch 0084, iter [01100, 05004], lr: 0.021443, loss: 1.4334
2022-07-11 08:53:13 - train: epoch 0084, iter [01200, 05004], lr: 0.021422, loss: 1.9071
2022-07-11 08:53:47 - train: epoch 0084, iter [01300, 05004], lr: 0.021400, loss: 1.5880
2022-07-11 08:54:23 - train: epoch 0084, iter [01400, 05004], lr: 0.021379, loss: 1.5678
2022-07-11 08:54:57 - train: epoch 0084, iter [01500, 05004], lr: 0.021357, loss: 1.6198
2022-07-11 08:55:32 - train: epoch 0084, iter [01600, 05004], lr: 0.021336, loss: 1.4549
2022-07-11 08:56:06 - train: epoch 0084, iter [01700, 05004], lr: 0.021314, loss: 1.5875
2022-07-11 08:56:41 - train: epoch 0084, iter [01800, 05004], lr: 0.021293, loss: 1.6750
2022-07-11 08:57:16 - train: epoch 0084, iter [01900, 05004], lr: 0.021271, loss: 1.6700
2022-07-11 08:57:51 - train: epoch 0084, iter [02000, 05004], lr: 0.021250, loss: 1.3314
2022-07-11 08:58:26 - train: epoch 0084, iter [02100, 05004], lr: 0.021229, loss: 1.5020
2022-07-11 08:59:01 - train: epoch 0084, iter [02200, 05004], lr: 0.021207, loss: 1.5397
2022-07-11 08:59:36 - train: epoch 0084, iter [02300, 05004], lr: 0.021186, loss: 1.5218
2022-07-11 09:00:11 - train: epoch 0084, iter [02400, 05004], lr: 0.021165, loss: 1.3668
2022-07-11 09:00:45 - train: epoch 0084, iter [02500, 05004], lr: 0.021143, loss: 1.8496
2022-07-11 09:01:20 - train: epoch 0084, iter [02600, 05004], lr: 0.021122, loss: 1.4721
2022-07-11 09:01:55 - train: epoch 0084, iter [02700, 05004], lr: 0.021100, loss: 1.4748
2022-07-11 09:02:30 - train: epoch 0084, iter [02800, 05004], lr: 0.021079, loss: 1.4991
2022-07-11 09:03:05 - train: epoch 0084, iter [02900, 05004], lr: 0.021058, loss: 1.7010
2022-07-11 09:03:40 - train: epoch 0084, iter [03000, 05004], lr: 0.021036, loss: 1.5903
2022-07-11 09:04:15 - train: epoch 0084, iter [03100, 05004], lr: 0.021015, loss: 1.5490
2022-07-11 09:04:50 - train: epoch 0084, iter [03200, 05004], lr: 0.020994, loss: 1.6918
2022-07-11 09:05:25 - train: epoch 0084, iter [03300, 05004], lr: 0.020973, loss: 1.5509
2022-07-11 09:06:00 - train: epoch 0084, iter [03400, 05004], lr: 0.020951, loss: 1.5826
2022-07-11 09:06:35 - train: epoch 0084, iter [03500, 05004], lr: 0.020930, loss: 1.3435
2022-07-11 09:07:10 - train: epoch 0084, iter [03600, 05004], lr: 0.020909, loss: 1.4814
2022-07-11 09:07:45 - train: epoch 0084, iter [03700, 05004], lr: 0.020887, loss: 1.7957
2022-07-11 09:08:21 - train: epoch 0084, iter [03800, 05004], lr: 0.020866, loss: 1.9254
2022-07-11 09:08:55 - train: epoch 0084, iter [03900, 05004], lr: 0.020845, loss: 1.6172
2022-07-11 09:09:31 - train: epoch 0084, iter [04000, 05004], lr: 0.020824, loss: 1.5020
2022-07-11 09:10:06 - train: epoch 0084, iter [04100, 05004], lr: 0.020802, loss: 1.5105
2022-07-11 09:10:40 - train: epoch 0084, iter [04200, 05004], lr: 0.020781, loss: 1.4829
2022-07-11 09:11:16 - train: epoch 0084, iter [04300, 05004], lr: 0.020760, loss: 1.5797
2022-07-11 09:11:50 - train: epoch 0084, iter [04400, 05004], lr: 0.020739, loss: 1.8691
2022-07-11 09:12:25 - train: epoch 0084, iter [04500, 05004], lr: 0.020718, loss: 1.6841
2022-07-11 09:13:01 - train: epoch 0084, iter [04600, 05004], lr: 0.020696, loss: 1.5336
2022-07-11 09:13:36 - train: epoch 0084, iter [04700, 05004], lr: 0.020675, loss: 1.3841
2022-07-11 09:14:11 - train: epoch 0084, iter [04800, 05004], lr: 0.020654, loss: 1.4297
2022-07-11 09:14:47 - train: epoch 0084, iter [04900, 05004], lr: 0.020633, loss: 1.4899
2022-07-11 09:15:20 - train: epoch 0084, iter [05000, 05004], lr: 0.020612, loss: 1.5551
2022-07-11 09:15:21 - train: epoch 084, train_loss: 1.5337
2022-07-11 09:16:39 - eval: epoch: 084, acc1: 66.726%, acc5: 87.812%, test_loss: 1.3592, per_image_load_time: 1.284ms, per_image_inference_time: 0.390ms
2022-07-11 09:16:40 - until epoch: 084, best_acc1: 67.132%
2022-07-11 09:16:40 - epoch 085 lr: 0.020611
2022-07-11 09:17:19 - train: epoch 0085, iter [00100, 05004], lr: 0.020590, loss: 1.2337
2022-07-11 09:17:54 - train: epoch 0085, iter [00200, 05004], lr: 0.020568, loss: 1.5866
2022-07-11 09:18:28 - train: epoch 0085, iter [00300, 05004], lr: 0.020547, loss: 1.5505
2022-07-11 09:19:03 - train: epoch 0085, iter [00400, 05004], lr: 0.020526, loss: 1.6801
2022-07-11 09:19:37 - train: epoch 0085, iter [00500, 05004], lr: 0.020505, loss: 1.6415
2022-07-11 09:20:11 - train: epoch 0085, iter [00600, 05004], lr: 0.020484, loss: 1.2118
2022-07-11 09:20:45 - train: epoch 0085, iter [00700, 05004], lr: 0.020463, loss: 1.5475
2022-07-11 09:21:20 - train: epoch 0085, iter [00800, 05004], lr: 0.020442, loss: 1.5831
2022-07-11 09:21:54 - train: epoch 0085, iter [00900, 05004], lr: 0.020421, loss: 1.4119
2022-07-11 09:22:29 - train: epoch 0085, iter [01000, 05004], lr: 0.020400, loss: 1.4948
2022-07-11 09:23:04 - train: epoch 0085, iter [01100, 05004], lr: 0.020378, loss: 1.6460
2022-07-11 09:23:39 - train: epoch 0085, iter [01200, 05004], lr: 0.020357, loss: 1.4219
2022-07-11 09:24:13 - train: epoch 0085, iter [01300, 05004], lr: 0.020336, loss: 1.3898
2022-07-11 09:24:48 - train: epoch 0085, iter [01400, 05004], lr: 0.020315, loss: 1.4741
2022-07-11 09:25:23 - train: epoch 0085, iter [01500, 05004], lr: 0.020294, loss: 1.6764
2022-07-11 09:25:58 - train: epoch 0085, iter [01600, 05004], lr: 0.020273, loss: 1.5440
2022-07-11 09:26:33 - train: epoch 0085, iter [01700, 05004], lr: 0.020252, loss: 1.7229
2022-07-11 09:27:07 - train: epoch 0085, iter [01800, 05004], lr: 0.020231, loss: 1.4181
2022-07-11 09:27:42 - train: epoch 0085, iter [01900, 05004], lr: 0.020210, loss: 1.4746
2022-07-11 09:28:16 - train: epoch 0085, iter [02000, 05004], lr: 0.020189, loss: 1.4611
2022-07-11 09:28:51 - train: epoch 0085, iter [02100, 05004], lr: 0.020168, loss: 1.4628
2022-07-11 09:29:26 - train: epoch 0085, iter [02200, 05004], lr: 0.020147, loss: 1.5363
2022-07-11 09:30:00 - train: epoch 0085, iter [02300, 05004], lr: 0.020126, loss: 1.4257
2022-07-11 09:30:35 - train: epoch 0085, iter [02400, 05004], lr: 0.020105, loss: 1.6930
2022-07-11 09:31:09 - train: epoch 0085, iter [02500, 05004], lr: 0.020084, loss: 1.7011
2022-07-11 09:31:43 - train: epoch 0085, iter [02600, 05004], lr: 0.020063, loss: 1.7545
2022-07-11 09:32:18 - train: epoch 0085, iter [02700, 05004], lr: 0.020042, loss: 1.3826
2022-07-11 09:32:53 - train: epoch 0085, iter [02800, 05004], lr: 0.020021, loss: 1.8870
2022-07-11 09:33:27 - train: epoch 0085, iter [02900, 05004], lr: 0.020000, loss: 1.5431
2022-07-11 09:34:02 - train: epoch 0085, iter [03000, 05004], lr: 0.019979, loss: 1.5525
2022-07-11 09:34:36 - train: epoch 0085, iter [03100, 05004], lr: 0.019959, loss: 1.4443
2022-07-11 09:35:11 - train: epoch 0085, iter [03200, 05004], lr: 0.019938, loss: 1.6340
2022-07-11 09:35:45 - train: epoch 0085, iter [03300, 05004], lr: 0.019917, loss: 1.5242
2022-07-11 09:36:20 - train: epoch 0085, iter [03400, 05004], lr: 0.019896, loss: 1.5445
2022-07-11 09:36:55 - train: epoch 0085, iter [03500, 05004], lr: 0.019875, loss: 1.6506
2022-07-11 09:37:29 - train: epoch 0085, iter [03600, 05004], lr: 0.019854, loss: 1.6114
2022-07-11 09:38:05 - train: epoch 0085, iter [03700, 05004], lr: 0.019833, loss: 1.3901
2022-07-11 09:38:39 - train: epoch 0085, iter [03800, 05004], lr: 0.019812, loss: 1.5563
2022-07-11 09:39:14 - train: epoch 0085, iter [03900, 05004], lr: 0.019792, loss: 1.4069
2022-07-11 09:39:49 - train: epoch 0085, iter [04000, 05004], lr: 0.019771, loss: 1.7473
2022-07-11 09:40:23 - train: epoch 0085, iter [04100, 05004], lr: 0.019750, loss: 1.7153
2022-07-11 09:40:58 - train: epoch 0085, iter [04200, 05004], lr: 0.019729, loss: 1.5721
2022-07-11 09:41:33 - train: epoch 0085, iter [04300, 05004], lr: 0.019708, loss: 1.5519
2022-07-11 09:42:07 - train: epoch 0085, iter [04400, 05004], lr: 0.019687, loss: 1.4867
2022-07-11 09:42:41 - train: epoch 0085, iter [04500, 05004], lr: 0.019667, loss: 1.4462
2022-07-11 09:43:17 - train: epoch 0085, iter [04600, 05004], lr: 0.019646, loss: 1.4574
2022-07-11 09:43:52 - train: epoch 0085, iter [04700, 05004], lr: 0.019625, loss: 1.7100
2022-07-11 09:44:26 - train: epoch 0085, iter [04800, 05004], lr: 0.019604, loss: 1.4634
2022-07-11 09:45:01 - train: epoch 0085, iter [04900, 05004], lr: 0.019584, loss: 1.4561
2022-07-11 09:45:34 - train: epoch 0085, iter [05000, 05004], lr: 0.019563, loss: 1.2883
2022-07-11 09:45:35 - train: epoch 085, train_loss: 1.5203
2022-07-11 09:46:51 - eval: epoch: 085, acc1: 68.028%, acc5: 88.442%, test_loss: 1.3059, per_image_load_time: 2.648ms, per_image_inference_time: 0.362ms
2022-07-11 09:46:52 - until epoch: 085, best_acc1: 68.028%
2022-07-11 09:46:52 - epoch 086 lr: 0.019562
2022-07-11 09:47:32 - train: epoch 0086, iter [00100, 05004], lr: 0.019541, loss: 1.4232
2022-07-11 09:48:06 - train: epoch 0086, iter [00200, 05004], lr: 0.019520, loss: 1.4949
2022-07-11 09:48:40 - train: epoch 0086, iter [00300, 05004], lr: 0.019500, loss: 1.7346
2022-07-11 09:49:14 - train: epoch 0086, iter [00400, 05004], lr: 0.019479, loss: 1.3835
2022-07-11 09:49:48 - train: epoch 0086, iter [00500, 05004], lr: 0.019458, loss: 1.4465
2022-07-11 09:50:23 - train: epoch 0086, iter [00600, 05004], lr: 0.019438, loss: 1.4953
2022-07-11 09:50:57 - train: epoch 0086, iter [00700, 05004], lr: 0.019417, loss: 1.4476
2022-07-11 09:51:31 - train: epoch 0086, iter [00800, 05004], lr: 0.019396, loss: 1.5717
2022-07-11 09:52:06 - train: epoch 0086, iter [00900, 05004], lr: 0.019375, loss: 1.5117
2022-07-11 09:52:40 - train: epoch 0086, iter [01000, 05004], lr: 0.019355, loss: 1.6780
2022-07-11 09:53:15 - train: epoch 0086, iter [01100, 05004], lr: 0.019334, loss: 1.5125
2022-07-11 09:53:49 - train: epoch 0086, iter [01200, 05004], lr: 0.019313, loss: 1.3907
2022-07-11 09:54:24 - train: epoch 0086, iter [01300, 05004], lr: 0.019293, loss: 1.6214
2022-07-11 09:54:58 - train: epoch 0086, iter [01400, 05004], lr: 0.019272, loss: 1.5833
2022-07-11 09:55:33 - train: epoch 0086, iter [01500, 05004], lr: 0.019252, loss: 1.3580
2022-07-11 09:56:07 - train: epoch 0086, iter [01600, 05004], lr: 0.019231, loss: 1.5354
2022-07-11 09:56:42 - train: epoch 0086, iter [01700, 05004], lr: 0.019210, loss: 1.3643
2022-07-11 09:57:17 - train: epoch 0086, iter [01800, 05004], lr: 0.019190, loss: 1.6954
2022-07-11 09:57:51 - train: epoch 0086, iter [01900, 05004], lr: 0.019169, loss: 1.4644
2022-07-11 09:58:27 - train: epoch 0086, iter [02000, 05004], lr: 0.019149, loss: 1.6905
2022-07-11 09:59:01 - train: epoch 0086, iter [02100, 05004], lr: 0.019128, loss: 1.4521
2022-07-11 09:59:35 - train: epoch 0086, iter [02200, 05004], lr: 0.019107, loss: 1.5747
2022-07-11 10:00:08 - train: epoch 0086, iter [02300, 05004], lr: 0.019087, loss: 1.3565
2022-07-11 10:00:43 - train: epoch 0086, iter [02400, 05004], lr: 0.019066, loss: 1.2872
2022-07-11 10:01:17 - train: epoch 0086, iter [02500, 05004], lr: 0.019046, loss: 1.4004
2022-07-11 10:01:52 - train: epoch 0086, iter [02600, 05004], lr: 0.019025, loss: 1.6811
2022-07-11 10:02:26 - train: epoch 0086, iter [02700, 05004], lr: 0.019005, loss: 1.6539
2022-07-11 10:03:00 - train: epoch 0086, iter [02800, 05004], lr: 0.018984, loss: 1.6191
2022-07-11 10:03:36 - train: epoch 0086, iter [02900, 05004], lr: 0.018964, loss: 1.3906
2022-07-11 10:04:09 - train: epoch 0086, iter [03000, 05004], lr: 0.018943, loss: 1.3974
2022-07-11 10:04:43 - train: epoch 0086, iter [03100, 05004], lr: 0.018923, loss: 1.3247
2022-07-11 10:05:17 - train: epoch 0086, iter [03200, 05004], lr: 0.018902, loss: 1.6652
2022-07-11 10:05:52 - train: epoch 0086, iter [03300, 05004], lr: 0.018882, loss: 1.4533
2022-07-11 10:06:27 - train: epoch 0086, iter [03400, 05004], lr: 0.018861, loss: 1.5121
2022-07-11 10:07:01 - train: epoch 0086, iter [03500, 05004], lr: 0.018841, loss: 1.6826
2022-07-11 10:07:36 - train: epoch 0086, iter [03600, 05004], lr: 0.018820, loss: 1.3296
2022-07-11 10:08:10 - train: epoch 0086, iter [03700, 05004], lr: 0.018800, loss: 1.4196
2022-07-11 10:08:45 - train: epoch 0086, iter [03800, 05004], lr: 0.018779, loss: 1.4856
2022-07-11 10:09:19 - train: epoch 0086, iter [03900, 05004], lr: 0.018759, loss: 1.6303
2022-07-11 10:09:54 - train: epoch 0086, iter [04000, 05004], lr: 0.018739, loss: 1.5347
2022-07-11 10:10:28 - train: epoch 0086, iter [04100, 05004], lr: 0.018718, loss: 1.5796
2022-07-11 10:11:02 - train: epoch 0086, iter [04200, 05004], lr: 0.018698, loss: 1.6657
2022-07-11 10:11:37 - train: epoch 0086, iter [04300, 05004], lr: 0.018677, loss: 1.6344
2022-07-11 10:12:11 - train: epoch 0086, iter [04400, 05004], lr: 0.018657, loss: 1.3409
2022-07-11 10:12:45 - train: epoch 0086, iter [04500, 05004], lr: 0.018637, loss: 1.4165
2022-07-11 10:13:20 - train: epoch 0086, iter [04600, 05004], lr: 0.018616, loss: 1.4568
2022-07-11 10:13:55 - train: epoch 0086, iter [04700, 05004], lr: 0.018596, loss: 1.5422
2022-07-11 10:14:28 - train: epoch 0086, iter [04800, 05004], lr: 0.018575, loss: 1.5512
2022-07-11 10:15:03 - train: epoch 0086, iter [04900, 05004], lr: 0.018555, loss: 1.5454
2022-07-11 10:15:36 - train: epoch 0086, iter [05000, 05004], lr: 0.018535, loss: 1.6456
2022-07-11 10:15:37 - train: epoch 086, train_loss: 1.5040
2022-07-11 10:16:54 - eval: epoch: 086, acc1: 67.984%, acc5: 88.334%, test_loss: 1.3171, per_image_load_time: 2.661ms, per_image_inference_time: 0.354ms
2022-07-11 10:16:54 - until epoch: 086, best_acc1: 68.028%
2022-07-11 10:16:54 - epoch 087 lr: 0.018534
2022-07-11 10:17:35 - train: epoch 0087, iter [00100, 05004], lr: 0.018514, loss: 1.3119
2022-07-11 10:18:09 - train: epoch 0087, iter [00200, 05004], lr: 0.018493, loss: 1.4447
2022-07-11 10:18:42 - train: epoch 0087, iter [00300, 05004], lr: 0.018473, loss: 1.5299
2022-07-11 10:19:16 - train: epoch 0087, iter [00400, 05004], lr: 0.018453, loss: 1.4333
2022-07-11 10:19:50 - train: epoch 0087, iter [00500, 05004], lr: 0.018432, loss: 1.2223
2022-07-11 10:20:24 - train: epoch 0087, iter [00600, 05004], lr: 0.018412, loss: 1.5779
2022-07-11 10:20:58 - train: epoch 0087, iter [00700, 05004], lr: 0.018392, loss: 1.4924
2022-07-11 10:21:32 - train: epoch 0087, iter [00800, 05004], lr: 0.018372, loss: 1.3812
2022-07-11 10:22:07 - train: epoch 0087, iter [00900, 05004], lr: 0.018351, loss: 1.4330
2022-07-11 10:22:41 - train: epoch 0087, iter [01000, 05004], lr: 0.018331, loss: 1.4166
2022-07-11 10:23:14 - train: epoch 0087, iter [01100, 05004], lr: 0.018311, loss: 1.7567
2022-07-11 10:23:49 - train: epoch 0087, iter [01200, 05004], lr: 0.018291, loss: 1.3731
2022-07-11 10:24:23 - train: epoch 0087, iter [01300, 05004], lr: 0.018270, loss: 1.8465
2022-07-11 10:24:58 - train: epoch 0087, iter [01400, 05004], lr: 0.018250, loss: 1.3555
2022-07-11 10:25:32 - train: epoch 0087, iter [01500, 05004], lr: 0.018230, loss: 1.5989
2022-07-11 10:26:06 - train: epoch 0087, iter [01600, 05004], lr: 0.018210, loss: 1.4520
2022-07-11 10:26:40 - train: epoch 0087, iter [01700, 05004], lr: 0.018190, loss: 1.6442
2022-07-11 10:27:15 - train: epoch 0087, iter [01800, 05004], lr: 0.018169, loss: 1.4609
2022-07-11 10:27:49 - train: epoch 0087, iter [01900, 05004], lr: 0.018149, loss: 1.6242
2022-07-11 10:28:24 - train: epoch 0087, iter [02000, 05004], lr: 0.018129, loss: 1.3854
2022-07-11 10:28:58 - train: epoch 0087, iter [02100, 05004], lr: 0.018109, loss: 1.6258
2022-07-11 10:29:33 - train: epoch 0087, iter [02200, 05004], lr: 0.018089, loss: 1.6090
2022-07-11 10:30:07 - train: epoch 0087, iter [02300, 05004], lr: 0.018069, loss: 1.6703
2022-07-11 10:30:41 - train: epoch 0087, iter [02400, 05004], lr: 0.018049, loss: 1.5672
2022-07-11 10:31:15 - train: epoch 0087, iter [02500, 05004], lr: 0.018028, loss: 1.5456
2022-07-11 10:31:49 - train: epoch 0087, iter [02600, 05004], lr: 0.018008, loss: 1.4672
2022-07-11 10:32:24 - train: epoch 0087, iter [02700, 05004], lr: 0.017988, loss: 1.5613
2022-07-11 10:32:58 - train: epoch 0087, iter [02800, 05004], lr: 0.017968, loss: 1.4046
2022-07-11 10:33:32 - train: epoch 0087, iter [02900, 05004], lr: 0.017948, loss: 1.3287
2022-07-11 10:34:07 - train: epoch 0087, iter [03000, 05004], lr: 0.017928, loss: 1.5393
2022-07-11 10:34:41 - train: epoch 0087, iter [03100, 05004], lr: 0.017908, loss: 1.4095
2022-07-11 10:35:14 - train: epoch 0087, iter [03200, 05004], lr: 0.017888, loss: 1.6516
2022-07-11 10:35:49 - train: epoch 0087, iter [03300, 05004], lr: 0.017868, loss: 1.4695
2022-07-11 10:36:23 - train: epoch 0087, iter [03400, 05004], lr: 0.017848, loss: 1.3773
2022-07-11 10:36:58 - train: epoch 0087, iter [03500, 05004], lr: 0.017828, loss: 1.3349
2022-07-11 10:37:32 - train: epoch 0087, iter [03600, 05004], lr: 0.017808, loss: 1.3595
2022-07-11 10:38:07 - train: epoch 0087, iter [03700, 05004], lr: 0.017788, loss: 1.2591
2022-07-11 10:38:41 - train: epoch 0087, iter [03800, 05004], lr: 0.017768, loss: 1.4193
2022-07-11 10:39:15 - train: epoch 0087, iter [03900, 05004], lr: 0.017748, loss: 1.6640
2022-07-11 10:39:50 - train: epoch 0087, iter [04000, 05004], lr: 0.017728, loss: 1.4601
2022-07-11 10:40:24 - train: epoch 0087, iter [04100, 05004], lr: 0.017708, loss: 1.6565
2022-07-11 10:40:59 - train: epoch 0087, iter [04200, 05004], lr: 0.017688, loss: 1.7381
2022-07-11 10:41:33 - train: epoch 0087, iter [04300, 05004], lr: 0.017668, loss: 1.4916
2022-07-11 10:42:07 - train: epoch 0087, iter [04400, 05004], lr: 0.017648, loss: 1.3784
2022-07-11 10:42:42 - train: epoch 0087, iter [04500, 05004], lr: 0.017628, loss: 1.3022
2022-07-11 10:43:15 - train: epoch 0087, iter [04600, 05004], lr: 0.017608, loss: 1.4822
2022-07-11 10:43:50 - train: epoch 0087, iter [04700, 05004], lr: 0.017588, loss: 1.3955
2022-07-11 10:44:25 - train: epoch 0087, iter [04800, 05004], lr: 0.017568, loss: 1.4489
2022-07-11 10:44:59 - train: epoch 0087, iter [04900, 05004], lr: 0.017548, loss: 1.6167
2022-07-11 10:45:31 - train: epoch 0087, iter [05000, 05004], lr: 0.017528, loss: 1.5398
2022-07-11 10:45:32 - train: epoch 087, train_loss: 1.4877
2022-07-11 10:46:49 - eval: epoch: 087, acc1: 67.642%, acc5: 88.034%, test_loss: 1.3286, per_image_load_time: 2.624ms, per_image_inference_time: 0.369ms
2022-07-11 10:46:49 - until epoch: 087, best_acc1: 68.028%
2022-07-11 10:46:49 - epoch 088 lr: 0.017527
2022-07-11 10:47:29 - train: epoch 0088, iter [00100, 05004], lr: 0.017508, loss: 1.3519
2022-07-11 10:48:03 - train: epoch 0088, iter [00200, 05004], lr: 0.017488, loss: 1.5454
2022-07-11 10:48:37 - train: epoch 0088, iter [00300, 05004], lr: 0.017468, loss: 1.5607
2022-07-11 10:49:12 - train: epoch 0088, iter [00400, 05004], lr: 0.017448, loss: 1.3908
2022-07-11 10:49:46 - train: epoch 0088, iter [00500, 05004], lr: 0.017428, loss: 1.5576
2022-07-11 10:50:20 - train: epoch 0088, iter [00600, 05004], lr: 0.017408, loss: 1.5314
2022-07-11 10:50:55 - train: epoch 0088, iter [00700, 05004], lr: 0.017389, loss: 1.4041
2022-07-11 10:51:29 - train: epoch 0088, iter [00800, 05004], lr: 0.017369, loss: 1.5136
2022-07-11 10:52:04 - train: epoch 0088, iter [00900, 05004], lr: 0.017349, loss: 1.3744
2022-07-11 10:52:39 - train: epoch 0088, iter [01000, 05004], lr: 0.017329, loss: 1.4275
2022-07-11 10:53:14 - train: epoch 0088, iter [01100, 05004], lr: 0.017309, loss: 1.3921
2022-07-11 10:53:48 - train: epoch 0088, iter [01200, 05004], lr: 0.017290, loss: 1.5515
2022-07-11 10:54:24 - train: epoch 0088, iter [01300, 05004], lr: 0.017270, loss: 1.3601
2022-07-11 10:54:58 - train: epoch 0088, iter [01400, 05004], lr: 0.017250, loss: 1.3681
2022-07-11 10:55:32 - train: epoch 0088, iter [01500, 05004], lr: 0.017230, loss: 1.4656
2022-07-11 10:56:07 - train: epoch 0088, iter [01600, 05004], lr: 0.017210, loss: 1.3027
2022-07-11 10:56:41 - train: epoch 0088, iter [01700, 05004], lr: 0.017191, loss: 1.4464
2022-07-11 10:57:15 - train: epoch 0088, iter [01800, 05004], lr: 0.017171, loss: 1.3139
2022-07-11 10:57:50 - train: epoch 0088, iter [01900, 05004], lr: 0.017151, loss: 1.5655
2022-07-11 10:58:23 - train: epoch 0088, iter [02000, 05004], lr: 0.017132, loss: 1.5617
2022-07-11 10:58:57 - train: epoch 0088, iter [02100, 05004], lr: 0.017112, loss: 1.4461
2022-07-11 10:59:32 - train: epoch 0088, iter [02200, 05004], lr: 0.017092, loss: 1.5175
2022-07-11 11:00:06 - train: epoch 0088, iter [02300, 05004], lr: 0.017072, loss: 1.4247
2022-07-11 11:00:39 - train: epoch 0088, iter [02400, 05004], lr: 0.017053, loss: 1.5742
2022-07-11 11:01:14 - train: epoch 0088, iter [02500, 05004], lr: 0.017033, loss: 1.5024
2022-07-11 11:01:47 - train: epoch 0088, iter [02600, 05004], lr: 0.017013, loss: 1.5301
2022-07-11 11:02:21 - train: epoch 0088, iter [02700, 05004], lr: 0.016994, loss: 1.5641
2022-07-11 11:02:55 - train: epoch 0088, iter [02800, 05004], lr: 0.016974, loss: 1.6422
2022-07-11 11:03:29 - train: epoch 0088, iter [02900, 05004], lr: 0.016955, loss: 1.4499
2022-07-11 11:04:03 - train: epoch 0088, iter [03000, 05004], lr: 0.016935, loss: 1.7280
2022-07-11 11:04:37 - train: epoch 0088, iter [03100, 05004], lr: 0.016915, loss: 1.3769
2022-07-11 11:05:11 - train: epoch 0088, iter [03200, 05004], lr: 0.016896, loss: 1.5476
2022-07-11 11:05:44 - train: epoch 0088, iter [03300, 05004], lr: 0.016876, loss: 1.2394
2022-07-11 11:06:17 - train: epoch 0088, iter [03400, 05004], lr: 0.016856, loss: 1.3504
2022-07-11 11:06:51 - train: epoch 0088, iter [03500, 05004], lr: 0.016837, loss: 1.4432
2022-07-11 11:07:24 - train: epoch 0088, iter [03600, 05004], lr: 0.016817, loss: 1.5003
2022-07-11 11:07:57 - train: epoch 0088, iter [03700, 05004], lr: 0.016798, loss: 1.6368
2022-07-11 11:08:31 - train: epoch 0088, iter [03800, 05004], lr: 0.016778, loss: 1.4403
2022-07-11 11:09:04 - train: epoch 0088, iter [03900, 05004], lr: 0.016759, loss: 1.4394
2022-07-11 11:09:38 - train: epoch 0088, iter [04000, 05004], lr: 0.016739, loss: 1.3199
2022-07-11 11:10:11 - train: epoch 0088, iter [04100, 05004], lr: 0.016720, loss: 1.5433
2022-07-11 11:10:45 - train: epoch 0088, iter [04200, 05004], lr: 0.016700, loss: 1.4972
2022-07-11 11:11:18 - train: epoch 0088, iter [04300, 05004], lr: 0.016681, loss: 1.5017
2022-07-11 11:11:52 - train: epoch 0088, iter [04400, 05004], lr: 0.016661, loss: 1.3670
2022-07-11 11:12:25 - train: epoch 0088, iter [04500, 05004], lr: 0.016642, loss: 1.4714
2022-07-11 11:12:58 - train: epoch 0088, iter [04600, 05004], lr: 0.016622, loss: 1.5972
2022-07-11 11:13:32 - train: epoch 0088, iter [04700, 05004], lr: 0.016603, loss: 1.4143
2022-07-11 11:14:05 - train: epoch 0088, iter [04800, 05004], lr: 0.016583, loss: 1.5417
2022-07-11 11:14:40 - train: epoch 0088, iter [04900, 05004], lr: 0.016564, loss: 1.4662
2022-07-11 11:15:12 - train: epoch 0088, iter [05000, 05004], lr: 0.016544, loss: 1.4626
2022-07-11 11:15:13 - train: epoch 088, train_loss: 1.4707
2022-07-11 11:16:29 - eval: epoch: 088, acc1: 68.402%, acc5: 88.674%, test_loss: 1.2892, per_image_load_time: 2.583ms, per_image_inference_time: 0.332ms
2022-07-11 11:16:29 - until epoch: 088, best_acc1: 68.402%
2022-07-11 11:16:29 - epoch 089 lr: 0.016543
2022-07-11 11:17:08 - train: epoch 0089, iter [00100, 05004], lr: 0.016524, loss: 1.4978
2022-07-11 11:17:42 - train: epoch 0089, iter [00200, 05004], lr: 0.016505, loss: 1.1282
2022-07-11 11:18:15 - train: epoch 0089, iter [00300, 05004], lr: 0.016485, loss: 1.5044
2022-07-11 11:18:49 - train: epoch 0089, iter [00400, 05004], lr: 0.016466, loss: 1.4000
2022-07-11 11:19:21 - train: epoch 0089, iter [00500, 05004], lr: 0.016446, loss: 1.5761
2022-07-11 11:19:52 - train: epoch 0089, iter [00600, 05004], lr: 0.016427, loss: 1.4548
2022-07-11 11:20:24 - train: epoch 0089, iter [00700, 05004], lr: 0.016408, loss: 1.5969
2022-07-11 11:20:55 - train: epoch 0089, iter [00800, 05004], lr: 0.016388, loss: 1.5747
2022-07-11 11:21:28 - train: epoch 0089, iter [00900, 05004], lr: 0.016369, loss: 1.3857
2022-07-11 11:22:00 - train: epoch 0089, iter [01000, 05004], lr: 0.016350, loss: 1.5167
2022-07-11 11:22:32 - train: epoch 0089, iter [01100, 05004], lr: 0.016330, loss: 1.4356
2022-07-11 11:23:04 - train: epoch 0089, iter [01200, 05004], lr: 0.016311, loss: 1.5456
2022-07-11 11:23:37 - train: epoch 0089, iter [01300, 05004], lr: 0.016292, loss: 1.4862
2022-07-11 11:24:09 - train: epoch 0089, iter [01400, 05004], lr: 0.016272, loss: 1.6068
2022-07-11 11:24:41 - train: epoch 0089, iter [01500, 05004], lr: 0.016253, loss: 1.4668
2022-07-11 11:25:14 - train: epoch 0089, iter [01600, 05004], lr: 0.016234, loss: 1.3321
2022-07-11 11:25:46 - train: epoch 0089, iter [01700, 05004], lr: 0.016214, loss: 1.4356
2022-07-11 11:26:19 - train: epoch 0089, iter [01800, 05004], lr: 0.016195, loss: 1.4023
2022-07-11 11:26:50 - train: epoch 0089, iter [01900, 05004], lr: 0.016176, loss: 1.3815
2022-07-11 11:27:24 - train: epoch 0089, iter [02000, 05004], lr: 0.016157, loss: 1.4899
2022-07-11 11:27:55 - train: epoch 0089, iter [02100, 05004], lr: 0.016137, loss: 1.5778
2022-07-11 11:28:27 - train: epoch 0089, iter [02200, 05004], lr: 0.016118, loss: 1.5669
2022-07-11 11:29:00 - train: epoch 0089, iter [02300, 05004], lr: 0.016099, loss: 1.2530
2022-07-11 11:29:32 - train: epoch 0089, iter [02400, 05004], lr: 0.016080, loss: 1.4977
2022-07-11 11:30:04 - train: epoch 0089, iter [02500, 05004], lr: 0.016060, loss: 1.5755
2022-07-11 11:30:37 - train: epoch 0089, iter [02600, 05004], lr: 0.016041, loss: 1.2461
2022-07-11 11:31:09 - train: epoch 0089, iter [02700, 05004], lr: 0.016022, loss: 1.5066
2022-07-11 11:31:41 - train: epoch 0089, iter [02800, 05004], lr: 0.016003, loss: 1.4843
2022-07-11 11:32:13 - train: epoch 0089, iter [02900, 05004], lr: 0.015984, loss: 1.4419
2022-07-11 11:32:46 - train: epoch 0089, iter [03000, 05004], lr: 0.015964, loss: 1.5363
2022-07-11 11:33:19 - train: epoch 0089, iter [03100, 05004], lr: 0.015945, loss: 1.4399
2022-07-11 11:33:52 - train: epoch 0089, iter [03200, 05004], lr: 0.015926, loss: 1.5347
2022-07-11 11:34:25 - train: epoch 0089, iter [03300, 05004], lr: 0.015907, loss: 1.7101
2022-07-11 11:34:59 - train: epoch 0089, iter [03400, 05004], lr: 0.015888, loss: 1.3632
2022-07-11 11:35:33 - train: epoch 0089, iter [03500, 05004], lr: 0.015869, loss: 1.2134
2022-07-11 11:36:06 - train: epoch 0089, iter [03600, 05004], lr: 0.015850, loss: 1.4183
2022-07-11 11:36:40 - train: epoch 0089, iter [03700, 05004], lr: 0.015831, loss: 1.4670
2022-07-11 11:37:14 - train: epoch 0089, iter [03800, 05004], lr: 0.015811, loss: 1.3212
2022-07-11 11:37:50 - train: epoch 0089, iter [03900, 05004], lr: 0.015792, loss: 1.6729
2022-07-11 11:38:23 - train: epoch 0089, iter [04000, 05004], lr: 0.015773, loss: 1.5093
2022-07-11 11:38:57 - train: epoch 0089, iter [04100, 05004], lr: 0.015754, loss: 1.4975
2022-07-11 11:39:31 - train: epoch 0089, iter [04200, 05004], lr: 0.015735, loss: 1.4448
2022-07-11 11:40:07 - train: epoch 0089, iter [04300, 05004], lr: 0.015716, loss: 1.4122
2022-07-11 11:40:40 - train: epoch 0089, iter [04400, 05004], lr: 0.015697, loss: 1.4368
2022-07-11 11:41:14 - train: epoch 0089, iter [04500, 05004], lr: 0.015678, loss: 1.3162
2022-07-11 11:41:49 - train: epoch 0089, iter [04600, 05004], lr: 0.015659, loss: 1.3543
2022-07-11 11:42:23 - train: epoch 0089, iter [04700, 05004], lr: 0.015640, loss: 1.4694
2022-07-11 11:42:58 - train: epoch 0089, iter [04800, 05004], lr: 0.015621, loss: 1.4579
2022-07-11 11:43:32 - train: epoch 0089, iter [04900, 05004], lr: 0.015602, loss: 1.3847
2022-07-11 11:44:05 - train: epoch 0089, iter [05000, 05004], lr: 0.015583, loss: 1.2428
2022-07-11 11:44:06 - train: epoch 089, train_loss: 1.4564
2022-07-11 11:45:23 - eval: epoch: 089, acc1: 68.240%, acc5: 88.528%, test_loss: 1.2963, per_image_load_time: 2.569ms, per_image_inference_time: 0.408ms
2022-07-11 11:45:23 - until epoch: 089, best_acc1: 68.402%
2022-07-11 11:45:23 - epoch 090 lr: 0.015582
2022-07-11 11:46:04 - train: epoch 0090, iter [00100, 05004], lr: 0.015563, loss: 1.5072
2022-07-11 11:46:38 - train: epoch 0090, iter [00200, 05004], lr: 0.015544, loss: 1.3623
2022-07-11 11:47:12 - train: epoch 0090, iter [00300, 05004], lr: 0.015525, loss: 1.2703
2022-07-11 11:47:46 - train: epoch 0090, iter [00400, 05004], lr: 0.015506, loss: 1.3810
2022-07-11 11:48:21 - train: epoch 0090, iter [00500, 05004], lr: 0.015488, loss: 1.5081
2022-07-11 11:48:55 - train: epoch 0090, iter [00600, 05004], lr: 0.015469, loss: 1.6718
2022-07-11 11:49:29 - train: epoch 0090, iter [00700, 05004], lr: 0.015450, loss: 1.2901
2022-07-11 11:50:04 - train: epoch 0090, iter [00800, 05004], lr: 0.015431, loss: 1.2907
2022-07-11 11:50:39 - train: epoch 0090, iter [00900, 05004], lr: 0.015412, loss: 1.3475
2022-07-11 11:51:13 - train: epoch 0090, iter [01000, 05004], lr: 0.015393, loss: 1.3729
2022-07-11 11:51:48 - train: epoch 0090, iter [01100, 05004], lr: 0.015374, loss: 1.3425
2022-07-11 11:52:21 - train: epoch 0090, iter [01200, 05004], lr: 0.015355, loss: 1.2014
2022-07-11 11:52:56 - train: epoch 0090, iter [01300, 05004], lr: 0.015336, loss: 1.3012
2022-07-11 11:53:30 - train: epoch 0090, iter [01400, 05004], lr: 0.015318, loss: 1.3089
2022-07-11 11:54:04 - train: epoch 0090, iter [01500, 05004], lr: 0.015299, loss: 1.6426
2022-07-11 11:54:39 - train: epoch 0090, iter [01600, 05004], lr: 0.015280, loss: 1.3580
2022-07-11 11:55:13 - train: epoch 0090, iter [01700, 05004], lr: 0.015261, loss: 1.3948
2022-07-11 11:55:46 - train: epoch 0090, iter [01800, 05004], lr: 0.015242, loss: 1.3494
2022-07-11 11:56:20 - train: epoch 0090, iter [01900, 05004], lr: 0.015223, loss: 1.1919
2022-07-11 11:56:55 - train: epoch 0090, iter [02000, 05004], lr: 0.015205, loss: 1.5664
2022-07-11 11:57:29 - train: epoch 0090, iter [02100, 05004], lr: 0.015186, loss: 1.3692
2022-07-11 11:58:03 - train: epoch 0090, iter [02200, 05004], lr: 0.015167, loss: 1.4191
2022-07-11 11:58:38 - train: epoch 0090, iter [02300, 05004], lr: 0.015148, loss: 1.5812
2022-07-11 11:59:12 - train: epoch 0090, iter [02400, 05004], lr: 0.015130, loss: 1.4674
2022-07-11 11:59:47 - train: epoch 0090, iter [02500, 05004], lr: 0.015111, loss: 1.5877
2022-07-11 12:00:20 - train: epoch 0090, iter [02600, 05004], lr: 0.015092, loss: 1.4854
2022-07-11 12:00:53 - train: epoch 0090, iter [02700, 05004], lr: 0.015073, loss: 1.3887
2022-07-11 12:01:28 - train: epoch 0090, iter [02800, 05004], lr: 0.015055, loss: 1.5776
2022-07-11 12:02:03 - train: epoch 0090, iter [02900, 05004], lr: 0.015036, loss: 1.4008
2022-07-11 12:02:38 - train: epoch 0090, iter [03000, 05004], lr: 0.015017, loss: 1.4288
2022-07-11 12:03:11 - train: epoch 0090, iter [03100, 05004], lr: 0.014999, loss: 1.3009
2022-07-11 12:03:46 - train: epoch 0090, iter [03200, 05004], lr: 0.014980, loss: 1.3726
2022-07-11 12:04:19 - train: epoch 0090, iter [03300, 05004], lr: 0.014961, loss: 1.5317
2022-07-11 12:04:54 - train: epoch 0090, iter [03400, 05004], lr: 0.014943, loss: 1.3814
2022-07-11 12:05:27 - train: epoch 0090, iter [03500, 05004], lr: 0.014924, loss: 1.4133
2022-07-11 12:06:03 - train: epoch 0090, iter [03600, 05004], lr: 0.014905, loss: 1.2603
2022-07-11 12:06:37 - train: epoch 0090, iter [03700, 05004], lr: 0.014887, loss: 1.5324
2022-07-11 12:07:11 - train: epoch 0090, iter [03800, 05004], lr: 0.014868, loss: 1.5301
2022-07-11 12:07:46 - train: epoch 0090, iter [03900, 05004], lr: 0.014849, loss: 1.2750
2022-07-11 12:08:20 - train: epoch 0090, iter [04000, 05004], lr: 0.014831, loss: 1.5173
2022-07-11 12:08:55 - train: epoch 0090, iter [04100, 05004], lr: 0.014812, loss: 1.6557
2022-07-11 12:09:30 - train: epoch 0090, iter [04200, 05004], lr: 0.014794, loss: 1.5485
2022-07-11 12:10:04 - train: epoch 0090, iter [04300, 05004], lr: 0.014775, loss: 1.2858
2022-07-11 12:10:38 - train: epoch 0090, iter [04400, 05004], lr: 0.014757, loss: 1.3750
2022-07-11 12:11:12 - train: epoch 0090, iter [04500, 05004], lr: 0.014738, loss: 1.2258
2022-07-11 12:11:47 - train: epoch 0090, iter [04600, 05004], lr: 0.014719, loss: 1.3685
2022-07-11 12:12:21 - train: epoch 0090, iter [04700, 05004], lr: 0.014701, loss: 1.5679
2022-07-11 12:12:56 - train: epoch 0090, iter [04800, 05004], lr: 0.014682, loss: 1.7847
2022-07-11 12:13:30 - train: epoch 0090, iter [04900, 05004], lr: 0.014664, loss: 1.2866
2022-07-11 12:14:02 - train: epoch 0090, iter [05000, 05004], lr: 0.014645, loss: 1.3565
2022-07-11 12:14:03 - train: epoch 090, train_loss: 1.4388
2022-07-11 12:15:20 - eval: epoch: 090, acc1: 68.600%, acc5: 88.720%, test_loss: 1.2808, per_image_load_time: 2.566ms, per_image_inference_time: 0.371ms
2022-07-11 12:15:20 - until epoch: 090, best_acc1: 68.600%
2022-07-11 12:15:20 - epoch 091 lr: 0.014644
2022-07-11 12:15:59 - train: epoch 0091, iter [00100, 05004], lr: 0.014626, loss: 1.3599
2022-07-11 12:16:33 - train: epoch 0091, iter [00200, 05004], lr: 0.014608, loss: 1.2899
2022-07-11 12:17:08 - train: epoch 0091, iter [00300, 05004], lr: 0.014589, loss: 1.3793
2022-07-11 12:17:41 - train: epoch 0091, iter [00400, 05004], lr: 0.014571, loss: 1.4089
2022-07-11 12:18:16 - train: epoch 0091, iter [00500, 05004], lr: 0.014552, loss: 1.5062
2022-07-11 12:18:49 - train: epoch 0091, iter [00600, 05004], lr: 0.014534, loss: 1.5751
2022-07-11 12:19:24 - train: epoch 0091, iter [00700, 05004], lr: 0.014515, loss: 1.5630
2022-07-11 12:19:57 - train: epoch 0091, iter [00800, 05004], lr: 0.014497, loss: 1.0801
2022-07-11 12:20:32 - train: epoch 0091, iter [00900, 05004], lr: 0.014479, loss: 1.4924
2022-07-11 12:21:06 - train: epoch 0091, iter [01000, 05004], lr: 0.014460, loss: 1.3599
2022-07-11 12:21:41 - train: epoch 0091, iter [01100, 05004], lr: 0.014442, loss: 1.2177
2022-07-11 12:22:15 - train: epoch 0091, iter [01200, 05004], lr: 0.014423, loss: 1.4393
2022-07-11 12:22:50 - train: epoch 0091, iter [01300, 05004], lr: 0.014405, loss: 1.4762
2022-07-11 12:23:25 - train: epoch 0091, iter [01400, 05004], lr: 0.014387, loss: 1.3416
2022-07-11 12:23:59 - train: epoch 0091, iter [01500, 05004], lr: 0.014368, loss: 1.4205
2022-07-11 12:24:34 - train: epoch 0091, iter [01600, 05004], lr: 0.014350, loss: 1.2996
2022-07-11 12:25:08 - train: epoch 0091, iter [01700, 05004], lr: 0.014332, loss: 1.4603
2022-07-11 12:25:43 - train: epoch 0091, iter [01800, 05004], lr: 0.014313, loss: 1.4560
2022-07-11 12:26:18 - train: epoch 0091, iter [01900, 05004], lr: 0.014295, loss: 1.3729
2022-07-11 12:26:52 - train: epoch 0091, iter [02000, 05004], lr: 0.014277, loss: 1.4242
2022-07-11 12:27:26 - train: epoch 0091, iter [02100, 05004], lr: 0.014258, loss: 1.3472
2022-07-11 12:28:02 - train: epoch 0091, iter [02200, 05004], lr: 0.014240, loss: 1.2982
2022-07-11 12:28:35 - train: epoch 0091, iter [02300, 05004], lr: 0.014222, loss: 1.5597
2022-07-11 12:29:10 - train: epoch 0091, iter [02400, 05004], lr: 0.014204, loss: 1.2682
2022-07-11 12:29:45 - train: epoch 0091, iter [02500, 05004], lr: 0.014185, loss: 1.4324
2022-07-11 12:30:20 - train: epoch 0091, iter [02600, 05004], lr: 0.014167, loss: 1.5050
2022-07-11 12:30:54 - train: epoch 0091, iter [02700, 05004], lr: 0.014149, loss: 1.3092
2022-07-11 12:31:29 - train: epoch 0091, iter [02800, 05004], lr: 0.014131, loss: 1.5462
2022-07-11 12:32:04 - train: epoch 0091, iter [02900, 05004], lr: 0.014112, loss: 1.4220
2022-07-11 12:32:38 - train: epoch 0091, iter [03000, 05004], lr: 0.014094, loss: 1.5216
2022-07-11 12:33:13 - train: epoch 0091, iter [03100, 05004], lr: 0.014076, loss: 1.4329
2022-07-11 12:33:47 - train: epoch 0091, iter [03200, 05004], lr: 0.014058, loss: 1.5743
2022-07-11 12:34:22 - train: epoch 0091, iter [03300, 05004], lr: 0.014040, loss: 1.4381
2022-07-11 12:34:57 - train: epoch 0091, iter [03400, 05004], lr: 0.014021, loss: 1.5186
2022-07-11 12:35:31 - train: epoch 0091, iter [03500, 05004], lr: 0.014003, loss: 1.5182
2022-07-11 12:36:06 - train: epoch 0091, iter [03600, 05004], lr: 0.013985, loss: 1.3445
2022-07-11 12:36:41 - train: epoch 0091, iter [03700, 05004], lr: 0.013967, loss: 1.7204
2022-07-11 12:37:16 - train: epoch 0091, iter [03800, 05004], lr: 0.013949, loss: 1.3879
2022-07-11 12:37:50 - train: epoch 0091, iter [03900, 05004], lr: 0.013931, loss: 1.3644
2022-07-11 12:38:25 - train: epoch 0091, iter [04000, 05004], lr: 0.013913, loss: 1.4509
2022-07-11 12:39:00 - train: epoch 0091, iter [04100, 05004], lr: 0.013894, loss: 1.4716
2022-07-11 12:39:34 - train: epoch 0091, iter [04200, 05004], lr: 0.013876, loss: 1.5293
2022-07-11 12:40:09 - train: epoch 0091, iter [04300, 05004], lr: 0.013858, loss: 1.3596
2022-07-11 12:40:43 - train: epoch 0091, iter [04400, 05004], lr: 0.013840, loss: 1.2227
2022-07-11 12:41:18 - train: epoch 0091, iter [04500, 05004], lr: 0.013822, loss: 1.2992
2022-07-11 12:41:52 - train: epoch 0091, iter [04600, 05004], lr: 0.013804, loss: 1.3828
2022-07-11 12:42:26 - train: epoch 0091, iter [04700, 05004], lr: 0.013786, loss: 1.5359
2022-07-11 12:43:01 - train: epoch 0091, iter [04800, 05004], lr: 0.013768, loss: 1.5871
2022-07-11 12:43:36 - train: epoch 0091, iter [04900, 05004], lr: 0.013750, loss: 1.5029
2022-07-11 12:44:09 - train: epoch 0091, iter [05000, 05004], lr: 0.013732, loss: 1.5840
2022-07-11 12:44:10 - train: epoch 091, train_loss: 1.4247
2022-07-11 12:45:26 - eval: epoch: 091, acc1: 69.006%, acc5: 89.182%, test_loss: 1.2595, per_image_load_time: 2.585ms, per_image_inference_time: 0.355ms
2022-07-11 12:45:26 - until epoch: 091, best_acc1: 69.006%
2022-07-11 12:45:26 - epoch 092 lr: 0.013731
2022-07-11 12:46:06 - train: epoch 0092, iter [00100, 05004], lr: 0.013713, loss: 1.2121
2022-07-11 12:46:41 - train: epoch 0092, iter [00200, 05004], lr: 0.013695, loss: 1.3487
2022-07-11 12:47:16 - train: epoch 0092, iter [00300, 05004], lr: 0.013677, loss: 1.3667
2022-07-11 12:47:49 - train: epoch 0092, iter [00400, 05004], lr: 0.013659, loss: 1.2826
2022-07-11 12:48:24 - train: epoch 0092, iter [00500, 05004], lr: 0.013641, loss: 1.3590
2022-07-11 12:48:58 - train: epoch 0092, iter [00600, 05004], lr: 0.013623, loss: 1.5519
2022-07-11 12:49:34 - train: epoch 0092, iter [00700, 05004], lr: 0.013605, loss: 1.4002
2022-07-11 12:50:08 - train: epoch 0092, iter [00800, 05004], lr: 0.013588, loss: 1.4116
2022-07-11 12:50:42 - train: epoch 0092, iter [00900, 05004], lr: 0.013570, loss: 1.6043
2022-07-11 12:51:16 - train: epoch 0092, iter [01000, 05004], lr: 0.013552, loss: 1.3713
2022-07-11 12:51:51 - train: epoch 0092, iter [01100, 05004], lr: 0.013534, loss: 1.3107
2022-07-11 12:52:25 - train: epoch 0092, iter [01200, 05004], lr: 0.013516, loss: 1.5752
2022-07-11 12:52:59 - train: epoch 0092, iter [01300, 05004], lr: 0.013498, loss: 1.2939
2022-07-11 12:53:34 - train: epoch 0092, iter [01400, 05004], lr: 0.013480, loss: 1.2346
2022-07-11 12:54:08 - train: epoch 0092, iter [01500, 05004], lr: 0.013462, loss: 1.3403
2022-07-11 12:54:43 - train: epoch 0092, iter [01600, 05004], lr: 0.013444, loss: 1.1673
2022-07-11 12:55:17 - train: epoch 0092, iter [01700, 05004], lr: 0.013427, loss: 1.3250
2022-07-11 12:55:51 - train: epoch 0092, iter [01800, 05004], lr: 0.013409, loss: 1.3985
2022-07-11 12:56:26 - train: epoch 0092, iter [01900, 05004], lr: 0.013391, loss: 1.2849
2022-07-11 12:57:00 - train: epoch 0092, iter [02000, 05004], lr: 0.013373, loss: 1.2366
2022-07-11 12:57:34 - train: epoch 0092, iter [02100, 05004], lr: 0.013355, loss: 1.4859
2022-07-11 12:58:09 - train: epoch 0092, iter [02200, 05004], lr: 0.013338, loss: 1.4626
2022-07-11 12:58:44 - train: epoch 0092, iter [02300, 05004], lr: 0.013320, loss: 1.3866
2022-07-11 12:59:17 - train: epoch 0092, iter [02400, 05004], lr: 0.013302, loss: 1.4069
2022-07-11 12:59:52 - train: epoch 0092, iter [02500, 05004], lr: 0.013284, loss: 1.3832
2022-07-11 13:00:27 - train: epoch 0092, iter [02600, 05004], lr: 0.013266, loss: 1.3529
2022-07-11 13:01:01 - train: epoch 0092, iter [02700, 05004], lr: 0.013249, loss: 1.4614
2022-07-11 13:01:36 - train: epoch 0092, iter [02800, 05004], lr: 0.013231, loss: 1.2613
2022-07-11 13:02:11 - train: epoch 0092, iter [02900, 05004], lr: 0.013213, loss: 1.6139
2022-07-11 13:02:44 - train: epoch 0092, iter [03000, 05004], lr: 0.013196, loss: 1.4100
2022-07-11 13:03:19 - train: epoch 0092, iter [03100, 05004], lr: 0.013178, loss: 1.5122
2022-07-11 13:03:54 - train: epoch 0092, iter [03200, 05004], lr: 0.013160, loss: 1.3626
2022-07-11 13:04:28 - train: epoch 0092, iter [03300, 05004], lr: 0.013142, loss: 1.4775
2022-07-11 13:05:03 - train: epoch 0092, iter [03400, 05004], lr: 0.013125, loss: 1.5598
2022-07-11 13:05:37 - train: epoch 0092, iter [03500, 05004], lr: 0.013107, loss: 1.4243
2022-07-11 13:06:12 - train: epoch 0092, iter [03600, 05004], lr: 0.013090, loss: 1.3115
2022-07-11 13:06:46 - train: epoch 0092, iter [03700, 05004], lr: 0.013072, loss: 1.3525
2022-07-11 13:07:21 - train: epoch 0092, iter [03800, 05004], lr: 0.013054, loss: 1.4969
2022-07-11 13:07:56 - train: epoch 0092, iter [03900, 05004], lr: 0.013037, loss: 1.3814
2022-07-11 13:08:30 - train: epoch 0092, iter [04000, 05004], lr: 0.013019, loss: 1.4287
2022-07-11 13:09:05 - train: epoch 0092, iter [04100, 05004], lr: 0.013001, loss: 1.3946
2022-07-11 13:09:39 - train: epoch 0092, iter [04200, 05004], lr: 0.012984, loss: 1.4730
2022-07-11 13:10:14 - train: epoch 0092, iter [04300, 05004], lr: 0.012966, loss: 1.2927
2022-07-11 13:10:48 - train: epoch 0092, iter [04400, 05004], lr: 0.012949, loss: 1.3261
2022-07-11 13:11:24 - train: epoch 0092, iter [04500, 05004], lr: 0.012931, loss: 1.2708
2022-07-11 13:11:58 - train: epoch 0092, iter [04600, 05004], lr: 0.012914, loss: 1.6126
2022-07-11 13:12:33 - train: epoch 0092, iter [04700, 05004], lr: 0.012896, loss: 1.1260
2022-07-11 13:13:07 - train: epoch 0092, iter [04800, 05004], lr: 0.012878, loss: 1.4939
2022-07-11 13:13:42 - train: epoch 0092, iter [04900, 05004], lr: 0.012861, loss: 1.4051
2022-07-11 13:14:15 - train: epoch 0092, iter [05000, 05004], lr: 0.012843, loss: 1.3830
2022-07-11 13:14:16 - train: epoch 092, train_loss: 1.4072
2022-07-11 13:15:33 - eval: epoch: 092, acc1: 69.530%, acc5: 89.312%, test_loss: 1.2402, per_image_load_time: 2.240ms, per_image_inference_time: 0.397ms
2022-07-11 13:15:33 - until epoch: 092, best_acc1: 69.530%
2022-07-11 13:15:33 - epoch 093 lr: 0.012843
2022-07-11 13:16:13 - train: epoch 0093, iter [00100, 05004], lr: 0.012825, loss: 1.3838
2022-07-11 13:16:48 - train: epoch 0093, iter [00200, 05004], lr: 0.012808, loss: 1.4797
2022-07-11 13:17:22 - train: epoch 0093, iter [00300, 05004], lr: 0.012790, loss: 1.4840
2022-07-11 13:17:57 - train: epoch 0093, iter [00400, 05004], lr: 0.012773, loss: 1.3762
2022-07-11 13:18:31 - train: epoch 0093, iter [00500, 05004], lr: 0.012755, loss: 1.4014
2022-07-11 13:19:06 - train: epoch 0093, iter [00600, 05004], lr: 0.012738, loss: 1.4783
2022-07-11 13:19:41 - train: epoch 0093, iter [00700, 05004], lr: 0.012720, loss: 1.4937
2022-07-11 13:20:16 - train: epoch 0093, iter [00800, 05004], lr: 0.012703, loss: 1.2153
2022-07-11 13:20:50 - train: epoch 0093, iter [00900, 05004], lr: 0.012686, loss: 1.4831
2022-07-11 13:21:24 - train: epoch 0093, iter [01000, 05004], lr: 0.012668, loss: 1.2008
2022-07-11 13:21:59 - train: epoch 0093, iter [01100, 05004], lr: 0.012651, loss: 1.4862
2022-07-11 13:22:34 - train: epoch 0093, iter [01200, 05004], lr: 0.012633, loss: 1.5004
2022-07-11 13:23:08 - train: epoch 0093, iter [01300, 05004], lr: 0.012616, loss: 1.2343
2022-07-11 13:23:43 - train: epoch 0093, iter [01400, 05004], lr: 0.012599, loss: 1.4615
2022-07-11 13:24:18 - train: epoch 0093, iter [01500, 05004], lr: 0.012581, loss: 1.4197
2022-07-11 13:24:53 - train: epoch 0093, iter [01600, 05004], lr: 0.012564, loss: 1.3035
2022-07-11 13:25:27 - train: epoch 0093, iter [01700, 05004], lr: 0.012547, loss: 1.2592
2022-07-11 13:26:02 - train: epoch 0093, iter [01800, 05004], lr: 0.012529, loss: 1.3922
2022-07-11 13:26:38 - train: epoch 0093, iter [01900, 05004], lr: 0.012512, loss: 1.3570
2022-07-11 13:27:12 - train: epoch 0093, iter [02000, 05004], lr: 0.012495, loss: 1.3013
2022-07-11 13:27:47 - train: epoch 0093, iter [02100, 05004], lr: 0.012477, loss: 1.3231
2022-07-11 13:28:22 - train: epoch 0093, iter [02200, 05004], lr: 0.012460, loss: 1.5831
2022-07-11 13:28:57 - train: epoch 0093, iter [02300, 05004], lr: 0.012443, loss: 1.4621
2022-07-11 13:29:32 - train: epoch 0093, iter [02400, 05004], lr: 0.012426, loss: 1.2869
2022-07-11 13:30:08 - train: epoch 0093, iter [02500, 05004], lr: 0.012408, loss: 1.3598
2022-07-11 13:30:42 - train: epoch 0093, iter [02600, 05004], lr: 0.012391, loss: 1.5386
2022-07-11 13:31:17 - train: epoch 0093, iter [02700, 05004], lr: 0.012374, loss: 1.2803
2022-07-11 13:31:53 - train: epoch 0093, iter [02800, 05004], lr: 0.012357, loss: 1.3837
2022-07-11 13:32:27 - train: epoch 0093, iter [02900, 05004], lr: 0.012339, loss: 1.2573
2022-07-11 13:33:02 - train: epoch 0093, iter [03000, 05004], lr: 0.012322, loss: 1.2092
2022-07-11 13:33:37 - train: epoch 0093, iter [03100, 05004], lr: 0.012305, loss: 1.4784
2022-07-11 13:34:12 - train: epoch 0093, iter [03200, 05004], lr: 0.012288, loss: 1.2075
2022-07-11 13:34:47 - train: epoch 0093, iter [03300, 05004], lr: 0.012271, loss: 1.4445
2022-07-11 13:35:23 - train: epoch 0093, iter [03400, 05004], lr: 0.012254, loss: 1.5052
2022-07-11 13:35:58 - train: epoch 0093, iter [03500, 05004], lr: 0.012236, loss: 1.3577
2022-07-11 13:36:32 - train: epoch 0093, iter [03600, 05004], lr: 0.012219, loss: 1.5618
2022-07-11 13:37:06 - train: epoch 0093, iter [03700, 05004], lr: 0.012202, loss: 1.2400
2022-07-11 13:37:42 - train: epoch 0093, iter [03800, 05004], lr: 0.012185, loss: 1.2865
2022-07-11 13:38:17 - train: epoch 0093, iter [03900, 05004], lr: 0.012168, loss: 1.2218
2022-07-11 13:38:52 - train: epoch 0093, iter [04000, 05004], lr: 0.012151, loss: 1.6626
2022-07-11 13:39:27 - train: epoch 0093, iter [04100, 05004], lr: 0.012134, loss: 1.5616
2022-07-11 13:40:02 - train: epoch 0093, iter [04200, 05004], lr: 0.012117, loss: 1.2032
2022-07-11 13:40:36 - train: epoch 0093, iter [04300, 05004], lr: 0.012100, loss: 1.5208
2022-07-11 13:41:12 - train: epoch 0093, iter [04400, 05004], lr: 0.012083, loss: 1.3180
2022-07-11 13:41:47 - train: epoch 0093, iter [04500, 05004], lr: 0.012065, loss: 1.4050
2022-07-11 13:42:22 - train: epoch 0093, iter [04600, 05004], lr: 0.012048, loss: 1.2701
2022-07-11 13:42:57 - train: epoch 0093, iter [04700, 05004], lr: 0.012031, loss: 1.7288
2022-07-11 13:43:32 - train: epoch 0093, iter [04800, 05004], lr: 0.012014, loss: 1.2225
2022-07-11 13:44:07 - train: epoch 0093, iter [04900, 05004], lr: 0.011997, loss: 1.3203
2022-07-11 13:44:40 - train: epoch 0093, iter [05000, 05004], lr: 0.011980, loss: 1.4070
2022-07-11 13:44:41 - train: epoch 093, train_loss: 1.3883
2022-07-11 13:45:57 - eval: epoch: 093, acc1: 69.544%, acc5: 89.200%, test_loss: 1.2393, per_image_load_time: 2.584ms, per_image_inference_time: 0.390ms
2022-07-11 13:45:58 - until epoch: 093, best_acc1: 69.544%
2022-07-11 13:45:58 - epoch 094 lr: 0.011980
2022-07-11 13:46:38 - train: epoch 0094, iter [00100, 05004], lr: 0.011963, loss: 1.3340
2022-07-11 13:47:12 - train: epoch 0094, iter [00200, 05004], lr: 0.011946, loss: 1.4318
2022-07-11 13:47:47 - train: epoch 0094, iter [00300, 05004], lr: 0.011929, loss: 1.3647
2022-07-11 13:48:21 - train: epoch 0094, iter [00400, 05004], lr: 0.011912, loss: 1.4406
2022-07-11 13:48:55 - train: epoch 0094, iter [00500, 05004], lr: 0.011895, loss: 1.3975
2022-07-11 13:49:29 - train: epoch 0094, iter [00600, 05004], lr: 0.011878, loss: 1.2584
2022-07-11 13:50:04 - train: epoch 0094, iter [00700, 05004], lr: 0.011861, loss: 1.2955
2022-07-11 13:50:39 - train: epoch 0094, iter [00800, 05004], lr: 0.011844, loss: 1.3656
2022-07-11 13:51:15 - train: epoch 0094, iter [00900, 05004], lr: 0.011827, loss: 1.2651
2022-07-11 13:51:49 - train: epoch 0094, iter [01000, 05004], lr: 0.011810, loss: 1.4113
2022-07-11 13:52:24 - train: epoch 0094, iter [01100, 05004], lr: 0.011793, loss: 1.5026
2022-07-11 13:52:58 - train: epoch 0094, iter [01200, 05004], lr: 0.011777, loss: 1.4067
2022-07-11 13:53:34 - train: epoch 0094, iter [01300, 05004], lr: 0.011760, loss: 1.4130
2022-07-11 13:54:08 - train: epoch 0094, iter [01400, 05004], lr: 0.011743, loss: 1.2530
2022-07-11 13:54:42 - train: epoch 0094, iter [01500, 05004], lr: 0.011726, loss: 1.3199
2022-07-11 13:55:17 - train: epoch 0094, iter [01600, 05004], lr: 0.011709, loss: 1.7398
2022-07-11 13:55:52 - train: epoch 0094, iter [01700, 05004], lr: 0.011692, loss: 1.3549
2022-07-11 13:56:27 - train: epoch 0094, iter [01800, 05004], lr: 0.011676, loss: 1.2572
2022-07-11 13:57:01 - train: epoch 0094, iter [01900, 05004], lr: 0.011659, loss: 1.4280
2022-07-11 13:57:36 - train: epoch 0094, iter [02000, 05004], lr: 0.011642, loss: 1.1503
2022-07-11 13:58:11 - train: epoch 0094, iter [02100, 05004], lr: 0.011625, loss: 1.1035
2022-07-11 13:58:46 - train: epoch 0094, iter [02200, 05004], lr: 0.011608, loss: 1.3243
2022-07-11 13:59:20 - train: epoch 0094, iter [02300, 05004], lr: 0.011592, loss: 1.2403
2022-07-11 13:59:55 - train: epoch 0094, iter [02400, 05004], lr: 0.011575, loss: 1.3046
2022-07-11 14:00:31 - train: epoch 0094, iter [02500, 05004], lr: 0.011558, loss: 1.2651
2022-07-11 14:01:06 - train: epoch 0094, iter [02600, 05004], lr: 0.011542, loss: 1.4311
2022-07-11 14:01:41 - train: epoch 0094, iter [02700, 05004], lr: 0.011525, loss: 1.4365
2022-07-11 14:02:15 - train: epoch 0094, iter [02800, 05004], lr: 0.011508, loss: 1.5919
2022-07-11 14:02:50 - train: epoch 0094, iter [02900, 05004], lr: 0.011491, loss: 1.2835
2022-07-11 14:03:25 - train: epoch 0094, iter [03000, 05004], lr: 0.011475, loss: 1.4517
2022-07-11 14:04:00 - train: epoch 0094, iter [03100, 05004], lr: 0.011458, loss: 1.6744
2022-07-11 14:04:34 - train: epoch 0094, iter [03200, 05004], lr: 0.011441, loss: 1.2287
2022-07-11 14:05:09 - train: epoch 0094, iter [03300, 05004], lr: 0.011425, loss: 1.1880
2022-07-11 14:05:44 - train: epoch 0094, iter [03400, 05004], lr: 0.011408, loss: 1.3665
2022-07-11 14:06:20 - train: epoch 0094, iter [03500, 05004], lr: 0.011391, loss: 1.7033
2022-07-11 14:06:54 - train: epoch 0094, iter [03600, 05004], lr: 0.011375, loss: 1.2696
2022-07-11 14:07:29 - train: epoch 0094, iter [03700, 05004], lr: 0.011358, loss: 1.6119
2022-07-11 14:08:03 - train: epoch 0094, iter [03800, 05004], lr: 0.011342, loss: 1.2863
2022-07-11 14:08:39 - train: epoch 0094, iter [03900, 05004], lr: 0.011325, loss: 1.3473
2022-07-11 14:09:13 - train: epoch 0094, iter [04000, 05004], lr: 0.011309, loss: 1.2693
2022-07-11 14:09:48 - train: epoch 0094, iter [04100, 05004], lr: 0.011292, loss: 1.5261
2022-07-11 14:10:22 - train: epoch 0094, iter [04200, 05004], lr: 0.011275, loss: 1.3292
2022-07-11 14:10:57 - train: epoch 0094, iter [04300, 05004], lr: 0.011259, loss: 1.4607
2022-07-11 14:11:32 - train: epoch 0094, iter [04400, 05004], lr: 0.011242, loss: 1.5108
2022-07-11 14:12:08 - train: epoch 0094, iter [04500, 05004], lr: 0.011226, loss: 1.2795
2022-07-11 14:12:42 - train: epoch 0094, iter [04600, 05004], lr: 0.011209, loss: 1.5955
2022-07-11 14:13:18 - train: epoch 0094, iter [04700, 05004], lr: 0.011193, loss: 1.4376
2022-07-11 14:13:52 - train: epoch 0094, iter [04800, 05004], lr: 0.011176, loss: 1.1912
2022-07-11 14:14:27 - train: epoch 0094, iter [04900, 05004], lr: 0.011160, loss: 1.3056
2022-07-11 14:15:00 - train: epoch 0094, iter [05000, 05004], lr: 0.011143, loss: 1.2382
2022-07-11 14:15:01 - train: epoch 094, train_loss: 1.3721
2022-07-11 14:16:18 - eval: epoch: 094, acc1: 70.148%, acc5: 89.564%, test_loss: 1.2184, per_image_load_time: 2.595ms, per_image_inference_time: 0.402ms
2022-07-11 14:16:18 - until epoch: 094, best_acc1: 70.148%
2022-07-11 14:16:18 - epoch 095 lr: 0.011143
2022-07-11 14:16:59 - train: epoch 0095, iter [00100, 05004], lr: 0.011126, loss: 1.4687
2022-07-11 14:17:33 - train: epoch 0095, iter [00200, 05004], lr: 0.011110, loss: 1.4150
2022-07-11 14:18:07 - train: epoch 0095, iter [00300, 05004], lr: 0.011093, loss: 1.5561
2022-07-11 14:18:41 - train: epoch 0095, iter [00400, 05004], lr: 0.011077, loss: 1.2316
2022-07-11 14:19:17 - train: epoch 0095, iter [00500, 05004], lr: 0.011061, loss: 1.2558
2022-07-11 14:19:50 - train: epoch 0095, iter [00600, 05004], lr: 0.011044, loss: 1.2792
2022-07-11 14:20:25 - train: epoch 0095, iter [00700, 05004], lr: 0.011028, loss: 1.5611
2022-07-11 14:21:00 - train: epoch 0095, iter [00800, 05004], lr: 0.011011, loss: 1.3707
2022-07-11 14:21:36 - train: epoch 0095, iter [00900, 05004], lr: 0.010995, loss: 1.4459
2022-07-11 14:22:10 - train: epoch 0095, iter [01000, 05004], lr: 0.010979, loss: 1.4494
2022-07-11 14:22:44 - train: epoch 0095, iter [01100, 05004], lr: 0.010962, loss: 1.1669
2022-07-11 14:23:18 - train: epoch 0095, iter [01200, 05004], lr: 0.010946, loss: 1.2155
2022-07-11 14:23:53 - train: epoch 0095, iter [01300, 05004], lr: 0.010930, loss: 1.3037
2022-07-11 14:24:29 - train: epoch 0095, iter [01400, 05004], lr: 0.010913, loss: 1.3499
2022-07-11 14:25:03 - train: epoch 0095, iter [01500, 05004], lr: 0.010897, loss: 1.2796
2022-07-11 14:25:38 - train: epoch 0095, iter [01600, 05004], lr: 0.010881, loss: 1.2417
2022-07-11 14:26:12 - train: epoch 0095, iter [01700, 05004], lr: 0.010864, loss: 1.3550
2022-07-11 14:26:47 - train: epoch 0095, iter [01800, 05004], lr: 0.010848, loss: 1.4769
2022-07-11 14:27:22 - train: epoch 0095, iter [01900, 05004], lr: 0.010832, loss: 1.3480
2022-07-11 14:27:56 - train: epoch 0095, iter [02000, 05004], lr: 0.010816, loss: 1.4282
2022-07-11 14:28:31 - train: epoch 0095, iter [02100, 05004], lr: 0.010799, loss: 1.3754
2022-07-11 14:29:06 - train: epoch 0095, iter [02200, 05004], lr: 0.010783, loss: 1.1565
2022-07-11 14:29:41 - train: epoch 0095, iter [02300, 05004], lr: 0.010767, loss: 1.1629
2022-07-11 14:30:17 - train: epoch 0095, iter [02400, 05004], lr: 0.010751, loss: 1.4288
2022-07-11 14:30:51 - train: epoch 0095, iter [02500, 05004], lr: 0.010734, loss: 1.4875
2022-07-11 14:31:26 - train: epoch 0095, iter [02600, 05004], lr: 0.010718, loss: 1.4257
2022-07-11 14:32:01 - train: epoch 0095, iter [02700, 05004], lr: 0.010702, loss: 1.3523
2022-07-11 14:32:36 - train: epoch 0095, iter [02800, 05004], lr: 0.010686, loss: 1.0686
2022-07-11 14:33:11 - train: epoch 0095, iter [02900, 05004], lr: 0.010670, loss: 1.3163
2022-07-11 14:33:46 - train: epoch 0095, iter [03000, 05004], lr: 0.010654, loss: 1.5350
2022-07-11 14:34:21 - train: epoch 0095, iter [03100, 05004], lr: 0.010638, loss: 1.4450
2022-07-11 14:34:56 - train: epoch 0095, iter [03200, 05004], lr: 0.010621, loss: 1.5526
2022-07-11 14:35:31 - train: epoch 0095, iter [03300, 05004], lr: 0.010605, loss: 1.4059
2022-07-11 14:36:06 - train: epoch 0095, iter [03400, 05004], lr: 0.010589, loss: 1.2246
2022-07-11 14:36:40 - train: epoch 0095, iter [03500, 05004], lr: 0.010573, loss: 1.1635
2022-07-11 14:37:16 - train: epoch 0095, iter [03600, 05004], lr: 0.010557, loss: 1.2129
2022-07-11 14:37:50 - train: epoch 0095, iter [03700, 05004], lr: 0.010541, loss: 1.3202
2022-07-11 14:38:26 - train: epoch 0095, iter [03800, 05004], lr: 0.010525, loss: 1.2150
2022-07-11 14:39:01 - train: epoch 0095, iter [03900, 05004], lr: 0.010509, loss: 1.3368
2022-07-11 14:39:36 - train: epoch 0095, iter [04000, 05004], lr: 0.010493, loss: 1.3363
2022-07-11 14:40:11 - train: epoch 0095, iter [04100, 05004], lr: 0.010477, loss: 1.2217
2022-07-11 14:40:46 - train: epoch 0095, iter [04200, 05004], lr: 0.010461, loss: 1.2179
2022-07-11 14:41:21 - train: epoch 0095, iter [04300, 05004], lr: 0.010445, loss: 1.5107
2022-07-11 14:41:56 - train: epoch 0095, iter [04400, 05004], lr: 0.010429, loss: 1.6232
2022-07-11 14:42:31 - train: epoch 0095, iter [04500, 05004], lr: 0.010413, loss: 1.3111
2022-07-11 14:43:06 - train: epoch 0095, iter [04600, 05004], lr: 0.010397, loss: 1.3619
2022-07-11 14:43:41 - train: epoch 0095, iter [04700, 05004], lr: 0.010381, loss: 1.3034
2022-07-11 14:44:16 - train: epoch 0095, iter [04800, 05004], lr: 0.010365, loss: 1.3670
2022-07-11 14:44:51 - train: epoch 0095, iter [04900, 05004], lr: 0.010349, loss: 1.2560
2022-07-11 14:45:24 - train: epoch 0095, iter [05000, 05004], lr: 0.010333, loss: 1.2679
2022-07-11 14:45:25 - train: epoch 095, train_loss: 1.3560
2022-07-11 14:46:43 - eval: epoch: 095, acc1: 70.188%, acc5: 89.796%, test_loss: 1.2003, per_image_load_time: 2.385ms, per_image_inference_time: 0.381ms
2022-07-11 14:46:43 - until epoch: 095, best_acc1: 70.188%
2022-07-11 14:46:43 - epoch 096 lr: 0.010332
2022-07-11 14:47:23 - train: epoch 0096, iter [00100, 05004], lr: 0.010316, loss: 1.2679
2022-07-11 14:47:58 - train: epoch 0096, iter [00200, 05004], lr: 0.010301, loss: 1.2613
2022-07-11 14:48:33 - train: epoch 0096, iter [00300, 05004], lr: 0.010285, loss: 1.3106
2022-07-11 14:49:07 - train: epoch 0096, iter [00400, 05004], lr: 0.010269, loss: 1.2892
2022-07-11 14:49:42 - train: epoch 0096, iter [00500, 05004], lr: 0.010253, loss: 1.2996
2022-07-11 14:50:17 - train: epoch 0096, iter [00600, 05004], lr: 0.010237, loss: 1.2398
2022-07-11 14:50:51 - train: epoch 0096, iter [00700, 05004], lr: 0.010221, loss: 1.2962
2022-07-11 14:51:26 - train: epoch 0096, iter [00800, 05004], lr: 0.010205, loss: 1.0823
2022-07-11 14:52:00 - train: epoch 0096, iter [00900, 05004], lr: 0.010189, loss: 1.4277
2022-07-11 14:52:36 - train: epoch 0096, iter [01000, 05004], lr: 0.010174, loss: 1.3242
2022-07-11 14:53:10 - train: epoch 0096, iter [01100, 05004], lr: 0.010158, loss: 1.3741
2022-07-11 14:53:46 - train: epoch 0096, iter [01200, 05004], lr: 0.010142, loss: 1.2479
2022-07-11 14:54:20 - train: epoch 0096, iter [01300, 05004], lr: 0.010126, loss: 1.3896
2022-07-11 14:54:55 - train: epoch 0096, iter [01400, 05004], lr: 0.010110, loss: 1.2451
2022-07-11 14:55:30 - train: epoch 0096, iter [01500, 05004], lr: 0.010095, loss: 1.4993
2022-07-11 14:56:05 - train: epoch 0096, iter [01600, 05004], lr: 0.010079, loss: 1.1714
2022-07-11 14:56:40 - train: epoch 0096, iter [01700, 05004], lr: 0.010063, loss: 1.1950
2022-07-11 14:57:15 - train: epoch 0096, iter [01800, 05004], lr: 0.010047, loss: 1.2879
2022-07-11 14:57:50 - train: epoch 0096, iter [01900, 05004], lr: 0.010032, loss: 1.3642
2022-07-11 14:58:25 - train: epoch 0096, iter [02000, 05004], lr: 0.010016, loss: 1.2039
2022-07-11 14:58:59 - train: epoch 0096, iter [02100, 05004], lr: 0.010000, loss: 1.2681
2022-07-11 14:59:35 - train: epoch 0096, iter [02200, 05004], lr: 0.009985, loss: 1.0430
2022-07-11 15:00:09 - train: epoch 0096, iter [02300, 05004], lr: 0.009969, loss: 1.3316
2022-07-11 15:00:45 - train: epoch 0096, iter [02400, 05004], lr: 0.009953, loss: 1.4407
2022-07-11 15:01:19 - train: epoch 0096, iter [02500, 05004], lr: 0.009938, loss: 1.5221
2022-07-11 15:01:54 - train: epoch 0096, iter [02600, 05004], lr: 0.009922, loss: 1.4100
2022-07-11 15:02:29 - train: epoch 0096, iter [02700, 05004], lr: 0.009906, loss: 1.5177
2022-07-11 15:03:04 - train: epoch 0096, iter [02800, 05004], lr: 0.009891, loss: 1.4444
2022-07-11 15:03:39 - train: epoch 0096, iter [02900, 05004], lr: 0.009875, loss: 1.4376
2022-07-11 15:04:14 - train: epoch 0096, iter [03000, 05004], lr: 0.009860, loss: 1.3140
2022-07-11 15:04:49 - train: epoch 0096, iter [03100, 05004], lr: 0.009844, loss: 1.3839
2022-07-11 15:05:23 - train: epoch 0096, iter [03200, 05004], lr: 0.009828, loss: 1.4144
2022-07-11 15:05:58 - train: epoch 0096, iter [03300, 05004], lr: 0.009813, loss: 1.4795
2022-07-11 15:06:34 - train: epoch 0096, iter [03400, 05004], lr: 0.009797, loss: 1.2739
2022-07-11 15:07:08 - train: epoch 0096, iter [03500, 05004], lr: 0.009782, loss: 1.2970
2022-07-11 15:07:43 - train: epoch 0096, iter [03600, 05004], lr: 0.009766, loss: 1.1931
2022-07-11 15:08:18 - train: epoch 0096, iter [03700, 05004], lr: 0.009751, loss: 1.4773
2022-07-11 15:08:54 - train: epoch 0096, iter [03800, 05004], lr: 0.009735, loss: 1.1924
2022-07-11 15:09:28 - train: epoch 0096, iter [03900, 05004], lr: 0.009720, loss: 1.1824
2022-07-11 15:10:03 - train: epoch 0096, iter [04000, 05004], lr: 0.009704, loss: 1.4856
2022-07-11 15:10:37 - train: epoch 0096, iter [04100, 05004], lr: 0.009689, loss: 1.3720
2022-07-11 15:11:13 - train: epoch 0096, iter [04200, 05004], lr: 0.009673, loss: 1.1885
2022-07-11 15:11:47 - train: epoch 0096, iter [04300, 05004], lr: 0.009658, loss: 1.0913
2022-07-11 15:12:22 - train: epoch 0096, iter [04400, 05004], lr: 0.009642, loss: 1.3008
2022-07-11 15:12:57 - train: epoch 0096, iter [04500, 05004], lr: 0.009627, loss: 1.4912
2022-07-11 15:13:32 - train: epoch 0096, iter [04600, 05004], lr: 0.009611, loss: 1.3035
2022-07-11 15:14:08 - train: epoch 0096, iter [04700, 05004], lr: 0.009596, loss: 1.5291
2022-07-11 15:14:42 - train: epoch 0096, iter [04800, 05004], lr: 0.009581, loss: 1.3557
2022-07-11 15:15:17 - train: epoch 0096, iter [04900, 05004], lr: 0.009565, loss: 1.4546
2022-07-11 15:15:50 - train: epoch 0096, iter [05000, 05004], lr: 0.009550, loss: 1.4333
2022-07-11 15:15:51 - train: epoch 096, train_loss: 1.3376
2022-07-11 15:17:08 - eval: epoch: 096, acc1: 70.272%, acc5: 89.864%, test_loss: 1.1998, per_image_load_time: 2.166ms, per_image_inference_time: 0.402ms
2022-07-11 15:17:08 - until epoch: 096, best_acc1: 70.272%
2022-07-11 15:17:08 - epoch 097 lr: 0.009549
2022-07-11 15:17:49 - train: epoch 0097, iter [00100, 05004], lr: 0.009534, loss: 1.3474
2022-07-11 15:18:24 - train: epoch 0097, iter [00200, 05004], lr: 0.009518, loss: 1.1471
2022-07-11 15:18:59 - train: epoch 0097, iter [00300, 05004], lr: 0.009503, loss: 1.4026
2022-07-11 15:19:33 - train: epoch 0097, iter [00400, 05004], lr: 0.009488, loss: 1.4067
2022-07-11 15:20:07 - train: epoch 0097, iter [00500, 05004], lr: 0.009472, loss: 1.4385
2022-07-11 15:20:41 - train: epoch 0097, iter [00600, 05004], lr: 0.009457, loss: 1.2429
2022-07-11 15:21:15 - train: epoch 0097, iter [00700, 05004], lr: 0.009442, loss: 1.2807
2022-07-11 15:21:49 - train: epoch 0097, iter [00800, 05004], lr: 0.009426, loss: 1.3266
2022-07-11 15:22:24 - train: epoch 0097, iter [00900, 05004], lr: 0.009411, loss: 1.3502
2022-07-11 15:22:59 - train: epoch 0097, iter [01000, 05004], lr: 0.009396, loss: 1.2675
2022-07-11 15:23:33 - train: epoch 0097, iter [01100, 05004], lr: 0.009381, loss: 1.0874
2022-07-11 15:24:07 - train: epoch 0097, iter [01200, 05004], lr: 0.009365, loss: 1.4708
2022-07-11 15:24:42 - train: epoch 0097, iter [01300, 05004], lr: 0.009350, loss: 1.3433
2022-07-11 15:25:16 - train: epoch 0097, iter [01400, 05004], lr: 0.009335, loss: 1.2339
2022-07-11 15:25:51 - train: epoch 0097, iter [01500, 05004], lr: 0.009320, loss: 1.4080
2022-07-11 15:26:26 - train: epoch 0097, iter [01600, 05004], lr: 0.009305, loss: 1.2022
2022-07-11 15:27:02 - train: epoch 0097, iter [01700, 05004], lr: 0.009289, loss: 1.2098
2022-07-11 15:27:36 - train: epoch 0097, iter [01800, 05004], lr: 0.009274, loss: 1.4544
2022-07-11 15:28:11 - train: epoch 0097, iter [01900, 05004], lr: 0.009259, loss: 1.2783
2022-07-11 15:28:46 - train: epoch 0097, iter [02000, 05004], lr: 0.009244, loss: 1.2056
2022-07-11 15:29:21 - train: epoch 0097, iter [02100, 05004], lr: 0.009229, loss: 1.3027
2022-07-11 15:29:55 - train: epoch 0097, iter [02200, 05004], lr: 0.009214, loss: 1.3410
2022-07-11 15:30:31 - train: epoch 0097, iter [02300, 05004], lr: 0.009198, loss: 0.9902
2022-07-11 15:31:05 - train: epoch 0097, iter [02400, 05004], lr: 0.009183, loss: 1.3318
2022-07-11 15:31:40 - train: epoch 0097, iter [02500, 05004], lr: 0.009168, loss: 1.1820
2022-07-11 15:32:15 - train: epoch 0097, iter [02600, 05004], lr: 0.009153, loss: 1.2491
2022-07-11 15:32:50 - train: epoch 0097, iter [02700, 05004], lr: 0.009138, loss: 1.1781
2022-07-11 15:33:25 - train: epoch 0097, iter [02800, 05004], lr: 0.009123, loss: 1.2259
2022-07-11 15:34:00 - train: epoch 0097, iter [02900, 05004], lr: 0.009108, loss: 1.4352
2022-07-11 15:34:35 - train: epoch 0097, iter [03000, 05004], lr: 0.009093, loss: 1.5067
2022-07-11 15:35:09 - train: epoch 0097, iter [03100, 05004], lr: 0.009078, loss: 1.0926
2022-07-11 15:35:45 - train: epoch 0097, iter [03200, 05004], lr: 0.009063, loss: 1.6112
2022-07-11 15:36:20 - train: epoch 0097, iter [03300, 05004], lr: 0.009048, loss: 1.4720
2022-07-11 15:36:54 - train: epoch 0097, iter [03400, 05004], lr: 0.009033, loss: 1.3767
2022-07-11 15:37:29 - train: epoch 0097, iter [03500, 05004], lr: 0.009018, loss: 1.3308
2022-07-11 15:38:05 - train: epoch 0097, iter [03600, 05004], lr: 0.009003, loss: 1.5433
2022-07-11 15:38:40 - train: epoch 0097, iter [03700, 05004], lr: 0.008988, loss: 1.0891
2022-07-11 15:39:14 - train: epoch 0097, iter [03800, 05004], lr: 0.008973, loss: 1.2058
2022-07-11 15:39:48 - train: epoch 0097, iter [03900, 05004], lr: 0.008958, loss: 1.3081
2022-07-11 15:40:23 - train: epoch 0097, iter [04000, 05004], lr: 0.008943, loss: 1.4241
2022-07-11 15:40:59 - train: epoch 0097, iter [04100, 05004], lr: 0.008928, loss: 1.2420
2022-07-11 15:41:33 - train: epoch 0097, iter [04200, 05004], lr: 0.008913, loss: 1.4030
2022-07-11 15:42:08 - train: epoch 0097, iter [04300, 05004], lr: 0.008898, loss: 1.3404
2022-07-11 15:42:43 - train: epoch 0097, iter [04400, 05004], lr: 0.008883, loss: 1.4102
2022-07-11 15:43:18 - train: epoch 0097, iter [04500, 05004], lr: 0.008869, loss: 1.3184
2022-07-11 15:43:53 - train: epoch 0097, iter [04600, 05004], lr: 0.008854, loss: 1.3590
2022-07-11 15:44:27 - train: epoch 0097, iter [04700, 05004], lr: 0.008839, loss: 1.3181
2022-07-11 15:45:03 - train: epoch 0097, iter [04800, 05004], lr: 0.008824, loss: 1.2783
2022-07-11 15:45:38 - train: epoch 0097, iter [04900, 05004], lr: 0.008809, loss: 1.4817
2022-07-11 15:46:11 - train: epoch 0097, iter [05000, 05004], lr: 0.008794, loss: 1.1625
2022-07-11 15:46:12 - train: epoch 097, train_loss: 1.3196
2022-07-11 15:47:30 - eval: epoch: 097, acc1: 70.900%, acc5: 90.146%, test_loss: 1.1765, per_image_load_time: 2.637ms, per_image_inference_time: 0.369ms
2022-07-11 15:47:30 - until epoch: 097, best_acc1: 70.900%
2022-07-11 15:47:30 - epoch 098 lr: 0.008794
2022-07-11 15:48:11 - train: epoch 0098, iter [00100, 05004], lr: 0.008779, loss: 1.3127
2022-07-11 15:48:45 - train: epoch 0098, iter [00200, 05004], lr: 0.008764, loss: 1.3666
2022-07-11 15:49:19 - train: epoch 0098, iter [00300, 05004], lr: 0.008749, loss: 1.2913
2022-07-11 15:49:53 - train: epoch 0098, iter [00400, 05004], lr: 0.008735, loss: 1.2651
2022-07-11 15:50:28 - train: epoch 0098, iter [00500, 05004], lr: 0.008720, loss: 1.3539
2022-07-11 15:51:02 - train: epoch 0098, iter [00600, 05004], lr: 0.008705, loss: 1.3153
2022-07-11 15:51:37 - train: epoch 0098, iter [00700, 05004], lr: 0.008690, loss: 1.3244
2022-07-11 15:52:12 - train: epoch 0098, iter [00800, 05004], lr: 0.008676, loss: 1.3158
2022-07-11 15:52:47 - train: epoch 0098, iter [00900, 05004], lr: 0.008661, loss: 1.2184
2022-07-11 15:53:21 - train: epoch 0098, iter [01000, 05004], lr: 0.008646, loss: 1.3040
2022-07-11 15:53:55 - train: epoch 0098, iter [01100, 05004], lr: 0.008631, loss: 1.4844
2022-07-11 15:54:30 - train: epoch 0098, iter [01200, 05004], lr: 0.008617, loss: 1.2806
2022-07-11 15:55:04 - train: epoch 0098, iter [01300, 05004], lr: 0.008602, loss: 1.1834
2022-07-11 15:55:38 - train: epoch 0098, iter [01400, 05004], lr: 0.008587, loss: 1.3997
2022-07-11 15:56:12 - train: epoch 0098, iter [01500, 05004], lr: 0.008573, loss: 1.3997
2022-07-11 15:56:47 - train: epoch 0098, iter [01600, 05004], lr: 0.008558, loss: 1.3098
2022-07-11 15:57:22 - train: epoch 0098, iter [01700, 05004], lr: 0.008543, loss: 1.2813
2022-07-11 15:57:56 - train: epoch 0098, iter [01800, 05004], lr: 0.008529, loss: 1.3349
2022-07-11 15:58:30 - train: epoch 0098, iter [01900, 05004], lr: 0.008514, loss: 1.0424
2022-07-11 15:59:04 - train: epoch 0098, iter [02000, 05004], lr: 0.008500, loss: 1.2878
2022-07-11 15:59:39 - train: epoch 0098, iter [02100, 05004], lr: 0.008485, loss: 1.2829
2022-07-11 16:00:14 - train: epoch 0098, iter [02200, 05004], lr: 0.008470, loss: 1.2358
2022-07-11 16:00:48 - train: epoch 0098, iter [02300, 05004], lr: 0.008456, loss: 1.0275
2022-07-11 16:01:22 - train: epoch 0098, iter [02400, 05004], lr: 0.008441, loss: 1.3533
2022-07-11 16:01:57 - train: epoch 0098, iter [02500, 05004], lr: 0.008427, loss: 1.2188
2022-07-11 16:02:31 - train: epoch 0098, iter [02600, 05004], lr: 0.008412, loss: 1.2754
2022-07-11 16:03:05 - train: epoch 0098, iter [02700, 05004], lr: 0.008398, loss: 1.2915
2022-07-11 16:03:39 - train: epoch 0098, iter [02800, 05004], lr: 0.008383, loss: 1.2907
2022-07-11 16:04:14 - train: epoch 0098, iter [02900, 05004], lr: 0.008369, loss: 1.3165
2022-07-11 16:04:49 - train: epoch 0098, iter [03000, 05004], lr: 0.008354, loss: 1.2655
2022-07-11 16:05:22 - train: epoch 0098, iter [03100, 05004], lr: 0.008340, loss: 1.3059
2022-07-11 16:05:57 - train: epoch 0098, iter [03200, 05004], lr: 0.008325, loss: 1.1716
2022-07-11 16:06:31 - train: epoch 0098, iter [03300, 05004], lr: 0.008311, loss: 1.2388
2022-07-11 16:07:05 - train: epoch 0098, iter [03400, 05004], lr: 0.008296, loss: 1.3008
2022-07-11 16:07:40 - train: epoch 0098, iter [03500, 05004], lr: 0.008282, loss: 1.5089
2022-07-11 16:08:14 - train: epoch 0098, iter [03600, 05004], lr: 0.008268, loss: 1.4968
2022-07-11 16:08:49 - train: epoch 0098, iter [03700, 05004], lr: 0.008253, loss: 1.3949
2022-07-11 16:09:23 - train: epoch 0098, iter [03800, 05004], lr: 0.008239, loss: 1.2738
2022-07-11 16:09:57 - train: epoch 0098, iter [03900, 05004], lr: 0.008224, loss: 1.1745
2022-07-11 16:10:31 - train: epoch 0098, iter [04000, 05004], lr: 0.008210, loss: 1.5098
2022-07-11 16:11:05 - train: epoch 0098, iter [04100, 05004], lr: 0.008196, loss: 1.2951
2022-07-11 16:11:39 - train: epoch 0098, iter [04200, 05004], lr: 0.008181, loss: 1.3548
2022-07-11 16:12:13 - train: epoch 0098, iter [04300, 05004], lr: 0.008167, loss: 1.2896
2022-07-11 16:12:47 - train: epoch 0098, iter [04400, 05004], lr: 0.008153, loss: 1.6011
2022-07-11 16:13:20 - train: epoch 0098, iter [04500, 05004], lr: 0.008138, loss: 1.3504
2022-07-11 16:13:54 - train: epoch 0098, iter [04600, 05004], lr: 0.008124, loss: 1.4369
2022-07-11 16:14:28 - train: epoch 0098, iter [04700, 05004], lr: 0.008110, loss: 1.3676
2022-07-11 16:15:02 - train: epoch 0098, iter [04800, 05004], lr: 0.008096, loss: 1.1910
2022-07-11 16:15:36 - train: epoch 0098, iter [04900, 05004], lr: 0.008081, loss: 1.3228
2022-07-11 16:16:08 - train: epoch 0098, iter [05000, 05004], lr: 0.008067, loss: 1.0868
2022-07-11 16:16:09 - train: epoch 098, train_loss: 1.3013
2022-07-11 16:17:25 - eval: epoch: 098, acc1: 71.072%, acc5: 90.196%, test_loss: 1.1683, per_image_load_time: 2.569ms, per_image_inference_time: 0.363ms
2022-07-11 16:17:25 - until epoch: 098, best_acc1: 71.072%
2022-07-11 16:17:25 - epoch 099 lr: 0.008066
2022-07-11 16:18:04 - train: epoch 0099, iter [00100, 05004], lr: 0.008052, loss: 1.2573
2022-07-11 16:18:38 - train: epoch 0099, iter [00200, 05004], lr: 0.008038, loss: 1.1351
2022-07-11 16:19:12 - train: epoch 0099, iter [00300, 05004], lr: 0.008024, loss: 1.1950
2022-07-11 16:19:46 - train: epoch 0099, iter [00400, 05004], lr: 0.008010, loss: 1.2527
2022-07-11 16:20:19 - train: epoch 0099, iter [00500, 05004], lr: 0.007995, loss: 1.2298
2022-07-11 16:20:52 - train: epoch 0099, iter [00600, 05004], lr: 0.007981, loss: 1.3193
2022-07-11 16:21:26 - train: epoch 0099, iter [00700, 05004], lr: 0.007967, loss: 1.3481
2022-07-11 16:22:00 - train: epoch 0099, iter [00800, 05004], lr: 0.007953, loss: 1.3258
2022-07-11 16:22:33 - train: epoch 0099, iter [00900, 05004], lr: 0.007939, loss: 1.2777
2022-07-11 16:23:07 - train: epoch 0099, iter [01000, 05004], lr: 0.007925, loss: 1.1122
2022-07-11 16:23:41 - train: epoch 0099, iter [01100, 05004], lr: 0.007910, loss: 1.3686
2022-07-11 16:24:15 - train: epoch 0099, iter [01200, 05004], lr: 0.007896, loss: 1.0230
2022-07-11 16:24:49 - train: epoch 0099, iter [01300, 05004], lr: 0.007882, loss: 1.2064
2022-07-11 16:25:23 - train: epoch 0099, iter [01400, 05004], lr: 0.007868, loss: 1.1900
2022-07-11 16:25:56 - train: epoch 0099, iter [01500, 05004], lr: 0.007854, loss: 1.2323
2022-07-11 16:26:30 - train: epoch 0099, iter [01600, 05004], lr: 0.007840, loss: 1.3214
2022-07-11 16:27:04 - train: epoch 0099, iter [01700, 05004], lr: 0.007826, loss: 1.2981
2022-07-11 16:27:38 - train: epoch 0099, iter [01800, 05004], lr: 0.007812, loss: 1.2721
2022-07-11 16:28:11 - train: epoch 0099, iter [01900, 05004], lr: 0.007798, loss: 1.4467
2022-07-11 16:28:46 - train: epoch 0099, iter [02000, 05004], lr: 0.007784, loss: 1.2399
2022-07-11 16:29:19 - train: epoch 0099, iter [02100, 05004], lr: 0.007770, loss: 1.2746
2022-07-11 16:29:53 - train: epoch 0099, iter [02200, 05004], lr: 0.007756, loss: 1.0918
2022-07-11 16:30:27 - train: epoch 0099, iter [02300, 05004], lr: 0.007742, loss: 1.2703
2022-07-11 16:31:01 - train: epoch 0099, iter [02400, 05004], lr: 0.007728, loss: 1.5587
2022-07-11 16:31:34 - train: epoch 0099, iter [02500, 05004], lr: 0.007714, loss: 1.2240
2022-07-11 16:32:08 - train: epoch 0099, iter [02600, 05004], lr: 0.007700, loss: 1.1796
2022-07-11 16:32:43 - train: epoch 0099, iter [02700, 05004], lr: 0.007686, loss: 1.2151
2022-07-11 16:33:18 - train: epoch 0099, iter [02800, 05004], lr: 0.007672, loss: 1.4351
2022-07-11 16:33:52 - train: epoch 0099, iter [02900, 05004], lr: 0.007658, loss: 1.3904
2022-07-11 16:34:26 - train: epoch 0099, iter [03000, 05004], lr: 0.007644, loss: 1.4822
2022-07-11 16:35:02 - train: epoch 0099, iter [03100, 05004], lr: 0.007630, loss: 1.1897
2022-07-11 16:35:37 - train: epoch 0099, iter [03200, 05004], lr: 0.007616, loss: 1.3409
2022-07-11 16:36:11 - train: epoch 0099, iter [03300, 05004], lr: 0.007603, loss: 1.2565
2022-07-11 16:36:47 - train: epoch 0099, iter [03400, 05004], lr: 0.007589, loss: 1.3414
2022-07-11 16:37:22 - train: epoch 0099, iter [03500, 05004], lr: 0.007575, loss: 1.3126
2022-07-11 16:37:57 - train: epoch 0099, iter [03600, 05004], lr: 0.007561, loss: 1.3937
2022-07-11 16:38:31 - train: epoch 0099, iter [03700, 05004], lr: 0.007547, loss: 1.1957
2022-07-11 16:39:06 - train: epoch 0099, iter [03800, 05004], lr: 0.007533, loss: 1.2976
2022-07-11 16:39:41 - train: epoch 0099, iter [03900, 05004], lr: 0.007520, loss: 1.2966
2022-07-11 16:40:16 - train: epoch 0099, iter [04000, 05004], lr: 0.007506, loss: 1.3825
2022-07-11 16:40:51 - train: epoch 0099, iter [04100, 05004], lr: 0.007492, loss: 1.3264
2022-07-11 16:41:26 - train: epoch 0099, iter [04200, 05004], lr: 0.007478, loss: 1.4538
2022-07-11 16:42:01 - train: epoch 0099, iter [04300, 05004], lr: 0.007465, loss: 1.3999
2022-07-11 16:42:36 - train: epoch 0099, iter [04400, 05004], lr: 0.007451, loss: 1.3927
2022-07-11 16:43:11 - train: epoch 0099, iter [04500, 05004], lr: 0.007437, loss: 1.5527
2022-07-11 16:43:46 - train: epoch 0099, iter [04600, 05004], lr: 0.007423, loss: 1.2005
2022-07-11 16:44:21 - train: epoch 0099, iter [04700, 05004], lr: 0.007410, loss: 1.3104
2022-07-11 16:44:56 - train: epoch 0099, iter [04800, 05004], lr: 0.007396, loss: 1.2690
2022-07-11 16:45:31 - train: epoch 0099, iter [04900, 05004], lr: 0.007382, loss: 1.3403
2022-07-11 16:46:05 - train: epoch 0099, iter [05000, 05004], lr: 0.007369, loss: 1.4516
2022-07-11 16:46:06 - train: epoch 099, train_loss: 1.2822
2022-07-11 16:47:24 - eval: epoch: 099, acc1: 71.396%, acc5: 90.164%, test_loss: 1.1605, per_image_load_time: 1.631ms, per_image_inference_time: 0.414ms
2022-07-11 16:47:24 - until epoch: 099, best_acc1: 71.396%
2022-07-11 16:47:24 - epoch 100 lr: 0.007368
2022-07-11 16:48:04 - train: epoch 0100, iter [00100, 05004], lr: 0.007354, loss: 1.2782
2022-07-11 16:48:39 - train: epoch 0100, iter [00200, 05004], lr: 0.007341, loss: 1.2929
2022-07-11 16:49:13 - train: epoch 0100, iter [00300, 05004], lr: 0.007327, loss: 1.2448
2022-07-11 16:49:47 - train: epoch 0100, iter [00400, 05004], lr: 0.007313, loss: 1.2529
2022-07-11 16:50:22 - train: epoch 0100, iter [00500, 05004], lr: 0.007300, loss: 1.3087
2022-07-11 16:50:56 - train: epoch 0100, iter [00600, 05004], lr: 0.007286, loss: 1.2385
2022-07-11 16:51:32 - train: epoch 0100, iter [00700, 05004], lr: 0.007273, loss: 1.2141
2022-07-11 16:52:07 - train: epoch 0100, iter [00800, 05004], lr: 0.007259, loss: 1.2560
2022-07-11 16:52:41 - train: epoch 0100, iter [00900, 05004], lr: 0.007245, loss: 1.2072
2022-07-11 16:53:15 - train: epoch 0100, iter [01000, 05004], lr: 0.007232, loss: 1.0626
2022-07-11 16:53:50 - train: epoch 0100, iter [01100, 05004], lr: 0.007218, loss: 1.1551
2022-07-11 16:54:25 - train: epoch 0100, iter [01200, 05004], lr: 0.007205, loss: 1.2893
2022-07-11 16:55:00 - train: epoch 0100, iter [01300, 05004], lr: 0.007191, loss: 1.3282
2022-07-11 16:55:35 - train: epoch 0100, iter [01400, 05004], lr: 0.007178, loss: 1.4407
2022-07-11 16:56:11 - train: epoch 0100, iter [01500, 05004], lr: 0.007164, loss: 1.4733
2022-07-11 16:56:45 - train: epoch 0100, iter [01600, 05004], lr: 0.007151, loss: 1.1687
2022-07-11 16:57:21 - train: epoch 0100, iter [01700, 05004], lr: 0.007137, loss: 1.0683
2022-07-11 16:57:55 - train: epoch 0100, iter [01800, 05004], lr: 0.007124, loss: 1.1229
2022-07-11 16:58:30 - train: epoch 0100, iter [01900, 05004], lr: 0.007110, loss: 1.2869
2022-07-11 16:59:06 - train: epoch 0100, iter [02000, 05004], lr: 0.007097, loss: 1.3689
2022-07-11 16:59:40 - train: epoch 0100, iter [02100, 05004], lr: 0.007084, loss: 1.2955
2022-07-11 17:00:16 - train: epoch 0100, iter [02200, 05004], lr: 0.007070, loss: 1.3352
2022-07-11 17:00:51 - train: epoch 0100, iter [02300, 05004], lr: 0.007057, loss: 1.1354
2022-07-11 17:01:26 - train: epoch 0100, iter [02400, 05004], lr: 0.007043, loss: 1.2897
2022-07-11 17:02:01 - train: epoch 0100, iter [02500, 05004], lr: 0.007030, loss: 1.0323
2022-07-11 17:02:35 - train: epoch 0100, iter [02600, 05004], lr: 0.007017, loss: 1.2658
2022-07-11 17:03:11 - train: epoch 0100, iter [02700, 05004], lr: 0.007003, loss: 1.1672
2022-07-11 17:03:45 - train: epoch 0100, iter [02800, 05004], lr: 0.006990, loss: 1.3182
2022-07-11 17:04:20 - train: epoch 0100, iter [02900, 05004], lr: 0.006977, loss: 1.4438
2022-07-11 17:04:54 - train: epoch 0100, iter [03000, 05004], lr: 0.006963, loss: 1.0906
2022-07-11 17:05:30 - train: epoch 0100, iter [03100, 05004], lr: 0.006950, loss: 1.1156
2022-07-11 17:06:04 - train: epoch 0100, iter [03200, 05004], lr: 0.006937, loss: 1.3933
2022-07-11 17:06:40 - train: epoch 0100, iter [03300, 05004], lr: 0.006923, loss: 1.2059
2022-07-11 17:07:15 - train: epoch 0100, iter [03400, 05004], lr: 0.006910, loss: 1.2494
2022-07-11 17:07:49 - train: epoch 0100, iter [03500, 05004], lr: 0.006897, loss: 1.1940
2022-07-11 17:08:24 - train: epoch 0100, iter [03600, 05004], lr: 0.006884, loss: 1.3419
2022-07-11 17:08:58 - train: epoch 0100, iter [03700, 05004], lr: 0.006870, loss: 1.1928
2022-07-11 17:09:34 - train: epoch 0100, iter [03800, 05004], lr: 0.006857, loss: 1.2429
2022-07-11 17:10:07 - train: epoch 0100, iter [03900, 05004], lr: 0.006844, loss: 1.3648
2022-07-11 17:10:43 - train: epoch 0100, iter [04000, 05004], lr: 0.006831, loss: 1.1579
2022-07-11 17:11:18 - train: epoch 0100, iter [04100, 05004], lr: 0.006817, loss: 1.3303
2022-07-11 17:11:53 - train: epoch 0100, iter [04200, 05004], lr: 0.006804, loss: 1.2938
2022-07-11 17:12:27 - train: epoch 0100, iter [04300, 05004], lr: 0.006791, loss: 1.3283
2022-07-11 17:13:03 - train: epoch 0100, iter [04400, 05004], lr: 0.006778, loss: 1.4274
2022-07-11 17:13:38 - train: epoch 0100, iter [04500, 05004], lr: 0.006765, loss: 1.1383
2022-07-11 17:14:13 - train: epoch 0100, iter [04600, 05004], lr: 0.006752, loss: 1.2016
2022-07-11 17:14:47 - train: epoch 0100, iter [04700, 05004], lr: 0.006739, loss: 1.3835
2022-07-11 17:15:22 - train: epoch 0100, iter [04800, 05004], lr: 0.006725, loss: 1.2639
2022-07-11 17:15:57 - train: epoch 0100, iter [04900, 05004], lr: 0.006712, loss: 1.2700
2022-07-11 17:16:30 - train: epoch 0100, iter [05000, 05004], lr: 0.006699, loss: 1.3526
2022-07-11 17:16:31 - train: epoch 100, train_loss: 1.2649
2022-07-11 17:17:48 - eval: epoch: 100, acc1: 71.760%, acc5: 90.482%, test_loss: 1.1424, per_image_load_time: 1.552ms, per_image_inference_time: 0.404ms
2022-07-11 17:17:48 - until epoch: 100, best_acc1: 71.760%
2022-07-11 17:17:48 - epoch 101 lr: 0.006699
2022-07-11 17:18:28 - train: epoch 0101, iter [00100, 05004], lr: 0.006686, loss: 1.1100
2022-07-11 17:19:03 - train: epoch 0101, iter [00200, 05004], lr: 0.006673, loss: 1.2634
2022-07-11 17:19:37 - train: epoch 0101, iter [00300, 05004], lr: 0.006660, loss: 0.9560
2022-07-11 17:20:12 - train: epoch 0101, iter [00400, 05004], lr: 0.006647, loss: 1.1197
2022-07-11 17:20:46 - train: epoch 0101, iter [00500, 05004], lr: 0.006633, loss: 1.3250
2022-07-11 17:21:22 - train: epoch 0101, iter [00600, 05004], lr: 0.006620, loss: 1.3328
2022-07-11 17:21:57 - train: epoch 0101, iter [00700, 05004], lr: 0.006607, loss: 1.5016
2022-07-11 17:22:31 - train: epoch 0101, iter [00800, 05004], lr: 0.006594, loss: 1.0919
2022-07-11 17:23:05 - train: epoch 0101, iter [00900, 05004], lr: 0.006581, loss: 1.2799
2022-07-11 17:23:39 - train: epoch 0101, iter [01000, 05004], lr: 0.006569, loss: 1.3508
2022-07-11 17:24:13 - train: epoch 0101, iter [01100, 05004], lr: 0.006556, loss: 1.4837
2022-07-11 17:24:47 - train: epoch 0101, iter [01200, 05004], lr: 0.006543, loss: 1.3540
2022-07-11 17:25:22 - train: epoch 0101, iter [01300, 05004], lr: 0.006530, loss: 1.2830
2022-07-11 17:25:56 - train: epoch 0101, iter [01400, 05004], lr: 0.006517, loss: 1.2945
2022-07-11 17:26:31 - train: epoch 0101, iter [01500, 05004], lr: 0.006504, loss: 1.3240
2022-07-11 17:27:06 - train: epoch 0101, iter [01600, 05004], lr: 0.006491, loss: 1.1831
2022-07-11 17:27:40 - train: epoch 0101, iter [01700, 05004], lr: 0.006478, loss: 1.3397
2022-07-11 17:28:15 - train: epoch 0101, iter [01800, 05004], lr: 0.006465, loss: 1.3000
2022-07-11 17:28:49 - train: epoch 0101, iter [01900, 05004], lr: 0.006452, loss: 1.2154
2022-07-11 17:29:24 - train: epoch 0101, iter [02000, 05004], lr: 0.006440, loss: 1.3552
2022-07-11 17:29:59 - train: epoch 0101, iter [02100, 05004], lr: 0.006427, loss: 1.1516
2022-07-11 17:30:33 - train: epoch 0101, iter [02200, 05004], lr: 0.006414, loss: 1.0496
2022-07-11 17:31:09 - train: epoch 0101, iter [02300, 05004], lr: 0.006401, loss: 1.1176
2022-07-11 17:31:43 - train: epoch 0101, iter [02400, 05004], lr: 0.006388, loss: 1.1049
2022-07-11 17:32:18 - train: epoch 0101, iter [02500, 05004], lr: 0.006375, loss: 1.3784
2022-07-11 17:32:53 - train: epoch 0101, iter [02600, 05004], lr: 0.006363, loss: 1.3051
2022-07-11 17:33:27 - train: epoch 0101, iter [02700, 05004], lr: 0.006350, loss: 1.3852
2022-07-11 17:34:01 - train: epoch 0101, iter [02800, 05004], lr: 0.006337, loss: 1.1922
2022-07-11 17:34:35 - train: epoch 0101, iter [02900, 05004], lr: 0.006324, loss: 1.2961
2022-07-11 17:35:10 - train: epoch 0101, iter [03000, 05004], lr: 0.006312, loss: 1.4473
2022-07-11 17:35:45 - train: epoch 0101, iter [03100, 05004], lr: 0.006299, loss: 1.3415
2022-07-11 17:36:19 - train: epoch 0101, iter [03200, 05004], lr: 0.006286, loss: 1.4053
2022-07-11 17:36:53 - train: epoch 0101, iter [03300, 05004], lr: 0.006274, loss: 1.3336
2022-07-11 17:37:28 - train: epoch 0101, iter [03400, 05004], lr: 0.006261, loss: 1.2102
2022-07-11 17:38:02 - train: epoch 0101, iter [03500, 05004], lr: 0.006248, loss: 1.2816
2022-07-11 17:38:36 - train: epoch 0101, iter [03600, 05004], lr: 0.006236, loss: 1.0827
2022-07-11 17:39:11 - train: epoch 0101, iter [03700, 05004], lr: 0.006223, loss: 1.3816
2022-07-11 17:39:45 - train: epoch 0101, iter [03800, 05004], lr: 0.006210, loss: 1.3076
2022-07-11 17:40:20 - train: epoch 0101, iter [03900, 05004], lr: 0.006198, loss: 1.3017
2022-07-11 17:40:54 - train: epoch 0101, iter [04000, 05004], lr: 0.006185, loss: 1.2878
2022-07-11 17:41:29 - train: epoch 0101, iter [04100, 05004], lr: 0.006172, loss: 1.0596
2022-07-11 17:42:04 - train: epoch 0101, iter [04200, 05004], lr: 0.006160, loss: 1.1706
2022-07-11 17:42:39 - train: epoch 0101, iter [04300, 05004], lr: 0.006147, loss: 1.0140
2022-07-11 17:43:14 - train: epoch 0101, iter [04400, 05004], lr: 0.006135, loss: 1.2707
2022-07-11 17:43:49 - train: epoch 0101, iter [04500, 05004], lr: 0.006122, loss: 1.5724
2022-07-11 17:44:23 - train: epoch 0101, iter [04600, 05004], lr: 0.006110, loss: 1.3024
2022-07-11 17:44:58 - train: epoch 0101, iter [04700, 05004], lr: 0.006097, loss: 1.2785
2022-07-11 17:45:33 - train: epoch 0101, iter [04800, 05004], lr: 0.006085, loss: 0.9950
2022-07-11 17:46:07 - train: epoch 0101, iter [04900, 05004], lr: 0.006072, loss: 1.3308
2022-07-11 17:46:40 - train: epoch 0101, iter [05000, 05004], lr: 0.006060, loss: 1.2501
2022-07-11 17:46:41 - train: epoch 101, train_loss: 1.2487
2022-07-11 17:47:58 - eval: epoch: 101, acc1: 71.636%, acc5: 90.522%, test_loss: 1.1448, per_image_load_time: 2.214ms, per_image_inference_time: 0.361ms
2022-07-11 17:47:58 - until epoch: 101, best_acc1: 71.760%
2022-07-11 17:47:58 - epoch 102 lr: 0.006059
2022-07-11 17:48:39 - train: epoch 0102, iter [00100, 05004], lr: 0.006047, loss: 1.1565
2022-07-11 17:49:14 - train: epoch 0102, iter [00200, 05004], lr: 0.006034, loss: 1.2256
2022-07-11 17:49:48 - train: epoch 0102, iter [00300, 05004], lr: 0.006022, loss: 1.3141
2022-07-11 17:50:23 - train: epoch 0102, iter [00400, 05004], lr: 0.006009, loss: 1.1389
2022-07-11 17:50:57 - train: epoch 0102, iter [00500, 05004], lr: 0.005997, loss: 1.1103
2022-07-11 17:51:32 - train: epoch 0102, iter [00600, 05004], lr: 0.005984, loss: 1.1264
2022-07-11 17:52:07 - train: epoch 0102, iter [00700, 05004], lr: 0.005972, loss: 1.2947
2022-07-11 17:52:42 - train: epoch 0102, iter [00800, 05004], lr: 0.005960, loss: 1.0944
2022-07-11 17:53:16 - train: epoch 0102, iter [00900, 05004], lr: 0.005947, loss: 1.2476
2022-07-11 17:53:52 - train: epoch 0102, iter [01000, 05004], lr: 0.005935, loss: 1.1827
2022-07-11 17:54:27 - train: epoch 0102, iter [01100, 05004], lr: 0.005923, loss: 1.1610
2022-07-11 17:55:01 - train: epoch 0102, iter [01200, 05004], lr: 0.005910, loss: 1.3769
2022-07-11 17:55:35 - train: epoch 0102, iter [01300, 05004], lr: 0.005898, loss: 1.0824
2022-07-11 17:56:09 - train: epoch 0102, iter [01400, 05004], lr: 0.005886, loss: 1.2403
2022-07-11 17:56:45 - train: epoch 0102, iter [01500, 05004], lr: 0.005873, loss: 1.1217
2022-07-11 17:57:19 - train: epoch 0102, iter [01600, 05004], lr: 0.005861, loss: 1.4297
2022-07-11 17:57:53 - train: epoch 0102, iter [01700, 05004], lr: 0.005849, loss: 1.4253
2022-07-11 17:58:28 - train: epoch 0102, iter [01800, 05004], lr: 0.005836, loss: 1.3458
2022-07-11 17:59:02 - train: epoch 0102, iter [01900, 05004], lr: 0.005824, loss: 1.1544
2022-07-11 17:59:37 - train: epoch 0102, iter [02000, 05004], lr: 0.005812, loss: 1.1873
2022-07-11 18:00:11 - train: epoch 0102, iter [02100, 05004], lr: 0.005800, loss: 1.1290
2022-07-11 18:00:46 - train: epoch 0102, iter [02200, 05004], lr: 0.005787, loss: 1.2442
2022-07-11 18:01:22 - train: epoch 0102, iter [02300, 05004], lr: 0.005775, loss: 1.2657
2022-07-11 18:01:56 - train: epoch 0102, iter [02400, 05004], lr: 0.005763, loss: 1.2341
2022-07-11 18:02:32 - train: epoch 0102, iter [02500, 05004], lr: 0.005751, loss: 1.0818
2022-07-11 18:03:07 - train: epoch 0102, iter [02600, 05004], lr: 0.005739, loss: 1.3891
2022-07-11 18:03:42 - train: epoch 0102, iter [02700, 05004], lr: 0.005727, loss: 1.3128
2022-07-11 18:04:17 - train: epoch 0102, iter [02800, 05004], lr: 0.005714, loss: 1.2480
2022-07-11 18:04:51 - train: epoch 0102, iter [02900, 05004], lr: 0.005702, loss: 1.2426
2022-07-11 18:05:27 - train: epoch 0102, iter [03000, 05004], lr: 0.005690, loss: 1.2797
2022-07-11 18:06:02 - train: epoch 0102, iter [03100, 05004], lr: 0.005678, loss: 1.2471
2022-07-11 18:06:37 - train: epoch 0102, iter [03200, 05004], lr: 0.005666, loss: 1.1845
2022-07-11 18:07:12 - train: epoch 0102, iter [03300, 05004], lr: 0.005654, loss: 1.3921
2022-07-11 18:07:47 - train: epoch 0102, iter [03400, 05004], lr: 0.005642, loss: 1.1373
2022-07-11 18:08:22 - train: epoch 0102, iter [03500, 05004], lr: 0.005630, loss: 1.1775
2022-07-11 18:08:56 - train: epoch 0102, iter [03600, 05004], lr: 0.005618, loss: 1.2291
2022-07-11 18:09:31 - train: epoch 0102, iter [03700, 05004], lr: 0.005606, loss: 1.3394
2022-07-11 18:10:06 - train: epoch 0102, iter [03800, 05004], lr: 0.005594, loss: 1.1394
2022-07-11 18:10:41 - train: epoch 0102, iter [03900, 05004], lr: 0.005582, loss: 1.2448
2022-07-11 18:11:16 - train: epoch 0102, iter [04000, 05004], lr: 0.005570, loss: 1.1123
2022-07-11 18:11:51 - train: epoch 0102, iter [04100, 05004], lr: 0.005558, loss: 1.3270
2022-07-11 18:12:26 - train: epoch 0102, iter [04200, 05004], lr: 0.005546, loss: 1.4596
2022-07-11 18:13:01 - train: epoch 0102, iter [04300, 05004], lr: 0.005534, loss: 1.1599
2022-07-11 18:13:35 - train: epoch 0102, iter [04400, 05004], lr: 0.005522, loss: 1.2013
2022-07-11 18:14:10 - train: epoch 0102, iter [04500, 05004], lr: 0.005510, loss: 1.3752
2022-07-11 18:14:44 - train: epoch 0102, iter [04600, 05004], lr: 0.005498, loss: 1.2290
2022-07-11 18:15:20 - train: epoch 0102, iter [04700, 05004], lr: 0.005486, loss: 1.2794
2022-07-11 18:15:55 - train: epoch 0102, iter [04800, 05004], lr: 0.005474, loss: 1.2723
2022-07-11 18:16:29 - train: epoch 0102, iter [04900, 05004], lr: 0.005462, loss: 1.1554
2022-07-11 18:17:03 - train: epoch 0102, iter [05000, 05004], lr: 0.005450, loss: 1.0886
2022-07-11 18:17:04 - train: epoch 102, train_loss: 1.2273
2022-07-11 18:18:21 - eval: epoch: 102, acc1: 72.060%, acc5: 90.688%, test_loss: 1.1249, per_image_load_time: 1.349ms, per_image_inference_time: 0.389ms
2022-07-11 18:18:21 - until epoch: 102, best_acc1: 72.060%
2022-07-11 18:18:21 - epoch 103 lr: 0.005450
2022-07-11 18:19:02 - train: epoch 0103, iter [00100, 05004], lr: 0.005438, loss: 1.2100
2022-07-11 18:19:37 - train: epoch 0103, iter [00200, 05004], lr: 0.005426, loss: 1.3342
2022-07-11 18:20:11 - train: epoch 0103, iter [00300, 05004], lr: 0.005414, loss: 1.0568
2022-07-11 18:20:46 - train: epoch 0103, iter [00400, 05004], lr: 0.005402, loss: 1.1980
2022-07-11 18:21:21 - train: epoch 0103, iter [00500, 05004], lr: 0.005390, loss: 1.2005
2022-07-11 18:21:56 - train: epoch 0103, iter [00600, 05004], lr: 0.005379, loss: 1.1641
2022-07-11 18:22:30 - train: epoch 0103, iter [00700, 05004], lr: 0.005367, loss: 1.1741
2022-07-11 18:23:05 - train: epoch 0103, iter [00800, 05004], lr: 0.005355, loss: 1.2244
2022-07-11 18:23:40 - train: epoch 0103, iter [00900, 05004], lr: 0.005343, loss: 1.1769
2022-07-11 18:24:15 - train: epoch 0103, iter [01000, 05004], lr: 0.005332, loss: 1.1965
2022-07-11 18:24:50 - train: epoch 0103, iter [01100, 05004], lr: 0.005320, loss: 1.1517
2022-07-11 18:25:25 - train: epoch 0103, iter [01200, 05004], lr: 0.005308, loss: 1.2871
2022-07-11 18:26:00 - train: epoch 0103, iter [01300, 05004], lr: 0.005296, loss: 1.2952
2022-07-11 18:26:35 - train: epoch 0103, iter [01400, 05004], lr: 0.005285, loss: 1.2899
2022-07-11 18:27:09 - train: epoch 0103, iter [01500, 05004], lr: 0.005273, loss: 1.3075
2022-07-11 18:27:43 - train: epoch 0103, iter [01600, 05004], lr: 0.005261, loss: 0.9040
2022-07-11 18:28:18 - train: epoch 0103, iter [01700, 05004], lr: 0.005250, loss: 1.1215
2022-07-11 18:28:54 - train: epoch 0103, iter [01800, 05004], lr: 0.005238, loss: 1.2491
2022-07-11 18:29:29 - train: epoch 0103, iter [01900, 05004], lr: 0.005226, loss: 1.3493
2022-07-11 18:30:03 - train: epoch 0103, iter [02000, 05004], lr: 0.005215, loss: 1.0376
2022-07-11 18:30:39 - train: epoch 0103, iter [02100, 05004], lr: 0.005203, loss: 1.2327
2022-07-11 18:31:14 - train: epoch 0103, iter [02200, 05004], lr: 0.005191, loss: 1.2152
2022-07-11 18:31:48 - train: epoch 0103, iter [02300, 05004], lr: 0.005180, loss: 1.2751
2022-07-11 18:32:23 - train: epoch 0103, iter [02400, 05004], lr: 0.005168, loss: 1.1593
2022-07-11 18:32:58 - train: epoch 0103, iter [02500, 05004], lr: 0.005157, loss: 1.2657
2022-07-11 18:33:33 - train: epoch 0103, iter [02600, 05004], lr: 0.005145, loss: 1.1245
2022-07-11 18:34:09 - train: epoch 0103, iter [02700, 05004], lr: 0.005133, loss: 1.1369
2022-07-11 18:34:43 - train: epoch 0103, iter [02800, 05004], lr: 0.005122, loss: 0.9820
2022-07-11 18:35:18 - train: epoch 0103, iter [02900, 05004], lr: 0.005110, loss: 1.0659
2022-07-11 18:35:53 - train: epoch 0103, iter [03000, 05004], lr: 0.005099, loss: 1.4344
2022-07-11 18:36:27 - train: epoch 0103, iter [03100, 05004], lr: 0.005087, loss: 1.1882
2022-07-11 18:37:02 - train: epoch 0103, iter [03200, 05004], lr: 0.005076, loss: 1.1248
2022-07-11 18:37:37 - train: epoch 0103, iter [03300, 05004], lr: 0.005064, loss: 1.4116
2022-07-11 18:38:12 - train: epoch 0103, iter [03400, 05004], lr: 0.005053, loss: 1.1613
2022-07-11 18:38:48 - train: epoch 0103, iter [03500, 05004], lr: 0.005042, loss: 1.2131
2022-07-11 18:39:22 - train: epoch 0103, iter [03600, 05004], lr: 0.005030, loss: 1.0702
2022-07-11 18:39:58 - train: epoch 0103, iter [03700, 05004], lr: 0.005019, loss: 1.1621
2022-07-11 18:40:32 - train: epoch 0103, iter [03800, 05004], lr: 0.005007, loss: 1.1533
2022-07-11 18:41:07 - train: epoch 0103, iter [03900, 05004], lr: 0.004996, loss: 1.0349
2022-07-11 18:41:42 - train: epoch 0103, iter [04000, 05004], lr: 0.004984, loss: 1.1002
2022-07-11 18:42:17 - train: epoch 0103, iter [04100, 05004], lr: 0.004973, loss: 1.1881
2022-07-11 18:42:51 - train: epoch 0103, iter [04200, 05004], lr: 0.004962, loss: 1.5886
2022-07-11 18:43:26 - train: epoch 0103, iter [04300, 05004], lr: 0.004950, loss: 1.1409
2022-07-11 18:44:01 - train: epoch 0103, iter [04400, 05004], lr: 0.004939, loss: 1.3141
2022-07-11 18:44:36 - train: epoch 0103, iter [04500, 05004], lr: 0.004928, loss: 1.1799
2022-07-11 18:45:11 - train: epoch 0103, iter [04600, 05004], lr: 0.004916, loss: 1.2939
2022-07-11 18:45:46 - train: epoch 0103, iter [04700, 05004], lr: 0.004905, loss: 1.1655
2022-07-11 18:46:20 - train: epoch 0103, iter [04800, 05004], lr: 0.004894, loss: 1.3278
2022-07-11 18:46:55 - train: epoch 0103, iter [04900, 05004], lr: 0.004882, loss: 1.0247
2022-07-11 18:47:28 - train: epoch 0103, iter [05000, 05004], lr: 0.004871, loss: 1.1351
2022-07-11 18:47:29 - train: epoch 103, train_loss: 1.2114
2022-07-11 18:48:47 - eval: epoch: 103, acc1: 72.296%, acc5: 90.758%, test_loss: 1.1144, per_image_load_time: 2.084ms, per_image_inference_time: 0.397ms
2022-07-11 18:48:47 - until epoch: 103, best_acc1: 72.296%
2022-07-11 18:48:47 - epoch 104 lr: 0.004871
2022-07-11 18:49:27 - train: epoch 0104, iter [00100, 05004], lr: 0.004859, loss: 1.1302
2022-07-11 18:50:00 - train: epoch 0104, iter [00200, 05004], lr: 0.004848, loss: 1.1528
2022-07-11 18:50:35 - train: epoch 0104, iter [00300, 05004], lr: 0.004837, loss: 1.2004
2022-07-11 18:51:09 - train: epoch 0104, iter [00400, 05004], lr: 0.004826, loss: 1.3183
2022-07-11 18:51:43 - train: epoch 0104, iter [00500, 05004], lr: 0.004815, loss: 1.2304
2022-07-11 18:52:16 - train: epoch 0104, iter [00600, 05004], lr: 0.004803, loss: 1.0655
2022-07-11 18:52:50 - train: epoch 0104, iter [00700, 05004], lr: 0.004792, loss: 1.3151
2022-07-11 18:53:24 - train: epoch 0104, iter [00800, 05004], lr: 0.004781, loss: 1.1330
2022-07-11 18:53:59 - train: epoch 0104, iter [00900, 05004], lr: 0.004770, loss: 1.1198
2022-07-11 18:54:33 - train: epoch 0104, iter [01000, 05004], lr: 0.004759, loss: 1.2487
2022-07-11 18:55:07 - train: epoch 0104, iter [01100, 05004], lr: 0.004748, loss: 1.1650
2022-07-11 18:55:42 - train: epoch 0104, iter [01200, 05004], lr: 0.004736, loss: 1.1398
2022-07-11 18:56:16 - train: epoch 0104, iter [01300, 05004], lr: 0.004725, loss: 1.3199
2022-07-11 18:56:51 - train: epoch 0104, iter [01400, 05004], lr: 0.004714, loss: 1.2109
2022-07-11 18:57:25 - train: epoch 0104, iter [01500, 05004], lr: 0.004703, loss: 1.2514
2022-07-11 18:58:00 - train: epoch 0104, iter [01600, 05004], lr: 0.004692, loss: 1.0575
2022-07-11 18:58:34 - train: epoch 0104, iter [01700, 05004], lr: 0.004681, loss: 1.3651
2022-07-11 18:59:08 - train: epoch 0104, iter [01800, 05004], lr: 0.004670, loss: 1.2420
2022-07-11 18:59:44 - train: epoch 0104, iter [01900, 05004], lr: 0.004659, loss: 0.9889
2022-07-11 19:00:18 - train: epoch 0104, iter [02000, 05004], lr: 0.004648, loss: 1.2378
2022-07-11 19:00:53 - train: epoch 0104, iter [02100, 05004], lr: 0.004637, loss: 1.1407
2022-07-11 19:01:27 - train: epoch 0104, iter [02200, 05004], lr: 0.004626, loss: 1.3017
2022-07-11 19:02:02 - train: epoch 0104, iter [02300, 05004], lr: 0.004615, loss: 1.2464
2022-07-11 19:02:37 - train: epoch 0104, iter [02400, 05004], lr: 0.004604, loss: 1.4026
2022-07-11 19:03:11 - train: epoch 0104, iter [02500, 05004], lr: 0.004593, loss: 1.2794
2022-07-11 19:03:45 - train: epoch 0104, iter [02600, 05004], lr: 0.004582, loss: 1.1409
2022-07-11 19:04:21 - train: epoch 0104, iter [02700, 05004], lr: 0.004571, loss: 1.2527
2022-07-11 19:04:55 - train: epoch 0104, iter [02800, 05004], lr: 0.004560, loss: 1.1871
2022-07-11 19:05:31 - train: epoch 0104, iter [02900, 05004], lr: 0.004549, loss: 1.2702
2022-07-11 19:06:04 - train: epoch 0104, iter [03000, 05004], lr: 0.004538, loss: 1.2263
2022-07-11 19:06:39 - train: epoch 0104, iter [03100, 05004], lr: 0.004528, loss: 1.0258
2022-07-11 19:07:13 - train: epoch 0104, iter [03200, 05004], lr: 0.004517, loss: 1.1694
2022-07-11 19:07:47 - train: epoch 0104, iter [03300, 05004], lr: 0.004506, loss: 1.1853
2022-07-11 19:08:21 - train: epoch 0104, iter [03400, 05004], lr: 0.004495, loss: 1.1697
2022-07-11 19:08:55 - train: epoch 0104, iter [03500, 05004], lr: 0.004484, loss: 1.0428
2022-07-11 19:09:29 - train: epoch 0104, iter [03600, 05004], lr: 0.004473, loss: 1.1086
2022-07-11 19:10:05 - train: epoch 0104, iter [03700, 05004], lr: 0.004463, loss: 1.3097
2022-07-11 19:10:39 - train: epoch 0104, iter [03800, 05004], lr: 0.004452, loss: 0.9480
2022-07-11 19:11:14 - train: epoch 0104, iter [03900, 05004], lr: 0.004441, loss: 1.0287
2022-07-11 19:11:48 - train: epoch 0104, iter [04000, 05004], lr: 0.004430, loss: 1.2104
2022-07-11 19:12:23 - train: epoch 0104, iter [04100, 05004], lr: 0.004419, loss: 1.0932
2022-07-11 19:12:57 - train: epoch 0104, iter [04200, 05004], lr: 0.004409, loss: 1.2041
2022-07-11 19:13:32 - train: epoch 0104, iter [04300, 05004], lr: 0.004398, loss: 1.3704
2022-07-11 19:14:06 - train: epoch 0104, iter [04400, 05004], lr: 0.004387, loss: 1.1622
2022-07-11 19:14:41 - train: epoch 0104, iter [04500, 05004], lr: 0.004377, loss: 1.4221
2022-07-11 19:15:15 - train: epoch 0104, iter [04600, 05004], lr: 0.004366, loss: 1.1642
2022-07-11 19:15:50 - train: epoch 0104, iter [04700, 05004], lr: 0.004355, loss: 1.3098
2022-07-11 19:16:24 - train: epoch 0104, iter [04800, 05004], lr: 0.004344, loss: 1.2962
2022-07-11 19:16:59 - train: epoch 0104, iter [04900, 05004], lr: 0.004334, loss: 1.1213
2022-07-11 19:17:32 - train: epoch 0104, iter [05000, 05004], lr: 0.004323, loss: 1.2288
2022-07-11 19:17:33 - train: epoch 104, train_loss: 1.1919
2022-07-11 19:18:51 - eval: epoch: 104, acc1: 72.678%, acc5: 90.944%, test_loss: 1.0969, per_image_load_time: 2.474ms, per_image_inference_time: 0.356ms
2022-07-11 19:18:51 - until epoch: 104, best_acc1: 72.678%
2022-07-11 19:18:51 - epoch 105 lr: 0.004323
2022-07-11 19:19:31 - train: epoch 0105, iter [00100, 05004], lr: 0.004312, loss: 1.3426
2022-07-11 19:20:06 - train: epoch 0105, iter [00200, 05004], lr: 0.004301, loss: 1.4853
2022-07-11 19:20:41 - train: epoch 0105, iter [00300, 05004], lr: 0.004291, loss: 1.2320
2022-07-11 19:21:15 - train: epoch 0105, iter [00400, 05004], lr: 0.004280, loss: 1.2490
2022-07-11 19:21:50 - train: epoch 0105, iter [00500, 05004], lr: 0.004270, loss: 1.1596
2022-07-11 19:22:24 - train: epoch 0105, iter [00600, 05004], lr: 0.004259, loss: 1.1635
2022-07-11 19:22:58 - train: epoch 0105, iter [00700, 05004], lr: 0.004249, loss: 1.1834
2022-07-11 19:23:32 - train: epoch 0105, iter [00800, 05004], lr: 0.004238, loss: 1.2355
2022-07-11 19:24:07 - train: epoch 0105, iter [00900, 05004], lr: 0.004227, loss: 1.1591
2022-07-11 19:24:42 - train: epoch 0105, iter [01000, 05004], lr: 0.004217, loss: 1.0324
2022-07-11 19:25:16 - train: epoch 0105, iter [01100, 05004], lr: 0.004206, loss: 1.3630
2022-07-11 19:25:49 - train: epoch 0105, iter [01200, 05004], lr: 0.004196, loss: 1.1863
2022-07-11 19:26:23 - train: epoch 0105, iter [01300, 05004], lr: 0.004185, loss: 1.0065
2022-07-11 19:26:56 - train: epoch 0105, iter [01400, 05004], lr: 0.004175, loss: 1.2608
2022-07-11 19:27:30 - train: epoch 0105, iter [01500, 05004], lr: 0.004165, loss: 1.1338
2022-07-11 19:28:03 - train: epoch 0105, iter [01600, 05004], lr: 0.004154, loss: 0.9773
2022-07-11 19:28:36 - train: epoch 0105, iter [01700, 05004], lr: 0.004144, loss: 1.1108
2022-07-11 19:29:11 - train: epoch 0105, iter [01800, 05004], lr: 0.004133, loss: 1.3273
2022-07-11 19:29:45 - train: epoch 0105, iter [01900, 05004], lr: 0.004123, loss: 1.3351
2022-07-11 19:30:20 - train: epoch 0105, iter [02000, 05004], lr: 0.004112, loss: 1.0718
2022-07-11 19:30:54 - train: epoch 0105, iter [02100, 05004], lr: 0.004102, loss: 1.2541
2022-07-11 19:31:28 - train: epoch 0105, iter [02200, 05004], lr: 0.004092, loss: 1.3570
2022-07-11 19:32:02 - train: epoch 0105, iter [02300, 05004], lr: 0.004081, loss: 1.2062
2022-07-11 19:32:36 - train: epoch 0105, iter [02400, 05004], lr: 0.004071, loss: 1.0003
2022-07-11 19:33:10 - train: epoch 0105, iter [02500, 05004], lr: 0.004061, loss: 1.1498
2022-07-11 19:33:45 - train: epoch 0105, iter [02600, 05004], lr: 0.004050, loss: 1.0865
2022-07-11 19:34:19 - train: epoch 0105, iter [02700, 05004], lr: 0.004040, loss: 1.1953
2022-07-11 19:34:53 - train: epoch 0105, iter [02800, 05004], lr: 0.004030, loss: 1.3376
2022-07-11 19:35:27 - train: epoch 0105, iter [02900, 05004], lr: 0.004019, loss: 1.1126
2022-07-11 19:36:02 - train: epoch 0105, iter [03000, 05004], lr: 0.004009, loss: 0.9935
2022-07-11 19:36:36 - train: epoch 0105, iter [03100, 05004], lr: 0.003999, loss: 1.3219
2022-07-11 19:37:10 - train: epoch 0105, iter [03200, 05004], lr: 0.003989, loss: 1.3805
2022-07-11 19:37:44 - train: epoch 0105, iter [03300, 05004], lr: 0.003978, loss: 1.1609
2022-07-11 19:38:19 - train: epoch 0105, iter [03400, 05004], lr: 0.003968, loss: 1.1338
2022-07-11 19:38:54 - train: epoch 0105, iter [03500, 05004], lr: 0.003958, loss: 1.2460
2022-07-11 19:39:28 - train: epoch 0105, iter [03600, 05004], lr: 0.003948, loss: 1.3002
2022-07-11 19:40:03 - train: epoch 0105, iter [03700, 05004], lr: 0.003938, loss: 1.2236
2022-07-11 19:40:37 - train: epoch 0105, iter [03800, 05004], lr: 0.003927, loss: 1.2334
2022-07-11 19:41:11 - train: epoch 0105, iter [03900, 05004], lr: 0.003917, loss: 1.2408
2022-07-11 19:41:45 - train: epoch 0105, iter [04000, 05004], lr: 0.003907, loss: 1.1722
2022-07-11 19:42:20 - train: epoch 0105, iter [04100, 05004], lr: 0.003897, loss: 1.0695
2022-07-11 19:42:54 - train: epoch 0105, iter [04200, 05004], lr: 0.003887, loss: 1.1558
2022-07-11 19:43:28 - train: epoch 0105, iter [04300, 05004], lr: 0.003877, loss: 1.2569
2022-07-11 19:44:02 - train: epoch 0105, iter [04400, 05004], lr: 0.003867, loss: 0.9417
2022-07-11 19:44:37 - train: epoch 0105, iter [04500, 05004], lr: 0.003857, loss: 1.3746
2022-07-11 19:45:11 - train: epoch 0105, iter [04600, 05004], lr: 0.003847, loss: 1.3447
2022-07-11 19:45:45 - train: epoch 0105, iter [04700, 05004], lr: 0.003837, loss: 1.2758
2022-07-11 19:46:19 - train: epoch 0105, iter [04800, 05004], lr: 0.003826, loss: 1.0216
2022-07-11 19:46:54 - train: epoch 0105, iter [04900, 05004], lr: 0.003816, loss: 1.1187
2022-07-11 19:47:26 - train: epoch 0105, iter [05000, 05004], lr: 0.003806, loss: 1.0510
2022-07-11 19:47:27 - train: epoch 105, train_loss: 1.1721
2022-07-11 19:48:44 - eval: epoch: 105, acc1: 72.930%, acc5: 91.174%, test_loss: 1.0791, per_image_load_time: 1.394ms, per_image_inference_time: 0.354ms
2022-07-11 19:48:44 - until epoch: 105, best_acc1: 72.930%
2022-07-11 19:48:44 - epoch 106 lr: 0.003806
2022-07-11 19:49:24 - train: epoch 0106, iter [00100, 05004], lr: 0.003796, loss: 1.0941
2022-07-11 19:49:58 - train: epoch 0106, iter [00200, 05004], lr: 0.003786, loss: 1.0172
2022-07-11 19:50:33 - train: epoch 0106, iter [00300, 05004], lr: 0.003776, loss: 0.8968
2022-07-11 19:51:06 - train: epoch 0106, iter [00400, 05004], lr: 0.003766, loss: 0.9661
2022-07-11 19:51:41 - train: epoch 0106, iter [00500, 05004], lr: 0.003756, loss: 1.2640
2022-07-11 19:52:15 - train: epoch 0106, iter [00600, 05004], lr: 0.003746, loss: 1.3082
2022-07-11 19:52:49 - train: epoch 0106, iter [00700, 05004], lr: 0.003736, loss: 1.2137
2022-07-11 19:53:23 - train: epoch 0106, iter [00800, 05004], lr: 0.003726, loss: 1.0917
2022-07-11 19:53:57 - train: epoch 0106, iter [00900, 05004], lr: 0.003716, loss: 1.0295
2022-07-11 19:54:31 - train: epoch 0106, iter [01000, 05004], lr: 0.003707, loss: 1.1805
2022-07-11 19:55:06 - train: epoch 0106, iter [01100, 05004], lr: 0.003697, loss: 1.0585
2022-07-11 19:55:40 - train: epoch 0106, iter [01200, 05004], lr: 0.003687, loss: 1.1646
2022-07-11 19:56:14 - train: epoch 0106, iter [01300, 05004], lr: 0.003677, loss: 1.1168
2022-07-11 19:56:47 - train: epoch 0106, iter [01400, 05004], lr: 0.003667, loss: 1.0912
2022-07-11 19:57:21 - train: epoch 0106, iter [01500, 05004], lr: 0.003657, loss: 1.1352
2022-07-11 19:57:55 - train: epoch 0106, iter [01600, 05004], lr: 0.003647, loss: 1.1229
2022-07-11 19:58:29 - train: epoch 0106, iter [01700, 05004], lr: 0.003638, loss: 1.0896
2022-07-11 19:59:04 - train: epoch 0106, iter [01800, 05004], lr: 0.003628, loss: 1.1575
2022-07-11 19:59:38 - train: epoch 0106, iter [01900, 05004], lr: 0.003618, loss: 0.9495
2022-07-11 20:00:12 - train: epoch 0106, iter [02000, 05004], lr: 0.003608, loss: 1.0780
2022-07-11 20:00:46 - train: epoch 0106, iter [02100, 05004], lr: 0.003599, loss: 1.0086
2022-07-11 20:01:19 - train: epoch 0106, iter [02200, 05004], lr: 0.003589, loss: 1.2093
2022-07-11 20:01:54 - train: epoch 0106, iter [02300, 05004], lr: 0.003579, loss: 1.2121
2022-07-11 20:02:26 - train: epoch 0106, iter [02400, 05004], lr: 0.003569, loss: 1.1907
2022-07-11 20:02:59 - train: epoch 0106, iter [02500, 05004], lr: 0.003560, loss: 1.0707
2022-07-11 20:03:32 - train: epoch 0106, iter [02600, 05004], lr: 0.003550, loss: 1.0463
2022-07-11 20:04:06 - train: epoch 0106, iter [02700, 05004], lr: 0.003540, loss: 1.2279
2022-07-11 20:04:39 - train: epoch 0106, iter [02800, 05004], lr: 0.003531, loss: 1.2326
2022-07-11 20:05:13 - train: epoch 0106, iter [02900, 05004], lr: 0.003521, loss: 0.9828
2022-07-11 20:05:47 - train: epoch 0106, iter [03000, 05004], lr: 0.003511, loss: 1.2467
2022-07-11 20:06:21 - train: epoch 0106, iter [03100, 05004], lr: 0.003502, loss: 1.1270
2022-07-11 20:06:55 - train: epoch 0106, iter [03200, 05004], lr: 0.003492, loss: 1.1397
2022-07-11 20:07:29 - train: epoch 0106, iter [03300, 05004], lr: 0.003483, loss: 1.1301
2022-07-11 20:08:04 - train: epoch 0106, iter [03400, 05004], lr: 0.003473, loss: 1.1084
2022-07-11 20:08:37 - train: epoch 0106, iter [03500, 05004], lr: 0.003463, loss: 0.9716
2022-07-11 20:09:11 - train: epoch 0106, iter [03600, 05004], lr: 0.003454, loss: 1.0431
2022-07-11 20:09:45 - train: epoch 0106, iter [03700, 05004], lr: 0.003444, loss: 1.1650
2022-07-11 20:10:18 - train: epoch 0106, iter [03800, 05004], lr: 0.003435, loss: 1.3592
2022-07-11 20:10:51 - train: epoch 0106, iter [03900, 05004], lr: 0.003425, loss: 1.2511
2022-07-11 20:11:23 - train: epoch 0106, iter [04000, 05004], lr: 0.003416, loss: 1.0396
2022-07-11 20:11:58 - train: epoch 0106, iter [04100, 05004], lr: 0.003406, loss: 1.1890
2022-07-11 20:12:32 - train: epoch 0106, iter [04200, 05004], lr: 0.003397, loss: 0.9420
2022-07-11 20:13:05 - train: epoch 0106, iter [04300, 05004], lr: 0.003387, loss: 0.9516
2022-07-11 20:13:41 - train: epoch 0106, iter [04400, 05004], lr: 0.003378, loss: 1.1663
2022-07-11 20:14:14 - train: epoch 0106, iter [04500, 05004], lr: 0.003368, loss: 1.2533
2022-07-11 20:14:48 - train: epoch 0106, iter [04600, 05004], lr: 0.003359, loss: 1.2771
2022-07-11 20:15:22 - train: epoch 0106, iter [04700, 05004], lr: 0.003350, loss: 1.2846
2022-07-11 20:15:57 - train: epoch 0106, iter [04800, 05004], lr: 0.003340, loss: 1.2407
2022-07-11 20:16:31 - train: epoch 0106, iter [04900, 05004], lr: 0.003331, loss: 0.9596
2022-07-11 20:17:03 - train: epoch 0106, iter [05000, 05004], lr: 0.003321, loss: 1.0594
2022-07-11 20:17:04 - train: epoch 106, train_loss: 1.1536
2022-07-11 20:18:22 - eval: epoch: 106, acc1: 73.210%, acc5: 91.386%, test_loss: 1.0719, per_image_load_time: 2.321ms, per_image_inference_time: 0.389ms
2022-07-11 20:18:22 - until epoch: 106, best_acc1: 73.210%
2022-07-11 20:18:22 - epoch 107 lr: 0.003321
2022-07-11 20:19:04 - train: epoch 0107, iter [00100, 05004], lr: 0.003312, loss: 1.0888
2022-07-11 20:19:38 - train: epoch 0107, iter [00200, 05004], lr: 0.003302, loss: 1.1617
2022-07-11 20:20:12 - train: epoch 0107, iter [00300, 05004], lr: 0.003293, loss: 1.3563
2022-07-11 20:20:47 - train: epoch 0107, iter [00400, 05004], lr: 0.003284, loss: 0.9948
2022-07-11 20:21:21 - train: epoch 0107, iter [00500, 05004], lr: 0.003274, loss: 0.9505
2022-07-11 20:21:55 - train: epoch 0107, iter [00600, 05004], lr: 0.003265, loss: 0.9672
2022-07-11 20:22:30 - train: epoch 0107, iter [00700, 05004], lr: 0.003256, loss: 1.2071
2022-07-11 20:23:03 - train: epoch 0107, iter [00800, 05004], lr: 0.003246, loss: 1.2074
2022-07-11 20:23:38 - train: epoch 0107, iter [00900, 05004], lr: 0.003237, loss: 1.1558
2022-07-11 20:24:12 - train: epoch 0107, iter [01000, 05004], lr: 0.003228, loss: 1.1546
2022-07-11 20:24:47 - train: epoch 0107, iter [01100, 05004], lr: 0.003219, loss: 1.0863
2022-07-11 20:25:21 - train: epoch 0107, iter [01200, 05004], lr: 0.003209, loss: 1.1677
2022-07-11 20:25:55 - train: epoch 0107, iter [01300, 05004], lr: 0.003200, loss: 1.0926
2022-07-11 20:26:30 - train: epoch 0107, iter [01400, 05004], lr: 0.003191, loss: 1.1679
2022-07-11 20:27:04 - train: epoch 0107, iter [01500, 05004], lr: 0.003182, loss: 1.1902
2022-07-11 20:27:38 - train: epoch 0107, iter [01600, 05004], lr: 0.003173, loss: 0.9702
2022-07-11 20:28:13 - train: epoch 0107, iter [01700, 05004], lr: 0.003163, loss: 1.0218
2022-07-11 20:28:47 - train: epoch 0107, iter [01800, 05004], lr: 0.003154, loss: 1.0965
2022-07-11 20:29:21 - train: epoch 0107, iter [01900, 05004], lr: 0.003145, loss: 1.2759
2022-07-11 20:29:55 - train: epoch 0107, iter [02000, 05004], lr: 0.003136, loss: 1.2805
2022-07-11 20:30:30 - train: epoch 0107, iter [02100, 05004], lr: 0.003127, loss: 1.1658
2022-07-11 20:31:03 - train: epoch 0107, iter [02200, 05004], lr: 0.003118, loss: 1.1455
2022-07-11 20:31:38 - train: epoch 0107, iter [02300, 05004], lr: 0.003109, loss: 1.1883
2022-07-11 20:32:11 - train: epoch 0107, iter [02400, 05004], lr: 0.003100, loss: 1.1386
2022-07-11 20:32:46 - train: epoch 0107, iter [02500, 05004], lr: 0.003091, loss: 1.2267
2022-07-11 20:33:20 - train: epoch 0107, iter [02600, 05004], lr: 0.003082, loss: 1.3767
2022-07-11 20:33:55 - train: epoch 0107, iter [02700, 05004], lr: 0.003073, loss: 1.2137
2022-07-11 20:34:30 - train: epoch 0107, iter [02800, 05004], lr: 0.003064, loss: 1.1248
2022-07-11 20:35:03 - train: epoch 0107, iter [02900, 05004], lr: 0.003054, loss: 1.2418
2022-07-11 20:35:38 - train: epoch 0107, iter [03000, 05004], lr: 0.003046, loss: 1.0966
2022-07-11 20:36:12 - train: epoch 0107, iter [03100, 05004], lr: 0.003037, loss: 1.1848
2022-07-11 20:36:46 - train: epoch 0107, iter [03200, 05004], lr: 0.003028, loss: 1.0589
2022-07-11 20:37:20 - train: epoch 0107, iter [03300, 05004], lr: 0.003019, loss: 1.1077
2022-07-11 20:37:54 - train: epoch 0107, iter [03400, 05004], lr: 0.003010, loss: 1.1773
2022-07-11 20:38:28 - train: epoch 0107, iter [03500, 05004], lr: 0.003001, loss: 0.9805
2022-07-11 20:39:02 - train: epoch 0107, iter [03600, 05004], lr: 0.002992, loss: 1.0804
2022-07-11 20:39:36 - train: epoch 0107, iter [03700, 05004], lr: 0.002983, loss: 1.1244
2022-07-11 20:40:10 - train: epoch 0107, iter [03800, 05004], lr: 0.002974, loss: 1.1358
2022-07-11 20:40:44 - train: epoch 0107, iter [03900, 05004], lr: 0.002965, loss: 1.1939
2022-07-11 20:41:18 - train: epoch 0107, iter [04000, 05004], lr: 0.002956, loss: 1.2509
2022-07-11 20:41:52 - train: epoch 0107, iter [04100, 05004], lr: 0.002947, loss: 1.0178
2022-07-11 20:42:26 - train: epoch 0107, iter [04200, 05004], lr: 0.002939, loss: 1.0597
2022-07-11 20:43:00 - train: epoch 0107, iter [04300, 05004], lr: 0.002930, loss: 1.2964
2022-07-11 20:43:34 - train: epoch 0107, iter [04400, 05004], lr: 0.002921, loss: 1.1957
2022-07-11 20:44:09 - train: epoch 0107, iter [04500, 05004], lr: 0.002912, loss: 1.2397
2022-07-11 20:44:43 - train: epoch 0107, iter [04600, 05004], lr: 0.002903, loss: 1.1122
2022-07-11 20:45:18 - train: epoch 0107, iter [04700, 05004], lr: 0.002895, loss: 1.1553
2022-07-11 20:45:51 - train: epoch 0107, iter [04800, 05004], lr: 0.002886, loss: 1.1330
2022-07-11 20:46:25 - train: epoch 0107, iter [04900, 05004], lr: 0.002877, loss: 1.0866
2022-07-11 20:46:58 - train: epoch 0107, iter [05000, 05004], lr: 0.002868, loss: 1.0653
2022-07-11 20:46:59 - train: epoch 107, train_loss: 1.1389
