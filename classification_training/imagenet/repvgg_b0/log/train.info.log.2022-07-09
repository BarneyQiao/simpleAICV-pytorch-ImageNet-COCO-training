2022-07-09 09:16:47 - network: RepVGG_B0
2022-07-09 09:16:47 - num_classes: 1000
2022-07-09 09:16:47 - input_image_size: 224
2022-07-09 09:16:47 - scale: 1.1428571428571428
2022-07-09 09:16:47 - trained_model_path: 
2022-07-09 09:16:47 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-09 09:16:47 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-09 09:16:47 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f448db2fe80>
2022-07-09 09:16:47 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4457f16190>
2022-07-09 09:16:47 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4457f161c0>
2022-07-09 09:16:47 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4457f16220>
2022-07-09 09:16:47 - seed: 0
2022-07-09 09:16:47 - batch_size: 256
2022-07-09 09:16:47 - num_workers: 16
2022-07-09 09:16:47 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-09 09:16:47 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-07-09 09:16:47 - epochs: 120
2022-07-09 09:16:47 - print_interval: 100
2022-07-09 09:16:47 - accumulation_steps: 1
2022-07-09 09:16:47 - sync_bn: False
2022-07-09 09:16:47 - apex: True
2022-07-09 09:16:47 - use_ema_model: False
2022-07-09 09:16:47 - ema_model_decay: 0.9999
2022-07-09 09:16:47 - gpus_type: NVIDIA RTX A5000
2022-07-09 09:16:47 - gpus_num: 2
2022-07-09 09:16:47 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f448d48e770>
2022-07-09 09:16:47 - --------------------parameters--------------------
2022-07-09 09:16:47 - name: stage0.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage0.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.0.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.0.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.1.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage1.1.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage1.1.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.1.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.2.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage1.2.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage1.2.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.2.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.3.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage1.3.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage1.3.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage1.3.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.0.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.0.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.1.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage2.1.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage2.1.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.1.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.2.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage2.2.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage2.2.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.2.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.3.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage2.3.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage2.3.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.3.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.4.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage2.4.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage2.4.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.4.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.5.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage2.5.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage2.5.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage2.5.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.0.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.0.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.1.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.1.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.1.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.1.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.2.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.2.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.2.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.2.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.3.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.3.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.3.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.3.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.4.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.4.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.4.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.4.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.5.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.5.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.5.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.5.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.6.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.6.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.6.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.6.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.7.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.7.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.7.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.7.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.8.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.8.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.8.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.8.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.9.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.9.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.9.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.9.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.10.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.10.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.10.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.10.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.11.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.11.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.11.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.11.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.12.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.12.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.12.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.12.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.13.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.13.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.13.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.13.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.14.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.14.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.14.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.14.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.15.identity.weight, grad: True
2022-07-09 09:16:47 - name: stage3.15.identity.bias, grad: True
2022-07-09 09:16:47 - name: stage3.15.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage3.15.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage4.0.conv3x3.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.bias, grad: True
2022-07-09 09:16:47 - name: stage4.0.conv1x1.conv.weight, grad: True
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.weight, grad: True
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.bias, grad: True
2022-07-09 09:16:47 - name: fc.weight, grad: True
2022-07-09 09:16:47 - name: fc.bias, grad: True
2022-07-09 09:16:47 - --------------------buffers--------------------
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.1.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.1.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.1.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.2.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.2.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.2.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.3.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.3.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.3.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.1.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.1.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.1.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.2.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.2.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.2.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.3.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.3.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.3.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.4.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.4.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.4.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.5.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.5.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.5.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.1.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.1.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.1.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.2.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.2.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.2.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.3.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.3.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.3.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.4.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.4.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.4.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.5.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.5.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.5.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.6.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.6.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.6.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.7.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.7.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.7.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.8.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.8.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.8.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.9.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.9.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.9.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.10.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.10.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.10.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.11.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.11.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.11.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.12.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.12.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.12.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.13.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.13.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.13.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.14.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.14.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.14.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.15.identity.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.15.identity.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.15.identity.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.running_mean, grad: False
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.running_var, grad: False
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-09 09:16:47 - -----------no weight decay layers--------------
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage4.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage4.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-09 09:16:47 - -------------weight decay layers---------------
2022-07-09 09:16:47 - name: stage0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage1.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage2.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.6.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.7.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.8.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.9.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.10.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.11.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.12.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.13.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.14.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage3.15.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage4.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: stage4.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-09 09:16:47 - epoch 001 lr: 0.100000
2022-07-09 09:17:27 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8764
2022-07-09 09:18:01 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.7345
2022-07-09 09:18:35 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6171
2022-07-09 09:19:09 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5375
2022-07-09 09:19:44 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4124
2022-07-09 09:20:18 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.2141
2022-07-09 09:20:52 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.1762
2022-07-09 09:21:27 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.0651
2022-07-09 09:22:00 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 6.0318
2022-07-09 09:22:35 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 6.0284
2022-07-09 09:23:09 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 5.7816
2022-07-09 09:23:43 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 5.6855
2022-07-09 09:24:17 - train: epoch 0001, iter [01300, 05004], lr: 0.099999, loss: 5.6099
2022-07-09 09:24:51 - train: epoch 0001, iter [01400, 05004], lr: 0.099999, loss: 5.5722
2022-07-09 09:25:26 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 5.4491
2022-07-09 09:26:00 - train: epoch 0001, iter [01600, 05004], lr: 0.099998, loss: 5.4892
2022-07-09 09:26:34 - train: epoch 0001, iter [01700, 05004], lr: 0.099998, loss: 5.3716
2022-07-09 09:27:09 - train: epoch 0001, iter [01800, 05004], lr: 0.099998, loss: 5.4497
2022-07-09 09:27:42 - train: epoch 0001, iter [01900, 05004], lr: 0.099998, loss: 5.2701
2022-07-09 09:28:16 - train: epoch 0001, iter [02000, 05004], lr: 0.099997, loss: 5.1564
2022-07-09 09:28:51 - train: epoch 0001, iter [02100, 05004], lr: 0.099997, loss: 5.1453
2022-07-09 09:29:24 - train: epoch 0001, iter [02200, 05004], lr: 0.099997, loss: 4.9974
2022-07-09 09:29:59 - train: epoch 0001, iter [02300, 05004], lr: 0.099996, loss: 4.9429
2022-07-09 09:30:33 - train: epoch 0001, iter [02400, 05004], lr: 0.099996, loss: 4.9676
2022-07-09 09:31:07 - train: epoch 0001, iter [02500, 05004], lr: 0.099996, loss: 4.9842
2022-07-09 09:31:41 - train: epoch 0001, iter [02600, 05004], lr: 0.099995, loss: 5.0988
2022-07-09 09:32:14 - train: epoch 0001, iter [02700, 05004], lr: 0.099995, loss: 5.0687
2022-07-09 09:32:48 - train: epoch 0001, iter [02800, 05004], lr: 0.099995, loss: 4.7497
2022-07-09 09:33:23 - train: epoch 0001, iter [02900, 05004], lr: 0.099994, loss: 4.6491
2022-07-09 09:33:57 - train: epoch 0001, iter [03000, 05004], lr: 0.099994, loss: 4.8461
2022-07-09 09:34:31 - train: epoch 0001, iter [03100, 05004], lr: 0.099993, loss: 4.9060
2022-07-09 09:35:05 - train: epoch 0001, iter [03200, 05004], lr: 0.099993, loss: 4.7225
2022-07-09 09:35:40 - train: epoch 0001, iter [03300, 05004], lr: 0.099993, loss: 4.5209
2022-07-09 09:36:13 - train: epoch 0001, iter [03400, 05004], lr: 0.099992, loss: 4.5417
2022-07-09 09:36:48 - train: epoch 0001, iter [03500, 05004], lr: 0.099992, loss: 4.5632
2022-07-09 09:37:22 - train: epoch 0001, iter [03600, 05004], lr: 0.099991, loss: 4.5235
2022-07-09 09:37:57 - train: epoch 0001, iter [03700, 05004], lr: 0.099991, loss: 4.6411
2022-07-09 09:38:30 - train: epoch 0001, iter [03800, 05004], lr: 0.099990, loss: 4.3417
2022-07-09 09:39:04 - train: epoch 0001, iter [03900, 05004], lr: 0.099990, loss: 4.4947
2022-07-09 09:39:38 - train: epoch 0001, iter [04000, 05004], lr: 0.099989, loss: 4.3404
2022-07-09 09:40:13 - train: epoch 0001, iter [04100, 05004], lr: 0.099988, loss: 4.4499
2022-07-09 09:40:46 - train: epoch 0001, iter [04200, 05004], lr: 0.099988, loss: 4.3751
2022-07-09 09:41:21 - train: epoch 0001, iter [04300, 05004], lr: 0.099987, loss: 4.3623
2022-07-09 09:41:54 - train: epoch 0001, iter [04400, 05004], lr: 0.099987, loss: 4.0422
2022-07-09 09:42:29 - train: epoch 0001, iter [04500, 05004], lr: 0.099986, loss: 4.2518
2022-07-09 09:43:03 - train: epoch 0001, iter [04600, 05004], lr: 0.099986, loss: 4.4819
2022-07-09 09:43:37 - train: epoch 0001, iter [04700, 05004], lr: 0.099985, loss: 4.1141
2022-07-09 09:44:11 - train: epoch 0001, iter [04800, 05004], lr: 0.099984, loss: 4.3366
2022-07-09 09:44:44 - train: epoch 0001, iter [04900, 05004], lr: 0.099984, loss: 4.2102
2022-07-09 09:45:17 - train: epoch 0001, iter [05000, 05004], lr: 0.099983, loss: 4.0166
2022-07-09 09:45:18 - train: epoch 001, train_loss: 5.1074
2022-07-09 09:46:34 - eval: epoch: 001, acc1: 19.302%, acc5: 41.638%, test_loss: 4.0571, per_image_load_time: 0.910ms, per_image_inference_time: 0.384ms
2022-07-09 09:46:34 - until epoch: 001, best_acc1: 19.302%
2022-07-09 09:46:34 - epoch 002 lr: 0.099983
2022-07-09 09:47:14 - train: epoch 0002, iter [00100, 05004], lr: 0.099982, loss: 4.2045
2022-07-09 09:47:48 - train: epoch 0002, iter [00200, 05004], lr: 0.099981, loss: 3.8689
2022-07-09 09:48:22 - train: epoch 0002, iter [00300, 05004], lr: 0.099981, loss: 4.1259
2022-07-09 09:48:56 - train: epoch 0002, iter [00400, 05004], lr: 0.099980, loss: 4.1302
2022-07-09 09:49:30 - train: epoch 0002, iter [00500, 05004], lr: 0.099979, loss: 3.8454
2022-07-09 09:50:05 - train: epoch 0002, iter [00600, 05004], lr: 0.099979, loss: 3.7796
2022-07-09 09:50:38 - train: epoch 0002, iter [00700, 05004], lr: 0.099978, loss: 4.0792
2022-07-09 09:51:13 - train: epoch 0002, iter [00800, 05004], lr: 0.099977, loss: 3.7934
2022-07-09 09:51:48 - train: epoch 0002, iter [00900, 05004], lr: 0.099976, loss: 3.6247
2022-07-09 09:52:22 - train: epoch 0002, iter [01000, 05004], lr: 0.099975, loss: 4.0538
2022-07-09 09:52:56 - train: epoch 0002, iter [01100, 05004], lr: 0.099975, loss: 4.0590
2022-07-09 09:53:31 - train: epoch 0002, iter [01200, 05004], lr: 0.099974, loss: 3.8809
2022-07-09 09:54:05 - train: epoch 0002, iter [01300, 05004], lr: 0.099973, loss: 3.7979
2022-07-09 09:54:39 - train: epoch 0002, iter [01400, 05004], lr: 0.099972, loss: 3.9077
2022-07-09 09:55:14 - train: epoch 0002, iter [01500, 05004], lr: 0.099971, loss: 3.9108
2022-07-09 09:55:49 - train: epoch 0002, iter [01600, 05004], lr: 0.099970, loss: 3.7128
2022-07-09 09:56:22 - train: epoch 0002, iter [01700, 05004], lr: 0.099969, loss: 3.8037
2022-07-09 09:56:56 - train: epoch 0002, iter [01800, 05004], lr: 0.099968, loss: 3.8436
2022-07-09 09:57:31 - train: epoch 0002, iter [01900, 05004], lr: 0.099967, loss: 3.7372
2022-07-09 09:58:05 - train: epoch 0002, iter [02000, 05004], lr: 0.099966, loss: 3.4241
2022-07-09 09:58:39 - train: epoch 0002, iter [02100, 05004], lr: 0.099965, loss: 3.6795
2022-07-09 09:59:13 - train: epoch 0002, iter [02200, 05004], lr: 0.099964, loss: 3.3996
2022-07-09 09:59:48 - train: epoch 0002, iter [02300, 05004], lr: 0.099963, loss: 3.7584
2022-07-09 10:00:22 - train: epoch 0002, iter [02400, 05004], lr: 0.099962, loss: 3.4980
2022-07-09 10:00:57 - train: epoch 0002, iter [02500, 05004], lr: 0.099961, loss: 3.4985
2022-07-09 10:01:30 - train: epoch 0002, iter [02600, 05004], lr: 0.099960, loss: 3.4661
2022-07-09 10:02:04 - train: epoch 0002, iter [02700, 05004], lr: 0.099959, loss: 3.7318
2022-07-09 10:02:38 - train: epoch 0002, iter [02800, 05004], lr: 0.099958, loss: 3.5660
2022-07-09 10:03:12 - train: epoch 0002, iter [02900, 05004], lr: 0.099957, loss: 3.4996
2022-07-09 10:03:47 - train: epoch 0002, iter [03000, 05004], lr: 0.099956, loss: 3.3910
2022-07-09 10:04:21 - train: epoch 0002, iter [03100, 05004], lr: 0.099955, loss: 3.4483
2022-07-09 10:04:55 - train: epoch 0002, iter [03200, 05004], lr: 0.099954, loss: 3.4222
2022-07-09 10:05:30 - train: epoch 0002, iter [03300, 05004], lr: 0.099953, loss: 3.5217
2022-07-09 10:06:04 - train: epoch 0002, iter [03400, 05004], lr: 0.099952, loss: 3.5526
2022-07-09 10:06:39 - train: epoch 0002, iter [03500, 05004], lr: 0.099951, loss: 3.3756
2022-07-09 10:07:14 - train: epoch 0002, iter [03600, 05004], lr: 0.099949, loss: 3.4737
2022-07-09 10:07:48 - train: epoch 0002, iter [03700, 05004], lr: 0.099948, loss: 3.6125
2022-07-09 10:08:22 - train: epoch 0002, iter [03800, 05004], lr: 0.099947, loss: 3.2796
2022-07-09 10:08:57 - train: epoch 0002, iter [03900, 05004], lr: 0.099946, loss: 3.4903
2022-07-09 10:09:32 - train: epoch 0002, iter [04000, 05004], lr: 0.099945, loss: 3.3503
2022-07-09 10:10:05 - train: epoch 0002, iter [04100, 05004], lr: 0.099943, loss: 3.5508
2022-07-09 10:10:40 - train: epoch 0002, iter [04200, 05004], lr: 0.099942, loss: 3.3016
2022-07-09 10:11:14 - train: epoch 0002, iter [04300, 05004], lr: 0.099941, loss: 3.3512
2022-07-09 10:11:49 - train: epoch 0002, iter [04400, 05004], lr: 0.099939, loss: 3.3591
2022-07-09 10:12:23 - train: epoch 0002, iter [04500, 05004], lr: 0.099938, loss: 3.2382
2022-07-09 10:12:58 - train: epoch 0002, iter [04600, 05004], lr: 0.099937, loss: 3.3242
2022-07-09 10:13:33 - train: epoch 0002, iter [04700, 05004], lr: 0.099936, loss: 3.2638
2022-07-09 10:14:07 - train: epoch 0002, iter [04800, 05004], lr: 0.099934, loss: 3.3723
2022-07-09 10:14:41 - train: epoch 0002, iter [04900, 05004], lr: 0.099933, loss: 3.2639
2022-07-09 10:15:14 - train: epoch 0002, iter [05000, 05004], lr: 0.099932, loss: 3.2587
2022-07-09 10:15:15 - train: epoch 002, train_loss: 3.6181
2022-07-09 10:16:31 - eval: epoch: 002, acc1: 33.216%, acc5: 59.528%, test_loss: 3.1032, per_image_load_time: 1.514ms, per_image_inference_time: 0.379ms
2022-07-09 10:16:31 - until epoch: 002, best_acc1: 33.216%
2022-07-09 10:16:31 - epoch 003 lr: 0.099931
2022-07-09 10:17:11 - train: epoch 0003, iter [00100, 05004], lr: 0.099930, loss: 3.3240
2022-07-09 10:17:45 - train: epoch 0003, iter [00200, 05004], lr: 0.099929, loss: 3.3545
2022-07-09 10:18:18 - train: epoch 0003, iter [00300, 05004], lr: 0.099927, loss: 3.1539
2022-07-09 10:18:53 - train: epoch 0003, iter [00400, 05004], lr: 0.099926, loss: 3.3055
2022-07-09 10:19:26 - train: epoch 0003, iter [00500, 05004], lr: 0.099924, loss: 3.3060
2022-07-09 10:20:00 - train: epoch 0003, iter [00600, 05004], lr: 0.099923, loss: 3.1295
2022-07-09 10:20:34 - train: epoch 0003, iter [00700, 05004], lr: 0.099922, loss: 3.3987
2022-07-09 10:21:08 - train: epoch 0003, iter [00800, 05004], lr: 0.099920, loss: 3.4076
2022-07-09 10:21:42 - train: epoch 0003, iter [00900, 05004], lr: 0.099919, loss: 3.1003
2022-07-09 10:22:15 - train: epoch 0003, iter [01000, 05004], lr: 0.099917, loss: 3.2515
2022-07-09 10:22:51 - train: epoch 0003, iter [01100, 05004], lr: 0.099916, loss: 3.1647
2022-07-09 10:23:24 - train: epoch 0003, iter [01200, 05004], lr: 0.099914, loss: 3.0707
2022-07-09 10:23:58 - train: epoch 0003, iter [01300, 05004], lr: 0.099913, loss: 3.1955
2022-07-09 10:24:32 - train: epoch 0003, iter [01400, 05004], lr: 0.099911, loss: 3.1555
2022-07-09 10:25:07 - train: epoch 0003, iter [01500, 05004], lr: 0.099909, loss: 3.4643
2022-07-09 10:25:41 - train: epoch 0003, iter [01600, 05004], lr: 0.099908, loss: 2.9894
2022-07-09 10:26:14 - train: epoch 0003, iter [01700, 05004], lr: 0.099906, loss: 3.0150
2022-07-09 10:26:49 - train: epoch 0003, iter [01800, 05004], lr: 0.099905, loss: 3.0079
2022-07-09 10:27:24 - train: epoch 0003, iter [01900, 05004], lr: 0.099903, loss: 3.1666
2022-07-09 10:27:57 - train: epoch 0003, iter [02000, 05004], lr: 0.099901, loss: 3.3980
2022-07-09 10:28:32 - train: epoch 0003, iter [02100, 05004], lr: 0.099900, loss: 3.4427
2022-07-09 10:29:06 - train: epoch 0003, iter [02200, 05004], lr: 0.099898, loss: 3.6642
2022-07-09 10:29:41 - train: epoch 0003, iter [02300, 05004], lr: 0.099896, loss: 3.0992
2022-07-09 10:30:16 - train: epoch 0003, iter [02400, 05004], lr: 0.099895, loss: 3.2169
2022-07-09 10:30:51 - train: epoch 0003, iter [02500, 05004], lr: 0.099893, loss: 3.1442
2022-07-09 10:31:26 - train: epoch 0003, iter [02600, 05004], lr: 0.099891, loss: 3.1703
2022-07-09 10:32:01 - train: epoch 0003, iter [02700, 05004], lr: 0.099890, loss: 3.4537
2022-07-09 10:32:36 - train: epoch 0003, iter [02800, 05004], lr: 0.099888, loss: 2.9206
2022-07-09 10:33:11 - train: epoch 0003, iter [02900, 05004], lr: 0.099886, loss: 3.1467
2022-07-09 10:33:46 - train: epoch 0003, iter [03000, 05004], lr: 0.099884, loss: 3.2527
2022-07-09 10:34:22 - train: epoch 0003, iter [03100, 05004], lr: 0.099882, loss: 3.1968
2022-07-09 10:34:55 - train: epoch 0003, iter [03200, 05004], lr: 0.099881, loss: 3.0892
2022-07-09 10:35:31 - train: epoch 0003, iter [03300, 05004], lr: 0.099879, loss: 3.1468
2022-07-09 10:36:06 - train: epoch 0003, iter [03400, 05004], lr: 0.099877, loss: 3.1546
2022-07-09 10:36:40 - train: epoch 0003, iter [03500, 05004], lr: 0.099875, loss: 2.8334
2022-07-09 10:37:15 - train: epoch 0003, iter [03600, 05004], lr: 0.099873, loss: 2.9135
2022-07-09 10:37:51 - train: epoch 0003, iter [03700, 05004], lr: 0.099871, loss: 2.9905
2022-07-09 10:38:25 - train: epoch 0003, iter [03800, 05004], lr: 0.099870, loss: 3.1375
2022-07-09 10:39:00 - train: epoch 0003, iter [03900, 05004], lr: 0.099868, loss: 3.3240
2022-07-09 10:39:35 - train: epoch 0003, iter [04000, 05004], lr: 0.099866, loss: 3.0958
2022-07-09 10:40:09 - train: epoch 0003, iter [04100, 05004], lr: 0.099864, loss: 3.0757
2022-07-09 10:40:44 - train: epoch 0003, iter [04200, 05004], lr: 0.099862, loss: 2.9527
2022-07-09 10:41:19 - train: epoch 0003, iter [04300, 05004], lr: 0.099860, loss: 2.7884
2022-07-09 10:41:54 - train: epoch 0003, iter [04400, 05004], lr: 0.099858, loss: 2.9318
2022-07-09 10:42:28 - train: epoch 0003, iter [04500, 05004], lr: 0.099856, loss: 2.9199
2022-07-09 10:43:04 - train: epoch 0003, iter [04600, 05004], lr: 0.099854, loss: 2.9328
2022-07-09 10:43:38 - train: epoch 0003, iter [04700, 05004], lr: 0.099852, loss: 2.8660
2022-07-09 10:44:14 - train: epoch 0003, iter [04800, 05004], lr: 0.099850, loss: 3.1040
2022-07-09 10:44:48 - train: epoch 0003, iter [04900, 05004], lr: 0.099848, loss: 2.9902
2022-07-09 10:45:22 - train: epoch 0003, iter [05000, 05004], lr: 0.099846, loss: 2.9940
2022-07-09 10:45:23 - train: epoch 003, train_loss: 3.1178
2022-07-09 10:46:39 - eval: epoch: 003, acc1: 38.776%, acc5: 65.214%, test_loss: 2.7946, per_image_load_time: 2.385ms, per_image_inference_time: 0.373ms
2022-07-09 10:46:40 - until epoch: 003, best_acc1: 38.776%
2022-07-09 10:46:40 - epoch 004 lr: 0.099846
2022-07-09 10:47:21 - train: epoch 0004, iter [00100, 05004], lr: 0.099844, loss: 3.0196
2022-07-09 10:47:56 - train: epoch 0004, iter [00200, 05004], lr: 0.099842, loss: 2.9531
2022-07-09 10:48:30 - train: epoch 0004, iter [00300, 05004], lr: 0.099840, loss: 2.9469
2022-07-09 10:49:05 - train: epoch 0004, iter [00400, 05004], lr: 0.099838, loss: 2.7795
2022-07-09 10:49:39 - train: epoch 0004, iter [00500, 05004], lr: 0.099835, loss: 2.8266
2022-07-09 10:50:14 - train: epoch 0004, iter [00600, 05004], lr: 0.099833, loss: 3.1422
2022-07-09 10:50:49 - train: epoch 0004, iter [00700, 05004], lr: 0.099831, loss: 3.0396
2022-07-09 10:51:24 - train: epoch 0004, iter [00800, 05004], lr: 0.099829, loss: 2.7639
2022-07-09 10:52:00 - train: epoch 0004, iter [00900, 05004], lr: 0.099827, loss: 2.6045
2022-07-09 10:52:35 - train: epoch 0004, iter [01000, 05004], lr: 0.099825, loss: 3.0283
2022-07-09 10:53:09 - train: epoch 0004, iter [01100, 05004], lr: 0.099822, loss: 2.9809
2022-07-09 10:53:44 - train: epoch 0004, iter [01200, 05004], lr: 0.099820, loss: 2.6405
2022-07-09 10:54:19 - train: epoch 0004, iter [01300, 05004], lr: 0.099818, loss: 2.7176
2022-07-09 10:54:53 - train: epoch 0004, iter [01400, 05004], lr: 0.099816, loss: 2.9831
2022-07-09 10:55:29 - train: epoch 0004, iter [01500, 05004], lr: 0.099814, loss: 3.0221
2022-07-09 10:56:03 - train: epoch 0004, iter [01600, 05004], lr: 0.099811, loss: 2.8885
2022-07-09 10:56:38 - train: epoch 0004, iter [01700, 05004], lr: 0.099809, loss: 2.9190
2022-07-09 10:57:14 - train: epoch 0004, iter [01800, 05004], lr: 0.099807, loss: 3.1529
2022-07-09 10:57:48 - train: epoch 0004, iter [01900, 05004], lr: 0.099804, loss: 2.9455
2022-07-09 10:58:23 - train: epoch 0004, iter [02000, 05004], lr: 0.099802, loss: 2.8706
2022-07-09 10:58:59 - train: epoch 0004, iter [02100, 05004], lr: 0.099800, loss: 2.9793
2022-07-09 10:59:34 - train: epoch 0004, iter [02200, 05004], lr: 0.099797, loss: 2.8933
2022-07-09 11:00:09 - train: epoch 0004, iter [02300, 05004], lr: 0.099795, loss: 2.8375
2022-07-09 11:00:43 - train: epoch 0004, iter [02400, 05004], lr: 0.099793, loss: 2.7975
2022-07-09 11:01:18 - train: epoch 0004, iter [02500, 05004], lr: 0.099790, loss: 2.9271
2022-07-09 11:01:53 - train: epoch 0004, iter [02600, 05004], lr: 0.099788, loss: 2.9626
2022-07-09 11:02:27 - train: epoch 0004, iter [02700, 05004], lr: 0.099785, loss: 2.6960
2022-07-09 11:03:02 - train: epoch 0004, iter [02800, 05004], lr: 0.099783, loss: 2.8102
2022-07-09 11:03:36 - train: epoch 0004, iter [02900, 05004], lr: 0.099781, loss: 2.7950
2022-07-09 11:04:11 - train: epoch 0004, iter [03000, 05004], lr: 0.099778, loss: 2.8523
2022-07-09 11:04:45 - train: epoch 0004, iter [03100, 05004], lr: 0.099776, loss: 2.9235
2022-07-09 11:05:21 - train: epoch 0004, iter [03200, 05004], lr: 0.099773, loss: 2.7995
2022-07-09 11:05:55 - train: epoch 0004, iter [03300, 05004], lr: 0.099771, loss: 3.0750
2022-07-09 11:06:31 - train: epoch 0004, iter [03400, 05004], lr: 0.099768, loss: 2.9405
2022-07-09 11:07:04 - train: epoch 0004, iter [03500, 05004], lr: 0.099766, loss: 2.7687
2022-07-09 11:07:40 - train: epoch 0004, iter [03600, 05004], lr: 0.099763, loss: 2.6476
2022-07-09 11:08:14 - train: epoch 0004, iter [03700, 05004], lr: 0.099761, loss: 2.8077
2022-07-09 11:08:50 - train: epoch 0004, iter [03800, 05004], lr: 0.099758, loss: 2.7548
2022-07-09 11:09:23 - train: epoch 0004, iter [03900, 05004], lr: 0.099755, loss: 2.8128
2022-07-09 11:09:58 - train: epoch 0004, iter [04000, 05004], lr: 0.099753, loss: 2.5415
2022-07-09 11:10:32 - train: epoch 0004, iter [04100, 05004], lr: 0.099750, loss: 2.8354
2022-07-09 11:11:07 - train: epoch 0004, iter [04200, 05004], lr: 0.099748, loss: 2.6895
2022-07-09 11:11:42 - train: epoch 0004, iter [04300, 05004], lr: 0.099745, loss: 2.6420
2022-07-09 11:12:16 - train: epoch 0004, iter [04400, 05004], lr: 0.099742, loss: 2.6526
2022-07-09 11:12:52 - train: epoch 0004, iter [04500, 05004], lr: 0.099740, loss: 2.2806
2022-07-09 11:13:26 - train: epoch 0004, iter [04600, 05004], lr: 0.099737, loss: 2.7714
2022-07-09 11:14:02 - train: epoch 0004, iter [04700, 05004], lr: 0.099734, loss: 2.7103
2022-07-09 11:14:36 - train: epoch 0004, iter [04800, 05004], lr: 0.099732, loss: 2.7079
2022-07-09 11:15:10 - train: epoch 0004, iter [04900, 05004], lr: 0.099729, loss: 2.8624
2022-07-09 11:15:44 - train: epoch 0004, iter [05000, 05004], lr: 0.099726, loss: 2.9339
2022-07-09 11:15:45 - train: epoch 004, train_loss: 2.8714
2022-07-09 11:17:02 - eval: epoch: 004, acc1: 41.738%, acc5: 67.828%, test_loss: 2.6449, per_image_load_time: 2.641ms, per_image_inference_time: 0.360ms
2022-07-09 11:17:03 - until epoch: 004, best_acc1: 41.738%
2022-07-09 11:17:03 - epoch 005 lr: 0.099726
2022-07-09 11:17:43 - train: epoch 0005, iter [00100, 05004], lr: 0.099723, loss: 2.8297
2022-07-09 11:18:18 - train: epoch 0005, iter [00200, 05004], lr: 0.099721, loss: 2.8654
2022-07-09 11:18:53 - train: epoch 0005, iter [00300, 05004], lr: 0.099718, loss: 2.9173
2022-07-09 11:19:27 - train: epoch 0005, iter [00400, 05004], lr: 0.099715, loss: 2.7930
2022-07-09 11:20:02 - train: epoch 0005, iter [00500, 05004], lr: 0.099712, loss: 2.5735
2022-07-09 11:20:36 - train: epoch 0005, iter [00600, 05004], lr: 0.099709, loss: 2.8151
2022-07-09 11:21:11 - train: epoch 0005, iter [00700, 05004], lr: 0.099707, loss: 2.8172
2022-07-09 11:21:47 - train: epoch 0005, iter [00800, 05004], lr: 0.099704, loss: 2.9913
2022-07-09 11:22:21 - train: epoch 0005, iter [00900, 05004], lr: 0.099701, loss: 2.7921
2022-07-09 11:22:56 - train: epoch 0005, iter [01000, 05004], lr: 0.099698, loss: 2.8202
2022-07-09 11:23:31 - train: epoch 0005, iter [01100, 05004], lr: 0.099695, loss: 2.8474
2022-07-09 11:24:05 - train: epoch 0005, iter [01200, 05004], lr: 0.099692, loss: 2.8129
2022-07-09 11:24:40 - train: epoch 0005, iter [01300, 05004], lr: 0.099689, loss: 2.7256
2022-07-09 11:25:13 - train: epoch 0005, iter [01400, 05004], lr: 0.099686, loss: 2.7963
2022-07-09 11:25:47 - train: epoch 0005, iter [01500, 05004], lr: 0.099684, loss: 2.5822
2022-07-09 11:26:20 - train: epoch 0005, iter [01600, 05004], lr: 0.099681, loss: 2.5444
2022-07-09 11:26:54 - train: epoch 0005, iter [01700, 05004], lr: 0.099678, loss: 2.6153
2022-07-09 11:27:28 - train: epoch 0005, iter [01800, 05004], lr: 0.099675, loss: 2.8127
2022-07-09 11:28:02 - train: epoch 0005, iter [01900, 05004], lr: 0.099672, loss: 2.5899
2022-07-09 11:28:35 - train: epoch 0005, iter [02000, 05004], lr: 0.099669, loss: 2.7318
2022-07-09 11:29:10 - train: epoch 0005, iter [02100, 05004], lr: 0.099666, loss: 2.5120
2022-07-09 11:29:44 - train: epoch 0005, iter [02200, 05004], lr: 0.099663, loss: 2.5929
2022-07-09 11:30:18 - train: epoch 0005, iter [02300, 05004], lr: 0.099660, loss: 2.5267
2022-07-09 11:30:53 - train: epoch 0005, iter [02400, 05004], lr: 0.099657, loss: 2.6312
2022-07-09 11:31:27 - train: epoch 0005, iter [02500, 05004], lr: 0.099653, loss: 2.8362
2022-07-09 11:32:01 - train: epoch 0005, iter [02600, 05004], lr: 0.099650, loss: 2.9460
2022-07-09 11:32:35 - train: epoch 0005, iter [02700, 05004], lr: 0.099647, loss: 2.9148
2022-07-09 11:33:08 - train: epoch 0005, iter [02800, 05004], lr: 0.099644, loss: 2.7475
2022-07-09 11:33:42 - train: epoch 0005, iter [02900, 05004], lr: 0.099641, loss: 2.5664
2022-07-09 11:34:16 - train: epoch 0005, iter [03000, 05004], lr: 0.099638, loss: 2.7287
2022-07-09 11:34:51 - train: epoch 0005, iter [03100, 05004], lr: 0.099635, loss: 2.7655
2022-07-09 11:35:24 - train: epoch 0005, iter [03200, 05004], lr: 0.099632, loss: 2.8761
2022-07-09 11:35:58 - train: epoch 0005, iter [03300, 05004], lr: 0.099628, loss: 2.4843
2022-07-09 11:36:32 - train: epoch 0005, iter [03400, 05004], lr: 0.099625, loss: 2.5464
2022-07-09 11:37:06 - train: epoch 0005, iter [03500, 05004], lr: 0.099622, loss: 2.6325
2022-07-09 11:37:40 - train: epoch 0005, iter [03600, 05004], lr: 0.099619, loss: 2.7982
2022-07-09 11:38:14 - train: epoch 0005, iter [03700, 05004], lr: 0.099616, loss: 2.6565
2022-07-09 11:38:49 - train: epoch 0005, iter [03800, 05004], lr: 0.099612, loss: 2.5057
2022-07-09 11:39:22 - train: epoch 0005, iter [03900, 05004], lr: 0.099609, loss: 2.9918
2022-07-09 11:39:57 - train: epoch 0005, iter [04000, 05004], lr: 0.099606, loss: 2.7876
2022-07-09 11:40:31 - train: epoch 0005, iter [04100, 05004], lr: 0.099603, loss: 2.5837
2022-07-09 11:41:05 - train: epoch 0005, iter [04200, 05004], lr: 0.099599, loss: 2.6545
2022-07-09 11:41:40 - train: epoch 0005, iter [04300, 05004], lr: 0.099596, loss: 2.5465
2022-07-09 11:42:14 - train: epoch 0005, iter [04400, 05004], lr: 0.099593, loss: 2.7540
2022-07-09 11:42:48 - train: epoch 0005, iter [04500, 05004], lr: 0.099589, loss: 2.7346
2022-07-09 11:43:23 - train: epoch 0005, iter [04600, 05004], lr: 0.099586, loss: 2.6551
2022-07-09 11:43:57 - train: epoch 0005, iter [04700, 05004], lr: 0.099583, loss: 2.4833
2022-07-09 11:44:31 - train: epoch 0005, iter [04800, 05004], lr: 0.099579, loss: 2.5241
2022-07-09 11:45:05 - train: epoch 0005, iter [04900, 05004], lr: 0.099576, loss: 2.8006
2022-07-09 11:45:37 - train: epoch 0005, iter [05000, 05004], lr: 0.099572, loss: 2.6138
2022-07-09 11:45:38 - train: epoch 005, train_loss: 2.7242
2022-07-09 11:46:53 - eval: epoch: 005, acc1: 45.022%, acc5: 71.486%, test_loss: 2.4283, per_image_load_time: 2.089ms, per_image_inference_time: 0.386ms
2022-07-09 11:46:54 - until epoch: 005, best_acc1: 45.022%
2022-07-09 11:46:54 - epoch 006 lr: 0.099572
2022-07-09 11:47:33 - train: epoch 0006, iter [00100, 05004], lr: 0.099569, loss: 2.6243
2022-07-09 11:48:06 - train: epoch 0006, iter [00200, 05004], lr: 0.099565, loss: 2.7118
2022-07-09 11:48:41 - train: epoch 0006, iter [00300, 05004], lr: 0.099562, loss: 2.4007
2022-07-09 11:49:15 - train: epoch 0006, iter [00400, 05004], lr: 0.099558, loss: 2.7717
2022-07-09 11:49:49 - train: epoch 0006, iter [00500, 05004], lr: 0.099555, loss: 2.5215
2022-07-09 11:50:22 - train: epoch 0006, iter [00600, 05004], lr: 0.099552, loss: 2.6848
2022-07-09 11:50:56 - train: epoch 0006, iter [00700, 05004], lr: 0.099548, loss: 2.6425
2022-07-09 11:51:31 - train: epoch 0006, iter [00800, 05004], lr: 0.099544, loss: 2.6568
2022-07-09 11:52:04 - train: epoch 0006, iter [00900, 05004], lr: 0.099541, loss: 2.5674
2022-07-09 11:52:39 - train: epoch 0006, iter [01000, 05004], lr: 0.099537, loss: 2.5127
2022-07-09 11:53:12 - train: epoch 0006, iter [01100, 05004], lr: 0.099534, loss: 2.6084
2022-07-09 11:53:46 - train: epoch 0006, iter [01200, 05004], lr: 0.099530, loss: 2.6000
2022-07-09 11:54:20 - train: epoch 0006, iter [01300, 05004], lr: 0.099527, loss: 2.7884
2022-07-09 11:54:54 - train: epoch 0006, iter [01400, 05004], lr: 0.099523, loss: 2.6697
2022-07-09 11:55:29 - train: epoch 0006, iter [01500, 05004], lr: 0.099520, loss: 2.7633
2022-07-09 11:56:03 - train: epoch 0006, iter [01600, 05004], lr: 0.099516, loss: 2.4401
2022-07-09 11:56:37 - train: epoch 0006, iter [01700, 05004], lr: 0.099512, loss: 2.8083
2022-07-09 11:57:11 - train: epoch 0006, iter [01800, 05004], lr: 0.099509, loss: 2.7839
2022-07-09 11:57:45 - train: epoch 0006, iter [01900, 05004], lr: 0.099505, loss: 2.5716
2022-07-09 11:58:19 - train: epoch 0006, iter [02000, 05004], lr: 0.099501, loss: 2.7426
2022-07-09 11:58:54 - train: epoch 0006, iter [02100, 05004], lr: 0.099498, loss: 2.7431
2022-07-09 11:59:28 - train: epoch 0006, iter [02200, 05004], lr: 0.099494, loss: 2.4944
2022-07-09 12:00:02 - train: epoch 0006, iter [02300, 05004], lr: 0.099490, loss: 2.5298
2022-07-09 12:00:37 - train: epoch 0006, iter [02400, 05004], lr: 0.099486, loss: 2.5828
2022-07-09 12:01:11 - train: epoch 0006, iter [02500, 05004], lr: 0.099483, loss: 2.8543
2022-07-09 12:01:45 - train: epoch 0006, iter [02600, 05004], lr: 0.099479, loss: 2.4133
2022-07-09 12:02:20 - train: epoch 0006, iter [02700, 05004], lr: 0.099475, loss: 2.7898
2022-07-09 12:02:54 - train: epoch 0006, iter [02800, 05004], lr: 0.099471, loss: 2.4886
2022-07-09 12:03:28 - train: epoch 0006, iter [02900, 05004], lr: 0.099468, loss: 2.7581
2022-07-09 12:04:02 - train: epoch 0006, iter [03000, 05004], lr: 0.099464, loss: 2.5696
2022-07-09 12:04:36 - train: epoch 0006, iter [03100, 05004], lr: 0.099460, loss: 2.4035
2022-07-09 12:05:10 - train: epoch 0006, iter [03200, 05004], lr: 0.099456, loss: 2.5822
2022-07-09 12:05:44 - train: epoch 0006, iter [03300, 05004], lr: 0.099452, loss: 2.4784
2022-07-09 12:06:18 - train: epoch 0006, iter [03400, 05004], lr: 0.099448, loss: 2.8125
2022-07-09 12:06:52 - train: epoch 0006, iter [03500, 05004], lr: 0.099444, loss: 2.7180
2022-07-09 12:07:26 - train: epoch 0006, iter [03600, 05004], lr: 0.099441, loss: 2.6097
2022-07-09 12:08:00 - train: epoch 0006, iter [03700, 05004], lr: 0.099437, loss: 2.6601
2022-07-09 12:08:35 - train: epoch 0006, iter [03800, 05004], lr: 0.099433, loss: 2.5438
2022-07-09 12:09:09 - train: epoch 0006, iter [03900, 05004], lr: 0.099429, loss: 2.5223
2022-07-09 12:09:43 - train: epoch 0006, iter [04000, 05004], lr: 0.099425, loss: 2.9159
2022-07-09 12:10:17 - train: epoch 0006, iter [04100, 05004], lr: 0.099421, loss: 2.5177
2022-07-09 12:10:52 - train: epoch 0006, iter [04200, 05004], lr: 0.099417, loss: 2.4549
2022-07-09 12:11:26 - train: epoch 0006, iter [04300, 05004], lr: 0.099413, loss: 2.5534
2022-07-09 12:12:01 - train: epoch 0006, iter [04400, 05004], lr: 0.099409, loss: 2.5935
2022-07-09 12:12:34 - train: epoch 0006, iter [04500, 05004], lr: 0.099405, loss: 2.6080
2022-07-09 12:13:08 - train: epoch 0006, iter [04600, 05004], lr: 0.099401, loss: 2.5621
2022-07-09 12:13:42 - train: epoch 0006, iter [04700, 05004], lr: 0.099397, loss: 2.5770
2022-07-09 12:14:16 - train: epoch 0006, iter [04800, 05004], lr: 0.099393, loss: 2.4725
2022-07-09 12:14:51 - train: epoch 0006, iter [04900, 05004], lr: 0.099389, loss: 2.5723
2022-07-09 12:15:24 - train: epoch 0006, iter [05000, 05004], lr: 0.099385, loss: 2.5358
2022-07-09 12:15:25 - train: epoch 006, train_loss: 2.6273
2022-07-09 12:16:40 - eval: epoch: 006, acc1: 48.018%, acc5: 73.636%, test_loss: 2.3001, per_image_load_time: 1.327ms, per_image_inference_time: 0.382ms
2022-07-09 12:16:40 - until epoch: 006, best_acc1: 48.018%
2022-07-09 12:16:40 - epoch 007 lr: 0.099384
2022-07-09 12:17:19 - train: epoch 0007, iter [00100, 05004], lr: 0.099380, loss: 2.5317
2022-07-09 12:17:53 - train: epoch 0007, iter [00200, 05004], lr: 0.099376, loss: 2.7827
2022-07-09 12:18:27 - train: epoch 0007, iter [00300, 05004], lr: 0.099372, loss: 2.7880
2022-07-09 12:19:01 - train: epoch 0007, iter [00400, 05004], lr: 0.099368, loss: 2.6592
2022-07-09 12:19:35 - train: epoch 0007, iter [00500, 05004], lr: 0.099364, loss: 2.5323
2022-07-09 12:20:09 - train: epoch 0007, iter [00600, 05004], lr: 0.099360, loss: 2.7170
2022-07-09 12:20:42 - train: epoch 0007, iter [00700, 05004], lr: 0.099355, loss: 2.6064
2022-07-09 12:21:16 - train: epoch 0007, iter [00800, 05004], lr: 0.099351, loss: 2.5683
2022-07-09 12:21:51 - train: epoch 0007, iter [00900, 05004], lr: 0.099347, loss: 2.6260
2022-07-09 12:22:24 - train: epoch 0007, iter [01000, 05004], lr: 0.099343, loss: 2.5474
2022-07-09 12:22:58 - train: epoch 0007, iter [01100, 05004], lr: 0.099339, loss: 2.4220
2022-07-09 12:23:32 - train: epoch 0007, iter [01200, 05004], lr: 0.099334, loss: 2.4967
2022-07-09 12:24:06 - train: epoch 0007, iter [01300, 05004], lr: 0.099330, loss: 2.4420
2022-07-09 12:24:39 - train: epoch 0007, iter [01400, 05004], lr: 0.099326, loss: 2.6300
2022-07-09 12:25:13 - train: epoch 0007, iter [01500, 05004], lr: 0.099322, loss: 2.7035
2022-07-09 12:25:47 - train: epoch 0007, iter [01600, 05004], lr: 0.099317, loss: 2.5302
2022-07-09 12:26:21 - train: epoch 0007, iter [01700, 05004], lr: 0.099313, loss: 2.5893
2022-07-09 12:26:55 - train: epoch 0007, iter [01800, 05004], lr: 0.099309, loss: 2.4338
2022-07-09 12:27:29 - train: epoch 0007, iter [01900, 05004], lr: 0.099304, loss: 2.5734
2022-07-09 12:28:04 - train: epoch 0007, iter [02000, 05004], lr: 0.099300, loss: 2.3778
2022-07-09 12:28:37 - train: epoch 0007, iter [02100, 05004], lr: 0.099296, loss: 2.5892
2022-07-09 12:29:12 - train: epoch 0007, iter [02200, 05004], lr: 0.099291, loss: 2.4381
2022-07-09 12:29:46 - train: epoch 0007, iter [02300, 05004], lr: 0.099287, loss: 2.7005
2022-07-09 12:30:20 - train: epoch 0007, iter [02400, 05004], lr: 0.099282, loss: 2.5223
2022-07-09 12:30:53 - train: epoch 0007, iter [02500, 05004], lr: 0.099278, loss: 2.4423
2022-07-09 12:31:28 - train: epoch 0007, iter [02600, 05004], lr: 0.099273, loss: 2.5649
2022-07-09 12:32:02 - train: epoch 0007, iter [02700, 05004], lr: 0.099269, loss: 2.4962
2022-07-09 12:32:37 - train: epoch 0007, iter [02800, 05004], lr: 0.099265, loss: 2.6029
2022-07-09 12:33:10 - train: epoch 0007, iter [02900, 05004], lr: 0.099260, loss: 2.5577
2022-07-09 12:33:45 - train: epoch 0007, iter [03000, 05004], lr: 0.099256, loss: 2.6561
2022-07-09 12:34:19 - train: epoch 0007, iter [03100, 05004], lr: 0.099251, loss: 2.3429
2022-07-09 12:34:54 - train: epoch 0007, iter [03200, 05004], lr: 0.099247, loss: 2.4782
2022-07-09 12:35:27 - train: epoch 0007, iter [03300, 05004], lr: 0.099242, loss: 2.8197
2022-07-09 12:36:01 - train: epoch 0007, iter [03400, 05004], lr: 0.099237, loss: 2.4203
2022-07-09 12:36:35 - train: epoch 0007, iter [03500, 05004], lr: 0.099233, loss: 2.6709
2022-07-09 12:37:10 - train: epoch 0007, iter [03600, 05004], lr: 0.099228, loss: 2.3651
2022-07-09 12:37:43 - train: epoch 0007, iter [03700, 05004], lr: 0.099224, loss: 2.6587
2022-07-09 12:38:18 - train: epoch 0007, iter [03800, 05004], lr: 0.099219, loss: 2.7462
2022-07-09 12:38:51 - train: epoch 0007, iter [03900, 05004], lr: 0.099215, loss: 2.4234
2022-07-09 12:39:26 - train: epoch 0007, iter [04000, 05004], lr: 0.099210, loss: 2.5762
2022-07-09 12:39:59 - train: epoch 0007, iter [04100, 05004], lr: 0.099205, loss: 2.4944
2022-07-09 12:40:33 - train: epoch 0007, iter [04200, 05004], lr: 0.099201, loss: 2.3584
2022-07-09 12:41:08 - train: epoch 0007, iter [04300, 05004], lr: 0.099196, loss: 2.6047
2022-07-09 12:41:42 - train: epoch 0007, iter [04400, 05004], lr: 0.099191, loss: 2.3363
2022-07-09 12:42:16 - train: epoch 0007, iter [04500, 05004], lr: 0.099187, loss: 2.7689
2022-07-09 12:42:50 - train: epoch 0007, iter [04600, 05004], lr: 0.099182, loss: 2.6092
2022-07-09 12:43:24 - train: epoch 0007, iter [04700, 05004], lr: 0.099177, loss: 2.6596
2022-07-09 12:43:59 - train: epoch 0007, iter [04800, 05004], lr: 0.099172, loss: 2.8390
2022-07-09 12:44:33 - train: epoch 0007, iter [04900, 05004], lr: 0.099168, loss: 2.5535
2022-07-09 12:45:06 - train: epoch 0007, iter [05000, 05004], lr: 0.099163, loss: 2.5429
2022-07-09 12:45:07 - train: epoch 007, train_loss: 2.5538
2022-07-09 12:46:23 - eval: epoch: 007, acc1: 48.976%, acc5: 74.778%, test_loss: 2.2256, per_image_load_time: 1.354ms, per_image_inference_time: 0.383ms
2022-07-09 12:46:23 - until epoch: 007, best_acc1: 48.976%
2022-07-09 12:46:23 - epoch 008 lr: 0.099163
2022-07-09 12:47:03 - train: epoch 0008, iter [00100, 05004], lr: 0.099158, loss: 2.5385
2022-07-09 12:47:35 - train: epoch 0008, iter [00200, 05004], lr: 0.099153, loss: 2.5308
2022-07-09 12:48:09 - train: epoch 0008, iter [00300, 05004], lr: 0.099148, loss: 2.3013
2022-07-09 12:48:43 - train: epoch 0008, iter [00400, 05004], lr: 0.099144, loss: 2.3082
2022-07-09 12:49:16 - train: epoch 0008, iter [00500, 05004], lr: 0.099139, loss: 2.3696
2022-07-09 12:49:50 - train: epoch 0008, iter [00600, 05004], lr: 0.099134, loss: 2.4875
2022-07-09 12:50:23 - train: epoch 0008, iter [00700, 05004], lr: 0.099129, loss: 2.8632
2022-07-09 12:50:58 - train: epoch 0008, iter [00800, 05004], lr: 0.099124, loss: 2.3364
2022-07-09 12:51:31 - train: epoch 0008, iter [00900, 05004], lr: 0.099119, loss: 2.4780
2022-07-09 12:52:04 - train: epoch 0008, iter [01000, 05004], lr: 0.099114, loss: 2.5497
2022-07-09 12:52:38 - train: epoch 0008, iter [01100, 05004], lr: 0.099109, loss: 2.3247
2022-07-09 12:53:13 - train: epoch 0008, iter [01200, 05004], lr: 0.099105, loss: 2.4114
2022-07-09 12:53:47 - train: epoch 0008, iter [01300, 05004], lr: 0.099100, loss: 2.4963
2022-07-09 12:54:20 - train: epoch 0008, iter [01400, 05004], lr: 0.099095, loss: 2.4508
2022-07-09 12:54:54 - train: epoch 0008, iter [01500, 05004], lr: 0.099090, loss: 2.6897
2022-07-09 12:55:29 - train: epoch 0008, iter [01600, 05004], lr: 0.099085, loss: 2.6143
2022-07-09 12:56:03 - train: epoch 0008, iter [01700, 05004], lr: 0.099080, loss: 2.3556
2022-07-09 12:56:37 - train: epoch 0008, iter [01800, 05004], lr: 0.099075, loss: 2.6812
2022-07-09 12:57:11 - train: epoch 0008, iter [01900, 05004], lr: 0.099070, loss: 2.2961
2022-07-09 12:57:45 - train: epoch 0008, iter [02000, 05004], lr: 0.099065, loss: 2.6348
2022-07-09 12:58:20 - train: epoch 0008, iter [02100, 05004], lr: 0.099060, loss: 2.5056
2022-07-09 12:58:53 - train: epoch 0008, iter [02200, 05004], lr: 0.099055, loss: 2.4345
2022-07-09 12:59:28 - train: epoch 0008, iter [02300, 05004], lr: 0.099050, loss: 2.5017
2022-07-09 13:00:01 - train: epoch 0008, iter [02400, 05004], lr: 0.099044, loss: 2.4246
2022-07-09 13:00:35 - train: epoch 0008, iter [02500, 05004], lr: 0.099039, loss: 2.4893
2022-07-09 13:01:09 - train: epoch 0008, iter [02600, 05004], lr: 0.099034, loss: 2.6790
2022-07-09 13:01:43 - train: epoch 0008, iter [02700, 05004], lr: 0.099029, loss: 2.6087
2022-07-09 13:02:17 - train: epoch 0008, iter [02800, 05004], lr: 0.099024, loss: 2.5761
2022-07-09 13:02:51 - train: epoch 0008, iter [02900, 05004], lr: 0.099019, loss: 2.3187
2022-07-09 13:03:25 - train: epoch 0008, iter [03000, 05004], lr: 0.099014, loss: 2.5709
2022-07-09 13:03:59 - train: epoch 0008, iter [03100, 05004], lr: 0.099009, loss: 2.4971
2022-07-09 13:04:33 - train: epoch 0008, iter [03200, 05004], lr: 0.099003, loss: 2.7229
2022-07-09 13:05:07 - train: epoch 0008, iter [03300, 05004], lr: 0.098998, loss: 2.7654
2022-07-09 13:05:41 - train: epoch 0008, iter [03400, 05004], lr: 0.098993, loss: 2.7455
2022-07-09 13:06:15 - train: epoch 0008, iter [03500, 05004], lr: 0.098988, loss: 2.4291
2022-07-09 13:06:49 - train: epoch 0008, iter [03600, 05004], lr: 0.098982, loss: 2.6397
2022-07-09 13:07:24 - train: epoch 0008, iter [03700, 05004], lr: 0.098977, loss: 2.4752
2022-07-09 13:07:58 - train: epoch 0008, iter [03800, 05004], lr: 0.098972, loss: 2.4536
2022-07-09 13:08:32 - train: epoch 0008, iter [03900, 05004], lr: 0.098967, loss: 2.5684
2022-07-09 13:09:06 - train: epoch 0008, iter [04000, 05004], lr: 0.098961, loss: 2.8129
2022-07-09 13:09:41 - train: epoch 0008, iter [04100, 05004], lr: 0.098956, loss: 2.3650
2022-07-09 13:10:15 - train: epoch 0008, iter [04200, 05004], lr: 0.098951, loss: 2.4211
2022-07-09 13:10:50 - train: epoch 0008, iter [04300, 05004], lr: 0.098945, loss: 2.2458
2022-07-09 13:11:23 - train: epoch 0008, iter [04400, 05004], lr: 0.098940, loss: 2.4294
2022-07-09 13:11:57 - train: epoch 0008, iter [04500, 05004], lr: 0.098935, loss: 2.7065
2022-07-09 13:12:32 - train: epoch 0008, iter [04600, 05004], lr: 0.098929, loss: 2.6198
2022-07-09 13:13:06 - train: epoch 0008, iter [04700, 05004], lr: 0.098924, loss: 2.4535
2022-07-09 13:13:40 - train: epoch 0008, iter [04800, 05004], lr: 0.098918, loss: 2.5848
2022-07-09 13:14:15 - train: epoch 0008, iter [04900, 05004], lr: 0.098913, loss: 2.5910
2022-07-09 13:14:47 - train: epoch 0008, iter [05000, 05004], lr: 0.098908, loss: 2.4510
2022-07-09 13:14:48 - train: epoch 008, train_loss: 2.5005
2022-07-09 13:16:04 - eval: epoch: 008, acc1: 49.130%, acc5: 75.078%, test_loss: 2.2316, per_image_load_time: 1.316ms, per_image_inference_time: 0.342ms
2022-07-09 13:16:04 - until epoch: 008, best_acc1: 49.130%
2022-07-09 13:16:04 - epoch 009 lr: 0.098907
2022-07-09 13:16:44 - train: epoch 0009, iter [00100, 05004], lr: 0.098902, loss: 2.2469
2022-07-09 13:17:18 - train: epoch 0009, iter [00200, 05004], lr: 0.098896, loss: 2.3912
2022-07-09 13:17:52 - train: epoch 0009, iter [00300, 05004], lr: 0.098891, loss: 2.2392
2022-07-09 13:18:26 - train: epoch 0009, iter [00400, 05004], lr: 0.098886, loss: 2.6607
2022-07-09 13:18:59 - train: epoch 0009, iter [00500, 05004], lr: 0.098880, loss: 2.3955
2022-07-09 13:19:33 - train: epoch 0009, iter [00600, 05004], lr: 0.098875, loss: 2.4691
2022-07-09 13:20:07 - train: epoch 0009, iter [00700, 05004], lr: 0.098869, loss: 2.3942
2022-07-09 13:20:42 - train: epoch 0009, iter [00800, 05004], lr: 0.098863, loss: 2.3342
2022-07-09 13:21:16 - train: epoch 0009, iter [00900, 05004], lr: 0.098858, loss: 2.3109
2022-07-09 13:21:50 - train: epoch 0009, iter [01000, 05004], lr: 0.098852, loss: 2.3408
2022-07-09 13:22:23 - train: epoch 0009, iter [01100, 05004], lr: 0.098847, loss: 2.7559
2022-07-09 13:22:57 - train: epoch 0009, iter [01200, 05004], lr: 0.098841, loss: 2.6372
2022-07-09 13:23:31 - train: epoch 0009, iter [01300, 05004], lr: 0.098836, loss: 2.5732
2022-07-09 13:24:05 - train: epoch 0009, iter [01400, 05004], lr: 0.098830, loss: 2.1750
2022-07-09 13:24:39 - train: epoch 0009, iter [01500, 05004], lr: 0.098824, loss: 2.2741
2022-07-09 13:25:13 - train: epoch 0009, iter [01600, 05004], lr: 0.098819, loss: 2.4540
2022-07-09 13:25:48 - train: epoch 0009, iter [01700, 05004], lr: 0.098813, loss: 2.6533
2022-07-09 13:26:22 - train: epoch 0009, iter [01800, 05004], lr: 0.098807, loss: 2.3703
2022-07-09 13:26:56 - train: epoch 0009, iter [01900, 05004], lr: 0.098802, loss: 2.2420
2022-07-09 13:27:30 - train: epoch 0009, iter [02000, 05004], lr: 0.098796, loss: 2.1526
2022-07-09 13:28:03 - train: epoch 0009, iter [02100, 05004], lr: 0.098790, loss: 2.5120
2022-07-09 13:28:38 - train: epoch 0009, iter [02200, 05004], lr: 0.098784, loss: 2.5362
2022-07-09 13:29:12 - train: epoch 0009, iter [02300, 05004], lr: 0.098779, loss: 2.2198
2022-07-09 13:29:46 - train: epoch 0009, iter [02400, 05004], lr: 0.098773, loss: 2.4107
2022-07-09 13:30:20 - train: epoch 0009, iter [02500, 05004], lr: 0.098767, loss: 2.2995
2022-07-09 13:30:54 - train: epoch 0009, iter [02600, 05004], lr: 0.098761, loss: 2.4999
2022-07-09 13:31:28 - train: epoch 0009, iter [02700, 05004], lr: 0.098756, loss: 2.3647
2022-07-09 13:32:03 - train: epoch 0009, iter [02800, 05004], lr: 0.098750, loss: 2.5203
2022-07-09 13:32:37 - train: epoch 0009, iter [02900, 05004], lr: 0.098744, loss: 2.1163
2022-07-09 13:33:11 - train: epoch 0009, iter [03000, 05004], lr: 0.098738, loss: 2.2842
2022-07-09 13:33:46 - train: epoch 0009, iter [03100, 05004], lr: 0.098732, loss: 2.6085
2022-07-09 13:34:20 - train: epoch 0009, iter [03200, 05004], lr: 0.098726, loss: 2.5238
2022-07-09 13:34:54 - train: epoch 0009, iter [03300, 05004], lr: 0.098721, loss: 2.4361
2022-07-09 13:35:29 - train: epoch 0009, iter [03400, 05004], lr: 0.098715, loss: 2.6978
2022-07-09 13:36:02 - train: epoch 0009, iter [03500, 05004], lr: 0.098709, loss: 2.5956
2022-07-09 13:36:37 - train: epoch 0009, iter [03600, 05004], lr: 0.098703, loss: 2.2715
2022-07-09 13:37:10 - train: epoch 0009, iter [03700, 05004], lr: 0.098697, loss: 2.5969
2022-07-09 13:37:45 - train: epoch 0009, iter [03800, 05004], lr: 0.098691, loss: 2.6706
2022-07-09 13:38:19 - train: epoch 0009, iter [03900, 05004], lr: 0.098685, loss: 2.1248
2022-07-09 13:38:54 - train: epoch 0009, iter [04000, 05004], lr: 0.098679, loss: 2.5917
2022-07-09 13:39:28 - train: epoch 0009, iter [04100, 05004], lr: 0.098673, loss: 2.4571
2022-07-09 13:40:02 - train: epoch 0009, iter [04200, 05004], lr: 0.098667, loss: 2.3497
2022-07-09 13:40:36 - train: epoch 0009, iter [04300, 05004], lr: 0.098661, loss: 2.6124
2022-07-09 13:41:10 - train: epoch 0009, iter [04400, 05004], lr: 0.098655, loss: 2.4321
2022-07-09 13:41:45 - train: epoch 0009, iter [04500, 05004], lr: 0.098649, loss: 2.3483
2022-07-09 13:42:19 - train: epoch 0009, iter [04600, 05004], lr: 0.098643, loss: 2.4587
2022-07-09 13:42:53 - train: epoch 0009, iter [04700, 05004], lr: 0.098637, loss: 2.6271
2022-07-09 13:43:27 - train: epoch 0009, iter [04800, 05004], lr: 0.098631, loss: 2.6634
2022-07-09 13:44:01 - train: epoch 0009, iter [04900, 05004], lr: 0.098625, loss: 2.5304
2022-07-09 13:44:34 - train: epoch 0009, iter [05000, 05004], lr: 0.098619, loss: 2.3894
2022-07-09 13:44:35 - train: epoch 009, train_loss: 2.4567
2022-07-09 13:45:50 - eval: epoch: 009, acc1: 50.916%, acc5: 76.354%, test_loss: 2.1455, per_image_load_time: 1.075ms, per_image_inference_time: 0.371ms
2022-07-09 13:45:51 - until epoch: 009, best_acc1: 50.916%
2022-07-09 13:45:51 - epoch 010 lr: 0.098618
2022-07-09 13:46:30 - train: epoch 0010, iter [00100, 05004], lr: 0.098612, loss: 2.4115
2022-07-09 13:47:04 - train: epoch 0010, iter [00200, 05004], lr: 0.098606, loss: 2.4984
2022-07-09 13:47:37 - train: epoch 0010, iter [00300, 05004], lr: 0.098600, loss: 2.3335
2022-07-09 13:48:13 - train: epoch 0010, iter [00400, 05004], lr: 0.098594, loss: 2.6136
2022-07-09 13:48:46 - train: epoch 0010, iter [00500, 05004], lr: 0.098588, loss: 2.3080
2022-07-09 13:49:20 - train: epoch 0010, iter [00600, 05004], lr: 0.098582, loss: 2.5208
2022-07-09 13:49:54 - train: epoch 0010, iter [00700, 05004], lr: 0.098575, loss: 2.5598
2022-07-09 13:50:28 - train: epoch 0010, iter [00800, 05004], lr: 0.098569, loss: 2.3876
2022-07-09 13:51:02 - train: epoch 0010, iter [00900, 05004], lr: 0.098563, loss: 2.2332
2022-07-09 13:51:36 - train: epoch 0010, iter [01000, 05004], lr: 0.098557, loss: 2.2660
2022-07-09 13:52:11 - train: epoch 0010, iter [01100, 05004], lr: 0.098551, loss: 2.4478
2022-07-09 13:52:44 - train: epoch 0010, iter [01200, 05004], lr: 0.098544, loss: 2.2482
2022-07-09 13:53:18 - train: epoch 0010, iter [01300, 05004], lr: 0.098538, loss: 2.2891
2022-07-09 13:53:52 - train: epoch 0010, iter [01400, 05004], lr: 0.098532, loss: 2.4905
2022-07-09 13:54:26 - train: epoch 0010, iter [01500, 05004], lr: 0.098525, loss: 2.1916
2022-07-09 13:55:00 - train: epoch 0010, iter [01600, 05004], lr: 0.098519, loss: 2.4524
2022-07-09 13:55:34 - train: epoch 0010, iter [01700, 05004], lr: 0.098513, loss: 2.5201
2022-07-09 13:56:08 - train: epoch 0010, iter [01800, 05004], lr: 0.098506, loss: 2.4026
2022-07-09 13:56:42 - train: epoch 0010, iter [01900, 05004], lr: 0.098500, loss: 2.4016
2022-07-09 13:57:16 - train: epoch 0010, iter [02000, 05004], lr: 0.098494, loss: 2.4890
2022-07-09 13:57:49 - train: epoch 0010, iter [02100, 05004], lr: 0.098487, loss: 2.3019
2022-07-09 13:58:23 - train: epoch 0010, iter [02200, 05004], lr: 0.098481, loss: 2.6070
2022-07-09 13:58:58 - train: epoch 0010, iter [02300, 05004], lr: 0.098475, loss: 2.6691
2022-07-09 13:59:32 - train: epoch 0010, iter [02400, 05004], lr: 0.098468, loss: 2.5925
2022-07-09 14:00:06 - train: epoch 0010, iter [02500, 05004], lr: 0.098462, loss: 2.4346
2022-07-09 14:00:39 - train: epoch 0010, iter [02600, 05004], lr: 0.098455, loss: 2.4668
2022-07-09 14:01:14 - train: epoch 0010, iter [02700, 05004], lr: 0.098449, loss: 2.2846
2022-07-09 14:01:48 - train: epoch 0010, iter [02800, 05004], lr: 0.098442, loss: 2.4034
2022-07-09 14:02:21 - train: epoch 0010, iter [02900, 05004], lr: 0.098436, loss: 2.5399
2022-07-09 14:02:55 - train: epoch 0010, iter [03000, 05004], lr: 0.098429, loss: 2.3476
2022-07-09 14:03:29 - train: epoch 0010, iter [03100, 05004], lr: 0.098423, loss: 2.6805
2022-07-09 14:04:03 - train: epoch 0010, iter [03200, 05004], lr: 0.098416, loss: 2.3604
2022-07-09 14:04:37 - train: epoch 0010, iter [03300, 05004], lr: 0.098410, loss: 2.5646
2022-07-09 14:05:12 - train: epoch 0010, iter [03400, 05004], lr: 0.098403, loss: 2.6653
2022-07-09 14:05:45 - train: epoch 0010, iter [03500, 05004], lr: 0.098397, loss: 2.6300
2022-07-09 14:06:19 - train: epoch 0010, iter [03600, 05004], lr: 0.098390, loss: 2.6672
2022-07-09 14:06:54 - train: epoch 0010, iter [03700, 05004], lr: 0.098383, loss: 2.2665
2022-07-09 14:07:28 - train: epoch 0010, iter [03800, 05004], lr: 0.098377, loss: 2.4365
2022-07-09 14:08:02 - train: epoch 0010, iter [03900, 05004], lr: 0.098370, loss: 2.1099
2022-07-09 14:08:36 - train: epoch 0010, iter [04000, 05004], lr: 0.098364, loss: 2.3228
2022-07-09 14:09:10 - train: epoch 0010, iter [04100, 05004], lr: 0.098357, loss: 2.2213
2022-07-09 14:09:44 - train: epoch 0010, iter [04200, 05004], lr: 0.098350, loss: 2.4912
2022-07-09 14:10:18 - train: epoch 0010, iter [04300, 05004], lr: 0.098344, loss: 2.3808
2022-07-09 14:10:53 - train: epoch 0010, iter [04400, 05004], lr: 0.098337, loss: 2.4568
2022-07-09 14:11:28 - train: epoch 0010, iter [04500, 05004], lr: 0.098330, loss: 2.2404
2022-07-09 14:12:01 - train: epoch 0010, iter [04600, 05004], lr: 0.098324, loss: 2.4084
2022-07-09 14:12:36 - train: epoch 0010, iter [04700, 05004], lr: 0.098317, loss: 2.4555
2022-07-09 14:13:10 - train: epoch 0010, iter [04800, 05004], lr: 0.098310, loss: 2.3914
2022-07-09 14:13:44 - train: epoch 0010, iter [04900, 05004], lr: 0.098303, loss: 2.3676
2022-07-09 14:14:17 - train: epoch 0010, iter [05000, 05004], lr: 0.098297, loss: 2.1839
2022-07-09 14:14:18 - train: epoch 010, train_loss: 2.4200
2022-07-09 14:15:34 - eval: epoch: 010, acc1: 51.502%, acc5: 76.990%, test_loss: 2.0944, per_image_load_time: 2.379ms, per_image_inference_time: 0.348ms
2022-07-09 14:15:34 - until epoch: 010, best_acc1: 51.502%
2022-07-09 14:15:34 - epoch 011 lr: 0.098296
2022-07-09 14:16:12 - train: epoch 0011, iter [00100, 05004], lr: 0.098290, loss: 2.2215
2022-07-09 14:16:47 - train: epoch 0011, iter [00200, 05004], lr: 0.098283, loss: 2.5538
2022-07-09 14:17:21 - train: epoch 0011, iter [00300, 05004], lr: 0.098276, loss: 2.2342
2022-07-09 14:17:54 - train: epoch 0011, iter [00400, 05004], lr: 0.098269, loss: 2.4095
2022-07-09 14:18:28 - train: epoch 0011, iter [00500, 05004], lr: 0.098262, loss: 2.3938
2022-07-09 14:19:01 - train: epoch 0011, iter [00600, 05004], lr: 0.098255, loss: 2.4335
2022-07-09 14:19:35 - train: epoch 0011, iter [00700, 05004], lr: 0.098249, loss: 2.3993
2022-07-09 14:20:09 - train: epoch 0011, iter [00800, 05004], lr: 0.098242, loss: 2.4236
2022-07-09 14:20:43 - train: epoch 0011, iter [00900, 05004], lr: 0.098235, loss: 2.5567
2022-07-09 14:21:17 - train: epoch 0011, iter [01000, 05004], lr: 0.098228, loss: 2.3749
2022-07-09 14:21:51 - train: epoch 0011, iter [01100, 05004], lr: 0.098221, loss: 2.4888
2022-07-09 14:22:25 - train: epoch 0011, iter [01200, 05004], lr: 0.098214, loss: 2.7749
2022-07-09 14:22:59 - train: epoch 0011, iter [01300, 05004], lr: 0.098207, loss: 2.5451
2022-07-09 14:23:32 - train: epoch 0011, iter [01400, 05004], lr: 0.098200, loss: 2.4330
2022-07-09 14:24:07 - train: epoch 0011, iter [01500, 05004], lr: 0.098193, loss: 2.2619
2022-07-09 14:24:40 - train: epoch 0011, iter [01600, 05004], lr: 0.098186, loss: 2.3505
2022-07-09 14:25:14 - train: epoch 0011, iter [01700, 05004], lr: 0.098179, loss: 2.3641
2022-07-09 14:25:48 - train: epoch 0011, iter [01800, 05004], lr: 0.098172, loss: 2.2147
2022-07-09 14:26:23 - train: epoch 0011, iter [01900, 05004], lr: 0.098165, loss: 2.1306
2022-07-09 14:26:57 - train: epoch 0011, iter [02000, 05004], lr: 0.098158, loss: 2.4021
2022-07-09 14:27:31 - train: epoch 0011, iter [02100, 05004], lr: 0.098151, loss: 2.3117
2022-07-09 14:28:05 - train: epoch 0011, iter [02200, 05004], lr: 0.098144, loss: 2.3133
2022-07-09 14:28:39 - train: epoch 0011, iter [02300, 05004], lr: 0.098137, loss: 2.6049
2022-07-09 14:29:13 - train: epoch 0011, iter [02400, 05004], lr: 0.098130, loss: 2.1902
2022-07-09 14:29:47 - train: epoch 0011, iter [02500, 05004], lr: 0.098123, loss: 2.6301
2022-07-09 14:30:21 - train: epoch 0011, iter [02600, 05004], lr: 0.098116, loss: 2.3944
2022-07-09 14:30:56 - train: epoch 0011, iter [02700, 05004], lr: 0.098109, loss: 2.4191
2022-07-09 14:31:30 - train: epoch 0011, iter [02800, 05004], lr: 0.098102, loss: 1.9976
2022-07-09 14:32:05 - train: epoch 0011, iter [02900, 05004], lr: 0.098094, loss: 2.3965
2022-07-09 14:32:39 - train: epoch 0011, iter [03000, 05004], lr: 0.098087, loss: 2.6123
2022-07-09 14:33:12 - train: epoch 0011, iter [03100, 05004], lr: 0.098080, loss: 2.4500
2022-07-09 14:33:46 - train: epoch 0011, iter [03200, 05004], lr: 0.098073, loss: 2.0965
2022-07-09 14:34:20 - train: epoch 0011, iter [03300, 05004], lr: 0.098066, loss: 2.4573
2022-07-09 14:34:54 - train: epoch 0011, iter [03400, 05004], lr: 0.098058, loss: 2.4341
2022-07-09 14:35:28 - train: epoch 0011, iter [03500, 05004], lr: 0.098051, loss: 2.4310
2022-07-09 14:36:03 - train: epoch 0011, iter [03600, 05004], lr: 0.098044, loss: 2.4337
2022-07-09 14:36:37 - train: epoch 0011, iter [03700, 05004], lr: 0.098037, loss: 2.5625
2022-07-09 14:37:11 - train: epoch 0011, iter [03800, 05004], lr: 0.098029, loss: 2.2297
2022-07-09 14:37:45 - train: epoch 0011, iter [03900, 05004], lr: 0.098022, loss: 2.3853
2022-07-09 14:38:19 - train: epoch 0011, iter [04000, 05004], lr: 0.098015, loss: 2.2967
2022-07-09 14:38:53 - train: epoch 0011, iter [04100, 05004], lr: 0.098008, loss: 2.1575
2022-07-09 14:39:27 - train: epoch 0011, iter [04200, 05004], lr: 0.098000, loss: 2.2855
2022-07-09 14:40:01 - train: epoch 0011, iter [04300, 05004], lr: 0.097993, loss: 2.3903
2022-07-09 14:40:36 - train: epoch 0011, iter [04400, 05004], lr: 0.097986, loss: 2.2810
2022-07-09 14:41:10 - train: epoch 0011, iter [04500, 05004], lr: 0.097978, loss: 2.2156
2022-07-09 14:41:43 - train: epoch 0011, iter [04600, 05004], lr: 0.097971, loss: 2.1821
2022-07-09 14:42:18 - train: epoch 0011, iter [04700, 05004], lr: 0.097964, loss: 2.1072
2022-07-09 14:42:52 - train: epoch 0011, iter [04800, 05004], lr: 0.097956, loss: 2.2538
2022-07-09 14:43:26 - train: epoch 0011, iter [04900, 05004], lr: 0.097949, loss: 2.1944
2022-07-09 14:43:59 - train: epoch 0011, iter [05000, 05004], lr: 0.097941, loss: 2.2977
2022-07-09 14:44:00 - train: epoch 011, train_loss: 2.3887
2022-07-09 14:45:15 - eval: epoch: 011, acc1: 51.624%, acc5: 76.884%, test_loss: 2.0974, per_image_load_time: 1.676ms, per_image_inference_time: 0.371ms
2022-07-09 14:45:15 - until epoch: 011, best_acc1: 51.624%
2022-07-09 14:45:15 - epoch 012 lr: 0.097941
2022-07-09 14:45:54 - train: epoch 0012, iter [00100, 05004], lr: 0.097934, loss: 2.3201
2022-07-09 14:46:28 - train: epoch 0012, iter [00200, 05004], lr: 0.097926, loss: 2.3016
2022-07-09 14:47:02 - train: epoch 0012, iter [00300, 05004], lr: 0.097919, loss: 2.3221
2022-07-09 14:47:37 - train: epoch 0012, iter [00400, 05004], lr: 0.097911, loss: 2.4762
2022-07-09 14:48:11 - train: epoch 0012, iter [00500, 05004], lr: 0.097904, loss: 2.6696
2022-07-09 14:48:44 - train: epoch 0012, iter [00600, 05004], lr: 0.097896, loss: 2.1538
2022-07-09 14:49:18 - train: epoch 0012, iter [00700, 05004], lr: 0.097889, loss: 2.2225
2022-07-09 14:49:53 - train: epoch 0012, iter [00800, 05004], lr: 0.097881, loss: 2.3711
2022-07-09 14:50:27 - train: epoch 0012, iter [00900, 05004], lr: 0.097874, loss: 2.4470
2022-07-09 14:51:01 - train: epoch 0012, iter [01000, 05004], lr: 0.097866, loss: 2.1679
2022-07-09 14:51:35 - train: epoch 0012, iter [01100, 05004], lr: 0.097858, loss: 2.6983
2022-07-09 14:52:09 - train: epoch 0012, iter [01200, 05004], lr: 0.097851, loss: 2.1921
2022-07-09 14:52:44 - train: epoch 0012, iter [01300, 05004], lr: 0.097843, loss: 2.2717
2022-07-09 14:53:18 - train: epoch 0012, iter [01400, 05004], lr: 0.097836, loss: 2.6734
2022-07-09 14:53:52 - train: epoch 0012, iter [01500, 05004], lr: 0.097828, loss: 2.1497
2022-07-09 14:54:27 - train: epoch 0012, iter [01600, 05004], lr: 0.097820, loss: 2.2783
2022-07-09 14:55:01 - train: epoch 0012, iter [01700, 05004], lr: 0.097813, loss: 2.1629
2022-07-09 14:55:34 - train: epoch 0012, iter [01800, 05004], lr: 0.097805, loss: 2.3876
2022-07-09 14:56:09 - train: epoch 0012, iter [01900, 05004], lr: 0.097797, loss: 2.3942
2022-07-09 14:56:43 - train: epoch 0012, iter [02000, 05004], lr: 0.097790, loss: 2.5311
2022-07-09 14:57:18 - train: epoch 0012, iter [02100, 05004], lr: 0.097782, loss: 2.3443
2022-07-09 14:57:52 - train: epoch 0012, iter [02200, 05004], lr: 0.097774, loss: 2.5277
2022-07-09 14:58:27 - train: epoch 0012, iter [02300, 05004], lr: 0.097767, loss: 2.3323
2022-07-09 14:59:01 - train: epoch 0012, iter [02400, 05004], lr: 0.097759, loss: 2.3339
2022-07-09 14:59:35 - train: epoch 0012, iter [02500, 05004], lr: 0.097751, loss: 2.1856
2022-07-09 15:00:09 - train: epoch 0012, iter [02600, 05004], lr: 0.097743, loss: 2.1101
2022-07-09 15:00:43 - train: epoch 0012, iter [02700, 05004], lr: 0.097736, loss: 2.2794
2022-07-09 15:01:18 - train: epoch 0012, iter [02800, 05004], lr: 0.097728, loss: 2.2814
2022-07-09 15:01:53 - train: epoch 0012, iter [02900, 05004], lr: 0.097720, loss: 2.1150
2022-07-09 15:02:27 - train: epoch 0012, iter [03000, 05004], lr: 0.097712, loss: 2.2754
2022-07-09 15:03:02 - train: epoch 0012, iter [03100, 05004], lr: 0.097704, loss: 2.4833
2022-07-09 15:03:36 - train: epoch 0012, iter [03200, 05004], lr: 0.097697, loss: 2.0541
2022-07-09 15:04:12 - train: epoch 0012, iter [03300, 05004], lr: 0.097689, loss: 2.3791
2022-07-09 15:04:45 - train: epoch 0012, iter [03400, 05004], lr: 0.097681, loss: 2.3259
2022-07-09 15:05:20 - train: epoch 0012, iter [03500, 05004], lr: 0.097673, loss: 2.4665
2022-07-09 15:05:54 - train: epoch 0012, iter [03600, 05004], lr: 0.097665, loss: 2.4685
2022-07-09 15:06:28 - train: epoch 0012, iter [03700, 05004], lr: 0.097657, loss: 2.3356
2022-07-09 15:07:03 - train: epoch 0012, iter [03800, 05004], lr: 0.097649, loss: 2.3738
2022-07-09 15:07:37 - train: epoch 0012, iter [03900, 05004], lr: 0.097641, loss: 2.2383
2022-07-09 15:08:11 - train: epoch 0012, iter [04000, 05004], lr: 0.097633, loss: 2.2844
2022-07-09 15:08:46 - train: epoch 0012, iter [04100, 05004], lr: 0.097625, loss: 2.2456
2022-07-09 15:09:20 - train: epoch 0012, iter [04200, 05004], lr: 0.097617, loss: 2.3046
2022-07-09 15:09:54 - train: epoch 0012, iter [04300, 05004], lr: 0.097609, loss: 2.5193
2022-07-09 15:10:29 - train: epoch 0012, iter [04400, 05004], lr: 0.097601, loss: 2.1821
2022-07-09 15:11:03 - train: epoch 0012, iter [04500, 05004], lr: 0.097593, loss: 2.1319
2022-07-09 15:11:38 - train: epoch 0012, iter [04600, 05004], lr: 0.097585, loss: 2.6418
2022-07-09 15:12:12 - train: epoch 0012, iter [04700, 05004], lr: 0.097577, loss: 2.3700
2022-07-09 15:12:47 - train: epoch 0012, iter [04800, 05004], lr: 0.097569, loss: 2.4955
2022-07-09 15:13:21 - train: epoch 0012, iter [04900, 05004], lr: 0.097561, loss: 2.2988
2022-07-09 15:13:54 - train: epoch 0012, iter [05000, 05004], lr: 0.097553, loss: 2.1172
2022-07-09 15:13:54 - train: epoch 012, train_loss: 2.3619
2022-07-09 15:15:11 - eval: epoch: 012, acc1: 52.476%, acc5: 77.404%, test_loss: 2.0595, per_image_load_time: 1.016ms, per_image_inference_time: 0.379ms
2022-07-09 15:15:11 - until epoch: 012, best_acc1: 52.476%
2022-07-09 15:15:11 - epoch 013 lr: 0.097553
2022-07-09 15:15:50 - train: epoch 0013, iter [00100, 05004], lr: 0.097545, loss: 2.1083
2022-07-09 15:16:25 - train: epoch 0013, iter [00200, 05004], lr: 0.097537, loss: 2.3061
2022-07-09 15:16:59 - train: epoch 0013, iter [00300, 05004], lr: 0.097529, loss: 2.2791
2022-07-09 15:17:34 - train: epoch 0013, iter [00400, 05004], lr: 0.097520, loss: 2.1788
2022-07-09 15:18:08 - train: epoch 0013, iter [00500, 05004], lr: 0.097512, loss: 2.2794
2022-07-09 15:18:42 - train: epoch 0013, iter [00600, 05004], lr: 0.097504, loss: 2.3298
2022-07-09 15:19:16 - train: epoch 0013, iter [00700, 05004], lr: 0.097496, loss: 2.2866
2022-07-09 15:19:50 - train: epoch 0013, iter [00800, 05004], lr: 0.097488, loss: 2.4546
2022-07-09 15:20:24 - train: epoch 0013, iter [00900, 05004], lr: 0.097480, loss: 2.3001
2022-07-09 15:20:58 - train: epoch 0013, iter [01000, 05004], lr: 0.097471, loss: 2.4081
2022-07-09 15:21:33 - train: epoch 0013, iter [01100, 05004], lr: 0.097463, loss: 2.3890
2022-07-09 15:22:07 - train: epoch 0013, iter [01200, 05004], lr: 0.097455, loss: 2.4978
2022-07-09 15:22:40 - train: epoch 0013, iter [01300, 05004], lr: 0.097447, loss: 2.4139
2022-07-09 15:23:15 - train: epoch 0013, iter [01400, 05004], lr: 0.097438, loss: 2.3237
2022-07-09 15:23:49 - train: epoch 0013, iter [01500, 05004], lr: 0.097430, loss: 2.5235
2022-07-09 15:24:22 - train: epoch 0013, iter [01600, 05004], lr: 0.097422, loss: 2.0617
2022-07-09 15:24:57 - train: epoch 0013, iter [01700, 05004], lr: 0.097414, loss: 2.3909
2022-07-09 15:25:31 - train: epoch 0013, iter [01800, 05004], lr: 0.097405, loss: 2.3621
2022-07-09 15:26:05 - train: epoch 0013, iter [01900, 05004], lr: 0.097397, loss: 2.4587
2022-07-09 15:26:39 - train: epoch 0013, iter [02000, 05004], lr: 0.097389, loss: 2.5886
2022-07-09 15:27:13 - train: epoch 0013, iter [02100, 05004], lr: 0.097380, loss: 2.5862
2022-07-09 15:27:48 - train: epoch 0013, iter [02200, 05004], lr: 0.097372, loss: 2.2332
2022-07-09 15:28:22 - train: epoch 0013, iter [02300, 05004], lr: 0.097363, loss: 2.3985
2022-07-09 15:28:56 - train: epoch 0013, iter [02400, 05004], lr: 0.097355, loss: 2.3769
2022-07-09 15:29:30 - train: epoch 0013, iter [02500, 05004], lr: 0.097347, loss: 2.1667
2022-07-09 15:30:05 - train: epoch 0013, iter [02600, 05004], lr: 0.097338, loss: 2.2025
2022-07-09 15:30:39 - train: epoch 0013, iter [02700, 05004], lr: 0.097330, loss: 2.1706
2022-07-09 15:31:13 - train: epoch 0013, iter [02800, 05004], lr: 0.097321, loss: 2.3542
2022-07-09 15:31:47 - train: epoch 0013, iter [02900, 05004], lr: 0.097313, loss: 2.4295
2022-07-09 15:32:21 - train: epoch 0013, iter [03000, 05004], lr: 0.097304, loss: 2.2863
2022-07-09 15:32:55 - train: epoch 0013, iter [03100, 05004], lr: 0.097296, loss: 2.1473
2022-07-09 15:33:30 - train: epoch 0013, iter [03200, 05004], lr: 0.097287, loss: 2.4000
2022-07-09 15:34:04 - train: epoch 0013, iter [03300, 05004], lr: 0.097279, loss: 2.1998
2022-07-09 15:34:39 - train: epoch 0013, iter [03400, 05004], lr: 0.097270, loss: 2.3143
2022-07-09 15:35:12 - train: epoch 0013, iter [03500, 05004], lr: 0.097262, loss: 2.2781
2022-07-09 15:35:46 - train: epoch 0013, iter [03600, 05004], lr: 0.097253, loss: 2.6366
2022-07-09 15:36:21 - train: epoch 0013, iter [03700, 05004], lr: 0.097245, loss: 2.1229
2022-07-09 15:36:55 - train: epoch 0013, iter [03800, 05004], lr: 0.097236, loss: 2.5360
2022-07-09 15:37:30 - train: epoch 0013, iter [03900, 05004], lr: 0.097228, loss: 2.3938
2022-07-09 15:38:04 - train: epoch 0013, iter [04000, 05004], lr: 0.097219, loss: 2.3675
2022-07-09 15:38:38 - train: epoch 0013, iter [04100, 05004], lr: 0.097210, loss: 2.2432
2022-07-09 15:39:13 - train: epoch 0013, iter [04200, 05004], lr: 0.097202, loss: 2.2476
2022-07-09 15:39:47 - train: epoch 0013, iter [04300, 05004], lr: 0.097193, loss: 2.3110
2022-07-09 15:40:21 - train: epoch 0013, iter [04400, 05004], lr: 0.097185, loss: 2.2424
2022-07-09 15:40:55 - train: epoch 0013, iter [04500, 05004], lr: 0.097176, loss: 2.3347
2022-07-09 15:41:30 - train: epoch 0013, iter [04600, 05004], lr: 0.097167, loss: 2.3189
2022-07-09 15:42:04 - train: epoch 0013, iter [04700, 05004], lr: 0.097159, loss: 2.4756
2022-07-09 15:42:39 - train: epoch 0013, iter [04800, 05004], lr: 0.097150, loss: 2.3904
2022-07-09 15:43:13 - train: epoch 0013, iter [04900, 05004], lr: 0.097141, loss: 2.3168
2022-07-09 15:43:46 - train: epoch 0013, iter [05000, 05004], lr: 0.097132, loss: 2.4409
2022-07-09 15:43:47 - train: epoch 013, train_loss: 2.3373
2022-07-09 15:45:03 - eval: epoch: 013, acc1: 52.058%, acc5: 77.676%, test_loss: 2.0838, per_image_load_time: 1.343ms, per_image_inference_time: 0.373ms
2022-07-09 15:45:04 - until epoch: 013, best_acc1: 52.476%
2022-07-09 15:45:04 - epoch 014 lr: 0.097132
2022-07-09 15:45:42 - train: epoch 0014, iter [00100, 05004], lr: 0.097123, loss: 2.2925
2022-07-09 15:46:17 - train: epoch 0014, iter [00200, 05004], lr: 0.097115, loss: 2.3874
2022-07-09 15:46:51 - train: epoch 0014, iter [00300, 05004], lr: 0.097106, loss: 2.0196
2022-07-09 15:47:24 - train: epoch 0014, iter [00400, 05004], lr: 0.097097, loss: 2.2723
2022-07-09 15:47:59 - train: epoch 0014, iter [00500, 05004], lr: 0.097088, loss: 2.2762
2022-07-09 15:48:32 - train: epoch 0014, iter [00600, 05004], lr: 0.097079, loss: 2.2559
2022-07-09 15:49:07 - train: epoch 0014, iter [00700, 05004], lr: 0.097071, loss: 2.2732
2022-07-09 15:49:41 - train: epoch 0014, iter [00800, 05004], lr: 0.097062, loss: 2.3087
2022-07-09 15:50:16 - train: epoch 0014, iter [00900, 05004], lr: 0.097053, loss: 2.3663
2022-07-09 15:50:49 - train: epoch 0014, iter [01000, 05004], lr: 0.097044, loss: 2.4770
2022-07-09 15:51:24 - train: epoch 0014, iter [01100, 05004], lr: 0.097035, loss: 2.2306
2022-07-09 15:51:58 - train: epoch 0014, iter [01200, 05004], lr: 0.097026, loss: 2.3006
2022-07-09 15:52:33 - train: epoch 0014, iter [01300, 05004], lr: 0.097017, loss: 2.4362
2022-07-09 15:53:07 - train: epoch 0014, iter [01400, 05004], lr: 0.097009, loss: 2.3809
2022-07-09 15:53:41 - train: epoch 0014, iter [01500, 05004], lr: 0.097000, loss: 2.4775
2022-07-09 15:54:15 - train: epoch 0014, iter [01600, 05004], lr: 0.096991, loss: 2.1906
2022-07-09 15:54:49 - train: epoch 0014, iter [01700, 05004], lr: 0.096982, loss: 2.5574
2022-07-09 15:55:24 - train: epoch 0014, iter [01800, 05004], lr: 0.096973, loss: 2.4755
2022-07-09 15:55:58 - train: epoch 0014, iter [01900, 05004], lr: 0.096964, loss: 2.2404
2022-07-09 15:56:33 - train: epoch 0014, iter [02000, 05004], lr: 0.096955, loss: 2.2360
2022-07-09 15:57:07 - train: epoch 0014, iter [02100, 05004], lr: 0.096946, loss: 2.4607
2022-07-09 15:57:41 - train: epoch 0014, iter [02200, 05004], lr: 0.096937, loss: 2.3060
2022-07-09 15:58:15 - train: epoch 0014, iter [02300, 05004], lr: 0.096928, loss: 2.2332
2022-07-09 15:58:50 - train: epoch 0014, iter [02400, 05004], lr: 0.096919, loss: 2.5323
2022-07-09 15:59:24 - train: epoch 0014, iter [02500, 05004], lr: 0.096910, loss: 2.2302
2022-07-09 15:59:58 - train: epoch 0014, iter [02600, 05004], lr: 0.096901, loss: 2.3179
2022-07-09 16:00:32 - train: epoch 0014, iter [02700, 05004], lr: 0.096892, loss: 2.2920
2022-07-09 16:01:07 - train: epoch 0014, iter [02800, 05004], lr: 0.096883, loss: 2.6258
2022-07-09 16:01:41 - train: epoch 0014, iter [02900, 05004], lr: 0.096873, loss: 2.1984
2022-07-09 16:02:15 - train: epoch 0014, iter [03000, 05004], lr: 0.096864, loss: 2.4005
2022-07-09 16:02:50 - train: epoch 0014, iter [03100, 05004], lr: 0.096855, loss: 2.1019
2022-07-09 16:03:24 - train: epoch 0014, iter [03200, 05004], lr: 0.096846, loss: 2.2119
2022-07-09 16:03:59 - train: epoch 0014, iter [03300, 05004], lr: 0.096837, loss: 2.1014
2022-07-09 16:04:34 - train: epoch 0014, iter [03400, 05004], lr: 0.096828, loss: 2.2536
2022-07-09 16:05:08 - train: epoch 0014, iter [03500, 05004], lr: 0.096819, loss: 2.2412
2022-07-09 16:05:42 - train: epoch 0014, iter [03600, 05004], lr: 0.096809, loss: 2.1743
2022-07-09 16:06:16 - train: epoch 0014, iter [03700, 05004], lr: 0.096800, loss: 2.1540
2022-07-09 16:06:50 - train: epoch 0014, iter [03800, 05004], lr: 0.096791, loss: 2.5477
2022-07-09 16:07:25 - train: epoch 0014, iter [03900, 05004], lr: 0.096782, loss: 2.2315
2022-07-09 16:07:59 - train: epoch 0014, iter [04000, 05004], lr: 0.096772, loss: 2.4513
2022-07-09 16:08:33 - train: epoch 0014, iter [04100, 05004], lr: 0.096763, loss: 2.2945
2022-07-09 16:09:08 - train: epoch 0014, iter [04200, 05004], lr: 0.096754, loss: 2.1876
2022-07-09 16:09:42 - train: epoch 0014, iter [04300, 05004], lr: 0.096745, loss: 2.2060
2022-07-09 16:10:16 - train: epoch 0014, iter [04400, 05004], lr: 0.096735, loss: 2.1374
2022-07-09 16:10:51 - train: epoch 0014, iter [04500, 05004], lr: 0.096726, loss: 2.3154
2022-07-09 16:11:25 - train: epoch 0014, iter [04600, 05004], lr: 0.096717, loss: 2.1981
2022-07-09 16:12:00 - train: epoch 0014, iter [04700, 05004], lr: 0.096707, loss: 2.2266
2022-07-09 16:12:34 - train: epoch 0014, iter [04800, 05004], lr: 0.096698, loss: 2.2022
2022-07-09 16:13:08 - train: epoch 0014, iter [04900, 05004], lr: 0.096689, loss: 2.0803
2022-07-09 16:13:40 - train: epoch 0014, iter [05000, 05004], lr: 0.096679, loss: 2.3409
2022-07-09 16:13:41 - train: epoch 014, train_loss: 2.3146
2022-07-09 16:14:57 - eval: epoch: 014, acc1: 52.228%, acc5: 77.718%, test_loss: 2.0752, per_image_load_time: 2.095ms, per_image_inference_time: 0.375ms
2022-07-09 16:14:57 - until epoch: 014, best_acc1: 52.476%
2022-07-09 16:14:57 - epoch 015 lr: 0.096679
2022-07-09 16:15:37 - train: epoch 0015, iter [00100, 05004], lr: 0.096670, loss: 2.0451
2022-07-09 16:16:12 - train: epoch 0015, iter [00200, 05004], lr: 0.096660, loss: 2.4986
2022-07-09 16:16:45 - train: epoch 0015, iter [00300, 05004], lr: 0.096651, loss: 2.5162
2022-07-09 16:17:19 - train: epoch 0015, iter [00400, 05004], lr: 0.096641, loss: 2.2322
2022-07-09 16:17:53 - train: epoch 0015, iter [00500, 05004], lr: 0.096632, loss: 2.2386
2022-07-09 16:18:27 - train: epoch 0015, iter [00600, 05004], lr: 0.096623, loss: 2.4823
2022-07-09 16:19:00 - train: epoch 0015, iter [00700, 05004], lr: 0.096613, loss: 2.2673
2022-07-09 16:19:35 - train: epoch 0015, iter [00800, 05004], lr: 0.096604, loss: 2.1071
2022-07-09 16:20:10 - train: epoch 0015, iter [00900, 05004], lr: 0.096594, loss: 2.1191
2022-07-09 16:20:43 - train: epoch 0015, iter [01000, 05004], lr: 0.096585, loss: 2.2543
2022-07-09 16:21:18 - train: epoch 0015, iter [01100, 05004], lr: 0.096575, loss: 2.2439
2022-07-09 16:21:52 - train: epoch 0015, iter [01200, 05004], lr: 0.096566, loss: 2.2114
2022-07-09 16:22:25 - train: epoch 0015, iter [01300, 05004], lr: 0.096556, loss: 2.5441
2022-07-09 16:23:00 - train: epoch 0015, iter [01400, 05004], lr: 0.096547, loss: 2.1100
2022-07-09 16:23:34 - train: epoch 0015, iter [01500, 05004], lr: 0.096537, loss: 1.9589
2022-07-09 16:24:08 - train: epoch 0015, iter [01600, 05004], lr: 0.096527, loss: 2.2732
2022-07-09 16:24:43 - train: epoch 0015, iter [01700, 05004], lr: 0.096518, loss: 2.4663
2022-07-09 16:25:16 - train: epoch 0015, iter [01800, 05004], lr: 0.096508, loss: 2.1740
2022-07-09 16:25:51 - train: epoch 0015, iter [01900, 05004], lr: 0.096499, loss: 2.1507
2022-07-09 16:26:25 - train: epoch 0015, iter [02000, 05004], lr: 0.096489, loss: 2.3330
2022-07-09 16:27:00 - train: epoch 0015, iter [02100, 05004], lr: 0.096479, loss: 2.1306
2022-07-09 16:27:34 - train: epoch 0015, iter [02200, 05004], lr: 0.096470, loss: 2.4921
2022-07-09 16:28:08 - train: epoch 0015, iter [02300, 05004], lr: 0.096460, loss: 2.1017
2022-07-09 16:28:42 - train: epoch 0015, iter [02400, 05004], lr: 0.096450, loss: 2.3398
2022-07-09 16:29:16 - train: epoch 0015, iter [02500, 05004], lr: 0.096441, loss: 2.4992
2022-07-09 16:29:51 - train: epoch 0015, iter [02600, 05004], lr: 0.096431, loss: 2.0453
2022-07-09 16:30:25 - train: epoch 0015, iter [02700, 05004], lr: 0.096421, loss: 2.4233
2022-07-09 16:30:59 - train: epoch 0015, iter [02800, 05004], lr: 0.096412, loss: 2.4937
2022-07-09 16:31:33 - train: epoch 0015, iter [02900, 05004], lr: 0.096402, loss: 2.2923
2022-07-09 16:32:07 - train: epoch 0015, iter [03000, 05004], lr: 0.096392, loss: 2.1013
2022-07-09 16:32:41 - train: epoch 0015, iter [03100, 05004], lr: 0.096382, loss: 2.2185
2022-07-09 16:33:16 - train: epoch 0015, iter [03200, 05004], lr: 0.096373, loss: 2.3522
2022-07-09 16:33:50 - train: epoch 0015, iter [03300, 05004], lr: 0.096363, loss: 2.0412
2022-07-09 16:34:23 - train: epoch 0015, iter [03400, 05004], lr: 0.096353, loss: 2.3286
2022-07-09 16:34:57 - train: epoch 0015, iter [03500, 05004], lr: 0.096343, loss: 2.4302
2022-07-09 16:35:32 - train: epoch 0015, iter [03600, 05004], lr: 0.096333, loss: 2.4093
2022-07-09 16:36:06 - train: epoch 0015, iter [03700, 05004], lr: 0.096323, loss: 2.1505
2022-07-09 16:36:40 - train: epoch 0015, iter [03800, 05004], lr: 0.096314, loss: 2.3257
2022-07-09 16:37:14 - train: epoch 0015, iter [03900, 05004], lr: 0.096304, loss: 2.4346
2022-07-09 16:37:48 - train: epoch 0015, iter [04000, 05004], lr: 0.096294, loss: 2.3094
2022-07-09 16:38:23 - train: epoch 0015, iter [04100, 05004], lr: 0.096284, loss: 2.5048
2022-07-09 16:38:57 - train: epoch 0015, iter [04200, 05004], lr: 0.096274, loss: 2.0532
2022-07-09 16:39:31 - train: epoch 0015, iter [04300, 05004], lr: 0.096264, loss: 2.2294
2022-07-09 16:40:06 - train: epoch 0015, iter [04400, 05004], lr: 0.096254, loss: 2.2675
2022-07-09 16:40:40 - train: epoch 0015, iter [04500, 05004], lr: 0.096244, loss: 2.3205
2022-07-09 16:41:15 - train: epoch 0015, iter [04600, 05004], lr: 0.096234, loss: 2.2948
2022-07-09 16:41:48 - train: epoch 0015, iter [04700, 05004], lr: 0.096224, loss: 2.4165
2022-07-09 16:42:23 - train: epoch 0015, iter [04800, 05004], lr: 0.096214, loss: 2.2478
2022-07-09 16:42:57 - train: epoch 0015, iter [04900, 05004], lr: 0.096204, loss: 2.2762
2022-07-09 16:43:29 - train: epoch 0015, iter [05000, 05004], lr: 0.096194, loss: 2.3785
2022-07-09 16:43:30 - train: epoch 015, train_loss: 2.2981
2022-07-09 16:44:45 - eval: epoch: 015, acc1: 52.344%, acc5: 77.342%, test_loss: 2.0755, per_image_load_time: 2.282ms, per_image_inference_time: 0.367ms
2022-07-09 16:44:45 - until epoch: 015, best_acc1: 52.476%
2022-07-09 16:44:45 - epoch 016 lr: 0.096194
2022-07-09 16:45:24 - train: epoch 0016, iter [00100, 05004], lr: 0.096184, loss: 2.2475
2022-07-09 16:45:58 - train: epoch 0016, iter [00200, 05004], lr: 0.096174, loss: 2.0332
2022-07-09 16:46:33 - train: epoch 0016, iter [00300, 05004], lr: 0.096164, loss: 2.2992
2022-07-09 16:47:07 - train: epoch 0016, iter [00400, 05004], lr: 0.096154, loss: 2.5851
2022-07-09 16:47:40 - train: epoch 0016, iter [00500, 05004], lr: 0.096144, loss: 2.0197
2022-07-09 16:48:15 - train: epoch 0016, iter [00600, 05004], lr: 0.096134, loss: 2.3648
2022-07-09 16:48:48 - train: epoch 0016, iter [00700, 05004], lr: 0.096124, loss: 1.9678
2022-07-09 16:49:22 - train: epoch 0016, iter [00800, 05004], lr: 0.096113, loss: 2.2434
2022-07-09 16:49:56 - train: epoch 0016, iter [00900, 05004], lr: 0.096103, loss: 2.2796
2022-07-09 16:50:30 - train: epoch 0016, iter [01000, 05004], lr: 0.096093, loss: 2.1161
2022-07-09 16:51:03 - train: epoch 0016, iter [01100, 05004], lr: 0.096083, loss: 2.1302
2022-07-09 16:51:37 - train: epoch 0016, iter [01200, 05004], lr: 0.096073, loss: 2.1511
2022-07-09 16:52:11 - train: epoch 0016, iter [01300, 05004], lr: 0.096063, loss: 2.3633
2022-07-09 16:52:46 - train: epoch 0016, iter [01400, 05004], lr: 0.096053, loss: 2.1464
2022-07-09 16:53:19 - train: epoch 0016, iter [01500, 05004], lr: 0.096042, loss: 2.2761
2022-07-09 16:53:55 - train: epoch 0016, iter [01600, 05004], lr: 0.096032, loss: 2.4670
2022-07-09 16:54:28 - train: epoch 0016, iter [01700, 05004], lr: 0.096022, loss: 2.2372
2022-07-09 16:55:04 - train: epoch 0016, iter [01800, 05004], lr: 0.096012, loss: 2.1568
2022-07-09 16:55:37 - train: epoch 0016, iter [01900, 05004], lr: 0.096001, loss: 2.2976
2022-07-09 16:56:11 - train: epoch 0016, iter [02000, 05004], lr: 0.095991, loss: 1.9942
2022-07-09 16:56:45 - train: epoch 0016, iter [02100, 05004], lr: 0.095981, loss: 2.4000
2022-07-09 16:57:20 - train: epoch 0016, iter [02200, 05004], lr: 0.095971, loss: 2.3120
2022-07-09 16:57:53 - train: epoch 0016, iter [02300, 05004], lr: 0.095960, loss: 2.5163
2022-07-09 16:58:27 - train: epoch 0016, iter [02400, 05004], lr: 0.095950, loss: 2.4231
2022-07-09 16:59:02 - train: epoch 0016, iter [02500, 05004], lr: 0.095940, loss: 2.2162
2022-07-09 16:59:35 - train: epoch 0016, iter [02600, 05004], lr: 0.095929, loss: 2.4457
2022-07-09 17:00:09 - train: epoch 0016, iter [02700, 05004], lr: 0.095919, loss: 2.2805
2022-07-09 17:00:44 - train: epoch 0016, iter [02800, 05004], lr: 0.095909, loss: 2.0736
2022-07-09 17:01:18 - train: epoch 0016, iter [02900, 05004], lr: 0.095898, loss: 2.3602
2022-07-09 17:01:52 - train: epoch 0016, iter [03000, 05004], lr: 0.095888, loss: 2.5300
2022-07-09 17:02:26 - train: epoch 0016, iter [03100, 05004], lr: 0.095878, loss: 2.4217
2022-07-09 17:03:01 - train: epoch 0016, iter [03200, 05004], lr: 0.095867, loss: 2.4400
2022-07-09 17:03:36 - train: epoch 0016, iter [03300, 05004], lr: 0.095857, loss: 2.3730
2022-07-09 17:04:09 - train: epoch 0016, iter [03400, 05004], lr: 0.095846, loss: 2.1791
2022-07-09 17:04:44 - train: epoch 0016, iter [03500, 05004], lr: 0.095836, loss: 2.2628
2022-07-09 17:05:18 - train: epoch 0016, iter [03600, 05004], lr: 0.095825, loss: 2.1963
2022-07-09 17:05:53 - train: epoch 0016, iter [03700, 05004], lr: 0.095815, loss: 2.4113
2022-07-09 17:06:26 - train: epoch 0016, iter [03800, 05004], lr: 0.095804, loss: 2.6356
2022-07-09 17:07:01 - train: epoch 0016, iter [03900, 05004], lr: 0.095794, loss: 2.3295
2022-07-09 17:07:35 - train: epoch 0016, iter [04000, 05004], lr: 0.095783, loss: 2.3990
2022-07-09 17:08:08 - train: epoch 0016, iter [04100, 05004], lr: 0.095773, loss: 2.1956
2022-07-09 17:08:43 - train: epoch 0016, iter [04200, 05004], lr: 0.095762, loss: 2.3334
2022-07-09 17:09:17 - train: epoch 0016, iter [04300, 05004], lr: 0.095752, loss: 2.1386
2022-07-09 17:09:51 - train: epoch 0016, iter [04400, 05004], lr: 0.095741, loss: 2.1666
2022-07-09 17:10:24 - train: epoch 0016, iter [04500, 05004], lr: 0.095731, loss: 2.4259
2022-07-09 17:10:58 - train: epoch 0016, iter [04600, 05004], lr: 0.095720, loss: 2.2190
2022-07-09 17:11:32 - train: epoch 0016, iter [04700, 05004], lr: 0.095710, loss: 2.5791
2022-07-09 17:12:05 - train: epoch 0016, iter [04800, 05004], lr: 0.095699, loss: 2.2400
2022-07-09 17:12:40 - train: epoch 0016, iter [04900, 05004], lr: 0.095688, loss: 2.1833
2022-07-09 17:13:12 - train: epoch 0016, iter [05000, 05004], lr: 0.095678, loss: 2.2561
2022-07-09 17:13:13 - train: epoch 016, train_loss: 2.2791
2022-07-09 17:14:28 - eval: epoch: 016, acc1: 52.914%, acc5: 78.176%, test_loss: 2.0229, per_image_load_time: 1.351ms, per_image_inference_time: 0.373ms
2022-07-09 17:14:28 - until epoch: 016, best_acc1: 52.914%
2022-07-09 17:14:28 - epoch 017 lr: 0.095677
2022-07-09 17:15:06 - train: epoch 0017, iter [00100, 05004], lr: 0.095667, loss: 2.1278
2022-07-09 17:15:41 - train: epoch 0017, iter [00200, 05004], lr: 0.095656, loss: 2.2571
2022-07-09 17:16:15 - train: epoch 0017, iter [00300, 05004], lr: 0.095645, loss: 2.5381
2022-07-09 17:16:49 - train: epoch 0017, iter [00400, 05004], lr: 0.095635, loss: 2.0149
2022-07-09 17:17:22 - train: epoch 0017, iter [00500, 05004], lr: 0.095624, loss: 2.2671
2022-07-09 17:17:57 - train: epoch 0017, iter [00600, 05004], lr: 0.095613, loss: 2.6615
2022-07-09 17:18:30 - train: epoch 0017, iter [00700, 05004], lr: 0.095602, loss: 2.2456
2022-07-09 17:19:04 - train: epoch 0017, iter [00800, 05004], lr: 0.095592, loss: 2.1300
2022-07-09 17:19:38 - train: epoch 0017, iter [00900, 05004], lr: 0.095581, loss: 2.2764
2022-07-09 17:20:12 - train: epoch 0017, iter [01000, 05004], lr: 0.095570, loss: 2.1231
2022-07-09 17:20:46 - train: epoch 0017, iter [01100, 05004], lr: 0.095559, loss: 2.5869
2022-07-09 17:21:20 - train: epoch 0017, iter [01200, 05004], lr: 0.095549, loss: 2.4092
2022-07-09 17:21:53 - train: epoch 0017, iter [01300, 05004], lr: 0.095538, loss: 2.2297
2022-07-09 17:22:27 - train: epoch 0017, iter [01400, 05004], lr: 0.095527, loss: 2.3500
2022-07-09 17:23:01 - train: epoch 0017, iter [01500, 05004], lr: 0.095516, loss: 1.9167
2022-07-09 17:23:35 - train: epoch 0017, iter [01600, 05004], lr: 0.095505, loss: 2.1965
2022-07-09 17:24:09 - train: epoch 0017, iter [01700, 05004], lr: 0.095495, loss: 2.1907
2022-07-09 17:24:43 - train: epoch 0017, iter [01800, 05004], lr: 0.095484, loss: 2.3059
2022-07-09 17:25:17 - train: epoch 0017, iter [01900, 05004], lr: 0.095473, loss: 2.1620
2022-07-09 17:25:51 - train: epoch 0017, iter [02000, 05004], lr: 0.095462, loss: 2.5702
2022-07-09 17:26:25 - train: epoch 0017, iter [02100, 05004], lr: 0.095451, loss: 2.2836
2022-07-09 17:26:58 - train: epoch 0017, iter [02200, 05004], lr: 0.095440, loss: 2.1780
2022-07-09 17:27:32 - train: epoch 0017, iter [02300, 05004], lr: 0.095429, loss: 2.2473
2022-07-09 17:28:06 - train: epoch 0017, iter [02400, 05004], lr: 0.095418, loss: 2.1961
2022-07-09 17:28:40 - train: epoch 0017, iter [02500, 05004], lr: 0.095407, loss: 2.4416
2022-07-09 17:29:14 - train: epoch 0017, iter [02600, 05004], lr: 0.095396, loss: 2.2059
2022-07-09 17:29:47 - train: epoch 0017, iter [02700, 05004], lr: 0.095385, loss: 2.1708
2022-07-09 17:30:21 - train: epoch 0017, iter [02800, 05004], lr: 0.095374, loss: 2.4792
2022-07-09 17:30:54 - train: epoch 0017, iter [02900, 05004], lr: 0.095363, loss: 2.5073
2022-07-09 17:31:28 - train: epoch 0017, iter [03000, 05004], lr: 0.095352, loss: 2.0809
2022-07-09 17:32:02 - train: epoch 0017, iter [03100, 05004], lr: 0.095341, loss: 2.4375
2022-07-09 17:32:36 - train: epoch 0017, iter [03200, 05004], lr: 0.095330, loss: 2.1818
2022-07-09 17:33:09 - train: epoch 0017, iter [03300, 05004], lr: 0.095319, loss: 2.3433
2022-07-09 17:33:44 - train: epoch 0017, iter [03400, 05004], lr: 0.095308, loss: 2.0927
2022-07-09 17:34:17 - train: epoch 0017, iter [03500, 05004], lr: 0.095297, loss: 2.3115
2022-07-09 17:34:50 - train: epoch 0017, iter [03600, 05004], lr: 0.095286, loss: 2.5601
2022-07-09 17:35:24 - train: epoch 0017, iter [03700, 05004], lr: 0.095275, loss: 2.3002
2022-07-09 17:35:58 - train: epoch 0017, iter [03800, 05004], lr: 0.095264, loss: 2.4310
2022-07-09 17:36:31 - train: epoch 0017, iter [03900, 05004], lr: 0.095253, loss: 2.1005
2022-07-09 17:37:05 - train: epoch 0017, iter [04000, 05004], lr: 0.095242, loss: 2.1260
2022-07-09 17:37:38 - train: epoch 0017, iter [04100, 05004], lr: 0.095231, loss: 2.2837
2022-07-09 17:38:12 - train: epoch 0017, iter [04200, 05004], lr: 0.095219, loss: 2.3215
2022-07-09 17:38:45 - train: epoch 0017, iter [04300, 05004], lr: 0.095208, loss: 2.3109
2022-07-09 17:39:19 - train: epoch 0017, iter [04400, 05004], lr: 0.095197, loss: 2.1140
2022-07-09 17:39:52 - train: epoch 0017, iter [04500, 05004], lr: 0.095186, loss: 2.3735
2022-07-09 17:40:26 - train: epoch 0017, iter [04600, 05004], lr: 0.095175, loss: 2.2414
2022-07-09 17:40:59 - train: epoch 0017, iter [04700, 05004], lr: 0.095163, loss: 2.3035
2022-07-09 17:41:34 - train: epoch 0017, iter [04800, 05004], lr: 0.095152, loss: 2.3627
2022-07-09 17:42:06 - train: epoch 0017, iter [04900, 05004], lr: 0.095141, loss: 2.0674
2022-07-09 17:42:38 - train: epoch 0017, iter [05000, 05004], lr: 0.095130, loss: 2.0536
2022-07-09 17:42:39 - train: epoch 017, train_loss: 2.2639
2022-07-09 17:43:55 - eval: epoch: 017, acc1: 54.260%, acc5: 79.120%, test_loss: 1.9657, per_image_load_time: 2.435ms, per_image_inference_time: 0.383ms
2022-07-09 17:43:55 - until epoch: 017, best_acc1: 54.260%
2022-07-09 17:43:55 - epoch 018 lr: 0.095129
2022-07-09 17:44:34 - train: epoch 0018, iter [00100, 05004], lr: 0.095118, loss: 2.2118
2022-07-09 17:45:08 - train: epoch 0018, iter [00200, 05004], lr: 0.095107, loss: 2.3312
2022-07-09 17:45:41 - train: epoch 0018, iter [00300, 05004], lr: 0.095095, loss: 2.3449
2022-07-09 17:46:15 - train: epoch 0018, iter [00400, 05004], lr: 0.095084, loss: 2.3228
2022-07-09 17:46:49 - train: epoch 0018, iter [00500, 05004], lr: 0.095073, loss: 2.2727
2022-07-09 17:47:22 - train: epoch 0018, iter [00600, 05004], lr: 0.095061, loss: 2.4408
2022-07-09 17:47:56 - train: epoch 0018, iter [00700, 05004], lr: 0.095050, loss: 2.0192
2022-07-09 17:48:29 - train: epoch 0018, iter [00800, 05004], lr: 0.095039, loss: 2.2321
2022-07-09 17:49:02 - train: epoch 0018, iter [00900, 05004], lr: 0.095027, loss: 2.3177
2022-07-09 17:49:36 - train: epoch 0018, iter [01000, 05004], lr: 0.095016, loss: 2.0602
2022-07-09 17:50:09 - train: epoch 0018, iter [01100, 05004], lr: 0.095005, loss: 2.4708
2022-07-09 17:50:43 - train: epoch 0018, iter [01200, 05004], lr: 0.094993, loss: 2.2820
2022-07-09 17:51:16 - train: epoch 0018, iter [01300, 05004], lr: 0.094982, loss: 2.5768
2022-07-09 17:51:51 - train: epoch 0018, iter [01400, 05004], lr: 0.094970, loss: 2.2432
2022-07-09 17:52:24 - train: epoch 0018, iter [01500, 05004], lr: 0.094959, loss: 2.4597
2022-07-09 17:52:57 - train: epoch 0018, iter [01600, 05004], lr: 0.094947, loss: 2.2471
2022-07-09 17:53:31 - train: epoch 0018, iter [01700, 05004], lr: 0.094936, loss: 2.2189
2022-07-09 17:54:04 - train: epoch 0018, iter [01800, 05004], lr: 0.094925, loss: 1.9589
2022-07-09 17:54:39 - train: epoch 0018, iter [01900, 05004], lr: 0.094913, loss: 2.2977
2022-07-09 17:55:12 - train: epoch 0018, iter [02000, 05004], lr: 0.094902, loss: 2.4457
2022-07-09 17:55:46 - train: epoch 0018, iter [02100, 05004], lr: 0.094890, loss: 2.3581
2022-07-09 17:56:19 - train: epoch 0018, iter [02200, 05004], lr: 0.094879, loss: 2.2076
2022-07-09 17:56:52 - train: epoch 0018, iter [02300, 05004], lr: 0.094867, loss: 2.3261
2022-07-09 17:57:26 - train: epoch 0018, iter [02400, 05004], lr: 0.094855, loss: 2.0329
2022-07-09 17:58:00 - train: epoch 0018, iter [02500, 05004], lr: 0.094844, loss: 1.9773
2022-07-09 17:58:34 - train: epoch 0018, iter [02600, 05004], lr: 0.094832, loss: 2.1387
2022-07-09 17:59:07 - train: epoch 0018, iter [02700, 05004], lr: 0.094821, loss: 2.3915
2022-07-09 17:59:41 - train: epoch 0018, iter [02800, 05004], lr: 0.094809, loss: 1.9282
2022-07-09 18:00:14 - train: epoch 0018, iter [02900, 05004], lr: 0.094797, loss: 2.2747
2022-07-09 18:00:47 - train: epoch 0018, iter [03000, 05004], lr: 0.094786, loss: 2.2289
2022-07-09 18:01:20 - train: epoch 0018, iter [03100, 05004], lr: 0.094774, loss: 2.6197
2022-07-09 18:01:53 - train: epoch 0018, iter [03200, 05004], lr: 0.094763, loss: 2.1244
2022-07-09 18:02:27 - train: epoch 0018, iter [03300, 05004], lr: 0.094751, loss: 2.1127
2022-07-09 18:03:00 - train: epoch 0018, iter [03400, 05004], lr: 0.094739, loss: 2.2792
2022-07-09 18:03:33 - train: epoch 0018, iter [03500, 05004], lr: 0.094728, loss: 2.4596
2022-07-09 18:04:07 - train: epoch 0018, iter [03600, 05004], lr: 0.094716, loss: 2.2898
2022-07-09 18:04:40 - train: epoch 0018, iter [03700, 05004], lr: 0.094704, loss: 2.6582
2022-07-09 18:05:15 - train: epoch 0018, iter [03800, 05004], lr: 0.094692, loss: 2.3147
2022-07-09 18:05:48 - train: epoch 0018, iter [03900, 05004], lr: 0.094681, loss: 2.3613
2022-07-09 18:06:22 - train: epoch 0018, iter [04000, 05004], lr: 0.094669, loss: 2.1487
2022-07-09 18:06:55 - train: epoch 0018, iter [04100, 05004], lr: 0.094657, loss: 2.2562
2022-07-09 18:07:28 - train: epoch 0018, iter [04200, 05004], lr: 0.094645, loss: 2.2321
2022-07-09 18:08:02 - train: epoch 0018, iter [04300, 05004], lr: 0.094634, loss: 2.0409
2022-07-09 18:08:36 - train: epoch 0018, iter [04400, 05004], lr: 0.094622, loss: 2.4354
2022-07-09 18:09:10 - train: epoch 0018, iter [04500, 05004], lr: 0.094610, loss: 2.1781
2022-07-09 18:09:44 - train: epoch 0018, iter [04600, 05004], lr: 0.094598, loss: 2.0961
2022-07-09 18:10:18 - train: epoch 0018, iter [04700, 05004], lr: 0.094586, loss: 2.4290
2022-07-09 18:10:51 - train: epoch 0018, iter [04800, 05004], lr: 0.094575, loss: 2.3120
2022-07-09 18:11:25 - train: epoch 0018, iter [04900, 05004], lr: 0.094563, loss: 2.3550
2022-07-09 18:11:57 - train: epoch 0018, iter [05000, 05004], lr: 0.094551, loss: 2.3764
2022-07-09 18:11:58 - train: epoch 018, train_loss: 2.2461
2022-07-09 18:13:13 - eval: epoch: 018, acc1: 53.678%, acc5: 78.468%, test_loss: 1.9942, per_image_load_time: 2.519ms, per_image_inference_time: 0.365ms
2022-07-09 18:13:13 - until epoch: 018, best_acc1: 54.260%
2022-07-09 18:13:13 - epoch 019 lr: 0.094550
2022-07-09 18:13:53 - train: epoch 0019, iter [00100, 05004], lr: 0.094538, loss: 2.0578
2022-07-09 18:14:25 - train: epoch 0019, iter [00200, 05004], lr: 0.094527, loss: 2.4428
2022-07-09 18:14:58 - train: epoch 0019, iter [00300, 05004], lr: 0.094515, loss: 2.4504
2022-07-09 18:15:32 - train: epoch 0019, iter [00400, 05004], lr: 0.094503, loss: 2.0740
2022-07-09 18:16:05 - train: epoch 0019, iter [00500, 05004], lr: 0.094491, loss: 2.2092
2022-07-09 18:16:39 - train: epoch 0019, iter [00600, 05004], lr: 0.094479, loss: 2.0948
2022-07-09 18:17:13 - train: epoch 0019, iter [00700, 05004], lr: 0.094467, loss: 1.9233
2022-07-09 18:17:47 - train: epoch 0019, iter [00800, 05004], lr: 0.094455, loss: 2.5172
2022-07-09 18:18:21 - train: epoch 0019, iter [00900, 05004], lr: 0.094443, loss: 2.2975
2022-07-09 18:18:54 - train: epoch 0019, iter [01000, 05004], lr: 0.094431, loss: 2.3344
2022-07-09 18:19:28 - train: epoch 0019, iter [01100, 05004], lr: 0.094419, loss: 2.0931
2022-07-09 18:20:03 - train: epoch 0019, iter [01200, 05004], lr: 0.094407, loss: 2.3084
2022-07-09 18:20:36 - train: epoch 0019, iter [01300, 05004], lr: 0.094395, loss: 2.4289
2022-07-09 18:21:09 - train: epoch 0019, iter [01400, 05004], lr: 0.094383, loss: 2.1377
2022-07-09 18:21:43 - train: epoch 0019, iter [01500, 05004], lr: 0.094371, loss: 2.6647
2022-07-09 18:22:17 - train: epoch 0019, iter [01600, 05004], lr: 0.094359, loss: 2.2553
2022-07-09 18:22:51 - train: epoch 0019, iter [01700, 05004], lr: 0.094347, loss: 2.2821
2022-07-09 18:23:26 - train: epoch 0019, iter [01800, 05004], lr: 0.094335, loss: 2.1003
2022-07-09 18:23:59 - train: epoch 0019, iter [01900, 05004], lr: 0.094322, loss: 2.4441
2022-07-09 18:24:34 - train: epoch 0019, iter [02000, 05004], lr: 0.094310, loss: 2.1766
2022-07-09 18:25:08 - train: epoch 0019, iter [02100, 05004], lr: 0.094298, loss: 2.0354
2022-07-09 18:25:42 - train: epoch 0019, iter [02200, 05004], lr: 0.094286, loss: 2.2201
2022-07-09 18:26:16 - train: epoch 0019, iter [02300, 05004], lr: 0.094274, loss: 2.2228
2022-07-09 18:26:49 - train: epoch 0019, iter [02400, 05004], lr: 0.094262, loss: 2.3071
2022-07-09 18:27:23 - train: epoch 0019, iter [02500, 05004], lr: 0.094250, loss: 2.1452
2022-07-09 18:27:57 - train: epoch 0019, iter [02600, 05004], lr: 0.094237, loss: 2.2005
2022-07-09 18:28:30 - train: epoch 0019, iter [02700, 05004], lr: 0.094225, loss: 2.4210
2022-07-09 18:29:04 - train: epoch 0019, iter [02800, 05004], lr: 0.094213, loss: 2.2691
2022-07-09 18:29:38 - train: epoch 0019, iter [02900, 05004], lr: 0.094201, loss: 2.1930
2022-07-09 18:30:12 - train: epoch 0019, iter [03000, 05004], lr: 0.094189, loss: 2.5761
2022-07-09 18:30:46 - train: epoch 0019, iter [03100, 05004], lr: 0.094176, loss: 2.3182
2022-07-09 18:31:19 - train: epoch 0019, iter [03200, 05004], lr: 0.094164, loss: 1.8492
2022-07-09 18:31:54 - train: epoch 0019, iter [03300, 05004], lr: 0.094152, loss: 2.1965
2022-07-09 18:32:27 - train: epoch 0019, iter [03400, 05004], lr: 0.094140, loss: 2.2939
2022-07-09 18:33:02 - train: epoch 0019, iter [03500, 05004], lr: 0.094127, loss: 2.2863
2022-07-09 18:33:35 - train: epoch 0019, iter [03600, 05004], lr: 0.094115, loss: 2.1056
2022-07-09 18:34:09 - train: epoch 0019, iter [03700, 05004], lr: 0.094103, loss: 2.2962
2022-07-09 18:34:43 - train: epoch 0019, iter [03800, 05004], lr: 0.094090, loss: 2.4714
2022-07-09 18:35:18 - train: epoch 0019, iter [03900, 05004], lr: 0.094078, loss: 2.2444
2022-07-09 18:35:52 - train: epoch 0019, iter [04000, 05004], lr: 0.094066, loss: 2.1600
2022-07-09 18:36:25 - train: epoch 0019, iter [04100, 05004], lr: 0.094053, loss: 2.3073
2022-07-09 18:36:59 - train: epoch 0019, iter [04200, 05004], lr: 0.094041, loss: 2.2783
2022-07-09 18:37:34 - train: epoch 0019, iter [04300, 05004], lr: 0.094028, loss: 2.1126
2022-07-09 18:38:08 - train: epoch 0019, iter [04400, 05004], lr: 0.094016, loss: 2.3444
2022-07-09 18:38:41 - train: epoch 0019, iter [04500, 05004], lr: 0.094004, loss: 2.5830
2022-07-09 18:39:15 - train: epoch 0019, iter [04600, 05004], lr: 0.093991, loss: 2.2272
2022-07-09 18:39:49 - train: epoch 0019, iter [04700, 05004], lr: 0.093979, loss: 2.1141
2022-07-09 18:40:23 - train: epoch 0019, iter [04800, 05004], lr: 0.093966, loss: 2.2959
2022-07-09 18:40:57 - train: epoch 0019, iter [04900, 05004], lr: 0.093954, loss: 2.3093
2022-07-09 18:41:29 - train: epoch 0019, iter [05000, 05004], lr: 0.093941, loss: 2.2232
2022-07-09 18:41:30 - train: epoch 019, train_loss: 2.2372
2022-07-09 18:42:45 - eval: epoch: 019, acc1: 54.456%, acc5: 79.236%, test_loss: 1.9534, per_image_load_time: 2.535ms, per_image_inference_time: 0.373ms
2022-07-09 18:42:46 - until epoch: 019, best_acc1: 54.456%
2022-07-09 18:42:46 - epoch 020 lr: 0.093941
2022-07-09 18:43:24 - train: epoch 0020, iter [00100, 05004], lr: 0.093928, loss: 2.3450
2022-07-09 18:43:59 - train: epoch 0020, iter [00200, 05004], lr: 0.093916, loss: 1.9486
2022-07-09 18:44:33 - train: epoch 0020, iter [00300, 05004], lr: 0.093903, loss: 2.2847
2022-07-09 18:45:06 - train: epoch 0020, iter [00400, 05004], lr: 0.093891, loss: 1.9974
2022-07-09 18:45:41 - train: epoch 0020, iter [00500, 05004], lr: 0.093878, loss: 2.0416
2022-07-09 18:46:13 - train: epoch 0020, iter [00600, 05004], lr: 0.093866, loss: 2.4202
2022-07-09 18:46:48 - train: epoch 0020, iter [00700, 05004], lr: 0.093853, loss: 2.0384
2022-07-09 18:47:21 - train: epoch 0020, iter [00800, 05004], lr: 0.093841, loss: 2.3423
2022-07-09 18:47:54 - train: epoch 0020, iter [00900, 05004], lr: 0.093828, loss: 2.4985
2022-07-09 18:48:28 - train: epoch 0020, iter [01000, 05004], lr: 0.093815, loss: 2.4049
2022-07-09 18:49:01 - train: epoch 0020, iter [01100, 05004], lr: 0.093803, loss: 2.1489
2022-07-09 18:49:34 - train: epoch 0020, iter [01200, 05004], lr: 0.093790, loss: 1.9663
2022-07-09 18:50:08 - train: epoch 0020, iter [01300, 05004], lr: 0.093778, loss: 2.1303
2022-07-09 18:50:41 - train: epoch 0020, iter [01400, 05004], lr: 0.093765, loss: 2.4003
2022-07-09 18:51:15 - train: epoch 0020, iter [01500, 05004], lr: 0.093752, loss: 2.2861
2022-07-09 18:51:48 - train: epoch 0020, iter [01600, 05004], lr: 0.093740, loss: 2.1420
2022-07-09 18:52:23 - train: epoch 0020, iter [01700, 05004], lr: 0.093727, loss: 1.9398
2022-07-09 18:52:56 - train: epoch 0020, iter [01800, 05004], lr: 0.093714, loss: 2.2323
2022-07-09 18:53:29 - train: epoch 0020, iter [01900, 05004], lr: 0.093702, loss: 2.0248
2022-07-09 18:54:03 - train: epoch 0020, iter [02000, 05004], lr: 0.093689, loss: 2.1536
2022-07-09 18:54:36 - train: epoch 0020, iter [02100, 05004], lr: 0.093676, loss: 2.2613
2022-07-09 18:55:10 - train: epoch 0020, iter [02200, 05004], lr: 0.093663, loss: 2.0361
2022-07-09 18:55:43 - train: epoch 0020, iter [02300, 05004], lr: 0.093651, loss: 2.0767
2022-07-09 18:56:17 - train: epoch 0020, iter [02400, 05004], lr: 0.093638, loss: 2.3961
2022-07-09 18:56:50 - train: epoch 0020, iter [02500, 05004], lr: 0.093625, loss: 2.1611
2022-07-09 18:57:25 - train: epoch 0020, iter [02600, 05004], lr: 0.093612, loss: 2.0655
2022-07-09 18:57:57 - train: epoch 0020, iter [02700, 05004], lr: 0.093599, loss: 2.3333
2022-07-09 18:58:31 - train: epoch 0020, iter [02800, 05004], lr: 0.093587, loss: 2.2751
2022-07-09 18:59:05 - train: epoch 0020, iter [02900, 05004], lr: 0.093574, loss: 2.3958
2022-07-09 18:59:38 - train: epoch 0020, iter [03000, 05004], lr: 0.093561, loss: 2.1645
2022-07-09 19:00:12 - train: epoch 0020, iter [03100, 05004], lr: 0.093548, loss: 2.2235
2022-07-09 19:00:45 - train: epoch 0020, iter [03200, 05004], lr: 0.093535, loss: 2.3471
2022-07-09 19:01:19 - train: epoch 0020, iter [03300, 05004], lr: 0.093522, loss: 2.0640
2022-07-09 19:01:53 - train: epoch 0020, iter [03400, 05004], lr: 0.093510, loss: 2.1888
2022-07-09 19:02:26 - train: epoch 0020, iter [03500, 05004], lr: 0.093497, loss: 2.0929
2022-07-09 19:02:59 - train: epoch 0020, iter [03600, 05004], lr: 0.093484, loss: 2.2891
2022-07-09 19:03:33 - train: epoch 0020, iter [03700, 05004], lr: 0.093471, loss: 1.9869
2022-07-09 19:04:07 - train: epoch 0020, iter [03800, 05004], lr: 0.093458, loss: 2.1799
2022-07-09 19:04:41 - train: epoch 0020, iter [03900, 05004], lr: 0.093445, loss: 2.3769
2022-07-09 19:05:15 - train: epoch 0020, iter [04000, 05004], lr: 0.093432, loss: 2.1400
2022-07-09 19:05:49 - train: epoch 0020, iter [04100, 05004], lr: 0.093419, loss: 2.0789
2022-07-09 19:06:23 - train: epoch 0020, iter [04200, 05004], lr: 0.093406, loss: 2.1553
2022-07-09 19:06:57 - train: epoch 0020, iter [04300, 05004], lr: 0.093393, loss: 2.3036
2022-07-09 19:07:31 - train: epoch 0020, iter [04400, 05004], lr: 0.093380, loss: 2.1957
2022-07-09 19:08:04 - train: epoch 0020, iter [04500, 05004], lr: 0.093367, loss: 2.2315
2022-07-09 19:08:38 - train: epoch 0020, iter [04600, 05004], lr: 0.093354, loss: 2.2442
2022-07-09 19:09:12 - train: epoch 0020, iter [04700, 05004], lr: 0.093341, loss: 2.1093
2022-07-09 19:09:45 - train: epoch 0020, iter [04800, 05004], lr: 0.093328, loss: 2.2249
2022-07-09 19:10:20 - train: epoch 0020, iter [04900, 05004], lr: 0.093315, loss: 2.2516
2022-07-09 19:10:52 - train: epoch 0020, iter [05000, 05004], lr: 0.093302, loss: 2.0364
2022-07-09 19:10:53 - train: epoch 020, train_loss: 2.2178
2022-07-09 19:12:10 - eval: epoch: 020, acc1: 54.690%, acc5: 79.512%, test_loss: 1.9323, per_image_load_time: 2.578ms, per_image_inference_time: 0.364ms
2022-07-09 19:12:10 - until epoch: 020, best_acc1: 54.690%
2022-07-09 19:12:10 - epoch 021 lr: 0.093301
2022-07-09 19:12:49 - train: epoch 0021, iter [00100, 05004], lr: 0.093288, loss: 2.0254
2022-07-09 19:13:23 - train: epoch 0021, iter [00200, 05004], lr: 0.093275, loss: 2.3236
2022-07-09 19:13:58 - train: epoch 0021, iter [00300, 05004], lr: 0.093262, loss: 1.9092
2022-07-09 19:14:32 - train: epoch 0021, iter [00400, 05004], lr: 0.093249, loss: 2.2119
2022-07-09 19:15:08 - train: epoch 0021, iter [00500, 05004], lr: 0.093236, loss: 2.0583
2022-07-09 19:15:41 - train: epoch 0021, iter [00600, 05004], lr: 0.093223, loss: 2.0723
2022-07-09 19:16:15 - train: epoch 0021, iter [00700, 05004], lr: 0.093209, loss: 2.0529
2022-07-09 19:16:49 - train: epoch 0021, iter [00800, 05004], lr: 0.093196, loss: 2.3284
2022-07-09 19:17:25 - train: epoch 0021, iter [00900, 05004], lr: 0.093183, loss: 2.1944
2022-07-09 19:17:58 - train: epoch 0021, iter [01000, 05004], lr: 0.093170, loss: 2.0428
2022-07-09 19:18:33 - train: epoch 0021, iter [01100, 05004], lr: 0.093157, loss: 1.9895
2022-07-09 19:19:07 - train: epoch 0021, iter [01200, 05004], lr: 0.093143, loss: 2.1229
2022-07-09 19:19:42 - train: epoch 0021, iter [01300, 05004], lr: 0.093130, loss: 2.1411
2022-07-09 19:20:16 - train: epoch 0021, iter [01400, 05004], lr: 0.093117, loss: 2.1490
2022-07-09 19:20:50 - train: epoch 0021, iter [01500, 05004], lr: 0.093104, loss: 1.9651
2022-07-09 19:21:24 - train: epoch 0021, iter [01600, 05004], lr: 0.093090, loss: 2.2352
2022-07-09 19:21:59 - train: epoch 0021, iter [01700, 05004], lr: 0.093077, loss: 2.2287
2022-07-09 19:22:33 - train: epoch 0021, iter [01800, 05004], lr: 0.093064, loss: 2.0554
2022-07-09 19:23:07 - train: epoch 0021, iter [01900, 05004], lr: 0.093051, loss: 2.3864
2022-07-09 19:23:42 - train: epoch 0021, iter [02000, 05004], lr: 0.093037, loss: 2.4608
2022-07-09 19:24:16 - train: epoch 0021, iter [02100, 05004], lr: 0.093024, loss: 1.9673
2022-07-09 19:24:51 - train: epoch 0021, iter [02200, 05004], lr: 0.093011, loss: 2.2077
2022-07-09 19:25:25 - train: epoch 0021, iter [02300, 05004], lr: 0.092997, loss: 2.0274
2022-07-09 19:25:59 - train: epoch 0021, iter [02400, 05004], lr: 0.092984, loss: 2.0712
2022-07-09 19:26:33 - train: epoch 0021, iter [02500, 05004], lr: 0.092971, loss: 2.3186
2022-07-09 19:27:08 - train: epoch 0021, iter [02600, 05004], lr: 0.092957, loss: 2.3804
2022-07-09 19:27:42 - train: epoch 0021, iter [02700, 05004], lr: 0.092944, loss: 2.1231
2022-07-09 19:28:16 - train: epoch 0021, iter [02800, 05004], lr: 0.092930, loss: 2.1242
2022-07-09 19:28:50 - train: epoch 0021, iter [02900, 05004], lr: 0.092917, loss: 2.1214
2022-07-09 19:29:25 - train: epoch 0021, iter [03000, 05004], lr: 0.092904, loss: 2.4604
2022-07-09 19:30:00 - train: epoch 0021, iter [03100, 05004], lr: 0.092890, loss: 2.1730
2022-07-09 19:30:34 - train: epoch 0021, iter [03200, 05004], lr: 0.092877, loss: 2.2066
2022-07-09 19:31:08 - train: epoch 0021, iter [03300, 05004], lr: 0.092863, loss: 2.4750
2022-07-09 19:31:42 - train: epoch 0021, iter [03400, 05004], lr: 0.092850, loss: 2.3665
2022-07-09 19:32:16 - train: epoch 0021, iter [03500, 05004], lr: 0.092836, loss: 2.1026
2022-07-09 19:32:50 - train: epoch 0021, iter [03600, 05004], lr: 0.092823, loss: 2.1096
2022-07-09 19:33:25 - train: epoch 0021, iter [03700, 05004], lr: 0.092809, loss: 2.1666
2022-07-09 19:34:00 - train: epoch 0021, iter [03800, 05004], lr: 0.092796, loss: 2.1546
2022-07-09 19:34:35 - train: epoch 0021, iter [03900, 05004], lr: 0.092782, loss: 2.1735
2022-07-09 19:35:09 - train: epoch 0021, iter [04000, 05004], lr: 0.092769, loss: 2.4844
2022-07-09 19:35:43 - train: epoch 0021, iter [04100, 05004], lr: 0.092755, loss: 2.1813
2022-07-09 19:36:17 - train: epoch 0021, iter [04200, 05004], lr: 0.092742, loss: 2.2368
2022-07-09 19:36:52 - train: epoch 0021, iter [04300, 05004], lr: 0.092728, loss: 2.1341
2022-07-09 19:37:26 - train: epoch 0021, iter [04400, 05004], lr: 0.092714, loss: 2.2890
2022-07-09 19:38:01 - train: epoch 0021, iter [04500, 05004], lr: 0.092701, loss: 2.4207
2022-07-09 19:38:35 - train: epoch 0021, iter [04600, 05004], lr: 0.092687, loss: 2.0992
2022-07-09 19:39:09 - train: epoch 0021, iter [04700, 05004], lr: 0.092674, loss: 2.3351
2022-07-09 19:39:43 - train: epoch 0021, iter [04800, 05004], lr: 0.092660, loss: 2.3529
2022-07-09 19:40:18 - train: epoch 0021, iter [04900, 05004], lr: 0.092646, loss: 2.0025
2022-07-09 19:40:51 - train: epoch 0021, iter [05000, 05004], lr: 0.092633, loss: 2.2822
2022-07-09 19:40:52 - train: epoch 021, train_loss: 2.2070
2022-07-09 19:42:07 - eval: epoch: 021, acc1: 55.558%, acc5: 80.258%, test_loss: 1.8872, per_image_load_time: 2.157ms, per_image_inference_time: 0.387ms
2022-07-09 19:42:07 - until epoch: 021, best_acc1: 55.558%
2022-07-10 01:31:30 - epoch 022 lr: 0.092632
2022-07-10 01:32:09 - train: epoch 0022, iter [00100, 05004], lr: 0.092618, loss: 1.9668
2022-07-10 01:32:43 - train: epoch 0022, iter [00200, 05004], lr: 0.092605, loss: 2.3210
2022-07-10 01:33:17 - train: epoch 0022, iter [00300, 05004], lr: 0.092591, loss: 2.0703
2022-07-10 01:33:51 - train: epoch 0022, iter [00400, 05004], lr: 0.092577, loss: 2.0730
2022-07-10 01:34:24 - train: epoch 0022, iter [00500, 05004], lr: 0.092564, loss: 2.2039
2022-07-10 01:34:58 - train: epoch 0022, iter [00600, 05004], lr: 0.092550, loss: 2.2554
2022-07-10 01:35:31 - train: epoch 0022, iter [00700, 05004], lr: 0.092536, loss: 2.2391
2022-07-10 01:36:06 - train: epoch 0022, iter [00800, 05004], lr: 0.092522, loss: 2.2077
2022-07-10 01:36:39 - train: epoch 0022, iter [00900, 05004], lr: 0.092509, loss: 2.2561
2022-07-10 01:37:13 - train: epoch 0022, iter [01000, 05004], lr: 0.092495, loss: 2.2485
2022-07-10 01:37:47 - train: epoch 0022, iter [01100, 05004], lr: 0.092481, loss: 1.9790
2022-07-10 01:38:21 - train: epoch 0022, iter [01200, 05004], lr: 0.092467, loss: 1.7853
2022-07-10 01:38:56 - train: epoch 0022, iter [01300, 05004], lr: 0.092453, loss: 2.0358
2022-07-10 01:39:30 - train: epoch 0022, iter [01400, 05004], lr: 0.092440, loss: 2.2101
2022-07-10 01:40:04 - train: epoch 0022, iter [01500, 05004], lr: 0.092426, loss: 2.2177
2022-07-10 01:40:38 - train: epoch 0022, iter [01600, 05004], lr: 0.092412, loss: 2.0304
2022-07-10 01:41:12 - train: epoch 0022, iter [01700, 05004], lr: 0.092398, loss: 2.2889
2022-07-10 01:41:46 - train: epoch 0022, iter [01800, 05004], lr: 0.092384, loss: 2.3241
2022-07-10 01:42:20 - train: epoch 0022, iter [01900, 05004], lr: 0.092370, loss: 2.2647
2022-07-10 01:42:54 - train: epoch 0022, iter [02000, 05004], lr: 0.092356, loss: 2.2026
2022-07-10 01:43:28 - train: epoch 0022, iter [02100, 05004], lr: 0.092342, loss: 2.0837
2022-07-10 01:44:02 - train: epoch 0022, iter [02200, 05004], lr: 0.092328, loss: 1.9031
2022-07-10 01:44:36 - train: epoch 0022, iter [02300, 05004], lr: 0.092315, loss: 2.3302
2022-07-10 01:45:10 - train: epoch 0022, iter [02400, 05004], lr: 0.092301, loss: 2.4377
2022-07-10 01:45:44 - train: epoch 0022, iter [02500, 05004], lr: 0.092287, loss: 2.2615
2022-07-10 01:46:19 - train: epoch 0022, iter [02600, 05004], lr: 0.092273, loss: 2.1245
2022-07-10 01:46:53 - train: epoch 0022, iter [02700, 05004], lr: 0.092259, loss: 2.0486
2022-07-10 01:47:26 - train: epoch 0022, iter [02800, 05004], lr: 0.092245, loss: 2.4458
2022-07-10 01:48:01 - train: epoch 0022, iter [02900, 05004], lr: 0.092231, loss: 2.1076
2022-07-10 01:48:34 - train: epoch 0022, iter [03000, 05004], lr: 0.092217, loss: 2.1533
2022-07-10 01:49:09 - train: epoch 0022, iter [03100, 05004], lr: 0.092203, loss: 2.3252
2022-07-10 01:49:42 - train: epoch 0022, iter [03200, 05004], lr: 0.092189, loss: 2.2087
2022-07-10 01:50:16 - train: epoch 0022, iter [03300, 05004], lr: 0.092175, loss: 2.1196
2022-07-10 01:50:51 - train: epoch 0022, iter [03400, 05004], lr: 0.092161, loss: 2.1415
2022-07-10 01:51:25 - train: epoch 0022, iter [03500, 05004], lr: 0.092147, loss: 2.1676
2022-07-10 01:51:58 - train: epoch 0022, iter [03600, 05004], lr: 0.092132, loss: 2.2912
2022-07-10 01:52:33 - train: epoch 0022, iter [03700, 05004], lr: 0.092118, loss: 2.4215
2022-07-10 01:53:07 - train: epoch 0022, iter [03800, 05004], lr: 0.092104, loss: 2.4201
2022-07-10 01:53:42 - train: epoch 0022, iter [03900, 05004], lr: 0.092090, loss: 2.2050
2022-07-10 01:54:16 - train: epoch 0022, iter [04000, 05004], lr: 0.092076, loss: 2.3646
2022-07-10 01:54:50 - train: epoch 0022, iter [04100, 05004], lr: 0.092062, loss: 2.0979
2022-07-10 01:55:24 - train: epoch 0022, iter [04200, 05004], lr: 0.092048, loss: 2.4124
2022-07-10 01:55:58 - train: epoch 0022, iter [04300, 05004], lr: 0.092034, loss: 2.4418
2022-07-10 01:56:32 - train: epoch 0022, iter [04400, 05004], lr: 0.092019, loss: 2.0342
2022-07-10 01:57:06 - train: epoch 0022, iter [04500, 05004], lr: 0.092005, loss: 1.9840
2022-07-10 01:57:40 - train: epoch 0022, iter [04600, 05004], lr: 0.091991, loss: 2.4406
2022-07-10 01:58:14 - train: epoch 0022, iter [04700, 05004], lr: 0.091977, loss: 2.1731
2022-07-10 01:58:48 - train: epoch 0022, iter [04800, 05004], lr: 0.091963, loss: 2.0192
2022-07-10 01:59:22 - train: epoch 0022, iter [04900, 05004], lr: 0.091948, loss: 2.1254
2022-07-10 01:59:55 - train: epoch 0022, iter [05000, 05004], lr: 0.091934, loss: 2.1216
2022-07-10 01:59:56 - train: epoch 022, train_loss: 2.1960
2022-07-10 02:01:10 - eval: epoch: 022, acc1: 55.022%, acc5: 79.588%, test_loss: 1.9301, per_image_load_time: 2.514ms, per_image_inference_time: 0.354ms
2022-07-10 02:01:10 - until epoch: 022, best_acc1: 55.558%
2022-07-10 02:01:10 - epoch 023 lr: 0.091933
2022-07-10 02:01:50 - train: epoch 0023, iter [00100, 05004], lr: 0.091919, loss: 2.1195
2022-07-10 02:02:23 - train: epoch 0023, iter [00200, 05004], lr: 0.091905, loss: 2.0811
2022-07-10 02:02:57 - train: epoch 0023, iter [00300, 05004], lr: 0.091891, loss: 2.0900
2022-07-10 02:03:31 - train: epoch 0023, iter [00400, 05004], lr: 0.091876, loss: 2.3317
2022-07-10 02:04:05 - train: epoch 0023, iter [00500, 05004], lr: 0.091862, loss: 2.1960
2022-07-10 02:04:38 - train: epoch 0023, iter [00600, 05004], lr: 0.091848, loss: 2.0874
2022-07-10 02:05:13 - train: epoch 0023, iter [00700, 05004], lr: 0.091834, loss: 1.9565
2022-07-10 02:05:48 - train: epoch 0023, iter [00800, 05004], lr: 0.091819, loss: 2.3011
2022-07-10 02:06:23 - train: epoch 0023, iter [00900, 05004], lr: 0.091805, loss: 2.2634
2022-07-10 02:06:57 - train: epoch 0023, iter [01000, 05004], lr: 0.091790, loss: 2.1059
2022-07-10 02:07:33 - train: epoch 0023, iter [01100, 05004], lr: 0.091776, loss: 2.2135
2022-07-10 02:08:07 - train: epoch 0023, iter [01200, 05004], lr: 0.091762, loss: 1.9551
2022-07-10 02:08:43 - train: epoch 0023, iter [01300, 05004], lr: 0.091747, loss: 2.1537
2022-07-10 02:09:17 - train: epoch 0023, iter [01400, 05004], lr: 0.091733, loss: 2.0772
2022-07-10 02:09:52 - train: epoch 0023, iter [01500, 05004], lr: 0.091719, loss: 1.9404
2022-07-10 02:10:27 - train: epoch 0023, iter [01600, 05004], lr: 0.091704, loss: 2.2274
2022-07-10 02:11:02 - train: epoch 0023, iter [01700, 05004], lr: 0.091690, loss: 2.2981
2022-07-10 02:11:37 - train: epoch 0023, iter [01800, 05004], lr: 0.091675, loss: 2.1213
2022-07-10 02:12:11 - train: epoch 0023, iter [01900, 05004], lr: 0.091661, loss: 2.2894
2022-07-10 02:12:47 - train: epoch 0023, iter [02000, 05004], lr: 0.091646, loss: 1.8850
2022-07-10 02:13:22 - train: epoch 0023, iter [02100, 05004], lr: 0.091632, loss: 2.2795
2022-07-10 02:13:57 - train: epoch 0023, iter [02200, 05004], lr: 0.091617, loss: 1.9318
2022-07-10 02:14:32 - train: epoch 0023, iter [02300, 05004], lr: 0.091603, loss: 2.1694
2022-07-10 02:15:06 - train: epoch 0023, iter [02400, 05004], lr: 0.091588, loss: 2.0852
2022-07-10 02:15:41 - train: epoch 0023, iter [02500, 05004], lr: 0.091574, loss: 2.2715
2022-07-10 02:16:17 - train: epoch 0023, iter [02600, 05004], lr: 0.091559, loss: 2.2442
2022-07-10 02:16:52 - train: epoch 0023, iter [02700, 05004], lr: 0.091545, loss: 2.1721
2022-07-10 02:17:26 - train: epoch 0023, iter [02800, 05004], lr: 0.091530, loss: 2.0852
2022-07-10 02:18:02 - train: epoch 0023, iter [02900, 05004], lr: 0.091516, loss: 2.1994
2022-07-10 02:18:37 - train: epoch 0023, iter [03000, 05004], lr: 0.091501, loss: 2.1001
2022-07-10 02:19:12 - train: epoch 0023, iter [03100, 05004], lr: 0.091486, loss: 2.3652
2022-07-10 02:19:47 - train: epoch 0023, iter [03200, 05004], lr: 0.091472, loss: 2.2623
2022-07-10 02:20:22 - train: epoch 0023, iter [03300, 05004], lr: 0.091457, loss: 2.3050
2022-07-10 02:20:57 - train: epoch 0023, iter [03400, 05004], lr: 0.091443, loss: 2.3334
2022-07-10 02:21:33 - train: epoch 0023, iter [03500, 05004], lr: 0.091428, loss: 2.0935
2022-07-10 02:22:07 - train: epoch 0023, iter [03600, 05004], lr: 0.091413, loss: 2.0385
2022-07-10 02:22:42 - train: epoch 0023, iter [03700, 05004], lr: 0.091399, loss: 2.1205
2022-07-10 02:23:18 - train: epoch 0023, iter [03800, 05004], lr: 0.091384, loss: 2.2338
2022-07-10 02:23:52 - train: epoch 0023, iter [03900, 05004], lr: 0.091369, loss: 2.1583
2022-07-10 02:24:28 - train: epoch 0023, iter [04000, 05004], lr: 0.091354, loss: 1.9801
2022-07-10 02:25:03 - train: epoch 0023, iter [04100, 05004], lr: 0.091340, loss: 2.0157
2022-07-10 02:25:38 - train: epoch 0023, iter [04200, 05004], lr: 0.091325, loss: 1.9841
2022-07-10 02:26:13 - train: epoch 0023, iter [04300, 05004], lr: 0.091310, loss: 2.0563
2022-07-10 02:26:48 - train: epoch 0023, iter [04400, 05004], lr: 0.091296, loss: 2.1270
2022-07-10 02:27:23 - train: epoch 0023, iter [04500, 05004], lr: 0.091281, loss: 2.1471
2022-07-10 02:27:58 - train: epoch 0023, iter [04600, 05004], lr: 0.091266, loss: 2.3820
2022-07-10 02:28:32 - train: epoch 0023, iter [04700, 05004], lr: 0.091251, loss: 2.0170
2022-07-10 02:29:08 - train: epoch 0023, iter [04800, 05004], lr: 0.091237, loss: 2.1272
2022-07-10 02:29:43 - train: epoch 0023, iter [04900, 05004], lr: 0.091222, loss: 2.0384
2022-07-10 02:30:17 - train: epoch 0023, iter [05000, 05004], lr: 0.091207, loss: 2.2115
2022-07-10 02:30:18 - train: epoch 023, train_loss: 2.1837
2022-07-10 02:31:35 - eval: epoch: 023, acc1: 55.506%, acc5: 80.256%, test_loss: 1.8953, per_image_load_time: 2.612ms, per_image_inference_time: 0.373ms
2022-07-10 02:31:35 - until epoch: 023, best_acc1: 55.558%
2022-07-10 02:31:35 - epoch 024 lr: 0.091206
2022-07-10 02:32:14 - train: epoch 0024, iter [00100, 05004], lr: 0.091191, loss: 2.2607
2022-07-10 02:32:49 - train: epoch 0024, iter [00200, 05004], lr: 0.091177, loss: 2.0446
2022-07-10 02:33:24 - train: epoch 0024, iter [00300, 05004], lr: 0.091162, loss: 2.1346
2022-07-10 02:33:59 - train: epoch 0024, iter [00400, 05004], lr: 0.091147, loss: 2.1422
2022-07-10 02:34:33 - train: epoch 0024, iter [00500, 05004], lr: 0.091132, loss: 2.3819
2022-07-10 02:35:08 - train: epoch 0024, iter [00600, 05004], lr: 0.091117, loss: 2.3537
2022-07-10 02:35:42 - train: epoch 0024, iter [00700, 05004], lr: 0.091102, loss: 2.1451
2022-07-10 02:36:17 - train: epoch 0024, iter [00800, 05004], lr: 0.091087, loss: 2.0456
2022-07-10 02:36:51 - train: epoch 0024, iter [00900, 05004], lr: 0.091073, loss: 2.2076
2022-07-10 02:37:26 - train: epoch 0024, iter [01000, 05004], lr: 0.091058, loss: 2.1219
2022-07-10 02:38:01 - train: epoch 0024, iter [01100, 05004], lr: 0.091043, loss: 2.0762
2022-07-10 02:38:36 - train: epoch 0024, iter [01200, 05004], lr: 0.091028, loss: 1.9807
2022-07-10 02:39:10 - train: epoch 0024, iter [01300, 05004], lr: 0.091013, loss: 2.3976
2022-07-10 02:39:45 - train: epoch 0024, iter [01400, 05004], lr: 0.090998, loss: 2.1138
2022-07-10 02:40:21 - train: epoch 0024, iter [01500, 05004], lr: 0.090983, loss: 2.3158
2022-07-10 02:40:55 - train: epoch 0024, iter [01600, 05004], lr: 0.090968, loss: 1.9619
2022-07-10 02:41:31 - train: epoch 0024, iter [01700, 05004], lr: 0.090953, loss: 1.9877
2022-07-10 02:42:06 - train: epoch 0024, iter [01800, 05004], lr: 0.090938, loss: 2.2708
2022-07-10 02:42:40 - train: epoch 0024, iter [01900, 05004], lr: 0.090923, loss: 1.9687
2022-07-10 02:43:16 - train: epoch 0024, iter [02000, 05004], lr: 0.090908, loss: 2.4048
2022-07-10 02:43:51 - train: epoch 0024, iter [02100, 05004], lr: 0.090893, loss: 2.2030
2022-07-10 02:44:26 - train: epoch 0024, iter [02200, 05004], lr: 0.090878, loss: 2.0629
2022-07-10 02:45:02 - train: epoch 0024, iter [02300, 05004], lr: 0.090863, loss: 2.2793
2022-07-10 02:45:37 - train: epoch 0024, iter [02400, 05004], lr: 0.090847, loss: 2.2830
2022-07-10 02:46:12 - train: epoch 0024, iter [02500, 05004], lr: 0.090832, loss: 2.3477
2022-07-10 02:46:47 - train: epoch 0024, iter [02600, 05004], lr: 0.090817, loss: 2.2397
2022-07-10 02:47:22 - train: epoch 0024, iter [02700, 05004], lr: 0.090802, loss: 2.2842
2022-07-10 02:47:57 - train: epoch 0024, iter [02800, 05004], lr: 0.090787, loss: 2.2028
2022-07-10 02:48:32 - train: epoch 0024, iter [02900, 05004], lr: 0.090772, loss: 2.3537
2022-07-10 02:49:07 - train: epoch 0024, iter [03000, 05004], lr: 0.090757, loss: 2.1092
2022-07-10 02:49:43 - train: epoch 0024, iter [03100, 05004], lr: 0.090742, loss: 1.9983
2022-07-10 02:50:17 - train: epoch 0024, iter [03200, 05004], lr: 0.090726, loss: 2.3497
2022-07-10 02:50:52 - train: epoch 0024, iter [03300, 05004], lr: 0.090711, loss: 2.0378
2022-07-10 02:51:27 - train: epoch 0024, iter [03400, 05004], lr: 0.090696, loss: 2.0972
2022-07-10 02:52:03 - train: epoch 0024, iter [03500, 05004], lr: 0.090681, loss: 2.1576
2022-07-10 02:52:37 - train: epoch 0024, iter [03600, 05004], lr: 0.090666, loss: 2.1558
2022-07-10 02:53:13 - train: epoch 0024, iter [03700, 05004], lr: 0.090650, loss: 2.0129
2022-07-10 02:53:49 - train: epoch 0024, iter [03800, 05004], lr: 0.090635, loss: 2.2175
2022-07-10 02:54:23 - train: epoch 0024, iter [03900, 05004], lr: 0.090620, loss: 2.1910
2022-07-10 02:54:58 - train: epoch 0024, iter [04000, 05004], lr: 0.090605, loss: 2.1087
2022-07-10 02:55:34 - train: epoch 0024, iter [04100, 05004], lr: 0.090589, loss: 2.1180
2022-07-10 02:56:09 - train: epoch 0024, iter [04200, 05004], lr: 0.090574, loss: 2.0668
2022-07-10 02:56:43 - train: epoch 0024, iter [04300, 05004], lr: 0.090559, loss: 1.9723
2022-07-10 02:57:18 - train: epoch 0024, iter [04400, 05004], lr: 0.090544, loss: 2.1764
2022-07-10 02:57:54 - train: epoch 0024, iter [04500, 05004], lr: 0.090528, loss: 1.9385
2022-07-10 02:58:28 - train: epoch 0024, iter [04600, 05004], lr: 0.090513, loss: 2.0390
2022-07-10 02:59:04 - train: epoch 0024, iter [04700, 05004], lr: 0.090498, loss: 1.9743
2022-07-10 02:59:39 - train: epoch 0024, iter [04800, 05004], lr: 0.090482, loss: 2.0155
2022-07-10 03:00:14 - train: epoch 0024, iter [04900, 05004], lr: 0.090467, loss: 2.0973
2022-07-10 03:00:47 - train: epoch 0024, iter [05000, 05004], lr: 0.090451, loss: 2.1636
2022-07-10 03:00:48 - train: epoch 024, train_loss: 2.1731
2022-07-10 03:02:05 - eval: epoch: 024, acc1: 55.766%, acc5: 80.294%, test_loss: 1.8945, per_image_load_time: 2.219ms, per_image_inference_time: 0.390ms
2022-07-10 03:02:05 - until epoch: 024, best_acc1: 55.766%
2022-07-10 03:02:05 - epoch 025 lr: 0.090451
2022-07-10 03:02:45 - train: epoch 0025, iter [00100, 05004], lr: 0.090435, loss: 2.0258
2022-07-10 03:03:19 - train: epoch 0025, iter [00200, 05004], lr: 0.090420, loss: 1.8562
2022-07-10 03:03:54 - train: epoch 0025, iter [00300, 05004], lr: 0.090405, loss: 1.9146
2022-07-10 03:04:29 - train: epoch 0025, iter [00400, 05004], lr: 0.090389, loss: 2.1645
2022-07-10 03:05:02 - train: epoch 0025, iter [00500, 05004], lr: 0.090374, loss: 1.9592
2022-07-10 03:05:37 - train: epoch 0025, iter [00600, 05004], lr: 0.090358, loss: 2.0445
2022-07-10 03:06:12 - train: epoch 0025, iter [00700, 05004], lr: 0.090343, loss: 2.2771
2022-07-10 03:06:47 - train: epoch 0025, iter [00800, 05004], lr: 0.090327, loss: 1.9910
2022-07-10 03:07:22 - train: epoch 0025, iter [00900, 05004], lr: 0.090312, loss: 1.8690
2022-07-10 03:07:56 - train: epoch 0025, iter [01000, 05004], lr: 0.090297, loss: 2.1076
2022-07-10 03:08:31 - train: epoch 0025, iter [01100, 05004], lr: 0.090281, loss: 2.0932
2022-07-10 03:09:06 - train: epoch 0025, iter [01200, 05004], lr: 0.090266, loss: 2.2506
2022-07-10 03:09:41 - train: epoch 0025, iter [01300, 05004], lr: 0.090250, loss: 2.1001
2022-07-10 03:10:16 - train: epoch 0025, iter [01400, 05004], lr: 0.090235, loss: 2.2186
2022-07-10 03:10:51 - train: epoch 0025, iter [01500, 05004], lr: 0.090219, loss: 2.1308
2022-07-10 03:11:26 - train: epoch 0025, iter [01600, 05004], lr: 0.090203, loss: 2.1442
2022-07-10 03:12:00 - train: epoch 0025, iter [01700, 05004], lr: 0.090188, loss: 2.3519
2022-07-10 03:12:35 - train: epoch 0025, iter [01800, 05004], lr: 0.090172, loss: 2.1575
2022-07-10 03:13:09 - train: epoch 0025, iter [01900, 05004], lr: 0.090157, loss: 2.1966
2022-07-10 03:13:43 - train: epoch 0025, iter [02000, 05004], lr: 0.090141, loss: 2.0837
2022-07-10 03:14:19 - train: epoch 0025, iter [02100, 05004], lr: 0.090126, loss: 1.8634
2022-07-10 03:14:53 - train: epoch 0025, iter [02200, 05004], lr: 0.090110, loss: 2.0219
2022-07-10 03:15:28 - train: epoch 0025, iter [02300, 05004], lr: 0.090094, loss: 1.9503
2022-07-10 03:16:01 - train: epoch 0025, iter [02400, 05004], lr: 0.090079, loss: 1.9628
2022-07-10 03:16:37 - train: epoch 0025, iter [02500, 05004], lr: 0.090063, loss: 2.0735
2022-07-10 03:17:11 - train: epoch 0025, iter [02600, 05004], lr: 0.090047, loss: 2.0751
2022-07-10 03:17:47 - train: epoch 0025, iter [02700, 05004], lr: 0.090032, loss: 2.2464
2022-07-10 03:18:20 - train: epoch 0025, iter [02800, 05004], lr: 0.090016, loss: 2.0790
2022-07-10 03:18:56 - train: epoch 0025, iter [02900, 05004], lr: 0.090000, loss: 2.1353
2022-07-10 03:19:30 - train: epoch 0025, iter [03000, 05004], lr: 0.089985, loss: 2.1577
2022-07-10 03:20:05 - train: epoch 0025, iter [03100, 05004], lr: 0.089969, loss: 2.1086
2022-07-10 03:20:40 - train: epoch 0025, iter [03200, 05004], lr: 0.089953, loss: 2.0901
2022-07-10 03:21:14 - train: epoch 0025, iter [03300, 05004], lr: 0.089937, loss: 2.2528
2022-07-10 03:21:48 - train: epoch 0025, iter [03400, 05004], lr: 0.089922, loss: 2.3029
2022-07-10 03:22:23 - train: epoch 0025, iter [03500, 05004], lr: 0.089906, loss: 2.1606
2022-07-10 03:22:57 - train: epoch 0025, iter [03600, 05004], lr: 0.089890, loss: 2.1302
2022-07-10 03:23:31 - train: epoch 0025, iter [03700, 05004], lr: 0.089874, loss: 2.1313
2022-07-10 03:24:05 - train: epoch 0025, iter [03800, 05004], lr: 0.089859, loss: 2.2245
2022-07-10 03:24:40 - train: epoch 0025, iter [03900, 05004], lr: 0.089843, loss: 2.3753
2022-07-10 03:25:13 - train: epoch 0025, iter [04000, 05004], lr: 0.089827, loss: 2.0810
2022-07-10 03:25:48 - train: epoch 0025, iter [04100, 05004], lr: 0.089811, loss: 2.3647
2022-07-10 03:26:22 - train: epoch 0025, iter [04200, 05004], lr: 0.089795, loss: 2.1472
2022-07-10 03:26:56 - train: epoch 0025, iter [04300, 05004], lr: 0.089780, loss: 1.9572
2022-07-10 03:27:31 - train: epoch 0025, iter [04400, 05004], lr: 0.089764, loss: 1.8850
2022-07-10 03:28:05 - train: epoch 0025, iter [04500, 05004], lr: 0.089748, loss: 2.1012
2022-07-10 03:28:39 - train: epoch 0025, iter [04600, 05004], lr: 0.089732, loss: 2.0119
2022-07-10 03:29:14 - train: epoch 0025, iter [04700, 05004], lr: 0.089716, loss: 2.1379
2022-07-10 03:29:48 - train: epoch 0025, iter [04800, 05004], lr: 0.089700, loss: 2.0765
2022-07-10 03:30:23 - train: epoch 0025, iter [04900, 05004], lr: 0.089684, loss: 2.1186
2022-07-10 03:30:55 - train: epoch 0025, iter [05000, 05004], lr: 0.089668, loss: 2.2050
2022-07-10 03:30:56 - train: epoch 025, train_loss: 2.1632
2022-07-10 03:32:12 - eval: epoch: 025, acc1: 55.104%, acc5: 80.018%, test_loss: 1.9218, per_image_load_time: 1.695ms, per_image_inference_time: 0.341ms
2022-07-10 03:32:13 - until epoch: 025, best_acc1: 55.766%
2022-07-10 03:32:13 - epoch 026 lr: 0.089668
2022-07-10 03:32:52 - train: epoch 0026, iter [00100, 05004], lr: 0.089652, loss: 2.0363
2022-07-10 03:33:26 - train: epoch 0026, iter [00200, 05004], lr: 0.089636, loss: 2.0720
2022-07-10 03:34:01 - train: epoch 0026, iter [00300, 05004], lr: 0.089620, loss: 2.3221
2022-07-10 03:34:35 - train: epoch 0026, iter [00400, 05004], lr: 0.089604, loss: 2.0038
2022-07-10 03:35:09 - train: epoch 0026, iter [00500, 05004], lr: 0.089588, loss: 2.0201
2022-07-10 03:35:43 - train: epoch 0026, iter [00600, 05004], lr: 0.089572, loss: 2.4522
2022-07-10 03:36:17 - train: epoch 0026, iter [00700, 05004], lr: 0.089556, loss: 2.1926
2022-07-10 03:36:50 - train: epoch 0026, iter [00800, 05004], lr: 0.089540, loss: 1.9394
2022-07-10 03:37:24 - train: epoch 0026, iter [00900, 05004], lr: 0.089524, loss: 2.0465
2022-07-10 03:37:58 - train: epoch 0026, iter [01000, 05004], lr: 0.089508, loss: 2.1727
2022-07-10 03:38:32 - train: epoch 0026, iter [01100, 05004], lr: 0.089492, loss: 2.1879
2022-07-10 03:39:07 - train: epoch 0026, iter [01200, 05004], lr: 0.089476, loss: 2.0242
2022-07-10 03:39:41 - train: epoch 0026, iter [01300, 05004], lr: 0.089460, loss: 2.1231
2022-07-10 03:40:15 - train: epoch 0026, iter [01400, 05004], lr: 0.089444, loss: 2.0580
2022-07-10 03:40:49 - train: epoch 0026, iter [01500, 05004], lr: 0.089428, loss: 1.9405
2022-07-10 03:41:24 - train: epoch 0026, iter [01600, 05004], lr: 0.089411, loss: 2.3014
2022-07-10 03:41:58 - train: epoch 0026, iter [01700, 05004], lr: 0.089395, loss: 2.1760
2022-07-10 03:42:32 - train: epoch 0026, iter [01800, 05004], lr: 0.089379, loss: 2.2058
2022-07-10 03:43:06 - train: epoch 0026, iter [01900, 05004], lr: 0.089363, loss: 2.4723
2022-07-10 03:43:40 - train: epoch 0026, iter [02000, 05004], lr: 0.089347, loss: 2.2586
2022-07-10 03:44:14 - train: epoch 0026, iter [02100, 05004], lr: 0.089331, loss: 2.1793
2022-07-10 03:44:49 - train: epoch 0026, iter [02200, 05004], lr: 0.089315, loss: 2.1615
2022-07-10 03:45:24 - train: epoch 0026, iter [02300, 05004], lr: 0.089299, loss: 2.2038
2022-07-10 03:46:00 - train: epoch 0026, iter [02400, 05004], lr: 0.089282, loss: 2.2470
2022-07-10 03:46:32 - train: epoch 0026, iter [02500, 05004], lr: 0.089266, loss: 1.9537
2022-07-10 03:47:07 - train: epoch 0026, iter [02600, 05004], lr: 0.089250, loss: 2.3607
2022-07-10 03:47:41 - train: epoch 0026, iter [02700, 05004], lr: 0.089234, loss: 2.1375
2022-07-10 03:48:15 - train: epoch 0026, iter [02800, 05004], lr: 0.089218, loss: 2.2120
2022-07-10 03:48:49 - train: epoch 0026, iter [02900, 05004], lr: 0.089201, loss: 2.1932
2022-07-10 03:49:24 - train: epoch 0026, iter [03000, 05004], lr: 0.089185, loss: 2.1563
2022-07-10 03:49:59 - train: epoch 0026, iter [03100, 05004], lr: 0.089169, loss: 2.2085
2022-07-10 03:50:33 - train: epoch 0026, iter [03200, 05004], lr: 0.089153, loss: 2.1022
2022-07-10 03:51:07 - train: epoch 0026, iter [03300, 05004], lr: 0.089136, loss: 2.1024
2022-07-10 03:51:42 - train: epoch 0026, iter [03400, 05004], lr: 0.089120, loss: 2.0935
2022-07-10 03:52:16 - train: epoch 0026, iter [03500, 05004], lr: 0.089104, loss: 2.3108
2022-07-10 03:52:51 - train: epoch 0026, iter [03600, 05004], lr: 0.089087, loss: 2.1579
2022-07-10 03:53:25 - train: epoch 0026, iter [03700, 05004], lr: 0.089071, loss: 2.1084
2022-07-10 03:53:59 - train: epoch 0026, iter [03800, 05004], lr: 0.089055, loss: 2.4103
2022-07-10 03:54:33 - train: epoch 0026, iter [03900, 05004], lr: 0.089038, loss: 2.4233
2022-07-10 03:55:08 - train: epoch 0026, iter [04000, 05004], lr: 0.089022, loss: 2.2238
2022-07-10 03:55:42 - train: epoch 0026, iter [04100, 05004], lr: 0.089006, loss: 2.0835
2022-07-10 03:56:16 - train: epoch 0026, iter [04200, 05004], lr: 0.088989, loss: 2.2555
2022-07-10 03:56:50 - train: epoch 0026, iter [04300, 05004], lr: 0.088973, loss: 2.1477
2022-07-10 03:57:25 - train: epoch 0026, iter [04400, 05004], lr: 0.088957, loss: 2.2212
2022-07-10 03:57:59 - train: epoch 0026, iter [04500, 05004], lr: 0.088940, loss: 2.3596
2022-07-10 03:58:33 - train: epoch 0026, iter [04600, 05004], lr: 0.088924, loss: 2.1471
2022-07-10 03:59:07 - train: epoch 0026, iter [04700, 05004], lr: 0.088907, loss: 2.0102
2022-07-10 03:59:41 - train: epoch 0026, iter [04800, 05004], lr: 0.088891, loss: 2.1474
2022-07-10 04:00:16 - train: epoch 0026, iter [04900, 05004], lr: 0.088874, loss: 2.2109
2022-07-10 04:00:48 - train: epoch 0026, iter [05000, 05004], lr: 0.088858, loss: 2.2220
2022-07-10 04:00:49 - train: epoch 026, train_loss: 2.1539
2022-07-10 04:02:04 - eval: epoch: 026, acc1: 55.652%, acc5: 80.328%, test_loss: 1.8958, per_image_load_time: 2.459ms, per_image_inference_time: 0.358ms
2022-07-10 04:02:05 - until epoch: 026, best_acc1: 55.766%
2022-07-10 04:02:05 - epoch 027 lr: 0.088857
2022-07-10 04:02:45 - train: epoch 0027, iter [00100, 05004], lr: 0.088841, loss: 2.2434
2022-07-10 04:03:19 - train: epoch 0027, iter [00200, 05004], lr: 0.088824, loss: 2.0046
2022-07-10 04:03:53 - train: epoch 0027, iter [00300, 05004], lr: 0.088808, loss: 2.0372
2022-07-10 04:04:26 - train: epoch 0027, iter [00400, 05004], lr: 0.088791, loss: 2.2778
2022-07-10 04:05:00 - train: epoch 0027, iter [00500, 05004], lr: 0.088775, loss: 2.3758
2022-07-10 04:05:34 - train: epoch 0027, iter [00600, 05004], lr: 0.088758, loss: 2.2027
2022-07-10 04:06:08 - train: epoch 0027, iter [00700, 05004], lr: 0.088742, loss: 2.2868
2022-07-10 04:06:41 - train: epoch 0027, iter [00800, 05004], lr: 0.088725, loss: 2.2317
2022-07-10 04:07:15 - train: epoch 0027, iter [00900, 05004], lr: 0.088709, loss: 2.2779
2022-07-10 04:07:49 - train: epoch 0027, iter [01000, 05004], lr: 0.088692, loss: 2.1726
2022-07-10 04:08:23 - train: epoch 0027, iter [01100, 05004], lr: 0.088676, loss: 2.0897
2022-07-10 04:08:58 - train: epoch 0027, iter [01200, 05004], lr: 0.088659, loss: 2.3420
2022-07-10 04:09:33 - train: epoch 0027, iter [01300, 05004], lr: 0.088642, loss: 2.3971
2022-07-10 04:10:07 - train: epoch 0027, iter [01400, 05004], lr: 0.088626, loss: 2.1690
2022-07-10 04:10:42 - train: epoch 0027, iter [01500, 05004], lr: 0.088609, loss: 2.1047
2022-07-10 04:11:16 - train: epoch 0027, iter [01600, 05004], lr: 0.088593, loss: 2.1632
2022-07-10 04:11:50 - train: epoch 0027, iter [01700, 05004], lr: 0.088576, loss: 2.1545
2022-07-10 04:12:24 - train: epoch 0027, iter [01800, 05004], lr: 0.088559, loss: 2.1596
2022-07-10 04:12:58 - train: epoch 0027, iter [01900, 05004], lr: 0.088543, loss: 2.1605
2022-07-10 04:13:33 - train: epoch 0027, iter [02000, 05004], lr: 0.088526, loss: 2.1203
2022-07-10 04:14:08 - train: epoch 0027, iter [02100, 05004], lr: 0.088509, loss: 2.3545
2022-07-10 04:14:42 - train: epoch 0027, iter [02200, 05004], lr: 0.088493, loss: 2.1898
2022-07-10 04:15:17 - train: epoch 0027, iter [02300, 05004], lr: 0.088476, loss: 2.5250
2022-07-10 04:15:51 - train: epoch 0027, iter [02400, 05004], lr: 0.088459, loss: 2.1159
2022-07-10 04:16:26 - train: epoch 0027, iter [02500, 05004], lr: 0.088442, loss: 2.2290
2022-07-10 04:17:00 - train: epoch 0027, iter [02600, 05004], lr: 0.088426, loss: 2.2855
2022-07-10 04:17:34 - train: epoch 0027, iter [02700, 05004], lr: 0.088409, loss: 2.1598
2022-07-10 04:18:08 - train: epoch 0027, iter [02800, 05004], lr: 0.088392, loss: 2.1132
2022-07-10 04:18:42 - train: epoch 0027, iter [02900, 05004], lr: 0.088375, loss: 2.2410
2022-07-10 04:19:17 - train: epoch 0027, iter [03000, 05004], lr: 0.088359, loss: 1.9418
2022-07-10 04:19:51 - train: epoch 0027, iter [03100, 05004], lr: 0.088342, loss: 2.0062
2022-07-10 04:20:25 - train: epoch 0027, iter [03200, 05004], lr: 0.088325, loss: 1.9650
2022-07-10 04:20:59 - train: epoch 0027, iter [03300, 05004], lr: 0.088308, loss: 2.1618
2022-07-10 04:21:34 - train: epoch 0027, iter [03400, 05004], lr: 0.088291, loss: 2.1296
2022-07-10 04:22:08 - train: epoch 0027, iter [03500, 05004], lr: 0.088275, loss: 2.1941
2022-07-10 04:22:43 - train: epoch 0027, iter [03600, 05004], lr: 0.088258, loss: 2.2386
2022-07-10 04:23:17 - train: epoch 0027, iter [03700, 05004], lr: 0.088241, loss: 2.1616
2022-07-10 04:23:52 - train: epoch 0027, iter [03800, 05004], lr: 0.088224, loss: 1.9864
2022-07-10 04:24:27 - train: epoch 0027, iter [03900, 05004], lr: 0.088207, loss: 1.9864
2022-07-10 04:25:00 - train: epoch 0027, iter [04000, 05004], lr: 0.088190, loss: 2.1563
2022-07-10 04:25:35 - train: epoch 0027, iter [04100, 05004], lr: 0.088173, loss: 2.3788
2022-07-10 04:26:09 - train: epoch 0027, iter [04200, 05004], lr: 0.088157, loss: 1.8446
2022-07-10 04:26:44 - train: epoch 0027, iter [04300, 05004], lr: 0.088140, loss: 2.1244
2022-07-10 04:27:19 - train: epoch 0027, iter [04400, 05004], lr: 0.088123, loss: 2.4769
2022-07-10 04:27:52 - train: epoch 0027, iter [04500, 05004], lr: 0.088106, loss: 2.0630
2022-07-10 04:28:26 - train: epoch 0027, iter [04600, 05004], lr: 0.088089, loss: 1.9813
2022-07-10 04:29:01 - train: epoch 0027, iter [04700, 05004], lr: 0.088072, loss: 2.2702
2022-07-10 04:29:36 - train: epoch 0027, iter [04800, 05004], lr: 0.088055, loss: 2.1082
2022-07-10 04:30:10 - train: epoch 0027, iter [04900, 05004], lr: 0.088038, loss: 2.0875
2022-07-10 04:30:43 - train: epoch 0027, iter [05000, 05004], lr: 0.088021, loss: 1.7459
2022-07-10 04:30:44 - train: epoch 027, train_loss: 2.1409
2022-07-10 04:31:59 - eval: epoch: 027, acc1: 56.182%, acc5: 80.584%, test_loss: 1.8715, per_image_load_time: 2.479ms, per_image_inference_time: 0.348ms
2022-07-10 04:32:00 - until epoch: 027, best_acc1: 56.182%
2022-07-10 04:32:00 - epoch 028 lr: 0.088020
2022-07-10 04:32:39 - train: epoch 0028, iter [00100, 05004], lr: 0.088003, loss: 1.9637
2022-07-10 04:33:12 - train: epoch 0028, iter [00200, 05004], lr: 0.087986, loss: 1.9476
2022-07-10 04:33:46 - train: epoch 0028, iter [00300, 05004], lr: 0.087969, loss: 2.2034
2022-07-10 04:34:20 - train: epoch 0028, iter [00400, 05004], lr: 0.087952, loss: 1.9825
2022-07-10 04:34:53 - train: epoch 0028, iter [00500, 05004], lr: 0.087935, loss: 1.9859
2022-07-10 04:35:29 - train: epoch 0028, iter [00600, 05004], lr: 0.087918, loss: 2.3308
2022-07-10 04:36:02 - train: epoch 0028, iter [00700, 05004], lr: 0.087901, loss: 2.3876
2022-07-10 04:36:36 - train: epoch 0028, iter [00800, 05004], lr: 0.087884, loss: 1.9644
2022-07-10 04:37:10 - train: epoch 0028, iter [00900, 05004], lr: 0.087867, loss: 2.1079
2022-07-10 04:37:44 - train: epoch 0028, iter [01000, 05004], lr: 0.087850, loss: 2.3533
2022-07-10 04:38:19 - train: epoch 0028, iter [01100, 05004], lr: 0.087833, loss: 1.8757
2022-07-10 04:38:52 - train: epoch 0028, iter [01200, 05004], lr: 0.087816, loss: 2.0161
2022-07-10 04:39:27 - train: epoch 0028, iter [01300, 05004], lr: 0.087799, loss: 2.0890
2022-07-10 04:40:00 - train: epoch 0028, iter [01400, 05004], lr: 0.087781, loss: 2.1770
2022-07-10 04:40:36 - train: epoch 0028, iter [01500, 05004], lr: 0.087764, loss: 2.0469
2022-07-10 04:41:09 - train: epoch 0028, iter [01600, 05004], lr: 0.087747, loss: 2.0151
2022-07-10 04:41:43 - train: epoch 0028, iter [01700, 05004], lr: 0.087730, loss: 2.2691
2022-07-10 04:42:18 - train: epoch 0028, iter [01800, 05004], lr: 0.087713, loss: 2.0374
2022-07-10 04:42:52 - train: epoch 0028, iter [01900, 05004], lr: 0.087696, loss: 2.0446
2022-07-10 04:43:27 - train: epoch 0028, iter [02000, 05004], lr: 0.087678, loss: 2.0987
2022-07-10 04:44:01 - train: epoch 0028, iter [02100, 05004], lr: 0.087661, loss: 2.3389
2022-07-10 04:44:35 - train: epoch 0028, iter [02200, 05004], lr: 0.087644, loss: 2.2367
2022-07-10 04:45:09 - train: epoch 0028, iter [02300, 05004], lr: 0.087627, loss: 2.1167
2022-07-10 04:45:44 - train: epoch 0028, iter [02400, 05004], lr: 0.087610, loss: 2.2077
2022-07-10 04:46:18 - train: epoch 0028, iter [02500, 05004], lr: 0.087592, loss: 2.2070
2022-07-10 04:46:53 - train: epoch 0028, iter [02600, 05004], lr: 0.087575, loss: 2.2075
2022-07-10 04:47:28 - train: epoch 0028, iter [02700, 05004], lr: 0.087558, loss: 2.1307
2022-07-10 04:48:02 - train: epoch 0028, iter [02800, 05004], lr: 0.087541, loss: 1.8739
2022-07-10 04:48:36 - train: epoch 0028, iter [02900, 05004], lr: 0.087523, loss: 2.2595
2022-07-10 04:49:10 - train: epoch 0028, iter [03000, 05004], lr: 0.087506, loss: 2.0220
2022-07-10 04:49:45 - train: epoch 0028, iter [03100, 05004], lr: 0.087489, loss: 2.1421
2022-07-10 04:50:20 - train: epoch 0028, iter [03200, 05004], lr: 0.087471, loss: 2.1693
2022-07-10 04:50:53 - train: epoch 0028, iter [03300, 05004], lr: 0.087454, loss: 2.0219
2022-07-10 04:51:28 - train: epoch 0028, iter [03400, 05004], lr: 0.087437, loss: 2.1260
2022-07-10 04:52:02 - train: epoch 0028, iter [03500, 05004], lr: 0.087419, loss: 2.0558
2022-07-10 04:52:37 - train: epoch 0028, iter [03600, 05004], lr: 0.087402, loss: 1.9615
2022-07-10 04:53:11 - train: epoch 0028, iter [03700, 05004], lr: 0.087385, loss: 2.2824
2022-07-10 04:53:45 - train: epoch 0028, iter [03800, 05004], lr: 0.087367, loss: 1.8977
2022-07-10 04:54:19 - train: epoch 0028, iter [03900, 05004], lr: 0.087350, loss: 2.3554
2022-07-10 04:54:53 - train: epoch 0028, iter [04000, 05004], lr: 0.087332, loss: 2.2375
2022-07-10 04:55:28 - train: epoch 0028, iter [04100, 05004], lr: 0.087315, loss: 2.1925
2022-07-10 04:56:02 - train: epoch 0028, iter [04200, 05004], lr: 0.087298, loss: 2.2580
2022-07-10 04:56:36 - train: epoch 0028, iter [04300, 05004], lr: 0.087280, loss: 2.1442
2022-07-10 04:57:11 - train: epoch 0028, iter [04400, 05004], lr: 0.087263, loss: 2.0351
2022-07-10 04:57:45 - train: epoch 0028, iter [04500, 05004], lr: 0.087245, loss: 2.0091
2022-07-10 04:58:20 - train: epoch 0028, iter [04600, 05004], lr: 0.087228, loss: 2.0572
2022-07-10 04:58:53 - train: epoch 0028, iter [04700, 05004], lr: 0.087210, loss: 2.4511
2022-07-10 04:59:28 - train: epoch 0028, iter [04800, 05004], lr: 0.087193, loss: 2.2346
2022-07-10 05:00:02 - train: epoch 0028, iter [04900, 05004], lr: 0.087175, loss: 2.1439
2022-07-10 05:00:34 - train: epoch 0028, iter [05000, 05004], lr: 0.087158, loss: 1.9593
2022-07-10 05:00:35 - train: epoch 028, train_loss: 2.1348
2022-07-10 05:01:50 - eval: epoch: 028, acc1: 56.366%, acc5: 80.472%, test_loss: 1.8672, per_image_load_time: 2.267ms, per_image_inference_time: 0.361ms
2022-07-10 05:01:51 - until epoch: 028, best_acc1: 56.366%
2022-07-10 05:01:51 - epoch 029 lr: 0.087157
2022-07-10 05:02:29 - train: epoch 0029, iter [00100, 05004], lr: 0.087140, loss: 2.0621
2022-07-10 05:03:04 - train: epoch 0029, iter [00200, 05004], lr: 0.087122, loss: 2.1535
2022-07-10 05:03:38 - train: epoch 0029, iter [00300, 05004], lr: 0.087105, loss: 2.1682
2022-07-10 05:04:11 - train: epoch 0029, iter [00400, 05004], lr: 0.087087, loss: 2.3090
2022-07-10 05:04:46 - train: epoch 0029, iter [00500, 05004], lr: 0.087070, loss: 2.1606
2022-07-10 05:05:20 - train: epoch 0029, iter [00600, 05004], lr: 0.087052, loss: 2.3269
2022-07-10 05:05:55 - train: epoch 0029, iter [00700, 05004], lr: 0.087034, loss: 1.9061
2022-07-10 05:06:28 - train: epoch 0029, iter [00800, 05004], lr: 0.087017, loss: 2.1233
2022-07-10 05:07:03 - train: epoch 0029, iter [00900, 05004], lr: 0.086999, loss: 2.2811
2022-07-10 05:07:37 - train: epoch 0029, iter [01000, 05004], lr: 0.086982, loss: 2.1095
2022-07-10 05:08:11 - train: epoch 0029, iter [01100, 05004], lr: 0.086964, loss: 2.0816
2022-07-10 05:08:45 - train: epoch 0029, iter [01200, 05004], lr: 0.086946, loss: 2.1806
2022-07-10 05:09:20 - train: epoch 0029, iter [01300, 05004], lr: 0.086929, loss: 2.2141
2022-07-10 05:09:54 - train: epoch 0029, iter [01400, 05004], lr: 0.086911, loss: 2.2789
2022-07-10 05:10:29 - train: epoch 0029, iter [01500, 05004], lr: 0.086894, loss: 2.2060
2022-07-10 05:11:04 - train: epoch 0029, iter [01600, 05004], lr: 0.086876, loss: 2.2430
2022-07-10 05:11:38 - train: epoch 0029, iter [01700, 05004], lr: 0.086858, loss: 2.1564
2022-07-10 05:12:12 - train: epoch 0029, iter [01800, 05004], lr: 0.086841, loss: 2.1683
2022-07-10 05:12:47 - train: epoch 0029, iter [01900, 05004], lr: 0.086823, loss: 1.8144
2022-07-10 05:13:22 - train: epoch 0029, iter [02000, 05004], lr: 0.086805, loss: 2.3964
2022-07-10 05:13:55 - train: epoch 0029, iter [02100, 05004], lr: 0.086787, loss: 2.1056
2022-07-10 05:14:30 - train: epoch 0029, iter [02200, 05004], lr: 0.086770, loss: 2.2770
2022-07-10 05:15:04 - train: epoch 0029, iter [02300, 05004], lr: 0.086752, loss: 2.2247
2022-07-10 05:15:40 - train: epoch 0029, iter [02400, 05004], lr: 0.086734, loss: 2.0844
2022-07-10 05:16:14 - train: epoch 0029, iter [02500, 05004], lr: 0.086716, loss: 2.1877
2022-07-10 05:16:49 - train: epoch 0029, iter [02600, 05004], lr: 0.086699, loss: 2.3161
2022-07-10 05:17:23 - train: epoch 0029, iter [02700, 05004], lr: 0.086681, loss: 2.0557
2022-07-10 05:17:58 - train: epoch 0029, iter [02800, 05004], lr: 0.086663, loss: 1.9744
2022-07-10 05:18:32 - train: epoch 0029, iter [02900, 05004], lr: 0.086645, loss: 1.9888
2022-07-10 05:19:06 - train: epoch 0029, iter [03000, 05004], lr: 0.086628, loss: 2.2045
2022-07-10 05:19:41 - train: epoch 0029, iter [03100, 05004], lr: 0.086610, loss: 2.0512
2022-07-10 05:20:15 - train: epoch 0029, iter [03200, 05004], lr: 0.086592, loss: 2.1502
2022-07-10 05:20:49 - train: epoch 0029, iter [03300, 05004], lr: 0.086574, loss: 2.0188
2022-07-10 05:21:24 - train: epoch 0029, iter [03400, 05004], lr: 0.086556, loss: 2.1104
2022-07-10 05:21:59 - train: epoch 0029, iter [03500, 05004], lr: 0.086538, loss: 2.2944
2022-07-10 05:22:33 - train: epoch 0029, iter [03600, 05004], lr: 0.086521, loss: 1.8918
2022-07-10 05:23:07 - train: epoch 0029, iter [03700, 05004], lr: 0.086503, loss: 2.1196
2022-07-10 05:23:41 - train: epoch 0029, iter [03800, 05004], lr: 0.086485, loss: 2.1749
2022-07-10 05:24:16 - train: epoch 0029, iter [03900, 05004], lr: 0.086467, loss: 2.0279
2022-07-10 05:24:50 - train: epoch 0029, iter [04000, 05004], lr: 0.086449, loss: 2.2767
2022-07-10 05:25:24 - train: epoch 0029, iter [04100, 05004], lr: 0.086431, loss: 2.1495
2022-07-10 05:25:58 - train: epoch 0029, iter [04200, 05004], lr: 0.086413, loss: 2.0113
2022-07-10 05:26:32 - train: epoch 0029, iter [04300, 05004], lr: 0.086395, loss: 1.9707
2022-07-10 05:27:06 - train: epoch 0029, iter [04400, 05004], lr: 0.086377, loss: 2.1861
2022-07-10 05:27:41 - train: epoch 0029, iter [04500, 05004], lr: 0.086359, loss: 2.3048
2022-07-10 05:28:15 - train: epoch 0029, iter [04600, 05004], lr: 0.086341, loss: 2.3310
2022-07-10 05:28:50 - train: epoch 0029, iter [04700, 05004], lr: 0.086323, loss: 1.8316
2022-07-10 05:29:23 - train: epoch 0029, iter [04800, 05004], lr: 0.086305, loss: 2.0163
2022-07-10 05:29:58 - train: epoch 0029, iter [04900, 05004], lr: 0.086287, loss: 2.2117
2022-07-10 05:30:30 - train: epoch 0029, iter [05000, 05004], lr: 0.086269, loss: 1.9669
2022-07-10 05:30:32 - train: epoch 029, train_loss: 2.1237
2022-07-10 05:31:47 - eval: epoch: 029, acc1: 55.678%, acc5: 80.166%, test_loss: 1.8934, per_image_load_time: 2.581ms, per_image_inference_time: 0.332ms
2022-07-10 05:31:48 - until epoch: 029, best_acc1: 56.366%
2022-07-10 05:31:48 - epoch 030 lr: 0.086269
2022-07-10 05:32:27 - train: epoch 0030, iter [00100, 05004], lr: 0.086251, loss: 2.0983
2022-07-10 05:33:01 - train: epoch 0030, iter [00200, 05004], lr: 0.086233, loss: 2.0610
2022-07-10 05:33:35 - train: epoch 0030, iter [00300, 05004], lr: 0.086215, loss: 1.9338
2022-07-10 05:34:09 - train: epoch 0030, iter [00400, 05004], lr: 0.086197, loss: 1.9932
2022-07-10 05:34:43 - train: epoch 0030, iter [00500, 05004], lr: 0.086179, loss: 2.2247
2022-07-10 05:35:18 - train: epoch 0030, iter [00600, 05004], lr: 0.086160, loss: 1.8161
2022-07-10 05:35:52 - train: epoch 0030, iter [00700, 05004], lr: 0.086142, loss: 2.2389
2022-07-10 05:36:26 - train: epoch 0030, iter [00800, 05004], lr: 0.086124, loss: 1.9540
2022-07-10 05:37:00 - train: epoch 0030, iter [00900, 05004], lr: 0.086106, loss: 2.2641
2022-07-10 05:37:34 - train: epoch 0030, iter [01000, 05004], lr: 0.086088, loss: 1.8936
2022-07-10 05:38:10 - train: epoch 0030, iter [01100, 05004], lr: 0.086070, loss: 2.0714
2022-07-10 05:38:43 - train: epoch 0030, iter [01200, 05004], lr: 0.086052, loss: 2.0084
2022-07-10 05:39:18 - train: epoch 0030, iter [01300, 05004], lr: 0.086034, loss: 1.9825
2022-07-10 05:39:52 - train: epoch 0030, iter [01400, 05004], lr: 0.086016, loss: 2.0711
2022-07-10 05:40:27 - train: epoch 0030, iter [01500, 05004], lr: 0.085998, loss: 2.0524
2022-07-10 05:41:01 - train: epoch 0030, iter [01600, 05004], lr: 0.085979, loss: 2.0662
2022-07-10 05:41:35 - train: epoch 0030, iter [01700, 05004], lr: 0.085961, loss: 2.3716
2022-07-10 05:42:09 - train: epoch 0030, iter [01800, 05004], lr: 0.085943, loss: 2.0720
2022-07-10 05:42:45 - train: epoch 0030, iter [01900, 05004], lr: 0.085925, loss: 2.1774
2022-07-10 05:43:18 - train: epoch 0030, iter [02000, 05004], lr: 0.085907, loss: 1.9708
2022-07-10 05:43:52 - train: epoch 0030, iter [02100, 05004], lr: 0.085888, loss: 2.2171
2022-07-10 05:44:26 - train: epoch 0030, iter [02200, 05004], lr: 0.085870, loss: 2.3076
2022-07-10 05:45:01 - train: epoch 0030, iter [02300, 05004], lr: 0.085852, loss: 1.9550
2022-07-10 05:45:35 - train: epoch 0030, iter [02400, 05004], lr: 0.085834, loss: 2.2971
2022-07-10 05:46:10 - train: epoch 0030, iter [02500, 05004], lr: 0.085815, loss: 2.1389
2022-07-10 05:46:44 - train: epoch 0030, iter [02600, 05004], lr: 0.085797, loss: 2.1861
2022-07-10 05:47:18 - train: epoch 0030, iter [02700, 05004], lr: 0.085779, loss: 1.9851
2022-07-10 05:47:53 - train: epoch 0030, iter [02800, 05004], lr: 0.085761, loss: 2.1512
2022-07-10 05:48:27 - train: epoch 0030, iter [02900, 05004], lr: 0.085742, loss: 2.0645
2022-07-10 05:49:01 - train: epoch 0030, iter [03000, 05004], lr: 0.085724, loss: 2.1075
2022-07-10 05:49:35 - train: epoch 0030, iter [03100, 05004], lr: 0.085706, loss: 2.0949
2022-07-10 05:50:10 - train: epoch 0030, iter [03200, 05004], lr: 0.085687, loss: 2.0718
2022-07-10 05:50:45 - train: epoch 0030, iter [03300, 05004], lr: 0.085669, loss: 1.9785
2022-07-10 05:51:19 - train: epoch 0030, iter [03400, 05004], lr: 0.085651, loss: 1.9256
2022-07-10 05:51:53 - train: epoch 0030, iter [03500, 05004], lr: 0.085632, loss: 2.0584
2022-07-10 05:52:28 - train: epoch 0030, iter [03600, 05004], lr: 0.085614, loss: 2.2161
2022-07-10 05:53:02 - train: epoch 0030, iter [03700, 05004], lr: 0.085596, loss: 1.9997
2022-07-10 05:53:36 - train: epoch 0030, iter [03800, 05004], lr: 0.085577, loss: 2.1087
2022-07-10 05:54:11 - train: epoch 0030, iter [03900, 05004], lr: 0.085559, loss: 1.8471
2022-07-10 05:54:45 - train: epoch 0030, iter [04000, 05004], lr: 0.085541, loss: 1.8340
2022-07-10 05:55:19 - train: epoch 0030, iter [04100, 05004], lr: 0.085522, loss: 1.9797
2022-07-10 05:55:54 - train: epoch 0030, iter [04200, 05004], lr: 0.085504, loss: 2.4076
2022-07-10 05:56:29 - train: epoch 0030, iter [04300, 05004], lr: 0.085485, loss: 2.0610
2022-07-10 05:57:02 - train: epoch 0030, iter [04400, 05004], lr: 0.085467, loss: 2.1157
2022-07-10 05:57:38 - train: epoch 0030, iter [04500, 05004], lr: 0.085448, loss: 2.1491
2022-07-10 05:58:12 - train: epoch 0030, iter [04600, 05004], lr: 0.085430, loss: 1.7928
2022-07-10 05:58:46 - train: epoch 0030, iter [04700, 05004], lr: 0.085412, loss: 1.9790
2022-07-10 05:59:20 - train: epoch 0030, iter [04800, 05004], lr: 0.085393, loss: 2.1742
2022-07-10 05:59:55 - train: epoch 0030, iter [04900, 05004], lr: 0.085375, loss: 2.1853
2022-07-10 06:00:28 - train: epoch 0030, iter [05000, 05004], lr: 0.085356, loss: 2.0655
2022-07-10 06:00:29 - train: epoch 030, train_loss: 2.1138
2022-07-10 06:01:44 - eval: epoch: 030, acc1: 55.082%, acc5: 79.862%, test_loss: 1.9231, per_image_load_time: 2.515ms, per_image_inference_time: 0.359ms
2022-07-10 06:01:45 - until epoch: 030, best_acc1: 56.366%
2022-07-10 06:01:45 - epoch 031 lr: 0.085355
2022-07-10 06:02:24 - train: epoch 0031, iter [00100, 05004], lr: 0.085337, loss: 2.1927
2022-07-10 06:02:58 - train: epoch 0031, iter [00200, 05004], lr: 0.085318, loss: 2.0461
2022-07-10 06:03:32 - train: epoch 0031, iter [00300, 05004], lr: 0.085300, loss: 2.1414
2022-07-10 06:04:06 - train: epoch 0031, iter [00400, 05004], lr: 0.085281, loss: 1.9182
2022-07-10 06:04:40 - train: epoch 0031, iter [00500, 05004], lr: 0.085263, loss: 1.9827
2022-07-10 06:05:14 - train: epoch 0031, iter [00600, 05004], lr: 0.085244, loss: 2.1864
2022-07-10 06:05:48 - train: epoch 0031, iter [00700, 05004], lr: 0.085226, loss: 2.1414
2022-07-10 06:06:22 - train: epoch 0031, iter [00800, 05004], lr: 0.085207, loss: 2.2879
2022-07-10 06:06:57 - train: epoch 0031, iter [00900, 05004], lr: 0.085188, loss: 2.1227
2022-07-10 06:07:31 - train: epoch 0031, iter [01000, 05004], lr: 0.085170, loss: 2.3936
2022-07-10 06:08:05 - train: epoch 0031, iter [01100, 05004], lr: 0.085151, loss: 2.1668
2022-07-10 06:08:40 - train: epoch 0031, iter [01200, 05004], lr: 0.085133, loss: 2.2091
2022-07-10 06:09:13 - train: epoch 0031, iter [01300, 05004], lr: 0.085114, loss: 1.8627
2022-07-10 06:09:48 - train: epoch 0031, iter [01400, 05004], lr: 0.085095, loss: 2.2285
2022-07-10 06:10:22 - train: epoch 0031, iter [01500, 05004], lr: 0.085077, loss: 2.1966
2022-07-10 06:10:57 - train: epoch 0031, iter [01600, 05004], lr: 0.085058, loss: 2.1757
2022-07-10 06:11:31 - train: epoch 0031, iter [01700, 05004], lr: 0.085039, loss: 2.1592
2022-07-10 06:12:05 - train: epoch 0031, iter [01800, 05004], lr: 0.085021, loss: 2.0921
2022-07-10 06:12:40 - train: epoch 0031, iter [01900, 05004], lr: 0.085002, loss: 2.0492
2022-07-10 06:13:14 - train: epoch 0031, iter [02000, 05004], lr: 0.084983, loss: 2.0720
2022-07-10 06:13:48 - train: epoch 0031, iter [02100, 05004], lr: 0.084965, loss: 2.0296
2022-07-10 06:14:22 - train: epoch 0031, iter [02200, 05004], lr: 0.084946, loss: 2.1148
2022-07-10 06:14:57 - train: epoch 0031, iter [02300, 05004], lr: 0.084927, loss: 1.9445
2022-07-10 06:15:30 - train: epoch 0031, iter [02400, 05004], lr: 0.084909, loss: 2.0571
2022-07-10 06:16:05 - train: epoch 0031, iter [02500, 05004], lr: 0.084890, loss: 1.9622
2022-07-10 06:16:39 - train: epoch 0031, iter [02600, 05004], lr: 0.084871, loss: 2.0175
2022-07-10 06:17:12 - train: epoch 0031, iter [02700, 05004], lr: 0.084852, loss: 2.3505
2022-07-10 06:17:47 - train: epoch 0031, iter [02800, 05004], lr: 0.084834, loss: 2.5068
2022-07-10 06:18:22 - train: epoch 0031, iter [02900, 05004], lr: 0.084815, loss: 2.0709
2022-07-10 06:18:55 - train: epoch 0031, iter [03000, 05004], lr: 0.084796, loss: 2.4151
2022-07-10 06:19:30 - train: epoch 0031, iter [03100, 05004], lr: 0.084777, loss: 2.2652
2022-07-10 06:20:04 - train: epoch 0031, iter [03200, 05004], lr: 0.084759, loss: 2.2650
2022-07-10 06:20:38 - train: epoch 0031, iter [03300, 05004], lr: 0.084740, loss: 2.1103
2022-07-10 06:21:13 - train: epoch 0031, iter [03400, 05004], lr: 0.084721, loss: 2.2457
2022-07-10 06:21:46 - train: epoch 0031, iter [03500, 05004], lr: 0.084702, loss: 2.0977
2022-07-10 06:22:20 - train: epoch 0031, iter [03600, 05004], lr: 0.084683, loss: 2.1066
2022-07-10 06:22:55 - train: epoch 0031, iter [03700, 05004], lr: 0.084664, loss: 2.0266
2022-07-10 06:23:28 - train: epoch 0031, iter [03800, 05004], lr: 0.084646, loss: 1.9789
2022-07-10 06:24:03 - train: epoch 0031, iter [03900, 05004], lr: 0.084627, loss: 2.2848
2022-07-10 06:24:37 - train: epoch 0031, iter [04000, 05004], lr: 0.084608, loss: 1.9059
2022-07-10 06:25:12 - train: epoch 0031, iter [04100, 05004], lr: 0.084589, loss: 2.0262
2022-07-10 06:25:45 - train: epoch 0031, iter [04200, 05004], lr: 0.084570, loss: 2.1804
2022-07-10 06:26:20 - train: epoch 0031, iter [04300, 05004], lr: 0.084551, loss: 1.8848
2022-07-10 06:26:55 - train: epoch 0031, iter [04400, 05004], lr: 0.084532, loss: 2.2831
2022-07-10 06:27:29 - train: epoch 0031, iter [04500, 05004], lr: 0.084513, loss: 2.2202
2022-07-10 06:28:02 - train: epoch 0031, iter [04600, 05004], lr: 0.084494, loss: 2.2246
2022-07-10 06:28:36 - train: epoch 0031, iter [04700, 05004], lr: 0.084475, loss: 2.1289
2022-07-10 06:29:10 - train: epoch 0031, iter [04800, 05004], lr: 0.084456, loss: 2.0429
2022-07-10 06:29:44 - train: epoch 0031, iter [04900, 05004], lr: 0.084437, loss: 1.8692
2022-07-10 06:30:17 - train: epoch 0031, iter [05000, 05004], lr: 0.084418, loss: 2.0576
2022-07-10 06:30:18 - train: epoch 031, train_loss: 2.1043
2022-07-10 06:31:33 - eval: epoch: 031, acc1: 56.998%, acc5: 81.452%, test_loss: 1.8177, per_image_load_time: 2.527ms, per_image_inference_time: 0.372ms
2022-07-10 06:31:33 - until epoch: 031, best_acc1: 56.998%
2022-07-10 06:31:33 - epoch 032 lr: 0.084418
2022-07-10 06:32:13 - train: epoch 0032, iter [00100, 05004], lr: 0.084399, loss: 1.9777
2022-07-10 06:32:46 - train: epoch 0032, iter [00200, 05004], lr: 0.084380, loss: 2.3009
2022-07-10 06:33:20 - train: epoch 0032, iter [00300, 05004], lr: 0.084361, loss: 1.9268
2022-07-10 06:33:54 - train: epoch 0032, iter [00400, 05004], lr: 0.084342, loss: 2.1172
2022-07-10 06:34:26 - train: epoch 0032, iter [00500, 05004], lr: 0.084323, loss: 2.1169
2022-07-10 06:35:01 - train: epoch 0032, iter [00600, 05004], lr: 0.084304, loss: 2.1563
2022-07-10 06:35:35 - train: epoch 0032, iter [00700, 05004], lr: 0.084285, loss: 1.9363
2022-07-10 06:36:08 - train: epoch 0032, iter [00800, 05004], lr: 0.084266, loss: 1.9595
2022-07-10 06:36:42 - train: epoch 0032, iter [00900, 05004], lr: 0.084247, loss: 2.1501
2022-07-10 06:37:16 - train: epoch 0032, iter [01000, 05004], lr: 0.084228, loss: 2.1118
2022-07-10 06:37:49 - train: epoch 0032, iter [01100, 05004], lr: 0.084208, loss: 2.0945
2022-07-10 06:38:24 - train: epoch 0032, iter [01200, 05004], lr: 0.084189, loss: 2.1673
2022-07-10 06:38:57 - train: epoch 0032, iter [01300, 05004], lr: 0.084170, loss: 2.3073
2022-07-10 06:39:32 - train: epoch 0032, iter [01400, 05004], lr: 0.084151, loss: 2.4369
2022-07-10 06:40:05 - train: epoch 0032, iter [01500, 05004], lr: 0.084132, loss: 2.0871
2022-07-10 06:40:39 - train: epoch 0032, iter [01600, 05004], lr: 0.084113, loss: 2.0264
2022-07-10 06:41:13 - train: epoch 0032, iter [01700, 05004], lr: 0.084094, loss: 2.1268
2022-07-10 06:41:48 - train: epoch 0032, iter [01800, 05004], lr: 0.084075, loss: 2.3042
2022-07-10 06:42:21 - train: epoch 0032, iter [01900, 05004], lr: 0.084056, loss: 1.8929
2022-07-10 06:42:55 - train: epoch 0032, iter [02000, 05004], lr: 0.084036, loss: 2.1077
2022-07-10 06:43:29 - train: epoch 0032, iter [02100, 05004], lr: 0.084017, loss: 2.0069
2022-07-10 06:44:02 - train: epoch 0032, iter [02200, 05004], lr: 0.083998, loss: 2.1944
2022-07-10 06:44:37 - train: epoch 0032, iter [02300, 05004], lr: 0.083979, loss: 2.2019
2022-07-10 06:45:11 - train: epoch 0032, iter [02400, 05004], lr: 0.083960, loss: 2.1715
2022-07-10 06:45:45 - train: epoch 0032, iter [02500, 05004], lr: 0.083940, loss: 1.8705
2022-07-10 06:46:19 - train: epoch 0032, iter [02600, 05004], lr: 0.083921, loss: 1.7809
2022-07-10 06:46:53 - train: epoch 0032, iter [02700, 05004], lr: 0.083902, loss: 2.0429
2022-07-10 06:47:26 - train: epoch 0032, iter [02800, 05004], lr: 0.083883, loss: 2.0035
2022-07-10 06:48:02 - train: epoch 0032, iter [02900, 05004], lr: 0.083864, loss: 1.9273
2022-07-10 06:48:35 - train: epoch 0032, iter [03000, 05004], lr: 0.083844, loss: 1.9655
2022-07-10 06:49:10 - train: epoch 0032, iter [03100, 05004], lr: 0.083825, loss: 2.1987
2022-07-10 06:49:45 - train: epoch 0032, iter [03200, 05004], lr: 0.083806, loss: 2.0818
2022-07-10 06:50:18 - train: epoch 0032, iter [03300, 05004], lr: 0.083786, loss: 2.0175
2022-07-10 06:50:53 - train: epoch 0032, iter [03400, 05004], lr: 0.083767, loss: 1.8927
2022-07-10 06:51:28 - train: epoch 0032, iter [03500, 05004], lr: 0.083748, loss: 1.9975
2022-07-10 06:52:02 - train: epoch 0032, iter [03600, 05004], lr: 0.083729, loss: 2.2457
2022-07-10 06:52:36 - train: epoch 0032, iter [03700, 05004], lr: 0.083709, loss: 2.2103
2022-07-10 06:53:10 - train: epoch 0032, iter [03800, 05004], lr: 0.083690, loss: 1.9098
2022-07-10 06:53:44 - train: epoch 0032, iter [03900, 05004], lr: 0.083671, loss: 2.0027
2022-07-10 06:54:18 - train: epoch 0032, iter [04000, 05004], lr: 0.083651, loss: 2.1888
2022-07-10 06:54:52 - train: epoch 0032, iter [04100, 05004], lr: 0.083632, loss: 2.0617
2022-07-10 06:55:27 - train: epoch 0032, iter [04200, 05004], lr: 0.083613, loss: 1.9603
2022-07-10 06:56:01 - train: epoch 0032, iter [04300, 05004], lr: 0.083593, loss: 2.4574
2022-07-10 06:56:35 - train: epoch 0032, iter [04400, 05004], lr: 0.083574, loss: 2.0872
2022-07-10 06:57:10 - train: epoch 0032, iter [04500, 05004], lr: 0.083554, loss: 2.2103
2022-07-10 06:57:44 - train: epoch 0032, iter [04600, 05004], lr: 0.083535, loss: 2.0357
2022-07-10 06:58:18 - train: epoch 0032, iter [04700, 05004], lr: 0.083516, loss: 2.1820
2022-07-10 06:58:52 - train: epoch 0032, iter [04800, 05004], lr: 0.083496, loss: 2.1046
2022-07-10 06:59:26 - train: epoch 0032, iter [04900, 05004], lr: 0.083477, loss: 2.0037
2022-07-10 06:59:58 - train: epoch 0032, iter [05000, 05004], lr: 0.083457, loss: 1.9333
2022-07-10 06:59:59 - train: epoch 032, train_loss: 2.0956
2022-07-10 07:01:13 - eval: epoch: 032, acc1: 57.064%, acc5: 81.488%, test_loss: 1.8093, per_image_load_time: 2.178ms, per_image_inference_time: 0.368ms
2022-07-10 07:01:13 - until epoch: 032, best_acc1: 57.064%
2022-07-10 07:01:13 - epoch 033 lr: 0.083456
2022-07-10 07:01:52 - train: epoch 0033, iter [00100, 05004], lr: 0.083437, loss: 1.7907
2022-07-10 07:02:27 - train: epoch 0033, iter [00200, 05004], lr: 0.083418, loss: 2.0693
2022-07-10 07:03:00 - train: epoch 0033, iter [00300, 05004], lr: 0.083398, loss: 2.2098
2022-07-10 07:03:34 - train: epoch 0033, iter [00400, 05004], lr: 0.083379, loss: 1.8569
2022-07-10 07:04:08 - train: epoch 0033, iter [00500, 05004], lr: 0.083359, loss: 2.2251
2022-07-10 07:04:41 - train: epoch 0033, iter [00600, 05004], lr: 0.083340, loss: 2.0062
2022-07-10 07:05:15 - train: epoch 0033, iter [00700, 05004], lr: 0.083320, loss: 2.3126
2022-07-10 07:05:49 - train: epoch 0033, iter [00800, 05004], lr: 0.083301, loss: 2.2955
2022-07-10 07:06:23 - train: epoch 0033, iter [00900, 05004], lr: 0.083281, loss: 2.2316
2022-07-10 07:06:57 - train: epoch 0033, iter [01000, 05004], lr: 0.083262, loss: 2.2016
2022-07-10 07:07:31 - train: epoch 0033, iter [01100, 05004], lr: 0.083242, loss: 1.8723
2022-07-10 07:08:05 - train: epoch 0033, iter [01200, 05004], lr: 0.083223, loss: 2.3323
2022-07-10 07:08:39 - train: epoch 0033, iter [01300, 05004], lr: 0.083203, loss: 2.0398
2022-07-10 07:09:13 - train: epoch 0033, iter [01400, 05004], lr: 0.083183, loss: 2.1098
2022-07-10 07:09:48 - train: epoch 0033, iter [01500, 05004], lr: 0.083164, loss: 2.2233
2022-07-10 07:10:21 - train: epoch 0033, iter [01600, 05004], lr: 0.083144, loss: 2.1621
2022-07-10 07:10:55 - train: epoch 0033, iter [01700, 05004], lr: 0.083125, loss: 1.9066
2022-07-10 07:11:30 - train: epoch 0033, iter [01800, 05004], lr: 0.083105, loss: 2.2674
2022-07-10 07:12:03 - train: epoch 0033, iter [01900, 05004], lr: 0.083086, loss: 2.1017
2022-07-10 07:12:36 - train: epoch 0033, iter [02000, 05004], lr: 0.083066, loss: 2.1199
2022-07-10 07:13:10 - train: epoch 0033, iter [02100, 05004], lr: 0.083046, loss: 2.1090
2022-07-10 07:13:44 - train: epoch 0033, iter [02200, 05004], lr: 0.083027, loss: 2.2618
2022-07-10 07:14:19 - train: epoch 0033, iter [02300, 05004], lr: 0.083007, loss: 2.2513
2022-07-10 07:14:53 - train: epoch 0033, iter [02400, 05004], lr: 0.082987, loss: 2.1369
2022-07-10 07:15:28 - train: epoch 0033, iter [02500, 05004], lr: 0.082968, loss: 2.0789
2022-07-10 07:16:01 - train: epoch 0033, iter [02600, 05004], lr: 0.082948, loss: 1.8762
2022-07-10 07:16:36 - train: epoch 0033, iter [02700, 05004], lr: 0.082928, loss: 1.8784
2022-07-10 07:17:09 - train: epoch 0033, iter [02800, 05004], lr: 0.082909, loss: 2.1448
2022-07-10 07:17:43 - train: epoch 0033, iter [02900, 05004], lr: 0.082889, loss: 2.1470
2022-07-10 07:18:18 - train: epoch 0033, iter [03000, 05004], lr: 0.082869, loss: 2.0927
2022-07-10 07:18:52 - train: epoch 0033, iter [03100, 05004], lr: 0.082850, loss: 2.3456
2022-07-10 07:19:26 - train: epoch 0033, iter [03200, 05004], lr: 0.082830, loss: 1.8980
2022-07-10 07:20:00 - train: epoch 0033, iter [03300, 05004], lr: 0.082810, loss: 2.2401
2022-07-10 07:20:34 - train: epoch 0033, iter [03400, 05004], lr: 0.082790, loss: 2.1453
2022-07-10 07:21:08 - train: epoch 0033, iter [03500, 05004], lr: 0.082771, loss: 2.3290
2022-07-10 07:21:43 - train: epoch 0033, iter [03600, 05004], lr: 0.082751, loss: 2.5027
2022-07-10 07:22:16 - train: epoch 0033, iter [03700, 05004], lr: 0.082731, loss: 2.1589
2022-07-10 07:22:51 - train: epoch 0033, iter [03800, 05004], lr: 0.082711, loss: 1.9792
2022-07-10 07:23:25 - train: epoch 0033, iter [03900, 05004], lr: 0.082691, loss: 2.3525
2022-07-10 07:23:58 - train: epoch 0033, iter [04000, 05004], lr: 0.082672, loss: 2.0496
2022-07-10 07:24:33 - train: epoch 0033, iter [04100, 05004], lr: 0.082652, loss: 2.0247
2022-07-10 07:25:08 - train: epoch 0033, iter [04200, 05004], lr: 0.082632, loss: 2.0384
2022-07-10 07:25:41 - train: epoch 0033, iter [04300, 05004], lr: 0.082612, loss: 2.2004
2022-07-10 07:26:15 - train: epoch 0033, iter [04400, 05004], lr: 0.082592, loss: 2.1083
2022-07-10 07:26:49 - train: epoch 0033, iter [04500, 05004], lr: 0.082573, loss: 2.1676
2022-07-10 07:27:23 - train: epoch 0033, iter [04600, 05004], lr: 0.082553, loss: 1.9936
2022-07-10 07:27:58 - train: epoch 0033, iter [04700, 05004], lr: 0.082533, loss: 2.1036
2022-07-10 07:28:32 - train: epoch 0033, iter [04800, 05004], lr: 0.082513, loss: 2.5273
2022-07-10 07:29:06 - train: epoch 0033, iter [04900, 05004], lr: 0.082493, loss: 1.9726
2022-07-10 07:29:39 - train: epoch 0033, iter [05000, 05004], lr: 0.082473, loss: 2.1107
2022-07-10 07:29:40 - train: epoch 033, train_loss: 2.0851
2022-07-10 07:30:54 - eval: epoch: 033, acc1: 55.744%, acc5: 80.206%, test_loss: 1.8862, per_image_load_time: 2.513ms, per_image_inference_time: 0.369ms
2022-07-10 07:30:54 - until epoch: 033, best_acc1: 57.064%
2022-07-10 07:30:54 - epoch 034 lr: 0.082472
2022-07-10 07:31:34 - train: epoch 0034, iter [00100, 05004], lr: 0.082453, loss: 1.9457
2022-07-10 07:32:07 - train: epoch 0034, iter [00200, 05004], lr: 0.082433, loss: 1.7801
2022-07-10 07:32:41 - train: epoch 0034, iter [00300, 05004], lr: 0.082413, loss: 1.9772
2022-07-10 07:33:15 - train: epoch 0034, iter [00400, 05004], lr: 0.082393, loss: 2.0708
2022-07-10 07:33:49 - train: epoch 0034, iter [00500, 05004], lr: 0.082373, loss: 1.9467
2022-07-10 07:34:24 - train: epoch 0034, iter [00600, 05004], lr: 0.082353, loss: 2.2859
2022-07-10 07:34:57 - train: epoch 0034, iter [00700, 05004], lr: 0.082333, loss: 1.9623
2022-07-10 07:35:32 - train: epoch 0034, iter [00800, 05004], lr: 0.082313, loss: 2.1963
2022-07-10 07:36:05 - train: epoch 0034, iter [00900, 05004], lr: 0.082293, loss: 2.0623
2022-07-10 07:36:40 - train: epoch 0034, iter [01000, 05004], lr: 0.082273, loss: 2.0108
2022-07-10 07:37:14 - train: epoch 0034, iter [01100, 05004], lr: 0.082253, loss: 2.2260
2022-07-10 07:37:48 - train: epoch 0034, iter [01200, 05004], lr: 0.082233, loss: 2.1926
2022-07-10 07:38:22 - train: epoch 0034, iter [01300, 05004], lr: 0.082213, loss: 2.1750
2022-07-10 07:38:56 - train: epoch 0034, iter [01400, 05004], lr: 0.082193, loss: 2.2092
2022-07-10 07:39:29 - train: epoch 0034, iter [01500, 05004], lr: 0.082173, loss: 2.0331
2022-07-10 07:40:04 - train: epoch 0034, iter [01600, 05004], lr: 0.082153, loss: 2.1093
2022-07-10 07:40:39 - train: epoch 0034, iter [01700, 05004], lr: 0.082133, loss: 2.1952
2022-07-10 07:41:12 - train: epoch 0034, iter [01800, 05004], lr: 0.082113, loss: 2.2353
2022-07-10 07:41:46 - train: epoch 0034, iter [01900, 05004], lr: 0.082093, loss: 2.0999
2022-07-10 07:42:21 - train: epoch 0034, iter [02000, 05004], lr: 0.082073, loss: 1.8949
2022-07-10 07:42:55 - train: epoch 0034, iter [02100, 05004], lr: 0.082053, loss: 2.1854
2022-07-10 07:43:30 - train: epoch 0034, iter [02200, 05004], lr: 0.082033, loss: 1.7658
2022-07-10 07:44:03 - train: epoch 0034, iter [02300, 05004], lr: 0.082013, loss: 2.1665
2022-07-10 07:44:38 - train: epoch 0034, iter [02400, 05004], lr: 0.081992, loss: 2.0387
2022-07-10 07:45:12 - train: epoch 0034, iter [02500, 05004], lr: 0.081972, loss: 2.0982
2022-07-10 07:45:46 - train: epoch 0034, iter [02600, 05004], lr: 0.081952, loss: 2.2332
2022-07-10 07:46:21 - train: epoch 0034, iter [02700, 05004], lr: 0.081932, loss: 2.0792
2022-07-10 07:46:54 - train: epoch 0034, iter [02800, 05004], lr: 0.081912, loss: 2.0224
2022-07-10 07:47:28 - train: epoch 0034, iter [02900, 05004], lr: 0.081892, loss: 1.9148
2022-07-10 07:48:03 - train: epoch 0034, iter [03000, 05004], lr: 0.081872, loss: 2.0205
2022-07-10 07:48:37 - train: epoch 0034, iter [03100, 05004], lr: 0.081852, loss: 1.9069
2022-07-10 07:49:11 - train: epoch 0034, iter [03200, 05004], lr: 0.081831, loss: 1.8816
2022-07-10 07:49:45 - train: epoch 0034, iter [03300, 05004], lr: 0.081811, loss: 2.0148
2022-07-10 07:50:19 - train: epoch 0034, iter [03400, 05004], lr: 0.081791, loss: 2.1112
2022-07-10 07:50:54 - train: epoch 0034, iter [03500, 05004], lr: 0.081771, loss: 2.0130
2022-07-10 07:51:28 - train: epoch 0034, iter [03600, 05004], lr: 0.081751, loss: 1.8941
2022-07-10 07:52:02 - train: epoch 0034, iter [03700, 05004], lr: 0.081730, loss: 1.9926
2022-07-10 07:52:36 - train: epoch 0034, iter [03800, 05004], lr: 0.081710, loss: 1.9269
2022-07-10 07:53:10 - train: epoch 0034, iter [03900, 05004], lr: 0.081690, loss: 2.1986
2022-07-10 07:53:44 - train: epoch 0034, iter [04000, 05004], lr: 0.081670, loss: 1.9963
2022-07-10 07:54:18 - train: epoch 0034, iter [04100, 05004], lr: 0.081649, loss: 2.2720
2022-07-10 07:54:53 - train: epoch 0034, iter [04200, 05004], lr: 0.081629, loss: 2.0235
2022-07-10 07:55:26 - train: epoch 0034, iter [04300, 05004], lr: 0.081609, loss: 2.1576
2022-07-10 07:56:00 - train: epoch 0034, iter [04400, 05004], lr: 0.081589, loss: 2.0744
2022-07-10 07:56:35 - train: epoch 0034, iter [04500, 05004], lr: 0.081568, loss: 2.0917
2022-07-10 07:57:09 - train: epoch 0034, iter [04600, 05004], lr: 0.081548, loss: 2.2954
2022-07-10 07:57:43 - train: epoch 0034, iter [04700, 05004], lr: 0.081528, loss: 2.1314
2022-07-10 07:58:18 - train: epoch 0034, iter [04800, 05004], lr: 0.081507, loss: 2.1892
2022-07-10 07:58:52 - train: epoch 0034, iter [04900, 05004], lr: 0.081487, loss: 2.1491
2022-07-10 07:59:24 - train: epoch 0034, iter [05000, 05004], lr: 0.081467, loss: 2.0962
2022-07-10 07:59:25 - train: epoch 034, train_loss: 2.0758
2022-07-10 08:00:40 - eval: epoch: 034, acc1: 55.860%, acc5: 80.290%, test_loss: 1.8864, per_image_load_time: 2.523ms, per_image_inference_time: 0.365ms
2022-07-10 08:00:40 - until epoch: 034, best_acc1: 57.064%
2022-07-10 08:00:40 - epoch 035 lr: 0.081466
2022-07-10 08:01:19 - train: epoch 0035, iter [00100, 05004], lr: 0.081446, loss: 1.8670
2022-07-10 08:01:53 - train: epoch 0035, iter [00200, 05004], lr: 0.081425, loss: 2.0385
2022-07-10 08:02:27 - train: epoch 0035, iter [00300, 05004], lr: 0.081405, loss: 2.2431
2022-07-10 08:03:00 - train: epoch 0035, iter [00400, 05004], lr: 0.081385, loss: 1.9258
2022-07-10 08:03:33 - train: epoch 0035, iter [00500, 05004], lr: 0.081364, loss: 1.8310
2022-07-10 08:04:07 - train: epoch 0035, iter [00600, 05004], lr: 0.081344, loss: 2.1162
2022-07-10 08:04:40 - train: epoch 0035, iter [00700, 05004], lr: 0.081324, loss: 2.0510
2022-07-10 08:05:15 - train: epoch 0035, iter [00800, 05004], lr: 0.081303, loss: 2.1085
2022-07-10 08:05:48 - train: epoch 0035, iter [00900, 05004], lr: 0.081283, loss: 2.1671
2022-07-10 08:06:22 - train: epoch 0035, iter [01000, 05004], lr: 0.081262, loss: 1.9509
2022-07-10 08:06:55 - train: epoch 0035, iter [01100, 05004], lr: 0.081242, loss: 2.1409
2022-07-10 08:07:29 - train: epoch 0035, iter [01200, 05004], lr: 0.081221, loss: 1.9609
2022-07-10 08:08:03 - train: epoch 0035, iter [01300, 05004], lr: 0.081201, loss: 2.0957
2022-07-10 08:08:37 - train: epoch 0035, iter [01400, 05004], lr: 0.081181, loss: 2.0783
2022-07-10 08:09:10 - train: epoch 0035, iter [01500, 05004], lr: 0.081160, loss: 2.1232
2022-07-10 08:09:45 - train: epoch 0035, iter [01600, 05004], lr: 0.081140, loss: 1.8509
2022-07-10 08:10:18 - train: epoch 0035, iter [01700, 05004], lr: 0.081119, loss: 2.0211
2022-07-10 08:10:53 - train: epoch 0035, iter [01800, 05004], lr: 0.081099, loss: 2.0701
2022-07-10 08:11:26 - train: epoch 0035, iter [01900, 05004], lr: 0.081078, loss: 2.0091
2022-07-10 08:12:01 - train: epoch 0035, iter [02000, 05004], lr: 0.081058, loss: 2.4927
2022-07-10 08:12:34 - train: epoch 0035, iter [02100, 05004], lr: 0.081037, loss: 1.9189
2022-07-10 08:13:08 - train: epoch 0035, iter [02200, 05004], lr: 0.081017, loss: 2.1707
2022-07-10 08:13:43 - train: epoch 0035, iter [02300, 05004], lr: 0.080996, loss: 1.9887
2022-07-10 08:14:16 - train: epoch 0035, iter [02400, 05004], lr: 0.080976, loss: 2.1500
2022-07-10 08:14:51 - train: epoch 0035, iter [02500, 05004], lr: 0.080955, loss: 2.0283
2022-07-10 08:15:25 - train: epoch 0035, iter [02600, 05004], lr: 0.080935, loss: 2.1100
2022-07-10 08:15:58 - train: epoch 0035, iter [02700, 05004], lr: 0.080914, loss: 1.8771
2022-07-10 08:16:33 - train: epoch 0035, iter [02800, 05004], lr: 0.080893, loss: 2.1020
2022-07-10 08:17:07 - train: epoch 0035, iter [02900, 05004], lr: 0.080873, loss: 1.9172
2022-07-10 08:17:41 - train: epoch 0035, iter [03000, 05004], lr: 0.080852, loss: 1.6999
2022-07-10 08:18:14 - train: epoch 0035, iter [03100, 05004], lr: 0.080832, loss: 2.0790
2022-07-10 08:18:48 - train: epoch 0035, iter [03200, 05004], lr: 0.080811, loss: 1.9651
2022-07-10 08:19:23 - train: epoch 0035, iter [03300, 05004], lr: 0.080790, loss: 1.8395
2022-07-10 08:19:56 - train: epoch 0035, iter [03400, 05004], lr: 0.080770, loss: 1.9760
2022-07-10 08:20:31 - train: epoch 0035, iter [03500, 05004], lr: 0.080749, loss: 1.8886
2022-07-10 08:21:06 - train: epoch 0035, iter [03600, 05004], lr: 0.080729, loss: 2.1341
2022-07-10 08:21:40 - train: epoch 0035, iter [03700, 05004], lr: 0.080708, loss: 2.0558
2022-07-10 08:22:15 - train: epoch 0035, iter [03800, 05004], lr: 0.080687, loss: 2.1462
2022-07-10 08:22:48 - train: epoch 0035, iter [03900, 05004], lr: 0.080667, loss: 1.9731
2022-07-10 08:23:23 - train: epoch 0035, iter [04000, 05004], lr: 0.080646, loss: 1.8312
2022-07-10 08:23:57 - train: epoch 0035, iter [04100, 05004], lr: 0.080625, loss: 2.2510
2022-07-10 08:24:31 - train: epoch 0035, iter [04200, 05004], lr: 0.080605, loss: 1.9036
2022-07-10 08:25:05 - train: epoch 0035, iter [04300, 05004], lr: 0.080584, loss: 2.1217
2022-07-10 08:25:38 - train: epoch 0035, iter [04400, 05004], lr: 0.080563, loss: 2.1444
2022-07-10 08:26:13 - train: epoch 0035, iter [04500, 05004], lr: 0.080543, loss: 2.0962
2022-07-10 08:26:45 - train: epoch 0035, iter [04600, 05004], lr: 0.080522, loss: 2.0687
2022-07-10 08:27:20 - train: epoch 0035, iter [04700, 05004], lr: 0.080501, loss: 2.1609
2022-07-10 08:27:54 - train: epoch 0035, iter [04800, 05004], lr: 0.080480, loss: 2.0207
2022-07-10 08:28:27 - train: epoch 0035, iter [04900, 05004], lr: 0.080460, loss: 2.0143
2022-07-10 08:29:00 - train: epoch 0035, iter [05000, 05004], lr: 0.080439, loss: 2.0612
2022-07-10 08:29:01 - train: epoch 035, train_loss: 2.0694
2022-07-10 08:30:16 - eval: epoch: 035, acc1: 57.788%, acc5: 81.684%, test_loss: 1.7990, per_image_load_time: 2.078ms, per_image_inference_time: 0.352ms
2022-07-10 08:30:16 - until epoch: 035, best_acc1: 57.788%
2022-07-10 08:30:16 - epoch 036 lr: 0.080438
2022-07-10 08:30:56 - train: epoch 0036, iter [00100, 05004], lr: 0.080417, loss: 2.0834
2022-07-10 08:31:29 - train: epoch 0036, iter [00200, 05004], lr: 0.080397, loss: 1.7813
2022-07-10 08:32:04 - train: epoch 0036, iter [00300, 05004], lr: 0.080376, loss: 1.8473
2022-07-10 08:32:38 - train: epoch 0036, iter [00400, 05004], lr: 0.080355, loss: 2.0702
2022-07-10 08:33:12 - train: epoch 0036, iter [00500, 05004], lr: 0.080334, loss: 2.0098
2022-07-10 08:33:46 - train: epoch 0036, iter [00600, 05004], lr: 0.080313, loss: 1.9659
2022-07-10 08:34:20 - train: epoch 0036, iter [00700, 05004], lr: 0.080293, loss: 1.8936
2022-07-10 08:34:55 - train: epoch 0036, iter [00800, 05004], lr: 0.080272, loss: 1.9221
2022-07-10 08:35:27 - train: epoch 0036, iter [00900, 05004], lr: 0.080251, loss: 2.0272
2022-07-10 08:36:02 - train: epoch 0036, iter [01000, 05004], lr: 0.080230, loss: 2.0831
2022-07-10 08:36:35 - train: epoch 0036, iter [01100, 05004], lr: 0.080209, loss: 1.9864
2022-07-10 08:37:09 - train: epoch 0036, iter [01200, 05004], lr: 0.080188, loss: 1.9631
2022-07-10 08:37:43 - train: epoch 0036, iter [01300, 05004], lr: 0.080168, loss: 2.0766
2022-07-10 08:38:17 - train: epoch 0036, iter [01400, 05004], lr: 0.080147, loss: 2.1173
2022-07-10 08:38:52 - train: epoch 0036, iter [01500, 05004], lr: 0.080126, loss: 2.1343
2022-07-10 08:39:24 - train: epoch 0036, iter [01600, 05004], lr: 0.080105, loss: 2.1302
2022-07-10 08:39:58 - train: epoch 0036, iter [01700, 05004], lr: 0.080084, loss: 2.2917
2022-07-10 08:40:32 - train: epoch 0036, iter [01800, 05004], lr: 0.080063, loss: 1.8739
2022-07-10 08:41:06 - train: epoch 0036, iter [01900, 05004], lr: 0.080042, loss: 2.0456
2022-07-10 08:41:39 - train: epoch 0036, iter [02000, 05004], lr: 0.080021, loss: 1.9737
2022-07-10 08:42:14 - train: epoch 0036, iter [02100, 05004], lr: 0.080000, loss: 2.0627
2022-07-10 08:42:47 - train: epoch 0036, iter [02200, 05004], lr: 0.079979, loss: 2.2222
2022-07-10 08:43:21 - train: epoch 0036, iter [02300, 05004], lr: 0.079959, loss: 2.1965
2022-07-10 08:43:55 - train: epoch 0036, iter [02400, 05004], lr: 0.079938, loss: 2.2600
2022-07-10 08:44:29 - train: epoch 0036, iter [02500, 05004], lr: 0.079917, loss: 2.1143
2022-07-10 08:45:04 - train: epoch 0036, iter [02600, 05004], lr: 0.079896, loss: 2.1386
2022-07-10 08:45:38 - train: epoch 0036, iter [02700, 05004], lr: 0.079875, loss: 2.0672
2022-07-10 08:46:11 - train: epoch 0036, iter [02800, 05004], lr: 0.079854, loss: 2.0158
2022-07-10 08:46:46 - train: epoch 0036, iter [02900, 05004], lr: 0.079833, loss: 1.8631
2022-07-10 08:47:21 - train: epoch 0036, iter [03000, 05004], lr: 0.079812, loss: 2.0403
2022-07-10 08:47:54 - train: epoch 0036, iter [03100, 05004], lr: 0.079791, loss: 2.2618
2022-07-10 08:48:28 - train: epoch 0036, iter [03200, 05004], lr: 0.079770, loss: 2.1543
2022-07-10 08:49:02 - train: epoch 0036, iter [03300, 05004], lr: 0.079749, loss: 1.8089
2022-07-10 08:49:36 - train: epoch 0036, iter [03400, 05004], lr: 0.079728, loss: 1.7575
2022-07-10 08:50:11 - train: epoch 0036, iter [03500, 05004], lr: 0.079707, loss: 2.1586
2022-07-10 08:50:44 - train: epoch 0036, iter [03600, 05004], lr: 0.079686, loss: 2.2453
2022-07-10 08:51:19 - train: epoch 0036, iter [03700, 05004], lr: 0.079665, loss: 2.1848
2022-07-10 08:51:53 - train: epoch 0036, iter [03800, 05004], lr: 0.079643, loss: 2.0134
2022-07-10 08:52:27 - train: epoch 0036, iter [03900, 05004], lr: 0.079622, loss: 1.9341
2022-07-10 08:53:01 - train: epoch 0036, iter [04000, 05004], lr: 0.079601, loss: 2.1088
2022-07-10 08:53:35 - train: epoch 0036, iter [04100, 05004], lr: 0.079580, loss: 1.9956
2022-07-10 08:54:09 - train: epoch 0036, iter [04200, 05004], lr: 0.079559, loss: 2.2834
2022-07-10 08:54:43 - train: epoch 0036, iter [04300, 05004], lr: 0.079538, loss: 1.9939
2022-07-10 08:55:18 - train: epoch 0036, iter [04400, 05004], lr: 0.079517, loss: 2.1615
2022-07-10 08:55:51 - train: epoch 0036, iter [04500, 05004], lr: 0.079496, loss: 2.0461
2022-07-10 08:56:24 - train: epoch 0036, iter [04600, 05004], lr: 0.079475, loss: 2.0549
2022-07-10 08:56:59 - train: epoch 0036, iter [04700, 05004], lr: 0.079454, loss: 2.0163
2022-07-10 08:57:33 - train: epoch 0036, iter [04800, 05004], lr: 0.079432, loss: 2.1772
2022-07-10 08:58:07 - train: epoch 0036, iter [04900, 05004], lr: 0.079411, loss: 2.0972
2022-07-10 08:58:39 - train: epoch 0036, iter [05000, 05004], lr: 0.079390, loss: 1.9903
2022-07-10 08:58:40 - train: epoch 036, train_loss: 2.0592
2022-07-10 08:59:56 - eval: epoch: 036, acc1: 58.320%, acc5: 82.210%, test_loss: 1.7578, per_image_load_time: 2.584ms, per_image_inference_time: 0.361ms
2022-07-10 08:59:57 - until epoch: 036, best_acc1: 58.320%
2022-07-10 08:59:57 - epoch 037 lr: 0.079389
2022-07-10 09:00:36 - train: epoch 0037, iter [00100, 05004], lr: 0.079368, loss: 1.8838
2022-07-10 09:01:10 - train: epoch 0037, iter [00200, 05004], lr: 0.079347, loss: 1.8520
2022-07-10 09:01:44 - train: epoch 0037, iter [00300, 05004], lr: 0.079326, loss: 1.9786
2022-07-10 09:02:18 - train: epoch 0037, iter [00400, 05004], lr: 0.079305, loss: 1.8608
2022-07-10 09:02:53 - train: epoch 0037, iter [00500, 05004], lr: 0.079283, loss: 2.0616
2022-07-10 09:03:27 - train: epoch 0037, iter [00600, 05004], lr: 0.079262, loss: 2.2589
2022-07-10 09:04:01 - train: epoch 0037, iter [00700, 05004], lr: 0.079241, loss: 2.0403
2022-07-10 09:04:35 - train: epoch 0037, iter [00800, 05004], lr: 0.079220, loss: 2.0612
2022-07-10 09:05:10 - train: epoch 0037, iter [00900, 05004], lr: 0.079198, loss: 2.3879
2022-07-10 09:05:45 - train: epoch 0037, iter [01000, 05004], lr: 0.079177, loss: 1.9731
2022-07-10 09:06:18 - train: epoch 0037, iter [01100, 05004], lr: 0.079156, loss: 2.1806
2022-07-10 09:06:53 - train: epoch 0037, iter [01200, 05004], lr: 0.079135, loss: 1.9897
2022-07-10 09:07:28 - train: epoch 0037, iter [01300, 05004], lr: 0.079113, loss: 2.1154
2022-07-10 09:08:02 - train: epoch 0037, iter [01400, 05004], lr: 0.079092, loss: 2.2197
2022-07-10 09:08:36 - train: epoch 0037, iter [01500, 05004], lr: 0.079071, loss: 2.0183
2022-07-10 09:09:11 - train: epoch 0037, iter [01600, 05004], lr: 0.079050, loss: 1.8892
2022-07-10 09:09:45 - train: epoch 0037, iter [01700, 05004], lr: 0.079028, loss: 2.1524
2022-07-10 09:10:19 - train: epoch 0037, iter [01800, 05004], lr: 0.079007, loss: 1.9493
2022-07-10 09:10:53 - train: epoch 0037, iter [01900, 05004], lr: 0.078986, loss: 2.0482
2022-07-10 09:11:28 - train: epoch 0037, iter [02000, 05004], lr: 0.078964, loss: 2.1044
2022-07-10 09:12:03 - train: epoch 0037, iter [02100, 05004], lr: 0.078943, loss: 2.1559
2022-07-10 09:12:37 - train: epoch 0037, iter [02200, 05004], lr: 0.078922, loss: 1.8789
2022-07-10 09:13:11 - train: epoch 0037, iter [02300, 05004], lr: 0.078900, loss: 1.9505
2022-07-10 09:13:45 - train: epoch 0037, iter [02400, 05004], lr: 0.078879, loss: 2.0729
2022-07-10 09:14:19 - train: epoch 0037, iter [02500, 05004], lr: 0.078858, loss: 2.0651
2022-07-10 09:14:54 - train: epoch 0037, iter [02600, 05004], lr: 0.078836, loss: 2.0695
2022-07-10 09:15:29 - train: epoch 0037, iter [02700, 05004], lr: 0.078815, loss: 2.2659
2022-07-10 09:16:03 - train: epoch 0037, iter [02800, 05004], lr: 0.078794, loss: 2.1834
2022-07-10 09:16:38 - train: epoch 0037, iter [02900, 05004], lr: 0.078772, loss: 1.9252
2022-07-10 09:17:12 - train: epoch 0037, iter [03000, 05004], lr: 0.078751, loss: 2.1757
2022-07-10 09:17:46 - train: epoch 0037, iter [03100, 05004], lr: 0.078729, loss: 2.0383
2022-07-10 09:18:21 - train: epoch 0037, iter [03200, 05004], lr: 0.078708, loss: 1.8015
2022-07-10 09:18:56 - train: epoch 0037, iter [03300, 05004], lr: 0.078687, loss: 1.9252
2022-07-10 09:19:30 - train: epoch 0037, iter [03400, 05004], lr: 0.078665, loss: 1.8533
2022-07-10 09:20:05 - train: epoch 0037, iter [03500, 05004], lr: 0.078644, loss: 2.0478
2022-07-10 09:20:39 - train: epoch 0037, iter [03600, 05004], lr: 0.078622, loss: 2.1115
2022-07-10 09:21:14 - train: epoch 0037, iter [03700, 05004], lr: 0.078601, loss: 1.9902
2022-07-10 09:21:48 - train: epoch 0037, iter [03800, 05004], lr: 0.078579, loss: 2.0767
2022-07-10 09:22:24 - train: epoch 0037, iter [03900, 05004], lr: 0.078558, loss: 2.1482
2022-07-10 09:22:58 - train: epoch 0037, iter [04000, 05004], lr: 0.078536, loss: 2.0248
2022-07-10 09:23:32 - train: epoch 0037, iter [04100, 05004], lr: 0.078515, loss: 1.9596
2022-07-10 09:24:06 - train: epoch 0037, iter [04200, 05004], lr: 0.078493, loss: 2.0295
2022-07-10 09:24:41 - train: epoch 0037, iter [04300, 05004], lr: 0.078472, loss: 1.9900
2022-07-10 09:25:15 - train: epoch 0037, iter [04400, 05004], lr: 0.078450, loss: 2.1323
2022-07-10 09:25:50 - train: epoch 0037, iter [04500, 05004], lr: 0.078429, loss: 1.8613
2022-07-10 09:26:25 - train: epoch 0037, iter [04600, 05004], lr: 0.078407, loss: 2.1916
2022-07-10 09:27:00 - train: epoch 0037, iter [04700, 05004], lr: 0.078386, loss: 2.3569
2022-07-10 09:27:34 - train: epoch 0037, iter [04800, 05004], lr: 0.078364, loss: 1.9808
2022-07-10 09:28:09 - train: epoch 0037, iter [04900, 05004], lr: 0.078343, loss: 2.0675
2022-07-10 09:28:42 - train: epoch 0037, iter [05000, 05004], lr: 0.078321, loss: 2.0084
2022-07-10 09:28:43 - train: epoch 037, train_loss: 2.0545
2022-07-10 09:29:58 - eval: epoch: 037, acc1: 57.876%, acc5: 81.884%, test_loss: 1.7853, per_image_load_time: 2.375ms, per_image_inference_time: 0.383ms
2022-07-10 09:29:58 - until epoch: 037, best_acc1: 58.320%
2022-07-10 09:29:58 - epoch 038 lr: 0.078320
2022-07-10 09:30:38 - train: epoch 0038, iter [00100, 05004], lr: 0.078299, loss: 1.8091
2022-07-10 09:31:12 - train: epoch 0038, iter [00200, 05004], lr: 0.078277, loss: 1.9295
2022-07-10 09:31:45 - train: epoch 0038, iter [00300, 05004], lr: 0.078256, loss: 1.9063
2022-07-10 09:32:20 - train: epoch 0038, iter [00400, 05004], lr: 0.078234, loss: 1.9747
2022-07-10 09:32:54 - train: epoch 0038, iter [00500, 05004], lr: 0.078212, loss: 2.0004
2022-07-10 09:33:28 - train: epoch 0038, iter [00600, 05004], lr: 0.078191, loss: 1.9996
2022-07-10 09:34:02 - train: epoch 0038, iter [00700, 05004], lr: 0.078169, loss: 2.0699
2022-07-10 09:34:37 - train: epoch 0038, iter [00800, 05004], lr: 0.078148, loss: 1.8992
2022-07-10 09:35:11 - train: epoch 0038, iter [00900, 05004], lr: 0.078126, loss: 1.9296
2022-07-10 09:35:45 - train: epoch 0038, iter [01000, 05004], lr: 0.078104, loss: 2.0332
2022-07-10 09:36:20 - train: epoch 0038, iter [01100, 05004], lr: 0.078083, loss: 2.2070
2022-07-10 09:36:55 - train: epoch 0038, iter [01200, 05004], lr: 0.078061, loss: 2.2246
2022-07-10 09:37:29 - train: epoch 0038, iter [01300, 05004], lr: 0.078039, loss: 1.9187
2022-07-10 09:38:03 - train: epoch 0038, iter [01400, 05004], lr: 0.078018, loss: 2.1104
2022-07-10 09:38:37 - train: epoch 0038, iter [01500, 05004], lr: 0.077996, loss: 1.9675
2022-07-10 09:39:12 - train: epoch 0038, iter [01600, 05004], lr: 0.077974, loss: 2.1794
2022-07-10 09:39:47 - train: epoch 0038, iter [01700, 05004], lr: 0.077953, loss: 2.1916
2022-07-10 09:40:22 - train: epoch 0038, iter [01800, 05004], lr: 0.077931, loss: 2.0425
2022-07-10 09:40:57 - train: epoch 0038, iter [01900, 05004], lr: 0.077909, loss: 1.9101
2022-07-10 09:41:31 - train: epoch 0038, iter [02000, 05004], lr: 0.077888, loss: 2.1399
2022-07-10 09:42:06 - train: epoch 0038, iter [02100, 05004], lr: 0.077866, loss: 2.1054
2022-07-10 09:42:41 - train: epoch 0038, iter [02200, 05004], lr: 0.077844, loss: 1.9282
2022-07-10 09:43:15 - train: epoch 0038, iter [02300, 05004], lr: 0.077822, loss: 2.0600
2022-07-10 09:43:50 - train: epoch 0038, iter [02400, 05004], lr: 0.077801, loss: 2.3147
2022-07-10 09:44:25 - train: epoch 0038, iter [02500, 05004], lr: 0.077779, loss: 2.0140
2022-07-10 09:45:00 - train: epoch 0038, iter [02600, 05004], lr: 0.077757, loss: 2.0096
2022-07-10 09:45:35 - train: epoch 0038, iter [02700, 05004], lr: 0.077735, loss: 2.1693
2022-07-10 09:46:10 - train: epoch 0038, iter [02800, 05004], lr: 0.077714, loss: 2.3275
2022-07-10 09:46:45 - train: epoch 0038, iter [02900, 05004], lr: 0.077692, loss: 2.0953
2022-07-10 09:47:19 - train: epoch 0038, iter [03000, 05004], lr: 0.077670, loss: 2.0114
2022-07-10 09:47:54 - train: epoch 0038, iter [03100, 05004], lr: 0.077648, loss: 1.9479
2022-07-10 09:48:28 - train: epoch 0038, iter [03200, 05004], lr: 0.077627, loss: 1.8315
2022-07-10 09:49:03 - train: epoch 0038, iter [03300, 05004], lr: 0.077605, loss: 1.8326
2022-07-10 09:49:38 - train: epoch 0038, iter [03400, 05004], lr: 0.077583, loss: 2.0232
2022-07-10 09:50:12 - train: epoch 0038, iter [03500, 05004], lr: 0.077561, loss: 2.0377
2022-07-10 09:50:46 - train: epoch 0038, iter [03600, 05004], lr: 0.077539, loss: 2.0358
2022-07-10 09:51:20 - train: epoch 0038, iter [03700, 05004], lr: 0.077517, loss: 1.8253
2022-07-10 09:51:54 - train: epoch 0038, iter [03800, 05004], lr: 0.077496, loss: 2.5545
2022-07-10 09:52:29 - train: epoch 0038, iter [03900, 05004], lr: 0.077474, loss: 1.8651
2022-07-10 09:53:03 - train: epoch 0038, iter [04000, 05004], lr: 0.077452, loss: 2.0645
2022-07-10 09:53:37 - train: epoch 0038, iter [04100, 05004], lr: 0.077430, loss: 1.9888
2022-07-10 09:54:11 - train: epoch 0038, iter [04200, 05004], lr: 0.077408, loss: 1.8837
2022-07-10 09:54:46 - train: epoch 0038, iter [04300, 05004], lr: 0.077386, loss: 2.2199
2022-07-10 09:55:20 - train: epoch 0038, iter [04400, 05004], lr: 0.077364, loss: 2.0926
2022-07-10 09:55:54 - train: epoch 0038, iter [04500, 05004], lr: 0.077342, loss: 2.0707
2022-07-10 09:56:28 - train: epoch 0038, iter [04600, 05004], lr: 0.077321, loss: 2.1606
2022-07-10 09:57:03 - train: epoch 0038, iter [04700, 05004], lr: 0.077299, loss: 1.9681
2022-07-10 09:57:37 - train: epoch 0038, iter [04800, 05004], lr: 0.077277, loss: 2.0616
2022-07-10 09:58:11 - train: epoch 0038, iter [04900, 05004], lr: 0.077255, loss: 1.9626
2022-07-10 09:58:43 - train: epoch 0038, iter [05000, 05004], lr: 0.077233, loss: 1.8385
2022-07-10 09:58:44 - train: epoch 038, train_loss: 2.0423
2022-07-10 10:00:00 - eval: epoch: 038, acc1: 57.854%, acc5: 82.058%, test_loss: 1.7677, per_image_load_time: 0.931ms, per_image_inference_time: 0.326ms
2022-07-10 10:00:00 - until epoch: 038, best_acc1: 58.320%
2022-07-10 10:00:00 - epoch 039 lr: 0.077232
2022-07-10 10:00:39 - train: epoch 0039, iter [00100, 05004], lr: 0.077210, loss: 2.1657
2022-07-10 10:01:14 - train: epoch 0039, iter [00200, 05004], lr: 0.077188, loss: 2.1871
2022-07-10 10:01:48 - train: epoch 0039, iter [00300, 05004], lr: 0.077166, loss: 1.8112
2022-07-10 10:02:22 - train: epoch 0039, iter [00400, 05004], lr: 0.077144, loss: 2.1037
2022-07-10 10:02:56 - train: epoch 0039, iter [00500, 05004], lr: 0.077122, loss: 1.9091
2022-07-10 10:03:31 - train: epoch 0039, iter [00600, 05004], lr: 0.077100, loss: 1.9331
2022-07-10 10:04:06 - train: epoch 0039, iter [00700, 05004], lr: 0.077078, loss: 2.2970
2022-07-10 10:04:39 - train: epoch 0039, iter [00800, 05004], lr: 0.077056, loss: 2.0321
2022-07-10 10:05:15 - train: epoch 0039, iter [00900, 05004], lr: 0.077034, loss: 2.1444
2022-07-10 10:05:49 - train: epoch 0039, iter [01000, 05004], lr: 0.077012, loss: 1.9467
2022-07-10 10:06:24 - train: epoch 0039, iter [01100, 05004], lr: 0.076990, loss: 2.1961
2022-07-10 10:06:58 - train: epoch 0039, iter [01200, 05004], lr: 0.076968, loss: 2.0499
2022-07-10 10:07:32 - train: epoch 0039, iter [01300, 05004], lr: 0.076946, loss: 2.0513
2022-07-10 10:08:07 - train: epoch 0039, iter [01400, 05004], lr: 0.076924, loss: 2.1070
2022-07-10 10:08:42 - train: epoch 0039, iter [01500, 05004], lr: 0.076902, loss: 1.8405
2022-07-10 10:09:17 - train: epoch 0039, iter [01600, 05004], lr: 0.076880, loss: 1.9474
2022-07-10 10:09:51 - train: epoch 0039, iter [01700, 05004], lr: 0.076858, loss: 2.0244
2022-07-10 10:10:26 - train: epoch 0039, iter [01800, 05004], lr: 0.076836, loss: 1.9372
2022-07-10 10:10:59 - train: epoch 0039, iter [01900, 05004], lr: 0.076814, loss: 1.8742
2022-07-10 10:11:34 - train: epoch 0039, iter [02000, 05004], lr: 0.076792, loss: 2.1762
2022-07-10 10:12:08 - train: epoch 0039, iter [02100, 05004], lr: 0.076770, loss: 2.2792
2022-07-10 10:12:42 - train: epoch 0039, iter [02200, 05004], lr: 0.076748, loss: 2.0986
2022-07-10 10:13:16 - train: epoch 0039, iter [02300, 05004], lr: 0.076725, loss: 2.1830
2022-07-10 10:13:51 - train: epoch 0039, iter [02400, 05004], lr: 0.076703, loss: 2.0469
2022-07-10 10:14:25 - train: epoch 0039, iter [02500, 05004], lr: 0.076681, loss: 2.1624
2022-07-10 10:14:59 - train: epoch 0039, iter [02600, 05004], lr: 0.076659, loss: 1.8501
2022-07-10 10:15:33 - train: epoch 0039, iter [02700, 05004], lr: 0.076637, loss: 1.8938
2022-07-10 10:16:08 - train: epoch 0039, iter [02800, 05004], lr: 0.076615, loss: 2.1173
2022-07-10 10:16:41 - train: epoch 0039, iter [02900, 05004], lr: 0.076593, loss: 1.8330
2022-07-10 10:17:16 - train: epoch 0039, iter [03000, 05004], lr: 0.076570, loss: 2.2056
2022-07-10 10:17:50 - train: epoch 0039, iter [03100, 05004], lr: 0.076548, loss: 2.0507
2022-07-10 10:18:25 - train: epoch 0039, iter [03200, 05004], lr: 0.076526, loss: 2.2435
2022-07-10 10:18:59 - train: epoch 0039, iter [03300, 05004], lr: 0.076504, loss: 2.1130
2022-07-10 10:19:33 - train: epoch 0039, iter [03400, 05004], lr: 0.076482, loss: 2.0934
2022-07-10 10:20:07 - train: epoch 0039, iter [03500, 05004], lr: 0.076460, loss: 2.3664
2022-07-10 10:20:42 - train: epoch 0039, iter [03600, 05004], lr: 0.076437, loss: 2.1025
2022-07-10 10:21:16 - train: epoch 0039, iter [03700, 05004], lr: 0.076415, loss: 1.9105
2022-07-10 10:21:50 - train: epoch 0039, iter [03800, 05004], lr: 0.076393, loss: 2.0116
2022-07-10 10:22:24 - train: epoch 0039, iter [03900, 05004], lr: 0.076371, loss: 2.0398
2022-07-10 10:22:58 - train: epoch 0039, iter [04000, 05004], lr: 0.076349, loss: 1.9972
2022-07-10 10:23:33 - train: epoch 0039, iter [04100, 05004], lr: 0.076326, loss: 2.1683
2022-07-10 10:24:07 - train: epoch 0039, iter [04200, 05004], lr: 0.076304, loss: 2.0458
2022-07-10 10:24:41 - train: epoch 0039, iter [04300, 05004], lr: 0.076282, loss: 2.1323
2022-07-10 10:25:16 - train: epoch 0039, iter [04400, 05004], lr: 0.076260, loss: 1.9330
2022-07-10 10:25:50 - train: epoch 0039, iter [04500, 05004], lr: 0.076237, loss: 1.9630
2022-07-10 10:26:25 - train: epoch 0039, iter [04600, 05004], lr: 0.076215, loss: 2.3368
2022-07-10 10:26:59 - train: epoch 0039, iter [04700, 05004], lr: 0.076193, loss: 2.0376
2022-07-10 10:27:34 - train: epoch 0039, iter [04800, 05004], lr: 0.076170, loss: 1.8840
2022-07-10 10:28:08 - train: epoch 0039, iter [04900, 05004], lr: 0.076148, loss: 1.9577
2022-07-10 10:28:41 - train: epoch 0039, iter [05000, 05004], lr: 0.076126, loss: 2.0482
2022-07-10 10:28:42 - train: epoch 039, train_loss: 2.0326
2022-07-10 10:29:58 - eval: epoch: 039, acc1: 58.278%, acc5: 82.400%, test_loss: 1.7540, per_image_load_time: 1.611ms, per_image_inference_time: 0.348ms
2022-07-10 10:29:58 - until epoch: 039, best_acc1: 58.320%
2022-07-10 10:29:58 - epoch 040 lr: 0.076125
2022-07-10 10:30:37 - train: epoch 0040, iter [00100, 05004], lr: 0.076103, loss: 2.2117
2022-07-10 10:31:10 - train: epoch 0040, iter [00200, 05004], lr: 0.076080, loss: 2.0831
2022-07-10 10:31:45 - train: epoch 0040, iter [00300, 05004], lr: 0.076058, loss: 1.9916
2022-07-10 10:32:18 - train: epoch 0040, iter [00400, 05004], lr: 0.076036, loss: 1.6762
2022-07-10 10:32:53 - train: epoch 0040, iter [00500, 05004], lr: 0.076013, loss: 1.9544
2022-07-10 10:33:27 - train: epoch 0040, iter [00600, 05004], lr: 0.075991, loss: 2.0178
2022-07-10 10:34:01 - train: epoch 0040, iter [00700, 05004], lr: 0.075969, loss: 2.2054
2022-07-10 10:34:34 - train: epoch 0040, iter [00800, 05004], lr: 0.075946, loss: 2.0984
2022-07-10 10:35:09 - train: epoch 0040, iter [00900, 05004], lr: 0.075924, loss: 1.9811
2022-07-10 10:35:42 - train: epoch 0040, iter [01000, 05004], lr: 0.075902, loss: 1.8642
2022-07-10 10:36:16 - train: epoch 0040, iter [01100, 05004], lr: 0.075879, loss: 1.9735
2022-07-10 10:36:50 - train: epoch 0040, iter [01200, 05004], lr: 0.075857, loss: 1.9986
2022-07-10 10:37:24 - train: epoch 0040, iter [01300, 05004], lr: 0.075834, loss: 2.0487
2022-07-10 10:37:58 - train: epoch 0040, iter [01400, 05004], lr: 0.075812, loss: 2.1359
2022-07-10 10:38:32 - train: epoch 0040, iter [01500, 05004], lr: 0.075790, loss: 2.1031
2022-07-10 10:39:06 - train: epoch 0040, iter [01600, 05004], lr: 0.075767, loss: 1.9950
2022-07-10 10:39:40 - train: epoch 0040, iter [01700, 05004], lr: 0.075745, loss: 1.9470
2022-07-10 10:40:14 - train: epoch 0040, iter [01800, 05004], lr: 0.075722, loss: 1.8825
2022-07-10 10:40:48 - train: epoch 0040, iter [01900, 05004], lr: 0.075700, loss: 1.8828
2022-07-10 10:41:23 - train: epoch 0040, iter [02000, 05004], lr: 0.075677, loss: 2.0389
2022-07-10 10:41:57 - train: epoch 0040, iter [02100, 05004], lr: 0.075655, loss: 1.7410
2022-07-10 10:42:31 - train: epoch 0040, iter [02200, 05004], lr: 0.075633, loss: 1.9579
2022-07-10 10:43:05 - train: epoch 0040, iter [02300, 05004], lr: 0.075610, loss: 2.0944
2022-07-10 10:43:40 - train: epoch 0040, iter [02400, 05004], lr: 0.075588, loss: 2.2085
2022-07-10 10:44:15 - train: epoch 0040, iter [02500, 05004], lr: 0.075565, loss: 2.0732
2022-07-10 10:44:49 - train: epoch 0040, iter [02600, 05004], lr: 0.075543, loss: 1.9973
2022-07-10 10:45:23 - train: epoch 0040, iter [02700, 05004], lr: 0.075520, loss: 2.0172
2022-07-10 10:45:57 - train: epoch 0040, iter [02800, 05004], lr: 0.075498, loss: 2.1453
2022-07-10 10:46:32 - train: epoch 0040, iter [02900, 05004], lr: 0.075475, loss: 2.2250
2022-07-10 10:47:06 - train: epoch 0040, iter [03000, 05004], lr: 0.075453, loss: 2.0746
2022-07-10 10:47:41 - train: epoch 0040, iter [03100, 05004], lr: 0.075430, loss: 1.9900
2022-07-10 10:48:15 - train: epoch 0040, iter [03200, 05004], lr: 0.075408, loss: 2.3087
2022-07-10 10:48:50 - train: epoch 0040, iter [03300, 05004], lr: 0.075385, loss: 2.0639
2022-07-10 10:49:24 - train: epoch 0040, iter [03400, 05004], lr: 0.075362, loss: 1.8769
2022-07-10 10:49:59 - train: epoch 0040, iter [03500, 05004], lr: 0.075340, loss: 2.1727
2022-07-10 10:50:33 - train: epoch 0040, iter [03600, 05004], lr: 0.075317, loss: 1.9389
2022-07-10 10:51:08 - train: epoch 0040, iter [03700, 05004], lr: 0.075295, loss: 2.1057
2022-07-10 10:51:43 - train: epoch 0040, iter [03800, 05004], lr: 0.075272, loss: 1.8735
2022-07-10 10:52:17 - train: epoch 0040, iter [03900, 05004], lr: 0.075250, loss: 2.0029
2022-07-10 10:52:51 - train: epoch 0040, iter [04000, 05004], lr: 0.075227, loss: 2.1726
2022-07-10 10:53:26 - train: epoch 0040, iter [04100, 05004], lr: 0.075205, loss: 2.0392
2022-07-10 10:54:00 - train: epoch 0040, iter [04200, 05004], lr: 0.075182, loss: 2.0197
2022-07-10 10:54:35 - train: epoch 0040, iter [04300, 05004], lr: 0.075159, loss: 2.1914
2022-07-10 10:55:09 - train: epoch 0040, iter [04400, 05004], lr: 0.075137, loss: 1.8856
2022-07-10 10:55:44 - train: epoch 0040, iter [04500, 05004], lr: 0.075114, loss: 1.9820
2022-07-10 10:56:18 - train: epoch 0040, iter [04600, 05004], lr: 0.075091, loss: 1.9246
2022-07-10 10:56:54 - train: epoch 0040, iter [04700, 05004], lr: 0.075069, loss: 2.2962
2022-07-10 10:57:27 - train: epoch 0040, iter [04800, 05004], lr: 0.075046, loss: 2.0504
2022-07-10 10:58:02 - train: epoch 0040, iter [04900, 05004], lr: 0.075024, loss: 2.1121
2022-07-10 10:58:35 - train: epoch 0040, iter [05000, 05004], lr: 0.075001, loss: 1.9409
2022-07-10 10:58:36 - train: epoch 040, train_loss: 2.0288
2022-07-10 10:59:52 - eval: epoch: 040, acc1: 58.344%, acc5: 82.334%, test_loss: 1.7738, per_image_load_time: 2.586ms, per_image_inference_time: 0.349ms
2022-07-10 10:59:52 - until epoch: 040, best_acc1: 58.344%
2022-07-10 10:59:52 - epoch 041 lr: 0.075000
2022-07-10 11:00:32 - train: epoch 0041, iter [00100, 05004], lr: 0.074977, loss: 2.2559
2022-07-10 11:01:06 - train: epoch 0041, iter [00200, 05004], lr: 0.074955, loss: 2.2733
2022-07-10 11:01:40 - train: epoch 0041, iter [00300, 05004], lr: 0.074932, loss: 2.0997
2022-07-10 11:02:15 - train: epoch 0041, iter [00400, 05004], lr: 0.074909, loss: 2.0074
2022-07-10 11:02:49 - train: epoch 0041, iter [00500, 05004], lr: 0.074887, loss: 1.7672
2022-07-10 11:03:24 - train: epoch 0041, iter [00600, 05004], lr: 0.074864, loss: 1.9965
2022-07-10 11:03:59 - train: epoch 0041, iter [00700, 05004], lr: 0.074841, loss: 1.9197
2022-07-10 11:04:33 - train: epoch 0041, iter [00800, 05004], lr: 0.074819, loss: 1.8140
2022-07-10 11:05:08 - train: epoch 0041, iter [00900, 05004], lr: 0.074796, loss: 1.7968
2022-07-10 11:05:42 - train: epoch 0041, iter [01000, 05004], lr: 0.074773, loss: 2.0247
2022-07-10 11:06:17 - train: epoch 0041, iter [01100, 05004], lr: 0.074750, loss: 2.0925
2022-07-10 11:06:51 - train: epoch 0041, iter [01200, 05004], lr: 0.074728, loss: 1.8621
2022-07-10 11:07:25 - train: epoch 0041, iter [01300, 05004], lr: 0.074705, loss: 1.6516
2022-07-10 11:08:00 - train: epoch 0041, iter [01400, 05004], lr: 0.074682, loss: 2.0840
2022-07-10 11:08:35 - train: epoch 0041, iter [01500, 05004], lr: 0.074659, loss: 2.1311
2022-07-10 11:09:10 - train: epoch 0041, iter [01600, 05004], lr: 0.074637, loss: 1.9696
2022-07-10 11:09:44 - train: epoch 0041, iter [01700, 05004], lr: 0.074614, loss: 1.9592
2022-07-10 11:10:19 - train: epoch 0041, iter [01800, 05004], lr: 0.074591, loss: 1.9707
2022-07-10 11:10:54 - train: epoch 0041, iter [01900, 05004], lr: 0.074568, loss: 2.0795
2022-07-10 11:11:28 - train: epoch 0041, iter [02000, 05004], lr: 0.074546, loss: 1.8353
2022-07-10 11:12:03 - train: epoch 0041, iter [02100, 05004], lr: 0.074523, loss: 1.8122
2022-07-10 11:12:37 - train: epoch 0041, iter [02200, 05004], lr: 0.074500, loss: 1.7390
2022-07-10 11:13:12 - train: epoch 0041, iter [02300, 05004], lr: 0.074477, loss: 1.8039
2022-07-10 11:13:47 - train: epoch 0041, iter [02400, 05004], lr: 0.074454, loss: 2.0248
2022-07-10 11:14:22 - train: epoch 0041, iter [02500, 05004], lr: 0.074432, loss: 1.9807
2022-07-10 11:14:56 - train: epoch 0041, iter [02600, 05004], lr: 0.074409, loss: 2.2635
2022-07-10 11:15:30 - train: epoch 0041, iter [02700, 05004], lr: 0.074386, loss: 2.3777
2022-07-10 11:16:05 - train: epoch 0041, iter [02800, 05004], lr: 0.074363, loss: 1.9223
2022-07-10 11:16:41 - train: epoch 0041, iter [02900, 05004], lr: 0.074340, loss: 1.9054
2022-07-10 11:17:14 - train: epoch 0041, iter [03000, 05004], lr: 0.074317, loss: 2.0455
2022-07-10 11:17:49 - train: epoch 0041, iter [03100, 05004], lr: 0.074294, loss: 2.2761
2022-07-10 11:18:23 - train: epoch 0041, iter [03200, 05004], lr: 0.074272, loss: 2.0147
2022-07-10 11:18:58 - train: epoch 0041, iter [03300, 05004], lr: 0.074249, loss: 2.0654
2022-07-10 11:19:33 - train: epoch 0041, iter [03400, 05004], lr: 0.074226, loss: 1.8631
2022-07-10 11:20:08 - train: epoch 0041, iter [03500, 05004], lr: 0.074203, loss: 1.8867
2022-07-10 11:20:42 - train: epoch 0041, iter [03600, 05004], lr: 0.074180, loss: 2.0533
2022-07-10 11:21:15 - train: epoch 0041, iter [03700, 05004], lr: 0.074157, loss: 2.0640
2022-07-10 11:21:49 - train: epoch 0041, iter [03800, 05004], lr: 0.074134, loss: 2.0840
2022-07-10 11:22:25 - train: epoch 0041, iter [03900, 05004], lr: 0.074111, loss: 1.9928
2022-07-10 11:22:59 - train: epoch 0041, iter [04000, 05004], lr: 0.074088, loss: 1.9597
2022-07-10 11:23:33 - train: epoch 0041, iter [04100, 05004], lr: 0.074065, loss: 2.0575
2022-07-10 11:24:07 - train: epoch 0041, iter [04200, 05004], lr: 0.074043, loss: 1.9385
2022-07-10 11:24:42 - train: epoch 0041, iter [04300, 05004], lr: 0.074020, loss: 1.8596
2022-07-10 11:25:16 - train: epoch 0041, iter [04400, 05004], lr: 0.073997, loss: 1.8420
2022-07-10 11:25:50 - train: epoch 0041, iter [04500, 05004], lr: 0.073974, loss: 1.9581
2022-07-10 11:26:25 - train: epoch 0041, iter [04600, 05004], lr: 0.073951, loss: 2.2174
2022-07-10 11:27:00 - train: epoch 0041, iter [04700, 05004], lr: 0.073928, loss: 2.0906
2022-07-10 11:27:34 - train: epoch 0041, iter [04800, 05004], lr: 0.073905, loss: 2.0606
2022-07-10 11:28:09 - train: epoch 0041, iter [04900, 05004], lr: 0.073882, loss: 2.0704
2022-07-10 11:28:42 - train: epoch 0041, iter [05000, 05004], lr: 0.073859, loss: 2.1252
2022-07-10 11:28:43 - train: epoch 041, train_loss: 2.0190
2022-07-10 11:29:58 - eval: epoch: 041, acc1: 58.970%, acc5: 82.558%, test_loss: 1.7302, per_image_load_time: 2.339ms, per_image_inference_time: 0.357ms
2022-07-10 11:29:58 - until epoch: 041, best_acc1: 58.970%
2022-07-10 11:29:58 - epoch 042 lr: 0.073858
2022-07-10 11:30:38 - train: epoch 0042, iter [00100, 05004], lr: 0.073835, loss: 1.6120
2022-07-10 11:31:12 - train: epoch 0042, iter [00200, 05004], lr: 0.073812, loss: 2.0284
2022-07-10 11:31:46 - train: epoch 0042, iter [00300, 05004], lr: 0.073789, loss: 2.0449
2022-07-10 11:32:19 - train: epoch 0042, iter [00400, 05004], lr: 0.073766, loss: 1.8100
2022-07-10 11:32:53 - train: epoch 0042, iter [00500, 05004], lr: 0.073743, loss: 1.9908
2022-07-10 11:33:27 - train: epoch 0042, iter [00600, 05004], lr: 0.073720, loss: 1.9851
2022-07-10 11:34:02 - train: epoch 0042, iter [00700, 05004], lr: 0.073697, loss: 2.1429
2022-07-10 11:34:36 - train: epoch 0042, iter [00800, 05004], lr: 0.073674, loss: 1.9661
2022-07-10 11:35:10 - train: epoch 0042, iter [00900, 05004], lr: 0.073651, loss: 2.0435
2022-07-10 11:35:45 - train: epoch 0042, iter [01000, 05004], lr: 0.073628, loss: 2.2710
2022-07-10 11:36:18 - train: epoch 0042, iter [01100, 05004], lr: 0.073605, loss: 1.8048
2022-07-10 11:36:53 - train: epoch 0042, iter [01200, 05004], lr: 0.073582, loss: 1.8180
2022-07-10 11:37:28 - train: epoch 0042, iter [01300, 05004], lr: 0.073559, loss: 2.2454
2022-07-10 11:38:02 - train: epoch 0042, iter [01400, 05004], lr: 0.073535, loss: 2.1034
2022-07-10 11:38:37 - train: epoch 0042, iter [01500, 05004], lr: 0.073512, loss: 1.9343
2022-07-10 11:39:12 - train: epoch 0042, iter [01600, 05004], lr: 0.073489, loss: 2.0376
2022-07-10 11:39:46 - train: epoch 0042, iter [01700, 05004], lr: 0.073466, loss: 1.8185
2022-07-10 11:40:20 - train: epoch 0042, iter [01800, 05004], lr: 0.073443, loss: 2.0223
2022-07-10 11:40:55 - train: epoch 0042, iter [01900, 05004], lr: 0.073420, loss: 2.0691
2022-07-10 11:41:30 - train: epoch 0042, iter [02000, 05004], lr: 0.073397, loss: 1.8435
2022-07-10 11:42:04 - train: epoch 0042, iter [02100, 05004], lr: 0.073374, loss: 1.8513
2022-07-10 11:42:39 - train: epoch 0042, iter [02200, 05004], lr: 0.073351, loss: 1.9035
2022-07-10 11:43:13 - train: epoch 0042, iter [02300, 05004], lr: 0.073327, loss: 2.1079
2022-07-10 11:43:48 - train: epoch 0042, iter [02400, 05004], lr: 0.073304, loss: 1.9620
2022-07-10 11:44:23 - train: epoch 0042, iter [02500, 05004], lr: 0.073281, loss: 1.9804
2022-07-10 11:44:56 - train: epoch 0042, iter [02600, 05004], lr: 0.073258, loss: 2.2872
2022-07-10 11:45:31 - train: epoch 0042, iter [02700, 05004], lr: 0.073235, loss: 1.8237
2022-07-10 11:46:05 - train: epoch 0042, iter [02800, 05004], lr: 0.073212, loss: 1.8034
2022-07-10 11:46:39 - train: epoch 0042, iter [02900, 05004], lr: 0.073189, loss: 1.7963
2022-07-10 11:47:14 - train: epoch 0042, iter [03000, 05004], lr: 0.073165, loss: 1.7314
2022-07-10 11:47:48 - train: epoch 0042, iter [03100, 05004], lr: 0.073142, loss: 2.0236
2022-07-10 11:48:22 - train: epoch 0042, iter [03200, 05004], lr: 0.073119, loss: 2.2109
2022-07-10 11:48:58 - train: epoch 0042, iter [03300, 05004], lr: 0.073096, loss: 2.0901
2022-07-10 11:49:32 - train: epoch 0042, iter [03400, 05004], lr: 0.073073, loss: 1.8516
2022-07-10 11:50:06 - train: epoch 0042, iter [03500, 05004], lr: 0.073049, loss: 1.9481
2022-07-10 11:50:41 - train: epoch 0042, iter [03600, 05004], lr: 0.073026, loss: 2.4020
2022-07-10 11:51:15 - train: epoch 0042, iter [03700, 05004], lr: 0.073003, loss: 2.0425
2022-07-10 11:51:50 - train: epoch 0042, iter [03800, 05004], lr: 0.072980, loss: 1.6702
2022-07-10 11:52:25 - train: epoch 0042, iter [03900, 05004], lr: 0.072956, loss: 2.1421
2022-07-10 11:52:59 - train: epoch 0042, iter [04000, 05004], lr: 0.072933, loss: 1.8770
2022-07-10 11:53:34 - train: epoch 0042, iter [04100, 05004], lr: 0.072910, loss: 2.2465
2022-07-10 11:54:08 - train: epoch 0042, iter [04200, 05004], lr: 0.072887, loss: 2.0262
2022-07-10 11:54:43 - train: epoch 0042, iter [04300, 05004], lr: 0.072863, loss: 1.8954
2022-07-10 11:55:18 - train: epoch 0042, iter [04400, 05004], lr: 0.072840, loss: 1.8939
2022-07-10 11:55:52 - train: epoch 0042, iter [04500, 05004], lr: 0.072817, loss: 2.0526
2022-07-10 11:56:26 - train: epoch 0042, iter [04600, 05004], lr: 0.072794, loss: 2.3131
2022-07-10 11:57:01 - train: epoch 0042, iter [04700, 05004], lr: 0.072770, loss: 1.7657
2022-07-10 11:57:36 - train: epoch 0042, iter [04800, 05004], lr: 0.072747, loss: 2.1255
2022-07-10 11:58:10 - train: epoch 0042, iter [04900, 05004], lr: 0.072724, loss: 2.1247
2022-07-10 11:58:43 - train: epoch 0042, iter [05000, 05004], lr: 0.072700, loss: 2.0073
2022-07-10 11:58:44 - train: epoch 042, train_loss: 2.0077
2022-07-10 12:00:00 - eval: epoch: 042, acc1: 57.814%, acc5: 81.822%, test_loss: 1.7856, per_image_load_time: 2.574ms, per_image_inference_time: 0.374ms
2022-07-10 12:00:01 - until epoch: 042, best_acc1: 58.970%
2022-07-10 12:00:01 - epoch 043 lr: 0.072699
2022-07-10 12:00:40 - train: epoch 0043, iter [00100, 05004], lr: 0.072676, loss: 2.0415
2022-07-10 12:01:14 - train: epoch 0043, iter [00200, 05004], lr: 0.072653, loss: 1.8714
2022-07-10 12:01:48 - train: epoch 0043, iter [00300, 05004], lr: 0.072630, loss: 1.8321
2022-07-10 12:02:23 - train: epoch 0043, iter [00400, 05004], lr: 0.072606, loss: 1.8717
2022-07-10 12:02:58 - train: epoch 0043, iter [00500, 05004], lr: 0.072583, loss: 1.9227
2022-07-10 12:03:32 - train: epoch 0043, iter [00600, 05004], lr: 0.072560, loss: 1.8688
2022-07-10 12:04:06 - train: epoch 0043, iter [00700, 05004], lr: 0.072536, loss: 1.8786
2022-07-10 12:04:40 - train: epoch 0043, iter [00800, 05004], lr: 0.072513, loss: 2.0585
2022-07-10 12:05:14 - train: epoch 0043, iter [00900, 05004], lr: 0.072490, loss: 1.9284
2022-07-10 12:05:49 - train: epoch 0043, iter [01000, 05004], lr: 0.072466, loss: 2.0071
2022-07-10 12:06:25 - train: epoch 0043, iter [01100, 05004], lr: 0.072443, loss: 2.1703
2022-07-10 12:06:59 - train: epoch 0043, iter [01200, 05004], lr: 0.072419, loss: 1.9869
2022-07-10 12:07:34 - train: epoch 0043, iter [01300, 05004], lr: 0.072396, loss: 2.1480
2022-07-10 12:08:07 - train: epoch 0043, iter [01400, 05004], lr: 0.072373, loss: 1.6740
2022-07-10 12:08:43 - train: epoch 0043, iter [01500, 05004], lr: 0.072349, loss: 1.9226
2022-07-10 12:09:17 - train: epoch 0043, iter [01600, 05004], lr: 0.072326, loss: 1.8640
2022-07-10 12:09:52 - train: epoch 0043, iter [01700, 05004], lr: 0.072302, loss: 1.9986
2022-07-10 12:10:26 - train: epoch 0043, iter [01800, 05004], lr: 0.072279, loss: 2.1600
2022-07-10 12:11:01 - train: epoch 0043, iter [01900, 05004], lr: 0.072256, loss: 1.8520
2022-07-10 12:11:35 - train: epoch 0043, iter [02000, 05004], lr: 0.072232, loss: 1.8482
2022-07-10 12:12:09 - train: epoch 0043, iter [02100, 05004], lr: 0.072209, loss: 1.9174
2022-07-10 12:12:43 - train: epoch 0043, iter [02200, 05004], lr: 0.072185, loss: 2.1363
2022-07-10 12:13:18 - train: epoch 0043, iter [02300, 05004], lr: 0.072162, loss: 2.3522
2022-07-10 12:13:52 - train: epoch 0043, iter [02400, 05004], lr: 0.072138, loss: 1.8279
2022-07-10 12:14:26 - train: epoch 0043, iter [02500, 05004], lr: 0.072115, loss: 2.1249
2022-07-10 12:15:01 - train: epoch 0043, iter [02600, 05004], lr: 0.072091, loss: 1.9148
2022-07-10 12:15:35 - train: epoch 0043, iter [02700, 05004], lr: 0.072068, loss: 2.0726
2022-07-10 12:16:09 - train: epoch 0043, iter [02800, 05004], lr: 0.072044, loss: 1.8678
2022-07-10 12:16:44 - train: epoch 0043, iter [02900, 05004], lr: 0.072021, loss: 2.0856
2022-07-10 12:17:19 - train: epoch 0043, iter [03000, 05004], lr: 0.071998, loss: 2.1573
2022-07-10 12:17:53 - train: epoch 0043, iter [03100, 05004], lr: 0.071974, loss: 2.3408
2022-07-10 12:18:27 - train: epoch 0043, iter [03200, 05004], lr: 0.071951, loss: 2.0709
2022-07-10 12:19:01 - train: epoch 0043, iter [03300, 05004], lr: 0.071927, loss: 2.0013
2022-07-10 12:19:35 - train: epoch 0043, iter [03400, 05004], lr: 0.071904, loss: 1.8715
2022-07-10 12:20:10 - train: epoch 0043, iter [03500, 05004], lr: 0.071880, loss: 1.9554
2022-07-10 12:20:44 - train: epoch 0043, iter [03600, 05004], lr: 0.071856, loss: 2.1090
2022-07-10 12:21:19 - train: epoch 0043, iter [03700, 05004], lr: 0.071833, loss: 1.7150
2022-07-10 12:21:54 - train: epoch 0043, iter [03800, 05004], lr: 0.071809, loss: 2.2023
2022-07-10 12:22:27 - train: epoch 0043, iter [03900, 05004], lr: 0.071786, loss: 2.1397
2022-07-10 12:23:02 - train: epoch 0043, iter [04000, 05004], lr: 0.071762, loss: 2.0969
2022-07-10 12:23:36 - train: epoch 0043, iter [04100, 05004], lr: 0.071739, loss: 2.2657
2022-07-10 12:24:11 - train: epoch 0043, iter [04200, 05004], lr: 0.071715, loss: 1.9794
2022-07-10 12:24:46 - train: epoch 0043, iter [04300, 05004], lr: 0.071692, loss: 1.8922
2022-07-10 12:25:20 - train: epoch 0043, iter [04400, 05004], lr: 0.071668, loss: 2.0304
2022-07-10 12:25:55 - train: epoch 0043, iter [04500, 05004], lr: 0.071644, loss: 2.1010
2022-07-10 12:26:29 - train: epoch 0043, iter [04600, 05004], lr: 0.071621, loss: 1.8697
2022-07-10 12:27:04 - train: epoch 0043, iter [04700, 05004], lr: 0.071597, loss: 2.1604
2022-07-10 12:27:38 - train: epoch 0043, iter [04800, 05004], lr: 0.071574, loss: 2.0729
2022-07-10 12:28:12 - train: epoch 0043, iter [04900, 05004], lr: 0.071550, loss: 1.9115
2022-07-10 12:28:45 - train: epoch 0043, iter [05000, 05004], lr: 0.071526, loss: 1.7888
2022-07-10 12:28:46 - train: epoch 043, train_loss: 1.9995
2022-07-10 12:30:02 - eval: epoch: 043, acc1: 58.238%, acc5: 82.412%, test_loss: 1.7553, per_image_load_time: 2.544ms, per_image_inference_time: 0.381ms
2022-07-10 12:30:03 - until epoch: 043, best_acc1: 58.970%
2022-07-10 12:30:03 - epoch 044 lr: 0.071525
2022-07-10 12:30:43 - train: epoch 0044, iter [00100, 05004], lr: 0.071502, loss: 2.1180
2022-07-10 12:31:16 - train: epoch 0044, iter [00200, 05004], lr: 0.071478, loss: 1.9345
2022-07-10 12:31:52 - train: epoch 0044, iter [00300, 05004], lr: 0.071455, loss: 1.8109
2022-07-10 12:32:25 - train: epoch 0044, iter [00400, 05004], lr: 0.071431, loss: 1.7684
2022-07-10 12:33:00 - train: epoch 0044, iter [00500, 05004], lr: 0.071407, loss: 1.8192
2022-07-10 12:33:34 - train: epoch 0044, iter [00600, 05004], lr: 0.071384, loss: 1.6742
2022-07-10 12:34:09 - train: epoch 0044, iter [00700, 05004], lr: 0.071360, loss: 1.9724
2022-07-10 12:34:43 - train: epoch 0044, iter [00800, 05004], lr: 0.071336, loss: 1.9279
2022-07-10 12:35:17 - train: epoch 0044, iter [00900, 05004], lr: 0.071313, loss: 2.0352
2022-07-10 12:35:51 - train: epoch 0044, iter [01000, 05004], lr: 0.071289, loss: 1.9293
2022-07-10 12:36:26 - train: epoch 0044, iter [01100, 05004], lr: 0.071265, loss: 1.9197
2022-07-10 12:37:01 - train: epoch 0044, iter [01200, 05004], lr: 0.071242, loss: 1.9476
2022-07-10 12:37:35 - train: epoch 0044, iter [01300, 05004], lr: 0.071218, loss: 1.9882
2022-07-10 12:38:10 - train: epoch 0044, iter [01400, 05004], lr: 0.071194, loss: 2.0876
2022-07-10 12:38:44 - train: epoch 0044, iter [01500, 05004], lr: 0.071171, loss: 1.8901
2022-07-10 12:39:19 - train: epoch 0044, iter [01600, 05004], lr: 0.071147, loss: 1.9545
2022-07-10 12:39:54 - train: epoch 0044, iter [01700, 05004], lr: 0.071123, loss: 1.9187
2022-07-10 12:40:28 - train: epoch 0044, iter [01800, 05004], lr: 0.071100, loss: 1.7451
2022-07-10 12:41:02 - train: epoch 0044, iter [01900, 05004], lr: 0.071076, loss: 2.0645
2022-07-10 12:41:37 - train: epoch 0044, iter [02000, 05004], lr: 0.071052, loss: 1.8611
2022-07-10 12:42:11 - train: epoch 0044, iter [02100, 05004], lr: 0.071028, loss: 2.2500
2022-07-10 12:42:46 - train: epoch 0044, iter [02200, 05004], lr: 0.071005, loss: 1.9058
2022-07-10 12:43:20 - train: epoch 0044, iter [02300, 05004], lr: 0.070981, loss: 2.2464
2022-07-10 12:43:55 - train: epoch 0044, iter [02400, 05004], lr: 0.070957, loss: 2.1006
2022-07-10 12:44:30 - train: epoch 0044, iter [02500, 05004], lr: 0.070933, loss: 1.9858
2022-07-10 12:45:04 - train: epoch 0044, iter [02600, 05004], lr: 0.070910, loss: 2.0902
2022-07-10 12:45:38 - train: epoch 0044, iter [02700, 05004], lr: 0.070886, loss: 2.1547
2022-07-10 12:46:13 - train: epoch 0044, iter [02800, 05004], lr: 0.070862, loss: 1.8399
2022-07-10 12:46:48 - train: epoch 0044, iter [02900, 05004], lr: 0.070838, loss: 1.7681
2022-07-10 12:47:23 - train: epoch 0044, iter [03000, 05004], lr: 0.070815, loss: 2.0305
2022-07-10 12:47:56 - train: epoch 0044, iter [03100, 05004], lr: 0.070791, loss: 1.9100
2022-07-10 12:48:30 - train: epoch 0044, iter [03200, 05004], lr: 0.070767, loss: 1.6775
2022-07-10 12:49:04 - train: epoch 0044, iter [03300, 05004], lr: 0.070743, loss: 1.8945
2022-07-10 12:49:39 - train: epoch 0044, iter [03400, 05004], lr: 0.070719, loss: 2.1780
2022-07-10 12:50:14 - train: epoch 0044, iter [03500, 05004], lr: 0.070696, loss: 2.0432
2022-07-10 12:50:48 - train: epoch 0044, iter [03600, 05004], lr: 0.070672, loss: 1.9541
2022-07-10 12:51:23 - train: epoch 0044, iter [03700, 05004], lr: 0.070648, loss: 2.1087
2022-07-10 12:51:58 - train: epoch 0044, iter [03800, 05004], lr: 0.070624, loss: 1.7982
2022-07-10 12:52:32 - train: epoch 0044, iter [03900, 05004], lr: 0.070600, loss: 2.0370
2022-07-10 12:53:07 - train: epoch 0044, iter [04000, 05004], lr: 0.070576, loss: 1.9658
2022-07-10 12:53:41 - train: epoch 0044, iter [04100, 05004], lr: 0.070553, loss: 1.9706
2022-07-10 12:54:15 - train: epoch 0044, iter [04200, 05004], lr: 0.070529, loss: 1.9749
2022-07-10 12:54:50 - train: epoch 0044, iter [04300, 05004], lr: 0.070505, loss: 1.8665
2022-07-10 12:55:25 - train: epoch 0044, iter [04400, 05004], lr: 0.070481, loss: 1.8768
2022-07-10 12:56:00 - train: epoch 0044, iter [04500, 05004], lr: 0.070457, loss: 2.1293
2022-07-10 12:56:34 - train: epoch 0044, iter [04600, 05004], lr: 0.070433, loss: 2.0528
2022-07-10 12:57:08 - train: epoch 0044, iter [04700, 05004], lr: 0.070409, loss: 2.1140
2022-07-10 12:57:43 - train: epoch 0044, iter [04800, 05004], lr: 0.070386, loss: 2.0774
2022-07-10 12:58:17 - train: epoch 0044, iter [04900, 05004], lr: 0.070362, loss: 1.8752
2022-07-10 12:58:51 - train: epoch 0044, iter [05000, 05004], lr: 0.070338, loss: 1.9995
2022-07-10 12:58:52 - train: epoch 044, train_loss: 1.9858
2022-07-10 13:00:07 - eval: epoch: 044, acc1: 58.142%, acc5: 82.130%, test_loss: 1.7707, per_image_load_time: 2.550ms, per_image_inference_time: 0.368ms
2022-07-10 13:00:07 - until epoch: 044, best_acc1: 58.970%
2022-07-10 13:00:07 - epoch 045 lr: 0.070337
2022-07-10 13:00:47 - train: epoch 0045, iter [00100, 05004], lr: 0.070313, loss: 1.7949
2022-07-10 13:01:20 - train: epoch 0045, iter [00200, 05004], lr: 0.070289, loss: 1.9309
2022-07-10 13:01:54 - train: epoch 0045, iter [00300, 05004], lr: 0.070265, loss: 1.8914
2022-07-10 13:02:28 - train: epoch 0045, iter [00400, 05004], lr: 0.070241, loss: 2.0417
2022-07-10 13:03:03 - train: epoch 0045, iter [00500, 05004], lr: 0.070217, loss: 2.1744
2022-07-10 13:03:36 - train: epoch 0045, iter [00600, 05004], lr: 0.070193, loss: 2.0337
2022-07-10 13:04:12 - train: epoch 0045, iter [00700, 05004], lr: 0.070169, loss: 1.8132
2022-07-10 13:04:45 - train: epoch 0045, iter [00800, 05004], lr: 0.070145, loss: 1.8977
2022-07-10 13:05:19 - train: epoch 0045, iter [00900, 05004], lr: 0.070122, loss: 1.9037
2022-07-10 13:05:53 - train: epoch 0045, iter [01000, 05004], lr: 0.070098, loss: 1.9563
2022-07-10 13:06:26 - train: epoch 0045, iter [01100, 05004], lr: 0.070074, loss: 1.9802
2022-07-10 13:07:01 - train: epoch 0045, iter [01200, 05004], lr: 0.070050, loss: 1.8944
2022-07-10 13:07:36 - train: epoch 0045, iter [01300, 05004], lr: 0.070026, loss: 2.0095
2022-07-10 13:08:11 - train: epoch 0045, iter [01400, 05004], lr: 0.070002, loss: 2.0113
2022-07-10 13:08:45 - train: epoch 0045, iter [01500, 05004], lr: 0.069978, loss: 1.9620
2022-07-10 13:09:20 - train: epoch 0045, iter [01600, 05004], lr: 0.069954, loss: 1.9697
2022-07-10 13:09:54 - train: epoch 0045, iter [01700, 05004], lr: 0.069930, loss: 1.7746
2022-07-10 13:10:28 - train: epoch 0045, iter [01800, 05004], lr: 0.069906, loss: 2.0252
2022-07-10 13:11:03 - train: epoch 0045, iter [01900, 05004], lr: 0.069882, loss: 1.8605
2022-07-10 13:11:38 - train: epoch 0045, iter [02000, 05004], lr: 0.069858, loss: 2.0862
2022-07-10 13:12:12 - train: epoch 0045, iter [02100, 05004], lr: 0.069834, loss: 2.2447
2022-07-10 13:12:47 - train: epoch 0045, iter [02200, 05004], lr: 0.069810, loss: 2.0658
2022-07-10 13:13:21 - train: epoch 0045, iter [02300, 05004], lr: 0.069786, loss: 1.9407
2022-07-10 13:13:55 - train: epoch 0045, iter [02400, 05004], lr: 0.069762, loss: 1.8451
2022-07-10 13:14:29 - train: epoch 0045, iter [02500, 05004], lr: 0.069738, loss: 2.1707
2022-07-10 13:15:04 - train: epoch 0045, iter [02600, 05004], lr: 0.069714, loss: 2.0299
2022-07-10 13:15:39 - train: epoch 0045, iter [02700, 05004], lr: 0.069690, loss: 1.8936
2022-07-10 13:16:13 - train: epoch 0045, iter [02800, 05004], lr: 0.069666, loss: 2.1028
2022-07-10 13:16:47 - train: epoch 0045, iter [02900, 05004], lr: 0.069641, loss: 2.1403
2022-07-10 13:17:22 - train: epoch 0045, iter [03000, 05004], lr: 0.069617, loss: 1.8981
2022-07-10 13:17:57 - train: epoch 0045, iter [03100, 05004], lr: 0.069593, loss: 2.0838
2022-07-10 13:18:31 - train: epoch 0045, iter [03200, 05004], lr: 0.069569, loss: 2.2507
2022-07-10 13:19:06 - train: epoch 0045, iter [03300, 05004], lr: 0.069545, loss: 2.0275
2022-07-10 13:19:40 - train: epoch 0045, iter [03400, 05004], lr: 0.069521, loss: 1.8297
2022-07-10 13:20:14 - train: epoch 0045, iter [03500, 05004], lr: 0.069497, loss: 2.0325
2022-07-10 13:20:49 - train: epoch 0045, iter [03600, 05004], lr: 0.069473, loss: 2.0146
2022-07-10 13:21:23 - train: epoch 0045, iter [03700, 05004], lr: 0.069449, loss: 1.9953
2022-07-10 13:21:57 - train: epoch 0045, iter [03800, 05004], lr: 0.069425, loss: 1.9620
2022-07-10 13:22:32 - train: epoch 0045, iter [03900, 05004], lr: 0.069401, loss: 2.2726
2022-07-10 13:23:06 - train: epoch 0045, iter [04000, 05004], lr: 0.069377, loss: 1.9846
2022-07-10 13:23:42 - train: epoch 0045, iter [04100, 05004], lr: 0.069352, loss: 1.8046
2022-07-10 13:24:15 - train: epoch 0045, iter [04200, 05004], lr: 0.069328, loss: 2.0608
2022-07-10 13:24:49 - train: epoch 0045, iter [04300, 05004], lr: 0.069304, loss: 2.0234
2022-07-10 13:25:24 - train: epoch 0045, iter [04400, 05004], lr: 0.069280, loss: 2.0181
2022-07-10 13:25:59 - train: epoch 0045, iter [04500, 05004], lr: 0.069256, loss: 2.0044
2022-07-10 13:26:34 - train: epoch 0045, iter [04600, 05004], lr: 0.069232, loss: 2.1401
2022-07-10 13:27:09 - train: epoch 0045, iter [04700, 05004], lr: 0.069208, loss: 2.2085
2022-07-10 13:27:42 - train: epoch 0045, iter [04800, 05004], lr: 0.069183, loss: 1.7330
2022-07-10 13:28:17 - train: epoch 0045, iter [04900, 05004], lr: 0.069159, loss: 2.0657
2022-07-10 13:28:50 - train: epoch 0045, iter [05000, 05004], lr: 0.069135, loss: 1.9258
2022-07-10 13:28:51 - train: epoch 045, train_loss: 1.9808
2022-07-10 13:30:05 - eval: epoch: 045, acc1: 58.916%, acc5: 82.770%, test_loss: 1.7241, per_image_load_time: 1.888ms, per_image_inference_time: 0.381ms
2022-07-10 13:30:06 - until epoch: 045, best_acc1: 58.970%
2022-07-10 13:30:06 - epoch 046 lr: 0.069134
2022-07-10 13:30:45 - train: epoch 0046, iter [00100, 05004], lr: 0.069110, loss: 1.7765
2022-07-10 13:31:19 - train: epoch 0046, iter [00200, 05004], lr: 0.069086, loss: 1.8931
2022-07-10 13:31:53 - train: epoch 0046, iter [00300, 05004], lr: 0.069062, loss: 2.0475
2022-07-10 13:32:28 - train: epoch 0046, iter [00400, 05004], lr: 0.069037, loss: 2.0860
2022-07-10 13:33:01 - train: epoch 0046, iter [00500, 05004], lr: 0.069013, loss: 1.8316
2022-07-10 13:33:36 - train: epoch 0046, iter [00600, 05004], lr: 0.068989, loss: 2.1605
2022-07-10 13:34:10 - train: epoch 0046, iter [00700, 05004], lr: 0.068965, loss: 1.6226
2022-07-10 13:34:44 - train: epoch 0046, iter [00800, 05004], lr: 0.068941, loss: 2.1408
2022-07-10 13:35:17 - train: epoch 0046, iter [00900, 05004], lr: 0.068916, loss: 1.7550
2022-07-10 13:35:52 - train: epoch 0046, iter [01000, 05004], lr: 0.068892, loss: 1.7188
2022-07-10 13:36:27 - train: epoch 0046, iter [01100, 05004], lr: 0.068868, loss: 2.0093
2022-07-10 13:37:01 - train: epoch 0046, iter [01200, 05004], lr: 0.068844, loss: 1.8713
2022-07-10 13:37:35 - train: epoch 0046, iter [01300, 05004], lr: 0.068820, loss: 1.9598
2022-07-10 13:38:09 - train: epoch 0046, iter [01400, 05004], lr: 0.068795, loss: 2.0322
2022-07-10 13:38:44 - train: epoch 0046, iter [01500, 05004], lr: 0.068771, loss: 1.9044
2022-07-10 13:39:18 - train: epoch 0046, iter [01600, 05004], lr: 0.068747, loss: 1.9886
2022-07-10 13:39:52 - train: epoch 0046, iter [01700, 05004], lr: 0.068723, loss: 2.0180
2022-07-10 13:40:27 - train: epoch 0046, iter [01800, 05004], lr: 0.068698, loss: 2.1048
2022-07-10 13:41:01 - train: epoch 0046, iter [01900, 05004], lr: 0.068674, loss: 1.8276
2022-07-10 13:41:35 - train: epoch 0046, iter [02000, 05004], lr: 0.068650, loss: 1.9304
2022-07-10 13:42:09 - train: epoch 0046, iter [02100, 05004], lr: 0.068626, loss: 2.2433
2022-07-10 13:42:44 - train: epoch 0046, iter [02200, 05004], lr: 0.068601, loss: 1.8204
2022-07-10 13:43:18 - train: epoch 0046, iter [02300, 05004], lr: 0.068577, loss: 2.1591
2022-07-10 13:43:53 - train: epoch 0046, iter [02400, 05004], lr: 0.068553, loss: 1.7620
2022-07-10 13:44:27 - train: epoch 0046, iter [02500, 05004], lr: 0.068528, loss: 2.0411
2022-07-10 13:45:02 - train: epoch 0046, iter [02600, 05004], lr: 0.068504, loss: 2.3715
2022-07-10 13:45:36 - train: epoch 0046, iter [02700, 05004], lr: 0.068480, loss: 1.6693
2022-07-10 13:46:10 - train: epoch 0046, iter [02800, 05004], lr: 0.068455, loss: 1.6898
2022-07-10 13:46:45 - train: epoch 0046, iter [02900, 05004], lr: 0.068431, loss: 2.0467
2022-07-10 13:47:19 - train: epoch 0046, iter [03000, 05004], lr: 0.068407, loss: 1.8165
2022-07-10 13:47:54 - train: epoch 0046, iter [03100, 05004], lr: 0.068382, loss: 1.9258
2022-07-10 13:48:28 - train: epoch 0046, iter [03200, 05004], lr: 0.068358, loss: 2.0240
2022-07-10 13:49:03 - train: epoch 0046, iter [03300, 05004], lr: 0.068334, loss: 1.8970
2022-07-10 13:49:37 - train: epoch 0046, iter [03400, 05004], lr: 0.068309, loss: 2.1111
2022-07-10 13:50:12 - train: epoch 0046, iter [03500, 05004], lr: 0.068285, loss: 2.1901
2022-07-10 13:50:47 - train: epoch 0046, iter [03600, 05004], lr: 0.068261, loss: 1.9067
2022-07-10 13:51:21 - train: epoch 0046, iter [03700, 05004], lr: 0.068236, loss: 1.8608
2022-07-10 13:51:55 - train: epoch 0046, iter [03800, 05004], lr: 0.068212, loss: 1.8395
2022-07-10 13:52:30 - train: epoch 0046, iter [03900, 05004], lr: 0.068188, loss: 2.1066
2022-07-10 13:53:04 - train: epoch 0046, iter [04000, 05004], lr: 0.068163, loss: 1.7767
2022-07-10 13:53:39 - train: epoch 0046, iter [04100, 05004], lr: 0.068139, loss: 2.1825
2022-07-10 13:54:14 - train: epoch 0046, iter [04200, 05004], lr: 0.068115, loss: 1.7923
2022-07-10 13:54:48 - train: epoch 0046, iter [04300, 05004], lr: 0.068090, loss: 2.1409
2022-07-10 13:55:23 - train: epoch 0046, iter [04400, 05004], lr: 0.068066, loss: 2.0479
2022-07-10 13:55:57 - train: epoch 0046, iter [04500, 05004], lr: 0.068041, loss: 1.8183
2022-07-10 13:56:31 - train: epoch 0046, iter [04600, 05004], lr: 0.068017, loss: 1.9528
2022-07-10 13:57:05 - train: epoch 0046, iter [04700, 05004], lr: 0.067993, loss: 1.8398
2022-07-10 13:57:40 - train: epoch 0046, iter [04800, 05004], lr: 0.067968, loss: 1.7944
2022-07-10 13:58:14 - train: epoch 0046, iter [04900, 05004], lr: 0.067944, loss: 2.0797
2022-07-10 13:58:47 - train: epoch 0046, iter [05000, 05004], lr: 0.067919, loss: 2.0298
2022-07-10 13:58:48 - train: epoch 046, train_loss: 1.9710
2022-07-10 14:00:04 - eval: epoch: 046, acc1: 58.814%, acc5: 82.532%, test_loss: 1.7401, per_image_load_time: 2.459ms, per_image_inference_time: 0.363ms
2022-07-10 14:00:04 - until epoch: 046, best_acc1: 58.970%
2022-07-10 14:00:04 - epoch 047 lr: 0.067918
2022-07-10 14:00:44 - train: epoch 0047, iter [00100, 05004], lr: 0.067894, loss: 1.8067
2022-07-10 14:01:18 - train: epoch 0047, iter [00200, 05004], lr: 0.067870, loss: 1.9712
2022-07-10 14:01:53 - train: epoch 0047, iter [00300, 05004], lr: 0.067845, loss: 1.7084
2022-07-10 14:02:27 - train: epoch 0047, iter [00400, 05004], lr: 0.067821, loss: 1.7271
2022-07-10 14:03:02 - train: epoch 0047, iter [00500, 05004], lr: 0.067796, loss: 1.8539
2022-07-10 14:03:35 - train: epoch 0047, iter [00600, 05004], lr: 0.067772, loss: 1.9998
2022-07-10 14:04:09 - train: epoch 0047, iter [00700, 05004], lr: 0.067747, loss: 1.9792
2022-07-10 14:04:44 - train: epoch 0047, iter [00800, 05004], lr: 0.067723, loss: 1.7711
2022-07-10 14:05:18 - train: epoch 0047, iter [00900, 05004], lr: 0.067698, loss: 2.0639
2022-07-10 14:05:53 - train: epoch 0047, iter [01000, 05004], lr: 0.067674, loss: 1.9808
2022-07-10 14:06:26 - train: epoch 0047, iter [01100, 05004], lr: 0.067649, loss: 2.0490
2022-07-10 14:07:01 - train: epoch 0047, iter [01200, 05004], lr: 0.067625, loss: 1.8148
2022-07-10 14:07:35 - train: epoch 0047, iter [01300, 05004], lr: 0.067601, loss: 2.1349
2022-07-10 14:08:09 - train: epoch 0047, iter [01400, 05004], lr: 0.067576, loss: 2.0525
2022-07-10 14:08:43 - train: epoch 0047, iter [01500, 05004], lr: 0.067552, loss: 1.8848
2022-07-10 14:09:18 - train: epoch 0047, iter [01600, 05004], lr: 0.067527, loss: 2.2221
2022-07-10 14:09:53 - train: epoch 0047, iter [01700, 05004], lr: 0.067503, loss: 1.6906
2022-07-10 14:10:27 - train: epoch 0047, iter [01800, 05004], lr: 0.067478, loss: 2.0630
2022-07-10 14:11:02 - train: epoch 0047, iter [01900, 05004], lr: 0.067454, loss: 1.8750
2022-07-10 14:11:36 - train: epoch 0047, iter [02000, 05004], lr: 0.067429, loss: 2.0241
2022-07-10 14:12:11 - train: epoch 0047, iter [02100, 05004], lr: 0.067404, loss: 2.1487
2022-07-10 14:12:44 - train: epoch 0047, iter [02200, 05004], lr: 0.067380, loss: 1.9807
2022-07-10 14:13:19 - train: epoch 0047, iter [02300, 05004], lr: 0.067355, loss: 1.9133
2022-07-10 14:13:54 - train: epoch 0047, iter [02400, 05004], lr: 0.067331, loss: 1.8376
2022-07-10 14:14:28 - train: epoch 0047, iter [02500, 05004], lr: 0.067306, loss: 2.0179
2022-07-10 14:15:03 - train: epoch 0047, iter [02600, 05004], lr: 0.067282, loss: 2.1350
2022-07-10 14:15:37 - train: epoch 0047, iter [02700, 05004], lr: 0.067257, loss: 1.8599
2022-07-10 14:16:11 - train: epoch 0047, iter [02800, 05004], lr: 0.067233, loss: 1.9007
2022-07-10 14:16:47 - train: epoch 0047, iter [02900, 05004], lr: 0.067208, loss: 2.0456
2022-07-10 14:17:20 - train: epoch 0047, iter [03000, 05004], lr: 0.067184, loss: 1.9303
2022-07-10 14:17:55 - train: epoch 0047, iter [03100, 05004], lr: 0.067159, loss: 2.0774
2022-07-10 14:18:30 - train: epoch 0047, iter [03200, 05004], lr: 0.067134, loss: 1.9560
2022-07-10 14:19:04 - train: epoch 0047, iter [03300, 05004], lr: 0.067110, loss: 1.8335
2022-07-10 14:19:38 - train: epoch 0047, iter [03400, 05004], lr: 0.067085, loss: 1.9817
2022-07-10 14:20:13 - train: epoch 0047, iter [03500, 05004], lr: 0.067061, loss: 2.2438
2022-07-10 14:20:47 - train: epoch 0047, iter [03600, 05004], lr: 0.067036, loss: 2.3992
2022-07-10 14:21:21 - train: epoch 0047, iter [03700, 05004], lr: 0.067011, loss: 2.0612
2022-07-10 14:21:55 - train: epoch 0047, iter [03800, 05004], lr: 0.066987, loss: 1.9497
2022-07-10 14:22:29 - train: epoch 0047, iter [03900, 05004], lr: 0.066962, loss: 1.8034
2022-07-10 14:23:03 - train: epoch 0047, iter [04000, 05004], lr: 0.066938, loss: 1.9981
2022-07-10 14:23:38 - train: epoch 0047, iter [04100, 05004], lr: 0.066913, loss: 2.1019
2022-07-10 14:24:13 - train: epoch 0047, iter [04200, 05004], lr: 0.066888, loss: 1.9344
2022-07-10 14:24:48 - train: epoch 0047, iter [04300, 05004], lr: 0.066864, loss: 1.6867
2022-07-10 14:25:22 - train: epoch 0047, iter [04400, 05004], lr: 0.066839, loss: 1.8301
2022-07-10 14:25:56 - train: epoch 0047, iter [04500, 05004], lr: 0.066815, loss: 1.8476
2022-07-10 14:26:31 - train: epoch 0047, iter [04600, 05004], lr: 0.066790, loss: 1.7601
2022-07-10 14:27:05 - train: epoch 0047, iter [04700, 05004], lr: 0.066765, loss: 2.2513
2022-07-10 14:27:40 - train: epoch 0047, iter [04800, 05004], lr: 0.066741, loss: 1.9275
2022-07-10 14:28:14 - train: epoch 0047, iter [04900, 05004], lr: 0.066716, loss: 2.0575
2022-07-10 14:28:47 - train: epoch 0047, iter [05000, 05004], lr: 0.066691, loss: 1.8027
2022-07-10 14:28:48 - train: epoch 047, train_loss: 1.9624
2022-07-10 14:30:04 - eval: epoch: 047, acc1: 59.356%, acc5: 82.914%, test_loss: 1.7097, per_image_load_time: 2.422ms, per_image_inference_time: 0.387ms
2022-07-10 14:30:04 - until epoch: 047, best_acc1: 59.356%
2022-07-10 14:30:04 - epoch 048 lr: 0.066690
2022-07-10 14:30:43 - train: epoch 0048, iter [00100, 05004], lr: 0.066666, loss: 1.8826
2022-07-10 14:31:18 - train: epoch 0048, iter [00200, 05004], lr: 0.066641, loss: 2.2474
2022-07-10 14:31:51 - train: epoch 0048, iter [00300, 05004], lr: 0.066616, loss: 1.9532
2022-07-10 14:32:24 - train: epoch 0048, iter [00400, 05004], lr: 0.066592, loss: 1.8402
2022-07-10 14:32:58 - train: epoch 0048, iter [00500, 05004], lr: 0.066567, loss: 1.9178
2022-07-10 14:33:33 - train: epoch 0048, iter [00600, 05004], lr: 0.066542, loss: 2.0479
2022-07-10 14:34:07 - train: epoch 0048, iter [00700, 05004], lr: 0.066518, loss: 2.0525
2022-07-10 14:34:41 - train: epoch 0048, iter [00800, 05004], lr: 0.066493, loss: 1.9090
2022-07-10 14:35:15 - train: epoch 0048, iter [00900, 05004], lr: 0.066468, loss: 1.9776
2022-07-10 14:35:49 - train: epoch 0048, iter [01000, 05004], lr: 0.066444, loss: 2.2190
2022-07-10 14:36:25 - train: epoch 0048, iter [01100, 05004], lr: 0.066419, loss: 2.2197
2022-07-10 14:36:58 - train: epoch 0048, iter [01200, 05004], lr: 0.066394, loss: 1.9391
2022-07-10 14:37:33 - train: epoch 0048, iter [01300, 05004], lr: 0.066369, loss: 1.9148
2022-07-10 14:38:08 - train: epoch 0048, iter [01400, 05004], lr: 0.066345, loss: 1.9064
2022-07-10 14:38:42 - train: epoch 0048, iter [01500, 05004], lr: 0.066320, loss: 2.0533
2022-07-10 14:39:16 - train: epoch 0048, iter [01600, 05004], lr: 0.066295, loss: 2.1405
2022-07-10 14:39:50 - train: epoch 0048, iter [01700, 05004], lr: 0.066270, loss: 1.9235
2022-07-10 14:40:24 - train: epoch 0048, iter [01800, 05004], lr: 0.066246, loss: 2.1301
2022-07-10 14:41:00 - train: epoch 0048, iter [01900, 05004], lr: 0.066221, loss: 1.9714
2022-07-10 14:41:34 - train: epoch 0048, iter [02000, 05004], lr: 0.066196, loss: 2.0100
2022-07-10 14:42:09 - train: epoch 0048, iter [02100, 05004], lr: 0.066172, loss: 1.9234
2022-07-10 14:42:43 - train: epoch 0048, iter [02200, 05004], lr: 0.066147, loss: 2.1558
2022-07-10 14:43:18 - train: epoch 0048, iter [02300, 05004], lr: 0.066122, loss: 1.8740
2022-07-10 14:43:52 - train: epoch 0048, iter [02400, 05004], lr: 0.066097, loss: 2.1015
2022-07-10 14:44:26 - train: epoch 0048, iter [02500, 05004], lr: 0.066072, loss: 2.0571
2022-07-10 14:45:01 - train: epoch 0048, iter [02600, 05004], lr: 0.066048, loss: 2.1137
2022-07-10 14:45:35 - train: epoch 0048, iter [02700, 05004], lr: 0.066023, loss: 2.0554
2022-07-10 14:46:10 - train: epoch 0048, iter [02800, 05004], lr: 0.065998, loss: 1.9930
2022-07-10 14:46:44 - train: epoch 0048, iter [02900, 05004], lr: 0.065973, loss: 2.0185
2022-07-10 14:47:18 - train: epoch 0048, iter [03000, 05004], lr: 0.065949, loss: 1.8500
2022-07-10 14:47:52 - train: epoch 0048, iter [03100, 05004], lr: 0.065924, loss: 2.0857
2022-07-10 14:48:27 - train: epoch 0048, iter [03200, 05004], lr: 0.065899, loss: 1.8656
2022-07-10 14:49:01 - train: epoch 0048, iter [03300, 05004], lr: 0.065874, loss: 2.0217
2022-07-10 14:49:35 - train: epoch 0048, iter [03400, 05004], lr: 0.065849, loss: 1.9513
2022-07-10 14:50:10 - train: epoch 0048, iter [03500, 05004], lr: 0.065825, loss: 2.1594
2022-07-10 14:50:45 - train: epoch 0048, iter [03600, 05004], lr: 0.065800, loss: 1.9017
2022-07-10 14:51:20 - train: epoch 0048, iter [03700, 05004], lr: 0.065775, loss: 1.9252
2022-07-10 14:51:54 - train: epoch 0048, iter [03800, 05004], lr: 0.065750, loss: 1.9039
2022-07-10 14:52:29 - train: epoch 0048, iter [03900, 05004], lr: 0.065725, loss: 1.9692
2022-07-10 14:53:03 - train: epoch 0048, iter [04000, 05004], lr: 0.065700, loss: 1.8726
2022-07-10 14:53:38 - train: epoch 0048, iter [04100, 05004], lr: 0.065676, loss: 2.2123
2022-07-10 14:54:12 - train: epoch 0048, iter [04200, 05004], lr: 0.065651, loss: 1.8683
2022-07-10 14:54:47 - train: epoch 0048, iter [04300, 05004], lr: 0.065626, loss: 1.9144
2022-07-10 14:55:21 - train: epoch 0048, iter [04400, 05004], lr: 0.065601, loss: 1.8674
2022-07-10 14:55:56 - train: epoch 0048, iter [04500, 05004], lr: 0.065576, loss: 2.1607
2022-07-10 14:56:30 - train: epoch 0048, iter [04600, 05004], lr: 0.065551, loss: 2.1276
2022-07-10 14:57:05 - train: epoch 0048, iter [04700, 05004], lr: 0.065526, loss: 1.9847
2022-07-10 14:57:39 - train: epoch 0048, iter [04800, 05004], lr: 0.065502, loss: 2.3478
2022-07-10 14:58:14 - train: epoch 0048, iter [04900, 05004], lr: 0.065477, loss: 1.8466
2022-07-10 14:58:47 - train: epoch 0048, iter [05000, 05004], lr: 0.065452, loss: 1.9387
2022-07-10 14:58:48 - train: epoch 048, train_loss: 1.9541
2022-07-10 15:00:03 - eval: epoch: 048, acc1: 59.586%, acc5: 83.228%, test_loss: 1.6950, per_image_load_time: 2.454ms, per_image_inference_time: 0.398ms
2022-07-10 15:00:03 - until epoch: 048, best_acc1: 59.586%
2022-07-10 15:00:03 - epoch 049 lr: 0.065451
2022-07-10 15:00:42 - train: epoch 0049, iter [00100, 05004], lr: 0.065426, loss: 2.1222
2022-07-10 15:01:17 - train: epoch 0049, iter [00200, 05004], lr: 0.065401, loss: 1.8192
2022-07-10 15:01:51 - train: epoch 0049, iter [00300, 05004], lr: 0.065376, loss: 2.1279
2022-07-10 15:02:25 - train: epoch 0049, iter [00400, 05004], lr: 0.065351, loss: 2.0019
2022-07-10 15:02:58 - train: epoch 0049, iter [00500, 05004], lr: 0.065326, loss: 1.9020
2022-07-10 15:03:32 - train: epoch 0049, iter [00600, 05004], lr: 0.065302, loss: 1.7714
2022-07-10 15:04:07 - train: epoch 0049, iter [00700, 05004], lr: 0.065277, loss: 2.0389
2022-07-10 15:04:41 - train: epoch 0049, iter [00800, 05004], lr: 0.065252, loss: 2.2382
2022-07-10 15:05:15 - train: epoch 0049, iter [00900, 05004], lr: 0.065227, loss: 1.8443
2022-07-10 15:05:49 - train: epoch 0049, iter [01000, 05004], lr: 0.065202, loss: 1.9433
2022-07-10 15:06:24 - train: epoch 0049, iter [01100, 05004], lr: 0.065177, loss: 1.9073
2022-07-10 15:06:58 - train: epoch 0049, iter [01200, 05004], lr: 0.065152, loss: 1.6566
2022-07-10 15:07:32 - train: epoch 0049, iter [01300, 05004], lr: 0.065127, loss: 2.2149
2022-07-10 15:08:07 - train: epoch 0049, iter [01400, 05004], lr: 0.065102, loss: 2.2115
2022-07-10 15:08:41 - train: epoch 0049, iter [01500, 05004], lr: 0.065077, loss: 1.8334
2022-07-10 15:09:14 - train: epoch 0049, iter [01600, 05004], lr: 0.065052, loss: 2.1131
2022-07-10 15:09:49 - train: epoch 0049, iter [01700, 05004], lr: 0.065027, loss: 1.8916
2022-07-10 15:10:23 - train: epoch 0049, iter [01800, 05004], lr: 0.065002, loss: 1.7808
2022-07-10 15:10:58 - train: epoch 0049, iter [01900, 05004], lr: 0.064977, loss: 1.8439
2022-07-10 15:11:32 - train: epoch 0049, iter [02000, 05004], lr: 0.064952, loss: 1.8764
2022-07-10 15:12:06 - train: epoch 0049, iter [02100, 05004], lr: 0.064927, loss: 1.8216
2022-07-10 15:12:42 - train: epoch 0049, iter [02200, 05004], lr: 0.064903, loss: 2.0675
2022-07-10 15:13:16 - train: epoch 0049, iter [02300, 05004], lr: 0.064878, loss: 1.9374
2022-07-10 15:13:50 - train: epoch 0049, iter [02400, 05004], lr: 0.064853, loss: 1.8774
2022-07-10 15:14:24 - train: epoch 0049, iter [02500, 05004], lr: 0.064828, loss: 2.0676
2022-07-10 15:14:59 - train: epoch 0049, iter [02600, 05004], lr: 0.064803, loss: 2.0230
2022-07-10 15:15:33 - train: epoch 0049, iter [02700, 05004], lr: 0.064778, loss: 1.8171
2022-07-10 15:16:08 - train: epoch 0049, iter [02800, 05004], lr: 0.064753, loss: 1.9030
2022-07-10 15:16:42 - train: epoch 0049, iter [02900, 05004], lr: 0.064728, loss: 1.8912
2022-07-10 15:17:16 - train: epoch 0049, iter [03000, 05004], lr: 0.064703, loss: 1.9441
2022-07-10 15:17:51 - train: epoch 0049, iter [03100, 05004], lr: 0.064678, loss: 2.0022
2022-07-10 15:18:24 - train: epoch 0049, iter [03200, 05004], lr: 0.064653, loss: 2.2831
2022-07-10 15:18:59 - train: epoch 0049, iter [03300, 05004], lr: 0.064628, loss: 1.8208
2022-07-10 15:19:33 - train: epoch 0049, iter [03400, 05004], lr: 0.064603, loss: 2.1372
2022-07-10 15:20:07 - train: epoch 0049, iter [03500, 05004], lr: 0.064578, loss: 2.0610
2022-07-10 15:20:42 - train: epoch 0049, iter [03600, 05004], lr: 0.064553, loss: 1.9612
2022-07-10 15:21:16 - train: epoch 0049, iter [03700, 05004], lr: 0.064528, loss: 1.8218
2022-07-10 15:21:50 - train: epoch 0049, iter [03800, 05004], lr: 0.064502, loss: 2.0897
2022-07-10 15:22:25 - train: epoch 0049, iter [03900, 05004], lr: 0.064477, loss: 2.2292
2022-07-10 15:22:59 - train: epoch 0049, iter [04000, 05004], lr: 0.064452, loss: 1.6411
2022-07-10 15:23:32 - train: epoch 0049, iter [04100, 05004], lr: 0.064427, loss: 1.8222
2022-07-10 15:24:07 - train: epoch 0049, iter [04200, 05004], lr: 0.064402, loss: 2.0674
2022-07-10 15:24:41 - train: epoch 0049, iter [04300, 05004], lr: 0.064377, loss: 2.2392
2022-07-10 15:25:16 - train: epoch 0049, iter [04400, 05004], lr: 0.064352, loss: 2.0108
2022-07-10 15:25:50 - train: epoch 0049, iter [04500, 05004], lr: 0.064327, loss: 1.6719
2022-07-10 15:26:25 - train: epoch 0049, iter [04600, 05004], lr: 0.064302, loss: 1.9373
2022-07-10 15:26:59 - train: epoch 0049, iter [04700, 05004], lr: 0.064277, loss: 2.1507
2022-07-10 15:27:34 - train: epoch 0049, iter [04800, 05004], lr: 0.064252, loss: 2.0689
2022-07-10 15:28:07 - train: epoch 0049, iter [04900, 05004], lr: 0.064227, loss: 1.7408
2022-07-10 15:28:40 - train: epoch 0049, iter [05000, 05004], lr: 0.064202, loss: 1.9988
2022-07-10 15:28:41 - train: epoch 049, train_loss: 1.9443
2022-07-10 15:29:56 - eval: epoch: 049, acc1: 59.926%, acc5: 83.518%, test_loss: 1.6715, per_image_load_time: 1.939ms, per_image_inference_time: 0.360ms
2022-07-10 15:29:57 - until epoch: 049, best_acc1: 59.926%
2022-07-10 15:29:57 - epoch 050 lr: 0.064201
2022-07-10 15:30:36 - train: epoch 0050, iter [00100, 05004], lr: 0.064176, loss: 1.9619
2022-07-10 15:31:09 - train: epoch 0050, iter [00200, 05004], lr: 0.064151, loss: 2.0417
2022-07-10 15:31:44 - train: epoch 0050, iter [00300, 05004], lr: 0.064126, loss: 1.7494
2022-07-10 15:32:17 - train: epoch 0050, iter [00400, 05004], lr: 0.064100, loss: 1.7855
2022-07-10 15:32:51 - train: epoch 0050, iter [00500, 05004], lr: 0.064075, loss: 1.9821
2022-07-10 15:33:24 - train: epoch 0050, iter [00600, 05004], lr: 0.064050, loss: 2.1672
2022-07-10 15:33:59 - train: epoch 0050, iter [00700, 05004], lr: 0.064025, loss: 1.7333
2022-07-10 15:34:33 - train: epoch 0050, iter [00800, 05004], lr: 0.064000, loss: 1.8193
2022-07-10 15:35:07 - train: epoch 0050, iter [00900, 05004], lr: 0.063975, loss: 1.8457
2022-07-10 15:35:41 - train: epoch 0050, iter [01000, 05004], lr: 0.063950, loss: 1.9853
2022-07-10 15:36:16 - train: epoch 0050, iter [01100, 05004], lr: 0.063925, loss: 1.8966
2022-07-10 15:36:49 - train: epoch 0050, iter [01200, 05004], lr: 0.063900, loss: 2.0193
2022-07-10 15:37:23 - train: epoch 0050, iter [01300, 05004], lr: 0.063874, loss: 1.8435
2022-07-10 15:37:57 - train: epoch 0050, iter [01400, 05004], lr: 0.063849, loss: 2.0295
2022-07-10 15:38:31 - train: epoch 0050, iter [01500, 05004], lr: 0.063824, loss: 1.7695
2022-07-10 15:39:05 - train: epoch 0050, iter [01600, 05004], lr: 0.063799, loss: 1.9773
2022-07-10 15:39:39 - train: epoch 0050, iter [01700, 05004], lr: 0.063774, loss: 1.8453
2022-07-10 15:40:12 - train: epoch 0050, iter [01800, 05004], lr: 0.063749, loss: 1.9407
2022-07-10 15:40:46 - train: epoch 0050, iter [01900, 05004], lr: 0.063724, loss: 1.8575
2022-07-10 15:41:20 - train: epoch 0050, iter [02000, 05004], lr: 0.063698, loss: 1.8024
2022-07-10 15:41:54 - train: epoch 0050, iter [02100, 05004], lr: 0.063673, loss: 1.8967
2022-07-10 15:42:29 - train: epoch 0050, iter [02200, 05004], lr: 0.063648, loss: 1.9915
2022-07-10 15:43:02 - train: epoch 0050, iter [02300, 05004], lr: 0.063623, loss: 1.7586
2022-07-10 15:43:36 - train: epoch 0050, iter [02400, 05004], lr: 0.063598, loss: 1.8168
2022-07-10 15:44:10 - train: epoch 0050, iter [02500, 05004], lr: 0.063573, loss: 1.8862
2022-07-10 15:44:45 - train: epoch 0050, iter [02600, 05004], lr: 0.063547, loss: 1.6329
2022-07-10 15:45:18 - train: epoch 0050, iter [02700, 05004], lr: 0.063522, loss: 1.9157
2022-07-10 15:45:52 - train: epoch 0050, iter [02800, 05004], lr: 0.063497, loss: 1.8720
2022-07-10 15:46:25 - train: epoch 0050, iter [02900, 05004], lr: 0.063472, loss: 2.1345
2022-07-10 15:47:00 - train: epoch 0050, iter [03000, 05004], lr: 0.063447, loss: 2.0973
2022-07-10 15:47:32 - train: epoch 0050, iter [03100, 05004], lr: 0.063421, loss: 2.0815
2022-07-10 15:48:05 - train: epoch 0050, iter [03200, 05004], lr: 0.063396, loss: 2.0237
2022-07-10 15:48:40 - train: epoch 0050, iter [03300, 05004], lr: 0.063371, loss: 1.6679
2022-07-10 15:49:14 - train: epoch 0050, iter [03400, 05004], lr: 0.063346, loss: 1.8094
2022-07-10 15:49:49 - train: epoch 0050, iter [03500, 05004], lr: 0.063321, loss: 1.8692
2022-07-10 15:50:22 - train: epoch 0050, iter [03600, 05004], lr: 0.063295, loss: 1.9677
2022-07-10 15:50:56 - train: epoch 0050, iter [03700, 05004], lr: 0.063270, loss: 2.1896
2022-07-10 15:51:29 - train: epoch 0050, iter [03800, 05004], lr: 0.063245, loss: 1.9661
2022-07-10 15:52:03 - train: epoch 0050, iter [03900, 05004], lr: 0.063220, loss: 1.6524
2022-07-10 15:52:36 - train: epoch 0050, iter [04000, 05004], lr: 0.063194, loss: 2.1451
2022-07-10 15:53:10 - train: epoch 0050, iter [04100, 05004], lr: 0.063169, loss: 1.8900
2022-07-10 15:53:44 - train: epoch 0050, iter [04200, 05004], lr: 0.063144, loss: 1.9921
2022-07-10 15:54:18 - train: epoch 0050, iter [04300, 05004], lr: 0.063119, loss: 2.0669
2022-07-10 15:54:51 - train: epoch 0050, iter [04400, 05004], lr: 0.063094, loss: 1.9047
2022-07-10 15:55:25 - train: epoch 0050, iter [04500, 05004], lr: 0.063068, loss: 2.0012
2022-07-10 15:55:59 - train: epoch 0050, iter [04600, 05004], lr: 0.063043, loss: 2.0587
2022-07-10 15:56:32 - train: epoch 0050, iter [04700, 05004], lr: 0.063018, loss: 1.9423
2022-07-10 15:57:05 - train: epoch 0050, iter [04800, 05004], lr: 0.062992, loss: 1.8806
2022-07-10 15:57:40 - train: epoch 0050, iter [04900, 05004], lr: 0.062967, loss: 1.5977
2022-07-10 15:58:12 - train: epoch 0050, iter [05000, 05004], lr: 0.062942, loss: 1.7264
2022-07-10 15:58:13 - train: epoch 050, train_loss: 1.9350
2022-07-10 15:59:28 - eval: epoch: 050, acc1: 59.566%, acc5: 82.940%, test_loss: 1.7218, per_image_load_time: 2.514ms, per_image_inference_time: 0.299ms
2022-07-10 15:59:28 - until epoch: 050, best_acc1: 59.926%
2022-07-10 15:59:28 - epoch 051 lr: 0.062941
2022-07-10 16:00:08 - train: epoch 0051, iter [00100, 05004], lr: 0.062916, loss: 2.1276
2022-07-10 16:00:41 - train: epoch 0051, iter [00200, 05004], lr: 0.062890, loss: 2.2576
2022-07-10 16:01:15 - train: epoch 0051, iter [00300, 05004], lr: 0.062865, loss: 1.9697
2022-07-10 16:01:49 - train: epoch 0051, iter [00400, 05004], lr: 0.062840, loss: 1.6648
2022-07-10 16:02:22 - train: epoch 0051, iter [00500, 05004], lr: 0.062815, loss: 2.0784
2022-07-10 16:02:55 - train: epoch 0051, iter [00600, 05004], lr: 0.062789, loss: 1.7448
2022-07-10 16:03:29 - train: epoch 0051, iter [00700, 05004], lr: 0.062764, loss: 1.8864
2022-07-10 16:04:03 - train: epoch 0051, iter [00800, 05004], lr: 0.062739, loss: 2.0568
2022-07-10 16:04:36 - train: epoch 0051, iter [00900, 05004], lr: 0.062713, loss: 1.6660
2022-07-10 16:05:09 - train: epoch 0051, iter [01000, 05004], lr: 0.062688, loss: 2.3667
2022-07-10 16:05:43 - train: epoch 0051, iter [01100, 05004], lr: 0.062663, loss: 1.9480
2022-07-10 16:06:18 - train: epoch 0051, iter [01200, 05004], lr: 0.062637, loss: 1.8544
2022-07-10 16:06:52 - train: epoch 0051, iter [01300, 05004], lr: 0.062612, loss: 1.5984
2022-07-10 16:07:25 - train: epoch 0051, iter [01400, 05004], lr: 0.062587, loss: 1.7559
2022-07-10 16:07:59 - train: epoch 0051, iter [01500, 05004], lr: 0.062562, loss: 1.8580
2022-07-10 16:08:33 - train: epoch 0051, iter [01600, 05004], lr: 0.062536, loss: 1.7080
2022-07-10 16:09:06 - train: epoch 0051, iter [01700, 05004], lr: 0.062511, loss: 2.0236
2022-07-10 16:09:39 - train: epoch 0051, iter [01800, 05004], lr: 0.062486, loss: 2.0940
2022-07-10 16:10:13 - train: epoch 0051, iter [01900, 05004], lr: 0.062460, loss: 1.8846
2022-07-10 16:10:48 - train: epoch 0051, iter [02000, 05004], lr: 0.062435, loss: 1.7730
2022-07-10 16:11:21 - train: epoch 0051, iter [02100, 05004], lr: 0.062410, loss: 1.8902
2022-07-10 16:11:54 - train: epoch 0051, iter [02200, 05004], lr: 0.062384, loss: 1.9035
2022-07-10 16:12:29 - train: epoch 0051, iter [02300, 05004], lr: 0.062359, loss: 2.1599
2022-07-10 16:13:03 - train: epoch 0051, iter [02400, 05004], lr: 0.062334, loss: 2.0393
2022-07-10 16:13:36 - train: epoch 0051, iter [02500, 05004], lr: 0.062308, loss: 1.7369
2022-07-10 16:14:10 - train: epoch 0051, iter [02600, 05004], lr: 0.062283, loss: 1.7524
2022-07-10 16:14:44 - train: epoch 0051, iter [02700, 05004], lr: 0.062257, loss: 2.0599
2022-07-10 16:15:18 - train: epoch 0051, iter [02800, 05004], lr: 0.062232, loss: 1.9596
2022-07-10 16:15:52 - train: epoch 0051, iter [02900, 05004], lr: 0.062207, loss: 1.9142
2022-07-10 16:16:25 - train: epoch 0051, iter [03000, 05004], lr: 0.062181, loss: 1.8150
2022-07-10 16:16:59 - train: epoch 0051, iter [03100, 05004], lr: 0.062156, loss: 1.8985
2022-07-10 16:17:33 - train: epoch 0051, iter [03200, 05004], lr: 0.062131, loss: 1.9314
2022-07-10 16:18:06 - train: epoch 0051, iter [03300, 05004], lr: 0.062105, loss: 1.9881
2022-07-10 16:18:40 - train: epoch 0051, iter [03400, 05004], lr: 0.062080, loss: 1.8245
2022-07-10 16:19:14 - train: epoch 0051, iter [03500, 05004], lr: 0.062054, loss: 1.9531
2022-07-10 16:19:48 - train: epoch 0051, iter [03600, 05004], lr: 0.062029, loss: 1.9292
2022-07-10 16:20:21 - train: epoch 0051, iter [03700, 05004], lr: 0.062004, loss: 1.9332
2022-07-10 16:20:55 - train: epoch 0051, iter [03800, 05004], lr: 0.061978, loss: 2.0186
2022-07-10 16:21:30 - train: epoch 0051, iter [03900, 05004], lr: 0.061953, loss: 1.9644
2022-07-10 16:22:03 - train: epoch 0051, iter [04000, 05004], lr: 0.061927, loss: 1.9957
2022-07-10 16:22:37 - train: epoch 0051, iter [04100, 05004], lr: 0.061902, loss: 1.7789
2022-07-10 16:23:12 - train: epoch 0051, iter [04200, 05004], lr: 0.061877, loss: 2.1464
2022-07-10 16:23:45 - train: epoch 0051, iter [04300, 05004], lr: 0.061851, loss: 1.7791
2022-07-10 16:24:17 - train: epoch 0051, iter [04400, 05004], lr: 0.061826, loss: 2.1443
2022-07-10 16:24:51 - train: epoch 0051, iter [04500, 05004], lr: 0.061800, loss: 1.7300
2022-07-10 16:25:26 - train: epoch 0051, iter [04600, 05004], lr: 0.061775, loss: 2.0616
2022-07-10 16:25:59 - train: epoch 0051, iter [04700, 05004], lr: 0.061750, loss: 1.8917
2022-07-10 16:26:33 - train: epoch 0051, iter [04800, 05004], lr: 0.061724, loss: 2.0012
2022-07-10 16:27:07 - train: epoch 0051, iter [04900, 05004], lr: 0.061699, loss: 2.0562
2022-07-10 16:27:40 - train: epoch 0051, iter [05000, 05004], lr: 0.061673, loss: 1.8620
2022-07-10 16:27:41 - train: epoch 051, train_loss: 1.9249
2022-07-10 16:28:55 - eval: epoch: 051, acc1: 60.232%, acc5: 83.572%, test_loss: 1.6698, per_image_load_time: 2.020ms, per_image_inference_time: 0.349ms
2022-07-10 16:28:55 - until epoch: 051, best_acc1: 60.232%
2022-07-10 16:28:55 - epoch 052 lr: 0.061672
2022-07-10 16:29:35 - train: epoch 0052, iter [00100, 05004], lr: 0.061647, loss: 1.9054
2022-07-10 16:30:09 - train: epoch 0052, iter [00200, 05004], lr: 0.061621, loss: 2.0408
2022-07-10 16:30:41 - train: epoch 0052, iter [00300, 05004], lr: 0.061596, loss: 1.9083
2022-07-10 16:31:14 - train: epoch 0052, iter [00400, 05004], lr: 0.061570, loss: 2.0091
2022-07-10 16:31:49 - train: epoch 0052, iter [00500, 05004], lr: 0.061545, loss: 1.8755
2022-07-10 16:32:23 - train: epoch 0052, iter [00600, 05004], lr: 0.061520, loss: 1.8419
2022-07-10 16:32:56 - train: epoch 0052, iter [00700, 05004], lr: 0.061494, loss: 2.0123
2022-07-10 16:33:29 - train: epoch 0052, iter [00800, 05004], lr: 0.061469, loss: 1.8511
2022-07-10 16:34:04 - train: epoch 0052, iter [00900, 05004], lr: 0.061443, loss: 1.9183
2022-07-10 16:34:37 - train: epoch 0052, iter [01000, 05004], lr: 0.061418, loss: 2.0390
2022-07-10 16:35:10 - train: epoch 0052, iter [01100, 05004], lr: 0.061392, loss: 1.9460
2022-07-10 16:35:45 - train: epoch 0052, iter [01200, 05004], lr: 0.061367, loss: 1.8195
2022-07-10 16:36:19 - train: epoch 0052, iter [01300, 05004], lr: 0.061341, loss: 1.8598
2022-07-10 16:36:53 - train: epoch 0052, iter [01400, 05004], lr: 0.061316, loss: 2.2331
2022-07-10 16:37:27 - train: epoch 0052, iter [01500, 05004], lr: 0.061290, loss: 1.6799
2022-07-10 16:38:02 - train: epoch 0052, iter [01600, 05004], lr: 0.061265, loss: 1.8525
2022-07-10 16:38:36 - train: epoch 0052, iter [01700, 05004], lr: 0.061239, loss: 1.6749
2022-07-10 16:39:10 - train: epoch 0052, iter [01800, 05004], lr: 0.061214, loss: 1.8091
2022-07-10 16:39:44 - train: epoch 0052, iter [01900, 05004], lr: 0.061188, loss: 1.7076
2022-07-10 16:40:18 - train: epoch 0052, iter [02000, 05004], lr: 0.061163, loss: 1.9775
2022-07-10 16:40:51 - train: epoch 0052, iter [02100, 05004], lr: 0.061137, loss: 2.0684
2022-07-10 16:41:26 - train: epoch 0052, iter [02200, 05004], lr: 0.061112, loss: 2.0948
2022-07-10 16:42:00 - train: epoch 0052, iter [02300, 05004], lr: 0.061086, loss: 1.7725
2022-07-10 16:42:34 - train: epoch 0052, iter [02400, 05004], lr: 0.061061, loss: 1.7779
2022-07-10 16:43:09 - train: epoch 0052, iter [02500, 05004], lr: 0.061035, loss: 1.9534
2022-07-10 16:43:43 - train: epoch 0052, iter [02600, 05004], lr: 0.061010, loss: 1.8213
2022-07-10 16:44:16 - train: epoch 0052, iter [02700, 05004], lr: 0.060984, loss: 1.8511
2022-07-10 16:44:51 - train: epoch 0052, iter [02800, 05004], lr: 0.060959, loss: 2.1384
2022-07-10 16:45:25 - train: epoch 0052, iter [02900, 05004], lr: 0.060933, loss: 1.7406
2022-07-10 16:45:58 - train: epoch 0052, iter [03000, 05004], lr: 0.060908, loss: 1.9026
2022-07-10 16:46:32 - train: epoch 0052, iter [03100, 05004], lr: 0.060882, loss: 1.9594
2022-07-10 16:47:07 - train: epoch 0052, iter [03200, 05004], lr: 0.060857, loss: 1.9288
2022-07-10 16:47:40 - train: epoch 0052, iter [03300, 05004], lr: 0.060831, loss: 1.9908
2022-07-10 16:48:14 - train: epoch 0052, iter [03400, 05004], lr: 0.060806, loss: 2.3793
2022-07-10 16:48:49 - train: epoch 0052, iter [03500, 05004], lr: 0.060780, loss: 2.0604
2022-07-10 16:49:22 - train: epoch 0052, iter [03600, 05004], lr: 0.060755, loss: 2.1707
2022-07-10 16:49:56 - train: epoch 0052, iter [03700, 05004], lr: 0.060729, loss: 1.9232
2022-07-10 16:50:30 - train: epoch 0052, iter [03800, 05004], lr: 0.060703, loss: 1.8883
2022-07-10 16:51:03 - train: epoch 0052, iter [03900, 05004], lr: 0.060678, loss: 1.7522
2022-07-10 16:51:37 - train: epoch 0052, iter [04000, 05004], lr: 0.060652, loss: 2.2567
2022-07-10 16:52:11 - train: epoch 0052, iter [04100, 05004], lr: 0.060627, loss: 1.5682
2022-07-10 16:52:45 - train: epoch 0052, iter [04200, 05004], lr: 0.060601, loss: 2.0478
2022-07-10 16:53:18 - train: epoch 0052, iter [04300, 05004], lr: 0.060576, loss: 1.8779
2022-07-10 16:53:53 - train: epoch 0052, iter [04400, 05004], lr: 0.060550, loss: 2.0530
2022-07-10 16:54:26 - train: epoch 0052, iter [04500, 05004], lr: 0.060525, loss: 1.8436
2022-07-10 16:55:00 - train: epoch 0052, iter [04600, 05004], lr: 0.060499, loss: 1.9094
2022-07-10 16:55:35 - train: epoch 0052, iter [04700, 05004], lr: 0.060473, loss: 2.0728
2022-07-10 16:56:10 - train: epoch 0052, iter [04800, 05004], lr: 0.060448, loss: 1.7345
2022-07-10 16:56:44 - train: epoch 0052, iter [04900, 05004], lr: 0.060422, loss: 1.9016
2022-07-10 16:57:17 - train: epoch 0052, iter [05000, 05004], lr: 0.060397, loss: 1.8781
2022-07-10 16:57:18 - train: epoch 052, train_loss: 1.9162
2022-07-10 16:58:33 - eval: epoch: 052, acc1: 60.516%, acc5: 83.910%, test_loss: 1.6496, per_image_load_time: 2.559ms, per_image_inference_time: 0.381ms
2022-07-10 16:58:33 - until epoch: 052, best_acc1: 60.516%
2022-07-10 16:58:33 - epoch 053 lr: 0.060395
2022-07-10 16:59:12 - train: epoch 0053, iter [00100, 05004], lr: 0.060370, loss: 1.9764
2022-07-10 16:59:47 - train: epoch 0053, iter [00200, 05004], lr: 0.060344, loss: 1.8338
2022-07-10 17:00:19 - train: epoch 0053, iter [00300, 05004], lr: 0.060319, loss: 1.7908
2022-07-10 17:00:55 - train: epoch 0053, iter [00400, 05004], lr: 0.060293, loss: 2.1329
2022-07-10 17:01:28 - train: epoch 0053, iter [00500, 05004], lr: 0.060268, loss: 1.9442
2022-07-10 17:02:02 - train: epoch 0053, iter [00600, 05004], lr: 0.060242, loss: 1.8797
2022-07-10 17:02:36 - train: epoch 0053, iter [00700, 05004], lr: 0.060216, loss: 1.7132
2022-07-10 17:03:11 - train: epoch 0053, iter [00800, 05004], lr: 0.060191, loss: 1.8458
2022-07-10 17:03:45 - train: epoch 0053, iter [00900, 05004], lr: 0.060165, loss: 1.8236
2022-07-10 17:04:19 - train: epoch 0053, iter [01000, 05004], lr: 0.060140, loss: 1.8738
2022-07-10 17:04:52 - train: epoch 0053, iter [01100, 05004], lr: 0.060114, loss: 1.9852
2022-07-10 17:05:27 - train: epoch 0053, iter [01200, 05004], lr: 0.060088, loss: 1.7807
2022-07-10 17:06:01 - train: epoch 0053, iter [01300, 05004], lr: 0.060063, loss: 1.8146
2022-07-10 17:06:36 - train: epoch 0053, iter [01400, 05004], lr: 0.060037, loss: 2.0705
2022-07-10 17:07:11 - train: epoch 0053, iter [01500, 05004], lr: 0.060011, loss: 1.9675
2022-07-10 17:07:45 - train: epoch 0053, iter [01600, 05004], lr: 0.059986, loss: 2.1200
2022-07-10 17:08:20 - train: epoch 0053, iter [01700, 05004], lr: 0.059960, loss: 2.3144
2022-07-10 17:08:53 - train: epoch 0053, iter [01800, 05004], lr: 0.059935, loss: 1.8591
2022-07-10 17:09:28 - train: epoch 0053, iter [01900, 05004], lr: 0.059909, loss: 1.8983
2022-07-10 17:10:02 - train: epoch 0053, iter [02000, 05004], lr: 0.059883, loss: 2.0292
2022-07-10 17:10:36 - train: epoch 0053, iter [02100, 05004], lr: 0.059858, loss: 2.0965
2022-07-10 17:11:11 - train: epoch 0053, iter [02200, 05004], lr: 0.059832, loss: 1.9254
2022-07-10 17:11:45 - train: epoch 0053, iter [02300, 05004], lr: 0.059806, loss: 1.8768
2022-07-10 17:12:19 - train: epoch 0053, iter [02400, 05004], lr: 0.059781, loss: 1.7680
2022-07-10 17:12:54 - train: epoch 0053, iter [02500, 05004], lr: 0.059755, loss: 2.0926
2022-07-10 17:13:27 - train: epoch 0053, iter [02600, 05004], lr: 0.059729, loss: 2.1632
2022-07-10 17:14:02 - train: epoch 0053, iter [02700, 05004], lr: 0.059704, loss: 2.0171
2022-07-10 17:14:35 - train: epoch 0053, iter [02800, 05004], lr: 0.059678, loss: 1.8916
2022-07-10 17:15:09 - train: epoch 0053, iter [02900, 05004], lr: 0.059652, loss: 1.7719
2022-07-10 17:15:44 - train: epoch 0053, iter [03000, 05004], lr: 0.059627, loss: 1.6605
2022-07-10 17:16:18 - train: epoch 0053, iter [03100, 05004], lr: 0.059601, loss: 2.0358
2022-07-10 17:16:52 - train: epoch 0053, iter [03200, 05004], lr: 0.059575, loss: 1.9100
2022-07-10 17:17:26 - train: epoch 0053, iter [03300, 05004], lr: 0.059550, loss: 1.9611
2022-07-10 17:18:00 - train: epoch 0053, iter [03400, 05004], lr: 0.059524, loss: 1.9614
2022-07-10 17:18:34 - train: epoch 0053, iter [03500, 05004], lr: 0.059498, loss: 1.9936
2022-07-10 17:19:09 - train: epoch 0053, iter [03600, 05004], lr: 0.059473, loss: 1.8862
2022-07-10 17:19:43 - train: epoch 0053, iter [03700, 05004], lr: 0.059447, loss: 2.1126
2022-07-10 17:20:18 - train: epoch 0053, iter [03800, 05004], lr: 0.059421, loss: 1.8849
2022-07-10 17:20:52 - train: epoch 0053, iter [03900, 05004], lr: 0.059396, loss: 2.1086
2022-07-10 17:21:27 - train: epoch 0053, iter [04000, 05004], lr: 0.059370, loss: 1.9918
2022-07-10 17:22:01 - train: epoch 0053, iter [04100, 05004], lr: 0.059344, loss: 1.8993
2022-07-10 17:22:36 - train: epoch 0053, iter [04200, 05004], lr: 0.059318, loss: 1.9680
2022-07-10 17:23:10 - train: epoch 0053, iter [04300, 05004], lr: 0.059293, loss: 1.9266
2022-07-10 17:23:44 - train: epoch 0053, iter [04400, 05004], lr: 0.059267, loss: 2.0068
2022-07-10 17:24:18 - train: epoch 0053, iter [04500, 05004], lr: 0.059241, loss: 2.0948
2022-07-10 17:24:54 - train: epoch 0053, iter [04600, 05004], lr: 0.059216, loss: 1.7794
2022-07-10 17:25:28 - train: epoch 0053, iter [04700, 05004], lr: 0.059190, loss: 1.9857
2022-07-10 17:26:03 - train: epoch 0053, iter [04800, 05004], lr: 0.059164, loss: 2.1541
2022-07-10 17:26:36 - train: epoch 0053, iter [04900, 05004], lr: 0.059139, loss: 1.9522
2022-07-10 17:27:09 - train: epoch 0053, iter [05000, 05004], lr: 0.059113, loss: 1.8756
2022-07-10 17:27:10 - train: epoch 053, train_loss: 1.9057
2022-07-10 17:28:25 - eval: epoch: 053, acc1: 60.826%, acc5: 83.982%, test_loss: 1.6404, per_image_load_time: 2.495ms, per_image_inference_time: 0.350ms
2022-07-10 17:28:25 - until epoch: 053, best_acc1: 60.826%
2022-07-10 17:28:25 - epoch 054 lr: 0.059112
2022-07-10 17:29:05 - train: epoch 0054, iter [00100, 05004], lr: 0.059086, loss: 1.7253
2022-07-10 17:29:39 - train: epoch 0054, iter [00200, 05004], lr: 0.059060, loss: 2.1542
2022-07-10 17:30:12 - train: epoch 0054, iter [00300, 05004], lr: 0.059035, loss: 1.7366
2022-07-10 17:30:45 - train: epoch 0054, iter [00400, 05004], lr: 0.059009, loss: 1.5378
2022-07-10 17:31:19 - train: epoch 0054, iter [00500, 05004], lr: 0.058983, loss: 1.8295
2022-07-10 17:31:53 - train: epoch 0054, iter [00600, 05004], lr: 0.058957, loss: 1.8131
2022-07-10 17:32:28 - train: epoch 0054, iter [00700, 05004], lr: 0.058932, loss: 1.9190
2022-07-10 17:33:02 - train: epoch 0054, iter [00800, 05004], lr: 0.058906, loss: 1.8857
2022-07-10 17:33:36 - train: epoch 0054, iter [00900, 05004], lr: 0.058880, loss: 1.6685
2022-07-10 17:34:09 - train: epoch 0054, iter [01000, 05004], lr: 0.058854, loss: 1.8056
2022-07-10 17:34:43 - train: epoch 0054, iter [01100, 05004], lr: 0.058829, loss: 1.8132
2022-07-10 17:35:18 - train: epoch 0054, iter [01200, 05004], lr: 0.058803, loss: 1.9943
2022-07-10 17:35:51 - train: epoch 0054, iter [01300, 05004], lr: 0.058777, loss: 1.7810
2022-07-10 17:36:26 - train: epoch 0054, iter [01400, 05004], lr: 0.058751, loss: 2.0391
2022-07-10 17:37:00 - train: epoch 0054, iter [01500, 05004], lr: 0.058726, loss: 1.8021
2022-07-10 17:37:34 - train: epoch 0054, iter [01600, 05004], lr: 0.058700, loss: 1.6651
2022-07-10 17:38:08 - train: epoch 0054, iter [01700, 05004], lr: 0.058674, loss: 1.8552
2022-07-10 17:38:42 - train: epoch 0054, iter [01800, 05004], lr: 0.058648, loss: 1.7358
2022-07-10 17:39:16 - train: epoch 0054, iter [01900, 05004], lr: 0.058623, loss: 2.0183
2022-07-10 17:39:51 - train: epoch 0054, iter [02000, 05004], lr: 0.058597, loss: 2.0029
2022-07-10 17:40:25 - train: epoch 0054, iter [02100, 05004], lr: 0.058571, loss: 1.8101
2022-07-10 17:40:59 - train: epoch 0054, iter [02200, 05004], lr: 0.058545, loss: 1.9697
2022-07-10 17:41:33 - train: epoch 0054, iter [02300, 05004], lr: 0.058520, loss: 1.6628
2022-07-10 17:42:07 - train: epoch 0054, iter [02400, 05004], lr: 0.058494, loss: 1.9101
2022-07-10 17:42:41 - train: epoch 0054, iter [02500, 05004], lr: 0.058468, loss: 1.9983
2022-07-10 17:43:13 - train: epoch 0054, iter [02600, 05004], lr: 0.058442, loss: 1.6751
2022-07-10 17:43:49 - train: epoch 0054, iter [02700, 05004], lr: 0.058416, loss: 1.8693
2022-07-10 17:44:22 - train: epoch 0054, iter [02800, 05004], lr: 0.058391, loss: 2.0882
2022-07-10 17:44:56 - train: epoch 0054, iter [02900, 05004], lr: 0.058365, loss: 1.8662
2022-07-10 17:45:30 - train: epoch 0054, iter [03000, 05004], lr: 0.058339, loss: 2.0689
2022-07-10 17:46:04 - train: epoch 0054, iter [03100, 05004], lr: 0.058313, loss: 1.9519
2022-07-10 17:46:38 - train: epoch 0054, iter [03200, 05004], lr: 0.058287, loss: 2.0328
2022-07-10 17:47:11 - train: epoch 0054, iter [03300, 05004], lr: 0.058262, loss: 1.9522
2022-07-10 17:47:46 - train: epoch 0054, iter [03400, 05004], lr: 0.058236, loss: 1.7445
2022-07-10 17:48:20 - train: epoch 0054, iter [03500, 05004], lr: 0.058210, loss: 1.8790
2022-07-10 17:48:54 - train: epoch 0054, iter [03600, 05004], lr: 0.058184, loss: 1.9350
2022-07-10 17:49:28 - train: epoch 0054, iter [03700, 05004], lr: 0.058158, loss: 1.9099
2022-07-10 17:50:02 - train: epoch 0054, iter [03800, 05004], lr: 0.058133, loss: 1.7906
2022-07-10 17:50:37 - train: epoch 0054, iter [03900, 05004], lr: 0.058107, loss: 1.8430
2022-07-10 17:51:10 - train: epoch 0054, iter [04000, 05004], lr: 0.058081, loss: 1.8763
2022-07-10 17:51:45 - train: epoch 0054, iter [04100, 05004], lr: 0.058055, loss: 2.0053
2022-07-10 17:52:18 - train: epoch 0054, iter [04200, 05004], lr: 0.058029, loss: 1.9864
2022-07-10 17:52:53 - train: epoch 0054, iter [04300, 05004], lr: 0.058004, loss: 1.9520
2022-07-10 17:53:27 - train: epoch 0054, iter [04400, 05004], lr: 0.057978, loss: 1.6880
2022-07-10 17:54:01 - train: epoch 0054, iter [04500, 05004], lr: 0.057952, loss: 1.8242
2022-07-10 17:54:35 - train: epoch 0054, iter [04600, 05004], lr: 0.057926, loss: 2.0119
2022-07-10 17:55:09 - train: epoch 0054, iter [04700, 05004], lr: 0.057900, loss: 2.1115
2022-07-10 17:55:43 - train: epoch 0054, iter [04800, 05004], lr: 0.057874, loss: 1.9815
2022-07-10 17:56:18 - train: epoch 0054, iter [04900, 05004], lr: 0.057849, loss: 2.0084
2022-07-10 17:56:49 - train: epoch 0054, iter [05000, 05004], lr: 0.057823, loss: 2.0117
2022-07-10 17:56:50 - train: epoch 054, train_loss: 1.8987
2022-07-10 17:58:06 - eval: epoch: 054, acc1: 58.922%, acc5: 82.464%, test_loss: 1.7309, per_image_load_time: 2.521ms, per_image_inference_time: 0.356ms
2022-07-10 17:58:06 - until epoch: 054, best_acc1: 60.826%
2022-07-10 17:58:06 - epoch 055 lr: 0.057821
2022-07-10 17:58:45 - train: epoch 0055, iter [00100, 05004], lr: 0.057796, loss: 1.9202
2022-07-10 17:59:19 - train: epoch 0055, iter [00200, 05004], lr: 0.057770, loss: 1.9823
2022-07-10 17:59:54 - train: epoch 0055, iter [00300, 05004], lr: 0.057744, loss: 1.7552
2022-07-10 18:00:28 - train: epoch 0055, iter [00400, 05004], lr: 0.057718, loss: 1.8548
2022-07-10 18:01:01 - train: epoch 0055, iter [00500, 05004], lr: 0.057693, loss: 1.7307
2022-07-10 18:01:35 - train: epoch 0055, iter [00600, 05004], lr: 0.057667, loss: 1.9649
2022-07-10 18:02:09 - train: epoch 0055, iter [00700, 05004], lr: 0.057641, loss: 1.8610
2022-07-10 18:02:43 - train: epoch 0055, iter [00800, 05004], lr: 0.057615, loss: 1.7134
2022-07-10 18:03:18 - train: epoch 0055, iter [00900, 05004], lr: 0.057589, loss: 1.8952
2022-07-10 18:03:52 - train: epoch 0055, iter [01000, 05004], lr: 0.057563, loss: 1.8148
2022-07-10 18:04:26 - train: epoch 0055, iter [01100, 05004], lr: 0.057537, loss: 1.9414
2022-07-10 18:05:00 - train: epoch 0055, iter [01200, 05004], lr: 0.057512, loss: 1.8435
2022-07-10 18:05:34 - train: epoch 0055, iter [01300, 05004], lr: 0.057486, loss: 1.8648
2022-07-10 18:06:08 - train: epoch 0055, iter [01400, 05004], lr: 0.057460, loss: 1.9844
2022-07-10 18:06:43 - train: epoch 0055, iter [01500, 05004], lr: 0.057434, loss: 1.9203
2022-07-10 18:07:17 - train: epoch 0055, iter [01600, 05004], lr: 0.057408, loss: 1.8196
2022-07-10 18:07:51 - train: epoch 0055, iter [01700, 05004], lr: 0.057382, loss: 2.0548
2022-07-10 18:08:26 - train: epoch 0055, iter [01800, 05004], lr: 0.057356, loss: 1.8406
2022-07-10 18:09:00 - train: epoch 0055, iter [01900, 05004], lr: 0.057330, loss: 2.0271
2022-07-10 18:09:35 - train: epoch 0055, iter [02000, 05004], lr: 0.057305, loss: 2.0743
2022-07-10 18:10:08 - train: epoch 0055, iter [02100, 05004], lr: 0.057279, loss: 1.6550
2022-07-10 18:10:43 - train: epoch 0055, iter [02200, 05004], lr: 0.057253, loss: 1.9699
2022-07-10 18:11:17 - train: epoch 0055, iter [02300, 05004], lr: 0.057227, loss: 1.7857
2022-07-10 18:11:51 - train: epoch 0055, iter [02400, 05004], lr: 0.057201, loss: 1.7814
2022-07-10 18:12:26 - train: epoch 0055, iter [02500, 05004], lr: 0.057175, loss: 1.8765
2022-07-10 18:13:00 - train: epoch 0055, iter [02600, 05004], lr: 0.057149, loss: 1.8391
2022-07-10 18:13:34 - train: epoch 0055, iter [02700, 05004], lr: 0.057123, loss: 2.0174
2022-07-10 18:14:09 - train: epoch 0055, iter [02800, 05004], lr: 0.057097, loss: 1.9514
2022-07-10 18:14:42 - train: epoch 0055, iter [02900, 05004], lr: 0.057072, loss: 1.7895
2022-07-10 18:15:17 - train: epoch 0055, iter [03000, 05004], lr: 0.057046, loss: 2.0071
2022-07-10 18:15:51 - train: epoch 0055, iter [03100, 05004], lr: 0.057020, loss: 2.0526
2022-07-10 18:16:26 - train: epoch 0055, iter [03200, 05004], lr: 0.056994, loss: 1.9164
2022-07-10 18:16:59 - train: epoch 0055, iter [03300, 05004], lr: 0.056968, loss: 1.7093
2022-07-10 18:17:34 - train: epoch 0055, iter [03400, 05004], lr: 0.056942, loss: 1.8427
2022-07-10 18:18:08 - train: epoch 0055, iter [03500, 05004], lr: 0.056916, loss: 1.8313
2022-07-10 18:18:43 - train: epoch 0055, iter [03600, 05004], lr: 0.056890, loss: 1.9082
2022-07-10 18:19:17 - train: epoch 0055, iter [03700, 05004], lr: 0.056864, loss: 1.7496
2022-07-10 18:19:51 - train: epoch 0055, iter [03800, 05004], lr: 0.056838, loss: 1.9702
2022-07-10 18:20:26 - train: epoch 0055, iter [03900, 05004], lr: 0.056813, loss: 2.1520
2022-07-10 18:21:00 - train: epoch 0055, iter [04000, 05004], lr: 0.056787, loss: 1.7906
2022-07-10 18:21:35 - train: epoch 0055, iter [04100, 05004], lr: 0.056761, loss: 1.7810
2022-07-10 18:22:08 - train: epoch 0055, iter [04200, 05004], lr: 0.056735, loss: 2.1387
2022-07-10 18:22:43 - train: epoch 0055, iter [04300, 05004], lr: 0.056709, loss: 1.9391
2022-07-10 18:23:18 - train: epoch 0055, iter [04400, 05004], lr: 0.056683, loss: 2.0011
2022-07-10 18:23:52 - train: epoch 0055, iter [04500, 05004], lr: 0.056657, loss: 1.9033
2022-07-10 18:24:27 - train: epoch 0055, iter [04600, 05004], lr: 0.056631, loss: 1.8093
2022-07-10 18:25:02 - train: epoch 0055, iter [04700, 05004], lr: 0.056605, loss: 1.8900
2022-07-10 18:25:36 - train: epoch 0055, iter [04800, 05004], lr: 0.056579, loss: 1.7969
2022-07-10 18:26:11 - train: epoch 0055, iter [04900, 05004], lr: 0.056553, loss: 1.9145
2022-07-10 18:26:44 - train: epoch 0055, iter [05000, 05004], lr: 0.056527, loss: 1.9593
2022-07-10 18:26:45 - train: epoch 055, train_loss: 1.8870
2022-07-10 18:28:00 - eval: epoch: 055, acc1: 61.004%, acc5: 84.072%, test_loss: 1.6254, per_image_load_time: 2.273ms, per_image_inference_time: 0.366ms
2022-07-10 18:28:00 - until epoch: 055, best_acc1: 61.004%
2022-07-10 18:28:00 - epoch 056 lr: 0.056526
2022-07-10 18:28:39 - train: epoch 0056, iter [00100, 05004], lr: 0.056500, loss: 1.8015
2022-07-10 18:29:13 - train: epoch 0056, iter [00200, 05004], lr: 0.056474, loss: 1.9851
2022-07-10 18:29:48 - train: epoch 0056, iter [00300, 05004], lr: 0.056448, loss: 1.8609
2022-07-10 18:30:22 - train: epoch 0056, iter [00400, 05004], lr: 0.056423, loss: 1.6712
2022-07-10 18:30:56 - train: epoch 0056, iter [00500, 05004], lr: 0.056397, loss: 1.8873
2022-07-10 18:31:31 - train: epoch 0056, iter [00600, 05004], lr: 0.056371, loss: 1.6939
2022-07-10 18:32:05 - train: epoch 0056, iter [00700, 05004], lr: 0.056345, loss: 1.8204
2022-07-10 18:32:39 - train: epoch 0056, iter [00800, 05004], lr: 0.056319, loss: 2.0645
2022-07-10 18:33:13 - train: epoch 0056, iter [00900, 05004], lr: 0.056293, loss: 1.9813
2022-07-10 18:33:48 - train: epoch 0056, iter [01000, 05004], lr: 0.056267, loss: 1.7987
2022-07-10 18:34:22 - train: epoch 0056, iter [01100, 05004], lr: 0.056241, loss: 1.5643
2022-07-10 18:34:57 - train: epoch 0056, iter [01200, 05004], lr: 0.056215, loss: 1.8424
2022-07-10 18:35:31 - train: epoch 0056, iter [01300, 05004], lr: 0.056189, loss: 1.9949
2022-07-10 18:36:05 - train: epoch 0056, iter [01400, 05004], lr: 0.056163, loss: 1.8772
2022-07-10 18:36:40 - train: epoch 0056, iter [01500, 05004], lr: 0.056137, loss: 1.9521
2022-07-10 18:37:14 - train: epoch 0056, iter [01600, 05004], lr: 0.056111, loss: 1.8797
2022-07-10 18:37:48 - train: epoch 0056, iter [01700, 05004], lr: 0.056085, loss: 2.0762
2022-07-10 18:38:23 - train: epoch 0056, iter [01800, 05004], lr: 0.056059, loss: 1.9748
2022-07-10 18:38:57 - train: epoch 0056, iter [01900, 05004], lr: 0.056033, loss: 2.1479
2022-07-10 18:39:32 - train: epoch 0056, iter [02000, 05004], lr: 0.056007, loss: 1.7471
2022-07-10 18:40:07 - train: epoch 0056, iter [02100, 05004], lr: 0.055981, loss: 1.8287
2022-07-10 18:40:41 - train: epoch 0056, iter [02200, 05004], lr: 0.055955, loss: 2.1099
2022-07-10 18:41:16 - train: epoch 0056, iter [02300, 05004], lr: 0.055929, loss: 1.7674
2022-07-10 18:41:50 - train: epoch 0056, iter [02400, 05004], lr: 0.055903, loss: 1.8961
2022-07-10 18:42:25 - train: epoch 0056, iter [02500, 05004], lr: 0.055877, loss: 2.2591
2022-07-10 18:43:00 - train: epoch 0056, iter [02600, 05004], lr: 0.055851, loss: 1.8537
2022-07-10 18:43:35 - train: epoch 0056, iter [02700, 05004], lr: 0.055825, loss: 1.8405
2022-07-10 18:44:09 - train: epoch 0056, iter [02800, 05004], lr: 0.055799, loss: 1.7696
2022-07-10 18:44:43 - train: epoch 0056, iter [02900, 05004], lr: 0.055773, loss: 1.8402
2022-07-10 18:45:19 - train: epoch 0056, iter [03000, 05004], lr: 0.055747, loss: 2.1582
2022-07-10 18:45:52 - train: epoch 0056, iter [03100, 05004], lr: 0.055721, loss: 1.6924
2022-07-10 18:46:27 - train: epoch 0056, iter [03200, 05004], lr: 0.055696, loss: 1.7398
2022-07-10 18:47:02 - train: epoch 0056, iter [03300, 05004], lr: 0.055670, loss: 1.9221
2022-07-10 18:47:36 - train: epoch 0056, iter [03400, 05004], lr: 0.055644, loss: 1.8841
2022-07-10 18:48:10 - train: epoch 0056, iter [03500, 05004], lr: 0.055618, loss: 2.0005
2022-07-10 18:48:46 - train: epoch 0056, iter [03600, 05004], lr: 0.055592, loss: 1.5711
2022-07-10 18:49:20 - train: epoch 0056, iter [03700, 05004], lr: 0.055566, loss: 1.9388
2022-07-10 18:49:55 - train: epoch 0056, iter [03800, 05004], lr: 0.055540, loss: 1.9318
2022-07-10 18:50:29 - train: epoch 0056, iter [03900, 05004], lr: 0.055514, loss: 2.1066
2022-07-10 18:51:03 - train: epoch 0056, iter [04000, 05004], lr: 0.055488, loss: 1.8841
2022-07-10 18:51:37 - train: epoch 0056, iter [04100, 05004], lr: 0.055462, loss: 1.9701
2022-07-10 18:52:12 - train: epoch 0056, iter [04200, 05004], lr: 0.055436, loss: 1.9850
2022-07-10 18:52:47 - train: epoch 0056, iter [04300, 05004], lr: 0.055410, loss: 1.9082
2022-07-10 18:53:21 - train: epoch 0056, iter [04400, 05004], lr: 0.055384, loss: 1.9293
2022-07-10 18:53:56 - train: epoch 0056, iter [04500, 05004], lr: 0.055358, loss: 1.9014
2022-07-10 18:54:30 - train: epoch 0056, iter [04600, 05004], lr: 0.055332, loss: 1.7598
2022-07-10 18:55:04 - train: epoch 0056, iter [04700, 05004], lr: 0.055306, loss: 1.8527
2022-07-10 18:55:39 - train: epoch 0056, iter [04800, 05004], lr: 0.055279, loss: 1.9650
2022-07-10 18:56:14 - train: epoch 0056, iter [04900, 05004], lr: 0.055253, loss: 1.9166
2022-07-10 18:56:47 - train: epoch 0056, iter [05000, 05004], lr: 0.055227, loss: 1.9781
2022-07-10 18:56:48 - train: epoch 056, train_loss: 1.8777
2022-07-10 18:58:04 - eval: epoch: 056, acc1: 61.002%, acc5: 84.252%, test_loss: 1.6304, per_image_load_time: 2.597ms, per_image_inference_time: 0.339ms
2022-07-10 18:58:04 - until epoch: 056, best_acc1: 61.004%
2022-07-10 18:58:04 - epoch 057 lr: 0.055226
2022-07-10 18:58:44 - train: epoch 0057, iter [00100, 05004], lr: 0.055200, loss: 1.8476
2022-07-10 18:59:17 - train: epoch 0057, iter [00200, 05004], lr: 0.055174, loss: 1.8639
2022-07-10 18:59:52 - train: epoch 0057, iter [00300, 05004], lr: 0.055148, loss: 1.7578
2022-07-10 19:00:26 - train: epoch 0057, iter [00400, 05004], lr: 0.055122, loss: 1.6487
2022-07-10 19:01:01 - train: epoch 0057, iter [00500, 05004], lr: 0.055096, loss: 1.8420
2022-07-10 19:01:35 - train: epoch 0057, iter [00600, 05004], lr: 0.055070, loss: 1.8651
2022-07-10 19:02:10 - train: epoch 0057, iter [00700, 05004], lr: 0.055044, loss: 1.6651
2022-07-10 19:02:44 - train: epoch 0057, iter [00800, 05004], lr: 0.055018, loss: 1.8281
2022-07-10 19:03:19 - train: epoch 0057, iter [00900, 05004], lr: 0.054992, loss: 1.6441
2022-07-10 19:03:53 - train: epoch 0057, iter [01000, 05004], lr: 0.054966, loss: 1.7106
2022-07-10 19:04:27 - train: epoch 0057, iter [01100, 05004], lr: 0.054940, loss: 1.6695
2022-07-10 19:05:02 - train: epoch 0057, iter [01200, 05004], lr: 0.054914, loss: 1.9106
2022-07-10 19:05:36 - train: epoch 0057, iter [01300, 05004], lr: 0.054888, loss: 2.0008
2022-07-10 19:06:10 - train: epoch 0057, iter [01400, 05004], lr: 0.054862, loss: 1.9838
2022-07-10 19:06:43 - train: epoch 0057, iter [01500, 05004], lr: 0.054836, loss: 1.9739
2022-07-10 19:07:18 - train: epoch 0057, iter [01600, 05004], lr: 0.054810, loss: 1.9346
2022-07-10 19:07:53 - train: epoch 0057, iter [01700, 05004], lr: 0.054784, loss: 1.9278
2022-07-10 19:08:27 - train: epoch 0057, iter [01800, 05004], lr: 0.054758, loss: 1.9941
2022-07-10 19:09:01 - train: epoch 0057, iter [01900, 05004], lr: 0.054732, loss: 1.7729
2022-07-10 19:09:37 - train: epoch 0057, iter [02000, 05004], lr: 0.054706, loss: 1.9212
2022-07-10 19:10:10 - train: epoch 0057, iter [02100, 05004], lr: 0.054680, loss: 1.9136
2022-07-10 19:10:45 - train: epoch 0057, iter [02200, 05004], lr: 0.054654, loss: 1.7203
2022-07-10 19:11:20 - train: epoch 0057, iter [02300, 05004], lr: 0.054628, loss: 1.9781
2022-07-10 19:11:55 - train: epoch 0057, iter [02400, 05004], lr: 0.054602, loss: 1.8852
2022-07-10 19:12:28 - train: epoch 0057, iter [02500, 05004], lr: 0.054576, loss: 2.2577
2022-07-10 19:13:03 - train: epoch 0057, iter [02600, 05004], lr: 0.054550, loss: 1.6430
2022-07-10 19:13:37 - train: epoch 0057, iter [02700, 05004], lr: 0.054524, loss: 1.6380
2022-07-10 19:14:12 - train: epoch 0057, iter [02800, 05004], lr: 0.054497, loss: 1.6541
2022-07-10 19:14:47 - train: epoch 0057, iter [02900, 05004], lr: 0.054471, loss: 1.8504
2022-07-10 19:15:21 - train: epoch 0057, iter [03000, 05004], lr: 0.054445, loss: 2.1080
2022-07-10 19:15:57 - train: epoch 0057, iter [03100, 05004], lr: 0.054419, loss: 1.9959
2022-07-10 19:16:32 - train: epoch 0057, iter [03200, 05004], lr: 0.054393, loss: 1.8961
2022-07-10 19:17:06 - train: epoch 0057, iter [03300, 05004], lr: 0.054367, loss: 1.8803
2022-07-10 19:17:41 - train: epoch 0057, iter [03400, 05004], lr: 0.054341, loss: 2.0110
2022-07-10 19:18:16 - train: epoch 0057, iter [03500, 05004], lr: 0.054315, loss: 1.9596
2022-07-10 19:18:51 - train: epoch 0057, iter [03600, 05004], lr: 0.054289, loss: 1.8867
2022-07-10 19:19:25 - train: epoch 0057, iter [03700, 05004], lr: 0.054263, loss: 1.7746
2022-07-10 19:19:59 - train: epoch 0057, iter [03800, 05004], lr: 0.054237, loss: 1.8113
2022-07-10 19:20:34 - train: epoch 0057, iter [03900, 05004], lr: 0.054211, loss: 1.8665
2022-07-10 19:21:08 - train: epoch 0057, iter [04000, 05004], lr: 0.054185, loss: 1.7751
2022-07-10 19:21:43 - train: epoch 0057, iter [04100, 05004], lr: 0.054159, loss: 1.8306
2022-07-10 19:22:18 - train: epoch 0057, iter [04200, 05004], lr: 0.054133, loss: 1.7897
2022-07-10 19:22:52 - train: epoch 0057, iter [04300, 05004], lr: 0.054107, loss: 1.8185
2022-07-10 19:23:28 - train: epoch 0057, iter [04400, 05004], lr: 0.054080, loss: 1.8150
2022-07-10 19:24:02 - train: epoch 0057, iter [04500, 05004], lr: 0.054054, loss: 2.0112
2022-07-10 19:24:36 - train: epoch 0057, iter [04600, 05004], lr: 0.054028, loss: 1.9170
2022-07-10 19:25:10 - train: epoch 0057, iter [04700, 05004], lr: 0.054002, loss: 1.9853
2022-07-10 19:25:46 - train: epoch 0057, iter [04800, 05004], lr: 0.053976, loss: 2.2273
2022-07-10 19:26:19 - train: epoch 0057, iter [04900, 05004], lr: 0.053950, loss: 2.0329
2022-07-10 19:26:53 - train: epoch 0057, iter [05000, 05004], lr: 0.053924, loss: 2.0760
2022-07-10 19:26:54 - train: epoch 057, train_loss: 1.8676
2022-07-10 19:28:09 - eval: epoch: 057, acc1: 61.194%, acc5: 84.204%, test_loss: 1.6314, per_image_load_time: 2.559ms, per_image_inference_time: 0.339ms
2022-07-10 19:28:09 - until epoch: 057, best_acc1: 61.194%
2022-07-10 19:28:09 - epoch 058 lr: 0.053923
2022-07-10 19:28:48 - train: epoch 0058, iter [00100, 05004], lr: 0.053897, loss: 1.8981
2022-07-10 19:29:24 - train: epoch 0058, iter [00200, 05004], lr: 0.053871, loss: 1.7773
2022-07-10 19:29:58 - train: epoch 0058, iter [00300, 05004], lr: 0.053845, loss: 1.9170
2022-07-10 19:30:32 - train: epoch 0058, iter [00400, 05004], lr: 0.053819, loss: 1.9169
2022-07-10 19:31:06 - train: epoch 0058, iter [00500, 05004], lr: 0.053793, loss: 1.8594
2022-07-10 19:31:40 - train: epoch 0058, iter [00600, 05004], lr: 0.053766, loss: 1.8930
2022-07-10 19:32:15 - train: epoch 0058, iter [00700, 05004], lr: 0.053740, loss: 1.8703
2022-07-10 19:32:49 - train: epoch 0058, iter [00800, 05004], lr: 0.053714, loss: 1.9987
2022-07-10 19:33:23 - train: epoch 0058, iter [00900, 05004], lr: 0.053688, loss: 1.7638
2022-07-10 19:33:58 - train: epoch 0058, iter [01000, 05004], lr: 0.053662, loss: 1.9151
2022-07-10 19:34:32 - train: epoch 0058, iter [01100, 05004], lr: 0.053636, loss: 1.6283
2022-07-10 19:35:07 - train: epoch 0058, iter [01200, 05004], lr: 0.053610, loss: 1.8735
2022-07-10 19:35:42 - train: epoch 0058, iter [01300, 05004], lr: 0.053584, loss: 1.9848
2022-07-10 19:36:15 - train: epoch 0058, iter [01400, 05004], lr: 0.053558, loss: 1.6760
2022-07-10 19:36:50 - train: epoch 0058, iter [01500, 05004], lr: 0.053532, loss: 1.7571
2022-07-10 19:37:24 - train: epoch 0058, iter [01600, 05004], lr: 0.053506, loss: 1.7943
2022-07-10 19:37:58 - train: epoch 0058, iter [01700, 05004], lr: 0.053479, loss: 1.8785
2022-07-10 19:38:33 - train: epoch 0058, iter [01800, 05004], lr: 0.053453, loss: 2.1557
2022-07-10 19:39:07 - train: epoch 0058, iter [01900, 05004], lr: 0.053427, loss: 1.9856
2022-07-10 19:39:42 - train: epoch 0058, iter [02000, 05004], lr: 0.053401, loss: 2.0330
2022-07-10 19:40:17 - train: epoch 0058, iter [02100, 05004], lr: 0.053375, loss: 1.7808
2022-07-10 19:40:50 - train: epoch 0058, iter [02200, 05004], lr: 0.053349, loss: 1.5971
2022-07-10 19:41:25 - train: epoch 0058, iter [02300, 05004], lr: 0.053323, loss: 1.8458
2022-07-10 19:42:00 - train: epoch 0058, iter [02400, 05004], lr: 0.053297, loss: 1.9482
2022-07-10 19:42:34 - train: epoch 0058, iter [02500, 05004], lr: 0.053271, loss: 1.7714
2022-07-10 19:43:08 - train: epoch 0058, iter [02600, 05004], lr: 0.053245, loss: 1.7005
2022-07-10 19:43:43 - train: epoch 0058, iter [02700, 05004], lr: 0.053218, loss: 2.2361
2022-07-10 19:44:17 - train: epoch 0058, iter [02800, 05004], lr: 0.053192, loss: 1.7649
2022-07-10 19:44:51 - train: epoch 0058, iter [02900, 05004], lr: 0.053166, loss: 1.6918
2022-07-10 19:45:26 - train: epoch 0058, iter [03000, 05004], lr: 0.053140, loss: 1.7888
2022-07-10 19:46:00 - train: epoch 0058, iter [03100, 05004], lr: 0.053114, loss: 1.7400
2022-07-10 19:46:35 - train: epoch 0058, iter [03200, 05004], lr: 0.053088, loss: 1.5056
2022-07-10 19:47:10 - train: epoch 0058, iter [03300, 05004], lr: 0.053062, loss: 1.8830
2022-07-10 19:47:45 - train: epoch 0058, iter [03400, 05004], lr: 0.053036, loss: 1.7772
2022-07-10 19:48:19 - train: epoch 0058, iter [03500, 05004], lr: 0.053010, loss: 1.7161
2022-07-10 19:48:54 - train: epoch 0058, iter [03600, 05004], lr: 0.052983, loss: 1.8026
2022-07-10 19:49:28 - train: epoch 0058, iter [03700, 05004], lr: 0.052957, loss: 1.7432
2022-07-10 19:50:03 - train: epoch 0058, iter [03800, 05004], lr: 0.052931, loss: 1.8499
2022-07-10 19:50:38 - train: epoch 0058, iter [03900, 05004], lr: 0.052905, loss: 1.8493
2022-07-10 19:51:13 - train: epoch 0058, iter [04000, 05004], lr: 0.052879, loss: 1.8744
2022-07-10 19:51:47 - train: epoch 0058, iter [04100, 05004], lr: 0.052853, loss: 2.0349
2022-07-10 19:52:21 - train: epoch 0058, iter [04200, 05004], lr: 0.052827, loss: 1.6449
2022-07-10 19:52:55 - train: epoch 0058, iter [04300, 05004], lr: 0.052801, loss: 2.0201
2022-07-10 19:53:30 - train: epoch 0058, iter [04400, 05004], lr: 0.052775, loss: 1.7322
2022-07-10 19:54:05 - train: epoch 0058, iter [04500, 05004], lr: 0.052748, loss: 1.7352
2022-07-10 19:54:40 - train: epoch 0058, iter [04600, 05004], lr: 0.052722, loss: 1.7957
2022-07-10 19:55:14 - train: epoch 0058, iter [04700, 05004], lr: 0.052696, loss: 2.1130
2022-07-10 19:55:49 - train: epoch 0058, iter [04800, 05004], lr: 0.052670, loss: 1.8690
2022-07-10 19:56:23 - train: epoch 0058, iter [04900, 05004], lr: 0.052644, loss: 1.9731
2022-07-10 19:56:56 - train: epoch 0058, iter [05000, 05004], lr: 0.052618, loss: 1.7407
2022-07-10 19:56:57 - train: epoch 058, train_loss: 1.8580
2022-07-10 19:58:13 - eval: epoch: 058, acc1: 61.662%, acc5: 84.698%, test_loss: 1.5998, per_image_load_time: 2.564ms, per_image_inference_time: 0.366ms
2022-07-10 19:58:13 - until epoch: 058, best_acc1: 61.662%
2022-07-10 19:58:13 - epoch 059 lr: 0.052617
2022-07-10 19:58:53 - train: epoch 0059, iter [00100, 05004], lr: 0.052591, loss: 1.9130
2022-07-10 19:59:27 - train: epoch 0059, iter [00200, 05004], lr: 0.052565, loss: 1.7502
2022-07-10 20:00:00 - train: epoch 0059, iter [00300, 05004], lr: 0.052538, loss: 1.6382
2022-07-10 20:00:34 - train: epoch 0059, iter [00400, 05004], lr: 0.052512, loss: 1.9016
2022-07-10 20:01:09 - train: epoch 0059, iter [00500, 05004], lr: 0.052486, loss: 1.7414
2022-07-10 20:01:44 - train: epoch 0059, iter [00600, 05004], lr: 0.052460, loss: 1.7449
