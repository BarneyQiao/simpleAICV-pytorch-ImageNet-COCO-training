2022-07-14 21:40:50 - network: RepVGG_A0
2022-07-14 21:40:50 - num_classes: 1000
2022-07-14 21:40:50 - input_image_size: 224
2022-07-14 21:40:50 - scale: 1.1428571428571428
2022-07-14 21:40:50 - trained_model_path: 
2022-07-14 21:40:50 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-14 21:40:50 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-07-14 21:40:50 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fb8062cf460>
2022-07-14 21:40:50 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7fb8062cf730>
2022-07-14 21:40:50 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fb8062cf760>
2022-07-14 21:40:50 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7fb8062cf7c0>
2022-07-14 21:40:50 - seed: 0
2022-07-14 21:40:50 - batch_size: 256
2022-07-14 21:40:50 - num_workers: 16
2022-07-14 21:40:50 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-07-14 21:40:50 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-07-14 21:40:50 - epochs: 120
2022-07-14 21:40:50 - print_interval: 100
2022-07-14 21:40:50 - accumulation_steps: 1
2022-07-14 21:40:50 - sync_bn: False
2022-07-14 21:40:50 - apex: True
2022-07-14 21:40:50 - use_ema_model: False
2022-07-14 21:40:50 - ema_model_decay: 0.9999
2022-07-14 21:40:50 - gpus_type: NVIDIA RTX A5000
2022-07-14 21:40:50 - gpus_num: 2
2022-07-14 21:40:50 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fb7d23c2230>
2022-07-14 21:40:50 - --------------------parameters--------------------
2022-07-14 21:40:50 - name: stage0.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage0.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage1.0.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage1.0.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage1.1.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage1.1.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage1.1.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage1.1.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.0.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.0.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.1.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage2.1.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage2.1.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.1.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.2.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage2.2.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage2.2.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.2.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.3.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage2.3.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage2.3.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage2.3.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.0.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.0.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.1.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.1.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.1.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.1.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.2.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.2.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.2.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.2.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.3.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.3.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.3.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.3.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.4.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.4.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.4.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.4.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.5.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.5.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.5.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.5.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.6.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.6.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.6.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.6.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.7.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.7.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.7.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.7.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.8.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.8.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.8.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.8.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.9.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.9.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.9.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.9.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.10.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.10.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.10.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.10.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.11.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.11.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.11.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.11.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.12.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.12.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.12.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.12.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.13.identity.weight, grad: True
2022-07-14 21:40:50 - name: stage3.13.identity.bias, grad: True
2022-07-14 21:40:50 - name: stage3.13.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage3.13.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage4.0.conv3x3.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.bias, grad: True
2022-07-14 21:40:50 - name: stage4.0.conv1x1.conv.weight, grad: True
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.weight, grad: True
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.bias, grad: True
2022-07-14 21:40:50 - name: fc.weight, grad: True
2022-07-14 21:40:50 - name: fc.bias, grad: True
2022-07-14 21:40:50 - --------------------buffers--------------------
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage1.1.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage1.1.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage1.1.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.1.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.1.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.1.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.2.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.2.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.2.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.3.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.3.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.3.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.1.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.1.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.1.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.2.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.2.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.2.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.3.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.3.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.3.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.4.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.4.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.4.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.5.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.5.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.5.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.6.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.6.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.6.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.7.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.7.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.7.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.8.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.8.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.8.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.9.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.9.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.9.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.10.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.10.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.10.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.11.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.11.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.11.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.12.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.12.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.12.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.13.identity.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.13.identity.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.13.identity.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.running_mean, grad: False
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.running_var, grad: False
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.num_batches_tracked, grad: False
2022-07-14 21:40:50 - -----------no weight decay layers--------------
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage4.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage4.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-07-14 21:40:50 - -------------weight decay layers---------------
2022-07-14 21:40:50 - name: stage0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage1.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage2.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.6.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.7.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.8.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.9.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.10.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.11.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.12.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage3.13.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage4.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: stage4.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-07-14 21:40:50 - epoch 001 lr: 0.100000
2022-07-14 21:41:32 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8712
2022-07-14 21:42:05 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.6743
2022-07-14 21:42:39 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.6144
2022-07-14 21:43:12 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.5568
2022-07-14 21:43:46 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.4240
2022-07-14 21:44:19 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.2389
2022-07-14 21:44:52 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.2299
2022-07-14 21:45:27 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 6.1070
2022-07-14 21:46:00 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 6.0542
2022-07-14 21:46:34 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 6.0140
2022-07-14 21:47:07 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 5.9392
2022-07-14 21:47:39 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 5.8004
2022-07-14 21:48:13 - train: epoch 0001, iter [01300, 05004], lr: 0.099999, loss: 5.7176
2022-07-14 21:48:45 - train: epoch 0001, iter [01400, 05004], lr: 0.099999, loss: 5.5838
2022-07-14 21:49:19 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 5.5121
2022-07-14 21:49:53 - train: epoch 0001, iter [01600, 05004], lr: 0.099998, loss: 5.5171
2022-07-14 21:50:26 - train: epoch 0001, iter [01700, 05004], lr: 0.099998, loss: 5.3813
2022-07-14 21:50:59 - train: epoch 0001, iter [01800, 05004], lr: 0.099998, loss: 5.4222
2022-07-14 21:51:32 - train: epoch 0001, iter [01900, 05004], lr: 0.099998, loss: 5.3020
2022-07-14 21:52:05 - train: epoch 0001, iter [02000, 05004], lr: 0.099997, loss: 5.1768
2022-07-14 21:52:38 - train: epoch 0001, iter [02100, 05004], lr: 0.099997, loss: 5.1509
2022-07-14 21:53:11 - train: epoch 0001, iter [02200, 05004], lr: 0.099997, loss: 5.1219
2022-07-14 21:53:44 - train: epoch 0001, iter [02300, 05004], lr: 0.099996, loss: 4.9749
2022-07-14 21:54:17 - train: epoch 0001, iter [02400, 05004], lr: 0.099996, loss: 4.9230
2022-07-14 21:54:51 - train: epoch 0001, iter [02500, 05004], lr: 0.099996, loss: 5.1059
2022-07-14 21:55:24 - train: epoch 0001, iter [02600, 05004], lr: 0.099995, loss: 5.1532
2022-07-14 21:55:56 - train: epoch 0001, iter [02700, 05004], lr: 0.099995, loss: 5.0487
2022-07-14 21:56:30 - train: epoch 0001, iter [02800, 05004], lr: 0.099995, loss: 4.8322
2022-07-14 21:57:03 - train: epoch 0001, iter [02900, 05004], lr: 0.099994, loss: 4.6961
2022-07-14 21:57:37 - train: epoch 0001, iter [03000, 05004], lr: 0.099994, loss: 4.9012
2022-07-14 21:58:10 - train: epoch 0001, iter [03100, 05004], lr: 0.099993, loss: 4.9446
2022-07-14 21:58:43 - train: epoch 0001, iter [03200, 05004], lr: 0.099993, loss: 4.8624
2022-07-14 21:59:16 - train: epoch 0001, iter [03300, 05004], lr: 0.099993, loss: 4.5889
2022-07-14 21:59:49 - train: epoch 0001, iter [03400, 05004], lr: 0.099992, loss: 4.5474
2022-07-14 22:00:22 - train: epoch 0001, iter [03500, 05004], lr: 0.099992, loss: 4.6831
2022-07-14 22:00:55 - train: epoch 0001, iter [03600, 05004], lr: 0.099991, loss: 4.6569
2022-07-14 22:01:28 - train: epoch 0001, iter [03700, 05004], lr: 0.099991, loss: 4.7060
2022-07-14 22:02:01 - train: epoch 0001, iter [03800, 05004], lr: 0.099990, loss: 4.5206
2022-07-14 22:02:35 - train: epoch 0001, iter [03900, 05004], lr: 0.099990, loss: 4.5664
2022-07-14 22:03:09 - train: epoch 0001, iter [04000, 05004], lr: 0.099989, loss: 4.5075
2022-07-14 22:03:41 - train: epoch 0001, iter [04100, 05004], lr: 0.099988, loss: 4.6601
2022-07-14 22:04:15 - train: epoch 0001, iter [04200, 05004], lr: 0.099988, loss: 4.3850
2022-07-14 22:04:50 - train: epoch 0001, iter [04300, 05004], lr: 0.099987, loss: 4.4786
2022-07-14 22:05:23 - train: epoch 0001, iter [04400, 05004], lr: 0.099987, loss: 4.0839
2022-07-14 22:05:57 - train: epoch 0001, iter [04500, 05004], lr: 0.099986, loss: 4.4182
2022-07-14 22:06:30 - train: epoch 0001, iter [04600, 05004], lr: 0.099986, loss: 4.4586
2022-07-14 22:07:03 - train: epoch 0001, iter [04700, 05004], lr: 0.099985, loss: 4.2140
2022-07-14 22:07:37 - train: epoch 0001, iter [04800, 05004], lr: 0.099984, loss: 4.5100
2022-07-14 22:08:11 - train: epoch 0001, iter [04900, 05004], lr: 0.099984, loss: 4.1852
2022-07-14 22:08:43 - train: epoch 0001, iter [05000, 05004], lr: 0.099983, loss: 4.1453
2022-07-14 22:08:44 - train: epoch 001, train_loss: 5.1701
2022-07-14 22:09:58 - eval: epoch: 001, acc1: 18.990%, acc5: 40.942%, test_loss: 4.0934, per_image_load_time: 2.655ms, per_image_inference_time: 0.221ms
2022-07-14 22:09:58 - until epoch: 001, best_acc1: 18.990%
2022-07-14 22:09:58 - epoch 002 lr: 0.099983
2022-07-14 22:10:38 - train: epoch 0002, iter [00100, 05004], lr: 0.099982, loss: 4.3234
2022-07-14 22:11:10 - train: epoch 0002, iter [00200, 05004], lr: 0.099981, loss: 4.0439
2022-07-14 22:11:44 - train: epoch 0002, iter [00300, 05004], lr: 0.099981, loss: 4.2986
2022-07-14 22:12:16 - train: epoch 0002, iter [00400, 05004], lr: 0.099980, loss: 4.1699
2022-07-14 22:12:50 - train: epoch 0002, iter [00500, 05004], lr: 0.099979, loss: 3.9882
2022-07-14 22:13:23 - train: epoch 0002, iter [00600, 05004], lr: 0.099979, loss: 3.9907
2022-07-14 22:13:56 - train: epoch 0002, iter [00700, 05004], lr: 0.099978, loss: 4.1643
2022-07-14 22:14:29 - train: epoch 0002, iter [00800, 05004], lr: 0.099977, loss: 3.8600
2022-07-14 22:15:04 - train: epoch 0002, iter [00900, 05004], lr: 0.099976, loss: 3.7641
2022-07-14 22:15:37 - train: epoch 0002, iter [01000, 05004], lr: 0.099975, loss: 4.2429
2022-07-14 22:16:10 - train: epoch 0002, iter [01100, 05004], lr: 0.099975, loss: 4.1075
2022-07-14 22:16:44 - train: epoch 0002, iter [01200, 05004], lr: 0.099974, loss: 3.9931
2022-07-14 22:17:18 - train: epoch 0002, iter [01300, 05004], lr: 0.099973, loss: 3.8567
2022-07-14 22:17:51 - train: epoch 0002, iter [01400, 05004], lr: 0.099972, loss: 4.0762
2022-07-14 22:18:25 - train: epoch 0002, iter [01500, 05004], lr: 0.099971, loss: 3.9847
2022-07-14 22:18:59 - train: epoch 0002, iter [01600, 05004], lr: 0.099970, loss: 3.9301
2022-07-14 22:19:32 - train: epoch 0002, iter [01700, 05004], lr: 0.099969, loss: 4.0257
2022-07-14 22:20:06 - train: epoch 0002, iter [01800, 05004], lr: 0.099968, loss: 3.9972
2022-07-14 22:20:39 - train: epoch 0002, iter [01900, 05004], lr: 0.099967, loss: 3.8549
2022-07-14 22:21:13 - train: epoch 0002, iter [02000, 05004], lr: 0.099966, loss: 3.5670
2022-07-14 22:21:46 - train: epoch 0002, iter [02100, 05004], lr: 0.099965, loss: 3.8299
2022-07-14 22:22:21 - train: epoch 0002, iter [02200, 05004], lr: 0.099964, loss: 3.5011
2022-07-14 22:22:55 - train: epoch 0002, iter [02300, 05004], lr: 0.099963, loss: 3.8474
2022-07-14 22:23:27 - train: epoch 0002, iter [02400, 05004], lr: 0.099962, loss: 3.6799
2022-07-14 22:24:02 - train: epoch 0002, iter [02500, 05004], lr: 0.099961, loss: 3.6130
2022-07-14 22:24:36 - train: epoch 0002, iter [02600, 05004], lr: 0.099960, loss: 3.5081
2022-07-14 22:25:10 - train: epoch 0002, iter [02700, 05004], lr: 0.099959, loss: 3.9111
2022-07-14 22:25:43 - train: epoch 0002, iter [02800, 05004], lr: 0.099958, loss: 3.6822
2022-07-14 22:26:18 - train: epoch 0002, iter [02900, 05004], lr: 0.099957, loss: 3.6157
2022-07-14 22:26:50 - train: epoch 0002, iter [03000, 05004], lr: 0.099956, loss: 3.4693
2022-07-14 22:27:25 - train: epoch 0002, iter [03100, 05004], lr: 0.099955, loss: 3.6504
2022-07-14 22:27:58 - train: epoch 0002, iter [03200, 05004], lr: 0.099954, loss: 3.6180
2022-07-14 22:28:32 - train: epoch 0002, iter [03300, 05004], lr: 0.099953, loss: 3.5397
2022-07-14 22:29:06 - train: epoch 0002, iter [03400, 05004], lr: 0.099952, loss: 3.6434
2022-07-14 22:29:40 - train: epoch 0002, iter [03500, 05004], lr: 0.099951, loss: 3.5682
2022-07-14 22:30:14 - train: epoch 0002, iter [03600, 05004], lr: 0.099949, loss: 3.6156
2022-07-14 22:30:48 - train: epoch 0002, iter [03700, 05004], lr: 0.099948, loss: 3.6914
2022-07-14 22:31:21 - train: epoch 0002, iter [03800, 05004], lr: 0.099947, loss: 3.4190
2022-07-14 22:31:55 - train: epoch 0002, iter [03900, 05004], lr: 0.099946, loss: 3.5881
2022-07-14 22:32:29 - train: epoch 0002, iter [04000, 05004], lr: 0.099945, loss: 3.4612
2022-07-14 22:33:03 - train: epoch 0002, iter [04100, 05004], lr: 0.099943, loss: 3.7132
2022-07-14 22:33:37 - train: epoch 0002, iter [04200, 05004], lr: 0.099942, loss: 3.4924
2022-07-14 22:34:09 - train: epoch 0002, iter [04300, 05004], lr: 0.099941, loss: 3.5368
2022-07-14 22:34:44 - train: epoch 0002, iter [04400, 05004], lr: 0.099939, loss: 3.4317
2022-07-14 22:35:16 - train: epoch 0002, iter [04500, 05004], lr: 0.099938, loss: 3.3935
2022-07-14 22:35:49 - train: epoch 0002, iter [04600, 05004], lr: 0.099937, loss: 3.4662
2022-07-14 22:36:23 - train: epoch 0002, iter [04700, 05004], lr: 0.099936, loss: 3.4935
2022-07-14 22:36:56 - train: epoch 0002, iter [04800, 05004], lr: 0.099934, loss: 3.5187
2022-07-14 22:37:31 - train: epoch 0002, iter [04900, 05004], lr: 0.099933, loss: 3.4052
2022-07-14 22:38:03 - train: epoch 0002, iter [05000, 05004], lr: 0.099932, loss: 3.2963
2022-07-14 22:38:04 - train: epoch 002, train_loss: 3.7607
2022-07-14 22:39:18 - eval: epoch: 002, acc1: 30.402%, acc5: 56.636%, test_loss: 3.2635, per_image_load_time: 2.551ms, per_image_inference_time: 0.269ms
2022-07-14 22:39:18 - until epoch: 002, best_acc1: 30.402%
2022-07-14 22:39:18 - epoch 003 lr: 0.099931
2022-07-14 22:39:57 - train: epoch 0003, iter [00100, 05004], lr: 0.099930, loss: 3.5057
2022-07-14 22:40:31 - train: epoch 0003, iter [00200, 05004], lr: 0.099929, loss: 3.4874
2022-07-14 22:41:04 - train: epoch 0003, iter [00300, 05004], lr: 0.099927, loss: 3.3685
2022-07-14 22:41:37 - train: epoch 0003, iter [00400, 05004], lr: 0.099926, loss: 3.4797
2022-07-14 22:42:11 - train: epoch 0003, iter [00500, 05004], lr: 0.099924, loss: 3.6121
2022-07-14 22:42:44 - train: epoch 0003, iter [00600, 05004], lr: 0.099923, loss: 3.2891
2022-07-14 22:43:18 - train: epoch 0003, iter [00700, 05004], lr: 0.099922, loss: 3.5388
2022-07-14 22:43:52 - train: epoch 0003, iter [00800, 05004], lr: 0.099920, loss: 3.4617
2022-07-14 22:44:26 - train: epoch 0003, iter [00900, 05004], lr: 0.099919, loss: 3.2742
2022-07-14 22:45:00 - train: epoch 0003, iter [01000, 05004], lr: 0.099917, loss: 3.4693
2022-07-14 22:45:33 - train: epoch 0003, iter [01100, 05004], lr: 0.099916, loss: 3.2807
2022-07-14 22:46:06 - train: epoch 0003, iter [01200, 05004], lr: 0.099914, loss: 3.1739
2022-07-14 22:46:40 - train: epoch 0003, iter [01300, 05004], lr: 0.099913, loss: 3.2977
2022-07-14 22:47:14 - train: epoch 0003, iter [01400, 05004], lr: 0.099911, loss: 3.3335
2022-07-14 22:47:48 - train: epoch 0003, iter [01500, 05004], lr: 0.099909, loss: 3.6833
2022-07-14 22:48:22 - train: epoch 0003, iter [01600, 05004], lr: 0.099908, loss: 3.3060
2022-07-14 22:48:56 - train: epoch 0003, iter [01700, 05004], lr: 0.099906, loss: 3.2263
2022-07-14 22:49:30 - train: epoch 0003, iter [01800, 05004], lr: 0.099905, loss: 3.1482
2022-07-14 22:50:03 - train: epoch 0003, iter [01900, 05004], lr: 0.099903, loss: 3.2612
2022-07-14 22:50:37 - train: epoch 0003, iter [02000, 05004], lr: 0.099901, loss: 3.5852
2022-07-14 22:51:11 - train: epoch 0003, iter [02100, 05004], lr: 0.099900, loss: 3.5992
2022-07-14 22:51:46 - train: epoch 0003, iter [02200, 05004], lr: 0.099898, loss: 3.7944
2022-07-14 22:52:18 - train: epoch 0003, iter [02300, 05004], lr: 0.099896, loss: 3.3040
2022-07-14 22:52:52 - train: epoch 0003, iter [02400, 05004], lr: 0.099895, loss: 3.2347
2022-07-14 22:53:26 - train: epoch 0003, iter [02500, 05004], lr: 0.099893, loss: 3.2742
2022-07-14 22:54:00 - train: epoch 0003, iter [02600, 05004], lr: 0.099891, loss: 3.4501
2022-07-14 22:54:34 - train: epoch 0003, iter [02700, 05004], lr: 0.099890, loss: 3.5459
2022-07-14 22:55:07 - train: epoch 0003, iter [02800, 05004], lr: 0.099888, loss: 3.1080
2022-07-14 22:55:41 - train: epoch 0003, iter [02900, 05004], lr: 0.099886, loss: 3.2673
2022-07-14 22:56:15 - train: epoch 0003, iter [03000, 05004], lr: 0.099884, loss: 3.4102
2022-07-14 22:56:49 - train: epoch 0003, iter [03100, 05004], lr: 0.099882, loss: 3.4798
2022-07-14 22:57:23 - train: epoch 0003, iter [03200, 05004], lr: 0.099881, loss: 3.2452
2022-07-14 22:57:58 - train: epoch 0003, iter [03300, 05004], lr: 0.099879, loss: 3.2631
2022-07-14 22:58:31 - train: epoch 0003, iter [03400, 05004], lr: 0.099877, loss: 3.3468
2022-07-14 22:59:04 - train: epoch 0003, iter [03500, 05004], lr: 0.099875, loss: 2.9984
2022-07-14 22:59:39 - train: epoch 0003, iter [03600, 05004], lr: 0.099873, loss: 3.2236
2022-07-14 23:00:14 - train: epoch 0003, iter [03700, 05004], lr: 0.099871, loss: 3.2425
2022-07-14 23:00:47 - train: epoch 0003, iter [03800, 05004], lr: 0.099870, loss: 3.3017
2022-07-14 23:01:22 - train: epoch 0003, iter [03900, 05004], lr: 0.099868, loss: 3.4244
2022-07-14 23:01:55 - train: epoch 0003, iter [04000, 05004], lr: 0.099866, loss: 3.2541
2022-07-14 23:02:29 - train: epoch 0003, iter [04100, 05004], lr: 0.099864, loss: 3.2122
2022-07-14 23:03:02 - train: epoch 0003, iter [04200, 05004], lr: 0.099862, loss: 3.0946
2022-07-14 23:03:37 - train: epoch 0003, iter [04300, 05004], lr: 0.099860, loss: 2.8899
2022-07-14 23:04:11 - train: epoch 0003, iter [04400, 05004], lr: 0.099858, loss: 3.0586
2022-07-14 23:04:44 - train: epoch 0003, iter [04500, 05004], lr: 0.099856, loss: 3.2358
2022-07-14 23:05:19 - train: epoch 0003, iter [04600, 05004], lr: 0.099854, loss: 3.1229
2022-07-14 23:05:51 - train: epoch 0003, iter [04700, 05004], lr: 0.099852, loss: 3.0398
2022-07-14 23:06:26 - train: epoch 0003, iter [04800, 05004], lr: 0.099850, loss: 3.2500
2022-07-14 23:07:00 - train: epoch 0003, iter [04900, 05004], lr: 0.099848, loss: 3.1808
2022-07-14 23:07:32 - train: epoch 0003, iter [05000, 05004], lr: 0.099846, loss: 3.1994
2022-07-14 23:07:33 - train: epoch 003, train_loss: 3.2789
2022-07-14 23:08:47 - eval: epoch: 003, acc1: 36.626%, acc5: 62.812%, test_loss: 2.9181, per_image_load_time: 2.491ms, per_image_inference_time: 0.211ms
2022-07-14 23:08:47 - until epoch: 003, best_acc1: 36.626%
2022-07-14 23:08:47 - epoch 004 lr: 0.099846
2022-07-14 23:09:26 - train: epoch 0004, iter [00100, 05004], lr: 0.099844, loss: 3.2224
2022-07-14 23:09:59 - train: epoch 0004, iter [00200, 05004], lr: 0.099842, loss: 3.0835
2022-07-14 23:10:33 - train: epoch 0004, iter [00300, 05004], lr: 0.099840, loss: 3.2096
2022-07-14 23:11:07 - train: epoch 0004, iter [00400, 05004], lr: 0.099838, loss: 3.0363
2022-07-14 23:11:40 - train: epoch 0004, iter [00500, 05004], lr: 0.099835, loss: 3.0445
2022-07-14 23:12:13 - train: epoch 0004, iter [00600, 05004], lr: 0.099833, loss: 3.3132
2022-07-14 23:12:47 - train: epoch 0004, iter [00700, 05004], lr: 0.099831, loss: 3.0762
2022-07-14 23:13:20 - train: epoch 0004, iter [00800, 05004], lr: 0.099829, loss: 2.9768
2022-07-14 23:13:53 - train: epoch 0004, iter [00900, 05004], lr: 0.099827, loss: 2.7752
2022-07-14 23:14:26 - train: epoch 0004, iter [01000, 05004], lr: 0.099825, loss: 3.0662
2022-07-14 23:15:00 - train: epoch 0004, iter [01100, 05004], lr: 0.099822, loss: 3.1607
2022-07-14 23:15:34 - train: epoch 0004, iter [01200, 05004], lr: 0.099820, loss: 2.8538
2022-07-14 23:16:07 - train: epoch 0004, iter [01300, 05004], lr: 0.099818, loss: 3.0277
2022-07-14 23:16:40 - train: epoch 0004, iter [01400, 05004], lr: 0.099816, loss: 3.0908
2022-07-14 23:17:15 - train: epoch 0004, iter [01500, 05004], lr: 0.099814, loss: 3.1893
2022-07-14 23:17:48 - train: epoch 0004, iter [01600, 05004], lr: 0.099811, loss: 2.9797
2022-07-14 23:18:22 - train: epoch 0004, iter [01700, 05004], lr: 0.099809, loss: 3.0844
2022-07-14 23:18:56 - train: epoch 0004, iter [01800, 05004], lr: 0.099807, loss: 3.2938
2022-07-14 23:19:29 - train: epoch 0004, iter [01900, 05004], lr: 0.099804, loss: 3.2001
2022-07-14 23:20:03 - train: epoch 0004, iter [02000, 05004], lr: 0.099802, loss: 3.0529
2022-07-14 23:20:37 - train: epoch 0004, iter [02100, 05004], lr: 0.099800, loss: 3.0783
2022-07-14 23:21:10 - train: epoch 0004, iter [02200, 05004], lr: 0.099797, loss: 2.9658
2022-07-14 23:21:44 - train: epoch 0004, iter [02300, 05004], lr: 0.099795, loss: 2.8835
2022-07-14 23:22:18 - train: epoch 0004, iter [02400, 05004], lr: 0.099793, loss: 2.9039
2022-07-14 23:22:51 - train: epoch 0004, iter [02500, 05004], lr: 0.099790, loss: 3.0115
2022-07-14 23:23:26 - train: epoch 0004, iter [02600, 05004], lr: 0.099788, loss: 3.1411
2022-07-14 23:23:59 - train: epoch 0004, iter [02700, 05004], lr: 0.099785, loss: 2.8313
2022-07-14 23:24:32 - train: epoch 0004, iter [02800, 05004], lr: 0.099783, loss: 2.9933
2022-07-14 23:25:07 - train: epoch 0004, iter [02900, 05004], lr: 0.099781, loss: 2.9892
2022-07-14 23:25:39 - train: epoch 0004, iter [03000, 05004], lr: 0.099778, loss: 2.9292
2022-07-14 23:26:14 - train: epoch 0004, iter [03100, 05004], lr: 0.099776, loss: 3.0707
2022-07-14 23:26:47 - train: epoch 0004, iter [03200, 05004], lr: 0.099773, loss: 2.9269
2022-07-14 23:27:22 - train: epoch 0004, iter [03300, 05004], lr: 0.099771, loss: 3.1300
2022-07-14 23:27:54 - train: epoch 0004, iter [03400, 05004], lr: 0.099768, loss: 2.9932
2022-07-14 23:28:28 - train: epoch 0004, iter [03500, 05004], lr: 0.099766, loss: 2.9672
2022-07-14 23:29:01 - train: epoch 0004, iter [03600, 05004], lr: 0.099763, loss: 2.7599
2022-07-14 23:29:36 - train: epoch 0004, iter [03700, 05004], lr: 0.099761, loss: 2.9300
2022-07-14 23:30:09 - train: epoch 0004, iter [03800, 05004], lr: 0.099758, loss: 2.8805
2022-07-14 23:30:43 - train: epoch 0004, iter [03900, 05004], lr: 0.099755, loss: 2.9481
2022-07-14 23:31:17 - train: epoch 0004, iter [04000, 05004], lr: 0.099753, loss: 2.6284
2022-07-14 23:31:51 - train: epoch 0004, iter [04100, 05004], lr: 0.099750, loss: 2.9247
2022-07-14 23:32:25 - train: epoch 0004, iter [04200, 05004], lr: 0.099748, loss: 2.9333
2022-07-14 23:32:58 - train: epoch 0004, iter [04300, 05004], lr: 0.099745, loss: 2.7137
2022-07-14 23:33:31 - train: epoch 0004, iter [04400, 05004], lr: 0.099742, loss: 2.8604
2022-07-14 23:34:05 - train: epoch 0004, iter [04500, 05004], lr: 0.099740, loss: 2.4567
2022-07-14 23:34:38 - train: epoch 0004, iter [04600, 05004], lr: 0.099737, loss: 2.8984
2022-07-14 23:35:12 - train: epoch 0004, iter [04700, 05004], lr: 0.099734, loss: 2.8601
2022-07-14 23:35:46 - train: epoch 0004, iter [04800, 05004], lr: 0.099732, loss: 2.8579
2022-07-14 23:36:19 - train: epoch 0004, iter [04900, 05004], lr: 0.099729, loss: 3.0083
2022-07-14 23:36:52 - train: epoch 0004, iter [05000, 05004], lr: 0.099726, loss: 3.0260
2022-07-14 23:36:53 - train: epoch 004, train_loss: 3.0359
2022-07-14 23:38:07 - eval: epoch: 004, acc1: 35.666%, acc5: 61.578%, test_loss: 3.0101, per_image_load_time: 2.667ms, per_image_inference_time: 0.251ms
2022-07-14 23:38:07 - until epoch: 004, best_acc1: 36.626%
2022-07-14 23:38:07 - epoch 005 lr: 0.099726
2022-07-14 23:38:46 - train: epoch 0005, iter [00100, 05004], lr: 0.099723, loss: 2.9967
2022-07-14 23:39:20 - train: epoch 0005, iter [00200, 05004], lr: 0.099721, loss: 3.0833
2022-07-14 23:39:54 - train: epoch 0005, iter [00300, 05004], lr: 0.099718, loss: 2.9969
2022-07-14 23:40:26 - train: epoch 0005, iter [00400, 05004], lr: 0.099715, loss: 3.0061
2022-07-14 23:41:00 - train: epoch 0005, iter [00500, 05004], lr: 0.099712, loss: 2.7713
2022-07-14 23:41:33 - train: epoch 0005, iter [00600, 05004], lr: 0.099709, loss: 3.0024
2022-07-14 23:42:07 - train: epoch 0005, iter [00700, 05004], lr: 0.099707, loss: 2.9435
2022-07-14 23:42:40 - train: epoch 0005, iter [00800, 05004], lr: 0.099704, loss: 3.1255
2022-07-14 23:43:14 - train: epoch 0005, iter [00900, 05004], lr: 0.099701, loss: 2.9317
2022-07-14 23:43:47 - train: epoch 0005, iter [01000, 05004], lr: 0.099698, loss: 3.0423
2022-07-14 23:44:21 - train: epoch 0005, iter [01100, 05004], lr: 0.099695, loss: 3.0637
2022-07-14 23:44:55 - train: epoch 0005, iter [01200, 05004], lr: 0.099692, loss: 2.9808
2022-07-14 23:45:29 - train: epoch 0005, iter [01300, 05004], lr: 0.099689, loss: 2.9697
2022-07-14 23:46:02 - train: epoch 0005, iter [01400, 05004], lr: 0.099686, loss: 2.9533
2022-07-14 23:46:35 - train: epoch 0005, iter [01500, 05004], lr: 0.099684, loss: 2.7634
2022-07-14 23:47:09 - train: epoch 0005, iter [01600, 05004], lr: 0.099681, loss: 2.6904
2022-07-14 23:47:43 - train: epoch 0005, iter [01700, 05004], lr: 0.099678, loss: 2.8896
2022-07-14 23:48:17 - train: epoch 0005, iter [01800, 05004], lr: 0.099675, loss: 2.9722
2022-07-14 23:48:50 - train: epoch 0005, iter [01900, 05004], lr: 0.099672, loss: 2.7693
2022-07-14 23:49:25 - train: epoch 0005, iter [02000, 05004], lr: 0.099669, loss: 2.8439
2022-07-14 23:49:58 - train: epoch 0005, iter [02100, 05004], lr: 0.099666, loss: 2.6242
2022-07-14 23:50:32 - train: epoch 0005, iter [02200, 05004], lr: 0.099663, loss: 2.7882
2022-07-14 23:51:07 - train: epoch 0005, iter [02300, 05004], lr: 0.099660, loss: 2.6907
2022-07-14 23:51:40 - train: epoch 0005, iter [02400, 05004], lr: 0.099657, loss: 2.8726
2022-07-14 23:52:15 - train: epoch 0005, iter [02500, 05004], lr: 0.099653, loss: 2.9833
2022-07-14 23:52:48 - train: epoch 0005, iter [02600, 05004], lr: 0.099650, loss: 3.0526
2022-07-14 23:53:23 - train: epoch 0005, iter [02700, 05004], lr: 0.099647, loss: 3.1167
2022-07-14 23:53:56 - train: epoch 0005, iter [02800, 05004], lr: 0.099644, loss: 2.8909
2022-07-14 23:54:30 - train: epoch 0005, iter [02900, 05004], lr: 0.099641, loss: 2.6830
2022-07-14 23:55:03 - train: epoch 0005, iter [03000, 05004], lr: 0.099638, loss: 2.8533
2022-07-14 23:55:38 - train: epoch 0005, iter [03100, 05004], lr: 0.099635, loss: 2.9577
2022-07-14 23:56:10 - train: epoch 0005, iter [03200, 05004], lr: 0.099632, loss: 2.9272
2022-07-14 23:56:45 - train: epoch 0005, iter [03300, 05004], lr: 0.099628, loss: 2.6778
2022-07-14 23:57:18 - train: epoch 0005, iter [03400, 05004], lr: 0.099625, loss: 2.7332
2022-07-14 23:57:53 - train: epoch 0005, iter [03500, 05004], lr: 0.099622, loss: 2.7868
2022-07-14 23:58:25 - train: epoch 0005, iter [03600, 05004], lr: 0.099619, loss: 2.9763
2022-07-14 23:58:59 - train: epoch 0005, iter [03700, 05004], lr: 0.099616, loss: 2.7762
2022-07-14 23:59:33 - train: epoch 0005, iter [03800, 05004], lr: 0.099612, loss: 2.6891
2022-07-15 00:00:06 - train: epoch 0005, iter [03900, 05004], lr: 0.099609, loss: 3.1811
2022-07-15 00:00:40 - train: epoch 0005, iter [04000, 05004], lr: 0.099606, loss: 2.8433
2022-07-15 00:01:13 - train: epoch 0005, iter [04100, 05004], lr: 0.099603, loss: 2.9069
2022-07-15 00:01:47 - train: epoch 0005, iter [04200, 05004], lr: 0.099599, loss: 2.9067
2022-07-15 00:02:21 - train: epoch 0005, iter [04300, 05004], lr: 0.099596, loss: 2.6617
2022-07-15 00:02:54 - train: epoch 0005, iter [04400, 05004], lr: 0.099593, loss: 2.8484
2022-07-15 00:03:30 - train: epoch 0005, iter [04500, 05004], lr: 0.099589, loss: 2.8959
2022-07-15 00:04:02 - train: epoch 0005, iter [04600, 05004], lr: 0.099586, loss: 2.8803
2022-07-15 00:04:35 - train: epoch 0005, iter [04700, 05004], lr: 0.099583, loss: 2.5838
2022-07-15 00:05:09 - train: epoch 0005, iter [04800, 05004], lr: 0.099579, loss: 2.6757
2022-07-15 00:05:42 - train: epoch 0005, iter [04900, 05004], lr: 0.099576, loss: 2.9345
2022-07-15 00:06:14 - train: epoch 0005, iter [05000, 05004], lr: 0.099572, loss: 2.6762
2022-07-15 00:06:14 - train: epoch 005, train_loss: 2.8913
2022-07-15 00:07:29 - eval: epoch: 005, acc1: 41.836%, acc5: 68.360%, test_loss: 2.6123, per_image_load_time: 2.393ms, per_image_inference_time: 0.240ms
2022-07-15 00:07:29 - until epoch: 005, best_acc1: 41.836%
2022-07-15 00:07:29 - epoch 006 lr: 0.099572
2022-07-15 00:08:07 - train: epoch 0006, iter [00100, 05004], lr: 0.099569, loss: 2.7511
2022-07-15 00:08:42 - train: epoch 0006, iter [00200, 05004], lr: 0.099565, loss: 2.8174
2022-07-15 00:09:16 - train: epoch 0006, iter [00300, 05004], lr: 0.099562, loss: 2.6086
2022-07-15 00:09:49 - train: epoch 0006, iter [00400, 05004], lr: 0.099558, loss: 2.9367
2022-07-15 00:10:22 - train: epoch 0006, iter [00500, 05004], lr: 0.099555, loss: 2.7913
2022-07-15 00:10:56 - train: epoch 0006, iter [00600, 05004], lr: 0.099552, loss: 2.9529
2022-07-15 00:11:30 - train: epoch 0006, iter [00700, 05004], lr: 0.099548, loss: 2.9100
2022-07-15 00:12:04 - train: epoch 0006, iter [00800, 05004], lr: 0.099544, loss: 2.8050
2022-07-15 00:12:38 - train: epoch 0006, iter [00900, 05004], lr: 0.099541, loss: 2.7295
2022-07-15 00:13:11 - train: epoch 0006, iter [01000, 05004], lr: 0.099537, loss: 2.6801
2022-07-15 00:13:45 - train: epoch 0006, iter [01100, 05004], lr: 0.099534, loss: 2.7694
2022-07-15 00:14:19 - train: epoch 0006, iter [01200, 05004], lr: 0.099530, loss: 2.8791
2022-07-15 00:14:53 - train: epoch 0006, iter [01300, 05004], lr: 0.099527, loss: 3.0255
2022-07-15 00:15:27 - train: epoch 0006, iter [01400, 05004], lr: 0.099523, loss: 2.9132
2022-07-15 00:16:01 - train: epoch 0006, iter [01500, 05004], lr: 0.099520, loss: 2.8782
2022-07-15 00:16:35 - train: epoch 0006, iter [01600, 05004], lr: 0.099516, loss: 2.5943
2022-07-15 00:17:09 - train: epoch 0006, iter [01700, 05004], lr: 0.099512, loss: 2.9243
2022-07-15 00:17:43 - train: epoch 0006, iter [01800, 05004], lr: 0.099509, loss: 2.8833
2022-07-15 00:18:18 - train: epoch 0006, iter [01900, 05004], lr: 0.099505, loss: 2.6275
2022-07-15 00:18:51 - train: epoch 0006, iter [02000, 05004], lr: 0.099501, loss: 2.9588
2022-07-15 00:19:25 - train: epoch 0006, iter [02100, 05004], lr: 0.099498, loss: 2.8309
2022-07-15 00:19:59 - train: epoch 0006, iter [02200, 05004], lr: 0.099494, loss: 2.6602
2022-07-15 00:20:33 - train: epoch 0006, iter [02300, 05004], lr: 0.099490, loss: 2.6418
2022-07-15 00:21:07 - train: epoch 0006, iter [02400, 05004], lr: 0.099486, loss: 2.8372
2022-07-15 00:21:40 - train: epoch 0006, iter [02500, 05004], lr: 0.099483, loss: 2.9876
2022-07-15 00:22:15 - train: epoch 0006, iter [02600, 05004], lr: 0.099479, loss: 2.8445
2022-07-15 00:22:48 - train: epoch 0006, iter [02700, 05004], lr: 0.099475, loss: 3.0028
2022-07-15 00:23:22 - train: epoch 0006, iter [02800, 05004], lr: 0.099471, loss: 2.6035
2022-07-15 00:23:55 - train: epoch 0006, iter [02900, 05004], lr: 0.099468, loss: 2.8467
2022-07-15 00:24:29 - train: epoch 0006, iter [03000, 05004], lr: 0.099464, loss: 2.7614
2022-07-15 00:25:02 - train: epoch 0006, iter [03100, 05004], lr: 0.099460, loss: 2.5699
2022-07-15 00:25:36 - train: epoch 0006, iter [03200, 05004], lr: 0.099456, loss: 2.7721
2022-07-15 00:26:08 - train: epoch 0006, iter [03300, 05004], lr: 0.099452, loss: 2.6197
2022-07-15 00:26:43 - train: epoch 0006, iter [03400, 05004], lr: 0.099448, loss: 2.9252
2022-07-15 00:27:16 - train: epoch 0006, iter [03500, 05004], lr: 0.099444, loss: 2.8550
2022-07-15 00:27:49 - train: epoch 0006, iter [03600, 05004], lr: 0.099441, loss: 2.6913
2022-07-15 00:28:23 - train: epoch 0006, iter [03700, 05004], lr: 0.099437, loss: 2.8019
2022-07-15 00:28:56 - train: epoch 0006, iter [03800, 05004], lr: 0.099433, loss: 2.6726
2022-07-15 00:29:30 - train: epoch 0006, iter [03900, 05004], lr: 0.099429, loss: 2.6143
2022-07-15 00:30:03 - train: epoch 0006, iter [04000, 05004], lr: 0.099425, loss: 3.1163
2022-07-15 00:30:37 - train: epoch 0006, iter [04100, 05004], lr: 0.099421, loss: 2.6960
2022-07-15 00:31:09 - train: epoch 0006, iter [04200, 05004], lr: 0.099417, loss: 2.5734
2022-07-15 00:31:43 - train: epoch 0006, iter [04300, 05004], lr: 0.099413, loss: 2.7796
2022-07-15 00:32:15 - train: epoch 0006, iter [04400, 05004], lr: 0.099409, loss: 2.6814
2022-07-15 00:32:49 - train: epoch 0006, iter [04500, 05004], lr: 0.099405, loss: 2.8638
2022-07-15 00:33:23 - train: epoch 0006, iter [04600, 05004], lr: 0.099401, loss: 2.7869
2022-07-15 00:33:56 - train: epoch 0006, iter [04700, 05004], lr: 0.099397, loss: 2.7963
2022-07-15 00:34:29 - train: epoch 0006, iter [04800, 05004], lr: 0.099393, loss: 2.7631
2022-07-15 00:35:03 - train: epoch 0006, iter [04900, 05004], lr: 0.099389, loss: 2.7782
2022-07-15 00:35:34 - train: epoch 0006, iter [05000, 05004], lr: 0.099385, loss: 2.7148
2022-07-15 00:35:35 - train: epoch 006, train_loss: 2.7981
2022-07-15 00:36:49 - eval: epoch: 006, acc1: 44.726%, acc5: 70.926%, test_loss: 2.4675, per_image_load_time: 2.643ms, per_image_inference_time: 0.203ms
2022-07-15 00:36:49 - until epoch: 006, best_acc1: 44.726%
2022-07-15 00:36:49 - epoch 007 lr: 0.099384
2022-07-15 00:37:27 - train: epoch 0007, iter [00100, 05004], lr: 0.099380, loss: 2.6260
2022-07-15 00:38:01 - train: epoch 0007, iter [00200, 05004], lr: 0.099376, loss: 3.0333
2022-07-15 00:38:34 - train: epoch 0007, iter [00300, 05004], lr: 0.099372, loss: 2.9699
2022-07-15 00:39:07 - train: epoch 0007, iter [00400, 05004], lr: 0.099368, loss: 2.7568
2022-07-15 00:39:41 - train: epoch 0007, iter [00500, 05004], lr: 0.099364, loss: 2.6798
2022-07-15 00:40:14 - train: epoch 0007, iter [00600, 05004], lr: 0.099360, loss: 2.8687
2022-07-15 00:40:47 - train: epoch 0007, iter [00700, 05004], lr: 0.099355, loss: 2.7554
2022-07-15 00:41:22 - train: epoch 0007, iter [00800, 05004], lr: 0.099351, loss: 2.7620
2022-07-15 00:41:54 - train: epoch 0007, iter [00900, 05004], lr: 0.099347, loss: 2.7974
2022-07-15 00:42:28 - train: epoch 0007, iter [01000, 05004], lr: 0.099343, loss: 2.8046
2022-07-15 00:43:01 - train: epoch 0007, iter [01100, 05004], lr: 0.099339, loss: 2.5840
2022-07-15 00:43:35 - train: epoch 0007, iter [01200, 05004], lr: 0.099334, loss: 2.7048
2022-07-15 00:44:07 - train: epoch 0007, iter [01300, 05004], lr: 0.099330, loss: 2.6222
2022-07-15 00:44:41 - train: epoch 0007, iter [01400, 05004], lr: 0.099326, loss: 2.8404
2022-07-15 00:45:14 - train: epoch 0007, iter [01500, 05004], lr: 0.099322, loss: 2.8172
2022-07-15 00:45:47 - train: epoch 0007, iter [01600, 05004], lr: 0.099317, loss: 2.7534
2022-07-15 00:46:21 - train: epoch 0007, iter [01700, 05004], lr: 0.099313, loss: 2.7938
2022-07-15 00:46:55 - train: epoch 0007, iter [01800, 05004], lr: 0.099309, loss: 2.6192
2022-07-15 00:47:27 - train: epoch 0007, iter [01900, 05004], lr: 0.099304, loss: 2.8500
2022-07-15 00:48:01 - train: epoch 0007, iter [02000, 05004], lr: 0.099300, loss: 2.6131
2022-07-15 00:48:34 - train: epoch 0007, iter [02100, 05004], lr: 0.099296, loss: 2.8163
2022-07-15 00:49:07 - train: epoch 0007, iter [02200, 05004], lr: 0.099291, loss: 2.6075
2022-07-15 00:49:40 - train: epoch 0007, iter [02300, 05004], lr: 0.099287, loss: 2.7568
2022-07-15 00:50:13 - train: epoch 0007, iter [02400, 05004], lr: 0.099282, loss: 2.7546
2022-07-15 00:50:46 - train: epoch 0007, iter [02500, 05004], lr: 0.099278, loss: 2.6614
2022-07-15 00:51:18 - train: epoch 0007, iter [02600, 05004], lr: 0.099273, loss: 2.7355
2022-07-15 00:51:52 - train: epoch 0007, iter [02700, 05004], lr: 0.099269, loss: 2.6457
2022-07-15 00:52:24 - train: epoch 0007, iter [02800, 05004], lr: 0.099265, loss: 2.6298
2022-07-15 00:52:58 - train: epoch 0007, iter [02900, 05004], lr: 0.099260, loss: 2.7782
2022-07-15 00:53:30 - train: epoch 0007, iter [03000, 05004], lr: 0.099256, loss: 2.7379
2022-07-15 00:54:04 - train: epoch 0007, iter [03100, 05004], lr: 0.099251, loss: 2.5178
2022-07-15 00:54:37 - train: epoch 0007, iter [03200, 05004], lr: 0.099247, loss: 2.6644
2022-07-15 00:55:10 - train: epoch 0007, iter [03300, 05004], lr: 0.099242, loss: 2.9176
2022-07-15 00:55:43 - train: epoch 0007, iter [03400, 05004], lr: 0.099237, loss: 2.5826
2022-07-15 00:56:16 - train: epoch 0007, iter [03500, 05004], lr: 0.099233, loss: 2.7975
2022-07-15 00:56:50 - train: epoch 0007, iter [03600, 05004], lr: 0.099228, loss: 2.5266
2022-07-15 00:57:22 - train: epoch 0007, iter [03700, 05004], lr: 0.099224, loss: 2.7058
2022-07-15 00:57:56 - train: epoch 0007, iter [03800, 05004], lr: 0.099219, loss: 2.8935
2022-07-15 00:58:28 - train: epoch 0007, iter [03900, 05004], lr: 0.099215, loss: 2.6375
2022-07-15 00:59:02 - train: epoch 0007, iter [04000, 05004], lr: 0.099210, loss: 2.7181
2022-07-15 00:59:34 - train: epoch 0007, iter [04100, 05004], lr: 0.099205, loss: 2.6446
2022-07-15 01:00:08 - train: epoch 0007, iter [04200, 05004], lr: 0.099201, loss: 2.5804
2022-07-15 01:00:40 - train: epoch 0007, iter [04300, 05004], lr: 0.099196, loss: 2.8648
2022-07-15 01:01:13 - train: epoch 0007, iter [04400, 05004], lr: 0.099191, loss: 2.4584
2022-07-15 01:01:47 - train: epoch 0007, iter [04500, 05004], lr: 0.099187, loss: 2.9867
2022-07-15 01:02:20 - train: epoch 0007, iter [04600, 05004], lr: 0.099182, loss: 2.8565
2022-07-15 01:02:54 - train: epoch 0007, iter [04700, 05004], lr: 0.099177, loss: 2.8031
2022-07-15 01:03:27 - train: epoch 0007, iter [04800, 05004], lr: 0.099172, loss: 3.0311
2022-07-15 01:04:01 - train: epoch 0007, iter [04900, 05004], lr: 0.099168, loss: 2.6815
2022-07-15 01:04:32 - train: epoch 0007, iter [05000, 05004], lr: 0.099163, loss: 2.7279
2022-07-15 01:04:33 - train: epoch 007, train_loss: 2.7191
2022-07-15 01:05:47 - eval: epoch: 007, acc1: 45.400%, acc5: 71.558%, test_loss: 2.4263, per_image_load_time: 0.799ms, per_image_inference_time: 0.229ms
2022-07-15 01:05:47 - until epoch: 007, best_acc1: 45.400%
2022-07-15 01:05:47 - epoch 008 lr: 0.099163
2022-07-15 01:06:26 - train: epoch 0008, iter [00100, 05004], lr: 0.099158, loss: 2.6810
2022-07-15 01:06:59 - train: epoch 0008, iter [00200, 05004], lr: 0.099153, loss: 2.7385
2022-07-15 01:07:32 - train: epoch 0008, iter [00300, 05004], lr: 0.099148, loss: 2.4215
2022-07-15 01:08:05 - train: epoch 0008, iter [00400, 05004], lr: 0.099144, loss: 2.4543
2022-07-15 01:08:37 - train: epoch 0008, iter [00500, 05004], lr: 0.099139, loss: 2.5314
2022-07-15 01:09:10 - train: epoch 0008, iter [00600, 05004], lr: 0.099134, loss: 2.6229
2022-07-15 01:09:43 - train: epoch 0008, iter [00700, 05004], lr: 0.099129, loss: 3.0527
2022-07-15 01:10:17 - train: epoch 0008, iter [00800, 05004], lr: 0.099124, loss: 2.4868
2022-07-15 01:10:49 - train: epoch 0008, iter [00900, 05004], lr: 0.099119, loss: 2.6332
2022-07-15 01:11:23 - train: epoch 0008, iter [01000, 05004], lr: 0.099114, loss: 2.6833
2022-07-15 01:11:56 - train: epoch 0008, iter [01100, 05004], lr: 0.099109, loss: 2.5128
2022-07-15 01:12:29 - train: epoch 0008, iter [01200, 05004], lr: 0.099105, loss: 2.4749
2022-07-15 01:13:02 - train: epoch 0008, iter [01300, 05004], lr: 0.099100, loss: 2.6089
2022-07-15 01:13:35 - train: epoch 0008, iter [01400, 05004], lr: 0.099095, loss: 2.5955
2022-07-15 01:14:09 - train: epoch 0008, iter [01500, 05004], lr: 0.099090, loss: 2.7199
2022-07-15 01:14:41 - train: epoch 0008, iter [01600, 05004], lr: 0.099085, loss: 2.7362
2022-07-15 01:15:15 - train: epoch 0008, iter [01700, 05004], lr: 0.099080, loss: 2.4845
2022-07-15 01:15:47 - train: epoch 0008, iter [01800, 05004], lr: 0.099075, loss: 2.9078
2022-07-15 01:16:21 - train: epoch 0008, iter [01900, 05004], lr: 0.099070, loss: 2.4713
2022-07-15 01:16:53 - train: epoch 0008, iter [02000, 05004], lr: 0.099065, loss: 2.7578
2022-07-15 01:17:27 - train: epoch 0008, iter [02100, 05004], lr: 0.099060, loss: 2.5970
2022-07-15 01:18:00 - train: epoch 0008, iter [02200, 05004], lr: 0.099055, loss: 2.5273
2022-07-15 01:18:33 - train: epoch 0008, iter [02300, 05004], lr: 0.099050, loss: 2.6153
2022-07-15 01:19:06 - train: epoch 0008, iter [02400, 05004], lr: 0.099044, loss: 2.6305
2022-07-15 01:19:40 - train: epoch 0008, iter [02500, 05004], lr: 0.099039, loss: 2.6725
2022-07-15 01:20:13 - train: epoch 0008, iter [02600, 05004], lr: 0.099034, loss: 2.6900
2022-07-15 01:20:46 - train: epoch 0008, iter [02700, 05004], lr: 0.099029, loss: 2.7893
2022-07-15 01:21:19 - train: epoch 0008, iter [02800, 05004], lr: 0.099024, loss: 2.7228
2022-07-15 01:21:52 - train: epoch 0008, iter [02900, 05004], lr: 0.099019, loss: 2.6509
2022-07-15 01:22:25 - train: epoch 0008, iter [03000, 05004], lr: 0.099014, loss: 2.7338
2022-07-15 01:22:58 - train: epoch 0008, iter [03100, 05004], lr: 0.099009, loss: 2.6530
2022-07-15 01:23:32 - train: epoch 0008, iter [03200, 05004], lr: 0.099003, loss: 2.8663
2022-07-15 01:24:05 - train: epoch 0008, iter [03300, 05004], lr: 0.098998, loss: 2.8427
2022-07-15 01:24:37 - train: epoch 0008, iter [03400, 05004], lr: 0.098993, loss: 2.8637
2022-07-15 01:25:11 - train: epoch 0008, iter [03500, 05004], lr: 0.098988, loss: 2.7062
2022-07-15 01:25:44 - train: epoch 0008, iter [03600, 05004], lr: 0.098982, loss: 2.7915
2022-07-15 01:26:18 - train: epoch 0008, iter [03700, 05004], lr: 0.098977, loss: 2.5480
2022-07-15 01:26:50 - train: epoch 0008, iter [03800, 05004], lr: 0.098972, loss: 2.6083
2022-07-15 01:27:23 - train: epoch 0008, iter [03900, 05004], lr: 0.098967, loss: 2.7947
2022-07-15 01:27:56 - train: epoch 0008, iter [04000, 05004], lr: 0.098961, loss: 2.9364
2022-07-15 01:28:30 - train: epoch 0008, iter [04100, 05004], lr: 0.098956, loss: 2.5634
2022-07-15 01:29:03 - train: epoch 0008, iter [04200, 05004], lr: 0.098951, loss: 2.6198
2022-07-15 01:29:37 - train: epoch 0008, iter [04300, 05004], lr: 0.098945, loss: 2.4497
2022-07-15 01:30:09 - train: epoch 0008, iter [04400, 05004], lr: 0.098940, loss: 2.6113
2022-07-15 01:30:42 - train: epoch 0008, iter [04500, 05004], lr: 0.098935, loss: 2.8157
2022-07-15 01:31:15 - train: epoch 0008, iter [04600, 05004], lr: 0.098929, loss: 2.7379
2022-07-15 01:31:49 - train: epoch 0008, iter [04700, 05004], lr: 0.098924, loss: 2.5430
2022-07-15 01:32:21 - train: epoch 0008, iter [04800, 05004], lr: 0.098918, loss: 2.6612
2022-07-15 01:32:54 - train: epoch 0008, iter [04900, 05004], lr: 0.098913, loss: 2.6725
2022-07-15 01:33:26 - train: epoch 0008, iter [05000, 05004], lr: 0.098908, loss: 2.6023
2022-07-15 01:33:27 - train: epoch 008, train_loss: 2.6607
2022-07-15 01:34:41 - eval: epoch: 008, acc1: 45.950%, acc5: 72.222%, test_loss: 2.4052, per_image_load_time: 0.850ms, per_image_inference_time: 0.225ms
2022-07-15 01:34:41 - until epoch: 008, best_acc1: 45.950%
2022-07-15 01:34:41 - epoch 009 lr: 0.098907
2022-07-15 01:35:19 - train: epoch 0009, iter [00100, 05004], lr: 0.098902, loss: 2.3480
2022-07-15 01:35:53 - train: epoch 0009, iter [00200, 05004], lr: 0.098896, loss: 2.3586
2022-07-15 01:36:25 - train: epoch 0009, iter [00300, 05004], lr: 0.098891, loss: 2.3889
2022-07-15 01:36:58 - train: epoch 0009, iter [00400, 05004], lr: 0.098886, loss: 2.9472
2022-07-15 01:37:31 - train: epoch 0009, iter [00500, 05004], lr: 0.098880, loss: 2.4880
2022-07-15 01:38:04 - train: epoch 0009, iter [00600, 05004], lr: 0.098875, loss: 2.7401
2022-07-15 01:38:37 - train: epoch 0009, iter [00700, 05004], lr: 0.098869, loss: 2.5167
2022-07-15 01:39:10 - train: epoch 0009, iter [00800, 05004], lr: 0.098863, loss: 2.5111
2022-07-15 01:39:44 - train: epoch 0009, iter [00900, 05004], lr: 0.098858, loss: 2.4008
2022-07-15 01:40:16 - train: epoch 0009, iter [01000, 05004], lr: 0.098852, loss: 2.5046
2022-07-15 01:40:49 - train: epoch 0009, iter [01100, 05004], lr: 0.098847, loss: 2.9329
2022-07-15 01:41:21 - train: epoch 0009, iter [01200, 05004], lr: 0.098841, loss: 2.7667
2022-07-15 01:41:55 - train: epoch 0009, iter [01300, 05004], lr: 0.098836, loss: 2.7894
2022-07-15 01:42:28 - train: epoch 0009, iter [01400, 05004], lr: 0.098830, loss: 2.3465
2022-07-15 01:43:01 - train: epoch 0009, iter [01500, 05004], lr: 0.098824, loss: 2.4067
2022-07-15 01:43:34 - train: epoch 0009, iter [01600, 05004], lr: 0.098819, loss: 2.7058
2022-07-15 01:44:07 - train: epoch 0009, iter [01700, 05004], lr: 0.098813, loss: 2.8293
2022-07-15 01:44:40 - train: epoch 0009, iter [01800, 05004], lr: 0.098807, loss: 2.4968
2022-07-15 01:45:13 - train: epoch 0009, iter [01900, 05004], lr: 0.098802, loss: 2.3312
2022-07-15 01:45:46 - train: epoch 0009, iter [02000, 05004], lr: 0.098796, loss: 2.3925
2022-07-15 01:46:18 - train: epoch 0009, iter [02100, 05004], lr: 0.098790, loss: 2.5979
2022-07-15 01:46:52 - train: epoch 0009, iter [02200, 05004], lr: 0.098784, loss: 2.7941
2022-07-15 01:47:25 - train: epoch 0009, iter [02300, 05004], lr: 0.098779, loss: 2.3274
2022-07-15 01:47:58 - train: epoch 0009, iter [02400, 05004], lr: 0.098773, loss: 2.5692
2022-07-15 01:48:30 - train: epoch 0009, iter [02500, 05004], lr: 0.098767, loss: 2.4691
2022-07-15 01:49:04 - train: epoch 0009, iter [02600, 05004], lr: 0.098761, loss: 2.7247
2022-07-15 01:49:37 - train: epoch 0009, iter [02700, 05004], lr: 0.098756, loss: 2.4917
2022-07-15 01:50:10 - train: epoch 0009, iter [02800, 05004], lr: 0.098750, loss: 2.6059
2022-07-15 01:50:43 - train: epoch 0009, iter [02900, 05004], lr: 0.098744, loss: 2.3113
2022-07-15 01:51:16 - train: epoch 0009, iter [03000, 05004], lr: 0.098738, loss: 2.4807
2022-07-15 01:51:49 - train: epoch 0009, iter [03100, 05004], lr: 0.098732, loss: 2.7199
2022-07-15 01:52:21 - train: epoch 0009, iter [03200, 05004], lr: 0.098726, loss: 2.6625
2022-07-15 01:52:55 - train: epoch 0009, iter [03300, 05004], lr: 0.098721, loss: 2.6809
2022-07-15 01:53:28 - train: epoch 0009, iter [03400, 05004], lr: 0.098715, loss: 2.8158
2022-07-15 01:54:01 - train: epoch 0009, iter [03500, 05004], lr: 0.098709, loss: 2.7019
2022-07-15 01:54:34 - train: epoch 0009, iter [03600, 05004], lr: 0.098703, loss: 2.5019
2022-07-15 01:55:07 - train: epoch 0009, iter [03700, 05004], lr: 0.098697, loss: 2.8483
2022-07-15 01:55:39 - train: epoch 0009, iter [03800, 05004], lr: 0.098691, loss: 2.8223
2022-07-15 01:56:13 - train: epoch 0009, iter [03900, 05004], lr: 0.098685, loss: 2.3397
2022-07-15 01:56:45 - train: epoch 0009, iter [04000, 05004], lr: 0.098679, loss: 2.8267
2022-07-15 01:57:19 - train: epoch 0009, iter [04100, 05004], lr: 0.098673, loss: 2.5416
2022-07-15 01:57:52 - train: epoch 0009, iter [04200, 05004], lr: 0.098667, loss: 2.5584
2022-07-15 01:58:25 - train: epoch 0009, iter [04300, 05004], lr: 0.098661, loss: 2.6978
2022-07-15 01:58:58 - train: epoch 0009, iter [04400, 05004], lr: 0.098655, loss: 2.6434
2022-07-15 01:59:31 - train: epoch 0009, iter [04500, 05004], lr: 0.098649, loss: 2.5413
2022-07-15 02:00:04 - train: epoch 0009, iter [04600, 05004], lr: 0.098643, loss: 2.8171
2022-07-15 02:00:37 - train: epoch 0009, iter [04700, 05004], lr: 0.098637, loss: 2.6813
2022-07-15 02:01:10 - train: epoch 0009, iter [04800, 05004], lr: 0.098631, loss: 2.9238
2022-07-15 02:01:43 - train: epoch 0009, iter [04900, 05004], lr: 0.098625, loss: 2.6993
2022-07-15 02:02:14 - train: epoch 0009, iter [05000, 05004], lr: 0.098619, loss: 2.5387
2022-07-15 02:02:15 - train: epoch 009, train_loss: 2.6167
2022-07-15 02:03:29 - eval: epoch: 009, acc1: 46.204%, acc5: 72.064%, test_loss: 2.3978, per_image_load_time: 1.092ms, per_image_inference_time: 0.228ms
2022-07-15 02:03:29 - until epoch: 009, best_acc1: 46.204%
2022-07-15 02:03:29 - epoch 010 lr: 0.098618
2022-07-15 02:04:08 - train: epoch 0010, iter [00100, 05004], lr: 0.098612, loss: 2.5732
2022-07-15 02:04:42 - train: epoch 0010, iter [00200, 05004], lr: 0.098606, loss: 2.6958
2022-07-15 02:05:15 - train: epoch 0010, iter [00300, 05004], lr: 0.098600, loss: 2.5213
2022-07-15 02:05:49 - train: epoch 0010, iter [00400, 05004], lr: 0.098594, loss: 2.7769
2022-07-15 02:06:21 - train: epoch 0010, iter [00500, 05004], lr: 0.098588, loss: 2.4723
2022-07-15 02:06:55 - train: epoch 0010, iter [00600, 05004], lr: 0.098582, loss: 2.6522
2022-07-15 02:07:28 - train: epoch 0010, iter [00700, 05004], lr: 0.098575, loss: 2.6926
2022-07-15 02:08:01 - train: epoch 0010, iter [00800, 05004], lr: 0.098569, loss: 2.4519
2022-07-15 02:08:33 - train: epoch 0010, iter [00900, 05004], lr: 0.098563, loss: 2.3903
2022-07-15 02:09:07 - train: epoch 0010, iter [01000, 05004], lr: 0.098557, loss: 2.4431
2022-07-15 02:09:40 - train: epoch 0010, iter [01100, 05004], lr: 0.098551, loss: 2.5332
2022-07-15 02:10:13 - train: epoch 0010, iter [01200, 05004], lr: 0.098544, loss: 2.4303
2022-07-15 02:10:46 - train: epoch 0010, iter [01300, 05004], lr: 0.098538, loss: 2.4149
2022-07-15 02:11:19 - train: epoch 0010, iter [01400, 05004], lr: 0.098532, loss: 2.6645
2022-07-15 02:11:52 - train: epoch 0010, iter [01500, 05004], lr: 0.098525, loss: 2.3022
2022-07-15 02:12:26 - train: epoch 0010, iter [01600, 05004], lr: 0.098519, loss: 2.6380
2022-07-15 02:12:59 - train: epoch 0010, iter [01700, 05004], lr: 0.098513, loss: 2.7434
2022-07-15 02:13:32 - train: epoch 0010, iter [01800, 05004], lr: 0.098506, loss: 2.5928
2022-07-15 02:14:05 - train: epoch 0010, iter [01900, 05004], lr: 0.098500, loss: 2.6519
2022-07-15 02:14:39 - train: epoch 0010, iter [02000, 05004], lr: 0.098494, loss: 2.6356
2022-07-15 02:15:12 - train: epoch 0010, iter [02100, 05004], lr: 0.098487, loss: 2.5332
2022-07-15 02:15:45 - train: epoch 0010, iter [02200, 05004], lr: 0.098481, loss: 2.7996
2022-07-15 02:16:17 - train: epoch 0010, iter [02300, 05004], lr: 0.098475, loss: 2.8600
2022-07-15 02:16:51 - train: epoch 0010, iter [02400, 05004], lr: 0.098468, loss: 2.7494
2022-07-15 02:17:24 - train: epoch 0010, iter [02500, 05004], lr: 0.098462, loss: 2.6108
2022-07-15 02:17:57 - train: epoch 0010, iter [02600, 05004], lr: 0.098455, loss: 2.5990
2022-07-15 02:18:30 - train: epoch 0010, iter [02700, 05004], lr: 0.098449, loss: 2.4745
2022-07-15 02:19:04 - train: epoch 0010, iter [02800, 05004], lr: 0.098442, loss: 2.6366
2022-07-15 02:19:37 - train: epoch 0010, iter [02900, 05004], lr: 0.098436, loss: 2.5993
2022-07-15 02:20:10 - train: epoch 0010, iter [03000, 05004], lr: 0.098429, loss: 2.5022
2022-07-15 02:20:43 - train: epoch 0010, iter [03100, 05004], lr: 0.098423, loss: 2.8077
2022-07-15 02:21:17 - train: epoch 0010, iter [03200, 05004], lr: 0.098416, loss: 2.5761
2022-07-15 02:21:50 - train: epoch 0010, iter [03300, 05004], lr: 0.098410, loss: 2.7231
2022-07-15 02:22:23 - train: epoch 0010, iter [03400, 05004], lr: 0.098403, loss: 2.7658
2022-07-15 02:22:56 - train: epoch 0010, iter [03500, 05004], lr: 0.098397, loss: 2.7632
2022-07-15 02:23:29 - train: epoch 0010, iter [03600, 05004], lr: 0.098390, loss: 2.7721
2022-07-15 02:24:02 - train: epoch 0010, iter [03700, 05004], lr: 0.098383, loss: 2.4967
2022-07-15 02:24:36 - train: epoch 0010, iter [03800, 05004], lr: 0.098377, loss: 2.6315
2022-07-15 02:25:09 - train: epoch 0010, iter [03900, 05004], lr: 0.098370, loss: 2.2437
2022-07-15 02:25:42 - train: epoch 0010, iter [04000, 05004], lr: 0.098364, loss: 2.5104
2022-07-15 02:26:15 - train: epoch 0010, iter [04100, 05004], lr: 0.098357, loss: 2.3604
2022-07-15 02:26:49 - train: epoch 0010, iter [04200, 05004], lr: 0.098350, loss: 2.6556
2022-07-15 02:27:22 - train: epoch 0010, iter [04300, 05004], lr: 0.098344, loss: 2.6286
2022-07-15 02:27:55 - train: epoch 0010, iter [04400, 05004], lr: 0.098337, loss: 2.5464
2022-07-15 02:28:29 - train: epoch 0010, iter [04500, 05004], lr: 0.098330, loss: 2.4848
2022-07-15 02:29:02 - train: epoch 0010, iter [04600, 05004], lr: 0.098324, loss: 2.5763
2022-07-15 02:29:35 - train: epoch 0010, iter [04700, 05004], lr: 0.098317, loss: 2.6253
2022-07-15 02:30:08 - train: epoch 0010, iter [04800, 05004], lr: 0.098310, loss: 2.5650
2022-07-15 02:30:42 - train: epoch 0010, iter [04900, 05004], lr: 0.098303, loss: 2.5062
2022-07-15 02:31:13 - train: epoch 0010, iter [05000, 05004], lr: 0.098297, loss: 2.2854
2022-07-15 02:31:14 - train: epoch 010, train_loss: 2.5781
2022-07-15 02:32:28 - eval: epoch: 010, acc1: 46.776%, acc5: 72.844%, test_loss: 2.3543, per_image_load_time: 1.031ms, per_image_inference_time: 0.194ms
2022-07-15 02:32:29 - until epoch: 010, best_acc1: 46.776%
2022-07-15 02:32:29 - epoch 011 lr: 0.098296
2022-07-15 02:33:07 - train: epoch 0011, iter [00100, 05004], lr: 0.098290, loss: 2.2975
2022-07-15 02:33:40 - train: epoch 0011, iter [00200, 05004], lr: 0.098283, loss: 2.7462
2022-07-15 02:34:13 - train: epoch 0011, iter [00300, 05004], lr: 0.098276, loss: 2.4114
2022-07-15 02:34:46 - train: epoch 0011, iter [00400, 05004], lr: 0.098269, loss: 2.6178
2022-07-15 02:35:19 - train: epoch 0011, iter [00500, 05004], lr: 0.098262, loss: 2.6022
2022-07-15 02:35:52 - train: epoch 0011, iter [00600, 05004], lr: 0.098255, loss: 2.5695
2022-07-15 02:36:26 - train: epoch 0011, iter [00700, 05004], lr: 0.098249, loss: 2.4673
2022-07-15 02:36:58 - train: epoch 0011, iter [00800, 05004], lr: 0.098242, loss: 2.5803
2022-07-15 02:37:31 - train: epoch 0011, iter [00900, 05004], lr: 0.098235, loss: 2.6346
2022-07-15 02:38:05 - train: epoch 0011, iter [01000, 05004], lr: 0.098228, loss: 2.4686
2022-07-15 02:38:38 - train: epoch 0011, iter [01100, 05004], lr: 0.098221, loss: 2.6327
2022-07-15 02:39:12 - train: epoch 0011, iter [01200, 05004], lr: 0.098214, loss: 2.9422
2022-07-15 02:39:45 - train: epoch 0011, iter [01300, 05004], lr: 0.098207, loss: 2.7759
2022-07-15 02:40:19 - train: epoch 0011, iter [01400, 05004], lr: 0.098200, loss: 2.5654
2022-07-15 02:40:52 - train: epoch 0011, iter [01500, 05004], lr: 0.098193, loss: 2.4148
2022-07-15 02:41:26 - train: epoch 0011, iter [01600, 05004], lr: 0.098186, loss: 2.5535
2022-07-15 02:41:59 - train: epoch 0011, iter [01700, 05004], lr: 0.098179, loss: 2.6040
2022-07-15 02:42:33 - train: epoch 0011, iter [01800, 05004], lr: 0.098172, loss: 2.4928
2022-07-15 02:43:06 - train: epoch 0011, iter [01900, 05004], lr: 0.098165, loss: 2.3067
2022-07-15 02:43:40 - train: epoch 0011, iter [02000, 05004], lr: 0.098158, loss: 2.6462
2022-07-15 02:44:12 - train: epoch 0011, iter [02100, 05004], lr: 0.098151, loss: 2.4804
2022-07-15 02:44:45 - train: epoch 0011, iter [02200, 05004], lr: 0.098144, loss: 2.4499
2022-07-15 02:45:19 - train: epoch 0011, iter [02300, 05004], lr: 0.098137, loss: 2.8379
2022-07-15 02:45:53 - train: epoch 0011, iter [02400, 05004], lr: 0.098130, loss: 2.3804
2022-07-15 02:46:25 - train: epoch 0011, iter [02500, 05004], lr: 0.098123, loss: 2.7376
2022-07-15 02:46:59 - train: epoch 0011, iter [02600, 05004], lr: 0.098116, loss: 2.5400
2022-07-15 02:47:33 - train: epoch 0011, iter [02700, 05004], lr: 0.098109, loss: 2.5651
2022-07-15 02:48:06 - train: epoch 0011, iter [02800, 05004], lr: 0.098102, loss: 2.2384
2022-07-15 02:48:39 - train: epoch 0011, iter [02900, 05004], lr: 0.098094, loss: 2.5678
2022-07-15 02:49:13 - train: epoch 0011, iter [03000, 05004], lr: 0.098087, loss: 2.7566
2022-07-15 02:49:46 - train: epoch 0011, iter [03100, 05004], lr: 0.098080, loss: 2.6210
2022-07-15 02:50:19 - train: epoch 0011, iter [03200, 05004], lr: 0.098073, loss: 2.3444
2022-07-15 02:50:52 - train: epoch 0011, iter [03300, 05004], lr: 0.098066, loss: 2.6582
2022-07-15 02:51:26 - train: epoch 0011, iter [03400, 05004], lr: 0.098058, loss: 2.5008
2022-07-15 02:51:59 - train: epoch 0011, iter [03500, 05004], lr: 0.098051, loss: 2.5462
2022-07-15 02:52:32 - train: epoch 0011, iter [03600, 05004], lr: 0.098044, loss: 2.5392
2022-07-15 02:53:06 - train: epoch 0011, iter [03700, 05004], lr: 0.098037, loss: 2.7124
2022-07-15 02:53:39 - train: epoch 0011, iter [03800, 05004], lr: 0.098029, loss: 2.3181
2022-07-15 02:54:12 - train: epoch 0011, iter [03900, 05004], lr: 0.098022, loss: 2.5277
2022-07-15 02:54:46 - train: epoch 0011, iter [04000, 05004], lr: 0.098015, loss: 2.5158
2022-07-15 02:55:19 - train: epoch 0011, iter [04100, 05004], lr: 0.098008, loss: 2.3578
2022-07-15 02:55:53 - train: epoch 0011, iter [04200, 05004], lr: 0.098000, loss: 2.5131
2022-07-15 02:56:26 - train: epoch 0011, iter [04300, 05004], lr: 0.097993, loss: 2.5243
2022-07-15 02:56:59 - train: epoch 0011, iter [04400, 05004], lr: 0.097986, loss: 2.5371
2022-07-15 02:57:32 - train: epoch 0011, iter [04500, 05004], lr: 0.097978, loss: 2.4209
2022-07-15 02:58:06 - train: epoch 0011, iter [04600, 05004], lr: 0.097971, loss: 2.3777
2022-07-15 02:58:38 - train: epoch 0011, iter [04700, 05004], lr: 0.097964, loss: 2.2453
2022-07-15 02:59:13 - train: epoch 0011, iter [04800, 05004], lr: 0.097956, loss: 2.3670
2022-07-15 02:59:46 - train: epoch 0011, iter [04900, 05004], lr: 0.097949, loss: 2.3666
2022-07-15 03:00:17 - train: epoch 0011, iter [05000, 05004], lr: 0.097941, loss: 2.4567
2022-07-15 03:00:18 - train: epoch 011, train_loss: 2.5476
2022-07-15 03:01:33 - eval: epoch: 011, acc1: 47.486%, acc5: 73.410%, test_loss: 2.3140, per_image_load_time: 0.870ms, per_image_inference_time: 0.225ms
2022-07-15 03:01:33 - until epoch: 011, best_acc1: 47.486%
2022-07-15 03:01:33 - epoch 012 lr: 0.097941
2022-07-15 03:02:12 - train: epoch 0012, iter [00100, 05004], lr: 0.097934, loss: 2.5342
2022-07-15 03:02:45 - train: epoch 0012, iter [00200, 05004], lr: 0.097926, loss: 2.4458
2022-07-15 03:03:18 - train: epoch 0012, iter [00300, 05004], lr: 0.097919, loss: 2.5415
2022-07-15 03:03:52 - train: epoch 0012, iter [00400, 05004], lr: 0.097911, loss: 2.5949
2022-07-15 03:04:25 - train: epoch 0012, iter [00500, 05004], lr: 0.097904, loss: 2.7175
2022-07-15 03:04:58 - train: epoch 0012, iter [00600, 05004], lr: 0.097896, loss: 2.1670
2022-07-15 03:05:31 - train: epoch 0012, iter [00700, 05004], lr: 0.097889, loss: 2.4368
2022-07-15 03:06:05 - train: epoch 0012, iter [00800, 05004], lr: 0.097881, loss: 2.5294
2022-07-15 03:06:38 - train: epoch 0012, iter [00900, 05004], lr: 0.097874, loss: 2.6457
2022-07-15 03:07:11 - train: epoch 0012, iter [01000, 05004], lr: 0.097866, loss: 2.4255
2022-07-15 03:07:44 - train: epoch 0012, iter [01100, 05004], lr: 0.097858, loss: 2.8305
2022-07-15 03:08:17 - train: epoch 0012, iter [01200, 05004], lr: 0.097851, loss: 2.3421
2022-07-15 03:08:51 - train: epoch 0012, iter [01300, 05004], lr: 0.097843, loss: 2.4788
2022-07-15 03:09:25 - train: epoch 0012, iter [01400, 05004], lr: 0.097836, loss: 2.6411
2022-07-15 03:09:58 - train: epoch 0012, iter [01500, 05004], lr: 0.097828, loss: 2.3258
2022-07-15 03:10:31 - train: epoch 0012, iter [01600, 05004], lr: 0.097820, loss: 2.3635
2022-07-15 03:11:05 - train: epoch 0012, iter [01700, 05004], lr: 0.097813, loss: 2.4631
2022-07-15 03:11:38 - train: epoch 0012, iter [01800, 05004], lr: 0.097805, loss: 2.4832
2022-07-15 03:12:11 - train: epoch 0012, iter [01900, 05004], lr: 0.097797, loss: 2.5856
2022-07-15 03:12:44 - train: epoch 0012, iter [02000, 05004], lr: 0.097790, loss: 2.6724
2022-07-15 03:13:18 - train: epoch 0012, iter [02100, 05004], lr: 0.097782, loss: 2.5675
2022-07-15 03:13:50 - train: epoch 0012, iter [02200, 05004], lr: 0.097774, loss: 2.7117
2022-07-15 03:14:24 - train: epoch 0012, iter [02300, 05004], lr: 0.097767, loss: 2.4870
2022-07-15 03:14:57 - train: epoch 0012, iter [02400, 05004], lr: 0.097759, loss: 2.6249
2022-07-15 03:15:31 - train: epoch 0012, iter [02500, 05004], lr: 0.097751, loss: 2.3030
2022-07-15 03:16:04 - train: epoch 0012, iter [02600, 05004], lr: 0.097743, loss: 2.2171
2022-07-15 03:16:37 - train: epoch 0012, iter [02700, 05004], lr: 0.097736, loss: 2.4019
2022-07-15 03:17:11 - train: epoch 0012, iter [02800, 05004], lr: 0.097728, loss: 2.5176
2022-07-15 03:17:44 - train: epoch 0012, iter [02900, 05004], lr: 0.097720, loss: 2.2671
2022-07-15 03:18:17 - train: epoch 0012, iter [03000, 05004], lr: 0.097712, loss: 2.4900
2022-07-15 03:18:51 - train: epoch 0012, iter [03100, 05004], lr: 0.097704, loss: 2.6024
2022-07-15 03:19:24 - train: epoch 0012, iter [03200, 05004], lr: 0.097697, loss: 2.1817
2022-07-15 03:19:58 - train: epoch 0012, iter [03300, 05004], lr: 0.097689, loss: 2.4885
2022-07-15 03:20:30 - train: epoch 0012, iter [03400, 05004], lr: 0.097681, loss: 2.4908
2022-07-15 03:21:04 - train: epoch 0012, iter [03500, 05004], lr: 0.097673, loss: 2.6309
2022-07-15 03:21:37 - train: epoch 0012, iter [03600, 05004], lr: 0.097665, loss: 2.4475
2022-07-15 03:22:11 - train: epoch 0012, iter [03700, 05004], lr: 0.097657, loss: 2.4733
2022-07-15 03:22:43 - train: epoch 0012, iter [03800, 05004], lr: 0.097649, loss: 2.6051
2022-07-15 03:23:17 - train: epoch 0012, iter [03900, 05004], lr: 0.097641, loss: 2.4400
2022-07-15 03:23:50 - train: epoch 0012, iter [04000, 05004], lr: 0.097633, loss: 2.4559
2022-07-15 03:24:24 - train: epoch 0012, iter [04100, 05004], lr: 0.097625, loss: 2.4679
2022-07-15 03:24:57 - train: epoch 0012, iter [04200, 05004], lr: 0.097617, loss: 2.3524
2022-07-15 03:25:31 - train: epoch 0012, iter [04300, 05004], lr: 0.097609, loss: 2.6957
2022-07-15 03:26:04 - train: epoch 0012, iter [04400, 05004], lr: 0.097601, loss: 2.3804
2022-07-15 03:26:37 - train: epoch 0012, iter [04500, 05004], lr: 0.097593, loss: 2.3971
2022-07-15 03:27:11 - train: epoch 0012, iter [04600, 05004], lr: 0.097585, loss: 2.7998
2022-07-15 03:27:44 - train: epoch 0012, iter [04700, 05004], lr: 0.097577, loss: 2.4343
2022-07-15 03:28:18 - train: epoch 0012, iter [04800, 05004], lr: 0.097569, loss: 2.6247
2022-07-15 03:28:51 - train: epoch 0012, iter [04900, 05004], lr: 0.097561, loss: 2.5121
2022-07-15 03:29:22 - train: epoch 0012, iter [05000, 05004], lr: 0.097553, loss: 2.2440
2022-07-15 03:29:23 - train: epoch 012, train_loss: 2.5169
2022-07-15 03:30:37 - eval: epoch: 012, acc1: 48.184%, acc5: 74.078%, test_loss: 2.2837, per_image_load_time: 1.186ms, per_image_inference_time: 0.208ms
2022-07-15 03:30:38 - until epoch: 012, best_acc1: 48.184%
2022-07-15 03:30:38 - epoch 013 lr: 0.097553
2022-07-15 03:31:17 - train: epoch 0013, iter [00100, 05004], lr: 0.097545, loss: 2.3233
2022-07-15 03:31:49 - train: epoch 0013, iter [00200, 05004], lr: 0.097537, loss: 2.4985
2022-07-15 03:32:23 - train: epoch 0013, iter [00300, 05004], lr: 0.097529, loss: 2.4190
2022-07-15 03:32:55 - train: epoch 0013, iter [00400, 05004], lr: 0.097520, loss: 2.4255
2022-07-15 03:33:29 - train: epoch 0013, iter [00500, 05004], lr: 0.097512, loss: 2.4029
2022-07-15 03:34:02 - train: epoch 0013, iter [00600, 05004], lr: 0.097504, loss: 2.5763
2022-07-15 03:34:36 - train: epoch 0013, iter [00700, 05004], lr: 0.097496, loss: 2.3476
2022-07-15 03:35:08 - train: epoch 0013, iter [00800, 05004], lr: 0.097488, loss: 2.6332
2022-07-15 03:35:42 - train: epoch 0013, iter [00900, 05004], lr: 0.097480, loss: 2.4662
2022-07-15 03:36:15 - train: epoch 0013, iter [01000, 05004], lr: 0.097471, loss: 2.5494
2022-07-15 03:36:49 - train: epoch 0013, iter [01100, 05004], lr: 0.097463, loss: 2.5195
2022-07-15 03:37:23 - train: epoch 0013, iter [01200, 05004], lr: 0.097455, loss: 2.7230
2022-07-15 03:37:56 - train: epoch 0013, iter [01300, 05004], lr: 0.097447, loss: 2.5681
2022-07-15 03:38:30 - train: epoch 0013, iter [01400, 05004], lr: 0.097438, loss: 2.3877
2022-07-15 03:39:03 - train: epoch 0013, iter [01500, 05004], lr: 0.097430, loss: 2.7439
2022-07-15 03:39:37 - train: epoch 0013, iter [01600, 05004], lr: 0.097422, loss: 2.2371
2022-07-15 03:40:10 - train: epoch 0013, iter [01700, 05004], lr: 0.097414, loss: 2.5126
2022-07-15 03:40:43 - train: epoch 0013, iter [01800, 05004], lr: 0.097405, loss: 2.4874
2022-07-15 03:41:16 - train: epoch 0013, iter [01900, 05004], lr: 0.097397, loss: 2.5251
2022-07-15 03:41:49 - train: epoch 0013, iter [02000, 05004], lr: 0.097389, loss: 2.7349
2022-07-15 03:42:23 - train: epoch 0013, iter [02100, 05004], lr: 0.097380, loss: 2.7683
2022-07-15 03:42:57 - train: epoch 0013, iter [02200, 05004], lr: 0.097372, loss: 2.3836
2022-07-15 03:43:29 - train: epoch 0013, iter [02300, 05004], lr: 0.097363, loss: 2.5816
2022-07-15 03:44:03 - train: epoch 0013, iter [02400, 05004], lr: 0.097355, loss: 2.5788
2022-07-15 03:44:35 - train: epoch 0013, iter [02500, 05004], lr: 0.097347, loss: 2.4141
2022-07-15 03:45:09 - train: epoch 0013, iter [02600, 05004], lr: 0.097338, loss: 2.3908
2022-07-15 03:45:41 - train: epoch 0013, iter [02700, 05004], lr: 0.097330, loss: 2.3310
2022-07-15 03:46:15 - train: epoch 0013, iter [02800, 05004], lr: 0.097321, loss: 2.5599
2022-07-15 03:46:47 - train: epoch 0013, iter [02900, 05004], lr: 0.097313, loss: 2.5474
2022-07-15 03:47:21 - train: epoch 0013, iter [03000, 05004], lr: 0.097304, loss: 2.3435
2022-07-15 03:47:53 - train: epoch 0013, iter [03100, 05004], lr: 0.097296, loss: 2.3164
2022-07-15 03:48:26 - train: epoch 0013, iter [03200, 05004], lr: 0.097287, loss: 2.4909
2022-07-15 03:48:58 - train: epoch 0013, iter [03300, 05004], lr: 0.097279, loss: 2.4743
2022-07-15 03:49:32 - train: epoch 0013, iter [03400, 05004], lr: 0.097270, loss: 2.3876
2022-07-15 03:50:03 - train: epoch 0013, iter [03500, 05004], lr: 0.097262, loss: 2.3347
2022-07-15 03:50:37 - train: epoch 0013, iter [03600, 05004], lr: 0.097253, loss: 2.7262
2022-07-15 03:51:08 - train: epoch 0013, iter [03700, 05004], lr: 0.097245, loss: 2.2298
2022-07-15 03:51:41 - train: epoch 0013, iter [03800, 05004], lr: 0.097236, loss: 2.6515
2022-07-15 03:52:13 - train: epoch 0013, iter [03900, 05004], lr: 0.097228, loss: 2.5954
2022-07-15 03:52:47 - train: epoch 0013, iter [04000, 05004], lr: 0.097219, loss: 2.5492
2022-07-15 03:53:19 - train: epoch 0013, iter [04100, 05004], lr: 0.097210, loss: 2.3775
2022-07-15 03:53:53 - train: epoch 0013, iter [04200, 05004], lr: 0.097202, loss: 2.4943
2022-07-15 03:54:25 - train: epoch 0013, iter [04300, 05004], lr: 0.097193, loss: 2.3671
2022-07-15 03:54:58 - train: epoch 0013, iter [04400, 05004], lr: 0.097185, loss: 2.3334
2022-07-15 03:55:30 - train: epoch 0013, iter [04500, 05004], lr: 0.097176, loss: 2.4440
2022-07-15 03:56:03 - train: epoch 0013, iter [04600, 05004], lr: 0.097167, loss: 2.3801
2022-07-15 03:56:35 - train: epoch 0013, iter [04700, 05004], lr: 0.097159, loss: 2.5564
2022-07-15 03:57:08 - train: epoch 0013, iter [04800, 05004], lr: 0.097150, loss: 2.5721
2022-07-15 03:57:40 - train: epoch 0013, iter [04900, 05004], lr: 0.097141, loss: 2.5374
2022-07-15 03:58:11 - train: epoch 0013, iter [05000, 05004], lr: 0.097132, loss: 2.5659
2022-07-15 03:58:12 - train: epoch 013, train_loss: 2.4904
2022-07-15 03:59:26 - eval: epoch: 013, acc1: 49.532%, acc5: 75.166%, test_loss: 2.2042, per_image_load_time: 0.851ms, per_image_inference_time: 0.209ms
2022-07-15 03:59:26 - until epoch: 013, best_acc1: 49.532%
2022-07-15 03:59:26 - epoch 014 lr: 0.097132
2022-07-15 04:00:03 - train: epoch 0014, iter [00100, 05004], lr: 0.097123, loss: 2.4500
2022-07-15 04:00:37 - train: epoch 0014, iter [00200, 05004], lr: 0.097115, loss: 2.5451
2022-07-15 04:01:09 - train: epoch 0014, iter [00300, 05004], lr: 0.097106, loss: 2.1811
2022-07-15 04:01:42 - train: epoch 0014, iter [00400, 05004], lr: 0.097097, loss: 2.2492
2022-07-15 04:02:15 - train: epoch 0014, iter [00500, 05004], lr: 0.097088, loss: 2.3992
2022-07-15 04:02:48 - train: epoch 0014, iter [00600, 05004], lr: 0.097079, loss: 2.4488
2022-07-15 04:03:20 - train: epoch 0014, iter [00700, 05004], lr: 0.097071, loss: 2.4402
2022-07-15 04:03:53 - train: epoch 0014, iter [00800, 05004], lr: 0.097062, loss: 2.5398
2022-07-15 04:04:26 - train: epoch 0014, iter [00900, 05004], lr: 0.097053, loss: 2.6151
2022-07-15 04:04:58 - train: epoch 0014, iter [01000, 05004], lr: 0.097044, loss: 2.5884
2022-07-15 04:05:31 - train: epoch 0014, iter [01100, 05004], lr: 0.097035, loss: 2.3892
2022-07-15 04:06:04 - train: epoch 0014, iter [01200, 05004], lr: 0.097026, loss: 2.4685
2022-07-15 04:06:36 - train: epoch 0014, iter [01300, 05004], lr: 0.097017, loss: 2.5239
2022-07-15 04:07:10 - train: epoch 0014, iter [01400, 05004], lr: 0.097009, loss: 2.4221
2022-07-15 04:07:42 - train: epoch 0014, iter [01500, 05004], lr: 0.097000, loss: 2.5696
2022-07-15 04:08:16 - train: epoch 0014, iter [01600, 05004], lr: 0.096991, loss: 2.3962
2022-07-15 04:08:48 - train: epoch 0014, iter [01700, 05004], lr: 0.096982, loss: 2.6916
2022-07-15 04:09:21 - train: epoch 0014, iter [01800, 05004], lr: 0.096973, loss: 2.6718
2022-07-15 04:09:55 - train: epoch 0014, iter [01900, 05004], lr: 0.096964, loss: 2.3344
2022-07-15 04:10:27 - train: epoch 0014, iter [02000, 05004], lr: 0.096955, loss: 2.3377
2022-07-15 04:10:59 - train: epoch 0014, iter [02100, 05004], lr: 0.096946, loss: 2.5402
2022-07-15 04:11:33 - train: epoch 0014, iter [02200, 05004], lr: 0.096937, loss: 2.4241
2022-07-15 04:12:04 - train: epoch 0014, iter [02300, 05004], lr: 0.096928, loss: 2.4461
2022-07-15 04:12:38 - train: epoch 0014, iter [02400, 05004], lr: 0.096919, loss: 2.7024
2022-07-15 04:13:11 - train: epoch 0014, iter [02500, 05004], lr: 0.096910, loss: 2.4443
2022-07-15 04:13:44 - train: epoch 0014, iter [02600, 05004], lr: 0.096901, loss: 2.5074
2022-07-15 04:14:17 - train: epoch 0014, iter [02700, 05004], lr: 0.096892, loss: 2.3314
2022-07-15 04:14:50 - train: epoch 0014, iter [02800, 05004], lr: 0.096883, loss: 2.7217
2022-07-15 04:15:23 - train: epoch 0014, iter [02900, 05004], lr: 0.096873, loss: 2.3303
2022-07-15 04:15:55 - train: epoch 0014, iter [03000, 05004], lr: 0.096864, loss: 2.6052
2022-07-15 04:16:28 - train: epoch 0014, iter [03100, 05004], lr: 0.096855, loss: 2.2143
2022-07-15 04:17:00 - train: epoch 0014, iter [03200, 05004], lr: 0.096846, loss: 2.4267
2022-07-15 04:17:34 - train: epoch 0014, iter [03300, 05004], lr: 0.096837, loss: 2.3234
2022-07-15 04:18:06 - train: epoch 0014, iter [03400, 05004], lr: 0.096828, loss: 2.4044
2022-07-15 04:18:40 - train: epoch 0014, iter [03500, 05004], lr: 0.096819, loss: 2.4347
2022-07-15 04:19:12 - train: epoch 0014, iter [03600, 05004], lr: 0.096809, loss: 2.3236
2022-07-15 04:19:45 - train: epoch 0014, iter [03700, 05004], lr: 0.096800, loss: 2.4026
2022-07-15 04:20:17 - train: epoch 0014, iter [03800, 05004], lr: 0.096791, loss: 2.7696
2022-07-15 04:20:50 - train: epoch 0014, iter [03900, 05004], lr: 0.096782, loss: 2.3717
2022-07-15 04:21:23 - train: epoch 0014, iter [04000, 05004], lr: 0.096772, loss: 2.5937
2022-07-15 04:21:56 - train: epoch 0014, iter [04100, 05004], lr: 0.096763, loss: 2.4474
2022-07-15 04:22:29 - train: epoch 0014, iter [04200, 05004], lr: 0.096754, loss: 2.3455
2022-07-15 04:23:02 - train: epoch 0014, iter [04300, 05004], lr: 0.096745, loss: 2.1920
2022-07-15 04:23:34 - train: epoch 0014, iter [04400, 05004], lr: 0.096735, loss: 2.2478
2022-07-15 04:24:08 - train: epoch 0014, iter [04500, 05004], lr: 0.096726, loss: 2.4485
2022-07-15 04:24:40 - train: epoch 0014, iter [04600, 05004], lr: 0.096717, loss: 2.3188
2022-07-15 04:25:13 - train: epoch 0014, iter [04700, 05004], lr: 0.096707, loss: 2.3762
2022-07-15 04:25:45 - train: epoch 0014, iter [04800, 05004], lr: 0.096698, loss: 2.3188
2022-07-15 04:26:19 - train: epoch 0014, iter [04900, 05004], lr: 0.096689, loss: 2.1839
2022-07-15 04:26:50 - train: epoch 0014, iter [05000, 05004], lr: 0.096679, loss: 2.4063
2022-07-15 04:26:51 - train: epoch 014, train_loss: 2.4677
2022-07-15 04:28:04 - eval: epoch: 014, acc1: 48.202%, acc5: 74.110%, test_loss: 2.2895, per_image_load_time: 1.004ms, per_image_inference_time: 0.212ms
2022-07-15 04:28:04 - until epoch: 014, best_acc1: 49.532%
2022-07-15 04:28:04 - epoch 015 lr: 0.096679
2022-07-15 04:28:42 - train: epoch 0015, iter [00100, 05004], lr: 0.096670, loss: 2.3086
2022-07-15 04:29:15 - train: epoch 0015, iter [00200, 05004], lr: 0.096660, loss: 2.6194
2022-07-15 04:29:47 - train: epoch 0015, iter [00300, 05004], lr: 0.096651, loss: 2.5998
2022-07-15 04:30:20 - train: epoch 0015, iter [00400, 05004], lr: 0.096641, loss: 2.4197
2022-07-15 04:30:52 - train: epoch 0015, iter [00500, 05004], lr: 0.096632, loss: 2.3967
2022-07-15 04:31:25 - train: epoch 0015, iter [00600, 05004], lr: 0.096623, loss: 2.6063
2022-07-15 04:31:58 - train: epoch 0015, iter [00700, 05004], lr: 0.096613, loss: 2.3841
2022-07-15 04:32:31 - train: epoch 0015, iter [00800, 05004], lr: 0.096604, loss: 2.2719
2022-07-15 04:33:04 - train: epoch 0015, iter [00900, 05004], lr: 0.096594, loss: 2.3061
2022-07-15 04:33:36 - train: epoch 0015, iter [01000, 05004], lr: 0.096585, loss: 2.4746
2022-07-15 04:34:10 - train: epoch 0015, iter [01100, 05004], lr: 0.096575, loss: 2.3554
2022-07-15 04:34:42 - train: epoch 0015, iter [01200, 05004], lr: 0.096566, loss: 2.4576
2022-07-15 04:35:15 - train: epoch 0015, iter [01300, 05004], lr: 0.096556, loss: 2.6757
2022-07-15 04:35:48 - train: epoch 0015, iter [01400, 05004], lr: 0.096547, loss: 2.2969
2022-07-15 04:36:21 - train: epoch 0015, iter [01500, 05004], lr: 0.096537, loss: 2.2457
2022-07-15 04:36:53 - train: epoch 0015, iter [01600, 05004], lr: 0.096527, loss: 2.4290
2022-07-15 04:37:26 - train: epoch 0015, iter [01700, 05004], lr: 0.096518, loss: 2.5832
2022-07-15 04:37:58 - train: epoch 0015, iter [01800, 05004], lr: 0.096508, loss: 2.3585
2022-07-15 04:38:31 - train: epoch 0015, iter [01900, 05004], lr: 0.096499, loss: 2.3028
2022-07-15 04:39:04 - train: epoch 0015, iter [02000, 05004], lr: 0.096489, loss: 2.4019
2022-07-15 04:39:37 - train: epoch 0015, iter [02100, 05004], lr: 0.096479, loss: 2.2740
2022-07-15 04:40:09 - train: epoch 0015, iter [02200, 05004], lr: 0.096470, loss: 2.7513
2022-07-15 04:40:42 - train: epoch 0015, iter [02300, 05004], lr: 0.096460, loss: 2.1868
2022-07-15 04:41:15 - train: epoch 0015, iter [02400, 05004], lr: 0.096450, loss: 2.5262
2022-07-15 04:41:48 - train: epoch 0015, iter [02500, 05004], lr: 0.096441, loss: 2.5812
2022-07-15 04:42:21 - train: epoch 0015, iter [02600, 05004], lr: 0.096431, loss: 2.2599
2022-07-15 04:42:54 - train: epoch 0015, iter [02700, 05004], lr: 0.096421, loss: 2.4631
2022-07-15 04:43:27 - train: epoch 0015, iter [02800, 05004], lr: 0.096412, loss: 2.5797
2022-07-15 04:43:59 - train: epoch 0015, iter [02900, 05004], lr: 0.096402, loss: 2.4542
2022-07-15 04:44:33 - train: epoch 0015, iter [03000, 05004], lr: 0.096392, loss: 2.2912
2022-07-15 04:45:05 - train: epoch 0015, iter [03100, 05004], lr: 0.096382, loss: 2.3597
2022-07-15 04:45:37 - train: epoch 0015, iter [03200, 05004], lr: 0.096373, loss: 2.3656
2022-07-15 04:46:11 - train: epoch 0015, iter [03300, 05004], lr: 0.096363, loss: 2.2668
2022-07-15 04:46:44 - train: epoch 0015, iter [03400, 05004], lr: 0.096353, loss: 2.5523
2022-07-15 04:47:17 - train: epoch 0015, iter [03500, 05004], lr: 0.096343, loss: 2.6091
2022-07-15 04:47:50 - train: epoch 0015, iter [03600, 05004], lr: 0.096333, loss: 2.3937
2022-07-15 04:48:24 - train: epoch 0015, iter [03700, 05004], lr: 0.096323, loss: 2.3282
2022-07-15 04:48:56 - train: epoch 0015, iter [03800, 05004], lr: 0.096314, loss: 2.4998
2022-07-15 04:49:29 - train: epoch 0015, iter [03900, 05004], lr: 0.096304, loss: 2.5717
2022-07-15 04:50:01 - train: epoch 0015, iter [04000, 05004], lr: 0.096294, loss: 2.4459
2022-07-15 04:50:34 - train: epoch 0015, iter [04100, 05004], lr: 0.096284, loss: 2.6086
2022-07-15 04:51:06 - train: epoch 0015, iter [04200, 05004], lr: 0.096274, loss: 2.1618
2022-07-15 04:51:40 - train: epoch 0015, iter [04300, 05004], lr: 0.096264, loss: 2.4465
2022-07-15 04:52:12 - train: epoch 0015, iter [04400, 05004], lr: 0.096254, loss: 2.4106
2022-07-15 04:52:45 - train: epoch 0015, iter [04500, 05004], lr: 0.096244, loss: 2.4679
2022-07-15 04:53:18 - train: epoch 0015, iter [04600, 05004], lr: 0.096234, loss: 2.4267
2022-07-15 04:53:51 - train: epoch 0015, iter [04700, 05004], lr: 0.096224, loss: 2.5518
2022-07-15 04:54:23 - train: epoch 0015, iter [04800, 05004], lr: 0.096214, loss: 2.3743
2022-07-15 04:54:56 - train: epoch 0015, iter [04900, 05004], lr: 0.096204, loss: 2.4155
2022-07-15 04:55:27 - train: epoch 0015, iter [05000, 05004], lr: 0.096194, loss: 2.6268
2022-07-15 04:55:28 - train: epoch 015, train_loss: 2.4507
2022-07-15 04:56:41 - eval: epoch: 015, acc1: 49.242%, acc5: 74.622%, test_loss: 2.2398, per_image_load_time: 1.040ms, per_image_inference_time: 0.201ms
2022-07-15 04:56:42 - until epoch: 015, best_acc1: 49.532%
2022-07-15 04:56:42 - epoch 016 lr: 0.096194
2022-07-15 04:57:19 - train: epoch 0016, iter [00100, 05004], lr: 0.096184, loss: 2.4877
2022-07-15 04:57:53 - train: epoch 0016, iter [00200, 05004], lr: 0.096174, loss: 2.2280
2022-07-15 04:58:25 - train: epoch 0016, iter [00300, 05004], lr: 0.096164, loss: 2.4071
2022-07-15 04:58:58 - train: epoch 0016, iter [00400, 05004], lr: 0.096154, loss: 2.6274
2022-07-15 04:59:31 - train: epoch 0016, iter [00500, 05004], lr: 0.096144, loss: 2.2787
2022-07-15 05:00:04 - train: epoch 0016, iter [00600, 05004], lr: 0.096134, loss: 2.5917
2022-07-15 05:00:36 - train: epoch 0016, iter [00700, 05004], lr: 0.096124, loss: 2.1353
2022-07-15 05:01:10 - train: epoch 0016, iter [00800, 05004], lr: 0.096113, loss: 2.4515
2022-07-15 05:01:42 - train: epoch 0016, iter [00900, 05004], lr: 0.096103, loss: 2.5965
2022-07-15 05:02:15 - train: epoch 0016, iter [01000, 05004], lr: 0.096093, loss: 2.2883
2022-07-15 05:02:48 - train: epoch 0016, iter [01100, 05004], lr: 0.096083, loss: 2.3413
2022-07-15 05:03:22 - train: epoch 0016, iter [01200, 05004], lr: 0.096073, loss: 2.1864
2022-07-15 05:03:54 - train: epoch 0016, iter [01300, 05004], lr: 0.096063, loss: 2.4654
2022-07-15 05:04:28 - train: epoch 0016, iter [01400, 05004], lr: 0.096053, loss: 2.2629
2022-07-15 05:05:00 - train: epoch 0016, iter [01500, 05004], lr: 0.096042, loss: 2.5261
2022-07-15 05:05:33 - train: epoch 0016, iter [01600, 05004], lr: 0.096032, loss: 2.5572
2022-07-15 05:06:06 - train: epoch 0016, iter [01700, 05004], lr: 0.096022, loss: 2.3621
2022-07-15 05:06:39 - train: epoch 0016, iter [01800, 05004], lr: 0.096012, loss: 2.3221
2022-07-15 05:07:13 - train: epoch 0016, iter [01900, 05004], lr: 0.096001, loss: 2.3555
2022-07-15 05:07:46 - train: epoch 0016, iter [02000, 05004], lr: 0.095991, loss: 2.1020
2022-07-15 05:08:19 - train: epoch 0016, iter [02100, 05004], lr: 0.095981, loss: 2.6296
2022-07-15 05:08:52 - train: epoch 0016, iter [02200, 05004], lr: 0.095971, loss: 2.5043
2022-07-15 05:09:25 - train: epoch 0016, iter [02300, 05004], lr: 0.095960, loss: 2.7477
2022-07-15 05:09:59 - train: epoch 0016, iter [02400, 05004], lr: 0.095950, loss: 2.5561
2022-07-15 05:10:31 - train: epoch 0016, iter [02500, 05004], lr: 0.095940, loss: 2.2844
2022-07-15 05:11:04 - train: epoch 0016, iter [02600, 05004], lr: 0.095929, loss: 2.5070
2022-07-15 05:11:38 - train: epoch 0016, iter [02700, 05004], lr: 0.095919, loss: 2.4311
2022-07-15 05:12:10 - train: epoch 0016, iter [02800, 05004], lr: 0.095909, loss: 2.2792
2022-07-15 05:12:45 - train: epoch 0016, iter [02900, 05004], lr: 0.095898, loss: 2.5620
2022-07-15 05:13:18 - train: epoch 0016, iter [03000, 05004], lr: 0.095888, loss: 2.7179
2022-07-15 05:13:51 - train: epoch 0016, iter [03100, 05004], lr: 0.095878, loss: 2.5740
2022-07-15 05:14:24 - train: epoch 0016, iter [03200, 05004], lr: 0.095867, loss: 2.6491
2022-07-15 05:14:58 - train: epoch 0016, iter [03300, 05004], lr: 0.095857, loss: 2.4708
2022-07-15 05:15:31 - train: epoch 0016, iter [03400, 05004], lr: 0.095846, loss: 2.4554
2022-07-15 05:16:05 - train: epoch 0016, iter [03500, 05004], lr: 0.095836, loss: 2.3890
2022-07-15 05:16:38 - train: epoch 0016, iter [03600, 05004], lr: 0.095825, loss: 2.3355
2022-07-15 05:17:11 - train: epoch 0016, iter [03700, 05004], lr: 0.095815, loss: 2.5031
2022-07-15 05:17:44 - train: epoch 0016, iter [03800, 05004], lr: 0.095804, loss: 2.7904
2022-07-15 05:18:18 - train: epoch 0016, iter [03900, 05004], lr: 0.095794, loss: 2.4447
2022-07-15 05:18:50 - train: epoch 0016, iter [04000, 05004], lr: 0.095783, loss: 2.4883
2022-07-15 05:19:23 - train: epoch 0016, iter [04100, 05004], lr: 0.095773, loss: 2.4385
2022-07-15 05:19:56 - train: epoch 0016, iter [04200, 05004], lr: 0.095762, loss: 2.4960
2022-07-15 05:20:30 - train: epoch 0016, iter [04300, 05004], lr: 0.095752, loss: 2.3146
2022-07-15 05:21:03 - train: epoch 0016, iter [04400, 05004], lr: 0.095741, loss: 2.3690
2022-07-15 05:21:36 - train: epoch 0016, iter [04500, 05004], lr: 0.095731, loss: 2.4955
2022-07-15 05:22:09 - train: epoch 0016, iter [04600, 05004], lr: 0.095720, loss: 2.3655
2022-07-15 05:22:42 - train: epoch 0016, iter [04700, 05004], lr: 0.095710, loss: 2.6530
2022-07-15 05:23:16 - train: epoch 0016, iter [04800, 05004], lr: 0.095699, loss: 2.3407
2022-07-15 05:23:49 - train: epoch 0016, iter [04900, 05004], lr: 0.095688, loss: 2.4326
2022-07-15 05:24:21 - train: epoch 0016, iter [05000, 05004], lr: 0.095678, loss: 2.4536
2022-07-15 05:24:22 - train: epoch 016, train_loss: 2.4318
2022-07-15 05:25:36 - eval: epoch: 016, acc1: 50.038%, acc5: 75.638%, test_loss: 2.1828, per_image_load_time: 2.659ms, per_image_inference_time: 0.223ms
2022-07-15 05:25:36 - until epoch: 016, best_acc1: 50.038%
2022-07-15 05:25:36 - epoch 017 lr: 0.095677
2022-07-15 05:26:14 - train: epoch 0017, iter [00100, 05004], lr: 0.095667, loss: 2.3078
2022-07-15 05:26:47 - train: epoch 0017, iter [00200, 05004], lr: 0.095656, loss: 2.4699
2022-07-15 05:27:20 - train: epoch 0017, iter [00300, 05004], lr: 0.095645, loss: 2.6107
2022-07-15 05:27:53 - train: epoch 0017, iter [00400, 05004], lr: 0.095635, loss: 2.2094
2022-07-15 05:28:27 - train: epoch 0017, iter [00500, 05004], lr: 0.095624, loss: 2.3194
2022-07-15 05:28:59 - train: epoch 0017, iter [00600, 05004], lr: 0.095613, loss: 2.8001
2022-07-15 05:29:31 - train: epoch 0017, iter [00700, 05004], lr: 0.095602, loss: 2.3610
2022-07-15 05:30:04 - train: epoch 0017, iter [00800, 05004], lr: 0.095592, loss: 2.3960
2022-07-15 05:30:37 - train: epoch 0017, iter [00900, 05004], lr: 0.095581, loss: 2.3647
2022-07-15 05:31:10 - train: epoch 0017, iter [01000, 05004], lr: 0.095570, loss: 2.2909
2022-07-15 05:31:44 - train: epoch 0017, iter [01100, 05004], lr: 0.095559, loss: 2.7266
2022-07-15 05:32:17 - train: epoch 0017, iter [01200, 05004], lr: 0.095549, loss: 2.6100
2022-07-15 05:32:51 - train: epoch 0017, iter [01300, 05004], lr: 0.095538, loss: 2.4060
2022-07-15 05:33:24 - train: epoch 0017, iter [01400, 05004], lr: 0.095527, loss: 2.5760
2022-07-15 05:33:56 - train: epoch 0017, iter [01500, 05004], lr: 0.095516, loss: 2.1689
2022-07-15 05:34:29 - train: epoch 0017, iter [01600, 05004], lr: 0.095505, loss: 2.2163
2022-07-15 05:35:03 - train: epoch 0017, iter [01700, 05004], lr: 0.095495, loss: 2.3576
2022-07-15 05:35:35 - train: epoch 0017, iter [01800, 05004], lr: 0.095484, loss: 2.4470
2022-07-15 05:36:08 - train: epoch 0017, iter [01900, 05004], lr: 0.095473, loss: 2.3321
2022-07-15 05:36:40 - train: epoch 0017, iter [02000, 05004], lr: 0.095462, loss: 2.5421
2022-07-15 05:37:13 - train: epoch 0017, iter [02100, 05004], lr: 0.095451, loss: 2.3510
2022-07-15 05:37:46 - train: epoch 0017, iter [02200, 05004], lr: 0.095440, loss: 2.3546
2022-07-15 05:38:19 - train: epoch 0017, iter [02300, 05004], lr: 0.095429, loss: 2.4102
2022-07-15 05:38:52 - train: epoch 0017, iter [02400, 05004], lr: 0.095418, loss: 2.3550
2022-07-15 05:39:25 - train: epoch 0017, iter [02500, 05004], lr: 0.095407, loss: 2.4810
2022-07-15 05:39:58 - train: epoch 0017, iter [02600, 05004], lr: 0.095396, loss: 2.3680
2022-07-15 05:40:31 - train: epoch 0017, iter [02700, 05004], lr: 0.095385, loss: 2.3884
2022-07-15 05:41:04 - train: epoch 0017, iter [02800, 05004], lr: 0.095374, loss: 2.5973
2022-07-15 05:41:36 - train: epoch 0017, iter [02900, 05004], lr: 0.095363, loss: 2.6116
2022-07-15 05:42:09 - train: epoch 0017, iter [03000, 05004], lr: 0.095352, loss: 2.1944
2022-07-15 05:42:43 - train: epoch 0017, iter [03100, 05004], lr: 0.095341, loss: 2.5900
2022-07-15 05:43:15 - train: epoch 0017, iter [03200, 05004], lr: 0.095330, loss: 2.2340
2022-07-15 05:43:48 - train: epoch 0017, iter [03300, 05004], lr: 0.095319, loss: 2.4067
2022-07-15 05:44:22 - train: epoch 0017, iter [03400, 05004], lr: 0.095308, loss: 2.2333
2022-07-15 05:44:54 - train: epoch 0017, iter [03500, 05004], lr: 0.095297, loss: 2.4828
2022-07-15 05:45:27 - train: epoch 0017, iter [03600, 05004], lr: 0.095286, loss: 2.8295
2022-07-15 05:45:59 - train: epoch 0017, iter [03700, 05004], lr: 0.095275, loss: 2.3801
2022-07-15 05:46:33 - train: epoch 0017, iter [03800, 05004], lr: 0.095264, loss: 2.6411
2022-07-15 05:47:06 - train: epoch 0017, iter [03900, 05004], lr: 0.095253, loss: 2.2990
2022-07-15 05:47:39 - train: epoch 0017, iter [04000, 05004], lr: 0.095242, loss: 2.2505
2022-07-15 05:48:12 - train: epoch 0017, iter [04100, 05004], lr: 0.095231, loss: 2.4735
2022-07-15 05:48:44 - train: epoch 0017, iter [04200, 05004], lr: 0.095219, loss: 2.3667
2022-07-15 05:49:18 - train: epoch 0017, iter [04300, 05004], lr: 0.095208, loss: 2.3684
2022-07-15 05:49:50 - train: epoch 0017, iter [04400, 05004], lr: 0.095197, loss: 2.3895
2022-07-15 05:50:23 - train: epoch 0017, iter [04500, 05004], lr: 0.095186, loss: 2.5239
2022-07-15 05:50:56 - train: epoch 0017, iter [04600, 05004], lr: 0.095175, loss: 2.5275
2022-07-15 05:51:30 - train: epoch 0017, iter [04700, 05004], lr: 0.095163, loss: 2.4758
2022-07-15 05:52:02 - train: epoch 0017, iter [04800, 05004], lr: 0.095152, loss: 2.5163
2022-07-15 05:52:36 - train: epoch 0017, iter [04900, 05004], lr: 0.095141, loss: 2.2433
2022-07-15 05:53:07 - train: epoch 0017, iter [05000, 05004], lr: 0.095130, loss: 2.1643
2022-07-15 05:53:08 - train: epoch 017, train_loss: 2.4165
2022-07-15 05:54:20 - eval: epoch: 017, acc1: 50.222%, acc5: 75.926%, test_loss: 2.1652, per_image_load_time: 2.092ms, per_image_inference_time: 0.222ms
2022-07-15 05:54:21 - until epoch: 017, best_acc1: 50.222%
2022-07-15 05:54:21 - epoch 018 lr: 0.095129
2022-07-15 05:54:58 - train: epoch 0018, iter [00100, 05004], lr: 0.095118, loss: 2.3375
2022-07-15 05:55:32 - train: epoch 0018, iter [00200, 05004], lr: 0.095107, loss: 2.3831
2022-07-15 05:56:05 - train: epoch 0018, iter [00300, 05004], lr: 0.095095, loss: 2.6367
2022-07-15 05:56:38 - train: epoch 0018, iter [00400, 05004], lr: 0.095084, loss: 2.5230
2022-07-15 05:57:11 - train: epoch 0018, iter [00500, 05004], lr: 0.095073, loss: 2.2974
2022-07-15 05:57:44 - train: epoch 0018, iter [00600, 05004], lr: 0.095061, loss: 2.5446
2022-07-15 05:58:17 - train: epoch 0018, iter [00700, 05004], lr: 0.095050, loss: 2.1619
2022-07-15 05:58:50 - train: epoch 0018, iter [00800, 05004], lr: 0.095039, loss: 2.3340
2022-07-15 05:59:23 - train: epoch 0018, iter [00900, 05004], lr: 0.095027, loss: 2.4988
2022-07-15 05:59:55 - train: epoch 0018, iter [01000, 05004], lr: 0.095016, loss: 2.2252
2022-07-15 06:00:28 - train: epoch 0018, iter [01100, 05004], lr: 0.095005, loss: 2.6299
2022-07-15 06:01:01 - train: epoch 0018, iter [01200, 05004], lr: 0.094993, loss: 2.3550
2022-07-15 06:01:34 - train: epoch 0018, iter [01300, 05004], lr: 0.094982, loss: 2.7240
2022-07-15 06:02:07 - train: epoch 0018, iter [01400, 05004], lr: 0.094970, loss: 2.4953
2022-07-15 06:02:40 - train: epoch 0018, iter [01500, 05004], lr: 0.094959, loss: 2.6118
2022-07-15 06:03:12 - train: epoch 0018, iter [01600, 05004], lr: 0.094947, loss: 2.4103
2022-07-15 06:03:46 - train: epoch 0018, iter [01700, 05004], lr: 0.094936, loss: 2.2863
2022-07-15 06:04:18 - train: epoch 0018, iter [01800, 05004], lr: 0.094925, loss: 2.1098
2022-07-15 06:04:52 - train: epoch 0018, iter [01900, 05004], lr: 0.094913, loss: 2.4703
2022-07-15 06:05:24 - train: epoch 0018, iter [02000, 05004], lr: 0.094902, loss: 2.7097
2022-07-15 06:05:57 - train: epoch 0018, iter [02100, 05004], lr: 0.094890, loss: 2.4991
2022-07-15 06:06:31 - train: epoch 0018, iter [02200, 05004], lr: 0.094879, loss: 2.3373
2022-07-15 06:07:03 - train: epoch 0018, iter [02300, 05004], lr: 0.094867, loss: 2.4186
2022-07-15 06:07:37 - train: epoch 0018, iter [02400, 05004], lr: 0.094855, loss: 2.2215
2022-07-15 06:08:09 - train: epoch 0018, iter [02500, 05004], lr: 0.094844, loss: 2.1346
2022-07-15 06:08:42 - train: epoch 0018, iter [02600, 05004], lr: 0.094832, loss: 2.2824
2022-07-15 06:09:15 - train: epoch 0018, iter [02700, 05004], lr: 0.094821, loss: 2.5893
2022-07-15 06:09:49 - train: epoch 0018, iter [02800, 05004], lr: 0.094809, loss: 2.1753
2022-07-15 06:10:21 - train: epoch 0018, iter [02900, 05004], lr: 0.094797, loss: 2.3407
2022-07-15 06:10:55 - train: epoch 0018, iter [03000, 05004], lr: 0.094786, loss: 2.2392
2022-07-15 06:11:26 - train: epoch 0018, iter [03100, 05004], lr: 0.094774, loss: 2.7591
2022-07-15 06:12:00 - train: epoch 0018, iter [03200, 05004], lr: 0.094763, loss: 2.2280
2022-07-15 06:12:33 - train: epoch 0018, iter [03300, 05004], lr: 0.094751, loss: 2.3592
2022-07-15 06:13:06 - train: epoch 0018, iter [03400, 05004], lr: 0.094739, loss: 2.3527
2022-07-15 06:13:39 - train: epoch 0018, iter [03500, 05004], lr: 0.094728, loss: 2.6026
2022-07-15 06:14:11 - train: epoch 0018, iter [03600, 05004], lr: 0.094716, loss: 2.4917
2022-07-15 06:14:44 - train: epoch 0018, iter [03700, 05004], lr: 0.094704, loss: 2.7987
2022-07-15 06:15:18 - train: epoch 0018, iter [03800, 05004], lr: 0.094692, loss: 2.5548
2022-07-15 06:15:50 - train: epoch 0018, iter [03900, 05004], lr: 0.094681, loss: 2.4604
2022-07-15 06:16:24 - train: epoch 0018, iter [04000, 05004], lr: 0.094669, loss: 2.3446
2022-07-15 06:16:56 - train: epoch 0018, iter [04100, 05004], lr: 0.094657, loss: 2.3723
2022-07-15 06:17:30 - train: epoch 0018, iter [04200, 05004], lr: 0.094645, loss: 2.3803
2022-07-15 06:18:03 - train: epoch 0018, iter [04300, 05004], lr: 0.094634, loss: 2.1754
2022-07-15 06:18:35 - train: epoch 0018, iter [04400, 05004], lr: 0.094622, loss: 2.5472
2022-07-15 06:19:09 - train: epoch 0018, iter [04500, 05004], lr: 0.094610, loss: 2.3271
2022-07-15 06:19:41 - train: epoch 0018, iter [04600, 05004], lr: 0.094598, loss: 2.2228
2022-07-15 06:20:15 - train: epoch 0018, iter [04700, 05004], lr: 0.094586, loss: 2.5995
2022-07-15 06:20:48 - train: epoch 0018, iter [04800, 05004], lr: 0.094575, loss: 2.4945
2022-07-15 06:21:20 - train: epoch 0018, iter [04900, 05004], lr: 0.094563, loss: 2.4007
2022-07-15 06:21:53 - train: epoch 0018, iter [05000, 05004], lr: 0.094551, loss: 2.4554
2022-07-15 06:21:54 - train: epoch 018, train_loss: 2.3988
2022-07-15 06:23:07 - eval: epoch: 018, acc1: 50.614%, acc5: 76.000%, test_loss: 2.1660, per_image_load_time: 2.353ms, per_image_inference_time: 0.254ms
2022-07-15 06:23:07 - until epoch: 018, best_acc1: 50.614%
2022-07-15 06:23:07 - epoch 019 lr: 0.094550
2022-07-15 06:23:46 - train: epoch 0019, iter [00100, 05004], lr: 0.094538, loss: 2.2225
2022-07-15 06:24:18 - train: epoch 0019, iter [00200, 05004], lr: 0.094527, loss: 2.5907
2022-07-15 06:24:52 - train: epoch 0019, iter [00300, 05004], lr: 0.094515, loss: 2.5871
2022-07-15 06:25:24 - train: epoch 0019, iter [00400, 05004], lr: 0.094503, loss: 2.3004
2022-07-15 06:25:57 - train: epoch 0019, iter [00500, 05004], lr: 0.094491, loss: 2.3050
2022-07-15 06:26:30 - train: epoch 0019, iter [00600, 05004], lr: 0.094479, loss: 2.3028
2022-07-15 06:27:02 - train: epoch 0019, iter [00700, 05004], lr: 0.094467, loss: 2.0489
2022-07-15 06:27:36 - train: epoch 0019, iter [00800, 05004], lr: 0.094455, loss: 2.6207
2022-07-15 06:28:08 - train: epoch 0019, iter [00900, 05004], lr: 0.094443, loss: 2.3673
2022-07-15 06:28:41 - train: epoch 0019, iter [01000, 05004], lr: 0.094431, loss: 2.5111
2022-07-15 06:29:14 - train: epoch 0019, iter [01100, 05004], lr: 0.094419, loss: 2.1975
2022-07-15 06:29:47 - train: epoch 0019, iter [01200, 05004], lr: 0.094407, loss: 2.4343
2022-07-15 06:30:20 - train: epoch 0019, iter [01300, 05004], lr: 0.094395, loss: 2.5370
2022-07-15 06:30:54 - train: epoch 0019, iter [01400, 05004], lr: 0.094383, loss: 2.2842
2022-07-15 06:31:25 - train: epoch 0019, iter [01500, 05004], lr: 0.094371, loss: 2.7950
2022-07-15 06:31:58 - train: epoch 0019, iter [01600, 05004], lr: 0.094359, loss: 2.4165
2022-07-15 06:32:31 - train: epoch 0019, iter [01700, 05004], lr: 0.094347, loss: 2.4258
2022-07-15 06:33:03 - train: epoch 0019, iter [01800, 05004], lr: 0.094335, loss: 2.2260
2022-07-15 06:33:36 - train: epoch 0019, iter [01900, 05004], lr: 0.094322, loss: 2.6567
2022-07-15 06:34:11 - train: epoch 0019, iter [02000, 05004], lr: 0.094310, loss: 2.3272
2022-07-15 06:34:43 - train: epoch 0019, iter [02100, 05004], lr: 0.094298, loss: 2.3800
2022-07-15 06:35:16 - train: epoch 0019, iter [02200, 05004], lr: 0.094286, loss: 2.3878
2022-07-15 06:35:49 - train: epoch 0019, iter [02300, 05004], lr: 0.094274, loss: 2.4074
2022-07-15 06:36:22 - train: epoch 0019, iter [02400, 05004], lr: 0.094262, loss: 2.5361
2022-07-15 06:36:54 - train: epoch 0019, iter [02500, 05004], lr: 0.094250, loss: 2.2242
2022-07-15 06:37:28 - train: epoch 0019, iter [02600, 05004], lr: 0.094237, loss: 2.4320
2022-07-15 06:38:01 - train: epoch 0019, iter [02700, 05004], lr: 0.094225, loss: 2.5164
2022-07-15 06:38:34 - train: epoch 0019, iter [02800, 05004], lr: 0.094213, loss: 2.4309
2022-07-15 06:39:07 - train: epoch 0019, iter [02900, 05004], lr: 0.094201, loss: 2.4672
2022-07-15 06:39:40 - train: epoch 0019, iter [03000, 05004], lr: 0.094189, loss: 2.6388
2022-07-15 06:40:13 - train: epoch 0019, iter [03100, 05004], lr: 0.094176, loss: 2.4027
2022-07-15 06:40:45 - train: epoch 0019, iter [03200, 05004], lr: 0.094164, loss: 2.1403
2022-07-15 06:41:19 - train: epoch 0019, iter [03300, 05004], lr: 0.094152, loss: 2.3393
2022-07-15 06:41:51 - train: epoch 0019, iter [03400, 05004], lr: 0.094140, loss: 2.3970
2022-07-15 06:42:25 - train: epoch 0019, iter [03500, 05004], lr: 0.094127, loss: 2.4719
2022-07-15 06:42:57 - train: epoch 0019, iter [03600, 05004], lr: 0.094115, loss: 2.2933
2022-07-15 06:43:30 - train: epoch 0019, iter [03700, 05004], lr: 0.094103, loss: 2.4899
2022-07-15 06:44:03 - train: epoch 0019, iter [03800, 05004], lr: 0.094090, loss: 2.5439
2022-07-15 06:44:36 - train: epoch 0019, iter [03900, 05004], lr: 0.094078, loss: 2.2770
2022-07-15 06:45:09 - train: epoch 0019, iter [04000, 05004], lr: 0.094066, loss: 2.3057
2022-07-15 06:45:42 - train: epoch 0019, iter [04100, 05004], lr: 0.094053, loss: 2.4163
2022-07-15 06:46:14 - train: epoch 0019, iter [04200, 05004], lr: 0.094041, loss: 2.3234
2022-07-15 06:46:48 - train: epoch 0019, iter [04300, 05004], lr: 0.094028, loss: 2.3054
2022-07-15 06:47:20 - train: epoch 0019, iter [04400, 05004], lr: 0.094016, loss: 2.5934
2022-07-15 06:47:53 - train: epoch 0019, iter [04500, 05004], lr: 0.094004, loss: 2.7442
2022-07-15 06:48:25 - train: epoch 0019, iter [04600, 05004], lr: 0.093991, loss: 2.4780
2022-07-15 06:48:58 - train: epoch 0019, iter [04700, 05004], lr: 0.093979, loss: 2.3754
2022-07-15 06:49:31 - train: epoch 0019, iter [04800, 05004], lr: 0.093966, loss: 2.3621
2022-07-15 06:50:03 - train: epoch 0019, iter [04900, 05004], lr: 0.093954, loss: 2.2407
2022-07-15 06:50:35 - train: epoch 0019, iter [05000, 05004], lr: 0.093941, loss: 2.3977
2022-07-15 06:50:36 - train: epoch 019, train_loss: 2.3897
2022-07-15 06:51:49 - eval: epoch: 019, acc1: 50.812%, acc5: 76.138%, test_loss: 2.1481, per_image_load_time: 2.167ms, per_image_inference_time: 0.216ms
2022-07-15 06:51:49 - until epoch: 019, best_acc1: 50.812%
2022-07-15 06:51:49 - epoch 020 lr: 0.093941
2022-07-15 06:52:27 - train: epoch 0020, iter [00100, 05004], lr: 0.093928, loss: 2.6647
2022-07-15 06:53:00 - train: epoch 0020, iter [00200, 05004], lr: 0.093916, loss: 2.1853
2022-07-15 06:53:33 - train: epoch 0020, iter [00300, 05004], lr: 0.093903, loss: 2.4182
2022-07-15 06:54:06 - train: epoch 0020, iter [00400, 05004], lr: 0.093891, loss: 2.0252
2022-07-15 06:54:40 - train: epoch 0020, iter [00500, 05004], lr: 0.093878, loss: 2.2726
2022-07-15 06:55:13 - train: epoch 0020, iter [00600, 05004], lr: 0.093866, loss: 2.5752
2022-07-15 06:55:46 - train: epoch 0020, iter [00700, 05004], lr: 0.093853, loss: 2.1415
2022-07-15 06:56:19 - train: epoch 0020, iter [00800, 05004], lr: 0.093841, loss: 2.5014
2022-07-15 06:56:52 - train: epoch 0020, iter [00900, 05004], lr: 0.093828, loss: 2.6862
2022-07-15 06:57:25 - train: epoch 0020, iter [01000, 05004], lr: 0.093815, loss: 2.3834
2022-07-15 06:57:58 - train: epoch 0020, iter [01100, 05004], lr: 0.093803, loss: 2.2071
2022-07-15 06:58:31 - train: epoch 0020, iter [01200, 05004], lr: 0.093790, loss: 2.1604
2022-07-15 06:59:05 - train: epoch 0020, iter [01300, 05004], lr: 0.093778, loss: 2.3300
2022-07-15 06:59:38 - train: epoch 0020, iter [01400, 05004], lr: 0.093765, loss: 2.5496
2022-07-15 07:00:11 - train: epoch 0020, iter [01500, 05004], lr: 0.093752, loss: 2.3763
2022-07-15 07:00:44 - train: epoch 0020, iter [01600, 05004], lr: 0.093740, loss: 2.3205
2022-07-15 07:01:17 - train: epoch 0020, iter [01700, 05004], lr: 0.093727, loss: 2.0878
2022-07-15 07:01:49 - train: epoch 0020, iter [01800, 05004], lr: 0.093714, loss: 2.3276
2022-07-15 07:02:22 - train: epoch 0020, iter [01900, 05004], lr: 0.093702, loss: 2.2194
2022-07-15 07:02:54 - train: epoch 0020, iter [02000, 05004], lr: 0.093689, loss: 2.3977
2022-07-15 07:03:28 - train: epoch 0020, iter [02100, 05004], lr: 0.093676, loss: 2.5029
2022-07-15 07:04:01 - train: epoch 0020, iter [02200, 05004], lr: 0.093663, loss: 2.2090
2022-07-15 07:04:34 - train: epoch 0020, iter [02300, 05004], lr: 0.093651, loss: 2.2823
2022-07-15 07:05:06 - train: epoch 0020, iter [02400, 05004], lr: 0.093638, loss: 2.6184
2022-07-15 07:05:40 - train: epoch 0020, iter [02500, 05004], lr: 0.093625, loss: 2.2792
2022-07-15 07:06:12 - train: epoch 0020, iter [02600, 05004], lr: 0.093612, loss: 2.1716
2022-07-15 07:06:45 - train: epoch 0020, iter [02700, 05004], lr: 0.093599, loss: 2.4392
2022-07-15 07:07:18 - train: epoch 0020, iter [02800, 05004], lr: 0.093587, loss: 2.4213
2022-07-15 07:07:51 - train: epoch 0020, iter [02900, 05004], lr: 0.093574, loss: 2.4819
2022-07-15 07:08:24 - train: epoch 0020, iter [03000, 05004], lr: 0.093561, loss: 2.2615
2022-07-15 07:08:56 - train: epoch 0020, iter [03100, 05004], lr: 0.093548, loss: 2.4573
2022-07-15 07:09:29 - train: epoch 0020, iter [03200, 05004], lr: 0.093535, loss: 2.4742
2022-07-15 07:10:02 - train: epoch 0020, iter [03300, 05004], lr: 0.093522, loss: 2.2782
2022-07-15 07:10:35 - train: epoch 0020, iter [03400, 05004], lr: 0.093510, loss: 2.3767
2022-07-15 07:11:09 - train: epoch 0020, iter [03500, 05004], lr: 0.093497, loss: 2.2910
2022-07-15 07:11:41 - train: epoch 0020, iter [03600, 05004], lr: 0.093484, loss: 2.3555
2022-07-15 07:12:15 - train: epoch 0020, iter [03700, 05004], lr: 0.093471, loss: 2.2160
2022-07-15 07:12:47 - train: epoch 0020, iter [03800, 05004], lr: 0.093458, loss: 2.3475
2022-07-15 07:13:21 - train: epoch 0020, iter [03900, 05004], lr: 0.093445, loss: 2.5328
2022-07-15 07:13:52 - train: epoch 0020, iter [04000, 05004], lr: 0.093432, loss: 2.2538
2022-07-15 07:14:24 - train: epoch 0020, iter [04100, 05004], lr: 0.093419, loss: 2.2382
2022-07-15 07:14:58 - train: epoch 0020, iter [04200, 05004], lr: 0.093406, loss: 2.2360
2022-07-15 07:15:30 - train: epoch 0020, iter [04300, 05004], lr: 0.093393, loss: 2.4518
2022-07-15 07:16:03 - train: epoch 0020, iter [04400, 05004], lr: 0.093380, loss: 2.3997
2022-07-15 07:16:37 - train: epoch 0020, iter [04500, 05004], lr: 0.093367, loss: 2.3931
2022-07-15 07:17:10 - train: epoch 0020, iter [04600, 05004], lr: 0.093354, loss: 2.3928
2022-07-15 07:17:43 - train: epoch 0020, iter [04700, 05004], lr: 0.093341, loss: 2.1666
2022-07-15 07:18:15 - train: epoch 0020, iter [04800, 05004], lr: 0.093328, loss: 2.3646
2022-07-15 07:18:48 - train: epoch 0020, iter [04900, 05004], lr: 0.093315, loss: 2.3439
2022-07-15 07:19:19 - train: epoch 0020, iter [05000, 05004], lr: 0.093302, loss: 2.1699
2022-07-15 07:19:20 - train: epoch 020, train_loss: 2.3712
2022-07-15 07:20:33 - eval: epoch: 020, acc1: 52.304%, acc5: 77.120%, test_loss: 2.0809, per_image_load_time: 2.444ms, per_image_inference_time: 0.228ms
2022-07-15 07:20:33 - until epoch: 020, best_acc1: 52.304%
2022-07-15 07:20:33 - epoch 021 lr: 0.093301
2022-07-15 07:21:11 - train: epoch 0021, iter [00100, 05004], lr: 0.093288, loss: 2.2131
2022-07-15 07:21:44 - train: epoch 0021, iter [00200, 05004], lr: 0.093275, loss: 2.5056
2022-07-15 07:22:17 - train: epoch 0021, iter [00300, 05004], lr: 0.093262, loss: 1.9697
2022-07-15 07:22:49 - train: epoch 0021, iter [00400, 05004], lr: 0.093249, loss: 2.3682
2022-07-15 07:23:22 - train: epoch 0021, iter [00500, 05004], lr: 0.093236, loss: 2.2091
2022-07-15 07:23:55 - train: epoch 0021, iter [00600, 05004], lr: 0.093223, loss: 2.1813
2022-07-15 07:24:28 - train: epoch 0021, iter [00700, 05004], lr: 0.093209, loss: 2.1594
2022-07-15 07:25:00 - train: epoch 0021, iter [00800, 05004], lr: 0.093196, loss: 2.5239
2022-07-15 07:25:34 - train: epoch 0021, iter [00900, 05004], lr: 0.093183, loss: 2.4590
2022-07-15 07:26:06 - train: epoch 0021, iter [01000, 05004], lr: 0.093170, loss: 2.2856
2022-07-15 07:26:39 - train: epoch 0021, iter [01100, 05004], lr: 0.093157, loss: 2.2691
2022-07-15 07:27:11 - train: epoch 0021, iter [01200, 05004], lr: 0.093143, loss: 2.3040
2022-07-15 07:27:44 - train: epoch 0021, iter [01300, 05004], lr: 0.093130, loss: 2.3306
2022-07-15 07:28:17 - train: epoch 0021, iter [01400, 05004], lr: 0.093117, loss: 2.4021
2022-07-15 07:28:50 - train: epoch 0021, iter [01500, 05004], lr: 0.093104, loss: 2.1653
2022-07-15 07:29:22 - train: epoch 0021, iter [01600, 05004], lr: 0.093090, loss: 2.4030
2022-07-15 07:29:56 - train: epoch 0021, iter [01700, 05004], lr: 0.093077, loss: 2.3611
2022-07-15 07:30:29 - train: epoch 0021, iter [01800, 05004], lr: 0.093064, loss: 2.2025
2022-07-15 07:31:02 - train: epoch 0021, iter [01900, 05004], lr: 0.093051, loss: 2.4754
2022-07-15 07:31:34 - train: epoch 0021, iter [02000, 05004], lr: 0.093037, loss: 2.4876
2022-07-15 07:32:07 - train: epoch 0021, iter [02100, 05004], lr: 0.093024, loss: 2.1928
2022-07-15 07:32:40 - train: epoch 0021, iter [02200, 05004], lr: 0.093011, loss: 2.3675
2022-07-15 07:33:13 - train: epoch 0021, iter [02300, 05004], lr: 0.092997, loss: 2.1382
2022-07-15 07:33:46 - train: epoch 0021, iter [02400, 05004], lr: 0.092984, loss: 2.2815
2022-07-15 07:34:19 - train: epoch 0021, iter [02500, 05004], lr: 0.092971, loss: 2.4340
2022-07-15 07:34:52 - train: epoch 0021, iter [02600, 05004], lr: 0.092957, loss: 2.6146
2022-07-15 07:35:25 - train: epoch 0021, iter [02700, 05004], lr: 0.092944, loss: 2.2527
2022-07-15 07:35:58 - train: epoch 0021, iter [02800, 05004], lr: 0.092930, loss: 2.3949
2022-07-15 07:36:30 - train: epoch 0021, iter [02900, 05004], lr: 0.092917, loss: 2.2716
2022-07-15 07:37:04 - train: epoch 0021, iter [03000, 05004], lr: 0.092904, loss: 2.6458
2022-07-15 07:37:36 - train: epoch 0021, iter [03100, 05004], lr: 0.092890, loss: 2.3389
2022-07-15 07:38:10 - train: epoch 0021, iter [03200, 05004], lr: 0.092877, loss: 2.3300
2022-07-15 07:38:43 - train: epoch 0021, iter [03300, 05004], lr: 0.092863, loss: 2.6535
2022-07-15 07:39:14 - train: epoch 0021, iter [03400, 05004], lr: 0.092850, loss: 2.4549
2022-07-15 07:39:48 - train: epoch 0021, iter [03500, 05004], lr: 0.092836, loss: 2.2369
2022-07-15 07:40:21 - train: epoch 0021, iter [03600, 05004], lr: 0.092823, loss: 2.3853
2022-07-15 07:40:54 - train: epoch 0021, iter [03700, 05004], lr: 0.092809, loss: 2.4265
2022-07-15 07:41:27 - train: epoch 0021, iter [03800, 05004], lr: 0.092796, loss: 2.2964
2022-07-15 07:42:00 - train: epoch 0021, iter [03900, 05004], lr: 0.092782, loss: 2.3129
2022-07-15 07:42:33 - train: epoch 0021, iter [04000, 05004], lr: 0.092769, loss: 2.6144
2022-07-15 07:43:07 - train: epoch 0021, iter [04100, 05004], lr: 0.092755, loss: 2.3089
2022-07-15 07:43:40 - train: epoch 0021, iter [04200, 05004], lr: 0.092742, loss: 2.3587
2022-07-15 07:44:13 - train: epoch 0021, iter [04300, 05004], lr: 0.092728, loss: 2.3996
2022-07-15 07:44:45 - train: epoch 0021, iter [04400, 05004], lr: 0.092714, loss: 2.3936
2022-07-15 07:45:19 - train: epoch 0021, iter [04500, 05004], lr: 0.092701, loss: 2.4598
2022-07-15 07:45:51 - train: epoch 0021, iter [04600, 05004], lr: 0.092687, loss: 2.2099
2022-07-15 07:46:25 - train: epoch 0021, iter [04700, 05004], lr: 0.092674, loss: 2.5547
2022-07-15 07:46:58 - train: epoch 0021, iter [04800, 05004], lr: 0.092660, loss: 2.4462
2022-07-15 07:47:31 - train: epoch 0021, iter [04900, 05004], lr: 0.092646, loss: 2.1869
2022-07-15 07:48:02 - train: epoch 0021, iter [05000, 05004], lr: 0.092633, loss: 2.3993
2022-07-15 07:48:03 - train: epoch 021, train_loss: 2.3609
2022-07-15 07:49:16 - eval: epoch: 021, acc1: 51.394%, acc5: 76.554%, test_loss: 2.1216, per_image_load_time: 1.961ms, per_image_inference_time: 0.208ms
2022-07-15 07:49:16 - until epoch: 021, best_acc1: 52.304%
2022-07-15 07:49:16 - epoch 022 lr: 0.092632
2022-07-15 07:49:54 - train: epoch 0022, iter [00100, 05004], lr: 0.092618, loss: 2.0810
2022-07-15 07:50:27 - train: epoch 0022, iter [00200, 05004], lr: 0.092605, loss: 2.1589
2022-07-15 07:51:01 - train: epoch 0022, iter [00300, 05004], lr: 0.092591, loss: 2.0626
2022-07-15 07:51:33 - train: epoch 0022, iter [00400, 05004], lr: 0.092577, loss: 2.1439
2022-07-15 07:52:07 - train: epoch 0022, iter [00500, 05004], lr: 0.092564, loss: 2.2846
2022-07-15 07:52:40 - train: epoch 0022, iter [00600, 05004], lr: 0.092550, loss: 2.4293
2022-07-15 07:53:14 - train: epoch 0022, iter [00700, 05004], lr: 0.092536, loss: 2.4166
2022-07-15 07:53:46 - train: epoch 0022, iter [00800, 05004], lr: 0.092522, loss: 2.4173
2022-07-15 07:54:20 - train: epoch 0022, iter [00900, 05004], lr: 0.092509, loss: 2.4477
2022-07-15 07:54:52 - train: epoch 0022, iter [01000, 05004], lr: 0.092495, loss: 2.3686
2022-07-15 07:55:26 - train: epoch 0022, iter [01100, 05004], lr: 0.092481, loss: 2.4054
2022-07-15 07:55:59 - train: epoch 0022, iter [01200, 05004], lr: 0.092467, loss: 2.0131
2022-07-15 07:56:32 - train: epoch 0022, iter [01300, 05004], lr: 0.092453, loss: 2.2569
2022-07-15 07:57:06 - train: epoch 0022, iter [01400, 05004], lr: 0.092440, loss: 2.4600
2022-07-15 07:57:38 - train: epoch 0022, iter [01500, 05004], lr: 0.092426, loss: 2.2580
2022-07-15 07:58:12 - train: epoch 0022, iter [01600, 05004], lr: 0.092412, loss: 2.2431
2022-07-15 07:58:46 - train: epoch 0022, iter [01700, 05004], lr: 0.092398, loss: 2.1214
2022-07-15 07:59:19 - train: epoch 0022, iter [01800, 05004], lr: 0.092384, loss: 2.5837
2022-07-15 07:59:52 - train: epoch 0022, iter [01900, 05004], lr: 0.092370, loss: 2.1623
2022-07-15 08:00:25 - train: epoch 0022, iter [02000, 05004], lr: 0.092356, loss: 2.4479
2022-07-15 08:00:58 - train: epoch 0022, iter [02100, 05004], lr: 0.092342, loss: 2.3280
2022-07-15 08:01:31 - train: epoch 0022, iter [02200, 05004], lr: 0.092328, loss: 2.1781
2022-07-15 08:02:05 - train: epoch 0022, iter [02300, 05004], lr: 0.092315, loss: 2.4834
2022-07-15 08:02:38 - train: epoch 0022, iter [02400, 05004], lr: 0.092301, loss: 2.3866
2022-07-15 08:03:12 - train: epoch 0022, iter [02500, 05004], lr: 0.092287, loss: 2.3373
2022-07-15 08:03:45 - train: epoch 0022, iter [02600, 05004], lr: 0.092273, loss: 2.2009
2022-07-15 08:04:18 - train: epoch 0022, iter [02700, 05004], lr: 0.092259, loss: 2.1967
2022-07-15 08:04:51 - train: epoch 0022, iter [02800, 05004], lr: 0.092245, loss: 2.7460
2022-07-15 08:05:24 - train: epoch 0022, iter [02900, 05004], lr: 0.092231, loss: 2.1646
2022-07-15 08:05:57 - train: epoch 0022, iter [03000, 05004], lr: 0.092217, loss: 2.4650
2022-07-15 08:06:31 - train: epoch 0022, iter [03100, 05004], lr: 0.092203, loss: 2.4378
2022-07-15 08:07:04 - train: epoch 0022, iter [03200, 05004], lr: 0.092189, loss: 2.3801
2022-07-15 08:07:37 - train: epoch 0022, iter [03300, 05004], lr: 0.092175, loss: 2.3134
2022-07-15 08:08:10 - train: epoch 0022, iter [03400, 05004], lr: 0.092161, loss: 2.2545
2022-07-15 08:08:43 - train: epoch 0022, iter [03500, 05004], lr: 0.092147, loss: 2.4243
2022-07-15 08:09:17 - train: epoch 0022, iter [03600, 05004], lr: 0.092132, loss: 2.4277
2022-07-15 08:09:50 - train: epoch 0022, iter [03700, 05004], lr: 0.092118, loss: 2.4603
2022-07-15 08:10:24 - train: epoch 0022, iter [03800, 05004], lr: 0.092104, loss: 2.4912
2022-07-15 08:10:57 - train: epoch 0022, iter [03900, 05004], lr: 0.092090, loss: 2.3033
2022-07-15 08:11:30 - train: epoch 0022, iter [04000, 05004], lr: 0.092076, loss: 2.3801
2022-07-15 08:12:04 - train: epoch 0022, iter [04100, 05004], lr: 0.092062, loss: 2.2872
2022-07-15 08:12:37 - train: epoch 0022, iter [04200, 05004], lr: 0.092048, loss: 2.2789
2022-07-15 08:13:10 - train: epoch 0022, iter [04300, 05004], lr: 0.092034, loss: 2.5104
2022-07-15 08:13:44 - train: epoch 0022, iter [04400, 05004], lr: 0.092019, loss: 2.4375
2022-07-15 08:14:17 - train: epoch 0022, iter [04500, 05004], lr: 0.092005, loss: 2.3095
2022-07-15 08:14:50 - train: epoch 0022, iter [04600, 05004], lr: 0.091991, loss: 2.6047
2022-07-15 08:15:24 - train: epoch 0022, iter [04700, 05004], lr: 0.091977, loss: 2.4888
2022-07-15 08:15:57 - train: epoch 0022, iter [04800, 05004], lr: 0.091963, loss: 2.1252
2022-07-15 08:16:30 - train: epoch 0022, iter [04900, 05004], lr: 0.091948, loss: 2.1391
2022-07-15 08:17:02 - train: epoch 0022, iter [05000, 05004], lr: 0.091934, loss: 2.2928
2022-07-15 08:17:03 - train: epoch 022, train_loss: 2.3500
2022-07-15 08:18:18 - eval: epoch: 022, acc1: 52.058%, acc5: 77.390%, test_loss: 2.0733, per_image_load_time: 1.047ms, per_image_inference_time: 0.212ms
2022-07-15 08:18:18 - until epoch: 022, best_acc1: 52.304%
2022-07-15 08:18:18 - epoch 023 lr: 0.091933
2022-07-15 08:18:57 - train: epoch 0023, iter [00100, 05004], lr: 0.091919, loss: 2.2253
2022-07-15 08:19:31 - train: epoch 0023, iter [00200, 05004], lr: 0.091905, loss: 1.9257
2022-07-15 08:20:04 - train: epoch 0023, iter [00300, 05004], lr: 0.091891, loss: 2.1295
2022-07-15 08:20:37 - train: epoch 0023, iter [00400, 05004], lr: 0.091876, loss: 2.4567
2022-07-15 08:21:10 - train: epoch 0023, iter [00500, 05004], lr: 0.091862, loss: 2.3963
2022-07-15 08:21:44 - train: epoch 0023, iter [00600, 05004], lr: 0.091848, loss: 2.2101
2022-07-15 08:22:18 - train: epoch 0023, iter [00700, 05004], lr: 0.091834, loss: 2.1358
2022-07-15 08:22:51 - train: epoch 0023, iter [00800, 05004], lr: 0.091819, loss: 2.2590
2022-07-15 08:23:24 - train: epoch 0023, iter [00900, 05004], lr: 0.091805, loss: 2.3795
2022-07-15 08:23:58 - train: epoch 0023, iter [01000, 05004], lr: 0.091790, loss: 2.2238
2022-07-15 08:24:31 - train: epoch 0023, iter [01100, 05004], lr: 0.091776, loss: 2.5097
2022-07-15 08:25:04 - train: epoch 0023, iter [01200, 05004], lr: 0.091762, loss: 2.1574
2022-07-15 08:25:38 - train: epoch 0023, iter [01300, 05004], lr: 0.091747, loss: 2.2869
2022-07-15 08:26:10 - train: epoch 0023, iter [01400, 05004], lr: 0.091733, loss: 2.3637
2022-07-15 08:26:44 - train: epoch 0023, iter [01500, 05004], lr: 0.091719, loss: 2.0876
2022-07-15 08:27:18 - train: epoch 0023, iter [01600, 05004], lr: 0.091704, loss: 2.2697
2022-07-15 08:27:51 - train: epoch 0023, iter [01700, 05004], lr: 0.091690, loss: 2.4127
2022-07-15 08:28:23 - train: epoch 0023, iter [01800, 05004], lr: 0.091675, loss: 2.3220
2022-07-15 08:28:57 - train: epoch 0023, iter [01900, 05004], lr: 0.091661, loss: 2.4095
2022-07-15 08:29:30 - train: epoch 0023, iter [02000, 05004], lr: 0.091646, loss: 2.0300
2022-07-15 08:30:03 - train: epoch 0023, iter [02100, 05004], lr: 0.091632, loss: 2.4514
2022-07-15 08:30:37 - train: epoch 0023, iter [02200, 05004], lr: 0.091617, loss: 1.9720
2022-07-15 08:31:10 - train: epoch 0023, iter [02300, 05004], lr: 0.091603, loss: 2.1993
2022-07-15 08:31:43 - train: epoch 0023, iter [02400, 05004], lr: 0.091588, loss: 2.3230
2022-07-15 08:32:16 - train: epoch 0023, iter [02500, 05004], lr: 0.091574, loss: 2.5095
2022-07-15 08:32:50 - train: epoch 0023, iter [02600, 05004], lr: 0.091559, loss: 2.3994
2022-07-15 08:33:23 - train: epoch 0023, iter [02700, 05004], lr: 0.091545, loss: 2.2557
2022-07-15 08:33:56 - train: epoch 0023, iter [02800, 05004], lr: 0.091530, loss: 2.2963
2022-07-15 08:34:31 - train: epoch 0023, iter [02900, 05004], lr: 0.091516, loss: 2.4383
2022-07-15 08:35:03 - train: epoch 0023, iter [03000, 05004], lr: 0.091501, loss: 2.5318
2022-07-15 08:35:36 - train: epoch 0023, iter [03100, 05004], lr: 0.091486, loss: 2.4574
2022-07-15 08:36:10 - train: epoch 0023, iter [03200, 05004], lr: 0.091472, loss: 2.5380
2022-07-15 08:36:43 - train: epoch 0023, iter [03300, 05004], lr: 0.091457, loss: 2.3261
2022-07-15 08:37:16 - train: epoch 0023, iter [03400, 05004], lr: 0.091443, loss: 2.5493
2022-07-15 08:37:50 - train: epoch 0023, iter [03500, 05004], lr: 0.091428, loss: 2.2187
2022-07-15 08:38:23 - train: epoch 0023, iter [03600, 05004], lr: 0.091413, loss: 2.1806
2022-07-15 08:38:57 - train: epoch 0023, iter [03700, 05004], lr: 0.091399, loss: 2.3166
2022-07-15 08:39:29 - train: epoch 0023, iter [03800, 05004], lr: 0.091384, loss: 2.3450
2022-07-15 08:40:03 - train: epoch 0023, iter [03900, 05004], lr: 0.091369, loss: 2.3689
2022-07-15 08:40:37 - train: epoch 0023, iter [04000, 05004], lr: 0.091354, loss: 2.3247
2022-07-15 08:41:10 - train: epoch 0023, iter [04100, 05004], lr: 0.091340, loss: 2.2201
2022-07-15 08:41:43 - train: epoch 0023, iter [04200, 05004], lr: 0.091325, loss: 2.1832
2022-07-15 08:42:16 - train: epoch 0023, iter [04300, 05004], lr: 0.091310, loss: 2.1121
2022-07-15 08:42:49 - train: epoch 0023, iter [04400, 05004], lr: 0.091296, loss: 2.1529
2022-07-15 08:43:22 - train: epoch 0023, iter [04500, 05004], lr: 0.091281, loss: 2.2294
2022-07-15 08:43:56 - train: epoch 0023, iter [04600, 05004], lr: 0.091266, loss: 2.3942
2022-07-15 08:44:30 - train: epoch 0023, iter [04700, 05004], lr: 0.091251, loss: 2.1689
2022-07-15 08:45:03 - train: epoch 0023, iter [04800, 05004], lr: 0.091237, loss: 2.2702
2022-07-15 08:45:37 - train: epoch 0023, iter [04900, 05004], lr: 0.091222, loss: 2.2419
2022-07-15 08:46:08 - train: epoch 0023, iter [05000, 05004], lr: 0.091207, loss: 2.4570
2022-07-15 08:46:09 - train: epoch 023, train_loss: 2.3384
2022-07-15 08:47:24 - eval: epoch: 023, acc1: 52.224%, acc5: 77.246%, test_loss: 2.0905, per_image_load_time: 2.727ms, per_image_inference_time: 0.225ms
2022-07-15 08:47:24 - until epoch: 023, best_acc1: 52.304%
2022-07-15 08:47:24 - epoch 024 lr: 0.091206
2022-07-15 08:48:03 - train: epoch 0024, iter [00100, 05004], lr: 0.091191, loss: 2.2062
2022-07-15 08:48:37 - train: epoch 0024, iter [00200, 05004], lr: 0.091177, loss: 2.3702
2022-07-15 08:49:11 - train: epoch 0024, iter [00300, 05004], lr: 0.091162, loss: 2.2820
2022-07-15 08:49:45 - train: epoch 0024, iter [00400, 05004], lr: 0.091147, loss: 2.4218
2022-07-15 08:50:18 - train: epoch 0024, iter [00500, 05004], lr: 0.091132, loss: 2.2775
2022-07-15 08:50:51 - train: epoch 0024, iter [00600, 05004], lr: 0.091117, loss: 2.1713
2022-07-15 08:51:25 - train: epoch 0024, iter [00700, 05004], lr: 0.091102, loss: 2.2093
2022-07-15 08:52:00 - train: epoch 0024, iter [00800, 05004], lr: 0.091087, loss: 2.1719
2022-07-15 08:52:32 - train: epoch 0024, iter [00900, 05004], lr: 0.091073, loss: 2.2332
2022-07-15 08:53:05 - train: epoch 0024, iter [01000, 05004], lr: 0.091058, loss: 2.2655
2022-07-15 08:53:39 - train: epoch 0024, iter [01100, 05004], lr: 0.091043, loss: 1.9790
2022-07-15 08:54:12 - train: epoch 0024, iter [01200, 05004], lr: 0.091028, loss: 2.0910
2022-07-15 08:54:44 - train: epoch 0024, iter [01300, 05004], lr: 0.091013, loss: 2.5091
2022-07-15 08:55:17 - train: epoch 0024, iter [01400, 05004], lr: 0.090998, loss: 2.2730
2022-07-15 08:55:51 - train: epoch 0024, iter [01500, 05004], lr: 0.090983, loss: 2.6981
2022-07-15 08:56:23 - train: epoch 0024, iter [01600, 05004], lr: 0.090968, loss: 2.4985
2022-07-15 08:56:58 - train: epoch 0024, iter [01700, 05004], lr: 0.090953, loss: 2.2871
2022-07-15 08:57:30 - train: epoch 0024, iter [01800, 05004], lr: 0.090938, loss: 2.5771
2022-07-15 08:58:03 - train: epoch 0024, iter [01900, 05004], lr: 0.090923, loss: 2.1142
2022-07-15 08:58:37 - train: epoch 0024, iter [02000, 05004], lr: 0.090908, loss: 2.3401
2022-07-15 08:59:11 - train: epoch 0024, iter [02100, 05004], lr: 0.090893, loss: 2.1940
2022-07-15 08:59:44 - train: epoch 0024, iter [02200, 05004], lr: 0.090878, loss: 2.1151
2022-07-15 09:00:18 - train: epoch 0024, iter [02300, 05004], lr: 0.090863, loss: 2.3328
2022-07-15 09:00:50 - train: epoch 0024, iter [02400, 05004], lr: 0.090847, loss: 2.2483
2022-07-15 09:01:23 - train: epoch 0024, iter [02500, 05004], lr: 0.090832, loss: 2.2688
2022-07-15 09:01:56 - train: epoch 0024, iter [02600, 05004], lr: 0.090817, loss: 2.2439
2022-07-15 09:02:30 - train: epoch 0024, iter [02700, 05004], lr: 0.090802, loss: 2.5009
2022-07-15 09:03:03 - train: epoch 0024, iter [02800, 05004], lr: 0.090787, loss: 2.4441
2022-07-15 09:03:36 - train: epoch 0024, iter [02900, 05004], lr: 0.090772, loss: 2.3971
2022-07-15 09:04:08 - train: epoch 0024, iter [03000, 05004], lr: 0.090757, loss: 2.1572
2022-07-15 09:04:41 - train: epoch 0024, iter [03100, 05004], lr: 0.090742, loss: 2.1031
2022-07-15 09:05:15 - train: epoch 0024, iter [03200, 05004], lr: 0.090726, loss: 2.5513
2022-07-15 09:05:48 - train: epoch 0024, iter [03300, 05004], lr: 0.090711, loss: 1.9755
2022-07-15 09:06:20 - train: epoch 0024, iter [03400, 05004], lr: 0.090696, loss: 2.3318
2022-07-15 09:06:54 - train: epoch 0024, iter [03500, 05004], lr: 0.090681, loss: 2.2356
2022-07-15 09:07:27 - train: epoch 0024, iter [03600, 05004], lr: 0.090666, loss: 2.3732
2022-07-15 09:08:01 - train: epoch 0024, iter [03700, 05004], lr: 0.090650, loss: 2.2990
2022-07-15 09:08:34 - train: epoch 0024, iter [03800, 05004], lr: 0.090635, loss: 2.5541
2022-07-15 09:09:07 - train: epoch 0024, iter [03900, 05004], lr: 0.090620, loss: 2.3495
2022-07-15 09:09:41 - train: epoch 0024, iter [04000, 05004], lr: 0.090605, loss: 2.3529
2022-07-15 09:10:14 - train: epoch 0024, iter [04100, 05004], lr: 0.090589, loss: 2.3132
2022-07-15 09:10:48 - train: epoch 0024, iter [04200, 05004], lr: 0.090574, loss: 2.3874
2022-07-15 09:11:20 - train: epoch 0024, iter [04300, 05004], lr: 0.090559, loss: 2.2878
2022-07-15 09:11:54 - train: epoch 0024, iter [04400, 05004], lr: 0.090544, loss: 2.2877
2022-07-15 09:12:27 - train: epoch 0024, iter [04500, 05004], lr: 0.090528, loss: 2.1248
2022-07-15 09:13:00 - train: epoch 0024, iter [04600, 05004], lr: 0.090513, loss: 2.3721
2022-07-15 09:13:34 - train: epoch 0024, iter [04700, 05004], lr: 0.090498, loss: 2.3213
2022-07-15 09:14:08 - train: epoch 0024, iter [04800, 05004], lr: 0.090482, loss: 2.2598
2022-07-15 09:14:43 - train: epoch 0024, iter [04900, 05004], lr: 0.090467, loss: 2.4115
2022-07-15 09:15:13 - train: epoch 0024, iter [05000, 05004], lr: 0.090451, loss: 2.4866
2022-07-15 09:15:14 - train: epoch 024, train_loss: 2.3271
2022-07-15 09:16:29 - eval: epoch: 024, acc1: 52.526%, acc5: 77.634%, test_loss: 2.0566, per_image_load_time: 2.564ms, per_image_inference_time: 0.207ms
2022-07-15 09:16:29 - until epoch: 024, best_acc1: 52.526%
2022-07-15 09:16:29 - epoch 025 lr: 0.090451
2022-07-15 09:17:08 - train: epoch 0025, iter [00100, 05004], lr: 0.090435, loss: 2.1843
2022-07-15 09:17:41 - train: epoch 0025, iter [00200, 05004], lr: 0.090420, loss: 2.1647
2022-07-15 09:18:14 - train: epoch 0025, iter [00300, 05004], lr: 0.090405, loss: 2.1086
2022-07-15 09:18:48 - train: epoch 0025, iter [00400, 05004], lr: 0.090389, loss: 2.3403
2022-07-15 09:19:22 - train: epoch 0025, iter [00500, 05004], lr: 0.090374, loss: 2.1360
2022-07-15 09:19:55 - train: epoch 0025, iter [00600, 05004], lr: 0.090358, loss: 2.3362
2022-07-15 09:20:29 - train: epoch 0025, iter [00700, 05004], lr: 0.090343, loss: 2.4777
2022-07-15 09:21:02 - train: epoch 0025, iter [00800, 05004], lr: 0.090327, loss: 2.3019
2022-07-15 09:21:35 - train: epoch 0025, iter [00900, 05004], lr: 0.090312, loss: 2.1915
2022-07-15 09:22:10 - train: epoch 0025, iter [01000, 05004], lr: 0.090297, loss: 2.2899
2022-07-15 09:22:43 - train: epoch 0025, iter [01100, 05004], lr: 0.090281, loss: 2.2027
2022-07-15 09:23:17 - train: epoch 0025, iter [01200, 05004], lr: 0.090266, loss: 2.3277
2022-07-15 09:23:49 - train: epoch 0025, iter [01300, 05004], lr: 0.090250, loss: 2.3304
2022-07-15 09:24:22 - train: epoch 0025, iter [01400, 05004], lr: 0.090235, loss: 2.3893
2022-07-15 09:24:56 - train: epoch 0025, iter [01500, 05004], lr: 0.090219, loss: 2.2078
2022-07-15 09:25:30 - train: epoch 0025, iter [01600, 05004], lr: 0.090203, loss: 1.9984
2022-07-15 09:26:02 - train: epoch 0025, iter [01700, 05004], lr: 0.090188, loss: 2.2083
2022-07-15 09:26:36 - train: epoch 0025, iter [01800, 05004], lr: 0.090172, loss: 2.1617
2022-07-15 09:27:09 - train: epoch 0025, iter [01900, 05004], lr: 0.090157, loss: 2.1821
2022-07-15 09:27:43 - train: epoch 0025, iter [02000, 05004], lr: 0.090141, loss: 2.3592
2022-07-15 09:28:17 - train: epoch 0025, iter [02100, 05004], lr: 0.090126, loss: 2.1131
2022-07-15 09:28:50 - train: epoch 0025, iter [02200, 05004], lr: 0.090110, loss: 2.1458
2022-07-15 09:29:24 - train: epoch 0025, iter [02300, 05004], lr: 0.090094, loss: 2.3195
2022-07-15 09:29:56 - train: epoch 0025, iter [02400, 05004], lr: 0.090079, loss: 2.0953
2022-07-15 09:30:30 - train: epoch 0025, iter [02500, 05004], lr: 0.090063, loss: 2.4067
2022-07-15 09:31:04 - train: epoch 0025, iter [02600, 05004], lr: 0.090047, loss: 2.4722
2022-07-15 09:31:38 - train: epoch 0025, iter [02700, 05004], lr: 0.090032, loss: 2.4224
2022-07-15 09:32:11 - train: epoch 0025, iter [02800, 05004], lr: 0.090016, loss: 2.3711
2022-07-15 09:32:45 - train: epoch 0025, iter [02900, 05004], lr: 0.090000, loss: 2.5945
2022-07-15 09:33:18 - train: epoch 0025, iter [03000, 05004], lr: 0.089985, loss: 2.6137
2022-07-15 09:33:51 - train: epoch 0025, iter [03100, 05004], lr: 0.089969, loss: 2.3733
2022-07-15 09:34:24 - train: epoch 0025, iter [03200, 05004], lr: 0.089953, loss: 2.5829
2022-07-15 09:34:59 - train: epoch 0025, iter [03300, 05004], lr: 0.089937, loss: 2.2379
2022-07-15 09:35:31 - train: epoch 0025, iter [03400, 05004], lr: 0.089922, loss: 2.4088
2022-07-15 09:36:05 - train: epoch 0025, iter [03500, 05004], lr: 0.089906, loss: 2.0917
2022-07-15 09:36:38 - train: epoch 0025, iter [03600, 05004], lr: 0.089890, loss: 2.2727
2022-07-15 09:37:12 - train: epoch 0025, iter [03700, 05004], lr: 0.089874, loss: 2.1891
2022-07-15 09:37:46 - train: epoch 0025, iter [03800, 05004], lr: 0.089859, loss: 2.3286
2022-07-15 09:38:19 - train: epoch 0025, iter [03900, 05004], lr: 0.089843, loss: 2.5589
2022-07-15 09:38:53 - train: epoch 0025, iter [04000, 05004], lr: 0.089827, loss: 2.3935
2022-07-15 09:39:26 - train: epoch 0025, iter [04100, 05004], lr: 0.089811, loss: 2.6313
2022-07-15 09:40:00 - train: epoch 0025, iter [04200, 05004], lr: 0.089795, loss: 2.4559
2022-07-15 09:40:33 - train: epoch 0025, iter [04300, 05004], lr: 0.089780, loss: 2.1289
2022-07-15 09:41:07 - train: epoch 0025, iter [04400, 05004], lr: 0.089764, loss: 2.3123
2022-07-15 09:41:40 - train: epoch 0025, iter [04500, 05004], lr: 0.089748, loss: 2.3072
2022-07-15 09:42:13 - train: epoch 0025, iter [04600, 05004], lr: 0.089732, loss: 2.2649
2022-07-15 09:42:47 - train: epoch 0025, iter [04700, 05004], lr: 0.089716, loss: 2.1566
2022-07-15 09:43:21 - train: epoch 0025, iter [04800, 05004], lr: 0.089700, loss: 2.0349
2022-07-15 09:43:54 - train: epoch 0025, iter [04900, 05004], lr: 0.089684, loss: 2.3583
2022-07-15 09:44:25 - train: epoch 0025, iter [05000, 05004], lr: 0.089668, loss: 2.4351
2022-07-15 09:44:26 - train: epoch 025, train_loss: 2.3174
2022-07-15 09:45:41 - eval: epoch: 025, acc1: 52.048%, acc5: 77.282%, test_loss: 2.0794, per_image_load_time: 2.682ms, per_image_inference_time: 0.230ms
2022-07-15 09:45:41 - until epoch: 025, best_acc1: 52.526%
2022-07-15 09:45:41 - epoch 026 lr: 0.089668
2022-07-15 09:46:21 - train: epoch 0026, iter [00100, 05004], lr: 0.089652, loss: 2.0806
2022-07-15 09:46:55 - train: epoch 0026, iter [00200, 05004], lr: 0.089636, loss: 2.2820
2022-07-15 09:47:28 - train: epoch 0026, iter [00300, 05004], lr: 0.089620, loss: 2.2533
2022-07-15 09:48:02 - train: epoch 0026, iter [00400, 05004], lr: 0.089604, loss: 2.2971
2022-07-15 09:48:34 - train: epoch 0026, iter [00500, 05004], lr: 0.089588, loss: 2.3277
2022-07-15 09:49:08 - train: epoch 0026, iter [00600, 05004], lr: 0.089572, loss: 2.4094
2022-07-15 09:49:41 - train: epoch 0026, iter [00700, 05004], lr: 0.089556, loss: 2.1799
2022-07-15 09:50:14 - train: epoch 0026, iter [00800, 05004], lr: 0.089540, loss: 2.0997
2022-07-15 09:50:49 - train: epoch 0026, iter [00900, 05004], lr: 0.089524, loss: 2.5233
2022-07-15 09:51:22 - train: epoch 0026, iter [01000, 05004], lr: 0.089508, loss: 2.2017
2022-07-15 09:51:56 - train: epoch 0026, iter [01100, 05004], lr: 0.089492, loss: 2.2868
2022-07-15 09:52:29 - train: epoch 0026, iter [01200, 05004], lr: 0.089476, loss: 2.4087
2022-07-15 09:53:02 - train: epoch 0026, iter [01300, 05004], lr: 0.089460, loss: 2.2872
2022-07-15 09:53:37 - train: epoch 0026, iter [01400, 05004], lr: 0.089444, loss: 2.3126
2022-07-15 09:54:10 - train: epoch 0026, iter [01500, 05004], lr: 0.089428, loss: 2.2168
2022-07-15 09:54:43 - train: epoch 0026, iter [01600, 05004], lr: 0.089411, loss: 2.4306
2022-07-15 09:55:17 - train: epoch 0026, iter [01700, 05004], lr: 0.089395, loss: 2.2107
2022-07-15 09:55:51 - train: epoch 0026, iter [01800, 05004], lr: 0.089379, loss: 2.3264
2022-07-15 09:56:24 - train: epoch 0026, iter [01900, 05004], lr: 0.089363, loss: 2.7436
2022-07-15 09:56:58 - train: epoch 0026, iter [02000, 05004], lr: 0.089347, loss: 2.4360
2022-07-15 09:57:31 - train: epoch 0026, iter [02100, 05004], lr: 0.089331, loss: 2.3887
2022-07-15 09:58:06 - train: epoch 0026, iter [02200, 05004], lr: 0.089315, loss: 2.3100
2022-07-15 09:58:39 - train: epoch 0026, iter [02300, 05004], lr: 0.089299, loss: 2.2193
2022-07-15 09:59:13 - train: epoch 0026, iter [02400, 05004], lr: 0.089282, loss: 2.4629
2022-07-15 09:59:46 - train: epoch 0026, iter [02500, 05004], lr: 0.089266, loss: 2.4909
2022-07-15 10:00:20 - train: epoch 0026, iter [02600, 05004], lr: 0.089250, loss: 2.4010
2022-07-15 10:00:54 - train: epoch 0026, iter [02700, 05004], lr: 0.089234, loss: 2.2554
2022-07-15 10:01:28 - train: epoch 0026, iter [02800, 05004], lr: 0.089218, loss: 2.1174
2022-07-15 10:02:00 - train: epoch 0026, iter [02900, 05004], lr: 0.089201, loss: 2.3905
2022-07-15 10:02:34 - train: epoch 0026, iter [03000, 05004], lr: 0.089185, loss: 2.1725
2022-07-15 10:03:07 - train: epoch 0026, iter [03100, 05004], lr: 0.089169, loss: 2.2938
2022-07-15 10:03:42 - train: epoch 0026, iter [03200, 05004], lr: 0.089153, loss: 2.3486
2022-07-15 10:04:15 - train: epoch 0026, iter [03300, 05004], lr: 0.089136, loss: 2.1887
2022-07-15 10:04:48 - train: epoch 0026, iter [03400, 05004], lr: 0.089120, loss: 2.4781
2022-07-15 10:05:21 - train: epoch 0026, iter [03500, 05004], lr: 0.089104, loss: 2.2857
2022-07-15 10:05:55 - train: epoch 0026, iter [03600, 05004], lr: 0.089087, loss: 2.2650
2022-07-15 10:06:29 - train: epoch 0026, iter [03700, 05004], lr: 0.089071, loss: 2.3292
2022-07-15 10:07:02 - train: epoch 0026, iter [03800, 05004], lr: 0.089055, loss: 2.5243
2022-07-15 10:07:37 - train: epoch 0026, iter [03900, 05004], lr: 0.089038, loss: 2.4417
2022-07-15 10:08:10 - train: epoch 0026, iter [04000, 05004], lr: 0.089022, loss: 2.4918
2022-07-15 10:08:43 - train: epoch 0026, iter [04100, 05004], lr: 0.089006, loss: 2.3446
2022-07-15 10:09:17 - train: epoch 0026, iter [04200, 05004], lr: 0.088989, loss: 2.2647
2022-07-15 10:09:50 - train: epoch 0026, iter [04300, 05004], lr: 0.088973, loss: 2.3549
2022-07-15 10:10:24 - train: epoch 0026, iter [04400, 05004], lr: 0.088957, loss: 2.4836
2022-07-15 10:10:58 - train: epoch 0026, iter [04500, 05004], lr: 0.088940, loss: 2.6223
2022-07-15 10:11:31 - train: epoch 0026, iter [04600, 05004], lr: 0.088924, loss: 2.3214
2022-07-15 10:12:05 - train: epoch 0026, iter [04700, 05004], lr: 0.088907, loss: 2.3030
2022-07-15 10:12:38 - train: epoch 0026, iter [04800, 05004], lr: 0.088891, loss: 2.2092
2022-07-15 10:13:12 - train: epoch 0026, iter [04900, 05004], lr: 0.088874, loss: 2.3457
2022-07-15 10:13:44 - train: epoch 0026, iter [05000, 05004], lr: 0.088858, loss: 2.5486
2022-07-15 10:13:45 - train: epoch 026, train_loss: 2.3069
2022-07-15 10:14:59 - eval: epoch: 026, acc1: 52.468%, acc5: 77.490%, test_loss: 2.0690, per_image_load_time: 2.637ms, per_image_inference_time: 0.229ms
2022-07-15 10:14:59 - until epoch: 026, best_acc1: 52.526%
2022-07-15 10:14:59 - epoch 027 lr: 0.088857
2022-07-15 10:15:37 - train: epoch 0027, iter [00100, 05004], lr: 0.088841, loss: 2.4174
2022-07-15 10:16:11 - train: epoch 0027, iter [00200, 05004], lr: 0.088824, loss: 2.1473
2022-07-15 10:16:44 - train: epoch 0027, iter [00300, 05004], lr: 0.088808, loss: 2.3752
2022-07-15 10:17:18 - train: epoch 0027, iter [00400, 05004], lr: 0.088791, loss: 2.4821
2022-07-15 10:17:51 - train: epoch 0027, iter [00500, 05004], lr: 0.088775, loss: 2.3473
2022-07-15 10:18:25 - train: epoch 0027, iter [00600, 05004], lr: 0.088758, loss: 2.5422
2022-07-15 10:18:59 - train: epoch 0027, iter [00700, 05004], lr: 0.088742, loss: 2.4797
2022-07-15 10:19:32 - train: epoch 0027, iter [00800, 05004], lr: 0.088725, loss: 2.3121
2022-07-15 10:20:05 - train: epoch 0027, iter [00900, 05004], lr: 0.088709, loss: 2.1080
2022-07-15 10:20:38 - train: epoch 0027, iter [01000, 05004], lr: 0.088692, loss: 2.3516
2022-07-15 10:21:11 - train: epoch 0027, iter [01100, 05004], lr: 0.088676, loss: 2.1482
2022-07-15 10:21:45 - train: epoch 0027, iter [01200, 05004], lr: 0.088659, loss: 2.5633
2022-07-15 10:22:19 - train: epoch 0027, iter [01300, 05004], lr: 0.088642, loss: 2.4399
2022-07-15 10:22:51 - train: epoch 0027, iter [01400, 05004], lr: 0.088626, loss: 2.5630
2022-07-15 10:23:25 - train: epoch 0027, iter [01500, 05004], lr: 0.088609, loss: 2.3326
2022-07-15 10:23:58 - train: epoch 0027, iter [01600, 05004], lr: 0.088593, loss: 2.3295
2022-07-15 10:24:32 - train: epoch 0027, iter [01700, 05004], lr: 0.088576, loss: 2.2864
2022-07-15 10:25:05 - train: epoch 0027, iter [01800, 05004], lr: 0.088559, loss: 2.4314
2022-07-15 10:25:38 - train: epoch 0027, iter [01900, 05004], lr: 0.088543, loss: 2.4317
2022-07-15 10:26:11 - train: epoch 0027, iter [02000, 05004], lr: 0.088526, loss: 2.2068
2022-07-15 10:26:45 - train: epoch 0027, iter [02100, 05004], lr: 0.088509, loss: 2.5868
2022-07-15 10:27:18 - train: epoch 0027, iter [02200, 05004], lr: 0.088493, loss: 2.5149
2022-07-15 10:27:52 - train: epoch 0027, iter [02300, 05004], lr: 0.088476, loss: 2.5592
2022-07-15 10:28:25 - train: epoch 0027, iter [02400, 05004], lr: 0.088459, loss: 2.2687
2022-07-15 10:28:58 - train: epoch 0027, iter [02500, 05004], lr: 0.088442, loss: 2.2217
2022-07-15 10:29:32 - train: epoch 0027, iter [02600, 05004], lr: 0.088426, loss: 2.5315
2022-07-15 10:30:05 - train: epoch 0027, iter [02700, 05004], lr: 0.088409, loss: 2.2604
2022-07-15 10:30:38 - train: epoch 0027, iter [02800, 05004], lr: 0.088392, loss: 2.4899
2022-07-15 10:31:11 - train: epoch 0027, iter [02900, 05004], lr: 0.088375, loss: 2.3893
2022-07-15 10:31:46 - train: epoch 0027, iter [03000, 05004], lr: 0.088359, loss: 2.1474
2022-07-15 10:32:19 - train: epoch 0027, iter [03100, 05004], lr: 0.088342, loss: 2.1891
2022-07-15 10:32:52 - train: epoch 0027, iter [03200, 05004], lr: 0.088325, loss: 2.1091
2022-07-15 10:33:26 - train: epoch 0027, iter [03300, 05004], lr: 0.088308, loss: 2.3801
2022-07-15 10:34:00 - train: epoch 0027, iter [03400, 05004], lr: 0.088291, loss: 2.2062
2022-07-15 10:34:32 - train: epoch 0027, iter [03500, 05004], lr: 0.088275, loss: 2.5497
2022-07-15 10:35:06 - train: epoch 0027, iter [03600, 05004], lr: 0.088258, loss: 2.1299
2022-07-15 10:35:39 - train: epoch 0027, iter [03700, 05004], lr: 0.088241, loss: 2.2847
2022-07-15 10:36:13 - train: epoch 0027, iter [03800, 05004], lr: 0.088224, loss: 2.0060
2022-07-15 10:36:46 - train: epoch 0027, iter [03900, 05004], lr: 0.088207, loss: 2.1596
2022-07-15 10:37:20 - train: epoch 0027, iter [04000, 05004], lr: 0.088190, loss: 2.3866
2022-07-15 10:37:53 - train: epoch 0027, iter [04100, 05004], lr: 0.088173, loss: 2.1961
2022-07-15 10:38:26 - train: epoch 0027, iter [04200, 05004], lr: 0.088157, loss: 2.3906
2022-07-15 10:39:00 - train: epoch 0027, iter [04300, 05004], lr: 0.088140, loss: 2.1655
2022-07-15 10:39:33 - train: epoch 0027, iter [04400, 05004], lr: 0.088123, loss: 2.2526
2022-07-15 10:40:06 - train: epoch 0027, iter [04500, 05004], lr: 0.088106, loss: 2.1714
2022-07-15 10:40:38 - train: epoch 0027, iter [04600, 05004], lr: 0.088089, loss: 2.2600
2022-07-15 10:41:12 - train: epoch 0027, iter [04700, 05004], lr: 0.088072, loss: 2.3583
2022-07-15 10:41:45 - train: epoch 0027, iter [04800, 05004], lr: 0.088055, loss: 2.4571
2022-07-15 10:42:19 - train: epoch 0027, iter [04900, 05004], lr: 0.088038, loss: 2.3726
2022-07-15 10:42:51 - train: epoch 0027, iter [05000, 05004], lr: 0.088021, loss: 1.9706
2022-07-15 10:42:52 - train: epoch 027, train_loss: 2.2959
2022-07-15 10:44:06 - eval: epoch: 027, acc1: 52.918%, acc5: 77.578%, test_loss: 2.0681, per_image_load_time: 1.766ms, per_image_inference_time: 0.222ms
2022-07-15 10:44:07 - until epoch: 027, best_acc1: 52.918%
2022-07-15 10:44:07 - epoch 028 lr: 0.088020
2022-07-15 10:44:45 - train: epoch 0028, iter [00100, 05004], lr: 0.088003, loss: 2.0429
2022-07-15 10:45:18 - train: epoch 0028, iter [00200, 05004], lr: 0.087986, loss: 2.1331
2022-07-15 10:45:51 - train: epoch 0028, iter [00300, 05004], lr: 0.087969, loss: 2.2425
2022-07-15 10:46:25 - train: epoch 0028, iter [00400, 05004], lr: 0.087952, loss: 2.2751
2022-07-15 10:46:57 - train: epoch 0028, iter [00500, 05004], lr: 0.087935, loss: 2.1500
2022-07-15 10:47:31 - train: epoch 0028, iter [00600, 05004], lr: 0.087918, loss: 2.3185
2022-07-15 10:48:05 - train: epoch 0028, iter [00700, 05004], lr: 0.087901, loss: 2.5222
2022-07-15 10:48:38 - train: epoch 0028, iter [00800, 05004], lr: 0.087884, loss: 1.9650
2022-07-15 10:49:11 - train: epoch 0028, iter [00900, 05004], lr: 0.087867, loss: 2.0397
2022-07-15 10:49:44 - train: epoch 0028, iter [01000, 05004], lr: 0.087850, loss: 2.3448
2022-07-15 10:50:18 - train: epoch 0028, iter [01100, 05004], lr: 0.087833, loss: 2.3002
2022-07-15 10:50:52 - train: epoch 0028, iter [01200, 05004], lr: 0.087816, loss: 2.0893
2022-07-15 10:51:25 - train: epoch 0028, iter [01300, 05004], lr: 0.087799, loss: 2.2791
2022-07-15 10:51:59 - train: epoch 0028, iter [01400, 05004], lr: 0.087781, loss: 2.5057
2022-07-15 10:52:32 - train: epoch 0028, iter [01500, 05004], lr: 0.087764, loss: 2.4009
2022-07-15 10:53:06 - train: epoch 0028, iter [01600, 05004], lr: 0.087747, loss: 2.2646
2022-07-15 10:53:39 - train: epoch 0028, iter [01700, 05004], lr: 0.087730, loss: 2.2502
2022-07-15 10:54:13 - train: epoch 0028, iter [01800, 05004], lr: 0.087713, loss: 2.1545
2022-07-15 10:54:45 - train: epoch 0028, iter [01900, 05004], lr: 0.087696, loss: 2.2711
2022-07-15 10:55:18 - train: epoch 0028, iter [02000, 05004], lr: 0.087678, loss: 2.4545
2022-07-15 10:55:51 - train: epoch 0028, iter [02100, 05004], lr: 0.087661, loss: 2.4436
2022-07-15 10:56:25 - train: epoch 0028, iter [02200, 05004], lr: 0.087644, loss: 2.4102
2022-07-15 10:56:58 - train: epoch 0028, iter [02300, 05004], lr: 0.087627, loss: 2.4161
2022-07-15 10:57:32 - train: epoch 0028, iter [02400, 05004], lr: 0.087610, loss: 2.5975
2022-07-15 10:58:05 - train: epoch 0028, iter [02500, 05004], lr: 0.087592, loss: 2.2951
2022-07-15 10:58:38 - train: epoch 0028, iter [02600, 05004], lr: 0.087575, loss: 2.0948
2022-07-15 10:59:11 - train: epoch 0028, iter [02700, 05004], lr: 0.087558, loss: 2.3528
2022-07-15 10:59:46 - train: epoch 0028, iter [02800, 05004], lr: 0.087541, loss: 2.2351
2022-07-15 11:00:18 - train: epoch 0028, iter [02900, 05004], lr: 0.087523, loss: 2.2982
2022-07-15 11:00:52 - train: epoch 0028, iter [03000, 05004], lr: 0.087506, loss: 2.3601
2022-07-15 11:01:25 - train: epoch 0028, iter [03100, 05004], lr: 0.087489, loss: 2.4305
2022-07-15 11:01:58 - train: epoch 0028, iter [03200, 05004], lr: 0.087471, loss: 2.1994
2022-07-15 11:02:32 - train: epoch 0028, iter [03300, 05004], lr: 0.087454, loss: 2.2638
2022-07-15 11:03:06 - train: epoch 0028, iter [03400, 05004], lr: 0.087437, loss: 2.2438
2022-07-15 11:03:38 - train: epoch 0028, iter [03500, 05004], lr: 0.087419, loss: 2.1320
2022-07-15 11:04:13 - train: epoch 0028, iter [03600, 05004], lr: 0.087402, loss: 2.0968
2022-07-15 11:04:45 - train: epoch 0028, iter [03700, 05004], lr: 0.087385, loss: 2.1748
2022-07-15 11:05:20 - train: epoch 0028, iter [03800, 05004], lr: 0.087367, loss: 1.9196
2022-07-15 11:05:53 - train: epoch 0028, iter [03900, 05004], lr: 0.087350, loss: 2.2481
2022-07-15 11:06:27 - train: epoch 0028, iter [04000, 05004], lr: 0.087332, loss: 2.4481
2022-07-15 11:07:00 - train: epoch 0028, iter [04100, 05004], lr: 0.087315, loss: 2.5020
2022-07-15 11:07:34 - train: epoch 0028, iter [04200, 05004], lr: 0.087298, loss: 2.4136
2022-07-15 11:08:08 - train: epoch 0028, iter [04300, 05004], lr: 0.087280, loss: 2.2654
2022-07-15 11:08:42 - train: epoch 0028, iter [04400, 05004], lr: 0.087263, loss: 2.2190
2022-07-15 11:09:14 - train: epoch 0028, iter [04500, 05004], lr: 0.087245, loss: 2.1647
2022-07-15 11:09:49 - train: epoch 0028, iter [04600, 05004], lr: 0.087228, loss: 2.3843
2022-07-15 11:10:21 - train: epoch 0028, iter [04700, 05004], lr: 0.087210, loss: 2.2393
2022-07-15 11:10:55 - train: epoch 0028, iter [04800, 05004], lr: 0.087193, loss: 2.1616
2022-07-15 11:11:28 - train: epoch 0028, iter [04900, 05004], lr: 0.087175, loss: 2.1655
2022-07-15 11:12:01 - train: epoch 0028, iter [05000, 05004], lr: 0.087158, loss: 2.2825
2022-07-15 11:12:02 - train: epoch 028, train_loss: 2.2850
2022-07-15 11:13:17 - eval: epoch: 028, acc1: 52.468%, acc5: 77.314%, test_loss: 2.0813, per_image_load_time: 2.351ms, per_image_inference_time: 0.227ms
2022-07-15 11:13:17 - until epoch: 028, best_acc1: 52.918%
2022-07-15 11:13:17 - epoch 029 lr: 0.087157
2022-07-15 11:13:56 - train: epoch 0029, iter [00100, 05004], lr: 0.087140, loss: 2.3604
2022-07-15 11:14:30 - train: epoch 0029, iter [00200, 05004], lr: 0.087122, loss: 2.2927
2022-07-15 11:15:03 - train: epoch 0029, iter [00300, 05004], lr: 0.087105, loss: 2.4335
2022-07-15 11:15:36 - train: epoch 0029, iter [00400, 05004], lr: 0.087087, loss: 2.2541
2022-07-15 11:16:10 - train: epoch 0029, iter [00500, 05004], lr: 0.087070, loss: 2.3405
2022-07-15 11:16:43 - train: epoch 0029, iter [00600, 05004], lr: 0.087052, loss: 2.5651
2022-07-15 11:17:16 - train: epoch 0029, iter [00700, 05004], lr: 0.087034, loss: 1.7975
2022-07-15 11:17:50 - train: epoch 0029, iter [00800, 05004], lr: 0.087017, loss: 2.4590
2022-07-15 11:18:23 - train: epoch 0029, iter [00900, 05004], lr: 0.086999, loss: 2.1439
2022-07-15 11:18:57 - train: epoch 0029, iter [01000, 05004], lr: 0.086982, loss: 2.1364
2022-07-15 11:19:30 - train: epoch 0029, iter [01100, 05004], lr: 0.086964, loss: 2.1664
2022-07-15 11:20:04 - train: epoch 0029, iter [01200, 05004], lr: 0.086946, loss: 2.2803
2022-07-15 11:20:36 - train: epoch 0029, iter [01300, 05004], lr: 0.086929, loss: 2.1540
2022-07-15 11:21:10 - train: epoch 0029, iter [01400, 05004], lr: 0.086911, loss: 2.4706
2022-07-15 11:21:44 - train: epoch 0029, iter [01500, 05004], lr: 0.086894, loss: 2.2688
2022-07-15 11:22:17 - train: epoch 0029, iter [01600, 05004], lr: 0.086876, loss: 2.2937
2022-07-15 11:22:50 - train: epoch 0029, iter [01700, 05004], lr: 0.086858, loss: 2.2669
2022-07-15 11:23:23 - train: epoch 0029, iter [01800, 05004], lr: 0.086841, loss: 2.2846
2022-07-15 11:23:57 - train: epoch 0029, iter [01900, 05004], lr: 0.086823, loss: 2.1226
2022-07-15 11:24:31 - train: epoch 0029, iter [02000, 05004], lr: 0.086805, loss: 2.4811
2022-07-15 11:25:04 - train: epoch 0029, iter [02100, 05004], lr: 0.086787, loss: 2.2603
2022-07-15 11:25:38 - train: epoch 0029, iter [02200, 05004], lr: 0.086770, loss: 2.3584
2022-07-15 11:26:10 - train: epoch 0029, iter [02300, 05004], lr: 0.086752, loss: 2.3299
2022-07-15 11:26:45 - train: epoch 0029, iter [02400, 05004], lr: 0.086734, loss: 2.1444
2022-07-15 11:27:18 - train: epoch 0029, iter [02500, 05004], lr: 0.086716, loss: 2.2023
2022-07-15 11:27:52 - train: epoch 0029, iter [02600, 05004], lr: 0.086699, loss: 2.2677
2022-07-15 11:28:25 - train: epoch 0029, iter [02700, 05004], lr: 0.086681, loss: 2.2410
2022-07-15 11:28:59 - train: epoch 0029, iter [02800, 05004], lr: 0.086663, loss: 2.1946
2022-07-15 11:29:32 - train: epoch 0029, iter [02900, 05004], lr: 0.086645, loss: 2.1422
2022-07-15 11:30:06 - train: epoch 0029, iter [03000, 05004], lr: 0.086628, loss: 2.2481
2022-07-15 11:30:39 - train: epoch 0029, iter [03100, 05004], lr: 0.086610, loss: 2.1590
2022-07-15 11:31:13 - train: epoch 0029, iter [03200, 05004], lr: 0.086592, loss: 2.3255
2022-07-15 11:31:47 - train: epoch 0029, iter [03300, 05004], lr: 0.086574, loss: 2.2385
2022-07-15 11:32:20 - train: epoch 0029, iter [03400, 05004], lr: 0.086556, loss: 1.9587
2022-07-15 11:32:54 - train: epoch 0029, iter [03500, 05004], lr: 0.086538, loss: 2.3711
2022-07-15 11:33:27 - train: epoch 0029, iter [03600, 05004], lr: 0.086521, loss: 2.0529
2022-07-15 11:34:01 - train: epoch 0029, iter [03700, 05004], lr: 0.086503, loss: 2.2242
2022-07-15 11:34:34 - train: epoch 0029, iter [03800, 05004], lr: 0.086485, loss: 2.2743
2022-07-15 11:35:07 - train: epoch 0029, iter [03900, 05004], lr: 0.086467, loss: 2.0718
2022-07-15 11:35:41 - train: epoch 0029, iter [04000, 05004], lr: 0.086449, loss: 2.1993
2022-07-15 11:36:13 - train: epoch 0029, iter [04100, 05004], lr: 0.086431, loss: 2.0539
2022-07-15 11:36:47 - train: epoch 0029, iter [04200, 05004], lr: 0.086413, loss: 2.1721
2022-07-15 11:37:20 - train: epoch 0029, iter [04300, 05004], lr: 0.086395, loss: 2.4381
2022-07-15 11:37:53 - train: epoch 0029, iter [04400, 05004], lr: 0.086377, loss: 2.2846
2022-07-15 11:38:27 - train: epoch 0029, iter [04500, 05004], lr: 0.086359, loss: 2.4555
2022-07-15 11:39:00 - train: epoch 0029, iter [04600, 05004], lr: 0.086341, loss: 2.4162
2022-07-15 11:39:32 - train: epoch 0029, iter [04700, 05004], lr: 0.086323, loss: 2.1562
2022-07-15 11:40:05 - train: epoch 0029, iter [04800, 05004], lr: 0.086305, loss: 2.3041
2022-07-15 11:40:38 - train: epoch 0029, iter [04900, 05004], lr: 0.086287, loss: 2.7188
2022-07-15 11:41:09 - train: epoch 0029, iter [05000, 05004], lr: 0.086269, loss: 2.0530
2022-07-15 11:41:10 - train: epoch 029, train_loss: 2.2765
2022-07-15 11:42:25 - eval: epoch: 029, acc1: 52.980%, acc5: 77.752%, test_loss: 2.0438, per_image_load_time: 1.142ms, per_image_inference_time: 0.219ms
2022-07-15 11:42:25 - until epoch: 029, best_acc1: 52.980%
2022-07-15 11:42:25 - epoch 030 lr: 0.086269
2022-07-15 11:43:04 - train: epoch 0030, iter [00100, 05004], lr: 0.086251, loss: 2.4276
2022-07-15 11:43:36 - train: epoch 0030, iter [00200, 05004], lr: 0.086233, loss: 2.4093
2022-07-15 11:44:10 - train: epoch 0030, iter [00300, 05004], lr: 0.086215, loss: 2.1561
2022-07-15 11:44:44 - train: epoch 0030, iter [00400, 05004], lr: 0.086197, loss: 1.9697
2022-07-15 11:45:17 - train: epoch 0030, iter [00500, 05004], lr: 0.086179, loss: 2.5384
2022-07-15 11:45:50 - train: epoch 0030, iter [00600, 05004], lr: 0.086160, loss: 2.0637
2022-07-15 11:46:23 - train: epoch 0030, iter [00700, 05004], lr: 0.086142, loss: 2.2346
2022-07-15 11:46:55 - train: epoch 0030, iter [00800, 05004], lr: 0.086124, loss: 2.3696
2022-07-15 11:47:29 - train: epoch 0030, iter [00900, 05004], lr: 0.086106, loss: 2.2741
2022-07-15 11:48:02 - train: epoch 0030, iter [01000, 05004], lr: 0.086088, loss: 1.9815
2022-07-15 11:48:35 - train: epoch 0030, iter [01100, 05004], lr: 0.086070, loss: 2.0253
2022-07-15 11:49:07 - train: epoch 0030, iter [01200, 05004], lr: 0.086052, loss: 2.2747
2022-07-15 11:49:40 - train: epoch 0030, iter [01300, 05004], lr: 0.086034, loss: 2.0352
2022-07-15 11:50:14 - train: epoch 0030, iter [01400, 05004], lr: 0.086016, loss: 2.1012
2022-07-15 11:50:47 - train: epoch 0030, iter [01500, 05004], lr: 0.085998, loss: 2.2308
2022-07-15 11:51:20 - train: epoch 0030, iter [01600, 05004], lr: 0.085979, loss: 2.2651
2022-07-15 11:51:53 - train: epoch 0030, iter [01700, 05004], lr: 0.085961, loss: 2.4470
2022-07-15 11:52:26 - train: epoch 0030, iter [01800, 05004], lr: 0.085943, loss: 2.3359
2022-07-15 11:52:59 - train: epoch 0030, iter [01900, 05004], lr: 0.085925, loss: 2.4701
2022-07-15 11:53:32 - train: epoch 0030, iter [02000, 05004], lr: 0.085907, loss: 2.2624
2022-07-15 11:54:05 - train: epoch 0030, iter [02100, 05004], lr: 0.085888, loss: 2.4394
2022-07-15 11:54:38 - train: epoch 0030, iter [02200, 05004], lr: 0.085870, loss: 2.2410
2022-07-15 11:55:11 - train: epoch 0030, iter [02300, 05004], lr: 0.085852, loss: 2.2670
2022-07-15 11:55:44 - train: epoch 0030, iter [02400, 05004], lr: 0.085834, loss: 2.4662
2022-07-15 11:56:17 - train: epoch 0030, iter [02500, 05004], lr: 0.085815, loss: 2.3755
2022-07-15 11:56:51 - train: epoch 0030, iter [02600, 05004], lr: 0.085797, loss: 2.3758
2022-07-15 11:57:23 - train: epoch 0030, iter [02700, 05004], lr: 0.085779, loss: 2.1573
2022-07-15 11:57:57 - train: epoch 0030, iter [02800, 05004], lr: 0.085761, loss: 2.2232
2022-07-15 11:58:30 - train: epoch 0030, iter [02900, 05004], lr: 0.085742, loss: 2.3034
2022-07-15 11:59:03 - train: epoch 0030, iter [03000, 05004], lr: 0.085724, loss: 2.4473
2022-07-15 11:59:36 - train: epoch 0030, iter [03100, 05004], lr: 0.085706, loss: 2.2145
2022-07-15 12:00:09 - train: epoch 0030, iter [03200, 05004], lr: 0.085687, loss: 2.2166
2022-07-15 12:00:43 - train: epoch 0030, iter [03300, 05004], lr: 0.085669, loss: 2.4409
2022-07-15 12:01:15 - train: epoch 0030, iter [03400, 05004], lr: 0.085651, loss: 2.2757
2022-07-15 12:01:49 - train: epoch 0030, iter [03500, 05004], lr: 0.085632, loss: 2.3696
2022-07-15 12:02:22 - train: epoch 0030, iter [03600, 05004], lr: 0.085614, loss: 2.0970
2022-07-15 12:02:55 - train: epoch 0030, iter [03700, 05004], lr: 0.085596, loss: 2.2629
2022-07-15 12:03:28 - train: epoch 0030, iter [03800, 05004], lr: 0.085577, loss: 2.3716
2022-07-15 12:04:01 - train: epoch 0030, iter [03900, 05004], lr: 0.085559, loss: 2.3336
2022-07-15 12:04:34 - train: epoch 0030, iter [04000, 05004], lr: 0.085541, loss: 2.1020
2022-07-15 12:05:07 - train: epoch 0030, iter [04100, 05004], lr: 0.085522, loss: 2.1471
2022-07-15 12:05:41 - train: epoch 0030, iter [04200, 05004], lr: 0.085504, loss: 2.4244
2022-07-15 12:06:14 - train: epoch 0030, iter [04300, 05004], lr: 0.085485, loss: 2.1694
2022-07-15 12:06:48 - train: epoch 0030, iter [04400, 05004], lr: 0.085467, loss: 2.3269
2022-07-15 12:07:20 - train: epoch 0030, iter [04500, 05004], lr: 0.085448, loss: 2.5397
2022-07-15 12:07:54 - train: epoch 0030, iter [04600, 05004], lr: 0.085430, loss: 1.9896
2022-07-15 12:08:27 - train: epoch 0030, iter [04700, 05004], lr: 0.085412, loss: 2.3040
2022-07-15 12:09:00 - train: epoch 0030, iter [04800, 05004], lr: 0.085393, loss: 2.3450
2022-07-15 12:09:33 - train: epoch 0030, iter [04900, 05004], lr: 0.085375, loss: 2.4127
2022-07-15 12:10:05 - train: epoch 0030, iter [05000, 05004], lr: 0.085356, loss: 2.3677
2022-07-15 12:10:06 - train: epoch 030, train_loss: 2.2715
2022-07-15 12:11:20 - eval: epoch: 030, acc1: 53.348%, acc5: 77.968%, test_loss: 2.0391, per_image_load_time: 1.614ms, per_image_inference_time: 0.220ms
2022-07-15 12:11:20 - until epoch: 030, best_acc1: 53.348%
2022-07-15 12:11:20 - epoch 031 lr: 0.085355
2022-07-15 12:11:59 - train: epoch 0031, iter [00100, 05004], lr: 0.085337, loss: 2.3491
2022-07-15 12:12:33 - train: epoch 0031, iter [00200, 05004], lr: 0.085318, loss: 2.1523
2022-07-15 12:13:07 - train: epoch 0031, iter [00300, 05004], lr: 0.085300, loss: 2.1201
2022-07-15 12:13:40 - train: epoch 0031, iter [00400, 05004], lr: 0.085281, loss: 2.3336
2022-07-15 12:14:13 - train: epoch 0031, iter [00500, 05004], lr: 0.085263, loss: 2.1189
2022-07-15 12:14:48 - train: epoch 0031, iter [00600, 05004], lr: 0.085244, loss: 2.2492
2022-07-15 12:15:21 - train: epoch 0031, iter [00700, 05004], lr: 0.085226, loss: 2.2720
2022-07-15 12:15:54 - train: epoch 0031, iter [00800, 05004], lr: 0.085207, loss: 2.1472
2022-07-15 12:16:28 - train: epoch 0031, iter [00900, 05004], lr: 0.085188, loss: 2.1202
2022-07-15 12:17:02 - train: epoch 0031, iter [01000, 05004], lr: 0.085170, loss: 2.5304
2022-07-15 12:17:34 - train: epoch 0031, iter [01100, 05004], lr: 0.085151, loss: 2.4810
2022-07-15 12:18:08 - train: epoch 0031, iter [01200, 05004], lr: 0.085133, loss: 2.4150
2022-07-15 12:18:42 - train: epoch 0031, iter [01300, 05004], lr: 0.085114, loss: 1.9761
2022-07-15 12:19:14 - train: epoch 0031, iter [01400, 05004], lr: 0.085095, loss: 2.2086
2022-07-15 12:19:47 - train: epoch 0031, iter [01500, 05004], lr: 0.085077, loss: 2.3650
2022-07-15 12:20:20 - train: epoch 0031, iter [01600, 05004], lr: 0.085058, loss: 2.2424
2022-07-15 12:20:53 - train: epoch 0031, iter [01700, 05004], lr: 0.085039, loss: 2.0228
2022-07-15 12:21:26 - train: epoch 0031, iter [01800, 05004], lr: 0.085021, loss: 2.1356
2022-07-15 12:21:59 - train: epoch 0031, iter [01900, 05004], lr: 0.085002, loss: 2.2620
2022-07-15 12:22:32 - train: epoch 0031, iter [02000, 05004], lr: 0.084983, loss: 2.3690
2022-07-15 12:23:05 - train: epoch 0031, iter [02100, 05004], lr: 0.084965, loss: 2.0738
2022-07-15 12:23:39 - train: epoch 0031, iter [02200, 05004], lr: 0.084946, loss: 2.1523
2022-07-15 12:24:13 - train: epoch 0031, iter [02300, 05004], lr: 0.084927, loss: 2.0292
2022-07-15 12:24:45 - train: epoch 0031, iter [02400, 05004], lr: 0.084909, loss: 2.3064
2022-07-15 12:25:19 - train: epoch 0031, iter [02500, 05004], lr: 0.084890, loss: 2.1583
2022-07-15 12:25:52 - train: epoch 0031, iter [02600, 05004], lr: 0.084871, loss: 2.1558
2022-07-15 12:26:25 - train: epoch 0031, iter [02700, 05004], lr: 0.084852, loss: 2.4239
2022-07-15 12:26:59 - train: epoch 0031, iter [02800, 05004], lr: 0.084834, loss: 2.5760
2022-07-15 12:27:34 - train: epoch 0031, iter [02900, 05004], lr: 0.084815, loss: 2.2377
2022-07-15 12:28:07 - train: epoch 0031, iter [03000, 05004], lr: 0.084796, loss: 2.4028
2022-07-15 12:28:40 - train: epoch 0031, iter [03100, 05004], lr: 0.084777, loss: 2.1103
2022-07-15 12:29:14 - train: epoch 0031, iter [03200, 05004], lr: 0.084759, loss: 2.4116
2022-07-15 12:29:48 - train: epoch 0031, iter [03300, 05004], lr: 0.084740, loss: 2.2983
2022-07-15 12:30:21 - train: epoch 0031, iter [03400, 05004], lr: 0.084721, loss: 2.3725
2022-07-15 12:30:56 - train: epoch 0031, iter [03500, 05004], lr: 0.084702, loss: 2.4141
2022-07-15 12:31:28 - train: epoch 0031, iter [03600, 05004], lr: 0.084683, loss: 2.2808
2022-07-15 12:32:02 - train: epoch 0031, iter [03700, 05004], lr: 0.084664, loss: 2.1115
2022-07-15 12:32:36 - train: epoch 0031, iter [03800, 05004], lr: 0.084646, loss: 2.1026
2022-07-15 12:33:09 - train: epoch 0031, iter [03900, 05004], lr: 0.084627, loss: 2.2259
2022-07-15 12:33:43 - train: epoch 0031, iter [04000, 05004], lr: 0.084608, loss: 2.1793
2022-07-15 12:34:17 - train: epoch 0031, iter [04100, 05004], lr: 0.084589, loss: 2.2493
2022-07-15 12:34:50 - train: epoch 0031, iter [04200, 05004], lr: 0.084570, loss: 2.3439
2022-07-15 12:35:24 - train: epoch 0031, iter [04300, 05004], lr: 0.084551, loss: 2.2117
2022-07-15 12:35:59 - train: epoch 0031, iter [04400, 05004], lr: 0.084532, loss: 2.4680
2022-07-15 12:36:32 - train: epoch 0031, iter [04500, 05004], lr: 0.084513, loss: 2.3629
2022-07-15 12:37:06 - train: epoch 0031, iter [04600, 05004], lr: 0.084494, loss: 2.3202
2022-07-15 12:37:39 - train: epoch 0031, iter [04700, 05004], lr: 0.084475, loss: 2.2937
2022-07-15 12:38:13 - train: epoch 0031, iter [04800, 05004], lr: 0.084456, loss: 2.1530
2022-07-15 12:38:46 - train: epoch 0031, iter [04900, 05004], lr: 0.084437, loss: 2.0454
2022-07-15 12:39:18 - train: epoch 0031, iter [05000, 05004], lr: 0.084418, loss: 2.2257
2022-07-15 12:39:19 - train: epoch 031, train_loss: 2.2590
2022-07-15 12:40:34 - eval: epoch: 031, acc1: 53.342%, acc5: 78.120%, test_loss: 2.0267, per_image_load_time: 1.063ms, per_image_inference_time: 0.209ms
2022-07-15 12:40:34 - until epoch: 031, best_acc1: 53.348%
2022-07-15 12:40:34 - epoch 032 lr: 0.084418
2022-07-15 12:41:13 - train: epoch 0032, iter [00100, 05004], lr: 0.084399, loss: 2.1460
2022-07-15 12:41:46 - train: epoch 0032, iter [00200, 05004], lr: 0.084380, loss: 2.3357
2022-07-15 12:42:20 - train: epoch 0032, iter [00300, 05004], lr: 0.084361, loss: 2.3161
2022-07-15 12:42:53 - train: epoch 0032, iter [00400, 05004], lr: 0.084342, loss: 2.4632
2022-07-15 12:43:27 - train: epoch 0032, iter [00500, 05004], lr: 0.084323, loss: 2.2982
2022-07-15 12:44:00 - train: epoch 0032, iter [00600, 05004], lr: 0.084304, loss: 2.3938
2022-07-15 12:44:32 - train: epoch 0032, iter [00700, 05004], lr: 0.084285, loss: 2.1483
2022-07-15 12:45:06 - train: epoch 0032, iter [00800, 05004], lr: 0.084266, loss: 2.2437
2022-07-15 12:45:40 - train: epoch 0032, iter [00900, 05004], lr: 0.084247, loss: 2.2068
2022-07-15 12:46:13 - train: epoch 0032, iter [01000, 05004], lr: 0.084228, loss: 2.3166
2022-07-15 12:46:47 - train: epoch 0032, iter [01100, 05004], lr: 0.084208, loss: 2.3617
2022-07-15 12:47:20 - train: epoch 0032, iter [01200, 05004], lr: 0.084189, loss: 2.3944
2022-07-15 12:47:54 - train: epoch 0032, iter [01300, 05004], lr: 0.084170, loss: 2.2412
2022-07-15 12:48:27 - train: epoch 0032, iter [01400, 05004], lr: 0.084151, loss: 2.4233
2022-07-15 12:49:01 - train: epoch 0032, iter [01500, 05004], lr: 0.084132, loss: 2.4016
2022-07-15 12:49:34 - train: epoch 0032, iter [01600, 05004], lr: 0.084113, loss: 2.4204
2022-07-15 12:50:08 - train: epoch 0032, iter [01700, 05004], lr: 0.084094, loss: 2.3675
2022-07-15 12:50:42 - train: epoch 0032, iter [01800, 05004], lr: 0.084075, loss: 2.5877
2022-07-15 12:51:14 - train: epoch 0032, iter [01900, 05004], lr: 0.084056, loss: 2.1274
2022-07-15 12:51:48 - train: epoch 0032, iter [02000, 05004], lr: 0.084036, loss: 2.2609
2022-07-15 12:52:22 - train: epoch 0032, iter [02100, 05004], lr: 0.084017, loss: 2.0855
2022-07-15 12:52:55 - train: epoch 0032, iter [02200, 05004], lr: 0.083998, loss: 2.1486
2022-07-15 12:53:28 - train: epoch 0032, iter [02300, 05004], lr: 0.083979, loss: 2.2771
2022-07-15 12:54:02 - train: epoch 0032, iter [02400, 05004], lr: 0.083960, loss: 2.2272
2022-07-15 12:54:35 - train: epoch 0032, iter [02500, 05004], lr: 0.083940, loss: 2.4742
2022-07-15 12:55:10 - train: epoch 0032, iter [02600, 05004], lr: 0.083921, loss: 2.0074
2022-07-15 12:55:42 - train: epoch 0032, iter [02700, 05004], lr: 0.083902, loss: 2.3579
2022-07-15 12:56:15 - train: epoch 0032, iter [02800, 05004], lr: 0.083883, loss: 2.3321
2022-07-15 12:56:48 - train: epoch 0032, iter [02900, 05004], lr: 0.083864, loss: 2.1058
2022-07-15 12:57:23 - train: epoch 0032, iter [03000, 05004], lr: 0.083844, loss: 2.0910
2022-07-15 12:57:55 - train: epoch 0032, iter [03100, 05004], lr: 0.083825, loss: 2.3941
2022-07-15 12:58:29 - train: epoch 0032, iter [03200, 05004], lr: 0.083806, loss: 2.2907
2022-07-15 12:59:01 - train: epoch 0032, iter [03300, 05004], lr: 0.083786, loss: 2.0424
2022-07-15 12:59:35 - train: epoch 0032, iter [03400, 05004], lr: 0.083767, loss: 2.1216
2022-07-15 13:00:09 - train: epoch 0032, iter [03500, 05004], lr: 0.083748, loss: 2.2315
2022-07-15 13:00:42 - train: epoch 0032, iter [03600, 05004], lr: 0.083729, loss: 2.4148
2022-07-15 13:01:16 - train: epoch 0032, iter [03700, 05004], lr: 0.083709, loss: 2.2344
2022-07-15 13:01:49 - train: epoch 0032, iter [03800, 05004], lr: 0.083690, loss: 2.0959
2022-07-15 13:02:23 - train: epoch 0032, iter [03900, 05004], lr: 0.083671, loss: 2.3015
2022-07-15 13:02:56 - train: epoch 0032, iter [04000, 05004], lr: 0.083651, loss: 2.1244
2022-07-15 13:03:30 - train: epoch 0032, iter [04100, 05004], lr: 0.083632, loss: 2.1411
2022-07-15 13:04:02 - train: epoch 0032, iter [04200, 05004], lr: 0.083613, loss: 2.1010
2022-07-15 13:04:36 - train: epoch 0032, iter [04300, 05004], lr: 0.083593, loss: 2.5148
2022-07-15 13:05:10 - train: epoch 0032, iter [04400, 05004], lr: 0.083574, loss: 2.3039
2022-07-15 13:05:43 - train: epoch 0032, iter [04500, 05004], lr: 0.083554, loss: 2.4511
2022-07-15 13:06:18 - train: epoch 0032, iter [04600, 05004], lr: 0.083535, loss: 2.2663
2022-07-15 13:06:51 - train: epoch 0032, iter [04700, 05004], lr: 0.083516, loss: 2.3430
2022-07-15 13:07:24 - train: epoch 0032, iter [04800, 05004], lr: 0.083496, loss: 2.0632
2022-07-15 13:07:58 - train: epoch 0032, iter [04900, 05004], lr: 0.083477, loss: 2.3969
2022-07-15 13:08:30 - train: epoch 0032, iter [05000, 05004], lr: 0.083457, loss: 2.2957
2022-07-15 13:08:31 - train: epoch 032, train_loss: 2.2514
2022-07-15 13:09:46 - eval: epoch: 032, acc1: 53.708%, acc5: 78.496%, test_loss: 1.9960, per_image_load_time: 2.311ms, per_image_inference_time: 0.227ms
2022-07-15 13:09:46 - until epoch: 032, best_acc1: 53.708%
2022-07-15 13:09:46 - epoch 033 lr: 0.083456
2022-07-15 13:10:24 - train: epoch 0033, iter [00100, 05004], lr: 0.083437, loss: 2.1255
2022-07-15 13:10:58 - train: epoch 0033, iter [00200, 05004], lr: 0.083418, loss: 2.3102
2022-07-15 13:11:32 - train: epoch 0033, iter [00300, 05004], lr: 0.083398, loss: 2.0241
2022-07-15 13:12:05 - train: epoch 0033, iter [00400, 05004], lr: 0.083379, loss: 2.0391
2022-07-15 13:12:39 - train: epoch 0033, iter [00500, 05004], lr: 0.083359, loss: 2.3631
2022-07-15 13:13:12 - train: epoch 0033, iter [00600, 05004], lr: 0.083340, loss: 2.2339
2022-07-15 13:13:46 - train: epoch 0033, iter [00700, 05004], lr: 0.083320, loss: 2.3321
2022-07-15 13:14:19 - train: epoch 0033, iter [00800, 05004], lr: 0.083301, loss: 2.3685
2022-07-15 13:14:52 - train: epoch 0033, iter [00900, 05004], lr: 0.083281, loss: 2.3245
2022-07-15 13:15:27 - train: epoch 0033, iter [01000, 05004], lr: 0.083262, loss: 2.2221
2022-07-15 13:15:59 - train: epoch 0033, iter [01100, 05004], lr: 0.083242, loss: 2.1219
2022-07-15 13:16:33 - train: epoch 0033, iter [01200, 05004], lr: 0.083223, loss: 2.2983
2022-07-15 13:17:06 - train: epoch 0033, iter [01300, 05004], lr: 0.083203, loss: 2.1084
2022-07-15 13:17:39 - train: epoch 0033, iter [01400, 05004], lr: 0.083183, loss: 2.4023
2022-07-15 13:18:13 - train: epoch 0033, iter [01500, 05004], lr: 0.083164, loss: 2.5293
2022-07-15 13:18:46 - train: epoch 0033, iter [01600, 05004], lr: 0.083144, loss: 2.4232
2022-07-15 13:19:20 - train: epoch 0033, iter [01700, 05004], lr: 0.083125, loss: 2.2191
2022-07-15 13:19:53 - train: epoch 0033, iter [01800, 05004], lr: 0.083105, loss: 2.4235
2022-07-15 13:20:26 - train: epoch 0033, iter [01900, 05004], lr: 0.083086, loss: 2.2878
2022-07-15 13:20:59 - train: epoch 0033, iter [02000, 05004], lr: 0.083066, loss: 2.0779
2022-07-15 13:21:32 - train: epoch 0033, iter [02100, 05004], lr: 0.083046, loss: 2.2109
2022-07-15 13:22:06 - train: epoch 0033, iter [02200, 05004], lr: 0.083027, loss: 2.2795
2022-07-15 13:22:39 - train: epoch 0033, iter [02300, 05004], lr: 0.083007, loss: 2.1167
2022-07-15 13:23:13 - train: epoch 0033, iter [02400, 05004], lr: 0.082987, loss: 2.4196
2022-07-15 13:23:46 - train: epoch 0033, iter [02500, 05004], lr: 0.082968, loss: 2.2851
2022-07-15 13:24:20 - train: epoch 0033, iter [02600, 05004], lr: 0.082948, loss: 2.0891
2022-07-15 13:24:53 - train: epoch 0033, iter [02700, 05004], lr: 0.082928, loss: 2.4772
2022-07-15 13:25:27 - train: epoch 0033, iter [02800, 05004], lr: 0.082909, loss: 2.1856
2022-07-15 13:26:00 - train: epoch 0033, iter [02900, 05004], lr: 0.082889, loss: 2.5054
2022-07-15 13:26:33 - train: epoch 0033, iter [03000, 05004], lr: 0.082869, loss: 2.2889
2022-07-15 13:27:07 - train: epoch 0033, iter [03100, 05004], lr: 0.082850, loss: 2.3091
2022-07-15 13:27:41 - train: epoch 0033, iter [03200, 05004], lr: 0.082830, loss: 2.0765
2022-07-15 13:28:13 - train: epoch 0033, iter [03300, 05004], lr: 0.082810, loss: 2.2238
2022-07-15 13:28:47 - train: epoch 0033, iter [03400, 05004], lr: 0.082790, loss: 2.1270
2022-07-15 13:29:20 - train: epoch 0033, iter [03500, 05004], lr: 0.082771, loss: 2.2814
2022-07-15 13:29:54 - train: epoch 0033, iter [03600, 05004], lr: 0.082751, loss: 2.5029
2022-07-15 13:30:27 - train: epoch 0033, iter [03700, 05004], lr: 0.082731, loss: 2.2907
2022-07-15 13:31:01 - train: epoch 0033, iter [03800, 05004], lr: 0.082711, loss: 2.2344
2022-07-15 13:31:34 - train: epoch 0033, iter [03900, 05004], lr: 0.082691, loss: 2.4659
2022-07-15 13:32:07 - train: epoch 0033, iter [04000, 05004], lr: 0.082672, loss: 2.3091
2022-07-15 13:32:41 - train: epoch 0033, iter [04100, 05004], lr: 0.082652, loss: 2.3360
2022-07-15 13:33:16 - train: epoch 0033, iter [04200, 05004], lr: 0.082632, loss: 2.3528
2022-07-15 13:33:48 - train: epoch 0033, iter [04300, 05004], lr: 0.082612, loss: 2.3311
2022-07-15 13:34:22 - train: epoch 0033, iter [04400, 05004], lr: 0.082592, loss: 2.2462
2022-07-15 13:34:55 - train: epoch 0033, iter [04500, 05004], lr: 0.082573, loss: 2.5031
2022-07-15 13:35:28 - train: epoch 0033, iter [04600, 05004], lr: 0.082553, loss: 2.1618
2022-07-15 13:36:02 - train: epoch 0033, iter [04700, 05004], lr: 0.082533, loss: 2.1681
2022-07-15 13:36:35 - train: epoch 0033, iter [04800, 05004], lr: 0.082513, loss: 2.7068
2022-07-15 13:37:09 - train: epoch 0033, iter [04900, 05004], lr: 0.082493, loss: 2.0975
2022-07-15 13:37:40 - train: epoch 0033, iter [05000, 05004], lr: 0.082473, loss: 2.2594
2022-07-15 13:37:41 - train: epoch 033, train_loss: 2.2415
2022-07-15 13:38:56 - eval: epoch: 033, acc1: 53.424%, acc5: 78.464%, test_loss: 2.0024, per_image_load_time: 2.293ms, per_image_inference_time: 0.223ms
2022-07-15 13:38:56 - until epoch: 033, best_acc1: 53.708%
2022-07-15 13:38:56 - epoch 034 lr: 0.082472
2022-07-15 13:39:35 - train: epoch 0034, iter [00100, 05004], lr: 0.082453, loss: 2.2154
2022-07-15 13:40:08 - train: epoch 0034, iter [00200, 05004], lr: 0.082433, loss: 2.0007
2022-07-15 13:40:42 - train: epoch 0034, iter [00300, 05004], lr: 0.082413, loss: 2.1033
2022-07-15 13:41:16 - train: epoch 0034, iter [00400, 05004], lr: 0.082393, loss: 1.9054
2022-07-15 13:41:50 - train: epoch 0034, iter [00500, 05004], lr: 0.082373, loss: 2.0474
2022-07-15 13:42:23 - train: epoch 0034, iter [00600, 05004], lr: 0.082353, loss: 2.4088
2022-07-15 13:42:56 - train: epoch 0034, iter [00700, 05004], lr: 0.082333, loss: 2.2676
2022-07-15 13:43:30 - train: epoch 0034, iter [00800, 05004], lr: 0.082313, loss: 2.2396
2022-07-15 13:44:03 - train: epoch 0034, iter [00900, 05004], lr: 0.082293, loss: 2.1620
2022-07-15 13:44:37 - train: epoch 0034, iter [01000, 05004], lr: 0.082273, loss: 2.2388
2022-07-15 13:45:10 - train: epoch 0034, iter [01100, 05004], lr: 0.082253, loss: 2.2936
2022-07-15 13:45:44 - train: epoch 0034, iter [01200, 05004], lr: 0.082233, loss: 2.2172
2022-07-15 13:46:17 - train: epoch 0034, iter [01300, 05004], lr: 0.082213, loss: 2.1936
2022-07-15 13:46:50 - train: epoch 0034, iter [01400, 05004], lr: 0.082193, loss: 2.2460
2022-07-15 13:47:24 - train: epoch 0034, iter [01500, 05004], lr: 0.082173, loss: 2.1857
2022-07-15 13:47:57 - train: epoch 0034, iter [01600, 05004], lr: 0.082153, loss: 2.1813
2022-07-15 13:48:31 - train: epoch 0034, iter [01700, 05004], lr: 0.082133, loss: 2.1736
2022-07-15 13:49:04 - train: epoch 0034, iter [01800, 05004], lr: 0.082113, loss: 2.5267
2022-07-15 13:49:37 - train: epoch 0034, iter [01900, 05004], lr: 0.082093, loss: 2.3018
2022-07-15 13:50:11 - train: epoch 0034, iter [02000, 05004], lr: 0.082073, loss: 2.2935
2022-07-15 13:50:45 - train: epoch 0034, iter [02100, 05004], lr: 0.082053, loss: 2.3629
2022-07-15 13:51:19 - train: epoch 0034, iter [02200, 05004], lr: 0.082033, loss: 2.0784
2022-07-15 13:51:52 - train: epoch 0034, iter [02300, 05004], lr: 0.082013, loss: 2.2853
2022-07-15 13:52:26 - train: epoch 0034, iter [02400, 05004], lr: 0.081992, loss: 2.1227
2022-07-15 13:52:59 - train: epoch 0034, iter [02500, 05004], lr: 0.081972, loss: 2.2208
2022-07-15 13:53:33 - train: epoch 0034, iter [02600, 05004], lr: 0.081952, loss: 2.2685
2022-07-15 13:54:06 - train: epoch 0034, iter [02700, 05004], lr: 0.081932, loss: 2.1638
2022-07-15 13:54:40 - train: epoch 0034, iter [02800, 05004], lr: 0.081912, loss: 2.0972
2022-07-15 13:55:14 - train: epoch 0034, iter [02900, 05004], lr: 0.081892, loss: 1.9160
2022-07-15 13:55:46 - train: epoch 0034, iter [03000, 05004], lr: 0.081872, loss: 2.0438
2022-07-15 13:56:20 - train: epoch 0034, iter [03100, 05004], lr: 0.081852, loss: 2.0548
2022-07-15 13:56:54 - train: epoch 0034, iter [03200, 05004], lr: 0.081831, loss: 2.2325
2022-07-15 13:57:28 - train: epoch 0034, iter [03300, 05004], lr: 0.081811, loss: 2.1568
2022-07-15 13:58:01 - train: epoch 0034, iter [03400, 05004], lr: 0.081791, loss: 2.2706
2022-07-15 13:58:35 - train: epoch 0034, iter [03500, 05004], lr: 0.081771, loss: 2.1161
2022-07-15 13:59:08 - train: epoch 0034, iter [03600, 05004], lr: 0.081751, loss: 1.9709
2022-07-15 13:59:42 - train: epoch 0034, iter [03700, 05004], lr: 0.081730, loss: 2.0711
2022-07-15 14:00:15 - train: epoch 0034, iter [03800, 05004], lr: 0.081710, loss: 2.0882
2022-07-15 14:00:48 - train: epoch 0034, iter [03900, 05004], lr: 0.081690, loss: 2.2832
2022-07-15 14:01:21 - train: epoch 0034, iter [04000, 05004], lr: 0.081670, loss: 2.0967
2022-07-15 14:01:55 - train: epoch 0034, iter [04100, 05004], lr: 0.081649, loss: 2.2939
2022-07-15 14:02:29 - train: epoch 0034, iter [04200, 05004], lr: 0.081629, loss: 2.0894
2022-07-15 14:03:02 - train: epoch 0034, iter [04300, 05004], lr: 0.081609, loss: 2.1882
2022-07-15 14:03:35 - train: epoch 0034, iter [04400, 05004], lr: 0.081589, loss: 2.3012
2022-07-15 14:04:09 - train: epoch 0034, iter [04500, 05004], lr: 0.081568, loss: 2.3874
2022-07-15 14:04:43 - train: epoch 0034, iter [04600, 05004], lr: 0.081548, loss: 2.3585
2022-07-15 14:05:15 - train: epoch 0034, iter [04700, 05004], lr: 0.081528, loss: 2.2835
2022-07-15 14:05:49 - train: epoch 0034, iter [04800, 05004], lr: 0.081507, loss: 2.3217
2022-07-15 14:06:22 - train: epoch 0034, iter [04900, 05004], lr: 0.081487, loss: 2.3011
2022-07-15 14:06:54 - train: epoch 0034, iter [05000, 05004], lr: 0.081467, loss: 2.0987
2022-07-15 14:06:55 - train: epoch 034, train_loss: 2.2324
2022-07-15 14:08:09 - eval: epoch: 034, acc1: 53.474%, acc5: 78.360%, test_loss: 2.0155, per_image_load_time: 2.061ms, per_image_inference_time: 0.215ms
2022-07-15 14:08:09 - until epoch: 034, best_acc1: 53.708%
2022-07-15 14:08:09 - epoch 035 lr: 0.081466
2022-07-15 14:08:48 - train: epoch 0035, iter [00100, 05004], lr: 0.081446, loss: 2.0103
2022-07-15 14:09:21 - train: epoch 0035, iter [00200, 05004], lr: 0.081425, loss: 2.0208
2022-07-15 14:09:56 - train: epoch 0035, iter [00300, 05004], lr: 0.081405, loss: 2.3370
2022-07-15 14:10:29 - train: epoch 0035, iter [00400, 05004], lr: 0.081385, loss: 2.1475
2022-07-15 14:11:03 - train: epoch 0035, iter [00500, 05004], lr: 0.081364, loss: 2.2175
2022-07-15 14:11:35 - train: epoch 0035, iter [00600, 05004], lr: 0.081344, loss: 2.2113
2022-07-15 14:12:09 - train: epoch 0035, iter [00700, 05004], lr: 0.081324, loss: 2.1975
2022-07-15 14:12:42 - train: epoch 0035, iter [00800, 05004], lr: 0.081303, loss: 2.2464
2022-07-15 14:13:16 - train: epoch 0035, iter [00900, 05004], lr: 0.081283, loss: 2.4093
2022-07-15 14:13:49 - train: epoch 0035, iter [01000, 05004], lr: 0.081262, loss: 2.3099
2022-07-15 14:14:22 - train: epoch 0035, iter [01100, 05004], lr: 0.081242, loss: 2.3881
2022-07-15 14:14:55 - train: epoch 0035, iter [01200, 05004], lr: 0.081221, loss: 2.1266
2022-07-15 14:15:28 - train: epoch 0035, iter [01300, 05004], lr: 0.081201, loss: 2.3154
2022-07-15 14:16:01 - train: epoch 0035, iter [01400, 05004], lr: 0.081181, loss: 2.2421
2022-07-15 14:16:34 - train: epoch 0035, iter [01500, 05004], lr: 0.081160, loss: 2.2444
2022-07-15 14:17:09 - train: epoch 0035, iter [01600, 05004], lr: 0.081140, loss: 2.2484
2022-07-15 14:17:41 - train: epoch 0035, iter [01700, 05004], lr: 0.081119, loss: 1.9587
2022-07-15 14:18:14 - train: epoch 0035, iter [01800, 05004], lr: 0.081099, loss: 2.2163
2022-07-15 14:18:48 - train: epoch 0035, iter [01900, 05004], lr: 0.081078, loss: 2.2383
2022-07-15 14:19:22 - train: epoch 0035, iter [02000, 05004], lr: 0.081058, loss: 2.2825
2022-07-15 14:19:55 - train: epoch 0035, iter [02100, 05004], lr: 0.081037, loss: 2.1798
2022-07-15 14:20:29 - train: epoch 0035, iter [02200, 05004], lr: 0.081017, loss: 2.3453
2022-07-15 14:21:02 - train: epoch 0035, iter [02300, 05004], lr: 0.080996, loss: 2.2029
2022-07-15 14:21:34 - train: epoch 0035, iter [02400, 05004], lr: 0.080976, loss: 2.3801
2022-07-15 14:22:08 - train: epoch 0035, iter [02500, 05004], lr: 0.080955, loss: 2.1406
2022-07-15 14:22:42 - train: epoch 0035, iter [02600, 05004], lr: 0.080935, loss: 2.4146
2022-07-15 14:23:14 - train: epoch 0035, iter [02700, 05004], lr: 0.080914, loss: 2.3955
2022-07-15 14:23:48 - train: epoch 0035, iter [02800, 05004], lr: 0.080893, loss: 2.1077
2022-07-15 14:24:20 - train: epoch 0035, iter [02900, 05004], lr: 0.080873, loss: 2.2085
2022-07-15 14:24:55 - train: epoch 0035, iter [03000, 05004], lr: 0.080852, loss: 2.2874
2022-07-15 14:25:27 - train: epoch 0035, iter [03100, 05004], lr: 0.080832, loss: 2.3623
2022-07-15 14:26:01 - train: epoch 0035, iter [03200, 05004], lr: 0.080811, loss: 2.0476
2022-07-15 14:26:33 - train: epoch 0035, iter [03300, 05004], lr: 0.080790, loss: 2.1177
2022-07-15 14:27:08 - train: epoch 0035, iter [03400, 05004], lr: 0.080770, loss: 2.2412
2022-07-15 14:27:41 - train: epoch 0035, iter [03500, 05004], lr: 0.080749, loss: 1.9220
2022-07-15 14:28:16 - train: epoch 0035, iter [03600, 05004], lr: 0.080729, loss: 2.2221
2022-07-15 14:28:49 - train: epoch 0035, iter [03700, 05004], lr: 0.080708, loss: 2.1277
2022-07-15 14:29:23 - train: epoch 0035, iter [03800, 05004], lr: 0.080687, loss: 2.1660
2022-07-15 14:29:56 - train: epoch 0035, iter [03900, 05004], lr: 0.080667, loss: 2.4561
2022-07-15 14:30:31 - train: epoch 0035, iter [04000, 05004], lr: 0.080646, loss: 1.8784
2022-07-15 14:31:04 - train: epoch 0035, iter [04100, 05004], lr: 0.080625, loss: 2.4113
2022-07-15 14:31:38 - train: epoch 0035, iter [04200, 05004], lr: 0.080605, loss: 2.2000
2022-07-15 14:32:12 - train: epoch 0035, iter [04300, 05004], lr: 0.080584, loss: 2.2073
2022-07-15 14:32:46 - train: epoch 0035, iter [04400, 05004], lr: 0.080563, loss: 2.2025
2022-07-15 14:33:20 - train: epoch 0035, iter [04500, 05004], lr: 0.080543, loss: 2.2691
2022-07-15 14:33:54 - train: epoch 0035, iter [04600, 05004], lr: 0.080522, loss: 2.0804
2022-07-15 14:34:28 - train: epoch 0035, iter [04700, 05004], lr: 0.080501, loss: 2.4573
2022-07-15 14:35:02 - train: epoch 0035, iter [04800, 05004], lr: 0.080480, loss: 2.5360
2022-07-15 14:35:35 - train: epoch 0035, iter [04900, 05004], lr: 0.080460, loss: 2.2785
2022-07-15 14:36:07 - train: epoch 0035, iter [05000, 05004], lr: 0.080439, loss: 2.0642
2022-07-15 14:36:08 - train: epoch 035, train_loss: 2.2237
2022-07-15 14:37:25 - eval: epoch: 035, acc1: 53.834%, acc5: 78.698%, test_loss: 1.9914, per_image_load_time: 1.148ms, per_image_inference_time: 0.225ms
2022-07-15 14:37:25 - until epoch: 035, best_acc1: 53.834%
2022-07-15 14:37:25 - epoch 036 lr: 0.080438
2022-07-15 14:38:05 - train: epoch 0036, iter [00100, 05004], lr: 0.080417, loss: 2.2837
2022-07-15 14:38:39 - train: epoch 0036, iter [00200, 05004], lr: 0.080397, loss: 1.8664
2022-07-15 14:39:12 - train: epoch 0036, iter [00300, 05004], lr: 0.080376, loss: 2.0576
2022-07-15 14:39:46 - train: epoch 0036, iter [00400, 05004], lr: 0.080355, loss: 2.1910
2022-07-15 14:40:20 - train: epoch 0036, iter [00500, 05004], lr: 0.080334, loss: 2.2507
2022-07-15 14:40:55 - train: epoch 0036, iter [00600, 05004], lr: 0.080313, loss: 2.1072
2022-07-15 14:41:28 - train: epoch 0036, iter [00700, 05004], lr: 0.080293, loss: 2.0742
2022-07-15 14:42:03 - train: epoch 0036, iter [00800, 05004], lr: 0.080272, loss: 2.0060
2022-07-15 14:42:37 - train: epoch 0036, iter [00900, 05004], lr: 0.080251, loss: 2.2180
2022-07-15 14:43:11 - train: epoch 0036, iter [01000, 05004], lr: 0.080230, loss: 2.1105
2022-07-15 14:43:44 - train: epoch 0036, iter [01100, 05004], lr: 0.080209, loss: 2.2101
2022-07-15 14:44:19 - train: epoch 0036, iter [01200, 05004], lr: 0.080188, loss: 2.1823
2022-07-15 14:44:53 - train: epoch 0036, iter [01300, 05004], lr: 0.080168, loss: 2.1915
2022-07-15 14:45:28 - train: epoch 0036, iter [01400, 05004], lr: 0.080147, loss: 2.3037
2022-07-15 14:46:02 - train: epoch 0036, iter [01500, 05004], lr: 0.080126, loss: 2.3402
2022-07-15 14:46:36 - train: epoch 0036, iter [01600, 05004], lr: 0.080105, loss: 2.3561
2022-07-15 14:47:10 - train: epoch 0036, iter [01700, 05004], lr: 0.080084, loss: 2.1712
2022-07-15 14:47:45 - train: epoch 0036, iter [01800, 05004], lr: 0.080063, loss: 2.0286
2022-07-15 14:48:18 - train: epoch 0036, iter [01900, 05004], lr: 0.080042, loss: 2.2524
2022-07-15 14:48:52 - train: epoch 0036, iter [02000, 05004], lr: 0.080021, loss: 2.1085
2022-07-15 14:49:26 - train: epoch 0036, iter [02100, 05004], lr: 0.080000, loss: 2.1375
2022-07-15 14:50:00 - train: epoch 0036, iter [02200, 05004], lr: 0.079979, loss: 2.1265
2022-07-15 14:50:34 - train: epoch 0036, iter [02300, 05004], lr: 0.079959, loss: 2.3209
2022-07-15 14:51:09 - train: epoch 0036, iter [02400, 05004], lr: 0.079938, loss: 2.3286
2022-07-15 14:51:43 - train: epoch 0036, iter [02500, 05004], lr: 0.079917, loss: 1.9623
2022-07-15 14:52:17 - train: epoch 0036, iter [02600, 05004], lr: 0.079896, loss: 2.2426
2022-07-15 14:52:50 - train: epoch 0036, iter [02700, 05004], lr: 0.079875, loss: 2.1135
2022-07-15 14:53:25 - train: epoch 0036, iter [02800, 05004], lr: 0.079854, loss: 2.0334
2022-07-15 14:53:59 - train: epoch 0036, iter [02900, 05004], lr: 0.079833, loss: 1.9841
2022-07-15 14:54:34 - train: epoch 0036, iter [03000, 05004], lr: 0.079812, loss: 2.3848
2022-07-15 14:55:07 - train: epoch 0036, iter [03100, 05004], lr: 0.079791, loss: 2.4303
2022-07-15 14:55:42 - train: epoch 0036, iter [03200, 05004], lr: 0.079770, loss: 2.1068
2022-07-15 14:56:16 - train: epoch 0036, iter [03300, 05004], lr: 0.079749, loss: 2.1311
2022-07-15 14:56:50 - train: epoch 0036, iter [03400, 05004], lr: 0.079728, loss: 2.0293
2022-07-15 14:57:24 - train: epoch 0036, iter [03500, 05004], lr: 0.079707, loss: 2.3929
2022-07-15 14:57:58 - train: epoch 0036, iter [03600, 05004], lr: 0.079686, loss: 2.3307
2022-07-15 14:58:33 - train: epoch 0036, iter [03700, 05004], lr: 0.079665, loss: 2.1287
2022-07-15 14:59:07 - train: epoch 0036, iter [03800, 05004], lr: 0.079643, loss: 2.1170
2022-07-15 14:59:41 - train: epoch 0036, iter [03900, 05004], lr: 0.079622, loss: 2.1264
2022-07-15 15:00:16 - train: epoch 0036, iter [04000, 05004], lr: 0.079601, loss: 2.2257
2022-07-15 15:00:49 - train: epoch 0036, iter [04100, 05004], lr: 0.079580, loss: 2.2010
2022-07-15 15:01:23 - train: epoch 0036, iter [04200, 05004], lr: 0.079559, loss: 2.2158
2022-07-15 15:01:57 - train: epoch 0036, iter [04300, 05004], lr: 0.079538, loss: 2.2287
2022-07-15 15:02:31 - train: epoch 0036, iter [04400, 05004], lr: 0.079517, loss: 2.1650
2022-07-15 15:03:06 - train: epoch 0036, iter [04500, 05004], lr: 0.079496, loss: 1.8503
2022-07-15 15:03:40 - train: epoch 0036, iter [04600, 05004], lr: 0.079475, loss: 2.1175
2022-07-15 15:04:15 - train: epoch 0036, iter [04700, 05004], lr: 0.079454, loss: 2.1033
2022-07-15 15:04:49 - train: epoch 0036, iter [04800, 05004], lr: 0.079432, loss: 2.1624
2022-07-15 15:05:23 - train: epoch 0036, iter [04900, 05004], lr: 0.079411, loss: 2.1661
2022-07-15 15:05:55 - train: epoch 0036, iter [05000, 05004], lr: 0.079390, loss: 2.2070
2022-07-15 15:05:56 - train: epoch 036, train_loss: 2.2123
2022-07-15 15:07:11 - eval: epoch: 036, acc1: 53.704%, acc5: 78.478%, test_loss: 2.0020, per_image_load_time: 1.959ms, per_image_inference_time: 0.220ms
2022-07-15 15:07:11 - until epoch: 036, best_acc1: 53.834%
2022-07-15 15:07:11 - epoch 037 lr: 0.079389
2022-07-15 15:07:51 - train: epoch 0037, iter [00100, 05004], lr: 0.079368, loss: 1.9170
2022-07-15 15:08:25 - train: epoch 0037, iter [00200, 05004], lr: 0.079347, loss: 1.7888
2022-07-15 15:08:58 - train: epoch 0037, iter [00300, 05004], lr: 0.079326, loss: 2.1246
2022-07-15 15:09:33 - train: epoch 0037, iter [00400, 05004], lr: 0.079305, loss: 2.1069
2022-07-15 15:10:06 - train: epoch 0037, iter [00500, 05004], lr: 0.079283, loss: 2.2605
2022-07-15 15:10:40 - train: epoch 0037, iter [00600, 05004], lr: 0.079262, loss: 2.3198
2022-07-15 15:11:14 - train: epoch 0037, iter [00700, 05004], lr: 0.079241, loss: 2.2360
2022-07-15 15:11:48 - train: epoch 0037, iter [00800, 05004], lr: 0.079220, loss: 2.2373
2022-07-15 15:12:22 - train: epoch 0037, iter [00900, 05004], lr: 0.079198, loss: 2.5850
2022-07-15 15:12:56 - train: epoch 0037, iter [01000, 05004], lr: 0.079177, loss: 2.1634
2022-07-15 15:13:32 - train: epoch 0037, iter [01100, 05004], lr: 0.079156, loss: 2.2449
2022-07-15 15:14:05 - train: epoch 0037, iter [01200, 05004], lr: 0.079135, loss: 2.1427
2022-07-15 15:14:40 - train: epoch 0037, iter [01300, 05004], lr: 0.079113, loss: 2.2919
2022-07-15 15:15:14 - train: epoch 0037, iter [01400, 05004], lr: 0.079092, loss: 2.2337
2022-07-15 15:15:49 - train: epoch 0037, iter [01500, 05004], lr: 0.079071, loss: 2.0580
2022-07-15 15:16:23 - train: epoch 0037, iter [01600, 05004], lr: 0.079050, loss: 2.0556
2022-07-15 15:16:58 - train: epoch 0037, iter [01700, 05004], lr: 0.079028, loss: 2.2622
2022-07-15 15:17:31 - train: epoch 0037, iter [01800, 05004], lr: 0.079007, loss: 2.2236
2022-07-15 15:18:06 - train: epoch 0037, iter [01900, 05004], lr: 0.078986, loss: 2.3163
2022-07-15 15:18:40 - train: epoch 0037, iter [02000, 05004], lr: 0.078964, loss: 2.3624
2022-07-15 15:19:14 - train: epoch 0037, iter [02100, 05004], lr: 0.078943, loss: 2.2906
2022-07-15 15:19:49 - train: epoch 0037, iter [02200, 05004], lr: 0.078922, loss: 2.1725
2022-07-15 15:20:22 - train: epoch 0037, iter [02300, 05004], lr: 0.078900, loss: 1.9622
2022-07-15 15:20:57 - train: epoch 0037, iter [02400, 05004], lr: 0.078879, loss: 2.3654
2022-07-15 15:21:31 - train: epoch 0037, iter [02500, 05004], lr: 0.078858, loss: 2.0667
2022-07-15 15:22:05 - train: epoch 0037, iter [02600, 05004], lr: 0.078836, loss: 2.4948
2022-07-15 15:22:40 - train: epoch 0037, iter [02700, 05004], lr: 0.078815, loss: 2.4000
2022-07-15 15:23:14 - train: epoch 0037, iter [02800, 05004], lr: 0.078794, loss: 2.2320
2022-07-15 15:23:48 - train: epoch 0037, iter [02900, 05004], lr: 0.078772, loss: 2.2928
2022-07-15 15:24:23 - train: epoch 0037, iter [03000, 05004], lr: 0.078751, loss: 2.4012
2022-07-15 15:24:57 - train: epoch 0037, iter [03100, 05004], lr: 0.078729, loss: 2.1237
2022-07-15 15:25:32 - train: epoch 0037, iter [03200, 05004], lr: 0.078708, loss: 2.2891
2022-07-15 15:26:07 - train: epoch 0037, iter [03300, 05004], lr: 0.078687, loss: 2.0463
2022-07-15 15:26:41 - train: epoch 0037, iter [03400, 05004], lr: 0.078665, loss: 2.0524
2022-07-15 15:27:17 - train: epoch 0037, iter [03500, 05004], lr: 0.078644, loss: 2.2011
2022-07-15 15:27:50 - train: epoch 0037, iter [03600, 05004], lr: 0.078622, loss: 2.2715
2022-07-15 15:28:25 - train: epoch 0037, iter [03700, 05004], lr: 0.078601, loss: 2.3022
2022-07-15 15:28:59 - train: epoch 0037, iter [03800, 05004], lr: 0.078579, loss: 2.0275
2022-07-15 15:29:34 - train: epoch 0037, iter [03900, 05004], lr: 0.078558, loss: 2.3622
2022-07-15 15:30:08 - train: epoch 0037, iter [04000, 05004], lr: 0.078536, loss: 2.0944
2022-07-15 15:30:43 - train: epoch 0037, iter [04100, 05004], lr: 0.078515, loss: 2.0549
2022-07-15 15:31:17 - train: epoch 0037, iter [04200, 05004], lr: 0.078493, loss: 2.4302
2022-07-15 15:31:52 - train: epoch 0037, iter [04300, 05004], lr: 0.078472, loss: 2.2063
2022-07-15 15:32:26 - train: epoch 0037, iter [04400, 05004], lr: 0.078450, loss: 2.1866
2022-07-15 15:33:01 - train: epoch 0037, iter [04500, 05004], lr: 0.078429, loss: 2.0826
2022-07-15 15:33:36 - train: epoch 0037, iter [04600, 05004], lr: 0.078407, loss: 2.1164
2022-07-15 15:34:09 - train: epoch 0037, iter [04700, 05004], lr: 0.078386, loss: 2.2097
2022-07-15 15:34:44 - train: epoch 0037, iter [04800, 05004], lr: 0.078364, loss: 2.2213
2022-07-15 15:35:18 - train: epoch 0037, iter [04900, 05004], lr: 0.078343, loss: 2.2277
2022-07-15 15:35:51 - train: epoch 0037, iter [05000, 05004], lr: 0.078321, loss: 2.1610
2022-07-15 15:35:52 - train: epoch 037, train_loss: 2.2071
2022-07-15 15:37:08 - eval: epoch: 037, acc1: 54.908%, acc5: 79.376%, test_loss: 1.9325, per_image_load_time: 2.512ms, per_image_inference_time: 0.227ms
2022-07-15 15:37:08 - until epoch: 037, best_acc1: 54.908%
2022-07-15 15:37:08 - epoch 038 lr: 0.078320
2022-07-15 15:37:47 - train: epoch 0038, iter [00100, 05004], lr: 0.078299, loss: 2.1463
2022-07-15 15:38:21 - train: epoch 0038, iter [00200, 05004], lr: 0.078277, loss: 2.0026
2022-07-15 15:38:54 - train: epoch 0038, iter [00300, 05004], lr: 0.078256, loss: 1.9755
2022-07-15 15:39:28 - train: epoch 0038, iter [00400, 05004], lr: 0.078234, loss: 2.1102
2022-07-15 15:40:02 - train: epoch 0038, iter [00500, 05004], lr: 0.078212, loss: 2.0231
2022-07-15 15:40:36 - train: epoch 0038, iter [00600, 05004], lr: 0.078191, loss: 2.3259
2022-07-15 15:41:09 - train: epoch 0038, iter [00700, 05004], lr: 0.078169, loss: 1.9381
2022-07-15 15:41:43 - train: epoch 0038, iter [00800, 05004], lr: 0.078148, loss: 2.0712
2022-07-15 15:42:17 - train: epoch 0038, iter [00900, 05004], lr: 0.078126, loss: 2.0928
2022-07-15 15:42:51 - train: epoch 0038, iter [01000, 05004], lr: 0.078104, loss: 2.3997
2022-07-15 15:43:25 - train: epoch 0038, iter [01100, 05004], lr: 0.078083, loss: 2.1953
2022-07-15 15:43:59 - train: epoch 0038, iter [01200, 05004], lr: 0.078061, loss: 2.2855
2022-07-15 15:44:33 - train: epoch 0038, iter [01300, 05004], lr: 0.078039, loss: 2.4460
2022-07-15 15:45:07 - train: epoch 0038, iter [01400, 05004], lr: 0.078018, loss: 2.2738
2022-07-15 15:45:41 - train: epoch 0038, iter [01500, 05004], lr: 0.077996, loss: 2.2862
2022-07-15 15:46:16 - train: epoch 0038, iter [01600, 05004], lr: 0.077974, loss: 2.4040
2022-07-15 15:46:50 - train: epoch 0038, iter [01700, 05004], lr: 0.077953, loss: 2.4382
2022-07-15 15:47:25 - train: epoch 0038, iter [01800, 05004], lr: 0.077931, loss: 2.2829
2022-07-15 15:47:58 - train: epoch 0038, iter [01900, 05004], lr: 0.077909, loss: 2.1653
2022-07-15 15:48:33 - train: epoch 0038, iter [02000, 05004], lr: 0.077888, loss: 2.0716
2022-07-15 15:49:07 - train: epoch 0038, iter [02100, 05004], lr: 0.077866, loss: 2.2080
2022-07-15 15:49:41 - train: epoch 0038, iter [02200, 05004], lr: 0.077844, loss: 2.1194
2022-07-15 15:50:16 - train: epoch 0038, iter [02300, 05004], lr: 0.077822, loss: 2.3276
2022-07-15 15:50:50 - train: epoch 0038, iter [02400, 05004], lr: 0.077801, loss: 2.4102
2022-07-15 15:51:25 - train: epoch 0038, iter [02500, 05004], lr: 0.077779, loss: 2.1089
2022-07-15 15:51:59 - train: epoch 0038, iter [02600, 05004], lr: 0.077757, loss: 2.2007
2022-07-15 15:52:33 - train: epoch 0038, iter [02700, 05004], lr: 0.077735, loss: 2.3293
2022-07-15 15:53:08 - train: epoch 0038, iter [02800, 05004], lr: 0.077714, loss: 2.4733
2022-07-15 15:53:43 - train: epoch 0038, iter [02900, 05004], lr: 0.077692, loss: 2.3500
2022-07-15 15:54:15 - train: epoch 0038, iter [03000, 05004], lr: 0.077670, loss: 2.0847
2022-07-15 15:54:49 - train: epoch 0038, iter [03100, 05004], lr: 0.077648, loss: 2.1718
2022-07-15 15:55:22 - train: epoch 0038, iter [03200, 05004], lr: 0.077627, loss: 1.9495
2022-07-15 15:55:55 - train: epoch 0038, iter [03300, 05004], lr: 0.077605, loss: 1.9439
2022-07-15 15:56:28 - train: epoch 0038, iter [03400, 05004], lr: 0.077583, loss: 2.0371
2022-07-15 15:57:02 - train: epoch 0038, iter [03500, 05004], lr: 0.077561, loss: 2.2226
2022-07-15 15:57:34 - train: epoch 0038, iter [03600, 05004], lr: 0.077539, loss: 2.2924
2022-07-15 15:58:08 - train: epoch 0038, iter [03700, 05004], lr: 0.077517, loss: 2.0094
2022-07-15 15:58:42 - train: epoch 0038, iter [03800, 05004], lr: 0.077496, loss: 2.4018
2022-07-15 15:59:15 - train: epoch 0038, iter [03900, 05004], lr: 0.077474, loss: 2.1756
2022-07-15 15:59:49 - train: epoch 0038, iter [04000, 05004], lr: 0.077452, loss: 2.3038
2022-07-15 16:00:21 - train: epoch 0038, iter [04100, 05004], lr: 0.077430, loss: 2.0522
2022-07-15 16:00:55 - train: epoch 0038, iter [04200, 05004], lr: 0.077408, loss: 1.9048
2022-07-15 16:01:29 - train: epoch 0038, iter [04300, 05004], lr: 0.077386, loss: 2.4192
2022-07-15 16:02:01 - train: epoch 0038, iter [04400, 05004], lr: 0.077364, loss: 2.2746
2022-07-15 16:02:35 - train: epoch 0038, iter [04500, 05004], lr: 0.077342, loss: 2.3390
2022-07-15 16:03:09 - train: epoch 0038, iter [04600, 05004], lr: 0.077321, loss: 2.3212
2022-07-15 16:03:42 - train: epoch 0038, iter [04700, 05004], lr: 0.077299, loss: 2.0153
2022-07-15 16:04:16 - train: epoch 0038, iter [04800, 05004], lr: 0.077277, loss: 2.2267
2022-07-15 16:04:49 - train: epoch 0038, iter [04900, 05004], lr: 0.077255, loss: 2.1744
2022-07-15 16:05:20 - train: epoch 0038, iter [05000, 05004], lr: 0.077233, loss: 2.0083
2022-07-15 16:05:21 - train: epoch 038, train_loss: 2.1972
2022-07-15 16:06:34 - eval: epoch: 038, acc1: 54.324%, acc5: 79.098%, test_loss: 1.9698, per_image_load_time: 1.904ms, per_image_inference_time: 0.208ms
2022-07-15 16:06:34 - until epoch: 038, best_acc1: 54.908%
2022-07-15 16:06:34 - epoch 039 lr: 0.077232
2022-07-15 16:07:12 - train: epoch 0039, iter [00100, 05004], lr: 0.077210, loss: 2.2076
2022-07-15 16:07:45 - train: epoch 0039, iter [00200, 05004], lr: 0.077188, loss: 2.3043
2022-07-15 16:08:19 - train: epoch 0039, iter [00300, 05004], lr: 0.077166, loss: 2.0291
2022-07-15 16:08:52 - train: epoch 0039, iter [00400, 05004], lr: 0.077144, loss: 2.1592
2022-07-15 16:09:25 - train: epoch 0039, iter [00500, 05004], lr: 0.077122, loss: 2.1745
2022-07-15 16:09:58 - train: epoch 0039, iter [00600, 05004], lr: 0.077100, loss: 2.0829
2022-07-15 16:10:32 - train: epoch 0039, iter [00700, 05004], lr: 0.077078, loss: 2.4211
2022-07-15 16:11:05 - train: epoch 0039, iter [00800, 05004], lr: 0.077056, loss: 2.1256
2022-07-15 16:11:39 - train: epoch 0039, iter [00900, 05004], lr: 0.077034, loss: 2.2901
2022-07-15 16:12:12 - train: epoch 0039, iter [01000, 05004], lr: 0.077012, loss: 1.9915
2022-07-15 16:12:46 - train: epoch 0039, iter [01100, 05004], lr: 0.076990, loss: 2.3236
2022-07-15 16:13:19 - train: epoch 0039, iter [01200, 05004], lr: 0.076968, loss: 2.3898
2022-07-15 16:13:53 - train: epoch 0039, iter [01300, 05004], lr: 0.076946, loss: 2.4294
2022-07-15 16:14:26 - train: epoch 0039, iter [01400, 05004], lr: 0.076924, loss: 2.2965
2022-07-15 16:14:59 - train: epoch 0039, iter [01500, 05004], lr: 0.076902, loss: 2.2105
2022-07-15 16:15:32 - train: epoch 0039, iter [01600, 05004], lr: 0.076880, loss: 2.3666
2022-07-15 16:16:06 - train: epoch 0039, iter [01700, 05004], lr: 0.076858, loss: 1.9260
2022-07-15 16:16:39 - train: epoch 0039, iter [01800, 05004], lr: 0.076836, loss: 2.0373
2022-07-15 16:17:13 - train: epoch 0039, iter [01900, 05004], lr: 0.076814, loss: 2.0254
2022-07-15 16:17:46 - train: epoch 0039, iter [02000, 05004], lr: 0.076792, loss: 2.2435
2022-07-15 16:18:20 - train: epoch 0039, iter [02100, 05004], lr: 0.076770, loss: 2.3010
2022-07-15 16:18:52 - train: epoch 0039, iter [02200, 05004], lr: 0.076748, loss: 2.1520
2022-07-15 16:19:26 - train: epoch 0039, iter [02300, 05004], lr: 0.076725, loss: 2.3688
2022-07-15 16:19:59 - train: epoch 0039, iter [02400, 05004], lr: 0.076703, loss: 2.2142
2022-07-15 16:20:32 - train: epoch 0039, iter [02500, 05004], lr: 0.076681, loss: 2.1787
2022-07-15 16:21:06 - train: epoch 0039, iter [02600, 05004], lr: 0.076659, loss: 2.2363
2022-07-15 16:21:40 - train: epoch 0039, iter [02700, 05004], lr: 0.076637, loss: 2.3718
2022-07-15 16:22:13 - train: epoch 0039, iter [02800, 05004], lr: 0.076615, loss: 2.0774
2022-07-15 16:22:48 - train: epoch 0039, iter [02900, 05004], lr: 0.076593, loss: 1.9509
2022-07-15 16:23:22 - train: epoch 0039, iter [03000, 05004], lr: 0.076570, loss: 2.1608
2022-07-15 16:23:55 - train: epoch 0039, iter [03100, 05004], lr: 0.076548, loss: 2.1360
2022-07-15 16:24:29 - train: epoch 0039, iter [03200, 05004], lr: 0.076526, loss: 2.2652
2022-07-15 16:25:04 - train: epoch 0039, iter [03300, 05004], lr: 0.076504, loss: 2.2372
2022-07-15 16:25:39 - train: epoch 0039, iter [03400, 05004], lr: 0.076482, loss: 2.2503
2022-07-15 16:26:12 - train: epoch 0039, iter [03500, 05004], lr: 0.076460, loss: 2.3947
2022-07-15 16:26:47 - train: epoch 0039, iter [03600, 05004], lr: 0.076437, loss: 2.3355
2022-07-15 16:27:22 - train: epoch 0039, iter [03700, 05004], lr: 0.076415, loss: 2.0692
2022-07-15 16:27:56 - train: epoch 0039, iter [03800, 05004], lr: 0.076393, loss: 2.1311
2022-07-15 16:28:30 - train: epoch 0039, iter [03900, 05004], lr: 0.076371, loss: 2.2960
2022-07-15 16:29:05 - train: epoch 0039, iter [04000, 05004], lr: 0.076349, loss: 2.0812
2022-07-15 16:29:39 - train: epoch 0039, iter [04100, 05004], lr: 0.076326, loss: 2.2308
2022-07-15 16:30:13 - train: epoch 0039, iter [04200, 05004], lr: 0.076304, loss: 2.2119
2022-07-15 16:30:47 - train: epoch 0039, iter [04300, 05004], lr: 0.076282, loss: 2.2152
2022-07-15 16:31:21 - train: epoch 0039, iter [04400, 05004], lr: 0.076260, loss: 1.8760
2022-07-15 16:31:57 - train: epoch 0039, iter [04500, 05004], lr: 0.076237, loss: 2.0901
2022-07-15 16:32:31 - train: epoch 0039, iter [04600, 05004], lr: 0.076215, loss: 2.3984
2022-07-15 16:33:06 - train: epoch 0039, iter [04700, 05004], lr: 0.076193, loss: 2.1788
2022-07-15 16:33:40 - train: epoch 0039, iter [04800, 05004], lr: 0.076170, loss: 2.2151
2022-07-15 16:34:15 - train: epoch 0039, iter [04900, 05004], lr: 0.076148, loss: 2.0107
2022-07-15 16:34:48 - train: epoch 0039, iter [05000, 05004], lr: 0.076126, loss: 1.9542
2022-07-15 16:34:49 - train: epoch 039, train_loss: 2.1897
2022-07-15 16:36:05 - eval: epoch: 039, acc1: 53.718%, acc5: 78.560%, test_loss: 1.9905, per_image_load_time: 1.731ms, per_image_inference_time: 0.216ms
2022-07-15 16:36:05 - until epoch: 039, best_acc1: 54.908%
2022-07-15 16:36:05 - epoch 040 lr: 0.076125
2022-07-15 16:36:45 - train: epoch 0040, iter [00100, 05004], lr: 0.076103, loss: 2.4625
2022-07-15 16:37:18 - train: epoch 0040, iter [00200, 05004], lr: 0.076080, loss: 2.4468
2022-07-15 16:37:54 - train: epoch 0040, iter [00300, 05004], lr: 0.076058, loss: 2.2801
2022-07-15 16:38:27 - train: epoch 0040, iter [00400, 05004], lr: 0.076036, loss: 2.1706
2022-07-15 16:39:01 - train: epoch 0040, iter [00500, 05004], lr: 0.076013, loss: 2.0420
2022-07-15 16:39:36 - train: epoch 0040, iter [00600, 05004], lr: 0.075991, loss: 2.3599
2022-07-15 16:40:10 - train: epoch 0040, iter [00700, 05004], lr: 0.075969, loss: 2.2554
2022-07-15 16:40:44 - train: epoch 0040, iter [00800, 05004], lr: 0.075946, loss: 2.3224
2022-07-15 16:41:18 - train: epoch 0040, iter [00900, 05004], lr: 0.075924, loss: 2.0656
2022-07-15 16:41:52 - train: epoch 0040, iter [01000, 05004], lr: 0.075902, loss: 1.9422
2022-07-15 16:42:26 - train: epoch 0040, iter [01100, 05004], lr: 0.075879, loss: 2.1673
2022-07-15 16:43:00 - train: epoch 0040, iter [01200, 05004], lr: 0.075857, loss: 2.1893
2022-07-15 16:43:35 - train: epoch 0040, iter [01300, 05004], lr: 0.075834, loss: 1.9896
2022-07-15 16:44:10 - train: epoch 0040, iter [01400, 05004], lr: 0.075812, loss: 2.1524
2022-07-15 16:44:43 - train: epoch 0040, iter [01500, 05004], lr: 0.075790, loss: 2.2441
2022-07-15 16:45:18 - train: epoch 0040, iter [01600, 05004], lr: 0.075767, loss: 2.1593
2022-07-15 16:45:52 - train: epoch 0040, iter [01700, 05004], lr: 0.075745, loss: 2.2803
2022-07-15 16:46:27 - train: epoch 0040, iter [01800, 05004], lr: 0.075722, loss: 1.9694
2022-07-15 16:47:00 - train: epoch 0040, iter [01900, 05004], lr: 0.075700, loss: 2.1157
2022-07-15 16:47:35 - train: epoch 0040, iter [02000, 05004], lr: 0.075677, loss: 2.2052
2022-07-15 16:48:09 - train: epoch 0040, iter [02100, 05004], lr: 0.075655, loss: 2.0993
2022-07-15 16:48:44 - train: epoch 0040, iter [02200, 05004], lr: 0.075633, loss: 1.9408
2022-07-15 16:49:18 - train: epoch 0040, iter [02300, 05004], lr: 0.075610, loss: 2.1096
2022-07-15 16:49:53 - train: epoch 0040, iter [02400, 05004], lr: 0.075588, loss: 2.2344
2022-07-15 16:50:27 - train: epoch 0040, iter [02500, 05004], lr: 0.075565, loss: 2.2349
2022-07-15 16:51:02 - train: epoch 0040, iter [02600, 05004], lr: 0.075543, loss: 2.0110
2022-07-15 16:51:36 - train: epoch 0040, iter [02700, 05004], lr: 0.075520, loss: 2.3398
2022-07-15 16:52:10 - train: epoch 0040, iter [02800, 05004], lr: 0.075498, loss: 2.1676
2022-07-15 16:52:43 - train: epoch 0040, iter [02900, 05004], lr: 0.075475, loss: 2.4359
2022-07-15 16:53:19 - train: epoch 0040, iter [03000, 05004], lr: 0.075453, loss: 2.2611
2022-07-15 16:53:53 - train: epoch 0040, iter [03100, 05004], lr: 0.075430, loss: 2.1501
2022-07-15 16:54:27 - train: epoch 0040, iter [03200, 05004], lr: 0.075408, loss: 2.4695
2022-07-15 16:55:01 - train: epoch 0040, iter [03300, 05004], lr: 0.075385, loss: 2.3119
2022-07-15 16:55:36 - train: epoch 0040, iter [03400, 05004], lr: 0.075362, loss: 2.1885
2022-07-15 16:56:11 - train: epoch 0040, iter [03500, 05004], lr: 0.075340, loss: 2.2378
2022-07-15 16:56:44 - train: epoch 0040, iter [03600, 05004], lr: 0.075317, loss: 2.2053
2022-07-15 16:57:19 - train: epoch 0040, iter [03700, 05004], lr: 0.075295, loss: 2.2440
2022-07-15 16:57:53 - train: epoch 0040, iter [03800, 05004], lr: 0.075272, loss: 2.0563
2022-07-15 16:58:27 - train: epoch 0040, iter [03900, 05004], lr: 0.075250, loss: 2.2122
2022-07-15 16:59:03 - train: epoch 0040, iter [04000, 05004], lr: 0.075227, loss: 2.3450
2022-07-15 16:59:37 - train: epoch 0040, iter [04100, 05004], lr: 0.075205, loss: 2.2163
2022-07-15 17:00:11 - train: epoch 0040, iter [04200, 05004], lr: 0.075182, loss: 2.1853
2022-07-15 17:00:46 - train: epoch 0040, iter [04300, 05004], lr: 0.075159, loss: 2.2013
2022-07-15 17:01:21 - train: epoch 0040, iter [04400, 05004], lr: 0.075137, loss: 2.2765
2022-07-15 17:01:56 - train: epoch 0040, iter [04500, 05004], lr: 0.075114, loss: 1.9656
2022-07-15 17:02:31 - train: epoch 0040, iter [04600, 05004], lr: 0.075091, loss: 2.1822
2022-07-15 17:03:06 - train: epoch 0040, iter [04700, 05004], lr: 0.075069, loss: 2.1615
2022-07-15 17:03:40 - train: epoch 0040, iter [04800, 05004], lr: 0.075046, loss: 2.2098
2022-07-15 17:04:16 - train: epoch 0040, iter [04900, 05004], lr: 0.075024, loss: 2.2748
2022-07-15 17:04:48 - train: epoch 0040, iter [05000, 05004], lr: 0.075001, loss: 2.1177
2022-07-15 17:04:49 - train: epoch 040, train_loss: 2.1794
2022-07-15 17:06:06 - eval: epoch: 040, acc1: 54.874%, acc5: 79.538%, test_loss: 1.9422, per_image_load_time: 0.857ms, per_image_inference_time: 0.215ms
2022-07-15 17:06:06 - until epoch: 040, best_acc1: 54.908%
2022-07-15 17:06:06 - epoch 041 lr: 0.075000
2022-07-15 17:06:46 - train: epoch 0041, iter [00100, 05004], lr: 0.074977, loss: 2.3457
2022-07-15 17:07:19 - train: epoch 0041, iter [00200, 05004], lr: 0.074955, loss: 2.4066
2022-07-15 17:07:54 - train: epoch 0041, iter [00300, 05004], lr: 0.074932, loss: 2.1334
2022-07-15 17:08:28 - train: epoch 0041, iter [00400, 05004], lr: 0.074909, loss: 2.2177
2022-07-15 17:09:03 - train: epoch 0041, iter [00500, 05004], lr: 0.074887, loss: 1.9117
2022-07-15 17:09:37 - train: epoch 0041, iter [00600, 05004], lr: 0.074864, loss: 2.3005
2022-07-15 17:10:12 - train: epoch 0041, iter [00700, 05004], lr: 0.074841, loss: 1.9974
2022-07-15 17:10:45 - train: epoch 0041, iter [00800, 05004], lr: 0.074819, loss: 1.9650
2022-07-15 17:11:19 - train: epoch 0041, iter [00900, 05004], lr: 0.074796, loss: 1.8734
2022-07-15 17:11:52 - train: epoch 0041, iter [01000, 05004], lr: 0.074773, loss: 2.4189
2022-07-15 17:12:27 - train: epoch 0041, iter [01100, 05004], lr: 0.074750, loss: 2.0571
2022-07-15 17:13:00 - train: epoch 0041, iter [01200, 05004], lr: 0.074728, loss: 1.8689
2022-07-15 17:13:34 - train: epoch 0041, iter [01300, 05004], lr: 0.074705, loss: 2.1850
2022-07-15 17:14:09 - train: epoch 0041, iter [01400, 05004], lr: 0.074682, loss: 2.3220
2022-07-15 17:14:42 - train: epoch 0041, iter [01500, 05004], lr: 0.074659, loss: 2.5596
2022-07-15 17:15:16 - train: epoch 0041, iter [01600, 05004], lr: 0.074637, loss: 1.9127
2022-07-15 17:15:49 - train: epoch 0041, iter [01700, 05004], lr: 0.074614, loss: 2.1788
2022-07-15 17:16:24 - train: epoch 0041, iter [01800, 05004], lr: 0.074591, loss: 2.2196
2022-07-15 17:16:58 - train: epoch 0041, iter [01900, 05004], lr: 0.074568, loss: 2.3838
2022-07-15 17:17:32 - train: epoch 0041, iter [02000, 05004], lr: 0.074546, loss: 2.0617
2022-07-15 17:18:05 - train: epoch 0041, iter [02100, 05004], lr: 0.074523, loss: 1.9646
2022-07-15 17:18:40 - train: epoch 0041, iter [02200, 05004], lr: 0.074500, loss: 2.0003
2022-07-15 17:19:14 - train: epoch 0041, iter [02300, 05004], lr: 0.074477, loss: 1.9749
2022-07-15 17:19:48 - train: epoch 0041, iter [02400, 05004], lr: 0.074454, loss: 2.3092
2022-07-15 17:20:22 - train: epoch 0041, iter [02500, 05004], lr: 0.074432, loss: 1.8828
2022-07-15 17:20:56 - train: epoch 0041, iter [02600, 05004], lr: 0.074409, loss: 2.3507
2022-07-15 17:21:30 - train: epoch 0041, iter [02700, 05004], lr: 0.074386, loss: 2.4250
2022-07-15 17:22:05 - train: epoch 0041, iter [02800, 05004], lr: 0.074363, loss: 2.0691
2022-07-15 17:22:38 - train: epoch 0041, iter [02900, 05004], lr: 0.074340, loss: 2.3863
2022-07-15 17:23:13 - train: epoch 0041, iter [03000, 05004], lr: 0.074317, loss: 2.2371
2022-07-15 17:23:46 - train: epoch 0041, iter [03100, 05004], lr: 0.074294, loss: 2.3406
2022-07-15 17:24:22 - train: epoch 0041, iter [03200, 05004], lr: 0.074272, loss: 2.1582
2022-07-15 17:24:55 - train: epoch 0041, iter [03300, 05004], lr: 0.074249, loss: 1.9416
2022-07-15 17:25:30 - train: epoch 0041, iter [03400, 05004], lr: 0.074226, loss: 2.0089
2022-07-15 17:26:03 - train: epoch 0041, iter [03500, 05004], lr: 0.074203, loss: 2.2071
2022-07-15 17:26:37 - train: epoch 0041, iter [03600, 05004], lr: 0.074180, loss: 2.3054
2022-07-15 17:27:11 - train: epoch 0041, iter [03700, 05004], lr: 0.074157, loss: 2.2800
2022-07-15 17:27:44 - train: epoch 0041, iter [03800, 05004], lr: 0.074134, loss: 2.1041
2022-07-15 17:28:18 - train: epoch 0041, iter [03900, 05004], lr: 0.074111, loss: 2.2441
2022-07-15 17:28:51 - train: epoch 0041, iter [04000, 05004], lr: 0.074088, loss: 2.0201
2022-07-15 17:29:25 - train: epoch 0041, iter [04100, 05004], lr: 0.074065, loss: 2.1760
2022-07-15 17:29:58 - train: epoch 0041, iter [04200, 05004], lr: 0.074043, loss: 2.1007
2022-07-15 17:30:31 - train: epoch 0041, iter [04300, 05004], lr: 0.074020, loss: 2.1527
2022-07-15 17:31:05 - train: epoch 0041, iter [04400, 05004], lr: 0.073997, loss: 2.0776
2022-07-15 17:31:38 - train: epoch 0041, iter [04500, 05004], lr: 0.073974, loss: 2.2052
2022-07-15 17:32:12 - train: epoch 0041, iter [04600, 05004], lr: 0.073951, loss: 2.3180
2022-07-15 17:32:45 - train: epoch 0041, iter [04700, 05004], lr: 0.073928, loss: 2.3627
2022-07-15 17:33:19 - train: epoch 0041, iter [04800, 05004], lr: 0.073905, loss: 2.1839
2022-07-15 17:33:52 - train: epoch 0041, iter [04900, 05004], lr: 0.073882, loss: 2.1520
2022-07-15 17:34:25 - train: epoch 0041, iter [05000, 05004], lr: 0.073859, loss: 2.2273
2022-07-15 17:34:25 - train: epoch 041, train_loss: 2.1735
2022-07-15 17:35:39 - eval: epoch: 041, acc1: 53.144%, acc5: 78.074%, test_loss: 2.0214, per_image_load_time: 1.139ms, per_image_inference_time: 0.210ms
2022-07-15 17:35:39 - until epoch: 041, best_acc1: 54.908%
2022-07-15 17:35:39 - epoch 042 lr: 0.073858
2022-07-15 17:36:18 - train: epoch 0042, iter [00100, 05004], lr: 0.073835, loss: 1.9255
2022-07-15 17:36:52 - train: epoch 0042, iter [00200, 05004], lr: 0.073812, loss: 2.3436
2022-07-15 17:37:26 - train: epoch 0042, iter [00300, 05004], lr: 0.073789, loss: 2.2668
2022-07-15 17:38:00 - train: epoch 0042, iter [00400, 05004], lr: 0.073766, loss: 1.9311
2022-07-15 17:38:33 - train: epoch 0042, iter [00500, 05004], lr: 0.073743, loss: 2.2109
2022-07-15 17:39:06 - train: epoch 0042, iter [00600, 05004], lr: 0.073720, loss: 1.9540
2022-07-15 17:39:39 - train: epoch 0042, iter [00700, 05004], lr: 0.073697, loss: 2.1845
2022-07-15 17:40:13 - train: epoch 0042, iter [00800, 05004], lr: 0.073674, loss: 1.9987
2022-07-15 17:40:47 - train: epoch 0042, iter [00900, 05004], lr: 0.073651, loss: 2.3346
2022-07-15 17:41:20 - train: epoch 0042, iter [01000, 05004], lr: 0.073628, loss: 2.2909
2022-07-15 17:41:55 - train: epoch 0042, iter [01100, 05004], lr: 0.073605, loss: 1.9506
2022-07-15 17:42:28 - train: epoch 0042, iter [01200, 05004], lr: 0.073582, loss: 2.0606
2022-07-15 17:43:03 - train: epoch 0042, iter [01300, 05004], lr: 0.073559, loss: 2.2674
2022-07-15 17:43:36 - train: epoch 0042, iter [01400, 05004], lr: 0.073535, loss: 2.4669
2022-07-15 17:44:09 - train: epoch 0042, iter [01500, 05004], lr: 0.073512, loss: 2.0550
2022-07-15 17:44:43 - train: epoch 0042, iter [01600, 05004], lr: 0.073489, loss: 2.1724
2022-07-15 17:45:17 - train: epoch 0042, iter [01700, 05004], lr: 0.073466, loss: 2.0754
2022-07-15 17:45:50 - train: epoch 0042, iter [01800, 05004], lr: 0.073443, loss: 2.0932
2022-07-15 17:46:25 - train: epoch 0042, iter [01900, 05004], lr: 0.073420, loss: 2.2967
2022-07-15 17:46:58 - train: epoch 0042, iter [02000, 05004], lr: 0.073397, loss: 2.0899
2022-07-15 17:47:32 - train: epoch 0042, iter [02100, 05004], lr: 0.073374, loss: 1.9958
2022-07-15 17:48:06 - train: epoch 0042, iter [02200, 05004], lr: 0.073351, loss: 2.1353
2022-07-15 17:48:39 - train: epoch 0042, iter [02300, 05004], lr: 0.073327, loss: 2.0427
2022-07-15 17:49:13 - train: epoch 0042, iter [02400, 05004], lr: 0.073304, loss: 2.4320
2022-07-15 17:49:47 - train: epoch 0042, iter [02500, 05004], lr: 0.073281, loss: 2.2696
2022-07-15 17:50:21 - train: epoch 0042, iter [02600, 05004], lr: 0.073258, loss: 2.1982
2022-07-15 17:50:54 - train: epoch 0042, iter [02700, 05004], lr: 0.073235, loss: 2.0007
2022-07-15 17:51:29 - train: epoch 0042, iter [02800, 05004], lr: 0.073212, loss: 2.0207
2022-07-15 17:52:02 - train: epoch 0042, iter [02900, 05004], lr: 0.073189, loss: 2.3065
2022-07-15 17:52:35 - train: epoch 0042, iter [03000, 05004], lr: 0.073165, loss: 2.1394
2022-07-15 17:53:10 - train: epoch 0042, iter [03100, 05004], lr: 0.073142, loss: 2.2998
2022-07-15 17:53:43 - train: epoch 0042, iter [03200, 05004], lr: 0.073119, loss: 2.2943
2022-07-15 17:54:17 - train: epoch 0042, iter [03300, 05004], lr: 0.073096, loss: 2.3209
2022-07-15 17:54:50 - train: epoch 0042, iter [03400, 05004], lr: 0.073073, loss: 2.0380
2022-07-15 17:55:24 - train: epoch 0042, iter [03500, 05004], lr: 0.073049, loss: 2.2032
2022-07-15 17:55:57 - train: epoch 0042, iter [03600, 05004], lr: 0.073026, loss: 2.3549
2022-07-15 17:56:31 - train: epoch 0042, iter [03700, 05004], lr: 0.073003, loss: 2.2274
2022-07-15 17:57:06 - train: epoch 0042, iter [03800, 05004], lr: 0.072980, loss: 2.0622
2022-07-15 17:57:40 - train: epoch 0042, iter [03900, 05004], lr: 0.072956, loss: 2.1001
2022-07-15 17:58:13 - train: epoch 0042, iter [04000, 05004], lr: 0.072933, loss: 2.1707
2022-07-15 17:58:46 - train: epoch 0042, iter [04100, 05004], lr: 0.072910, loss: 2.2922
2022-07-15 17:59:20 - train: epoch 0042, iter [04200, 05004], lr: 0.072887, loss: 2.3270
2022-07-15 17:59:54 - train: epoch 0042, iter [04300, 05004], lr: 0.072863, loss: 1.9795
2022-07-15 18:00:28 - train: epoch 0042, iter [04400, 05004], lr: 0.072840, loss: 2.2608
2022-07-15 18:01:01 - train: epoch 0042, iter [04500, 05004], lr: 0.072817, loss: 2.1455
2022-07-15 18:01:34 - train: epoch 0042, iter [04600, 05004], lr: 0.072794, loss: 2.4601
2022-07-15 18:02:08 - train: epoch 0042, iter [04700, 05004], lr: 0.072770, loss: 2.0827
2022-07-15 18:02:42 - train: epoch 0042, iter [04800, 05004], lr: 0.072747, loss: 2.2600
2022-07-15 18:03:16 - train: epoch 0042, iter [04900, 05004], lr: 0.072724, loss: 2.2090
2022-07-15 18:03:49 - train: epoch 0042, iter [05000, 05004], lr: 0.072700, loss: 2.1361
2022-07-15 18:03:49 - train: epoch 042, train_loss: 2.1634
2022-07-15 18:05:04 - eval: epoch: 042, acc1: 55.648%, acc5: 80.010%, test_loss: 1.9096, per_image_load_time: 1.025ms, per_image_inference_time: 0.213ms
2022-07-15 18:05:05 - until epoch: 042, best_acc1: 55.648%
2022-07-15 18:05:05 - epoch 043 lr: 0.072699
2022-07-15 18:05:43 - train: epoch 0043, iter [00100, 05004], lr: 0.072676, loss: 2.1524
2022-07-15 18:06:17 - train: epoch 0043, iter [00200, 05004], lr: 0.072653, loss: 2.2458
2022-07-15 18:06:51 - train: epoch 0043, iter [00300, 05004], lr: 0.072630, loss: 1.9498
2022-07-15 18:07:24 - train: epoch 0043, iter [00400, 05004], lr: 0.072606, loss: 2.0637
2022-07-15 18:07:58 - train: epoch 0043, iter [00500, 05004], lr: 0.072583, loss: 2.0375
2022-07-15 18:08:31 - train: epoch 0043, iter [00600, 05004], lr: 0.072560, loss: 2.1151
2022-07-15 18:09:05 - train: epoch 0043, iter [00700, 05004], lr: 0.072536, loss: 2.2152
2022-07-15 18:09:38 - train: epoch 0043, iter [00800, 05004], lr: 0.072513, loss: 2.2702
2022-07-15 18:10:12 - train: epoch 0043, iter [00900, 05004], lr: 0.072490, loss: 2.0918
2022-07-15 18:10:45 - train: epoch 0043, iter [01000, 05004], lr: 0.072466, loss: 2.3678
2022-07-15 18:11:19 - train: epoch 0043, iter [01100, 05004], lr: 0.072443, loss: 2.1716
2022-07-15 18:11:52 - train: epoch 0043, iter [01200, 05004], lr: 0.072419, loss: 2.0167
2022-07-15 18:12:26 - train: epoch 0043, iter [01300, 05004], lr: 0.072396, loss: 2.1746
2022-07-15 18:13:00 - train: epoch 0043, iter [01400, 05004], lr: 0.072373, loss: 2.1319
2022-07-15 18:13:34 - train: epoch 0043, iter [01500, 05004], lr: 0.072349, loss: 1.9913
2022-07-15 18:14:08 - train: epoch 0043, iter [01600, 05004], lr: 0.072326, loss: 2.1788
2022-07-15 18:14:42 - train: epoch 0043, iter [01700, 05004], lr: 0.072302, loss: 2.2413
2022-07-15 18:15:16 - train: epoch 0043, iter [01800, 05004], lr: 0.072279, loss: 2.2563
2022-07-15 18:15:50 - train: epoch 0043, iter [01900, 05004], lr: 0.072256, loss: 2.1680
2022-07-15 18:16:24 - train: epoch 0043, iter [02000, 05004], lr: 0.072232, loss: 1.9848
2022-07-15 18:16:56 - train: epoch 0043, iter [02100, 05004], lr: 0.072209, loss: 2.0487
2022-07-15 18:17:31 - train: epoch 0043, iter [02200, 05004], lr: 0.072185, loss: 2.2780
2022-07-15 18:18:04 - train: epoch 0043, iter [02300, 05004], lr: 0.072162, loss: 2.3173
2022-07-15 18:18:38 - train: epoch 0043, iter [02400, 05004], lr: 0.072138, loss: 1.9588
2022-07-15 18:19:13 - train: epoch 0043, iter [02500, 05004], lr: 0.072115, loss: 2.4508
2022-07-15 18:19:46 - train: epoch 0043, iter [02600, 05004], lr: 0.072091, loss: 1.7138
2022-07-15 18:20:20 - train: epoch 0043, iter [02700, 05004], lr: 0.072068, loss: 2.2480
2022-07-15 18:20:54 - train: epoch 0043, iter [02800, 05004], lr: 0.072044, loss: 2.0268
2022-07-15 18:21:28 - train: epoch 0043, iter [02900, 05004], lr: 0.072021, loss: 2.1570
2022-07-15 18:22:01 - train: epoch 0043, iter [03000, 05004], lr: 0.071998, loss: 2.4137
2022-07-15 18:22:35 - train: epoch 0043, iter [03100, 05004], lr: 0.071974, loss: 2.2943
2022-07-15 18:23:09 - train: epoch 0043, iter [03200, 05004], lr: 0.071951, loss: 2.3536
2022-07-15 18:23:43 - train: epoch 0043, iter [03300, 05004], lr: 0.071927, loss: 2.3107
2022-07-15 18:24:17 - train: epoch 0043, iter [03400, 05004], lr: 0.071904, loss: 2.1893
2022-07-15 18:24:50 - train: epoch 0043, iter [03500, 05004], lr: 0.071880, loss: 2.1473
2022-07-15 18:25:24 - train: epoch 0043, iter [03600, 05004], lr: 0.071856, loss: 2.1973
2022-07-15 18:25:59 - train: epoch 0043, iter [03700, 05004], lr: 0.071833, loss: 2.1596
2022-07-15 18:26:32 - train: epoch 0043, iter [03800, 05004], lr: 0.071809, loss: 2.3613
2022-07-15 18:27:06 - train: epoch 0043, iter [03900, 05004], lr: 0.071786, loss: 2.3955
2022-07-15 18:27:39 - train: epoch 0043, iter [04000, 05004], lr: 0.071762, loss: 2.1771
2022-07-15 18:28:14 - train: epoch 0043, iter [04100, 05004], lr: 0.071739, loss: 2.5136
2022-07-15 18:28:48 - train: epoch 0043, iter [04200, 05004], lr: 0.071715, loss: 2.2179
2022-07-15 18:29:22 - train: epoch 0043, iter [04300, 05004], lr: 0.071692, loss: 2.1040
2022-07-15 18:29:55 - train: epoch 0043, iter [04400, 05004], lr: 0.071668, loss: 2.0653
2022-07-15 18:30:29 - train: epoch 0043, iter [04500, 05004], lr: 0.071644, loss: 2.0309
2022-07-15 18:31:03 - train: epoch 0043, iter [04600, 05004], lr: 0.071621, loss: 2.1303
2022-07-15 18:31:37 - train: epoch 0043, iter [04700, 05004], lr: 0.071597, loss: 2.0596
2022-07-15 18:32:10 - train: epoch 0043, iter [04800, 05004], lr: 0.071574, loss: 2.1153
2022-07-15 18:32:44 - train: epoch 0043, iter [04900, 05004], lr: 0.071550, loss: 1.9962
2022-07-15 18:33:16 - train: epoch 0043, iter [05000, 05004], lr: 0.071526, loss: 2.0823
2022-07-15 18:33:17 - train: epoch 043, train_loss: 2.1540
2022-07-15 18:34:32 - eval: epoch: 043, acc1: 55.878%, acc5: 79.882%, test_loss: 1.9162, per_image_load_time: 0.852ms, per_image_inference_time: 0.212ms
2022-07-15 18:34:33 - until epoch: 043, best_acc1: 55.878%
2022-07-15 18:34:33 - epoch 044 lr: 0.071525
2022-07-15 18:35:12 - train: epoch 0044, iter [00100, 05004], lr: 0.071502, loss: 2.2000
2022-07-15 18:35:45 - train: epoch 0044, iter [00200, 05004], lr: 0.071478, loss: 2.2316
2022-07-15 18:36:18 - train: epoch 0044, iter [00300, 05004], lr: 0.071455, loss: 2.0046
2022-07-15 18:36:53 - train: epoch 0044, iter [00400, 05004], lr: 0.071431, loss: 1.8866
2022-07-15 18:37:26 - train: epoch 0044, iter [00500, 05004], lr: 0.071407, loss: 2.0349
2022-07-15 18:38:00 - train: epoch 0044, iter [00600, 05004], lr: 0.071384, loss: 1.8302
2022-07-15 18:38:33 - train: epoch 0044, iter [00700, 05004], lr: 0.071360, loss: 1.9540
2022-07-15 18:39:08 - train: epoch 0044, iter [00800, 05004], lr: 0.071336, loss: 2.1075
2022-07-15 18:39:41 - train: epoch 0044, iter [00900, 05004], lr: 0.071313, loss: 2.1671
2022-07-15 18:40:15 - train: epoch 0044, iter [01000, 05004], lr: 0.071289, loss: 2.1947
2022-07-15 18:40:49 - train: epoch 0044, iter [01100, 05004], lr: 0.071265, loss: 1.9680
2022-07-15 18:41:23 - train: epoch 0044, iter [01200, 05004], lr: 0.071242, loss: 2.0610
2022-07-15 18:41:56 - train: epoch 0044, iter [01300, 05004], lr: 0.071218, loss: 2.1556
2022-07-15 18:42:30 - train: epoch 0044, iter [01400, 05004], lr: 0.071194, loss: 1.9983
2022-07-15 18:43:03 - train: epoch 0044, iter [01500, 05004], lr: 0.071171, loss: 1.9947
2022-07-15 18:43:38 - train: epoch 0044, iter [01600, 05004], lr: 0.071147, loss: 2.1508
2022-07-15 18:44:12 - train: epoch 0044, iter [01700, 05004], lr: 0.071123, loss: 2.1426
2022-07-15 18:44:46 - train: epoch 0044, iter [01800, 05004], lr: 0.071100, loss: 2.1381
2022-07-15 18:45:19 - train: epoch 0044, iter [01900, 05004], lr: 0.071076, loss: 2.3389
2022-07-15 18:45:53 - train: epoch 0044, iter [02000, 05004], lr: 0.071052, loss: 2.0150
2022-07-15 18:46:27 - train: epoch 0044, iter [02100, 05004], lr: 0.071028, loss: 2.2734
2022-07-15 18:47:02 - train: epoch 0044, iter [02200, 05004], lr: 0.071005, loss: 2.1673
2022-07-15 18:47:36 - train: epoch 0044, iter [02300, 05004], lr: 0.070981, loss: 2.2523
2022-07-15 18:48:09 - train: epoch 0044, iter [02400, 05004], lr: 0.070957, loss: 2.1203
2022-07-15 18:48:44 - train: epoch 0044, iter [02500, 05004], lr: 0.070933, loss: 2.2006
2022-07-15 18:49:17 - train: epoch 0044, iter [02600, 05004], lr: 0.070910, loss: 2.0386
2022-07-15 18:49:52 - train: epoch 0044, iter [02700, 05004], lr: 0.070886, loss: 2.2803
2022-07-15 18:50:26 - train: epoch 0044, iter [02800, 05004], lr: 0.070862, loss: 1.9020
2022-07-15 18:51:01 - train: epoch 0044, iter [02900, 05004], lr: 0.070838, loss: 1.9036
2022-07-15 18:51:34 - train: epoch 0044, iter [03000, 05004], lr: 0.070815, loss: 2.0737
2022-07-15 18:52:08 - train: epoch 0044, iter [03100, 05004], lr: 0.070791, loss: 2.0695
2022-07-15 18:52:42 - train: epoch 0044, iter [03200, 05004], lr: 0.070767, loss: 1.9273
2022-07-15 18:53:17 - train: epoch 0044, iter [03300, 05004], lr: 0.070743, loss: 2.1135
2022-07-15 18:53:50 - train: epoch 0044, iter [03400, 05004], lr: 0.070719, loss: 2.4588
2022-07-15 18:54:25 - train: epoch 0044, iter [03500, 05004], lr: 0.070696, loss: 2.0923
2022-07-15 18:54:58 - train: epoch 0044, iter [03600, 05004], lr: 0.070672, loss: 2.4308
2022-07-15 18:55:33 - train: epoch 0044, iter [03700, 05004], lr: 0.070648, loss: 2.2092
2022-07-15 18:56:06 - train: epoch 0044, iter [03800, 05004], lr: 0.070624, loss: 1.7824
2022-07-15 18:56:40 - train: epoch 0044, iter [03900, 05004], lr: 0.070600, loss: 2.2090
2022-07-15 18:57:15 - train: epoch 0044, iter [04000, 05004], lr: 0.070576, loss: 2.1858
2022-07-15 18:57:48 - train: epoch 0044, iter [04100, 05004], lr: 0.070553, loss: 2.0750
2022-07-15 18:58:22 - train: epoch 0044, iter [04200, 05004], lr: 0.070529, loss: 2.3398
2022-07-15 18:58:56 - train: epoch 0044, iter [04300, 05004], lr: 0.070505, loss: 2.3263
2022-07-15 18:59:30 - train: epoch 0044, iter [04400, 05004], lr: 0.070481, loss: 1.8965
2022-07-15 19:00:04 - train: epoch 0044, iter [04500, 05004], lr: 0.070457, loss: 2.1809
2022-07-15 19:00:37 - train: epoch 0044, iter [04600, 05004], lr: 0.070433, loss: 2.1912
2022-07-15 19:01:11 - train: epoch 0044, iter [04700, 05004], lr: 0.070409, loss: 2.2422
2022-07-15 19:01:45 - train: epoch 0044, iter [04800, 05004], lr: 0.070386, loss: 2.3772
2022-07-15 19:02:19 - train: epoch 0044, iter [04900, 05004], lr: 0.070362, loss: 1.9655
2022-07-15 19:02:51 - train: epoch 0044, iter [05000, 05004], lr: 0.070338, loss: 2.3088
2022-07-15 19:02:52 - train: epoch 044, train_loss: 2.1461
2022-07-15 19:04:08 - eval: epoch: 044, acc1: 54.564%, acc5: 79.282%, test_loss: 1.9551, per_image_load_time: 1.813ms, per_image_inference_time: 0.219ms
2022-07-15 19:04:08 - until epoch: 044, best_acc1: 55.878%
2022-07-15 19:04:08 - epoch 045 lr: 0.070337
2022-07-15 19:04:47 - train: epoch 0045, iter [00100, 05004], lr: 0.070313, loss: 1.9454
2022-07-15 19:05:20 - train: epoch 0045, iter [00200, 05004], lr: 0.070289, loss: 2.1815
2022-07-15 19:05:55 - train: epoch 0045, iter [00300, 05004], lr: 0.070265, loss: 2.1356
2022-07-15 19:06:28 - train: epoch 0045, iter [00400, 05004], lr: 0.070241, loss: 1.9510
2022-07-15 19:07:01 - train: epoch 0045, iter [00500, 05004], lr: 0.070217, loss: 2.2946
2022-07-15 19:07:35 - train: epoch 0045, iter [00600, 05004], lr: 0.070193, loss: 2.1513
2022-07-15 19:08:09 - train: epoch 0045, iter [00700, 05004], lr: 0.070169, loss: 1.8048
2022-07-15 19:08:43 - train: epoch 0045, iter [00800, 05004], lr: 0.070145, loss: 1.9357
2022-07-15 19:09:16 - train: epoch 0045, iter [00900, 05004], lr: 0.070122, loss: 2.2829
2022-07-15 19:09:50 - train: epoch 0045, iter [01000, 05004], lr: 0.070098, loss: 2.1011
2022-07-15 19:10:24 - train: epoch 0045, iter [01100, 05004], lr: 0.070074, loss: 2.2229
2022-07-15 19:10:57 - train: epoch 0045, iter [01200, 05004], lr: 0.070050, loss: 2.1572
2022-07-15 19:11:31 - train: epoch 0045, iter [01300, 05004], lr: 0.070026, loss: 2.2120
2022-07-15 19:12:05 - train: epoch 0045, iter [01400, 05004], lr: 0.070002, loss: 2.1242
2022-07-15 19:12:39 - train: epoch 0045, iter [01500, 05004], lr: 0.069978, loss: 2.2118
2022-07-15 19:13:12 - train: epoch 0045, iter [01600, 05004], lr: 0.069954, loss: 1.9760
2022-07-15 19:13:46 - train: epoch 0045, iter [01700, 05004], lr: 0.069930, loss: 1.9618
2022-07-15 19:14:19 - train: epoch 0045, iter [01800, 05004], lr: 0.069906, loss: 2.0272
2022-07-15 19:14:53 - train: epoch 0045, iter [01900, 05004], lr: 0.069882, loss: 2.2650
2022-07-15 19:15:27 - train: epoch 0045, iter [02000, 05004], lr: 0.069858, loss: 2.3359
2022-07-15 19:16:01 - train: epoch 0045, iter [02100, 05004], lr: 0.069834, loss: 2.2523
2022-07-15 19:16:35 - train: epoch 0045, iter [02200, 05004], lr: 0.069810, loss: 2.2270
2022-07-15 19:17:09 - train: epoch 0045, iter [02300, 05004], lr: 0.069786, loss: 1.9702
2022-07-15 19:17:42 - train: epoch 0045, iter [02400, 05004], lr: 0.069762, loss: 2.2124
2022-07-15 19:18:17 - train: epoch 0045, iter [02500, 05004], lr: 0.069738, loss: 2.0491
2022-07-15 19:18:50 - train: epoch 0045, iter [02600, 05004], lr: 0.069714, loss: 2.0961
2022-07-15 19:19:24 - train: epoch 0045, iter [02700, 05004], lr: 0.069690, loss: 2.0317
2022-07-15 19:19:58 - train: epoch 0045, iter [02800, 05004], lr: 0.069666, loss: 2.1212
2022-07-15 19:20:31 - train: epoch 0045, iter [02900, 05004], lr: 0.069641, loss: 2.3920
2022-07-15 19:21:05 - train: epoch 0045, iter [03000, 05004], lr: 0.069617, loss: 2.3898
2022-07-15 19:21:38 - train: epoch 0045, iter [03100, 05004], lr: 0.069593, loss: 2.0649
2022-07-15 19:22:12 - train: epoch 0045, iter [03200, 05004], lr: 0.069569, loss: 2.2582
2022-07-15 19:22:45 - train: epoch 0045, iter [03300, 05004], lr: 0.069545, loss: 2.1019
2022-07-15 19:23:20 - train: epoch 0045, iter [03400, 05004], lr: 0.069521, loss: 1.7921
2022-07-15 19:23:53 - train: epoch 0045, iter [03500, 05004], lr: 0.069497, loss: 2.3350
2022-07-15 19:24:27 - train: epoch 0045, iter [03600, 05004], lr: 0.069473, loss: 2.3240
2022-07-15 19:25:00 - train: epoch 0045, iter [03700, 05004], lr: 0.069449, loss: 2.0738
2022-07-15 19:25:35 - train: epoch 0045, iter [03800, 05004], lr: 0.069425, loss: 1.9644
2022-07-15 19:26:08 - train: epoch 0045, iter [03900, 05004], lr: 0.069401, loss: 2.2761
2022-07-15 19:26:42 - train: epoch 0045, iter [04000, 05004], lr: 0.069377, loss: 2.2583
2022-07-15 19:27:15 - train: epoch 0045, iter [04100, 05004], lr: 0.069352, loss: 2.1134
2022-07-15 19:27:50 - train: epoch 0045, iter [04200, 05004], lr: 0.069328, loss: 2.3420
2022-07-15 19:28:23 - train: epoch 0045, iter [04300, 05004], lr: 0.069304, loss: 2.3495
2022-07-15 19:28:58 - train: epoch 0045, iter [04400, 05004], lr: 0.069280, loss: 2.2347
2022-07-15 19:29:32 - train: epoch 0045, iter [04500, 05004], lr: 0.069256, loss: 1.9550
2022-07-15 19:30:05 - train: epoch 0045, iter [04600, 05004], lr: 0.069232, loss: 2.3665
2022-07-15 19:30:41 - train: epoch 0045, iter [04700, 05004], lr: 0.069208, loss: 2.1222
2022-07-15 19:31:13 - train: epoch 0045, iter [04800, 05004], lr: 0.069183, loss: 2.0376
2022-07-15 19:31:47 - train: epoch 0045, iter [04900, 05004], lr: 0.069159, loss: 2.0016
2022-07-15 19:32:19 - train: epoch 0045, iter [05000, 05004], lr: 0.069135, loss: 2.1107
2022-07-15 19:32:20 - train: epoch 045, train_loss: 2.1339
2022-07-15 19:33:35 - eval: epoch: 045, acc1: 55.726%, acc5: 79.806%, test_loss: 1.9018, per_image_load_time: 2.634ms, per_image_inference_time: 0.198ms
2022-07-15 19:33:35 - until epoch: 045, best_acc1: 55.878%
2022-07-15 19:33:35 - epoch 046 lr: 0.069134
2022-07-15 19:34:14 - train: epoch 0046, iter [00100, 05004], lr: 0.069110, loss: 1.8278
2022-07-15 19:34:47 - train: epoch 0046, iter [00200, 05004], lr: 0.069086, loss: 1.9991
2022-07-15 19:35:20 - train: epoch 0046, iter [00300, 05004], lr: 0.069062, loss: 2.1731
2022-07-15 19:35:54 - train: epoch 0046, iter [00400, 05004], lr: 0.069037, loss: 2.2958
2022-07-15 19:36:28 - train: epoch 0046, iter [00500, 05004], lr: 0.069013, loss: 1.9296
2022-07-15 19:37:01 - train: epoch 0046, iter [00600, 05004], lr: 0.068989, loss: 2.1647
2022-07-15 19:37:35 - train: epoch 0046, iter [00700, 05004], lr: 0.068965, loss: 2.0905
2022-07-15 19:38:09 - train: epoch 0046, iter [00800, 05004], lr: 0.068941, loss: 2.3168
2022-07-15 19:38:42 - train: epoch 0046, iter [00900, 05004], lr: 0.068916, loss: 2.1510
2022-07-15 19:39:16 - train: epoch 0046, iter [01000, 05004], lr: 0.068892, loss: 1.8668
2022-07-15 19:39:50 - train: epoch 0046, iter [01100, 05004], lr: 0.068868, loss: 2.0762
2022-07-15 19:40:24 - train: epoch 0046, iter [01200, 05004], lr: 0.068844, loss: 1.9619
2022-07-15 19:40:57 - train: epoch 0046, iter [01300, 05004], lr: 0.068820, loss: 2.0619
2022-07-15 19:41:31 - train: epoch 0046, iter [01400, 05004], lr: 0.068795, loss: 2.3334
2022-07-15 19:42:04 - train: epoch 0046, iter [01500, 05004], lr: 0.068771, loss: 2.0710
2022-07-15 19:42:38 - train: epoch 0046, iter [01600, 05004], lr: 0.068747, loss: 2.1437
2022-07-15 19:43:12 - train: epoch 0046, iter [01700, 05004], lr: 0.068723, loss: 2.0362
2022-07-15 19:43:46 - train: epoch 0046, iter [01800, 05004], lr: 0.068698, loss: 2.1616
2022-07-15 19:44:20 - train: epoch 0046, iter [01900, 05004], lr: 0.068674, loss: 1.9104
2022-07-15 19:44:54 - train: epoch 0046, iter [02000, 05004], lr: 0.068650, loss: 2.1505
2022-07-15 19:45:27 - train: epoch 0046, iter [02100, 05004], lr: 0.068626, loss: 2.1727
2022-07-15 19:46:02 - train: epoch 0046, iter [02200, 05004], lr: 0.068601, loss: 1.7527
2022-07-15 19:46:35 - train: epoch 0046, iter [02300, 05004], lr: 0.068577, loss: 2.4178
2022-07-15 19:47:09 - train: epoch 0046, iter [02400, 05004], lr: 0.068553, loss: 2.0521
2022-07-15 19:47:42 - train: epoch 0046, iter [02500, 05004], lr: 0.068528, loss: 2.2880
2022-07-15 19:48:17 - train: epoch 0046, iter [02600, 05004], lr: 0.068504, loss: 2.3083
2022-07-15 19:48:50 - train: epoch 0046, iter [02700, 05004], lr: 0.068480, loss: 2.0502
2022-07-15 19:49:24 - train: epoch 0046, iter [02800, 05004], lr: 0.068455, loss: 2.0489
2022-07-15 19:49:58 - train: epoch 0046, iter [02900, 05004], lr: 0.068431, loss: 2.1677
2022-07-15 19:50:31 - train: epoch 0046, iter [03000, 05004], lr: 0.068407, loss: 1.9640
2022-07-15 19:51:05 - train: epoch 0046, iter [03100, 05004], lr: 0.068382, loss: 1.8646
2022-07-15 19:51:39 - train: epoch 0046, iter [03200, 05004], lr: 0.068358, loss: 2.2763
2022-07-15 19:52:13 - train: epoch 0046, iter [03300, 05004], lr: 0.068334, loss: 2.1111
2022-07-15 19:52:46 - train: epoch 0046, iter [03400, 05004], lr: 0.068309, loss: 2.0356
2022-07-15 19:53:21 - train: epoch 0046, iter [03500, 05004], lr: 0.068285, loss: 2.0582
2022-07-15 19:53:54 - train: epoch 0046, iter [03600, 05004], lr: 0.068261, loss: 2.0839
2022-07-15 19:54:28 - train: epoch 0046, iter [03700, 05004], lr: 0.068236, loss: 1.9404
2022-07-15 19:55:02 - train: epoch 0046, iter [03800, 05004], lr: 0.068212, loss: 2.2781
2022-07-15 19:55:36 - train: epoch 0046, iter [03900, 05004], lr: 0.068188, loss: 2.1522
2022-07-15 19:56:09 - train: epoch 0046, iter [04000, 05004], lr: 0.068163, loss: 2.0396
2022-07-15 19:56:44 - train: epoch 0046, iter [04100, 05004], lr: 0.068139, loss: 2.4043
2022-07-15 19:57:18 - train: epoch 0046, iter [04200, 05004], lr: 0.068115, loss: 2.1424
2022-07-15 19:57:51 - train: epoch 0046, iter [04300, 05004], lr: 0.068090, loss: 2.2337
2022-07-15 19:58:24 - train: epoch 0046, iter [04400, 05004], lr: 0.068066, loss: 2.4250
2022-07-15 19:58:59 - train: epoch 0046, iter [04500, 05004], lr: 0.068041, loss: 2.0331
2022-07-15 19:59:32 - train: epoch 0046, iter [04600, 05004], lr: 0.068017, loss: 2.3204
2022-07-15 20:00:06 - train: epoch 0046, iter [04700, 05004], lr: 0.067993, loss: 2.0784
2022-07-15 20:00:40 - train: epoch 0046, iter [04800, 05004], lr: 0.067968, loss: 2.0749
2022-07-15 20:01:14 - train: epoch 0046, iter [04900, 05004], lr: 0.067944, loss: 2.3272
2022-07-15 20:01:47 - train: epoch 0046, iter [05000, 05004], lr: 0.067919, loss: 2.1393
2022-07-15 20:01:47 - train: epoch 046, train_loss: 2.1251
2022-07-15 20:03:03 - eval: epoch: 046, acc1: 56.294%, acc5: 80.144%, test_loss: 1.8924, per_image_load_time: 1.032ms, per_image_inference_time: 0.193ms
2022-07-15 20:03:03 - until epoch: 046, best_acc1: 56.294%
2022-07-15 20:03:03 - epoch 047 lr: 0.067918
2022-07-15 20:03:43 - train: epoch 0047, iter [00100, 05004], lr: 0.067894, loss: 2.0420
2022-07-15 20:04:17 - train: epoch 0047, iter [00200, 05004], lr: 0.067870, loss: 2.2152
2022-07-15 20:04:50 - train: epoch 0047, iter [00300, 05004], lr: 0.067845, loss: 2.0476
2022-07-15 20:05:25 - train: epoch 0047, iter [00400, 05004], lr: 0.067821, loss: 1.9411
2022-07-15 20:05:59 - train: epoch 0047, iter [00500, 05004], lr: 0.067796, loss: 2.2172
2022-07-15 20:06:33 - train: epoch 0047, iter [00600, 05004], lr: 0.067772, loss: 2.1801
2022-07-15 20:07:06 - train: epoch 0047, iter [00700, 05004], lr: 0.067747, loss: 2.2174
2022-07-15 20:07:40 - train: epoch 0047, iter [00800, 05004], lr: 0.067723, loss: 1.9946
2022-07-15 20:08:13 - train: epoch 0047, iter [00900, 05004], lr: 0.067698, loss: 2.3979
2022-07-15 20:08:48 - train: epoch 0047, iter [01000, 05004], lr: 0.067674, loss: 2.1350
2022-07-15 20:09:21 - train: epoch 0047, iter [01100, 05004], lr: 0.067649, loss: 2.1657
2022-07-15 20:09:55 - train: epoch 0047, iter [01200, 05004], lr: 0.067625, loss: 2.0390
2022-07-15 20:10:29 - train: epoch 0047, iter [01300, 05004], lr: 0.067601, loss: 2.1345
2022-07-15 20:11:02 - train: epoch 0047, iter [01400, 05004], lr: 0.067576, loss: 2.0480
2022-07-15 20:11:37 - train: epoch 0047, iter [01500, 05004], lr: 0.067552, loss: 2.0069
2022-07-15 20:12:10 - train: epoch 0047, iter [01600, 05004], lr: 0.067527, loss: 2.1503
2022-07-15 20:12:45 - train: epoch 0047, iter [01700, 05004], lr: 0.067503, loss: 2.0202
2022-07-15 20:13:18 - train: epoch 0047, iter [01800, 05004], lr: 0.067478, loss: 2.1754
2022-07-15 20:13:53 - train: epoch 0047, iter [01900, 05004], lr: 0.067454, loss: 2.0699
2022-07-15 20:14:27 - train: epoch 0047, iter [02000, 05004], lr: 0.067429, loss: 2.0836
2022-07-15 20:15:01 - train: epoch 0047, iter [02100, 05004], lr: 0.067404, loss: 2.3901
2022-07-15 20:15:34 - train: epoch 0047, iter [02200, 05004], lr: 0.067380, loss: 2.2937
2022-07-15 20:16:08 - train: epoch 0047, iter [02300, 05004], lr: 0.067355, loss: 2.1870
2022-07-15 20:16:42 - train: epoch 0047, iter [02400, 05004], lr: 0.067331, loss: 1.9731
2022-07-15 20:17:16 - train: epoch 0047, iter [02500, 05004], lr: 0.067306, loss: 2.0576
2022-07-15 20:17:50 - train: epoch 0047, iter [02600, 05004], lr: 0.067282, loss: 2.4269
2022-07-15 20:18:25 - train: epoch 0047, iter [02700, 05004], lr: 0.067257, loss: 2.1174
2022-07-15 20:18:59 - train: epoch 0047, iter [02800, 05004], lr: 0.067233, loss: 2.4350
2022-07-15 20:19:33 - train: epoch 0047, iter [02900, 05004], lr: 0.067208, loss: 2.0869
2022-07-15 20:20:07 - train: epoch 0047, iter [03000, 05004], lr: 0.067184, loss: 2.1512
2022-07-15 20:20:41 - train: epoch 0047, iter [03100, 05004], lr: 0.067159, loss: 1.9394
2022-07-15 20:21:15 - train: epoch 0047, iter [03200, 05004], lr: 0.067134, loss: 2.3796
2022-07-15 20:21:49 - train: epoch 0047, iter [03300, 05004], lr: 0.067110, loss: 2.0088
2022-07-15 20:22:23 - train: epoch 0047, iter [03400, 05004], lr: 0.067085, loss: 2.0529
2022-07-15 20:22:57 - train: epoch 0047, iter [03500, 05004], lr: 0.067061, loss: 2.3698
2022-07-15 20:23:31 - train: epoch 0047, iter [03600, 05004], lr: 0.067036, loss: 2.1723
2022-07-15 20:24:05 - train: epoch 0047, iter [03700, 05004], lr: 0.067011, loss: 2.2790
2022-07-15 20:24:38 - train: epoch 0047, iter [03800, 05004], lr: 0.066987, loss: 2.0550
2022-07-15 20:25:13 - train: epoch 0047, iter [03900, 05004], lr: 0.066962, loss: 2.0061
2022-07-15 20:25:46 - train: epoch 0047, iter [04000, 05004], lr: 0.066938, loss: 2.2147
2022-07-15 20:26:20 - train: epoch 0047, iter [04100, 05004], lr: 0.066913, loss: 2.1899
2022-07-15 20:26:55 - train: epoch 0047, iter [04200, 05004], lr: 0.066888, loss: 2.2232
2022-07-15 20:27:29 - train: epoch 0047, iter [04300, 05004], lr: 0.066864, loss: 2.0488
2022-07-15 20:28:03 - train: epoch 0047, iter [04400, 05004], lr: 0.066839, loss: 2.0866
2022-07-15 20:28:39 - train: epoch 0047, iter [04500, 05004], lr: 0.066815, loss: 2.1336
2022-07-15 20:29:12 - train: epoch 0047, iter [04600, 05004], lr: 0.066790, loss: 2.0558
2022-07-15 20:29:46 - train: epoch 0047, iter [04700, 05004], lr: 0.066765, loss: 2.0940
2022-07-15 20:30:22 - train: epoch 0047, iter [04800, 05004], lr: 0.066741, loss: 2.0115
2022-07-15 20:30:55 - train: epoch 0047, iter [04900, 05004], lr: 0.066716, loss: 2.2935
2022-07-15 20:31:28 - train: epoch 0047, iter [05000, 05004], lr: 0.066691, loss: 1.9914
2022-07-15 20:31:29 - train: epoch 047, train_loss: 2.1175
2022-07-15 20:32:45 - eval: epoch: 047, acc1: 54.756%, acc5: 79.240%, test_loss: 1.9536, per_image_load_time: 2.011ms, per_image_inference_time: 0.218ms
2022-07-15 20:32:45 - until epoch: 047, best_acc1: 56.294%
2022-07-15 20:32:45 - epoch 048 lr: 0.066690
2022-07-15 20:33:25 - train: epoch 0048, iter [00100, 05004], lr: 0.066666, loss: 2.1535
2022-07-15 20:33:59 - train: epoch 0048, iter [00200, 05004], lr: 0.066641, loss: 2.4873
2022-07-15 20:34:34 - train: epoch 0048, iter [00300, 05004], lr: 0.066616, loss: 2.1187
2022-07-15 20:35:06 - train: epoch 0048, iter [00400, 05004], lr: 0.066592, loss: 2.0599
2022-07-15 20:35:41 - train: epoch 0048, iter [00500, 05004], lr: 0.066567, loss: 2.0171
2022-07-15 20:36:15 - train: epoch 0048, iter [00600, 05004], lr: 0.066542, loss: 2.1717
2022-07-15 20:36:50 - train: epoch 0048, iter [00700, 05004], lr: 0.066518, loss: 2.0889
2022-07-15 20:37:24 - train: epoch 0048, iter [00800, 05004], lr: 0.066493, loss: 2.1352
2022-07-15 20:37:58 - train: epoch 0048, iter [00900, 05004], lr: 0.066468, loss: 2.3566
2022-07-15 20:38:31 - train: epoch 0048, iter [01000, 05004], lr: 0.066444, loss: 2.1177
2022-07-15 20:39:06 - train: epoch 0048, iter [01100, 05004], lr: 0.066419, loss: 2.1714
2022-07-15 20:39:40 - train: epoch 0048, iter [01200, 05004], lr: 0.066394, loss: 2.0915
2022-07-15 20:40:14 - train: epoch 0048, iter [01300, 05004], lr: 0.066369, loss: 1.8665
2022-07-15 20:40:48 - train: epoch 0048, iter [01400, 05004], lr: 0.066345, loss: 2.1039
2022-07-15 20:41:22 - train: epoch 0048, iter [01500, 05004], lr: 0.066320, loss: 2.1675
2022-07-15 20:41:56 - train: epoch 0048, iter [01600, 05004], lr: 0.066295, loss: 2.1350
2022-07-15 20:42:31 - train: epoch 0048, iter [01700, 05004], lr: 0.066270, loss: 2.3212
2022-07-15 20:43:04 - train: epoch 0048, iter [01800, 05004], lr: 0.066246, loss: 2.0970
2022-07-15 20:43:39 - train: epoch 0048, iter [01900, 05004], lr: 0.066221, loss: 2.0455
2022-07-15 20:44:13 - train: epoch 0048, iter [02000, 05004], lr: 0.066196, loss: 2.2718
2022-07-15 20:44:48 - train: epoch 0048, iter [02100, 05004], lr: 0.066172, loss: 2.1487
2022-07-15 20:45:21 - train: epoch 0048, iter [02200, 05004], lr: 0.066147, loss: 2.3713
2022-07-15 20:45:55 - train: epoch 0048, iter [02300, 05004], lr: 0.066122, loss: 1.9754
2022-07-15 20:46:30 - train: epoch 0048, iter [02400, 05004], lr: 0.066097, loss: 2.3919
2022-07-15 20:47:04 - train: epoch 0048, iter [02500, 05004], lr: 0.066072, loss: 2.2522
2022-07-15 20:47:38 - train: epoch 0048, iter [02600, 05004], lr: 0.066048, loss: 2.2725
2022-07-15 20:48:12 - train: epoch 0048, iter [02700, 05004], lr: 0.066023, loss: 2.2536
2022-07-15 20:48:46 - train: epoch 0048, iter [02800, 05004], lr: 0.065998, loss: 2.1104
2022-07-15 20:49:20 - train: epoch 0048, iter [02900, 05004], lr: 0.065973, loss: 2.2059
2022-07-15 20:49:54 - train: epoch 0048, iter [03000, 05004], lr: 0.065949, loss: 2.1533
2022-07-15 20:50:28 - train: epoch 0048, iter [03100, 05004], lr: 0.065924, loss: 2.1344
2022-07-15 20:51:02 - train: epoch 0048, iter [03200, 05004], lr: 0.065899, loss: 1.8702
2022-07-15 20:51:37 - train: epoch 0048, iter [03300, 05004], lr: 0.065874, loss: 2.2508
2022-07-15 20:52:10 - train: epoch 0048, iter [03400, 05004], lr: 0.065849, loss: 2.0276
2022-07-15 20:52:45 - train: epoch 0048, iter [03500, 05004], lr: 0.065825, loss: 2.3583
2022-07-15 20:53:19 - train: epoch 0048, iter [03600, 05004], lr: 0.065800, loss: 2.1353
2022-07-15 20:53:53 - train: epoch 0048, iter [03700, 05004], lr: 0.065775, loss: 2.1348
2022-07-15 20:54:27 - train: epoch 0048, iter [03800, 05004], lr: 0.065750, loss: 2.1102
2022-07-15 20:55:02 - train: epoch 0048, iter [03900, 05004], lr: 0.065725, loss: 2.2583
2022-07-15 20:55:36 - train: epoch 0048, iter [04000, 05004], lr: 0.065700, loss: 1.8375
2022-07-15 20:56:11 - train: epoch 0048, iter [04100, 05004], lr: 0.065676, loss: 2.3307
2022-07-15 20:56:44 - train: epoch 0048, iter [04200, 05004], lr: 0.065651, loss: 2.0139
2022-07-15 20:57:18 - train: epoch 0048, iter [04300, 05004], lr: 0.065626, loss: 2.0746
2022-07-15 20:57:52 - train: epoch 0048, iter [04400, 05004], lr: 0.065601, loss: 1.9813
2022-07-15 20:58:27 - train: epoch 0048, iter [04500, 05004], lr: 0.065576, loss: 2.1047
2022-07-15 20:59:01 - train: epoch 0048, iter [04600, 05004], lr: 0.065551, loss: 2.1560
2022-07-15 20:59:35 - train: epoch 0048, iter [04700, 05004], lr: 0.065526, loss: 2.1305
2022-07-15 21:00:09 - train: epoch 0048, iter [04800, 05004], lr: 0.065502, loss: 2.4505
2022-07-15 21:00:44 - train: epoch 0048, iter [04900, 05004], lr: 0.065477, loss: 2.1774
2022-07-15 21:01:16 - train: epoch 0048, iter [05000, 05004], lr: 0.065452, loss: 2.1175
2022-07-15 21:01:17 - train: epoch 048, train_loss: 2.1117
2022-07-15 21:02:33 - eval: epoch: 048, acc1: 56.598%, acc5: 80.632%, test_loss: 1.8618, per_image_load_time: 1.795ms, per_image_inference_time: 0.206ms
2022-07-15 21:02:33 - until epoch: 048, best_acc1: 56.598%
2022-07-15 21:02:33 - epoch 049 lr: 0.065451
2022-07-15 21:03:13 - train: epoch 0049, iter [00100, 05004], lr: 0.065426, loss: 2.2669
2022-07-15 21:03:46 - train: epoch 0049, iter [00200, 05004], lr: 0.065401, loss: 2.0393
2022-07-15 21:04:21 - train: epoch 0049, iter [00300, 05004], lr: 0.065376, loss: 2.1672
2022-07-15 21:04:55 - train: epoch 0049, iter [00400, 05004], lr: 0.065351, loss: 2.1062
2022-07-15 21:05:29 - train: epoch 0049, iter [00500, 05004], lr: 0.065326, loss: 1.9878
2022-07-15 21:06:03 - train: epoch 0049, iter [00600, 05004], lr: 0.065302, loss: 2.0102
2022-07-15 21:06:38 - train: epoch 0049, iter [00700, 05004], lr: 0.065277, loss: 2.1538
2022-07-15 21:07:12 - train: epoch 0049, iter [00800, 05004], lr: 0.065252, loss: 2.4265
2022-07-15 21:07:46 - train: epoch 0049, iter [00900, 05004], lr: 0.065227, loss: 2.0360
2022-07-15 21:08:20 - train: epoch 0049, iter [01000, 05004], lr: 0.065202, loss: 2.0824
2022-07-15 21:08:55 - train: epoch 0049, iter [01100, 05004], lr: 0.065177, loss: 1.8629
2022-07-15 21:09:29 - train: epoch 0049, iter [01200, 05004], lr: 0.065152, loss: 1.8501
2022-07-15 21:10:03 - train: epoch 0049, iter [01300, 05004], lr: 0.065127, loss: 2.2155
2022-07-15 21:10:37 - train: epoch 0049, iter [01400, 05004], lr: 0.065102, loss: 2.2776
2022-07-15 21:11:11 - train: epoch 0049, iter [01500, 05004], lr: 0.065077, loss: 2.0555
2022-07-15 21:11:46 - train: epoch 0049, iter [01600, 05004], lr: 0.065052, loss: 2.2564
2022-07-15 21:12:19 - train: epoch 0049, iter [01700, 05004], lr: 0.065027, loss: 2.2270
2022-07-15 21:12:54 - train: epoch 0049, iter [01800, 05004], lr: 0.065002, loss: 1.9822
2022-07-15 21:13:28 - train: epoch 0049, iter [01900, 05004], lr: 0.064977, loss: 2.0177
2022-07-15 21:14:03 - train: epoch 0049, iter [02000, 05004], lr: 0.064952, loss: 1.9784
2022-07-15 21:14:37 - train: epoch 0049, iter [02100, 05004], lr: 0.064927, loss: 2.0572
2022-07-15 21:15:12 - train: epoch 0049, iter [02200, 05004], lr: 0.064903, loss: 1.9963
2022-07-15 21:15:46 - train: epoch 0049, iter [02300, 05004], lr: 0.064878, loss: 2.0151
2022-07-15 21:16:21 - train: epoch 0049, iter [02400, 05004], lr: 0.064853, loss: 2.3399
2022-07-15 21:16:55 - train: epoch 0049, iter [02500, 05004], lr: 0.064828, loss: 2.1020
2022-07-15 21:17:29 - train: epoch 0049, iter [02600, 05004], lr: 0.064803, loss: 2.1381
2022-07-15 21:18:05 - train: epoch 0049, iter [02700, 05004], lr: 0.064778, loss: 1.9321
2022-07-15 21:18:38 - train: epoch 0049, iter [02800, 05004], lr: 0.064753, loss: 2.0382
2022-07-15 21:19:13 - train: epoch 0049, iter [02900, 05004], lr: 0.064728, loss: 2.1722
2022-07-15 21:19:47 - train: epoch 0049, iter [03000, 05004], lr: 0.064703, loss: 2.3142
2022-07-15 21:20:22 - train: epoch 0049, iter [03100, 05004], lr: 0.064678, loss: 2.1780
2022-07-15 21:20:56 - train: epoch 0049, iter [03200, 05004], lr: 0.064653, loss: 2.1951
2022-07-15 21:21:30 - train: epoch 0049, iter [03300, 05004], lr: 0.064628, loss: 2.0245
2022-07-15 21:22:05 - train: epoch 0049, iter [03400, 05004], lr: 0.064603, loss: 2.3114
2022-07-15 21:22:39 - train: epoch 0049, iter [03500, 05004], lr: 0.064578, loss: 2.1686
2022-07-15 21:23:14 - train: epoch 0049, iter [03600, 05004], lr: 0.064553, loss: 2.2092
2022-07-15 21:23:49 - train: epoch 0049, iter [03700, 05004], lr: 0.064528, loss: 2.1232
2022-07-15 21:24:22 - train: epoch 0049, iter [03800, 05004], lr: 0.064502, loss: 2.3584
2022-07-15 21:24:57 - train: epoch 0049, iter [03900, 05004], lr: 0.064477, loss: 2.3375
2022-07-15 21:25:32 - train: epoch 0049, iter [04000, 05004], lr: 0.064452, loss: 1.9672
2022-07-15 21:26:06 - train: epoch 0049, iter [04100, 05004], lr: 0.064427, loss: 1.8815
2022-07-15 21:26:40 - train: epoch 0049, iter [04200, 05004], lr: 0.064402, loss: 2.1780
2022-07-15 21:27:15 - train: epoch 0049, iter [04300, 05004], lr: 0.064377, loss: 2.5244
2022-07-15 21:27:49 - train: epoch 0049, iter [04400, 05004], lr: 0.064352, loss: 2.1068
2022-07-15 21:28:24 - train: epoch 0049, iter [04500, 05004], lr: 0.064327, loss: 2.0532
2022-07-15 21:28:58 - train: epoch 0049, iter [04600, 05004], lr: 0.064302, loss: 2.2521
2022-07-15 21:29:33 - train: epoch 0049, iter [04700, 05004], lr: 0.064277, loss: 2.2769
2022-07-15 21:30:08 - train: epoch 0049, iter [04800, 05004], lr: 0.064252, loss: 1.9300
2022-07-15 21:30:41 - train: epoch 0049, iter [04900, 05004], lr: 0.064227, loss: 2.0641
2022-07-15 21:31:14 - train: epoch 0049, iter [05000, 05004], lr: 0.064202, loss: 1.9278
2022-07-15 21:31:15 - train: epoch 049, train_loss: 2.0985
2022-07-15 21:32:30 - eval: epoch: 049, acc1: 55.384%, acc5: 79.748%, test_loss: 1.9216, per_image_load_time: 1.297ms, per_image_inference_time: 0.249ms
2022-07-15 21:32:30 - until epoch: 049, best_acc1: 56.598%
2022-07-15 21:32:30 - epoch 050 lr: 0.064201
2022-07-15 21:33:10 - train: epoch 0050, iter [00100, 05004], lr: 0.064176, loss: 2.2149
2022-07-15 21:33:43 - train: epoch 0050, iter [00200, 05004], lr: 0.064151, loss: 2.1055
2022-07-15 21:34:17 - train: epoch 0050, iter [00300, 05004], lr: 0.064126, loss: 2.1446
2022-07-15 21:34:51 - train: epoch 0050, iter [00400, 05004], lr: 0.064100, loss: 1.8949
2022-07-15 21:35:25 - train: epoch 0050, iter [00500, 05004], lr: 0.064075, loss: 2.0203
2022-07-15 21:35:58 - train: epoch 0050, iter [00600, 05004], lr: 0.064050, loss: 2.2641
2022-07-15 21:36:33 - train: epoch 0050, iter [00700, 05004], lr: 0.064025, loss: 1.8495
2022-07-15 21:37:05 - train: epoch 0050, iter [00800, 05004], lr: 0.064000, loss: 1.7849
2022-07-15 21:37:38 - train: epoch 0050, iter [00900, 05004], lr: 0.063975, loss: 1.8945
2022-07-15 21:38:13 - train: epoch 0050, iter [01000, 05004], lr: 0.063950, loss: 2.2533
2022-07-15 21:38:47 - train: epoch 0050, iter [01100, 05004], lr: 0.063925, loss: 2.3069
2022-07-15 21:39:20 - train: epoch 0050, iter [01200, 05004], lr: 0.063900, loss: 2.1299
2022-07-15 21:39:54 - train: epoch 0050, iter [01300, 05004], lr: 0.063874, loss: 1.8475
2022-07-15 21:40:28 - train: epoch 0050, iter [01400, 05004], lr: 0.063849, loss: 2.1215
