2022-07-15 21:41:02 - train: epoch 0050, iter [01500, 05004], lr: 0.063824, loss: 2.0378
2022-07-15 21:41:35 - train: epoch 0050, iter [01600, 05004], lr: 0.063799, loss: 1.9917
2022-07-15 21:42:10 - train: epoch 0050, iter [01700, 05004], lr: 0.063774, loss: 2.1109
2022-07-15 21:42:43 - train: epoch 0050, iter [01800, 05004], lr: 0.063749, loss: 2.1216
2022-07-15 21:43:18 - train: epoch 0050, iter [01900, 05004], lr: 0.063724, loss: 1.9141
2022-07-15 21:43:52 - train: epoch 0050, iter [02000, 05004], lr: 0.063698, loss: 2.0662
2022-07-15 21:44:26 - train: epoch 0050, iter [02100, 05004], lr: 0.063673, loss: 1.7230
2022-07-15 21:44:59 - train: epoch 0050, iter [02200, 05004], lr: 0.063648, loss: 2.1078
2022-07-15 21:45:34 - train: epoch 0050, iter [02300, 05004], lr: 0.063623, loss: 2.0204
2022-07-15 21:46:07 - train: epoch 0050, iter [02400, 05004], lr: 0.063598, loss: 2.1203
2022-07-15 21:46:42 - train: epoch 0050, iter [02500, 05004], lr: 0.063573, loss: 2.1219
2022-07-15 21:47:16 - train: epoch 0050, iter [02600, 05004], lr: 0.063547, loss: 2.0026
2022-07-15 21:47:50 - train: epoch 0050, iter [02700, 05004], lr: 0.063522, loss: 2.1772
2022-07-15 21:48:24 - train: epoch 0050, iter [02800, 05004], lr: 0.063497, loss: 2.3681
2022-07-15 21:48:58 - train: epoch 0050, iter [02900, 05004], lr: 0.063472, loss: 2.3059
2022-07-15 21:49:32 - train: epoch 0050, iter [03000, 05004], lr: 0.063447, loss: 2.2924
2022-07-15 21:50:06 - train: epoch 0050, iter [03100, 05004], lr: 0.063421, loss: 2.1585
2022-07-15 21:50:40 - train: epoch 0050, iter [03200, 05004], lr: 0.063396, loss: 2.0696
2022-07-15 21:51:13 - train: epoch 0050, iter [03300, 05004], lr: 0.063371, loss: 1.8678
2022-07-15 21:51:47 - train: epoch 0050, iter [03400, 05004], lr: 0.063346, loss: 2.0259
2022-07-15 21:52:21 - train: epoch 0050, iter [03500, 05004], lr: 0.063321, loss: 1.9861
2022-07-15 21:52:54 - train: epoch 0050, iter [03600, 05004], lr: 0.063295, loss: 2.1698
2022-07-15 21:53:28 - train: epoch 0050, iter [03700, 05004], lr: 0.063270, loss: 2.2431
2022-07-15 21:54:02 - train: epoch 0050, iter [03800, 05004], lr: 0.063245, loss: 1.9186
2022-07-15 21:54:37 - train: epoch 0050, iter [03900, 05004], lr: 0.063220, loss: 1.9863
2022-07-15 21:55:09 - train: epoch 0050, iter [04000, 05004], lr: 0.063194, loss: 2.3239
2022-07-15 21:55:44 - train: epoch 0050, iter [04100, 05004], lr: 0.063169, loss: 2.0881
2022-07-15 21:56:17 - train: epoch 0050, iter [04200, 05004], lr: 0.063144, loss: 2.0866
2022-07-15 21:56:51 - train: epoch 0050, iter [04300, 05004], lr: 0.063119, loss: 2.0139
2022-07-15 21:57:25 - train: epoch 0050, iter [04400, 05004], lr: 0.063094, loss: 2.0109
2022-07-15 21:57:59 - train: epoch 0050, iter [04500, 05004], lr: 0.063068, loss: 2.1457
2022-07-15 21:58:33 - train: epoch 0050, iter [04600, 05004], lr: 0.063043, loss: 2.0232
2022-07-15 21:59:06 - train: epoch 0050, iter [04700, 05004], lr: 0.063018, loss: 2.1893
2022-07-15 21:59:40 - train: epoch 0050, iter [04800, 05004], lr: 0.062992, loss: 1.8483
2022-07-15 22:00:14 - train: epoch 0050, iter [04900, 05004], lr: 0.062967, loss: 1.8961
2022-07-15 22:00:47 - train: epoch 0050, iter [05000, 05004], lr: 0.062942, loss: 2.0265
2022-07-15 22:00:48 - train: epoch 050, train_loss: 2.0899
2022-07-15 22:02:02 - eval: epoch: 050, acc1: 56.966%, acc5: 80.822%, test_loss: 1.8514, per_image_load_time: 0.850ms, per_image_inference_time: 0.217ms
2022-07-15 22:02:03 - until epoch: 050, best_acc1: 56.966%
2022-07-15 22:02:03 - epoch 051 lr: 0.062941
2022-07-15 22:02:41 - train: epoch 0051, iter [00100, 05004], lr: 0.062916, loss: 2.3178
2022-07-15 22:03:15 - train: epoch 0051, iter [00200, 05004], lr: 0.062890, loss: 2.3316
2022-07-15 22:03:49 - train: epoch 0051, iter [00300, 05004], lr: 0.062865, loss: 2.0797
2022-07-15 22:04:24 - train: epoch 0051, iter [00400, 05004], lr: 0.062840, loss: 1.8829
2022-07-15 22:04:57 - train: epoch 0051, iter [00500, 05004], lr: 0.062815, loss: 2.1180
2022-07-15 22:05:32 - train: epoch 0051, iter [00600, 05004], lr: 0.062789, loss: 1.9757
2022-07-15 22:06:05 - train: epoch 0051, iter [00700, 05004], lr: 0.062764, loss: 2.1087
2022-07-15 22:06:39 - train: epoch 0051, iter [00800, 05004], lr: 0.062739, loss: 2.3168
2022-07-15 22:07:13 - train: epoch 0051, iter [00900, 05004], lr: 0.062713, loss: 1.9612
2022-07-15 22:07:47 - train: epoch 0051, iter [01000, 05004], lr: 0.062688, loss: 2.4128
2022-07-15 22:08:21 - train: epoch 0051, iter [01100, 05004], lr: 0.062663, loss: 2.2247
2022-07-15 22:08:55 - train: epoch 0051, iter [01200, 05004], lr: 0.062637, loss: 1.9690
2022-07-15 22:09:29 - train: epoch 0051, iter [01300, 05004], lr: 0.062612, loss: 1.8290
2022-07-15 22:10:03 - train: epoch 0051, iter [01400, 05004], lr: 0.062587, loss: 1.9408
2022-07-15 22:10:37 - train: epoch 0051, iter [01500, 05004], lr: 0.062562, loss: 2.0149
2022-07-15 22:11:11 - train: epoch 0051, iter [01600, 05004], lr: 0.062536, loss: 1.8300
2022-07-15 22:11:45 - train: epoch 0051, iter [01700, 05004], lr: 0.062511, loss: 2.1319
2022-07-15 22:12:19 - train: epoch 0051, iter [01800, 05004], lr: 0.062486, loss: 2.0454
2022-07-15 22:12:53 - train: epoch 0051, iter [01900, 05004], lr: 0.062460, loss: 1.8758
2022-07-15 22:13:27 - train: epoch 0051, iter [02000, 05004], lr: 0.062435, loss: 1.9844
2022-07-15 22:14:01 - train: epoch 0051, iter [02100, 05004], lr: 0.062410, loss: 2.0687
2022-07-15 22:14:36 - train: epoch 0051, iter [02200, 05004], lr: 0.062384, loss: 2.1596
2022-07-15 22:15:09 - train: epoch 0051, iter [02300, 05004], lr: 0.062359, loss: 2.2704
2022-07-15 22:15:43 - train: epoch 0051, iter [02400, 05004], lr: 0.062334, loss: 2.2319
2022-07-15 22:16:17 - train: epoch 0051, iter [02500, 05004], lr: 0.062308, loss: 2.0483
2022-07-15 22:16:52 - train: epoch 0051, iter [02600, 05004], lr: 0.062283, loss: 1.8378
2022-07-15 22:17:24 - train: epoch 0051, iter [02700, 05004], lr: 0.062257, loss: 2.1100
2022-07-15 22:17:58 - train: epoch 0051, iter [02800, 05004], lr: 0.062232, loss: 2.0557
2022-07-15 22:18:33 - train: epoch 0051, iter [02900, 05004], lr: 0.062207, loss: 2.0934
2022-07-15 22:19:07 - train: epoch 0051, iter [03000, 05004], lr: 0.062181, loss: 2.0267
2022-07-15 22:19:40 - train: epoch 0051, iter [03100, 05004], lr: 0.062156, loss: 1.9883
2022-07-15 22:20:14 - train: epoch 0051, iter [03200, 05004], lr: 0.062131, loss: 2.2319
2022-07-15 22:20:48 - train: epoch 0051, iter [03300, 05004], lr: 0.062105, loss: 2.1162
2022-07-15 22:21:22 - train: epoch 0051, iter [03400, 05004], lr: 0.062080, loss: 2.1412
2022-07-15 22:21:57 - train: epoch 0051, iter [03500, 05004], lr: 0.062054, loss: 2.0352
2022-07-15 22:22:31 - train: epoch 0051, iter [03600, 05004], lr: 0.062029, loss: 2.0392
2022-07-15 22:23:06 - train: epoch 0051, iter [03700, 05004], lr: 0.062004, loss: 2.1785
2022-07-15 22:23:40 - train: epoch 0051, iter [03800, 05004], lr: 0.061978, loss: 2.1336
2022-07-15 22:24:14 - train: epoch 0051, iter [03900, 05004], lr: 0.061953, loss: 2.1959
2022-07-15 22:24:48 - train: epoch 0051, iter [04000, 05004], lr: 0.061927, loss: 2.1112
2022-07-15 22:25:22 - train: epoch 0051, iter [04100, 05004], lr: 0.061902, loss: 2.0985
2022-07-15 22:25:55 - train: epoch 0051, iter [04200, 05004], lr: 0.061877, loss: 2.4043
2022-07-15 22:26:28 - train: epoch 0051, iter [04300, 05004], lr: 0.061851, loss: 2.0573
2022-07-15 22:27:01 - train: epoch 0051, iter [04400, 05004], lr: 0.061826, loss: 2.0585
2022-07-15 22:27:35 - train: epoch 0051, iter [04500, 05004], lr: 0.061800, loss: 1.7280
2022-07-15 22:28:09 - train: epoch 0051, iter [04600, 05004], lr: 0.061775, loss: 2.2659
2022-07-15 22:28:44 - train: epoch 0051, iter [04700, 05004], lr: 0.061750, loss: 2.1988
2022-07-15 22:29:17 - train: epoch 0051, iter [04800, 05004], lr: 0.061724, loss: 2.1203
2022-07-15 22:29:50 - train: epoch 0051, iter [04900, 05004], lr: 0.061699, loss: 2.2024
2022-07-15 22:30:22 - train: epoch 0051, iter [05000, 05004], lr: 0.061673, loss: 1.9386
2022-07-15 22:30:23 - train: epoch 051, train_loss: 2.0809
2022-07-15 22:31:36 - eval: epoch: 051, acc1: 57.214%, acc5: 80.908%, test_loss: 1.8278, per_image_load_time: 1.575ms, per_image_inference_time: 0.221ms
2022-07-15 22:31:37 - until epoch: 051, best_acc1: 57.214%
2022-07-15 22:31:37 - epoch 052 lr: 0.061672
2022-07-15 22:32:15 - train: epoch 0052, iter [00100, 05004], lr: 0.061647, loss: 1.9386
2022-07-15 22:32:50 - train: epoch 0052, iter [00200, 05004], lr: 0.061621, loss: 2.0436
2022-07-15 22:33:22 - train: epoch 0052, iter [00300, 05004], lr: 0.061596, loss: 2.1856
2022-07-15 22:33:55 - train: epoch 0052, iter [00400, 05004], lr: 0.061570, loss: 2.1259
2022-07-15 22:34:29 - train: epoch 0052, iter [00500, 05004], lr: 0.061545, loss: 2.0841
2022-07-15 22:35:02 - train: epoch 0052, iter [00600, 05004], lr: 0.061520, loss: 1.8563
2022-07-15 22:35:36 - train: epoch 0052, iter [00700, 05004], lr: 0.061494, loss: 2.0635
2022-07-15 22:36:09 - train: epoch 0052, iter [00800, 05004], lr: 0.061469, loss: 2.0647
2022-07-15 22:36:42 - train: epoch 0052, iter [00900, 05004], lr: 0.061443, loss: 2.1982
2022-07-15 22:37:15 - train: epoch 0052, iter [01000, 05004], lr: 0.061418, loss: 2.2980
2022-07-15 22:37:48 - train: epoch 0052, iter [01100, 05004], lr: 0.061392, loss: 1.9518
2022-07-15 22:38:23 - train: epoch 0052, iter [01200, 05004], lr: 0.061367, loss: 1.9590
2022-07-15 22:38:55 - train: epoch 0052, iter [01300, 05004], lr: 0.061341, loss: 1.8981
2022-07-15 22:39:28 - train: epoch 0052, iter [01400, 05004], lr: 0.061316, loss: 2.3750
2022-07-15 22:40:01 - train: epoch 0052, iter [01500, 05004], lr: 0.061290, loss: 1.9121
2022-07-15 22:40:34 - train: epoch 0052, iter [01600, 05004], lr: 0.061265, loss: 1.7855
2022-07-15 22:41:08 - train: epoch 0052, iter [01700, 05004], lr: 0.061239, loss: 1.8891
2022-07-15 22:41:41 - train: epoch 0052, iter [01800, 05004], lr: 0.061214, loss: 1.9531
2022-07-15 22:42:15 - train: epoch 0052, iter [01900, 05004], lr: 0.061188, loss: 1.9921
2022-07-15 22:42:49 - train: epoch 0052, iter [02000, 05004], lr: 0.061163, loss: 2.3274
2022-07-15 22:43:22 - train: epoch 0052, iter [02100, 05004], lr: 0.061137, loss: 1.9750
2022-07-15 22:43:56 - train: epoch 0052, iter [02200, 05004], lr: 0.061112, loss: 2.0513
2022-07-15 22:44:28 - train: epoch 0052, iter [02300, 05004], lr: 0.061086, loss: 1.8567
2022-07-15 22:45:02 - train: epoch 0052, iter [02400, 05004], lr: 0.061061, loss: 1.8927
2022-07-15 22:45:36 - train: epoch 0052, iter [02500, 05004], lr: 0.061035, loss: 1.8217
2022-07-15 22:46:11 - train: epoch 0052, iter [02600, 05004], lr: 0.061010, loss: 1.8010
2022-07-15 22:46:44 - train: epoch 0052, iter [02700, 05004], lr: 0.060984, loss: 2.0266
2022-07-15 22:47:17 - train: epoch 0052, iter [02800, 05004], lr: 0.060959, loss: 2.1535
2022-07-15 22:47:51 - train: epoch 0052, iter [02900, 05004], lr: 0.060933, loss: 1.8501
2022-07-15 22:48:24 - train: epoch 0052, iter [03000, 05004], lr: 0.060908, loss: 2.1357
2022-07-15 22:48:58 - train: epoch 0052, iter [03100, 05004], lr: 0.060882, loss: 2.2054
2022-07-15 22:49:31 - train: epoch 0052, iter [03200, 05004], lr: 0.060857, loss: 2.1960
2022-07-15 22:50:05 - train: epoch 0052, iter [03300, 05004], lr: 0.060831, loss: 2.0715
2022-07-15 22:50:37 - train: epoch 0052, iter [03400, 05004], lr: 0.060806, loss: 2.1984
2022-07-15 22:51:11 - train: epoch 0052, iter [03500, 05004], lr: 0.060780, loss: 2.1905
2022-07-15 22:51:45 - train: epoch 0052, iter [03600, 05004], lr: 0.060755, loss: 2.2001
2022-07-15 22:52:18 - train: epoch 0052, iter [03700, 05004], lr: 0.060729, loss: 2.1935
2022-07-15 22:52:52 - train: epoch 0052, iter [03800, 05004], lr: 0.060703, loss: 1.9380
2022-07-15 22:53:26 - train: epoch 0052, iter [03900, 05004], lr: 0.060678, loss: 2.0071
2022-07-15 22:53:58 - train: epoch 0052, iter [04000, 05004], lr: 0.060652, loss: 2.2991
2022-07-15 22:54:33 - train: epoch 0052, iter [04100, 05004], lr: 0.060627, loss: 1.8224
2022-07-15 22:55:07 - train: epoch 0052, iter [04200, 05004], lr: 0.060601, loss: 2.0257
2022-07-15 22:55:39 - train: epoch 0052, iter [04300, 05004], lr: 0.060576, loss: 2.1725
2022-07-15 22:56:13 - train: epoch 0052, iter [04400, 05004], lr: 0.060550, loss: 2.1323
2022-07-15 22:56:46 - train: epoch 0052, iter [04500, 05004], lr: 0.060525, loss: 1.9267
2022-07-15 22:57:20 - train: epoch 0052, iter [04600, 05004], lr: 0.060499, loss: 2.0506
2022-07-15 22:57:54 - train: epoch 0052, iter [04700, 05004], lr: 0.060473, loss: 2.0813
2022-07-15 22:58:27 - train: epoch 0052, iter [04800, 05004], lr: 0.060448, loss: 1.7988
2022-07-15 22:59:00 - train: epoch 0052, iter [04900, 05004], lr: 0.060422, loss: 1.9923
2022-07-15 22:59:32 - train: epoch 0052, iter [05000, 05004], lr: 0.060397, loss: 2.0177
2022-07-15 22:59:33 - train: epoch 052, train_loss: 2.0720
2022-07-15 23:00:47 - eval: epoch: 052, acc1: 56.870%, acc5: 80.804%, test_loss: 1.8457, per_image_load_time: 2.532ms, per_image_inference_time: 0.217ms
2022-07-15 23:00:47 - until epoch: 052, best_acc1: 57.214%
2022-07-15 23:00:47 - epoch 053 lr: 0.060395
2022-07-15 23:01:25 - train: epoch 0053, iter [00100, 05004], lr: 0.060370, loss: 2.0539
2022-07-15 23:01:58 - train: epoch 0053, iter [00200, 05004], lr: 0.060344, loss: 2.3242
2022-07-15 23:02:31 - train: epoch 0053, iter [00300, 05004], lr: 0.060319, loss: 2.0774
2022-07-15 23:03:05 - train: epoch 0053, iter [00400, 05004], lr: 0.060293, loss: 2.1159
2022-07-15 23:03:38 - train: epoch 0053, iter [00500, 05004], lr: 0.060268, loss: 2.2056
2022-07-15 23:04:11 - train: epoch 0053, iter [00600, 05004], lr: 0.060242, loss: 2.1762
2022-07-15 23:04:45 - train: epoch 0053, iter [00700, 05004], lr: 0.060216, loss: 1.9532
2022-07-15 23:05:18 - train: epoch 0053, iter [00800, 05004], lr: 0.060191, loss: 2.0661
2022-07-15 23:05:52 - train: epoch 0053, iter [00900, 05004], lr: 0.060165, loss: 1.9514
2022-07-15 23:06:25 - train: epoch 0053, iter [01000, 05004], lr: 0.060140, loss: 1.9484
2022-07-15 23:06:58 - train: epoch 0053, iter [01100, 05004], lr: 0.060114, loss: 2.0069
2022-07-15 23:07:33 - train: epoch 0053, iter [01200, 05004], lr: 0.060088, loss: 2.0655
2022-07-15 23:08:05 - train: epoch 0053, iter [01300, 05004], lr: 0.060063, loss: 1.9857
2022-07-15 23:08:40 - train: epoch 0053, iter [01400, 05004], lr: 0.060037, loss: 2.4988
2022-07-15 23:09:12 - train: epoch 0053, iter [01500, 05004], lr: 0.060011, loss: 2.1121
2022-07-15 23:09:46 - train: epoch 0053, iter [01600, 05004], lr: 0.059986, loss: 2.3683
2022-07-15 23:10:21 - train: epoch 0053, iter [01700, 05004], lr: 0.059960, loss: 2.2267
2022-07-15 23:10:55 - train: epoch 0053, iter [01800, 05004], lr: 0.059935, loss: 2.1461
2022-07-15 23:11:30 - train: epoch 0053, iter [01900, 05004], lr: 0.059909, loss: 1.9223
2022-07-15 23:12:04 - train: epoch 0053, iter [02000, 05004], lr: 0.059883, loss: 2.1829
2022-07-15 23:12:38 - train: epoch 0053, iter [02100, 05004], lr: 0.059858, loss: 2.1851
2022-07-15 23:13:13 - train: epoch 0053, iter [02200, 05004], lr: 0.059832, loss: 2.1547
2022-07-15 23:13:48 - train: epoch 0053, iter [02300, 05004], lr: 0.059806, loss: 2.1382
2022-07-15 23:14:23 - train: epoch 0053, iter [02400, 05004], lr: 0.059781, loss: 2.0211
2022-07-15 23:14:58 - train: epoch 0053, iter [02500, 05004], lr: 0.059755, loss: 2.3071
2022-07-15 23:15:32 - train: epoch 0053, iter [02600, 05004], lr: 0.059729, loss: 2.1559
2022-07-15 23:16:07 - train: epoch 0053, iter [02700, 05004], lr: 0.059704, loss: 2.2730
2022-07-15 23:16:41 - train: epoch 0053, iter [02800, 05004], lr: 0.059678, loss: 2.2559
2022-07-15 23:17:17 - train: epoch 0053, iter [02900, 05004], lr: 0.059652, loss: 1.8445
2022-07-15 23:17:52 - train: epoch 0053, iter [03000, 05004], lr: 0.059627, loss: 1.8015
2022-07-15 23:18:26 - train: epoch 0053, iter [03100, 05004], lr: 0.059601, loss: 2.2518
2022-07-15 23:19:01 - train: epoch 0053, iter [03200, 05004], lr: 0.059575, loss: 2.4622
2022-07-15 23:19:36 - train: epoch 0053, iter [03300, 05004], lr: 0.059550, loss: 1.9090
2022-07-15 23:20:11 - train: epoch 0053, iter [03400, 05004], lr: 0.059524, loss: 2.1536
2022-07-15 23:20:46 - train: epoch 0053, iter [03500, 05004], lr: 0.059498, loss: 2.0029
2022-07-15 23:21:21 - train: epoch 0053, iter [03600, 05004], lr: 0.059473, loss: 2.1072
2022-07-15 23:21:56 - train: epoch 0053, iter [03700, 05004], lr: 0.059447, loss: 2.1884
2022-07-15 23:22:30 - train: epoch 0053, iter [03800, 05004], lr: 0.059421, loss: 1.9469
2022-07-15 23:23:05 - train: epoch 0053, iter [03900, 05004], lr: 0.059396, loss: 2.3198
2022-07-15 23:23:41 - train: epoch 0053, iter [04000, 05004], lr: 0.059370, loss: 2.0179
2022-07-15 23:24:15 - train: epoch 0053, iter [04100, 05004], lr: 0.059344, loss: 2.1050
2022-07-15 23:24:50 - train: epoch 0053, iter [04200, 05004], lr: 0.059318, loss: 2.1030
2022-07-15 23:25:24 - train: epoch 0053, iter [04300, 05004], lr: 0.059293, loss: 2.2940
2022-07-15 23:25:59 - train: epoch 0053, iter [04400, 05004], lr: 0.059267, loss: 2.0300
2022-07-15 23:26:34 - train: epoch 0053, iter [04500, 05004], lr: 0.059241, loss: 2.2173
2022-07-15 23:27:09 - train: epoch 0053, iter [04600, 05004], lr: 0.059216, loss: 2.0150
2022-07-15 23:27:44 - train: epoch 0053, iter [04700, 05004], lr: 0.059190, loss: 2.2777
2022-07-15 23:28:19 - train: epoch 0053, iter [04800, 05004], lr: 0.059164, loss: 2.2554
2022-07-15 23:28:54 - train: epoch 0053, iter [04900, 05004], lr: 0.059139, loss: 2.0550
2022-07-15 23:29:26 - train: epoch 0053, iter [05000, 05004], lr: 0.059113, loss: 2.0279
2022-07-15 23:29:28 - train: epoch 053, train_loss: 2.0623
2022-07-15 23:30:44 - eval: epoch: 053, acc1: 57.300%, acc5: 81.294%, test_loss: 1.8147, per_image_load_time: 1.178ms, per_image_inference_time: 0.237ms
2022-07-15 23:30:44 - until epoch: 053, best_acc1: 57.300%
2022-07-15 23:30:44 - epoch 054 lr: 0.059112
2022-07-15 23:31:25 - train: epoch 0054, iter [00100, 05004], lr: 0.059086, loss: 1.8044
2022-07-15 23:31:59 - train: epoch 0054, iter [00200, 05004], lr: 0.059060, loss: 2.3781
2022-07-15 23:32:33 - train: epoch 0054, iter [00300, 05004], lr: 0.059035, loss: 2.1232
2022-07-15 23:33:08 - train: epoch 0054, iter [00400, 05004], lr: 0.059009, loss: 1.8381
2022-07-15 23:33:43 - train: epoch 0054, iter [00500, 05004], lr: 0.058983, loss: 2.1122
2022-07-15 23:34:16 - train: epoch 0054, iter [00600, 05004], lr: 0.058957, loss: 1.9075
2022-07-15 23:34:51 - train: epoch 0054, iter [00700, 05004], lr: 0.058932, loss: 2.1514
2022-07-15 23:35:26 - train: epoch 0054, iter [00800, 05004], lr: 0.058906, loss: 2.0901
2022-07-15 23:36:01 - train: epoch 0054, iter [00900, 05004], lr: 0.058880, loss: 1.9271
2022-07-15 23:36:35 - train: epoch 0054, iter [01000, 05004], lr: 0.058854, loss: 1.8294
2022-07-15 23:37:11 - train: epoch 0054, iter [01100, 05004], lr: 0.058829, loss: 1.9399
2022-07-15 23:37:45 - train: epoch 0054, iter [01200, 05004], lr: 0.058803, loss: 2.2407
2022-07-15 23:38:20 - train: epoch 0054, iter [01300, 05004], lr: 0.058777, loss: 1.9486
2022-07-15 23:38:54 - train: epoch 0054, iter [01400, 05004], lr: 0.058751, loss: 2.1825
2022-07-15 23:39:29 - train: epoch 0054, iter [01500, 05004], lr: 0.058726, loss: 1.9717
2022-07-15 23:40:03 - train: epoch 0054, iter [01600, 05004], lr: 0.058700, loss: 1.7511
2022-07-15 23:40:39 - train: epoch 0054, iter [01700, 05004], lr: 0.058674, loss: 2.1135
2022-07-15 23:41:13 - train: epoch 0054, iter [01800, 05004], lr: 0.058648, loss: 1.9619
2022-07-15 23:41:48 - train: epoch 0054, iter [01900, 05004], lr: 0.058623, loss: 2.4978
2022-07-15 23:42:22 - train: epoch 0054, iter [02000, 05004], lr: 0.058597, loss: 2.2281
2022-07-15 23:42:57 - train: epoch 0054, iter [02100, 05004], lr: 0.058571, loss: 1.7357
2022-07-15 23:43:32 - train: epoch 0054, iter [02200, 05004], lr: 0.058545, loss: 2.0852
2022-07-15 23:44:07 - train: epoch 0054, iter [02300, 05004], lr: 0.058520, loss: 1.9244
2022-07-15 23:44:41 - train: epoch 0054, iter [02400, 05004], lr: 0.058494, loss: 2.0184
2022-07-15 23:45:16 - train: epoch 0054, iter [02500, 05004], lr: 0.058468, loss: 2.1594
2022-07-15 23:45:51 - train: epoch 0054, iter [02600, 05004], lr: 0.058442, loss: 1.9444
2022-07-15 23:46:26 - train: epoch 0054, iter [02700, 05004], lr: 0.058416, loss: 2.1385
2022-07-15 23:47:01 - train: epoch 0054, iter [02800, 05004], lr: 0.058391, loss: 2.3629
2022-07-15 23:47:35 - train: epoch 0054, iter [02900, 05004], lr: 0.058365, loss: 1.9206
2022-07-15 23:48:10 - train: epoch 0054, iter [03000, 05004], lr: 0.058339, loss: 2.2028
2022-07-15 23:48:45 - train: epoch 0054, iter [03100, 05004], lr: 0.058313, loss: 2.0788
2022-07-15 23:49:20 - train: epoch 0054, iter [03200, 05004], lr: 0.058287, loss: 2.2905
2022-07-15 23:49:55 - train: epoch 0054, iter [03300, 05004], lr: 0.058262, loss: 1.9824
2022-07-15 23:50:30 - train: epoch 0054, iter [03400, 05004], lr: 0.058236, loss: 2.0618
2022-07-15 23:51:04 - train: epoch 0054, iter [03500, 05004], lr: 0.058210, loss: 2.1885
2022-07-15 23:51:39 - train: epoch 0054, iter [03600, 05004], lr: 0.058184, loss: 1.9661
2022-07-15 23:52:14 - train: epoch 0054, iter [03700, 05004], lr: 0.058158, loss: 1.9245
2022-07-15 23:52:48 - train: epoch 0054, iter [03800, 05004], lr: 0.058133, loss: 2.1235
2022-07-15 23:53:24 - train: epoch 0054, iter [03900, 05004], lr: 0.058107, loss: 2.0312
2022-07-15 23:53:59 - train: epoch 0054, iter [04000, 05004], lr: 0.058081, loss: 1.8910
2022-07-15 23:54:33 - train: epoch 0054, iter [04100, 05004], lr: 0.058055, loss: 2.1628
2022-07-15 23:55:08 - train: epoch 0054, iter [04200, 05004], lr: 0.058029, loss: 2.0116
2022-07-15 23:55:42 - train: epoch 0054, iter [04300, 05004], lr: 0.058004, loss: 2.1081
2022-07-15 23:56:18 - train: epoch 0054, iter [04400, 05004], lr: 0.057978, loss: 1.9050
2022-07-15 23:56:52 - train: epoch 0054, iter [04500, 05004], lr: 0.057952, loss: 2.0375
2022-07-15 23:57:27 - train: epoch 0054, iter [04600, 05004], lr: 0.057926, loss: 2.1204
2022-07-15 23:58:02 - train: epoch 0054, iter [04700, 05004], lr: 0.057900, loss: 2.4073
2022-07-15 23:58:37 - train: epoch 0054, iter [04800, 05004], lr: 0.057874, loss: 2.2134
2022-07-15 23:59:11 - train: epoch 0054, iter [04900, 05004], lr: 0.057849, loss: 1.9879
2022-07-15 23:59:45 - train: epoch 0054, iter [05000, 05004], lr: 0.057823, loss: 2.1045
2022-07-15 23:59:46 - train: epoch 054, train_loss: 2.0555
2022-07-16 00:01:02 - eval: epoch: 054, acc1: 56.360%, acc5: 80.690%, test_loss: 1.8662, per_image_load_time: 1.240ms, per_image_inference_time: 0.213ms
2022-07-16 00:01:02 - until epoch: 054, best_acc1: 57.300%
2022-07-16 00:01:02 - epoch 055 lr: 0.057821
2022-07-16 00:01:42 - train: epoch 0055, iter [00100, 05004], lr: 0.057796, loss: 1.9162
2022-07-16 00:02:17 - train: epoch 0055, iter [00200, 05004], lr: 0.057770, loss: 1.9674
2022-07-16 00:02:50 - train: epoch 0055, iter [00300, 05004], lr: 0.057744, loss: 1.8167
2022-07-16 00:03:24 - train: epoch 0055, iter [00400, 05004], lr: 0.057718, loss: 1.9902
2022-07-16 00:03:59 - train: epoch 0055, iter [00500, 05004], lr: 0.057693, loss: 1.8642
2022-07-16 00:04:33 - train: epoch 0055, iter [00600, 05004], lr: 0.057667, loss: 1.9277
2022-07-16 00:05:07 - train: epoch 0055, iter [00700, 05004], lr: 0.057641, loss: 2.1600
2022-07-16 00:05:42 - train: epoch 0055, iter [00800, 05004], lr: 0.057615, loss: 1.9219
2022-07-16 00:06:16 - train: epoch 0055, iter [00900, 05004], lr: 0.057589, loss: 2.1213
2022-07-16 00:06:51 - train: epoch 0055, iter [01000, 05004], lr: 0.057563, loss: 2.0213
2022-07-16 00:07:24 - train: epoch 0055, iter [01100, 05004], lr: 0.057537, loss: 2.1021
2022-07-16 00:07:59 - train: epoch 0055, iter [01200, 05004], lr: 0.057512, loss: 2.1358
2022-07-16 00:08:34 - train: epoch 0055, iter [01300, 05004], lr: 0.057486, loss: 2.2380
2022-07-16 00:09:09 - train: epoch 0055, iter [01400, 05004], lr: 0.057460, loss: 2.1384
2022-07-16 00:09:44 - train: epoch 0055, iter [01500, 05004], lr: 0.057434, loss: 2.0524
2022-07-16 00:10:18 - train: epoch 0055, iter [01600, 05004], lr: 0.057408, loss: 2.1727
2022-07-16 00:10:53 - train: epoch 0055, iter [01700, 05004], lr: 0.057382, loss: 2.1676
2022-07-16 00:11:28 - train: epoch 0055, iter [01800, 05004], lr: 0.057356, loss: 2.1238
2022-07-16 00:12:03 - train: epoch 0055, iter [01900, 05004], lr: 0.057330, loss: 2.1124
2022-07-16 00:12:38 - train: epoch 0055, iter [02000, 05004], lr: 0.057305, loss: 2.0017
2022-07-16 00:13:12 - train: epoch 0055, iter [02100, 05004], lr: 0.057279, loss: 1.8130
2022-07-16 00:13:47 - train: epoch 0055, iter [02200, 05004], lr: 0.057253, loss: 2.1839
2022-07-16 00:14:21 - train: epoch 0055, iter [02300, 05004], lr: 0.057227, loss: 1.9651
2022-07-16 00:14:57 - train: epoch 0055, iter [02400, 05004], lr: 0.057201, loss: 1.9312
2022-07-16 00:15:32 - train: epoch 0055, iter [02500, 05004], lr: 0.057175, loss: 2.0784
2022-07-16 00:16:06 - train: epoch 0055, iter [02600, 05004], lr: 0.057149, loss: 1.8889
2022-07-16 00:16:40 - train: epoch 0055, iter [02700, 05004], lr: 0.057123, loss: 1.9500
2022-07-16 00:17:15 - train: epoch 0055, iter [02800, 05004], lr: 0.057097, loss: 2.0053
2022-07-16 00:17:49 - train: epoch 0055, iter [02900, 05004], lr: 0.057072, loss: 2.1460
2022-07-16 00:18:24 - train: epoch 0055, iter [03000, 05004], lr: 0.057046, loss: 2.0007
2022-07-16 00:18:58 - train: epoch 0055, iter [03100, 05004], lr: 0.057020, loss: 2.0647
2022-07-16 00:19:34 - train: epoch 0055, iter [03200, 05004], lr: 0.056994, loss: 1.9056
2022-07-16 00:20:07 - train: epoch 0055, iter [03300, 05004], lr: 0.056968, loss: 1.8188
2022-07-16 00:20:43 - train: epoch 0055, iter [03400, 05004], lr: 0.056942, loss: 1.9795
2022-07-16 00:21:17 - train: epoch 0055, iter [03500, 05004], lr: 0.056916, loss: 1.8689
2022-07-16 00:21:52 - train: epoch 0055, iter [03600, 05004], lr: 0.056890, loss: 2.0608
2022-07-16 00:22:26 - train: epoch 0055, iter [03700, 05004], lr: 0.056864, loss: 2.1109
2022-07-16 00:23:01 - train: epoch 0055, iter [03800, 05004], lr: 0.056838, loss: 2.0497
2022-07-16 00:23:36 - train: epoch 0055, iter [03900, 05004], lr: 0.056813, loss: 2.5491
2022-07-16 00:24:11 - train: epoch 0055, iter [04000, 05004], lr: 0.056787, loss: 1.9954
2022-07-16 00:24:45 - train: epoch 0055, iter [04100, 05004], lr: 0.056761, loss: 2.0320
2022-07-16 00:25:20 - train: epoch 0055, iter [04200, 05004], lr: 0.056735, loss: 2.0967
2022-07-16 00:25:54 - train: epoch 0055, iter [04300, 05004], lr: 0.056709, loss: 2.1805
2022-07-16 00:26:30 - train: epoch 0055, iter [04400, 05004], lr: 0.056683, loss: 2.1656
2022-07-16 00:27:04 - train: epoch 0055, iter [04500, 05004], lr: 0.056657, loss: 2.0823
2022-07-16 00:27:38 - train: epoch 0055, iter [04600, 05004], lr: 0.056631, loss: 2.1132
2022-07-16 00:28:13 - train: epoch 0055, iter [04700, 05004], lr: 0.056605, loss: 1.9564
2022-07-16 00:28:47 - train: epoch 0055, iter [04800, 05004], lr: 0.056579, loss: 2.1232
2022-07-16 00:29:23 - train: epoch 0055, iter [04900, 05004], lr: 0.056553, loss: 2.0326
2022-07-16 00:29:56 - train: epoch 0055, iter [05000, 05004], lr: 0.056527, loss: 2.1510
2022-07-16 00:29:57 - train: epoch 055, train_loss: 2.0435
2022-07-16 00:31:14 - eval: epoch: 055, acc1: 58.020%, acc5: 81.676%, test_loss: 1.7781, per_image_load_time: 2.053ms, per_image_inference_time: 0.237ms
2022-07-16 00:31:14 - until epoch: 055, best_acc1: 58.020%
2022-07-16 00:31:14 - epoch 056 lr: 0.056526
2022-07-16 00:31:54 - train: epoch 0056, iter [00100, 05004], lr: 0.056500, loss: 2.0891
2022-07-16 00:32:28 - train: epoch 0056, iter [00200, 05004], lr: 0.056474, loss: 2.2492
2022-07-16 00:33:02 - train: epoch 0056, iter [00300, 05004], lr: 0.056448, loss: 2.0702
2022-07-16 00:33:37 - train: epoch 0056, iter [00400, 05004], lr: 0.056423, loss: 1.9151
2022-07-16 00:34:12 - train: epoch 0056, iter [00500, 05004], lr: 0.056397, loss: 1.9954
2022-07-16 00:34:46 - train: epoch 0056, iter [00600, 05004], lr: 0.056371, loss: 1.9228
2022-07-16 00:35:21 - train: epoch 0056, iter [00700, 05004], lr: 0.056345, loss: 2.0484
2022-07-16 00:35:54 - train: epoch 0056, iter [00800, 05004], lr: 0.056319, loss: 2.1947
2022-07-16 00:36:30 - train: epoch 0056, iter [00900, 05004], lr: 0.056293, loss: 2.0822
2022-07-16 00:37:04 - train: epoch 0056, iter [01000, 05004], lr: 0.056267, loss: 2.0184
2022-07-16 00:37:38 - train: epoch 0056, iter [01100, 05004], lr: 0.056241, loss: 1.8683
2022-07-16 00:38:14 - train: epoch 0056, iter [01200, 05004], lr: 0.056215, loss: 1.9781
2022-07-16 00:38:49 - train: epoch 0056, iter [01300, 05004], lr: 0.056189, loss: 2.1148
2022-07-16 00:39:23 - train: epoch 0056, iter [01400, 05004], lr: 0.056163, loss: 1.9300
2022-07-16 00:39:58 - train: epoch 0056, iter [01500, 05004], lr: 0.056137, loss: 2.4957
2022-07-16 00:40:32 - train: epoch 0056, iter [01600, 05004], lr: 0.056111, loss: 1.8278
2022-07-16 00:41:06 - train: epoch 0056, iter [01700, 05004], lr: 0.056085, loss: 2.1295
2022-07-16 00:41:40 - train: epoch 0056, iter [01800, 05004], lr: 0.056059, loss: 2.3147
2022-07-16 00:42:14 - train: epoch 0056, iter [01900, 05004], lr: 0.056033, loss: 2.1052
2022-07-16 00:42:48 - train: epoch 0056, iter [02000, 05004], lr: 0.056007, loss: 2.1027
2022-07-16 00:43:22 - train: epoch 0056, iter [02100, 05004], lr: 0.055981, loss: 2.0909
2022-07-16 00:43:56 - train: epoch 0056, iter [02200, 05004], lr: 0.055955, loss: 2.2266
2022-07-16 00:44:31 - train: epoch 0056, iter [02300, 05004], lr: 0.055929, loss: 2.1394
2022-07-16 00:45:04 - train: epoch 0056, iter [02400, 05004], lr: 0.055903, loss: 1.9025
2022-07-16 00:45:39 - train: epoch 0056, iter [02500, 05004], lr: 0.055877, loss: 2.3037
2022-07-16 00:46:13 - train: epoch 0056, iter [02600, 05004], lr: 0.055851, loss: 2.0435
2022-07-16 00:46:47 - train: epoch 0056, iter [02700, 05004], lr: 0.055825, loss: 2.0351
2022-07-16 00:47:20 - train: epoch 0056, iter [02800, 05004], lr: 0.055799, loss: 1.8873
2022-07-16 00:47:55 - train: epoch 0056, iter [02900, 05004], lr: 0.055773, loss: 2.2617
2022-07-16 00:48:29 - train: epoch 0056, iter [03000, 05004], lr: 0.055747, loss: 2.1578
2022-07-16 00:49:04 - train: epoch 0056, iter [03100, 05004], lr: 0.055721, loss: 1.8824
2022-07-16 00:49:37 - train: epoch 0056, iter [03200, 05004], lr: 0.055696, loss: 1.7783
2022-07-16 00:50:12 - train: epoch 0056, iter [03300, 05004], lr: 0.055670, loss: 2.3284
2022-07-16 00:50:46 - train: epoch 0056, iter [03400, 05004], lr: 0.055644, loss: 2.0895
2022-07-16 00:51:20 - train: epoch 0056, iter [03500, 05004], lr: 0.055618, loss: 1.9174
2022-07-16 00:51:54 - train: epoch 0056, iter [03600, 05004], lr: 0.055592, loss: 1.7274
2022-07-16 00:52:29 - train: epoch 0056, iter [03700, 05004], lr: 0.055566, loss: 1.9225
2022-07-16 00:53:02 - train: epoch 0056, iter [03800, 05004], lr: 0.055540, loss: 1.8621
2022-07-16 00:53:36 - train: epoch 0056, iter [03900, 05004], lr: 0.055514, loss: 2.2728
2022-07-16 00:54:10 - train: epoch 0056, iter [04000, 05004], lr: 0.055488, loss: 2.1315
2022-07-16 00:54:45 - train: epoch 0056, iter [04100, 05004], lr: 0.055462, loss: 2.2112
2022-07-16 00:55:19 - train: epoch 0056, iter [04200, 05004], lr: 0.055436, loss: 1.9716
2022-07-16 00:55:54 - train: epoch 0056, iter [04300, 05004], lr: 0.055410, loss: 1.8736
2022-07-16 00:56:27 - train: epoch 0056, iter [04400, 05004], lr: 0.055384, loss: 2.1626
2022-07-16 00:57:02 - train: epoch 0056, iter [04500, 05004], lr: 0.055358, loss: 2.0659
2022-07-16 00:57:36 - train: epoch 0056, iter [04600, 05004], lr: 0.055332, loss: 2.1171
2022-07-16 00:58:11 - train: epoch 0056, iter [04700, 05004], lr: 0.055306, loss: 2.0755
2022-07-16 00:58:44 - train: epoch 0056, iter [04800, 05004], lr: 0.055279, loss: 2.2615
2022-07-16 00:59:19 - train: epoch 0056, iter [04900, 05004], lr: 0.055253, loss: 2.0827
2022-07-16 00:59:51 - train: epoch 0056, iter [05000, 05004], lr: 0.055227, loss: 2.1607
2022-07-16 00:59:52 - train: epoch 056, train_loss: 2.0323
2022-07-16 01:01:07 - eval: epoch: 056, acc1: 58.144%, acc5: 81.688%, test_loss: 1.8024, per_image_load_time: 2.719ms, per_image_inference_time: 0.227ms
2022-07-16 01:01:07 - until epoch: 056, best_acc1: 58.144%
2022-07-16 01:01:07 - epoch 057 lr: 0.055226
2022-07-16 01:01:47 - train: epoch 0057, iter [00100, 05004], lr: 0.055200, loss: 2.1132
2022-07-16 01:02:21 - train: epoch 0057, iter [00200, 05004], lr: 0.055174, loss: 1.9910
2022-07-16 01:02:55 - train: epoch 0057, iter [00300, 05004], lr: 0.055148, loss: 1.8199
2022-07-16 01:03:29 - train: epoch 0057, iter [00400, 05004], lr: 0.055122, loss: 2.0579
2022-07-16 01:04:03 - train: epoch 0057, iter [00500, 05004], lr: 0.055096, loss: 1.7492
2022-07-16 01:04:37 - train: epoch 0057, iter [00600, 05004], lr: 0.055070, loss: 2.1455
2022-07-16 01:05:12 - train: epoch 0057, iter [00700, 05004], lr: 0.055044, loss: 1.7338
2022-07-16 01:05:46 - train: epoch 0057, iter [00800, 05004], lr: 0.055018, loss: 2.0308
2022-07-16 01:06:19 - train: epoch 0057, iter [00900, 05004], lr: 0.054992, loss: 2.1223
2022-07-16 01:06:54 - train: epoch 0057, iter [01000, 05004], lr: 0.054966, loss: 1.9893
2022-07-16 01:07:28 - train: epoch 0057, iter [01100, 05004], lr: 0.054940, loss: 1.8643
2022-07-16 01:08:01 - train: epoch 0057, iter [01200, 05004], lr: 0.054914, loss: 1.9478
2022-07-16 01:08:36 - train: epoch 0057, iter [01300, 05004], lr: 0.054888, loss: 2.1071
2022-07-16 01:09:09 - train: epoch 0057, iter [01400, 05004], lr: 0.054862, loss: 2.1324
2022-07-16 01:09:43 - train: epoch 0057, iter [01500, 05004], lr: 0.054836, loss: 2.2058
2022-07-16 01:10:17 - train: epoch 0057, iter [01600, 05004], lr: 0.054810, loss: 2.1691
2022-07-16 01:10:51 - train: epoch 0057, iter [01700, 05004], lr: 0.054784, loss: 2.1616
2022-07-16 01:11:25 - train: epoch 0057, iter [01800, 05004], lr: 0.054758, loss: 2.0898
2022-07-16 01:11:58 - train: epoch 0057, iter [01900, 05004], lr: 0.054732, loss: 1.9345
2022-07-16 01:12:34 - train: epoch 0057, iter [02000, 05004], lr: 0.054706, loss: 2.0250
2022-07-16 01:13:07 - train: epoch 0057, iter [02100, 05004], lr: 0.054680, loss: 2.1324
2022-07-16 01:13:41 - train: epoch 0057, iter [02200, 05004], lr: 0.054654, loss: 2.1568
2022-07-16 01:14:16 - train: epoch 0057, iter [02300, 05004], lr: 0.054628, loss: 1.9363
2022-07-16 01:14:50 - train: epoch 0057, iter [02400, 05004], lr: 0.054602, loss: 1.9265
2022-07-16 01:15:24 - train: epoch 0057, iter [02500, 05004], lr: 0.054576, loss: 2.1200
2022-07-16 01:15:58 - train: epoch 0057, iter [02600, 05004], lr: 0.054550, loss: 1.7749
2022-07-16 01:16:32 - train: epoch 0057, iter [02700, 05004], lr: 0.054524, loss: 1.7448
2022-07-16 01:17:06 - train: epoch 0057, iter [02800, 05004], lr: 0.054497, loss: 1.6341
2022-07-16 01:17:40 - train: epoch 0057, iter [02900, 05004], lr: 0.054471, loss: 2.1327
2022-07-16 01:18:14 - train: epoch 0057, iter [03000, 05004], lr: 0.054445, loss: 2.1139
2022-07-16 01:18:50 - train: epoch 0057, iter [03100, 05004], lr: 0.054419, loss: 2.2417
2022-07-16 01:19:23 - train: epoch 0057, iter [03200, 05004], lr: 0.054393, loss: 2.2067
2022-07-16 01:19:58 - train: epoch 0057, iter [03300, 05004], lr: 0.054367, loss: 1.9397
2022-07-16 01:20:31 - train: epoch 0057, iter [03400, 05004], lr: 0.054341, loss: 2.0230
2022-07-16 01:21:07 - train: epoch 0057, iter [03500, 05004], lr: 0.054315, loss: 2.0925
2022-07-16 01:21:40 - train: epoch 0057, iter [03600, 05004], lr: 0.054289, loss: 1.8049
2022-07-16 01:22:15 - train: epoch 0057, iter [03700, 05004], lr: 0.054263, loss: 1.9114
2022-07-16 01:22:48 - train: epoch 0057, iter [03800, 05004], lr: 0.054237, loss: 2.0219
2022-07-16 01:23:23 - train: epoch 0057, iter [03900, 05004], lr: 0.054211, loss: 2.1529
2022-07-16 01:23:57 - train: epoch 0057, iter [04000, 05004], lr: 0.054185, loss: 1.9997
2022-07-16 01:24:31 - train: epoch 0057, iter [04100, 05004], lr: 0.054159, loss: 2.1362
2022-07-16 01:25:05 - train: epoch 0057, iter [04200, 05004], lr: 0.054133, loss: 2.1836
2022-07-16 01:25:40 - train: epoch 0057, iter [04300, 05004], lr: 0.054107, loss: 1.9139
2022-07-16 01:26:14 - train: epoch 0057, iter [04400, 05004], lr: 0.054080, loss: 1.9558
2022-07-16 01:26:49 - train: epoch 0057, iter [04500, 05004], lr: 0.054054, loss: 2.2304
2022-07-16 01:27:23 - train: epoch 0057, iter [04600, 05004], lr: 0.054028, loss: 2.0458
2022-07-16 01:27:57 - train: epoch 0057, iter [04700, 05004], lr: 0.054002, loss: 1.9767
2022-07-16 01:28:32 - train: epoch 0057, iter [04800, 05004], lr: 0.053976, loss: 2.2858
2022-07-16 01:29:05 - train: epoch 0057, iter [04900, 05004], lr: 0.053950, loss: 2.2131
2022-07-16 01:29:38 - train: epoch 0057, iter [05000, 05004], lr: 0.053924, loss: 2.0605
2022-07-16 01:29:39 - train: epoch 057, train_loss: 2.0234
2022-07-16 01:30:54 - eval: epoch: 057, acc1: 58.574%, acc5: 82.168%, test_loss: 1.7537, per_image_load_time: 2.555ms, per_image_inference_time: 0.222ms
2022-07-16 01:30:54 - until epoch: 057, best_acc1: 58.574%
2022-07-16 01:30:54 - epoch 058 lr: 0.053923
2022-07-16 01:31:34 - train: epoch 0058, iter [00100, 05004], lr: 0.053897, loss: 2.0499
2022-07-16 01:32:08 - train: epoch 0058, iter [00200, 05004], lr: 0.053871, loss: 1.8566
2022-07-16 01:32:42 - train: epoch 0058, iter [00300, 05004], lr: 0.053845, loss: 2.0883
2022-07-16 01:33:16 - train: epoch 0058, iter [00400, 05004], lr: 0.053819, loss: 2.0554
2022-07-16 01:33:50 - train: epoch 0058, iter [00500, 05004], lr: 0.053793, loss: 1.8413
2022-07-16 01:34:24 - train: epoch 0058, iter [00600, 05004], lr: 0.053766, loss: 2.0425
2022-07-16 01:34:58 - train: epoch 0058, iter [00700, 05004], lr: 0.053740, loss: 2.0005
2022-07-16 01:35:32 - train: epoch 0058, iter [00800, 05004], lr: 0.053714, loss: 1.8337
2022-07-16 01:36:06 - train: epoch 0058, iter [00900, 05004], lr: 0.053688, loss: 1.8377
2022-07-16 01:36:40 - train: epoch 0058, iter [01000, 05004], lr: 0.053662, loss: 2.1159
2022-07-16 01:37:16 - train: epoch 0058, iter [01100, 05004], lr: 0.053636, loss: 1.7565
2022-07-16 01:37:49 - train: epoch 0058, iter [01200, 05004], lr: 0.053610, loss: 2.0064
2022-07-16 01:38:23 - train: epoch 0058, iter [01300, 05004], lr: 0.053584, loss: 2.2055
2022-07-16 01:38:58 - train: epoch 0058, iter [01400, 05004], lr: 0.053558, loss: 2.1058
2022-07-16 01:39:31 - train: epoch 0058, iter [01500, 05004], lr: 0.053532, loss: 1.8643
2022-07-16 01:40:06 - train: epoch 0058, iter [01600, 05004], lr: 0.053506, loss: 1.9611
2022-07-16 01:40:40 - train: epoch 0058, iter [01700, 05004], lr: 0.053479, loss: 2.2225
2022-07-16 01:41:14 - train: epoch 0058, iter [01800, 05004], lr: 0.053453, loss: 2.0462
2022-07-16 01:41:48 - train: epoch 0058, iter [01900, 05004], lr: 0.053427, loss: 2.2305
2022-07-16 01:42:23 - train: epoch 0058, iter [02000, 05004], lr: 0.053401, loss: 2.1891
2022-07-16 01:42:57 - train: epoch 0058, iter [02100, 05004], lr: 0.053375, loss: 1.7806
2022-07-16 01:43:31 - train: epoch 0058, iter [02200, 05004], lr: 0.053349, loss: 1.8403
2022-07-16 01:44:04 - train: epoch 0058, iter [02300, 05004], lr: 0.053323, loss: 2.0278
2022-07-16 01:44:39 - train: epoch 0058, iter [02400, 05004], lr: 0.053297, loss: 2.0895
2022-07-16 01:45:13 - train: epoch 0058, iter [02500, 05004], lr: 0.053271, loss: 2.1584
2022-07-16 01:45:47 - train: epoch 0058, iter [02600, 05004], lr: 0.053245, loss: 1.8387
2022-07-16 01:46:22 - train: epoch 0058, iter [02700, 05004], lr: 0.053218, loss: 2.2326
2022-07-16 01:46:56 - train: epoch 0058, iter [02800, 05004], lr: 0.053192, loss: 1.7899
2022-07-16 01:47:31 - train: epoch 0058, iter [02900, 05004], lr: 0.053166, loss: 1.8573
2022-07-16 01:48:05 - train: epoch 0058, iter [03000, 05004], lr: 0.053140, loss: 2.1797
2022-07-16 01:48:39 - train: epoch 0058, iter [03100, 05004], lr: 0.053114, loss: 1.9076
2022-07-16 01:49:13 - train: epoch 0058, iter [03200, 05004], lr: 0.053088, loss: 1.7371
2022-07-16 01:49:47 - train: epoch 0058, iter [03300, 05004], lr: 0.053062, loss: 1.9998
2022-07-16 01:50:22 - train: epoch 0058, iter [03400, 05004], lr: 0.053036, loss: 1.8818
2022-07-16 01:50:56 - train: epoch 0058, iter [03500, 05004], lr: 0.053010, loss: 1.8874
2022-07-16 01:51:30 - train: epoch 0058, iter [03600, 05004], lr: 0.052983, loss: 1.9031
2022-07-16 01:52:04 - train: epoch 0058, iter [03700, 05004], lr: 0.052957, loss: 2.0352
2022-07-16 01:52:38 - train: epoch 0058, iter [03800, 05004], lr: 0.052931, loss: 2.0819
2022-07-16 01:53:13 - train: epoch 0058, iter [03900, 05004], lr: 0.052905, loss: 2.0788
2022-07-16 01:53:47 - train: epoch 0058, iter [04000, 05004], lr: 0.052879, loss: 2.1979
2022-07-16 01:54:22 - train: epoch 0058, iter [04100, 05004], lr: 0.052853, loss: 2.1037
2022-07-16 01:54:55 - train: epoch 0058, iter [04200, 05004], lr: 0.052827, loss: 1.6870
2022-07-16 01:55:30 - train: epoch 0058, iter [04300, 05004], lr: 0.052801, loss: 2.3018
2022-07-16 01:56:04 - train: epoch 0058, iter [04400, 05004], lr: 0.052775, loss: 1.8267
2022-07-16 01:56:38 - train: epoch 0058, iter [04500, 05004], lr: 0.052748, loss: 2.0992
2022-07-16 01:57:12 - train: epoch 0058, iter [04600, 05004], lr: 0.052722, loss: 1.8533
2022-07-16 01:57:46 - train: epoch 0058, iter [04700, 05004], lr: 0.052696, loss: 2.0610
2022-07-16 01:58:20 - train: epoch 0058, iter [04800, 05004], lr: 0.052670, loss: 2.1008
2022-07-16 01:58:55 - train: epoch 0058, iter [04900, 05004], lr: 0.052644, loss: 1.8687
2022-07-16 01:59:27 - train: epoch 0058, iter [05000, 05004], lr: 0.052618, loss: 1.9335
2022-07-16 01:59:28 - train: epoch 058, train_loss: 2.0113
2022-07-16 02:00:44 - eval: epoch: 058, acc1: 58.480%, acc5: 81.984%, test_loss: 1.7723, per_image_load_time: 2.643ms, per_image_inference_time: 0.222ms
2022-07-16 02:00:44 - until epoch: 058, best_acc1: 58.574%
2022-07-16 02:00:44 - epoch 059 lr: 0.052617
2022-07-16 02:01:24 - train: epoch 0059, iter [00100, 05004], lr: 0.052591, loss: 2.1803
2022-07-16 02:01:58 - train: epoch 0059, iter [00200, 05004], lr: 0.052565, loss: 1.8979
2022-07-16 02:02:31 - train: epoch 0059, iter [00300, 05004], lr: 0.052538, loss: 2.0935
2022-07-16 02:03:04 - train: epoch 0059, iter [00400, 05004], lr: 0.052512, loss: 2.1625
2022-07-16 02:03:39 - train: epoch 0059, iter [00500, 05004], lr: 0.052486, loss: 2.2536
2022-07-16 02:04:12 - train: epoch 0059, iter [00600, 05004], lr: 0.052460, loss: 1.9852
2022-07-16 02:04:46 - train: epoch 0059, iter [00700, 05004], lr: 0.052434, loss: 1.8917
2022-07-16 02:05:20 - train: epoch 0059, iter [00800, 05004], lr: 0.052408, loss: 1.9950
2022-07-16 02:05:54 - train: epoch 0059, iter [00900, 05004], lr: 0.052382, loss: 1.8641
2022-07-16 02:06:28 - train: epoch 0059, iter [01000, 05004], lr: 0.052356, loss: 2.1153
2022-07-16 02:07:02 - train: epoch 0059, iter [01100, 05004], lr: 0.052329, loss: 2.2575
2022-07-16 02:07:36 - train: epoch 0059, iter [01200, 05004], lr: 0.052303, loss: 1.6726
2022-07-16 02:08:09 - train: epoch 0059, iter [01300, 05004], lr: 0.052277, loss: 2.1975
2022-07-16 02:08:44 - train: epoch 0059, iter [01400, 05004], lr: 0.052251, loss: 2.1012
2022-07-16 02:09:17 - train: epoch 0059, iter [01500, 05004], lr: 0.052225, loss: 2.0103
2022-07-16 02:09:52 - train: epoch 0059, iter [01600, 05004], lr: 0.052199, loss: 1.8118
2022-07-16 02:10:25 - train: epoch 0059, iter [01700, 05004], lr: 0.052173, loss: 2.0500
2022-07-16 02:10:59 - train: epoch 0059, iter [01800, 05004], lr: 0.052146, loss: 1.9074
2022-07-16 02:11:33 - train: epoch 0059, iter [01900, 05004], lr: 0.052120, loss: 2.0105
2022-07-16 02:12:08 - train: epoch 0059, iter [02000, 05004], lr: 0.052094, loss: 1.9231
2022-07-16 02:12:41 - train: epoch 0059, iter [02100, 05004], lr: 0.052068, loss: 2.2298
2022-07-16 02:13:15 - train: epoch 0059, iter [02200, 05004], lr: 0.052042, loss: 2.1834
2022-07-16 02:13:49 - train: epoch 0059, iter [02300, 05004], lr: 0.052016, loss: 1.8661
2022-07-16 02:14:23 - train: epoch 0059, iter [02400, 05004], lr: 0.051990, loss: 2.0666
2022-07-16 02:14:57 - train: epoch 0059, iter [02500, 05004], lr: 0.051964, loss: 1.9958
2022-07-16 02:15:32 - train: epoch 0059, iter [02600, 05004], lr: 0.051937, loss: 1.8716
2022-07-16 02:16:05 - train: epoch 0059, iter [02700, 05004], lr: 0.051911, loss: 2.0172
2022-07-16 02:16:39 - train: epoch 0059, iter [02800, 05004], lr: 0.051885, loss: 2.2209
2022-07-16 02:17:13 - train: epoch 0059, iter [02900, 05004], lr: 0.051859, loss: 2.0801
2022-07-16 02:17:48 - train: epoch 0059, iter [03000, 05004], lr: 0.051833, loss: 2.4665
2022-07-16 02:18:21 - train: epoch 0059, iter [03100, 05004], lr: 0.051807, loss: 1.9056
2022-07-16 02:18:55 - train: epoch 0059, iter [03200, 05004], lr: 0.051781, loss: 2.0828
2022-07-16 02:19:29 - train: epoch 0059, iter [03300, 05004], lr: 0.051754, loss: 2.0405
2022-07-16 02:20:02 - train: epoch 0059, iter [03400, 05004], lr: 0.051728, loss: 2.4183
2022-07-16 02:20:35 - train: epoch 0059, iter [03500, 05004], lr: 0.051702, loss: 1.9598
2022-07-16 02:21:09 - train: epoch 0059, iter [03600, 05004], lr: 0.051676, loss: 1.9478
2022-07-16 02:21:44 - train: epoch 0059, iter [03700, 05004], lr: 0.051650, loss: 1.8368
2022-07-16 02:22:16 - train: epoch 0059, iter [03800, 05004], lr: 0.051624, loss: 1.9366
2022-07-16 02:22:50 - train: epoch 0059, iter [03900, 05004], lr: 0.051598, loss: 1.9526
2022-07-16 02:23:24 - train: epoch 0059, iter [04000, 05004], lr: 0.051571, loss: 2.2688
2022-07-16 02:23:57 - train: epoch 0059, iter [04100, 05004], lr: 0.051545, loss: 2.0615
2022-07-16 02:24:31 - train: epoch 0059, iter [04200, 05004], lr: 0.051519, loss: 2.0712
2022-07-16 02:25:04 - train: epoch 0059, iter [04300, 05004], lr: 0.051493, loss: 2.2894
2022-07-16 02:25:38 - train: epoch 0059, iter [04400, 05004], lr: 0.051467, loss: 2.1469
2022-07-16 02:26:12 - train: epoch 0059, iter [04500, 05004], lr: 0.051441, loss: 2.0717
2022-07-16 02:26:45 - train: epoch 0059, iter [04600, 05004], lr: 0.051414, loss: 2.0610
2022-07-16 02:27:19 - train: epoch 0059, iter [04700, 05004], lr: 0.051388, loss: 1.8748
2022-07-16 02:27:52 - train: epoch 0059, iter [04800, 05004], lr: 0.051362, loss: 1.8816
2022-07-16 02:28:26 - train: epoch 0059, iter [04900, 05004], lr: 0.051336, loss: 2.2820
2022-07-16 02:28:58 - train: epoch 0059, iter [05000, 05004], lr: 0.051310, loss: 2.1119
2022-07-16 02:28:59 - train: epoch 059, train_loss: 2.0026
2022-07-16 02:30:14 - eval: epoch: 059, acc1: 57.850%, acc5: 81.572%, test_loss: 1.7943, per_image_load_time: 2.692ms, per_image_inference_time: 0.226ms
2022-07-16 02:30:15 - until epoch: 059, best_acc1: 58.574%
2022-07-16 02:30:15 - epoch 060 lr: 0.051309
2022-07-16 02:30:54 - train: epoch 0060, iter [00100, 05004], lr: 0.051283, loss: 1.8704
2022-07-16 02:31:27 - train: epoch 0060, iter [00200, 05004], lr: 0.051257, loss: 2.1401
2022-07-16 02:32:00 - train: epoch 0060, iter [00300, 05004], lr: 0.051230, loss: 1.9950
2022-07-16 02:32:33 - train: epoch 0060, iter [00400, 05004], lr: 0.051204, loss: 1.9890
2022-07-16 02:33:07 - train: epoch 0060, iter [00500, 05004], lr: 0.051178, loss: 2.1073
2022-07-16 02:33:41 - train: epoch 0060, iter [00600, 05004], lr: 0.051152, loss: 2.1025
2022-07-16 02:34:13 - train: epoch 0060, iter [00700, 05004], lr: 0.051126, loss: 2.0049
2022-07-16 02:34:46 - train: epoch 0060, iter [00800, 05004], lr: 0.051100, loss: 2.2245
2022-07-16 02:35:20 - train: epoch 0060, iter [00900, 05004], lr: 0.051073, loss: 1.8209
2022-07-16 02:35:54 - train: epoch 0060, iter [01000, 05004], lr: 0.051047, loss: 1.6306
2022-07-16 02:36:27 - train: epoch 0060, iter [01100, 05004], lr: 0.051021, loss: 1.6980
2022-07-16 02:37:01 - train: epoch 0060, iter [01200, 05004], lr: 0.050995, loss: 1.8955
2022-07-16 02:37:35 - train: epoch 0060, iter [01300, 05004], lr: 0.050969, loss: 1.9494
2022-07-16 02:38:08 - train: epoch 0060, iter [01400, 05004], lr: 0.050943, loss: 1.9722
2022-07-16 02:38:41 - train: epoch 0060, iter [01500, 05004], lr: 0.050917, loss: 2.0397
2022-07-16 02:39:15 - train: epoch 0060, iter [01600, 05004], lr: 0.050890, loss: 2.0134
2022-07-16 02:39:48 - train: epoch 0060, iter [01700, 05004], lr: 0.050864, loss: 1.9772
2022-07-16 02:40:23 - train: epoch 0060, iter [01800, 05004], lr: 0.050838, loss: 1.9865
2022-07-16 02:40:56 - train: epoch 0060, iter [01900, 05004], lr: 0.050812, loss: 2.1507
2022-07-16 02:41:30 - train: epoch 0060, iter [02000, 05004], lr: 0.050786, loss: 1.8826
2022-07-16 02:42:03 - train: epoch 0060, iter [02100, 05004], lr: 0.050760, loss: 2.0993
2022-07-16 02:42:37 - train: epoch 0060, iter [02200, 05004], lr: 0.050733, loss: 2.1494
2022-07-16 02:43:10 - train: epoch 0060, iter [02300, 05004], lr: 0.050707, loss: 1.5646
2022-07-16 02:43:44 - train: epoch 0060, iter [02400, 05004], lr: 0.050681, loss: 1.9244
2022-07-16 02:44:18 - train: epoch 0060, iter [02500, 05004], lr: 0.050655, loss: 2.0022
2022-07-16 02:44:51 - train: epoch 0060, iter [02600, 05004], lr: 0.050629, loss: 2.1042
2022-07-16 02:45:24 - train: epoch 0060, iter [02700, 05004], lr: 0.050603, loss: 1.9811
2022-07-16 02:45:59 - train: epoch 0060, iter [02800, 05004], lr: 0.050577, loss: 1.9148
2022-07-16 02:46:32 - train: epoch 0060, iter [02900, 05004], lr: 0.050550, loss: 2.1234
2022-07-16 02:47:06 - train: epoch 0060, iter [03000, 05004], lr: 0.050524, loss: 2.2924
2022-07-16 02:47:39 - train: epoch 0060, iter [03100, 05004], lr: 0.050498, loss: 2.1013
2022-07-16 02:48:14 - train: epoch 0060, iter [03200, 05004], lr: 0.050472, loss: 2.0195
2022-07-16 02:48:46 - train: epoch 0060, iter [03300, 05004], lr: 0.050446, loss: 1.7053
2022-07-16 02:49:21 - train: epoch 0060, iter [03400, 05004], lr: 0.050420, loss: 2.0173
2022-07-16 02:49:54 - train: epoch 0060, iter [03500, 05004], lr: 0.050393, loss: 2.1489
2022-07-16 02:50:28 - train: epoch 0060, iter [03600, 05004], lr: 0.050367, loss: 2.0870
2022-07-16 02:51:01 - train: epoch 0060, iter [03700, 05004], lr: 0.050341, loss: 1.9520
2022-07-16 02:51:35 - train: epoch 0060, iter [03800, 05004], lr: 0.050315, loss: 2.0696
2022-07-16 02:52:09 - train: epoch 0060, iter [03900, 05004], lr: 0.050289, loss: 2.2728
2022-07-16 02:52:42 - train: epoch 0060, iter [04000, 05004], lr: 0.050263, loss: 1.9130
2022-07-16 02:53:16 - train: epoch 0060, iter [04100, 05004], lr: 0.050236, loss: 2.1094
2022-07-16 02:53:49 - train: epoch 0060, iter [04200, 05004], lr: 0.050210, loss: 2.1562
2022-07-16 02:54:24 - train: epoch 0060, iter [04300, 05004], lr: 0.050184, loss: 1.9542
2022-07-16 02:54:58 - train: epoch 0060, iter [04400, 05004], lr: 0.050158, loss: 2.1648
2022-07-16 02:55:31 - train: epoch 0060, iter [04500, 05004], lr: 0.050132, loss: 1.9759
2022-07-16 02:56:05 - train: epoch 0060, iter [04600, 05004], lr: 0.050106, loss: 1.8731
2022-07-16 02:56:38 - train: epoch 0060, iter [04700, 05004], lr: 0.050080, loss: 2.1484
2022-07-16 02:57:11 - train: epoch 0060, iter [04800, 05004], lr: 0.050053, loss: 1.8326
2022-07-16 02:57:45 - train: epoch 0060, iter [04900, 05004], lr: 0.050027, loss: 2.0144
2022-07-16 02:58:17 - train: epoch 0060, iter [05000, 05004], lr: 0.050001, loss: 2.0360
2022-07-16 02:58:18 - train: epoch 060, train_loss: 1.9921
2022-07-16 02:59:33 - eval: epoch: 060, acc1: 58.314%, acc5: 81.864%, test_loss: 1.7710, per_image_load_time: 2.687ms, per_image_inference_time: 0.239ms
2022-07-16 02:59:33 - until epoch: 060, best_acc1: 58.574%
2022-07-16 02:59:33 - epoch 061 lr: 0.050000
2022-07-16 03:00:12 - train: epoch 0061, iter [00100, 05004], lr: 0.049974, loss: 1.7037
2022-07-16 03:00:46 - train: epoch 0061, iter [00200, 05004], lr: 0.049948, loss: 1.9733
2022-07-16 03:01:19 - train: epoch 0061, iter [00300, 05004], lr: 0.049922, loss: 1.7020
2022-07-16 03:01:53 - train: epoch 0061, iter [00400, 05004], lr: 0.049895, loss: 2.1610
2022-07-16 03:02:27 - train: epoch 0061, iter [00500, 05004], lr: 0.049869, loss: 1.9446
2022-07-16 03:03:01 - train: epoch 0061, iter [00600, 05004], lr: 0.049843, loss: 1.9775
2022-07-16 03:03:34 - train: epoch 0061, iter [00700, 05004], lr: 0.049817, loss: 1.7333
2022-07-16 03:04:08 - train: epoch 0061, iter [00800, 05004], lr: 0.049791, loss: 2.0062
2022-07-16 03:04:41 - train: epoch 0061, iter [00900, 05004], lr: 0.049765, loss: 1.9142
2022-07-16 03:05:15 - train: epoch 0061, iter [01000, 05004], lr: 0.049738, loss: 1.7003
2022-07-16 03:05:49 - train: epoch 0061, iter [01100, 05004], lr: 0.049712, loss: 1.7801
2022-07-16 03:06:22 - train: epoch 0061, iter [01200, 05004], lr: 0.049686, loss: 1.9774
2022-07-16 03:06:55 - train: epoch 0061, iter [01300, 05004], lr: 0.049660, loss: 1.9389
2022-07-16 03:07:29 - train: epoch 0061, iter [01400, 05004], lr: 0.049634, loss: 1.9622
2022-07-16 03:08:02 - train: epoch 0061, iter [01500, 05004], lr: 0.049608, loss: 2.0182
2022-07-16 03:08:35 - train: epoch 0061, iter [01600, 05004], lr: 0.049581, loss: 1.7395
2022-07-16 03:09:08 - train: epoch 0061, iter [01700, 05004], lr: 0.049555, loss: 1.9909
2022-07-16 03:09:42 - train: epoch 0061, iter [01800, 05004], lr: 0.049529, loss: 2.0738
2022-07-16 03:10:15 - train: epoch 0061, iter [01900, 05004], lr: 0.049503, loss: 2.1959
2022-07-16 03:10:49 - train: epoch 0061, iter [02000, 05004], lr: 0.049477, loss: 1.9475
2022-07-16 03:11:21 - train: epoch 0061, iter [02100, 05004], lr: 0.049451, loss: 2.4682
2022-07-16 03:11:56 - train: epoch 0061, iter [02200, 05004], lr: 0.049425, loss: 1.8483
2022-07-16 03:12:29 - train: epoch 0061, iter [02300, 05004], lr: 0.049398, loss: 1.8524
2022-07-16 03:13:02 - train: epoch 0061, iter [02400, 05004], lr: 0.049372, loss: 1.7808
2022-07-16 03:13:37 - train: epoch 0061, iter [02500, 05004], lr: 0.049346, loss: 1.9132
2022-07-16 03:14:09 - train: epoch 0061, iter [02600, 05004], lr: 0.049320, loss: 1.9713
2022-07-16 03:14:43 - train: epoch 0061, iter [02700, 05004], lr: 0.049294, loss: 2.0774
2022-07-16 03:15:16 - train: epoch 0061, iter [02800, 05004], lr: 0.049268, loss: 2.0216
2022-07-16 03:15:49 - train: epoch 0061, iter [02900, 05004], lr: 0.049241, loss: 2.1075
2022-07-16 03:16:23 - train: epoch 0061, iter [03000, 05004], lr: 0.049215, loss: 1.9643
2022-07-16 03:16:57 - train: epoch 0061, iter [03100, 05004], lr: 0.049189, loss: 2.2152
2022-07-16 03:17:30 - train: epoch 0061, iter [03200, 05004], lr: 0.049163, loss: 2.0139
2022-07-16 03:18:04 - train: epoch 0061, iter [03300, 05004], lr: 0.049137, loss: 1.9044
2022-07-16 03:18:38 - train: epoch 0061, iter [03400, 05004], lr: 0.049111, loss: 1.8152
2022-07-16 03:19:11 - train: epoch 0061, iter [03500, 05004], lr: 0.049084, loss: 2.0426
2022-07-16 03:19:45 - train: epoch 0061, iter [03600, 05004], lr: 0.049058, loss: 2.0003
2022-07-16 03:20:19 - train: epoch 0061, iter [03700, 05004], lr: 0.049032, loss: 2.0930
2022-07-16 03:20:52 - train: epoch 0061, iter [03800, 05004], lr: 0.049006, loss: 1.9528
2022-07-16 03:21:26 - train: epoch 0061, iter [03900, 05004], lr: 0.048980, loss: 1.8530
2022-07-16 03:21:59 - train: epoch 0061, iter [04000, 05004], lr: 0.048954, loss: 2.0659
2022-07-16 03:22:33 - train: epoch 0061, iter [04100, 05004], lr: 0.048928, loss: 2.2765
2022-07-16 03:23:06 - train: epoch 0061, iter [04200, 05004], lr: 0.048901, loss: 1.8920
2022-07-16 03:23:40 - train: epoch 0061, iter [04300, 05004], lr: 0.048875, loss: 2.0436
2022-07-16 03:24:13 - train: epoch 0061, iter [04400, 05004], lr: 0.048849, loss: 2.0709
2022-07-16 03:24:47 - train: epoch 0061, iter [04500, 05004], lr: 0.048823, loss: 2.0350
2022-07-16 03:25:22 - train: epoch 0061, iter [04600, 05004], lr: 0.048797, loss: 2.1766
2022-07-16 03:25:55 - train: epoch 0061, iter [04700, 05004], lr: 0.048771, loss: 1.9410
2022-07-16 03:26:28 - train: epoch 0061, iter [04800, 05004], lr: 0.048744, loss: 1.9430
2022-07-16 03:27:02 - train: epoch 0061, iter [04900, 05004], lr: 0.048718, loss: 1.9872
2022-07-16 03:27:33 - train: epoch 0061, iter [05000, 05004], lr: 0.048692, loss: 2.0983
2022-07-16 03:27:34 - train: epoch 061, train_loss: 1.9821
2022-07-16 03:28:50 - eval: epoch: 061, acc1: 58.506%, acc5: 81.914%, test_loss: 1.7774, per_image_load_time: 2.680ms, per_image_inference_time: 0.199ms
2022-07-16 03:28:50 - until epoch: 061, best_acc1: 58.574%
2022-07-16 03:28:50 - epoch 062 lr: 0.048691
2022-07-16 03:29:29 - train: epoch 0062, iter [00100, 05004], lr: 0.048665, loss: 2.0568
2022-07-16 03:30:02 - train: epoch 0062, iter [00200, 05004], lr: 0.048639, loss: 2.1513
2022-07-16 03:30:36 - train: epoch 0062, iter [00300, 05004], lr: 0.048613, loss: 1.8366
2022-07-16 03:31:09 - train: epoch 0062, iter [00400, 05004], lr: 0.048587, loss: 1.8879
2022-07-16 03:31:43 - train: epoch 0062, iter [00500, 05004], lr: 0.048560, loss: 1.8781
2022-07-16 03:32:16 - train: epoch 0062, iter [00600, 05004], lr: 0.048534, loss: 1.8522
2022-07-16 03:32:50 - train: epoch 0062, iter [00700, 05004], lr: 0.048508, loss: 2.1244
2022-07-16 03:33:23 - train: epoch 0062, iter [00800, 05004], lr: 0.048482, loss: 1.9021
2022-07-16 03:33:56 - train: epoch 0062, iter [00900, 05004], lr: 0.048456, loss: 1.8970
2022-07-16 03:34:31 - train: epoch 0062, iter [01000, 05004], lr: 0.048430, loss: 2.1048
2022-07-16 03:35:03 - train: epoch 0062, iter [01100, 05004], lr: 0.048404, loss: 1.7948
2022-07-16 03:35:37 - train: epoch 0062, iter [01200, 05004], lr: 0.048377, loss: 2.1659
2022-07-16 03:36:09 - train: epoch 0062, iter [01300, 05004], lr: 0.048351, loss: 2.0011
2022-07-16 03:36:43 - train: epoch 0062, iter [01400, 05004], lr: 0.048325, loss: 1.8644
2022-07-16 03:37:16 - train: epoch 0062, iter [01500, 05004], lr: 0.048299, loss: 2.0106
2022-07-16 03:37:50 - train: epoch 0062, iter [01600, 05004], lr: 0.048273, loss: 2.1109
2022-07-16 03:38:23 - train: epoch 0062, iter [01700, 05004], lr: 0.048247, loss: 1.9398
2022-07-16 03:38:57 - train: epoch 0062, iter [01800, 05004], lr: 0.048221, loss: 1.8614
2022-07-16 03:39:30 - train: epoch 0062, iter [01900, 05004], lr: 0.048194, loss: 1.9952
2022-07-16 03:40:03 - train: epoch 0062, iter [02000, 05004], lr: 0.048168, loss: 1.8917
2022-07-16 03:40:37 - train: epoch 0062, iter [02100, 05004], lr: 0.048142, loss: 2.0668
2022-07-16 03:41:10 - train: epoch 0062, iter [02200, 05004], lr: 0.048116, loss: 1.8345
2022-07-16 03:41:44 - train: epoch 0062, iter [02300, 05004], lr: 0.048090, loss: 1.9451
2022-07-16 03:42:17 - train: epoch 0062, iter [02400, 05004], lr: 0.048064, loss: 2.1112
2022-07-16 03:42:51 - train: epoch 0062, iter [02500, 05004], lr: 0.048038, loss: 2.1074
2022-07-16 03:43:24 - train: epoch 0062, iter [02600, 05004], lr: 0.048011, loss: 1.8370
2022-07-16 03:43:58 - train: epoch 0062, iter [02700, 05004], lr: 0.047985, loss: 1.9039
2022-07-16 03:44:31 - train: epoch 0062, iter [02800, 05004], lr: 0.047959, loss: 2.2506
2022-07-16 03:45:05 - train: epoch 0062, iter [02900, 05004], lr: 0.047933, loss: 1.9713
2022-07-16 03:45:39 - train: epoch 0062, iter [03000, 05004], lr: 0.047907, loss: 2.0842
2022-07-16 03:46:12 - train: epoch 0062, iter [03100, 05004], lr: 0.047881, loss: 2.0545
2022-07-16 03:46:45 - train: epoch 0062, iter [03200, 05004], lr: 0.047855, loss: 1.7923
2022-07-16 03:47:18 - train: epoch 0062, iter [03300, 05004], lr: 0.047828, loss: 2.2028
2022-07-16 03:47:51 - train: epoch 0062, iter [03400, 05004], lr: 0.047802, loss: 1.9994
2022-07-16 03:48:25 - train: epoch 0062, iter [03500, 05004], lr: 0.047776, loss: 1.9691
2022-07-16 03:48:58 - train: epoch 0062, iter [03600, 05004], lr: 0.047750, loss: 2.0675
2022-07-16 03:49:32 - train: epoch 0062, iter [03700, 05004], lr: 0.047724, loss: 1.9988
2022-07-16 03:50:06 - train: epoch 0062, iter [03800, 05004], lr: 0.047698, loss: 1.9849
2022-07-16 03:50:40 - train: epoch 0062, iter [03900, 05004], lr: 0.047672, loss: 1.9927
2022-07-16 03:51:13 - train: epoch 0062, iter [04000, 05004], lr: 0.047646, loss: 1.6219
2022-07-16 03:51:46 - train: epoch 0062, iter [04100, 05004], lr: 0.047619, loss: 2.1181
2022-07-16 03:52:19 - train: epoch 0062, iter [04200, 05004], lr: 0.047593, loss: 1.8339
2022-07-16 03:52:52 - train: epoch 0062, iter [04300, 05004], lr: 0.047567, loss: 2.1205
2022-07-16 03:53:25 - train: epoch 0062, iter [04400, 05004], lr: 0.047541, loss: 2.0284
2022-07-16 03:53:59 - train: epoch 0062, iter [04500, 05004], lr: 0.047515, loss: 1.8217
2022-07-16 03:54:33 - train: epoch 0062, iter [04600, 05004], lr: 0.047489, loss: 1.7107
2022-07-16 03:55:06 - train: epoch 0062, iter [04700, 05004], lr: 0.047463, loss: 1.9509
2022-07-16 03:55:40 - train: epoch 0062, iter [04800, 05004], lr: 0.047436, loss: 1.9103
2022-07-16 03:56:13 - train: epoch 0062, iter [04900, 05004], lr: 0.047410, loss: 1.8854
2022-07-16 03:56:45 - train: epoch 0062, iter [05000, 05004], lr: 0.047384, loss: 2.0613
2022-07-16 03:56:46 - train: epoch 062, train_loss: 1.9699
2022-07-16 03:58:01 - eval: epoch: 062, acc1: 58.572%, acc5: 82.098%, test_loss: 1.7550, per_image_load_time: 2.526ms, per_image_inference_time: 0.229ms
2022-07-16 03:58:01 - until epoch: 062, best_acc1: 58.574%
2022-07-16 03:58:01 - epoch 063 lr: 0.047383
2022-07-16 03:58:40 - train: epoch 0063, iter [00100, 05004], lr: 0.047357, loss: 1.9661
2022-07-16 03:59:13 - train: epoch 0063, iter [00200, 05004], lr: 0.047331, loss: 1.9591
2022-07-16 03:59:47 - train: epoch 0063, iter [00300, 05004], lr: 0.047305, loss: 2.0692
2022-07-16 04:00:20 - train: epoch 0063, iter [00400, 05004], lr: 0.047279, loss: 2.0290
2022-07-16 04:00:53 - train: epoch 0063, iter [00500, 05004], lr: 0.047253, loss: 1.7819
2022-07-16 04:01:25 - train: epoch 0063, iter [00600, 05004], lr: 0.047226, loss: 1.9725
2022-07-16 04:02:00 - train: epoch 0063, iter [00700, 05004], lr: 0.047200, loss: 1.8746
2022-07-16 04:02:32 - train: epoch 0063, iter [00800, 05004], lr: 0.047174, loss: 1.9209
2022-07-16 04:03:05 - train: epoch 0063, iter [00900, 05004], lr: 0.047148, loss: 1.9012
2022-07-16 04:03:39 - train: epoch 0063, iter [01000, 05004], lr: 0.047122, loss: 2.0599
2022-07-16 04:04:12 - train: epoch 0063, iter [01100, 05004], lr: 0.047096, loss: 1.8966
2022-07-16 04:04:45 - train: epoch 0063, iter [01200, 05004], lr: 0.047070, loss: 1.9031
2022-07-16 04:05:19 - train: epoch 0063, iter [01300, 05004], lr: 0.047044, loss: 1.9050
2022-07-16 04:05:52 - train: epoch 0063, iter [01400, 05004], lr: 0.047018, loss: 2.2616
2022-07-16 04:06:26 - train: epoch 0063, iter [01500, 05004], lr: 0.046991, loss: 1.9310
2022-07-16 04:06:59 - train: epoch 0063, iter [01600, 05004], lr: 0.046965, loss: 1.8483
2022-07-16 04:07:33 - train: epoch 0063, iter [01700, 05004], lr: 0.046939, loss: 1.7908
2022-07-16 04:08:07 - train: epoch 0063, iter [01800, 05004], lr: 0.046913, loss: 2.1228
2022-07-16 04:08:40 - train: epoch 0063, iter [01900, 05004], lr: 0.046887, loss: 2.1741
2022-07-16 04:09:14 - train: epoch 0063, iter [02000, 05004], lr: 0.046861, loss: 1.9083
2022-07-16 04:09:46 - train: epoch 0063, iter [02100, 05004], lr: 0.046835, loss: 1.9403
2022-07-16 04:10:20 - train: epoch 0063, iter [02200, 05004], lr: 0.046809, loss: 2.3314
2022-07-16 04:10:53 - train: epoch 0063, iter [02300, 05004], lr: 0.046783, loss: 2.0929
2022-07-16 04:11:27 - train: epoch 0063, iter [02400, 05004], lr: 0.046756, loss: 2.1827
2022-07-16 04:12:01 - train: epoch 0063, iter [02500, 05004], lr: 0.046730, loss: 1.9955
2022-07-16 04:12:35 - train: epoch 0063, iter [02600, 05004], lr: 0.046704, loss: 1.8104
2022-07-16 04:13:08 - train: epoch 0063, iter [02700, 05004], lr: 0.046678, loss: 2.1684
2022-07-16 04:13:41 - train: epoch 0063, iter [02800, 05004], lr: 0.046652, loss: 2.0500
2022-07-16 04:14:16 - train: epoch 0063, iter [02900, 05004], lr: 0.046626, loss: 1.8777
2022-07-16 04:14:48 - train: epoch 0063, iter [03000, 05004], lr: 0.046600, loss: 2.0893
2022-07-16 04:15:22 - train: epoch 0063, iter [03100, 05004], lr: 0.046574, loss: 2.3494
2022-07-16 04:15:54 - train: epoch 0063, iter [03200, 05004], lr: 0.046548, loss: 2.0214
2022-07-16 04:16:28 - train: epoch 0063, iter [03300, 05004], lr: 0.046522, loss: 1.9310
2022-07-16 04:17:01 - train: epoch 0063, iter [03400, 05004], lr: 0.046495, loss: 1.7128
2022-07-16 04:17:35 - train: epoch 0063, iter [03500, 05004], lr: 0.046469, loss: 2.1154
2022-07-16 04:18:08 - train: epoch 0063, iter [03600, 05004], lr: 0.046443, loss: 1.7996
2022-07-16 04:18:43 - train: epoch 0063, iter [03700, 05004], lr: 0.046417, loss: 2.0814
2022-07-16 04:19:15 - train: epoch 0063, iter [03800, 05004], lr: 0.046391, loss: 2.2304
2022-07-16 04:19:49 - train: epoch 0063, iter [03900, 05004], lr: 0.046365, loss: 1.7204
2022-07-16 04:20:23 - train: epoch 0063, iter [04000, 05004], lr: 0.046339, loss: 1.6677
2022-07-16 04:20:57 - train: epoch 0063, iter [04100, 05004], lr: 0.046313, loss: 2.0820
2022-07-16 04:21:30 - train: epoch 0063, iter [04200, 05004], lr: 0.046287, loss: 1.7604
2022-07-16 04:22:04 - train: epoch 0063, iter [04300, 05004], lr: 0.046261, loss: 2.1345
2022-07-16 04:22:37 - train: epoch 0063, iter [04400, 05004], lr: 0.046235, loss: 1.9580
2022-07-16 04:23:11 - train: epoch 0063, iter [04500, 05004], lr: 0.046208, loss: 1.8469
2022-07-16 04:23:44 - train: epoch 0063, iter [04600, 05004], lr: 0.046182, loss: 1.9086
2022-07-16 04:24:18 - train: epoch 0063, iter [04700, 05004], lr: 0.046156, loss: 1.8344
2022-07-16 04:24:52 - train: epoch 0063, iter [04800, 05004], lr: 0.046130, loss: 2.0015
2022-07-16 04:25:25 - train: epoch 0063, iter [04900, 05004], lr: 0.046104, loss: 1.9225
2022-07-16 04:25:57 - train: epoch 0063, iter [05000, 05004], lr: 0.046078, loss: 1.9955
2022-07-16 04:25:58 - train: epoch 063, train_loss: 1.9599
2022-07-16 04:27:12 - eval: epoch: 063, acc1: 56.850%, acc5: 80.376%, test_loss: 1.8774, per_image_load_time: 2.670ms, per_image_inference_time: 0.241ms
2022-07-16 04:27:13 - until epoch: 063, best_acc1: 58.574%
2022-07-16 04:27:13 - epoch 064 lr: 0.046077
2022-07-16 04:27:51 - train: epoch 0064, iter [00100, 05004], lr: 0.046051, loss: 1.7672
2022-07-16 04:28:25 - train: epoch 0064, iter [00200, 05004], lr: 0.046025, loss: 1.9181
2022-07-16 04:28:58 - train: epoch 0064, iter [00300, 05004], lr: 0.045999, loss: 1.7240
2022-07-16 04:29:32 - train: epoch 0064, iter [00400, 05004], lr: 0.045973, loss: 1.9423
2022-07-16 04:30:05 - train: epoch 0064, iter [00500, 05004], lr: 0.045947, loss: 1.7455
2022-07-16 04:30:40 - train: epoch 0064, iter [00600, 05004], lr: 0.045921, loss: 2.1660
2022-07-16 04:31:12 - train: epoch 0064, iter [00700, 05004], lr: 0.045895, loss: 2.1105
2022-07-16 04:31:46 - train: epoch 0064, iter [00800, 05004], lr: 0.045868, loss: 1.7507
2022-07-16 04:32:20 - train: epoch 0064, iter [00900, 05004], lr: 0.045842, loss: 1.9259
2022-07-16 04:32:53 - train: epoch 0064, iter [01000, 05004], lr: 0.045816, loss: 1.9911
2022-07-16 04:33:26 - train: epoch 0064, iter [01100, 05004], lr: 0.045790, loss: 1.8096
2022-07-16 04:34:00 - train: epoch 0064, iter [01200, 05004], lr: 0.045764, loss: 1.8127
2022-07-16 04:34:35 - train: epoch 0064, iter [01300, 05004], lr: 0.045738, loss: 1.9216
2022-07-16 04:35:08 - train: epoch 0064, iter [01400, 05004], lr: 0.045712, loss: 2.1083
2022-07-16 04:35:41 - train: epoch 0064, iter [01500, 05004], lr: 0.045686, loss: 2.1073
2022-07-16 04:36:14 - train: epoch 0064, iter [01600, 05004], lr: 0.045660, loss: 1.8136
2022-07-16 04:36:48 - train: epoch 0064, iter [01700, 05004], lr: 0.045634, loss: 1.7877
2022-07-16 04:37:22 - train: epoch 0064, iter [01800, 05004], lr: 0.045608, loss: 1.8880
2022-07-16 04:37:55 - train: epoch 0064, iter [01900, 05004], lr: 0.045582, loss: 1.8930
2022-07-16 04:38:28 - train: epoch 0064, iter [02000, 05004], lr: 0.045556, loss: 1.7951
2022-07-16 04:39:02 - train: epoch 0064, iter [02100, 05004], lr: 0.045530, loss: 1.9895
2022-07-16 04:39:36 - train: epoch 0064, iter [02200, 05004], lr: 0.045504, loss: 2.0321
2022-07-16 04:40:09 - train: epoch 0064, iter [02300, 05004], lr: 0.045478, loss: 1.9997
2022-07-16 04:40:42 - train: epoch 0064, iter [02400, 05004], lr: 0.045451, loss: 1.9831
2022-07-16 04:41:16 - train: epoch 0064, iter [02500, 05004], lr: 0.045425, loss: 1.8052
2022-07-16 04:41:49 - train: epoch 0064, iter [02600, 05004], lr: 0.045399, loss: 1.9243
2022-07-16 04:42:23 - train: epoch 0064, iter [02700, 05004], lr: 0.045373, loss: 1.8900
2022-07-16 04:42:56 - train: epoch 0064, iter [02800, 05004], lr: 0.045347, loss: 1.7901
2022-07-16 04:43:30 - train: epoch 0064, iter [02900, 05004], lr: 0.045321, loss: 2.2722
2022-07-16 04:44:04 - train: epoch 0064, iter [03000, 05004], lr: 0.045295, loss: 1.9593
2022-07-16 04:44:36 - train: epoch 0064, iter [03100, 05004], lr: 0.045269, loss: 1.8820
2022-07-16 04:45:11 - train: epoch 0064, iter [03200, 05004], lr: 0.045243, loss: 1.9682
2022-07-16 04:45:43 - train: epoch 0064, iter [03300, 05004], lr: 0.045217, loss: 1.8344
2022-07-16 04:46:17 - train: epoch 0064, iter [03400, 05004], lr: 0.045191, loss: 2.0800
2022-07-16 04:46:50 - train: epoch 0064, iter [03500, 05004], lr: 0.045165, loss: 1.8587
2022-07-16 04:47:24 - train: epoch 0064, iter [03600, 05004], lr: 0.045139, loss: 1.9625
2022-07-16 04:47:57 - train: epoch 0064, iter [03700, 05004], lr: 0.045113, loss: 1.5381
2022-07-16 04:48:31 - train: epoch 0064, iter [03800, 05004], lr: 0.045087, loss: 2.2034
2022-07-16 04:49:04 - train: epoch 0064, iter [03900, 05004], lr: 0.045061, loss: 1.8587
2022-07-16 04:49:38 - train: epoch 0064, iter [04000, 05004], lr: 0.045035, loss: 1.8918
2022-07-16 04:50:11 - train: epoch 0064, iter [04100, 05004], lr: 0.045009, loss: 1.9342
2022-07-16 04:50:45 - train: epoch 0064, iter [04200, 05004], lr: 0.044983, loss: 1.8703
2022-07-16 04:51:18 - train: epoch 0064, iter [04300, 05004], lr: 0.044957, loss: 2.1883
2022-07-16 04:51:53 - train: epoch 0064, iter [04400, 05004], lr: 0.044931, loss: 1.9108
2022-07-16 04:52:25 - train: epoch 0064, iter [04500, 05004], lr: 0.044905, loss: 1.8740
2022-07-16 04:52:59 - train: epoch 0064, iter [04600, 05004], lr: 0.044879, loss: 2.1506
2022-07-16 04:53:33 - train: epoch 0064, iter [04700, 05004], lr: 0.044853, loss: 2.3436
2022-07-16 04:54:06 - train: epoch 0064, iter [04800, 05004], lr: 0.044827, loss: 2.0107
2022-07-16 04:54:39 - train: epoch 0064, iter [04900, 05004], lr: 0.044801, loss: 2.0784
2022-07-16 04:55:11 - train: epoch 0064, iter [05000, 05004], lr: 0.044775, loss: 1.8486
2022-07-16 04:55:12 - train: epoch 064, train_loss: 1.9497
2022-07-16 04:56:27 - eval: epoch: 064, acc1: 59.108%, acc5: 82.404%, test_loss: 1.7381, per_image_load_time: 2.551ms, per_image_inference_time: 0.221ms
2022-07-16 04:56:27 - until epoch: 064, best_acc1: 59.108%
2022-07-16 04:56:27 - epoch 065 lr: 0.044773
2022-07-16 04:57:07 - train: epoch 0065, iter [00100, 05004], lr: 0.044748, loss: 1.9083
2022-07-16 04:57:40 - train: epoch 0065, iter [00200, 05004], lr: 0.044722, loss: 1.9569
2022-07-16 04:58:14 - train: epoch 0065, iter [00300, 05004], lr: 0.044696, loss: 1.8482
2022-07-16 04:58:47 - train: epoch 0065, iter [00400, 05004], lr: 0.044670, loss: 2.1496
2022-07-16 04:59:20 - train: epoch 0065, iter [00500, 05004], lr: 0.044644, loss: 1.8775
2022-07-16 04:59:54 - train: epoch 0065, iter [00600, 05004], lr: 0.044618, loss: 1.9579
2022-07-16 05:00:27 - train: epoch 0065, iter [00700, 05004], lr: 0.044592, loss: 2.0099
2022-07-16 05:01:01 - train: epoch 0065, iter [00800, 05004], lr: 0.044565, loss: 1.8956
2022-07-16 05:01:34 - train: epoch 0065, iter [00900, 05004], lr: 0.044539, loss: 1.8792
2022-07-16 05:02:08 - train: epoch 0065, iter [01000, 05004], lr: 0.044513, loss: 1.8780
2022-07-16 05:02:41 - train: epoch 0065, iter [01100, 05004], lr: 0.044487, loss: 1.8571
2022-07-16 05:03:15 - train: epoch 0065, iter [01200, 05004], lr: 0.044461, loss: 2.1284
2022-07-16 05:03:48 - train: epoch 0065, iter [01300, 05004], lr: 0.044435, loss: 1.9937
2022-07-16 05:04:21 - train: epoch 0065, iter [01400, 05004], lr: 0.044410, loss: 1.7796
2022-07-16 05:04:55 - train: epoch 0065, iter [01500, 05004], lr: 0.044384, loss: 1.8925
2022-07-16 05:05:28 - train: epoch 0065, iter [01600, 05004], lr: 0.044358, loss: 2.0375
2022-07-16 05:06:02 - train: epoch 0065, iter [01700, 05004], lr: 0.044332, loss: 1.8375
2022-07-16 05:06:35 - train: epoch 0065, iter [01800, 05004], lr: 0.044306, loss: 1.7452
2022-07-16 05:07:09 - train: epoch 0065, iter [01900, 05004], lr: 0.044280, loss: 1.6550
2022-07-16 05:07:42 - train: epoch 0065, iter [02000, 05004], lr: 0.044254, loss: 1.9221
2022-07-16 05:08:16 - train: epoch 0065, iter [02100, 05004], lr: 0.044228, loss: 1.8662
2022-07-16 05:08:49 - train: epoch 0065, iter [02200, 05004], lr: 0.044202, loss: 1.9221
2022-07-16 05:09:23 - train: epoch 0065, iter [02300, 05004], lr: 0.044176, loss: 1.8790
2022-07-16 05:09:56 - train: epoch 0065, iter [02400, 05004], lr: 0.044150, loss: 1.8910
2022-07-16 05:10:30 - train: epoch 0065, iter [02500, 05004], lr: 0.044124, loss: 1.8060
2022-07-16 05:11:03 - train: epoch 0065, iter [02600, 05004], lr: 0.044098, loss: 2.1315
2022-07-16 05:11:37 - train: epoch 0065, iter [02700, 05004], lr: 0.044072, loss: 2.0452
2022-07-16 05:12:10 - train: epoch 0065, iter [02800, 05004], lr: 0.044046, loss: 1.8856
2022-07-16 05:12:45 - train: epoch 0065, iter [02900, 05004], lr: 0.044020, loss: 1.9897
2022-07-16 05:13:18 - train: epoch 0065, iter [03000, 05004], lr: 0.043994, loss: 1.8072
2022-07-16 05:13:52 - train: epoch 0065, iter [03100, 05004], lr: 0.043968, loss: 2.0181
2022-07-16 05:14:24 - train: epoch 0065, iter [03200, 05004], lr: 0.043942, loss: 2.1588
2022-07-16 05:14:58 - train: epoch 0065, iter [03300, 05004], lr: 0.043916, loss: 1.9776
2022-07-16 05:15:32 - train: epoch 0065, iter [03400, 05004], lr: 0.043890, loss: 1.8367
2022-07-16 05:16:06 - train: epoch 0065, iter [03500, 05004], lr: 0.043864, loss: 2.2123
2022-07-16 05:16:39 - train: epoch 0065, iter [03600, 05004], lr: 0.043838, loss: 2.0415
2022-07-16 05:17:13 - train: epoch 0065, iter [03700, 05004], lr: 0.043812, loss: 1.8673
2022-07-16 05:17:45 - train: epoch 0065, iter [03800, 05004], lr: 0.043786, loss: 1.8243
2022-07-16 05:18:19 - train: epoch 0065, iter [03900, 05004], lr: 0.043760, loss: 2.0548
2022-07-16 05:18:52 - train: epoch 0065, iter [04000, 05004], lr: 0.043734, loss: 1.9606
2022-07-16 05:19:26 - train: epoch 0065, iter [04100, 05004], lr: 0.043708, loss: 1.7493
2022-07-16 05:19:59 - train: epoch 0065, iter [04200, 05004], lr: 0.043682, loss: 1.9015
2022-07-16 05:20:33 - train: epoch 0065, iter [04300, 05004], lr: 0.043656, loss: 1.9371
2022-07-16 05:21:05 - train: epoch 0065, iter [04400, 05004], lr: 0.043630, loss: 1.9834
2022-07-16 05:21:39 - train: epoch 0065, iter [04500, 05004], lr: 0.043604, loss: 1.9907
2022-07-16 05:22:13 - train: epoch 0065, iter [04600, 05004], lr: 0.043578, loss: 2.0257
2022-07-16 05:22:47 - train: epoch 0065, iter [04700, 05004], lr: 0.043553, loss: 2.0154
2022-07-16 05:23:20 - train: epoch 0065, iter [04800, 05004], lr: 0.043527, loss: 1.6369
2022-07-16 05:23:53 - train: epoch 0065, iter [04900, 05004], lr: 0.043501, loss: 1.9221
2022-07-16 05:24:24 - train: epoch 0065, iter [05000, 05004], lr: 0.043475, loss: 2.0581
2022-07-16 05:24:25 - train: epoch 065, train_loss: 1.9379
2022-07-16 05:25:39 - eval: epoch: 065, acc1: 59.128%, acc5: 82.370%, test_loss: 1.7353, per_image_load_time: 2.562ms, per_image_inference_time: 0.236ms
2022-07-16 05:25:40 - until epoch: 065, best_acc1: 59.128%
2022-07-16 05:25:40 - epoch 066 lr: 0.043473
2022-07-16 05:26:18 - train: epoch 0066, iter [00100, 05004], lr: 0.043448, loss: 1.8943
2022-07-16 05:26:53 - train: epoch 0066, iter [00200, 05004], lr: 0.043422, loss: 2.0703
2022-07-16 05:27:26 - train: epoch 0066, iter [00300, 05004], lr: 0.043396, loss: 1.8973
2022-07-16 05:28:01 - train: epoch 0066, iter [00400, 05004], lr: 0.043370, loss: 1.6355
2022-07-16 05:28:33 - train: epoch 0066, iter [00500, 05004], lr: 0.043344, loss: 2.0426
2022-07-16 05:29:07 - train: epoch 0066, iter [00600, 05004], lr: 0.043318, loss: 1.9247
2022-07-16 05:29:40 - train: epoch 0066, iter [00700, 05004], lr: 0.043292, loss: 1.7711
2022-07-16 05:30:13 - train: epoch 0066, iter [00800, 05004], lr: 0.043266, loss: 2.3142
2022-07-16 05:30:47 - train: epoch 0066, iter [00900, 05004], lr: 0.043240, loss: 2.0542
2022-07-16 05:31:21 - train: epoch 0066, iter [01000, 05004], lr: 0.043214, loss: 2.0291
2022-07-16 05:31:54 - train: epoch 0066, iter [01100, 05004], lr: 0.043189, loss: 2.0177
2022-07-16 05:32:28 - train: epoch 0066, iter [01200, 05004], lr: 0.043163, loss: 2.0958
2022-07-16 05:33:02 - train: epoch 0066, iter [01300, 05004], lr: 0.043137, loss: 2.1896
2022-07-16 05:33:34 - train: epoch 0066, iter [01400, 05004], lr: 0.043111, loss: 1.8564
2022-07-16 05:34:09 - train: epoch 0066, iter [01500, 05004], lr: 0.043085, loss: 2.0367
2022-07-16 05:34:41 - train: epoch 0066, iter [01600, 05004], lr: 0.043059, loss: 1.9883
2022-07-16 05:35:15 - train: epoch 0066, iter [01700, 05004], lr: 0.043033, loss: 1.8634
2022-07-16 05:35:49 - train: epoch 0066, iter [01800, 05004], lr: 0.043007, loss: 1.7403
2022-07-16 05:36:22 - train: epoch 0066, iter [01900, 05004], lr: 0.042981, loss: 2.2222
2022-07-16 05:36:55 - train: epoch 0066, iter [02000, 05004], lr: 0.042955, loss: 1.9482
2022-07-16 05:37:29 - train: epoch 0066, iter [02100, 05004], lr: 0.042929, loss: 2.0403
2022-07-16 05:38:02 - train: epoch 0066, iter [02200, 05004], lr: 0.042904, loss: 1.9598
2022-07-16 05:38:37 - train: epoch 0066, iter [02300, 05004], lr: 0.042878, loss: 2.0919
2022-07-16 05:39:09 - train: epoch 0066, iter [02400, 05004], lr: 0.042852, loss: 1.7620
2022-07-16 05:39:43 - train: epoch 0066, iter [02500, 05004], lr: 0.042826, loss: 1.8961
2022-07-16 05:40:16 - train: epoch 0066, iter [02600, 05004], lr: 0.042800, loss: 1.8460
2022-07-16 05:40:50 - train: epoch 0066, iter [02700, 05004], lr: 0.042774, loss: 2.0984
2022-07-16 05:41:24 - train: epoch 0066, iter [02800, 05004], lr: 0.042748, loss: 1.9858
2022-07-16 05:41:58 - train: epoch 0066, iter [02900, 05004], lr: 0.042722, loss: 2.0277
2022-07-16 05:42:32 - train: epoch 0066, iter [03000, 05004], lr: 0.042696, loss: 1.9039
2022-07-16 05:43:06 - train: epoch 0066, iter [03100, 05004], lr: 0.042671, loss: 1.9799
2022-07-16 05:43:39 - train: epoch 0066, iter [03200, 05004], lr: 0.042645, loss: 1.7696
2022-07-16 05:44:13 - train: epoch 0066, iter [03300, 05004], lr: 0.042619, loss: 1.8656
2022-07-16 05:44:46 - train: epoch 0066, iter [03400, 05004], lr: 0.042593, loss: 2.2358
2022-07-16 05:45:20 - train: epoch 0066, iter [03500, 05004], lr: 0.042567, loss: 1.9684
2022-07-16 05:45:53 - train: epoch 0066, iter [03600, 05004], lr: 0.042541, loss: 2.0586
2022-07-16 05:46:26 - train: epoch 0066, iter [03700, 05004], lr: 0.042515, loss: 1.9280
2022-07-16 05:47:00 - train: epoch 0066, iter [03800, 05004], lr: 0.042490, loss: 1.7915
2022-07-16 05:47:34 - train: epoch 0066, iter [03900, 05004], lr: 0.042464, loss: 1.7566
2022-07-16 05:48:07 - train: epoch 0066, iter [04000, 05004], lr: 0.042438, loss: 2.0393
2022-07-16 05:48:41 - train: epoch 0066, iter [04100, 05004], lr: 0.042412, loss: 1.7391
2022-07-16 05:49:14 - train: epoch 0066, iter [04200, 05004], lr: 0.042386, loss: 1.8161
2022-07-16 05:49:49 - train: epoch 0066, iter [04300, 05004], lr: 0.042360, loss: 1.7376
2022-07-16 05:50:21 - train: epoch 0066, iter [04400, 05004], lr: 0.042334, loss: 1.7834
2022-07-16 05:50:55 - train: epoch 0066, iter [04500, 05004], lr: 0.042309, loss: 2.0958
2022-07-16 05:51:29 - train: epoch 0066, iter [04600, 05004], lr: 0.042283, loss: 2.1659
2022-07-16 05:52:02 - train: epoch 0066, iter [04700, 05004], lr: 0.042257, loss: 1.8199
2022-07-16 05:52:36 - train: epoch 0066, iter [04800, 05004], lr: 0.042231, loss: 1.8646
2022-07-16 05:53:09 - train: epoch 0066, iter [04900, 05004], lr: 0.042205, loss: 1.9397
2022-07-16 05:53:42 - train: epoch 0066, iter [05000, 05004], lr: 0.042179, loss: 1.8153
2022-07-16 05:53:42 - train: epoch 066, train_loss: 1.9272
2022-07-16 05:54:58 - eval: epoch: 066, acc1: 60.038%, acc5: 83.202%, test_loss: 1.6875, per_image_load_time: 2.535ms, per_image_inference_time: 0.228ms
2022-07-16 05:54:58 - until epoch: 066, best_acc1: 60.038%
2022-07-16 05:54:58 - epoch 067 lr: 0.042178
2022-07-16 05:55:38 - train: epoch 0067, iter [00100, 05004], lr: 0.042152, loss: 1.8988
2022-07-16 05:56:12 - train: epoch 0067, iter [00200, 05004], lr: 0.042127, loss: 1.8416
2022-07-16 05:56:45 - train: epoch 0067, iter [00300, 05004], lr: 0.042101, loss: 2.0386
2022-07-16 05:57:19 - train: epoch 0067, iter [00400, 05004], lr: 0.042075, loss: 1.8543
2022-07-16 05:57:52 - train: epoch 0067, iter [00500, 05004], lr: 0.042049, loss: 1.7656
2022-07-16 05:58:26 - train: epoch 0067, iter [00600, 05004], lr: 0.042023, loss: 1.7623
2022-07-16 05:58:59 - train: epoch 0067, iter [00700, 05004], lr: 0.041997, loss: 1.8979
2022-07-16 05:59:33 - train: epoch 0067, iter [00800, 05004], lr: 0.041972, loss: 2.0301
2022-07-16 06:00:07 - train: epoch 0067, iter [00900, 05004], lr: 0.041946, loss: 2.1688
2022-07-16 06:00:40 - train: epoch 0067, iter [01000, 05004], lr: 0.041920, loss: 1.7871
2022-07-16 06:01:13 - train: epoch 0067, iter [01100, 05004], lr: 0.041894, loss: 1.9104
2022-07-16 06:01:47 - train: epoch 0067, iter [01200, 05004], lr: 0.041868, loss: 1.8034
2022-07-16 06:02:20 - train: epoch 0067, iter [01300, 05004], lr: 0.041843, loss: 2.1271
2022-07-16 06:02:54 - train: epoch 0067, iter [01400, 05004], lr: 0.041817, loss: 1.9666
2022-07-16 06:03:27 - train: epoch 0067, iter [01500, 05004], lr: 0.041791, loss: 1.7996
2022-07-16 06:04:01 - train: epoch 0067, iter [01600, 05004], lr: 0.041765, loss: 1.8736
2022-07-16 06:04:35 - train: epoch 0067, iter [01700, 05004], lr: 0.041739, loss: 1.8080
2022-07-16 06:05:09 - train: epoch 0067, iter [01800, 05004], lr: 0.041714, loss: 2.1156
2022-07-16 06:05:41 - train: epoch 0067, iter [01900, 05004], lr: 0.041688, loss: 1.9102
2022-07-16 06:06:16 - train: epoch 0067, iter [02000, 05004], lr: 0.041662, loss: 1.9129
2022-07-16 06:06:50 - train: epoch 0067, iter [02100, 05004], lr: 0.041636, loss: 1.6463
2022-07-16 06:07:23 - train: epoch 0067, iter [02200, 05004], lr: 0.041610, loss: 1.9067
2022-07-16 06:07:57 - train: epoch 0067, iter [02300, 05004], lr: 0.041585, loss: 1.8511
2022-07-16 06:08:31 - train: epoch 0067, iter [02400, 05004], lr: 0.041559, loss: 2.0203
2022-07-16 06:09:05 - train: epoch 0067, iter [02500, 05004], lr: 0.041533, loss: 1.8170
2022-07-16 06:09:37 - train: epoch 0067, iter [02600, 05004], lr: 0.041507, loss: 1.8146
2022-07-16 06:10:12 - train: epoch 0067, iter [02700, 05004], lr: 0.041481, loss: 1.7351
2022-07-16 06:10:45 - train: epoch 0067, iter [02800, 05004], lr: 0.041456, loss: 1.9945
2022-07-16 06:11:19 - train: epoch 0067, iter [02900, 05004], lr: 0.041430, loss: 1.8777
2022-07-16 06:11:53 - train: epoch 0067, iter [03000, 05004], lr: 0.041404, loss: 1.8043
2022-07-16 06:12:27 - train: epoch 0067, iter [03100, 05004], lr: 0.041378, loss: 1.7139
2022-07-16 06:13:00 - train: epoch 0067, iter [03200, 05004], lr: 0.041353, loss: 2.0573
2022-07-16 06:13:34 - train: epoch 0067, iter [03300, 05004], lr: 0.041327, loss: 1.7761
2022-07-16 06:14:08 - train: epoch 0067, iter [03400, 05004], lr: 0.041301, loss: 1.8967
2022-07-16 06:14:42 - train: epoch 0067, iter [03500, 05004], lr: 0.041275, loss: 1.8055
2022-07-16 06:15:16 - train: epoch 0067, iter [03600, 05004], lr: 0.041250, loss: 2.0079
2022-07-16 06:15:50 - train: epoch 0067, iter [03700, 05004], lr: 0.041224, loss: 2.0709
2022-07-16 06:16:24 - train: epoch 0067, iter [03800, 05004], lr: 0.041198, loss: 1.9382
2022-07-16 06:16:58 - train: epoch 0067, iter [03900, 05004], lr: 0.041172, loss: 2.0994
2022-07-16 06:17:31 - train: epoch 0067, iter [04000, 05004], lr: 0.041147, loss: 1.9816
2022-07-16 06:18:05 - train: epoch 0067, iter [04100, 05004], lr: 0.041121, loss: 1.9924
2022-07-16 06:18:39 - train: epoch 0067, iter [04200, 05004], lr: 0.041095, loss: 2.1370
2022-07-16 06:19:13 - train: epoch 0067, iter [04300, 05004], lr: 0.041069, loss: 1.7002
2022-07-16 06:19:47 - train: epoch 0067, iter [04400, 05004], lr: 0.041044, loss: 2.0561
2022-07-16 06:20:21 - train: epoch 0067, iter [04500, 05004], lr: 0.041018, loss: 1.8168
2022-07-16 06:20:54 - train: epoch 0067, iter [04600, 05004], lr: 0.040992, loss: 1.6315
2022-07-16 06:21:29 - train: epoch 0067, iter [04700, 05004], lr: 0.040966, loss: 1.8917
2022-07-16 06:22:02 - train: epoch 0067, iter [04800, 05004], lr: 0.040941, loss: 1.5762
2022-07-16 06:22:35 - train: epoch 0067, iter [04900, 05004], lr: 0.040915, loss: 1.9229
2022-07-16 06:23:08 - train: epoch 0067, iter [05000, 05004], lr: 0.040889, loss: 1.9676
2022-07-16 06:23:08 - train: epoch 067, train_loss: 1.9154
2022-07-16 06:24:24 - eval: epoch: 067, acc1: 59.754%, acc5: 82.872%, test_loss: 1.6944, per_image_load_time: 1.859ms, per_image_inference_time: 0.221ms
2022-07-16 06:24:24 - until epoch: 067, best_acc1: 60.038%
2022-07-16 06:24:24 - epoch 068 lr: 0.040888
2022-07-16 06:25:03 - train: epoch 0068, iter [00100, 05004], lr: 0.040863, loss: 1.9660
2022-07-16 06:25:37 - train: epoch 0068, iter [00200, 05004], lr: 0.040837, loss: 1.9808
2022-07-16 06:26:12 - train: epoch 0068, iter [00300, 05004], lr: 0.040811, loss: 2.0542
2022-07-16 06:26:45 - train: epoch 0068, iter [00400, 05004], lr: 0.040785, loss: 1.7461
2022-07-16 06:27:18 - train: epoch 0068, iter [00500, 05004], lr: 0.040760, loss: 1.8879
2022-07-16 06:27:52 - train: epoch 0068, iter [00600, 05004], lr: 0.040734, loss: 1.9132
2022-07-16 06:28:26 - train: epoch 0068, iter [00700, 05004], lr: 0.040708, loss: 2.1378
2022-07-16 06:29:00 - train: epoch 0068, iter [00800, 05004], lr: 0.040683, loss: 1.9372
2022-07-16 06:29:35 - train: epoch 0068, iter [00900, 05004], lr: 0.040657, loss: 1.7832
2022-07-16 06:30:08 - train: epoch 0068, iter [01000, 05004], lr: 0.040631, loss: 1.8395
2022-07-16 06:30:42 - train: epoch 0068, iter [01100, 05004], lr: 0.040605, loss: 2.2645
2022-07-16 06:31:16 - train: epoch 0068, iter [01200, 05004], lr: 0.040580, loss: 1.7606
2022-07-16 06:31:49 - train: epoch 0068, iter [01300, 05004], lr: 0.040554, loss: 1.7956
2022-07-16 06:32:25 - train: epoch 0068, iter [01400, 05004], lr: 0.040528, loss: 1.9275
2022-07-16 06:32:59 - train: epoch 0068, iter [01500, 05004], lr: 0.040503, loss: 2.0538
2022-07-16 06:33:33 - train: epoch 0068, iter [01600, 05004], lr: 0.040477, loss: 1.8423
2022-07-16 06:34:07 - train: epoch 0068, iter [01700, 05004], lr: 0.040451, loss: 2.0932
2022-07-16 06:34:42 - train: epoch 0068, iter [01800, 05004], lr: 0.040426, loss: 2.0260
2022-07-16 06:35:15 - train: epoch 0068, iter [01900, 05004], lr: 0.040400, loss: 2.0442
2022-07-16 06:35:50 - train: epoch 0068, iter [02000, 05004], lr: 0.040374, loss: 1.9432
2022-07-16 06:36:24 - train: epoch 0068, iter [02100, 05004], lr: 0.040349, loss: 1.9104
2022-07-16 06:36:58 - train: epoch 0068, iter [02200, 05004], lr: 0.040323, loss: 1.9271
2022-07-16 06:37:32 - train: epoch 0068, iter [02300, 05004], lr: 0.040297, loss: 1.8868
2022-07-16 06:38:06 - train: epoch 0068, iter [02400, 05004], lr: 0.040272, loss: 1.9166
2022-07-16 06:38:40 - train: epoch 0068, iter [02500, 05004], lr: 0.040246, loss: 2.0056
2022-07-16 06:39:14 - train: epoch 0068, iter [02600, 05004], lr: 0.040220, loss: 1.8786
2022-07-16 06:39:48 - train: epoch 0068, iter [02700, 05004], lr: 0.040195, loss: 1.8543
2022-07-16 06:40:22 - train: epoch 0068, iter [02800, 05004], lr: 0.040169, loss: 2.0553
2022-07-16 06:40:56 - train: epoch 0068, iter [02900, 05004], lr: 0.040143, loss: 2.0173
2022-07-16 06:41:30 - train: epoch 0068, iter [03000, 05004], lr: 0.040118, loss: 2.1239
2022-07-16 06:42:04 - train: epoch 0068, iter [03100, 05004], lr: 0.040092, loss: 1.6376
2022-07-16 06:42:39 - train: epoch 0068, iter [03200, 05004], lr: 0.040066, loss: 2.1542
2022-07-16 06:43:13 - train: epoch 0068, iter [03300, 05004], lr: 0.040041, loss: 1.8629
2022-07-16 06:43:47 - train: epoch 0068, iter [03400, 05004], lr: 0.040015, loss: 1.7937
2022-07-16 06:44:22 - train: epoch 0068, iter [03500, 05004], lr: 0.039990, loss: 1.9619
2022-07-16 06:44:55 - train: epoch 0068, iter [03600, 05004], lr: 0.039964, loss: 1.7687
2022-07-16 06:45:29 - train: epoch 0068, iter [03700, 05004], lr: 0.039938, loss: 1.8822
2022-07-16 06:46:03 - train: epoch 0068, iter [03800, 05004], lr: 0.039913, loss: 1.9810
2022-07-16 06:46:38 - train: epoch 0068, iter [03900, 05004], lr: 0.039887, loss: 1.9807
2022-07-16 06:47:12 - train: epoch 0068, iter [04000, 05004], lr: 0.039861, loss: 1.8056
2022-07-16 06:47:47 - train: epoch 0068, iter [04100, 05004], lr: 0.039836, loss: 1.6250
2022-07-16 06:48:20 - train: epoch 0068, iter [04200, 05004], lr: 0.039810, loss: 2.0234
2022-07-16 06:48:56 - train: epoch 0068, iter [04300, 05004], lr: 0.039785, loss: 2.0182
2022-07-16 06:49:30 - train: epoch 0068, iter [04400, 05004], lr: 0.039759, loss: 1.8666
2022-07-16 06:50:05 - train: epoch 0068, iter [04500, 05004], lr: 0.039733, loss: 2.0059
2022-07-16 06:50:39 - train: epoch 0068, iter [04600, 05004], lr: 0.039708, loss: 2.0384
2022-07-16 06:51:14 - train: epoch 0068, iter [04700, 05004], lr: 0.039682, loss: 2.1164
2022-07-16 06:51:47 - train: epoch 0068, iter [04800, 05004], lr: 0.039657, loss: 2.0062
2022-07-16 06:52:21 - train: epoch 0068, iter [04900, 05004], lr: 0.039631, loss: 2.1286
2022-07-16 06:52:53 - train: epoch 0068, iter [05000, 05004], lr: 0.039605, loss: 1.9234
2022-07-16 06:52:54 - train: epoch 068, train_loss: 1.9044
2022-07-16 06:54:09 - eval: epoch: 068, acc1: 60.098%, acc5: 83.358%, test_loss: 1.6740, per_image_load_time: 1.211ms, per_image_inference_time: 0.242ms
2022-07-16 06:54:09 - until epoch: 068, best_acc1: 60.098%
2022-07-16 06:54:09 - epoch 069 lr: 0.039604
2022-07-16 06:54:48 - train: epoch 0069, iter [00100, 05004], lr: 0.039579, loss: 2.2120
2022-07-16 06:55:23 - train: epoch 0069, iter [00200, 05004], lr: 0.039553, loss: 2.1698
2022-07-16 06:55:56 - train: epoch 0069, iter [00300, 05004], lr: 0.039528, loss: 1.9226
2022-07-16 06:56:30 - train: epoch 0069, iter [00400, 05004], lr: 0.039502, loss: 1.8313
2022-07-16 06:57:04 - train: epoch 0069, iter [00500, 05004], lr: 0.039477, loss: 1.8432
2022-07-16 06:57:37 - train: epoch 0069, iter [00600, 05004], lr: 0.039451, loss: 1.6788
2022-07-16 06:58:11 - train: epoch 0069, iter [00700, 05004], lr: 0.039425, loss: 1.8858
2022-07-16 06:58:45 - train: epoch 0069, iter [00800, 05004], lr: 0.039400, loss: 1.8410
2022-07-16 06:59:19 - train: epoch 0069, iter [00900, 05004], lr: 0.039374, loss: 1.7909
2022-07-16 06:59:52 - train: epoch 0069, iter [01000, 05004], lr: 0.039349, loss: 1.7525
2022-07-16 07:00:27 - train: epoch 0069, iter [01100, 05004], lr: 0.039323, loss: 1.8181
2022-07-16 07:01:01 - train: epoch 0069, iter [01200, 05004], lr: 0.039298, loss: 1.7943
2022-07-16 07:01:35 - train: epoch 0069, iter [01300, 05004], lr: 0.039272, loss: 2.1141
2022-07-16 07:02:08 - train: epoch 0069, iter [01400, 05004], lr: 0.039246, loss: 1.9431
2022-07-16 07:02:42 - train: epoch 0069, iter [01500, 05004], lr: 0.039221, loss: 1.8783
2022-07-16 07:03:18 - train: epoch 0069, iter [01600, 05004], lr: 0.039195, loss: 2.0618
2022-07-16 07:03:52 - train: epoch 0069, iter [01700, 05004], lr: 0.039170, loss: 1.8232
2022-07-16 07:04:25 - train: epoch 0069, iter [01800, 05004], lr: 0.039144, loss: 1.6795
2022-07-16 07:04:59 - train: epoch 0069, iter [01900, 05004], lr: 0.039119, loss: 1.8928
2022-07-16 07:05:34 - train: epoch 0069, iter [02000, 05004], lr: 0.039093, loss: 1.7332
2022-07-16 07:06:07 - train: epoch 0069, iter [02100, 05004], lr: 0.039068, loss: 1.9357
2022-07-16 07:06:42 - train: epoch 0069, iter [02200, 05004], lr: 0.039042, loss: 1.8841
2022-07-16 07:07:15 - train: epoch 0069, iter [02300, 05004], lr: 0.039017, loss: 1.9215
2022-07-16 07:07:50 - train: epoch 0069, iter [02400, 05004], lr: 0.038991, loss: 1.9652
2022-07-16 07:08:23 - train: epoch 0069, iter [02500, 05004], lr: 0.038966, loss: 1.7915
2022-07-16 07:08:57 - train: epoch 0069, iter [02600, 05004], lr: 0.038940, loss: 1.8747
2022-07-16 07:09:31 - train: epoch 0069, iter [02700, 05004], lr: 0.038915, loss: 2.1228
2022-07-16 07:10:06 - train: epoch 0069, iter [02800, 05004], lr: 0.038889, loss: 2.0080
2022-07-16 07:10:39 - train: epoch 0069, iter [02900, 05004], lr: 0.038864, loss: 1.6570
2022-07-16 07:11:14 - train: epoch 0069, iter [03000, 05004], lr: 0.038838, loss: 1.7633
2022-07-16 07:11:48 - train: epoch 0069, iter [03100, 05004], lr: 0.038813, loss: 1.8482
2022-07-16 07:12:23 - train: epoch 0069, iter [03200, 05004], lr: 0.038787, loss: 1.8666
2022-07-16 07:12:56 - train: epoch 0069, iter [03300, 05004], lr: 0.038762, loss: 1.7421
2022-07-16 07:13:32 - train: epoch 0069, iter [03400, 05004], lr: 0.038736, loss: 1.8035
2022-07-16 07:14:05 - train: epoch 0069, iter [03500, 05004], lr: 0.038711, loss: 1.7971
2022-07-16 07:14:40 - train: epoch 0069, iter [03600, 05004], lr: 0.038685, loss: 1.8371
2022-07-16 07:15:14 - train: epoch 0069, iter [03700, 05004], lr: 0.038660, loss: 2.0061
2022-07-16 07:15:49 - train: epoch 0069, iter [03800, 05004], lr: 0.038634, loss: 1.9721
2022-07-16 07:16:22 - train: epoch 0069, iter [03900, 05004], lr: 0.038609, loss: 1.8608
2022-07-16 07:16:57 - train: epoch 0069, iter [04000, 05004], lr: 0.038583, loss: 1.9839
2022-07-16 07:17:30 - train: epoch 0069, iter [04100, 05004], lr: 0.038558, loss: 1.9879
2022-07-16 07:18:04 - train: epoch 0069, iter [04200, 05004], lr: 0.038532, loss: 1.6987
2022-07-16 07:18:39 - train: epoch 0069, iter [04300, 05004], lr: 0.038507, loss: 1.9631
2022-07-16 07:19:13 - train: epoch 0069, iter [04400, 05004], lr: 0.038481, loss: 1.9877
2022-07-16 07:19:47 - train: epoch 0069, iter [04500, 05004], lr: 0.038456, loss: 1.9730
2022-07-16 07:20:21 - train: epoch 0069, iter [04600, 05004], lr: 0.038431, loss: 2.1939
2022-07-16 07:20:56 - train: epoch 0069, iter [04700, 05004], lr: 0.038405, loss: 1.8893
2022-07-16 07:21:31 - train: epoch 0069, iter [04800, 05004], lr: 0.038380, loss: 1.8833
2022-07-16 07:22:05 - train: epoch 0069, iter [04900, 05004], lr: 0.038354, loss: 1.7769
2022-07-16 07:22:37 - train: epoch 0069, iter [05000, 05004], lr: 0.038329, loss: 1.9430
2022-07-16 07:22:38 - train: epoch 069, train_loss: 1.8914
2022-07-16 07:23:53 - eval: epoch: 069, acc1: 59.998%, acc5: 83.194%, test_loss: 1.6974, per_image_load_time: 1.711ms, per_image_inference_time: 0.241ms
2022-07-16 07:23:53 - until epoch: 069, best_acc1: 60.098%
2022-07-16 07:23:53 - epoch 070 lr: 0.038327
2022-07-16 07:24:32 - train: epoch 0070, iter [00100, 05004], lr: 0.038302, loss: 1.9479
2022-07-16 07:25:06 - train: epoch 0070, iter [00200, 05004], lr: 0.038277, loss: 1.8375
2022-07-16 07:25:40 - train: epoch 0070, iter [00300, 05004], lr: 0.038251, loss: 2.0055
2022-07-16 07:26:14 - train: epoch 0070, iter [00400, 05004], lr: 0.038226, loss: 1.8336
2022-07-16 07:26:48 - train: epoch 0070, iter [00500, 05004], lr: 0.038201, loss: 1.8328
2022-07-16 07:27:23 - train: epoch 0070, iter [00600, 05004], lr: 0.038175, loss: 1.7363
2022-07-16 07:27:57 - train: epoch 0070, iter [00700, 05004], lr: 0.038150, loss: 1.8315
2022-07-16 07:28:31 - train: epoch 0070, iter [00800, 05004], lr: 0.038124, loss: 1.8049
2022-07-16 07:29:05 - train: epoch 0070, iter [00900, 05004], lr: 0.038099, loss: 1.9323
2022-07-16 07:29:40 - train: epoch 0070, iter [01000, 05004], lr: 0.038074, loss: 1.8675
2022-07-16 07:30:14 - train: epoch 0070, iter [01100, 05004], lr: 0.038048, loss: 2.3191
2022-07-16 07:30:48 - train: epoch 0070, iter [01200, 05004], lr: 0.038023, loss: 1.7276
2022-07-16 07:31:22 - train: epoch 0070, iter [01300, 05004], lr: 0.037997, loss: 1.8266
2022-07-16 07:31:56 - train: epoch 0070, iter [01400, 05004], lr: 0.037972, loss: 1.8899
2022-07-16 07:32:30 - train: epoch 0070, iter [01500, 05004], lr: 0.037947, loss: 1.9204
2022-07-16 07:33:03 - train: epoch 0070, iter [01600, 05004], lr: 0.037921, loss: 2.0338
2022-07-16 07:33:38 - train: epoch 0070, iter [01700, 05004], lr: 0.037896, loss: 2.0185
2022-07-16 07:34:12 - train: epoch 0070, iter [01800, 05004], lr: 0.037870, loss: 1.7891
2022-07-16 07:34:46 - train: epoch 0070, iter [01900, 05004], lr: 0.037845, loss: 1.6665
2022-07-16 07:35:19 - train: epoch 0070, iter [02000, 05004], lr: 0.037820, loss: 1.8820
2022-07-16 07:35:54 - train: epoch 0070, iter [02100, 05004], lr: 0.037794, loss: 1.9811
2022-07-16 07:36:28 - train: epoch 0070, iter [02200, 05004], lr: 0.037769, loss: 1.9227
2022-07-16 07:37:02 - train: epoch 0070, iter [02300, 05004], lr: 0.037744, loss: 1.8177
2022-07-16 07:37:37 - train: epoch 0070, iter [02400, 05004], lr: 0.037718, loss: 2.0649
2022-07-16 07:38:10 - train: epoch 0070, iter [02500, 05004], lr: 0.037693, loss: 1.9832
2022-07-16 07:38:44 - train: epoch 0070, iter [02600, 05004], lr: 0.037667, loss: 1.9462
2022-07-16 07:39:19 - train: epoch 0070, iter [02700, 05004], lr: 0.037642, loss: 1.9688
2022-07-16 07:39:53 - train: epoch 0070, iter [02800, 05004], lr: 0.037617, loss: 1.9541
2022-07-16 07:40:27 - train: epoch 0070, iter [02900, 05004], lr: 0.037591, loss: 2.1359
2022-07-16 07:41:01 - train: epoch 0070, iter [03000, 05004], lr: 0.037566, loss: 1.8473
2022-07-16 07:41:35 - train: epoch 0070, iter [03100, 05004], lr: 0.037541, loss: 1.9229
2022-07-16 07:42:09 - train: epoch 0070, iter [03200, 05004], lr: 0.037515, loss: 2.1223
2022-07-16 07:42:43 - train: epoch 0070, iter [03300, 05004], lr: 0.037490, loss: 1.8060
2022-07-16 07:43:18 - train: epoch 0070, iter [03400, 05004], lr: 0.037465, loss: 1.8961
2022-07-16 07:43:53 - train: epoch 0070, iter [03500, 05004], lr: 0.037439, loss: 1.9485
2022-07-16 07:44:27 - train: epoch 0070, iter [03600, 05004], lr: 0.037414, loss: 2.0719
2022-07-16 07:45:01 - train: epoch 0070, iter [03700, 05004], lr: 0.037389, loss: 1.8686
2022-07-16 07:45:34 - train: epoch 0070, iter [03800, 05004], lr: 0.037364, loss: 1.9794
2022-07-16 07:46:09 - train: epoch 0070, iter [03900, 05004], lr: 0.037338, loss: 1.6970
2022-07-16 07:46:43 - train: epoch 0070, iter [04000, 05004], lr: 0.037313, loss: 1.7513
2022-07-16 07:47:17 - train: epoch 0070, iter [04100, 05004], lr: 0.037288, loss: 1.7573
2022-07-16 07:47:51 - train: epoch 0070, iter [04200, 05004], lr: 0.037262, loss: 1.9074
2022-07-16 07:48:25 - train: epoch 0070, iter [04300, 05004], lr: 0.037237, loss: 1.9928
2022-07-16 07:49:00 - train: epoch 0070, iter [04400, 05004], lr: 0.037212, loss: 1.8754
2022-07-16 07:49:34 - train: epoch 0070, iter [04500, 05004], lr: 0.037186, loss: 2.0081
2022-07-16 07:50:08 - train: epoch 0070, iter [04600, 05004], lr: 0.037161, loss: 2.0510
2022-07-16 07:50:44 - train: epoch 0070, iter [04700, 05004], lr: 0.037136, loss: 1.9974
2022-07-16 07:51:17 - train: epoch 0070, iter [04800, 05004], lr: 0.037111, loss: 2.0317
2022-07-16 07:51:52 - train: epoch 0070, iter [04900, 05004], lr: 0.037085, loss: 1.9712
2022-07-16 07:52:24 - train: epoch 0070, iter [05000, 05004], lr: 0.037060, loss: 1.9003
2022-07-16 07:52:25 - train: epoch 070, train_loss: 1.8806
2022-07-16 07:53:40 - eval: epoch: 070, acc1: 60.534%, acc5: 83.454%, test_loss: 1.6612, per_image_load_time: 1.264ms, per_image_inference_time: 0.211ms
2022-07-16 07:53:40 - until epoch: 070, best_acc1: 60.534%
2022-07-16 07:53:40 - epoch 071 lr: 0.037059
2022-07-16 07:54:19 - train: epoch 0071, iter [00100, 05004], lr: 0.037034, loss: 1.6493
2022-07-16 07:54:53 - train: epoch 0071, iter [00200, 05004], lr: 0.037009, loss: 1.7493
2022-07-16 07:55:28 - train: epoch 0071, iter [00300, 05004], lr: 0.036983, loss: 1.8028
2022-07-16 07:56:01 - train: epoch 0071, iter [00400, 05004], lr: 0.036958, loss: 1.9628
2022-07-16 07:56:36 - train: epoch 0071, iter [00500, 05004], lr: 0.036933, loss: 1.8784
2022-07-16 07:57:09 - train: epoch 0071, iter [00600, 05004], lr: 0.036908, loss: 1.9003
2022-07-16 07:57:43 - train: epoch 0071, iter [00700, 05004], lr: 0.036882, loss: 2.0771
2022-07-16 07:58:16 - train: epoch 0071, iter [00800, 05004], lr: 0.036857, loss: 1.7855
2022-07-16 07:58:52 - train: epoch 0071, iter [00900, 05004], lr: 0.036832, loss: 1.9980
2022-07-16 07:59:25 - train: epoch 0071, iter [01000, 05004], lr: 0.036807, loss: 1.9718
2022-07-16 07:59:59 - train: epoch 0071, iter [01100, 05004], lr: 0.036781, loss: 2.0141
2022-07-16 08:00:34 - train: epoch 0071, iter [01200, 05004], lr: 0.036756, loss: 1.8597
2022-07-16 08:01:07 - train: epoch 0071, iter [01300, 05004], lr: 0.036731, loss: 1.9139
2022-07-16 08:01:43 - train: epoch 0071, iter [01400, 05004], lr: 0.036706, loss: 1.9118
2022-07-16 08:02:16 - train: epoch 0071, iter [01500, 05004], lr: 0.036680, loss: 1.7107
2022-07-16 08:02:51 - train: epoch 0071, iter [01600, 05004], lr: 0.036655, loss: 1.6955
2022-07-16 08:03:25 - train: epoch 0071, iter [01700, 05004], lr: 0.036630, loss: 1.9724
2022-07-16 08:03:59 - train: epoch 0071, iter [01800, 05004], lr: 0.036605, loss: 1.7749
2022-07-16 08:04:33 - train: epoch 0071, iter [01900, 05004], lr: 0.036580, loss: 1.8087
2022-07-16 08:05:07 - train: epoch 0071, iter [02000, 05004], lr: 0.036554, loss: 1.9313
2022-07-16 08:05:42 - train: epoch 0071, iter [02100, 05004], lr: 0.036529, loss: 1.6839
2022-07-16 08:06:16 - train: epoch 0071, iter [02200, 05004], lr: 0.036504, loss: 1.7391
2022-07-16 08:06:49 - train: epoch 0071, iter [02300, 05004], lr: 0.036479, loss: 1.8434
2022-07-16 08:07:25 - train: epoch 0071, iter [02400, 05004], lr: 0.036454, loss: 1.7654
2022-07-16 08:07:59 - train: epoch 0071, iter [02500, 05004], lr: 0.036428, loss: 2.1521
2022-07-16 08:08:33 - train: epoch 0071, iter [02600, 05004], lr: 0.036403, loss: 1.6136
2022-07-16 08:09:06 - train: epoch 0071, iter [02700, 05004], lr: 0.036378, loss: 1.9420
2022-07-16 08:09:41 - train: epoch 0071, iter [02800, 05004], lr: 0.036353, loss: 1.8958
2022-07-16 08:10:15 - train: epoch 0071, iter [02900, 05004], lr: 0.036328, loss: 1.7334
2022-07-16 08:10:50 - train: epoch 0071, iter [03000, 05004], lr: 0.036303, loss: 2.0400
2022-07-16 08:11:24 - train: epoch 0071, iter [03100, 05004], lr: 0.036277, loss: 1.7276
2022-07-16 08:12:00 - train: epoch 0071, iter [03200, 05004], lr: 0.036252, loss: 1.6356
2022-07-16 08:12:32 - train: epoch 0071, iter [03300, 05004], lr: 0.036227, loss: 1.8125
2022-07-16 08:13:07 - train: epoch 0071, iter [03400, 05004], lr: 0.036202, loss: 1.7850
2022-07-16 08:13:41 - train: epoch 0071, iter [03500, 05004], lr: 0.036177, loss: 2.0283
2022-07-16 08:14:16 - train: epoch 0071, iter [03600, 05004], lr: 0.036152, loss: 2.0202
2022-07-16 08:14:50 - train: epoch 0071, iter [03700, 05004], lr: 0.036127, loss: 1.8738
2022-07-16 08:15:24 - train: epoch 0071, iter [03800, 05004], lr: 0.036101, loss: 1.7697
2022-07-16 08:15:59 - train: epoch 0071, iter [03900, 05004], lr: 0.036076, loss: 1.8589
2022-07-16 08:16:33 - train: epoch 0071, iter [04000, 05004], lr: 0.036051, loss: 2.0659
2022-07-16 08:17:08 - train: epoch 0071, iter [04100, 05004], lr: 0.036026, loss: 1.7933
2022-07-16 08:17:42 - train: epoch 0071, iter [04200, 05004], lr: 0.036001, loss: 1.9822
2022-07-16 08:18:16 - train: epoch 0071, iter [04300, 05004], lr: 0.035976, loss: 1.8559
2022-07-16 08:18:51 - train: epoch 0071, iter [04400, 05004], lr: 0.035951, loss: 2.0139
2022-07-16 08:19:24 - train: epoch 0071, iter [04500, 05004], lr: 0.035926, loss: 1.8210
2022-07-16 08:19:59 - train: epoch 0071, iter [04600, 05004], lr: 0.035901, loss: 1.9517
2022-07-16 08:20:34 - train: epoch 0071, iter [04700, 05004], lr: 0.035875, loss: 1.7487
2022-07-16 08:21:09 - train: epoch 0071, iter [04800, 05004], lr: 0.035850, loss: 1.8583
2022-07-16 08:21:42 - train: epoch 0071, iter [04900, 05004], lr: 0.035825, loss: 1.6755
2022-07-16 08:22:15 - train: epoch 0071, iter [05000, 05004], lr: 0.035800, loss: 1.7843
2022-07-16 08:22:15 - train: epoch 071, train_loss: 1.8668
2022-07-16 08:23:31 - eval: epoch: 071, acc1: 60.590%, acc5: 83.644%, test_loss: 1.6657, per_image_load_time: 2.254ms, per_image_inference_time: 0.227ms
2022-07-16 08:23:31 - until epoch: 071, best_acc1: 60.590%
2022-07-16 08:23:31 - epoch 072 lr: 0.035799
2022-07-16 08:24:10 - train: epoch 0072, iter [00100, 05004], lr: 0.035774, loss: 2.0394
2022-07-16 08:24:43 - train: epoch 0072, iter [00200, 05004], lr: 0.035749, loss: 1.6796
2022-07-16 08:25:18 - train: epoch 0072, iter [00300, 05004], lr: 0.035724, loss: 1.7385
2022-07-16 08:25:52 - train: epoch 0072, iter [00400, 05004], lr: 0.035699, loss: 1.8187
2022-07-16 08:26:25 - train: epoch 0072, iter [00500, 05004], lr: 0.035674, loss: 1.6232
2022-07-16 08:27:00 - train: epoch 0072, iter [00600, 05004], lr: 0.035649, loss: 1.8093
2022-07-16 08:27:34 - train: epoch 0072, iter [00700, 05004], lr: 0.035624, loss: 1.7790
2022-07-16 08:28:07 - train: epoch 0072, iter [00800, 05004], lr: 0.035599, loss: 2.0674
2022-07-16 08:28:42 - train: epoch 0072, iter [00900, 05004], lr: 0.035574, loss: 1.6600
2022-07-16 08:29:15 - train: epoch 0072, iter [01000, 05004], lr: 0.035549, loss: 1.8760
2022-07-16 08:29:49 - train: epoch 0072, iter [01100, 05004], lr: 0.035524, loss: 2.0055
2022-07-16 08:30:23 - train: epoch 0072, iter [01200, 05004], lr: 0.035499, loss: 1.7662
2022-07-16 08:30:58 - train: epoch 0072, iter [01300, 05004], lr: 0.035474, loss: 1.7866
2022-07-16 08:31:31 - train: epoch 0072, iter [01400, 05004], lr: 0.035448, loss: 1.9355
2022-07-16 08:32:07 - train: epoch 0072, iter [01500, 05004], lr: 0.035423, loss: 1.8414
2022-07-16 08:32:41 - train: epoch 0072, iter [01600, 05004], lr: 0.035398, loss: 1.9702
2022-07-16 08:33:15 - train: epoch 0072, iter [01700, 05004], lr: 0.035373, loss: 1.6913
2022-07-16 08:33:49 - train: epoch 0072, iter [01800, 05004], lr: 0.035348, loss: 1.6780
2022-07-16 08:34:23 - train: epoch 0072, iter [01900, 05004], lr: 0.035323, loss: 1.7927
2022-07-16 08:34:58 - train: epoch 0072, iter [02000, 05004], lr: 0.035298, loss: 1.9881
2022-07-16 08:35:33 - train: epoch 0072, iter [02100, 05004], lr: 0.035273, loss: 1.9851
2022-07-16 08:36:07 - train: epoch 0072, iter [02200, 05004], lr: 0.035248, loss: 2.0369
2022-07-16 08:36:41 - train: epoch 0072, iter [02300, 05004], lr: 0.035223, loss: 1.9815
2022-07-16 08:37:16 - train: epoch 0072, iter [02400, 05004], lr: 0.035198, loss: 1.8092
2022-07-16 08:37:50 - train: epoch 0072, iter [02500, 05004], lr: 0.035173, loss: 1.7730
2022-07-16 08:38:25 - train: epoch 0072, iter [02600, 05004], lr: 0.035148, loss: 1.6685
2022-07-16 08:38:57 - train: epoch 0072, iter [02700, 05004], lr: 0.035123, loss: 1.8545
2022-07-16 08:39:33 - train: epoch 0072, iter [02800, 05004], lr: 0.035098, loss: 1.9567
2022-07-16 08:40:07 - train: epoch 0072, iter [02900, 05004], lr: 0.035074, loss: 1.7644
2022-07-16 08:40:41 - train: epoch 0072, iter [03000, 05004], lr: 0.035049, loss: 1.7726
2022-07-16 08:41:16 - train: epoch 0072, iter [03100, 05004], lr: 0.035024, loss: 1.9330
2022-07-16 08:41:51 - train: epoch 0072, iter [03200, 05004], lr: 0.034999, loss: 1.7581
2022-07-16 08:42:24 - train: epoch 0072, iter [03300, 05004], lr: 0.034974, loss: 1.7901
2022-07-16 08:42:58 - train: epoch 0072, iter [03400, 05004], lr: 0.034949, loss: 2.0762
2022-07-16 08:43:32 - train: epoch 0072, iter [03500, 05004], lr: 0.034924, loss: 1.9686
2022-07-16 08:44:07 - train: epoch 0072, iter [03600, 05004], lr: 0.034899, loss: 1.8404
2022-07-16 08:44:40 - train: epoch 0072, iter [03700, 05004], lr: 0.034874, loss: 1.9572
2022-07-16 08:45:15 - train: epoch 0072, iter [03800, 05004], lr: 0.034849, loss: 1.9764
2022-07-16 08:45:49 - train: epoch 0072, iter [03900, 05004], lr: 0.034824, loss: 2.0066
2022-07-16 08:46:24 - train: epoch 0072, iter [04000, 05004], lr: 0.034799, loss: 1.8655
2022-07-16 08:46:59 - train: epoch 0072, iter [04100, 05004], lr: 0.034774, loss: 2.0681
2022-07-16 08:47:33 - train: epoch 0072, iter [04200, 05004], lr: 0.034749, loss: 1.8989
2022-07-16 08:48:07 - train: epoch 0072, iter [04300, 05004], lr: 0.034724, loss: 1.8482
2022-07-16 08:48:42 - train: epoch 0072, iter [04400, 05004], lr: 0.034699, loss: 1.7774
2022-07-16 08:49:16 - train: epoch 0072, iter [04500, 05004], lr: 0.034675, loss: 2.0163
2022-07-16 08:49:50 - train: epoch 0072, iter [04600, 05004], lr: 0.034650, loss: 1.7935
2022-07-16 08:50:25 - train: epoch 0072, iter [04700, 05004], lr: 0.034625, loss: 1.8578
2022-07-16 08:50:59 - train: epoch 0072, iter [04800, 05004], lr: 0.034600, loss: 2.1546
2022-07-16 08:51:32 - train: epoch 0072, iter [04900, 05004], lr: 0.034575, loss: 1.9328
2022-07-16 08:52:06 - train: epoch 0072, iter [05000, 05004], lr: 0.034550, loss: 1.8116
2022-07-16 08:52:07 - train: epoch 072, train_loss: 1.8575
2022-07-16 08:53:23 - eval: epoch: 072, acc1: 60.630%, acc5: 83.616%, test_loss: 1.6532, per_image_load_time: 2.496ms, per_image_inference_time: 0.256ms
2022-07-16 08:53:23 - until epoch: 072, best_acc1: 60.630%
2022-07-16 08:53:23 - epoch 073 lr: 0.034549
2022-07-16 08:54:03 - train: epoch 0073, iter [00100, 05004], lr: 0.034524, loss: 2.1757
2022-07-16 08:54:37 - train: epoch 0073, iter [00200, 05004], lr: 0.034499, loss: 1.8006
2022-07-16 08:55:11 - train: epoch 0073, iter [00300, 05004], lr: 0.034475, loss: 1.8360
2022-07-16 08:55:45 - train: epoch 0073, iter [00400, 05004], lr: 0.034450, loss: 1.5117
2022-07-16 08:56:18 - train: epoch 0073, iter [00500, 05004], lr: 0.034425, loss: 1.7234
2022-07-16 08:56:52 - train: epoch 0073, iter [00600, 05004], lr: 0.034400, loss: 1.7471
2022-07-16 08:57:27 - train: epoch 0073, iter [00700, 05004], lr: 0.034375, loss: 1.9085
2022-07-16 08:58:02 - train: epoch 0073, iter [00800, 05004], lr: 0.034350, loss: 1.7490
2022-07-16 08:58:35 - train: epoch 0073, iter [00900, 05004], lr: 0.034325, loss: 1.6156
2022-07-16 08:59:10 - train: epoch 0073, iter [01000, 05004], lr: 0.034301, loss: 1.6173
2022-07-16 08:59:44 - train: epoch 0073, iter [01100, 05004], lr: 0.034276, loss: 1.9208
2022-07-16 09:00:19 - train: epoch 0073, iter [01200, 05004], lr: 0.034251, loss: 1.8632
2022-07-16 09:00:53 - train: epoch 0073, iter [01300, 05004], lr: 0.034226, loss: 1.9109
2022-07-16 09:01:28 - train: epoch 0073, iter [01400, 05004], lr: 0.034201, loss: 1.7627
2022-07-16 09:02:02 - train: epoch 0073, iter [01500, 05004], lr: 0.034176, loss: 1.7547
2022-07-16 09:02:36 - train: epoch 0073, iter [01600, 05004], lr: 0.034152, loss: 1.8461
2022-07-16 09:03:10 - train: epoch 0073, iter [01700, 05004], lr: 0.034127, loss: 2.1411
2022-07-16 09:03:44 - train: epoch 0073, iter [01800, 05004], lr: 0.034102, loss: 1.5753
2022-07-16 09:04:19 - train: epoch 0073, iter [01900, 05004], lr: 0.034077, loss: 1.9419
2022-07-16 09:04:53 - train: epoch 0073, iter [02000, 05004], lr: 0.034052, loss: 1.5958
2022-07-16 09:05:27 - train: epoch 0073, iter [02100, 05004], lr: 0.034028, loss: 1.8842
2022-07-16 09:06:01 - train: epoch 0073, iter [02200, 05004], lr: 0.034003, loss: 2.0456
2022-07-16 09:06:36 - train: epoch 0073, iter [02300, 05004], lr: 0.033978, loss: 1.9400
2022-07-16 09:07:11 - train: epoch 0073, iter [02400, 05004], lr: 0.033953, loss: 1.7579
2022-07-16 09:07:44 - train: epoch 0073, iter [02500, 05004], lr: 0.033929, loss: 2.1498
2022-07-16 09:08:18 - train: epoch 0073, iter [02600, 05004], lr: 0.033904, loss: 1.8342
2022-07-16 09:08:53 - train: epoch 0073, iter [02700, 05004], lr: 0.033879, loss: 1.8591
2022-07-16 09:09:27 - train: epoch 0073, iter [02800, 05004], lr: 0.033854, loss: 1.8183
2022-07-16 09:10:01 - train: epoch 0073, iter [02900, 05004], lr: 0.033829, loss: 1.9185
2022-07-16 09:10:36 - train: epoch 0073, iter [03000, 05004], lr: 0.033805, loss: 1.6668
2022-07-16 09:11:10 - train: epoch 0073, iter [03100, 05004], lr: 0.033780, loss: 1.7753
2022-07-16 09:11:44 - train: epoch 0073, iter [03200, 05004], lr: 0.033755, loss: 1.7717
2022-07-16 09:12:18 - train: epoch 0073, iter [03300, 05004], lr: 0.033730, loss: 1.9423
2022-07-16 09:12:53 - train: epoch 0073, iter [03400, 05004], lr: 0.033706, loss: 1.7839
2022-07-16 09:13:27 - train: epoch 0073, iter [03500, 05004], lr: 0.033681, loss: 1.8629
2022-07-16 09:14:02 - train: epoch 0073, iter [03600, 05004], lr: 0.033656, loss: 1.7484
2022-07-16 09:14:36 - train: epoch 0073, iter [03700, 05004], lr: 0.033632, loss: 1.8691
2022-07-16 09:15:10 - train: epoch 0073, iter [03800, 05004], lr: 0.033607, loss: 2.0366
2022-07-16 09:15:44 - train: epoch 0073, iter [03900, 05004], lr: 0.033582, loss: 1.8339
2022-07-16 09:16:19 - train: epoch 0073, iter [04000, 05004], lr: 0.033557, loss: 1.9968
2022-07-16 09:16:53 - train: epoch 0073, iter [04100, 05004], lr: 0.033533, loss: 1.8429
2022-07-16 09:17:27 - train: epoch 0073, iter [04200, 05004], lr: 0.033508, loss: 2.0299
2022-07-16 09:18:02 - train: epoch 0073, iter [04300, 05004], lr: 0.033483, loss: 1.8533
2022-07-16 09:18:36 - train: epoch 0073, iter [04400, 05004], lr: 0.033459, loss: 2.0155
2022-07-16 09:19:11 - train: epoch 0073, iter [04500, 05004], lr: 0.033434, loss: 1.8614
2022-07-16 09:19:45 - train: epoch 0073, iter [04600, 05004], lr: 0.033409, loss: 2.0529
2022-07-16 09:20:20 - train: epoch 0073, iter [04700, 05004], lr: 0.033385, loss: 1.5946
2022-07-16 09:20:55 - train: epoch 0073, iter [04800, 05004], lr: 0.033360, loss: 1.9307
2022-07-16 09:21:28 - train: epoch 0073, iter [04900, 05004], lr: 0.033335, loss: 1.7533
2022-07-16 09:22:02 - train: epoch 0073, iter [05000, 05004], lr: 0.033311, loss: 2.0032
2022-07-16 09:22:02 - train: epoch 073, train_loss: 1.8450
2022-07-16 09:23:18 - eval: epoch: 073, acc1: 60.806%, acc5: 83.422%, test_loss: 1.6610, per_image_load_time: 1.969ms, per_image_inference_time: 0.214ms
2022-07-16 09:23:18 - until epoch: 073, best_acc1: 60.806%
2022-07-16 09:23:18 - epoch 074 lr: 0.033309
2022-07-16 09:23:58 - train: epoch 0074, iter [00100, 05004], lr: 0.033285, loss: 1.7423
2022-07-16 09:24:32 - train: epoch 0074, iter [00200, 05004], lr: 0.033260, loss: 1.9241
2022-07-16 09:25:06 - train: epoch 0074, iter [00300, 05004], lr: 0.033236, loss: 1.9450
2022-07-16 09:25:39 - train: epoch 0074, iter [00400, 05004], lr: 0.033211, loss: 2.1209
2022-07-16 09:26:15 - train: epoch 0074, iter [00500, 05004], lr: 0.033186, loss: 1.7608
2022-07-16 09:26:48 - train: epoch 0074, iter [00600, 05004], lr: 0.033162, loss: 1.8972
2022-07-16 09:27:23 - train: epoch 0074, iter [00700, 05004], lr: 0.033137, loss: 1.7936
2022-07-16 09:27:57 - train: epoch 0074, iter [00800, 05004], lr: 0.033113, loss: 1.9060
2022-07-16 09:28:30 - train: epoch 0074, iter [00900, 05004], lr: 0.033088, loss: 1.7077
2022-07-16 09:29:06 - train: epoch 0074, iter [01000, 05004], lr: 0.033063, loss: 1.9938
2022-07-16 09:29:38 - train: epoch 0074, iter [01100, 05004], lr: 0.033039, loss: 2.0227
2022-07-16 09:30:13 - train: epoch 0074, iter [01200, 05004], lr: 0.033014, loss: 1.7026
2022-07-16 09:30:47 - train: epoch 0074, iter [01300, 05004], lr: 0.032989, loss: 1.9221
2022-07-16 09:31:23 - train: epoch 0074, iter [01400, 05004], lr: 0.032965, loss: 1.7657
2022-07-16 09:31:55 - train: epoch 0074, iter [01500, 05004], lr: 0.032940, loss: 1.9341
2022-07-16 09:32:30 - train: epoch 0074, iter [01600, 05004], lr: 0.032916, loss: 1.6620
2022-07-16 09:33:04 - train: epoch 0074, iter [01700, 05004], lr: 0.032891, loss: 1.8602
2022-07-16 09:33:38 - train: epoch 0074, iter [01800, 05004], lr: 0.032867, loss: 2.2812
2022-07-16 09:34:13 - train: epoch 0074, iter [01900, 05004], lr: 0.032842, loss: 1.9999
2022-07-16 09:34:48 - train: epoch 0074, iter [02000, 05004], lr: 0.032817, loss: 1.5314
2022-07-16 09:35:22 - train: epoch 0074, iter [02100, 05004], lr: 0.032793, loss: 1.7375
2022-07-16 09:35:56 - train: epoch 0074, iter [02200, 05004], lr: 0.032768, loss: 2.1258
2022-07-16 09:36:30 - train: epoch 0074, iter [02300, 05004], lr: 0.032744, loss: 1.9082
2022-07-16 09:37:05 - train: epoch 0074, iter [02400, 05004], lr: 0.032719, loss: 1.8598
2022-07-16 09:37:39 - train: epoch 0074, iter [02500, 05004], lr: 0.032695, loss: 1.7192
2022-07-16 09:38:12 - train: epoch 0074, iter [02600, 05004], lr: 0.032670, loss: 1.5477
2022-07-16 09:38:47 - train: epoch 0074, iter [02700, 05004], lr: 0.032646, loss: 1.8047
2022-07-16 09:39:21 - train: epoch 0074, iter [02800, 05004], lr: 0.032621, loss: 2.1439
2022-07-16 09:39:56 - train: epoch 0074, iter [02900, 05004], lr: 0.032597, loss: 1.8170
2022-07-16 09:40:29 - train: epoch 0074, iter [03000, 05004], lr: 0.032572, loss: 1.8576
2022-07-16 09:41:04 - train: epoch 0074, iter [03100, 05004], lr: 0.032547, loss: 1.8852
2022-07-16 09:41:38 - train: epoch 0074, iter [03200, 05004], lr: 0.032523, loss: 1.6537
2022-07-16 09:42:13 - train: epoch 0074, iter [03300, 05004], lr: 0.032498, loss: 1.9131
2022-07-16 09:42:47 - train: epoch 0074, iter [03400, 05004], lr: 0.032474, loss: 1.9990
2022-07-16 09:43:22 - train: epoch 0074, iter [03500, 05004], lr: 0.032449, loss: 1.6686
2022-07-16 09:43:57 - train: epoch 0074, iter [03600, 05004], lr: 0.032425, loss: 1.8824
2022-07-16 09:44:30 - train: epoch 0074, iter [03700, 05004], lr: 0.032400, loss: 1.9681
2022-07-16 09:45:05 - train: epoch 0074, iter [03800, 05004], lr: 0.032376, loss: 1.7923
2022-07-16 09:45:38 - train: epoch 0074, iter [03900, 05004], lr: 0.032352, loss: 1.5778
2022-07-16 09:46:13 - train: epoch 0074, iter [04000, 05004], lr: 0.032327, loss: 1.7040
2022-07-16 09:46:47 - train: epoch 0074, iter [04100, 05004], lr: 0.032303, loss: 1.9915
2022-07-16 09:47:22 - train: epoch 0074, iter [04200, 05004], lr: 0.032278, loss: 1.8013
2022-07-16 09:47:56 - train: epoch 0074, iter [04300, 05004], lr: 0.032254, loss: 1.8009
2022-07-16 09:48:31 - train: epoch 0074, iter [04400, 05004], lr: 0.032229, loss: 1.7095
2022-07-16 09:49:04 - train: epoch 0074, iter [04500, 05004], lr: 0.032205, loss: 1.7375
2022-07-16 09:49:37 - train: epoch 0074, iter [04600, 05004], lr: 0.032180, loss: 1.9806
2022-07-16 09:50:12 - train: epoch 0074, iter [04700, 05004], lr: 0.032156, loss: 1.8420
2022-07-16 09:50:45 - train: epoch 0074, iter [04800, 05004], lr: 0.032131, loss: 1.9326
2022-07-16 09:51:19 - train: epoch 0074, iter [04900, 05004], lr: 0.032107, loss: 2.0884
2022-07-16 09:51:52 - train: epoch 0074, iter [05000, 05004], lr: 0.032083, loss: 1.8479
2022-07-16 09:51:53 - train: epoch 074, train_loss: 1.8309
2022-07-16 09:53:08 - eval: epoch: 074, acc1: 61.178%, acc5: 83.944%, test_loss: 1.6290, per_image_load_time: 2.590ms, per_image_inference_time: 0.240ms
2022-07-16 09:53:08 - until epoch: 074, best_acc1: 61.178%
2022-07-16 09:53:08 - epoch 075 lr: 0.032081
2022-07-16 09:53:47 - train: epoch 0075, iter [00100, 05004], lr: 0.032057, loss: 1.9115
2022-07-16 09:54:22 - train: epoch 0075, iter [00200, 05004], lr: 0.032033, loss: 1.7576
2022-07-16 09:54:55 - train: epoch 0075, iter [00300, 05004], lr: 0.032008, loss: 1.8555
2022-07-16 09:55:28 - train: epoch 0075, iter [00400, 05004], lr: 0.031984, loss: 1.8712
2022-07-16 09:56:03 - train: epoch 0075, iter [00500, 05004], lr: 0.031960, loss: 1.6744
2022-07-16 09:56:36 - train: epoch 0075, iter [00600, 05004], lr: 0.031935, loss: 1.7775
2022-07-16 09:57:10 - train: epoch 0075, iter [00700, 05004], lr: 0.031911, loss: 2.0696
2022-07-16 09:57:44 - train: epoch 0075, iter [00800, 05004], lr: 0.031886, loss: 1.9651
2022-07-16 09:58:18 - train: epoch 0075, iter [00900, 05004], lr: 0.031862, loss: 1.9039
2022-07-16 09:58:51 - train: epoch 0075, iter [01000, 05004], lr: 0.031838, loss: 1.6895
2022-07-16 09:59:26 - train: epoch 0075, iter [01100, 05004], lr: 0.031813, loss: 1.8403
2022-07-16 09:59:59 - train: epoch 0075, iter [01200, 05004], lr: 0.031789, loss: 1.7416
2022-07-16 10:00:33 - train: epoch 0075, iter [01300, 05004], lr: 0.031765, loss: 1.6352
2022-07-16 10:01:07 - train: epoch 0075, iter [01400, 05004], lr: 0.031740, loss: 1.7097
2022-07-16 10:01:41 - train: epoch 0075, iter [01500, 05004], lr: 0.031716, loss: 1.9698
2022-07-16 10:02:15 - train: epoch 0075, iter [01600, 05004], lr: 0.031691, loss: 1.4414
2022-07-16 10:02:48 - train: epoch 0075, iter [01700, 05004], lr: 0.031667, loss: 1.7990
2022-07-16 10:03:23 - train: epoch 0075, iter [01800, 05004], lr: 0.031643, loss: 1.7618
2022-07-16 10:03:57 - train: epoch 0075, iter [01900, 05004], lr: 0.031618, loss: 1.6782
2022-07-16 10:04:32 - train: epoch 0075, iter [02000, 05004], lr: 0.031594, loss: 1.7822
2022-07-16 10:05:06 - train: epoch 0075, iter [02100, 05004], lr: 0.031570, loss: 1.6728
2022-07-16 10:05:40 - train: epoch 0075, iter [02200, 05004], lr: 0.031546, loss: 1.8528
2022-07-16 10:06:14 - train: epoch 0075, iter [02300, 05004], lr: 0.031521, loss: 1.7083
2022-07-16 10:06:49 - train: epoch 0075, iter [02400, 05004], lr: 0.031497, loss: 1.8834
2022-07-16 10:07:23 - train: epoch 0075, iter [02500, 05004], lr: 0.031473, loss: 1.9566
2022-07-16 10:07:56 - train: epoch 0075, iter [02600, 05004], lr: 0.031448, loss: 1.8814
2022-07-16 10:08:31 - train: epoch 0075, iter [02700, 05004], lr: 0.031424, loss: 1.7158
2022-07-16 10:09:05 - train: epoch 0075, iter [02800, 05004], lr: 0.031400, loss: 1.8688
2022-07-16 10:09:39 - train: epoch 0075, iter [02900, 05004], lr: 0.031375, loss: 2.0110
2022-07-16 10:10:13 - train: epoch 0075, iter [03000, 05004], lr: 0.031351, loss: 1.9160
2022-07-16 10:10:47 - train: epoch 0075, iter [03100, 05004], lr: 0.031327, loss: 1.9938
2022-07-16 10:11:21 - train: epoch 0075, iter [03200, 05004], lr: 0.031303, loss: 1.7477
2022-07-16 10:11:56 - train: epoch 0075, iter [03300, 05004], lr: 0.031278, loss: 1.8704
2022-07-16 10:12:29 - train: epoch 0075, iter [03400, 05004], lr: 0.031254, loss: 1.7877
2022-07-16 10:13:04 - train: epoch 0075, iter [03500, 05004], lr: 0.031230, loss: 1.6460
2022-07-16 10:13:39 - train: epoch 0075, iter [03600, 05004], lr: 0.031206, loss: 1.9497
2022-07-16 10:14:12 - train: epoch 0075, iter [03700, 05004], lr: 0.031181, loss: 1.9042
2022-07-16 10:14:47 - train: epoch 0075, iter [03800, 05004], lr: 0.031157, loss: 1.6696
2022-07-16 10:15:19 - train: epoch 0075, iter [03900, 05004], lr: 0.031133, loss: 1.8849
2022-07-16 10:15:54 - train: epoch 0075, iter [04000, 05004], lr: 0.031109, loss: 1.7018
2022-07-16 10:16:29 - train: epoch 0075, iter [04100, 05004], lr: 0.031085, loss: 1.6154
2022-07-16 10:17:03 - train: epoch 0075, iter [04200, 05004], lr: 0.031060, loss: 1.8873
2022-07-16 10:17:36 - train: epoch 0075, iter [04300, 05004], lr: 0.031036, loss: 2.0184
2022-07-16 10:18:11 - train: epoch 0075, iter [04400, 05004], lr: 0.031012, loss: 1.6373
2022-07-16 10:18:45 - train: epoch 0075, iter [04500, 05004], lr: 0.030988, loss: 1.7297
2022-07-16 10:19:18 - train: epoch 0075, iter [04600, 05004], lr: 0.030964, loss: 1.5777
2022-07-16 10:19:53 - train: epoch 0075, iter [04700, 05004], lr: 0.030939, loss: 1.9764
2022-07-16 10:20:27 - train: epoch 0075, iter [04800, 05004], lr: 0.030915, loss: 1.9436
2022-07-16 10:21:01 - train: epoch 0075, iter [04900, 05004], lr: 0.030891, loss: 1.7165
2022-07-16 10:21:33 - train: epoch 0075, iter [05000, 05004], lr: 0.030867, loss: 1.8036
2022-07-16 10:21:34 - train: epoch 075, train_loss: 1.8193
2022-07-16 10:22:50 - eval: epoch: 075, acc1: 61.438%, acc5: 84.014%, test_loss: 1.6263, per_image_load_time: 1.521ms, per_image_inference_time: 0.227ms
2022-07-16 10:22:50 - until epoch: 075, best_acc1: 61.438%
2022-07-16 10:22:50 - epoch 076 lr: 0.030866
2022-07-16 10:23:30 - train: epoch 0076, iter [00100, 05004], lr: 0.030842, loss: 1.6353
2022-07-16 10:24:04 - train: epoch 0076, iter [00200, 05004], lr: 0.030818, loss: 1.7710
2022-07-16 10:24:38 - train: epoch 0076, iter [00300, 05004], lr: 0.030793, loss: 1.8162
2022-07-16 10:25:12 - train: epoch 0076, iter [00400, 05004], lr: 0.030769, loss: 1.7970
2022-07-16 10:25:46 - train: epoch 0076, iter [00500, 05004], lr: 0.030745, loss: 1.7731
2022-07-16 10:26:21 - train: epoch 0076, iter [00600, 05004], lr: 0.030721, loss: 1.7382
2022-07-16 10:26:55 - train: epoch 0076, iter [00700, 05004], lr: 0.030697, loss: 1.7711
2022-07-16 10:27:29 - train: epoch 0076, iter [00800, 05004], lr: 0.030673, loss: 1.7592
2022-07-16 10:28:03 - train: epoch 0076, iter [00900, 05004], lr: 0.030649, loss: 1.7522
2022-07-16 10:28:37 - train: epoch 0076, iter [01000, 05004], lr: 0.030624, loss: 1.7749
2022-07-16 10:29:12 - train: epoch 0076, iter [01100, 05004], lr: 0.030600, loss: 1.5959
2022-07-16 10:29:45 - train: epoch 0076, iter [01200, 05004], lr: 0.030576, loss: 1.7156
2022-07-16 10:30:18 - train: epoch 0076, iter [01300, 05004], lr: 0.030552, loss: 1.7826
2022-07-16 10:30:53 - train: epoch 0076, iter [01400, 05004], lr: 0.030528, loss: 1.7116
2022-07-16 10:31:27 - train: epoch 0076, iter [01500, 05004], lr: 0.030504, loss: 1.8088
2022-07-16 10:32:01 - train: epoch 0076, iter [01600, 05004], lr: 0.030480, loss: 1.7421
2022-07-16 10:32:36 - train: epoch 0076, iter [01700, 05004], lr: 0.030456, loss: 1.7147
2022-07-16 10:33:11 - train: epoch 0076, iter [01800, 05004], lr: 0.030432, loss: 1.5891
2022-07-16 10:33:45 - train: epoch 0076, iter [01900, 05004], lr: 0.030408, loss: 1.9543
2022-07-16 10:34:19 - train: epoch 0076, iter [02000, 05004], lr: 0.030384, loss: 1.9224
2022-07-16 10:34:52 - train: epoch 0076, iter [02100, 05004], lr: 0.030359, loss: 1.9020
2022-07-16 10:35:27 - train: epoch 0076, iter [02200, 05004], lr: 0.030335, loss: 1.8882
2022-07-16 10:36:01 - train: epoch 0076, iter [02300, 05004], lr: 0.030311, loss: 1.6967
2022-07-16 10:36:36 - train: epoch 0076, iter [02400, 05004], lr: 0.030287, loss: 1.9429
2022-07-16 10:37:09 - train: epoch 0076, iter [02500, 05004], lr: 0.030263, loss: 1.7578
2022-07-16 10:37:44 - train: epoch 0076, iter [02600, 05004], lr: 0.030239, loss: 2.0862
2022-07-16 10:38:17 - train: epoch 0076, iter [02700, 05004], lr: 0.030215, loss: 1.8598
2022-07-16 10:38:51 - train: epoch 0076, iter [02800, 05004], lr: 0.030191, loss: 1.8092
2022-07-16 10:39:26 - train: epoch 0076, iter [02900, 05004], lr: 0.030167, loss: 1.8317
2022-07-16 10:40:00 - train: epoch 0076, iter [03000, 05004], lr: 0.030143, loss: 1.6661
2022-07-16 10:40:33 - train: epoch 0076, iter [03100, 05004], lr: 0.030119, loss: 2.0045
2022-07-16 10:41:08 - train: epoch 0076, iter [03200, 05004], lr: 0.030095, loss: 1.8377
2022-07-16 10:41:41 - train: epoch 0076, iter [03300, 05004], lr: 0.030071, loss: 1.6432
2022-07-16 10:42:16 - train: epoch 0076, iter [03400, 05004], lr: 0.030047, loss: 1.7446
2022-07-16 10:42:49 - train: epoch 0076, iter [03500, 05004], lr: 0.030023, loss: 1.6441
2022-07-16 10:43:24 - train: epoch 0076, iter [03600, 05004], lr: 0.029999, loss: 1.5703
2022-07-16 10:43:58 - train: epoch 0076, iter [03700, 05004], lr: 0.029975, loss: 1.7058
2022-07-16 10:44:33 - train: epoch 0076, iter [03800, 05004], lr: 0.029951, loss: 1.6028
2022-07-16 10:45:06 - train: epoch 0076, iter [03900, 05004], lr: 0.029927, loss: 1.8741
2022-07-16 10:45:40 - train: epoch 0076, iter [04000, 05004], lr: 0.029903, loss: 1.8864
2022-07-16 10:46:15 - train: epoch 0076, iter [04100, 05004], lr: 0.029879, loss: 1.8718
2022-07-16 10:46:49 - train: epoch 0076, iter [04200, 05004], lr: 0.029855, loss: 1.8903
2022-07-16 10:47:24 - train: epoch 0076, iter [04300, 05004], lr: 0.029832, loss: 1.9587
2022-07-16 10:47:58 - train: epoch 0076, iter [04400, 05004], lr: 0.029808, loss: 1.7397
2022-07-16 10:48:32 - train: epoch 0076, iter [04500, 05004], lr: 0.029784, loss: 1.8344
2022-07-16 10:49:06 - train: epoch 0076, iter [04600, 05004], lr: 0.029760, loss: 1.7890
2022-07-16 10:49:41 - train: epoch 0076, iter [04700, 05004], lr: 0.029736, loss: 1.9123
2022-07-16 10:50:14 - train: epoch 0076, iter [04800, 05004], lr: 0.029712, loss: 1.6216
2022-07-16 10:50:50 - train: epoch 0076, iter [04900, 05004], lr: 0.029688, loss: 1.7106
2022-07-16 10:51:21 - train: epoch 0076, iter [05000, 05004], lr: 0.029664, loss: 1.7969
2022-07-16 10:51:23 - train: epoch 076, train_loss: 1.8071
2022-07-16 10:52:39 - eval: epoch: 076, acc1: 61.858%, acc5: 84.428%, test_loss: 1.5952, per_image_load_time: 1.586ms, per_image_inference_time: 0.239ms
2022-07-16 10:52:39 - until epoch: 076, best_acc1: 61.858%
2022-07-16 10:52:39 - epoch 077 lr: 0.029663
2022-07-16 10:53:18 - train: epoch 0077, iter [00100, 05004], lr: 0.029639, loss: 1.9823
2022-07-16 10:53:52 - train: epoch 0077, iter [00200, 05004], lr: 0.029615, loss: 1.8695
2022-07-16 10:54:26 - train: epoch 0077, iter [00300, 05004], lr: 0.029592, loss: 1.5563
2022-07-16 10:54:59 - train: epoch 0077, iter [00400, 05004], lr: 0.029568, loss: 1.8876
2022-07-16 10:55:33 - train: epoch 0077, iter [00500, 05004], lr: 0.029544, loss: 1.6328
2022-07-16 10:56:06 - train: epoch 0077, iter [00600, 05004], lr: 0.029520, loss: 1.5398
2022-07-16 10:56:40 - train: epoch 0077, iter [00700, 05004], lr: 0.029496, loss: 1.8276
2022-07-16 10:57:14 - train: epoch 0077, iter [00800, 05004], lr: 0.029472, loss: 1.8852
2022-07-16 10:57:48 - train: epoch 0077, iter [00900, 05004], lr: 0.029448, loss: 2.1031
2022-07-16 10:58:21 - train: epoch 0077, iter [01000, 05004], lr: 0.029424, loss: 1.5449
2022-07-16 10:58:57 - train: epoch 0077, iter [01100, 05004], lr: 0.029401, loss: 1.8623
2022-07-16 10:59:30 - train: epoch 0077, iter [01200, 05004], lr: 0.029377, loss: 1.9379
2022-07-16 11:00:05 - train: epoch 0077, iter [01300, 05004], lr: 0.029353, loss: 1.3752
2022-07-16 11:00:38 - train: epoch 0077, iter [01400, 05004], lr: 0.029329, loss: 1.9215
2022-07-16 11:01:12 - train: epoch 0077, iter [01500, 05004], lr: 0.029305, loss: 1.6670
2022-07-16 11:01:46 - train: epoch 0077, iter [01600, 05004], lr: 0.029282, loss: 1.7936
2022-07-16 11:02:19 - train: epoch 0077, iter [01700, 05004], lr: 0.029258, loss: 2.1458
2022-07-16 11:02:53 - train: epoch 0077, iter [01800, 05004], lr: 0.029234, loss: 1.6330
2022-07-16 11:03:27 - train: epoch 0077, iter [01900, 05004], lr: 0.029210, loss: 1.7167
2022-07-16 11:04:01 - train: epoch 0077, iter [02000, 05004], lr: 0.029186, loss: 1.7325
2022-07-16 11:04:35 - train: epoch 0077, iter [02100, 05004], lr: 0.029163, loss: 1.8497
2022-07-16 11:05:09 - train: epoch 0077, iter [02200, 05004], lr: 0.029139, loss: 1.8476
2022-07-16 11:05:42 - train: epoch 0077, iter [02300, 05004], lr: 0.029115, loss: 1.9696
2022-07-16 11:06:16 - train: epoch 0077, iter [02400, 05004], lr: 0.029091, loss: 1.7279
2022-07-16 11:06:51 - train: epoch 0077, iter [02500, 05004], lr: 0.029067, loss: 1.9983
2022-07-16 11:07:25 - train: epoch 0077, iter [02600, 05004], lr: 0.029044, loss: 1.5525
2022-07-16 11:07:59 - train: epoch 0077, iter [02700, 05004], lr: 0.029020, loss: 1.7356
2022-07-16 11:08:34 - train: epoch 0077, iter [02800, 05004], lr: 0.028996, loss: 1.9957
2022-07-16 11:09:09 - train: epoch 0077, iter [02900, 05004], lr: 0.028973, loss: 2.1655
2022-07-16 11:09:42 - train: epoch 0077, iter [03000, 05004], lr: 0.028949, loss: 1.6449
2022-07-16 11:10:17 - train: epoch 0077, iter [03100, 05004], lr: 0.028925, loss: 1.7427
2022-07-16 11:10:51 - train: epoch 0077, iter [03200, 05004], lr: 0.028901, loss: 1.9441
2022-07-16 11:11:24 - train: epoch 0077, iter [03300, 05004], lr: 0.028878, loss: 1.5993
2022-07-16 11:11:59 - train: epoch 0077, iter [03400, 05004], lr: 0.028854, loss: 2.0836
2022-07-16 11:12:34 - train: epoch 0077, iter [03500, 05004], lr: 0.028830, loss: 1.6222
2022-07-16 11:13:08 - train: epoch 0077, iter [03600, 05004], lr: 0.028807, loss: 1.6595
2022-07-16 11:13:42 - train: epoch 0077, iter [03700, 05004], lr: 0.028783, loss: 1.7878
2022-07-16 11:14:16 - train: epoch 0077, iter [03800, 05004], lr: 0.028759, loss: 1.7776
2022-07-16 11:14:50 - train: epoch 0077, iter [03900, 05004], lr: 0.028735, loss: 1.8313
2022-07-16 11:15:24 - train: epoch 0077, iter [04000, 05004], lr: 0.028712, loss: 1.7571
2022-07-16 11:15:59 - train: epoch 0077, iter [04100, 05004], lr: 0.028688, loss: 1.9712
2022-07-16 11:16:32 - train: epoch 0077, iter [04200, 05004], lr: 0.028664, loss: 1.9516
2022-07-16 11:17:07 - train: epoch 0077, iter [04300, 05004], lr: 0.028641, loss: 1.7778
2022-07-16 11:17:41 - train: epoch 0077, iter [04400, 05004], lr: 0.028617, loss: 1.7311
2022-07-16 11:18:16 - train: epoch 0077, iter [04500, 05004], lr: 0.028594, loss: 1.9686
2022-07-16 11:18:51 - train: epoch 0077, iter [04600, 05004], lr: 0.028570, loss: 1.7762
2022-07-16 11:19:25 - train: epoch 0077, iter [04700, 05004], lr: 0.028546, loss: 1.6886
2022-07-16 11:19:58 - train: epoch 0077, iter [04800, 05004], lr: 0.028523, loss: 1.7833
2022-07-16 11:20:34 - train: epoch 0077, iter [04900, 05004], lr: 0.028499, loss: 1.9544
2022-07-16 11:21:06 - train: epoch 0077, iter [05000, 05004], lr: 0.028475, loss: 2.0297
2022-07-16 11:21:07 - train: epoch 077, train_loss: 1.7924
2022-07-16 11:22:23 - eval: epoch: 077, acc1: 61.778%, acc5: 84.640%, test_loss: 1.5937, per_image_load_time: 2.695ms, per_image_inference_time: 0.239ms
2022-07-16 11:22:24 - until epoch: 077, best_acc1: 61.858%
2022-07-16 11:22:24 - epoch 078 lr: 0.028474
2022-07-16 11:23:02 - train: epoch 0078, iter [00100, 05004], lr: 0.028451, loss: 1.7313
2022-07-16 11:23:37 - train: epoch 0078, iter [00200, 05004], lr: 0.028427, loss: 1.9835
2022-07-16 11:24:11 - train: epoch 0078, iter [00300, 05004], lr: 0.028404, loss: 1.8216
2022-07-16 11:24:46 - train: epoch 0078, iter [00400, 05004], lr: 0.028380, loss: 1.6922
2022-07-16 11:25:20 - train: epoch 0078, iter [00500, 05004], lr: 0.028356, loss: 1.6866
2022-07-16 11:25:55 - train: epoch 0078, iter [00600, 05004], lr: 0.028333, loss: 1.7782
2022-07-16 11:26:29 - train: epoch 0078, iter [00700, 05004], lr: 0.028309, loss: 1.8211
2022-07-16 11:27:03 - train: epoch 0078, iter [00800, 05004], lr: 0.028286, loss: 2.1157
2022-07-16 11:27:37 - train: epoch 0078, iter [00900, 05004], lr: 0.028262, loss: 1.9067
2022-07-16 11:28:12 - train: epoch 0078, iter [01000, 05004], lr: 0.028239, loss: 1.8374
2022-07-16 11:28:46 - train: epoch 0078, iter [01100, 05004], lr: 0.028215, loss: 1.6248
2022-07-16 11:29:20 - train: epoch 0078, iter [01200, 05004], lr: 0.028192, loss: 1.5135
2022-07-16 11:29:54 - train: epoch 0078, iter [01300, 05004], lr: 0.028168, loss: 1.8663
2022-07-16 11:30:29 - train: epoch 0078, iter [01400, 05004], lr: 0.028144, loss: 1.6511
2022-07-16 11:31:03 - train: epoch 0078, iter [01500, 05004], lr: 0.028121, loss: 1.7528
2022-07-16 11:31:37 - train: epoch 0078, iter [01600, 05004], lr: 0.028097, loss: 1.8763
2022-07-16 11:32:11 - train: epoch 0078, iter [01700, 05004], lr: 0.028074, loss: 1.8377
2022-07-16 11:32:45 - train: epoch 0078, iter [01800, 05004], lr: 0.028050, loss: 1.7467
2022-07-16 11:33:19 - train: epoch 0078, iter [01900, 05004], lr: 0.028027, loss: 1.5741
2022-07-16 11:33:53 - train: epoch 0078, iter [02000, 05004], lr: 0.028003, loss: 1.7206
2022-07-16 11:34:27 - train: epoch 0078, iter [02100, 05004], lr: 0.027980, loss: 2.0771
2022-07-16 11:35:02 - train: epoch 0078, iter [02200, 05004], lr: 0.027956, loss: 1.8224
2022-07-16 11:35:37 - train: epoch 0078, iter [02300, 05004], lr: 0.027933, loss: 1.8762
2022-07-16 11:36:11 - train: epoch 0078, iter [02400, 05004], lr: 0.027909, loss: 1.7314
2022-07-16 11:36:46 - train: epoch 0078, iter [02500, 05004], lr: 0.027886, loss: 1.7651
2022-07-16 11:37:20 - train: epoch 0078, iter [02600, 05004], lr: 0.027863, loss: 1.7340
2022-07-16 11:37:55 - train: epoch 0078, iter [02700, 05004], lr: 0.027839, loss: 1.7995
2022-07-16 11:38:30 - train: epoch 0078, iter [02800, 05004], lr: 0.027816, loss: 1.7590
2022-07-16 11:39:05 - train: epoch 0078, iter [02900, 05004], lr: 0.027792, loss: 1.6922
2022-07-16 11:39:39 - train: epoch 0078, iter [03000, 05004], lr: 0.027769, loss: 1.8071
2022-07-16 11:40:14 - train: epoch 0078, iter [03100, 05004], lr: 0.027745, loss: 1.9274
2022-07-16 11:40:48 - train: epoch 0078, iter [03200, 05004], lr: 0.027722, loss: 1.8544
2022-07-16 11:41:23 - train: epoch 0078, iter [03300, 05004], lr: 0.027699, loss: 1.9494
2022-07-16 11:41:57 - train: epoch 0078, iter [03400, 05004], lr: 0.027675, loss: 1.8211
2022-07-16 11:42:32 - train: epoch 0078, iter [03500, 05004], lr: 0.027652, loss: 1.7110
2022-07-16 11:43:06 - train: epoch 0078, iter [03600, 05004], lr: 0.027628, loss: 1.8691
2022-07-16 11:43:40 - train: epoch 0078, iter [03700, 05004], lr: 0.027605, loss: 1.6195
2022-07-16 11:44:15 - train: epoch 0078, iter [03800, 05004], lr: 0.027582, loss: 1.7316
2022-07-16 11:44:49 - train: epoch 0078, iter [03900, 05004], lr: 0.027558, loss: 1.8118
2022-07-16 11:45:23 - train: epoch 0078, iter [04000, 05004], lr: 0.027535, loss: 1.7417
2022-07-16 11:45:58 - train: epoch 0078, iter [04100, 05004], lr: 0.027511, loss: 1.8527
2022-07-16 11:46:33 - train: epoch 0078, iter [04200, 05004], lr: 0.027488, loss: 2.0447
2022-07-16 11:47:06 - train: epoch 0078, iter [04300, 05004], lr: 0.027465, loss: 1.7929
2022-07-16 11:47:43 - train: epoch 0078, iter [04400, 05004], lr: 0.027441, loss: 1.8348
2022-07-16 11:48:16 - train: epoch 0078, iter [04500, 05004], lr: 0.027418, loss: 1.7396
2022-07-16 11:48:52 - train: epoch 0078, iter [04600, 05004], lr: 0.027395, loss: 1.5567
2022-07-16 11:49:27 - train: epoch 0078, iter [04700, 05004], lr: 0.027371, loss: 1.8464
2022-07-16 11:50:01 - train: epoch 0078, iter [04800, 05004], lr: 0.027348, loss: 2.0953
2022-07-16 11:50:36 - train: epoch 0078, iter [04900, 05004], lr: 0.027325, loss: 1.9035
2022-07-16 11:51:09 - train: epoch 0078, iter [05000, 05004], lr: 0.027301, loss: 1.7885
2022-07-16 11:51:10 - train: epoch 078, train_loss: 1.7774
2022-07-16 11:52:27 - eval: epoch: 078, acc1: 62.404%, acc5: 84.628%, test_loss: 1.5726, per_image_load_time: 0.999ms, per_image_inference_time: 0.215ms
2022-07-16 11:52:27 - until epoch: 078, best_acc1: 62.404%
2022-07-16 11:52:27 - epoch 079 lr: 0.027300
2022-07-16 11:53:06 - train: epoch 0079, iter [00100, 05004], lr: 0.027277, loss: 1.6397
2022-07-16 11:53:41 - train: epoch 0079, iter [00200, 05004], lr: 0.027254, loss: 1.7476
2022-07-16 11:54:15 - train: epoch 0079, iter [00300, 05004], lr: 0.027231, loss: 1.8479
2022-07-16 11:54:49 - train: epoch 0079, iter [00400, 05004], lr: 0.027207, loss: 1.9210
2022-07-16 11:55:24 - train: epoch 0079, iter [00500, 05004], lr: 0.027184, loss: 1.7671
2022-07-16 11:55:59 - train: epoch 0079, iter [00600, 05004], lr: 0.027161, loss: 1.7482
2022-07-16 11:56:33 - train: epoch 0079, iter [00700, 05004], lr: 0.027137, loss: 1.6451
2022-07-16 11:57:08 - train: epoch 0079, iter [00800, 05004], lr: 0.027114, loss: 1.8437
2022-07-16 11:57:42 - train: epoch 0079, iter [00900, 05004], lr: 0.027091, loss: 1.6207
2022-07-16 11:58:17 - train: epoch 0079, iter [01000, 05004], lr: 0.027068, loss: 1.6938
2022-07-16 11:58:51 - train: epoch 0079, iter [01100, 05004], lr: 0.027044, loss: 1.9393
2022-07-16 11:59:26 - train: epoch 0079, iter [01200, 05004], lr: 0.027021, loss: 2.0720
2022-07-16 12:00:00 - train: epoch 0079, iter [01300, 05004], lr: 0.026998, loss: 1.6199
2022-07-16 12:00:35 - train: epoch 0079, iter [01400, 05004], lr: 0.026975, loss: 1.6701
2022-07-16 12:01:09 - train: epoch 0079, iter [01500, 05004], lr: 0.026952, loss: 1.7255
2022-07-16 12:01:44 - train: epoch 0079, iter [01600, 05004], lr: 0.026928, loss: 1.5097
2022-07-16 12:02:18 - train: epoch 0079, iter [01700, 05004], lr: 0.026905, loss: 1.7675
2022-07-16 12:02:53 - train: epoch 0079, iter [01800, 05004], lr: 0.026882, loss: 1.7490
2022-07-16 12:03:28 - train: epoch 0079, iter [01900, 05004], lr: 0.026859, loss: 1.7488
2022-07-16 12:04:02 - train: epoch 0079, iter [02000, 05004], lr: 0.026836, loss: 1.9445
2022-07-16 12:04:37 - train: epoch 0079, iter [02100, 05004], lr: 0.026812, loss: 1.5346
2022-07-16 12:05:10 - train: epoch 0079, iter [02200, 05004], lr: 0.026789, loss: 1.8715
2022-07-16 12:05:46 - train: epoch 0079, iter [02300, 05004], lr: 0.026766, loss: 1.6514
2022-07-16 12:06:21 - train: epoch 0079, iter [02400, 05004], lr: 0.026743, loss: 1.7222
2022-07-16 12:06:54 - train: epoch 0079, iter [02500, 05004], lr: 0.026720, loss: 1.7309
2022-07-16 12:07:29 - train: epoch 0079, iter [02600, 05004], lr: 0.026697, loss: 1.7306
2022-07-16 12:08:04 - train: epoch 0079, iter [02700, 05004], lr: 0.026673, loss: 1.4736
2022-07-16 12:08:38 - train: epoch 0079, iter [02800, 05004], lr: 0.026650, loss: 1.6220
2022-07-16 12:09:13 - train: epoch 0079, iter [02900, 05004], lr: 0.026627, loss: 1.6396
2022-07-16 12:09:47 - train: epoch 0079, iter [03000, 05004], lr: 0.026604, loss: 1.7636
2022-07-16 12:10:22 - train: epoch 0079, iter [03100, 05004], lr: 0.026581, loss: 1.7652
2022-07-16 12:10:57 - train: epoch 0079, iter [03200, 05004], lr: 0.026558, loss: 2.0762
2022-07-16 12:11:32 - train: epoch 0079, iter [03300, 05004], lr: 0.026535, loss: 1.7448
2022-07-16 12:12:07 - train: epoch 0079, iter [03400, 05004], lr: 0.026512, loss: 1.6115
2022-07-16 12:12:41 - train: epoch 0079, iter [03500, 05004], lr: 0.026489, loss: 1.9353
2022-07-16 12:13:16 - train: epoch 0079, iter [03600, 05004], lr: 0.026465, loss: 1.6904
2022-07-16 12:13:50 - train: epoch 0079, iter [03700, 05004], lr: 0.026442, loss: 1.6223
2022-07-16 12:14:25 - train: epoch 0079, iter [03800, 05004], lr: 0.026419, loss: 1.8437
2022-07-16 12:15:00 - train: epoch 0079, iter [03900, 05004], lr: 0.026396, loss: 1.7011
2022-07-16 12:15:33 - train: epoch 0079, iter [04000, 05004], lr: 0.026373, loss: 1.5379
2022-07-16 12:16:08 - train: epoch 0079, iter [04100, 05004], lr: 0.026350, loss: 1.9621
2022-07-16 12:16:43 - train: epoch 0079, iter [04200, 05004], lr: 0.026327, loss: 1.5654
2022-07-16 12:17:18 - train: epoch 0079, iter [04300, 05004], lr: 0.026304, loss: 1.5453
2022-07-16 12:17:52 - train: epoch 0079, iter [04400, 05004], lr: 0.026281, loss: 1.9552
2022-07-16 12:18:27 - train: epoch 0079, iter [04500, 05004], lr: 0.026258, loss: 2.0626
2022-07-16 12:19:02 - train: epoch 0079, iter [04600, 05004], lr: 0.026235, loss: 2.0508
2022-07-16 12:19:36 - train: epoch 0079, iter [04700, 05004], lr: 0.026212, loss: 1.8096
2022-07-16 12:20:11 - train: epoch 0079, iter [04800, 05004], lr: 0.026189, loss: 1.9204
2022-07-16 12:20:45 - train: epoch 0079, iter [04900, 05004], lr: 0.026166, loss: 1.8849
2022-07-16 12:21:18 - train: epoch 0079, iter [05000, 05004], lr: 0.026143, loss: 1.5916
2022-07-16 12:21:19 - train: epoch 079, train_loss: 1.7666
2022-07-16 12:22:36 - eval: epoch: 079, acc1: 62.378%, acc5: 84.648%, test_loss: 1.5798, per_image_load_time: 1.179ms, per_image_inference_time: 0.214ms
2022-07-16 12:22:36 - until epoch: 079, best_acc1: 62.404%
2022-07-16 12:22:36 - epoch 080 lr: 0.026142
2022-07-16 12:23:16 - train: epoch 0080, iter [00100, 05004], lr: 0.026119, loss: 1.6235
2022-07-16 12:23:50 - train: epoch 0080, iter [00200, 05004], lr: 0.026096, loss: 1.7226
2022-07-16 12:24:24 - train: epoch 0080, iter [00300, 05004], lr: 0.026073, loss: 1.7412
2022-07-16 12:24:58 - train: epoch 0080, iter [00400, 05004], lr: 0.026050, loss: 1.5986
2022-07-16 12:25:33 - train: epoch 0080, iter [00500, 05004], lr: 0.026027, loss: 1.6957
2022-07-16 12:26:07 - train: epoch 0080, iter [00600, 05004], lr: 0.026004, loss: 1.6342
2022-07-16 12:26:42 - train: epoch 0080, iter [00700, 05004], lr: 0.025981, loss: 1.6107
2022-07-16 12:27:16 - train: epoch 0080, iter [00800, 05004], lr: 0.025958, loss: 1.6234
2022-07-16 12:27:50 - train: epoch 0080, iter [00900, 05004], lr: 0.025935, loss: 1.7471
2022-07-16 12:28:25 - train: epoch 0080, iter [01000, 05004], lr: 0.025912, loss: 1.6607
2022-07-16 12:29:00 - train: epoch 0080, iter [01100, 05004], lr: 0.025890, loss: 1.8111
2022-07-16 12:29:34 - train: epoch 0080, iter [01200, 05004], lr: 0.025867, loss: 1.7401
2022-07-16 12:30:09 - train: epoch 0080, iter [01300, 05004], lr: 0.025844, loss: 1.6409
2022-07-16 12:30:43 - train: epoch 0080, iter [01400, 05004], lr: 0.025821, loss: 1.7429
2022-07-16 12:31:18 - train: epoch 0080, iter [01500, 05004], lr: 0.025798, loss: 1.6025
2022-07-16 12:31:51 - train: epoch 0080, iter [01600, 05004], lr: 0.025775, loss: 1.6514
2022-07-16 12:32:25 - train: epoch 0080, iter [01700, 05004], lr: 0.025752, loss: 1.8525
2022-07-16 12:33:00 - train: epoch 0080, iter [01800, 05004], lr: 0.025729, loss: 1.8754
2022-07-16 12:33:34 - train: epoch 0080, iter [01900, 05004], lr: 0.025706, loss: 1.7971
2022-07-16 12:34:09 - train: epoch 0080, iter [02000, 05004], lr: 0.025684, loss: 1.7821
2022-07-16 12:34:44 - train: epoch 0080, iter [02100, 05004], lr: 0.025661, loss: 1.9196
2022-07-16 12:35:18 - train: epoch 0080, iter [02200, 05004], lr: 0.025638, loss: 1.8598
2022-07-16 12:35:52 - train: epoch 0080, iter [02300, 05004], lr: 0.025615, loss: 1.7170
2022-07-16 12:36:27 - train: epoch 0080, iter [02400, 05004], lr: 0.025592, loss: 1.7642
2022-07-16 12:37:01 - train: epoch 0080, iter [02500, 05004], lr: 0.025569, loss: 1.6748
2022-07-16 12:37:36 - train: epoch 0080, iter [02600, 05004], lr: 0.025547, loss: 1.6564
2022-07-16 12:38:11 - train: epoch 0080, iter [02700, 05004], lr: 0.025524, loss: 1.9038
2022-07-16 12:38:46 - train: epoch 0080, iter [02800, 05004], lr: 0.025501, loss: 1.9310
2022-07-16 12:39:20 - train: epoch 0080, iter [02900, 05004], lr: 0.025478, loss: 1.7992
2022-07-16 12:39:55 - train: epoch 0080, iter [03000, 05004], lr: 0.025455, loss: 1.7302
2022-07-16 12:40:29 - train: epoch 0080, iter [03100, 05004], lr: 0.025433, loss: 1.9362
2022-07-16 12:41:04 - train: epoch 0080, iter [03200, 05004], lr: 0.025410, loss: 1.3875
2022-07-16 12:41:38 - train: epoch 0080, iter [03300, 05004], lr: 0.025387, loss: 1.6354
2022-07-16 12:42:14 - train: epoch 0080, iter [03400, 05004], lr: 0.025364, loss: 1.5926
2022-07-16 12:42:48 - train: epoch 0080, iter [03500, 05004], lr: 0.025341, loss: 1.6480
2022-07-16 12:43:22 - train: epoch 0080, iter [03600, 05004], lr: 0.025319, loss: 1.7803
2022-07-16 12:43:56 - train: epoch 0080, iter [03700, 05004], lr: 0.025296, loss: 1.6784
2022-07-16 12:44:31 - train: epoch 0080, iter [03800, 05004], lr: 0.025273, loss: 1.8743
2022-07-16 12:45:06 - train: epoch 0080, iter [03900, 05004], lr: 0.025251, loss: 1.6000
2022-07-16 12:45:40 - train: epoch 0080, iter [04000, 05004], lr: 0.025228, loss: 1.8107
2022-07-16 12:46:14 - train: epoch 0080, iter [04100, 05004], lr: 0.025205, loss: 1.8719
2022-07-16 12:46:49 - train: epoch 0080, iter [04200, 05004], lr: 0.025182, loss: 1.6489
2022-07-16 12:47:23 - train: epoch 0080, iter [04300, 05004], lr: 0.025160, loss: 1.8871
2022-07-16 12:47:57 - train: epoch 0080, iter [04400, 05004], lr: 0.025137, loss: 1.8293
2022-07-16 12:48:32 - train: epoch 0080, iter [04500, 05004], lr: 0.025114, loss: 1.6425
2022-07-16 12:49:06 - train: epoch 0080, iter [04600, 05004], lr: 0.025092, loss: 1.8562
2022-07-16 12:49:41 - train: epoch 0080, iter [04700, 05004], lr: 0.025069, loss: 1.8881
2022-07-16 12:50:15 - train: epoch 0080, iter [04800, 05004], lr: 0.025046, loss: 1.7304
2022-07-16 12:50:51 - train: epoch 0080, iter [04900, 05004], lr: 0.025024, loss: 1.9264
2022-07-16 12:51:23 - train: epoch 0080, iter [05000, 05004], lr: 0.025001, loss: 1.5977
2022-07-16 12:51:24 - train: epoch 080, train_loss: 1.7532
2022-07-16 12:52:41 - eval: epoch: 080, acc1: 63.338%, acc5: 85.278%, test_loss: 1.5308, per_image_load_time: 2.078ms, per_image_inference_time: 0.210ms
2022-07-16 12:52:41 - until epoch: 080, best_acc1: 63.338%
2022-07-16 12:52:41 - epoch 081 lr: 0.025000
2022-07-16 12:53:20 - train: epoch 0081, iter [00100, 05004], lr: 0.024977, loss: 1.5918
2022-07-16 12:53:55 - train: epoch 0081, iter [00200, 05004], lr: 0.024955, loss: 1.7687
2022-07-16 12:54:29 - train: epoch 0081, iter [00300, 05004], lr: 0.024932, loss: 1.6784
2022-07-16 12:55:04 - train: epoch 0081, iter [00400, 05004], lr: 0.024909, loss: 1.8938
2022-07-16 12:55:38 - train: epoch 0081, iter [00500, 05004], lr: 0.024887, loss: 1.8246
2022-07-16 12:56:13 - train: epoch 0081, iter [00600, 05004], lr: 0.024864, loss: 1.7431
2022-07-16 12:56:47 - train: epoch 0081, iter [00700, 05004], lr: 0.024842, loss: 1.5570
2022-07-16 12:57:22 - train: epoch 0081, iter [00800, 05004], lr: 0.024819, loss: 1.9633
2022-07-16 12:57:57 - train: epoch 0081, iter [00900, 05004], lr: 0.024796, loss: 1.6913
2022-07-16 12:58:31 - train: epoch 0081, iter [01000, 05004], lr: 0.024774, loss: 1.7115
2022-07-16 12:59:05 - train: epoch 0081, iter [01100, 05004], lr: 0.024751, loss: 1.5926
2022-07-16 12:59:40 - train: epoch 0081, iter [01200, 05004], lr: 0.024729, loss: 1.7044
2022-07-16 13:00:14 - train: epoch 0081, iter [01300, 05004], lr: 0.024706, loss: 1.5450
2022-07-16 13:00:49 - train: epoch 0081, iter [01400, 05004], lr: 0.024684, loss: 1.4839
2022-07-16 13:01:23 - train: epoch 0081, iter [01500, 05004], lr: 0.024661, loss: 1.8297
2022-07-16 13:01:59 - train: epoch 0081, iter [01600, 05004], lr: 0.024638, loss: 1.6492
2022-07-16 13:02:32 - train: epoch 0081, iter [01700, 05004], lr: 0.024616, loss: 1.7739
2022-07-16 13:03:08 - train: epoch 0081, iter [01800, 05004], lr: 0.024593, loss: 1.6979
2022-07-16 13:03:42 - train: epoch 0081, iter [01900, 05004], lr: 0.024571, loss: 1.6358
2022-07-16 13:04:16 - train: epoch 0081, iter [02000, 05004], lr: 0.024548, loss: 1.7907
2022-07-16 13:04:51 - train: epoch 0081, iter [02100, 05004], lr: 0.024526, loss: 1.7404
2022-07-16 13:05:25 - train: epoch 0081, iter [02200, 05004], lr: 0.024503, loss: 1.7414
2022-07-16 13:06:00 - train: epoch 0081, iter [02300, 05004], lr: 0.024481, loss: 1.7584
2022-07-16 13:06:34 - train: epoch 0081, iter [02400, 05004], lr: 0.024458, loss: 1.7326
2022-07-16 13:07:08 - train: epoch 0081, iter [02500, 05004], lr: 0.024436, loss: 1.9911
2022-07-16 13:07:43 - train: epoch 0081, iter [02600, 05004], lr: 0.024413, loss: 2.0237
2022-07-16 13:08:17 - train: epoch 0081, iter [02700, 05004], lr: 0.024391, loss: 1.7163
2022-07-16 13:08:52 - train: epoch 0081, iter [02800, 05004], lr: 0.024368, loss: 1.7113
2022-07-16 13:09:28 - train: epoch 0081, iter [02900, 05004], lr: 0.024346, loss: 1.5563
2022-07-16 13:10:01 - train: epoch 0081, iter [03000, 05004], lr: 0.024323, loss: 1.7942
2022-07-16 13:10:36 - train: epoch 0081, iter [03100, 05004], lr: 0.024301, loss: 1.5955
2022-07-16 13:11:10 - train: epoch 0081, iter [03200, 05004], lr: 0.024279, loss: 1.7448
2022-07-16 13:11:45 - train: epoch 0081, iter [03300, 05004], lr: 0.024256, loss: 1.7132
2022-07-16 13:12:19 - train: epoch 0081, iter [03400, 05004], lr: 0.024234, loss: 1.9160
2022-07-16 13:12:54 - train: epoch 0081, iter [03500, 05004], lr: 0.024211, loss: 1.8722
2022-07-16 13:13:28 - train: epoch 0081, iter [03600, 05004], lr: 0.024189, loss: 1.6018
2022-07-16 13:14:03 - train: epoch 0081, iter [03700, 05004], lr: 0.024167, loss: 1.9157
2022-07-16 13:14:37 - train: epoch 0081, iter [03800, 05004], lr: 0.024144, loss: 1.5989
2022-07-16 13:15:13 - train: epoch 0081, iter [03900, 05004], lr: 0.024122, loss: 1.7836
2022-07-16 13:15:48 - train: epoch 0081, iter [04000, 05004], lr: 0.024099, loss: 1.6005
2022-07-16 13:16:22 - train: epoch 0081, iter [04100, 05004], lr: 0.024077, loss: 1.7910
2022-07-16 13:16:57 - train: epoch 0081, iter [04200, 05004], lr: 0.024055, loss: 1.7469
2022-07-16 13:17:32 - train: epoch 0081, iter [04300, 05004], lr: 0.024032, loss: 1.8158
2022-07-16 13:18:06 - train: epoch 0081, iter [04400, 05004], lr: 0.024010, loss: 1.9227
2022-07-16 13:18:41 - train: epoch 0081, iter [04500, 05004], lr: 0.023988, loss: 1.9054
2022-07-16 13:19:15 - train: epoch 0081, iter [04600, 05004], lr: 0.023965, loss: 1.5881
2022-07-16 13:19:50 - train: epoch 0081, iter [04700, 05004], lr: 0.023943, loss: 1.7936
2022-07-16 13:20:24 - train: epoch 0081, iter [04800, 05004], lr: 0.023921, loss: 1.7576
2022-07-16 13:20:58 - train: epoch 0081, iter [04900, 05004], lr: 0.023898, loss: 1.7984
2022-07-16 13:21:31 - train: epoch 0081, iter [05000, 05004], lr: 0.023876, loss: 1.5740
2022-07-16 13:21:32 - train: epoch 081, train_loss: 1.7385
2022-07-16 13:22:48 - eval: epoch: 081, acc1: 63.560%, acc5: 85.408%, test_loss: 1.5268, per_image_load_time: 2.728ms, per_image_inference_time: 0.217ms
2022-07-16 13:22:48 - until epoch: 081, best_acc1: 63.560%
2022-07-16 13:22:48 - epoch 082 lr: 0.023875
2022-07-16 13:23:28 - train: epoch 0082, iter [00100, 05004], lr: 0.023853, loss: 1.4069
2022-07-16 13:24:02 - train: epoch 0082, iter [00200, 05004], lr: 0.023830, loss: 1.5714
2022-07-16 13:24:36 - train: epoch 0082, iter [00300, 05004], lr: 0.023808, loss: 1.7055
2022-07-16 13:25:10 - train: epoch 0082, iter [00400, 05004], lr: 0.023786, loss: 1.8104
2022-07-16 13:25:45 - train: epoch 0082, iter [00500, 05004], lr: 0.023764, loss: 1.8140
2022-07-16 13:26:20 - train: epoch 0082, iter [00600, 05004], lr: 0.023741, loss: 1.6217
2022-07-16 13:26:54 - train: epoch 0082, iter [00700, 05004], lr: 0.023719, loss: 1.9278
2022-07-16 13:27:29 - train: epoch 0082, iter [00800, 05004], lr: 0.023697, loss: 1.6276
2022-07-16 13:28:03 - train: epoch 0082, iter [00900, 05004], lr: 0.023675, loss: 1.9166
2022-07-16 13:28:38 - train: epoch 0082, iter [01000, 05004], lr: 0.023652, loss: 1.7945
2022-07-16 13:29:12 - train: epoch 0082, iter [01100, 05004], lr: 0.023630, loss: 1.7622
2022-07-16 13:29:46 - train: epoch 0082, iter [01200, 05004], lr: 0.023608, loss: 1.7123
2022-07-16 13:30:21 - train: epoch 0082, iter [01300, 05004], lr: 0.023586, loss: 1.9494
2022-07-16 13:30:55 - train: epoch 0082, iter [01400, 05004], lr: 0.023564, loss: 1.6566
2022-07-16 13:31:30 - train: epoch 0082, iter [01500, 05004], lr: 0.023541, loss: 1.5515
2022-07-16 13:32:05 - train: epoch 0082, iter [01600, 05004], lr: 0.023519, loss: 1.6226
2022-07-16 13:32:39 - train: epoch 0082, iter [01700, 05004], lr: 0.023497, loss: 1.6340
2022-07-16 13:33:14 - train: epoch 0082, iter [01800, 05004], lr: 0.023475, loss: 1.6170
2022-07-16 13:33:48 - train: epoch 0082, iter [01900, 05004], lr: 0.023453, loss: 1.5825
2022-07-16 13:34:23 - train: epoch 0082, iter [02000, 05004], lr: 0.023430, loss: 1.5142
2022-07-16 13:34:58 - train: epoch 0082, iter [02100, 05004], lr: 0.023408, loss: 1.7559
2022-07-16 13:35:32 - train: epoch 0082, iter [02200, 05004], lr: 0.023386, loss: 1.7412
2022-07-16 13:36:07 - train: epoch 0082, iter [02300, 05004], lr: 0.023364, loss: 1.7176
2022-07-16 13:36:42 - train: epoch 0082, iter [02400, 05004], lr: 0.023342, loss: 1.7830
2022-07-16 13:37:16 - train: epoch 0082, iter [02500, 05004], lr: 0.023320, loss: 1.7361
2022-07-16 13:37:51 - train: epoch 0082, iter [02600, 05004], lr: 0.023298, loss: 1.5544
2022-07-16 13:38:26 - train: epoch 0082, iter [02700, 05004], lr: 0.023275, loss: 1.6181
2022-07-16 13:39:01 - train: epoch 0082, iter [02800, 05004], lr: 0.023253, loss: 1.4903
2022-07-16 13:39:35 - train: epoch 0082, iter [02900, 05004], lr: 0.023231, loss: 1.5287
2022-07-16 13:40:10 - train: epoch 0082, iter [03000, 05004], lr: 0.023209, loss: 1.6266
2022-07-16 13:40:44 - train: epoch 0082, iter [03100, 05004], lr: 0.023187, loss: 1.7841
2022-07-16 13:41:19 - train: epoch 0082, iter [03200, 05004], lr: 0.023165, loss: 1.9852
2022-07-16 13:41:53 - train: epoch 0082, iter [03300, 05004], lr: 0.023143, loss: 1.7737
2022-07-16 13:42:29 - train: epoch 0082, iter [03400, 05004], lr: 0.023121, loss: 1.6118
2022-07-16 13:43:02 - train: epoch 0082, iter [03500, 05004], lr: 0.023099, loss: 1.7441
2022-07-16 13:43:38 - train: epoch 0082, iter [03600, 05004], lr: 0.023077, loss: 1.6728
2022-07-16 13:44:12 - train: epoch 0082, iter [03700, 05004], lr: 0.023055, loss: 1.5735
2022-07-16 13:44:48 - train: epoch 0082, iter [03800, 05004], lr: 0.023033, loss: 1.9385
2022-07-16 13:45:22 - train: epoch 0082, iter [03900, 05004], lr: 0.023011, loss: 1.7307
2022-07-16 13:45:57 - train: epoch 0082, iter [04000, 05004], lr: 0.022989, loss: 1.7088
2022-07-16 13:46:31 - train: epoch 0082, iter [04100, 05004], lr: 0.022967, loss: 1.7255
2022-07-16 13:47:06 - train: epoch 0082, iter [04200, 05004], lr: 0.022945, loss: 1.9194
2022-07-16 13:47:40 - train: epoch 0082, iter [04300, 05004], lr: 0.022923, loss: 1.6059
2022-07-16 13:48:16 - train: epoch 0082, iter [04400, 05004], lr: 0.022901, loss: 1.6796
2022-07-16 13:48:50 - train: epoch 0082, iter [04500, 05004], lr: 0.022879, loss: 1.8281
2022-07-16 13:49:25 - train: epoch 0082, iter [04600, 05004], lr: 0.022857, loss: 1.7412
2022-07-16 13:49:59 - train: epoch 0082, iter [04700, 05004], lr: 0.022835, loss: 1.5540
2022-07-16 13:50:34 - train: epoch 0082, iter [04800, 05004], lr: 0.022813, loss: 1.6423
2022-07-16 13:51:08 - train: epoch 0082, iter [04900, 05004], lr: 0.022791, loss: 1.7494
2022-07-16 13:51:41 - train: epoch 0082, iter [05000, 05004], lr: 0.022769, loss: 1.7040
2022-07-16 13:51:42 - train: epoch 082, train_loss: 1.7233
2022-07-16 13:52:58 - eval: epoch: 082, acc1: 63.426%, acc5: 85.516%, test_loss: 1.5226, per_image_load_time: 1.229ms, per_image_inference_time: 0.230ms
2022-07-16 13:52:58 - until epoch: 082, best_acc1: 63.560%
2022-07-16 13:52:58 - epoch 083 lr: 0.022768
2022-07-16 13:53:38 - train: epoch 0083, iter [00100, 05004], lr: 0.022746, loss: 1.6159
2022-07-16 13:54:13 - train: epoch 0083, iter [00200, 05004], lr: 0.022724, loss: 1.6851
2022-07-16 13:54:46 - train: epoch 0083, iter [00300, 05004], lr: 0.022702, loss: 1.6993
2022-07-16 13:55:22 - train: epoch 0083, iter [00400, 05004], lr: 0.022680, loss: 1.7573
2022-07-16 13:55:56 - train: epoch 0083, iter [00500, 05004], lr: 0.022658, loss: 1.7100
2022-07-16 13:56:30 - train: epoch 0083, iter [00600, 05004], lr: 0.022637, loss: 1.5868
2022-07-16 13:57:05 - train: epoch 0083, iter [00700, 05004], lr: 0.022615, loss: 1.6250
2022-07-16 13:57:39 - train: epoch 0083, iter [00800, 05004], lr: 0.022593, loss: 1.6454
2022-07-16 13:58:13 - train: epoch 0083, iter [00900, 05004], lr: 0.022571, loss: 1.7521
2022-07-16 13:58:47 - train: epoch 0083, iter [01000, 05004], lr: 0.022549, loss: 1.7882
2022-07-16 13:59:23 - train: epoch 0083, iter [01100, 05004], lr: 0.022527, loss: 1.6735
2022-07-16 13:59:57 - train: epoch 0083, iter [01200, 05004], lr: 0.022505, loss: 1.6661
2022-07-16 14:00:32 - train: epoch 0083, iter [01300, 05004], lr: 0.022483, loss: 1.5939
2022-07-16 14:01:06 - train: epoch 0083, iter [01400, 05004], lr: 0.022462, loss: 1.7502
2022-07-16 14:01:40 - train: epoch 0083, iter [01500, 05004], lr: 0.022440, loss: 1.5187
2022-07-16 14:02:15 - train: epoch 0083, iter [01600, 05004], lr: 0.022418, loss: 1.7437
2022-07-16 14:02:49 - train: epoch 0083, iter [01700, 05004], lr: 0.022396, loss: 1.8199
2022-07-16 14:03:23 - train: epoch 0083, iter [01800, 05004], lr: 0.022374, loss: 2.0018
2022-07-16 14:03:58 - train: epoch 0083, iter [01900, 05004], lr: 0.022353, loss: 1.5636
2022-07-16 14:04:32 - train: epoch 0083, iter [02000, 05004], lr: 0.022331, loss: 1.4322
2022-07-16 14:05:06 - train: epoch 0083, iter [02100, 05004], lr: 0.022309, loss: 1.5430
2022-07-16 14:05:41 - train: epoch 0083, iter [02200, 05004], lr: 0.022287, loss: 1.7327
2022-07-16 14:06:15 - train: epoch 0083, iter [02300, 05004], lr: 0.022265, loss: 1.8690
2022-07-16 14:06:49 - train: epoch 0083, iter [02400, 05004], lr: 0.022244, loss: 1.6547
2022-07-16 14:07:24 - train: epoch 0083, iter [02500, 05004], lr: 0.022222, loss: 1.6591
2022-07-16 14:07:59 - train: epoch 0083, iter [02600, 05004], lr: 0.022200, loss: 1.5447
2022-07-16 14:08:34 - train: epoch 0083, iter [02700, 05004], lr: 0.022178, loss: 1.6429
2022-07-16 14:09:08 - train: epoch 0083, iter [02800, 05004], lr: 0.022157, loss: 1.6293
2022-07-16 14:09:42 - train: epoch 0083, iter [02900, 05004], lr: 0.022135, loss: 1.7719
2022-07-16 14:10:17 - train: epoch 0083, iter [03000, 05004], lr: 0.022113, loss: 1.8468
2022-07-16 14:10:51 - train: epoch 0083, iter [03100, 05004], lr: 0.022092, loss: 1.5231
2022-07-16 14:11:26 - train: epoch 0083, iter [03200, 05004], lr: 0.022070, loss: 1.5524
2022-07-16 14:12:00 - train: epoch 0083, iter [03300, 05004], lr: 0.022048, loss: 1.8420
2022-07-16 14:12:35 - train: epoch 0083, iter [03400, 05004], lr: 0.022026, loss: 1.8038
2022-07-16 14:13:09 - train: epoch 0083, iter [03500, 05004], lr: 0.022005, loss: 1.8301
2022-07-16 14:13:44 - train: epoch 0083, iter [03600, 05004], lr: 0.021983, loss: 1.7693
2022-07-16 14:14:18 - train: epoch 0083, iter [03700, 05004], lr: 0.021961, loss: 1.6616
2022-07-16 14:14:53 - train: epoch 0083, iter [03800, 05004], lr: 0.021940, loss: 1.5855
2022-07-16 14:15:28 - train: epoch 0083, iter [03900, 05004], lr: 0.021918, loss: 1.7566
2022-07-16 14:16:02 - train: epoch 0083, iter [04000, 05004], lr: 0.021897, loss: 1.8574
2022-07-16 14:16:37 - train: epoch 0083, iter [04100, 05004], lr: 0.021875, loss: 1.6420
2022-07-16 14:17:12 - train: epoch 0083, iter [04200, 05004], lr: 0.021853, loss: 2.1758
2022-07-16 14:17:45 - train: epoch 0083, iter [04300, 05004], lr: 0.021832, loss: 1.7652
2022-07-16 14:18:20 - train: epoch 0083, iter [04400, 05004], lr: 0.021810, loss: 1.6821
2022-07-16 14:18:56 - train: epoch 0083, iter [04500, 05004], lr: 0.021788, loss: 1.7750
2022-07-16 14:19:29 - train: epoch 0083, iter [04600, 05004], lr: 0.021767, loss: 1.7332
2022-07-16 14:20:04 - train: epoch 0083, iter [04700, 05004], lr: 0.021745, loss: 1.8509
2022-07-16 14:20:39 - train: epoch 0083, iter [04800, 05004], lr: 0.021724, loss: 1.7758
2022-07-16 14:21:13 - train: epoch 0083, iter [04900, 05004], lr: 0.021702, loss: 1.9387
2022-07-16 14:21:46 - train: epoch 0083, iter [05000, 05004], lr: 0.021681, loss: 1.7997
2022-07-16 14:21:47 - train: epoch 083, train_loss: 1.7105
2022-07-16 14:23:03 - eval: epoch: 083, acc1: 64.002%, acc5: 85.466%, test_loss: 1.5148, per_image_load_time: 0.912ms, per_image_inference_time: 0.224ms
2022-07-16 14:23:03 - until epoch: 083, best_acc1: 64.002%
2022-07-16 14:23:03 - epoch 084 lr: 0.021679
2022-07-16 14:23:43 - train: epoch 0084, iter [00100, 05004], lr: 0.021658, loss: 1.6842
2022-07-16 14:24:17 - train: epoch 0084, iter [00200, 05004], lr: 0.021637, loss: 1.7443
2022-07-16 14:24:52 - train: epoch 0084, iter [00300, 05004], lr: 0.021615, loss: 1.4809
2022-07-16 14:25:27 - train: epoch 0084, iter [00400, 05004], lr: 0.021594, loss: 1.8088
2022-07-16 14:26:01 - train: epoch 0084, iter [00500, 05004], lr: 0.021572, loss: 1.7049
2022-07-16 14:26:35 - train: epoch 0084, iter [00600, 05004], lr: 0.021550, loss: 2.0198
2022-07-16 14:27:10 - train: epoch 0084, iter [00700, 05004], lr: 0.021529, loss: 1.7220
2022-07-16 14:27:44 - train: epoch 0084, iter [00800, 05004], lr: 0.021507, loss: 1.8355
2022-07-16 14:28:19 - train: epoch 0084, iter [00900, 05004], lr: 0.021486, loss: 1.6419
2022-07-16 14:28:53 - train: epoch 0084, iter [01000, 05004], lr: 0.021464, loss: 1.9006
2022-07-16 14:29:28 - train: epoch 0084, iter [01100, 05004], lr: 0.021443, loss: 1.6953
2022-07-16 14:30:03 - train: epoch 0084, iter [01200, 05004], lr: 0.021422, loss: 1.9660
2022-07-16 14:30:38 - train: epoch 0084, iter [01300, 05004], lr: 0.021400, loss: 1.7526
2022-07-16 14:31:12 - train: epoch 0084, iter [01400, 05004], lr: 0.021379, loss: 1.7651
2022-07-16 14:31:47 - train: epoch 0084, iter [01500, 05004], lr: 0.021357, loss: 1.8319
2022-07-16 14:32:21 - train: epoch 0084, iter [01600, 05004], lr: 0.021336, loss: 1.6086
2022-07-16 14:32:56 - train: epoch 0084, iter [01700, 05004], lr: 0.021314, loss: 1.7205
2022-07-16 14:33:31 - train: epoch 0084, iter [01800, 05004], lr: 0.021293, loss: 1.6822
2022-07-16 14:34:04 - train: epoch 0084, iter [01900, 05004], lr: 0.021271, loss: 1.6178
2022-07-16 14:34:39 - train: epoch 0084, iter [02000, 05004], lr: 0.021250, loss: 1.6643
2022-07-16 14:35:14 - train: epoch 0084, iter [02100, 05004], lr: 0.021229, loss: 1.5719
2022-07-16 14:35:49 - train: epoch 0084, iter [02200, 05004], lr: 0.021207, loss: 1.4207
2022-07-16 14:36:23 - train: epoch 0084, iter [02300, 05004], lr: 0.021186, loss: 1.6600
2022-07-16 14:36:58 - train: epoch 0084, iter [02400, 05004], lr: 0.021165, loss: 1.6128
2022-07-16 14:37:32 - train: epoch 0084, iter [02500, 05004], lr: 0.021143, loss: 1.8004
2022-07-16 14:38:07 - train: epoch 0084, iter [02600, 05004], lr: 0.021122, loss: 1.7987
2022-07-16 14:38:41 - train: epoch 0084, iter [02700, 05004], lr: 0.021100, loss: 1.7215
2022-07-16 14:39:15 - train: epoch 0084, iter [02800, 05004], lr: 0.021079, loss: 1.7100
2022-07-16 14:39:50 - train: epoch 0084, iter [02900, 05004], lr: 0.021058, loss: 1.5538
2022-07-16 14:40:24 - train: epoch 0084, iter [03000, 05004], lr: 0.021036, loss: 1.6628
2022-07-16 14:40:59 - train: epoch 0084, iter [03100, 05004], lr: 0.021015, loss: 1.5802
2022-07-16 14:41:34 - train: epoch 0084, iter [03200, 05004], lr: 0.020994, loss: 1.5956
2022-07-16 14:42:08 - train: epoch 0084, iter [03300, 05004], lr: 0.020973, loss: 1.6958
2022-07-16 14:42:44 - train: epoch 0084, iter [03400, 05004], lr: 0.020951, loss: 1.5822
2022-07-16 14:43:18 - train: epoch 0084, iter [03500, 05004], lr: 0.020930, loss: 1.6647
2022-07-16 14:43:52 - train: epoch 0084, iter [03600, 05004], lr: 0.020909, loss: 1.7603
2022-07-16 14:44:26 - train: epoch 0084, iter [03700, 05004], lr: 0.020887, loss: 1.8541
2022-07-16 14:45:01 - train: epoch 0084, iter [03800, 05004], lr: 0.020866, loss: 1.9399
2022-07-16 14:45:35 - train: epoch 0084, iter [03900, 05004], lr: 0.020845, loss: 1.7488
2022-07-16 14:46:10 - train: epoch 0084, iter [04000, 05004], lr: 0.020824, loss: 1.6936
2022-07-16 14:46:43 - train: epoch 0084, iter [04100, 05004], lr: 0.020802, loss: 1.6223
2022-07-16 14:47:19 - train: epoch 0084, iter [04200, 05004], lr: 0.020781, loss: 1.7342
2022-07-16 14:47:53 - train: epoch 0084, iter [04300, 05004], lr: 0.020760, loss: 1.7771
2022-07-16 14:48:29 - train: epoch 0084, iter [04400, 05004], lr: 0.020739, loss: 1.9655
2022-07-16 14:49:03 - train: epoch 0084, iter [04500, 05004], lr: 0.020718, loss: 1.6381
2022-07-16 14:49:38 - train: epoch 0084, iter [04600, 05004], lr: 0.020696, loss: 1.7985
2022-07-16 14:50:11 - train: epoch 0084, iter [04700, 05004], lr: 0.020675, loss: 1.9728
2022-07-16 14:50:46 - train: epoch 0084, iter [04800, 05004], lr: 0.020654, loss: 1.6286
2022-07-16 14:51:21 - train: epoch 0084, iter [04900, 05004], lr: 0.020633, loss: 1.5804
2022-07-16 14:51:54 - train: epoch 0084, iter [05000, 05004], lr: 0.020612, loss: 1.6831
2022-07-16 14:51:55 - train: epoch 084, train_loss: 1.6944
2022-07-16 14:53:12 - eval: epoch: 084, acc1: 63.644%, acc5: 85.498%, test_loss: 1.5134, per_image_load_time: 1.614ms, per_image_inference_time: 0.215ms
2022-07-16 14:53:12 - until epoch: 084, best_acc1: 64.002%
2022-07-16 14:53:12 - epoch 085 lr: 0.020611
2022-07-16 14:53:52 - train: epoch 0085, iter [00100, 05004], lr: 0.020590, loss: 1.6039
2022-07-16 14:54:26 - train: epoch 0085, iter [00200, 05004], lr: 0.020568, loss: 1.5203
2022-07-16 14:55:01 - train: epoch 0085, iter [00300, 05004], lr: 0.020547, loss: 1.7711
2022-07-16 14:55:35 - train: epoch 0085, iter [00400, 05004], lr: 0.020526, loss: 1.6769
2022-07-16 14:56:09 - train: epoch 0085, iter [00500, 05004], lr: 0.020505, loss: 1.8469
2022-07-16 14:56:43 - train: epoch 0085, iter [00600, 05004], lr: 0.020484, loss: 1.4911
2022-07-16 14:57:17 - train: epoch 0085, iter [00700, 05004], lr: 0.020463, loss: 1.6079
2022-07-16 14:57:51 - train: epoch 0085, iter [00800, 05004], lr: 0.020442, loss: 1.4323
2022-07-16 14:58:27 - train: epoch 0085, iter [00900, 05004], lr: 0.020421, loss: 1.5843
2022-07-16 14:59:01 - train: epoch 0085, iter [01000, 05004], lr: 0.020400, loss: 1.8280
2022-07-16 14:59:37 - train: epoch 0085, iter [01100, 05004], lr: 0.020378, loss: 1.7146
2022-07-16 15:00:11 - train: epoch 0085, iter [01200, 05004], lr: 0.020357, loss: 1.6411
2022-07-16 15:00:46 - train: epoch 0085, iter [01300, 05004], lr: 0.020336, loss: 1.6737
2022-07-16 15:01:20 - train: epoch 0085, iter [01400, 05004], lr: 0.020315, loss: 1.5805
2022-07-16 15:01:55 - train: epoch 0085, iter [01500, 05004], lr: 0.020294, loss: 1.5604
2022-07-16 15:02:30 - train: epoch 0085, iter [01600, 05004], lr: 0.020273, loss: 1.5781
2022-07-16 15:03:05 - train: epoch 0085, iter [01700, 05004], lr: 0.020252, loss: 1.9491
2022-07-16 15:03:38 - train: epoch 0085, iter [01800, 05004], lr: 0.020231, loss: 1.4896
2022-07-16 15:04:13 - train: epoch 0085, iter [01900, 05004], lr: 0.020210, loss: 1.5110
2022-07-16 15:04:47 - train: epoch 0085, iter [02000, 05004], lr: 0.020189, loss: 1.6837
2022-07-16 15:05:22 - train: epoch 0085, iter [02100, 05004], lr: 0.020168, loss: 1.2890
2022-07-16 15:05:56 - train: epoch 0085, iter [02200, 05004], lr: 0.020147, loss: 1.7601
2022-07-16 15:06:30 - train: epoch 0085, iter [02300, 05004], lr: 0.020126, loss: 1.4816
2022-07-16 15:07:05 - train: epoch 0085, iter [02400, 05004], lr: 0.020105, loss: 1.8087
2022-07-16 15:07:40 - train: epoch 0085, iter [02500, 05004], lr: 0.020084, loss: 1.9439
2022-07-16 15:08:13 - train: epoch 0085, iter [02600, 05004], lr: 0.020063, loss: 1.6544
2022-07-16 15:08:48 - train: epoch 0085, iter [02700, 05004], lr: 0.020042, loss: 1.5915
2022-07-16 15:09:22 - train: epoch 0085, iter [02800, 05004], lr: 0.020021, loss: 1.8950
2022-07-16 15:09:57 - train: epoch 0085, iter [02900, 05004], lr: 0.020000, loss: 1.7524
2022-07-16 15:10:31 - train: epoch 0085, iter [03000, 05004], lr: 0.019979, loss: 1.8392
2022-07-16 15:11:05 - train: epoch 0085, iter [03100, 05004], lr: 0.019959, loss: 1.6720
2022-07-16 15:11:40 - train: epoch 0085, iter [03200, 05004], lr: 0.019938, loss: 1.7300
2022-07-16 15:12:14 - train: epoch 0085, iter [03300, 05004], lr: 0.019917, loss: 1.7001
2022-07-16 15:12:48 - train: epoch 0085, iter [03400, 05004], lr: 0.019896, loss: 1.7491
2022-07-16 15:13:22 - train: epoch 0085, iter [03500, 05004], lr: 0.019875, loss: 1.7749
2022-07-16 15:13:56 - train: epoch 0085, iter [03600, 05004], lr: 0.019854, loss: 1.8486
2022-07-16 15:14:32 - train: epoch 0085, iter [03700, 05004], lr: 0.019833, loss: 1.4974
2022-07-16 15:15:06 - train: epoch 0085, iter [03800, 05004], lr: 0.019812, loss: 1.6439
2022-07-16 15:15:41 - train: epoch 0085, iter [03900, 05004], lr: 0.019792, loss: 1.6610
2022-07-16 15:16:15 - train: epoch 0085, iter [04000, 05004], lr: 0.019771, loss: 1.9475
2022-07-16 15:16:50 - train: epoch 0085, iter [04100, 05004], lr: 0.019750, loss: 1.6859
2022-07-16 15:17:23 - train: epoch 0085, iter [04200, 05004], lr: 0.019729, loss: 1.8146
2022-07-16 15:17:59 - train: epoch 0085, iter [04300, 05004], lr: 0.019708, loss: 1.6147
2022-07-16 15:18:32 - train: epoch 0085, iter [04400, 05004], lr: 0.019687, loss: 1.6473
2022-07-16 15:19:07 - train: epoch 0085, iter [04500, 05004], lr: 0.019667, loss: 1.8501
2022-07-16 15:19:41 - train: epoch 0085, iter [04600, 05004], lr: 0.019646, loss: 1.7127
2022-07-16 15:20:16 - train: epoch 0085, iter [04700, 05004], lr: 0.019625, loss: 1.9505
2022-07-16 15:20:51 - train: epoch 0085, iter [04800, 05004], lr: 0.019604, loss: 1.3595
2022-07-16 15:21:25 - train: epoch 0085, iter [04900, 05004], lr: 0.019584, loss: 1.7299
2022-07-16 15:21:58 - train: epoch 0085, iter [05000, 05004], lr: 0.019563, loss: 1.5820
2022-07-16 15:21:59 - train: epoch 085, train_loss: 1.6827
2022-07-16 15:23:15 - eval: epoch: 085, acc1: 64.340%, acc5: 85.956%, test_loss: 1.4858, per_image_load_time: 1.909ms, per_image_inference_time: 0.222ms
2022-07-16 15:23:15 - until epoch: 085, best_acc1: 64.340%
2022-07-16 15:23:15 - epoch 086 lr: 0.019562
2022-07-16 15:23:55 - train: epoch 0086, iter [00100, 05004], lr: 0.019541, loss: 1.4892
2022-07-16 15:24:29 - train: epoch 0086, iter [00200, 05004], lr: 0.019520, loss: 1.5705
2022-07-16 15:25:04 - train: epoch 0086, iter [00300, 05004], lr: 0.019500, loss: 1.8005
2022-07-16 15:25:38 - train: epoch 0086, iter [00400, 05004], lr: 0.019479, loss: 1.7539
2022-07-16 15:26:12 - train: epoch 0086, iter [00500, 05004], lr: 0.019458, loss: 1.6172
2022-07-16 15:26:47 - train: epoch 0086, iter [00600, 05004], lr: 0.019438, loss: 1.6374
2022-07-16 15:27:21 - train: epoch 0086, iter [00700, 05004], lr: 0.019417, loss: 1.5556
2022-07-16 15:27:56 - train: epoch 0086, iter [00800, 05004], lr: 0.019396, loss: 1.8730
2022-07-16 15:28:31 - train: epoch 0086, iter [00900, 05004], lr: 0.019375, loss: 1.7559
2022-07-16 15:29:04 - train: epoch 0086, iter [01000, 05004], lr: 0.019355, loss: 1.6921
2022-07-16 15:29:39 - train: epoch 0086, iter [01100, 05004], lr: 0.019334, loss: 1.7649
2022-07-16 15:30:13 - train: epoch 0086, iter [01200, 05004], lr: 0.019313, loss: 1.5272
2022-07-16 15:30:49 - train: epoch 0086, iter [01300, 05004], lr: 0.019293, loss: 1.7960
2022-07-16 15:31:22 - train: epoch 0086, iter [01400, 05004], lr: 0.019272, loss: 1.5992
2022-07-16 15:31:57 - train: epoch 0086, iter [01500, 05004], lr: 0.019252, loss: 1.5012
2022-07-16 15:32:31 - train: epoch 0086, iter [01600, 05004], lr: 0.019231, loss: 1.7037
2022-07-16 15:33:06 - train: epoch 0086, iter [01700, 05004], lr: 0.019210, loss: 1.5828
2022-07-16 15:33:41 - train: epoch 0086, iter [01800, 05004], lr: 0.019190, loss: 1.8031
2022-07-16 15:34:15 - train: epoch 0086, iter [01900, 05004], lr: 0.019169, loss: 1.8201
2022-07-16 15:34:50 - train: epoch 0086, iter [02000, 05004], lr: 0.019149, loss: 1.6269
2022-07-16 15:35:24 - train: epoch 0086, iter [02100, 05004], lr: 0.019128, loss: 1.5205
2022-07-16 15:35:59 - train: epoch 0086, iter [02200, 05004], lr: 0.019107, loss: 1.6289
2022-07-16 15:36:33 - train: epoch 0086, iter [02300, 05004], lr: 0.019087, loss: 1.6713
2022-07-16 15:37:08 - train: epoch 0086, iter [02400, 05004], lr: 0.019066, loss: 1.3992
2022-07-16 15:37:42 - train: epoch 0086, iter [02500, 05004], lr: 0.019046, loss: 1.5617
2022-07-16 15:38:16 - train: epoch 0086, iter [02600, 05004], lr: 0.019025, loss: 1.6393
2022-07-16 15:38:51 - train: epoch 0086, iter [02700, 05004], lr: 0.019005, loss: 1.6429
2022-07-16 15:39:26 - train: epoch 0086, iter [02800, 05004], lr: 0.018984, loss: 1.7041
2022-07-16 15:40:01 - train: epoch 0086, iter [02900, 05004], lr: 0.018964, loss: 1.4713
2022-07-16 15:40:35 - train: epoch 0086, iter [03000, 05004], lr: 0.018943, loss: 1.7481
2022-07-16 15:41:10 - train: epoch 0086, iter [03100, 05004], lr: 0.018923, loss: 1.5746
2022-07-16 15:41:44 - train: epoch 0086, iter [03200, 05004], lr: 0.018902, loss: 1.8118
2022-07-16 15:42:20 - train: epoch 0086, iter [03300, 05004], lr: 0.018882, loss: 1.8550
2022-07-16 15:42:54 - train: epoch 0086, iter [03400, 05004], lr: 0.018861, loss: 1.7743
2022-07-16 15:43:29 - train: epoch 0086, iter [03500, 05004], lr: 0.018841, loss: 1.6606
2022-07-16 15:44:04 - train: epoch 0086, iter [03600, 05004], lr: 0.018820, loss: 1.7350
2022-07-16 15:44:39 - train: epoch 0086, iter [03700, 05004], lr: 0.018800, loss: 1.7517
2022-07-16 15:45:13 - train: epoch 0086, iter [03800, 05004], lr: 0.018779, loss: 1.6100
2022-07-16 15:45:47 - train: epoch 0086, iter [03900, 05004], lr: 0.018759, loss: 1.7214
2022-07-16 15:46:22 - train: epoch 0086, iter [04000, 05004], lr: 0.018739, loss: 1.7742
2022-07-16 15:46:57 - train: epoch 0086, iter [04100, 05004], lr: 0.018718, loss: 1.6095
2022-07-16 15:47:33 - train: epoch 0086, iter [04200, 05004], lr: 0.018698, loss: 1.7703
2022-07-16 15:48:07 - train: epoch 0086, iter [04300, 05004], lr: 0.018677, loss: 1.7798
2022-07-16 15:48:42 - train: epoch 0086, iter [04400, 05004], lr: 0.018657, loss: 1.4875
2022-07-16 15:49:17 - train: epoch 0086, iter [04500, 05004], lr: 0.018637, loss: 1.5016
2022-07-16 15:49:51 - train: epoch 0086, iter [04600, 05004], lr: 0.018616, loss: 1.5078
2022-07-16 15:50:26 - train: epoch 0086, iter [04700, 05004], lr: 0.018596, loss: 1.7061
2022-07-16 15:51:01 - train: epoch 0086, iter [04800, 05004], lr: 0.018575, loss: 1.7763
2022-07-16 15:51:36 - train: epoch 0086, iter [04900, 05004], lr: 0.018555, loss: 1.6980
2022-07-16 15:52:08 - train: epoch 0086, iter [05000, 05004], lr: 0.018535, loss: 1.6003
2022-07-16 15:52:09 - train: epoch 086, train_loss: 1.6663
2022-07-16 15:53:26 - eval: epoch: 086, acc1: 64.616%, acc5: 85.990%, test_loss: 1.4809, per_image_load_time: 2.301ms, per_image_inference_time: 0.243ms
2022-07-16 15:53:26 - until epoch: 086, best_acc1: 64.616%
2022-07-16 15:53:26 - epoch 087 lr: 0.018534
2022-07-16 15:54:06 - train: epoch 0087, iter [00100, 05004], lr: 0.018514, loss: 1.7605
2022-07-16 15:54:40 - train: epoch 0087, iter [00200, 05004], lr: 0.018493, loss: 1.4387
2022-07-16 15:55:14 - train: epoch 0087, iter [00300, 05004], lr: 0.018473, loss: 1.7444
2022-07-16 15:55:49 - train: epoch 0087, iter [00400, 05004], lr: 0.018453, loss: 1.8152
2022-07-16 15:56:22 - train: epoch 0087, iter [00500, 05004], lr: 0.018432, loss: 1.3674
2022-07-16 15:56:57 - train: epoch 0087, iter [00600, 05004], lr: 0.018412, loss: 1.6958
2022-07-16 15:57:31 - train: epoch 0087, iter [00700, 05004], lr: 0.018392, loss: 1.4260
2022-07-16 15:58:06 - train: epoch 0087, iter [00800, 05004], lr: 0.018372, loss: 1.7276
2022-07-16 15:58:40 - train: epoch 0087, iter [00900, 05004], lr: 0.018351, loss: 1.7277
2022-07-16 15:59:15 - train: epoch 0087, iter [01000, 05004], lr: 0.018331, loss: 1.5951
2022-07-16 15:59:49 - train: epoch 0087, iter [01100, 05004], lr: 0.018311, loss: 1.7682
2022-07-16 16:00:23 - train: epoch 0087, iter [01200, 05004], lr: 0.018291, loss: 1.6228
2022-07-16 16:00:57 - train: epoch 0087, iter [01300, 05004], lr: 0.018270, loss: 1.8522
2022-07-16 16:01:33 - train: epoch 0087, iter [01400, 05004], lr: 0.018250, loss: 1.5982
2022-07-16 16:02:06 - train: epoch 0087, iter [01500, 05004], lr: 0.018230, loss: 1.5085
2022-07-16 16:02:41 - train: epoch 0087, iter [01600, 05004], lr: 0.018210, loss: 1.4421
2022-07-16 16:03:16 - train: epoch 0087, iter [01700, 05004], lr: 0.018190, loss: 1.7037
2022-07-16 16:03:50 - train: epoch 0087, iter [01800, 05004], lr: 0.018169, loss: 1.6491
2022-07-16 16:04:25 - train: epoch 0087, iter [01900, 05004], lr: 0.018149, loss: 1.7381
2022-07-16 16:04:59 - train: epoch 0087, iter [02000, 05004], lr: 0.018129, loss: 1.6179
2022-07-16 16:05:34 - train: epoch 0087, iter [02100, 05004], lr: 0.018109, loss: 1.8370
2022-07-16 16:06:10 - train: epoch 0087, iter [02200, 05004], lr: 0.018089, loss: 1.7573
2022-07-16 16:06:43 - train: epoch 0087, iter [02300, 05004], lr: 0.018069, loss: 1.6347
2022-07-16 16:07:18 - train: epoch 0087, iter [02400, 05004], lr: 0.018049, loss: 1.5202
2022-07-16 16:07:52 - train: epoch 0087, iter [02500, 05004], lr: 0.018028, loss: 1.6305
2022-07-16 16:08:26 - train: epoch 0087, iter [02600, 05004], lr: 0.018008, loss: 1.5917
2022-07-16 16:09:00 - train: epoch 0087, iter [02700, 05004], lr: 0.017988, loss: 1.5840
2022-07-16 16:09:35 - train: epoch 0087, iter [02800, 05004], lr: 0.017968, loss: 1.5430
2022-07-16 16:10:09 - train: epoch 0087, iter [02900, 05004], lr: 0.017948, loss: 1.5313
2022-07-16 16:10:43 - train: epoch 0087, iter [03000, 05004], lr: 0.017928, loss: 1.7116
2022-07-16 16:11:17 - train: epoch 0087, iter [03100, 05004], lr: 0.017908, loss: 1.7219
2022-07-16 16:11:52 - train: epoch 0087, iter [03200, 05004], lr: 0.017888, loss: 1.6305
2022-07-16 16:12:27 - train: epoch 0087, iter [03300, 05004], lr: 0.017868, loss: 1.6690
2022-07-16 16:13:01 - train: epoch 0087, iter [03400, 05004], lr: 0.017848, loss: 1.6476
2022-07-16 16:13:35 - train: epoch 0087, iter [03500, 05004], lr: 0.017828, loss: 1.5680
2022-07-16 16:14:10 - train: epoch 0087, iter [03600, 05004], lr: 0.017808, loss: 1.4984
2022-07-16 16:14:44 - train: epoch 0087, iter [03700, 05004], lr: 0.017788, loss: 1.5919
2022-07-16 16:15:19 - train: epoch 0087, iter [03800, 05004], lr: 0.017768, loss: 1.7189
2022-07-16 16:15:53 - train: epoch 0087, iter [03900, 05004], lr: 0.017748, loss: 1.5450
2022-07-16 16:16:27 - train: epoch 0087, iter [04000, 05004], lr: 0.017728, loss: 1.7624
2022-07-16 16:17:01 - train: epoch 0087, iter [04100, 05004], lr: 0.017708, loss: 1.6924
2022-07-16 16:17:35 - train: epoch 0087, iter [04200, 05004], lr: 0.017688, loss: 1.6891
2022-07-16 16:18:10 - train: epoch 0087, iter [04300, 05004], lr: 0.017668, loss: 1.6365
2022-07-16 16:18:45 - train: epoch 0087, iter [04400, 05004], lr: 0.017648, loss: 1.7504
2022-07-16 16:19:19 - train: epoch 0087, iter [04500, 05004], lr: 0.017628, loss: 1.7343
2022-07-16 16:19:54 - train: epoch 0087, iter [04600, 05004], lr: 0.017608, loss: 1.7016
2022-07-16 16:20:28 - train: epoch 0087, iter [04700, 05004], lr: 0.017588, loss: 1.6931
2022-07-16 16:21:03 - train: epoch 0087, iter [04800, 05004], lr: 0.017568, loss: 1.6384
2022-07-16 16:21:38 - train: epoch 0087, iter [04900, 05004], lr: 0.017548, loss: 1.6463
2022-07-16 16:22:11 - train: epoch 0087, iter [05000, 05004], lr: 0.017528, loss: 1.6744
2022-07-16 16:22:11 - train: epoch 087, train_loss: 1.6513
2022-07-16 16:23:28 - eval: epoch: 087, acc1: 64.916%, acc5: 86.070%, test_loss: 1.4628, per_image_load_time: 1.076ms, per_image_inference_time: 0.215ms
2022-07-16 16:23:28 - until epoch: 087, best_acc1: 64.916%
2022-07-16 16:23:28 - epoch 088 lr: 0.017527
2022-07-16 16:24:08 - train: epoch 0088, iter [00100, 05004], lr: 0.017508, loss: 1.5390
2022-07-16 16:24:42 - train: epoch 0088, iter [00200, 05004], lr: 0.017488, loss: 1.5584
2022-07-16 16:25:16 - train: epoch 0088, iter [00300, 05004], lr: 0.017468, loss: 1.6262
2022-07-16 16:25:50 - train: epoch 0088, iter [00400, 05004], lr: 0.017448, loss: 1.6910
2022-07-16 16:26:24 - train: epoch 0088, iter [00500, 05004], lr: 0.017428, loss: 1.5259
2022-07-16 16:26:58 - train: epoch 0088, iter [00600, 05004], lr: 0.017408, loss: 1.6238
2022-07-16 16:27:33 - train: epoch 0088, iter [00700, 05004], lr: 0.017389, loss: 1.5930
2022-07-16 16:28:07 - train: epoch 0088, iter [00800, 05004], lr: 0.017369, loss: 1.5021
2022-07-16 16:28:41 - train: epoch 0088, iter [00900, 05004], lr: 0.017349, loss: 1.7421
2022-07-16 16:29:15 - train: epoch 0088, iter [01000, 05004], lr: 0.017329, loss: 1.5963
2022-07-16 16:29:49 - train: epoch 0088, iter [01100, 05004], lr: 0.017309, loss: 1.6855
2022-07-16 16:30:23 - train: epoch 0088, iter [01200, 05004], lr: 0.017290, loss: 1.4645
2022-07-16 16:30:58 - train: epoch 0088, iter [01300, 05004], lr: 0.017270, loss: 1.6357
2022-07-16 16:31:31 - train: epoch 0088, iter [01400, 05004], lr: 0.017250, loss: 1.5041
2022-07-16 16:32:06 - train: epoch 0088, iter [01500, 05004], lr: 0.017230, loss: 1.6490
2022-07-16 16:32:41 - train: epoch 0088, iter [01600, 05004], lr: 0.017210, loss: 1.5599
2022-07-16 16:33:16 - train: epoch 0088, iter [01700, 05004], lr: 0.017191, loss: 1.6647
2022-07-16 16:33:51 - train: epoch 0088, iter [01800, 05004], lr: 0.017171, loss: 1.5977
2022-07-16 16:34:26 - train: epoch 0088, iter [01900, 05004], lr: 0.017151, loss: 1.7948
2022-07-16 16:35:01 - train: epoch 0088, iter [02000, 05004], lr: 0.017132, loss: 1.5189
2022-07-16 16:35:36 - train: epoch 0088, iter [02100, 05004], lr: 0.017112, loss: 1.6356
2022-07-16 16:36:10 - train: epoch 0088, iter [02200, 05004], lr: 0.017092, loss: 1.5191
2022-07-16 16:36:45 - train: epoch 0088, iter [02300, 05004], lr: 0.017072, loss: 1.4912
2022-07-16 16:37:19 - train: epoch 0088, iter [02400, 05004], lr: 0.017053, loss: 1.7944
2022-07-16 16:37:54 - train: epoch 0088, iter [02500, 05004], lr: 0.017033, loss: 1.7551
2022-07-16 16:38:28 - train: epoch 0088, iter [02600, 05004], lr: 0.017013, loss: 1.6618
2022-07-16 16:39:02 - train: epoch 0088, iter [02700, 05004], lr: 0.016994, loss: 1.6725
2022-07-16 16:39:37 - train: epoch 0088, iter [02800, 05004], lr: 0.016974, loss: 1.7127
2022-07-16 16:40:11 - train: epoch 0088, iter [02900, 05004], lr: 0.016955, loss: 1.5855
2022-07-16 16:40:46 - train: epoch 0088, iter [03000, 05004], lr: 0.016935, loss: 1.7786
2022-07-16 16:41:20 - train: epoch 0088, iter [03100, 05004], lr: 0.016915, loss: 1.5278
2022-07-16 16:41:54 - train: epoch 0088, iter [03200, 05004], lr: 0.016896, loss: 1.5358
2022-07-16 16:42:29 - train: epoch 0088, iter [03300, 05004], lr: 0.016876, loss: 1.4080
2022-07-16 16:43:03 - train: epoch 0088, iter [03400, 05004], lr: 0.016856, loss: 1.4180
2022-07-16 16:43:38 - train: epoch 0088, iter [03500, 05004], lr: 0.016837, loss: 1.5309
2022-07-16 16:44:12 - train: epoch 0088, iter [03600, 05004], lr: 0.016817, loss: 1.6365
2022-07-16 16:44:46 - train: epoch 0088, iter [03700, 05004], lr: 0.016798, loss: 1.6233
2022-07-16 16:45:19 - train: epoch 0088, iter [03800, 05004], lr: 0.016778, loss: 1.7060
2022-07-16 16:45:54 - train: epoch 0088, iter [03900, 05004], lr: 0.016759, loss: 1.7665
2022-07-16 16:46:29 - train: epoch 0088, iter [04000, 05004], lr: 0.016739, loss: 1.4706
2022-07-16 16:47:02 - train: epoch 0088, iter [04100, 05004], lr: 0.016720, loss: 1.6840
2022-07-16 16:47:37 - train: epoch 0088, iter [04200, 05004], lr: 0.016700, loss: 1.8085
2022-07-16 16:48:11 - train: epoch 0088, iter [04300, 05004], lr: 0.016681, loss: 1.6651
2022-07-16 16:48:45 - train: epoch 0088, iter [04400, 05004], lr: 0.016661, loss: 1.4833
2022-07-16 16:49:19 - train: epoch 0088, iter [04500, 05004], lr: 0.016642, loss: 1.5991
2022-07-16 16:49:52 - train: epoch 0088, iter [04600, 05004], lr: 0.016622, loss: 1.7310
2022-07-16 16:50:27 - train: epoch 0088, iter [04700, 05004], lr: 0.016603, loss: 1.7564
2022-07-16 16:51:02 - train: epoch 0088, iter [04800, 05004], lr: 0.016583, loss: 1.4974
2022-07-16 16:51:37 - train: epoch 0088, iter [04900, 05004], lr: 0.016564, loss: 1.4088
2022-07-16 16:52:09 - train: epoch 0088, iter [05000, 05004], lr: 0.016544, loss: 1.6666
2022-07-16 16:52:10 - train: epoch 088, train_loss: 1.6357
2022-07-16 16:53:26 - eval: epoch: 088, acc1: 65.074%, acc5: 86.476%, test_loss: 1.4446, per_image_load_time: 2.706ms, per_image_inference_time: 0.234ms
2022-07-16 16:53:26 - until epoch: 088, best_acc1: 65.074%
2022-07-16 16:53:26 - epoch 089 lr: 0.016543
2022-07-16 16:54:06 - train: epoch 0089, iter [00100, 05004], lr: 0.016524, loss: 1.7831
2022-07-16 16:54:40 - train: epoch 0089, iter [00200, 05004], lr: 0.016505, loss: 1.4000
2022-07-16 16:55:15 - train: epoch 0089, iter [00300, 05004], lr: 0.016485, loss: 1.6528
2022-07-16 16:55:49 - train: epoch 0089, iter [00400, 05004], lr: 0.016466, loss: 1.5128
2022-07-16 16:56:24 - train: epoch 0089, iter [00500, 05004], lr: 0.016446, loss: 1.6403
2022-07-16 16:56:58 - train: epoch 0089, iter [00600, 05004], lr: 0.016427, loss: 1.4851
2022-07-16 16:57:32 - train: epoch 0089, iter [00700, 05004], lr: 0.016408, loss: 1.6849
2022-07-16 16:58:06 - train: epoch 0089, iter [00800, 05004], lr: 0.016388, loss: 1.8938
2022-07-16 16:58:42 - train: epoch 0089, iter [00900, 05004], lr: 0.016369, loss: 1.6062
2022-07-16 16:59:15 - train: epoch 0089, iter [01000, 05004], lr: 0.016350, loss: 1.7379
2022-07-16 16:59:51 - train: epoch 0089, iter [01100, 05004], lr: 0.016330, loss: 1.6167
2022-07-16 17:00:27 - train: epoch 0089, iter [01200, 05004], lr: 0.016311, loss: 1.7748
2022-07-16 17:01:00 - train: epoch 0089, iter [01300, 05004], lr: 0.016292, loss: 1.6335
2022-07-16 17:01:35 - train: epoch 0089, iter [01400, 05004], lr: 0.016272, loss: 1.6095
2022-07-16 17:02:10 - train: epoch 0089, iter [01500, 05004], lr: 0.016253, loss: 1.5718
2022-07-16 17:02:45 - train: epoch 0089, iter [01600, 05004], lr: 0.016234, loss: 1.4499
2022-07-16 17:03:20 - train: epoch 0089, iter [01700, 05004], lr: 0.016214, loss: 1.6386
2022-07-16 17:03:56 - train: epoch 0089, iter [01800, 05004], lr: 0.016195, loss: 1.6604
2022-07-16 17:04:30 - train: epoch 0089, iter [01900, 05004], lr: 0.016176, loss: 1.5624
2022-07-16 17:05:06 - train: epoch 0089, iter [02000, 05004], lr: 0.016157, loss: 1.5163
2022-07-16 17:05:40 - train: epoch 0089, iter [02100, 05004], lr: 0.016137, loss: 1.7462
2022-07-16 17:06:14 - train: epoch 0089, iter [02200, 05004], lr: 0.016118, loss: 1.6644
2022-07-16 17:06:50 - train: epoch 0089, iter [02300, 05004], lr: 0.016099, loss: 1.5209
2022-07-16 17:07:24 - train: epoch 0089, iter [02400, 05004], lr: 0.016080, loss: 1.8229
2022-07-16 17:08:00 - train: epoch 0089, iter [02500, 05004], lr: 0.016060, loss: 1.6781
2022-07-16 17:08:34 - train: epoch 0089, iter [02600, 05004], lr: 0.016041, loss: 1.5776
2022-07-16 17:09:08 - train: epoch 0089, iter [02700, 05004], lr: 0.016022, loss: 1.6067
2022-07-16 17:09:44 - train: epoch 0089, iter [02800, 05004], lr: 0.016003, loss: 1.7588
2022-07-16 17:10:18 - train: epoch 0089, iter [02900, 05004], lr: 0.015984, loss: 1.5492
2022-07-16 17:10:54 - train: epoch 0089, iter [03000, 05004], lr: 0.015964, loss: 1.8025
2022-07-16 17:11:29 - train: epoch 0089, iter [03100, 05004], lr: 0.015945, loss: 1.7501
2022-07-16 17:12:04 - train: epoch 0089, iter [03200, 05004], lr: 0.015926, loss: 1.7222
2022-07-16 17:12:39 - train: epoch 0089, iter [03300, 05004], lr: 0.015907, loss: 1.9026
2022-07-16 17:13:14 - train: epoch 0089, iter [03400, 05004], lr: 0.015888, loss: 1.6679
2022-07-16 17:13:49 - train: epoch 0089, iter [03500, 05004], lr: 0.015869, loss: 1.3374
2022-07-16 17:14:23 - train: epoch 0089, iter [03600, 05004], lr: 0.015850, loss: 1.5938
2022-07-16 17:14:58 - train: epoch 0089, iter [03700, 05004], lr: 0.015831, loss: 1.8225
2022-07-16 17:15:34 - train: epoch 0089, iter [03800, 05004], lr: 0.015811, loss: 1.5631
2022-07-16 17:16:08 - train: epoch 0089, iter [03900, 05004], lr: 0.015792, loss: 1.7673
2022-07-16 17:16:43 - train: epoch 0089, iter [04000, 05004], lr: 0.015773, loss: 1.5443
2022-07-16 17:17:18 - train: epoch 0089, iter [04100, 05004], lr: 0.015754, loss: 1.8485
2022-07-16 17:17:52 - train: epoch 0089, iter [04200, 05004], lr: 0.015735, loss: 1.6404
2022-07-16 17:18:28 - train: epoch 0089, iter [04300, 05004], lr: 0.015716, loss: 1.6482
2022-07-16 17:19:02 - train: epoch 0089, iter [04400, 05004], lr: 0.015697, loss: 1.6188
2022-07-16 17:19:37 - train: epoch 0089, iter [04500, 05004], lr: 0.015678, loss: 1.4388
2022-07-16 17:20:11 - train: epoch 0089, iter [04600, 05004], lr: 0.015659, loss: 1.5380
2022-07-16 17:20:47 - train: epoch 0089, iter [04700, 05004], lr: 0.015640, loss: 1.5652
2022-07-16 17:21:22 - train: epoch 0089, iter [04800, 05004], lr: 0.015621, loss: 1.7373
2022-07-16 17:21:57 - train: epoch 0089, iter [04900, 05004], lr: 0.015602, loss: 1.6459
2022-07-16 17:22:30 - train: epoch 0089, iter [05000, 05004], lr: 0.015583, loss: 1.3120
2022-07-16 17:22:31 - train: epoch 089, train_loss: 1.6213
2022-07-16 17:23:48 - eval: epoch: 089, acc1: 65.224%, acc5: 86.632%, test_loss: 1.4337, per_image_load_time: 1.511ms, per_image_inference_time: 0.230ms
2022-07-16 17:23:48 - until epoch: 089, best_acc1: 65.224%
2022-07-16 17:23:48 - epoch 090 lr: 0.015582
2022-07-16 17:24:29 - train: epoch 0090, iter [00100, 05004], lr: 0.015563, loss: 1.5770
2022-07-16 17:25:03 - train: epoch 0090, iter [00200, 05004], lr: 0.015544, loss: 1.6218
2022-07-16 17:25:38 - train: epoch 0090, iter [00300, 05004], lr: 0.015525, loss: 1.5138
2022-07-16 17:26:12 - train: epoch 0090, iter [00400, 05004], lr: 0.015506, loss: 1.4705
2022-07-16 17:26:47 - train: epoch 0090, iter [00500, 05004], lr: 0.015488, loss: 1.5972
2022-07-16 17:27:22 - train: epoch 0090, iter [00600, 05004], lr: 0.015469, loss: 1.7043
2022-07-16 17:27:57 - train: epoch 0090, iter [00700, 05004], lr: 0.015450, loss: 1.5752
2022-07-16 17:28:32 - train: epoch 0090, iter [00800, 05004], lr: 0.015431, loss: 1.5825
2022-07-16 17:29:07 - train: epoch 0090, iter [00900, 05004], lr: 0.015412, loss: 1.5438
2022-07-16 17:29:42 - train: epoch 0090, iter [01000, 05004], lr: 0.015393, loss: 1.4186
2022-07-16 17:30:17 - train: epoch 0090, iter [01100, 05004], lr: 0.015374, loss: 1.6280
2022-07-16 17:30:52 - train: epoch 0090, iter [01200, 05004], lr: 0.015355, loss: 1.5176
2022-07-16 17:31:26 - train: epoch 0090, iter [01300, 05004], lr: 0.015336, loss: 1.7584
2022-07-16 17:32:02 - train: epoch 0090, iter [01400, 05004], lr: 0.015318, loss: 1.3468
2022-07-16 17:32:36 - train: epoch 0090, iter [01500, 05004], lr: 0.015299, loss: 1.7886
2022-07-16 17:33:12 - train: epoch 0090, iter [01600, 05004], lr: 0.015280, loss: 1.6561
2022-07-16 17:33:46 - train: epoch 0090, iter [01700, 05004], lr: 0.015261, loss: 1.4199
2022-07-16 17:34:22 - train: epoch 0090, iter [01800, 05004], lr: 0.015242, loss: 1.4679
2022-07-16 17:34:56 - train: epoch 0090, iter [01900, 05004], lr: 0.015223, loss: 1.4327
2022-07-16 17:35:32 - train: epoch 0090, iter [02000, 05004], lr: 0.015205, loss: 1.6578
2022-07-16 17:36:07 - train: epoch 0090, iter [02100, 05004], lr: 0.015186, loss: 1.7450
2022-07-16 17:36:42 - train: epoch 0090, iter [02200, 05004], lr: 0.015167, loss: 1.6393
2022-07-16 17:37:17 - train: epoch 0090, iter [02300, 05004], lr: 0.015148, loss: 1.7819
2022-07-16 17:37:52 - train: epoch 0090, iter [02400, 05004], lr: 0.015130, loss: 1.6516
2022-07-16 17:38:28 - train: epoch 0090, iter [02500, 05004], lr: 0.015111, loss: 1.6585
2022-07-16 17:39:02 - train: epoch 0090, iter [02600, 05004], lr: 0.015092, loss: 1.5570
2022-07-16 17:39:37 - train: epoch 0090, iter [02700, 05004], lr: 0.015073, loss: 1.5161
2022-07-16 17:40:12 - train: epoch 0090, iter [02800, 05004], lr: 0.015055, loss: 1.5641
2022-07-16 17:40:47 - train: epoch 0090, iter [02900, 05004], lr: 0.015036, loss: 1.5534
2022-07-16 17:41:22 - train: epoch 0090, iter [03000, 05004], lr: 0.015017, loss: 1.5282
2022-07-16 17:41:57 - train: epoch 0090, iter [03100, 05004], lr: 0.014999, loss: 1.4849
2022-07-16 17:42:32 - train: epoch 0090, iter [03200, 05004], lr: 0.014980, loss: 1.4775
2022-07-16 17:43:06 - train: epoch 0090, iter [03300, 05004], lr: 0.014961, loss: 1.7332
2022-07-16 17:43:41 - train: epoch 0090, iter [03400, 05004], lr: 0.014943, loss: 1.4343
2022-07-16 17:44:16 - train: epoch 0090, iter [03500, 05004], lr: 0.014924, loss: 1.6446
2022-07-16 17:44:51 - train: epoch 0090, iter [03600, 05004], lr: 0.014905, loss: 1.4822
2022-07-16 17:45:26 - train: epoch 0090, iter [03700, 05004], lr: 0.014887, loss: 1.6974
2022-07-16 17:46:01 - train: epoch 0090, iter [03800, 05004], lr: 0.014868, loss: 1.6611
2022-07-16 17:46:36 - train: epoch 0090, iter [03900, 05004], lr: 0.014849, loss: 1.3960
2022-07-16 17:47:11 - train: epoch 0090, iter [04000, 05004], lr: 0.014831, loss: 1.5577
2022-07-16 17:47:46 - train: epoch 0090, iter [04100, 05004], lr: 0.014812, loss: 1.9905
2022-07-16 17:48:22 - train: epoch 0090, iter [04200, 05004], lr: 0.014794, loss: 1.7380
2022-07-16 17:48:56 - train: epoch 0090, iter [04300, 05004], lr: 0.014775, loss: 1.6672
2022-07-16 17:49:31 - train: epoch 0090, iter [04400, 05004], lr: 0.014757, loss: 1.5835
2022-07-16 17:50:05 - train: epoch 0090, iter [04500, 05004], lr: 0.014738, loss: 1.5362
2022-07-16 17:50:41 - train: epoch 0090, iter [04600, 05004], lr: 0.014719, loss: 1.6190
2022-07-16 17:51:15 - train: epoch 0090, iter [04700, 05004], lr: 0.014701, loss: 1.4658
2022-07-16 17:51:50 - train: epoch 0090, iter [04800, 05004], lr: 0.014682, loss: 1.7276
2022-07-16 17:52:26 - train: epoch 0090, iter [04900, 05004], lr: 0.014664, loss: 1.5451
2022-07-16 17:52:59 - train: epoch 0090, iter [05000, 05004], lr: 0.014645, loss: 1.5271
2022-07-16 17:53:00 - train: epoch 090, train_loss: 1.6043
2022-07-16 17:54:16 - eval: epoch: 090, acc1: 65.518%, acc5: 86.654%, test_loss: 1.4290, per_image_load_time: 1.225ms, per_image_inference_time: 0.236ms
2022-07-16 17:54:17 - until epoch: 090, best_acc1: 65.518%
2022-07-16 18:03:27 - epoch 091 lr: 0.014644
2022-07-16 18:04:07 - train: epoch 0091, iter [00100, 05004], lr: 0.014626, loss: 1.3609
2022-07-16 18:04:42 - train: epoch 0091, iter [00200, 05004], lr: 0.014608, loss: 1.5217
2022-07-16 18:05:17 - train: epoch 0091, iter [00300, 05004], lr: 0.014589, loss: 1.4990
2022-07-16 18:05:51 - train: epoch 0091, iter [00400, 05004], lr: 0.014571, loss: 1.5796
2022-07-16 18:06:25 - train: epoch 0091, iter [00500, 05004], lr: 0.014552, loss: 1.4018
2022-07-16 18:06:59 - train: epoch 0091, iter [00600, 05004], lr: 0.014534, loss: 1.5666
2022-07-16 18:07:34 - train: epoch 0091, iter [00700, 05004], lr: 0.014515, loss: 1.7099
2022-07-16 18:08:08 - train: epoch 0091, iter [00800, 05004], lr: 0.014497, loss: 1.3536
2022-07-16 18:08:42 - train: epoch 0091, iter [00900, 05004], lr: 0.014479, loss: 1.5213
2022-07-16 18:09:16 - train: epoch 0091, iter [01000, 05004], lr: 0.014460, loss: 1.4867
2022-07-16 18:09:50 - train: epoch 0091, iter [01100, 05004], lr: 0.014442, loss: 1.4592
2022-07-16 18:10:24 - train: epoch 0091, iter [01200, 05004], lr: 0.014423, loss: 1.4404
2022-07-16 18:10:58 - train: epoch 0091, iter [01300, 05004], lr: 0.014405, loss: 1.3704
2022-07-16 18:11:32 - train: epoch 0091, iter [01400, 05004], lr: 0.014387, loss: 1.7484
2022-07-16 18:12:06 - train: epoch 0091, iter [01500, 05004], lr: 0.014368, loss: 1.5436
2022-07-16 18:12:40 - train: epoch 0091, iter [01600, 05004], lr: 0.014350, loss: 1.6488
2022-07-16 18:13:14 - train: epoch 0091, iter [01700, 05004], lr: 0.014332, loss: 1.6440
2022-07-16 18:13:50 - train: epoch 0091, iter [01800, 05004], lr: 0.014313, loss: 1.6970
2022-07-16 18:14:24 - train: epoch 0091, iter [01900, 05004], lr: 0.014295, loss: 1.6161
2022-07-16 18:14:57 - train: epoch 0091, iter [02000, 05004], lr: 0.014277, loss: 1.5930
2022-07-16 18:15:32 - train: epoch 0091, iter [02100, 05004], lr: 0.014258, loss: 1.5793
2022-07-16 18:16:06 - train: epoch 0091, iter [02200, 05004], lr: 0.014240, loss: 1.5699
2022-07-16 18:16:40 - train: epoch 0091, iter [02300, 05004], lr: 0.014222, loss: 1.4150
2022-07-16 18:17:14 - train: epoch 0091, iter [02400, 05004], lr: 0.014204, loss: 1.3578
2022-07-16 18:17:49 - train: epoch 0091, iter [02500, 05004], lr: 0.014185, loss: 1.6540
2022-07-16 18:18:23 - train: epoch 0091, iter [02600, 05004], lr: 0.014167, loss: 1.5249
2022-07-16 18:18:58 - train: epoch 0091, iter [02700, 05004], lr: 0.014149, loss: 1.7288
2022-07-16 18:19:32 - train: epoch 0091, iter [02800, 05004], lr: 0.014131, loss: 1.6223
2022-07-16 18:20:07 - train: epoch 0091, iter [02900, 05004], lr: 0.014112, loss: 1.6821
2022-07-16 18:20:42 - train: epoch 0091, iter [03000, 05004], lr: 0.014094, loss: 1.7348
2022-07-16 18:21:15 - train: epoch 0091, iter [03100, 05004], lr: 0.014076, loss: 1.5855
2022-07-16 18:21:49 - train: epoch 0091, iter [03200, 05004], lr: 0.014058, loss: 1.6392
2022-07-16 18:22:22 - train: epoch 0091, iter [03300, 05004], lr: 0.014040, loss: 1.7248
2022-07-16 18:22:58 - train: epoch 0091, iter [03400, 05004], lr: 0.014021, loss: 1.6219
2022-07-16 18:23:33 - train: epoch 0091, iter [03500, 05004], lr: 0.014003, loss: 1.4486
2022-07-16 18:24:07 - train: epoch 0091, iter [03600, 05004], lr: 0.013985, loss: 1.6481
2022-07-16 18:24:41 - train: epoch 0091, iter [03700, 05004], lr: 0.013967, loss: 1.6066
2022-07-16 18:25:16 - train: epoch 0091, iter [03800, 05004], lr: 0.013949, loss: 1.5044
2022-07-16 18:25:50 - train: epoch 0091, iter [03900, 05004], lr: 0.013931, loss: 1.4637
2022-07-16 18:26:24 - train: epoch 0091, iter [04000, 05004], lr: 0.013913, loss: 1.5938
2022-07-16 18:26:58 - train: epoch 0091, iter [04100, 05004], lr: 0.013894, loss: 1.7165
2022-07-16 18:27:33 - train: epoch 0091, iter [04200, 05004], lr: 0.013876, loss: 1.7007
2022-07-16 18:28:07 - train: epoch 0091, iter [04300, 05004], lr: 0.013858, loss: 1.4338
2022-07-16 18:28:42 - train: epoch 0091, iter [04400, 05004], lr: 0.013840, loss: 1.3620
2022-07-16 18:29:15 - train: epoch 0091, iter [04500, 05004], lr: 0.013822, loss: 1.6172
2022-07-16 18:29:50 - train: epoch 0091, iter [04600, 05004], lr: 0.013804, loss: 1.4797
2022-07-16 18:30:25 - train: epoch 0091, iter [04700, 05004], lr: 0.013786, loss: 1.6421
2022-07-16 18:30:59 - train: epoch 0091, iter [04800, 05004], lr: 0.013768, loss: 1.5123
2022-07-16 18:31:34 - train: epoch 0091, iter [04900, 05004], lr: 0.013750, loss: 1.5371
2022-07-16 18:32:07 - train: epoch 0091, iter [05000, 05004], lr: 0.013732, loss: 1.8298
2022-07-16 18:32:08 - train: epoch 091, train_loss: 1.5880
2022-07-16 18:33:25 - eval: epoch: 091, acc1: 66.182%, acc5: 87.106%, test_loss: 1.3991, per_image_load_time: 2.062ms, per_image_inference_time: 0.234ms
2022-07-16 18:33:25 - until epoch: 091, best_acc1: 66.182%
2022-07-16 18:33:25 - epoch 092 lr: 0.013731
2022-07-16 18:34:05 - train: epoch 0092, iter [00100, 05004], lr: 0.013713, loss: 1.4935
2022-07-16 18:34:40 - train: epoch 0092, iter [00200, 05004], lr: 0.013695, loss: 1.5820
2022-07-16 18:35:15 - train: epoch 0092, iter [00300, 05004], lr: 0.013677, loss: 1.5184
2022-07-16 18:35:50 - train: epoch 0092, iter [00400, 05004], lr: 0.013659, loss: 1.5328
2022-07-16 18:36:24 - train: epoch 0092, iter [00500, 05004], lr: 0.013641, loss: 1.3432
2022-07-16 18:36:58 - train: epoch 0092, iter [00600, 05004], lr: 0.013623, loss: 1.5374
2022-07-16 18:37:33 - train: epoch 0092, iter [00700, 05004], lr: 0.013605, loss: 1.4909
2022-07-16 18:38:08 - train: epoch 0092, iter [00800, 05004], lr: 0.013588, loss: 1.5941
2022-07-16 18:38:42 - train: epoch 0092, iter [00900, 05004], lr: 0.013570, loss: 1.6152
2022-07-16 18:39:16 - train: epoch 0092, iter [01000, 05004], lr: 0.013552, loss: 1.5285
2022-07-16 18:39:51 - train: epoch 0092, iter [01100, 05004], lr: 0.013534, loss: 1.4795
2022-07-16 18:40:26 - train: epoch 0092, iter [01200, 05004], lr: 0.013516, loss: 1.6750
2022-07-16 18:41:00 - train: epoch 0092, iter [01300, 05004], lr: 0.013498, loss: 1.4480
2022-07-16 18:41:36 - train: epoch 0092, iter [01400, 05004], lr: 0.013480, loss: 1.4779
2022-07-16 18:42:11 - train: epoch 0092, iter [01500, 05004], lr: 0.013462, loss: 1.4829
2022-07-16 18:42:45 - train: epoch 0092, iter [01600, 05004], lr: 0.013444, loss: 1.4886
2022-07-16 18:43:20 - train: epoch 0092, iter [01700, 05004], lr: 0.013427, loss: 1.6099
2022-07-16 18:43:54 - train: epoch 0092, iter [01800, 05004], lr: 0.013409, loss: 1.6352
2022-07-16 18:44:29 - train: epoch 0092, iter [01900, 05004], lr: 0.013391, loss: 1.4648
2022-07-16 18:45:03 - train: epoch 0092, iter [02000, 05004], lr: 0.013373, loss: 1.3987
2022-07-16 18:45:38 - train: epoch 0092, iter [02100, 05004], lr: 0.013355, loss: 1.6019
2022-07-16 18:46:13 - train: epoch 0092, iter [02200, 05004], lr: 0.013338, loss: 1.5702
2022-07-16 18:46:47 - train: epoch 0092, iter [02300, 05004], lr: 0.013320, loss: 1.5313
2022-07-16 18:47:22 - train: epoch 0092, iter [02400, 05004], lr: 0.013302, loss: 1.5289
2022-07-16 18:47:56 - train: epoch 0092, iter [02500, 05004], lr: 0.013284, loss: 1.7377
2022-07-16 18:48:31 - train: epoch 0092, iter [02600, 05004], lr: 0.013266, loss: 1.6674
2022-07-16 18:49:07 - train: epoch 0092, iter [02700, 05004], lr: 0.013249, loss: 1.6241
2022-07-16 18:49:42 - train: epoch 0092, iter [02800, 05004], lr: 0.013231, loss: 1.5011
2022-07-16 18:50:17 - train: epoch 0092, iter [02900, 05004], lr: 0.013213, loss: 1.7131
2022-07-16 18:50:51 - train: epoch 0092, iter [03000, 05004], lr: 0.013196, loss: 1.4265
2022-07-16 18:51:26 - train: epoch 0092, iter [03100, 05004], lr: 0.013178, loss: 1.7587
2022-07-16 18:52:00 - train: epoch 0092, iter [03200, 05004], lr: 0.013160, loss: 1.4846
2022-07-16 18:52:36 - train: epoch 0092, iter [03300, 05004], lr: 0.013142, loss: 1.5860
2022-07-16 18:53:10 - train: epoch 0092, iter [03400, 05004], lr: 0.013125, loss: 1.6629
2022-07-16 18:53:46 - train: epoch 0092, iter [03500, 05004], lr: 0.013107, loss: 1.5096
2022-07-16 18:54:20 - train: epoch 0092, iter [03600, 05004], lr: 0.013090, loss: 1.6036
2022-07-16 18:54:55 - train: epoch 0092, iter [03700, 05004], lr: 0.013072, loss: 1.2266
2022-07-16 18:55:30 - train: epoch 0092, iter [03800, 05004], lr: 0.013054, loss: 1.6503
2022-07-16 18:56:05 - train: epoch 0092, iter [03900, 05004], lr: 0.013037, loss: 1.6101
2022-07-16 18:56:39 - train: epoch 0092, iter [04000, 05004], lr: 0.013019, loss: 1.5936
2022-07-16 18:57:14 - train: epoch 0092, iter [04100, 05004], lr: 0.013001, loss: 1.4447
2022-07-16 18:57:48 - train: epoch 0092, iter [04200, 05004], lr: 0.012984, loss: 1.7408
2022-07-16 18:58:24 - train: epoch 0092, iter [04300, 05004], lr: 0.012966, loss: 1.5906
2022-07-16 18:58:59 - train: epoch 0092, iter [04400, 05004], lr: 0.012949, loss: 1.6166
2022-07-16 18:59:34 - train: epoch 0092, iter [04500, 05004], lr: 0.012931, loss: 1.5154
2022-07-16 19:00:09 - train: epoch 0092, iter [04600, 05004], lr: 0.012914, loss: 1.7840
2022-07-16 19:00:45 - train: epoch 0092, iter [04700, 05004], lr: 0.012896, loss: 1.3385
2022-07-16 19:01:18 - train: epoch 0092, iter [04800, 05004], lr: 0.012878, loss: 1.6830
2022-07-16 19:01:55 - train: epoch 0092, iter [04900, 05004], lr: 0.012861, loss: 1.3565
2022-07-16 19:02:27 - train: epoch 0092, iter [05000, 05004], lr: 0.012843, loss: 1.4399
2022-07-16 19:02:28 - train: epoch 092, train_loss: 1.5696
2022-07-16 19:03:45 - eval: epoch: 092, acc1: 65.970%, acc5: 86.912%, test_loss: 1.4106, per_image_load_time: 1.804ms, per_image_inference_time: 0.235ms
2022-07-16 19:03:45 - until epoch: 092, best_acc1: 66.182%
2022-07-16 19:03:45 - epoch 093 lr: 0.012843
2022-07-16 19:04:26 - train: epoch 0093, iter [00100, 05004], lr: 0.012825, loss: 1.5316
2022-07-16 19:05:00 - train: epoch 0093, iter [00200, 05004], lr: 0.012808, loss: 1.3596
2022-07-16 19:05:35 - train: epoch 0093, iter [00300, 05004], lr: 0.012790, loss: 1.5574
2022-07-16 19:06:09 - train: epoch 0093, iter [00400, 05004], lr: 0.012773, loss: 1.6637
2022-07-16 19:06:44 - train: epoch 0093, iter [00500, 05004], lr: 0.012755, loss: 1.7561
2022-07-16 19:07:18 - train: epoch 0093, iter [00600, 05004], lr: 0.012738, loss: 1.5520
2022-07-16 19:07:52 - train: epoch 0093, iter [00700, 05004], lr: 0.012720, loss: 1.6407
2022-07-16 19:08:27 - train: epoch 0093, iter [00800, 05004], lr: 0.012703, loss: 1.4176
2022-07-16 19:09:01 - train: epoch 0093, iter [00900, 05004], lr: 0.012686, loss: 1.6670
2022-07-16 19:09:36 - train: epoch 0093, iter [01000, 05004], lr: 0.012668, loss: 1.3523
2022-07-16 19:10:11 - train: epoch 0093, iter [01100, 05004], lr: 0.012651, loss: 1.5096
2022-07-16 19:10:46 - train: epoch 0093, iter [01200, 05004], lr: 0.012633, loss: 1.7081
2022-07-16 19:11:19 - train: epoch 0093, iter [01300, 05004], lr: 0.012616, loss: 1.5340
2022-07-16 19:11:54 - train: epoch 0093, iter [01400, 05004], lr: 0.012599, loss: 1.5927
2022-07-16 19:12:28 - train: epoch 0093, iter [01500, 05004], lr: 0.012581, loss: 1.7036
2022-07-16 19:13:02 - train: epoch 0093, iter [01600, 05004], lr: 0.012564, loss: 1.4323
2022-07-16 19:13:37 - train: epoch 0093, iter [01700, 05004], lr: 0.012547, loss: 1.5138
2022-07-16 19:14:12 - train: epoch 0093, iter [01800, 05004], lr: 0.012529, loss: 1.6764
2022-07-16 19:14:46 - train: epoch 0093, iter [01900, 05004], lr: 0.012512, loss: 1.4447
2022-07-16 19:15:20 - train: epoch 0093, iter [02000, 05004], lr: 0.012495, loss: 1.6170
2022-07-16 19:15:55 - train: epoch 0093, iter [02100, 05004], lr: 0.012477, loss: 1.4448
2022-07-16 19:16:29 - train: epoch 0093, iter [02200, 05004], lr: 0.012460, loss: 1.6497
2022-07-16 19:17:06 - train: epoch 0093, iter [02300, 05004], lr: 0.012443, loss: 1.5926
2022-07-16 19:17:39 - train: epoch 0093, iter [02400, 05004], lr: 0.012426, loss: 1.5012
2022-07-16 19:18:14 - train: epoch 0093, iter [02500, 05004], lr: 0.012408, loss: 1.6098
2022-07-16 19:18:48 - train: epoch 0093, iter [02600, 05004], lr: 0.012391, loss: 1.6908
2022-07-16 19:19:23 - train: epoch 0093, iter [02700, 05004], lr: 0.012374, loss: 1.6232
2022-07-16 19:19:58 - train: epoch 0093, iter [02800, 05004], lr: 0.012357, loss: 1.5865
2022-07-16 19:20:32 - train: epoch 0093, iter [02900, 05004], lr: 0.012339, loss: 1.4018
2022-07-16 19:21:07 - train: epoch 0093, iter [03000, 05004], lr: 0.012322, loss: 1.4569
2022-07-16 19:21:41 - train: epoch 0093, iter [03100, 05004], lr: 0.012305, loss: 1.5909
2022-07-16 19:22:16 - train: epoch 0093, iter [03200, 05004], lr: 0.012288, loss: 1.4587
2022-07-16 19:22:50 - train: epoch 0093, iter [03300, 05004], lr: 0.012271, loss: 1.5190
2022-07-16 19:23:25 - train: epoch 0093, iter [03400, 05004], lr: 0.012254, loss: 1.7502
2022-07-16 19:24:00 - train: epoch 0093, iter [03500, 05004], lr: 0.012236, loss: 1.3845
2022-07-16 19:24:34 - train: epoch 0093, iter [03600, 05004], lr: 0.012219, loss: 1.6805
2022-07-16 19:25:08 - train: epoch 0093, iter [03700, 05004], lr: 0.012202, loss: 1.6414
2022-07-16 19:25:43 - train: epoch 0093, iter [03800, 05004], lr: 0.012185, loss: 1.6675
2022-07-16 19:26:17 - train: epoch 0093, iter [03900, 05004], lr: 0.012168, loss: 1.4351
2022-07-16 19:26:52 - train: epoch 0093, iter [04000, 05004], lr: 0.012151, loss: 1.6356
2022-07-16 19:27:27 - train: epoch 0093, iter [04100, 05004], lr: 0.012134, loss: 1.7369
2022-07-16 19:28:02 - train: epoch 0093, iter [04200, 05004], lr: 0.012117, loss: 1.5314
2022-07-16 19:28:36 - train: epoch 0093, iter [04300, 05004], lr: 0.012100, loss: 1.7256
2022-07-16 19:29:11 - train: epoch 0093, iter [04400, 05004], lr: 0.012083, loss: 1.4501
2022-07-16 19:29:45 - train: epoch 0093, iter [04500, 05004], lr: 0.012065, loss: 1.4931
2022-07-16 19:30:20 - train: epoch 0093, iter [04600, 05004], lr: 0.012048, loss: 1.3418
2022-07-16 19:30:54 - train: epoch 0093, iter [04700, 05004], lr: 0.012031, loss: 1.8628
2022-07-16 19:31:29 - train: epoch 0093, iter [04800, 05004], lr: 0.012014, loss: 1.5456
2022-07-16 19:32:03 - train: epoch 0093, iter [04900, 05004], lr: 0.011997, loss: 1.5507
2022-07-16 19:32:35 - train: epoch 0093, iter [05000, 05004], lr: 0.011980, loss: 1.4846
2022-07-16 19:32:36 - train: epoch 093, train_loss: 1.5549
2022-07-16 19:33:52 - eval: epoch: 093, acc1: 66.558%, acc5: 87.280%, test_loss: 1.3770, per_image_load_time: 2.042ms, per_image_inference_time: 0.204ms
2022-07-16 19:33:52 - until epoch: 093, best_acc1: 66.558%
2022-07-16 19:33:52 - epoch 094 lr: 0.011980
2022-07-16 19:34:33 - train: epoch 0094, iter [00100, 05004], lr: 0.011963, loss: 1.7083
2022-07-16 19:35:06 - train: epoch 0094, iter [00200, 05004], lr: 0.011946, loss: 1.5287
2022-07-16 19:35:40 - train: epoch 0094, iter [00300, 05004], lr: 0.011929, loss: 1.5121
2022-07-16 19:36:15 - train: epoch 0094, iter [00400, 05004], lr: 0.011912, loss: 1.5799
2022-07-16 19:36:49 - train: epoch 0094, iter [00500, 05004], lr: 0.011895, loss: 1.4176
2022-07-16 19:37:24 - train: epoch 0094, iter [00600, 05004], lr: 0.011878, loss: 1.5352
2022-07-16 19:37:58 - train: epoch 0094, iter [00700, 05004], lr: 0.011861, loss: 1.7067
2022-07-16 19:38:33 - train: epoch 0094, iter [00800, 05004], lr: 0.011844, loss: 1.5781
2022-07-16 19:39:07 - train: epoch 0094, iter [00900, 05004], lr: 0.011827, loss: 1.4829
2022-07-16 19:39:42 - train: epoch 0094, iter [01000, 05004], lr: 0.011810, loss: 1.5635
2022-07-16 19:40:15 - train: epoch 0094, iter [01100, 05004], lr: 0.011793, loss: 1.5629
2022-07-16 19:40:50 - train: epoch 0094, iter [01200, 05004], lr: 0.011777, loss: 1.4841
2022-07-16 19:41:25 - train: epoch 0094, iter [01300, 05004], lr: 0.011760, loss: 1.5605
2022-07-16 19:41:59 - train: epoch 0094, iter [01400, 05004], lr: 0.011743, loss: 1.4438
2022-07-16 19:42:34 - train: epoch 0094, iter [01500, 05004], lr: 0.011726, loss: 1.5137
2022-07-16 19:43:09 - train: epoch 0094, iter [01600, 05004], lr: 0.011709, loss: 1.7368
2022-07-16 19:43:43 - train: epoch 0094, iter [01700, 05004], lr: 0.011692, loss: 1.4102
2022-07-16 19:44:18 - train: epoch 0094, iter [01800, 05004], lr: 0.011676, loss: 1.4864
2022-07-16 19:44:52 - train: epoch 0094, iter [01900, 05004], lr: 0.011659, loss: 1.5828
2022-07-16 19:45:27 - train: epoch 0094, iter [02000, 05004], lr: 0.011642, loss: 1.4593
2022-07-16 19:46:01 - train: epoch 0094, iter [02100, 05004], lr: 0.011625, loss: 1.3880
2022-07-16 19:46:35 - train: epoch 0094, iter [02200, 05004], lr: 0.011608, loss: 1.4325
2022-07-16 19:47:10 - train: epoch 0094, iter [02300, 05004], lr: 0.011592, loss: 1.4815
2022-07-16 19:47:45 - train: epoch 0094, iter [02400, 05004], lr: 0.011575, loss: 1.4661
2022-07-16 19:48:18 - train: epoch 0094, iter [02500, 05004], lr: 0.011558, loss: 1.5149
2022-07-16 19:48:53 - train: epoch 0094, iter [02600, 05004], lr: 0.011542, loss: 1.5495
2022-07-16 19:49:28 - train: epoch 0094, iter [02700, 05004], lr: 0.011525, loss: 1.4501
2022-07-16 19:50:02 - train: epoch 0094, iter [02800, 05004], lr: 0.011508, loss: 1.6255
2022-07-16 19:50:37 - train: epoch 0094, iter [02900, 05004], lr: 0.011491, loss: 1.4183
2022-07-16 19:51:11 - train: epoch 0094, iter [03000, 05004], lr: 0.011475, loss: 1.4775
2022-07-16 19:51:46 - train: epoch 0094, iter [03100, 05004], lr: 0.011458, loss: 1.6101
2022-07-16 19:52:20 - train: epoch 0094, iter [03200, 05004], lr: 0.011441, loss: 1.6360
2022-07-16 19:52:55 - train: epoch 0094, iter [03300, 05004], lr: 0.011425, loss: 1.4528
2022-07-16 19:53:30 - train: epoch 0094, iter [03400, 05004], lr: 0.011408, loss: 1.6775
2022-07-16 19:54:05 - train: epoch 0094, iter [03500, 05004], lr: 0.011391, loss: 1.7070
2022-07-16 19:54:39 - train: epoch 0094, iter [03600, 05004], lr: 0.011375, loss: 1.2744
2022-07-16 19:55:14 - train: epoch 0094, iter [03700, 05004], lr: 0.011358, loss: 1.7076
2022-07-16 19:55:48 - train: epoch 0094, iter [03800, 05004], lr: 0.011342, loss: 1.3274
2022-07-16 19:56:23 - train: epoch 0094, iter [03900, 05004], lr: 0.011325, loss: 1.6151
2022-07-16 19:56:58 - train: epoch 0094, iter [04000, 05004], lr: 0.011309, loss: 1.5195
2022-07-16 19:57:33 - train: epoch 0094, iter [04100, 05004], lr: 0.011292, loss: 1.6820
2022-07-16 19:58:06 - train: epoch 0094, iter [04200, 05004], lr: 0.011275, loss: 1.5536
2022-07-16 19:58:41 - train: epoch 0094, iter [04300, 05004], lr: 0.011259, loss: 1.7455
2022-07-16 19:59:15 - train: epoch 0094, iter [04400, 05004], lr: 0.011242, loss: 1.7855
2022-07-16 19:59:50 - train: epoch 0094, iter [04500, 05004], lr: 0.011226, loss: 1.5174
2022-07-16 20:00:25 - train: epoch 0094, iter [04600, 05004], lr: 0.011209, loss: 1.6770
2022-07-16 20:00:59 - train: epoch 0094, iter [04700, 05004], lr: 0.011193, loss: 1.6619
2022-07-16 20:01:34 - train: epoch 0094, iter [04800, 05004], lr: 0.011176, loss: 1.4420
2022-07-16 20:02:09 - train: epoch 0094, iter [04900, 05004], lr: 0.011160, loss: 1.6402
2022-07-16 20:02:41 - train: epoch 0094, iter [05000, 05004], lr: 0.011143, loss: 1.4922
2022-07-16 20:02:42 - train: epoch 094, train_loss: 1.5386
2022-07-16 20:03:58 - eval: epoch: 094, acc1: 66.674%, acc5: 87.226%, test_loss: 1.3818, per_image_load_time: 1.098ms, per_image_inference_time: 0.215ms
2022-07-16 20:03:58 - until epoch: 094, best_acc1: 66.674%
2022-07-16 20:03:58 - epoch 095 lr: 0.011143
2022-07-16 20:04:39 - train: epoch 0095, iter [00100, 05004], lr: 0.011126, loss: 1.6145
2022-07-16 20:05:12 - train: epoch 0095, iter [00200, 05004], lr: 0.011110, loss: 1.6072
2022-07-16 20:05:47 - train: epoch 0095, iter [00300, 05004], lr: 0.011093, loss: 1.4471
2022-07-16 20:06:21 - train: epoch 0095, iter [00400, 05004], lr: 0.011077, loss: 1.6119
2022-07-16 20:06:55 - train: epoch 0095, iter [00500, 05004], lr: 0.011061, loss: 1.4688
2022-07-16 20:07:29 - train: epoch 0095, iter [00600, 05004], lr: 0.011044, loss: 1.4737
2022-07-16 20:08:04 - train: epoch 0095, iter [00700, 05004], lr: 0.011028, loss: 1.4782
2022-07-16 20:08:38 - train: epoch 0095, iter [00800, 05004], lr: 0.011011, loss: 1.6654
2022-07-16 20:09:13 - train: epoch 0095, iter [00900, 05004], lr: 0.010995, loss: 1.7825
2022-07-16 20:09:46 - train: epoch 0095, iter [01000, 05004], lr: 0.010979, loss: 1.6383
2022-07-16 20:10:21 - train: epoch 0095, iter [01100, 05004], lr: 0.010962, loss: 1.3614
2022-07-16 20:10:55 - train: epoch 0095, iter [01200, 05004], lr: 0.010946, loss: 1.3833
2022-07-16 20:11:29 - train: epoch 0095, iter [01300, 05004], lr: 0.010930, loss: 1.5045
2022-07-16 20:12:04 - train: epoch 0095, iter [01400, 05004], lr: 0.010913, loss: 1.5210
2022-07-16 20:12:38 - train: epoch 0095, iter [01500, 05004], lr: 0.010897, loss: 1.4036
2022-07-16 20:13:13 - train: epoch 0095, iter [01600, 05004], lr: 0.010881, loss: 1.2039
2022-07-16 20:13:47 - train: epoch 0095, iter [01700, 05004], lr: 0.010864, loss: 1.4415
2022-07-16 20:14:22 - train: epoch 0095, iter [01800, 05004], lr: 0.010848, loss: 1.5551
2022-07-16 20:14:55 - train: epoch 0095, iter [01900, 05004], lr: 0.010832, loss: 1.5621
2022-07-16 20:15:30 - train: epoch 0095, iter [02000, 05004], lr: 0.010816, loss: 1.4986
2022-07-16 20:16:04 - train: epoch 0095, iter [02100, 05004], lr: 0.010799, loss: 1.4980
2022-07-16 20:16:38 - train: epoch 0095, iter [02200, 05004], lr: 0.010783, loss: 1.2138
2022-07-16 20:17:13 - train: epoch 0095, iter [02300, 05004], lr: 0.010767, loss: 1.3353
2022-07-16 20:17:47 - train: epoch 0095, iter [02400, 05004], lr: 0.010751, loss: 1.5616
2022-07-16 20:18:21 - train: epoch 0095, iter [02500, 05004], lr: 0.010734, loss: 1.5084
2022-07-16 20:18:56 - train: epoch 0095, iter [02600, 05004], lr: 0.010718, loss: 1.5563
2022-07-16 20:19:30 - train: epoch 0095, iter [02700, 05004], lr: 0.010702, loss: 1.4848
2022-07-16 20:20:05 - train: epoch 0095, iter [02800, 05004], lr: 0.010686, loss: 1.4657
2022-07-16 20:20:40 - train: epoch 0095, iter [02900, 05004], lr: 0.010670, loss: 1.8142
2022-07-16 20:21:15 - train: epoch 0095, iter [03000, 05004], lr: 0.010654, loss: 1.7647
2022-07-16 20:21:49 - train: epoch 0095, iter [03100, 05004], lr: 0.010638, loss: 1.7419
2022-07-16 20:22:24 - train: epoch 0095, iter [03200, 05004], lr: 0.010621, loss: 1.5583
2022-07-16 20:22:57 - train: epoch 0095, iter [03300, 05004], lr: 0.010605, loss: 1.5761
2022-07-16 20:23:32 - train: epoch 0095, iter [03400, 05004], lr: 0.010589, loss: 1.2672
2022-07-16 20:24:05 - train: epoch 0095, iter [03500, 05004], lr: 0.010573, loss: 1.5138
2022-07-16 20:24:41 - train: epoch 0095, iter [03600, 05004], lr: 0.010557, loss: 1.2582
2022-07-16 20:25:15 - train: epoch 0095, iter [03700, 05004], lr: 0.010541, loss: 1.5215
2022-07-16 20:25:49 - train: epoch 0095, iter [03800, 05004], lr: 0.010525, loss: 1.4673
2022-07-16 20:26:23 - train: epoch 0095, iter [03900, 05004], lr: 0.010509, loss: 1.5702
2022-07-16 20:26:57 - train: epoch 0095, iter [04000, 05004], lr: 0.010493, loss: 1.3160
2022-07-16 20:27:31 - train: epoch 0095, iter [04100, 05004], lr: 0.010477, loss: 1.5734
2022-07-16 20:28:06 - train: epoch 0095, iter [04200, 05004], lr: 0.010461, loss: 1.4347
2022-07-16 20:28:40 - train: epoch 0095, iter [04300, 05004], lr: 0.010445, loss: 1.5901
2022-07-16 20:29:15 - train: epoch 0095, iter [04400, 05004], lr: 0.010429, loss: 1.6348
2022-07-16 20:29:49 - train: epoch 0095, iter [04500, 05004], lr: 0.010413, loss: 1.4400
2022-07-16 20:30:23 - train: epoch 0095, iter [04600, 05004], lr: 0.010397, loss: 1.4917
2022-07-16 20:30:57 - train: epoch 0095, iter [04700, 05004], lr: 0.010381, loss: 1.4173
2022-07-16 20:31:32 - train: epoch 0095, iter [04800, 05004], lr: 0.010365, loss: 1.6196
2022-07-16 20:32:06 - train: epoch 0095, iter [04900, 05004], lr: 0.010349, loss: 1.4032
2022-07-16 20:32:39 - train: epoch 0095, iter [05000, 05004], lr: 0.010333, loss: 1.4478
2022-07-16 20:32:40 - train: epoch 095, train_loss: 1.5211
2022-07-16 20:33:56 - eval: epoch: 095, acc1: 67.062%, acc5: 87.572%, test_loss: 1.3523, per_image_load_time: 2.093ms, per_image_inference_time: 0.226ms
2022-07-16 20:33:56 - until epoch: 095, best_acc1: 67.062%
2022-07-16 20:33:56 - epoch 096 lr: 0.010332
2022-07-16 20:34:36 - train: epoch 0096, iter [00100, 05004], lr: 0.010316, loss: 1.6292
2022-07-16 20:35:09 - train: epoch 0096, iter [00200, 05004], lr: 0.010301, loss: 1.4934
2022-07-16 20:35:43 - train: epoch 0096, iter [00300, 05004], lr: 0.010285, loss: 1.4411
2022-07-16 20:36:18 - train: epoch 0096, iter [00400, 05004], lr: 0.010269, loss: 1.1730
2022-07-16 20:36:52 - train: epoch 0096, iter [00500, 05004], lr: 0.010253, loss: 1.4454
2022-07-16 20:37:27 - train: epoch 0096, iter [00600, 05004], lr: 0.010237, loss: 1.5373
2022-07-16 20:38:01 - train: epoch 0096, iter [00700, 05004], lr: 0.010221, loss: 1.3473
2022-07-16 20:38:36 - train: epoch 0096, iter [00800, 05004], lr: 0.010205, loss: 1.2445
2022-07-16 20:39:10 - train: epoch 0096, iter [00900, 05004], lr: 0.010189, loss: 1.5128
2022-07-16 20:39:45 - train: epoch 0096, iter [01000, 05004], lr: 0.010174, loss: 1.3088
2022-07-16 20:40:19 - train: epoch 0096, iter [01100, 05004], lr: 0.010158, loss: 1.6271
2022-07-16 20:40:53 - train: epoch 0096, iter [01200, 05004], lr: 0.010142, loss: 1.4497
2022-07-16 20:41:27 - train: epoch 0096, iter [01300, 05004], lr: 0.010126, loss: 1.6083
2022-07-16 20:42:02 - train: epoch 0096, iter [01400, 05004], lr: 0.010110, loss: 1.5047
2022-07-16 20:42:36 - train: epoch 0096, iter [01500, 05004], lr: 0.010095, loss: 1.6160
2022-07-16 20:43:11 - train: epoch 0096, iter [01600, 05004], lr: 0.010079, loss: 1.4740
2022-07-16 20:43:46 - train: epoch 0096, iter [01700, 05004], lr: 0.010063, loss: 1.2735
2022-07-16 20:44:20 - train: epoch 0096, iter [01800, 05004], lr: 0.010047, loss: 1.4854
2022-07-16 20:44:55 - train: epoch 0096, iter [01900, 05004], lr: 0.010032, loss: 1.5389
2022-07-16 20:45:30 - train: epoch 0096, iter [02000, 05004], lr: 0.010016, loss: 1.3465
2022-07-16 20:46:03 - train: epoch 0096, iter [02100, 05004], lr: 0.010000, loss: 1.4660
2022-07-16 20:46:39 - train: epoch 0096, iter [02200, 05004], lr: 0.009985, loss: 1.4320
2022-07-16 20:47:14 - train: epoch 0096, iter [02300, 05004], lr: 0.009969, loss: 1.4949
2022-07-16 20:47:48 - train: epoch 0096, iter [02400, 05004], lr: 0.009953, loss: 1.4511
2022-07-16 20:48:22 - train: epoch 0096, iter [02500, 05004], lr: 0.009938, loss: 1.5230
2022-07-16 20:48:57 - train: epoch 0096, iter [02600, 05004], lr: 0.009922, loss: 1.5279
2022-07-16 20:49:32 - train: epoch 0096, iter [02700, 05004], lr: 0.009906, loss: 1.6869
2022-07-16 20:50:06 - train: epoch 0096, iter [02800, 05004], lr: 0.009891, loss: 1.6078
2022-07-16 20:50:41 - train: epoch 0096, iter [02900, 05004], lr: 0.009875, loss: 1.4798
2022-07-16 20:51:16 - train: epoch 0096, iter [03000, 05004], lr: 0.009860, loss: 1.5016
2022-07-16 20:51:51 - train: epoch 0096, iter [03100, 05004], lr: 0.009844, loss: 1.5527
2022-07-16 20:52:25 - train: epoch 0096, iter [03200, 05004], lr: 0.009828, loss: 1.5190
2022-07-16 20:53:00 - train: epoch 0096, iter [03300, 05004], lr: 0.009813, loss: 1.6224
2022-07-16 20:53:34 - train: epoch 0096, iter [03400, 05004], lr: 0.009797, loss: 1.4910
2022-07-16 20:54:09 - train: epoch 0096, iter [03500, 05004], lr: 0.009782, loss: 1.4471
2022-07-16 20:54:42 - train: epoch 0096, iter [03600, 05004], lr: 0.009766, loss: 1.4512
2022-07-16 20:55:18 - train: epoch 0096, iter [03700, 05004], lr: 0.009751, loss: 1.4715
2022-07-16 20:55:51 - train: epoch 0096, iter [03800, 05004], lr: 0.009735, loss: 1.2805
2022-07-16 20:56:26 - train: epoch 0096, iter [03900, 05004], lr: 0.009720, loss: 1.3189
2022-07-16 20:57:00 - train: epoch 0096, iter [04000, 05004], lr: 0.009704, loss: 1.5346
2022-07-16 20:57:35 - train: epoch 0096, iter [04100, 05004], lr: 0.009689, loss: 1.5271
2022-07-16 20:58:09 - train: epoch 0096, iter [04200, 05004], lr: 0.009673, loss: 1.5710
2022-07-16 20:58:44 - train: epoch 0096, iter [04300, 05004], lr: 0.009658, loss: 1.2060
2022-07-16 20:59:18 - train: epoch 0096, iter [04400, 05004], lr: 0.009642, loss: 1.4869
2022-07-16 20:59:53 - train: epoch 0096, iter [04500, 05004], lr: 0.009627, loss: 1.5668
2022-07-16 21:00:27 - train: epoch 0096, iter [04600, 05004], lr: 0.009611, loss: 1.3828
2022-07-16 21:01:02 - train: epoch 0096, iter [04700, 05004], lr: 0.009596, loss: 1.5334
2022-07-16 21:01:37 - train: epoch 0096, iter [04800, 05004], lr: 0.009581, loss: 1.5133
2022-07-16 21:02:11 - train: epoch 0096, iter [04900, 05004], lr: 0.009565, loss: 1.7347
2022-07-16 21:02:44 - train: epoch 0096, iter [05000, 05004], lr: 0.009550, loss: 1.6434
2022-07-16 21:02:45 - train: epoch 096, train_loss: 1.5062
2022-07-16 21:04:01 - eval: epoch: 096, acc1: 67.594%, acc5: 87.812%, test_loss: 1.3314, per_image_load_time: 1.463ms, per_image_inference_time: 0.235ms
2022-07-16 21:04:02 - until epoch: 096, best_acc1: 67.594%
2022-07-16 21:04:02 - epoch 097 lr: 0.009549
2022-07-16 21:04:41 - train: epoch 0097, iter [00100, 05004], lr: 0.009534, loss: 1.4231
2022-07-16 21:05:15 - train: epoch 0097, iter [00200, 05004], lr: 0.009518, loss: 1.3103
2022-07-16 21:05:49 - train: epoch 0097, iter [00300, 05004], lr: 0.009503, loss: 1.5948
2022-07-16 21:06:24 - train: epoch 0097, iter [00400, 05004], lr: 0.009488, loss: 1.5420
2022-07-16 21:06:58 - train: epoch 0097, iter [00500, 05004], lr: 0.009472, loss: 1.5035
2022-07-16 21:07:32 - train: epoch 0097, iter [00600, 05004], lr: 0.009457, loss: 1.5471
2022-07-16 21:08:07 - train: epoch 0097, iter [00700, 05004], lr: 0.009442, loss: 1.3418
2022-07-16 21:08:40 - train: epoch 0097, iter [00800, 05004], lr: 0.009426, loss: 1.3074
2022-07-16 21:09:15 - train: epoch 0097, iter [00900, 05004], lr: 0.009411, loss: 1.3545
2022-07-16 21:09:49 - train: epoch 0097, iter [01000, 05004], lr: 0.009396, loss: 1.6364
2022-07-16 21:10:23 - train: epoch 0097, iter [01100, 05004], lr: 0.009381, loss: 1.2805
2022-07-16 21:10:56 - train: epoch 0097, iter [01200, 05004], lr: 0.009365, loss: 1.6612
2022-07-16 21:11:30 - train: epoch 0097, iter [01300, 05004], lr: 0.009350, loss: 1.3692
2022-07-16 21:12:04 - train: epoch 0097, iter [01400, 05004], lr: 0.009335, loss: 1.6464
2022-07-16 21:12:37 - train: epoch 0097, iter [01500, 05004], lr: 0.009320, loss: 1.4235
2022-07-16 21:13:11 - train: epoch 0097, iter [01600, 05004], lr: 0.009305, loss: 1.3198
2022-07-16 21:13:44 - train: epoch 0097, iter [01700, 05004], lr: 0.009289, loss: 1.5922
2022-07-16 21:14:17 - train: epoch 0097, iter [01800, 05004], lr: 0.009274, loss: 1.4719
2022-07-16 21:14:50 - train: epoch 0097, iter [01900, 05004], lr: 0.009259, loss: 1.2927
2022-07-16 21:15:24 - train: epoch 0097, iter [02000, 05004], lr: 0.009244, loss: 1.5523
2022-07-16 21:15:58 - train: epoch 0097, iter [02100, 05004], lr: 0.009229, loss: 1.4880
2022-07-16 21:16:31 - train: epoch 0097, iter [02200, 05004], lr: 0.009214, loss: 1.4612
2022-07-16 21:17:05 - train: epoch 0097, iter [02300, 05004], lr: 0.009198, loss: 1.3039
2022-07-16 21:17:38 - train: epoch 0097, iter [02400, 05004], lr: 0.009183, loss: 1.4148
2022-07-16 21:18:12 - train: epoch 0097, iter [02500, 05004], lr: 0.009168, loss: 1.4626
2022-07-16 21:18:44 - train: epoch 0097, iter [02600, 05004], lr: 0.009153, loss: 1.5454
2022-07-16 21:19:19 - train: epoch 0097, iter [02700, 05004], lr: 0.009138, loss: 1.5887
2022-07-16 21:19:52 - train: epoch 0097, iter [02800, 05004], lr: 0.009123, loss: 1.5122
2022-07-16 21:20:25 - train: epoch 0097, iter [02900, 05004], lr: 0.009108, loss: 1.5113
2022-07-16 21:20:59 - train: epoch 0097, iter [03000, 05004], lr: 0.009093, loss: 1.5287
2022-07-16 21:21:32 - train: epoch 0097, iter [03100, 05004], lr: 0.009078, loss: 1.5681
2022-07-16 21:22:06 - train: epoch 0097, iter [03200, 05004], lr: 0.009063, loss: 1.8506
2022-07-16 21:22:40 - train: epoch 0097, iter [03300, 05004], lr: 0.009048, loss: 1.4922
2022-07-16 21:23:12 - train: epoch 0097, iter [03400, 05004], lr: 0.009033, loss: 1.4424
2022-07-16 21:23:46 - train: epoch 0097, iter [03500, 05004], lr: 0.009018, loss: 1.3880
2022-07-16 21:24:20 - train: epoch 0097, iter [03600, 05004], lr: 0.009003, loss: 1.4562
2022-07-16 21:24:52 - train: epoch 0097, iter [03700, 05004], lr: 0.008988, loss: 1.2494
2022-07-16 21:25:26 - train: epoch 0097, iter [03800, 05004], lr: 0.008973, loss: 1.2250
2022-07-16 21:25:59 - train: epoch 0097, iter [03900, 05004], lr: 0.008958, loss: 1.2908
2022-07-16 21:26:33 - train: epoch 0097, iter [04000, 05004], lr: 0.008943, loss: 1.6230
2022-07-16 21:27:07 - train: epoch 0097, iter [04100, 05004], lr: 0.008928, loss: 1.5444
2022-07-16 21:27:40 - train: epoch 0097, iter [04200, 05004], lr: 0.008913, loss: 1.2536
2022-07-16 21:28:14 - train: epoch 0097, iter [04300, 05004], lr: 0.008898, loss: 1.2990
2022-07-16 21:28:48 - train: epoch 0097, iter [04400, 05004], lr: 0.008883, loss: 1.4561
2022-07-16 21:29:21 - train: epoch 0097, iter [04500, 05004], lr: 0.008869, loss: 1.5267
2022-07-16 21:29:55 - train: epoch 0097, iter [04600, 05004], lr: 0.008854, loss: 1.5632
2022-07-16 21:30:28 - train: epoch 0097, iter [04700, 05004], lr: 0.008839, loss: 1.4634
2022-07-16 21:31:02 - train: epoch 0097, iter [04800, 05004], lr: 0.008824, loss: 1.6956
2022-07-16 21:31:36 - train: epoch 0097, iter [04900, 05004], lr: 0.008809, loss: 1.5524
2022-07-16 21:32:08 - train: epoch 0097, iter [05000, 05004], lr: 0.008794, loss: 1.3995
2022-07-16 21:32:09 - train: epoch 097, train_loss: 1.4850
2022-07-16 21:33:23 - eval: epoch: 097, acc1: 67.724%, acc5: 88.106%, test_loss: 1.3264, per_image_load_time: 2.254ms, per_image_inference_time: 0.225ms
2022-07-16 21:33:23 - until epoch: 097, best_acc1: 67.724%
2022-07-16 21:33:23 - epoch 098 lr: 0.008794
2022-07-16 21:34:02 - train: epoch 0098, iter [00100, 05004], lr: 0.008779, loss: 1.6093
2022-07-16 21:34:35 - train: epoch 0098, iter [00200, 05004], lr: 0.008764, loss: 1.6186
2022-07-16 21:35:08 - train: epoch 0098, iter [00300, 05004], lr: 0.008749, loss: 1.5609
2022-07-16 21:35:42 - train: epoch 0098, iter [00400, 05004], lr: 0.008735, loss: 1.3900
2022-07-16 21:36:15 - train: epoch 0098, iter [00500, 05004], lr: 0.008720, loss: 1.5858
2022-07-16 21:36:47 - train: epoch 0098, iter [00600, 05004], lr: 0.008705, loss: 1.5775
2022-07-16 21:37:21 - train: epoch 0098, iter [00700, 05004], lr: 0.008690, loss: 1.3597
2022-07-16 21:37:53 - train: epoch 0098, iter [00800, 05004], lr: 0.008676, loss: 1.5922
2022-07-16 21:38:27 - train: epoch 0098, iter [00900, 05004], lr: 0.008661, loss: 1.2833
2022-07-16 21:39:00 - train: epoch 0098, iter [01000, 05004], lr: 0.008646, loss: 1.5093
2022-07-16 21:39:35 - train: epoch 0098, iter [01100, 05004], lr: 0.008631, loss: 1.3415
2022-07-16 21:40:08 - train: epoch 0098, iter [01200, 05004], lr: 0.008617, loss: 1.3866
2022-07-16 21:40:42 - train: epoch 0098, iter [01300, 05004], lr: 0.008602, loss: 1.6261
2022-07-16 21:41:15 - train: epoch 0098, iter [01400, 05004], lr: 0.008587, loss: 1.4139
2022-07-16 21:41:48 - train: epoch 0098, iter [01500, 05004], lr: 0.008573, loss: 1.2709
2022-07-16 21:42:22 - train: epoch 0098, iter [01600, 05004], lr: 0.008558, loss: 1.5068
2022-07-16 21:42:55 - train: epoch 0098, iter [01700, 05004], lr: 0.008543, loss: 1.4886
2022-07-16 21:43:29 - train: epoch 0098, iter [01800, 05004], lr: 0.008529, loss: 1.5628
2022-07-16 21:44:02 - train: epoch 0098, iter [01900, 05004], lr: 0.008514, loss: 1.1666
2022-07-16 21:44:36 - train: epoch 0098, iter [02000, 05004], lr: 0.008500, loss: 1.4691
2022-07-16 21:45:09 - train: epoch 0098, iter [02100, 05004], lr: 0.008485, loss: 1.4440
2022-07-16 21:45:43 - train: epoch 0098, iter [02200, 05004], lr: 0.008470, loss: 1.5360
2022-07-16 21:46:16 - train: epoch 0098, iter [02300, 05004], lr: 0.008456, loss: 1.3953
2022-07-16 21:46:50 - train: epoch 0098, iter [02400, 05004], lr: 0.008441, loss: 1.3722
2022-07-16 21:47:23 - train: epoch 0098, iter [02500, 05004], lr: 0.008427, loss: 1.3789
2022-07-16 21:47:57 - train: epoch 0098, iter [02600, 05004], lr: 0.008412, loss: 1.4975
2022-07-16 21:48:30 - train: epoch 0098, iter [02700, 05004], lr: 0.008398, loss: 1.4284
2022-07-16 21:49:04 - train: epoch 0098, iter [02800, 05004], lr: 0.008383, loss: 1.6396
2022-07-16 21:49:37 - train: epoch 0098, iter [02900, 05004], lr: 0.008369, loss: 1.4050
2022-07-16 21:50:10 - train: epoch 0098, iter [03000, 05004], lr: 0.008354, loss: 1.3822
2022-07-16 21:50:44 - train: epoch 0098, iter [03100, 05004], lr: 0.008340, loss: 1.3475
2022-07-16 21:51:18 - train: epoch 0098, iter [03200, 05004], lr: 0.008325, loss: 1.2226
2022-07-16 21:51:51 - train: epoch 0098, iter [03300, 05004], lr: 0.008311, loss: 1.3345
2022-07-16 21:52:24 - train: epoch 0098, iter [03400, 05004], lr: 0.008296, loss: 1.5173
2022-07-16 21:52:58 - train: epoch 0098, iter [03500, 05004], lr: 0.008282, loss: 1.6026
2022-07-16 21:53:32 - train: epoch 0098, iter [03600, 05004], lr: 0.008268, loss: 1.6141
2022-07-16 21:54:06 - train: epoch 0098, iter [03700, 05004], lr: 0.008253, loss: 1.6027
2022-07-16 21:54:39 - train: epoch 0098, iter [03800, 05004], lr: 0.008239, loss: 1.4652
2022-07-16 21:55:12 - train: epoch 0098, iter [03900, 05004], lr: 0.008224, loss: 1.5006
2022-07-16 21:55:47 - train: epoch 0098, iter [04000, 05004], lr: 0.008210, loss: 1.5625
2022-07-16 21:56:20 - train: epoch 0098, iter [04100, 05004], lr: 0.008196, loss: 1.5862
2022-07-16 21:56:55 - train: epoch 0098, iter [04200, 05004], lr: 0.008181, loss: 1.5273
2022-07-16 21:57:29 - train: epoch 0098, iter [04300, 05004], lr: 0.008167, loss: 1.3755
2022-07-16 21:58:03 - train: epoch 0098, iter [04400, 05004], lr: 0.008153, loss: 1.6425
2022-07-16 21:58:39 - train: epoch 0098, iter [04500, 05004], lr: 0.008138, loss: 1.5987
2022-07-16 21:59:12 - train: epoch 0098, iter [04600, 05004], lr: 0.008124, loss: 1.5740
2022-07-16 21:59:45 - train: epoch 0098, iter [04700, 05004], lr: 0.008110, loss: 1.4048
2022-07-16 22:00:20 - train: epoch 0098, iter [04800, 05004], lr: 0.008096, loss: 1.2626
2022-07-16 22:00:53 - train: epoch 0098, iter [04900, 05004], lr: 0.008081, loss: 1.6282
2022-07-16 22:01:26 - train: epoch 0098, iter [05000, 05004], lr: 0.008067, loss: 1.4695
2022-07-16 22:01:27 - train: epoch 098, train_loss: 1.4703
2022-07-16 22:02:41 - eval: epoch: 098, acc1: 68.200%, acc5: 88.132%, test_loss: 1.3100, per_image_load_time: 1.644ms, per_image_inference_time: 0.230ms
2022-07-16 22:02:41 - until epoch: 098, best_acc1: 68.200%
2022-07-16 22:02:41 - epoch 099 lr: 0.008066
2022-07-16 22:03:20 - train: epoch 0099, iter [00100, 05004], lr: 0.008052, loss: 1.4966
2022-07-16 22:03:53 - train: epoch 0099, iter [00200, 05004], lr: 0.008038, loss: 1.3261
2022-07-16 22:04:26 - train: epoch 0099, iter [00300, 05004], lr: 0.008024, loss: 1.3233
2022-07-16 22:04:58 - train: epoch 0099, iter [00400, 05004], lr: 0.008010, loss: 1.6101
2022-07-16 22:05:33 - train: epoch 0099, iter [00500, 05004], lr: 0.007995, loss: 1.5242
2022-07-16 22:06:05 - train: epoch 0099, iter [00600, 05004], lr: 0.007981, loss: 1.4588
2022-07-16 22:06:37 - train: epoch 0099, iter [00700, 05004], lr: 0.007967, loss: 1.5050
2022-07-16 22:07:12 - train: epoch 0099, iter [00800, 05004], lr: 0.007953, loss: 1.4413
2022-07-16 22:07:43 - train: epoch 0099, iter [00900, 05004], lr: 0.007939, loss: 1.5055
2022-07-16 22:08:18 - train: epoch 0099, iter [01000, 05004], lr: 0.007925, loss: 1.4331
2022-07-16 22:08:51 - train: epoch 0099, iter [01100, 05004], lr: 0.007910, loss: 1.5739
2022-07-16 22:09:25 - train: epoch 0099, iter [01200, 05004], lr: 0.007896, loss: 1.1952
2022-07-16 22:09:58 - train: epoch 0099, iter [01300, 05004], lr: 0.007882, loss: 1.4019
2022-07-16 22:10:31 - train: epoch 0099, iter [01400, 05004], lr: 0.007868, loss: 1.4566
2022-07-16 22:11:04 - train: epoch 0099, iter [01500, 05004], lr: 0.007854, loss: 1.2356
2022-07-16 22:11:38 - train: epoch 0099, iter [01600, 05004], lr: 0.007840, loss: 1.4456
2022-07-16 22:12:10 - train: epoch 0099, iter [01700, 05004], lr: 0.007826, loss: 1.4072
2022-07-16 22:12:44 - train: epoch 0099, iter [01800, 05004], lr: 0.007812, loss: 1.4820
2022-07-16 22:13:17 - train: epoch 0099, iter [01900, 05004], lr: 0.007798, loss: 1.4637
2022-07-16 22:13:51 - train: epoch 0099, iter [02000, 05004], lr: 0.007784, loss: 1.4568
2022-07-16 22:14:23 - train: epoch 0099, iter [02100, 05004], lr: 0.007770, loss: 1.5413
2022-07-16 22:14:57 - train: epoch 0099, iter [02200, 05004], lr: 0.007756, loss: 1.3350
2022-07-16 22:15:30 - train: epoch 0099, iter [02300, 05004], lr: 0.007742, loss: 1.4467
2022-07-16 22:16:04 - train: epoch 0099, iter [02400, 05004], lr: 0.007728, loss: 1.5262
2022-07-16 22:16:37 - train: epoch 0099, iter [02500, 05004], lr: 0.007714, loss: 1.3288
2022-07-16 22:17:11 - train: epoch 0099, iter [02600, 05004], lr: 0.007700, loss: 1.3117
2022-07-16 22:17:44 - train: epoch 0099, iter [02700, 05004], lr: 0.007686, loss: 1.5187
2022-07-16 22:18:18 - train: epoch 0099, iter [02800, 05004], lr: 0.007672, loss: 1.5760
2022-07-16 22:18:51 - train: epoch 0099, iter [02900, 05004], lr: 0.007658, loss: 1.5277
2022-07-16 22:19:24 - train: epoch 0099, iter [03000, 05004], lr: 0.007644, loss: 1.5945
2022-07-16 22:19:57 - train: epoch 0099, iter [03100, 05004], lr: 0.007630, loss: 1.2887
2022-07-16 22:20:31 - train: epoch 0099, iter [03200, 05004], lr: 0.007616, loss: 1.5846
2022-07-16 22:21:04 - train: epoch 0099, iter [03300, 05004], lr: 0.007603, loss: 1.3992
2022-07-16 22:21:37 - train: epoch 0099, iter [03400, 05004], lr: 0.007589, loss: 1.3637
2022-07-16 22:22:10 - train: epoch 0099, iter [03500, 05004], lr: 0.007575, loss: 1.6198
2022-07-16 22:22:44 - train: epoch 0099, iter [03600, 05004], lr: 0.007561, loss: 1.5235
2022-07-16 22:23:17 - train: epoch 0099, iter [03700, 05004], lr: 0.007547, loss: 1.3008
2022-07-16 22:23:51 - train: epoch 0099, iter [03800, 05004], lr: 0.007533, loss: 1.6067
2022-07-16 22:24:26 - train: epoch 0099, iter [03900, 05004], lr: 0.007520, loss: 1.4444
2022-07-16 22:24:58 - train: epoch 0099, iter [04000, 05004], lr: 0.007506, loss: 1.4292
2022-07-16 22:25:32 - train: epoch 0099, iter [04100, 05004], lr: 0.007492, loss: 1.3520
2022-07-16 22:26:06 - train: epoch 0099, iter [04200, 05004], lr: 0.007478, loss: 1.4826
2022-07-16 22:26:39 - train: epoch 0099, iter [04300, 05004], lr: 0.007465, loss: 1.5159
2022-07-16 22:27:13 - train: epoch 0099, iter [04400, 05004], lr: 0.007451, loss: 1.4029
2022-07-16 22:27:46 - train: epoch 0099, iter [04500, 05004], lr: 0.007437, loss: 1.5704
2022-07-16 22:28:19 - train: epoch 0099, iter [04600, 05004], lr: 0.007423, loss: 1.5964
2022-07-16 22:28:52 - train: epoch 0099, iter [04700, 05004], lr: 0.007410, loss: 1.3494
2022-07-16 22:29:26 - train: epoch 0099, iter [04800, 05004], lr: 0.007396, loss: 1.3518
2022-07-16 22:29:59 - train: epoch 0099, iter [04900, 05004], lr: 0.007382, loss: 1.4371
2022-07-16 22:30:32 - train: epoch 0099, iter [05000, 05004], lr: 0.007369, loss: 1.5817
2022-07-16 22:30:32 - train: epoch 099, train_loss: 1.4500
2022-07-16 22:31:46 - eval: epoch: 099, acc1: 68.570%, acc5: 88.400%, test_loss: 1.2978, per_image_load_time: 2.300ms, per_image_inference_time: 0.222ms
2022-07-16 22:31:46 - until epoch: 099, best_acc1: 68.570%
2022-07-16 22:31:46 - epoch 100 lr: 0.007368
2022-07-16 22:32:24 - train: epoch 0100, iter [00100, 05004], lr: 0.007354, loss: 1.6008
2022-07-16 22:32:58 - train: epoch 0100, iter [00200, 05004], lr: 0.007341, loss: 1.3961
2022-07-16 22:33:31 - train: epoch 0100, iter [00300, 05004], lr: 0.007327, loss: 1.5553
2022-07-16 22:34:04 - train: epoch 0100, iter [00400, 05004], lr: 0.007313, loss: 1.1369
2022-07-16 22:34:37 - train: epoch 0100, iter [00500, 05004], lr: 0.007300, loss: 1.4903
2022-07-16 22:35:10 - train: epoch 0100, iter [00600, 05004], lr: 0.007286, loss: 1.5725
2022-07-16 22:35:44 - train: epoch 0100, iter [00700, 05004], lr: 0.007273, loss: 1.3378
2022-07-16 22:36:17 - train: epoch 0100, iter [00800, 05004], lr: 0.007259, loss: 1.5327
2022-07-16 22:36:49 - train: epoch 0100, iter [00900, 05004], lr: 0.007245, loss: 1.4490
2022-07-16 22:37:22 - train: epoch 0100, iter [01000, 05004], lr: 0.007232, loss: 1.4716
2022-07-16 22:37:56 - train: epoch 0100, iter [01100, 05004], lr: 0.007218, loss: 1.3017
2022-07-16 22:38:29 - train: epoch 0100, iter [01200, 05004], lr: 0.007205, loss: 1.4018
2022-07-16 22:39:02 - train: epoch 0100, iter [01300, 05004], lr: 0.007191, loss: 1.5578
2022-07-16 22:39:34 - train: epoch 0100, iter [01400, 05004], lr: 0.007178, loss: 1.4542
2022-07-16 22:40:08 - train: epoch 0100, iter [01500, 05004], lr: 0.007164, loss: 1.5861
2022-07-16 22:40:41 - train: epoch 0100, iter [01600, 05004], lr: 0.007151, loss: 1.3527
2022-07-16 22:41:14 - train: epoch 0100, iter [01700, 05004], lr: 0.007137, loss: 1.4663
2022-07-16 22:41:47 - train: epoch 0100, iter [01800, 05004], lr: 0.007124, loss: 1.4333
2022-07-16 22:42:21 - train: epoch 0100, iter [01900, 05004], lr: 0.007110, loss: 1.4625
2022-07-16 22:42:54 - train: epoch 0100, iter [02000, 05004], lr: 0.007097, loss: 1.6028
2022-07-16 22:43:27 - train: epoch 0100, iter [02100, 05004], lr: 0.007084, loss: 1.4588
2022-07-16 22:44:00 - train: epoch 0100, iter [02200, 05004], lr: 0.007070, loss: 1.6434
2022-07-16 22:44:34 - train: epoch 0100, iter [02300, 05004], lr: 0.007057, loss: 1.6885
2022-07-16 22:45:06 - train: epoch 0100, iter [02400, 05004], lr: 0.007043, loss: 1.5889
2022-07-16 22:45:39 - train: epoch 0100, iter [02500, 05004], lr: 0.007030, loss: 1.4200
2022-07-16 22:46:13 - train: epoch 0100, iter [02600, 05004], lr: 0.007017, loss: 1.4264
2022-07-16 22:46:47 - train: epoch 0100, iter [02700, 05004], lr: 0.007003, loss: 1.5239
2022-07-16 22:47:21 - train: epoch 0100, iter [02800, 05004], lr: 0.006990, loss: 1.4003
2022-07-16 22:47:56 - train: epoch 0100, iter [02900, 05004], lr: 0.006977, loss: 1.4040
2022-07-16 22:48:29 - train: epoch 0100, iter [03000, 05004], lr: 0.006963, loss: 1.2445
2022-07-16 22:49:04 - train: epoch 0100, iter [03100, 05004], lr: 0.006950, loss: 1.2187
2022-07-16 22:49:39 - train: epoch 0100, iter [03200, 05004], lr: 0.006937, loss: 1.7077
2022-07-16 22:50:13 - train: epoch 0100, iter [03300, 05004], lr: 0.006923, loss: 1.6223
2022-07-16 22:50:47 - train: epoch 0100, iter [03400, 05004], lr: 0.006910, loss: 1.6073
2022-07-16 22:51:21 - train: epoch 0100, iter [03500, 05004], lr: 0.006897, loss: 1.2620
2022-07-16 22:51:55 - train: epoch 0100, iter [03600, 05004], lr: 0.006884, loss: 1.3538
2022-07-16 22:52:29 - train: epoch 0100, iter [03700, 05004], lr: 0.006870, loss: 1.4187
2022-07-16 22:53:03 - train: epoch 0100, iter [03800, 05004], lr: 0.006857, loss: 1.3815
2022-07-16 22:53:38 - train: epoch 0100, iter [03900, 05004], lr: 0.006844, loss: 1.5644
2022-07-16 22:54:12 - train: epoch 0100, iter [04000, 05004], lr: 0.006831, loss: 1.5018
2022-07-16 22:54:46 - train: epoch 0100, iter [04100, 05004], lr: 0.006817, loss: 1.5237
2022-07-16 22:55:20 - train: epoch 0100, iter [04200, 05004], lr: 0.006804, loss: 1.6979
2022-07-16 22:55:56 - train: epoch 0100, iter [04300, 05004], lr: 0.006791, loss: 1.3796
2022-07-16 22:56:30 - train: epoch 0100, iter [04400, 05004], lr: 0.006778, loss: 1.6131
2022-07-16 22:57:03 - train: epoch 0100, iter [04500, 05004], lr: 0.006765, loss: 1.1737
2022-07-16 22:57:37 - train: epoch 0100, iter [04600, 05004], lr: 0.006752, loss: 1.4289
2022-07-16 22:58:12 - train: epoch 0100, iter [04700, 05004], lr: 0.006739, loss: 1.6680
2022-07-16 22:58:47 - train: epoch 0100, iter [04800, 05004], lr: 0.006725, loss: 1.2067
2022-07-16 22:59:20 - train: epoch 0100, iter [04900, 05004], lr: 0.006712, loss: 1.5091
2022-07-16 22:59:53 - train: epoch 0100, iter [05000, 05004], lr: 0.006699, loss: 1.6566
2022-07-16 22:59:54 - train: epoch 100, train_loss: 1.4333
2022-07-16 23:01:09 - eval: epoch: 100, acc1: 68.476%, acc5: 88.516%, test_loss: 1.2886, per_image_load_time: 1.271ms, per_image_inference_time: 0.229ms
2022-07-16 23:01:09 - until epoch: 100, best_acc1: 68.570%
2022-07-16 23:01:09 - epoch 101 lr: 0.006699
2022-07-16 23:01:48 - train: epoch 0101, iter [00100, 05004], lr: 0.006686, loss: 1.2339
2022-07-16 23:02:22 - train: epoch 0101, iter [00200, 05004], lr: 0.006673, loss: 1.4436
2022-07-16 23:02:56 - train: epoch 0101, iter [00300, 05004], lr: 0.006660, loss: 1.4471
2022-07-16 23:03:30 - train: epoch 0101, iter [00400, 05004], lr: 0.006647, loss: 1.5029
2022-07-16 23:04:03 - train: epoch 0101, iter [00500, 05004], lr: 0.006633, loss: 1.5039
2022-07-16 23:04:38 - train: epoch 0101, iter [00600, 05004], lr: 0.006620, loss: 1.2314
2022-07-16 23:05:11 - train: epoch 0101, iter [00700, 05004], lr: 0.006607, loss: 1.6708
2022-07-16 23:05:45 - train: epoch 0101, iter [00800, 05004], lr: 0.006594, loss: 1.4539
2022-07-16 23:06:18 - train: epoch 0101, iter [00900, 05004], lr: 0.006581, loss: 1.3973
2022-07-16 23:06:53 - train: epoch 0101, iter [01000, 05004], lr: 0.006569, loss: 1.6311
2022-07-16 23:07:26 - train: epoch 0101, iter [01100, 05004], lr: 0.006556, loss: 1.3931
2022-07-16 23:08:00 - train: epoch 0101, iter [01200, 05004], lr: 0.006543, loss: 1.5281
2022-07-16 23:08:34 - train: epoch 0101, iter [01300, 05004], lr: 0.006530, loss: 1.6356
2022-07-16 23:09:08 - train: epoch 0101, iter [01400, 05004], lr: 0.006517, loss: 1.2946
2022-07-16 23:09:42 - train: epoch 0101, iter [01500, 05004], lr: 0.006504, loss: 1.2183
2022-07-16 23:10:15 - train: epoch 0101, iter [01600, 05004], lr: 0.006491, loss: 1.2752
2022-07-16 23:10:50 - train: epoch 0101, iter [01700, 05004], lr: 0.006478, loss: 1.5419
2022-07-16 23:11:24 - train: epoch 0101, iter [01800, 05004], lr: 0.006465, loss: 1.4640
2022-07-16 23:11:58 - train: epoch 0101, iter [01900, 05004], lr: 0.006452, loss: 1.2898
2022-07-16 23:12:32 - train: epoch 0101, iter [02000, 05004], lr: 0.006440, loss: 1.3335
2022-07-16 23:13:05 - train: epoch 0101, iter [02100, 05004], lr: 0.006427, loss: 1.4379
2022-07-16 23:13:40 - train: epoch 0101, iter [02200, 05004], lr: 0.006414, loss: 1.2597
2022-07-16 23:14:13 - train: epoch 0101, iter [02300, 05004], lr: 0.006401, loss: 1.3688
2022-07-16 23:14:48 - train: epoch 0101, iter [02400, 05004], lr: 0.006388, loss: 1.4012
2022-07-16 23:15:22 - train: epoch 0101, iter [02500, 05004], lr: 0.006375, loss: 1.4850
2022-07-16 23:15:56 - train: epoch 0101, iter [02600, 05004], lr: 0.006363, loss: 1.4192
2022-07-16 23:16:31 - train: epoch 0101, iter [02700, 05004], lr: 0.006350, loss: 1.5538
2022-07-16 23:17:04 - train: epoch 0101, iter [02800, 05004], lr: 0.006337, loss: 1.4057
2022-07-16 23:17:38 - train: epoch 0101, iter [02900, 05004], lr: 0.006324, loss: 1.3435
2022-07-16 23:18:13 - train: epoch 0101, iter [03000, 05004], lr: 0.006312, loss: 1.4987
2022-07-16 23:18:46 - train: epoch 0101, iter [03100, 05004], lr: 0.006299, loss: 1.5720
2022-07-16 23:19:21 - train: epoch 0101, iter [03200, 05004], lr: 0.006286, loss: 1.4953
2022-07-16 23:19:55 - train: epoch 0101, iter [03300, 05004], lr: 0.006274, loss: 1.5463
2022-07-16 23:20:30 - train: epoch 0101, iter [03400, 05004], lr: 0.006261, loss: 1.3948
2022-07-16 23:21:02 - train: epoch 0101, iter [03500, 05004], lr: 0.006248, loss: 1.3648
2022-07-16 23:21:37 - train: epoch 0101, iter [03600, 05004], lr: 0.006236, loss: 1.3740
2022-07-16 23:22:12 - train: epoch 0101, iter [03700, 05004], lr: 0.006223, loss: 1.4399
2022-07-16 23:22:46 - train: epoch 0101, iter [03800, 05004], lr: 0.006210, loss: 1.5902
2022-07-16 23:23:21 - train: epoch 0101, iter [03900, 05004], lr: 0.006198, loss: 1.5019
2022-07-16 23:23:55 - train: epoch 0101, iter [04000, 05004], lr: 0.006185, loss: 1.4509
2022-07-16 23:24:29 - train: epoch 0101, iter [04100, 05004], lr: 0.006172, loss: 1.1593
2022-07-16 23:25:03 - train: epoch 0101, iter [04200, 05004], lr: 0.006160, loss: 1.4084
2022-07-16 23:25:38 - train: epoch 0101, iter [04300, 05004], lr: 0.006147, loss: 1.2662
2022-07-16 23:26:12 - train: epoch 0101, iter [04400, 05004], lr: 0.006135, loss: 1.4470
2022-07-16 23:26:46 - train: epoch 0101, iter [04500, 05004], lr: 0.006122, loss: 1.5275
2022-07-16 23:27:20 - train: epoch 0101, iter [04600, 05004], lr: 0.006110, loss: 1.4695
2022-07-16 23:27:54 - train: epoch 0101, iter [04700, 05004], lr: 0.006097, loss: 1.2734
2022-07-16 23:28:28 - train: epoch 0101, iter [04800, 05004], lr: 0.006085, loss: 1.2978
2022-07-16 23:29:03 - train: epoch 0101, iter [04900, 05004], lr: 0.006072, loss: 1.2687
2022-07-16 23:29:36 - train: epoch 0101, iter [05000, 05004], lr: 0.006060, loss: 1.4745
2022-07-16 23:29:37 - train: epoch 101, train_loss: 1.4179
2022-07-16 23:30:52 - eval: epoch: 101, acc1: 68.798%, acc5: 88.662%, test_loss: 1.2777, per_image_load_time: 2.693ms, per_image_inference_time: 0.217ms
2022-07-16 23:30:52 - until epoch: 101, best_acc1: 68.798%
2022-07-16 23:30:52 - epoch 102 lr: 0.006059
2022-07-16 23:31:31 - train: epoch 0102, iter [00100, 05004], lr: 0.006047, loss: 1.4470
2022-07-16 23:32:06 - train: epoch 0102, iter [00200, 05004], lr: 0.006034, loss: 1.5706
2022-07-16 23:32:39 - train: epoch 0102, iter [00300, 05004], lr: 0.006022, loss: 1.4462
2022-07-16 23:33:14 - train: epoch 0102, iter [00400, 05004], lr: 0.006009, loss: 1.4920
2022-07-16 23:33:48 - train: epoch 0102, iter [00500, 05004], lr: 0.005997, loss: 1.2004
2022-07-16 23:34:23 - train: epoch 0102, iter [00600, 05004], lr: 0.005984, loss: 1.3604
2022-07-16 23:34:56 - train: epoch 0102, iter [00700, 05004], lr: 0.005972, loss: 1.3133
2022-07-16 23:35:30 - train: epoch 0102, iter [00800, 05004], lr: 0.005960, loss: 1.3576
2022-07-16 23:36:05 - train: epoch 0102, iter [00900, 05004], lr: 0.005947, loss: 1.3734
2022-07-16 23:36:38 - train: epoch 0102, iter [01000, 05004], lr: 0.005935, loss: 1.5360
2022-07-16 23:37:13 - train: epoch 0102, iter [01100, 05004], lr: 0.005923, loss: 1.3132
2022-07-16 23:37:47 - train: epoch 0102, iter [01200, 05004], lr: 0.005910, loss: 1.4657
2022-07-16 23:38:20 - train: epoch 0102, iter [01300, 05004], lr: 0.005898, loss: 1.5138
2022-07-16 23:38:54 - train: epoch 0102, iter [01400, 05004], lr: 0.005886, loss: 1.4543
2022-07-16 23:39:28 - train: epoch 0102, iter [01500, 05004], lr: 0.005873, loss: 1.4965
2022-07-16 23:40:04 - train: epoch 0102, iter [01600, 05004], lr: 0.005861, loss: 1.4147
2022-07-16 23:40:37 - train: epoch 0102, iter [01700, 05004], lr: 0.005849, loss: 1.4393
2022-07-16 23:41:11 - train: epoch 0102, iter [01800, 05004], lr: 0.005836, loss: 1.3703
2022-07-16 23:41:45 - train: epoch 0102, iter [01900, 05004], lr: 0.005824, loss: 1.3792
2022-07-16 23:42:19 - train: epoch 0102, iter [02000, 05004], lr: 0.005812, loss: 1.4237
2022-07-16 23:42:53 - train: epoch 0102, iter [02100, 05004], lr: 0.005800, loss: 1.2509
2022-07-16 23:43:26 - train: epoch 0102, iter [02200, 05004], lr: 0.005787, loss: 1.5072
2022-07-16 23:44:01 - train: epoch 0102, iter [02300, 05004], lr: 0.005775, loss: 1.5581
2022-07-16 23:44:34 - train: epoch 0102, iter [02400, 05004], lr: 0.005763, loss: 1.3687
2022-07-16 23:45:08 - train: epoch 0102, iter [02500, 05004], lr: 0.005751, loss: 1.4202
2022-07-16 23:45:43 - train: epoch 0102, iter [02600, 05004], lr: 0.005739, loss: 1.4418
2022-07-16 23:46:16 - train: epoch 0102, iter [02700, 05004], lr: 0.005727, loss: 1.8539
2022-07-16 23:46:51 - train: epoch 0102, iter [02800, 05004], lr: 0.005714, loss: 1.4551
2022-07-16 23:47:24 - train: epoch 0102, iter [02900, 05004], lr: 0.005702, loss: 1.3907
2022-07-16 23:47:58 - train: epoch 0102, iter [03000, 05004], lr: 0.005690, loss: 1.4915
2022-07-16 23:48:31 - train: epoch 0102, iter [03100, 05004], lr: 0.005678, loss: 1.3063
2022-07-16 23:49:06 - train: epoch 0102, iter [03200, 05004], lr: 0.005666, loss: 1.1571
2022-07-16 23:49:40 - train: epoch 0102, iter [03300, 05004], lr: 0.005654, loss: 1.3695
2022-07-16 23:50:14 - train: epoch 0102, iter [03400, 05004], lr: 0.005642, loss: 1.4495
2022-07-16 23:50:47 - train: epoch 0102, iter [03500, 05004], lr: 0.005630, loss: 1.4316
2022-07-16 23:51:21 - train: epoch 0102, iter [03600, 05004], lr: 0.005618, loss: 1.2608
2022-07-16 23:51:55 - train: epoch 0102, iter [03700, 05004], lr: 0.005606, loss: 1.4175
2022-07-16 23:52:30 - train: epoch 0102, iter [03800, 05004], lr: 0.005594, loss: 1.5481
2022-07-16 23:53:04 - train: epoch 0102, iter [03900, 05004], lr: 0.005582, loss: 1.3704
2022-07-16 23:53:38 - train: epoch 0102, iter [04000, 05004], lr: 0.005570, loss: 1.1339
2022-07-16 23:54:11 - train: epoch 0102, iter [04100, 05004], lr: 0.005558, loss: 1.1290
2022-07-16 23:54:46 - train: epoch 0102, iter [04200, 05004], lr: 0.005546, loss: 1.6776
2022-07-16 23:55:19 - train: epoch 0102, iter [04300, 05004], lr: 0.005534, loss: 1.2101
2022-07-16 23:55:53 - train: epoch 0102, iter [04400, 05004], lr: 0.005522, loss: 1.3673
2022-07-16 23:56:27 - train: epoch 0102, iter [04500, 05004], lr: 0.005510, loss: 1.5698
2022-07-16 23:57:00 - train: epoch 0102, iter [04600, 05004], lr: 0.005498, loss: 1.5495
2022-07-16 23:57:35 - train: epoch 0102, iter [04700, 05004], lr: 0.005486, loss: 1.4548
2022-07-16 23:58:09 - train: epoch 0102, iter [04800, 05004], lr: 0.005474, loss: 1.5419
2022-07-16 23:58:43 - train: epoch 0102, iter [04900, 05004], lr: 0.005462, loss: 1.4518
2022-07-16 23:59:15 - train: epoch 0102, iter [05000, 05004], lr: 0.005450, loss: 1.2478
2022-07-16 23:59:16 - train: epoch 102, train_loss: 1.3989
2022-07-17 00:00:31 - eval: epoch: 102, acc1: 69.410%, acc5: 88.980%, test_loss: 1.2537, per_image_load_time: 1.521ms, per_image_inference_time: 0.206ms
2022-07-17 00:00:31 - until epoch: 102, best_acc1: 69.410%
2022-07-17 00:00:31 - epoch 103 lr: 0.005450
2022-07-17 00:01:10 - train: epoch 0103, iter [00100, 05004], lr: 0.005438, loss: 1.1867
2022-07-17 00:01:45 - train: epoch 0103, iter [00200, 05004], lr: 0.005426, loss: 1.6252
2022-07-17 00:02:17 - train: epoch 0103, iter [00300, 05004], lr: 0.005414, loss: 1.3733
2022-07-17 00:02:51 - train: epoch 0103, iter [00400, 05004], lr: 0.005402, loss: 1.3893
2022-07-17 00:03:25 - train: epoch 0103, iter [00500, 05004], lr: 0.005390, loss: 1.2344
2022-07-17 00:03:58 - train: epoch 0103, iter [00600, 05004], lr: 0.005379, loss: 1.3950
2022-07-17 00:04:32 - train: epoch 0103, iter [00700, 05004], lr: 0.005367, loss: 1.4884
2022-07-17 00:05:05 - train: epoch 0103, iter [00800, 05004], lr: 0.005355, loss: 1.2613
2022-07-17 00:05:39 - train: epoch 0103, iter [00900, 05004], lr: 0.005343, loss: 1.3475
2022-07-17 00:06:13 - train: epoch 0103, iter [01000, 05004], lr: 0.005332, loss: 1.4616
2022-07-17 00:06:47 - train: epoch 0103, iter [01100, 05004], lr: 0.005320, loss: 1.4788
2022-07-17 00:07:21 - train: epoch 0103, iter [01200, 05004], lr: 0.005308, loss: 1.3337
2022-07-17 00:07:55 - train: epoch 0103, iter [01300, 05004], lr: 0.005296, loss: 1.6323
2022-07-17 00:08:28 - train: epoch 0103, iter [01400, 05004], lr: 0.005285, loss: 1.4164
2022-07-17 00:09:02 - train: epoch 0103, iter [01500, 05004], lr: 0.005273, loss: 1.4531
2022-07-17 00:09:36 - train: epoch 0103, iter [01600, 05004], lr: 0.005261, loss: 0.9774
2022-07-17 00:10:10 - train: epoch 0103, iter [01700, 05004], lr: 0.005250, loss: 1.4227
2022-07-17 00:10:43 - train: epoch 0103, iter [01800, 05004], lr: 0.005238, loss: 1.4854
2022-07-17 00:11:17 - train: epoch 0103, iter [01900, 05004], lr: 0.005226, loss: 1.4143
2022-07-17 00:11:51 - train: epoch 0103, iter [02000, 05004], lr: 0.005215, loss: 1.0598
2022-07-17 00:12:24 - train: epoch 0103, iter [02100, 05004], lr: 0.005203, loss: 1.5679
2022-07-17 00:12:58 - train: epoch 0103, iter [02200, 05004], lr: 0.005191, loss: 1.4615
2022-07-17 00:13:31 - train: epoch 0103, iter [02300, 05004], lr: 0.005180, loss: 1.2784
2022-07-17 00:14:05 - train: epoch 0103, iter [02400, 05004], lr: 0.005168, loss: 1.3972
2022-07-17 00:14:39 - train: epoch 0103, iter [02500, 05004], lr: 0.005157, loss: 1.3489
2022-07-17 00:15:13 - train: epoch 0103, iter [02600, 05004], lr: 0.005145, loss: 1.3674
2022-07-17 00:15:46 - train: epoch 0103, iter [02700, 05004], lr: 0.005133, loss: 1.2749
2022-07-17 00:16:20 - train: epoch 0103, iter [02800, 05004], lr: 0.005122, loss: 1.0840
2022-07-17 00:16:54 - train: epoch 0103, iter [02900, 05004], lr: 0.005110, loss: 1.2952
2022-07-17 00:17:28 - train: epoch 0103, iter [03000, 05004], lr: 0.005099, loss: 1.6390
2022-07-17 00:18:02 - train: epoch 0103, iter [03100, 05004], lr: 0.005087, loss: 1.3359
2022-07-17 00:18:35 - train: epoch 0103, iter [03200, 05004], lr: 0.005076, loss: 1.3050
2022-07-17 00:19:09 - train: epoch 0103, iter [03300, 05004], lr: 0.005064, loss: 1.6217
2022-07-17 00:19:43 - train: epoch 0103, iter [03400, 05004], lr: 0.005053, loss: 1.3284
2022-07-17 00:20:16 - train: epoch 0103, iter [03500, 05004], lr: 0.005042, loss: 1.3513
2022-07-17 00:20:51 - train: epoch 0103, iter [03600, 05004], lr: 0.005030, loss: 1.3032
2022-07-17 00:21:24 - train: epoch 0103, iter [03700, 05004], lr: 0.005019, loss: 1.1466
2022-07-17 00:21:58 - train: epoch 0103, iter [03800, 05004], lr: 0.005007, loss: 1.2965
2022-07-17 00:22:32 - train: epoch 0103, iter [03900, 05004], lr: 0.004996, loss: 1.4668
2022-07-17 00:23:07 - train: epoch 0103, iter [04000, 05004], lr: 0.004984, loss: 1.5561
2022-07-17 00:23:40 - train: epoch 0103, iter [04100, 05004], lr: 0.004973, loss: 1.1205
2022-07-17 00:24:14 - train: epoch 0103, iter [04200, 05004], lr: 0.004962, loss: 1.3797
2022-07-17 00:24:47 - train: epoch 0103, iter [04300, 05004], lr: 0.004950, loss: 1.2866
2022-07-17 00:25:22 - train: epoch 0103, iter [04400, 05004], lr: 0.004939, loss: 1.3736
2022-07-17 00:25:55 - train: epoch 0103, iter [04500, 05004], lr: 0.004928, loss: 1.3930
2022-07-17 00:26:29 - train: epoch 0103, iter [04600, 05004], lr: 0.004916, loss: 1.4461
2022-07-17 00:27:02 - train: epoch 0103, iter [04700, 05004], lr: 0.004905, loss: 1.3102
2022-07-17 00:27:37 - train: epoch 0103, iter [04800, 05004], lr: 0.004894, loss: 1.6287
2022-07-17 00:28:10 - train: epoch 0103, iter [04900, 05004], lr: 0.004882, loss: 1.2445
2022-07-17 00:28:42 - train: epoch 0103, iter [05000, 05004], lr: 0.004871, loss: 1.4482
2022-07-17 00:28:43 - train: epoch 103, train_loss: 1.3825
2022-07-17 00:29:58 - eval: epoch: 103, acc1: 69.324%, acc5: 88.842%, test_loss: 1.2575, per_image_load_time: 1.137ms, per_image_inference_time: 0.215ms
2022-07-17 00:29:58 - until epoch: 103, best_acc1: 69.410%
2022-07-17 00:29:58 - epoch 104 lr: 0.004871
2022-07-17 00:30:37 - train: epoch 0104, iter [00100, 05004], lr: 0.004859, loss: 1.4670
2022-07-17 00:31:10 - train: epoch 0104, iter [00200, 05004], lr: 0.004848, loss: 1.2629
2022-07-17 00:31:45 - train: epoch 0104, iter [00300, 05004], lr: 0.004837, loss: 1.2887
2022-07-17 00:32:18 - train: epoch 0104, iter [00400, 05004], lr: 0.004826, loss: 1.5092
2022-07-17 00:32:51 - train: epoch 0104, iter [00500, 05004], lr: 0.004815, loss: 1.4257
2022-07-17 00:33:25 - train: epoch 0104, iter [00600, 05004], lr: 0.004803, loss: 1.5024
2022-07-17 00:33:59 - train: epoch 0104, iter [00700, 05004], lr: 0.004792, loss: 1.5581
2022-07-17 00:34:32 - train: epoch 0104, iter [00800, 05004], lr: 0.004781, loss: 1.3980
2022-07-17 00:35:07 - train: epoch 0104, iter [00900, 05004], lr: 0.004770, loss: 1.1472
2022-07-17 00:35:41 - train: epoch 0104, iter [01000, 05004], lr: 0.004759, loss: 1.5134
2022-07-17 00:36:14 - train: epoch 0104, iter [01100, 05004], lr: 0.004748, loss: 1.3014
2022-07-17 00:36:48 - train: epoch 0104, iter [01200, 05004], lr: 0.004736, loss: 1.2244
2022-07-17 00:37:23 - train: epoch 0104, iter [01300, 05004], lr: 0.004725, loss: 1.3974
2022-07-17 00:37:55 - train: epoch 0104, iter [01400, 05004], lr: 0.004714, loss: 1.3088
2022-07-17 00:38:30 - train: epoch 0104, iter [01500, 05004], lr: 0.004703, loss: 1.3674
2022-07-17 00:39:03 - train: epoch 0104, iter [01600, 05004], lr: 0.004692, loss: 1.3337
2022-07-17 00:39:37 - train: epoch 0104, iter [01700, 05004], lr: 0.004681, loss: 1.3508
2022-07-17 00:40:11 - train: epoch 0104, iter [01800, 05004], lr: 0.004670, loss: 1.3171
2022-07-17 00:40:45 - train: epoch 0104, iter [01900, 05004], lr: 0.004659, loss: 1.1650
2022-07-17 00:41:19 - train: epoch 0104, iter [02000, 05004], lr: 0.004648, loss: 1.3048
2022-07-17 00:41:53 - train: epoch 0104, iter [02100, 05004], lr: 0.004637, loss: 1.1694
2022-07-17 00:42:27 - train: epoch 0104, iter [02200, 05004], lr: 0.004626, loss: 1.3320
2022-07-17 00:43:01 - train: epoch 0104, iter [02300, 05004], lr: 0.004615, loss: 1.4079
2022-07-17 00:43:34 - train: epoch 0104, iter [02400, 05004], lr: 0.004604, loss: 1.5114
2022-07-17 00:44:08 - train: epoch 0104, iter [02500, 05004], lr: 0.004593, loss: 1.5270
2022-07-17 00:44:41 - train: epoch 0104, iter [02600, 05004], lr: 0.004582, loss: 1.2793
2022-07-17 00:45:16 - train: epoch 0104, iter [02700, 05004], lr: 0.004571, loss: 1.3698
2022-07-17 00:45:50 - train: epoch 0104, iter [02800, 05004], lr: 0.004560, loss: 1.4259
2022-07-17 00:46:25 - train: epoch 0104, iter [02900, 05004], lr: 0.004549, loss: 1.3323
2022-07-17 00:46:59 - train: epoch 0104, iter [03000, 05004], lr: 0.004538, loss: 1.3141
2022-07-17 00:47:33 - train: epoch 0104, iter [03100, 05004], lr: 0.004528, loss: 1.2899
2022-07-17 00:48:06 - train: epoch 0104, iter [03200, 05004], lr: 0.004517, loss: 1.2992
2022-07-17 00:48:40 - train: epoch 0104, iter [03300, 05004], lr: 0.004506, loss: 1.3432
2022-07-17 00:49:14 - train: epoch 0104, iter [03400, 05004], lr: 0.004495, loss: 1.2136
2022-07-17 00:49:48 - train: epoch 0104, iter [03500, 05004], lr: 0.004484, loss: 1.4555
2022-07-17 00:50:22 - train: epoch 0104, iter [03600, 05004], lr: 0.004473, loss: 1.3813
2022-07-17 00:50:55 - train: epoch 0104, iter [03700, 05004], lr: 0.004463, loss: 1.3871
2022-07-17 00:51:30 - train: epoch 0104, iter [03800, 05004], lr: 0.004452, loss: 1.2210
2022-07-17 00:52:04 - train: epoch 0104, iter [03900, 05004], lr: 0.004441, loss: 1.2565
2022-07-17 00:52:38 - train: epoch 0104, iter [04000, 05004], lr: 0.004430, loss: 1.5374
2022-07-17 00:53:11 - train: epoch 0104, iter [04100, 05004], lr: 0.004419, loss: 1.3832
2022-07-17 00:53:46 - train: epoch 0104, iter [04200, 05004], lr: 0.004409, loss: 1.5654
2022-07-17 00:54:19 - train: epoch 0104, iter [04300, 05004], lr: 0.004398, loss: 1.2715
2022-07-17 00:54:54 - train: epoch 0104, iter [04400, 05004], lr: 0.004387, loss: 1.4555
2022-07-17 00:55:28 - train: epoch 0104, iter [04500, 05004], lr: 0.004377, loss: 1.5291
2022-07-17 00:56:02 - train: epoch 0104, iter [04600, 05004], lr: 0.004366, loss: 1.3182
2022-07-17 00:56:35 - train: epoch 0104, iter [04700, 05004], lr: 0.004355, loss: 1.4574
2022-07-17 00:57:10 - train: epoch 0104, iter [04800, 05004], lr: 0.004344, loss: 1.3857
2022-07-17 00:57:43 - train: epoch 0104, iter [04900, 05004], lr: 0.004334, loss: 1.2365
2022-07-17 00:58:17 - train: epoch 0104, iter [05000, 05004], lr: 0.004323, loss: 1.2781
2022-07-17 00:58:17 - train: epoch 104, train_loss: 1.3627
2022-07-17 00:59:33 - eval: epoch: 104, acc1: 69.796%, acc5: 89.106%, test_loss: 1.2381, per_image_load_time: 1.580ms, per_image_inference_time: 0.225ms
2022-07-17 00:59:33 - until epoch: 104, best_acc1: 69.796%
2022-07-17 00:59:33 - epoch 105 lr: 0.004323
2022-07-17 01:00:12 - train: epoch 0105, iter [00100, 05004], lr: 0.004312, loss: 1.4188
2022-07-17 01:00:45 - train: epoch 0105, iter [00200, 05004], lr: 0.004301, loss: 1.6327
2022-07-17 01:01:19 - train: epoch 0105, iter [00300, 05004], lr: 0.004291, loss: 1.3026
2022-07-17 01:01:52 - train: epoch 0105, iter [00400, 05004], lr: 0.004280, loss: 1.4254
2022-07-17 01:02:26 - train: epoch 0105, iter [00500, 05004], lr: 0.004270, loss: 1.2868
2022-07-17 01:02:59 - train: epoch 0105, iter [00600, 05004], lr: 0.004259, loss: 1.4680
2022-07-17 01:03:33 - train: epoch 0105, iter [00700, 05004], lr: 0.004249, loss: 1.3714
2022-07-17 01:04:08 - train: epoch 0105, iter [00800, 05004], lr: 0.004238, loss: 1.5229
2022-07-17 01:04:41 - train: epoch 0105, iter [00900, 05004], lr: 0.004227, loss: 1.2449
2022-07-17 01:05:16 - train: epoch 0105, iter [01000, 05004], lr: 0.004217, loss: 1.2371
2022-07-17 01:05:49 - train: epoch 0105, iter [01100, 05004], lr: 0.004206, loss: 1.5674
2022-07-17 01:06:22 - train: epoch 0105, iter [01200, 05004], lr: 0.004196, loss: 1.4566
2022-07-17 01:06:57 - train: epoch 0105, iter [01300, 05004], lr: 0.004185, loss: 1.3063
2022-07-17 01:07:30 - train: epoch 0105, iter [01400, 05004], lr: 0.004175, loss: 1.3542
2022-07-17 01:08:05 - train: epoch 0105, iter [01500, 05004], lr: 0.004165, loss: 1.3755
2022-07-17 01:08:38 - train: epoch 0105, iter [01600, 05004], lr: 0.004154, loss: 1.1398
2022-07-17 01:09:12 - train: epoch 0105, iter [01700, 05004], lr: 0.004144, loss: 1.3023
2022-07-17 01:09:45 - train: epoch 0105, iter [01800, 05004], lr: 0.004133, loss: 1.3914
2022-07-17 01:10:20 - train: epoch 0105, iter [01900, 05004], lr: 0.004123, loss: 1.5033
2022-07-17 01:10:53 - train: epoch 0105, iter [02000, 05004], lr: 0.004112, loss: 1.3424
2022-07-17 01:11:27 - train: epoch 0105, iter [02100, 05004], lr: 0.004102, loss: 1.3571
2022-07-17 01:12:01 - train: epoch 0105, iter [02200, 05004], lr: 0.004092, loss: 1.5973
2022-07-17 01:12:34 - train: epoch 0105, iter [02300, 05004], lr: 0.004081, loss: 1.3663
2022-07-17 01:13:08 - train: epoch 0105, iter [02400, 05004], lr: 0.004071, loss: 1.1524
2022-07-17 01:13:42 - train: epoch 0105, iter [02500, 05004], lr: 0.004061, loss: 1.1651
2022-07-17 01:14:16 - train: epoch 0105, iter [02600, 05004], lr: 0.004050, loss: 1.3638
2022-07-17 01:14:50 - train: epoch 0105, iter [02700, 05004], lr: 0.004040, loss: 1.5372
2022-07-17 01:15:24 - train: epoch 0105, iter [02800, 05004], lr: 0.004030, loss: 1.4952
2022-07-17 01:15:58 - train: epoch 0105, iter [02900, 05004], lr: 0.004019, loss: 1.3761
2022-07-17 01:16:32 - train: epoch 0105, iter [03000, 05004], lr: 0.004009, loss: 1.3932
2022-07-17 01:17:06 - train: epoch 0105, iter [03100, 05004], lr: 0.003999, loss: 1.4933
2022-07-17 01:17:40 - train: epoch 0105, iter [03200, 05004], lr: 0.003989, loss: 1.3197
2022-07-17 01:18:15 - train: epoch 0105, iter [03300, 05004], lr: 0.003978, loss: 1.2250
2022-07-17 01:18:49 - train: epoch 0105, iter [03400, 05004], lr: 0.003968, loss: 1.2637
2022-07-17 01:19:23 - train: epoch 0105, iter [03500, 05004], lr: 0.003958, loss: 1.5043
2022-07-17 01:19:57 - train: epoch 0105, iter [03600, 05004], lr: 0.003948, loss: 1.3123
2022-07-17 01:20:31 - train: epoch 0105, iter [03700, 05004], lr: 0.003938, loss: 1.3291
2022-07-17 01:21:05 - train: epoch 0105, iter [03800, 05004], lr: 0.003927, loss: 1.3668
2022-07-17 01:21:39 - train: epoch 0105, iter [03900, 05004], lr: 0.003917, loss: 1.3872
2022-07-17 01:22:13 - train: epoch 0105, iter [04000, 05004], lr: 0.003907, loss: 1.2757
2022-07-17 01:22:48 - train: epoch 0105, iter [04100, 05004], lr: 0.003897, loss: 1.5291
2022-07-17 01:23:22 - train: epoch 0105, iter [04200, 05004], lr: 0.003887, loss: 1.2447
2022-07-17 01:23:57 - train: epoch 0105, iter [04300, 05004], lr: 0.003877, loss: 1.5685
2022-07-17 01:24:30 - train: epoch 0105, iter [04400, 05004], lr: 0.003867, loss: 1.3059
2022-07-17 01:25:03 - train: epoch 0105, iter [04500, 05004], lr: 0.003857, loss: 1.6007
2022-07-17 01:25:38 - train: epoch 0105, iter [04600, 05004], lr: 0.003847, loss: 1.3234
2022-07-17 01:26:12 - train: epoch 0105, iter [04700, 05004], lr: 0.003837, loss: 1.3148
2022-07-17 01:26:45 - train: epoch 0105, iter [04800, 05004], lr: 0.003826, loss: 1.1495
2022-07-17 01:27:20 - train: epoch 0105, iter [04900, 05004], lr: 0.003816, loss: 1.4442
2022-07-17 01:27:51 - train: epoch 0105, iter [05000, 05004], lr: 0.003806, loss: 1.3738
2022-07-17 01:27:52 - train: epoch 105, train_loss: 1.3448
2022-07-17 01:29:08 - eval: epoch: 105, acc1: 70.126%, acc5: 89.306%, test_loss: 1.2203, per_image_load_time: 1.126ms, per_image_inference_time: 0.217ms
2022-07-17 01:29:08 - until epoch: 105, best_acc1: 70.126%
2022-07-17 01:29:08 - epoch 106 lr: 0.003806
2022-07-17 01:29:47 - train: epoch 0106, iter [00100, 05004], lr: 0.003796, loss: 1.3240
2022-07-17 01:30:21 - train: epoch 0106, iter [00200, 05004], lr: 0.003786, loss: 1.3631
2022-07-17 01:30:56 - train: epoch 0106, iter [00300, 05004], lr: 0.003776, loss: 1.0678
2022-07-17 01:31:29 - train: epoch 0106, iter [00400, 05004], lr: 0.003766, loss: 1.5870
2022-07-17 01:32:04 - train: epoch 0106, iter [00500, 05004], lr: 0.003756, loss: 1.5784
2022-07-17 01:32:37 - train: epoch 0106, iter [00600, 05004], lr: 0.003746, loss: 1.3731
2022-07-17 01:33:11 - train: epoch 0106, iter [00700, 05004], lr: 0.003736, loss: 1.2671
2022-07-17 01:33:45 - train: epoch 0106, iter [00800, 05004], lr: 0.003726, loss: 1.1434
2022-07-17 01:34:19 - train: epoch 0106, iter [00900, 05004], lr: 0.003716, loss: 1.2796
2022-07-17 01:34:53 - train: epoch 0106, iter [01000, 05004], lr: 0.003707, loss: 1.5373
2022-07-17 01:35:27 - train: epoch 0106, iter [01100, 05004], lr: 0.003697, loss: 1.0014
2022-07-17 01:36:01 - train: epoch 0106, iter [01200, 05004], lr: 0.003687, loss: 1.0882
2022-07-17 01:36:35 - train: epoch 0106, iter [01300, 05004], lr: 0.003677, loss: 1.2041
2022-07-17 01:37:08 - train: epoch 0106, iter [01400, 05004], lr: 0.003667, loss: 1.2486
2022-07-17 01:37:42 - train: epoch 0106, iter [01500, 05004], lr: 0.003657, loss: 1.4346
2022-07-17 01:38:16 - train: epoch 0106, iter [01600, 05004], lr: 0.003647, loss: 1.4732
2022-07-17 01:38:51 - train: epoch 0106, iter [01700, 05004], lr: 0.003638, loss: 1.1999
2022-07-17 01:39:25 - train: epoch 0106, iter [01800, 05004], lr: 0.003628, loss: 1.1712
2022-07-17 01:39:59 - train: epoch 0106, iter [01900, 05004], lr: 0.003618, loss: 1.3410
2022-07-17 01:40:32 - train: epoch 0106, iter [02000, 05004], lr: 0.003608, loss: 1.0870
2022-07-17 01:41:06 - train: epoch 0106, iter [02100, 05004], lr: 0.003599, loss: 1.1197
2022-07-17 01:41:40 - train: epoch 0106, iter [02200, 05004], lr: 0.003589, loss: 1.4685
2022-07-17 01:42:14 - train: epoch 0106, iter [02300, 05004], lr: 0.003579, loss: 1.4732
2022-07-17 01:42:48 - train: epoch 0106, iter [02400, 05004], lr: 0.003569, loss: 1.3829
2022-07-17 01:43:22 - train: epoch 0106, iter [02500, 05004], lr: 0.003560, loss: 1.2454
2022-07-17 01:43:56 - train: epoch 0106, iter [02600, 05004], lr: 0.003550, loss: 1.1793
2022-07-17 01:44:30 - train: epoch 0106, iter [02700, 05004], lr: 0.003540, loss: 1.4576
2022-07-17 01:45:03 - train: epoch 0106, iter [02800, 05004], lr: 0.003531, loss: 1.4240
2022-07-17 01:45:38 - train: epoch 0106, iter [02900, 05004], lr: 0.003521, loss: 1.2252
2022-07-17 01:46:11 - train: epoch 0106, iter [03000, 05004], lr: 0.003511, loss: 1.4406
2022-07-17 01:46:45 - train: epoch 0106, iter [03100, 05004], lr: 0.003502, loss: 1.2409
2022-07-17 01:47:19 - train: epoch 0106, iter [03200, 05004], lr: 0.003492, loss: 1.2293
2022-07-17 01:47:53 - train: epoch 0106, iter [03300, 05004], lr: 0.003483, loss: 1.2144
2022-07-17 01:48:27 - train: epoch 0106, iter [03400, 05004], lr: 0.003473, loss: 1.1275
2022-07-17 01:49:02 - train: epoch 0106, iter [03500, 05004], lr: 0.003463, loss: 1.1735
2022-07-17 01:49:36 - train: epoch 0106, iter [03600, 05004], lr: 0.003454, loss: 1.0904
2022-07-17 01:50:10 - train: epoch 0106, iter [03700, 05004], lr: 0.003444, loss: 1.3093
2022-07-17 01:50:43 - train: epoch 0106, iter [03800, 05004], lr: 0.003435, loss: 1.4424
2022-07-17 01:51:18 - train: epoch 0106, iter [03900, 05004], lr: 0.003425, loss: 1.4377
2022-07-17 01:51:52 - train: epoch 0106, iter [04000, 05004], lr: 0.003416, loss: 1.4380
2022-07-17 01:52:26 - train: epoch 0106, iter [04100, 05004], lr: 0.003406, loss: 1.3951
2022-07-17 01:53:00 - train: epoch 0106, iter [04200, 05004], lr: 0.003397, loss: 1.3175
2022-07-17 01:53:35 - train: epoch 0106, iter [04300, 05004], lr: 0.003387, loss: 1.1318
2022-07-17 01:54:10 - train: epoch 0106, iter [04400, 05004], lr: 0.003378, loss: 1.5176
2022-07-17 01:54:44 - train: epoch 0106, iter [04500, 05004], lr: 0.003368, loss: 1.2523
2022-07-17 01:55:19 - train: epoch 0106, iter [04600, 05004], lr: 0.003359, loss: 1.4609
2022-07-17 01:55:52 - train: epoch 0106, iter [04700, 05004], lr: 0.003350, loss: 1.4526
2022-07-17 01:56:27 - train: epoch 0106, iter [04800, 05004], lr: 0.003340, loss: 1.4634
2022-07-17 01:57:00 - train: epoch 0106, iter [04900, 05004], lr: 0.003331, loss: 1.1887
2022-07-17 01:57:33 - train: epoch 0106, iter [05000, 05004], lr: 0.003321, loss: 1.3087
2022-07-17 01:57:34 - train: epoch 106, train_loss: 1.3282
2022-07-17 01:58:49 - eval: epoch: 106, acc1: 70.398%, acc5: 89.416%, test_loss: 1.2091, per_image_load_time: 1.313ms, per_image_inference_time: 0.226ms
2022-07-17 01:58:49 - until epoch: 106, best_acc1: 70.398%
2022-07-17 01:58:49 - epoch 107 lr: 0.003321
2022-07-17 01:59:29 - train: epoch 0107, iter [00100, 05004], lr: 0.003312, loss: 1.2516
2022-07-17 02:00:03 - train: epoch 0107, iter [00200, 05004], lr: 0.003302, loss: 1.2593
2022-07-17 02:00:37 - train: epoch 0107, iter [00300, 05004], lr: 0.003293, loss: 1.2767
2022-07-17 02:01:11 - train: epoch 0107, iter [00400, 05004], lr: 0.003284, loss: 1.3083
2022-07-17 02:01:45 - train: epoch 0107, iter [00500, 05004], lr: 0.003274, loss: 1.3573
2022-07-17 02:02:18 - train: epoch 0107, iter [00600, 05004], lr: 0.003265, loss: 1.2436
2022-07-17 02:02:53 - train: epoch 0107, iter [00700, 05004], lr: 0.003256, loss: 1.3846
2022-07-17 02:03:27 - train: epoch 0107, iter [00800, 05004], lr: 0.003246, loss: 1.2100
2022-07-17 02:04:01 - train: epoch 0107, iter [00900, 05004], lr: 0.003237, loss: 1.3278
2022-07-17 02:04:35 - train: epoch 0107, iter [01000, 05004], lr: 0.003228, loss: 1.2365
2022-07-17 02:05:09 - train: epoch 0107, iter [01100, 05004], lr: 0.003219, loss: 1.1566
2022-07-17 02:05:42 - train: epoch 0107, iter [01200, 05004], lr: 0.003209, loss: 1.2870
2022-07-17 02:06:16 - train: epoch 0107, iter [01300, 05004], lr: 0.003200, loss: 1.3663
2022-07-17 02:06:50 - train: epoch 0107, iter [01400, 05004], lr: 0.003191, loss: 1.3436
2022-07-17 02:07:24 - train: epoch 0107, iter [01500, 05004], lr: 0.003182, loss: 1.3428
2022-07-17 02:07:58 - train: epoch 0107, iter [01600, 05004], lr: 0.003173, loss: 1.2010
2022-07-17 02:08:32 - train: epoch 0107, iter [01700, 05004], lr: 0.003163, loss: 1.3585
2022-07-17 02:09:06 - train: epoch 0107, iter [01800, 05004], lr: 0.003154, loss: 1.2969
2022-07-17 02:09:40 - train: epoch 0107, iter [01900, 05004], lr: 0.003145, loss: 1.3425
2022-07-17 02:10:14 - train: epoch 0107, iter [02000, 05004], lr: 0.003136, loss: 1.2365
2022-07-17 02:10:49 - train: epoch 0107, iter [02100, 05004], lr: 0.003127, loss: 1.3177
2022-07-17 02:11:22 - train: epoch 0107, iter [02200, 05004], lr: 0.003118, loss: 1.6001
2022-07-17 02:11:56 - train: epoch 0107, iter [02300, 05004], lr: 0.003109, loss: 1.1993
2022-07-17 02:12:30 - train: epoch 0107, iter [02400, 05004], lr: 0.003100, loss: 1.5601
2022-07-17 02:13:05 - train: epoch 0107, iter [02500, 05004], lr: 0.003091, loss: 1.3270
2022-07-17 02:13:39 - train: epoch 0107, iter [02600, 05004], lr: 0.003082, loss: 1.4736
2022-07-17 02:14:13 - train: epoch 0107, iter [02700, 05004], lr: 0.003073, loss: 1.3719
2022-07-17 02:14:47 - train: epoch 0107, iter [02800, 05004], lr: 0.003064, loss: 1.4078
2022-07-17 02:15:20 - train: epoch 0107, iter [02900, 05004], lr: 0.003054, loss: 1.2280
2022-07-17 02:15:55 - train: epoch 0107, iter [03000, 05004], lr: 0.003046, loss: 1.2153
2022-07-17 02:16:29 - train: epoch 0107, iter [03100, 05004], lr: 0.003037, loss: 1.2841
2022-07-17 02:17:03 - train: epoch 0107, iter [03200, 05004], lr: 0.003028, loss: 1.2418
2022-07-17 02:17:36 - train: epoch 0107, iter [03300, 05004], lr: 0.003019, loss: 1.3621
2022-07-17 02:18:11 - train: epoch 0107, iter [03400, 05004], lr: 0.003010, loss: 1.0839
2022-07-17 02:18:44 - train: epoch 0107, iter [03500, 05004], lr: 0.003001, loss: 1.0927
2022-07-17 02:19:19 - train: epoch 0107, iter [03600, 05004], lr: 0.002992, loss: 1.5162
2022-07-17 02:19:51 - train: epoch 0107, iter [03700, 05004], lr: 0.002983, loss: 1.2616
2022-07-17 02:20:26 - train: epoch 0107, iter [03800, 05004], lr: 0.002974, loss: 1.2165
2022-07-17 02:20:59 - train: epoch 0107, iter [03900, 05004], lr: 0.002965, loss: 1.1202
2022-07-17 02:21:34 - train: epoch 0107, iter [04000, 05004], lr: 0.002956, loss: 1.3782
2022-07-17 02:22:07 - train: epoch 0107, iter [04100, 05004], lr: 0.002947, loss: 1.0871
2022-07-17 02:22:41 - train: epoch 0107, iter [04200, 05004], lr: 0.002939, loss: 1.3630
2022-07-17 02:23:16 - train: epoch 0107, iter [04300, 05004], lr: 0.002930, loss: 1.5997
2022-07-17 02:23:50 - train: epoch 0107, iter [04400, 05004], lr: 0.002921, loss: 1.3597
2022-07-17 02:24:24 - train: epoch 0107, iter [04500, 05004], lr: 0.002912, loss: 1.1923
2022-07-17 02:24:57 - train: epoch 0107, iter [04600, 05004], lr: 0.002903, loss: 1.1808
2022-07-17 02:25:31 - train: epoch 0107, iter [04700, 05004], lr: 0.002895, loss: 1.3638
2022-07-17 02:26:04 - train: epoch 0107, iter [04800, 05004], lr: 0.002886, loss: 1.1573
2022-07-17 02:26:39 - train: epoch 0107, iter [04900, 05004], lr: 0.002877, loss: 1.2773
2022-07-17 02:27:11 - train: epoch 0107, iter [05000, 05004], lr: 0.002868, loss: 1.2458
2022-07-17 02:27:12 - train: epoch 107, train_loss: 1.3105
2022-07-17 02:28:27 - eval: epoch: 107, acc1: 70.430%, acc5: 89.398%, test_loss: 1.2049, per_image_load_time: 1.022ms, per_image_inference_time: 0.219ms
2022-07-17 02:28:27 - until epoch: 107, best_acc1: 70.430%
2022-07-17 02:28:27 - epoch 108 lr: 0.002868
2022-07-17 02:29:06 - train: epoch 0108, iter [00100, 05004], lr: 0.002859, loss: 1.1429
2022-07-17 02:29:40 - train: epoch 0108, iter [00200, 05004], lr: 0.002850, loss: 1.3084
2022-07-17 02:30:14 - train: epoch 0108, iter [00300, 05004], lr: 0.002842, loss: 1.2782
2022-07-17 02:30:49 - train: epoch 0108, iter [00400, 05004], lr: 0.002833, loss: 1.2515
2022-07-17 02:31:22 - train: epoch 0108, iter [00500, 05004], lr: 0.002824, loss: 1.2512
2022-07-17 02:31:56 - train: epoch 0108, iter [00600, 05004], lr: 0.002816, loss: 1.2868
2022-07-17 02:32:30 - train: epoch 0108, iter [00700, 05004], lr: 0.002807, loss: 1.1805
2022-07-17 02:33:05 - train: epoch 0108, iter [00800, 05004], lr: 0.002798, loss: 1.3376
2022-07-17 02:33:38 - train: epoch 0108, iter [00900, 05004], lr: 0.002790, loss: 1.3527
2022-07-17 02:34:12 - train: epoch 0108, iter [01000, 05004], lr: 0.002781, loss: 1.3209
2022-07-17 02:34:46 - train: epoch 0108, iter [01100, 05004], lr: 0.002773, loss: 1.2576
2022-07-17 02:35:21 - train: epoch 0108, iter [01200, 05004], lr: 0.002764, loss: 1.2575
2022-07-17 02:35:54 - train: epoch 0108, iter [01300, 05004], lr: 0.002755, loss: 1.2252
2022-07-17 02:36:27 - train: epoch 0108, iter [01400, 05004], lr: 0.002747, loss: 1.1221
2022-07-17 02:37:03 - train: epoch 0108, iter [01500, 05004], lr: 0.002738, loss: 1.2762
2022-07-17 02:37:36 - train: epoch 0108, iter [01600, 05004], lr: 0.002730, loss: 1.3113
2022-07-17 02:38:11 - train: epoch 0108, iter [01700, 05004], lr: 0.002721, loss: 1.3331
2022-07-17 02:38:45 - train: epoch 0108, iter [01800, 05004], lr: 0.002713, loss: 1.1842
2022-07-17 02:39:19 - train: epoch 0108, iter [01900, 05004], lr: 0.002704, loss: 1.2568
2022-07-17 02:39:54 - train: epoch 0108, iter [02000, 05004], lr: 0.002696, loss: 1.3736
2022-07-17 02:40:28 - train: epoch 0108, iter [02100, 05004], lr: 0.002687, loss: 1.3968
2022-07-17 02:41:04 - train: epoch 0108, iter [02200, 05004], lr: 0.002679, loss: 1.2976
2022-07-17 02:41:37 - train: epoch 0108, iter [02300, 05004], lr: 0.002671, loss: 1.3187
2022-07-17 02:42:12 - train: epoch 0108, iter [02400, 05004], lr: 0.002662, loss: 1.3755
2022-07-17 02:42:46 - train: epoch 0108, iter [02500, 05004], lr: 0.002654, loss: 1.5392
2022-07-17 02:43:21 - train: epoch 0108, iter [02600, 05004], lr: 0.002645, loss: 1.2986
2022-07-17 02:43:54 - train: epoch 0108, iter [02700, 05004], lr: 0.002637, loss: 1.3280
2022-07-17 02:44:29 - train: epoch 0108, iter [02800, 05004], lr: 0.002628, loss: 1.3313
2022-07-17 02:45:03 - train: epoch 0108, iter [02900, 05004], lr: 0.002620, loss: 1.4706
2022-07-17 02:45:37 - train: epoch 0108, iter [03000, 05004], lr: 0.002612, loss: 1.2221
2022-07-17 02:46:12 - train: epoch 0108, iter [03100, 05004], lr: 0.002603, loss: 1.4180
2022-07-17 02:46:47 - train: epoch 0108, iter [03200, 05004], lr: 0.002595, loss: 1.1131
2022-07-17 02:47:21 - train: epoch 0108, iter [03300, 05004], lr: 0.002587, loss: 1.3780
2022-07-17 02:47:56 - train: epoch 0108, iter [03400, 05004], lr: 0.002579, loss: 1.3266
2022-07-17 02:48:31 - train: epoch 0108, iter [03500, 05004], lr: 0.002570, loss: 1.2359
2022-07-17 02:49:05 - train: epoch 0108, iter [03600, 05004], lr: 0.002562, loss: 1.2911
2022-07-17 02:49:40 - train: epoch 0108, iter [03700, 05004], lr: 0.002554, loss: 1.1863
2022-07-17 02:50:13 - train: epoch 0108, iter [03800, 05004], lr: 0.002545, loss: 1.2416
2022-07-17 02:50:48 - train: epoch 0108, iter [03900, 05004], lr: 0.002537, loss: 1.1952
2022-07-17 02:51:22 - train: epoch 0108, iter [04000, 05004], lr: 0.002529, loss: 1.3703
2022-07-17 02:51:57 - train: epoch 0108, iter [04100, 05004], lr: 0.002521, loss: 1.2746
2022-07-17 02:52:30 - train: epoch 0108, iter [04200, 05004], lr: 0.002513, loss: 1.1496
2022-07-17 02:53:05 - train: epoch 0108, iter [04300, 05004], lr: 0.002504, loss: 1.2174
2022-07-17 02:53:40 - train: epoch 0108, iter [04400, 05004], lr: 0.002496, loss: 1.2585
2022-07-17 02:54:15 - train: epoch 0108, iter [04500, 05004], lr: 0.002488, loss: 1.2926
2022-07-17 02:54:48 - train: epoch 0108, iter [04600, 05004], lr: 0.002480, loss: 1.2715
2022-07-17 02:55:23 - train: epoch 0108, iter [04700, 05004], lr: 0.002472, loss: 1.2836
2022-07-17 02:55:57 - train: epoch 0108, iter [04800, 05004], lr: 0.002464, loss: 1.1961
2022-07-17 02:56:32 - train: epoch 0108, iter [04900, 05004], lr: 0.002456, loss: 1.3128
2022-07-17 02:57:04 - train: epoch 0108, iter [05000, 05004], lr: 0.002447, loss: 1.1222
2022-07-17 02:57:05 - train: epoch 108, train_loss: 1.2950
2022-07-17 02:58:20 - eval: epoch: 108, acc1: 70.824%, acc5: 89.672%, test_loss: 1.1911, per_image_load_time: 1.523ms, per_image_inference_time: 0.252ms
2022-07-17 02:58:20 - until epoch: 108, best_acc1: 70.824%
2022-07-17 02:58:20 - epoch 109 lr: 0.002447
2022-07-17 02:59:00 - train: epoch 0109, iter [00100, 05004], lr: 0.002439, loss: 1.3264
2022-07-17 02:59:34 - train: epoch 0109, iter [00200, 05004], lr: 0.002431, loss: 1.2318
2022-07-17 03:00:08 - train: epoch 0109, iter [00300, 05004], lr: 0.002423, loss: 1.3493
2022-07-17 03:00:41 - train: epoch 0109, iter [00400, 05004], lr: 0.002415, loss: 1.2699
2022-07-17 03:01:15 - train: epoch 0109, iter [00500, 05004], lr: 0.002407, loss: 1.2931
2022-07-17 03:01:49 - train: epoch 0109, iter [00600, 05004], lr: 0.002399, loss: 1.1147
2022-07-17 03:02:23 - train: epoch 0109, iter [00700, 05004], lr: 0.002391, loss: 1.4779
2022-07-17 03:02:57 - train: epoch 0109, iter [00800, 05004], lr: 0.002383, loss: 1.2582
2022-07-17 03:03:31 - train: epoch 0109, iter [00900, 05004], lr: 0.002375, loss: 1.2437
2022-07-17 03:04:06 - train: epoch 0109, iter [01000, 05004], lr: 0.002367, loss: 1.0612
2022-07-17 03:04:39 - train: epoch 0109, iter [01100, 05004], lr: 0.002359, loss: 1.2073
2022-07-17 03:05:15 - train: epoch 0109, iter [01200, 05004], lr: 0.002351, loss: 1.3468
2022-07-17 03:05:48 - train: epoch 0109, iter [01300, 05004], lr: 0.002343, loss: 1.1259
2022-07-17 03:06:23 - train: epoch 0109, iter [01400, 05004], lr: 0.002335, loss: 1.3310
2022-07-17 03:06:57 - train: epoch 0109, iter [01500, 05004], lr: 0.002327, loss: 1.1981
2022-07-17 03:07:31 - train: epoch 0109, iter [01600, 05004], lr: 0.002320, loss: 1.0937
2022-07-17 03:08:06 - train: epoch 0109, iter [01700, 05004], lr: 0.002312, loss: 1.3385
2022-07-17 03:08:40 - train: epoch 0109, iter [01800, 05004], lr: 0.002304, loss: 1.3762
2022-07-17 03:09:14 - train: epoch 0109, iter [01900, 05004], lr: 0.002296, loss: 1.4281
2022-07-17 03:09:49 - train: epoch 0109, iter [02000, 05004], lr: 0.002288, loss: 1.4663
2022-07-17 03:10:23 - train: epoch 0109, iter [02100, 05004], lr: 0.002280, loss: 1.3482
2022-07-17 03:10:58 - train: epoch 0109, iter [02200, 05004], lr: 0.002272, loss: 1.1971
2022-07-17 03:11:31 - train: epoch 0109, iter [02300, 05004], lr: 0.002265, loss: 1.3184
2022-07-17 03:12:06 - train: epoch 0109, iter [02400, 05004], lr: 0.002257, loss: 1.2165
2022-07-17 03:12:40 - train: epoch 0109, iter [02500, 05004], lr: 0.002249, loss: 1.2787
2022-07-17 03:13:14 - train: epoch 0109, iter [02600, 05004], lr: 0.002241, loss: 1.4078
2022-07-17 03:13:48 - train: epoch 0109, iter [02700, 05004], lr: 0.002234, loss: 1.3407
2022-07-17 03:14:23 - train: epoch 0109, iter [02800, 05004], lr: 0.002226, loss: 1.2757
2022-07-17 03:14:57 - train: epoch 0109, iter [02900, 05004], lr: 0.002218, loss: 1.1202
2022-07-17 03:15:32 - train: epoch 0109, iter [03000, 05004], lr: 0.002211, loss: 1.3265
2022-07-17 03:16:07 - train: epoch 0109, iter [03100, 05004], lr: 0.002203, loss: 1.3472
2022-07-17 03:16:40 - train: epoch 0109, iter [03200, 05004], lr: 0.002195, loss: 1.2506
2022-07-17 03:17:14 - train: epoch 0109, iter [03300, 05004], lr: 0.002188, loss: 1.2640
2022-07-17 03:17:49 - train: epoch 0109, iter [03400, 05004], lr: 0.002180, loss: 1.1995
2022-07-17 03:18:23 - train: epoch 0109, iter [03500, 05004], lr: 0.002172, loss: 1.3296
2022-07-17 03:18:57 - train: epoch 0109, iter [03600, 05004], lr: 0.002165, loss: 1.1281
2022-07-17 03:19:32 - train: epoch 0109, iter [03700, 05004], lr: 0.002157, loss: 1.0905
2022-07-17 03:20:06 - train: epoch 0109, iter [03800, 05004], lr: 0.002149, loss: 1.4446
2022-07-17 03:20:40 - train: epoch 0109, iter [03900, 05004], lr: 0.002142, loss: 1.1238
2022-07-17 03:21:15 - train: epoch 0109, iter [04000, 05004], lr: 0.002134, loss: 1.4451
2022-07-17 03:21:49 - train: epoch 0109, iter [04100, 05004], lr: 0.002127, loss: 1.3279
2022-07-17 03:22:23 - train: epoch 0109, iter [04200, 05004], lr: 0.002119, loss: 1.1118
2022-07-17 03:22:58 - train: epoch 0109, iter [04300, 05004], lr: 0.002112, loss: 1.2989
2022-07-17 03:23:32 - train: epoch 0109, iter [04400, 05004], lr: 0.002104, loss: 1.1914
2022-07-17 03:24:06 - train: epoch 0109, iter [04500, 05004], lr: 0.002097, loss: 1.1168
2022-07-17 03:24:41 - train: epoch 0109, iter [04600, 05004], lr: 0.002089, loss: 1.2351
2022-07-17 03:25:15 - train: epoch 0109, iter [04700, 05004], lr: 0.002082, loss: 1.2686
2022-07-17 03:25:49 - train: epoch 0109, iter [04800, 05004], lr: 0.002074, loss: 1.3652
2022-07-17 03:26:23 - train: epoch 0109, iter [04900, 05004], lr: 0.002067, loss: 1.3707
2022-07-17 03:26:55 - train: epoch 0109, iter [05000, 05004], lr: 0.002059, loss: 1.1265
2022-07-17 03:26:56 - train: epoch 109, train_loss: 1.2794
2022-07-17 03:28:12 - eval: epoch: 109, acc1: 71.090%, acc5: 89.900%, test_loss: 1.1793, per_image_load_time: 1.221ms, per_image_inference_time: 0.237ms
2022-07-17 03:28:12 - until epoch: 109, best_acc1: 71.090%
2022-07-17 03:28:12 - epoch 110 lr: 0.002059
2022-07-17 03:28:51 - train: epoch 0110, iter [00100, 05004], lr: 0.002052, loss: 1.1536
2022-07-17 03:29:25 - train: epoch 0110, iter [00200, 05004], lr: 0.002044, loss: 1.3298
2022-07-17 03:29:59 - train: epoch 0110, iter [00300, 05004], lr: 0.002037, loss: 1.4682
2022-07-17 03:30:33 - train: epoch 0110, iter [00400, 05004], lr: 0.002029, loss: 1.3467
2022-07-17 03:31:07 - train: epoch 0110, iter [00500, 05004], lr: 0.002022, loss: 1.1593
2022-07-17 03:31:42 - train: epoch 0110, iter [00600, 05004], lr: 0.002015, loss: 1.2711
2022-07-17 03:32:15 - train: epoch 0110, iter [00700, 05004], lr: 0.002007, loss: 1.4207
2022-07-17 03:32:49 - train: epoch 0110, iter [00800, 05004], lr: 0.002000, loss: 1.5881
2022-07-17 03:33:23 - train: epoch 0110, iter [00900, 05004], lr: 0.001993, loss: 1.3739
2022-07-17 03:33:58 - train: epoch 0110, iter [01000, 05004], lr: 0.001985, loss: 1.3622
2022-07-17 03:34:31 - train: epoch 0110, iter [01100, 05004], lr: 0.001978, loss: 1.5214
2022-07-17 03:35:06 - train: epoch 0110, iter [01200, 05004], lr: 0.001971, loss: 1.1487
2022-07-17 03:35:39 - train: epoch 0110, iter [01300, 05004], lr: 0.001964, loss: 1.2365
2022-07-17 03:36:14 - train: epoch 0110, iter [01400, 05004], lr: 0.001956, loss: 1.2005
2022-07-17 03:36:47 - train: epoch 0110, iter [01500, 05004], lr: 0.001949, loss: 1.1863
2022-07-17 03:37:22 - train: epoch 0110, iter [01600, 05004], lr: 0.001942, loss: 1.1780
2022-07-17 03:37:56 - train: epoch 0110, iter [01700, 05004], lr: 0.001935, loss: 1.2354
2022-07-17 03:38:31 - train: epoch 0110, iter [01800, 05004], lr: 0.001927, loss: 1.4279
2022-07-17 03:39:05 - train: epoch 0110, iter [01900, 05004], lr: 0.001920, loss: 1.3700
2022-07-17 03:39:39 - train: epoch 0110, iter [02000, 05004], lr: 0.001913, loss: 1.1835
2022-07-17 03:40:13 - train: epoch 0110, iter [02100, 05004], lr: 0.001906, loss: 1.3371
2022-07-17 03:40:47 - train: epoch 0110, iter [02200, 05004], lr: 0.001899, loss: 1.3377
2022-07-17 03:41:20 - train: epoch 0110, iter [02300, 05004], lr: 0.001892, loss: 1.0843
2022-07-17 03:41:55 - train: epoch 0110, iter [02400, 05004], lr: 0.001884, loss: 1.2590
2022-07-17 03:42:29 - train: epoch 0110, iter [02500, 05004], lr: 0.001877, loss: 1.2165
2022-07-17 03:43:03 - train: epoch 0110, iter [02600, 05004], lr: 0.001870, loss: 1.5554
2022-07-17 03:43:38 - train: epoch 0110, iter [02700, 05004], lr: 0.001863, loss: 1.1816
2022-07-17 03:44:12 - train: epoch 0110, iter [02800, 05004], lr: 0.001856, loss: 0.9930
2022-07-17 03:44:47 - train: epoch 0110, iter [02900, 05004], lr: 0.001849, loss: 1.3587
2022-07-17 03:45:21 - train: epoch 0110, iter [03000, 05004], lr: 0.001842, loss: 1.1616
2022-07-17 03:45:55 - train: epoch 0110, iter [03100, 05004], lr: 0.001835, loss: 1.2616
2022-07-17 03:46:29 - train: epoch 0110, iter [03200, 05004], lr: 0.001828, loss: 1.2766
2022-07-17 03:47:03 - train: epoch 0110, iter [03300, 05004], lr: 0.001821, loss: 1.4139
2022-07-17 03:47:37 - train: epoch 0110, iter [03400, 05004], lr: 0.001814, loss: 1.2947
2022-07-17 03:48:11 - train: epoch 0110, iter [03500, 05004], lr: 0.001807, loss: 1.4519
2022-07-17 03:48:45 - train: epoch 0110, iter [03600, 05004], lr: 0.001800, loss: 1.3986
2022-07-17 03:49:20 - train: epoch 0110, iter [03700, 05004], lr: 0.001793, loss: 1.2799
2022-07-17 03:49:54 - train: epoch 0110, iter [03800, 05004], lr: 0.001786, loss: 1.0877
2022-07-17 03:50:27 - train: epoch 0110, iter [03900, 05004], lr: 0.001779, loss: 1.0865
2022-07-17 03:51:01 - train: epoch 0110, iter [04000, 05004], lr: 0.001772, loss: 1.3168
2022-07-17 03:51:35 - train: epoch 0110, iter [04100, 05004], lr: 0.001765, loss: 1.5117
2022-07-17 03:52:09 - train: epoch 0110, iter [04200, 05004], lr: 0.001759, loss: 1.2592
2022-07-17 03:52:43 - train: epoch 0110, iter [04300, 05004], lr: 0.001752, loss: 1.0317
2022-07-17 03:53:17 - train: epoch 0110, iter [04400, 05004], lr: 0.001745, loss: 1.4849
2022-07-17 03:53:51 - train: epoch 0110, iter [04500, 05004], lr: 0.001738, loss: 1.1179
2022-07-17 03:54:25 - train: epoch 0110, iter [04600, 05004], lr: 0.001731, loss: 1.2627
2022-07-17 03:55:00 - train: epoch 0110, iter [04700, 05004], lr: 0.001724, loss: 1.3505
2022-07-17 03:55:34 - train: epoch 0110, iter [04800, 05004], lr: 0.001718, loss: 1.4056
2022-07-17 03:56:07 - train: epoch 0110, iter [04900, 05004], lr: 0.001711, loss: 1.2471
2022-07-17 03:56:40 - train: epoch 0110, iter [05000, 05004], lr: 0.001704, loss: 1.3536
2022-07-17 03:56:41 - train: epoch 110, train_loss: 1.2630
2022-07-17 03:57:57 - eval: epoch: 110, acc1: 71.350%, acc5: 89.842%, test_loss: 1.1725, per_image_load_time: 2.697ms, per_image_inference_time: 0.214ms
2022-07-17 03:57:57 - until epoch: 110, best_acc1: 71.350%
2022-07-17 03:57:57 - epoch 111 lr: 0.001704
2022-07-17 03:58:36 - train: epoch 0111, iter [00100, 05004], lr: 0.001697, loss: 1.2218
2022-07-17 03:59:11 - train: epoch 0111, iter [00200, 05004], lr: 0.001690, loss: 1.2984
2022-07-17 03:59:44 - train: epoch 0111, iter [00300, 05004], lr: 0.001683, loss: 1.3010
2022-07-17 04:00:17 - train: epoch 0111, iter [00400, 05004], lr: 0.001677, loss: 1.2756
2022-07-17 04:00:52 - train: epoch 0111, iter [00500, 05004], lr: 0.001670, loss: 1.0794
2022-07-17 04:01:26 - train: epoch 0111, iter [00600, 05004], lr: 0.001663, loss: 1.3422
2022-07-17 04:01:58 - train: epoch 0111, iter [00700, 05004], lr: 0.001657, loss: 1.2558
2022-07-17 04:02:33 - train: epoch 0111, iter [00800, 05004], lr: 0.001650, loss: 1.1998
2022-07-17 04:03:07 - train: epoch 0111, iter [00900, 05004], lr: 0.001643, loss: 1.2938
2022-07-17 04:03:41 - train: epoch 0111, iter [01000, 05004], lr: 0.001637, loss: 1.1297
2022-07-17 04:04:15 - train: epoch 0111, iter [01100, 05004], lr: 0.001630, loss: 1.1183
2022-07-17 04:04:50 - train: epoch 0111, iter [01200, 05004], lr: 0.001623, loss: 1.2445
2022-07-17 04:05:23 - train: epoch 0111, iter [01300, 05004], lr: 0.001617, loss: 1.3083
2022-07-17 04:05:56 - train: epoch 0111, iter [01400, 05004], lr: 0.001610, loss: 1.2904
2022-07-17 04:06:30 - train: epoch 0111, iter [01500, 05004], lr: 0.001604, loss: 1.3380
2022-07-17 04:07:05 - train: epoch 0111, iter [01600, 05004], lr: 0.001597, loss: 1.2743
2022-07-17 04:07:39 - train: epoch 0111, iter [01700, 05004], lr: 0.001591, loss: 1.2806
2022-07-17 04:08:13 - train: epoch 0111, iter [01800, 05004], lr: 0.001584, loss: 1.1251
2022-07-17 04:08:47 - train: epoch 0111, iter [01900, 05004], lr: 0.001577, loss: 1.1077
2022-07-17 04:09:21 - train: epoch 0111, iter [02000, 05004], lr: 0.001571, loss: 1.3731
2022-07-17 04:09:55 - train: epoch 0111, iter [02100, 05004], lr: 0.001564, loss: 1.5404
2022-07-17 04:10:29 - train: epoch 0111, iter [02200, 05004], lr: 0.001558, loss: 1.3195
2022-07-17 04:11:03 - train: epoch 0111, iter [02300, 05004], lr: 0.001551, loss: 1.2505
2022-07-17 04:11:37 - train: epoch 0111, iter [02400, 05004], lr: 0.001545, loss: 1.1864
2022-07-17 04:12:12 - train: epoch 0111, iter [02500, 05004], lr: 0.001539, loss: 1.2235
2022-07-17 04:12:46 - train: epoch 0111, iter [02600, 05004], lr: 0.001532, loss: 1.3507
2022-07-17 04:13:21 - train: epoch 0111, iter [02700, 05004], lr: 0.001526, loss: 1.3027
2022-07-17 04:13:55 - train: epoch 0111, iter [02800, 05004], lr: 0.001519, loss: 1.0820
2022-07-17 04:14:29 - train: epoch 0111, iter [02900, 05004], lr: 0.001513, loss: 1.1718
2022-07-17 04:15:03 - train: epoch 0111, iter [03000, 05004], lr: 0.001507, loss: 1.3416
2022-07-17 04:15:38 - train: epoch 0111, iter [03100, 05004], lr: 0.001500, loss: 1.4122
2022-07-17 04:16:11 - train: epoch 0111, iter [03200, 05004], lr: 0.001494, loss: 1.2715
2022-07-17 04:16:46 - train: epoch 0111, iter [03300, 05004], lr: 0.001487, loss: 1.4319
2022-07-17 04:17:20 - train: epoch 0111, iter [03400, 05004], lr: 0.001481, loss: 1.4343
2022-07-17 04:17:55 - train: epoch 0111, iter [03500, 05004], lr: 0.001475, loss: 1.3348
2022-07-17 04:18:30 - train: epoch 0111, iter [03600, 05004], lr: 0.001469, loss: 1.1405
2022-07-17 04:19:03 - train: epoch 0111, iter [03700, 05004], lr: 0.001462, loss: 1.2164
2022-07-17 04:19:38 - train: epoch 0111, iter [03800, 05004], lr: 0.001456, loss: 1.3756
2022-07-17 04:20:12 - train: epoch 0111, iter [03900, 05004], lr: 0.001450, loss: 1.1402
2022-07-17 04:20:46 - train: epoch 0111, iter [04000, 05004], lr: 0.001443, loss: 1.2317
2022-07-17 04:21:20 - train: epoch 0111, iter [04100, 05004], lr: 0.001437, loss: 1.3602
2022-07-17 04:21:55 - train: epoch 0111, iter [04200, 05004], lr: 0.001431, loss: 1.2569
2022-07-17 04:22:28 - train: epoch 0111, iter [04300, 05004], lr: 0.001425, loss: 1.3685
2022-07-17 04:23:03 - train: epoch 0111, iter [04400, 05004], lr: 0.001419, loss: 1.1400
2022-07-17 04:23:37 - train: epoch 0111, iter [04500, 05004], lr: 0.001412, loss: 1.2867
2022-07-17 04:24:10 - train: epoch 0111, iter [04600, 05004], lr: 0.001406, loss: 1.1875
2022-07-17 04:24:45 - train: epoch 0111, iter [04700, 05004], lr: 0.001400, loss: 1.2861
2022-07-17 04:25:19 - train: epoch 0111, iter [04800, 05004], lr: 0.001394, loss: 1.1769
2022-07-17 04:25:53 - train: epoch 0111, iter [04900, 05004], lr: 0.001388, loss: 1.3287
2022-07-17 04:26:26 - train: epoch 0111, iter [05000, 05004], lr: 0.001382, loss: 1.2002
2022-07-17 04:26:26 - train: epoch 111, train_loss: 1.2507
2022-07-17 04:27:42 - eval: epoch: 111, acc1: 71.404%, acc5: 89.958%, test_loss: 1.1636, per_image_load_time: 2.441ms, per_image_inference_time: 0.236ms
2022-07-17 04:27:42 - until epoch: 111, best_acc1: 71.404%
2022-07-17 04:27:42 - epoch 112 lr: 0.001381
2022-07-17 04:28:22 - train: epoch 0112, iter [00100, 05004], lr: 0.001375, loss: 1.1220
2022-07-17 04:28:55 - train: epoch 0112, iter [00200, 05004], lr: 0.001369, loss: 1.1949
2022-07-17 04:29:30 - train: epoch 0112, iter [00300, 05004], lr: 0.001363, loss: 1.2059
2022-07-17 04:30:04 - train: epoch 0112, iter [00400, 05004], lr: 0.001357, loss: 1.2516
2022-07-17 04:30:37 - train: epoch 0112, iter [00500, 05004], lr: 0.001351, loss: 1.4599
2022-07-17 04:31:12 - train: epoch 0112, iter [00600, 05004], lr: 0.001345, loss: 1.3158
2022-07-17 04:31:45 - train: epoch 0112, iter [00700, 05004], lr: 0.001339, loss: 1.3855
2022-07-17 04:32:20 - train: epoch 0112, iter [00800, 05004], lr: 0.001333, loss: 1.0159
2022-07-17 04:32:53 - train: epoch 0112, iter [00900, 05004], lr: 0.001327, loss: 1.0687
2022-07-17 04:33:27 - train: epoch 0112, iter [01000, 05004], lr: 0.001321, loss: 1.0554
2022-07-17 04:34:01 - train: epoch 0112, iter [01100, 05004], lr: 0.001315, loss: 1.2440
2022-07-17 04:34:37 - train: epoch 0112, iter [01200, 05004], lr: 0.001309, loss: 1.1343
2022-07-17 04:35:09 - train: epoch 0112, iter [01300, 05004], lr: 0.001303, loss: 1.5123
2022-07-17 04:35:44 - train: epoch 0112, iter [01400, 05004], lr: 0.001297, loss: 1.1851
2022-07-17 04:36:18 - train: epoch 0112, iter [01500, 05004], lr: 0.001291, loss: 1.3977
2022-07-17 04:36:52 - train: epoch 0112, iter [01600, 05004], lr: 0.001286, loss: 1.2882
2022-07-17 04:37:27 - train: epoch 0112, iter [01700, 05004], lr: 0.001280, loss: 0.9609
2022-07-17 04:38:00 - train: epoch 0112, iter [01800, 05004], lr: 0.001274, loss: 1.0809
2022-07-17 04:38:34 - train: epoch 0112, iter [01900, 05004], lr: 0.001268, loss: 0.9071
2022-07-17 04:39:08 - train: epoch 0112, iter [02000, 05004], lr: 0.001262, loss: 1.3010
2022-07-17 04:39:43 - train: epoch 0112, iter [02100, 05004], lr: 0.001256, loss: 1.4627
2022-07-17 04:40:16 - train: epoch 0112, iter [02200, 05004], lr: 0.001250, loss: 1.0749
2022-07-17 04:40:51 - train: epoch 0112, iter [02300, 05004], lr: 0.001245, loss: 1.2312
2022-07-17 04:41:24 - train: epoch 0112, iter [02400, 05004], lr: 0.001239, loss: 1.0232
2022-07-17 04:41:59 - train: epoch 0112, iter [02500, 05004], lr: 0.001233, loss: 1.2923
2022-07-17 04:42:32 - train: epoch 0112, iter [02600, 05004], lr: 0.001227, loss: 1.3040
2022-07-17 04:43:06 - train: epoch 0112, iter [02700, 05004], lr: 0.001221, loss: 1.3822
2022-07-17 04:43:40 - train: epoch 0112, iter [02800, 05004], lr: 0.001216, loss: 1.3885
2022-07-17 04:44:15 - train: epoch 0112, iter [02900, 05004], lr: 0.001210, loss: 1.2775
2022-07-17 04:44:50 - train: epoch 0112, iter [03000, 05004], lr: 0.001204, loss: 1.2301
2022-07-17 04:45:23 - train: epoch 0112, iter [03100, 05004], lr: 0.001199, loss: 1.2227
2022-07-17 04:45:58 - train: epoch 0112, iter [03200, 05004], lr: 0.001193, loss: 1.1784
2022-07-17 04:46:32 - train: epoch 0112, iter [03300, 05004], lr: 0.001187, loss: 1.2473
2022-07-17 04:47:06 - train: epoch 0112, iter [03400, 05004], lr: 0.001182, loss: 1.1153
2022-07-17 04:47:40 - train: epoch 0112, iter [03500, 05004], lr: 0.001176, loss: 1.2336
2022-07-17 04:48:15 - train: epoch 0112, iter [03600, 05004], lr: 0.001170, loss: 1.3550
2022-07-17 04:48:48 - train: epoch 0112, iter [03700, 05004], lr: 0.001165, loss: 1.2550
2022-07-17 04:49:23 - train: epoch 0112, iter [03800, 05004], lr: 0.001159, loss: 1.3427
2022-07-17 04:49:56 - train: epoch 0112, iter [03900, 05004], lr: 0.001153, loss: 1.0159
2022-07-17 04:50:31 - train: epoch 0112, iter [04000, 05004], lr: 0.001148, loss: 1.1760
2022-07-17 04:51:05 - train: epoch 0112, iter [04100, 05004], lr: 0.001142, loss: 1.0855
2022-07-17 04:51:39 - train: epoch 0112, iter [04200, 05004], lr: 0.001137, loss: 1.1372
2022-07-17 04:52:14 - train: epoch 0112, iter [04300, 05004], lr: 0.001131, loss: 1.2580
2022-07-17 04:52:48 - train: epoch 0112, iter [04400, 05004], lr: 0.001126, loss: 1.5198
2022-07-17 04:53:23 - train: epoch 0112, iter [04500, 05004], lr: 0.001120, loss: 1.2351
2022-07-17 04:53:56 - train: epoch 0112, iter [04600, 05004], lr: 0.001115, loss: 1.2260
2022-07-17 04:54:32 - train: epoch 0112, iter [04700, 05004], lr: 0.001109, loss: 1.2463
2022-07-17 04:55:05 - train: epoch 0112, iter [04800, 05004], lr: 0.001104, loss: 1.2900
2022-07-17 04:55:40 - train: epoch 0112, iter [04900, 05004], lr: 0.001098, loss: 1.1484
2022-07-17 04:56:13 - train: epoch 0112, iter [05000, 05004], lr: 0.001093, loss: 1.0497
2022-07-17 04:56:14 - train: epoch 112, train_loss: 1.2374
2022-07-17 04:57:30 - eval: epoch: 112, acc1: 71.704%, acc5: 90.138%, test_loss: 1.1555, per_image_load_time: 2.687ms, per_image_inference_time: 0.220ms
2022-07-17 04:57:30 - until epoch: 112, best_acc1: 71.704%
2022-07-17 04:57:30 - epoch 113 lr: 0.001093
2022-07-17 04:58:09 - train: epoch 0113, iter [00100, 05004], lr: 0.001087, loss: 1.2086
2022-07-17 04:58:43 - train: epoch 0113, iter [00200, 05004], lr: 0.001082, loss: 1.2100
2022-07-17 04:59:18 - train: epoch 0113, iter [00300, 05004], lr: 0.001076, loss: 1.2768
2022-07-17 04:59:52 - train: epoch 0113, iter [00400, 05004], lr: 0.001071, loss: 1.3474
2022-07-17 05:00:26 - train: epoch 0113, iter [00500, 05004], lr: 0.001066, loss: 1.2508
2022-07-17 05:00:59 - train: epoch 0113, iter [00600, 05004], lr: 0.001060, loss: 1.4335
2022-07-17 05:01:32 - train: epoch 0113, iter [00700, 05004], lr: 0.001055, loss: 1.0721
2022-07-17 05:02:07 - train: epoch 0113, iter [00800, 05004], lr: 0.001050, loss: 1.1346
2022-07-17 05:02:41 - train: epoch 0113, iter [00900, 05004], lr: 0.001044, loss: 1.2727
2022-07-17 05:03:15 - train: epoch 0113, iter [01000, 05004], lr: 0.001039, loss: 1.0550
2022-07-17 05:03:50 - train: epoch 0113, iter [01100, 05004], lr: 0.001034, loss: 1.2858
2022-07-17 05:04:23 - train: epoch 0113, iter [01200, 05004], lr: 0.001028, loss: 1.1632
2022-07-17 05:04:57 - train: epoch 0113, iter [01300, 05004], lr: 0.001023, loss: 1.2111
2022-07-17 05:05:31 - train: epoch 0113, iter [01400, 05004], lr: 0.001018, loss: 1.2336
2022-07-17 05:06:05 - train: epoch 0113, iter [01500, 05004], lr: 0.001013, loss: 1.1795
2022-07-17 05:06:39 - train: epoch 0113, iter [01600, 05004], lr: 0.001007, loss: 1.1950
2022-07-17 05:07:13 - train: epoch 0113, iter [01700, 05004], lr: 0.001002, loss: 1.0489
2022-07-17 05:07:47 - train: epoch 0113, iter [01800, 05004], lr: 0.000997, loss: 1.0730
2022-07-17 05:08:20 - train: epoch 0113, iter [01900, 05004], lr: 0.000992, loss: 1.0687
2022-07-17 05:08:56 - train: epoch 0113, iter [02000, 05004], lr: 0.000987, loss: 1.0500
2022-07-17 05:09:30 - train: epoch 0113, iter [02100, 05004], lr: 0.000981, loss: 1.2911
2022-07-17 05:10:04 - train: epoch 0113, iter [02200, 05004], lr: 0.000976, loss: 1.3604
2022-07-17 05:10:39 - train: epoch 0113, iter [02300, 05004], lr: 0.000971, loss: 1.2841
2022-07-17 05:11:12 - train: epoch 0113, iter [02400, 05004], lr: 0.000966, loss: 1.1207
2022-07-17 05:11:47 - train: epoch 0113, iter [02500, 05004], lr: 0.000961, loss: 1.2629
2022-07-17 05:12:21 - train: epoch 0113, iter [02600, 05004], lr: 0.000956, loss: 1.1898
2022-07-17 05:12:54 - train: epoch 0113, iter [02700, 05004], lr: 0.000951, loss: 1.2060
2022-07-17 05:13:28 - train: epoch 0113, iter [02800, 05004], lr: 0.000946, loss: 1.1256
2022-07-17 05:14:02 - train: epoch 0113, iter [02900, 05004], lr: 0.000941, loss: 1.2264
2022-07-17 05:14:36 - train: epoch 0113, iter [03000, 05004], lr: 0.000935, loss: 1.1164
2022-07-17 05:15:11 - train: epoch 0113, iter [03100, 05004], lr: 0.000930, loss: 1.2295
2022-07-17 05:15:45 - train: epoch 0113, iter [03200, 05004], lr: 0.000925, loss: 1.1205
2022-07-17 05:16:18 - train: epoch 0113, iter [03300, 05004], lr: 0.000920, loss: 1.2525
2022-07-17 05:16:53 - train: epoch 0113, iter [03400, 05004], lr: 0.000915, loss: 1.1122
2022-07-17 05:17:27 - train: epoch 0113, iter [03500, 05004], lr: 0.000910, loss: 1.1896
2022-07-17 05:18:00 - train: epoch 0113, iter [03600, 05004], lr: 0.000906, loss: 1.3679
2022-07-17 05:18:35 - train: epoch 0113, iter [03700, 05004], lr: 0.000901, loss: 1.2289
2022-07-17 05:19:09 - train: epoch 0113, iter [03800, 05004], lr: 0.000896, loss: 1.2324
2022-07-17 05:19:44 - train: epoch 0113, iter [03900, 05004], lr: 0.000891, loss: 1.4075
2022-07-17 05:20:18 - train: epoch 0113, iter [04000, 05004], lr: 0.000886, loss: 1.1660
2022-07-17 05:20:53 - train: epoch 0113, iter [04100, 05004], lr: 0.000881, loss: 1.3385
2022-07-17 05:21:26 - train: epoch 0113, iter [04200, 05004], lr: 0.000876, loss: 1.2649
2022-07-17 05:22:01 - train: epoch 0113, iter [04300, 05004], lr: 0.000871, loss: 1.1315
2022-07-17 05:22:35 - train: epoch 0113, iter [04400, 05004], lr: 0.000866, loss: 1.0826
2022-07-17 05:23:09 - train: epoch 0113, iter [04500, 05004], lr: 0.000861, loss: 1.2585
2022-07-17 05:23:43 - train: epoch 0113, iter [04600, 05004], lr: 0.000857, loss: 1.1876
2022-07-17 05:24:18 - train: epoch 0113, iter [04700, 05004], lr: 0.000852, loss: 1.2365
2022-07-17 05:24:52 - train: epoch 0113, iter [04800, 05004], lr: 0.000847, loss: 1.2753
2022-07-17 05:25:26 - train: epoch 0113, iter [04900, 05004], lr: 0.000842, loss: 1.1900
2022-07-17 05:25:59 - train: epoch 0113, iter [05000, 05004], lr: 0.000837, loss: 1.1883
2022-07-17 05:26:00 - train: epoch 113, train_loss: 1.2245
2022-07-17 05:27:15 - eval: epoch: 113, acc1: 71.646%, acc5: 90.238%, test_loss: 1.1474, per_image_load_time: 1.993ms, per_image_inference_time: 0.229ms
2022-07-17 05:27:15 - until epoch: 113, best_acc1: 71.704%
2022-07-17 05:27:15 - epoch 114 lr: 0.000837
2022-07-17 05:27:55 - train: epoch 0114, iter [00100, 05004], lr: 0.000832, loss: 1.3317
2022-07-17 05:28:29 - train: epoch 0114, iter [00200, 05004], lr: 0.000828, loss: 1.2800
2022-07-17 05:29:03 - train: epoch 0114, iter [00300, 05004], lr: 0.000823, loss: 1.2329
2022-07-17 05:29:37 - train: epoch 0114, iter [00400, 05004], lr: 0.000818, loss: 1.3279
2022-07-17 05:30:11 - train: epoch 0114, iter [00500, 05004], lr: 0.000814, loss: 1.1477
2022-07-17 05:30:45 - train: epoch 0114, iter [00600, 05004], lr: 0.000809, loss: 1.1112
2022-07-17 05:31:19 - train: epoch 0114, iter [00700, 05004], lr: 0.000804, loss: 1.2186
2022-07-17 05:31:54 - train: epoch 0114, iter [00800, 05004], lr: 0.000800, loss: 1.1749
2022-07-17 05:32:26 - train: epoch 0114, iter [00900, 05004], lr: 0.000795, loss: 1.1023
2022-07-17 05:33:01 - train: epoch 0114, iter [01000, 05004], lr: 0.000790, loss: 1.1122
2022-07-17 05:33:35 - train: epoch 0114, iter [01100, 05004], lr: 0.000786, loss: 1.2066
2022-07-17 05:34:09 - train: epoch 0114, iter [01200, 05004], lr: 0.000781, loss: 1.2161
2022-07-17 05:34:42 - train: epoch 0114, iter [01300, 05004], lr: 0.000776, loss: 1.0825
2022-07-17 05:35:17 - train: epoch 0114, iter [01400, 05004], lr: 0.000772, loss: 1.2668
2022-07-17 05:35:52 - train: epoch 0114, iter [01500, 05004], lr: 0.000767, loss: 1.2971
2022-07-17 05:36:26 - train: epoch 0114, iter [01600, 05004], lr: 0.000763, loss: 1.0467
2022-07-17 05:36:59 - train: epoch 0114, iter [01700, 05004], lr: 0.000758, loss: 0.9859
2022-07-17 05:37:34 - train: epoch 0114, iter [01800, 05004], lr: 0.000754, loss: 1.2102
2022-07-17 05:38:08 - train: epoch 0114, iter [01900, 05004], lr: 0.000749, loss: 0.9838
2022-07-17 05:38:41 - train: epoch 0114, iter [02000, 05004], lr: 0.000745, loss: 1.2124
2022-07-17 05:39:16 - train: epoch 0114, iter [02100, 05004], lr: 0.000740, loss: 1.1155
2022-07-17 05:39:51 - train: epoch 0114, iter [02200, 05004], lr: 0.000736, loss: 1.0512
2022-07-17 05:40:25 - train: epoch 0114, iter [02300, 05004], lr: 0.000731, loss: 1.3888
2022-07-17 05:40:59 - train: epoch 0114, iter [02400, 05004], lr: 0.000727, loss: 1.1111
2022-07-17 05:41:33 - train: epoch 0114, iter [02500, 05004], lr: 0.000722, loss: 1.0464
2022-07-17 05:42:07 - train: epoch 0114, iter [02600, 05004], lr: 0.000718, loss: 1.2134
2022-07-17 05:42:41 - train: epoch 0114, iter [02700, 05004], lr: 0.000713, loss: 1.2636
2022-07-17 05:43:16 - train: epoch 0114, iter [02800, 05004], lr: 0.000709, loss: 1.3620
2022-07-17 05:43:49 - train: epoch 0114, iter [02900, 05004], lr: 0.000705, loss: 1.0665
2022-07-17 05:44:23 - train: epoch 0114, iter [03000, 05004], lr: 0.000700, loss: 1.1131
2022-07-17 05:44:57 - train: epoch 0114, iter [03100, 05004], lr: 0.000696, loss: 1.2078
2022-07-17 05:45:32 - train: epoch 0114, iter [03200, 05004], lr: 0.000692, loss: 1.3108
2022-07-17 05:46:05 - train: epoch 0114, iter [03300, 05004], lr: 0.000687, loss: 1.2031
2022-07-17 05:46:40 - train: epoch 0114, iter [03400, 05004], lr: 0.000683, loss: 1.3126
2022-07-17 05:47:14 - train: epoch 0114, iter [03500, 05004], lr: 0.000679, loss: 1.1696
2022-07-17 05:47:49 - train: epoch 0114, iter [03600, 05004], lr: 0.000674, loss: 1.2816
2022-07-17 05:48:23 - train: epoch 0114, iter [03700, 05004], lr: 0.000670, loss: 1.2030
2022-07-17 05:48:58 - train: epoch 0114, iter [03800, 05004], lr: 0.000666, loss: 1.1886
2022-07-17 05:49:31 - train: epoch 0114, iter [03900, 05004], lr: 0.000662, loss: 1.1365
2022-07-17 05:50:05 - train: epoch 0114, iter [04000, 05004], lr: 0.000657, loss: 1.1252
2022-07-17 05:50:39 - train: epoch 0114, iter [04100, 05004], lr: 0.000653, loss: 1.1168
2022-07-17 05:51:14 - train: epoch 0114, iter [04200, 05004], lr: 0.000649, loss: 1.3062
2022-07-17 05:51:48 - train: epoch 0114, iter [04300, 05004], lr: 0.000645, loss: 1.2287
2022-07-17 05:52:22 - train: epoch 0114, iter [04400, 05004], lr: 0.000641, loss: 1.3225
2022-07-17 05:52:56 - train: epoch 0114, iter [04500, 05004], lr: 0.000636, loss: 1.3308
2022-07-17 05:53:31 - train: epoch 0114, iter [04600, 05004], lr: 0.000632, loss: 1.3793
2022-07-17 05:54:05 - train: epoch 0114, iter [04700, 05004], lr: 0.000628, loss: 1.1418
2022-07-17 05:54:39 - train: epoch 0114, iter [04800, 05004], lr: 0.000624, loss: 1.3168
2022-07-17 05:55:13 - train: epoch 0114, iter [04900, 05004], lr: 0.000620, loss: 1.1232
2022-07-17 05:55:45 - train: epoch 0114, iter [05000, 05004], lr: 0.000616, loss: 1.3685
2022-07-17 05:55:46 - train: epoch 114, train_loss: 1.2134
2022-07-17 05:57:03 - eval: epoch: 114, acc1: 71.680%, acc5: 90.260%, test_loss: 1.1447, per_image_load_time: 2.753ms, per_image_inference_time: 0.232ms
2022-07-17 05:57:03 - until epoch: 114, best_acc1: 71.704%
2022-07-17 05:57:03 - epoch 115 lr: 0.000616
2022-07-17 05:57:42 - train: epoch 0115, iter [00100, 05004], lr: 0.000611, loss: 1.0814
2022-07-17 05:58:16 - train: epoch 0115, iter [00200, 05004], lr: 0.000607, loss: 1.1871
2022-07-17 05:58:50 - train: epoch 0115, iter [00300, 05004], lr: 0.000603, loss: 1.4168
2022-07-17 05:59:25 - train: epoch 0115, iter [00400, 05004], lr: 0.000599, loss: 1.1064
2022-07-17 05:59:58 - train: epoch 0115, iter [00500, 05004], lr: 0.000595, loss: 1.1521
2022-07-17 06:00:31 - train: epoch 0115, iter [00600, 05004], lr: 0.000591, loss: 1.2900
2022-07-17 06:01:05 - train: epoch 0115, iter [00700, 05004], lr: 0.000587, loss: 1.1780
2022-07-17 06:01:40 - train: epoch 0115, iter [00800, 05004], lr: 0.000583, loss: 1.1161
2022-07-17 06:02:14 - train: epoch 0115, iter [00900, 05004], lr: 0.000579, loss: 1.2431
2022-07-17 06:02:48 - train: epoch 0115, iter [01000, 05004], lr: 0.000575, loss: 1.1878
2022-07-17 06:03:22 - train: epoch 0115, iter [01100, 05004], lr: 0.000571, loss: 1.2150
2022-07-17 06:03:57 - train: epoch 0115, iter [01200, 05004], lr: 0.000567, loss: 1.2170
2022-07-17 06:04:31 - train: epoch 0115, iter [01300, 05004], lr: 0.000564, loss: 1.1412
2022-07-17 06:05:05 - train: epoch 0115, iter [01400, 05004], lr: 0.000560, loss: 1.3214
2022-07-17 06:05:39 - train: epoch 0115, iter [01500, 05004], lr: 0.000556, loss: 1.4214
2022-07-17 06:06:13 - train: epoch 0115, iter [01600, 05004], lr: 0.000552, loss: 1.1133
2022-07-17 06:06:48 - train: epoch 0115, iter [01700, 05004], lr: 0.000548, loss: 1.3355
2022-07-17 06:07:21 - train: epoch 0115, iter [01800, 05004], lr: 0.000544, loss: 1.3480
2022-07-17 06:07:56 - train: epoch 0115, iter [01900, 05004], lr: 0.000540, loss: 1.3071
2022-07-17 06:08:30 - train: epoch 0115, iter [02000, 05004], lr: 0.000536, loss: 1.3514
2022-07-17 06:09:04 - train: epoch 0115, iter [02100, 05004], lr: 0.000533, loss: 1.0078
2022-07-17 06:09:39 - train: epoch 0115, iter [02200, 05004], lr: 0.000529, loss: 1.3661
2022-07-17 06:10:13 - train: epoch 0115, iter [02300, 05004], lr: 0.000525, loss: 1.3151
2022-07-17 06:10:47 - train: epoch 0115, iter [02400, 05004], lr: 0.000521, loss: 1.2197
2022-07-17 06:11:21 - train: epoch 0115, iter [02500, 05004], lr: 0.000518, loss: 1.0505
2022-07-17 06:11:55 - train: epoch 0115, iter [02600, 05004], lr: 0.000514, loss: 1.1416
2022-07-17 06:12:30 - train: epoch 0115, iter [02700, 05004], lr: 0.000510, loss: 1.0038
2022-07-17 06:13:05 - train: epoch 0115, iter [02800, 05004], lr: 0.000506, loss: 1.2777
2022-07-17 06:13:40 - train: epoch 0115, iter [02900, 05004], lr: 0.000503, loss: 1.1154
2022-07-17 06:14:14 - train: epoch 0115, iter [03000, 05004], lr: 0.000499, loss: 0.9787
2022-07-17 06:14:47 - train: epoch 0115, iter [03100, 05004], lr: 0.000495, loss: 1.2822
2022-07-17 06:15:21 - train: epoch 0115, iter [03200, 05004], lr: 0.000492, loss: 1.2611
2022-07-17 06:15:56 - train: epoch 0115, iter [03300, 05004], lr: 0.000488, loss: 1.2830
2022-07-17 06:16:30 - train: epoch 0115, iter [03400, 05004], lr: 0.000484, loss: 1.2119
2022-07-17 06:17:04 - train: epoch 0115, iter [03500, 05004], lr: 0.000481, loss: 1.2357
2022-07-17 06:17:39 - train: epoch 0115, iter [03600, 05004], lr: 0.000477, loss: 1.1585
2022-07-17 06:18:13 - train: epoch 0115, iter [03700, 05004], lr: 0.000473, loss: 1.2136
2022-07-17 06:18:47 - train: epoch 0115, iter [03800, 05004], lr: 0.000470, loss: 1.3959
2022-07-17 06:19:22 - train: epoch 0115, iter [03900, 05004], lr: 0.000466, loss: 1.1598
2022-07-17 06:19:56 - train: epoch 0115, iter [04000, 05004], lr: 0.000463, loss: 1.1131
2022-07-17 06:20:31 - train: epoch 0115, iter [04100, 05004], lr: 0.000459, loss: 1.2579
2022-07-17 06:21:05 - train: epoch 0115, iter [04200, 05004], lr: 0.000456, loss: 0.9851
2022-07-17 06:21:40 - train: epoch 0115, iter [04300, 05004], lr: 0.000452, loss: 1.2741
2022-07-17 06:22:14 - train: epoch 0115, iter [04400, 05004], lr: 0.000449, loss: 1.1585
2022-07-17 06:22:48 - train: epoch 0115, iter [04500, 05004], lr: 0.000445, loss: 1.2821
2022-07-17 06:23:23 - train: epoch 0115, iter [04600, 05004], lr: 0.000442, loss: 1.2884
2022-07-17 06:23:57 - train: epoch 0115, iter [04700, 05004], lr: 0.000438, loss: 1.2784
2022-07-17 06:24:31 - train: epoch 0115, iter [04800, 05004], lr: 0.000435, loss: 1.1929
2022-07-17 06:25:06 - train: epoch 0115, iter [04900, 05004], lr: 0.000431, loss: 1.3084
2022-07-17 06:25:38 - train: epoch 0115, iter [05000, 05004], lr: 0.000428, loss: 1.3650
2022-07-17 06:25:39 - train: epoch 115, train_loss: 1.2035
2022-07-17 06:26:55 - eval: epoch: 115, acc1: 71.976%, acc5: 90.412%, test_loss: 1.1371, per_image_load_time: 1.602ms, per_image_inference_time: 0.226ms
2022-07-17 06:26:55 - until epoch: 115, best_acc1: 71.976%
2022-07-17 06:26:55 - epoch 116 lr: 0.000428
2022-07-17 06:27:35 - train: epoch 0116, iter [00100, 05004], lr: 0.000424, loss: 1.0263
2022-07-17 06:28:09 - train: epoch 0116, iter [00200, 05004], lr: 0.000421, loss: 1.2343
2022-07-17 06:28:44 - train: epoch 0116, iter [00300, 05004], lr: 0.000418, loss: 1.0369
2022-07-17 06:29:17 - train: epoch 0116, iter [00400, 05004], lr: 0.000414, loss: 1.2099
2022-07-17 06:29:51 - train: epoch 0116, iter [00500, 05004], lr: 0.000411, loss: 1.3286
2022-07-17 06:30:25 - train: epoch 0116, iter [00600, 05004], lr: 0.000408, loss: 1.0143
2022-07-17 06:30:59 - train: epoch 0116, iter [00700, 05004], lr: 0.000404, loss: 1.2493
2022-07-17 06:31:34 - train: epoch 0116, iter [00800, 05004], lr: 0.000401, loss: 1.2345
2022-07-17 06:32:08 - train: epoch 0116, iter [00900, 05004], lr: 0.000398, loss: 1.1725
2022-07-17 06:32:42 - train: epoch 0116, iter [01000, 05004], lr: 0.000394, loss: 1.3416
2022-07-17 06:33:16 - train: epoch 0116, iter [01100, 05004], lr: 0.000391, loss: 1.1711
2022-07-17 06:33:49 - train: epoch 0116, iter [01200, 05004], lr: 0.000388, loss: 1.1755
2022-07-17 06:34:24 - train: epoch 0116, iter [01300, 05004], lr: 0.000385, loss: 1.2938
2022-07-17 06:34:57 - train: epoch 0116, iter [01400, 05004], lr: 0.000381, loss: 1.3397
2022-07-17 06:35:32 - train: epoch 0116, iter [01500, 05004], lr: 0.000378, loss: 0.9781
2022-07-17 06:36:07 - train: epoch 0116, iter [01600, 05004], lr: 0.000375, loss: 1.1196
2022-07-17 06:36:41 - train: epoch 0116, iter [01700, 05004], lr: 0.000372, loss: 1.1000
2022-07-17 06:37:15 - train: epoch 0116, iter [01800, 05004], lr: 0.000368, loss: 1.3395
2022-07-17 06:37:49 - train: epoch 0116, iter [01900, 05004], lr: 0.000365, loss: 1.0072
2022-07-17 06:38:23 - train: epoch 0116, iter [02000, 05004], lr: 0.000362, loss: 1.1289
2022-07-17 06:38:57 - train: epoch 0116, iter [02100, 05004], lr: 0.000359, loss: 1.1049
2022-07-17 06:39:32 - train: epoch 0116, iter [02200, 05004], lr: 0.000356, loss: 1.2571
2022-07-17 06:40:05 - train: epoch 0116, iter [02300, 05004], lr: 0.000353, loss: 1.1741
2022-07-17 06:40:41 - train: epoch 0116, iter [02400, 05004], lr: 0.000350, loss: 1.3203
2022-07-17 06:41:15 - train: epoch 0116, iter [02500, 05004], lr: 0.000347, loss: 0.9962
2022-07-17 06:41:49 - train: epoch 0116, iter [02600, 05004], lr: 0.000344, loss: 1.1430
2022-07-17 06:42:23 - train: epoch 0116, iter [02700, 05004], lr: 0.000341, loss: 1.4048
2022-07-17 06:42:57 - train: epoch 0116, iter [02800, 05004], lr: 0.000337, loss: 1.3218
2022-07-17 06:43:31 - train: epoch 0116, iter [02900, 05004], lr: 0.000334, loss: 1.2201
2022-07-17 06:44:06 - train: epoch 0116, iter [03000, 05004], lr: 0.000331, loss: 1.2341
2022-07-17 06:44:40 - train: epoch 0116, iter [03100, 05004], lr: 0.000328, loss: 1.2878
2022-07-17 06:45:15 - train: epoch 0116, iter [03200, 05004], lr: 0.000325, loss: 1.0875
2022-07-17 06:45:49 - train: epoch 0116, iter [03300, 05004], lr: 0.000322, loss: 1.3545
2022-07-17 06:46:23 - train: epoch 0116, iter [03400, 05004], lr: 0.000320, loss: 1.2512
2022-07-17 06:46:58 - train: epoch 0116, iter [03500, 05004], lr: 0.000317, loss: 1.3343
2022-07-17 06:47:32 - train: epoch 0116, iter [03600, 05004], lr: 0.000314, loss: 1.1979
2022-07-17 06:48:06 - train: epoch 0116, iter [03700, 05004], lr: 0.000311, loss: 1.0981
2022-07-17 06:48:41 - train: epoch 0116, iter [03800, 05004], lr: 0.000308, loss: 0.9819
2022-07-17 06:49:14 - train: epoch 0116, iter [03900, 05004], lr: 0.000305, loss: 1.2010
2022-07-17 06:49:49 - train: epoch 0116, iter [04000, 05004], lr: 0.000302, loss: 1.2768
2022-07-17 06:50:24 - train: epoch 0116, iter [04100, 05004], lr: 0.000299, loss: 1.3979
2022-07-17 06:50:58 - train: epoch 0116, iter [04200, 05004], lr: 0.000296, loss: 1.1158
2022-07-17 06:51:33 - train: epoch 0116, iter [04300, 05004], lr: 0.000293, loss: 1.2733
2022-07-17 06:52:06 - train: epoch 0116, iter [04400, 05004], lr: 0.000291, loss: 1.0688
2022-07-17 06:52:41 - train: epoch 0116, iter [04500, 05004], lr: 0.000288, loss: 1.2374
2022-07-17 06:53:15 - train: epoch 0116, iter [04600, 05004], lr: 0.000285, loss: 1.3492
2022-07-17 06:53:50 - train: epoch 0116, iter [04700, 05004], lr: 0.000282, loss: 1.2365
2022-07-17 06:54:25 - train: epoch 0116, iter [04800, 05004], lr: 0.000280, loss: 1.1202
2022-07-17 06:54:59 - train: epoch 0116, iter [04900, 05004], lr: 0.000277, loss: 1.1395
2022-07-17 06:55:32 - train: epoch 0116, iter [05000, 05004], lr: 0.000274, loss: 1.1540
2022-07-17 06:55:33 - train: epoch 116, train_loss: 1.1992
2022-07-17 06:56:48 - eval: epoch: 116, acc1: 71.996%, acc5: 90.438%, test_loss: 1.1350, per_image_load_time: 1.588ms, per_image_inference_time: 0.240ms
2022-07-17 06:56:48 - until epoch: 116, best_acc1: 71.996%
2022-07-17 06:56:48 - epoch 117 lr: 0.000274
2022-07-17 06:57:28 - train: epoch 0117, iter [00100, 05004], lr: 0.000271, loss: 1.3199
2022-07-17 06:58:01 - train: epoch 0117, iter [00200, 05004], lr: 0.000268, loss: 1.1414
2022-07-17 06:58:35 - train: epoch 0117, iter [00300, 05004], lr: 0.000266, loss: 1.3361
2022-07-17 06:59:10 - train: epoch 0117, iter [00400, 05004], lr: 0.000263, loss: 1.0847
2022-07-17 06:59:43 - train: epoch 0117, iter [00500, 05004], lr: 0.000260, loss: 1.3660
2022-07-17 07:00:18 - train: epoch 0117, iter [00600, 05004], lr: 0.000258, loss: 1.2813
2022-07-17 07:00:52 - train: epoch 0117, iter [00700, 05004], lr: 0.000255, loss: 1.2037
2022-07-17 07:01:26 - train: epoch 0117, iter [00800, 05004], lr: 0.000252, loss: 1.1923
2022-07-17 07:02:01 - train: epoch 0117, iter [00900, 05004], lr: 0.000250, loss: 1.2255
2022-07-17 07:02:34 - train: epoch 0117, iter [01000, 05004], lr: 0.000247, loss: 1.3304
2022-07-17 07:03:09 - train: epoch 0117, iter [01100, 05004], lr: 0.000245, loss: 1.2922
2022-07-17 07:03:43 - train: epoch 0117, iter [01200, 05004], lr: 0.000242, loss: 1.3241
2022-07-17 07:04:17 - train: epoch 0117, iter [01300, 05004], lr: 0.000240, loss: 1.2092
2022-07-17 07:04:51 - train: epoch 0117, iter [01400, 05004], lr: 0.000237, loss: 1.2233
2022-07-17 07:05:25 - train: epoch 0117, iter [01500, 05004], lr: 0.000234, loss: 1.2326
2022-07-17 07:05:59 - train: epoch 0117, iter [01600, 05004], lr: 0.000232, loss: 1.0619
2022-07-17 07:06:33 - train: epoch 0117, iter [01700, 05004], lr: 0.000229, loss: 1.2408
2022-07-17 07:07:07 - train: epoch 0117, iter [01800, 05004], lr: 0.000227, loss: 1.1867
2022-07-17 07:07:42 - train: epoch 0117, iter [01900, 05004], lr: 0.000224, loss: 1.1185
2022-07-17 07:08:16 - train: epoch 0117, iter [02000, 05004], lr: 0.000222, loss: 1.2398
2022-07-17 07:08:49 - train: epoch 0117, iter [02100, 05004], lr: 0.000219, loss: 1.1984
2022-07-17 07:09:24 - train: epoch 0117, iter [02200, 05004], lr: 0.000217, loss: 1.1836
2022-07-17 07:09:59 - train: epoch 0117, iter [02300, 05004], lr: 0.000215, loss: 1.1352
2022-07-17 07:10:33 - train: epoch 0117, iter [02400, 05004], lr: 0.000212, loss: 1.1343
2022-07-17 07:11:07 - train: epoch 0117, iter [02500, 05004], lr: 0.000210, loss: 1.1831
2022-07-17 07:11:42 - train: epoch 0117, iter [02600, 05004], lr: 0.000207, loss: 1.1923
2022-07-17 07:12:17 - train: epoch 0117, iter [02700, 05004], lr: 0.000205, loss: 1.1114
2022-07-17 07:12:51 - train: epoch 0117, iter [02800, 05004], lr: 0.000203, loss: 0.9389
2022-07-17 07:13:25 - train: epoch 0117, iter [02900, 05004], lr: 0.000200, loss: 1.1398
2022-07-17 07:13:59 - train: epoch 0117, iter [03000, 05004], lr: 0.000198, loss: 1.0939
2022-07-17 07:14:33 - train: epoch 0117, iter [03100, 05004], lr: 0.000196, loss: 1.2319
2022-07-17 07:15:07 - train: epoch 0117, iter [03200, 05004], lr: 0.000193, loss: 1.0370
2022-07-17 07:15:41 - train: epoch 0117, iter [03300, 05004], lr: 0.000191, loss: 1.2072
2022-07-17 07:16:16 - train: epoch 0117, iter [03400, 05004], lr: 0.000189, loss: 1.0872
2022-07-17 07:16:51 - train: epoch 0117, iter [03500, 05004], lr: 0.000187, loss: 1.2626
2022-07-17 07:17:26 - train: epoch 0117, iter [03600, 05004], lr: 0.000184, loss: 1.2333
2022-07-17 07:18:00 - train: epoch 0117, iter [03700, 05004], lr: 0.000182, loss: 1.1340
2022-07-17 07:18:34 - train: epoch 0117, iter [03800, 05004], lr: 0.000180, loss: 1.2525
2022-07-17 07:19:09 - train: epoch 0117, iter [03900, 05004], lr: 0.000178, loss: 1.1484
2022-07-17 07:19:44 - train: epoch 0117, iter [04000, 05004], lr: 0.000175, loss: 1.0986
2022-07-17 07:20:18 - train: epoch 0117, iter [04100, 05004], lr: 0.000173, loss: 1.2465
2022-07-17 07:20:52 - train: epoch 0117, iter [04200, 05004], lr: 0.000171, loss: 1.1894
2022-07-17 07:21:26 - train: epoch 0117, iter [04300, 05004], lr: 0.000169, loss: 0.9972
2022-07-17 07:22:01 - train: epoch 0117, iter [04400, 05004], lr: 0.000167, loss: 1.0399
2022-07-17 07:22:36 - train: epoch 0117, iter [04500, 05004], lr: 0.000165, loss: 1.3139
2022-07-17 07:23:10 - train: epoch 0117, iter [04600, 05004], lr: 0.000163, loss: 1.2960
2022-07-17 07:23:44 - train: epoch 0117, iter [04700, 05004], lr: 0.000160, loss: 1.2464
2022-07-17 07:24:19 - train: epoch 0117, iter [04800, 05004], lr: 0.000158, loss: 1.2093
2022-07-17 07:24:52 - train: epoch 0117, iter [04900, 05004], lr: 0.000156, loss: 1.2874
2022-07-17 07:25:26 - train: epoch 0117, iter [05000, 05004], lr: 0.000154, loss: 1.4190
2022-07-17 07:25:26 - train: epoch 117, train_loss: 1.1939
2022-07-17 07:26:42 - eval: epoch: 117, acc1: 72.150%, acc5: 90.426%, test_loss: 1.1327, per_image_load_time: 1.934ms, per_image_inference_time: 0.245ms
2022-07-17 07:26:42 - until epoch: 117, best_acc1: 72.150%
2022-07-17 07:26:42 - epoch 118 lr: 0.000154
2022-07-17 07:27:22 - train: epoch 0118, iter [00100, 05004], lr: 0.000152, loss: 1.2266
2022-07-17 07:27:58 - train: epoch 0118, iter [00200, 05004], lr: 0.000150, loss: 1.2070
2022-07-17 07:28:30 - train: epoch 0118, iter [00300, 05004], lr: 0.000148, loss: 1.3213
2022-07-17 07:29:03 - train: epoch 0118, iter [00400, 05004], lr: 0.000146, loss: 1.3981
2022-07-17 07:29:37 - train: epoch 0118, iter [00500, 05004], lr: 0.000144, loss: 1.2177
2022-07-17 07:30:12 - train: epoch 0118, iter [00600, 05004], lr: 0.000142, loss: 1.2221
2022-07-17 07:30:45 - train: epoch 0118, iter [00700, 05004], lr: 0.000140, loss: 1.3154
2022-07-17 07:31:20 - train: epoch 0118, iter [00800, 05004], lr: 0.000138, loss: 1.0319
2022-07-17 07:31:54 - train: epoch 0118, iter [00900, 05004], lr: 0.000136, loss: 1.0202
2022-07-17 07:32:29 - train: epoch 0118, iter [01000, 05004], lr: 0.000134, loss: 1.3564
2022-07-17 07:33:03 - train: epoch 0118, iter [01100, 05004], lr: 0.000132, loss: 1.2904
2022-07-17 07:33:37 - train: epoch 0118, iter [01200, 05004], lr: 0.000130, loss: 1.3648
2022-07-17 07:34:11 - train: epoch 0118, iter [01300, 05004], lr: 0.000129, loss: 1.2058
2022-07-17 07:34:45 - train: epoch 0118, iter [01400, 05004], lr: 0.000127, loss: 1.1202
2022-07-17 07:35:19 - train: epoch 0118, iter [01500, 05004], lr: 0.000125, loss: 1.1810
2022-07-17 07:35:54 - train: epoch 0118, iter [01600, 05004], lr: 0.000123, loss: 1.0898
2022-07-17 07:36:27 - train: epoch 0118, iter [01700, 05004], lr: 0.000121, loss: 0.9695
2022-07-17 07:37:03 - train: epoch 0118, iter [01800, 05004], lr: 0.000119, loss: 1.1517
2022-07-17 07:37:37 - train: epoch 0118, iter [01900, 05004], lr: 0.000118, loss: 1.3627
2022-07-17 07:38:12 - train: epoch 0118, iter [02000, 05004], lr: 0.000116, loss: 1.0273
2022-07-17 07:38:45 - train: epoch 0118, iter [02100, 05004], lr: 0.000114, loss: 1.0430
2022-07-17 07:39:19 - train: epoch 0118, iter [02200, 05004], lr: 0.000112, loss: 1.1314
2022-07-17 07:39:54 - train: epoch 0118, iter [02300, 05004], lr: 0.000111, loss: 1.2141
2022-07-17 07:40:29 - train: epoch 0118, iter [02400, 05004], lr: 0.000109, loss: 1.2368
2022-07-17 07:41:02 - train: epoch 0118, iter [02500, 05004], lr: 0.000107, loss: 1.1323
2022-07-17 07:41:37 - train: epoch 0118, iter [02600, 05004], lr: 0.000105, loss: 1.2210
2022-07-17 07:42:11 - train: epoch 0118, iter [02700, 05004], lr: 0.000104, loss: 1.0652
2022-07-17 07:42:45 - train: epoch 0118, iter [02800, 05004], lr: 0.000102, loss: 1.1141
2022-07-17 07:43:19 - train: epoch 0118, iter [02900, 05004], lr: 0.000100, loss: 1.2387
2022-07-17 07:43:53 - train: epoch 0118, iter [03000, 05004], lr: 0.000099, loss: 1.0793
2022-07-17 07:44:28 - train: epoch 0118, iter [03100, 05004], lr: 0.000097, loss: 1.1935
2022-07-17 07:45:02 - train: epoch 0118, iter [03200, 05004], lr: 0.000095, loss: 1.3281
2022-07-17 07:45:36 - train: epoch 0118, iter [03300, 05004], lr: 0.000094, loss: 1.1940
2022-07-17 07:46:11 - train: epoch 0118, iter [03400, 05004], lr: 0.000092, loss: 1.2122
2022-07-17 07:46:45 - train: epoch 0118, iter [03500, 05004], lr: 0.000091, loss: 1.1415
2022-07-17 07:47:20 - train: epoch 0118, iter [03600, 05004], lr: 0.000089, loss: 1.3002
2022-07-17 07:47:54 - train: epoch 0118, iter [03700, 05004], lr: 0.000088, loss: 1.1905
2022-07-17 07:48:28 - train: epoch 0118, iter [03800, 05004], lr: 0.000086, loss: 1.2666
2022-07-17 07:49:03 - train: epoch 0118, iter [03900, 05004], lr: 0.000084, loss: 1.1935
2022-07-17 07:49:37 - train: epoch 0118, iter [04000, 05004], lr: 0.000083, loss: 1.1541
2022-07-17 07:50:11 - train: epoch 0118, iter [04100, 05004], lr: 0.000081, loss: 1.1451
2022-07-17 07:50:45 - train: epoch 0118, iter [04200, 05004], lr: 0.000080, loss: 1.1751
2022-07-17 07:51:20 - train: epoch 0118, iter [04300, 05004], lr: 0.000079, loss: 0.9520
2022-07-17 07:51:54 - train: epoch 0118, iter [04400, 05004], lr: 0.000077, loss: 1.3315
2022-07-17 07:52:29 - train: epoch 0118, iter [04500, 05004], lr: 0.000076, loss: 1.1559
2022-07-17 07:53:03 - train: epoch 0118, iter [04600, 05004], lr: 0.000074, loss: 1.2376
2022-07-17 07:53:37 - train: epoch 0118, iter [04700, 05004], lr: 0.000073, loss: 1.2056
2022-07-17 07:54:11 - train: epoch 0118, iter [04800, 05004], lr: 0.000071, loss: 1.1916
2022-07-17 07:54:46 - train: epoch 0118, iter [04900, 05004], lr: 0.000070, loss: 1.1507
2022-07-17 07:55:19 - train: epoch 0118, iter [05000, 05004], lr: 0.000069, loss: 1.2130
2022-07-17 07:55:20 - train: epoch 118, train_loss: 1.1875
2022-07-17 07:56:36 - eval: epoch: 118, acc1: 72.160%, acc5: 90.462%, test_loss: 1.1306, per_image_load_time: 1.291ms, per_image_inference_time: 0.227ms
2022-07-17 07:56:36 - until epoch: 118, best_acc1: 72.160%
2022-07-17 07:56:36 - epoch 119 lr: 0.000069
2022-07-17 07:57:15 - train: epoch 0119, iter [00100, 05004], lr: 0.000067, loss: 1.4394
2022-07-17 07:57:50 - train: epoch 0119, iter [00200, 05004], lr: 0.000066, loss: 1.1789
2022-07-17 07:58:24 - train: epoch 0119, iter [00300, 05004], lr: 0.000064, loss: 1.1333
2022-07-17 07:58:57 - train: epoch 0119, iter [00400, 05004], lr: 0.000063, loss: 1.1554
2022-07-17 07:59:31 - train: epoch 0119, iter [00500, 05004], lr: 0.000062, loss: 1.1241
2022-07-17 08:00:06 - train: epoch 0119, iter [00600, 05004], lr: 0.000061, loss: 1.2334
2022-07-17 08:00:40 - train: epoch 0119, iter [00700, 05004], lr: 0.000059, loss: 1.2065
2022-07-17 08:01:14 - train: epoch 0119, iter [00800, 05004], lr: 0.000058, loss: 1.1340
2022-07-17 08:01:48 - train: epoch 0119, iter [00900, 05004], lr: 0.000057, loss: 1.1223
2022-07-17 08:02:23 - train: epoch 0119, iter [01000, 05004], lr: 0.000056, loss: 1.2075
2022-07-17 08:02:57 - train: epoch 0119, iter [01100, 05004], lr: 0.000054, loss: 1.3223
2022-07-17 08:03:31 - train: epoch 0119, iter [01200, 05004], lr: 0.000053, loss: 1.1534
2022-07-17 08:04:05 - train: epoch 0119, iter [01300, 05004], lr: 0.000052, loss: 1.2002
2022-07-17 08:04:40 - train: epoch 0119, iter [01400, 05004], lr: 0.000051, loss: 1.2636
2022-07-17 08:05:14 - train: epoch 0119, iter [01500, 05004], lr: 0.000050, loss: 1.1115
2022-07-17 08:05:48 - train: epoch 0119, iter [01600, 05004], lr: 0.000048, loss: 1.2538
2022-07-17 08:06:23 - train: epoch 0119, iter [01700, 05004], lr: 0.000047, loss: 1.0945
2022-07-17 08:06:57 - train: epoch 0119, iter [01800, 05004], lr: 0.000046, loss: 1.2155
2022-07-17 08:07:30 - train: epoch 0119, iter [01900, 05004], lr: 0.000045, loss: 1.2072
2022-07-17 08:08:05 - train: epoch 0119, iter [02000, 05004], lr: 0.000044, loss: 1.0787
2022-07-17 08:08:40 - train: epoch 0119, iter [02100, 05004], lr: 0.000043, loss: 1.4357
2022-07-17 08:09:14 - train: epoch 0119, iter [02200, 05004], lr: 0.000042, loss: 1.1983
2022-07-17 08:09:48 - train: epoch 0119, iter [02300, 05004], lr: 0.000041, loss: 1.1386
2022-07-17 08:10:23 - train: epoch 0119, iter [02400, 05004], lr: 0.000040, loss: 1.2657
2022-07-17 08:10:58 - train: epoch 0119, iter [02500, 05004], lr: 0.000039, loss: 1.1776
2022-07-17 08:11:32 - train: epoch 0119, iter [02600, 05004], lr: 0.000038, loss: 1.2711
2022-07-17 08:12:06 - train: epoch 0119, iter [02700, 05004], lr: 0.000037, loss: 1.2718
2022-07-17 08:12:42 - train: epoch 0119, iter [02800, 05004], lr: 0.000036, loss: 1.2755
2022-07-17 08:13:15 - train: epoch 0119, iter [02900, 05004], lr: 0.000035, loss: 1.1833
2022-07-17 08:13:49 - train: epoch 0119, iter [03000, 05004], lr: 0.000034, loss: 1.1661
2022-07-17 08:14:25 - train: epoch 0119, iter [03100, 05004], lr: 0.000033, loss: 1.1127
2022-07-17 08:14:59 - train: epoch 0119, iter [03200, 05004], lr: 0.000032, loss: 1.0398
2022-07-17 08:15:34 - train: epoch 0119, iter [03300, 05004], lr: 0.000031, loss: 1.1130
2022-07-17 08:16:07 - train: epoch 0119, iter [03400, 05004], lr: 0.000030, loss: 1.3145
2022-07-17 08:16:42 - train: epoch 0119, iter [03500, 05004], lr: 0.000029, loss: 1.1318
2022-07-17 08:17:16 - train: epoch 0119, iter [03600, 05004], lr: 0.000028, loss: 1.3310
2022-07-17 08:17:51 - train: epoch 0119, iter [03700, 05004], lr: 0.000027, loss: 1.3196
2022-07-17 08:18:26 - train: epoch 0119, iter [03800, 05004], lr: 0.000026, loss: 1.2001
2022-07-17 08:18:59 - train: epoch 0119, iter [03900, 05004], lr: 0.000026, loss: 1.4983
2022-07-17 08:19:34 - train: epoch 0119, iter [04000, 05004], lr: 0.000025, loss: 0.9899
2022-07-17 08:20:08 - train: epoch 0119, iter [04100, 05004], lr: 0.000024, loss: 1.2791
2022-07-17 08:20:43 - train: epoch 0119, iter [04200, 05004], lr: 0.000023, loss: 1.3377
2022-07-17 08:21:17 - train: epoch 0119, iter [04300, 05004], lr: 0.000022, loss: 1.1437
2022-07-17 08:21:50 - train: epoch 0119, iter [04400, 05004], lr: 0.000022, loss: 1.1857
2022-07-17 08:22:26 - train: epoch 0119, iter [04500, 05004], lr: 0.000021, loss: 1.2642
2022-07-17 08:22:59 - train: epoch 0119, iter [04600, 05004], lr: 0.000020, loss: 1.1575
2022-07-17 08:23:35 - train: epoch 0119, iter [04700, 05004], lr: 0.000019, loss: 1.0084
2022-07-17 08:24:08 - train: epoch 0119, iter [04800, 05004], lr: 0.000019, loss: 1.2614
2022-07-17 08:24:44 - train: epoch 0119, iter [04900, 05004], lr: 0.000018, loss: 1.3202
2022-07-17 08:25:16 - train: epoch 0119, iter [05000, 05004], lr: 0.000017, loss: 1.3137
2022-07-17 08:25:17 - train: epoch 119, train_loss: 1.1854
2022-07-17 08:26:33 - eval: epoch: 119, acc1: 72.116%, acc5: 90.406%, test_loss: 1.1329, per_image_load_time: 1.629ms, per_image_inference_time: 0.245ms
2022-07-17 08:26:33 - until epoch: 119, best_acc1: 72.160%
2022-07-17 08:26:33 - epoch 120 lr: 0.000017
2022-07-17 08:27:13 - train: epoch 0120, iter [00100, 05004], lr: 0.000016, loss: 1.1882
2022-07-17 08:27:47 - train: epoch 0120, iter [00200, 05004], lr: 0.000016, loss: 1.1747
2022-07-17 08:28:21 - train: epoch 0120, iter [00300, 05004], lr: 0.000015, loss: 1.1913
2022-07-17 08:28:55 - train: epoch 0120, iter [00400, 05004], lr: 0.000015, loss: 1.2905
2022-07-17 08:29:30 - train: epoch 0120, iter [00500, 05004], lr: 0.000014, loss: 1.1043
2022-07-17 08:30:03 - train: epoch 0120, iter [00600, 05004], lr: 0.000013, loss: 0.9744
2022-07-17 08:30:37 - train: epoch 0120, iter [00700, 05004], lr: 0.000013, loss: 1.3193
2022-07-17 08:31:11 - train: epoch 0120, iter [00800, 05004], lr: 0.000012, loss: 1.2326
2022-07-17 08:31:47 - train: epoch 0120, iter [00900, 05004], lr: 0.000012, loss: 1.2444
2022-07-17 08:32:20 - train: epoch 0120, iter [01000, 05004], lr: 0.000011, loss: 1.0503
2022-07-17 08:32:54 - train: epoch 0120, iter [01100, 05004], lr: 0.000010, loss: 1.1908
2022-07-17 08:33:28 - train: epoch 0120, iter [01200, 05004], lr: 0.000010, loss: 1.2361
2022-07-17 08:34:02 - train: epoch 0120, iter [01300, 05004], lr: 0.000009, loss: 0.9944
2022-07-17 08:34:36 - train: epoch 0120, iter [01400, 05004], lr: 0.000009, loss: 1.1384
2022-07-17 08:35:10 - train: epoch 0120, iter [01500, 05004], lr: 0.000008, loss: 1.1486
2022-07-17 08:35:45 - train: epoch 0120, iter [01600, 05004], lr: 0.000008, loss: 1.2753
2022-07-17 08:36:19 - train: epoch 0120, iter [01700, 05004], lr: 0.000007, loss: 1.1814
2022-07-17 08:36:54 - train: epoch 0120, iter [01800, 05004], lr: 0.000007, loss: 1.2038
2022-07-17 08:37:28 - train: epoch 0120, iter [01900, 05004], lr: 0.000007, loss: 1.1763
2022-07-17 08:38:01 - train: epoch 0120, iter [02000, 05004], lr: 0.000006, loss: 1.1327
2022-07-17 08:38:36 - train: epoch 0120, iter [02100, 05004], lr: 0.000006, loss: 1.1287
2022-07-17 08:39:10 - train: epoch 0120, iter [02200, 05004], lr: 0.000005, loss: 1.0490
2022-07-17 08:39:45 - train: epoch 0120, iter [02300, 05004], lr: 0.000005, loss: 1.2589
2022-07-17 08:40:20 - train: epoch 0120, iter [02400, 05004], lr: 0.000005, loss: 1.2996
2022-07-17 08:40:54 - train: epoch 0120, iter [02500, 05004], lr: 0.000004, loss: 1.1597
2022-07-17 08:41:29 - train: epoch 0120, iter [02600, 05004], lr: 0.000004, loss: 1.3993
2022-07-17 08:42:04 - train: epoch 0120, iter [02700, 05004], lr: 0.000004, loss: 1.1715
2022-07-17 08:42:38 - train: epoch 0120, iter [02800, 05004], lr: 0.000003, loss: 1.2420
2022-07-17 08:43:13 - train: epoch 0120, iter [02900, 05004], lr: 0.000003, loss: 1.0945
2022-07-17 08:43:46 - train: epoch 0120, iter [03000, 05004], lr: 0.000003, loss: 0.9869
2022-07-17 08:44:21 - train: epoch 0120, iter [03100, 05004], lr: 0.000002, loss: 1.0712
2022-07-17 08:44:56 - train: epoch 0120, iter [03200, 05004], lr: 0.000002, loss: 1.2408
2022-07-17 08:45:30 - train: epoch 0120, iter [03300, 05004], lr: 0.000002, loss: 1.1070
2022-07-17 08:46:04 - train: epoch 0120, iter [03400, 05004], lr: 0.000002, loss: 1.1022
2022-07-17 08:46:39 - train: epoch 0120, iter [03500, 05004], lr: 0.000002, loss: 1.2396
2022-07-17 08:47:14 - train: epoch 0120, iter [03600, 05004], lr: 0.000001, loss: 1.2767
2022-07-17 08:47:49 - train: epoch 0120, iter [03700, 05004], lr: 0.000001, loss: 1.0609
2022-07-17 08:48:24 - train: epoch 0120, iter [03800, 05004], lr: 0.000001, loss: 0.9818
2022-07-17 08:48:57 - train: epoch 0120, iter [03900, 05004], lr: 0.000001, loss: 1.2364
2022-07-17 08:49:32 - train: epoch 0120, iter [04000, 05004], lr: 0.000001, loss: 1.1870
2022-07-17 08:50:06 - train: epoch 0120, iter [04100, 05004], lr: 0.000001, loss: 1.2350
2022-07-17 08:50:42 - train: epoch 0120, iter [04200, 05004], lr: 0.000000, loss: 1.1957
2022-07-17 08:51:16 - train: epoch 0120, iter [04300, 05004], lr: 0.000000, loss: 1.1327
2022-07-17 08:51:51 - train: epoch 0120, iter [04400, 05004], lr: 0.000000, loss: 1.1115
2022-07-17 08:52:26 - train: epoch 0120, iter [04500, 05004], lr: 0.000000, loss: 1.2584
2022-07-17 08:53:00 - train: epoch 0120, iter [04600, 05004], lr: 0.000000, loss: 1.2344
2022-07-17 08:53:34 - train: epoch 0120, iter [04700, 05004], lr: 0.000000, loss: 1.3731
2022-07-17 08:54:08 - train: epoch 0120, iter [04800, 05004], lr: 0.000000, loss: 1.3766
2022-07-17 08:54:43 - train: epoch 0120, iter [04900, 05004], lr: 0.000000, loss: 1.2719
2022-07-17 08:55:16 - train: epoch 0120, iter [05000, 05004], lr: 0.000000, loss: 1.0127
2022-07-17 08:55:17 - train: epoch 120, train_loss: 1.1850
2022-07-17 08:56:32 - eval: epoch: 120, acc1: 72.176%, acc5: 90.438%, test_loss: 1.1312, per_image_load_time: 2.114ms, per_image_inference_time: 0.230ms
2022-07-17 08:56:32 - until epoch: 120, best_acc1: 72.176%
2022-07-17 08:56:32 - train done. model: RepVGG_A0, train time: 59.103 hours, best_acc1: 72.176%
