2022-08-04 00:00:27 - network: RegNetX_3_2GF
2022-08-04 00:00:27 - num_classes: 1000
2022-08-04 00:00:27 - input_image_size: 224
2022-08-04 00:00:27 - scale: 1.1428571428571428
2022-08-04 00:00:27 - trained_model_path: 
2022-08-04 00:00:27 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-04 00:00:27 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-08-04 00:00:27 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f65f0f51be0>
2022-08-04 00:00:27 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f65f0f51d90>
2022-08-04 00:00:27 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f65f0f51dc0>
2022-08-04 00:00:27 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f65f0f51e20>
2022-08-04 00:00:27 - seed: 0
2022-08-04 00:00:27 - batch_size: 256
2022-08-04 00:00:27 - num_workers: 16
2022-08-04 00:00:27 - optimizer: ('SGD', {'lr': 0.2, 'momentum': 0.9, 'global_weight_decay': False, 'nesterov': True, 'weight_decay': 5e-05, 'no_weight_decay_layer_name_list': []})
2022-08-04 00:00:27 - scheduler: ('CosineLR', {'warm_up_epochs': 5})
2022-08-04 00:00:27 - epochs: 100
2022-08-04 00:00:27 - print_interval: 100
2022-08-04 00:00:27 - accumulation_steps: 1
2022-08-04 00:00:27 - sync_bn: False
2022-08-04 00:00:27 - apex: True
2022-08-04 00:00:27 - use_ema_model: False
2022-08-04 00:00:27 - ema_model_decay: 0.9999
2022-08-04 00:00:27 - gpus_type: NVIDIA RTX A5000
2022-08-04 00:00:27 - gpus_num: 2
2022-08-04 00:00:27 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f65d2087c70>
2022-08-04 00:00:27 - --------------------parameters--------------------
2022-08-04 00:00:27 - name: conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.0.weight, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.weight, grad: True
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.bias, grad: True
2022-08-04 00:00:27 - name: fc.weight, grad: True
2022-08-04 00:00:27 - name: fc.bias, grad: True
2022-08-04 00:00:27 - --------------------buffers--------------------
2022-08-04 00:00:27 - name: conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.running_mean, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.running_var, grad: False
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.num_batches_tracked, grad: False
2022-08-04 00:00:27 - -----------no weight decay layers--------------
2022-08-04 00:00:27 - name: conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.weight, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.1.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-08-04 00:00:27 - -------------weight decay layers---------------
2022-08-04 00:00:27 - name: conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer1.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer2.5.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.2.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.3.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.4.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.5.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.6.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.7.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.8.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.9.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.10.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.11.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.12.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.13.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer3.14.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.downsample_layer.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.0.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv1.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv2.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: layer4.1.conv3.layer.0.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - name: fc.weight, weight_decay: 5e-05, lr_scale: not setting!
2022-08-04 00:00:27 - epoch 001 lr: 0.200000
2022-08-04 00:02:09 - train: epoch 0001, iter [00100, 05004], lr: 0.040799, loss: 6.9414
2022-08-04 00:03:41 - train: epoch 0001, iter [00200, 05004], lr: 0.041599, loss: 6.7617
2022-08-04 00:05:14 - train: epoch 0001, iter [00300, 05004], lr: 0.042398, loss: 6.5393
2022-08-04 00:06:47 - train: epoch 0001, iter [00400, 05004], lr: 0.043197, loss: 6.4674
2022-08-04 00:08:19 - train: epoch 0001, iter [00500, 05004], lr: 0.043997, loss: 6.3826
2022-08-04 00:09:52 - train: epoch 0001, iter [00600, 05004], lr: 0.044796, loss: 6.1077
2022-08-04 00:11:25 - train: epoch 0001, iter [00700, 05004], lr: 0.045596, loss: 6.1816
2022-08-04 00:12:57 - train: epoch 0001, iter [00800, 05004], lr: 0.046395, loss: 6.1185
2022-08-04 00:14:30 - train: epoch 0001, iter [00900, 05004], lr: 0.047194, loss: 5.9556
2022-08-04 00:16:02 - train: epoch 0001, iter [01000, 05004], lr: 0.047994, loss: 5.9498
2022-08-04 00:17:35 - train: epoch 0001, iter [01100, 05004], lr: 0.048793, loss: 5.8597
2022-08-04 00:19:07 - train: epoch 0001, iter [01200, 05004], lr: 0.049592, loss: 5.6001
2022-08-04 00:20:40 - train: epoch 0001, iter [01300, 05004], lr: 0.050392, loss: 5.6552
2022-08-04 00:22:13 - train: epoch 0001, iter [01400, 05004], lr: 0.051191, loss: 5.5947
2022-08-04 00:23:45 - train: epoch 0001, iter [01500, 05004], lr: 0.051990, loss: 5.4433
2022-08-04 00:25:18 - train: epoch 0001, iter [01600, 05004], lr: 0.052790, loss: 5.4699
2022-08-04 00:26:50 - train: epoch 0001, iter [01700, 05004], lr: 0.053589, loss: 5.3013
2022-08-04 00:28:23 - train: epoch 0001, iter [01800, 05004], lr: 0.054388, loss: 5.4102
2022-08-04 00:29:55 - train: epoch 0001, iter [01900, 05004], lr: 0.055188, loss: 5.3023
2022-08-04 00:31:28 - train: epoch 0001, iter [02000, 05004], lr: 0.055987, loss: 5.0962
2022-08-04 00:33:00 - train: epoch 0001, iter [02100, 05004], lr: 0.056787, loss: 5.1530
2022-08-04 00:34:33 - train: epoch 0001, iter [02200, 05004], lr: 0.057586, loss: 4.9403
2022-08-04 00:36:06 - train: epoch 0001, iter [02300, 05004], lr: 0.058385, loss: 4.9049
2022-08-04 00:37:38 - train: epoch 0001, iter [02400, 05004], lr: 0.059185, loss: 4.9126
2022-08-04 00:39:11 - train: epoch 0001, iter [02500, 05004], lr: 0.059984, loss: 4.9694
2022-08-04 00:40:43 - train: epoch 0001, iter [02600, 05004], lr: 0.060783, loss: 4.9457
2022-08-04 00:42:16 - train: epoch 0001, iter [02700, 05004], lr: 0.061583, loss: 4.8801
2022-08-04 00:43:48 - train: epoch 0001, iter [02800, 05004], lr: 0.062382, loss: 4.7620
2022-08-04 00:45:21 - train: epoch 0001, iter [02900, 05004], lr: 0.063181, loss: 4.7599
2022-08-04 00:46:53 - train: epoch 0001, iter [03000, 05004], lr: 0.063981, loss: 4.9200
2022-08-04 00:48:26 - train: epoch 0001, iter [03100, 05004], lr: 0.064780, loss: 4.9025
2022-08-04 00:49:58 - train: epoch 0001, iter [03200, 05004], lr: 0.065580, loss: 4.8275
2022-08-04 00:51:31 - train: epoch 0001, iter [03300, 05004], lr: 0.066379, loss: 4.5676
2022-08-04 00:53:03 - train: epoch 0001, iter [03400, 05004], lr: 0.067178, loss: 4.5407
2022-08-04 00:54:36 - train: epoch 0001, iter [03500, 05004], lr: 0.067978, loss: 4.4378
2022-08-04 00:56:09 - train: epoch 0001, iter [03600, 05004], lr: 0.068777, loss: 4.2567
2022-08-04 00:57:41 - train: epoch 0001, iter [03700, 05004], lr: 0.069576, loss: 4.4467
2022-08-04 00:59:14 - train: epoch 0001, iter [03800, 05004], lr: 0.070376, loss: 4.3049
2022-08-04 01:00:47 - train: epoch 0001, iter [03900, 05004], lr: 0.071175, loss: 4.3974
2022-08-04 01:02:19 - train: epoch 0001, iter [04000, 05004], lr: 0.071974, loss: 4.3892
2022-08-04 01:03:52 - train: epoch 0001, iter [04100, 05004], lr: 0.072774, loss: 4.3907
2022-08-04 01:05:25 - train: epoch 0001, iter [04200, 05004], lr: 0.073573, loss: 4.2820
2022-08-04 01:06:57 - train: epoch 0001, iter [04300, 05004], lr: 0.074373, loss: 4.1018
2022-08-04 01:08:30 - train: epoch 0001, iter [04400, 05004], lr: 0.075172, loss: 4.0042
2022-08-04 01:10:03 - train: epoch 0001, iter [04500, 05004], lr: 0.075971, loss: 4.2215
2022-08-04 01:11:35 - train: epoch 0001, iter [04600, 05004], lr: 0.076771, loss: 4.4635
2022-08-04 01:13:08 - train: epoch 0001, iter [04700, 05004], lr: 0.077570, loss: 4.1329
2022-08-04 01:14:41 - train: epoch 0001, iter [04800, 05004], lr: 0.078369, loss: 4.3481
2022-08-04 01:16:14 - train: epoch 0001, iter [04900, 05004], lr: 0.079169, loss: 4.1048
2022-08-04 01:17:46 - train: epoch 0001, iter [05000, 05004], lr: 0.079968, loss: 4.0977
2022-08-04 01:17:51 - train: epoch 001, train_loss: 5.0630
2022-08-04 01:20:01 - eval: epoch: 001, acc1: 21.700%, acc5: 45.364%, test_loss: 3.8469, per_image_load_time: 3.762ms, per_image_inference_time: 1.241ms
2022-08-04 01:20:01 - until epoch: 001, best_acc1: 21.700%
2022-08-04 01:20:01 - epoch 002 lr: 0.080008
2022-08-04 01:21:44 - train: epoch 0002, iter [00100, 05004], lr: 0.080799, loss: 3.8462
2022-08-04 01:23:17 - train: epoch 0002, iter [00200, 05004], lr: 0.081599, loss: 3.8127
2022-08-04 01:24:50 - train: epoch 0002, iter [00300, 05004], lr: 0.082398, loss: 4.0213
2022-08-04 01:26:23 - train: epoch 0002, iter [00400, 05004], lr: 0.083197, loss: 3.8869
2022-08-04 01:27:56 - train: epoch 0002, iter [00500, 05004], lr: 0.083997, loss: 3.8240
2022-08-04 01:29:29 - train: epoch 0002, iter [00600, 05004], lr: 0.084796, loss: 3.7139
2022-08-04 01:31:02 - train: epoch 0002, iter [00700, 05004], lr: 0.085596, loss: 4.0396
2022-08-04 01:32:35 - train: epoch 0002, iter [00800, 05004], lr: 0.086395, loss: 3.7368
2022-08-04 01:34:08 - train: epoch 0002, iter [00900, 05004], lr: 0.087194, loss: 3.6121
2022-08-04 01:35:40 - train: epoch 0002, iter [01000, 05004], lr: 0.087994, loss: 3.9012
2022-08-04 01:37:13 - train: epoch 0002, iter [01100, 05004], lr: 0.088793, loss: 3.7806
2022-08-04 01:38:46 - train: epoch 0002, iter [01200, 05004], lr: 0.089592, loss: 3.6894
2022-08-04 01:40:19 - train: epoch 0002, iter [01300, 05004], lr: 0.090392, loss: 3.7441
2022-08-04 01:41:52 - train: epoch 0002, iter [01400, 05004], lr: 0.091191, loss: 3.8013
2022-08-04 01:43:25 - train: epoch 0002, iter [01500, 05004], lr: 0.091990, loss: 3.7606
2022-08-04 01:44:57 - train: epoch 0002, iter [01600, 05004], lr: 0.092790, loss: 3.6597
2022-08-04 01:46:30 - train: epoch 0002, iter [01700, 05004], lr: 0.093589, loss: 3.4879
2022-08-04 01:48:03 - train: epoch 0002, iter [01800, 05004], lr: 0.094388, loss: 3.6622
2022-08-04 01:49:36 - train: epoch 0002, iter [01900, 05004], lr: 0.095188, loss: 3.5509
2022-08-04 01:51:09 - train: epoch 0002, iter [02000, 05004], lr: 0.095987, loss: 3.2917
2022-08-04 01:52:42 - train: epoch 0002, iter [02100, 05004], lr: 0.096787, loss: 3.6182
2022-08-04 01:54:15 - train: epoch 0002, iter [02200, 05004], lr: 0.097586, loss: 3.4447
2022-08-04 01:55:48 - train: epoch 0002, iter [02300, 05004], lr: 0.098385, loss: 3.5498
2022-08-04 01:57:21 - train: epoch 0002, iter [02400, 05004], lr: 0.099185, loss: 3.4052
2022-08-04 01:58:54 - train: epoch 0002, iter [02500, 05004], lr: 0.099984, loss: 3.2250
2022-08-04 02:00:27 - train: epoch 0002, iter [02600, 05004], lr: 0.100783, loss: 3.5013
2022-08-04 02:02:00 - train: epoch 0002, iter [02700, 05004], lr: 0.101583, loss: 3.5027
2022-08-04 02:03:33 - train: epoch 0002, iter [02800, 05004], lr: 0.102382, loss: 3.5678
2022-08-04 02:05:05 - train: epoch 0002, iter [02900, 05004], lr: 0.103181, loss: 3.6211
2022-08-04 02:06:38 - train: epoch 0002, iter [03000, 05004], lr: 0.103981, loss: 3.2567
2022-08-04 02:08:11 - train: epoch 0002, iter [03100, 05004], lr: 0.104780, loss: 3.3206
2022-08-04 02:09:44 - train: epoch 0002, iter [03200, 05004], lr: 0.105580, loss: 3.3968
2022-08-04 02:11:16 - train: epoch 0002, iter [03300, 05004], lr: 0.106379, loss: 3.5032
2022-08-04 02:12:49 - train: epoch 0002, iter [03400, 05004], lr: 0.107178, loss: 3.1902
2022-08-04 02:14:22 - train: epoch 0002, iter [03500, 05004], lr: 0.107978, loss: 3.3260
2022-08-04 02:15:55 - train: epoch 0002, iter [03600, 05004], lr: 0.108777, loss: 3.3599
2022-08-04 02:17:27 - train: epoch 0002, iter [03700, 05004], lr: 0.109576, loss: 3.4191
2022-08-04 02:19:00 - train: epoch 0002, iter [03800, 05004], lr: 0.110376, loss: 3.1211
2022-08-04 02:20:33 - train: epoch 0002, iter [03900, 05004], lr: 0.111175, loss: 3.1306
2022-08-04 02:22:06 - train: epoch 0002, iter [04000, 05004], lr: 0.111974, loss: 3.2799
2022-08-04 02:23:39 - train: epoch 0002, iter [04100, 05004], lr: 0.112774, loss: 3.3977
2022-08-04 02:25:12 - train: epoch 0002, iter [04200, 05004], lr: 0.113573, loss: 3.2412
2022-08-04 02:26:44 - train: epoch 0002, iter [04300, 05004], lr: 0.114373, loss: 3.2401
2022-08-04 02:28:17 - train: epoch 0002, iter [04400, 05004], lr: 0.115172, loss: 3.2510
2022-08-04 02:29:50 - train: epoch 0002, iter [04500, 05004], lr: 0.115971, loss: 2.9198
2022-08-04 02:31:22 - train: epoch 0002, iter [04600, 05004], lr: 0.116771, loss: 3.0222
2022-08-04 02:32:55 - train: epoch 0002, iter [04700, 05004], lr: 0.117570, loss: 3.2359
2022-08-04 02:34:28 - train: epoch 0002, iter [04800, 05004], lr: 0.118369, loss: 3.2791
2022-08-04 02:36:00 - train: epoch 0002, iter [04900, 05004], lr: 0.119169, loss: 3.0768
2022-08-04 02:37:33 - train: epoch 0002, iter [05000, 05004], lr: 0.119968, loss: 3.1544
2022-08-04 02:37:37 - train: epoch 002, train_loss: 3.4921
2022-08-04 02:39:41 - eval: epoch: 002, acc1: 36.096%, acc5: 62.954%, test_loss: 2.9329, per_image_load_time: 2.975ms, per_image_inference_time: 1.241ms
2022-08-04 02:39:42 - until epoch: 002, best_acc1: 36.096%
2022-08-04 02:39:42 - epoch 003 lr: 0.120008
2022-08-04 02:41:23 - train: epoch 0003, iter [00100, 05004], lr: 0.120799, loss: 3.2633
2022-08-04 02:42:56 - train: epoch 0003, iter [00200, 05004], lr: 0.121599, loss: 3.3475
2022-08-04 02:44:28 - train: epoch 0003, iter [00300, 05004], lr: 0.122398, loss: 3.1928
2022-08-04 02:46:01 - train: epoch 0003, iter [00400, 05004], lr: 0.123197, loss: 3.1233
2022-08-04 02:47:33 - train: epoch 0003, iter [00500, 05004], lr: 0.123997, loss: 3.2248
2022-08-04 02:49:05 - train: epoch 0003, iter [00600, 05004], lr: 0.124796, loss: 2.9332
2022-08-04 02:50:38 - train: epoch 0003, iter [00700, 05004], lr: 0.125596, loss: 3.1918
2022-08-04 02:52:10 - train: epoch 0003, iter [00800, 05004], lr: 0.126395, loss: 3.1068
2022-08-04 02:53:43 - train: epoch 0003, iter [00900, 05004], lr: 0.127194, loss: 2.9776
2022-08-04 02:55:15 - train: epoch 0003, iter [01000, 05004], lr: 0.127994, loss: 3.1460
2022-08-04 02:56:48 - train: epoch 0003, iter [01100, 05004], lr: 0.128793, loss: 3.0597
2022-08-04 02:58:20 - train: epoch 0003, iter [01200, 05004], lr: 0.129592, loss: 3.0688
2022-08-04 02:59:53 - train: epoch 0003, iter [01300, 05004], lr: 0.130392, loss: 3.0389
2022-08-04 03:01:25 - train: epoch 0003, iter [01400, 05004], lr: 0.131191, loss: 2.9054
2022-08-04 03:02:57 - train: epoch 0003, iter [01500, 05004], lr: 0.131990, loss: 3.2721
2022-08-04 03:04:30 - train: epoch 0003, iter [01600, 05004], lr: 0.132790, loss: 2.9301
2022-08-04 03:06:02 - train: epoch 0003, iter [01700, 05004], lr: 0.133589, loss: 3.0390
2022-08-04 03:07:35 - train: epoch 0003, iter [01800, 05004], lr: 0.134388, loss: 3.0573
2022-08-04 03:09:07 - train: epoch 0003, iter [01900, 05004], lr: 0.135188, loss: 3.0349
2022-08-04 03:10:40 - train: epoch 0003, iter [02000, 05004], lr: 0.135987, loss: 2.9596
2022-08-04 03:12:12 - train: epoch 0003, iter [02100, 05004], lr: 0.136787, loss: 2.9959
2022-08-04 03:13:45 - train: epoch 0003, iter [02200, 05004], lr: 0.137586, loss: 3.1552
2022-08-04 03:15:17 - train: epoch 0003, iter [02300, 05004], lr: 0.138385, loss: 2.8406
2022-08-04 03:16:50 - train: epoch 0003, iter [02400, 05004], lr: 0.139185, loss: 2.8408
2022-08-04 03:18:22 - train: epoch 0003, iter [02500, 05004], lr: 0.139984, loss: 2.9958
2022-08-04 03:19:55 - train: epoch 0003, iter [02600, 05004], lr: 0.140783, loss: 3.0313
2022-08-04 03:21:27 - train: epoch 0003, iter [02700, 05004], lr: 0.141583, loss: 3.2652
2022-08-04 03:23:00 - train: epoch 0003, iter [02800, 05004], lr: 0.142382, loss: 2.8626
2022-08-04 03:24:32 - train: epoch 0003, iter [02900, 05004], lr: 0.143181, loss: 2.8891
2022-08-04 03:26:05 - train: epoch 0003, iter [03000, 05004], lr: 0.143981, loss: 2.9993
2022-08-04 03:27:37 - train: epoch 0003, iter [03100, 05004], lr: 0.144780, loss: 3.2266
2022-08-04 03:29:10 - train: epoch 0003, iter [03200, 05004], lr: 0.145580, loss: 3.1414
2022-08-04 03:30:42 - train: epoch 0003, iter [03300, 05004], lr: 0.146379, loss: 2.9883
2022-08-04 03:32:15 - train: epoch 0003, iter [03400, 05004], lr: 0.147178, loss: 3.0778
2022-08-04 03:33:47 - train: epoch 0003, iter [03500, 05004], lr: 0.147978, loss: 2.7605
2022-08-04 03:35:20 - train: epoch 0003, iter [03600, 05004], lr: 0.148777, loss: 2.7320
2022-08-04 03:36:52 - train: epoch 0003, iter [03700, 05004], lr: 0.149576, loss: 2.9962
2022-08-04 03:38:25 - train: epoch 0003, iter [03800, 05004], lr: 0.150376, loss: 3.1651
2022-08-04 03:39:57 - train: epoch 0003, iter [03900, 05004], lr: 0.151175, loss: 3.1488
2022-08-04 03:41:30 - train: epoch 0003, iter [04000, 05004], lr: 0.151974, loss: 2.5369
2022-08-04 03:43:03 - train: epoch 0003, iter [04100, 05004], lr: 0.152774, loss: 2.8020
2022-08-04 03:44:35 - train: epoch 0003, iter [04200, 05004], lr: 0.153573, loss: 2.8073
2022-08-04 03:46:08 - train: epoch 0003, iter [04300, 05004], lr: 0.154373, loss: 2.8546
2022-08-04 03:47:41 - train: epoch 0003, iter [04400, 05004], lr: 0.155172, loss: 2.6503
2022-08-04 03:49:13 - train: epoch 0003, iter [04500, 05004], lr: 0.155971, loss: 2.8981
2022-08-04 03:50:46 - train: epoch 0003, iter [04600, 05004], lr: 0.156771, loss: 2.9347
2022-08-04 03:52:18 - train: epoch 0003, iter [04700, 05004], lr: 0.157570, loss: 2.8050
2022-08-04 03:53:51 - train: epoch 0003, iter [04800, 05004], lr: 0.158369, loss: 2.9865
2022-08-04 03:55:24 - train: epoch 0003, iter [04900, 05004], lr: 0.159169, loss: 3.0898
2022-08-04 03:56:56 - train: epoch 0003, iter [05000, 05004], lr: 0.159968, loss: 2.8242
2022-08-04 03:57:01 - train: epoch 003, train_loss: 2.9665
2022-08-04 03:59:04 - eval: epoch: 003, acc1: 41.834%, acc5: 68.670%, test_loss: 2.6014, per_image_load_time: 3.509ms, per_image_inference_time: 1.204ms
2022-08-04 03:59:05 - until epoch: 003, best_acc1: 41.834%
2022-08-04 03:59:05 - epoch 004 lr: 0.160008
2022-08-04 04:00:46 - train: epoch 0004, iter [00100, 05004], lr: 0.160799, loss: 2.6802
2022-08-04 04:02:18 - train: epoch 0004, iter [00200, 05004], lr: 0.161599, loss: 2.6942
2022-08-04 04:03:50 - train: epoch 0004, iter [00300, 05004], lr: 0.162398, loss: 2.6878
2022-08-04 04:05:23 - train: epoch 0004, iter [00400, 05004], lr: 0.163197, loss: 2.7579
2022-08-04 04:06:55 - train: epoch 0004, iter [00500, 05004], lr: 0.163997, loss: 2.6099
2022-08-04 04:08:27 - train: epoch 0004, iter [00600, 05004], lr: 0.164796, loss: 2.9993
2022-08-04 04:10:00 - train: epoch 0004, iter [00700, 05004], lr: 0.165596, loss: 2.8685
2022-08-04 04:11:32 - train: epoch 0004, iter [00800, 05004], lr: 0.166395, loss: 2.7952
2022-08-04 04:13:04 - train: epoch 0004, iter [00900, 05004], lr: 0.167194, loss: 2.4736
2022-08-04 04:14:37 - train: epoch 0004, iter [01000, 05004], lr: 0.167994, loss: 2.8342
2022-08-04 04:16:09 - train: epoch 0004, iter [01100, 05004], lr: 0.168793, loss: 3.0742
2022-08-04 04:17:42 - train: epoch 0004, iter [01200, 05004], lr: 0.169592, loss: 2.6767
2022-08-04 04:19:14 - train: epoch 0004, iter [01300, 05004], lr: 0.170392, loss: 2.4923
2022-08-04 04:20:47 - train: epoch 0004, iter [01400, 05004], lr: 0.171191, loss: 2.7779
2022-08-04 04:22:19 - train: epoch 0004, iter [01500, 05004], lr: 0.171990, loss: 2.8471
2022-08-04 04:23:52 - train: epoch 0004, iter [01600, 05004], lr: 0.172790, loss: 2.8629
2022-08-04 04:25:24 - train: epoch 0004, iter [01700, 05004], lr: 0.173589, loss: 2.6876
2022-08-04 04:26:57 - train: epoch 0004, iter [01800, 05004], lr: 0.174388, loss: 2.8848
2022-08-04 04:28:29 - train: epoch 0004, iter [01900, 05004], lr: 0.175188, loss: 2.7892
2022-08-04 04:30:02 - train: epoch 0004, iter [02000, 05004], lr: 0.175987, loss: 2.5516
2022-08-04 04:31:34 - train: epoch 0004, iter [02100, 05004], lr: 0.176787, loss: 2.7839
2022-08-04 04:33:07 - train: epoch 0004, iter [02200, 05004], lr: 0.177586, loss: 2.6753
2022-08-04 04:34:39 - train: epoch 0004, iter [02300, 05004], lr: 0.178385, loss: 2.4452
2022-08-04 04:36:11 - train: epoch 0004, iter [02400, 05004], lr: 0.179185, loss: 2.5528
2022-08-04 04:37:44 - train: epoch 0004, iter [02500, 05004], lr: 0.179984, loss: 2.7142
2022-08-04 04:39:16 - train: epoch 0004, iter [02600, 05004], lr: 0.180783, loss: 2.6569
2022-08-04 04:40:49 - train: epoch 0004, iter [02700, 05004], lr: 0.181583, loss: 2.6515
2022-08-04 04:42:21 - train: epoch 0004, iter [02800, 05004], lr: 0.182382, loss: 2.7742
2022-08-04 04:43:54 - train: epoch 0004, iter [02900, 05004], lr: 0.183181, loss: 2.7759
2022-08-04 04:45:26 - train: epoch 0004, iter [03000, 05004], lr: 0.183981, loss: 2.6295
2022-08-04 04:46:58 - train: epoch 0004, iter [03100, 05004], lr: 0.184780, loss: 2.7262
2022-08-04 04:48:31 - train: epoch 0004, iter [03200, 05004], lr: 0.185580, loss: 2.6072
2022-08-04 04:50:04 - train: epoch 0004, iter [03300, 05004], lr: 0.186379, loss: 2.8346
2022-08-04 04:51:36 - train: epoch 0004, iter [03400, 05004], lr: 0.187178, loss: 2.7450
2022-08-04 04:53:09 - train: epoch 0004, iter [03500, 05004], lr: 0.187978, loss: 2.7266
2022-08-04 04:54:41 - train: epoch 0004, iter [03600, 05004], lr: 0.188777, loss: 2.6344
2022-08-04 04:56:14 - train: epoch 0004, iter [03700, 05004], lr: 0.189576, loss: 2.7633
2022-08-04 04:57:46 - train: epoch 0004, iter [03800, 05004], lr: 0.190376, loss: 2.4158
2022-08-04 04:59:18 - train: epoch 0004, iter [03900, 05004], lr: 0.191175, loss: 2.5890
2022-08-04 05:00:51 - train: epoch 0004, iter [04000, 05004], lr: 0.191974, loss: 2.3972
2022-08-04 05:02:23 - train: epoch 0004, iter [04100, 05004], lr: 0.192774, loss: 2.6343
2022-08-04 05:03:56 - train: epoch 0004, iter [04200, 05004], lr: 0.193573, loss: 2.6268
2022-08-04 05:05:28 - train: epoch 0004, iter [04300, 05004], lr: 0.194373, loss: 2.6352
2022-08-04 05:07:01 - train: epoch 0004, iter [04400, 05004], lr: 0.195172, loss: 2.5116
2022-08-04 05:08:33 - train: epoch 0004, iter [04500, 05004], lr: 0.195971, loss: 2.2907
2022-08-04 05:10:06 - train: epoch 0004, iter [04600, 05004], lr: 0.196771, loss: 2.7784
2022-08-04 05:11:38 - train: epoch 0004, iter [04700, 05004], lr: 0.197570, loss: 2.6423
2022-08-04 05:13:11 - train: epoch 0004, iter [04800, 05004], lr: 0.198369, loss: 2.4763
2022-08-04 05:14:43 - train: epoch 0004, iter [04900, 05004], lr: 0.199169, loss: 2.5887
2022-08-04 05:16:16 - train: epoch 0004, iter [05000, 05004], lr: 0.199968, loss: 2.6249
2022-08-04 05:16:20 - train: epoch 004, train_loss: 2.7164
2022-08-04 05:18:26 - eval: epoch: 004, acc1: 46.304%, acc5: 72.744%, test_loss: 2.3515, per_image_load_time: 3.592ms, per_image_inference_time: 1.223ms
2022-08-04 05:18:26 - until epoch: 004, best_acc1: 46.304%
2022-08-04 05:18:26 - epoch 005 lr: 0.200008
2022-08-04 05:20:08 - train: epoch 0005, iter [00100, 05004], lr: 0.200799, loss: 2.7197
2022-08-04 05:21:40 - train: epoch 0005, iter [00200, 05004], lr: 0.201599, loss: 2.5475
2022-08-04 05:23:12 - train: epoch 0005, iter [00300, 05004], lr: 0.202398, loss: 2.6351
2022-08-04 05:24:44 - train: epoch 0005, iter [00400, 05004], lr: 0.203197, loss: 2.5494
2022-08-04 05:26:17 - train: epoch 0005, iter [00500, 05004], lr: 0.203997, loss: 2.5345
2022-08-04 05:27:49 - train: epoch 0005, iter [00600, 05004], lr: 0.204796, loss: 2.5823
2022-08-04 05:29:21 - train: epoch 0005, iter [00700, 05004], lr: 0.205596, loss: 2.6230
2022-08-04 05:30:53 - train: epoch 0005, iter [00800, 05004], lr: 0.206395, loss: 2.7611
2022-08-04 05:32:25 - train: epoch 0005, iter [00900, 05004], lr: 0.207194, loss: 2.5660
2022-08-04 05:33:57 - train: epoch 0005, iter [01000, 05004], lr: 0.207994, loss: 2.5307
2022-08-04 05:35:30 - train: epoch 0005, iter [01100, 05004], lr: 0.208793, loss: 2.5939
2022-08-04 05:37:02 - train: epoch 0005, iter [01200, 05004], lr: 0.209592, loss: 2.6311
2022-08-04 05:38:34 - train: epoch 0005, iter [01300, 05004], lr: 0.210392, loss: 2.3634
2022-08-04 05:40:07 - train: epoch 0005, iter [01400, 05004], lr: 0.211191, loss: 2.6205
2022-08-04 05:41:39 - train: epoch 0005, iter [01500, 05004], lr: 0.211990, loss: 2.4360
2022-08-04 05:43:11 - train: epoch 0005, iter [01600, 05004], lr: 0.212790, loss: 2.4084
2022-08-04 05:44:43 - train: epoch 0005, iter [01700, 05004], lr: 0.213589, loss: 2.6043
2022-08-04 05:46:15 - train: epoch 0005, iter [01800, 05004], lr: 0.214388, loss: 2.6301
2022-08-04 05:47:48 - train: epoch 0005, iter [01900, 05004], lr: 0.215188, loss: 2.5130
2022-08-04 05:49:20 - train: epoch 0005, iter [02000, 05004], lr: 0.215987, loss: 2.6300
2022-08-04 05:50:52 - train: epoch 0005, iter [02100, 05004], lr: 0.216787, loss: 2.4406
2022-08-04 05:52:24 - train: epoch 0005, iter [02200, 05004], lr: 0.217586, loss: 2.5358
2022-08-04 05:53:57 - train: epoch 0005, iter [02300, 05004], lr: 0.218385, loss: 2.5847
2022-08-04 05:55:29 - train: epoch 0005, iter [02400, 05004], lr: 0.219185, loss: 2.5696
2022-08-04 05:57:01 - train: epoch 0005, iter [02500, 05004], lr: 0.219984, loss: 2.6991
2022-08-04 05:58:33 - train: epoch 0005, iter [02600, 05004], lr: 0.220783, loss: 2.6116
2022-08-04 06:00:06 - train: epoch 0005, iter [02700, 05004], lr: 0.221583, loss: 2.6317
2022-08-04 06:01:38 - train: epoch 0005, iter [02800, 05004], lr: 0.222382, loss: 2.4007
2022-08-04 06:03:10 - train: epoch 0005, iter [02900, 05004], lr: 0.223181, loss: 2.4125
2022-08-04 06:04:43 - train: epoch 0005, iter [03000, 05004], lr: 0.223981, loss: 2.3466
2022-08-04 06:06:15 - train: epoch 0005, iter [03100, 05004], lr: 0.224780, loss: 2.5205
2022-08-04 06:07:48 - train: epoch 0005, iter [03200, 05004], lr: 0.225580, loss: 2.6853
2022-08-04 06:09:20 - train: epoch 0005, iter [03300, 05004], lr: 0.226379, loss: 2.5269
2022-08-04 06:10:53 - train: epoch 0005, iter [03400, 05004], lr: 0.227178, loss: 2.5322
2022-08-04 06:12:25 - train: epoch 0005, iter [03500, 05004], lr: 0.227978, loss: 2.6810
2022-08-04 06:13:57 - train: epoch 0005, iter [03600, 05004], lr: 0.228777, loss: 2.4539
2022-08-04 06:15:30 - train: epoch 0005, iter [03700, 05004], lr: 0.229576, loss: 2.3717
2022-08-04 06:17:02 - train: epoch 0005, iter [03800, 05004], lr: 0.230376, loss: 2.4694
2022-08-04 06:18:34 - train: epoch 0005, iter [03900, 05004], lr: 0.231175, loss: 2.6316
2022-08-04 06:20:07 - train: epoch 0005, iter [04000, 05004], lr: 0.231974, loss: 2.4965
2022-08-04 06:21:39 - train: epoch 0005, iter [04100, 05004], lr: 0.232774, loss: 2.5648
2022-08-04 06:23:11 - train: epoch 0005, iter [04200, 05004], lr: 0.233573, loss: 2.6021
2022-08-04 06:24:44 - train: epoch 0005, iter [04300, 05004], lr: 0.234373, loss: 2.5425
2022-08-04 06:26:16 - train: epoch 0005, iter [04400, 05004], lr: 0.235172, loss: 2.6769
2022-08-04 06:27:48 - train: epoch 0005, iter [04500, 05004], lr: 0.235971, loss: 2.5592
2022-08-04 06:29:21 - train: epoch 0005, iter [04600, 05004], lr: 0.236771, loss: 2.5662
2022-08-04 06:30:53 - train: epoch 0005, iter [04700, 05004], lr: 0.237570, loss: 2.4589
2022-08-04 06:32:25 - train: epoch 0005, iter [04800, 05004], lr: 0.238369, loss: 2.3430
2022-08-04 06:33:58 - train: epoch 0005, iter [04900, 05004], lr: 0.239169, loss: 2.7038
2022-08-04 06:35:30 - train: epoch 0005, iter [05000, 05004], lr: 0.239968, loss: 2.4660
2022-08-04 06:35:35 - train: epoch 005, train_loss: 2.5897
2022-08-04 06:37:38 - eval: epoch: 005, acc1: 47.576%, acc5: 74.102%, test_loss: 2.2913, per_image_load_time: 2.764ms, per_image_inference_time: 1.229ms
2022-08-04 06:37:39 - until epoch: 005, best_acc1: 47.576%
2022-08-04 06:37:39 - epoch 006 lr: 0.200000
2022-08-04 06:39:20 - train: epoch 0006, iter [00100, 05004], lr: 0.200000, loss: 2.5260
2022-08-04 06:40:52 - train: epoch 0006, iter [00200, 05004], lr: 0.200000, loss: 2.3698
2022-08-04 06:42:25 - train: epoch 0006, iter [00300, 05004], lr: 0.200000, loss: 2.4552
2022-08-04 06:43:57 - train: epoch 0006, iter [00400, 05004], lr: 0.200000, loss: 2.4591
2022-08-04 06:45:29 - train: epoch 0006, iter [00500, 05004], lr: 0.199999, loss: 2.1899
2022-08-04 06:47:02 - train: epoch 0006, iter [00600, 05004], lr: 0.199999, loss: 2.6380
2022-08-04 06:48:35 - train: epoch 0006, iter [00700, 05004], lr: 0.199999, loss: 2.4579
2022-08-04 06:50:07 - train: epoch 0006, iter [00800, 05004], lr: 0.199999, loss: 2.5426
2022-08-04 06:51:39 - train: epoch 0006, iter [00900, 05004], lr: 0.199998, loss: 2.3127
2022-08-04 06:53:12 - train: epoch 0006, iter [01000, 05004], lr: 0.199998, loss: 2.2618
2022-08-04 06:54:44 - train: epoch 0006, iter [01100, 05004], lr: 0.199997, loss: 2.3596
2022-08-04 06:56:17 - train: epoch 0006, iter [01200, 05004], lr: 0.199997, loss: 2.4473
2022-08-04 06:57:49 - train: epoch 0006, iter [01300, 05004], lr: 0.199996, loss: 2.3763
2022-08-04 06:59:22 - train: epoch 0006, iter [01400, 05004], lr: 0.199996, loss: 2.4617
2022-08-04 07:00:54 - train: epoch 0006, iter [01500, 05004], lr: 0.199995, loss: 2.6121
2022-08-04 07:02:27 - train: epoch 0006, iter [01600, 05004], lr: 0.199994, loss: 2.2664
2022-08-04 07:03:59 - train: epoch 0006, iter [01700, 05004], lr: 0.199994, loss: 2.5073
2022-08-04 07:05:32 - train: epoch 0006, iter [01800, 05004], lr: 0.199993, loss: 2.5034
2022-08-04 07:07:04 - train: epoch 0006, iter [01900, 05004], lr: 0.199992, loss: 2.4377
2022-08-04 07:08:37 - train: epoch 0006, iter [02000, 05004], lr: 0.199991, loss: 2.5120
2022-08-04 07:10:09 - train: epoch 0006, iter [02100, 05004], lr: 0.199990, loss: 2.3193
2022-08-04 07:11:42 - train: epoch 0006, iter [02200, 05004], lr: 0.199989, loss: 2.2682
2022-08-04 07:13:14 - train: epoch 0006, iter [02300, 05004], lr: 0.199988, loss: 2.3760
2022-08-04 07:14:47 - train: epoch 0006, iter [02400, 05004], lr: 0.199987, loss: 2.4205
2022-08-04 07:16:19 - train: epoch 0006, iter [02500, 05004], lr: 0.199986, loss: 2.2733
2022-08-04 07:17:52 - train: epoch 0006, iter [02600, 05004], lr: 0.199985, loss: 2.2319
2022-08-04 07:19:25 - train: epoch 0006, iter [02700, 05004], lr: 0.199984, loss: 2.6215
2022-08-04 07:20:57 - train: epoch 0006, iter [02800, 05004], lr: 0.199983, loss: 2.0907
2022-08-04 07:22:30 - train: epoch 0006, iter [02900, 05004], lr: 0.199982, loss: 2.5173
2022-08-04 07:24:02 - train: epoch 0006, iter [03000, 05004], lr: 0.199980, loss: 2.3843
2022-08-04 07:25:35 - train: epoch 0006, iter [03100, 05004], lr: 0.199979, loss: 2.2883
2022-08-04 07:27:07 - train: epoch 0006, iter [03200, 05004], lr: 0.199978, loss: 2.4035
2022-08-04 07:28:40 - train: epoch 0006, iter [03300, 05004], lr: 0.199976, loss: 2.3566
2022-08-04 07:30:12 - train: epoch 0006, iter [03400, 05004], lr: 0.199975, loss: 2.4851
2022-08-04 07:31:45 - train: epoch 0006, iter [03500, 05004], lr: 0.199973, loss: 2.5665
2022-08-04 07:33:17 - train: epoch 0006, iter [03600, 05004], lr: 0.199972, loss: 2.3990
2022-08-04 07:34:50 - train: epoch 0006, iter [03700, 05004], lr: 0.199970, loss: 2.2270
2022-08-04 07:36:22 - train: epoch 0006, iter [03800, 05004], lr: 0.199968, loss: 2.1705
2022-08-04 07:37:55 - train: epoch 0006, iter [03900, 05004], lr: 0.199967, loss: 2.4250
2022-08-04 07:39:27 - train: epoch 0006, iter [04000, 05004], lr: 0.199965, loss: 2.5124
2022-08-04 07:41:00 - train: epoch 0006, iter [04100, 05004], lr: 0.199963, loss: 2.3610
2022-08-04 07:42:32 - train: epoch 0006, iter [04200, 05004], lr: 0.199961, loss: 2.1534
2022-08-04 07:44:05 - train: epoch 0006, iter [04300, 05004], lr: 0.199960, loss: 2.3675
2022-08-04 07:45:37 - train: epoch 0006, iter [04400, 05004], lr: 0.199958, loss: 2.4226
2022-08-04 07:47:09 - train: epoch 0006, iter [04500, 05004], lr: 0.199956, loss: 2.4701
2022-08-04 07:48:42 - train: epoch 0006, iter [04600, 05004], lr: 0.199954, loss: 2.4883
2022-08-04 07:50:15 - train: epoch 0006, iter [04700, 05004], lr: 0.199952, loss: 2.4087
2022-08-04 07:51:47 - train: epoch 0006, iter [04800, 05004], lr: 0.199950, loss: 2.4966
2022-08-04 07:53:19 - train: epoch 0006, iter [04900, 05004], lr: 0.199948, loss: 2.1579
2022-08-04 07:54:52 - train: epoch 0006, iter [05000, 05004], lr: 0.199945, loss: 2.1781
2022-08-04 07:54:56 - train: epoch 006, train_loss: 2.3997
2022-08-04 07:57:00 - eval: epoch: 006, acc1: 50.218%, acc5: 75.678%, test_loss: 2.1697, per_image_load_time: 3.488ms, per_image_inference_time: 1.174ms
2022-08-04 07:57:01 - until epoch: 006, best_acc1: 50.218%
2022-08-04 07:57:01 - epoch 007 lr: 0.199945
2022-08-04 07:58:42 - train: epoch 0007, iter [00100, 05004], lr: 0.199943, loss: 2.2427
2022-08-04 08:00:15 - train: epoch 0007, iter [00200, 05004], lr: 0.199941, loss: 2.4566
2022-08-04 08:01:47 - train: epoch 0007, iter [00300, 05004], lr: 0.199939, loss: 2.5398
2022-08-04 08:03:20 - train: epoch 0007, iter [00400, 05004], lr: 0.199936, loss: 2.4113
2022-08-04 08:04:52 - train: epoch 0007, iter [00500, 05004], lr: 0.199934, loss: 2.0623
2022-08-04 08:06:24 - train: epoch 0007, iter [00600, 05004], lr: 0.199931, loss: 2.3282
2022-08-04 08:07:57 - train: epoch 0007, iter [00700, 05004], lr: 0.199929, loss: 2.3119
2022-08-04 08:09:29 - train: epoch 0007, iter [00800, 05004], lr: 0.199926, loss: 2.4004
2022-08-04 08:11:01 - train: epoch 0007, iter [00900, 05004], lr: 0.199924, loss: 2.3923
2022-08-04 08:12:34 - train: epoch 0007, iter [01000, 05004], lr: 0.199921, loss: 2.4671
2022-08-04 08:14:06 - train: epoch 0007, iter [01100, 05004], lr: 0.199919, loss: 2.2961
2022-08-04 08:15:39 - train: epoch 0007, iter [01200, 05004], lr: 0.199916, loss: 2.3299
2022-08-04 08:17:12 - train: epoch 0007, iter [01300, 05004], lr: 0.199913, loss: 2.1160
2022-08-04 08:18:44 - train: epoch 0007, iter [01400, 05004], lr: 0.199910, loss: 2.2577
2022-08-04 08:20:17 - train: epoch 0007, iter [01500, 05004], lr: 0.199908, loss: 2.4404
2022-08-04 08:21:49 - train: epoch 0007, iter [01600, 05004], lr: 0.199905, loss: 2.2521
2022-08-04 08:23:22 - train: epoch 0007, iter [01700, 05004], lr: 0.199902, loss: 2.4643
2022-08-04 08:24:55 - train: epoch 0007, iter [01800, 05004], lr: 0.199899, loss: 2.2874
2022-08-04 08:26:27 - train: epoch 0007, iter [01900, 05004], lr: 0.199896, loss: 2.5787
2022-08-04 08:27:59 - train: epoch 0007, iter [02000, 05004], lr: 0.199893, loss: 2.1401
2022-08-04 08:29:32 - train: epoch 0007, iter [02100, 05004], lr: 0.199890, loss: 2.6407
2022-08-04 08:31:04 - train: epoch 0007, iter [02200, 05004], lr: 0.199887, loss: 2.3751
2022-08-04 08:32:36 - train: epoch 0007, iter [02300, 05004], lr: 0.199884, loss: 2.3369
2022-08-04 08:34:09 - train: epoch 0007, iter [02400, 05004], lr: 0.199880, loss: 2.5157
2022-08-04 08:35:41 - train: epoch 0007, iter [02500, 05004], lr: 0.199877, loss: 2.4571
2022-08-04 08:37:14 - train: epoch 0007, iter [02600, 05004], lr: 0.199874, loss: 2.2557
2022-08-04 08:38:46 - train: epoch 0007, iter [02700, 05004], lr: 0.199870, loss: 2.2953
2022-08-04 08:40:18 - train: epoch 0007, iter [02800, 05004], lr: 0.199867, loss: 2.2403
2022-08-04 08:41:51 - train: epoch 0007, iter [02900, 05004], lr: 0.199864, loss: 2.1240
2022-08-04 08:43:23 - train: epoch 0007, iter [03000, 05004], lr: 0.199860, loss: 2.2855
2022-08-04 08:44:56 - train: epoch 0007, iter [03100, 05004], lr: 0.199857, loss: 2.3143
2022-08-04 08:46:28 - train: epoch 0007, iter [03200, 05004], lr: 0.199853, loss: 2.2137
2022-08-04 08:48:01 - train: epoch 0007, iter [03300, 05004], lr: 0.199849, loss: 2.4908
2022-08-04 08:49:33 - train: epoch 0007, iter [03400, 05004], lr: 0.199846, loss: 2.2852
2022-08-04 08:51:05 - train: epoch 0007, iter [03500, 05004], lr: 0.199842, loss: 2.3267
2022-08-04 08:52:38 - train: epoch 0007, iter [03600, 05004], lr: 0.199838, loss: 2.0908
2022-08-04 08:54:10 - train: epoch 0007, iter [03700, 05004], lr: 0.199835, loss: 2.1386
2022-08-04 08:55:43 - train: epoch 0007, iter [03800, 05004], lr: 0.199831, loss: 2.3921
2022-08-04 08:57:15 - train: epoch 0007, iter [03900, 05004], lr: 0.199827, loss: 2.2773
2022-08-04 08:58:48 - train: epoch 0007, iter [04000, 05004], lr: 0.199823, loss: 2.4734
2022-08-04 09:00:20 - train: epoch 0007, iter [04100, 05004], lr: 0.199819, loss: 2.1929
2022-08-04 09:01:53 - train: epoch 0007, iter [04200, 05004], lr: 0.199815, loss: 2.2098
2022-08-04 09:03:25 - train: epoch 0007, iter [04300, 05004], lr: 0.199811, loss: 2.4427
2022-08-04 09:04:58 - train: epoch 0007, iter [04400, 05004], lr: 0.199807, loss: 2.1265
2022-08-04 09:06:30 - train: epoch 0007, iter [04500, 05004], lr: 0.199803, loss: 2.5416
2022-08-04 09:08:03 - train: epoch 0007, iter [04600, 05004], lr: 0.199799, loss: 2.5033
2022-08-04 09:09:41 - train: epoch 0007, iter [04700, 05004], lr: 0.199794, loss: 2.1357
2022-08-04 09:11:24 - train: epoch 0007, iter [04800, 05004], lr: 0.199790, loss: 2.5125
2022-08-04 09:13:10 - train: epoch 0007, iter [04900, 05004], lr: 0.199786, loss: 2.3319
2022-08-04 09:14:55 - train: epoch 0007, iter [05000, 05004], lr: 0.199782, loss: 2.1813
2022-08-04 09:15:00 - train: epoch 007, train_loss: 2.3210
2022-08-04 09:18:00 - eval: epoch: 007, acc1: 52.498%, acc5: 78.096%, test_loss: 2.0359, per_image_load_time: 4.895ms, per_image_inference_time: 1.331ms
2022-08-04 09:18:01 - until epoch: 007, best_acc1: 52.498%
2022-08-04 09:18:01 - epoch 008 lr: 0.199781
2022-08-04 09:20:03 - train: epoch 0008, iter [00100, 05004], lr: 0.199777, loss: 2.2945
2022-08-04 09:21:50 - train: epoch 0008, iter [00200, 05004], lr: 0.199773, loss: 2.2064
2022-08-04 09:23:35 - train: epoch 0008, iter [00300, 05004], lr: 0.199768, loss: 2.2703
2022-08-04 09:25:20 - train: epoch 0008, iter [00400, 05004], lr: 0.199764, loss: 2.1461
2022-08-04 09:27:04 - train: epoch 0008, iter [00500, 05004], lr: 0.199759, loss: 2.2271
2022-08-04 09:28:49 - train: epoch 0008, iter [00600, 05004], lr: 0.199754, loss: 2.1080
2022-08-04 09:30:36 - train: epoch 0008, iter [00700, 05004], lr: 0.199750, loss: 2.3983
2022-08-04 09:32:20 - train: epoch 0008, iter [00800, 05004], lr: 0.199745, loss: 2.2197
2022-08-04 09:34:06 - train: epoch 0008, iter [00900, 05004], lr: 0.199740, loss: 2.3233
2022-08-04 09:35:51 - train: epoch 0008, iter [01000, 05004], lr: 0.199736, loss: 2.3000
2022-08-04 09:37:37 - train: epoch 0008, iter [01100, 05004], lr: 0.199731, loss: 2.0459
2022-08-04 09:39:22 - train: epoch 0008, iter [01200, 05004], lr: 0.199726, loss: 2.0889
2022-08-04 09:41:06 - train: epoch 0008, iter [01300, 05004], lr: 0.199721, loss: 2.2551
2022-08-04 09:42:52 - train: epoch 0008, iter [01400, 05004], lr: 0.199716, loss: 2.1614
2022-08-04 09:44:37 - train: epoch 0008, iter [01500, 05004], lr: 0.199711, loss: 2.3091
2022-08-04 09:46:24 - train: epoch 0008, iter [01600, 05004], lr: 0.199706, loss: 2.3508
2022-08-04 09:48:08 - train: epoch 0008, iter [01700, 05004], lr: 0.199701, loss: 2.2578
2022-08-04 09:49:52 - train: epoch 0008, iter [01800, 05004], lr: 0.199696, loss: 2.2442
2022-08-04 09:51:38 - train: epoch 0008, iter [01900, 05004], lr: 0.199691, loss: 2.2379
2022-08-04 09:53:23 - train: epoch 0008, iter [02000, 05004], lr: 0.199685, loss: 2.2656
2022-08-04 09:55:08 - train: epoch 0008, iter [02100, 05004], lr: 0.199680, loss: 2.2993
2022-08-04 09:56:54 - train: epoch 0008, iter [02200, 05004], lr: 0.199675, loss: 2.2225
2022-08-04 09:58:39 - train: epoch 0008, iter [02300, 05004], lr: 0.199669, loss: 2.4537
2022-08-04 10:00:26 - train: epoch 0008, iter [02400, 05004], lr: 0.199664, loss: 2.2380
2022-08-04 10:02:13 - train: epoch 0008, iter [02500, 05004], lr: 0.199659, loss: 2.0699
2022-08-04 10:03:58 - train: epoch 0008, iter [02600, 05004], lr: 0.199653, loss: 2.2417
2022-08-04 10:05:42 - train: epoch 0008, iter [02700, 05004], lr: 0.199648, loss: 2.2927
2022-08-04 10:07:27 - train: epoch 0008, iter [02800, 05004], lr: 0.199642, loss: 2.4881
2022-08-04 10:09:13 - train: epoch 0008, iter [02900, 05004], lr: 0.199636, loss: 2.2329
2022-08-04 10:10:59 - train: epoch 0008, iter [03000, 05004], lr: 0.199631, loss: 2.4573
2022-08-04 10:12:44 - train: epoch 0008, iter [03100, 05004], lr: 0.199625, loss: 2.3878
2022-08-04 10:14:29 - train: epoch 0008, iter [03200, 05004], lr: 0.199619, loss: 2.2928
2022-08-04 10:16:14 - train: epoch 0008, iter [03300, 05004], lr: 0.199614, loss: 2.2613
2022-08-04 10:17:58 - train: epoch 0008, iter [03400, 05004], lr: 0.199608, loss: 2.2280
2022-08-04 10:19:44 - train: epoch 0008, iter [03500, 05004], lr: 0.199602, loss: 2.3119
2022-08-04 10:21:31 - train: epoch 0008, iter [03600, 05004], lr: 0.199596, loss: 2.4582
2022-08-04 10:23:18 - train: epoch 0008, iter [03700, 05004], lr: 0.199590, loss: 2.2867
2022-08-04 10:25:04 - train: epoch 0008, iter [03800, 05004], lr: 0.199584, loss: 2.1720
2022-08-04 10:26:47 - train: epoch 0008, iter [03900, 05004], lr: 0.199578, loss: 2.6509
2022-08-04 10:28:33 - train: epoch 0008, iter [04000, 05004], lr: 0.199572, loss: 2.7060
2022-08-04 10:30:19 - train: epoch 0008, iter [04100, 05004], lr: 0.199566, loss: 2.1109
2022-08-04 10:32:04 - train: epoch 0008, iter [04200, 05004], lr: 0.199560, loss: 2.2770
2022-08-04 10:33:49 - train: epoch 0008, iter [04300, 05004], lr: 0.199553, loss: 1.8800
2022-08-04 10:35:35 - train: epoch 0008, iter [04400, 05004], lr: 0.199547, loss: 2.3965
2022-08-04 10:37:19 - train: epoch 0008, iter [04500, 05004], lr: 0.199541, loss: 2.1749
2022-08-04 10:39:05 - train: epoch 0008, iter [04600, 05004], lr: 0.199534, loss: 2.2568
2022-08-04 10:40:52 - train: epoch 0008, iter [04700, 05004], lr: 0.199528, loss: 2.2936
2022-08-04 10:42:39 - train: epoch 0008, iter [04800, 05004], lr: 0.199522, loss: 2.2853
2022-08-04 10:44:26 - train: epoch 0008, iter [04900, 05004], lr: 0.199515, loss: 2.4157
2022-08-04 10:46:10 - train: epoch 0008, iter [05000, 05004], lr: 0.199509, loss: 2.0348
2022-08-04 10:46:15 - train: epoch 008, train_loss: 2.2594
2022-08-04 10:49:20 - eval: epoch: 008, acc1: 53.036%, acc5: 78.648%, test_loss: 2.0016, per_image_load_time: 5.832ms, per_image_inference_time: 1.313ms
2022-08-04 10:49:21 - until epoch: 008, best_acc1: 53.036%
2022-08-04 10:49:21 - epoch 009 lr: 0.199508
2022-08-04 10:51:20 - train: epoch 0009, iter [00100, 05004], lr: 0.199502, loss: 2.0056
2022-08-04 10:53:04 - train: epoch 0009, iter [00200, 05004], lr: 0.199495, loss: 2.1528
2022-08-04 10:54:48 - train: epoch 0009, iter [00300, 05004], lr: 0.199488, loss: 2.0934
2022-08-04 10:56:33 - train: epoch 0009, iter [00400, 05004], lr: 0.199482, loss: 2.5236
2022-08-04 10:58:19 - train: epoch 0009, iter [00500, 05004], lr: 0.199475, loss: 2.1432
2022-08-04 11:00:04 - train: epoch 0009, iter [00600, 05004], lr: 0.199468, loss: 2.1022
2022-08-04 11:01:50 - train: epoch 0009, iter [00700, 05004], lr: 0.199461, loss: 2.1785
2022-08-04 11:03:37 - train: epoch 0009, iter [00800, 05004], lr: 0.199455, loss: 2.2176
2022-08-04 11:05:37 - train: epoch 0009, iter [00900, 05004], lr: 0.199448, loss: 2.0071
2022-08-04 11:07:26 - train: epoch 0009, iter [01000, 05004], lr: 0.199441, loss: 2.1282
2022-08-04 11:09:12 - train: epoch 0009, iter [01100, 05004], lr: 0.199434, loss: 2.4286
2022-08-04 11:10:58 - train: epoch 0009, iter [01200, 05004], lr: 0.199427, loss: 2.1576
2022-08-04 11:12:44 - train: epoch 0009, iter [01300, 05004], lr: 0.199420, loss: 2.6062
2022-08-04 11:14:30 - train: epoch 0009, iter [01400, 05004], lr: 0.199412, loss: 1.9974
2022-08-04 11:16:16 - train: epoch 0009, iter [01500, 05004], lr: 0.199405, loss: 2.0549
2022-08-04 11:18:02 - train: epoch 0009, iter [01600, 05004], lr: 0.199398, loss: 2.1284
2022-08-04 11:19:48 - train: epoch 0009, iter [01700, 05004], lr: 0.199391, loss: 2.2288
2022-08-04 11:21:34 - train: epoch 0009, iter [01800, 05004], lr: 0.199383, loss: 2.0636
2022-08-04 11:23:18 - train: epoch 0009, iter [01900, 05004], lr: 0.199376, loss: 2.0788
2022-08-04 11:25:06 - train: epoch 0009, iter [02000, 05004], lr: 0.199369, loss: 1.8534
2022-08-04 11:26:58 - train: epoch 0009, iter [02100, 05004], lr: 0.199361, loss: 2.2583
2022-08-04 11:28:51 - train: epoch 0009, iter [02200, 05004], lr: 0.199354, loss: 2.2768
2022-08-04 11:30:38 - train: epoch 0009, iter [02300, 05004], lr: 0.199346, loss: 2.0746
2022-08-04 11:32:23 - train: epoch 0009, iter [02400, 05004], lr: 0.199339, loss: 2.2512
2022-08-04 11:34:08 - train: epoch 0009, iter [02500, 05004], lr: 0.199331, loss: 2.1676
2022-08-04 11:35:54 - train: epoch 0009, iter [02600, 05004], lr: 0.199323, loss: 2.3042
2022-08-04 11:37:39 - train: epoch 0009, iter [02700, 05004], lr: 0.199316, loss: 2.1487
2022-08-04 11:39:24 - train: epoch 0009, iter [02800, 05004], lr: 0.199308, loss: 2.2536
2022-08-04 11:41:08 - train: epoch 0009, iter [02900, 05004], lr: 0.199300, loss: 2.0963
2022-08-04 11:42:53 - train: epoch 0009, iter [03000, 05004], lr: 0.199292, loss: 2.2685
2022-08-04 11:44:38 - train: epoch 0009, iter [03100, 05004], lr: 0.199285, loss: 2.2487
2022-08-04 11:46:22 - train: epoch 0009, iter [03200, 05004], lr: 0.199277, loss: 2.1713
2022-08-04 11:48:07 - train: epoch 0009, iter [03300, 05004], lr: 0.199269, loss: 2.4298
2022-08-04 11:49:51 - train: epoch 0009, iter [03400, 05004], lr: 0.199261, loss: 2.4133
2022-08-04 11:51:36 - train: epoch 0009, iter [03500, 05004], lr: 0.199253, loss: 2.1645
2022-08-04 11:53:22 - train: epoch 0009, iter [03600, 05004], lr: 0.199245, loss: 2.0928
2022-08-04 11:55:08 - train: epoch 0009, iter [03700, 05004], lr: 0.199236, loss: 2.0386
2022-08-04 11:56:53 - train: epoch 0009, iter [03800, 05004], lr: 0.199228, loss: 2.4815
2022-08-04 11:58:37 - train: epoch 0009, iter [03900, 05004], lr: 0.199220, loss: 2.0659
2022-08-04 12:00:22 - train: epoch 0009, iter [04000, 05004], lr: 0.199212, loss: 2.2080
2022-08-04 12:02:10 - train: epoch 0009, iter [04100, 05004], lr: 0.199203, loss: 2.0029
2022-08-04 12:03:58 - train: epoch 0009, iter [04200, 05004], lr: 0.199195, loss: 2.0127
2022-08-04 12:05:45 - train: epoch 0009, iter [04300, 05004], lr: 0.199187, loss: 1.9987
2022-08-04 12:07:29 - train: epoch 0009, iter [04400, 05004], lr: 0.199178, loss: 2.3458
2022-08-04 12:09:16 - train: epoch 0009, iter [04500, 05004], lr: 0.199170, loss: 2.2942
2022-08-04 12:11:06 - train: epoch 0009, iter [04600, 05004], lr: 0.199161, loss: 2.4866
2022-08-04 12:13:00 - train: epoch 0009, iter [04700, 05004], lr: 0.199153, loss: 2.2646
2022-08-04 12:14:46 - train: epoch 0009, iter [04800, 05004], lr: 0.199144, loss: 2.1683
2022-08-04 12:16:31 - train: epoch 0009, iter [04900, 05004], lr: 0.199135, loss: 2.4125
2022-08-04 12:18:15 - train: epoch 0009, iter [05000, 05004], lr: 0.199127, loss: 1.9970
2022-08-04 12:18:19 - train: epoch 009, train_loss: 2.2097
2022-08-04 12:21:37 - eval: epoch: 009, acc1: 55.294%, acc5: 79.768%, test_loss: 1.9119, per_image_load_time: 6.306ms, per_image_inference_time: 1.309ms
2022-08-04 12:21:37 - until epoch: 009, best_acc1: 55.294%
2022-08-04 12:21:37 - epoch 010 lr: 0.199126
2022-08-04 12:23:36 - train: epoch 0010, iter [00100, 05004], lr: 0.199118, loss: 2.0729
2022-08-04 12:25:21 - train: epoch 0010, iter [00200, 05004], lr: 0.199109, loss: 2.1074
2022-08-04 12:27:06 - train: epoch 0010, iter [00300, 05004], lr: 0.199100, loss: 2.1294
2022-08-04 12:28:52 - train: epoch 0010, iter [00400, 05004], lr: 0.199091, loss: 2.3941
2022-08-04 12:30:38 - train: epoch 0010, iter [00500, 05004], lr: 0.199082, loss: 2.0830
2022-08-04 12:32:31 - train: epoch 0010, iter [00600, 05004], lr: 0.199073, loss: 2.3970
2022-08-04 12:34:22 - train: epoch 0010, iter [00700, 05004], lr: 0.199064, loss: 2.1941
2022-08-04 12:36:13 - train: epoch 0010, iter [00800, 05004], lr: 0.199055, loss: 2.0440
2022-08-04 12:38:04 - train: epoch 0010, iter [00900, 05004], lr: 0.199046, loss: 2.1140
2022-08-04 12:39:48 - train: epoch 0010, iter [01000, 05004], lr: 0.199037, loss: 2.0085
2022-08-04 12:41:34 - train: epoch 0010, iter [01100, 05004], lr: 0.199028, loss: 1.8530
2022-08-04 12:43:19 - train: epoch 0010, iter [01200, 05004], lr: 0.199019, loss: 2.1744
2022-08-04 12:45:02 - train: epoch 0010, iter [01300, 05004], lr: 0.199009, loss: 2.1074
2022-08-04 12:46:46 - train: epoch 0010, iter [01400, 05004], lr: 0.199000, loss: 2.2877
2022-08-04 12:48:31 - train: epoch 0010, iter [01500, 05004], lr: 0.198991, loss: 1.8916
2022-08-04 12:50:26 - train: epoch 0010, iter [01600, 05004], lr: 0.198981, loss: 2.1718
2022-08-04 12:52:19 - train: epoch 0010, iter [01700, 05004], lr: 0.198972, loss: 2.3459
2022-08-04 12:54:07 - train: epoch 0010, iter [01800, 05004], lr: 0.198963, loss: 1.9620
2022-08-04 12:55:53 - train: epoch 0010, iter [01900, 05004], lr: 0.198953, loss: 2.2526
2022-08-04 12:57:39 - train: epoch 0010, iter [02000, 05004], lr: 0.198943, loss: 2.0240
2022-08-04 12:59:24 - train: epoch 0010, iter [02100, 05004], lr: 0.198934, loss: 2.0123
2022-08-04 13:01:09 - train: epoch 0010, iter [02200, 05004], lr: 0.198924, loss: 2.3620
2022-08-04 13:02:54 - train: epoch 0010, iter [02300, 05004], lr: 0.198914, loss: 2.3081
2022-08-04 13:04:38 - train: epoch 0010, iter [02400, 05004], lr: 0.198905, loss: 2.4615
2022-08-04 13:06:24 - train: epoch 0010, iter [02500, 05004], lr: 0.198895, loss: 2.3244
2022-08-04 13:08:11 - train: epoch 0010, iter [02600, 05004], lr: 0.198885, loss: 2.3048
2022-08-04 13:09:58 - train: epoch 0010, iter [02700, 05004], lr: 0.198875, loss: 2.1376
2022-08-04 13:11:43 - train: epoch 0010, iter [02800, 05004], lr: 0.198865, loss: 2.2900
2022-08-04 13:13:29 - train: epoch 0010, iter [02900, 05004], lr: 0.198855, loss: 2.2874
2022-08-04 13:15:14 - train: epoch 0010, iter [03000, 05004], lr: 0.198845, loss: 2.1512
2022-08-04 13:16:59 - train: epoch 0010, iter [03100, 05004], lr: 0.198835, loss: 2.1735
2022-08-04 13:18:44 - train: epoch 0010, iter [03200, 05004], lr: 0.198825, loss: 2.0951
2022-08-04 13:20:29 - train: epoch 0010, iter [03300, 05004], lr: 0.198815, loss: 2.3075
2022-08-04 13:22:15 - train: epoch 0010, iter [03400, 05004], lr: 0.198805, loss: 2.2828
2022-08-04 13:24:02 - train: epoch 0010, iter [03500, 05004], lr: 0.198795, loss: 2.3100
2022-08-04 13:25:49 - train: epoch 0010, iter [03600, 05004], lr: 0.198785, loss: 2.2346
2022-08-04 13:27:33 - train: epoch 0010, iter [03700, 05004], lr: 0.198774, loss: 2.2007
2022-08-04 13:29:19 - train: epoch 0010, iter [03800, 05004], lr: 0.198764, loss: 2.1928
2022-08-04 13:31:04 - train: epoch 0010, iter [03900, 05004], lr: 0.198754, loss: 1.9888
2022-08-04 13:32:50 - train: epoch 0010, iter [04000, 05004], lr: 0.198743, loss: 1.9629
2022-08-04 13:34:35 - train: epoch 0010, iter [04100, 05004], lr: 0.198733, loss: 2.0801
2022-08-04 13:36:22 - train: epoch 0010, iter [04200, 05004], lr: 0.198722, loss: 2.2174
2022-08-04 13:38:06 - train: epoch 0010, iter [04300, 05004], lr: 0.198712, loss: 2.0366
2022-08-04 13:39:50 - train: epoch 0010, iter [04400, 05004], lr: 0.198701, loss: 1.9783
2022-08-04 13:41:34 - train: epoch 0010, iter [04500, 05004], lr: 0.198690, loss: 2.2952
2022-08-04 13:43:17 - train: epoch 0010, iter [04600, 05004], lr: 0.198680, loss: 2.0295
2022-08-04 13:45:03 - train: epoch 0010, iter [04700, 05004], lr: 0.198669, loss: 2.2421
2022-08-04 13:46:48 - train: epoch 0010, iter [04800, 05004], lr: 0.198658, loss: 2.1188
2022-08-04 13:48:33 - train: epoch 0010, iter [04900, 05004], lr: 0.198647, loss: 2.0775
2022-08-04 13:50:17 - train: epoch 0010, iter [05000, 05004], lr: 0.198637, loss: 2.2221
2022-08-04 13:50:22 - train: epoch 010, train_loss: 2.1710
2022-08-04 13:53:29 - eval: epoch: 010, acc1: 55.738%, acc5: 80.488%, test_loss: 1.8739, per_image_load_time: 5.150ms, per_image_inference_time: 1.320ms
2022-08-04 13:53:29 - until epoch: 010, best_acc1: 55.738%
2022-08-04 13:53:29 - epoch 011 lr: 0.198636
2022-08-04 13:55:28 - train: epoch 0011, iter [00100, 05004], lr: 0.198625, loss: 2.0620
2022-08-04 13:57:13 - train: epoch 0011, iter [00200, 05004], lr: 0.198614, loss: 2.4070
2022-08-04 13:58:56 - train: epoch 0011, iter [00300, 05004], lr: 0.198603, loss: 2.1117
2022-08-04 14:00:41 - train: epoch 0011, iter [00400, 05004], lr: 0.198592, loss: 2.2432
2022-08-04 14:02:24 - train: epoch 0011, iter [00500, 05004], lr: 0.198581, loss: 2.1727
2022-08-04 14:04:10 - train: epoch 0011, iter [00600, 05004], lr: 0.198570, loss: 2.1739
2022-08-04 14:05:55 - train: epoch 0011, iter [00700, 05004], lr: 0.198559, loss: 2.1966
2022-08-04 14:07:40 - train: epoch 0011, iter [00800, 05004], lr: 0.198548, loss: 2.0735
2022-08-04 14:09:24 - train: epoch 0011, iter [00900, 05004], lr: 0.198536, loss: 2.2058
2022-08-04 14:11:10 - train: epoch 0011, iter [01000, 05004], lr: 0.198525, loss: 2.0103
2022-08-04 14:12:57 - train: epoch 0011, iter [01100, 05004], lr: 0.198514, loss: 2.2001
2022-08-04 14:14:43 - train: epoch 0011, iter [01200, 05004], lr: 0.198503, loss: 2.3550
2022-08-04 14:16:28 - train: epoch 0011, iter [01300, 05004], lr: 0.198491, loss: 2.2185
2022-08-04 14:18:06 - train: epoch 0011, iter [01400, 05004], lr: 0.198480, loss: 2.0232
2022-08-04 14:19:38 - train: epoch 0011, iter [01500, 05004], lr: 0.198468, loss: 1.9899
2022-08-04 14:21:11 - train: epoch 0011, iter [01600, 05004], lr: 0.198457, loss: 2.1574
2022-08-04 14:22:44 - train: epoch 0011, iter [01700, 05004], lr: 0.198445, loss: 2.1736
2022-08-04 14:24:16 - train: epoch 0011, iter [01800, 05004], lr: 0.198433, loss: 2.1260
2022-08-04 14:25:49 - train: epoch 0011, iter [01900, 05004], lr: 0.198422, loss: 2.0185
2022-08-04 14:27:22 - train: epoch 0011, iter [02000, 05004], lr: 0.198410, loss: 2.2327
2022-08-04 14:28:54 - train: epoch 0011, iter [02100, 05004], lr: 0.198398, loss: 2.1430
2022-08-04 14:30:26 - train: epoch 0011, iter [02200, 05004], lr: 0.198386, loss: 2.1108
2022-08-04 14:31:59 - train: epoch 0011, iter [02300, 05004], lr: 0.198375, loss: 2.1275
2022-08-04 14:33:32 - train: epoch 0011, iter [02400, 05004], lr: 0.198363, loss: 2.0821
2022-08-04 14:35:04 - train: epoch 0011, iter [02500, 05004], lr: 0.198351, loss: 2.0845
2022-08-04 14:36:37 - train: epoch 0011, iter [02600, 05004], lr: 0.198339, loss: 2.1028
2022-08-04 14:38:09 - train: epoch 0011, iter [02700, 05004], lr: 0.198327, loss: 1.9439
2022-08-04 14:39:42 - train: epoch 0011, iter [02800, 05004], lr: 0.198315, loss: 1.8076
2022-08-04 14:41:15 - train: epoch 0011, iter [02900, 05004], lr: 0.198303, loss: 2.1193
2022-08-04 14:42:47 - train: epoch 0011, iter [03000, 05004], lr: 0.198290, loss: 2.4856
2022-08-04 14:44:20 - train: epoch 0011, iter [03100, 05004], lr: 0.198278, loss: 2.0565
2022-08-04 14:45:52 - train: epoch 0011, iter [03200, 05004], lr: 0.198266, loss: 1.9264
2022-08-04 14:47:25 - train: epoch 0011, iter [03300, 05004], lr: 0.198254, loss: 2.0646
2022-08-04 14:48:58 - train: epoch 0011, iter [03400, 05004], lr: 0.198241, loss: 1.9655
2022-08-04 14:50:30 - train: epoch 0011, iter [03500, 05004], lr: 0.198229, loss: 2.1466
2022-08-04 14:52:03 - train: epoch 0011, iter [03600, 05004], lr: 0.198217, loss: 2.0258
2022-08-04 14:53:36 - train: epoch 0011, iter [03700, 05004], lr: 0.198204, loss: 2.2763
2022-08-04 14:55:08 - train: epoch 0011, iter [03800, 05004], lr: 0.198192, loss: 2.1309
2022-08-04 14:56:41 - train: epoch 0011, iter [03900, 05004], lr: 0.198179, loss: 2.2353
2022-08-04 14:58:14 - train: epoch 0011, iter [04000, 05004], lr: 0.198167, loss: 2.2801
2022-08-04 14:59:46 - train: epoch 0011, iter [04100, 05004], lr: 0.198154, loss: 1.8853
2022-08-04 15:01:19 - train: epoch 0011, iter [04200, 05004], lr: 0.198141, loss: 2.2502
2022-08-04 15:02:51 - train: epoch 0011, iter [04300, 05004], lr: 0.198129, loss: 2.2017
2022-08-04 15:04:24 - train: epoch 0011, iter [04400, 05004], lr: 0.198116, loss: 2.3398
2022-08-04 15:05:56 - train: epoch 0011, iter [04500, 05004], lr: 0.198103, loss: 2.0964
2022-08-04 15:07:29 - train: epoch 0011, iter [04600, 05004], lr: 0.198090, loss: 2.1081
2022-08-04 15:09:02 - train: epoch 0011, iter [04700, 05004], lr: 0.198077, loss: 1.7617
2022-08-04 15:10:34 - train: epoch 0011, iter [04800, 05004], lr: 0.198064, loss: 2.0014
2022-08-04 15:12:07 - train: epoch 0011, iter [04900, 05004], lr: 0.198052, loss: 1.8834
2022-08-04 15:13:39 - train: epoch 0011, iter [05000, 05004], lr: 0.198039, loss: 2.0780
2022-08-04 15:13:44 - train: epoch 011, train_loss: 2.1381
2022-08-04 15:15:49 - eval: epoch: 011, acc1: 57.034%, acc5: 81.464%, test_loss: 1.8166, per_image_load_time: 3.531ms, per_image_inference_time: 1.199ms
2022-08-04 15:15:49 - until epoch: 011, best_acc1: 57.034%
2022-08-04 15:15:49 - epoch 012 lr: 0.198038
2022-08-04 15:17:30 - train: epoch 0012, iter [00100, 05004], lr: 0.198025, loss: 2.0043
2022-08-04 15:19:03 - train: epoch 0012, iter [00200, 05004], lr: 0.198012, loss: 1.8356
2022-08-04 15:20:35 - train: epoch 0012, iter [00300, 05004], lr: 0.197999, loss: 2.0851
2022-08-04 15:22:07 - train: epoch 0012, iter [00400, 05004], lr: 0.197986, loss: 2.1733
2022-08-04 15:23:40 - train: epoch 0012, iter [00500, 05004], lr: 0.197972, loss: 2.1760
2022-08-04 15:25:12 - train: epoch 0012, iter [00600, 05004], lr: 0.197959, loss: 1.9522
2022-08-04 15:26:44 - train: epoch 0012, iter [00700, 05004], lr: 0.197946, loss: 1.9598
2022-08-04 15:28:17 - train: epoch 0012, iter [00800, 05004], lr: 0.197932, loss: 2.2151
2022-08-04 15:29:49 - train: epoch 0012, iter [00900, 05004], lr: 0.197919, loss: 2.2244
2022-08-04 15:31:22 - train: epoch 0012, iter [01000, 05004], lr: 0.197906, loss: 2.1290
2022-08-04 15:32:55 - train: epoch 0012, iter [01100, 05004], lr: 0.197892, loss: 2.4239
2022-08-04 15:34:27 - train: epoch 0012, iter [01200, 05004], lr: 0.197879, loss: 1.8814
2022-08-04 15:36:00 - train: epoch 0012, iter [01300, 05004], lr: 0.197865, loss: 1.8326
2022-08-04 15:37:32 - train: epoch 0012, iter [01400, 05004], lr: 0.197851, loss: 2.2301
2022-08-04 15:39:05 - train: epoch 0012, iter [01500, 05004], lr: 0.197838, loss: 1.9200
2022-08-04 15:40:37 - train: epoch 0012, iter [01600, 05004], lr: 0.197824, loss: 2.0475
2022-08-04 15:42:10 - train: epoch 0012, iter [01700, 05004], lr: 0.197810, loss: 2.0238
2022-08-04 15:43:42 - train: epoch 0012, iter [01800, 05004], lr: 0.197797, loss: 2.2847
2022-08-04 15:45:15 - train: epoch 0012, iter [01900, 05004], lr: 0.197783, loss: 2.0677
2022-08-04 15:46:47 - train: epoch 0012, iter [02000, 05004], lr: 0.197769, loss: 2.3909
2022-08-04 15:48:20 - train: epoch 0012, iter [02100, 05004], lr: 0.197755, loss: 2.3935
2022-08-04 15:49:52 - train: epoch 0012, iter [02200, 05004], lr: 0.197741, loss: 2.2034
2022-08-04 15:51:25 - train: epoch 0012, iter [02300, 05004], lr: 0.197727, loss: 1.9164
2022-08-04 15:52:57 - train: epoch 0012, iter [02400, 05004], lr: 0.197713, loss: 2.1637
2022-08-04 15:54:29 - train: epoch 0012, iter [02500, 05004], lr: 0.197699, loss: 1.9768
2022-08-04 15:56:02 - train: epoch 0012, iter [02600, 05004], lr: 0.197685, loss: 1.9405
2022-08-04 15:57:34 - train: epoch 0012, iter [02700, 05004], lr: 0.197671, loss: 2.0362
2022-08-04 15:59:07 - train: epoch 0012, iter [02800, 05004], lr: 0.197656, loss: 2.1338
2022-08-04 16:00:39 - train: epoch 0012, iter [02900, 05004], lr: 0.197642, loss: 2.0990
2022-08-04 16:02:11 - train: epoch 0012, iter [03000, 05004], lr: 0.197628, loss: 1.8180
2022-08-04 16:03:44 - train: epoch 0012, iter [03100, 05004], lr: 0.197614, loss: 2.1655
2022-08-04 16:05:16 - train: epoch 0012, iter [03200, 05004], lr: 0.197599, loss: 2.0962
2022-08-04 16:06:48 - train: epoch 0012, iter [03300, 05004], lr: 0.197585, loss: 1.8918
2022-08-04 16:08:21 - train: epoch 0012, iter [03400, 05004], lr: 0.197570, loss: 2.1206
2022-08-04 16:09:53 - train: epoch 0012, iter [03500, 05004], lr: 0.197556, loss: 1.9785
2022-08-04 16:11:26 - train: epoch 0012, iter [03600, 05004], lr: 0.197541, loss: 2.1072
2022-08-04 16:12:58 - train: epoch 0012, iter [03700, 05004], lr: 0.197527, loss: 2.1067
2022-08-04 16:14:30 - train: epoch 0012, iter [03800, 05004], lr: 0.197512, loss: 2.1430
2022-08-04 16:16:03 - train: epoch 0012, iter [03900, 05004], lr: 0.197497, loss: 1.9700
2022-08-04 16:17:35 - train: epoch 0012, iter [04000, 05004], lr: 0.197483, loss: 2.1549
2022-08-04 16:19:08 - train: epoch 0012, iter [04100, 05004], lr: 0.197468, loss: 2.2007
2022-08-04 16:20:40 - train: epoch 0012, iter [04200, 05004], lr: 0.197453, loss: 2.0491
2022-08-04 16:22:12 - train: epoch 0012, iter [04300, 05004], lr: 0.197438, loss: 2.1747
2022-08-04 16:23:45 - train: epoch 0012, iter [04400, 05004], lr: 0.197423, loss: 1.9907
2022-08-04 16:25:17 - train: epoch 0012, iter [04500, 05004], lr: 0.197409, loss: 2.1233
2022-08-04 16:26:49 - train: epoch 0012, iter [04600, 05004], lr: 0.197394, loss: 2.3062
2022-08-04 16:28:22 - train: epoch 0012, iter [04700, 05004], lr: 0.197379, loss: 2.0892
2022-08-04 16:29:54 - train: epoch 0012, iter [04800, 05004], lr: 0.197364, loss: 2.2174
2022-08-04 16:31:26 - train: epoch 0012, iter [04900, 05004], lr: 0.197348, loss: 2.0288
2022-08-04 16:32:59 - train: epoch 0012, iter [05000, 05004], lr: 0.197333, loss: 2.0292
2022-08-04 16:33:03 - train: epoch 012, train_loss: 2.1107
2022-08-04 16:35:11 - eval: epoch: 012, acc1: 57.316%, acc5: 81.806%, test_loss: 1.7900, per_image_load_time: 2.923ms, per_image_inference_time: 1.247ms
2022-08-04 16:35:11 - until epoch: 012, best_acc1: 57.316%
2022-08-04 16:35:11 - epoch 013 lr: 0.197333
2022-08-04 16:36:53 - train: epoch 0013, iter [00100, 05004], lr: 0.197317, loss: 1.9466
2022-08-04 16:38:25 - train: epoch 0013, iter [00200, 05004], lr: 0.197302, loss: 2.0375
2022-08-04 16:39:58 - train: epoch 0013, iter [00300, 05004], lr: 0.197287, loss: 2.0252
2022-08-04 16:41:30 - train: epoch 0013, iter [00400, 05004], lr: 0.197272, loss: 1.9611
2022-08-04 16:43:03 - train: epoch 0013, iter [00500, 05004], lr: 0.197256, loss: 2.0184
2022-08-04 16:44:35 - train: epoch 0013, iter [00600, 05004], lr: 0.197241, loss: 2.0834
2022-08-04 16:46:08 - train: epoch 0013, iter [00700, 05004], lr: 0.197225, loss: 1.9942
2022-08-04 16:47:40 - train: epoch 0013, iter [00800, 05004], lr: 0.197210, loss: 2.2464
2022-08-04 16:49:13 - train: epoch 0013, iter [00900, 05004], lr: 0.197194, loss: 2.0062
2022-08-04 16:50:45 - train: epoch 0013, iter [01000, 05004], lr: 0.197179, loss: 2.1302
2022-08-04 16:52:17 - train: epoch 0013, iter [01100, 05004], lr: 0.197163, loss: 2.0770
2022-08-04 16:53:49 - train: epoch 0013, iter [01200, 05004], lr: 0.197148, loss: 2.1876
2022-08-04 16:55:22 - train: epoch 0013, iter [01300, 05004], lr: 0.197132, loss: 1.9688
2022-08-04 16:56:54 - train: epoch 0013, iter [01400, 05004], lr: 0.197116, loss: 2.2127
2022-08-04 16:58:27 - train: epoch 0013, iter [01500, 05004], lr: 0.197100, loss: 2.2359
2022-08-04 16:59:59 - train: epoch 0013, iter [01600, 05004], lr: 0.197085, loss: 1.9248
2022-08-04 17:01:31 - train: epoch 0013, iter [01700, 05004], lr: 0.197069, loss: 2.0042
2022-08-04 17:03:04 - train: epoch 0013, iter [01800, 05004], lr: 0.197053, loss: 2.0586
2022-08-04 17:04:36 - train: epoch 0013, iter [01900, 05004], lr: 0.197037, loss: 2.2962
2022-08-04 17:06:08 - train: epoch 0013, iter [02000, 05004], lr: 0.197021, loss: 2.1506
2022-08-04 17:07:41 - train: epoch 0013, iter [02100, 05004], lr: 0.197005, loss: 2.1665
2022-08-04 17:09:13 - train: epoch 0013, iter [02200, 05004], lr: 0.196989, loss: 1.9226
2022-08-04 17:10:45 - train: epoch 0013, iter [02300, 05004], lr: 0.196973, loss: 1.9539
2022-08-04 17:12:18 - train: epoch 0013, iter [02400, 05004], lr: 0.196957, loss: 2.3079
2022-08-04 17:13:50 - train: epoch 0013, iter [02500, 05004], lr: 0.196940, loss: 2.0321
2022-08-04 17:15:22 - train: epoch 0013, iter [02600, 05004], lr: 0.196924, loss: 2.0327
2022-08-04 17:16:55 - train: epoch 0013, iter [02700, 05004], lr: 0.196908, loss: 1.9961
2022-08-04 17:18:27 - train: epoch 0013, iter [02800, 05004], lr: 0.196891, loss: 2.2830
2022-08-04 17:19:59 - train: epoch 0013, iter [02900, 05004], lr: 0.196875, loss: 2.0817
2022-08-04 17:21:32 - train: epoch 0013, iter [03000, 05004], lr: 0.196859, loss: 1.9068
2022-08-04 17:23:04 - train: epoch 0013, iter [03100, 05004], lr: 0.196842, loss: 1.9568
2022-08-04 17:24:37 - train: epoch 0013, iter [03200, 05004], lr: 0.196826, loss: 2.0221
2022-08-04 17:26:09 - train: epoch 0013, iter [03300, 05004], lr: 0.196809, loss: 1.8383
2022-08-04 17:27:42 - train: epoch 0013, iter [03400, 05004], lr: 0.196793, loss: 2.0989
2022-08-04 17:29:14 - train: epoch 0013, iter [03500, 05004], lr: 0.196776, loss: 1.8989
2022-08-04 17:30:47 - train: epoch 0013, iter [03600, 05004], lr: 0.196759, loss: 2.1314
2022-08-04 17:32:19 - train: epoch 0013, iter [03700, 05004], lr: 0.196743, loss: 1.8292
2022-08-04 17:33:52 - train: epoch 0013, iter [03800, 05004], lr: 0.196726, loss: 2.2343
2022-08-04 17:35:25 - train: epoch 0013, iter [03900, 05004], lr: 0.196709, loss: 2.1628
2022-08-04 17:36:58 - train: epoch 0013, iter [04000, 05004], lr: 0.196692, loss: 2.1844
2022-08-04 17:38:31 - train: epoch 0013, iter [04100, 05004], lr: 0.196675, loss: 2.0965
2022-08-04 17:40:03 - train: epoch 0013, iter [04200, 05004], lr: 0.196658, loss: 2.0438
2022-08-04 17:41:36 - train: epoch 0013, iter [04300, 05004], lr: 0.196641, loss: 2.0174
2022-08-04 17:43:09 - train: epoch 0013, iter [04400, 05004], lr: 0.196624, loss: 2.1011
2022-08-04 17:44:42 - train: epoch 0013, iter [04500, 05004], lr: 0.196607, loss: 2.0621
2022-08-04 17:46:15 - train: epoch 0013, iter [04600, 05004], lr: 0.196590, loss: 2.2482
2022-08-04 17:47:47 - train: epoch 0013, iter [04700, 05004], lr: 0.196573, loss: 2.0314
2022-08-04 17:49:20 - train: epoch 0013, iter [04800, 05004], lr: 0.196556, loss: 2.1663
2022-08-04 17:50:53 - train: epoch 0013, iter [04900, 05004], lr: 0.196539, loss: 2.2391
2022-08-04 17:52:26 - train: epoch 0013, iter [05000, 05004], lr: 0.196522, loss: 2.2567
2022-08-04 17:52:30 - train: epoch 013, train_loss: 2.0859
2022-08-04 17:54:28 - eval: epoch: 013, acc1: 57.582%, acc5: 82.002%, test_loss: 1.7849, per_image_load_time: 3.435ms, per_image_inference_time: 1.089ms
2022-08-04 17:54:28 - until epoch: 013, best_acc1: 57.582%
2022-08-04 17:54:28 - epoch 014 lr: 0.196521
2022-08-04 17:56:10 - train: epoch 0014, iter [00100, 05004], lr: 0.196504, loss: 1.9842
2022-08-04 17:57:43 - train: epoch 0014, iter [00200, 05004], lr: 0.196486, loss: 2.0576
2022-08-04 17:59:16 - train: epoch 0014, iter [00300, 05004], lr: 0.196469, loss: 2.0620
2022-08-04 18:00:48 - train: epoch 0014, iter [00400, 05004], lr: 0.196451, loss: 1.9009
2022-08-04 18:02:21 - train: epoch 0014, iter [00500, 05004], lr: 0.196434, loss: 1.8018
2022-08-04 18:03:54 - train: epoch 0014, iter [00600, 05004], lr: 0.196416, loss: 2.0704
2022-08-04 18:05:27 - train: epoch 0014, iter [00700, 05004], lr: 0.196399, loss: 1.8603
2022-08-04 18:07:00 - train: epoch 0014, iter [00800, 05004], lr: 0.196381, loss: 1.9560
2022-08-04 18:08:33 - train: epoch 0014, iter [00900, 05004], lr: 0.196364, loss: 1.9731
2022-08-04 18:10:06 - train: epoch 0014, iter [01000, 05004], lr: 0.196346, loss: 2.1490
2022-08-04 18:11:38 - train: epoch 0014, iter [01100, 05004], lr: 0.196328, loss: 1.8951
2022-08-04 18:13:11 - train: epoch 0014, iter [01200, 05004], lr: 0.196310, loss: 1.9545
2022-08-04 18:14:44 - train: epoch 0014, iter [01300, 05004], lr: 0.196293, loss: 2.2040
2022-08-04 18:16:16 - train: epoch 0014, iter [01400, 05004], lr: 0.196275, loss: 2.0000
2022-08-04 18:17:49 - train: epoch 0014, iter [01500, 05004], lr: 0.196257, loss: 2.2428
2022-08-04 18:19:22 - train: epoch 0014, iter [01600, 05004], lr: 0.196239, loss: 2.0019
2022-08-04 18:20:55 - train: epoch 0014, iter [01700, 05004], lr: 0.196221, loss: 2.2654
2022-08-04 18:22:28 - train: epoch 0014, iter [01800, 05004], lr: 0.196203, loss: 2.3413
2022-08-04 18:24:00 - train: epoch 0014, iter [01900, 05004], lr: 0.196185, loss: 1.9642
2022-08-04 18:25:33 - train: epoch 0014, iter [02000, 05004], lr: 0.196167, loss: 1.9461
2022-08-04 18:27:06 - train: epoch 0014, iter [02100, 05004], lr: 0.196149, loss: 2.1674
2022-08-04 18:28:39 - train: epoch 0014, iter [02200, 05004], lr: 0.196131, loss: 2.1004
2022-08-04 18:30:12 - train: epoch 0014, iter [02300, 05004], lr: 0.196112, loss: 2.0447
2022-08-04 18:31:45 - train: epoch 0014, iter [02400, 05004], lr: 0.196094, loss: 2.0637
2022-08-04 18:33:17 - train: epoch 0014, iter [02500, 05004], lr: 0.196076, loss: 2.0656
2022-08-04 18:34:50 - train: epoch 0014, iter [02600, 05004], lr: 0.196057, loss: 1.9215
2022-08-04 18:36:23 - train: epoch 0014, iter [02700, 05004], lr: 0.196039, loss: 1.8193
2022-08-04 18:37:56 - train: epoch 0014, iter [02800, 05004], lr: 0.196021, loss: 2.0063
2022-08-04 18:39:28 - train: epoch 0014, iter [02900, 05004], lr: 0.196002, loss: 2.1188
2022-08-04 18:41:01 - train: epoch 0014, iter [03000, 05004], lr: 0.195984, loss: 2.0017
2022-08-04 18:42:34 - train: epoch 0014, iter [03100, 05004], lr: 0.195965, loss: 2.0562
2022-08-04 18:44:07 - train: epoch 0014, iter [03200, 05004], lr: 0.195946, loss: 2.1200
2022-08-04 18:45:40 - train: epoch 0014, iter [03300, 05004], lr: 0.195928, loss: 1.8680
2022-08-04 18:47:13 - train: epoch 0014, iter [03400, 05004], lr: 0.195909, loss: 1.9806
2022-08-04 18:48:45 - train: epoch 0014, iter [03500, 05004], lr: 0.195890, loss: 1.9996
2022-08-04 18:50:18 - train: epoch 0014, iter [03600, 05004], lr: 0.195872, loss: 2.0037
2022-08-04 18:51:51 - train: epoch 0014, iter [03700, 05004], lr: 0.195853, loss: 2.0389
2022-08-04 18:53:24 - train: epoch 0014, iter [03800, 05004], lr: 0.195834, loss: 2.1213
2022-08-04 18:54:57 - train: epoch 0014, iter [03900, 05004], lr: 0.195815, loss: 2.0661
2022-08-04 18:56:29 - train: epoch 0014, iter [04000, 05004], lr: 0.195796, loss: 2.1157
2022-08-04 18:58:02 - train: epoch 0014, iter [04100, 05004], lr: 0.195777, loss: 2.1739
2022-08-04 18:59:35 - train: epoch 0014, iter [04200, 05004], lr: 0.195758, loss: 1.9081
2022-08-04 19:01:08 - train: epoch 0014, iter [04300, 05004], lr: 0.195739, loss: 2.0562
2022-08-04 19:02:41 - train: epoch 0014, iter [04400, 05004], lr: 0.195720, loss: 2.0949
2022-08-04 19:04:14 - train: epoch 0014, iter [04500, 05004], lr: 0.195701, loss: 1.9842
2022-08-04 19:05:47 - train: epoch 0014, iter [04600, 05004], lr: 0.195682, loss: 2.0877
2022-08-04 19:07:20 - train: epoch 0014, iter [04700, 05004], lr: 0.195662, loss: 2.0110
2022-08-04 19:08:53 - train: epoch 0014, iter [04800, 05004], lr: 0.195643, loss: 2.0542
2022-08-04 19:10:26 - train: epoch 0014, iter [04900, 05004], lr: 0.195624, loss: 1.9684
2022-08-04 19:11:58 - train: epoch 0014, iter [05000, 05004], lr: 0.195604, loss: 2.0823
2022-08-04 19:12:03 - train: epoch 014, train_loss: 2.0638
2022-08-04 19:14:08 - eval: epoch: 014, acc1: 57.488%, acc5: 81.758%, test_loss: 1.8009, per_image_load_time: 3.572ms, per_image_inference_time: 1.187ms
2022-08-04 19:14:09 - until epoch: 014, best_acc1: 57.582%
2022-08-04 19:14:09 - epoch 015 lr: 0.195603
2022-08-04 19:15:52 - train: epoch 0015, iter [00100, 05004], lr: 0.195584, loss: 1.7338
2022-08-04 19:17:25 - train: epoch 0015, iter [00200, 05004], lr: 0.195565, loss: 2.1378
2022-08-04 19:18:58 - train: epoch 0015, iter [00300, 05004], lr: 0.195545, loss: 2.0201
2022-08-04 19:20:31 - train: epoch 0015, iter [00400, 05004], lr: 0.195526, loss: 2.1307
2022-08-04 19:22:04 - train: epoch 0015, iter [00500, 05004], lr: 0.195506, loss: 2.0531
2022-08-04 19:23:37 - train: epoch 0015, iter [00600, 05004], lr: 0.195487, loss: 2.0320
2022-08-04 19:25:10 - train: epoch 0015, iter [00700, 05004], lr: 0.195467, loss: 2.1379
2022-08-04 19:26:43 - train: epoch 0015, iter [00800, 05004], lr: 0.195447, loss: 1.9915
2022-08-04 19:28:16 - train: epoch 0015, iter [00900, 05004], lr: 0.195427, loss: 2.0888
2022-08-04 19:29:49 - train: epoch 0015, iter [01000, 05004], lr: 0.195408, loss: 2.0114
2022-08-04 19:31:22 - train: epoch 0015, iter [01100, 05004], lr: 0.195388, loss: 1.7994
2022-08-04 19:32:55 - train: epoch 0015, iter [01200, 05004], lr: 0.195368, loss: 2.1007
2022-08-04 19:34:28 - train: epoch 0015, iter [01300, 05004], lr: 0.195348, loss: 2.1706
2022-08-04 19:36:01 - train: epoch 0015, iter [01400, 05004], lr: 0.195328, loss: 2.0046
2022-08-04 19:37:33 - train: epoch 0015, iter [01500, 05004], lr: 0.195308, loss: 1.8894
2022-08-04 19:39:06 - train: epoch 0015, iter [01600, 05004], lr: 0.195288, loss: 2.0957
2022-08-04 19:40:39 - train: epoch 0015, iter [01700, 05004], lr: 0.195268, loss: 2.0888
2022-08-04 19:42:12 - train: epoch 0015, iter [01800, 05004], lr: 0.195248, loss: 2.1410
2022-08-04 19:43:45 - train: epoch 0015, iter [01900, 05004], lr: 0.195228, loss: 1.9011
2022-08-04 19:45:18 - train: epoch 0015, iter [02000, 05004], lr: 0.195208, loss: 1.8796
2022-08-04 19:46:51 - train: epoch 0015, iter [02100, 05004], lr: 0.195187, loss: 1.8842
2022-08-04 19:48:24 - train: epoch 0015, iter [02200, 05004], lr: 0.195167, loss: 1.9823
2022-08-04 19:49:57 - train: epoch 0015, iter [02300, 05004], lr: 0.195147, loss: 2.0495
2022-08-04 19:51:30 - train: epoch 0015, iter [02400, 05004], lr: 0.195126, loss: 2.0630
2022-08-04 19:53:03 - train: epoch 0015, iter [02500, 05004], lr: 0.195106, loss: 2.2382
2022-08-04 19:54:35 - train: epoch 0015, iter [02600, 05004], lr: 0.195086, loss: 2.0398
2022-08-04 19:56:08 - train: epoch 0015, iter [02700, 05004], lr: 0.195065, loss: 2.1110
2022-08-04 19:57:41 - train: epoch 0015, iter [02800, 05004], lr: 0.195045, loss: 2.0856
2022-08-04 19:59:14 - train: epoch 0015, iter [02900, 05004], lr: 0.195024, loss: 2.1774
2022-08-04 20:00:47 - train: epoch 0015, iter [03000, 05004], lr: 0.195003, loss: 2.0578
2022-08-04 20:02:20 - train: epoch 0015, iter [03100, 05004], lr: 0.194983, loss: 2.0297
2022-08-04 20:03:53 - train: epoch 0015, iter [03200, 05004], lr: 0.194962, loss: 1.9476
2022-08-04 20:05:26 - train: epoch 0015, iter [03300, 05004], lr: 0.194941, loss: 1.8403
2022-08-04 20:06:59 - train: epoch 0015, iter [03400, 05004], lr: 0.194921, loss: 2.0869
2022-08-04 20:08:31 - train: epoch 0015, iter [03500, 05004], lr: 0.194900, loss: 2.3676
2022-08-04 20:10:04 - train: epoch 0015, iter [03600, 05004], lr: 0.194879, loss: 1.9704
2022-08-04 20:11:37 - train: epoch 0015, iter [03700, 05004], lr: 0.194858, loss: 1.8987
2022-08-04 20:13:10 - train: epoch 0015, iter [03800, 05004], lr: 0.194837, loss: 1.9896
2022-08-04 20:14:43 - train: epoch 0015, iter [03900, 05004], lr: 0.194816, loss: 1.9795
2022-08-04 20:16:15 - train: epoch 0015, iter [04000, 05004], lr: 0.194795, loss: 2.1593
2022-08-04 20:17:48 - train: epoch 0015, iter [04100, 05004], lr: 0.194774, loss: 2.0770
2022-08-04 20:19:21 - train: epoch 0015, iter [04200, 05004], lr: 0.194753, loss: 1.9262
2022-08-04 20:20:54 - train: epoch 0015, iter [04300, 05004], lr: 0.194732, loss: 1.9316
2022-08-04 20:22:27 - train: epoch 0015, iter [04400, 05004], lr: 0.194711, loss: 2.0352
2022-08-04 20:23:59 - train: epoch 0015, iter [04500, 05004], lr: 0.194689, loss: 1.8402
2022-08-04 20:25:32 - train: epoch 0015, iter [04600, 05004], lr: 0.194668, loss: 2.1875
2022-08-04 20:27:05 - train: epoch 0015, iter [04700, 05004], lr: 0.194647, loss: 2.0363
2022-08-04 20:28:38 - train: epoch 0015, iter [04800, 05004], lr: 0.194625, loss: 2.2394
2022-08-04 20:30:11 - train: epoch 0015, iter [04900, 05004], lr: 0.194604, loss: 1.9399
2022-08-04 20:31:44 - train: epoch 0015, iter [05000, 05004], lr: 0.194583, loss: 2.2911
2022-08-04 20:31:48 - train: epoch 015, train_loss: 2.0454
2022-08-04 20:33:57 - eval: epoch: 015, acc1: 58.472%, acc5: 82.296%, test_loss: 1.7559, per_image_load_time: 2.342ms, per_image_inference_time: 1.241ms
2022-08-04 20:33:58 - until epoch: 015, best_acc1: 58.472%
2022-08-04 20:33:58 - epoch 016 lr: 0.194582
2022-08-04 20:35:40 - train: epoch 0016, iter [00100, 05004], lr: 0.194560, loss: 1.9755
2022-08-04 20:37:12 - train: epoch 0016, iter [00200, 05004], lr: 0.194539, loss: 2.0869
2022-08-04 20:38:45 - train: epoch 0016, iter [00300, 05004], lr: 0.194517, loss: 1.9298
2022-08-04 20:40:17 - train: epoch 0016, iter [00400, 05004], lr: 0.194496, loss: 2.3424
2022-08-04 20:41:50 - train: epoch 0016, iter [00500, 05004], lr: 0.194474, loss: 1.8020
2022-08-04 20:43:23 - train: epoch 0016, iter [00600, 05004], lr: 0.194452, loss: 1.9195
2022-08-04 20:44:55 - train: epoch 0016, iter [00700, 05004], lr: 0.194431, loss: 1.9018
2022-08-04 20:46:28 - train: epoch 0016, iter [00800, 05004], lr: 0.194409, loss: 2.0921
2022-08-04 20:48:00 - train: epoch 0016, iter [00900, 05004], lr: 0.194387, loss: 2.1039
2022-08-04 20:49:33 - train: epoch 0016, iter [01000, 05004], lr: 0.194365, loss: 1.7368
2022-08-04 20:51:05 - train: epoch 0016, iter [01100, 05004], lr: 0.194343, loss: 1.9488
2022-08-04 20:52:38 - train: epoch 0016, iter [01200, 05004], lr: 0.194321, loss: 1.7234
2022-08-04 20:54:11 - train: epoch 0016, iter [01300, 05004], lr: 0.194299, loss: 2.0958
2022-08-04 20:55:43 - train: epoch 0016, iter [01400, 05004], lr: 0.194277, loss: 1.9370
2022-08-04 20:57:16 - train: epoch 0016, iter [01500, 05004], lr: 0.194255, loss: 2.2886
2022-08-04 20:58:49 - train: epoch 0016, iter [01600, 05004], lr: 0.194233, loss: 2.2140
2022-08-04 21:00:21 - train: epoch 0016, iter [01700, 05004], lr: 0.194211, loss: 2.0496
2022-08-04 21:01:54 - train: epoch 0016, iter [01800, 05004], lr: 0.194189, loss: 2.1645
2022-08-04 21:03:27 - train: epoch 0016, iter [01900, 05004], lr: 0.194167, loss: 1.9794
2022-08-04 21:04:59 - train: epoch 0016, iter [02000, 05004], lr: 0.194144, loss: 1.8832
2022-08-04 21:06:32 - train: epoch 0016, iter [02100, 05004], lr: 0.194122, loss: 1.9813
2022-08-04 21:08:05 - train: epoch 0016, iter [02200, 05004], lr: 0.194100, loss: 1.9663
2022-08-04 21:09:37 - train: epoch 0016, iter [02300, 05004], lr: 0.194077, loss: 2.1731
2022-08-04 21:11:10 - train: epoch 0016, iter [02400, 05004], lr: 0.194055, loss: 1.9865
2022-08-04 21:12:43 - train: epoch 0016, iter [02500, 05004], lr: 0.194032, loss: 1.9917
2022-08-04 21:14:16 - train: epoch 0016, iter [02600, 05004], lr: 0.194010, loss: 2.1507
2022-08-04 21:15:48 - train: epoch 0016, iter [02700, 05004], lr: 0.193987, loss: 1.8076
2022-08-04 21:17:21 - train: epoch 0016, iter [02800, 05004], lr: 0.193965, loss: 1.7731
2022-08-04 21:18:54 - train: epoch 0016, iter [02900, 05004], lr: 0.193942, loss: 1.9899
2022-08-04 21:20:26 - train: epoch 0016, iter [03000, 05004], lr: 0.193919, loss: 2.1996
2022-08-04 21:21:59 - train: epoch 0016, iter [03100, 05004], lr: 0.193897, loss: 2.2304
2022-08-04 21:23:32 - train: epoch 0016, iter [03200, 05004], lr: 0.193874, loss: 2.2462
2022-08-04 21:25:04 - train: epoch 0016, iter [03300, 05004], lr: 0.193851, loss: 2.0615
2022-08-04 21:26:36 - train: epoch 0016, iter [03400, 05004], lr: 0.193828, loss: 1.9673
2022-08-04 21:28:09 - train: epoch 0016, iter [03500, 05004], lr: 0.193805, loss: 2.1509
2022-08-04 21:29:42 - train: epoch 0016, iter [03600, 05004], lr: 0.193783, loss: 1.8624
2022-08-04 21:31:14 - train: epoch 0016, iter [03700, 05004], lr: 0.193760, loss: 2.0214
2022-08-04 21:32:47 - train: epoch 0016, iter [03800, 05004], lr: 0.193737, loss: 2.1725
2022-08-04 21:34:20 - train: epoch 0016, iter [03900, 05004], lr: 0.193714, loss: 2.0356
2022-08-04 21:35:52 - train: epoch 0016, iter [04000, 05004], lr: 0.193690, loss: 2.1816
2022-08-04 21:37:25 - train: epoch 0016, iter [04100, 05004], lr: 0.193667, loss: 2.1156
2022-08-04 21:38:57 - train: epoch 0016, iter [04200, 05004], lr: 0.193644, loss: 1.8955
2022-08-04 21:40:30 - train: epoch 0016, iter [04300, 05004], lr: 0.193621, loss: 2.1333
2022-08-04 21:42:02 - train: epoch 0016, iter [04400, 05004], lr: 0.193598, loss: 1.8366
2022-08-04 21:43:35 - train: epoch 0016, iter [04500, 05004], lr: 0.193574, loss: 2.1210
2022-08-04 21:45:08 - train: epoch 0016, iter [04600, 05004], lr: 0.193551, loss: 1.9160
2022-08-04 21:46:40 - train: epoch 0016, iter [04700, 05004], lr: 0.193528, loss: 2.2145
2022-08-04 21:48:13 - train: epoch 0016, iter [04800, 05004], lr: 0.193504, loss: 1.8870
2022-08-04 21:49:46 - train: epoch 0016, iter [04900, 05004], lr: 0.193481, loss: 1.9901
2022-08-04 21:51:19 - train: epoch 0016, iter [05000, 05004], lr: 0.193457, loss: 1.9376
2022-08-04 21:51:23 - train: epoch 016, train_loss: 2.0257
2022-08-04 21:53:25 - eval: epoch: 016, acc1: 57.872%, acc5: 82.116%, test_loss: 1.7698, per_image_load_time: 3.387ms, per_image_inference_time: 1.106ms
2022-08-04 21:53:26 - until epoch: 016, best_acc1: 58.472%
2022-08-04 21:53:26 - epoch 017 lr: 0.193456
2022-08-04 21:55:07 - train: epoch 0017, iter [00100, 05004], lr: 0.193433, loss: 2.0247
2022-08-04 21:56:40 - train: epoch 0017, iter [00200, 05004], lr: 0.193409, loss: 2.1378
2022-08-04 21:58:12 - train: epoch 0017, iter [00300, 05004], lr: 0.193386, loss: 2.1982
2022-08-04 21:59:45 - train: epoch 0017, iter [00400, 05004], lr: 0.193362, loss: 1.8170
2022-08-04 22:01:18 - train: epoch 0017, iter [00500, 05004], lr: 0.193338, loss: 2.0858
2022-08-04 22:02:51 - train: epoch 0017, iter [00600, 05004], lr: 0.193315, loss: 2.3289
2022-08-04 22:04:23 - train: epoch 0017, iter [00700, 05004], lr: 0.193291, loss: 1.8887
2022-08-04 22:05:56 - train: epoch 0017, iter [00800, 05004], lr: 0.193267, loss: 1.9668
2022-08-04 22:07:29 - train: epoch 0017, iter [00900, 05004], lr: 0.193243, loss: 1.8730
2022-08-04 22:09:01 - train: epoch 0017, iter [01000, 05004], lr: 0.193219, loss: 2.0116
2022-08-04 22:10:34 - train: epoch 0017, iter [01100, 05004], lr: 0.193195, loss: 2.0655
2022-08-04 22:12:06 - train: epoch 0017, iter [01200, 05004], lr: 0.193171, loss: 2.0049
2022-08-04 22:13:39 - train: epoch 0017, iter [01300, 05004], lr: 0.193147, loss: 2.0866
2022-08-04 22:15:11 - train: epoch 0017, iter [01400, 05004], lr: 0.193123, loss: 1.7982
2022-08-04 22:16:44 - train: epoch 0017, iter [01500, 05004], lr: 0.193099, loss: 1.8463
2022-08-04 22:18:17 - train: epoch 0017, iter [01600, 05004], lr: 0.193075, loss: 1.8563
2022-08-04 22:19:49 - train: epoch 0017, iter [01700, 05004], lr: 0.193051, loss: 1.9081
2022-08-04 22:21:22 - train: epoch 0017, iter [01800, 05004], lr: 0.193027, loss: 1.9295
2022-08-04 22:22:54 - train: epoch 0017, iter [01900, 05004], lr: 0.193002, loss: 1.7437
2022-08-04 22:24:27 - train: epoch 0017, iter [02000, 05004], lr: 0.192978, loss: 2.0224
2022-08-04 22:25:59 - train: epoch 0017, iter [02100, 05004], lr: 0.192954, loss: 1.8468
2022-08-04 22:27:32 - train: epoch 0017, iter [02200, 05004], lr: 0.192929, loss: 1.6393
2022-08-04 22:29:04 - train: epoch 0017, iter [02300, 05004], lr: 0.192905, loss: 2.1844
2022-08-04 22:30:37 - train: epoch 0017, iter [02400, 05004], lr: 0.192880, loss: 2.0899
2022-08-04 22:32:09 - train: epoch 0017, iter [02500, 05004], lr: 0.192856, loss: 1.9209
2022-08-04 22:33:42 - train: epoch 0017, iter [02600, 05004], lr: 0.192831, loss: 2.1433
2022-08-04 22:35:14 - train: epoch 0017, iter [02700, 05004], lr: 0.192807, loss: 1.9873
2022-08-04 22:36:47 - train: epoch 0017, iter [02800, 05004], lr: 0.192782, loss: 1.9728
2022-08-04 22:38:20 - train: epoch 0017, iter [02900, 05004], lr: 0.192757, loss: 2.1812
2022-08-04 22:39:52 - train: epoch 0017, iter [03000, 05004], lr: 0.192733, loss: 1.7795
2022-08-04 22:41:25 - train: epoch 0017, iter [03100, 05004], lr: 0.192708, loss: 2.1618
2022-08-04 22:42:57 - train: epoch 0017, iter [03200, 05004], lr: 0.192683, loss: 1.8239
2022-08-04 22:44:30 - train: epoch 0017, iter [03300, 05004], lr: 0.192658, loss: 2.1917
2022-08-04 22:46:02 - train: epoch 0017, iter [03400, 05004], lr: 0.192633, loss: 1.8730
2022-08-04 22:47:35 - train: epoch 0017, iter [03500, 05004], lr: 0.192609, loss: 2.1144
2022-08-04 22:49:08 - train: epoch 0017, iter [03600, 05004], lr: 0.192584, loss: 2.1365
2022-08-04 22:50:40 - train: epoch 0017, iter [03700, 05004], lr: 0.192559, loss: 1.9558
2022-08-04 22:52:13 - train: epoch 0017, iter [03800, 05004], lr: 0.192534, loss: 2.1329
2022-08-04 22:53:46 - train: epoch 0017, iter [03900, 05004], lr: 0.192509, loss: 1.9085
2022-08-04 22:55:19 - train: epoch 0017, iter [04000, 05004], lr: 0.192483, loss: 1.9109
2022-08-04 22:56:51 - train: epoch 0017, iter [04100, 05004], lr: 0.192458, loss: 2.0688
2022-08-04 22:58:24 - train: epoch 0017, iter [04200, 05004], lr: 0.192433, loss: 1.9433
2022-08-04 22:59:57 - train: epoch 0017, iter [04300, 05004], lr: 0.192408, loss: 1.9431
2022-08-04 23:01:30 - train: epoch 0017, iter [04400, 05004], lr: 0.192383, loss: 2.2952
2022-08-04 23:03:02 - train: epoch 0017, iter [04500, 05004], lr: 0.192357, loss: 2.0480
2022-08-04 23:04:35 - train: epoch 0017, iter [04600, 05004], lr: 0.192332, loss: 1.8476
2022-08-04 23:06:08 - train: epoch 0017, iter [04700, 05004], lr: 0.192306, loss: 2.0368
2022-08-04 23:07:41 - train: epoch 0017, iter [04800, 05004], lr: 0.192281, loss: 2.1998
2022-08-04 23:09:14 - train: epoch 0017, iter [04900, 05004], lr: 0.192256, loss: 1.8617
2022-08-04 23:10:46 - train: epoch 0017, iter [05000, 05004], lr: 0.192230, loss: 1.7916
2022-08-04 23:10:51 - train: epoch 017, train_loss: 2.0078
2022-08-04 23:12:57 - eval: epoch: 017, acc1: 58.602%, acc5: 82.690%, test_loss: 1.7327, per_image_load_time: 3.017ms, per_image_inference_time: 1.197ms
2022-08-04 23:12:58 - until epoch: 017, best_acc1: 58.602%
2022-08-04 23:12:58 - epoch 018 lr: 0.192229
2022-08-04 23:14:40 - train: epoch 0018, iter [00100, 05004], lr: 0.192203, loss: 1.9098
2022-08-04 23:16:13 - train: epoch 0018, iter [00200, 05004], lr: 0.192178, loss: 2.0170
2022-08-04 23:17:45 - train: epoch 0018, iter [00300, 05004], lr: 0.192152, loss: 2.2685
2022-08-04 23:19:18 - train: epoch 0018, iter [00400, 05004], lr: 0.192126, loss: 2.1070
2022-08-04 23:20:51 - train: epoch 0018, iter [00500, 05004], lr: 0.192101, loss: 1.9907
2022-08-04 23:22:24 - train: epoch 0018, iter [00600, 05004], lr: 0.192075, loss: 1.8067
2022-08-04 23:23:56 - train: epoch 0018, iter [00700, 05004], lr: 0.192049, loss: 1.9288
2022-08-04 23:25:29 - train: epoch 0018, iter [00800, 05004], lr: 0.192023, loss: 2.1866
2022-08-04 23:27:02 - train: epoch 0018, iter [00900, 05004], lr: 0.191997, loss: 2.0556
2022-08-04 23:28:35 - train: epoch 0018, iter [01000, 05004], lr: 0.191972, loss: 2.1485
2022-08-04 23:30:07 - train: epoch 0018, iter [01100, 05004], lr: 0.191946, loss: 2.4212
2022-08-04 23:31:40 - train: epoch 0018, iter [01200, 05004], lr: 0.191920, loss: 1.9493
2022-08-04 23:33:13 - train: epoch 0018, iter [01300, 05004], lr: 0.191894, loss: 2.1697
2022-08-04 23:34:46 - train: epoch 0018, iter [01400, 05004], lr: 0.191867, loss: 2.1504
2022-08-04 23:36:19 - train: epoch 0018, iter [01500, 05004], lr: 0.191841, loss: 2.1564
2022-08-04 23:37:51 - train: epoch 0018, iter [01600, 05004], lr: 0.191815, loss: 1.8602
2022-08-04 23:39:24 - train: epoch 0018, iter [01700, 05004], lr: 0.191789, loss: 2.0596
2022-08-04 23:40:57 - train: epoch 0018, iter [01800, 05004], lr: 0.191763, loss: 1.9248
2022-08-04 23:42:30 - train: epoch 0018, iter [01900, 05004], lr: 0.191736, loss: 2.0696
2022-08-04 23:44:03 - train: epoch 0018, iter [02000, 05004], lr: 0.191710, loss: 2.2892
2022-08-04 23:45:36 - train: epoch 0018, iter [02100, 05004], lr: 0.191684, loss: 2.3086
2022-08-04 23:47:08 - train: epoch 0018, iter [02200, 05004], lr: 0.191657, loss: 2.1875
2022-08-04 23:48:41 - train: epoch 0018, iter [02300, 05004], lr: 0.191631, loss: 2.0272
2022-08-04 23:50:13 - train: epoch 0018, iter [02400, 05004], lr: 0.191604, loss: 1.9580
2022-08-04 23:51:46 - train: epoch 0018, iter [02500, 05004], lr: 0.191578, loss: 1.7371
2022-08-04 23:53:19 - train: epoch 0018, iter [02600, 05004], lr: 0.191551, loss: 2.0131
2022-08-04 23:54:51 - train: epoch 0018, iter [02700, 05004], lr: 0.191525, loss: 2.0402
2022-08-04 23:56:24 - train: epoch 0018, iter [02800, 05004], lr: 0.191498, loss: 1.7877
2022-08-04 23:57:57 - train: epoch 0018, iter [02900, 05004], lr: 0.191471, loss: 1.9912
2022-08-04 23:59:29 - train: epoch 0018, iter [03000, 05004], lr: 0.191445, loss: 1.9778
2022-08-05 00:01:02 - train: epoch 0018, iter [03100, 05004], lr: 0.191418, loss: 2.1489
2022-08-05 00:02:35 - train: epoch 0018, iter [03200, 05004], lr: 0.191391, loss: 1.9885
2022-08-05 00:04:08 - train: epoch 0018, iter [03300, 05004], lr: 0.191364, loss: 1.9219
2022-08-05 00:05:40 - train: epoch 0018, iter [03400, 05004], lr: 0.191337, loss: 2.0412
2022-08-05 00:07:13 - train: epoch 0018, iter [03500, 05004], lr: 0.191310, loss: 2.1187
2022-08-05 00:08:45 - train: epoch 0018, iter [03600, 05004], lr: 0.191283, loss: 1.9841
2022-08-05 00:10:18 - train: epoch 0018, iter [03700, 05004], lr: 0.191256, loss: 2.1966
2022-08-05 00:11:51 - train: epoch 0018, iter [03800, 05004], lr: 0.191229, loss: 1.8993
2022-08-05 00:13:23 - train: epoch 0018, iter [03900, 05004], lr: 0.191202, loss: 2.1430
2022-08-05 00:14:56 - train: epoch 0018, iter [04000, 05004], lr: 0.191175, loss: 1.9456
2022-08-05 00:16:29 - train: epoch 0018, iter [04100, 05004], lr: 0.191148, loss: 2.0491
2022-08-05 00:18:01 - train: epoch 0018, iter [04200, 05004], lr: 0.191121, loss: 2.1163
2022-08-05 00:19:34 - train: epoch 0018, iter [04300, 05004], lr: 0.191094, loss: 2.0348
2022-08-05 00:21:07 - train: epoch 0018, iter [04400, 05004], lr: 0.191066, loss: 2.1604
2022-08-05 00:22:40 - train: epoch 0018, iter [04500, 05004], lr: 0.191039, loss: 2.0320
2022-08-05 00:24:13 - train: epoch 0018, iter [04600, 05004], lr: 0.191012, loss: 2.0831
2022-08-05 00:25:45 - train: epoch 0018, iter [04700, 05004], lr: 0.190984, loss: 2.2553
2022-08-05 00:27:18 - train: epoch 0018, iter [04800, 05004], lr: 0.190957, loss: 2.2491
2022-08-05 00:28:51 - train: epoch 0018, iter [04900, 05004], lr: 0.190929, loss: 1.9915
2022-08-05 00:30:24 - train: epoch 0018, iter [05000, 05004], lr: 0.190902, loss: 2.0581
2022-08-05 00:30:28 - train: epoch 018, train_loss: 1.9914
2022-08-05 00:32:36 - eval: epoch: 018, acc1: 59.400%, acc5: 83.214%, test_loss: 1.7041, per_image_load_time: 3.054ms, per_image_inference_time: 1.219ms
2022-08-05 00:32:37 - until epoch: 018, best_acc1: 59.400%
2022-08-05 00:32:37 - epoch 019 lr: 0.190900
2022-08-05 00:34:20 - train: epoch 0019, iter [00100, 05004], lr: 0.190873, loss: 1.8622
2022-08-05 00:35:52 - train: epoch 0019, iter [00200, 05004], lr: 0.190845, loss: 1.9540
2022-08-05 00:37:25 - train: epoch 0019, iter [00300, 05004], lr: 0.190818, loss: 1.8135
2022-08-05 00:38:58 - train: epoch 0019, iter [00400, 05004], lr: 0.190790, loss: 2.0366
2022-08-05 00:40:31 - train: epoch 0019, iter [00500, 05004], lr: 0.190762, loss: 2.0615
2022-08-05 00:42:04 - train: epoch 0019, iter [00600, 05004], lr: 0.190735, loss: 2.1173
2022-08-05 00:43:36 - train: epoch 0019, iter [00700, 05004], lr: 0.190707, loss: 1.7407
2022-08-05 00:45:09 - train: epoch 0019, iter [00800, 05004], lr: 0.190679, loss: 2.2866
2022-08-05 00:46:42 - train: epoch 0019, iter [00900, 05004], lr: 0.190651, loss: 1.9604
2022-08-05 00:48:14 - train: epoch 0019, iter [01000, 05004], lr: 0.190623, loss: 2.0496
2022-08-05 00:49:47 - train: epoch 0019, iter [01100, 05004], lr: 0.190595, loss: 1.9542
2022-08-05 00:51:19 - train: epoch 0019, iter [01200, 05004], lr: 0.190567, loss: 1.9416
2022-08-05 00:52:52 - train: epoch 0019, iter [01300, 05004], lr: 0.190539, loss: 1.9674
2022-08-05 00:54:24 - train: epoch 0019, iter [01400, 05004], lr: 0.190511, loss: 1.7626
2022-08-05 00:55:57 - train: epoch 0019, iter [01500, 05004], lr: 0.190483, loss: 2.1272
2022-08-05 00:57:29 - train: epoch 0019, iter [01600, 05004], lr: 0.190455, loss: 1.9205
2022-08-05 00:59:02 - train: epoch 0019, iter [01700, 05004], lr: 0.190427, loss: 2.0584
2022-08-05 01:00:35 - train: epoch 0019, iter [01800, 05004], lr: 0.190398, loss: 1.9599
2022-08-05 01:02:07 - train: epoch 0019, iter [01900, 05004], lr: 0.190370, loss: 2.1828
2022-08-05 01:03:40 - train: epoch 0019, iter [02000, 05004], lr: 0.190342, loss: 1.8601
2022-08-05 01:05:12 - train: epoch 0019, iter [02100, 05004], lr: 0.190314, loss: 1.8086
2022-08-05 01:06:45 - train: epoch 0019, iter [02200, 05004], lr: 0.190285, loss: 1.9303
2022-08-05 01:08:17 - train: epoch 0019, iter [02300, 05004], lr: 0.190257, loss: 2.0163
2022-08-05 01:09:49 - train: epoch 0019, iter [02400, 05004], lr: 0.190228, loss: 2.1361
2022-08-05 01:11:22 - train: epoch 0019, iter [02500, 05004], lr: 0.190200, loss: 1.9510
2022-08-05 01:12:54 - train: epoch 0019, iter [02600, 05004], lr: 0.190171, loss: 2.0148
2022-08-05 01:14:27 - train: epoch 0019, iter [02700, 05004], lr: 0.190143, loss: 1.9668
2022-08-05 01:15:59 - train: epoch 0019, iter [02800, 05004], lr: 0.190114, loss: 1.9947
2022-08-05 01:17:32 - train: epoch 0019, iter [02900, 05004], lr: 0.190085, loss: 2.1886
2022-08-05 01:19:04 - train: epoch 0019, iter [03000, 05004], lr: 0.190057, loss: 1.9490
2022-08-05 01:20:37 - train: epoch 0019, iter [03100, 05004], lr: 0.190028, loss: 2.0447
2022-08-05 01:22:09 - train: epoch 0019, iter [03200, 05004], lr: 0.189999, loss: 1.6231
2022-08-05 01:23:42 - train: epoch 0019, iter [03300, 05004], lr: 0.189970, loss: 1.9367
2022-08-05 01:25:14 - train: epoch 0019, iter [03400, 05004], lr: 0.189941, loss: 1.8807
2022-08-05 01:26:47 - train: epoch 0019, iter [03500, 05004], lr: 0.189912, loss: 2.0129
2022-08-05 01:28:19 - train: epoch 0019, iter [03600, 05004], lr: 0.189883, loss: 1.7415
2022-08-05 01:29:52 - train: epoch 0019, iter [03700, 05004], lr: 0.189854, loss: 1.9852
2022-08-05 01:31:24 - train: epoch 0019, iter [03800, 05004], lr: 0.189825, loss: 2.2178
2022-08-05 01:32:57 - train: epoch 0019, iter [03900, 05004], lr: 0.189796, loss: 1.9888
2022-08-05 01:34:29 - train: epoch 0019, iter [04000, 05004], lr: 0.189767, loss: 2.0003
2022-08-05 01:36:02 - train: epoch 0019, iter [04100, 05004], lr: 0.189738, loss: 2.1039
2022-08-05 01:37:34 - train: epoch 0019, iter [04200, 05004], lr: 0.189709, loss: 1.8956
2022-08-05 01:39:07 - train: epoch 0019, iter [04300, 05004], lr: 0.189680, loss: 1.8276
2022-08-05 01:40:40 - train: epoch 0019, iter [04400, 05004], lr: 0.189650, loss: 2.0000
2022-08-05 01:42:12 - train: epoch 0019, iter [04500, 05004], lr: 0.189621, loss: 2.2834
2022-08-05 01:43:45 - train: epoch 0019, iter [04600, 05004], lr: 0.189592, loss: 2.0318
2022-08-05 01:45:18 - train: epoch 0019, iter [04700, 05004], lr: 0.189562, loss: 2.1208
2022-08-05 01:46:50 - train: epoch 0019, iter [04800, 05004], lr: 0.189533, loss: 1.9964
2022-08-05 01:48:23 - train: epoch 0019, iter [04900, 05004], lr: 0.189504, loss: 1.7617
2022-08-05 01:49:55 - train: epoch 0019, iter [05000, 05004], lr: 0.189474, loss: 2.0225
2022-08-05 01:50:00 - train: epoch 019, train_loss: 1.9773
2022-08-05 01:52:10 - eval: epoch: 019, acc1: 59.572%, acc5: 83.338%, test_loss: 1.6896, per_image_load_time: 2.854ms, per_image_inference_time: 1.228ms
2022-08-05 01:52:10 - until epoch: 019, best_acc1: 59.572%
2022-08-05 01:52:10 - epoch 020 lr: 0.189473
2022-08-05 01:53:52 - train: epoch 0020, iter [00100, 05004], lr: 0.189443, loss: 2.0571
2022-08-05 01:55:25 - train: epoch 0020, iter [00200, 05004], lr: 0.189414, loss: 1.7453
2022-08-05 01:56:58 - train: epoch 0020, iter [00300, 05004], lr: 0.189384, loss: 1.9532
2022-08-05 01:58:30 - train: epoch 0020, iter [00400, 05004], lr: 0.189355, loss: 1.8608
2022-08-05 02:00:03 - train: epoch 0020, iter [00500, 05004], lr: 0.189325, loss: 1.8257
2022-08-05 02:01:35 - train: epoch 0020, iter [00600, 05004], lr: 0.189295, loss: 1.9770
2022-08-05 02:03:07 - train: epoch 0020, iter [00700, 05004], lr: 0.189265, loss: 1.5994
2022-08-05 02:04:40 - train: epoch 0020, iter [00800, 05004], lr: 0.189236, loss: 2.1287
2022-08-05 02:06:12 - train: epoch 0020, iter [00900, 05004], lr: 0.189206, loss: 2.0738
2022-08-05 02:07:45 - train: epoch 0020, iter [01000, 05004], lr: 0.189176, loss: 1.8138
2022-08-05 02:09:17 - train: epoch 0020, iter [01100, 05004], lr: 0.189146, loss: 1.9307
2022-08-05 02:10:50 - train: epoch 0020, iter [01200, 05004], lr: 0.189116, loss: 1.8263
2022-08-05 02:12:22 - train: epoch 0020, iter [01300, 05004], lr: 0.189086, loss: 1.9134
2022-08-05 02:13:55 - train: epoch 0020, iter [01400, 05004], lr: 0.189056, loss: 2.0883
2022-08-05 02:15:27 - train: epoch 0020, iter [01500, 05004], lr: 0.189026, loss: 2.1573
2022-08-05 02:17:00 - train: epoch 0020, iter [01600, 05004], lr: 0.188996, loss: 2.1586
2022-08-05 02:18:32 - train: epoch 0020, iter [01700, 05004], lr: 0.188966, loss: 1.7394
2022-08-05 02:20:05 - train: epoch 0020, iter [01800, 05004], lr: 0.188935, loss: 2.0257
2022-08-05 02:21:37 - train: epoch 0020, iter [01900, 05004], lr: 0.188905, loss: 1.8942
2022-08-05 02:23:09 - train: epoch 0020, iter [02000, 05004], lr: 0.188875, loss: 1.8242
2022-08-05 02:24:42 - train: epoch 0020, iter [02100, 05004], lr: 0.188845, loss: 2.1586
2022-08-05 02:26:14 - train: epoch 0020, iter [02200, 05004], lr: 0.188814, loss: 1.8109
2022-08-05 02:27:47 - train: epoch 0020, iter [02300, 05004], lr: 0.188784, loss: 1.9313
2022-08-05 02:29:19 - train: epoch 0020, iter [02400, 05004], lr: 0.188753, loss: 2.3845
2022-08-05 02:30:52 - train: epoch 0020, iter [02500, 05004], lr: 0.188723, loss: 1.8811
2022-08-05 02:32:24 - train: epoch 0020, iter [02600, 05004], lr: 0.188692, loss: 1.8375
2022-08-05 02:33:57 - train: epoch 0020, iter [02700, 05004], lr: 0.188662, loss: 1.9533
2022-08-05 02:35:29 - train: epoch 0020, iter [02800, 05004], lr: 0.188631, loss: 2.1212
2022-08-05 02:37:02 - train: epoch 0020, iter [02900, 05004], lr: 0.188601, loss: 2.1004
2022-08-05 02:38:34 - train: epoch 0020, iter [03000, 05004], lr: 0.188570, loss: 2.0298
2022-08-05 02:40:07 - train: epoch 0020, iter [03100, 05004], lr: 0.188539, loss: 2.2730
2022-08-05 02:41:39 - train: epoch 0020, iter [03200, 05004], lr: 0.188509, loss: 2.0912
2022-08-05 02:43:12 - train: epoch 0020, iter [03300, 05004], lr: 0.188478, loss: 1.7035
2022-08-05 02:44:44 - train: epoch 0020, iter [03400, 05004], lr: 0.188447, loss: 2.0813
2022-08-05 02:46:17 - train: epoch 0020, iter [03500, 05004], lr: 0.188416, loss: 1.9318
2022-08-05 02:47:50 - train: epoch 0020, iter [03600, 05004], lr: 0.188385, loss: 1.9360
2022-08-05 02:49:23 - train: epoch 0020, iter [03700, 05004], lr: 0.188354, loss: 1.9485
2022-08-05 02:50:56 - train: epoch 0020, iter [03800, 05004], lr: 0.188323, loss: 2.0444
2022-08-05 02:52:28 - train: epoch 0020, iter [03900, 05004], lr: 0.188292, loss: 2.0646
2022-08-05 02:54:01 - train: epoch 0020, iter [04000, 05004], lr: 0.188261, loss: 1.6159
2022-08-05 02:55:34 - train: epoch 0020, iter [04100, 05004], lr: 0.188230, loss: 2.0618
2022-08-05 02:57:07 - train: epoch 0020, iter [04200, 05004], lr: 0.188199, loss: 1.8832
2022-08-05 02:58:40 - train: epoch 0020, iter [04300, 05004], lr: 0.188168, loss: 2.1113
2022-08-05 03:00:13 - train: epoch 0020, iter [04400, 05004], lr: 0.188137, loss: 1.9251
2022-08-05 03:01:45 - train: epoch 0020, iter [04500, 05004], lr: 0.188105, loss: 2.0436
2022-08-05 03:03:18 - train: epoch 0020, iter [04600, 05004], lr: 0.188074, loss: 1.9217
2022-08-05 03:04:51 - train: epoch 0020, iter [04700, 05004], lr: 0.188043, loss: 1.9436
2022-08-05 03:06:24 - train: epoch 0020, iter [04800, 05004], lr: 0.188011, loss: 1.9314
2022-08-05 03:07:56 - train: epoch 0020, iter [04900, 05004], lr: 0.187980, loss: 1.9909
2022-08-05 03:09:29 - train: epoch 0020, iter [05000, 05004], lr: 0.187949, loss: 1.8231
2022-08-05 03:09:34 - train: epoch 020, train_loss: 1.9631
2022-08-05 03:11:41 - eval: epoch: 020, acc1: 60.110%, acc5: 83.862%, test_loss: 1.6745, per_image_load_time: 3.256ms, per_image_inference_time: 1.241ms
2022-08-05 03:11:41 - until epoch: 020, best_acc1: 60.110%
2022-08-05 03:11:41 - epoch 021 lr: 0.187947
2022-08-05 03:13:23 - train: epoch 0021, iter [00100, 05004], lr: 0.187916, loss: 2.0474
2022-08-05 03:14:55 - train: epoch 0021, iter [00200, 05004], lr: 0.187884, loss: 1.8935
2022-08-05 03:16:28 - train: epoch 0021, iter [00300, 05004], lr: 0.187853, loss: 1.6742
2022-08-05 03:18:00 - train: epoch 0021, iter [00400, 05004], lr: 0.187821, loss: 1.8874
2022-08-05 03:19:33 - train: epoch 0021, iter [00500, 05004], lr: 0.187790, loss: 1.6631
2022-08-05 03:21:05 - train: epoch 0021, iter [00600, 05004], lr: 0.187758, loss: 1.7005
2022-08-05 03:22:38 - train: epoch 0021, iter [00700, 05004], lr: 0.187726, loss: 1.6394
2022-08-05 03:24:10 - train: epoch 0021, iter [00800, 05004], lr: 0.187695, loss: 1.8589
2022-08-05 03:25:42 - train: epoch 0021, iter [00900, 05004], lr: 0.187663, loss: 1.8080
2022-08-05 03:27:15 - train: epoch 0021, iter [01000, 05004], lr: 0.187631, loss: 1.6536
2022-08-05 03:28:47 - train: epoch 0021, iter [01100, 05004], lr: 0.187599, loss: 1.7795
2022-08-05 03:30:20 - train: epoch 0021, iter [01200, 05004], lr: 0.187567, loss: 2.0974
2022-08-05 03:31:52 - train: epoch 0021, iter [01300, 05004], lr: 0.187535, loss: 1.6833
2022-08-05 03:33:25 - train: epoch 0021, iter [01400, 05004], lr: 0.187503, loss: 1.8693
2022-08-05 03:34:57 - train: epoch 0021, iter [01500, 05004], lr: 0.187471, loss: 1.7927
2022-08-05 03:36:30 - train: epoch 0021, iter [01600, 05004], lr: 0.187439, loss: 1.9377
2022-08-05 03:38:02 - train: epoch 0021, iter [01700, 05004], lr: 0.187407, loss: 2.0530
2022-08-05 03:39:35 - train: epoch 0021, iter [01800, 05004], lr: 0.187375, loss: 1.7072
2022-08-05 03:41:07 - train: epoch 0021, iter [01900, 05004], lr: 0.187343, loss: 2.1444
2022-08-05 03:42:40 - train: epoch 0021, iter [02000, 05004], lr: 0.187311, loss: 2.1378
2022-08-05 03:44:12 - train: epoch 0021, iter [02100, 05004], lr: 0.187278, loss: 1.7582
2022-08-05 03:45:44 - train: epoch 0021, iter [02200, 05004], lr: 0.187246, loss: 1.8898
2022-08-05 03:47:17 - train: epoch 0021, iter [02300, 05004], lr: 0.187214, loss: 2.0627
2022-08-05 03:48:49 - train: epoch 0021, iter [02400, 05004], lr: 0.187181, loss: 1.7924
2022-08-05 03:50:21 - train: epoch 0021, iter [02500, 05004], lr: 0.187149, loss: 1.9403
2022-08-05 03:51:53 - train: epoch 0021, iter [02600, 05004], lr: 0.187117, loss: 2.0321
2022-08-05 03:53:26 - train: epoch 0021, iter [02700, 05004], lr: 0.187084, loss: 2.1378
2022-08-05 03:54:58 - train: epoch 0021, iter [02800, 05004], lr: 0.187052, loss: 1.9710
2022-08-05 03:56:30 - train: epoch 0021, iter [02900, 05004], lr: 0.187019, loss: 1.9163
2022-08-05 03:58:03 - train: epoch 0021, iter [03000, 05004], lr: 0.186987, loss: 2.0627
2022-08-05 03:59:35 - train: epoch 0021, iter [03100, 05004], lr: 0.186954, loss: 2.0114
2022-08-05 04:01:07 - train: epoch 0021, iter [03200, 05004], lr: 0.186921, loss: 1.8807
2022-08-05 04:02:39 - train: epoch 0021, iter [03300, 05004], lr: 0.186889, loss: 2.1399
2022-08-05 04:04:12 - train: epoch 0021, iter [03400, 05004], lr: 0.186856, loss: 2.0078
2022-08-05 04:05:44 - train: epoch 0021, iter [03500, 05004], lr: 0.186823, loss: 2.0105
2022-08-05 04:07:16 - train: epoch 0021, iter [03600, 05004], lr: 0.186790, loss: 1.8754
2022-08-05 04:08:49 - train: epoch 0021, iter [03700, 05004], lr: 0.186757, loss: 1.8905
2022-08-05 04:10:21 - train: epoch 0021, iter [03800, 05004], lr: 0.186725, loss: 1.9890
2022-08-05 04:11:53 - train: epoch 0021, iter [03900, 05004], lr: 0.186692, loss: 1.9716
2022-08-05 04:13:26 - train: epoch 0021, iter [04000, 05004], lr: 0.186659, loss: 2.3036
2022-08-05 04:14:58 - train: epoch 0021, iter [04100, 05004], lr: 0.186626, loss: 1.8417
2022-08-05 04:16:31 - train: epoch 0021, iter [04200, 05004], lr: 0.186593, loss: 2.0196
2022-08-05 04:18:04 - train: epoch 0021, iter [04300, 05004], lr: 0.186560, loss: 2.1716
2022-08-05 04:19:36 - train: epoch 0021, iter [04400, 05004], lr: 0.186526, loss: 2.0501
2022-08-05 04:21:09 - train: epoch 0021, iter [04500, 05004], lr: 0.186493, loss: 1.9735
2022-08-05 04:22:42 - train: epoch 0021, iter [04600, 05004], lr: 0.186460, loss: 1.7716
2022-08-05 04:24:14 - train: epoch 0021, iter [04700, 05004], lr: 0.186427, loss: 2.0142
2022-08-05 04:25:47 - train: epoch 0021, iter [04800, 05004], lr: 0.186394, loss: 1.9265
2022-08-05 04:27:20 - train: epoch 0021, iter [04900, 05004], lr: 0.186360, loss: 1.8112
2022-08-05 04:28:52 - train: epoch 0021, iter [05000, 05004], lr: 0.186327, loss: 1.9629
2022-08-05 04:28:57 - train: epoch 021, train_loss: 1.9500
2022-08-05 04:31:00 - eval: epoch: 021, acc1: 58.610%, acc5: 83.022%, test_loss: 1.7208, per_image_load_time: 3.513ms, per_image_inference_time: 1.218ms
2022-08-05 04:31:00 - until epoch: 021, best_acc1: 60.110%
2022-08-05 04:31:00 - epoch 022 lr: 0.186325
2022-08-05 04:32:42 - train: epoch 0022, iter [00100, 05004], lr: 0.186292, loss: 1.7233
2022-08-05 04:34:14 - train: epoch 0022, iter [00200, 05004], lr: 0.186259, loss: 1.7930
2022-08-05 04:35:46 - train: epoch 0022, iter [00300, 05004], lr: 0.186225, loss: 1.7281
2022-08-05 04:37:18 - train: epoch 0022, iter [00400, 05004], lr: 0.186192, loss: 1.9506
2022-08-05 04:38:51 - train: epoch 0022, iter [00500, 05004], lr: 0.186158, loss: 1.6515
2022-08-05 04:40:23 - train: epoch 0022, iter [00600, 05004], lr: 0.186125, loss: 2.1028
2022-08-05 04:41:55 - train: epoch 0022, iter [00700, 05004], lr: 0.186091, loss: 2.0312
2022-08-05 04:43:27 - train: epoch 0022, iter [00800, 05004], lr: 0.186058, loss: 2.0396
2022-08-05 04:45:00 - train: epoch 0022, iter [00900, 05004], lr: 0.186024, loss: 1.9853
2022-08-05 04:46:32 - train: epoch 0022, iter [01000, 05004], lr: 0.185990, loss: 2.0640
2022-08-05 04:48:04 - train: epoch 0022, iter [01100, 05004], lr: 0.185956, loss: 1.8735
2022-08-05 04:49:36 - train: epoch 0022, iter [01200, 05004], lr: 0.185923, loss: 1.6155
2022-08-05 04:51:09 - train: epoch 0022, iter [01300, 05004], lr: 0.185889, loss: 1.8843
2022-08-05 04:52:41 - train: epoch 0022, iter [01400, 05004], lr: 0.185855, loss: 1.8714
2022-08-05 04:54:13 - train: epoch 0022, iter [01500, 05004], lr: 0.185821, loss: 2.2601
2022-08-05 04:55:45 - train: epoch 0022, iter [01600, 05004], lr: 0.185787, loss: 1.7758
2022-08-05 04:57:18 - train: epoch 0022, iter [01700, 05004], lr: 0.185753, loss: 2.0309
2022-08-05 04:58:50 - train: epoch 0022, iter [01800, 05004], lr: 0.185719, loss: 2.1148
2022-08-05 05:00:22 - train: epoch 0022, iter [01900, 05004], lr: 0.185685, loss: 2.0773
2022-08-05 05:01:54 - train: epoch 0022, iter [02000, 05004], lr: 0.185651, loss: 1.9016
2022-08-05 05:03:26 - train: epoch 0022, iter [02100, 05004], lr: 0.185617, loss: 1.8021
2022-08-05 05:04:59 - train: epoch 0022, iter [02200, 05004], lr: 0.185583, loss: 1.7081
2022-08-05 05:06:31 - train: epoch 0022, iter [02300, 05004], lr: 0.185548, loss: 2.1236
2022-08-05 05:08:03 - train: epoch 0022, iter [02400, 05004], lr: 0.185514, loss: 1.9024
2022-08-05 05:09:35 - train: epoch 0022, iter [02500, 05004], lr: 0.185480, loss: 2.0095
2022-08-05 05:11:08 - train: epoch 0022, iter [02600, 05004], lr: 0.185446, loss: 1.8392
2022-08-05 05:12:40 - train: epoch 0022, iter [02700, 05004], lr: 0.185411, loss: 1.8365
2022-08-05 05:14:12 - train: epoch 0022, iter [02800, 05004], lr: 0.185377, loss: 1.9651
2022-08-05 05:15:45 - train: epoch 0022, iter [02900, 05004], lr: 0.185342, loss: 1.9498
2022-08-05 05:17:17 - train: epoch 0022, iter [03000, 05004], lr: 0.185308, loss: 1.8397
2022-08-05 05:18:49 - train: epoch 0022, iter [03100, 05004], lr: 0.185274, loss: 2.0614
2022-08-05 05:20:22 - train: epoch 0022, iter [03200, 05004], lr: 0.185239, loss: 1.8596
2022-08-05 05:21:54 - train: epoch 0022, iter [03300, 05004], lr: 0.185204, loss: 1.9350
2022-08-05 05:23:27 - train: epoch 0022, iter [03400, 05004], lr: 0.185170, loss: 1.8656
2022-08-05 05:24:59 - train: epoch 0022, iter [03500, 05004], lr: 0.185135, loss: 1.9529
2022-08-05 05:26:32 - train: epoch 0022, iter [03600, 05004], lr: 0.185100, loss: 2.1633
2022-08-05 05:28:04 - train: epoch 0022, iter [03700, 05004], lr: 0.185066, loss: 2.1155
2022-08-05 05:29:37 - train: epoch 0022, iter [03800, 05004], lr: 0.185031, loss: 2.0229
2022-08-05 05:31:09 - train: epoch 0022, iter [03900, 05004], lr: 0.184996, loss: 1.8929
2022-08-05 05:32:42 - train: epoch 0022, iter [04000, 05004], lr: 0.184961, loss: 1.8744
2022-08-05 05:34:15 - train: epoch 0022, iter [04100, 05004], lr: 0.184926, loss: 2.1011
2022-08-05 05:35:47 - train: epoch 0022, iter [04200, 05004], lr: 0.184892, loss: 2.0405
2022-08-05 05:37:20 - train: epoch 0022, iter [04300, 05004], lr: 0.184857, loss: 2.0662
2022-08-05 05:38:52 - train: epoch 0022, iter [04400, 05004], lr: 0.184822, loss: 1.7896
2022-08-05 05:40:25 - train: epoch 0022, iter [04500, 05004], lr: 0.184787, loss: 1.7956
2022-08-05 05:41:57 - train: epoch 0022, iter [04600, 05004], lr: 0.184752, loss: 2.1992
2022-08-05 05:43:30 - train: epoch 0022, iter [04700, 05004], lr: 0.184716, loss: 1.9222
2022-08-05 05:45:03 - train: epoch 0022, iter [04800, 05004], lr: 0.184681, loss: 1.9099
2022-08-05 05:46:35 - train: epoch 0022, iter [04900, 05004], lr: 0.184646, loss: 1.9219
2022-08-05 05:48:08 - train: epoch 0022, iter [05000, 05004], lr: 0.184611, loss: 1.8843
2022-08-05 05:48:12 - train: epoch 022, train_loss: 1.9364
2022-08-05 05:50:17 - eval: epoch: 022, acc1: 60.178%, acc5: 83.762%, test_loss: 1.6710, per_image_load_time: 3.549ms, per_image_inference_time: 1.215ms
2022-08-05 05:50:18 - until epoch: 022, best_acc1: 60.178%
2022-08-05 05:50:18 - epoch 023 lr: 0.184609
2022-08-05 05:51:59 - train: epoch 0023, iter [00100, 05004], lr: 0.184574, loss: 1.9054
2022-08-05 05:53:32 - train: epoch 0023, iter [00200, 05004], lr: 0.184539, loss: 1.7375
2022-08-05 05:55:04 - train: epoch 0023, iter [00300, 05004], lr: 0.184504, loss: 1.9697
2022-08-05 05:56:36 - train: epoch 0023, iter [00400, 05004], lr: 0.184468, loss: 2.0436
2022-08-05 05:58:08 - train: epoch 0023, iter [00500, 05004], lr: 0.184433, loss: 1.9027
2022-08-05 05:59:41 - train: epoch 0023, iter [00600, 05004], lr: 0.184398, loss: 1.9761
2022-08-05 06:01:13 - train: epoch 0023, iter [00700, 05004], lr: 0.184362, loss: 1.6904
2022-08-05 06:02:45 - train: epoch 0023, iter [00800, 05004], lr: 0.184327, loss: 2.0071
2022-08-05 06:04:18 - train: epoch 0023, iter [00900, 05004], lr: 0.184291, loss: 1.8892
2022-08-05 06:05:50 - train: epoch 0023, iter [01000, 05004], lr: 0.184255, loss: 1.8545
2022-08-05 06:07:22 - train: epoch 0023, iter [01100, 05004], lr: 0.184220, loss: 1.8457
2022-08-05 06:08:55 - train: epoch 0023, iter [01200, 05004], lr: 0.184184, loss: 1.8000
2022-08-05 06:10:27 - train: epoch 0023, iter [01300, 05004], lr: 0.184148, loss: 1.8338
2022-08-05 06:11:59 - train: epoch 0023, iter [01400, 05004], lr: 0.184113, loss: 1.9377
2022-08-05 06:13:32 - train: epoch 0023, iter [01500, 05004], lr: 0.184077, loss: 1.5910
2022-08-05 06:15:04 - train: epoch 0023, iter [01600, 05004], lr: 0.184041, loss: 1.8182
2022-08-05 06:16:36 - train: epoch 0023, iter [01700, 05004], lr: 0.184005, loss: 1.9707
2022-08-05 06:18:09 - train: epoch 0023, iter [01800, 05004], lr: 0.183969, loss: 1.6650
2022-08-05 06:19:41 - train: epoch 0023, iter [01900, 05004], lr: 0.183934, loss: 2.2082
2022-08-05 06:21:13 - train: epoch 0023, iter [02000, 05004], lr: 0.183898, loss: 1.6697
2022-08-05 06:22:46 - train: epoch 0023, iter [02100, 05004], lr: 0.183862, loss: 1.8655
2022-08-05 06:24:18 - train: epoch 0023, iter [02200, 05004], lr: 0.183826, loss: 1.7923
2022-08-05 06:25:50 - train: epoch 0023, iter [02300, 05004], lr: 0.183790, loss: 1.8937
2022-08-05 06:27:23 - train: epoch 0023, iter [02400, 05004], lr: 0.183753, loss: 1.9282
2022-08-05 06:28:55 - train: epoch 0023, iter [02500, 05004], lr: 0.183717, loss: 1.8390
2022-08-05 06:30:28 - train: epoch 0023, iter [02600, 05004], lr: 0.183681, loss: 2.0546
2022-08-05 06:32:00 - train: epoch 0023, iter [02700, 05004], lr: 0.183645, loss: 2.0412
2022-08-05 06:33:32 - train: epoch 0023, iter [02800, 05004], lr: 0.183609, loss: 2.0221
2022-08-05 06:35:05 - train: epoch 0023, iter [02900, 05004], lr: 0.183572, loss: 1.8872
2022-08-05 06:36:37 - train: epoch 0023, iter [03000, 05004], lr: 0.183536, loss: 2.0171
2022-08-05 06:38:09 - train: epoch 0023, iter [03100, 05004], lr: 0.183500, loss: 1.8444
2022-08-05 06:39:42 - train: epoch 0023, iter [03200, 05004], lr: 0.183463, loss: 1.7725
2022-08-05 06:41:14 - train: epoch 0023, iter [03300, 05004], lr: 0.183427, loss: 1.8778
2022-08-05 06:42:47 - train: epoch 0023, iter [03400, 05004], lr: 0.183391, loss: 2.2720
2022-08-05 06:44:19 - train: epoch 0023, iter [03500, 05004], lr: 0.183354, loss: 1.8393
2022-08-05 06:45:51 - train: epoch 0023, iter [03600, 05004], lr: 0.183318, loss: 1.8402
2022-08-05 06:47:24 - train: epoch 0023, iter [03700, 05004], lr: 0.183281, loss: 1.9028
2022-08-05 06:48:56 - train: epoch 0023, iter [03800, 05004], lr: 0.183244, loss: 2.1045
2022-08-05 06:50:29 - train: epoch 0023, iter [03900, 05004], lr: 0.183208, loss: 1.8828
2022-08-05 06:52:01 - train: epoch 0023, iter [04000, 05004], lr: 0.183171, loss: 1.7104
2022-08-05 06:53:33 - train: epoch 0023, iter [04100, 05004], lr: 0.183134, loss: 1.6725
2022-08-05 06:55:06 - train: epoch 0023, iter [04200, 05004], lr: 0.183098, loss: 1.9100
2022-08-05 06:56:38 - train: epoch 0023, iter [04300, 05004], lr: 0.183061, loss: 1.9872
2022-08-05 06:58:10 - train: epoch 0023, iter [04400, 05004], lr: 0.183024, loss: 1.9594
2022-08-05 06:59:43 - train: epoch 0023, iter [04500, 05004], lr: 0.182987, loss: 1.9418
2022-08-05 07:01:15 - train: epoch 0023, iter [04600, 05004], lr: 0.182950, loss: 2.0263
2022-08-05 07:02:47 - train: epoch 0023, iter [04700, 05004], lr: 0.182913, loss: 1.8286
2022-08-05 07:04:20 - train: epoch 0023, iter [04800, 05004], lr: 0.182876, loss: 1.8090
2022-08-05 07:05:52 - train: epoch 0023, iter [04900, 05004], lr: 0.182839, loss: 1.9741
2022-08-05 07:07:25 - train: epoch 0023, iter [05000, 05004], lr: 0.182802, loss: 2.0283
2022-08-05 07:07:29 - train: epoch 023, train_loss: 1.9245
2022-08-05 07:09:32 - eval: epoch: 023, acc1: 60.390%, acc5: 83.936%, test_loss: 1.6589, per_image_load_time: 3.474ms, per_image_inference_time: 1.221ms
2022-08-05 07:09:33 - until epoch: 023, best_acc1: 60.390%
2022-08-05 07:09:33 - epoch 024 lr: 0.182801
2022-08-05 07:11:14 - train: epoch 0024, iter [00100, 05004], lr: 0.182764, loss: 2.0698
2022-08-05 07:12:46 - train: epoch 0024, iter [00200, 05004], lr: 0.182727, loss: 1.9766
2022-08-05 07:14:19 - train: epoch 0024, iter [00300, 05004], lr: 0.182690, loss: 1.8068
2022-08-05 07:15:51 - train: epoch 0024, iter [00400, 05004], lr: 0.182652, loss: 1.8569
2022-08-05 07:17:23 - train: epoch 0024, iter [00500, 05004], lr: 0.182615, loss: 2.0144
2022-08-05 07:18:55 - train: epoch 0024, iter [00600, 05004], lr: 0.182578, loss: 1.8775
2022-08-05 07:20:28 - train: epoch 0024, iter [00700, 05004], lr: 0.182541, loss: 2.0832
2022-08-05 07:22:00 - train: epoch 0024, iter [00800, 05004], lr: 0.182503, loss: 1.9327
2022-08-05 07:23:32 - train: epoch 0024, iter [00900, 05004], lr: 0.182466, loss: 1.8327
2022-08-05 07:25:04 - train: epoch 0024, iter [01000, 05004], lr: 0.182429, loss: 2.0275
2022-08-05 07:26:37 - train: epoch 0024, iter [01100, 05004], lr: 0.182391, loss: 1.6648
2022-08-05 07:28:09 - train: epoch 0024, iter [01200, 05004], lr: 0.182354, loss: 1.8648
2022-08-05 07:29:41 - train: epoch 0024, iter [01300, 05004], lr: 0.182316, loss: 1.9962
2022-08-05 07:31:14 - train: epoch 0024, iter [01400, 05004], lr: 0.182279, loss: 1.7836
2022-08-05 07:32:46 - train: epoch 0024, iter [01500, 05004], lr: 0.182241, loss: 1.9613
2022-08-05 07:34:19 - train: epoch 0024, iter [01600, 05004], lr: 0.182203, loss: 2.0326
2022-08-05 07:35:51 - train: epoch 0024, iter [01700, 05004], lr: 0.182166, loss: 1.7411
2022-08-05 07:37:23 - train: epoch 0024, iter [01800, 05004], lr: 0.182128, loss: 2.0801
2022-08-05 07:38:56 - train: epoch 0024, iter [01900, 05004], lr: 0.182090, loss: 1.7216
2022-08-05 07:40:28 - train: epoch 0024, iter [02000, 05004], lr: 0.182053, loss: 1.8819
2022-08-05 07:42:00 - train: epoch 0024, iter [02100, 05004], lr: 0.182015, loss: 1.9339
2022-08-05 07:43:33 - train: epoch 0024, iter [02200, 05004], lr: 0.181977, loss: 1.8433
2022-08-05 07:45:05 - train: epoch 0024, iter [02300, 05004], lr: 0.181939, loss: 1.9405
2022-08-05 07:46:38 - train: epoch 0024, iter [02400, 05004], lr: 0.181901, loss: 2.1507
2022-08-05 07:48:10 - train: epoch 0024, iter [02500, 05004], lr: 0.181863, loss: 2.0039
2022-08-05 07:49:42 - train: epoch 0024, iter [02600, 05004], lr: 0.181825, loss: 2.1056
2022-08-05 07:51:15 - train: epoch 0024, iter [02700, 05004], lr: 0.181787, loss: 2.0340
2022-08-05 07:52:47 - train: epoch 0024, iter [02800, 05004], lr: 0.181749, loss: 2.1205
2022-08-05 07:54:20 - train: epoch 0024, iter [02900, 05004], lr: 0.181711, loss: 2.0160
2022-08-05 07:55:52 - train: epoch 0024, iter [03000, 05004], lr: 0.181673, loss: 1.8483
2022-08-05 07:57:25 - train: epoch 0024, iter [03100, 05004], lr: 0.181635, loss: 1.9067
2022-08-05 07:58:57 - train: epoch 0024, iter [03200, 05004], lr: 0.181597, loss: 2.1215
2022-08-05 08:00:30 - train: epoch 0024, iter [03300, 05004], lr: 0.181558, loss: 1.7990
2022-08-05 08:02:03 - train: epoch 0024, iter [03400, 05004], lr: 0.181520, loss: 1.8744
2022-08-05 08:03:35 - train: epoch 0024, iter [03500, 05004], lr: 0.181482, loss: 1.7529
2022-08-05 08:05:08 - train: epoch 0024, iter [03600, 05004], lr: 0.181444, loss: 1.9488
2022-08-05 08:06:41 - train: epoch 0024, iter [03700, 05004], lr: 0.181405, loss: 1.9352
2022-08-05 08:08:14 - train: epoch 0024, iter [03800, 05004], lr: 0.181367, loss: 1.8757
2022-08-05 08:09:47 - train: epoch 0024, iter [03900, 05004], lr: 0.181328, loss: 1.8131
2022-08-05 08:11:20 - train: epoch 0024, iter [04000, 05004], lr: 0.181290, loss: 1.9741
2022-08-05 08:12:52 - train: epoch 0024, iter [04100, 05004], lr: 0.181251, loss: 1.9283
2022-08-05 08:14:25 - train: epoch 0024, iter [04200, 05004], lr: 0.181213, loss: 1.6931
2022-08-05 08:15:58 - train: epoch 0024, iter [04300, 05004], lr: 0.181174, loss: 1.8391
2022-08-05 08:17:31 - train: epoch 0024, iter [04400, 05004], lr: 0.181136, loss: 1.9632
2022-08-05 08:19:03 - train: epoch 0024, iter [04500, 05004], lr: 0.181097, loss: 1.7009
2022-08-05 08:20:36 - train: epoch 0024, iter [04600, 05004], lr: 0.181058, loss: 1.8088
2022-08-05 08:22:09 - train: epoch 0024, iter [04700, 05004], lr: 0.181020, loss: 1.8421
2022-08-05 08:23:41 - train: epoch 0024, iter [04800, 05004], lr: 0.180981, loss: 1.5755
2022-08-05 08:25:14 - train: epoch 0024, iter [04900, 05004], lr: 0.180942, loss: 1.9101
2022-08-05 08:26:46 - train: epoch 0024, iter [05000, 05004], lr: 0.180903, loss: 1.9866
2022-08-05 08:26:51 - train: epoch 024, train_loss: 1.9090
2022-08-05 08:28:56 - eval: epoch: 024, acc1: 60.950%, acc5: 84.216%, test_loss: 1.6257, per_image_load_time: 3.622ms, per_image_inference_time: 1.173ms
2022-08-05 08:28:56 - until epoch: 024, best_acc1: 60.950%
2022-08-05 08:28:56 - epoch 025 lr: 0.180901
2022-08-05 08:30:37 - train: epoch 0025, iter [00100, 05004], lr: 0.180863, loss: 1.8251
2022-08-05 08:32:10 - train: epoch 0025, iter [00200, 05004], lr: 0.180824, loss: 1.5406
2022-08-05 08:33:42 - train: epoch 0025, iter [00300, 05004], lr: 0.180785, loss: 1.9126
2022-08-05 08:35:14 - train: epoch 0025, iter [00400, 05004], lr: 0.180746, loss: 1.9138
2022-08-05 08:36:47 - train: epoch 0025, iter [00500, 05004], lr: 0.180707, loss: 1.7632
2022-08-05 08:38:19 - train: epoch 0025, iter [00600, 05004], lr: 0.180668, loss: 1.8698
2022-08-05 08:39:52 - train: epoch 0025, iter [00700, 05004], lr: 0.180629, loss: 2.0510
2022-08-05 08:41:24 - train: epoch 0025, iter [00800, 05004], lr: 0.180590, loss: 1.9798
2022-08-05 08:42:57 - train: epoch 0025, iter [00900, 05004], lr: 0.180551, loss: 1.6675
2022-08-05 08:44:29 - train: epoch 0025, iter [01000, 05004], lr: 0.180511, loss: 1.9795
2022-08-05 08:46:02 - train: epoch 0025, iter [01100, 05004], lr: 0.180472, loss: 1.8934
2022-08-05 08:47:34 - train: epoch 0025, iter [01200, 05004], lr: 0.180433, loss: 1.9982
2022-08-05 08:49:07 - train: epoch 0025, iter [01300, 05004], lr: 0.180394, loss: 1.9221
2022-08-05 08:50:39 - train: epoch 0025, iter [01400, 05004], lr: 0.180354, loss: 1.8646
2022-08-05 08:52:12 - train: epoch 0025, iter [01500, 05004], lr: 0.180315, loss: 1.9286
2022-08-05 08:53:45 - train: epoch 0025, iter [01600, 05004], lr: 0.180276, loss: 1.8153
2022-08-05 08:55:17 - train: epoch 0025, iter [01700, 05004], lr: 0.180236, loss: 1.9601
2022-08-05 08:56:50 - train: epoch 0025, iter [01800, 05004], lr: 0.180197, loss: 1.6355
2022-08-05 08:58:22 - train: epoch 0025, iter [01900, 05004], lr: 0.180157, loss: 1.8244
2022-08-05 08:59:55 - train: epoch 0025, iter [02000, 05004], lr: 0.180118, loss: 2.0982
2022-08-05 09:01:28 - train: epoch 0025, iter [02100, 05004], lr: 0.180078, loss: 1.5815
2022-08-05 09:03:00 - train: epoch 0025, iter [02200, 05004], lr: 0.180039, loss: 1.6422
2022-08-05 09:04:33 - train: epoch 0025, iter [02300, 05004], lr: 0.179999, loss: 1.8727
2022-08-05 09:06:05 - train: epoch 0025, iter [02400, 05004], lr: 0.179959, loss: 1.7272
2022-08-05 09:07:38 - train: epoch 0025, iter [02500, 05004], lr: 0.179920, loss: 1.9515
2022-08-05 09:09:10 - train: epoch 0025, iter [02600, 05004], lr: 0.179880, loss: 1.9578
2022-08-05 09:10:43 - train: epoch 0025, iter [02700, 05004], lr: 0.179840, loss: 1.9606
2022-08-05 09:12:16 - train: epoch 0025, iter [02800, 05004], lr: 0.179800, loss: 1.9315
2022-08-05 09:13:48 - train: epoch 0025, iter [02900, 05004], lr: 0.179760, loss: 2.0004
2022-08-05 09:15:21 - train: epoch 0025, iter [03000, 05004], lr: 0.179721, loss: 1.9983
2022-08-05 09:16:53 - train: epoch 0025, iter [03100, 05004], lr: 0.179681, loss: 1.9949
2022-08-05 09:18:26 - train: epoch 0025, iter [03200, 05004], lr: 0.179641, loss: 1.8998
2022-08-05 09:19:58 - train: epoch 0025, iter [03300, 05004], lr: 0.179601, loss: 1.7202
2022-08-05 09:21:31 - train: epoch 0025, iter [03400, 05004], lr: 0.179561, loss: 2.0628
2022-08-05 09:23:03 - train: epoch 0025, iter [03500, 05004], lr: 0.179521, loss: 1.6457
2022-08-05 09:24:36 - train: epoch 0025, iter [03600, 05004], lr: 0.179481, loss: 1.8505
2022-08-05 09:26:09 - train: epoch 0025, iter [03700, 05004], lr: 0.179440, loss: 2.0156
2022-08-05 09:27:42 - train: epoch 0025, iter [03800, 05004], lr: 0.179400, loss: 1.9800
2022-08-05 09:29:14 - train: epoch 0025, iter [03900, 05004], lr: 0.179360, loss: 2.0427
2022-08-05 09:30:47 - train: epoch 0025, iter [04000, 05004], lr: 0.179320, loss: 1.9959
2022-08-05 09:32:20 - train: epoch 0025, iter [04100, 05004], lr: 0.179280, loss: 2.0890
2022-08-05 09:33:52 - train: epoch 0025, iter [04200, 05004], lr: 0.179239, loss: 1.9327
2022-08-05 09:35:25 - train: epoch 0025, iter [04300, 05004], lr: 0.179199, loss: 1.8183
2022-08-05 09:36:58 - train: epoch 0025, iter [04400, 05004], lr: 0.179159, loss: 1.6977
2022-08-05 09:38:31 - train: epoch 0025, iter [04500, 05004], lr: 0.179118, loss: 1.8780
2022-08-05 09:40:04 - train: epoch 0025, iter [04600, 05004], lr: 0.179078, loss: 1.7180
2022-08-05 09:41:37 - train: epoch 0025, iter [04700, 05004], lr: 0.179037, loss: 1.8479
2022-08-05 09:43:10 - train: epoch 0025, iter [04800, 05004], lr: 0.178997, loss: 1.8252
2022-08-05 09:44:43 - train: epoch 0025, iter [04900, 05004], lr: 0.178956, loss: 2.0859
2022-08-05 09:46:16 - train: epoch 0025, iter [05000, 05004], lr: 0.178916, loss: 2.0352
2022-08-05 09:46:20 - train: epoch 025, train_loss: 1.8997
2022-08-05 09:48:18 - eval: epoch: 025, acc1: 60.782%, acc5: 84.226%, test_loss: 1.6417, per_image_load_time: 3.337ms, per_image_inference_time: 1.147ms
2022-08-05 09:48:18 - until epoch: 025, best_acc1: 60.950%
2022-08-05 09:48:18 - epoch 026 lr: 0.178914
2022-08-05 09:49:59 - train: epoch 0026, iter [00100, 05004], lr: 0.178873, loss: 1.7841
2022-08-05 09:51:32 - train: epoch 0026, iter [00200, 05004], lr: 0.178833, loss: 1.7657
2022-08-05 09:53:05 - train: epoch 0026, iter [00300, 05004], lr: 0.178792, loss: 1.9437
2022-08-05 09:54:38 - train: epoch 0026, iter [00400, 05004], lr: 0.178751, loss: 1.7479
2022-08-05 09:56:10 - train: epoch 0026, iter [00500, 05004], lr: 0.178711, loss: 1.9023
2022-08-05 09:57:43 - train: epoch 0026, iter [00600, 05004], lr: 0.178670, loss: 2.0591
2022-08-05 09:59:16 - train: epoch 0026, iter [00700, 05004], lr: 0.178629, loss: 2.2636
2022-08-05 10:00:48 - train: epoch 0026, iter [00800, 05004], lr: 0.178588, loss: 1.6501
2022-08-05 10:02:21 - train: epoch 0026, iter [00900, 05004], lr: 0.178547, loss: 1.9029
2022-08-05 10:03:54 - train: epoch 0026, iter [01000, 05004], lr: 0.178506, loss: 1.8279
2022-08-05 10:05:26 - train: epoch 0026, iter [01100, 05004], lr: 0.178465, loss: 1.9958
2022-08-05 10:06:59 - train: epoch 0026, iter [01200, 05004], lr: 0.178424, loss: 1.8630
2022-08-05 10:08:32 - train: epoch 0026, iter [01300, 05004], lr: 0.178383, loss: 1.8115
2022-08-05 10:10:05 - train: epoch 0026, iter [01400, 05004], lr: 0.178342, loss: 1.9198
2022-08-05 10:11:38 - train: epoch 0026, iter [01500, 05004], lr: 0.178301, loss: 1.8853
2022-08-05 10:13:11 - train: epoch 0026, iter [01600, 05004], lr: 0.178260, loss: 1.7542
2022-08-05 10:14:44 - train: epoch 0026, iter [01700, 05004], lr: 0.178219, loss: 1.8736
2022-08-05 10:16:16 - train: epoch 0026, iter [01800, 05004], lr: 0.178178, loss: 1.9747
2022-08-05 10:17:49 - train: epoch 0026, iter [01900, 05004], lr: 0.178137, loss: 2.0767
2022-08-05 10:19:21 - train: epoch 0026, iter [02000, 05004], lr: 0.178095, loss: 1.8779
2022-08-05 10:20:54 - train: epoch 0026, iter [02100, 05004], lr: 0.178054, loss: 1.8157
2022-08-05 10:22:26 - train: epoch 0026, iter [02200, 05004], lr: 0.178013, loss: 1.8336
2022-08-05 10:23:59 - train: epoch 0026, iter [02300, 05004], lr: 0.177971, loss: 2.0479
2022-08-05 10:25:32 - train: epoch 0026, iter [02400, 05004], lr: 0.177930, loss: 2.0451
2022-08-05 10:27:05 - train: epoch 0026, iter [02500, 05004], lr: 0.177889, loss: 2.0323
2022-08-05 10:28:37 - train: epoch 0026, iter [02600, 05004], lr: 0.177847, loss: 1.9169
2022-08-05 10:30:10 - train: epoch 0026, iter [02700, 05004], lr: 0.177806, loss: 1.8351
2022-08-05 10:31:43 - train: epoch 0026, iter [02800, 05004], lr: 0.177764, loss: 1.8857
2022-08-05 10:33:16 - train: epoch 0026, iter [02900, 05004], lr: 0.177722, loss: 1.9316
2022-08-05 10:34:49 - train: epoch 0026, iter [03000, 05004], lr: 0.177681, loss: 1.9407
2022-08-05 10:36:22 - train: epoch 0026, iter [03100, 05004], lr: 0.177639, loss: 1.9341
2022-08-05 10:37:55 - train: epoch 0026, iter [03200, 05004], lr: 0.177598, loss: 1.9644
2022-08-05 10:39:28 - train: epoch 0026, iter [03300, 05004], lr: 0.177556, loss: 1.9592
2022-08-05 10:41:00 - train: epoch 0026, iter [03400, 05004], lr: 0.177514, loss: 1.9938
2022-08-05 10:42:33 - train: epoch 0026, iter [03500, 05004], lr: 0.177472, loss: 2.0636
2022-08-05 10:44:06 - train: epoch 0026, iter [03600, 05004], lr: 0.177431, loss: 1.9380
2022-08-05 10:45:38 - train: epoch 0026, iter [03700, 05004], lr: 0.177389, loss: 1.8764
2022-08-05 10:47:11 - train: epoch 0026, iter [03800, 05004], lr: 0.177347, loss: 2.1115
2022-08-05 10:48:43 - train: epoch 0026, iter [03900, 05004], lr: 0.177305, loss: 2.1523
2022-08-05 10:50:16 - train: epoch 0026, iter [04000, 05004], lr: 0.177263, loss: 2.0422
2022-08-05 10:51:49 - train: epoch 0026, iter [04100, 05004], lr: 0.177221, loss: 1.7852
2022-08-05 10:53:22 - train: epoch 0026, iter [04200, 05004], lr: 0.177179, loss: 1.8574
2022-08-05 10:54:55 - train: epoch 0026, iter [04300, 05004], lr: 0.177137, loss: 1.9603
2022-08-05 10:56:27 - train: epoch 0026, iter [04400, 05004], lr: 0.177095, loss: 2.0352
2022-08-05 10:58:00 - train: epoch 0026, iter [04500, 05004], lr: 0.177053, loss: 1.9546
2022-08-05 10:59:33 - train: epoch 0026, iter [04600, 05004], lr: 0.177011, loss: 1.9716
2022-08-05 11:01:06 - train: epoch 0026, iter [04700, 05004], lr: 0.176969, loss: 1.7941
2022-08-05 11:02:39 - train: epoch 0026, iter [04800, 05004], lr: 0.176926, loss: 1.9025
2022-08-05 11:04:12 - train: epoch 0026, iter [04900, 05004], lr: 0.176884, loss: 1.8164
2022-08-05 11:05:44 - train: epoch 0026, iter [05000, 05004], lr: 0.176842, loss: 2.0059
2022-08-05 11:05:49 - train: epoch 026, train_loss: 1.8876
2022-08-05 11:07:49 - eval: epoch: 026, acc1: 60.686%, acc5: 84.250%, test_loss: 1.6309, per_image_load_time: 3.396ms, per_image_inference_time: 1.203ms
2022-08-05 11:07:50 - until epoch: 026, best_acc1: 60.950%
2022-08-05 11:07:50 - epoch 027 lr: 0.176840
2022-08-05 11:09:31 - train: epoch 0027, iter [00100, 05004], lr: 0.176798, loss: 1.6999
2022-08-05 11:11:03 - train: epoch 0027, iter [00200, 05004], lr: 0.176755, loss: 1.8706
2022-08-05 11:12:36 - train: epoch 0027, iter [00300, 05004], lr: 0.176713, loss: 1.8494
2022-08-05 11:14:08 - train: epoch 0027, iter [00400, 05004], lr: 0.176671, loss: 1.9568
2022-08-05 11:15:40 - train: epoch 0027, iter [00500, 05004], lr: 0.176628, loss: 2.0440
2022-08-05 11:17:13 - train: epoch 0027, iter [00600, 05004], lr: 0.176586, loss: 1.9117
2022-08-05 11:18:45 - train: epoch 0027, iter [00700, 05004], lr: 0.176543, loss: 1.9787
2022-08-05 11:20:18 - train: epoch 0027, iter [00800, 05004], lr: 0.176501, loss: 1.8377
2022-08-05 11:21:50 - train: epoch 0027, iter [00900, 05004], lr: 0.176458, loss: 1.8051
2022-08-05 11:23:23 - train: epoch 0027, iter [01000, 05004], lr: 0.176416, loss: 2.0579
2022-08-05 11:24:56 - train: epoch 0027, iter [01100, 05004], lr: 0.176373, loss: 1.8367
2022-08-05 11:26:28 - train: epoch 0027, iter [01200, 05004], lr: 0.176330, loss: 1.9858
2022-08-05 11:28:01 - train: epoch 0027, iter [01300, 05004], lr: 0.176287, loss: 1.7713
2022-08-05 11:29:33 - train: epoch 0027, iter [01400, 05004], lr: 0.176245, loss: 1.8563
2022-08-05 11:31:06 - train: epoch 0027, iter [01500, 05004], lr: 0.176202, loss: 2.0185
2022-08-05 11:32:38 - train: epoch 0027, iter [01600, 05004], lr: 0.176159, loss: 1.8271
2022-08-05 11:34:11 - train: epoch 0027, iter [01700, 05004], lr: 0.176116, loss: 1.7237
2022-08-05 11:35:44 - train: epoch 0027, iter [01800, 05004], lr: 0.176073, loss: 1.7180
2022-08-05 11:37:16 - train: epoch 0027, iter [01900, 05004], lr: 0.176031, loss: 1.8656
2022-08-05 11:38:49 - train: epoch 0027, iter [02000, 05004], lr: 0.175988, loss: 1.8032
2022-08-05 11:40:21 - train: epoch 0027, iter [02100, 05004], lr: 0.175945, loss: 1.8848
2022-08-05 11:41:54 - train: epoch 0027, iter [02200, 05004], lr: 0.175902, loss: 2.1363
2022-08-05 11:43:26 - train: epoch 0027, iter [02300, 05004], lr: 0.175859, loss: 2.1398
2022-08-05 11:44:59 - train: epoch 0027, iter [02400, 05004], lr: 0.175815, loss: 1.7950
2022-08-05 11:46:31 - train: epoch 0027, iter [02500, 05004], lr: 0.175772, loss: 1.8723
2022-08-05 11:48:04 - train: epoch 0027, iter [02600, 05004], lr: 0.175729, loss: 1.8642
2022-08-05 11:49:37 - train: epoch 0027, iter [02700, 05004], lr: 0.175686, loss: 1.8519
2022-08-05 11:51:09 - train: epoch 0027, iter [02800, 05004], lr: 0.175643, loss: 1.8728
2022-08-05 11:52:42 - train: epoch 0027, iter [02900, 05004], lr: 0.175600, loss: 1.7723
2022-08-05 11:54:15 - train: epoch 0027, iter [03000, 05004], lr: 0.175556, loss: 1.7863
2022-08-05 11:55:48 - train: epoch 0027, iter [03100, 05004], lr: 0.175513, loss: 1.6726
2022-08-05 11:57:21 - train: epoch 0027, iter [03200, 05004], lr: 0.175470, loss: 1.7788
2022-08-05 11:58:53 - train: epoch 0027, iter [03300, 05004], lr: 0.175426, loss: 2.0458
2022-08-05 12:00:26 - train: epoch 0027, iter [03400, 05004], lr: 0.175383, loss: 1.8621
2022-08-05 12:01:59 - train: epoch 0027, iter [03500, 05004], lr: 0.175339, loss: 2.0418
2022-08-05 12:03:32 - train: epoch 0027, iter [03600, 05004], lr: 0.175296, loss: 2.0639
2022-08-05 12:05:04 - train: epoch 0027, iter [03700, 05004], lr: 0.175252, loss: 1.8008
2022-08-05 12:06:37 - train: epoch 0027, iter [03800, 05004], lr: 0.175209, loss: 1.7548
2022-08-05 12:08:10 - train: epoch 0027, iter [03900, 05004], lr: 0.175165, loss: 1.8847
2022-08-05 12:09:43 - train: epoch 0027, iter [04000, 05004], lr: 0.175122, loss: 1.9415
2022-08-05 12:11:15 - train: epoch 0027, iter [04100, 05004], lr: 0.175078, loss: 1.9208
2022-08-05 12:12:48 - train: epoch 0027, iter [04200, 05004], lr: 0.175034, loss: 1.8918
2022-08-05 12:14:20 - train: epoch 0027, iter [04300, 05004], lr: 0.174991, loss: 1.8777
2022-08-05 12:15:53 - train: epoch 0027, iter [04400, 05004], lr: 0.174947, loss: 1.8952
2022-08-05 12:17:25 - train: epoch 0027, iter [04500, 05004], lr: 0.174903, loss: 1.6614
2022-08-05 12:18:58 - train: epoch 0027, iter [04600, 05004], lr: 0.174859, loss: 1.5831
2022-08-05 12:20:30 - train: epoch 0027, iter [04700, 05004], lr: 0.174816, loss: 1.9309
2022-08-05 12:22:03 - train: epoch 0027, iter [04800, 05004], lr: 0.174772, loss: 2.1795
2022-08-05 12:23:35 - train: epoch 0027, iter [04900, 05004], lr: 0.174728, loss: 1.8272
2022-08-05 12:25:08 - train: epoch 0027, iter [05000, 05004], lr: 0.174684, loss: 1.6603
2022-08-05 12:25:12 - train: epoch 027, train_loss: 1.8741
2022-08-05 12:27:15 - eval: epoch: 027, acc1: 61.618%, acc5: 84.758%, test_loss: 1.5913, per_image_load_time: 3.462ms, per_image_inference_time: 1.210ms
2022-08-05 12:27:15 - until epoch: 027, best_acc1: 61.618%
2022-08-05 12:27:15 - epoch 028 lr: 0.174682
2022-08-05 12:28:56 - train: epoch 0028, iter [00100, 05004], lr: 0.174638, loss: 1.7509
2022-08-05 12:30:29 - train: epoch 0028, iter [00200, 05004], lr: 0.174594, loss: 1.6444
2022-08-05 12:32:01 - train: epoch 0028, iter [00300, 05004], lr: 0.174550, loss: 1.7862
2022-08-05 12:33:34 - train: epoch 0028, iter [00400, 05004], lr: 0.174506, loss: 1.9468
2022-08-05 12:35:06 - train: epoch 0028, iter [00500, 05004], lr: 0.174462, loss: 1.9611
2022-08-05 12:36:39 - train: epoch 0028, iter [00600, 05004], lr: 0.174418, loss: 2.0482
2022-08-05 12:38:11 - train: epoch 0028, iter [00700, 05004], lr: 0.174374, loss: 1.9492
2022-08-05 12:39:44 - train: epoch 0028, iter [00800, 05004], lr: 0.174330, loss: 1.5364
2022-08-05 12:41:17 - train: epoch 0028, iter [00900, 05004], lr: 0.174285, loss: 1.8358
2022-08-05 12:42:49 - train: epoch 0028, iter [01000, 05004], lr: 0.174241, loss: 2.2826
2022-08-05 12:44:22 - train: epoch 0028, iter [01100, 05004], lr: 0.174197, loss: 1.9539
2022-08-05 12:45:55 - train: epoch 0028, iter [01200, 05004], lr: 0.174152, loss: 1.7642
2022-08-05 12:47:28 - train: epoch 0028, iter [01300, 05004], lr: 0.174108, loss: 1.6713
2022-08-05 12:49:01 - train: epoch 0028, iter [01400, 05004], lr: 0.174064, loss: 2.0282
2022-08-05 12:50:33 - train: epoch 0028, iter [01500, 05004], lr: 0.174019, loss: 1.7147
2022-08-05 12:52:06 - train: epoch 0028, iter [01600, 05004], lr: 0.173975, loss: 1.8999
2022-08-05 12:53:39 - train: epoch 0028, iter [01700, 05004], lr: 0.173930, loss: 1.8511
2022-08-05 12:55:12 - train: epoch 0028, iter [01800, 05004], lr: 0.173886, loss: 1.7947
2022-08-05 12:56:44 - train: epoch 0028, iter [01900, 05004], lr: 0.173841, loss: 1.9272
2022-08-05 12:58:17 - train: epoch 0028, iter [02000, 05004], lr: 0.173797, loss: 2.0408
2022-08-05 12:59:50 - train: epoch 0028, iter [02100, 05004], lr: 0.173752, loss: 2.0306
2022-08-05 13:01:23 - train: epoch 0028, iter [02200, 05004], lr: 0.173707, loss: 1.9574
2022-08-05 13:02:56 - train: epoch 0028, iter [02300, 05004], lr: 0.173663, loss: 2.1014
2022-08-05 13:04:29 - train: epoch 0028, iter [02400, 05004], lr: 0.173618, loss: 2.0614
2022-08-05 13:06:01 - train: epoch 0028, iter [02500, 05004], lr: 0.173573, loss: 1.8304
2022-08-05 13:07:34 - train: epoch 0028, iter [02600, 05004], lr: 0.173529, loss: 1.7354
2022-08-05 13:09:07 - train: epoch 0028, iter [02700, 05004], lr: 0.173484, loss: 1.9039
2022-08-05 13:10:40 - train: epoch 0028, iter [02800, 05004], lr: 0.173439, loss: 1.8081
2022-08-05 13:12:13 - train: epoch 0028, iter [02900, 05004], lr: 0.173394, loss: 1.9138
2022-08-05 13:13:46 - train: epoch 0028, iter [03000, 05004], lr: 0.173349, loss: 1.8046
2022-08-05 13:15:19 - train: epoch 0028, iter [03100, 05004], lr: 0.173304, loss: 2.0621
2022-08-05 13:16:52 - train: epoch 0028, iter [03200, 05004], lr: 0.173259, loss: 1.7371
2022-08-05 13:18:25 - train: epoch 0028, iter [03300, 05004], lr: 0.173214, loss: 1.7858
2022-08-05 13:19:58 - train: epoch 0028, iter [03400, 05004], lr: 0.173169, loss: 2.0614
2022-08-05 13:21:31 - train: epoch 0028, iter [03500, 05004], lr: 0.173124, loss: 1.9121
2022-08-05 13:23:04 - train: epoch 0028, iter [03600, 05004], lr: 0.173079, loss: 1.7943
2022-08-05 13:24:37 - train: epoch 0028, iter [03700, 05004], lr: 0.173034, loss: 1.9401
2022-08-05 13:26:10 - train: epoch 0028, iter [03800, 05004], lr: 0.172989, loss: 1.5967
2022-08-05 13:27:43 - train: epoch 0028, iter [03900, 05004], lr: 0.172944, loss: 1.9555
2022-08-05 13:29:16 - train: epoch 0028, iter [04000, 05004], lr: 0.172898, loss: 1.8785
2022-08-05 13:30:49 - train: epoch 0028, iter [04100, 05004], lr: 0.172853, loss: 1.7624
2022-08-05 13:32:22 - train: epoch 0028, iter [04200, 05004], lr: 0.172808, loss: 2.0947
2022-08-05 13:33:55 - train: epoch 0028, iter [04300, 05004], lr: 0.172762, loss: 1.8592
2022-08-05 13:35:28 - train: epoch 0028, iter [04400, 05004], lr: 0.172717, loss: 1.7705
2022-08-05 13:37:01 - train: epoch 0028, iter [04500, 05004], lr: 0.172672, loss: 1.8367
2022-08-05 13:38:34 - train: epoch 0028, iter [04600, 05004], lr: 0.172626, loss: 1.9316
2022-08-05 13:40:07 - train: epoch 0028, iter [04700, 05004], lr: 0.172581, loss: 2.0421
2022-08-05 13:41:40 - train: epoch 0028, iter [04800, 05004], lr: 0.172535, loss: 1.6757
2022-08-05 13:43:13 - train: epoch 0028, iter [04900, 05004], lr: 0.172490, loss: 1.7813
2022-08-05 13:44:46 - train: epoch 0028, iter [05000, 05004], lr: 0.172444, loss: 1.6142
2022-08-05 13:44:50 - train: epoch 028, train_loss: 1.8622
2022-08-05 13:46:46 - eval: epoch: 028, acc1: 61.536%, acc5: 84.962%, test_loss: 1.5966, per_image_load_time: 3.334ms, per_image_inference_time: 1.095ms
2022-08-05 13:46:46 - until epoch: 028, best_acc1: 61.618%
2022-08-05 13:46:46 - epoch 029 lr: 0.172442
2022-08-05 13:48:27 - train: epoch 0029, iter [00100, 05004], lr: 0.172397, loss: 1.7821
2022-08-05 13:49:59 - train: epoch 0029, iter [00200, 05004], lr: 0.172351, loss: 2.0443
2022-08-05 13:51:32 - train: epoch 0029, iter [00300, 05004], lr: 0.172306, loss: 1.8230
2022-08-05 13:53:05 - train: epoch 0029, iter [00400, 05004], lr: 0.172260, loss: 1.9454
2022-08-05 13:54:37 - train: epoch 0029, iter [00500, 05004], lr: 0.172214, loss: 1.8741
2022-08-05 13:56:10 - train: epoch 0029, iter [00600, 05004], lr: 0.172169, loss: 1.7065
2022-08-05 13:57:42 - train: epoch 0029, iter [00700, 05004], lr: 0.172123, loss: 1.6409
2022-08-05 13:59:15 - train: epoch 0029, iter [00800, 05004], lr: 0.172077, loss: 1.9070
2022-08-05 14:00:47 - train: epoch 0029, iter [00900, 05004], lr: 0.172031, loss: 2.1087
2022-08-05 14:02:19 - train: epoch 0029, iter [01000, 05004], lr: 0.171985, loss: 1.6685
2022-08-05 14:03:52 - train: epoch 0029, iter [01100, 05004], lr: 0.171939, loss: 1.7537
2022-08-05 14:05:24 - train: epoch 0029, iter [01200, 05004], lr: 0.171894, loss: 1.8329
2022-08-05 14:06:57 - train: epoch 0029, iter [01300, 05004], lr: 0.171848, loss: 1.8853
2022-08-05 14:08:29 - train: epoch 0029, iter [01400, 05004], lr: 0.171802, loss: 1.8519
2022-08-05 14:10:01 - train: epoch 0029, iter [01500, 05004], lr: 0.171756, loss: 2.0642
2022-08-05 14:11:34 - train: epoch 0029, iter [01600, 05004], lr: 0.171710, loss: 1.8877
2022-08-05 14:13:07 - train: epoch 0029, iter [01700, 05004], lr: 0.171664, loss: 1.8101
2022-08-05 14:14:39 - train: epoch 0029, iter [01800, 05004], lr: 0.171617, loss: 1.8528
2022-08-05 14:16:12 - train: epoch 0029, iter [01900, 05004], lr: 0.171571, loss: 1.5817
2022-08-05 14:17:44 - train: epoch 0029, iter [02000, 05004], lr: 0.171525, loss: 1.9053
2022-08-05 14:19:17 - train: epoch 0029, iter [02100, 05004], lr: 0.171479, loss: 1.8219
2022-08-05 14:20:50 - train: epoch 0029, iter [02200, 05004], lr: 0.171433, loss: 1.8131
2022-08-05 14:22:22 - train: epoch 0029, iter [02300, 05004], lr: 0.171386, loss: 1.8939
2022-08-05 14:23:55 - train: epoch 0029, iter [02400, 05004], lr: 0.171340, loss: 1.6313
2022-08-05 14:25:28 - train: epoch 0029, iter [02500, 05004], lr: 0.171294, loss: 1.8411
2022-08-05 14:27:01 - train: epoch 0029, iter [02600, 05004], lr: 0.171247, loss: 1.8600
2022-08-05 14:28:34 - train: epoch 0029, iter [02700, 05004], lr: 0.171201, loss: 1.7378
2022-08-05 14:30:06 - train: epoch 0029, iter [02800, 05004], lr: 0.171155, loss: 1.6694
2022-08-05 14:31:39 - train: epoch 0029, iter [02900, 05004], lr: 0.171108, loss: 2.0388
2022-08-05 14:33:12 - train: epoch 0029, iter [03000, 05004], lr: 0.171062, loss: 1.7742
2022-08-05 14:34:45 - train: epoch 0029, iter [03100, 05004], lr: 0.171015, loss: 1.8951
2022-08-05 14:36:17 - train: epoch 0029, iter [03200, 05004], lr: 0.170969, loss: 1.8751
2022-08-05 14:37:50 - train: epoch 0029, iter [03300, 05004], lr: 0.170922, loss: 1.7448
2022-08-05 14:39:22 - train: epoch 0029, iter [03400, 05004], lr: 0.170875, loss: 1.7621
2022-08-05 14:40:55 - train: epoch 0029, iter [03500, 05004], lr: 0.170829, loss: 1.8573
2022-08-05 14:42:28 - train: epoch 0029, iter [03600, 05004], lr: 0.170782, loss: 1.8004
2022-08-05 14:44:00 - train: epoch 0029, iter [03700, 05004], lr: 0.170735, loss: 1.7195
2022-08-05 14:45:33 - train: epoch 0029, iter [03800, 05004], lr: 0.170689, loss: 1.8728
2022-08-05 14:47:06 - train: epoch 0029, iter [03900, 05004], lr: 0.170642, loss: 1.7480
2022-08-05 14:48:38 - train: epoch 0029, iter [04000, 05004], lr: 0.170595, loss: 1.9061
2022-08-05 14:50:11 - train: epoch 0029, iter [04100, 05004], lr: 0.170548, loss: 1.8530
2022-08-05 14:51:44 - train: epoch 0029, iter [04200, 05004], lr: 0.170501, loss: 1.8264
2022-08-05 14:53:17 - train: epoch 0029, iter [04300, 05004], lr: 0.170455, loss: 1.9145
2022-08-05 14:54:50 - train: epoch 0029, iter [04400, 05004], lr: 0.170408, loss: 1.8979
2022-08-05 14:56:22 - train: epoch 0029, iter [04500, 05004], lr: 0.170361, loss: 2.0210
2022-08-05 14:57:55 - train: epoch 0029, iter [04600, 05004], lr: 0.170314, loss: 1.9566
2022-08-05 14:59:28 - train: epoch 0029, iter [04700, 05004], lr: 0.170267, loss: 1.5625
2022-08-05 15:01:01 - train: epoch 0029, iter [04800, 05004], lr: 0.170220, loss: 2.0252
2022-08-05 15:02:34 - train: epoch 0029, iter [04900, 05004], lr: 0.170173, loss: 2.1056
2022-08-05 15:04:06 - train: epoch 0029, iter [05000, 05004], lr: 0.170126, loss: 1.5412
2022-08-05 15:04:11 - train: epoch 029, train_loss: 1.8530
2022-08-05 15:06:11 - eval: epoch: 029, acc1: 61.846%, acc5: 84.858%, test_loss: 1.5810, per_image_load_time: 2.671ms, per_image_inference_time: 1.240ms
2022-08-05 15:06:12 - until epoch: 029, best_acc1: 61.846%
2022-08-05 15:06:12 - epoch 030 lr: 0.170123
2022-08-05 15:07:53 - train: epoch 0030, iter [00100, 05004], lr: 0.170077, loss: 2.0330
2022-08-05 15:09:25 - train: epoch 0030, iter [00200, 05004], lr: 0.170029, loss: 1.9951
2022-08-05 15:10:58 - train: epoch 0030, iter [00300, 05004], lr: 0.169982, loss: 1.5476
2022-08-05 15:12:30 - train: epoch 0030, iter [00400, 05004], lr: 0.169935, loss: 1.8107
2022-08-05 15:14:03 - train: epoch 0030, iter [00500, 05004], lr: 0.169888, loss: 1.9165
2022-08-05 15:15:35 - train: epoch 0030, iter [00600, 05004], lr: 0.169840, loss: 1.7829
2022-08-05 15:17:08 - train: epoch 0030, iter [00700, 05004], lr: 0.169793, loss: 1.7942
2022-08-05 15:18:40 - train: epoch 0030, iter [00800, 05004], lr: 0.169746, loss: 1.7413
2022-08-05 15:20:13 - train: epoch 0030, iter [00900, 05004], lr: 0.169698, loss: 1.7684
2022-08-05 15:21:45 - train: epoch 0030, iter [01000, 05004], lr: 0.169651, loss: 1.9485
2022-08-05 15:23:18 - train: epoch 0030, iter [01100, 05004], lr: 0.169604, loss: 1.7950
2022-08-05 15:24:50 - train: epoch 0030, iter [01200, 05004], lr: 0.169556, loss: 1.7749
2022-08-05 15:26:23 - train: epoch 0030, iter [01300, 05004], lr: 0.169509, loss: 1.9171
2022-08-05 15:27:55 - train: epoch 0030, iter [01400, 05004], lr: 0.169461, loss: 1.7907
2022-08-05 15:29:28 - train: epoch 0030, iter [01500, 05004], lr: 0.169414, loss: 1.6195
2022-08-05 15:31:00 - train: epoch 0030, iter [01600, 05004], lr: 0.169366, loss: 1.9266
2022-08-05 15:32:32 - train: epoch 0030, iter [01700, 05004], lr: 0.169318, loss: 2.0326
2022-08-05 15:34:05 - train: epoch 0030, iter [01800, 05004], lr: 0.169271, loss: 1.7068
2022-08-05 15:35:37 - train: epoch 0030, iter [01900, 05004], lr: 0.169223, loss: 1.7882
2022-08-05 15:37:09 - train: epoch 0030, iter [02000, 05004], lr: 0.169175, loss: 1.7250
2022-08-05 15:38:42 - train: epoch 0030, iter [02100, 05004], lr: 0.169128, loss: 1.9345
2022-08-05 15:40:14 - train: epoch 0030, iter [02200, 05004], lr: 0.169080, loss: 1.9492
2022-08-05 15:41:47 - train: epoch 0030, iter [02300, 05004], lr: 0.169032, loss: 1.8617
2022-08-05 15:43:19 - train: epoch 0030, iter [02400, 05004], lr: 0.168984, loss: 1.9120
2022-08-05 15:44:52 - train: epoch 0030, iter [02500, 05004], lr: 0.168936, loss: 2.0517
2022-08-05 15:46:25 - train: epoch 0030, iter [02600, 05004], lr: 0.168888, loss: 1.9123
2022-08-05 15:47:57 - train: epoch 0030, iter [02700, 05004], lr: 0.168840, loss: 1.9542
2022-08-05 15:49:30 - train: epoch 0030, iter [02800, 05004], lr: 0.168793, loss: 2.0953
2022-08-05 15:51:03 - train: epoch 0030, iter [02900, 05004], lr: 0.168745, loss: 1.8724
2022-08-05 15:52:35 - train: epoch 0030, iter [03000, 05004], lr: 0.168697, loss: 1.9464
2022-08-05 15:54:08 - train: epoch 0030, iter [03100, 05004], lr: 0.168649, loss: 1.8524
2022-08-05 15:55:40 - train: epoch 0030, iter [03200, 05004], lr: 0.168600, loss: 1.8997
2022-08-05 15:57:13 - train: epoch 0030, iter [03300, 05004], lr: 0.168552, loss: 2.0603
2022-08-05 15:58:46 - train: epoch 0030, iter [03400, 05004], lr: 0.168504, loss: 1.7662
2022-08-05 16:00:18 - train: epoch 0030, iter [03500, 05004], lr: 0.168456, loss: 1.8893
2022-08-05 16:01:51 - train: epoch 0030, iter [03600, 05004], lr: 0.168408, loss: 1.9085
2022-08-05 16:03:24 - train: epoch 0030, iter [03700, 05004], lr: 0.168360, loss: 1.7240
2022-08-05 16:04:56 - train: epoch 0030, iter [03800, 05004], lr: 0.168311, loss: 1.9970
2022-08-05 16:06:29 - train: epoch 0030, iter [03900, 05004], lr: 0.168263, loss: 1.6268
2022-08-05 16:08:01 - train: epoch 0030, iter [04000, 05004], lr: 0.168215, loss: 1.7978
2022-08-05 16:09:34 - train: epoch 0030, iter [04100, 05004], lr: 0.168166, loss: 1.7586
2022-08-05 16:11:07 - train: epoch 0030, iter [04200, 05004], lr: 0.168118, loss: 2.0689
2022-08-05 16:12:39 - train: epoch 0030, iter [04300, 05004], lr: 0.168070, loss: 1.8575
2022-08-05 16:14:12 - train: epoch 0030, iter [04400, 05004], lr: 0.168021, loss: 1.8986
2022-08-05 16:15:45 - train: epoch 0030, iter [04500, 05004], lr: 0.167973, loss: 2.0166
2022-08-05 16:17:17 - train: epoch 0030, iter [04600, 05004], lr: 0.167924, loss: 1.5912
2022-08-05 16:18:50 - train: epoch 0030, iter [04700, 05004], lr: 0.167876, loss: 1.8360
2022-08-05 16:20:22 - train: epoch 0030, iter [04800, 05004], lr: 0.167827, loss: 1.8320
2022-08-05 16:21:55 - train: epoch 0030, iter [04900, 05004], lr: 0.167779, loss: 1.8729
2022-08-05 16:23:28 - train: epoch 0030, iter [05000, 05004], lr: 0.167730, loss: 1.9057
2022-08-05 16:23:32 - train: epoch 030, train_loss: 1.8445
2022-08-05 16:25:39 - eval: epoch: 030, acc1: 61.786%, acc5: 84.942%, test_loss: 1.5748, per_image_load_time: 2.029ms, per_image_inference_time: 1.252ms
2022-08-05 16:25:39 - until epoch: 030, best_acc1: 61.846%
2022-08-05 16:25:39 - epoch 031 lr: 0.167728
2022-08-05 16:27:20 - train: epoch 0031, iter [00100, 05004], lr: 0.167680, loss: 1.9204
2022-08-05 16:28:52 - train: epoch 0031, iter [00200, 05004], lr: 0.167631, loss: 1.7573
2022-08-05 16:30:25 - train: epoch 0031, iter [00300, 05004], lr: 0.167582, loss: 1.7464
2022-08-05 16:31:57 - train: epoch 0031, iter [00400, 05004], lr: 0.167533, loss: 1.9234
2022-08-05 16:33:29 - train: epoch 0031, iter [00500, 05004], lr: 0.167485, loss: 1.9064
2022-08-05 16:35:01 - train: epoch 0031, iter [00600, 05004], lr: 0.167436, loss: 1.7922
2022-08-05 16:36:34 - train: epoch 0031, iter [00700, 05004], lr: 0.167387, loss: 1.9718
2022-08-05 16:38:06 - train: epoch 0031, iter [00800, 05004], lr: 0.167338, loss: 1.9065
2022-08-05 16:39:38 - train: epoch 0031, iter [00900, 05004], lr: 0.167289, loss: 1.8395
2022-08-05 16:41:11 - train: epoch 0031, iter [01000, 05004], lr: 0.167240, loss: 1.9306
2022-08-05 16:42:43 - train: epoch 0031, iter [01100, 05004], lr: 0.167192, loss: 1.8039
2022-08-05 16:44:15 - train: epoch 0031, iter [01200, 05004], lr: 0.167143, loss: 1.9171
2022-08-05 16:45:48 - train: epoch 0031, iter [01300, 05004], lr: 0.167094, loss: 1.6004
2022-08-05 16:47:20 - train: epoch 0031, iter [01400, 05004], lr: 0.167045, loss: 1.8612
2022-08-05 16:48:53 - train: epoch 0031, iter [01500, 05004], lr: 0.166996, loss: 2.0635
2022-08-05 16:50:25 - train: epoch 0031, iter [01600, 05004], lr: 0.166946, loss: 1.7060
2022-08-05 16:51:58 - train: epoch 0031, iter [01700, 05004], lr: 0.166897, loss: 1.7074
2022-08-05 16:53:30 - train: epoch 0031, iter [01800, 05004], lr: 0.166848, loss: 1.7886
2022-08-05 16:55:03 - train: epoch 0031, iter [01900, 05004], lr: 0.166799, loss: 1.8313
2022-08-05 16:56:35 - train: epoch 0031, iter [02000, 05004], lr: 0.166750, loss: 1.9354
2022-08-05 16:58:08 - train: epoch 0031, iter [02100, 05004], lr: 0.166701, loss: 1.8484
2022-08-05 16:59:40 - train: epoch 0031, iter [02200, 05004], lr: 0.166651, loss: 1.6167
2022-08-05 17:01:13 - train: epoch 0031, iter [02300, 05004], lr: 0.166602, loss: 1.8447
2022-08-05 17:02:45 - train: epoch 0031, iter [02400, 05004], lr: 0.166553, loss: 1.8432
2022-08-05 17:04:18 - train: epoch 0031, iter [02500, 05004], lr: 0.166503, loss: 1.7930
2022-08-05 17:05:50 - train: epoch 0031, iter [02600, 05004], lr: 0.166454, loss: 1.7855
2022-08-05 17:07:23 - train: epoch 0031, iter [02700, 05004], lr: 0.166405, loss: 1.8640
2022-08-05 17:08:55 - train: epoch 0031, iter [02800, 05004], lr: 0.166355, loss: 2.1951
2022-08-05 17:10:28 - train: epoch 0031, iter [02900, 05004], lr: 0.166306, loss: 1.9876
2022-08-05 17:12:00 - train: epoch 0031, iter [03000, 05004], lr: 0.166256, loss: 2.1165
2022-08-05 17:13:33 - train: epoch 0031, iter [03100, 05004], lr: 0.166207, loss: 1.9537
2022-08-05 17:15:05 - train: epoch 0031, iter [03200, 05004], lr: 0.166157, loss: 1.9127
2022-08-05 17:16:38 - train: epoch 0031, iter [03300, 05004], lr: 0.166108, loss: 1.9434
2022-08-05 17:18:10 - train: epoch 0031, iter [03400, 05004], lr: 0.166058, loss: 1.8888
2022-08-05 17:19:43 - train: epoch 0031, iter [03500, 05004], lr: 0.166008, loss: 1.8404
2022-08-05 17:21:15 - train: epoch 0031, iter [03600, 05004], lr: 0.165959, loss: 1.7464
2022-08-05 17:22:48 - train: epoch 0031, iter [03700, 05004], lr: 0.165909, loss: 1.6458
2022-08-05 17:24:20 - train: epoch 0031, iter [03800, 05004], lr: 0.165859, loss: 1.7277
2022-08-05 17:25:53 - train: epoch 0031, iter [03900, 05004], lr: 0.165810, loss: 1.9922
2022-08-05 17:27:25 - train: epoch 0031, iter [04000, 05004], lr: 0.165760, loss: 1.9461
2022-08-05 17:28:58 - train: epoch 0031, iter [04100, 05004], lr: 0.165710, loss: 1.8456
2022-08-05 17:30:30 - train: epoch 0031, iter [04200, 05004], lr: 0.165660, loss: 2.0071
2022-08-05 17:32:03 - train: epoch 0031, iter [04300, 05004], lr: 0.165610, loss: 1.8865
2022-08-05 17:33:36 - train: epoch 0031, iter [04400, 05004], lr: 0.165561, loss: 1.8241
2022-08-05 17:35:08 - train: epoch 0031, iter [04500, 05004], lr: 0.165511, loss: 1.9519
2022-08-05 17:36:41 - train: epoch 0031, iter [04600, 05004], lr: 0.165461, loss: 1.8016
2022-08-05 17:38:13 - train: epoch 0031, iter [04700, 05004], lr: 0.165411, loss: 1.6980
2022-08-05 17:39:46 - train: epoch 0031, iter [04800, 05004], lr: 0.165361, loss: 1.8181
2022-08-05 17:41:18 - train: epoch 0031, iter [04900, 05004], lr: 0.165311, loss: 1.8554
2022-08-05 17:42:51 - train: epoch 0031, iter [05000, 05004], lr: 0.165261, loss: 2.0010
2022-08-05 17:42:55 - train: epoch 031, train_loss: 1.8317
2022-08-05 17:45:01 - eval: epoch: 031, acc1: 62.688%, acc5: 85.568%, test_loss: 1.5416, per_image_load_time: 1.995ms, per_image_inference_time: 1.258ms
2022-08-05 17:45:01 - until epoch: 031, best_acc1: 62.688%
2022-08-05 17:45:01 - epoch 032 lr: 0.165258
2022-08-05 17:46:42 - train: epoch 0032, iter [00100, 05004], lr: 0.165208, loss: 1.6533
2022-08-05 17:48:15 - train: epoch 0032, iter [00200, 05004], lr: 0.165158, loss: 1.8537
2022-08-05 17:49:47 - train: epoch 0032, iter [00300, 05004], lr: 0.165108, loss: 1.5779
2022-08-05 17:51:20 - train: epoch 0032, iter [00400, 05004], lr: 0.165058, loss: 1.7184
2022-08-05 17:52:52 - train: epoch 0032, iter [00500, 05004], lr: 0.165008, loss: 1.9079
2022-08-05 17:54:25 - train: epoch 0032, iter [00600, 05004], lr: 0.164958, loss: 1.7138
2022-08-05 17:55:57 - train: epoch 0032, iter [00700, 05004], lr: 0.164907, loss: 1.6755
2022-08-05 17:57:29 - train: epoch 0032, iter [00800, 05004], lr: 0.164857, loss: 1.8922
2022-08-05 17:59:02 - train: epoch 0032, iter [00900, 05004], lr: 0.164807, loss: 2.0674
2022-08-05 18:00:34 - train: epoch 0032, iter [01000, 05004], lr: 0.164756, loss: 1.9621
2022-08-05 18:02:07 - train: epoch 0032, iter [01100, 05004], lr: 0.164706, loss: 1.7738
2022-08-05 18:03:39 - train: epoch 0032, iter [01200, 05004], lr: 0.164656, loss: 1.8234
2022-08-05 18:05:12 - train: epoch 0032, iter [01300, 05004], lr: 0.164605, loss: 1.7389
2022-08-05 18:06:44 - train: epoch 0032, iter [01400, 05004], lr: 0.164555, loss: 2.0284
2022-08-05 18:08:17 - train: epoch 0032, iter [01500, 05004], lr: 0.164504, loss: 1.5816
2022-08-05 18:09:49 - train: epoch 0032, iter [01600, 05004], lr: 0.164454, loss: 1.8379
2022-08-05 18:11:22 - train: epoch 0032, iter [01700, 05004], lr: 0.164403, loss: 1.8434
2022-08-05 18:12:54 - train: epoch 0032, iter [01800, 05004], lr: 0.164353, loss: 2.0380
2022-08-05 18:14:27 - train: epoch 0032, iter [01900, 05004], lr: 0.164302, loss: 1.6449
2022-08-05 18:15:59 - train: epoch 0032, iter [02000, 05004], lr: 0.164251, loss: 1.6887
2022-08-05 18:17:32 - train: epoch 0032, iter [02100, 05004], lr: 0.164201, loss: 1.8031
2022-08-05 18:19:04 - train: epoch 0032, iter [02200, 05004], lr: 0.164150, loss: 1.9847
2022-08-05 18:20:37 - train: epoch 0032, iter [02300, 05004], lr: 0.164099, loss: 1.8530
2022-08-05 18:22:09 - train: epoch 0032, iter [02400, 05004], lr: 0.164049, loss: 1.8872
2022-08-05 18:23:41 - train: epoch 0032, iter [02500, 05004], lr: 0.163998, loss: 1.6708
2022-08-05 18:25:14 - train: epoch 0032, iter [02600, 05004], lr: 0.163947, loss: 1.5544
2022-08-05 18:26:46 - train: epoch 0032, iter [02700, 05004], lr: 0.163896, loss: 1.7436
2022-08-05 18:28:19 - train: epoch 0032, iter [02800, 05004], lr: 0.163845, loss: 1.8391
2022-08-05 18:29:51 - train: epoch 0032, iter [02900, 05004], lr: 0.163795, loss: 1.7872
2022-08-05 18:31:24 - train: epoch 0032, iter [03000, 05004], lr: 0.163744, loss: 1.7555
2022-08-05 18:32:56 - train: epoch 0032, iter [03100, 05004], lr: 0.163693, loss: 1.8128
2022-08-05 18:34:29 - train: epoch 0032, iter [03200, 05004], lr: 0.163642, loss: 2.0141
2022-08-05 18:36:01 - train: epoch 0032, iter [03300, 05004], lr: 0.163591, loss: 1.6370
2022-08-05 18:37:34 - train: epoch 0032, iter [03400, 05004], lr: 0.163540, loss: 1.5617
2022-08-05 18:39:06 - train: epoch 0032, iter [03500, 05004], lr: 0.163489, loss: 1.7930
2022-08-05 18:40:39 - train: epoch 0032, iter [03600, 05004], lr: 0.163438, loss: 1.9164
2022-08-05 18:42:11 - train: epoch 0032, iter [03700, 05004], lr: 0.163387, loss: 1.8421
2022-08-05 18:43:44 - train: epoch 0032, iter [03800, 05004], lr: 0.163335, loss: 1.5835
2022-08-05 18:45:16 - train: epoch 0032, iter [03900, 05004], lr: 0.163284, loss: 1.6436
2022-08-05 18:46:49 - train: epoch 0032, iter [04000, 05004], lr: 0.163233, loss: 1.7396
2022-08-05 18:48:21 - train: epoch 0032, iter [04100, 05004], lr: 0.163182, loss: 1.9038
2022-08-05 18:49:54 - train: epoch 0032, iter [04200, 05004], lr: 0.163131, loss: 1.5263
2022-08-05 18:51:26 - train: epoch 0032, iter [04300, 05004], lr: 0.163079, loss: 1.9958
2022-08-05 18:52:59 - train: epoch 0032, iter [04400, 05004], lr: 0.163028, loss: 1.8756
2022-08-05 18:54:31 - train: epoch 0032, iter [04500, 05004], lr: 0.162977, loss: 1.8958
2022-08-05 18:56:04 - train: epoch 0032, iter [04600, 05004], lr: 0.162925, loss: 1.8195
2022-08-05 18:57:36 - train: epoch 0032, iter [04700, 05004], lr: 0.162874, loss: 1.8504
2022-08-05 18:59:09 - train: epoch 0032, iter [04800, 05004], lr: 0.162823, loss: 1.6606
2022-08-05 19:00:41 - train: epoch 0032, iter [04900, 05004], lr: 0.162771, loss: 1.7157
2022-08-05 19:02:14 - train: epoch 0032, iter [05000, 05004], lr: 0.162720, loss: 1.7804
2022-08-05 19:02:18 - train: epoch 032, train_loss: 1.8207
2022-08-05 19:04:20 - eval: epoch: 032, acc1: 63.202%, acc5: 85.898%, test_loss: 1.5154, per_image_load_time: 3.364ms, per_image_inference_time: 1.214ms
2022-08-05 19:04:20 - until epoch: 032, best_acc1: 63.202%
2022-08-05 19:04:20 - epoch 033 lr: 0.162717
2022-08-05 19:06:01 - train: epoch 0033, iter [00100, 05004], lr: 0.162666, loss: 1.6579
2022-08-05 19:07:33 - train: epoch 0033, iter [00200, 05004], lr: 0.162615, loss: 1.8920
2022-08-05 19:09:06 - train: epoch 0033, iter [00300, 05004], lr: 0.162563, loss: 1.7374
2022-08-05 19:10:39 - train: epoch 0033, iter [00400, 05004], lr: 0.162512, loss: 1.6107
2022-08-05 19:12:11 - train: epoch 0033, iter [00500, 05004], lr: 0.162460, loss: 1.9611
2022-08-05 19:13:44 - train: epoch 0033, iter [00600, 05004], lr: 0.162408, loss: 1.8008
2022-08-05 19:15:17 - train: epoch 0033, iter [00700, 05004], lr: 0.162357, loss: 2.1052
2022-08-05 19:16:49 - train: epoch 0033, iter [00800, 05004], lr: 0.162305, loss: 1.8692
2022-08-05 19:18:22 - train: epoch 0033, iter [00900, 05004], lr: 0.162253, loss: 1.8152
2022-08-05 19:19:55 - train: epoch 0033, iter [01000, 05004], lr: 0.162202, loss: 1.8420
2022-08-05 19:21:27 - train: epoch 0033, iter [01100, 05004], lr: 0.162150, loss: 1.6249
2022-08-05 19:23:00 - train: epoch 0033, iter [01200, 05004], lr: 0.162098, loss: 2.0237
2022-08-05 19:24:32 - train: epoch 0033, iter [01300, 05004], lr: 0.162046, loss: 1.6559
2022-08-05 19:26:05 - train: epoch 0033, iter [01400, 05004], lr: 0.161994, loss: 1.8229
2022-08-05 19:27:38 - train: epoch 0033, iter [01500, 05004], lr: 0.161942, loss: 1.9632
2022-08-05 19:29:10 - train: epoch 0033, iter [01600, 05004], lr: 0.161891, loss: 2.0814
2022-08-05 19:30:43 - train: epoch 0033, iter [01700, 05004], lr: 0.161839, loss: 1.8416
2022-08-05 19:32:15 - train: epoch 0033, iter [01800, 05004], lr: 0.161787, loss: 1.8357
2022-08-05 19:33:48 - train: epoch 0033, iter [01900, 05004], lr: 0.161735, loss: 1.7738
2022-08-05 19:35:20 - train: epoch 0033, iter [02000, 05004], lr: 0.161683, loss: 1.6965
2022-08-05 19:36:53 - train: epoch 0033, iter [02100, 05004], lr: 0.161631, loss: 1.7905
2022-08-05 19:38:26 - train: epoch 0033, iter [02200, 05004], lr: 0.161579, loss: 1.9607
2022-08-05 19:39:58 - train: epoch 0033, iter [02300, 05004], lr: 0.161527, loss: 1.7770
2022-08-05 19:41:31 - train: epoch 0033, iter [02400, 05004], lr: 0.161474, loss: 1.9985
2022-08-05 19:43:04 - train: epoch 0033, iter [02500, 05004], lr: 0.161422, loss: 1.6911
2022-08-05 19:44:36 - train: epoch 0033, iter [02600, 05004], lr: 0.161370, loss: 1.6329
2022-08-05 19:46:09 - train: epoch 0033, iter [02700, 05004], lr: 0.161318, loss: 1.7948
2022-08-05 19:47:42 - train: epoch 0033, iter [02800, 05004], lr: 0.161266, loss: 1.9766
2022-08-05 19:49:14 - train: epoch 0033, iter [02900, 05004], lr: 0.161213, loss: 1.7172
2022-08-05 19:50:47 - train: epoch 0033, iter [03000, 05004], lr: 0.161161, loss: 1.7995
2022-08-05 19:52:19 - train: epoch 0033, iter [03100, 05004], lr: 0.161109, loss: 1.8600
2022-08-05 19:53:52 - train: epoch 0033, iter [03200, 05004], lr: 0.161057, loss: 1.8370
2022-08-05 19:55:25 - train: epoch 0033, iter [03300, 05004], lr: 0.161004, loss: 1.6380
2022-08-05 19:56:57 - train: epoch 0033, iter [03400, 05004], lr: 0.160952, loss: 1.6159
2022-08-05 19:58:30 - train: epoch 0033, iter [03500, 05004], lr: 0.160899, loss: 1.8285
2022-08-05 20:00:03 - train: epoch 0033, iter [03600, 05004], lr: 0.160847, loss: 1.8218
2022-08-05 20:01:36 - train: epoch 0033, iter [03700, 05004], lr: 0.160795, loss: 1.6060
2022-08-05 20:03:09 - train: epoch 0033, iter [03800, 05004], lr: 0.160742, loss: 1.8514
2022-08-05 20:04:42 - train: epoch 0033, iter [03900, 05004], lr: 0.160690, loss: 2.0336
2022-08-05 20:06:15 - train: epoch 0033, iter [04000, 05004], lr: 0.160637, loss: 1.8933
2022-08-05 20:07:47 - train: epoch 0033, iter [04100, 05004], lr: 0.160584, loss: 1.7994
2022-08-05 20:09:20 - train: epoch 0033, iter [04200, 05004], lr: 0.160532, loss: 1.9212
2022-08-05 20:10:52 - train: epoch 0033, iter [04300, 05004], lr: 0.160479, loss: 1.9699
2022-08-05 20:12:25 - train: epoch 0033, iter [04400, 05004], lr: 0.160427, loss: 1.9381
2022-08-05 20:13:57 - train: epoch 0033, iter [04500, 05004], lr: 0.160374, loss: 1.9979
2022-08-05 20:15:30 - train: epoch 0033, iter [04600, 05004], lr: 0.160321, loss: 1.7298
2022-08-05 20:17:02 - train: epoch 0033, iter [04700, 05004], lr: 0.160269, loss: 1.8042
2022-08-05 20:18:35 - train: epoch 0033, iter [04800, 05004], lr: 0.160216, loss: 2.2198
2022-08-05 20:20:07 - train: epoch 0033, iter [04900, 05004], lr: 0.160163, loss: 1.8564
2022-08-05 20:21:39 - train: epoch 0033, iter [05000, 05004], lr: 0.160110, loss: 1.8536
2022-08-05 20:21:44 - train: epoch 033, train_loss: 1.8085
2022-08-05 20:23:47 - eval: epoch: 033, acc1: 62.804%, acc5: 85.616%, test_loss: 1.5313, per_image_load_time: 3.525ms, per_image_inference_time: 1.216ms
2022-08-05 20:23:47 - until epoch: 033, best_acc1: 63.202%
2022-08-05 21:10:44 - epoch 034 lr: 0.160108
2022-08-05 21:12:28 - train: epoch 0034, iter [00100, 05004], lr: 0.160055, loss: 1.7694
2022-08-05 21:14:01 - train: epoch 0034, iter [00200, 05004], lr: 0.160002, loss: 1.6864
2022-08-05 21:15:35 - train: epoch 0034, iter [00300, 05004], lr: 0.159950, loss: 1.6315
2022-08-05 21:17:09 - train: epoch 0034, iter [00400, 05004], lr: 0.159897, loss: 1.7459
2022-08-05 21:18:44 - train: epoch 0034, iter [00500, 05004], lr: 0.159844, loss: 1.8151
2022-08-05 21:20:19 - train: epoch 0034, iter [00600, 05004], lr: 0.159791, loss: 1.8797
2022-08-05 21:21:53 - train: epoch 0034, iter [00700, 05004], lr: 0.159738, loss: 1.7469
2022-08-05 21:23:27 - train: epoch 0034, iter [00800, 05004], lr: 0.159685, loss: 1.7854
2022-08-05 21:25:00 - train: epoch 0034, iter [00900, 05004], lr: 0.159632, loss: 1.7634
2022-08-05 21:26:34 - train: epoch 0034, iter [01000, 05004], lr: 0.159579, loss: 1.7916
2022-08-05 21:28:08 - train: epoch 0034, iter [01100, 05004], lr: 0.159526, loss: 1.7358
2022-08-05 21:29:42 - train: epoch 0034, iter [01200, 05004], lr: 0.159472, loss: 1.8830
2022-08-05 21:31:16 - train: epoch 0034, iter [01300, 05004], lr: 0.159419, loss: 1.7679
2022-08-05 21:32:50 - train: epoch 0034, iter [01400, 05004], lr: 0.159366, loss: 1.9118
2022-08-05 21:34:24 - train: epoch 0034, iter [01500, 05004], lr: 0.159313, loss: 1.7829
2022-08-05 21:35:58 - train: epoch 0034, iter [01600, 05004], lr: 0.159260, loss: 2.0433
2022-08-05 21:37:32 - train: epoch 0034, iter [01700, 05004], lr: 0.159206, loss: 1.6671
2022-08-05 21:39:06 - train: epoch 0034, iter [01800, 05004], lr: 0.159153, loss: 1.8698
2022-08-05 21:40:40 - train: epoch 0034, iter [01900, 05004], lr: 0.159100, loss: 1.9933
2022-08-05 21:42:14 - train: epoch 0034, iter [02000, 05004], lr: 0.159047, loss: 1.6933
2022-08-05 21:43:48 - train: epoch 0034, iter [02100, 05004], lr: 0.158993, loss: 1.9175
2022-08-05 21:45:22 - train: epoch 0034, iter [02200, 05004], lr: 0.158940, loss: 1.5936
2022-08-05 21:46:55 - train: epoch 0034, iter [02300, 05004], lr: 0.158886, loss: 1.8625
2022-08-05 21:48:29 - train: epoch 0034, iter [02400, 05004], lr: 0.158833, loss: 1.8479
2022-08-05 21:50:03 - train: epoch 0034, iter [02500, 05004], lr: 0.158780, loss: 1.8131
2022-08-05 21:51:37 - train: epoch 0034, iter [02600, 05004], lr: 0.158726, loss: 1.8611
2022-08-05 21:53:11 - train: epoch 0034, iter [02700, 05004], lr: 0.158673, loss: 1.8634
2022-08-05 21:54:45 - train: epoch 0034, iter [02800, 05004], lr: 0.158619, loss: 1.6107
2022-08-05 21:56:19 - train: epoch 0034, iter [02900, 05004], lr: 0.158566, loss: 1.7481
2022-08-05 21:57:53 - train: epoch 0034, iter [03000, 05004], lr: 0.158512, loss: 1.6851
2022-08-05 21:59:27 - train: epoch 0034, iter [03100, 05004], lr: 0.158458, loss: 1.6955
2022-08-05 22:01:01 - train: epoch 0034, iter [03200, 05004], lr: 0.158405, loss: 1.7697
2022-08-05 22:02:35 - train: epoch 0034, iter [03300, 05004], lr: 0.158351, loss: 1.8038
2022-08-05 22:04:08 - train: epoch 0034, iter [03400, 05004], lr: 0.158297, loss: 1.7892
2022-08-05 22:05:42 - train: epoch 0034, iter [03500, 05004], lr: 0.158244, loss: 1.5338
2022-08-05 22:07:16 - train: epoch 0034, iter [03600, 05004], lr: 0.158190, loss: 1.6305
2022-08-05 22:08:50 - train: epoch 0034, iter [03700, 05004], lr: 0.158136, loss: 1.7887
2022-08-05 22:10:24 - train: epoch 0034, iter [03800, 05004], lr: 0.158082, loss: 1.7359
2022-08-05 22:11:58 - train: epoch 0034, iter [03900, 05004], lr: 0.158029, loss: 1.8905
2022-08-05 22:13:31 - train: epoch 0034, iter [04000, 05004], lr: 0.157975, loss: 1.6103
2022-08-05 22:15:05 - train: epoch 0034, iter [04100, 05004], lr: 0.157921, loss: 1.9623
2022-08-05 22:16:39 - train: epoch 0034, iter [04200, 05004], lr: 0.157867, loss: 1.7419
2022-08-05 22:18:13 - train: epoch 0034, iter [04300, 05004], lr: 0.157813, loss: 1.8665
2022-08-05 22:19:47 - train: epoch 0034, iter [04400, 05004], lr: 0.157759, loss: 1.7247
2022-08-05 22:21:20 - train: epoch 0034, iter [04500, 05004], lr: 0.157705, loss: 1.7526
2022-08-05 22:22:54 - train: epoch 0034, iter [04600, 05004], lr: 0.157651, loss: 1.9520
2022-08-05 22:24:28 - train: epoch 0034, iter [04700, 05004], lr: 0.157597, loss: 2.0384
2022-08-05 22:26:02 - train: epoch 0034, iter [04800, 05004], lr: 0.157543, loss: 1.9540
2022-08-05 22:27:36 - train: epoch 0034, iter [04900, 05004], lr: 0.157489, loss: 1.7734
2022-08-05 22:29:09 - train: epoch 0034, iter [05000, 05004], lr: 0.157435, loss: 1.9125
2022-08-05 22:29:14 - train: epoch 034, train_loss: 1.7969
2022-08-05 22:31:14 - eval: epoch: 034, acc1: 62.836%, acc5: 85.776%, test_loss: 1.5254, per_image_load_time: 3.334ms, per_image_inference_time: 1.215ms
2022-08-05 22:31:14 - until epoch: 034, best_acc1: 63.202%
2022-08-05 22:31:14 - epoch 035 lr: 0.157432
2022-08-05 22:32:57 - train: epoch 0035, iter [00100, 05004], lr: 0.157379, loss: 1.6012
2022-08-05 22:34:31 - train: epoch 0035, iter [00200, 05004], lr: 0.157325, loss: 1.6537
2022-08-05 22:36:05 - train: epoch 0035, iter [00300, 05004], lr: 0.157270, loss: 1.9375
2022-08-05 22:37:39 - train: epoch 0035, iter [00400, 05004], lr: 0.157216, loss: 1.5801
2022-08-05 22:39:12 - train: epoch 0035, iter [00500, 05004], lr: 0.157162, loss: 1.7318
2022-08-05 22:40:46 - train: epoch 0035, iter [00600, 05004], lr: 0.157108, loss: 1.8901
2022-08-05 22:42:20 - train: epoch 0035, iter [00700, 05004], lr: 0.157054, loss: 1.8853
2022-08-05 22:43:54 - train: epoch 0035, iter [00800, 05004], lr: 0.156999, loss: 1.6575
2022-08-05 22:45:28 - train: epoch 0035, iter [00900, 05004], lr: 0.156945, loss: 1.7881
2022-08-05 22:47:02 - train: epoch 0035, iter [01000, 05004], lr: 0.156891, loss: 1.6126
2022-08-05 22:48:36 - train: epoch 0035, iter [01100, 05004], lr: 0.156836, loss: 1.7475
2022-08-05 22:50:10 - train: epoch 0035, iter [01200, 05004], lr: 0.156782, loss: 1.6532
2022-08-05 22:51:44 - train: epoch 0035, iter [01300, 05004], lr: 0.156727, loss: 1.8946
2022-08-05 22:53:18 - train: epoch 0035, iter [01400, 05004], lr: 0.156673, loss: 1.7689
2022-08-05 22:54:52 - train: epoch 0035, iter [01500, 05004], lr: 0.156619, loss: 1.8005
2022-08-05 22:56:26 - train: epoch 0035, iter [01600, 05004], lr: 0.156564, loss: 1.7598
2022-08-05 22:58:00 - train: epoch 0035, iter [01700, 05004], lr: 0.156510, loss: 1.6769
2022-08-05 22:59:34 - train: epoch 0035, iter [01800, 05004], lr: 0.156455, loss: 1.8378
2022-08-05 23:01:08 - train: epoch 0035, iter [01900, 05004], lr: 0.156400, loss: 1.8814
2022-08-05 23:02:42 - train: epoch 0035, iter [02000, 05004], lr: 0.156346, loss: 1.9621
2022-08-05 23:04:16 - train: epoch 0035, iter [02100, 05004], lr: 0.156291, loss: 1.7276
2022-08-05 23:05:50 - train: epoch 0035, iter [02200, 05004], lr: 0.156237, loss: 1.9145
2022-08-05 23:07:24 - train: epoch 0035, iter [02300, 05004], lr: 0.156182, loss: 1.6769
2022-08-05 23:08:57 - train: epoch 0035, iter [02400, 05004], lr: 0.156127, loss: 1.8719
2022-08-05 23:10:31 - train: epoch 0035, iter [02500, 05004], lr: 0.156073, loss: 1.8006
2022-08-05 23:12:05 - train: epoch 0035, iter [02600, 05004], lr: 0.156018, loss: 1.9102
2022-08-05 23:13:38 - train: epoch 0035, iter [02700, 05004], lr: 0.155963, loss: 1.5960
2022-08-05 23:15:12 - train: epoch 0035, iter [02800, 05004], lr: 0.155908, loss: 1.9652
2022-08-05 23:16:46 - train: epoch 0035, iter [02900, 05004], lr: 0.155854, loss: 1.7758
2022-08-05 23:18:20 - train: epoch 0035, iter [03000, 05004], lr: 0.155799, loss: 1.7251
2022-08-05 23:19:53 - train: epoch 0035, iter [03100, 05004], lr: 0.155744, loss: 1.6711
2022-08-05 23:21:27 - train: epoch 0035, iter [03200, 05004], lr: 0.155689, loss: 1.7611
2022-08-05 23:23:01 - train: epoch 0035, iter [03300, 05004], lr: 0.155634, loss: 1.8743
2022-08-05 23:24:34 - train: epoch 0035, iter [03400, 05004], lr: 0.155579, loss: 1.8013
2022-08-05 23:26:08 - train: epoch 0035, iter [03500, 05004], lr: 0.155524, loss: 1.7138
2022-08-05 23:27:42 - train: epoch 0035, iter [03600, 05004], lr: 0.155469, loss: 1.8382
2022-08-05 23:29:15 - train: epoch 0035, iter [03700, 05004], lr: 0.155414, loss: 1.6666
2022-08-05 23:30:49 - train: epoch 0035, iter [03800, 05004], lr: 0.155359, loss: 1.9256
2022-08-05 23:32:23 - train: epoch 0035, iter [03900, 05004], lr: 0.155304, loss: 1.7811
2022-08-05 23:33:57 - train: epoch 0035, iter [04000, 05004], lr: 0.155249, loss: 1.5918
2022-08-05 23:35:30 - train: epoch 0035, iter [04100, 05004], lr: 0.155194, loss: 2.0117
2022-08-05 23:37:04 - train: epoch 0035, iter [04200, 05004], lr: 0.155139, loss: 1.6944
2022-08-05 23:38:38 - train: epoch 0035, iter [04300, 05004], lr: 0.155084, loss: 1.9775
2022-08-05 23:40:12 - train: epoch 0035, iter [04400, 05004], lr: 0.155029, loss: 2.0209
2022-08-05 23:41:46 - train: epoch 0035, iter [04500, 05004], lr: 0.154973, loss: 1.7905
2022-08-05 23:43:19 - train: epoch 0035, iter [04600, 05004], lr: 0.154918, loss: 1.7250
2022-08-05 23:44:53 - train: epoch 0035, iter [04700, 05004], lr: 0.154863, loss: 2.0572
2022-08-05 23:46:27 - train: epoch 0035, iter [04800, 05004], lr: 0.154808, loss: 1.9786
2022-08-05 23:48:01 - train: epoch 0035, iter [04900, 05004], lr: 0.154752, loss: 1.7678
2022-08-05 23:49:34 - train: epoch 0035, iter [05000, 05004], lr: 0.154697, loss: 1.7008
2022-08-05 23:49:39 - train: epoch 035, train_loss: 1.7861
2022-08-05 23:51:43 - eval: epoch: 035, acc1: 63.160%, acc5: 85.792%, test_loss: 1.5298, per_image_load_time: 3.206ms, per_image_inference_time: 1.268ms
2022-08-05 23:51:43 - until epoch: 035, best_acc1: 63.202%
2022-08-05 23:51:43 - epoch 036 lr: 0.154694
2022-08-05 23:53:27 - train: epoch 0036, iter [00100, 05004], lr: 0.154639, loss: 1.9135
2022-08-05 23:55:00 - train: epoch 0036, iter [00200, 05004], lr: 0.154584, loss: 1.6166
2022-08-05 23:56:34 - train: epoch 0036, iter [00300, 05004], lr: 0.154529, loss: 1.5527
2022-08-05 23:58:08 - train: epoch 0036, iter [00400, 05004], lr: 0.154473, loss: 1.8669
2022-08-05 23:59:42 - train: epoch 0036, iter [00500, 05004], lr: 0.154418, loss: 1.7443
2022-08-06 00:01:16 - train: epoch 0036, iter [00600, 05004], lr: 0.154362, loss: 1.8288
2022-08-06 00:02:50 - train: epoch 0036, iter [00700, 05004], lr: 0.154307, loss: 1.5987
2022-08-06 00:04:23 - train: epoch 0036, iter [00800, 05004], lr: 0.154251, loss: 1.4960
2022-08-06 00:05:57 - train: epoch 0036, iter [00900, 05004], lr: 0.154196, loss: 1.7492
2022-08-06 00:07:31 - train: epoch 0036, iter [01000, 05004], lr: 0.154140, loss: 1.7316
2022-08-06 00:09:05 - train: epoch 0036, iter [01100, 05004], lr: 0.154085, loss: 1.6074
2022-08-06 00:10:39 - train: epoch 0036, iter [01200, 05004], lr: 0.154029, loss: 1.8890
2022-08-06 00:12:12 - train: epoch 0036, iter [01300, 05004], lr: 0.153974, loss: 1.6565
2022-08-06 00:13:46 - train: epoch 0036, iter [01400, 05004], lr: 0.153918, loss: 1.5636
2022-08-06 00:15:20 - train: epoch 0036, iter [01500, 05004], lr: 0.153862, loss: 1.9563
2022-08-06 00:16:54 - train: epoch 0036, iter [01600, 05004], lr: 0.153807, loss: 1.8356
2022-08-06 00:18:28 - train: epoch 0036, iter [01700, 05004], lr: 0.153751, loss: 1.7838
2022-08-06 00:20:02 - train: epoch 0036, iter [01800, 05004], lr: 0.153695, loss: 1.5728
2022-08-06 00:21:35 - train: epoch 0036, iter [01900, 05004], lr: 0.153639, loss: 1.7755
2022-08-06 00:23:09 - train: epoch 0036, iter [02000, 05004], lr: 0.153584, loss: 1.5995
2022-08-06 00:24:43 - train: epoch 0036, iter [02100, 05004], lr: 0.153528, loss: 1.7791
2022-08-06 00:26:17 - train: epoch 0036, iter [02200, 05004], lr: 0.153472, loss: 1.8457
2022-08-06 00:27:51 - train: epoch 0036, iter [02300, 05004], lr: 0.153416, loss: 1.8761
2022-08-06 00:29:25 - train: epoch 0036, iter [02400, 05004], lr: 0.153360, loss: 1.8139
2022-08-06 00:30:58 - train: epoch 0036, iter [02500, 05004], lr: 0.153304, loss: 1.6260
2022-08-06 00:32:32 - train: epoch 0036, iter [02600, 05004], lr: 0.153248, loss: 1.8541
2022-08-06 00:34:06 - train: epoch 0036, iter [02700, 05004], lr: 0.153192, loss: 1.7135
2022-08-06 00:35:40 - train: epoch 0036, iter [02800, 05004], lr: 0.153136, loss: 1.7114
2022-08-06 00:37:14 - train: epoch 0036, iter [02900, 05004], lr: 0.153080, loss: 1.6911
2022-08-06 00:38:47 - train: epoch 0036, iter [03000, 05004], lr: 0.153024, loss: 1.7665
2022-08-06 00:40:21 - train: epoch 0036, iter [03100, 05004], lr: 0.152968, loss: 1.9724
2022-08-06 00:41:55 - train: epoch 0036, iter [03200, 05004], lr: 0.152912, loss: 1.7596
2022-08-06 00:43:29 - train: epoch 0036, iter [03300, 05004], lr: 0.152856, loss: 1.6722
2022-08-06 00:45:03 - train: epoch 0036, iter [03400, 05004], lr: 0.152800, loss: 1.7106
2022-08-06 00:46:36 - train: epoch 0036, iter [03500, 05004], lr: 0.152744, loss: 1.9440
2022-08-06 00:48:10 - train: epoch 0036, iter [03600, 05004], lr: 0.152688, loss: 1.9185
2022-08-06 00:49:44 - train: epoch 0036, iter [03700, 05004], lr: 0.152632, loss: 1.7980
2022-08-06 00:51:17 - train: epoch 0036, iter [03800, 05004], lr: 0.152575, loss: 1.5965
2022-08-06 00:52:51 - train: epoch 0036, iter [03900, 05004], lr: 0.152519, loss: 1.7101
2022-08-06 00:54:25 - train: epoch 0036, iter [04000, 05004], lr: 0.152463, loss: 1.6717
2022-08-06 00:55:59 - train: epoch 0036, iter [04100, 05004], lr: 0.152407, loss: 1.6325
2022-08-06 00:57:32 - train: epoch 0036, iter [04200, 05004], lr: 0.152350, loss: 1.7600
2022-08-06 00:59:06 - train: epoch 0036, iter [04300, 05004], lr: 0.152294, loss: 1.5247
2022-08-06 01:00:40 - train: epoch 0036, iter [04400, 05004], lr: 0.152238, loss: 1.9642
2022-08-06 01:02:14 - train: epoch 0036, iter [04500, 05004], lr: 0.152181, loss: 1.6475
2022-08-06 01:03:48 - train: epoch 0036, iter [04600, 05004], lr: 0.152125, loss: 1.6843
2022-08-06 01:05:21 - train: epoch 0036, iter [04700, 05004], lr: 0.152069, loss: 1.8944
2022-08-06 01:06:55 - train: epoch 0036, iter [04800, 05004], lr: 0.152012, loss: 1.5997
2022-08-06 01:08:29 - train: epoch 0036, iter [04900, 05004], lr: 0.151956, loss: 1.6924
2022-08-06 01:10:03 - train: epoch 0036, iter [05000, 05004], lr: 0.151899, loss: 1.8590
2022-08-06 01:10:07 - train: epoch 036, train_loss: 1.7762
2022-08-06 01:12:12 - eval: epoch: 036, acc1: 62.130%, acc5: 85.092%, test_loss: 1.5649, per_image_load_time: 1.319ms, per_image_inference_time: 1.283ms
2022-08-06 01:12:12 - until epoch: 036, best_acc1: 63.202%
2022-08-06 01:12:12 - epoch 037 lr: 0.151896
2022-08-06 01:13:55 - train: epoch 0037, iter [00100, 05004], lr: 0.151840, loss: 1.6138
2022-08-06 01:15:30 - train: epoch 0037, iter [00200, 05004], lr: 0.151784, loss: 1.4737
2022-08-06 01:17:03 - train: epoch 0037, iter [00300, 05004], lr: 0.151727, loss: 1.7456
2022-08-06 01:18:37 - train: epoch 0037, iter [00400, 05004], lr: 0.151671, loss: 1.5632
2022-08-06 01:20:11 - train: epoch 0037, iter [00500, 05004], lr: 0.151614, loss: 1.6655
2022-08-06 01:21:45 - train: epoch 0037, iter [00600, 05004], lr: 0.151558, loss: 1.9550
2022-08-06 01:23:19 - train: epoch 0037, iter [00700, 05004], lr: 0.151501, loss: 1.6478
2022-08-06 01:24:53 - train: epoch 0037, iter [00800, 05004], lr: 0.151444, loss: 1.8261
2022-08-06 01:26:27 - train: epoch 0037, iter [00900, 05004], lr: 0.151388, loss: 1.8140
2022-08-06 01:28:01 - train: epoch 0037, iter [01000, 05004], lr: 0.151331, loss: 1.7456
2022-08-06 01:29:35 - train: epoch 0037, iter [01100, 05004], lr: 0.151274, loss: 1.6882
2022-08-06 01:31:09 - train: epoch 0037, iter [01200, 05004], lr: 0.151217, loss: 1.7508
2022-08-06 01:32:43 - train: epoch 0037, iter [01300, 05004], lr: 0.151161, loss: 1.7954
2022-08-06 01:34:17 - train: epoch 0037, iter [01400, 05004], lr: 0.151104, loss: 2.0178
2022-08-06 01:35:51 - train: epoch 0037, iter [01500, 05004], lr: 0.151047, loss: 1.6381
2022-08-06 01:37:25 - train: epoch 0037, iter [01600, 05004], lr: 0.150990, loss: 1.6095
2022-08-06 01:38:59 - train: epoch 0037, iter [01700, 05004], lr: 0.150933, loss: 1.7855
2022-08-06 01:40:33 - train: epoch 0037, iter [01800, 05004], lr: 0.150876, loss: 1.7161
2022-08-06 01:42:07 - train: epoch 0037, iter [01900, 05004], lr: 0.150820, loss: 1.6801
2022-08-06 01:43:41 - train: epoch 0037, iter [02000, 05004], lr: 0.150763, loss: 1.6797
2022-08-06 01:45:15 - train: epoch 0037, iter [02100, 05004], lr: 0.150706, loss: 1.6082
2022-08-06 01:46:49 - train: epoch 0037, iter [02200, 05004], lr: 0.150649, loss: 1.8645
2022-08-06 01:48:23 - train: epoch 0037, iter [02300, 05004], lr: 0.150592, loss: 1.7480
2022-08-06 01:49:57 - train: epoch 0037, iter [02400, 05004], lr: 0.150535, loss: 1.7470
2022-08-06 01:51:31 - train: epoch 0037, iter [02500, 05004], lr: 0.150478, loss: 1.7151
2022-08-06 01:53:05 - train: epoch 0037, iter [02600, 05004], lr: 0.150421, loss: 2.0330
2022-08-06 01:54:39 - train: epoch 0037, iter [02700, 05004], lr: 0.150364, loss: 1.8570
2022-08-06 01:56:13 - train: epoch 0037, iter [02800, 05004], lr: 0.150306, loss: 1.9580
2022-08-06 01:57:47 - train: epoch 0037, iter [02900, 05004], lr: 0.150249, loss: 1.9471
2022-08-06 01:59:21 - train: epoch 0037, iter [03000, 05004], lr: 0.150192, loss: 1.7733
2022-08-06 02:00:55 - train: epoch 0037, iter [03100, 05004], lr: 0.150135, loss: 1.8764
2022-08-06 02:02:29 - train: epoch 0037, iter [03200, 05004], lr: 0.150078, loss: 1.6268
2022-08-06 02:04:03 - train: epoch 0037, iter [03300, 05004], lr: 0.150021, loss: 1.6073
2022-08-06 02:05:37 - train: epoch 0037, iter [03400, 05004], lr: 0.149963, loss: 1.5987
2022-08-06 02:07:11 - train: epoch 0037, iter [03500, 05004], lr: 0.149906, loss: 1.7892
2022-08-06 02:08:45 - train: epoch 0037, iter [03600, 05004], lr: 0.149849, loss: 1.8721
2022-08-06 02:10:19 - train: epoch 0037, iter [03700, 05004], lr: 0.149792, loss: 1.6933
2022-08-06 02:11:53 - train: epoch 0037, iter [03800, 05004], lr: 0.149734, loss: 1.7711
2022-08-06 02:13:27 - train: epoch 0037, iter [03900, 05004], lr: 0.149677, loss: 1.8615
2022-08-06 02:15:01 - train: epoch 0037, iter [04000, 05004], lr: 0.149619, loss: 1.7776
2022-08-06 02:16:35 - train: epoch 0037, iter [04100, 05004], lr: 0.149562, loss: 1.7078
2022-08-06 02:18:09 - train: epoch 0037, iter [04200, 05004], lr: 0.149505, loss: 1.7769
2022-08-06 02:19:43 - train: epoch 0037, iter [04300, 05004], lr: 0.149447, loss: 1.4786
2022-08-06 02:21:17 - train: epoch 0037, iter [04400, 05004], lr: 0.149390, loss: 1.7784
2022-08-06 02:22:51 - train: epoch 0037, iter [04500, 05004], lr: 0.149332, loss: 1.9525
2022-08-06 02:24:25 - train: epoch 0037, iter [04600, 05004], lr: 0.149275, loss: 1.7894
2022-08-06 02:25:59 - train: epoch 0037, iter [04700, 05004], lr: 0.149217, loss: 1.9495
2022-08-06 02:27:33 - train: epoch 0037, iter [04800, 05004], lr: 0.149160, loss: 1.7592
2022-08-06 02:29:07 - train: epoch 0037, iter [04900, 05004], lr: 0.149102, loss: 1.8594
2022-08-06 02:30:41 - train: epoch 0037, iter [05000, 05004], lr: 0.149045, loss: 1.6283
2022-08-06 02:30:45 - train: epoch 037, train_loss: 1.7663
2022-08-06 02:32:50 - eval: epoch: 037, acc1: 63.996%, acc5: 86.260%, test_loss: 1.4843, per_image_load_time: 2.335ms, per_image_inference_time: 1.158ms
2022-08-06 02:32:50 - until epoch: 037, best_acc1: 63.996%
2022-08-06 02:32:50 - epoch 038 lr: 0.149042
2022-08-06 02:34:33 - train: epoch 0038, iter [00100, 05004], lr: 0.148985, loss: 1.9019
2022-08-06 02:36:07 - train: epoch 0038, iter [00200, 05004], lr: 0.148927, loss: 1.5200
2022-08-06 02:37:41 - train: epoch 0038, iter [00300, 05004], lr: 0.148869, loss: 1.5624
2022-08-06 02:39:15 - train: epoch 0038, iter [00400, 05004], lr: 0.148812, loss: 1.7636
2022-08-06 02:40:49 - train: epoch 0038, iter [00500, 05004], lr: 0.148754, loss: 1.6866
2022-08-06 02:42:23 - train: epoch 0038, iter [00600, 05004], lr: 0.148696, loss: 1.8221
2022-08-06 02:43:57 - train: epoch 0038, iter [00700, 05004], lr: 0.148639, loss: 1.5928
2022-08-06 02:45:31 - train: epoch 0038, iter [00800, 05004], lr: 0.148581, loss: 1.9025
2022-08-06 02:47:05 - train: epoch 0038, iter [00900, 05004], lr: 0.148523, loss: 1.7649
2022-08-06 02:48:39 - train: epoch 0038, iter [01000, 05004], lr: 0.148465, loss: 1.9090
2022-08-06 02:50:13 - train: epoch 0038, iter [01100, 05004], lr: 0.148408, loss: 1.7004
2022-08-06 02:51:47 - train: epoch 0038, iter [01200, 05004], lr: 0.148350, loss: 1.8448
2022-08-06 02:53:21 - train: epoch 0038, iter [01300, 05004], lr: 0.148292, loss: 1.7191
2022-08-06 02:54:55 - train: epoch 0038, iter [01400, 05004], lr: 0.148234, loss: 2.0211
2022-08-06 02:56:29 - train: epoch 0038, iter [01500, 05004], lr: 0.148176, loss: 1.6916
2022-08-06 02:58:03 - train: epoch 0038, iter [01600, 05004], lr: 0.148118, loss: 1.8434
2022-08-06 02:59:37 - train: epoch 0038, iter [01700, 05004], lr: 0.148060, loss: 1.8547
2022-08-06 03:01:11 - train: epoch 0038, iter [01800, 05004], lr: 0.148002, loss: 1.8804
2022-08-06 03:02:45 - train: epoch 0038, iter [01900, 05004], lr: 0.147944, loss: 1.6061
2022-08-06 03:04:19 - train: epoch 0038, iter [02000, 05004], lr: 0.147886, loss: 1.6620
2022-08-06 03:05:53 - train: epoch 0038, iter [02100, 05004], lr: 0.147828, loss: 1.7440
2022-08-06 03:07:27 - train: epoch 0038, iter [02200, 05004], lr: 0.147770, loss: 1.5735
2022-08-06 03:09:01 - train: epoch 0038, iter [02300, 05004], lr: 0.147712, loss: 1.9387
2022-08-06 03:10:35 - train: epoch 0038, iter [02400, 05004], lr: 0.147654, loss: 2.0423
2022-08-06 03:12:09 - train: epoch 0038, iter [02500, 05004], lr: 0.147596, loss: 1.5093
2022-08-06 03:13:43 - train: epoch 0038, iter [02600, 05004], lr: 0.147538, loss: 1.9211
2022-08-06 03:15:17 - train: epoch 0038, iter [02700, 05004], lr: 0.147480, loss: 1.8646
2022-08-06 03:16:51 - train: epoch 0038, iter [02800, 05004], lr: 0.147421, loss: 2.0791
2022-08-06 03:18:24 - train: epoch 0038, iter [02900, 05004], lr: 0.147363, loss: 1.8129
2022-08-06 03:19:58 - train: epoch 0038, iter [03000, 05004], lr: 0.147305, loss: 1.5897
2022-08-06 03:21:32 - train: epoch 0038, iter [03100, 05004], lr: 0.147247, loss: 1.9031
2022-08-06 03:23:06 - train: epoch 0038, iter [03200, 05004], lr: 0.147189, loss: 1.5839
2022-08-06 03:24:40 - train: epoch 0038, iter [03300, 05004], lr: 0.147130, loss: 1.5119
2022-08-06 03:26:14 - train: epoch 0038, iter [03400, 05004], lr: 0.147072, loss: 1.5680
2022-08-06 03:27:48 - train: epoch 0038, iter [03500, 05004], lr: 0.147014, loss: 1.5789
2022-08-06 03:29:22 - train: epoch 0038, iter [03600, 05004], lr: 0.146955, loss: 1.5995
2022-08-06 03:30:56 - train: epoch 0038, iter [03700, 05004], lr: 0.146897, loss: 1.6074
2022-08-06 03:32:30 - train: epoch 0038, iter [03800, 05004], lr: 0.146839, loss: 1.7600
2022-08-06 03:34:04 - train: epoch 0038, iter [03900, 05004], lr: 0.146780, loss: 1.7885
2022-08-06 03:35:38 - train: epoch 0038, iter [04000, 05004], lr: 0.146722, loss: 1.7626
2022-08-06 03:37:12 - train: epoch 0038, iter [04100, 05004], lr: 0.146663, loss: 1.6740
2022-08-06 03:38:46 - train: epoch 0038, iter [04200, 05004], lr: 0.146605, loss: 1.7598
2022-08-06 03:40:20 - train: epoch 0038, iter [04300, 05004], lr: 0.146546, loss: 1.8858
2022-08-06 03:41:54 - train: epoch 0038, iter [04400, 05004], lr: 0.146488, loss: 1.7662
2022-08-06 03:43:28 - train: epoch 0038, iter [04500, 05004], lr: 0.146429, loss: 1.9378
2022-08-06 03:45:02 - train: epoch 0038, iter [04600, 05004], lr: 0.146371, loss: 1.8209
2022-08-06 03:46:36 - train: epoch 0038, iter [04700, 05004], lr: 0.146312, loss: 1.8614
2022-08-06 03:48:10 - train: epoch 0038, iter [04800, 05004], lr: 0.146254, loss: 1.5617
2022-08-06 03:49:44 - train: epoch 0038, iter [04900, 05004], lr: 0.146195, loss: 1.6955
2022-08-06 03:51:18 - train: epoch 0038, iter [05000, 05004], lr: 0.146136, loss: 1.5916
2022-08-06 03:51:23 - train: epoch 038, train_loss: 1.7545
2022-08-06 03:53:24 - eval: epoch: 038, acc1: 63.522%, acc5: 85.884%, test_loss: 1.5068, per_image_load_time: 1.227ms, per_image_inference_time: 1.157ms
2022-08-06 03:53:24 - until epoch: 038, best_acc1: 63.996%
2022-08-06 03:53:24 - epoch 039 lr: 0.146134
2022-08-06 03:55:07 - train: epoch 0039, iter [00100, 05004], lr: 0.146075, loss: 1.9925
2022-08-06 03:56:41 - train: epoch 0039, iter [00200, 05004], lr: 0.146017, loss: 1.7294
2022-08-06 03:58:15 - train: epoch 0039, iter [00300, 05004], lr: 0.145958, loss: 1.5271
2022-08-06 03:59:49 - train: epoch 0039, iter [00400, 05004], lr: 0.145899, loss: 1.5877
2022-08-06 04:01:23 - train: epoch 0039, iter [00500, 05004], lr: 0.145841, loss: 1.8390
2022-08-06 04:02:57 - train: epoch 0039, iter [00600, 05004], lr: 0.145782, loss: 1.6893
2022-08-06 04:04:31 - train: epoch 0039, iter [00700, 05004], lr: 0.145723, loss: 2.2204
2022-08-06 04:06:05 - train: epoch 0039, iter [00800, 05004], lr: 0.145664, loss: 1.6378
2022-08-06 04:07:39 - train: epoch 0039, iter [00900, 05004], lr: 0.145606, loss: 1.7758
2022-08-06 04:09:13 - train: epoch 0039, iter [01000, 05004], lr: 0.145547, loss: 1.7246
2022-08-06 04:10:47 - train: epoch 0039, iter [01100, 05004], lr: 0.145488, loss: 1.7240
2022-08-06 04:12:21 - train: epoch 0039, iter [01200, 05004], lr: 0.145429, loss: 1.7103
2022-08-06 04:13:55 - train: epoch 0039, iter [01300, 05004], lr: 0.145370, loss: 1.7585
2022-08-06 04:15:29 - train: epoch 0039, iter [01400, 05004], lr: 0.145311, loss: 1.8027
2022-08-06 04:17:03 - train: epoch 0039, iter [01500, 05004], lr: 0.145252, loss: 1.8515
2022-08-06 04:18:37 - train: epoch 0039, iter [01600, 05004], lr: 0.145193, loss: 1.7801
2022-08-06 04:20:11 - train: epoch 0039, iter [01700, 05004], lr: 0.145134, loss: 1.7312
2022-08-06 04:21:45 - train: epoch 0039, iter [01800, 05004], lr: 0.145075, loss: 1.7700
2022-08-06 04:23:19 - train: epoch 0039, iter [01900, 05004], lr: 0.145016, loss: 1.7097
2022-08-06 04:24:53 - train: epoch 0039, iter [02000, 05004], lr: 0.144957, loss: 1.6737
2022-08-06 04:26:27 - train: epoch 0039, iter [02100, 05004], lr: 0.144898, loss: 1.8435
2022-08-06 04:28:01 - train: epoch 0039, iter [02200, 05004], lr: 0.144839, loss: 1.7403
2022-08-06 04:29:35 - train: epoch 0039, iter [02300, 05004], lr: 0.144780, loss: 1.9942
2022-08-06 04:31:09 - train: epoch 0039, iter [02400, 05004], lr: 0.144721, loss: 1.8684
2022-08-06 04:32:43 - train: epoch 0039, iter [02500, 05004], lr: 0.144662, loss: 1.7950
2022-08-06 04:34:17 - train: epoch 0039, iter [02600, 05004], lr: 0.144603, loss: 1.6694
2022-08-06 04:35:51 - train: epoch 0039, iter [02700, 05004], lr: 0.144544, loss: 1.8555
2022-08-06 04:37:25 - train: epoch 0039, iter [02800, 05004], lr: 0.144485, loss: 1.7466
2022-08-06 04:38:59 - train: epoch 0039, iter [02900, 05004], lr: 0.144425, loss: 1.6941
2022-08-06 04:40:33 - train: epoch 0039, iter [03000, 05004], lr: 0.144366, loss: 1.8732
2022-08-06 04:42:07 - train: epoch 0039, iter [03100, 05004], lr: 0.144307, loss: 1.6833
2022-08-06 04:43:41 - train: epoch 0039, iter [03200, 05004], lr: 0.144248, loss: 1.9190
2022-08-06 04:45:15 - train: epoch 0039, iter [03300, 05004], lr: 0.144188, loss: 1.9891
2022-08-06 04:46:49 - train: epoch 0039, iter [03400, 05004], lr: 0.144129, loss: 1.7534
2022-08-06 04:48:23 - train: epoch 0039, iter [03500, 05004], lr: 0.144070, loss: 1.9212
2022-08-06 04:49:57 - train: epoch 0039, iter [03600, 05004], lr: 0.144010, loss: 1.9430
2022-08-06 04:51:31 - train: epoch 0039, iter [03700, 05004], lr: 0.143951, loss: 1.5933
2022-08-06 04:53:05 - train: epoch 0039, iter [03800, 05004], lr: 0.143892, loss: 1.6120
2022-08-06 04:54:39 - train: epoch 0039, iter [03900, 05004], lr: 0.143832, loss: 1.8542
2022-08-06 04:56:13 - train: epoch 0039, iter [04000, 05004], lr: 0.143773, loss: 1.7947
2022-08-06 04:57:47 - train: epoch 0039, iter [04100, 05004], lr: 0.143714, loss: 1.6641
2022-08-06 04:59:21 - train: epoch 0039, iter [04200, 05004], lr: 0.143654, loss: 1.8704
2022-08-06 05:00:55 - train: epoch 0039, iter [04300, 05004], lr: 0.143595, loss: 1.7094
2022-08-06 05:02:29 - train: epoch 0039, iter [04400, 05004], lr: 0.143535, loss: 1.6271
2022-08-06 05:04:03 - train: epoch 0039, iter [04500, 05004], lr: 0.143476, loss: 1.6123
2022-08-06 05:05:37 - train: epoch 0039, iter [04600, 05004], lr: 0.143416, loss: 1.7058
2022-08-06 05:07:11 - train: epoch 0039, iter [04700, 05004], lr: 0.143357, loss: 1.4991
2022-08-06 05:08:45 - train: epoch 0039, iter [04800, 05004], lr: 0.143297, loss: 1.6418
2022-08-06 05:10:19 - train: epoch 0039, iter [04900, 05004], lr: 0.143237, loss: 1.6430
2022-08-06 05:11:53 - train: epoch 0039, iter [05000, 05004], lr: 0.143178, loss: 1.6861
2022-08-06 05:11:58 - train: epoch 039, train_loss: 1.7414
2022-08-06 05:14:04 - eval: epoch: 039, acc1: 63.956%, acc5: 86.414%, test_loss: 1.4790, per_image_load_time: 1.556ms, per_image_inference_time: 1.183ms
2022-08-06 05:14:04 - until epoch: 039, best_acc1: 63.996%
2022-08-06 05:14:04 - epoch 040 lr: 0.143175
2022-08-06 05:15:47 - train: epoch 0040, iter [00100, 05004], lr: 0.143116, loss: 1.8173
2022-08-06 05:17:21 - train: epoch 0040, iter [00200, 05004], lr: 0.143056, loss: 1.6746
2022-08-06 05:18:55 - train: epoch 0040, iter [00300, 05004], lr: 0.142997, loss: 1.9037
2022-08-06 05:20:29 - train: epoch 0040, iter [00400, 05004], lr: 0.142937, loss: 1.5322
2022-08-06 05:22:03 - train: epoch 0040, iter [00500, 05004], lr: 0.142877, loss: 1.6385
2022-08-06 05:23:37 - train: epoch 0040, iter [00600, 05004], lr: 0.142817, loss: 1.7883
2022-08-06 05:25:11 - train: epoch 0040, iter [00700, 05004], lr: 0.142758, loss: 1.7543
2022-08-06 05:26:45 - train: epoch 0040, iter [00800, 05004], lr: 0.142698, loss: 1.7811
2022-08-06 05:28:19 - train: epoch 0040, iter [00900, 05004], lr: 0.142638, loss: 1.8425
2022-08-06 05:29:53 - train: epoch 0040, iter [01000, 05004], lr: 0.142578, loss: 1.5281
2022-08-06 05:31:27 - train: epoch 0040, iter [01100, 05004], lr: 0.142519, loss: 1.5375
2022-08-06 05:33:01 - train: epoch 0040, iter [01200, 05004], lr: 0.142459, loss: 1.7267
2022-08-06 05:34:35 - train: epoch 0040, iter [01300, 05004], lr: 0.142399, loss: 1.5383
2022-08-06 05:36:09 - train: epoch 0040, iter [01400, 05004], lr: 0.142339, loss: 1.8476
2022-08-06 05:37:43 - train: epoch 0040, iter [01500, 05004], lr: 0.142279, loss: 1.7515
2022-08-06 05:39:17 - train: epoch 0040, iter [01600, 05004], lr: 0.142219, loss: 1.8578
2022-08-06 05:40:51 - train: epoch 0040, iter [01700, 05004], lr: 0.142159, loss: 1.4915
2022-08-06 05:42:25 - train: epoch 0040, iter [01800, 05004], lr: 0.142099, loss: 1.7079
2022-08-06 05:43:59 - train: epoch 0040, iter [01900, 05004], lr: 0.142039, loss: 1.7012
2022-08-06 05:45:33 - train: epoch 0040, iter [02000, 05004], lr: 0.141980, loss: 1.7877
2022-08-06 05:47:07 - train: epoch 0040, iter [02100, 05004], lr: 0.141920, loss: 1.8169
2022-08-06 05:48:41 - train: epoch 0040, iter [02200, 05004], lr: 0.141860, loss: 1.6314
2022-08-06 05:50:15 - train: epoch 0040, iter [02300, 05004], lr: 0.141799, loss: 1.7816
2022-08-06 05:51:49 - train: epoch 0040, iter [02400, 05004], lr: 0.141739, loss: 1.7016
2022-08-06 05:53:23 - train: epoch 0040, iter [02500, 05004], lr: 0.141679, loss: 1.9049
2022-08-06 05:54:57 - train: epoch 0040, iter [02600, 05004], lr: 0.141619, loss: 1.7058
2022-08-06 05:56:31 - train: epoch 0040, iter [02700, 05004], lr: 0.141559, loss: 1.7356
2022-08-06 05:58:05 - train: epoch 0040, iter [02800, 05004], lr: 0.141499, loss: 1.5971
2022-08-06 05:59:39 - train: epoch 0040, iter [02900, 05004], lr: 0.141439, loss: 1.8810
2022-08-06 06:01:13 - train: epoch 0040, iter [03000, 05004], lr: 0.141379, loss: 1.6736
2022-08-06 06:02:48 - train: epoch 0040, iter [03100, 05004], lr: 0.141319, loss: 1.8503
2022-08-06 06:04:22 - train: epoch 0040, iter [03200, 05004], lr: 0.141258, loss: 1.9628
2022-08-06 06:05:56 - train: epoch 0040, iter [03300, 05004], lr: 0.141198, loss: 1.9954
2022-08-06 06:07:30 - train: epoch 0040, iter [03400, 05004], lr: 0.141138, loss: 1.6995
2022-08-06 06:09:04 - train: epoch 0040, iter [03500, 05004], lr: 0.141078, loss: 1.8905
2022-08-06 06:10:38 - train: epoch 0040, iter [03600, 05004], lr: 0.141017, loss: 1.3880
2022-08-06 06:12:12 - train: epoch 0040, iter [03700, 05004], lr: 0.140957, loss: 1.6434
2022-08-06 06:13:46 - train: epoch 0040, iter [03800, 05004], lr: 0.140897, loss: 1.8078
2022-08-06 06:15:21 - train: epoch 0040, iter [03900, 05004], lr: 0.140837, loss: 1.7634
2022-08-06 06:16:55 - train: epoch 0040, iter [04000, 05004], lr: 0.140776, loss: 1.8564
2022-08-06 06:18:29 - train: epoch 0040, iter [04100, 05004], lr: 0.140716, loss: 1.6339
2022-08-06 06:20:03 - train: epoch 0040, iter [04200, 05004], lr: 0.140656, loss: 1.6123
2022-08-06 06:21:37 - train: epoch 0040, iter [04300, 05004], lr: 0.140595, loss: 1.6910
2022-08-06 06:23:11 - train: epoch 0040, iter [04400, 05004], lr: 0.140535, loss: 1.5693
2022-08-06 06:24:45 - train: epoch 0040, iter [04500, 05004], lr: 0.140474, loss: 1.5315
2022-08-06 06:26:19 - train: epoch 0040, iter [04600, 05004], lr: 0.140414, loss: 1.6639
2022-08-06 06:27:53 - train: epoch 0040, iter [04700, 05004], lr: 0.140353, loss: 1.8069
2022-08-06 06:29:27 - train: epoch 0040, iter [04800, 05004], lr: 0.140293, loss: 1.6845
2022-08-06 06:31:01 - train: epoch 0040, iter [04900, 05004], lr: 0.140232, loss: 1.7194
2022-08-06 06:32:35 - train: epoch 0040, iter [05000, 05004], lr: 0.140172, loss: 1.6699
2022-08-06 06:32:40 - train: epoch 040, train_loss: 1.7307
2022-08-06 06:34:43 - eval: epoch: 040, acc1: 64.272%, acc5: 86.324%, test_loss: 1.4820, per_image_load_time: 1.197ms, per_image_inference_time: 1.197ms
2022-08-06 06:34:43 - until epoch: 040, best_acc1: 64.272%
2022-08-06 06:34:43 - epoch 041 lr: 0.140169
2022-08-06 06:36:26 - train: epoch 0041, iter [00100, 05004], lr: 0.140109, loss: 1.9286
2022-08-06 06:38:00 - train: epoch 0041, iter [00200, 05004], lr: 0.140048, loss: 1.9018
2022-08-06 06:39:34 - train: epoch 0041, iter [00300, 05004], lr: 0.139988, loss: 1.6326
2022-08-06 06:41:08 - train: epoch 0041, iter [00400, 05004], lr: 0.139927, loss: 1.7098
2022-08-06 06:42:42 - train: epoch 0041, iter [00500, 05004], lr: 0.139867, loss: 1.4386
2022-08-06 06:44:16 - train: epoch 0041, iter [00600, 05004], lr: 0.139806, loss: 1.6307
2022-08-06 06:45:50 - train: epoch 0041, iter [00700, 05004], lr: 0.139745, loss: 1.4855
2022-08-06 06:47:24 - train: epoch 0041, iter [00800, 05004], lr: 0.139685, loss: 1.9661
2022-08-06 06:48:58 - train: epoch 0041, iter [00900, 05004], lr: 0.139624, loss: 1.5202
2022-08-06 06:50:32 - train: epoch 0041, iter [01000, 05004], lr: 0.139563, loss: 1.8890
2022-08-06 06:52:06 - train: epoch 0041, iter [01100, 05004], lr: 0.139503, loss: 1.7452
2022-08-06 06:53:40 - train: epoch 0041, iter [01200, 05004], lr: 0.139442, loss: 1.7638
2022-08-06 06:55:14 - train: epoch 0041, iter [01300, 05004], lr: 0.139381, loss: 1.5461
2022-08-06 06:56:47 - train: epoch 0041, iter [01400, 05004], lr: 0.139321, loss: 1.7692
2022-08-06 06:58:21 - train: epoch 0041, iter [01500, 05004], lr: 0.139260, loss: 1.8818
2022-08-06 06:59:56 - train: epoch 0041, iter [01600, 05004], lr: 0.139199, loss: 1.8227
2022-08-06 07:01:30 - train: epoch 0041, iter [01700, 05004], lr: 0.139138, loss: 1.5569
2022-08-06 07:03:04 - train: epoch 0041, iter [01800, 05004], lr: 0.139077, loss: 1.5363
2022-08-06 07:04:38 - train: epoch 0041, iter [01900, 05004], lr: 0.139017, loss: 1.9133
2022-08-06 07:06:12 - train: epoch 0041, iter [02000, 05004], lr: 0.138956, loss: 1.6810
2022-08-06 07:07:46 - train: epoch 0041, iter [02100, 05004], lr: 0.138895, loss: 1.6491
2022-08-06 07:09:19 - train: epoch 0041, iter [02200, 05004], lr: 0.138834, loss: 1.4933
2022-08-06 07:10:54 - train: epoch 0041, iter [02300, 05004], lr: 0.138773, loss: 1.4247
2022-08-06 07:12:28 - train: epoch 0041, iter [02400, 05004], lr: 0.138712, loss: 1.9170
2022-08-06 07:14:02 - train: epoch 0041, iter [02500, 05004], lr: 0.138651, loss: 1.6629
2022-08-06 07:15:36 - train: epoch 0041, iter [02600, 05004], lr: 0.138590, loss: 1.8949
2022-08-06 07:17:10 - train: epoch 0041, iter [02700, 05004], lr: 0.138529, loss: 1.8422
2022-08-06 07:18:44 - train: epoch 0041, iter [02800, 05004], lr: 0.138468, loss: 1.7445
2022-08-06 07:20:18 - train: epoch 0041, iter [02900, 05004], lr: 0.138407, loss: 1.8280
2022-08-06 07:21:53 - train: epoch 0041, iter [03000, 05004], lr: 0.138346, loss: 1.8567
2022-08-06 07:23:27 - train: epoch 0041, iter [03100, 05004], lr: 0.138285, loss: 1.7052
2022-08-06 07:25:01 - train: epoch 0041, iter [03200, 05004], lr: 0.138224, loss: 1.5711
2022-08-06 07:26:35 - train: epoch 0041, iter [03300, 05004], lr: 0.138163, loss: 1.6911
2022-08-06 07:28:09 - train: epoch 0041, iter [03400, 05004], lr: 0.138102, loss: 1.6524
2022-08-06 07:29:43 - train: epoch 0041, iter [03500, 05004], lr: 0.138041, loss: 1.6355
2022-08-06 07:31:18 - train: epoch 0041, iter [03600, 05004], lr: 0.137980, loss: 1.7189
2022-08-06 07:32:52 - train: epoch 0041, iter [03700, 05004], lr: 0.137919, loss: 1.6625
2022-08-06 07:34:26 - train: epoch 0041, iter [03800, 05004], lr: 0.137857, loss: 1.6004
2022-08-06 07:36:00 - train: epoch 0041, iter [03900, 05004], lr: 0.137796, loss: 1.7575
2022-08-06 07:37:34 - train: epoch 0041, iter [04000, 05004], lr: 0.137735, loss: 1.6289
2022-08-06 07:39:08 - train: epoch 0041, iter [04100, 05004], lr: 0.137674, loss: 1.8024
2022-08-06 07:40:43 - train: epoch 0041, iter [04200, 05004], lr: 0.137613, loss: 1.5022
2022-08-06 07:42:17 - train: epoch 0041, iter [04300, 05004], lr: 0.137551, loss: 1.6276
2022-08-06 07:43:51 - train: epoch 0041, iter [04400, 05004], lr: 0.137490, loss: 1.5465
2022-08-06 07:45:25 - train: epoch 0041, iter [04500, 05004], lr: 0.137429, loss: 2.0446
2022-08-06 07:46:59 - train: epoch 0041, iter [04600, 05004], lr: 0.137368, loss: 1.8989
2022-08-06 07:48:33 - train: epoch 0041, iter [04700, 05004], lr: 0.137306, loss: 1.8859
2022-08-06 07:50:07 - train: epoch 0041, iter [04800, 05004], lr: 0.137245, loss: 1.7203
2022-08-06 07:51:41 - train: epoch 0041, iter [04900, 05004], lr: 0.137184, loss: 1.8397
2022-08-06 07:53:16 - train: epoch 0041, iter [05000, 05004], lr: 0.137122, loss: 1.7071
2022-08-06 07:53:20 - train: epoch 041, train_loss: 1.7206
2022-08-06 07:55:27 - eval: epoch: 041, acc1: 64.098%, acc5: 86.458%, test_loss: 1.4841, per_image_load_time: 2.288ms, per_image_inference_time: 1.149ms
2022-08-06 07:55:27 - until epoch: 041, best_acc1: 64.272%
2022-08-06 07:55:27 - epoch 042 lr: 0.137119
2022-08-06 07:57:09 - train: epoch 0042, iter [00100, 05004], lr: 0.137058, loss: 1.3450
2022-08-06 07:58:44 - train: epoch 0042, iter [00200, 05004], lr: 0.136997, loss: 1.6336
2022-08-06 08:00:18 - train: epoch 0042, iter [00300, 05004], lr: 0.136936, loss: 1.7447
2022-08-06 08:01:52 - train: epoch 0042, iter [00400, 05004], lr: 0.136874, loss: 1.4421
2022-08-06 08:03:26 - train: epoch 0042, iter [00500, 05004], lr: 0.136813, loss: 1.6715
2022-08-06 08:05:00 - train: epoch 0042, iter [00600, 05004], lr: 0.136751, loss: 1.4410
2022-08-06 08:06:34 - train: epoch 0042, iter [00700, 05004], lr: 0.136690, loss: 1.8681
2022-08-06 08:08:08 - train: epoch 0042, iter [00800, 05004], lr: 0.136628, loss: 1.7051
2022-08-06 08:09:42 - train: epoch 0042, iter [00900, 05004], lr: 0.136567, loss: 1.8849
2022-08-06 08:11:16 - train: epoch 0042, iter [01000, 05004], lr: 0.136505, loss: 1.7727
2022-08-06 08:12:50 - train: epoch 0042, iter [01100, 05004], lr: 0.136444, loss: 1.5569
2022-08-06 08:14:24 - train: epoch 0042, iter [01200, 05004], lr: 0.136382, loss: 1.3141
2022-08-06 08:15:59 - train: epoch 0042, iter [01300, 05004], lr: 0.136321, loss: 1.7815
2022-08-06 08:17:33 - train: epoch 0042, iter [01400, 05004], lr: 0.136259, loss: 1.8421
2022-08-06 08:19:07 - train: epoch 0042, iter [01500, 05004], lr: 0.136197, loss: 1.5995
2022-08-06 08:20:41 - train: epoch 0042, iter [01600, 05004], lr: 0.136136, loss: 1.8107
2022-08-06 08:22:15 - train: epoch 0042, iter [01700, 05004], lr: 0.136074, loss: 1.6927
2022-08-06 08:23:49 - train: epoch 0042, iter [01800, 05004], lr: 0.136013, loss: 1.7507
2022-08-06 08:25:23 - train: epoch 0042, iter [01900, 05004], lr: 0.135951, loss: 1.8211
2022-08-06 08:26:57 - train: epoch 0042, iter [02000, 05004], lr: 0.135889, loss: 1.8242
2022-08-06 08:28:31 - train: epoch 0042, iter [02100, 05004], lr: 0.135828, loss: 1.5230
2022-08-06 08:30:05 - train: epoch 0042, iter [02200, 05004], lr: 0.135766, loss: 1.7343
2022-08-06 08:31:39 - train: epoch 0042, iter [02300, 05004], lr: 0.135704, loss: 1.6420
2022-08-06 08:33:13 - train: epoch 0042, iter [02400, 05004], lr: 0.135642, loss: 1.7530
2022-08-06 08:34:47 - train: epoch 0042, iter [02500, 05004], lr: 0.135581, loss: 1.7174
2022-08-06 08:36:22 - train: epoch 0042, iter [02600, 05004], lr: 0.135519, loss: 1.6569
2022-08-06 08:37:56 - train: epoch 0042, iter [02700, 05004], lr: 0.135457, loss: 1.4412
2022-08-06 08:39:30 - train: epoch 0042, iter [02800, 05004], lr: 0.135395, loss: 1.6608
2022-08-06 08:41:04 - train: epoch 0042, iter [02900, 05004], lr: 0.135333, loss: 1.5638
2022-08-06 08:42:39 - train: epoch 0042, iter [03000, 05004], lr: 0.135272, loss: 1.6955
2022-08-06 08:44:13 - train: epoch 0042, iter [03100, 05004], lr: 0.135210, loss: 1.8170
2022-08-06 08:45:47 - train: epoch 0042, iter [03200, 05004], lr: 0.135148, loss: 1.8871
2022-08-06 08:47:21 - train: epoch 0042, iter [03300, 05004], lr: 0.135086, loss: 1.7607
2022-08-06 08:48:55 - train: epoch 0042, iter [03400, 05004], lr: 0.135024, loss: 1.6211
2022-08-06 08:50:29 - train: epoch 0042, iter [03500, 05004], lr: 0.134962, loss: 1.5700
2022-08-06 08:52:04 - train: epoch 0042, iter [03600, 05004], lr: 0.134900, loss: 1.9380
2022-08-06 08:53:38 - train: epoch 0042, iter [03700, 05004], lr: 0.134838, loss: 1.8289
2022-08-06 08:55:12 - train: epoch 0042, iter [03800, 05004], lr: 0.134776, loss: 1.3975
2022-08-06 08:56:46 - train: epoch 0042, iter [03900, 05004], lr: 0.134714, loss: 1.8073
2022-08-06 08:58:20 - train: epoch 0042, iter [04000, 05004], lr: 0.134652, loss: 1.6173
2022-08-06 08:59:55 - train: epoch 0042, iter [04100, 05004], lr: 0.134590, loss: 1.7769
2022-08-06 09:01:29 - train: epoch 0042, iter [04200, 05004], lr: 0.134528, loss: 1.8099
2022-08-06 09:03:03 - train: epoch 0042, iter [04300, 05004], lr: 0.134466, loss: 1.5780
2022-08-06 09:04:37 - train: epoch 0042, iter [04400, 05004], lr: 0.134404, loss: 1.6915
2022-08-06 09:06:11 - train: epoch 0042, iter [04500, 05004], lr: 0.134342, loss: 1.7738
2022-08-06 09:07:45 - train: epoch 0042, iter [04600, 05004], lr: 0.134280, loss: 1.9473
2022-08-06 09:09:20 - train: epoch 0042, iter [04700, 05004], lr: 0.134218, loss: 1.7571
2022-08-06 09:10:54 - train: epoch 0042, iter [04800, 05004], lr: 0.134156, loss: 1.6477
2022-08-06 09:12:28 - train: epoch 0042, iter [04900, 05004], lr: 0.134094, loss: 1.6989
2022-08-06 09:14:02 - train: epoch 0042, iter [05000, 05004], lr: 0.134032, loss: 1.8302
2022-08-06 09:14:07 - train: epoch 042, train_loss: 1.7083
2022-08-06 09:16:09 - eval: epoch: 042, acc1: 64.434%, acc5: 86.514%, test_loss: 1.4743, per_image_load_time: 3.551ms, per_image_inference_time: 1.159ms
2022-08-06 09:16:10 - until epoch: 042, best_acc1: 64.434%
2022-08-06 09:16:10 - epoch 043 lr: 0.134029
2022-08-06 09:17:52 - train: epoch 0043, iter [00100, 05004], lr: 0.133967, loss: 1.8981
2022-08-06 09:19:26 - train: epoch 0043, iter [00200, 05004], lr: 0.133905, loss: 1.6882
2022-08-06 09:21:00 - train: epoch 0043, iter [00300, 05004], lr: 0.133843, loss: 1.3812
2022-08-06 09:22:34 - train: epoch 0043, iter [00400, 05004], lr: 0.133781, loss: 1.5091
2022-08-06 09:24:08 - train: epoch 0043, iter [00500, 05004], lr: 0.133718, loss: 1.4500
2022-08-06 09:25:42 - train: epoch 0043, iter [00600, 05004], lr: 0.133656, loss: 1.4562
2022-08-06 09:27:16 - train: epoch 0043, iter [00700, 05004], lr: 0.133594, loss: 1.7452
2022-08-06 09:28:50 - train: epoch 0043, iter [00800, 05004], lr: 0.133532, loss: 1.7715
2022-08-06 09:30:24 - train: epoch 0043, iter [00900, 05004], lr: 0.133469, loss: 1.6253
2022-08-06 09:31:57 - train: epoch 0043, iter [01000, 05004], lr: 0.133407, loss: 1.9831
2022-08-06 09:33:31 - train: epoch 0043, iter [01100, 05004], lr: 0.133345, loss: 1.8536
2022-08-06 09:35:05 - train: epoch 0043, iter [01200, 05004], lr: 0.133283, loss: 1.5057
2022-08-06 09:36:39 - train: epoch 0043, iter [01300, 05004], lr: 0.133220, loss: 1.7333
2022-08-06 09:38:13 - train: epoch 0043, iter [01400, 05004], lr: 0.133158, loss: 1.6693
2022-08-06 09:39:47 - train: epoch 0043, iter [01500, 05004], lr: 0.133096, loss: 1.5768
2022-08-06 09:41:21 - train: epoch 0043, iter [01600, 05004], lr: 0.133033, loss: 1.5137
2022-08-06 09:42:55 - train: epoch 0043, iter [01700, 05004], lr: 0.132971, loss: 1.7665
2022-08-06 09:44:29 - train: epoch 0043, iter [01800, 05004], lr: 0.132908, loss: 1.9768
2022-08-06 09:46:03 - train: epoch 0043, iter [01900, 05004], lr: 0.132846, loss: 1.7734
2022-08-06 09:47:37 - train: epoch 0043, iter [02000, 05004], lr: 0.132784, loss: 1.6541
2022-08-06 09:49:11 - train: epoch 0043, iter [02100, 05004], lr: 0.132721, loss: 1.5791
2022-08-06 09:50:45 - train: epoch 0043, iter [02200, 05004], lr: 0.132659, loss: 1.8308
2022-08-06 09:52:19 - train: epoch 0043, iter [02300, 05004], lr: 0.132596, loss: 1.8649
2022-08-06 09:53:53 - train: epoch 0043, iter [02400, 05004], lr: 0.132534, loss: 1.5196
2022-08-06 09:55:27 - train: epoch 0043, iter [02500, 05004], lr: 0.132471, loss: 1.8075
2022-08-06 09:57:01 - train: epoch 0043, iter [02600, 05004], lr: 0.132409, loss: 1.6318
2022-08-06 09:58:35 - train: epoch 0043, iter [02700, 05004], lr: 0.132346, loss: 1.8956
2022-08-06 10:00:09 - train: epoch 0043, iter [02800, 05004], lr: 0.132284, loss: 1.4736
2022-08-06 10:01:43 - train: epoch 0043, iter [02900, 05004], lr: 0.132221, loss: 1.5302
2022-08-06 10:03:17 - train: epoch 0043, iter [03000, 05004], lr: 0.132158, loss: 2.0323
2022-08-06 10:04:51 - train: epoch 0043, iter [03100, 05004], lr: 0.132096, loss: 1.8344
2022-08-06 10:06:25 - train: epoch 0043, iter [03200, 05004], lr: 0.132033, loss: 1.9813
2022-08-06 10:07:59 - train: epoch 0043, iter [03300, 05004], lr: 0.131971, loss: 1.7841
2022-08-06 10:09:33 - train: epoch 0043, iter [03400, 05004], lr: 0.131908, loss: 1.6672
2022-08-06 10:11:07 - train: epoch 0043, iter [03500, 05004], lr: 0.131845, loss: 1.7638
2022-08-06 10:12:41 - train: epoch 0043, iter [03600, 05004], lr: 0.131783, loss: 1.8206
2022-08-06 10:14:15 - train: epoch 0043, iter [03700, 05004], lr: 0.131720, loss: 1.5995
2022-08-06 10:15:49 - train: epoch 0043, iter [03800, 05004], lr: 0.131657, loss: 1.8772
2022-08-06 10:17:23 - train: epoch 0043, iter [03900, 05004], lr: 0.131595, loss: 1.8076
2022-08-06 10:18:57 - train: epoch 0043, iter [04000, 05004], lr: 0.131532, loss: 1.9367
2022-08-06 10:20:31 - train: epoch 0043, iter [04100, 05004], lr: 0.131469, loss: 1.8797
2022-08-06 10:22:05 - train: epoch 0043, iter [04200, 05004], lr: 0.131407, loss: 1.6970
2022-08-06 10:23:39 - train: epoch 0043, iter [04300, 05004], lr: 0.131344, loss: 1.6605
2022-08-06 10:25:13 - train: epoch 0043, iter [04400, 05004], lr: 0.131281, loss: 1.8015
2022-08-06 10:26:47 - train: epoch 0043, iter [04500, 05004], lr: 0.131218, loss: 1.6170
2022-08-06 10:28:21 - train: epoch 0043, iter [04600, 05004], lr: 0.131156, loss: 1.6665
2022-08-06 10:29:55 - train: epoch 0043, iter [04700, 05004], lr: 0.131093, loss: 1.7299
2022-08-06 10:31:29 - train: epoch 0043, iter [04800, 05004], lr: 0.131030, loss: 1.5374
2022-08-06 10:33:03 - train: epoch 0043, iter [04900, 05004], lr: 0.130967, loss: 1.6395
2022-08-06 10:34:37 - train: epoch 0043, iter [05000, 05004], lr: 0.130904, loss: 1.7354
2022-08-06 10:34:41 - train: epoch 043, train_loss: 1.6959
2022-08-06 10:36:46 - eval: epoch: 043, acc1: 64.958%, acc5: 87.020%, test_loss: 1.4405, per_image_load_time: 3.102ms, per_image_inference_time: 1.178ms
2022-08-06 10:36:47 - until epoch: 043, best_acc1: 64.958%
2022-08-06 10:36:47 - epoch 044 lr: 0.130901
2022-08-06 10:38:29 - train: epoch 0044, iter [00100, 05004], lr: 0.130839, loss: 1.7628
2022-08-06 10:40:03 - train: epoch 0044, iter [00200, 05004], lr: 0.130776, loss: 1.8360
2022-08-06 10:41:37 - train: epoch 0044, iter [00300, 05004], lr: 0.130713, loss: 1.5548
2022-08-06 10:43:11 - train: epoch 0044, iter [00400, 05004], lr: 0.130650, loss: 1.3226
2022-08-06 10:44:45 - train: epoch 0044, iter [00500, 05004], lr: 0.130587, loss: 1.7072
2022-08-06 10:46:19 - train: epoch 0044, iter [00600, 05004], lr: 0.130524, loss: 1.4718
2022-08-06 10:47:53 - train: epoch 0044, iter [00700, 05004], lr: 0.130461, loss: 1.5234
2022-08-06 10:49:27 - train: epoch 0044, iter [00800, 05004], lr: 0.130398, loss: 1.5561
2022-08-06 10:51:01 - train: epoch 0044, iter [00900, 05004], lr: 0.130335, loss: 1.6572
2022-08-06 10:52:36 - train: epoch 0044, iter [01000, 05004], lr: 0.130273, loss: 1.7899
2022-08-06 10:54:10 - train: epoch 0044, iter [01100, 05004], lr: 0.130210, loss: 1.5152
2022-08-06 10:55:44 - train: epoch 0044, iter [01200, 05004], lr: 0.130147, loss: 1.6444
2022-08-06 10:57:18 - train: epoch 0044, iter [01300, 05004], lr: 0.130084, loss: 1.9812
2022-08-06 10:58:52 - train: epoch 0044, iter [01400, 05004], lr: 0.130020, loss: 1.6634
2022-08-06 11:00:26 - train: epoch 0044, iter [01500, 05004], lr: 0.129957, loss: 1.6004
2022-08-06 11:02:00 - train: epoch 0044, iter [01600, 05004], lr: 0.129894, loss: 1.6185
2022-08-06 11:03:34 - train: epoch 0044, iter [01700, 05004], lr: 0.129831, loss: 1.6685
2022-08-06 11:05:08 - train: epoch 0044, iter [01800, 05004], lr: 0.129768, loss: 1.7115
2022-08-06 11:06:42 - train: epoch 0044, iter [01900, 05004], lr: 0.129705, loss: 1.6571
2022-08-06 11:08:16 - train: epoch 0044, iter [02000, 05004], lr: 0.129642, loss: 1.6756
2022-08-06 11:09:50 - train: epoch 0044, iter [02100, 05004], lr: 0.129579, loss: 1.8105
2022-08-06 11:11:24 - train: epoch 0044, iter [02200, 05004], lr: 0.129516, loss: 1.5529
2022-08-06 11:12:59 - train: epoch 0044, iter [02300, 05004], lr: 0.129453, loss: 1.8655
2022-08-06 11:14:33 - train: epoch 0044, iter [02400, 05004], lr: 0.129389, loss: 1.6121
2022-08-06 11:16:07 - train: epoch 0044, iter [02500, 05004], lr: 0.129326, loss: 1.6281
2022-08-06 11:17:41 - train: epoch 0044, iter [02600, 05004], lr: 0.129263, loss: 1.9515
2022-08-06 11:19:15 - train: epoch 0044, iter [02700, 05004], lr: 0.129200, loss: 1.7522
2022-08-06 11:20:49 - train: epoch 0044, iter [02800, 05004], lr: 0.129137, loss: 1.5562
2022-08-06 11:22:23 - train: epoch 0044, iter [02900, 05004], lr: 0.129073, loss: 1.6051
2022-08-06 11:23:57 - train: epoch 0044, iter [03000, 05004], lr: 0.129010, loss: 1.3414
2022-08-06 11:25:31 - train: epoch 0044, iter [03100, 05004], lr: 0.128947, loss: 1.6823
2022-08-06 11:27:05 - train: epoch 0044, iter [03200, 05004], lr: 0.128884, loss: 1.5350
2022-08-06 11:28:39 - train: epoch 0044, iter [03300, 05004], lr: 0.128820, loss: 1.5858
2022-08-06 11:30:14 - train: epoch 0044, iter [03400, 05004], lr: 0.128757, loss: 1.9272
2022-08-06 11:31:48 - train: epoch 0044, iter [03500, 05004], lr: 0.128694, loss: 1.5652
2022-08-06 11:33:22 - train: epoch 0044, iter [03600, 05004], lr: 0.128631, loss: 1.6198
2022-08-06 11:34:56 - train: epoch 0044, iter [03700, 05004], lr: 0.128567, loss: 1.5883
2022-08-06 11:36:30 - train: epoch 0044, iter [03800, 05004], lr: 0.128504, loss: 1.5786
2022-08-06 11:38:04 - train: epoch 0044, iter [03900, 05004], lr: 0.128441, loss: 1.7370
2022-08-06 11:39:38 - train: epoch 0044, iter [04000, 05004], lr: 0.128377, loss: 1.6592
2022-08-06 11:41:12 - train: epoch 0044, iter [04100, 05004], lr: 0.128314, loss: 1.5053
2022-08-06 11:42:46 - train: epoch 0044, iter [04200, 05004], lr: 0.128250, loss: 1.5605
2022-08-06 11:44:20 - train: epoch 0044, iter [04300, 05004], lr: 0.128187, loss: 1.7388
2022-08-06 11:45:55 - train: epoch 0044, iter [04400, 05004], lr: 0.128124, loss: 1.7564
2022-08-06 11:47:28 - train: epoch 0044, iter [04500, 05004], lr: 0.128060, loss: 1.9028
2022-08-06 11:49:02 - train: epoch 0044, iter [04600, 05004], lr: 0.127997, loss: 1.6838
2022-08-06 11:50:37 - train: epoch 0044, iter [04700, 05004], lr: 0.127933, loss: 1.8756
2022-08-06 11:52:11 - train: epoch 0044, iter [04800, 05004], lr: 0.127870, loss: 1.5279
2022-08-06 11:53:45 - train: epoch 0044, iter [04900, 05004], lr: 0.127806, loss: 1.4415
2022-08-06 11:55:19 - train: epoch 0044, iter [05000, 05004], lr: 0.127743, loss: 1.7924
2022-08-06 11:55:23 - train: epoch 044, train_loss: 1.6841
2022-08-06 11:57:31 - eval: epoch: 044, acc1: 65.276%, acc5: 87.258%, test_loss: 1.4213, per_image_load_time: 1.916ms, per_image_inference_time: 1.224ms
2022-08-06 11:57:31 - until epoch: 044, best_acc1: 65.276%
2022-08-06 11:57:31 - epoch 045 lr: 0.127740
2022-08-06 11:59:14 - train: epoch 0045, iter [00100, 05004], lr: 0.127677, loss: 1.6691
2022-08-06 12:00:48 - train: epoch 0045, iter [00200, 05004], lr: 0.127613, loss: 1.4830
2022-08-06 12:02:22 - train: epoch 0045, iter [00300, 05004], lr: 0.127550, loss: 1.7065
2022-08-06 12:03:56 - train: epoch 0045, iter [00400, 05004], lr: 0.127486, loss: 1.6202
2022-08-06 12:05:30 - train: epoch 0045, iter [00500, 05004], lr: 0.127423, loss: 1.7480
2022-08-06 12:07:04 - train: epoch 0045, iter [00600, 05004], lr: 0.127359, loss: 1.8733
2022-08-06 12:08:38 - train: epoch 0045, iter [00700, 05004], lr: 0.127296, loss: 1.4010
2022-08-06 12:10:12 - train: epoch 0045, iter [00800, 05004], lr: 0.127232, loss: 1.4168
2022-08-06 12:11:46 - train: epoch 0045, iter [00900, 05004], lr: 0.127168, loss: 1.4756
2022-08-06 12:13:20 - train: epoch 0045, iter [01000, 05004], lr: 0.127105, loss: 1.4679
2022-08-06 12:14:54 - train: epoch 0045, iter [01100, 05004], lr: 0.127041, loss: 1.7511
2022-08-06 12:16:28 - train: epoch 0045, iter [01200, 05004], lr: 0.126978, loss: 1.5953
2022-08-06 12:18:02 - train: epoch 0045, iter [01300, 05004], lr: 0.126914, loss: 1.7299
2022-08-06 12:19:36 - train: epoch 0045, iter [01400, 05004], lr: 0.126850, loss: 1.7703
2022-08-06 12:21:10 - train: epoch 0045, iter [01500, 05004], lr: 0.126787, loss: 1.6309
2022-08-06 12:22:44 - train: epoch 0045, iter [01600, 05004], lr: 0.126723, loss: 1.4706
2022-08-06 12:24:18 - train: epoch 0045, iter [01700, 05004], lr: 0.126659, loss: 1.5711
2022-08-06 12:25:52 - train: epoch 0045, iter [01800, 05004], lr: 0.126595, loss: 1.5842
2022-08-06 12:27:26 - train: epoch 0045, iter [01900, 05004], lr: 0.126532, loss: 1.6011
2022-08-06 12:29:00 - train: epoch 0045, iter [02000, 05004], lr: 0.126468, loss: 1.7147
2022-08-06 12:30:34 - train: epoch 0045, iter [02100, 05004], lr: 0.126404, loss: 1.8152
2022-08-06 12:32:08 - train: epoch 0045, iter [02200, 05004], lr: 0.126341, loss: 1.8089
2022-08-06 12:33:42 - train: epoch 0045, iter [02300, 05004], lr: 0.126277, loss: 1.6620
2022-08-06 12:35:16 - train: epoch 0045, iter [02400, 05004], lr: 0.126213, loss: 1.8269
2022-08-06 12:36:50 - train: epoch 0045, iter [02500, 05004], lr: 0.126149, loss: 1.8852
2022-08-06 12:38:24 - train: epoch 0045, iter [02600, 05004], lr: 0.126085, loss: 1.8654
2022-08-06 12:39:58 - train: epoch 0045, iter [02700, 05004], lr: 0.126022, loss: 1.4386
2022-08-06 12:41:32 - train: epoch 0045, iter [02800, 05004], lr: 0.125958, loss: 1.6660
2022-08-06 12:43:06 - train: epoch 0045, iter [02900, 05004], lr: 0.125894, loss: 1.8775
2022-08-06 12:44:40 - train: epoch 0045, iter [03000, 05004], lr: 0.125830, loss: 1.6485
2022-08-06 12:46:14 - train: epoch 0045, iter [03100, 05004], lr: 0.125766, loss: 1.6416
2022-08-06 12:47:48 - train: epoch 0045, iter [03200, 05004], lr: 0.125702, loss: 1.6797
2022-08-06 12:49:22 - train: epoch 0045, iter [03300, 05004], lr: 0.125639, loss: 1.6234
2022-08-06 12:50:56 - train: epoch 0045, iter [03400, 05004], lr: 0.125575, loss: 1.5194
2022-08-06 12:52:30 - train: epoch 0045, iter [03500, 05004], lr: 0.125511, loss: 1.8812
2022-08-06 12:54:04 - train: epoch 0045, iter [03600, 05004], lr: 0.125447, loss: 1.5849
2022-08-06 12:55:38 - train: epoch 0045, iter [03700, 05004], lr: 0.125383, loss: 1.6308
2022-08-06 12:57:12 - train: epoch 0045, iter [03800, 05004], lr: 0.125319, loss: 1.6718
2022-08-06 12:58:46 - train: epoch 0045, iter [03900, 05004], lr: 0.125255, loss: 1.9012
2022-08-06 13:00:20 - train: epoch 0045, iter [04000, 05004], lr: 0.125191, loss: 1.7272
2022-08-06 13:01:54 - train: epoch 0045, iter [04100, 05004], lr: 0.125127, loss: 1.7819
2022-08-06 13:03:28 - train: epoch 0045, iter [04200, 05004], lr: 0.125063, loss: 1.8280
2022-08-06 13:05:02 - train: epoch 0045, iter [04300, 05004], lr: 0.124999, loss: 1.7279
2022-08-06 13:06:36 - train: epoch 0045, iter [04400, 05004], lr: 0.124935, loss: 1.9084
2022-08-06 13:08:10 - train: epoch 0045, iter [04500, 05004], lr: 0.124871, loss: 1.5650
2022-08-06 13:09:45 - train: epoch 0045, iter [04600, 05004], lr: 0.124807, loss: 1.9472
2022-08-06 13:11:19 - train: epoch 0045, iter [04700, 05004], lr: 0.124743, loss: 1.6473
2022-08-06 13:12:53 - train: epoch 0045, iter [04800, 05004], lr: 0.124679, loss: 1.6265
2022-08-06 13:14:27 - train: epoch 0045, iter [04900, 05004], lr: 0.124615, loss: 1.8363
2022-08-06 13:16:01 - train: epoch 0045, iter [05000, 05004], lr: 0.124551, loss: 1.6151
2022-08-06 13:16:05 - train: epoch 045, train_loss: 1.6724
2022-08-06 13:18:12 - eval: epoch: 045, acc1: 64.762%, acc5: 86.948%, test_loss: 1.4353, per_image_load_time: 2.453ms, per_image_inference_time: 1.223ms
2022-08-06 13:18:12 - until epoch: 045, best_acc1: 65.276%
2022-08-06 13:18:12 - epoch 046 lr: 0.124548
2022-08-06 13:19:56 - train: epoch 0046, iter [00100, 05004], lr: 0.124484, loss: 1.7214
2022-08-06 13:21:30 - train: epoch 0046, iter [00200, 05004], lr: 0.124420, loss: 1.5791
2022-08-06 13:23:04 - train: epoch 0046, iter [00300, 05004], lr: 0.124356, loss: 1.6533
2022-08-06 13:24:38 - train: epoch 0046, iter [00400, 05004], lr: 0.124292, loss: 1.5515
2022-08-06 13:26:12 - train: epoch 0046, iter [00500, 05004], lr: 0.124228, loss: 1.4801
2022-08-06 13:27:46 - train: epoch 0046, iter [00600, 05004], lr: 0.124164, loss: 1.7653
2022-08-06 13:29:20 - train: epoch 0046, iter [00700, 05004], lr: 0.124100, loss: 1.5143
2022-08-06 13:30:54 - train: epoch 0046, iter [00800, 05004], lr: 0.124036, loss: 1.9701
2022-08-06 13:32:28 - train: epoch 0046, iter [00900, 05004], lr: 0.123972, loss: 1.5625
2022-08-06 13:34:02 - train: epoch 0046, iter [01000, 05004], lr: 0.123907, loss: 1.4444
2022-08-06 13:35:36 - train: epoch 0046, iter [01100, 05004], lr: 0.123843, loss: 1.5746
2022-08-06 13:37:10 - train: epoch 0046, iter [01200, 05004], lr: 0.123779, loss: 1.5227
2022-08-06 13:38:44 - train: epoch 0046, iter [01300, 05004], lr: 0.123715, loss: 1.5351
2022-08-06 13:40:17 - train: epoch 0046, iter [01400, 05004], lr: 0.123651, loss: 1.9391
2022-08-06 13:41:51 - train: epoch 0046, iter [01500, 05004], lr: 0.123586, loss: 1.6067
2022-08-06 13:43:25 - train: epoch 0046, iter [01600, 05004], lr: 0.123522, loss: 1.7710
2022-08-06 13:44:59 - train: epoch 0046, iter [01700, 05004], lr: 0.123458, loss: 1.5321
2022-08-06 13:46:33 - train: epoch 0046, iter [01800, 05004], lr: 0.123394, loss: 1.5969
2022-08-06 13:48:07 - train: epoch 0046, iter [01900, 05004], lr: 0.123329, loss: 1.6456
2022-08-06 13:49:41 - train: epoch 0046, iter [02000, 05004], lr: 0.123265, loss: 1.6547
2022-08-06 13:51:15 - train: epoch 0046, iter [02100, 05004], lr: 0.123201, loss: 1.9236
2022-08-06 13:52:49 - train: epoch 0046, iter [02200, 05004], lr: 0.123137, loss: 1.4413
2022-08-06 13:54:22 - train: epoch 0046, iter [02300, 05004], lr: 0.123072, loss: 1.7567
2022-08-06 13:55:56 - train: epoch 0046, iter [02400, 05004], lr: 0.123008, loss: 1.6134
2022-08-06 13:57:30 - train: epoch 0046, iter [02500, 05004], lr: 0.122944, loss: 1.8132
2022-08-06 13:59:04 - train: epoch 0046, iter [02600, 05004], lr: 0.122879, loss: 1.9816
2022-08-06 14:00:38 - train: epoch 0046, iter [02700, 05004], lr: 0.122815, loss: 1.6764
2022-08-06 14:02:12 - train: epoch 0046, iter [02800, 05004], lr: 0.122751, loss: 1.6095
2022-08-06 14:03:46 - train: epoch 0046, iter [02900, 05004], lr: 0.122686, loss: 1.8555
2022-08-06 14:05:20 - train: epoch 0046, iter [03000, 05004], lr: 0.122622, loss: 1.4623
2022-08-06 14:06:54 - train: epoch 0046, iter [03100, 05004], lr: 0.122558, loss: 1.5434
2022-08-06 14:08:28 - train: epoch 0046, iter [03200, 05004], lr: 0.122493, loss: 1.7694
2022-08-06 14:10:02 - train: epoch 0046, iter [03300, 05004], lr: 0.122429, loss: 1.5810
2022-08-06 14:11:36 - train: epoch 0046, iter [03400, 05004], lr: 0.122364, loss: 1.7425
2022-08-06 14:13:10 - train: epoch 0046, iter [03500, 05004], lr: 0.122300, loss: 1.8478
2022-08-06 14:14:44 - train: epoch 0046, iter [03600, 05004], lr: 0.122236, loss: 1.8147
2022-08-06 14:16:18 - train: epoch 0046, iter [03700, 05004], lr: 0.122171, loss: 1.4541
2022-08-06 14:17:52 - train: epoch 0046, iter [03800, 05004], lr: 0.122107, loss: 1.6519
2022-08-06 14:19:26 - train: epoch 0046, iter [03900, 05004], lr: 0.122042, loss: 1.6540
2022-08-06 14:21:00 - train: epoch 0046, iter [04000, 05004], lr: 0.121978, loss: 1.6425
2022-08-06 14:22:34 - train: epoch 0046, iter [04100, 05004], lr: 0.121913, loss: 1.8331
2022-08-06 14:24:08 - train: epoch 0046, iter [04200, 05004], lr: 0.121849, loss: 1.4644
2022-08-06 14:25:42 - train: epoch 0046, iter [04300, 05004], lr: 0.121784, loss: 1.7642
2022-08-06 14:27:16 - train: epoch 0046, iter [04400, 05004], lr: 0.121720, loss: 1.7758
2022-08-06 14:28:50 - train: epoch 0046, iter [04500, 05004], lr: 0.121655, loss: 1.4154
2022-08-06 14:30:24 - train: epoch 0046, iter [04600, 05004], lr: 0.121591, loss: 1.6137
2022-08-06 14:31:58 - train: epoch 0046, iter [04700, 05004], lr: 0.121526, loss: 1.5456
2022-08-06 14:33:32 - train: epoch 0046, iter [04800, 05004], lr: 0.121462, loss: 1.4967
2022-08-06 14:35:06 - train: epoch 0046, iter [04900, 05004], lr: 0.121397, loss: 1.8870
2022-08-06 14:36:40 - train: epoch 0046, iter [05000, 05004], lr: 0.121333, loss: 1.7554
2022-08-06 14:36:44 - train: epoch 046, train_loss: 1.6624
2022-08-06 14:38:48 - eval: epoch: 046, acc1: 65.204%, acc5: 87.240%, test_loss: 1.4179, per_image_load_time: 2.018ms, per_image_inference_time: 1.238ms
2022-08-06 14:38:48 - until epoch: 046, best_acc1: 65.276%
2022-08-06 14:38:48 - epoch 047 lr: 0.121329
2022-08-06 14:40:32 - train: epoch 0047, iter [00100, 05004], lr: 0.121265, loss: 1.6406
2022-08-06 14:42:06 - train: epoch 0047, iter [00200, 05004], lr: 0.121201, loss: 1.7052
2022-08-06 14:43:40 - train: epoch 0047, iter [00300, 05004], lr: 0.121136, loss: 1.5492
2022-08-06 14:45:14 - train: epoch 0047, iter [00400, 05004], lr: 0.121072, loss: 1.5597
2022-08-06 14:46:48 - train: epoch 0047, iter [00500, 05004], lr: 0.121007, loss: 1.6211
2022-08-06 14:48:22 - train: epoch 0047, iter [00600, 05004], lr: 0.120942, loss: 1.7073
2022-08-06 14:49:56 - train: epoch 0047, iter [00700, 05004], lr: 0.120878, loss: 1.7016
2022-08-06 14:51:30 - train: epoch 0047, iter [00800, 05004], lr: 0.120813, loss: 1.6211
2022-08-06 14:53:04 - train: epoch 0047, iter [00900, 05004], lr: 0.120749, loss: 1.5840
2022-08-06 14:54:39 - train: epoch 0047, iter [01000, 05004], lr: 0.120684, loss: 1.6206
2022-08-06 14:56:13 - train: epoch 0047, iter [01100, 05004], lr: 0.120619, loss: 1.8868
2022-08-06 14:57:47 - train: epoch 0047, iter [01200, 05004], lr: 0.120555, loss: 1.6017
2022-08-06 14:59:21 - train: epoch 0047, iter [01300, 05004], lr: 0.120490, loss: 1.7073
2022-08-06 15:00:55 - train: epoch 0047, iter [01400, 05004], lr: 0.120425, loss: 1.7264
2022-08-06 15:02:29 - train: epoch 0047, iter [01500, 05004], lr: 0.120360, loss: 1.7207
2022-08-06 15:04:03 - train: epoch 0047, iter [01600, 05004], lr: 0.120296, loss: 1.5552
2022-08-06 15:05:37 - train: epoch 0047, iter [01700, 05004], lr: 0.120231, loss: 1.5185
2022-08-06 15:07:11 - train: epoch 0047, iter [01800, 05004], lr: 0.120166, loss: 1.7843
2022-08-06 15:08:45 - train: epoch 0047, iter [01900, 05004], lr: 0.120102, loss: 1.6249
2022-08-06 15:10:19 - train: epoch 0047, iter [02000, 05004], lr: 0.120037, loss: 1.7438
2022-08-06 15:11:53 - train: epoch 0047, iter [02100, 05004], lr: 0.119972, loss: 1.8399
2022-08-06 15:13:27 - train: epoch 0047, iter [02200, 05004], lr: 0.119907, loss: 1.7200
2022-08-06 15:15:01 - train: epoch 0047, iter [02300, 05004], lr: 0.119843, loss: 1.7103
2022-08-06 15:16:35 - train: epoch 0047, iter [02400, 05004], lr: 0.119778, loss: 1.7099
2022-08-06 15:18:09 - train: epoch 0047, iter [02500, 05004], lr: 0.119713, loss: 1.6483
2022-08-06 15:19:44 - train: epoch 0047, iter [02600, 05004], lr: 0.119648, loss: 1.9705
2022-08-06 15:21:17 - train: epoch 0047, iter [02700, 05004], lr: 0.119583, loss: 1.4946
2022-08-06 15:22:52 - train: epoch 0047, iter [02800, 05004], lr: 0.119519, loss: 1.7824
2022-08-06 15:24:26 - train: epoch 0047, iter [02900, 05004], lr: 0.119454, loss: 1.4404
2022-08-06 15:26:00 - train: epoch 0047, iter [03000, 05004], lr: 0.119389, loss: 1.7986
2022-08-06 15:27:34 - train: epoch 0047, iter [03100, 05004], lr: 0.119324, loss: 1.7130
2022-08-06 15:29:08 - train: epoch 0047, iter [03200, 05004], lr: 0.119259, loss: 1.7081
2022-08-06 15:30:42 - train: epoch 0047, iter [03300, 05004], lr: 0.119194, loss: 1.4187
2022-08-06 15:32:16 - train: epoch 0047, iter [03400, 05004], lr: 0.119130, loss: 1.5471
2022-08-06 15:33:50 - train: epoch 0047, iter [03500, 05004], lr: 0.119065, loss: 1.8235
2022-08-06 15:35:24 - train: epoch 0047, iter [03600, 05004], lr: 0.119000, loss: 1.5785
2022-08-06 15:36:58 - train: epoch 0047, iter [03700, 05004], lr: 0.118935, loss: 1.6212
2022-08-06 15:38:32 - train: epoch 0047, iter [03800, 05004], lr: 0.118870, loss: 1.6804
2022-08-06 15:40:06 - train: epoch 0047, iter [03900, 05004], lr: 0.118805, loss: 1.7815
2022-08-06 15:41:40 - train: epoch 0047, iter [04000, 05004], lr: 0.118740, loss: 1.4778
2022-08-06 15:43:14 - train: epoch 0047, iter [04100, 05004], lr: 0.118675, loss: 1.7846
2022-08-06 15:44:48 - train: epoch 0047, iter [04200, 05004], lr: 0.118610, loss: 1.6847
2022-08-06 15:46:22 - train: epoch 0047, iter [04300, 05004], lr: 0.118545, loss: 1.4014
2022-08-06 15:47:56 - train: epoch 0047, iter [04400, 05004], lr: 0.118480, loss: 1.5768
2022-08-06 15:49:30 - train: epoch 0047, iter [04500, 05004], lr: 0.118416, loss: 1.4555
2022-08-06 15:51:04 - train: epoch 0047, iter [04600, 05004], lr: 0.118351, loss: 1.6718
2022-08-06 15:52:38 - train: epoch 0047, iter [04700, 05004], lr: 0.118286, loss: 1.7990
2022-08-06 15:54:12 - train: epoch 0047, iter [04800, 05004], lr: 0.118221, loss: 1.4535
2022-08-06 15:55:47 - train: epoch 0047, iter [04900, 05004], lr: 0.118156, loss: 1.6924
2022-08-06 15:57:20 - train: epoch 0047, iter [05000, 05004], lr: 0.118091, loss: 1.7701
2022-08-06 15:57:25 - train: epoch 047, train_loss: 1.6485
2022-08-06 15:59:29 - eval: epoch: 047, acc1: 65.622%, acc5: 87.282%, test_loss: 1.4136, per_image_load_time: 2.336ms, per_image_inference_time: 1.226ms
2022-08-06 15:59:30 - until epoch: 047, best_acc1: 65.622%
2022-08-06 15:59:30 - epoch 048 lr: 0.118087
2022-08-06 16:01:13 - train: epoch 0048, iter [00100, 05004], lr: 0.118023, loss: 1.7920
2022-08-06 16:02:47 - train: epoch 0048, iter [00200, 05004], lr: 0.117958, loss: 1.9698
2022-08-06 16:04:22 - train: epoch 0048, iter [00300, 05004], lr: 0.117893, loss: 1.7584
2022-08-06 16:05:55 - train: epoch 0048, iter [00400, 05004], lr: 0.117828, loss: 1.7258
2022-08-06 16:07:29 - train: epoch 0048, iter [00500, 05004], lr: 0.117763, loss: 1.5561
2022-08-06 16:09:04 - train: epoch 0048, iter [00600, 05004], lr: 0.117698, loss: 1.6732
2022-08-06 16:10:38 - train: epoch 0048, iter [00700, 05004], lr: 0.117633, loss: 1.6544
2022-08-06 16:12:11 - train: epoch 0048, iter [00800, 05004], lr: 0.117568, loss: 1.6588
2022-08-06 16:13:45 - train: epoch 0048, iter [00900, 05004], lr: 0.117503, loss: 1.7431
2022-08-06 16:15:19 - train: epoch 0048, iter [01000, 05004], lr: 0.117438, loss: 1.7780
2022-08-06 16:16:53 - train: epoch 0048, iter [01100, 05004], lr: 0.117373, loss: 1.7444
2022-08-06 16:18:27 - train: epoch 0048, iter [01200, 05004], lr: 0.117308, loss: 1.6526
2022-08-06 16:20:00 - train: epoch 0048, iter [01300, 05004], lr: 0.117242, loss: 1.3293
2022-08-06 16:21:34 - train: epoch 0048, iter [01400, 05004], lr: 0.117177, loss: 1.6854
2022-08-06 16:23:08 - train: epoch 0048, iter [01500, 05004], lr: 0.117112, loss: 1.7037
2022-08-06 16:24:42 - train: epoch 0048, iter [01600, 05004], lr: 0.117047, loss: 1.5274
2022-08-06 16:26:16 - train: epoch 0048, iter [01700, 05004], lr: 0.116982, loss: 1.6285
2022-08-06 16:27:50 - train: epoch 0048, iter [01800, 05004], lr: 0.116917, loss: 1.4562
2022-08-06 16:29:24 - train: epoch 0048, iter [01900, 05004], lr: 0.116852, loss: 1.7388
2022-08-06 16:30:57 - train: epoch 0048, iter [02000, 05004], lr: 0.116787, loss: 1.7670
2022-08-06 16:32:31 - train: epoch 0048, iter [02100, 05004], lr: 0.116721, loss: 1.6372
2022-08-06 16:34:05 - train: epoch 0048, iter [02200, 05004], lr: 0.116656, loss: 1.7653
2022-08-06 16:35:39 - train: epoch 0048, iter [02300, 05004], lr: 0.116591, loss: 1.4403
2022-08-06 16:37:13 - train: epoch 0048, iter [02400, 05004], lr: 0.116526, loss: 1.7744
2022-08-06 16:38:47 - train: epoch 0048, iter [02500, 05004], lr: 0.116461, loss: 1.7832
2022-08-06 16:40:21 - train: epoch 0048, iter [02600, 05004], lr: 0.116396, loss: 1.6377
2022-08-06 16:41:55 - train: epoch 0048, iter [02700, 05004], lr: 0.116330, loss: 1.8192
2022-08-06 16:43:29 - train: epoch 0048, iter [02800, 05004], lr: 0.116265, loss: 1.6652
2022-08-06 16:45:02 - train: epoch 0048, iter [02900, 05004], lr: 0.116200, loss: 1.7119
2022-08-06 16:46:36 - train: epoch 0048, iter [03000, 05004], lr: 0.116135, loss: 1.6811
2022-08-06 16:48:10 - train: epoch 0048, iter [03100, 05004], lr: 0.116070, loss: 1.7108
2022-08-06 16:49:44 - train: epoch 0048, iter [03200, 05004], lr: 0.116004, loss: 1.4671
2022-08-06 16:51:18 - train: epoch 0048, iter [03300, 05004], lr: 0.115939, loss: 1.8073
2022-08-06 16:52:52 - train: epoch 0048, iter [03400, 05004], lr: 0.115874, loss: 1.7139
2022-08-06 16:54:26 - train: epoch 0048, iter [03500, 05004], lr: 0.115809, loss: 1.8489
2022-08-06 16:55:59 - train: epoch 0048, iter [03600, 05004], lr: 0.115743, loss: 1.7687
2022-08-06 16:57:33 - train: epoch 0048, iter [03700, 05004], lr: 0.115678, loss: 1.9156
2022-08-06 16:59:07 - train: epoch 0048, iter [03800, 05004], lr: 0.115613, loss: 1.4319
2022-08-06 17:00:41 - train: epoch 0048, iter [03900, 05004], lr: 0.115547, loss: 1.7535
2022-08-06 17:02:15 - train: epoch 0048, iter [04000, 05004], lr: 0.115482, loss: 1.3785
2022-08-06 17:03:49 - train: epoch 0048, iter [04100, 05004], lr: 0.115417, loss: 1.9245
2022-08-06 17:05:22 - train: epoch 0048, iter [04200, 05004], lr: 0.115352, loss: 1.6003
2022-08-06 17:06:56 - train: epoch 0048, iter [04300, 05004], lr: 0.115286, loss: 1.6860
2022-08-06 17:08:30 - train: epoch 0048, iter [04400, 05004], lr: 0.115221, loss: 1.8555
2022-08-06 17:10:04 - train: epoch 0048, iter [04500, 05004], lr: 0.115156, loss: 1.9981
2022-08-06 17:11:38 - train: epoch 0048, iter [04600, 05004], lr: 0.115090, loss: 1.6860
2022-08-06 17:13:12 - train: epoch 0048, iter [04700, 05004], lr: 0.115025, loss: 1.6832
2022-08-06 17:14:45 - train: epoch 0048, iter [04800, 05004], lr: 0.114960, loss: 1.9100
2022-08-06 17:16:19 - train: epoch 0048, iter [04900, 05004], lr: 0.114894, loss: 1.7658
2022-08-06 17:17:53 - train: epoch 0048, iter [05000, 05004], lr: 0.114829, loss: 1.5865
2022-08-06 17:17:58 - train: epoch 048, train_loss: 1.6396
2022-08-06 17:20:03 - eval: epoch: 048, acc1: 65.758%, acc5: 87.546%, test_loss: 1.3939, per_image_load_time: 3.626ms, per_image_inference_time: 1.241ms
2022-08-06 17:20:04 - until epoch: 048, best_acc1: 65.758%
2022-08-06 17:20:04 - epoch 049 lr: 0.114826
2022-08-06 17:21:48 - train: epoch 0049, iter [00100, 05004], lr: 0.114761, loss: 1.6405
2022-08-06 17:23:22 - train: epoch 0049, iter [00200, 05004], lr: 0.114696, loss: 1.6096
2022-08-06 17:24:56 - train: epoch 0049, iter [00300, 05004], lr: 0.114630, loss: 1.6421
2022-08-06 17:26:30 - train: epoch 0049, iter [00400, 05004], lr: 0.114565, loss: 1.6367
2022-08-06 17:28:03 - train: epoch 0049, iter [00500, 05004], lr: 0.114500, loss: 1.6657
2022-08-06 17:29:37 - train: epoch 0049, iter [00600, 05004], lr: 0.114434, loss: 1.5368
2022-08-06 17:31:11 - train: epoch 0049, iter [00700, 05004], lr: 0.114369, loss: 1.5543
2022-08-06 17:32:45 - train: epoch 0049, iter [00800, 05004], lr: 0.114303, loss: 1.7995
2022-08-06 17:34:19 - train: epoch 0049, iter [00900, 05004], lr: 0.114238, loss: 1.4448
2022-08-06 17:35:53 - train: epoch 0049, iter [01000, 05004], lr: 0.114172, loss: 1.5182
2022-08-06 17:37:27 - train: epoch 0049, iter [01100, 05004], lr: 0.114107, loss: 1.5922
2022-08-06 17:39:01 - train: epoch 0049, iter [01200, 05004], lr: 0.114042, loss: 1.5081
2022-08-06 17:40:34 - train: epoch 0049, iter [01300, 05004], lr: 0.113976, loss: 1.7728
2022-08-06 17:42:08 - train: epoch 0049, iter [01400, 05004], lr: 0.113911, loss: 1.6709
2022-08-06 17:43:42 - train: epoch 0049, iter [01500, 05004], lr: 0.113845, loss: 1.5151
2022-08-06 17:45:16 - train: epoch 0049, iter [01600, 05004], lr: 0.113780, loss: 1.5466
2022-08-06 17:46:50 - train: epoch 0049, iter [01700, 05004], lr: 0.113714, loss: 1.5022
2022-08-06 17:48:24 - train: epoch 0049, iter [01800, 05004], lr: 0.113649, loss: 1.7901
2022-08-06 17:49:58 - train: epoch 0049, iter [01900, 05004], lr: 0.113583, loss: 1.4455
2022-08-06 17:51:32 - train: epoch 0049, iter [02000, 05004], lr: 0.113518, loss: 1.6057
2022-08-06 17:53:05 - train: epoch 0049, iter [02100, 05004], lr: 0.113453, loss: 1.4816
2022-08-06 17:54:39 - train: epoch 0049, iter [02200, 05004], lr: 0.113387, loss: 1.6074
2022-08-06 17:56:13 - train: epoch 0049, iter [02300, 05004], lr: 0.113322, loss: 1.5138
2022-08-06 17:57:47 - train: epoch 0049, iter [02400, 05004], lr: 0.113256, loss: 1.6014
2022-08-06 17:59:21 - train: epoch 0049, iter [02500, 05004], lr: 0.113191, loss: 1.6451
2022-08-06 18:00:55 - train: epoch 0049, iter [02600, 05004], lr: 0.113125, loss: 1.6482
2022-08-06 18:02:29 - train: epoch 0049, iter [02700, 05004], lr: 0.113059, loss: 1.6183
2022-08-06 18:04:02 - train: epoch 0049, iter [02800, 05004], lr: 0.112994, loss: 1.7123
2022-08-06 18:05:36 - train: epoch 0049, iter [02900, 05004], lr: 0.112928, loss: 1.7201
2022-08-06 18:07:10 - train: epoch 0049, iter [03000, 05004], lr: 0.112863, loss: 1.6602
2022-08-06 18:08:44 - train: epoch 0049, iter [03100, 05004], lr: 0.112797, loss: 1.7507
2022-08-06 18:10:18 - train: epoch 0049, iter [03200, 05004], lr: 0.112732, loss: 1.8018
2022-08-06 18:11:52 - train: epoch 0049, iter [03300, 05004], lr: 0.112666, loss: 1.8009
2022-08-06 18:13:25 - train: epoch 0049, iter [03400, 05004], lr: 0.112601, loss: 1.4336
2022-08-06 18:14:59 - train: epoch 0049, iter [03500, 05004], lr: 0.112535, loss: 1.8290
2022-08-06 18:16:33 - train: epoch 0049, iter [03600, 05004], lr: 0.112470, loss: 1.5526
2022-08-06 18:18:07 - train: epoch 0049, iter [03700, 05004], lr: 0.112404, loss: 1.3743
2022-08-06 18:19:41 - train: epoch 0049, iter [03800, 05004], lr: 0.112338, loss: 1.7524
2022-08-06 18:21:14 - train: epoch 0049, iter [03900, 05004], lr: 0.112273, loss: 1.8750
2022-08-06 18:22:48 - train: epoch 0049, iter [04000, 05004], lr: 0.112207, loss: 1.4759
2022-08-06 18:24:22 - train: epoch 0049, iter [04100, 05004], lr: 0.112142, loss: 1.4963
2022-08-06 18:25:56 - train: epoch 0049, iter [04200, 05004], lr: 0.112076, loss: 1.6300
2022-08-06 18:27:30 - train: epoch 0049, iter [04300, 05004], lr: 0.112010, loss: 1.6739
2022-08-06 18:29:04 - train: epoch 0049, iter [04400, 05004], lr: 0.111945, loss: 1.6792
2022-08-06 18:30:37 - train: epoch 0049, iter [04500, 05004], lr: 0.111879, loss: 1.7413
2022-08-06 18:32:11 - train: epoch 0049, iter [04600, 05004], lr: 0.111814, loss: 1.8550
2022-08-06 18:33:45 - train: epoch 0049, iter [04700, 05004], lr: 0.111748, loss: 1.8019
2022-08-06 18:35:19 - train: epoch 0049, iter [04800, 05004], lr: 0.111682, loss: 1.4575
2022-08-06 18:36:53 - train: epoch 0049, iter [04900, 05004], lr: 0.111617, loss: 1.6195
2022-08-06 18:38:27 - train: epoch 0049, iter [05000, 05004], lr: 0.111551, loss: 1.5282
2022-08-06 18:38:31 - train: epoch 049, train_loss: 1.6238
2022-08-06 18:40:38 - eval: epoch: 049, acc1: 66.484%, acc5: 87.770%, test_loss: 1.3748, per_image_load_time: 3.234ms, per_image_inference_time: 1.234ms
2022-08-06 18:40:39 - until epoch: 049, best_acc1: 66.484%
2022-08-06 18:40:39 - epoch 050 lr: 0.111548
2022-08-06 18:42:21 - train: epoch 0050, iter [00100, 05004], lr: 0.111483, loss: 1.4232
2022-08-06 18:43:55 - train: epoch 0050, iter [00200, 05004], lr: 0.111417, loss: 1.4207
2022-08-06 18:45:29 - train: epoch 0050, iter [00300, 05004], lr: 0.111352, loss: 1.4574
2022-08-06 18:47:03 - train: epoch 0050, iter [00400, 05004], lr: 0.111286, loss: 1.3708
2022-08-06 18:48:36 - train: epoch 0050, iter [00500, 05004], lr: 0.111220, loss: 1.4116
2022-08-06 18:50:10 - train: epoch 0050, iter [00600, 05004], lr: 0.111155, loss: 1.8720
2022-08-06 18:51:44 - train: epoch 0050, iter [00700, 05004], lr: 0.111089, loss: 1.3935
2022-08-06 18:53:18 - train: epoch 0050, iter [00800, 05004], lr: 0.111023, loss: 1.3514
2022-08-06 18:54:52 - train: epoch 0050, iter [00900, 05004], lr: 0.110957, loss: 1.6238
2022-08-06 18:56:26 - train: epoch 0050, iter [01000, 05004], lr: 0.110892, loss: 1.5160
2022-08-06 18:57:59 - train: epoch 0050, iter [01100, 05004], lr: 0.110826, loss: 1.5507
2022-08-06 18:59:33 - train: epoch 0050, iter [01200, 05004], lr: 0.110760, loss: 1.6401
2022-08-06 19:01:07 - train: epoch 0050, iter [01300, 05004], lr: 0.110695, loss: 1.5855
2022-08-06 19:02:41 - train: epoch 0050, iter [01400, 05004], lr: 0.110629, loss: 1.5502
2022-08-06 19:04:15 - train: epoch 0050, iter [01500, 05004], lr: 0.110563, loss: 1.6324
2022-08-06 19:05:49 - train: epoch 0050, iter [01600, 05004], lr: 0.110498, loss: 1.6311
2022-08-06 19:07:23 - train: epoch 0050, iter [01700, 05004], lr: 0.110432, loss: 1.5363
2022-08-06 19:08:57 - train: epoch 0050, iter [01800, 05004], lr: 0.110366, loss: 1.7219
2022-08-06 19:10:30 - train: epoch 0050, iter [01900, 05004], lr: 0.110300, loss: 1.6267
2022-08-06 19:12:04 - train: epoch 0050, iter [02000, 05004], lr: 0.110235, loss: 1.4959
2022-08-06 19:13:38 - train: epoch 0050, iter [02100, 05004], lr: 0.110169, loss: 1.6076
2022-08-06 19:15:12 - train: epoch 0050, iter [02200, 05004], lr: 0.110103, loss: 1.8892
2022-08-06 19:16:46 - train: epoch 0050, iter [02300, 05004], lr: 0.110037, loss: 1.4911
2022-08-06 19:18:19 - train: epoch 0050, iter [02400, 05004], lr: 0.109972, loss: 1.7134
2022-08-06 19:19:53 - train: epoch 0050, iter [02500, 05004], lr: 0.109906, loss: 1.5425
2022-08-06 19:21:27 - train: epoch 0050, iter [02600, 05004], lr: 0.109840, loss: 1.3959
2022-08-06 19:23:01 - train: epoch 0050, iter [02700, 05004], lr: 0.109774, loss: 1.6867
2022-08-06 19:24:35 - train: epoch 0050, iter [02800, 05004], lr: 0.109709, loss: 1.8063
2022-08-06 19:26:09 - train: epoch 0050, iter [02900, 05004], lr: 0.109643, loss: 1.8022
2022-08-06 19:27:42 - train: epoch 0050, iter [03000, 05004], lr: 0.109577, loss: 1.6371
2022-08-06 19:29:16 - train: epoch 0050, iter [03100, 05004], lr: 0.109511, loss: 1.5705
2022-08-06 19:30:50 - train: epoch 0050, iter [03200, 05004], lr: 0.109445, loss: 1.5736
2022-08-06 19:32:24 - train: epoch 0050, iter [03300, 05004], lr: 0.109380, loss: 1.5313
2022-08-06 19:33:58 - train: epoch 0050, iter [03400, 05004], lr: 0.109314, loss: 1.7334
2022-08-06 19:35:32 - train: epoch 0050, iter [03500, 05004], lr: 0.109248, loss: 1.5641
2022-08-06 19:37:06 - train: epoch 0050, iter [03600, 05004], lr: 0.109182, loss: 1.5312
2022-08-06 19:38:40 - train: epoch 0050, iter [03700, 05004], lr: 0.109116, loss: 1.8942
2022-08-06 19:40:14 - train: epoch 0050, iter [03800, 05004], lr: 0.109051, loss: 1.7206
2022-08-06 19:41:48 - train: epoch 0050, iter [03900, 05004], lr: 0.108985, loss: 1.4986
2022-08-06 19:43:22 - train: epoch 0050, iter [04000, 05004], lr: 0.108919, loss: 1.6548
2022-08-06 19:44:56 - train: epoch 0050, iter [04100, 05004], lr: 0.108853, loss: 1.6592
2022-08-06 19:46:30 - train: epoch 0050, iter [04200, 05004], lr: 0.108787, loss: 1.7497
2022-08-06 19:48:03 - train: epoch 0050, iter [04300, 05004], lr: 0.108721, loss: 1.6365
2022-08-06 19:49:37 - train: epoch 0050, iter [04400, 05004], lr: 0.108656, loss: 1.4371
2022-08-06 19:51:11 - train: epoch 0050, iter [04500, 05004], lr: 0.108590, loss: 1.6468
2022-08-06 19:52:45 - train: epoch 0050, iter [04600, 05004], lr: 0.108524, loss: 1.7326
2022-08-06 19:54:19 - train: epoch 0050, iter [04700, 05004], lr: 0.108458, loss: 1.6510
2022-08-06 19:55:53 - train: epoch 0050, iter [04800, 05004], lr: 0.108392, loss: 1.6652
2022-08-06 19:57:27 - train: epoch 0050, iter [04900, 05004], lr: 0.108326, loss: 1.6257
2022-08-06 19:59:01 - train: epoch 0050, iter [05000, 05004], lr: 0.108261, loss: 1.4786
2022-08-06 19:59:05 - train: epoch 050, train_loss: 1.6091
2022-08-06 20:01:10 - eval: epoch: 050, acc1: 65.976%, acc5: 87.654%, test_loss: 1.3879, per_image_load_time: 3.526ms, per_image_inference_time: 1.193ms
2022-08-06 20:01:10 - until epoch: 050, best_acc1: 66.484%
2022-08-06 20:01:10 - epoch 051 lr: 0.108257
2022-08-06 20:02:53 - train: epoch 0051, iter [00100, 05004], lr: 0.108192, loss: 1.9827
2022-08-06 20:04:27 - train: epoch 0051, iter [00200, 05004], lr: 0.108126, loss: 1.7562
2022-08-06 20:06:01 - train: epoch 0051, iter [00300, 05004], lr: 0.108060, loss: 1.3877
2022-08-06 20:07:35 - train: epoch 0051, iter [00400, 05004], lr: 0.107994, loss: 1.5029
2022-08-06 20:09:09 - train: epoch 0051, iter [00500, 05004], lr: 0.107929, loss: 1.7789
2022-08-06 20:10:43 - train: epoch 0051, iter [00600, 05004], lr: 0.107863, loss: 1.4768
2022-08-06 20:12:16 - train: epoch 0051, iter [00700, 05004], lr: 0.107797, loss: 1.5815
2022-08-06 20:13:50 - train: epoch 0051, iter [00800, 05004], lr: 0.107731, loss: 1.6415
2022-08-06 20:15:24 - train: epoch 0051, iter [00900, 05004], lr: 0.107665, loss: 1.3414
2022-08-06 20:16:58 - train: epoch 0051, iter [01000, 05004], lr: 0.107599, loss: 1.9023
2022-08-06 20:18:32 - train: epoch 0051, iter [01100, 05004], lr: 0.107533, loss: 1.6457
2022-08-06 20:20:06 - train: epoch 0051, iter [01200, 05004], lr: 0.107467, loss: 1.3967
2022-08-06 20:21:40 - train: epoch 0051, iter [01300, 05004], lr: 0.107401, loss: 1.4054
2022-08-06 20:23:13 - train: epoch 0051, iter [01400, 05004], lr: 0.107336, loss: 1.4368
2022-08-06 20:24:47 - train: epoch 0051, iter [01500, 05004], lr: 0.107270, loss: 1.6833
2022-08-06 20:26:21 - train: epoch 0051, iter [01600, 05004], lr: 0.107204, loss: 1.5152
2022-08-06 20:27:55 - train: epoch 0051, iter [01700, 05004], lr: 0.107138, loss: 1.6792
2022-08-06 20:29:29 - train: epoch 0051, iter [01800, 05004], lr: 0.107072, loss: 1.7036
2022-08-06 20:31:02 - train: epoch 0051, iter [01900, 05004], lr: 0.107006, loss: 1.5696
2022-08-06 20:32:36 - train: epoch 0051, iter [02000, 05004], lr: 0.106940, loss: 1.4086
2022-08-06 20:34:10 - train: epoch 0051, iter [02100, 05004], lr: 0.106874, loss: 1.5682
2022-08-06 20:35:44 - train: epoch 0051, iter [02200, 05004], lr: 0.106808, loss: 1.6661
2022-08-06 20:37:18 - train: epoch 0051, iter [02300, 05004], lr: 0.106742, loss: 1.8436
2022-08-06 20:38:51 - train: epoch 0051, iter [02400, 05004], lr: 0.106676, loss: 1.7467
2022-08-06 20:40:25 - train: epoch 0051, iter [02500, 05004], lr: 0.106610, loss: 1.5313
2022-08-06 20:41:59 - train: epoch 0051, iter [02600, 05004], lr: 0.106544, loss: 1.5476
2022-08-06 20:43:33 - train: epoch 0051, iter [02700, 05004], lr: 0.106478, loss: 1.4560
2022-08-06 20:45:07 - train: epoch 0051, iter [02800, 05004], lr: 0.106413, loss: 1.4073
2022-08-06 20:46:41 - train: epoch 0051, iter [02900, 05004], lr: 0.106347, loss: 1.6859
2022-08-06 20:48:15 - train: epoch 0051, iter [03000, 05004], lr: 0.106281, loss: 1.4721
2022-08-06 20:49:48 - train: epoch 0051, iter [03100, 05004], lr: 0.106215, loss: 1.5943
2022-08-06 20:51:22 - train: epoch 0051, iter [03200, 05004], lr: 0.106149, loss: 1.6126
2022-08-06 20:52:56 - train: epoch 0051, iter [03300, 05004], lr: 0.106083, loss: 1.7476
2022-08-06 20:54:30 - train: epoch 0051, iter [03400, 05004], lr: 0.106017, loss: 1.5252
2022-08-06 20:56:04 - train: epoch 0051, iter [03500, 05004], lr: 0.105951, loss: 1.5148
2022-08-06 20:57:37 - train: epoch 0051, iter [03600, 05004], lr: 0.105885, loss: 1.8112
2022-08-06 20:59:11 - train: epoch 0051, iter [03700, 05004], lr: 0.105819, loss: 1.6110
2022-08-06 21:00:45 - train: epoch 0051, iter [03800, 05004], lr: 0.105753, loss: 1.6323
2022-08-06 21:02:19 - train: epoch 0051, iter [03900, 05004], lr: 0.105687, loss: 1.7125
2022-08-06 21:03:53 - train: epoch 0051, iter [04000, 05004], lr: 0.105621, loss: 1.6701
2022-08-06 21:05:27 - train: epoch 0051, iter [04100, 05004], lr: 0.105555, loss: 1.3968
2022-08-06 21:07:00 - train: epoch 0051, iter [04200, 05004], lr: 0.105489, loss: 1.8382
2022-08-06 21:08:34 - train: epoch 0051, iter [04300, 05004], lr: 0.105423, loss: 1.3773
2022-08-06 21:10:08 - train: epoch 0051, iter [04400, 05004], lr: 0.105357, loss: 1.6453
2022-08-06 21:11:42 - train: epoch 0051, iter [04500, 05004], lr: 0.105291, loss: 1.5233
2022-08-06 21:13:16 - train: epoch 0051, iter [04600, 05004], lr: 0.105225, loss: 1.5984
2022-08-06 21:14:49 - train: epoch 0051, iter [04700, 05004], lr: 0.105159, loss: 1.6173
2022-08-06 21:16:23 - train: epoch 0051, iter [04800, 05004], lr: 0.105093, loss: 1.6970
2022-08-06 21:17:57 - train: epoch 0051, iter [04900, 05004], lr: 0.105027, loss: 1.6893
2022-08-06 21:19:31 - train: epoch 0051, iter [05000, 05004], lr: 0.104961, loss: 1.6836
2022-08-06 21:19:35 - train: epoch 051, train_loss: 1.5983
2022-08-06 21:21:41 - eval: epoch: 051, acc1: 66.774%, acc5: 88.052%, test_loss: 1.3534, per_image_load_time: 3.114ms, per_image_inference_time: 1.275ms
2022-08-06 21:21:42 - until epoch: 051, best_acc1: 66.774%
2022-08-06 21:21:42 - epoch 052 lr: 0.104958
2022-08-06 21:23:25 - train: epoch 0052, iter [00100, 05004], lr: 0.104892, loss: 1.4857
2022-08-06 21:24:59 - train: epoch 0052, iter [00200, 05004], lr: 0.104826, loss: 1.6097
2022-08-06 21:26:33 - train: epoch 0052, iter [00300, 05004], lr: 0.104760, loss: 1.4027
2022-08-06 21:28:07 - train: epoch 0052, iter [00400, 05004], lr: 0.104694, loss: 1.6370
2022-08-06 21:29:41 - train: epoch 0052, iter [00500, 05004], lr: 0.104628, loss: 1.6647
2022-08-06 21:31:15 - train: epoch 0052, iter [00600, 05004], lr: 0.104562, loss: 1.5008
2022-08-06 21:32:49 - train: epoch 0052, iter [00700, 05004], lr: 0.104496, loss: 1.5739
2022-08-06 21:34:23 - train: epoch 0052, iter [00800, 05004], lr: 0.104430, loss: 1.4905
2022-08-06 21:35:57 - train: epoch 0052, iter [00900, 05004], lr: 0.104364, loss: 1.5681
2022-08-06 21:37:31 - train: epoch 0052, iter [01000, 05004], lr: 0.104298, loss: 1.8414
2022-08-06 21:39:05 - train: epoch 0052, iter [01100, 05004], lr: 0.104232, loss: 1.8310
2022-08-06 21:40:39 - train: epoch 0052, iter [01200, 05004], lr: 0.104166, loss: 1.6040
2022-08-06 21:42:13 - train: epoch 0052, iter [01300, 05004], lr: 0.104100, loss: 1.4516
2022-08-06 21:43:47 - train: epoch 0052, iter [01400, 05004], lr: 0.104034, loss: 1.8583
2022-08-06 21:45:21 - train: epoch 0052, iter [01500, 05004], lr: 0.103968, loss: 1.3974
2022-08-06 21:46:55 - train: epoch 0052, iter [01600, 05004], lr: 0.103902, loss: 1.4942
2022-08-06 21:48:29 - train: epoch 0052, iter [01700, 05004], lr: 0.103836, loss: 1.4651
2022-08-06 21:50:03 - train: epoch 0052, iter [01800, 05004], lr: 0.103770, loss: 1.5739
2022-08-06 21:51:37 - train: epoch 0052, iter [01900, 05004], lr: 0.103704, loss: 1.4059
2022-08-06 21:53:11 - train: epoch 0052, iter [02000, 05004], lr: 0.103638, loss: 1.6472
2022-08-06 21:54:45 - train: epoch 0052, iter [02100, 05004], lr: 0.103572, loss: 1.6111
2022-08-06 21:56:19 - train: epoch 0052, iter [02200, 05004], lr: 0.103506, loss: 1.6954
2022-08-06 21:57:53 - train: epoch 0052, iter [02300, 05004], lr: 0.103440, loss: 1.4358
2022-08-06 21:59:26 - train: epoch 0052, iter [02400, 05004], lr: 0.103374, loss: 1.3693
2022-08-06 22:01:00 - train: epoch 0052, iter [02500, 05004], lr: 0.103308, loss: 1.3601
2022-08-06 22:02:35 - train: epoch 0052, iter [02600, 05004], lr: 0.103242, loss: 1.3685
2022-08-06 22:04:09 - train: epoch 0052, iter [02700, 05004], lr: 0.103176, loss: 1.5225
2022-08-06 22:05:43 - train: epoch 0052, iter [02800, 05004], lr: 0.103110, loss: 1.6458
2022-08-06 22:07:17 - train: epoch 0052, iter [02900, 05004], lr: 0.103043, loss: 1.5432
2022-08-06 22:08:52 - train: epoch 0052, iter [03000, 05004], lr: 0.102977, loss: 1.4577
2022-08-06 22:10:26 - train: epoch 0052, iter [03100, 05004], lr: 0.102911, loss: 1.8365
2022-08-06 22:12:02 - train: epoch 0052, iter [03200, 05004], lr: 0.102845, loss: 1.6560
2022-08-06 22:13:35 - train: epoch 0052, iter [03300, 05004], lr: 0.102779, loss: 1.5844
2022-08-06 22:15:10 - train: epoch 0052, iter [03400, 05004], lr: 0.102713, loss: 1.6914
2022-08-06 22:16:44 - train: epoch 0052, iter [03500, 05004], lr: 0.102647, loss: 1.6461
2022-08-06 22:18:19 - train: epoch 0052, iter [03600, 05004], lr: 0.102581, loss: 1.7048
2022-08-06 22:19:53 - train: epoch 0052, iter [03700, 05004], lr: 0.102515, loss: 1.7142
2022-08-06 22:21:27 - train: epoch 0052, iter [03800, 05004], lr: 0.102449, loss: 1.4797
2022-08-06 22:23:01 - train: epoch 0052, iter [03900, 05004], lr: 0.102383, loss: 1.4209
2022-08-06 22:24:35 - train: epoch 0052, iter [04000, 05004], lr: 0.102317, loss: 1.7522
2022-08-06 22:26:09 - train: epoch 0052, iter [04100, 05004], lr: 0.102251, loss: 1.4535
2022-08-06 22:27:43 - train: epoch 0052, iter [04200, 05004], lr: 0.102185, loss: 1.6018
2022-08-06 22:29:17 - train: epoch 0052, iter [04300, 05004], lr: 0.102119, loss: 1.6219
2022-08-06 22:30:51 - train: epoch 0052, iter [04400, 05004], lr: 0.102052, loss: 1.6425
2022-08-06 22:32:25 - train: epoch 0052, iter [04500, 05004], lr: 0.101986, loss: 1.4957
2022-08-06 22:33:59 - train: epoch 0052, iter [04600, 05004], lr: 0.101920, loss: 1.5841
2022-08-06 22:35:32 - train: epoch 0052, iter [04700, 05004], lr: 0.101854, loss: 1.4825
2022-08-06 22:37:06 - train: epoch 0052, iter [04800, 05004], lr: 0.101788, loss: 1.5063
2022-08-06 22:38:40 - train: epoch 0052, iter [04900, 05004], lr: 0.101722, loss: 1.4757
2022-08-06 22:40:14 - train: epoch 0052, iter [05000, 05004], lr: 0.101656, loss: 1.3885
2022-08-06 22:40:19 - train: epoch 052, train_loss: 1.5829
2022-08-06 22:42:25 - eval: epoch: 052, acc1: 67.154%, acc5: 88.274%, test_loss: 1.3433, per_image_load_time: 3.624ms, per_image_inference_time: 1.251ms
2022-08-06 22:42:26 - until epoch: 052, best_acc1: 67.154%
2022-08-06 22:42:26 - epoch 053 lr: 0.101653
2022-08-06 22:44:09 - train: epoch 0053, iter [00100, 05004], lr: 0.101587, loss: 1.6691
2022-08-06 22:45:43 - train: epoch 0053, iter [00200, 05004], lr: 0.101521, loss: 1.6779
2022-08-06 22:47:17 - train: epoch 0053, iter [00300, 05004], lr: 0.101455, loss: 1.6172
2022-08-06 22:48:50 - train: epoch 0053, iter [00400, 05004], lr: 0.101389, loss: 1.5542
2022-08-06 22:50:24 - train: epoch 0053, iter [00500, 05004], lr: 0.101323, loss: 1.5707
2022-08-06 22:51:58 - train: epoch 0053, iter [00600, 05004], lr: 0.101257, loss: 1.5374
2022-08-06 22:53:32 - train: epoch 0053, iter [00700, 05004], lr: 0.101191, loss: 1.5382
2022-08-06 22:55:05 - train: epoch 0053, iter [00800, 05004], lr: 0.101125, loss: 1.6008
2022-08-06 22:56:39 - train: epoch 0053, iter [00900, 05004], lr: 0.101059, loss: 1.4673
2022-08-06 22:58:13 - train: epoch 0053, iter [01000, 05004], lr: 0.100993, loss: 1.4560
2022-08-06 22:59:47 - train: epoch 0053, iter [01100, 05004], lr: 0.100927, loss: 1.5416
2022-08-06 23:01:21 - train: epoch 0053, iter [01200, 05004], lr: 0.100860, loss: 1.5430
2022-08-06 23:02:55 - train: epoch 0053, iter [01300, 05004], lr: 0.100794, loss: 1.5618
2022-08-06 23:04:28 - train: epoch 0053, iter [01400, 05004], lr: 0.100728, loss: 1.5614
2022-08-06 23:06:02 - train: epoch 0053, iter [01500, 05004], lr: 0.100662, loss: 1.3544
2022-08-06 23:07:36 - train: epoch 0053, iter [01600, 05004], lr: 0.100596, loss: 1.6202
2022-08-06 23:09:10 - train: epoch 0053, iter [01700, 05004], lr: 0.100530, loss: 1.6372
2022-08-06 23:10:44 - train: epoch 0053, iter [01800, 05004], lr: 0.100464, loss: 1.6960
2022-08-06 23:12:17 - train: epoch 0053, iter [01900, 05004], lr: 0.100398, loss: 1.5446
2022-08-06 23:13:51 - train: epoch 0053, iter [02000, 05004], lr: 0.100332, loss: 1.5071
2022-08-06 23:15:25 - train: epoch 0053, iter [02100, 05004], lr: 0.100266, loss: 1.9300
2022-08-06 23:16:59 - train: epoch 0053, iter [02200, 05004], lr: 0.100200, loss: 1.4762
2022-08-06 23:18:33 - train: epoch 0053, iter [02300, 05004], lr: 0.100133, loss: 1.6957
2022-08-06 23:20:07 - train: epoch 0053, iter [02400, 05004], lr: 0.100067, loss: 1.5353
2022-08-06 23:21:41 - train: epoch 0053, iter [02500, 05004], lr: 0.100001, loss: 1.7873
2022-08-06 23:23:14 - train: epoch 0053, iter [02600, 05004], lr: 0.099935, loss: 1.7453
2022-08-06 23:24:48 - train: epoch 0053, iter [02700, 05004], lr: 0.099869, loss: 1.7235
2022-08-06 23:26:22 - train: epoch 0053, iter [02800, 05004], lr: 0.099803, loss: 1.7766
2022-08-06 23:27:56 - train: epoch 0053, iter [02900, 05004], lr: 0.099737, loss: 1.5713
2022-08-06 23:29:30 - train: epoch 0053, iter [03000, 05004], lr: 0.099671, loss: 1.4838
2022-08-06 23:31:04 - train: epoch 0053, iter [03100, 05004], lr: 0.099605, loss: 1.6709
2022-08-06 23:32:37 - train: epoch 0053, iter [03200, 05004], lr: 0.099539, loss: 1.5735
2022-08-06 23:34:11 - train: epoch 0053, iter [03300, 05004], lr: 0.099473, loss: 1.5951
2022-08-06 23:35:45 - train: epoch 0053, iter [03400, 05004], lr: 0.099407, loss: 1.4975
2022-08-06 23:37:19 - train: epoch 0053, iter [03500, 05004], lr: 0.099340, loss: 1.4462
2022-08-06 23:38:53 - train: epoch 0053, iter [03600, 05004], lr: 0.099274, loss: 1.7751
2022-08-06 23:40:26 - train: epoch 0053, iter [03700, 05004], lr: 0.099208, loss: 1.6576
2022-08-06 23:42:00 - train: epoch 0053, iter [03800, 05004], lr: 0.099142, loss: 1.6050
2022-08-06 23:43:34 - train: epoch 0053, iter [03900, 05004], lr: 0.099076, loss: 1.8044
2022-08-06 23:45:08 - train: epoch 0053, iter [04000, 05004], lr: 0.099010, loss: 1.6314
2022-08-06 23:46:42 - train: epoch 0053, iter [04100, 05004], lr: 0.098944, loss: 1.6267
2022-08-06 23:48:15 - train: epoch 0053, iter [04200, 05004], lr: 0.098878, loss: 1.6200
2022-08-06 23:49:49 - train: epoch 0053, iter [04300, 05004], lr: 0.098812, loss: 1.7172
2022-08-06 23:51:23 - train: epoch 0053, iter [04400, 05004], lr: 0.098746, loss: 1.6412
2022-08-06 23:52:57 - train: epoch 0053, iter [04500, 05004], lr: 0.098680, loss: 1.7459
2022-08-06 23:54:31 - train: epoch 0053, iter [04600, 05004], lr: 0.098614, loss: 1.4980
2022-08-06 23:56:04 - train: epoch 0053, iter [04700, 05004], lr: 0.098547, loss: 1.6700
2022-08-06 23:57:38 - train: epoch 0053, iter [04800, 05004], lr: 0.098481, loss: 1.8289
2022-08-06 23:59:12 - train: epoch 0053, iter [04900, 05004], lr: 0.098415, loss: 1.6362
2022-08-07 00:00:46 - train: epoch 0053, iter [05000, 05004], lr: 0.098349, loss: 1.5042
2022-08-07 00:00:50 - train: epoch 053, train_loss: 1.5690
2022-08-07 00:02:55 - eval: epoch: 053, acc1: 67.028%, acc5: 88.190%, test_loss: 1.3469, per_image_load_time: 2.892ms, per_image_inference_time: 1.231ms
2022-08-07 00:02:56 - until epoch: 053, best_acc1: 67.154%
2022-08-07 00:02:56 - epoch 054 lr: 0.098346
2022-08-07 00:04:40 - train: epoch 0054, iter [00100, 05004], lr: 0.098281, loss: 1.3725
2022-08-07 00:06:14 - train: epoch 0054, iter [00200, 05004], lr: 0.098214, loss: 1.5983
2022-08-07 00:07:48 - train: epoch 0054, iter [00300, 05004], lr: 0.098148, loss: 1.5995
2022-08-07 00:09:22 - train: epoch 0054, iter [00400, 05004], lr: 0.098082, loss: 1.3040
2022-08-07 00:10:56 - train: epoch 0054, iter [00500, 05004], lr: 0.098016, loss: 1.4749
2022-08-07 00:12:30 - train: epoch 0054, iter [00600, 05004], lr: 0.097950, loss: 1.5341
2022-08-07 00:14:04 - train: epoch 0054, iter [00700, 05004], lr: 0.097884, loss: 1.6819
2022-08-07 00:15:38 - train: epoch 0054, iter [00800, 05004], lr: 0.097818, loss: 1.5214
2022-08-07 00:17:12 - train: epoch 0054, iter [00900, 05004], lr: 0.097752, loss: 1.3516
2022-08-07 00:18:46 - train: epoch 0054, iter [01000, 05004], lr: 0.097686, loss: 1.3540
2022-08-07 00:20:20 - train: epoch 0054, iter [01100, 05004], lr: 0.097620, loss: 1.4053
2022-08-07 00:21:54 - train: epoch 0054, iter [01200, 05004], lr: 0.097554, loss: 1.8299
2022-08-07 00:23:28 - train: epoch 0054, iter [01300, 05004], lr: 0.097488, loss: 1.4527
2022-08-07 00:25:02 - train: epoch 0054, iter [01400, 05004], lr: 0.097422, loss: 1.6322
2022-08-07 00:26:36 - train: epoch 0054, iter [01500, 05004], lr: 0.097356, loss: 1.5260
2022-08-07 00:28:10 - train: epoch 0054, iter [01600, 05004], lr: 0.097289, loss: 1.3675
2022-08-07 00:29:44 - train: epoch 0054, iter [01700, 05004], lr: 0.097223, loss: 1.5049
2022-08-07 00:31:18 - train: epoch 0054, iter [01800, 05004], lr: 0.097157, loss: 1.2873
2022-08-07 00:32:52 - train: epoch 0054, iter [01900, 05004], lr: 0.097091, loss: 1.8469
2022-08-07 00:34:26 - train: epoch 0054, iter [02000, 05004], lr: 0.097025, loss: 1.6075
2022-08-07 00:36:00 - train: epoch 0054, iter [02100, 05004], lr: 0.096959, loss: 1.4329
2022-08-07 00:37:34 - train: epoch 0054, iter [02200, 05004], lr: 0.096893, loss: 1.5663
2022-08-07 00:39:08 - train: epoch 0054, iter [02300, 05004], lr: 0.096827, loss: 1.4025
2022-08-07 00:40:42 - train: epoch 0054, iter [02400, 05004], lr: 0.096761, loss: 1.6009
2022-08-07 00:42:16 - train: epoch 0054, iter [02500, 05004], lr: 0.096695, loss: 1.5831
2022-08-07 00:43:50 - train: epoch 0054, iter [02600, 05004], lr: 0.096629, loss: 1.5005
2022-08-07 00:45:24 - train: epoch 0054, iter [02700, 05004], lr: 0.096563, loss: 1.4419
2022-08-07 00:46:58 - train: epoch 0054, iter [02800, 05004], lr: 0.096497, loss: 1.7407
2022-08-07 00:48:32 - train: epoch 0054, iter [02900, 05004], lr: 0.096431, loss: 1.4637
2022-08-07 00:50:06 - train: epoch 0054, iter [03000, 05004], lr: 0.096365, loss: 1.5528
2022-08-07 00:51:40 - train: epoch 0054, iter [03100, 05004], lr: 0.096299, loss: 1.6093
2022-08-07 00:53:14 - train: epoch 0054, iter [03200, 05004], lr: 0.096233, loss: 1.6964
2022-08-07 00:54:48 - train: epoch 0054, iter [03300, 05004], lr: 0.096167, loss: 1.5434
2022-08-07 00:56:22 - train: epoch 0054, iter [03400, 05004], lr: 0.096101, loss: 1.4563
2022-08-07 00:57:56 - train: epoch 0054, iter [03500, 05004], lr: 0.096035, loss: 1.7723
2022-08-07 00:59:30 - train: epoch 0054, iter [03600, 05004], lr: 0.095969, loss: 1.6204
2022-08-07 01:01:04 - train: epoch 0054, iter [03700, 05004], lr: 0.095902, loss: 1.4414
2022-08-07 01:02:38 - train: epoch 0054, iter [03800, 05004], lr: 0.095836, loss: 1.3983
2022-08-07 01:04:12 - train: epoch 0054, iter [03900, 05004], lr: 0.095770, loss: 1.5864
2022-08-07 01:05:45 - train: epoch 0054, iter [04000, 05004], lr: 0.095704, loss: 1.4494
2022-08-07 01:07:19 - train: epoch 0054, iter [04100, 05004], lr: 0.095638, loss: 1.7500
2022-08-07 01:08:53 - train: epoch 0054, iter [04200, 05004], lr: 0.095572, loss: 1.4204
2022-08-07 01:10:27 - train: epoch 0054, iter [04300, 05004], lr: 0.095506, loss: 1.6592
2022-08-07 01:12:01 - train: epoch 0054, iter [04400, 05004], lr: 0.095440, loss: 1.4277
2022-08-07 01:13:35 - train: epoch 0054, iter [04500, 05004], lr: 0.095374, loss: 1.4326
2022-08-07 01:15:09 - train: epoch 0054, iter [04600, 05004], lr: 0.095308, loss: 1.6578
2022-08-07 01:16:43 - train: epoch 0054, iter [04700, 05004], lr: 0.095242, loss: 1.8733
2022-08-07 01:18:17 - train: epoch 0054, iter [04800, 05004], lr: 0.095176, loss: 1.6049
2022-08-07 01:19:51 - train: epoch 0054, iter [04900, 05004], lr: 0.095110, loss: 1.4777
2022-08-07 01:21:26 - train: epoch 0054, iter [05000, 05004], lr: 0.095044, loss: 1.5449
2022-08-07 01:21:30 - train: epoch 054, train_loss: 1.5559
2022-08-07 01:23:36 - eval: epoch: 054, acc1: 66.970%, acc5: 88.084%, test_loss: 1.3404, per_image_load_time: 3.352ms, per_image_inference_time: 1.261ms
2022-08-07 01:23:36 - until epoch: 054, best_acc1: 67.154%
2022-08-07 01:23:36 - epoch 055 lr: 0.095041
2022-08-07 01:25:20 - train: epoch 0055, iter [00100, 05004], lr: 0.094976, loss: 1.2306
2022-08-07 01:26:54 - train: epoch 0055, iter [00200, 05004], lr: 0.094910, loss: 1.1634
2022-08-07 01:28:28 - train: epoch 0055, iter [00300, 05004], lr: 0.094844, loss: 1.3333
2022-08-07 01:30:02 - train: epoch 0055, iter [00400, 05004], lr: 0.094778, loss: 1.3306
2022-08-07 01:31:36 - train: epoch 0055, iter [00500, 05004], lr: 0.094712, loss: 1.4106
2022-08-07 01:33:10 - train: epoch 0055, iter [00600, 05004], lr: 0.094646, loss: 1.6334
2022-08-07 01:34:44 - train: epoch 0055, iter [00700, 05004], lr: 0.094580, loss: 1.6295
2022-08-07 01:36:18 - train: epoch 0055, iter [00800, 05004], lr: 0.094514, loss: 1.3294
2022-08-07 01:37:52 - train: epoch 0055, iter [00900, 05004], lr: 0.094448, loss: 1.6711
2022-08-07 01:39:25 - train: epoch 0055, iter [01000, 05004], lr: 0.094382, loss: 1.6579
2022-08-07 01:40:59 - train: epoch 0055, iter [01100, 05004], lr: 0.094316, loss: 1.3696
2022-08-07 01:42:33 - train: epoch 0055, iter [01200, 05004], lr: 0.094250, loss: 1.4827
2022-08-07 01:44:07 - train: epoch 0055, iter [01300, 05004], lr: 0.094184, loss: 1.5073
2022-08-07 01:45:41 - train: epoch 0055, iter [01400, 05004], lr: 0.094118, loss: 1.5258
2022-08-07 01:47:15 - train: epoch 0055, iter [01500, 05004], lr: 0.094052, loss: 1.5191
2022-08-07 01:48:48 - train: epoch 0055, iter [01600, 05004], lr: 0.093986, loss: 1.6627
2022-08-07 01:50:22 - train: epoch 0055, iter [01700, 05004], lr: 0.093920, loss: 1.5639
2022-08-07 01:51:56 - train: epoch 0055, iter [01800, 05004], lr: 0.093854, loss: 1.4783
2022-08-07 01:53:30 - train: epoch 0055, iter [01900, 05004], lr: 0.093788, loss: 1.5910
2022-08-07 01:55:04 - train: epoch 0055, iter [02000, 05004], lr: 0.093722, loss: 1.5177
2022-08-07 01:56:38 - train: epoch 0055, iter [02100, 05004], lr: 0.093656, loss: 1.3406
2022-08-07 01:58:11 - train: epoch 0055, iter [02200, 05004], lr: 0.093590, loss: 1.8195
2022-08-07 01:59:45 - train: epoch 0055, iter [02300, 05004], lr: 0.093524, loss: 1.6088
2022-08-07 02:01:19 - train: epoch 0055, iter [02400, 05004], lr: 0.093458, loss: 1.5491
2022-08-07 02:02:53 - train: epoch 0055, iter [02500, 05004], lr: 0.093392, loss: 1.4494
2022-08-07 02:04:27 - train: epoch 0055, iter [02600, 05004], lr: 0.093326, loss: 1.5522
2022-08-07 02:06:00 - train: epoch 0055, iter [02700, 05004], lr: 0.093260, loss: 1.4331
2022-08-07 02:07:34 - train: epoch 0055, iter [02800, 05004], lr: 0.093194, loss: 1.6285
2022-08-07 02:09:08 - train: epoch 0055, iter [02900, 05004], lr: 0.093129, loss: 1.6176
2022-08-07 02:10:42 - train: epoch 0055, iter [03000, 05004], lr: 0.093063, loss: 1.4387
2022-08-07 02:12:16 - train: epoch 0055, iter [03100, 05004], lr: 0.092997, loss: 1.7067
2022-08-07 02:13:49 - train: epoch 0055, iter [03200, 05004], lr: 0.092931, loss: 1.5118
2022-08-07 02:15:23 - train: epoch 0055, iter [03300, 05004], lr: 0.092865, loss: 1.3788
2022-08-07 02:16:57 - train: epoch 0055, iter [03400, 05004], lr: 0.092799, loss: 1.4022
2022-08-07 02:18:31 - train: epoch 0055, iter [03500, 05004], lr: 0.092733, loss: 1.5974
2022-08-07 02:20:05 - train: epoch 0055, iter [03600, 05004], lr: 0.092667, loss: 1.5996
2022-08-07 02:21:39 - train: epoch 0055, iter [03700, 05004], lr: 0.092601, loss: 1.5020
2022-08-07 02:23:13 - train: epoch 0055, iter [03800, 05004], lr: 0.092535, loss: 1.7099
2022-08-07 02:24:47 - train: epoch 0055, iter [03900, 05004], lr: 0.092469, loss: 1.8279
2022-08-07 02:26:21 - train: epoch 0055, iter [04000, 05004], lr: 0.092403, loss: 1.6149
2022-08-07 02:27:55 - train: epoch 0055, iter [04100, 05004], lr: 0.092338, loss: 1.5510
2022-08-07 02:29:29 - train: epoch 0055, iter [04200, 05004], lr: 0.092272, loss: 1.5872
2022-08-07 02:31:03 - train: epoch 0055, iter [04300, 05004], lr: 0.092206, loss: 1.7919
2022-08-07 02:32:37 - train: epoch 0055, iter [04400, 05004], lr: 0.092140, loss: 1.8088
2022-08-07 02:34:11 - train: epoch 0055, iter [04500, 05004], lr: 0.092074, loss: 1.6843
2022-08-07 02:35:45 - train: epoch 0055, iter [04600, 05004], lr: 0.092008, loss: 1.5509
2022-08-07 02:37:19 - train: epoch 0055, iter [04700, 05004], lr: 0.091942, loss: 1.4721
2022-08-07 02:38:53 - train: epoch 0055, iter [04800, 05004], lr: 0.091876, loss: 1.4487
2022-08-07 02:40:27 - train: epoch 0055, iter [04900, 05004], lr: 0.091811, loss: 1.5467
2022-08-07 02:42:00 - train: epoch 0055, iter [05000, 05004], lr: 0.091745, loss: 1.6014
2022-08-07 02:42:05 - train: epoch 055, train_loss: 1.5420
2022-08-07 02:44:13 - eval: epoch: 055, acc1: 67.360%, acc5: 88.392%, test_loss: 1.3309, per_image_load_time: 3.098ms, per_image_inference_time: 1.258ms
2022-08-07 02:44:14 - until epoch: 055, best_acc1: 67.360%
2022-08-07 02:44:14 - epoch 056 lr: 0.091741
2022-08-07 02:45:58 - train: epoch 0056, iter [00100, 05004], lr: 0.091676, loss: 1.5134
2022-08-07 02:47:32 - train: epoch 0056, iter [00200, 05004], lr: 0.091610, loss: 1.3908
2022-08-07 02:49:06 - train: epoch 0056, iter [00300, 05004], lr: 0.091545, loss: 1.3436
2022-08-07 02:50:40 - train: epoch 0056, iter [00400, 05004], lr: 0.091479, loss: 1.5231
2022-08-07 02:52:14 - train: epoch 0056, iter [00500, 05004], lr: 0.091413, loss: 1.6075
2022-08-07 02:53:48 - train: epoch 0056, iter [00600, 05004], lr: 0.091347, loss: 1.5678
2022-08-07 02:55:22 - train: epoch 0056, iter [00700, 05004], lr: 0.091281, loss: 1.5649
2022-08-07 02:56:56 - train: epoch 0056, iter [00800, 05004], lr: 0.091215, loss: 1.5894
2022-08-07 02:58:30 - train: epoch 0056, iter [00900, 05004], lr: 0.091149, loss: 1.6210
2022-08-07 03:00:04 - train: epoch 0056, iter [01000, 05004], lr: 0.091084, loss: 1.5359
2022-08-07 03:01:38 - train: epoch 0056, iter [01100, 05004], lr: 0.091018, loss: 1.5081
2022-08-07 03:03:11 - train: epoch 0056, iter [01200, 05004], lr: 0.090952, loss: 1.3819
2022-08-07 03:04:45 - train: epoch 0056, iter [01300, 05004], lr: 0.090886, loss: 1.7969
2022-08-07 03:06:19 - train: epoch 0056, iter [01400, 05004], lr: 0.090820, loss: 1.3314
2022-08-07 03:07:53 - train: epoch 0056, iter [01500, 05004], lr: 0.090755, loss: 1.8879
2022-08-07 03:09:27 - train: epoch 0056, iter [01600, 05004], lr: 0.090689, loss: 1.6408
2022-08-07 03:11:02 - train: epoch 0056, iter [01700, 05004], lr: 0.090623, loss: 1.4871
2022-08-07 03:12:36 - train: epoch 0056, iter [01800, 05004], lr: 0.090557, loss: 1.6927
2022-08-07 03:14:10 - train: epoch 0056, iter [01900, 05004], lr: 0.090491, loss: 1.6015
2022-08-07 03:15:44 - train: epoch 0056, iter [02000, 05004], lr: 0.090426, loss: 1.5344
2022-08-07 03:17:18 - train: epoch 0056, iter [02100, 05004], lr: 0.090360, loss: 1.7391
2022-08-07 03:18:51 - train: epoch 0056, iter [02200, 05004], lr: 0.090294, loss: 1.6011
2022-08-07 03:20:25 - train: epoch 0056, iter [02300, 05004], lr: 0.090228, loss: 1.6867
2022-08-07 03:21:59 - train: epoch 0056, iter [02400, 05004], lr: 0.090163, loss: 1.6594
2022-08-07 03:23:33 - train: epoch 0056, iter [02500, 05004], lr: 0.090097, loss: 1.6853
2022-08-07 03:25:07 - train: epoch 0056, iter [02600, 05004], lr: 0.090031, loss: 1.5875
2022-08-07 03:26:41 - train: epoch 0056, iter [02700, 05004], lr: 0.089965, loss: 1.4619
2022-08-07 03:28:15 - train: epoch 0056, iter [02800, 05004], lr: 0.089899, loss: 1.3641
2022-08-07 03:29:49 - train: epoch 0056, iter [02900, 05004], lr: 0.089834, loss: 1.5831
2022-08-07 03:31:23 - train: epoch 0056, iter [03000, 05004], lr: 0.089768, loss: 1.5036
2022-08-07 03:32:57 - train: epoch 0056, iter [03100, 05004], lr: 0.089702, loss: 1.3738
2022-08-07 03:34:31 - train: epoch 0056, iter [03200, 05004], lr: 0.089637, loss: 1.3313
2022-08-07 03:36:05 - train: epoch 0056, iter [03300, 05004], lr: 0.089571, loss: 1.7384
2022-08-07 03:37:39 - train: epoch 0056, iter [03400, 05004], lr: 0.089505, loss: 1.5210
2022-08-07 03:39:13 - train: epoch 0056, iter [03500, 05004], lr: 0.089439, loss: 1.3786
2022-08-07 03:40:47 - train: epoch 0056, iter [03600, 05004], lr: 0.089374, loss: 1.2470
2022-08-07 03:42:21 - train: epoch 0056, iter [03700, 05004], lr: 0.089308, loss: 1.5274
2022-08-07 03:43:54 - train: epoch 0056, iter [03800, 05004], lr: 0.089242, loss: 1.4986
2022-08-07 03:45:28 - train: epoch 0056, iter [03900, 05004], lr: 0.089177, loss: 1.5424
2022-08-07 03:47:02 - train: epoch 0056, iter [04000, 05004], lr: 0.089111, loss: 1.6096
2022-08-07 03:48:36 - train: epoch 0056, iter [04100, 05004], lr: 0.089045, loss: 1.6476
2022-08-07 03:50:10 - train: epoch 0056, iter [04200, 05004], lr: 0.088979, loss: 1.5351
2022-08-07 03:51:43 - train: epoch 0056, iter [04300, 05004], lr: 0.088914, loss: 1.4812
2022-08-07 03:53:17 - train: epoch 0056, iter [04400, 05004], lr: 0.088848, loss: 1.6694
2022-08-07 03:54:51 - train: epoch 0056, iter [04500, 05004], lr: 0.088782, loss: 1.4796
2022-08-07 03:56:25 - train: epoch 0056, iter [04600, 05004], lr: 0.088717, loss: 1.6525
2022-08-07 03:57:59 - train: epoch 0056, iter [04700, 05004], lr: 0.088651, loss: 1.7308
2022-08-07 03:59:33 - train: epoch 0056, iter [04800, 05004], lr: 0.088585, loss: 1.5911
2022-08-07 04:01:07 - train: epoch 0056, iter [04900, 05004], lr: 0.088520, loss: 1.6532
2022-08-07 04:02:41 - train: epoch 0056, iter [05000, 05004], lr: 0.088454, loss: 1.5212
2022-08-07 04:02:45 - train: epoch 056, train_loss: 1.5292
2022-08-07 04:04:49 - eval: epoch: 056, acc1: 68.250%, acc5: 88.808%, test_loss: 1.3021, per_image_load_time: 3.521ms, per_image_inference_time: 1.253ms
2022-08-07 04:04:49 - until epoch: 056, best_acc1: 68.250%
2022-08-07 04:04:49 - epoch 057 lr: 0.088451
2022-08-07 04:06:31 - train: epoch 0057, iter [00100, 05004], lr: 0.088386, loss: 1.4863
2022-08-07 04:08:05 - train: epoch 0057, iter [00200, 05004], lr: 0.088320, loss: 1.5555
2022-08-07 04:09:38 - train: epoch 0057, iter [00300, 05004], lr: 0.088255, loss: 1.3979
2022-08-07 04:11:12 - train: epoch 0057, iter [00400, 05004], lr: 0.088189, loss: 1.4789
2022-08-07 04:12:45 - train: epoch 0057, iter [00500, 05004], lr: 0.088123, loss: 1.2838
2022-08-07 04:14:19 - train: epoch 0057, iter [00600, 05004], lr: 0.088058, loss: 1.5814
2022-08-07 04:15:52 - train: epoch 0057, iter [00700, 05004], lr: 0.087992, loss: 1.3862
2022-08-07 04:17:26 - train: epoch 0057, iter [00800, 05004], lr: 0.087927, loss: 1.5779
2022-08-07 04:18:59 - train: epoch 0057, iter [00900, 05004], lr: 0.087861, loss: 1.3573
2022-08-07 04:20:33 - train: epoch 0057, iter [01000, 05004], lr: 0.087795, loss: 1.3124
2022-08-07 04:22:06 - train: epoch 0057, iter [01100, 05004], lr: 0.087730, loss: 1.5210
2022-08-07 04:23:40 - train: epoch 0057, iter [01200, 05004], lr: 0.087664, loss: 1.4576
2022-08-07 04:25:13 - train: epoch 0057, iter [01300, 05004], lr: 0.087599, loss: 1.5734
2022-08-07 04:26:47 - train: epoch 0057, iter [01400, 05004], lr: 0.087533, loss: 1.5047
2022-08-07 04:28:20 - train: epoch 0057, iter [01500, 05004], lr: 0.087467, loss: 1.7691
2022-08-07 04:29:54 - train: epoch 0057, iter [01600, 05004], lr: 0.087402, loss: 1.5690
2022-08-07 04:31:27 - train: epoch 0057, iter [01700, 05004], lr: 0.087336, loss: 1.7753
2022-08-07 04:33:01 - train: epoch 0057, iter [01800, 05004], lr: 0.087271, loss: 1.4780
2022-08-07 04:34:34 - train: epoch 0057, iter [01900, 05004], lr: 0.087205, loss: 1.6194
2022-08-07 04:36:08 - train: epoch 0057, iter [02000, 05004], lr: 0.087140, loss: 1.6172
2022-08-07 04:37:41 - train: epoch 0057, iter [02100, 05004], lr: 0.087074, loss: 1.4370
2022-08-07 04:39:15 - train: epoch 0057, iter [02200, 05004], lr: 0.087009, loss: 1.3776
2022-08-07 04:40:48 - train: epoch 0057, iter [02300, 05004], lr: 0.086943, loss: 1.5532
2022-08-07 04:42:22 - train: epoch 0057, iter [02400, 05004], lr: 0.086878, loss: 1.3980
2022-08-07 04:43:55 - train: epoch 0057, iter [02500, 05004], lr: 0.086812, loss: 1.5614
2022-08-07 04:45:29 - train: epoch 0057, iter [02600, 05004], lr: 0.086747, loss: 1.4347
2022-08-07 04:47:02 - train: epoch 0057, iter [02700, 05004], lr: 0.086681, loss: 1.3326
2022-08-07 04:48:36 - train: epoch 0057, iter [02800, 05004], lr: 0.086616, loss: 1.4205
2022-08-07 04:50:10 - train: epoch 0057, iter [02900, 05004], lr: 0.086550, loss: 1.4680
2022-08-07 04:51:43 - train: epoch 0057, iter [03000, 05004], lr: 0.086485, loss: 1.6909
2022-08-07 04:53:17 - train: epoch 0057, iter [03100, 05004], lr: 0.086419, loss: 1.6521
2022-08-07 04:54:51 - train: epoch 0057, iter [03200, 05004], lr: 0.086354, loss: 1.6335
2022-08-07 04:56:25 - train: epoch 0057, iter [03300, 05004], lr: 0.086288, loss: 1.5728
2022-08-07 04:57:58 - train: epoch 0057, iter [03400, 05004], lr: 0.086223, loss: 1.5178
2022-08-07 04:59:32 - train: epoch 0057, iter [03500, 05004], lr: 0.086157, loss: 1.7051
2022-08-07 05:01:06 - train: epoch 0057, iter [03600, 05004], lr: 0.086092, loss: 1.5642
2022-08-07 05:02:40 - train: epoch 0057, iter [03700, 05004], lr: 0.086026, loss: 1.3972
2022-08-07 05:04:13 - train: epoch 0057, iter [03800, 05004], lr: 0.085961, loss: 1.3073
2022-08-07 05:05:47 - train: epoch 0057, iter [03900, 05004], lr: 0.085896, loss: 1.7135
2022-08-07 05:07:21 - train: epoch 0057, iter [04000, 05004], lr: 0.085830, loss: 1.3926
2022-08-07 05:08:55 - train: epoch 0057, iter [04100, 05004], lr: 0.085765, loss: 1.5901
2022-08-07 05:10:29 - train: epoch 0057, iter [04200, 05004], lr: 0.085699, loss: 1.5832
2022-08-07 05:12:03 - train: epoch 0057, iter [04300, 05004], lr: 0.085634, loss: 1.3265
2022-08-07 05:13:37 - train: epoch 0057, iter [04400, 05004], lr: 0.085568, loss: 1.6035
2022-08-07 05:15:11 - train: epoch 0057, iter [04500, 05004], lr: 0.085503, loss: 1.7062
2022-08-07 05:16:45 - train: epoch 0057, iter [04600, 05004], lr: 0.085438, loss: 1.5889
2022-08-07 05:18:19 - train: epoch 0057, iter [04700, 05004], lr: 0.085372, loss: 1.5849
2022-08-07 05:19:53 - train: epoch 0057, iter [04800, 05004], lr: 0.085307, loss: 1.8036
2022-08-07 05:21:27 - train: epoch 0057, iter [04900, 05004], lr: 0.085242, loss: 1.6633
2022-08-07 05:23:01 - train: epoch 0057, iter [05000, 05004], lr: 0.085176, loss: 1.5410
2022-08-07 05:23:05 - train: epoch 057, train_loss: 1.5133
2022-08-07 05:25:11 - eval: epoch: 057, acc1: 67.980%, acc5: 88.714%, test_loss: 1.3068, per_image_load_time: 3.603ms, per_image_inference_time: 1.240ms
2022-08-07 05:25:11 - until epoch: 057, best_acc1: 68.250%
2022-08-07 05:25:11 - epoch 058 lr: 0.085173
2022-08-07 05:26:53 - train: epoch 0058, iter [00100, 05004], lr: 0.085108, loss: 1.6032
2022-08-07 05:28:27 - train: epoch 0058, iter [00200, 05004], lr: 0.085043, loss: 1.3704
2022-08-07 05:30:00 - train: epoch 0058, iter [00300, 05004], lr: 0.084978, loss: 1.4934
2022-08-07 05:31:34 - train: epoch 0058, iter [00400, 05004], lr: 0.084912, loss: 1.5890
2022-08-07 05:33:07 - train: epoch 0058, iter [00500, 05004], lr: 0.084847, loss: 1.2982
2022-08-07 05:34:41 - train: epoch 0058, iter [00600, 05004], lr: 0.084782, loss: 1.5701
2022-08-07 05:36:14 - train: epoch 0058, iter [00700, 05004], lr: 0.084716, loss: 1.4929
2022-08-07 05:37:47 - train: epoch 0058, iter [00800, 05004], lr: 0.084651, loss: 1.5774
2022-08-07 05:39:21 - train: epoch 0058, iter [00900, 05004], lr: 0.084586, loss: 1.4348
2022-08-07 05:40:54 - train: epoch 0058, iter [01000, 05004], lr: 0.084520, loss: 1.5274
2022-08-07 05:42:27 - train: epoch 0058, iter [01100, 05004], lr: 0.084455, loss: 1.4297
2022-08-07 05:44:01 - train: epoch 0058, iter [01200, 05004], lr: 0.084390, loss: 1.3306
2022-08-07 05:45:34 - train: epoch 0058, iter [01300, 05004], lr: 0.084325, loss: 1.4832
2022-08-07 05:47:08 - train: epoch 0058, iter [01400, 05004], lr: 0.084259, loss: 1.5613
2022-08-07 05:48:41 - train: epoch 0058, iter [01500, 05004], lr: 0.084194, loss: 1.6440
2022-08-07 05:50:15 - train: epoch 0058, iter [01600, 05004], lr: 0.084129, loss: 1.4107
2022-08-07 05:51:48 - train: epoch 0058, iter [01700, 05004], lr: 0.084064, loss: 1.6083
2022-08-07 05:53:21 - train: epoch 0058, iter [01800, 05004], lr: 0.083998, loss: 1.6584
2022-08-07 05:54:55 - train: epoch 0058, iter [01900, 05004], lr: 0.083933, loss: 1.7398
2022-08-07 05:56:28 - train: epoch 0058, iter [02000, 05004], lr: 0.083868, loss: 1.5864
2022-08-07 05:58:02 - train: epoch 0058, iter [02100, 05004], lr: 0.083803, loss: 1.6235
2022-08-07 05:59:35 - train: epoch 0058, iter [02200, 05004], lr: 0.083737, loss: 1.4130
2022-08-07 06:01:09 - train: epoch 0058, iter [02300, 05004], lr: 0.083672, loss: 1.4772
2022-08-07 06:02:42 - train: epoch 0058, iter [02400, 05004], lr: 0.083607, loss: 1.5954
2022-08-07 06:04:16 - train: epoch 0058, iter [02500, 05004], lr: 0.083542, loss: 1.4522
2022-08-07 06:05:49 - train: epoch 0058, iter [02600, 05004], lr: 0.083477, loss: 1.5625
2022-08-07 06:07:22 - train: epoch 0058, iter [02700, 05004], lr: 0.083411, loss: 1.8222
2022-08-07 06:08:56 - train: epoch 0058, iter [02800, 05004], lr: 0.083346, loss: 1.4009
2022-08-07 06:10:30 - train: epoch 0058, iter [02900, 05004], lr: 0.083281, loss: 1.4237
2022-08-07 06:12:03 - train: epoch 0058, iter [03000, 05004], lr: 0.083216, loss: 1.4755
2022-08-07 06:13:37 - train: epoch 0058, iter [03100, 05004], lr: 0.083151, loss: 1.5081
2022-08-07 06:15:10 - train: epoch 0058, iter [03200, 05004], lr: 0.083086, loss: 1.3720
2022-08-07 06:16:44 - train: epoch 0058, iter [03300, 05004], lr: 0.083021, loss: 1.5542
2022-08-07 06:18:17 - train: epoch 0058, iter [03400, 05004], lr: 0.082955, loss: 1.4902
2022-08-07 06:19:51 - train: epoch 0058, iter [03500, 05004], lr: 0.082890, loss: 1.5567
2022-08-07 06:21:24 - train: epoch 0058, iter [03600, 05004], lr: 0.082825, loss: 1.5068
2022-08-07 06:22:58 - train: epoch 0058, iter [03700, 05004], lr: 0.082760, loss: 1.7456
2022-08-07 06:24:31 - train: epoch 0058, iter [03800, 05004], lr: 0.082695, loss: 1.3805
2022-08-07 06:26:05 - train: epoch 0058, iter [03900, 05004], lr: 0.082630, loss: 1.4845
2022-08-07 06:27:38 - train: epoch 0058, iter [04000, 05004], lr: 0.082565, loss: 1.5341
2022-08-07 06:29:11 - train: epoch 0058, iter [04100, 05004], lr: 0.082500, loss: 1.3767
2022-08-07 06:30:45 - train: epoch 0058, iter [04200, 05004], lr: 0.082435, loss: 1.4942
2022-08-07 06:32:18 - train: epoch 0058, iter [04300, 05004], lr: 0.082370, loss: 1.6648
2022-08-07 06:33:52 - train: epoch 0058, iter [04400, 05004], lr: 0.082305, loss: 1.2464
2022-08-07 06:35:25 - train: epoch 0058, iter [04500, 05004], lr: 0.082240, loss: 1.4347
2022-08-07 06:36:59 - train: epoch 0058, iter [04600, 05004], lr: 0.082175, loss: 1.4716
2022-08-07 06:38:32 - train: epoch 0058, iter [04700, 05004], lr: 0.082110, loss: 1.4627
2022-08-07 06:40:06 - train: epoch 0058, iter [04800, 05004], lr: 0.082045, loss: 1.3988
2022-08-07 06:41:39 - train: epoch 0058, iter [04900, 05004], lr: 0.081980, loss: 1.4792
2022-08-07 06:43:12 - train: epoch 0058, iter [05000, 05004], lr: 0.081915, loss: 1.3657
2022-08-07 06:43:17 - train: epoch 058, train_loss: 1.4986
2022-08-07 06:45:21 - eval: epoch: 058, acc1: 68.376%, acc5: 89.040%, test_loss: 1.2781, per_image_load_time: 3.541ms, per_image_inference_time: 1.249ms
2022-08-07 06:45:21 - until epoch: 058, best_acc1: 68.376%
2022-08-07 06:45:21 - epoch 059 lr: 0.081911
2022-08-07 06:47:04 - train: epoch 0059, iter [00100, 05004], lr: 0.081847, loss: 1.3117
2022-08-07 06:48:37 - train: epoch 0059, iter [00200, 05004], lr: 0.081782, loss: 1.3655
2022-08-07 06:50:11 - train: epoch 0059, iter [00300, 05004], lr: 0.081717, loss: 1.3596
2022-08-07 06:51:45 - train: epoch 0059, iter [00400, 05004], lr: 0.081652, loss: 1.3341
2022-08-07 06:53:19 - train: epoch 0059, iter [00500, 05004], lr: 0.081587, loss: 1.5430
2022-08-07 06:54:52 - train: epoch 0059, iter [00600, 05004], lr: 0.081522, loss: 1.4944
2022-08-07 06:56:26 - train: epoch 0059, iter [00700, 05004], lr: 0.081457, loss: 1.3451
2022-08-07 06:57:59 - train: epoch 0059, iter [00800, 05004], lr: 0.081392, loss: 1.5049
2022-08-07 06:59:33 - train: epoch 0059, iter [00900, 05004], lr: 0.081327, loss: 1.4508
2022-08-07 07:01:06 - train: epoch 0059, iter [01000, 05004], lr: 0.081262, loss: 1.7157
2022-08-07 07:02:40 - train: epoch 0059, iter [01100, 05004], lr: 0.081197, loss: 1.7953
2022-08-07 07:04:14 - train: epoch 0059, iter [01200, 05004], lr: 0.081133, loss: 1.2861
2022-08-07 07:05:48 - train: epoch 0059, iter [01300, 05004], lr: 0.081068, loss: 1.7500
2022-08-07 07:07:21 - train: epoch 0059, iter [01400, 05004], lr: 0.081003, loss: 1.6394
2022-08-07 07:08:55 - train: epoch 0059, iter [01500, 05004], lr: 0.080938, loss: 1.5724
2022-08-07 07:10:29 - train: epoch 0059, iter [01600, 05004], lr: 0.080873, loss: 1.4593
2022-08-07 07:12:02 - train: epoch 0059, iter [01700, 05004], lr: 0.080808, loss: 1.4291
2022-08-07 07:13:36 - train: epoch 0059, iter [01800, 05004], lr: 0.080743, loss: 1.4100
2022-08-07 07:15:09 - train: epoch 0059, iter [01900, 05004], lr: 0.080678, loss: 1.4241
2022-08-07 07:16:43 - train: epoch 0059, iter [02000, 05004], lr: 0.080614, loss: 1.3249
2022-08-07 07:18:17 - train: epoch 0059, iter [02100, 05004], lr: 0.080549, loss: 1.5178
2022-08-07 07:19:50 - train: epoch 0059, iter [02200, 05004], lr: 0.080484, loss: 1.3042
2022-08-07 07:21:24 - train: epoch 0059, iter [02300, 05004], lr: 0.080419, loss: 1.4066
2022-08-07 07:22:57 - train: epoch 0059, iter [02400, 05004], lr: 0.080354, loss: 1.5310
2022-08-07 07:24:31 - train: epoch 0059, iter [02500, 05004], lr: 0.080290, loss: 1.7305
2022-08-07 07:26:04 - train: epoch 0059, iter [02600, 05004], lr: 0.080225, loss: 1.4986
2022-08-07 07:27:38 - train: epoch 0059, iter [02700, 05004], lr: 0.080160, loss: 1.4298
2022-08-07 07:29:12 - train: epoch 0059, iter [02800, 05004], lr: 0.080095, loss: 1.6077
2022-08-07 07:30:45 - train: epoch 0059, iter [02900, 05004], lr: 0.080031, loss: 1.3793
2022-08-07 07:32:19 - train: epoch 0059, iter [03000, 05004], lr: 0.079966, loss: 1.6365
2022-08-07 07:33:53 - train: epoch 0059, iter [03100, 05004], lr: 0.079901, loss: 1.4188
2022-08-07 07:35:27 - train: epoch 0059, iter [03200, 05004], lr: 0.079836, loss: 1.7134
2022-08-07 07:37:00 - train: epoch 0059, iter [03300, 05004], lr: 0.079772, loss: 1.7034
2022-08-07 07:38:34 - train: epoch 0059, iter [03400, 05004], lr: 0.079707, loss: 1.8320
2022-08-07 07:40:07 - train: epoch 0059, iter [03500, 05004], lr: 0.079642, loss: 1.5049
2022-08-07 07:41:41 - train: epoch 0059, iter [03600, 05004], lr: 0.079577, loss: 1.6258
2022-08-07 07:43:14 - train: epoch 0059, iter [03700, 05004], lr: 0.079513, loss: 1.7149
2022-08-07 07:44:48 - train: epoch 0059, iter [03800, 05004], lr: 0.079448, loss: 1.6053
2022-08-07 07:46:21 - train: epoch 0059, iter [03900, 05004], lr: 0.079383, loss: 1.2140
2022-08-07 07:47:55 - train: epoch 0059, iter [04000, 05004], lr: 0.079319, loss: 1.7094
2022-08-07 07:49:28 - train: epoch 0059, iter [04100, 05004], lr: 0.079254, loss: 1.2965
2022-08-07 07:51:02 - train: epoch 0059, iter [04200, 05004], lr: 0.079189, loss: 1.4566
2022-08-07 07:52:35 - train: epoch 0059, iter [04300, 05004], lr: 0.079125, loss: 1.5917
2022-08-07 07:54:08 - train: epoch 0059, iter [04400, 05004], lr: 0.079060, loss: 1.5951
2022-08-07 07:55:42 - train: epoch 0059, iter [04500, 05004], lr: 0.078996, loss: 1.5353
2022-08-07 07:57:15 - train: epoch 0059, iter [04600, 05004], lr: 0.078931, loss: 1.5351
2022-08-07 07:58:49 - train: epoch 0059, iter [04700, 05004], lr: 0.078866, loss: 1.3695
2022-08-07 08:00:22 - train: epoch 0059, iter [04800, 05004], lr: 0.078802, loss: 1.4967
2022-08-07 08:01:55 - train: epoch 0059, iter [04900, 05004], lr: 0.078737, loss: 1.4814
2022-08-07 08:03:29 - train: epoch 0059, iter [05000, 05004], lr: 0.078673, loss: 1.7598
2022-08-07 08:03:33 - train: epoch 059, train_loss: 1.4813
2022-08-07 08:05:40 - eval: epoch: 059, acc1: 68.330%, acc5: 88.864%, test_loss: 1.2753, per_image_load_time: 3.694ms, per_image_inference_time: 1.267ms
2022-08-07 08:05:40 - until epoch: 059, best_acc1: 68.376%
2022-08-07 08:05:40 - epoch 060 lr: 0.078669
2022-08-07 08:07:23 - train: epoch 0060, iter [00100, 05004], lr: 0.078605, loss: 1.5679
2022-08-07 08:08:57 - train: epoch 0060, iter [00200, 05004], lr: 0.078541, loss: 1.3523
2022-08-07 08:10:30 - train: epoch 0060, iter [00300, 05004], lr: 0.078476, loss: 1.3119
2022-08-07 08:12:04 - train: epoch 0060, iter [00400, 05004], lr: 0.078412, loss: 1.3946
2022-08-07 08:13:37 - train: epoch 0060, iter [00500, 05004], lr: 0.078347, loss: 1.6092
2022-08-07 08:15:11 - train: epoch 0060, iter [00600, 05004], lr: 0.078283, loss: 1.4546
2022-08-07 08:16:44 - train: epoch 0060, iter [00700, 05004], lr: 0.078218, loss: 1.5445
2022-08-07 08:18:17 - train: epoch 0060, iter [00800, 05004], lr: 0.078154, loss: 1.5659
2022-08-07 08:19:51 - train: epoch 0060, iter [00900, 05004], lr: 0.078089, loss: 1.4199
2022-08-07 08:21:24 - train: epoch 0060, iter [01000, 05004], lr: 0.078025, loss: 1.1026
2022-08-07 08:22:57 - train: epoch 0060, iter [01100, 05004], lr: 0.077960, loss: 1.3400
2022-08-07 08:24:31 - train: epoch 0060, iter [01200, 05004], lr: 0.077896, loss: 1.3522
2022-08-07 08:26:06 - train: epoch 0060, iter [01300, 05004], lr: 0.077831, loss: 1.4105
2022-08-07 08:27:40 - train: epoch 0060, iter [01400, 05004], lr: 0.077767, loss: 1.5996
2022-08-07 08:29:14 - train: epoch 0060, iter [01500, 05004], lr: 0.077703, loss: 1.6018
2022-08-07 08:30:47 - train: epoch 0060, iter [01600, 05004], lr: 0.077638, loss: 1.3469
2022-08-07 08:32:23 - train: epoch 0060, iter [01700, 05004], lr: 0.077574, loss: 1.5003
2022-08-07 08:33:56 - train: epoch 0060, iter [01800, 05004], lr: 0.077509, loss: 1.4395
2022-08-07 08:35:30 - train: epoch 0060, iter [01900, 05004], lr: 0.077445, loss: 1.7361
2022-08-07 08:37:04 - train: epoch 0060, iter [02000, 05004], lr: 0.077381, loss: 1.3305
2022-08-07 08:38:38 - train: epoch 0060, iter [02100, 05004], lr: 0.077316, loss: 1.5079
2022-08-07 08:40:12 - train: epoch 0060, iter [02200, 05004], lr: 0.077252, loss: 1.7675
2022-08-07 08:41:46 - train: epoch 0060, iter [02300, 05004], lr: 0.077188, loss: 1.4320
2022-08-07 08:43:20 - train: epoch 0060, iter [02400, 05004], lr: 0.077123, loss: 1.4932
2022-08-07 08:44:53 - train: epoch 0060, iter [02500, 05004], lr: 0.077059, loss: 1.5963
2022-08-07 08:46:27 - train: epoch 0060, iter [02600, 05004], lr: 0.076995, loss: 1.6084
2022-08-07 08:48:01 - train: epoch 0060, iter [02700, 05004], lr: 0.076930, loss: 1.2450
2022-08-07 08:49:35 - train: epoch 0060, iter [02800, 05004], lr: 0.076866, loss: 1.7996
2022-08-07 08:51:08 - train: epoch 0060, iter [02900, 05004], lr: 0.076802, loss: 1.4601
2022-08-07 08:52:42 - train: epoch 0060, iter [03000, 05004], lr: 0.076737, loss: 1.5503
2022-08-07 08:54:16 - train: epoch 0060, iter [03100, 05004], lr: 0.076673, loss: 1.5580
2022-08-07 08:55:49 - train: epoch 0060, iter [03200, 05004], lr: 0.076609, loss: 1.3675
2022-08-07 08:57:23 - train: epoch 0060, iter [03300, 05004], lr: 0.076545, loss: 1.2691
2022-08-07 08:58:57 - train: epoch 0060, iter [03400, 05004], lr: 0.076480, loss: 1.5679
2022-08-07 09:00:30 - train: epoch 0060, iter [03500, 05004], lr: 0.076416, loss: 1.6977
2022-08-07 09:02:04 - train: epoch 0060, iter [03600, 05004], lr: 0.076352, loss: 1.6019
2022-08-07 09:03:37 - train: epoch 0060, iter [03700, 05004], lr: 0.076288, loss: 1.5814
2022-08-07 09:05:11 - train: epoch 0060, iter [03800, 05004], lr: 0.076224, loss: 1.5355
2022-08-07 09:06:45 - train: epoch 0060, iter [03900, 05004], lr: 0.076159, loss: 1.7267
2022-08-07 09:08:18 - train: epoch 0060, iter [04000, 05004], lr: 0.076095, loss: 1.4132
2022-08-07 09:09:52 - train: epoch 0060, iter [04100, 05004], lr: 0.076031, loss: 1.5310
2022-08-07 09:11:25 - train: epoch 0060, iter [04200, 05004], lr: 0.075967, loss: 1.6003
2022-08-07 09:12:59 - train: epoch 0060, iter [04300, 05004], lr: 0.075903, loss: 1.3256
2022-08-07 09:14:33 - train: epoch 0060, iter [04400, 05004], lr: 0.075839, loss: 1.6057
2022-08-07 09:16:07 - train: epoch 0060, iter [04500, 05004], lr: 0.075774, loss: 1.5335
2022-08-07 09:17:41 - train: epoch 0060, iter [04600, 05004], lr: 0.075710, loss: 1.3279
2022-08-07 09:19:15 - train: epoch 0060, iter [04700, 05004], lr: 0.075646, loss: 1.3897
2022-08-07 09:20:49 - train: epoch 0060, iter [04800, 05004], lr: 0.075582, loss: 1.4037
2022-08-07 09:22:22 - train: epoch 0060, iter [04900, 05004], lr: 0.075518, loss: 1.3902
2022-08-07 09:23:56 - train: epoch 0060, iter [05000, 05004], lr: 0.075454, loss: 1.4869
2022-08-07 09:24:01 - train: epoch 060, train_loss: 1.4667
2022-08-07 09:26:15 - eval: epoch: 060, acc1: 68.750%, acc5: 89.242%, test_loss: 1.2674, per_image_load_time: 4.002ms, per_image_inference_time: 1.221ms
2022-08-07 09:26:16 - until epoch: 060, best_acc1: 68.750%
2022-08-07 09:26:16 - epoch 061 lr: 0.075451
2022-08-07 09:27:58 - train: epoch 0061, iter [00100, 05004], lr: 0.075387, loss: 1.2890
2022-08-07 09:29:31 - train: epoch 0061, iter [00200, 05004], lr: 0.075323, loss: 1.3895
2022-08-07 09:31:05 - train: epoch 0061, iter [00300, 05004], lr: 0.075259, loss: 1.3297
2022-08-07 09:32:38 - train: epoch 0061, iter [00400, 05004], lr: 0.075195, loss: 1.6396
2022-08-07 09:34:12 - train: epoch 0061, iter [00500, 05004], lr: 0.075131, loss: 1.4449
2022-08-07 09:35:45 - train: epoch 0061, iter [00600, 05004], lr: 0.075067, loss: 1.3945
2022-08-07 09:37:19 - train: epoch 0061, iter [00700, 05004], lr: 0.075003, loss: 1.2756
2022-08-07 09:38:52 - train: epoch 0061, iter [00800, 05004], lr: 0.074939, loss: 1.2934
2022-08-07 09:40:25 - train: epoch 0061, iter [00900, 05004], lr: 0.074875, loss: 1.3497
2022-08-07 09:41:59 - train: epoch 0061, iter [01000, 05004], lr: 0.074811, loss: 1.4464
2022-08-07 09:43:32 - train: epoch 0061, iter [01100, 05004], lr: 0.074747, loss: 1.2014
2022-08-07 09:45:08 - train: epoch 0061, iter [01200, 05004], lr: 0.074683, loss: 1.3879
2022-08-07 09:46:42 - train: epoch 0061, iter [01300, 05004], lr: 0.074620, loss: 1.5404
2022-08-07 09:48:19 - train: epoch 0061, iter [01400, 05004], lr: 0.074556, loss: 1.5782
2022-08-07 09:49:53 - train: epoch 0061, iter [01500, 05004], lr: 0.074492, loss: 1.4070
2022-08-07 09:51:27 - train: epoch 0061, iter [01600, 05004], lr: 0.074428, loss: 1.2612
2022-08-07 09:53:01 - train: epoch 0061, iter [01700, 05004], lr: 0.074364, loss: 1.4080
2022-08-07 09:54:39 - train: epoch 0061, iter [01800, 05004], lr: 0.074300, loss: 1.4785
2022-08-07 09:56:12 - train: epoch 0061, iter [01900, 05004], lr: 0.074236, loss: 1.6298
2022-08-07 09:57:46 - train: epoch 0061, iter [02000, 05004], lr: 0.074172, loss: 1.3586
2022-08-07 09:59:20 - train: epoch 0061, iter [02100, 05004], lr: 0.074109, loss: 1.6897
2022-08-07 10:00:54 - train: epoch 0061, iter [02200, 05004], lr: 0.074045, loss: 1.3705
2022-08-07 10:02:27 - train: epoch 0061, iter [02300, 05004], lr: 0.073981, loss: 1.4322
2022-08-07 10:04:01 - train: epoch 0061, iter [02400, 05004], lr: 0.073917, loss: 1.3366
2022-08-07 10:05:34 - train: epoch 0061, iter [02500, 05004], lr: 0.073853, loss: 1.2409
2022-08-07 10:07:08 - train: epoch 0061, iter [02600, 05004], lr: 0.073790, loss: 1.6214
2022-08-07 10:08:41 - train: epoch 0061, iter [02700, 05004], lr: 0.073726, loss: 1.6185
2022-08-07 10:10:15 - train: epoch 0061, iter [02800, 05004], lr: 0.073662, loss: 1.5180
2022-08-07 10:11:48 - train: epoch 0061, iter [02900, 05004], lr: 0.073598, loss: 1.6745
2022-08-07 10:13:22 - train: epoch 0061, iter [03000, 05004], lr: 0.073534, loss: 1.4479
2022-08-07 10:14:56 - train: epoch 0061, iter [03100, 05004], lr: 0.073471, loss: 1.4320
2022-08-07 10:16:29 - train: epoch 0061, iter [03200, 05004], lr: 0.073407, loss: 1.2665
2022-08-07 10:18:03 - train: epoch 0061, iter [03300, 05004], lr: 0.073343, loss: 1.3871
2022-08-07 10:19:38 - train: epoch 0061, iter [03400, 05004], lr: 0.073280, loss: 1.3227
2022-08-07 10:21:11 - train: epoch 0061, iter [03500, 05004], lr: 0.073216, loss: 1.4976
2022-08-07 10:22:45 - train: epoch 0061, iter [03600, 05004], lr: 0.073152, loss: 1.4597
2022-08-07 10:24:19 - train: epoch 0061, iter [03700, 05004], lr: 0.073089, loss: 1.4783
2022-08-07 10:25:53 - train: epoch 0061, iter [03800, 05004], lr: 0.073025, loss: 1.5927
2022-08-07 10:27:26 - train: epoch 0061, iter [03900, 05004], lr: 0.072961, loss: 1.5914
2022-08-07 10:29:01 - train: epoch 0061, iter [04000, 05004], lr: 0.072898, loss: 1.3609
2022-08-07 10:30:35 - train: epoch 0061, iter [04100, 05004], lr: 0.072834, loss: 1.5071
2022-08-07 10:32:09 - train: epoch 0061, iter [04200, 05004], lr: 0.072771, loss: 1.3966
2022-08-07 10:33:43 - train: epoch 0061, iter [04300, 05004], lr: 0.072707, loss: 1.5490
2022-08-07 10:35:17 - train: epoch 0061, iter [04400, 05004], lr: 0.072643, loss: 1.5684
2022-08-07 10:36:52 - train: epoch 0061, iter [04500, 05004], lr: 0.072580, loss: 1.4580
2022-08-07 10:38:25 - train: epoch 0061, iter [04600, 05004], lr: 0.072516, loss: 1.5523
2022-08-07 10:40:01 - train: epoch 0061, iter [04700, 05004], lr: 0.072453, loss: 1.5293
2022-08-07 10:41:34 - train: epoch 0061, iter [04800, 05004], lr: 0.072389, loss: 1.5209
2022-08-07 10:43:08 - train: epoch 0061, iter [04900, 05004], lr: 0.072326, loss: 1.4745
2022-08-07 10:44:41 - train: epoch 0061, iter [05000, 05004], lr: 0.072262, loss: 1.5956
2022-08-07 10:44:46 - train: epoch 061, train_loss: 1.4493
2022-08-07 10:46:50 - eval: epoch: 061, acc1: 68.696%, acc5: 88.982%, test_loss: 1.2842, per_image_load_time: 3.627ms, per_image_inference_time: 1.213ms
2022-08-07 10:46:50 - until epoch: 061, best_acc1: 68.750%
2022-08-07 10:46:50 - epoch 062 lr: 0.072259
2022-08-07 10:48:33 - train: epoch 0062, iter [00100, 05004], lr: 0.072196, loss: 1.6042
2022-08-07 10:50:06 - train: epoch 0062, iter [00200, 05004], lr: 0.072133, loss: 1.6391
2022-08-07 10:51:40 - train: epoch 0062, iter [00300, 05004], lr: 0.072069, loss: 1.4951
2022-08-07 10:53:13 - train: epoch 0062, iter [00400, 05004], lr: 0.072006, loss: 1.2665
2022-08-07 10:54:47 - train: epoch 0062, iter [00500, 05004], lr: 0.071942, loss: 1.3800
2022-08-07 10:56:20 - train: epoch 0062, iter [00600, 05004], lr: 0.071879, loss: 1.3465
2022-08-07 10:57:54 - train: epoch 0062, iter [00700, 05004], lr: 0.071816, loss: 1.4672
2022-08-07 10:59:27 - train: epoch 0062, iter [00800, 05004], lr: 0.071752, loss: 1.4242
2022-08-07 11:01:01 - train: epoch 0062, iter [00900, 05004], lr: 0.071689, loss: 1.4632
2022-08-07 11:02:34 - train: epoch 0062, iter [01000, 05004], lr: 0.071625, loss: 1.5830
2022-08-07 11:04:08 - train: epoch 0062, iter [01100, 05004], lr: 0.071562, loss: 1.2676
2022-08-07 11:05:41 - train: epoch 0062, iter [01200, 05004], lr: 0.071499, loss: 1.4071
2022-08-07 11:07:15 - train: epoch 0062, iter [01300, 05004], lr: 0.071435, loss: 1.5034
2022-08-07 11:08:48 - train: epoch 0062, iter [01400, 05004], lr: 0.071372, loss: 1.3757
2022-08-07 11:10:22 - train: epoch 0062, iter [01500, 05004], lr: 0.071309, loss: 1.4364
2022-08-07 11:11:55 - train: epoch 0062, iter [01600, 05004], lr: 0.071245, loss: 1.5861
2022-08-07 11:13:29 - train: epoch 0062, iter [01700, 05004], lr: 0.071182, loss: 1.2742
2022-08-07 11:15:02 - train: epoch 0062, iter [01800, 05004], lr: 0.071119, loss: 1.4847
2022-08-07 11:16:35 - train: epoch 0062, iter [01900, 05004], lr: 0.071056, loss: 1.4234
2022-08-07 11:18:09 - train: epoch 0062, iter [02000, 05004], lr: 0.070992, loss: 1.3795
2022-08-07 11:19:42 - train: epoch 0062, iter [02100, 05004], lr: 0.070929, loss: 1.4849
2022-08-07 11:21:16 - train: epoch 0062, iter [02200, 05004], lr: 0.070866, loss: 1.3346
2022-08-07 11:22:49 - train: epoch 0062, iter [02300, 05004], lr: 0.070803, loss: 1.4359
2022-08-07 11:24:23 - train: epoch 0062, iter [02400, 05004], lr: 0.070739, loss: 1.5328
2022-08-07 11:25:56 - train: epoch 0062, iter [02500, 05004], lr: 0.070676, loss: 1.6802
2022-08-07 11:27:30 - train: epoch 0062, iter [02600, 05004], lr: 0.070613, loss: 1.2986
2022-08-07 11:29:03 - train: epoch 0062, iter [02700, 05004], lr: 0.070550, loss: 1.4969
2022-08-07 11:30:37 - train: epoch 0062, iter [02800, 05004], lr: 0.070487, loss: 1.5107
2022-08-07 11:32:10 - train: epoch 0062, iter [02900, 05004], lr: 0.070424, loss: 1.3607
2022-08-07 11:33:44 - train: epoch 0062, iter [03000, 05004], lr: 0.070361, loss: 1.4739
2022-08-07 11:35:17 - train: epoch 0062, iter [03100, 05004], lr: 0.070297, loss: 1.4879
2022-08-07 11:36:51 - train: epoch 0062, iter [03200, 05004], lr: 0.070234, loss: 1.3535
2022-08-07 11:38:25 - train: epoch 0062, iter [03300, 05004], lr: 0.070171, loss: 1.5250
2022-08-07 11:39:58 - train: epoch 0062, iter [03400, 05004], lr: 0.070108, loss: 1.5908
2022-08-07 11:41:32 - train: epoch 0062, iter [03500, 05004], lr: 0.070045, loss: 1.3152
2022-08-07 11:43:06 - train: epoch 0062, iter [03600, 05004], lr: 0.069982, loss: 1.5120
2022-08-07 11:44:39 - train: epoch 0062, iter [03700, 05004], lr: 0.069919, loss: 1.5957
2022-08-07 11:46:13 - train: epoch 0062, iter [03800, 05004], lr: 0.069856, loss: 1.3855
2022-08-07 11:47:47 - train: epoch 0062, iter [03900, 05004], lr: 0.069793, loss: 1.4403
2022-08-07 11:49:20 - train: epoch 0062, iter [04000, 05004], lr: 0.069730, loss: 1.1900
2022-08-07 11:50:54 - train: epoch 0062, iter [04100, 05004], lr: 0.069667, loss: 1.4928
2022-08-07 11:52:28 - train: epoch 0062, iter [04200, 05004], lr: 0.069604, loss: 1.3981
2022-08-07 11:54:01 - train: epoch 0062, iter [04300, 05004], lr: 0.069541, loss: 1.3740
2022-08-07 11:55:35 - train: epoch 0062, iter [04400, 05004], lr: 0.069478, loss: 1.3847
2022-08-07 11:57:09 - train: epoch 0062, iter [04500, 05004], lr: 0.069415, loss: 1.4021
2022-08-07 11:58:42 - train: epoch 0062, iter [04600, 05004], lr: 0.069352, loss: 1.3885
2022-08-07 12:00:16 - train: epoch 0062, iter [04700, 05004], lr: 0.069289, loss: 1.3243
2022-08-07 12:01:50 - train: epoch 0062, iter [04800, 05004], lr: 0.069227, loss: 1.4303
2022-08-07 12:03:23 - train: epoch 0062, iter [04900, 05004], lr: 0.069164, loss: 1.5799
2022-08-07 12:04:57 - train: epoch 0062, iter [05000, 05004], lr: 0.069101, loss: 1.6164
2022-08-07 12:05:01 - train: epoch 062, train_loss: 1.4350
2022-08-07 12:07:03 - eval: epoch: 062, acc1: 69.086%, acc5: 89.470%, test_loss: 1.2430, per_image_load_time: 3.465ms, per_image_inference_time: 1.224ms
2022-08-07 12:07:04 - until epoch: 062, best_acc1: 69.086%
2022-08-07 12:07:04 - epoch 063 lr: 0.069098
2022-08-07 12:08:46 - train: epoch 0063, iter [00100, 05004], lr: 0.069035, loss: 1.2237
2022-08-07 12:10:20 - train: epoch 0063, iter [00200, 05004], lr: 0.068973, loss: 1.3202
2022-08-07 12:11:53 - train: epoch 0063, iter [00300, 05004], lr: 0.068910, loss: 1.3667
2022-08-07 12:13:27 - train: epoch 0063, iter [00400, 05004], lr: 0.068847, loss: 1.2100
2022-08-07 12:15:00 - train: epoch 0063, iter [00500, 05004], lr: 0.068784, loss: 1.3233
2022-08-07 12:16:33 - train: epoch 0063, iter [00600, 05004], lr: 0.068721, loss: 1.3252
2022-08-07 12:18:07 - train: epoch 0063, iter [00700, 05004], lr: 0.068659, loss: 1.4499
2022-08-07 12:19:40 - train: epoch 0063, iter [00800, 05004], lr: 0.068596, loss: 1.2160
2022-08-07 12:21:14 - train: epoch 0063, iter [00900, 05004], lr: 0.068533, loss: 1.5214
2022-08-07 12:22:48 - train: epoch 0063, iter [01000, 05004], lr: 0.068470, loss: 1.3566
2022-08-07 12:24:21 - train: epoch 0063, iter [01100, 05004], lr: 0.068408, loss: 1.4814
2022-08-07 12:25:55 - train: epoch 0063, iter [01200, 05004], lr: 0.068345, loss: 1.3987
2022-08-07 12:27:28 - train: epoch 0063, iter [01300, 05004], lr: 0.068282, loss: 1.2904
2022-08-07 12:29:02 - train: epoch 0063, iter [01400, 05004], lr: 0.068220, loss: 1.6929
2022-08-07 12:30:36 - train: epoch 0063, iter [01500, 05004], lr: 0.068157, loss: 1.5008
2022-08-07 12:32:09 - train: epoch 0063, iter [01600, 05004], lr: 0.068094, loss: 1.4447
2022-08-07 12:33:43 - train: epoch 0063, iter [01700, 05004], lr: 0.068032, loss: 1.2838
2022-08-07 12:35:17 - train: epoch 0063, iter [01800, 05004], lr: 0.067969, loss: 1.4389
2022-08-07 12:36:51 - train: epoch 0063, iter [01900, 05004], lr: 0.067907, loss: 1.6749
2022-08-07 12:38:24 - train: epoch 0063, iter [02000, 05004], lr: 0.067844, loss: 1.2071
2022-08-07 12:39:58 - train: epoch 0063, iter [02100, 05004], lr: 0.067781, loss: 1.2853
2022-08-07 12:41:32 - train: epoch 0063, iter [02200, 05004], lr: 0.067719, loss: 1.6324
2022-08-07 12:43:05 - train: epoch 0063, iter [02300, 05004], lr: 0.067656, loss: 1.5664
2022-08-07 12:44:39 - train: epoch 0063, iter [02400, 05004], lr: 0.067594, loss: 1.3870
2022-08-07 12:46:13 - train: epoch 0063, iter [02500, 05004], lr: 0.067531, loss: 1.3599
2022-08-07 12:47:47 - train: epoch 0063, iter [02600, 05004], lr: 0.067469, loss: 1.1037
2022-08-07 12:49:22 - train: epoch 0063, iter [02700, 05004], lr: 0.067406, loss: 1.6608
2022-08-07 12:50:56 - train: epoch 0063, iter [02800, 05004], lr: 0.067344, loss: 1.3431
2022-08-07 12:52:30 - train: epoch 0063, iter [02900, 05004], lr: 0.067281, loss: 1.5337
2022-08-07 12:54:04 - train: epoch 0063, iter [03000, 05004], lr: 0.067219, loss: 1.6214
2022-08-07 12:55:38 - train: epoch 0063, iter [03100, 05004], lr: 0.067157, loss: 1.5973
2022-08-07 12:57:12 - train: epoch 0063, iter [03200, 05004], lr: 0.067094, loss: 1.4522
2022-08-07 12:58:47 - train: epoch 0063, iter [03300, 05004], lr: 0.067032, loss: 1.2821
2022-08-07 13:00:21 - train: epoch 0063, iter [03400, 05004], lr: 0.066969, loss: 1.2459
2022-08-07 13:01:55 - train: epoch 0063, iter [03500, 05004], lr: 0.066907, loss: 1.6614
2022-08-07 13:03:29 - train: epoch 0063, iter [03600, 05004], lr: 0.066845, loss: 1.5144
2022-08-07 13:05:04 - train: epoch 0063, iter [03700, 05004], lr: 0.066782, loss: 1.6688
2022-08-07 13:06:38 - train: epoch 0063, iter [03800, 05004], lr: 0.066720, loss: 1.4494
2022-08-07 13:08:13 - train: epoch 0063, iter [03900, 05004], lr: 0.066658, loss: 1.3149
2022-08-07 13:09:47 - train: epoch 0063, iter [04000, 05004], lr: 0.066595, loss: 1.4508
2022-08-07 13:11:21 - train: epoch 0063, iter [04100, 05004], lr: 0.066533, loss: 1.4972
2022-08-07 13:12:56 - train: epoch 0063, iter [04200, 05004], lr: 0.066471, loss: 1.4908
2022-08-07 13:14:30 - train: epoch 0063, iter [04300, 05004], lr: 0.066409, loss: 1.5634
2022-08-07 13:16:04 - train: epoch 0063, iter [04400, 05004], lr: 0.066346, loss: 1.3515
2022-08-07 13:17:38 - train: epoch 0063, iter [04500, 05004], lr: 0.066284, loss: 1.5362
2022-08-07 13:19:13 - train: epoch 0063, iter [04600, 05004], lr: 0.066222, loss: 1.3523
2022-08-07 13:20:47 - train: epoch 0063, iter [04700, 05004], lr: 0.066160, loss: 1.4697
2022-08-07 13:22:21 - train: epoch 0063, iter [04800, 05004], lr: 0.066097, loss: 1.3314
2022-08-07 13:23:56 - train: epoch 0063, iter [04900, 05004], lr: 0.066035, loss: 1.3597
2022-08-07 13:25:30 - train: epoch 0063, iter [05000, 05004], lr: 0.065973, loss: 1.4728
2022-08-07 13:25:35 - train: epoch 063, train_loss: 1.4154
2022-08-07 13:27:34 - eval: epoch: 063, acc1: 69.548%, acc5: 89.592%, test_loss: 1.2426, per_image_load_time: 3.460ms, per_image_inference_time: 1.118ms
2022-08-07 13:27:35 - until epoch: 063, best_acc1: 69.548%
2022-08-07 13:27:35 - epoch 064 lr: 0.065970
2022-08-07 13:29:18 - train: epoch 0064, iter [00100, 05004], lr: 0.065909, loss: 1.2696
2022-08-07 13:30:52 - train: epoch 0064, iter [00200, 05004], lr: 0.065846, loss: 1.4261
2022-08-07 13:32:26 - train: epoch 0064, iter [00300, 05004], lr: 0.065784, loss: 1.3282
2022-08-07 13:34:00 - train: epoch 0064, iter [00400, 05004], lr: 0.065722, loss: 1.2691
2022-08-07 13:35:34 - train: epoch 0064, iter [00500, 05004], lr: 0.065660, loss: 1.1831
2022-08-07 13:37:08 - train: epoch 0064, iter [00600, 05004], lr: 0.065598, loss: 1.3096
2022-08-07 13:38:41 - train: epoch 0064, iter [00700, 05004], lr: 0.065536, loss: 1.6045
2022-08-07 13:40:15 - train: epoch 0064, iter [00800, 05004], lr: 0.065474, loss: 1.2174
2022-08-07 13:41:49 - train: epoch 0064, iter [00900, 05004], lr: 0.065412, loss: 1.2632
2022-08-07 13:43:23 - train: epoch 0064, iter [01000, 05004], lr: 0.065350, loss: 1.3392
2022-08-07 13:44:57 - train: epoch 0064, iter [01100, 05004], lr: 0.065288, loss: 1.4597
2022-08-07 13:46:31 - train: epoch 0064, iter [01200, 05004], lr: 0.065226, loss: 1.1337
2022-08-07 13:48:05 - train: epoch 0064, iter [01300, 05004], lr: 0.065164, loss: 1.3641
2022-08-07 13:49:39 - train: epoch 0064, iter [01400, 05004], lr: 0.065102, loss: 1.5819
2022-08-07 13:51:14 - train: epoch 0064, iter [01500, 05004], lr: 0.065040, loss: 1.3807
2022-08-07 13:52:48 - train: epoch 0064, iter [01600, 05004], lr: 0.064978, loss: 1.3407
2022-08-07 13:54:22 - train: epoch 0064, iter [01700, 05004], lr: 0.064916, loss: 1.2123
2022-08-07 13:55:57 - train: epoch 0064, iter [01800, 05004], lr: 0.064855, loss: 1.2036
2022-08-07 13:57:31 - train: epoch 0064, iter [01900, 05004], lr: 0.064793, loss: 1.4798
2022-08-07 13:59:05 - train: epoch 0064, iter [02000, 05004], lr: 0.064731, loss: 1.3332
2022-08-07 14:00:39 - train: epoch 0064, iter [02100, 05004], lr: 0.064669, loss: 1.4710
2022-08-07 14:02:13 - train: epoch 0064, iter [02200, 05004], lr: 0.064607, loss: 1.5306
2022-08-07 14:03:47 - train: epoch 0064, iter [02300, 05004], lr: 0.064545, loss: 1.3906
2022-08-07 14:05:21 - train: epoch 0064, iter [02400, 05004], lr: 0.064484, loss: 1.4193
2022-08-07 14:06:55 - train: epoch 0064, iter [02500, 05004], lr: 0.064422, loss: 1.1920
2022-08-07 14:08:29 - train: epoch 0064, iter [02600, 05004], lr: 0.064360, loss: 1.2986
2022-08-07 14:10:04 - train: epoch 0064, iter [02700, 05004], lr: 0.064298, loss: 1.4699
2022-08-07 14:11:38 - train: epoch 0064, iter [02800, 05004], lr: 0.064237, loss: 1.2181
2022-08-07 14:13:12 - train: epoch 0064, iter [02900, 05004], lr: 0.064175, loss: 1.3916
2022-08-07 14:14:46 - train: epoch 0064, iter [03000, 05004], lr: 0.064113, loss: 1.3883
2022-08-07 14:16:20 - train: epoch 0064, iter [03100, 05004], lr: 0.064052, loss: 1.5926
2022-08-07 14:17:54 - train: epoch 0064, iter [03200, 05004], lr: 0.063990, loss: 1.6180
2022-08-07 14:19:28 - train: epoch 0064, iter [03300, 05004], lr: 0.063928, loss: 1.4775
2022-08-07 14:21:02 - train: epoch 0064, iter [03400, 05004], lr: 0.063867, loss: 1.4322
2022-08-07 14:22:36 - train: epoch 0064, iter [03500, 05004], lr: 0.063805, loss: 1.3637
2022-08-07 14:24:10 - train: epoch 0064, iter [03600, 05004], lr: 0.063743, loss: 1.2968
2022-08-07 14:25:43 - train: epoch 0064, iter [03700, 05004], lr: 0.063682, loss: 1.2315
2022-08-07 14:27:17 - train: epoch 0064, iter [03800, 05004], lr: 0.063620, loss: 1.4359
2022-08-07 14:28:51 - train: epoch 0064, iter [03900, 05004], lr: 0.063559, loss: 1.3239
2022-08-07 14:30:25 - train: epoch 0064, iter [04000, 05004], lr: 0.063497, loss: 1.2705
2022-08-07 14:31:59 - train: epoch 0064, iter [04100, 05004], lr: 0.063436, loss: 1.4784
2022-08-07 14:33:32 - train: epoch 0064, iter [04200, 05004], lr: 0.063374, loss: 1.2342
2022-08-07 14:35:06 - train: epoch 0064, iter [04300, 05004], lr: 0.063313, loss: 1.3634
2022-08-07 14:36:40 - train: epoch 0064, iter [04400, 05004], lr: 0.063251, loss: 1.2805
2022-08-07 14:38:14 - train: epoch 0064, iter [04500, 05004], lr: 0.063190, loss: 1.2004
2022-08-07 14:39:48 - train: epoch 0064, iter [04600, 05004], lr: 0.063128, loss: 1.5434
2022-08-07 14:41:21 - train: epoch 0064, iter [04700, 05004], lr: 0.063067, loss: 1.7332
2022-08-07 14:42:55 - train: epoch 0064, iter [04800, 05004], lr: 0.063005, loss: 1.3017
2022-08-07 14:44:29 - train: epoch 0064, iter [04900, 05004], lr: 0.062944, loss: 1.5486
2022-08-07 14:46:03 - train: epoch 0064, iter [05000, 05004], lr: 0.062883, loss: 1.1343
2022-08-07 14:46:08 - train: epoch 064, train_loss: 1.3988
2022-08-07 14:48:10 - eval: epoch: 064, acc1: 69.784%, acc5: 89.646%, test_loss: 1.2342, per_image_load_time: 2.049ms, per_image_inference_time: 1.233ms
2022-08-07 14:48:10 - until epoch: 064, best_acc1: 69.784%
2022-08-07 14:48:10 - epoch 065 lr: 0.062880
2022-08-07 14:49:55 - train: epoch 0065, iter [00100, 05004], lr: 0.062819, loss: 1.2790
2022-08-07 14:51:29 - train: epoch 0065, iter [00200, 05004], lr: 0.062758, loss: 1.3820
2022-08-07 14:53:02 - train: epoch 0065, iter [00300, 05004], lr: 0.062696, loss: 1.4003
2022-08-07 14:54:36 - train: epoch 0065, iter [00400, 05004], lr: 0.062635, loss: 1.2530
2022-08-07 14:56:10 - train: epoch 0065, iter [00500, 05004], lr: 0.062574, loss: 1.3834
2022-08-07 14:57:43 - train: epoch 0065, iter [00600, 05004], lr: 0.062512, loss: 1.2875
2022-08-07 14:59:17 - train: epoch 0065, iter [00700, 05004], lr: 0.062451, loss: 1.3845
2022-08-07 15:00:51 - train: epoch 0065, iter [00800, 05004], lr: 0.062390, loss: 1.3658
2022-08-07 15:02:25 - train: epoch 0065, iter [00900, 05004], lr: 0.062329, loss: 1.1801
2022-08-07 15:03:58 - train: epoch 0065, iter [01000, 05004], lr: 0.062267, loss: 1.5175
2022-08-07 15:05:32 - train: epoch 0065, iter [01100, 05004], lr: 0.062206, loss: 1.3430
2022-08-07 15:07:06 - train: epoch 0065, iter [01200, 05004], lr: 0.062145, loss: 1.5153
2022-08-07 15:08:40 - train: epoch 0065, iter [01300, 05004], lr: 0.062084, loss: 1.4125
2022-08-07 15:10:13 - train: epoch 0065, iter [01400, 05004], lr: 0.062023, loss: 1.2005
2022-08-07 15:11:47 - train: epoch 0065, iter [01500, 05004], lr: 0.061962, loss: 1.2748
2022-08-07 15:13:21 - train: epoch 0065, iter [01600, 05004], lr: 0.061901, loss: 1.4823
2022-08-07 15:14:55 - train: epoch 0065, iter [01700, 05004], lr: 0.061839, loss: 1.3482
2022-08-07 15:16:29 - train: epoch 0065, iter [01800, 05004], lr: 0.061778, loss: 1.3682
2022-08-07 15:18:02 - train: epoch 0065, iter [01900, 05004], lr: 0.061717, loss: 1.1941
2022-08-07 15:19:36 - train: epoch 0065, iter [02000, 05004], lr: 0.061656, loss: 1.3207
2022-08-07 15:21:10 - train: epoch 0065, iter [02100, 05004], lr: 0.061595, loss: 1.2759
2022-08-07 15:22:44 - train: epoch 0065, iter [02200, 05004], lr: 0.061534, loss: 1.2568
2022-08-07 15:24:18 - train: epoch 0065, iter [02300, 05004], lr: 0.061473, loss: 1.1971
2022-08-07 15:25:51 - train: epoch 0065, iter [02400, 05004], lr: 0.061412, loss: 1.3761
2022-08-07 15:27:25 - train: epoch 0065, iter [02500, 05004], lr: 0.061351, loss: 1.2576
2022-08-07 15:28:59 - train: epoch 0065, iter [02600, 05004], lr: 0.061290, loss: 1.5501
2022-08-07 15:30:33 - train: epoch 0065, iter [02700, 05004], lr: 0.061229, loss: 1.4836
2022-08-07 15:32:07 - train: epoch 0065, iter [02800, 05004], lr: 0.061169, loss: 1.3338
2022-08-07 15:33:41 - train: epoch 0065, iter [02900, 05004], lr: 0.061108, loss: 1.4150
2022-08-07 15:35:14 - train: epoch 0065, iter [03000, 05004], lr: 0.061047, loss: 1.3440
2022-08-07 15:36:48 - train: epoch 0065, iter [03100, 05004], lr: 0.060986, loss: 1.4143
2022-08-07 15:38:22 - train: epoch 0065, iter [03200, 05004], lr: 0.060925, loss: 1.3644
2022-08-07 15:39:56 - train: epoch 0065, iter [03300, 05004], lr: 0.060864, loss: 1.4441
2022-08-07 15:41:29 - train: epoch 0065, iter [03400, 05004], lr: 0.060803, loss: 1.3397
2022-08-07 15:43:03 - train: epoch 0065, iter [03500, 05004], lr: 0.060743, loss: 1.5254
2022-08-07 15:44:37 - train: epoch 0065, iter [03600, 05004], lr: 0.060682, loss: 1.4836
2022-08-07 15:46:11 - train: epoch 0065, iter [03700, 05004], lr: 0.060621, loss: 1.3150
2022-08-07 15:47:44 - train: epoch 0065, iter [03800, 05004], lr: 0.060560, loss: 1.3523
2022-08-07 15:49:18 - train: epoch 0065, iter [03900, 05004], lr: 0.060500, loss: 1.5638
2022-08-07 15:50:52 - train: epoch 0065, iter [04000, 05004], lr: 0.060439, loss: 1.4475
2022-08-07 15:52:26 - train: epoch 0065, iter [04100, 05004], lr: 0.060378, loss: 1.4474
2022-08-07 15:53:59 - train: epoch 0065, iter [04200, 05004], lr: 0.060318, loss: 1.5053
2022-08-07 15:55:33 - train: epoch 0065, iter [04300, 05004], lr: 0.060257, loss: 1.3609
2022-08-07 15:57:07 - train: epoch 0065, iter [04400, 05004], lr: 0.060196, loss: 1.3182
2022-08-07 15:58:41 - train: epoch 0065, iter [04500, 05004], lr: 0.060136, loss: 1.4613
2022-08-07 16:00:15 - train: epoch 0065, iter [04600, 05004], lr: 0.060075, loss: 1.5272
2022-08-07 16:01:49 - train: epoch 0065, iter [04700, 05004], lr: 0.060015, loss: 1.4239
2022-08-07 16:03:23 - train: epoch 0065, iter [04800, 05004], lr: 0.059954, loss: 1.1222
2022-08-07 16:04:57 - train: epoch 0065, iter [04900, 05004], lr: 0.059893, loss: 1.5972
2022-08-07 16:06:30 - train: epoch 0065, iter [05000, 05004], lr: 0.059833, loss: 1.3779
2022-08-07 16:06:35 - train: epoch 065, train_loss: 1.3827
2022-08-07 16:08:38 - eval: epoch: 065, acc1: 69.986%, acc5: 89.880%, test_loss: 1.2179, per_image_load_time: 3.482ms, per_image_inference_time: 1.203ms
2022-08-07 16:08:38 - until epoch: 065, best_acc1: 69.986%
2022-08-07 16:08:38 - epoch 066 lr: 0.059830
2022-08-07 16:10:22 - train: epoch 0066, iter [00100, 05004], lr: 0.059770, loss: 1.2034
2022-08-07 16:11:56 - train: epoch 0066, iter [00200, 05004], lr: 0.059709, loss: 1.3264
2022-08-07 16:13:30 - train: epoch 0066, iter [00300, 05004], lr: 0.059649, loss: 1.3936
2022-08-07 16:15:05 - train: epoch 0066, iter [00400, 05004], lr: 0.059589, loss: 1.1763
2022-08-07 16:16:39 - train: epoch 0066, iter [00500, 05004], lr: 0.059528, loss: 1.3428
2022-08-07 16:18:12 - train: epoch 0066, iter [00600, 05004], lr: 0.059468, loss: 1.2496
2022-08-07 16:19:46 - train: epoch 0066, iter [00700, 05004], lr: 0.059407, loss: 1.4175
2022-08-07 16:21:20 - train: epoch 0066, iter [00800, 05004], lr: 0.059347, loss: 1.4771
2022-08-07 16:22:54 - train: epoch 0066, iter [00900, 05004], lr: 0.059286, loss: 1.3714
2022-08-07 16:24:28 - train: epoch 0066, iter [01000, 05004], lr: 0.059226, loss: 1.3951
2022-08-07 16:26:02 - train: epoch 0066, iter [01100, 05004], lr: 0.059166, loss: 1.4747
2022-08-07 16:27:36 - train: epoch 0066, iter [01200, 05004], lr: 0.059105, loss: 1.5851
2022-08-07 16:29:10 - train: epoch 0066, iter [01300, 05004], lr: 0.059045, loss: 1.4988
2022-08-07 16:30:44 - train: epoch 0066, iter [01400, 05004], lr: 0.058985, loss: 1.3019
2022-08-07 16:32:18 - train: epoch 0066, iter [01500, 05004], lr: 0.058925, loss: 1.2833
2022-08-07 16:33:52 - train: epoch 0066, iter [01600, 05004], lr: 0.058864, loss: 1.5791
2022-08-07 16:35:26 - train: epoch 0066, iter [01700, 05004], lr: 0.058804, loss: 1.5586
2022-08-07 16:37:00 - train: epoch 0066, iter [01800, 05004], lr: 0.058744, loss: 1.4099
2022-08-07 16:38:33 - train: epoch 0066, iter [01900, 05004], lr: 0.058684, loss: 1.6290
2022-08-07 16:40:07 - train: epoch 0066, iter [02000, 05004], lr: 0.058624, loss: 1.3194
2022-08-07 16:41:41 - train: epoch 0066, iter [02100, 05004], lr: 0.058563, loss: 1.3245
2022-08-07 16:43:15 - train: epoch 0066, iter [02200, 05004], lr: 0.058503, loss: 1.3091
2022-08-07 16:44:49 - train: epoch 0066, iter [02300, 05004], lr: 0.058443, loss: 1.3038
2022-08-07 16:46:23 - train: epoch 0066, iter [02400, 05004], lr: 0.058383, loss: 1.2661
2022-08-07 16:47:57 - train: epoch 0066, iter [02500, 05004], lr: 0.058323, loss: 1.2426
2022-08-07 16:49:31 - train: epoch 0066, iter [02600, 05004], lr: 0.058263, loss: 1.3168
2022-08-07 16:51:05 - train: epoch 0066, iter [02700, 05004], lr: 0.058203, loss: 1.6659
2022-08-07 16:52:39 - train: epoch 0066, iter [02800, 05004], lr: 0.058143, loss: 1.3592
2022-08-07 16:54:12 - train: epoch 0066, iter [02900, 05004], lr: 0.058083, loss: 1.2046
2022-08-07 16:55:46 - train: epoch 0066, iter [03000, 05004], lr: 0.058023, loss: 1.2898
2022-08-07 16:57:20 - train: epoch 0066, iter [03100, 05004], lr: 0.057963, loss: 1.4893
2022-08-07 16:58:54 - train: epoch 0066, iter [03200, 05004], lr: 0.057903, loss: 1.3735
2022-08-07 17:00:28 - train: epoch 0066, iter [03300, 05004], lr: 0.057843, loss: 1.1256
2022-08-07 17:02:02 - train: epoch 0066, iter [03400, 05004], lr: 0.057783, loss: 1.4619
2022-08-07 17:03:35 - train: epoch 0066, iter [03500, 05004], lr: 0.057723, loss: 1.4665
2022-08-07 17:05:09 - train: epoch 0066, iter [03600, 05004], lr: 0.057663, loss: 1.4379
2022-08-07 17:06:43 - train: epoch 0066, iter [03700, 05004], lr: 0.057603, loss: 1.5407
2022-08-07 17:08:17 - train: epoch 0066, iter [03800, 05004], lr: 0.057544, loss: 1.5539
2022-08-07 17:09:51 - train: epoch 0066, iter [03900, 05004], lr: 0.057484, loss: 1.3178
2022-08-07 17:11:24 - train: epoch 0066, iter [04000, 05004], lr: 0.057424, loss: 1.4250
2022-08-07 17:12:58 - train: epoch 0066, iter [04100, 05004], lr: 0.057364, loss: 1.3653
2022-08-07 17:14:32 - train: epoch 0066, iter [04200, 05004], lr: 0.057304, loss: 1.2623
2022-08-07 17:16:06 - train: epoch 0066, iter [04300, 05004], lr: 0.057245, loss: 1.3799
2022-08-07 17:17:40 - train: epoch 0066, iter [04400, 05004], lr: 0.057185, loss: 1.4000
2022-08-07 17:19:14 - train: epoch 0066, iter [04500, 05004], lr: 0.057125, loss: 1.4752
2022-08-07 17:20:48 - train: epoch 0066, iter [04600, 05004], lr: 0.057066, loss: 1.5003
2022-08-07 17:22:21 - train: epoch 0066, iter [04700, 05004], lr: 0.057006, loss: 1.3373
2022-08-07 17:23:55 - train: epoch 0066, iter [04800, 05004], lr: 0.056946, loss: 1.4336
2022-08-07 17:25:29 - train: epoch 0066, iter [04900, 05004], lr: 0.056887, loss: 1.2154
2022-08-07 17:27:03 - train: epoch 0066, iter [05000, 05004], lr: 0.056827, loss: 1.2034
2022-08-07 17:27:07 - train: epoch 066, train_loss: 1.3643
2022-08-07 17:29:07 - eval: epoch: 066, acc1: 70.504%, acc5: 90.108%, test_loss: 1.1886, per_image_load_time: 3.096ms, per_image_inference_time: 1.227ms
2022-08-07 17:29:08 - until epoch: 066, best_acc1: 70.504%
2022-08-07 17:29:08 - epoch 067 lr: 0.056824
2022-08-07 17:30:50 - train: epoch 0067, iter [00100, 05004], lr: 0.056765, loss: 1.3345
2022-08-07 17:32:24 - train: epoch 0067, iter [00200, 05004], lr: 0.056705, loss: 1.4865
2022-08-07 17:33:57 - train: epoch 0067, iter [00300, 05004], lr: 0.056646, loss: 1.2165
2022-08-07 17:35:31 - train: epoch 0067, iter [00400, 05004], lr: 0.056586, loss: 1.2401
2022-08-07 17:37:04 - train: epoch 0067, iter [00500, 05004], lr: 0.056527, loss: 1.3474
2022-08-07 17:38:38 - train: epoch 0067, iter [00600, 05004], lr: 0.056467, loss: 1.1613
2022-08-07 17:40:11 - train: epoch 0067, iter [00700, 05004], lr: 0.056408, loss: 1.3203
2022-08-07 17:41:45 - train: epoch 0067, iter [00800, 05004], lr: 0.056348, loss: 1.3727
2022-08-07 17:43:18 - train: epoch 0067, iter [00900, 05004], lr: 0.056289, loss: 1.3580
2022-08-07 17:44:52 - train: epoch 0067, iter [01000, 05004], lr: 0.056229, loss: 1.2251
2022-08-07 17:46:25 - train: epoch 0067, iter [01100, 05004], lr: 0.056170, loss: 1.2305
2022-08-07 17:47:59 - train: epoch 0067, iter [01200, 05004], lr: 0.056111, loss: 1.3640
2022-08-07 17:49:32 - train: epoch 0067, iter [01300, 05004], lr: 0.056051, loss: 1.4772
2022-08-07 17:51:06 - train: epoch 0067, iter [01400, 05004], lr: 0.055992, loss: 1.5054
2022-08-07 17:52:39 - train: epoch 0067, iter [01500, 05004], lr: 0.055933, loss: 1.2220
2022-08-07 17:54:13 - train: epoch 0067, iter [01600, 05004], lr: 0.055873, loss: 1.4005
2022-08-07 17:55:46 - train: epoch 0067, iter [01700, 05004], lr: 0.055814, loss: 1.1609
2022-08-07 17:57:20 - train: epoch 0067, iter [01800, 05004], lr: 0.055755, loss: 1.4856
2022-08-07 17:58:54 - train: epoch 0067, iter [01900, 05004], lr: 0.055695, loss: 1.6352
2022-08-07 18:00:28 - train: epoch 0067, iter [02000, 05004], lr: 0.055636, loss: 1.3708
2022-08-07 18:02:01 - train: epoch 0067, iter [02100, 05004], lr: 0.055577, loss: 1.1824
2022-08-07 18:03:35 - train: epoch 0067, iter [02200, 05004], lr: 0.055518, loss: 1.2429
2022-08-07 18:05:09 - train: epoch 0067, iter [02300, 05004], lr: 0.055459, loss: 1.2754
2022-08-07 18:06:43 - train: epoch 0067, iter [02400, 05004], lr: 0.055399, loss: 1.4117
2022-08-07 18:08:16 - train: epoch 0067, iter [02500, 05004], lr: 0.055340, loss: 1.3860
2022-08-07 18:09:50 - train: epoch 0067, iter [02600, 05004], lr: 0.055281, loss: 1.4299
2022-08-07 18:11:24 - train: epoch 0067, iter [02700, 05004], lr: 0.055222, loss: 1.3293
2022-08-07 18:12:57 - train: epoch 0067, iter [02800, 05004], lr: 0.055163, loss: 1.5908
2022-08-07 18:14:31 - train: epoch 0067, iter [02900, 05004], lr: 0.055104, loss: 1.4108
2022-08-07 18:16:04 - train: epoch 0067, iter [03000, 05004], lr: 0.055045, loss: 1.3590
2022-08-07 18:17:38 - train: epoch 0067, iter [03100, 05004], lr: 0.054986, loss: 1.1944
2022-08-07 18:19:11 - train: epoch 0067, iter [03200, 05004], lr: 0.054927, loss: 1.5000
2022-08-07 18:20:45 - train: epoch 0067, iter [03300, 05004], lr: 0.054868, loss: 1.3066
2022-08-07 18:22:19 - train: epoch 0067, iter [03400, 05004], lr: 0.054809, loss: 1.1809
2022-08-07 18:23:52 - train: epoch 0067, iter [03500, 05004], lr: 0.054750, loss: 1.2456
2022-08-07 18:25:26 - train: epoch 0067, iter [03600, 05004], lr: 0.054691, loss: 1.4094
2022-08-07 18:26:59 - train: epoch 0067, iter [03700, 05004], lr: 0.054632, loss: 1.3213
2022-08-07 18:28:33 - train: epoch 0067, iter [03800, 05004], lr: 0.054573, loss: 1.4396
2022-08-07 18:30:07 - train: epoch 0067, iter [03900, 05004], lr: 0.054514, loss: 1.4750
2022-08-07 18:31:41 - train: epoch 0067, iter [04000, 05004], lr: 0.054456, loss: 1.2893
2022-08-07 18:33:15 - train: epoch 0067, iter [04100, 05004], lr: 0.054397, loss: 1.5108
2022-08-07 18:34:48 - train: epoch 0067, iter [04200, 05004], lr: 0.054338, loss: 1.3229
2022-08-07 18:36:22 - train: epoch 0067, iter [04300, 05004], lr: 0.054279, loss: 1.2851
2022-08-07 18:37:56 - train: epoch 0067, iter [04400, 05004], lr: 0.054220, loss: 1.1749
2022-08-07 18:39:30 - train: epoch 0067, iter [04500, 05004], lr: 0.054162, loss: 1.2451
2022-08-07 18:41:04 - train: epoch 0067, iter [04600, 05004], lr: 0.054103, loss: 1.1821
2022-08-07 18:42:37 - train: epoch 0067, iter [04700, 05004], lr: 0.054044, loss: 1.3075
2022-08-07 18:44:11 - train: epoch 0067, iter [04800, 05004], lr: 0.053986, loss: 1.2167
2022-08-07 18:45:45 - train: epoch 0067, iter [04900, 05004], lr: 0.053927, loss: 1.3093
2022-08-07 18:47:19 - train: epoch 0067, iter [05000, 05004], lr: 0.053868, loss: 1.4182
2022-08-07 18:47:24 - train: epoch 067, train_loss: 1.3470
2022-08-07 18:49:25 - eval: epoch: 067, acc1: 70.306%, acc5: 90.230%, test_loss: 1.1951, per_image_load_time: 2.853ms, per_image_inference_time: 1.267ms
2022-08-07 18:49:25 - until epoch: 067, best_acc1: 70.504%
2022-08-07 18:49:25 - epoch 068 lr: 0.053865
2022-08-07 18:51:08 - train: epoch 0068, iter [00100, 05004], lr: 0.053807, loss: 1.2901
2022-08-07 18:52:42 - train: epoch 0068, iter [00200, 05004], lr: 0.053749, loss: 1.2094
2022-08-07 18:54:15 - train: epoch 0068, iter [00300, 05004], lr: 0.053690, loss: 1.2878
2022-08-07 18:55:48 - train: epoch 0068, iter [00400, 05004], lr: 0.053632, loss: 1.1975
2022-08-07 18:57:22 - train: epoch 0068, iter [00500, 05004], lr: 0.053573, loss: 1.2408
2022-08-07 18:58:55 - train: epoch 0068, iter [00600, 05004], lr: 0.053514, loss: 1.3718
2022-08-07 19:00:29 - train: epoch 0068, iter [00700, 05004], lr: 0.053456, loss: 1.2970
2022-08-07 19:02:02 - train: epoch 0068, iter [00800, 05004], lr: 0.053397, loss: 1.2404
2022-08-07 19:03:36 - train: epoch 0068, iter [00900, 05004], lr: 0.053339, loss: 1.3921
2022-08-07 19:05:10 - train: epoch 0068, iter [01000, 05004], lr: 0.053281, loss: 1.1827
2022-08-07 19:06:43 - train: epoch 0068, iter [01100, 05004], lr: 0.053222, loss: 1.5690
2022-08-07 19:08:16 - train: epoch 0068, iter [01200, 05004], lr: 0.053164, loss: 1.2657
2022-08-07 19:09:50 - train: epoch 0068, iter [01300, 05004], lr: 0.053105, loss: 1.3807
2022-08-07 19:11:24 - train: epoch 0068, iter [01400, 05004], lr: 0.053047, loss: 1.3233
2022-08-07 19:12:57 - train: epoch 0068, iter [01500, 05004], lr: 0.052989, loss: 1.4744
2022-08-07 19:14:31 - train: epoch 0068, iter [01600, 05004], lr: 0.052930, loss: 1.4036
2022-08-07 19:16:04 - train: epoch 0068, iter [01700, 05004], lr: 0.052872, loss: 1.3579
2022-08-07 19:17:38 - train: epoch 0068, iter [01800, 05004], lr: 0.052814, loss: 1.3907
2022-08-07 19:19:11 - train: epoch 0068, iter [01900, 05004], lr: 0.052756, loss: 1.3088
2022-08-07 19:20:45 - train: epoch 0068, iter [02000, 05004], lr: 0.052697, loss: 1.4319
2022-08-07 19:22:18 - train: epoch 0068, iter [02100, 05004], lr: 0.052639, loss: 1.2706
2022-08-07 19:23:52 - train: epoch 0068, iter [02200, 05004], lr: 0.052581, loss: 1.2862
2022-08-07 19:25:25 - train: epoch 0068, iter [02300, 05004], lr: 0.052523, loss: 1.2891
2022-08-07 19:26:59 - train: epoch 0068, iter [02400, 05004], lr: 0.052465, loss: 1.3338
2022-08-07 19:28:32 - train: epoch 0068, iter [02500, 05004], lr: 0.052406, loss: 1.3789
2022-08-07 19:30:06 - train: epoch 0068, iter [02600, 05004], lr: 0.052348, loss: 1.2608
2022-08-07 19:31:39 - train: epoch 0068, iter [02700, 05004], lr: 0.052290, loss: 1.2763
2022-08-07 19:33:13 - train: epoch 0068, iter [02800, 05004], lr: 0.052232, loss: 1.3805
2022-08-07 19:34:47 - train: epoch 0068, iter [02900, 05004], lr: 0.052174, loss: 1.3449
2022-08-07 19:36:20 - train: epoch 0068, iter [03000, 05004], lr: 0.052116, loss: 1.5346
2022-08-07 19:37:54 - train: epoch 0068, iter [03100, 05004], lr: 0.052058, loss: 1.1046
2022-08-07 19:39:27 - train: epoch 0068, iter [03200, 05004], lr: 0.052000, loss: 1.4370
2022-08-07 19:41:01 - train: epoch 0068, iter [03300, 05004], lr: 0.051942, loss: 1.1413
2022-08-07 19:42:34 - train: epoch 0068, iter [03400, 05004], lr: 0.051884, loss: 1.2106
2022-08-07 19:44:08 - train: epoch 0068, iter [03500, 05004], lr: 0.051826, loss: 1.5379
2022-08-07 19:45:42 - train: epoch 0068, iter [03600, 05004], lr: 0.051768, loss: 1.3827
2022-08-07 19:47:15 - train: epoch 0068, iter [03700, 05004], lr: 0.051710, loss: 1.3660
2022-08-07 19:48:49 - train: epoch 0068, iter [03800, 05004], lr: 0.051653, loss: 1.4546
2022-08-07 19:50:22 - train: epoch 0068, iter [03900, 05004], lr: 0.051595, loss: 1.3940
2022-08-07 19:51:56 - train: epoch 0068, iter [04000, 05004], lr: 0.051537, loss: 1.4705
2022-08-07 19:53:29 - train: epoch 0068, iter [04100, 05004], lr: 0.051479, loss: 1.1826
2022-08-07 19:55:03 - train: epoch 0068, iter [04200, 05004], lr: 0.051421, loss: 1.4461
2022-08-07 19:56:37 - train: epoch 0068, iter [04300, 05004], lr: 0.051364, loss: 1.4808
2022-08-07 19:58:10 - train: epoch 0068, iter [04400, 05004], lr: 0.051306, loss: 1.3959
2022-08-07 19:59:44 - train: epoch 0068, iter [04500, 05004], lr: 0.051248, loss: 1.2207
2022-08-07 20:01:17 - train: epoch 0068, iter [04600, 05004], lr: 0.051190, loss: 1.4425
2022-08-07 20:02:51 - train: epoch 0068, iter [04700, 05004], lr: 0.051133, loss: 1.5338
2022-08-07 20:04:24 - train: epoch 0068, iter [04800, 05004], lr: 0.051075, loss: 1.2272
2022-08-07 20:05:58 - train: epoch 0068, iter [04900, 05004], lr: 0.051018, loss: 1.2796
2022-08-07 20:07:32 - train: epoch 0068, iter [05000, 05004], lr: 0.050960, loss: 1.4211
2022-08-07 20:07:36 - train: epoch 068, train_loss: 1.3294
2022-08-07 20:09:37 - eval: epoch: 068, acc1: 70.694%, acc5: 90.464%, test_loss: 1.1763, per_image_load_time: 3.301ms, per_image_inference_time: 1.268ms
2022-08-07 20:09:37 - until epoch: 068, best_acc1: 70.694%
2022-08-07 20:09:37 - epoch 069 lr: 0.050957
2022-08-07 20:11:19 - train: epoch 0069, iter [00100, 05004], lr: 0.050900, loss: 1.5366
2022-08-07 20:12:53 - train: epoch 0069, iter [00200, 05004], lr: 0.050843, loss: 1.2708
2022-08-07 20:14:26 - train: epoch 0069, iter [00300, 05004], lr: 0.050785, loss: 1.3091
2022-08-07 20:16:00 - train: epoch 0069, iter [00400, 05004], lr: 0.050727, loss: 1.4010
2022-08-07 20:17:33 - train: epoch 0069, iter [00500, 05004], lr: 0.050670, loss: 1.2914
2022-08-07 20:19:06 - train: epoch 0069, iter [00600, 05004], lr: 0.050612, loss: 1.0756
2022-08-07 20:20:40 - train: epoch 0069, iter [00700, 05004], lr: 0.050555, loss: 1.3519
2022-08-07 20:22:14 - train: epoch 0069, iter [00800, 05004], lr: 0.050498, loss: 1.3503
2022-08-07 20:23:47 - train: epoch 0069, iter [00900, 05004], lr: 0.050440, loss: 1.1194
2022-08-07 20:25:21 - train: epoch 0069, iter [01000, 05004], lr: 0.050383, loss: 1.4128
2022-08-07 20:26:54 - train: epoch 0069, iter [01100, 05004], lr: 0.050325, loss: 1.2058
2022-08-07 20:28:28 - train: epoch 0069, iter [01200, 05004], lr: 0.050268, loss: 1.1906
2022-08-07 20:30:01 - train: epoch 0069, iter [01300, 05004], lr: 0.050211, loss: 1.3430
2022-08-07 20:31:35 - train: epoch 0069, iter [01400, 05004], lr: 0.050153, loss: 1.2322
2022-08-07 20:33:08 - train: epoch 0069, iter [01500, 05004], lr: 0.050096, loss: 1.2781
2022-08-07 20:34:42 - train: epoch 0069, iter [01600, 05004], lr: 0.050039, loss: 1.2143
2022-08-07 20:36:15 - train: epoch 0069, iter [01700, 05004], lr: 0.049982, loss: 1.2814
2022-08-07 20:37:49 - train: epoch 0069, iter [01800, 05004], lr: 0.049924, loss: 1.2626
2022-08-07 20:39:23 - train: epoch 0069, iter [01900, 05004], lr: 0.049867, loss: 1.4501
2022-08-07 20:40:56 - train: epoch 0069, iter [02000, 05004], lr: 0.049810, loss: 1.1843
2022-08-07 20:42:30 - train: epoch 0069, iter [02100, 05004], lr: 0.049753, loss: 1.2287
2022-08-07 20:44:04 - train: epoch 0069, iter [02200, 05004], lr: 0.049696, loss: 1.5111
2022-08-07 20:45:37 - train: epoch 0069, iter [02300, 05004], lr: 0.049639, loss: 1.2552
2022-08-07 20:47:11 - train: epoch 0069, iter [02400, 05004], lr: 0.049582, loss: 1.4529
2022-08-07 20:48:44 - train: epoch 0069, iter [02500, 05004], lr: 0.049525, loss: 1.2965
2022-08-07 20:50:18 - train: epoch 0069, iter [02600, 05004], lr: 0.049468, loss: 1.3289
2022-08-07 20:51:51 - train: epoch 0069, iter [02700, 05004], lr: 0.049411, loss: 1.5493
2022-08-07 20:53:25 - train: epoch 0069, iter [02800, 05004], lr: 0.049354, loss: 1.3990
2022-08-07 20:54:59 - train: epoch 0069, iter [02900, 05004], lr: 0.049297, loss: 1.4270
2022-08-07 20:56:32 - train: epoch 0069, iter [03000, 05004], lr: 0.049240, loss: 1.3651
2022-08-07 20:58:05 - train: epoch 0069, iter [03100, 05004], lr: 0.049183, loss: 1.4114
2022-08-07 20:59:39 - train: epoch 0069, iter [03200, 05004], lr: 0.049126, loss: 1.2201
2022-08-07 21:01:13 - train: epoch 0069, iter [03300, 05004], lr: 0.049069, loss: 1.1931
2022-08-07 21:02:46 - train: epoch 0069, iter [03400, 05004], lr: 0.049012, loss: 1.2436
2022-08-07 21:04:20 - train: epoch 0069, iter [03500, 05004], lr: 0.048955, loss: 1.2681
2022-08-07 21:05:53 - train: epoch 0069, iter [03600, 05004], lr: 0.048898, loss: 1.0831
2022-08-07 21:07:27 - train: epoch 0069, iter [03700, 05004], lr: 0.048842, loss: 1.2733
2022-08-07 21:09:00 - train: epoch 0069, iter [03800, 05004], lr: 0.048785, loss: 1.3034
2022-08-07 21:10:34 - train: epoch 0069, iter [03900, 05004], lr: 0.048728, loss: 1.5045
2022-08-07 21:12:08 - train: epoch 0069, iter [04000, 05004], lr: 0.048671, loss: 1.5315
2022-08-07 21:13:41 - train: epoch 0069, iter [04100, 05004], lr: 0.048615, loss: 1.4099
2022-08-07 21:15:15 - train: epoch 0069, iter [04200, 05004], lr: 0.048558, loss: 1.2074
2022-08-07 21:16:49 - train: epoch 0069, iter [04300, 05004], lr: 0.048501, loss: 1.3282
2022-08-07 21:18:22 - train: epoch 0069, iter [04400, 05004], lr: 0.048445, loss: 1.5041
2022-08-07 21:19:56 - train: epoch 0069, iter [04500, 05004], lr: 0.048388, loss: 1.4134
2022-08-07 21:21:29 - train: epoch 0069, iter [04600, 05004], lr: 0.048331, loss: 1.4110
2022-08-07 21:23:03 - train: epoch 0069, iter [04700, 05004], lr: 0.048275, loss: 1.2414
2022-08-07 21:24:37 - train: epoch 0069, iter [04800, 05004], lr: 0.048218, loss: 1.5762
2022-08-07 21:26:10 - train: epoch 0069, iter [04900, 05004], lr: 0.048162, loss: 1.3420
2022-08-07 21:27:44 - train: epoch 0069, iter [05000, 05004], lr: 0.048105, loss: 1.3750
2022-08-07 21:27:48 - train: epoch 069, train_loss: 1.3090
2022-08-07 21:29:51 - eval: epoch: 069, acc1: 71.264%, acc5: 90.634%, test_loss: 1.1563, per_image_load_time: 3.496ms, per_image_inference_time: 1.239ms
2022-08-07 21:29:52 - until epoch: 069, best_acc1: 71.264%
2022-08-07 21:29:52 - epoch 070 lr: 0.048102
2022-08-07 21:31:34 - train: epoch 0070, iter [00100, 05004], lr: 0.048047, loss: 1.1380
2022-08-07 21:33:08 - train: epoch 0070, iter [00200, 05004], lr: 0.047990, loss: 1.2255
2022-08-07 21:34:41 - train: epoch 0070, iter [00300, 05004], lr: 0.047934, loss: 1.5318
2022-08-07 21:36:15 - train: epoch 0070, iter [00400, 05004], lr: 0.047877, loss: 1.2261
2022-08-07 21:37:49 - train: epoch 0070, iter [00500, 05004], lr: 0.047821, loss: 1.1853
2022-08-07 21:39:22 - train: epoch 0070, iter [00600, 05004], lr: 0.047765, loss: 1.1655
2022-08-07 21:40:56 - train: epoch 0070, iter [00700, 05004], lr: 0.047708, loss: 1.3173
2022-08-07 21:42:29 - train: epoch 0070, iter [00800, 05004], lr: 0.047652, loss: 1.2803
2022-08-07 21:44:03 - train: epoch 0070, iter [00900, 05004], lr: 0.047596, loss: 1.0901
2022-08-07 21:45:36 - train: epoch 0070, iter [01000, 05004], lr: 0.047539, loss: 1.3112
2022-08-07 21:47:10 - train: epoch 0070, iter [01100, 05004], lr: 0.047483, loss: 1.5479
2022-08-07 21:48:44 - train: epoch 0070, iter [01200, 05004], lr: 0.047427, loss: 1.1243
2022-08-07 21:50:17 - train: epoch 0070, iter [01300, 05004], lr: 0.047371, loss: 1.4438
2022-08-07 21:51:51 - train: epoch 0070, iter [01400, 05004], lr: 0.047314, loss: 1.3479
2022-08-07 21:53:25 - train: epoch 0070, iter [01500, 05004], lr: 0.047258, loss: 1.3236
2022-08-07 21:54:58 - train: epoch 0070, iter [01600, 05004], lr: 0.047202, loss: 1.2515
2022-08-07 21:56:32 - train: epoch 0070, iter [01700, 05004], lr: 0.047146, loss: 1.2019
2022-08-07 21:58:06 - train: epoch 0070, iter [01800, 05004], lr: 0.047090, loss: 1.2804
2022-08-07 21:59:39 - train: epoch 0070, iter [01900, 05004], lr: 0.047034, loss: 1.3704
2022-08-07 22:01:13 - train: epoch 0070, iter [02000, 05004], lr: 0.046978, loss: 1.1462
2022-08-07 22:02:47 - train: epoch 0070, iter [02100, 05004], lr: 0.046922, loss: 1.3529
2022-08-07 22:04:21 - train: epoch 0070, iter [02200, 05004], lr: 0.046866, loss: 1.1396
2022-08-07 22:05:54 - train: epoch 0070, iter [02300, 05004], lr: 0.046810, loss: 1.3954
2022-08-07 22:07:28 - train: epoch 0070, iter [02400, 05004], lr: 0.046754, loss: 1.4023
2022-08-07 22:09:02 - train: epoch 0070, iter [02500, 05004], lr: 0.046698, loss: 1.3020
2022-08-07 22:10:35 - train: epoch 0070, iter [02600, 05004], lr: 0.046642, loss: 1.1325
2022-08-07 22:12:09 - train: epoch 0070, iter [02700, 05004], lr: 0.046586, loss: 1.4221
2022-08-07 22:13:42 - train: epoch 0070, iter [02800, 05004], lr: 0.046530, loss: 1.3916
2022-08-07 22:15:16 - train: epoch 0070, iter [02900, 05004], lr: 0.046474, loss: 1.3363
2022-08-07 22:16:49 - train: epoch 0070, iter [03000, 05004], lr: 0.046419, loss: 1.2731
2022-08-07 22:18:23 - train: epoch 0070, iter [03100, 05004], lr: 0.046363, loss: 1.2502
2022-08-07 22:19:56 - train: epoch 0070, iter [03200, 05004], lr: 0.046307, loss: 1.4337
2022-08-07 22:21:30 - train: epoch 0070, iter [03300, 05004], lr: 0.046251, loss: 1.2351
2022-08-07 22:23:04 - train: epoch 0070, iter [03400, 05004], lr: 0.046196, loss: 1.2958
2022-08-07 22:24:37 - train: epoch 0070, iter [03500, 05004], lr: 0.046140, loss: 1.2245
2022-08-07 22:26:11 - train: epoch 0070, iter [03600, 05004], lr: 0.046084, loss: 1.4411
2022-08-07 22:27:44 - train: epoch 0070, iter [03700, 05004], lr: 0.046029, loss: 1.3429
2022-08-07 22:29:18 - train: epoch 0070, iter [03800, 05004], lr: 0.045973, loss: 1.2411
2022-08-07 22:30:52 - train: epoch 0070, iter [03900, 05004], lr: 0.045917, loss: 1.2294
2022-08-07 22:32:25 - train: epoch 0070, iter [04000, 05004], lr: 0.045862, loss: 1.2685
2022-08-07 22:33:59 - train: epoch 0070, iter [04100, 05004], lr: 0.045806, loss: 1.2215
2022-08-07 22:35:33 - train: epoch 0070, iter [04200, 05004], lr: 0.045751, loss: 1.3271
2022-08-07 22:37:06 - train: epoch 0070, iter [04300, 05004], lr: 0.045695, loss: 1.3136
2022-08-07 22:38:40 - train: epoch 0070, iter [04400, 05004], lr: 0.045640, loss: 1.3097
2022-08-07 22:40:14 - train: epoch 0070, iter [04500, 05004], lr: 0.045584, loss: 1.2566
2022-08-07 22:41:47 - train: epoch 0070, iter [04600, 05004], lr: 0.045529, loss: 1.3206
2022-08-07 22:43:21 - train: epoch 0070, iter [04700, 05004], lr: 0.045473, loss: 1.2852
2022-08-07 22:44:55 - train: epoch 0070, iter [04800, 05004], lr: 0.045418, loss: 1.3971
2022-08-07 22:46:29 - train: epoch 0070, iter [04900, 05004], lr: 0.045363, loss: 1.4889
2022-08-07 22:48:02 - train: epoch 0070, iter [05000, 05004], lr: 0.045307, loss: 1.1658
2022-08-07 22:48:07 - train: epoch 070, train_loss: 1.2873
2022-08-07 22:50:11 - eval: epoch: 070, acc1: 71.532%, acc5: 90.684%, test_loss: 1.1534, per_image_load_time: 3.534ms, per_image_inference_time: 1.251ms
2022-08-07 22:50:12 - until epoch: 070, best_acc1: 71.532%
2022-08-07 22:50:12 - epoch 071 lr: 0.045305
2022-08-07 22:51:54 - train: epoch 0071, iter [00100, 05004], lr: 0.045250, loss: 1.2716
2022-08-07 22:53:28 - train: epoch 0071, iter [00200, 05004], lr: 0.045195, loss: 1.2067
2022-08-07 22:55:03 - train: epoch 0071, iter [00300, 05004], lr: 0.045139, loss: 1.3999
2022-08-07 22:56:37 - train: epoch 0071, iter [00400, 05004], lr: 0.045084, loss: 1.2803
2022-08-07 22:58:11 - train: epoch 0071, iter [00500, 05004], lr: 0.045029, loss: 1.3698
2022-08-07 22:59:45 - train: epoch 0071, iter [00600, 05004], lr: 0.044974, loss: 1.3751
2022-08-07 23:01:19 - train: epoch 0071, iter [00700, 05004], lr: 0.044918, loss: 1.2446
2022-08-07 23:02:54 - train: epoch 0071, iter [00800, 05004], lr: 0.044863, loss: 1.1888
2022-08-07 23:04:28 - train: epoch 0071, iter [00900, 05004], lr: 0.044808, loss: 1.2559
2022-08-07 23:06:02 - train: epoch 0071, iter [01000, 05004], lr: 0.044753, loss: 1.4516
2022-08-07 23:07:36 - train: epoch 0071, iter [01100, 05004], lr: 0.044698, loss: 1.3988
2022-08-07 23:09:10 - train: epoch 0071, iter [01200, 05004], lr: 0.044643, loss: 1.1430
2022-08-07 23:10:45 - train: epoch 0071, iter [01300, 05004], lr: 0.044588, loss: 1.1692
2022-08-07 23:12:19 - train: epoch 0071, iter [01400, 05004], lr: 0.044533, loss: 1.2803
2022-08-07 23:13:53 - train: epoch 0071, iter [01500, 05004], lr: 0.044478, loss: 1.1192
2022-08-07 23:15:27 - train: epoch 0071, iter [01600, 05004], lr: 0.044423, loss: 1.0218
2022-08-07 23:17:01 - train: epoch 0071, iter [01700, 05004], lr: 0.044368, loss: 1.2819
2022-08-07 23:18:34 - train: epoch 0071, iter [01800, 05004], lr: 0.044313, loss: 1.4034
2022-08-07 23:20:08 - train: epoch 0071, iter [01900, 05004], lr: 0.044258, loss: 1.2995
2022-08-07 23:21:42 - train: epoch 0071, iter [02000, 05004], lr: 0.044203, loss: 1.3062
2022-08-07 23:23:16 - train: epoch 0071, iter [02100, 05004], lr: 0.044149, loss: 1.2312
2022-08-07 23:24:50 - train: epoch 0071, iter [02200, 05004], lr: 0.044094, loss: 1.0383
2022-08-07 23:26:24 - train: epoch 0071, iter [02300, 05004], lr: 0.044039, loss: 1.1632
2022-08-07 23:27:57 - train: epoch 0071, iter [02400, 05004], lr: 0.043984, loss: 1.1456
2022-08-07 23:29:31 - train: epoch 0071, iter [02500, 05004], lr: 0.043930, loss: 1.4440
2022-08-07 23:31:05 - train: epoch 0071, iter [02600, 05004], lr: 0.043875, loss: 1.2065
2022-08-07 23:32:38 - train: epoch 0071, iter [02700, 05004], lr: 0.043820, loss: 1.2780
2022-08-07 23:34:12 - train: epoch 0071, iter [02800, 05004], lr: 0.043766, loss: 1.4305
2022-08-07 23:35:46 - train: epoch 0071, iter [02900, 05004], lr: 0.043711, loss: 1.1719
2022-08-07 23:37:20 - train: epoch 0071, iter [03000, 05004], lr: 0.043656, loss: 1.2863
2022-08-07 23:38:54 - train: epoch 0071, iter [03100, 05004], lr: 0.043602, loss: 1.1886
2022-08-07 23:40:27 - train: epoch 0071, iter [03200, 05004], lr: 0.043547, loss: 1.1959
2022-08-07 23:42:01 - train: epoch 0071, iter [03300, 05004], lr: 0.043493, loss: 1.3931
2022-08-07 23:43:35 - train: epoch 0071, iter [03400, 05004], lr: 0.043438, loss: 1.0906
2022-08-07 23:45:09 - train: epoch 0071, iter [03500, 05004], lr: 0.043384, loss: 1.2724
2022-08-07 23:46:43 - train: epoch 0071, iter [03600, 05004], lr: 0.043329, loss: 1.2744
2022-08-07 23:48:17 - train: epoch 0071, iter [03700, 05004], lr: 0.043275, loss: 1.2357
2022-08-07 23:49:51 - train: epoch 0071, iter [03800, 05004], lr: 0.043220, loss: 1.5401
2022-08-07 23:51:24 - train: epoch 0071, iter [03900, 05004], lr: 0.043166, loss: 1.2749
2022-08-07 23:52:58 - train: epoch 0071, iter [04000, 05004], lr: 0.043112, loss: 1.2115
2022-08-07 23:54:32 - train: epoch 0071, iter [04100, 05004], lr: 0.043057, loss: 1.1359
2022-08-07 23:56:06 - train: epoch 0071, iter [04200, 05004], lr: 0.043003, loss: 1.2847
2022-08-07 23:57:40 - train: epoch 0071, iter [04300, 05004], lr: 0.042949, loss: 1.2355
2022-08-07 23:59:14 - train: epoch 0071, iter [04400, 05004], lr: 0.042894, loss: 1.3420
2022-08-08 00:00:47 - train: epoch 0071, iter [04500, 05004], lr: 0.042840, loss: 1.2575
2022-08-08 00:02:21 - train: epoch 0071, iter [04600, 05004], lr: 0.042786, loss: 1.3316
2022-08-08 00:03:55 - train: epoch 0071, iter [04700, 05004], lr: 0.042732, loss: 1.2148
2022-08-08 00:05:29 - train: epoch 0071, iter [04800, 05004], lr: 0.042678, loss: 1.0866
2022-08-08 00:07:03 - train: epoch 0071, iter [04900, 05004], lr: 0.042623, loss: 1.2333
2022-08-08 00:08:37 - train: epoch 0071, iter [05000, 05004], lr: 0.042569, loss: 1.3476
2022-08-08 00:08:41 - train: epoch 071, train_loss: 1.2677
2022-08-08 00:10:43 - eval: epoch: 071, acc1: 71.830%, acc5: 90.836%, test_loss: 1.1384, per_image_load_time: 2.844ms, per_image_inference_time: 1.235ms
2022-08-08 00:10:44 - until epoch: 071, best_acc1: 71.830%
2022-08-08 00:10:44 - epoch 072 lr: 0.042567
2022-08-08 00:12:26 - train: epoch 0072, iter [00100, 05004], lr: 0.042513, loss: 1.2131
2022-08-08 00:13:59 - train: epoch 0072, iter [00200, 05004], lr: 0.042459, loss: 1.1701
2022-08-08 00:15:33 - train: epoch 0072, iter [00300, 05004], lr: 0.042405, loss: 1.1550
2022-08-08 00:17:06 - train: epoch 0072, iter [00400, 05004], lr: 0.042351, loss: 1.5323
2022-08-08 00:18:40 - train: epoch 0072, iter [00500, 05004], lr: 0.042297, loss: 1.1744
2022-08-08 00:20:14 - train: epoch 0072, iter [00600, 05004], lr: 0.042243, loss: 1.0960
2022-08-08 00:21:47 - train: epoch 0072, iter [00700, 05004], lr: 0.042189, loss: 1.2533
2022-08-08 00:23:21 - train: epoch 0072, iter [00800, 05004], lr: 0.042135, loss: 1.4363
2022-08-08 00:24:55 - train: epoch 0072, iter [00900, 05004], lr: 0.042081, loss: 1.0163
2022-08-08 00:26:28 - train: epoch 0072, iter [01000, 05004], lr: 0.042027, loss: 1.2299
2022-08-08 00:28:02 - train: epoch 0072, iter [01100, 05004], lr: 0.041974, loss: 1.3456
2022-08-08 00:29:36 - train: epoch 0072, iter [01200, 05004], lr: 0.041920, loss: 1.2208
2022-08-08 00:31:09 - train: epoch 0072, iter [01300, 05004], lr: 0.041866, loss: 1.2531
2022-08-08 00:32:43 - train: epoch 0072, iter [01400, 05004], lr: 0.041812, loss: 1.2885
2022-08-08 00:34:17 - train: epoch 0072, iter [01500, 05004], lr: 0.041758, loss: 1.3705
2022-08-08 00:35:50 - train: epoch 0072, iter [01600, 05004], lr: 0.041705, loss: 1.2822
2022-08-08 00:37:24 - train: epoch 0072, iter [01700, 05004], lr: 0.041651, loss: 1.2774
2022-08-08 00:38:58 - train: epoch 0072, iter [01800, 05004], lr: 0.041597, loss: 1.2636
2022-08-08 00:40:32 - train: epoch 0072, iter [01900, 05004], lr: 0.041544, loss: 1.2713
2022-08-08 00:42:05 - train: epoch 0072, iter [02000, 05004], lr: 0.041490, loss: 1.2973
2022-08-08 00:43:39 - train: epoch 0072, iter [02100, 05004], lr: 0.041437, loss: 1.2330
2022-08-08 00:45:13 - train: epoch 0072, iter [02200, 05004], lr: 0.041383, loss: 1.5703
2022-08-08 00:46:47 - train: epoch 0072, iter [02300, 05004], lr: 0.041330, loss: 1.2612
2022-08-08 00:48:21 - train: epoch 0072, iter [02400, 05004], lr: 0.041276, loss: 1.2107
2022-08-08 00:49:54 - train: epoch 0072, iter [02500, 05004], lr: 0.041223, loss: 1.2941
2022-08-08 00:51:28 - train: epoch 0072, iter [02600, 05004], lr: 0.041169, loss: 1.1909
2022-08-08 00:53:01 - train: epoch 0072, iter [02700, 05004], lr: 0.041116, loss: 1.1247
2022-08-08 00:54:35 - train: epoch 0072, iter [02800, 05004], lr: 0.041062, loss: 1.3166
2022-08-08 00:56:09 - train: epoch 0072, iter [02900, 05004], lr: 0.041009, loss: 1.0869
2022-08-08 00:57:43 - train: epoch 0072, iter [03000, 05004], lr: 0.040956, loss: 1.4302
2022-08-08 00:59:16 - train: epoch 0072, iter [03100, 05004], lr: 0.040902, loss: 1.3514
2022-08-08 01:00:50 - train: epoch 0072, iter [03200, 05004], lr: 0.040849, loss: 1.1537
2022-08-08 01:02:24 - train: epoch 0072, iter [03300, 05004], lr: 0.040796, loss: 1.4388
2022-08-08 01:03:57 - train: epoch 0072, iter [03400, 05004], lr: 0.040742, loss: 1.3571
2022-08-08 01:05:31 - train: epoch 0072, iter [03500, 05004], lr: 0.040689, loss: 1.3224
2022-08-08 01:07:05 - train: epoch 0072, iter [03600, 05004], lr: 0.040636, loss: 1.1720
2022-08-08 01:08:38 - train: epoch 0072, iter [03700, 05004], lr: 0.040583, loss: 1.3669
2022-08-08 01:10:12 - train: epoch 0072, iter [03800, 05004], lr: 0.040530, loss: 1.2904
2022-08-08 01:11:46 - train: epoch 0072, iter [03900, 05004], lr: 0.040477, loss: 1.2151
2022-08-08 01:13:19 - train: epoch 0072, iter [04000, 05004], lr: 0.040423, loss: 1.1856
2022-08-08 01:14:53 - train: epoch 0072, iter [04100, 05004], lr: 0.040370, loss: 1.4585
2022-08-08 01:16:26 - train: epoch 0072, iter [04200, 05004], lr: 0.040317, loss: 1.0494
2022-08-08 01:18:00 - train: epoch 0072, iter [04300, 05004], lr: 0.040264, loss: 1.1868
2022-08-08 01:19:34 - train: epoch 0072, iter [04400, 05004], lr: 0.040211, loss: 1.2363
2022-08-08 01:21:07 - train: epoch 0072, iter [04500, 05004], lr: 0.040158, loss: 1.3692
2022-08-08 01:22:41 - train: epoch 0072, iter [04600, 05004], lr: 0.040105, loss: 1.0906
2022-08-08 01:24:15 - train: epoch 0072, iter [04700, 05004], lr: 0.040053, loss: 1.1898
2022-08-08 01:25:49 - train: epoch 0072, iter [04800, 05004], lr: 0.040000, loss: 1.3365
2022-08-08 01:27:22 - train: epoch 0072, iter [04900, 05004], lr: 0.039947, loss: 1.2095
2022-08-08 01:28:56 - train: epoch 0072, iter [05000, 05004], lr: 0.039894, loss: 1.2611
2022-08-08 01:29:00 - train: epoch 072, train_loss: 1.2480
2022-08-08 01:31:07 - eval: epoch: 072, acc1: 71.830%, acc5: 91.066%, test_loss: 1.1371, per_image_load_time: 2.224ms, per_image_inference_time: 1.262ms
2022-08-08 01:31:07 - until epoch: 072, best_acc1: 71.830%
2022-08-08 01:31:07 - epoch 073 lr: 0.039891
2022-08-08 01:32:50 - train: epoch 0073, iter [00100, 05004], lr: 0.039839, loss: 1.4098
2022-08-08 01:34:24 - train: epoch 0073, iter [00200, 05004], lr: 0.039786, loss: 1.1650
2022-08-08 01:35:57 - train: epoch 0073, iter [00300, 05004], lr: 0.039734, loss: 1.2729
2022-08-08 01:37:31 - train: epoch 0073, iter [00400, 05004], lr: 0.039681, loss: 0.9088
2022-08-08 01:39:05 - train: epoch 0073, iter [00500, 05004], lr: 0.039628, loss: 1.1054
2022-08-08 01:40:38 - train: epoch 0073, iter [00600, 05004], lr: 0.039575, loss: 1.1745
2022-08-08 01:42:12 - train: epoch 0073, iter [00700, 05004], lr: 0.039523, loss: 1.4280
2022-08-08 01:43:45 - train: epoch 0073, iter [00800, 05004], lr: 0.039470, loss: 1.2319
2022-08-08 01:45:19 - train: epoch 0073, iter [00900, 05004], lr: 0.039418, loss: 1.2297
2022-08-08 01:46:52 - train: epoch 0073, iter [01000, 05004], lr: 0.039365, loss: 1.0644
2022-08-08 01:48:26 - train: epoch 0073, iter [01100, 05004], lr: 0.039313, loss: 1.1381
2022-08-08 01:50:00 - train: epoch 0073, iter [01200, 05004], lr: 0.039260, loss: 1.2028
2022-08-08 01:51:33 - train: epoch 0073, iter [01300, 05004], lr: 0.039208, loss: 1.4274
2022-08-08 01:53:07 - train: epoch 0073, iter [01400, 05004], lr: 0.039155, loss: 1.1513
2022-08-08 01:54:41 - train: epoch 0073, iter [01500, 05004], lr: 0.039103, loss: 1.3138
2022-08-08 01:56:14 - train: epoch 0073, iter [01600, 05004], lr: 0.039050, loss: 1.2673
2022-08-08 01:57:48 - train: epoch 0073, iter [01700, 05004], lr: 0.038998, loss: 1.4575
2022-08-08 01:59:22 - train: epoch 0073, iter [01800, 05004], lr: 0.038945, loss: 0.9513
2022-08-08 02:00:56 - train: epoch 0073, iter [01900, 05004], lr: 0.038893, loss: 1.3211
2022-08-08 02:02:30 - train: epoch 0073, iter [02000, 05004], lr: 0.038841, loss: 1.0683
2022-08-08 02:04:03 - train: epoch 0073, iter [02100, 05004], lr: 0.038789, loss: 1.1972
2022-08-08 02:05:37 - train: epoch 0073, iter [02200, 05004], lr: 0.038736, loss: 1.3403
2022-08-08 02:07:11 - train: epoch 0073, iter [02300, 05004], lr: 0.038684, loss: 1.2172
2022-08-08 02:08:44 - train: epoch 0073, iter [02400, 05004], lr: 0.038632, loss: 1.0863
2022-08-08 02:10:18 - train: epoch 0073, iter [02500, 05004], lr: 0.038580, loss: 1.0705
2022-08-08 02:11:52 - train: epoch 0073, iter [02600, 05004], lr: 0.038528, loss: 1.1909
2022-08-08 02:13:26 - train: epoch 0073, iter [02700, 05004], lr: 0.038476, loss: 1.0542
2022-08-08 02:14:59 - train: epoch 0073, iter [02800, 05004], lr: 0.038423, loss: 1.1463
2022-08-08 02:16:33 - train: epoch 0073, iter [02900, 05004], lr: 0.038371, loss: 1.3099
2022-08-08 02:18:07 - train: epoch 0073, iter [03000, 05004], lr: 0.038319, loss: 1.0315
2022-08-08 02:19:41 - train: epoch 0073, iter [03100, 05004], lr: 0.038267, loss: 1.1643
2022-08-08 02:21:14 - train: epoch 0073, iter [03200, 05004], lr: 0.038215, loss: 1.0502
2022-08-08 02:22:48 - train: epoch 0073, iter [03300, 05004], lr: 0.038163, loss: 1.2170
2022-08-08 02:24:22 - train: epoch 0073, iter [03400, 05004], lr: 0.038111, loss: 1.2573
2022-08-08 02:25:56 - train: epoch 0073, iter [03500, 05004], lr: 0.038060, loss: 1.3785
2022-08-08 02:27:29 - train: epoch 0073, iter [03600, 05004], lr: 0.038008, loss: 1.1538
2022-08-08 02:29:03 - train: epoch 0073, iter [03700, 05004], lr: 0.037956, loss: 1.2894
2022-08-08 02:30:37 - train: epoch 0073, iter [03800, 05004], lr: 0.037904, loss: 1.4212
2022-08-08 02:32:11 - train: epoch 0073, iter [03900, 05004], lr: 0.037852, loss: 1.2371
2022-08-08 02:33:44 - train: epoch 0073, iter [04000, 05004], lr: 0.037801, loss: 1.2093
2022-08-08 02:35:18 - train: epoch 0073, iter [04100, 05004], lr: 0.037749, loss: 1.3048
2022-08-08 02:36:52 - train: epoch 0073, iter [04200, 05004], lr: 0.037697, loss: 1.3457
2022-08-08 02:38:26 - train: epoch 0073, iter [04300, 05004], lr: 0.037645, loss: 1.3125
2022-08-08 02:39:59 - train: epoch 0073, iter [04400, 05004], lr: 0.037594, loss: 1.4459
2022-08-08 02:41:33 - train: epoch 0073, iter [04500, 05004], lr: 0.037542, loss: 1.3634
2022-08-08 02:43:07 - train: epoch 0073, iter [04600, 05004], lr: 0.037491, loss: 1.1939
2022-08-08 02:44:41 - train: epoch 0073, iter [04700, 05004], lr: 0.037439, loss: 1.1680
2022-08-08 02:46:14 - train: epoch 0073, iter [04800, 05004], lr: 0.037387, loss: 1.1224
2022-08-08 02:47:48 - train: epoch 0073, iter [04900, 05004], lr: 0.037336, loss: 1.2363
2022-08-08 02:49:22 - train: epoch 0073, iter [05000, 05004], lr: 0.037284, loss: 1.2290
2022-08-08 02:49:26 - train: epoch 073, train_loss: 1.2271
2022-08-08 02:51:29 - eval: epoch: 073, acc1: 72.154%, acc5: 91.086%, test_loss: 1.1166, per_image_load_time: 1.957ms, per_image_inference_time: 1.156ms
2022-08-08 02:51:29 - until epoch: 073, best_acc1: 72.154%
2022-08-08 02:51:29 - epoch 074 lr: 0.037282
2022-08-08 02:53:11 - train: epoch 0074, iter [00100, 05004], lr: 0.037231, loss: 1.3472
2022-08-08 02:54:45 - train: epoch 0074, iter [00200, 05004], lr: 0.037179, loss: 1.4108
2022-08-08 02:56:19 - train: epoch 0074, iter [00300, 05004], lr: 0.037128, loss: 1.2167
2022-08-08 02:57:52 - train: epoch 0074, iter [00400, 05004], lr: 0.037077, loss: 1.3320
2022-08-08 02:59:26 - train: epoch 0074, iter [00500, 05004], lr: 0.037025, loss: 1.1228
2022-08-08 03:01:00 - train: epoch 0074, iter [00600, 05004], lr: 0.036974, loss: 1.0089
2022-08-08 03:02:34 - train: epoch 0074, iter [00700, 05004], lr: 0.036923, loss: 1.1192
2022-08-08 03:04:07 - train: epoch 0074, iter [00800, 05004], lr: 0.036871, loss: 1.4407
2022-08-08 03:05:41 - train: epoch 0074, iter [00900, 05004], lr: 0.036820, loss: 1.0533
2022-08-08 03:07:15 - train: epoch 0074, iter [01000, 05004], lr: 0.036769, loss: 1.3093
2022-08-08 03:08:48 - train: epoch 0074, iter [01100, 05004], lr: 0.036718, loss: 1.2898
2022-08-08 03:10:22 - train: epoch 0074, iter [01200, 05004], lr: 0.036667, loss: 1.1195
2022-08-08 03:11:56 - train: epoch 0074, iter [01300, 05004], lr: 0.036616, loss: 1.2437
2022-08-08 03:13:29 - train: epoch 0074, iter [01400, 05004], lr: 0.036564, loss: 1.2149
2022-08-08 03:15:03 - train: epoch 0074, iter [01500, 05004], lr: 0.036513, loss: 1.3009
2022-08-08 03:16:37 - train: epoch 0074, iter [01600, 05004], lr: 0.036462, loss: 1.1851
2022-08-08 03:18:10 - train: epoch 0074, iter [01700, 05004], lr: 0.036411, loss: 1.2326
2022-08-08 03:19:44 - train: epoch 0074, iter [01800, 05004], lr: 0.036360, loss: 1.2604
2022-08-08 03:21:18 - train: epoch 0074, iter [01900, 05004], lr: 0.036309, loss: 1.2203
2022-08-08 03:22:51 - train: epoch 0074, iter [02000, 05004], lr: 0.036258, loss: 0.9593
2022-08-08 03:24:25 - train: epoch 0074, iter [02100, 05004], lr: 0.036208, loss: 1.1512
2022-08-08 03:25:59 - train: epoch 0074, iter [02200, 05004], lr: 0.036157, loss: 1.5258
2022-08-08 03:27:33 - train: epoch 0074, iter [02300, 05004], lr: 0.036106, loss: 1.1744
2022-08-08 03:29:06 - train: epoch 0074, iter [02400, 05004], lr: 0.036055, loss: 1.3262
2022-08-08 03:30:40 - train: epoch 0074, iter [02500, 05004], lr: 0.036004, loss: 1.2037
2022-08-08 03:32:14 - train: epoch 0074, iter [02600, 05004], lr: 0.035953, loss: 0.9190
2022-08-08 03:33:48 - train: epoch 0074, iter [02700, 05004], lr: 0.035903, loss: 1.1962
2022-08-08 03:35:21 - train: epoch 0074, iter [02800, 05004], lr: 0.035852, loss: 1.6174
2022-08-08 03:36:55 - train: epoch 0074, iter [02900, 05004], lr: 0.035801, loss: 1.2349
2022-08-08 03:38:29 - train: epoch 0074, iter [03000, 05004], lr: 0.035751, loss: 1.3009
2022-08-08 03:40:02 - train: epoch 0074, iter [03100, 05004], lr: 0.035700, loss: 1.2343
2022-08-08 03:41:36 - train: epoch 0074, iter [03200, 05004], lr: 0.035649, loss: 1.2187
2022-08-08 03:43:10 - train: epoch 0074, iter [03300, 05004], lr: 0.035599, loss: 1.1557
2022-08-08 03:44:44 - train: epoch 0074, iter [03400, 05004], lr: 0.035548, loss: 1.2900
2022-08-08 03:46:17 - train: epoch 0074, iter [03500, 05004], lr: 0.035498, loss: 1.1471
2022-08-08 03:47:51 - train: epoch 0074, iter [03600, 05004], lr: 0.035447, loss: 1.3600
2022-08-08 03:49:25 - train: epoch 0074, iter [03700, 05004], lr: 0.035397, loss: 1.2211
2022-08-08 03:50:59 - train: epoch 0074, iter [03800, 05004], lr: 0.035346, loss: 1.3277
2022-08-08 03:52:33 - train: epoch 0074, iter [03900, 05004], lr: 0.035296, loss: 1.0976
2022-08-08 03:54:06 - train: epoch 0074, iter [04000, 05004], lr: 0.035246, loss: 1.5314
2022-08-08 03:55:40 - train: epoch 0074, iter [04100, 05004], lr: 0.035195, loss: 1.1481
2022-08-08 03:57:14 - train: epoch 0074, iter [04200, 05004], lr: 0.035145, loss: 1.1458
2022-08-08 03:58:48 - train: epoch 0074, iter [04300, 05004], lr: 0.035095, loss: 1.0457
2022-08-08 04:00:21 - train: epoch 0074, iter [04400, 05004], lr: 0.035044, loss: 1.1444
2022-08-08 04:01:55 - train: epoch 0074, iter [04500, 05004], lr: 0.034994, loss: 1.0835
2022-08-08 04:03:29 - train: epoch 0074, iter [04600, 05004], lr: 0.034944, loss: 1.3327
2022-08-08 04:05:03 - train: epoch 0074, iter [04700, 05004], lr: 0.034894, loss: 1.1284
2022-08-08 04:06:36 - train: epoch 0074, iter [04800, 05004], lr: 0.034844, loss: 1.1527
2022-08-08 04:08:10 - train: epoch 0074, iter [04900, 05004], lr: 0.034794, loss: 1.3284
2022-08-08 04:09:44 - train: epoch 0074, iter [05000, 05004], lr: 0.034743, loss: 1.1699
2022-08-08 04:09:48 - train: epoch 074, train_loss: 1.2047
2022-08-08 04:11:52 - eval: epoch: 074, acc1: 72.458%, acc5: 91.134%, test_loss: 1.1042, per_image_load_time: 1.609ms, per_image_inference_time: 1.214ms
2022-08-08 04:11:52 - until epoch: 074, best_acc1: 72.458%
2022-08-08 04:11:52 - epoch 075 lr: 0.034741
2022-08-08 04:13:35 - train: epoch 0075, iter [00100, 05004], lr: 0.034691, loss: 1.2606
2022-08-08 04:15:09 - train: epoch 0075, iter [00200, 05004], lr: 0.034641, loss: 1.1276
2022-08-08 04:16:43 - train: epoch 0075, iter [00300, 05004], lr: 0.034591, loss: 1.3542
2022-08-08 04:18:18 - train: epoch 0075, iter [00400, 05004], lr: 0.034541, loss: 1.2061
2022-08-08 04:19:52 - train: epoch 0075, iter [00500, 05004], lr: 0.034491, loss: 1.1478
2022-08-08 04:21:26 - train: epoch 0075, iter [00600, 05004], lr: 0.034441, loss: 1.1257
2022-08-08 04:23:00 - train: epoch 0075, iter [00700, 05004], lr: 0.034392, loss: 1.2711
2022-08-08 04:24:35 - train: epoch 0075, iter [00800, 05004], lr: 0.034342, loss: 1.0943
2022-08-08 04:26:09 - train: epoch 0075, iter [00900, 05004], lr: 0.034292, loss: 1.2803
2022-08-08 04:27:43 - train: epoch 0075, iter [01000, 05004], lr: 0.034242, loss: 1.1080
2022-08-08 04:29:17 - train: epoch 0075, iter [01100, 05004], lr: 0.034192, loss: 1.1305
2022-08-08 04:30:51 - train: epoch 0075, iter [01200, 05004], lr: 0.034143, loss: 1.0906
2022-08-08 04:32:25 - train: epoch 0075, iter [01300, 05004], lr: 0.034093, loss: 1.0647
2022-08-08 04:34:00 - train: epoch 0075, iter [01400, 05004], lr: 0.034043, loss: 1.2211
2022-08-08 04:35:34 - train: epoch 0075, iter [01500, 05004], lr: 0.033994, loss: 1.3702
2022-08-08 04:37:08 - train: epoch 0075, iter [01600, 05004], lr: 0.033944, loss: 1.0808
2022-08-08 04:38:42 - train: epoch 0075, iter [01700, 05004], lr: 0.033894, loss: 1.1795
2022-08-08 04:40:16 - train: epoch 0075, iter [01800, 05004], lr: 0.033845, loss: 1.1974
2022-08-08 04:41:51 - train: epoch 0075, iter [01900, 05004], lr: 0.033795, loss: 0.9930
2022-08-08 04:43:25 - train: epoch 0075, iter [02000, 05004], lr: 0.033746, loss: 1.1834
2022-08-08 04:44:59 - train: epoch 0075, iter [02100, 05004], lr: 0.033696, loss: 1.1013
2022-08-08 04:46:33 - train: epoch 0075, iter [02200, 05004], lr: 0.033647, loss: 1.2619
2022-08-08 04:48:07 - train: epoch 0075, iter [02300, 05004], lr: 0.033597, loss: 1.1124
2022-08-08 04:49:41 - train: epoch 0075, iter [02400, 05004], lr: 0.033548, loss: 1.0553
2022-08-08 04:51:16 - train: epoch 0075, iter [02500, 05004], lr: 0.033499, loss: 1.2407
2022-08-08 04:52:50 - train: epoch 0075, iter [02600, 05004], lr: 0.033449, loss: 1.2609
2022-08-08 04:54:24 - train: epoch 0075, iter [02700, 05004], lr: 0.033400, loss: 1.0999
2022-08-08 04:55:58 - train: epoch 0075, iter [02800, 05004], lr: 0.033351, loss: 1.1382
2022-08-08 04:57:32 - train: epoch 0075, iter [02900, 05004], lr: 0.033301, loss: 1.3087
2022-08-08 04:59:07 - train: epoch 0075, iter [03000, 05004], lr: 0.033252, loss: 1.3046
2022-08-08 05:00:41 - train: epoch 0075, iter [03100, 05004], lr: 0.033203, loss: 1.1655
2022-08-08 05:02:15 - train: epoch 0075, iter [03200, 05004], lr: 0.033154, loss: 1.2288
2022-08-08 05:03:49 - train: epoch 0075, iter [03300, 05004], lr: 0.033105, loss: 1.2216
2022-08-08 05:05:23 - train: epoch 0075, iter [03400, 05004], lr: 0.033056, loss: 1.1381
2022-08-08 05:06:57 - train: epoch 0075, iter [03500, 05004], lr: 0.033006, loss: 1.1954
2022-08-08 05:08:31 - train: epoch 0075, iter [03600, 05004], lr: 0.032957, loss: 1.4087
2022-08-08 05:10:06 - train: epoch 0075, iter [03700, 05004], lr: 0.032908, loss: 1.3439
2022-08-08 05:11:40 - train: epoch 0075, iter [03800, 05004], lr: 0.032859, loss: 1.1425
2022-08-08 05:13:14 - train: epoch 0075, iter [03900, 05004], lr: 0.032810, loss: 1.3748
2022-08-08 05:14:48 - train: epoch 0075, iter [04000, 05004], lr: 0.032761, loss: 1.0340
2022-08-08 05:16:22 - train: epoch 0075, iter [04100, 05004], lr: 0.032713, loss: 1.1663
2022-08-08 05:17:56 - train: epoch 0075, iter [04200, 05004], lr: 0.032664, loss: 1.0961
2022-08-08 05:19:31 - train: epoch 0075, iter [04300, 05004], lr: 0.032615, loss: 1.1948
2022-08-08 05:21:05 - train: epoch 0075, iter [04400, 05004], lr: 0.032566, loss: 1.0845
2022-08-08 05:22:39 - train: epoch 0075, iter [04500, 05004], lr: 0.032517, loss: 1.2317
2022-08-08 05:24:13 - train: epoch 0075, iter [04600, 05004], lr: 0.032469, loss: 1.1897
2022-08-08 05:25:46 - train: epoch 0075, iter [04700, 05004], lr: 0.032420, loss: 1.1786
2022-08-08 05:27:20 - train: epoch 0075, iter [04800, 05004], lr: 0.032371, loss: 1.0097
2022-08-08 05:28:54 - train: epoch 0075, iter [04900, 05004], lr: 0.032322, loss: 1.2285
2022-08-08 05:30:28 - train: epoch 0075, iter [05000, 05004], lr: 0.032274, loss: 1.1905
2022-08-08 05:30:32 - train: epoch 075, train_loss: 1.1843
2022-08-08 05:32:34 - eval: epoch: 075, acc1: 72.926%, acc5: 91.352%, test_loss: 1.0949, per_image_load_time: 1.953ms, per_image_inference_time: 1.199ms
2022-08-08 05:32:35 - until epoch: 075, best_acc1: 72.926%
2022-08-08 05:32:35 - epoch 076 lr: 0.032271
2022-08-08 05:34:18 - train: epoch 0076, iter [00100, 05004], lr: 0.032223, loss: 1.1565
2022-08-08 05:35:52 - train: epoch 0076, iter [00200, 05004], lr: 0.032175, loss: 1.1449
2022-08-08 05:37:26 - train: epoch 0076, iter [00300, 05004], lr: 0.032126, loss: 1.1665
2022-08-08 05:39:00 - train: epoch 0076, iter [00400, 05004], lr: 0.032078, loss: 1.1681
2022-08-08 05:40:34 - train: epoch 0076, iter [00500, 05004], lr: 0.032029, loss: 1.0910
2022-08-08 05:42:09 - train: epoch 0076, iter [00600, 05004], lr: 0.031981, loss: 1.3770
2022-08-08 05:43:43 - train: epoch 0076, iter [00700, 05004], lr: 0.031932, loss: 1.1476
2022-08-08 05:45:17 - train: epoch 0076, iter [00800, 05004], lr: 0.031884, loss: 1.3826
2022-08-08 05:46:51 - train: epoch 0076, iter [00900, 05004], lr: 0.031835, loss: 1.1260
2022-08-08 05:48:25 - train: epoch 0076, iter [01000, 05004], lr: 0.031787, loss: 1.1724
2022-08-08 05:49:59 - train: epoch 0076, iter [01100, 05004], lr: 0.031739, loss: 1.1724
2022-08-08 05:51:34 - train: epoch 0076, iter [01200, 05004], lr: 0.031691, loss: 1.1315
2022-08-08 05:53:08 - train: epoch 0076, iter [01300, 05004], lr: 0.031642, loss: 1.1791
2022-08-08 05:54:42 - train: epoch 0076, iter [01400, 05004], lr: 0.031594, loss: 1.0328
2022-08-08 05:56:16 - train: epoch 0076, iter [01500, 05004], lr: 0.031546, loss: 1.1976
2022-08-08 05:57:50 - train: epoch 0076, iter [01600, 05004], lr: 0.031498, loss: 1.1469
2022-08-08 05:59:24 - train: epoch 0076, iter [01700, 05004], lr: 0.031450, loss: 1.2140
2022-08-08 06:00:58 - train: epoch 0076, iter [01800, 05004], lr: 0.031401, loss: 1.0602
2022-08-08 06:02:32 - train: epoch 0076, iter [01900, 05004], lr: 0.031353, loss: 1.2176
2022-08-08 06:04:05 - train: epoch 0076, iter [02000, 05004], lr: 0.031305, loss: 1.0692
2022-08-08 06:05:39 - train: epoch 0076, iter [02100, 05004], lr: 0.031257, loss: 1.3232
2022-08-08 06:07:13 - train: epoch 0076, iter [02200, 05004], lr: 0.031209, loss: 0.9592
2022-08-08 06:08:47 - train: epoch 0076, iter [02300, 05004], lr: 0.031161, loss: 1.1409
2022-08-08 06:10:21 - train: epoch 0076, iter [02400, 05004], lr: 0.031114, loss: 1.2132
2022-08-08 06:11:55 - train: epoch 0076, iter [02500, 05004], lr: 0.031066, loss: 1.3340
2022-08-08 06:13:29 - train: epoch 0076, iter [02600, 05004], lr: 0.031018, loss: 1.4067
2022-08-08 06:15:02 - train: epoch 0076, iter [02700, 05004], lr: 0.030970, loss: 1.2866
2022-08-08 06:16:36 - train: epoch 0076, iter [02800, 05004], lr: 0.030922, loss: 1.1041
2022-08-08 06:18:10 - train: epoch 0076, iter [02900, 05004], lr: 0.030874, loss: 1.2259
2022-08-08 06:19:44 - train: epoch 0076, iter [03000, 05004], lr: 0.030827, loss: 1.1964
2022-08-08 06:21:17 - train: epoch 0076, iter [03100, 05004], lr: 0.030779, loss: 1.2806
2022-08-08 06:22:51 - train: epoch 0076, iter [03200, 05004], lr: 0.030731, loss: 0.9430
2022-08-08 06:24:25 - train: epoch 0076, iter [03300, 05004], lr: 0.030684, loss: 1.1432
2022-08-08 06:25:59 - train: epoch 0076, iter [03400, 05004], lr: 0.030636, loss: 1.0377
2022-08-08 06:27:33 - train: epoch 0076, iter [03500, 05004], lr: 0.030588, loss: 1.0686
2022-08-08 06:29:06 - train: epoch 0076, iter [03600, 05004], lr: 0.030541, loss: 1.1063
2022-08-08 06:30:40 - train: epoch 0076, iter [03700, 05004], lr: 0.030493, loss: 1.1782
2022-08-08 06:32:14 - train: epoch 0076, iter [03800, 05004], lr: 0.030446, loss: 1.1666
2022-08-08 06:33:48 - train: epoch 0076, iter [03900, 05004], lr: 0.030398, loss: 1.0617
2022-08-08 06:35:22 - train: epoch 0076, iter [04000, 05004], lr: 0.030351, loss: 1.2232
2022-08-08 06:36:56 - train: epoch 0076, iter [04100, 05004], lr: 0.030303, loss: 1.0929
2022-08-08 06:38:30 - train: epoch 0076, iter [04200, 05004], lr: 0.030256, loss: 1.1557
2022-08-08 06:40:04 - train: epoch 0076, iter [04300, 05004], lr: 0.030209, loss: 1.3687
2022-08-08 06:41:38 - train: epoch 0076, iter [04400, 05004], lr: 0.030161, loss: 1.2383
2022-08-08 06:43:11 - train: epoch 0076, iter [04500, 05004], lr: 0.030114, loss: 1.2682
2022-08-08 06:44:45 - train: epoch 0076, iter [04600, 05004], lr: 0.030067, loss: 1.0814
2022-08-08 06:46:19 - train: epoch 0076, iter [04700, 05004], lr: 0.030020, loss: 1.2383
2022-08-08 06:47:53 - train: epoch 0076, iter [04800, 05004], lr: 0.029972, loss: 1.0403
2022-08-08 06:49:27 - train: epoch 0076, iter [04900, 05004], lr: 0.029925, loss: 1.0519
2022-08-08 06:51:01 - train: epoch 0076, iter [05000, 05004], lr: 0.029878, loss: 1.3055
2022-08-08 06:51:05 - train: epoch 076, train_loss: 1.1648
2022-08-08 06:53:10 - eval: epoch: 076, acc1: 72.958%, acc5: 91.430%, test_loss: 1.0930, per_image_load_time: 3.602ms, per_image_inference_time: 1.166ms
2022-08-08 06:53:10 - until epoch: 076, best_acc1: 72.958%
2022-08-08 06:53:10 - epoch 077 lr: 0.029876
2022-08-08 06:54:52 - train: epoch 0077, iter [00100, 05004], lr: 0.029829, loss: 1.1934
2022-08-08 06:56:26 - train: epoch 0077, iter [00200, 05004], lr: 0.029782, loss: 1.2233
2022-08-08 06:57:59 - train: epoch 0077, iter [00300, 05004], lr: 0.029735, loss: 0.9546
2022-08-08 06:59:33 - train: epoch 0077, iter [00400, 05004], lr: 0.029688, loss: 0.9526
2022-08-08 07:01:07 - train: epoch 0077, iter [00500, 05004], lr: 0.029641, loss: 1.1151
2022-08-08 07:02:41 - train: epoch 0077, iter [00600, 05004], lr: 0.029594, loss: 1.1094
2022-08-08 07:04:14 - train: epoch 0077, iter [00700, 05004], lr: 0.029547, loss: 1.1626
2022-08-08 07:05:48 - train: epoch 0077, iter [00800, 05004], lr: 0.029500, loss: 1.1241
2022-08-08 07:07:22 - train: epoch 0077, iter [00900, 05004], lr: 0.029454, loss: 1.1516
2022-08-08 07:08:56 - train: epoch 0077, iter [01000, 05004], lr: 0.029407, loss: 1.1287
2022-08-08 07:10:29 - train: epoch 0077, iter [01100, 05004], lr: 0.029360, loss: 1.1723
2022-08-08 07:12:03 - train: epoch 0077, iter [01200, 05004], lr: 0.029313, loss: 1.3996
2022-08-08 07:13:37 - train: epoch 0077, iter [01300, 05004], lr: 0.029266, loss: 1.0768
2022-08-08 07:15:10 - train: epoch 0077, iter [01400, 05004], lr: 0.029220, loss: 1.0327
2022-08-08 07:16:44 - train: epoch 0077, iter [01500, 05004], lr: 0.029173, loss: 1.0734
2022-08-08 07:18:18 - train: epoch 0077, iter [01600, 05004], lr: 0.029126, loss: 1.0232
2022-08-08 07:19:51 - train: epoch 0077, iter [01700, 05004], lr: 0.029080, loss: 1.3166
2022-08-08 07:21:25 - train: epoch 0077, iter [01800, 05004], lr: 0.029033, loss: 1.0993
2022-08-08 07:22:59 - train: epoch 0077, iter [01900, 05004], lr: 0.028987, loss: 1.1788
2022-08-08 07:24:33 - train: epoch 0077, iter [02000, 05004], lr: 0.028940, loss: 1.0658
2022-08-08 07:26:07 - train: epoch 0077, iter [02100, 05004], lr: 0.028894, loss: 1.1971
2022-08-08 07:27:40 - train: epoch 0077, iter [02200, 05004], lr: 0.028847, loss: 1.0676
2022-08-08 07:29:14 - train: epoch 0077, iter [02300, 05004], lr: 0.028801, loss: 1.1621
2022-08-08 07:30:48 - train: epoch 0077, iter [02400, 05004], lr: 0.028754, loss: 1.0520
2022-08-08 07:32:21 - train: epoch 0077, iter [02500, 05004], lr: 0.028708, loss: 1.2952
2022-08-08 07:33:55 - train: epoch 0077, iter [02600, 05004], lr: 0.028662, loss: 0.9306
2022-08-08 07:35:29 - train: epoch 0077, iter [02700, 05004], lr: 0.028615, loss: 1.1717
2022-08-08 07:37:03 - train: epoch 0077, iter [02800, 05004], lr: 0.028569, loss: 1.1469
2022-08-08 07:38:37 - train: epoch 0077, iter [02900, 05004], lr: 0.028523, loss: 1.3175
2022-08-08 07:40:10 - train: epoch 0077, iter [03000, 05004], lr: 0.028477, loss: 1.3029
2022-08-08 07:41:44 - train: epoch 0077, iter [03100, 05004], lr: 0.028431, loss: 1.2056
2022-08-08 07:43:18 - train: epoch 0077, iter [03200, 05004], lr: 0.028384, loss: 1.3183
2022-08-08 07:44:51 - train: epoch 0077, iter [03300, 05004], lr: 0.028338, loss: 1.1524
2022-08-08 07:46:25 - train: epoch 0077, iter [03400, 05004], lr: 0.028292, loss: 1.2754
2022-08-08 07:47:59 - train: epoch 0077, iter [03500, 05004], lr: 0.028246, loss: 1.0933
2022-08-08 07:49:33 - train: epoch 0077, iter [03600, 05004], lr: 0.028200, loss: 1.0335
2022-08-08 07:51:06 - train: epoch 0077, iter [03700, 05004], lr: 0.028154, loss: 1.0180
2022-08-08 07:52:40 - train: epoch 0077, iter [03800, 05004], lr: 0.028108, loss: 1.1109
2022-08-08 07:54:14 - train: epoch 0077, iter [03900, 05004], lr: 0.028062, loss: 1.1589
2022-08-08 07:55:48 - train: epoch 0077, iter [04000, 05004], lr: 0.028016, loss: 1.1763
2022-08-08 07:57:21 - train: epoch 0077, iter [04100, 05004], lr: 0.027971, loss: 1.0959
2022-08-08 07:58:55 - train: epoch 0077, iter [04200, 05004], lr: 0.027925, loss: 1.2818
2022-08-08 08:00:29 - train: epoch 0077, iter [04300, 05004], lr: 0.027879, loss: 1.2143
2022-08-08 08:02:03 - train: epoch 0077, iter [04400, 05004], lr: 0.027833, loss: 1.2407
2022-08-08 08:03:37 - train: epoch 0077, iter [04500, 05004], lr: 0.027787, loss: 1.2865
2022-08-08 08:05:11 - train: epoch 0077, iter [04600, 05004], lr: 0.027742, loss: 0.9579
2022-08-08 08:06:45 - train: epoch 0077, iter [04700, 05004], lr: 0.027696, loss: 1.0806
2022-08-08 08:08:19 - train: epoch 0077, iter [04800, 05004], lr: 0.027650, loss: 1.0445
2022-08-08 08:09:52 - train: epoch 0077, iter [04900, 05004], lr: 0.027605, loss: 1.2897
2022-08-08 08:11:26 - train: epoch 0077, iter [05000, 05004], lr: 0.027559, loss: 1.2170
2022-08-08 08:11:31 - train: epoch 077, train_loss: 1.1378
2022-08-08 08:13:33 - eval: epoch: 077, acc1: 73.534%, acc5: 91.812%, test_loss: 1.0659, per_image_load_time: 1.990ms, per_image_inference_time: 1.159ms
2022-08-08 08:13:34 - until epoch: 077, best_acc1: 73.534%
2022-08-08 08:13:34 - epoch 078 lr: 0.027557
2022-08-08 08:15:17 - train: epoch 0078, iter [00100, 05004], lr: 0.027512, loss: 1.1111
2022-08-08 08:16:51 - train: epoch 0078, iter [00200, 05004], lr: 0.027466, loss: 1.1355
2022-08-08 08:18:25 - train: epoch 0078, iter [00300, 05004], lr: 0.027421, loss: 1.1825
2022-08-08 08:19:59 - train: epoch 0078, iter [00400, 05004], lr: 0.027376, loss: 1.2702
2022-08-08 08:21:33 - train: epoch 0078, iter [00500, 05004], lr: 0.027330, loss: 0.9515
2022-08-08 08:23:07 - train: epoch 0078, iter [00600, 05004], lr: 0.027285, loss: 1.0898
2022-08-08 08:24:41 - train: epoch 0078, iter [00700, 05004], lr: 0.027239, loss: 1.3242
2022-08-08 08:26:15 - train: epoch 0078, iter [00800, 05004], lr: 0.027194, loss: 1.1171
2022-08-08 08:27:48 - train: epoch 0078, iter [00900, 05004], lr: 0.027149, loss: 1.2013
2022-08-08 08:29:23 - train: epoch 0078, iter [01000, 05004], lr: 0.027103, loss: 1.1340
2022-08-08 08:30:57 - train: epoch 0078, iter [01100, 05004], lr: 0.027058, loss: 1.0526
2022-08-08 08:32:31 - train: epoch 0078, iter [01200, 05004], lr: 0.027013, loss: 1.1024
2022-08-08 08:34:05 - train: epoch 0078, iter [01300, 05004], lr: 0.026968, loss: 1.0125
2022-08-08 08:35:39 - train: epoch 0078, iter [01400, 05004], lr: 0.026923, loss: 0.9123
2022-08-08 08:37:13 - train: epoch 0078, iter [01500, 05004], lr: 0.026878, loss: 1.0883
2022-08-08 08:38:47 - train: epoch 0078, iter [01600, 05004], lr: 0.026833, loss: 1.1363
2022-08-08 08:40:21 - train: epoch 0078, iter [01700, 05004], lr: 0.026788, loss: 1.1968
2022-08-08 08:41:54 - train: epoch 0078, iter [01800, 05004], lr: 0.026743, loss: 1.1458
2022-08-08 08:43:28 - train: epoch 0078, iter [01900, 05004], lr: 0.026698, loss: 1.2478
2022-08-08 08:45:02 - train: epoch 0078, iter [02000, 05004], lr: 0.026653, loss: 1.1186
2022-08-08 08:46:36 - train: epoch 0078, iter [02100, 05004], lr: 0.026608, loss: 1.2798
2022-08-08 08:48:09 - train: epoch 0078, iter [02200, 05004], lr: 0.026563, loss: 1.1423
2022-08-08 08:49:43 - train: epoch 0078, iter [02300, 05004], lr: 0.026518, loss: 1.2794
2022-08-08 08:51:16 - train: epoch 0078, iter [02400, 05004], lr: 0.026473, loss: 0.9974
2022-08-08 08:52:50 - train: epoch 0078, iter [02500, 05004], lr: 0.026429, loss: 1.2993
2022-08-08 08:54:24 - train: epoch 0078, iter [02600, 05004], lr: 0.026384, loss: 1.0103
2022-08-08 08:55:57 - train: epoch 0078, iter [02700, 05004], lr: 0.026339, loss: 1.0315
2022-08-08 08:57:31 - train: epoch 0078, iter [02800, 05004], lr: 0.026294, loss: 1.2518
2022-08-08 08:59:05 - train: epoch 0078, iter [02900, 05004], lr: 0.026250, loss: 1.2087
2022-08-08 09:00:39 - train: epoch 0078, iter [03000, 05004], lr: 0.026205, loss: 1.1547
2022-08-08 09:02:13 - train: epoch 0078, iter [03100, 05004], lr: 0.026161, loss: 1.0460
2022-08-08 09:03:47 - train: epoch 0078, iter [03200, 05004], lr: 0.026116, loss: 1.0883
2022-08-08 09:05:21 - train: epoch 0078, iter [03300, 05004], lr: 0.026071, loss: 1.1159
2022-08-08 09:06:55 - train: epoch 0078, iter [03400, 05004], lr: 0.026027, loss: 1.0196
2022-08-08 09:08:29 - train: epoch 0078, iter [03500, 05004], lr: 0.025983, loss: 1.3143
2022-08-08 09:10:03 - train: epoch 0078, iter [03600, 05004], lr: 0.025938, loss: 1.2384
2022-08-08 09:11:38 - train: epoch 0078, iter [03700, 05004], lr: 0.025894, loss: 0.9762
2022-08-08 09:13:12 - train: epoch 0078, iter [03800, 05004], lr: 0.025849, loss: 0.9848
2022-08-08 09:14:46 - train: epoch 0078, iter [03900, 05004], lr: 0.025805, loss: 1.0852
2022-08-08 09:16:20 - train: epoch 0078, iter [04000, 05004], lr: 0.025761, loss: 1.1312
2022-08-08 09:17:54 - train: epoch 0078, iter [04100, 05004], lr: 0.025716, loss: 1.2800
2022-08-08 09:19:28 - train: epoch 0078, iter [04200, 05004], lr: 0.025672, loss: 1.3103
2022-08-08 09:21:02 - train: epoch 0078, iter [04300, 05004], lr: 0.025628, loss: 1.1549
2022-08-08 09:22:36 - train: epoch 0078, iter [04400, 05004], lr: 0.025584, loss: 1.1184
2022-08-08 09:24:11 - train: epoch 0078, iter [04500, 05004], lr: 0.025540, loss: 1.2424
2022-08-08 09:25:44 - train: epoch 0078, iter [04600, 05004], lr: 0.025496, loss: 0.8783
2022-08-08 09:27:19 - train: epoch 0078, iter [04700, 05004], lr: 0.025452, loss: 1.0583
2022-08-08 09:28:53 - train: epoch 0078, iter [04800, 05004], lr: 0.025408, loss: 1.3558
2022-08-08 09:30:27 - train: epoch 0078, iter [04900, 05004], lr: 0.025364, loss: 1.1986
2022-08-08 09:32:01 - train: epoch 0078, iter [05000, 05004], lr: 0.025320, loss: 1.1756
2022-08-08 09:32:05 - train: epoch 078, train_loss: 1.1160
2022-08-08 09:34:06 - eval: epoch: 078, acc1: 73.562%, acc5: 91.714%, test_loss: 1.0650, per_image_load_time: 1.778ms, per_image_inference_time: 1.188ms
2022-08-08 09:34:06 - until epoch: 078, best_acc1: 73.562%
2022-08-08 09:34:06 - epoch 079 lr: 0.025317
2022-08-08 09:35:49 - train: epoch 0079, iter [00100, 05004], lr: 0.025274, loss: 0.9238
2022-08-08 09:37:23 - train: epoch 0079, iter [00200, 05004], lr: 0.025230, loss: 0.9326
2022-08-08 09:38:57 - train: epoch 0079, iter [00300, 05004], lr: 0.025186, loss: 1.1399
2022-08-08 09:40:31 - train: epoch 0079, iter [00400, 05004], lr: 0.025142, loss: 1.2227
2022-08-08 09:42:04 - train: epoch 0079, iter [00500, 05004], lr: 0.025099, loss: 1.0745
2022-08-08 09:43:39 - train: epoch 0079, iter [00600, 05004], lr: 0.025055, loss: 1.0058
2022-08-08 09:45:14 - train: epoch 0079, iter [00700, 05004], lr: 0.025011, loss: 1.1156
2022-08-08 09:46:48 - train: epoch 0079, iter [00800, 05004], lr: 0.024967, loss: 1.2514
2022-08-08 09:48:22 - train: epoch 0079, iter [00900, 05004], lr: 0.024924, loss: 1.0501
2022-08-08 09:49:56 - train: epoch 0079, iter [01000, 05004], lr: 0.024880, loss: 1.0471
2022-08-08 09:51:30 - train: epoch 0079, iter [01100, 05004], lr: 0.024836, loss: 1.1473
2022-08-08 09:53:04 - train: epoch 0079, iter [01200, 05004], lr: 0.024793, loss: 1.1740
2022-08-08 09:54:38 - train: epoch 0079, iter [01300, 05004], lr: 0.024749, loss: 1.0260
2022-08-08 09:56:12 - train: epoch 0079, iter [01400, 05004], lr: 0.024706, loss: 1.2794
2022-08-08 09:57:46 - train: epoch 0079, iter [01500, 05004], lr: 0.024662, loss: 1.0795
2022-08-08 09:59:21 - train: epoch 0079, iter [01600, 05004], lr: 0.024619, loss: 0.9498
2022-08-08 10:00:54 - train: epoch 0079, iter [01700, 05004], lr: 0.024575, loss: 1.0110
2022-08-08 10:02:28 - train: epoch 0079, iter [01800, 05004], lr: 0.024532, loss: 1.1532
2022-08-08 10:04:02 - train: epoch 0079, iter [01900, 05004], lr: 0.024489, loss: 1.0867
2022-08-08 10:05:36 - train: epoch 0079, iter [02000, 05004], lr: 0.024445, loss: 1.3367
2022-08-08 10:07:10 - train: epoch 0079, iter [02100, 05004], lr: 0.024402, loss: 1.1480
2022-08-08 10:08:44 - train: epoch 0079, iter [02200, 05004], lr: 0.024359, loss: 1.0971
2022-08-08 10:10:18 - train: epoch 0079, iter [02300, 05004], lr: 0.024316, loss: 1.1189
2022-08-08 10:11:52 - train: epoch 0079, iter [02400, 05004], lr: 0.024273, loss: 1.0615
2022-08-08 10:13:26 - train: epoch 0079, iter [02500, 05004], lr: 0.024229, loss: 1.1766
2022-08-08 10:15:00 - train: epoch 0079, iter [02600, 05004], lr: 0.024186, loss: 1.0884
2022-08-08 10:16:34 - train: epoch 0079, iter [02700, 05004], lr: 0.024143, loss: 0.9692
2022-08-08 10:18:08 - train: epoch 0079, iter [02800, 05004], lr: 0.024100, loss: 1.1890
2022-08-08 10:19:42 - train: epoch 0079, iter [02900, 05004], lr: 0.024057, loss: 1.0120
2022-08-08 10:21:16 - train: epoch 0079, iter [03000, 05004], lr: 0.024014, loss: 1.2141
2022-08-08 10:22:50 - train: epoch 0079, iter [03100, 05004], lr: 0.023971, loss: 1.1156
2022-08-08 10:24:24 - train: epoch 0079, iter [03200, 05004], lr: 0.023928, loss: 1.0325
2022-08-08 10:25:58 - train: epoch 0079, iter [03300, 05004], lr: 0.023885, loss: 1.0326
2022-08-08 10:27:32 - train: epoch 0079, iter [03400, 05004], lr: 0.023843, loss: 1.1549
2022-08-08 10:29:06 - train: epoch 0079, iter [03500, 05004], lr: 0.023800, loss: 1.1398
2022-08-08 10:30:40 - train: epoch 0079, iter [03600, 05004], lr: 0.023757, loss: 0.8529
2022-08-08 10:32:14 - train: epoch 0079, iter [03700, 05004], lr: 0.023714, loss: 0.9901
2022-08-08 10:33:48 - train: epoch 0079, iter [03800, 05004], lr: 0.023672, loss: 1.1850
2022-08-08 10:35:22 - train: epoch 0079, iter [03900, 05004], lr: 0.023629, loss: 1.0182
2022-08-08 10:36:56 - train: epoch 0079, iter [04000, 05004], lr: 0.023586, loss: 0.8642
2022-08-08 10:38:30 - train: epoch 0079, iter [04100, 05004], lr: 0.023544, loss: 1.3133
2022-08-08 10:40:04 - train: epoch 0079, iter [04200, 05004], lr: 0.023501, loss: 0.9898
2022-08-08 10:41:39 - train: epoch 0079, iter [04300, 05004], lr: 0.023458, loss: 0.9016
2022-08-08 10:43:13 - train: epoch 0079, iter [04400, 05004], lr: 0.023416, loss: 1.2475
2022-08-08 10:44:47 - train: epoch 0079, iter [04500, 05004], lr: 0.023373, loss: 1.2820
2022-08-08 10:46:21 - train: epoch 0079, iter [04600, 05004], lr: 0.023331, loss: 0.9929
2022-08-08 10:47:55 - train: epoch 0079, iter [04700, 05004], lr: 0.023289, loss: 0.9987
2022-08-08 10:49:29 - train: epoch 0079, iter [04800, 05004], lr: 0.023246, loss: 1.1122
2022-08-08 10:51:03 - train: epoch 0079, iter [04900, 05004], lr: 0.023204, loss: 1.1544
2022-08-08 10:52:38 - train: epoch 0079, iter [05000, 05004], lr: 0.023162, loss: 0.9890
2022-08-08 10:52:42 - train: epoch 079, train_loss: 1.0916
2022-08-08 10:54:43 - eval: epoch: 079, acc1: 73.918%, acc5: 91.942%, test_loss: 1.0487, per_image_load_time: 1.919ms, per_image_inference_time: 1.163ms
2022-08-08 10:54:43 - until epoch: 079, best_acc1: 73.918%
2022-08-08 10:54:43 - epoch 080 lr: 0.023159
2022-08-08 10:56:26 - train: epoch 0080, iter [00100, 05004], lr: 0.023118, loss: 1.0977
2022-08-08 10:58:00 - train: epoch 0080, iter [00200, 05004], lr: 0.023075, loss: 1.1673
2022-08-08 10:59:34 - train: epoch 0080, iter [00300, 05004], lr: 0.023033, loss: 1.0887
2022-08-08 11:01:08 - train: epoch 0080, iter [00400, 05004], lr: 0.022991, loss: 0.8266
2022-08-08 11:02:42 - train: epoch 0080, iter [00500, 05004], lr: 0.022949, loss: 0.9321
2022-08-08 11:04:16 - train: epoch 0080, iter [00600, 05004], lr: 0.022907, loss: 0.8274
2022-08-08 11:05:50 - train: epoch 0080, iter [00700, 05004], lr: 0.022865, loss: 1.1230
2022-08-08 11:07:24 - train: epoch 0080, iter [00800, 05004], lr: 0.022823, loss: 0.8922
2022-08-08 11:08:58 - train: epoch 0080, iter [00900, 05004], lr: 0.022781, loss: 0.9308
2022-08-08 11:10:32 - train: epoch 0080, iter [01000, 05004], lr: 0.022739, loss: 1.1708
2022-08-08 11:12:06 - train: epoch 0080, iter [01100, 05004], lr: 0.022697, loss: 1.0333
2022-08-08 11:13:40 - train: epoch 0080, iter [01200, 05004], lr: 0.022655, loss: 0.9494
2022-08-08 11:15:14 - train: epoch 0080, iter [01300, 05004], lr: 0.022613, loss: 0.8734
2022-08-08 11:16:49 - train: epoch 0080, iter [01400, 05004], lr: 0.022571, loss: 1.0891
2022-08-08 11:18:23 - train: epoch 0080, iter [01500, 05004], lr: 0.022529, loss: 0.9660
2022-08-08 11:19:57 - train: epoch 0080, iter [01600, 05004], lr: 0.022488, loss: 1.1632
2022-08-08 11:21:31 - train: epoch 0080, iter [01700, 05004], lr: 0.022446, loss: 1.1277
2022-08-08 11:23:05 - train: epoch 0080, iter [01800, 05004], lr: 0.022404, loss: 1.0832
2022-08-08 11:24:39 - train: epoch 0080, iter [01900, 05004], lr: 0.022362, loss: 1.0676
2022-08-08 11:26:13 - train: epoch 0080, iter [02000, 05004], lr: 0.022321, loss: 1.1558
2022-08-08 11:27:47 - train: epoch 0080, iter [02100, 05004], lr: 0.022279, loss: 1.2177
2022-08-08 11:29:22 - train: epoch 0080, iter [02200, 05004], lr: 0.022238, loss: 1.2052
2022-08-08 11:30:56 - train: epoch 0080, iter [02300, 05004], lr: 0.022196, loss: 1.0468
2022-08-08 11:32:30 - train: epoch 0080, iter [02400, 05004], lr: 0.022155, loss: 1.1162
2022-08-08 11:34:04 - train: epoch 0080, iter [02500, 05004], lr: 0.022113, loss: 0.9727
2022-08-08 11:35:38 - train: epoch 0080, iter [02600, 05004], lr: 0.022072, loss: 1.0645
2022-08-08 11:37:12 - train: epoch 0080, iter [02700, 05004], lr: 0.022030, loss: 1.0608
2022-08-08 11:38:46 - train: epoch 0080, iter [02800, 05004], lr: 0.021989, loss: 1.0142
2022-08-08 11:40:20 - train: epoch 0080, iter [02900, 05004], lr: 0.021948, loss: 1.1773
2022-08-08 11:41:54 - train: epoch 0080, iter [03000, 05004], lr: 0.021906, loss: 1.0263
2022-08-08 11:43:28 - train: epoch 0080, iter [03100, 05004], lr: 0.021865, loss: 1.0964
2022-08-08 11:45:02 - train: epoch 0080, iter [03200, 05004], lr: 0.021824, loss: 0.9321
2022-08-08 11:46:36 - train: epoch 0080, iter [03300, 05004], lr: 0.021783, loss: 1.3414
2022-08-08 11:48:10 - train: epoch 0080, iter [03400, 05004], lr: 0.021741, loss: 0.9581
2022-08-08 11:49:44 - train: epoch 0080, iter [03500, 05004], lr: 0.021700, loss: 1.1293
2022-08-08 11:51:18 - train: epoch 0080, iter [03600, 05004], lr: 0.021659, loss: 1.0285
2022-08-08 11:52:52 - train: epoch 0080, iter [03700, 05004], lr: 0.021618, loss: 1.0920
2022-08-08 11:54:26 - train: epoch 0080, iter [03800, 05004], lr: 0.021577, loss: 1.0622
2022-08-08 11:56:01 - train: epoch 0080, iter [03900, 05004], lr: 0.021536, loss: 1.0693
2022-08-08 11:57:35 - train: epoch 0080, iter [04000, 05004], lr: 0.021495, loss: 1.1700
2022-08-08 11:59:09 - train: epoch 0080, iter [04100, 05004], lr: 0.021454, loss: 1.4143
2022-08-08 12:00:43 - train: epoch 0080, iter [04200, 05004], lr: 0.021413, loss: 1.1519
2022-08-08 12:02:17 - train: epoch 0080, iter [04300, 05004], lr: 0.021373, loss: 1.2126
2022-08-08 12:03:51 - train: epoch 0080, iter [04400, 05004], lr: 0.021332, loss: 1.1553
2022-08-08 12:05:25 - train: epoch 0080, iter [04500, 05004], lr: 0.021291, loss: 0.8546
2022-08-08 12:06:59 - train: epoch 0080, iter [04600, 05004], lr: 0.021250, loss: 0.9998
2022-08-08 12:08:33 - train: epoch 0080, iter [04700, 05004], lr: 0.021210, loss: 1.1044
2022-08-08 12:10:07 - train: epoch 0080, iter [04800, 05004], lr: 0.021169, loss: 1.3606
2022-08-08 12:11:41 - train: epoch 0080, iter [04900, 05004], lr: 0.021128, loss: 1.2630
2022-08-08 12:13:15 - train: epoch 0080, iter [05000, 05004], lr: 0.021088, loss: 1.0027
2022-08-08 12:13:20 - train: epoch 080, train_loss: 1.0634
2022-08-08 12:15:21 - eval: epoch: 080, acc1: 74.248%, acc5: 92.228%, test_loss: 1.0357, per_image_load_time: 1.461ms, per_image_inference_time: 1.170ms
2022-08-08 12:15:22 - until epoch: 080, best_acc1: 74.248%
2022-08-08 12:15:22 - epoch 081 lr: 0.021086
2022-08-08 12:17:04 - train: epoch 0081, iter [00100, 05004], lr: 0.021045, loss: 0.9284
2022-08-08 12:18:38 - train: epoch 0081, iter [00200, 05004], lr: 0.021005, loss: 1.0963
2022-08-08 12:20:12 - train: epoch 0081, iter [00300, 05004], lr: 0.020964, loss: 1.0596
2022-08-08 12:21:46 - train: epoch 0081, iter [00400, 05004], lr: 0.020924, loss: 1.0217
2022-08-08 12:23:20 - train: epoch 0081, iter [00500, 05004], lr: 0.020883, loss: 1.1466
2022-08-08 12:24:54 - train: epoch 0081, iter [00600, 05004], lr: 0.020843, loss: 1.1849
2022-08-08 12:26:28 - train: epoch 0081, iter [00700, 05004], lr: 0.020803, loss: 1.0594
2022-08-08 12:28:02 - train: epoch 0081, iter [00800, 05004], lr: 0.020762, loss: 0.9751
2022-08-08 12:29:36 - train: epoch 0081, iter [00900, 05004], lr: 0.020722, loss: 1.1392
2022-08-08 12:31:10 - train: epoch 0081, iter [01000, 05004], lr: 0.020682, loss: 1.0380
2022-08-08 12:32:44 - train: epoch 0081, iter [01100, 05004], lr: 0.020642, loss: 0.9180
2022-08-08 12:34:18 - train: epoch 0081, iter [01200, 05004], lr: 0.020601, loss: 1.1189
2022-08-08 12:35:52 - train: epoch 0081, iter [01300, 05004], lr: 0.020561, loss: 1.0264
2022-08-08 12:37:26 - train: epoch 0081, iter [01400, 05004], lr: 0.020521, loss: 1.1125
2022-08-08 12:39:00 - train: epoch 0081, iter [01500, 05004], lr: 0.020481, loss: 1.2203
2022-08-08 12:40:34 - train: epoch 0081, iter [01600, 05004], lr: 0.020441, loss: 1.0094
2022-08-08 12:42:07 - train: epoch 0081, iter [01700, 05004], lr: 0.020401, loss: 0.9105
2022-08-08 12:43:41 - train: epoch 0081, iter [01800, 05004], lr: 0.020361, loss: 1.1206
2022-08-08 12:45:15 - train: epoch 0081, iter [01900, 05004], lr: 0.020321, loss: 1.0695
2022-08-08 12:46:49 - train: epoch 0081, iter [02000, 05004], lr: 0.020281, loss: 1.0492
2022-08-08 12:48:23 - train: epoch 0081, iter [02100, 05004], lr: 0.020241, loss: 1.1049
2022-08-08 12:49:57 - train: epoch 0081, iter [02200, 05004], lr: 0.020201, loss: 0.9307
2022-08-08 12:51:31 - train: epoch 0081, iter [02300, 05004], lr: 0.020162, loss: 0.9754
2022-08-08 12:53:05 - train: epoch 0081, iter [02400, 05004], lr: 0.020122, loss: 1.0714
2022-08-08 12:54:39 - train: epoch 0081, iter [02500, 05004], lr: 0.020082, loss: 1.1375
2022-08-08 12:56:13 - train: epoch 0081, iter [02600, 05004], lr: 0.020042, loss: 1.2903
2022-08-08 12:57:47 - train: epoch 0081, iter [02700, 05004], lr: 0.020003, loss: 1.1179
2022-08-08 12:59:21 - train: epoch 0081, iter [02800, 05004], lr: 0.019963, loss: 1.1013
2022-08-08 13:00:55 - train: epoch 0081, iter [02900, 05004], lr: 0.019923, loss: 1.0385
2022-08-08 13:02:29 - train: epoch 0081, iter [03000, 05004], lr: 0.019884, loss: 0.9813
2022-08-08 13:04:03 - train: epoch 0081, iter [03100, 05004], lr: 0.019844, loss: 1.0830
2022-08-08 13:05:37 - train: epoch 0081, iter [03200, 05004], lr: 0.019805, loss: 0.8797
2022-08-08 13:07:11 - train: epoch 0081, iter [03300, 05004], lr: 0.019765, loss: 0.9750
2022-08-08 13:08:45 - train: epoch 0081, iter [03400, 05004], lr: 0.019726, loss: 1.1708
2022-08-08 13:10:19 - train: epoch 0081, iter [03500, 05004], lr: 0.019687, loss: 1.2056
2022-08-08 13:11:54 - train: epoch 0081, iter [03600, 05004], lr: 0.019647, loss: 0.9450
2022-08-08 13:13:28 - train: epoch 0081, iter [03700, 05004], lr: 0.019608, loss: 1.0611
2022-08-08 13:15:02 - train: epoch 0081, iter [03800, 05004], lr: 0.019569, loss: 0.8501
2022-08-08 13:16:36 - train: epoch 0081, iter [03900, 05004], lr: 0.019529, loss: 1.1321
2022-08-08 13:18:10 - train: epoch 0081, iter [04000, 05004], lr: 0.019490, loss: 0.8889
2022-08-08 13:19:45 - train: epoch 0081, iter [04100, 05004], lr: 0.019451, loss: 1.0055
2022-08-08 13:21:19 - train: epoch 0081, iter [04200, 05004], lr: 0.019412, loss: 1.0832
2022-08-08 13:22:53 - train: epoch 0081, iter [04300, 05004], lr: 0.019373, loss: 1.0947
2022-08-08 13:24:27 - train: epoch 0081, iter [04400, 05004], lr: 0.019334, loss: 1.0510
2022-08-08 13:26:01 - train: epoch 0081, iter [04500, 05004], lr: 0.019295, loss: 1.2219
2022-08-08 13:27:35 - train: epoch 0081, iter [04600, 05004], lr: 0.019256, loss: 1.0965
2022-08-08 13:29:09 - train: epoch 0081, iter [04700, 05004], lr: 0.019217, loss: 1.2341
2022-08-08 13:30:43 - train: epoch 0081, iter [04800, 05004], lr: 0.019178, loss: 1.0738
2022-08-08 13:32:18 - train: epoch 0081, iter [04900, 05004], lr: 0.019139, loss: 1.1490
2022-08-08 13:33:52 - train: epoch 0081, iter [05000, 05004], lr: 0.019100, loss: 0.9847
2022-08-08 13:33:56 - train: epoch 081, train_loss: 1.0396
2022-08-08 13:35:59 - eval: epoch: 081, acc1: 74.778%, acc5: 92.214%, test_loss: 1.0285, per_image_load_time: 1.529ms, per_image_inference_time: 1.170ms
2022-08-08 13:35:59 - until epoch: 081, best_acc1: 74.778%
2022-08-08 13:35:59 - epoch 082 lr: 0.019098
2022-08-08 13:37:42 - train: epoch 0082, iter [00100, 05004], lr: 0.019059, loss: 0.9707
2022-08-08 13:39:17 - train: epoch 0082, iter [00200, 05004], lr: 0.019021, loss: 0.9484
2022-08-08 13:40:51 - train: epoch 0082, iter [00300, 05004], lr: 0.018982, loss: 1.0470
2022-08-08 13:42:25 - train: epoch 0082, iter [00400, 05004], lr: 0.018943, loss: 1.0007
2022-08-08 13:43:59 - train: epoch 0082, iter [00500, 05004], lr: 0.018905, loss: 1.0778
2022-08-08 13:45:33 - train: epoch 0082, iter [00600, 05004], lr: 0.018866, loss: 0.9235
2022-08-08 13:47:07 - train: epoch 0082, iter [00700, 05004], lr: 0.018827, loss: 1.2010
2022-08-08 13:48:41 - train: epoch 0082, iter [00800, 05004], lr: 0.018789, loss: 1.1074
2022-08-08 13:50:15 - train: epoch 0082, iter [00900, 05004], lr: 0.018750, loss: 1.2009
2022-08-08 13:51:49 - train: epoch 0082, iter [01000, 05004], lr: 0.018712, loss: 0.8616
2022-08-08 13:53:23 - train: epoch 0082, iter [01100, 05004], lr: 0.018673, loss: 0.9900
2022-08-08 13:54:57 - train: epoch 0082, iter [01200, 05004], lr: 0.018635, loss: 0.9339
2022-08-08 13:56:31 - train: epoch 0082, iter [01300, 05004], lr: 0.018596, loss: 1.0833
2022-08-08 13:58:05 - train: epoch 0082, iter [01400, 05004], lr: 0.018558, loss: 1.1015
2022-08-08 13:59:39 - train: epoch 0082, iter [01500, 05004], lr: 0.018520, loss: 0.9624
2022-08-08 14:01:13 - train: epoch 0082, iter [01600, 05004], lr: 0.018481, loss: 0.9178
2022-08-08 14:02:47 - train: epoch 0082, iter [01700, 05004], lr: 0.018443, loss: 0.9602
2022-08-08 14:04:21 - train: epoch 0082, iter [01800, 05004], lr: 0.018405, loss: 1.0032
2022-08-08 14:05:55 - train: epoch 0082, iter [01900, 05004], lr: 0.018367, loss: 0.9473
2022-08-08 14:07:29 - train: epoch 0082, iter [02000, 05004], lr: 0.018329, loss: 0.9270
2022-08-08 14:09:03 - train: epoch 0082, iter [02100, 05004], lr: 0.018290, loss: 1.2107
2022-08-08 14:10:37 - train: epoch 0082, iter [02200, 05004], lr: 0.018252, loss: 1.0564
2022-08-08 14:12:11 - train: epoch 0082, iter [02300, 05004], lr: 0.018214, loss: 0.9186
2022-08-08 14:13:45 - train: epoch 0082, iter [02400, 05004], lr: 0.018176, loss: 0.9669
2022-08-08 14:15:19 - train: epoch 0082, iter [02500, 05004], lr: 0.018138, loss: 0.9180
2022-08-08 14:16:53 - train: epoch 0082, iter [02600, 05004], lr: 0.018100, loss: 0.9562
2022-08-08 14:18:27 - train: epoch 0082, iter [02700, 05004], lr: 0.018062, loss: 0.8651
2022-08-08 14:20:01 - train: epoch 0082, iter [02800, 05004], lr: 0.018025, loss: 1.0520
2022-08-08 14:21:35 - train: epoch 0082, iter [02900, 05004], lr: 0.017987, loss: 0.8793
2022-08-08 14:23:09 - train: epoch 0082, iter [03000, 05004], lr: 0.017949, loss: 0.9162
2022-08-08 14:24:43 - train: epoch 0082, iter [03100, 05004], lr: 0.017911, loss: 1.0440
2022-08-08 14:26:17 - train: epoch 0082, iter [03200, 05004], lr: 0.017873, loss: 1.2434
2022-08-08 14:27:52 - train: epoch 0082, iter [03300, 05004], lr: 0.017836, loss: 1.0655
2022-08-08 14:29:26 - train: epoch 0082, iter [03400, 05004], lr: 0.017798, loss: 1.0192
2022-08-08 14:31:00 - train: epoch 0082, iter [03500, 05004], lr: 0.017761, loss: 1.0484
2022-08-08 14:32:34 - train: epoch 0082, iter [03600, 05004], lr: 0.017723, loss: 1.0553
2022-08-08 14:34:08 - train: epoch 0082, iter [03700, 05004], lr: 0.017685, loss: 1.0246
2022-08-08 14:35:43 - train: epoch 0082, iter [03800, 05004], lr: 0.017648, loss: 1.1368
2022-08-08 14:37:17 - train: epoch 0082, iter [03900, 05004], lr: 0.017610, loss: 1.0826
2022-08-08 14:38:51 - train: epoch 0082, iter [04000, 05004], lr: 0.017573, loss: 0.9127
2022-08-08 14:40:25 - train: epoch 0082, iter [04100, 05004], lr: 0.017536, loss: 0.9332
2022-08-08 14:42:00 - train: epoch 0082, iter [04200, 05004], lr: 0.017498, loss: 0.9659
2022-08-08 14:43:34 - train: epoch 0082, iter [04300, 05004], lr: 0.017461, loss: 0.8778
2022-08-08 14:45:09 - train: epoch 0082, iter [04400, 05004], lr: 0.017424, loss: 1.2362
2022-08-08 14:46:43 - train: epoch 0082, iter [04500, 05004], lr: 0.017386, loss: 0.8820
2022-08-08 14:48:17 - train: epoch 0082, iter [04600, 05004], lr: 0.017349, loss: 0.8551
2022-08-08 14:49:51 - train: epoch 0082, iter [04700, 05004], lr: 0.017312, loss: 0.9070
2022-08-08 14:51:25 - train: epoch 0082, iter [04800, 05004], lr: 0.017275, loss: 0.7913
2022-08-08 14:52:59 - train: epoch 0082, iter [04900, 05004], lr: 0.017238, loss: 1.0261
2022-08-08 14:54:34 - train: epoch 0082, iter [05000, 05004], lr: 0.017201, loss: 0.9302
2022-08-08 14:54:38 - train: epoch 082, train_loss: 1.0125
2022-08-08 14:56:39 - eval: epoch: 082, acc1: 74.808%, acc5: 92.350%, test_loss: 1.0098, per_image_load_time: 1.280ms, per_image_inference_time: 1.171ms
2022-08-08 14:56:40 - until epoch: 082, best_acc1: 74.808%
2022-08-08 14:56:40 - epoch 083 lr: 0.017199
2022-08-08 14:58:23 - train: epoch 0083, iter [00100, 05004], lr: 0.017162, loss: 0.8171
2022-08-08 14:59:57 - train: epoch 0083, iter [00200, 05004], lr: 0.017125, loss: 0.8546
2022-08-08 15:01:32 - train: epoch 0083, iter [00300, 05004], lr: 0.017088, loss: 1.0553
2022-08-08 15:03:07 - train: epoch 0083, iter [00400, 05004], lr: 0.017051, loss: 1.1844
2022-08-08 15:04:41 - train: epoch 0083, iter [00500, 05004], lr: 0.017014, loss: 0.9562
2022-08-08 15:06:15 - train: epoch 0083, iter [00600, 05004], lr: 0.016977, loss: 0.8136
2022-08-08 15:07:49 - train: epoch 0083, iter [00700, 05004], lr: 0.016941, loss: 0.9196
2022-08-08 15:09:23 - train: epoch 0083, iter [00800, 05004], lr: 0.016904, loss: 1.0104
2022-08-08 15:10:58 - train: epoch 0083, iter [00900, 05004], lr: 0.016867, loss: 1.0728
2022-08-08 15:12:32 - train: epoch 0083, iter [01000, 05004], lr: 0.016830, loss: 1.2626
2022-08-08 15:14:06 - train: epoch 0083, iter [01100, 05004], lr: 0.016794, loss: 0.9959
2022-08-08 15:15:40 - train: epoch 0083, iter [01200, 05004], lr: 0.016757, loss: 0.9307
2022-08-08 15:17:15 - train: epoch 0083, iter [01300, 05004], lr: 0.016720, loss: 1.0899
2022-08-08 15:18:49 - train: epoch 0083, iter [01400, 05004], lr: 0.016684, loss: 1.0512
2022-08-08 15:20:23 - train: epoch 0083, iter [01500, 05004], lr: 0.016647, loss: 0.9498
2022-08-08 15:21:58 - train: epoch 0083, iter [01600, 05004], lr: 0.016611, loss: 0.9329
2022-08-08 15:23:32 - train: epoch 0083, iter [01700, 05004], lr: 0.016574, loss: 0.9900
2022-08-08 15:25:06 - train: epoch 0083, iter [01800, 05004], lr: 0.016538, loss: 1.1305
2022-08-08 15:26:40 - train: epoch 0083, iter [01900, 05004], lr: 0.016502, loss: 0.9239
2022-08-08 15:28:15 - train: epoch 0083, iter [02000, 05004], lr: 0.016465, loss: 0.8204
2022-08-08 15:29:49 - train: epoch 0083, iter [02100, 05004], lr: 0.016429, loss: 0.8772
2022-08-08 15:31:23 - train: epoch 0083, iter [02200, 05004], lr: 0.016393, loss: 0.9832
2022-08-08 15:32:57 - train: epoch 0083, iter [02300, 05004], lr: 0.016356, loss: 1.1338
2022-08-08 15:34:32 - train: epoch 0083, iter [02400, 05004], lr: 0.016320, loss: 1.0087
2022-08-08 15:36:06 - train: epoch 0083, iter [02500, 05004], lr: 0.016284, loss: 1.1228
2022-08-08 15:37:40 - train: epoch 0083, iter [02600, 05004], lr: 0.016248, loss: 0.9149
2022-08-08 15:39:14 - train: epoch 0083, iter [02700, 05004], lr: 0.016212, loss: 0.9658
2022-08-08 15:40:49 - train: epoch 0083, iter [02800, 05004], lr: 0.016176, loss: 0.8803
2022-08-08 15:42:23 - train: epoch 0083, iter [02900, 05004], lr: 0.016140, loss: 0.9414
2022-08-08 15:43:57 - train: epoch 0083, iter [03000, 05004], lr: 0.016104, loss: 1.1021
2022-08-08 15:45:31 - train: epoch 0083, iter [03100, 05004], lr: 0.016068, loss: 0.9740
2022-08-08 15:47:05 - train: epoch 0083, iter [03200, 05004], lr: 0.016032, loss: 0.9125
2022-08-08 15:48:40 - train: epoch 0083, iter [03300, 05004], lr: 0.015996, loss: 1.1528
2022-08-08 15:50:14 - train: epoch 0083, iter [03400, 05004], lr: 0.015960, loss: 0.9264
2022-08-08 15:51:48 - train: epoch 0083, iter [03500, 05004], lr: 0.015924, loss: 1.0584
2022-08-08 15:53:22 - train: epoch 0083, iter [03600, 05004], lr: 0.015889, loss: 1.0806
2022-08-08 15:54:57 - train: epoch 0083, iter [03700, 05004], lr: 0.015853, loss: 0.9243
2022-08-08 15:56:31 - train: epoch 0083, iter [03800, 05004], lr: 0.015817, loss: 1.0475
2022-08-08 15:58:05 - train: epoch 0083, iter [03900, 05004], lr: 0.015782, loss: 1.0156
2022-08-08 15:59:40 - train: epoch 0083, iter [04000, 05004], lr: 0.015746, loss: 1.0846
2022-08-08 16:01:14 - train: epoch 0083, iter [04100, 05004], lr: 0.015710, loss: 0.9042
2022-08-08 16:02:48 - train: epoch 0083, iter [04200, 05004], lr: 0.015675, loss: 1.1564
2022-08-08 16:04:22 - train: epoch 0083, iter [04300, 05004], lr: 0.015639, loss: 0.8797
2022-08-08 16:05:57 - train: epoch 0083, iter [04400, 05004], lr: 0.015604, loss: 1.0513
2022-08-08 16:07:31 - train: epoch 0083, iter [04500, 05004], lr: 0.015568, loss: 1.0460
2022-08-08 16:09:05 - train: epoch 0083, iter [04600, 05004], lr: 0.015533, loss: 1.0600
2022-08-08 16:10:39 - train: epoch 0083, iter [04700, 05004], lr: 0.015498, loss: 1.2235
2022-08-08 16:12:13 - train: epoch 0083, iter [04800, 05004], lr: 0.015462, loss: 0.9716
2022-08-08 16:13:47 - train: epoch 0083, iter [04900, 05004], lr: 0.015427, loss: 0.9953
2022-08-08 16:15:22 - train: epoch 0083, iter [05000, 05004], lr: 0.015392, loss: 1.1180
2022-08-08 16:15:26 - train: epoch 083, train_loss: 0.9880
2022-08-08 16:17:29 - eval: epoch: 083, acc1: 75.190%, acc5: 92.434%, test_loss: 1.0064, per_image_load_time: 1.546ms, per_image_inference_time: 1.101ms
2022-08-08 16:17:29 - until epoch: 083, best_acc1: 75.190%
2022-08-08 16:17:29 - epoch 084 lr: 0.015390
2022-08-08 16:19:12 - train: epoch 0084, iter [00100, 05004], lr: 0.015355, loss: 0.9389
2022-08-08 16:20:46 - train: epoch 0084, iter [00200, 05004], lr: 0.015320, loss: 1.0505
2022-08-08 16:22:21 - train: epoch 0084, iter [00300, 05004], lr: 0.015285, loss: 0.7519
2022-08-08 16:23:55 - train: epoch 0084, iter [00400, 05004], lr: 0.015250, loss: 0.7787
2022-08-08 16:25:30 - train: epoch 0084, iter [00500, 05004], lr: 0.015215, loss: 1.0697
2022-08-08 16:27:04 - train: epoch 0084, iter [00600, 05004], lr: 0.015180, loss: 1.1085
2022-08-08 16:28:38 - train: epoch 0084, iter [00700, 05004], lr: 0.015145, loss: 1.0004
2022-08-08 16:30:12 - train: epoch 0084, iter [00800, 05004], lr: 0.015110, loss: 1.0790
2022-08-08 16:31:47 - train: epoch 0084, iter [00900, 05004], lr: 0.015075, loss: 1.0194
2022-08-08 16:33:21 - train: epoch 0084, iter [01000, 05004], lr: 0.015040, loss: 0.9419
2022-08-08 16:34:55 - train: epoch 0084, iter [01100, 05004], lr: 0.015005, loss: 0.8607
2022-08-08 16:36:29 - train: epoch 0084, iter [01200, 05004], lr: 0.014970, loss: 1.2134
2022-08-08 16:38:04 - train: epoch 0084, iter [01300, 05004], lr: 0.014936, loss: 1.0866
2022-08-08 16:39:38 - train: epoch 0084, iter [01400, 05004], lr: 0.014901, loss: 0.8827
2022-08-08 16:41:12 - train: epoch 0084, iter [01500, 05004], lr: 0.014866, loss: 0.8050
2022-08-08 16:42:46 - train: epoch 0084, iter [01600, 05004], lr: 0.014832, loss: 0.9022
2022-08-08 16:44:20 - train: epoch 0084, iter [01700, 05004], lr: 0.014797, loss: 1.0851
2022-08-08 16:45:55 - train: epoch 0084, iter [01800, 05004], lr: 0.014762, loss: 1.1131
2022-08-08 16:47:29 - train: epoch 0084, iter [01900, 05004], lr: 0.014728, loss: 1.1074
2022-08-08 16:49:03 - train: epoch 0084, iter [02000, 05004], lr: 0.014693, loss: 0.8543
2022-08-08 16:50:37 - train: epoch 0084, iter [02100, 05004], lr: 0.014659, loss: 0.9484
2022-08-08 16:52:12 - train: epoch 0084, iter [02200, 05004], lr: 0.014624, loss: 0.7388
2022-08-08 16:53:46 - train: epoch 0084, iter [02300, 05004], lr: 0.014590, loss: 0.9162
2022-08-08 16:55:20 - train: epoch 0084, iter [02400, 05004], lr: 0.014556, loss: 0.9029
2022-08-08 16:56:54 - train: epoch 0084, iter [02500, 05004], lr: 0.014521, loss: 1.0215
2022-08-08 16:58:28 - train: epoch 0084, iter [02600, 05004], lr: 0.014487, loss: 0.9183
2022-08-08 17:00:02 - train: epoch 0084, iter [02700, 05004], lr: 0.014453, loss: 0.9466
2022-08-08 17:01:36 - train: epoch 0084, iter [02800, 05004], lr: 0.014419, loss: 0.9829
2022-08-08 17:03:09 - train: epoch 0084, iter [02900, 05004], lr: 0.014385, loss: 0.9201
2022-08-08 17:04:43 - train: epoch 0084, iter [03000, 05004], lr: 0.014350, loss: 0.9853
2022-08-08 17:06:17 - train: epoch 0084, iter [03100, 05004], lr: 0.014316, loss: 1.1485
2022-08-08 17:07:51 - train: epoch 0084, iter [03200, 05004], lr: 0.014282, loss: 0.8652
2022-08-08 17:09:25 - train: epoch 0084, iter [03300, 05004], lr: 0.014248, loss: 0.9236
2022-08-08 17:10:59 - train: epoch 0084, iter [03400, 05004], lr: 0.014214, loss: 0.9118
2022-08-08 17:12:33 - train: epoch 0084, iter [03500, 05004], lr: 0.014180, loss: 0.7923
2022-08-08 17:14:07 - train: epoch 0084, iter [03600, 05004], lr: 0.014146, loss: 0.9688
2022-08-08 17:15:41 - train: epoch 0084, iter [03700, 05004], lr: 0.014113, loss: 1.2016
2022-08-08 17:17:15 - train: epoch 0084, iter [03800, 05004], lr: 0.014079, loss: 1.0985
2022-08-08 17:18:49 - train: epoch 0084, iter [03900, 05004], lr: 0.014045, loss: 1.1871
2022-08-08 17:20:23 - train: epoch 0084, iter [04000, 05004], lr: 0.014011, loss: 0.8804
2022-08-08 17:21:57 - train: epoch 0084, iter [04100, 05004], lr: 0.013977, loss: 1.0223
2022-08-08 17:23:31 - train: epoch 0084, iter [04200, 05004], lr: 0.013944, loss: 0.9941
2022-08-08 17:25:05 - train: epoch 0084, iter [04300, 05004], lr: 0.013910, loss: 1.0788
2022-08-08 17:26:39 - train: epoch 0084, iter [04400, 05004], lr: 0.013877, loss: 1.1312
2022-08-08 17:28:13 - train: epoch 0084, iter [04500, 05004], lr: 0.013843, loss: 1.0590
2022-08-08 17:29:47 - train: epoch 0084, iter [04600, 05004], lr: 0.013809, loss: 0.9229
2022-08-08 17:31:21 - train: epoch 0084, iter [04700, 05004], lr: 0.013776, loss: 0.8652
2022-08-08 17:32:55 - train: epoch 0084, iter [04800, 05004], lr: 0.013742, loss: 0.9850
2022-08-08 17:34:29 - train: epoch 0084, iter [04900, 05004], lr: 0.013709, loss: 0.7272
2022-08-08 17:36:03 - train: epoch 0084, iter [05000, 05004], lr: 0.013676, loss: 1.0405
2022-08-08 17:36:07 - train: epoch 084, train_loss: 0.9615
2022-08-08 17:38:10 - eval: epoch: 084, acc1: 75.224%, acc5: 92.532%, test_loss: 1.0064, per_image_load_time: 2.749ms, per_image_inference_time: 1.175ms
2022-08-08 17:38:10 - until epoch: 084, best_acc1: 75.224%
2022-08-08 17:38:10 - epoch 085 lr: 0.013674
2022-08-08 17:39:53 - train: epoch 0085, iter [00100, 05004], lr: 0.013641, loss: 0.8471
2022-08-08 17:41:27 - train: epoch 0085, iter [00200, 05004], lr: 0.013608, loss: 0.7893
2022-08-08 17:43:01 - train: epoch 0085, iter [00300, 05004], lr: 0.013574, loss: 1.0301
2022-08-08 17:44:34 - train: epoch 0085, iter [00400, 05004], lr: 0.013541, loss: 0.8828
2022-08-08 17:46:08 - train: epoch 0085, iter [00500, 05004], lr: 0.013508, loss: 1.0631
2022-08-08 17:47:41 - train: epoch 0085, iter [00600, 05004], lr: 0.013475, loss: 0.7542
2022-08-08 17:49:15 - train: epoch 0085, iter [00700, 05004], lr: 0.013442, loss: 0.8903
2022-08-08 17:50:49 - train: epoch 0085, iter [00800, 05004], lr: 0.013409, loss: 0.6798
2022-08-08 17:52:22 - train: epoch 0085, iter [00900, 05004], lr: 0.013376, loss: 0.8754
2022-08-08 17:53:56 - train: epoch 0085, iter [01000, 05004], lr: 0.013343, loss: 0.9391
2022-08-08 17:55:30 - train: epoch 0085, iter [01100, 05004], lr: 0.013310, loss: 0.9065
2022-08-08 17:57:03 - train: epoch 0085, iter [01200, 05004], lr: 0.013277, loss: 0.8954
2022-08-08 17:58:37 - train: epoch 0085, iter [01300, 05004], lr: 0.013244, loss: 1.0908
2022-08-08 18:00:10 - train: epoch 0085, iter [01400, 05004], lr: 0.013211, loss: 0.7853
2022-08-08 18:01:44 - train: epoch 0085, iter [01500, 05004], lr: 0.013178, loss: 0.9745
2022-08-08 18:03:17 - train: epoch 0085, iter [01600, 05004], lr: 0.013145, loss: 0.8858
2022-08-08 18:04:50 - train: epoch 0085, iter [01700, 05004], lr: 0.013113, loss: 1.2125
2022-08-08 18:06:24 - train: epoch 0085, iter [01800, 05004], lr: 0.013080, loss: 0.7216
2022-08-08 18:07:58 - train: epoch 0085, iter [01900, 05004], lr: 0.013047, loss: 0.7734
2022-08-08 18:09:31 - train: epoch 0085, iter [02000, 05004], lr: 0.013015, loss: 0.8628
2022-08-08 18:11:05 - train: epoch 0085, iter [02100, 05004], lr: 0.012982, loss: 0.8408
2022-08-08 18:12:38 - train: epoch 0085, iter [02200, 05004], lr: 0.012950, loss: 1.1379
2022-08-08 18:14:12 - train: epoch 0085, iter [02300, 05004], lr: 0.012917, loss: 0.8478
2022-08-08 18:15:46 - train: epoch 0085, iter [02400, 05004], lr: 0.012885, loss: 0.9491
2022-08-08 18:17:19 - train: epoch 0085, iter [02500, 05004], lr: 0.012852, loss: 0.9491
2022-08-08 18:18:53 - train: epoch 0085, iter [02600, 05004], lr: 0.012820, loss: 1.1533
2022-08-08 18:20:27 - train: epoch 0085, iter [02700, 05004], lr: 0.012787, loss: 0.6712
2022-08-08 18:22:01 - train: epoch 0085, iter [02800, 05004], lr: 0.012755, loss: 1.1931
2022-08-08 18:23:34 - train: epoch 0085, iter [02900, 05004], lr: 0.012723, loss: 0.9599
2022-08-08 18:25:08 - train: epoch 0085, iter [03000, 05004], lr: 0.012691, loss: 1.0732
2022-08-08 18:26:41 - train: epoch 0085, iter [03100, 05004], lr: 0.012658, loss: 0.8890
2022-08-08 18:28:15 - train: epoch 0085, iter [03200, 05004], lr: 0.012626, loss: 1.1388
2022-08-08 18:29:49 - train: epoch 0085, iter [03300, 05004], lr: 0.012594, loss: 0.8843
2022-08-08 18:31:22 - train: epoch 0085, iter [03400, 05004], lr: 0.012562, loss: 0.9662
2022-08-08 18:32:56 - train: epoch 0085, iter [03500, 05004], lr: 0.012530, loss: 1.0485
2022-08-08 18:34:30 - train: epoch 0085, iter [03600, 05004], lr: 0.012498, loss: 0.7865
2022-08-08 18:36:04 - train: epoch 0085, iter [03700, 05004], lr: 0.012466, loss: 0.8472
2022-08-08 18:37:37 - train: epoch 0085, iter [03800, 05004], lr: 0.012434, loss: 0.9637
2022-08-08 18:39:11 - train: epoch 0085, iter [03900, 05004], lr: 0.012402, loss: 1.0461
2022-08-08 18:40:45 - train: epoch 0085, iter [04000, 05004], lr: 0.012370, loss: 1.0469
2022-08-08 18:42:19 - train: epoch 0085, iter [04100, 05004], lr: 0.012339, loss: 0.9066
2022-08-08 18:43:52 - train: epoch 0085, iter [04200, 05004], lr: 0.012307, loss: 0.8919
2022-08-08 18:45:26 - train: epoch 0085, iter [04300, 05004], lr: 0.012275, loss: 0.8304
2022-08-08 18:47:00 - train: epoch 0085, iter [04400, 05004], lr: 0.012243, loss: 0.9033
2022-08-08 18:48:34 - train: epoch 0085, iter [04500, 05004], lr: 0.012212, loss: 1.0017
2022-08-08 18:50:08 - train: epoch 0085, iter [04600, 05004], lr: 0.012180, loss: 0.9453
2022-08-08 18:51:41 - train: epoch 0085, iter [04700, 05004], lr: 0.012148, loss: 1.2541
2022-08-08 18:53:15 - train: epoch 0085, iter [04800, 05004], lr: 0.012117, loss: 0.8804
2022-08-08 18:54:49 - train: epoch 0085, iter [04900, 05004], lr: 0.012085, loss: 0.8900
2022-08-08 18:56:22 - train: epoch 0085, iter [05000, 05004], lr: 0.012054, loss: 0.8224
2022-08-08 18:56:27 - train: epoch 085, train_loss: 0.9331
2022-08-08 18:58:32 - eval: epoch: 085, acc1: 75.742%, acc5: 92.812%, test_loss: 0.9838, per_image_load_time: 2.062ms, per_image_inference_time: 1.220ms
2022-08-08 18:58:32 - until epoch: 085, best_acc1: 75.742%
2022-08-08 18:58:32 - epoch 086 lr: 0.012052
2022-08-08 19:00:15 - train: epoch 0086, iter [00100, 05004], lr: 0.012021, loss: 0.8377
2022-08-08 19:01:49 - train: epoch 0086, iter [00200, 05004], lr: 0.011990, loss: 0.8474
2022-08-08 19:03:23 - train: epoch 0086, iter [00300, 05004], lr: 0.011958, loss: 1.0171
2022-08-08 19:04:56 - train: epoch 0086, iter [00400, 05004], lr: 0.011927, loss: 0.9774
2022-08-08 19:06:30 - train: epoch 0086, iter [00500, 05004], lr: 0.011896, loss: 0.9003
2022-08-08 19:08:03 - train: epoch 0086, iter [00600, 05004], lr: 0.011865, loss: 1.0772
2022-08-08 19:09:37 - train: epoch 0086, iter [00700, 05004], lr: 0.011833, loss: 0.7015
2022-08-08 19:11:11 - train: epoch 0086, iter [00800, 05004], lr: 0.011802, loss: 0.9553
2022-08-08 19:12:45 - train: epoch 0086, iter [00900, 05004], lr: 0.011771, loss: 0.8400
2022-08-08 19:14:18 - train: epoch 0086, iter [01000, 05004], lr: 0.011740, loss: 1.0966
2022-08-08 19:15:52 - train: epoch 0086, iter [01100, 05004], lr: 0.011709, loss: 0.9336
2022-08-08 19:17:26 - train: epoch 0086, iter [01200, 05004], lr: 0.011678, loss: 0.9772
2022-08-08 19:19:00 - train: epoch 0086, iter [01300, 05004], lr: 0.011647, loss: 0.9612
2022-08-08 19:20:33 - train: epoch 0086, iter [01400, 05004], lr: 0.011616, loss: 0.9256
2022-08-08 19:22:07 - train: epoch 0086, iter [01500, 05004], lr: 0.011585, loss: 0.7413
2022-08-08 19:23:41 - train: epoch 0086, iter [01600, 05004], lr: 0.011554, loss: 1.0123
2022-08-08 19:25:14 - train: epoch 0086, iter [01700, 05004], lr: 0.011523, loss: 0.8233
2022-08-08 19:26:48 - train: epoch 0086, iter [01800, 05004], lr: 0.011493, loss: 0.9482
2022-08-08 19:28:22 - train: epoch 0086, iter [01900, 05004], lr: 0.011462, loss: 1.0219
2022-08-08 19:29:56 - train: epoch 0086, iter [02000, 05004], lr: 0.011431, loss: 0.9619
2022-08-08 19:31:29 - train: epoch 0086, iter [02100, 05004], lr: 0.011401, loss: 0.7759
2022-08-08 19:33:03 - train: epoch 0086, iter [02200, 05004], lr: 0.011370, loss: 0.8489
2022-08-08 19:34:37 - train: epoch 0086, iter [02300, 05004], lr: 0.011339, loss: 1.0255
2022-08-08 19:36:11 - train: epoch 0086, iter [02400, 05004], lr: 0.011309, loss: 0.8255
2022-08-08 19:37:44 - train: epoch 0086, iter [02500, 05004], lr: 0.011278, loss: 0.9543
2022-08-08 19:39:18 - train: epoch 0086, iter [02600, 05004], lr: 0.011248, loss: 0.9465
2022-08-08 19:40:52 - train: epoch 0086, iter [02700, 05004], lr: 0.011217, loss: 0.8249
2022-08-08 19:42:26 - train: epoch 0086, iter [02800, 05004], lr: 0.011187, loss: 1.0584
2022-08-08 19:44:00 - train: epoch 0086, iter [02900, 05004], lr: 0.011157, loss: 0.7359
2022-08-08 19:45:34 - train: epoch 0086, iter [03000, 05004], lr: 0.011126, loss: 0.7099
2022-08-08 19:47:08 - train: epoch 0086, iter [03100, 05004], lr: 0.011096, loss: 0.9921
2022-08-08 19:48:42 - train: epoch 0086, iter [03200, 05004], lr: 0.011066, loss: 0.9857
2022-08-08 19:50:16 - train: epoch 0086, iter [03300, 05004], lr: 0.011036, loss: 0.7597
2022-08-08 19:51:50 - train: epoch 0086, iter [03400, 05004], lr: 0.011005, loss: 0.9330
2022-08-08 19:53:24 - train: epoch 0086, iter [03500, 05004], lr: 0.010975, loss: 0.9517
2022-08-08 19:54:58 - train: epoch 0086, iter [03600, 05004], lr: 0.010945, loss: 0.9295
2022-08-08 19:56:32 - train: epoch 0086, iter [03700, 05004], lr: 0.010915, loss: 0.8760
2022-08-08 19:58:06 - train: epoch 0086, iter [03800, 05004], lr: 0.010885, loss: 1.0695
2022-08-08 19:59:40 - train: epoch 0086, iter [03900, 05004], lr: 0.010855, loss: 0.9112
2022-08-08 20:01:14 - train: epoch 0086, iter [04000, 05004], lr: 0.010825, loss: 1.0088
2022-08-08 20:02:48 - train: epoch 0086, iter [04100, 05004], lr: 0.010795, loss: 0.9645
2022-08-08 20:04:22 - train: epoch 0086, iter [04200, 05004], lr: 0.010766, loss: 0.7131
2022-08-08 20:05:56 - train: epoch 0086, iter [04300, 05004], lr: 0.010736, loss: 0.8691
2022-08-08 20:07:30 - train: epoch 0086, iter [04400, 05004], lr: 0.010706, loss: 0.9013
2022-08-08 20:09:04 - train: epoch 0086, iter [04500, 05004], lr: 0.010676, loss: 0.8707
2022-08-08 20:10:38 - train: epoch 0086, iter [04600, 05004], lr: 0.010647, loss: 0.8483
2022-08-08 20:12:11 - train: epoch 0086, iter [04700, 05004], lr: 0.010617, loss: 0.8326
2022-08-08 20:13:45 - train: epoch 0086, iter [04800, 05004], lr: 0.010587, loss: 0.9530
2022-08-08 20:15:19 - train: epoch 0086, iter [04900, 05004], lr: 0.010558, loss: 1.1215
2022-08-08 20:16:53 - train: epoch 0086, iter [05000, 05004], lr: 0.010528, loss: 1.0011
2022-08-08 20:16:58 - train: epoch 086, train_loss: 0.9093
2022-08-08 20:19:00 - eval: epoch: 086, acc1: 75.810%, acc5: 92.782%, test_loss: 0.9793, per_image_load_time: 3.526ms, per_image_inference_time: 1.212ms
2022-08-08 20:19:01 - until epoch: 086, best_acc1: 75.810%
2022-08-08 20:19:01 - epoch 087 lr: 0.010527
2022-08-08 20:20:43 - train: epoch 0087, iter [00100, 05004], lr: 0.010498, loss: 0.6257
2022-08-08 20:22:17 - train: epoch 0087, iter [00200, 05004], lr: 0.010468, loss: 1.0264
2022-08-08 20:23:50 - train: epoch 0087, iter [00300, 05004], lr: 0.010439, loss: 0.8365
2022-08-08 20:25:24 - train: epoch 0087, iter [00400, 05004], lr: 0.010409, loss: 1.0826
2022-08-08 20:26:58 - train: epoch 0087, iter [00500, 05004], lr: 0.010380, loss: 0.7457
2022-08-08 20:28:32 - train: epoch 0087, iter [00600, 05004], lr: 0.010351, loss: 0.8439
2022-08-08 20:30:06 - train: epoch 0087, iter [00700, 05004], lr: 0.010321, loss: 0.7591
2022-08-08 20:31:40 - train: epoch 0087, iter [00800, 05004], lr: 0.010292, loss: 0.7899
2022-08-08 20:33:14 - train: epoch 0087, iter [00900, 05004], lr: 0.010263, loss: 0.8212
2022-08-08 20:34:47 - train: epoch 0087, iter [01000, 05004], lr: 0.010234, loss: 0.8185
2022-08-08 20:36:21 - train: epoch 0087, iter [01100, 05004], lr: 0.010205, loss: 0.7996
2022-08-08 20:37:55 - train: epoch 0087, iter [01200, 05004], lr: 0.010176, loss: 0.8948
2022-08-08 20:39:29 - train: epoch 0087, iter [01300, 05004], lr: 0.010147, loss: 1.1286
2022-08-08 20:41:03 - train: epoch 0087, iter [01400, 05004], lr: 0.010118, loss: 0.8450
2022-08-08 20:42:37 - train: epoch 0087, iter [01500, 05004], lr: 0.010089, loss: 0.7591
2022-08-08 20:44:11 - train: epoch 0087, iter [01600, 05004], lr: 0.010060, loss: 0.8296
2022-08-08 20:45:45 - train: epoch 0087, iter [01700, 05004], lr: 0.010031, loss: 1.0058
2022-08-08 20:47:19 - train: epoch 0087, iter [01800, 05004], lr: 0.010002, loss: 0.9382
2022-08-08 20:48:53 - train: epoch 0087, iter [01900, 05004], lr: 0.009973, loss: 1.1062
2022-08-08 20:50:27 - train: epoch 0087, iter [02000, 05004], lr: 0.009945, loss: 1.0739
2022-08-08 20:52:00 - train: epoch 0087, iter [02100, 05004], lr: 0.009916, loss: 0.8885
2022-08-08 20:53:34 - train: epoch 0087, iter [02200, 05004], lr: 0.009887, loss: 1.0011
2022-08-08 20:55:08 - train: epoch 0087, iter [02300, 05004], lr: 0.009859, loss: 1.0023
2022-08-08 20:56:42 - train: epoch 0087, iter [02400, 05004], lr: 0.009830, loss: 0.9079
2022-08-08 20:58:16 - train: epoch 0087, iter [02500, 05004], lr: 0.009801, loss: 0.8972
2022-08-08 20:59:50 - train: epoch 0087, iter [02600, 05004], lr: 0.009773, loss: 0.8057
2022-08-08 21:01:23 - train: epoch 0087, iter [02700, 05004], lr: 0.009744, loss: 0.8340
2022-08-08 21:02:57 - train: epoch 0087, iter [02800, 05004], lr: 0.009716, loss: 0.7919
2022-08-08 21:04:31 - train: epoch 0087, iter [02900, 05004], lr: 0.009688, loss: 0.6377
2022-08-08 21:06:05 - train: epoch 0087, iter [03000, 05004], lr: 0.009659, loss: 0.8822
2022-08-08 21:07:39 - train: epoch 0087, iter [03100, 05004], lr: 0.009631, loss: 0.7761
2022-08-08 21:09:13 - train: epoch 0087, iter [03200, 05004], lr: 0.009603, loss: 0.8491
2022-08-08 21:10:47 - train: epoch 0087, iter [03300, 05004], lr: 0.009574, loss: 0.8181
2022-08-08 21:12:21 - train: epoch 0087, iter [03400, 05004], lr: 0.009546, loss: 0.9756
2022-08-08 21:13:55 - train: epoch 0087, iter [03500, 05004], lr: 0.009518, loss: 0.7490
2022-08-08 21:15:29 - train: epoch 0087, iter [03600, 05004], lr: 0.009490, loss: 0.7764
2022-08-08 21:17:03 - train: epoch 0087, iter [03700, 05004], lr: 0.009462, loss: 0.8685
2022-08-08 21:18:37 - train: epoch 0087, iter [03800, 05004], lr: 0.009434, loss: 0.9493
2022-08-08 21:20:11 - train: epoch 0087, iter [03900, 05004], lr: 0.009406, loss: 1.0938
2022-08-08 21:21:45 - train: epoch 0087, iter [04000, 05004], lr: 0.009378, loss: 0.8076
2022-08-08 21:23:19 - train: epoch 0087, iter [04100, 05004], lr: 0.009350, loss: 0.8254
2022-08-08 21:24:53 - train: epoch 0087, iter [04200, 05004], lr: 0.009322, loss: 1.0473
2022-08-08 21:26:27 - train: epoch 0087, iter [04300, 05004], lr: 0.009294, loss: 0.9955
2022-08-08 21:28:01 - train: epoch 0087, iter [04400, 05004], lr: 0.009266, loss: 0.8360
2022-08-08 21:29:35 - train: epoch 0087, iter [04500, 05004], lr: 0.009239, loss: 1.0167
2022-08-08 21:31:09 - train: epoch 0087, iter [04600, 05004], lr: 0.009211, loss: 1.1447
2022-08-08 21:32:43 - train: epoch 0087, iter [04700, 05004], lr: 0.009183, loss: 0.7509
2022-08-08 21:34:17 - train: epoch 0087, iter [04800, 05004], lr: 0.009156, loss: 0.7751
2022-08-08 21:35:51 - train: epoch 0087, iter [04900, 05004], lr: 0.009128, loss: 0.8972
2022-08-08 21:37:26 - train: epoch 0087, iter [05000, 05004], lr: 0.009100, loss: 0.7414
2022-08-08 21:37:30 - train: epoch 087, train_loss: 0.8821
2022-08-08 21:39:34 - eval: epoch: 087, acc1: 76.144%, acc5: 93.024%, test_loss: 0.9729, per_image_load_time: 2.924ms, per_image_inference_time: 1.178ms
2022-08-08 21:39:34 - until epoch: 087, best_acc1: 76.144%
2022-08-08 21:39:34 - epoch 088 lr: 0.009099
2022-08-08 21:41:18 - train: epoch 0088, iter [00100, 05004], lr: 0.009072, loss: 0.8450
2022-08-08 21:42:52 - train: epoch 0088, iter [00200, 05004], lr: 0.009044, loss: 0.9719
2022-08-08 21:44:26 - train: epoch 0088, iter [00300, 05004], lr: 0.009017, loss: 0.8398
2022-08-08 21:46:00 - train: epoch 0088, iter [00400, 05004], lr: 0.008989, loss: 1.0998
2022-08-08 21:47:34 - train: epoch 0088, iter [00500, 05004], lr: 0.008962, loss: 0.7792
2022-08-08 21:49:08 - train: epoch 0088, iter [00600, 05004], lr: 0.008935, loss: 0.7870
2022-08-08 21:50:42 - train: epoch 0088, iter [00700, 05004], lr: 0.008908, loss: 0.8353
2022-08-08 21:52:16 - train: epoch 0088, iter [00800, 05004], lr: 0.008880, loss: 0.8836
2022-08-08 21:53:50 - train: epoch 0088, iter [00900, 05004], lr: 0.008853, loss: 0.7260
2022-08-08 21:55:24 - train: epoch 0088, iter [01000, 05004], lr: 0.008826, loss: 0.7474
2022-08-08 21:56:58 - train: epoch 0088, iter [01100, 05004], lr: 0.008799, loss: 0.8080
2022-08-08 21:58:32 - train: epoch 0088, iter [01200, 05004], lr: 0.008772, loss: 0.9938
2022-08-08 22:00:06 - train: epoch 0088, iter [01300, 05004], lr: 0.008745, loss: 1.0416
2022-08-08 22:01:40 - train: epoch 0088, iter [01400, 05004], lr: 0.008718, loss: 0.8911
2022-08-08 22:03:14 - train: epoch 0088, iter [01500, 05004], lr: 0.008691, loss: 0.8939
2022-08-08 22:04:49 - train: epoch 0088, iter [01600, 05004], lr: 0.008664, loss: 0.8062
2022-08-08 22:06:23 - train: epoch 0088, iter [01700, 05004], lr: 0.008637, loss: 0.9118
2022-08-08 22:07:57 - train: epoch 0088, iter [01800, 05004], lr: 0.008610, loss: 0.8249
2022-08-08 22:09:31 - train: epoch 0088, iter [01900, 05004], lr: 0.008583, loss: 0.7225
2022-08-08 22:11:05 - train: epoch 0088, iter [02000, 05004], lr: 0.008556, loss: 0.6455
2022-08-08 22:12:40 - train: epoch 0088, iter [02100, 05004], lr: 0.008530, loss: 0.9305
2022-08-08 22:14:14 - train: epoch 0088, iter [02200, 05004], lr: 0.008503, loss: 0.8079
2022-08-08 22:15:48 - train: epoch 0088, iter [02300, 05004], lr: 0.008476, loss: 0.8138
2022-08-08 22:17:22 - train: epoch 0088, iter [02400, 05004], lr: 0.008450, loss: 0.8838
2022-08-08 22:18:56 - train: epoch 0088, iter [02500, 05004], lr: 0.008423, loss: 0.8927
2022-08-08 22:20:30 - train: epoch 0088, iter [02600, 05004], lr: 0.008397, loss: 0.8369
2022-08-08 22:22:04 - train: epoch 0088, iter [02700, 05004], lr: 0.008370, loss: 1.0318
2022-08-08 22:23:39 - train: epoch 0088, iter [02800, 05004], lr: 0.008344, loss: 0.8545
2022-08-08 22:25:13 - train: epoch 0088, iter [02900, 05004], lr: 0.008317, loss: 0.8191
2022-08-08 22:26:47 - train: epoch 0088, iter [03000, 05004], lr: 0.008291, loss: 0.8233
2022-08-08 22:28:21 - train: epoch 0088, iter [03100, 05004], lr: 0.008265, loss: 0.8417
2022-08-08 22:29:56 - train: epoch 0088, iter [03200, 05004], lr: 0.008238, loss: 0.9057
2022-08-08 22:31:30 - train: epoch 0088, iter [03300, 05004], lr: 0.008212, loss: 0.7951
2022-08-08 22:33:04 - train: epoch 0088, iter [03400, 05004], lr: 0.008186, loss: 0.6832
2022-08-08 22:34:39 - train: epoch 0088, iter [03500, 05004], lr: 0.008160, loss: 0.8133
2022-08-08 22:36:13 - train: epoch 0088, iter [03600, 05004], lr: 0.008134, loss: 0.7070
2022-08-08 22:37:48 - train: epoch 0088, iter [03700, 05004], lr: 0.008108, loss: 1.0187
2022-08-08 22:39:22 - train: epoch 0088, iter [03800, 05004], lr: 0.008081, loss: 0.9039
2022-08-08 22:40:56 - train: epoch 0088, iter [03900, 05004], lr: 0.008055, loss: 0.9975
2022-08-08 22:42:31 - train: epoch 0088, iter [04000, 05004], lr: 0.008029, loss: 0.7041
2022-08-08 22:44:05 - train: epoch 0088, iter [04100, 05004], lr: 0.008004, loss: 0.8646
2022-08-08 22:45:39 - train: epoch 0088, iter [04200, 05004], lr: 0.007978, loss: 1.0009
2022-08-08 22:47:14 - train: epoch 0088, iter [04300, 05004], lr: 0.007952, loss: 0.8084
2022-08-08 22:48:48 - train: epoch 0088, iter [04400, 05004], lr: 0.007926, loss: 0.9982
2022-08-08 22:50:22 - train: epoch 0088, iter [04500, 05004], lr: 0.007900, loss: 0.8793
2022-08-08 22:51:57 - train: epoch 0088, iter [04600, 05004], lr: 0.007875, loss: 1.1265
2022-08-08 22:53:31 - train: epoch 0088, iter [04700, 05004], lr: 0.007849, loss: 0.9388
2022-08-08 22:55:05 - train: epoch 0088, iter [04800, 05004], lr: 0.007823, loss: 0.8695
2022-08-08 22:56:39 - train: epoch 0088, iter [04900, 05004], lr: 0.007798, loss: 0.7527
2022-08-08 22:58:14 - train: epoch 0088, iter [05000, 05004], lr: 0.007772, loss: 0.7082
2022-08-08 22:58:18 - train: epoch 088, train_loss: 0.8536
2022-08-08 23:00:20 - eval: epoch: 088, acc1: 76.298%, acc5: 92.998%, test_loss: 0.9662, per_image_load_time: 2.263ms, per_image_inference_time: 1.149ms
2022-08-08 23:00:20 - until epoch: 088, best_acc1: 76.298%
2022-08-08 23:00:20 - epoch 089 lr: 0.007771
2022-08-08 23:02:03 - train: epoch 0089, iter [00100, 05004], lr: 0.007746, loss: 0.9708
2022-08-08 23:03:38 - train: epoch 0089, iter [00200, 05004], lr: 0.007720, loss: 0.7498
2022-08-08 23:05:12 - train: epoch 0089, iter [00300, 05004], lr: 0.007695, loss: 0.9402
2022-08-08 23:06:46 - train: epoch 0089, iter [00400, 05004], lr: 0.007669, loss: 0.8818
2022-08-08 23:08:21 - train: epoch 0089, iter [00500, 05004], lr: 0.007644, loss: 0.8500
2022-08-08 23:09:55 - train: epoch 0089, iter [00600, 05004], lr: 0.007618, loss: 0.7002
2022-08-08 23:11:29 - train: epoch 0089, iter [00700, 05004], lr: 0.007593, loss: 0.6397
2022-08-08 23:13:04 - train: epoch 0089, iter [00800, 05004], lr: 0.007568, loss: 0.9639
2022-08-08 23:14:38 - train: epoch 0089, iter [00900, 05004], lr: 0.007543, loss: 0.7339
2022-08-08 23:16:12 - train: epoch 0089, iter [01000, 05004], lr: 0.007518, loss: 0.7755
2022-08-08 23:17:46 - train: epoch 0089, iter [01100, 05004], lr: 0.007493, loss: 0.7964
2022-08-08 23:19:20 - train: epoch 0089, iter [01200, 05004], lr: 0.007467, loss: 0.8995
2022-08-08 23:20:54 - train: epoch 0089, iter [01300, 05004], lr: 0.007442, loss: 0.8688
2022-08-08 23:22:28 - train: epoch 0089, iter [01400, 05004], lr: 0.007417, loss: 0.8797
2022-08-08 23:24:02 - train: epoch 0089, iter [01500, 05004], lr: 0.007392, loss: 0.7329
2022-08-08 23:25:36 - train: epoch 0089, iter [01600, 05004], lr: 0.007368, loss: 0.7682
2022-08-08 23:27:10 - train: epoch 0089, iter [01700, 05004], lr: 0.007343, loss: 0.8960
2022-08-08 23:28:44 - train: epoch 0089, iter [01800, 05004], lr: 0.007318, loss: 0.8850
2022-08-08 23:30:18 - train: epoch 0089, iter [01900, 05004], lr: 0.007293, loss: 0.7980
2022-08-08 23:31:52 - train: epoch 0089, iter [02000, 05004], lr: 0.007268, loss: 0.7561
2022-08-08 23:33:26 - train: epoch 0089, iter [02100, 05004], lr: 0.007244, loss: 0.8947
2022-08-08 23:35:00 - train: epoch 0089, iter [02200, 05004], lr: 0.007219, loss: 0.8100
2022-08-08 23:36:34 - train: epoch 0089, iter [02300, 05004], lr: 0.007194, loss: 0.7955
2022-08-08 23:38:08 - train: epoch 0089, iter [02400, 05004], lr: 0.007170, loss: 0.9300
2022-08-08 23:39:42 - train: epoch 0089, iter [02500, 05004], lr: 0.007145, loss: 0.7335
2022-08-08 23:41:16 - train: epoch 0089, iter [02600, 05004], lr: 0.007121, loss: 0.6639
2022-08-08 23:42:50 - train: epoch 0089, iter [02700, 05004], lr: 0.007096, loss: 0.7743
2022-08-08 23:44:24 - train: epoch 0089, iter [02800, 05004], lr: 0.007072, loss: 0.8123
2022-08-08 23:45:58 - train: epoch 0089, iter [02900, 05004], lr: 0.007047, loss: 0.7976
2022-08-08 23:47:32 - train: epoch 0089, iter [03000, 05004], lr: 0.007023, loss: 0.8873
2022-08-08 23:49:06 - train: epoch 0089, iter [03100, 05004], lr: 0.006999, loss: 0.7413
2022-08-08 23:50:41 - train: epoch 0089, iter [03200, 05004], lr: 0.006974, loss: 0.8832
2022-08-08 23:52:14 - train: epoch 0089, iter [03300, 05004], lr: 0.006950, loss: 0.8977
2022-08-08 23:53:48 - train: epoch 0089, iter [03400, 05004], lr: 0.006926, loss: 0.7842
2022-08-08 23:55:22 - train: epoch 0089, iter [03500, 05004], lr: 0.006902, loss: 0.7840
2022-08-08 23:56:56 - train: epoch 0089, iter [03600, 05004], lr: 0.006878, loss: 0.8544
2022-08-08 23:58:30 - train: epoch 0089, iter [03700, 05004], lr: 0.006854, loss: 1.0242
