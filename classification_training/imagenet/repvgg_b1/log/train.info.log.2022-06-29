2022-06-29 08:44:42 - network: RepVGG_B1
2022-06-29 08:44:42 - num_classes: 1000
2022-06-29 08:44:42 - input_image_size: 224
2022-06-29 08:44:42 - scale: 1.1428571428571428
2022-06-29 08:44:42 - trained_model_path: 
2022-06-29 08:44:42 - train_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-29 08:44:42 - test_criterion: CELoss(
  (loss): CrossEntropyLoss()
)
2022-06-29 08:44:42 - train_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4dc593fe80>
2022-06-29 08:44:42 - test_dataset: <simpleAICV.classification.datasets.ilsvrc2012dataset.ILSVRC2012Dataset object at 0x7f4d88eff190>
2022-06-29 08:44:42 - train_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4d88eff1c0>
2022-06-29 08:44:42 - test_collater: <simpleAICV.classification.common.ClassificationCollater object at 0x7f4d88eff220>
2022-06-29 08:44:42 - seed: 0
2022-06-29 08:44:42 - batch_size: 256
2022-06-29 08:44:42 - num_workers: 16
2022-06-29 08:44:42 - optimizer: ('SGD', {'lr': 0.1, 'momentum': 0.9, 'global_weight_decay': False, 'weight_decay': 0.0001, 'no_weight_decay_layer_name_list': []})
2022-06-29 08:44:42 - scheduler: ('CosineLR', {'warm_up_epochs': 0})
2022-06-29 08:44:42 - epochs: 120
2022-06-29 08:44:42 - print_interval: 100
2022-06-29 08:44:42 - sync_bn: False
2022-06-29 08:44:42 - apex: True
2022-06-29 08:44:42 - use_ema_model: False
2022-06-29 08:44:42 - ema_model_decay: 0.9999
2022-06-29 08:44:42 - gpus_type: NVIDIA RTX A5000
2022-06-29 08:44:42 - gpus_num: 2
2022-06-29 08:44:42 - group: <torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f4db7a1df70>
2022-06-29 08:44:42 - --------------------parameters--------------------
2022-06-29 08:44:42 - name: stage0.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage0.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.0.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.0.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.1.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage1.1.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage1.1.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.1.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.2.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage1.2.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage1.2.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.2.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.3.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage1.3.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage1.3.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage1.3.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.0.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.0.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.1.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage2.1.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage2.1.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.1.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.2.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage2.2.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage2.2.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.2.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.3.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage2.3.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage2.3.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.3.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.4.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage2.4.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage2.4.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.4.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.5.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage2.5.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage2.5.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage2.5.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.0.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.0.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.1.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.1.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.1.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.1.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.2.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.2.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.2.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.2.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.3.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.3.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.3.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.3.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.4.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.4.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.4.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.4.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.5.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.5.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.5.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.5.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.6.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.6.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.6.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.6.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.7.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.7.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.7.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.7.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.8.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.8.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.8.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.8.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.9.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.9.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.9.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.9.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.10.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.10.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.10.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.10.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.11.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.11.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.11.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.11.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.12.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.12.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.12.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.12.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.13.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.13.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.13.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.13.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.14.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.14.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.14.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.14.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.15.identity.weight, grad: True
2022-06-29 08:44:42 - name: stage3.15.identity.bias, grad: True
2022-06-29 08:44:42 - name: stage3.15.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage3.15.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage4.0.conv3x3.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.bias, grad: True
2022-06-29 08:44:42 - name: stage4.0.conv1x1.conv.weight, grad: True
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.weight, grad: True
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.bias, grad: True
2022-06-29 08:44:42 - name: fc.weight, grad: True
2022-06-29 08:44:42 - name: fc.bias, grad: True
2022-06-29 08:44:42 - --------------------buffers--------------------
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.1.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.1.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.1.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.2.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.2.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.2.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.3.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.3.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.3.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.1.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.1.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.1.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.2.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.2.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.2.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.3.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.3.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.3.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.4.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.4.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.4.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.5.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.5.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.5.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.1.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.1.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.1.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.2.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.2.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.2.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.3.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.3.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.3.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.4.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.4.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.4.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.5.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.5.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.5.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.6.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.6.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.6.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.7.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.7.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.7.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.8.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.8.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.8.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.9.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.9.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.9.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.10.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.10.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.10.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.11.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.11.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.11.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.12.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.12.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.12.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.13.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.13.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.13.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.14.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.14.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.14.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.15.identity.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.15.identity.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.15.identity.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.running_mean, grad: False
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.running_var, grad: False
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.num_batches_tracked, grad: False
2022-06-29 08:44:42 - -----------no weight decay layers--------------
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.identity.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.identity.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage4.0.conv3x3.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.weight, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage4.0.conv1x1.bn.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - name: fc.bias, weight_decay: 0.0, lr_scale: not setting!
2022-06-29 08:44:42 - -------------weight decay layers---------------
2022-06-29 08:44:42 - name: stage0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage1.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage2.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.1.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.2.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.3.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.4.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.5.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.6.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.7.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.8.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.9.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.10.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.11.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.12.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.13.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.14.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage3.15.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage4.0.conv3x3.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: stage4.0.conv1x1.conv.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - name: fc.weight, weight_decay: 0.0001, lr_scale: not setting!
2022-06-29 08:44:42 - epoch 001 lr: 0.100000
2022-06-29 08:45:21 - train: epoch 0001, iter [00100, 05004], lr: 0.100000, loss: 6.8631
2022-06-29 08:45:53 - train: epoch 0001, iter [00200, 05004], lr: 0.100000, loss: 6.6841
2022-06-29 08:46:25 - train: epoch 0001, iter [00300, 05004], lr: 0.100000, loss: 6.5894
2022-06-29 08:46:58 - train: epoch 0001, iter [00400, 05004], lr: 0.100000, loss: 6.4384
2022-06-29 08:47:30 - train: epoch 0001, iter [00500, 05004], lr: 0.100000, loss: 6.2769
2022-06-29 08:48:03 - train: epoch 0001, iter [00600, 05004], lr: 0.100000, loss: 6.0953
2022-06-29 08:48:35 - train: epoch 0001, iter [00700, 05004], lr: 0.100000, loss: 6.0868
2022-06-29 08:49:08 - train: epoch 0001, iter [00800, 05004], lr: 0.100000, loss: 5.9410
2022-06-29 08:49:40 - train: epoch 0001, iter [00900, 05004], lr: 0.099999, loss: 5.9112
2022-06-29 08:50:12 - train: epoch 0001, iter [01000, 05004], lr: 0.099999, loss: 5.8644
2022-06-29 08:50:45 - train: epoch 0001, iter [01100, 05004], lr: 0.099999, loss: 5.7854
2022-06-29 08:51:18 - train: epoch 0001, iter [01200, 05004], lr: 0.099999, loss: 5.6225
2022-06-29 08:51:50 - train: epoch 0001, iter [01300, 05004], lr: 0.099999, loss: 5.4304
2022-06-29 08:52:23 - train: epoch 0001, iter [01400, 05004], lr: 0.099999, loss: 5.4633
2022-06-29 08:52:56 - train: epoch 0001, iter [01500, 05004], lr: 0.099998, loss: 5.3384
2022-06-29 08:53:29 - train: epoch 0001, iter [01600, 05004], lr: 0.099998, loss: 5.4163
2022-06-29 08:54:02 - train: epoch 0001, iter [01700, 05004], lr: 0.099998, loss: 5.1482
2022-06-29 08:54:36 - train: epoch 0001, iter [01800, 05004], lr: 0.099998, loss: 5.2173
2022-06-29 08:55:09 - train: epoch 0001, iter [01900, 05004], lr: 0.099998, loss: 5.0043
2022-06-29 08:55:42 - train: epoch 0001, iter [02000, 05004], lr: 0.099997, loss: 4.9086
2022-06-29 08:56:15 - train: epoch 0001, iter [02100, 05004], lr: 0.099997, loss: 4.9132
2022-06-29 08:56:48 - train: epoch 0001, iter [02200, 05004], lr: 0.099997, loss: 4.8714
2022-06-29 08:57:21 - train: epoch 0001, iter [02300, 05004], lr: 0.099996, loss: 4.6752
2022-06-29 08:57:54 - train: epoch 0001, iter [02400, 05004], lr: 0.099996, loss: 4.6982
2022-06-29 08:58:27 - train: epoch 0001, iter [02500, 05004], lr: 0.099996, loss: 4.7963
2022-06-29 08:59:00 - train: epoch 0001, iter [02600, 05004], lr: 0.099995, loss: 4.8217
2022-06-29 08:59:33 - train: epoch 0001, iter [02700, 05004], lr: 0.099995, loss: 4.6967
2022-06-29 09:00:06 - train: epoch 0001, iter [02800, 05004], lr: 0.099995, loss: 4.5097
2022-06-29 09:00:40 - train: epoch 0001, iter [02900, 05004], lr: 0.099994, loss: 4.4423
2022-06-29 09:01:13 - train: epoch 0001, iter [03000, 05004], lr: 0.099994, loss: 4.5255
2022-06-29 09:01:46 - train: epoch 0001, iter [03100, 05004], lr: 0.099993, loss: 4.6410
2022-06-29 09:02:19 - train: epoch 0001, iter [03200, 05004], lr: 0.099993, loss: 4.3951
2022-06-29 09:02:53 - train: epoch 0001, iter [03300, 05004], lr: 0.099993, loss: 4.1721
2022-06-29 09:03:26 - train: epoch 0001, iter [03400, 05004], lr: 0.099992, loss: 4.2979
2022-06-29 09:03:59 - train: epoch 0001, iter [03500, 05004], lr: 0.099992, loss: 4.2357
2022-06-29 09:04:32 - train: epoch 0001, iter [03600, 05004], lr: 0.099991, loss: 4.2400
2022-06-29 09:05:06 - train: epoch 0001, iter [03700, 05004], lr: 0.099991, loss: 4.4012
2022-06-29 09:05:39 - train: epoch 0001, iter [03800, 05004], lr: 0.099990, loss: 4.0631
2022-06-29 09:06:12 - train: epoch 0001, iter [03900, 05004], lr: 0.099990, loss: 4.2278
2022-06-29 09:06:45 - train: epoch 0001, iter [04000, 05004], lr: 0.099989, loss: 4.0052
2022-06-29 09:07:18 - train: epoch 0001, iter [04100, 05004], lr: 0.099988, loss: 4.0672
2022-06-29 09:07:51 - train: epoch 0001, iter [04200, 05004], lr: 0.099988, loss: 4.1055
2022-06-29 09:08:24 - train: epoch 0001, iter [04300, 05004], lr: 0.099987, loss: 4.0179
2022-06-29 09:08:57 - train: epoch 0001, iter [04400, 05004], lr: 0.099987, loss: 3.7652
2022-06-29 09:09:31 - train: epoch 0001, iter [04500, 05004], lr: 0.099986, loss: 3.9213
2022-06-29 09:10:04 - train: epoch 0001, iter [04600, 05004], lr: 0.099986, loss: 4.1011
2022-06-29 09:10:38 - train: epoch 0001, iter [04700, 05004], lr: 0.099985, loss: 3.9356
2022-06-29 09:11:11 - train: epoch 0001, iter [04800, 05004], lr: 0.099984, loss: 4.0512
2022-06-29 09:11:44 - train: epoch 0001, iter [04900, 05004], lr: 0.099984, loss: 3.7863
2022-06-29 09:12:17 - train: epoch 0001, iter [05000, 05004], lr: 0.099983, loss: 3.7541
2022-06-29 09:12:18 - train: epoch 001, train_loss: 4.8877
2022-06-29 09:13:34 - eval: epoch: 001, acc1: 26.048%, acc5: 51.014%, test_loss: 3.5509, per_image_load_time: 2.306ms, per_image_inference_time: 0.637ms
2022-06-29 09:13:35 - until epoch: 001, best_acc1: 26.048%
2022-06-29 09:13:35 - epoch 002 lr: 0.099983
2022-06-29 09:14:14 - train: epoch 0002, iter [00100, 05004], lr: 0.099982, loss: 3.8877
2022-06-29 09:14:46 - train: epoch 0002, iter [00200, 05004], lr: 0.099981, loss: 3.6283
2022-06-29 09:15:18 - train: epoch 0002, iter [00300, 05004], lr: 0.099981, loss: 3.8072
2022-06-29 09:15:51 - train: epoch 0002, iter [00400, 05004], lr: 0.099980, loss: 3.7218
2022-06-29 09:16:23 - train: epoch 0002, iter [00500, 05004], lr: 0.099979, loss: 3.4806
2022-06-29 09:16:55 - train: epoch 0002, iter [00600, 05004], lr: 0.099979, loss: 3.5672
2022-06-29 09:17:28 - train: epoch 0002, iter [00700, 05004], lr: 0.099978, loss: 3.7420
2022-06-29 09:18:01 - train: epoch 0002, iter [00800, 05004], lr: 0.099977, loss: 3.4481
2022-06-29 09:18:34 - train: epoch 0002, iter [00900, 05004], lr: 0.099976, loss: 3.3078
2022-06-29 09:19:06 - train: epoch 0002, iter [01000, 05004], lr: 0.099975, loss: 3.7403
2022-06-29 09:19:39 - train: epoch 0002, iter [01100, 05004], lr: 0.099975, loss: 3.7454
2022-06-29 09:20:12 - train: epoch 0002, iter [01200, 05004], lr: 0.099974, loss: 3.5295
2022-06-29 09:20:45 - train: epoch 0002, iter [01300, 05004], lr: 0.099973, loss: 3.3953
2022-06-29 09:21:17 - train: epoch 0002, iter [01400, 05004], lr: 0.099972, loss: 3.5208
2022-06-29 09:21:50 - train: epoch 0002, iter [01500, 05004], lr: 0.099971, loss: 3.5196
2022-06-29 09:22:23 - train: epoch 0002, iter [01600, 05004], lr: 0.099970, loss: 3.5001
2022-06-29 09:22:55 - train: epoch 0002, iter [01700, 05004], lr: 0.099969, loss: 3.4962
2022-06-29 09:23:28 - train: epoch 0002, iter [01800, 05004], lr: 0.099968, loss: 3.5867
2022-06-29 09:24:00 - train: epoch 0002, iter [01900, 05004], lr: 0.099967, loss: 3.2930
2022-06-29 09:24:33 - train: epoch 0002, iter [02000, 05004], lr: 0.099966, loss: 3.0401
2022-06-29 09:25:05 - train: epoch 0002, iter [02100, 05004], lr: 0.099965, loss: 3.3326
2022-06-29 09:25:38 - train: epoch 0002, iter [02200, 05004], lr: 0.099964, loss: 3.0747
2022-06-29 09:26:11 - train: epoch 0002, iter [02300, 05004], lr: 0.099963, loss: 3.5729
2022-06-29 09:26:44 - train: epoch 0002, iter [02400, 05004], lr: 0.099962, loss: 3.2087
2022-06-29 09:27:16 - train: epoch 0002, iter [02500, 05004], lr: 0.099961, loss: 3.1553
2022-06-29 09:27:49 - train: epoch 0002, iter [02600, 05004], lr: 0.099960, loss: 3.1190
2022-06-29 09:28:22 - train: epoch 0002, iter [02700, 05004], lr: 0.099959, loss: 3.3827
2022-06-29 09:28:54 - train: epoch 0002, iter [02800, 05004], lr: 0.099958, loss: 3.2442
2022-06-29 09:29:27 - train: epoch 0002, iter [02900, 05004], lr: 0.099957, loss: 3.1325
2022-06-29 09:29:59 - train: epoch 0002, iter [03000, 05004], lr: 0.099956, loss: 3.0868
2022-06-29 09:30:32 - train: epoch 0002, iter [03100, 05004], lr: 0.099955, loss: 3.1534
2022-06-29 09:31:05 - train: epoch 0002, iter [03200, 05004], lr: 0.099954, loss: 3.1065
2022-06-29 09:31:38 - train: epoch 0002, iter [03300, 05004], lr: 0.099953, loss: 3.0761
2022-06-29 09:32:11 - train: epoch 0002, iter [03400, 05004], lr: 0.099952, loss: 3.0959
2022-06-29 09:32:44 - train: epoch 0002, iter [03500, 05004], lr: 0.099951, loss: 3.0443
2022-06-29 09:33:18 - train: epoch 0002, iter [03600, 05004], lr: 0.099949, loss: 3.1338
2022-06-29 09:33:51 - train: epoch 0002, iter [03700, 05004], lr: 0.099948, loss: 3.2589
2022-06-29 09:34:24 - train: epoch 0002, iter [03800, 05004], lr: 0.099947, loss: 3.0163
2022-06-29 09:34:57 - train: epoch 0002, iter [03900, 05004], lr: 0.099946, loss: 3.0984
2022-06-29 09:35:31 - train: epoch 0002, iter [04000, 05004], lr: 0.099945, loss: 2.9381
2022-06-29 09:36:04 - train: epoch 0002, iter [04100, 05004], lr: 0.099943, loss: 3.2213
2022-06-29 09:36:38 - train: epoch 0002, iter [04200, 05004], lr: 0.099942, loss: 3.0003
2022-06-29 09:37:11 - train: epoch 0002, iter [04300, 05004], lr: 0.099941, loss: 3.0951
2022-06-29 09:37:44 - train: epoch 0002, iter [04400, 05004], lr: 0.099939, loss: 2.8973
2022-06-29 09:38:18 - train: epoch 0002, iter [04500, 05004], lr: 0.099938, loss: 2.8793
2022-06-29 09:38:51 - train: epoch 0002, iter [04600, 05004], lr: 0.099937, loss: 2.9620
2022-06-29 09:39:24 - train: epoch 0002, iter [04700, 05004], lr: 0.099936, loss: 3.0059
2022-06-29 09:39:58 - train: epoch 0002, iter [04800, 05004], lr: 0.099934, loss: 3.0124
2022-06-29 09:40:32 - train: epoch 0002, iter [04900, 05004], lr: 0.099933, loss: 2.9228
2022-06-29 09:41:05 - train: epoch 0002, iter [05000, 05004], lr: 0.099932, loss: 2.7783
2022-06-29 09:41:07 - train: epoch 002, train_loss: 3.2978
2022-06-29 09:42:23 - eval: epoch: 002, acc1: 40.942%, acc5: 67.674%, test_loss: 2.6522, per_image_load_time: 1.194ms, per_image_inference_time: 0.673ms
2022-06-29 09:42:24 - until epoch: 002, best_acc1: 40.942%
2022-06-29 09:42:24 - epoch 003 lr: 0.099931
2022-06-29 09:43:03 - train: epoch 0003, iter [00100, 05004], lr: 0.099930, loss: 3.0711
2022-06-29 09:43:36 - train: epoch 0003, iter [00200, 05004], lr: 0.099929, loss: 2.9552
2022-06-29 09:44:09 - train: epoch 0003, iter [00300, 05004], lr: 0.099927, loss: 2.8688
2022-06-29 09:44:41 - train: epoch 0003, iter [00400, 05004], lr: 0.099926, loss: 2.9507
2022-06-29 09:45:14 - train: epoch 0003, iter [00500, 05004], lr: 0.099924, loss: 3.0840
2022-06-29 09:45:47 - train: epoch 0003, iter [00600, 05004], lr: 0.099923, loss: 2.8835
2022-06-29 09:46:20 - train: epoch 0003, iter [00700, 05004], lr: 0.099922, loss: 3.1001
2022-06-29 09:46:54 - train: epoch 0003, iter [00800, 05004], lr: 0.099920, loss: 3.0678
2022-06-29 09:47:27 - train: epoch 0003, iter [00900, 05004], lr: 0.099919, loss: 2.7031
2022-06-29 09:48:01 - train: epoch 0003, iter [01000, 05004], lr: 0.099917, loss: 2.9776
2022-06-29 09:48:34 - train: epoch 0003, iter [01100, 05004], lr: 0.099916, loss: 2.8894
2022-06-29 09:49:08 - train: epoch 0003, iter [01200, 05004], lr: 0.099914, loss: 2.6717
2022-06-29 09:49:41 - train: epoch 0003, iter [01300, 05004], lr: 0.099913, loss: 2.8678
2022-06-29 09:50:15 - train: epoch 0003, iter [01400, 05004], lr: 0.099911, loss: 2.8025
2022-06-29 09:50:49 - train: epoch 0003, iter [01500, 05004], lr: 0.099909, loss: 3.0203
2022-06-29 09:51:22 - train: epoch 0003, iter [01600, 05004], lr: 0.099908, loss: 2.7729
2022-06-29 09:51:56 - train: epoch 0003, iter [01700, 05004], lr: 0.099906, loss: 2.7503
2022-06-29 09:52:30 - train: epoch 0003, iter [01800, 05004], lr: 0.099905, loss: 2.8227
2022-06-29 09:53:03 - train: epoch 0003, iter [01900, 05004], lr: 0.099903, loss: 2.8642
2022-06-29 09:53:37 - train: epoch 0003, iter [02000, 05004], lr: 0.099901, loss: 3.0634
2022-06-29 09:54:11 - train: epoch 0003, iter [02100, 05004], lr: 0.099900, loss: 3.0944
2022-06-29 09:54:44 - train: epoch 0003, iter [02200, 05004], lr: 0.099898, loss: 3.3520
2022-06-29 09:55:18 - train: epoch 0003, iter [02300, 05004], lr: 0.099896, loss: 2.8136
2022-06-29 09:55:52 - train: epoch 0003, iter [02400, 05004], lr: 0.099895, loss: 2.6944
2022-06-29 09:56:25 - train: epoch 0003, iter [02500, 05004], lr: 0.099893, loss: 2.8741
2022-06-29 09:56:59 - train: epoch 0003, iter [02600, 05004], lr: 0.099891, loss: 2.7903
2022-06-29 09:57:33 - train: epoch 0003, iter [02700, 05004], lr: 0.099890, loss: 3.0793
2022-06-29 09:58:07 - train: epoch 0003, iter [02800, 05004], lr: 0.099888, loss: 2.7101
2022-06-29 09:58:41 - train: epoch 0003, iter [02900, 05004], lr: 0.099886, loss: 2.7087
2022-06-29 09:59:14 - train: epoch 0003, iter [03000, 05004], lr: 0.099884, loss: 2.9919
2022-06-29 09:59:48 - train: epoch 0003, iter [03100, 05004], lr: 0.099882, loss: 2.9929
2022-06-29 10:00:22 - train: epoch 0003, iter [03200, 05004], lr: 0.099881, loss: 2.8004
2022-06-29 10:00:56 - train: epoch 0003, iter [03300, 05004], lr: 0.099879, loss: 2.8127
2022-06-29 10:01:29 - train: epoch 0003, iter [03400, 05004], lr: 0.099877, loss: 2.9589
2022-06-29 10:02:03 - train: epoch 0003, iter [03500, 05004], lr: 0.099875, loss: 2.5778
2022-06-29 10:02:36 - train: epoch 0003, iter [03600, 05004], lr: 0.099873, loss: 2.6910
2022-06-29 10:03:10 - train: epoch 0003, iter [03700, 05004], lr: 0.099871, loss: 2.7052
2022-06-29 10:03:43 - train: epoch 0003, iter [03800, 05004], lr: 0.099870, loss: 2.8654
2022-06-29 10:04:17 - train: epoch 0003, iter [03900, 05004], lr: 0.099868, loss: 2.9644
2022-06-29 10:04:50 - train: epoch 0003, iter [04000, 05004], lr: 0.099866, loss: 2.7367
2022-06-29 10:05:23 - train: epoch 0003, iter [04100, 05004], lr: 0.099864, loss: 2.7561
2022-06-29 10:05:57 - train: epoch 0003, iter [04200, 05004], lr: 0.099862, loss: 2.7077
2022-06-29 10:06:30 - train: epoch 0003, iter [04300, 05004], lr: 0.099860, loss: 2.4857
2022-06-29 10:07:04 - train: epoch 0003, iter [04400, 05004], lr: 0.099858, loss: 2.5679
2022-06-29 10:07:38 - train: epoch 0003, iter [04500, 05004], lr: 0.099856, loss: 2.7132
2022-06-29 10:08:11 - train: epoch 0003, iter [04600, 05004], lr: 0.099854, loss: 2.7489
2022-06-29 10:08:45 - train: epoch 0003, iter [04700, 05004], lr: 0.099852, loss: 2.5721
2022-06-29 10:09:18 - train: epoch 0003, iter [04800, 05004], lr: 0.099850, loss: 2.7792
2022-06-29 10:09:51 - train: epoch 0003, iter [04900, 05004], lr: 0.099848, loss: 2.8141
2022-06-29 10:10:24 - train: epoch 0003, iter [05000, 05004], lr: 0.099846, loss: 2.7501
2022-06-29 10:10:26 - train: epoch 003, train_loss: 2.8081
2022-06-29 10:11:43 - eval: epoch: 003, acc1: 46.814%, acc5: 72.974%, test_loss: 2.3429, per_image_load_time: 2.370ms, per_image_inference_time: 0.638ms
2022-06-29 10:11:44 - until epoch: 003, best_acc1: 46.814%
2022-06-29 10:11:44 - epoch 004 lr: 0.099846
2022-06-29 10:12:23 - train: epoch 0004, iter [00100, 05004], lr: 0.099844, loss: 2.7551
2022-06-29 10:12:56 - train: epoch 0004, iter [00200, 05004], lr: 0.099842, loss: 2.6022
2022-06-29 10:13:29 - train: epoch 0004, iter [00300, 05004], lr: 0.099840, loss: 2.6984
2022-06-29 10:14:02 - train: epoch 0004, iter [00400, 05004], lr: 0.099838, loss: 2.6151
2022-06-29 10:14:35 - train: epoch 0004, iter [00500, 05004], lr: 0.099835, loss: 2.4288
2022-06-29 10:15:08 - train: epoch 0004, iter [00600, 05004], lr: 0.099833, loss: 2.8053
2022-06-29 10:15:41 - train: epoch 0004, iter [00700, 05004], lr: 0.099831, loss: 2.7237
2022-06-29 10:16:15 - train: epoch 0004, iter [00800, 05004], lr: 0.099829, loss: 2.5185
2022-06-29 10:16:48 - train: epoch 0004, iter [00900, 05004], lr: 0.099827, loss: 2.3533
2022-06-29 10:17:22 - train: epoch 0004, iter [01000, 05004], lr: 0.099825, loss: 2.5510
2022-06-29 10:17:55 - train: epoch 0004, iter [01100, 05004], lr: 0.099822, loss: 2.6326
2022-06-29 10:18:29 - train: epoch 0004, iter [01200, 05004], lr: 0.099820, loss: 2.3872
2022-06-29 10:19:02 - train: epoch 0004, iter [01300, 05004], lr: 0.099818, loss: 2.4874
2022-06-29 10:19:36 - train: epoch 0004, iter [01400, 05004], lr: 0.099816, loss: 2.6526
2022-06-29 10:20:09 - train: epoch 0004, iter [01500, 05004], lr: 0.099814, loss: 2.7526
2022-06-29 10:20:43 - train: epoch 0004, iter [01600, 05004], lr: 0.099811, loss: 2.6602
2022-06-29 10:21:16 - train: epoch 0004, iter [01700, 05004], lr: 0.099809, loss: 2.7110
2022-06-29 10:21:50 - train: epoch 0004, iter [01800, 05004], lr: 0.099807, loss: 2.7725
2022-06-29 10:22:24 - train: epoch 0004, iter [01900, 05004], lr: 0.099804, loss: 2.7216
2022-06-29 10:22:57 - train: epoch 0004, iter [02000, 05004], lr: 0.099802, loss: 2.5519
2022-06-29 10:23:30 - train: epoch 0004, iter [02100, 05004], lr: 0.099800, loss: 2.6080
2022-06-29 10:24:03 - train: epoch 0004, iter [02200, 05004], lr: 0.099797, loss: 2.5707
2022-06-29 10:24:36 - train: epoch 0004, iter [02300, 05004], lr: 0.099795, loss: 2.4332
2022-06-29 10:25:09 - train: epoch 0004, iter [02400, 05004], lr: 0.099793, loss: 2.5443
2022-06-29 10:25:42 - train: epoch 0004, iter [02500, 05004], lr: 0.099790, loss: 2.5798
2022-06-29 10:26:15 - train: epoch 0004, iter [02600, 05004], lr: 0.099788, loss: 2.6266
2022-06-29 10:26:49 - train: epoch 0004, iter [02700, 05004], lr: 0.099785, loss: 2.4559
2022-06-29 10:27:22 - train: epoch 0004, iter [02800, 05004], lr: 0.099783, loss: 2.5092
2022-06-29 10:27:56 - train: epoch 0004, iter [02900, 05004], lr: 0.099781, loss: 2.4874
2022-06-29 10:28:29 - train: epoch 0004, iter [03000, 05004], lr: 0.099778, loss: 2.4420
2022-06-29 10:29:02 - train: epoch 0004, iter [03100, 05004], lr: 0.099776, loss: 2.6244
2022-06-29 10:29:35 - train: epoch 0004, iter [03200, 05004], lr: 0.099773, loss: 2.5232
2022-06-29 10:30:09 - train: epoch 0004, iter [03300, 05004], lr: 0.099771, loss: 2.6792
2022-06-29 10:30:42 - train: epoch 0004, iter [03400, 05004], lr: 0.099768, loss: 2.6266
2022-06-29 10:31:15 - train: epoch 0004, iter [03500, 05004], lr: 0.099766, loss: 2.5011
2022-06-29 10:31:48 - train: epoch 0004, iter [03600, 05004], lr: 0.099763, loss: 2.3463
2022-06-29 10:32:22 - train: epoch 0004, iter [03700, 05004], lr: 0.099761, loss: 2.5275
2022-06-29 10:32:55 - train: epoch 0004, iter [03800, 05004], lr: 0.099758, loss: 2.5416
2022-06-29 10:33:28 - train: epoch 0004, iter [03900, 05004], lr: 0.099755, loss: 2.4678
2022-06-29 10:34:02 - train: epoch 0004, iter [04000, 05004], lr: 0.099753, loss: 2.2510
2022-06-29 10:34:36 - train: epoch 0004, iter [04100, 05004], lr: 0.099750, loss: 2.4527
2022-06-29 10:35:09 - train: epoch 0004, iter [04200, 05004], lr: 0.099748, loss: 2.3848
2022-06-29 10:35:43 - train: epoch 0004, iter [04300, 05004], lr: 0.099745, loss: 2.2993
2022-06-29 10:36:16 - train: epoch 0004, iter [04400, 05004], lr: 0.099742, loss: 2.3205
2022-06-29 10:36:49 - train: epoch 0004, iter [04500, 05004], lr: 0.099740, loss: 2.0157
2022-06-29 10:37:22 - train: epoch 0004, iter [04600, 05004], lr: 0.099737, loss: 2.6133
2022-06-29 10:37:56 - train: epoch 0004, iter [04700, 05004], lr: 0.099734, loss: 2.4688
2022-06-29 10:38:29 - train: epoch 0004, iter [04800, 05004], lr: 0.099732, loss: 2.4440
2022-06-29 10:39:02 - train: epoch 0004, iter [04900, 05004], lr: 0.099729, loss: 2.5650
2022-06-29 10:39:35 - train: epoch 0004, iter [05000, 05004], lr: 0.099726, loss: 2.6626
2022-06-29 10:39:37 - train: epoch 004, train_loss: 2.5767
2022-06-29 10:40:54 - eval: epoch: 004, acc1: 49.796%, acc5: 75.610%, test_loss: 2.1811, per_image_load_time: 2.340ms, per_image_inference_time: 0.637ms
2022-06-29 10:40:55 - until epoch: 004, best_acc1: 49.796%
2022-06-29 10:40:55 - epoch 005 lr: 0.099726
2022-06-29 10:41:35 - train: epoch 0005, iter [00100, 05004], lr: 0.099723, loss: 2.5621
2022-06-29 10:42:08 - train: epoch 0005, iter [00200, 05004], lr: 0.099721, loss: 2.5259
2022-06-29 10:42:40 - train: epoch 0005, iter [00300, 05004], lr: 0.099718, loss: 2.5843
2022-06-29 10:43:13 - train: epoch 0005, iter [00400, 05004], lr: 0.099715, loss: 2.4471
2022-06-29 10:43:47 - train: epoch 0005, iter [00500, 05004], lr: 0.099712, loss: 2.3044
2022-06-29 10:44:20 - train: epoch 0005, iter [00600, 05004], lr: 0.099709, loss: 2.5096
2022-06-29 10:44:53 - train: epoch 0005, iter [00700, 05004], lr: 0.099707, loss: 2.5100
2022-06-29 10:45:26 - train: epoch 0005, iter [00800, 05004], lr: 0.099704, loss: 2.6597
2022-06-29 10:45:59 - train: epoch 0005, iter [00900, 05004], lr: 0.099701, loss: 2.5469
2022-06-29 10:46:32 - train: epoch 0005, iter [01000, 05004], lr: 0.099698, loss: 2.5959
2022-06-29 10:47:05 - train: epoch 0005, iter [01100, 05004], lr: 0.099695, loss: 2.5927
2022-06-29 10:47:38 - train: epoch 0005, iter [01200, 05004], lr: 0.099692, loss: 2.5307
2022-06-29 10:48:11 - train: epoch 0005, iter [01300, 05004], lr: 0.099689, loss: 2.4997
2022-06-29 10:48:43 - train: epoch 0005, iter [01400, 05004], lr: 0.099686, loss: 2.5345
2022-06-29 10:49:17 - train: epoch 0005, iter [01500, 05004], lr: 0.099684, loss: 2.2944
2022-06-29 10:49:50 - train: epoch 0005, iter [01600, 05004], lr: 0.099681, loss: 2.2292
2022-06-29 10:50:23 - train: epoch 0005, iter [01700, 05004], lr: 0.099678, loss: 2.3990
2022-06-29 10:50:56 - train: epoch 0005, iter [01800, 05004], lr: 0.099675, loss: 2.3454
2022-06-29 10:51:29 - train: epoch 0005, iter [01900, 05004], lr: 0.099672, loss: 2.2862
2022-06-29 10:52:02 - train: epoch 0005, iter [02000, 05004], lr: 0.099669, loss: 2.4256
2022-06-29 10:52:35 - train: epoch 0005, iter [02100, 05004], lr: 0.099666, loss: 2.2433
2022-06-29 10:53:09 - train: epoch 0005, iter [02200, 05004], lr: 0.099663, loss: 2.2756
2022-06-29 10:53:42 - train: epoch 0005, iter [02300, 05004], lr: 0.099660, loss: 2.1653
2022-06-29 10:54:15 - train: epoch 0005, iter [02400, 05004], lr: 0.099657, loss: 2.3759
2022-06-29 10:54:49 - train: epoch 0005, iter [02500, 05004], lr: 0.099653, loss: 2.5869
2022-06-29 10:55:22 - train: epoch 0005, iter [02600, 05004], lr: 0.099650, loss: 2.5575
2022-06-29 10:55:55 - train: epoch 0005, iter [02700, 05004], lr: 0.099647, loss: 2.5006
2022-06-29 10:56:28 - train: epoch 0005, iter [02800, 05004], lr: 0.099644, loss: 2.3704
2022-06-29 10:57:01 - train: epoch 0005, iter [02900, 05004], lr: 0.099641, loss: 2.2503
2022-06-29 10:57:35 - train: epoch 0005, iter [03000, 05004], lr: 0.099638, loss: 2.4456
2022-06-29 10:58:08 - train: epoch 0005, iter [03100, 05004], lr: 0.099635, loss: 2.4854
2022-06-29 10:58:41 - train: epoch 0005, iter [03200, 05004], lr: 0.099632, loss: 2.6273
2022-06-29 10:59:14 - train: epoch 0005, iter [03300, 05004], lr: 0.099628, loss: 2.2587
2022-06-29 10:59:47 - train: epoch 0005, iter [03400, 05004], lr: 0.099625, loss: 2.1853
2022-06-29 11:00:21 - train: epoch 0005, iter [03500, 05004], lr: 0.099622, loss: 2.4051
2022-06-29 11:00:54 - train: epoch 0005, iter [03600, 05004], lr: 0.099619, loss: 2.4915
2022-06-29 11:01:27 - train: epoch 0005, iter [03700, 05004], lr: 0.099616, loss: 2.3133
2022-06-29 11:02:00 - train: epoch 0005, iter [03800, 05004], lr: 0.099612, loss: 2.2906
2022-06-29 11:02:33 - train: epoch 0005, iter [03900, 05004], lr: 0.099609, loss: 2.8717
2022-06-29 11:03:06 - train: epoch 0005, iter [04000, 05004], lr: 0.099606, loss: 2.3924
2022-06-29 11:03:39 - train: epoch 0005, iter [04100, 05004], lr: 0.099603, loss: 2.3646
2022-06-29 11:04:12 - train: epoch 0005, iter [04200, 05004], lr: 0.099599, loss: 2.4257
2022-06-29 11:04:45 - train: epoch 0005, iter [04300, 05004], lr: 0.099596, loss: 2.3781
2022-06-29 11:05:18 - train: epoch 0005, iter [04400, 05004], lr: 0.099593, loss: 2.4896
2022-06-29 11:05:51 - train: epoch 0005, iter [04500, 05004], lr: 0.099589, loss: 2.4027
2022-06-29 11:06:25 - train: epoch 0005, iter [04600, 05004], lr: 0.099586, loss: 2.3111
2022-06-29 11:06:58 - train: epoch 0005, iter [04700, 05004], lr: 0.099583, loss: 2.1953
2022-06-29 11:07:31 - train: epoch 0005, iter [04800, 05004], lr: 0.099579, loss: 2.2727
2022-06-29 11:08:04 - train: epoch 0005, iter [04900, 05004], lr: 0.099576, loss: 2.4933
2022-06-29 11:08:38 - train: epoch 0005, iter [05000, 05004], lr: 0.099572, loss: 2.2436
2022-06-29 11:08:39 - train: epoch 005, train_loss: 2.4397
2022-06-29 11:09:55 - eval: epoch: 005, acc1: 51.884%, acc5: 77.452%, test_loss: 2.0705, per_image_load_time: 1.639ms, per_image_inference_time: 0.642ms
2022-06-29 11:09:56 - until epoch: 005, best_acc1: 51.884%
2022-06-29 11:09:56 - epoch 006 lr: 0.099572
2022-06-29 11:10:36 - train: epoch 0006, iter [00100, 05004], lr: 0.099569, loss: 2.3258
2022-06-29 11:11:09 - train: epoch 0006, iter [00200, 05004], lr: 0.099565, loss: 2.4277
2022-06-29 11:11:42 - train: epoch 0006, iter [00300, 05004], lr: 0.099562, loss: 2.1201
2022-06-29 11:12:15 - train: epoch 0006, iter [00400, 05004], lr: 0.099558, loss: 2.4032
2022-06-29 11:12:48 - train: epoch 0006, iter [00500, 05004], lr: 0.099555, loss: 2.3839
2022-06-29 11:13:21 - train: epoch 0006, iter [00600, 05004], lr: 0.099552, loss: 2.4126
2022-06-29 11:13:54 - train: epoch 0006, iter [00700, 05004], lr: 0.099548, loss: 2.2908
2022-06-29 11:14:28 - train: epoch 0006, iter [00800, 05004], lr: 0.099544, loss: 2.3299
2022-06-29 11:15:01 - train: epoch 0006, iter [00900, 05004], lr: 0.099541, loss: 2.2458
2022-06-29 11:15:34 - train: epoch 0006, iter [01000, 05004], lr: 0.099537, loss: 2.3277
2022-06-29 11:16:08 - train: epoch 0006, iter [01100, 05004], lr: 0.099534, loss: 2.2814
2022-06-29 11:16:42 - train: epoch 0006, iter [01200, 05004], lr: 0.099530, loss: 2.4905
2022-06-29 11:17:15 - train: epoch 0006, iter [01300, 05004], lr: 0.099527, loss: 2.4907
2022-06-29 11:17:48 - train: epoch 0006, iter [01400, 05004], lr: 0.099523, loss: 2.4568
2022-06-29 11:18:22 - train: epoch 0006, iter [01500, 05004], lr: 0.099520, loss: 2.4713
2022-06-29 11:18:55 - train: epoch 0006, iter [01600, 05004], lr: 0.099516, loss: 2.2143
2022-06-29 11:19:28 - train: epoch 0006, iter [01700, 05004], lr: 0.099512, loss: 2.5473
2022-06-29 11:20:02 - train: epoch 0006, iter [01800, 05004], lr: 0.099509, loss: 2.3799
2022-06-29 11:20:36 - train: epoch 0006, iter [01900, 05004], lr: 0.099505, loss: 2.2377
2022-06-29 11:21:09 - train: epoch 0006, iter [02000, 05004], lr: 0.099501, loss: 2.4436
2022-06-29 11:21:43 - train: epoch 0006, iter [02100, 05004], lr: 0.099498, loss: 2.4765
2022-06-29 11:22:16 - train: epoch 0006, iter [02200, 05004], lr: 0.099494, loss: 2.2621
2022-06-29 11:22:50 - train: epoch 0006, iter [02300, 05004], lr: 0.099490, loss: 2.1751
2022-06-29 11:23:23 - train: epoch 0006, iter [02400, 05004], lr: 0.099486, loss: 2.4451
2022-06-29 11:23:56 - train: epoch 0006, iter [02500, 05004], lr: 0.099483, loss: 2.5713
2022-06-29 11:24:29 - train: epoch 0006, iter [02600, 05004], lr: 0.099479, loss: 2.2767
2022-06-29 11:25:03 - train: epoch 0006, iter [02700, 05004], lr: 0.099475, loss: 2.4955
2022-06-29 11:25:36 - train: epoch 0006, iter [02800, 05004], lr: 0.099471, loss: 2.1903
2022-06-29 11:26:10 - train: epoch 0006, iter [02900, 05004], lr: 0.099468, loss: 2.5549
2022-06-29 11:26:43 - train: epoch 0006, iter [03000, 05004], lr: 0.099464, loss: 2.3737
2022-06-29 11:27:16 - train: epoch 0006, iter [03100, 05004], lr: 0.099460, loss: 2.1221
2022-06-29 11:27:49 - train: epoch 0006, iter [03200, 05004], lr: 0.099456, loss: 2.2329
2022-06-29 11:28:22 - train: epoch 0006, iter [03300, 05004], lr: 0.099452, loss: 2.1845
2022-06-29 11:28:56 - train: epoch 0006, iter [03400, 05004], lr: 0.099448, loss: 2.4793
2022-06-29 11:29:29 - train: epoch 0006, iter [03500, 05004], lr: 0.099444, loss: 2.4324
2022-06-29 11:30:02 - train: epoch 0006, iter [03600, 05004], lr: 0.099441, loss: 2.2910
2022-06-29 11:30:35 - train: epoch 0006, iter [03700, 05004], lr: 0.099437, loss: 2.4010
2022-06-29 11:31:08 - train: epoch 0006, iter [03800, 05004], lr: 0.099433, loss: 2.1693
2022-06-29 11:31:41 - train: epoch 0006, iter [03900, 05004], lr: 0.099429, loss: 2.2883
2022-06-29 11:32:14 - train: epoch 0006, iter [04000, 05004], lr: 0.099425, loss: 2.5875
2022-06-29 11:32:47 - train: epoch 0006, iter [04100, 05004], lr: 0.099421, loss: 2.2510
2022-06-29 11:33:20 - train: epoch 0006, iter [04200, 05004], lr: 0.099417, loss: 2.2039
2022-06-29 11:33:53 - train: epoch 0006, iter [04300, 05004], lr: 0.099413, loss: 2.2934
2022-06-29 11:34:26 - train: epoch 0006, iter [04400, 05004], lr: 0.099409, loss: 2.3623
2022-06-29 11:34:59 - train: epoch 0006, iter [04500, 05004], lr: 0.099405, loss: 2.3087
2022-06-29 11:35:33 - train: epoch 0006, iter [04600, 05004], lr: 0.099401, loss: 2.2504
2022-06-29 11:36:06 - train: epoch 0006, iter [04700, 05004], lr: 0.099397, loss: 2.3911
2022-06-29 11:36:39 - train: epoch 0006, iter [04800, 05004], lr: 0.099393, loss: 2.2870
2022-06-29 11:37:12 - train: epoch 0006, iter [04900, 05004], lr: 0.099389, loss: 2.3539
2022-06-29 11:37:45 - train: epoch 0006, iter [05000, 05004], lr: 0.099385, loss: 2.1745
2022-06-29 11:37:47 - train: epoch 006, train_loss: 2.3471
2022-06-29 11:39:04 - eval: epoch: 006, acc1: 53.636%, acc5: 78.354%, test_loss: 2.0009, per_image_load_time: 2.251ms, per_image_inference_time: 0.646ms
2022-06-29 11:39:05 - until epoch: 006, best_acc1: 53.636%
2022-06-29 11:39:05 - epoch 007 lr: 0.099384
2022-06-29 11:39:44 - train: epoch 0007, iter [00100, 05004], lr: 0.099380, loss: 2.2294
2022-06-29 11:40:17 - train: epoch 0007, iter [00200, 05004], lr: 0.099376, loss: 2.5982
2022-06-29 11:40:50 - train: epoch 0007, iter [00300, 05004], lr: 0.099372, loss: 2.4987
2022-06-29 11:41:23 - train: epoch 0007, iter [00400, 05004], lr: 0.099368, loss: 2.4071
2022-06-29 11:41:56 - train: epoch 0007, iter [00500, 05004], lr: 0.099364, loss: 2.1921
2022-06-29 11:42:29 - train: epoch 0007, iter [00600, 05004], lr: 0.099360, loss: 2.4422
2022-06-29 11:43:02 - train: epoch 0007, iter [00700, 05004], lr: 0.099355, loss: 2.2400
2022-06-29 11:43:35 - train: epoch 0007, iter [00800, 05004], lr: 0.099351, loss: 2.3071
2022-06-29 11:44:08 - train: epoch 0007, iter [00900, 05004], lr: 0.099347, loss: 2.3422
2022-06-29 11:44:41 - train: epoch 0007, iter [01000, 05004], lr: 0.099343, loss: 2.3221
2022-06-29 11:45:14 - train: epoch 0007, iter [01100, 05004], lr: 0.099339, loss: 2.1884
2022-06-29 11:45:48 - train: epoch 0007, iter [01200, 05004], lr: 0.099334, loss: 2.2849
2022-06-29 11:46:21 - train: epoch 0007, iter [01300, 05004], lr: 0.099330, loss: 2.2382
2022-06-29 11:46:54 - train: epoch 0007, iter [01400, 05004], lr: 0.099326, loss: 2.2333
2022-06-29 11:47:27 - train: epoch 0007, iter [01500, 05004], lr: 0.099322, loss: 2.4811
2022-06-29 11:48:00 - train: epoch 0007, iter [01600, 05004], lr: 0.099317, loss: 2.2526
2022-06-29 11:48:33 - train: epoch 0007, iter [01700, 05004], lr: 0.099313, loss: 2.3604
2022-06-29 11:49:06 - train: epoch 0007, iter [01800, 05004], lr: 0.099309, loss: 2.1926
2022-06-29 11:49:39 - train: epoch 0007, iter [01900, 05004], lr: 0.099304, loss: 2.3200
2022-06-29 11:50:13 - train: epoch 0007, iter [02000, 05004], lr: 0.099300, loss: 2.1054
2022-06-29 11:50:46 - train: epoch 0007, iter [02100, 05004], lr: 0.099296, loss: 2.3443
2022-06-29 11:51:19 - train: epoch 0007, iter [02200, 05004], lr: 0.099291, loss: 2.2263
2022-06-29 11:51:52 - train: epoch 0007, iter [02300, 05004], lr: 0.099287, loss: 2.2323
2022-06-29 11:52:25 - train: epoch 0007, iter [02400, 05004], lr: 0.099282, loss: 2.3134
2022-06-29 11:52:58 - train: epoch 0007, iter [02500, 05004], lr: 0.099278, loss: 2.2451
2022-06-29 11:53:31 - train: epoch 0007, iter [02600, 05004], lr: 0.099273, loss: 2.2215
2022-06-29 11:54:04 - train: epoch 0007, iter [02700, 05004], lr: 0.099269, loss: 2.1677
2022-06-29 11:54:38 - train: epoch 0007, iter [02800, 05004], lr: 0.099265, loss: 2.3011
2022-06-29 11:55:11 - train: epoch 0007, iter [02900, 05004], lr: 0.099260, loss: 2.1638
2022-06-29 11:55:45 - train: epoch 0007, iter [03000, 05004], lr: 0.099256, loss: 2.3633
2022-06-29 11:56:18 - train: epoch 0007, iter [03100, 05004], lr: 0.099251, loss: 2.1296
2022-06-29 11:56:51 - train: epoch 0007, iter [03200, 05004], lr: 0.099247, loss: 2.2434
2022-06-29 11:57:24 - train: epoch 0007, iter [03300, 05004], lr: 0.099242, loss: 2.5076
2022-06-29 11:57:57 - train: epoch 0007, iter [03400, 05004], lr: 0.099237, loss: 2.0827
2022-06-29 11:58:30 - train: epoch 0007, iter [03500, 05004], lr: 0.099233, loss: 2.3491
2022-06-29 11:59:03 - train: epoch 0007, iter [03600, 05004], lr: 0.099228, loss: 2.0622
2022-06-29 11:59:36 - train: epoch 0007, iter [03700, 05004], lr: 0.099224, loss: 2.2933
2022-06-29 12:00:10 - train: epoch 0007, iter [03800, 05004], lr: 0.099219, loss: 2.5608
2022-06-29 12:00:43 - train: epoch 0007, iter [03900, 05004], lr: 0.099215, loss: 2.1099
2022-06-29 12:01:16 - train: epoch 0007, iter [04000, 05004], lr: 0.099210, loss: 2.3612
2022-06-29 12:01:49 - train: epoch 0007, iter [04100, 05004], lr: 0.099205, loss: 2.2557
2022-06-29 12:02:22 - train: epoch 0007, iter [04200, 05004], lr: 0.099201, loss: 2.1761
2022-06-29 12:02:55 - train: epoch 0007, iter [04300, 05004], lr: 0.099196, loss: 2.4098
2022-06-29 12:03:28 - train: epoch 0007, iter [04400, 05004], lr: 0.099191, loss: 2.1754
2022-06-29 12:04:01 - train: epoch 0007, iter [04500, 05004], lr: 0.099187, loss: 2.3895
2022-06-29 12:04:34 - train: epoch 0007, iter [04600, 05004], lr: 0.099182, loss: 2.4162
2022-06-29 12:05:07 - train: epoch 0007, iter [04700, 05004], lr: 0.099177, loss: 2.3957
2022-06-29 12:05:41 - train: epoch 0007, iter [04800, 05004], lr: 0.099172, loss: 2.5134
2022-06-29 12:06:14 - train: epoch 0007, iter [04900, 05004], lr: 0.099168, loss: 2.2061
2022-06-29 12:06:48 - train: epoch 0007, iter [05000, 05004], lr: 0.099163, loss: 2.2719
2022-06-29 12:06:49 - train: epoch 007, train_loss: 2.2804
2022-06-29 12:08:05 - eval: epoch: 007, acc1: 54.306%, acc5: 79.422%, test_loss: 1.9414, per_image_load_time: 2.189ms, per_image_inference_time: 0.659ms
2022-06-29 12:08:06 - until epoch: 007, best_acc1: 54.306%
2022-06-29 12:08:06 - epoch 008 lr: 0.099163
2022-06-29 12:08:45 - train: epoch 0008, iter [00100, 05004], lr: 0.099158, loss: 2.1860
2022-06-29 12:09:18 - train: epoch 0008, iter [00200, 05004], lr: 0.099153, loss: 2.3733
2022-06-29 12:09:50 - train: epoch 0008, iter [00300, 05004], lr: 0.099148, loss: 2.0089
2022-06-29 12:10:23 - train: epoch 0008, iter [00400, 05004], lr: 0.099144, loss: 2.0861
2022-06-29 12:10:56 - train: epoch 0008, iter [00500, 05004], lr: 0.099139, loss: 2.0600
2022-06-29 12:11:29 - train: epoch 0008, iter [00600, 05004], lr: 0.099134, loss: 2.1623
2022-06-29 12:12:02 - train: epoch 0008, iter [00700, 05004], lr: 0.099129, loss: 2.5003
2022-06-29 12:12:35 - train: epoch 0008, iter [00800, 05004], lr: 0.099124, loss: 2.1797
2022-06-29 12:13:09 - train: epoch 0008, iter [00900, 05004], lr: 0.099119, loss: 2.2076
2022-06-29 12:13:42 - train: epoch 0008, iter [01000, 05004], lr: 0.099114, loss: 2.2569
2022-06-29 12:14:16 - train: epoch 0008, iter [01100, 05004], lr: 0.099109, loss: 2.1081
2022-06-29 12:14:49 - train: epoch 0008, iter [01200, 05004], lr: 0.099105, loss: 2.0432
2022-06-29 12:15:22 - train: epoch 0008, iter [01300, 05004], lr: 0.099100, loss: 2.1905
2022-06-29 12:15:56 - train: epoch 0008, iter [01400, 05004], lr: 0.099095, loss: 2.2145
2022-06-29 12:16:29 - train: epoch 0008, iter [01500, 05004], lr: 0.099090, loss: 2.2527
2022-06-29 12:17:02 - train: epoch 0008, iter [01600, 05004], lr: 0.099085, loss: 2.2944
2022-06-29 12:17:36 - train: epoch 0008, iter [01700, 05004], lr: 0.099080, loss: 2.1660
2022-06-29 12:18:09 - train: epoch 0008, iter [01800, 05004], lr: 0.099075, loss: 2.3787
2022-06-29 12:18:42 - train: epoch 0008, iter [01900, 05004], lr: 0.099070, loss: 2.0590
2022-06-29 12:19:15 - train: epoch 0008, iter [02000, 05004], lr: 0.099065, loss: 2.1761
2022-06-29 12:19:48 - train: epoch 0008, iter [02100, 05004], lr: 0.099060, loss: 2.2491
2022-06-29 12:20:21 - train: epoch 0008, iter [02200, 05004], lr: 0.099055, loss: 2.1962
2022-06-29 12:20:55 - train: epoch 0008, iter [02300, 05004], lr: 0.099050, loss: 2.3005
2022-06-29 12:21:28 - train: epoch 0008, iter [02400, 05004], lr: 0.099044, loss: 2.0766
2022-06-29 12:22:01 - train: epoch 0008, iter [02500, 05004], lr: 0.099039, loss: 2.1877
2022-06-29 12:22:34 - train: epoch 0008, iter [02600, 05004], lr: 0.099034, loss: 2.3587
2022-06-29 12:23:07 - train: epoch 0008, iter [02700, 05004], lr: 0.099029, loss: 2.2572
2022-06-29 12:23:40 - train: epoch 0008, iter [02800, 05004], lr: 0.099024, loss: 2.2931
2022-06-29 12:24:13 - train: epoch 0008, iter [02900, 05004], lr: 0.099019, loss: 2.1196
2022-06-29 12:24:47 - train: epoch 0008, iter [03000, 05004], lr: 0.099014, loss: 2.2382
2022-06-29 12:25:20 - train: epoch 0008, iter [03100, 05004], lr: 0.099009, loss: 2.1946
2022-06-29 12:25:53 - train: epoch 0008, iter [03200, 05004], lr: 0.099003, loss: 2.4718
2022-06-29 12:26:26 - train: epoch 0008, iter [03300, 05004], lr: 0.098998, loss: 2.4550
2022-06-29 12:27:00 - train: epoch 0008, iter [03400, 05004], lr: 0.098993, loss: 2.5596
2022-06-29 12:27:33 - train: epoch 0008, iter [03500, 05004], lr: 0.098988, loss: 2.1280
2022-06-29 12:28:06 - train: epoch 0008, iter [03600, 05004], lr: 0.098982, loss: 2.3802
2022-06-29 12:28:39 - train: epoch 0008, iter [03700, 05004], lr: 0.098977, loss: 2.2108
2022-06-29 12:29:13 - train: epoch 0008, iter [03800, 05004], lr: 0.098972, loss: 2.1435
2022-06-29 12:29:46 - train: epoch 0008, iter [03900, 05004], lr: 0.098967, loss: 2.2327
2022-06-29 12:30:19 - train: epoch 0008, iter [04000, 05004], lr: 0.098961, loss: 2.4644
2022-06-29 12:30:53 - train: epoch 0008, iter [04100, 05004], lr: 0.098956, loss: 2.1918
2022-06-29 12:31:26 - train: epoch 0008, iter [04200, 05004], lr: 0.098951, loss: 2.1853
2022-06-29 12:31:59 - train: epoch 0008, iter [04300, 05004], lr: 0.098945, loss: 1.9299
2022-06-29 12:32:33 - train: epoch 0008, iter [04400, 05004], lr: 0.098940, loss: 2.1103
2022-06-29 12:33:06 - train: epoch 0008, iter [04500, 05004], lr: 0.098935, loss: 2.3500
2022-06-29 12:33:39 - train: epoch 0008, iter [04600, 05004], lr: 0.098929, loss: 2.3441
2022-06-29 12:34:12 - train: epoch 0008, iter [04700, 05004], lr: 0.098924, loss: 2.1203
2022-06-29 12:34:46 - train: epoch 0008, iter [04800, 05004], lr: 0.098918, loss: 2.2652
2022-06-29 12:35:19 - train: epoch 0008, iter [04900, 05004], lr: 0.098913, loss: 2.3209
2022-06-29 12:35:52 - train: epoch 0008, iter [05000, 05004], lr: 0.098908, loss: 2.1992
2022-06-29 12:35:54 - train: epoch 008, train_loss: 2.2320
2022-06-29 12:37:09 - eval: epoch: 008, acc1: 54.626%, acc5: 79.608%, test_loss: 1.9440, per_image_load_time: 2.284ms, per_image_inference_time: 0.631ms
2022-06-29 12:37:10 - until epoch: 008, best_acc1: 54.626%
2022-06-29 12:37:10 - epoch 009 lr: 0.098907
2022-06-29 12:37:49 - train: epoch 0009, iter [00100, 05004], lr: 0.098902, loss: 2.0090
2022-06-29 12:38:22 - train: epoch 0009, iter [00200, 05004], lr: 0.098896, loss: 1.9897
2022-06-29 12:38:55 - train: epoch 0009, iter [00300, 05004], lr: 0.098891, loss: 1.9034
2022-06-29 12:39:28 - train: epoch 0009, iter [00400, 05004], lr: 0.098886, loss: 2.2691
2022-06-29 12:40:01 - train: epoch 0009, iter [00500, 05004], lr: 0.098880, loss: 2.1992
2022-06-29 12:40:34 - train: epoch 0009, iter [00600, 05004], lr: 0.098875, loss: 2.0497
2022-06-29 12:41:07 - train: epoch 0009, iter [00700, 05004], lr: 0.098869, loss: 2.1815
2022-06-29 12:41:40 - train: epoch 0009, iter [00800, 05004], lr: 0.098863, loss: 2.0819
2022-06-29 12:42:12 - train: epoch 0009, iter [00900, 05004], lr: 0.098858, loss: 2.1007
2022-06-29 12:42:45 - train: epoch 0009, iter [01000, 05004], lr: 0.098852, loss: 1.9516
2022-06-29 12:43:18 - train: epoch 0009, iter [01100, 05004], lr: 0.098847, loss: 2.4746
2022-06-29 12:43:51 - train: epoch 0009, iter [01200, 05004], lr: 0.098841, loss: 2.3231
2022-06-29 12:44:24 - train: epoch 0009, iter [01300, 05004], lr: 0.098836, loss: 2.3288
2022-06-29 12:44:57 - train: epoch 0009, iter [01400, 05004], lr: 0.098830, loss: 1.8752
2022-06-29 12:45:31 - train: epoch 0009, iter [01500, 05004], lr: 0.098824, loss: 2.0248
2022-06-29 12:46:04 - train: epoch 0009, iter [01600, 05004], lr: 0.098819, loss: 2.3201
2022-06-29 12:46:36 - train: epoch 0009, iter [01700, 05004], lr: 0.098813, loss: 2.3489
2022-06-29 12:47:09 - train: epoch 0009, iter [01800, 05004], lr: 0.098807, loss: 2.1720
2022-06-29 12:47:42 - train: epoch 0009, iter [01900, 05004], lr: 0.098802, loss: 2.0207
2022-06-29 12:48:15 - train: epoch 0009, iter [02000, 05004], lr: 0.098796, loss: 1.8430
2022-06-29 12:48:48 - train: epoch 0009, iter [02100, 05004], lr: 0.098790, loss: 2.1999
2022-06-29 12:49:21 - train: epoch 0009, iter [02200, 05004], lr: 0.098784, loss: 2.3048
2022-06-29 12:49:55 - train: epoch 0009, iter [02300, 05004], lr: 0.098779, loss: 1.8756
2022-06-29 12:50:28 - train: epoch 0009, iter [02400, 05004], lr: 0.098773, loss: 2.1476
2022-06-29 12:51:00 - train: epoch 0009, iter [02500, 05004], lr: 0.098767, loss: 2.0177
2022-06-29 12:51:34 - train: epoch 0009, iter [02600, 05004], lr: 0.098761, loss: 2.1369
2022-06-29 12:52:07 - train: epoch 0009, iter [02700, 05004], lr: 0.098756, loss: 2.1694
2022-06-29 12:52:40 - train: epoch 0009, iter [02800, 05004], lr: 0.098750, loss: 2.2640
2022-06-29 12:53:12 - train: epoch 0009, iter [02900, 05004], lr: 0.098744, loss: 2.0189
2022-06-29 12:53:45 - train: epoch 0009, iter [03000, 05004], lr: 0.098738, loss: 1.9766
2022-06-29 12:54:18 - train: epoch 0009, iter [03100, 05004], lr: 0.098732, loss: 2.2263
2022-06-29 12:54:52 - train: epoch 0009, iter [03200, 05004], lr: 0.098726, loss: 2.3011
2022-06-29 12:55:25 - train: epoch 0009, iter [03300, 05004], lr: 0.098721, loss: 2.2334
2022-06-29 12:55:58 - train: epoch 0009, iter [03400, 05004], lr: 0.098715, loss: 2.3965
2022-06-29 12:56:31 - train: epoch 0009, iter [03500, 05004], lr: 0.098709, loss: 2.3956
2022-06-29 12:57:04 - train: epoch 0009, iter [03600, 05004], lr: 0.098703, loss: 2.1835
2022-06-29 12:57:37 - train: epoch 0009, iter [03700, 05004], lr: 0.098697, loss: 2.3673
2022-06-29 12:58:10 - train: epoch 0009, iter [03800, 05004], lr: 0.098691, loss: 2.3338
2022-06-29 12:58:43 - train: epoch 0009, iter [03900, 05004], lr: 0.098685, loss: 1.9299
2022-06-29 12:59:16 - train: epoch 0009, iter [04000, 05004], lr: 0.098679, loss: 2.3520
2022-06-29 12:59:49 - train: epoch 0009, iter [04100, 05004], lr: 0.098673, loss: 2.0745
2022-06-29 13:00:22 - train: epoch 0009, iter [04200, 05004], lr: 0.098667, loss: 2.0683
2022-06-29 13:00:55 - train: epoch 0009, iter [04300, 05004], lr: 0.098661, loss: 2.2450
2022-06-29 13:01:28 - train: epoch 0009, iter [04400, 05004], lr: 0.098655, loss: 2.1803
2022-06-29 13:02:01 - train: epoch 0009, iter [04500, 05004], lr: 0.098649, loss: 2.0437
2022-06-29 13:02:34 - train: epoch 0009, iter [04600, 05004], lr: 0.098643, loss: 2.2157
2022-06-29 13:03:07 - train: epoch 0009, iter [04700, 05004], lr: 0.098637, loss: 2.3353
2022-06-29 13:03:40 - train: epoch 0009, iter [04800, 05004], lr: 0.098631, loss: 2.4092
2022-06-29 13:04:13 - train: epoch 0009, iter [04900, 05004], lr: 0.098625, loss: 2.2404
2022-06-29 13:04:46 - train: epoch 0009, iter [05000, 05004], lr: 0.098619, loss: 2.0474
2022-06-29 13:04:48 - train: epoch 009, train_loss: 2.1915
2022-06-29 13:06:02 - eval: epoch: 009, acc1: 55.974%, acc5: 80.422%, test_loss: 1.8718, per_image_load_time: 2.133ms, per_image_inference_time: 0.636ms
2022-06-29 13:06:03 - until epoch: 009, best_acc1: 55.974%
2022-06-29 13:06:03 - epoch 010 lr: 0.098618
2022-06-29 13:06:42 - train: epoch 0010, iter [00100, 05004], lr: 0.098612, loss: 2.0386
2022-06-29 13:07:15 - train: epoch 0010, iter [00200, 05004], lr: 0.098606, loss: 2.2797
2022-06-29 13:07:48 - train: epoch 0010, iter [00300, 05004], lr: 0.098600, loss: 2.1804
2022-06-29 13:08:21 - train: epoch 0010, iter [00400, 05004], lr: 0.098594, loss: 2.2348
2022-06-29 13:08:54 - train: epoch 0010, iter [00500, 05004], lr: 0.098588, loss: 2.0759
2022-06-29 13:09:26 - train: epoch 0010, iter [00600, 05004], lr: 0.098582, loss: 2.1617
2022-06-29 13:09:59 - train: epoch 0010, iter [00700, 05004], lr: 0.098575, loss: 2.2374
2022-06-29 13:10:32 - train: epoch 0010, iter [00800, 05004], lr: 0.098569, loss: 1.9769
2022-06-29 13:11:05 - train: epoch 0010, iter [00900, 05004], lr: 0.098563, loss: 1.9462
2022-06-29 13:11:38 - train: epoch 0010, iter [01000, 05004], lr: 0.098557, loss: 1.9340
2022-06-29 13:12:11 - train: epoch 0010, iter [01100, 05004], lr: 0.098551, loss: 2.1132
2022-06-29 13:12:44 - train: epoch 0010, iter [01200, 05004], lr: 0.098544, loss: 1.9739
2022-06-29 13:13:17 - train: epoch 0010, iter [01300, 05004], lr: 0.098538, loss: 2.0048
2022-06-29 13:13:50 - train: epoch 0010, iter [01400, 05004], lr: 0.098532, loss: 2.1642
2022-06-29 13:14:23 - train: epoch 0010, iter [01500, 05004], lr: 0.098525, loss: 1.9779
2022-06-29 13:14:56 - train: epoch 0010, iter [01600, 05004], lr: 0.098519, loss: 2.1842
2022-06-29 13:15:30 - train: epoch 0010, iter [01700, 05004], lr: 0.098513, loss: 2.3353
2022-06-29 13:16:03 - train: epoch 0010, iter [01800, 05004], lr: 0.098506, loss: 2.1898
2022-06-29 13:16:36 - train: epoch 0010, iter [01900, 05004], lr: 0.098500, loss: 2.2411
2022-06-29 13:17:09 - train: epoch 0010, iter [02000, 05004], lr: 0.098494, loss: 2.2827
2022-06-29 13:17:42 - train: epoch 0010, iter [02100, 05004], lr: 0.098487, loss: 2.0782
2022-06-29 13:18:15 - train: epoch 0010, iter [02200, 05004], lr: 0.098481, loss: 2.3461
2022-06-29 13:18:48 - train: epoch 0010, iter [02300, 05004], lr: 0.098475, loss: 2.2975
2022-06-29 13:19:22 - train: epoch 0010, iter [02400, 05004], lr: 0.098468, loss: 2.3021
2022-06-29 13:19:55 - train: epoch 0010, iter [02500, 05004], lr: 0.098462, loss: 2.0141
2022-06-29 13:20:28 - train: epoch 0010, iter [02600, 05004], lr: 0.098455, loss: 2.3277
2022-06-29 13:21:02 - train: epoch 0010, iter [02700, 05004], lr: 0.098449, loss: 1.9675
2022-06-29 13:21:35 - train: epoch 0010, iter [02800, 05004], lr: 0.098442, loss: 2.1619
2022-06-29 13:22:08 - train: epoch 0010, iter [02900, 05004], lr: 0.098436, loss: 2.2209
2022-06-29 13:22:41 - train: epoch 0010, iter [03000, 05004], lr: 0.098429, loss: 2.1775
2022-06-29 13:23:14 - train: epoch 0010, iter [03100, 05004], lr: 0.098423, loss: 2.3971
2022-06-29 13:23:48 - train: epoch 0010, iter [03200, 05004], lr: 0.098416, loss: 2.2140
2022-06-29 13:24:21 - train: epoch 0010, iter [03300, 05004], lr: 0.098410, loss: 2.4016
2022-06-29 13:24:54 - train: epoch 0010, iter [03400, 05004], lr: 0.098403, loss: 2.3557
2022-06-29 13:25:27 - train: epoch 0010, iter [03500, 05004], lr: 0.098397, loss: 2.3885
2022-06-29 13:26:01 - train: epoch 0010, iter [03600, 05004], lr: 0.098390, loss: 2.3355
2022-06-29 13:26:34 - train: epoch 0010, iter [03700, 05004], lr: 0.098383, loss: 2.1434
2022-06-29 13:27:08 - train: epoch 0010, iter [03800, 05004], lr: 0.098377, loss: 2.2287
2022-06-29 13:27:41 - train: epoch 0010, iter [03900, 05004], lr: 0.098370, loss: 1.8042
2022-06-29 13:28:15 - train: epoch 0010, iter [04000, 05004], lr: 0.098364, loss: 2.1522
2022-06-29 13:28:48 - train: epoch 0010, iter [04100, 05004], lr: 0.098357, loss: 2.0032
2022-06-29 13:29:22 - train: epoch 0010, iter [04200, 05004], lr: 0.098350, loss: 2.2787
2022-06-29 13:29:55 - train: epoch 0010, iter [04300, 05004], lr: 0.098344, loss: 2.2513
2022-06-29 13:30:29 - train: epoch 0010, iter [04400, 05004], lr: 0.098337, loss: 2.0903
2022-06-29 13:31:02 - train: epoch 0010, iter [04500, 05004], lr: 0.098330, loss: 2.0435
2022-06-29 13:31:35 - train: epoch 0010, iter [04600, 05004], lr: 0.098324, loss: 2.1374
2022-06-29 13:32:09 - train: epoch 0010, iter [04700, 05004], lr: 0.098317, loss: 2.2501
2022-06-29 13:32:42 - train: epoch 0010, iter [04800, 05004], lr: 0.098310, loss: 2.0718
2022-06-29 13:33:15 - train: epoch 0010, iter [04900, 05004], lr: 0.098303, loss: 2.0073
2022-06-29 13:33:48 - train: epoch 0010, iter [05000, 05004], lr: 0.098297, loss: 1.9453
2022-06-29 13:33:49 - train: epoch 010, train_loss: 2.1576
2022-06-29 13:35:05 - eval: epoch: 010, acc1: 56.454%, acc5: 80.804%, test_loss: 1.8481, per_image_load_time: 2.250ms, per_image_inference_time: 0.638ms
2022-06-29 13:35:06 - until epoch: 010, best_acc1: 56.454%
2022-06-29 13:35:06 - epoch 011 lr: 0.098296
2022-06-29 13:35:46 - train: epoch 0011, iter [00100, 05004], lr: 0.098290, loss: 1.9365
2022-06-29 13:36:18 - train: epoch 0011, iter [00200, 05004], lr: 0.098283, loss: 2.2414
2022-06-29 13:36:50 - train: epoch 0011, iter [00300, 05004], lr: 0.098276, loss: 1.9270
2022-06-29 13:37:23 - train: epoch 0011, iter [00400, 05004], lr: 0.098269, loss: 2.1620
2022-06-29 13:37:56 - train: epoch 0011, iter [00500, 05004], lr: 0.098262, loss: 2.1085
2022-06-29 13:38:29 - train: epoch 0011, iter [00600, 05004], lr: 0.098255, loss: 2.1528
2022-06-29 13:39:02 - train: epoch 0011, iter [00700, 05004], lr: 0.098249, loss: 2.0811
2022-06-29 13:39:35 - train: epoch 0011, iter [00800, 05004], lr: 0.098242, loss: 2.0685
2022-06-29 13:40:08 - train: epoch 0011, iter [00900, 05004], lr: 0.098235, loss: 2.2360
2022-06-29 13:40:41 - train: epoch 0011, iter [01000, 05004], lr: 0.098228, loss: 2.0735
2022-06-29 13:41:14 - train: epoch 0011, iter [01100, 05004], lr: 0.098221, loss: 2.2579
2022-06-29 13:41:48 - train: epoch 0011, iter [01200, 05004], lr: 0.098214, loss: 2.4057
2022-06-29 13:42:21 - train: epoch 0011, iter [01300, 05004], lr: 0.098207, loss: 2.4249
2022-06-29 13:42:55 - train: epoch 0011, iter [01400, 05004], lr: 0.098200, loss: 2.2208
2022-06-29 13:43:28 - train: epoch 0011, iter [01500, 05004], lr: 0.098193, loss: 2.1468
2022-06-29 13:44:02 - train: epoch 0011, iter [01600, 05004], lr: 0.098186, loss: 2.1772
2022-06-29 13:44:35 - train: epoch 0011, iter [01700, 05004], lr: 0.098179, loss: 2.1721
2022-06-29 13:45:09 - train: epoch 0011, iter [01800, 05004], lr: 0.098172, loss: 1.9935
2022-06-29 13:45:42 - train: epoch 0011, iter [01900, 05004], lr: 0.098165, loss: 1.9282
2022-06-29 13:46:16 - train: epoch 0011, iter [02000, 05004], lr: 0.098158, loss: 2.2286
2022-06-29 13:46:49 - train: epoch 0011, iter [02100, 05004], lr: 0.098151, loss: 2.1570
2022-06-29 13:47:22 - train: epoch 0011, iter [02200, 05004], lr: 0.098144, loss: 2.0982
2022-06-29 13:47:56 - train: epoch 0011, iter [02300, 05004], lr: 0.098137, loss: 2.4401
2022-06-29 13:48:29 - train: epoch 0011, iter [02400, 05004], lr: 0.098130, loss: 1.9073
2022-06-29 13:49:03 - train: epoch 0011, iter [02500, 05004], lr: 0.098123, loss: 2.3703
2022-06-29 13:49:36 - train: epoch 0011, iter [02600, 05004], lr: 0.098116, loss: 2.1063
2022-06-29 13:50:10 - train: epoch 0011, iter [02700, 05004], lr: 0.098109, loss: 2.2464
2022-06-29 13:50:43 - train: epoch 0011, iter [02800, 05004], lr: 0.098102, loss: 1.8239
2022-06-29 13:51:17 - train: epoch 0011, iter [02900, 05004], lr: 0.098094, loss: 2.2300
2022-06-29 13:51:51 - train: epoch 0011, iter [03000, 05004], lr: 0.098087, loss: 2.4374
2022-06-29 13:52:24 - train: epoch 0011, iter [03100, 05004], lr: 0.098080, loss: 2.2374
2022-06-29 13:52:58 - train: epoch 0011, iter [03200, 05004], lr: 0.098073, loss: 2.0356
2022-06-29 13:53:32 - train: epoch 0011, iter [03300, 05004], lr: 0.098066, loss: 2.1579
2022-06-29 13:54:07 - train: epoch 0011, iter [03400, 05004], lr: 0.098058, loss: 2.0791
2022-06-29 13:54:40 - train: epoch 0011, iter [03500, 05004], lr: 0.098051, loss: 2.1159
2022-06-29 13:55:14 - train: epoch 0011, iter [03600, 05004], lr: 0.098044, loss: 2.1370
2022-06-29 13:55:48 - train: epoch 0011, iter [03700, 05004], lr: 0.098037, loss: 2.2443
2022-06-29 13:56:21 - train: epoch 0011, iter [03800, 05004], lr: 0.098029, loss: 1.9230
2022-06-29 13:56:55 - train: epoch 0011, iter [03900, 05004], lr: 0.098022, loss: 2.2220
2022-06-29 13:57:29 - train: epoch 0011, iter [04000, 05004], lr: 0.098015, loss: 2.0015
2022-06-29 13:58:02 - train: epoch 0011, iter [04100, 05004], lr: 0.098008, loss: 1.8628
2022-06-29 13:58:36 - train: epoch 0011, iter [04200, 05004], lr: 0.098000, loss: 2.0720
2022-06-29 13:59:09 - train: epoch 0011, iter [04300, 05004], lr: 0.097993, loss: 2.1551
2022-06-29 13:59:43 - train: epoch 0011, iter [04400, 05004], lr: 0.097986, loss: 1.9748
2022-06-29 14:00:16 - train: epoch 0011, iter [04500, 05004], lr: 0.097978, loss: 2.0069
2022-06-29 14:00:50 - train: epoch 0011, iter [04600, 05004], lr: 0.097971, loss: 2.0500
2022-06-29 14:01:24 - train: epoch 0011, iter [04700, 05004], lr: 0.097964, loss: 1.8930
2022-06-29 14:01:57 - train: epoch 0011, iter [04800, 05004], lr: 0.097956, loss: 2.0064
2022-06-29 14:02:31 - train: epoch 0011, iter [04900, 05004], lr: 0.097949, loss: 1.8713
2022-06-29 14:03:04 - train: epoch 0011, iter [05000, 05004], lr: 0.097941, loss: 2.1187
2022-06-29 14:03:06 - train: epoch 011, train_loss: 2.1280
2022-06-29 14:04:22 - eval: epoch: 011, acc1: 56.910%, acc5: 81.268%, test_loss: 1.8188, per_image_load_time: 2.228ms, per_image_inference_time: 0.636ms
2022-06-29 14:04:23 - until epoch: 011, best_acc1: 56.910%
2022-06-29 14:04:23 - epoch 012 lr: 0.097941
2022-06-29 14:05:02 - train: epoch 0012, iter [00100, 05004], lr: 0.097934, loss: 2.1363
2022-06-29 14:05:36 - train: epoch 0012, iter [00200, 05004], lr: 0.097926, loss: 2.0964
2022-06-29 14:06:08 - train: epoch 0012, iter [00300, 05004], lr: 0.097919, loss: 2.0276
2022-06-29 14:06:41 - train: epoch 0012, iter [00400, 05004], lr: 0.097911, loss: 2.0989
2022-06-29 14:07:14 - train: epoch 0012, iter [00500, 05004], lr: 0.097904, loss: 2.3366
2022-06-29 14:07:47 - train: epoch 0012, iter [00600, 05004], lr: 0.097896, loss: 1.8404
2022-06-29 14:08:21 - train: epoch 0012, iter [00700, 05004], lr: 0.097889, loss: 1.9576
2022-06-29 14:08:54 - train: epoch 0012, iter [00800, 05004], lr: 0.097881, loss: 2.1276
2022-06-29 14:09:28 - train: epoch 0012, iter [00900, 05004], lr: 0.097874, loss: 2.1922
2022-06-29 14:10:01 - train: epoch 0012, iter [01000, 05004], lr: 0.097866, loss: 1.9700
2022-06-29 14:10:34 - train: epoch 0012, iter [01100, 05004], lr: 0.097858, loss: 2.4455
2022-06-29 14:11:07 - train: epoch 0012, iter [01200, 05004], lr: 0.097851, loss: 1.9416
2022-06-29 14:11:40 - train: epoch 0012, iter [01300, 05004], lr: 0.097843, loss: 2.0146
2022-06-29 14:12:14 - train: epoch 0012, iter [01400, 05004], lr: 0.097836, loss: 2.2877
2022-06-29 14:12:48 - train: epoch 0012, iter [01500, 05004], lr: 0.097828, loss: 1.9233
2022-06-29 14:13:22 - train: epoch 0012, iter [01600, 05004], lr: 0.097820, loss: 1.9877
2022-06-29 14:13:56 - train: epoch 0012, iter [01700, 05004], lr: 0.097813, loss: 1.9163
2022-06-29 14:14:29 - train: epoch 0012, iter [01800, 05004], lr: 0.097805, loss: 2.0546
2022-06-29 14:15:03 - train: epoch 0012, iter [01900, 05004], lr: 0.097797, loss: 2.1256
2022-06-29 14:15:36 - train: epoch 0012, iter [02000, 05004], lr: 0.097790, loss: 2.1500
2022-06-29 14:16:10 - train: epoch 0012, iter [02100, 05004], lr: 0.097782, loss: 2.0397
2022-06-29 14:16:43 - train: epoch 0012, iter [02200, 05004], lr: 0.097774, loss: 2.2531
2022-06-29 14:17:17 - train: epoch 0012, iter [02300, 05004], lr: 0.097767, loss: 2.1692
2022-06-29 14:17:50 - train: epoch 0012, iter [02400, 05004], lr: 0.097759, loss: 2.1469
2022-06-29 14:18:24 - train: epoch 0012, iter [02500, 05004], lr: 0.097751, loss: 1.8778
2022-06-29 14:18:57 - train: epoch 0012, iter [02600, 05004], lr: 0.097743, loss: 1.9186
2022-06-29 14:19:31 - train: epoch 0012, iter [02700, 05004], lr: 0.097736, loss: 2.0525
2022-06-29 14:20:05 - train: epoch 0012, iter [02800, 05004], lr: 0.097728, loss: 2.1011
2022-06-29 14:20:38 - train: epoch 0012, iter [02900, 05004], lr: 0.097720, loss: 1.9144
2022-06-29 14:21:12 - train: epoch 0012, iter [03000, 05004], lr: 0.097712, loss: 1.9476
2022-06-29 14:21:45 - train: epoch 0012, iter [03100, 05004], lr: 0.097704, loss: 2.2714
2022-06-29 14:22:19 - train: epoch 0012, iter [03200, 05004], lr: 0.097697, loss: 1.9470
2022-06-29 14:22:53 - train: epoch 0012, iter [03300, 05004], lr: 0.097689, loss: 2.1662
2022-06-29 14:23:26 - train: epoch 0012, iter [03400, 05004], lr: 0.097681, loss: 2.1138
2022-06-29 14:23:59 - train: epoch 0012, iter [03500, 05004], lr: 0.097673, loss: 2.2184
2022-06-29 14:24:33 - train: epoch 0012, iter [03600, 05004], lr: 0.097665, loss: 2.0876
2022-06-29 14:25:07 - train: epoch 0012, iter [03700, 05004], lr: 0.097657, loss: 2.0836
2022-06-29 14:25:40 - train: epoch 0012, iter [03800, 05004], lr: 0.097649, loss: 2.0341
2022-06-29 14:26:14 - train: epoch 0012, iter [03900, 05004], lr: 0.097641, loss: 2.0292
2022-06-29 14:26:48 - train: epoch 0012, iter [04000, 05004], lr: 0.097633, loss: 2.0599
2022-06-29 14:27:22 - train: epoch 0012, iter [04100, 05004], lr: 0.097625, loss: 2.0278
2022-06-29 14:27:55 - train: epoch 0012, iter [04200, 05004], lr: 0.097617, loss: 1.9550
2022-06-29 14:28:29 - train: epoch 0012, iter [04300, 05004], lr: 0.097609, loss: 2.1615
2022-06-29 14:29:02 - train: epoch 0012, iter [04400, 05004], lr: 0.097601, loss: 1.9451
2022-06-29 14:29:36 - train: epoch 0012, iter [04500, 05004], lr: 0.097593, loss: 1.9850
2022-06-29 14:30:10 - train: epoch 0012, iter [04600, 05004], lr: 0.097585, loss: 2.4174
2022-06-29 14:30:44 - train: epoch 0012, iter [04700, 05004], lr: 0.097577, loss: 2.1150
2022-06-29 14:31:17 - train: epoch 0012, iter [04800, 05004], lr: 0.097569, loss: 2.2375
2022-06-29 14:31:51 - train: epoch 0012, iter [04900, 05004], lr: 0.097561, loss: 2.1815
2022-06-29 14:32:24 - train: epoch 0012, iter [05000, 05004], lr: 0.097553, loss: 1.8310
2022-06-29 14:32:26 - train: epoch 012, train_loss: 2.1083
2022-06-29 14:33:43 - eval: epoch: 012, acc1: 57.670%, acc5: 82.006%, test_loss: 1.7808, per_image_load_time: 2.380ms, per_image_inference_time: 0.611ms
2022-06-29 14:33:44 - until epoch: 012, best_acc1: 57.670%
2022-06-29 14:33:44 - epoch 013 lr: 0.097553
2022-06-29 14:34:23 - train: epoch 0013, iter [00100, 05004], lr: 0.097545, loss: 1.9116
2022-06-29 14:34:56 - train: epoch 0013, iter [00200, 05004], lr: 0.097537, loss: 2.0539
2022-06-29 14:35:29 - train: epoch 0013, iter [00300, 05004], lr: 0.097529, loss: 2.1245
2022-06-29 14:36:03 - train: epoch 0013, iter [00400, 05004], lr: 0.097520, loss: 1.9428
2022-06-29 14:36:36 - train: epoch 0013, iter [00500, 05004], lr: 0.097512, loss: 2.1126
2022-06-29 14:37:10 - train: epoch 0013, iter [00600, 05004], lr: 0.097504, loss: 2.2101
2022-06-29 14:37:43 - train: epoch 0013, iter [00700, 05004], lr: 0.097496, loss: 1.9508
2022-06-29 14:38:17 - train: epoch 0013, iter [00800, 05004], lr: 0.097488, loss: 2.2031
2022-06-29 14:38:50 - train: epoch 0013, iter [00900, 05004], lr: 0.097480, loss: 1.9260
2022-06-29 14:39:23 - train: epoch 0013, iter [01000, 05004], lr: 0.097471, loss: 2.1070
2022-06-29 14:39:57 - train: epoch 0013, iter [01100, 05004], lr: 0.097463, loss: 2.1256
2022-06-29 14:40:30 - train: epoch 0013, iter [01200, 05004], lr: 0.097455, loss: 2.2307
2022-06-29 14:41:04 - train: epoch 0013, iter [01300, 05004], lr: 0.097447, loss: 2.0300
2022-06-29 14:41:37 - train: epoch 0013, iter [01400, 05004], lr: 0.097438, loss: 1.9924
2022-06-29 14:42:11 - train: epoch 0013, iter [01500, 05004], lr: 0.097430, loss: 2.2090
2022-06-29 14:42:44 - train: epoch 0013, iter [01600, 05004], lr: 0.097422, loss: 1.8155
2022-06-29 14:43:17 - train: epoch 0013, iter [01700, 05004], lr: 0.097414, loss: 2.0563
2022-06-29 14:43:50 - train: epoch 0013, iter [01800, 05004], lr: 0.097405, loss: 2.1634
2022-06-29 14:44:24 - train: epoch 0013, iter [01900, 05004], lr: 0.097397, loss: 2.2049
2022-06-29 14:44:57 - train: epoch 0013, iter [02000, 05004], lr: 0.097389, loss: 2.2189
2022-06-29 14:45:31 - train: epoch 0013, iter [02100, 05004], lr: 0.097380, loss: 2.3126
2022-06-29 14:46:05 - train: epoch 0013, iter [02200, 05004], lr: 0.097372, loss: 2.0018
2022-06-29 14:46:38 - train: epoch 0013, iter [02300, 05004], lr: 0.097363, loss: 2.1706
2022-06-29 14:47:12 - train: epoch 0013, iter [02400, 05004], lr: 0.097355, loss: 2.0640
2022-06-29 14:47:45 - train: epoch 0013, iter [02500, 05004], lr: 0.097347, loss: 2.0230
2022-06-29 14:48:19 - train: epoch 0013, iter [02600, 05004], lr: 0.097338, loss: 1.9928
2022-06-29 14:48:52 - train: epoch 0013, iter [02700, 05004], lr: 0.097330, loss: 2.0117
2022-06-29 14:49:26 - train: epoch 0013, iter [02800, 05004], lr: 0.097321, loss: 2.1060
2022-06-29 14:50:00 - train: epoch 0013, iter [02900, 05004], lr: 0.097313, loss: 2.2312
2022-06-29 14:50:33 - train: epoch 0013, iter [03000, 05004], lr: 0.097304, loss: 1.8082
2022-06-29 14:51:07 - train: epoch 0013, iter [03100, 05004], lr: 0.097296, loss: 1.9754
2022-06-29 14:51:41 - train: epoch 0013, iter [03200, 05004], lr: 0.097287, loss: 2.0390
2022-06-29 14:52:14 - train: epoch 0013, iter [03300, 05004], lr: 0.097279, loss: 1.9459
2022-06-29 14:52:48 - train: epoch 0013, iter [03400, 05004], lr: 0.097270, loss: 2.0865
2022-06-29 14:53:21 - train: epoch 0013, iter [03500, 05004], lr: 0.097262, loss: 1.9815
2022-06-29 14:53:55 - train: epoch 0013, iter [03600, 05004], lr: 0.097253, loss: 2.3903
2022-06-29 14:54:28 - train: epoch 0013, iter [03700, 05004], lr: 0.097245, loss: 1.9083
2022-06-29 14:55:02 - train: epoch 0013, iter [03800, 05004], lr: 0.097236, loss: 2.2538
2022-06-29 14:55:35 - train: epoch 0013, iter [03900, 05004], lr: 0.097228, loss: 2.2003
2022-06-29 14:56:09 - train: epoch 0013, iter [04000, 05004], lr: 0.097219, loss: 2.0471
2022-06-29 14:56:42 - train: epoch 0013, iter [04100, 05004], lr: 0.097210, loss: 2.0561
2022-06-29 14:57:16 - train: epoch 0013, iter [04200, 05004], lr: 0.097202, loss: 1.9952
2022-06-29 14:57:50 - train: epoch 0013, iter [04300, 05004], lr: 0.097193, loss: 2.0051
2022-06-29 14:58:23 - train: epoch 0013, iter [04400, 05004], lr: 0.097185, loss: 2.0987
2022-06-29 14:58:57 - train: epoch 0013, iter [04500, 05004], lr: 0.097176, loss: 2.0185
2022-06-29 14:59:31 - train: epoch 0013, iter [04600, 05004], lr: 0.097167, loss: 2.1190
2022-06-29 15:00:04 - train: epoch 0013, iter [04700, 05004], lr: 0.097159, loss: 2.2220
2022-06-29 15:00:38 - train: epoch 0013, iter [04800, 05004], lr: 0.097150, loss: 2.1557
2022-06-29 15:01:11 - train: epoch 0013, iter [04900, 05004], lr: 0.097141, loss: 2.0766
2022-06-29 15:01:44 - train: epoch 0013, iter [05000, 05004], lr: 0.097132, loss: 2.1153
2022-06-29 15:01:46 - train: epoch 013, train_loss: 2.0860
2022-06-29 15:03:02 - eval: epoch: 013, acc1: 57.266%, acc5: 81.658%, test_loss: 1.8150, per_image_load_time: 2.353ms, per_image_inference_time: 0.621ms
2022-06-29 15:03:03 - until epoch: 013, best_acc1: 57.670%
2022-06-29 15:03:03 - epoch 014 lr: 0.097132
2022-06-29 15:03:42 - train: epoch 0014, iter [00100, 05004], lr: 0.097123, loss: 2.0228
2022-06-29 15:04:16 - train: epoch 0014, iter [00200, 05004], lr: 0.097115, loss: 2.0890
2022-06-29 15:04:49 - train: epoch 0014, iter [00300, 05004], lr: 0.097106, loss: 1.7948
2022-06-29 15:05:21 - train: epoch 0014, iter [00400, 05004], lr: 0.097097, loss: 1.9533
2022-06-29 15:05:54 - train: epoch 0014, iter [00500, 05004], lr: 0.097088, loss: 1.9368
2022-06-29 15:06:27 - train: epoch 0014, iter [00600, 05004], lr: 0.097079, loss: 2.1104
2022-06-29 15:07:00 - train: epoch 0014, iter [00700, 05004], lr: 0.097071, loss: 2.0414
2022-06-29 15:07:34 - train: epoch 0014, iter [00800, 05004], lr: 0.097062, loss: 2.0161
2022-06-29 15:08:07 - train: epoch 0014, iter [00900, 05004], lr: 0.097053, loss: 2.0850
2022-06-29 15:08:40 - train: epoch 0014, iter [01000, 05004], lr: 0.097044, loss: 2.2761
2022-06-29 15:09:14 - train: epoch 0014, iter [01100, 05004], lr: 0.097035, loss: 2.0626
2022-06-29 15:09:47 - train: epoch 0014, iter [01200, 05004], lr: 0.097026, loss: 2.1020
2022-06-29 15:10:21 - train: epoch 0014, iter [01300, 05004], lr: 0.097017, loss: 2.0775
2022-06-29 15:10:54 - train: epoch 0014, iter [01400, 05004], lr: 0.097009, loss: 2.1777
2022-06-29 15:11:28 - train: epoch 0014, iter [01500, 05004], lr: 0.097000, loss: 2.2186
2022-06-29 15:12:02 - train: epoch 0014, iter [01600, 05004], lr: 0.096991, loss: 1.9962
2022-06-29 15:12:35 - train: epoch 0014, iter [01700, 05004], lr: 0.096982, loss: 2.2598
2022-06-29 15:13:09 - train: epoch 0014, iter [01800, 05004], lr: 0.096973, loss: 2.3192
2022-06-29 15:13:43 - train: epoch 0014, iter [01900, 05004], lr: 0.096964, loss: 1.9809
2022-06-29 15:14:16 - train: epoch 0014, iter [02000, 05004], lr: 0.096955, loss: 1.9041
2022-06-29 15:14:49 - train: epoch 0014, iter [02100, 05004], lr: 0.096946, loss: 2.1259
2022-06-29 15:15:23 - train: epoch 0014, iter [02200, 05004], lr: 0.096937, loss: 2.0470
2022-06-29 15:15:56 - train: epoch 0014, iter [02300, 05004], lr: 0.096928, loss: 2.0246
2022-06-29 15:16:29 - train: epoch 0014, iter [02400, 05004], lr: 0.096919, loss: 2.2873
2022-06-29 15:17:02 - train: epoch 0014, iter [02500, 05004], lr: 0.096910, loss: 2.0425
2022-06-29 15:17:35 - train: epoch 0014, iter [02600, 05004], lr: 0.096901, loss: 2.0915
2022-06-29 15:18:09 - train: epoch 0014, iter [02700, 05004], lr: 0.096892, loss: 1.8610
2022-06-29 15:18:42 - train: epoch 0014, iter [02800, 05004], lr: 0.096883, loss: 2.4252
2022-06-29 15:19:15 - train: epoch 0014, iter [02900, 05004], lr: 0.096873, loss: 1.9819
2022-06-29 15:19:49 - train: epoch 0014, iter [03000, 05004], lr: 0.096864, loss: 2.1761
2022-06-29 15:20:22 - train: epoch 0014, iter [03100, 05004], lr: 0.096855, loss: 1.9039
2022-06-29 15:20:55 - train: epoch 0014, iter [03200, 05004], lr: 0.096846, loss: 2.0564
2022-06-29 15:21:29 - train: epoch 0014, iter [03300, 05004], lr: 0.096837, loss: 1.8767
2022-06-29 15:22:02 - train: epoch 0014, iter [03400, 05004], lr: 0.096828, loss: 2.0458
2022-06-29 15:22:35 - train: epoch 0014, iter [03500, 05004], lr: 0.096819, loss: 2.0092
2022-06-29 15:23:09 - train: epoch 0014, iter [03600, 05004], lr: 0.096809, loss: 2.0726
2022-06-29 15:23:42 - train: epoch 0014, iter [03700, 05004], lr: 0.096800, loss: 1.9737
2022-06-29 15:24:15 - train: epoch 0014, iter [03800, 05004], lr: 0.096791, loss: 2.2764
2022-06-29 15:24:48 - train: epoch 0014, iter [03900, 05004], lr: 0.096782, loss: 1.8992
2022-06-29 15:25:21 - train: epoch 0014, iter [04000, 05004], lr: 0.096772, loss: 2.2187
2022-06-29 15:25:54 - train: epoch 0014, iter [04100, 05004], lr: 0.096763, loss: 1.8834
2022-06-29 15:26:28 - train: epoch 0014, iter [04200, 05004], lr: 0.096754, loss: 2.0338
2022-06-29 15:27:00 - train: epoch 0014, iter [04300, 05004], lr: 0.096745, loss: 1.8488
2022-06-29 15:27:33 - train: epoch 0014, iter [04400, 05004], lr: 0.096735, loss: 1.8155
2022-06-29 15:28:05 - train: epoch 0014, iter [04500, 05004], lr: 0.096726, loss: 1.9690
2022-06-29 15:28:38 - train: epoch 0014, iter [04600, 05004], lr: 0.096717, loss: 1.9252
2022-06-29 15:29:11 - train: epoch 0014, iter [04700, 05004], lr: 0.096707, loss: 2.0495
2022-06-29 15:29:44 - train: epoch 0014, iter [04800, 05004], lr: 0.096698, loss: 1.9441
2022-06-29 15:30:16 - train: epoch 0014, iter [04900, 05004], lr: 0.096689, loss: 1.8034
2022-06-29 15:30:49 - train: epoch 0014, iter [05000, 05004], lr: 0.096679, loss: 2.0122
2022-06-29 15:30:51 - train: epoch 014, train_loss: 2.0606
2022-06-29 15:32:09 - eval: epoch: 014, acc1: 57.448%, acc5: 81.654%, test_loss: 1.8038, per_image_load_time: 2.344ms, per_image_inference_time: 0.644ms
2022-06-29 15:32:09 - until epoch: 014, best_acc1: 57.670%
2022-06-29 15:32:09 - epoch 015 lr: 0.096679
2022-06-29 15:32:50 - train: epoch 0015, iter [00100, 05004], lr: 0.096670, loss: 1.8804
2022-06-29 15:33:23 - train: epoch 0015, iter [00200, 05004], lr: 0.096660, loss: 2.0894
2022-06-29 15:33:56 - train: epoch 0015, iter [00300, 05004], lr: 0.096651, loss: 2.1912
2022-06-29 15:34:29 - train: epoch 0015, iter [00400, 05004], lr: 0.096641, loss: 1.9926
2022-06-29 15:35:02 - train: epoch 0015, iter [00500, 05004], lr: 0.096632, loss: 1.9917
2022-06-29 15:35:35 - train: epoch 0015, iter [00600, 05004], lr: 0.096623, loss: 2.2102
2022-06-29 15:36:07 - train: epoch 0015, iter [00700, 05004], lr: 0.096613, loss: 1.9674
2022-06-29 15:36:41 - train: epoch 0015, iter [00800, 05004], lr: 0.096604, loss: 1.8974
2022-06-29 15:37:14 - train: epoch 0015, iter [00900, 05004], lr: 0.096594, loss: 1.9490
2022-06-29 15:37:47 - train: epoch 0015, iter [01000, 05004], lr: 0.096585, loss: 2.1296
2022-06-29 15:38:20 - train: epoch 0015, iter [01100, 05004], lr: 0.096575, loss: 1.8455
2022-06-29 15:38:53 - train: epoch 0015, iter [01200, 05004], lr: 0.096566, loss: 1.9478
2022-06-29 15:39:27 - train: epoch 0015, iter [01300, 05004], lr: 0.096556, loss: 2.4043
2022-06-29 15:40:00 - train: epoch 0015, iter [01400, 05004], lr: 0.096547, loss: 1.8817
2022-06-29 15:40:34 - train: epoch 0015, iter [01500, 05004], lr: 0.096537, loss: 1.7315
2022-06-29 15:41:07 - train: epoch 0015, iter [01600, 05004], lr: 0.096527, loss: 2.0383
2022-06-29 15:41:41 - train: epoch 0015, iter [01700, 05004], lr: 0.096518, loss: 2.0983
2022-06-29 15:42:14 - train: epoch 0015, iter [01800, 05004], lr: 0.096508, loss: 1.9256
2022-06-29 15:42:48 - train: epoch 0015, iter [01900, 05004], lr: 0.096499, loss: 1.9387
2022-06-29 15:43:21 - train: epoch 0015, iter [02000, 05004], lr: 0.096489, loss: 2.0074
2022-06-29 15:43:54 - train: epoch 0015, iter [02100, 05004], lr: 0.096479, loss: 1.9271
2022-06-29 15:44:27 - train: epoch 0015, iter [02200, 05004], lr: 0.096470, loss: 2.2559
2022-06-29 15:45:00 - train: epoch 0015, iter [02300, 05004], lr: 0.096460, loss: 1.8084
2022-06-29 15:45:34 - train: epoch 0015, iter [02400, 05004], lr: 0.096450, loss: 2.1731
2022-06-29 15:46:07 - train: epoch 0015, iter [02500, 05004], lr: 0.096441, loss: 2.1366
2022-06-29 15:46:40 - train: epoch 0015, iter [02600, 05004], lr: 0.096431, loss: 1.7870
2022-06-29 15:47:14 - train: epoch 0015, iter [02700, 05004], lr: 0.096421, loss: 2.0732
2022-06-29 15:47:47 - train: epoch 0015, iter [02800, 05004], lr: 0.096412, loss: 2.2660
2022-06-29 15:48:21 - train: epoch 0015, iter [02900, 05004], lr: 0.096402, loss: 1.9699
2022-06-29 15:48:54 - train: epoch 0015, iter [03000, 05004], lr: 0.096392, loss: 1.8711
2022-06-29 15:49:27 - train: epoch 0015, iter [03100, 05004], lr: 0.096382, loss: 2.0196
2022-06-29 15:50:00 - train: epoch 0015, iter [03200, 05004], lr: 0.096373, loss: 1.9394
2022-06-29 15:50:34 - train: epoch 0015, iter [03300, 05004], lr: 0.096363, loss: 1.7940
2022-06-29 15:51:07 - train: epoch 0015, iter [03400, 05004], lr: 0.096353, loss: 2.1170
2022-06-29 15:51:41 - train: epoch 0015, iter [03500, 05004], lr: 0.096343, loss: 2.2496
2022-06-29 15:52:14 - train: epoch 0015, iter [03600, 05004], lr: 0.096333, loss: 1.9666
2022-06-29 15:52:48 - train: epoch 0015, iter [03700, 05004], lr: 0.096323, loss: 2.0292
2022-06-29 15:53:22 - train: epoch 0015, iter [03800, 05004], lr: 0.096314, loss: 2.0666
2022-06-29 15:53:55 - train: epoch 0015, iter [03900, 05004], lr: 0.096304, loss: 2.1237
2022-06-29 15:54:28 - train: epoch 0015, iter [04000, 05004], lr: 0.096294, loss: 2.0883
2022-06-29 15:55:00 - train: epoch 0015, iter [04100, 05004], lr: 0.096284, loss: 2.2810
2022-06-29 15:55:33 - train: epoch 0015, iter [04200, 05004], lr: 0.096274, loss: 1.7426
2022-06-29 15:56:07 - train: epoch 0015, iter [04300, 05004], lr: 0.096264, loss: 2.0300
2022-06-29 15:56:40 - train: epoch 0015, iter [04400, 05004], lr: 0.096254, loss: 2.1549
2022-06-29 15:57:12 - train: epoch 0015, iter [04500, 05004], lr: 0.096244, loss: 2.1069
2022-06-29 15:57:45 - train: epoch 0015, iter [04600, 05004], lr: 0.096234, loss: 1.9536
2022-06-29 15:58:18 - train: epoch 0015, iter [04700, 05004], lr: 0.096224, loss: 2.0853
2022-06-29 15:58:52 - train: epoch 0015, iter [04800, 05004], lr: 0.096214, loss: 1.9036
2022-06-29 15:59:25 - train: epoch 0015, iter [04900, 05004], lr: 0.096204, loss: 2.0186
2022-06-29 15:59:58 - train: epoch 0015, iter [05000, 05004], lr: 0.096194, loss: 2.1920
2022-06-29 16:00:00 - train: epoch 015, train_loss: 2.0430
2022-06-29 16:01:16 - eval: epoch: 015, acc1: 58.242%, acc5: 82.342%, test_loss: 1.7667, per_image_load_time: 2.276ms, per_image_inference_time: 0.634ms
2022-06-29 16:01:17 - until epoch: 015, best_acc1: 58.242%
2022-06-29 16:01:17 - epoch 016 lr: 0.096194
2022-06-29 16:01:56 - train: epoch 0016, iter [00100, 05004], lr: 0.096184, loss: 2.0443
2022-06-29 16:02:28 - train: epoch 0016, iter [00200, 05004], lr: 0.096174, loss: 1.7703
2022-06-29 16:03:01 - train: epoch 0016, iter [00300, 05004], lr: 0.096164, loss: 2.0295
2022-06-29 16:03:34 - train: epoch 0016, iter [00400, 05004], lr: 0.096154, loss: 2.1679
2022-06-29 16:04:07 - train: epoch 0016, iter [00500, 05004], lr: 0.096144, loss: 1.7994
2022-06-29 16:04:40 - train: epoch 0016, iter [00600, 05004], lr: 0.096134, loss: 2.0955
2022-06-29 16:05:13 - train: epoch 0016, iter [00700, 05004], lr: 0.096124, loss: 1.7059
2022-06-29 16:05:46 - train: epoch 0016, iter [00800, 05004], lr: 0.096113, loss: 2.0412
2022-06-29 16:06:18 - train: epoch 0016, iter [00900, 05004], lr: 0.096103, loss: 2.0375
2022-06-29 16:06:51 - train: epoch 0016, iter [01000, 05004], lr: 0.096093, loss: 1.8227
2022-06-29 16:07:23 - train: epoch 0016, iter [01100, 05004], lr: 0.096083, loss: 1.9655
2022-06-29 16:07:56 - train: epoch 0016, iter [01200, 05004], lr: 0.096073, loss: 1.9417
2022-06-29 16:08:28 - train: epoch 0016, iter [01300, 05004], lr: 0.096063, loss: 2.0236
2022-06-29 16:09:01 - train: epoch 0016, iter [01400, 05004], lr: 0.096053, loss: 1.9564
2022-06-29 16:09:33 - train: epoch 0016, iter [01500, 05004], lr: 0.096042, loss: 2.1145
2022-06-29 16:10:05 - train: epoch 0016, iter [01600, 05004], lr: 0.096032, loss: 2.1368
2022-06-29 16:10:37 - train: epoch 0016, iter [01700, 05004], lr: 0.096022, loss: 1.9968
2022-06-29 16:11:09 - train: epoch 0016, iter [01800, 05004], lr: 0.096012, loss: 2.0346
2022-06-29 16:11:42 - train: epoch 0016, iter [01900, 05004], lr: 0.096001, loss: 2.0008
2022-06-29 16:12:14 - train: epoch 0016, iter [02000, 05004], lr: 0.095991, loss: 1.8326
2022-06-29 16:12:46 - train: epoch 0016, iter [02100, 05004], lr: 0.095981, loss: 2.1492
2022-06-29 16:13:18 - train: epoch 0016, iter [02200, 05004], lr: 0.095971, loss: 2.0990
2022-06-29 16:13:51 - train: epoch 0016, iter [02300, 05004], lr: 0.095960, loss: 2.2472
2022-06-29 16:14:23 - train: epoch 0016, iter [02400, 05004], lr: 0.095950, loss: 2.1787
2022-06-29 16:14:55 - train: epoch 0016, iter [02500, 05004], lr: 0.095940, loss: 1.9129
2022-06-29 16:15:28 - train: epoch 0016, iter [02600, 05004], lr: 0.095929, loss: 2.1507
2022-06-29 16:16:00 - train: epoch 0016, iter [02700, 05004], lr: 0.095919, loss: 2.0693
2022-06-29 16:16:32 - train: epoch 0016, iter [02800, 05004], lr: 0.095909, loss: 1.8805
2022-06-29 16:17:04 - train: epoch 0016, iter [02900, 05004], lr: 0.095898, loss: 2.1609
2022-06-29 16:17:37 - train: epoch 0016, iter [03000, 05004], lr: 0.095888, loss: 2.2569
2022-06-29 16:18:09 - train: epoch 0016, iter [03100, 05004], lr: 0.095878, loss: 2.2273
2022-06-29 16:18:42 - train: epoch 0016, iter [03200, 05004], lr: 0.095867, loss: 2.1613
2022-06-29 16:19:15 - train: epoch 0016, iter [03300, 05004], lr: 0.095857, loss: 2.0762
2022-06-29 16:19:48 - train: epoch 0016, iter [03400, 05004], lr: 0.095846, loss: 1.9650
2022-06-29 16:20:21 - train: epoch 0016, iter [03500, 05004], lr: 0.095836, loss: 2.0038
2022-06-29 16:20:54 - train: epoch 0016, iter [03600, 05004], lr: 0.095825, loss: 1.8792
2022-06-29 16:21:26 - train: epoch 0016, iter [03700, 05004], lr: 0.095815, loss: 2.1704
2022-06-29 16:21:59 - train: epoch 0016, iter [03800, 05004], lr: 0.095804, loss: 2.4123
2022-06-29 16:22:32 - train: epoch 0016, iter [03900, 05004], lr: 0.095794, loss: 2.0782
2022-06-29 16:23:05 - train: epoch 0016, iter [04000, 05004], lr: 0.095783, loss: 2.1276
2022-06-29 16:23:38 - train: epoch 0016, iter [04100, 05004], lr: 0.095773, loss: 2.0178
2022-06-29 16:24:11 - train: epoch 0016, iter [04200, 05004], lr: 0.095762, loss: 2.1305
2022-06-29 16:24:43 - train: epoch 0016, iter [04300, 05004], lr: 0.095752, loss: 1.8778
2022-06-29 16:25:16 - train: epoch 0016, iter [04400, 05004], lr: 0.095741, loss: 1.9263
2022-06-29 16:25:49 - train: epoch 0016, iter [04500, 05004], lr: 0.095731, loss: 2.1138
2022-06-29 16:26:21 - train: epoch 0016, iter [04600, 05004], lr: 0.095720, loss: 1.9364
2022-06-29 16:26:53 - train: epoch 0016, iter [04700, 05004], lr: 0.095710, loss: 2.3330
2022-06-29 16:27:26 - train: epoch 0016, iter [04800, 05004], lr: 0.095699, loss: 1.9571
2022-06-29 16:27:58 - train: epoch 0016, iter [04900, 05004], lr: 0.095688, loss: 1.9489
2022-06-29 16:28:31 - train: epoch 0016, iter [05000, 05004], lr: 0.095678, loss: 2.1316
2022-06-29 16:28:32 - train: epoch 016, train_loss: 2.0258
2022-06-29 16:29:49 - eval: epoch: 016, acc1: 58.548%, acc5: 82.822%, test_loss: 1.7269, per_image_load_time: 2.301ms, per_image_inference_time: 0.628ms
2022-06-29 16:29:50 - until epoch: 016, best_acc1: 58.548%
2022-06-29 16:29:50 - epoch 017 lr: 0.095677
2022-06-29 16:30:29 - train: epoch 0017, iter [00100, 05004], lr: 0.095667, loss: 1.9187
2022-06-29 16:31:02 - train: epoch 0017, iter [00200, 05004], lr: 0.095656, loss: 2.0077
2022-06-29 16:31:34 - train: epoch 0017, iter [00300, 05004], lr: 0.095645, loss: 2.2920
2022-06-29 16:32:08 - train: epoch 0017, iter [00400, 05004], lr: 0.095635, loss: 1.7951
2022-06-29 16:32:41 - train: epoch 0017, iter [00500, 05004], lr: 0.095624, loss: 2.0027
2022-06-29 16:33:14 - train: epoch 0017, iter [00600, 05004], lr: 0.095613, loss: 2.4589
2022-06-29 16:33:48 - train: epoch 0017, iter [00700, 05004], lr: 0.095602, loss: 2.0458
2022-06-29 16:34:21 - train: epoch 0017, iter [00800, 05004], lr: 0.095592, loss: 1.9385
2022-06-29 16:34:54 - train: epoch 0017, iter [00900, 05004], lr: 0.095581, loss: 1.9632
2022-06-29 16:35:27 - train: epoch 0017, iter [01000, 05004], lr: 0.095570, loss: 1.8983
2022-06-29 16:36:01 - train: epoch 0017, iter [01100, 05004], lr: 0.095559, loss: 2.2089
2022-06-29 16:36:34 - train: epoch 0017, iter [01200, 05004], lr: 0.095549, loss: 2.1817
2022-06-29 16:37:08 - train: epoch 0017, iter [01300, 05004], lr: 0.095538, loss: 1.9566
2022-06-29 16:37:41 - train: epoch 0017, iter [01400, 05004], lr: 0.095527, loss: 2.1438
2022-06-29 16:38:14 - train: epoch 0017, iter [01500, 05004], lr: 0.095516, loss: 1.6933
2022-06-29 16:38:48 - train: epoch 0017, iter [01600, 05004], lr: 0.095505, loss: 1.8688
2022-06-29 16:39:21 - train: epoch 0017, iter [01700, 05004], lr: 0.095495, loss: 2.0241
2022-06-29 16:39:54 - train: epoch 0017, iter [01800, 05004], lr: 0.095484, loss: 1.9217
2022-06-29 16:40:26 - train: epoch 0017, iter [01900, 05004], lr: 0.095473, loss: 1.9551
2022-06-29 16:41:00 - train: epoch 0017, iter [02000, 05004], lr: 0.095462, loss: 2.2169
2022-06-29 16:41:33 - train: epoch 0017, iter [02100, 05004], lr: 0.095451, loss: 2.0814
2022-06-29 16:42:06 - train: epoch 0017, iter [02200, 05004], lr: 0.095440, loss: 1.8456
2022-06-29 16:42:39 - train: epoch 0017, iter [02300, 05004], lr: 0.095429, loss: 2.0052
2022-06-29 16:43:12 - train: epoch 0017, iter [02400, 05004], lr: 0.095418, loss: 1.9969
2022-06-29 16:43:46 - train: epoch 0017, iter [02500, 05004], lr: 0.095407, loss: 2.0952
2022-06-29 16:44:19 - train: epoch 0017, iter [02600, 05004], lr: 0.095396, loss: 1.9476
2022-06-29 16:44:52 - train: epoch 0017, iter [02700, 05004], lr: 0.095385, loss: 1.9413
2022-06-29 16:45:26 - train: epoch 0017, iter [02800, 05004], lr: 0.095374, loss: 2.1689
2022-06-29 16:45:59 - train: epoch 0017, iter [02900, 05004], lr: 0.095363, loss: 2.2693
2022-06-29 16:46:32 - train: epoch 0017, iter [03000, 05004], lr: 0.095352, loss: 1.8150
2022-06-29 16:47:06 - train: epoch 0017, iter [03100, 05004], lr: 0.095341, loss: 2.1369
2022-06-29 16:47:39 - train: epoch 0017, iter [03200, 05004], lr: 0.095330, loss: 1.8810
2022-06-29 16:48:12 - train: epoch 0017, iter [03300, 05004], lr: 0.095319, loss: 2.0948
2022-06-29 16:48:46 - train: epoch 0017, iter [03400, 05004], lr: 0.095308, loss: 1.8785
2022-06-29 16:49:19 - train: epoch 0017, iter [03500, 05004], lr: 0.095297, loss: 1.9672
2022-06-29 16:49:52 - train: epoch 0017, iter [03600, 05004], lr: 0.095286, loss: 2.2973
2022-06-29 16:50:25 - train: epoch 0017, iter [03700, 05004], lr: 0.095275, loss: 2.0116
2022-06-29 16:50:59 - train: epoch 0017, iter [03800, 05004], lr: 0.095264, loss: 2.2083
2022-06-29 16:51:32 - train: epoch 0017, iter [03900, 05004], lr: 0.095253, loss: 1.9416
2022-06-29 16:52:06 - train: epoch 0017, iter [04000, 05004], lr: 0.095242, loss: 1.8660
2022-06-29 16:52:39 - train: epoch 0017, iter [04100, 05004], lr: 0.095231, loss: 2.0995
2022-06-29 16:53:13 - train: epoch 0017, iter [04200, 05004], lr: 0.095219, loss: 2.0261
2022-06-29 16:53:46 - train: epoch 0017, iter [04300, 05004], lr: 0.095208, loss: 1.9827
2022-06-29 16:54:20 - train: epoch 0017, iter [04400, 05004], lr: 0.095197, loss: 1.9834
2022-06-29 16:54:53 - train: epoch 0017, iter [04500, 05004], lr: 0.095186, loss: 2.1359
2022-06-29 16:55:27 - train: epoch 0017, iter [04600, 05004], lr: 0.095175, loss: 1.9934
2022-06-29 16:56:00 - train: epoch 0017, iter [04700, 05004], lr: 0.095163, loss: 2.1902
2022-06-29 16:56:34 - train: epoch 0017, iter [04800, 05004], lr: 0.095152, loss: 2.2787
2022-06-29 16:57:07 - train: epoch 0017, iter [04900, 05004], lr: 0.095141, loss: 1.8676
2022-06-29 16:57:40 - train: epoch 0017, iter [05000, 05004], lr: 0.095130, loss: 1.8139
2022-06-29 16:57:42 - train: epoch 017, train_loss: 2.0119
2022-06-29 16:59:00 - eval: epoch: 017, acc1: 59.648%, acc5: 83.570%, test_loss: 1.6882, per_image_load_time: 2.333ms, per_image_inference_time: 0.637ms
2022-06-29 16:59:01 - until epoch: 017, best_acc1: 59.648%
2022-06-29 16:59:01 - epoch 018 lr: 0.095129
2022-06-29 16:59:40 - train: epoch 0018, iter [00100, 05004], lr: 0.095118, loss: 1.9875
2022-06-29 17:00:13 - train: epoch 0018, iter [00200, 05004], lr: 0.095107, loss: 1.9717
2022-06-29 17:00:47 - train: epoch 0018, iter [00300, 05004], lr: 0.095095, loss: 2.1658
2022-06-29 17:01:19 - train: epoch 0018, iter [00400, 05004], lr: 0.095084, loss: 2.1512
2022-06-29 17:01:52 - train: epoch 0018, iter [00500, 05004], lr: 0.095073, loss: 1.9497
2022-06-29 17:02:25 - train: epoch 0018, iter [00600, 05004], lr: 0.095061, loss: 2.1222
2022-06-29 17:02:58 - train: epoch 0018, iter [00700, 05004], lr: 0.095050, loss: 1.7599
2022-06-29 17:03:31 - train: epoch 0018, iter [00800, 05004], lr: 0.095039, loss: 2.0671
2022-06-29 17:04:03 - train: epoch 0018, iter [00900, 05004], lr: 0.095027, loss: 1.9937
2022-06-29 17:04:36 - train: epoch 0018, iter [01000, 05004], lr: 0.095016, loss: 1.8459
2022-06-29 17:05:08 - train: epoch 0018, iter [01100, 05004], lr: 0.095005, loss: 2.2245
2022-06-29 17:05:41 - train: epoch 0018, iter [01200, 05004], lr: 0.094993, loss: 1.9688
2022-06-29 17:06:14 - train: epoch 0018, iter [01300, 05004], lr: 0.094982, loss: 2.2896
2022-06-29 17:06:46 - train: epoch 0018, iter [01400, 05004], lr: 0.094970, loss: 1.9300
2022-06-29 17:07:19 - train: epoch 0018, iter [01500, 05004], lr: 0.094959, loss: 2.0966
2022-06-29 17:07:52 - train: epoch 0018, iter [01600, 05004], lr: 0.094947, loss: 1.9914
2022-06-29 17:08:25 - train: epoch 0018, iter [01700, 05004], lr: 0.094936, loss: 1.9399
2022-06-29 17:08:58 - train: epoch 0018, iter [01800, 05004], lr: 0.094925, loss: 1.7993
2022-06-29 17:09:30 - train: epoch 0018, iter [01900, 05004], lr: 0.094913, loss: 2.0709
2022-06-29 17:10:03 - train: epoch 0018, iter [02000, 05004], lr: 0.094902, loss: 2.1798
2022-06-29 17:10:36 - train: epoch 0018, iter [02100, 05004], lr: 0.094890, loss: 2.1766
2022-06-29 17:11:09 - train: epoch 0018, iter [02200, 05004], lr: 0.094879, loss: 1.9894
2022-06-29 17:11:42 - train: epoch 0018, iter [02300, 05004], lr: 0.094867, loss: 1.9566
2022-06-29 17:12:15 - train: epoch 0018, iter [02400, 05004], lr: 0.094855, loss: 1.8756
2022-06-29 17:12:47 - train: epoch 0018, iter [02500, 05004], lr: 0.094844, loss: 1.7124
2022-06-29 17:13:20 - train: epoch 0018, iter [02600, 05004], lr: 0.094832, loss: 1.8755
2022-06-29 17:13:53 - train: epoch 0018, iter [02700, 05004], lr: 0.094821, loss: 2.1717
2022-06-29 17:14:26 - train: epoch 0018, iter [02800, 05004], lr: 0.094809, loss: 1.7382
2022-06-29 17:14:58 - train: epoch 0018, iter [02900, 05004], lr: 0.094797, loss: 2.0059
2022-06-29 17:15:31 - train: epoch 0018, iter [03000, 05004], lr: 0.094786, loss: 2.0087
2022-06-29 17:16:04 - train: epoch 0018, iter [03100, 05004], lr: 0.094774, loss: 2.3628
2022-06-29 17:16:37 - train: epoch 0018, iter [03200, 05004], lr: 0.094763, loss: 1.8920
2022-06-29 17:17:10 - train: epoch 0018, iter [03300, 05004], lr: 0.094751, loss: 1.9585
2022-06-29 17:17:43 - train: epoch 0018, iter [03400, 05004], lr: 0.094739, loss: 1.9403
2022-06-29 17:18:16 - train: epoch 0018, iter [03500, 05004], lr: 0.094728, loss: 2.2161
2022-06-29 17:18:48 - train: epoch 0018, iter [03600, 05004], lr: 0.094716, loss: 1.9985
2022-06-29 17:19:21 - train: epoch 0018, iter [03700, 05004], lr: 0.094704, loss: 2.3251
2022-06-29 17:19:54 - train: epoch 0018, iter [03800, 05004], lr: 0.094692, loss: 2.1139
2022-06-29 17:20:27 - train: epoch 0018, iter [03900, 05004], lr: 0.094681, loss: 2.0022
2022-06-29 17:20:59 - train: epoch 0018, iter [04000, 05004], lr: 0.094669, loss: 1.9378
2022-06-29 17:21:32 - train: epoch 0018, iter [04100, 05004], lr: 0.094657, loss: 2.0623
2022-06-29 17:22:06 - train: epoch 0018, iter [04200, 05004], lr: 0.094645, loss: 2.0129
2022-06-29 17:22:39 - train: epoch 0018, iter [04300, 05004], lr: 0.094634, loss: 1.7503
2022-06-29 17:23:12 - train: epoch 0018, iter [04400, 05004], lr: 0.094622, loss: 2.1254
2022-06-29 17:23:45 - train: epoch 0018, iter [04500, 05004], lr: 0.094610, loss: 1.9427
2022-06-29 17:24:19 - train: epoch 0018, iter [04600, 05004], lr: 0.094598, loss: 1.8216
2022-06-29 17:24:52 - train: epoch 0018, iter [04700, 05004], lr: 0.094586, loss: 2.2053
2022-06-29 17:25:26 - train: epoch 0018, iter [04800, 05004], lr: 0.094575, loss: 2.0705
2022-06-29 17:25:59 - train: epoch 0018, iter [04900, 05004], lr: 0.094563, loss: 2.0339
2022-06-29 17:26:32 - train: epoch 0018, iter [05000, 05004], lr: 0.094551, loss: 2.1454
2022-06-29 17:26:34 - train: epoch 018, train_loss: 1.9951
2022-06-29 17:27:50 - eval: epoch: 018, acc1: 59.882%, acc5: 83.434%, test_loss: 1.6834, per_image_load_time: 2.223ms, per_image_inference_time: 0.638ms
2022-06-29 17:27:51 - until epoch: 018, best_acc1: 59.882%
2022-06-29 17:27:51 - epoch 019 lr: 0.094550
2022-06-29 17:28:31 - train: epoch 0019, iter [00100, 05004], lr: 0.094538, loss: 1.7723
2022-06-29 17:29:04 - train: epoch 0019, iter [00200, 05004], lr: 0.094527, loss: 2.1540
2022-06-29 17:29:37 - train: epoch 0019, iter [00300, 05004], lr: 0.094515, loss: 2.2121
2022-06-29 17:30:09 - train: epoch 0019, iter [00400, 05004], lr: 0.094503, loss: 1.8451
2022-06-29 17:30:42 - train: epoch 0019, iter [00500, 05004], lr: 0.094491, loss: 2.0055
2022-06-29 17:31:15 - train: epoch 0019, iter [00600, 05004], lr: 0.094479, loss: 1.9341
2022-06-29 17:31:47 - train: epoch 0019, iter [00700, 05004], lr: 0.094467, loss: 1.7977
2022-06-29 17:32:20 - train: epoch 0019, iter [00800, 05004], lr: 0.094455, loss: 2.1635
2022-06-29 17:32:52 - train: epoch 0019, iter [00900, 05004], lr: 0.094443, loss: 2.0460
2022-06-29 17:33:24 - train: epoch 0019, iter [01000, 05004], lr: 0.094431, loss: 2.0114
2022-06-29 17:33:57 - train: epoch 0019, iter [01100, 05004], lr: 0.094419, loss: 1.9050
2022-06-29 17:34:29 - train: epoch 0019, iter [01200, 05004], lr: 0.094407, loss: 2.0242
2022-06-29 17:35:02 - train: epoch 0019, iter [01300, 05004], lr: 0.094395, loss: 2.1147
2022-06-29 17:35:34 - train: epoch 0019, iter [01400, 05004], lr: 0.094383, loss: 1.9820
2022-06-29 17:36:07 - train: epoch 0019, iter [01500, 05004], lr: 0.094371, loss: 2.2713
2022-06-29 17:36:39 - train: epoch 0019, iter [01600, 05004], lr: 0.094359, loss: 2.0147
2022-06-29 17:37:13 - train: epoch 0019, iter [01700, 05004], lr: 0.094347, loss: 2.0526
2022-06-29 17:37:46 - train: epoch 0019, iter [01800, 05004], lr: 0.094335, loss: 1.9099
2022-06-29 17:38:19 - train: epoch 0019, iter [01900, 05004], lr: 0.094322, loss: 2.0808
2022-06-29 17:38:52 - train: epoch 0019, iter [02000, 05004], lr: 0.094310, loss: 1.9150
2022-06-29 17:39:24 - train: epoch 0019, iter [02100, 05004], lr: 0.094298, loss: 1.8261
2022-06-29 17:39:57 - train: epoch 0019, iter [02200, 05004], lr: 0.094286, loss: 1.9653
2022-06-29 17:40:30 - train: epoch 0019, iter [02300, 05004], lr: 0.094274, loss: 1.9724
2022-06-29 17:41:03 - train: epoch 0019, iter [02400, 05004], lr: 0.094262, loss: 2.0816
2022-06-29 17:41:36 - train: epoch 0019, iter [02500, 05004], lr: 0.094250, loss: 1.8726
2022-06-29 17:42:09 - train: epoch 0019, iter [02600, 05004], lr: 0.094237, loss: 1.9243
2022-06-29 17:42:41 - train: epoch 0019, iter [02700, 05004], lr: 0.094225, loss: 2.0783
2022-06-29 17:43:14 - train: epoch 0019, iter [02800, 05004], lr: 0.094213, loss: 2.1023
2022-06-29 17:43:46 - train: epoch 0019, iter [02900, 05004], lr: 0.094201, loss: 1.9628
2022-06-29 17:44:19 - train: epoch 0019, iter [03000, 05004], lr: 0.094189, loss: 2.1821
2022-06-29 17:44:51 - train: epoch 0019, iter [03100, 05004], lr: 0.094176, loss: 2.0003
2022-06-29 17:45:24 - train: epoch 0019, iter [03200, 05004], lr: 0.094164, loss: 1.6310
2022-06-29 17:45:57 - train: epoch 0019, iter [03300, 05004], lr: 0.094152, loss: 1.9817
2022-06-29 17:46:30 - train: epoch 0019, iter [03400, 05004], lr: 0.094140, loss: 2.1449
2022-06-29 17:47:02 - train: epoch 0019, iter [03500, 05004], lr: 0.094127, loss: 2.0100
2022-06-29 17:47:35 - train: epoch 0019, iter [03600, 05004], lr: 0.094115, loss: 1.8687
2022-06-29 17:48:08 - train: epoch 0019, iter [03700, 05004], lr: 0.094103, loss: 2.0523
2022-06-29 17:48:41 - train: epoch 0019, iter [03800, 05004], lr: 0.094090, loss: 2.1726
2022-06-29 17:49:14 - train: epoch 0019, iter [03900, 05004], lr: 0.094078, loss: 1.9588
2022-06-29 17:49:47 - train: epoch 0019, iter [04000, 05004], lr: 0.094066, loss: 1.8687
2022-06-29 17:50:20 - train: epoch 0019, iter [04100, 05004], lr: 0.094053, loss: 1.9750
2022-06-29 17:50:52 - train: epoch 0019, iter [04200, 05004], lr: 0.094041, loss: 1.9657
2022-06-29 17:51:25 - train: epoch 0019, iter [04300, 05004], lr: 0.094028, loss: 1.8405
2022-06-29 17:51:58 - train: epoch 0019, iter [04400, 05004], lr: 0.094016, loss: 2.0923
2022-06-29 17:52:31 - train: epoch 0019, iter [04500, 05004], lr: 0.094004, loss: 2.3280
2022-06-29 17:53:04 - train: epoch 0019, iter [04600, 05004], lr: 0.093991, loss: 1.9327
2022-06-29 17:53:37 - train: epoch 0019, iter [04700, 05004], lr: 0.093979, loss: 1.8822
2022-06-29 17:54:10 - train: epoch 0019, iter [04800, 05004], lr: 0.093966, loss: 1.9955
2022-06-29 17:54:43 - train: epoch 0019, iter [04900, 05004], lr: 0.093954, loss: 2.0999
2022-06-29 17:55:16 - train: epoch 0019, iter [05000, 05004], lr: 0.093941, loss: 2.0207
2022-06-29 17:55:17 - train: epoch 019, train_loss: 1.9874
2022-06-29 17:56:34 - eval: epoch: 019, acc1: 59.772%, acc5: 83.434%, test_loss: 1.6862, per_image_load_time: 2.300ms, per_image_inference_time: 0.640ms
2022-06-29 17:56:35 - until epoch: 019, best_acc1: 59.882%
2022-06-29 17:56:35 - epoch 020 lr: 0.093941
2022-06-29 17:57:15 - train: epoch 0020, iter [00100, 05004], lr: 0.093928, loss: 2.0989
2022-06-29 17:57:47 - train: epoch 0020, iter [00200, 05004], lr: 0.093916, loss: 1.7780
2022-06-29 17:58:19 - train: epoch 0020, iter [00300, 05004], lr: 0.093903, loss: 1.9368
2022-06-29 17:58:52 - train: epoch 0020, iter [00400, 05004], lr: 0.093891, loss: 1.7389
2022-06-29 17:59:25 - train: epoch 0020, iter [00500, 05004], lr: 0.093878, loss: 1.8138
2022-06-29 17:59:58 - train: epoch 0020, iter [00600, 05004], lr: 0.093866, loss: 2.0832
2022-06-29 18:00:31 - train: epoch 0020, iter [00700, 05004], lr: 0.093853, loss: 1.7731
2022-06-29 18:01:04 - train: epoch 0020, iter [00800, 05004], lr: 0.093841, loss: 2.1008
2022-06-29 18:01:37 - train: epoch 0020, iter [00900, 05004], lr: 0.093828, loss: 2.3088
2022-06-29 18:02:10 - train: epoch 0020, iter [01000, 05004], lr: 0.093815, loss: 1.9763
2022-06-29 18:02:43 - train: epoch 0020, iter [01100, 05004], lr: 0.093803, loss: 1.9404
2022-06-29 18:03:16 - train: epoch 0020, iter [01200, 05004], lr: 0.093790, loss: 1.7659
2022-06-29 18:03:48 - train: epoch 0020, iter [01300, 05004], lr: 0.093778, loss: 1.8991
2022-06-29 18:04:21 - train: epoch 0020, iter [01400, 05004], lr: 0.093765, loss: 2.1122
2022-06-29 18:04:54 - train: epoch 0020, iter [01500, 05004], lr: 0.093752, loss: 2.0431
2022-06-29 18:05:26 - train: epoch 0020, iter [01600, 05004], lr: 0.093740, loss: 1.8672
2022-06-29 18:06:00 - train: epoch 0020, iter [01700, 05004], lr: 0.093727, loss: 1.6628
2022-06-29 18:06:32 - train: epoch 0020, iter [01800, 05004], lr: 0.093714, loss: 1.9035
2022-06-29 18:07:05 - train: epoch 0020, iter [01900, 05004], lr: 0.093702, loss: 1.8251
2022-06-29 18:07:37 - train: epoch 0020, iter [02000, 05004], lr: 0.093689, loss: 2.0106
2022-06-29 18:08:10 - train: epoch 0020, iter [02100, 05004], lr: 0.093676, loss: 2.2747
2022-06-29 18:08:43 - train: epoch 0020, iter [02200, 05004], lr: 0.093663, loss: 1.8052
2022-06-29 18:09:16 - train: epoch 0020, iter [02300, 05004], lr: 0.093651, loss: 1.9543
2022-06-29 18:09:49 - train: epoch 0020, iter [02400, 05004], lr: 0.093638, loss: 2.1915
2022-06-29 18:10:22 - train: epoch 0020, iter [02500, 05004], lr: 0.093625, loss: 1.9051
2022-06-29 18:10:55 - train: epoch 0020, iter [02600, 05004], lr: 0.093612, loss: 1.8080
2022-06-29 18:11:28 - train: epoch 0020, iter [02700, 05004], lr: 0.093599, loss: 2.0115
2022-06-29 18:12:01 - train: epoch 0020, iter [02800, 05004], lr: 0.093587, loss: 1.9641
2022-06-29 18:12:34 - train: epoch 0020, iter [02900, 05004], lr: 0.093574, loss: 2.0495
2022-06-29 18:13:07 - train: epoch 0020, iter [03000, 05004], lr: 0.093561, loss: 1.9438
2022-06-29 18:13:40 - train: epoch 0020, iter [03100, 05004], lr: 0.093548, loss: 2.0784
2022-06-29 18:14:13 - train: epoch 0020, iter [03200, 05004], lr: 0.093535, loss: 2.0735
2022-06-29 18:14:47 - train: epoch 0020, iter [03300, 05004], lr: 0.093522, loss: 1.8833
2022-06-29 18:15:20 - train: epoch 0020, iter [03400, 05004], lr: 0.093510, loss: 2.0389
2022-06-29 18:15:53 - train: epoch 0020, iter [03500, 05004], lr: 0.093497, loss: 1.8048
2022-06-29 18:16:27 - train: epoch 0020, iter [03600, 05004], lr: 0.093484, loss: 1.9348
2022-06-29 18:17:00 - train: epoch 0020, iter [03700, 05004], lr: 0.093471, loss: 1.8748
2022-06-29 18:17:33 - train: epoch 0020, iter [03800, 05004], lr: 0.093458, loss: 1.9596
2022-06-29 18:18:06 - train: epoch 0020, iter [03900, 05004], lr: 0.093445, loss: 2.0693
2022-06-29 18:18:39 - train: epoch 0020, iter [04000, 05004], lr: 0.093432, loss: 1.8808
2022-06-29 18:19:12 - train: epoch 0020, iter [04100, 05004], lr: 0.093419, loss: 1.8466
2022-06-29 18:19:45 - train: epoch 0020, iter [04200, 05004], lr: 0.093406, loss: 1.9104
2022-06-29 18:20:18 - train: epoch 0020, iter [04300, 05004], lr: 0.093393, loss: 1.9432
2022-06-29 18:20:51 - train: epoch 0020, iter [04400, 05004], lr: 0.093380, loss: 2.0231
2022-06-29 18:21:25 - train: epoch 0020, iter [04500, 05004], lr: 0.093367, loss: 2.0651
2022-06-29 18:21:58 - train: epoch 0020, iter [04600, 05004], lr: 0.093354, loss: 2.0368
2022-06-29 18:22:32 - train: epoch 0020, iter [04700, 05004], lr: 0.093341, loss: 1.8110
2022-06-29 18:23:05 - train: epoch 0020, iter [04800, 05004], lr: 0.093328, loss: 1.9656
2022-06-29 18:23:38 - train: epoch 0020, iter [04900, 05004], lr: 0.093315, loss: 2.1501
2022-06-29 18:24:11 - train: epoch 0020, iter [05000, 05004], lr: 0.093302, loss: 1.8400
2022-06-29 18:24:13 - train: epoch 020, train_loss: 1.9701
2022-06-29 18:25:31 - eval: epoch: 020, acc1: 60.698%, acc5: 84.056%, test_loss: 1.6477, per_image_load_time: 2.354ms, per_image_inference_time: 0.627ms
2022-06-29 18:25:31 - until epoch: 020, best_acc1: 60.698%
2022-06-29 18:25:31 - epoch 021 lr: 0.093301
2022-06-29 18:26:11 - train: epoch 0021, iter [00100, 05004], lr: 0.093288, loss: 1.7934
2022-06-29 18:26:44 - train: epoch 0021, iter [00200, 05004], lr: 0.093275, loss: 1.8798
2022-06-29 18:27:16 - train: epoch 0021, iter [00300, 05004], lr: 0.093262, loss: 1.6092
2022-06-29 18:27:49 - train: epoch 0021, iter [00400, 05004], lr: 0.093249, loss: 2.0650
2022-06-29 18:28:22 - train: epoch 0021, iter [00500, 05004], lr: 0.093236, loss: 1.9214
2022-06-29 18:28:56 - train: epoch 0021, iter [00600, 05004], lr: 0.093223, loss: 1.8212
2022-06-29 18:29:29 - train: epoch 0021, iter [00700, 05004], lr: 0.093209, loss: 1.7272
2022-06-29 18:30:03 - train: epoch 0021, iter [00800, 05004], lr: 0.093196, loss: 2.1310
2022-06-29 18:30:36 - train: epoch 0021, iter [00900, 05004], lr: 0.093183, loss: 2.0357
2022-06-29 18:31:10 - train: epoch 0021, iter [01000, 05004], lr: 0.093170, loss: 1.8542
2022-06-29 18:31:44 - train: epoch 0021, iter [01100, 05004], lr: 0.093157, loss: 1.6929
2022-06-29 18:32:17 - train: epoch 0021, iter [01200, 05004], lr: 0.093143, loss: 1.8786
2022-06-29 18:32:51 - train: epoch 0021, iter [01300, 05004], lr: 0.093130, loss: 1.9766
2022-06-29 18:33:25 - train: epoch 0021, iter [01400, 05004], lr: 0.093117, loss: 1.9188
2022-06-29 18:33:58 - train: epoch 0021, iter [01500, 05004], lr: 0.093104, loss: 1.7508
2022-06-29 18:34:32 - train: epoch 0021, iter [01600, 05004], lr: 0.093090, loss: 2.0339
2022-06-29 18:35:06 - train: epoch 0021, iter [01700, 05004], lr: 0.093077, loss: 1.9496
2022-06-29 18:35:40 - train: epoch 0021, iter [01800, 05004], lr: 0.093064, loss: 1.8038
2022-06-29 18:36:13 - train: epoch 0021, iter [01900, 05004], lr: 0.093051, loss: 2.0396
2022-06-29 18:36:47 - train: epoch 0021, iter [02000, 05004], lr: 0.093037, loss: 2.1631
2022-06-29 18:37:21 - train: epoch 0021, iter [02100, 05004], lr: 0.093024, loss: 1.7994
2022-06-29 18:37:54 - train: epoch 0021, iter [02200, 05004], lr: 0.093011, loss: 1.8867
2022-06-29 18:38:28 - train: epoch 0021, iter [02300, 05004], lr: 0.092997, loss: 1.8454
2022-06-29 18:39:02 - train: epoch 0021, iter [02400, 05004], lr: 0.092984, loss: 1.9599
2022-06-29 18:39:35 - train: epoch 0021, iter [02500, 05004], lr: 0.092971, loss: 1.9780
2022-06-29 18:40:09 - train: epoch 0021, iter [02600, 05004], lr: 0.092957, loss: 2.1505
2022-06-29 18:40:43 - train: epoch 0021, iter [02700, 05004], lr: 0.092944, loss: 1.8170
2022-06-29 18:41:17 - train: epoch 0021, iter [02800, 05004], lr: 0.092930, loss: 1.7665
2022-06-29 18:41:51 - train: epoch 0021, iter [02900, 05004], lr: 0.092917, loss: 1.8445
2022-06-29 18:42:24 - train: epoch 0021, iter [03000, 05004], lr: 0.092904, loss: 2.0658
2022-06-29 18:42:58 - train: epoch 0021, iter [03100, 05004], lr: 0.092890, loss: 2.0254
2022-06-29 18:43:32 - train: epoch 0021, iter [03200, 05004], lr: 0.092877, loss: 2.0005
2022-06-29 18:44:05 - train: epoch 0021, iter [03300, 05004], lr: 0.092863, loss: 2.2369
2022-06-29 18:44:39 - train: epoch 0021, iter [03400, 05004], lr: 0.092850, loss: 2.0121
2022-06-29 18:45:13 - train: epoch 0021, iter [03500, 05004], lr: 0.092836, loss: 1.7977
2022-06-29 18:45:47 - train: epoch 0021, iter [03600, 05004], lr: 0.092823, loss: 1.9012
2022-06-29 18:46:21 - train: epoch 0021, iter [03700, 05004], lr: 0.092809, loss: 1.8838
2022-06-29 18:46:54 - train: epoch 0021, iter [03800, 05004], lr: 0.092796, loss: 1.9921
2022-06-29 18:47:28 - train: epoch 0021, iter [03900, 05004], lr: 0.092782, loss: 1.8373
2022-06-29 18:48:02 - train: epoch 0021, iter [04000, 05004], lr: 0.092769, loss: 2.1730
2022-06-29 18:48:36 - train: epoch 0021, iter [04100, 05004], lr: 0.092755, loss: 1.8663
2022-06-29 18:49:09 - train: epoch 0021, iter [04200, 05004], lr: 0.092742, loss: 1.8559
2022-06-29 18:49:43 - train: epoch 0021, iter [04300, 05004], lr: 0.092728, loss: 1.8681
2022-06-29 18:50:16 - train: epoch 0021, iter [04400, 05004], lr: 0.092714, loss: 1.9783
2022-06-29 18:50:50 - train: epoch 0021, iter [04500, 05004], lr: 0.092701, loss: 2.1796
2022-06-29 18:51:23 - train: epoch 0021, iter [04600, 05004], lr: 0.092687, loss: 1.8493
2022-06-29 18:51:57 - train: epoch 0021, iter [04700, 05004], lr: 0.092674, loss: 2.0999
2022-06-29 18:52:31 - train: epoch 0021, iter [04800, 05004], lr: 0.092660, loss: 2.1229
2022-06-29 18:53:04 - train: epoch 0021, iter [04900, 05004], lr: 0.092646, loss: 1.7666
2022-06-29 18:53:38 - train: epoch 0021, iter [05000, 05004], lr: 0.092633, loss: 1.8953
2022-06-29 18:53:39 - train: epoch 021, train_loss: 1.9607
2022-06-29 18:54:55 - eval: epoch: 021, acc1: 60.320%, acc5: 83.876%, test_loss: 1.6634, per_image_load_time: 1.880ms, per_image_inference_time: 0.622ms
2022-06-29 18:54:56 - until epoch: 021, best_acc1: 60.698%
2022-06-29 18:54:56 - epoch 022 lr: 0.092632
2022-06-29 18:55:36 - train: epoch 0022, iter [00100, 05004], lr: 0.092618, loss: 1.6954
2022-06-29 18:56:08 - train: epoch 0022, iter [00200, 05004], lr: 0.092605, loss: 1.8319
2022-06-29 18:56:41 - train: epoch 0022, iter [00300, 05004], lr: 0.092591, loss: 1.6589
2022-06-29 18:57:14 - train: epoch 0022, iter [00400, 05004], lr: 0.092577, loss: 1.7302
2022-06-29 18:57:47 - train: epoch 0022, iter [00500, 05004], lr: 0.092564, loss: 1.8766
2022-06-29 18:58:20 - train: epoch 0022, iter [00600, 05004], lr: 0.092550, loss: 2.1341
2022-06-29 18:58:53 - train: epoch 0022, iter [00700, 05004], lr: 0.092536, loss: 2.0338
2022-06-29 18:59:25 - train: epoch 0022, iter [00800, 05004], lr: 0.092522, loss: 2.1153
2022-06-29 18:59:59 - train: epoch 0022, iter [00900, 05004], lr: 0.092509, loss: 2.0607
2022-06-29 19:00:32 - train: epoch 0022, iter [01000, 05004], lr: 0.092495, loss: 2.0469
2022-06-29 19:01:05 - train: epoch 0022, iter [01100, 05004], lr: 0.092481, loss: 1.9187
2022-06-29 19:01:38 - train: epoch 0022, iter [01200, 05004], lr: 0.092467, loss: 1.6618
2022-06-29 19:02:11 - train: epoch 0022, iter [01300, 05004], lr: 0.092453, loss: 1.9740
2022-06-29 19:02:45 - train: epoch 0022, iter [01400, 05004], lr: 0.092440, loss: 2.0197
2022-06-29 19:03:18 - train: epoch 0022, iter [01500, 05004], lr: 0.092426, loss: 1.8985
2022-06-29 19:03:50 - train: epoch 0022, iter [01600, 05004], lr: 0.092412, loss: 1.8315
2022-06-29 19:04:23 - train: epoch 0022, iter [01700, 05004], lr: 0.092398, loss: 1.6423
2022-06-29 19:04:56 - train: epoch 0022, iter [01800, 05004], lr: 0.092384, loss: 2.1343
2022-06-29 19:05:29 - train: epoch 0022, iter [01900, 05004], lr: 0.092370, loss: 1.7511
2022-06-29 19:06:02 - train: epoch 0022, iter [02000, 05004], lr: 0.092356, loss: 2.0555
2022-06-29 19:06:34 - train: epoch 0022, iter [02100, 05004], lr: 0.092342, loss: 2.0711
2022-06-29 19:07:07 - train: epoch 0022, iter [02200, 05004], lr: 0.092328, loss: 1.8377
2022-06-29 19:07:40 - train: epoch 0022, iter [02300, 05004], lr: 0.092315, loss: 2.0509
2022-06-29 19:08:13 - train: epoch 0022, iter [02400, 05004], lr: 0.092301, loss: 2.0399
2022-06-29 19:08:47 - train: epoch 0022, iter [02500, 05004], lr: 0.092287, loss: 1.9026
2022-06-29 19:09:19 - train: epoch 0022, iter [02600, 05004], lr: 0.092273, loss: 1.7666
2022-06-29 19:09:51 - train: epoch 0022, iter [02700, 05004], lr: 0.092259, loss: 1.8639
2022-06-29 19:10:24 - train: epoch 0022, iter [02800, 05004], lr: 0.092245, loss: 2.2770
2022-06-29 19:10:56 - train: epoch 0022, iter [02900, 05004], lr: 0.092231, loss: 1.9218
2022-06-29 19:11:29 - train: epoch 0022, iter [03000, 05004], lr: 0.092217, loss: 1.9867
2022-06-29 19:12:01 - train: epoch 0022, iter [03100, 05004], lr: 0.092203, loss: 2.0250
2022-06-29 19:12:33 - train: epoch 0022, iter [03200, 05004], lr: 0.092189, loss: 2.0673
2022-06-29 19:13:05 - train: epoch 0022, iter [03300, 05004], lr: 0.092175, loss: 1.9196
2022-06-29 19:13:37 - train: epoch 0022, iter [03400, 05004], lr: 0.092161, loss: 1.7404
2022-06-29 19:14:10 - train: epoch 0022, iter [03500, 05004], lr: 0.092147, loss: 2.0312
2022-06-29 19:14:42 - train: epoch 0022, iter [03600, 05004], lr: 0.092132, loss: 1.9613
2022-06-29 19:15:15 - train: epoch 0022, iter [03700, 05004], lr: 0.092118, loss: 2.0190
2022-06-29 19:15:47 - train: epoch 0022, iter [03800, 05004], lr: 0.092104, loss: 2.1922
2022-06-29 19:16:20 - train: epoch 0022, iter [03900, 05004], lr: 0.092090, loss: 1.8929
2022-06-29 19:16:53 - train: epoch 0022, iter [04000, 05004], lr: 0.092076, loss: 2.0217
2022-06-29 19:17:25 - train: epoch 0022, iter [04100, 05004], lr: 0.092062, loss: 1.9075
2022-06-29 19:17:58 - train: epoch 0022, iter [04200, 05004], lr: 0.092048, loss: 1.9652
2022-06-29 19:18:31 - train: epoch 0022, iter [04300, 05004], lr: 0.092034, loss: 2.1995
2022-06-29 19:19:04 - train: epoch 0022, iter [04400, 05004], lr: 0.092019, loss: 1.9359
2022-06-29 19:19:37 - train: epoch 0022, iter [04500, 05004], lr: 0.092005, loss: 1.8950
2022-06-29 19:20:09 - train: epoch 0022, iter [04600, 05004], lr: 0.091991, loss: 2.1487
2022-06-29 19:20:42 - train: epoch 0022, iter [04700, 05004], lr: 0.091977, loss: 2.0738
2022-06-29 19:21:15 - train: epoch 0022, iter [04800, 05004], lr: 0.091963, loss: 1.6994
2022-06-29 19:21:47 - train: epoch 0022, iter [04900, 05004], lr: 0.091948, loss: 1.7322
2022-06-29 19:22:20 - train: epoch 0022, iter [05000, 05004], lr: 0.091934, loss: 1.8386
2022-06-29 19:22:21 - train: epoch 022, train_loss: 1.9509
2022-06-29 19:23:37 - eval: epoch: 022, acc1: 60.306%, acc5: 83.856%, test_loss: 1.6635, per_image_load_time: 2.233ms, per_image_inference_time: 0.642ms
2022-06-29 19:23:38 - until epoch: 022, best_acc1: 60.698%
2022-06-29 19:23:38 - epoch 023 lr: 0.091933
2022-06-29 19:24:17 - train: epoch 0023, iter [00100, 05004], lr: 0.091919, loss: 1.7557
2022-06-29 19:24:50 - train: epoch 0023, iter [00200, 05004], lr: 0.091905, loss: 1.5894
2022-06-29 19:25:23 - train: epoch 0023, iter [00300, 05004], lr: 0.091891, loss: 1.7461
2022-06-29 19:25:55 - train: epoch 0023, iter [00400, 05004], lr: 0.091876, loss: 1.9608
2022-06-29 19:26:28 - train: epoch 0023, iter [00500, 05004], lr: 0.091862, loss: 1.8849
2022-06-29 19:27:00 - train: epoch 0023, iter [00600, 05004], lr: 0.091848, loss: 1.8197
2022-06-29 19:27:32 - train: epoch 0023, iter [00700, 05004], lr: 0.091834, loss: 1.6594
2022-06-29 19:28:05 - train: epoch 0023, iter [00800, 05004], lr: 0.091819, loss: 1.8219
2022-06-29 19:28:38 - train: epoch 0023, iter [00900, 05004], lr: 0.091805, loss: 1.9623
2022-06-29 19:29:10 - train: epoch 0023, iter [01000, 05004], lr: 0.091790, loss: 1.8001
2022-06-29 19:29:43 - train: epoch 0023, iter [01100, 05004], lr: 0.091776, loss: 2.0728
2022-06-29 19:30:16 - train: epoch 0023, iter [01200, 05004], lr: 0.091762, loss: 1.7666
2022-06-29 19:30:49 - train: epoch 0023, iter [01300, 05004], lr: 0.091747, loss: 1.8730
2022-06-29 19:31:21 - train: epoch 0023, iter [01400, 05004], lr: 0.091733, loss: 2.1229
2022-06-29 19:31:54 - train: epoch 0023, iter [01500, 05004], lr: 0.091719, loss: 1.7159
2022-06-29 19:32:26 - train: epoch 0023, iter [01600, 05004], lr: 0.091704, loss: 1.8970
2022-06-29 19:32:59 - train: epoch 0023, iter [01700, 05004], lr: 0.091690, loss: 2.0288
2022-06-29 19:33:32 - train: epoch 0023, iter [01800, 05004], lr: 0.091675, loss: 1.8573
2022-06-29 19:34:05 - train: epoch 0023, iter [01900, 05004], lr: 0.091661, loss: 1.9846
2022-06-29 19:34:37 - train: epoch 0023, iter [02000, 05004], lr: 0.091646, loss: 1.6004
2022-06-29 19:35:10 - train: epoch 0023, iter [02100, 05004], lr: 0.091632, loss: 2.1057
2022-06-29 19:35:43 - train: epoch 0023, iter [02200, 05004], lr: 0.091617, loss: 1.7680
2022-06-29 19:36:16 - train: epoch 0023, iter [02300, 05004], lr: 0.091603, loss: 1.8109
2022-06-29 19:36:49 - train: epoch 0023, iter [02400, 05004], lr: 0.091588, loss: 1.9433
2022-06-29 19:37:22 - train: epoch 0023, iter [02500, 05004], lr: 0.091574, loss: 2.0745
2022-06-29 19:37:55 - train: epoch 0023, iter [02600, 05004], lr: 0.091559, loss: 1.9871
2022-06-29 19:38:28 - train: epoch 0023, iter [02700, 05004], lr: 0.091545, loss: 1.8216
2022-06-29 19:39:01 - train: epoch 0023, iter [02800, 05004], lr: 0.091530, loss: 1.8842
2022-06-29 19:39:35 - train: epoch 0023, iter [02900, 05004], lr: 0.091516, loss: 1.9267
2022-06-29 19:40:08 - train: epoch 0023, iter [03000, 05004], lr: 0.091501, loss: 2.2370
2022-06-29 19:40:42 - train: epoch 0023, iter [03100, 05004], lr: 0.091486, loss: 1.9337
2022-06-29 19:41:15 - train: epoch 0023, iter [03200, 05004], lr: 0.091472, loss: 2.0597
2022-06-29 19:41:48 - train: epoch 0023, iter [03300, 05004], lr: 0.091457, loss: 1.9129
2022-06-29 19:42:21 - train: epoch 0023, iter [03400, 05004], lr: 0.091443, loss: 2.1161
2022-06-29 19:42:55 - train: epoch 0023, iter [03500, 05004], lr: 0.091428, loss: 1.8012
2022-06-29 19:43:28 - train: epoch 0023, iter [03600, 05004], lr: 0.091413, loss: 1.7487
2022-06-29 19:44:01 - train: epoch 0023, iter [03700, 05004], lr: 0.091399, loss: 1.8885
2022-06-29 19:44:35 - train: epoch 0023, iter [03800, 05004], lr: 0.091384, loss: 1.9426
2022-06-29 19:45:08 - train: epoch 0023, iter [03900, 05004], lr: 0.091369, loss: 1.9031
2022-06-29 19:45:41 - train: epoch 0023, iter [04000, 05004], lr: 0.091354, loss: 1.9294
2022-06-29 19:46:14 - train: epoch 0023, iter [04100, 05004], lr: 0.091340, loss: 1.8342
2022-06-29 19:46:48 - train: epoch 0023, iter [04200, 05004], lr: 0.091325, loss: 1.8533
2022-06-29 19:47:21 - train: epoch 0023, iter [04300, 05004], lr: 0.091310, loss: 1.7693
2022-06-29 19:47:54 - train: epoch 0023, iter [04400, 05004], lr: 0.091296, loss: 1.8112
2022-06-29 19:48:27 - train: epoch 0023, iter [04500, 05004], lr: 0.091281, loss: 1.8659
2022-06-29 19:49:01 - train: epoch 0023, iter [04600, 05004], lr: 0.091266, loss: 2.0912
2022-06-29 19:49:34 - train: epoch 0023, iter [04700, 05004], lr: 0.091251, loss: 1.7611
2022-06-29 19:50:07 - train: epoch 0023, iter [04800, 05004], lr: 0.091237, loss: 1.9664
2022-06-29 19:50:41 - train: epoch 0023, iter [04900, 05004], lr: 0.091222, loss: 1.8952
2022-06-29 19:51:14 - train: epoch 0023, iter [05000, 05004], lr: 0.091207, loss: 2.0141
2022-06-29 19:51:16 - train: epoch 023, train_loss: 1.9394
2022-06-29 19:52:33 - eval: epoch: 023, acc1: 59.078%, acc5: 82.962%, test_loss: 1.7260, per_image_load_time: 2.367ms, per_image_inference_time: 0.610ms
2022-06-29 19:52:33 - until epoch: 023, best_acc1: 60.698%
2022-06-29 19:52:33 - epoch 024 lr: 0.091206
2022-06-29 19:53:13 - train: epoch 0024, iter [00100, 05004], lr: 0.091191, loss: 1.9096
2022-06-29 19:53:46 - train: epoch 0024, iter [00200, 05004], lr: 0.091177, loss: 1.9778
2022-06-29 19:54:19 - train: epoch 0024, iter [00300, 05004], lr: 0.091162, loss: 1.8230
2022-06-29 19:54:52 - train: epoch 0024, iter [00400, 05004], lr: 0.091147, loss: 1.9961
2022-06-29 19:55:26 - train: epoch 0024, iter [00500, 05004], lr: 0.091132, loss: 1.8750
2022-06-29 19:55:59 - train: epoch 0024, iter [00600, 05004], lr: 0.091117, loss: 1.8225
2022-06-29 19:56:33 - train: epoch 0024, iter [00700, 05004], lr: 0.091102, loss: 1.8025
2022-06-29 19:57:06 - train: epoch 0024, iter [00800, 05004], lr: 0.091087, loss: 1.8332
2022-06-29 19:57:40 - train: epoch 0024, iter [00900, 05004], lr: 0.091073, loss: 1.9309
2022-06-29 19:58:13 - train: epoch 0024, iter [01000, 05004], lr: 0.091058, loss: 1.9095
2022-06-29 19:58:47 - train: epoch 0024, iter [01100, 05004], lr: 0.091043, loss: 1.6039
2022-06-29 19:59:20 - train: epoch 0024, iter [01200, 05004], lr: 0.091028, loss: 1.7867
2022-06-29 19:59:54 - train: epoch 0024, iter [01300, 05004], lr: 0.091013, loss: 2.1769
2022-06-29 20:00:27 - train: epoch 0024, iter [01400, 05004], lr: 0.090998, loss: 1.8008
2022-06-29 20:01:01 - train: epoch 0024, iter [01500, 05004], lr: 0.090983, loss: 2.1332
2022-06-29 20:01:34 - train: epoch 0024, iter [01600, 05004], lr: 0.090968, loss: 1.9854
2022-06-29 20:02:08 - train: epoch 0024, iter [01700, 05004], lr: 0.090953, loss: 1.9528
2022-06-29 20:02:42 - train: epoch 0024, iter [01800, 05004], lr: 0.090938, loss: 2.1667
2022-06-29 20:03:15 - train: epoch 0024, iter [01900, 05004], lr: 0.090923, loss: 1.7142
2022-06-29 20:03:49 - train: epoch 0024, iter [02000, 05004], lr: 0.090908, loss: 1.9366
2022-06-29 20:04:23 - train: epoch 0024, iter [02100, 05004], lr: 0.090893, loss: 1.9236
2022-06-29 20:04:57 - train: epoch 0024, iter [02200, 05004], lr: 0.090878, loss: 1.8079
2022-06-29 20:05:31 - train: epoch 0024, iter [02300, 05004], lr: 0.090863, loss: 1.9450
2022-06-29 20:06:05 - train: epoch 0024, iter [02400, 05004], lr: 0.090847, loss: 1.9233
2022-06-29 20:06:38 - train: epoch 0024, iter [02500, 05004], lr: 0.090832, loss: 1.8962
2022-06-29 20:07:12 - train: epoch 0024, iter [02600, 05004], lr: 0.090817, loss: 1.8782
2022-06-29 20:07:46 - train: epoch 0024, iter [02700, 05004], lr: 0.090802, loss: 2.1414
2022-06-29 20:08:20 - train: epoch 0024, iter [02800, 05004], lr: 0.090787, loss: 2.0779
2022-06-29 20:08:54 - train: epoch 0024, iter [02900, 05004], lr: 0.090772, loss: 1.8964
2022-06-29 20:09:27 - train: epoch 0024, iter [03000, 05004], lr: 0.090757, loss: 1.8011
2022-06-29 20:10:01 - train: epoch 0024, iter [03100, 05004], lr: 0.090742, loss: 1.8210
2022-06-29 20:10:35 - train: epoch 0024, iter [03200, 05004], lr: 0.090726, loss: 2.0812
2022-06-29 20:11:09 - train: epoch 0024, iter [03300, 05004], lr: 0.090711, loss: 1.5442
2022-06-29 20:11:43 - train: epoch 0024, iter [03400, 05004], lr: 0.090696, loss: 1.9132
2022-06-29 20:12:17 - train: epoch 0024, iter [03500, 05004], lr: 0.090681, loss: 1.8299
2022-06-29 20:12:51 - train: epoch 0024, iter [03600, 05004], lr: 0.090666, loss: 2.1815
2022-06-29 20:13:24 - train: epoch 0024, iter [03700, 05004], lr: 0.090650, loss: 1.8922
2022-06-29 20:13:58 - train: epoch 0024, iter [03800, 05004], lr: 0.090635, loss: 2.1738
2022-06-29 20:14:32 - train: epoch 0024, iter [03900, 05004], lr: 0.090620, loss: 1.8218
2022-06-29 20:15:06 - train: epoch 0024, iter [04000, 05004], lr: 0.090605, loss: 1.9643
2022-06-29 20:15:40 - train: epoch 0024, iter [04100, 05004], lr: 0.090589, loss: 1.9218
2022-06-29 20:16:13 - train: epoch 0024, iter [04200, 05004], lr: 0.090574, loss: 1.8830
2022-06-29 20:16:47 - train: epoch 0024, iter [04300, 05004], lr: 0.090559, loss: 1.9422
2022-06-29 20:17:20 - train: epoch 0024, iter [04400, 05004], lr: 0.090544, loss: 1.8890
2022-06-29 20:17:54 - train: epoch 0024, iter [04500, 05004], lr: 0.090528, loss: 1.7830
2022-06-29 20:18:28 - train: epoch 0024, iter [04600, 05004], lr: 0.090513, loss: 1.9950
2022-06-29 20:19:01 - train: epoch 0024, iter [04700, 05004], lr: 0.090498, loss: 1.8353
2022-06-29 20:19:35 - train: epoch 0024, iter [04800, 05004], lr: 0.090482, loss: 1.9587
2022-06-29 20:20:09 - train: epoch 0024, iter [04900, 05004], lr: 0.090467, loss: 1.9131
2022-06-29 20:20:42 - train: epoch 0024, iter [05000, 05004], lr: 0.090451, loss: 1.9237
2022-06-29 20:20:44 - train: epoch 024, train_loss: 1.9282
2022-06-29 20:22:02 - eval: epoch: 024, acc1: 60.504%, acc5: 83.882%, test_loss: 1.6503, per_image_load_time: 2.372ms, per_image_inference_time: 0.639ms
2022-06-29 20:22:03 - until epoch: 024, best_acc1: 60.698%
2022-06-29 20:22:03 - epoch 025 lr: 0.090451
2022-06-29 20:22:43 - train: epoch 0025, iter [00100, 05004], lr: 0.090435, loss: 1.8172
2022-06-29 20:23:16 - train: epoch 0025, iter [00200, 05004], lr: 0.090420, loss: 1.7415
2022-06-29 20:23:49 - train: epoch 0025, iter [00300, 05004], lr: 0.090405, loss: 1.8183
2022-06-29 20:24:23 - train: epoch 0025, iter [00400, 05004], lr: 0.090389, loss: 1.7238
2022-06-29 20:24:56 - train: epoch 0025, iter [00500, 05004], lr: 0.090374, loss: 1.7301
2022-06-29 20:25:29 - train: epoch 0025, iter [00600, 05004], lr: 0.090358, loss: 2.0195
2022-06-29 20:26:02 - train: epoch 0025, iter [00700, 05004], lr: 0.090343, loss: 2.0032
2022-06-29 20:26:36 - train: epoch 0025, iter [00800, 05004], lr: 0.090327, loss: 1.8479
2022-06-29 20:27:09 - train: epoch 0025, iter [00900, 05004], lr: 0.090312, loss: 1.6302
2022-06-29 20:27:43 - train: epoch 0025, iter [01000, 05004], lr: 0.090297, loss: 1.9289
2022-06-29 20:28:16 - train: epoch 0025, iter [01100, 05004], lr: 0.090281, loss: 1.9225
2022-06-29 20:28:50 - train: epoch 0025, iter [01200, 05004], lr: 0.090266, loss: 2.0169
2022-06-29 20:29:23 - train: epoch 0025, iter [01300, 05004], lr: 0.090250, loss: 1.8896
2022-06-29 20:29:56 - train: epoch 0025, iter [01400, 05004], lr: 0.090235, loss: 1.9979
2022-06-29 20:30:30 - train: epoch 0025, iter [01500, 05004], lr: 0.090219, loss: 1.9527
2022-06-29 20:31:03 - train: epoch 0025, iter [01600, 05004], lr: 0.090203, loss: 1.6399
2022-06-29 20:31:37 - train: epoch 0025, iter [01700, 05004], lr: 0.090188, loss: 1.7904
2022-06-29 20:32:11 - train: epoch 0025, iter [01800, 05004], lr: 0.090172, loss: 1.7192
2022-06-29 20:32:44 - train: epoch 0025, iter [01900, 05004], lr: 0.090157, loss: 1.8463
2022-06-29 20:33:19 - train: epoch 0025, iter [02000, 05004], lr: 0.090141, loss: 1.8688
2022-06-29 20:33:53 - train: epoch 0025, iter [02100, 05004], lr: 0.090126, loss: 1.7837
2022-06-29 20:34:27 - train: epoch 0025, iter [02200, 05004], lr: 0.090110, loss: 1.8174
2022-06-29 20:35:00 - train: epoch 0025, iter [02300, 05004], lr: 0.090094, loss: 1.9664
2022-06-29 20:35:34 - train: epoch 0025, iter [02400, 05004], lr: 0.090079, loss: 1.7055
2022-06-29 20:36:08 - train: epoch 0025, iter [02500, 05004], lr: 0.090063, loss: 1.9349
2022-06-29 20:36:42 - train: epoch 0025, iter [02600, 05004], lr: 0.090047, loss: 1.9935
2022-06-29 20:37:15 - train: epoch 0025, iter [02700, 05004], lr: 0.090032, loss: 1.9864
2022-06-29 20:37:49 - train: epoch 0025, iter [02800, 05004], lr: 0.090016, loss: 1.9021
2022-06-29 20:38:22 - train: epoch 0025, iter [02900, 05004], lr: 0.090000, loss: 2.1181
2022-06-29 20:38:56 - train: epoch 0025, iter [03000, 05004], lr: 0.089985, loss: 2.1498
2022-06-29 20:39:29 - train: epoch 0025, iter [03100, 05004], lr: 0.089969, loss: 1.8754
2022-06-29 20:40:03 - train: epoch 0025, iter [03200, 05004], lr: 0.089953, loss: 2.2470
2022-06-29 20:40:36 - train: epoch 0025, iter [03300, 05004], lr: 0.089937, loss: 1.8152
2022-06-29 20:41:10 - train: epoch 0025, iter [03400, 05004], lr: 0.089922, loss: 2.1044
2022-06-29 20:41:43 - train: epoch 0025, iter [03500, 05004], lr: 0.089906, loss: 1.6991
2022-06-29 20:42:17 - train: epoch 0025, iter [03600, 05004], lr: 0.089890, loss: 2.0206
2022-06-29 20:42:50 - train: epoch 0025, iter [03700, 05004], lr: 0.089874, loss: 1.8626
2022-06-29 20:43:23 - train: epoch 0025, iter [03800, 05004], lr: 0.089859, loss: 1.9438
2022-06-29 20:43:57 - train: epoch 0025, iter [03900, 05004], lr: 0.089843, loss: 2.0835
2022-06-29 20:44:30 - train: epoch 0025, iter [04000, 05004], lr: 0.089827, loss: 1.9535
2022-06-29 20:45:04 - train: epoch 0025, iter [04100, 05004], lr: 0.089811, loss: 2.1788
2022-06-29 20:45:37 - train: epoch 0025, iter [04200, 05004], lr: 0.089795, loss: 1.9659
2022-06-29 20:46:10 - train: epoch 0025, iter [04300, 05004], lr: 0.089780, loss: 1.7886
2022-06-29 20:46:44 - train: epoch 0025, iter [04400, 05004], lr: 0.089764, loss: 1.7733
2022-06-29 20:47:18 - train: epoch 0025, iter [04500, 05004], lr: 0.089748, loss: 1.9128
2022-06-29 20:47:51 - train: epoch 0025, iter [04600, 05004], lr: 0.089732, loss: 1.8971
2022-06-29 20:48:25 - train: epoch 0025, iter [04700, 05004], lr: 0.089716, loss: 1.7895
2022-06-29 20:48:59 - train: epoch 0025, iter [04800, 05004], lr: 0.089700, loss: 1.7617
2022-06-29 20:49:32 - train: epoch 0025, iter [04900, 05004], lr: 0.089684, loss: 1.9167
2022-06-29 20:50:05 - train: epoch 0025, iter [05000, 05004], lr: 0.089668, loss: 2.0514
2022-06-29 20:50:07 - train: epoch 025, train_loss: 1.9191
2022-06-29 20:51:24 - eval: epoch: 025, acc1: 60.690%, acc5: 84.060%, test_loss: 1.6398, per_image_load_time: 2.312ms, per_image_inference_time: 0.620ms
2022-06-29 20:51:25 - until epoch: 025, best_acc1: 60.698%
2022-06-29 20:51:25 - epoch 026 lr: 0.089668
2022-06-29 20:52:06 - train: epoch 0026, iter [00100, 05004], lr: 0.089652, loss: 1.6824
2022-06-29 20:52:39 - train: epoch 0026, iter [00200, 05004], lr: 0.089636, loss: 1.7376
2022-06-29 20:53:12 - train: epoch 0026, iter [00300, 05004], lr: 0.089620, loss: 1.8129
2022-06-29 20:53:45 - train: epoch 0026, iter [00400, 05004], lr: 0.089604, loss: 1.8878
2022-06-29 20:54:19 - train: epoch 0026, iter [00500, 05004], lr: 0.089588, loss: 1.9308
2022-06-29 20:54:53 - train: epoch 0026, iter [00600, 05004], lr: 0.089572, loss: 2.0723
2022-06-29 20:55:26 - train: epoch 0026, iter [00700, 05004], lr: 0.089556, loss: 1.8245
2022-06-29 20:56:00 - train: epoch 0026, iter [00800, 05004], lr: 0.089540, loss: 1.7184
2022-06-29 20:56:33 - train: epoch 0026, iter [00900, 05004], lr: 0.089524, loss: 2.1353
2022-06-29 20:57:07 - train: epoch 0026, iter [01000, 05004], lr: 0.089508, loss: 1.7412
2022-06-29 20:57:41 - train: epoch 0026, iter [01100, 05004], lr: 0.089492, loss: 1.8463
2022-06-29 20:58:14 - train: epoch 0026, iter [01200, 05004], lr: 0.089476, loss: 1.8946
2022-06-29 20:58:48 - train: epoch 0026, iter [01300, 05004], lr: 0.089460, loss: 1.8377
2022-06-29 20:59:22 - train: epoch 0026, iter [01400, 05004], lr: 0.089444, loss: 1.9344
2022-06-29 20:59:55 - train: epoch 0026, iter [01500, 05004], lr: 0.089428, loss: 1.8429
2022-06-29 21:00:29 - train: epoch 0026, iter [01600, 05004], lr: 0.089411, loss: 2.1172
2022-06-29 21:01:03 - train: epoch 0026, iter [01700, 05004], lr: 0.089395, loss: 1.8785
2022-06-29 21:01:36 - train: epoch 0026, iter [01800, 05004], lr: 0.089379, loss: 1.9564
2022-06-29 21:02:09 - train: epoch 0026, iter [01900, 05004], lr: 0.089363, loss: 2.1549
2022-06-29 21:02:42 - train: epoch 0026, iter [02000, 05004], lr: 0.089347, loss: 1.9263
2022-06-29 21:03:16 - train: epoch 0026, iter [02100, 05004], lr: 0.089331, loss: 1.9501
2022-06-29 21:03:49 - train: epoch 0026, iter [02200, 05004], lr: 0.089315, loss: 1.9035
2022-06-29 21:04:23 - train: epoch 0026, iter [02300, 05004], lr: 0.089299, loss: 1.8359
2022-06-29 21:04:56 - train: epoch 0026, iter [02400, 05004], lr: 0.089282, loss: 2.2091
2022-06-29 21:05:29 - train: epoch 0026, iter [02500, 05004], lr: 0.089266, loss: 2.0049
2022-06-29 21:06:02 - train: epoch 0026, iter [02600, 05004], lr: 0.089250, loss: 1.9080
2022-06-29 21:06:35 - train: epoch 0026, iter [02700, 05004], lr: 0.089234, loss: 1.7273
2022-06-29 21:07:08 - train: epoch 0026, iter [02800, 05004], lr: 0.089218, loss: 1.7444
2022-06-29 21:07:41 - train: epoch 0026, iter [02900, 05004], lr: 0.089201, loss: 1.9915
2022-06-29 21:08:15 - train: epoch 0026, iter [03000, 05004], lr: 0.089185, loss: 1.8169
2022-06-29 21:08:48 - train: epoch 0026, iter [03100, 05004], lr: 0.089169, loss: 1.8988
2022-06-29 21:09:21 - train: epoch 0026, iter [03200, 05004], lr: 0.089153, loss: 1.9275
2022-06-29 21:09:55 - train: epoch 0026, iter [03300, 05004], lr: 0.089136, loss: 1.9349
2022-06-29 21:10:29 - train: epoch 0026, iter [03400, 05004], lr: 0.089120, loss: 2.0331
2022-06-29 21:11:02 - train: epoch 0026, iter [03500, 05004], lr: 0.089104, loss: 2.0347
2022-06-29 21:11:35 - train: epoch 0026, iter [03600, 05004], lr: 0.089087, loss: 1.7474
2022-06-29 21:12:09 - train: epoch 0026, iter [03700, 05004], lr: 0.089071, loss: 2.0174
2022-06-29 21:12:42 - train: epoch 0026, iter [03800, 05004], lr: 0.089055, loss: 2.0246
2022-06-29 21:13:15 - train: epoch 0026, iter [03900, 05004], lr: 0.089038, loss: 2.0856
2022-06-29 21:13:49 - train: epoch 0026, iter [04000, 05004], lr: 0.089022, loss: 2.1098
2022-06-29 21:14:22 - train: epoch 0026, iter [04100, 05004], lr: 0.089006, loss: 1.9679
2022-06-29 21:14:55 - train: epoch 0026, iter [04200, 05004], lr: 0.088989, loss: 2.0190
2022-06-29 21:15:29 - train: epoch 0026, iter [04300, 05004], lr: 0.088973, loss: 2.0327
2022-06-29 21:16:03 - train: epoch 0026, iter [04400, 05004], lr: 0.088957, loss: 2.0806
2022-06-29 21:16:36 - train: epoch 0026, iter [04500, 05004], lr: 0.088940, loss: 2.2218
2022-06-29 21:17:10 - train: epoch 0026, iter [04600, 05004], lr: 0.088924, loss: 1.8695
2022-06-29 21:17:44 - train: epoch 0026, iter [04700, 05004], lr: 0.088907, loss: 1.8041
2022-06-29 21:18:17 - train: epoch 0026, iter [04800, 05004], lr: 0.088891, loss: 1.8295
2022-06-29 21:18:51 - train: epoch 0026, iter [04900, 05004], lr: 0.088874, loss: 1.9521
2022-06-29 21:19:24 - train: epoch 0026, iter [05000, 05004], lr: 0.088858, loss: 2.0880
2022-06-29 21:19:26 - train: epoch 026, train_loss: 1.9098
2022-06-29 21:20:42 - eval: epoch: 026, acc1: 60.830%, acc5: 84.096%, test_loss: 1.6338, per_image_load_time: 2.371ms, per_image_inference_time: 0.622ms
2022-06-29 21:20:43 - until epoch: 026, best_acc1: 60.830%
2022-06-29 21:20:43 - epoch 027 lr: 0.088857
2022-06-29 21:21:23 - train: epoch 0027, iter [00100, 05004], lr: 0.088841, loss: 1.9836
2022-06-29 21:21:56 - train: epoch 0027, iter [00200, 05004], lr: 0.088824, loss: 1.9143
2022-06-29 21:22:29 - train: epoch 0027, iter [00300, 05004], lr: 0.088808, loss: 1.9008
2022-06-29 21:23:03 - train: epoch 0027, iter [00400, 05004], lr: 0.088791, loss: 2.0319
2022-06-29 21:23:37 - train: epoch 0027, iter [00500, 05004], lr: 0.088775, loss: 2.0049
2022-06-29 21:24:10 - train: epoch 0027, iter [00600, 05004], lr: 0.088758, loss: 2.1906
2022-06-29 21:24:44 - train: epoch 0027, iter [00700, 05004], lr: 0.088742, loss: 2.2465
2022-06-29 21:25:18 - train: epoch 0027, iter [00800, 05004], lr: 0.088725, loss: 1.9351
2022-06-29 21:25:51 - train: epoch 0027, iter [00900, 05004], lr: 0.088709, loss: 1.7908
2022-06-29 21:26:25 - train: epoch 0027, iter [01000, 05004], lr: 0.088692, loss: 1.8408
2022-06-29 21:26:58 - train: epoch 0027, iter [01100, 05004], lr: 0.088676, loss: 1.7629
2022-06-29 21:27:31 - train: epoch 0027, iter [01200, 05004], lr: 0.088659, loss: 2.0737
2022-06-29 21:28:05 - train: epoch 0027, iter [01300, 05004], lr: 0.088642, loss: 1.8519
2022-06-29 21:28:38 - train: epoch 0027, iter [01400, 05004], lr: 0.088626, loss: 2.0604
2022-06-29 21:29:11 - train: epoch 0027, iter [01500, 05004], lr: 0.088609, loss: 1.9367
2022-06-29 21:29:44 - train: epoch 0027, iter [01600, 05004], lr: 0.088593, loss: 2.0092
2022-06-29 21:30:16 - train: epoch 0027, iter [01700, 05004], lr: 0.088576, loss: 1.9829
2022-06-29 21:30:49 - train: epoch 0027, iter [01800, 05004], lr: 0.088559, loss: 1.9650
2022-06-29 21:31:22 - train: epoch 0027, iter [01900, 05004], lr: 0.088543, loss: 2.0116
2022-06-29 21:31:55 - train: epoch 0027, iter [02000, 05004], lr: 0.088526, loss: 1.8527
2022-06-29 21:32:28 - train: epoch 0027, iter [02100, 05004], lr: 0.088509, loss: 2.2577
2022-06-29 21:33:01 - train: epoch 0027, iter [02200, 05004], lr: 0.088493, loss: 2.0978
2022-06-29 21:33:35 - train: epoch 0027, iter [02300, 05004], lr: 0.088476, loss: 2.1971
2022-06-29 21:34:08 - train: epoch 0027, iter [02400, 05004], lr: 0.088459, loss: 1.8644
2022-06-29 21:34:41 - train: epoch 0027, iter [02500, 05004], lr: 0.088442, loss: 1.6981
2022-06-29 21:35:14 - train: epoch 0027, iter [02600, 05004], lr: 0.088426, loss: 2.1232
2022-06-29 21:35:48 - train: epoch 0027, iter [02700, 05004], lr: 0.088409, loss: 1.8035
2022-06-29 21:36:20 - train: epoch 0027, iter [02800, 05004], lr: 0.088392, loss: 2.0598
2022-06-29 21:36:53 - train: epoch 0027, iter [02900, 05004], lr: 0.088375, loss: 1.8550
2022-06-29 21:37:27 - train: epoch 0027, iter [03000, 05004], lr: 0.088359, loss: 1.7499
2022-06-29 21:38:00 - train: epoch 0027, iter [03100, 05004], lr: 0.088342, loss: 1.7657
2022-06-29 21:38:33 - train: epoch 0027, iter [03200, 05004], lr: 0.088325, loss: 1.7462
2022-06-29 21:39:05 - train: epoch 0027, iter [03300, 05004], lr: 0.088308, loss: 1.9545
2022-06-29 21:39:39 - train: epoch 0027, iter [03400, 05004], lr: 0.088291, loss: 1.8685
2022-06-29 21:40:12 - train: epoch 0027, iter [03500, 05004], lr: 0.088275, loss: 2.1397
2022-06-29 21:40:45 - train: epoch 0027, iter [03600, 05004], lr: 0.088258, loss: 1.7045
2022-06-29 21:41:18 - train: epoch 0027, iter [03700, 05004], lr: 0.088241, loss: 1.7687
2022-06-29 21:41:51 - train: epoch 0027, iter [03800, 05004], lr: 0.088224, loss: 1.6117
2022-06-29 21:42:24 - train: epoch 0027, iter [03900, 05004], lr: 0.088207, loss: 1.7847
2022-06-29 21:42:57 - train: epoch 0027, iter [04000, 05004], lr: 0.088190, loss: 1.8910
2022-06-29 21:43:30 - train: epoch 0027, iter [04100, 05004], lr: 0.088173, loss: 1.8715
2022-06-29 21:44:03 - train: epoch 0027, iter [04200, 05004], lr: 0.088157, loss: 1.9480
2022-06-29 21:44:36 - train: epoch 0027, iter [04300, 05004], lr: 0.088140, loss: 1.8335
2022-06-29 21:45:09 - train: epoch 0027, iter [04400, 05004], lr: 0.088123, loss: 1.8892
2022-06-29 21:45:42 - train: epoch 0027, iter [04500, 05004], lr: 0.088106, loss: 1.8209
2022-06-29 21:46:15 - train: epoch 0027, iter [04600, 05004], lr: 0.088089, loss: 1.8814
2022-06-29 21:46:48 - train: epoch 0027, iter [04700, 05004], lr: 0.088072, loss: 1.9933
2022-06-29 21:47:21 - train: epoch 0027, iter [04800, 05004], lr: 0.088055, loss: 1.9797
2022-06-29 21:47:54 - train: epoch 0027, iter [04900, 05004], lr: 0.088038, loss: 1.9437
2022-06-29 21:48:27 - train: epoch 0027, iter [05000, 05004], lr: 0.088021, loss: 1.5753
2022-06-29 21:48:28 - train: epoch 027, train_loss: 1.8998
2022-06-29 21:49:44 - eval: epoch: 027, acc1: 61.956%, acc5: 84.816%, test_loss: 1.5843, per_image_load_time: 2.361ms, per_image_inference_time: 0.596ms
2022-06-29 21:49:45 - until epoch: 027, best_acc1: 61.956%
2022-06-29 21:49:45 - epoch 028 lr: 0.088020
2022-06-29 21:50:24 - train: epoch 0028, iter [00100, 05004], lr: 0.088003, loss: 1.5689
2022-06-29 21:50:57 - train: epoch 0028, iter [00200, 05004], lr: 0.087986, loss: 1.7094
2022-06-29 21:51:29 - train: epoch 0028, iter [00300, 05004], lr: 0.087969, loss: 1.8111
2022-06-29 21:52:02 - train: epoch 0028, iter [00400, 05004], lr: 0.087952, loss: 1.8323
2022-06-29 21:52:35 - train: epoch 0028, iter [00500, 05004], lr: 0.087935, loss: 1.7923
2022-06-29 21:53:08 - train: epoch 0028, iter [00600, 05004], lr: 0.087918, loss: 2.1097
2022-06-29 21:53:41 - train: epoch 0028, iter [00700, 05004], lr: 0.087901, loss: 2.0875
2022-06-29 21:54:14 - train: epoch 0028, iter [00800, 05004], lr: 0.087884, loss: 1.5796
2022-06-29 21:54:47 - train: epoch 0028, iter [00900, 05004], lr: 0.087867, loss: 1.7493
2022-06-29 21:55:20 - train: epoch 0028, iter [01000, 05004], lr: 0.087850, loss: 1.9696
2022-06-29 21:55:53 - train: epoch 0028, iter [01100, 05004], lr: 0.087833, loss: 1.9663
2022-06-29 21:56:26 - train: epoch 0028, iter [01200, 05004], lr: 0.087816, loss: 1.7814
2022-06-29 21:56:59 - train: epoch 0028, iter [01300, 05004], lr: 0.087799, loss: 1.8153
2022-06-29 21:57:32 - train: epoch 0028, iter [01400, 05004], lr: 0.087781, loss: 2.1566
2022-06-29 21:58:05 - train: epoch 0028, iter [01500, 05004], lr: 0.087764, loss: 1.8279
2022-06-29 21:58:38 - train: epoch 0028, iter [01600, 05004], lr: 0.087747, loss: 1.8740
2022-06-29 21:59:11 - train: epoch 0028, iter [01700, 05004], lr: 0.087730, loss: 1.8054
2022-06-29 21:59:44 - train: epoch 0028, iter [01800, 05004], lr: 0.087713, loss: 1.8551
2022-06-29 22:00:17 - train: epoch 0028, iter [01900, 05004], lr: 0.087696, loss: 1.8117
2022-06-29 22:00:50 - train: epoch 0028, iter [02000, 05004], lr: 0.087678, loss: 2.0775
2022-06-29 22:01:23 - train: epoch 0028, iter [02100, 05004], lr: 0.087661, loss: 1.9060
2022-06-29 22:01:56 - train: epoch 0028, iter [02200, 05004], lr: 0.087644, loss: 1.9680
2022-06-29 22:02:29 - train: epoch 0028, iter [02300, 05004], lr: 0.087627, loss: 2.0396
2022-06-29 22:03:03 - train: epoch 0028, iter [02400, 05004], lr: 0.087610, loss: 2.1263
2022-06-29 22:03:36 - train: epoch 0028, iter [02500, 05004], lr: 0.087592, loss: 1.8452
2022-06-29 22:04:09 - train: epoch 0028, iter [02600, 05004], lr: 0.087575, loss: 1.7144
2022-06-29 22:04:43 - train: epoch 0028, iter [02700, 05004], lr: 0.087558, loss: 1.8530
2022-06-29 22:05:16 - train: epoch 0028, iter [02800, 05004], lr: 0.087541, loss: 1.8601
2022-06-29 22:05:49 - train: epoch 0028, iter [02900, 05004], lr: 0.087523, loss: 1.9235
2022-06-29 22:06:22 - train: epoch 0028, iter [03000, 05004], lr: 0.087506, loss: 1.9492
2022-06-29 22:06:56 - train: epoch 0028, iter [03100, 05004], lr: 0.087489, loss: 2.1267
2022-06-29 22:07:29 - train: epoch 0028, iter [03200, 05004], lr: 0.087471, loss: 1.8206
2022-06-29 22:08:03 - train: epoch 0028, iter [03300, 05004], lr: 0.087454, loss: 1.9605
2022-06-29 22:08:36 - train: epoch 0028, iter [03400, 05004], lr: 0.087437, loss: 1.9245
2022-06-29 22:09:09 - train: epoch 0028, iter [03500, 05004], lr: 0.087419, loss: 1.8974
2022-06-29 22:09:42 - train: epoch 0028, iter [03600, 05004], lr: 0.087402, loss: 1.6830
2022-06-29 22:10:15 - train: epoch 0028, iter [03700, 05004], lr: 0.087385, loss: 1.7924
2022-06-29 22:10:48 - train: epoch 0028, iter [03800, 05004], lr: 0.087367, loss: 1.5153
2022-06-29 22:11:21 - train: epoch 0028, iter [03900, 05004], lr: 0.087350, loss: 1.8666
2022-06-29 22:11:54 - train: epoch 0028, iter [04000, 05004], lr: 0.087332, loss: 2.0591
2022-06-29 22:12:27 - train: epoch 0028, iter [04100, 05004], lr: 0.087315, loss: 2.0138
2022-06-29 22:13:00 - train: epoch 0028, iter [04200, 05004], lr: 0.087298, loss: 2.0691
2022-06-29 22:13:33 - train: epoch 0028, iter [04300, 05004], lr: 0.087280, loss: 1.9006
2022-06-29 22:14:06 - train: epoch 0028, iter [04400, 05004], lr: 0.087263, loss: 1.8319
2022-06-29 22:14:40 - train: epoch 0028, iter [04500, 05004], lr: 0.087245, loss: 1.8171
2022-06-29 22:15:13 - train: epoch 0028, iter [04600, 05004], lr: 0.087228, loss: 2.0327
2022-06-29 22:15:46 - train: epoch 0028, iter [04700, 05004], lr: 0.087210, loss: 1.8782
2022-06-29 22:16:19 - train: epoch 0028, iter [04800, 05004], lr: 0.087193, loss: 1.8578
2022-06-29 22:16:52 - train: epoch 0028, iter [04900, 05004], lr: 0.087175, loss: 1.7821
2022-06-29 22:17:25 - train: epoch 0028, iter [05000, 05004], lr: 0.087158, loss: 1.8304
2022-06-29 22:17:26 - train: epoch 028, train_loss: 1.8871
2022-06-29 22:18:43 - eval: epoch: 028, acc1: 61.138%, acc5: 84.232%, test_loss: 1.6302, per_image_load_time: 2.373ms, per_image_inference_time: 0.602ms
2022-06-29 22:18:44 - until epoch: 028, best_acc1: 61.956%
2022-06-29 22:18:44 - epoch 029 lr: 0.087157
2022-06-29 22:19:23 - train: epoch 0029, iter [00100, 05004], lr: 0.087140, loss: 1.9833
2022-06-29 22:19:56 - train: epoch 0029, iter [00200, 05004], lr: 0.087122, loss: 1.8382
2022-06-29 22:20:29 - train: epoch 0029, iter [00300, 05004], lr: 0.087105, loss: 2.0301
2022-06-29 22:21:02 - train: epoch 0029, iter [00400, 05004], lr: 0.087087, loss: 1.7944
2022-06-29 22:21:34 - train: epoch 0029, iter [00500, 05004], lr: 0.087070, loss: 1.8875
2022-06-29 22:22:07 - train: epoch 0029, iter [00600, 05004], lr: 0.087052, loss: 2.0884
2022-06-29 22:22:40 - train: epoch 0029, iter [00700, 05004], lr: 0.087034, loss: 1.4637
2022-06-29 22:23:13 - train: epoch 0029, iter [00800, 05004], lr: 0.087017, loss: 2.0078
2022-06-29 22:23:46 - train: epoch 0029, iter [00900, 05004], lr: 0.086999, loss: 1.7827
2022-06-29 22:24:19 - train: epoch 0029, iter [01000, 05004], lr: 0.086982, loss: 1.7246
2022-06-29 22:24:53 - train: epoch 0029, iter [01100, 05004], lr: 0.086964, loss: 1.8211
2022-06-29 22:25:26 - train: epoch 0029, iter [01200, 05004], lr: 0.086946, loss: 1.9819
2022-06-29 22:25:59 - train: epoch 0029, iter [01300, 05004], lr: 0.086929, loss: 1.8910
2022-06-29 22:26:32 - train: epoch 0029, iter [01400, 05004], lr: 0.086911, loss: 2.1159
2022-06-29 22:27:05 - train: epoch 0029, iter [01500, 05004], lr: 0.086894, loss: 1.9260
2022-06-29 22:27:38 - train: epoch 0029, iter [01600, 05004], lr: 0.086876, loss: 1.7969
2022-06-29 22:28:11 - train: epoch 0029, iter [01700, 05004], lr: 0.086858, loss: 1.8779
2022-06-29 22:28:44 - train: epoch 0029, iter [01800, 05004], lr: 0.086841, loss: 2.0091
2022-06-29 22:29:17 - train: epoch 0029, iter [01900, 05004], lr: 0.086823, loss: 1.7371
2022-06-29 22:29:50 - train: epoch 0029, iter [02000, 05004], lr: 0.086805, loss: 2.0450
2022-06-29 22:30:24 - train: epoch 0029, iter [02100, 05004], lr: 0.086787, loss: 1.8693
2022-06-29 22:30:57 - train: epoch 0029, iter [02200, 05004], lr: 0.086770, loss: 1.9532
2022-06-29 22:31:30 - train: epoch 0029, iter [02300, 05004], lr: 0.086752, loss: 1.9358
2022-06-29 22:32:03 - train: epoch 0029, iter [02400, 05004], lr: 0.086734, loss: 1.7668
2022-06-29 22:32:36 - train: epoch 0029, iter [02500, 05004], lr: 0.086716, loss: 1.8271
2022-06-29 22:33:09 - train: epoch 0029, iter [02600, 05004], lr: 0.086699, loss: 1.9320
2022-06-29 22:33:42 - train: epoch 0029, iter [02700, 05004], lr: 0.086681, loss: 1.8570
2022-06-29 22:34:15 - train: epoch 0029, iter [02800, 05004], lr: 0.086663, loss: 1.8663
2022-06-29 22:34:48 - train: epoch 0029, iter [02900, 05004], lr: 0.086645, loss: 1.9332
2022-06-29 22:35:21 - train: epoch 0029, iter [03000, 05004], lr: 0.086628, loss: 1.8296
2022-06-29 22:35:54 - train: epoch 0029, iter [03100, 05004], lr: 0.086610, loss: 1.8740
2022-06-29 22:36:27 - train: epoch 0029, iter [03200, 05004], lr: 0.086592, loss: 1.9633
2022-06-29 22:37:00 - train: epoch 0029, iter [03300, 05004], lr: 0.086574, loss: 1.9111
2022-06-29 22:37:33 - train: epoch 0029, iter [03400, 05004], lr: 0.086556, loss: 1.6730
2022-06-29 22:38:07 - train: epoch 0029, iter [03500, 05004], lr: 0.086538, loss: 1.9583
2022-06-29 22:38:40 - train: epoch 0029, iter [03600, 05004], lr: 0.086521, loss: 1.8367
2022-06-29 22:39:13 - train: epoch 0029, iter [03700, 05004], lr: 0.086503, loss: 1.7271
2022-06-29 22:39:46 - train: epoch 0029, iter [03800, 05004], lr: 0.086485, loss: 1.8536
2022-06-29 22:40:19 - train: epoch 0029, iter [03900, 05004], lr: 0.086467, loss: 1.6831
2022-06-29 22:40:53 - train: epoch 0029, iter [04000, 05004], lr: 0.086449, loss: 1.9734
2022-06-29 22:41:26 - train: epoch 0029, iter [04100, 05004], lr: 0.086431, loss: 1.7071
2022-06-29 22:41:59 - train: epoch 0029, iter [04200, 05004], lr: 0.086413, loss: 1.6578
2022-06-29 22:42:32 - train: epoch 0029, iter [04300, 05004], lr: 0.086395, loss: 1.9754
2022-06-29 22:43:05 - train: epoch 0029, iter [04400, 05004], lr: 0.086377, loss: 1.8099
2022-06-29 22:43:38 - train: epoch 0029, iter [04500, 05004], lr: 0.086359, loss: 2.0867
2022-06-29 22:44:11 - train: epoch 0029, iter [04600, 05004], lr: 0.086341, loss: 1.9413
2022-06-29 22:44:45 - train: epoch 0029, iter [04700, 05004], lr: 0.086323, loss: 1.6766
2022-06-29 22:45:18 - train: epoch 0029, iter [04800, 05004], lr: 0.086305, loss: 1.9955
2022-06-29 22:45:51 - train: epoch 0029, iter [04900, 05004], lr: 0.086287, loss: 2.2106
2022-06-29 22:46:24 - train: epoch 0029, iter [05000, 05004], lr: 0.086269, loss: 1.6000
2022-06-29 22:46:26 - train: epoch 029, train_loss: 1.8800
2022-06-29 22:47:41 - eval: epoch: 029, acc1: 61.418%, acc5: 84.492%, test_loss: 1.6034, per_image_load_time: 2.323ms, per_image_inference_time: 0.599ms
2022-06-29 22:47:42 - until epoch: 029, best_acc1: 61.956%
2022-06-29 22:47:42 - epoch 030 lr: 0.086269
2022-06-29 22:48:21 - train: epoch 0030, iter [00100, 05004], lr: 0.086251, loss: 1.9670
2022-06-29 22:48:54 - train: epoch 0030, iter [00200, 05004], lr: 0.086233, loss: 1.8660
2022-06-29 22:49:26 - train: epoch 0030, iter [00300, 05004], lr: 0.086215, loss: 1.7305
2022-06-29 22:49:59 - train: epoch 0030, iter [00400, 05004], lr: 0.086197, loss: 1.5537
2022-06-29 22:50:32 - train: epoch 0030, iter [00500, 05004], lr: 0.086179, loss: 2.1330
2022-06-29 22:51:05 - train: epoch 0030, iter [00600, 05004], lr: 0.086160, loss: 1.6950
2022-06-29 22:51:38 - train: epoch 0030, iter [00700, 05004], lr: 0.086142, loss: 1.7725
2022-06-29 22:52:11 - train: epoch 0030, iter [00800, 05004], lr: 0.086124, loss: 2.0445
2022-06-29 22:52:44 - train: epoch 0030, iter [00900, 05004], lr: 0.086106, loss: 1.8966
2022-06-29 22:53:18 - train: epoch 0030, iter [01000, 05004], lr: 0.086088, loss: 1.5925
2022-06-29 22:53:51 - train: epoch 0030, iter [01100, 05004], lr: 0.086070, loss: 1.6970
2022-06-29 22:54:25 - train: epoch 0030, iter [01200, 05004], lr: 0.086052, loss: 1.8436
2022-06-29 22:54:58 - train: epoch 0030, iter [01300, 05004], lr: 0.086034, loss: 1.7600
2022-06-29 22:55:31 - train: epoch 0030, iter [01400, 05004], lr: 0.086016, loss: 1.8085
2022-06-29 22:56:04 - train: epoch 0030, iter [01500, 05004], lr: 0.085998, loss: 1.8704
2022-06-29 22:56:38 - train: epoch 0030, iter [01600, 05004], lr: 0.085979, loss: 1.8116
2022-06-29 22:57:11 - train: epoch 0030, iter [01700, 05004], lr: 0.085961, loss: 2.0977
2022-06-29 22:57:44 - train: epoch 0030, iter [01800, 05004], lr: 0.085943, loss: 1.8948
2022-06-29 22:58:17 - train: epoch 0030, iter [01900, 05004], lr: 0.085925, loss: 2.0337
2022-06-29 22:58:50 - train: epoch 0030, iter [02000, 05004], lr: 0.085907, loss: 1.9078
2022-06-29 22:59:24 - train: epoch 0030, iter [02100, 05004], lr: 0.085888, loss: 1.9574
2022-06-29 22:59:56 - train: epoch 0030, iter [02200, 05004], lr: 0.085870, loss: 1.8932
2022-06-29 23:00:29 - train: epoch 0030, iter [02300, 05004], lr: 0.085852, loss: 1.7723
2022-06-29 23:01:02 - train: epoch 0030, iter [02400, 05004], lr: 0.085834, loss: 2.1299
2022-06-29 23:01:36 - train: epoch 0030, iter [02500, 05004], lr: 0.085815, loss: 1.8584
2022-06-29 23:02:09 - train: epoch 0030, iter [02600, 05004], lr: 0.085797, loss: 1.9130
2022-06-29 23:02:42 - train: epoch 0030, iter [02700, 05004], lr: 0.085779, loss: 1.7674
2022-06-29 23:03:15 - train: epoch 0030, iter [02800, 05004], lr: 0.085761, loss: 1.8223
2022-06-29 23:03:48 - train: epoch 0030, iter [02900, 05004], lr: 0.085742, loss: 1.9306
2022-06-29 23:04:21 - train: epoch 0030, iter [03000, 05004], lr: 0.085724, loss: 2.0824
2022-06-29 23:04:54 - train: epoch 0030, iter [03100, 05004], lr: 0.085706, loss: 1.8339
2022-06-29 23:05:27 - train: epoch 0030, iter [03200, 05004], lr: 0.085687, loss: 1.7456
2022-06-29 23:06:00 - train: epoch 0030, iter [03300, 05004], lr: 0.085669, loss: 2.1064
2022-06-29 23:06:33 - train: epoch 0030, iter [03400, 05004], lr: 0.085651, loss: 1.9310
2022-06-29 23:07:06 - train: epoch 0030, iter [03500, 05004], lr: 0.085632, loss: 1.9632
2022-06-29 23:07:39 - train: epoch 0030, iter [03600, 05004], lr: 0.085614, loss: 1.6993
2022-06-29 23:08:12 - train: epoch 0030, iter [03700, 05004], lr: 0.085596, loss: 1.9602
2022-06-29 23:08:45 - train: epoch 0030, iter [03800, 05004], lr: 0.085577, loss: 2.1222
2022-06-29 23:09:18 - train: epoch 0030, iter [03900, 05004], lr: 0.085559, loss: 1.9366
2022-06-29 23:09:51 - train: epoch 0030, iter [04000, 05004], lr: 0.085541, loss: 1.7255
2022-06-29 23:10:24 - train: epoch 0030, iter [04100, 05004], lr: 0.085522, loss: 1.8130
2022-06-29 23:10:57 - train: epoch 0030, iter [04200, 05004], lr: 0.085504, loss: 2.0778
2022-06-29 23:11:30 - train: epoch 0030, iter [04300, 05004], lr: 0.085485, loss: 1.7866
2022-06-29 23:12:03 - train: epoch 0030, iter [04400, 05004], lr: 0.085467, loss: 1.8991
2022-06-29 23:12:36 - train: epoch 0030, iter [04500, 05004], lr: 0.085448, loss: 2.0931
2022-06-29 23:13:09 - train: epoch 0030, iter [04600, 05004], lr: 0.085430, loss: 1.6090
2022-06-29 23:13:41 - train: epoch 0030, iter [04700, 05004], lr: 0.085412, loss: 1.8380
2022-06-29 23:14:15 - train: epoch 0030, iter [04800, 05004], lr: 0.085393, loss: 1.8963
2022-06-29 23:14:48 - train: epoch 0030, iter [04900, 05004], lr: 0.085375, loss: 1.9947
2022-06-29 23:15:21 - train: epoch 0030, iter [05000, 05004], lr: 0.085356, loss: 1.9119
2022-06-29 23:15:22 - train: epoch 030, train_loss: 1.8733
2022-06-29 23:16:38 - eval: epoch: 030, acc1: 61.776%, acc5: 84.782%, test_loss: 1.5953, per_image_load_time: 2.330ms, per_image_inference_time: 0.595ms
2022-06-29 23:16:39 - until epoch: 030, best_acc1: 61.956%
2022-06-29 23:16:39 - epoch 031 lr: 0.085355
2022-06-29 23:17:19 - train: epoch 0031, iter [00100, 05004], lr: 0.085337, loss: 2.0084
2022-06-29 23:17:52 - train: epoch 0031, iter [00200, 05004], lr: 0.085318, loss: 1.6747
2022-06-29 23:18:25 - train: epoch 0031, iter [00300, 05004], lr: 0.085300, loss: 1.7636
2022-06-29 23:18:57 - train: epoch 0031, iter [00400, 05004], lr: 0.085281, loss: 1.9755
2022-06-29 23:19:30 - train: epoch 0031, iter [00500, 05004], lr: 0.085263, loss: 1.7551
2022-06-29 23:20:03 - train: epoch 0031, iter [00600, 05004], lr: 0.085244, loss: 1.8673
2022-06-29 23:20:37 - train: epoch 0031, iter [00700, 05004], lr: 0.085226, loss: 1.8579
2022-06-29 23:21:10 - train: epoch 0031, iter [00800, 05004], lr: 0.085207, loss: 1.7639
2022-06-29 23:21:43 - train: epoch 0031, iter [00900, 05004], lr: 0.085188, loss: 1.8134
2022-06-29 23:22:16 - train: epoch 0031, iter [01000, 05004], lr: 0.085170, loss: 2.1284
2022-06-29 23:22:49 - train: epoch 0031, iter [01100, 05004], lr: 0.085151, loss: 2.1435
2022-06-29 23:23:23 - train: epoch 0031, iter [01200, 05004], lr: 0.085133, loss: 1.9623
2022-06-29 23:23:56 - train: epoch 0031, iter [01300, 05004], lr: 0.085114, loss: 1.5732
2022-06-29 23:24:29 - train: epoch 0031, iter [01400, 05004], lr: 0.085095, loss: 1.9558
2022-06-29 23:25:02 - train: epoch 0031, iter [01500, 05004], lr: 0.085077, loss: 2.0367
2022-06-29 23:25:36 - train: epoch 0031, iter [01600, 05004], lr: 0.085058, loss: 1.7051
2022-06-29 23:26:09 - train: epoch 0031, iter [01700, 05004], lr: 0.085039, loss: 1.7395
2022-06-29 23:26:42 - train: epoch 0031, iter [01800, 05004], lr: 0.085021, loss: 1.7560
2022-06-29 23:27:15 - train: epoch 0031, iter [01900, 05004], lr: 0.085002, loss: 1.9129
2022-06-29 23:27:48 - train: epoch 0031, iter [02000, 05004], lr: 0.084983, loss: 1.8978
2022-06-29 23:28:21 - train: epoch 0031, iter [02100, 05004], lr: 0.084965, loss: 1.6262
2022-06-29 23:28:54 - train: epoch 0031, iter [02200, 05004], lr: 0.084946, loss: 1.7514
2022-06-29 23:29:27 - train: epoch 0031, iter [02300, 05004], lr: 0.084927, loss: 1.6387
2022-06-29 23:30:00 - train: epoch 0031, iter [02400, 05004], lr: 0.084909, loss: 1.8067
2022-06-29 23:30:33 - train: epoch 0031, iter [02500, 05004], lr: 0.084890, loss: 1.8374
2022-06-29 23:31:05 - train: epoch 0031, iter [02600, 05004], lr: 0.084871, loss: 1.7964
2022-06-29 23:31:38 - train: epoch 0031, iter [02700, 05004], lr: 0.084852, loss: 1.9103
2022-06-29 23:32:11 - train: epoch 0031, iter [02800, 05004], lr: 0.084834, loss: 2.1359
2022-06-29 23:32:44 - train: epoch 0031, iter [02900, 05004], lr: 0.084815, loss: 1.9788
2022-06-29 23:33:17 - train: epoch 0031, iter [03000, 05004], lr: 0.084796, loss: 1.9424
2022-06-29 23:33:50 - train: epoch 0031, iter [03100, 05004], lr: 0.084777, loss: 1.8766
2022-06-29 23:34:23 - train: epoch 0031, iter [03200, 05004], lr: 0.084759, loss: 1.9577
2022-06-29 23:34:56 - train: epoch 0031, iter [03300, 05004], lr: 0.084740, loss: 1.8832
2022-06-29 23:35:29 - train: epoch 0031, iter [03400, 05004], lr: 0.084721, loss: 1.9322
2022-06-29 23:36:02 - train: epoch 0031, iter [03500, 05004], lr: 0.084702, loss: 2.0157
2022-06-29 23:36:35 - train: epoch 0031, iter [03600, 05004], lr: 0.084683, loss: 1.8305
2022-06-29 23:37:08 - train: epoch 0031, iter [03700, 05004], lr: 0.084664, loss: 1.7809
2022-06-29 23:37:41 - train: epoch 0031, iter [03800, 05004], lr: 0.084646, loss: 1.7142
2022-06-29 23:38:14 - train: epoch 0031, iter [03900, 05004], lr: 0.084627, loss: 1.8136
2022-06-29 23:38:48 - train: epoch 0031, iter [04000, 05004], lr: 0.084608, loss: 1.8951
2022-06-29 23:39:21 - train: epoch 0031, iter [04100, 05004], lr: 0.084589, loss: 1.8014
2022-06-29 23:39:54 - train: epoch 0031, iter [04200, 05004], lr: 0.084570, loss: 1.9468
2022-06-29 23:40:27 - train: epoch 0031, iter [04300, 05004], lr: 0.084551, loss: 1.8333
2022-06-29 23:41:01 - train: epoch 0031, iter [04400, 05004], lr: 0.084532, loss: 2.0005
2022-06-29 23:41:34 - train: epoch 0031, iter [04500, 05004], lr: 0.084513, loss: 2.0376
2022-06-29 23:42:07 - train: epoch 0031, iter [04600, 05004], lr: 0.084494, loss: 2.0068
2022-06-29 23:42:40 - train: epoch 0031, iter [04700, 05004], lr: 0.084475, loss: 1.9184
2022-06-29 23:43:13 - train: epoch 0031, iter [04800, 05004], lr: 0.084456, loss: 1.8257
2022-06-29 23:43:46 - train: epoch 0031, iter [04900, 05004], lr: 0.084437, loss: 1.7465
2022-06-29 23:44:18 - train: epoch 0031, iter [05000, 05004], lr: 0.084418, loss: 1.8580
2022-06-29 23:44:20 - train: epoch 031, train_loss: 1.8625
2022-06-29 23:45:35 - eval: epoch: 031, acc1: 62.292%, acc5: 85.130%, test_loss: 1.5644, per_image_load_time: 2.344ms, per_image_inference_time: 0.589ms
2022-06-29 23:45:36 - until epoch: 031, best_acc1: 62.292%
2022-06-29 23:45:36 - epoch 032 lr: 0.084418
2022-06-29 23:46:16 - train: epoch 0032, iter [00100, 05004], lr: 0.084399, loss: 1.7980
2022-06-29 23:46:48 - train: epoch 0032, iter [00200, 05004], lr: 0.084380, loss: 1.8184
2022-06-29 23:47:21 - train: epoch 0032, iter [00300, 05004], lr: 0.084361, loss: 1.8549
2022-06-29 23:47:53 - train: epoch 0032, iter [00400, 05004], lr: 0.084342, loss: 2.0457
2022-06-29 23:48:26 - train: epoch 0032, iter [00500, 05004], lr: 0.084323, loss: 1.9381
2022-06-29 23:49:00 - train: epoch 0032, iter [00600, 05004], lr: 0.084304, loss: 1.8866
2022-06-29 23:49:33 - train: epoch 0032, iter [00700, 05004], lr: 0.084285, loss: 1.8502
2022-06-29 23:50:06 - train: epoch 0032, iter [00800, 05004], lr: 0.084266, loss: 1.7636
2022-06-29 23:50:39 - train: epoch 0032, iter [00900, 05004], lr: 0.084247, loss: 1.8923
2022-06-29 23:51:13 - train: epoch 0032, iter [01000, 05004], lr: 0.084228, loss: 1.9234
2022-06-29 23:51:46 - train: epoch 0032, iter [01100, 05004], lr: 0.084208, loss: 2.0031
2022-06-29 23:52:19 - train: epoch 0032, iter [01200, 05004], lr: 0.084189, loss: 1.8932
2022-06-29 23:52:51 - train: epoch 0032, iter [01300, 05004], lr: 0.084170, loss: 1.8267
2022-06-29 23:53:24 - train: epoch 0032, iter [01400, 05004], lr: 0.084151, loss: 2.0540
2022-06-29 23:53:56 - train: epoch 0032, iter [01500, 05004], lr: 0.084132, loss: 1.8621
2022-06-29 23:54:29 - train: epoch 0032, iter [01600, 05004], lr: 0.084113, loss: 1.9620
2022-06-29 23:55:02 - train: epoch 0032, iter [01700, 05004], lr: 0.084094, loss: 1.9711
2022-06-29 23:55:34 - train: epoch 0032, iter [01800, 05004], lr: 0.084075, loss: 2.0980
2022-06-29 23:56:07 - train: epoch 0032, iter [01900, 05004], lr: 0.084056, loss: 1.6923
2022-06-29 23:56:40 - train: epoch 0032, iter [02000, 05004], lr: 0.084036, loss: 1.8051
2022-06-29 23:57:13 - train: epoch 0032, iter [02100, 05004], lr: 0.084017, loss: 1.7575
2022-06-29 23:57:46 - train: epoch 0032, iter [02200, 05004], lr: 0.083998, loss: 1.7211
2022-06-29 23:58:19 - train: epoch 0032, iter [02300, 05004], lr: 0.083979, loss: 1.8935
2022-06-29 23:58:52 - train: epoch 0032, iter [02400, 05004], lr: 0.083960, loss: 1.7965
2022-06-29 23:59:25 - train: epoch 0032, iter [02500, 05004], lr: 0.083940, loss: 2.0629
2022-06-29 23:59:58 - train: epoch 0032, iter [02600, 05004], lr: 0.083921, loss: 1.6116
2022-06-30 00:00:31 - train: epoch 0032, iter [02700, 05004], lr: 0.083902, loss: 1.8485
2022-06-30 00:01:04 - train: epoch 0032, iter [02800, 05004], lr: 0.083883, loss: 1.9703
2022-06-30 00:01:37 - train: epoch 0032, iter [02900, 05004], lr: 0.083864, loss: 1.6288
2022-06-30 00:02:10 - train: epoch 0032, iter [03000, 05004], lr: 0.083844, loss: 1.6145
2022-06-30 00:02:43 - train: epoch 0032, iter [03100, 05004], lr: 0.083825, loss: 2.0535
2022-06-30 00:03:16 - train: epoch 0032, iter [03200, 05004], lr: 0.083806, loss: 1.8512
2022-06-30 00:03:49 - train: epoch 0032, iter [03300, 05004], lr: 0.083786, loss: 1.7029
2022-06-30 00:04:22 - train: epoch 0032, iter [03400, 05004], lr: 0.083767, loss: 1.6707
2022-06-30 00:04:55 - train: epoch 0032, iter [03500, 05004], lr: 0.083748, loss: 1.8330
2022-06-30 00:05:27 - train: epoch 0032, iter [03600, 05004], lr: 0.083729, loss: 2.0155
2022-06-30 00:06:01 - train: epoch 0032, iter [03700, 05004], lr: 0.083709, loss: 1.8869
2022-06-30 00:06:34 - train: epoch 0032, iter [03800, 05004], lr: 0.083690, loss: 1.6244
2022-06-30 00:07:07 - train: epoch 0032, iter [03900, 05004], lr: 0.083671, loss: 1.8843
2022-06-30 00:07:40 - train: epoch 0032, iter [04000, 05004], lr: 0.083651, loss: 1.7388
2022-06-30 00:08:13 - train: epoch 0032, iter [04100, 05004], lr: 0.083632, loss: 1.7237
2022-06-30 00:08:46 - train: epoch 0032, iter [04200, 05004], lr: 0.083613, loss: 1.7878
2022-06-30 00:09:19 - train: epoch 0032, iter [04300, 05004], lr: 0.083593, loss: 2.1429
2022-06-30 00:09:52 - train: epoch 0032, iter [04400, 05004], lr: 0.083574, loss: 1.8303
2022-06-30 00:10:25 - train: epoch 0032, iter [04500, 05004], lr: 0.083554, loss: 1.9736
2022-06-30 00:10:58 - train: epoch 0032, iter [04600, 05004], lr: 0.083535, loss: 1.8256
2022-06-30 00:11:31 - train: epoch 0032, iter [04700, 05004], lr: 0.083516, loss: 2.0139
2022-06-30 00:12:04 - train: epoch 0032, iter [04800, 05004], lr: 0.083496, loss: 1.6752
2022-06-30 00:12:37 - train: epoch 0032, iter [04900, 05004], lr: 0.083477, loss: 1.9805
2022-06-30 00:13:10 - train: epoch 0032, iter [05000, 05004], lr: 0.083457, loss: 1.8521
2022-06-30 00:13:12 - train: epoch 032, train_loss: 1.8548
2022-06-30 00:14:28 - eval: epoch: 032, acc1: 62.306%, acc5: 85.044%, test_loss: 1.5618, per_image_load_time: 2.348ms, per_image_inference_time: 0.598ms
2022-06-30 00:14:28 - until epoch: 032, best_acc1: 62.306%
2022-06-30 00:14:28 - epoch 033 lr: 0.083456
2022-06-30 00:15:08 - train: epoch 0033, iter [00100, 05004], lr: 0.083437, loss: 1.6473
2022-06-30 00:15:40 - train: epoch 0033, iter [00200, 05004], lr: 0.083418, loss: 1.8702
2022-06-30 00:16:13 - train: epoch 0033, iter [00300, 05004], lr: 0.083398, loss: 1.6870
2022-06-30 00:16:46 - train: epoch 0033, iter [00400, 05004], lr: 0.083379, loss: 1.6018
2022-06-30 00:17:19 - train: epoch 0033, iter [00500, 05004], lr: 0.083359, loss: 1.9012
2022-06-30 00:17:52 - train: epoch 0033, iter [00600, 05004], lr: 0.083340, loss: 1.7861
2022-06-30 00:18:25 - train: epoch 0033, iter [00700, 05004], lr: 0.083320, loss: 1.8987
2022-06-30 00:18:58 - train: epoch 0033, iter [00800, 05004], lr: 0.083301, loss: 1.9506
2022-06-30 00:19:32 - train: epoch 0033, iter [00900, 05004], lr: 0.083281, loss: 1.9527
2022-06-30 00:20:05 - train: epoch 0033, iter [01000, 05004], lr: 0.083262, loss: 1.9649
2022-06-30 00:20:38 - train: epoch 0033, iter [01100, 05004], lr: 0.083242, loss: 1.7666
2022-06-30 00:21:11 - train: epoch 0033, iter [01200, 05004], lr: 0.083223, loss: 1.8794
2022-06-30 00:21:44 - train: epoch 0033, iter [01300, 05004], lr: 0.083203, loss: 1.7815
2022-06-30 00:22:17 - train: epoch 0033, iter [01400, 05004], lr: 0.083183, loss: 2.0485
2022-06-30 00:22:50 - train: epoch 0033, iter [01500, 05004], lr: 0.083164, loss: 2.1645
2022-06-30 00:23:23 - train: epoch 0033, iter [01600, 05004], lr: 0.083144, loss: 2.0434
2022-06-30 00:23:56 - train: epoch 0033, iter [01700, 05004], lr: 0.083125, loss: 1.7228
2022-06-30 00:24:29 - train: epoch 0033, iter [01800, 05004], lr: 0.083105, loss: 2.0828
2022-06-30 00:25:02 - train: epoch 0033, iter [01900, 05004], lr: 0.083086, loss: 2.0196
2022-06-30 00:25:35 - train: epoch 0033, iter [02000, 05004], lr: 0.083066, loss: 1.7008
2022-06-30 00:26:08 - train: epoch 0033, iter [02100, 05004], lr: 0.083046, loss: 1.9369
2022-06-30 00:26:41 - train: epoch 0033, iter [02200, 05004], lr: 0.083027, loss: 1.9254
2022-06-30 00:27:14 - train: epoch 0033, iter [02300, 05004], lr: 0.083007, loss: 1.7022
2022-06-30 00:27:47 - train: epoch 0033, iter [02400, 05004], lr: 0.082987, loss: 2.0485
2022-06-30 00:28:20 - train: epoch 0033, iter [02500, 05004], lr: 0.082968, loss: 1.7470
2022-06-30 00:28:54 - train: epoch 0033, iter [02600, 05004], lr: 0.082948, loss: 1.6072
2022-06-30 00:29:27 - train: epoch 0033, iter [02700, 05004], lr: 0.082928, loss: 2.0590
2022-06-30 00:30:00 - train: epoch 0033, iter [02800, 05004], lr: 0.082909, loss: 1.9069
2022-06-30 00:30:32 - train: epoch 0033, iter [02900, 05004], lr: 0.082889, loss: 2.0146
2022-06-30 00:31:06 - train: epoch 0033, iter [03000, 05004], lr: 0.082869, loss: 1.8070
2022-06-30 00:31:39 - train: epoch 0033, iter [03100, 05004], lr: 0.082850, loss: 1.9609
2022-06-30 00:32:12 - train: epoch 0033, iter [03200, 05004], lr: 0.082830, loss: 1.7797
2022-06-30 00:32:45 - train: epoch 0033, iter [03300, 05004], lr: 0.082810, loss: 1.9166
2022-06-30 00:33:18 - train: epoch 0033, iter [03400, 05004], lr: 0.082790, loss: 1.8267
2022-06-30 00:33:50 - train: epoch 0033, iter [03500, 05004], lr: 0.082771, loss: 1.8608
2022-06-30 00:34:23 - train: epoch 0033, iter [03600, 05004], lr: 0.082751, loss: 2.0513
2022-06-30 00:34:57 - train: epoch 0033, iter [03700, 05004], lr: 0.082731, loss: 1.9254
2022-06-30 00:35:30 - train: epoch 0033, iter [03800, 05004], lr: 0.082711, loss: 1.8523
2022-06-30 00:36:03 - train: epoch 0033, iter [03900, 05004], lr: 0.082691, loss: 1.9431
2022-06-30 00:36:36 - train: epoch 0033, iter [04000, 05004], lr: 0.082672, loss: 1.9268
2022-06-30 00:37:09 - train: epoch 0033, iter [04100, 05004], lr: 0.082652, loss: 1.9571
2022-06-30 00:37:43 - train: epoch 0033, iter [04200, 05004], lr: 0.082632, loss: 1.8365
2022-06-30 00:38:16 - train: epoch 0033, iter [04300, 05004], lr: 0.082612, loss: 2.0052
2022-06-30 00:38:49 - train: epoch 0033, iter [04400, 05004], lr: 0.082592, loss: 1.8719
2022-06-30 00:39:22 - train: epoch 0033, iter [04500, 05004], lr: 0.082573, loss: 2.1174
2022-06-30 00:39:55 - train: epoch 0033, iter [04600, 05004], lr: 0.082553, loss: 1.7603
2022-06-30 00:40:28 - train: epoch 0033, iter [04700, 05004], lr: 0.082533, loss: 1.7421
2022-06-30 00:41:01 - train: epoch 0033, iter [04800, 05004], lr: 0.082513, loss: 2.2956
2022-06-30 00:41:34 - train: epoch 0033, iter [04900, 05004], lr: 0.082493, loss: 1.7123
2022-06-30 00:42:07 - train: epoch 0033, iter [05000, 05004], lr: 0.082473, loss: 1.7909
2022-06-30 00:42:08 - train: epoch 033, train_loss: 1.8436
2022-06-30 00:43:24 - eval: epoch: 033, acc1: 62.244%, acc5: 85.210%, test_loss: 1.5613, per_image_load_time: 2.344ms, per_image_inference_time: 0.599ms
2022-06-30 00:43:25 - until epoch: 033, best_acc1: 62.306%
2022-06-30 00:43:25 - epoch 034 lr: 0.082472
2022-06-30 00:44:04 - train: epoch 0034, iter [00100, 05004], lr: 0.082453, loss: 1.7490
2022-06-30 00:44:36 - train: epoch 0034, iter [00200, 05004], lr: 0.082433, loss: 1.6919
2022-06-30 00:45:09 - train: epoch 0034, iter [00300, 05004], lr: 0.082413, loss: 1.8042
2022-06-30 00:45:42 - train: epoch 0034, iter [00400, 05004], lr: 0.082393, loss: 1.5495
2022-06-30 00:46:14 - train: epoch 0034, iter [00500, 05004], lr: 0.082373, loss: 1.7531
2022-06-30 00:46:47 - train: epoch 0034, iter [00600, 05004], lr: 0.082353, loss: 2.0806
2022-06-30 00:47:20 - train: epoch 0034, iter [00700, 05004], lr: 0.082333, loss: 1.9248
2022-06-30 00:47:53 - train: epoch 0034, iter [00800, 05004], lr: 0.082313, loss: 1.7462
2022-06-30 00:48:26 - train: epoch 0034, iter [00900, 05004], lr: 0.082293, loss: 1.7173
2022-06-30 00:48:58 - train: epoch 0034, iter [01000, 05004], lr: 0.082273, loss: 1.7633
2022-06-30 00:49:31 - train: epoch 0034, iter [01100, 05004], lr: 0.082253, loss: 1.7776
2022-06-30 00:50:04 - train: epoch 0034, iter [01200, 05004], lr: 0.082233, loss: 1.8866
2022-06-30 00:50:37 - train: epoch 0034, iter [01300, 05004], lr: 0.082213, loss: 1.7247
2022-06-30 00:51:10 - train: epoch 0034, iter [01400, 05004], lr: 0.082193, loss: 1.8737
2022-06-30 00:51:43 - train: epoch 0034, iter [01500, 05004], lr: 0.082173, loss: 1.7933
2022-06-30 00:52:16 - train: epoch 0034, iter [01600, 05004], lr: 0.082153, loss: 1.7134
2022-06-30 00:52:49 - train: epoch 0034, iter [01700, 05004], lr: 0.082133, loss: 1.7240
2022-06-30 00:53:22 - train: epoch 0034, iter [01800, 05004], lr: 0.082113, loss: 2.1043
2022-06-30 00:53:55 - train: epoch 0034, iter [01900, 05004], lr: 0.082093, loss: 1.9534
2022-06-30 00:54:28 - train: epoch 0034, iter [02000, 05004], lr: 0.082073, loss: 1.9618
2022-06-30 00:55:01 - train: epoch 0034, iter [02100, 05004], lr: 0.082053, loss: 2.0568
2022-06-30 00:55:34 - train: epoch 0034, iter [02200, 05004], lr: 0.082033, loss: 1.7249
2022-06-30 00:56:07 - train: epoch 0034, iter [02300, 05004], lr: 0.082013, loss: 1.9326
2022-06-30 00:56:39 - train: epoch 0034, iter [02400, 05004], lr: 0.081992, loss: 1.7604
2022-06-30 00:57:13 - train: epoch 0034, iter [02500, 05004], lr: 0.081972, loss: 1.7849
2022-06-30 00:57:46 - train: epoch 0034, iter [02600, 05004], lr: 0.081952, loss: 1.8387
2022-06-30 00:58:19 - train: epoch 0034, iter [02700, 05004], lr: 0.081932, loss: 1.9362
2022-06-30 00:58:52 - train: epoch 0034, iter [02800, 05004], lr: 0.081912, loss: 1.6053
2022-06-30 00:59:25 - train: epoch 0034, iter [02900, 05004], lr: 0.081892, loss: 1.5761
2022-06-30 00:59:58 - train: epoch 0034, iter [03000, 05004], lr: 0.081872, loss: 1.6126
2022-06-30 01:00:31 - train: epoch 0034, iter [03100, 05004], lr: 0.081852, loss: 1.6825
2022-06-30 01:01:04 - train: epoch 0034, iter [03200, 05004], lr: 0.081831, loss: 1.8534
2022-06-30 01:01:37 - train: epoch 0034, iter [03300, 05004], lr: 0.081811, loss: 1.8059
2022-06-30 01:02:10 - train: epoch 0034, iter [03400, 05004], lr: 0.081791, loss: 1.9506
2022-06-30 01:02:43 - train: epoch 0034, iter [03500, 05004], lr: 0.081771, loss: 1.7254
2022-06-30 01:03:16 - train: epoch 0034, iter [03600, 05004], lr: 0.081751, loss: 1.6550
2022-06-30 01:03:49 - train: epoch 0034, iter [03700, 05004], lr: 0.081730, loss: 1.7512
2022-06-30 01:04:22 - train: epoch 0034, iter [03800, 05004], lr: 0.081710, loss: 1.8011
2022-06-30 01:04:55 - train: epoch 0034, iter [03900, 05004], lr: 0.081690, loss: 1.8677
2022-06-30 01:05:28 - train: epoch 0034, iter [04000, 05004], lr: 0.081670, loss: 1.6956
2022-06-30 01:06:01 - train: epoch 0034, iter [04100, 05004], lr: 0.081649, loss: 1.9408
2022-06-30 01:06:34 - train: epoch 0034, iter [04200, 05004], lr: 0.081629, loss: 1.8174
2022-06-30 01:07:07 - train: epoch 0034, iter [04300, 05004], lr: 0.081609, loss: 1.8030
2022-06-30 01:07:40 - train: epoch 0034, iter [04400, 05004], lr: 0.081589, loss: 1.8827
2022-06-30 01:08:13 - train: epoch 0034, iter [04500, 05004], lr: 0.081568, loss: 1.9195
2022-06-30 01:08:46 - train: epoch 0034, iter [04600, 05004], lr: 0.081548, loss: 1.9705
2022-06-30 01:09:19 - train: epoch 0034, iter [04700, 05004], lr: 0.081528, loss: 1.8170
2022-06-30 01:09:52 - train: epoch 0034, iter [04800, 05004], lr: 0.081507, loss: 1.8458
2022-06-30 01:10:25 - train: epoch 0034, iter [04900, 05004], lr: 0.081487, loss: 1.8334
2022-06-30 01:10:57 - train: epoch 0034, iter [05000, 05004], lr: 0.081467, loss: 1.6784
2022-06-30 01:10:59 - train: epoch 034, train_loss: 1.8351
2022-06-30 01:12:14 - eval: epoch: 034, acc1: 61.094%, acc5: 84.730%, test_loss: 1.6114, per_image_load_time: 2.316ms, per_image_inference_time: 0.604ms
2022-06-30 01:12:15 - until epoch: 034, best_acc1: 62.306%
2022-06-30 01:12:15 - epoch 035 lr: 0.081466
2022-06-30 01:12:55 - train: epoch 0035, iter [00100, 05004], lr: 0.081446, loss: 1.6395
2022-06-30 01:13:28 - train: epoch 0035, iter [00200, 05004], lr: 0.081425, loss: 1.6675
2022-06-30 01:14:00 - train: epoch 0035, iter [00300, 05004], lr: 0.081405, loss: 1.9893
2022-06-30 01:14:33 - train: epoch 0035, iter [00400, 05004], lr: 0.081385, loss: 1.6826
2022-06-30 01:15:06 - train: epoch 0035, iter [00500, 05004], lr: 0.081364, loss: 1.7905
2022-06-30 01:15:38 - train: epoch 0035, iter [00600, 05004], lr: 0.081344, loss: 1.8377
2022-06-30 01:16:11 - train: epoch 0035, iter [00700, 05004], lr: 0.081324, loss: 1.7850
2022-06-30 01:16:44 - train: epoch 0035, iter [00800, 05004], lr: 0.081303, loss: 1.8047
2022-06-30 01:17:17 - train: epoch 0035, iter [00900, 05004], lr: 0.081283, loss: 2.0992
2022-06-30 01:17:50 - train: epoch 0035, iter [01000, 05004], lr: 0.081262, loss: 1.8914
2022-06-30 01:18:23 - train: epoch 0035, iter [01100, 05004], lr: 0.081242, loss: 1.8954
2022-06-30 01:18:55 - train: epoch 0035, iter [01200, 05004], lr: 0.081221, loss: 1.7505
2022-06-30 01:19:28 - train: epoch 0035, iter [01300, 05004], lr: 0.081201, loss: 1.8307
2022-06-30 01:20:01 - train: epoch 0035, iter [01400, 05004], lr: 0.081181, loss: 1.7985
2022-06-30 01:20:34 - train: epoch 0035, iter [01500, 05004], lr: 0.081160, loss: 1.8996
2022-06-30 01:21:07 - train: epoch 0035, iter [01600, 05004], lr: 0.081140, loss: 1.7906
2022-06-30 01:21:40 - train: epoch 0035, iter [01700, 05004], lr: 0.081119, loss: 1.6940
2022-06-30 01:22:13 - train: epoch 0035, iter [01800, 05004], lr: 0.081099, loss: 1.8181
2022-06-30 01:22:46 - train: epoch 0035, iter [01900, 05004], lr: 0.081078, loss: 1.8565
2022-06-30 01:23:19 - train: epoch 0035, iter [02000, 05004], lr: 0.081058, loss: 1.8447
2022-06-30 01:23:52 - train: epoch 0035, iter [02100, 05004], lr: 0.081037, loss: 1.7919
2022-06-30 01:24:25 - train: epoch 0035, iter [02200, 05004], lr: 0.081017, loss: 1.9533
2022-06-30 01:24:58 - train: epoch 0035, iter [02300, 05004], lr: 0.080996, loss: 1.8274
2022-06-30 01:25:31 - train: epoch 0035, iter [02400, 05004], lr: 0.080976, loss: 1.9826
2022-06-30 01:26:04 - train: epoch 0035, iter [02500, 05004], lr: 0.080955, loss: 1.7056
2022-06-30 01:26:37 - train: epoch 0035, iter [02600, 05004], lr: 0.080935, loss: 1.9828
2022-06-30 01:27:10 - train: epoch 0035, iter [02700, 05004], lr: 0.080914, loss: 1.9576
2022-06-30 01:27:43 - train: epoch 0035, iter [02800, 05004], lr: 0.080893, loss: 1.7866
2022-06-30 01:28:16 - train: epoch 0035, iter [02900, 05004], lr: 0.080873, loss: 1.9133
2022-06-30 01:28:49 - train: epoch 0035, iter [03000, 05004], lr: 0.080852, loss: 1.9251
2022-06-30 01:29:22 - train: epoch 0035, iter [03100, 05004], lr: 0.080832, loss: 1.7918
2022-06-30 01:29:55 - train: epoch 0035, iter [03200, 05004], lr: 0.080811, loss: 1.7561
2022-06-30 01:30:28 - train: epoch 0035, iter [03300, 05004], lr: 0.080790, loss: 1.7319
2022-06-30 01:31:01 - train: epoch 0035, iter [03400, 05004], lr: 0.080770, loss: 1.7871
2022-06-30 01:31:34 - train: epoch 0035, iter [03500, 05004], lr: 0.080749, loss: 1.5070
2022-06-30 01:32:07 - train: epoch 0035, iter [03600, 05004], lr: 0.080729, loss: 1.8805
2022-06-30 01:32:40 - train: epoch 0035, iter [03700, 05004], lr: 0.080708, loss: 1.7059
2022-06-30 01:33:13 - train: epoch 0035, iter [03800, 05004], lr: 0.080687, loss: 1.8802
2022-06-30 01:33:46 - train: epoch 0035, iter [03900, 05004], lr: 0.080667, loss: 2.0272
2022-06-30 01:34:20 - train: epoch 0035, iter [04000, 05004], lr: 0.080646, loss: 1.5603
2022-06-30 01:34:52 - train: epoch 0035, iter [04100, 05004], lr: 0.080625, loss: 2.0877
2022-06-30 01:35:25 - train: epoch 0035, iter [04200, 05004], lr: 0.080605, loss: 1.8244
2022-06-30 01:35:58 - train: epoch 0035, iter [04300, 05004], lr: 0.080584, loss: 1.8721
2022-06-30 01:36:31 - train: epoch 0035, iter [04400, 05004], lr: 0.080563, loss: 1.7976
2022-06-30 01:37:04 - train: epoch 0035, iter [04500, 05004], lr: 0.080543, loss: 1.8642
2022-06-30 01:37:37 - train: epoch 0035, iter [04600, 05004], lr: 0.080522, loss: 1.7226
2022-06-30 01:38:09 - train: epoch 0035, iter [04700, 05004], lr: 0.080501, loss: 2.0737
2022-06-30 01:38:42 - train: epoch 0035, iter [04800, 05004], lr: 0.080480, loss: 1.9243
2022-06-30 01:39:15 - train: epoch 0035, iter [04900, 05004], lr: 0.080460, loss: 1.8864
2022-06-30 01:39:48 - train: epoch 0035, iter [05000, 05004], lr: 0.080439, loss: 1.6587
2022-06-30 01:39:49 - train: epoch 035, train_loss: 1.8243
2022-06-30 01:41:05 - eval: epoch: 035, acc1: 61.442%, acc5: 84.498%, test_loss: 1.6176, per_image_load_time: 2.327ms, per_image_inference_time: 0.599ms
2022-06-30 01:41:05 - until epoch: 035, best_acc1: 62.306%
2022-06-30 01:41:05 - epoch 036 lr: 0.080438
2022-06-30 01:41:45 - train: epoch 0036, iter [00100, 05004], lr: 0.080417, loss: 1.9244
2022-06-30 01:42:17 - train: epoch 0036, iter [00200, 05004], lr: 0.080397, loss: 1.4792
2022-06-30 01:42:50 - train: epoch 0036, iter [00300, 05004], lr: 0.080376, loss: 1.6662
2022-06-30 01:43:23 - train: epoch 0036, iter [00400, 05004], lr: 0.080355, loss: 1.7323
2022-06-30 01:43:56 - train: epoch 0036, iter [00500, 05004], lr: 0.080334, loss: 1.7876
2022-06-30 01:44:29 - train: epoch 0036, iter [00600, 05004], lr: 0.080313, loss: 1.7513
2022-06-30 01:45:02 - train: epoch 0036, iter [00700, 05004], lr: 0.080293, loss: 1.7304
2022-06-30 01:45:35 - train: epoch 0036, iter [00800, 05004], lr: 0.080272, loss: 1.6204
2022-06-30 01:46:08 - train: epoch 0036, iter [00900, 05004], lr: 0.080251, loss: 1.7435
2022-06-30 01:46:40 - train: epoch 0036, iter [01000, 05004], lr: 0.080230, loss: 1.7677
2022-06-30 01:47:14 - train: epoch 0036, iter [01100, 05004], lr: 0.080209, loss: 1.8282
2022-06-30 01:47:47 - train: epoch 0036, iter [01200, 05004], lr: 0.080188, loss: 1.7740
2022-06-30 01:48:20 - train: epoch 0036, iter [01300, 05004], lr: 0.080168, loss: 1.7469
2022-06-30 01:48:53 - train: epoch 0036, iter [01400, 05004], lr: 0.080147, loss: 1.9933
2022-06-30 01:49:26 - train: epoch 0036, iter [01500, 05004], lr: 0.080126, loss: 1.9892
2022-06-30 01:50:00 - train: epoch 0036, iter [01600, 05004], lr: 0.080105, loss: 1.7816
2022-06-30 01:50:33 - train: epoch 0036, iter [01700, 05004], lr: 0.080084, loss: 1.7518
2022-06-30 01:51:06 - train: epoch 0036, iter [01800, 05004], lr: 0.080063, loss: 1.6202
2022-06-30 01:51:39 - train: epoch 0036, iter [01900, 05004], lr: 0.080042, loss: 1.8641
2022-06-30 01:52:12 - train: epoch 0036, iter [02000, 05004], lr: 0.080021, loss: 1.8119
2022-06-30 01:52:45 - train: epoch 0036, iter [02100, 05004], lr: 0.080000, loss: 1.8017
2022-06-30 01:53:18 - train: epoch 0036, iter [02200, 05004], lr: 0.079979, loss: 1.7526
2022-06-30 01:53:51 - train: epoch 0036, iter [02300, 05004], lr: 0.079959, loss: 1.9086
2022-06-30 01:54:24 - train: epoch 0036, iter [02400, 05004], lr: 0.079938, loss: 1.9442
2022-06-30 01:54:57 - train: epoch 0036, iter [02500, 05004], lr: 0.079917, loss: 1.6832
2022-06-30 01:55:30 - train: epoch 0036, iter [02600, 05004], lr: 0.079896, loss: 1.8986
2022-06-30 01:56:03 - train: epoch 0036, iter [02700, 05004], lr: 0.079875, loss: 1.6675
2022-06-30 01:56:36 - train: epoch 0036, iter [02800, 05004], lr: 0.079854, loss: 1.6316
2022-06-30 01:57:09 - train: epoch 0036, iter [02900, 05004], lr: 0.079833, loss: 1.6247
2022-06-30 01:57:42 - train: epoch 0036, iter [03000, 05004], lr: 0.079812, loss: 2.0255
2022-06-30 01:58:15 - train: epoch 0036, iter [03100, 05004], lr: 0.079791, loss: 1.8409
2022-06-30 01:58:48 - train: epoch 0036, iter [03200, 05004], lr: 0.079770, loss: 1.8225
2022-06-30 01:59:21 - train: epoch 0036, iter [03300, 05004], lr: 0.079749, loss: 1.6562
2022-06-30 01:59:54 - train: epoch 0036, iter [03400, 05004], lr: 0.079728, loss: 1.6547
2022-06-30 02:00:27 - train: epoch 0036, iter [03500, 05004], lr: 0.079707, loss: 1.8378
2022-06-30 02:01:00 - train: epoch 0036, iter [03600, 05004], lr: 0.079686, loss: 2.0051
2022-06-30 02:01:33 - train: epoch 0036, iter [03700, 05004], lr: 0.079665, loss: 1.8835
2022-06-30 02:02:06 - train: epoch 0036, iter [03800, 05004], lr: 0.079643, loss: 1.7543
2022-06-30 02:02:39 - train: epoch 0036, iter [03900, 05004], lr: 0.079622, loss: 1.7603
2022-06-30 02:03:12 - train: epoch 0036, iter [04000, 05004], lr: 0.079601, loss: 1.8458
2022-06-30 02:03:45 - train: epoch 0036, iter [04100, 05004], lr: 0.079580, loss: 1.8288
2022-06-30 02:04:18 - train: epoch 0036, iter [04200, 05004], lr: 0.079559, loss: 1.9124
2022-06-30 02:04:51 - train: epoch 0036, iter [04300, 05004], lr: 0.079538, loss: 1.8717
2022-06-30 02:05:23 - train: epoch 0036, iter [04400, 05004], lr: 0.079517, loss: 1.8548
2022-06-30 02:05:56 - train: epoch 0036, iter [04500, 05004], lr: 0.079496, loss: 1.5792
2022-06-30 02:06:29 - train: epoch 0036, iter [04600, 05004], lr: 0.079475, loss: 1.6367
2022-06-30 02:07:02 - train: epoch 0036, iter [04700, 05004], lr: 0.079454, loss: 1.7760
2022-06-30 02:07:35 - train: epoch 0036, iter [04800, 05004], lr: 0.079432, loss: 1.8382
2022-06-30 02:08:08 - train: epoch 0036, iter [04900, 05004], lr: 0.079411, loss: 1.8512
2022-06-30 02:08:40 - train: epoch 0036, iter [05000, 05004], lr: 0.079390, loss: 1.8689
2022-06-30 02:08:42 - train: epoch 036, train_loss: 1.8142
2022-06-30 02:09:58 - eval: epoch: 036, acc1: 62.198%, acc5: 85.080%, test_loss: 1.5722, per_image_load_time: 2.351ms, per_image_inference_time: 0.605ms
2022-06-30 02:09:58 - until epoch: 036, best_acc1: 62.306%
2022-06-30 02:09:58 - epoch 037 lr: 0.079389
2022-06-30 02:10:38 - train: epoch 0037, iter [00100, 05004], lr: 0.079368, loss: 1.5725
2022-06-30 02:11:10 - train: epoch 0037, iter [00200, 05004], lr: 0.079347, loss: 1.4847
2022-06-30 02:11:43 - train: epoch 0037, iter [00300, 05004], lr: 0.079326, loss: 1.7509
2022-06-30 02:12:16 - train: epoch 0037, iter [00400, 05004], lr: 0.079305, loss: 1.8983
2022-06-30 02:12:49 - train: epoch 0037, iter [00500, 05004], lr: 0.079283, loss: 1.8042
2022-06-30 02:13:21 - train: epoch 0037, iter [00600, 05004], lr: 0.079262, loss: 1.8681
2022-06-30 02:13:54 - train: epoch 0037, iter [00700, 05004], lr: 0.079241, loss: 1.8375
2022-06-30 02:14:27 - train: epoch 0037, iter [00800, 05004], lr: 0.079220, loss: 1.7884
2022-06-30 02:15:01 - train: epoch 0037, iter [00900, 05004], lr: 0.079198, loss: 2.0936
2022-06-30 02:15:34 - train: epoch 0037, iter [01000, 05004], lr: 0.079177, loss: 1.6899
2022-06-30 02:16:07 - train: epoch 0037, iter [01100, 05004], lr: 0.079156, loss: 1.8696
2022-06-30 02:16:40 - train: epoch 0037, iter [01200, 05004], lr: 0.079135, loss: 1.8048
2022-06-30 02:17:13 - train: epoch 0037, iter [01300, 05004], lr: 0.079113, loss: 1.8251
2022-06-30 02:17:46 - train: epoch 0037, iter [01400, 05004], lr: 0.079092, loss: 1.8338
2022-06-30 02:18:19 - train: epoch 0037, iter [01500, 05004], lr: 0.079071, loss: 1.6642
2022-06-30 02:18:51 - train: epoch 0037, iter [01600, 05004], lr: 0.079050, loss: 1.7195
2022-06-30 02:19:24 - train: epoch 0037, iter [01700, 05004], lr: 0.079028, loss: 1.9887
2022-06-30 02:19:57 - train: epoch 0037, iter [01800, 05004], lr: 0.079007, loss: 1.9999
2022-06-30 02:20:30 - train: epoch 0037, iter [01900, 05004], lr: 0.078986, loss: 1.9268
2022-06-30 02:21:03 - train: epoch 0037, iter [02000, 05004], lr: 0.078964, loss: 1.8353
2022-06-30 02:21:36 - train: epoch 0037, iter [02100, 05004], lr: 0.078943, loss: 1.7796
2022-06-30 02:22:09 - train: epoch 0037, iter [02200, 05004], lr: 0.078922, loss: 1.6489
2022-06-30 02:22:41 - train: epoch 0037, iter [02300, 05004], lr: 0.078900, loss: 1.5970
2022-06-30 02:23:14 - train: epoch 0037, iter [02400, 05004], lr: 0.078879, loss: 1.7942
2022-06-30 02:23:47 - train: epoch 0037, iter [02500, 05004], lr: 0.078858, loss: 1.7406
2022-06-30 02:24:20 - train: epoch 0037, iter [02600, 05004], lr: 0.078836, loss: 1.9783
2022-06-30 02:24:53 - train: epoch 0037, iter [02700, 05004], lr: 0.078815, loss: 1.9557
2022-06-30 02:25:26 - train: epoch 0037, iter [02800, 05004], lr: 0.078794, loss: 1.9506
2022-06-30 02:25:59 - train: epoch 0037, iter [02900, 05004], lr: 0.078772, loss: 1.8992
2022-06-30 02:26:32 - train: epoch 0037, iter [03000, 05004], lr: 0.078751, loss: 1.9562
2022-06-30 02:27:05 - train: epoch 0037, iter [03100, 05004], lr: 0.078729, loss: 1.6849
2022-06-30 02:27:39 - train: epoch 0037, iter [03200, 05004], lr: 0.078708, loss: 1.8784
2022-06-30 02:28:11 - train: epoch 0037, iter [03300, 05004], lr: 0.078687, loss: 1.6384
2022-06-30 02:28:44 - train: epoch 0037, iter [03400, 05004], lr: 0.078665, loss: 1.6197
2022-06-30 02:29:18 - train: epoch 0037, iter [03500, 05004], lr: 0.078644, loss: 1.8191
2022-06-30 02:29:51 - train: epoch 0037, iter [03600, 05004], lr: 0.078622, loss: 1.8742
2022-06-30 02:30:24 - train: epoch 0037, iter [03700, 05004], lr: 0.078601, loss: 1.8802
2022-06-30 02:30:57 - train: epoch 0037, iter [03800, 05004], lr: 0.078579, loss: 1.7821
2022-06-30 02:31:30 - train: epoch 0037, iter [03900, 05004], lr: 0.078558, loss: 1.9890
2022-06-30 02:32:03 - train: epoch 0037, iter [04000, 05004], lr: 0.078536, loss: 1.7225
2022-06-30 02:32:35 - train: epoch 0037, iter [04100, 05004], lr: 0.078515, loss: 1.7772
2022-06-30 02:33:08 - train: epoch 0037, iter [04200, 05004], lr: 0.078493, loss: 2.1158
2022-06-30 02:33:41 - train: epoch 0037, iter [04300, 05004], lr: 0.078472, loss: 1.8294
2022-06-30 02:34:14 - train: epoch 0037, iter [04400, 05004], lr: 0.078450, loss: 1.8179
2022-06-30 02:34:47 - train: epoch 0037, iter [04500, 05004], lr: 0.078429, loss: 1.9048
2022-06-30 02:35:20 - train: epoch 0037, iter [04600, 05004], lr: 0.078407, loss: 1.8961
2022-06-30 02:35:52 - train: epoch 0037, iter [04700, 05004], lr: 0.078386, loss: 1.8475
2022-06-30 02:36:25 - train: epoch 0037, iter [04800, 05004], lr: 0.078364, loss: 1.8185
2022-06-30 02:36:58 - train: epoch 0037, iter [04900, 05004], lr: 0.078343, loss: 1.8517
2022-06-30 02:37:31 - train: epoch 0037, iter [05000, 05004], lr: 0.078321, loss: 1.7077
2022-06-30 02:37:33 - train: epoch 037, train_loss: 1.8090
2022-06-30 02:38:48 - eval: epoch: 037, acc1: 63.764%, acc5: 86.042%, test_loss: 1.5059, per_image_load_time: 2.320ms, per_image_inference_time: 0.596ms
2022-06-30 02:38:49 - until epoch: 037, best_acc1: 63.764%
2022-06-30 02:38:49 - epoch 038 lr: 0.078320
2022-06-30 02:39:27 - train: epoch 0038, iter [00100, 05004], lr: 0.078299, loss: 1.7241
2022-06-30 02:39:59 - train: epoch 0038, iter [00200, 05004], lr: 0.078277, loss: 1.5385
2022-06-30 02:40:32 - train: epoch 0038, iter [00300, 05004], lr: 0.078256, loss: 1.5655
2022-06-30 02:41:05 - train: epoch 0038, iter [00400, 05004], lr: 0.078234, loss: 1.7150
2022-06-30 02:41:38 - train: epoch 0038, iter [00500, 05004], lr: 0.078212, loss: 1.6266
2022-06-30 02:42:11 - train: epoch 0038, iter [00600, 05004], lr: 0.078191, loss: 1.8696
2022-06-30 02:42:44 - train: epoch 0038, iter [00700, 05004], lr: 0.078169, loss: 1.5664
2022-06-30 02:43:17 - train: epoch 0038, iter [00800, 05004], lr: 0.078148, loss: 1.6260
2022-06-30 02:43:49 - train: epoch 0038, iter [00900, 05004], lr: 0.078126, loss: 1.7066
2022-06-30 02:44:22 - train: epoch 0038, iter [01000, 05004], lr: 0.078104, loss: 2.0560
2022-06-30 02:44:55 - train: epoch 0038, iter [01100, 05004], lr: 0.078083, loss: 1.8564
2022-06-30 02:45:29 - train: epoch 0038, iter [01200, 05004], lr: 0.078061, loss: 1.7687
2022-06-30 02:46:02 - train: epoch 0038, iter [01300, 05004], lr: 0.078039, loss: 1.9562
2022-06-30 02:46:35 - train: epoch 0038, iter [01400, 05004], lr: 0.078018, loss: 1.9530
2022-06-30 02:47:08 - train: epoch 0038, iter [01500, 05004], lr: 0.077996, loss: 1.8721
2022-06-30 02:47:41 - train: epoch 0038, iter [01600, 05004], lr: 0.077974, loss: 1.8814
2022-06-30 02:48:14 - train: epoch 0038, iter [01700, 05004], lr: 0.077953, loss: 1.9825
2022-06-30 02:48:47 - train: epoch 0038, iter [01800, 05004], lr: 0.077931, loss: 1.9307
2022-06-30 02:49:20 - train: epoch 0038, iter [01900, 05004], lr: 0.077909, loss: 1.8602
2022-06-30 02:49:53 - train: epoch 0038, iter [02000, 05004], lr: 0.077888, loss: 1.7855
2022-06-30 02:50:26 - train: epoch 0038, iter [02100, 05004], lr: 0.077866, loss: 1.8955
2022-06-30 02:50:59 - train: epoch 0038, iter [02200, 05004], lr: 0.077844, loss: 1.6789
2022-06-30 02:51:33 - train: epoch 0038, iter [02300, 05004], lr: 0.077822, loss: 1.9776
2022-06-30 02:52:06 - train: epoch 0038, iter [02400, 05004], lr: 0.077801, loss: 2.1454
2022-06-30 02:52:39 - train: epoch 0038, iter [02500, 05004], lr: 0.077779, loss: 1.7722
2022-06-30 02:53:12 - train: epoch 0038, iter [02600, 05004], lr: 0.077757, loss: 1.7269
2022-06-30 02:53:45 - train: epoch 0038, iter [02700, 05004], lr: 0.077735, loss: 1.9125
2022-06-30 02:54:19 - train: epoch 0038, iter [02800, 05004], lr: 0.077714, loss: 1.9455
2022-06-30 02:54:52 - train: epoch 0038, iter [02900, 05004], lr: 0.077692, loss: 1.9791
2022-06-30 02:55:25 - train: epoch 0038, iter [03000, 05004], lr: 0.077670, loss: 1.8144
2022-06-30 02:55:58 - train: epoch 0038, iter [03100, 05004], lr: 0.077648, loss: 1.7701
2022-06-30 02:56:31 - train: epoch 0038, iter [03200, 05004], lr: 0.077627, loss: 1.5454
2022-06-30 02:57:04 - train: epoch 0038, iter [03300, 05004], lr: 0.077605, loss: 1.5918
2022-06-30 02:57:37 - train: epoch 0038, iter [03400, 05004], lr: 0.077583, loss: 1.7056
2022-06-30 02:58:11 - train: epoch 0038, iter [03500, 05004], lr: 0.077561, loss: 1.8326
2022-06-30 02:58:44 - train: epoch 0038, iter [03600, 05004], lr: 0.077539, loss: 1.8206
2022-06-30 02:59:17 - train: epoch 0038, iter [03700, 05004], lr: 0.077517, loss: 1.4873
2022-06-30 02:59:50 - train: epoch 0038, iter [03800, 05004], lr: 0.077496, loss: 1.9487
2022-06-30 03:00:23 - train: epoch 0038, iter [03900, 05004], lr: 0.077474, loss: 1.8475
2022-06-30 03:00:56 - train: epoch 0038, iter [04000, 05004], lr: 0.077452, loss: 1.8487
2022-06-30 03:01:29 - train: epoch 0038, iter [04100, 05004], lr: 0.077430, loss: 1.7121
2022-06-30 03:02:02 - train: epoch 0038, iter [04200, 05004], lr: 0.077408, loss: 1.5974
2022-06-30 03:02:35 - train: epoch 0038, iter [04300, 05004], lr: 0.077386, loss: 1.9489
2022-06-30 03:03:08 - train: epoch 0038, iter [04400, 05004], lr: 0.077364, loss: 1.8580
2022-06-30 03:03:42 - train: epoch 0038, iter [04500, 05004], lr: 0.077342, loss: 1.8427
2022-06-30 03:04:15 - train: epoch 0038, iter [04600, 05004], lr: 0.077321, loss: 1.9555
2022-06-30 03:04:48 - train: epoch 0038, iter [04700, 05004], lr: 0.077299, loss: 1.8018
2022-06-30 03:05:21 - train: epoch 0038, iter [04800, 05004], lr: 0.077277, loss: 1.8457
2022-06-30 03:05:54 - train: epoch 0038, iter [04900, 05004], lr: 0.077255, loss: 1.7717
2022-06-30 03:06:26 - train: epoch 0038, iter [05000, 05004], lr: 0.077233, loss: 1.5512
2022-06-30 03:06:28 - train: epoch 038, train_loss: 1.7974
2022-06-30 03:07:44 - eval: epoch: 038, acc1: 62.890%, acc5: 85.530%, test_loss: 1.5351, per_image_load_time: 2.380ms, per_image_inference_time: 0.599ms
2022-06-30 03:07:45 - until epoch: 038, best_acc1: 63.764%
2022-06-30 03:07:45 - epoch 039 lr: 0.077232
2022-06-30 03:08:24 - train: epoch 0039, iter [00100, 05004], lr: 0.077210, loss: 1.8220
2022-06-30 03:08:57 - train: epoch 0039, iter [00200, 05004], lr: 0.077188, loss: 1.9491
2022-06-30 03:09:30 - train: epoch 0039, iter [00300, 05004], lr: 0.077166, loss: 1.5519
2022-06-30 03:10:02 - train: epoch 0039, iter [00400, 05004], lr: 0.077144, loss: 1.7681
2022-06-30 03:10:35 - train: epoch 0039, iter [00500, 05004], lr: 0.077122, loss: 1.7549
2022-06-30 03:11:08 - train: epoch 0039, iter [00600, 05004], lr: 0.077100, loss: 1.6800
2022-06-30 03:11:41 - train: epoch 0039, iter [00700, 05004], lr: 0.077078, loss: 1.9196
2022-06-30 03:12:14 - train: epoch 0039, iter [00800, 05004], lr: 0.077056, loss: 1.7674
2022-06-30 03:12:47 - train: epoch 0039, iter [00900, 05004], lr: 0.077034, loss: 1.8609
2022-06-30 03:13:20 - train: epoch 0039, iter [01000, 05004], lr: 0.077012, loss: 1.6884
2022-06-30 03:13:53 - train: epoch 0039, iter [01100, 05004], lr: 0.076990, loss: 1.7986
2022-06-30 03:14:26 - train: epoch 0039, iter [01200, 05004], lr: 0.076968, loss: 1.9026
2022-06-30 03:14:59 - train: epoch 0039, iter [01300, 05004], lr: 0.076946, loss: 1.9996
2022-06-30 03:15:32 - train: epoch 0039, iter [01400, 05004], lr: 0.076924, loss: 1.8527
2022-06-30 03:16:05 - train: epoch 0039, iter [01500, 05004], lr: 0.076902, loss: 1.7432
2022-06-30 03:16:39 - train: epoch 0039, iter [01600, 05004], lr: 0.076880, loss: 1.8812
2022-06-30 03:17:12 - train: epoch 0039, iter [01700, 05004], lr: 0.076858, loss: 1.5405
2022-06-30 03:17:45 - train: epoch 0039, iter [01800, 05004], lr: 0.076836, loss: 1.7045
2022-06-30 03:18:19 - train: epoch 0039, iter [01900, 05004], lr: 0.076814, loss: 1.5145
2022-06-30 03:18:52 - train: epoch 0039, iter [02000, 05004], lr: 0.076792, loss: 1.7195
2022-06-30 03:19:25 - train: epoch 0039, iter [02100, 05004], lr: 0.076770, loss: 1.8725
2022-06-30 03:19:58 - train: epoch 0039, iter [02200, 05004], lr: 0.076748, loss: 1.7173
2022-06-30 03:20:30 - train: epoch 0039, iter [02300, 05004], lr: 0.076725, loss: 1.9722
2022-06-30 03:21:03 - train: epoch 0039, iter [02400, 05004], lr: 0.076703, loss: 1.8834
2022-06-30 03:21:36 - train: epoch 0039, iter [02500, 05004], lr: 0.076681, loss: 1.6520
2022-06-30 03:22:09 - train: epoch 0039, iter [02600, 05004], lr: 0.076659, loss: 1.7724
2022-06-30 03:22:42 - train: epoch 0039, iter [02700, 05004], lr: 0.076637, loss: 1.9613
2022-06-30 03:23:15 - train: epoch 0039, iter [02800, 05004], lr: 0.076615, loss: 1.7742
2022-06-30 03:23:48 - train: epoch 0039, iter [02900, 05004], lr: 0.076593, loss: 1.5847
2022-06-30 03:24:21 - train: epoch 0039, iter [03000, 05004], lr: 0.076570, loss: 1.7633
2022-06-30 03:24:54 - train: epoch 0039, iter [03100, 05004], lr: 0.076548, loss: 1.6508
2022-06-30 03:25:28 - train: epoch 0039, iter [03200, 05004], lr: 0.076526, loss: 1.7734
2022-06-30 03:26:01 - train: epoch 0039, iter [03300, 05004], lr: 0.076504, loss: 1.8663
2022-06-30 03:26:34 - train: epoch 0039, iter [03400, 05004], lr: 0.076482, loss: 1.8419
2022-06-30 03:27:07 - train: epoch 0039, iter [03500, 05004], lr: 0.076460, loss: 2.0805
2022-06-30 03:27:40 - train: epoch 0039, iter [03600, 05004], lr: 0.076437, loss: 1.8775
2022-06-30 03:28:13 - train: epoch 0039, iter [03700, 05004], lr: 0.076415, loss: 1.6755
2022-06-30 03:28:46 - train: epoch 0039, iter [03800, 05004], lr: 0.076393, loss: 1.6159
2022-06-30 03:29:19 - train: epoch 0039, iter [03900, 05004], lr: 0.076371, loss: 1.9947
2022-06-30 03:29:52 - train: epoch 0039, iter [04000, 05004], lr: 0.076349, loss: 1.8432
2022-06-30 03:30:25 - train: epoch 0039, iter [04100, 05004], lr: 0.076326, loss: 1.8728
2022-06-30 03:30:58 - train: epoch 0039, iter [04200, 05004], lr: 0.076304, loss: 1.9228
2022-06-30 03:31:31 - train: epoch 0039, iter [04300, 05004], lr: 0.076282, loss: 1.7536
2022-06-30 03:32:04 - train: epoch 0039, iter [04400, 05004], lr: 0.076260, loss: 1.4761
2022-06-30 03:32:37 - train: epoch 0039, iter [04500, 05004], lr: 0.076237, loss: 1.7668
2022-06-30 03:33:10 - train: epoch 0039, iter [04600, 05004], lr: 0.076215, loss: 2.0230
2022-06-30 03:33:43 - train: epoch 0039, iter [04700, 05004], lr: 0.076193, loss: 1.8429
2022-06-30 03:34:17 - train: epoch 0039, iter [04800, 05004], lr: 0.076170, loss: 1.8079
2022-06-30 03:34:50 - train: epoch 0039, iter [04900, 05004], lr: 0.076148, loss: 1.6779
2022-06-30 03:35:22 - train: epoch 0039, iter [05000, 05004], lr: 0.076126, loss: 1.6741
2022-06-30 03:35:24 - train: epoch 039, train_loss: 1.7892
2022-06-30 03:36:40 - eval: epoch: 039, acc1: 63.192%, acc5: 85.966%, test_loss: 1.5145, per_image_load_time: 2.391ms, per_image_inference_time: 0.589ms
2022-06-30 03:36:41 - until epoch: 039, best_acc1: 63.764%
2022-06-30 03:36:41 - epoch 040 lr: 0.076125
2022-06-30 03:37:20 - train: epoch 0040, iter [00100, 05004], lr: 0.076103, loss: 2.0233
2022-06-30 03:37:53 - train: epoch 0040, iter [00200, 05004], lr: 0.076080, loss: 2.0173
2022-06-30 03:38:26 - train: epoch 0040, iter [00300, 05004], lr: 0.076058, loss: 1.8034
2022-06-30 03:38:59 - train: epoch 0040, iter [00400, 05004], lr: 0.076036, loss: 1.8675
2022-06-30 03:39:32 - train: epoch 0040, iter [00500, 05004], lr: 0.076013, loss: 1.7093
2022-06-30 03:40:05 - train: epoch 0040, iter [00600, 05004], lr: 0.075991, loss: 1.9542
2022-06-30 03:40:38 - train: epoch 0040, iter [00700, 05004], lr: 0.075969, loss: 1.8136
2022-06-30 03:41:11 - train: epoch 0040, iter [00800, 05004], lr: 0.075946, loss: 1.9635
2022-06-30 03:41:44 - train: epoch 0040, iter [00900, 05004], lr: 0.075924, loss: 1.6631
2022-06-30 03:42:17 - train: epoch 0040, iter [01000, 05004], lr: 0.075902, loss: 1.5130
2022-06-30 03:42:50 - train: epoch 0040, iter [01100, 05004], lr: 0.075879, loss: 1.7350
2022-06-30 03:43:23 - train: epoch 0040, iter [01200, 05004], lr: 0.075857, loss: 1.6712
2022-06-30 03:43:56 - train: epoch 0040, iter [01300, 05004], lr: 0.075834, loss: 1.6946
2022-06-30 03:44:28 - train: epoch 0040, iter [01400, 05004], lr: 0.075812, loss: 1.7204
2022-06-30 03:45:01 - train: epoch 0040, iter [01500, 05004], lr: 0.075790, loss: 1.9593
2022-06-30 03:45:33 - train: epoch 0040, iter [01600, 05004], lr: 0.075767, loss: 1.7168
2022-06-30 03:46:06 - train: epoch 0040, iter [01700, 05004], lr: 0.075745, loss: 1.8745
2022-06-30 03:46:39 - train: epoch 0040, iter [01800, 05004], lr: 0.075722, loss: 1.7145
2022-06-30 03:47:12 - train: epoch 0040, iter [01900, 05004], lr: 0.075700, loss: 1.7764
2022-06-30 03:47:45 - train: epoch 0040, iter [02000, 05004], lr: 0.075677, loss: 1.7818
2022-06-30 03:48:18 - train: epoch 0040, iter [02100, 05004], lr: 0.075655, loss: 1.7439
2022-06-30 03:48:51 - train: epoch 0040, iter [02200, 05004], lr: 0.075633, loss: 1.6029
2022-06-30 03:49:24 - train: epoch 0040, iter [02300, 05004], lr: 0.075610, loss: 1.7710
2022-06-30 03:49:57 - train: epoch 0040, iter [02400, 05004], lr: 0.075588, loss: 1.7909
2022-06-30 03:50:30 - train: epoch 0040, iter [02500, 05004], lr: 0.075565, loss: 1.8905
2022-06-30 03:51:03 - train: epoch 0040, iter [02600, 05004], lr: 0.075543, loss: 1.7039
2022-06-30 03:51:36 - train: epoch 0040, iter [02700, 05004], lr: 0.075520, loss: 1.8802
2022-06-30 03:52:09 - train: epoch 0040, iter [02800, 05004], lr: 0.075498, loss: 1.7882
2022-06-30 03:52:42 - train: epoch 0040, iter [02900, 05004], lr: 0.075475, loss: 2.0715
2022-06-30 03:53:15 - train: epoch 0040, iter [03000, 05004], lr: 0.075453, loss: 1.9428
2022-06-30 03:53:48 - train: epoch 0040, iter [03100, 05004], lr: 0.075430, loss: 1.7892
2022-06-30 03:54:21 - train: epoch 0040, iter [03200, 05004], lr: 0.075408, loss: 1.9729
2022-06-30 03:54:53 - train: epoch 0040, iter [03300, 05004], lr: 0.075385, loss: 1.8093
2022-06-30 03:55:26 - train: epoch 0040, iter [03400, 05004], lr: 0.075362, loss: 1.7766
2022-06-30 03:55:59 - train: epoch 0040, iter [03500, 05004], lr: 0.075340, loss: 1.8058
2022-06-30 03:56:32 - train: epoch 0040, iter [03600, 05004], lr: 0.075317, loss: 1.7776
2022-06-30 03:57:05 - train: epoch 0040, iter [03700, 05004], lr: 0.075295, loss: 1.7761
2022-06-30 03:57:39 - train: epoch 0040, iter [03800, 05004], lr: 0.075272, loss: 1.5727
2022-06-30 03:58:12 - train: epoch 0040, iter [03900, 05004], lr: 0.075250, loss: 1.9173
2022-06-30 03:58:45 - train: epoch 0040, iter [04000, 05004], lr: 0.075227, loss: 1.9937
2022-06-30 03:59:18 - train: epoch 0040, iter [04100, 05004], lr: 0.075205, loss: 1.8458
2022-06-30 03:59:51 - train: epoch 0040, iter [04200, 05004], lr: 0.075182, loss: 1.7611
2022-06-30 04:00:24 - train: epoch 0040, iter [04300, 05004], lr: 0.075159, loss: 1.7952
2022-06-30 04:00:57 - train: epoch 0040, iter [04400, 05004], lr: 0.075137, loss: 1.9463
2022-06-30 04:01:30 - train: epoch 0040, iter [04500, 05004], lr: 0.075114, loss: 1.5456
2022-06-30 04:02:03 - train: epoch 0040, iter [04600, 05004], lr: 0.075091, loss: 1.7085
2022-06-30 04:02:36 - train: epoch 0040, iter [04700, 05004], lr: 0.075069, loss: 1.8063
2022-06-30 04:03:09 - train: epoch 0040, iter [04800, 05004], lr: 0.075046, loss: 1.7584
2022-06-30 04:03:43 - train: epoch 0040, iter [04900, 05004], lr: 0.075024, loss: 1.9327
2022-06-30 04:04:16 - train: epoch 0040, iter [05000, 05004], lr: 0.075001, loss: 1.8055
2022-06-30 04:04:17 - train: epoch 040, train_loss: 1.7802
2022-06-30 04:05:33 - eval: epoch: 040, acc1: 63.688%, acc5: 86.338%, test_loss: 1.4889, per_image_load_time: 2.351ms, per_image_inference_time: 0.605ms
2022-06-30 04:05:33 - until epoch: 040, best_acc1: 63.764%
2022-06-30 04:05:33 - epoch 041 lr: 0.075000
2022-06-30 04:06:13 - train: epoch 0041, iter [00100, 05004], lr: 0.074977, loss: 1.9071
2022-06-30 04:06:46 - train: epoch 0041, iter [00200, 05004], lr: 0.074955, loss: 1.8688
2022-06-30 04:07:18 - train: epoch 0041, iter [00300, 05004], lr: 0.074932, loss: 1.7305
2022-06-30 04:07:51 - train: epoch 0041, iter [00400, 05004], lr: 0.074909, loss: 1.7846
2022-06-30 04:08:24 - train: epoch 0041, iter [00500, 05004], lr: 0.074887, loss: 1.5504
2022-06-30 04:08:57 - train: epoch 0041, iter [00600, 05004], lr: 0.074864, loss: 1.8902
2022-06-30 04:09:29 - train: epoch 0041, iter [00700, 05004], lr: 0.074841, loss: 1.6780
2022-06-30 04:10:02 - train: epoch 0041, iter [00800, 05004], lr: 0.074819, loss: 1.5455
2022-06-30 04:10:35 - train: epoch 0041, iter [00900, 05004], lr: 0.074796, loss: 1.6075
2022-06-30 04:11:08 - train: epoch 0041, iter [01000, 05004], lr: 0.074773, loss: 1.9793
2022-06-30 04:11:41 - train: epoch 0041, iter [01100, 05004], lr: 0.074750, loss: 1.7145
2022-06-30 04:12:14 - train: epoch 0041, iter [01200, 05004], lr: 0.074728, loss: 1.4971
2022-06-30 04:12:47 - train: epoch 0041, iter [01300, 05004], lr: 0.074705, loss: 1.7622
2022-06-30 04:13:20 - train: epoch 0041, iter [01400, 05004], lr: 0.074682, loss: 1.8859
2022-06-30 04:13:53 - train: epoch 0041, iter [01500, 05004], lr: 0.074659, loss: 2.1666
2022-06-30 04:14:25 - train: epoch 0041, iter [01600, 05004], lr: 0.074637, loss: 1.5819
2022-06-30 04:14:58 - train: epoch 0041, iter [01700, 05004], lr: 0.074614, loss: 1.9226
2022-06-30 04:15:31 - train: epoch 0041, iter [01800, 05004], lr: 0.074591, loss: 1.7106
2022-06-30 04:16:04 - train: epoch 0041, iter [01900, 05004], lr: 0.074568, loss: 1.9184
2022-06-30 04:16:37 - train: epoch 0041, iter [02000, 05004], lr: 0.074546, loss: 1.5668
2022-06-30 04:17:10 - train: epoch 0041, iter [02100, 05004], lr: 0.074523, loss: 1.5378
2022-06-30 04:17:43 - train: epoch 0041, iter [02200, 05004], lr: 0.074500, loss: 1.5890
2022-06-30 04:18:16 - train: epoch 0041, iter [02300, 05004], lr: 0.074477, loss: 1.4936
2022-06-30 04:18:49 - train: epoch 0041, iter [02400, 05004], lr: 0.074454, loss: 1.9441
2022-06-30 04:19:22 - train: epoch 0041, iter [02500, 05004], lr: 0.074432, loss: 1.5519
2022-06-30 04:19:55 - train: epoch 0041, iter [02600, 05004], lr: 0.074409, loss: 2.0138
2022-06-30 04:20:28 - train: epoch 0041, iter [02700, 05004], lr: 0.074386, loss: 2.0354
2022-06-30 04:21:02 - train: epoch 0041, iter [02800, 05004], lr: 0.074363, loss: 1.7533
2022-06-30 04:21:35 - train: epoch 0041, iter [02900, 05004], lr: 0.074340, loss: 1.9064
2022-06-30 04:22:08 - train: epoch 0041, iter [03000, 05004], lr: 0.074317, loss: 1.9070
2022-06-30 04:22:41 - train: epoch 0041, iter [03100, 05004], lr: 0.074294, loss: 1.9081
2022-06-30 04:23:14 - train: epoch 0041, iter [03200, 05004], lr: 0.074272, loss: 1.6845
2022-06-30 04:23:47 - train: epoch 0041, iter [03300, 05004], lr: 0.074249, loss: 1.6861
2022-06-30 04:24:20 - train: epoch 0041, iter [03400, 05004], lr: 0.074226, loss: 1.5869
2022-06-30 04:24:53 - train: epoch 0041, iter [03500, 05004], lr: 0.074203, loss: 1.8541
2022-06-30 04:25:26 - train: epoch 0041, iter [03600, 05004], lr: 0.074180, loss: 1.8641
2022-06-30 04:25:59 - train: epoch 0041, iter [03700, 05004], lr: 0.074157, loss: 1.8394
2022-06-30 04:26:32 - train: epoch 0041, iter [03800, 05004], lr: 0.074134, loss: 1.7011
2022-06-30 04:27:05 - train: epoch 0041, iter [03900, 05004], lr: 0.074111, loss: 1.7955
2022-06-30 04:27:38 - train: epoch 0041, iter [04000, 05004], lr: 0.074088, loss: 1.6774
2022-06-30 04:28:12 - train: epoch 0041, iter [04100, 05004], lr: 0.074065, loss: 1.8015
2022-06-30 04:28:45 - train: epoch 0041, iter [04200, 05004], lr: 0.074043, loss: 1.6894
2022-06-30 04:29:18 - train: epoch 0041, iter [04300, 05004], lr: 0.074020, loss: 1.7988
2022-06-30 04:29:51 - train: epoch 0041, iter [04400, 05004], lr: 0.073997, loss: 1.7164
2022-06-30 04:30:24 - train: epoch 0041, iter [04500, 05004], lr: 0.073974, loss: 1.8829
2022-06-30 04:30:57 - train: epoch 0041, iter [04600, 05004], lr: 0.073951, loss: 1.9029
2022-06-30 04:31:31 - train: epoch 0041, iter [04700, 05004], lr: 0.073928, loss: 1.9949
2022-06-30 04:32:04 - train: epoch 0041, iter [04800, 05004], lr: 0.073905, loss: 1.7762
2022-06-30 04:32:37 - train: epoch 0041, iter [04900, 05004], lr: 0.073882, loss: 1.8741
2022-06-30 04:33:10 - train: epoch 0041, iter [05000, 05004], lr: 0.073859, loss: 1.8680
2022-06-30 04:33:11 - train: epoch 041, train_loss: 1.7739
2022-06-30 04:34:29 - eval: epoch: 041, acc1: 63.978%, acc5: 86.100%, test_loss: 1.5025, per_image_load_time: 2.345ms, per_image_inference_time: 0.612ms
2022-06-30 04:34:30 - until epoch: 041, best_acc1: 63.978%
2022-06-30 04:34:30 - epoch 042 lr: 0.073858
2022-06-30 04:35:09 - train: epoch 0042, iter [00100, 05004], lr: 0.073835, loss: 1.5027
2022-06-30 04:35:42 - train: epoch 0042, iter [00200, 05004], lr: 0.073812, loss: 1.8165
2022-06-30 04:36:15 - train: epoch 0042, iter [00300, 05004], lr: 0.073789, loss: 1.7614
2022-06-30 04:36:48 - train: epoch 0042, iter [00400, 05004], lr: 0.073766, loss: 1.5036
2022-06-30 04:37:22 - train: epoch 0042, iter [00500, 05004], lr: 0.073743, loss: 1.8495
2022-06-30 04:37:56 - train: epoch 0042, iter [00600, 05004], lr: 0.073720, loss: 1.5564
2022-06-30 04:38:29 - train: epoch 0042, iter [00700, 05004], lr: 0.073697, loss: 1.7440
2022-06-30 04:39:03 - train: epoch 0042, iter [00800, 05004], lr: 0.073674, loss: 1.6942
2022-06-30 04:39:37 - train: epoch 0042, iter [00900, 05004], lr: 0.073651, loss: 1.9284
2022-06-30 04:40:11 - train: epoch 0042, iter [01000, 05004], lr: 0.073628, loss: 1.8475
2022-06-30 04:40:44 - train: epoch 0042, iter [01100, 05004], lr: 0.073605, loss: 1.5800
2022-06-30 04:41:18 - train: epoch 0042, iter [01200, 05004], lr: 0.073582, loss: 1.6418
2022-06-30 04:41:52 - train: epoch 0042, iter [01300, 05004], lr: 0.073559, loss: 1.7791
2022-06-30 04:42:26 - train: epoch 0042, iter [01400, 05004], lr: 0.073535, loss: 2.0277
2022-06-30 04:43:00 - train: epoch 0042, iter [01500, 05004], lr: 0.073512, loss: 1.6952
2022-06-30 04:43:34 - train: epoch 0042, iter [01600, 05004], lr: 0.073489, loss: 1.8831
2022-06-30 04:44:07 - train: epoch 0042, iter [01700, 05004], lr: 0.073466, loss: 1.7364
2022-06-30 04:44:41 - train: epoch 0042, iter [01800, 05004], lr: 0.073443, loss: 1.6234
2022-06-30 04:45:15 - train: epoch 0042, iter [01900, 05004], lr: 0.073420, loss: 1.8948
2022-06-30 04:45:49 - train: epoch 0042, iter [02000, 05004], lr: 0.073397, loss: 1.6173
2022-06-30 04:46:23 - train: epoch 0042, iter [02100, 05004], lr: 0.073374, loss: 1.5992
2022-06-30 04:46:57 - train: epoch 0042, iter [02200, 05004], lr: 0.073351, loss: 1.6560
2022-06-30 04:47:31 - train: epoch 0042, iter [02300, 05004], lr: 0.073327, loss: 1.6484
2022-06-30 04:48:05 - train: epoch 0042, iter [02400, 05004], lr: 0.073304, loss: 2.0165
2022-06-30 04:48:39 - train: epoch 0042, iter [02500, 05004], lr: 0.073281, loss: 1.8378
2022-06-30 04:49:13 - train: epoch 0042, iter [02600, 05004], lr: 0.073258, loss: 1.7809
2022-06-30 04:49:46 - train: epoch 0042, iter [02700, 05004], lr: 0.073235, loss: 1.6298
2022-06-30 04:50:20 - train: epoch 0042, iter [02800, 05004], lr: 0.073212, loss: 1.7288
2022-06-30 04:50:55 - train: epoch 0042, iter [02900, 05004], lr: 0.073189, loss: 1.8586
2022-06-30 04:51:28 - train: epoch 0042, iter [03000, 05004], lr: 0.073165, loss: 1.7704
2022-06-30 04:52:02 - train: epoch 0042, iter [03100, 05004], lr: 0.073142, loss: 1.7668
2022-06-30 04:52:36 - train: epoch 0042, iter [03200, 05004], lr: 0.073119, loss: 1.7978
2022-06-30 04:53:09 - train: epoch 0042, iter [03300, 05004], lr: 0.073096, loss: 2.0545
2022-06-30 04:53:43 - train: epoch 0042, iter [03400, 05004], lr: 0.073073, loss: 1.6007
2022-06-30 04:54:17 - train: epoch 0042, iter [03500, 05004], lr: 0.073049, loss: 1.7747
2022-06-30 04:54:51 - train: epoch 0042, iter [03600, 05004], lr: 0.073026, loss: 1.8911
2022-06-30 04:55:25 - train: epoch 0042, iter [03700, 05004], lr: 0.073003, loss: 1.7995
2022-06-30 04:55:59 - train: epoch 0042, iter [03800, 05004], lr: 0.072980, loss: 1.6069
2022-06-30 04:56:33 - train: epoch 0042, iter [03900, 05004], lr: 0.072956, loss: 1.7124
2022-06-30 04:57:07 - train: epoch 0042, iter [04000, 05004], lr: 0.072933, loss: 1.8093
2022-06-30 04:57:41 - train: epoch 0042, iter [04100, 05004], lr: 0.072910, loss: 1.9994
2022-06-30 04:58:15 - train: epoch 0042, iter [04200, 05004], lr: 0.072887, loss: 1.9264
2022-06-30 04:58:49 - train: epoch 0042, iter [04300, 05004], lr: 0.072863, loss: 1.6260
2022-06-30 04:59:23 - train: epoch 0042, iter [04400, 05004], lr: 0.072840, loss: 1.6880
2022-06-30 04:59:58 - train: epoch 0042, iter [04500, 05004], lr: 0.072817, loss: 1.7705
2022-06-30 05:00:31 - train: epoch 0042, iter [04600, 05004], lr: 0.072794, loss: 2.1129
2022-06-30 05:01:05 - train: epoch 0042, iter [04700, 05004], lr: 0.072770, loss: 1.6208
2022-06-30 05:01:39 - train: epoch 0042, iter [04800, 05004], lr: 0.072747, loss: 1.8337
2022-06-30 05:02:13 - train: epoch 0042, iter [04900, 05004], lr: 0.072724, loss: 1.9273
2022-06-30 05:02:46 - train: epoch 0042, iter [05000, 05004], lr: 0.072700, loss: 1.6291
2022-06-30 05:02:47 - train: epoch 042, train_loss: 1.7642
2022-06-30 05:04:04 - eval: epoch: 042, acc1: 64.036%, acc5: 86.104%, test_loss: 1.4935, per_image_load_time: 2.339ms, per_image_inference_time: 0.623ms
2022-06-30 05:04:05 - until epoch: 042, best_acc1: 64.036%
2022-06-30 05:04:05 - epoch 043 lr: 0.072699
2022-06-30 05:04:44 - train: epoch 0043, iter [00100, 05004], lr: 0.072676, loss: 1.7301
2022-06-30 05:05:17 - train: epoch 0043, iter [00200, 05004], lr: 0.072653, loss: 1.8888
2022-06-30 05:05:50 - train: epoch 0043, iter [00300, 05004], lr: 0.072630, loss: 1.4226
2022-06-30 05:06:23 - train: epoch 0043, iter [00400, 05004], lr: 0.072606, loss: 1.6244
2022-06-30 05:06:56 - train: epoch 0043, iter [00500, 05004], lr: 0.072583, loss: 1.7626
2022-06-30 05:07:29 - train: epoch 0043, iter [00600, 05004], lr: 0.072560, loss: 1.6469
2022-06-30 05:08:02 - train: epoch 0043, iter [00700, 05004], lr: 0.072536, loss: 1.9343
2022-06-30 05:08:36 - train: epoch 0043, iter [00800, 05004], lr: 0.072513, loss: 1.9936
2022-06-30 05:09:10 - train: epoch 0043, iter [00900, 05004], lr: 0.072490, loss: 1.7183
2022-06-30 05:09:43 - train: epoch 0043, iter [01000, 05004], lr: 0.072466, loss: 1.8981
2022-06-30 05:10:17 - train: epoch 0043, iter [01100, 05004], lr: 0.072443, loss: 1.7768
2022-06-30 05:10:51 - train: epoch 0043, iter [01200, 05004], lr: 0.072419, loss: 1.7427
2022-06-30 05:11:25 - train: epoch 0043, iter [01300, 05004], lr: 0.072396, loss: 1.8360
2022-06-30 05:11:58 - train: epoch 0043, iter [01400, 05004], lr: 0.072373, loss: 1.6532
2022-06-30 05:12:32 - train: epoch 0043, iter [01500, 05004], lr: 0.072349, loss: 1.5860
2022-06-30 05:13:06 - train: epoch 0043, iter [01600, 05004], lr: 0.072326, loss: 1.8079
2022-06-30 05:13:40 - train: epoch 0043, iter [01700, 05004], lr: 0.072302, loss: 1.7784
2022-06-30 05:14:13 - train: epoch 0043, iter [01800, 05004], lr: 0.072279, loss: 1.7872
2022-06-30 05:14:47 - train: epoch 0043, iter [01900, 05004], lr: 0.072256, loss: 1.7263
2022-06-30 05:15:20 - train: epoch 0043, iter [02000, 05004], lr: 0.072232, loss: 1.5747
2022-06-30 05:15:54 - train: epoch 0043, iter [02100, 05004], lr: 0.072209, loss: 1.6556
2022-06-30 05:16:27 - train: epoch 0043, iter [02200, 05004], lr: 0.072185, loss: 1.8449
2022-06-30 05:17:00 - train: epoch 0043, iter [02300, 05004], lr: 0.072162, loss: 1.8653
2022-06-30 05:17:34 - train: epoch 0043, iter [02400, 05004], lr: 0.072138, loss: 1.6320
2022-06-30 05:18:07 - train: epoch 0043, iter [02500, 05004], lr: 0.072115, loss: 1.9029
2022-06-30 05:18:41 - train: epoch 0043, iter [02600, 05004], lr: 0.072091, loss: 1.3738
2022-06-30 05:19:15 - train: epoch 0043, iter [02700, 05004], lr: 0.072068, loss: 1.8512
2022-06-30 05:19:48 - train: epoch 0043, iter [02800, 05004], lr: 0.072044, loss: 1.6765
2022-06-30 05:20:22 - train: epoch 0043, iter [02900, 05004], lr: 0.072021, loss: 1.8012
2022-06-30 05:20:56 - train: epoch 0043, iter [03000, 05004], lr: 0.071998, loss: 1.9838
2022-06-30 05:21:30 - train: epoch 0043, iter [03100, 05004], lr: 0.071974, loss: 1.8952
2022-06-30 05:22:04 - train: epoch 0043, iter [03200, 05004], lr: 0.071951, loss: 1.9573
2022-06-30 05:22:37 - train: epoch 0043, iter [03300, 05004], lr: 0.071927, loss: 1.8339
2022-06-30 05:23:11 - train: epoch 0043, iter [03400, 05004], lr: 0.071904, loss: 1.7862
2022-06-30 05:23:44 - train: epoch 0043, iter [03500, 05004], lr: 0.071880, loss: 1.7437
2022-06-30 05:24:18 - train: epoch 0043, iter [03600, 05004], lr: 0.071856, loss: 1.7585
2022-06-30 05:24:51 - train: epoch 0043, iter [03700, 05004], lr: 0.071833, loss: 1.6513
2022-06-30 05:25:25 - train: epoch 0043, iter [03800, 05004], lr: 0.071809, loss: 1.8830
2022-06-30 05:25:59 - train: epoch 0043, iter [03900, 05004], lr: 0.071786, loss: 1.9823
2022-06-30 05:26:33 - train: epoch 0043, iter [04000, 05004], lr: 0.071762, loss: 1.7914
2022-06-30 05:27:06 - train: epoch 0043, iter [04100, 05004], lr: 0.071739, loss: 2.0523
2022-06-30 05:27:40 - train: epoch 0043, iter [04200, 05004], lr: 0.071715, loss: 1.8514
2022-06-30 05:28:14 - train: epoch 0043, iter [04300, 05004], lr: 0.071692, loss: 1.8047
2022-06-30 05:28:47 - train: epoch 0043, iter [04400, 05004], lr: 0.071668, loss: 1.7080
2022-06-30 05:29:21 - train: epoch 0043, iter [04500, 05004], lr: 0.071644, loss: 1.7179
2022-06-30 05:29:55 - train: epoch 0043, iter [04600, 05004], lr: 0.071621, loss: 1.8271
2022-06-30 05:30:29 - train: epoch 0043, iter [04700, 05004], lr: 0.071597, loss: 1.6839
2022-06-30 05:31:03 - train: epoch 0043, iter [04800, 05004], lr: 0.071574, loss: 1.7465
2022-06-30 05:31:36 - train: epoch 0043, iter [04900, 05004], lr: 0.071550, loss: 1.6624
2022-06-30 05:32:10 - train: epoch 0043, iter [05000, 05004], lr: 0.071526, loss: 1.6693
2022-06-30 05:32:12 - train: epoch 043, train_loss: 1.7546
2022-06-30 05:33:28 - eval: epoch: 043, acc1: 63.860%, acc5: 86.216%, test_loss: 1.5040, per_image_load_time: 2.379ms, per_image_inference_time: 0.609ms
2022-06-30 05:33:29 - until epoch: 043, best_acc1: 64.036%
2022-06-30 05:33:29 - epoch 044 lr: 0.071525
2022-06-30 05:34:08 - train: epoch 0044, iter [00100, 05004], lr: 0.071502, loss: 1.8180
2022-06-30 05:34:42 - train: epoch 0044, iter [00200, 05004], lr: 0.071478, loss: 1.8392
2022-06-30 05:35:16 - train: epoch 0044, iter [00300, 05004], lr: 0.071455, loss: 1.7699
2022-06-30 05:35:50 - train: epoch 0044, iter [00400, 05004], lr: 0.071431, loss: 1.3931
2022-06-30 05:36:24 - train: epoch 0044, iter [00500, 05004], lr: 0.071407, loss: 1.6924
2022-06-30 05:36:58 - train: epoch 0044, iter [00600, 05004], lr: 0.071384, loss: 1.4939
2022-06-30 05:37:32 - train: epoch 0044, iter [00700, 05004], lr: 0.071360, loss: 1.5228
2022-06-30 05:38:06 - train: epoch 0044, iter [00800, 05004], lr: 0.071336, loss: 1.5179
2022-06-30 05:38:39 - train: epoch 0044, iter [00900, 05004], lr: 0.071313, loss: 1.7158
2022-06-30 05:39:13 - train: epoch 0044, iter [01000, 05004], lr: 0.071289, loss: 1.6279
2022-06-30 05:39:47 - train: epoch 0044, iter [01100, 05004], lr: 0.071265, loss: 1.6620
2022-06-30 05:40:22 - train: epoch 0044, iter [01200, 05004], lr: 0.071242, loss: 1.7085
2022-06-30 05:40:56 - train: epoch 0044, iter [01300, 05004], lr: 0.071218, loss: 1.8190
2022-06-30 05:41:30 - train: epoch 0044, iter [01400, 05004], lr: 0.071194, loss: 1.6560
2022-06-30 05:42:04 - train: epoch 0044, iter [01500, 05004], lr: 0.071171, loss: 1.6318
2022-06-30 05:42:38 - train: epoch 0044, iter [01600, 05004], lr: 0.071147, loss: 1.7032
2022-06-30 05:43:12 - train: epoch 0044, iter [01700, 05004], lr: 0.071123, loss: 1.6776
2022-06-30 05:43:46 - train: epoch 0044, iter [01800, 05004], lr: 0.071100, loss: 1.7480
2022-06-30 05:44:19 - train: epoch 0044, iter [01900, 05004], lr: 0.071076, loss: 1.8010
2022-06-30 05:44:53 - train: epoch 0044, iter [02000, 05004], lr: 0.071052, loss: 1.6546
2022-06-30 05:45:27 - train: epoch 0044, iter [02100, 05004], lr: 0.071028, loss: 1.8688
2022-06-30 05:46:01 - train: epoch 0044, iter [02200, 05004], lr: 0.071005, loss: 1.7601
2022-06-30 05:46:34 - train: epoch 0044, iter [02300, 05004], lr: 0.070981, loss: 1.8699
2022-06-30 05:47:08 - train: epoch 0044, iter [02400, 05004], lr: 0.070957, loss: 1.7201
2022-06-30 05:47:42 - train: epoch 0044, iter [02500, 05004], lr: 0.070933, loss: 1.8664
2022-06-30 05:48:16 - train: epoch 0044, iter [02600, 05004], lr: 0.070910, loss: 1.6740
2022-06-30 05:48:50 - train: epoch 0044, iter [02700, 05004], lr: 0.070886, loss: 1.7891
2022-06-30 05:49:23 - train: epoch 0044, iter [02800, 05004], lr: 0.070862, loss: 1.5312
2022-06-30 05:49:57 - train: epoch 0044, iter [02900, 05004], lr: 0.070838, loss: 1.6248
2022-06-30 05:50:31 - train: epoch 0044, iter [03000, 05004], lr: 0.070815, loss: 1.6461
2022-06-30 05:51:05 - train: epoch 0044, iter [03100, 05004], lr: 0.070791, loss: 1.7799
2022-06-30 05:51:38 - train: epoch 0044, iter [03200, 05004], lr: 0.070767, loss: 1.4237
2022-06-30 05:52:12 - train: epoch 0044, iter [03300, 05004], lr: 0.070743, loss: 1.6669
2022-06-30 05:52:46 - train: epoch 0044, iter [03400, 05004], lr: 0.070719, loss: 2.0129
2022-06-30 05:53:20 - train: epoch 0044, iter [03500, 05004], lr: 0.070696, loss: 1.6797
2022-06-30 05:53:53 - train: epoch 0044, iter [03600, 05004], lr: 0.070672, loss: 1.9775
2022-06-30 05:54:27 - train: epoch 0044, iter [03700, 05004], lr: 0.070648, loss: 1.8501
2022-06-30 05:55:00 - train: epoch 0044, iter [03800, 05004], lr: 0.070624, loss: 1.4526
2022-06-30 05:55:34 - train: epoch 0044, iter [03900, 05004], lr: 0.070600, loss: 1.8163
2022-06-30 05:56:07 - train: epoch 0044, iter [04000, 05004], lr: 0.070576, loss: 1.7966
2022-06-30 05:56:41 - train: epoch 0044, iter [04100, 05004], lr: 0.070553, loss: 1.6711
2022-06-30 05:57:15 - train: epoch 0044, iter [04200, 05004], lr: 0.070529, loss: 1.8201
2022-06-30 05:57:49 - train: epoch 0044, iter [04300, 05004], lr: 0.070505, loss: 1.9149
2022-06-30 05:58:22 - train: epoch 0044, iter [04400, 05004], lr: 0.070481, loss: 1.4764
2022-06-30 05:58:56 - train: epoch 0044, iter [04500, 05004], lr: 0.070457, loss: 1.8425
2022-06-30 05:59:30 - train: epoch 0044, iter [04600, 05004], lr: 0.070433, loss: 1.8744
2022-06-30 06:00:03 - train: epoch 0044, iter [04700, 05004], lr: 0.070409, loss: 1.8349
2022-06-30 06:00:36 - train: epoch 0044, iter [04800, 05004], lr: 0.070386, loss: 1.9773
2022-06-30 06:01:10 - train: epoch 0044, iter [04900, 05004], lr: 0.070362, loss: 1.5902
2022-06-30 06:01:44 - train: epoch 0044, iter [05000, 05004], lr: 0.070338, loss: 1.8686
2022-06-30 06:01:45 - train: epoch 044, train_loss: 1.7451
2022-06-30 06:03:02 - eval: epoch: 044, acc1: 63.556%, acc5: 85.856%, test_loss: 1.5243, per_image_load_time: 2.351ms, per_image_inference_time: 0.610ms
2022-06-30 06:03:02 - until epoch: 044, best_acc1: 64.036%
2022-06-30 06:03:02 - epoch 045 lr: 0.070337
2022-06-30 06:03:42 - train: epoch 0045, iter [00100, 05004], lr: 0.070313, loss: 1.6274
2022-06-30 06:04:15 - train: epoch 0045, iter [00200, 05004], lr: 0.070289, loss: 1.7295
2022-06-30 06:04:49 - train: epoch 0045, iter [00300, 05004], lr: 0.070265, loss: 1.8603
2022-06-30 06:05:22 - train: epoch 0045, iter [00400, 05004], lr: 0.070241, loss: 1.5545
2022-06-30 06:05:55 - train: epoch 0045, iter [00500, 05004], lr: 0.070217, loss: 1.8872
2022-06-30 06:06:28 - train: epoch 0045, iter [00600, 05004], lr: 0.070193, loss: 1.7593
2022-06-30 06:07:02 - train: epoch 0045, iter [00700, 05004], lr: 0.070169, loss: 1.3547
2022-06-30 06:07:35 - train: epoch 0045, iter [00800, 05004], lr: 0.070145, loss: 1.6750
2022-06-30 06:08:09 - train: epoch 0045, iter [00900, 05004], lr: 0.070122, loss: 1.8241
2022-06-30 06:08:43 - train: epoch 0045, iter [01000, 05004], lr: 0.070098, loss: 1.7469
2022-06-30 06:09:17 - train: epoch 0045, iter [01100, 05004], lr: 0.070074, loss: 1.8373
2022-06-30 06:09:50 - train: epoch 0045, iter [01200, 05004], lr: 0.070050, loss: 1.7608
2022-06-30 06:10:24 - train: epoch 0045, iter [01300, 05004], lr: 0.070026, loss: 1.7342
2022-06-30 06:10:58 - train: epoch 0045, iter [01400, 05004], lr: 0.070002, loss: 1.6795
2022-06-30 06:11:32 - train: epoch 0045, iter [01500, 05004], lr: 0.069978, loss: 1.7815
2022-06-30 06:12:06 - train: epoch 0045, iter [01600, 05004], lr: 0.069954, loss: 1.7307
2022-06-30 06:12:39 - train: epoch 0045, iter [01700, 05004], lr: 0.069930, loss: 1.5524
2022-06-30 06:13:13 - train: epoch 0045, iter [01800, 05004], lr: 0.069906, loss: 1.6020
2022-06-30 06:13:47 - train: epoch 0045, iter [01900, 05004], lr: 0.069882, loss: 1.8730
2022-06-30 06:14:21 - train: epoch 0045, iter [02000, 05004], lr: 0.069858, loss: 1.7844
2022-06-30 06:14:55 - train: epoch 0045, iter [02100, 05004], lr: 0.069834, loss: 1.9075
2022-06-30 06:15:29 - train: epoch 0045, iter [02200, 05004], lr: 0.069810, loss: 1.7915
2022-06-30 06:16:02 - train: epoch 0045, iter [02300, 05004], lr: 0.069786, loss: 1.6148
2022-06-30 06:16:36 - train: epoch 0045, iter [02400, 05004], lr: 0.069762, loss: 1.9076
2022-06-30 06:17:10 - train: epoch 0045, iter [02500, 05004], lr: 0.069738, loss: 1.7616
2022-06-30 06:17:44 - train: epoch 0045, iter [02600, 05004], lr: 0.069714, loss: 1.6885
2022-06-30 06:18:18 - train: epoch 0045, iter [02700, 05004], lr: 0.069690, loss: 1.6003
2022-06-30 06:18:51 - train: epoch 0045, iter [02800, 05004], lr: 0.069666, loss: 1.6585
2022-06-30 06:19:25 - train: epoch 0045, iter [02900, 05004], lr: 0.069641, loss: 1.9068
2022-06-30 06:19:59 - train: epoch 0045, iter [03000, 05004], lr: 0.069617, loss: 1.9625
2022-06-30 06:20:33 - train: epoch 0045, iter [03100, 05004], lr: 0.069593, loss: 1.7904
2022-06-30 06:21:06 - train: epoch 0045, iter [03200, 05004], lr: 0.069569, loss: 1.7676
2022-06-30 06:21:40 - train: epoch 0045, iter [03300, 05004], lr: 0.069545, loss: 1.8589
2022-06-30 06:22:14 - train: epoch 0045, iter [03400, 05004], lr: 0.069521, loss: 1.4679
2022-06-30 06:22:48 - train: epoch 0045, iter [03500, 05004], lr: 0.069497, loss: 1.9302
2022-06-30 06:23:22 - train: epoch 0045, iter [03600, 05004], lr: 0.069473, loss: 1.8238
2022-06-30 06:23:56 - train: epoch 0045, iter [03700, 05004], lr: 0.069449, loss: 1.7079
2022-06-30 06:24:29 - train: epoch 0045, iter [03800, 05004], lr: 0.069425, loss: 1.5252
2022-06-30 06:25:03 - train: epoch 0045, iter [03900, 05004], lr: 0.069401, loss: 1.8636
2022-06-30 06:25:37 - train: epoch 0045, iter [04000, 05004], lr: 0.069377, loss: 1.9929
2022-06-30 06:26:11 - train: epoch 0045, iter [04100, 05004], lr: 0.069352, loss: 1.6592
2022-06-30 06:26:44 - train: epoch 0045, iter [04200, 05004], lr: 0.069328, loss: 1.9228
2022-06-30 06:27:18 - train: epoch 0045, iter [04300, 05004], lr: 0.069304, loss: 2.0206
2022-06-30 06:27:52 - train: epoch 0045, iter [04400, 05004], lr: 0.069280, loss: 1.8776
2022-06-30 06:28:26 - train: epoch 0045, iter [04500, 05004], lr: 0.069256, loss: 1.6062
2022-06-30 06:28:59 - train: epoch 0045, iter [04600, 05004], lr: 0.069232, loss: 1.9619
2022-06-30 06:29:33 - train: epoch 0045, iter [04700, 05004], lr: 0.069208, loss: 1.7741
2022-06-30 06:30:07 - train: epoch 0045, iter [04800, 05004], lr: 0.069183, loss: 1.6820
2022-06-30 06:30:41 - train: epoch 0045, iter [04900, 05004], lr: 0.069159, loss: 1.6664
2022-06-30 06:31:14 - train: epoch 0045, iter [05000, 05004], lr: 0.069135, loss: 1.6358
2022-06-30 06:31:16 - train: epoch 045, train_loss: 1.7346
2022-06-30 06:32:32 - eval: epoch: 045, acc1: 64.556%, acc5: 86.380%, test_loss: 1.4726, per_image_load_time: 2.381ms, per_image_inference_time: 0.596ms
2022-06-30 06:32:33 - until epoch: 045, best_acc1: 64.556%
2022-06-30 06:32:33 - epoch 046 lr: 0.069134
2022-06-30 06:33:12 - train: epoch 0046, iter [00100, 05004], lr: 0.069110, loss: 1.4374
2022-06-30 06:33:46 - train: epoch 0046, iter [00200, 05004], lr: 0.069086, loss: 1.6379
2022-06-30 06:34:19 - train: epoch 0046, iter [00300, 05004], lr: 0.069062, loss: 1.7385
2022-06-30 06:34:53 - train: epoch 0046, iter [00400, 05004], lr: 0.069037, loss: 1.8644
2022-06-30 06:35:26 - train: epoch 0046, iter [00500, 05004], lr: 0.069013, loss: 1.4561
2022-06-30 06:36:00 - train: epoch 0046, iter [00600, 05004], lr: 0.068989, loss: 1.7326
2022-06-30 06:36:33 - train: epoch 0046, iter [00700, 05004], lr: 0.068965, loss: 1.6172
2022-06-30 06:37:06 - train: epoch 0046, iter [00800, 05004], lr: 0.068941, loss: 2.0087
2022-06-30 06:37:39 - train: epoch 0046, iter [00900, 05004], lr: 0.068916, loss: 1.7650
2022-06-30 06:38:13 - train: epoch 0046, iter [01000, 05004], lr: 0.068892, loss: 1.5090
2022-06-30 06:38:46 - train: epoch 0046, iter [01100, 05004], lr: 0.068868, loss: 1.7797
2022-06-30 06:39:19 - train: epoch 0046, iter [01200, 05004], lr: 0.068844, loss: 1.6299
2022-06-30 06:39:53 - train: epoch 0046, iter [01300, 05004], lr: 0.068820, loss: 1.7089
2022-06-30 06:40:26 - train: epoch 0046, iter [01400, 05004], lr: 0.068795, loss: 2.0145
2022-06-30 06:41:00 - train: epoch 0046, iter [01500, 05004], lr: 0.068771, loss: 1.6713
2022-06-30 06:41:33 - train: epoch 0046, iter [01600, 05004], lr: 0.068747, loss: 1.7935
2022-06-30 06:42:06 - train: epoch 0046, iter [01700, 05004], lr: 0.068723, loss: 1.6068
2022-06-30 06:42:40 - train: epoch 0046, iter [01800, 05004], lr: 0.068698, loss: 1.7380
2022-06-30 06:43:13 - train: epoch 0046, iter [01900, 05004], lr: 0.068674, loss: 1.5377
2022-06-30 06:43:47 - train: epoch 0046, iter [02000, 05004], lr: 0.068650, loss: 1.7148
2022-06-30 06:44:20 - train: epoch 0046, iter [02100, 05004], lr: 0.068626, loss: 1.8578
2022-06-30 06:44:54 - train: epoch 0046, iter [02200, 05004], lr: 0.068601, loss: 1.5541
2022-06-30 06:45:27 - train: epoch 0046, iter [02300, 05004], lr: 0.068577, loss: 1.9381
2022-06-30 06:46:01 - train: epoch 0046, iter [02400, 05004], lr: 0.068553, loss: 1.6194
2022-06-30 06:46:34 - train: epoch 0046, iter [02500, 05004], lr: 0.068528, loss: 1.8565
2022-06-30 06:47:08 - train: epoch 0046, iter [02600, 05004], lr: 0.068504, loss: 1.8962
2022-06-30 06:47:41 - train: epoch 0046, iter [02700, 05004], lr: 0.068480, loss: 1.6030
2022-06-30 06:48:14 - train: epoch 0046, iter [02800, 05004], lr: 0.068455, loss: 1.5353
2022-06-30 06:48:47 - train: epoch 0046, iter [02900, 05004], lr: 0.068431, loss: 1.7918
2022-06-30 06:49:21 - train: epoch 0046, iter [03000, 05004], lr: 0.068407, loss: 1.5239
2022-06-30 06:49:54 - train: epoch 0046, iter [03100, 05004], lr: 0.068382, loss: 1.5103
2022-06-30 06:50:27 - train: epoch 0046, iter [03200, 05004], lr: 0.068358, loss: 1.8816
2022-06-30 06:51:01 - train: epoch 0046, iter [03300, 05004], lr: 0.068334, loss: 1.7280
2022-06-30 06:51:35 - train: epoch 0046, iter [03400, 05004], lr: 0.068309, loss: 1.7671
2022-06-30 06:52:08 - train: epoch 0046, iter [03500, 05004], lr: 0.068285, loss: 1.7288
2022-06-30 06:52:42 - train: epoch 0046, iter [03600, 05004], lr: 0.068261, loss: 1.6243
2022-06-30 06:53:15 - train: epoch 0046, iter [03700, 05004], lr: 0.068236, loss: 1.5556
2022-06-30 06:53:48 - train: epoch 0046, iter [03800, 05004], lr: 0.068212, loss: 1.8843
2022-06-30 06:54:22 - train: epoch 0046, iter [03900, 05004], lr: 0.068188, loss: 1.7514
2022-06-30 06:54:55 - train: epoch 0046, iter [04000, 05004], lr: 0.068163, loss: 1.5895
2022-06-30 06:55:28 - train: epoch 0046, iter [04100, 05004], lr: 0.068139, loss: 2.0170
2022-06-30 06:56:02 - train: epoch 0046, iter [04200, 05004], lr: 0.068115, loss: 1.6613
2022-06-30 06:56:35 - train: epoch 0046, iter [04300, 05004], lr: 0.068090, loss: 1.9337
2022-06-30 06:57:09 - train: epoch 0046, iter [04400, 05004], lr: 0.068066, loss: 1.8327
2022-06-30 06:57:42 - train: epoch 0046, iter [04500, 05004], lr: 0.068041, loss: 1.6959
2022-06-30 06:58:16 - train: epoch 0046, iter [04600, 05004], lr: 0.068017, loss: 1.9156
2022-06-30 06:58:49 - train: epoch 0046, iter [04700, 05004], lr: 0.067993, loss: 1.5622
2022-06-30 06:59:23 - train: epoch 0046, iter [04800, 05004], lr: 0.067968, loss: 1.6688
2022-06-30 06:59:56 - train: epoch 0046, iter [04900, 05004], lr: 0.067944, loss: 1.7984
2022-06-30 07:00:29 - train: epoch 0046, iter [05000, 05004], lr: 0.067919, loss: 1.6323
2022-06-30 07:00:31 - train: epoch 046, train_loss: 1.7243
2022-06-30 07:01:50 - eval: epoch: 046, acc1: 64.210%, acc5: 86.206%, test_loss: 1.4830, per_image_load_time: 2.345ms, per_image_inference_time: 0.631ms
2022-06-30 07:01:50 - until epoch: 046, best_acc1: 64.556%
2022-06-30 07:01:50 - epoch 047 lr: 0.067918
2022-06-30 07:02:32 - train: epoch 0047, iter [00100, 05004], lr: 0.067894, loss: 1.5782
2022-06-30 07:03:05 - train: epoch 0047, iter [00200, 05004], lr: 0.067870, loss: 1.7982
2022-06-30 07:03:38 - train: epoch 0047, iter [00300, 05004], lr: 0.067845, loss: 1.5864
2022-06-30 07:04:11 - train: epoch 0047, iter [00400, 05004], lr: 0.067821, loss: 1.6274
2022-06-30 07:04:44 - train: epoch 0047, iter [00500, 05004], lr: 0.067796, loss: 1.9148
2022-06-30 07:05:17 - train: epoch 0047, iter [00600, 05004], lr: 0.067772, loss: 1.7209
2022-06-30 07:05:50 - train: epoch 0047, iter [00700, 05004], lr: 0.067747, loss: 1.8757
2022-06-30 07:06:23 - train: epoch 0047, iter [00800, 05004], lr: 0.067723, loss: 1.5531
2022-06-30 07:06:55 - train: epoch 0047, iter [00900, 05004], lr: 0.067698, loss: 1.9104
2022-06-30 07:07:28 - train: epoch 0047, iter [01000, 05004], lr: 0.067674, loss: 1.7159
2022-06-30 07:08:01 - train: epoch 0047, iter [01100, 05004], lr: 0.067649, loss: 1.7757
2022-06-30 07:08:33 - train: epoch 0047, iter [01200, 05004], lr: 0.067625, loss: 1.5816
2022-06-30 07:09:07 - train: epoch 0047, iter [01300, 05004], lr: 0.067601, loss: 1.7093
2022-06-30 07:09:40 - train: epoch 0047, iter [01400, 05004], lr: 0.067576, loss: 1.6543
2022-06-30 07:10:13 - train: epoch 0047, iter [01500, 05004], lr: 0.067552, loss: 1.5492
2022-06-30 07:10:46 - train: epoch 0047, iter [01600, 05004], lr: 0.067527, loss: 1.6061
2022-06-30 07:11:19 - train: epoch 0047, iter [01700, 05004], lr: 0.067503, loss: 1.6538
2022-06-30 07:11:52 - train: epoch 0047, iter [01800, 05004], lr: 0.067478, loss: 1.6850
2022-06-30 07:12:25 - train: epoch 0047, iter [01900, 05004], lr: 0.067454, loss: 1.5634
2022-06-30 07:12:58 - train: epoch 0047, iter [02000, 05004], lr: 0.067429, loss: 1.7108
2022-06-30 07:13:31 - train: epoch 0047, iter [02100, 05004], lr: 0.067404, loss: 1.9263
2022-06-30 07:14:04 - train: epoch 0047, iter [02200, 05004], lr: 0.067380, loss: 1.8814
2022-06-30 07:14:37 - train: epoch 0047, iter [02300, 05004], lr: 0.067355, loss: 1.7243
2022-06-30 07:15:10 - train: epoch 0047, iter [02400, 05004], lr: 0.067331, loss: 1.5810
2022-06-30 07:15:43 - train: epoch 0047, iter [02500, 05004], lr: 0.067306, loss: 1.7641
2022-06-30 07:16:16 - train: epoch 0047, iter [02600, 05004], lr: 0.067282, loss: 1.9897
2022-06-30 07:16:49 - train: epoch 0047, iter [02700, 05004], lr: 0.067257, loss: 1.7707
2022-06-30 07:17:22 - train: epoch 0047, iter [02800, 05004], lr: 0.067233, loss: 1.9198
2022-06-30 07:17:56 - train: epoch 0047, iter [02900, 05004], lr: 0.067208, loss: 1.7361
2022-06-30 07:18:29 - train: epoch 0047, iter [03000, 05004], lr: 0.067184, loss: 1.8497
2022-06-30 07:19:02 - train: epoch 0047, iter [03100, 05004], lr: 0.067159, loss: 1.7657
2022-06-30 07:19:35 - train: epoch 0047, iter [03200, 05004], lr: 0.067134, loss: 1.9790
2022-06-30 07:20:07 - train: epoch 0047, iter [03300, 05004], lr: 0.067110, loss: 1.6575
2022-06-30 07:20:40 - train: epoch 0047, iter [03400, 05004], lr: 0.067085, loss: 1.6931
2022-06-30 07:21:13 - train: epoch 0047, iter [03500, 05004], lr: 0.067061, loss: 1.9292
2022-06-30 07:21:46 - train: epoch 0047, iter [03600, 05004], lr: 0.067036, loss: 1.7696
2022-06-30 07:22:19 - train: epoch 0047, iter [03700, 05004], lr: 0.067011, loss: 1.8095
2022-06-30 07:22:52 - train: epoch 0047, iter [03800, 05004], lr: 0.066987, loss: 1.7036
2022-06-30 07:23:26 - train: epoch 0047, iter [03900, 05004], lr: 0.066962, loss: 1.5540
2022-06-30 07:23:59 - train: epoch 0047, iter [04000, 05004], lr: 0.066938, loss: 1.7993
2022-06-30 07:24:32 - train: epoch 0047, iter [04100, 05004], lr: 0.066913, loss: 1.8102
2022-06-30 07:25:05 - train: epoch 0047, iter [04200, 05004], lr: 0.066888, loss: 1.7163
2022-06-30 07:25:38 - train: epoch 0047, iter [04300, 05004], lr: 0.066864, loss: 1.6406
2022-06-30 07:26:12 - train: epoch 0047, iter [04400, 05004], lr: 0.066839, loss: 1.6657
2022-06-30 07:26:45 - train: epoch 0047, iter [04500, 05004], lr: 0.066815, loss: 1.8655
2022-06-30 07:27:18 - train: epoch 0047, iter [04600, 05004], lr: 0.066790, loss: 1.7019
2022-06-30 07:27:51 - train: epoch 0047, iter [04700, 05004], lr: 0.066765, loss: 1.8203
2022-06-30 07:28:24 - train: epoch 0047, iter [04800, 05004], lr: 0.066741, loss: 1.6194
2022-06-30 07:28:57 - train: epoch 0047, iter [04900, 05004], lr: 0.066716, loss: 1.8172
2022-06-30 07:29:30 - train: epoch 0047, iter [05000, 05004], lr: 0.066691, loss: 1.6623
2022-06-30 07:29:31 - train: epoch 047, train_loss: 1.7175
2022-06-30 07:30:49 - eval: epoch: 047, acc1: 64.232%, acc5: 86.286%, test_loss: 1.4868, per_image_load_time: 1.948ms, per_image_inference_time: 0.620ms
2022-06-30 07:30:50 - until epoch: 047, best_acc1: 64.556%
2022-06-30 07:30:50 - epoch 048 lr: 0.066690
2022-06-30 07:31:31 - train: epoch 0048, iter [00100, 05004], lr: 0.066666, loss: 1.8060
2022-06-30 07:32:04 - train: epoch 0048, iter [00200, 05004], lr: 0.066641, loss: 2.0395
2022-06-30 07:32:36 - train: epoch 0048, iter [00300, 05004], lr: 0.066616, loss: 1.6457
2022-06-30 07:33:09 - train: epoch 0048, iter [00400, 05004], lr: 0.066592, loss: 1.6135
2022-06-30 07:33:42 - train: epoch 0048, iter [00500, 05004], lr: 0.066567, loss: 1.5174
2022-06-30 07:34:15 - train: epoch 0048, iter [00600, 05004], lr: 0.066542, loss: 1.7622
2022-06-30 07:34:48 - train: epoch 0048, iter [00700, 05004], lr: 0.066518, loss: 1.7416
2022-06-30 07:35:22 - train: epoch 0048, iter [00800, 05004], lr: 0.066493, loss: 1.7999
2022-06-30 07:35:55 - train: epoch 0048, iter [00900, 05004], lr: 0.066468, loss: 1.9502
2022-06-30 07:36:27 - train: epoch 0048, iter [01000, 05004], lr: 0.066444, loss: 1.7019
2022-06-30 07:37:00 - train: epoch 0048, iter [01100, 05004], lr: 0.066419, loss: 1.7753
2022-06-30 07:37:33 - train: epoch 0048, iter [01200, 05004], lr: 0.066394, loss: 1.6976
2022-06-30 07:38:06 - train: epoch 0048, iter [01300, 05004], lr: 0.066369, loss: 1.5455
2022-06-30 07:38:39 - train: epoch 0048, iter [01400, 05004], lr: 0.066345, loss: 1.6674
2022-06-30 07:39:12 - train: epoch 0048, iter [01500, 05004], lr: 0.066320, loss: 1.7694
2022-06-30 07:39:45 - train: epoch 0048, iter [01600, 05004], lr: 0.066295, loss: 1.6493
2022-06-30 07:40:18 - train: epoch 0048, iter [01700, 05004], lr: 0.066270, loss: 1.8386
2022-06-30 07:40:51 - train: epoch 0048, iter [01800, 05004], lr: 0.066246, loss: 1.8955
2022-06-30 07:41:24 - train: epoch 0048, iter [01900, 05004], lr: 0.066221, loss: 1.7596
2022-06-30 07:41:57 - train: epoch 0048, iter [02000, 05004], lr: 0.066196, loss: 1.9776
2022-06-30 07:42:30 - train: epoch 0048, iter [02100, 05004], lr: 0.066172, loss: 1.6862
2022-06-30 07:43:03 - train: epoch 0048, iter [02200, 05004], lr: 0.066147, loss: 1.8998
2022-06-30 07:43:35 - train: epoch 0048, iter [02300, 05004], lr: 0.066122, loss: 1.5396
2022-06-30 07:44:08 - train: epoch 0048, iter [02400, 05004], lr: 0.066097, loss: 1.8071
2022-06-30 07:44:41 - train: epoch 0048, iter [02500, 05004], lr: 0.066072, loss: 1.7835
2022-06-30 07:45:15 - train: epoch 0048, iter [02600, 05004], lr: 0.066048, loss: 1.8287
2022-06-30 07:45:48 - train: epoch 0048, iter [02700, 05004], lr: 0.066023, loss: 1.9762
2022-06-30 07:46:20 - train: epoch 0048, iter [02800, 05004], lr: 0.065998, loss: 1.6648
2022-06-30 07:46:53 - train: epoch 0048, iter [02900, 05004], lr: 0.065973, loss: 1.8669
2022-06-30 07:47:26 - train: epoch 0048, iter [03000, 05004], lr: 0.065949, loss: 1.7828
2022-06-30 07:47:59 - train: epoch 0048, iter [03100, 05004], lr: 0.065924, loss: 1.6227
2022-06-30 07:48:32 - train: epoch 0048, iter [03200, 05004], lr: 0.065899, loss: 1.6319
2022-06-30 07:49:05 - train: epoch 0048, iter [03300, 05004], lr: 0.065874, loss: 1.9071
2022-06-30 07:49:38 - train: epoch 0048, iter [03400, 05004], lr: 0.065849, loss: 1.6548
2022-06-30 07:50:11 - train: epoch 0048, iter [03500, 05004], lr: 0.065825, loss: 2.0311
2022-06-30 07:50:44 - train: epoch 0048, iter [03600, 05004], lr: 0.065800, loss: 1.7005
2022-06-30 07:51:17 - train: epoch 0048, iter [03700, 05004], lr: 0.065775, loss: 1.7059
2022-06-30 07:51:51 - train: epoch 0048, iter [03800, 05004], lr: 0.065750, loss: 1.7255
2022-06-30 07:52:24 - train: epoch 0048, iter [03900, 05004], lr: 0.065725, loss: 1.8754
2022-06-30 07:52:57 - train: epoch 0048, iter [04000, 05004], lr: 0.065700, loss: 1.5722
2022-06-30 07:53:30 - train: epoch 0048, iter [04100, 05004], lr: 0.065676, loss: 1.9982
2022-06-30 07:54:03 - train: epoch 0048, iter [04200, 05004], lr: 0.065651, loss: 1.5770
2022-06-30 07:54:35 - train: epoch 0048, iter [04300, 05004], lr: 0.065626, loss: 1.6970
2022-06-30 07:55:08 - train: epoch 0048, iter [04400, 05004], lr: 0.065601, loss: 1.5204
2022-06-30 07:55:41 - train: epoch 0048, iter [04500, 05004], lr: 0.065576, loss: 1.6729
2022-06-30 07:56:14 - train: epoch 0048, iter [04600, 05004], lr: 0.065551, loss: 1.6606
2022-06-30 07:56:47 - train: epoch 0048, iter [04700, 05004], lr: 0.065526, loss: 1.7728
2022-06-30 07:57:20 - train: epoch 0048, iter [04800, 05004], lr: 0.065502, loss: 2.0637
2022-06-30 07:57:53 - train: epoch 0048, iter [04900, 05004], lr: 0.065477, loss: 1.7234
2022-06-30 07:58:26 - train: epoch 0048, iter [05000, 05004], lr: 0.065452, loss: 1.6479
2022-06-30 07:58:28 - train: epoch 048, train_loss: 1.7108
2022-06-30 07:59:44 - eval: epoch: 048, acc1: 64.182%, acc5: 86.338%, test_loss: 1.4788, per_image_load_time: 2.259ms, per_image_inference_time: 0.618ms
2022-06-30 07:59:45 - until epoch: 048, best_acc1: 64.556%
2022-06-30 07:59:45 - epoch 049 lr: 0.065451
2022-06-30 08:00:26 - train: epoch 0049, iter [00100, 05004], lr: 0.065426, loss: 1.8021
2022-06-30 08:00:59 - train: epoch 0049, iter [00200, 05004], lr: 0.065401, loss: 1.5802
2022-06-30 08:01:32 - train: epoch 0049, iter [00300, 05004], lr: 0.065376, loss: 1.7673
2022-06-30 08:02:05 - train: epoch 0049, iter [00400, 05004], lr: 0.065351, loss: 1.7519
2022-06-30 08:02:38 - train: epoch 0049, iter [00500, 05004], lr: 0.065326, loss: 1.6676
2022-06-30 08:03:11 - train: epoch 0049, iter [00600, 05004], lr: 0.065302, loss: 1.6281
2022-06-30 08:03:44 - train: epoch 0049, iter [00700, 05004], lr: 0.065277, loss: 1.7040
2022-06-30 08:04:18 - train: epoch 0049, iter [00800, 05004], lr: 0.065252, loss: 1.9873
2022-06-30 08:04:50 - train: epoch 0049, iter [00900, 05004], lr: 0.065227, loss: 1.6827
2022-06-30 08:05:23 - train: epoch 0049, iter [01000, 05004], lr: 0.065202, loss: 1.7172
2022-06-30 08:05:57 - train: epoch 0049, iter [01100, 05004], lr: 0.065177, loss: 1.5516
2022-06-30 08:06:30 - train: epoch 0049, iter [01200, 05004], lr: 0.065152, loss: 1.4157
2022-06-30 08:07:03 - train: epoch 0049, iter [01300, 05004], lr: 0.065127, loss: 1.8559
2022-06-30 08:07:36 - train: epoch 0049, iter [01400, 05004], lr: 0.065102, loss: 1.8097
2022-06-30 08:08:08 - train: epoch 0049, iter [01500, 05004], lr: 0.065077, loss: 1.5279
2022-06-30 08:08:41 - train: epoch 0049, iter [01600, 05004], lr: 0.065052, loss: 1.8025
2022-06-30 08:09:14 - train: epoch 0049, iter [01700, 05004], lr: 0.065027, loss: 1.8106
2022-06-30 08:09:47 - train: epoch 0049, iter [01800, 05004], lr: 0.065002, loss: 1.5588
2022-06-30 08:10:20 - train: epoch 0049, iter [01900, 05004], lr: 0.064977, loss: 1.5901
2022-06-30 08:10:53 - train: epoch 0049, iter [02000, 05004], lr: 0.064952, loss: 1.5403
2022-06-30 08:11:27 - train: epoch 0049, iter [02100, 05004], lr: 0.064927, loss: 1.6474
2022-06-30 08:12:00 - train: epoch 0049, iter [02200, 05004], lr: 0.064903, loss: 1.6258
2022-06-30 08:12:33 - train: epoch 0049, iter [02300, 05004], lr: 0.064878, loss: 1.6323
2022-06-30 08:13:06 - train: epoch 0049, iter [02400, 05004], lr: 0.064853, loss: 1.8495
2022-06-30 08:13:39 - train: epoch 0049, iter [02500, 05004], lr: 0.064828, loss: 1.7705
2022-06-30 08:14:12 - train: epoch 0049, iter [02600, 05004], lr: 0.064803, loss: 1.7793
2022-06-30 08:14:46 - train: epoch 0049, iter [02700, 05004], lr: 0.064778, loss: 1.5230
2022-06-30 08:15:19 - train: epoch 0049, iter [02800, 05004], lr: 0.064753, loss: 1.6033
2022-06-30 08:15:52 - train: epoch 0049, iter [02900, 05004], lr: 0.064728, loss: 1.8336
2022-06-30 08:16:25 - train: epoch 0049, iter [03000, 05004], lr: 0.064703, loss: 1.8453
2022-06-30 08:16:59 - train: epoch 0049, iter [03100, 05004], lr: 0.064678, loss: 1.7441
2022-06-30 08:17:33 - train: epoch 0049, iter [03200, 05004], lr: 0.064653, loss: 1.7226
2022-06-30 08:18:06 - train: epoch 0049, iter [03300, 05004], lr: 0.064628, loss: 1.6570
2022-06-30 08:18:39 - train: epoch 0049, iter [03400, 05004], lr: 0.064603, loss: 1.8478
2022-06-30 08:19:12 - train: epoch 0049, iter [03500, 05004], lr: 0.064578, loss: 1.8278
2022-06-30 08:19:45 - train: epoch 0049, iter [03600, 05004], lr: 0.064553, loss: 1.8276
2022-06-30 08:20:18 - train: epoch 0049, iter [03700, 05004], lr: 0.064528, loss: 1.6985
2022-06-30 08:20:52 - train: epoch 0049, iter [03800, 05004], lr: 0.064502, loss: 1.8364
2022-06-30 08:21:25 - train: epoch 0049, iter [03900, 05004], lr: 0.064477, loss: 2.0041
2022-06-30 08:21:58 - train: epoch 0049, iter [04000, 05004], lr: 0.064452, loss: 1.6294
2022-06-30 08:22:31 - train: epoch 0049, iter [04100, 05004], lr: 0.064427, loss: 1.5751
2022-06-30 08:23:04 - train: epoch 0049, iter [04200, 05004], lr: 0.064402, loss: 1.7264
2022-06-30 08:23:37 - train: epoch 0049, iter [04300, 05004], lr: 0.064377, loss: 1.9097
2022-06-30 08:24:10 - train: epoch 0049, iter [04400, 05004], lr: 0.064352, loss: 1.7887
2022-06-30 08:24:43 - train: epoch 0049, iter [04500, 05004], lr: 0.064327, loss: 1.5694
2022-06-30 08:25:16 - train: epoch 0049, iter [04600, 05004], lr: 0.064302, loss: 1.7944
2022-06-30 08:25:50 - train: epoch 0049, iter [04700, 05004], lr: 0.064277, loss: 1.9312
2022-06-30 08:26:23 - train: epoch 0049, iter [04800, 05004], lr: 0.064252, loss: 1.5317
2022-06-30 08:26:56 - train: epoch 0049, iter [04900, 05004], lr: 0.064227, loss: 1.5497
2022-06-30 08:27:29 - train: epoch 0049, iter [05000, 05004], lr: 0.064202, loss: 1.5853
2022-06-30 08:27:31 - train: epoch 049, train_loss: 1.6980
2022-06-30 08:28:49 - eval: epoch: 049, acc1: 64.812%, acc5: 86.608%, test_loss: 1.4634, per_image_load_time: 2.429ms, per_image_inference_time: 0.607ms
2022-06-30 08:28:50 - until epoch: 049, best_acc1: 64.812%
2022-06-30 08:28:50 - epoch 050 lr: 0.064201
2022-06-30 08:29:31 - train: epoch 0050, iter [00100, 05004], lr: 0.064176, loss: 1.7995
2022-06-30 08:30:04 - train: epoch 0050, iter [00200, 05004], lr: 0.064151, loss: 1.7035
2022-06-30 08:30:37 - train: epoch 0050, iter [00300, 05004], lr: 0.064126, loss: 1.6870
2022-06-30 08:31:10 - train: epoch 0050, iter [00400, 05004], lr: 0.064100, loss: 1.6091
2022-06-30 08:31:43 - train: epoch 0050, iter [00500, 05004], lr: 0.064075, loss: 1.6822
2022-06-30 08:32:17 - train: epoch 0050, iter [00600, 05004], lr: 0.064050, loss: 1.7509
2022-06-30 08:32:50 - train: epoch 0050, iter [00700, 05004], lr: 0.064025, loss: 1.5501
2022-06-30 08:33:24 - train: epoch 0050, iter [00800, 05004], lr: 0.064000, loss: 1.4418
2022-06-30 08:33:56 - train: epoch 0050, iter [00900, 05004], lr: 0.063975, loss: 1.5954
2022-06-30 08:34:30 - train: epoch 0050, iter [01000, 05004], lr: 0.063950, loss: 1.7991
2022-06-30 08:35:03 - train: epoch 0050, iter [01100, 05004], lr: 0.063925, loss: 1.7791
2022-06-30 08:35:36 - train: epoch 0050, iter [01200, 05004], lr: 0.063900, loss: 1.6358
2022-06-30 08:36:09 - train: epoch 0050, iter [01300, 05004], lr: 0.063874, loss: 1.5106
2022-06-30 08:36:43 - train: epoch 0050, iter [01400, 05004], lr: 0.063849, loss: 1.6919
2022-06-30 08:37:16 - train: epoch 0050, iter [01500, 05004], lr: 0.063824, loss: 1.6130
2022-06-30 08:37:49 - train: epoch 0050, iter [01600, 05004], lr: 0.063799, loss: 1.5947
2022-06-30 08:38:22 - train: epoch 0050, iter [01700, 05004], lr: 0.063774, loss: 1.7302
2022-06-30 08:38:55 - train: epoch 0050, iter [01800, 05004], lr: 0.063749, loss: 1.6508
2022-06-30 08:39:28 - train: epoch 0050, iter [01900, 05004], lr: 0.063724, loss: 1.6402
2022-06-30 08:40:01 - train: epoch 0050, iter [02000, 05004], lr: 0.063698, loss: 1.6997
2022-06-30 08:40:34 - train: epoch 0050, iter [02100, 05004], lr: 0.063673, loss: 1.3806
2022-06-30 08:41:07 - train: epoch 0050, iter [02200, 05004], lr: 0.063648, loss: 1.6977
2022-06-30 08:41:40 - train: epoch 0050, iter [02300, 05004], lr: 0.063623, loss: 1.6368
2022-06-30 08:42:13 - train: epoch 0050, iter [02400, 05004], lr: 0.063598, loss: 1.6504
2022-06-30 08:42:46 - train: epoch 0050, iter [02500, 05004], lr: 0.063573, loss: 1.7974
2022-06-30 08:43:18 - train: epoch 0050, iter [02600, 05004], lr: 0.063547, loss: 1.5339
2022-06-30 08:43:51 - train: epoch 0050, iter [02700, 05004], lr: 0.063522, loss: 1.7277
2022-06-30 08:44:24 - train: epoch 0050, iter [02800, 05004], lr: 0.063497, loss: 1.8572
