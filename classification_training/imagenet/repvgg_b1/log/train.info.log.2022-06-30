2022-06-30 08:44:57 - train: epoch 0050, iter [02900, 05004], lr: 0.063472, loss: 1.9195
2022-06-30 08:45:31 - train: epoch 0050, iter [03000, 05004], lr: 0.063447, loss: 1.8606
2022-06-30 08:46:04 - train: epoch 0050, iter [03100, 05004], lr: 0.063421, loss: 1.7840
2022-06-30 08:46:37 - train: epoch 0050, iter [03200, 05004], lr: 0.063396, loss: 1.6626
2022-06-30 08:47:10 - train: epoch 0050, iter [03300, 05004], lr: 0.063371, loss: 1.6092
2022-06-30 08:47:43 - train: epoch 0050, iter [03400, 05004], lr: 0.063346, loss: 1.5607
2022-06-30 08:48:16 - train: epoch 0050, iter [03500, 05004], lr: 0.063321, loss: 1.6992
2022-06-30 08:48:49 - train: epoch 0050, iter [03600, 05004], lr: 0.063295, loss: 1.8219
2022-06-30 08:49:22 - train: epoch 0050, iter [03700, 05004], lr: 0.063270, loss: 1.7079
2022-06-30 08:49:55 - train: epoch 0050, iter [03800, 05004], lr: 0.063245, loss: 1.5342
2022-06-30 08:50:28 - train: epoch 0050, iter [03900, 05004], lr: 0.063220, loss: 1.4989
2022-06-30 08:51:00 - train: epoch 0050, iter [04000, 05004], lr: 0.063194, loss: 1.7983
2022-06-30 08:51:33 - train: epoch 0050, iter [04100, 05004], lr: 0.063169, loss: 1.6236
2022-06-30 08:52:06 - train: epoch 0050, iter [04200, 05004], lr: 0.063144, loss: 1.7398
2022-06-30 08:52:39 - train: epoch 0050, iter [04300, 05004], lr: 0.063119, loss: 1.6808
2022-06-30 08:53:12 - train: epoch 0050, iter [04400, 05004], lr: 0.063094, loss: 1.5760
2022-06-30 08:53:45 - train: epoch 0050, iter [04500, 05004], lr: 0.063068, loss: 1.6818
2022-06-30 08:54:18 - train: epoch 0050, iter [04600, 05004], lr: 0.063043, loss: 1.6404
2022-06-30 08:54:52 - train: epoch 0050, iter [04700, 05004], lr: 0.063018, loss: 1.7977
2022-06-30 08:55:25 - train: epoch 0050, iter [04800, 05004], lr: 0.062992, loss: 1.4685
2022-06-30 08:55:58 - train: epoch 0050, iter [04900, 05004], lr: 0.062967, loss: 1.6043
2022-06-30 08:56:31 - train: epoch 0050, iter [05000, 05004], lr: 0.062942, loss: 1.6451
2022-06-30 08:56:33 - train: epoch 050, train_loss: 1.6871
2022-06-30 08:57:50 - eval: epoch: 050, acc1: 65.512%, acc5: 87.052%, test_loss: 1.4327, per_image_load_time: 1.606ms, per_image_inference_time: 0.654ms
2022-06-30 08:57:51 - until epoch: 050, best_acc1: 65.512%
2022-06-30 08:57:51 - epoch 051 lr: 0.062941
2022-06-30 08:58:32 - train: epoch 0051, iter [00100, 05004], lr: 0.062916, loss: 1.8822
2022-06-30 08:59:06 - train: epoch 0051, iter [00200, 05004], lr: 0.062890, loss: 1.9727
2022-06-30 08:59:39 - train: epoch 0051, iter [00300, 05004], lr: 0.062865, loss: 1.7730
2022-06-30 09:00:12 - train: epoch 0051, iter [00400, 05004], lr: 0.062840, loss: 1.5386
2022-06-30 09:00:45 - train: epoch 0051, iter [00500, 05004], lr: 0.062815, loss: 1.6952
2022-06-30 09:01:18 - train: epoch 0051, iter [00600, 05004], lr: 0.062789, loss: 1.6517
2022-06-30 09:01:51 - train: epoch 0051, iter [00700, 05004], lr: 0.062764, loss: 1.5948
2022-06-30 09:02:24 - train: epoch 0051, iter [00800, 05004], lr: 0.062739, loss: 1.9755
2022-06-30 09:02:57 - train: epoch 0051, iter [00900, 05004], lr: 0.062713, loss: 1.5385
2022-06-30 09:03:30 - train: epoch 0051, iter [01000, 05004], lr: 0.062688, loss: 2.0289
2022-06-30 09:04:03 - train: epoch 0051, iter [01100, 05004], lr: 0.062663, loss: 1.8881
2022-06-30 09:04:36 - train: epoch 0051, iter [01200, 05004], lr: 0.062637, loss: 1.6157
2022-06-30 09:05:09 - train: epoch 0051, iter [01300, 05004], lr: 0.062612, loss: 1.3148
2022-06-30 09:05:43 - train: epoch 0051, iter [01400, 05004], lr: 0.062587, loss: 1.5239
2022-06-30 09:06:16 - train: epoch 0051, iter [01500, 05004], lr: 0.062562, loss: 1.6151
2022-06-30 09:06:49 - train: epoch 0051, iter [01600, 05004], lr: 0.062536, loss: 1.4662
2022-06-30 09:07:23 - train: epoch 0051, iter [01700, 05004], lr: 0.062511, loss: 1.7656
2022-06-30 09:07:55 - train: epoch 0051, iter [01800, 05004], lr: 0.062486, loss: 1.6662
2022-06-30 09:08:28 - train: epoch 0051, iter [01900, 05004], lr: 0.062460, loss: 1.3482
2022-06-30 09:09:00 - train: epoch 0051, iter [02000, 05004], lr: 0.062435, loss: 1.6622
2022-06-30 09:09:33 - train: epoch 0051, iter [02100, 05004], lr: 0.062410, loss: 1.6941
2022-06-30 09:10:06 - train: epoch 0051, iter [02200, 05004], lr: 0.062384, loss: 1.7105
2022-06-30 09:10:38 - train: epoch 0051, iter [02300, 05004], lr: 0.062359, loss: 1.7000
2022-06-30 09:11:11 - train: epoch 0051, iter [02400, 05004], lr: 0.062334, loss: 1.8031
2022-06-30 09:11:44 - train: epoch 0051, iter [02500, 05004], lr: 0.062308, loss: 1.6567
2022-06-30 09:12:17 - train: epoch 0051, iter [02600, 05004], lr: 0.062283, loss: 1.5587
2022-06-30 09:12:50 - train: epoch 0051, iter [02700, 05004], lr: 0.062257, loss: 1.7437
2022-06-30 09:13:22 - train: epoch 0051, iter [02800, 05004], lr: 0.062232, loss: 1.4985
2022-06-30 09:13:55 - train: epoch 0051, iter [02900, 05004], lr: 0.062207, loss: 1.6620
2022-06-30 09:14:28 - train: epoch 0051, iter [03000, 05004], lr: 0.062181, loss: 1.6017
2022-06-30 09:15:01 - train: epoch 0051, iter [03100, 05004], lr: 0.062156, loss: 1.6239
2022-06-30 09:15:34 - train: epoch 0051, iter [03200, 05004], lr: 0.062131, loss: 1.6328
2022-06-30 09:16:07 - train: epoch 0051, iter [03300, 05004], lr: 0.062105, loss: 1.7858
2022-06-30 09:16:40 - train: epoch 0051, iter [03400, 05004], lr: 0.062080, loss: 1.7809
2022-06-30 09:17:12 - train: epoch 0051, iter [03500, 05004], lr: 0.062054, loss: 1.5650
2022-06-30 09:17:45 - train: epoch 0051, iter [03600, 05004], lr: 0.062029, loss: 1.5933
2022-06-30 09:18:18 - train: epoch 0051, iter [03700, 05004], lr: 0.062004, loss: 1.8152
2022-06-30 09:18:51 - train: epoch 0051, iter [03800, 05004], lr: 0.061978, loss: 1.6830
2022-06-30 09:19:24 - train: epoch 0051, iter [03900, 05004], lr: 0.061953, loss: 1.8245
2022-06-30 09:19:57 - train: epoch 0051, iter [04000, 05004], lr: 0.061927, loss: 1.7206
2022-06-30 09:20:30 - train: epoch 0051, iter [04100, 05004], lr: 0.061902, loss: 1.7403
2022-06-30 09:21:03 - train: epoch 0051, iter [04200, 05004], lr: 0.061877, loss: 1.9060
2022-06-30 09:21:36 - train: epoch 0051, iter [04300, 05004], lr: 0.061851, loss: 1.6222
2022-06-30 09:22:09 - train: epoch 0051, iter [04400, 05004], lr: 0.061826, loss: 1.6670
2022-06-30 09:22:42 - train: epoch 0051, iter [04500, 05004], lr: 0.061800, loss: 1.5696
2022-06-30 09:23:15 - train: epoch 0051, iter [04600, 05004], lr: 0.061775, loss: 1.7627
2022-06-30 09:23:48 - train: epoch 0051, iter [04700, 05004], lr: 0.061750, loss: 1.7106
2022-06-30 09:24:21 - train: epoch 0051, iter [04800, 05004], lr: 0.061724, loss: 1.6714
2022-06-30 09:24:53 - train: epoch 0051, iter [04900, 05004], lr: 0.061699, loss: 1.8361
2022-06-30 09:25:26 - train: epoch 0051, iter [05000, 05004], lr: 0.061673, loss: 1.5300
2022-06-30 09:25:27 - train: epoch 051, train_loss: 1.6787
2022-06-30 09:26:43 - eval: epoch: 051, acc1: 65.092%, acc5: 86.906%, test_loss: 1.4405, per_image_load_time: 2.157ms, per_image_inference_time: 0.670ms
2022-06-30 09:26:44 - until epoch: 051, best_acc1: 65.512%
2022-06-30 09:26:44 - epoch 052 lr: 0.061672
2022-06-30 09:27:24 - train: epoch 0052, iter [00100, 05004], lr: 0.061647, loss: 1.4974
2022-06-30 09:27:56 - train: epoch 0052, iter [00200, 05004], lr: 0.061621, loss: 1.5855
2022-06-30 09:28:28 - train: epoch 0052, iter [00300, 05004], lr: 0.061596, loss: 1.7553
2022-06-30 09:29:00 - train: epoch 0052, iter [00400, 05004], lr: 0.061570, loss: 1.6719
2022-06-30 09:29:33 - train: epoch 0052, iter [00500, 05004], lr: 0.061545, loss: 1.6524
2022-06-30 09:30:06 - train: epoch 0052, iter [00600, 05004], lr: 0.061520, loss: 1.5400
2022-06-30 09:30:38 - train: epoch 0052, iter [00700, 05004], lr: 0.061494, loss: 1.6946
2022-06-30 09:31:11 - train: epoch 0052, iter [00800, 05004], lr: 0.061469, loss: 1.6993
2022-06-30 09:31:44 - train: epoch 0052, iter [00900, 05004], lr: 0.061443, loss: 1.7232
2022-06-30 09:32:16 - train: epoch 0052, iter [01000, 05004], lr: 0.061418, loss: 2.0244
2022-06-30 09:32:49 - train: epoch 0052, iter [01100, 05004], lr: 0.061392, loss: 1.5082
2022-06-30 09:33:22 - train: epoch 0052, iter [01200, 05004], lr: 0.061367, loss: 1.5310
2022-06-30 09:33:55 - train: epoch 0052, iter [01300, 05004], lr: 0.061341, loss: 1.5523
2022-06-30 09:34:28 - train: epoch 0052, iter [01400, 05004], lr: 0.061316, loss: 1.9507
2022-06-30 09:35:01 - train: epoch 0052, iter [01500, 05004], lr: 0.061290, loss: 1.4853
2022-06-30 09:35:33 - train: epoch 0052, iter [01600, 05004], lr: 0.061265, loss: 1.4413
2022-06-30 09:36:07 - train: epoch 0052, iter [01700, 05004], lr: 0.061239, loss: 1.4844
2022-06-30 09:36:39 - train: epoch 0052, iter [01800, 05004], lr: 0.061214, loss: 1.4368
2022-06-30 09:37:12 - train: epoch 0052, iter [01900, 05004], lr: 0.061188, loss: 1.6046
2022-06-30 09:37:45 - train: epoch 0052, iter [02000, 05004], lr: 0.061163, loss: 1.7030
2022-06-30 09:38:18 - train: epoch 0052, iter [02100, 05004], lr: 0.061137, loss: 1.6158
2022-06-30 09:38:51 - train: epoch 0052, iter [02200, 05004], lr: 0.061112, loss: 1.7907
2022-06-30 09:39:24 - train: epoch 0052, iter [02300, 05004], lr: 0.061086, loss: 1.4321
2022-06-30 09:39:57 - train: epoch 0052, iter [02400, 05004], lr: 0.061061, loss: 1.5479
2022-06-30 09:40:29 - train: epoch 0052, iter [02500, 05004], lr: 0.061035, loss: 1.5228
2022-06-30 09:41:02 - train: epoch 0052, iter [02600, 05004], lr: 0.061010, loss: 1.3882
2022-06-30 09:41:36 - train: epoch 0052, iter [02700, 05004], lr: 0.060984, loss: 1.5582
2022-06-30 09:42:09 - train: epoch 0052, iter [02800, 05004], lr: 0.060959, loss: 1.6485
2022-06-30 09:42:42 - train: epoch 0052, iter [02900, 05004], lr: 0.060933, loss: 1.5244
2022-06-30 09:43:15 - train: epoch 0052, iter [03000, 05004], lr: 0.060908, loss: 1.6538
2022-06-30 09:43:48 - train: epoch 0052, iter [03100, 05004], lr: 0.060882, loss: 1.7416
2022-06-30 09:44:21 - train: epoch 0052, iter [03200, 05004], lr: 0.060857, loss: 1.8012
2022-06-30 09:44:54 - train: epoch 0052, iter [03300, 05004], lr: 0.060831, loss: 1.6606
2022-06-30 09:45:27 - train: epoch 0052, iter [03400, 05004], lr: 0.060806, loss: 1.7688
2022-06-30 09:46:00 - train: epoch 0052, iter [03500, 05004], lr: 0.060780, loss: 1.7237
2022-06-30 09:46:32 - train: epoch 0052, iter [03600, 05004], lr: 0.060755, loss: 1.7539
2022-06-30 09:47:05 - train: epoch 0052, iter [03700, 05004], lr: 0.060729, loss: 1.8994
2022-06-30 09:47:38 - train: epoch 0052, iter [03800, 05004], lr: 0.060703, loss: 1.5733
2022-06-30 09:48:11 - train: epoch 0052, iter [03900, 05004], lr: 0.060678, loss: 1.5352
2022-06-30 09:48:44 - train: epoch 0052, iter [04000, 05004], lr: 0.060652, loss: 1.8117
2022-06-30 09:49:18 - train: epoch 0052, iter [04100, 05004], lr: 0.060627, loss: 1.4774
2022-06-30 09:49:51 - train: epoch 0052, iter [04200, 05004], lr: 0.060601, loss: 1.5990
2022-06-30 09:50:25 - train: epoch 0052, iter [04300, 05004], lr: 0.060576, loss: 1.7123
2022-06-30 09:50:58 - train: epoch 0052, iter [04400, 05004], lr: 0.060550, loss: 1.7288
2022-06-30 09:51:30 - train: epoch 0052, iter [04500, 05004], lr: 0.060525, loss: 1.5955
2022-06-30 09:52:03 - train: epoch 0052, iter [04600, 05004], lr: 0.060499, loss: 1.7287
2022-06-30 09:52:36 - train: epoch 0052, iter [04700, 05004], lr: 0.060473, loss: 1.6864
2022-06-30 09:53:09 - train: epoch 0052, iter [04800, 05004], lr: 0.060448, loss: 1.4855
2022-06-30 09:53:41 - train: epoch 0052, iter [04900, 05004], lr: 0.060422, loss: 1.5640
2022-06-30 09:54:14 - train: epoch 0052, iter [05000, 05004], lr: 0.060397, loss: 1.6291
2022-06-30 09:54:15 - train: epoch 052, train_loss: 1.6692
2022-06-30 09:55:32 - eval: epoch: 052, acc1: 65.446%, acc5: 87.110%, test_loss: 1.4222, per_image_load_time: 2.331ms, per_image_inference_time: 0.646ms
2022-06-30 09:55:33 - until epoch: 052, best_acc1: 65.512%
2022-06-30 09:55:33 - epoch 053 lr: 0.060395
2022-06-30 09:56:12 - train: epoch 0053, iter [00100, 05004], lr: 0.060370, loss: 1.6038
2022-06-30 09:56:45 - train: epoch 0053, iter [00200, 05004], lr: 0.060344, loss: 1.7956
2022-06-30 09:57:17 - train: epoch 0053, iter [00300, 05004], lr: 0.060319, loss: 1.6635
2022-06-30 09:57:49 - train: epoch 0053, iter [00400, 05004], lr: 0.060293, loss: 1.7001
2022-06-30 09:58:21 - train: epoch 0053, iter [00500, 05004], lr: 0.060268, loss: 1.8743
2022-06-30 09:58:54 - train: epoch 0053, iter [00600, 05004], lr: 0.060242, loss: 1.6551
2022-06-30 09:59:26 - train: epoch 0053, iter [00700, 05004], lr: 0.060216, loss: 1.4990
2022-06-30 09:59:58 - train: epoch 0053, iter [00800, 05004], lr: 0.060191, loss: 1.6556
2022-06-30 10:00:31 - train: epoch 0053, iter [00900, 05004], lr: 0.060165, loss: 1.6257
2022-06-30 10:01:03 - train: epoch 0053, iter [01000, 05004], lr: 0.060140, loss: 1.5611
2022-06-30 10:01:34 - train: epoch 0053, iter [01100, 05004], lr: 0.060114, loss: 1.6638
2022-06-30 10:02:07 - train: epoch 0053, iter [01200, 05004], lr: 0.060088, loss: 1.6412
2022-06-30 10:02:39 - train: epoch 0053, iter [01300, 05004], lr: 0.060063, loss: 1.6197
2022-06-30 10:03:11 - train: epoch 0053, iter [01400, 05004], lr: 0.060037, loss: 1.9828
2022-06-30 10:03:43 - train: epoch 0053, iter [01500, 05004], lr: 0.060011, loss: 1.6779
2022-06-30 10:04:16 - train: epoch 0053, iter [01600, 05004], lr: 0.059986, loss: 1.8715
2022-06-30 10:04:48 - train: epoch 0053, iter [01700, 05004], lr: 0.059960, loss: 1.8097
2022-06-30 10:05:21 - train: epoch 0053, iter [01800, 05004], lr: 0.059935, loss: 1.7386
2022-06-30 10:05:53 - train: epoch 0053, iter [01900, 05004], lr: 0.059909, loss: 1.5315
2022-06-30 10:06:26 - train: epoch 0053, iter [02000, 05004], lr: 0.059883, loss: 1.6385
2022-06-30 10:06:58 - train: epoch 0053, iter [02100, 05004], lr: 0.059858, loss: 1.6445
2022-06-30 10:07:31 - train: epoch 0053, iter [02200, 05004], lr: 0.059832, loss: 1.6098
2022-06-30 10:08:05 - train: epoch 0053, iter [02300, 05004], lr: 0.059806, loss: 1.6245
2022-06-30 10:08:37 - train: epoch 0053, iter [02400, 05004], lr: 0.059781, loss: 1.6847
2022-06-30 10:09:10 - train: epoch 0053, iter [02500, 05004], lr: 0.059755, loss: 1.8647
2022-06-30 10:09:43 - train: epoch 0053, iter [02600, 05004], lr: 0.059729, loss: 1.7109
2022-06-30 10:10:16 - train: epoch 0053, iter [02700, 05004], lr: 0.059704, loss: 1.8571
2022-06-30 10:10:49 - train: epoch 0053, iter [02800, 05004], lr: 0.059678, loss: 1.8515
2022-06-30 10:11:22 - train: epoch 0053, iter [02900, 05004], lr: 0.059652, loss: 1.4489
2022-06-30 10:11:55 - train: epoch 0053, iter [03000, 05004], lr: 0.059627, loss: 1.4146
2022-06-30 10:12:27 - train: epoch 0053, iter [03100, 05004], lr: 0.059601, loss: 1.8215
2022-06-30 10:13:00 - train: epoch 0053, iter [03200, 05004], lr: 0.059575, loss: 1.9053
2022-06-30 10:13:32 - train: epoch 0053, iter [03300, 05004], lr: 0.059550, loss: 1.5909
2022-06-30 10:14:05 - train: epoch 0053, iter [03400, 05004], lr: 0.059524, loss: 1.8035
2022-06-30 10:14:37 - train: epoch 0053, iter [03500, 05004], lr: 0.059498, loss: 1.7577
2022-06-30 10:15:10 - train: epoch 0053, iter [03600, 05004], lr: 0.059473, loss: 1.7829
2022-06-30 10:15:42 - train: epoch 0053, iter [03700, 05004], lr: 0.059447, loss: 1.7584
2022-06-30 10:16:15 - train: epoch 0053, iter [03800, 05004], lr: 0.059421, loss: 1.6437
2022-06-30 10:16:48 - train: epoch 0053, iter [03900, 05004], lr: 0.059396, loss: 1.8074
2022-06-30 10:17:20 - train: epoch 0053, iter [04000, 05004], lr: 0.059370, loss: 1.6110
2022-06-30 10:17:52 - train: epoch 0053, iter [04100, 05004], lr: 0.059344, loss: 1.7565
2022-06-30 10:18:25 - train: epoch 0053, iter [04200, 05004], lr: 0.059318, loss: 1.6599
2022-06-30 10:18:57 - train: epoch 0053, iter [04300, 05004], lr: 0.059293, loss: 1.9489
2022-06-30 10:19:30 - train: epoch 0053, iter [04400, 05004], lr: 0.059267, loss: 1.6261
2022-06-30 10:20:02 - train: epoch 0053, iter [04500, 05004], lr: 0.059241, loss: 1.7888
2022-06-30 10:20:35 - train: epoch 0053, iter [04600, 05004], lr: 0.059216, loss: 1.5976
2022-06-30 10:21:07 - train: epoch 0053, iter [04700, 05004], lr: 0.059190, loss: 1.8609
2022-06-30 10:21:40 - train: epoch 0053, iter [04800, 05004], lr: 0.059164, loss: 1.9047
2022-06-30 10:22:12 - train: epoch 0053, iter [04900, 05004], lr: 0.059139, loss: 1.5600
2022-06-30 10:22:45 - train: epoch 0053, iter [05000, 05004], lr: 0.059113, loss: 1.4986
2022-06-30 10:22:46 - train: epoch 053, train_loss: 1.6587
2022-06-30 10:24:02 - eval: epoch: 053, acc1: 65.558%, acc5: 87.352%, test_loss: 1.4237, per_image_load_time: 1.620ms, per_image_inference_time: 0.637ms
2022-06-30 10:24:03 - until epoch: 053, best_acc1: 65.558%
2022-06-30 10:24:03 - epoch 054 lr: 0.059112
2022-06-30 10:24:44 - train: epoch 0054, iter [00100, 05004], lr: 0.059086, loss: 1.4767
2022-06-30 10:25:16 - train: epoch 0054, iter [00200, 05004], lr: 0.059060, loss: 2.0603
2022-06-30 10:25:49 - train: epoch 0054, iter [00300, 05004], lr: 0.059035, loss: 1.6418
2022-06-30 10:26:21 - train: epoch 0054, iter [00400, 05004], lr: 0.059009, loss: 1.3651
2022-06-30 10:26:54 - train: epoch 0054, iter [00500, 05004], lr: 0.058983, loss: 1.7634
2022-06-30 10:27:27 - train: epoch 0054, iter [00600, 05004], lr: 0.058957, loss: 1.5660
2022-06-30 10:28:00 - train: epoch 0054, iter [00700, 05004], lr: 0.058932, loss: 1.6017
2022-06-30 10:28:32 - train: epoch 0054, iter [00800, 05004], lr: 0.058906, loss: 1.7206
2022-06-30 10:29:05 - train: epoch 0054, iter [00900, 05004], lr: 0.058880, loss: 1.3958
2022-06-30 10:29:38 - train: epoch 0054, iter [01000, 05004], lr: 0.058854, loss: 1.4760
2022-06-30 10:30:11 - train: epoch 0054, iter [01100, 05004], lr: 0.058829, loss: 1.4223
2022-06-30 10:30:44 - train: epoch 0054, iter [01200, 05004], lr: 0.058803, loss: 1.7204
2022-06-30 10:31:17 - train: epoch 0054, iter [01300, 05004], lr: 0.058777, loss: 1.6029
2022-06-30 10:31:50 - train: epoch 0054, iter [01400, 05004], lr: 0.058751, loss: 1.6471
2022-06-30 10:32:23 - train: epoch 0054, iter [01500, 05004], lr: 0.058726, loss: 1.5827
2022-06-30 10:32:56 - train: epoch 0054, iter [01600, 05004], lr: 0.058700, loss: 1.2600
2022-06-30 10:33:29 - train: epoch 0054, iter [01700, 05004], lr: 0.058674, loss: 1.7176
2022-06-30 10:34:02 - train: epoch 0054, iter [01800, 05004], lr: 0.058648, loss: 1.5941
2022-06-30 10:34:35 - train: epoch 0054, iter [01900, 05004], lr: 0.058623, loss: 1.9895
2022-06-30 10:35:07 - train: epoch 0054, iter [02000, 05004], lr: 0.058597, loss: 1.6609
2022-06-30 10:35:40 - train: epoch 0054, iter [02100, 05004], lr: 0.058571, loss: 1.4323
2022-06-30 10:36:13 - train: epoch 0054, iter [02200, 05004], lr: 0.058545, loss: 1.7364
2022-06-30 10:36:46 - train: epoch 0054, iter [02300, 05004], lr: 0.058520, loss: 1.6572
2022-06-30 10:37:19 - train: epoch 0054, iter [02400, 05004], lr: 0.058494, loss: 1.5540
2022-06-30 10:37:51 - train: epoch 0054, iter [02500, 05004], lr: 0.058468, loss: 1.7408
2022-06-30 10:38:24 - train: epoch 0054, iter [02600, 05004], lr: 0.058442, loss: 1.5495
2022-06-30 10:38:57 - train: epoch 0054, iter [02700, 05004], lr: 0.058416, loss: 1.8207
2022-06-30 10:39:30 - train: epoch 0054, iter [02800, 05004], lr: 0.058391, loss: 1.9633
2022-06-30 10:40:03 - train: epoch 0054, iter [02900, 05004], lr: 0.058365, loss: 1.5694
2022-06-30 10:40:35 - train: epoch 0054, iter [03000, 05004], lr: 0.058339, loss: 1.7935
2022-06-30 10:41:08 - train: epoch 0054, iter [03100, 05004], lr: 0.058313, loss: 1.5856
2022-06-30 10:41:41 - train: epoch 0054, iter [03200, 05004], lr: 0.058287, loss: 1.8506
2022-06-30 10:42:14 - train: epoch 0054, iter [03300, 05004], lr: 0.058262, loss: 1.5168
2022-06-30 10:42:46 - train: epoch 0054, iter [03400, 05004], lr: 0.058236, loss: 1.6850
2022-06-30 10:43:19 - train: epoch 0054, iter [03500, 05004], lr: 0.058210, loss: 1.7383
2022-06-30 10:43:52 - train: epoch 0054, iter [03600, 05004], lr: 0.058184, loss: 1.5862
2022-06-30 10:44:25 - train: epoch 0054, iter [03700, 05004], lr: 0.058158, loss: 1.5886
2022-06-30 10:44:58 - train: epoch 0054, iter [03800, 05004], lr: 0.058133, loss: 1.7911
2022-06-30 10:45:31 - train: epoch 0054, iter [03900, 05004], lr: 0.058107, loss: 1.5970
2022-06-30 10:46:04 - train: epoch 0054, iter [04000, 05004], lr: 0.058081, loss: 1.5362
2022-06-30 10:46:37 - train: epoch 0054, iter [04100, 05004], lr: 0.058055, loss: 1.7178
2022-06-30 10:47:09 - train: epoch 0054, iter [04200, 05004], lr: 0.058029, loss: 1.6379
2022-06-30 10:47:42 - train: epoch 0054, iter [04300, 05004], lr: 0.058004, loss: 1.6292
2022-06-30 10:48:15 - train: epoch 0054, iter [04400, 05004], lr: 0.057978, loss: 1.5212
2022-06-30 10:48:48 - train: epoch 0054, iter [04500, 05004], lr: 0.057952, loss: 1.5749
2022-06-30 10:49:21 - train: epoch 0054, iter [04600, 05004], lr: 0.057926, loss: 1.7331
2022-06-30 10:49:54 - train: epoch 0054, iter [04700, 05004], lr: 0.057900, loss: 1.8399
2022-06-30 10:50:27 - train: epoch 0054, iter [04800, 05004], lr: 0.057874, loss: 1.8105
2022-06-30 10:51:00 - train: epoch 0054, iter [04900, 05004], lr: 0.057849, loss: 1.4964
2022-06-30 10:51:33 - train: epoch 0054, iter [05000, 05004], lr: 0.057823, loss: 1.7601
2022-06-30 10:51:34 - train: epoch 054, train_loss: 1.6510
2022-06-30 10:52:51 - eval: epoch: 054, acc1: 65.118%, acc5: 86.828%, test_loss: 1.4636, per_image_load_time: 2.048ms, per_image_inference_time: 0.606ms
2022-06-30 10:52:51 - until epoch: 054, best_acc1: 65.558%
2022-06-30 10:52:51 - epoch 055 lr: 0.057821
2022-06-30 10:53:32 - train: epoch 0055, iter [00100, 05004], lr: 0.057796, loss: 1.5979
2022-06-30 10:54:05 - train: epoch 0055, iter [00200, 05004], lr: 0.057770, loss: 1.5200
2022-06-30 10:54:37 - train: epoch 0055, iter [00300, 05004], lr: 0.057744, loss: 1.4232
2022-06-30 10:55:10 - train: epoch 0055, iter [00400, 05004], lr: 0.057718, loss: 1.5618
2022-06-30 10:55:43 - train: epoch 0055, iter [00500, 05004], lr: 0.057693, loss: 1.3637
2022-06-30 10:56:15 - train: epoch 0055, iter [00600, 05004], lr: 0.057667, loss: 1.5805
2022-06-30 10:56:48 - train: epoch 0055, iter [00700, 05004], lr: 0.057641, loss: 1.7412
2022-06-30 10:57:21 - train: epoch 0055, iter [00800, 05004], lr: 0.057615, loss: 1.3814
2022-06-30 10:57:53 - train: epoch 0055, iter [00900, 05004], lr: 0.057589, loss: 1.6244
2022-06-30 10:58:26 - train: epoch 0055, iter [01000, 05004], lr: 0.057563, loss: 1.5888
2022-06-30 10:58:59 - train: epoch 0055, iter [01100, 05004], lr: 0.057537, loss: 1.6648
2022-06-30 10:59:32 - train: epoch 0055, iter [01200, 05004], lr: 0.057512, loss: 1.6519
2022-06-30 11:00:05 - train: epoch 0055, iter [01300, 05004], lr: 0.057486, loss: 1.7628
2022-06-30 11:00:37 - train: epoch 0055, iter [01400, 05004], lr: 0.057460, loss: 1.6051
2022-06-30 11:01:10 - train: epoch 0055, iter [01500, 05004], lr: 0.057434, loss: 1.6770
2022-06-30 11:01:43 - train: epoch 0055, iter [01600, 05004], lr: 0.057408, loss: 1.7101
2022-06-30 11:02:16 - train: epoch 0055, iter [01700, 05004], lr: 0.057382, loss: 1.7180
2022-06-30 11:02:49 - train: epoch 0055, iter [01800, 05004], lr: 0.057356, loss: 1.7361
2022-06-30 11:03:22 - train: epoch 0055, iter [01900, 05004], lr: 0.057330, loss: 1.6697
2022-06-30 11:03:55 - train: epoch 0055, iter [02000, 05004], lr: 0.057305, loss: 1.6942
2022-06-30 11:04:27 - train: epoch 0055, iter [02100, 05004], lr: 0.057279, loss: 1.4286
2022-06-30 11:05:00 - train: epoch 0055, iter [02200, 05004], lr: 0.057253, loss: 1.7535
2022-06-30 11:05:33 - train: epoch 0055, iter [02300, 05004], lr: 0.057227, loss: 1.6217
2022-06-30 11:06:06 - train: epoch 0055, iter [02400, 05004], lr: 0.057201, loss: 1.4595
2022-06-30 11:06:39 - train: epoch 0055, iter [02500, 05004], lr: 0.057175, loss: 1.7048
2022-06-30 11:07:11 - train: epoch 0055, iter [02600, 05004], lr: 0.057149, loss: 1.5803
2022-06-30 11:07:44 - train: epoch 0055, iter [02700, 05004], lr: 0.057123, loss: 1.5087
2022-06-30 11:08:17 - train: epoch 0055, iter [02800, 05004], lr: 0.057097, loss: 1.6097
2022-06-30 11:08:49 - train: epoch 0055, iter [02900, 05004], lr: 0.057072, loss: 1.7635
2022-06-30 11:09:22 - train: epoch 0055, iter [03000, 05004], lr: 0.057046, loss: 1.6445
2022-06-30 11:09:55 - train: epoch 0055, iter [03100, 05004], lr: 0.057020, loss: 1.7313
2022-06-30 11:10:28 - train: epoch 0055, iter [03200, 05004], lr: 0.056994, loss: 1.5151
2022-06-30 11:11:01 - train: epoch 0055, iter [03300, 05004], lr: 0.056968, loss: 1.4010
2022-06-30 11:11:34 - train: epoch 0055, iter [03400, 05004], lr: 0.056942, loss: 1.5694
2022-06-30 11:12:07 - train: epoch 0055, iter [03500, 05004], lr: 0.056916, loss: 1.4880
2022-06-30 11:12:40 - train: epoch 0055, iter [03600, 05004], lr: 0.056890, loss: 1.6368
2022-06-30 11:13:13 - train: epoch 0055, iter [03700, 05004], lr: 0.056864, loss: 1.6556
2022-06-30 11:13:46 - train: epoch 0055, iter [03800, 05004], lr: 0.056838, loss: 1.7847
2022-06-30 11:14:19 - train: epoch 0055, iter [03900, 05004], lr: 0.056813, loss: 1.9803
2022-06-30 11:14:52 - train: epoch 0055, iter [04000, 05004], lr: 0.056787, loss: 1.6477
2022-06-30 11:15:24 - train: epoch 0055, iter [04100, 05004], lr: 0.056761, loss: 1.6517
2022-06-30 11:15:57 - train: epoch 0055, iter [04200, 05004], lr: 0.056735, loss: 1.6412
2022-06-30 11:16:30 - train: epoch 0055, iter [04300, 05004], lr: 0.056709, loss: 1.7248
2022-06-30 11:17:03 - train: epoch 0055, iter [04400, 05004], lr: 0.056683, loss: 1.9487
2022-06-30 11:17:36 - train: epoch 0055, iter [04500, 05004], lr: 0.056657, loss: 1.6102
2022-06-30 11:18:09 - train: epoch 0055, iter [04600, 05004], lr: 0.056631, loss: 1.7398
2022-06-30 11:18:42 - train: epoch 0055, iter [04700, 05004], lr: 0.056605, loss: 1.4524
2022-06-30 11:19:15 - train: epoch 0055, iter [04800, 05004], lr: 0.056579, loss: 1.7017
2022-06-30 11:19:47 - train: epoch 0055, iter [04900, 05004], lr: 0.056553, loss: 1.5739
2022-06-30 11:20:20 - train: epoch 0055, iter [05000, 05004], lr: 0.056527, loss: 1.6825
2022-06-30 11:20:22 - train: epoch 055, train_loss: 1.6398
2022-06-30 11:21:38 - eval: epoch: 055, acc1: 65.874%, acc5: 87.716%, test_loss: 1.3805, per_image_load_time: 1.474ms, per_image_inference_time: 0.617ms
2022-06-30 11:21:39 - until epoch: 055, best_acc1: 65.874%
2022-06-30 11:21:39 - epoch 056 lr: 0.056526
2022-06-30 11:22:19 - train: epoch 0056, iter [00100, 05004], lr: 0.056500, loss: 1.7107
2022-06-30 11:22:51 - train: epoch 0056, iter [00200, 05004], lr: 0.056474, loss: 1.7354
2022-06-30 11:23:24 - train: epoch 0056, iter [00300, 05004], lr: 0.056448, loss: 1.6045
2022-06-30 11:23:56 - train: epoch 0056, iter [00400, 05004], lr: 0.056423, loss: 1.6355
2022-06-30 11:24:29 - train: epoch 0056, iter [00500, 05004], lr: 0.056397, loss: 1.5255
2022-06-30 11:25:02 - train: epoch 0056, iter [00600, 05004], lr: 0.056371, loss: 1.5756
2022-06-30 11:25:35 - train: epoch 0056, iter [00700, 05004], lr: 0.056345, loss: 1.6355
2022-06-30 11:26:08 - train: epoch 0056, iter [00800, 05004], lr: 0.056319, loss: 1.7786
2022-06-30 11:26:41 - train: epoch 0056, iter [00900, 05004], lr: 0.056293, loss: 1.7233
2022-06-30 11:27:14 - train: epoch 0056, iter [01000, 05004], lr: 0.056267, loss: 1.5920
2022-06-30 11:27:47 - train: epoch 0056, iter [01100, 05004], lr: 0.056241, loss: 1.5540
2022-06-30 11:28:20 - train: epoch 0056, iter [01200, 05004], lr: 0.056215, loss: 1.5348
2022-06-30 11:28:52 - train: epoch 0056, iter [01300, 05004], lr: 0.056189, loss: 1.6880
2022-06-30 11:29:25 - train: epoch 0056, iter [01400, 05004], lr: 0.056163, loss: 1.6587
2022-06-30 11:29:58 - train: epoch 0056, iter [01500, 05004], lr: 0.056137, loss: 1.8955
2022-06-30 11:30:31 - train: epoch 0056, iter [01600, 05004], lr: 0.056111, loss: 1.4620
2022-06-30 11:31:04 - train: epoch 0056, iter [01700, 05004], lr: 0.056085, loss: 1.7853
2022-06-30 11:31:36 - train: epoch 0056, iter [01800, 05004], lr: 0.056059, loss: 1.8018
2022-06-30 11:32:09 - train: epoch 0056, iter [01900, 05004], lr: 0.056033, loss: 1.5951
2022-06-30 11:32:41 - train: epoch 0056, iter [02000, 05004], lr: 0.056007, loss: 1.6635
2022-06-30 11:33:14 - train: epoch 0056, iter [02100, 05004], lr: 0.055981, loss: 1.6329
2022-06-30 11:33:46 - train: epoch 0056, iter [02200, 05004], lr: 0.055955, loss: 1.7293
2022-06-30 11:34:18 - train: epoch 0056, iter [02300, 05004], lr: 0.055929, loss: 1.7424
2022-06-30 11:34:51 - train: epoch 0056, iter [02400, 05004], lr: 0.055903, loss: 1.5942
2022-06-30 11:35:24 - train: epoch 0056, iter [02500, 05004], lr: 0.055877, loss: 1.8698
2022-06-30 11:35:56 - train: epoch 0056, iter [02600, 05004], lr: 0.055851, loss: 1.5638
2022-06-30 11:36:29 - train: epoch 0056, iter [02700, 05004], lr: 0.055825, loss: 1.6876
2022-06-30 11:37:02 - train: epoch 0056, iter [02800, 05004], lr: 0.055799, loss: 1.4735
2022-06-30 11:37:35 - train: epoch 0056, iter [02900, 05004], lr: 0.055773, loss: 1.7938
2022-06-30 11:38:08 - train: epoch 0056, iter [03000, 05004], lr: 0.055747, loss: 1.7669
2022-06-30 11:38:41 - train: epoch 0056, iter [03100, 05004], lr: 0.055721, loss: 1.4887
2022-06-30 11:39:14 - train: epoch 0056, iter [03200, 05004], lr: 0.055696, loss: 1.5232
2022-06-30 11:39:46 - train: epoch 0056, iter [03300, 05004], lr: 0.055670, loss: 1.9039
2022-06-30 11:40:19 - train: epoch 0056, iter [03400, 05004], lr: 0.055644, loss: 1.5801
2022-06-30 11:40:52 - train: epoch 0056, iter [03500, 05004], lr: 0.055618, loss: 1.5393
2022-06-30 11:41:24 - train: epoch 0056, iter [03600, 05004], lr: 0.055592, loss: 1.3444
2022-06-30 11:41:57 - train: epoch 0056, iter [03700, 05004], lr: 0.055566, loss: 1.5845
2022-06-30 11:42:30 - train: epoch 0056, iter [03800, 05004], lr: 0.055540, loss: 1.4945
2022-06-30 11:43:03 - train: epoch 0056, iter [03900, 05004], lr: 0.055514, loss: 1.8535
2022-06-30 11:43:36 - train: epoch 0056, iter [04000, 05004], lr: 0.055488, loss: 1.6325
2022-06-30 11:44:09 - train: epoch 0056, iter [04100, 05004], lr: 0.055462, loss: 1.8203
2022-06-30 11:44:42 - train: epoch 0056, iter [04200, 05004], lr: 0.055436, loss: 1.5919
2022-06-30 11:45:14 - train: epoch 0056, iter [04300, 05004], lr: 0.055410, loss: 1.5864
2022-06-30 11:45:47 - train: epoch 0056, iter [04400, 05004], lr: 0.055384, loss: 1.8576
2022-06-30 11:46:20 - train: epoch 0056, iter [04500, 05004], lr: 0.055358, loss: 1.6309
2022-06-30 11:46:53 - train: epoch 0056, iter [04600, 05004], lr: 0.055332, loss: 1.7676
2022-06-30 11:47:25 - train: epoch 0056, iter [04700, 05004], lr: 0.055306, loss: 1.6851
2022-06-30 11:47:58 - train: epoch 0056, iter [04800, 05004], lr: 0.055279, loss: 1.7770
2022-06-30 11:48:30 - train: epoch 0056, iter [04900, 05004], lr: 0.055253, loss: 1.6268
2022-06-30 11:49:03 - train: epoch 0056, iter [05000, 05004], lr: 0.055227, loss: 1.7113
2022-06-30 11:49:05 - train: epoch 056, train_loss: 1.6266
2022-06-30 11:50:21 - eval: epoch: 056, acc1: 66.086%, acc5: 87.586%, test_loss: 1.3998, per_image_load_time: 1.198ms, per_image_inference_time: 0.625ms
2022-06-30 11:50:22 - until epoch: 056, best_acc1: 66.086%
2022-06-30 11:50:22 - epoch 057 lr: 0.055226
2022-06-30 11:51:01 - train: epoch 0057, iter [00100, 05004], lr: 0.055200, loss: 1.6353
2022-06-30 11:51:34 - train: epoch 0057, iter [00200, 05004], lr: 0.055174, loss: 1.5169
2022-06-30 11:52:06 - train: epoch 0057, iter [00300, 05004], lr: 0.055148, loss: 1.3975
2022-06-30 11:52:39 - train: epoch 0057, iter [00400, 05004], lr: 0.055122, loss: 1.6754
2022-06-30 11:53:12 - train: epoch 0057, iter [00500, 05004], lr: 0.055096, loss: 1.3646
2022-06-30 11:53:44 - train: epoch 0057, iter [00600, 05004], lr: 0.055070, loss: 1.6875
2022-06-30 11:54:17 - train: epoch 0057, iter [00700, 05004], lr: 0.055044, loss: 1.2870
2022-06-30 11:54:49 - train: epoch 0057, iter [00800, 05004], lr: 0.055018, loss: 1.6617
2022-06-30 11:55:22 - train: epoch 0057, iter [00900, 05004], lr: 0.054992, loss: 1.7306
2022-06-30 11:55:55 - train: epoch 0057, iter [01000, 05004], lr: 0.054966, loss: 1.5109
2022-06-30 11:56:27 - train: epoch 0057, iter [01100, 05004], lr: 0.054940, loss: 1.5969
2022-06-30 11:57:00 - train: epoch 0057, iter [01200, 05004], lr: 0.054914, loss: 1.4479
2022-06-30 11:57:32 - train: epoch 0057, iter [01300, 05004], lr: 0.054888, loss: 1.5909
2022-06-30 11:58:05 - train: epoch 0057, iter [01400, 05004], lr: 0.054862, loss: 1.6868
2022-06-30 11:58:37 - train: epoch 0057, iter [01500, 05004], lr: 0.054836, loss: 1.8327
2022-06-30 11:59:10 - train: epoch 0057, iter [01600, 05004], lr: 0.054810, loss: 1.7923
2022-06-30 11:59:42 - train: epoch 0057, iter [01700, 05004], lr: 0.054784, loss: 1.7452
2022-06-30 12:00:15 - train: epoch 0057, iter [01800, 05004], lr: 0.054758, loss: 1.7214
2022-06-30 12:00:48 - train: epoch 0057, iter [01900, 05004], lr: 0.054732, loss: 1.4988
2022-06-30 12:01:21 - train: epoch 0057, iter [02000, 05004], lr: 0.054706, loss: 1.6859
2022-06-30 12:01:53 - train: epoch 0057, iter [02100, 05004], lr: 0.054680, loss: 1.7195
2022-06-30 12:02:26 - train: epoch 0057, iter [02200, 05004], lr: 0.054654, loss: 1.7211
2022-06-30 12:02:58 - train: epoch 0057, iter [02300, 05004], lr: 0.054628, loss: 1.5348
2022-06-30 12:03:31 - train: epoch 0057, iter [02400, 05004], lr: 0.054602, loss: 1.4950
2022-06-30 12:04:04 - train: epoch 0057, iter [02500, 05004], lr: 0.054576, loss: 1.6422
2022-06-30 12:04:37 - train: epoch 0057, iter [02600, 05004], lr: 0.054550, loss: 1.4333
2022-06-30 12:05:10 - train: epoch 0057, iter [02700, 05004], lr: 0.054524, loss: 1.2963
2022-06-30 12:05:42 - train: epoch 0057, iter [02800, 05004], lr: 0.054497, loss: 1.3527
2022-06-30 12:06:15 - train: epoch 0057, iter [02900, 05004], lr: 0.054471, loss: 1.7724
2022-06-30 12:06:48 - train: epoch 0057, iter [03000, 05004], lr: 0.054445, loss: 1.7953
2022-06-30 12:07:20 - train: epoch 0057, iter [03100, 05004], lr: 0.054419, loss: 1.8782
2022-06-30 12:07:53 - train: epoch 0057, iter [03200, 05004], lr: 0.054393, loss: 1.6803
2022-06-30 12:08:26 - train: epoch 0057, iter [03300, 05004], lr: 0.054367, loss: 1.5520
2022-06-30 12:08:58 - train: epoch 0057, iter [03400, 05004], lr: 0.054341, loss: 1.6499
2022-06-30 12:09:31 - train: epoch 0057, iter [03500, 05004], lr: 0.054315, loss: 1.5771
2022-06-30 12:10:04 - train: epoch 0057, iter [03600, 05004], lr: 0.054289, loss: 1.5696
2022-06-30 12:10:37 - train: epoch 0057, iter [03700, 05004], lr: 0.054263, loss: 1.5436
2022-06-30 12:11:09 - train: epoch 0057, iter [03800, 05004], lr: 0.054237, loss: 1.5080
2022-06-30 12:11:42 - train: epoch 0057, iter [03900, 05004], lr: 0.054211, loss: 1.8487
2022-06-30 12:12:15 - train: epoch 0057, iter [04000, 05004], lr: 0.054185, loss: 1.4999
2022-06-30 12:12:48 - train: epoch 0057, iter [04100, 05004], lr: 0.054159, loss: 1.7585
2022-06-30 12:13:20 - train: epoch 0057, iter [04200, 05004], lr: 0.054133, loss: 1.7027
2022-06-30 12:13:53 - train: epoch 0057, iter [04300, 05004], lr: 0.054107, loss: 1.5193
2022-06-30 12:14:26 - train: epoch 0057, iter [04400, 05004], lr: 0.054080, loss: 1.6859
2022-06-30 12:14:58 - train: epoch 0057, iter [04500, 05004], lr: 0.054054, loss: 1.7269
2022-06-30 12:15:31 - train: epoch 0057, iter [04600, 05004], lr: 0.054028, loss: 1.7581
2022-06-30 12:16:04 - train: epoch 0057, iter [04700, 05004], lr: 0.054002, loss: 1.5435
2022-06-30 12:16:37 - train: epoch 0057, iter [04800, 05004], lr: 0.053976, loss: 1.8993
2022-06-30 12:17:09 - train: epoch 0057, iter [04900, 05004], lr: 0.053950, loss: 1.8281
2022-06-30 12:17:42 - train: epoch 0057, iter [05000, 05004], lr: 0.053924, loss: 1.7541
2022-06-30 12:17:44 - train: epoch 057, train_loss: 1.6185
2022-06-30 12:19:00 - eval: epoch: 057, acc1: 65.490%, acc5: 87.234%, test_loss: 1.4216, per_image_load_time: 2.142ms, per_image_inference_time: 0.631ms
2022-06-30 12:19:00 - until epoch: 057, best_acc1: 66.086%
2022-06-30 12:19:00 - epoch 058 lr: 0.053923
2022-06-30 12:19:40 - train: epoch 0058, iter [00100, 05004], lr: 0.053897, loss: 1.5986
2022-06-30 12:20:12 - train: epoch 0058, iter [00200, 05004], lr: 0.053871, loss: 1.4906
2022-06-30 12:20:44 - train: epoch 0058, iter [00300, 05004], lr: 0.053845, loss: 1.5923
2022-06-30 12:21:16 - train: epoch 0058, iter [00400, 05004], lr: 0.053819, loss: 1.7069
2022-06-30 12:21:49 - train: epoch 0058, iter [00500, 05004], lr: 0.053793, loss: 1.4509
2022-06-30 12:22:22 - train: epoch 0058, iter [00600, 05004], lr: 0.053766, loss: 1.8112
2022-06-30 12:22:54 - train: epoch 0058, iter [00700, 05004], lr: 0.053740, loss: 1.5744
2022-06-30 12:23:27 - train: epoch 0058, iter [00800, 05004], lr: 0.053714, loss: 1.4714
2022-06-30 12:24:00 - train: epoch 0058, iter [00900, 05004], lr: 0.053688, loss: 1.4277
2022-06-30 12:24:32 - train: epoch 0058, iter [01000, 05004], lr: 0.053662, loss: 1.6696
2022-06-30 12:25:05 - train: epoch 0058, iter [01100, 05004], lr: 0.053636, loss: 1.4108
2022-06-30 12:25:37 - train: epoch 0058, iter [01200, 05004], lr: 0.053610, loss: 1.4489
2022-06-30 12:26:10 - train: epoch 0058, iter [01300, 05004], lr: 0.053584, loss: 1.6646
2022-06-30 12:26:43 - train: epoch 0058, iter [01400, 05004], lr: 0.053558, loss: 1.6740
2022-06-30 12:27:15 - train: epoch 0058, iter [01500, 05004], lr: 0.053532, loss: 1.5358
2022-06-30 12:27:48 - train: epoch 0058, iter [01600, 05004], lr: 0.053506, loss: 1.5073
2022-06-30 12:28:21 - train: epoch 0058, iter [01700, 05004], lr: 0.053479, loss: 1.8029
2022-06-30 12:28:53 - train: epoch 0058, iter [01800, 05004], lr: 0.053453, loss: 1.7434
2022-06-30 12:29:26 - train: epoch 0058, iter [01900, 05004], lr: 0.053427, loss: 1.8599
2022-06-30 12:29:59 - train: epoch 0058, iter [02000, 05004], lr: 0.053401, loss: 1.8046
2022-06-30 12:30:31 - train: epoch 0058, iter [02100, 05004], lr: 0.053375, loss: 1.4818
2022-06-30 12:31:04 - train: epoch 0058, iter [02200, 05004], lr: 0.053349, loss: 1.4149
2022-06-30 12:31:37 - train: epoch 0058, iter [02300, 05004], lr: 0.053323, loss: 1.5936
2022-06-30 12:32:09 - train: epoch 0058, iter [02400, 05004], lr: 0.053297, loss: 1.5542
2022-06-30 12:32:42 - train: epoch 0058, iter [02500, 05004], lr: 0.053271, loss: 1.7217
2022-06-30 12:33:15 - train: epoch 0058, iter [02600, 05004], lr: 0.053245, loss: 1.4782
2022-06-30 12:33:47 - train: epoch 0058, iter [02700, 05004], lr: 0.053218, loss: 1.7671
2022-06-30 12:34:20 - train: epoch 0058, iter [02800, 05004], lr: 0.053192, loss: 1.3333
2022-06-30 12:34:53 - train: epoch 0058, iter [02900, 05004], lr: 0.053166, loss: 1.5062
2022-06-30 12:35:26 - train: epoch 0058, iter [03000, 05004], lr: 0.053140, loss: 1.7578
2022-06-30 12:35:58 - train: epoch 0058, iter [03100, 05004], lr: 0.053114, loss: 1.5428
2022-06-30 12:36:31 - train: epoch 0058, iter [03200, 05004], lr: 0.053088, loss: 1.4564
2022-06-30 12:37:04 - train: epoch 0058, iter [03300, 05004], lr: 0.053062, loss: 1.5380
2022-06-30 12:37:37 - train: epoch 0058, iter [03400, 05004], lr: 0.053036, loss: 1.4664
2022-06-30 12:38:09 - train: epoch 0058, iter [03500, 05004], lr: 0.053010, loss: 1.4998
2022-06-30 12:38:42 - train: epoch 0058, iter [03600, 05004], lr: 0.052983, loss: 1.4863
2022-06-30 12:39:15 - train: epoch 0058, iter [03700, 05004], lr: 0.052957, loss: 1.6666
2022-06-30 12:39:48 - train: epoch 0058, iter [03800, 05004], lr: 0.052931, loss: 1.6780
2022-06-30 12:40:21 - train: epoch 0058, iter [03900, 05004], lr: 0.052905, loss: 1.7054
2022-06-30 12:40:54 - train: epoch 0058, iter [04000, 05004], lr: 0.052879, loss: 1.7087
2022-06-30 12:41:27 - train: epoch 0058, iter [04100, 05004], lr: 0.052853, loss: 1.7741
2022-06-30 12:41:59 - train: epoch 0058, iter [04200, 05004], lr: 0.052827, loss: 1.3530
2022-06-30 12:42:32 - train: epoch 0058, iter [04300, 05004], lr: 0.052801, loss: 1.8344
2022-06-30 12:43:05 - train: epoch 0058, iter [04400, 05004], lr: 0.052775, loss: 1.3915
2022-06-30 12:43:38 - train: epoch 0058, iter [04500, 05004], lr: 0.052748, loss: 1.6470
2022-06-30 12:44:11 - train: epoch 0058, iter [04600, 05004], lr: 0.052722, loss: 1.5213
2022-06-30 12:44:44 - train: epoch 0058, iter [04700, 05004], lr: 0.052696, loss: 1.6608
2022-06-30 12:45:17 - train: epoch 0058, iter [04800, 05004], lr: 0.052670, loss: 1.6606
2022-06-30 12:45:50 - train: epoch 0058, iter [04900, 05004], lr: 0.052644, loss: 1.5178
2022-06-30 12:46:22 - train: epoch 0058, iter [05000, 05004], lr: 0.052618, loss: 1.6150
2022-06-30 12:46:24 - train: epoch 058, train_loss: 1.6063
2022-06-30 12:47:40 - eval: epoch: 058, acc1: 66.274%, acc5: 87.542%, test_loss: 1.3937, per_image_load_time: 2.292ms, per_image_inference_time: 0.637ms
2022-06-30 12:47:41 - until epoch: 058, best_acc1: 66.274%
2022-06-30 12:47:41 - epoch 059 lr: 0.052617
2022-06-30 12:48:21 - train: epoch 0059, iter [00100, 05004], lr: 0.052591, loss: 1.8016
2022-06-30 12:48:53 - train: epoch 0059, iter [00200, 05004], lr: 0.052565, loss: 1.5772
2022-06-30 12:49:25 - train: epoch 0059, iter [00300, 05004], lr: 0.052538, loss: 1.6449
2022-06-30 12:49:57 - train: epoch 0059, iter [00400, 05004], lr: 0.052512, loss: 1.6373
2022-06-30 12:50:29 - train: epoch 0059, iter [00500, 05004], lr: 0.052486, loss: 1.8462
2022-06-30 12:51:02 - train: epoch 0059, iter [00600, 05004], lr: 0.052460, loss: 1.5726
2022-06-30 12:51:35 - train: epoch 0059, iter [00700, 05004], lr: 0.052434, loss: 1.4900
2022-06-30 12:52:07 - train: epoch 0059, iter [00800, 05004], lr: 0.052408, loss: 1.6231
2022-06-30 12:52:40 - train: epoch 0059, iter [00900, 05004], lr: 0.052382, loss: 1.5957
2022-06-30 12:53:13 - train: epoch 0059, iter [01000, 05004], lr: 0.052356, loss: 1.5534
2022-06-30 12:53:46 - train: epoch 0059, iter [01100, 05004], lr: 0.052329, loss: 1.7686
2022-06-30 12:54:18 - train: epoch 0059, iter [01200, 05004], lr: 0.052303, loss: 1.3204
2022-06-30 12:54:51 - train: epoch 0059, iter [01300, 05004], lr: 0.052277, loss: 1.7888
2022-06-30 12:55:24 - train: epoch 0059, iter [01400, 05004], lr: 0.052251, loss: 1.7644
2022-06-30 12:55:56 - train: epoch 0059, iter [01500, 05004], lr: 0.052225, loss: 1.7012
2022-06-30 12:56:29 - train: epoch 0059, iter [01600, 05004], lr: 0.052199, loss: 1.4336
2022-06-30 12:57:01 - train: epoch 0059, iter [01700, 05004], lr: 0.052173, loss: 1.7420
2022-06-30 12:57:34 - train: epoch 0059, iter [01800, 05004], lr: 0.052146, loss: 1.5269
2022-06-30 12:58:06 - train: epoch 0059, iter [01900, 05004], lr: 0.052120, loss: 1.5600
2022-06-30 12:58:39 - train: epoch 0059, iter [02000, 05004], lr: 0.052094, loss: 1.5228
2022-06-30 12:59:12 - train: epoch 0059, iter [02100, 05004], lr: 0.052068, loss: 1.8018
2022-06-30 12:59:45 - train: epoch 0059, iter [02200, 05004], lr: 0.052042, loss: 1.7986
2022-06-30 13:00:17 - train: epoch 0059, iter [02300, 05004], lr: 0.052016, loss: 1.5095
2022-06-30 13:00:50 - train: epoch 0059, iter [02400, 05004], lr: 0.051990, loss: 1.7782
2022-06-30 13:01:23 - train: epoch 0059, iter [02500, 05004], lr: 0.051964, loss: 1.6881
2022-06-30 13:01:55 - train: epoch 0059, iter [02600, 05004], lr: 0.051937, loss: 1.5459
2022-06-30 13:02:28 - train: epoch 0059, iter [02700, 05004], lr: 0.051911, loss: 1.5544
2022-06-30 13:03:00 - train: epoch 0059, iter [02800, 05004], lr: 0.051885, loss: 1.8087
2022-06-30 13:03:33 - train: epoch 0059, iter [02900, 05004], lr: 0.051859, loss: 1.5121
2022-06-30 13:04:05 - train: epoch 0059, iter [03000, 05004], lr: 0.051833, loss: 2.0334
2022-06-30 13:04:38 - train: epoch 0059, iter [03100, 05004], lr: 0.051807, loss: 1.5463
2022-06-30 13:05:11 - train: epoch 0059, iter [03200, 05004], lr: 0.051781, loss: 1.6232
2022-06-30 13:05:43 - train: epoch 0059, iter [03300, 05004], lr: 0.051754, loss: 1.7066
2022-06-30 13:06:16 - train: epoch 0059, iter [03400, 05004], lr: 0.051728, loss: 1.8888
2022-06-30 13:06:49 - train: epoch 0059, iter [03500, 05004], lr: 0.051702, loss: 1.5363
2022-06-30 13:07:22 - train: epoch 0059, iter [03600, 05004], lr: 0.051676, loss: 1.5858
2022-06-30 13:07:54 - train: epoch 0059, iter [03700, 05004], lr: 0.051650, loss: 1.4988
2022-06-30 13:08:27 - train: epoch 0059, iter [03800, 05004], lr: 0.051624, loss: 1.6007
2022-06-30 13:09:00 - train: epoch 0059, iter [03900, 05004], lr: 0.051598, loss: 1.5303
2022-06-30 13:09:33 - train: epoch 0059, iter [04000, 05004], lr: 0.051571, loss: 1.9037
2022-06-30 13:10:06 - train: epoch 0059, iter [04100, 05004], lr: 0.051545, loss: 1.6773
2022-06-30 13:10:38 - train: epoch 0059, iter [04200, 05004], lr: 0.051519, loss: 1.6099
2022-06-30 13:11:11 - train: epoch 0059, iter [04300, 05004], lr: 0.051493, loss: 1.7106
2022-06-30 13:11:44 - train: epoch 0059, iter [04400, 05004], lr: 0.051467, loss: 1.7999
2022-06-30 13:12:17 - train: epoch 0059, iter [04500, 05004], lr: 0.051441, loss: 1.6557
2022-06-30 13:12:49 - train: epoch 0059, iter [04600, 05004], lr: 0.051414, loss: 1.6407
2022-06-30 13:13:22 - train: epoch 0059, iter [04700, 05004], lr: 0.051388, loss: 1.5559
2022-06-30 13:13:55 - train: epoch 0059, iter [04800, 05004], lr: 0.051362, loss: 1.5002
2022-06-30 13:14:28 - train: epoch 0059, iter [04900, 05004], lr: 0.051336, loss: 1.8053
2022-06-30 13:15:00 - train: epoch 0059, iter [05000, 05004], lr: 0.051310, loss: 1.7462
2022-06-30 13:15:02 - train: epoch 059, train_loss: 1.5963
2022-06-30 13:16:17 - eval: epoch: 059, acc1: 66.724%, acc5: 87.938%, test_loss: 1.3670, per_image_load_time: 2.221ms, per_image_inference_time: 0.642ms
2022-06-30 13:16:18 - until epoch: 059, best_acc1: 66.724%
2022-06-30 13:16:18 - epoch 060 lr: 0.051309
2022-06-30 13:16:58 - train: epoch 0060, iter [00100, 05004], lr: 0.051283, loss: 1.4583
2022-06-30 13:17:30 - train: epoch 0060, iter [00200, 05004], lr: 0.051257, loss: 1.6562
2022-06-30 13:18:02 - train: epoch 0060, iter [00300, 05004], lr: 0.051230, loss: 1.5734
2022-06-30 13:18:35 - train: epoch 0060, iter [00400, 05004], lr: 0.051204, loss: 1.5889
2022-06-30 13:19:07 - train: epoch 0060, iter [00500, 05004], lr: 0.051178, loss: 1.7537
2022-06-30 13:19:39 - train: epoch 0060, iter [00600, 05004], lr: 0.051152, loss: 1.6674
2022-06-30 13:20:12 - train: epoch 0060, iter [00700, 05004], lr: 0.051126, loss: 1.5937
2022-06-30 13:20:44 - train: epoch 0060, iter [00800, 05004], lr: 0.051100, loss: 1.8411
2022-06-30 13:21:17 - train: epoch 0060, iter [00900, 05004], lr: 0.051073, loss: 1.3032
2022-06-30 13:21:49 - train: epoch 0060, iter [01000, 05004], lr: 0.051047, loss: 1.3502
2022-06-30 13:22:22 - train: epoch 0060, iter [01100, 05004], lr: 0.051021, loss: 1.3929
2022-06-30 13:22:54 - train: epoch 0060, iter [01200, 05004], lr: 0.050995, loss: 1.5138
2022-06-30 13:23:26 - train: epoch 0060, iter [01300, 05004], lr: 0.050969, loss: 1.5535
2022-06-30 13:23:59 - train: epoch 0060, iter [01400, 05004], lr: 0.050943, loss: 1.6081
2022-06-30 13:24:32 - train: epoch 0060, iter [01500, 05004], lr: 0.050917, loss: 1.6439
2022-06-30 13:25:04 - train: epoch 0060, iter [01600, 05004], lr: 0.050890, loss: 1.6664
2022-06-30 13:25:37 - train: epoch 0060, iter [01700, 05004], lr: 0.050864, loss: 1.6120
2022-06-30 13:26:10 - train: epoch 0060, iter [01800, 05004], lr: 0.050838, loss: 1.5735
2022-06-30 13:26:43 - train: epoch 0060, iter [01900, 05004], lr: 0.050812, loss: 1.8156
2022-06-30 13:27:16 - train: epoch 0060, iter [02000, 05004], lr: 0.050786, loss: 1.4807
2022-06-30 13:27:49 - train: epoch 0060, iter [02100, 05004], lr: 0.050760, loss: 1.5282
2022-06-30 13:28:21 - train: epoch 0060, iter [02200, 05004], lr: 0.050733, loss: 1.6566
2022-06-30 13:28:53 - train: epoch 0060, iter [02300, 05004], lr: 0.050707, loss: 1.3706
2022-06-30 13:29:26 - train: epoch 0060, iter [02400, 05004], lr: 0.050681, loss: 1.5764
2022-06-30 13:29:58 - train: epoch 0060, iter [02500, 05004], lr: 0.050655, loss: 1.5867
2022-06-30 13:30:31 - train: epoch 0060, iter [02600, 05004], lr: 0.050629, loss: 1.6423
2022-06-30 13:31:04 - train: epoch 0060, iter [02700, 05004], lr: 0.050603, loss: 1.6996
2022-06-30 13:31:37 - train: epoch 0060, iter [02800, 05004], lr: 0.050577, loss: 1.5413
2022-06-30 13:32:10 - train: epoch 0060, iter [02900, 05004], lr: 0.050550, loss: 1.6067
2022-06-30 13:32:43 - train: epoch 0060, iter [03000, 05004], lr: 0.050524, loss: 1.8501
2022-06-30 13:33:16 - train: epoch 0060, iter [03100, 05004], lr: 0.050498, loss: 1.6151
2022-06-30 13:33:49 - train: epoch 0060, iter [03200, 05004], lr: 0.050472, loss: 1.6365
2022-06-30 13:34:21 - train: epoch 0060, iter [03300, 05004], lr: 0.050446, loss: 1.3590
2022-06-30 13:34:54 - train: epoch 0060, iter [03400, 05004], lr: 0.050420, loss: 1.6849
2022-06-30 13:35:27 - train: epoch 0060, iter [03500, 05004], lr: 0.050393, loss: 1.6155
2022-06-30 13:35:59 - train: epoch 0060, iter [03600, 05004], lr: 0.050367, loss: 1.5444
2022-06-30 13:36:32 - train: epoch 0060, iter [03700, 05004], lr: 0.050341, loss: 1.6642
2022-06-30 13:37:05 - train: epoch 0060, iter [03800, 05004], lr: 0.050315, loss: 1.6135
2022-06-30 13:37:38 - train: epoch 0060, iter [03900, 05004], lr: 0.050289, loss: 1.9307
2022-06-30 13:38:10 - train: epoch 0060, iter [04000, 05004], lr: 0.050263, loss: 1.5591
2022-06-30 13:38:43 - train: epoch 0060, iter [04100, 05004], lr: 0.050236, loss: 1.7439
2022-06-30 13:39:16 - train: epoch 0060, iter [04200, 05004], lr: 0.050210, loss: 1.6585
2022-06-30 13:39:48 - train: epoch 0060, iter [04300, 05004], lr: 0.050184, loss: 1.5652
2022-06-30 13:40:21 - train: epoch 0060, iter [04400, 05004], lr: 0.050158, loss: 1.7112
2022-06-30 13:40:54 - train: epoch 0060, iter [04500, 05004], lr: 0.050132, loss: 1.5850
2022-06-30 13:41:27 - train: epoch 0060, iter [04600, 05004], lr: 0.050106, loss: 1.4657
2022-06-30 13:42:00 - train: epoch 0060, iter [04700, 05004], lr: 0.050080, loss: 1.6631
2022-06-30 13:42:32 - train: epoch 0060, iter [04800, 05004], lr: 0.050053, loss: 1.4117
2022-06-30 13:43:05 - train: epoch 0060, iter [04900, 05004], lr: 0.050027, loss: 1.5803
2022-06-30 13:43:37 - train: epoch 0060, iter [05000, 05004], lr: 0.050001, loss: 1.6769
2022-06-30 13:43:39 - train: epoch 060, train_loss: 1.5851
2022-06-30 13:44:54 - eval: epoch: 060, acc1: 66.408%, acc5: 87.906%, test_loss: 1.3676, per_image_load_time: 1.962ms, per_image_inference_time: 0.646ms
2022-06-30 13:44:55 - until epoch: 060, best_acc1: 66.724%
2022-06-30 13:44:55 - epoch 061 lr: 0.050000
2022-06-30 13:45:35 - train: epoch 0061, iter [00100, 05004], lr: 0.049974, loss: 1.4822
2022-06-30 13:46:07 - train: epoch 0061, iter [00200, 05004], lr: 0.049948, loss: 1.5653
2022-06-30 13:46:39 - train: epoch 0061, iter [00300, 05004], lr: 0.049922, loss: 1.3165
2022-06-30 13:47:12 - train: epoch 0061, iter [00400, 05004], lr: 0.049895, loss: 1.8234
2022-06-30 13:47:44 - train: epoch 0061, iter [00500, 05004], lr: 0.049869, loss: 1.5198
2022-06-30 13:48:16 - train: epoch 0061, iter [00600, 05004], lr: 0.049843, loss: 1.5621
2022-06-30 13:48:49 - train: epoch 0061, iter [00700, 05004], lr: 0.049817, loss: 1.3690
2022-06-30 13:49:22 - train: epoch 0061, iter [00800, 05004], lr: 0.049791, loss: 1.6426
2022-06-30 13:49:55 - train: epoch 0061, iter [00900, 05004], lr: 0.049765, loss: 1.5343
2022-06-30 13:50:28 - train: epoch 0061, iter [01000, 05004], lr: 0.049738, loss: 1.3456
2022-06-30 13:51:01 - train: epoch 0061, iter [01100, 05004], lr: 0.049712, loss: 1.3020
2022-06-30 13:51:33 - train: epoch 0061, iter [01200, 05004], lr: 0.049686, loss: 1.5015
2022-06-30 13:52:06 - train: epoch 0061, iter [01300, 05004], lr: 0.049660, loss: 1.5028
2022-06-30 13:52:39 - train: epoch 0061, iter [01400, 05004], lr: 0.049634, loss: 1.6006
2022-06-30 13:53:11 - train: epoch 0061, iter [01500, 05004], lr: 0.049608, loss: 1.7718
2022-06-30 13:53:44 - train: epoch 0061, iter [01600, 05004], lr: 0.049581, loss: 1.3473
2022-06-30 13:54:17 - train: epoch 0061, iter [01700, 05004], lr: 0.049555, loss: 1.6051
2022-06-30 13:54:49 - train: epoch 0061, iter [01800, 05004], lr: 0.049529, loss: 1.6371
2022-06-30 13:55:22 - train: epoch 0061, iter [01900, 05004], lr: 0.049503, loss: 1.6659
2022-06-30 13:55:55 - train: epoch 0061, iter [02000, 05004], lr: 0.049477, loss: 1.5864
2022-06-30 13:56:28 - train: epoch 0061, iter [02100, 05004], lr: 0.049451, loss: 1.9589
2022-06-30 13:57:01 - train: epoch 0061, iter [02200, 05004], lr: 0.049425, loss: 1.4201
2022-06-30 13:57:34 - train: epoch 0061, iter [02300, 05004], lr: 0.049398, loss: 1.5037
2022-06-30 13:58:07 - train: epoch 0061, iter [02400, 05004], lr: 0.049372, loss: 1.4221
2022-06-30 13:58:40 - train: epoch 0061, iter [02500, 05004], lr: 0.049346, loss: 1.4770
2022-06-30 13:59:12 - train: epoch 0061, iter [02600, 05004], lr: 0.049320, loss: 1.7206
2022-06-30 13:59:45 - train: epoch 0061, iter [02700, 05004], lr: 0.049294, loss: 1.7819
2022-06-30 14:00:18 - train: epoch 0061, iter [02800, 05004], lr: 0.049268, loss: 1.6522
2022-06-30 14:00:51 - train: epoch 0061, iter [02900, 05004], lr: 0.049241, loss: 1.6874
2022-06-30 14:01:24 - train: epoch 0061, iter [03000, 05004], lr: 0.049215, loss: 1.4819
2022-06-30 14:01:57 - train: epoch 0061, iter [03100, 05004], lr: 0.049189, loss: 1.7273
2022-06-30 14:02:30 - train: epoch 0061, iter [03200, 05004], lr: 0.049163, loss: 1.5786
2022-06-30 14:03:03 - train: epoch 0061, iter [03300, 05004], lr: 0.049137, loss: 1.5419
2022-06-30 14:03:36 - train: epoch 0061, iter [03400, 05004], lr: 0.049111, loss: 1.3557
2022-06-30 14:04:09 - train: epoch 0061, iter [03500, 05004], lr: 0.049084, loss: 1.6388
2022-06-30 14:04:42 - train: epoch 0061, iter [03600, 05004], lr: 0.049058, loss: 1.6017
2022-06-30 14:05:15 - train: epoch 0061, iter [03700, 05004], lr: 0.049032, loss: 1.6252
2022-06-30 14:05:48 - train: epoch 0061, iter [03800, 05004], lr: 0.049006, loss: 1.5598
2022-06-30 14:06:21 - train: epoch 0061, iter [03900, 05004], lr: 0.048980, loss: 1.5424
2022-06-30 14:06:54 - train: epoch 0061, iter [04000, 05004], lr: 0.048954, loss: 1.6294
2022-06-30 14:07:27 - train: epoch 0061, iter [04100, 05004], lr: 0.048928, loss: 1.8623
2022-06-30 14:08:00 - train: epoch 0061, iter [04200, 05004], lr: 0.048901, loss: 1.4763
2022-06-30 14:08:33 - train: epoch 0061, iter [04300, 05004], lr: 0.048875, loss: 1.6241
2022-06-30 14:09:05 - train: epoch 0061, iter [04400, 05004], lr: 0.048849, loss: 1.6544
2022-06-30 14:09:38 - train: epoch 0061, iter [04500, 05004], lr: 0.048823, loss: 1.6197
2022-06-30 14:10:11 - train: epoch 0061, iter [04600, 05004], lr: 0.048797, loss: 1.7666
2022-06-30 14:10:44 - train: epoch 0061, iter [04700, 05004], lr: 0.048771, loss: 1.4875
2022-06-30 14:11:17 - train: epoch 0061, iter [04800, 05004], lr: 0.048744, loss: 1.5315
2022-06-30 14:11:50 - train: epoch 0061, iter [04900, 05004], lr: 0.048718, loss: 1.5784
2022-06-30 14:12:22 - train: epoch 0061, iter [05000, 05004], lr: 0.048692, loss: 1.6699
2022-06-30 14:12:24 - train: epoch 061, train_loss: 1.5756
2022-06-30 14:13:40 - eval: epoch: 061, acc1: 67.186%, acc5: 88.332%, test_loss: 1.3441, per_image_load_time: 2.319ms, per_image_inference_time: 0.629ms
2022-06-30 14:13:41 - until epoch: 061, best_acc1: 67.186%
2022-06-30 14:13:41 - epoch 062 lr: 0.048691
2022-06-30 14:14:20 - train: epoch 0062, iter [00100, 05004], lr: 0.048665, loss: 1.5028
2022-06-30 14:14:52 - train: epoch 0062, iter [00200, 05004], lr: 0.048639, loss: 1.5806
2022-06-30 14:15:25 - train: epoch 0062, iter [00300, 05004], lr: 0.048613, loss: 1.5287
2022-06-30 14:15:57 - train: epoch 0062, iter [00400, 05004], lr: 0.048587, loss: 1.3982
2022-06-30 14:16:30 - train: epoch 0062, iter [00500, 05004], lr: 0.048560, loss: 1.5462
2022-06-30 14:17:02 - train: epoch 0062, iter [00600, 05004], lr: 0.048534, loss: 1.5175
2022-06-30 14:17:35 - train: epoch 0062, iter [00700, 05004], lr: 0.048508, loss: 1.7026
2022-06-30 14:18:07 - train: epoch 0062, iter [00800, 05004], lr: 0.048482, loss: 1.5142
2022-06-30 14:18:40 - train: epoch 0062, iter [00900, 05004], lr: 0.048456, loss: 1.5191
2022-06-30 14:19:13 - train: epoch 0062, iter [01000, 05004], lr: 0.048430, loss: 1.6585
2022-06-30 14:19:46 - train: epoch 0062, iter [01100, 05004], lr: 0.048404, loss: 1.5309
2022-06-30 14:20:19 - train: epoch 0062, iter [01200, 05004], lr: 0.048377, loss: 1.8839
2022-06-30 14:20:51 - train: epoch 0062, iter [01300, 05004], lr: 0.048351, loss: 1.5822
2022-06-30 14:21:24 - train: epoch 0062, iter [01400, 05004], lr: 0.048325, loss: 1.5509
2022-06-30 14:21:57 - train: epoch 0062, iter [01500, 05004], lr: 0.048299, loss: 1.6218
2022-06-30 14:22:30 - train: epoch 0062, iter [01600, 05004], lr: 0.048273, loss: 1.6285
2022-06-30 14:23:02 - train: epoch 0062, iter [01700, 05004], lr: 0.048247, loss: 1.6375
2022-06-30 14:23:35 - train: epoch 0062, iter [01800, 05004], lr: 0.048221, loss: 1.3895
2022-06-30 14:24:08 - train: epoch 0062, iter [01900, 05004], lr: 0.048194, loss: 1.4785
2022-06-30 14:24:41 - train: epoch 0062, iter [02000, 05004], lr: 0.048168, loss: 1.3885
2022-06-30 14:25:14 - train: epoch 0062, iter [02100, 05004], lr: 0.048142, loss: 1.6453
2022-06-30 14:25:47 - train: epoch 0062, iter [02200, 05004], lr: 0.048116, loss: 1.5117
2022-06-30 14:26:20 - train: epoch 0062, iter [02300, 05004], lr: 0.048090, loss: 1.5614
2022-06-30 14:26:53 - train: epoch 0062, iter [02400, 05004], lr: 0.048064, loss: 1.5937
2022-06-30 14:27:25 - train: epoch 0062, iter [02500, 05004], lr: 0.048038, loss: 1.6960
2022-06-30 14:27:58 - train: epoch 0062, iter [02600, 05004], lr: 0.048011, loss: 1.4701
2022-06-30 14:28:31 - train: epoch 0062, iter [02700, 05004], lr: 0.047985, loss: 1.4903
2022-06-30 14:29:04 - train: epoch 0062, iter [02800, 05004], lr: 0.047959, loss: 1.7479
2022-06-30 14:29:37 - train: epoch 0062, iter [02900, 05004], lr: 0.047933, loss: 1.6081
2022-06-30 14:30:09 - train: epoch 0062, iter [03000, 05004], lr: 0.047907, loss: 1.6826
2022-06-30 14:30:43 - train: epoch 0062, iter [03100, 05004], lr: 0.047881, loss: 1.6297
2022-06-30 14:31:15 - train: epoch 0062, iter [03200, 05004], lr: 0.047855, loss: 1.3693
2022-06-30 14:31:48 - train: epoch 0062, iter [03300, 05004], lr: 0.047828, loss: 1.6886
2022-06-30 14:32:21 - train: epoch 0062, iter [03400, 05004], lr: 0.047802, loss: 1.5991
2022-06-30 14:32:53 - train: epoch 0062, iter [03500, 05004], lr: 0.047776, loss: 1.6224
2022-06-30 14:33:26 - train: epoch 0062, iter [03600, 05004], lr: 0.047750, loss: 1.5448
2022-06-30 14:33:59 - train: epoch 0062, iter [03700, 05004], lr: 0.047724, loss: 1.5957
2022-06-30 14:34:32 - train: epoch 0062, iter [03800, 05004], lr: 0.047698, loss: 1.5239
2022-06-30 14:35:05 - train: epoch 0062, iter [03900, 05004], lr: 0.047672, loss: 1.5506
2022-06-30 14:35:37 - train: epoch 0062, iter [04000, 05004], lr: 0.047646, loss: 1.3281
2022-06-30 14:36:10 - train: epoch 0062, iter [04100, 05004], lr: 0.047619, loss: 1.6616
2022-06-30 14:36:42 - train: epoch 0062, iter [04200, 05004], lr: 0.047593, loss: 1.4620
2022-06-30 14:37:15 - train: epoch 0062, iter [04300, 05004], lr: 0.047567, loss: 1.5874
2022-06-30 14:37:48 - train: epoch 0062, iter [04400, 05004], lr: 0.047541, loss: 1.5509
2022-06-30 14:38:21 - train: epoch 0062, iter [04500, 05004], lr: 0.047515, loss: 1.4383
2022-06-30 14:38:54 - train: epoch 0062, iter [04600, 05004], lr: 0.047489, loss: 1.1682
2022-06-30 14:39:27 - train: epoch 0062, iter [04700, 05004], lr: 0.047463, loss: 1.5649
2022-06-30 14:40:00 - train: epoch 0062, iter [04800, 05004], lr: 0.047436, loss: 1.5828
2022-06-30 14:40:33 - train: epoch 0062, iter [04900, 05004], lr: 0.047410, loss: 1.5343
2022-06-30 14:41:05 - train: epoch 0062, iter [05000, 05004], lr: 0.047384, loss: 1.6155
2022-06-30 14:41:07 - train: epoch 062, train_loss: 1.5617
2022-06-30 14:42:23 - eval: epoch: 062, acc1: 67.098%, acc5: 88.258%, test_loss: 1.3376, per_image_load_time: 1.287ms, per_image_inference_time: 0.639ms
2022-06-30 14:42:24 - until epoch: 062, best_acc1: 67.186%
2022-06-30 14:42:24 - epoch 063 lr: 0.047383
2022-06-30 14:43:04 - train: epoch 0063, iter [00100, 05004], lr: 0.047357, loss: 1.4471
2022-06-30 14:43:36 - train: epoch 0063, iter [00200, 05004], lr: 0.047331, loss: 1.3720
2022-06-30 14:44:08 - train: epoch 0063, iter [00300, 05004], lr: 0.047305, loss: 1.7158
2022-06-30 14:44:40 - train: epoch 0063, iter [00400, 05004], lr: 0.047279, loss: 1.6160
2022-06-30 14:45:12 - train: epoch 0063, iter [00500, 05004], lr: 0.047253, loss: 1.3736
2022-06-30 14:45:44 - train: epoch 0063, iter [00600, 05004], lr: 0.047226, loss: 1.6884
2022-06-30 14:46:17 - train: epoch 0063, iter [00700, 05004], lr: 0.047200, loss: 1.5144
2022-06-30 14:46:49 - train: epoch 0063, iter [00800, 05004], lr: 0.047174, loss: 1.4474
2022-06-30 14:47:22 - train: epoch 0063, iter [00900, 05004], lr: 0.047148, loss: 1.5675
2022-06-30 14:47:54 - train: epoch 0063, iter [01000, 05004], lr: 0.047122, loss: 1.7378
2022-06-30 14:48:27 - train: epoch 0063, iter [01100, 05004], lr: 0.047096, loss: 1.5346
2022-06-30 14:49:00 - train: epoch 0063, iter [01200, 05004], lr: 0.047070, loss: 1.5442
2022-06-30 14:49:32 - train: epoch 0063, iter [01300, 05004], lr: 0.047044, loss: 1.5832
2022-06-30 14:50:05 - train: epoch 0063, iter [01400, 05004], lr: 0.047018, loss: 1.9338
2022-06-30 14:50:37 - train: epoch 0063, iter [01500, 05004], lr: 0.046991, loss: 1.6789
2022-06-30 14:51:10 - train: epoch 0063, iter [01600, 05004], lr: 0.046965, loss: 1.5145
2022-06-30 14:51:42 - train: epoch 0063, iter [01700, 05004], lr: 0.046939, loss: 1.3416
2022-06-30 14:52:15 - train: epoch 0063, iter [01800, 05004], lr: 0.046913, loss: 1.7518
2022-06-30 14:52:47 - train: epoch 0063, iter [01900, 05004], lr: 0.046887, loss: 1.7006
2022-06-30 14:53:20 - train: epoch 0063, iter [02000, 05004], lr: 0.046861, loss: 1.4318
2022-06-30 14:53:52 - train: epoch 0063, iter [02100, 05004], lr: 0.046835, loss: 1.4511
2022-06-30 14:54:25 - train: epoch 0063, iter [02200, 05004], lr: 0.046809, loss: 1.9609
2022-06-30 14:54:57 - train: epoch 0063, iter [02300, 05004], lr: 0.046783, loss: 1.7011
2022-06-30 14:55:30 - train: epoch 0063, iter [02400, 05004], lr: 0.046756, loss: 1.7216
2022-06-30 14:56:03 - train: epoch 0063, iter [02500, 05004], lr: 0.046730, loss: 1.4948
2022-06-30 14:56:36 - train: epoch 0063, iter [02600, 05004], lr: 0.046704, loss: 1.4323
2022-06-30 14:57:09 - train: epoch 0063, iter [02700, 05004], lr: 0.046678, loss: 1.6556
2022-06-30 14:57:41 - train: epoch 0063, iter [02800, 05004], lr: 0.046652, loss: 1.5287
2022-06-30 14:58:14 - train: epoch 0063, iter [02900, 05004], lr: 0.046626, loss: 1.5043
2022-06-30 14:58:47 - train: epoch 0063, iter [03000, 05004], lr: 0.046600, loss: 1.6573
2022-06-30 14:59:19 - train: epoch 0063, iter [03100, 05004], lr: 0.046574, loss: 2.0249
2022-06-30 14:59:52 - train: epoch 0063, iter [03200, 05004], lr: 0.046548, loss: 1.6455
2022-06-30 15:00:25 - train: epoch 0063, iter [03300, 05004], lr: 0.046522, loss: 1.5261
2022-06-30 15:00:58 - train: epoch 0063, iter [03400, 05004], lr: 0.046495, loss: 1.3679
2022-06-30 15:01:30 - train: epoch 0063, iter [03500, 05004], lr: 0.046469, loss: 1.6544
2022-06-30 15:02:03 - train: epoch 0063, iter [03600, 05004], lr: 0.046443, loss: 1.4414
2022-06-30 15:02:36 - train: epoch 0063, iter [03700, 05004], lr: 0.046417, loss: 1.6328
2022-06-30 15:03:09 - train: epoch 0063, iter [03800, 05004], lr: 0.046391, loss: 1.8009
2022-06-30 15:03:41 - train: epoch 0063, iter [03900, 05004], lr: 0.046365, loss: 1.3188
2022-06-30 15:04:14 - train: epoch 0063, iter [04000, 05004], lr: 0.046339, loss: 1.3513
2022-06-30 15:04:47 - train: epoch 0063, iter [04100, 05004], lr: 0.046313, loss: 1.7753
2022-06-30 15:05:20 - train: epoch 0063, iter [04200, 05004], lr: 0.046287, loss: 1.4898
2022-06-30 15:05:53 - train: epoch 0063, iter [04300, 05004], lr: 0.046261, loss: 1.6856
2022-06-30 15:06:26 - train: epoch 0063, iter [04400, 05004], lr: 0.046235, loss: 1.6263
2022-06-30 15:06:58 - train: epoch 0063, iter [04500, 05004], lr: 0.046208, loss: 1.4562
2022-06-30 15:07:31 - train: epoch 0063, iter [04600, 05004], lr: 0.046182, loss: 1.4638
2022-06-30 15:08:04 - train: epoch 0063, iter [04700, 05004], lr: 0.046156, loss: 1.5412
2022-06-30 15:08:36 - train: epoch 0063, iter [04800, 05004], lr: 0.046130, loss: 1.5221
2022-06-30 15:09:09 - train: epoch 0063, iter [04900, 05004], lr: 0.046104, loss: 1.5085
2022-06-30 15:09:42 - train: epoch 0063, iter [05000, 05004], lr: 0.046078, loss: 1.4736
2022-06-30 15:09:43 - train: epoch 063, train_loss: 1.5522
2022-06-30 15:11:00 - eval: epoch: 063, acc1: 67.680%, acc5: 88.384%, test_loss: 1.3317, per_image_load_time: 0.820ms, per_image_inference_time: 0.654ms
2022-06-30 15:11:00 - until epoch: 063, best_acc1: 67.680%
2022-06-30 15:11:00 - epoch 064 lr: 0.046077
2022-06-30 15:11:40 - train: epoch 0064, iter [00100, 05004], lr: 0.046051, loss: 1.4919
2022-06-30 15:12:12 - train: epoch 0064, iter [00200, 05004], lr: 0.046025, loss: 1.4791
2022-06-30 15:12:44 - train: epoch 0064, iter [00300, 05004], lr: 0.045999, loss: 1.2326
2022-06-30 15:13:17 - train: epoch 0064, iter [00400, 05004], lr: 0.045973, loss: 1.6265
2022-06-30 15:13:49 - train: epoch 0064, iter [00500, 05004], lr: 0.045947, loss: 1.3581
2022-06-30 15:14:22 - train: epoch 0064, iter [00600, 05004], lr: 0.045921, loss: 1.7433
2022-06-30 15:14:54 - train: epoch 0064, iter [00700, 05004], lr: 0.045895, loss: 1.6411
2022-06-30 15:15:27 - train: epoch 0064, iter [00800, 05004], lr: 0.045868, loss: 1.3782
2022-06-30 15:16:00 - train: epoch 0064, iter [00900, 05004], lr: 0.045842, loss: 1.5206
2022-06-30 15:16:32 - train: epoch 0064, iter [01000, 05004], lr: 0.045816, loss: 1.3673
2022-06-30 15:17:05 - train: epoch 0064, iter [01100, 05004], lr: 0.045790, loss: 1.4759
2022-06-30 15:17:37 - train: epoch 0064, iter [01200, 05004], lr: 0.045764, loss: 1.3456
2022-06-30 15:18:10 - train: epoch 0064, iter [01300, 05004], lr: 0.045738, loss: 1.6752
2022-06-30 15:18:42 - train: epoch 0064, iter [01400, 05004], lr: 0.045712, loss: 1.7727
2022-06-30 15:19:15 - train: epoch 0064, iter [01500, 05004], lr: 0.045686, loss: 1.5661
2022-06-30 15:19:48 - train: epoch 0064, iter [01600, 05004], lr: 0.045660, loss: 1.4692
2022-06-30 15:20:20 - train: epoch 0064, iter [01700, 05004], lr: 0.045634, loss: 1.4011
2022-06-30 15:20:53 - train: epoch 0064, iter [01800, 05004], lr: 0.045608, loss: 1.4436
2022-06-30 15:21:25 - train: epoch 0064, iter [01900, 05004], lr: 0.045582, loss: 1.5345
2022-06-30 15:21:58 - train: epoch 0064, iter [02000, 05004], lr: 0.045556, loss: 1.4021
2022-06-30 15:22:31 - train: epoch 0064, iter [02100, 05004], lr: 0.045530, loss: 1.5944
2022-06-30 15:23:03 - train: epoch 0064, iter [02200, 05004], lr: 0.045504, loss: 1.6504
2022-06-30 15:23:36 - train: epoch 0064, iter [02300, 05004], lr: 0.045478, loss: 1.5813
2022-06-30 15:24:09 - train: epoch 0064, iter [02400, 05004], lr: 0.045451, loss: 1.5161
2022-06-30 15:24:41 - train: epoch 0064, iter [02500, 05004], lr: 0.045425, loss: 1.3683
2022-06-30 15:25:14 - train: epoch 0064, iter [02600, 05004], lr: 0.045399, loss: 1.4584
2022-06-30 15:25:47 - train: epoch 0064, iter [02700, 05004], lr: 0.045373, loss: 1.5163
2022-06-30 15:26:20 - train: epoch 0064, iter [02800, 05004], lr: 0.045347, loss: 1.4027
2022-06-30 15:26:52 - train: epoch 0064, iter [02900, 05004], lr: 0.045321, loss: 1.8568
2022-06-30 15:27:25 - train: epoch 0064, iter [03000, 05004], lr: 0.045295, loss: 1.6194
2022-06-30 15:27:58 - train: epoch 0064, iter [03100, 05004], lr: 0.045269, loss: 1.4644
2022-06-30 15:28:31 - train: epoch 0064, iter [03200, 05004], lr: 0.045243, loss: 1.6115
2022-06-30 15:29:04 - train: epoch 0064, iter [03300, 05004], lr: 0.045217, loss: 1.4989
2022-06-30 15:29:37 - train: epoch 0064, iter [03400, 05004], lr: 0.045191, loss: 1.7679
2022-06-30 15:30:09 - train: epoch 0064, iter [03500, 05004], lr: 0.045165, loss: 1.5364
2022-06-30 15:30:42 - train: epoch 0064, iter [03600, 05004], lr: 0.045139, loss: 1.5501
2022-06-30 15:31:15 - train: epoch 0064, iter [03700, 05004], lr: 0.045113, loss: 1.1658
2022-06-30 15:31:47 - train: epoch 0064, iter [03800, 05004], lr: 0.045087, loss: 1.7758
2022-06-30 15:32:20 - train: epoch 0064, iter [03900, 05004], lr: 0.045061, loss: 1.4371
2022-06-30 15:32:53 - train: epoch 0064, iter [04000, 05004], lr: 0.045035, loss: 1.3401
2022-06-30 15:33:25 - train: epoch 0064, iter [04100, 05004], lr: 0.045009, loss: 1.6003
2022-06-30 15:33:59 - train: epoch 0064, iter [04200, 05004], lr: 0.044983, loss: 1.4162
2022-06-30 15:34:32 - train: epoch 0064, iter [04300, 05004], lr: 0.044957, loss: 1.5876
2022-06-30 15:35:05 - train: epoch 0064, iter [04400, 05004], lr: 0.044931, loss: 1.5907
2022-06-30 15:35:37 - train: epoch 0064, iter [04500, 05004], lr: 0.044905, loss: 1.5202
2022-06-30 15:36:10 - train: epoch 0064, iter [04600, 05004], lr: 0.044879, loss: 1.6989
2022-06-30 15:36:43 - train: epoch 0064, iter [04700, 05004], lr: 0.044853, loss: 1.8155
2022-06-30 15:37:16 - train: epoch 0064, iter [04800, 05004], lr: 0.044827, loss: 1.5858
2022-06-30 15:37:49 - train: epoch 0064, iter [04900, 05004], lr: 0.044801, loss: 1.7282
2022-06-30 15:38:21 - train: epoch 0064, iter [05000, 05004], lr: 0.044775, loss: 1.4420
2022-06-30 15:38:23 - train: epoch 064, train_loss: 1.5413
2022-06-30 15:39:38 - eval: epoch: 064, acc1: 67.298%, acc5: 88.218%, test_loss: 1.3386, per_image_load_time: 2.276ms, per_image_inference_time: 0.646ms
2022-06-30 15:39:39 - until epoch: 064, best_acc1: 67.680%
2022-06-30 15:39:39 - epoch 065 lr: 0.044773
2022-06-30 15:40:19 - train: epoch 0065, iter [00100, 05004], lr: 0.044748, loss: 1.5274
2022-06-30 15:40:51 - train: epoch 0065, iter [00200, 05004], lr: 0.044722, loss: 1.5289
2022-06-30 15:41:23 - train: epoch 0065, iter [00300, 05004], lr: 0.044696, loss: 1.4067
2022-06-30 15:41:55 - train: epoch 0065, iter [00400, 05004], lr: 0.044670, loss: 1.6938
2022-06-30 15:42:28 - train: epoch 0065, iter [00500, 05004], lr: 0.044644, loss: 1.4529
2022-06-30 15:43:01 - train: epoch 0065, iter [00600, 05004], lr: 0.044618, loss: 1.5578
2022-06-30 15:43:33 - train: epoch 0065, iter [00700, 05004], lr: 0.044592, loss: 1.5276
2022-06-30 15:44:06 - train: epoch 0065, iter [00800, 05004], lr: 0.044565, loss: 1.4779
2022-06-30 15:44:39 - train: epoch 0065, iter [00900, 05004], lr: 0.044539, loss: 1.4771
2022-06-30 15:45:11 - train: epoch 0065, iter [01000, 05004], lr: 0.044513, loss: 1.4646
2022-06-30 15:45:43 - train: epoch 0065, iter [01100, 05004], lr: 0.044487, loss: 1.4182
2022-06-30 15:46:16 - train: epoch 0065, iter [01200, 05004], lr: 0.044461, loss: 1.7654
2022-06-30 15:46:48 - train: epoch 0065, iter [01300, 05004], lr: 0.044435, loss: 1.4739
2022-06-30 15:47:21 - train: epoch 0065, iter [01400, 05004], lr: 0.044410, loss: 1.3927
2022-06-30 15:47:54 - train: epoch 0065, iter [01500, 05004], lr: 0.044384, loss: 1.4193
2022-06-30 15:48:26 - train: epoch 0065, iter [01600, 05004], lr: 0.044358, loss: 1.5391
2022-06-30 15:48:59 - train: epoch 0065, iter [01700, 05004], lr: 0.044332, loss: 1.4785
2022-06-30 15:49:31 - train: epoch 0065, iter [01800, 05004], lr: 0.044306, loss: 1.4948
2022-06-30 15:50:04 - train: epoch 0065, iter [01900, 05004], lr: 0.044280, loss: 1.3096
2022-06-30 15:50:36 - train: epoch 0065, iter [02000, 05004], lr: 0.044254, loss: 1.5653
2022-06-30 15:51:09 - train: epoch 0065, iter [02100, 05004], lr: 0.044228, loss: 1.4220
2022-06-30 15:51:42 - train: epoch 0065, iter [02200, 05004], lr: 0.044202, loss: 1.6383
2022-06-30 15:52:14 - train: epoch 0065, iter [02300, 05004], lr: 0.044176, loss: 1.5095
2022-06-30 15:52:47 - train: epoch 0065, iter [02400, 05004], lr: 0.044150, loss: 1.4286
2022-06-30 15:53:20 - train: epoch 0065, iter [02500, 05004], lr: 0.044124, loss: 1.5070
2022-06-30 15:53:52 - train: epoch 0065, iter [02600, 05004], lr: 0.044098, loss: 1.7258
2022-06-30 15:54:25 - train: epoch 0065, iter [02700, 05004], lr: 0.044072, loss: 1.6133
2022-06-30 15:54:57 - train: epoch 0065, iter [02800, 05004], lr: 0.044046, loss: 1.4430
2022-06-30 15:55:30 - train: epoch 0065, iter [02900, 05004], lr: 0.044020, loss: 1.5735
2022-06-30 15:56:02 - train: epoch 0065, iter [03000, 05004], lr: 0.043994, loss: 1.4009
2022-06-30 15:56:34 - train: epoch 0065, iter [03100, 05004], lr: 0.043968, loss: 1.5512
2022-06-30 15:57:07 - train: epoch 0065, iter [03200, 05004], lr: 0.043942, loss: 1.6670
2022-06-30 15:57:40 - train: epoch 0065, iter [03300, 05004], lr: 0.043916, loss: 1.5241
2022-06-30 15:58:12 - train: epoch 0065, iter [03400, 05004], lr: 0.043890, loss: 1.4156
2022-06-30 15:58:45 - train: epoch 0065, iter [03500, 05004], lr: 0.043864, loss: 1.8343
2022-06-30 15:59:17 - train: epoch 0065, iter [03600, 05004], lr: 0.043838, loss: 1.5205
2022-06-30 15:59:50 - train: epoch 0065, iter [03700, 05004], lr: 0.043812, loss: 1.4964
2022-06-30 16:00:22 - train: epoch 0065, iter [03800, 05004], lr: 0.043786, loss: 1.4099
2022-06-30 16:00:55 - train: epoch 0065, iter [03900, 05004], lr: 0.043760, loss: 1.7015
2022-06-30 16:01:27 - train: epoch 0065, iter [04000, 05004], lr: 0.043734, loss: 1.6867
2022-06-30 16:02:00 - train: epoch 0065, iter [04100, 05004], lr: 0.043708, loss: 1.3122
2022-06-30 16:02:32 - train: epoch 0065, iter [04200, 05004], lr: 0.043682, loss: 1.5811
2022-06-30 16:03:05 - train: epoch 0065, iter [04300, 05004], lr: 0.043656, loss: 1.4624
2022-06-30 16:03:38 - train: epoch 0065, iter [04400, 05004], lr: 0.043630, loss: 1.5757
2022-06-30 16:04:11 - train: epoch 0065, iter [04500, 05004], lr: 0.043604, loss: 1.6456
2022-06-30 16:04:44 - train: epoch 0065, iter [04600, 05004], lr: 0.043578, loss: 1.5090
2022-06-30 16:05:16 - train: epoch 0065, iter [04700, 05004], lr: 0.043553, loss: 1.6250
2022-06-30 16:05:49 - train: epoch 0065, iter [04800, 05004], lr: 0.043527, loss: 1.2522
2022-06-30 16:06:22 - train: epoch 0065, iter [04900, 05004], lr: 0.043501, loss: 1.4196
2022-06-30 16:06:54 - train: epoch 0065, iter [05000, 05004], lr: 0.043475, loss: 1.5779
2022-06-30 16:06:56 - train: epoch 065, train_loss: 1.5292
2022-06-30 16:08:11 - eval: epoch: 065, acc1: 67.920%, acc5: 88.576%, test_loss: 1.3175, per_image_load_time: 2.189ms, per_image_inference_time: 0.627ms
2022-06-30 16:08:12 - until epoch: 065, best_acc1: 67.920%
2022-06-30 16:08:12 - epoch 066 lr: 0.043473
2022-06-30 16:08:51 - train: epoch 0066, iter [00100, 05004], lr: 0.043448, loss: 1.3412
2022-06-30 16:09:23 - train: epoch 0066, iter [00200, 05004], lr: 0.043422, loss: 1.6286
2022-06-30 16:09:56 - train: epoch 0066, iter [00300, 05004], lr: 0.043396, loss: 1.3427
2022-06-30 16:10:28 - train: epoch 0066, iter [00400, 05004], lr: 0.043370, loss: 1.2893
2022-06-30 16:11:01 - train: epoch 0066, iter [00500, 05004], lr: 0.043344, loss: 1.5187
2022-06-30 16:11:33 - train: epoch 0066, iter [00600, 05004], lr: 0.043318, loss: 1.4560
2022-06-30 16:12:06 - train: epoch 0066, iter [00700, 05004], lr: 0.043292, loss: 1.4059
2022-06-30 16:12:38 - train: epoch 0066, iter [00800, 05004], lr: 0.043266, loss: 1.9017
2022-06-30 16:13:11 - train: epoch 0066, iter [00900, 05004], lr: 0.043240, loss: 1.6363
2022-06-30 16:13:44 - train: epoch 0066, iter [01000, 05004], lr: 0.043214, loss: 1.6581
2022-06-30 16:14:16 - train: epoch 0066, iter [01100, 05004], lr: 0.043189, loss: 1.5991
2022-06-30 16:14:49 - train: epoch 0066, iter [01200, 05004], lr: 0.043163, loss: 1.6293
2022-06-30 16:15:21 - train: epoch 0066, iter [01300, 05004], lr: 0.043137, loss: 1.6746
2022-06-30 16:15:54 - train: epoch 0066, iter [01400, 05004], lr: 0.043111, loss: 1.3214
2022-06-30 16:16:27 - train: epoch 0066, iter [01500, 05004], lr: 0.043085, loss: 1.5447
2022-06-30 16:16:59 - train: epoch 0066, iter [01600, 05004], lr: 0.043059, loss: 1.5263
2022-06-30 16:17:32 - train: epoch 0066, iter [01700, 05004], lr: 0.043033, loss: 1.5270
2022-06-30 16:18:05 - train: epoch 0066, iter [01800, 05004], lr: 0.043007, loss: 1.4398
2022-06-30 16:18:37 - train: epoch 0066, iter [01900, 05004], lr: 0.042981, loss: 1.7882
2022-06-30 16:19:10 - train: epoch 0066, iter [02000, 05004], lr: 0.042955, loss: 1.6107
2022-06-30 16:19:43 - train: epoch 0066, iter [02100, 05004], lr: 0.042929, loss: 1.5288
2022-06-30 16:20:16 - train: epoch 0066, iter [02200, 05004], lr: 0.042904, loss: 1.4859
2022-06-30 16:20:49 - train: epoch 0066, iter [02300, 05004], lr: 0.042878, loss: 1.6851
2022-06-30 16:21:22 - train: epoch 0066, iter [02400, 05004], lr: 0.042852, loss: 1.4417
2022-06-30 16:21:54 - train: epoch 0066, iter [02500, 05004], lr: 0.042826, loss: 1.5199
2022-06-30 16:22:27 - train: epoch 0066, iter [02600, 05004], lr: 0.042800, loss: 1.4735
2022-06-30 16:23:00 - train: epoch 0066, iter [02700, 05004], lr: 0.042774, loss: 1.8105
2022-06-30 16:23:33 - train: epoch 0066, iter [02800, 05004], lr: 0.042748, loss: 1.6592
2022-06-30 16:24:06 - train: epoch 0066, iter [02900, 05004], lr: 0.042722, loss: 1.5173
2022-06-30 16:24:39 - train: epoch 0066, iter [03000, 05004], lr: 0.042696, loss: 1.4761
2022-06-30 16:25:12 - train: epoch 0066, iter [03100, 05004], lr: 0.042671, loss: 1.5762
2022-06-30 16:25:45 - train: epoch 0066, iter [03200, 05004], lr: 0.042645, loss: 1.3839
2022-06-30 16:26:17 - train: epoch 0066, iter [03300, 05004], lr: 0.042619, loss: 1.4066
2022-06-30 16:26:50 - train: epoch 0066, iter [03400, 05004], lr: 0.042593, loss: 1.7617
2022-06-30 16:27:23 - train: epoch 0066, iter [03500, 05004], lr: 0.042567, loss: 1.6196
2022-06-30 16:27:57 - train: epoch 0066, iter [03600, 05004], lr: 0.042541, loss: 1.6393
2022-06-30 16:28:29 - train: epoch 0066, iter [03700, 05004], lr: 0.042515, loss: 1.5546
2022-06-30 16:29:03 - train: epoch 0066, iter [03800, 05004], lr: 0.042490, loss: 1.3198
2022-06-30 16:29:36 - train: epoch 0066, iter [03900, 05004], lr: 0.042464, loss: 1.3511
2022-06-30 16:30:09 - train: epoch 0066, iter [04000, 05004], lr: 0.042438, loss: 1.7701
2022-06-30 16:30:42 - train: epoch 0066, iter [04100, 05004], lr: 0.042412, loss: 1.3192
2022-06-30 16:31:15 - train: epoch 0066, iter [04200, 05004], lr: 0.042386, loss: 1.2909
2022-06-30 16:31:48 - train: epoch 0066, iter [04300, 05004], lr: 0.042360, loss: 1.3331
2022-06-30 16:32:21 - train: epoch 0066, iter [04400, 05004], lr: 0.042334, loss: 1.4834
2022-06-30 16:32:54 - train: epoch 0066, iter [04500, 05004], lr: 0.042309, loss: 1.6560
2022-06-30 16:33:27 - train: epoch 0066, iter [04600, 05004], lr: 0.042283, loss: 1.8184
2022-06-30 16:34:00 - train: epoch 0066, iter [04700, 05004], lr: 0.042257, loss: 1.4567
2022-06-30 16:34:33 - train: epoch 0066, iter [04800, 05004], lr: 0.042231, loss: 1.5397
2022-06-30 16:35:07 - train: epoch 0066, iter [04900, 05004], lr: 0.042205, loss: 1.5151
2022-06-30 16:35:40 - train: epoch 0066, iter [05000, 05004], lr: 0.042179, loss: 1.4068
2022-06-30 16:35:41 - train: epoch 066, train_loss: 1.5170
2022-06-30 16:36:58 - eval: epoch: 066, acc1: 67.662%, acc5: 88.616%, test_loss: 1.3193, per_image_load_time: 2.079ms, per_image_inference_time: 0.635ms
2022-06-30 16:36:59 - until epoch: 066, best_acc1: 67.920%
2022-06-30 16:36:59 - epoch 067 lr: 0.042178
2022-06-30 16:37:38 - train: epoch 0067, iter [00100, 05004], lr: 0.042152, loss: 1.5453
2022-06-30 16:38:11 - train: epoch 0067, iter [00200, 05004], lr: 0.042127, loss: 1.3891
2022-06-30 16:38:43 - train: epoch 0067, iter [00300, 05004], lr: 0.042101, loss: 1.7405
2022-06-30 16:39:16 - train: epoch 0067, iter [00400, 05004], lr: 0.042075, loss: 1.5598
2022-06-30 16:39:49 - train: epoch 0067, iter [00500, 05004], lr: 0.042049, loss: 1.3528
2022-06-30 16:40:21 - train: epoch 0067, iter [00600, 05004], lr: 0.042023, loss: 1.3131
2022-06-30 16:40:54 - train: epoch 0067, iter [00700, 05004], lr: 0.041997, loss: 1.5679
2022-06-30 16:41:27 - train: epoch 0067, iter [00800, 05004], lr: 0.041972, loss: 1.5587
2022-06-30 16:42:00 - train: epoch 0067, iter [00900, 05004], lr: 0.041946, loss: 1.6574
2022-06-30 16:42:33 - train: epoch 0067, iter [01000, 05004], lr: 0.041920, loss: 1.3884
2022-06-30 16:43:06 - train: epoch 0067, iter [01100, 05004], lr: 0.041894, loss: 1.5078
2022-06-30 16:43:39 - train: epoch 0067, iter [01200, 05004], lr: 0.041868, loss: 1.4742
2022-06-30 16:44:13 - train: epoch 0067, iter [01300, 05004], lr: 0.041843, loss: 1.7402
2022-06-30 16:44:46 - train: epoch 0067, iter [01400, 05004], lr: 0.041817, loss: 1.5319
2022-06-30 16:45:20 - train: epoch 0067, iter [01500, 05004], lr: 0.041791, loss: 1.4343
2022-06-30 16:45:54 - train: epoch 0067, iter [01600, 05004], lr: 0.041765, loss: 1.5225
2022-06-30 16:46:28 - train: epoch 0067, iter [01700, 05004], lr: 0.041739, loss: 1.3669
2022-06-30 16:47:02 - train: epoch 0067, iter [01800, 05004], lr: 0.041714, loss: 1.6478
2022-06-30 16:47:35 - train: epoch 0067, iter [01900, 05004], lr: 0.041688, loss: 1.4887
2022-06-30 16:48:08 - train: epoch 0067, iter [02000, 05004], lr: 0.041662, loss: 1.5697
2022-06-30 16:48:42 - train: epoch 0067, iter [02100, 05004], lr: 0.041636, loss: 1.2209
2022-06-30 16:49:15 - train: epoch 0067, iter [02200, 05004], lr: 0.041610, loss: 1.5684
2022-06-30 16:49:48 - train: epoch 0067, iter [02300, 05004], lr: 0.041585, loss: 1.4158
2022-06-30 16:50:21 - train: epoch 0067, iter [02400, 05004], lr: 0.041559, loss: 1.5216
2022-06-30 16:50:54 - train: epoch 0067, iter [02500, 05004], lr: 0.041533, loss: 1.3970
2022-06-30 16:51:27 - train: epoch 0067, iter [02600, 05004], lr: 0.041507, loss: 1.4264
2022-06-30 16:52:00 - train: epoch 0067, iter [02700, 05004], lr: 0.041481, loss: 1.4655
2022-06-30 16:52:33 - train: epoch 0067, iter [02800, 05004], lr: 0.041456, loss: 1.6593
2022-06-30 16:53:05 - train: epoch 0067, iter [02900, 05004], lr: 0.041430, loss: 1.4477
2022-06-30 16:53:38 - train: epoch 0067, iter [03000, 05004], lr: 0.041404, loss: 1.4798
2022-06-30 16:54:10 - train: epoch 0067, iter [03100, 05004], lr: 0.041378, loss: 1.3573
2022-06-30 16:54:43 - train: epoch 0067, iter [03200, 05004], lr: 0.041353, loss: 1.6669
2022-06-30 16:55:16 - train: epoch 0067, iter [03300, 05004], lr: 0.041327, loss: 1.3609
2022-06-30 16:55:49 - train: epoch 0067, iter [03400, 05004], lr: 0.041301, loss: 1.4392
2022-06-30 16:56:21 - train: epoch 0067, iter [03500, 05004], lr: 0.041275, loss: 1.4157
2022-06-30 16:56:54 - train: epoch 0067, iter [03600, 05004], lr: 0.041250, loss: 1.5871
2022-06-30 16:57:27 - train: epoch 0067, iter [03700, 05004], lr: 0.041224, loss: 1.6360
2022-06-30 16:57:59 - train: epoch 0067, iter [03800, 05004], lr: 0.041198, loss: 1.4657
2022-06-30 16:58:32 - train: epoch 0067, iter [03900, 05004], lr: 0.041172, loss: 1.5566
2022-06-30 16:59:05 - train: epoch 0067, iter [04000, 05004], lr: 0.041147, loss: 1.5660
2022-06-30 16:59:37 - train: epoch 0067, iter [04100, 05004], lr: 0.041121, loss: 1.6281
2022-06-30 17:00:10 - train: epoch 0067, iter [04200, 05004], lr: 0.041095, loss: 1.7184
2022-06-30 17:00:43 - train: epoch 0067, iter [04300, 05004], lr: 0.041069, loss: 1.3996
2022-06-30 17:01:16 - train: epoch 0067, iter [04400, 05004], lr: 0.041044, loss: 1.5349
2022-06-30 17:01:49 - train: epoch 0067, iter [04500, 05004], lr: 0.041018, loss: 1.4254
2022-06-30 17:02:22 - train: epoch 0067, iter [04600, 05004], lr: 0.040992, loss: 1.3336
2022-06-30 17:02:55 - train: epoch 0067, iter [04700, 05004], lr: 0.040966, loss: 1.4775
2022-06-30 17:03:28 - train: epoch 0067, iter [04800, 05004], lr: 0.040941, loss: 1.3181
2022-06-30 17:04:01 - train: epoch 0067, iter [04900, 05004], lr: 0.040915, loss: 1.5757
2022-06-30 17:04:34 - train: epoch 0067, iter [05000, 05004], lr: 0.040889, loss: 1.5969
2022-06-30 17:04:35 - train: epoch 067, train_loss: 1.5039
2022-06-30 17:05:51 - eval: epoch: 067, acc1: 67.914%, acc5: 88.646%, test_loss: 1.3049, per_image_load_time: 2.215ms, per_image_inference_time: 0.640ms
2022-06-30 17:05:52 - until epoch: 067, best_acc1: 67.920%
2022-06-30 17:05:52 - epoch 068 lr: 0.040888
2022-06-30 17:06:32 - train: epoch 0068, iter [00100, 05004], lr: 0.040863, loss: 1.5083
2022-06-30 17:07:04 - train: epoch 0068, iter [00200, 05004], lr: 0.040837, loss: 1.4831
2022-06-30 17:07:36 - train: epoch 0068, iter [00300, 05004], lr: 0.040811, loss: 1.5588
2022-06-30 17:08:08 - train: epoch 0068, iter [00400, 05004], lr: 0.040785, loss: 1.4260
2022-06-30 17:08:40 - train: epoch 0068, iter [00500, 05004], lr: 0.040760, loss: 1.4556
2022-06-30 17:09:13 - train: epoch 0068, iter [00600, 05004], lr: 0.040734, loss: 1.4759
2022-06-30 17:09:45 - train: epoch 0068, iter [00700, 05004], lr: 0.040708, loss: 1.8282
2022-06-30 17:10:17 - train: epoch 0068, iter [00800, 05004], lr: 0.040683, loss: 1.5325
2022-06-30 17:10:50 - train: epoch 0068, iter [00900, 05004], lr: 0.040657, loss: 1.4155
2022-06-30 17:11:22 - train: epoch 0068, iter [01000, 05004], lr: 0.040631, loss: 1.4450
2022-06-30 17:11:54 - train: epoch 0068, iter [01100, 05004], lr: 0.040605, loss: 1.7817
2022-06-30 17:12:27 - train: epoch 0068, iter [01200, 05004], lr: 0.040580, loss: 1.4010
2022-06-30 17:13:00 - train: epoch 0068, iter [01300, 05004], lr: 0.040554, loss: 1.3259
2022-06-30 17:13:32 - train: epoch 0068, iter [01400, 05004], lr: 0.040528, loss: 1.5030
2022-06-30 17:14:05 - train: epoch 0068, iter [01500, 05004], lr: 0.040503, loss: 1.5781
2022-06-30 17:14:38 - train: epoch 0068, iter [01600, 05004], lr: 0.040477, loss: 1.4782
2022-06-30 17:15:11 - train: epoch 0068, iter [01700, 05004], lr: 0.040451, loss: 1.6193
2022-06-30 17:15:43 - train: epoch 0068, iter [01800, 05004], lr: 0.040426, loss: 1.5980
2022-06-30 17:16:16 - train: epoch 0068, iter [01900, 05004], lr: 0.040400, loss: 1.5644
2022-06-30 17:16:49 - train: epoch 0068, iter [02000, 05004], lr: 0.040374, loss: 1.5066
2022-06-30 17:17:22 - train: epoch 0068, iter [02100, 05004], lr: 0.040349, loss: 1.4355
2022-06-30 17:17:54 - train: epoch 0068, iter [02200, 05004], lr: 0.040323, loss: 1.5795
2022-06-30 17:18:27 - train: epoch 0068, iter [02300, 05004], lr: 0.040297, loss: 1.4265
2022-06-30 17:19:00 - train: epoch 0068, iter [02400, 05004], lr: 0.040272, loss: 1.5398
2022-06-30 17:19:32 - train: epoch 0068, iter [02500, 05004], lr: 0.040246, loss: 1.5190
2022-06-30 17:20:05 - train: epoch 0068, iter [02600, 05004], lr: 0.040220, loss: 1.4676
2022-06-30 17:20:38 - train: epoch 0068, iter [02700, 05004], lr: 0.040195, loss: 1.6251
2022-06-30 17:21:11 - train: epoch 0068, iter [02800, 05004], lr: 0.040169, loss: 1.7012
2022-06-30 17:21:44 - train: epoch 0068, iter [02900, 05004], lr: 0.040143, loss: 1.6411
2022-06-30 17:22:17 - train: epoch 0068, iter [03000, 05004], lr: 0.040118, loss: 1.6666
2022-06-30 17:22:49 - train: epoch 0068, iter [03100, 05004], lr: 0.040092, loss: 1.3311
2022-06-30 17:23:22 - train: epoch 0068, iter [03200, 05004], lr: 0.040066, loss: 1.6200
2022-06-30 17:23:55 - train: epoch 0068, iter [03300, 05004], lr: 0.040041, loss: 1.4616
2022-06-30 17:24:28 - train: epoch 0068, iter [03400, 05004], lr: 0.040015, loss: 1.3703
2022-06-30 17:25:01 - train: epoch 0068, iter [03500, 05004], lr: 0.039990, loss: 1.5567
2022-06-30 17:25:34 - train: epoch 0068, iter [03600, 05004], lr: 0.039964, loss: 1.4045
2022-06-30 17:26:07 - train: epoch 0068, iter [03700, 05004], lr: 0.039938, loss: 1.5330
2022-06-30 17:26:40 - train: epoch 0068, iter [03800, 05004], lr: 0.039913, loss: 1.6243
2022-06-30 17:27:14 - train: epoch 0068, iter [03900, 05004], lr: 0.039887, loss: 1.6188
2022-06-30 17:27:47 - train: epoch 0068, iter [04000, 05004], lr: 0.039861, loss: 1.6342
2022-06-30 17:28:21 - train: epoch 0068, iter [04100, 05004], lr: 0.039836, loss: 1.2920
2022-06-30 17:28:54 - train: epoch 0068, iter [04200, 05004], lr: 0.039810, loss: 1.6463
2022-06-30 17:29:27 - train: epoch 0068, iter [04300, 05004], lr: 0.039785, loss: 1.5769
2022-06-30 17:30:01 - train: epoch 0068, iter [04400, 05004], lr: 0.039759, loss: 1.5190
2022-06-30 17:30:34 - train: epoch 0068, iter [04500, 05004], lr: 0.039733, loss: 1.5355
2022-06-30 17:31:08 - train: epoch 0068, iter [04600, 05004], lr: 0.039708, loss: 1.6129
2022-06-30 17:31:41 - train: epoch 0068, iter [04700, 05004], lr: 0.039682, loss: 1.8452
2022-06-30 17:32:14 - train: epoch 0068, iter [04800, 05004], lr: 0.039657, loss: 1.6883
2022-06-30 17:32:48 - train: epoch 0068, iter [04900, 05004], lr: 0.039631, loss: 1.7886
2022-06-30 17:33:21 - train: epoch 0068, iter [05000, 05004], lr: 0.039605, loss: 1.4601
2022-06-30 17:33:23 - train: epoch 068, train_loss: 1.4928
2022-06-30 17:34:41 - eval: epoch: 068, acc1: 68.310%, acc5: 88.974%, test_loss: 1.2845, per_image_load_time: 2.417ms, per_image_inference_time: 0.598ms
2022-06-30 17:34:42 - until epoch: 068, best_acc1: 68.310%
2022-06-30 17:34:42 - epoch 069 lr: 0.039604
2022-06-30 17:35:22 - train: epoch 0069, iter [00100, 05004], lr: 0.039579, loss: 1.6533
2022-06-30 17:35:55 - train: epoch 0069, iter [00200, 05004], lr: 0.039553, loss: 1.7280
2022-06-30 17:36:28 - train: epoch 0069, iter [00300, 05004], lr: 0.039528, loss: 1.3594
2022-06-30 17:37:02 - train: epoch 0069, iter [00400, 05004], lr: 0.039502, loss: 1.3745
2022-06-30 17:37:35 - train: epoch 0069, iter [00500, 05004], lr: 0.039477, loss: 1.3614
2022-06-30 17:38:08 - train: epoch 0069, iter [00600, 05004], lr: 0.039451, loss: 1.2711
2022-06-30 17:38:41 - train: epoch 0069, iter [00700, 05004], lr: 0.039425, loss: 1.4287
2022-06-30 17:39:14 - train: epoch 0069, iter [00800, 05004], lr: 0.039400, loss: 1.5552
2022-06-30 17:39:47 - train: epoch 0069, iter [00900, 05004], lr: 0.039374, loss: 1.3868
2022-06-30 17:40:20 - train: epoch 0069, iter [01000, 05004], lr: 0.039349, loss: 1.3547
2022-06-30 17:40:54 - train: epoch 0069, iter [01100, 05004], lr: 0.039323, loss: 1.4628
2022-06-30 17:41:27 - train: epoch 0069, iter [01200, 05004], lr: 0.039298, loss: 1.3870
2022-06-30 17:42:01 - train: epoch 0069, iter [01300, 05004], lr: 0.039272, loss: 1.8060
2022-06-30 17:42:35 - train: epoch 0069, iter [01400, 05004], lr: 0.039246, loss: 1.5529
2022-06-30 17:43:08 - train: epoch 0069, iter [01500, 05004], lr: 0.039221, loss: 1.4772
2022-06-30 17:43:41 - train: epoch 0069, iter [01600, 05004], lr: 0.039195, loss: 1.6223
2022-06-30 17:44:15 - train: epoch 0069, iter [01700, 05004], lr: 0.039170, loss: 1.5599
2022-06-30 17:44:48 - train: epoch 0069, iter [01800, 05004], lr: 0.039144, loss: 1.2654
2022-06-30 17:45:21 - train: epoch 0069, iter [01900, 05004], lr: 0.039119, loss: 1.4251
2022-06-30 17:45:55 - train: epoch 0069, iter [02000, 05004], lr: 0.039093, loss: 1.4513
2022-06-30 17:46:28 - train: epoch 0069, iter [02100, 05004], lr: 0.039068, loss: 1.4928
2022-06-30 17:47:01 - train: epoch 0069, iter [02200, 05004], lr: 0.039042, loss: 1.5922
2022-06-30 17:47:34 - train: epoch 0069, iter [02300, 05004], lr: 0.039017, loss: 1.5178
2022-06-30 17:48:07 - train: epoch 0069, iter [02400, 05004], lr: 0.038991, loss: 1.5831
2022-06-30 17:48:40 - train: epoch 0069, iter [02500, 05004], lr: 0.038966, loss: 1.4850
2022-06-30 17:49:13 - train: epoch 0069, iter [02600, 05004], lr: 0.038940, loss: 1.6525
2022-06-30 17:49:46 - train: epoch 0069, iter [02700, 05004], lr: 0.038915, loss: 1.7618
2022-06-30 17:50:19 - train: epoch 0069, iter [02800, 05004], lr: 0.038889, loss: 1.5176
2022-06-30 17:50:53 - train: epoch 0069, iter [02900, 05004], lr: 0.038864, loss: 1.2800
2022-06-30 17:51:26 - train: epoch 0069, iter [03000, 05004], lr: 0.038838, loss: 1.4367
2022-06-30 17:51:59 - train: epoch 0069, iter [03100, 05004], lr: 0.038813, loss: 1.4640
2022-06-30 17:52:32 - train: epoch 0069, iter [03200, 05004], lr: 0.038787, loss: 1.4426
2022-06-30 17:53:06 - train: epoch 0069, iter [03300, 05004], lr: 0.038762, loss: 1.3748
2022-06-30 17:53:39 - train: epoch 0069, iter [03400, 05004], lr: 0.038736, loss: 1.3413
2022-06-30 17:54:12 - train: epoch 0069, iter [03500, 05004], lr: 0.038711, loss: 1.4041
2022-06-30 17:54:45 - train: epoch 0069, iter [03600, 05004], lr: 0.038685, loss: 1.2808
2022-06-30 17:55:18 - train: epoch 0069, iter [03700, 05004], lr: 0.038660, loss: 1.6176
2022-06-30 17:55:51 - train: epoch 0069, iter [03800, 05004], lr: 0.038634, loss: 1.5864
2022-06-30 17:56:24 - train: epoch 0069, iter [03900, 05004], lr: 0.038609, loss: 1.5658
2022-06-30 17:56:57 - train: epoch 0069, iter [04000, 05004], lr: 0.038583, loss: 1.5935
2022-06-30 17:57:30 - train: epoch 0069, iter [04100, 05004], lr: 0.038558, loss: 1.6123
2022-06-30 17:58:03 - train: epoch 0069, iter [04200, 05004], lr: 0.038532, loss: 1.2359
2022-06-30 17:58:36 - train: epoch 0069, iter [04300, 05004], lr: 0.038507, loss: 1.4541
2022-06-30 17:59:10 - train: epoch 0069, iter [04400, 05004], lr: 0.038481, loss: 1.5780
2022-06-30 17:59:43 - train: epoch 0069, iter [04500, 05004], lr: 0.038456, loss: 1.4755
2022-06-30 18:00:16 - train: epoch 0069, iter [04600, 05004], lr: 0.038431, loss: 1.7040
2022-06-30 18:00:49 - train: epoch 0069, iter [04700, 05004], lr: 0.038405, loss: 1.4906
2022-06-30 18:01:22 - train: epoch 0069, iter [04800, 05004], lr: 0.038380, loss: 1.4671
2022-06-30 18:01:56 - train: epoch 0069, iter [04900, 05004], lr: 0.038354, loss: 1.3236
2022-06-30 18:02:29 - train: epoch 0069, iter [05000, 05004], lr: 0.038329, loss: 1.6417
2022-06-30 18:02:30 - train: epoch 069, train_loss: 1.4790
2022-06-30 18:03:48 - eval: epoch: 069, acc1: 68.206%, acc5: 88.696%, test_loss: 1.2945, per_image_load_time: 1.446ms, per_image_inference_time: 0.627ms
2022-06-30 18:03:48 - until epoch: 069, best_acc1: 68.310%
2022-06-30 18:03:48 - epoch 070 lr: 0.038327
2022-06-30 18:04:30 - train: epoch 0070, iter [00100, 05004], lr: 0.038302, loss: 1.4915
2022-06-30 18:05:02 - train: epoch 0070, iter [00200, 05004], lr: 0.038277, loss: 1.4592
2022-06-30 18:05:35 - train: epoch 0070, iter [00300, 05004], lr: 0.038251, loss: 1.6039
2022-06-30 18:06:08 - train: epoch 0070, iter [00400, 05004], lr: 0.038226, loss: 1.3743
2022-06-30 18:06:40 - train: epoch 0070, iter [00500, 05004], lr: 0.038201, loss: 1.4658
2022-06-30 18:07:13 - train: epoch 0070, iter [00600, 05004], lr: 0.038175, loss: 1.4637
2022-06-30 18:07:46 - train: epoch 0070, iter [00700, 05004], lr: 0.038150, loss: 1.4721
2022-06-30 18:08:19 - train: epoch 0070, iter [00800, 05004], lr: 0.038124, loss: 1.3084
2022-06-30 18:08:51 - train: epoch 0070, iter [00900, 05004], lr: 0.038099, loss: 1.5189
2022-06-30 18:09:24 - train: epoch 0070, iter [01000, 05004], lr: 0.038074, loss: 1.3580
2022-06-30 18:09:57 - train: epoch 0070, iter [01100, 05004], lr: 0.038048, loss: 1.7978
2022-06-30 18:10:30 - train: epoch 0070, iter [01200, 05004], lr: 0.038023, loss: 1.2339
2022-06-30 18:11:03 - train: epoch 0070, iter [01300, 05004], lr: 0.037997, loss: 1.4855
2022-06-30 18:11:36 - train: epoch 0070, iter [01400, 05004], lr: 0.037972, loss: 1.4650
2022-06-30 18:12:10 - train: epoch 0070, iter [01500, 05004], lr: 0.037947, loss: 1.3639
2022-06-30 18:12:43 - train: epoch 0070, iter [01600, 05004], lr: 0.037921, loss: 1.5315
2022-06-30 18:13:17 - train: epoch 0070, iter [01700, 05004], lr: 0.037896, loss: 1.5599
2022-06-30 18:13:50 - train: epoch 0070, iter [01800, 05004], lr: 0.037870, loss: 1.3188
2022-06-30 18:14:23 - train: epoch 0070, iter [01900, 05004], lr: 0.037845, loss: 1.3535
2022-06-30 18:14:56 - train: epoch 0070, iter [02000, 05004], lr: 0.037820, loss: 1.5173
2022-06-30 18:15:30 - train: epoch 0070, iter [02100, 05004], lr: 0.037794, loss: 1.5515
2022-06-30 18:16:03 - train: epoch 0070, iter [02200, 05004], lr: 0.037769, loss: 1.5758
2022-06-30 18:16:37 - train: epoch 0070, iter [02300, 05004], lr: 0.037744, loss: 1.4322
2022-06-30 18:17:10 - train: epoch 0070, iter [02400, 05004], lr: 0.037718, loss: 1.5723
2022-06-30 18:17:43 - train: epoch 0070, iter [02500, 05004], lr: 0.037693, loss: 1.5375
2022-06-30 18:18:16 - train: epoch 0070, iter [02600, 05004], lr: 0.037667, loss: 1.3520
2022-06-30 18:18:49 - train: epoch 0070, iter [02700, 05004], lr: 0.037642, loss: 1.5465
2022-06-30 18:19:23 - train: epoch 0070, iter [02800, 05004], lr: 0.037617, loss: 1.4959
2022-06-30 18:19:56 - train: epoch 0070, iter [02900, 05004], lr: 0.037591, loss: 1.7498
2022-06-30 18:20:29 - train: epoch 0070, iter [03000, 05004], lr: 0.037566, loss: 1.3958
2022-06-30 18:21:03 - train: epoch 0070, iter [03100, 05004], lr: 0.037541, loss: 1.5216
2022-06-30 18:21:36 - train: epoch 0070, iter [03200, 05004], lr: 0.037515, loss: 1.6255
2022-06-30 18:22:09 - train: epoch 0070, iter [03300, 05004], lr: 0.037490, loss: 1.4479
2022-06-30 18:22:43 - train: epoch 0070, iter [03400, 05004], lr: 0.037465, loss: 1.5075
2022-06-30 18:23:15 - train: epoch 0070, iter [03500, 05004], lr: 0.037439, loss: 1.4386
2022-06-30 18:23:49 - train: epoch 0070, iter [03600, 05004], lr: 0.037414, loss: 1.5552
2022-06-30 18:24:22 - train: epoch 0070, iter [03700, 05004], lr: 0.037389, loss: 1.5028
2022-06-30 18:24:55 - train: epoch 0070, iter [03800, 05004], lr: 0.037364, loss: 1.4846
2022-06-30 18:25:28 - train: epoch 0070, iter [03900, 05004], lr: 0.037338, loss: 1.3216
2022-06-30 18:26:01 - train: epoch 0070, iter [04000, 05004], lr: 0.037313, loss: 1.3774
2022-06-30 18:26:34 - train: epoch 0070, iter [04100, 05004], lr: 0.037288, loss: 1.3993
2022-06-30 18:27:07 - train: epoch 0070, iter [04200, 05004], lr: 0.037262, loss: 1.5081
2022-06-30 18:27:40 - train: epoch 0070, iter [04300, 05004], lr: 0.037237, loss: 1.5822
2022-06-30 18:28:13 - train: epoch 0070, iter [04400, 05004], lr: 0.037212, loss: 1.4901
2022-06-30 18:28:46 - train: epoch 0070, iter [04500, 05004], lr: 0.037186, loss: 1.4973
2022-06-30 18:29:19 - train: epoch 0070, iter [04600, 05004], lr: 0.037161, loss: 1.6334
2022-06-30 18:29:52 - train: epoch 0070, iter [04700, 05004], lr: 0.037136, loss: 1.5539
2022-06-30 18:30:25 - train: epoch 0070, iter [04800, 05004], lr: 0.037111, loss: 1.5722
2022-06-30 18:30:58 - train: epoch 0070, iter [04900, 05004], lr: 0.037085, loss: 1.5601
2022-06-30 18:31:31 - train: epoch 0070, iter [05000, 05004], lr: 0.037060, loss: 1.4228
2022-06-30 18:31:32 - train: epoch 070, train_loss: 1.4687
2022-06-30 18:32:50 - eval: epoch: 070, acc1: 68.828%, acc5: 89.168%, test_loss: 1.2666, per_image_load_time: 2.296ms, per_image_inference_time: 0.631ms
2022-06-30 18:32:51 - until epoch: 070, best_acc1: 68.828%
2022-06-30 18:32:51 - epoch 071 lr: 0.037059
2022-06-30 18:33:31 - train: epoch 0071, iter [00100, 05004], lr: 0.037034, loss: 1.2845
2022-06-30 18:34:04 - train: epoch 0071, iter [00200, 05004], lr: 0.037009, loss: 1.4423
2022-06-30 18:34:37 - train: epoch 0071, iter [00300, 05004], lr: 0.036983, loss: 1.4668
2022-06-30 18:35:10 - train: epoch 0071, iter [00400, 05004], lr: 0.036958, loss: 1.5081
2022-06-30 18:35:43 - train: epoch 0071, iter [00500, 05004], lr: 0.036933, loss: 1.4656
2022-06-30 18:36:16 - train: epoch 0071, iter [00600, 05004], lr: 0.036908, loss: 1.5688
2022-06-30 18:36:48 - train: epoch 0071, iter [00700, 05004], lr: 0.036882, loss: 1.5602
2022-06-30 18:37:21 - train: epoch 0071, iter [00800, 05004], lr: 0.036857, loss: 1.4440
2022-06-30 18:37:54 - train: epoch 0071, iter [00900, 05004], lr: 0.036832, loss: 1.5316
2022-06-30 18:38:27 - train: epoch 0071, iter [01000, 05004], lr: 0.036807, loss: 1.5349
2022-06-30 18:39:00 - train: epoch 0071, iter [01100, 05004], lr: 0.036781, loss: 1.5878
2022-06-30 18:39:32 - train: epoch 0071, iter [01200, 05004], lr: 0.036756, loss: 1.4223
2022-06-30 18:40:06 - train: epoch 0071, iter [01300, 05004], lr: 0.036731, loss: 1.4590
2022-06-30 18:40:38 - train: epoch 0071, iter [01400, 05004], lr: 0.036706, loss: 1.4549
2022-06-30 18:41:12 - train: epoch 0071, iter [01500, 05004], lr: 0.036680, loss: 1.1691
2022-06-30 18:41:45 - train: epoch 0071, iter [01600, 05004], lr: 0.036655, loss: 1.3730
2022-06-30 18:42:18 - train: epoch 0071, iter [01700, 05004], lr: 0.036630, loss: 1.4036
2022-06-30 18:42:51 - train: epoch 0071, iter [01800, 05004], lr: 0.036605, loss: 1.4735
2022-06-30 18:43:23 - train: epoch 0071, iter [01900, 05004], lr: 0.036580, loss: 1.4107
2022-06-30 18:43:56 - train: epoch 0071, iter [02000, 05004], lr: 0.036554, loss: 1.5575
2022-06-30 18:44:30 - train: epoch 0071, iter [02100, 05004], lr: 0.036529, loss: 1.3125
2022-06-30 18:45:02 - train: epoch 0071, iter [02200, 05004], lr: 0.036504, loss: 1.3469
2022-06-30 18:45:35 - train: epoch 0071, iter [02300, 05004], lr: 0.036479, loss: 1.4120
2022-06-30 18:46:08 - train: epoch 0071, iter [02400, 05004], lr: 0.036454, loss: 1.4079
2022-06-30 18:46:41 - train: epoch 0071, iter [02500, 05004], lr: 0.036428, loss: 1.5766
2022-06-30 18:47:14 - train: epoch 0071, iter [02600, 05004], lr: 0.036403, loss: 1.2963
2022-06-30 18:47:47 - train: epoch 0071, iter [02700, 05004], lr: 0.036378, loss: 1.4492
2022-06-30 18:48:20 - train: epoch 0071, iter [02800, 05004], lr: 0.036353, loss: 1.5890
2022-06-30 18:48:54 - train: epoch 0071, iter [02900, 05004], lr: 0.036328, loss: 1.3948
2022-06-30 18:49:27 - train: epoch 0071, iter [03000, 05004], lr: 0.036303, loss: 1.6026
2022-06-30 18:50:00 - train: epoch 0071, iter [03100, 05004], lr: 0.036277, loss: 1.5241
2022-06-30 18:50:34 - train: epoch 0071, iter [03200, 05004], lr: 0.036252, loss: 1.3269
2022-06-30 18:51:07 - train: epoch 0071, iter [03300, 05004], lr: 0.036227, loss: 1.3804
2022-06-30 18:51:40 - train: epoch 0071, iter [03400, 05004], lr: 0.036202, loss: 1.3719
2022-06-30 18:52:13 - train: epoch 0071, iter [03500, 05004], lr: 0.036177, loss: 1.6264
2022-06-30 18:52:46 - train: epoch 0071, iter [03600, 05004], lr: 0.036152, loss: 1.6336
2022-06-30 18:53:19 - train: epoch 0071, iter [03700, 05004], lr: 0.036127, loss: 1.4423
2022-06-30 18:53:53 - train: epoch 0071, iter [03800, 05004], lr: 0.036101, loss: 1.4408
2022-06-30 18:54:26 - train: epoch 0071, iter [03900, 05004], lr: 0.036076, loss: 1.5132
2022-06-30 18:54:59 - train: epoch 0071, iter [04000, 05004], lr: 0.036051, loss: 1.7172
2022-06-30 18:55:33 - train: epoch 0071, iter [04100, 05004], lr: 0.036026, loss: 1.4505
2022-06-30 18:56:06 - train: epoch 0071, iter [04200, 05004], lr: 0.036001, loss: 1.5611
2022-06-30 18:56:39 - train: epoch 0071, iter [04300, 05004], lr: 0.035976, loss: 1.4503
2022-06-30 18:57:12 - train: epoch 0071, iter [04400, 05004], lr: 0.035951, loss: 1.4760
2022-06-30 18:57:46 - train: epoch 0071, iter [04500, 05004], lr: 0.035926, loss: 1.4211
2022-06-30 18:58:19 - train: epoch 0071, iter [04600, 05004], lr: 0.035901, loss: 1.4696
2022-06-30 18:58:52 - train: epoch 0071, iter [04700, 05004], lr: 0.035875, loss: 1.4388
2022-06-30 18:59:26 - train: epoch 0071, iter [04800, 05004], lr: 0.035850, loss: 1.3903
2022-06-30 18:59:59 - train: epoch 0071, iter [04900, 05004], lr: 0.035825, loss: 1.3260
2022-06-30 19:00:32 - train: epoch 0071, iter [05000, 05004], lr: 0.035800, loss: 1.3674
2022-06-30 19:00:34 - train: epoch 071, train_loss: 1.4545
2022-06-30 19:01:50 - eval: epoch: 071, acc1: 69.574%, acc5: 89.326%, test_loss: 1.2568, per_image_load_time: 2.112ms, per_image_inference_time: 0.634ms
2022-06-30 19:01:52 - until epoch: 071, best_acc1: 69.574%
2022-06-30 19:01:52 - epoch 072 lr: 0.035799
2022-06-30 19:02:32 - train: epoch 0072, iter [00100, 05004], lr: 0.035774, loss: 1.5341
2022-06-30 19:03:04 - train: epoch 0072, iter [00200, 05004], lr: 0.035749, loss: 1.2611
2022-06-30 19:03:38 - train: epoch 0072, iter [00300, 05004], lr: 0.035724, loss: 1.2524
2022-06-30 19:04:11 - train: epoch 0072, iter [00400, 05004], lr: 0.035699, loss: 1.3344
2022-06-30 19:04:44 - train: epoch 0072, iter [00500, 05004], lr: 0.035674, loss: 1.2544
2022-06-30 19:05:17 - train: epoch 0072, iter [00600, 05004], lr: 0.035649, loss: 1.3378
2022-06-30 19:05:50 - train: epoch 0072, iter [00700, 05004], lr: 0.035624, loss: 1.4592
2022-06-30 19:06:23 - train: epoch 0072, iter [00800, 05004], lr: 0.035599, loss: 1.6382
2022-06-30 19:06:57 - train: epoch 0072, iter [00900, 05004], lr: 0.035574, loss: 1.3006
2022-06-30 19:07:30 - train: epoch 0072, iter [01000, 05004], lr: 0.035549, loss: 1.2768
2022-06-30 19:08:03 - train: epoch 0072, iter [01100, 05004], lr: 0.035524, loss: 1.5927
2022-06-30 19:08:37 - train: epoch 0072, iter [01200, 05004], lr: 0.035499, loss: 1.3370
2022-06-30 19:09:10 - train: epoch 0072, iter [01300, 05004], lr: 0.035474, loss: 1.3552
2022-06-30 19:09:43 - train: epoch 0072, iter [01400, 05004], lr: 0.035448, loss: 1.5138
2022-06-30 19:10:17 - train: epoch 0072, iter [01500, 05004], lr: 0.035423, loss: 1.4687
2022-06-30 19:10:50 - train: epoch 0072, iter [01600, 05004], lr: 0.035398, loss: 1.4491
2022-06-30 19:11:24 - train: epoch 0072, iter [01700, 05004], lr: 0.035373, loss: 1.2488
2022-06-30 19:11:57 - train: epoch 0072, iter [01800, 05004], lr: 0.035348, loss: 1.3232
2022-06-30 19:12:30 - train: epoch 0072, iter [01900, 05004], lr: 0.035323, loss: 1.2754
2022-06-30 19:13:03 - train: epoch 0072, iter [02000, 05004], lr: 0.035298, loss: 1.5388
2022-06-30 19:13:37 - train: epoch 0072, iter [02100, 05004], lr: 0.035273, loss: 1.4842
2022-06-30 19:14:10 - train: epoch 0072, iter [02200, 05004], lr: 0.035248, loss: 1.5664
2022-06-30 19:14:42 - train: epoch 0072, iter [02300, 05004], lr: 0.035223, loss: 1.5628
2022-06-30 19:15:16 - train: epoch 0072, iter [02400, 05004], lr: 0.035198, loss: 1.3765
2022-06-30 19:15:49 - train: epoch 0072, iter [02500, 05004], lr: 0.035173, loss: 1.3702
2022-06-30 19:16:22 - train: epoch 0072, iter [02600, 05004], lr: 0.035148, loss: 1.3363
2022-06-30 19:16:55 - train: epoch 0072, iter [02700, 05004], lr: 0.035123, loss: 1.3620
2022-06-30 19:17:28 - train: epoch 0072, iter [02800, 05004], lr: 0.035098, loss: 1.4336
2022-06-30 19:18:01 - train: epoch 0072, iter [02900, 05004], lr: 0.035074, loss: 1.4288
2022-06-30 19:18:34 - train: epoch 0072, iter [03000, 05004], lr: 0.035049, loss: 1.4290
2022-06-30 19:19:07 - train: epoch 0072, iter [03100, 05004], lr: 0.035024, loss: 1.5859
2022-06-30 19:19:41 - train: epoch 0072, iter [03200, 05004], lr: 0.034999, loss: 1.3246
2022-06-30 19:20:14 - train: epoch 0072, iter [03300, 05004], lr: 0.034974, loss: 1.3059
2022-06-30 19:20:47 - train: epoch 0072, iter [03400, 05004], lr: 0.034949, loss: 1.5516
2022-06-30 19:21:20 - train: epoch 0072, iter [03500, 05004], lr: 0.034924, loss: 1.5749
2022-06-30 19:21:54 - train: epoch 0072, iter [03600, 05004], lr: 0.034899, loss: 1.4768
2022-06-30 19:22:27 - train: epoch 0072, iter [03700, 05004], lr: 0.034874, loss: 1.4916
2022-06-30 19:23:00 - train: epoch 0072, iter [03800, 05004], lr: 0.034849, loss: 1.5098
2022-06-30 19:23:33 - train: epoch 0072, iter [03900, 05004], lr: 0.034824, loss: 1.5137
2022-06-30 19:24:06 - train: epoch 0072, iter [04000, 05004], lr: 0.034799, loss: 1.4337
2022-06-30 19:24:39 - train: epoch 0072, iter [04100, 05004], lr: 0.034774, loss: 1.7258
2022-06-30 19:25:13 - train: epoch 0072, iter [04200, 05004], lr: 0.034749, loss: 1.4292
2022-06-30 19:25:46 - train: epoch 0072, iter [04300, 05004], lr: 0.034724, loss: 1.4607
2022-06-30 19:26:19 - train: epoch 0072, iter [04400, 05004], lr: 0.034699, loss: 1.3193
2022-06-30 19:26:52 - train: epoch 0072, iter [04500, 05004], lr: 0.034675, loss: 1.6293
2022-06-30 19:27:25 - train: epoch 0072, iter [04600, 05004], lr: 0.034650, loss: 1.3229
2022-06-30 19:27:58 - train: epoch 0072, iter [04700, 05004], lr: 0.034625, loss: 1.4841
2022-06-30 19:28:30 - train: epoch 0072, iter [04800, 05004], lr: 0.034600, loss: 1.7161
2022-06-30 19:29:03 - train: epoch 0072, iter [04900, 05004], lr: 0.034575, loss: 1.5655
2022-06-30 19:29:36 - train: epoch 0072, iter [05000, 05004], lr: 0.034550, loss: 1.4042
2022-06-30 19:29:38 - train: epoch 072, train_loss: 1.4423
2022-06-30 19:30:55 - eval: epoch: 072, acc1: 68.472%, acc5: 88.930%, test_loss: 1.2896, per_image_load_time: 2.355ms, per_image_inference_time: 0.644ms
2022-06-30 19:30:56 - until epoch: 072, best_acc1: 69.574%
2022-06-30 19:30:56 - epoch 073 lr: 0.034549
2022-06-30 19:31:36 - train: epoch 0073, iter [00100, 05004], lr: 0.034524, loss: 1.5871
2022-06-30 19:32:09 - train: epoch 0073, iter [00200, 05004], lr: 0.034499, loss: 1.5104
2022-06-30 19:32:43 - train: epoch 0073, iter [00300, 05004], lr: 0.034475, loss: 1.5102
2022-06-30 19:33:16 - train: epoch 0073, iter [00400, 05004], lr: 0.034450, loss: 1.1087
2022-06-30 19:33:49 - train: epoch 0073, iter [00500, 05004], lr: 0.034425, loss: 1.3032
2022-06-30 19:34:22 - train: epoch 0073, iter [00600, 05004], lr: 0.034400, loss: 1.3060
2022-06-30 19:34:55 - train: epoch 0073, iter [00700, 05004], lr: 0.034375, loss: 1.4851
2022-06-30 19:35:29 - train: epoch 0073, iter [00800, 05004], lr: 0.034350, loss: 1.3131
2022-06-30 19:36:02 - train: epoch 0073, iter [00900, 05004], lr: 0.034325, loss: 1.1850
2022-06-30 19:36:35 - train: epoch 0073, iter [01000, 05004], lr: 0.034301, loss: 1.2711
2022-06-30 19:37:08 - train: epoch 0073, iter [01100, 05004], lr: 0.034276, loss: 1.4585
2022-06-30 19:37:41 - train: epoch 0073, iter [01200, 05004], lr: 0.034251, loss: 1.3776
2022-06-30 19:38:15 - train: epoch 0073, iter [01300, 05004], lr: 0.034226, loss: 1.5061
2022-06-30 19:38:48 - train: epoch 0073, iter [01400, 05004], lr: 0.034201, loss: 1.3488
2022-06-30 19:39:21 - train: epoch 0073, iter [01500, 05004], lr: 0.034176, loss: 1.4894
2022-06-30 19:39:54 - train: epoch 0073, iter [01600, 05004], lr: 0.034152, loss: 1.4684
2022-06-30 19:40:27 - train: epoch 0073, iter [01700, 05004], lr: 0.034127, loss: 1.7008
2022-06-30 19:41:01 - train: epoch 0073, iter [01800, 05004], lr: 0.034102, loss: 1.2233
2022-06-30 19:41:34 - train: epoch 0073, iter [01900, 05004], lr: 0.034077, loss: 1.4572
2022-06-30 19:42:07 - train: epoch 0073, iter [02000, 05004], lr: 0.034052, loss: 1.2733
2022-06-30 19:42:41 - train: epoch 0073, iter [02100, 05004], lr: 0.034028, loss: 1.4327
2022-06-30 19:43:14 - train: epoch 0073, iter [02200, 05004], lr: 0.034003, loss: 1.4922
2022-06-30 19:43:47 - train: epoch 0073, iter [02300, 05004], lr: 0.033978, loss: 1.5087
2022-06-30 19:44:20 - train: epoch 0073, iter [02400, 05004], lr: 0.033953, loss: 1.3907
2022-06-30 19:44:53 - train: epoch 0073, iter [02500, 05004], lr: 0.033929, loss: 1.6065
2022-06-30 19:45:27 - train: epoch 0073, iter [02600, 05004], lr: 0.033904, loss: 1.4636
2022-06-30 19:46:00 - train: epoch 0073, iter [02700, 05004], lr: 0.033879, loss: 1.4898
2022-06-30 19:46:33 - train: epoch 0073, iter [02800, 05004], lr: 0.033854, loss: 1.4293
2022-06-30 19:47:07 - train: epoch 0073, iter [02900, 05004], lr: 0.033829, loss: 1.5185
2022-06-30 19:47:40 - train: epoch 0073, iter [03000, 05004], lr: 0.033805, loss: 1.2813
2022-06-30 19:48:13 - train: epoch 0073, iter [03100, 05004], lr: 0.033780, loss: 1.3128
2022-06-30 19:48:47 - train: epoch 0073, iter [03200, 05004], lr: 0.033755, loss: 1.2383
2022-06-30 19:49:20 - train: epoch 0073, iter [03300, 05004], lr: 0.033730, loss: 1.5089
2022-06-30 19:49:53 - train: epoch 0073, iter [03400, 05004], lr: 0.033706, loss: 1.5286
2022-06-30 19:50:27 - train: epoch 0073, iter [03500, 05004], lr: 0.033681, loss: 1.5131
2022-06-30 19:51:00 - train: epoch 0073, iter [03600, 05004], lr: 0.033656, loss: 1.4161
2022-06-30 19:51:33 - train: epoch 0073, iter [03700, 05004], lr: 0.033632, loss: 1.4533
2022-06-30 19:52:06 - train: epoch 0073, iter [03800, 05004], lr: 0.033607, loss: 1.7482
2022-06-30 19:52:40 - train: epoch 0073, iter [03900, 05004], lr: 0.033582, loss: 1.4674
2022-06-30 19:53:13 - train: epoch 0073, iter [04000, 05004], lr: 0.033557, loss: 1.6176
2022-06-30 19:53:46 - train: epoch 0073, iter [04100, 05004], lr: 0.033533, loss: 1.4554
2022-06-30 19:54:19 - train: epoch 0073, iter [04200, 05004], lr: 0.033508, loss: 1.5532
2022-06-30 19:54:53 - train: epoch 0073, iter [04300, 05004], lr: 0.033483, loss: 1.4375
2022-06-30 19:55:26 - train: epoch 0073, iter [04400, 05004], lr: 0.033459, loss: 1.4735
2022-06-30 19:55:59 - train: epoch 0073, iter [04500, 05004], lr: 0.033434, loss: 1.3910
2022-06-30 19:56:32 - train: epoch 0073, iter [04600, 05004], lr: 0.033409, loss: 1.6041
2022-06-30 19:57:05 - train: epoch 0073, iter [04700, 05004], lr: 0.033385, loss: 1.1839
2022-06-30 19:57:38 - train: epoch 0073, iter [04800, 05004], lr: 0.033360, loss: 1.5172
2022-06-30 19:58:12 - train: epoch 0073, iter [04900, 05004], lr: 0.033335, loss: 1.2893
2022-06-30 19:58:45 - train: epoch 0073, iter [05000, 05004], lr: 0.033311, loss: 1.5707
2022-06-30 19:58:46 - train: epoch 073, train_loss: 1.4296
2022-06-30 20:00:04 - eval: epoch: 073, acc1: 69.218%, acc5: 89.246%, test_loss: 1.2477, per_image_load_time: 2.294ms, per_image_inference_time: 0.602ms
2022-06-30 20:00:04 - until epoch: 073, best_acc1: 69.574%
2022-06-30 20:00:04 - epoch 074 lr: 0.033309
2022-06-30 20:00:45 - train: epoch 0074, iter [00100, 05004], lr: 0.033285, loss: 1.3231
2022-06-30 20:01:18 - train: epoch 0074, iter [00200, 05004], lr: 0.033260, loss: 1.4303
2022-06-30 20:01:51 - train: epoch 0074, iter [00300, 05004], lr: 0.033236, loss: 1.4509
2022-06-30 20:02:24 - train: epoch 0074, iter [00400, 05004], lr: 0.033211, loss: 1.6001
2022-06-30 20:02:57 - train: epoch 0074, iter [00500, 05004], lr: 0.033186, loss: 1.3803
2022-06-30 20:03:30 - train: epoch 0074, iter [00600, 05004], lr: 0.033162, loss: 1.4616
2022-06-30 20:04:03 - train: epoch 0074, iter [00700, 05004], lr: 0.033137, loss: 1.3420
2022-06-30 20:04:36 - train: epoch 0074, iter [00800, 05004], lr: 0.033113, loss: 1.4904
2022-06-30 20:05:09 - train: epoch 0074, iter [00900, 05004], lr: 0.033088, loss: 1.3152
2022-06-30 20:05:42 - train: epoch 0074, iter [01000, 05004], lr: 0.033063, loss: 1.5848
2022-06-30 20:06:16 - train: epoch 0074, iter [01100, 05004], lr: 0.033039, loss: 1.5057
2022-06-30 20:06:49 - train: epoch 0074, iter [01200, 05004], lr: 0.033014, loss: 1.3613
2022-06-30 20:07:22 - train: epoch 0074, iter [01300, 05004], lr: 0.032989, loss: 1.5331
2022-06-30 20:07:55 - train: epoch 0074, iter [01400, 05004], lr: 0.032965, loss: 1.3587
2022-06-30 20:08:29 - train: epoch 0074, iter [01500, 05004], lr: 0.032940, loss: 1.4740
2022-06-30 20:09:02 - train: epoch 0074, iter [01600, 05004], lr: 0.032916, loss: 1.1555
2022-06-30 20:09:36 - train: epoch 0074, iter [01700, 05004], lr: 0.032891, loss: 1.4630
2022-06-30 20:10:09 - train: epoch 0074, iter [01800, 05004], lr: 0.032867, loss: 1.7884
2022-06-30 20:10:42 - train: epoch 0074, iter [01900, 05004], lr: 0.032842, loss: 1.4977
2022-06-30 20:11:16 - train: epoch 0074, iter [02000, 05004], lr: 0.032817, loss: 1.1014
2022-06-30 20:11:49 - train: epoch 0074, iter [02100, 05004], lr: 0.032793, loss: 1.4103
2022-06-30 20:12:22 - train: epoch 0074, iter [02200, 05004], lr: 0.032768, loss: 1.6927
2022-06-30 20:12:56 - train: epoch 0074, iter [02300, 05004], lr: 0.032744, loss: 1.4608
2022-06-30 20:13:28 - train: epoch 0074, iter [02400, 05004], lr: 0.032719, loss: 1.4216
2022-06-30 20:14:01 - train: epoch 0074, iter [02500, 05004], lr: 0.032695, loss: 1.2688
2022-06-30 20:14:34 - train: epoch 0074, iter [02600, 05004], lr: 0.032670, loss: 1.1636
2022-06-30 20:15:08 - train: epoch 0074, iter [02700, 05004], lr: 0.032646, loss: 1.3970
2022-06-30 20:15:41 - train: epoch 0074, iter [02800, 05004], lr: 0.032621, loss: 1.6872
2022-06-30 20:16:14 - train: epoch 0074, iter [02900, 05004], lr: 0.032597, loss: 1.4002
2022-06-30 20:16:47 - train: epoch 0074, iter [03000, 05004], lr: 0.032572, loss: 1.4628
2022-06-30 20:17:20 - train: epoch 0074, iter [03100, 05004], lr: 0.032547, loss: 1.4300
2022-06-30 20:17:54 - train: epoch 0074, iter [03200, 05004], lr: 0.032523, loss: 1.3018
2022-06-30 20:18:27 - train: epoch 0074, iter [03300, 05004], lr: 0.032498, loss: 1.3901
2022-06-30 20:19:00 - train: epoch 0074, iter [03400, 05004], lr: 0.032474, loss: 1.5395
2022-06-30 20:19:34 - train: epoch 0074, iter [03500, 05004], lr: 0.032449, loss: 1.2350
2022-06-30 20:20:07 - train: epoch 0074, iter [03600, 05004], lr: 0.032425, loss: 1.4458
2022-06-30 20:20:40 - train: epoch 0074, iter [03700, 05004], lr: 0.032400, loss: 1.4709
2022-06-30 20:21:13 - train: epoch 0074, iter [03800, 05004], lr: 0.032376, loss: 1.4025
2022-06-30 20:21:47 - train: epoch 0074, iter [03900, 05004], lr: 0.032352, loss: 1.1634
2022-06-30 20:22:20 - train: epoch 0074, iter [04000, 05004], lr: 0.032327, loss: 1.3527
2022-06-30 20:22:53 - train: epoch 0074, iter [04100, 05004], lr: 0.032303, loss: 1.6262
2022-06-30 20:23:26 - train: epoch 0074, iter [04200, 05004], lr: 0.032278, loss: 1.4700
2022-06-30 20:23:59 - train: epoch 0074, iter [04300, 05004], lr: 0.032254, loss: 1.4071
2022-06-30 20:24:32 - train: epoch 0074, iter [04400, 05004], lr: 0.032229, loss: 1.2178
2022-06-30 20:25:05 - train: epoch 0074, iter [04500, 05004], lr: 0.032205, loss: 1.2375
2022-06-30 20:25:39 - train: epoch 0074, iter [04600, 05004], lr: 0.032180, loss: 1.5174
2022-06-30 20:26:12 - train: epoch 0074, iter [04700, 05004], lr: 0.032156, loss: 1.4294
2022-06-30 20:26:45 - train: epoch 0074, iter [04800, 05004], lr: 0.032131, loss: 1.4018
2022-06-30 20:27:19 - train: epoch 0074, iter [04900, 05004], lr: 0.032107, loss: 1.8399
2022-06-30 20:27:52 - train: epoch 0074, iter [05000, 05004], lr: 0.032083, loss: 1.4790
2022-06-30 20:27:54 - train: epoch 074, train_loss: 1.4142
2022-06-30 20:29:11 - eval: epoch: 074, acc1: 69.486%, acc5: 89.492%, test_loss: 1.2401, per_image_load_time: 2.333ms, per_image_inference_time: 0.634ms
2022-06-30 20:29:12 - until epoch: 074, best_acc1: 69.574%
2022-06-30 20:29:12 - epoch 075 lr: 0.032081
2022-06-30 20:29:53 - train: epoch 0075, iter [00100, 05004], lr: 0.032057, loss: 1.4477
2022-06-30 20:30:27 - train: epoch 0075, iter [00200, 05004], lr: 0.032033, loss: 1.3078
2022-06-30 20:31:00 - train: epoch 0075, iter [00300, 05004], lr: 0.032008, loss: 1.3738
2022-06-30 20:31:33 - train: epoch 0075, iter [00400, 05004], lr: 0.031984, loss: 1.4400
2022-06-30 20:32:07 - train: epoch 0075, iter [00500, 05004], lr: 0.031960, loss: 1.1996
2022-06-30 20:32:40 - train: epoch 0075, iter [00600, 05004], lr: 0.031935, loss: 1.2650
2022-06-30 20:33:13 - train: epoch 0075, iter [00700, 05004], lr: 0.031911, loss: 1.5602
2022-06-30 20:33:47 - train: epoch 0075, iter [00800, 05004], lr: 0.031886, loss: 1.5797
2022-06-30 20:34:20 - train: epoch 0075, iter [00900, 05004], lr: 0.031862, loss: 1.5584
2022-06-30 20:34:54 - train: epoch 0075, iter [01000, 05004], lr: 0.031838, loss: 1.2132
2022-06-30 20:35:27 - train: epoch 0075, iter [01100, 05004], lr: 0.031813, loss: 1.4045
2022-06-30 20:36:01 - train: epoch 0075, iter [01200, 05004], lr: 0.031789, loss: 1.3210
2022-06-30 20:36:34 - train: epoch 0075, iter [01300, 05004], lr: 0.031765, loss: 1.2431
2022-06-30 20:37:07 - train: epoch 0075, iter [01400, 05004], lr: 0.031740, loss: 1.4436
2022-06-30 20:37:41 - train: epoch 0075, iter [01500, 05004], lr: 0.031716, loss: 1.4384
2022-06-30 20:38:14 - train: epoch 0075, iter [01600, 05004], lr: 0.031691, loss: 1.0351
2022-06-30 20:38:47 - train: epoch 0075, iter [01700, 05004], lr: 0.031667, loss: 1.3721
2022-06-30 20:39:20 - train: epoch 0075, iter [01800, 05004], lr: 0.031643, loss: 1.3865
2022-06-30 20:39:53 - train: epoch 0075, iter [01900, 05004], lr: 0.031618, loss: 1.2286
2022-06-30 20:40:27 - train: epoch 0075, iter [02000, 05004], lr: 0.031594, loss: 1.4418
2022-06-30 20:41:00 - train: epoch 0075, iter [02100, 05004], lr: 0.031570, loss: 1.2874
2022-06-30 20:41:34 - train: epoch 0075, iter [02200, 05004], lr: 0.031546, loss: 1.4389
2022-06-30 20:42:07 - train: epoch 0075, iter [02300, 05004], lr: 0.031521, loss: 1.3152
2022-06-30 20:42:40 - train: epoch 0075, iter [02400, 05004], lr: 0.031497, loss: 1.4554
2022-06-30 20:43:13 - train: epoch 0075, iter [02500, 05004], lr: 0.031473, loss: 1.4981
2022-06-30 20:43:46 - train: epoch 0075, iter [02600, 05004], lr: 0.031448, loss: 1.4816
2022-06-30 20:44:19 - train: epoch 0075, iter [02700, 05004], lr: 0.031424, loss: 1.2796
2022-06-30 20:44:52 - train: epoch 0075, iter [02800, 05004], lr: 0.031400, loss: 1.3270
2022-06-30 20:45:25 - train: epoch 0075, iter [02900, 05004], lr: 0.031375, loss: 1.5315
2022-06-30 20:45:58 - train: epoch 0075, iter [03000, 05004], lr: 0.031351, loss: 1.5139
2022-06-30 20:46:31 - train: epoch 0075, iter [03100, 05004], lr: 0.031327, loss: 1.5139
2022-06-30 20:47:04 - train: epoch 0075, iter [03200, 05004], lr: 0.031303, loss: 1.4019
2022-06-30 20:47:38 - train: epoch 0075, iter [03300, 05004], lr: 0.031278, loss: 1.4329
2022-06-30 20:48:11 - train: epoch 0075, iter [03400, 05004], lr: 0.031254, loss: 1.2587
2022-06-30 20:48:44 - train: epoch 0075, iter [03500, 05004], lr: 0.031230, loss: 1.4215
2022-06-30 20:49:17 - train: epoch 0075, iter [03600, 05004], lr: 0.031206, loss: 1.4200
2022-06-30 20:49:51 - train: epoch 0075, iter [03700, 05004], lr: 0.031181, loss: 1.4341
2022-06-30 20:50:24 - train: epoch 0075, iter [03800, 05004], lr: 0.031157, loss: 1.2919
2022-06-30 20:50:57 - train: epoch 0075, iter [03900, 05004], lr: 0.031133, loss: 1.4850
2022-06-30 20:51:30 - train: epoch 0075, iter [04000, 05004], lr: 0.031109, loss: 1.2977
2022-06-30 20:52:03 - train: epoch 0075, iter [04100, 05004], lr: 0.031085, loss: 1.0446
2022-06-30 20:52:37 - train: epoch 0075, iter [04200, 05004], lr: 0.031060, loss: 1.5005
2022-06-30 20:53:09 - train: epoch 0075, iter [04300, 05004], lr: 0.031036, loss: 1.5952
2022-06-30 20:53:42 - train: epoch 0075, iter [04400, 05004], lr: 0.031012, loss: 1.2174
2022-06-30 20:54:16 - train: epoch 0075, iter [04500, 05004], lr: 0.030988, loss: 1.4373
2022-06-30 20:54:49 - train: epoch 0075, iter [04600, 05004], lr: 0.030964, loss: 1.2108
2022-06-30 20:55:23 - train: epoch 0075, iter [04700, 05004], lr: 0.030939, loss: 1.6600
2022-06-30 20:55:56 - train: epoch 0075, iter [04800, 05004], lr: 0.030915, loss: 1.4571
2022-06-30 20:56:29 - train: epoch 0075, iter [04900, 05004], lr: 0.030891, loss: 1.2892
2022-06-30 20:57:02 - train: epoch 0075, iter [05000, 05004], lr: 0.030867, loss: 1.4371
2022-06-30 20:57:04 - train: epoch 075, train_loss: 1.4029
2022-06-30 20:58:21 - eval: epoch: 075, acc1: 69.674%, acc5: 89.640%, test_loss: 1.2313, per_image_load_time: 2.399ms, per_image_inference_time: 0.625ms
2022-06-30 20:58:22 - until epoch: 075, best_acc1: 69.674%
2022-06-30 20:58:22 - epoch 076 lr: 0.030866
2022-06-30 20:59:02 - train: epoch 0076, iter [00100, 05004], lr: 0.030842, loss: 1.2042
2022-06-30 20:59:35 - train: epoch 0076, iter [00200, 05004], lr: 0.030818, loss: 1.3240
2022-06-30 21:00:08 - train: epoch 0076, iter [00300, 05004], lr: 0.030793, loss: 1.5011
2022-06-30 21:00:42 - train: epoch 0076, iter [00400, 05004], lr: 0.030769, loss: 1.3671
2022-06-30 21:01:16 - train: epoch 0076, iter [00500, 05004], lr: 0.030745, loss: 1.3362
2022-06-30 21:01:49 - train: epoch 0076, iter [00600, 05004], lr: 0.030721, loss: 1.3366
2022-06-30 21:02:23 - train: epoch 0076, iter [00700, 05004], lr: 0.030697, loss: 1.4746
2022-06-30 21:02:56 - train: epoch 0076, iter [00800, 05004], lr: 0.030673, loss: 1.2876
2022-06-30 21:03:30 - train: epoch 0076, iter [00900, 05004], lr: 0.030649, loss: 1.3489
2022-06-30 21:04:03 - train: epoch 0076, iter [01000, 05004], lr: 0.030624, loss: 1.3559
2022-06-30 21:04:36 - train: epoch 0076, iter [01100, 05004], lr: 0.030600, loss: 1.2354
2022-06-30 21:05:10 - train: epoch 0076, iter [01200, 05004], lr: 0.030576, loss: 1.3557
2022-06-30 21:05:43 - train: epoch 0076, iter [01300, 05004], lr: 0.030552, loss: 1.3392
2022-06-30 21:06:17 - train: epoch 0076, iter [01400, 05004], lr: 0.030528, loss: 1.2838
2022-06-30 21:06:50 - train: epoch 0076, iter [01500, 05004], lr: 0.030504, loss: 1.2973
2022-06-30 21:07:24 - train: epoch 0076, iter [01600, 05004], lr: 0.030480, loss: 1.3458
2022-06-30 21:07:57 - train: epoch 0076, iter [01700, 05004], lr: 0.030456, loss: 1.3940
2022-06-30 21:08:31 - train: epoch 0076, iter [01800, 05004], lr: 0.030432, loss: 1.2174
2022-06-30 21:09:04 - train: epoch 0076, iter [01900, 05004], lr: 0.030408, loss: 1.6021
2022-06-30 21:09:38 - train: epoch 0076, iter [02000, 05004], lr: 0.030384, loss: 1.4452
2022-06-30 21:10:11 - train: epoch 0076, iter [02100, 05004], lr: 0.030359, loss: 1.5446
2022-06-30 21:10:45 - train: epoch 0076, iter [02200, 05004], lr: 0.030335, loss: 1.5766
2022-06-30 21:11:18 - train: epoch 0076, iter [02300, 05004], lr: 0.030311, loss: 1.3476
2022-06-30 21:11:52 - train: epoch 0076, iter [02400, 05004], lr: 0.030287, loss: 1.5371
2022-06-30 21:12:25 - train: epoch 0076, iter [02500, 05004], lr: 0.030263, loss: 1.3383
2022-06-30 21:12:58 - train: epoch 0076, iter [02600, 05004], lr: 0.030239, loss: 1.6539
2022-06-30 21:13:31 - train: epoch 0076, iter [02700, 05004], lr: 0.030215, loss: 1.4447
2022-06-30 21:14:04 - train: epoch 0076, iter [02800, 05004], lr: 0.030191, loss: 1.3919
2022-06-30 21:14:37 - train: epoch 0076, iter [02900, 05004], lr: 0.030167, loss: 1.5037
2022-06-30 21:15:11 - train: epoch 0076, iter [03000, 05004], lr: 0.030143, loss: 1.3792
2022-06-30 21:15:44 - train: epoch 0076, iter [03100, 05004], lr: 0.030119, loss: 1.5645
2022-06-30 21:16:16 - train: epoch 0076, iter [03200, 05004], lr: 0.030095, loss: 1.3909
2022-06-30 21:16:49 - train: epoch 0076, iter [03300, 05004], lr: 0.030071, loss: 1.3552
2022-06-30 21:17:22 - train: epoch 0076, iter [03400, 05004], lr: 0.030047, loss: 1.3477
2022-06-30 21:17:55 - train: epoch 0076, iter [03500, 05004], lr: 0.030023, loss: 1.2884
2022-06-30 21:18:29 - train: epoch 0076, iter [03600, 05004], lr: 0.029999, loss: 1.2046
2022-06-30 21:19:02 - train: epoch 0076, iter [03700, 05004], lr: 0.029975, loss: 1.2840
2022-06-30 21:19:34 - train: epoch 0076, iter [03800, 05004], lr: 0.029951, loss: 1.2740
2022-06-30 21:20:08 - train: epoch 0076, iter [03900, 05004], lr: 0.029927, loss: 1.2919
2022-06-30 21:20:41 - train: epoch 0076, iter [04000, 05004], lr: 0.029903, loss: 1.3756
2022-06-30 21:21:14 - train: epoch 0076, iter [04100, 05004], lr: 0.029879, loss: 1.3934
2022-06-30 21:21:47 - train: epoch 0076, iter [04200, 05004], lr: 0.029855, loss: 1.5874
2022-06-30 21:22:20 - train: epoch 0076, iter [04300, 05004], lr: 0.029832, loss: 1.4541
2022-06-30 21:22:53 - train: epoch 0076, iter [04400, 05004], lr: 0.029808, loss: 1.5471
2022-06-30 21:23:26 - train: epoch 0076, iter [04500, 05004], lr: 0.029784, loss: 1.4699
2022-06-30 21:23:59 - train: epoch 0076, iter [04600, 05004], lr: 0.029760, loss: 1.4456
2022-06-30 21:24:32 - train: epoch 0076, iter [04700, 05004], lr: 0.029736, loss: 1.5024
2022-06-30 21:25:06 - train: epoch 0076, iter [04800, 05004], lr: 0.029712, loss: 1.2299
2022-06-30 21:25:39 - train: epoch 0076, iter [04900, 05004], lr: 0.029688, loss: 1.2949
2022-06-30 21:26:12 - train: epoch 0076, iter [05000, 05004], lr: 0.029664, loss: 1.4271
2022-06-30 21:26:14 - train: epoch 076, train_loss: 1.3887
2022-06-30 21:27:32 - eval: epoch: 076, acc1: 69.392%, acc5: 89.658%, test_loss: 1.2430, per_image_load_time: 2.366ms, per_image_inference_time: 0.614ms
2022-06-30 21:27:32 - until epoch: 076, best_acc1: 69.674%
2022-06-30 21:27:32 - epoch 077 lr: 0.029663
2022-06-30 21:28:13 - train: epoch 0077, iter [00100, 05004], lr: 0.029639, loss: 1.5533
2022-06-30 21:28:47 - train: epoch 0077, iter [00200, 05004], lr: 0.029615, loss: 1.4227
2022-06-30 21:29:20 - train: epoch 0077, iter [00300, 05004], lr: 0.029592, loss: 1.1522
2022-06-30 21:29:53 - train: epoch 0077, iter [00400, 05004], lr: 0.029568, loss: 1.4851
2022-06-30 21:30:26 - train: epoch 0077, iter [00500, 05004], lr: 0.029544, loss: 1.2077
2022-06-30 21:30:59 - train: epoch 0077, iter [00600, 05004], lr: 0.029520, loss: 1.1796
2022-06-30 21:31:33 - train: epoch 0077, iter [00700, 05004], lr: 0.029496, loss: 1.3631
2022-06-30 21:32:05 - train: epoch 0077, iter [00800, 05004], lr: 0.029472, loss: 1.4507
2022-06-30 21:32:39 - train: epoch 0077, iter [00900, 05004], lr: 0.029448, loss: 1.6315
2022-06-30 21:33:12 - train: epoch 0077, iter [01000, 05004], lr: 0.029424, loss: 1.1886
2022-06-30 21:33:45 - train: epoch 0077, iter [01100, 05004], lr: 0.029401, loss: 1.3935
2022-06-30 21:34:19 - train: epoch 0077, iter [01200, 05004], lr: 0.029377, loss: 1.3968
2022-06-30 21:34:53 - train: epoch 0077, iter [01300, 05004], lr: 0.029353, loss: 1.1642
2022-06-30 21:35:26 - train: epoch 0077, iter [01400, 05004], lr: 0.029329, loss: 1.5173
2022-06-30 21:35:59 - train: epoch 0077, iter [01500, 05004], lr: 0.029305, loss: 1.1807
2022-06-30 21:36:32 - train: epoch 0077, iter [01600, 05004], lr: 0.029282, loss: 1.3525
2022-06-30 21:37:05 - train: epoch 0077, iter [01700, 05004], lr: 0.029258, loss: 1.5845
2022-06-30 21:37:39 - train: epoch 0077, iter [01800, 05004], lr: 0.029234, loss: 1.3588
2022-06-30 21:38:11 - train: epoch 0077, iter [01900, 05004], lr: 0.029210, loss: 1.3375
2022-06-30 21:38:45 - train: epoch 0077, iter [02000, 05004], lr: 0.029186, loss: 1.1930
2022-06-30 21:39:18 - train: epoch 0077, iter [02100, 05004], lr: 0.029163, loss: 1.3355
2022-06-30 21:39:51 - train: epoch 0077, iter [02200, 05004], lr: 0.029139, loss: 1.3918
2022-06-30 21:40:24 - train: epoch 0077, iter [02300, 05004], lr: 0.029115, loss: 1.4926
2022-06-30 21:40:57 - train: epoch 0077, iter [02400, 05004], lr: 0.029091, loss: 1.3609
2022-06-30 21:41:31 - train: epoch 0077, iter [02500, 05004], lr: 0.029067, loss: 1.6254
2022-06-30 21:42:04 - train: epoch 0077, iter [02600, 05004], lr: 0.029044, loss: 1.1196
2022-06-30 21:42:37 - train: epoch 0077, iter [02700, 05004], lr: 0.029020, loss: 1.4198
2022-06-30 21:43:10 - train: epoch 0077, iter [02800, 05004], lr: 0.028996, loss: 1.5321
2022-06-30 21:43:43 - train: epoch 0077, iter [02900, 05004], lr: 0.028973, loss: 1.7653
2022-06-30 21:44:17 - train: epoch 0077, iter [03000, 05004], lr: 0.028949, loss: 1.2380
2022-06-30 21:44:50 - train: epoch 0077, iter [03100, 05004], lr: 0.028925, loss: 1.4785
2022-06-30 21:45:24 - train: epoch 0077, iter [03200, 05004], lr: 0.028901, loss: 1.5721
2022-06-30 21:45:57 - train: epoch 0077, iter [03300, 05004], lr: 0.028878, loss: 1.3036
2022-06-30 21:46:30 - train: epoch 0077, iter [03400, 05004], lr: 0.028854, loss: 1.6076
2022-06-30 21:47:03 - train: epoch 0077, iter [03500, 05004], lr: 0.028830, loss: 1.2185
2022-06-30 21:47:36 - train: epoch 0077, iter [03600, 05004], lr: 0.028807, loss: 1.3386
2022-06-30 21:48:09 - train: epoch 0077, iter [03700, 05004], lr: 0.028783, loss: 1.4973
2022-06-30 21:48:42 - train: epoch 0077, iter [03800, 05004], lr: 0.028759, loss: 1.3514
2022-06-30 21:49:16 - train: epoch 0077, iter [03900, 05004], lr: 0.028735, loss: 1.4679
2022-06-30 21:49:49 - train: epoch 0077, iter [04000, 05004], lr: 0.028712, loss: 1.3447
2022-06-30 21:50:22 - train: epoch 0077, iter [04100, 05004], lr: 0.028688, loss: 1.5032
2022-06-30 21:50:55 - train: epoch 0077, iter [04200, 05004], lr: 0.028664, loss: 1.5122
2022-06-30 21:51:29 - train: epoch 0077, iter [04300, 05004], lr: 0.028641, loss: 1.3238
2022-06-30 21:52:02 - train: epoch 0077, iter [04400, 05004], lr: 0.028617, loss: 1.3622
2022-06-30 21:52:35 - train: epoch 0077, iter [04500, 05004], lr: 0.028594, loss: 1.4272
2022-06-30 21:53:08 - train: epoch 0077, iter [04600, 05004], lr: 0.028570, loss: 1.3228
2022-06-30 21:53:41 - train: epoch 0077, iter [04700, 05004], lr: 0.028546, loss: 1.1699
2022-06-30 21:54:14 - train: epoch 0077, iter [04800, 05004], lr: 0.028523, loss: 1.3831
2022-06-30 21:54:47 - train: epoch 0077, iter [04900, 05004], lr: 0.028499, loss: 1.5566
2022-06-30 21:55:20 - train: epoch 0077, iter [05000, 05004], lr: 0.028475, loss: 1.5886
2022-06-30 21:55:22 - train: epoch 077, train_loss: 1.3734
2022-06-30 21:56:40 - eval: epoch: 077, acc1: 69.932%, acc5: 89.664%, test_loss: 1.2277, per_image_load_time: 2.386ms, per_image_inference_time: 0.621ms
2022-06-30 21:56:41 - until epoch: 077, best_acc1: 69.932%
2022-06-30 21:56:41 - epoch 078 lr: 0.028474
2022-06-30 21:57:22 - train: epoch 0078, iter [00100, 05004], lr: 0.028451, loss: 1.2705
2022-06-30 21:57:55 - train: epoch 0078, iter [00200, 05004], lr: 0.028427, loss: 1.5161
2022-06-30 21:58:29 - train: epoch 0078, iter [00300, 05004], lr: 0.028404, loss: 1.4232
2022-06-30 21:59:02 - train: epoch 0078, iter [00400, 05004], lr: 0.028380, loss: 1.3528
2022-06-30 21:59:36 - train: epoch 0078, iter [00500, 05004], lr: 0.028356, loss: 1.3193
2022-06-30 22:00:09 - train: epoch 0078, iter [00600, 05004], lr: 0.028333, loss: 1.4012
2022-06-30 22:00:43 - train: epoch 0078, iter [00700, 05004], lr: 0.028309, loss: 1.4271
2022-06-30 22:01:16 - train: epoch 0078, iter [00800, 05004], lr: 0.028286, loss: 1.5495
2022-06-30 22:01:49 - train: epoch 0078, iter [00900, 05004], lr: 0.028262, loss: 1.5325
2022-06-30 22:02:22 - train: epoch 0078, iter [01000, 05004], lr: 0.028239, loss: 1.3570
2022-06-30 22:02:56 - train: epoch 0078, iter [01100, 05004], lr: 0.028215, loss: 1.2062
2022-06-30 22:03:29 - train: epoch 0078, iter [01200, 05004], lr: 0.028192, loss: 1.1181
2022-06-30 22:04:02 - train: epoch 0078, iter [01300, 05004], lr: 0.028168, loss: 1.3703
2022-06-30 22:04:34 - train: epoch 0078, iter [01400, 05004], lr: 0.028144, loss: 1.2492
2022-06-30 22:05:08 - train: epoch 0078, iter [01500, 05004], lr: 0.028121, loss: 1.4204
2022-06-30 22:05:41 - train: epoch 0078, iter [01600, 05004], lr: 0.028097, loss: 1.4006
2022-06-30 22:06:14 - train: epoch 0078, iter [01700, 05004], lr: 0.028074, loss: 1.2442
2022-06-30 22:06:47 - train: epoch 0078, iter [01800, 05004], lr: 0.028050, loss: 1.3097
2022-06-30 22:07:20 - train: epoch 0078, iter [01900, 05004], lr: 0.028027, loss: 1.0914
2022-06-30 22:07:54 - train: epoch 0078, iter [02000, 05004], lr: 0.028003, loss: 1.2915
2022-06-30 22:08:27 - train: epoch 0078, iter [02100, 05004], lr: 0.027980, loss: 1.7089
2022-06-30 22:09:01 - train: epoch 0078, iter [02200, 05004], lr: 0.027956, loss: 1.2864
2022-06-30 22:09:34 - train: epoch 0078, iter [02300, 05004], lr: 0.027933, loss: 1.4555
2022-06-30 22:10:07 - train: epoch 0078, iter [02400, 05004], lr: 0.027909, loss: 1.3090
2022-06-30 22:10:41 - train: epoch 0078, iter [02500, 05004], lr: 0.027886, loss: 1.4190
2022-06-30 22:11:14 - train: epoch 0078, iter [02600, 05004], lr: 0.027863, loss: 1.3474
2022-06-30 22:11:47 - train: epoch 0078, iter [02700, 05004], lr: 0.027839, loss: 1.3911
2022-06-30 22:12:20 - train: epoch 0078, iter [02800, 05004], lr: 0.027816, loss: 1.4008
2022-06-30 22:12:54 - train: epoch 0078, iter [02900, 05004], lr: 0.027792, loss: 1.2360
2022-06-30 22:13:27 - train: epoch 0078, iter [03000, 05004], lr: 0.027769, loss: 1.4175
2022-06-30 22:14:00 - train: epoch 0078, iter [03100, 05004], lr: 0.027745, loss: 1.4810
2022-06-30 22:14:33 - train: epoch 0078, iter [03200, 05004], lr: 0.027722, loss: 1.4076
2022-06-30 22:15:06 - train: epoch 0078, iter [03300, 05004], lr: 0.027699, loss: 1.4851
2022-06-30 22:15:40 - train: epoch 0078, iter [03400, 05004], lr: 0.027675, loss: 1.3621
2022-06-30 22:16:13 - train: epoch 0078, iter [03500, 05004], lr: 0.027652, loss: 1.2804
2022-06-30 22:16:46 - train: epoch 0078, iter [03600, 05004], lr: 0.027628, loss: 1.4366
2022-06-30 22:17:19 - train: epoch 0078, iter [03700, 05004], lr: 0.027605, loss: 1.2712
2022-06-30 22:17:52 - train: epoch 0078, iter [03800, 05004], lr: 0.027582, loss: 1.3305
2022-06-30 22:18:26 - train: epoch 0078, iter [03900, 05004], lr: 0.027558, loss: 1.3258
2022-06-30 22:18:59 - train: epoch 0078, iter [04000, 05004], lr: 0.027535, loss: 1.3404
2022-06-30 22:19:32 - train: epoch 0078, iter [04100, 05004], lr: 0.027511, loss: 1.5017
2022-06-30 22:20:06 - train: epoch 0078, iter [04200, 05004], lr: 0.027488, loss: 1.4960
2022-06-30 22:20:39 - train: epoch 0078, iter [04300, 05004], lr: 0.027465, loss: 1.4399
2022-06-30 22:21:12 - train: epoch 0078, iter [04400, 05004], lr: 0.027441, loss: 1.3281
2022-06-30 22:21:45 - train: epoch 0078, iter [04500, 05004], lr: 0.027418, loss: 1.4401
2022-06-30 22:22:19 - train: epoch 0078, iter [04600, 05004], lr: 0.027395, loss: 1.1316
2022-06-30 22:22:52 - train: epoch 0078, iter [04700, 05004], lr: 0.027371, loss: 1.4672
2022-06-30 22:23:25 - train: epoch 0078, iter [04800, 05004], lr: 0.027348, loss: 1.5571
2022-06-30 22:23:58 - train: epoch 0078, iter [04900, 05004], lr: 0.027325, loss: 1.3802
2022-06-30 22:24:31 - train: epoch 0078, iter [05000, 05004], lr: 0.027301, loss: 1.3041
2022-06-30 22:24:33 - train: epoch 078, train_loss: 1.3570
2022-06-30 22:25:51 - eval: epoch: 078, acc1: 70.312%, acc5: 90.128%, test_loss: 1.1971, per_image_load_time: 2.347ms, per_image_inference_time: 0.621ms
2022-06-30 22:25:51 - until epoch: 078, best_acc1: 70.312%
2022-06-30 22:25:51 - epoch 079 lr: 0.027300
2022-06-30 22:26:32 - train: epoch 0079, iter [00100, 05004], lr: 0.027277, loss: 1.2414
2022-06-30 22:27:05 - train: epoch 0079, iter [00200, 05004], lr: 0.027254, loss: 1.2639
2022-06-30 22:27:38 - train: epoch 0079, iter [00300, 05004], lr: 0.027231, loss: 1.3865
2022-06-30 22:28:11 - train: epoch 0079, iter [00400, 05004], lr: 0.027207, loss: 1.4174
2022-06-30 22:28:44 - train: epoch 0079, iter [00500, 05004], lr: 0.027184, loss: 1.3991
2022-06-30 22:29:17 - train: epoch 0079, iter [00600, 05004], lr: 0.027161, loss: 1.2771
2022-06-30 22:29:50 - train: epoch 0079, iter [00700, 05004], lr: 0.027137, loss: 1.2359
2022-06-30 22:30:23 - train: epoch 0079, iter [00800, 05004], lr: 0.027114, loss: 1.4194
2022-06-30 22:30:57 - train: epoch 0079, iter [00900, 05004], lr: 0.027091, loss: 1.1765
2022-06-30 22:31:30 - train: epoch 0079, iter [01000, 05004], lr: 0.027068, loss: 1.2726
2022-06-30 22:32:03 - train: epoch 0079, iter [01100, 05004], lr: 0.027044, loss: 1.3847
2022-06-30 22:32:36 - train: epoch 0079, iter [01200, 05004], lr: 0.027021, loss: 1.6026
2022-06-30 22:33:09 - train: epoch 0079, iter [01300, 05004], lr: 0.026998, loss: 1.1641
2022-06-30 22:33:42 - train: epoch 0079, iter [01400, 05004], lr: 0.026975, loss: 1.2953
2022-06-30 22:34:15 - train: epoch 0079, iter [01500, 05004], lr: 0.026952, loss: 1.1334
2022-06-30 22:34:48 - train: epoch 0079, iter [01600, 05004], lr: 0.026928, loss: 1.1516
2022-06-30 22:35:20 - train: epoch 0079, iter [01700, 05004], lr: 0.026905, loss: 1.2620
2022-06-30 22:35:53 - train: epoch 0079, iter [01800, 05004], lr: 0.026882, loss: 1.3151
2022-06-30 22:36:26 - train: epoch 0079, iter [01900, 05004], lr: 0.026859, loss: 1.3475
2022-06-30 22:36:59 - train: epoch 0079, iter [02000, 05004], lr: 0.026836, loss: 1.6574
2022-06-30 22:37:31 - train: epoch 0079, iter [02100, 05004], lr: 0.026812, loss: 1.1209
2022-06-30 22:38:04 - train: epoch 0079, iter [02200, 05004], lr: 0.026789, loss: 1.3068
2022-06-30 22:38:37 - train: epoch 0079, iter [02300, 05004], lr: 0.026766, loss: 1.2013
2022-06-30 22:39:10 - train: epoch 0079, iter [02400, 05004], lr: 0.026743, loss: 1.3140
2022-06-30 22:39:43 - train: epoch 0079, iter [02500, 05004], lr: 0.026720, loss: 1.3724
2022-06-30 22:40:16 - train: epoch 0079, iter [02600, 05004], lr: 0.026697, loss: 1.3456
2022-06-30 22:40:48 - train: epoch 0079, iter [02700, 05004], lr: 0.026673, loss: 1.1786
2022-06-30 22:41:21 - train: epoch 0079, iter [02800, 05004], lr: 0.026650, loss: 1.1841
2022-06-30 22:41:54 - train: epoch 0079, iter [02900, 05004], lr: 0.026627, loss: 1.2031
2022-06-30 22:42:26 - train: epoch 0079, iter [03000, 05004], lr: 0.026604, loss: 1.3134
2022-06-30 22:42:59 - train: epoch 0079, iter [03100, 05004], lr: 0.026581, loss: 1.3782
2022-06-30 22:43:31 - train: epoch 0079, iter [03200, 05004], lr: 0.026558, loss: 1.6105
2022-06-30 22:44:04 - train: epoch 0079, iter [03300, 05004], lr: 0.026535, loss: 1.2581
2022-06-30 22:44:37 - train: epoch 0079, iter [03400, 05004], lr: 0.026512, loss: 1.1811
2022-06-30 22:45:10 - train: epoch 0079, iter [03500, 05004], lr: 0.026489, loss: 1.5056
2022-06-30 22:45:43 - train: epoch 0079, iter [03600, 05004], lr: 0.026465, loss: 1.2753
2022-06-30 22:46:15 - train: epoch 0079, iter [03700, 05004], lr: 0.026442, loss: 1.2615
2022-06-30 22:46:48 - train: epoch 0079, iter [03800, 05004], lr: 0.026419, loss: 1.4131
2022-06-30 22:47:21 - train: epoch 0079, iter [03900, 05004], lr: 0.026396, loss: 1.2069
2022-06-30 22:47:54 - train: epoch 0079, iter [04000, 05004], lr: 0.026373, loss: 1.2262
2022-06-30 22:48:27 - train: epoch 0079, iter [04100, 05004], lr: 0.026350, loss: 1.6164
2022-06-30 22:49:00 - train: epoch 0079, iter [04200, 05004], lr: 0.026327, loss: 1.2441
2022-06-30 22:49:33 - train: epoch 0079, iter [04300, 05004], lr: 0.026304, loss: 1.0604
2022-06-30 22:50:06 - train: epoch 0079, iter [04400, 05004], lr: 0.026281, loss: 1.5592
2022-06-30 22:50:39 - train: epoch 0079, iter [04500, 05004], lr: 0.026258, loss: 1.6302
2022-06-30 22:51:12 - train: epoch 0079, iter [04600, 05004], lr: 0.026235, loss: 1.5999
2022-06-30 22:51:45 - train: epoch 0079, iter [04700, 05004], lr: 0.026212, loss: 1.3238
2022-06-30 22:52:17 - train: epoch 0079, iter [04800, 05004], lr: 0.026189, loss: 1.5037
2022-06-30 22:52:50 - train: epoch 0079, iter [04900, 05004], lr: 0.026166, loss: 1.5385
2022-06-30 22:53:23 - train: epoch 0079, iter [05000, 05004], lr: 0.026143, loss: 1.2048
2022-06-30 22:53:25 - train: epoch 079, train_loss: 1.3439
2022-06-30 22:54:42 - eval: epoch: 079, acc1: 70.272%, acc5: 90.030%, test_loss: 1.2041, per_image_load_time: 2.393ms, per_image_inference_time: 0.617ms
2022-06-30 22:54:43 - until epoch: 079, best_acc1: 70.312%
2022-06-30 22:54:43 - epoch 080 lr: 0.026142
2022-06-30 22:55:23 - train: epoch 0080, iter [00100, 05004], lr: 0.026119, loss: 1.1236
2022-06-30 22:55:56 - train: epoch 0080, iter [00200, 05004], lr: 0.026096, loss: 1.2563
2022-06-30 22:56:29 - train: epoch 0080, iter [00300, 05004], lr: 0.026073, loss: 1.4467
2022-06-30 22:57:02 - train: epoch 0080, iter [00400, 05004], lr: 0.026050, loss: 1.1955
2022-06-30 22:57:35 - train: epoch 0080, iter [00500, 05004], lr: 0.026027, loss: 1.3694
2022-06-30 22:58:07 - train: epoch 0080, iter [00600, 05004], lr: 0.026004, loss: 1.2403
2022-06-30 22:58:40 - train: epoch 0080, iter [00700, 05004], lr: 0.025981, loss: 1.3226
2022-06-30 22:59:13 - train: epoch 0080, iter [00800, 05004], lr: 0.025958, loss: 1.1442
2022-06-30 22:59:46 - train: epoch 0080, iter [00900, 05004], lr: 0.025935, loss: 1.3556
2022-06-30 23:00:19 - train: epoch 0080, iter [01000, 05004], lr: 0.025912, loss: 1.2377
2022-06-30 23:00:51 - train: epoch 0080, iter [01100, 05004], lr: 0.025890, loss: 1.3075
2022-06-30 23:01:24 - train: epoch 0080, iter [01200, 05004], lr: 0.025867, loss: 1.2653
2022-06-30 23:01:57 - train: epoch 0080, iter [01300, 05004], lr: 0.025844, loss: 1.2629
2022-06-30 23:02:30 - train: epoch 0080, iter [01400, 05004], lr: 0.025821, loss: 1.2801
2022-06-30 23:03:03 - train: epoch 0080, iter [01500, 05004], lr: 0.025798, loss: 1.1876
2022-06-30 23:03:35 - train: epoch 0080, iter [01600, 05004], lr: 0.025775, loss: 1.2602
2022-06-30 23:04:08 - train: epoch 0080, iter [01700, 05004], lr: 0.025752, loss: 1.3599
2022-06-30 23:04:40 - train: epoch 0080, iter [01800, 05004], lr: 0.025729, loss: 1.4595
2022-06-30 23:05:13 - train: epoch 0080, iter [01900, 05004], lr: 0.025706, loss: 1.3000
2022-06-30 23:05:46 - train: epoch 0080, iter [02000, 05004], lr: 0.025684, loss: 1.3131
2022-06-30 23:06:19 - train: epoch 0080, iter [02100, 05004], lr: 0.025661, loss: 1.3823
2022-06-30 23:06:51 - train: epoch 0080, iter [02200, 05004], lr: 0.025638, loss: 1.3921
2022-06-30 23:07:24 - train: epoch 0080, iter [02300, 05004], lr: 0.025615, loss: 1.1847
2022-06-30 23:07:56 - train: epoch 0080, iter [02400, 05004], lr: 0.025592, loss: 1.3355
2022-06-30 23:08:29 - train: epoch 0080, iter [02500, 05004], lr: 0.025569, loss: 1.2082
2022-06-30 23:09:01 - train: epoch 0080, iter [02600, 05004], lr: 0.025547, loss: 1.2273
2022-06-30 23:09:34 - train: epoch 0080, iter [02700, 05004], lr: 0.025524, loss: 1.4293
2022-06-30 23:10:07 - train: epoch 0080, iter [02800, 05004], lr: 0.025501, loss: 1.4547
2022-06-30 23:10:39 - train: epoch 0080, iter [02900, 05004], lr: 0.025478, loss: 1.2200
2022-06-30 23:11:12 - train: epoch 0080, iter [03000, 05004], lr: 0.025455, loss: 1.2818
2022-06-30 23:11:45 - train: epoch 0080, iter [03100, 05004], lr: 0.025433, loss: 1.4775
2022-06-30 23:12:17 - train: epoch 0080, iter [03200, 05004], lr: 0.025410, loss: 1.0205
2022-06-30 23:12:49 - train: epoch 0080, iter [03300, 05004], lr: 0.025387, loss: 1.3119
2022-06-30 23:13:22 - train: epoch 0080, iter [03400, 05004], lr: 0.025364, loss: 1.2589
2022-06-30 23:13:55 - train: epoch 0080, iter [03500, 05004], lr: 0.025341, loss: 1.1661
2022-06-30 23:14:27 - train: epoch 0080, iter [03600, 05004], lr: 0.025319, loss: 1.3949
2022-06-30 23:15:00 - train: epoch 0080, iter [03700, 05004], lr: 0.025296, loss: 1.2684
2022-06-30 23:15:33 - train: epoch 0080, iter [03800, 05004], lr: 0.025273, loss: 1.4592
2022-06-30 23:16:06 - train: epoch 0080, iter [03900, 05004], lr: 0.025251, loss: 1.3093
2022-06-30 23:16:39 - train: epoch 0080, iter [04000, 05004], lr: 0.025228, loss: 1.5126
2022-06-30 23:17:12 - train: epoch 0080, iter [04100, 05004], lr: 0.025205, loss: 1.4401
2022-06-30 23:17:45 - train: epoch 0080, iter [04200, 05004], lr: 0.025182, loss: 1.3309
2022-06-30 23:18:18 - train: epoch 0080, iter [04300, 05004], lr: 0.025160, loss: 1.4382
2022-06-30 23:18:51 - train: epoch 0080, iter [04400, 05004], lr: 0.025137, loss: 1.3063
2022-06-30 23:19:23 - train: epoch 0080, iter [04500, 05004], lr: 0.025114, loss: 1.2714
2022-06-30 23:19:56 - train: epoch 0080, iter [04600, 05004], lr: 0.025092, loss: 1.4609
2022-06-30 23:20:28 - train: epoch 0080, iter [04700, 05004], lr: 0.025069, loss: 1.4633
2022-06-30 23:21:01 - train: epoch 0080, iter [04800, 05004], lr: 0.025046, loss: 1.3266
2022-06-30 23:21:34 - train: epoch 0080, iter [04900, 05004], lr: 0.025024, loss: 1.4779
2022-06-30 23:22:07 - train: epoch 0080, iter [05000, 05004], lr: 0.025001, loss: 1.1456
2022-06-30 23:22:08 - train: epoch 080, train_loss: 1.3304
2022-06-30 23:23:24 - eval: epoch: 080, acc1: 70.716%, acc5: 90.194%, test_loss: 1.1864, per_image_load_time: 2.296ms, per_image_inference_time: 0.641ms
2022-06-30 23:23:25 - until epoch: 080, best_acc1: 70.716%
2022-06-30 23:23:25 - epoch 081 lr: 0.025000
2022-06-30 23:24:05 - train: epoch 0081, iter [00100, 05004], lr: 0.024977, loss: 1.0698
2022-06-30 23:24:37 - train: epoch 0081, iter [00200, 05004], lr: 0.024955, loss: 1.3534
2022-06-30 23:25:10 - train: epoch 0081, iter [00300, 05004], lr: 0.024932, loss: 1.2337
2022-06-30 23:25:43 - train: epoch 0081, iter [00400, 05004], lr: 0.024909, loss: 1.4401
2022-06-30 23:26:16 - train: epoch 0081, iter [00500, 05004], lr: 0.024887, loss: 1.3858
2022-06-30 23:26:48 - train: epoch 0081, iter [00600, 05004], lr: 0.024864, loss: 1.3526
2022-06-30 23:27:20 - train: epoch 0081, iter [00700, 05004], lr: 0.024842, loss: 1.1488
2022-06-30 23:27:53 - train: epoch 0081, iter [00800, 05004], lr: 0.024819, loss: 1.3935
2022-06-30 23:28:25 - train: epoch 0081, iter [00900, 05004], lr: 0.024796, loss: 1.1615
2022-06-30 23:28:58 - train: epoch 0081, iter [01000, 05004], lr: 0.024774, loss: 1.2648
2022-06-30 23:29:31 - train: epoch 0081, iter [01100, 05004], lr: 0.024751, loss: 1.2728
2022-06-30 23:30:03 - train: epoch 0081, iter [01200, 05004], lr: 0.024729, loss: 1.2517
2022-06-30 23:30:36 - train: epoch 0081, iter [01300, 05004], lr: 0.024706, loss: 1.2374
2022-06-30 23:31:09 - train: epoch 0081, iter [01400, 05004], lr: 0.024684, loss: 0.9879
2022-06-30 23:31:41 - train: epoch 0081, iter [01500, 05004], lr: 0.024661, loss: 1.3939
2022-06-30 23:32:14 - train: epoch 0081, iter [01600, 05004], lr: 0.024638, loss: 1.1806
2022-06-30 23:32:47 - train: epoch 0081, iter [01700, 05004], lr: 0.024616, loss: 1.4017
2022-06-30 23:33:20 - train: epoch 0081, iter [01800, 05004], lr: 0.024593, loss: 1.2456
2022-06-30 23:33:52 - train: epoch 0081, iter [01900, 05004], lr: 0.024571, loss: 1.2818
2022-06-30 23:34:25 - train: epoch 0081, iter [02000, 05004], lr: 0.024548, loss: 1.3189
2022-06-30 23:34:58 - train: epoch 0081, iter [02100, 05004], lr: 0.024526, loss: 1.2920
2022-06-30 23:35:31 - train: epoch 0081, iter [02200, 05004], lr: 0.024503, loss: 1.2712
2022-06-30 23:36:03 - train: epoch 0081, iter [02300, 05004], lr: 0.024481, loss: 1.2405
2022-06-30 23:36:36 - train: epoch 0081, iter [02400, 05004], lr: 0.024458, loss: 1.4140
2022-06-30 23:37:09 - train: epoch 0081, iter [02500, 05004], lr: 0.024436, loss: 1.4862
2022-06-30 23:37:42 - train: epoch 0081, iter [02600, 05004], lr: 0.024413, loss: 1.4695
2022-06-30 23:38:14 - train: epoch 0081, iter [02700, 05004], lr: 0.024391, loss: 1.2016
2022-06-30 23:38:47 - train: epoch 0081, iter [02800, 05004], lr: 0.024368, loss: 1.2526
2022-06-30 23:39:20 - train: epoch 0081, iter [02900, 05004], lr: 0.024346, loss: 1.1081
2022-06-30 23:39:52 - train: epoch 0081, iter [03000, 05004], lr: 0.024323, loss: 1.3889
2022-06-30 23:40:25 - train: epoch 0081, iter [03100, 05004], lr: 0.024301, loss: 1.1908
2022-06-30 23:40:58 - train: epoch 0081, iter [03200, 05004], lr: 0.024279, loss: 1.3097
2022-06-30 23:41:30 - train: epoch 0081, iter [03300, 05004], lr: 0.024256, loss: 1.3312
2022-06-30 23:42:03 - train: epoch 0081, iter [03400, 05004], lr: 0.024234, loss: 1.3716
2022-06-30 23:42:36 - train: epoch 0081, iter [03500, 05004], lr: 0.024211, loss: 1.3581
2022-06-30 23:43:09 - train: epoch 0081, iter [03600, 05004], lr: 0.024189, loss: 1.1844
2022-06-30 23:43:41 - train: epoch 0081, iter [03700, 05004], lr: 0.024167, loss: 1.4977
2022-06-30 23:44:14 - train: epoch 0081, iter [03800, 05004], lr: 0.024144, loss: 1.1762
2022-06-30 23:44:47 - train: epoch 0081, iter [03900, 05004], lr: 0.024122, loss: 1.3380
2022-06-30 23:45:20 - train: epoch 0081, iter [04000, 05004], lr: 0.024099, loss: 1.1218
2022-06-30 23:45:53 - train: epoch 0081, iter [04100, 05004], lr: 0.024077, loss: 1.4112
2022-06-30 23:46:26 - train: epoch 0081, iter [04200, 05004], lr: 0.024055, loss: 1.2947
2022-06-30 23:46:58 - train: epoch 0081, iter [04300, 05004], lr: 0.024032, loss: 1.3419
2022-06-30 23:47:31 - train: epoch 0081, iter [04400, 05004], lr: 0.024010, loss: 1.3966
2022-06-30 23:48:04 - train: epoch 0081, iter [04500, 05004], lr: 0.023988, loss: 1.3582
2022-06-30 23:48:37 - train: epoch 0081, iter [04600, 05004], lr: 0.023965, loss: 1.1512
2022-06-30 23:49:10 - train: epoch 0081, iter [04700, 05004], lr: 0.023943, loss: 1.3468
2022-06-30 23:49:42 - train: epoch 0081, iter [04800, 05004], lr: 0.023921, loss: 1.3868
2022-06-30 23:50:15 - train: epoch 0081, iter [04900, 05004], lr: 0.023898, loss: 1.3995
2022-06-30 23:50:48 - train: epoch 0081, iter [05000, 05004], lr: 0.023876, loss: 1.1471
2022-06-30 23:50:49 - train: epoch 081, train_loss: 1.3138
2022-06-30 23:52:06 - eval: epoch: 081, acc1: 70.562%, acc5: 90.128%, test_loss: 1.1918, per_image_load_time: 2.038ms, per_image_inference_time: 0.624ms
2022-06-30 23:52:06 - until epoch: 081, best_acc1: 70.716%
2022-06-30 23:52:06 - epoch 082 lr: 0.023875
2022-06-30 23:52:47 - train: epoch 0082, iter [00100, 05004], lr: 0.023853, loss: 1.0282
2022-06-30 23:53:19 - train: epoch 0082, iter [00200, 05004], lr: 0.023830, loss: 1.0992
2022-06-30 23:53:51 - train: epoch 0082, iter [00300, 05004], lr: 0.023808, loss: 1.3292
2022-06-30 23:54:23 - train: epoch 0082, iter [00400, 05004], lr: 0.023786, loss: 1.3393
2022-06-30 23:54:56 - train: epoch 0082, iter [00500, 05004], lr: 0.023764, loss: 1.3448
2022-06-30 23:55:28 - train: epoch 0082, iter [00600, 05004], lr: 0.023741, loss: 1.1837
2022-06-30 23:56:01 - train: epoch 0082, iter [00700, 05004], lr: 0.023719, loss: 1.4244
2022-06-30 23:56:33 - train: epoch 0082, iter [00800, 05004], lr: 0.023697, loss: 1.2831
2022-06-30 23:57:05 - train: epoch 0082, iter [00900, 05004], lr: 0.023675, loss: 1.4707
2022-06-30 23:57:38 - train: epoch 0082, iter [01000, 05004], lr: 0.023652, loss: 1.3418
2022-06-30 23:58:10 - train: epoch 0082, iter [01100, 05004], lr: 0.023630, loss: 1.4361
2022-06-30 23:58:43 - train: epoch 0082, iter [01200, 05004], lr: 0.023608, loss: 1.3234
2022-06-30 23:59:15 - train: epoch 0082, iter [01300, 05004], lr: 0.023586, loss: 1.5972
2022-06-30 23:59:48 - train: epoch 0082, iter [01400, 05004], lr: 0.023564, loss: 1.2813
2022-07-01 00:00:21 - train: epoch 0082, iter [01500, 05004], lr: 0.023541, loss: 1.1844
2022-07-01 00:00:54 - train: epoch 0082, iter [01600, 05004], lr: 0.023519, loss: 1.3599
2022-07-01 00:01:27 - train: epoch 0082, iter [01700, 05004], lr: 0.023497, loss: 1.1763
2022-07-01 00:02:00 - train: epoch 0082, iter [01800, 05004], lr: 0.023475, loss: 1.2200
2022-07-01 00:02:33 - train: epoch 0082, iter [01900, 05004], lr: 0.023453, loss: 1.1460
2022-07-01 00:03:06 - train: epoch 0082, iter [02000, 05004], lr: 0.023430, loss: 1.1255
2022-07-01 00:03:38 - train: epoch 0082, iter [02100, 05004], lr: 0.023408, loss: 1.2510
2022-07-01 00:04:11 - train: epoch 0082, iter [02200, 05004], lr: 0.023386, loss: 1.3036
2022-07-01 00:04:44 - train: epoch 0082, iter [02300, 05004], lr: 0.023364, loss: 1.1971
2022-07-01 00:05:17 - train: epoch 0082, iter [02400, 05004], lr: 0.023342, loss: 1.3851
2022-07-01 00:05:49 - train: epoch 0082, iter [02500, 05004], lr: 0.023320, loss: 1.2220
2022-07-01 00:06:22 - train: epoch 0082, iter [02600, 05004], lr: 0.023298, loss: 1.2714
2022-07-01 00:06:55 - train: epoch 0082, iter [02700, 05004], lr: 0.023275, loss: 1.2296
2022-07-01 00:07:28 - train: epoch 0082, iter [02800, 05004], lr: 0.023253, loss: 1.1534
2022-07-01 00:08:00 - train: epoch 0082, iter [02900, 05004], lr: 0.023231, loss: 1.1074
2022-07-01 00:08:33 - train: epoch 0082, iter [03000, 05004], lr: 0.023209, loss: 1.2624
2022-07-01 00:09:06 - train: epoch 0082, iter [03100, 05004], lr: 0.023187, loss: 1.2062
2022-07-01 00:09:38 - train: epoch 0082, iter [03200, 05004], lr: 0.023165, loss: 1.5959
2022-07-01 00:10:11 - train: epoch 0082, iter [03300, 05004], lr: 0.023143, loss: 1.2488
2022-07-01 00:10:44 - train: epoch 0082, iter [03400, 05004], lr: 0.023121, loss: 1.2718
2022-07-01 00:11:17 - train: epoch 0082, iter [03500, 05004], lr: 0.023099, loss: 1.1653
2022-07-01 00:11:49 - train: epoch 0082, iter [03600, 05004], lr: 0.023077, loss: 1.1318
2022-07-01 00:12:22 - train: epoch 0082, iter [03700, 05004], lr: 0.023055, loss: 1.2382
2022-07-01 00:12:54 - train: epoch 0082, iter [03800, 05004], lr: 0.023033, loss: 1.5137
2022-07-01 00:13:27 - train: epoch 0082, iter [03900, 05004], lr: 0.023011, loss: 1.3621
2022-07-01 00:14:00 - train: epoch 0082, iter [04000, 05004], lr: 0.022989, loss: 1.3743
2022-07-01 00:14:33 - train: epoch 0082, iter [04100, 05004], lr: 0.022967, loss: 1.3235
2022-07-01 00:15:06 - train: epoch 0082, iter [04200, 05004], lr: 0.022945, loss: 1.4467
2022-07-01 00:15:39 - train: epoch 0082, iter [04300, 05004], lr: 0.022923, loss: 1.1403
2022-07-01 00:16:11 - train: epoch 0082, iter [04400, 05004], lr: 0.022901, loss: 1.2443
2022-07-01 00:16:44 - train: epoch 0082, iter [04500, 05004], lr: 0.022879, loss: 1.3282
2022-07-01 00:17:17 - train: epoch 0082, iter [04600, 05004], lr: 0.022857, loss: 1.2872
2022-07-01 00:17:50 - train: epoch 0082, iter [04700, 05004], lr: 0.022835, loss: 1.1614
2022-07-01 00:18:22 - train: epoch 0082, iter [04800, 05004], lr: 0.022813, loss: 1.1680
2022-07-01 00:18:55 - train: epoch 0082, iter [04900, 05004], lr: 0.022791, loss: 1.3117
2022-07-01 00:19:28 - train: epoch 0082, iter [05000, 05004], lr: 0.022769, loss: 1.3191
2022-07-01 00:19:29 - train: epoch 082, train_loss: 1.2982
2022-07-01 00:20:45 - eval: epoch: 082, acc1: 71.262%, acc5: 90.592%, test_loss: 1.1621, per_image_load_time: 1.075ms, per_image_inference_time: 0.623ms
2022-07-01 00:20:46 - until epoch: 082, best_acc1: 71.262%
2022-07-01 00:20:46 - epoch 083 lr: 0.022768
2022-07-01 00:21:26 - train: epoch 0083, iter [00100, 05004], lr: 0.022746, loss: 1.1994
2022-07-01 00:21:58 - train: epoch 0083, iter [00200, 05004], lr: 0.022724, loss: 1.1700
2022-07-01 00:22:31 - train: epoch 0083, iter [00300, 05004], lr: 0.022702, loss: 1.3233
2022-07-01 00:23:03 - train: epoch 0083, iter [00400, 05004], lr: 0.022680, loss: 1.4130
2022-07-01 00:23:36 - train: epoch 0083, iter [00500, 05004], lr: 0.022658, loss: 1.2310
2022-07-01 00:24:08 - train: epoch 0083, iter [00600, 05004], lr: 0.022637, loss: 1.3026
2022-07-01 00:24:41 - train: epoch 0083, iter [00700, 05004], lr: 0.022615, loss: 1.2064
2022-07-01 00:25:13 - train: epoch 0083, iter [00800, 05004], lr: 0.022593, loss: 1.2750
2022-07-01 00:25:46 - train: epoch 0083, iter [00900, 05004], lr: 0.022571, loss: 1.2747
2022-07-01 00:26:19 - train: epoch 0083, iter [01000, 05004], lr: 0.022549, loss: 1.3540
2022-07-01 00:26:51 - train: epoch 0083, iter [01100, 05004], lr: 0.022527, loss: 1.3005
2022-07-01 00:27:24 - train: epoch 0083, iter [01200, 05004], lr: 0.022505, loss: 1.2114
2022-07-01 00:27:57 - train: epoch 0083, iter [01300, 05004], lr: 0.022483, loss: 1.1411
2022-07-01 00:28:30 - train: epoch 0083, iter [01400, 05004], lr: 0.022462, loss: 1.2708
2022-07-01 00:29:03 - train: epoch 0083, iter [01500, 05004], lr: 0.022440, loss: 1.1085
2022-07-01 00:29:35 - train: epoch 0083, iter [01600, 05004], lr: 0.022418, loss: 1.2846
2022-07-01 00:30:08 - train: epoch 0083, iter [01700, 05004], lr: 0.022396, loss: 1.4570
2022-07-01 00:30:40 - train: epoch 0083, iter [01800, 05004], lr: 0.022374, loss: 1.4566
2022-07-01 00:31:13 - train: epoch 0083, iter [01900, 05004], lr: 0.022353, loss: 1.0769
2022-07-01 00:31:45 - train: epoch 0083, iter [02000, 05004], lr: 0.022331, loss: 0.9986
2022-07-01 00:32:18 - train: epoch 0083, iter [02100, 05004], lr: 0.022309, loss: 1.1975
2022-07-01 00:32:50 - train: epoch 0083, iter [02200, 05004], lr: 0.022287, loss: 1.2881
2022-07-01 00:33:23 - train: epoch 0083, iter [02300, 05004], lr: 0.022265, loss: 1.2890
2022-07-01 00:33:56 - train: epoch 0083, iter [02400, 05004], lr: 0.022244, loss: 1.2103
2022-07-01 00:34:29 - train: epoch 0083, iter [02500, 05004], lr: 0.022222, loss: 1.3599
2022-07-01 00:35:01 - train: epoch 0083, iter [02600, 05004], lr: 0.022200, loss: 1.2060
2022-07-01 00:35:34 - train: epoch 0083, iter [02700, 05004], lr: 0.022178, loss: 1.2264
2022-07-01 00:36:07 - train: epoch 0083, iter [02800, 05004], lr: 0.022157, loss: 1.1778
2022-07-01 00:36:40 - train: epoch 0083, iter [02900, 05004], lr: 0.022135, loss: 1.3478
2022-07-01 00:37:13 - train: epoch 0083, iter [03000, 05004], lr: 0.022113, loss: 1.4572
2022-07-01 00:37:45 - train: epoch 0083, iter [03100, 05004], lr: 0.022092, loss: 1.1797
2022-07-01 00:38:18 - train: epoch 0083, iter [03200, 05004], lr: 0.022070, loss: 1.1393
2022-07-01 00:38:51 - train: epoch 0083, iter [03300, 05004], lr: 0.022048, loss: 1.4809
2022-07-01 00:39:24 - train: epoch 0083, iter [03400, 05004], lr: 0.022026, loss: 1.3912
2022-07-01 00:39:56 - train: epoch 0083, iter [03500, 05004], lr: 0.022005, loss: 1.3864
2022-07-01 00:40:29 - train: epoch 0083, iter [03600, 05004], lr: 0.021983, loss: 1.3874
2022-07-01 00:41:02 - train: epoch 0083, iter [03700, 05004], lr: 0.021961, loss: 1.3321
2022-07-01 00:41:35 - train: epoch 0083, iter [03800, 05004], lr: 0.021940, loss: 1.2290
2022-07-01 00:42:08 - train: epoch 0083, iter [03900, 05004], lr: 0.021918, loss: 1.2803
2022-07-01 00:42:40 - train: epoch 0083, iter [04000, 05004], lr: 0.021897, loss: 1.4520
2022-07-01 00:43:13 - train: epoch 0083, iter [04100, 05004], lr: 0.021875, loss: 1.2791
2022-07-01 00:43:45 - train: epoch 0083, iter [04200, 05004], lr: 0.021853, loss: 1.5810
2022-07-01 00:44:18 - train: epoch 0083, iter [04300, 05004], lr: 0.021832, loss: 1.3219
2022-07-01 00:44:51 - train: epoch 0083, iter [04400, 05004], lr: 0.021810, loss: 1.1908
2022-07-01 00:45:24 - train: epoch 0083, iter [04500, 05004], lr: 0.021788, loss: 1.4118
2022-07-01 00:45:56 - train: epoch 0083, iter [04600, 05004], lr: 0.021767, loss: 1.3861
2022-07-01 00:46:29 - train: epoch 0083, iter [04700, 05004], lr: 0.021745, loss: 1.3807
2022-07-01 00:47:02 - train: epoch 0083, iter [04800, 05004], lr: 0.021724, loss: 1.2817
2022-07-01 00:47:34 - train: epoch 0083, iter [04900, 05004], lr: 0.021702, loss: 1.5039
2022-07-01 00:48:07 - train: epoch 0083, iter [05000, 05004], lr: 0.021681, loss: 1.4329
2022-07-01 00:48:08 - train: epoch 083, train_loss: 1.2827
2022-07-01 00:49:25 - eval: epoch: 083, acc1: 71.000%, acc5: 90.294%, test_loss: 1.1781, per_image_load_time: 1.049ms, per_image_inference_time: 0.635ms
2022-07-01 00:49:26 - until epoch: 083, best_acc1: 71.262%
2022-07-01 00:49:26 - epoch 084 lr: 0.021679
2022-07-01 00:50:06 - train: epoch 0084, iter [00100, 05004], lr: 0.021658, loss: 1.2328
2022-07-01 00:50:39 - train: epoch 0084, iter [00200, 05004], lr: 0.021637, loss: 1.3732
2022-07-01 00:51:11 - train: epoch 0084, iter [00300, 05004], lr: 0.021615, loss: 1.0554
2022-07-01 00:51:43 - train: epoch 0084, iter [00400, 05004], lr: 0.021594, loss: 1.2541
2022-07-01 00:52:16 - train: epoch 0084, iter [00500, 05004], lr: 0.021572, loss: 1.2810
2022-07-01 00:52:48 - train: epoch 0084, iter [00600, 05004], lr: 0.021550, loss: 1.5350
2022-07-01 00:53:21 - train: epoch 0084, iter [00700, 05004], lr: 0.021529, loss: 1.3648
2022-07-01 00:53:54 - train: epoch 0084, iter [00800, 05004], lr: 0.021507, loss: 1.3245
2022-07-01 00:54:26 - train: epoch 0084, iter [00900, 05004], lr: 0.021486, loss: 1.2115
2022-07-01 00:54:59 - train: epoch 0084, iter [01000, 05004], lr: 0.021464, loss: 1.3569
2022-07-01 00:55:31 - train: epoch 0084, iter [01100, 05004], lr: 0.021443, loss: 1.1237
2022-07-01 00:56:04 - train: epoch 0084, iter [01200, 05004], lr: 0.021422, loss: 1.4342
2022-07-01 00:56:37 - train: epoch 0084, iter [01300, 05004], lr: 0.021400, loss: 1.2324
2022-07-01 00:57:09 - train: epoch 0084, iter [01400, 05004], lr: 0.021379, loss: 1.2918
2022-07-01 00:57:42 - train: epoch 0084, iter [01500, 05004], lr: 0.021357, loss: 1.3188
2022-07-01 00:58:14 - train: epoch 0084, iter [01600, 05004], lr: 0.021336, loss: 1.1458
2022-07-01 00:58:47 - train: epoch 0084, iter [01700, 05004], lr: 0.021314, loss: 1.3337
2022-07-01 00:59:19 - train: epoch 0084, iter [01800, 05004], lr: 0.021293, loss: 1.2089
2022-07-01 00:59:52 - train: epoch 0084, iter [01900, 05004], lr: 0.021271, loss: 1.3172
2022-07-01 01:00:24 - train: epoch 0084, iter [02000, 05004], lr: 0.021250, loss: 1.3718
2022-07-01 01:00:57 - train: epoch 0084, iter [02100, 05004], lr: 0.021229, loss: 1.1829
2022-07-01 01:01:30 - train: epoch 0084, iter [02200, 05004], lr: 0.021207, loss: 0.9726
2022-07-01 01:02:02 - train: epoch 0084, iter [02300, 05004], lr: 0.021186, loss: 1.2864
2022-07-01 01:02:35 - train: epoch 0084, iter [02400, 05004], lr: 0.021165, loss: 1.1680
2022-07-01 01:03:08 - train: epoch 0084, iter [02500, 05004], lr: 0.021143, loss: 1.4069
2022-07-01 01:03:41 - train: epoch 0084, iter [02600, 05004], lr: 0.021122, loss: 1.4466
2022-07-01 01:04:13 - train: epoch 0084, iter [02700, 05004], lr: 0.021100, loss: 1.2682
2022-07-01 01:04:46 - train: epoch 0084, iter [02800, 05004], lr: 0.021079, loss: 1.3585
2022-07-01 01:05:19 - train: epoch 0084, iter [02900, 05004], lr: 0.021058, loss: 1.1780
2022-07-01 01:05:52 - train: epoch 0084, iter [03000, 05004], lr: 0.021036, loss: 1.2655
2022-07-01 01:06:24 - train: epoch 0084, iter [03100, 05004], lr: 0.021015, loss: 1.1538
2022-07-01 01:06:57 - train: epoch 0084, iter [03200, 05004], lr: 0.020994, loss: 1.2197
2022-07-01 01:07:30 - train: epoch 0084, iter [03300, 05004], lr: 0.020973, loss: 1.3231
2022-07-01 01:08:03 - train: epoch 0084, iter [03400, 05004], lr: 0.020951, loss: 1.1808
2022-07-01 01:08:35 - train: epoch 0084, iter [03500, 05004], lr: 0.020930, loss: 1.0961
2022-07-01 01:09:08 - train: epoch 0084, iter [03600, 05004], lr: 0.020909, loss: 1.3974
2022-07-01 01:09:41 - train: epoch 0084, iter [03700, 05004], lr: 0.020887, loss: 1.3331
2022-07-01 01:10:13 - train: epoch 0084, iter [03800, 05004], lr: 0.020866, loss: 1.5576
2022-07-01 01:10:46 - train: epoch 0084, iter [03900, 05004], lr: 0.020845, loss: 1.3205
2022-07-01 01:11:19 - train: epoch 0084, iter [04000, 05004], lr: 0.020824, loss: 1.2156
2022-07-01 01:11:52 - train: epoch 0084, iter [04100, 05004], lr: 0.020802, loss: 1.2543
2022-07-01 01:12:24 - train: epoch 0084, iter [04200, 05004], lr: 0.020781, loss: 1.3749
2022-07-01 01:12:57 - train: epoch 0084, iter [04300, 05004], lr: 0.020760, loss: 1.2477
2022-07-01 01:13:29 - train: epoch 0084, iter [04400, 05004], lr: 0.020739, loss: 1.5774
2022-07-01 01:14:02 - train: epoch 0084, iter [04500, 05004], lr: 0.020718, loss: 1.2745
2022-07-01 01:14:35 - train: epoch 0084, iter [04600, 05004], lr: 0.020696, loss: 1.3232
2022-07-01 01:15:07 - train: epoch 0084, iter [04700, 05004], lr: 0.020675, loss: 1.5238
2022-07-01 01:15:40 - train: epoch 0084, iter [04800, 05004], lr: 0.020654, loss: 1.1702
2022-07-01 01:16:12 - train: epoch 0084, iter [04900, 05004], lr: 0.020633, loss: 1.0857
2022-07-01 01:16:45 - train: epoch 0084, iter [05000, 05004], lr: 0.020612, loss: 1.3300
2022-07-01 01:16:46 - train: epoch 084, train_loss: 1.2658
2022-07-01 01:18:02 - eval: epoch: 084, acc1: 71.398%, acc5: 90.692%, test_loss: 1.1450, per_image_load_time: 1.683ms, per_image_inference_time: 0.642ms
2022-07-01 01:18:03 - until epoch: 084, best_acc1: 71.398%
2022-07-01 01:18:03 - epoch 085 lr: 0.020611
2022-07-01 01:18:42 - train: epoch 0085, iter [00100, 05004], lr: 0.020590, loss: 1.0812
2022-07-01 01:19:14 - train: epoch 0085, iter [00200, 05004], lr: 0.020568, loss: 1.1655
2022-07-01 01:19:46 - train: epoch 0085, iter [00300, 05004], lr: 0.020547, loss: 1.3070
2022-07-01 01:20:19 - train: epoch 0085, iter [00400, 05004], lr: 0.020526, loss: 1.1553
2022-07-01 01:20:51 - train: epoch 0085, iter [00500, 05004], lr: 0.020505, loss: 1.4415
2022-07-01 01:21:23 - train: epoch 0085, iter [00600, 05004], lr: 0.020484, loss: 1.0240
2022-07-01 01:21:56 - train: epoch 0085, iter [00700, 05004], lr: 0.020463, loss: 1.1338
2022-07-01 01:22:28 - train: epoch 0085, iter [00800, 05004], lr: 0.020442, loss: 1.0745
2022-07-01 01:23:01 - train: epoch 0085, iter [00900, 05004], lr: 0.020421, loss: 1.1760
2022-07-01 01:23:34 - train: epoch 0085, iter [01000, 05004], lr: 0.020400, loss: 1.3286
2022-07-01 01:24:07 - train: epoch 0085, iter [01100, 05004], lr: 0.020378, loss: 1.2728
2022-07-01 01:24:40 - train: epoch 0085, iter [01200, 05004], lr: 0.020357, loss: 1.1602
2022-07-01 01:25:13 - train: epoch 0085, iter [01300, 05004], lr: 0.020336, loss: 1.1725
2022-07-01 01:25:45 - train: epoch 0085, iter [01400, 05004], lr: 0.020315, loss: 1.2642
2022-07-01 01:26:18 - train: epoch 0085, iter [01500, 05004], lr: 0.020294, loss: 1.1796
2022-07-01 01:26:51 - train: epoch 0085, iter [01600, 05004], lr: 0.020273, loss: 1.1910
2022-07-01 01:27:24 - train: epoch 0085, iter [01700, 05004], lr: 0.020252, loss: 1.5003
2022-07-01 01:27:56 - train: epoch 0085, iter [01800, 05004], lr: 0.020231, loss: 1.0528
2022-07-01 01:28:29 - train: epoch 0085, iter [01900, 05004], lr: 0.020210, loss: 1.1839
2022-07-01 01:29:02 - train: epoch 0085, iter [02000, 05004], lr: 0.020189, loss: 1.2651
2022-07-01 01:29:34 - train: epoch 0085, iter [02100, 05004], lr: 0.020168, loss: 0.9810
2022-07-01 01:30:07 - train: epoch 0085, iter [02200, 05004], lr: 0.020147, loss: 1.3142
2022-07-01 01:30:40 - train: epoch 0085, iter [02300, 05004], lr: 0.020126, loss: 1.0783
2022-07-01 01:31:13 - train: epoch 0085, iter [02400, 05004], lr: 0.020105, loss: 1.3727
2022-07-01 01:31:45 - train: epoch 0085, iter [02500, 05004], lr: 0.020084, loss: 1.5841
2022-07-01 01:32:19 - train: epoch 0085, iter [02600, 05004], lr: 0.020063, loss: 1.2958
2022-07-01 01:32:51 - train: epoch 0085, iter [02700, 05004], lr: 0.020042, loss: 1.1252
2022-07-01 01:33:24 - train: epoch 0085, iter [02800, 05004], lr: 0.020021, loss: 1.5580
2022-07-01 01:33:57 - train: epoch 0085, iter [02900, 05004], lr: 0.020000, loss: 1.3702
2022-07-01 01:34:30 - train: epoch 0085, iter [03000, 05004], lr: 0.019979, loss: 1.3965
2022-07-01 01:35:03 - train: epoch 0085, iter [03100, 05004], lr: 0.019959, loss: 1.2486
2022-07-01 01:35:36 - train: epoch 0085, iter [03200, 05004], lr: 0.019938, loss: 1.1664
2022-07-01 01:36:09 - train: epoch 0085, iter [03300, 05004], lr: 0.019917, loss: 1.2005
2022-07-01 01:36:42 - train: epoch 0085, iter [03400, 05004], lr: 0.019896, loss: 1.3780
2022-07-01 01:37:15 - train: epoch 0085, iter [03500, 05004], lr: 0.019875, loss: 1.3308
2022-07-01 01:37:48 - train: epoch 0085, iter [03600, 05004], lr: 0.019854, loss: 1.3396
2022-07-01 01:38:21 - train: epoch 0085, iter [03700, 05004], lr: 0.019833, loss: 1.1794
2022-07-01 01:38:54 - train: epoch 0085, iter [03800, 05004], lr: 0.019812, loss: 1.1686
2022-07-01 01:39:27 - train: epoch 0085, iter [03900, 05004], lr: 0.019792, loss: 1.2353
2022-07-01 01:40:00 - train: epoch 0085, iter [04000, 05004], lr: 0.019771, loss: 1.4965
2022-07-01 01:40:33 - train: epoch 0085, iter [04100, 05004], lr: 0.019750, loss: 1.2586
2022-07-01 01:41:06 - train: epoch 0085, iter [04200, 05004], lr: 0.019729, loss: 1.3810
2022-07-01 01:41:38 - train: epoch 0085, iter [04300, 05004], lr: 0.019708, loss: 1.2379
2022-07-01 01:42:11 - train: epoch 0085, iter [04400, 05004], lr: 0.019687, loss: 1.2968
2022-07-01 01:42:44 - train: epoch 0085, iter [04500, 05004], lr: 0.019667, loss: 1.4297
2022-07-01 01:43:17 - train: epoch 0085, iter [04600, 05004], lr: 0.019646, loss: 1.2946
2022-07-01 01:43:50 - train: epoch 0085, iter [04700, 05004], lr: 0.019625, loss: 1.5940
2022-07-01 01:44:23 - train: epoch 0085, iter [04800, 05004], lr: 0.019604, loss: 1.0168
2022-07-01 01:44:56 - train: epoch 0085, iter [04900, 05004], lr: 0.019584, loss: 1.3233
2022-07-01 01:45:28 - train: epoch 0085, iter [05000, 05004], lr: 0.019563, loss: 1.1367
2022-07-01 01:45:30 - train: epoch 085, train_loss: 1.2530
2022-07-01 01:46:46 - eval: epoch: 085, acc1: 71.918%, acc5: 90.856%, test_loss: 1.1323, per_image_load_time: 2.337ms, per_image_inference_time: 0.619ms
2022-07-01 01:46:47 - until epoch: 085, best_acc1: 71.918%
2022-07-01 01:46:47 - epoch 086 lr: 0.019562
2022-07-01 01:47:27 - train: epoch 0086, iter [00100, 05004], lr: 0.019541, loss: 1.0429
2022-07-01 01:47:58 - train: epoch 0086, iter [00200, 05004], lr: 0.019520, loss: 1.1201
2022-07-01 01:48:31 - train: epoch 0086, iter [00300, 05004], lr: 0.019500, loss: 1.3171
2022-07-01 01:49:04 - train: epoch 0086, iter [00400, 05004], lr: 0.019479, loss: 1.2785
2022-07-01 01:49:36 - train: epoch 0086, iter [00500, 05004], lr: 0.019458, loss: 1.2160
2022-07-01 01:50:08 - train: epoch 0086, iter [00600, 05004], lr: 0.019438, loss: 1.3559
2022-07-01 01:50:41 - train: epoch 0086, iter [00700, 05004], lr: 0.019417, loss: 1.1025
2022-07-01 01:51:13 - train: epoch 0086, iter [00800, 05004], lr: 0.019396, loss: 1.3145
2022-07-01 01:51:45 - train: epoch 0086, iter [00900, 05004], lr: 0.019375, loss: 1.3881
2022-07-01 01:52:18 - train: epoch 0086, iter [01000, 05004], lr: 0.019355, loss: 1.2758
2022-07-01 01:52:50 - train: epoch 0086, iter [01100, 05004], lr: 0.019334, loss: 1.2529
2022-07-01 01:53:23 - train: epoch 0086, iter [01200, 05004], lr: 0.019313, loss: 1.0703
2022-07-01 01:53:56 - train: epoch 0086, iter [01300, 05004], lr: 0.019293, loss: 1.3186
2022-07-01 01:54:29 - train: epoch 0086, iter [01400, 05004], lr: 0.019272, loss: 1.0708
2022-07-01 01:55:01 - train: epoch 0086, iter [01500, 05004], lr: 0.019252, loss: 1.1086
2022-07-01 01:55:33 - train: epoch 0086, iter [01600, 05004], lr: 0.019231, loss: 1.3038
2022-07-01 01:56:06 - train: epoch 0086, iter [01700, 05004], lr: 0.019210, loss: 1.2457
2022-07-01 01:56:38 - train: epoch 0086, iter [01800, 05004], lr: 0.019190, loss: 1.3628
2022-07-01 01:57:10 - train: epoch 0086, iter [01900, 05004], lr: 0.019169, loss: 1.3918
2022-07-01 01:57:43 - train: epoch 0086, iter [02000, 05004], lr: 0.019149, loss: 1.2781
2022-07-01 01:58:16 - train: epoch 0086, iter [02100, 05004], lr: 0.019128, loss: 1.1514
2022-07-01 01:58:49 - train: epoch 0086, iter [02200, 05004], lr: 0.019107, loss: 1.2694
2022-07-01 01:59:22 - train: epoch 0086, iter [02300, 05004], lr: 0.019087, loss: 1.1682
2022-07-01 01:59:54 - train: epoch 0086, iter [02400, 05004], lr: 0.019066, loss: 0.9610
2022-07-01 02:00:27 - train: epoch 0086, iter [02500, 05004], lr: 0.019046, loss: 1.1487
2022-07-01 02:01:00 - train: epoch 0086, iter [02600, 05004], lr: 0.019025, loss: 1.2624
2022-07-01 02:01:33 - train: epoch 0086, iter [02700, 05004], lr: 0.019005, loss: 1.0910
2022-07-01 02:02:06 - train: epoch 0086, iter [02800, 05004], lr: 0.018984, loss: 1.3552
2022-07-01 02:02:39 - train: epoch 0086, iter [02900, 05004], lr: 0.018964, loss: 1.1334
2022-07-01 02:03:11 - train: epoch 0086, iter [03000, 05004], lr: 0.018943, loss: 1.3382
2022-07-01 02:03:44 - train: epoch 0086, iter [03100, 05004], lr: 0.018923, loss: 1.2829
2022-07-01 02:04:16 - train: epoch 0086, iter [03200, 05004], lr: 0.018902, loss: 1.2969
2022-07-01 02:04:49 - train: epoch 0086, iter [03300, 05004], lr: 0.018882, loss: 1.3913
2022-07-01 02:05:22 - train: epoch 0086, iter [03400, 05004], lr: 0.018861, loss: 1.3442
2022-07-01 02:05:54 - train: epoch 0086, iter [03500, 05004], lr: 0.018841, loss: 1.2807
2022-07-01 02:06:27 - train: epoch 0086, iter [03600, 05004], lr: 0.018820, loss: 1.2408
2022-07-01 02:07:00 - train: epoch 0086, iter [03700, 05004], lr: 0.018800, loss: 1.3564
2022-07-01 02:07:33 - train: epoch 0086, iter [03800, 05004], lr: 0.018779, loss: 1.2411
2022-07-01 02:08:06 - train: epoch 0086, iter [03900, 05004], lr: 0.018759, loss: 1.2377
2022-07-01 02:08:39 - train: epoch 0086, iter [04000, 05004], lr: 0.018739, loss: 1.3152
2022-07-01 02:09:12 - train: epoch 0086, iter [04100, 05004], lr: 0.018718, loss: 1.3083
2022-07-01 02:09:45 - train: epoch 0086, iter [04200, 05004], lr: 0.018698, loss: 1.3197
2022-07-01 02:10:17 - train: epoch 0086, iter [04300, 05004], lr: 0.018677, loss: 1.4016
2022-07-01 02:10:50 - train: epoch 0086, iter [04400, 05004], lr: 0.018657, loss: 1.1071
2022-07-01 02:11:23 - train: epoch 0086, iter [04500, 05004], lr: 0.018637, loss: 1.1597
2022-07-01 02:11:56 - train: epoch 0086, iter [04600, 05004], lr: 0.018616, loss: 1.1901
2022-07-01 02:12:29 - train: epoch 0086, iter [04700, 05004], lr: 0.018596, loss: 1.1796
2022-07-01 02:13:02 - train: epoch 0086, iter [04800, 05004], lr: 0.018575, loss: 1.3132
2022-07-01 02:13:35 - train: epoch 0086, iter [04900, 05004], lr: 0.018555, loss: 1.2772
2022-07-01 02:14:08 - train: epoch 0086, iter [05000, 05004], lr: 0.018535, loss: 1.3089
2022-07-01 02:14:09 - train: epoch 086, train_loss: 1.2334
2022-07-01 02:15:25 - eval: epoch: 086, acc1: 71.786%, acc5: 90.988%, test_loss: 1.1267, per_image_load_time: 2.279ms, per_image_inference_time: 0.624ms
2022-07-01 02:15:26 - until epoch: 086, best_acc1: 71.918%
2022-07-01 02:15:26 - epoch 087 lr: 0.018534
2022-07-01 02:16:05 - train: epoch 0087, iter [00100, 05004], lr: 0.018514, loss: 1.2507
2022-07-01 02:16:38 - train: epoch 0087, iter [00200, 05004], lr: 0.018493, loss: 0.9753
2022-07-01 02:17:10 - train: epoch 0087, iter [00300, 05004], lr: 0.018473, loss: 1.4346
2022-07-01 02:17:42 - train: epoch 0087, iter [00400, 05004], lr: 0.018453, loss: 1.2857
2022-07-01 02:18:14 - train: epoch 0087, iter [00500, 05004], lr: 0.018432, loss: 0.9378
2022-07-01 02:18:47 - train: epoch 0087, iter [00600, 05004], lr: 0.018412, loss: 1.1431
2022-07-01 02:19:20 - train: epoch 0087, iter [00700, 05004], lr: 0.018392, loss: 0.9547
2022-07-01 02:19:52 - train: epoch 0087, iter [00800, 05004], lr: 0.018372, loss: 1.2584
2022-07-01 02:20:25 - train: epoch 0087, iter [00900, 05004], lr: 0.018351, loss: 1.2337
2022-07-01 02:20:58 - train: epoch 0087, iter [01000, 05004], lr: 0.018331, loss: 1.1294
2022-07-01 02:21:31 - train: epoch 0087, iter [01100, 05004], lr: 0.018311, loss: 1.2955
2022-07-01 02:22:03 - train: epoch 0087, iter [01200, 05004], lr: 0.018291, loss: 1.2621
2022-07-01 02:22:36 - train: epoch 0087, iter [01300, 05004], lr: 0.018270, loss: 1.5028
2022-07-01 02:23:09 - train: epoch 0087, iter [01400, 05004], lr: 0.018250, loss: 1.2097
2022-07-01 02:23:42 - train: epoch 0087, iter [01500, 05004], lr: 0.018230, loss: 1.1095
2022-07-01 02:24:15 - train: epoch 0087, iter [01600, 05004], lr: 0.018210, loss: 1.1336
2022-07-01 02:24:48 - train: epoch 0087, iter [01700, 05004], lr: 0.018190, loss: 1.2893
2022-07-01 02:25:21 - train: epoch 0087, iter [01800, 05004], lr: 0.018169, loss: 1.2719
2022-07-01 02:25:54 - train: epoch 0087, iter [01900, 05004], lr: 0.018149, loss: 1.2100
2022-07-01 02:26:27 - train: epoch 0087, iter [02000, 05004], lr: 0.018129, loss: 1.1505
2022-07-01 02:27:00 - train: epoch 0087, iter [02100, 05004], lr: 0.018109, loss: 1.3108
2022-07-01 02:27:32 - train: epoch 0087, iter [02200, 05004], lr: 0.018089, loss: 1.2354
2022-07-01 02:28:05 - train: epoch 0087, iter [02300, 05004], lr: 0.018069, loss: 1.3043
2022-07-01 02:28:38 - train: epoch 0087, iter [02400, 05004], lr: 0.018049, loss: 1.1514
2022-07-01 02:29:11 - train: epoch 0087, iter [02500, 05004], lr: 0.018028, loss: 1.1235
2022-07-01 02:29:44 - train: epoch 0087, iter [02600, 05004], lr: 0.018008, loss: 1.1391
2022-07-01 02:30:17 - train: epoch 0087, iter [02700, 05004], lr: 0.017988, loss: 1.1744
2022-07-01 02:30:50 - train: epoch 0087, iter [02800, 05004], lr: 0.017968, loss: 1.0095
2022-07-01 02:31:23 - train: epoch 0087, iter [02900, 05004], lr: 0.017948, loss: 1.0667
2022-07-01 02:31:56 - train: epoch 0087, iter [03000, 05004], lr: 0.017928, loss: 1.3657
2022-07-01 02:32:29 - train: epoch 0087, iter [03100, 05004], lr: 0.017908, loss: 1.2409
2022-07-01 02:33:02 - train: epoch 0087, iter [03200, 05004], lr: 0.017888, loss: 1.2012
2022-07-01 02:33:35 - train: epoch 0087, iter [03300, 05004], lr: 0.017868, loss: 1.2197
2022-07-01 02:34:07 - train: epoch 0087, iter [03400, 05004], lr: 0.017848, loss: 1.2342
2022-07-01 02:34:40 - train: epoch 0087, iter [03500, 05004], lr: 0.017828, loss: 1.0596
2022-07-01 02:35:13 - train: epoch 0087, iter [03600, 05004], lr: 0.017808, loss: 0.9881
2022-07-01 02:35:46 - train: epoch 0087, iter [03700, 05004], lr: 0.017788, loss: 1.1051
2022-07-01 02:36:19 - train: epoch 0087, iter [03800, 05004], lr: 0.017768, loss: 1.3051
2022-07-01 02:36:52 - train: epoch 0087, iter [03900, 05004], lr: 0.017748, loss: 1.1564
2022-07-01 02:37:24 - train: epoch 0087, iter [04000, 05004], lr: 0.017728, loss: 1.2851
2022-07-01 02:37:57 - train: epoch 0087, iter [04100, 05004], lr: 0.017708, loss: 1.2922
2022-07-01 02:38:30 - train: epoch 0087, iter [04200, 05004], lr: 0.017688, loss: 1.3417
2022-07-01 02:39:03 - train: epoch 0087, iter [04300, 05004], lr: 0.017668, loss: 1.2659
2022-07-01 02:39:36 - train: epoch 0087, iter [04400, 05004], lr: 0.017648, loss: 1.2973
2022-07-01 02:40:09 - train: epoch 0087, iter [04500, 05004], lr: 0.017628, loss: 1.2924
2022-07-01 02:40:42 - train: epoch 0087, iter [04600, 05004], lr: 0.017608, loss: 1.2711
2022-07-01 02:41:14 - train: epoch 0087, iter [04700, 05004], lr: 0.017588, loss: 1.2850
2022-07-01 02:41:48 - train: epoch 0087, iter [04800, 05004], lr: 0.017568, loss: 1.1239
2022-07-01 02:42:21 - train: epoch 0087, iter [04900, 05004], lr: 0.017548, loss: 1.2245
2022-07-01 02:42:54 - train: epoch 0087, iter [05000, 05004], lr: 0.017528, loss: 1.2917
2022-07-01 02:42:55 - train: epoch 087, train_loss: 1.2161
2022-07-01 02:44:12 - eval: epoch: 087, acc1: 72.628%, acc5: 91.112%, test_loss: 1.1106, per_image_load_time: 1.230ms, per_image_inference_time: 0.648ms
2022-07-01 02:44:13 - until epoch: 087, best_acc1: 72.628%
2022-07-01 02:44:13 - epoch 088 lr: 0.017527
2022-07-01 02:44:52 - train: epoch 0088, iter [00100, 05004], lr: 0.017508, loss: 1.1103
2022-07-01 02:45:25 - train: epoch 0088, iter [00200, 05004], lr: 0.017488, loss: 1.1180
2022-07-01 02:45:56 - train: epoch 0088, iter [00300, 05004], lr: 0.017468, loss: 1.2304
2022-07-01 02:46:29 - train: epoch 0088, iter [00400, 05004], lr: 0.017448, loss: 1.1352
2022-07-01 02:47:01 - train: epoch 0088, iter [00500, 05004], lr: 0.017428, loss: 1.0749
2022-07-01 02:47:34 - train: epoch 0088, iter [00600, 05004], lr: 0.017408, loss: 1.1779
2022-07-01 02:48:07 - train: epoch 0088, iter [00700, 05004], lr: 0.017389, loss: 1.0709
2022-07-01 02:48:39 - train: epoch 0088, iter [00800, 05004], lr: 0.017369, loss: 1.1141
2022-07-01 02:49:12 - train: epoch 0088, iter [00900, 05004], lr: 0.017349, loss: 1.2558
2022-07-01 02:49:46 - train: epoch 0088, iter [01000, 05004], lr: 0.017329, loss: 1.1472
2022-07-01 02:50:18 - train: epoch 0088, iter [01100, 05004], lr: 0.017309, loss: 1.2844
2022-07-01 02:50:51 - train: epoch 0088, iter [01200, 05004], lr: 0.017290, loss: 1.1442
2022-07-01 02:51:24 - train: epoch 0088, iter [01300, 05004], lr: 0.017270, loss: 1.2551
2022-07-01 02:51:56 - train: epoch 0088, iter [01400, 05004], lr: 0.017250, loss: 1.0770
2022-07-01 02:52:29 - train: epoch 0088, iter [01500, 05004], lr: 0.017230, loss: 1.1703
2022-07-01 02:53:02 - train: epoch 0088, iter [01600, 05004], lr: 0.017210, loss: 0.9899
2022-07-01 02:53:35 - train: epoch 0088, iter [01700, 05004], lr: 0.017191, loss: 1.2066
2022-07-01 02:54:08 - train: epoch 0088, iter [01800, 05004], lr: 0.017171, loss: 1.1415
2022-07-01 02:54:41 - train: epoch 0088, iter [01900, 05004], lr: 0.017151, loss: 1.2405
2022-07-01 02:55:14 - train: epoch 0088, iter [02000, 05004], lr: 0.017132, loss: 1.1435
2022-07-01 02:55:47 - train: epoch 0088, iter [02100, 05004], lr: 0.017112, loss: 1.1839
2022-07-01 02:56:20 - train: epoch 0088, iter [02200, 05004], lr: 0.017092, loss: 1.1370
2022-07-01 02:56:53 - train: epoch 0088, iter [02300, 05004], lr: 0.017072, loss: 0.9668
2022-07-01 02:57:26 - train: epoch 0088, iter [02400, 05004], lr: 0.017053, loss: 1.2324
2022-07-01 02:57:59 - train: epoch 0088, iter [02500, 05004], lr: 0.017033, loss: 1.2638
2022-07-01 02:58:32 - train: epoch 0088, iter [02600, 05004], lr: 0.017013, loss: 1.2660
2022-07-01 02:59:05 - train: epoch 0088, iter [02700, 05004], lr: 0.016994, loss: 1.2351
2022-07-01 02:59:38 - train: epoch 0088, iter [02800, 05004], lr: 0.016974, loss: 1.1884
2022-07-01 03:00:11 - train: epoch 0088, iter [02900, 05004], lr: 0.016955, loss: 1.1952
2022-07-01 03:00:44 - train: epoch 0088, iter [03000, 05004], lr: 0.016935, loss: 1.2681
2022-07-01 03:01:17 - train: epoch 0088, iter [03100, 05004], lr: 0.016915, loss: 1.0826
2022-07-01 03:01:49 - train: epoch 0088, iter [03200, 05004], lr: 0.016896, loss: 1.1040
2022-07-01 03:02:22 - train: epoch 0088, iter [03300, 05004], lr: 0.016876, loss: 0.9952
2022-07-01 03:02:55 - train: epoch 0088, iter [03400, 05004], lr: 0.016856, loss: 1.0937
2022-07-01 03:03:28 - train: epoch 0088, iter [03500, 05004], lr: 0.016837, loss: 1.0964
2022-07-01 03:04:01 - train: epoch 0088, iter [03600, 05004], lr: 0.016817, loss: 1.2651
2022-07-01 03:04:34 - train: epoch 0088, iter [03700, 05004], lr: 0.016798, loss: 1.2712
2022-07-01 03:05:07 - train: epoch 0088, iter [03800, 05004], lr: 0.016778, loss: 1.2025
2022-07-01 03:05:40 - train: epoch 0088, iter [03900, 05004], lr: 0.016759, loss: 1.4241
2022-07-01 03:06:13 - train: epoch 0088, iter [04000, 05004], lr: 0.016739, loss: 1.0802
2022-07-01 03:06:46 - train: epoch 0088, iter [04100, 05004], lr: 0.016720, loss: 1.3046
2022-07-01 03:07:20 - train: epoch 0088, iter [04200, 05004], lr: 0.016700, loss: 1.4129
2022-07-01 03:07:52 - train: epoch 0088, iter [04300, 05004], lr: 0.016681, loss: 1.1357
2022-07-01 03:08:25 - train: epoch 0088, iter [04400, 05004], lr: 0.016661, loss: 1.0933
2022-07-01 03:08:58 - train: epoch 0088, iter [04500, 05004], lr: 0.016642, loss: 1.1603
2022-07-01 03:09:31 - train: epoch 0088, iter [04600, 05004], lr: 0.016622, loss: 1.3695
2022-07-01 03:10:04 - train: epoch 0088, iter [04700, 05004], lr: 0.016603, loss: 1.2307
2022-07-01 03:10:37 - train: epoch 0088, iter [04800, 05004], lr: 0.016583, loss: 1.1451
2022-07-01 03:11:10 - train: epoch 0088, iter [04900, 05004], lr: 0.016564, loss: 1.0894
2022-07-01 03:11:43 - train: epoch 0088, iter [05000, 05004], lr: 0.016544, loss: 1.1731
2022-07-01 03:11:44 - train: epoch 088, train_loss: 1.2003
2022-07-01 03:13:00 - eval: epoch: 088, acc1: 72.832%, acc5: 91.192%, test_loss: 1.0988, per_image_load_time: 2.310ms, per_image_inference_time: 0.654ms
2022-07-01 03:13:01 - until epoch: 088, best_acc1: 72.832%
2022-07-01 03:13:01 - epoch 089 lr: 0.016543
2022-07-01 03:13:41 - train: epoch 0089, iter [00100, 05004], lr: 0.016524, loss: 1.3330
2022-07-01 03:14:13 - train: epoch 0089, iter [00200, 05004], lr: 0.016505, loss: 1.0174
2022-07-01 03:14:45 - train: epoch 0089, iter [00300, 05004], lr: 0.016485, loss: 1.2564
2022-07-01 03:15:18 - train: epoch 0089, iter [00400, 05004], lr: 0.016466, loss: 1.0379
2022-07-01 03:15:50 - train: epoch 0089, iter [00500, 05004], lr: 0.016446, loss: 1.1912
2022-07-01 03:16:23 - train: epoch 0089, iter [00600, 05004], lr: 0.016427, loss: 1.0968
2022-07-01 03:16:56 - train: epoch 0089, iter [00700, 05004], lr: 0.016408, loss: 1.2663
2022-07-01 03:17:28 - train: epoch 0089, iter [00800, 05004], lr: 0.016388, loss: 1.3587
2022-07-01 03:18:01 - train: epoch 0089, iter [00900, 05004], lr: 0.016369, loss: 1.1384
2022-07-01 03:18:34 - train: epoch 0089, iter [01000, 05004], lr: 0.016350, loss: 1.3318
2022-07-01 03:19:06 - train: epoch 0089, iter [01100, 05004], lr: 0.016330, loss: 1.1127
2022-07-01 03:19:39 - train: epoch 0089, iter [01200, 05004], lr: 0.016311, loss: 1.2379
2022-07-01 03:20:12 - train: epoch 0089, iter [01300, 05004], lr: 0.016292, loss: 1.2110
2022-07-01 03:20:45 - train: epoch 0089, iter [01400, 05004], lr: 0.016272, loss: 1.2421
2022-07-01 03:21:18 - train: epoch 0089, iter [01500, 05004], lr: 0.016253, loss: 1.1865
2022-07-01 03:21:51 - train: epoch 0089, iter [01600, 05004], lr: 0.016234, loss: 1.1449
2022-07-01 03:22:24 - train: epoch 0089, iter [01700, 05004], lr: 0.016214, loss: 1.2106
2022-07-01 03:22:57 - train: epoch 0089, iter [01800, 05004], lr: 0.016195, loss: 1.1170
2022-07-01 03:23:31 - train: epoch 0089, iter [01900, 05004], lr: 0.016176, loss: 1.1450
2022-07-01 03:24:04 - train: epoch 0089, iter [02000, 05004], lr: 0.016157, loss: 1.1184
2022-07-01 03:24:37 - train: epoch 0089, iter [02100, 05004], lr: 0.016137, loss: 1.2609
2022-07-01 03:25:09 - train: epoch 0089, iter [02200, 05004], lr: 0.016118, loss: 1.1804
2022-07-01 03:25:42 - train: epoch 0089, iter [02300, 05004], lr: 0.016099, loss: 1.0298
2022-07-01 03:26:14 - train: epoch 0089, iter [02400, 05004], lr: 0.016080, loss: 1.2325
2022-07-01 03:26:47 - train: epoch 0089, iter [02500, 05004], lr: 0.016060, loss: 1.2317
2022-07-01 03:27:20 - train: epoch 0089, iter [02600, 05004], lr: 0.016041, loss: 1.2261
2022-07-01 03:27:52 - train: epoch 0089, iter [02700, 05004], lr: 0.016022, loss: 1.1661
2022-07-01 03:28:25 - train: epoch 0089, iter [02800, 05004], lr: 0.016003, loss: 1.1785
2022-07-01 03:28:58 - train: epoch 0089, iter [02900, 05004], lr: 0.015984, loss: 1.1088
2022-07-01 03:29:31 - train: epoch 0089, iter [03000, 05004], lr: 0.015964, loss: 1.2387
2022-07-01 03:30:04 - train: epoch 0089, iter [03100, 05004], lr: 0.015945, loss: 1.2610
2022-07-01 03:30:37 - train: epoch 0089, iter [03200, 05004], lr: 0.015926, loss: 1.3399
2022-07-01 03:31:09 - train: epoch 0089, iter [03300, 05004], lr: 0.015907, loss: 1.4247
2022-07-01 03:31:42 - train: epoch 0089, iter [03400, 05004], lr: 0.015888, loss: 1.1875
2022-07-01 03:32:15 - train: epoch 0089, iter [03500, 05004], lr: 0.015869, loss: 0.9102
2022-07-01 03:32:47 - train: epoch 0089, iter [03600, 05004], lr: 0.015850, loss: 1.2036
2022-07-01 03:33:20 - train: epoch 0089, iter [03700, 05004], lr: 0.015831, loss: 1.3339
2022-07-01 03:33:53 - train: epoch 0089, iter [03800, 05004], lr: 0.015811, loss: 1.1665
2022-07-01 03:34:26 - train: epoch 0089, iter [03900, 05004], lr: 0.015792, loss: 1.3830
2022-07-01 03:34:59 - train: epoch 0089, iter [04000, 05004], lr: 0.015773, loss: 0.9352
2022-07-01 03:35:32 - train: epoch 0089, iter [04100, 05004], lr: 0.015754, loss: 1.3181
2022-07-01 03:36:05 - train: epoch 0089, iter [04200, 05004], lr: 0.015735, loss: 1.2508
2022-07-01 03:36:38 - train: epoch 0089, iter [04300, 05004], lr: 0.015716, loss: 1.2358
2022-07-01 03:37:11 - train: epoch 0089, iter [04400, 05004], lr: 0.015697, loss: 1.2410
2022-07-01 03:37:43 - train: epoch 0089, iter [04500, 05004], lr: 0.015678, loss: 0.9858
2022-07-01 03:38:16 - train: epoch 0089, iter [04600, 05004], lr: 0.015659, loss: 1.1037
2022-07-01 03:38:49 - train: epoch 0089, iter [04700, 05004], lr: 0.015640, loss: 1.2523
2022-07-01 03:39:22 - train: epoch 0089, iter [04800, 05004], lr: 0.015621, loss: 1.2039
2022-07-01 03:39:54 - train: epoch 0089, iter [04900, 05004], lr: 0.015602, loss: 1.1645
2022-07-01 03:40:27 - train: epoch 0089, iter [05000, 05004], lr: 0.015583, loss: 0.9676
2022-07-01 03:40:29 - train: epoch 089, train_loss: 1.1833
2022-07-01 03:41:44 - eval: epoch: 089, acc1: 72.912%, acc5: 91.386%, test_loss: 1.0860, per_image_load_time: 1.661ms, per_image_inference_time: 0.644ms
2022-07-01 03:41:45 - until epoch: 089, best_acc1: 72.912%
2022-07-01 03:41:45 - epoch 090 lr: 0.015582
2022-07-01 03:42:24 - train: epoch 0090, iter [00100, 05004], lr: 0.015563, loss: 1.1701
2022-07-01 03:42:57 - train: epoch 0090, iter [00200, 05004], lr: 0.015544, loss: 1.2510
2022-07-01 03:43:29 - train: epoch 0090, iter [00300, 05004], lr: 0.015525, loss: 1.0687
2022-07-01 03:44:02 - train: epoch 0090, iter [00400, 05004], lr: 0.015506, loss: 1.0771
2022-07-01 03:44:34 - train: epoch 0090, iter [00500, 05004], lr: 0.015488, loss: 1.2094
2022-07-01 03:45:07 - train: epoch 0090, iter [00600, 05004], lr: 0.015469, loss: 1.1588
2022-07-01 03:45:40 - train: epoch 0090, iter [00700, 05004], lr: 0.015450, loss: 1.1443
2022-07-01 03:46:12 - train: epoch 0090, iter [00800, 05004], lr: 0.015431, loss: 1.1192
2022-07-01 03:46:45 - train: epoch 0090, iter [00900, 05004], lr: 0.015412, loss: 1.0850
2022-07-01 03:47:17 - train: epoch 0090, iter [01000, 05004], lr: 0.015393, loss: 1.0155
2022-07-01 03:47:50 - train: epoch 0090, iter [01100, 05004], lr: 0.015374, loss: 1.2756
2022-07-01 03:48:22 - train: epoch 0090, iter [01200, 05004], lr: 0.015355, loss: 1.0985
2022-07-01 03:48:55 - train: epoch 0090, iter [01300, 05004], lr: 0.015336, loss: 1.2853
2022-07-01 03:49:27 - train: epoch 0090, iter [01400, 05004], lr: 0.015318, loss: 1.0140
2022-07-01 03:50:00 - train: epoch 0090, iter [01500, 05004], lr: 0.015299, loss: 1.4146
2022-07-01 03:50:33 - train: epoch 0090, iter [01600, 05004], lr: 0.015280, loss: 1.1634
2022-07-01 03:51:06 - train: epoch 0090, iter [01700, 05004], lr: 0.015261, loss: 0.9321
2022-07-01 03:51:39 - train: epoch 0090, iter [01800, 05004], lr: 0.015242, loss: 1.0464
2022-07-01 03:52:12 - train: epoch 0090, iter [01900, 05004], lr: 0.015223, loss: 0.9678
2022-07-01 03:52:45 - train: epoch 0090, iter [02000, 05004], lr: 0.015205, loss: 1.3388
2022-07-01 03:53:18 - train: epoch 0090, iter [02100, 05004], lr: 0.015186, loss: 1.3507
2022-07-01 03:53:51 - train: epoch 0090, iter [02200, 05004], lr: 0.015167, loss: 1.1794
2022-07-01 03:54:24 - train: epoch 0090, iter [02300, 05004], lr: 0.015148, loss: 1.3232
2022-07-01 03:54:56 - train: epoch 0090, iter [02400, 05004], lr: 0.015130, loss: 1.1993
2022-07-01 03:55:29 - train: epoch 0090, iter [02500, 05004], lr: 0.015111, loss: 1.2500
2022-07-01 03:56:02 - train: epoch 0090, iter [02600, 05004], lr: 0.015092, loss: 1.2211
2022-07-01 03:56:35 - train: epoch 0090, iter [02700, 05004], lr: 0.015073, loss: 1.1726
2022-07-01 03:57:08 - train: epoch 0090, iter [02800, 05004], lr: 0.015055, loss: 1.1503
2022-07-01 03:57:41 - train: epoch 0090, iter [02900, 05004], lr: 0.015036, loss: 1.1600
2022-07-01 03:58:13 - train: epoch 0090, iter [03000, 05004], lr: 0.015017, loss: 1.1553
2022-07-01 03:58:46 - train: epoch 0090, iter [03100, 05004], lr: 0.014999, loss: 1.0799
2022-07-01 03:59:19 - train: epoch 0090, iter [03200, 05004], lr: 0.014980, loss: 1.0372
2022-07-01 03:59:52 - train: epoch 0090, iter [03300, 05004], lr: 0.014961, loss: 1.2656
2022-07-01 04:00:24 - train: epoch 0090, iter [03400, 05004], lr: 0.014943, loss: 1.0308
2022-07-01 04:00:57 - train: epoch 0090, iter [03500, 05004], lr: 0.014924, loss: 1.0883
2022-07-01 04:01:30 - train: epoch 0090, iter [03600, 05004], lr: 0.014905, loss: 1.0800
2022-07-01 04:02:03 - train: epoch 0090, iter [03700, 05004], lr: 0.014887, loss: 1.1188
2022-07-01 04:02:35 - train: epoch 0090, iter [03800, 05004], lr: 0.014868, loss: 1.2322
2022-07-01 04:03:08 - train: epoch 0090, iter [03900, 05004], lr: 0.014849, loss: 1.0093
2022-07-01 04:03:41 - train: epoch 0090, iter [04000, 05004], lr: 0.014831, loss: 1.1690
2022-07-01 04:04:14 - train: epoch 0090, iter [04100, 05004], lr: 0.014812, loss: 1.4607
2022-07-01 04:04:47 - train: epoch 0090, iter [04200, 05004], lr: 0.014794, loss: 1.2733
2022-07-01 04:05:20 - train: epoch 0090, iter [04300, 05004], lr: 0.014775, loss: 1.2192
2022-07-01 04:05:53 - train: epoch 0090, iter [04400, 05004], lr: 0.014757, loss: 1.0631
2022-07-01 04:06:26 - train: epoch 0090, iter [04500, 05004], lr: 0.014738, loss: 1.0732
2022-07-01 04:06:59 - train: epoch 0090, iter [04600, 05004], lr: 0.014719, loss: 1.1867
2022-07-01 04:07:32 - train: epoch 0090, iter [04700, 05004], lr: 0.014701, loss: 1.1507
2022-07-01 04:08:04 - train: epoch 0090, iter [04800, 05004], lr: 0.014682, loss: 1.2856
2022-07-01 04:08:37 - train: epoch 0090, iter [04900, 05004], lr: 0.014664, loss: 1.0638
2022-07-01 04:09:10 - train: epoch 0090, iter [05000, 05004], lr: 0.014645, loss: 1.1468
2022-07-01 04:09:11 - train: epoch 090, train_loss: 1.1655
2022-07-01 04:10:27 - eval: epoch: 090, acc1: 72.768%, acc5: 91.310%, test_loss: 1.0945, per_image_load_time: 1.205ms, per_image_inference_time: 0.641ms
2022-07-01 04:10:27 - until epoch: 090, best_acc1: 72.912%
2022-07-01 04:10:27 - epoch 091 lr: 0.014644
2022-07-01 04:11:08 - train: epoch 0091, iter [00100, 05004], lr: 0.014626, loss: 1.0060
2022-07-01 04:11:40 - train: epoch 0091, iter [00200, 05004], lr: 0.014608, loss: 1.1169
2022-07-01 04:12:12 - train: epoch 0091, iter [00300, 05004], lr: 0.014589, loss: 1.1471
2022-07-01 04:12:45 - train: epoch 0091, iter [00400, 05004], lr: 0.014571, loss: 0.9789
2022-07-01 04:13:17 - train: epoch 0091, iter [00500, 05004], lr: 0.014552, loss: 1.0944
2022-07-01 04:13:50 - train: epoch 0091, iter [00600, 05004], lr: 0.014534, loss: 1.0925
2022-07-01 04:14:23 - train: epoch 0091, iter [00700, 05004], lr: 0.014515, loss: 1.1665
2022-07-01 04:14:55 - train: epoch 0091, iter [00800, 05004], lr: 0.014497, loss: 1.0033
2022-07-01 04:15:28 - train: epoch 0091, iter [00900, 05004], lr: 0.014479, loss: 1.2025
2022-07-01 04:16:01 - train: epoch 0091, iter [01000, 05004], lr: 0.014460, loss: 1.0413
2022-07-01 04:16:33 - train: epoch 0091, iter [01100, 05004], lr: 0.014442, loss: 0.9735
2022-07-01 04:17:06 - train: epoch 0091, iter [01200, 05004], lr: 0.014423, loss: 1.2223
2022-07-01 04:17:39 - train: epoch 0091, iter [01300, 05004], lr: 0.014405, loss: 1.0512
2022-07-01 04:18:12 - train: epoch 0091, iter [01400, 05004], lr: 0.014387, loss: 1.1433
2022-07-01 04:18:45 - train: epoch 0091, iter [01500, 05004], lr: 0.014368, loss: 1.1035
2022-07-01 04:19:18 - train: epoch 0091, iter [01600, 05004], lr: 0.014350, loss: 1.2561
2022-07-01 04:19:51 - train: epoch 0091, iter [01700, 05004], lr: 0.014332, loss: 1.1638
2022-07-01 04:20:24 - train: epoch 0091, iter [01800, 05004], lr: 0.014313, loss: 1.1346
2022-07-01 04:20:57 - train: epoch 0091, iter [01900, 05004], lr: 0.014295, loss: 1.0412
2022-07-01 04:21:30 - train: epoch 0091, iter [02000, 05004], lr: 0.014277, loss: 1.1682
2022-07-01 04:22:03 - train: epoch 0091, iter [02100, 05004], lr: 0.014258, loss: 0.9765
2022-07-01 04:22:35 - train: epoch 0091, iter [02200, 05004], lr: 0.014240, loss: 1.2733
2022-07-01 04:23:08 - train: epoch 0091, iter [02300, 05004], lr: 0.014222, loss: 1.1553
2022-07-01 04:23:41 - train: epoch 0091, iter [02400, 05004], lr: 0.014204, loss: 1.0821
2022-07-01 04:24:14 - train: epoch 0091, iter [02500, 05004], lr: 0.014185, loss: 1.1557
2022-07-01 04:24:46 - train: epoch 0091, iter [02600, 05004], lr: 0.014167, loss: 1.2102
2022-07-01 04:25:19 - train: epoch 0091, iter [02700, 05004], lr: 0.014149, loss: 0.9673
2022-07-01 04:25:52 - train: epoch 0091, iter [02800, 05004], lr: 0.014131, loss: 1.1822
2022-07-01 04:26:25 - train: epoch 0091, iter [02900, 05004], lr: 0.014112, loss: 1.2405
2022-07-01 04:26:57 - train: epoch 0091, iter [03000, 05004], lr: 0.014094, loss: 1.3049
2022-07-01 04:27:30 - train: epoch 0091, iter [03100, 05004], lr: 0.014076, loss: 1.1491
2022-07-01 04:28:02 - train: epoch 0091, iter [03200, 05004], lr: 0.014058, loss: 1.2869
2022-07-01 04:28:35 - train: epoch 0091, iter [03300, 05004], lr: 0.014040, loss: 1.0600
2022-07-01 04:29:08 - train: epoch 0091, iter [03400, 05004], lr: 0.014021, loss: 1.1274
2022-07-01 04:29:40 - train: epoch 0091, iter [03500, 05004], lr: 0.014003, loss: 1.1235
2022-07-01 04:30:13 - train: epoch 0091, iter [03600, 05004], lr: 0.013985, loss: 1.0711
2022-07-01 04:30:46 - train: epoch 0091, iter [03700, 05004], lr: 0.013967, loss: 1.2159
2022-07-01 04:31:19 - train: epoch 0091, iter [03800, 05004], lr: 0.013949, loss: 1.1742
2022-07-01 04:31:52 - train: epoch 0091, iter [03900, 05004], lr: 0.013931, loss: 1.1019
2022-07-01 04:32:25 - train: epoch 0091, iter [04000, 05004], lr: 0.013913, loss: 1.2344
2022-07-01 04:32:58 - train: epoch 0091, iter [04100, 05004], lr: 0.013894, loss: 1.1688
2022-07-01 04:33:31 - train: epoch 0091, iter [04200, 05004], lr: 0.013876, loss: 1.2154
2022-07-01 04:34:04 - train: epoch 0091, iter [04300, 05004], lr: 0.013858, loss: 1.1971
2022-07-01 04:34:36 - train: epoch 0091, iter [04400, 05004], lr: 0.013840, loss: 1.0338
2022-07-01 04:35:09 - train: epoch 0091, iter [04500, 05004], lr: 0.013822, loss: 1.0344
2022-07-01 04:35:42 - train: epoch 0091, iter [04600, 05004], lr: 0.013804, loss: 1.1252
2022-07-01 04:36:15 - train: epoch 0091, iter [04700, 05004], lr: 0.013786, loss: 1.2114
2022-07-01 04:36:47 - train: epoch 0091, iter [04800, 05004], lr: 0.013768, loss: 1.0962
2022-07-01 04:37:20 - train: epoch 0091, iter [04900, 05004], lr: 0.013750, loss: 1.1007
2022-07-01 04:37:53 - train: epoch 0091, iter [05000, 05004], lr: 0.013732, loss: 1.3052
2022-07-01 04:37:55 - train: epoch 091, train_loss: 1.1469
2022-07-01 04:39:11 - eval: epoch: 091, acc1: 73.368%, acc5: 91.484%, test_loss: 1.0760, per_image_load_time: 1.924ms, per_image_inference_time: 0.644ms
2022-07-01 04:39:12 - until epoch: 091, best_acc1: 73.368%
2022-07-01 04:39:12 - epoch 092 lr: 0.013731
2022-07-01 04:39:51 - train: epoch 0092, iter [00100, 05004], lr: 0.013713, loss: 1.0725
2022-07-01 04:40:24 - train: epoch 0092, iter [00200, 05004], lr: 0.013695, loss: 1.1859
2022-07-01 04:40:56 - train: epoch 0092, iter [00300, 05004], lr: 0.013677, loss: 1.1290
2022-07-01 04:41:29 - train: epoch 0092, iter [00400, 05004], lr: 0.013659, loss: 0.9818
2022-07-01 04:42:01 - train: epoch 0092, iter [00500, 05004], lr: 0.013641, loss: 1.1656
2022-07-01 04:42:34 - train: epoch 0092, iter [00600, 05004], lr: 0.013623, loss: 1.2011
2022-07-01 04:43:06 - train: epoch 0092, iter [00700, 05004], lr: 0.013605, loss: 1.1414
2022-07-01 04:43:39 - train: epoch 0092, iter [00800, 05004], lr: 0.013588, loss: 1.2482
2022-07-01 04:44:12 - train: epoch 0092, iter [00900, 05004], lr: 0.013570, loss: 1.1107
2022-07-01 04:44:44 - train: epoch 0092, iter [01000, 05004], lr: 0.013552, loss: 1.3731
2022-07-01 04:45:17 - train: epoch 0092, iter [01100, 05004], lr: 0.013534, loss: 1.0297
2022-07-01 04:45:49 - train: epoch 0092, iter [01200, 05004], lr: 0.013516, loss: 1.0681
2022-07-01 04:46:22 - train: epoch 0092, iter [01300, 05004], lr: 0.013498, loss: 1.0878
2022-07-01 04:46:54 - train: epoch 0092, iter [01400, 05004], lr: 0.013480, loss: 1.0905
2022-07-01 04:47:27 - train: epoch 0092, iter [01500, 05004], lr: 0.013462, loss: 1.0911
2022-07-01 04:48:00 - train: epoch 0092, iter [01600, 05004], lr: 0.013444, loss: 1.1509
2022-07-01 04:48:33 - train: epoch 0092, iter [01700, 05004], lr: 0.013427, loss: 1.1687
2022-07-01 04:49:05 - train: epoch 0092, iter [01800, 05004], lr: 0.013409, loss: 1.0618
2022-07-01 04:49:38 - train: epoch 0092, iter [01900, 05004], lr: 0.013391, loss: 1.0693
2022-07-01 04:50:11 - train: epoch 0092, iter [02000, 05004], lr: 0.013373, loss: 1.0447
2022-07-01 04:50:44 - train: epoch 0092, iter [02100, 05004], lr: 0.013355, loss: 1.0748
2022-07-01 04:51:17 - train: epoch 0092, iter [02200, 05004], lr: 0.013338, loss: 1.1749
2022-07-01 04:51:51 - train: epoch 0092, iter [02300, 05004], lr: 0.013320, loss: 1.1285
2022-07-01 04:52:24 - train: epoch 0092, iter [02400, 05004], lr: 0.013302, loss: 1.0338
2022-07-01 04:52:57 - train: epoch 0092, iter [02500, 05004], lr: 0.013284, loss: 1.1992
2022-07-01 04:53:30 - train: epoch 0092, iter [02600, 05004], lr: 0.013266, loss: 1.1997
2022-07-01 04:54:03 - train: epoch 0092, iter [02700, 05004], lr: 0.013249, loss: 1.1116
2022-07-01 04:54:36 - train: epoch 0092, iter [02800, 05004], lr: 0.013231, loss: 1.0205
2022-07-01 04:55:09 - train: epoch 0092, iter [02900, 05004], lr: 0.013213, loss: 1.2014
2022-07-01 04:55:42 - train: epoch 0092, iter [03000, 05004], lr: 0.013196, loss: 0.9935
2022-07-01 04:56:15 - train: epoch 0092, iter [03100, 05004], lr: 0.013178, loss: 1.0716
2022-07-01 04:56:48 - train: epoch 0092, iter [03200, 05004], lr: 0.013160, loss: 1.0300
2022-07-01 04:57:21 - train: epoch 0092, iter [03300, 05004], lr: 0.013142, loss: 1.1503
2022-07-01 04:57:54 - train: epoch 0092, iter [03400, 05004], lr: 0.013125, loss: 1.3553
2022-07-01 04:58:27 - train: epoch 0092, iter [03500, 05004], lr: 0.013107, loss: 0.9847
2022-07-01 04:58:59 - train: epoch 0092, iter [03600, 05004], lr: 0.013090, loss: 1.1596
2022-07-01 04:59:32 - train: epoch 0092, iter [03700, 05004], lr: 0.013072, loss: 0.9846
2022-07-01 05:00:04 - train: epoch 0092, iter [03800, 05004], lr: 0.013054, loss: 1.3615
2022-07-01 05:00:37 - train: epoch 0092, iter [03900, 05004], lr: 0.013037, loss: 1.2185
2022-07-01 05:01:10 - train: epoch 0092, iter [04000, 05004], lr: 0.013019, loss: 1.3271
2022-07-01 05:01:42 - train: epoch 0092, iter [04100, 05004], lr: 0.013001, loss: 1.0441
2022-07-01 05:02:15 - train: epoch 0092, iter [04200, 05004], lr: 0.012984, loss: 1.2263
2022-07-01 05:02:47 - train: epoch 0092, iter [04300, 05004], lr: 0.012966, loss: 1.1218
2022-07-01 05:03:20 - train: epoch 0092, iter [04400, 05004], lr: 0.012949, loss: 0.9926
2022-07-01 05:03:53 - train: epoch 0092, iter [04500, 05004], lr: 0.012931, loss: 1.0971
2022-07-01 05:04:25 - train: epoch 0092, iter [04600, 05004], lr: 0.012914, loss: 1.0940
2022-07-01 05:04:58 - train: epoch 0092, iter [04700, 05004], lr: 0.012896, loss: 0.8600
2022-07-01 05:05:30 - train: epoch 0092, iter [04800, 05004], lr: 0.012878, loss: 1.0161
2022-07-01 05:06:03 - train: epoch 0092, iter [04900, 05004], lr: 0.012861, loss: 1.1049
2022-07-01 05:06:36 - train: epoch 0092, iter [05000, 05004], lr: 0.012843, loss: 1.2790
2022-07-01 05:06:38 - train: epoch 092, train_loss: 1.1272
2022-07-01 05:07:55 - eval: epoch: 092, acc1: 73.362%, acc5: 91.766%, test_loss: 1.0716, per_image_load_time: 2.203ms, per_image_inference_time: 0.650ms
2022-07-01 05:07:55 - until epoch: 092, best_acc1: 73.368%
2022-07-01 05:07:55 - epoch 093 lr: 0.012843
2022-07-01 05:08:36 - train: epoch 0093, iter [00100, 05004], lr: 0.012825, loss: 0.9447
2022-07-01 05:09:08 - train: epoch 0093, iter [00200, 05004], lr: 0.012808, loss: 0.9747
2022-07-01 05:09:40 - train: epoch 0093, iter [00300, 05004], lr: 0.012790, loss: 1.1135
2022-07-01 05:10:12 - train: epoch 0093, iter [00400, 05004], lr: 0.012773, loss: 1.0278
2022-07-01 05:10:45 - train: epoch 0093, iter [00500, 05004], lr: 0.012755, loss: 1.2212
2022-07-01 05:11:17 - train: epoch 0093, iter [00600, 05004], lr: 0.012738, loss: 1.0879
2022-07-01 05:11:50 - train: epoch 0093, iter [00700, 05004], lr: 0.012720, loss: 1.0868
2022-07-01 05:12:23 - train: epoch 0093, iter [00800, 05004], lr: 0.012703, loss: 1.0038
2022-07-01 05:12:55 - train: epoch 0093, iter [00900, 05004], lr: 0.012686, loss: 1.0528
2022-07-01 05:13:28 - train: epoch 0093, iter [01000, 05004], lr: 0.012668, loss: 1.0022
2022-07-01 05:14:00 - train: epoch 0093, iter [01100, 05004], lr: 0.012651, loss: 0.9814
2022-07-01 05:14:33 - train: epoch 0093, iter [01200, 05004], lr: 0.012633, loss: 0.9243
2022-07-01 05:15:06 - train: epoch 0093, iter [01300, 05004], lr: 0.012616, loss: 1.0859
2022-07-01 05:15:38 - train: epoch 0093, iter [01400, 05004], lr: 0.012599, loss: 1.1006
2022-07-01 05:16:11 - train: epoch 0093, iter [01500, 05004], lr: 0.012581, loss: 1.2921
2022-07-01 05:16:44 - train: epoch 0093, iter [01600, 05004], lr: 0.012564, loss: 1.1617
2022-07-01 05:17:17 - train: epoch 0093, iter [01700, 05004], lr: 0.012547, loss: 1.0815
2022-07-01 05:17:50 - train: epoch 0093, iter [01800, 05004], lr: 0.012529, loss: 1.1393
2022-07-01 05:18:22 - train: epoch 0093, iter [01900, 05004], lr: 0.012512, loss: 1.0498
2022-07-01 05:18:55 - train: epoch 0093, iter [02000, 05004], lr: 0.012495, loss: 0.9944
2022-07-01 05:19:28 - train: epoch 0093, iter [02100, 05004], lr: 0.012477, loss: 1.0219
2022-07-01 05:20:01 - train: epoch 0093, iter [02200, 05004], lr: 0.012460, loss: 1.3171
2022-07-01 05:20:34 - train: epoch 0093, iter [02300, 05004], lr: 0.012443, loss: 1.0790
2022-07-01 05:21:07 - train: epoch 0093, iter [02400, 05004], lr: 0.012426, loss: 1.0166
2022-07-01 05:21:40 - train: epoch 0093, iter [02500, 05004], lr: 0.012408, loss: 1.0941
2022-07-01 05:22:13 - train: epoch 0093, iter [02600, 05004], lr: 0.012391, loss: 1.3383
2022-07-01 05:22:45 - train: epoch 0093, iter [02700, 05004], lr: 0.012374, loss: 0.9803
2022-07-01 05:23:18 - train: epoch 0093, iter [02800, 05004], lr: 0.012357, loss: 1.0557
2022-07-01 05:23:51 - train: epoch 0093, iter [02900, 05004], lr: 0.012339, loss: 1.1153
2022-07-01 05:24:24 - train: epoch 0093, iter [03000, 05004], lr: 0.012322, loss: 1.0605
2022-07-01 05:24:57 - train: epoch 0093, iter [03100, 05004], lr: 0.012305, loss: 1.3136
2022-07-01 05:25:29 - train: epoch 0093, iter [03200, 05004], lr: 0.012288, loss: 0.9722
2022-07-01 05:26:02 - train: epoch 0093, iter [03300, 05004], lr: 0.012271, loss: 1.1972
2022-07-01 05:26:35 - train: epoch 0093, iter [03400, 05004], lr: 0.012254, loss: 1.3219
2022-07-01 05:27:08 - train: epoch 0093, iter [03500, 05004], lr: 0.012236, loss: 0.9894
2022-07-01 05:27:41 - train: epoch 0093, iter [03600, 05004], lr: 0.012219, loss: 1.2314
2022-07-01 05:28:13 - train: epoch 0093, iter [03700, 05004], lr: 0.012202, loss: 1.1240
2022-07-01 05:28:46 - train: epoch 0093, iter [03800, 05004], lr: 0.012185, loss: 0.8849
2022-07-01 05:29:19 - train: epoch 0093, iter [03900, 05004], lr: 0.012168, loss: 1.2130
2022-07-01 05:29:52 - train: epoch 0093, iter [04000, 05004], lr: 0.012151, loss: 1.3427
2022-07-01 05:30:25 - train: epoch 0093, iter [04100, 05004], lr: 0.012134, loss: 1.2306
2022-07-01 05:30:57 - train: epoch 0093, iter [04200, 05004], lr: 0.012117, loss: 1.1637
2022-07-01 05:31:30 - train: epoch 0093, iter [04300, 05004], lr: 0.012100, loss: 1.3194
2022-07-01 05:32:03 - train: epoch 0093, iter [04400, 05004], lr: 0.012083, loss: 1.0785
2022-07-01 05:32:35 - train: epoch 0093, iter [04500, 05004], lr: 0.012065, loss: 1.0577
2022-07-01 05:33:08 - train: epoch 0093, iter [04600, 05004], lr: 0.012048, loss: 0.9730
2022-07-01 05:33:41 - train: epoch 0093, iter [04700, 05004], lr: 0.012031, loss: 1.3838
2022-07-01 05:34:14 - train: epoch 0093, iter [04800, 05004], lr: 0.012014, loss: 1.1170
2022-07-01 05:34:47 - train: epoch 0093, iter [04900, 05004], lr: 0.011997, loss: 1.2967
2022-07-01 05:35:19 - train: epoch 0093, iter [05000, 05004], lr: 0.011980, loss: 1.0736
2022-07-01 05:35:21 - train: epoch 093, train_loss: 1.1079
2022-07-01 05:36:36 - eval: epoch: 093, acc1: 73.852%, acc5: 91.724%, test_loss: 1.0581, per_image_load_time: 2.281ms, per_image_inference_time: 0.647ms
2022-07-01 05:36:37 - until epoch: 093, best_acc1: 73.852%
2022-07-01 05:36:37 - epoch 094 lr: 0.011980
2022-07-01 05:37:17 - train: epoch 0094, iter [00100, 05004], lr: 0.011963, loss: 1.0758
2022-07-01 05:37:49 - train: epoch 0094, iter [00200, 05004], lr: 0.011946, loss: 1.1678
2022-07-01 05:38:21 - train: epoch 0094, iter [00300, 05004], lr: 0.011929, loss: 1.2642
2022-07-01 05:38:53 - train: epoch 0094, iter [00400, 05004], lr: 0.011912, loss: 1.2683
2022-07-01 05:39:25 - train: epoch 0094, iter [00500, 05004], lr: 0.011895, loss: 1.0129
2022-07-01 05:39:58 - train: epoch 0094, iter [00600, 05004], lr: 0.011878, loss: 0.9887
2022-07-01 05:40:30 - train: epoch 0094, iter [00700, 05004], lr: 0.011861, loss: 1.1406
2022-07-01 05:41:03 - train: epoch 0094, iter [00800, 05004], lr: 0.011844, loss: 0.9968
2022-07-01 05:41:36 - train: epoch 0094, iter [00900, 05004], lr: 0.011827, loss: 1.0107
2022-07-01 05:42:08 - train: epoch 0094, iter [01000, 05004], lr: 0.011810, loss: 1.1862
2022-07-01 05:42:41 - train: epoch 0094, iter [01100, 05004], lr: 0.011793, loss: 1.2217
2022-07-01 05:43:14 - train: epoch 0094, iter [01200, 05004], lr: 0.011777, loss: 1.1008
2022-07-01 05:43:47 - train: epoch 0094, iter [01300, 05004], lr: 0.011760, loss: 1.0976
2022-07-01 05:44:20 - train: epoch 0094, iter [01400, 05004], lr: 0.011743, loss: 1.0290
2022-07-01 05:44:53 - train: epoch 0094, iter [01500, 05004], lr: 0.011726, loss: 1.2031
2022-07-01 05:45:26 - train: epoch 0094, iter [01600, 05004], lr: 0.011709, loss: 1.4981
2022-07-01 05:45:58 - train: epoch 0094, iter [01700, 05004], lr: 0.011692, loss: 1.0335
2022-07-01 05:46:31 - train: epoch 0094, iter [01800, 05004], lr: 0.011676, loss: 1.0627
2022-07-01 05:47:04 - train: epoch 0094, iter [01900, 05004], lr: 0.011659, loss: 1.1284
2022-07-01 05:47:37 - train: epoch 0094, iter [02000, 05004], lr: 0.011642, loss: 0.8519
2022-07-01 05:48:10 - train: epoch 0094, iter [02100, 05004], lr: 0.011625, loss: 1.1222
2022-07-01 05:48:43 - train: epoch 0094, iter [02200, 05004], lr: 0.011608, loss: 0.9183
2022-07-01 05:49:16 - train: epoch 0094, iter [02300, 05004], lr: 0.011592, loss: 0.8825
2022-07-01 05:49:49 - train: epoch 0094, iter [02400, 05004], lr: 0.011575, loss: 1.0659
2022-07-01 05:50:22 - train: epoch 0094, iter [02500, 05004], lr: 0.011558, loss: 1.0366
2022-07-01 05:50:55 - train: epoch 0094, iter [02600, 05004], lr: 0.011542, loss: 1.0162
2022-07-01 05:51:28 - train: epoch 0094, iter [02700, 05004], lr: 0.011525, loss: 1.0283
2022-07-01 05:52:02 - train: epoch 0094, iter [02800, 05004], lr: 0.011508, loss: 1.0738
2022-07-01 05:52:35 - train: epoch 0094, iter [02900, 05004], lr: 0.011491, loss: 1.1180
2022-07-01 05:53:08 - train: epoch 0094, iter [03000, 05004], lr: 0.011475, loss: 1.0550
2022-07-01 05:53:41 - train: epoch 0094, iter [03100, 05004], lr: 0.011458, loss: 1.2391
2022-07-01 05:54:14 - train: epoch 0094, iter [03200, 05004], lr: 0.011441, loss: 1.1352
2022-07-01 05:54:47 - train: epoch 0094, iter [03300, 05004], lr: 0.011425, loss: 1.0798
2022-07-01 05:55:20 - train: epoch 0094, iter [03400, 05004], lr: 0.011408, loss: 1.1436
2022-07-01 05:55:52 - train: epoch 0094, iter [03500, 05004], lr: 0.011391, loss: 1.3089
2022-07-01 05:56:25 - train: epoch 0094, iter [03600, 05004], lr: 0.011375, loss: 1.0641
2022-07-01 05:56:58 - train: epoch 0094, iter [03700, 05004], lr: 0.011358, loss: 1.2640
2022-07-01 05:57:31 - train: epoch 0094, iter [03800, 05004], lr: 0.011342, loss: 1.0148
2022-07-01 05:58:04 - train: epoch 0094, iter [03900, 05004], lr: 0.011325, loss: 1.0462
2022-07-01 05:58:36 - train: epoch 0094, iter [04000, 05004], lr: 0.011309, loss: 1.2590
2022-07-01 05:59:09 - train: epoch 0094, iter [04100, 05004], lr: 0.011292, loss: 1.3763
2022-07-01 05:59:42 - train: epoch 0094, iter [04200, 05004], lr: 0.011275, loss: 0.8996
2022-07-01 06:00:15 - train: epoch 0094, iter [04300, 05004], lr: 0.011259, loss: 1.2030
2022-07-01 06:00:48 - train: epoch 0094, iter [04400, 05004], lr: 0.011242, loss: 1.2891
2022-07-01 06:01:21 - train: epoch 0094, iter [04500, 05004], lr: 0.011226, loss: 1.0970
2022-07-01 06:01:54 - train: epoch 0094, iter [04600, 05004], lr: 0.011209, loss: 1.1365
2022-07-01 06:02:27 - train: epoch 0094, iter [04700, 05004], lr: 0.011193, loss: 1.1398
2022-07-01 06:03:00 - train: epoch 0094, iter [04800, 05004], lr: 0.011176, loss: 1.0607
2022-07-01 06:03:33 - train: epoch 0094, iter [04900, 05004], lr: 0.011160, loss: 1.0808
2022-07-01 06:04:05 - train: epoch 0094, iter [05000, 05004], lr: 0.011143, loss: 0.9875
2022-07-01 06:04:07 - train: epoch 094, train_loss: 1.0921
2022-07-01 06:05:23 - eval: epoch: 094, acc1: 73.478%, acc5: 91.716%, test_loss: 1.0577, per_image_load_time: 1.252ms, per_image_inference_time: 0.639ms
2022-07-01 06:05:24 - until epoch: 094, best_acc1: 73.852%
2022-07-01 06:05:24 - epoch 095 lr: 0.011143
2022-07-01 06:06:03 - train: epoch 0095, iter [00100, 05004], lr: 0.011126, loss: 0.8738
2022-07-01 06:06:36 - train: epoch 0095, iter [00200, 05004], lr: 0.011110, loss: 1.1656
2022-07-01 06:07:08 - train: epoch 0095, iter [00300, 05004], lr: 0.011093, loss: 0.9235
2022-07-01 06:07:41 - train: epoch 0095, iter [00400, 05004], lr: 0.011077, loss: 1.1412
2022-07-01 06:08:14 - train: epoch 0095, iter [00500, 05004], lr: 0.011061, loss: 1.1502
2022-07-01 06:08:47 - train: epoch 0095, iter [00600, 05004], lr: 0.011044, loss: 1.0400
2022-07-01 06:09:20 - train: epoch 0095, iter [00700, 05004], lr: 0.011028, loss: 1.2101
2022-07-01 06:09:52 - train: epoch 0095, iter [00800, 05004], lr: 0.011011, loss: 1.2028
2022-07-01 06:10:25 - train: epoch 0095, iter [00900, 05004], lr: 0.010995, loss: 1.1631
2022-07-01 06:10:58 - train: epoch 0095, iter [01000, 05004], lr: 0.010979, loss: 1.1535
2022-07-01 06:11:31 - train: epoch 0095, iter [01100, 05004], lr: 0.010962, loss: 0.9281
2022-07-01 06:12:04 - train: epoch 0095, iter [01200, 05004], lr: 0.010946, loss: 0.8846
2022-07-01 06:12:37 - train: epoch 0095, iter [01300, 05004], lr: 0.010930, loss: 0.9958
2022-07-01 06:13:10 - train: epoch 0095, iter [01400, 05004], lr: 0.010913, loss: 1.0355
2022-07-01 06:13:43 - train: epoch 0095, iter [01500, 05004], lr: 0.010897, loss: 1.0759
2022-07-01 06:14:15 - train: epoch 0095, iter [01600, 05004], lr: 0.010881, loss: 0.7858
2022-07-01 06:14:48 - train: epoch 0095, iter [01700, 05004], lr: 0.010864, loss: 1.1369
2022-07-01 06:15:21 - train: epoch 0095, iter [01800, 05004], lr: 0.010848, loss: 1.0964
2022-07-01 06:15:53 - train: epoch 0095, iter [01900, 05004], lr: 0.010832, loss: 1.0286
2022-07-01 06:16:26 - train: epoch 0095, iter [02000, 05004], lr: 0.010816, loss: 1.1223
2022-07-01 06:16:59 - train: epoch 0095, iter [02100, 05004], lr: 0.010799, loss: 1.1387
2022-07-01 06:17:32 - train: epoch 0095, iter [02200, 05004], lr: 0.010783, loss: 0.8154
2022-07-01 06:18:05 - train: epoch 0095, iter [02300, 05004], lr: 0.010767, loss: 1.0410
2022-07-01 06:18:37 - train: epoch 0095, iter [02400, 05004], lr: 0.010751, loss: 1.1443
2022-07-01 06:19:10 - train: epoch 0095, iter [02500, 05004], lr: 0.010734, loss: 0.9889
2022-07-01 06:19:43 - train: epoch 0095, iter [02600, 05004], lr: 0.010718, loss: 1.1496
2022-07-01 06:20:16 - train: epoch 0095, iter [02700, 05004], lr: 0.010702, loss: 1.1416
2022-07-01 06:20:49 - train: epoch 0095, iter [02800, 05004], lr: 0.010686, loss: 0.9416
2022-07-01 06:21:22 - train: epoch 0095, iter [02900, 05004], lr: 0.010670, loss: 1.0102
2022-07-01 06:21:54 - train: epoch 0095, iter [03000, 05004], lr: 0.010654, loss: 1.2727
2022-07-01 06:22:27 - train: epoch 0095, iter [03100, 05004], lr: 0.010638, loss: 1.2401
2022-07-01 06:23:00 - train: epoch 0095, iter [03200, 05004], lr: 0.010621, loss: 1.1199
2022-07-01 06:23:33 - train: epoch 0095, iter [03300, 05004], lr: 0.010605, loss: 1.1294
2022-07-01 06:24:07 - train: epoch 0095, iter [03400, 05004], lr: 0.010589, loss: 1.0943
2022-07-01 06:24:39 - train: epoch 0095, iter [03500, 05004], lr: 0.010573, loss: 1.0173
2022-07-01 06:25:12 - train: epoch 0095, iter [03600, 05004], lr: 0.010557, loss: 1.1212
2022-07-01 06:25:45 - train: epoch 0095, iter [03700, 05004], lr: 0.010541, loss: 1.0211
2022-07-01 06:26:18 - train: epoch 0095, iter [03800, 05004], lr: 0.010525, loss: 0.8475
2022-07-01 06:26:51 - train: epoch 0095, iter [03900, 05004], lr: 0.010509, loss: 1.2130
2022-07-01 06:27:24 - train: epoch 0095, iter [04000, 05004], lr: 0.010493, loss: 0.8887
2022-07-01 06:27:56 - train: epoch 0095, iter [04100, 05004], lr: 0.010477, loss: 1.1499
2022-07-01 06:28:29 - train: epoch 0095, iter [04200, 05004], lr: 0.010461, loss: 0.9445
2022-07-01 06:29:02 - train: epoch 0095, iter [04300, 05004], lr: 0.010445, loss: 1.1845
2022-07-01 06:29:35 - train: epoch 0095, iter [04400, 05004], lr: 0.010429, loss: 1.1676
2022-07-01 06:30:08 - train: epoch 0095, iter [04500, 05004], lr: 0.010413, loss: 0.9547
2022-07-01 06:30:41 - train: epoch 0095, iter [04600, 05004], lr: 0.010397, loss: 1.1358
2022-07-01 06:31:14 - train: epoch 0095, iter [04700, 05004], lr: 0.010381, loss: 0.9827
2022-07-01 06:31:47 - train: epoch 0095, iter [04800, 05004], lr: 0.010365, loss: 1.1869
2022-07-01 06:32:20 - train: epoch 0095, iter [04900, 05004], lr: 0.010349, loss: 1.0128
2022-07-01 06:32:52 - train: epoch 0095, iter [05000, 05004], lr: 0.010333, loss: 0.9281
2022-07-01 06:32:54 - train: epoch 095, train_loss: 1.0735
2022-07-01 06:34:11 - eval: epoch: 095, acc1: 74.082%, acc5: 92.018%, test_loss: 1.0366, per_image_load_time: 2.317ms, per_image_inference_time: 0.658ms
2022-07-01 06:34:12 - until epoch: 095, best_acc1: 74.082%
2022-07-01 06:34:12 - epoch 096 lr: 0.010332
2022-07-01 06:34:53 - train: epoch 0096, iter [00100, 05004], lr: 0.010316, loss: 1.0736
2022-07-01 06:35:25 - train: epoch 0096, iter [00200, 05004], lr: 0.010301, loss: 1.0768
2022-07-01 06:35:58 - train: epoch 0096, iter [00300, 05004], lr: 0.010285, loss: 1.1003
2022-07-01 06:36:30 - train: epoch 0096, iter [00400, 05004], lr: 0.010269, loss: 0.9394
2022-07-01 06:37:04 - train: epoch 0096, iter [00500, 05004], lr: 0.010253, loss: 0.9685
2022-07-01 06:37:36 - train: epoch 0096, iter [00600, 05004], lr: 0.010237, loss: 1.1687
2022-07-01 06:38:10 - train: epoch 0096, iter [00700, 05004], lr: 0.010221, loss: 0.9303
2022-07-01 06:38:43 - train: epoch 0096, iter [00800, 05004], lr: 0.010205, loss: 0.9914
2022-07-01 06:39:16 - train: epoch 0096, iter [00900, 05004], lr: 0.010189, loss: 1.0805
2022-07-01 06:39:49 - train: epoch 0096, iter [01000, 05004], lr: 0.010174, loss: 0.9640
2022-07-01 06:40:23 - train: epoch 0096, iter [01100, 05004], lr: 0.010158, loss: 1.0755
2022-07-01 06:40:56 - train: epoch 0096, iter [01200, 05004], lr: 0.010142, loss: 1.0626
2022-07-01 06:41:30 - train: epoch 0096, iter [01300, 05004], lr: 0.010126, loss: 0.9566
2022-07-01 06:42:03 - train: epoch 0096, iter [01400, 05004], lr: 0.010110, loss: 1.0649
2022-07-01 06:42:36 - train: epoch 0096, iter [01500, 05004], lr: 0.010095, loss: 0.9834
2022-07-01 06:43:10 - train: epoch 0096, iter [01600, 05004], lr: 0.010079, loss: 0.8381
2022-07-01 06:43:43 - train: epoch 0096, iter [01700, 05004], lr: 0.010063, loss: 1.1151
2022-07-01 06:44:17 - train: epoch 0096, iter [01800, 05004], lr: 0.010047, loss: 1.2004
2022-07-01 06:44:50 - train: epoch 0096, iter [01900, 05004], lr: 0.010032, loss: 1.1249
2022-07-01 06:45:23 - train: epoch 0096, iter [02000, 05004], lr: 0.010016, loss: 0.9382
2022-07-01 06:45:56 - train: epoch 0096, iter [02100, 05004], lr: 0.010000, loss: 0.9848
2022-07-01 06:46:30 - train: epoch 0096, iter [02200, 05004], lr: 0.009985, loss: 0.8740
2022-07-01 06:47:03 - train: epoch 0096, iter [02300, 05004], lr: 0.009969, loss: 0.9840
2022-07-01 06:47:36 - train: epoch 0096, iter [02400, 05004], lr: 0.009953, loss: 0.8886
2022-07-01 06:48:09 - train: epoch 0096, iter [02500, 05004], lr: 0.009938, loss: 1.0056
2022-07-01 06:48:43 - train: epoch 0096, iter [02600, 05004], lr: 0.009922, loss: 1.0888
2022-07-01 06:49:16 - train: epoch 0096, iter [02700, 05004], lr: 0.009906, loss: 1.0604
2022-07-01 06:49:49 - train: epoch 0096, iter [02800, 05004], lr: 0.009891, loss: 1.1175
2022-07-01 06:50:22 - train: epoch 0096, iter [02900, 05004], lr: 0.009875, loss: 0.9743
2022-07-01 06:50:56 - train: epoch 0096, iter [03000, 05004], lr: 0.009860, loss: 1.0387
2022-07-01 06:51:29 - train: epoch 0096, iter [03100, 05004], lr: 0.009844, loss: 1.1308
2022-07-01 06:52:02 - train: epoch 0096, iter [03200, 05004], lr: 0.009828, loss: 1.0598
2022-07-01 06:52:35 - train: epoch 0096, iter [03300, 05004], lr: 0.009813, loss: 1.1303
2022-07-01 06:53:09 - train: epoch 0096, iter [03400, 05004], lr: 0.009797, loss: 0.9167
2022-07-01 06:53:43 - train: epoch 0096, iter [03500, 05004], lr: 0.009782, loss: 0.9479
2022-07-01 06:54:16 - train: epoch 0096, iter [03600, 05004], lr: 0.009766, loss: 0.9101
2022-07-01 06:54:50 - train: epoch 0096, iter [03700, 05004], lr: 0.009751, loss: 1.0554
2022-07-01 06:55:23 - train: epoch 0096, iter [03800, 05004], lr: 0.009735, loss: 1.0522
2022-07-01 06:55:57 - train: epoch 0096, iter [03900, 05004], lr: 0.009720, loss: 0.9862
2022-07-01 06:56:30 - train: epoch 0096, iter [04000, 05004], lr: 0.009704, loss: 0.8841
2022-07-01 06:57:03 - train: epoch 0096, iter [04100, 05004], lr: 0.009689, loss: 0.9620
2022-07-01 06:57:37 - train: epoch 0096, iter [04200, 05004], lr: 0.009673, loss: 1.1024
2022-07-01 06:58:09 - train: epoch 0096, iter [04300, 05004], lr: 0.009658, loss: 0.7865
2022-07-01 06:58:43 - train: epoch 0096, iter [04400, 05004], lr: 0.009642, loss: 0.9728
2022-07-01 06:59:16 - train: epoch 0096, iter [04500, 05004], lr: 0.009627, loss: 0.9358
2022-07-01 06:59:50 - train: epoch 0096, iter [04600, 05004], lr: 0.009611, loss: 1.0722
2022-07-01 07:00:24 - train: epoch 0096, iter [04700, 05004], lr: 0.009596, loss: 0.9522
2022-07-01 07:00:57 - train: epoch 0096, iter [04800, 05004], lr: 0.009581, loss: 1.1126
2022-07-01 07:01:31 - train: epoch 0096, iter [04900, 05004], lr: 0.009565, loss: 1.2411
2022-07-01 07:02:04 - train: epoch 0096, iter [05000, 05004], lr: 0.009550, loss: 1.0057
2022-07-01 07:02:06 - train: epoch 096, train_loss: 1.0507
2022-07-01 07:03:23 - eval: epoch: 096, acc1: 74.134%, acc5: 92.210%, test_loss: 1.0333, per_image_load_time: 1.022ms, per_image_inference_time: 0.663ms
2022-07-01 07:03:24 - until epoch: 096, best_acc1: 74.134%
2022-07-01 07:03:24 - epoch 097 lr: 0.009549
2022-07-01 07:04:04 - train: epoch 0097, iter [00100, 05004], lr: 0.009534, loss: 1.0976
2022-07-01 07:04:37 - train: epoch 0097, iter [00200, 05004], lr: 0.009518, loss: 0.8638
2022-07-01 07:05:10 - train: epoch 0097, iter [00300, 05004], lr: 0.009503, loss: 1.1785
2022-07-01 07:05:43 - train: epoch 0097, iter [00400, 05004], lr: 0.009488, loss: 1.2655
2022-07-01 07:06:16 - train: epoch 0097, iter [00500, 05004], lr: 0.009472, loss: 1.0117
2022-07-01 07:06:49 - train: epoch 0097, iter [00600, 05004], lr: 0.009457, loss: 1.1429
2022-07-01 07:07:22 - train: epoch 0097, iter [00700, 05004], lr: 0.009442, loss: 0.9554
2022-07-01 07:07:55 - train: epoch 0097, iter [00800, 05004], lr: 0.009426, loss: 0.9031
2022-07-01 07:08:28 - train: epoch 0097, iter [00900, 05004], lr: 0.009411, loss: 0.9012
2022-07-01 07:09:01 - train: epoch 0097, iter [01000, 05004], lr: 0.009396, loss: 0.9794
2022-07-01 07:09:34 - train: epoch 0097, iter [01100, 05004], lr: 0.009381, loss: 0.8367
2022-07-01 07:10:07 - train: epoch 0097, iter [01200, 05004], lr: 0.009365, loss: 0.9626
2022-07-01 07:10:41 - train: epoch 0097, iter [01300, 05004], lr: 0.009350, loss: 0.9764
2022-07-01 07:11:14 - train: epoch 0097, iter [01400, 05004], lr: 0.009335, loss: 0.9357
2022-07-01 07:11:47 - train: epoch 0097, iter [01500, 05004], lr: 0.009320, loss: 1.0362
2022-07-01 07:12:21 - train: epoch 0097, iter [01600, 05004], lr: 0.009305, loss: 0.9774
2022-07-01 07:12:55 - train: epoch 0097, iter [01700, 05004], lr: 0.009289, loss: 0.9554
2022-07-01 07:13:28 - train: epoch 0097, iter [01800, 05004], lr: 0.009274, loss: 1.0278
2022-07-01 07:14:01 - train: epoch 0097, iter [01900, 05004], lr: 0.009259, loss: 1.0599
2022-07-01 07:14:35 - train: epoch 0097, iter [02000, 05004], lr: 0.009244, loss: 0.9594
2022-07-01 07:15:08 - train: epoch 0097, iter [02100, 05004], lr: 0.009229, loss: 1.0739
2022-07-01 07:15:42 - train: epoch 0097, iter [02200, 05004], lr: 0.009214, loss: 1.2385
2022-07-01 07:16:15 - train: epoch 0097, iter [02300, 05004], lr: 0.009198, loss: 0.9065
2022-07-01 07:16:49 - train: epoch 0097, iter [02400, 05004], lr: 0.009183, loss: 1.0439
2022-07-01 07:17:22 - train: epoch 0097, iter [02500, 05004], lr: 0.009168, loss: 1.0027
2022-07-01 07:17:56 - train: epoch 0097, iter [02600, 05004], lr: 0.009153, loss: 1.0827
2022-07-01 07:18:29 - train: epoch 0097, iter [02700, 05004], lr: 0.009138, loss: 0.9452
2022-07-01 07:19:03 - train: epoch 0097, iter [02800, 05004], lr: 0.009123, loss: 1.0134
2022-07-01 07:19:36 - train: epoch 0097, iter [02900, 05004], lr: 0.009108, loss: 1.1889
2022-07-01 07:20:09 - train: epoch 0097, iter [03000, 05004], lr: 0.009093, loss: 0.9829
2022-07-01 07:20:42 - train: epoch 0097, iter [03100, 05004], lr: 0.009078, loss: 0.9292
2022-07-01 07:21:16 - train: epoch 0097, iter [03200, 05004], lr: 0.009063, loss: 1.1989
2022-07-01 07:21:49 - train: epoch 0097, iter [03300, 05004], lr: 0.009048, loss: 1.2056
2022-07-01 07:22:22 - train: epoch 0097, iter [03400, 05004], lr: 0.009033, loss: 1.2225
2022-07-01 07:22:55 - train: epoch 0097, iter [03500, 05004], lr: 0.009018, loss: 1.1518
2022-07-01 07:23:28 - train: epoch 0097, iter [03600, 05004], lr: 0.009003, loss: 1.0199
2022-07-01 07:24:01 - train: epoch 0097, iter [03700, 05004], lr: 0.008988, loss: 0.9051
2022-07-01 07:24:34 - train: epoch 0097, iter [03800, 05004], lr: 0.008973, loss: 0.8647
2022-07-01 07:25:07 - train: epoch 0097, iter [03900, 05004], lr: 0.008958, loss: 1.0529
2022-07-01 07:25:40 - train: epoch 0097, iter [04000, 05004], lr: 0.008943, loss: 1.0457
2022-07-01 07:26:14 - train: epoch 0097, iter [04100, 05004], lr: 0.008928, loss: 1.0021
2022-07-01 07:26:47 - train: epoch 0097, iter [04200, 05004], lr: 0.008913, loss: 1.0058
2022-07-01 07:27:20 - train: epoch 0097, iter [04300, 05004], lr: 0.008898, loss: 0.9127
2022-07-01 07:27:53 - train: epoch 0097, iter [04400, 05004], lr: 0.008883, loss: 1.0143
2022-07-01 07:28:26 - train: epoch 0097, iter [04500, 05004], lr: 0.008869, loss: 1.0290
2022-07-01 07:29:00 - train: epoch 0097, iter [04600, 05004], lr: 0.008854, loss: 1.2126
2022-07-01 07:29:34 - train: epoch 0097, iter [04700, 05004], lr: 0.008839, loss: 1.1019
2022-07-01 07:30:07 - train: epoch 0097, iter [04800, 05004], lr: 0.008824, loss: 1.0730
2022-07-01 07:30:41 - train: epoch 0097, iter [04900, 05004], lr: 0.008809, loss: 1.2488
2022-07-01 07:31:14 - train: epoch 0097, iter [05000, 05004], lr: 0.008794, loss: 1.0788
2022-07-01 07:31:15 - train: epoch 097, train_loss: 1.0335
2022-07-01 07:32:33 - eval: epoch: 097, acc1: 74.656%, acc5: 92.312%, test_loss: 1.0164, per_image_load_time: 2.353ms, per_image_inference_time: 0.634ms
2022-07-01 07:32:34 - until epoch: 097, best_acc1: 74.656%
2022-07-01 07:32:34 - epoch 098 lr: 0.008794
2022-07-01 07:33:14 - train: epoch 0098, iter [00100, 05004], lr: 0.008779, loss: 1.1732
2022-07-01 07:33:47 - train: epoch 0098, iter [00200, 05004], lr: 0.008764, loss: 1.0367
2022-07-01 07:34:19 - train: epoch 0098, iter [00300, 05004], lr: 0.008749, loss: 1.1798
2022-07-01 07:34:52 - train: epoch 0098, iter [00400, 05004], lr: 0.008735, loss: 0.7891
2022-07-01 07:35:25 - train: epoch 0098, iter [00500, 05004], lr: 0.008720, loss: 0.9796
2022-07-01 07:35:58 - train: epoch 0098, iter [00600, 05004], lr: 0.008705, loss: 1.0952
2022-07-01 07:36:31 - train: epoch 0098, iter [00700, 05004], lr: 0.008690, loss: 0.9846
2022-07-01 07:37:04 - train: epoch 0098, iter [00800, 05004], lr: 0.008676, loss: 1.0475
2022-07-01 07:37:37 - train: epoch 0098, iter [00900, 05004], lr: 0.008661, loss: 1.0989
2022-07-01 07:38:10 - train: epoch 0098, iter [01000, 05004], lr: 0.008646, loss: 0.8726
2022-07-01 07:38:43 - train: epoch 0098, iter [01100, 05004], lr: 0.008631, loss: 0.8405
2022-07-01 07:39:16 - train: epoch 0098, iter [01200, 05004], lr: 0.008617, loss: 0.8769
2022-07-01 07:39:50 - train: epoch 0098, iter [01300, 05004], lr: 0.008602, loss: 1.1063
2022-07-01 07:40:23 - train: epoch 0098, iter [01400, 05004], lr: 0.008587, loss: 0.9324
2022-07-01 07:40:55 - train: epoch 0098, iter [01500, 05004], lr: 0.008573, loss: 0.9543
2022-07-01 07:41:28 - train: epoch 0098, iter [01600, 05004], lr: 0.008558, loss: 0.9495
2022-07-01 07:42:01 - train: epoch 0098, iter [01700, 05004], lr: 0.008543, loss: 1.0378
2022-07-01 07:42:34 - train: epoch 0098, iter [01800, 05004], lr: 0.008529, loss: 0.9790
2022-07-01 07:43:08 - train: epoch 0098, iter [01900, 05004], lr: 0.008514, loss: 0.8211
2022-07-01 07:43:41 - train: epoch 0098, iter [02000, 05004], lr: 0.008500, loss: 1.2045
2022-07-01 07:44:14 - train: epoch 0098, iter [02100, 05004], lr: 0.008485, loss: 0.9076
2022-07-01 07:44:47 - train: epoch 0098, iter [02200, 05004], lr: 0.008470, loss: 0.9809
2022-07-01 07:45:21 - train: epoch 0098, iter [02300, 05004], lr: 0.008456, loss: 1.0238
2022-07-01 07:45:54 - train: epoch 0098, iter [02400, 05004], lr: 0.008441, loss: 0.9652
2022-07-01 07:46:27 - train: epoch 0098, iter [02500, 05004], lr: 0.008427, loss: 1.1614
2022-07-01 07:47:00 - train: epoch 0098, iter [02600, 05004], lr: 0.008412, loss: 0.9976
2022-07-01 07:47:33 - train: epoch 0098, iter [02700, 05004], lr: 0.008398, loss: 1.0884
2022-07-01 07:48:06 - train: epoch 0098, iter [02800, 05004], lr: 0.008383, loss: 1.0061
2022-07-01 07:48:40 - train: epoch 0098, iter [02900, 05004], lr: 0.008369, loss: 0.9843
2022-07-01 07:49:13 - train: epoch 0098, iter [03000, 05004], lr: 0.008354, loss: 0.9961
2022-07-01 07:49:47 - train: epoch 0098, iter [03100, 05004], lr: 0.008340, loss: 0.9770
2022-07-01 07:50:20 - train: epoch 0098, iter [03200, 05004], lr: 0.008325, loss: 0.8836
2022-07-01 07:50:54 - train: epoch 0098, iter [03300, 05004], lr: 0.008311, loss: 0.8427
2022-07-01 07:51:27 - train: epoch 0098, iter [03400, 05004], lr: 0.008296, loss: 1.0226
2022-07-01 07:52:00 - train: epoch 0098, iter [03500, 05004], lr: 0.008282, loss: 1.0242
2022-07-01 07:52:33 - train: epoch 0098, iter [03600, 05004], lr: 0.008268, loss: 1.2611
2022-07-01 07:53:06 - train: epoch 0098, iter [03700, 05004], lr: 0.008253, loss: 1.0985
2022-07-01 07:53:39 - train: epoch 0098, iter [03800, 05004], lr: 0.008239, loss: 1.0069
2022-07-01 07:54:13 - train: epoch 0098, iter [03900, 05004], lr: 0.008224, loss: 0.9548
2022-07-01 07:54:46 - train: epoch 0098, iter [04000, 05004], lr: 0.008210, loss: 1.0287
2022-07-01 07:55:20 - train: epoch 0098, iter [04100, 05004], lr: 0.008196, loss: 1.1013
2022-07-01 07:55:53 - train: epoch 0098, iter [04200, 05004], lr: 0.008181, loss: 0.9964
2022-07-01 07:56:27 - train: epoch 0098, iter [04300, 05004], lr: 0.008167, loss: 0.7943
2022-07-01 07:57:00 - train: epoch 0098, iter [04400, 05004], lr: 0.008153, loss: 1.1836
2022-07-01 07:57:34 - train: epoch 0098, iter [04500, 05004], lr: 0.008138, loss: 1.1737
2022-07-01 07:58:07 - train: epoch 0098, iter [04600, 05004], lr: 0.008124, loss: 1.0567
2022-07-01 07:58:40 - train: epoch 0098, iter [04700, 05004], lr: 0.008110, loss: 1.0354
2022-07-01 07:59:14 - train: epoch 0098, iter [04800, 05004], lr: 0.008096, loss: 0.7734
2022-07-01 07:59:46 - train: epoch 0098, iter [04900, 05004], lr: 0.008081, loss: 1.1124
2022-07-01 08:00:19 - train: epoch 0098, iter [05000, 05004], lr: 0.008067, loss: 1.1237
2022-07-01 08:00:21 - train: epoch 098, train_loss: 1.0129
2022-07-01 08:01:38 - eval: epoch: 098, acc1: 74.738%, acc5: 92.276%, test_loss: 1.0087, per_image_load_time: 2.278ms, per_image_inference_time: 0.651ms
2022-07-01 08:01:38 - until epoch: 098, best_acc1: 74.738%
2022-07-01 08:01:38 - epoch 099 lr: 0.008066
2022-07-01 08:02:19 - train: epoch 0099, iter [00100, 05004], lr: 0.008052, loss: 0.8411
2022-07-01 08:02:52 - train: epoch 0099, iter [00200, 05004], lr: 0.008038, loss: 0.8932
2022-07-01 08:03:24 - train: epoch 0099, iter [00300, 05004], lr: 0.008024, loss: 0.9528
2022-07-01 08:03:57 - train: epoch 0099, iter [00400, 05004], lr: 0.008010, loss: 0.9989
2022-07-01 08:04:30 - train: epoch 0099, iter [00500, 05004], lr: 0.007995, loss: 1.0571
2022-07-01 08:05:02 - train: epoch 0099, iter [00600, 05004], lr: 0.007981, loss: 0.9454
2022-07-01 08:05:35 - train: epoch 0099, iter [00700, 05004], lr: 0.007967, loss: 0.9752
2022-07-01 08:06:08 - train: epoch 0099, iter [00800, 05004], lr: 0.007953, loss: 0.9957
2022-07-01 08:06:42 - train: epoch 0099, iter [00900, 05004], lr: 0.007939, loss: 0.8771
2022-07-01 08:07:15 - train: epoch 0099, iter [01000, 05004], lr: 0.007925, loss: 1.0114
2022-07-01 08:07:48 - train: epoch 0099, iter [01100, 05004], lr: 0.007910, loss: 0.9543
2022-07-01 08:08:21 - train: epoch 0099, iter [01200, 05004], lr: 0.007896, loss: 0.8828
2022-07-01 08:08:54 - train: epoch 0099, iter [01300, 05004], lr: 0.007882, loss: 1.0649
2022-07-01 08:09:27 - train: epoch 0099, iter [01400, 05004], lr: 0.007868, loss: 0.9954
2022-07-01 08:10:00 - train: epoch 0099, iter [01500, 05004], lr: 0.007854, loss: 0.8356
2022-07-01 08:10:33 - train: epoch 0099, iter [01600, 05004], lr: 0.007840, loss: 1.3320
2022-07-01 08:11:07 - train: epoch 0099, iter [01700, 05004], lr: 0.007826, loss: 0.8847
2022-07-01 08:11:40 - train: epoch 0099, iter [01800, 05004], lr: 0.007812, loss: 1.0130
2022-07-01 08:12:13 - train: epoch 0099, iter [01900, 05004], lr: 0.007798, loss: 1.0362
2022-07-01 08:12:46 - train: epoch 0099, iter [02000, 05004], lr: 0.007784, loss: 0.8964
2022-07-01 08:13:20 - train: epoch 0099, iter [02100, 05004], lr: 0.007770, loss: 0.8591
2022-07-01 08:13:53 - train: epoch 0099, iter [02200, 05004], lr: 0.007756, loss: 1.0787
2022-07-01 08:14:26 - train: epoch 0099, iter [02300, 05004], lr: 0.007742, loss: 1.0457
2022-07-01 08:14:59 - train: epoch 0099, iter [02400, 05004], lr: 0.007728, loss: 1.0698
2022-07-01 08:15:32 - train: epoch 0099, iter [02500, 05004], lr: 0.007714, loss: 0.8777
2022-07-01 08:16:05 - train: epoch 0099, iter [02600, 05004], lr: 0.007700, loss: 0.9027
2022-07-01 08:16:38 - train: epoch 0099, iter [02700, 05004], lr: 0.007686, loss: 0.9285
2022-07-01 08:17:11 - train: epoch 0099, iter [02800, 05004], lr: 0.007672, loss: 1.2069
2022-07-01 08:17:45 - train: epoch 0099, iter [02900, 05004], lr: 0.007658, loss: 1.0104
2022-07-01 08:18:18 - train: epoch 0099, iter [03000, 05004], lr: 0.007644, loss: 1.1209
2022-07-01 08:18:51 - train: epoch 0099, iter [03100, 05004], lr: 0.007630, loss: 0.8632
2022-07-01 08:19:24 - train: epoch 0099, iter [03200, 05004], lr: 0.007616, loss: 1.0742
2022-07-01 08:19:57 - train: epoch 0099, iter [03300, 05004], lr: 0.007603, loss: 0.9341
2022-07-01 08:20:31 - train: epoch 0099, iter [03400, 05004], lr: 0.007589, loss: 1.0034
2022-07-01 08:21:04 - train: epoch 0099, iter [03500, 05004], lr: 0.007575, loss: 1.1045
2022-07-01 08:21:37 - train: epoch 0099, iter [03600, 05004], lr: 0.007561, loss: 0.8322
2022-07-01 08:22:11 - train: epoch 0099, iter [03700, 05004], lr: 0.007547, loss: 0.8995
2022-07-01 08:22:44 - train: epoch 0099, iter [03800, 05004], lr: 0.007533, loss: 1.1981
2022-07-01 08:23:17 - train: epoch 0099, iter [03900, 05004], lr: 0.007520, loss: 0.9954
2022-07-01 08:23:51 - train: epoch 0099, iter [04000, 05004], lr: 0.007506, loss: 1.1306
2022-07-01 08:24:24 - train: epoch 0099, iter [04100, 05004], lr: 0.007492, loss: 0.9394
2022-07-01 08:24:57 - train: epoch 0099, iter [04200, 05004], lr: 0.007478, loss: 0.9241
2022-07-01 08:25:31 - train: epoch 0099, iter [04300, 05004], lr: 0.007465, loss: 0.9640
2022-07-01 08:26:04 - train: epoch 0099, iter [04400, 05004], lr: 0.007451, loss: 1.0782
2022-07-01 08:26:37 - train: epoch 0099, iter [04500, 05004], lr: 0.007437, loss: 1.0705
2022-07-01 08:27:10 - train: epoch 0099, iter [04600, 05004], lr: 0.007423, loss: 1.1031
2022-07-01 08:27:43 - train: epoch 0099, iter [04700, 05004], lr: 0.007410, loss: 0.9091
2022-07-01 08:28:17 - train: epoch 0099, iter [04800, 05004], lr: 0.007396, loss: 1.2075
2022-07-01 08:28:50 - train: epoch 0099, iter [04900, 05004], lr: 0.007382, loss: 0.9439
2022-07-01 08:29:23 - train: epoch 0099, iter [05000, 05004], lr: 0.007369, loss: 1.0175
2022-07-01 08:29:24 - train: epoch 099, train_loss: 0.9930
2022-07-01 08:30:42 - eval: epoch: 099, acc1: 74.938%, acc5: 92.342%, test_loss: 1.0021, per_image_load_time: 2.382ms, per_image_inference_time: 0.629ms
2022-07-01 08:30:43 - until epoch: 099, best_acc1: 74.938%
2022-07-01 08:30:43 - epoch 100 lr: 0.007368
2022-07-01 08:31:24 - train: epoch 0100, iter [00100, 05004], lr: 0.007354, loss: 0.9609
2022-07-01 08:31:56 - train: epoch 0100, iter [00200, 05004], lr: 0.007341, loss: 1.0736
2022-07-01 08:32:28 - train: epoch 0100, iter [00300, 05004], lr: 0.007327, loss: 0.9094
2022-07-01 08:33:01 - train: epoch 0100, iter [00400, 05004], lr: 0.007313, loss: 0.7895
2022-07-01 08:33:33 - train: epoch 0100, iter [00500, 05004], lr: 0.007300, loss: 1.0619
2022-07-01 08:34:05 - train: epoch 0100, iter [00600, 05004], lr: 0.007286, loss: 1.2027
2022-07-01 08:34:38 - train: epoch 0100, iter [00700, 05004], lr: 0.007273, loss: 0.9018
2022-07-01 08:35:11 - train: epoch 0100, iter [00800, 05004], lr: 0.007259, loss: 1.0516
2022-07-01 08:35:44 - train: epoch 0100, iter [00900, 05004], lr: 0.007245, loss: 0.7660
2022-07-01 08:36:17 - train: epoch 0100, iter [01000, 05004], lr: 0.007232, loss: 1.0419
2022-07-01 08:36:50 - train: epoch 0100, iter [01100, 05004], lr: 0.007218, loss: 0.8638
2022-07-01 08:37:23 - train: epoch 0100, iter [01200, 05004], lr: 0.007205, loss: 0.9095
2022-07-01 08:37:56 - train: epoch 0100, iter [01300, 05004], lr: 0.007191, loss: 1.0928
2022-07-01 08:38:29 - train: epoch 0100, iter [01400, 05004], lr: 0.007178, loss: 1.0053
2022-07-01 08:39:02 - train: epoch 0100, iter [01500, 05004], lr: 0.007164, loss: 0.9723
2022-07-01 08:39:35 - train: epoch 0100, iter [01600, 05004], lr: 0.007151, loss: 0.9549
2022-07-01 08:40:07 - train: epoch 0100, iter [01700, 05004], lr: 0.007137, loss: 1.0362
2022-07-01 08:40:40 - train: epoch 0100, iter [01800, 05004], lr: 0.007124, loss: 1.0289
2022-07-01 08:41:13 - train: epoch 0100, iter [01900, 05004], lr: 0.007110, loss: 0.9037
2022-07-01 08:41:46 - train: epoch 0100, iter [02000, 05004], lr: 0.007097, loss: 0.9736
2022-07-01 08:42:20 - train: epoch 0100, iter [02100, 05004], lr: 0.007084, loss: 0.9411
2022-07-01 08:42:53 - train: epoch 0100, iter [02200, 05004], lr: 0.007070, loss: 0.9977
2022-07-01 08:43:26 - train: epoch 0100, iter [02300, 05004], lr: 0.007057, loss: 0.9709
2022-07-01 08:43:59 - train: epoch 0100, iter [02400, 05004], lr: 0.007043, loss: 0.8589
2022-07-01 08:44:32 - train: epoch 0100, iter [02500, 05004], lr: 0.007030, loss: 0.9841
